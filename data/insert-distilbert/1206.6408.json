{"id": "1206.6408", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Sequential Nonparametric Regression", "abstract": "we present algorithms for nonparametric regression generators in settings where the data are obtained sequentially. while traditional estimators select bandwidths that depend upon preserving the sample size, for sequential convergence data the effective sample size is dynamically changing. basically we then propose a linear time algorithm that adjusts the bandwidth amount for each new data point, and show that the estimator achieves the current optimal minimax rate of convergence. we also propose the use of naive online expert mixing algorithms to adapt to unknown smoothness characteristics of the regression generator function. we provide simulations that confirm the theoretical results, and significantly demonstrate the effectiveness of the methods.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (219kb)", "http://arxiv.org/abs/1206.6408v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "stat.ME astro-ph.IM cs.LG", "authors": ["haijie gu", "john d lafferty"], "accepted": true, "id": "1206.6408"}, "pdf": {"name": "1206.6408.pdf", "metadata": {"source": "CRF", "title": "Sequential Nonparametric Regression", "authors": ["Haijie Gu", "John Lafferty"], "emails": ["haijieg@cs.cmu.edu", "lafferty@galton.uchicago.edu"], "sections": [{"heading": "1. Introduction", "text": "Bandwidth selection is arguably the most important aspect of nonparametric regression using smoothing kernels. It is well understood how the optimal bandwidth for regression depends on the sample size. Considering the one dimensional case, let\nYi = m(Xi) + \u01ebi, i = 1, . . . , n (1.1)\nwhere the Xis are independent and identically distributed, m : R \u2192 R is the unknown function to estimate, and \u01ebi \u223c N(0, \u03c32). Assuming m\u2032\u2032 is absolutely continuous and \u222b m\u2032\u2032(x)2dx < \u221e, the risk of the Nadaraya-Watson kernel regression estimator with bandwidth h has the form\nR(m\u0302n,m) = c1h 4 + c2 nh + o(nh\u22121) + o(h4) (1.2)\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nwhere\nR(m\u0302n,m) = Em \u222b (m\u0302n(x)\u2212m(x))2dx (1.3)\nis defined as the risk of the estimate m\u0302n on a sample of size n. Here c1 and c2 are constants that depend on the kernel and the distribution of X. The optimal bandwidth that minimizes (1.2) thus has order h\u2217 = O(n\u22121/5), which leads to the optimal minimax rate of convergence O(n\u22124/5); see Gyo\u0308rfi et al. (2002). More generally, if we assume further smoothness of m so that the d-th derivative of m exists and is bounded, then the minimax rate n\u22122d/(2d+1) is achieved using local polynomial regression of order d\u22121 with a bandwidth of order h\u2217 = O(n\u22121/(2d+1)) (Fan & Gijbels, 1996). Thus, bandwidth selection is of the essence in nonparametric regression, and a large body of research has been devoted to this problem in various settings.\nThese classical results assume that a training data set of size n is given; the sample size n formally increases to infinity in the theoretical analysis. In an online setting, however, the data arrive sequentially, and the size of the data set is always changing. In this case the bandwidth needs to adapt somehow to the changing sample size. A simple variation of the classical methods would carry out a batch regression on the entire data set seen up to the current time T , with an appropriately sized bandwidth. However, this would require quadratic complexity O(T 2) to compute the estimates after T points are observed, and is prohibitive for large sample sizes. This motivates the problem studied in this paper\u2014to develop computationally efficient estimators in the online setting that preserve the statistical efficiency of the classical batch estimators. We propose an algorithm for sequential regression that requires linear computational cost, and prove that the algorithm achieves the optimal minimax rate of convergence. The essential idea is to avoid recomputation by shrinking the bandwidth for each new observed data point. Our analysis assumes that the data are iid, and the algorithm assumes they are obtained sequentially.\nAlthough online local polynomial smoothing has not been previously studied, significant previous work has been devoted to related problems. Steland (2010) investigates a cross-validation scheme for sequential data and establishes theoretical results. However, this does not consider the cost of recomputing the entire model for each new bandwidth; performing leaveone-out cross-validation adds extra computation and would be impractical for many online settings. Kivinen et al. (2004) develop variants of stochastic gradient descent for online learning in a reproducing kernel Hilbert space. The algorithms require linear computation cost at each step, and their RKHS analysis does not consider selection of tuning parameters for the kernel, or adaptation to the unknown smoothness of the regression function. A great deal of work has been carried out for the problem of adaptation in the classical batch setting. Fan & Gijbels (1995) consider using Residual Squares Criteria (RCS) for performing data-driven bandwidth selection; Ruppert et al. (1995) propose a plug-in bandwidth selection scheme for local linear kernel estimators. These are effective methods for adaptive estimation; however, they do not take into account the computational cost for online updating. The mixing expert framework has been a popular strategy for online prediction, and there is a rich literature on this topic (Cesa-Bianchi & Lugosi, 2006). Results by Bunea & Nobel (2008) give oracle inequalities for regression in terms of generalized simplex combinations of a set of fixed estimators in the online setting. However, this analysis does not allow the experts to change over time, as we require with dynamically changing bandwidths. Further related work is discussed below.\nIn the following section we present the algorithm for sequential local polynomial regression. In Section 3 we outline a theoretical analysis of the risk achieved by this algorithm for both sequential density estimation (Theorem 3.2) and regression (Theorem 3.3). In Section 4 we briefly discuss the problem of adapting to unknown smoothness, using the expert mixing framework and also the extension to additive models. In Section 5 we present experimental results showing that our algorithm is comparable to the batch algorithm but much more computationally efficient, and adapts to unknown smoothness of the true function."}, {"heading": "2. Sequential Local Polynomial Smoothing", "text": "Our efficient sequential estimator is based on local polynomial regression with a sequence of shrinking bandwidths. Among the various nonparametric re-\ngression methods, local polynomial regression enjoys strong minimax properties, and gracefully deals with the problem of boundary bias (Fan et al., 1993). Let {(X1, Y1), (X2, Y2), . . . } be an observation sequence according to the model given in equation (1.1). We assume that the pairs (Xi, Yi) are independent and identically distributed random variables. Throughout we assume for simplicity that the domain of m(x) is x \u2208 [0, 1]. An extension to higher dimensional x is straightforward. We discuss an extension to additive models for the high dimensional case in Section 4.2."}, {"heading": "2.1. Sequential kernel regression", "text": "We first present the simplest version of the method. Recall that the kernel regression (Nadaraya-Watson) estimator in the batch setting is\nm\u0302n(x0) = \u2211n i=1 Kh(Xi, x0)Yi\u2211n i=1 Kh(Xi, x0)\n(2.1)\nwhere h is the bandwidth. Under standard assumptions that the regression function is in a second-order Sobolev space, the bandwidth is chosen as h = c\u00b7n\u22121/5, and the estimator achieves the minimax rate of convergence R(m\u0302n,m) = O(n\n\u22124/5). Now, suppose the data arrive sequentially. Our sequential estimator takes the form\nm\u0303n(x0) = \u2211n t=1 Kht(Xt, x0)Yt\u2211n t=1 Kht(Xt, x0)\n(2.2)\nwhere the bandwidth ht = c \u00b7 t\u22121/5 is used only for the tth point. The algorithm incrementally computes the numerator and denominator, shrinking the bandwidth for each new term added. A corollary of our main technical result is that this estimator achieves the same rate of convergence as the Nadaraya-Watson estimator.\nThis result is surprising since the first examples in the sequence, (X1, Y1), (X2, Y2), . . . , are assigned bandwidths ht = c \u00b7 t\u22121/5 that are too large, introducing a bias in the estimate. However, our analysis shows that this bias is asymptotically \u201cwashed out.\u201d To gain some intuition for how this happens, note that we can write the estimate as\nm\u0303n(x0) = 1 n\n\u2211n t=1 Kht(Xt, x0)Yt\n1 n \u2211n t=1 Kht(Xt, x0) . (2.3)\nDefine f : R \u2192 [0, 1] to be the marginal probability density function of X. As we show below, the denominator is a consistent density estimate of f(x0), and moreover it attains the optimal rate of convergence. The early contributions to the series Kh1(X1, x0)Y1 + Kh2(X1, x0)Y2 + \u00b7 \u00b7 \u00b7 are then effectively weighted by 1/nf\u0302(x0), which removes the bias they introduce as n increases."}, {"heading": "2.2. Sequential local polynomial smoothing", "text": "In the classical batch estimation case, the order-d local polynomial regression at a prediction point x0 is computed by minimizing the locally weighted squared error\nn\u2211\nt=1\nK\n( Xt \u2212 x0\nhn\n)( Yt\u2212 d\u2211\nj=0\n\u03b2j(x0)(Xt\u2212x0)j )2 (2.4)\nwhere K(\u00b7) is the kernel. Denote by \u03b2\u0302(x0) the (d+1)vector that minimizes this objective. The regression estimate is then given by the intercept m\u0302(x0) = \u03b2\u03020(x0). Let Xn be the n\u00d7 (d+ 1) design matrix\nXn =   1 (X1 \u2212 x0) \u00b7 \u00b7 \u00b7 (X1 \u2212 x0)d ... ... ...\n... 1 (Xn \u2212 x0) \u00b7 \u00b7 \u00b7 (Xn \u2212 x0)d\n  .\nFurthermore, let yn = (Y1, Y2, . . . , Yn) T , and let Wn = diag{Khn(Xt, x0)}1\u2264t\u2264n be an n \u00d7 n diagonal matrix of weights, where Kh(Xt, x) = 1 hK( Xt\u2212x h ). Then the solution that minimizes (2.4) is given as\n\u03b2\u0302(x0) = (X T n WnXn) \u22121XTn Wnyn, (2.5)\nand m\u0302(x0) = \u03b2\u03020(x0) is an estimate of the regression function at x0 (Fan et al., 1993).\nIf we were to adapt hn+1 to the increased sample size n+1 at the next time step, we would have to recompute the entire XTWX matrix. To save computation, we allow a variable bandwidth in W . In other words, we select a new bandwidth hn+1 that only applies to (Xn+1, Yn+1).\nSpecifically, we propose the following sequential local polynomial regression algorithm. Let\nW\u0303n = diag{Kht(Xt, x0)}1\u2264t\u2264n where ht = c \u00b7 t\u22121/(2d+1) is the bandwidth that would be asymptotically optimal with respect to a sample of size t, with c a constant. Our proposed online estimate after n samples are observed is then\nm\u0303n(x0) = e T 1 (X T n W\u0303nXn) \u22121XTn W\u0303nyn. (2.6)\nwhere e1 = (1, 0, . . . , 0). Denote by Sn the (d + 1) \u00d7 (d+ 1) matrix XTW\u0303nX, with (i, j) entry\nSn(i, j) = n\u2211\nt=1\nKht(Xt, x0)(Xt \u2212 x0)i+j . (2.7)\nTo update the model after having observed (Xn+1, Yn+1), note that\nSn+1 = Sn +Khn+1(Xn+1, x0)xn+1x T n+1 (2.8)\nwhere xn+1 is the (d + 1) vector (1, (Xn+1 \u2212 x0), (Xn+1\u2212x0)2, . . . , (Xn+1\u2212x0)d). Now, using (2.8) and the Woodbury formula for the inverse of a rankone matrix update,\n(A+ vvT )\u22121 = A\u22121 \u2212A\u22121v(1 + vTA\u22121v)\u22121vTA\u22121, (2.9) we have that updating S\u22121n+1 given S \u22121 n has complexity O(d2). Similarly, updating\nXTn+1W\u0303 n+1yn+1\n=XTnW\u0303 nyn +Khn+1(Xn+1, x0)xn+1Yn+1 (2.10)\nhas O(d) cost. Therefore, the complexity of updating the estimate from a sample of size n to one of size n+1 at each point x0 in a grid G of size |G| is O(|G|d2), independent of n. However, the update cost in the batch setting is O(|G|(nd2+d3)) because changing the bandwidth forces re-evaluating equation (2.5)."}, {"heading": "3. Risk Analysis", "text": "In this section, we give a risk analysis of sequential density estimation and regression. Assuming the regression function m lies in Cd, the class of functions with d continuous derivatives, our goal is to show that the asymptotic risk of the online algorithm given in the previous section achieves the statistical rate of convergence of n\u22122d/(2d+1). This is the minimax optimal rate for this function class.\nWe begin by analyzing the risk of sequential kernel density estimation and kernel regression (d = 2), because the analysis is simpler and more transparent for these special cases than for the general case. We then generalize the results to order d\u2212 1 sequential polynomial regression in Cd.\nWe assume that the true density function f and the true regression function m have d \u2265 2 continuous derivatives. The kernel K is assumed to satisfy the following properties: \u222b K(u) du = 1, \u222b K(u)u du = 0, \u03c32K \u2261 \u222b K(u)u2du < \u221e. We restrict the sequence of bandwidths {ht | t = 1, 2, 3, . . . } to satisfy limt\u2192\u221e ht = 0 and limn\u2192\u221e 1 n2 \u2211n t=1 1 ht = 0."}, {"heading": "3.1. Sequential Kernel Density Estimation", "text": "Our sequential kernel density estimator f\u0303 is given by f\u0303n(x) = 1 n \u2211n t=1 1 ht K ( x\u2212Xt ht ) . This is computed incrementally according to the update rule\nf\u0303n+1(x) = n\nn+ 1 f\u0303n(x)+\n1\n(n+ 1)hn+1 K\n( x\u2212Xn+1\nhn+1\n) .\nThus, updating f\u0303n has cost O(|G|), where |G| is the size of the grid of x values at which the estimates are\nmade.\nLemma 3.1. The risk of the sequential density estimate f\u0303n at time t = n is\nR(f, f\u0303n) = 1\n4\n\u222b f \u2032\u2032(x)2 dx  \u03c32K 1\nn\nn\u2211\nt=1\nh2t\n  2\n(3.1)\n+\n(\u2211n t=1 1 ht\nn2\n)\u222b K2(u) du\n+o\n( ( \u2211n\nt=1 h 2 t ) 2\nn2\n) + o (\u2211n t=1 1 ht\nn2\n) . (3.2)\nThe proof of Lemma 3.1 is given in Section A.1. We note in passing that a similar algorithm for sequential density estimation appears, without theoretical analysis, in Kristan et al. (2010).\nTheorem 3.1. Let ht = c t \u22121/5 for some constant c. Then the risk of sequential kernel density estimator f\u0303n satisfies R(f, f\u0303n) = O(n \u22124/5).\nProof. Let c1 = 1 4 (\u03c3 2 K) 2 \u222b f \u2032\u2032(x)2dx, and c2 =\u222b\nK2(u) du. Then Lemma 3.1 states that\nR(f, f\u0303n) = 1\nn2\n c1   n\u2211\nt=1\nh2t\n  2\n+ c2\nn\u2211\nt=1\n1\nht\n  . (3.3)\nLet ht = c t \u2212k for 0 < k < 12 . Then with this choice of bandwidth sequence\nR(f, f\u0303n) = 1\nn2\n c1   n\u2211\nt=1\nt\u22122k\n  2\n+ c2\nn\u2211\nt=1\ntk   (3.4)\n\u2264 1 n2\n[ c1 (\u222b n\n0\nt\u22122kdt )2 + c2n k+1 ] (3.5)\n= c1 1\n(1\u2212 2k)2n \u22124k + c2n k\u22121. (3.6)\nMinimizing over k, we find that k\u2217 = 15 , and optimal risk is of order R\u2217 = O(n\u22124/5).\nThe above analysis assumes that the density f has a continuous second derivative. The result can be extended to the case where f is in Cd, using a higher order kernel satisfying \u03c3jK = \u222b ujK(u) du = 0 for all j < d.\nTheorem 3.2. If the density function f lies in Cd, then the optimal risk of the sequential kernel density estimator f\u0303n satisfies R \u2217 = O(n\u22122d/2d+1) when the bandwidth sequence is taken to be h\u2217t = c t \u22121/2d+1.\nProof sketch. By a Taylor expansion and calculations similar to those in the proof of Lemma 3.1, we have\nEf\u0303n(x) = 1\nn\nn\u2211\nt=1\n\u222b K(u)f(x\u2212 htu)du (3.7)\n= 1\nn\nn\u2211\nt=1\n[ f(x) +\nh2t 2 f \u2032\u2032(x)\u03c32K + \u00b7 \u00b7 \u00b7+ hdt d! fd(x)\u03c3dK\n]\n+ o(hdt ).\nAssuming a higher order kernel, we have \u03c3jK = 0 for j < d, so the leading order of the bias is\nBias(f\u0303n(x)) = 1\nd! fd(x)\u03c3dK\n(\u2211n t=1 h d i\nn\n) + o (\u2211n t=1 h d i\nn\n) .\nThe variance satisfies Var(f\u0303n(x)) = f(x) \u222b K2(u)du \u00b7(\n1 n2 \u2211n t=1 1 ht ) +o ( 1 n2 \u2211n t=1 1 ht ) as shown in the proof of Lemma 3.1. Thus, using the bias-variance decomposition the risk takes the form\nc1   1 n n\u2211\nt=1\nhdt\n  2\n+ c2 1\nn2\nn\u2211\nt=1\n1\nht . (3.8)\nAssuming that ht = c t \u2212k for k > 0 and optimizing over k yields k\u2217 = 12d+1 , bandwidth sequence h \u2217 t = c t\u22121/2d+1, and optimal risk R\u2217 = O(n\u22122d/2d+1)."}, {"heading": "3.2. Sequential Local Polynomial Regression", "text": "Similar results hold for the sequential local polynomial regression estimator. In particular, for d = 2 we use\nm\u0303n(x) = 1\nnf\u0303n(x)\nn\u2211\nt=1\nKht(x,Xt)Yt (3.9)\nwhere f\u0303n(x) = 1 n \u2211n t=1 Kht(x,Xt). The following result is proved in Section A.1.\nLemma 3.2. The risk of the estimator m\u0303n(x) in (3.9) satisfies\nR(m\u0303n,m) (3.10)\n= 1\n4\n \u03c32K 1\nn\nn\u2211\nt=1\nh2t\n  2 \u222b ( m\u2032\u2032(x) + 2m\u2032(x) f \u2032(x)\nf(x)\n)2 dx\n+\u03c32 \u222b K2(x) dx \u222b dx\nf(x)   1 n2 n\u2211\nt=1\n1\nht\n \n+o   1 n n\u2211\nt=1\nh2t\n  2\n+ o   1 n2 n\u2211\nt=1\n1\nht\n  .\nThe analogue of Theorem 3.2 for density estimation can also be obtained in the regression setting. Instead of choosing a special kernel to cancel out the lower order terms in the Taylor\u2019s series, we leverage the minimax optimality of local polynomial regression as introduced in Section 2.\nTheorem 3.3. Suppose that the regression function m has d continuous derivatives. Let ht = c t \u22121/2d+1. Then at t = n, the order d \u2212 1 sequential local polynomial regression attains the optimal minimax risk R\u2217 = O(n\u22122d/2d+1).\nThe proof of Theorem 3.3 is given in Section A.3."}, {"heading": "4. Extensions", "text": "In this section we extend the above analysis in two ways. First, we show how the expert mixing framework can be used to adapt to the smoothness exponent. Second, we show how the procedure can be extended to additive models."}, {"heading": "4.1. Adapting to unknown smoothness", "text": "The theoretical performance of the sequential estimators presented above hinges on selecting the correct order d of the local polynomial, and in practice it depends on the constant c in the bandwidth as well. Traditional statistical model selection methods, e.g. AIC and cross validation, are not practical in an online scenario.\nIn order to maintain a reasonable computational cost, we combine estimators that use different parameters (order d and constant c) through an exponential weighting strategy. Leveraging our analysis in Section 3, it can be shown that the procedure adapts to the unknown smoothness at the optimal rate, while maintaining a linear computational cost.\nIn more detail, our mixing online regression estimates procedure forms an exponential weighting of a set of sequential local polynomial regression estimates with different orders d and bandwidth constants c.\nLet C = {ci}1\u2264i\u2264C , and D = {dj}1\u2264j\u2264D. Define M = CD to be the size of the family of sequential regression estimates M = {m\u0303(i,j),t} where 1 \u2264 i \u2264 C and 1 \u2264 j \u2264 D. At time t, the bandwidth of the regression m\u0303(i,j),t is given by h(i,j),t = ci \u00b7 t\u22121/2dj+1, and the corresponding estimator is computed as in equation (2.6); specifically,\nm\u0303(i,j),t(x0) = e T 1 (X T t W\u0303(i,j),tXt) \u22121XTt W\u0303(i,j),tyt (4.1)\nwhere\nW\u0303(i,j),t = diag{Kcis\u22121/2dj+1(Xs, x0)}1\u2264s\u2264t.\nThe double index (i, j) is used to illustrate the construction of the expert set; in the following, for simplicity, we use a single index k to index the M sequential estimators. Let \u2113k,t = \u2211t\ns=1(Ys \u2212 m\u0303k,s\u22121(Xs))2 be the cumulative loss of estimator k at time t. At each time s, the prediction of Ys is made with the estimator m\u0303k,s\u22121 constructed using the previous data {(X1, Y1), (X2, Y2), . . . , (Xs\u22121, Ys\u22121)}. Let wk,0 = M\u22121, and for t \u2265 1 let\nwk,t = exp(\u2212\u03b7\u2113k,t)\u2211M\nk\u2032=1 exp(\u2212\u03b7\u2113k\u2032,t) (4.2)\nwhere \u03b7 is a positive learning rate, to be chosen later. The combined estimator m\u0303\u2217 at time t is the convex combination given by\nm\u0303\u2217,t(x0) = M\u2211\nk=1\nwk,tm\u0303k,t(x0). (4.3)\nNote that the weight at time t can be updated in linear time O(M) using\nwk,t = wk,t\u22121 exp\n{ \u2212\u03b7(Yt \u2212 m\u0303k,t\u22121(Xt))2 } \u2211K\nk\u2032=1 wk\u2032,t\u22121 exp { \u2212\u03b7(Yt \u2212 m\u0303k\u2032,t\u22121(Xt))2 } .\n(4.4)\nA large literature is devoted to the study of regretbased bounds for online learning, which hold for any realization of data (Cesa-Bianchi & Lugosi, 2006). However, we are primarily interested in analyzing the statistical risk of our estimators. In particular, it is of interest to obtain an oracle inequality of the form\nE\u2016m\u0303\u2217,n \u2212m\u20162 \u2264 min k=1,...,M E\u2016m\u0303k,n \u2212m\u20162+\u03b4(n) (4.5)\nwhere \u03b4(n) is the additive penalty paid for adaptation. Having already established the minimax rate of the oracle m\u0303\u22c6,n = argmink=1,...,M E\u2016m\u0303k,n \u2212m\u20162, this will enable us to show that the weighted combination adapts to achieve a (near) minimax rate.\nTheorem 4.1. Let m \u2208 Cd\u22c6. Assume that\n1. m\u0303k,n \u2208 [\u2212A,A], for some constant A, for all k, n;\n2. mink Eexp |m\u0303k,n \u2212 Yn| \u2264 L, for some 0 < L < \u221e.\nSuppose that the optimal order d\u22c6 is contained in the set of candidate degrees D. Then, for sufficiently large n,\nE\u2016m\u0303\u2217,n \u2212m\u20162 \u2264 min k=1,...,K\nE\u2016m\u0303k,n \u2212m\u20162 + lnM\nn\u03b7 (4.6)\nwhere \u03b7 is chosen to be a small constant depending on A. In particular, we have that R(m, m\u0303\u2217,n) =\nO ( n\u22122d \u22c6/2d\u22c6+1 ) .\nThis result is proved by adapting analysis by Yang (2004) (Theorem 5, see also Catoni (2004))."}, {"heading": "4.2. Additive models", "text": "It is also possible to derive a sequential extension to the backfitting algorithm for additive models (Hastie & Tibshirani, 1990). Assuming now thatX is p dimensional, an additive regression model takes the form\nYi = m0+\np\u2211\nj=1\nmj(X j i )+\u01ebi, i = 1, . . . , n j = 1, . . . , p\nwhere m0 is an overall mean or intercept, the nonparametric regression functions mj are one-dimensional, and \u01ebi is mean zero noise. The backfitting algorithm is a type of coordinate descent or Gauss-Seidel procedure that iteratively computes the residuals for a variable j, and then smoothes those residuals to get a nonparametric estimate of the component function.\nIn our sequential setting, a complication comes from the fact that the residuals must be updated sequentially as well\u2014for computational efficiency we cannot afford to compute the residuals over all of the previous data. Since the residuals for all of the p variables depend on one another, we update them iteratively using our sequential regression estimator until convergence. We then cycle through the variables to sequentially update each component function in terms of the converged residuals.\nThis algorithm is made explicit below, where update(mj , X, resid, t) updates the jth model mj with the (X, resid) pair, using the appropriate bandwidth at time t. This can be viewed as an incremental version of the smoothing step in the classical backfitting algorithm and can be efficiently computed using (2.8) and (2.10) in the case of local-polynomial smoothing, using (2.3) in the case of kernel regression. We note that the numerical convergence of even the classical backfitting is difficult to analyze. Example simulations of our sequential backfitting algorithm are given in Section ??, where it compares favorably to the classical batch backfitting algorithm.\nInput: (X1, Y1), . . . , (Xn, Yn), . . . sequentially Output: m\u03021, \u00b7 \u00b7 \u00b7 , m\u0302n \u2208 Rp Initialize mean m0 = 0; Initialize mj = array(0, |Xj |), j = 1, \u00b7 \u00b7 \u00b7 , p; foreach timestep t do\nm0 = t\u22121 t m0 + Yt t ; Initialize resid = array(0, p); while resid not converged: do\nforeach j = 1, \u00b7 \u00b7 \u00b7 , p do foreach k 6= j do\nm\u2032k = update(mk, X k t , resid[k], t); m\u2032k = m \u2032 k \u2212mean(m\u2032k) ;\nend resid[j] = Yt \u2212m0 \u2212 \u2211 k 6=j m \u2032 k(X k t );\nend\nend for j in 1, \u00b7 \u00b7 \u00b7 , p do mj = update(mj , X j t , resid[j], t);\nmj = mj \u2212mean(mj) ; end\nOutput: m\u0302t = [m1 \u00b7 \u00b7 \u00b7mp]; end\nAlgorithm 1: Sequential Backfitting"}, {"heading": "5. Experiments", "text": "To illustrate the performance of a single online estimator (descirbed in \u00a72), we compare it with the batch algorithm using the same bandwidth h = cn\u22121/5. We consider the following 4 regression functions of different smoothness on [0, 1] used by Yang (2001): f1(x) = exp(\u2212x), f2(x) = 1+2x2+exp(\u22125(x\u2212 0.5)2), f3(x) = 1 + 2x2 + exp(\u2212200(x \u2212 0.5)), f4(x) = exp(\u2212200(x \u2212 0.2)2)/ \u221a 0.005\u03c0 + exp(\u2212200(x\u2212 0.8)2)/ \u221a 0.005\u03c0. The sample size is 150 and \u03c32 = 0.5.\nWe choose the bandwidth constant c from C = {0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 1, 1.5}. In addition, we add a comparison to a batch estimator that does leave one out cross validation at each time step to choose the bandwidth constant from C. The first 50 samples are used to initialize the batch estimators.\nFigure 1 (first row) shows the average loss as a function of the sample size for the best sequential estimator (with optimal constant), the best batch estimator, and the batch cross-validation estimator for the functions. In each case, the best sequential estimator has very similar loss to the best batch estimator, which supports our theory showing that the sequential estimator achieves the same statistical rate of convergence as the classical batch algorithm. Note that in each case the average loss converges to around the noise level \u03c32 = .5. The middle row compares the fits of the se-\nquential, batch, and batch with cross validation, and indicates that the fits are comparable, and very close to the true function. The third row shows the standard errors for the sequential estimators running the simulations 100 times for each function.\nHere we study the estimator\u2019s risk under unknown smoothness. We use the following parameterized set of functions: m\u03b1(x) = 2|0.5(x \u2212 0.5)|\u03b1, x \u2208 [0, 1] and let \u03b1 = {1, 1.5, 2, 2.5}. At the point x0 = 0.5, the smoothness of the function m\u03b1 is Ho\u0308lder \u03b1 continuous. We consider four \u201cexperts\u201d of online kernel estimators using the parameter set \u03b1 for their bandwidths: m\u0303\u03b1 takes the variable bandwidth ht = 0.4\u00d7 t\u22121/(2\u03b1+1). We use the Gaussian kernel Kh(u) = (2\u03c0) \u22121/2e\u2212u 2/(2h2) for the estimator, and set the noise to be \u03c32 = 0.01. Although we have not included the analysis due to lack of space, it can be shown that our sequential estimator with shrinking bandwidths c \u00b7 t\u22121/(2\u03b1+1) achieves\nthe minimax rate n\u22122/(2\u03b1+1) for this Ho\u0308lder family of regression functions. The task here is to adapt to the unknown exponent \u03b1.\nFigure 2 shows the risk at the critical point x0 = 0.5 of the four experts under the four functions, having different degrees of smoothness. We run simulations for each of the functions 1000 times, and show a plot of the average risk as a function of sample size. For each true function m\u03b1, the expert with the correct \u03b1 obtains the lowest risk for any sample size. This suppports the analysis in \u00a74, showing the best expert has the highest weight when the experts are mixed together. As a result, the mixing expert framework of online estimators adapts to the unknown smoothness of the true function. While the sequential algorithm makes a tradeoff between performance and computational efficiency, its performance is quite comparable to that of the optimal batch estimator."}, {"heading": "6. Summary and Conclusions", "text": "We proposed and analyzed an efficient sequential algorithm for local polynomial smoothing in nonparametric regression. The first contribution of this work is the online algorithm that shrinks the bandwidth for each new point that arrives. The second is the analysis showing that order d sequential local polynomial smoothing achieves the optimal minimax rate of convergence n\u22122d/2d+1. Finally, we show that exponential weight mixing of a family of such sequential estimators adapts to unknown smoothness at the optimal rate, and extended the algorithm to sequential backfitting for nonparametric additive models. Our experimental results confirm the theoretical analysis, and show that little loss in statistical efficiency is sacrificed by the computationally efficient online procedure. While we have shown adaptation to global smoothness, an interesting direction for future work is to consider sequential estimators that adapt to spatially inhomogeneous function classes. One promising direction is to adapt the variable bandwidth estimator of Lepski et al. (1997) to online regression for Besov spaces."}, {"heading": "Acknowledgements", "text": "Research supported in part by NSF grant IIS-1116730 and AFOSR contract FA9550-09-1-0373."}, {"heading": "Fan, Jianqing and Gijbels, Irene. Data-driven bandwidth", "text": "selection in local polynomial fitting: variable bandwidth and spatial adaptation. Journal of the Royal Statistical Society. Series B(Methodological), 57(2):371\u2013394, 1995.\nFan, Jianqing and Gijbels, Irene. Local Polynomial Modelling and Its Applications, pp. 57\u2013105. Chapman and Hall/CRC, 1996."}, {"heading": "Fan, Jianqing, Gijbels, Irene, Gasser, Theo, Brockmann,", "text": "Michael, and Engel, Joachim. Local polynomial fitting: A standard for nonparametric regression, 1993."}, {"heading": "Gyo\u0308rfi, La\u0301slo\u0301, Kohler, Michael, Krzyz\u0307ak, Adam, and Walk,", "text": "Harro. A Distribution-Free Theory of Nonparametric Regression. Springer-Verlag, 2002.\nHastie, T. J. and Tibshirani, R. J. Generalized additive models. London: Chapman & Hall, 1990. ISBN 0412343908."}, {"heading": "Kivinen, J., Smola, A. J., and Williamson, R. C. Online", "text": "learning with kernels. Signal Processing, IEEE Transactions on, 52:2165\u20132176, 2004."}, {"heading": "Kristan, M., Skocaj, D., and Leonardis, A. Online kernel", "text": "density estimation for interactive learning. Image and Vision Computing, 28:1106\u20131116, 2010."}, {"heading": "Lepski, O.V., Mammen, E., and Spokoiny, V.G. Optimal", "text": "spatial adaptation to inhomogeneous smoothness: An approach based on kernel estimates with variable bandwidth selectors. The Annals of Statistics, 25(3):929\u2013947, 1997.\nRuppert, D., Sheather, S. J, and Wand, M. P. An effective bandwidth selector for local least squares regression. Journal of the American Statistical Association, 90(432), 1995.\nSteland, Ansgar. Sequential data-adaptive bandwidth selection by cross validation for nonparametric prediction, 2010.\nYang, Yuhong. Adaptive regression by mixing. Journal of the American Statistical Association, 96(454):574\u2013588, 2001.\nYang, Yuhong. Combining forecasting procedures: Some theoretical results. Econometric Theory, 20:176\u2013222, 2004."}], "references": [{"title": "Sequential procedures for aggregating arbitrary estimators of a conditional mean", "author": ["Bunea", "Florentina", "Nobel", "Andrew"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Bunea et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bunea et al\\.", "year": 2008}, {"title": "Statistical learning theory and stochastic optimization", "author": ["Catoni", "Olivier"], "venue": "Ecole d\u2019Ete\u0301 des Probabilite\u0301s de Saint Flour,", "citeRegEx": "Catoni and Olivier.,? \\Q2001\\E", "shortCiteRegEx": "Catoni and Olivier.", "year": 2001}, {"title": "Prediction, Learning and Games", "author": ["Cesa-Bianchi", "Nicolo", "Lugosi", "Gabor"], "venue": null, "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "Data-driven bandwidth selection in local polynomial fitting: variable bandwidth and spatial adaptation", "author": ["Fan", "Jianqing", "Gijbels", "Irene"], "venue": "Journal of the Royal Statistical Society. Series B(Methodological),", "citeRegEx": "Fan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Fan et al\\.", "year": 1995}, {"title": "Local Polynomial Modelling and Its Applications, pp. 57\u2013105", "author": ["Fan", "Jianqing", "Gijbels", "Irene"], "venue": "Chapman and Hall/CRC,", "citeRegEx": "Fan et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Fan et al\\.", "year": 1996}, {"title": "Local polynomial fitting: A standard for nonparametric regression", "author": ["Fan", "Jianqing", "Gijbels", "Irene", "Gasser", "Theo", "Brockmann", "Michael", "Engel", "Joachim"], "venue": null, "citeRegEx": "Fan et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Fan et al\\.", "year": 1993}, {"title": "A Distribution-Free Theory of Nonparametric Regression", "author": ["Gy\u00f6rfi", "L\u00e1sl\u00f3", "Kohler", "Michael", "Krzy\u017cak", "Adam", "Walk", "Harro"], "venue": null, "citeRegEx": "Gy\u00f6rfi et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gy\u00f6rfi et al\\.", "year": 2002}, {"title": "Generalized additive models", "author": ["T.J. Hastie", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Hastie and Tibshirani,? \\Q1990\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1990}, {"title": "Online learning with kernels", "author": ["J. Kivinen", "A.J. Smola", "R.C. Williamson"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Kivinen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 2004}, {"title": "Online kernel density estimation for interactive learning", "author": ["M. Kristan", "D. Skocaj", "A. Leonardis"], "venue": "Image and Vision Computing,", "citeRegEx": "Kristan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kristan et al\\.", "year": 2010}, {"title": "Optimal spatial adaptation to inhomogeneous smoothness: An approach based on kernel estimates with variable bandwidth selectors", "author": ["O.V. Lepski", "E. Mammen", "V.G. Spokoiny"], "venue": "The Annals of Statistics,", "citeRegEx": "Lepski et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Lepski et al\\.", "year": 1997}, {"title": "An effective bandwidth selector for local least squares regression", "author": ["D. Ruppert", "Sheather", "S. J", "M.P. Wand"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ruppert et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ruppert et al\\.", "year": 1995}, {"title": "Sequential data-adaptive bandwidth selection by cross validation for nonparametric prediction", "author": ["Steland", "Ansgar"], "venue": null, "citeRegEx": "Steland and Ansgar.,? \\Q2010\\E", "shortCiteRegEx": "Steland and Ansgar.", "year": 2010}, {"title": "Adaptive regression by mixing", "author": ["Yang", "Yuhong"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Yang and Yuhong.,? \\Q2001\\E", "shortCiteRegEx": "Yang and Yuhong.", "year": 2001}, {"title": "Combining forecasting procedures: Some theoretical results", "author": ["Yang", "Yuhong"], "venue": "Econometric Theory,", "citeRegEx": "Yang and Yuhong.,? \\Q2004\\E", "shortCiteRegEx": "Yang and Yuhong.", "year": 2004}], "referenceMentions": [{"referenceID": 6, "context": "2) thus has order h = O(n), which leads to the optimal minimax rate of convergence O(n); see Gy\u00f6rfi et al. (2002). More generally, if we assume further smoothness of m so that the d-th derivative of m exists and is bounded, then the minimax rate n is achieved using local polynomial regression of order d\u22121 with a bandwidth of order h = O(n) (Fan & Gijbels, 1996).", "startOffset": 93, "endOffset": 114}, {"referenceID": 8, "context": "Kivinen et al. (2004) develop variants of stochastic gradient descent for online learning in a reproducing kernel Hilbert space.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "Kivinen et al. (2004) develop variants of stochastic gradient descent for online learning in a reproducing kernel Hilbert space. The algorithms require linear computation cost at each step, and their RKHS analysis does not consider selection of tuning parameters for the kernel, or adaptation to the unknown smoothness of the regression function. A great deal of work has been carried out for the problem of adaptation in the classical batch setting. Fan & Gijbels (1995) consider using Residual Squares Criteria (RCS) for performing data-driven bandwidth selection; Ruppert et al.", "startOffset": 0, "endOffset": 472}, {"referenceID": 8, "context": "Kivinen et al. (2004) develop variants of stochastic gradient descent for online learning in a reproducing kernel Hilbert space. The algorithms require linear computation cost at each step, and their RKHS analysis does not consider selection of tuning parameters for the kernel, or adaptation to the unknown smoothness of the regression function. A great deal of work has been carried out for the problem of adaptation in the classical batch setting. Fan & Gijbels (1995) consider using Residual Squares Criteria (RCS) for performing data-driven bandwidth selection; Ruppert et al. (1995) propose a plug-in bandwidth selection scheme for local linear kernel estimators.", "startOffset": 0, "endOffset": 589}, {"referenceID": 8, "context": "Kivinen et al. (2004) develop variants of stochastic gradient descent for online learning in a reproducing kernel Hilbert space. The algorithms require linear computation cost at each step, and their RKHS analysis does not consider selection of tuning parameters for the kernel, or adaptation to the unknown smoothness of the regression function. A great deal of work has been carried out for the problem of adaptation in the classical batch setting. Fan & Gijbels (1995) consider using Residual Squares Criteria (RCS) for performing data-driven bandwidth selection; Ruppert et al. (1995) propose a plug-in bandwidth selection scheme for local linear kernel estimators. These are effective methods for adaptive estimation; however, they do not take into account the computational cost for online updating. The mixing expert framework has been a popular strategy for online prediction, and there is a rich literature on this topic (Cesa-Bianchi & Lugosi, 2006). Results by Bunea & Nobel (2008) give oracle inequalities for regression in terms of generalized simplex combinations of a set of fixed estimators in the online setting.", "startOffset": 0, "endOffset": 993}, {"referenceID": 5, "context": "Among the various nonparametric regression methods, local polynomial regression enjoys strong minimax properties, and gracefully deals with the problem of boundary bias (Fan et al., 1993).", "startOffset": 169, "endOffset": 187}, {"referenceID": 5, "context": "5) and m\u0302(x0) = \u03b2\u03020(x0) is an estimate of the regression function at x0 (Fan et al., 1993).", "startOffset": 72, "endOffset": 90}, {"referenceID": 9, "context": "We note in passing that a similar algorithm for sequential density estimation appears, without theoretical analysis, in Kristan et al. (2010). Theorem 3.", "startOffset": 120, "endOffset": 142}, {"referenceID": 10, "context": "One promising direction is to adapt the variable bandwidth estimator of Lepski et al. (1997) to online regression for Besov spaces.", "startOffset": 72, "endOffset": 93}], "year": 2012, "abstractText": "We present algorithms for nonparametric regression in settings where the data are obtained sequentially. While traditional estimators select bandwidths that depend upon the sample size, for sequential data the effective sample size is dynamically changing. We propose a linear time algorithm that adjusts the bandwidth for each new data point, and show that the estimator achieves the optimal minimax rate of convergence. We also propose the use of online expert mixing algorithms to adapt to unknown smoothness of the regression function. We provide simulations that confirm the theoretical results, and demonstrate the effectiveness of the methods.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}