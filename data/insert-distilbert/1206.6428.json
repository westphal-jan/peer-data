{"id": "1206.6428", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Binary Classification Framework for Two-Stage Multiple Kernel Learning", "abstract": "with the advent of kernel methods, automating the task of specifying a suitable kernel has become increasingly important. in this context, the multiple kernel learning ( ( mkl ) problem of continuously finding a hypothetical combination of pre - formulated specified base kernels that is suitable also for the formal task at which hand has received significant attention from researchers. in this paper we show that multiple kernel learning can be uniquely framed as a standard binary classification problem with additional constraints that ensure the positive definiteness of adapting the learned kernel. framing mkl in this way has the distinct advantage that it makes it easy to leverage the extensive rigorous research in binary classification to constantly develop a better performing and more scalable mkl algorithms that are essentially conceptually called simpler, and, arguably, more accessible to practitioners. experiments on nine data sets from different domains each show that, despite largely its simplicity, the proposed technique compares favorably with current leading core mkl approaches.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (392kb)", "http://arxiv.org/abs/1206.6428v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["abhishek kumar 0001", "alexandru niculescu-mizil", "koray kavukcuoglu", "hal daum\u00e9 iii"], "accepted": true, "id": "1206.6428"}, "pdf": {"name": "1206.6428.pdf", "metadata": {"source": "META", "title": "A Binary Classification Framework for Two-Stage Multiple Kernel Learning", "authors": ["Abhishek Kumar", "Koray Kavukcoglu"], "emails": ["abhishek@cs.umd.edu", "alex@nec-labs.com", "koray@nec-labs.com", "hal@umiacs.umd.edu"], "sections": [{"heading": "1. Introduction", "text": "Kernel methods such as support vector machines (SVM) (Cortes & Vapnik, 1995), kernel ridge regression, or kernel PCA (Smola & Muller, 1999), use a positive semi-definite (PSD) kernel to implicitly map the\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ninstances from the original instance space to a feature space where the standard linear algorithm is applied. The main drawback of kernel methods is that they require the user to specify a single suitable kernel in the first place, which is often critical to the method\u2019s success, but is usually a hard task even when the user has a good familiarity with the problem domain. To ease this burden, significant attention has been given the problem of automatically learning the kernel. The majority of the previous work in this area has focused on the Multiple Kernel Learning (MKL) setting, where the user is only tasked with specifying a set of base kernels, and the learning algorithm is in charge of finding a combination of these base kernels that is appropriate for the problem at hand.\nThere have been two main lines of work in this direction. The first one learns both the the weights of the kernel combination and the parameters of the classifier by solving a single joint optimization problem. This one-stage approach was first proposed by (Lanckriet et al., 2004) and has since received significant attention (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien & Ong, 2007; Cortes et al., 2009; Sindhwani & Lozano, 2011).\nThe second line of work in kernel learning follows a two-stage approach: first learn a \u201cgood\u201d combination of base kernels using the training data, then use the learned kernel with a standard kernel method such as SVM or kernel ridge regression to obtain a classifier/regressor. This approach has been initially proposed in (Cristianini et al., 2001) and (Kandola et al., 2002), and recently revisited by (Cortes et al., 2010b). The two-stage leaning approaches so far have been\nbased on the notion of target alignment. Intuitively, target alignment, is a measure of similarity (agreement) between a kernel and the target kernel, which is derived from the training labels, and represents the optimal kernel for the training sample.\nIn this paper we introduce TS-MKL, a general approach to Two-Stage Multiple Kernel Learning that encompasses the previous work based on target alignment as special cases. We formulate the kernel learning problem as a standard linear classification problem in a new instance space. In this space, any linear classifier with weights \u00b5 directly corresponds to a linear combination of base kernels with weights \u00b5. To avoid confusions, we will denote this new instance space as the K-space, and a classifier in the K-space as a Kclassifier throughout the paper. Thus the problem of finding a \u201cgood\u201d kernel combination reduces to finding a \u201cgood\u201d linear classifier in the K-space, a very familiar problem. One big advantage of this approach is that one can easily adapt techniques from binary classification to solve the MKL problem. For instance, one can use familiar and well understood max-margin methods to obtain better performing MKL algorithms, or take advantage of the recent advances in large scale learning to scale up and/or parallelize the MKL implementations. For the results presented in this paper we learn K-classifiers (and hence kernels) by training L2 regularized linear SVMs with positive weights using the stochastic projected sub-gradient descent method from Pegasos (Shalev-Shwartz et al., 2007).\nOn the theoretical side, we prove a finite sample generalization bound for the original classification task in terms of the expected hinge loss and the margin of a K-classifier in the K-space. This justifies our approach of training a K-classifier that has low hinge loss and high margin in the K-space in order to learn a good kernel for the original classification problem. To the best of our knowledge, this result represent the first finite sample bound for two-stage kernel learning, improving on previous bounds that were only asymptotic. We also give a concentration bound for the expected hinge loss of a K-classifier.\nOn the empirical side, we run a comprehensive evaluation on two object recognition datasets (Caltech 101 and 256), three bioinformatics datasets (Psort+, Psort-, Plant) and four UCI datasets. On all these datasets our method performs better than, or the same as target alignment, showing that choosing a better K-classifier is beneficial. Our method also fares well against one-stage multiple kernel learning approaches significantly outperforming them on Caltech-256 and being essentially tied on the others."}, {"heading": "2. Method", "text": "We consider a classification problem where instances (x, y) are drawn from a distribution P over X \u00d7 Y, with Y a finite discrete set of labels. We assume that we have access to p positive semi-definite (PSD) base kernel functions K1, \u00b7 \u00b7 \u00b7 ,Kp with Ki : X \u00d7 X \u2192 R. Our goal is to learn a combination of these kernel functions that is itself a positive semi-definite function and is a \u201cgood\u201d kernel for the classification task at hand. To achieve this, we define a new binary classification problem over a new instance space {(zxx\u2032 , tyy\u2032)|((x, y), (x\u2032, y\u2032)) \u223c P \u00d7 P} \u2282 Rp \u00d7 {\u00b11} where\nzxx\u2032 = (K1(x, x \u2032), \u00b7 \u00b7 \u00b7 ,Kp(x, x\u2032)) tyy\u2032 = 2 \u00b7 1{y = y\u2032} \u2212 1 (1)\nWe will call this space the K-space, and call zxx\u2032 a Kexample or K-instance and tyy\u2032 a K-label. Any function h : Rp \u2192 R in this space induces a similarity function K\u0303h between instances in the original space:\nK\u0303h(x, x \u2032) = h(zxx\u2032) = h(K1(x, x \u2032), \u00b7 \u00b7 \u00b7 ,Kp(x, x\u2032))\nIf K\u0303h is also positive semi-definite, hence a valid kernel, we say that h is a K-classifier. For example, all linear functions with positive coefficients (i.e. h\u00b5(zxx\u2032) = \u00b5 \u00b7 zxx\u2032 with \u00b5 \u2265 0) are K-classifiers with the induced kernels K\u0303\u00b5 being linear combinations of the p base kernels. Figure 1 shows a toy example for the case of two base kernels. Each point in the figure is a labeled K-example (zxx\u2032 , tyy\u2032) corresponding to a pair (x, y), (x\u2032, y\u2032) of original instances. Note that the figure is drawn in K-space, not in input space. For a linear K-classifier h\u00b5, the value of its induced kernel for a parir of original instances, K\u0303\u00b5(x, x \u2032), is the projection of the corresponding K-example zxx\u2032 on the vector \u00b5 (represented by the green line). The left and center sub-figures show the cases where \u00b5 is (0, 1) and (1, 0) respecively. In both cases the induced kernel combination is suboptimal. The linear combination in the right sub-figure corresponds to \u00b5 = (1, 1) and is a good combination because the kernel values of pairs of instances in the same class are separated from the kernel values of pairs of instances in different classes.\nThe key insight behind our method is that, if a Kclassifier h is a good classifier in the K-space, then\nthe induced kernel K\u0303h(x, x \u2032) = h(zxx\u2032) will likely be positive when x and x\u2032 belong to the same class and negative otherwise. This makes K\u0303h a good kernel for the original classification task. This intuition is made more precise in Section 3 where we provide a generalization bound that shows that a K-classifier that separates the positive and negative K-examples with high margin will indeed induce a kernel that allows learning a good classifier for the original task. Note that having a good K-classifier is a sufficient condition, not a necessary one. There can very well exist combinations of base kernels that do not correspond to a good K-classifier, but are good kernels nevertheless. Unlike one-stage kernel learning approaches, our method will not be able to find such combinations and it might miss on some good kernels. The results in Section 4, however, show that this does not seem to be the case in practice, as we consistently matched or exceeded the performance of one-stage MKL.\nThus the problem of learning a good kernel can be reduced to the problem of learning a good K-classifier in the newly defined K-space: given a training sample (xi, yi) n i=1 for the original classification task, construct a K-training set (zij , tij)1\u2264i\u2264j\u2264n and learn a Kclassifier h from this sample. Any learning algorithm can be used for learning h provided that the induced kernel can be guaranteed to be a valid PSD kernel1.\nIn line with the majority of the MKL work, in this paper we focus on learning linear K-classifiers, and hence linear combinations of base kernels. The results in Section 3 suggest that it is desirable to have a maximum margin K-classifier, thus we use L2 regularized linear SVM to learn the K-classifier, and ensure that the induced kernel is PSD by constraining the weights to be positive. One could, however, use a sparsity promoting regularizer (e.g., L1 penalty) if a sparse combination of kernels is desired.\nThe optimization problem for learning the kernel weights \u00b5 is thus given by\nmin \u00b5\u22650\n\u03bb 2 ||\u00b5||2 + 1(n\n2\n) + n \u2211 1\u2264i\u2264j\u2264n [1\u2212 tij\u00b5 \u00b7 zij ]+ (2)\nwhere [1\u2212 s]+ = max{0, 1\u2212 s} is the hinge loss. To optimize this objective we use the stochastic projected sub-gradient descent implemented in Pega-\n1One could drop the PSD requirement and use any classifier, even a non-linear one, to obtain a similarity function rather than a proper kernel. The theory of learning with similarity functions (Balcan & Blum, 2006) can be then applied to learn a classifier for the original task. Generalization bounds similar to the ones in Section 3 would also hold for this case.\nsos (Shalev-Shwartz et al., 2007), with an additional projection to the non-negative constraint set after every gradient step. Using a stochastic optimization method allows us to scale very well despite the quadratic number of K-examples: computation time is not directly dependent on the number of instances, linear in the number of base kernels, and independent of the number of classes. If needed, memory usage can be reduced through streaming techniques or on the fly construction of the K-examples."}, {"heading": "2.1. Connection to Target Alignment", "text": "Previous two-stage kernel learning approaches (Cristianini et al., 2001; Cortes et al., 2010b) learn a non-negative linear combination of base kernels that maximizes the alignment with the target kernel K(t)(xi,xj) = yiyj on the training set. This is achieved by solving the optimization problem\nmax \u00b5\u22650\n\u3008 \u2211p l=1 \u00b5lKl,K\n(t)\u3009 || \u2211p l=1 \u00b5lKl||F , s.t. ||\u00b5||2 = 1, (3)\nwhere A is the Gram matrix of kernel A on the training set, \u3008A,B\u3009 = tr(ABT ) and ||A||2F = tr(AAT ).\nThe above optimization problem can be re-written in our terminology of K-examples as follows:\nmax \u00b5\u22650\n\u00b5T (\u2211\ntij=1 zij \u2212 \u2211 tij=\u22121 zij ) \u221a \u00b5T (\u2211\n\u2200i,j zijz T ij\n) \u00b5\n, s.t. ||\u00b5||2 = 1\nWhen the base kernels are centered, as proposed in (Cortes et al., 2010b), the denominator represents the overall standard deviation of the projections of the K-examples on the vector \u00b5. Hence target alignment attempts to find a projection direction \u00b5 that maximize the difference between the sums of the projections of the positive and negative K-examples, while minimizing the overall variance of the projected Kexamples. This is very similar to using Fisher-LDA in the K-space, with non-negativity constraints on \u00b5. In fact, viewing target alignment from this perspective, makes it clear that it implicitly makes the assumption that the data is homoscedastic (the positive and negative K-examples have the same covariance), which might not be appropriate in real applications."}, {"heading": "2.2. Connection to Learning with Hyperkernels", "text": "The approach proposed in this paper can also be cast in the framework of learning with hyperkernels (Ong et al., 2005) which provides a general recipe for kernel learning and includes Multiple Kernel Learning as a\nspecial case. It introduces the notions of kernel quality functional, a measure of \u201cgoodness\u201d of a kernel that depends on the training data, and Hyper Reproducing Kernel Hilbert Space, an RKHS over kernel functions that defines the class of kernels that can be learned. Once the desired Hyper-RKHS and quality functional are specified, one has to solve a semi-definite program (SDP) to optimize the quality functional regularized by the norm induced by the Hyper-RKHS.\nWhen using an SVM as the K-classifier, TS-MKL can be put in the learning with hyperkernels framework by defining the Hyper-RKHS to be the set of non-negative linear combinations of base kernels, and the quality functional to be the hinge loss in K-space. Considering this specific setting has significant advantages: it enables the use of simple and well understood binary classification techniques to learn the kernel, it enables a theoretical analysis, and it allows a significantly more scalable implementation. Equally important, all these advantages do not seem to come at the cost of reduced performance, as we are still performing on par with or better than competing MKL techniques.\n3. Theoretical Results2\nIn this section we make the connection between the performance a K-classifier in the K-space and the performance on the original problem precise. This justifies the approach taken in this paper not only intuitively, but also from a theoretical standpoint. Specifically, we bound the generalization error of an SVM that uses the kernel induced by a K-classifier in terms of the expected hinge loss and the margin of the Kclassifier in the K-space:\nTheorem 3.1 Let P be a distribution on X \u00d7 {\u00b11}, zxx\u2032 and tyy\u2032 be as in Equation 1, h be a K-classifier, and R be a constant s.t. h(zxx) \u2264 R2 \u2200x \u2208 X . Let\nHLh,\u03b3 = E((x,y),(x\u2032,y\u2032))\u2208P\u00d7P\nt[ 1\u2212 tyy \u2032h(zxx\u2032)\n\u03b3\n] + |\nbe the expected K-space hinge loss relative to margin \u03b3 of the K-classifier h. Then, with probability 1 \u2212 \u03b4, a classifier f\u0302 with generalization error\nP(x,y)\nr yf\u0302(x) \u2264 0 z \u2264 HLh,\u03b3 +O\n(\u221a R4 ln(1/\u03b4)\n\u03b32n\n)\ncan be learned efficiently from a training sample of n instances drawn IID from P .\n2Due to lack of space, all proofs are included in the supplementary material.\nThe theorem follows from the two lemmas stated below. The first lemma shows that a K-classifier that has low expected hinge loss in the K-space will induce a \u201cgood\u201d kernel. The second lemma shows that a good kernel allows for a classifier with low generalization error to be efficiently learned from a finite training sample. The following definition states formally what we mean by a good kernel (Srebro, 2007).3\nDefinition A kernel K is an ( , \u03b3) good kernel in hinge loss with respect to a distribution P on X\u00d7{\u00b11} if there exist a classifier w \u2208 HK with \u2016w\u2016HK = 1 s.t.\nE(x,y)\nt[ 1\u2212 y\u3008w, \u03c6(x)\u3009\n\u03b3\n] + | \u2264\nwhere HK is the Hilbert space and \u03c6(\u00b7) is the feature mapping corresponding to K.\nLemma 3.2 Let P , h, HLh,\u03b3 , R be as in Theorem 3.1. Then the K\u0303h is a (HLh,\u03b3 , \u03b3 R ) good kernel in hinge loss with respect to P .\nLemma 3.3 Let K be an ( , \u03b3) good kernel in hinge loss, with K(x, x) \u2264 R2 \u2200x \u2208 X . Let (xi, yi)ni=1 be an IID training sample, and f\u0302(x) = w\u0302 \u00b7 \u03c6(x) with\nw\u0302 = arg min ||w||HK\u22641\n1\nn n\u2211 i=1 [ 1\u2212 yiw \u00b7 \u03c6(xi) \u03b3 ] +\nbe a kernel classifier that minimizes the average hinge loss relative to \u03b3 on the training sample. Then, with probability at least 1\u2212 \u03b4, we have:\nP(x,y)\nr yf\u0302(x) \u2264 0 z \u2264 +O\n(\u221a R2 ln(1/\u03b4)\n\u03b32n\n)\nLemma 3.3 follows directly from Theorem 21 in (Bartlett & Mendelson, 2002).\nNote that, unlike in the one-stage kernel learning case, the generalization bound in Theorem 3.1 is in terms of the expected hinge loss of the K-classifier not the training hinge loss. While we are hopeful a generalization bound for the classification problem in the K-space can be obtained, as of now it remains an open problem.\nWe can, however, prove a concentration bound for the expected hinge loss of a K-classifier. This is the analog of the concentration bounds for target alignment in (Cortes et al., 2010b; Cristianini et al., 2001).4\n3A kernel that does not satisfy this definition is not necessarily a \u201cbad\u201d kernel. We just can not make any formal statements with respect to its performance.\n4This is not a regular generalization bound as the Kclassifier is not allowed to depend on the IID sample.\nTheorem 3.4 Let P , h, HLh,\u03b3 , R be as in Theorem 3.1. Let (xi, yi) n i=1 be an IID sample distributed according to P . Then the following inequality holds with probability at least 1\u2212 \u03b4\nHLh,\u03b3 \u2264 2 n(n\u2212 1) \u2211\n1\u2264i<j\u2264n\n[ 1\u2212 tijh(zij)\n\u03b3\n] +\n+ \u221a\u221a\u221a\u221a2(1 + R2\u03b3 )2 ln 1/\u03b4 n"}, {"heading": "4. Empirical Evaluation", "text": "We evaluate the proposed method on two object recognition datasets (Caltech-101 and Caltech-256), three bioinformatics datasets (Psort+, Psort- and Plant), and four UCI datasets (Sonar, Pima, Vertebral and Ionosphere). We compare our method with several baselines: best kernel, uniform combination of base kernels (Average), target alignment, and the onestage MKL algorithms SILP (Sonnenburg et al., 2006), SimpleMKL (Rakotomamonjy et al., 2007), L2-Norm MLK (Kloft et al., 2011), and UFO-MKL (Orabona & Jie, 2011). For two-stage methods we use LIBSVM (Chang & Lin, 2011) to train the data classifier and select the regularization parameter C via 4-fold cross-validation for all datasets except Caltech where it is fixed at 1000. On multi-class problems, we use a one-vs-rest SVM. For one-stage approaches other than UFO-MKL, we selected C as above and use a one-vsrest scheme for multi-class problems. For UFO-MKL we use the joint multi-class formulation and search over \u03b1 and C using a bi-dimensional grid. Following (Orabona & Jie, 2011), we run the optimization for 20 epochs on UCI datasets, 30 epochs on Caltech-101 and 100 epochs on Caltech-256. All kernels used in the experiments are centered and standardized to have zero mean and unit variance in feature space."}, {"heading": "4.1. Methodology for TS-MKL", "text": "To learn kernel combination weights \u00b5 with TSMKL we optimize the objective in Eq. 2 using Pegasos (Shalev-Shwartz et al., 2007) with an additional projection to the non-negative constraint set after each sub-gradient step. We use a batch size of 100 for each sub-gradient computation and run 103 sub-gradient steps for UCI datasets and 105 for all others. Figure 2, plots the test data accuracy versus the number of gradient iterations on Caltech-101, showing that after 105 iterations the change in accuracy is minimal. For the bigger Caltech-256 there is also essentially no change after 105 iterations. We use subsampling to balance the positive and negative K-examples.\nTo select the parameter \u03bb, we use a single 80%-20% random split of the Pegasos training set and search for the \u03bb with the lowest validation hinge loss5. The search grid for \u03bb is taken to be in the range of 100 to 10\u22128 dividing in each step by 4. A big advantage of this selection scheme for \u03bb is that it is completely independent from the data classifier that will ultimately use the learned kernel. This keeps the setup simple and avoids intricate multi-level multi-dimensional validation schemes across the parameters of the data classifier and the K-classifier. Fig. 2, shows the hinge-loss in K-space, the accuracy of the K-classifier, and the accuracy of the data classifier that uses the learned kernel, as a function of \u03bb. The plot shows a clear correlation between hinge loss in K-space and data accuracy with the learned kernel. The data accuracy increases when the hinge loss in K-space decreases and vice versa. This experiment provides further emprical evidence for our theoretical results that show that a good K-classifier (having low hinge loss in K-space) corresponds to a good learned kernel.\nAfter \u03bb is selected, Pegasos is retrained on the full training set of K-examples. The obtained weight vector \u00b5 is then used to linearly combine the base kernels, and the SVM data classifier is trained using this learned kernel with C selected as described above."}, {"heading": "4.2. Caltech-101 and Caltech-256", "text": "Both these datasets contain pictures of objects and the task is to recognize the object category. Caltech101 has 102 classes and Caltech-256 has 256 classes. Caltech-101 is perceived as an easier dataset than Caltech-256 in which images are not left-right aligned and there are more categories. We follow the experimental setup used in (Gehler & Nowozin, 2009) and use the same 39 base kernels and train test splits.\nWe report results using all 102 classes for Caltech-\n5Since the K-examples are dependent, the training and validation set will not be fully independent. Nevertheless, this does not seem to negatively affect the performance.\n101 averaged over five splits. For Caltech-256, the results are for 256 classes (excluding the clutter category), for a single split. The performance measure used is mean prediction rate per class. The number of training images per class is varied in the range 5, 10, 15, 20, 25, 30 for Caltech-101, and in the range 5, 10, 15, 20, 25, 30, 40, 50 for Caltech-256. The number of test images used is up to 50 images per class for Caltech-101 and 25 images per class for Caltech-256. The regularization parameter for the data SVM, C, is fixed to 1000 for all methods6.\nThe results for Caltech-101 and Caltech-256 are shown in Fig. 3.7 On Caltech-101 our approach yields a mean accuracy of 0.512, 0.630, 0.691, 0.725, 0.752, 0.772 for 5, 10, 15, 20, 25, 30 samples per class respectively. Comparing to UFO-MKL, our performance is higher for 5 samples per class, and very similar for all other sample sizes. One-stage MKL methods using the onevs-all multi-class scheme perform significantly worse and do not even outperform the average kernel until the training set has 25 samples per class. This is probably because data is too scarce to allow learning a separate kernel for each class. Target alignment performs a little better than the average kernel, but is still significantly worse than TS-MKL. We also show the performance of LP-\u03b2 (Gehler & Nowozin, 2009), which, to the best of our knowledge, is the state of the art method on this data set8. The performance of TS-MKL and UFO-MKL is almost on par with LP-\u03b2, especially for larger sample sizes. While LP-\u03b2 is similar in spirit to multiple-kernel learning, it is not a true kernel learning algorithm as it does not produce a ker-\n6C = 1000 is the best setting for the one-stage MKL algorithms (Gehler & Nowozin, 2009)\n7We take the results for LP-\u03b2 and MKL from (Gehler & Nowozin, 2009).\n8LP-\u03b2 achieves state of the art performance when using additional kernels (http://www.vision.ee.ethz.ch/ ~pgehler/projects/iccv09). We could not obtain all 48 kernels, so we only report results with only 39 kernels for all methods\nnel, but rather learns an ensemble of SVM classifiers, each of which is trained on an individual kernel.\nOn Caltech-256 dataset, our approach performs better than all competing kernel learning baselines. We achieve 0.245, 0.320, 0.370, 0.426, 0.448, 0.475, 0.494 mean accuracy for 5, 10, 15, 20, 25, 30, 40, 50 training samples per class. This performance is significantly higher than the best results reported in the literature for 5, 10, and 15 training samples, after which we again perform on par with LP-\u03b2. On this dataset, UFO-MKL performance9 is similar to that of the average kernel, while the rest of the one-stage MKL techniques perform worse. Exact target alignment is worst among all other approaches, however approximate target alignment is able to at least match the performance of the average kernel."}, {"heading": "4.3. Bioinformatics datasets", "text": "We evaluate our method on a problem relevant to cell-biology predicting: the subcellular localization of proteins, which is crucial in making inference about protein function and protein interactions. We follow the experimental setup of (Zien & Ong, 2007) and use the same 69 kernels. The kernels used are: 2 kernels on phylogenetic trees, 3 kernels from BLAST E-values and 64 sequence motif kernels.\nWe experiment with three datasets. The first two datasets are for the problem of bacterial protein locations (Gardy et al., 2004). The Psort+ dataset has 541 data points with 4 classes and Psort\u2013 dataset has 1444 data points with 5 classes. We report average F1 score over all classes over 10 random splits for both these datasets as done in (Zien & Ong, 2007). The third dataset used is the original plant dataset of TargetP (Emanuelsson et al., 2000), and has 940 examples with 4 classes. We use the performance measure of Matthew\u2019s Correlation Coefficient (MCC) following the evaluation in (Zien & Ong, 2007). Again, average MCC score over all 4 classes is reported.\nThe results are shown in Table 1. The papers that have used the Psort datasets in the past (Gardy et al., 2004; Zien & Ong, 2007), reported results after filtering out the most unsure predictions in the test set. For Psort+ and Psort-, about 15% and 13.3% of the test examples were filtered out respectively and the performance is reported only for the remaining predictions. We follow the same procedure to be able to compare with these methods. We also report performance for full test set. On these datasets, all the kernel learning methods have\n9The UFO-MKL performance at 40 and 50 samples is missing because the code we are using runs out of memory.\nsimilar performance, and are better than the best kernel and average kernel baselines. Multi-class multiple kernel learning (MC-MKL) of (Zien & Ong, 2007) is also close to our method and other baselines."}, {"heading": "4.4. UCI datasets", "text": "We use four UCI datasets: Sonar, Ionosphere, Pima and Vertebral (the three class version). For each of these datasets, we perform two types of MKL experiments. In first setting, we construct a total of 13 kernels on the full feature vectors: 9 Gaussian kernels (e\u2212\u03b3||xi\u2212xj || 2\n) with \u03b3 = {2\u221210, 2\u22129, . . . , 2\u22122}, 3 polynomial kernels of degree 2,3 and 4, and a linear kernel. In the second setting, we augment these 13 kernels with another set of Gaussian, polynomial and linear kernels constructed on individual features of the data. The range of parameter \u03b3 for Gaussian and degree parameter for polynomial kernel is kept same as before. If the data has d features, we have total 13d+ 13 kernels in the second setting. We report average accuracy accuracy over 10 random 80%\u2212 20% train-test splits.\nThe results are shown in Table 2. On all these datasets, no kernel learning approach seems to improve performance over the straightforward baselines of best kernel and average kernel. Although further study is needed to reach a definite conclusion, these results seem to indicate that blindly using a kitchen sink of standard kernels is not beneficial if the goal is to combine these kernels using an MKL approach. This highlights the importance of evaluating MKL techniques using datasets like Caltech and PSORT, where the kernels have been carefully designed using domain knowledge to capture different, potentially useful, notions of similarity in the data."}, {"heading": "4.5. Computational Efficiency", "text": "Since the number of K-examples is quadratic in the number of training instances, one might worry about the scalability of the TS-MKL method. In this section we compare the running time of TS-MKL with Target Alignment, and UFO-MKL (Ultra-Fast Optimization MKL) which, to the best of our knowledge, is the fastest one-stage MKL technique to date.\nTable 3 shows the running times for the Sonar, Pima and Caltech 101 datasets. The running time is for a single run using the best setting of parameters (i.e. it does not include the time for parameter selection). For TS-MKL and Target Alingment we also show in paranthesis the time taken by the kernel learning stage alone, without the final data SVM, on Caltech-101. For Sonar, which has only 166 training samples, the running time of UFO-MKL and TS-MKL is comparable. However, on Pima, which has 614 samples, and on Caltech, which has 3060 samples and 102 classes, TS-MKL is more than an order of magnitude faster than UFO-MKL. This shows that, by taking advantage of the advances in large scale stochastic optimization, TS-MKL is not only able to gracefully handle the quadratic increase in the number of K-examples, but it is actually the fastest MKL method to date."}, {"heading": "5. Conclusions and Future Work", "text": "Framing kernel learning as a standard classification problem in a properly defined instance space allows us to easilly adapt well understood classification techniques to obtain a scalable and high performing twostage multiple kernel learning algorithm. Our approach is backed up by formal theoretical guarantees, and by empirical evaluation that shows it always outperforms or is on par with leading one-stage and twostage kernel learning methods. This is a remarkable feat for a method that is quite simple and intuitive.\nThis new perspective on multiple kernel learning opens the door to a number of interesting questions to be addressed in subsequent research. Examples are: exploring the use of non-linear K-classifiers in conjunction with the learning with similarity functions framework; improving performance in scarce data condi-\ntions through semi-supervised and multi-task multiple kernel learning by using such techniques to learn the K-classifier; or applying TS-MKL to semisupervised clustering and dimensionality reduction problems where the supervised signal is usually given in terms of pairwise must-link and can-not-link constraints rather than labels."}], "references": [{"title": "Consistency of the Group Lasso and Multiple Kernel Learning", "author": ["F. Bach"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "On a Theory of Learning with Similarity Functions", "author": ["Balcan", "M.-F", "A. Blum"], "venue": "In ICML,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Rademacher and Gaussian Complexities: Risk Bounds and Structural Results", "author": ["P. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson", "year": 2002}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Learning non-linear combinations of kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cortes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2009}, {"title": "Generalization bounds for learning kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Two-Stage Learning Kernel Algorithms", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "On Kernel-Target Alignment", "author": ["N. Cristianini", "J. Shawe-Taylor", "A. Elisseeff", "J.S. Kandola"], "venue": "In NIPS,", "citeRegEx": "Cristianini et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cristianini et al\\.", "year": 2001}, {"title": "Predicting subcellular localization of proteins based on their N-terminal amino acid sequence", "author": ["O. Emanuelsson", "H. Nielsen", "S. Brunak", "G. von Heijne"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Emanuelsson et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Emanuelsson et al\\.", "year": 2000}, {"title": "PSORTb v.2.0: expanded prediction of bacterial protein subcellular localization and insights gained from comparative proteome analysis", "author": ["J.L. Gardy", "M.R. Laird", "F. Chen", "S. Rey", "C.J. Walsh", "M. Ester", "F.S.L. Brinkman"], "venue": "Bioinfomatics, 21:617\u2013623,", "citeRegEx": "Gardy et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gardy et al\\.", "year": 2004}, {"title": "On Feature Combination for Multiclass Object Detection", "author": ["P. Gehler", "S. Nowozin"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Gehler and Nowozin,? \\Q2009\\E", "shortCiteRegEx": "Gehler and Nowozin", "year": 2009}, {"title": "Optimizing Kernel Alignment over Combination of Kernels", "author": ["J.S. Kandola", "J. Shawe-Taylor", "N. Cristianini"], "venue": "In Tech. Report 121, Dept. of CS, Univ. of London, UK,", "citeRegEx": "Kandola et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kandola et al\\.", "year": 2002}, {"title": "`pNorm Multiple Kernel Learning", "author": ["M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kloft et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kloft et al\\.", "year": 2011}, {"title": "Learning the Kernel Matrix with Semidefinite Programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "Ghaoui", "L. El", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Learning the kernel with hyperkernels", "author": ["C.S. Ong", "A. Smola", "R. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ong et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2005}, {"title": "Ultra-fast optimization algorithm for sparse multi kernel learning", "author": ["F. Orabona", "L. Jie"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Orabona and Jie,? \\Q2011\\E", "shortCiteRegEx": "Orabona and Jie", "year": 2011}, {"title": "More efficiency in multiple kernel learning", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y. Grandvalet"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Rakotomamonjy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rakotomamonjy et al\\.", "year": 2007}, {"title": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Non-parametric group orthogonal matching pursuit for sparse learning with multiple kernels", "author": ["V. Sindhwani", "A.C. Lozano"], "venue": "In NIPS,", "citeRegEx": "Sindhwani and Lozano,? \\Q2011\\E", "shortCiteRegEx": "Sindhwani and Lozano", "year": 2011}, {"title": "Kernel Principal Component Analysis", "author": ["Smola", "B. Scholkopf A", "Muller", "K.-R"], "venue": "Advances in Kernel Methods - Support Vector Learning,", "citeRegEx": "Smola et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Smola et al\\.", "year": 1999}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. Ratsch", "C. Schafer", "B. Scholkopf"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sonnenburg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2006}, {"title": "How Good is a Kernel When Used as a Similarity Measure", "author": ["N. Srebro"], "venue": "In COLT,", "citeRegEx": "Srebro,? \\Q2007\\E", "shortCiteRegEx": "Srebro", "year": 2007}, {"title": "Multiclass Multiple Kernel Learning", "author": ["A. Zien", "C.S. Ong"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Zien and Ong,? \\Q2007\\E", "shortCiteRegEx": "Zien and Ong", "year": 2007}], "referenceMentions": [{"referenceID": 13, "context": "This one-stage approach was first proposed by (Lanckriet et al., 2004) and has since received significant attention (Rakotomamonjy et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 16, "context": ", 2004) and has since received significant attention (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien & Ong, 2007; Cortes et al., 2009; Sindhwani & Lozano, 2011).", "startOffset": 53, "endOffset": 225}, {"referenceID": 20, "context": ", 2004) and has since received significant attention (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien & Ong, 2007; Cortes et al., 2009; Sindhwani & Lozano, 2011).", "startOffset": 53, "endOffset": 225}, {"referenceID": 12, "context": ", 2004) and has since received significant attention (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien & Ong, 2007; Cortes et al., 2009; Sindhwani & Lozano, 2011).", "startOffset": 53, "endOffset": 225}, {"referenceID": 0, "context": ", 2004) and has since received significant attention (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien & Ong, 2007; Cortes et al., 2009; Sindhwani & Lozano, 2011).", "startOffset": 53, "endOffset": 225}, {"referenceID": 4, "context": ", 2004) and has since received significant attention (Rakotomamonjy et al., 2007; Sonnenburg et al., 2006; Cortes et al., 2010a; Kloft et al., 2011; Bach, 2008; Zien & Ong, 2007; Cortes et al., 2009; Sindhwani & Lozano, 2011).", "startOffset": 53, "endOffset": 225}, {"referenceID": 7, "context": "This approach has been initially proposed in (Cristianini et al., 2001) and (Kandola et al.", "startOffset": 45, "endOffset": 71}, {"referenceID": 11, "context": ", 2001) and (Kandola et al., 2002), and recently revisited by (Cortes et al.", "startOffset": 12, "endOffset": 34}, {"referenceID": 17, "context": "For the results presented in this paper we learn K-classifiers (and hence kernels) by training L2 regularized linear SVMs with positive weights using the stochastic projected sub-gradient descent method from Pegasos (Shalev-Shwartz et al., 2007).", "startOffset": 216, "endOffset": 245}, {"referenceID": 17, "context": "sos (Shalev-Shwartz et al., 2007), with an additional projection to the non-negative constraint set after every gradient step.", "startOffset": 4, "endOffset": 33}, {"referenceID": 7, "context": "Previous two-stage kernel learning approaches (Cristianini et al., 2001; Cortes et al., 2010b) learn a non-negative linear combination of base kernels that maximizes the alignment with the target kernel K(xi,xj) = yiyj on the training set.", "startOffset": 46, "endOffset": 94}, {"referenceID": 14, "context": "The approach proposed in this paper can also be cast in the framework of learning with hyperkernels (Ong et al., 2005) which provides a general recipe for kernel learning and includes Multiple Kernel Learning as a", "startOffset": 100, "endOffset": 118}, {"referenceID": 21, "context": "The following definition states formally what we mean by a good kernel (Srebro, 2007).", "startOffset": 71, "endOffset": 85}, {"referenceID": 7, "context": "This is the analog of the concentration bounds for target alignment in (Cortes et al., 2010b; Cristianini et al., 2001).", "startOffset": 71, "endOffset": 119}, {"referenceID": 20, "context": "We compare our method with several baselines: best kernel, uniform combination of base kernels (Average), target alignment, and the onestage MKL algorithms SILP (Sonnenburg et al., 2006), SimpleMKL (Rakotomamonjy et al.", "startOffset": 161, "endOffset": 186}, {"referenceID": 16, "context": ", 2006), SimpleMKL (Rakotomamonjy et al., 2007), L2-Norm MLK (Kloft et al.", "startOffset": 19, "endOffset": 47}, {"referenceID": 12, "context": ", 2007), L2-Norm MLK (Kloft et al., 2011), and UFO-MKL (Orabona & Jie, 2011).", "startOffset": 21, "endOffset": 41}, {"referenceID": 17, "context": "2 using Pegasos (Shalev-Shwartz et al., 2007) with an additional projection to the non-negative constraint set after each sub-gradient step.", "startOffset": 16, "endOffset": 45}, {"referenceID": 9, "context": "The first two datasets are for the problem of bacterial protein locations (Gardy et al., 2004).", "startOffset": 74, "endOffset": 94}, {"referenceID": 8, "context": "The third dataset used is the original plant dataset of TargetP (Emanuelsson et al., 2000), and has 940 examples with 4 classes.", "startOffset": 64, "endOffset": 90}, {"referenceID": 9, "context": "The papers that have used the Psort datasets in the past (Gardy et al., 2004; Zien & Ong, 2007), reported results after filtering out the most unsure predictions in the test set.", "startOffset": 57, "endOffset": 95}], "year": 2012, "abstractText": "With the advent of kernel methods, automating the task of specifying a suitable kernel has become increasingly important. In this context, the Multiple Kernel Learning (MKL) problem of finding a combination of prespecified base kernels that is suitable for the task at hand has received significant attention from researchers. In this paper we show that Multiple Kernel Learning can be framed as a standard binary classification problem with additional constraints that ensure the positive definiteness of the learned kernel. Framing MKL in this way has the distinct advantage that it makes it easy to leverage the extensive research in binary classification to develop better performing and more scalable MKL algorithms that are conceptually simpler, and, arguably, more accessible to practitioners. Experiments on nine data sets from different domains show that, despite its simplicity, the proposed technique compares favorably with current leading MKL approaches.", "creator": "LaTeX with hyperref package"}}}