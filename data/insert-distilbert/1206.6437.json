{"id": "1206.6437", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Large Scale Variational Bayesian Inference for Structured Scale Mixture Models", "abstract": "natural homogeneous image dependency statistics exhibit hierarchical dependencies across multiple scales. representing such prior knowledge in non - factorial to latent tree models can boost performance of image denoising, inpainting, deconvolution or reconstruction substantially, beyond yet standard synthetic factorial \" sparse \" methodology. we derive a sophisticated large scale approximate bayesian inference algorithm adapted for building linear texture models with non - factorial ( latent tree - structured ) scale mixture priors. experimental results on a range of computational denoising and inpainting problems demonstrate substantially improved performance compared to map estimation or to inference with factorial matching priors.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (750kb)", "http://arxiv.org/abs/1206.6437v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["young-jun ko", "matthias w seeger"], "accepted": true, "id": "1206.6437"}, "pdf": {"name": "1206.6437.pdf", "metadata": {"source": "META", "title": "Large Scale Variational Bayesian Inference for  Structured Scale Mixture Models", "authors": ["Young Jun Ko", "Matthias Seeger"], "emails": ["youngjun.ko@epfl.ch", "matthias.seeger@epfl.ch"], "sections": [{"heading": "1. Introduction", "text": "Leaps in performance have been realized for low-level computer vision problems, such as denoising, inpainting, deconvolution (debluring), image coding, undersampled reconstruction or acquisition optimization, by adopting super-Gaussian (\u201csparse\u201d) image priors. While most such methods employ simple factorial priors on single coefficients or groups, further substantial gains can be obtained by modelling higher-order dependencies via structured non-factorial prior distributions (Portilla et al., 2003; Wipf & Nagarajan, 2008; Cevher et al., 2010). For example, representing the dependencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010). However, previous approaches employing such non-factorial priors either run much slower than standard factorial methodology, or sacrifice performance by adopting\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nsuboptimal MAP estimation or naive mean field factorization assumptions, ignoring posterior covariance or uncertainty altogether in problems which are highly underdetermined.\nIn this paper, we derive a large scale approximate Bayesian inference algorithm for generalized linear models with non-factorial (latent tree-structured) scale mixture priors. Our contributions are as follows:\n\u2022 An image model with scale mixture prior on multi-scale wavelet coefficients, based on a latent discrete tree distribution. Mixture potentials can have arbitrary super-Gaussian components.\n\u2022 A large scale double loop algorithm for Bayesian inference in these hybrid models. We do not require factorization assumptions between image pixels or wavelet coefficients. Our method is based on standard scalable technology (preconditioned conjugate gradients, penalized least squares) and can operate at the same scales as MAP estimation.\n\u2022 An extension to incorporate non-log-concave potentials, such as Student\u2019s t, without sacrificing robustness of the optimization.\n\u2022 Automatic Bayesian learning of a substantial number of hyperparameters from raw data. Folded into the variational optimization, this process does not require much overhead. It improves performance very significantly.\n\u2022 An extensive evaluation on many inpainting and denoising datasets, comparing variational inference and MAP estimation for factorial and nonfactorial priors featuring different sparsity potentials.\nOur findings suggest (a) that predicting the variational posterior mean strongly and consistently outperforms the popular posterior mode (MAP estimation), (b) that non-factorial priors tend to\nboost performance compared to factorial ones, and (c) that Bayesian hyperparameter learning improves posterior mean prediction substantially compared to a default initialization.\nThe structure of the paper is as follows. We describe and motivate our image model in Section 2, and develop our large scale Bayesian inference and learning algorithm in Section 3. We present experimental results on image denoising and inpainting in Section 4, and close with conclusions."}, {"heading": "1.1. Related Work", "text": "A range of prior work has employed latent treestructured priors to represent dependencies between wavelet coefficients. Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities. They estimate parameters by nonlinear optimization. Papandreou et al. (2008) employ a hidden Markov tree and Gaussian mixture potentials on an overcomplete wavelet representation, estimating signal and parameters by Viterbi training. None of these employ Bayesian inference over the image or non-Gaussian potentials. He et al. (2010) use spike-and-slab prior potentials with a hidden Markov tree over the indicators. They perform standard naive mean field variational inference, employing a posterior distribution which does not represent any dependencies between coefficients. These assumptions lead to simple update equations, which they iterate in parallel. Their algorithm does not seem to reduce to standard scalable optimization primitives. Moreover, their method does not extend beyond spikeand-slab to other sparsity potentials.\nOur framework can be seen as extension of the scalable double loop algorithm for variational inference in super-Gaussian models proposed in (Seeger & Nickisch, 2011). However, they did not consider hybrid models (featuring discrete and continuous variables), latent tree non-factorial prior distributions, or Bayesian learning of hyperparameters, as we do here."}, {"heading": "2. Structured Image Model", "text": "In problems such as image denoising, inpainting or deconvolution (debluring), we seek to reconstruct a latent image u \u2208 Rn (an ny \u00d7 nx bitmap, where\nn = nynx) from noisy linear measurements y \u2208 Rm:\ny = Xu + \u03b5, \u03b5 \u223c N(0, \u03c32I),\nor P (y |u) = N(y |Xu, \u03c32I), where \u03c32 > 0 is the noise variance. For example, X = I for denoising, X = IJ,\u00b7 for inpainting (J the index of observed pixel positions), or Xu = k \u2217 u for deconvolution (k the blur kernel). With m \u2264 n (more unknowns than measurements), noise and/or blur, only additional statistical information in form of a image prior distribution P (u) renders image reconstruction a well-posed problem. Given P (u), we can infer the image from the posterior distribution\nP (u|y) = P (y |u)P (u) P (y) , P (y) =\n\u222b P (y |u)P (u) du.\n(1) For example, image statistics tend to be leptokurtic (super-Gaussian, or \u201csparse\u201d), and even simple factorial image priors P (u) respecting these properties can lead to dramatic improvements over classical methodology (least squares, Wiener filtering). Beyond marginals, image statistics exhibit complex dependencies, and capturing these in non-factorial priors can lead to further leaps in performance (Portilla et al., 2003). However, with probabilistic inference becoming much more difficult and expensive, such extended models enjoy little popularity so far compared to simpler factorial alternatives. In contrast, the methodology1 developed in this paper scales up in the same way as MAP estimation and variational inference for factorial priors.\nConsider an orthonormal discrete wavelet transform B , and denote corresponding wavelet coefficients by s = Bu. A factorial image prior has the form P (u) \u221d\u220fn j=1 tj(sj), where common super-Gaussian potentials include Laplacian\ntj(sj) \u221d \u03c4je\u2212\u03c4j |sj |, \u03c4j > 0, (2)\nor Student\u2019s t potentials\ntj(sj) \u221d \u03c41/2j ( 1 + (\u03c4j/\u03bd)s 2 j )\u2212(\u03bd+1)/2 , \u03c4j , \u03bd > 0. (3)\nIn the sequel, we assume that \u222b tj(sj) dsj = 1. Each coefficient sj belongs to a scale level of analysis l(j) \u2208 {1, . . . , L}, l = 1 the coarsest, l = L the finest scale (assume that ny, nx are multiples of 2\nL). Even though the sj are approximately uncorrelated for natural images, it is well known that there are substantial causal dependencies between coefficients in neighbouring levels (Portilla et al., 2003). These are typically mod-\n1 Our model is somewhat simpler than that of (Portilla et al., 2003).\nelled by a quad-tree2 T , linking a coefficient at level l < L to four children at level l + 1. For example, energy localized at a fine scale coefficient percolates its way up the tree through coarser scales. In order to capture this signature in a non-factorial prior, we use binary variables \u03b4j \u2208 {0, 1}, one for each sj , as well as mixture potentials tj(sj ; \u03b4j) = t0j(sj) 1\u2212\u03b4j t1j(sj) \u03b4j , where t0j(sj) enforces sj \u2248 0 more strongly than t1j(sj). Each coefficient sj can be in low (\u03b4j = 0) or high (\u03b4j = 1) state, with |sj | penalized accordingly. Moreover, we use a prior P (\u03b4) of directed graphical quad-tree structure T , which encourages the inheritance of high/low states from a parent node to its children: P (\u03b4) = \u220f j P (\u03b4j |\u03b4\u03c0(j)), where \u03c0(j) is the parent node of j in the quad-tree. All nodes of a level share a common conditional probability table, so that P (\u03b4) is parameterized by 2L hyperparameters \u03b8r,l \u2261 P (\u03b4j = 1|\u03b4\u03c0(j) = r) for l(j) = l. The implied prior on the image\nP (u) = \u2211 \u03b4 n\u220f j=1 t0j(sj) 1\u2212\u03b4j t1j(sj)\n\u03b4j\ufe38 \ufe37\ufe37 \ufe38 =:tj(sj ;\u03b4j) P (\u03b4), s = Bu,\n(4) is a non-factorial scale mixture model, which faithfully represents the causal inheritance between wavelet coefficient sizes from coarse to fine levels. The normalization constant of P (u) is one, sinceB is an orthonormal transform.\nIn Figure 1, we illustrate the effect a non-factorial prior (4) on results for an inpainting problem (75% pixels removed). The latent tree prior employs Student\u2019s t potentials t1j(sj) for the high, Gaussian potentials t0j(sj) = N(0, \u03be \u22121 0l(j)) for the low state, where hyperparameters \u03c41l, \u03be0l (two at each level) are learned automatically (results in third, \u03b4j marginals in fourth row). We also show results for a factorial prior with Student\u2019s t potentials for comparison (second row), whose hyperparameters \u03c4l are learned in the same way. The nonfactorial prior leads to a more faithful reconstruction of wavelet coefficient at coarser scales, which motivates superior inpainting results in Section 4.\nThe outcome of our procedure is an approximate posterior distribution Q(u|y)Q(\u03b4 |y). The covariance of Q(u|y) can be used for decision making, e.g. Bayesian experimental design (Seeger & Nickisch, 2011), and our results indicate that it helps the hyperparameter learning. Moreover, we predict the (approximate) posterior mean E[u|y ], not the posterior mode (MAP estimation). Our experiments demonstrate that the\n2 T is the computation tree for the fast Haar wavelet transform and describes dominating dependencies also for wavelets with larger support.\nvariational posterior mean leads to superior results in strongly ill-posed problems."}, {"heading": "3. Large Scale Variational Inference", "text": "In this section, we derive a scalable algorithm for computing variational approximations to the posterior (1) for a non-factorial scale mixture prior (4). There are\nobviously strong dependencies between components of u or s, and in contrast to previous work (He et al., 2010), we do not require any factorization assumptions between them. The high level idea behind our approach is iterative decoupling. We combine a standard variational bound to decouple u and \u03b4 (allowing us to tackle inference over the latter by belief propagation) with the double loop framework of (Seeger & Nickisch, 2011), which decouples mean and covariance computations over u. The latter provides a computational reduction to convex penalized least squares optimization and Gaussian sampling, which is crucial for scalability. As is shown below, minor simplifications result in inference or maximum a posteriori (MAP) estimation algorithms for non-factorial or factorial priors, all based on the same underlying code.\nFor the purpose of image priors, we restrict ourselves to even, super-Gaussian potentials trj(sj) (Palmer et al., 2006), which can be represented as\n\u2212 2 log t(s) = min \u03b3\u22650 s2/\u03b3 + h(\u03b3). (5)\nIntuitively, t(s) can be tightly lower bounded by (unnormalized) Gaussians of any variance \u03b3. Here, h(\u03b3) is convex if and only if \u22122 log t(s) is convex (Seeger & Nickisch, 2011). Both Laplacian (2) and Student\u2019s t potentials (3) are super-Gaussian, while \u22122 log t(s) is convex for Laplacian, but not for Student\u2019s t potentials. The criterion we will minimize is an upper bound on the negative log marginal likelihood \u22122 logP (y) from (1). First, we introduce \u03b3 from (5), consisting of \u03b30j for t0j , h0j and \u03b31j for t1j , h1j , and pull min\u03b3 out of the integral:\n\u2212 2 logP (y) = \u22122 log \u222b P (y |u)P (u) du\n\u2264 min \u03b3 \u22122 log \u2211 \u03b4 \u222b P (y |u)e\u2212 12s T (diag\u03c0)sP (\u03b4) du\n+ \u2211\nj (1\u2212 \u03b4j)h0j(\u03b30j) + \u03b4jh1j(\u03b31j)\ufe38 \ufe37\ufe37 \ufe38\n=:h(\u03b3 ;\u03b4)\n, \u03c0j := 1\u2212\u03b4j \u03b30j + \u03b4j \u03b31j ,\nwhere s = Bu. Second, we apply the variational mean field bound to the remaining log partition function:\nmax Q(u|y),Q(\u03b4 |y) EQ\n[ log\nP (y |u)e\u2212 12sT (diag\u03c0)sP (\u03b4) Q(u|y)Q(\u03b4 |y)\n] ,\nusing the factorization assumption Q(u, \u03b4 |y) = Q(u|y)Q(\u03b4 |y). In the sequel, we denote EQ(\u03b4 |y)[\u00b7] by \u3008\u00b7\u3009. The bound maximizer is Q(u|y) = ZQ(\u3008\u03c0\u3009)\u22121P (y |u)e\u2212 1 2s T (diag\u3008\u03c0\u3009)s , where ZQ(\u3008\u03c0\u3009) is a Gaussian partition function. Plugging this in, we ob-\ntain the bound\nmin Q(\u03b4 |y),\u03b3\n\u22122 logZQ(\u3008\u03c0\u3009)+h(\u03b3 ; \u3008\u03b4\u3009)+2D[Q(\u03b4 |y) \u2016P (\u03b4)],\nusing that h(\u03b3 ; \u03b4) is linear in \u03b4 . The relative entropy term is defined as D[Q(\u03b4 |y) \u2016P (\u03b4)] = \u3008logQ(\u03b4 |y) \u2212 logP (\u03b4)\u3009. Finally, and crucially for scalability, we use the variational transformation from (Seeger & Nickisch, 2011): \u22122 logZQ is equal to\nmin u\u2217,z (z + s2\u2217) T \u3008\u03c0\u3009+ \u03c3\u22122\u2016y \u2212Xu\u2217\u20162\ufe38 \ufe37\ufe37 \ufe38\n=:Rz (u\u2217,\u3008\u03c0\u3009)\n\u2212g\u2217(z),\nwhere s\u2217 = Bu\u2217, and z 0. If A(\u03c0) := \u03c3\u22122XTX + BT (diag\u03c0)B denotes the inverse covariance matrix of Q(u|y), then \u03c0 7\u2192 log |A(\u03c0)| is concave, and the \u22122 logZQ representation is based on the corresponding Fenchel duality. Since the dependence of A on \u03b4 is through \u03c0 , neither z nor g\u2217(z) depends on \u03b4 . Plugging this in and pulling minu\u2217,z outside, we obtain our final upper bound on \u22122 logP (y):\n\u03c6 = Rz (u\u2217, \u3008\u03c0\u3009)+h(\u03b3 ; \u3008\u03b4\u3009)+2D[Q(\u03b4 |y) \u2016P (\u03b4)]\u2212g\u2217(z),\nto be minimized over Q(\u03b4 |y), z , u\u2217, \u03b3 . Notice that \u3008\u03c0(\u03b4)\u3009 = \u03c0(\u3008\u03b4\u3009) by linearity.\nWe adapt the scalable convergent double loop algorithm from (Seeger & Nickisch, 2011). During the inner loop, we fix z and minimize \u03c6 over Q(\u03b4 |y) and (u\u2217,\u03b3). In between, once per outer loop iteration, we update z to obtain a tangential fit to log |A(\u3008\u03c0\u3009)|: z \u2190 VarQ[s|y ], where Q(u|y) is based on \u3008\u03c0\u3009 for the current \u03b3 and \u3008\u03b4\u3009. Computing these Gaussian variances is the most computationally intensive part of a variational inference method. We approximate z using the Perturb&MAP technique from (Papandreou & Yuille, 2010), at the cost of solving a small number of linear systems A(\u3008\u03c0\u3009)xk = rk by preconditioned conjugate gradients.\nIn the inner loop, we update Q(\u03b4 |y) and (u\u2217,\u03b3) alternatingly. For the latter, we can eliminate \u03b3 by reversing the super-Gaussian representation of\u2212 log trj(s\u2217j), plugging in pj := (zj +(s\u2217j)\n2)1/2 instead of s\u2217j . Dropping terms independent of u\u2217, we need to solve\nmin u\u2217 \u03c3\u22122\u2016y \u2212Xu\u2217\u20162 \u2212 2 \u2211 j log tj (pj ; \u3008\u03b4j\u3009) ,\npj = \u221a zj + (s\u2217j)2, s\u2217 = Bu\u2217.\n(6)\nHere, we used that log tj(sj ; \u03b4j) is linear in \u03b4j . This is a standard-form penalized least squares problem, which can be solved by any of a large number of recent algorithms developed for MAP estimation. In our experiments, we employ a nonlinear conjugate gradients\nalgorithm from the glm-ie toolbox (see Section 4). Importantly, this inner loop problem is convex iff the \u2212 log trj(sj) are convex, thus iff MAP estimation is convex for the same model (the precise relationship to MAP is detailed shortly). It is not convex if Student\u2019s t potentials (3) are used, and we will use additional bounding in order to retain inner loop convexity (Section 3.3). For the update of Q(\u03b4 |y), note that\n\u03c6 . = \u2329 \u22122 \u2211\nj log tj(pj ; \u03b4j)\n\u232a + 2D[Q(\u03b4 |y) \u2016P (\u03b4)]\n. = 2 \u2329 log\nQ(\u03b4 |y) P (\u03b4) \u220f j e log tj(pj ;\u03b4j)\n\u232a ,\nwhere \u201c . =\u201d denotes equality up to an additive constant. We can read off the minimizer Q(\u03b4 |y) \u221d P (\u03b4) \u220f j tj(pj ; \u03b4j). This is a distribution of the same quad-tree structure T as P (\u03b4), differing from the latter in the single node potentials only. We can compute both \u3008\u03b4\u3009 and D[Q(\u03b4 |y) \u2016P (\u03b4)] in O(n), using Pearl\u2019s belief propagation algorithm.\nThis completes the description of our large scale inference algorithm. At convergence, u\u2217 constitutes our posterior mean prediction EQ[u|y ]. Apart from simple direct primitives, it reduces entirely to solving a moderate number of linear systems A(\u03c0)x = r for different (\u03c0 , r), which can be done very efficiently by stateof-the-art preconditioned conjugate gradients solvers. Our double loop structure implies that the most expensive updates of z have to be done least frequently."}, {"heading": "3.1. MAP Estimation. Factorial Priors", "text": "We derived an algorithm for variational Bayesian inference with a structured non-factorial prior (4). Importantly, we can obtain scalable algorithms for MAP estimation or inference with factorial or non-factorial priors by making minor simplifying modification, otherwise using exactly the same underlying code. First, when using a factorial prior of the form P (u) =\u220f j tj(sj), we simply eliminate t1j , set \u3008\u03b4j\u3009 = 0 and eliminate Q(\u03b4 |y) altogether. The inner loop now consists of a single penalized least squares problem (6). Second, MAP estimation is obtained by simply setting z = 0, skipping variances computations and running a single outer loop iteration. MAP estimation for a non-factorial prior proceeds in an expectationmaximization fashion, alternating between penalized least squares (PLS) and belief propagation (Crouse et al., 1998)."}, {"heading": "3.2. Learning Prior Hyperparameters", "text": "In order to obtain best performance, it is necessary to endow an image prior P (u) (whether factorial or not)\nwith a substantial number of hyperparameters, which have to be adjusted to the problem at hand. For example, wavelet coefficients sj exhibit higher variance at coarser than at finer scales l(j), and corresponding prior potentials tj(sj) should take this into account, via \u03c4j in (2) or (3), or \u03bej in Gaussians N(0, \u03be \u22121 j ). In our experiments with non-factorial priors, we use L = 8 scale levels, giving rise to 16 hyperparameters (one for each level l and low/high state r): too many to be reasonably set by non-Bayesian methods like cross-validation. In this section, we show how to learn hyperparameters in an automatic Bayesian way. Our method is folded into the variational inference process, which lets us optimize hyperparameters for each dataset at hand. We stress that Bayesian learning operates on the raw data (noisy, incomplete, blurred): clean underlying images are not required.\nBayesian learning works by maximizing the log marginal likelihood logP (y) w.r.t. hyperparameters. The obvious variational approximation is to maximize the lower bound (or, equivalently, minimize \u03c6) instead. Indeed, we can treat the hyperparameters (say, \u03b8) as just another set of parameters to minimize \u03c6 over, thereby folding the learning into the inference approximation. Importantly, we can update \u03b8 as part of our inner loop optimization, for fixed z , without compromising overall convergence (to a stationary point). Recall that z comes from the Fenchel duality log |A(\u3008\u03c0\u3009)| = minz zT \u3008\u03c0\u3009 \u2212 g\u2217(z). Since A depends on all other parameters3 only through \u3008\u03c0\u3009, there is no direct dependence between z and \u03b8. Suppose that all potentials trj(sj) for fixed r \u2208 {0, 1} and l = l(j) share a hyperparameter \u03c4rl. Denote qrj := Q(\u03b4j = r|y). We write \u201cj : l\u201d short for \u201cj : l(j) = l\u201d. The relevant part of \u03c6 is\n\u03c6 . = \u22122 \u2211 j:l qrj log trj(pj), pj = \u221a zj + (s\u2217j)2.\nWe need to minimize \u03c6 w.r.t. \u03c4rl, which can often be done analytically. For Laplacian potentials (2):\n\u03c6 . = 2 \u2211 j:l qrj (\u03c4rlpj \u2212 log \u03c4rl) \u21d2 \u03c4rl \u2190 \u2211 j:l qrj\u2211 j:l qrjpj .\nFor Gaussian potentials trj(sj) \u221d \u03be1/2rl e \u2212 12 \u03berls 2 j :\n\u03c6 . = \u2211 j:l qrj ( \u03berlp 2 j \u2212 log \u03berl ) \u21d2 \u03berl \u2190 \u2211 j:l qrj\u2211 j:l qrjp 2 j .\nFor Student\u2019s t potentials, we update hyperparameters\n3 An exception would be the noise variance \u03c32. We can decouple log |A(\u3008\u03c0\u3009)| w.r.t. \u03c32 as well, but this is not done here. \u03c32 is fixed in our experiments.\n\u03c4rl only once per outer loop iteration, as detailed in Section 3.3.\nRecall the parameterization of the tree prior P (\u03b4) in terms of \u03b8r,l from Section 2. The relevant criterion part is \u03c6 . = \u3008logP (\u03b4)\u3009, which implies the updates\n\u03b8r,l \u2190  \u2211 j:1 q1j\u2211 j:1 1 l = 1\u2211 j:l q1rj\u2211\nj:l(q1rj+q0rj) l > 1\nwhere qkrj := Q(\u03b4j = k, \u03b4\u03c0(j) = r|y), k, r = 0, 1, are double node marginals."}, {"heading": "3.3. Student\u2019s T Potentials", "text": "When applied to models featuring Student\u2019s t potentials (3), the algorithm just detailed requires nonconvex PLS problems (6) to be solved in the inner loop. Commonly used first order solvers can fail dramatically on non-convex problems. Since we adopt a double loop strategy anyway, it is simpler and far more robust to use additional bounding in order to obtain a convex inner loop problem. This idea has previously been described in (Seeger & Nickisch, 2011) and applied to Student\u2019s t potentials, but our applications here, as well as our hyperparameter learning method, are novel.\nRecall the representation (5) of t(s). For a Student\u2019s t potential, h(\u03b3) is not convex, but can be written as sum of a convex term h\u222a(\u03b3) and a concave term h\u2229(\u03b3) (Seeger & Nickisch, 2011, Appendix A.6). Moreover, the concave part can be represented by Fenchel duality: h\u2229(\u03b3) = mine>0 e\u03b3 \u2212 g\u2217\u2229(e). Overall, we end up with a further parameter vector e = [erj ], which is updated alongside z . Define t\u222a(s; e) as\n\u22122 log t\u222a(s; e) = min \u03b3\u22650 s2/\u03b3 + h\u222a(\u03b3) + e\u03b3.\nA minimum over jointly convex functions in (s, \u03b3), this is a convex function in s. We end up with a modified convex inner loop PLS problem of the form (6), where \u22122 log trj(pj) is replaced by \u22122 log t\u222a;rj(pj ; erj) for every Student\u2019s t potential trj . Here, \u22122 log t\u222a(s; e) is easily computed by a single case distinction. We have to replace log trj(pj) by log t\u222a;rj(pj ; erj) also in the update of Q(\u03b4 |y).\nThe \u03c4rl hyperparameters for Student\u2019s t potentials are updated once per outer loop, alongside the erj . We use the same procedure as in Section 3.2, but applied to \u22122 log trj(pj), not its convexification. The relevant criterion part is\n\u03c6 . = \u2211 j:l qrj ( (\u03bd + 1) log ( 1 + (p2j/\u03bd)e log \u03c4rl ) \u2212 log \u03c4rl ) .\nThis is a smooth convex function in log \u03c4rl, which is easily minimized by a one-dimensional Newton solver. Once all Student\u2019s t hyperparameters have been updated, we refit the corresponding erj and continue with another inner loop."}, {"heading": "4. Experiments", "text": "We present experiments on a range of denoising and inpainting problems, comparing variational inference and MAP estimation for different models. Our results are averaged over 77 frequently used images (greyscale, 256\u00d7256), a dataset4 from (Seeger & Nickisch, 2008). Our implementation is based on the glm-ie toolbox (www.mloss.org/software/view/269/). We compare 8 methods: MAP estimation (MAP) vs. variational inference (VB), factorial prior (fact) vs. latent tree scale mixture prior (tree), and Laplacian (Lap) vs. Student\u2019s t potentials (T). The Lap-tree model uses two Laplace potentials (2) t0j(sj), t1j(sj) with different hyperparameters \u03c40l, \u03c41l, a pair for each level. The T-tree model employs Gaussian N(sj |0, \u03be\u221210l ) for the low, Student\u2019s t potentials (3) for the high state, with a pair of hyperparameters (\u03be0l, \u03c41l) at each level. We use 2L hyperparameters in the tree, L (namely, {\u03c4l}) in the fact setups. The Student\u2019s t shape parameter \u03bd is fixed to 2.1. For each run, we initialize hyperparameters \u03b8 as in (Crouse et al., 1998), by maximizing the prior probability of the raw5 data y (for the tree cases, this involves a few steps of expectation maximization), then optimize them by minimizing \u03c6. Hyperparameters were updated once per outer loop iteration.\nVB runs use up to 15 outer loop (OL) iterations. Perturb&MAP estimation of z , required for inpainting only, is run with 30 samples a\u0300 70 conjugate gradients (CG) iterations. We did 3 belief propagation and PLS calls per OL iteration for tree setups, PLS ran up to 150 iterations of nonlinear CG. Each iteration of CG requires two matrix-vector multiplications with B and X . These choices have not been optimized for maximum efficiency."}, {"heading": "4.1. Denoising", "text": "We add Gaussian random noise of variance \u03c32 = 0.01 to each image (with pixel values ui \u2208 [0, 1]). All methods use the correct value of \u03c32 in their likelihood. Notice that in this case, Gaussian variances z can be computed exactly at no cost. Namely,\n4 Thanks to H. Nickisch for providing the data. We added 2 further images.\n5 For inpainting, the missing pixels are set to mean(yi).\nA(\u03c0) = \u03c3\u22122I +BT\u03a0B , where BTB = I, so that\nz = diag\u22121 ( BA(\u03c0)\u22121BT ) = (\u03c3\u221221 + \u03c0)\u22121.\nTherefore, Perturb&MAP, the dominating cost for VB in general, is not required. Results are shown in Table 1. For this application, differences between MAP and VB reconstruction are not significant. On the other hand, the non-factorial prior improves PSNR somewhat. Hyperparameter learning improves VB performance substantially, especially when Student\u2019s t potentials are used. In contrast, it does not help6 (and can even hurt) MAP performance."}, {"heading": "4.2. Inpainting", "text": "We remove 75% of pixels at random, using the same mask J \u2282 {1, . . . , n} for all images. The design matrix is X = IJ,\u00b7, the noise variance was fixed to \u03c3\n2 = 10\u22125. Results are shown in Table 2. As PSNR does not always correlate well with visual quality, we show a range of images in Figure 2, Figure 3, Figure 4.\nVB posterior mean predictions are clearly superior to MAP reconstruction, and VB with non-factorial latent tree prior performs best. While VB with a factorial Laplace prior (Lap-fact) shows similar PSNR values to VB-Lap-tree, the visual appearance of results with the latter is clearly superior (further results, provided in the supplemental material, support these findings). The additional runtime compared to MAP estimation, mainly due to the estimation of variances z , pays off for these problems.\n6 There is no justification for maximizing the posterior w.r.t. \u03b8 . We include these results only for the fact that \u201calternating MAP\u201d learning is frequently done in practice."}, {"heading": "5. Conclusion", "text": "We presented a double loop algorithm for variational Bayesian inference in linear models with non-factorial scale mixture priors, based on a latent discrete tree distribution. Our method can operate at the same scales as MAP estimation, yet its (approximate) posterior mean prediction strongly and consistently outperforms the posterior mode across a range of inpainting problems. Both the selective smoothing by predictive variances and the coupling of wavelet coefficient across scales by way of the latent tree contribute to the removal of artefacts which plague results of MAP estimation, and of inference with factorial priors. Free hyperparameters are learned automatically by marginal likelihood maximization folded into the variational optimization.\nIn future work, we will try to adapt our large scale inference methodology to more complex hierarchical models, featuring non-Gaussian continuous latent variables (Portilla et al., 2003)."}, {"heading": "Acknowledgments", "text": "Support through a DFG Sachbeihilfe SE 2008/1-1 (AOBJ 578593) and an ERC Starting Grant (277815 \u2013 SCALABIM) are gratefully acknowledged."}], "references": [{"title": "Sparse signal acquisition and recovery with graphical models", "author": ["V. Cevher", "P. Indyk", "L. Carin", "R. Baraniuk"], "venue": "IEEE Sig. Proc. Mag.,", "citeRegEx": "Cevher et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cevher et al\\.", "year": 2010}, {"title": "Waveletbased statistical signal processing using hidden Markov models", "author": ["M. Crouse", "R. Nowak", "R. Baraniuk"], "venue": "IEEE Trans. Sig. Proc.,", "citeRegEx": "Crouse et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Crouse et al\\.", "year": 1998}, {"title": "Tree-structured compressive sensing with variational Bayesian analysis", "author": ["L. He", "H. Chen", "L. Carin"], "venue": "IEEE Sig. Proc. Letters,", "citeRegEx": "He et al\\.,? \\Q2010\\E", "shortCiteRegEx": "He et al\\.", "year": 2010}, {"title": "Variational EM algorithms for non-Gaussian latent variable models", "author": ["J. Palmer", "D. Wipf", "K. Kreutz-Delgado", "B. Rao"], "venue": "In NIPS", "citeRegEx": "Palmer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Palmer et al\\.", "year": 2006}, {"title": "Gaussian sampling by local perturbations", "author": ["G. Papandreou", "A. Yuille"], "venue": "In NIPS", "citeRegEx": "Papandreou and Yuille,? \\Q2010\\E", "shortCiteRegEx": "Papandreou and Yuille", "year": 2010}, {"title": "Image inpainting with a wavelet domain hidden markov tree model", "author": ["G. Papandreou", "P. Maragos", "A. Kokaram"], "venue": "In ICASSP,", "citeRegEx": "Papandreou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Papandreou et al\\.", "year": 2008}, {"title": "Image denoising using Gaussian scale mixtures in the wavelet domain", "author": ["J. Portilla", "V. Strela", "M. Wainwright", "E. Simoncelli"], "venue": "IEEE Trans. Image Proc.,", "citeRegEx": "Portilla et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Portilla et al\\.", "year": 2003}, {"title": "Compressed sensing and Bayesian experimental design", "author": ["M. Seeger", "H. Nickisch"], "venue": "In ICML", "citeRegEx": "Seeger and Nickisch,? \\Q2008\\E", "shortCiteRegEx": "Seeger and Nickisch", "year": 2008}, {"title": "Large scale Bayesian inference and experimental design for sparse linear models", "author": ["M. Seeger", "H. Nickisch"], "venue": "SIAM J. Imag. Sciences,", "citeRegEx": "Seeger and Nickisch,? \\Q2011\\E", "shortCiteRegEx": "Seeger and Nickisch", "year": 2011}, {"title": "A new view of automatic relevance determination", "author": ["D. Wipf", "S. Nagarajan"], "venue": "In NIPS", "citeRegEx": "Wipf and Nagarajan,? \\Q2008\\E", "shortCiteRegEx": "Wipf and Nagarajan", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "While most such methods employ simple factorial priors on single coefficients or groups, further substantial gains can be obtained by modelling higher-order dependencies via structured non-factorial prior distributions (Portilla et al., 2003; Wipf & Nagarajan, 2008; Cevher et al., 2010).", "startOffset": 219, "endOffset": 287}, {"referenceID": 0, "context": "While most such methods employ simple factorial priors on single coefficients or groups, further substantial gains can be obtained by modelling higher-order dependencies via structured non-factorial prior distributions (Portilla et al., 2003; Wipf & Nagarajan, 2008; Cevher et al., 2010).", "startOffset": 219, "endOffset": 287}, {"referenceID": 1, "context": "For example, representing the dependencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010).", "startOffset": 171, "endOffset": 234}, {"referenceID": 5, "context": "For example, representing the dependencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010).", "startOffset": 171, "endOffset": 234}, {"referenceID": 2, "context": "For example, representing the dependencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010).", "startOffset": 171, "endOffset": 234}, {"referenceID": 1, "context": "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities.", "startOffset": 0, "endOffset": 221}, {"referenceID": 1, "context": "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities. They estimate parameters by nonlinear optimization. Papandreou et al. (2008) employ a hidden Markov tree and Gaussian mixture potentials on an overcomplete wavelet representation, estimating signal and parameters by Viterbi training.", "startOffset": 0, "endOffset": 411}, {"referenceID": 1, "context": "Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent signal and mixture parameters by expectation maximization. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities. They estimate parameters by nonlinear optimization. Papandreou et al. (2008) employ a hidden Markov tree and Gaussian mixture potentials on an overcomplete wavelet representation, estimating signal and parameters by Viterbi training. None of these employ Bayesian inference over the image or non-Gaussian potentials. He et al. (2010) use spike-and-slab prior potentials with a hidden Markov tree over the indicators.", "startOffset": 0, "endOffset": 668}, {"referenceID": 6, "context": "Beyond marginals, image statistics exhibit complex dependencies, and capturing these in non-factorial priors can lead to further leaps in performance (Portilla et al., 2003).", "startOffset": 150, "endOffset": 173}, {"referenceID": 6, "context": "Even though the sj are approximately uncorrelated for natural images, it is well known that there are substantial causal dependencies between coefficients in neighbouring levels (Portilla et al., 2003).", "startOffset": 178, "endOffset": 201}, {"referenceID": 6, "context": "1 Our model is somewhat simpler than that of (Portilla et al., 2003).", "startOffset": 45, "endOffset": 68}, {"referenceID": 2, "context": "obviously strong dependencies between components of u or s, and in contrast to previous work (He et al., 2010), we do not require any factorization assumptions between them.", "startOffset": 93, "endOffset": 110}, {"referenceID": 3, "context": "For the purpose of image priors, we restrict ourselves to even, super-Gaussian potentials trj(sj) (Palmer et al., 2006), which can be represented as", "startOffset": 98, "endOffset": 119}, {"referenceID": 1, "context": "MAP estimation for a non-factorial prior proceeds in an expectationmaximization fashion, alternating between penalized least squares (PLS) and belief propagation (Crouse et al., 1998).", "startOffset": 162, "endOffset": 183}, {"referenceID": 1, "context": "For each run, we initialize hyperparameters \u03b8 as in (Crouse et al., 1998), by maximizing the prior probability of the raw data y (for the tree cases, this involves a few steps of expectation maximization), then optimize them by minimizing \u03c6.", "startOffset": 52, "endOffset": 73}, {"referenceID": 6, "context": "In future work, we will try to adapt our large scale inference methodology to more complex hierarchical models, featuring non-Gaussian continuous latent variables (Portilla et al., 2003).", "startOffset": 163, "endOffset": 186}], "year": 2012, "abstractText": "Natural image statistics exhibit hierarchical dependencies across multiple scales. Representing such prior knowledge in non-factorial latent tree models can boost performance of image denoising, inpainting, deconvolution or reconstruction substantially, beyond standard factorial \u201csparse\u201d methodology. We derive a large scale approximate Bayesian inference algorithm for linear models with nonfactorial (latent tree-structured) scale mixture priors. Experimental results on a range of denoising and inpainting problems demonstrate substantially improved performance compared to MAP estimation or to inference with factorial priors.", "creator": "LaTeX with hyperref package"}}}