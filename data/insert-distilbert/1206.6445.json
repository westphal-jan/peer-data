{"id": "1206.6445", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Deep Lambertian Networks", "abstract": "visual perception is a challenging problem in part due to illumination map variations. a possible solution is to first estimate an illumination invariant representation before using such it for recognition. the object albedo and surface intensity normals are examples of such representations. in following this paper, we introduce a multilayer generative model where the latent variables include the albedo, illuminated surface normals, and the light source. combining deep belief nets with the lambertian reflectance assumption, our model can learn good priors over the albedo from 2d images. illumination variations can be explained by changing only the lighting latent variable in our model. by transferring learned knowledge from similar particular objects, albedo and surface normals estimation from whenever a single image is possible in our ideal model. experiments demonstrate that our model is able to generalize as well as improve over some standard baselines in one - shot face plate recognition.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (5632kb)", "http://arxiv.org/abs/1206.6445v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["yichuan tang", "ruslan salakhutdinov", "geoffrey e hinton"], "accepted": true, "id": "1206.6445"}, "pdf": {"name": "1206.6445.pdf", "metadata": {"source": "META", "title": "Deep Lambertian Networks", "authors": ["Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey Hinton"], "emails": ["tang@cs.toronto.edu", "rsalakhu@cs.toronto.edu", "hinton@cs.toronto.edu"], "sections": [{"heading": "1. Introduction", "text": "Multilayer generative models have recently achieved excellent recognition results on many challenging datasets (Ranzato & Hinton, 2010; Quoc et al., 2010; Mohamed et al., 2011). These models share the same underlying principle of first learning generatively from data before using the learned latent variables (features) for discriminative tasks. The advantage of using this indirect approach for discrimination is that it is possible to learn meaningful latent variables that achieve strong generalization. In vision, illumination is a major cause of variation. When the light source\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ndirection and intensity changes in a scene, dramatic changes in image intensity occur. This is detrimental to recognition performance as most algorithms use image intensities as inputs. A natural way of attacking this problem is to learn a model where the albedo, surface normals, and the lighting are explicitly represented as the latent variables. Since the albedo and surface normals are physical properties of an object, they are features which are invariant w.r.t. illumination.\nSeparating the surface normals and the albedo of objects using multiple images obtained under different lighting conditions is known as photometric stereo (Woodham, 1980). Hayakawa (1994) described a method for photometric stereo using SVD, which estimated the shape and albedo up to a linear transformation. Using integrability constraints, Yuille et al. (1999) proposed a similar method to reduce the ambiguities to a generalized bas relief ambiguity. A related problem is the estimation of intrinsic images (Barrow & Tenenbaum, 1978; Gehler et al., 2011). However, in those works, the shading (inner product of the lighting vector and the surface normal vector) instead of the surface normals is estimated. In addition, the use of three color channels simplifies that task.\nIn the domain of face recognition, Belhumeur & Kriegman (1996) showed that the set of images of an object under varying lighting conditions lie on a polyhedral cone (illumination cone), assuming a Lambertian reflectance and a fixed object pose. Recognition algorithms were developed based on the estimation of the illumination cone (Georghiades et al., 2001; Lee et al., 2005). The main drawback of these models is that they require multiple images of an object under varying lighting conditions for estimation. While Zhang & Samaras (2006); Wang et al. (2009) present algorithms that only use a single training image, their algorithms require bootstrapping with a 3D morphable face model. For every generic object class, building a 3D morphable model would be labor intensive.\nIn this paper, we introduce a generative model which (a) incorporates albedo, surface normals, and the lighting as latent variables; (b) uses multiplicative interaction to approximate the Lambertian reflectance model; (c) learns from sets of 2D images the distributions over the 3D object shapes; and (d) is capable of one-shot recognition from a single training example.\nThe Deep Lambertian Network (DLN) is a hybrid undirected-directed model with Gaussian Restricted Boltzmann Machines (and potentially Deep Belief Networks) modeling the prior over the albedo and surface normals. Good priors over the albedo and normals are necessary since for inference with a single image, the number of latent variables is 4 times the number of observed pixels. Estimation is an ill-posed problem and requires priors to find a unique solution. A density model of the albedo and the normals also allows for parameter sharing across individual objects that belong to the same class. The conditional distribution for image generation follows from the Lambertian reflectance model. Estimating the albedo and surface normals amounts to performing posterior inference in the DLN model with no requirements on the number of observed images. Inference is efficient as we can use alternating Gibbs sampling to approximately sample latent variables in the higher layers. The DLN is a permutation invariant model which can learn from any object class and strikes a balance between laborious approaches in vision (which require 3D scanning (Blanz & Vetter, 1999)) and the generic unsupervised deep learning approaches."}, {"heading": "2. Gaussian Restricted Boltzmann Machines", "text": "We briefly describe the Gaussian Restricted Boltzmann Machines (GRBMs), which are used to model the albedo and surface normals. As the extension of binary RBMs to real-valued visible units, GRBMs (Hinton & Salakhutdinov, 2006) have been successfully ap-\nplied to tasks including image classification, video action recognition, and speech recognition (Lee et al., 2009; Krizhevsky, 2009; Taylor et al., 2010; Mohamed et al., 2011). GRBMs can be viewed as a mixture of diagonal Gaussians with shared parameters, where the number of mixture components is exponential in the number of hidden nodes. With visible nodes v \u2208 RNv and hidden nodes h \u2208 {0, 1}Nh , the energy of the joint configuration is given by:\nEGRBM (v,h) = 1\n2 \u2211 i (vi \u2212 bi)2 \u03c32i \u2212 \u2211 j cjhj\u2212 \u2211 ij Wijvihj\nThe conditional distributions needed for inference and generation are given by:\np(hj = 1|v) = 1 1 + exp(\u2212 \u2211 iWijvi \u2212 cj) , (1)\np(vi|h) = N (vi|\u00b5i, \u03c32i ), (2)\nwhere \u00b5i = bi + \u03c3 2 i \u2211 jWijhj . Additional layers of binary RBMs are often stacked on top of a GRBM to form a Deep Belief Net (DBN) (Hinton et al., 2006). Inference in a DBN is approximate but efficient, where the probability of the higher layer states is a function of the lower layer states (see Eq. 1)."}, {"heading": "3. Deep Lambertian Networks", "text": "GRBMs and DBNs use Eq. 2 to generate the intensity of a particular pixel vi. This generative model is inefficient when dealing with illumination variations in v. Specifically, the hidden activations needed to generate a bright image of an object are very different from the activations needed to generate a dark image of the same object.\nThe Lambertian reflectance model is widely used for modeling illumination variations and is a good approximation for diffuse object surfaces (those without any specular highlights). Under the Lambertian model, illustrated in Fig. 1, the i-th pixel intensity is modelled as vi = ai \u00d7max(~nTi ~\u0300, 0). The albedo ai, also known as the reflection coefficient, is the diffuse reflectivity of a surface at pixel i, which is material dependent but illumination invariant. In contrast to the generative process of the GRBM, the image of an object under different lighting conditions can be generated without changing the albedo and the surface normals. Multiplications within hidden variables in the Lambertian model give rise to this nice property."}, {"heading": "3.1. The Model", "text": "The DLN is a hybrid undirected-directed generative model that combines DBNs with the Lambertian re-\nflectance model. In the DLN, the visible layer consists of image pixel intensities v \u2208 RNv , where Nv is the number of pixels in the image. The first layer hidden variables are the albedo, surface normals, and a light source vector. Specifically, for every pixel i, there are two corresponding latent random variables: the albedo ai \u2208 R1 and surface normal ni \u2208 R3. Over an image, a \u2208 RNv is the image albedo, N is the surface normals matrix of dimension Nv \u00d7 3, where ni denotes the ith row of N. The light source variable ` \u2208 R3 points in the direction of the light source in the scene. We use GRBMs to model the albedo and surface normals, and a Gaussian prior to model `. It is important to use GRBMs since we expect the distribution over albedo and surface normals to be multi-modal (see Fig. 4).\nFig. 2 shows the architecture of the DLN model: Panel (a) displays a standard network where filled triangles denote multiplicative gating between pixels and the first hidden layer. Panel (b) demonstrates the desired latent representations inferred by our model given input v. While we use GRBMs as the prior models on albedo and surface normals, Deep Belief Network priors can be obtained by stacking additional binary RBM layers on top of the g and h layers. For clarity of presentation, in this section we use GRBM priors1.\nThe DLN combines the elegant properties of the Lambertian model with the GRBMs, resulting in a deep model capable of learning albedo and surface normal statistics from images in a weakly-supervised fashion. The DLN has the following generative process:\np(v,a,N, `) = p(a)p(N)p(`)p(v|a,N, `) (3) p(a) \u223c GRBM(a) p(N) \u2248 GRBM(vec(N)) (4) p(`) \u223c N (`|\u00b5`,\u039b)\np(v|a,N, `) = Nv\u220f i N (vi|ai(nTi `);\u03c32vi), (5)\nwhere vec(N) denotes the vectorization of matrix N.\nThe GRBM prior in Eq. 4 is only approximate since we enforce the soft constraint that the norm of ni is equal to 1.0. We achieve this via an extra energy term in Eq. 6. Eq. 5 represents the probabilistic version of the Lambertian reflectance model. We have dropped \u201cmax\u201d for convenience. \u201cmax\u201d is not critical in our model as maximum likelihood learning regulates the generation process. In addition, a prior on lighting direction fits well with the psychophysical observations that human perception of shape relies on the assump-\n1Extending our model to more flexible DBN priors is trivial.\ntion that light originates from above (Kleffner & Ramachandran, 1992).\nDLNs can also handle multiple images of the same object under varying lighting conditions. Let P be the number of images of the same object. We use L \u2208 R3\u00d7P to represent the lighting matrix with columns {`p : p = 1, 2, . . . , P}, and V \u2208 RNv\u00d7P to represent the matrix of corresponding images. The DLN energy function is defined as:\nEDLN (V,a,N,L,g,h) = 1\n2 P\u2211 p Nv\u2211 i (vip \u2212 ai(nTi `p))2 \u03c32vi\n+ 1\n2 P\u2211 p (`p \u2212 \u00b5l)T\u039b(`p \u2212 \u00b5l) + \u03b7 2 Nv\u2211 i (nTi ni \u2212 1.0)2\n+ EGRBM (a,h) + EGRBM (vec(N),g) (6)\nThe first line in the energy function is proportional to log p(v|a,N, `), the multiplicative interaction term from the Lambertian model. The second line corresponds to the quadratic energy of log p(`) and the soft norm constraint on ni. This constraint is critical for the correct estimation of the albedo, since we can interpret the albedo at each pixel as the L2 norm of the pixel surface normal. The third line contains the two GRBM energies: h \u2208 RNh represents the binary hidden variables of the albedo GRBM and g \u2208 RNg represents the hiddens of the surface normal GRBM:\nEGRBM (a,h) =\n1\n2 Nv\u2211 i (ai \u2212 bi)2 \u03c32ai \u2212 Nh\u2211 j cjhj \u2212 Nv,Nh\u2211 i,j Wijaihj\n(7)\nEGRBM (vec(N),g) = 1\n2 Nv,3\u2211 i,m=1 n2im \u03c32nim \u2212 Nv,3\u2211 i,m=1 dimnim \u03c32nim \u2212 Ng\u2211 k ekgk \u2212 Nv,3,Ng\u2211 i,m=1,k Uimknimgk\n(8)"}, {"heading": "3.2. Inference", "text": "Given images of the same object under one or more lighting conditions, we want to infer the posterior distribution over the latent variables (including albedo, surface normals and light source): p(a,N,L,g,h|V). With GRBMs modeling the albedo a and surface normals N, the posterior is complicated with no closed form solution. However, we can resort to Gibbs sampling using 4 sets of conditional distributions:\n\u2022 Conditional 1: p(g,h|a,N,L,V) \u2022 Conditional 2: p(a|N,L,h,V) \u2022 Conditional 3: p(L|N,a,V) \u2022 Conditional 4: p(N|a,L,g,V)\nConditional 1 is easy to compute as it factorizes over g, and h: p(g,h|a,N,L,v) = p(h|a)p(g|N). Since Gaussian RBMs model the albedo a and the surface normals N, the two factorized conditional distributions have the same form as Eq. 1.\nConditional 2 factorizes into a product of Gaussian distributions over Nv pixel-specific albedo variables: p(a|N,L,h,V) = Nv\u220f i p(ai|N,L,h,V) \u223c\nNv\u220f i N ( ai \u2223\u2223\u2223\u03c32ai \u2211p sipvip + \u03c6hi \u03c32vi \u03c32ai \u2211 p s 2 ip + \u03c3 2 vi ; \u03c32ai\u03c3 2 vi \u03c32ai \u2211 p s 2 ip + \u03c3 2 vi ) ,\nwhere sip = n T i `p is the illumination shading at pixel i and \u03c6hi = bi + \u03c3 2 ai \u2211 jWijhj is the top-down influence of the albedo GRBM.\nThis conditional distribution has a very intuitive interpretation. When a light source has zero strength, (`p = 0 \u2192 sip = 0), then p(ai|ni, `p,h, vi) has mean at \u03c6hi , which is purely the top-down activation.\nConditional 3 factorizes into a product distribution over P separate light variables: p(L|N,a,V) =\u220fP p=1 p(`p|N,a,vp), where p(`p|N,a,vp) is defined by a quadratic energy function:\nE(`p|N,a,v) = 1\n2 `Tp\n( \u039b + \u2211 i mim T i \u03c32vi ) `p\n\u2212 ( \u00b5Tl \u039b + \u2211 i vipmi \u03c32vi )T `p.\nHence the conditional distribution over `p is a multivariate Gaussian of the form:\np(`p|N,a,v) \u223c N (`p|\u039b\u0303 \u22121( \u00b5Tl \u039b + \u2211 i vipmi \u03c32vi ) ; \u039b\u0303 \u22121 ),\nwhere \u039b\u0303 = \u039b + \u2211 i mim T i\n\u03c32vi , and mi = aini.\nConditional 4 can be decomposed into a product of distributions over the surface normals of each pixel:\np(N|L,a,g,V) = \u220f i p(ni|L,g, ai,vi)\nSince in our model we have the soft norm constraint on ni ( \u03b7 2 \u2211Nv i (n T i ni\u22121.0)2), there is no simple closed form for p(ni|L,g, ai,vi). We use the Hamiltonian Monte Carlo (HMC) algorithm for sampling.\nHMC (Duane et al., 1987; Neal, 2010) is an auxiliary variable MCMC method which combines Hamiltonian dynamics with the Metropolis algorithm to sample continuous random variables. In order to use HMC, we must have a differentiable energy function over the variables. In this case, the energy of conditional 4 takes form:\nE(ni) = 1\n2 ni\nT\n(\u2211 p(ai`p)(ai`p) T\n\u03c32vi + Di\n) ni\n\u2212 ( ai \u2211 p vip`p\n\u03c32vi + \u03c6gi T Di\n) + \u03b7\n2 (ni\nTni \u2212 1)2,\nwhere \u03c6gi is the top-down mean of ni from the g-layer, and Di = diag(\u03c3 \u22122 ni1 , \u03c3 \u22122 ni2 , \u03c3 \u22122 ni3) is the 3 \u00d7 3 diagonal matrix.\nWe note that there is a linear ambiguity when we estimate the normals and lighting direction. In Eq. 5, nTi `p = n T i RR\n\u22121`p. This means that we can only estimate ni and `p up to a linear transformation. Fortunately, while R is unknown, it is constant across\n{vi}Pi=1 due to the learned priors over N, a and `. Therefore, recognition and image relighting tasks (Sec. 4) are not affected."}, {"heading": "3.3. Learning", "text": "Learning is accomplished using a variant of the EM algorithm. In the E-step, MCMC samples are drawn from the approximate posterior distribution (Neal & Hinton, 1998). We first sample from the conditional distributions in Sec. 3.2 to approximate the posterior p(a,N,L,h,g|V; \u03b8old). We then optimize the joint log-likelihood function w.r.t. the model parameters. Specifically,\n4\u03b8 = \u2212\u03b1 \u222b p(a,N,L,h,g|V; \u03b8old) \u2202\n\u2202\u03b8\n{ E(V,a,N,L,h,g; \u03b8) } (9)\nwhere \u03b1 is the learning rate. We approximate the integral using:\n1\nN N\u2211 i \u2202 \u2202\u03b8 { E(V,a(i),N(i),L(i),h(i),g(i); \u03b8) } ,\nwhere the samples {a(i),N(i),L(i),h(i),g(i)} are approximately drawn from the posterior distribution p(a,N,L,h,g|V; \u03b8old) in the E-step. Maximum likelihood learning of GRBMs (and DBNs) is intractable. We therefore turn to Contrastive Divergence (CD) (Hinton, 2002) to compute an approximate gradient during learning. The complete training algorithm for the DLN in presented in Alg. 1.\nRather than starting with randomly initialized weights, we can achieve better convergence by first training the albedo GRBM on a separate face database. We can then transfer the learned weights before learning the complete DLN."}, {"heading": "4. Experiments", "text": "We experiment with the Yale B and the Extended Yale B face databases. Combined, the two databases contain 64 frontal images of 38 different subjects. 45 images for each subject are further divided into 4 subsets of increasing illumination variations. Fig. 3 shows samples from the Yale B and Extended Yale B database.\nFor each subject, we used approximately 45 frontal images for our experiments2. We separated 28 subjects from the Extended Yale B database for training and held-out all 10 subjects from the original Yale B database for testing3. The preprocessing step involved\n2A few of the images are corrupted. 3We used the cropped images provided by the Yale B\nAlgorithm 1 Learning Deep Lambertian Networks 1: Pretrain the {a, h} albedo GRBM with faces images and initialize {W,b, c} of the DLN with the GRBM\u2019s parameters. 2: Initialize other weights \u223c N (0, 0.012), \u03c32 \u2190 1.0. repeat\n//Approximate E-step: for n = 1 to #training subjects do\n3: Given Vn, sample p(a,N,L,h,g|Vn; \u03b8old) using the conditionals defined in Sec. 3.2, obtaining samples of {a(i),N(i),L(i)}.\nend for\n//Approximate M-step:\n4: Treating {a(i)} as training data, CD is used to learn the weights of the albedo GRBM. 5: Treating {N(i)} as training data, CD is used to learn the weights of the surface normal GRBM. 6: Maximum likelihood estimations of the parameters \u03c32vi , \u00b5`, and \u039b are computed.\nuntil convergence\ndownsizing the face images to the resolution of 24\u00d724. Using equations of Sec. 3.2, we can infer one albedo image and one set of surface normals from each of the 28 subjects. These 28 training albedo and surface normal samples are insufficient for multilayer generative models with millions of parameters. Therefore, we leverage a large set of the face images from the Toronto Face Database (TFD) (Susskind et al., 2011). The TFD is a collection of 100,000 face images from a variety of other datasets. To create more training data for the surface normals, we randomly translated all 28 sets of them by \u00b12 pixels.\nThe DLN used 2 layer DBNs (instead of single layer GRBMs) to model the priors over a and N. The albedo DBN had 800 h1 nodes and 200 h2 nodes. The normals DBN had 1000 g1 nodes and 100 g2 nodes. To see what the DLN\u2019s prior on the albedo looks like, we show samples generated by the albedo DBN in Fig. 4.\nExtended database website: http://goo.gl/LKwtX.\nLearning the multi-modal albedo prior is made possible by the use of unsupervised TFD data."}, {"heading": "4.1. Inference", "text": "After learning, we investigated the inference process in the DLN. Although the DLN can use multiple images of the same object during inference, it is important to investigate how well it performs with a single test image. We are also interested in the number of iterations that sampling would take to find the posterior modes.\nIn our first experiment, we presented the model with a single Yale B face image from a held-out test subject, as shown in Fig. 5. The light source illuminates the subject from the bottom right, causing a significant shadow across the top left of the subject\u2019s face. Since the albedo captures a lighting invariant representation of the face, correct posterior distribution should automatically perform illumination normalization. Using the algorithm described in Sec. 3.2, we clamp the visible nodes to the test face image and sample from the 4 conditionals in an alternating fashion. HMC was used to sample from N. In total, we perform 50 iterations of alternating Gibbs sampling. During each iteration, the N variables are sampled using HMC with 20 leapfrog iterations and 10 HMC epochs. The step size was set to 0.01 with a momentum of 2.0. The acceptance rate was around 0.7.\nWe plot the intermediate samples from iterations 1 to 50 in Fig. 5. The top row displays the inferred albedo a. At every pixel, there is a surface normal vector ni \u2208 R3. For visual presentation, we treat each ni as a RGB pixel and plot them as color images in the bottom row. Note that the Gibbs chain quickly jumps (at iteration 5) into the correct mode. Good results are obtained due to the knowledge transfer of the albedo and surface normals learned from other subjects.\nWe next randomly selected single test images from the 10 Yale B test subjects. Using exactly the same sampling algorithm, Fig. 6(a) shows their inferred albedo and surface normals. The first column displays the test image, the middle and right columns contain the estimated albedo and surface normals. We also found that using two test images per subject improves performance. Specifically, we sampled from p(a,N|V \u2208 RNv\u00d72) instead of p(a,N|v \u2208 RNv ). The results are\ndisplayed in Fig. 6(b)."}, {"heading": "4.2. Relighting", "text": "The task of face relighting is useful to demonstrate strong generalization capabilities of the model. The goal is to generate face images of a particular person under never-before seen lighting conditions. Realistic images can only be generated if the albedo and surface normals of that particular person were correctly inferred. We first sample the lighting variable ` from its Gaussian prior defined by {\u00b5,\u039b}. Conditioned on the inferred a and N (see Fig. 6(b)), we use Eq. 5 to draw samples of v. Fig. 6(c) shows relighted face images of held-out test subjects."}, {"heading": "4.3. Recognition", "text": "We next test the performance of DLN at the task of face recognition. For the 10 test subjects of Yale B, only image(s) from subset 1 (with 7 images) are used for training. Images from subsets 2-4 are used for testing. In order to use DLN for recognition, we first infer the albedo (ai) and surface normals (ni) conditioned on the provided training image(s) of test subjects. For every subject, a 3 dimensional linear subspace is spanned by the inferred albedo and surface normals. In particular, we consider the matrix M of dimensions Nv\u00d73, with the i-th row set to mi = aini. The columns of M spans the 3 dimensional linear subspace. Test images of the test subjects are compared to all 10 subspaces and are labeled according to the label of its nearest subspace.\nFig. 7 plots the recognition errors as a function of number of training images used. DBN results are obtained by training a 2 layer DBN directly on the training images, and a linear SVM was trained on the top-most hidden activations of the DBN. That standard DBN can not handle lighting variations very accurately. Another approach, called Normalized Correlation, first normalizes images to unit norm. For each test image,\nits cosine similarity to all training images is computed. The test image takes on the label of the closest training image. Normalized Correlation performs significantly better than Nearest Neighbor due to its normalization, which removes some of the lighting variations. Finally, the SVD method finds a 3 dimensional linear subspace (with the largest singular values) spanned by the training images of each of the test subjects. A test image is assigned to the closest subspace.\nWe note that for the important task of one-shot recognition, DLN significantly outperforms many other methods. In the computer vision literature, Zhang & Samaras (2006); Wang et al. (2009) report lower error rates on the Yale B dataset. However, their algorithms make use of pre-existing 3D morphable models, whereas the DLN learns the 3D information automatically from 2D images."}, {"heading": "4.4. Generic Objects", "text": "The DLN is applicable not only on face images but also images of generic objects. We used 50 objects from the Amsterdam Library of Images (ALOI) database (Geusebroek et al., 2005). For every object, 15 images of varying lighting were divided into 10 for training and 5 for testing. Using the provided masks for each object, images are cropped and rescaled to the resolution of 48\u00d748. We used a DLN with Nh = 1000 and Ng = 1500. A 500 h\n2 layer and 500 g2 layer were also added. After training, we performed posterior inference using one of the held-out image. Fig. 8 shows results. The top row contains test images, the middle\nrow displays the inferred albedo images after 50 alternating Gibbs iterations, and the bottom row shows the inferred surface normals."}, {"heading": "5. Discussions", "text": "We have introduced a generative model with meaningful latent variables and multiplicative interactions simulating the Lambertian Reflectance model. We have shown that by learning priors on these illuminationinvariant variables directly from data, we can improve on one-shot recognition tasks as well as generate images under novel illuminations."}, {"heading": "Acknowledgements", "text": "We thank Maksims Volkovs, James Martens, and Abdelrahman Mohamed for discussions. This research was supported by NSERC and CIFAR."}], "references": [{"title": "Recovering intrinsic scene characteristics from images", "author": ["H.G. Barrow", "J.M. Tenenbaum"], "venue": "In CVS78,", "citeRegEx": "Barrow and Tenenbaum,? \\Q1978\\E", "shortCiteRegEx": "Barrow and Tenenbaum", "year": 1978}, {"title": "What is the set of images of an object under all possible lighting conditions", "author": ["P.N. Belhumeur", "D.J. Kriegman"], "venue": "In CVPR, pp", "citeRegEx": "Belhumeur and Kriegman,? \\Q1996\\E", "shortCiteRegEx": "Belhumeur and Kriegman", "year": 1996}, {"title": "A morphable model for the synthesis of 3-D faces", "author": ["V. Blanz", "T. Vetter"], "venue": "In SIGGraph-99,", "citeRegEx": "Blanz and Vetter,? \\Q1999\\E", "shortCiteRegEx": "Blanz and Vetter", "year": 1999}, {"title": "Recovering intrinsic images with a global sparsity prior on reflectance", "author": ["P. Gehler", "C. Rother", "M. Kiefel", "L. Zhang", "B. Sch\u00f6lkopf"], "venue": "In NIPS,", "citeRegEx": "Gehler et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gehler et al\\.", "year": 2011}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["Georghiades", "Athinodoros S", "Belhumeur", "Peter N", "Kriegman", "David J"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell,", "citeRegEx": "Georghiades et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Georghiades et al\\.", "year": 2001}, {"title": "The amsterdam library of object images", "author": ["J.M. Geusebroek", "G.J. Burghouts", "A.W.M. Smeulders"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Geusebroek et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Geusebroek et al\\.", "year": 2005}, {"title": "Photometric stereo under a light-source with arbitrary motion", "author": ["H. Hayakawa"], "venue": "Journal of the Optical Society of America,", "citeRegEx": "Hayakawa,? \\Q1994\\E", "shortCiteRegEx": "Hayakawa", "year": 1994}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "On the perception of shape from shading", "author": ["Kleffner", "Dorothy A", "V.S. Ramachandran"], "venue": "Perception and Psychophysics,", "citeRegEx": "Kleffner et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Kleffner et al\\.", "year": 1992}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": null, "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K.C. Lee", "Ho", "Jeffrey", "Kriegman", "David"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Lee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2005}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Mohamed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mohamed et al\\.", "year": 2011}, {"title": "MCMC using Hamiltonian dynamics. in Handbook of Markov Chain Monte Carlo", "author": ["R.M. Neal"], "venue": null, "citeRegEx": "Neal,? \\Q2010\\E", "shortCiteRegEx": "Neal", "year": 2010}, {"title": "A new view of the EM algorithm that justifies incremental, sparse and other variants", "author": ["R.M. Neal", "G.E. Hinton"], "venue": "Learning in Graphical Models,", "citeRegEx": "Neal and Hinton,? \\Q1998\\E", "shortCiteRegEx": "Neal and Hinton", "year": 1998}, {"title": "Modeling pixel means and covariances using factorized third-order boltzmann machines", "author": ["M. Ranzato", "G. Hinton"], "venue": null, "citeRegEx": "Ranzato and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Ranzato and Hinton", "year": 2010}, {"title": "The Toronto Face Database", "author": ["J.M. Susskind", "A.K. Anderson", "G.E. Hinton"], "venue": "Technical report,", "citeRegEx": "Susskind et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Susskind et al\\.", "year": 2011}, {"title": "Springer, 2010", "author": ["Taylor", "Graham W.", "Fergus", "Rob", "LeCun", "Yann", "Bregler", "Christoph. Convolutional learning of spatiotemporal features. In ECCV"], "venue": "ISBN 978-3-642-15566-6. URL http://dx.doi.org/10.1007/ 978-3-642-15567-3.", "citeRegEx": "Taylor et al\\.,? 2010", "shortCiteRegEx": "Taylor et al\\.", "year": 2010}, {"title": "Face relighting from a single image under arbitrary unknown lighting conditions", "author": ["Y. Wang", "L. Zhang", "Z.C. Liu", "G. Hua", "Z. Wen", "Z.Y. Zhang", "D. Samaras"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Photometric method for determining surface orientation from multiple images", "author": ["R.J. Woodham"], "venue": "Optical Engineering,", "citeRegEx": "Woodham,? \\Q1980\\E", "shortCiteRegEx": "Woodham", "year": 1980}, {"title": "Determining generative models of objects under varying illumination: Shape and albedo from multiple images using SVD and integrability", "author": ["A.L. Yuille", "D. Snow", "R. Epstein", "P.N. Belhumeur"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Yuille et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Yuille et al\\.", "year": 1999}, {"title": "Face recognition from a single training image under arbitrary unknown lighting using spherical harmonics", "author": ["L. Zhang", "D. Samaras"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "Zhang and Samaras,? \\Q2006\\E", "shortCiteRegEx": "Zhang and Samaras", "year": 2006}], "referenceMentions": [{"referenceID": 13, "context": "Multilayer generative models have recently achieved excellent recognition results on many challenging datasets (Ranzato & Hinton, 2010; Quoc et al., 2010; Mohamed et al., 2011).", "startOffset": 111, "endOffset": 176}, {"referenceID": 20, "context": "Separating the surface normals and the albedo of objects using multiple images obtained under different lighting conditions is known as photometric stereo (Woodham, 1980).", "startOffset": 155, "endOffset": 170}, {"referenceID": 3, "context": "A related problem is the estimation of intrinsic images (Barrow & Tenenbaum, 1978; Gehler et al., 2011).", "startOffset": 56, "endOffset": 103}, {"referenceID": 5, "context": "Hayakawa (1994) described a method for photometric stereo using SVD, which estimated the shape and albedo up to a linear transformation.", "startOffset": 0, "endOffset": 16}, {"referenceID": 5, "context": "Hayakawa (1994) described a method for photometric stereo using SVD, which estimated the shape and albedo up to a linear transformation. Using integrability constraints, Yuille et al. (1999) proposed a similar method to reduce the ambiguities to a generalized bas relief ambiguity.", "startOffset": 0, "endOffset": 191}, {"referenceID": 4, "context": "Recognition algorithms were developed based on the estimation of the illumination cone (Georghiades et al., 2001; Lee et al., 2005).", "startOffset": 87, "endOffset": 131}, {"referenceID": 12, "context": "Recognition algorithms were developed based on the estimation of the illumination cone (Georghiades et al., 2001; Lee et al., 2005).", "startOffset": 87, "endOffset": 131}, {"referenceID": 4, "context": "Recognition algorithms were developed based on the estimation of the illumination cone (Georghiades et al., 2001; Lee et al., 2005). The main drawback of these models is that they require multiple images of an object under varying lighting conditions for estimation. While Zhang & Samaras (2006); Wang et al.", "startOffset": 88, "endOffset": 296}, {"referenceID": 4, "context": "Recognition algorithms were developed based on the estimation of the illumination cone (Georghiades et al., 2001; Lee et al., 2005). The main drawback of these models is that they require multiple images of an object under varying lighting conditions for estimation. While Zhang & Samaras (2006); Wang et al. (2009) present algorithms that only use a single training image, their algorithms require bootstrapping with a 3D morphable face model.", "startOffset": 88, "endOffset": 316}, {"referenceID": 11, "context": "As the extension of binary RBMs to real-valued visible units, GRBMs (Hinton & Salakhutdinov, 2006) have been successfully applied to tasks including image classification, video action recognition, and speech recognition (Lee et al., 2009; Krizhevsky, 2009; Taylor et al., 2010; Mohamed et al., 2011).", "startOffset": 220, "endOffset": 299}, {"referenceID": 18, "context": "As the extension of binary RBMs to real-valued visible units, GRBMs (Hinton & Salakhutdinov, 2006) have been successfully applied to tasks including image classification, video action recognition, and speech recognition (Lee et al., 2009; Krizhevsky, 2009; Taylor et al., 2010; Mohamed et al., 2011).", "startOffset": 220, "endOffset": 299}, {"referenceID": 13, "context": "As the extension of binary RBMs to real-valued visible units, GRBMs (Hinton & Salakhutdinov, 2006) have been successfully applied to tasks including image classification, video action recognition, and speech recognition (Lee et al., 2009; Krizhevsky, 2009; Taylor et al., 2010; Mohamed et al., 2011).", "startOffset": 220, "endOffset": 299}, {"referenceID": 9, "context": "Additional layers of binary RBMs are often stacked on top of a GRBM to form a Deep Belief Net (DBN) (Hinton et al., 2006).", "startOffset": 100, "endOffset": 121}, {"referenceID": 14, "context": "HMC (Duane et al., 1987; Neal, 2010) is an auxiliary variable MCMC method which combines Hamiltonian dynamics with the Metropolis algorithm to sample continuous random variables.", "startOffset": 4, "endOffset": 36}, {"referenceID": 7, "context": "We therefore turn to Contrastive Divergence (CD) (Hinton, 2002) to compute an approximate gradient during learning.", "startOffset": 49, "endOffset": 63}, {"referenceID": 17, "context": "Therefore, we leverage a large set of the face images from the Toronto Face Database (TFD) (Susskind et al., 2011).", "startOffset": 91, "endOffset": 114}, {"referenceID": 19, "context": "In the computer vision literature, Zhang & Samaras (2006); Wang et al. (2009) report lower error rates on the Yale B dataset.", "startOffset": 59, "endOffset": 78}, {"referenceID": 5, "context": "We used 50 objects from the Amsterdam Library of Images (ALOI) database (Geusebroek et al., 2005).", "startOffset": 72, "endOffset": 97}], "year": 2012, "abstractText": "Visual perception is a challenging problem in part due to illumination variations. A possible solution is to first estimate an illumination invariant representation before using it for recognition. The object albedo and surface normals are examples of such representations. In this paper, we introduce a multilayer generative model where the latent variables include the albedo, surface normals, and the light source. Combining Deep Belief Nets with the Lambertian reflectance assumption, our model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable in our model. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is possible in our model. Experiments demonstrate that our model is able to generalize as well as improve over standard baselines in one-shot face recognition.", "creator": "LaTeX with hyperref package"}}}