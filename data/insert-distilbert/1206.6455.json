{"id": "1206.6455", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Regularizers versus Losses for Nonlinear Dimensionality Reduction: A Factored View with New Convex Relaxations", "abstract": "we demonstrate why that almost all non - parametric dimensionality conflict reduction methods can be expressed by a simple procedure : regularized loss minimization plus singular value : truncation. by somehow distinguishing the role of the loss and regularizer in such a process, we recover a factored perspective that reveals why some gaps in the difficult current literature. beyond actually identifying a useful new loss for manifold unfolding, a key contribution motivation is to derive new convex regularizers that combine distance maximization with rank reduction. these regularizers can readily be fully applied to any loss.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (451kb)", "http://arxiv.org/abs/1206.6455v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["james neufeld", "yaoliang yu", "xinhua zhang", "ryan kiros", "dale schuurmans"], "accepted": true, "id": "1206.6455"}, "pdf": {"name": "1206.6455.pdf", "metadata": {"source": "META", "title": "Regularizers versus Losses for Nonlinear Dimensionality Reduction", "authors": ["Yaoliang Yu", "James Neufeld", "Ryan Kiros", "Xinhua Zhang", "Dale Schuurmans"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "Dimensionality reduction is an ubiquitous and important form of data analysis. Recovering the inherent manifold structure of data\u2014i.e. the local directions of large versus minimal variation\u2014enables useful representations based on encoding highly varying directions. Not only can this reveal important structure in data, and hence support visualization, it also provides an automated form of noise removal and data normalization that can aide subsequent data analysis.\nAlthough linear dimensionality reduction is a well studied topic, recent progress continues to be made with the investigation of convex regularizers, such as the trace norm or 2,1-norm, that enable application to general losses beyond squared error (Candes et al., 2011; Xu et al., 2010; Srebro & Shraibman, 2005). The literature on nonlinear dimensionality reduction, by comparison, has grown more rapidly yet devoted relatively less attention to developing appropriate regularizers. This provides one of our main motivations.\nThe focus of this paper is on unsupervised dimensionality reduction; that is, we will not directly address su-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\npervised variants, e.g. (Weinberger & Saul, 2009). We will also focus on non-parametric formulations that do not require an explicit map connecting the low and high dimensional representations; e.g. as in restricted Boltzmann machines (Larochelle & Bengio, 2008), auto-encoders (Rifai et al., 2011), or parameterized kernel reductions (Wang et al., 2010).\nThe primary benefit of focusing on unsupervised, nonparametric formulations is that it allows a simple yet comprehensive overview of current methods. In particular, we observe that nearly all current non-parametric methods can be expressed as regularized loss minimization of a reconstruction matrix followed by singular value truncation. This perspective allows us to distinguish the role of the loss from that of the regularizer : the loss relates the learned reconstruction to the data, whereas the regularizer relates the reconstruction to the desired topology independent of the data. Such a separation allows a simpler organization of current methods than current overviews (Burges, 2010; Lee & Verleysen, 2010a). More importantly, it reveals new research directions. A brief overview of current losses, for example, reveals a useful alternative that remains uninvestigated. Similarly, an assessment of current regularizers reveals that very few have been explored: in fact, only one family of convex regularizers has been widely used in the nonlinear case (distance maximization), which has known weaknesses. Although non-convex regularizers have been proposed to mitigate these weaknesses, these introduce intractability. Our main contribution is to derive efficient new convex regularizers that are able to combine distance maximization with rank reduction."}, {"heading": "2. Preliminaries", "text": "Below we will need to manipulate and relate data, kernel, and Euclidean distance matrices respectively. Assume one is given t observations, either expressed as a t\u00d7 n data matrix X; a t\u00d7 t kernel matrix K where K = K \u2032 and K < 0; or a t\u00d7 t squared Euclidean distance matrix D where D = D\u2032, D \u2265 0, \u03b4(D) = 0 and HDH 4 0, such that \u03b4 denotes diagonal and\nH = I \u2212 1t11 \u2032 denotes the centering matrix. Then we can map between these various matrices via\nK(X) = XX \u2032 (1) D(X) = \u03b4(XX \u2032)1\u2032 + 1\u03b4(XX \u2032)\u2032 \u2212 2XX \u2032 (2) D(K) = \u03b4(K)1\u2032 + 1\u03b4(K)\u2032 \u2212 2K (3) K(D) = \u2212 12HDH (4)\n(where the function intended is determined by the argument). Note that it is easy to map a data matrix to its corresponding kernel or Euclidean distance matrix, but such a map is neither linear nor invertible: kernel matrices drop orientation, while Euclidean distance matrices drop orientation and translation. However, by centering the data or kernel matrix, thus removing translation information, the relationship between kernel and Euclidean distance matrices becomes simple.\nProposition 1 A linear bijection exists between centered kernel and squared Euclidean distance matrices.\nIt is easy to verify that D(K(D)) = D and K(D(K)) = HKH for a valid Euclidean distance and kernel matrix respectively. Therefore if f(D) is convex in D then f(D(K)) must be convex in K. Proposition 1 thus allows one to equivalently re-express problems in terms of centered kernel matrices or Euclidean distance matrices without affecting expressiveness or convexity.\nIn this paper we assume the target dimensionality d is fixed beforehand. That is, we are not addressing the problem of estimating the intrinsic dimensionality; for a survey see (Lee & Verleysen, 2010a, Ch.3). We will also need to make use of the indicator function\n[[\u03c6]] = { 0 if the predicate \u03c6 is true \u221e if the predicate \u03c6 is false . (5)"}, {"heading": "3. Background: Linear Case", "text": "First briefly consider linear dimensionality reduction, which illustrates some basic points. Here one is given a data matrix X and seeks a reduced rank representation X\u0302. It turns out that a simple, generic strategy covers almost all methods that have been proposed: First, solve the regularized loss minimization problem\nmin X\u0303 L(X\u0303;X) +R(X\u0303) (6)\nfor a given loss L and regularizer R, obtaining the reconstruction X\u0303. Then recover the low rank representation X\u0302 by truncating all but the top d singular values of X\u0303; that is, X\u0302 = U\u0303:,1:d\u03a3\u03031:d,1:dV\u0303 \u2032 :,1:d such that U\u0303 \u03a3\u0303V\u0303 \u2032 = X\u0303 is the singular value decomposition. Note that the loss relates X\u0303 to the data, X, whereas the regularizer enforces assumptions on the reconstruction X\u0303\nthat are independent of the data. Interestingly, it is the regularizer, not the loss, that typically determines the computational difficulty of this problem.\nRegularizers: The role of the regularizer is to encourage the desired topology. To illustrate, consider the common regularizers proposed for the linear case\nR1(X\u0303) = [[rank(X\u0303) \u2264 d]] (7) R2(X\u0303) = \u03b1\u2016X\u0303\u2016tr (8) R3(X\u0303) = \u03b1\u2016X\u0303 \u2032\u20162,1 (9)\nwhere \u03b1 \u2265 0 is a regularization parameter. All of these clearly encode a desire for reduced rank.\nIn particular, the rank indicator (7) is the standard regularizer for spectral dimensionality reduction, which eliminates the need for truncation. Unfortunately rank is not convex, and enforcing (7) is only known to be tractable for squared loss (10) (Jolliffe, 1986). For other losses, such as absolute error (11) or Bregman divergence (12), rank is normally enforced by means of alternating descent in a factored representation: minAB L(AB;X) where A and B are t \u00d7 d and d\u00d7n respectively (Collins et al., 2001; Gordon, 2002). Unfortunately, this cannot guarantee optimality.1\nThe difficulty of working with rank explains the emergence of convex, rank-reducing regularizers such as the trace norm (8) (Candes et al., 2011; Srebro & Shraibman, 2005) and block norm (9) (Xu et al., 2010). In fact, the trace norm is known to be the tightest convex approximation to rank.2 These regularizers allow a tractable formulation for general convex losses, and also allow a desired rank to be enforced by appropriately choosing \u03b1 (Cai et al., 2008).\nLoss Functions: Despite the importance of regularization, it is interesting to observe that until recently much of the research effort in linear dimensionality reduction investigated alternative losses, including\nL1(X\u0303;X) = \u2016X \u2212 X\u0303\u20162F (10) L2(X\u0303;X) = \u2016X \u2212 X\u0303\u20161,1 (11) L3(X\u0303;X) = BF (X\u0303\u2016X), such that (12) BF (X\u0303\u2016X) = F (X\u0303)\u2212F (X)\u2212 tr(\u2207F (X)\u2032(X\u0303\u2212X)). (13)\nHere \u2016 \u00b7 \u2016F denotes Frobenius norm, \u2016 \u00b7 \u20161,1 denotes 1,1 block norm, and (13) is a Bregman divergence associated with strictly convex potential F . Beyond the squared error (10) used for PCA (Jolliffe, 1986), the\n1 Random projection has also been considered, but it is difficult to establish guarantees for general losses (Brinkman & Charikar, 2003).\n2 Specifically, it is the bi-conjugate of the rank function over the unit sphere in spectral-norm (Recht et al., 2010).\nabsolute loss (11) has been recently proposed for robust PCA (Candes et al., 2011; Xu et al., 2010), and Bregman divergences (12) have been implicitly proposed in exponential family PCA (Collins et al., 2001; Gordon, 2002).3 Interestingly, all these standard loss functions are convex in X\u0303.\nThe conclusion we draw from the linear case that the loss functions considered are standard, convex, and not the source of computational difficulty. Instead, it is regularization that has posed the greatest difficulty, particularly the desire to reduce rank. Interestingly, for the non-parametric case we find the same holds: almost every method follows regularized loss minimization plus truncation, almost every loss adopted is standard (if expressed between Euclidean distance matrices), and the main difficulty lies in devising regularizers that encourage desired topology. Such a simple perspective is surprisingly not widely appreciated."}, {"heading": "4. Nonlinear Case", "text": "A general non-parametric approach to dimensionality reduction can be obtained by expressing the problem in terms of kernel matrices. (Recall that data or Euclidean distance matrices can always be converted to kernel matrices.) Here we assume one is given a t \u00d7 t kernel matrix K determined by the data and seeks a reconstruction K\u0303 that allows a reduced rank representation. Here too it turns out that a simple, generic strategy covers almost all methods that have been proposed: First, solve the regularized loss minimization\nmin K\u0303=K\u0303\u2032,K\u0303<0,K\u03031=1\nL(D(K\u0303);D(K)) +R(K\u0303) (14)\nfor a given loss L and regularizer R, obtaining the reconstruction K\u0303. Then recover the low rank representation X\u0302 by truncating all but the top d eigenvalues and factoring; that is, X\u0302 = Q\u0303:,1:d\u039b\u0303 1/2 1:d,1:d, where Q\u0303\u039b\u0303Q\u0303 \u2032 = K\u0303 is the eigenvalue decomposition of K\u0303.\n(The reason for stating the loss in (14) in terms of D(K\u0303) will be explained below. Note that if the loss function L is convex in its first argument it must also be convex in K\u0303 by Proposition 1.)\nAlthough the difference between the linear and nonparametric formulations does not appear large, (14) offers far greater flexibility in recovering alternative topological structures, such as nonlinear manifolds in the original data. In fact, a key intuition behind most non-parametric dimensionality reduction methods is unfolding, where one supposes the data lay on a low di-\n3 Noting the equivalence between regular exponential families and Bregman divergences (Banerjee et al., 2005).\nmensional curved submanifold that is to be \u201cunfolded\u201d into a linear subspace. Unfortunately, current proposals conflate the role of the loss and regularizer in such a process, and propose only specific combinations of the two. We find it revealing to consider them separately."}, {"heading": "4.1. Regularizers", "text": "The natural role for regularization in dimensionality reduction is to relate the reconstruction K\u0303 to a desired topology, expressing prior assumptions about the nature of the target representation independent of the data. For example, one might seek a coordinate representation in a linear subspace, in which case it is desirable to encourage \u201cflattening\u201d by spreading distances. However, regularization can express other topologies (see below). As before, the computational challenges appear to be primarily dictated by the regularizer.\nThe most common target topology is a linear subspace, for which the most common regularizers considered are\nR1(K\u0303) = [[rank(K\u0303) \u2264 d]] (15) R2(K\u0303) = \u03b2tr(K\u0303) (16) R3(K\u0303) = \u2212\u03b1tr(K\u0303) (17)\nwhere \u03b1 \u2265 0 and \u03b2 \u2265 0 are regularization parameters.\nFor example, the rank indicator (15) is the most commonly used regularizer, often associated with classical spectral dimensionality reduction (kernel PCA) using squared error (22) (Schoelkopf et al., 1999; Ham et al., 2004). Unfortunately, rank is not convex, and efficient training procedures are not known for other losses. Instead, for other losses, rank is typically enforced by resorting to local minimization in a factored representation: minX\u0302 L(D(X\u0302X\u0302 \u2032);D) where X\u0302 is a t\u00d7 d representation matrix. Unfortunately, no current loss provides a convex formulation in X\u0302, and optimal solutions usually cannot be guaranteed.\nConsequently, convex regularizers have also played a prominent role in non-parametric dimensionality reduction. For example, applying the trace norm here yields (16). Unfortunately, tr(K\u0303) = 1\u2032D(K\u0303)1/(2t) for centered K\u0303, thus minimizing trace is equivalent to shrinking distances; in opposition to the desire to unfold manifolds. Consequently, the negated regularizer (17) has proved more effective, forming one of the key components of maximum variance unfolding (MVU) (28) (Weinberger et al., 2004; 2007). Neither of these regularizers is completely satisfactory however.\nPartitioned regularizer: It is obvious what one would desire: to spread distances in the top d dimensions and shrink distances in the remaining dimensions, for the target dimensionality d. Such a regu-\nlarizer has been proposed by (Shaw & Jebara, 2007):\nR4(K\u0303) = \u2212\u03b1max P\u2208P tr(PK\u0303) + \u03b2 min P\u2208P tr((I \u2212 P )K\u0303) (18)\n= min P\u2208P\n\u03b2tr(K\u0303)\u2212 (\u03b1+ \u03b2)tr(PK\u0303), where (19)\nP = {P : P \u2032 = P, I < P < 0, tr(P ) = d}. (20)\nUnfortunately, (19) is not convex; (Shaw & Jebara, 2007) resort to alternating minimization between K\u0303 and P . Below we provide new convex regularizers that approximate (19), providing our main contribution.\nTopographic Methods: As an aside, it is interesting to note that alternative topologies can be encouraged via regularization. For example, given a graph expressed as an adjacency matrix G \u2208 {0, 1}t\u00d7t, a regularizer can encourage K\u0303 to adopt G\u2019s structure\nR5(K\u0303) = \u2212 max M\u2208{0,1}t\u00d7t,M1=1, 1\u2032M=1\u2032\ntr(M \u2032K\u0303MG), (21)\nwhich provides a generalized approach to topographic embedding (Quadrianto et al., 2010; Bishop et al., 1998). Unfortunately, (21) is concave in K\u0303 and the inner optimization over M is an NP-hard quadratic assignment problem, so we do not pursue this further."}, {"heading": "4.2. Loss Functions", "text": "The role of the loss function is to relate the reconstruction to the data. In the formulation (14) we have chosen to express losses as between Euclidean distance matrices. We will see that such a viewpoint, although nonstandard, provides clarity. For example, unfolding is naturally enabled by loss locality : errors in reconstructing small target distances should be punished more harshly than errors in larger distances. Loss locality allows larger distances in the reconstruction more leeway to adapt to the desired target topology.\nExpressing current losses in terms of distances reveals that they are almost all standard, convex, and obvious. We briefly survey standard losses to demonstrate how broadly the perspective applies to current methods, and to highlight useful alternatives.\nClassical Losses: The oldest losses used for dimensionality reduction do not express locality, and therefore tend to recover linear subspace representations:\nL1(D\u0302;D) = \u2016H(D \u2212 D\u0302)H\u20162F (22) L2(D\u0302;D) = \u2016D \u2212 D\u0302\u20162F (23) L3(D\u0302;D) = \u2016 \u221a D \u2212 \u221a D\u0302\u20162F (24) L4(D\u0302;D) = \u2016D \u2212 D\u0302\u20161,1 (25)\nHere \u221a \u00b7 is applied component-wise. These losses are all convex in D\u0302 (Dattorro, 2012, Ch.7). The doubly centered squared loss (22) is used in kernel PCA\n(Schoelkopf et al., 1999) (recall HDH = \u22122K(D)). Although (23) is frequently mentioned in the multidimensional scaling (MDS) literature (Cox & Cox, 2001), its use is rare since it cannot be tractably combined with rank (15) (Dattorro, 2012). The absolute loss (25) in an important alternative that has been used in robust MDS (Cayton & Dasgupta, 2006).\nLocal Losses: Unlike the classical losses, however, local losses encourage unfolding by de-emphasizing errors on large distances. Let N (D) \u2208 {0, 1}t\u00d7t denote an adjacency function, such that N (D)ij = 1 indicates that i and j are neighbors in D (i.e. either within a distance of or among the k nearest neighbors). The best known examples of local losses are L5(D\u0302;D) = \u2211 ij(Dij \u2212 D\u0302ij)2/Dij (26)\nL6(D\u0302;D) = \u2211 ij(Dij \u2212 D\u0302ij)2 w(D\u0302ij) (27)\nL7(D\u0302;D) = \u2211 ij [[N (D)ij = 1 and D\u0302ij 6= Dij ]] (28)\nL8(D\u0302;D) = \u2211 ij N (D)ij(Dij \u2212 D\u0302ij)2. (29)\nThese are the Sammon loss (26) (Sun et al., 2011; Lee & Verleysen, 2010a); the curvilinear components loss (27) (Sun et al., 2010); the neighborhood indicator used in MVU and Isomap (28) (Weinberger et al., 2004; Tenenbaum et al., 2000); and the relaxed loss introduced in regularized MVU (29) (Weinberger et al., 2007), respectively. All such losses emphasize errors on small target distances (or predictions) over errors on large target distances. Moreover, they are all convex in D\u0302.4 Although the convexity of these losses with respect to D\u0302 has not always received significant notice in the literature, this is an important fact for (14).\nBregman Divergences: Bregman divergences provide another loss specification that emphasizes locality. Recall that a Bregman divergence is defined by BF (D\u0302\u2016D) = F (D\u0302)\u2212F (D)\u2212 tr(\u2207F (D)\u2032(D\u0302\u2212D)) for a strictly convex differentiable potential F , which by construction must be convex in D\u0302. A number of such divergences have proved to be important in the dimensionality reduction literature, including\nBF9(D\u0302ij\u2016Dij) = D\u0302ij(log D\u0302ij\u2212logDij)+Dij\u2212D\u0302ij (30) BF10(D\u0302ij\u2016Dij) = exp(\u2212D\u0302ij/\u03c3)\u2212 exp(\u2212Dij/\u03c3)\n+ exp(\u2212Dij/\u03c3)(D\u0302ij \u2212Dij)/\u03c3 (31) BF11(D\u0302i:\u2016Di:) = p(Di:)(log p(Di:)\u2212 log p(D\u0302i:))\u2032 (32)\nsuch that p(Di:) = exp(\u2212Di:)\nexp(\u2212Di:1) (33)\n4 The convexity of curvilinear components loss (27) depends on w(d\u0302); for example exp(\u2212d\u0302/\u03c3), 1(d\u0302\u2264 ), or 1/d\u0302 (Lee & Verleysen, 2010a). The latter makes (27) convex.\nBF12(D\u0302\u2016D) = tr(p(D)\u2032(log p(D)\u2212 log p(D\u0302))) (34)\nsuch that p(D) = exp(\u2212D)\n1\u2032 exp(\u2212D)1 , (35)\nwhere \u03c3 > 0 is a scale parameter, log and exp are applied component-wise, and it is assumed the divergences are summed over all ij or all i where necessary.\nThe unnormalized entropy (30) was proposed in (Sun et al., 2011) to approximate the Sammon loss (26), whereas the reciprocal exponential Bregman divergence (31) was proposed in (Sun et al., 2010) to approximate the curvilinear components loss (27) under\nw(d\u0302) = exp(d\u0302/\u03c3). However, the latter approximation was achieve by placing D\u0302 in the second position, using `(D\u0302ij ;Dij) = BF (Dij\u2016D\u0302ij), which is no longer convex (see below). The Bregman divergence (32) matches the loss used in SNE (van der Maaten & Hinton, 2008) up to a minor variation: the transfer p in SNE does not normalize over the entire vector, but only over the vector minus the current entry. The later, symmetric SNE error can also be recovered (almost) from the matrix-wise Bregman divergence (34) up to the same minor variation, plus a second exception: even though p(D\u0302) is computed as in (35), p(D) is computed by averaging the column and row probabilities through ij using (33).\nSurprisingly, the exponential divergence (31) has not previously been used with D\u0302 in the first, convex position. This yields a highly localized loss that is well suited to manifold unfolding, demonstrating even stronger locality than the other divergences. Therefore, we investigate its behavior further below.\nLarge Margin Losses: Large margin losses for nonlinear dimensionality reduction have also been proposed in (Shaw & Jebara, 2009): L13(D\u0302;D) = \u2211 i max j N\u0304 (D)ij(max k N (D)ikD\u0302ik \u2212 D\u0302ij) where N\u0304 (D)ij = 1\u2212N (D)ij (36) L14(D\u0302;D) = max\nN\u2208N `(N,N (D)) + tr(D\u0302(N (D)\u2212N)) (37)\nwhere ` is a local margin loss function. The intuition behind the loss (36) is simple: for each node i, one would like the distances to all disconnected nodes j (such that N (D)ij = 0) to be greater than the distance to the furthest connected node k (i.e. such that N (D)ik = 1). The second loss (37) is defined with respect to a class of alternative adjacency matrices N producible by running an efficient dynamic program on candidate distance matrices D\u0302. This loss, termed \u201cstructure preserving\u201d in (Shaw & Jebara, 2009), behaves like a structured output SVM loss that tries\nto make sure that the sum of estimated distances on N (D) are less than on any alternative adjacency matrix in N, plus a margin dictated by how far N \u2208 N is from N (D). Both losses are convex.\nNon-convex Losses: Finally, even though nonconvex losses are computationally problematic, a few important methods are expressible in this manner:\nL15(D\u0302;D) = \u2211 ij p(D)ij(log p(D)ij\u2212log q(D\u0302)ij) (38)\nwhere q(D\u0302)ij = (1 + D\u0302ij) \u22121\u2211 k 6=l(1 + D\u0302kl) \u22121 (39)\nL16(D\u0302;D) = \u2212 max W :\u03b4(W )=0,WN\u0304=0,W1=1 tr ( (I \u2212W )((1\u2212 \u03c1)D + \u03c1D\u0302)(I \u2212W )\u2032 ) (40)\nwhere \u03c1 \u2265 0 is a weighting parameter. The first loss corresponds to tSNE (38), which is a modification of the symmetric SNE loss (34), using the same transfer p(D) on the target distances D, but using a \u201cheavier tailed\u201d transfer function on D\u0302 (van der Maaten & Hinton, 2008). The second loss (40) can be shown to be equivalent to local linear embedding (LLE) (Roweis & Saul, 2000; Saul & Roweis, 2003; Ham et al., 2004) if one tracks the solution in the limit as \u03c1\u2198 0. Clearly, this formulation shows that the LLE loss is highly nonconvex in D\u0302.\nNote that other non-parametric dimensionality reduction methods can also be expressed in terms of regularized minimization of a loss between distance matrices, but the above suffices to illustrate how comprehensively current methods can be covered. Interestingly, this loss-based framework generalizes probabilistic formulations (Lawrence, 2011), since e.g. large margin losses cannot be naturally expressed as log-likelihoods."}, {"heading": "5. New Convex Regularizers", "text": "Our main contribution is to propose two new convex regularizers for non-parametric dimensionality reduction. In particular, we formulate convex relaxations of the partitioned regularizer (19), which simultenously seeks to spread distances on the top d dimensions while shrinking distances in the remaining directions. The consequences for both rank reduction and manifold unfolding are clear. We first introduce a slight modification of (19) by adding a small quadratic smoother\nR5(K\u0303) = min P\u2208P\n\u03b3 2 tr(K\u03032)+\u03b2tr(K\u0303)\u2212(\u03b1+\u03b2)tr(PK\u0303), (41)\nwhere \u03b3 > 0, and P is the same as defined previously in (20). Although this modified regularizer is still not convex in K\u0303, it enables two useful relaxations."}, {"heading": "5.1. Completed Square", "text": "The first relaxation we propose is extremely simple: (41) can be made jointly convex in K\u0303 and P simply by completing the square, yielding\nR6(K\u0303) = min P\u2208P\n\u03b2tr(K\u0303) + \u03b3\n2 \u2225\u2225\u2225K\u0303 \u2212 \u03b1+\u03b2\u03b3 P\u2225\u2225\u22252 F . (42)\nThis is guaranteed to be an upper bound on (41) since we are merely adding a nonnegative term \u03b32 \u2016 \u03b1+\u03b2 \u03b3 P\u2016 2 F . A lower bound on (41) can be recovered by subtracting d(\u03b1+\u03b2)2\n2\u03b3 from (42) since \u2016P\u2016 2 F \u2264 d for all P \u2208 P.\nThe main benefit of this relaxation is that it is extremely simple and computationally attractive: a simple modification of the alternating minimization strategy of (Shaw & Jebara, 2007) now yields a global solution. This does not, however, yield the tightest convex approximation of (41), as we now demonstrate."}, {"heading": "5.2. Bi-conjugation", "text": "Recall that the conjugate of a function f is defined by f\u2217(y) = supx\u2208dom(f)\u3008x, y\u3009 \u2212 f(x) (Borwein & Lewis, 2006). Importantly, any function is lower bounded by its bi-conjugate; i.e. f \u2265 f\u2217\u2217 (Borwein & Lewis, 2006, \u00a74.2). Therefore, a general strategy for deriving maximal convex lower bounds on objective functions can be based on Fenchel bi-conjugation (Jojic et al., 2011). Here, we obtain a new regularizer by formulating the tightest convex relaxation of (41) based on its bi-conjugate and define\nR7(K\u0303) = R \u2217\u2217 5 (K\u0303). (43)\nBy construction, this must be a convex function in K\u0303.\nTheorem 1\nR\u2217\u22175 (U) = max Z=Z\u2032 tr(UZ)\n\u2212 1 2\u03b3 max P\u2208P \u2016[Z\u2212\u03b2I+(\u03b1+\u03b2)P ]+\u20162F , (44)\nwhere [\u00b7]+ = max(0, \u00b7) is applied to the eigenvalues.\nProof: We compute the Fenchel bi-conjugate of\ng(K) =\n{ R5(K) if K < 0\n\u221e otherwise . (45)\nFirst, the conjugate of g(K) is easily derived as\ng\u2217(Z) = max K<0 tr(Z \u2032K)\u2212R5(K) (46)\n= max K<0 max P\u2208P tr(Z \u2032K)\u2212 \u03b3 2 tr(K2)\u2212 \u03b2tr(K)\n+ (\u03b1+ \u03b2)tr(KP ). (47)\nNote that the domain of g\u2217 is actually all t\u00d7t matrices, but (47) implies that g\u2217(Z) = g\u2217(Z \u2032) = g\u2217((Z+Z \u2032)/2), hence for the purpose of computing g\u2217\u2217 no generality is lost if we restrict the domain of g\u2217 to symmetric matrices. Now for any U < 0 we obtain\ng\u2217\u2217(U) = max Z\u2208St tr(UZ)\u2212 g\u2217(Z) (48)\n= max Z\u2208St min P\u2208P min K<0\ntr(UZ)\u2212 tr(ZK)\n+ \u03b3\n2 tr(K2) + \u03b2tr(K)\u2212 (\u03b1+ \u03b2)tr(KP )\n= max Z\u2208St min P\u2208P\ntr(UZ)\u2212\nmax K<0 tr[K(Z \u2212 \u03b2I + (\u03b1+ \u03b2)P )]\u2212 \u03b3 2 \u2016K\u20162F (49)\n= max Z\u2208St tr(UZ)\u2212 1 2\u03b3 max P\u2208P \u2016[Z\u2212\u03b2I+(\u03b1+\u03b2)P ]+\u20162F .\nThe last equality is due to von Neumann\u2019s trace inequality (Borwein & Lewis, 2006) and the elementary equality maxx\u22650 xy \u2212 \u03b32x 2 = 12\u03b3 (y) 2 +.\nPutting \u03b3 = 0, Theorem 1 implies that the Fenchel biconjugate of (19) is exactly (17) (for Z = \u2212\u03b1I) if d > 0 and is (16) (for Z = \u03b2I) if d = 0, which might explain the success of MVU (Weinberger et al., 2004).\nAlthough (44) appears to be a complex regularizer, it is not computationally much harder to optimize than (42). To evaluate R\u2217\u22175 (U), an optimal Z in (44) must be solved, but this is a convex problem and admits efficient optimization. First note that the inner maximization in (44) necessarily attains its maximum at some extreme point of the set P (for we are maximizing a convex function). Next inspecting (49) and invoking von Neumann\u2019s trace inequality one more time we reduce (44) to its vector counterpart. Let {zi} and {ui} (both arranged in decreasing order) be the eigenvalues of Z and U , respectively, then we just need to solve:\nmin zi\u2265zi+1 \u2212(2\u03b3)u\u2032z + d\u2211 i=1 [zi + \u03b1] 2 + + t\u2211 i=d+1 [zi \u2212 \u03b2]2+. (50)\nTemporarily ignoring the constraints, the optimal z is obvious since all elements of z are separated in (50):\nz\u0302i = { \u03b3ui \u2212 \u03b1 if i \u2264 d \u03b3ui + \u03b2 if i \u2265 d+ 1 . (51)\nNow observe that z\u0302i automatically satisfies the order constraints in the two blocks, thus the only possibility of violating the order constraint is between the blocks, i.e. z\u0302d < z\u0302d+1. But if we knew z\u0302d = \u03bb, we would be able to fix the order by setting\nzi(\u03bb) = { max(z\u0302i, \u03bb) if i \u2264 d min(z\u0302i, \u03bb) if i \u2265 d+ 1 . (52)\nTherefore we only need to find \u03bb that minimizes (50), which is a univariate convex piecewise quadratic function in \u03bb, hence can be done very quickly.\nTo summarize, for evaluating R\u2217\u22175 (U) (and obtaining a subgradient), we need to perform SVD on U and then solve (50). The former costs O(t3) while the latter costs O(t), where t is the number of points."}, {"heading": "6. Experimental Evaluation", "text": "Most evaluations of dimensionality reduction methods resort to subjective assessments of specific case studies. However, many proposals exist for quantitatively evaluating the quality of different methods in a somewhat objective manner (Sun et al., 2011; Lee & Verleysen, 2010b; van der Maaten, 2009). Evaluation is clearer when the regularizer and loss components are considered separately. In particular, to compare regularizers, an objective assessment can be based on the loss values achieved by the low-rank reconstruction. For a given loss function L, we measure the reconstruction loss L(D(X\u0302X\u0302 \u2032);D) achieved by the recovered low rank representation X\u0302. Another objective assessment can be based on the run time of the corresponding methods.5 Finally, we can assess the quality of the convex relaxations by measuring the gap between the final objective values achieved using the relaxed regularizers versus their source regularizer R5 (41).\n5 Here we measure run time using a common implementation based on accelerated projected subgradient descent (Tseng, 2008). Accelerated projected subgradient descent proves to be a far more scalable and generally applicable method for these problems than a generic SDP solver.\nIn our experiments, we compare different regularizers on a representative set of loss functions. The losses we considered are: regularized MVU loss (29), SNE (34), and the reciprocal exponential Bregman divergence (31). The results are given in Table 1 for six regularizers and three losses on four different data sets.6 The regularizers used are R2 (16), R3 (17), R4 (19), R5 (41), R6 (42), and R7 (44). Additionally, we report the objective values for R5 and its relaxations R6 and R7, to assess the approximation gap.\nIn terms of reconstruction error, the new convex regularizers obtain the best reconstruction errors overall, almost always outperforming the other competitors. A particular disadvantage of the partitioned regularizers is that they are not convex, hence sensitive to initialization: the objective values (shown in the square brackets in the rightmost three columns) demonstrate that inferior local minima do exist, since the completed square regularizer (R6) provides an upper bound on the optimal partition(\u03b3) objective. The run times for the new regularizers are somewhat slower."}, {"heading": "7. Conclusion", "text": "We have presented simple view of non-parametric dimensionality reduction by separating the roles of the regularizer and loss function. The result is a compact survey of a large fraction of the literature, which reveals a natural loss function that has not been thoroughly explored. More importantly, we developed two new convex relaxations of a useful, but non-convex reg-\n6 Data set details are given in the supplement.\nularizer. We investigated the behavior of the new regularizers across a representative sample of loss functions. Important directions for future research include extending the convex regularization framework to consider other target topologies, sparsity, and mixtures."}], "references": [{"title": "Clustering with Bregman divergences", "author": ["A. Banerjee", "S. Merugu", "I. Dhillon", "J. Ghosh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "GTM: the generative topographic mapping", "author": ["C. Bishop", "M. Svensen", "C. Williams"], "venue": "Neural Computation,", "citeRegEx": "Bishop et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 1998}, {"title": "On the impossibility of dimension reduction in l1", "author": ["B. Brinkman", "M. Charikar"], "venue": "In Proc. STOC,", "citeRegEx": "Brinkman and Charikar,? \\Q2003\\E", "shortCiteRegEx": "Brinkman and Charikar", "year": 2003}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J. Cai", "E. Candes", "Z. Shen"], "venue": "SIAM Journal on Optimzation,", "citeRegEx": "Cai et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2008}, {"title": "Robust principal component analysis", "author": ["E. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": "JACM, 58:1\u201337,", "citeRegEx": "Candes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2011}, {"title": "Robust Euclidean embedding", "author": ["L. Cayton", "S. Dasgupta"], "venue": "In ICML,", "citeRegEx": "Cayton and Dasgupta,? \\Q2006\\E", "shortCiteRegEx": "Cayton and Dasgupta", "year": 2006}, {"title": "A generalization of principal component analysis to the exponential family", "author": ["M. Collins", "S. Dasgupta", "R. Schapire"], "venue": "In NIPS", "citeRegEx": "Collins et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2001}, {"title": "Convex Optimization and Euclidean Distance Geometry", "author": ["J. Dattorro"], "venue": "Meboo Publishing,", "citeRegEx": "Dattorro,? \\Q2012\\E", "shortCiteRegEx": "Dattorro", "year": 2012}, {"title": "A kernel view of dimensionality reduction of manifolds", "author": ["J. Ham", "D. Lee", "S. Mika", "B. Schoelkopf"], "venue": "In ICML,", "citeRegEx": "Ham et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2004}, {"title": "Convex envelopes of complexity controlling penalties", "author": ["V. Jojic", "S. Saria", "D. Koller"], "venue": "In AISTATS,", "citeRegEx": "Jojic et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jojic et al\\.", "year": 2011}, {"title": "Classification using discriminative restricted Boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Larochelle and Bengio,? \\Q2008\\E", "shortCiteRegEx": "Larochelle and Bengio", "year": 2008}, {"title": "Spectral dimensionality reduction via maximum entropy", "author": ["N. Lawrence"], "venue": "In AISTATS,", "citeRegEx": "Lawrence,? \\Q2011\\E", "shortCiteRegEx": "Lawrence", "year": 2011}, {"title": "Nonlinear Dimensionality Reduction", "author": ["J. Lee", "M. Verleysen"], "venue": null, "citeRegEx": "Lee and Verleysen,? \\Q2010\\E", "shortCiteRegEx": "Lee and Verleysen", "year": 2010}, {"title": "Scale-independent quality criteria for dimensionality reduction", "author": ["J. Lee", "M. Verleysen"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Lee and Verleysen,? \\Q2010\\E", "shortCiteRegEx": "Lee and Verleysen", "year": 2010}, {"title": "Kernelized sorting", "author": ["N. Quadrianto", "A. Smola", "L. Song", "T. Tuytelaars"], "venue": "IEEE PAMI,", "citeRegEx": "Quadrianto et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Quadrianto et al\\.", "year": 2010}, {"title": "Guaranteed minimumrank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P. Parrilo"], "venue": "SIAM Review,", "citeRegEx": "Recht et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2010}, {"title": "Contractive auto-encoders: explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": null, "citeRegEx": "Roweis and Saul,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Think globally, fit locally: unsupervised learning of low dimensional manifolds", "author": ["L. Saul", "S. Roweis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Saul and Roweis,? \\Q2003\\E", "shortCiteRegEx": "Saul and Roweis", "year": 2003}, {"title": "Kernel principal component analysis", "author": ["B. Schoelkopf", "A. Smola", "K. Mueller"], "venue": "In NIPS", "citeRegEx": "Schoelkopf et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schoelkopf et al\\.", "year": 1999}, {"title": "Minimum volume embedding", "author": ["B. Shaw", "T. Jebara"], "venue": "In AISTATS,", "citeRegEx": "Shaw and Jebara,? \\Q2007\\E", "shortCiteRegEx": "Shaw and Jebara", "year": 2007}, {"title": "Structure preserving embedding", "author": ["B. Shaw", "T. Jebara"], "venue": "In ICML,", "citeRegEx": "Shaw and Jebara,? \\Q2009\\E", "shortCiteRegEx": "Shaw and Jebara", "year": 2009}, {"title": "Curvilinear components analysis and Bregman divergences", "author": ["J. Sun", "M. Crowe", "C. Fyfe"], "venue": "In ESANN,", "citeRegEx": "Sun et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2010}, {"title": "Extending metric multidimensional scaling with Bregman divergences", "author": ["J. Sun", "M. Crowe", "C. Fyfe"], "venue": "Pattern Recognition,", "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J. Tenenbaum", "V. de Silva", "J. Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["P. Tseng"], "venue": "Submitted to SIAM Journal on Optimization,", "citeRegEx": "Tseng,? \\Q2008\\E", "shortCiteRegEx": "Tseng", "year": 2008}, {"title": "Learning a parametric embedding by preserving local structure", "author": ["L. van der Maaten"], "venue": "In AISTATS,", "citeRegEx": "Maaten,? \\Q2009\\E", "shortCiteRegEx": "Maaten", "year": 2009}, {"title": "Visualizing data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "J. Mach. Learn. Research,", "citeRegEx": "Maaten and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Unsupervised kernel dimension reduction", "author": ["M. Wang", "F. Sha", "M. Jordan"], "venue": "In NIPS", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "L. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weinberger and Saul,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul", "year": 2009}, {"title": "Learning a kernel matrix for nonlinear dimensionality reduction", "author": ["K. Weinberger", "F. Sha", "L. Saul"], "venue": "In ICML,", "citeRegEx": "Weinberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2004}, {"title": "Graph Laplacian regularization for large-scale semidefinite programming", "author": ["K. Weinberger", "F. Sha", "Q. Zhu", "L. Saul"], "venue": "In NIPS", "citeRegEx": "Weinberger et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2007}, {"title": "Robust PCA via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "In NIPS", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Although linear dimensionality reduction is a well studied topic, recent progress continues to be made with the investigation of convex regularizers, such as the trace norm or 2,1-norm, that enable application to general losses beyond squared error (Candes et al., 2011; Xu et al., 2010; Srebro & Shraibman, 2005).", "startOffset": 249, "endOffset": 313}, {"referenceID": 32, "context": "Although linear dimensionality reduction is a well studied topic, recent progress continues to be made with the investigation of convex regularizers, such as the trace norm or 2,1-norm, that enable application to general losses beyond squared error (Candes et al., 2011; Xu et al., 2010; Srebro & Shraibman, 2005).", "startOffset": 249, "endOffset": 313}, {"referenceID": 16, "context": "as in restricted Boltzmann machines (Larochelle & Bengio, 2008), auto-encoders (Rifai et al., 2011), or parameterized kernel reductions (Wang et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 28, "context": ", 2011), or parameterized kernel reductions (Wang et al., 2010).", "startOffset": 44, "endOffset": 63}, {"referenceID": 6, "context": "For other losses, such as absolute error (11) or Bregman divergence (12), rank is normally enforced by means of alternating descent in a factored representation: minAB L(AB;X) where A and B are t \u00d7 d and d\u00d7n respectively (Collins et al., 2001; Gordon, 2002).", "startOffset": 221, "endOffset": 257}, {"referenceID": 4, "context": "The difficulty of working with rank explains the emergence of convex, rank-reducing regularizers such as the trace norm (8) (Candes et al., 2011; Srebro & Shraibman, 2005) and block norm (9) (Xu et al.", "startOffset": 124, "endOffset": 171}, {"referenceID": 32, "context": ", 2011; Srebro & Shraibman, 2005) and block norm (9) (Xu et al., 2010).", "startOffset": 53, "endOffset": 70}, {"referenceID": 3, "context": "These regularizers allow a tractable formulation for general convex losses, and also allow a desired rank to be enforced by appropriately choosing \u03b1 (Cai et al., 2008).", "startOffset": 149, "endOffset": 167}, {"referenceID": 15, "context": "2 Specifically, it is the bi-conjugate of the rank function over the unit sphere in spectral-norm (Recht et al., 2010).", "startOffset": 98, "endOffset": 118}, {"referenceID": 4, "context": "absolute loss (11) has been recently proposed for robust PCA (Candes et al., 2011; Xu et al., 2010), and Bregman divergences (12) have been implicitly proposed in exponential family PCA (Collins et al.", "startOffset": 61, "endOffset": 99}, {"referenceID": 32, "context": "absolute loss (11) has been recently proposed for robust PCA (Candes et al., 2011; Xu et al., 2010), and Bregman divergences (12) have been implicitly proposed in exponential family PCA (Collins et al.", "startOffset": 61, "endOffset": 99}, {"referenceID": 6, "context": ", 2010), and Bregman divergences (12) have been implicitly proposed in exponential family PCA (Collins et al., 2001; Gordon, 2002).", "startOffset": 94, "endOffset": 130}, {"referenceID": 0, "context": "3 Noting the equivalence between regular exponential families and Bregman divergences (Banerjee et al., 2005).", "startOffset": 86, "endOffset": 109}, {"referenceID": 19, "context": "For example, the rank indicator (15) is the most commonly used regularizer, often associated with classical spectral dimensionality reduction (kernel PCA) using squared error (22) (Schoelkopf et al., 1999; Ham et al., 2004).", "startOffset": 180, "endOffset": 223}, {"referenceID": 8, "context": "For example, the rank indicator (15) is the most commonly used regularizer, often associated with classical spectral dimensionality reduction (kernel PCA) using squared error (22) (Schoelkopf et al., 1999; Ham et al., 2004).", "startOffset": 180, "endOffset": 223}, {"referenceID": 30, "context": "Consequently, the negated regularizer (17) has proved more effective, forming one of the key components of maximum variance unfolding (MVU) (28) (Weinberger et al., 2004; 2007).", "startOffset": 145, "endOffset": 176}, {"referenceID": 14, "context": "which provides a generalized approach to topographic embedding (Quadrianto et al., 2010; Bishop et al., 1998).", "startOffset": 63, "endOffset": 109}, {"referenceID": 1, "context": "which provides a generalized approach to topographic embedding (Quadrianto et al., 2010; Bishop et al., 1998).", "startOffset": 63, "endOffset": 109}, {"referenceID": 19, "context": "The doubly centered squared loss (22) is used in kernel PCA (Schoelkopf et al., 1999) (recall HDH = \u22122K(D)).", "startOffset": 60, "endOffset": 85}, {"referenceID": 7, "context": "Although (23) is frequently mentioned in the multidimensional scaling (MDS) literature (Cox & Cox, 2001), its use is rare since it cannot be tractably combined with rank (15) (Dattorro, 2012).", "startOffset": 175, "endOffset": 191}, {"referenceID": 23, "context": "These are the Sammon loss (26) (Sun et al., 2011; Lee & Verleysen, 2010a); the curvilinear components loss (27) (Sun et al.", "startOffset": 31, "endOffset": 73}, {"referenceID": 22, "context": ", 2011; Lee & Verleysen, 2010a); the curvilinear components loss (27) (Sun et al., 2010); the neighborhood indicator used in MVU and Isomap (28) (Weinberger et al.", "startOffset": 70, "endOffset": 88}, {"referenceID": 30, "context": ", 2010); the neighborhood indicator used in MVU and Isomap (28) (Weinberger et al., 2004; Tenenbaum et al., 2000); and the relaxed loss introduced in regularized MVU (29) (Weinberger et al.", "startOffset": 64, "endOffset": 113}, {"referenceID": 24, "context": ", 2010); the neighborhood indicator used in MVU and Isomap (28) (Weinberger et al., 2004; Tenenbaum et al., 2000); and the relaxed loss introduced in regularized MVU (29) (Weinberger et al.", "startOffset": 64, "endOffset": 113}, {"referenceID": 31, "context": ", 2000); and the relaxed loss introduced in regularized MVU (29) (Weinberger et al., 2007), respectively.", "startOffset": 65, "endOffset": 90}, {"referenceID": 23, "context": "The unnormalized entropy (30) was proposed in (Sun et al., 2011) to approximate the Sammon loss (26), whereas the reciprocal exponential Bregman divergence (31) was proposed in (Sun et al.", "startOffset": 46, "endOffset": 64}, {"referenceID": 22, "context": ", 2011) to approximate the Sammon loss (26), whereas the reciprocal exponential Bregman divergence (31) was proposed in (Sun et al., 2010) to approximate the curvilinear components loss (27) under w(d\u0302) = exp(d\u0302/\u03c3).", "startOffset": 120, "endOffset": 138}, {"referenceID": 8, "context": "The second loss (40) can be shown to be equivalent to local linear embedding (LLE) (Roweis & Saul, 2000; Saul & Roweis, 2003; Ham et al., 2004) if one tracks the solution in the limit as \u03c1\u2198 0.", "startOffset": 83, "endOffset": 143}, {"referenceID": 11, "context": "Interestingly, this loss-based framework generalizes probabilistic formulations (Lawrence, 2011), since e.", "startOffset": 80, "endOffset": 96}, {"referenceID": 9, "context": "Therefore, a general strategy for deriving maximal convex lower bounds on objective functions can be based on Fenchel bi-conjugation (Jojic et al., 2011).", "startOffset": 133, "endOffset": 153}, {"referenceID": 30, "context": "Putting \u03b3 = 0, Theorem 1 implies that the Fenchel biconjugate of (19) is exactly (17) (for Z = \u2212\u03b1I) if d > 0 and is (16) (for Z = \u03b2I) if d = 0, which might explain the success of MVU (Weinberger et al., 2004).", "startOffset": 183, "endOffset": 208}, {"referenceID": 23, "context": "However, many proposals exist for quantitatively evaluating the quality of different methods in a somewhat objective manner (Sun et al., 2011; Lee & Verleysen, 2010b; van der Maaten, 2009).", "startOffset": 124, "endOffset": 188}, {"referenceID": 25, "context": "5 Here we measure run time using a common implementation based on accelerated projected subgradient descent (Tseng, 2008).", "startOffset": 108, "endOffset": 121}], "year": 2012, "abstractText": "We demonstrate that almost all nonparametric dimensionality reduction methods can be expressed by a simple procedure: regularized loss minimization plus singular value truncation. By distinguishing the role of the loss and regularizer in such a process, we recover a factored perspective that reveals some gaps in the current literature. Beyond identifying a useful new loss for manifold unfolding, a key contribution is to derive new convex regularizers that combine distance maximization with rank reduction. These regularizers can be applied to any loss.", "creator": "LaTeX with hyperref package"}}}