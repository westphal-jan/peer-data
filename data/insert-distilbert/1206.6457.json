{"id": "1206.6457", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Exponential Regret Bounds for Gaussian Process Bandits with Deterministic Observations", "abstract": "this paper analyzes the problem of gaussian process ( gp ) filter bandits with deterministic observations. the analysis uses a branch and bound algorithm that is related to the ucb algorithm of ( srinivas et al, 2010 ). for gps with moderate gaussian observation noise, with variance strictly greater than zero, srinivas rao et - al proved that the regret vanishes at the approximate rate of $ o ( 1 / \\ sqrt { fixed t } ) $, where t is the number of observations. needed to complement their result, we attack the deterministic case and attain a much faster exponential convergence rate. under some regularity assumptions, we show consistency that the regret decreases asymptotically according to $ s o ( e ^ { - \\ frac { \\ tau t } { ( \\ ln t ) ^ { d / 4 } } } ) $ with high probability. here, d is the dimension of the search space and tau is set a constant that all depends on the behaviour of the objective function near its global maximum.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (295kb)", "http://arxiv.org/abs/1206.6457v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap witharXiv:1203.2177"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap witharXiv:1203.2177", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nando de freitas", "alexander j smola", "masrour zoghi"], "accepted": true, "id": "1206.6457"}, "pdf": {"name": "1206.6457.pdf", "metadata": {"source": "META", "title": "Exponential Regret Bounds for Gaussian Process Bandits with Deterministic Observations", "authors": ["Nando de Freitas", "Alex J. Smola"], "emails": ["nando@cs.ubc.ca", "alex@smola.org", "mzoghi@cs.ubc.ca"], "sections": [{"heading": null, "text": "( 1\u221a t ) , where t is the number of\nobservations. To complement their result, we attack the deterministic case and attain a much faster exponential convergence rate. Under some regularity assumptions, we show that the regret decreases asymp-\ntotically according to O ( e \u2212 \u03c4t (ln t)d/4 )\nwith high probability. Here, d is the dimension of the search space and \u03c4 is a constant that depends on the behaviour of the objective function near its global maximum."}, {"heading": "1. Introduction", "text": "Let f : D \u2192 R be a function on a compact subset D \u2286 Rd. We would like to address the global optimization problem\nxM = argmax x\u2208D f(x).\nLet us assume for the sake of simplicity that the objective function f has a unique global maximum\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\n(although it may have many local maxima).\nThe space D might be the set of free parameters that one could feed into a time-consuming algorithm or the locations where a sensor could be deployed, and the function f might be a measure of the performance of the algorithm (e.g. how long it takes to run). We refer the reader to (Moc\u030ckus, 1982; Schonlau et al., 1998; Gramacy et al., 2004; Brochu et al., 2007; Lizotte, 2008; Martinez\u2013Cantin et al., 2009; Garnett et al., 2010) for practical examples. In this paper, our assumption is that once the function has been probed at point x \u2208 D, then the value f(x) can be observed with very high precision. This is the case when the deployed sensors are very accurate or if the algorithm is deterministic. An example of this is the configuration of CPLEX parameters in mixed-integer programming (Hutter et al., 2010). More ambitiously, we might be interested in the simultaneous automatic configuration of an entire system (algorithms, architectures and hardware) whose performance is deterministic in terms of several free parameters and design choices.\nGlobal optimization is a difficult problem without any assumptions on the objective function f . The main complicating factor is the uncertainty over the extent of the variations of f , e.g. one could consider the characteristic function, which is equal to 1 at xM and 0 elsewhere, and none of the methods we mention here can optimize this function without exhaustively searching through every point in D.\nThe way a large number of global optimization methods address this problem is by imposing some prior assumption on how fast the objective function f can vary. The most explicit manifestation of this remedy is the imposition of a Lipschitz assumption\non f , which requires the change in the value of f(x), as the point x moves around, to be smaller than a constant multiple of the distance traveled by x (Hansen et al., 1992). As pointed out in (Bubeck et al., 2011, Figure 3), it is only important to have this kind of tight control over the function near its optimum: elsewhere in the space, we can have what they have dubbed a \u201cweak Lipschitz\u201d condition.\nOne way to relax these hard Lipschitz constraints is by putting a Gaussian Process (GP) prior on the function. Instead of restricting the function from oscillating too fast, a GP prior requires those fast oscillations to have low probability, cf. (Ghosal & Roy, 2006, Theorem 5).\nThe main point of these bounds (be they hard or soft) is to assist with the exploration-exploitation trade-off that global optimization algorithms have to grapple with. In the absence of any assumptions of convexity on the objective function, a global algorithm is forced to explore enough until it reaches a point in the process when with some degree of certainty it can localize its search space and perform local optimization (exploitation). Derivative bounds such as the ones discussed here together with the boundedness of the search space, guaranteed by the compactness assumption on D, provide us with such certainty by producing a useful upper bound that allows us to shrink the search space. This is illustrated in Figure 1. Suppose we know that our function is Lipschitz with constant L, then given sample points as shown in the figure, we can use the Lipschitz property to discard pieces of the search space. This is done by finding points in the search space where the function could not possibly be higher than the maximum value already encountered. Such points are found by placing cones at the sampled points with slope equal to L and checking where those cones lie below the maximum observed value.\nThis crude approach is wasteful because very often the slope of the function is much smaller than L. As shown in Figure 2), GPs do a better job of providing lower and upper bounds that can be used to limit the search space, by essentially choosing Lipschitz constants that vary over the search space and the algorithm run time.\nWe also assume that the objective function f is costly to evaluate. We would like to avoid probing f as much as possible and to get close to the optimum as quickly as possible. A solution to this problem is to approximate f with a surrogate function that provides a good upper bound for f and which is easier to calculate and optimize (Brochu et al.,\n2009). Surrogate functions also aid with global optimization by restricting the domain of interest. The surrogate that we will make extensive use of here is called the Upper Confidence Bound (UCB). It is defined to be \u00b5+B\u03c3, where \u00b5 and \u03c3 are the posterior predictive mean and standard deviation of the GP and B is a constant to be chosen by the algorithm. This surrogate function has been studied extensively in the literature and this paper relies heavily on the ideas put forth in the paper by Srinivas et al (Srinivas et al., 2010), in which the algorithm consists of repeated optimization of the UCB surrogate function after each sample. It must be noted however that our algorithm is distinctly different from their UCB algorithm.\nOne key difference between our setting and that of (Srinivas et al., 2010) is that, whereas we assume that the value of the function can be observed exactly, for the analysis presented in (Srinivas et al., 2010) it is necessary for the noise to be non-trivial (and Gaussian) because the main quantity that is used in the estimates, namely information gain, cf. (Srinivas et al., 2010, Equation 3), becomes undefined when the variance of the observation noise (\u03c32 in their notation) is set to 0, cf. the expression for I(yA; fA) that was given in the paragraph following Equation (3). So, our analysis is complementary to theirs. Of course, one could still use their algorithm in the noiseless setting, but their analytical results are inapplicable to that case. Moreover, we show that the regret, r(xt) = maxD f \u2212 f(xt), decreases according to O ( e \u2212 \u03c4t (ln t)d/4 )\n, implying that the cumulative regret is bounded from above.\nThe paper whose results are most similar to ours is (Munos, 2011), but there are some key differences in the methodology, analysis and obtained rates. For instance, we are interested in cumulative regret, whereas the results of (Munos, 2011) are proven for finite stop-time regret. In our case, the ideal application is the optimization of a function that is C2-smooth and has an unknown nonsingular Hessian at the maximum. We obtain a\nregret rate O ( e \u2212 \u03c4t (ln t)d/4 ) , whereas the DOO algo-\nrithm in (Munos, 2011) has regret rate O(e\u2212t) if the Hessian is known and the SOO algorithm has regret rate O(e\u2212 \u221a t) if the Hessian is unknown. In addition, the algorithms in (Munos, 2011) can handle functions that behave like \u2212c\u2016x \u2212 xM\u2016\u03b1 near the maximum (cf. Example 2 therein). Moreover, the hierarchical decomposition of the search space utilized by DOO and SOO makes them much more efficient in practice than the algorithm presented in\nthis paper: this is a shortcoming of our algorithm that we would like to remedy in the future.\nThis problem was also studied by (Vazquez & Bect, 2010) and (Bull, 2011), but using the Expected Improvement surrogate instead of UCB. Our methodology and results are different, but complementary to theirs."}, {"heading": "2. Gaussian process bandits", "text": ""}, {"heading": "2.1. Gaussian processes", "text": "As in (Srinivas et al., 2010), the objective function is distributed according to a Gaussian process prior:\nf(x) \u223c GP(m(\u00b7), \u03ba(\u00b7, \u00b7)). (1)\nFor convenience, and without loss of generality, we assume that the prior mean vanishes, i.e., m(\u00b7) = 0. There are many possible choices for the covariance kernel. One obvious choice is the anisotropic kernel \u03ba with a vector of known hyperparameters (Ras-\nmussen & Williams, 2006):\n\u03ba(xi, xj) = \u03ba\u0303 ( \u2212(xi \u2212 xj)>D(xi \u2212 xj) ) , (2)\nwhere \u03ba\u0303 is an isotropic kernel and D is a diagonal matrix with positive hyperparameters along the diagonal and zeros elsewhere. Our results apply to squared exponential kernels and Mate\u0301rn kernels with parameter \u03bd \u2265 2. In this paper, we assume that the hyperparameters are fixed and known in advance.\nWe can sample the GP at t points by choosing points x1:t := {x1, . . . , xt} and sampling the values of the function at these points to produce the vector f1:t = [f(x1) \u00b7 \u00b7 \u00b7 f(xt)]>. The function values are distributed according to a multivariate Gaussian distribution N (0,K), with covariance entries \u03ba(xi, xj). Assume that we already have several observations from previous steps, and that we want to decide what action xt+1 should be considered next. Let us denote the value of the function at this arbitrary new point as ft+1. Then, by the properties of GPs,\nf1:t and ft+1 are jointly Gaussian:[ f1:t ft+1 ] \u223c N ( 0, [ K k> k \u03ba(xt+1, xt+1) ]) ,\nwhere k = [\u03ba(xt+1, x1) \u00b7 \u00b7 \u00b7\u03ba(xt+1, xt)]>. Using the Schur complement, one arrives at an expression for the posterior predictive distribution:\nP (ft+1|x1:t+1, f1:t) = N (\u00b5t(xt+1), \u03c32t (xt+1)),\nwhere\n\u00b5t(xt+1) = k>K\u22121f1:t, \u03c32t (xt+1) = \u03ba(xt+1, xt+1)\u2212 k>K\u22121k\n(3)\nand f1:t = [f(x1) \u00b7 \u00b7 \u00b7 f(xt)]>."}, {"heading": "2.2. Surrogates for optimization", "text": "When it is assumed that the objective function f is sampled from a GP, one can use a combination of the posterior predictive mean and variance given by Equations (3) to construct surrogate functions, which tell us where to sample next. Here we use the UCB combination, which is given by\n\u00b5t(x) +Bt\u03c3t(x),\nwhere {Bt}\u221et=1 is a sequence of numbers specified by the algorithm. This surrogate trades-off exploration and exploitation since it is optimized by choosing points where the mean is high (exploitation) and where the variance is large (exploration). Since the surrogate has an analytical expression that is easy to evaluate, it is much easier to optimize than the original objective function. Other popular surrogate functions constructed using the sufficient statistics of the GP include the Probability of Improvement, Expected Improvement and Thompson sampling. We refer the reader to (Brochu et al., 2009; May et al., 2010; Hoffman et al., 2011) for details on these."}, {"heading": "2.3. Our algorithm", "text": "The main idea of our algorithm (Algorithm 1) is to tighten the bound on f given by the UCB surrogate function by sampling the search space more and more densely and shrinking this space as more and more of the UCB surrogate function is \u201csubmerged\u201d under the maximum of the Lower Confidence Bound (LCB). Figure 2 illustrates this intuition.\nMore specifically, the algorithm consists of two iterative stages. During the first stage, the function is sampled at enough points in L (the red crosses in Figure 3) until every point in the search space is\ncontained inside a simplex of diameter \u03b4, where by the diameter of a set we mean the maximum length between any pair of points in the set. In the second stage, the search space is shrunk to discard regions where the maximum is very unlikely to reside. Such regions are obtained by finding points where the UCB is lower than the LCB (the complement of the colored region in the same panel as before). The remaining set of relevant points is denoted by R\u0303. To simplify the task of shrinking the search space, we simply find an enclosing ball, which is denoted by R in Algorithm 1. Back to the first stage, we consider a lattice that is twice as dense as in the first stage of the previous iteration, but we only sample at points that lie within our new smaller search space.\nIn the second stage, the auxiliary step of approximating the relevant set R\u0303 with the ball R introduces inefficiencies in the algorithm, since we only need to sample inside R\u0303. This can be easily remedied in practice to obtain an efficient algorithm. Our analysis will show that even without these improvements it is already possible to obtain very strong exponential convergence rates. Of course, practical improvement will result in better constants and ought to be considered seriously.\nNote that Algorithm 1 terminates once the relevant region becomes too small to intersect the lattice L. Our analysis requires for the algorithm to sample\nAlgorithm 1 Branch and Bound\nInput: A compact subset D \u2286 Rd, a function f : D \u2192 R and a discrete lattice L \u2286 D that is divisible by powers of 2. Set R \u2190 D and \u03b4 \u2190 1. repeat\nSample Twice as Densely: \u2022 \u03b4 \u2190 \u03b4/2. \u2022 Sample f at enough points in L so that every point in R is contained in a simplex of diameter \u03b4. Shrink the Relevant Region: \u2022 Set\nR\u0303 := { x \u2208 R \u2223\u2223\u2223\u2223\u00b5T (x) +\u221a\u03b2T\u03c3T (x) > sup R \u00b5T (x)\u2212 \u221a \u03b2T\u03c3T (x) } .\nT is the number points sampled so far and \u03b2T = 2 ln ( |L|T 2 \u03b1 ) = 4 lnT + 2 ln |L|\u03b1 with \u03b1 \u2208 (0, 1).\n\u2022 Solve the following constrained optimization problem: (x\u22171, x\u22172) = argsup(x1,x2)\u2208 eR\u00d7 eR \u2016x1 \u2212 x2\u2016. \u2022 R \u2190 B ( x\u22171 + x \u2217 2\n2 , \u2016x\u22171 \u2212 x\u22172\u2016\n) , where B(p, r) is the ball of radius r centred around p.\nuntil R\u2229 L = \u2205\npoints from a fixed finite set of points, although we can pick L to be the set of all points in D with floating point coordinates."}, {"heading": "3. Analysis", "text": "We begin our analysis by showing that, given sufficient explored locations, the posterior predictive variance is small. Specifically, the following approximation result is proved in the supplementary material:\nProposition 1 (Variance Bound) Let \u03ba : Rd \u00d7 Rd \u2192 R be a kernel that is four times differentiable along the diagonal {(x, x) |x \u2208 Rd}, with Q defined as in part 2 of Lemma 5, and f \u223c GP (0, \u03ba(\u00b7, \u00b7)) a sample from the corresponding GP. If f is sampled at points x1:T = {x1, . . . , xT } that form a \u03b4-cover of a subset D \u2286 Rd, then the resulting posterior predictive standard deviation \u03c3T satisfies\nsup D \u03c3T \u2264\nQ\u03b42\n4 ."}, {"heading": "3.1. Finiteness of regret", "text": "Having shown that the variance vanishes according to the square of the resolution of the lattice of sampled points, we now move on to show that this estimate implies an exponential asymptotic vanishing of the regret encountered by our Branch and Bound algorithm. This is laid out in our main theorem stated below and proven in the supplementary material.\nRecall that D \u2286 Rd is assumed to be a nonempty compact subset and f a sample from the\nGP GP (0, \u03ba(\u00b7, \u00b7)) on D. Moreover, in what follows we will denote the global maximum by xM := argmax x\u2208D f(x) and the regret by r(xt) = f(xM ) \u2212 f(xt). Also, by convention, for any set S, we will denote its interior by S\u25e6, its boundary by \u2202S and if S is a subset of Rd, then conv(S) will denote its convex hull. The following holds true:\nTheorem 2 Suppose we are given: 1. \u03b1 > 0, a compact subset D \u2286 Rd, and \u03ba a kernel\non Rd that is four times differentiable along the diagonal;\n2. f \u223c GP(0, \u03ba) a continuous sample on D that has a unique global maximum xM , which satisfies one of the following two conditions: (\u2020) xM \u2208 D\u25e6 and f(xM ) \u2212 c1\u2016x \u2212 xM\u20162 <\nf(x) \u2264 f(xM )\u2212 c2\u2016x\u2212 xM\u20162 for all x satisfying x \u2208 B(xM , \u03c10) for some \u03c10 > 0; (\u2021) xM \u2208 \u2202D and both f and \u2202D are smooth at xM , with \u2207f(xM ) 6= 0;\n3. any lattice L \u2286 D satisfying the following two conditions\n\u2022 2L \u2229 conv(L) \u2286 L (4)\n\u2022 2d\u2212 log2 \u03c10 diam(D)e+1L \u2229 L 6= \u2205 (5) if f satisfies (\u2020)\nThen, there exist positive numbers A and \u03c4 and an integer T such that the points specified by the Branch and Bound algorithm, {xt}, will satisfy the following asymptotic bound: For all t > T , with probability 1\u2212 \u03b1 we have\nr(xt) < Ae \u2212 \u03c4t (ln t)d/4 .\nGiven the exponential rate of convergence we obtain in Theorem 2, we have the following finiteness conclusion for the cumulative regret accrued by our Branch and Bound algorithm:\nCorollary 3 Given \u03ba, f \u223c GP(0, \u03ba) and L \u2286 D as in Theorem 2, the cumulative regret is bounded from above.\nRemark 4 It is worth pointing out the trivial observation that using a simple UCB algorithm with monotonically increasing and unbounded factor \u221a \u03b2t, without any shrinking of the search space as we do here, necessarily leads to unbounded cumulative regret since eventually \u221a \u03b2t becomes large enough so that at points x\u2032 far away from the maximum,\u221a \u03b2t\u03c3t(x\u2032) becomes larger than f(xM ) \u2212 f(x). In fact, eventually the UCB algorithm will sample every point in the lattice L."}, {"heading": "3.2. Remarks on the main theorem", "text": "This section includes a discussion of the assumptions placed on the objective function in Theorem 2 as well as an outline of the proof, the full details of which are included in the appendix."}, {"heading": "3.2.1. On the statement of Theorem 2", "text": "A few remarks on the assumptions and the conclusion of the main theorem are in order:\nA. Relationship between the local and global assumptions on f : The theorem has two seemingly unrelated restrictions on the function f : the global GP prior and the local behaviour near the global maximum. However, in many circumstances of interest, the local condition follows almost surely from the global condition. Two such circumstances are if \u03ba is a Mate\u0301rn kernel with \u03bd > 2 (including the squared exponential kernel) or if \u03ba is six times differentiable. In either case, the sample f is twice differentiable almost surely, in the former case by (Adler & Taylor, 2007, Theorem 1.4.2) and (Stein, 1999, \u00a72.6)) and in the latter situation by (Ghosal & Roy, 2006, Theorem 5). If the global maximum xM lies in the interior of D, the Hessian of f at xM will almost surely be non-singular since the vanishing of at least one of the eigenvalues of the Hessian is a codimension 1 condition in the space of all functions that are smooth at a given point, hence justifying condition (\u2020).\nOn the other hand, if xM lies on the boundary of D, then condition (\u2021) will be satisfied almost surely, since the additional event of the vanishing\nof \u2207f(xM ) is a codimension d phenomenon in the space of functions with global maximum at xM .\nB. Uniqueness of the global maximum: A randomly drawn continuous sample from a GP on a compact domain will almost surely have a unique global maximum: this is because the space of continuous functions on a compact domain that attain their global maximum at more than one point have codimension one in the space of all continuous functions on that domain.\nC. Assumptions on L: The two conditions (4) and (5) simply require that the lattice be \u201cdivisible by 2\u201d and that it be fine enough so that the algorithm can sample inside the ball B(xM , \u03c10) when the maximum of the function is located in the interior of the search space D. One can simply choose L to be the set of points in D that have floating point coordinates: it\u2019s just the points at which the algorithm is allowed to sample the function.\nD. On \u03c4 \u2019s dependence: Finally, it is important to point out that the decay rate \u03c4 does not depend on the choice of the lattice L, even though as stated, the statement of the theorem chooses \u03c4 only after L is specified. The theorem was written this way simply for the sake of readability."}, {"heading": "3.2.2. Outline of the proof of Theorem 2", "text": "The starting point for the proof is the observation that one can use the posterior predictive mean and standard deviation of the GP to obtain a high probability envelope around the objective function (cf. Lemma 8 in the appendix). Given the fact that the thickness of this envelope is determined by the height of the posterior predictive standard deviation, \u03c3, we can use the bound given by Proposition 1 to show that asymptotically one can rapidly dispense with large portions of the search space, as illustrated in Figure 2.\nOne disconcerting component of Algorithm 1 is the step that requires sampling twice as densely in each iteration, since the number of samples can start to grow exponentially, hence killing any hope of obtaining exponentially decreasing regret. However, this is where the assumption on the local behaviour near the global maximum becomes relevant. Since Proposition 1 tells us that every time the function is sampled twice as densely, \u03c3 decreases by a factor of 4, and given our assumption that the function has quadratic behaviour near the global maximum, we can conclude that the radius of the search space is halved after each iteration and so the number of sam-\npled points added in each iteration roughly remains constant. Of course, this assumes that the multiplicative factor \u221a \u03b2t remains constant in this process. However, the algorithm requires \u221a \u03b2t to grow logarithmically, and so to fill this gap we need to bound the growth of \u221a \u03b2t, which is tied to the number of samples needed in each iteration of the algorithm, which in turn is linked to the resolution of the lattice of sampled points \u03b4 and the size of the relevant set R, which in turn depends on the size of \u221a \u03b2t\u03c3t. This circular dependence gives rise to a difference equation, whose solutions we bound by solving the corresponding differential equation."}, {"heading": "3.2.3. Further remarks on the GP prior", "text": "Let us step back for a moment and pose the question of whether it would be possible to carry out a similar line of reasoning under other circumstances. To answer this, one needs to identify the key ingredients of the proof, which are the following:\nA. A mechanism for calculating a high probability envelope around the objective function (cf. Lemma 8);\nB. An estimate showing that the thickness of the envelope diminishes rapidly as the function is sampled more and more densely (cf. Proposition 1), so that the search space can be shrunk under reasonable assumptions on the behaviour of the function near the peak.\nThe reason for our imposing a GP prior on f is that it gives us property A, while our smoothness assumption on the kernel guarantees property B. However, GPs are but one way one could obtain these properties and they do this essentially by coming up with local estimates of the Lipschitz constant based on the observed values of the objective function nearby. Perhaps one could explicitly incorporate similar local estimates on the Lipschitz constant into tree based approaches like HOO and SOO, cf. (Bubeck et al., 2011) and (Munos, 2011), in which case one would be able to dispense with the GP assumption and get similar performance. But, that is beyond the scope of this paper and will be left for future work."}, {"heading": "4. Discussion", "text": "In this paper we proposed a modification of the UCB algorithm of (Srinivas et al., 2010) which addresses the noise free case. The key difference is that while the original algorithm achieves an O(t\u2212 1 2 ) rate of\nconvergence to the regret minimizer, we obtain an exponential rate in the number of function evaluations. In other words, the noise free problem is significantly easier, statistically speaking, than the noisy case. The key difference is that we need not invest any samples in noise reduction to determine whether our observations deviate far from their expectation.\nThis allows us to discard pieces of the search space where the maximum is very unlikely to be, when compared to (Srinivas et al., 2010). We show that this additional step leads to a considerable improvement of the regret accrued by the algorithm. In particular, the cumulative regret obtained by our Branch and Bound algorithm is bounded from above, whereas the cumulative regret bound obtained in the noisy bandit algorithm is unbounded. The possibility of dispensing with chunks of the search space can also be seen in the works involving hierarchical partitioning, e.g. (Munos, 2011), where regions of the space are deemed as less worthy of probing as time goes on.\nOur results mirror the observation in active learning that noise free and large margin learning of half spaces can be achieved much more rapidly than identifying a linear separator in the noisy case (Bshouty & Wattad, 2006; Dasgupta et al., 2009). This is also reflected in classical uniform convergence results for supervised learning (Audibert & Tsybakov, 2007; Vapnik, 1998) where the achievable rate depends on the decay of probability mass near the margin.\nThis suggests that the ability to extend our results to the noisy case is somewhat limited. An indication of what might be possible can be found in (Balcan et al., 2009), where regions of the version space are eliminated once they can be excluded with sufficiently high probability. One could model a corresponding Branch and Bound algorithm, which dispenses with points that lie outside the current (or perhaps the previous) relevant set when calculating the covariance matrix K in the posterior equations (3). Analysis of how much of an effect such a computational cost-cutting measure would have on the regret encountered by the algorithm is a subject of future research."}, {"heading": "Acknowledgements", "text": "We are very grateful to the anonymous reviewers for outstanding feedback. This research was supported by NSERC and the Institute for Computing, Information and Cognitive Systems (ICICS) at UBC."}], "references": [{"title": "Fast learning rates for plug-in classifiers", "author": ["Audibert", "J.-Y", "A.B. Tsybakov"], "venue": "Annals of Statistics,", "citeRegEx": "Audibert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2007}, {"title": "Agnostic active learning", "author": ["Balcan", "M.-F", "A. Beygelzimer", "J. Langford"], "venue": "J. Comput. Syst. Sci,", "citeRegEx": "Balcan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2009}, {"title": "Active preference learning with discrete choice data", "author": ["E. Brochu", "N. de Freitas", "A. Ghosh"], "venue": "In NIPS, pp", "citeRegEx": "Brochu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2007}, {"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. de Freitas"], "venue": "Technical Report TR-2009-023,", "citeRegEx": "Brochu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Brochu et al\\.", "year": 2009}, {"title": "On exact learning halfspaces with random consistent hypothesis oracle", "author": ["N.H. Bshouty", "E. Wattad"], "venue": "In International Conference on Algorithmic Learning Theory, pp", "citeRegEx": "Bshouty and Wattad,? \\Q2006\\E", "shortCiteRegEx": "Bshouty and Wattad", "year": 2006}, {"title": "Convergence rates of efficient global optimization algorithms", "author": ["A.D. Bull"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bull,? \\Q2011\\E", "shortCiteRegEx": "Bull", "year": 2011}, {"title": "Analysis of perceptron-based active learning", "author": ["S. Dasgupta", "A.T. Kalai", "C. Monteleoni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dasgupta et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2009}, {"title": "Bayesian optimization for sensor set selection", "author": ["R. Garnett", "M.A. Osborne", "S.J. Roberts"], "venue": "In ACM/IEEE International Conference on Information Processing in Sensor Networks,", "citeRegEx": "Garnett et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Garnett et al\\.", "year": 2010}, {"title": "Posterior consistency of Gaussian process prior for nonparametric binary regression", "author": ["S. Ghosal", "A. Roy"], "venue": "Ann. Stat.,", "citeRegEx": "Ghosal and Roy,? \\Q2006\\E", "shortCiteRegEx": "Ghosal and Roy", "year": 2006}, {"title": "Parameter space exploration with Gaussian process trees", "author": ["R.B. Gramacy", "H.K.H. Lee", "W. MacReady"], "venue": "In ICML, pp", "citeRegEx": "Gramacy et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gramacy et al\\.", "year": 2004}, {"title": "Global optimization of univariate Lipschitz functions: I. survey and properties", "author": ["P. Hansen", "B. Jaumard", "S. Lu"], "venue": "Mathematical Programming,", "citeRegEx": "Hansen et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1992}, {"title": "Portfolio allocation for Bayesian optimization", "author": ["M. Hoffman", "E. Brochu", "N. de Freitas"], "venue": "In UAI, pp", "citeRegEx": "Hoffman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2011}, {"title": "Automated configuration of mixed integer programming solvers", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "In Proceedings of CPAIOR-10,", "citeRegEx": "Hutter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2010}, {"title": "Practical Bayesian Optimization", "author": ["D. Lizotte"], "venue": "PhD thesis, University of Alberta,", "citeRegEx": "Lizotte,? \\Q2008\\E", "shortCiteRegEx": "Lizotte", "year": 2008}, {"title": "Optimistic Bayesian sampling in contextual-bandit problems", "author": ["B. May", "N. Korda", "A. Lee", "D. Leslie"], "venue": null, "citeRegEx": "May et al\\.,? \\Q2010\\E", "shortCiteRegEx": "May et al\\.", "year": 2010}, {"title": "The Bayesian approach to global optimization", "author": ["J. Mo\u010dkus"], "venue": "In System Modeling and Optimization,", "citeRegEx": "Mo\u010dkus,? \\Q1982\\E", "shortCiteRegEx": "Mo\u010dkus", "year": 1982}, {"title": "Optimistic optimization of a deterministic function without the knowledge of its smoothness", "author": ["R. Munos"], "venue": "In NIPS,", "citeRegEx": "Munos,? \\Q2011\\E", "shortCiteRegEx": "Munos", "year": 2011}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Global versus local search in constrained optimization of computer models", "author": ["M. Schonlau", "W.J. Welch", "D.R. Jones"], "venue": "Lecture Notes-Monograph Series,", "citeRegEx": "Schonlau et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Schonlau et al\\.", "year": 1998}, {"title": "Gaussian process optimization in the bandit setting: No regret and experimental design", "author": ["N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger"], "venue": "In ICML,", "citeRegEx": "Srinivas et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2010}, {"title": "Interpolation of Spatial Data: Some Theory for Kriging", "author": ["M.L. Stein"], "venue": null, "citeRegEx": "Stein,? \\Q1999\\E", "shortCiteRegEx": "Stein", "year": 1999}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1998\\E", "shortCiteRegEx": "Vapnik", "year": 1998}, {"title": "Convergence properties of the expected improvement algorithm with fixed mean and covariance functions", "author": ["E. Vazquez", "J. Bect"], "venue": "J. of Statistical Planning and Inference,", "citeRegEx": "Vazquez and Bect,? \\Q2010\\E", "shortCiteRegEx": "Vazquez and Bect", "year": 2010}], "referenceMentions": [{"referenceID": 19, "context": "The analysis uses a branch and bound algorithm that is related to the UCB algorithm of (Srinivas et al., 2010).", "startOffset": 87, "endOffset": 110}, {"referenceID": 19, "context": "For GPs with Gaussian observation noise, with variance strictly greater than zero, (Srinivas et al., 2010) proved that the regret vanishes at the approximate", "startOffset": 83, "endOffset": 106}, {"referenceID": 15, "context": "We refer the reader to (Mo\u010dkus, 1982; Schonlau et al., 1998; Gramacy et al., 2004; Brochu et al., 2007; Lizotte, 2008; Martinez\u2013Cantin et al., 2009; Garnett et al., 2010) for practical examples.", "startOffset": 23, "endOffset": 170}, {"referenceID": 18, "context": "We refer the reader to (Mo\u010dkus, 1982; Schonlau et al., 1998; Gramacy et al., 2004; Brochu et al., 2007; Lizotte, 2008; Martinez\u2013Cantin et al., 2009; Garnett et al., 2010) for practical examples.", "startOffset": 23, "endOffset": 170}, {"referenceID": 9, "context": "We refer the reader to (Mo\u010dkus, 1982; Schonlau et al., 1998; Gramacy et al., 2004; Brochu et al., 2007; Lizotte, 2008; Martinez\u2013Cantin et al., 2009; Garnett et al., 2010) for practical examples.", "startOffset": 23, "endOffset": 170}, {"referenceID": 2, "context": "We refer the reader to (Mo\u010dkus, 1982; Schonlau et al., 1998; Gramacy et al., 2004; Brochu et al., 2007; Lizotte, 2008; Martinez\u2013Cantin et al., 2009; Garnett et al., 2010) for practical examples.", "startOffset": 23, "endOffset": 170}, {"referenceID": 13, "context": "We refer the reader to (Mo\u010dkus, 1982; Schonlau et al., 1998; Gramacy et al., 2004; Brochu et al., 2007; Lizotte, 2008; Martinez\u2013Cantin et al., 2009; Garnett et al., 2010) for practical examples.", "startOffset": 23, "endOffset": 170}, {"referenceID": 7, "context": "We refer the reader to (Mo\u010dkus, 1982; Schonlau et al., 1998; Gramacy et al., 2004; Brochu et al., 2007; Lizotte, 2008; Martinez\u2013Cantin et al., 2009; Garnett et al., 2010) for practical examples.", "startOffset": 23, "endOffset": 170}, {"referenceID": 12, "context": "An example of this is the configuration of CPLEX parameters in mixed-integer programming (Hutter et al., 2010).", "startOffset": 89, "endOffset": 110}, {"referenceID": 10, "context": "on f , which requires the change in the value of f(x), as the point x moves around, to be smaller than a constant multiple of the distance traveled by x (Hansen et al., 1992).", "startOffset": 153, "endOffset": 174}, {"referenceID": 3, "context": "A solution to this problem is to approximate f with a surrogate function that provides a good upper bound for f and which is easier to calculate and optimize (Brochu et al., 2009).", "startOffset": 158, "endOffset": 179}, {"referenceID": 19, "context": "This surrogate function has been studied extensively in the literature and this paper relies heavily on the ideas put forth in the paper by Srinivas et al (Srinivas et al., 2010), in which the algorithm consists of repeated optimization of the UCB surrogate function after each sample.", "startOffset": 155, "endOffset": 178}, {"referenceID": 19, "context": "One key difference between our setting and that of (Srinivas et al., 2010) is that, whereas we assume that the value of the function can be observed exactly, for the analysis presented in (Srinivas et al.", "startOffset": 51, "endOffset": 74}, {"referenceID": 19, "context": ", 2010) is that, whereas we assume that the value of the function can be observed exactly, for the analysis presented in (Srinivas et al., 2010) it is necessary for the noise to be non-trivial (and Gaussian) because the main quantity that is used in the estimates, namely information gain, cf.", "startOffset": 121, "endOffset": 144}, {"referenceID": 16, "context": "The paper whose results are most similar to ours is (Munos, 2011), but there are some key differences in the methodology, analysis and obtained rates.", "startOffset": 52, "endOffset": 65}, {"referenceID": 16, "context": "For instance, we are interested in cumulative regret, whereas the results of (Munos, 2011) are proven for finite stop-time regret.", "startOffset": 77, "endOffset": 90}, {"referenceID": 16, "context": "regret rate O ( e \u2212 \u03c4t (ln t)d/4 ) , whereas the DOO algorithm in (Munos, 2011) has regret rate O(e\u2212t) if the Hessian is known and the SOO algorithm has regret rate O(e\u2212 \u221a ) if the Hessian is unknown.", "startOffset": 66, "endOffset": 79}, {"referenceID": 16, "context": "In addition, the algorithms in (Munos, 2011) can handle functions that behave like \u2212c\u2016x \u2212 xM\u2016 near the maximum (cf.", "startOffset": 31, "endOffset": 44}, {"referenceID": 5, "context": "This problem was also studied by (Vazquez & Bect, 2010) and (Bull, 2011), but using the Expected Improvement surrogate instead of UCB.", "startOffset": 60, "endOffset": 72}, {"referenceID": 19, "context": "As in (Srinivas et al., 2010), the objective function is distributed according to a Gaussian process prior:", "startOffset": 6, "endOffset": 29}, {"referenceID": 3, "context": "We refer the reader to (Brochu et al., 2009; May et al., 2010; Hoffman et al., 2011) for details on these.", "startOffset": 23, "endOffset": 84}, {"referenceID": 14, "context": "We refer the reader to (Brochu et al., 2009; May et al., 2010; Hoffman et al., 2011) for details on these.", "startOffset": 23, "endOffset": 84}, {"referenceID": 11, "context": "We refer the reader to (Brochu et al., 2009; May et al., 2010; Hoffman et al., 2011) for details on these.", "startOffset": 23, "endOffset": 84}, {"referenceID": 16, "context": ", 2011) and (Munos, 2011), in which case one would be able to dispense with the GP assumption and get similar performance.", "startOffset": 12, "endOffset": 25}, {"referenceID": 19, "context": "In this paper we proposed a modification of the UCB algorithm of (Srinivas et al., 2010) which addresses the noise free case.", "startOffset": 65, "endOffset": 88}, {"referenceID": 19, "context": "This allows us to discard pieces of the search space where the maximum is very unlikely to be, when compared to (Srinivas et al., 2010).", "startOffset": 112, "endOffset": 135}, {"referenceID": 16, "context": "(Munos, 2011), where regions of the space are deemed as less worthy of probing as time goes on.", "startOffset": 0, "endOffset": 13}, {"referenceID": 6, "context": "Our results mirror the observation in active learning that noise free and large margin learning of half spaces can be achieved much more rapidly than identifying a linear separator in the noisy case (Bshouty & Wattad, 2006; Dasgupta et al., 2009).", "startOffset": 199, "endOffset": 246}, {"referenceID": 21, "context": "This is also reflected in classical uniform convergence results for supervised learning (Audibert & Tsybakov, 2007; Vapnik, 1998) where the achievable rate depends on the decay of probability mass near the margin.", "startOffset": 88, "endOffset": 129}, {"referenceID": 1, "context": "An indication of what might be possible can be found in (Balcan et al., 2009), where regions of the version space are eliminated once they can be excluded with sufficiently high probability.", "startOffset": 56, "endOffset": 77}], "year": 2012, "abstractText": "This paper analyzes the problem of Gaussian process (GP) bandits with deterministic observations. The analysis uses a branch and bound algorithm that is related to the UCB algorithm of (Srinivas et al., 2010). For GPs with Gaussian observation noise, with variance strictly greater than zero, (Srinivas et al., 2010) proved that the regret vanishes at the approximate rate of O ( 1 \u221a t ) , where t is the number of observations. To complement their result, we attack the deterministic case and attain a much faster exponential convergence rate. Under some regularity assumptions, we show that the regret decreases asymptotically according to O ( e \u2212 \u03c4t (ln t)d/4 ) with high probability. Here, d is the dimension of the search space and \u03c4 is a constant that depends on the behaviour of the objective function near its global maximum.", "creator": "LaTeX with hyperref package"}}}