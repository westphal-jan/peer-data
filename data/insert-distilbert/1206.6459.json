{"id": "1206.6459", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Bayesian Conditional Cointegration", "abstract": "cointegration relativity is an important topic for time - series, and describes a relationship between two series in which supposedly a linear combination is stationary. classically, the test for temporal cointegration is based on a two _ stage process in which first the linear relation between the resulting series is estimated by ordinary discrete least squares. subsequently a unit root test is performed on the residuals. a frequently well - known deficiency of this classical approach constraint is that it can lead to erroneous conclusions about the presence of cointegration. as an alternative, we present a framework for estimating whether cointegration exists using bayesian inference formulas which is empirically superior to the classical approach. finally, we apply our technique to model segmented cointegration in which cointegration may exist only for limited time. in contrast to previous approaches our model makes no rigorous restriction on the number of possible cointegration segments.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (426kb)", "http://arxiv.org/abs/1206.6459v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CE cs.LG stat.ME", "authors": ["chris bracegirdle", "david barber"], "accepted": true, "id": "1206.6459"}, "pdf": {"name": "1206.6459.pdf", "metadata": {"source": "META", "title": "Bayesian Conditional Cointegration", "authors": ["Chris Bracegirdle", "David Barber"], "emails": ["c.bracegirdle@cs.ucl.ac.uk", "d.barber@cs.ucl.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Cointegration is an important concept in time-series analysis and relates to the fact that the differences between two time-series may be more predictable than any individual time-series itself. We make two contributions to this area\u2014first, we formulate determining whether two series are cointegrated as an instance of Bayesian inference in an associated probabilistic model. This enables us to bring modern statistical and machine learning techniques to the analysis of cointegration and also provides a useful conceptual framework to describe cointegration. The resulting regression estimation algorithm is shown to be more robust than least-squares estimation to spurious results. This is we believe an\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nimportant result that establishes a new and coherent approach to the long-standing problem of inconsistency in establishing cointegration. Second, in practice two series may only be intermittently cointegrated\u2014that is, they are only cointegrated over shorter segments of time. To date the identification of these segments has been attempted with rather limiting assumptions such as the presence of either only two or three segments (Gregory & Hansen, 1996; Hatemi-J, 2008). To address this we phrase the problem as inference in a corresponding changepoint model, placing no limitation on the number of segments. We develop an efficient specialised inference scheme for this model and demonstrate its practical applicability on real-world problems."}, {"heading": "1.1. Cointegration", "text": "Whilst individual time-series may not be predictable, the relationships between two time-series may be more predictable. For example, series x1:T , y1:T formed by\nxt+1 = xt + t+1, yt+1 = yt + t+1, t \u223c N (0, 1)\nboth follow an unpredictable random walk. However, the future difference xt+1 \u2212 yt+1 = xt \u2212 yt, is perfectly predictable given knowledge of the current difference. In the economics and finance literature such instances are common; for example underlying mechanisms such as limited money supply may forge dependencies between time-series (Dickey et al., 1991). It is of significant interest in finance to find pairs of asset price series that are cointegrated\u2014such estimation underpins one of the classical statistical arbitrage strategies known as \u2018pairs trading\u2019.\nFrom an individual series x1:T we can form a new series x12:T by taking the difference x 1 t = xt \u2212 xt\u22121. Repeating this differencing process d times, a series is order d integrated, written I(d), if the series xdt formed from repeatedly taking the difference d times yields a stationary series I(0). In our example above, both x and y are each I(1) and hence integrated. As we showed, there is a linear combination that is I(0). More generally, two series x1:T and y1:T are cointegrated\nif they are each individually integrated and a linear combination of the two is integrated with a lower order.\nFor our purposes, we define xt and yt to be cointegrated within a specified time segment if there is a linear regression relationship between the two variables that forms a stationary process. For the most simple case of a single regressor, we write such a relationship as\nyt = \u03b1+ \u03b2xt + t t = \u03c6 t\u22121 + \u03b7t, \u03b7t \u223c N ( 0, \u03c32 ) , |\u03c6| < 1\nwhere \u03b1 represents a constant, and \u03b2 the regression coefficient. Here 1:T forms a mean-reverting, stationary process. For brevity, we focus attention on estimation in this model, however more flexible cases can easily be considered, including the case with a trend coefficient \u03b3 in which the regression is written\nyt = \u03b1+ \u03b2xt + \u03b3t+ t\nwith the same autoregression for t. Adapting our technique to more complex models with exogenous variables, and to the vector case, is also possible.\nTesting for and estimating a cointegration relationship is classically a two-step process (Granger, 1986). Firstly, the regression equation is estimated based on a simple ordinary least squares (OLS) fit to minimise\u2211 t(yt\u2212\u03b1\u2212\u03b2xt)2 (which we show is equivalent to maximum likelihood (ML) parameter optimisation assuming that \u03c6 = 0 in a corresponding model). Subsequently, a test for a unit root in the residuals is performed using the Dickey-Fuller test, see for example Harris & Sollis (2003), which tests the hypothesis that \u03c6 = 1 against the alternative |\u03c6| < 1 (for |\u03c6| < 1 the series is stationary and non-stationary otherwise). In the case that \u03c6 = 1, it is well-known that OLS can deliver a spurious regression (Granger & Newbold, 1974), and this problem is not limited to cointegration. The classical approach is conceptually undesirable since it makes strong, potentially conflicting assumptions about the data: in the regression part, the residuals are effectively assumed uncorrelated whilst in the subsequent unit root test, they may be determined not to be (under the null hypothesis of the test, the regression was spurious). In this work we relax the assumptions placed on \u03c6 during parameter estimation to reduce the impact of any conflicting assumptions."}, {"heading": "2. Modelling Cointegration", "text": "Our approach is to form a generative model of observations p(y1:T x1:T , \u03b8), where \u03b8 = { \u03b1, \u03b2, \u03c32, \u03c6 } . First,\nwe form a generative model on observations1 y1:T and latent variables 1:T\np(y1:T , 1:T x1:T ) = \u220f t p(yt xt, t) p( t t\u22121) , 0 = \u2205\nwhere2\np(yt xt, t) = \u03b4(yt \u2212 \u03b1\u2212 \u03b2xt \u2212 t)\nand the transition for t is given as p( t t\u22121) = N ( t \u03c6 t\u22121, \u03c3 2 ) .\nThe belief network (see Barber (2012) for an introduction) for this model is given in figure 1. The likelihood on the observations y1:T is given by\np(y1:T x1:T ) = \u222b 1:T p(y1:T , 1:T x1:T ) .\nThe integration distributes,\np(y1:T x1:T ) = \u222b 1:T\u22121 T\u22121\u220f t=1 p(yt xt, t) p( t t\u22121)\n\u00d7 \u222b T p(yT xT , T ) p( T T\u22121)\nand since we have a delta function, the final term evaluates to N ( yT \u2212 \u03b1\u2212 \u03b2xT \u03c6 T\u22121, \u03c32 ) . Iterating this yields a product of Gaussian terms\nT\u220f t=2 N ( yt \u2212 \u03b1\u2212 \u03b2xt \u03c6 (yt\u22121 \u2212 \u03b1\u2212 \u03b2xt\u22121) , \u03c32 ) .\nFinally, applying labels according to t = yt \u2212 \u03b1\u2212 \u03b2xt,\np(y1:T x1:T ) = p( 1) T\u220f t=2 N ( t \u03c6 t\u22121, \u03c3 2 ) = p( 1:T ) .\nHence the likelihood p(y1:T x1:T ) is equivalent to the likelihood on the Markov chain with \u2018observations\u2019 t shown in figure 2. The log likelihood log p( 1:T ) is\nlog p( 1)\u2212 1\n2\u03c32 T\u2211 t=2 ( t \u2212 \u03c6 t\u22121)2 \u2212 T \u2212 1 2 log 2\u03c0\u03c32\n1Note that we make no assumption about the underlying process x; we work with the conditional relationship p(y1:T x1:T ) not the joint p(x1:T , y1:T ). As far as we are aware previous Bayesian approaches to cointegration make stronger assumptions about the underlying process, see for example Koop et al. (2006).\n2The Dirac delta distribution \u03b4(x\u2212 a) represents a degenerate probability density function with the property that\u222b x \u03b4(x\u2212 a) f(x) = f(a) .\nand when \u03c6 = 0, maximising this degenerates to minimising the sum of squared residual terms\nT\u2211 t=2 2t = T\u2211 t=2 (yt \u2212 \u03b1\u2212 \u03b2xt)2 .\nOLS estimation for the regression parameters \u03b1 and \u03b2 therefore corresponds to the ML solution of this model, on the assumption that \u03c6 = 0. In the case that \u03c6 is non-zero, a different solution may be optimal in the ML sense. This is a source of potential inconsistency in the classical approach to cointegration testing."}, {"heading": "3. Bayesian Cointegration", "text": "We can construct a model for estimating both \u03c6 and the parameters \u03b1, \u03b2, \u03c32 by considering \u03c6 to be a latent variable and providing a prior distribution. In this case, for cointegration we require |\u03c6| < 1, and we can encode this with a uniform distribution p(\u03c6) = U(\u03c6 (\u22121, 1)) = 1 2 [\u03c6 \u2208 (\u22121, 1)]. We complete the model specification with a distribution for 1 and form the joint model\np(y1:T , 1:T , \u03c6 x1:T ) = p(y1:T 1:T , x1:T ) p( 1:T \u03c6) p(\u03c6)\nfrom which the marginal model\np(y1:T , \u03c6 x1:T ) = p(\u03c6) \u222b 1:T p(y1:T 1:T , x1:T ) p( 1:T \u03c6)\nis formed by integration. The posterior p(\u03c6 x1:T , y1:T ) is then equivalent to p(\u03c6 1:T ) \u221d p(\u03c6) p( 1:T \u03c6)."}, {"heading": "3.1. Inference of \u03c6", "text": "The process 1:T is stationary when each 3 \u3008 t\u3009 = 0 and \u2329 2t \u232a\nis constant, independent of t. According to the recurrence relation for t, the variance satisfies\u2329 2t+1 \u232a = \u03c62 \u2329 2t \u232a + \u03c32 and we can therefore ensure\nstationary of 1:T if \u2329 21 \u232a = \u03c32/ ( 1\u2212 \u03c62 ) . We then\nchoose p( 1 \u03c6) = N ( 1 0, \u03c3 2/ ( 1\u2212 \u03c62 )) . The posterior p(\u03c6 1:T ) is proportional to p(\u03c6) \u221a\n1\u2212 \u03c62 exp \u22121 2\u03c32\n[ 21 ( 1\u2212 \u03c62 ) + T\u2211 t=2 ( t \u2212 \u03c6 t\u22121)2 ]\nand by completing the square we see that the posterior is given by a truncated Gaussian distribution with a prefactor (subject to normalisation),\np(\u03c6) \u221a 1\u2212 \u03c62N ( \u03c6 e\u030212 e\u03021 , \u03c32 e\u03021 ) where\ne\u030212 \u2261 T\u2211 t=2 t t\u22121, e\u03021 \u2261 T\u2211 t=3 2t\u22121.\nFor the data likelihood, we write for p( 1:T ),\n( 2\u03c0\u03c32 ) 1\u2212T 2 exp\u2212 1\n2\u03c32 ( T\u2211 t=1 2t \u2212 (e\u030212) 2 e\u03021 )\n\u00d7 1\u221a e\u03021 \u222b 1 \u22121 \u03c6 1 2 \u221a 1\u2212 \u03c62N ( \u03c6 e\u030212 e\u03021 , \u03c32 e\u030212 ) ."}, {"heading": "3.2. Estimating \u03b1, \u03b2, \u03c32", "text": "Our interest is to set the parameters \u03b8 = { \u03b1, \u03b2, \u03c32 } based on maximising the likelihood\np(y1:T x1:T , \u03b8) = \u222b \u03c6 p(\u03c6) p( 1:T \u03c6) .\nSince \u03c6 is a latent variable, it is convenient to approach this using the Expectation-Maximisation algorithm. Writing v for the observations and h for the latent variables EM is based on the Kullback-Leibler bound\n0 \u2264 KL(p q) \u2261 \u2329 log q(h v)\np(h v) \u232a q(h v)\nlog p(v) \u2265 \u3008log p(h, v)\u3009q(h v)\ufe38 \ufe37\ufe37 \ufe38 energy \u2212\u3008log q(h v)\u3009q(h v)\ufe38 \ufe37\ufe37 \ufe38 entropy\nwhere p is a variational distribution and q a fixed posterior. EM corresponds to iteratively maximising this\n3Angled brackets represent expectation, \u3008x\u3009 = E [x].\nlower bound with respect to the variational distribution p, and using this as the next q following inference. For this model, we replace h\u2192 \u03c6, v \u2192 1:T .\nThe energy term4 is the log of the model joint, and since we are interested in maximising with respect to the regression parameters \u03b1 and \u03b2 the only relevant terms are given as\n\u3008log p( 1 \u03c6)\u3009q(\u03c6 1:T ) + T\u2211 t=2 \u3008log p( t t\u22121, \u03c6)\u3009q(\u03c6 1:T )\nand up to a constant, this equals\n\u22121 2\u03c32\n[ 21 \u2329 1\u2212 \u03c62 \u232a + T\u2211 t=2 \u2329 ( t \u2212 \u03c6 t\u22121)2 \u232a] \u2212 T2 log 2\u03c0\u03c3 2\nwhere q(\u03c6 1:T ) = p ( \u03c6 1:T , \u03b8 old )\nis the posterior from the previous iteration. The energy can be optimised by finding the stationary point by differentiating by \u03b1 and \u03b2. Since\u2329\n( t \u2212 \u03c6 t\u22121)2 \u232a = 2t \u2212 2 t t\u22121 \u3008\u03c6\u3009+ 2t\u22121 \u2329 \u03c62 \u232a\nthe result is a system of linear equations for \u03b1 and \u03b2 involving the first and second (non-central) moments of \u03c6 from the posterior q(\u03c6 1:T ).\nBy differentiating the above energy term with respect to \u03c32, we find that optimally\n\u03c3\u03022 = 1\nT\n[ 21 \u2329 1\u2212 \u03c62 \u232a + T\u2211 t=2 \u2329 ( t \u2212 \u03c6 t\u22121)2 \u232a] so the variance can be estimated once the new regression estimates \u03b1 and \u03b2 have been found."}, {"heading": "4. The Random Walk model", "text": "A primary concern in the economics literature is to test the hypothesis of cointegration for series. The analogue with our Bayesian generative model is to compare the likelihoods of the above cointegration model and a random walk (RW) model. The likelihood for 1:T a RW is calculated as\np( 1:T \u03c6 = 1) = p( 1) T\u220f t=2 N ( t t\u22121, \u03c3 2 )\nFor p( 1) we choose a wide-interval uniform distribution. The cointegration model and random walk model can then be compared according to the Bayes factor,\np(y1:T x1:T ;\u03c6 = 1)\np(y1:T x1:T ; |\u03c6| < 1) =\np( 1:T \u03c6 = 1) p( 1:T |\u03c6| < 1) \u2261 lRW lC\n4Also called the expected completed data log likelihood.\nAlgorithm 1 Bayesian cointegration testing{ \u03b1, \u03b2, \u03c32 } \u2190 LinearRegression (x1:T , y1:T )\nrepeat 1:T \u2190 y1:T \u2212 \u03b1\u2212 \u03b2x1:T{ lC, \u3008\u03c6\u3009 , \u2329 \u03c62 \u232a} \u2190 CointInference ( 1:T , \u03c3 2 ){\n\u03b1, \u03b2, \u03c32 } \u2190 EM ( x1:T , y1:T , \u3008\u03c6\u3009 , \u2329 \u03c62 \u232a)\nuntil convergence lRW \u2190 \u220fT t=2N ( t t\u22121, \u03c3\u0302 2 ) return cointegrated\u2190 lRW/lC < threshold\nwhere the numerator represents the \u2018null hypothesis\u2019 of a unit root in the residuals process. The denominator is simply the marginal likelihood for the Bayesian cointegration model given in section 3.1.\nWhilst a fully-Bayesian approach to cointegration testing may be considered by placing prior distributions and seeking to integrate over the parameters \u03b1, \u03b2, \u03c32 separately for each model, here we restrict our analysis to point estimation for reasons of both simplicity and computational speed. Furthermore we wish to relate closely to the classical point estimate approach to cointegration estimation and testing. The purpose of this study is demonstrate the potential advantage of using a \u2018partial\u2019 Bayesian approach for \u03c6 alone."}, {"heading": "4.1. Point Estimation", "text": "Both the numerator and denominator of the Bayes factor are functions in the variance \u03c32, and we set \u03c3 for each of the two models by ML. The estimate for \u03c32 for the RW model is given according to\n\u03c3\u03022 = 1\nT \u2212 1 T\u2211 t=2 ( t \u2212 t\u22121)2 .\nNote that here t depends on \u03b1 and \u03b2, and we now discuss possible ways to determine point estimates for these parameters. There are three primary options: (i) find the parameters by OLS and use these values for both the cointegration and RW models; (ii) find the parameters by ML in the cointegration model and use the same parameters for the RW model; and (iii) find the parameters for each of the cointegration and RW models by ML. Conceptually, our interest is to first estimate a cointegration relationship, and second to test whether this relationship is more likely to be a random walk\u2014and for this reason, we consider the third option undesirable. Whilst the first option is computationally the simplest it suffers from the implicit assumption that \u03c6 = 0. For this reason the second is our preferred method.\nAlgorithm 1 shows the calculation steps for the Bayesian cointegration test. The results in figure 3\nshow that, compared with OLS estimation and unit root testing, our Bayesian technique is less likely to result in a spurious relationship for series of length T > 20. In figure 4 we show comparisons for differing decision boundaries5."}, {"heading": "5. Intermittent Cointegration", "text": "For some series, cointegration may only apply for certain segments. A good example is the Interconnector gas pipeline between Bacton, UK and Zeebrugge, Belgium, which allows gas to flow between the two countries, providing a direct link in the gas price at each end. When the pipeline closes each year for maintenance, the link between the prices is temporarily broken. It was shown by Kim (2003) that classical tests for cointegration may fail to detect a relationship even though, for a part of the series, there is such a relationship. The following model seeks to detect a cointegration relationship between two series, while allowing for regions when the relationship is in fact a random walk.\n5Differing decision thresholds correspond to different widths of uniform prior for 1 in the RW model since\nlRW lC < C \u21d4 l\u0303RW lC < 1, l\u0303RW \u2261 lRW C .\nPrevious works in this area typically limit the number of regimes in which cointegration can occur to either only two or three segments (Gregory & Hansen, 1996; Hatemi-J, 2008). In contrast, for our model we allow time-varying \u03c6t arranged to be piecewise-constant with no a priori restriction on the number of piecewiseconstant regions. The model will switch between the cointegrated case |\u03c6t| < 1 and the alternative \u03c6t = 1, corresponding to a random walk.\nWe use the binary state switch it to denote whether there is a cointegration relationship at time t: it = 1 denotes regions of t corresponding to a random walk; alternatively when it = 0, t follows a stationary cointegration relation. In the random walk regions, we have \u03c6t = 1, which is encoded in the prior distribution p1(\u03c6t) = \u03b4(\u03c6t \u2212 1). We therefore specify\np(\u03c62 i2 = 0) = { p1(\u03c62) = \u03b4(\u03c62 \u2212 1) i2 = 1 p0(\u03c62) = U(\u03c62 (\u22121, 1)) i2 = 0\nand for the piecewise-constant transition,\np(\u03c6t \u03c6t\u22121, it, it\u22121) =  p1(\u03c6t) it = 1\n\u03b4(\u03c6t \u2212 \u03c6t\u22121) it = 0, it\u22121 = 0 p0(\u03c6t) it = 0, it\u22121 = 1.\nWhilst the state transition p(it it\u22121) and p(i2) can also in principle be learned on the basis of ML, we\nleave these quantities to be user specified. Note that whilst \u03c6t can change with time, for computational tractability we assume that the remaining parameters \u03b1, \u03b2, \u03c32 are fixed throughout time. The belief network representation is given in figure 5.\nThe switching inference routine described in the following section relies on the recursive updates given in appendix A, in which we use the uniform prior for t at the start of each cointegrated segment.\n5.1. Inference of i2:T , \u03c62:T\nThe regime-switching model with piecewise-constant \u03c6t is an example of a changepoint model, in which a change of regime drops the dependency of the continuous latent variable \u03c6t on the past. The model is therefore an example of the \u2018reset model\u2019 as set out by Bracegirdle & Barber (2011), and we apply such results here. Exact inference is then possible by applying recursions for filtering (calculating each p(\u03c6t, it 1:t)) and correctionsmoothing (for p(\u03c6t, it 1:T )). For brevity, we show here only some of the key considerations; a full derivation is given in the supplementary material.\nIt is easy to show that both the filtered and smoothed posteriors for \u03c6t in the case that it = 1 degenerate in an intuitive way to\np(\u03c6t it = 1, 1:t) = p(\u03c6t it = 1, 1:T ) = p 1(\u03c6t) .\nFor the case it = 0, the posterior for \u03c6t is more complex because of the temporal dependency. For filtering, the posterior depends on the previous regime: in the event that it\u22121 = 1, a new cointegration regime has begun, and a Gaussian term is contributed to the posterior in accordance with (2); otherwise, each component is updated as for the simple cointegration case (1). We therefore see that the posterior p(\u03c6t it = 0, 1:t) is given as a truncated mixture of Gaussian components.\nFor correction smoothing, it can be difficult to derive a recursion analytically because the backwards-recursive\nstep requires a \u2018dynamics reversal\u2019 term that is calculated with the filtered posterior in the denominator. In the case that the filtered posterior is a mixture distribution, the algebra is intractable. However, we appeal to the result of Bracegirdle & Barber (2011) that by indexing the components in the filtered posterior according to the number of observations since a switch to cointegration, the smoothing recursion can be written exactly. The resulting recursion shows that, as we may expect, a subset of the Gaussian components from p(\u03c6t+1 it+1 = 0, 1:T ) are contributed to the posterior p(\u03c6t it = 0, 1:T ) without change, along with the components from filtering p(\u03c6t it = 0, 1:t) in the case that the cointegration regime ends at t.\nThe discrete filtering and smoothing components, p(it 1:t) and p(it 1:T ), are also calculated as part of the recursions. The result is an algorithm for exact inference in this switching model that scales as T 2."}, {"heading": "5.2. Likelihood", "text": "It is useful to evaluate the likelihood in order to (i) ensure that the likelihood is maximised, and (ii) provide a convergence criterion. Fortunately the likelihood is easy to calculate since the value is given by the product of normalisation constants from each step of the filtering recursion,\np( 1:T ) = p( 1) T\u220f t=2 p( t 1:t\u22121) = p( 1) T\u220f t=2 Zt\nwhere each Zt is calculated when filtering, see the supplementary material."}, {"heading": "5.3. Learning", "text": "EM is again used for parameter estimation in this model. For this regime-switching problem, the latent variables are h \u2192 {i2:T , \u03c62:T }, and the observations remain as before, v \u2192 1:T .\nTerms relevant to the optimal solution are the sum of quadratic forms derived in section 3.2 with varying \u03c6t,\u2329\n( t \u2212 \u03c6t t\u22121)2 \u232a = 2t \u2212 2 t t\u22121 \u3008\u03c6t\u3009+ 2t\u22121 \u2329 \u03c62t \u232a .\nBy retaining the first and second (non-central) moments of each component found while filtering, the linear system of equations for the regression parameters can be solved exactly in this switching model, and the variance estimate can be updated accordingly."}, {"heading": "5.4. Point Estimate of \u03c6t", "text": "Whilst our model gives a distribution over \u03c6t for each timepoint, in order to be able to compare our approach\nSep 2010 Sep 2011 1\n1.5\n2\n2.5\nSep 2010 Sep 2011 \u22120.6\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\nSep 2010 Sep 2011 0\n0.2\n0.4\n0.6\n0.8\n1\nSep 2010 Sep 2011 \u22120.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nFigure 6. Results of learning for the Interconnector gas prices\u2014maintenance occurs each September. We show (NW) the price series xt, yt; (SW) residuals t calculated with the final estimates for \u03b1, \u03b2; (NE) filtered (blue) and smoothed (red) posterior for random walk it = 1; and (SE) the maximum posterior marginal result for \u03c6t.\nto other intermittent cointegration methods that only give point estimates, we may take each\ni\u0302t = arg max it p(it 1:T )\nand then calculate the posteriors p ( \u03c6t i\u03022:T , 1:T ) to\ngive the point estimates\n\u03c6\u0302t = arg max \u03c6t\np ( \u03c6t i\u03022:T , 1:T ) ."}, {"heading": "6. Experiments", "text": "We consider real-world applications for the intermittent cointegration model. For these experiments, we ran the inference and learning algorithm for the parameters \u03b8 = {\u03b1, \u03b2, \u03c3} initialised by OLS, and use the state transition distributions fixed at values designed to match our a priori belief about the cointegration regimes."}, {"heading": "6.1. Gas Prices", "text": "The Interconnector, as noted in section 5, is a sub-sea natural gas pipeline: we took for y UK gas prices6 and for x the Zeebrugge prices7. There are approximately\n6UK SAP natural gas prices were downloaded from www.nationalgrid.com/uk/Gas/Data.\n7Prices for Zeebrugge are from www.apxendex.com and were converted into kWh using 1 kWh=29.3072222 therm.\n245 pricing days per year, and the pipeline closes annually for two weeks, so we specified the state transition distribution accordingly. The algorithm reaches convergence within four minutes with parameters \u03b1, \u03b2 that show cointegration between the annual maintenance period, see figure 6."}, {"heading": "6.2. Euro-area Bond Yields", "text": "We consider possible cointegration between Greek and German 10-year benchmark bond yields8 prior to the Euro-area financial crisis. We use the cointegration switching model to find regions of cointegration, which we expect to hold until the yield on Greek debt spiralled as Greece\u2019s debt burden took toll. The results are shown in figure 7; inference and learning takes roughly five minutes. This is an example for which, over the time window shown in the figure, the data do not show cointegration according to the classical test (Dickey-Fuller test shows p-value 0.927241, and does not reject the null hypothesis of random walk), but the intermittent cointegration model does show a segment of cointegration\u2014the classical test passes in the detected region, and fails for the remainder.\nWhilst our approach is both flexible and simple, there are limitations. In particular, the switching model\n8The bond yield data were obtained from Reuters.\nrequires that the regions of cointegration are governed by a time-invariant linear relationship. For example, in the case that deviations from the relationship in the random-walk segments are permanent, the model presented here would struggle to properly estimate a relationship since piecewise-constant values of the constant term \u03b1 would be required. Such considerations provide opportunities for future research."}, {"heading": "7. Discussion", "text": "We presented two novel techniques for estimation of a linear cointegration relationship for time-series. First, the method in section 3 allows estimation of a linear relationship between variables and we showed that our resulting algorithm is less likely to deliver a spurious relationship than the classical OLS\u2013unit root testing approach. The benefit of our approach is that it results in a fast O(T ) algorithm for testing cointegration. A natural extension would be to consider a more complete Bayesian analysis and marginalise over all parameters in both the cointegration and random walk models.\nSecond, we developed a switching model to detect a relationship and regimes in which that relationship is a random walk. Whilst the model has an obvious limitation (shared by other existing intermittent cointegration models) it has the benefit of fast exact inference, scaling O(T 2). A further natural extension would be to consider regimes in which the linear relationship can also vary. A full derivation of inference for the switching model is given in the supplementary material."}, {"heading": "Acknowledgements", "text": "We thank Ricardo Silva for useful discussions."}, {"heading": "A. Sequential inference of \u03c6", "text": "Given a sequence of \u2018observations\u2019 1:T we describe here a method to sequentially update the posterior distribution p(\u03c6 1:T ). An extension of this inference routine is used in the intermittent cointegration model in section 5. We place an improper uniform prior p( 1) = U( 1 R), representing our belief that 1 may take any real value, and use a recursive update filtering routine\u2014see for example Barber (2012).\nInitially, we begin with the prior p(\u03c6) = U(\u03c6 (\u22121, 1)), and then update the distribution by finding the posterior after observing each t. For 1, there is no update to make since p(\u03c6 1) \u221d p(\u03c6) p( 1). Thereafter, p(\u03c6 1:t) \u221d p( t t\u22121, \u03c6) p(\u03c6 1:t\u22121). This can be calculated analytically since, if p(\u03c6 1:t\u22121) \u221d p(\u03c6)N (\u03c6 ft\u22121, Ft\u22121), then9p(\u03c6 1:t) \u221d p(\u03c6)N (\u03c6 ft, Ft) where\nft = ft\u22121\u03c3 2 + t t\u22121Ft\u22121 \u03c32 + 2t\u22121Ft\u22121 , Ft = \u03c32Ft\u22121 \u03c32 + 2t\u22121Ft\u22121 (1)\nwhich on setting the initial10 f2, F2 from the emission\nN ( 2 \u03c6 1, \u03c3 2 ) = 1 | 1| N ( \u03c6 2 1 , \u03c32 21 ) (2)\ndefines a recursion for the parameters ft, Ft. After completing the updates, the required posterior p(\u03c6 1:T ) is given by a Gaussian distribution with mean fT and covariance FT truncated to the interval (\u22121, 1).\n9Details of the derivation are omitted; recursions correspond to Kalman updates\u2014see for example Barber (2012).\n10In the unlikely event that 1 = 0, the Gaussian term arises from the first non-zero t\u22121."}], "references": [{"title": "Bayesian Reasoning and Machine Learning", "author": ["D. Barber"], "venue": null, "citeRegEx": "Barber,? \\Q2012\\E", "shortCiteRegEx": "Barber", "year": 2012}, {"title": "Switch-Reset Models : Exact and Approximate Inference", "author": ["C. Bracegirdle", "D. Barber"], "venue": "In AISTATS,", "citeRegEx": "Bracegirdle and Barber,? \\Q2011\\E", "shortCiteRegEx": "Bracegirdle and Barber", "year": 2011}, {"title": "A primer on cointegration with an application to money and income", "author": ["D.A. Dickey", "D.W. Jansen", "D.L. Thornton"], "venue": "Federal Reserve Bank of St. Louis Review,", "citeRegEx": "Dickey et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Dickey et al\\.", "year": 1991}, {"title": "Developments in the study of cointegrated economic variables", "author": ["C.W.J. Granger"], "venue": "Oxford Bulletin of Economics and Statistics,", "citeRegEx": "Granger,? \\Q1986\\E", "shortCiteRegEx": "Granger", "year": 1986}, {"title": "Applied Time Series Modelling", "author": ["R. Harris", "R. Sollis"], "venue": "nal of Econometrics,", "citeRegEx": "Harris and Sollis,? \\Q1996\\E", "shortCiteRegEx": "Harris and Sollis", "year": 1996}], "referenceMentions": [{"referenceID": 2, "context": "In the economics and finance literature such instances are common; for example underlying mechanisms such as limited money supply may forge dependencies between time-series (Dickey et al., 1991).", "startOffset": 173, "endOffset": 194}, {"referenceID": 3, "context": "Testing for and estimating a cointegration relationship is classically a two-step process (Granger, 1986).", "startOffset": 90, "endOffset": 105}, {"referenceID": 3, "context": "Testing for and estimating a cointegration relationship is classically a two-step process (Granger, 1986). Firstly, the regression equation is estimated based on a simple ordinary least squares (OLS) fit to minimise \u2211 t(yt\u2212\u03b1\u2212\u03b2xt) (which we show is equivalent to maximum likelihood (ML) parameter optimisation assuming that \u03c6 = 0 in a corresponding model). Subsequently, a test for a unit root in the residuals is performed using the Dickey-Fuller test, see for example Harris & Sollis (2003), which tests the hypothesis that \u03c6 = 1 against the alternative |\u03c6| < 1 (for |\u03c6| < 1 the series is stationary and non-stationary otherwise).", "startOffset": 91, "endOffset": 492}, {"referenceID": 0, "context": "The belief network (see Barber (2012) for an introduction) for this model is given in figure 1.", "startOffset": 24, "endOffset": 38}, {"referenceID": 0, "context": "The model is therefore an example of the \u2018reset model\u2019 as set out by Bracegirdle & Barber (2011), and we apply such results here.", "startOffset": 83, "endOffset": 97}, {"referenceID": 0, "context": "However, we appeal to the result of Bracegirdle & Barber (2011) that by indexing the components in the filtered posterior according to the number of observations since a switch to cointegration, the smoothing recursion can be written exactly.", "startOffset": 50, "endOffset": 64}], "year": 2012, "abstractText": "Cointegration is an important topic for timeseries, and describes a relationship between two series in which a linear combination is stationary. Classically, the test for cointegration is based on a two stage process in which first the linear relation between the series is estimated by Ordinary Least Squares. Subsequently a unit root test is performed on the residuals. A well-known deficiency of this classical approach is that it can lead to erroneous conclusions about the presence of cointegration. As an alternative, we present a framework for estimating whether cointegration exists using Bayesian inference which is empirically superior to the classical approach. Finally, we apply our technique to model segmented cointegration in which cointegration may exist only for limited time. In contrast to previous approaches our model makes no restriction on the number of possible cointegration segments.", "creator": "LaTeX with hyperref package"}}}