{"id": "1206.6469", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Inferring Latent Structure From Mixed Real and Categorical Relational Data", "abstract": "we consider analysis of relational data ( a matching matrix ), in which iteration the rows correspond to subjects ( e. g., people ) and indeed the columns correspond to significant attributes. the elements of the matrix may be a mix of real and categorical. each subject and attribute is characterized by expressing a latent binary feature vector, and an inferred matrix maps all each row - column pair of binary major feature vectors to an observed matrix element. the latent binary features of the rows are modeled via presenting a multivariate gaussian distribution with low - rank covariance matrix, and the gaussian random variables are weakly mapped to consistent latent binary features via a parallel probit link. the same type construction is applied jointly to the constituent columns. the model infers consistent latent, low - dimensional binary features associated with each row indices and each pointer column, as follows well correlation structure distance between all rows and between all columns.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (1047kb)", "http://arxiv.org/abs/1206.6469v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["esther salazar", "lawrence carin"], "accepted": true, "id": "1206.6469"}, "pdf": {"name": "1206.6469.pdf", "metadata": {"source": "META", "title": "Inferring Latent Structure From Mixed Real and Categorical Relational Data", "authors": ["Esther Salazar", "Matthew S. Cain", "Elise F. Darling", "Stephen R. Mitroff", "Lawrence Carin"], "emails": ["esther.salazar@duke.edu", "matthew.s.cain@duke.edu", "ed75@duke.edu", "mitroff@duke.edu", "lcarin@duke.edu"], "sections": [{"heading": "1. Introduction", "text": "The inference of low-dimensional latent structure in matrix and tensor data constitutes a problem of increasing interest. For example, there has been a significant focus on exploiting low-rank and related structure in many types of matrices, primarily for matrix completion (Lawrence & Urtasun, 2009; Yu et al., 2009; Salakhutdinov & Mnih, 2008). In that problem one is typically given a very small fraction of the total matrix, and the goal is to infer the missing entries. In other problems, all or most of the matrix is\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ngiven, and the goal is to infer relationships between the rows, and between the columns. For that problem, coclustering has received significant attention (Dhillon et al., 2003; Wang et al., 2011). In co-clustering the rows/columns are typically mapped to hierarchical clusters, which may be overly restrictive in some cases. Specifically, there are situations for which two or more rows/columns may have a subset of (latent) characteristics in common, but differ in other respects, and therefore explicit assignment to clusters is inappropriate. This motivates so-called mixed-membership models. For instance, in (Meeds et al., 2007) the authors develop a model in which each row and column has an associated binary feature vector, representing each in terms of the presence/absence of particular latent features. Rather than explicitly assigning cluster membership, the binary features assign \u201cmixed memberships,\u201d because rows/columns may partially share particular latent features. In (Meeds et al., 2007) the latent binary features are mapped to observed matrix elements via an intervening regression matrix, which is also inferred. Rather than using binary features to represent the rows and columns, one may also use a sparse real feature vector for each row and column (Salakhutdinov & Mnih, 2008; Wang et al., 2010), as is effectively done in factor analysis (Carvalho et al., 2008). As noted in (Meeds et al., 2007), and discussed further below, the use of binary feature vectors aids model interpretation, and may also enhance data sharing.\nThe Indian buffet process (IBP) (Griffiths & Ghahramani, 2005) is a natural tool for modeling latent binary feature vectors, and that approach was taken in (Meeds et al., 2007). The IBP is closely related to the beta-Bernoulli process (Thibaux & Jordan, 2007), which implies that each row (subject) effectively selects a given feature i.i.d. from an underlying Bernoulli distribution, with feature-dependent\n(but subject-independent) Bernoulli probability. It is expected that many subjects may be closely related, and therefore these are likely to have similar latent binary features, with the same expected of the columns. This statistical correlation between subsets of rows/columns motivates the co-clustering approaches, but for reasons stated above clustering is often too restrictive.\nTo address these limitations of existing approaches, we propose a new model, in which the rows/columns are each characterized by latent binary feature vectors, as in (Meeds et al., 2007). However, a new construction is used to model the binary row/column features, moving beyond the i.i.d. assumption that underlies the IBP. Assume the matrix of interest is characterized by N rows. For each of the latent features of the rows, an N -dimensional real vector is drawn from a zeromean multivariate Gaussian distribution, with an unknown covariance structure. This N -dimensional real vector is employed with a probit link function to constitute an N -dimensional binary vector, manifesting the row-dependent binary value for one of the latent features. This is done for all latent binary features. By inferring the underlying covariance matrix, we uncover the latent correlation between the rows, without explicit clustering. Multivariate probit models are utilized jointly for the simultaneous analysis of rows and columns; this embodies the joint analysis of rows and columns manifested by co-clustering, while performing such in a mixed-membership setting.\nFor large N (i.e., many rows or columns), one must impose structure on the row/column covariance matrices, to achieve computational efficiency, and to enhance the uncovering of structure. This is implemented by imposing a low-rank covariance matrix model, via factor analysis with sparse factor loadings (Carvalho et al., 2008). The rows (columns) that share non-zero values in the factor loadings are inferred to be statistically correlated, without the necessity of imposing explicit clustering.\nBayesian model inference is performed, via efficient MCMC. The model is demonstrated on three real datasets."}, {"heading": "2. Basic Model Setup", "text": ""}, {"heading": "2.1. Problem statement", "text": "We consider data from N subjects, with the data in general a mix of categorical and real. There are M1 categorical entries andM2 real entries. The categorical data are represented as an N \u00d7M1 matrix X, and the real entries are represented by the N \u00d7M2 matrix Y ;\nwe wish to analyze X and Y jointly.\nThe transpose of the column vector xi represents the ith row of X, and the transpose of the column vector yi represents the ith row of Y . Vector xi = (xi1, . . . , xiM1)\nT contains categorical observations where xij \u2208 {0, . . . , qj \u2212 1} and qj corresponds to the number of categories associated with the jth component."}, {"heading": "2.2. Factor analysis", "text": "A qj-dimensional probit-regression model is employed for xij . Specifically, assume that there is a feature vector vi \u2208 RKx associated with subject i (as discussed below, the need to set Kx disappears in the final form of the model). The observed multinomial variable xij is modeled in terms of a latent variable \u03b2ij \u2208 Rqj\u22121 such that\n\u03b2ij = Sj Tvi + ij , ij \u223c N (0,\u03a3j)\nxij =\n{ 0 if max1\u2264l\u2264qj\u22121 \u03b2 (l) ij < 0\np if max1\u2264l\u2264qj\u22121 \u03b2 (l) ij = \u03b2 (p) ij > 0\nwhere p = 1, . . . , qj \u2212 1, Sj = (s(1)j , . . . , s (qj\u22121) j ) \u2208 RKx\u00d7qj\u22121, s(p)j \u2208 RKx , {\u03a3j}11 = 1 and xij = 0 corresponds to the base category. Note that \u03a3j is a (qj \u2212 1) \u00d7 (qj \u2212 1) covariance matrix, and the first element of \u03a3j is fixed to 1 in order to avoid identifiability problems (Chib & Greenberg, 1998; Zhang et al., 2008). The covariance matrix \u03a3j infers statistical correlation between the qj possible categories associated with attribute j, and an inverse-Wishart prior is employed for \u03a3j .\nThe pth component of \u03b2ij is given by\n\u03b2 (p) ij = v T i s (p) j + (p) ij , p = 1, . . . , qj \u2212 1, (1)\nwhere (p) ij \u223c N (0, {\u03a3j}pp) and s (p) j \u2208 RKx represents a feature vector associated with the choice p \u2208 {1, . . . , qj \u2212 1} for the component j of xi.\nA similar construction is employed for the components of the real matrix Y , without the need for the probit link. Specifically, for row i and column j we respectively define real feature vectors ai \u2208 RKy and bj \u2208 RKy , and the (i, j)th matrix entry is modeled as\nyij = a T i bj + \u03b5ij , \u03b5ij \u223c N(0, \u03c32y). (2)\nwith Ky again disappearing in the final form of the model."}, {"heading": "2.3. Binary row and column feature vectors", "text": "Let ri \u2208 {0, 1}K represent a latent binary feature vector characteristic of row i in both X and Y . We assu-\nme that\nvi = R (X)ri and ai = R (Y )ri, (3)\nwhere R(X) \u2208 RKx\u00d7K , R(Y ) \u2208 RKy\u00d7K . Similarly, let d (p) j \u2208 {0, 1}K represent the latent binary feature vector associated with s (p) j , with cj \u2208 {0, 1}K so defined for bj (for notational simplicity we write all binary vectors as being of same dimension K, but in practice the model infers the number of binary components needed to represent each of these vectors). We assume\ns (p) j = C (X)d (p) j and bj = C (Y )cj , (4)\nwhere C(X) \u2208 RKx\u00d7K and C(Y ) \u2208 RKy\u00d7K . Using the above constructions, we can rewrite (1) and (2) as\n\u03b2 (p) ij = r T i M (X)d (p) j + (p) ij , (5)\nyij = r T i M (Y )cj + \u03b5ij . (6)\nNote that the need to set the aforementioned dimensions Kx and Ky has been removed, and what remains are the matrices M (X) = R(X) T C(X) \u2208 RK\u00d7K and M (Y ) = R(Y ) T C(Y ) \u2208 RK\u00d7K ; K is typically set as a large upper bound on the number of binary features needed to represent the rows and columns, with only a subset of these K variables inferred as important when performing computations.\nThe advantage of this construction is that real feature vectors {vi}, {s(p)j }, {ai} and {bi} are constituted in terms of binary feature vectors, with the regression matrices M (X) and M (Y ) between the binary and real vectors shared for all rows and columns. This imposes significant structure and sharing on the learning of {vi}, {s(p)j }, {ai} and {bi}, as considered in (Meeds et al., 2007). This paper differs from (Meeds et al., 2007) in three ways: (i) we jointly consider real and categorical data jointly, (ii) we impose low-rank structure on M (X) and M (Y ) (discussed next), and (ii) a new framework is developed for modeling the binary vectors {ri}, {d(p)j } and {cj} (discussed in Section 3)."}, {"heading": "2.4. Low-rank regression matrices", "text": "We model M (X) and M (Y ) as low-rank matrices:\nM (X) = K\u2211 l=1 \u03bb (X) l b (X) l u (X) l (v (X) l ) T , (7)\nM (Y ) = K\u2211 l=1 \u03bb (Y ) l b (Y ) l u (Y ) l (v (Y ) l ) T , (8)\nwith \u03bb (X) l \u223c N0,\u221e(0, \u03c32\u03bb), corresponding to a truncated normal distribution, over (0,\u221e), with an inversegamma prior on \u03c32\u03bb; \u03bb (Y ) l is defined similarly. The\nvectors u (X) l , v (X) l , u (Y ) l and v (Y ) l are all defined similarly, and we discuss one in detail, for conciseness. Specifically, we draw u (X) l \u223c N (0, IK), where IK is the K \u00d7 K identity matrix. The variables b(X)l and b (Y ) l are binary, and they allow inference of the associated matrix rank. Again illustrating one of these for conciseness, we employ a sparseness-inducing betaBernoulli representation, with b (X) l \u223c Bernoulli(\u03c0(X)), with \u03c0(X) \u223c Beta(1/K, 1).\nUsing, for example, (7) in (5), we observe that the model imposes\n\u03b2 (p) ij = \u2211 l\u2208S \u03bb (X) l < ri,u (X) l >< d (p) j ,v (X) l > + (p) ij (9)\nwhere < \u00b7, \u00b7 > corresponds to a vector inner product, and S defines the set of indices for which b(X)l = 1. Note that for the probit link function this construction implies that we do not require random-effect terms for the subjects and attributes (as is typically done when using real feature vectors (Wang et al., 2010)), since the sum of terms in (9) automatically allow randomeffect terms, if needed (such will correspond to one of the terms in the sum).\nFrom (9), and considering (1), note that we may rep-\nresent vi as a vector composed of \u221a \u03bb (X) l < ri,u (X)) l >\nfor l \u2208 S; s(p)j is similarly defined by \u221a \u03bb (X) l < d (p) j ,v (X)) l > for l \u2208 S. Therefore, via the low-rank construction in (7), we infer Kx to be the size of set S (rank ofM (X)). As discussed when presenting results, the low-rank construction in (8) allows us to similarly infer Ky. This property is a principal reason for imposing low-rank structure on M (X) and M (Y ), it aiding interpretation of the model results."}, {"heading": "3. Correlated Binary Feature Vectors", "text": "We describe in detail the proposed modeling of ri, with a similar construction employed for {d(p)j } and {cj}. A sparse multivariate probit model is imposed: \u03b7k \u223c N (0,\u03a3), with rik|\u03b7ik = 1 if \u03b7ik > 0, and rik|\u03b7ik = 0 otherwise; k = 1, . . . ,K, with \u03b7ik the ith component of \u03b7k, and rik is the kth component of ri. Marginally, rik \u223c Ber(\u03c0ik) where \u03c0ik = Pr(\u03b7ik > 0). The covariance matrix \u03a3 \u2208 RN\u00d7N imposes an underlying correlation structure between (r1k, . . . , rNk), for all binary features k.\nWe must now place a prior on the covariance matrix \u03a3. A large class of models impose sparsity on the inverse of \u03a3 (i.e., the precision matrix), corresponding to a sparse Gaussian graphical models (GGM). The GGM approach for covariance matrix estimation is attrac-\ntive and many approaches have been proposed (AtayKayis & Massam, 2005; Dobra et al., 2011). Alternatively, other approaches have been proposed for directly modeling the covariance matrix, placing shrinkage priors on various parameterizations of \u03a3. For instance, (Liechty et al., 2004) considered shrinkage priors in terms of the correlation matrix, and (Yang & Berger, 1994) used reference priors based on the spectral decomposition of \u03a3.\nWe choose to directly model the components of \u03a3. In fact, given that N (dimensionality of the covariance matrix) is typically larger than K, standard estimators are liable to be unstable (Sun & Berger, 2006; Hahn et al., 2012). Hence, we impose a factor structure on a covariance matrix, in a similar fashion to (Hahn et al., 2012). Such regularization is crucial when the number of variables is large relative to the sample size, and also when the covariance corresponds to an unobservable latent variable (Rajaratman et al., 2008).\nWe assume that cov(\u03b7k) = BB T + \u03a8 where \u03a8 \u2208 RN\u00d7N is a diagonal matrix, rank(B) = K < N and B \u2208 RN\u00d7K . That construction implies that \u03b7k \u223c N (Bfk,\u03a8) where fk \u223c N (0, IK). The matrixB must be constrained to be zero for upper-triangular entries and positive along the diagonal, to avoid identifiability problems. Further, \u03a8 is fixed to be the identity to allow a simple identification strategy. The prior on the loadings B is given by\n(bik|vi, \u03c0k) \u223c \u03c0kN (0, vi) + (1\u2212 \u03c0k)\u03b40 (10)\nwith vi \u223c IG(c/2, cd/2) and \u03c0i \u223c Beta(1, 1), with IG an inverse-gamma distribution and \u03b40 a unit point measure concentrated at 0. The sparsity prior permits some of the unconstrained elements in the factorloadings matrix B to be identically zero."}, {"heading": "4. Posterior Computation", "text": "An approximation to the full posterior of model parameters is performed based on a Gibbs sampler, with Metropolis-Hastings updates for a subset of the parameters. We now briefly describe how to sample some of the most interesting parameters, based on their full conditional posterior distributions.\n\u2022 Sample u(X)l as (u (X) l |\u2212) \u223c N (m (X) l ,V (X) l ), where V (X) l = ( IK + \u2211N i=1(E\u0303 (l) i ) T \u03a3\u0303\u22121E\u0303 (l) i )\u22121 , m (X) l = V (X) l \u2211N i=1(E\u0303 (l) i ) T \u03a3\u0303\u22121\u03b2 (\u2212l) i , E\u0303 (l) i = \u03bb (X) l D Tv (X) l r T i , D = (d1, . . . ,dM1), dj = (d (1) j , . . . ,d (qj\u22121) j ), \u03a3\u0303 = diag(\u03a31, . . . ,\u03a3M1), \u03b2 (\u2212l) i =\n\u03b2i \u2212 DT ( \u2211 k 6=l \u03bb (X) k b (X) k v (X) k (u (X) k ) T )ri, and \u03b2 T i = (\u03b2Ti1, . . . ,\u03b2 T iM1 ) \u2208 Rq1+...+qM1\u2212M1 .\n\u2022 In order to sample rik \u2208 {0, 1}, i = 1, . . . , N , k = 1, . . . ,K, let yi = M\u0303Y ri + i and \u03b2i = M\u0303Xri + \u03b5i where M\u0303Y = C TM (Y ) T and M\u0303X = D TM (X) T . Also, let y (\u2212k) i = yi \u2212 M\u0303 (\u2212k) Y r (\u2212k) i and \u03b2 (\u2212k) i = \u03b2i \u2212 M\u0303 (\u2212k) X r (\u2212k) i . Then, (rik|\u2212) \u223c Bernoulli(p1/(p1 + p2)), where p1 = \u03c0ik exp{\u22120.5(\u03c3\u22122y eTi ei + fTi \u03a3\u0303\u22121fi)}, p2 = (1 \u2212 \u03c0ik) exp{\u22120.5(\u03c3\u22122y y (\u2212k) i T y (\u2212k) i + \u03b2 (\u2212k) i T \u03a3\u0303\u22121\u03b2 (\u2212k) i )}, ei = y (\u2212k) i \u2212 M (\u2212k) Y , fi = \u03b2 (\u2212k) i \u2212 M (\u2212k) X and \u03c0ik = Pr(\u03b7ik > 0). Samples for cjk, d (p) jk , b (X) l and b (Y ) l are obtained similarly. \u2022 The parameter-extended Metropolis-Hastings algorithm is employed to sample \u03a3j given the restriction {\u03a3j}11 = 1 (Zhang et al., 2008) only when qj > 2, otherwise \u03a3j is fixed to one. Considering a Wishart prior \u03a3j \u223c W(m0,\u2126j), the algorithm is as follows: (1) at iteration t, set the values (R\n(t) j , D (t) j ) by gener-\nating \u03a3j = D (t)1/2 j R (t) j D (t)1/2 j , where R (t) j is the correlation matrix and D (t) j the diagonal variance matrix with the first element equal to one. (2) Generate candidate values \u03a3\u2217j = D \u22171/2 j R \u2217 jD \u22171/2 j \u223c W (m0,\u03a3j), D\u2217j is a diagonal matrix without restrictions. (3) Accept the new values (replacing {D\u2217j }11 = 1) with\nprobability \u03b1 = min { 1, p(R\u2217j ,D \u2217 j |\u2212)\np(R (t) j ,D (t) j |\u2212)\nq(\u03a3j |\u03a3\u2217j ) q(\u03a3\u2217j |\u03a3j)\n} , where\np(Rj , Dj |\u2212) is the joint posterior distribution and q(\u00b7|\u03a3j) is the proposal distribution given by the product of the Jacobian term for the transformation from \u03a3j to (Rj , Dj) and the Wishart density W (m,\u03a3j),\nsuch that J\u03a3j\u2192Rj ,Dj = \u220fqj\u22121 l=1 d qj\u22122 2 l ."}, {"heading": "5. Applications", "text": ""}, {"heading": "5.1. Analysis of the animals dataset", "text": "We first test the performance of the proposed model on the animals dataset (Kok & Domingos, 2007; Sutskever et al., 2009). This consists of 50 animal classes and 85 binary attributes (with no missing data). Note that in this experiment we only have a categorical (binary) observation matrix X \u2208 {0, 1}50\u00d785.\nThe model is fitted using the proposed MCMC scheme. We ran the algorithm considering 20,000 iterations with a burn-in of 5,000 draws, and we collect every third sample that give us a total of 5,000 saved samples. The analysis was performed with K = 20, c = d = 1, and \u03c32\u03bb = 1 (many other similar settings yielded similar results). For the sparse probit factor model (discussed in Section 3) we consider six factors; larger models are possible and were considered, how-\never in our experiments we noticed that less than six factors were enough to capture the underlying correlation structure of the binary features. Indeed, only three and four of the columns of Br and Bd, respectively, have non zero elements. In general, the results are very insensitive to the setting of K, as long as it is set relatively large.\nWe examine the latent correlations (between the rows and columns) learned by the model by inspecting the most likely sample produced by the Gibbs sampler. Figure 1 shows the latent correlation structure between {ri}i=1,...,50 learned for the animals as well as between {dj}j=1,...,85 learned for the attributes. By the analysis of those correlations, we are able to identify hierarchical clusters and affinities between rows and columns; we use a clustering algorithm (Kaufman & Rousseeuw, 1990) to identify row and column hierarchical cluster structure based on the inferred correlation matrix (this is done for illustration; we do not perform clustering when implementing the model, rather the full covariance between rows and columns is inferred). The hierarchical clustering algorithm yields a dendrogram, plotted jointly with the learned correlation matrices in Figure 1. The closeness of any two clusters is determined by a dissimilarity matrix I \u2212R where R is the correlation matrix (see Eisen et al., 1998; Wilkinson & Friendly, 2009, for more details). The learned groups are described on the right panel of the figure. Some interesting interpretations are derived from the correlation structure for the attributes.\nFor example, cluster G1 (which includes attributes of marine animals) is highly correlated to cluster G8 and negative correlated to cluster G10 and G11 (which includes attributes like quadrupedal, ground and mountain)."}, {"heading": "5.2. Senate voting data", "text": "We next examine a binary vote matrix from the United States Senate during the 110th Congress, from January 3, 2007 to January 3, 2009. The binary matrix, X, has dimension 102 \u00d7 657, where each row corresponds to a senator and each column corresponds to a piece of legislation; X is manifested by mapping all \u201cyes\u201d votes to one and \u201cno\u201d votes (or abstentions) to zero. The percentage of missing values is about 7.1%. We perform analysis of the voting data considering K = 50. We use the same priors and MCMC setup considered in the previous application. We inferred that there are approximately 10 binary features for the senators, 13 for the legislation, and M (X) had a rank of Kx \u2248 4, with one dominant factor, with dominant corresponding \u03bb\n(X) l (consistent with related\nresearch that indicates one dominant factor for such data (Wang et al., 2010)). Figure 2 shows the dendrogram derived from the correlation matrix associated with the senators, to illustrate the clustering of people. The correlation matrix reveals significant differences between two groups of senators, which are constituted by Democrats and Republicans. As an example interpretation of the dendrogram for the senators, note that Republican senators Collins, Snowe and Specter are inferred as being at the \u201cedge\u201d of the Republican party, nearest to the Democrats; these were the only Republicans who joined most Democrats in voting for the 2009 Economic Stimulus Package (and Specter later switched to the Democratic party). Also, Barack Obama and Hillary Clinton, who competed closely for the Democratic presidential nomination in 2008, are very weakly correlated with any of the Republicans.\nIn Figure 2, middle panel, we show the reordered voting data matrix X. The matrix was reordered by rows and columns according to similarities learned from the correlations matrices associated with the senators and legislation. The matrix reveals interesting patterns. For example, the first 300 columns are primarily Democrat-sponsored legislation, the following 200 legislation are primarily Republican-sponsored legislation, and the last columns are unanimous votes, for things like nominations for various government posts. We performed LDA (Blei et al., 2003) topic modeling on the text documents (separate analysis), to infer structure in the legislation, and help interpret the inferred relationships; three types of legislation so in-\nferred are shown in Figure 2.\nWe compare our results with two related models. The first follows the proposed construction, except that the latent binary vectors are modeled via an IBP; the second is the logistic binary matrix factorization (BMF) model (Meeds et al., 2007); the main difference between the first and second alternatives is that the former imposes the low-rank model of Section 2.4. Figure 2, right panel, shows the average fraction of correct predictions for each model as a function of the fraction of missing data (held-out data, averaged over 15 runs). These results reveal the advantage of the low-rank construction (by comparing the two IBP solutions), and of the imposition of correlation in the latent binary features (omitted in the two IBP-based constructions)."}, {"heading": "5.3. Behavioral dataset", "text": "The behavioral dataset comes from a survey conducted by the Duke Visual Cognition Lab during 2010 and 2011 (details omitted here to keep authors anonymous during review). The 508 responders were members of a university community, answering different types of questionnaires; the questions regarded media multitasking (MMI), an attention deficit hyperactivity disorder (ADHD) test, the Autism Spectrum Quotient, eating attitudes (EAT) test, video games (VG) activities, a NEO-AC personality inventory (neuroticism, extraversion, openness, agreeableness, conscientiousness scores) and Barratt Impulsivity Scale (BIS-11); almost all of these questions come from standard surveys in the respective areas (discussed further below). The total dataset consists of M1 = 20 categorical and M2 = 106 real-valued questions. Among the 20 categorical variables considered in the analysis, there are 16 binary observations and 4 variables with more than 2 nominal categories. Concerning the real-valued observations, the 106 studied variables were classified as\nfollows: 40 variables related to VG-playing habits, 23 variables related to passtime activities, 30 associated with the NEO-AC facets, 5 autism subscales, 3 impulsivity subscales, and the last 5 variables related to EAT score, MMI, age, years of education and ADHD score. The percentage of missing values is approximately 13%.\nWe perform a joint analysis of the categorical and realvalued data matrices considering K = 50. The realvalued data matrix Y was column-normalized to zero mean and unit variance before the analysis. In addition, \u03c32\u03bb = 10, m0 = 8, \u2126 = Iqj\u22121 and c = d = 1. The MCMC algorithm was run for 50,000 iterations, with the first 25,000 discarded, and then every 5th collected to produce a posterior sample of size 5,000.\nFigure 3 shows the approximate posterior distribution for the number of features associated with questions (categorical and real-valued answers) and people. From these results we note that approximately 8 and 6 features in d (p) j and cj are used by the model, while there are approximately 20 binary features inferred as associated with the people.\nIn a similar fashion to the analysis of the Animals dataset, we analyze the learned correlation matrices associated with the questions and people. Figure 4 shows those matrices with the rows and columns or-\ndered such that similar rows and columns are near each other. In the vertical margin appears the hierarchical cluster tree derived from the correlation (as discussed above). Based upon these results, we are able to identify blocks of correlated questions and clusters of people. Interesting interpretations can be derived from these results. For example, men are highly correlated with fighting and real time strategy VG and negatively correlated with normal vision and monolingual. Figure 4, right panel, displays the learned correlation structure between people. It shows three big clusters; G1 is primarily composed of women (82.3% women, 17.7% men), G2 represents a heterogeneousgender group (59% women, 41% men), and G3 is predominantly men (20.6% women, 79.4% men).\nWe are also interested in the analysis of questions related to behavior scores like the NEO-AC characteristics, autism, ADHD, EAT and MMI. The analysis of these variables is of particular interested in Psychology, where the Big Five factors of personality traits (McCrae & John, 1992; Costa & McCrae, 1992) has emerged as a robust model to describe human personality. Specifically, the five factors are directly related with the NEO-AC data (real, non-categorical answers, represented in Y ) and we seek to connect our inferred latent features with what is known from Psychology, to analyze the quality of inferred structure. From (6)\nand (8) we have that yij = \u2211 l\u2208S \u03bb (Y ) l < ri,u (Y ) l >< cj ,v (Y ) l > +\u03b5ij , and therefore from (2) we may express\nthe lth component of bj as bjl = \u221a \u03bb (Y ) l < cj ,v (Y ) l >, this corresponding to the factor loading for question j, factor l. Considering the most likely sample in the Gibbs sampling, we infer bj \u2208 R6, with Ky = 6 factors coming from the rank of M (Y ).\nFigure 5 shows a diagram where groups of questions are associated to the 6 inferred factors. The plot shows connections between factors and questions in terms of the major values on each factor loading. An interesting finding is that the model uncovered the proper number of factors, i.e., five factors that group thirty facets of personality and an additional factor that groups autism scores (to our knowledge, this is the first quantitative analysis of this sort that demonstrates that the question in these questionnaires indeed capture the aspects of personality intended by subject-matter experts in their design). The first five features are clearly related to personality traits, each of them involving different facets of neuroticism (N), extraversion (E), openness (O), agreeableness (A) and conscientiousness (C). Autism scores like communication, social skill and imagination form an additional independent factor. Also, impulsivity scores belong to the\nfactor associated with the conscientiousness characteristic but with negative values."}, {"heading": "5.4. Computations", "text": "The code for these experiments was written in Matlab, and the computations were run on a computer with 2.53GHz processor and 4GB memory. To give a sense of computation times, for the Behavioral dataset considered above, approximately 11 seconds were required per MCMC sample."}, {"heading": "6. Summary", "text": "A new model has been developed for representing real, categorical and mixed real-categorical relational data. A multivariate probit model was employed, jointly imposing correlation between the subjects and between the attributes. These covariances were used in the experiments to infer hierarchical structure between the subjects and between the attributes. Encouraging results were demonstrated on three real-world data sets, the last of which is new, characterized by mixed realcategorical survey data for several interesting psychological conditions."}, {"heading": "Acknowledgements", "text": "The research reported here was supported by ARO, ONR and DARPA (under the MSEE Program)."}], "references": [{"title": "The marginal likelihood for decomposable and non-decomposable graphical Gaussian models", "author": ["A. Atay-Kayis", "H. Massam"], "venue": null, "citeRegEx": "Atay.Kayis and Massam,? \\Q2005\\E", "shortCiteRegEx": "Atay.Kayis and Massam", "year": 2005}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Machine Learning Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "High-dimensional sparse factor modelling: Applications in gene expression genomics", "author": ["C. Carvalho", "J. Chang", "J. Lucas", "J.R. Nevins", "Q. Wang", "M. West"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Carvalho et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Carvalho et al\\.", "year": 2008}, {"title": "Revised NEO Personality Inventory and NEO Five-Factor Inventory manual", "author": ["P.T. Costa", "R.R. McCrae"], "venue": "Odessa, FL: Psychological Assessment Resources,", "citeRegEx": "Costa and McCrae,? \\Q1992\\E", "shortCiteRegEx": "Costa and McCrae", "year": 1992}, {"title": "Bayesian inference for general Gaussian graphical models with application to multivariate lattice data", "author": ["A. Dobra", "A. Lenkoski", "A. Rodriguez"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Dobra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dobra et al\\.", "year": 2011}, {"title": "Cluster analysis and display of genome-wide expression patterns", "author": ["M. Eisen", "P. Spellman", "P. Brown", "D. Botstein"], "venue": "In Proc. National Academy of Sciences,", "citeRegEx": "Eisen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Eisen et al\\.", "year": 1998}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "In Proc. NIPS", "citeRegEx": "Griffiths and Ghahramani,? \\Q2005\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2005}, {"title": "A sparse factor-analytic probit model for congressional voting patterns", "author": ["P.R. Hahn", "C.M. Carvalho", "J.G. Scott"], "venue": "Journal of the Royal Statistical Society, Series C,", "citeRegEx": "Hahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hahn et al\\.", "year": 2012}, {"title": "Finding Groups in Data. An Introduction to Cluster Analysis", "author": ["L. Kaufman", "P.J. Rousseeuw"], "venue": null, "citeRegEx": "Kaufman and Rousseeuw,? \\Q1990\\E", "shortCiteRegEx": "Kaufman and Rousseeuw", "year": 1990}, {"title": "Statistical predicate invention", "author": ["S. Kok", "P. Domingos"], "venue": "In Proc. ICML", "citeRegEx": "Kok and Domingos,? \\Q2007\\E", "shortCiteRegEx": "Kok and Domingos", "year": 2007}, {"title": "Non-linear matrix factorization with Gaussian processes", "author": ["N. Lawrence", "R. Urtasun"], "venue": "In Proc. ICML", "citeRegEx": "Lawrence and Urtasun,? \\Q2009\\E", "shortCiteRegEx": "Lawrence and Urtasun", "year": 2009}, {"title": "An introduction to the five-factor model and its applications", "author": ["R.R. McCrae", "O.P. John"], "venue": "Journal of Personality,", "citeRegEx": "McCrae and John,? \\Q1992\\E", "shortCiteRegEx": "McCrae and John", "year": 1992}, {"title": "Modeling dyadic data with binary latent factors", "author": ["E. Meeds", "Z. Ghahramani", "R. Neal", "S. Roweis"], "venue": "In Proc. NIPS", "citeRegEx": "Meeds et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Meeds et al\\.", "year": 2007}, {"title": "Flexible covariance estimation in graphical Gaussian models", "author": ["B. Rajaratman", "H. Hassam", "C.M. Carvalho"], "venue": "The Annals of Statistics,", "citeRegEx": "Rajaratman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rajaratman et al\\.", "year": 2008}, {"title": "Bayesian probabilistic matrix factorization with MCMC", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In Proc. NIPS", "citeRegEx": "Salakhutdinov and Mnih,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Mnih", "year": 2008}, {"title": "Objective priors for the multivariate normal model", "author": ["D. Sun", "J.O. Berger"], "venue": "In Proc. 8th Valencia World Meeting on Bayesian Statistics", "citeRegEx": "Sun and Berger,? \\Q2006\\E", "shortCiteRegEx": "Sun and Berger", "year": 2006}, {"title": "Modelling relational data using Bayesian clustered tensor factorization", "author": ["I. Sutskever", "R. Salakhutdinov", "J. Tenenbaum"], "venue": "In Proc. NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Hierarchical beta processes and the Indian buffet process", "author": ["R.J. Thibaux", "M.I. Jordan"], "venue": "In Proc. AISTATS", "citeRegEx": "Thibaux and Jordan,? \\Q2007\\E", "shortCiteRegEx": "Thibaux and Jordan", "year": 2007}, {"title": "Joint analysis of time-evolving binary matrices and associated documents", "author": ["E. Wang", "D. Liu", "J. Silva", "D. Dunson", "L. Carin"], "venue": "In Proc. NIPS", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Nonparametric Bayesian co-clustering ensembles", "author": ["P. Wang", "K.B. Laskey", "C. Domeniconi", "M.I. Jordan"], "venue": "In SIAM SDM", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "The history of the cluster heat map", "author": ["L. Wilkinson", "M. Friendly"], "venue": "The American Statistician,", "citeRegEx": "Wilkinson and Friendly,? \\Q2009\\E", "shortCiteRegEx": "Wilkinson and Friendly", "year": 2009}, {"title": "Estimation of a covariance matrix using the reference prior", "author": ["R. Yang", "J.O. Berger"], "venue": "The Annals of Statistics,", "citeRegEx": "Yang and Berger,? \\Q1994\\E", "shortCiteRegEx": "Yang and Berger", "year": 1994}, {"title": "Large-scale collaborative prediction using a nonparametric random effects model", "author": ["K. Yu", "J. Lafferty", "S. Zhu", "Y. Gong"], "venue": "In Proc. ICML", "citeRegEx": "Yu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2009}, {"title": "Bayesian analysis of multivariate nominal measures using multivariate multinomial probit models", "author": ["X. Zhang", "W.J. Boscardin", "T.R. Belin"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 22, "context": "For example, there has been a significant focus on exploiting low-rank and related structure in many types of matrices, primarily for matrix completion (Lawrence & Urtasun, 2009; Yu et al., 2009; Salakhutdinov & Mnih, 2008).", "startOffset": 152, "endOffset": 223}, {"referenceID": 19, "context": "For that problem, coclustering has received significant attention (Dhillon et al., 2003; Wang et al., 2011).", "startOffset": 66, "endOffset": 107}, {"referenceID": 12, "context": "For instance, in (Meeds et al., 2007) the authors develop a model in which each row and column has an associated binary feature vector, representing each in terms of the presence/absence of particular latent features.", "startOffset": 17, "endOffset": 37}, {"referenceID": 12, "context": "In (Meeds et al., 2007) the latent binary features are mapped to observed matrix elements via an intervening regression matrix, which is also inferred.", "startOffset": 3, "endOffset": 23}, {"referenceID": 18, "context": "Rather than using binary features to represent the rows and columns, one may also use a sparse real feature vector for each row and column (Salakhutdinov & Mnih, 2008; Wang et al., 2010), as is effectively done in factor analysis (Carvalho et al.", "startOffset": 139, "endOffset": 186}, {"referenceID": 2, "context": ", 2010), as is effectively done in factor analysis (Carvalho et al., 2008).", "startOffset": 51, "endOffset": 74}, {"referenceID": 12, "context": "As noted in (Meeds et al., 2007), and discussed further below, the use of binary feature vectors aids model interpretation, and may also enhance data sharing.", "startOffset": 12, "endOffset": 32}, {"referenceID": 12, "context": "The Indian buffet process (IBP) (Griffiths & Ghahramani, 2005) is a natural tool for modeling latent binary feature vectors, and that approach was taken in (Meeds et al., 2007).", "startOffset": 156, "endOffset": 176}, {"referenceID": 12, "context": "To address these limitations of existing approaches, we propose a new model, in which the rows/columns are each characterized by latent binary feature vectors, as in (Meeds et al., 2007).", "startOffset": 166, "endOffset": 186}, {"referenceID": 2, "context": "This is implemented by imposing a low-rank covariance matrix model, via factor analysis with sparse factor loadings (Carvalho et al., 2008).", "startOffset": 116, "endOffset": 139}, {"referenceID": 23, "context": "Note that \u03a3j is a (qj \u2212 1) \u00d7 (qj \u2212 1) covariance matrix, and the first element of \u03a3j is fixed to 1 in order to avoid identifiability problems (Chib & Greenberg, 1998; Zhang et al., 2008).", "startOffset": 142, "endOffset": 186}, {"referenceID": 12, "context": "This imposes significant structure and sharing on the learning of {vi}, {s j }, {ai} and {bi}, as considered in (Meeds et al., 2007).", "startOffset": 112, "endOffset": 132}, {"referenceID": 12, "context": "This paper differs from (Meeds et al., 2007) in three ways: (i) we jointly consider real and categorical data jointly, (ii) we impose low-rank structure on M (X) and M (Y ) (discussed next), and (ii) a new framework is developed for modeling the binary vectors {ri}, {d j } and {cj} (discussed in Section 3).", "startOffset": 24, "endOffset": 44}, {"referenceID": 18, "context": "Note that for the probit link function this construction implies that we do not require random-effect terms for the subjects and attributes (as is typically done when using real feature vectors (Wang et al., 2010)), since the sum of terms in (9) automatically allow randomeffect terms, if needed (such will correspond to one of the terms in the sum).", "startOffset": 194, "endOffset": 213}, {"referenceID": 4, "context": "tive and many approaches have been proposed (AtayKayis & Massam, 2005; Dobra et al., 2011).", "startOffset": 44, "endOffset": 90}, {"referenceID": 7, "context": "In fact, given that N (dimensionality of the covariance matrix) is typically larger than K, standard estimators are liable to be unstable (Sun & Berger, 2006; Hahn et al., 2012).", "startOffset": 138, "endOffset": 177}, {"referenceID": 7, "context": "Hence, we impose a factor structure on a covariance matrix, in a similar fashion to (Hahn et al., 2012).", "startOffset": 84, "endOffset": 103}, {"referenceID": 13, "context": "Such regularization is crucial when the number of variables is large relative to the sample size, and also when the covariance corresponds to an unobservable latent variable (Rajaratman et al., 2008).", "startOffset": 174, "endOffset": 199}, {"referenceID": 23, "context": "\u2022 The parameter-extended Metropolis-Hastings algorithm is employed to sample \u03a3j given the restriction {\u03a3j}11 = 1 (Zhang et al., 2008) only when qj > 2, otherwise \u03a3j is fixed to one.", "startOffset": 113, "endOffset": 133}, {"referenceID": 16, "context": "We first test the performance of the proposed model on the animals dataset (Kok & Domingos, 2007; Sutskever et al., 2009).", "startOffset": 75, "endOffset": 121}, {"referenceID": 18, "context": "We inferred that there are approximately 10 binary features for the senators, 13 for the legislation, and M (X) had a rank of Kx \u2248 4, with one dominant factor, with dominant corresponding \u03bb (X) l (consistent with related research that indicates one dominant factor for such data (Wang et al., 2010)).", "startOffset": 279, "endOffset": 298}, {"referenceID": 1, "context": "We performed LDA (Blei et al., 2003) topic modeling on the text documents (separate analysis), to infer structure in the legislation, and help interpret the inferred relationships; three types of legislation so in-", "startOffset": 17, "endOffset": 36}, {"referenceID": 1, "context": "Three types of legislation are inferred (rectangles at right), based upon topic modeling (Blei et al., 2003) applied separately to the text legislation.", "startOffset": 89, "endOffset": 108}, {"referenceID": 12, "context": "The first follows the proposed construction, except that the latent binary vectors are modeled via an IBP; the second is the logistic binary matrix factorization (BMF) model (Meeds et al., 2007); the main difference between the first and second alternatives is that the former imposes the low-rank model of Section 2.", "startOffset": 174, "endOffset": 194}], "year": 2012, "abstractText": "We consider analysis of relational data (a matrix), in which the rows correspond to subjects (e.g., people) and the columns correspond to attributes. The elements of the matrix may be a mix of real and categorical. Each subject and attribute is characterized by a latent binary feature vector, and an inferred matrix maps each row-column pair of binary feature vectors to an observed matrix element. The latent binary features of the rows are modeled via a multivariate Gaussian distribution with low-rank covariance matrix, and the Gaussian random variables are mapped to latent binary features via a probit link. The same type construction is applied jointly to the columns. The model infers latent, low-dimensional binary features associated with each row and each column, as well correlation structure between all rows and between all columns.", "creator": "LaTeX with hyperref package"}}}