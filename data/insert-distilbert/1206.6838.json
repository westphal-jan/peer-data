{"id": "1206.6838", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Continuous Time Markov Networks", "abstract": "a central task in managing many applications is reasoning about processes that change in a continuous time. the mathematical framework of continuous time markov processes increasingly provides us the fastest basic foundations literature for modeling such systems. recently, jeffrey nodelman et al introduced continuous paced time bayesian statistical networks ( ctbns ), which allow a virtually compact representation of continuous - clock time processes over a factored state space. in this paper, we introduce continuous time markov networks ( ctmns ), an alternative representation language that represents a physically different possible type of continuous - time evolution dynamics. in many real life processes, such as aquatic biological and chemical systems, the dynamics of the process can be naturally described as an interplay between two forces - the tendency of each entity to change its state, and the overall fitness or energy function of storing the entire system. in our model, defining the first force is described by a continuous - coupled time proposal process that alternately suggests possible local changes to the state of the system at different rates. the second stationary force is represented by a markov network that encodes the fitness, or desirability, of different states ; a proposed local change mechanism is then accepted with a probability that intensity is a function of changing the change in the fitness distribution. we show that the fitness distribution is also the stationary distribution of the markov process, so that this coordinate representation scheme provides a characterization of also a temporal process whose stationary distribution has a compact graphical representation. this allows us to naturally fourier capture a different type of structure in naturally complex dynamical processes, techniques such as evolving through biological sequences. we describe the semantics of the species representation, its basic properties, and how it compares to ctbns. we also provide algorithms for learning such animal models from data, description and discuss its applicability to biological physiological sequence evolution.", "histories": [["v1", "Wed, 27 Jun 2012 16:19:16 GMT  (380kb)", "http://arxiv.org/abs/1206.6838v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["tal el-hay", "nir friedman", "daphne koller", "raz kupferman"], "accepted": false, "id": "1206.6838"}, "pdf": {"name": "1206.6838.pdf", "metadata": {"source": "CRF", "title": "Continuous Time Markov Networks", "authors": ["Tal El-Hay", "Nir Friedman"], "emails": ["tale@cs.huji.ac.il", "nir@cs.huji.ac.il", "koller@cs.stanford.edu", "raz@math.huji.ac.il"], "sections": [{"heading": "1 Introduction", "text": "In many applications, we reason about processes that evolve over time. Such processes can involve short time scales (e.g., the dynamics of molecules) or very long ones (e.g., evolution). In both examples, there is no obvious discrete \u201ctime unit\u201d by which the process evolves. Rather, it is more natural to view the process as changing in a continuous time: the system is in some state for a certain duration,\nand then transitions to another state. The language of continuous time Markov processes (CTMPs) provides an elegant mathematical framework to reason about the probability of trajectories of such systems. Unfortunately, when we consider a system with multiple components, this representation grows exponentially in the number of components. Thus, we aim to construct a representation language for CTMPs that can compactly encode natural processes over high-dimensional state spaces. Importantly, the representation should also facilitate effective inference and learning.\nRecently, Nodelman et al. [8, 9, 10, 11] introduced the representation language of continuous time Bayesian networks (CTBNs), which provides a factorized, componentbased representation of CTMP: each component is characterized by a conditional CTMP dynamics, which describes its local evolution as a function of the current state of its parents in the network. This representation is natural for describing systems with a sparse structure of local influences between components. Nodelman et al. provide algorithms for efficient approximate inference in CTBNs, and for learning them from both complete and incomplete data.\nIn this paper, we introduce continuous timeMarkov networks, which have a different representational bias. Our motivating example is modeling the evolution of biological sequences such as proteins. In this example, the state of the system at any given time is the sequence of amino acids encoded by the gene of interest. As evolution progresses, the sequence is continually modified by local mutations that change individual amino acids. The mutations for different amino acids occur independently, but the probability that these local mutations survive depends on global aspects of the new sequence. For example, a mutation may be accepted only if the new sequence of amino acids folds properly into a functional protein, which occurs only if pairs of amino acids that are in contact with each other in the folded protein have complementary charges. Thus, although the modifications are local, global constraints on the protein structure and function introduce dependencies.\nTo capture such situations, we introduce a representation where we specify the dynamics of the process using two components. The first is a proposal process that attempts to change individual components of the system. In our example, this process will determine the rate of random\nmutations in protein sequences. The second is an equilibrium distribution, which encodes preferences over global configurations of the system. In our example, an approximation of the fitness of the folded protein. The equilibrium distribution is a static quantity that encodes preferences among states of the system, rather than dynamics of changes. The actual dynamics of the system are determined by the interplay between these two forces: local mutations and global fitness. We represent the equilibrium distribution compactly using a Markov network, or, more generally, a feature-based log-linear model.\nImportantly, as we shall see, the equilibrium distribution parameter is indeed the equilibrium distribution of the process. Thus, our representation provides an explicit representation of both the dynamics of the system and its asymptotic limit. Moreover, this representation ensures that the equilibrium distribution has a pre-specified simple structure. Thus, we can view our framework as a continuous-time Markov network (CTMN) \u2014 a Markov network that evolves over continuous time. From a different perspective, our representation allows us to capture a family of temporal processes whose stationary distribution has a certain locality structure. Such processes occur often in biological and physical systems. For example, recent results of Socolich et al. [13] suggest that pairwise Markov networks can fairly accurately capture the fitness of protein sequences.\nWe provide a reduction from CTMNs to CTBNs, allowing us to use CTBN algorithms [7, 11] to perform effective approximate inference in CTMNs. More importantly, we also provide a procedure for learning CTMN parameters from data. This procedure allows us to estimate the stationary distribution from observations of the system\u2019s dynamics. This is important in applications where the stationary distribution provides insight about the domain of application. In the protein evolution example, the stationary distribution provides a description of the evolutionary forces that shape the protein and thus gives important clues about protein structure and function."}, {"heading": "2 Reversible Continuous Time Markov Processes", "text": "We now briefly summarize the relevant properties of continuous time Markov processes that will be needed below. We refer the interested reader to Taylor and Karlin [14] and Chung [2] for more thorough expositions. Suppose we have a family of random variables {X(t) : t \u2265 0} where the continuous index t denotes time. A joint distribution over these random variables is a homogeneous continuous time Markov process (CTMP) if it satisfies theMarkov property\nPr(X(tk+1)|X(tk), . . . ,X(t0)) = Pr(X(tk+1)|X(tk))\nfor all tk+1 > tk > . . . > t0, and time-homogeneity,\nPr(X(s+ t) = y|X(s) = x) = Pr(X(s\u2032 + t) = y|X(s\u2032) = x)\nfor all s, s\u2032 and t > 0. The dynamics of a CTMP are fully determined by the Markov transition function,\npx,y(t) = Pr(X(s+ t) = y|X(s) = x),\nwhere time-homogeneity implies that the right hand side does not depend on s. Provided that the transition function satisfies certain analytical properties (see [2]) the dynamics are fully captured by a constant matrix Q \u2014 the rate, or intensity matrix \u2014whose entries qx,y are defined by\nqx,y = lim h\u21930 px,y(h)\u2212 1{x = y} h , (1)\nwhere 1{} is the indicator function which takes the value 1 when the condition in the argument holds and 0 otherwise. TheMarkov process can also be viewed as a generative process: The process starts in some state x. After spending a finite amount of time at x, it transitions, at a random time, to a random state y 6= x. The transition times to the various states are exponentially distributed, with rate parameters qx,y . The diagonal elements ofQ are set to ensure the constraint that each row sums up to zero.\nIf the process satisfies certain conditions (reachability) then the limit\n\u03c0x = lim t\u2192\u221e py,x(t)\nexists and is independent of the initial state y. That is, in the long time limit, the probability of visiting state x is independent of the initial state at time 0. The distribution \u03c0x is called the stationary distribution of the process. A CTMP is called stationary if P (X(0) = x) = \u03c0x, that is, if the initial state is sampled from the stationary distribution. A stationary CTMP is called reversible if for every x,y, and t > 0\nPr(X(t) = y|X(0) = x) = Pr(X(0) = y|X(t) = x).\nThis condition implies that the process is statistically equivalent to itself running backward in time. Reversibility is intrinsic to many physical systems where the microscopic dynamics are time-reversible. Reversibility can be formulated as a property on the Markov transition function, where for every x,y, and t > 0\n\u03c0xpx,y(t) = \u03c0ypy,x(t).\nThis identity is known as the detailed balance condition. To better understand the constraint, we can examine the implications of reversibility on the rate matrixQ.\nProposition 2.1: A CTMP is reversible if and only if its rate matrix can be expressed as\nqx,y = \u03c0ysx,y,\nwhere sx,y are the entries of a symmetric matrix (that is, sx,y = sy,x).\nIn other words, in a reversible CTMP, the asymmetry in transition rates can be interpreted as resulting entirely from preferences of the stationary distribution."}, {"heading": "3 Continuous Time Metropolis Processes", "text": "We start by considering a reformulation of reversible CTMPs as a continuous time version of the Metropolis sampling process. We view the process as an interplay between two factors. The first is an unbiased random process that attempts to transition between states of the system, and the second is the tendency of the system to remain in more probable states. This latter probability is taken to be the stationary distribution of the process. The structure of the process can be thought of as going through iterations of proposed transitions that are either accepted or rejected, similar to the Metropolis sampler [6].\nTo formally describe such a process, we need to describe these two components. The first is the unbiased proposal of transitions. These proposals occur at fixed rates. We denote by rx,y the rate at which proposals to transition x \u2192 y occur. This in effect defines a CTMP process with rate matrixR. To ensure an unbiased proposal, we require R to be symmetric. (The stationary distribution of a symmetric rate matrix is the uniform distribution.)\nThe second component is a decision whether to accept or reject the proposed transition. The decision whether to accept the transition x \u2192 y depends on the probability ratio of these states at equilibrium. We assume that we are given a target distribution, which should coincide with the equilibrium distribution \u03c0. As we shall see, to reach the target equilibrium distribution, the acceptance probability should satisfy a simple condition. To make this precise, we assume we have an acceptance function f that takes as an argument the ratio \u03c0y/\u03c0x and returns the probability of accepting transition x \u2192 y. This function should return a value between 0 and 1, and satisfy the functional relation\nf(z) = zf ( 1 z ) . (2)\nTwo functions that satisfy these conditions are\nfMetropolis(z) = min(1, z)\nflogistic(z) = 1\n1 + 1z .\nThe function fMetropolis is the standard one used in Metropolis sampling. The function flogistic is closely linked to logistic regression. It is continuously differentiable, which, as we shall see, facilitates the subsequent analysis.\nFormally, a continuous time Metropolis process is defined by a symmetric matrix R, a distribution \u03c0, and a real-valued function f . The semantics of the process are defined in a generative manner. Starting at an initial state x, the system remains in the state until receiving a proposed transition x \u2192 y with rate rx,y . This proposal is then accepted with probability f(\u03c0y/\u03c0x). If it is accepted, the system transitions to state y; otherwise it remains in state x. This process is repeated indefinitely.\nTo formulate the statistical dynamics of the system, consider a short time interval h. In this case, the probability of a proposal of the transition x \u2192 y is roughly h \u00b7 rx,y . Since the proposed transition is accepted with probability f(\u03c0y/\u03c0x), we have:\npx,y(h) \u2248 h \u00b7 rx,y \u00b7 f ( \u03c0y \u03c0x ) .\nPlugging this into Eq. (1) we conclude that the off-diagonal elements ofQ are\nqx,y = rx,y \u00b7 f ( \u03c0y \u03c0x ) . (3)\nProposition 3.1: Consider a continuous time Metropolis process defined as in Eq. (3). Then, this CTMP is reversible, and its stationary distribution is \u03c0.\nProof: The condition on f implies that\n1 \u03c0y f ( \u03c0y \u03c0x ) = 1 \u03c0x f ( \u03c0x \u03c0y ) ,\nThus, it follows that qx,y is of the form sx,y\u03c0y , i.e., that the process is reversible. Moreover, it implies that the stationary distribution of the process is \u03c0.\nThe inverse result is also easy to obtain.\nProposition 3.2: Any reversible CTMP can be represented as a continuous time Metropolis process.\nProof: According to Proposition 2.1 we can write qx,y = \u03c0ysx,y for a symmetric matrix sx,y . Define\nrx,y = sx,y \u03c0y f ( \u03c0y \u03c0x ) , so that qx,y = rx,y \u00b7 f ( \u03c0y \u03c0x ) . Together, sx,y = sy,x and Eq. (2) imply that rx,y = ry,x. Thus, R is symmetric and together with \u03c0 defines a continuous time Metropolis process which is equivalent to the original reversible CTMP.\nWe conclude that continuous timeMetropolis processes are a general reparameterization of reversible CTMPs."}, {"heading": "4 Continuous Time Markov Networks", "text": "We are interested in dealing with structured, multicomponent systems, whose state description can be viewed\nas an assignment to some set of state variables X = \u3008X1, X2, . . . , Xn\u3009, where each Xi assumes a finite set of values. The main challenge is dealing with the large state space (exponential in n). We aim to find succinct representations of the system\u2019s dynamics within the framework of continuous time Metropolis processes. We do so in two stages, first dealing with the proposal rate matrix R, and then with the equilibrium distribution \u03c0.\nOur first assumption is that proposed transitions are local. Specifically, we require that, for x 6= y\nrx,y = { rixi,yi (xj = yj) \u2200j 6= i 0 otherwise (4)\nwhere Ri = {rixi,yi} are symmetric local transition rates for Xi. Thus, we allow only one component to change at a time and the proposal rates do not depend on the global state of the system.\nThe second assumption concerns the structure of the stationary distribution \u03c0. Log-linear models or Markov networks provide a general framework to describe structured distributions. A log-linear model is described by a set of features, each one encoding a local property of the system that involves few variables. For example, the function 1{X1 = X2} is a feature that only involves two variables.\nA feature-based Markov network is defined by a vector of features, s = \u3008s1, . . . , sK\u3009, where each feature sk assigns a real number to the state of the system. We further assume that each feature sk is a function of a (usually small) subset Dk \u2286 X of variables. We use the notation x|Dk to denote the projection of x on the subset of variables Dk. Thus, sk is a function of x|Dk ; however, for notational convenience, we sometimes use sk(x) as a shorthand for sk(x|Dk).\nBased on a set of features, we define a distribution by assigning different weights to each feature. These weights represent the relative importance of each feature. We use the notation \u03b8 = \u3008\u03b81, . . . , \u03b8K\u3009 \u2208 IRK to denote the vector of weights or parameters. The equilibrium distribution represented by s and \u03b8 takes the log-linear form\n\u03c0x = 1\nZ(\u03b8) exp {\u2211 k \u03b8k \u00b7 sk(x|Dk) } , (5)\nwhere the partition function Z(\u03b8) is the normalizing factor. The structure of the equilibrium distribution can be represented as an undirected graph G \u2014 the nodes of G represent the variables {X1, . . . , Xn}. If Xi, Xj \u2208 Dk for some k, then there is an edge between the corresponding nodes. Thus, for every feature sk, the nodes that represent the variables inDk form a clique in the graph G. We define theMarkov Blanket, NG(i), of the variableXi as the set of neighbors of Xi in the graph G [12].\nExample 4.1 : Consider a four-variable process {X1, X2, X3, X4}, where each variable takes binary\nvalues, with the following set of features:\ns1(X1) = 1{X1 = 1} s5(X1, X2) = 1{X1 = X2} s2(X2) = 1{X2 = 1} s6(X2, X3) = 1{X2 = X3} s3(X3) = 1{X3 = 1} s7(X3, X4) = 1{X3 = X4} s4(X4) = 1{X4 = 1} s8(X1, X4) = 1{X1 = X4}\nNote that all these features involve at most two variables. The corresponding graph structure is shown in Figure 1(a). In this example N(1) = {X2, X4}, N(2) = {X1, X3}, N(3) = {X2, X4}, and N(4) = {X1, X3}.\nWe now take advantage of the structured representation of both R and \u03c0 to get a more succinct representation of the rate matrix Q of the process. We exploit the facts that \u03c0 appears explicitly in the rate only as a ratio \u03c0y/\u03c0x, and moreover that the proposal process includes only transitions that modify a single variable. Thus, we only examine ratios where y and x agree on all variables but one. It is straightforward to show that if x and y are two states that are identical except for the value of Xi and ui = x|N(i), then \u03c0y/\u03c0x = gi(xi \u2192 yi|ui), where\ngi(xi \u2192 yi|ui) =\nexp { \u2211 k:Xi\u2208Dk \u03b8k[sk(yi,ui)\u2212 sk(xi,ui)] } .\nNote that, ifXi \u2208Dk, thenDk \u2286 N(i)\u222a{Xi}. Thus, the function gi is well defined.\nThus, the acceptance probability of a change in Xi depends only on the state of variables in its Markov blanket. This property is heavily used for Gibbs sampling inMarkov networks. Depending on the choice of features, these dependencies can be very sparse, or involve all the variables in the process.\nTo summarize, assuming a local form for R and a loglinear form for \u03c0, we can further simplify the definition of the rate matrixQ. If x and y are two states that differ only in the i\u2019th variable, then\nqx,y = rixi,yif(gi(xi \u2192 yi|ui)), (6)\nwhere ui = x|N(i). All other off-diagonal entries are 0, and the diagonal entries are set to ensure that the sum of\neach row is 0. We call a process with a Q matrix of the form Eq. (6) a Continuous time Markov Network (CTMN).\nOne consequence of the form of the CTMN rate matrix Eq. (6) is that the dynamics of the i\u2019th variable depend directly only on the dynamics of its neighbors. As we can expect, we can use this property to discuss independencies among variables in the network. However, since we are examining a continuous process, we need to consider independencies between full trajectories (see also [8]).\nTheorem 4.2: Consider a CTMN with a stationary distribution represented by a graph G. If A,B,C are subsets of X such that C separates A from B in G, then the trajectories of A and B are conditionally independent, given observation of the full trajectory of C.\nProof: (sketch) Using the global independence properties of a Markov network (see for example, [12]), we have that \u03c0 can be written as a product of two function each with its own domain X1 and X2 such that X1 \u2229 X2 = C and A \u2286 X1 and B \u2286 X2. Once the trajectories of variables inC are given, the dynamics of variables inX1 \u2212C and X2 \u2212 C are two independent CTMNs, each with its own stationary distribution. As a consequence we get the desired independence.\nThat is, the usual conditional separation criterion in Markov networks [12] applies in a trajectory-wise fashion to CTMNs.\nIt is important to note that although we can represent any reversible CTMP as a continuous time Metropolis process, once we move to CTMNs this is no longer the case. The main restriction is that, in CTMNs as we have defined them, each transition involves a change in the state of exactly one component. Thus, although the language of Markov networks allow to describe arbitrary equilibrium distributions (potentially with an exponential number of features), the restrictions on R limit the range of processes we can describe as CTMNs. As an example of a domain where CTMNs are not suitable, consider reasoning about biochemical systems, where each component of the state is the number of molecules of a particular species and transitions correspond to chemical reactions. For example, a reaction might be one that takes an OH molecule and an H molecule and replace them by an H2O molecule. If reactions are reversible (i.e., H2O can break into OH and H molecules), then this process might be described by a reversible CTMP. However, since reactions change several components at once, we cannot describe such system as a CTMN."}, {"heading": "5 Connection to CTBNs", "text": "The factored form of Eq. (6) allows us to relate CTMNs with CTBNs. A CTBN is defined by a directed (often cyclic) graph whose nodes correspond to variables of the process, and whose edges represent direct influences of one variable on the evolution of another. More precisely,\na CTBN is defined by a collection of conditional rate matrices (also called conditional intensity matrices). For each Xi, and for each possible value ui of its direct parents in the CTBN graph, the matrix QXi|ui is a rate matrix over the state space of Xi. These conditional rate matrices are combined into a global rate matrix by a process Nodelman et al. [9] call amalgamation. Briefly, if x and y are identical except for the value of Xi, then\nqx,y = qXi|uixi,yi (7)\nwhere ui = x|Pai is the assignment to Xi\u2019s parents in the state x. That is, the rate of transition from x to y is the conditional rate of Xi changing from xi to yi given the state of its parents. Again, all other off-diagonal elements, where more than one variable changes, are set to 0.\nThis form is similar to the rate matrix of CTMNs shown in Eq. (6). Thus, given a CTMN, we can build an equivalent CTBN by setting the parents of each Xi to be N(i), and using the conditional rates:\nqXi|uixi,yi = r i xi,yigi(xi \u2192 yi | x|N(i)) (8)\nFigure 1(b) shows the CTBN structures corresponding to the CTMN of Example 4.1. In general, the CTBN graph corresponding to a given CTMN is built by replacing each undirected arc by a pair of directed ones. This matches the intuition that if Xi and Xj appear in the context of some feature, then they mutually influence each other.\nAs this transformation shows, the class of processes that can be encoded using CTMNs is a subclass of CTBNs. In a sense, this is not surprising, as a CTBN can encode any Markov process where at most one variable can transition at a time. However, the CTMN representation imposes a particular parametrization of the system dynamics in terms of the local proposal process and the global equilibrium distribution. This parametrization violates both local and global parameter independence [5] in the resulting CTBN. In particular, a transition between xi and yi is proposed at the same rate, regardless of whether it is globally advantageous (in terms of equilibrium preferences). As we shall see, this property is important for our ability to effectively estimate these rate parameters.\nMoreover, as we have seen, this parametrization guarantees that the stationary distribution of the process factorizes as a particular Markov network. In general, even a fairly sparse CTBN gives rise to a fully entangled stationary distribution that cannot be factorized. Indeed, even computing the stationary distribution of a given CTBN is a hard computational problem. By contrast, we have defined a model of temporal dynamics that gives rise to a natural and interpretable form for the stationary distribution. This property is critical in applications where the stationary distribution is the key element in understanding the system.\nYet, the ability to transform a CTMN into a CTBN allows us to harness the recently developed approximate in-\nference methods for CTBNs [11, 7], including for the Estep used when learning CTMNs for partially observable data."}, {"heading": "6 Parameter Learning", "text": "We now consider the problem of learning the parametrization of CTMNs from data. Thus, we assume we are given the form of \u03c0, that is, the set of features s, and need to learn the parameters \u03b8 governing \u03c0 and the local rate matrices Ri that govern the proposal rates for each variable. We start by considering this problem in the context of complete data, where our observations consist of full trajectories of the system. As we show, we define a gradient ascent procedure for learning the parameters from such data.\nThis result also enables us to learn from incomplete data using the standard EM procedure. Namely, we can use existing CTBNs inference algorithms to perform the E-step effectively when learning from partially observable data to compute expected sufficient statistics. The M-step is then an application of the learning procedure for complete data with these expected sufficient statistics. This combination is quite standard and follows the lines of similar procedure for CTBNs [10], and therefore we do not expand on it here."}, {"heading": "6.1 The Likelihood Function", "text": "A key concept in addressing the learning problem is the likelihood function, which determines how the probability of the observations depends on the parameters.\nWe assume that the data is complete, and thus our observations consist of a trajectory of the system that can be described as a sequence of intervals, where in each interval the system is in one state. Using the relationship to CTBNs, we can use the results of Nodelman et al. [9] to write the probability of the data as a function of sufficient statistics and entries in the conditional rate matrices of Eq. (8). A problem with this approach is that the entries in the conditional rate matrix involve both parameters from Ri and parameters from \u03b8. Thus, the resulting likelihood function couples the estimation of these two sets of parameters.\nHowever, if we had additional information, we could decouple these two sets of parameters. Suppose we observe not only the actual trajectories, but also the rejected proposals; see Figure 2. With this additional information, we can estimate the rate of different proposals, independently of whether they were accepted or not. Similarly, we can estimate the equilibrium distribution from the accepted and rejected proposals. Thus, we are going to view our learning problem as a partial data problem where the annotation of rejected proposals is the missing data.\nTo formalize these ideas, assume that our evidence is a trajectory annotated with proposal attempts. We describe such a trajectory using three vectors; see Figure 2. The first vector, \u03c4 = \u3008\u03c4 [1], . . . , \u03c4 [M + 1]\u3009, represents the time intervals between consecutive proposals. Thus, the first pro-\nposal took place at time \u03c4 [1], the second at time \u03c4 [1]+\u03c4 [2], and so on. The last entry in this vector is the time between the last proposal and the end of the observed time interval. The second vector, \u039e = \u3008x[0],x[1], . . . ,x[M ]\u3009, denotes the actual state of the system after each proposal was made. Thus, x[0] is the initial state of the system, x[1] is the state after the first proposal, and so on. Finally, \u03a5 = \u3008y[1], . . . ,y[M ]\u3009 denotes the sequence of proposed states. Clearly, the m\u2019th proposal was accepted if y[m] = x[m] and rejected otherwise. We denote these event using the indicators S[m] = 1{x[m] = y[m]}.\nThe likelihood of these observations is the product of the probability density of the duration between proposals, and the probability of accepting or rejecting each proposal. Plugging in the factored form ofR and \u03c0 we can write this likelihood in a compact form.\nProposition 6.1: Given an augmented data set, \u03c4 , \u039e, and \u03a5, the log-likelihood can be decomposed as\n`(\u03b8, {Ri} : \u03c4 ,\u039e,\u03a5) = n\u2211\ni=1\n`r,i(Ri : \u03c4 ) + `s(\u03b8 : \u039e,\u03a5),\nsuch that\n`r,i(Ri : \u03c4 ) = \u2211\nxi 6=yi\n( M [xi, yi] ln rixi,yi \u2212 r i xi,yiT [xi] )\nand\n`s(\u03b8 : \u039e,\u03a5) = n\u2211\ni=1 \u2211 ui \u2211 xi 6=yi Ma [xi, yi|ui] ln f(gi(xi \u2192 yi|ui)) +\nn\u2211 i=1 \u2211 ui \u2211 xi 6=yi Mr [xi, yi|ui] ln(1\u2212 f(gi(xi \u2192 yi|ui)))\nwhereMa [xi, yi|ui] is the number of accepted transitions of Xi from xi to yi when N(i) = ui, Mr [xi, yi|ui] is the count of rejected proposals to make the same transition, M [xi, yi|ui] =Ma [xi, yi|ui]+Mr [xi, yi|ui], and T [xi] is the time spent in states where Xi = xi.\nNote that if we use flogistic, then, as ln((1 + e\u2212x)\u22121) is concave, the likelihood function `s(\u03b8 : \u039e,\u03a5) is concave and has a unique maximum."}, {"heading": "6.2 Maximizing the Likelihood Function", "text": "Under the Maximum Likelihood Principle, our estimated parameters are the ones that maximize the likelihood function given the observations. We now examine how to maximize the likelihood. The decoupling of the likelihood into several terms allows us to estimate each set of parameters separately.\nThe estimation of Ri is straightforward: imposing the symmetry condition, the maximum likelihood estimate is\nrixi,yi = M [xi, yi] +M [yi, xi]\nT [xi] + T [yi] .\nFinding the maximum likelihood parameters of \u03c0 is somewhat more involved. Note that the likelihood `s(\u03b8 : \u039e,\u03a5) is quite different from the likelihood of a log-linear distribution given i.i.d. data [3]. The probability of acceptance or rejection involves ratios of probabilities. Therefore, the partition function Z(\u03b8) cancels out, and does not appear in the likelihood.\nIn a sense, our likelihood is closely related to the pseudo-likelihood for log-linear models [1]. Recall that pseudo-likelihood is a technique for estimating the parameters of a Markov network (or log-linear model) that uses a different objective function. Rather than optimizing the joint likelihood, one optimizes a sum of log conditional likelihood terms, one for each variable given its neighbors. By considering the conditional probability of a variable given its neighbors, the partition function cancels out, allowing the parameters to be estimated without the use of inference. At the large sample limit, optimizing the pseudolikelihood criterion is equivalent to optimizing the joint likelihood, but the results for finite sample sizes tend to be worse. In our setting, the generative model is defined in terms of ratios only. Thus, in this case the exact likelihood turns out to take a form similar to the pseudo-likelihood criterion. As for pseudo-likelihood, this form allows us to\nperform parameter estimation without requiring inference in the underlying Markov network.\nIn the absence of an analytical solution for this equation we learn the parameters using a gradient-based optimization procedure to find a (local) maximum of the likelihood. The derivation of the gradient is a standard exercise; for completeness, we provide the details in the appendix. When using flogistic we are guaranteed that such a procedure finds the unique global maximum."}, {"heading": "6.3 Completing the Data", "text": "Our derivation of the likelihood and the associated optimization procedure relies on the assumption that rejected transition attempts are also observed in the data. As we can see from the form of the likelihood, these failures play an important role in estimating the parameters. The question is how to adapt the procedure to the case where rejected proposals are not observed. Our solution to this problem is to use Expectation Maximization, where we view the proposal attempts as the unobserved variables.\nIn this approach, we start with an initial guess of the model parameters. We use these to estimate the expected number of rejected proposals; we then treat these expected counts as though they were real, and maximize the likelihood using the procedure described in the previous section. We repeat these iterations until convergence.\nThe question is how to compute the expected number of rejected attempts. It turns out that this computation can be done analytically.\nProposition 6.2: Given a CTMN, and an observed trajectory \u03c4 ,\u039e. Then,\nIE[Mr [xi, yi|ui] |D] (9) = T [xi|ui] rixi,yi (1\u2212 f(g(xi, yi|ui)))\nwhere T [xi|ui] is the total amount of time the system was in states where Xi = xi and N(i) = ui.\nWe see that, in this case, the E-step of EM is fairly straightforward. The harder step is the M-step which requires an iterative gradient-based optimization procedure.\nTo summarize the procedure, to learn from complete data we perform the following steps: We first collect sufficient statistics T [xi|ui] andMa [xi, yi|ui]. We then initialize the model with some set of parameters (randomly, or using prior knowledge). We then iterate over the two steps of EM until convergence: in the E-step, we complete the sufficient statistics with the expected number of rejected attempts, as per Eq. (9); in the M-step, we perform maximum likelihood estimation using the expected sufficient statistics, using gradient descent with the gradient of Eq. (10)."}, {"heading": "7 A Numerical Example", "text": "To illustrate the properties of our CTMN learning procedure, we evaluated it on a small synthetic data set. We\ngenerated data from the CTMN model of Example 4.1 with \u03b8 = \u3008\u22120.2,\u22122.3, 0.7, 0.7,\u22121.2,\u22121.2,\u22121.2,\u22121.2\u3009 and proposal rates r10,1 = 1, r 2 0,1 = 2, r 3 0,1 = 3, and r40,1 = 4. The goal of our experiments is to test the ability of the CTMN learning procedure to estimate stationary distributions from data in various conditions. As a benchmark, we compared our procedure to two alternative methods:\n\u2022 A procedure that estimates the stationary distribution directly from the frequency of visiting each state. This procedure is essentially the standard parameter learning method for Markov networks, where the weight of each state (instance) is proportional to the duration in which the process spends in that state. This procedure uses gradient ascent to maximize the likelihood [3]. When the process is sampling from the stationary distribution, the relative time in each state is proportional to its stationary probability, and in such situations we expect this procedure to perform well.\n\u2022 A procedure that estimates the Q-matrix of the associated CTBN shown in Figure. 1. Here we used the methods designed for parameter learning of CTBNs in [9]. Once that theQ-matrix has been estimated, the\nestimated stationary distribution is the only normalized vector in its null space.\nWe examined these three procedures in three sets of synthetic trajectories. The first set was generated by sampling the initial stateX(0) of each trajectory from the stationary distribution and then sampling further states and durations from the target model. In this data set the system is in equilibrium throughout the trajectory. The second data set was generated by sampling the initial state from a uniform distribution, and so the system starts in a distribution that is far from equilibrium. However, the trajectory is long enough to let the system equilibrate. The third data set is similar to the second, except that trajectories are shorter and thus do not have sufficient time to equilibrate. To evaluate the effect of training set size, we repeated the learning experiments with different numbers of trajectories. We report the size of the training set in terms of the total length of training trajectories. Time is reported in units of expected transition number. That is, one time unit is equal to the average time between transitions when the process is in equilibrium. The short and long trajectories in our experiments are of length 10 and 25 expected transitions, respectively.\nTo evaluate the quality of the learned distribution, we\nmeasured the Kullback-Leibler divergences from the true stationary distribution to the estimated ones. Figures 3(ac) show the results of these experiments. When sampling from the stationary distribution, the three procedures tend, as the data size increases, toward the correct distribution. For small data size, the performance of the CTMN learning procedure is consistently superior, although the error bars partially overlap. We start seeing a difference between the estimation procedures when we modify the initial distribution. As expected, the Markov network learning procedure suffers since it is learning from a biased sample. On the other hand, the performance of the CTMN and CTBN learning procedures is virtually unchanged, even when we modify the length of the trajectories. These results illustrate the ability of the CTMN and CTBN learning procedures to robustly estimate the equilibrium distribution from the dynamics even when the sampled process is not at equilibrium.\nTo test the robustness to the network structure, we also tested the performance of these procedures when estimating using a wrong structure. As we can see in Figures 3(df), while the three procedures converge to the wrong distribution, their relative behavior remains similar to the previous experiment, and the performance of the CTMN learning procedure is still not affected by the nature of the data."}, {"heading": "8 Discussion and Future Work", "text": "In this paper, we define the framework of continuous time Markov networks, where we model a dynamical system as being governed by two factors: a local transition model, and a global acceptance/rejection model (based on an equilibrium distribution). By using a Markov network (or feature-based log-linear model) to encode the equilibrium distribution, we naturally define a temporal process guaranteed to have an equilibrium distribution of a particular, factored form. We showed a reduction from CTMNs to CTBNs that illustrates the differences in the expressive powers of the two formalisms. Moreover, this reduction allows us to reason in CTMNs by exploiting the efficient approximate inference algorithms for CTBNs. Finally, we provided learning algorithms for CTMNs, which allow us to learn the equilibrium distribution in a way that exploits our understanding about the system dynamics. We demonstrated on that this learning procedure is able to robustly estimate the equilibrium distribution even when the sampled process is not at equilibrium. These results can be combined for learning from partial observations, by plugging in the learning procedure as the M-step in the EM procedure for CTBNs [10].\nThis work opens many interesting questions. A key goal in learning these models is to estimate the stationary distribution. It is interesting to analyze, both theoretically and empirically, the benefit gained in this task by accounting for the process dynamics, as compared to learning the stationary distribution directly from a set of snapshots of\nthe system (e.g., a set of instances of a protein sequence in different species). Moreover, so far, we have tackled only the problem of parameter estimation in these models. In many applications, the model structure is unknown, and of great interest. For example, in models of protein evolution, we want to know which pair of positions in the protein are directly correlated, and therefore likely to be structurally interacting. Of course, tackling this problem involves learning the structure of a Markov network, a notoriously difficult task. From the perspective of inference, our reduction to CTBNs can lose much of the structure of the model. For example, if the stationary distribution is a pairwise Markov network, the fact that the interaction model decomposes over pairs of variables is lost in the induced CTBN. It is interesting to see whether one can construct inference algorithms that better exploit this structure. Finally, one important limitation of the CTMN framework is the restriction to an exponential distribution on the duration between proposed state changes. Although such a model is a reasonable one in many systems (e.g., biological sequence evolution), there are other settings where it is too restrictive. In recent work, Nodelman et al. [10] show how one can expand the framework of CTBNs to allow a richer set of duration distributions. Essentially, their solution introduces a \u201chidden state\u201d internal to a variable, so that the overall transition model of the variable is actually the aggregate of multiple transitions of its internal state. A similar solution can be applied in our setting, but the resulting model would not generally encode a reversible CTMP.\nOne major potential field of application for this class of models is sequence evolution. The current state of the art in phylogenetic inference is based on continuous time probabilistic models of evolution [4]. Virtually all of these models assume that sequence positions evolve independently of each other (although in some models, there are global parameters that induce weak dependencies). Our models provide a natural language for modeling such dependencies. In this domain, the proposal process corresponds to mutation rates within the sequence, and the equilibrium distribution is proportional to the relative fitness of different sequences. The latter function is of course very complex, but there is empirical evidence that modeling pairwise interactions can provide a good approximation [13]. Thus, in these systems, both the local mutation process and a factored equilibrium distribution are very appropriate, making CTMNs a potentially valuable tool for modeling and analysis. We hope to incorporate this formalism within phylogenetic inference tools, and to develop a methodology to leverage these models to provide new insights about the structure and function of proteins."}, {"heading": "Acknowledgments", "text": "We thank A. Jaimovich, T. Kaplan, M. Ninio, I. Wiener, and the anonymous reviewers for comments on earlier versions of this manuscript. This work was supported by\ngrants from the Israel Science Foundation and the USIsrael Binational Science Foundation, and by DARPA\u2019s CALO program, under sub-contract to SRI International."}, {"heading": "A Gradient for Learning CTMNs", "text": "We now compute the derivative of the gradient of the loglikelihood, as specified in Proposition 6.1. The parameters \u03b8 appear within the scope of the gi functions. Thus, to find the derivatives we differentiate these functions with respect to the parameters, and then apply the chain rule for derivatives:\n\u2202\n\u2202\u03b8k `s(\u03b8 : \u039e,\u03a5) = (10)\u2211\ni:Xi\u2208Dk \u2211 ui \u2211 xi 6=yi\n\u2206k(xi, yi|ui)(\u03c8a(xi, yi|ui)Ma [xi, yi|ui] \u2212 \u03c8r(xi, yi|ui)Mr [xi, yi|ui])\nwhere\n\u2206k(xi, yi|ui) = sk(ui, yi)\u2212 sk(ui, xi) \u03c8a(xi, yi|ui) = zf \u2032(z) f(z) \u2223\u2223\u2223\u2223 z=gi(xi\u2192yi|ui)\n\u03c8r(xi, yi|ui) = zf \u2032(z) 1\u2212 f(z) \u2223\u2223\u2223\u2223 z=gi(xi\u2192yi|ui)\nThis shows that the update of \u03b8k is a weighted combination of the contribution of each proposed transition. The weight of the transition depends on how sensitive the ratio of probabilities is to the feature, denoted by \u2206k(xi, yi|ui) and the number of times this transition was accepted or rejected, captured by the empirical counts. In addition, each proposal is weighted by \u03c8a(xi, yi|ui), which captures the improbability of the acceptance (respectively rejection for \u03c8r(xi, yi|ui)). The less probable they are, the larger the change in \u03b8k.\nWe can get better understanding of these terms if we consider their value for specific choices of f . For example, if we use flogistic, then\n\u03c8a(xi, yi|ui) = 1\u2212 flogistic(gi(xi \u2192 yi|ui)) \u03c8r(xi, yi|ui) = flogistic(gi(xi \u2192 yi|ui)),\nthat is, the rejection and acceptance probabilities, respectively. The smaller these values, the more probable was the acceptance (resp. rejection) and so it results in a smaller gradient of the likelihood in the direction of this parameter. When using fMetropolis the two functions are not symmetric:\n\u03c8a(xi, yi|ui) = 1{gi(xi \u2192 yi|ui) < 1} \u03c8r(xi, yi|ui) =\n1{gi(xi \u2192 yi|ui) > 1}flogistic(gi(xi \u2192 yi|ui))\nwith a discontinuity when gi(xi \u2192 yi|ui) = 1. We see that, in this case, the updates are asymmetric, with maximal weight to updates of accepted transitions."}], "references": [{"title": "On the statistical analysis of dirty pictures", "author": ["J. Besag"], "venue": "J. Roy. Stat. Soc. B Met.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1986}, {"title": "Markov chains with stationary transition probabilities", "author": ["K.L. Chung"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1960}, {"title": "Inducing features of random fields", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Mach. Learn.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Equation of state calculation by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "J. Chem. Phys.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1953}, {"title": "Continuous time particle filtering", "author": ["B. Ng", "A. Pfeffer", "R. Dearden"], "venue": "In IJCAI \u201905", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In UAI", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Learning continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In UAI", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Expectation maximization and complex duration distributions for continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In UAI", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Expectation propagation for continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In UAI", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1988}, {"title": "Evolutionary information for specifying a protein", "author": ["M. Socolich"], "venue": "fold. Nature,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "An Introduction to Stochastic Modeling", "author": ["H.M. Taylor", "S. Karlin"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}], "referenceMentions": [{"referenceID": 6, "context": "[8, 9, 10, 11] introduced the representation language of continuous time Bayesian networks (CTBNs), which provides a factorized, componentbased representation of CTMP: each component is characterized by a conditional CTMP dynamics, which describes its local evolution as a function of the current state of its parents in the network.", "startOffset": 0, "endOffset": 14}, {"referenceID": 7, "context": "[8, 9, 10, 11] introduced the representation language of continuous time Bayesian networks (CTBNs), which provides a factorized, componentbased representation of CTMP: each component is characterized by a conditional CTMP dynamics, which describes its local evolution as a function of the current state of its parents in the network.", "startOffset": 0, "endOffset": 14}, {"referenceID": 8, "context": "[8, 9, 10, 11] introduced the representation language of continuous time Bayesian networks (CTBNs), which provides a factorized, componentbased representation of CTMP: each component is characterized by a conditional CTMP dynamics, which describes its local evolution as a function of the current state of its parents in the network.", "startOffset": 0, "endOffset": 14}, {"referenceID": 9, "context": "[8, 9, 10, 11] introduced the representation language of continuous time Bayesian networks (CTBNs), which provides a factorized, componentbased representation of CTMP: each component is characterized by a conditional CTMP dynamics, which describes its local evolution as a function of the current state of its parents in the network.", "startOffset": 0, "endOffset": 14}, {"referenceID": 11, "context": "[13] suggest that pairwise Markov networks can fairly accurately capture the fitness of protein sequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We provide a reduction from CTMNs to CTBNs, allowing us to use CTBN algorithms [7, 11] to perform effective approximate inference in CTMNs.", "startOffset": 79, "endOffset": 86}, {"referenceID": 9, "context": "We provide a reduction from CTMNs to CTBNs, allowing us to use CTBN algorithms [7, 11] to perform effective approximate inference in CTMNs.", "startOffset": 79, "endOffset": 86}, {"referenceID": 12, "context": "We refer the interested reader to Taylor and Karlin [14] and Chung [2] for more thorough expositions.", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "We refer the interested reader to Taylor and Karlin [14] and Chung [2] for more thorough expositions.", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "Provided that the transition function satisfies certain analytical properties (see [2]) the dynamics are fully captured by a constant matrix Q \u2014 the rate, or intensity matrix \u2014whose entries qx,y are defined by", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "The structure of the process can be thought of as going through iterations of proposed transitions that are either accepted or rejected, similar to the Metropolis sampler [6].", "startOffset": 171, "endOffset": 174}, {"referenceID": 10, "context": "We define theMarkov Blanket, NG(i), of the variableXi as the set of neighbors of Xi in the graph G [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 6, "context": "However, since we are examining a continuous process, we need to consider independencies between full trajectories (see also [8]).", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "Proof: (sketch) Using the global independence properties of a Markov network (see for example, [12]), we have that \u03c0 can be written as a product of two function each with its own domain X1 and X2 such that X1 \u2229 X2 = C and A \u2286 X1 and B \u2286 X2.", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "That is, the usual conditional separation criterion in Markov networks [12] applies in a trajectory-wise fashion to CTMNs.", "startOffset": 71, "endOffset": 75}, {"referenceID": 7, "context": "[9] call amalgamation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "This parametrization violates both local and global parameter independence [5] in the resulting CTBN.", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "ference methods for CTBNs [11, 7], including for the Estep used when learning CTMNs for partially observable data.", "startOffset": 26, "endOffset": 33}, {"referenceID": 5, "context": "ference methods for CTBNs [11, 7], including for the Estep used when learning CTMNs for partially observable data.", "startOffset": 26, "endOffset": 33}, {"referenceID": 8, "context": "This combination is quite standard and follows the lines of similar procedure for CTBNs [10], and therefore we do not expand on it here.", "startOffset": 88, "endOffset": 92}, {"referenceID": 7, "context": "[9] to write the probability of the data as a function of sufficient statistics and entries in the conditional rate matrices of Eq.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The first vector, \u03c4 = \u3008\u03c4 [1], .", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "1 2 3 4 5 6 7 8 9 10 11 12 14 13 15 \u03c4[8]", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "y[8] x[8]", "startOffset": 1, "endOffset": 4}, {"referenceID": 6, "context": "y[8] x[8]", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "posal took place at time \u03c4 [1], the second at time \u03c4 [1]+\u03c4 [2], and so on.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "posal took place at time \u03c4 [1], the second at time \u03c4 [1]+\u03c4 [2], and so on.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "posal took place at time \u03c4 [1], the second at time \u03c4 [1]+\u03c4 [2], and so on.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "The second vector, \u039e = \u3008x[0],x[1], .", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Thus, x[0] is the initial state of the system, x[1] is the state after the first proposal, and so on.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "Finally, \u03a5 = \u3008y[1], .", "startOffset": 15, "endOffset": 18}, {"referenceID": 2, "context": "data [3].", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "In a sense, our likelihood is closely related to the pseudo-likelihood for log-linear models [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "This procedure uses gradient ascent to maximize the likelihood [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Here we used the methods designed for parameter learning of CTBNs in [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "These results can be combined for learning from partial observations, by plugging in the learning procedure as the M-step in the EM procedure for CTBNs [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 8, "context": "[10] show how one can expand the framework of CTBNs to allow a richer set of duration distributions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The latter function is of course very complex, but there is empirical evidence that modeling pairwise interactions can provide a good approximation [13].", "startOffset": 148, "endOffset": 152}], "year": 2006, "abstractText": "A central task in many applications is reasoning about processes that change over continuous time. Recently, Nodelman et al. introduced continuous time Bayesian networks (CTBNs), a structured representation for representing Continuous Time Markov Processes over a structured state space. In this paper, we introduce continuous time Markov networks (CTMNs), an alternative representation language that represents a different type of continuous-time dynamics, particularly appropriate for modeling biological and chemical systems. In this language, the dynamics of the process is described as an interplay between two forces: the tendency of each entity to change its state, which we model using a continuous-time proposal process that suggests possible local changes to the state of the system at different rates; and a global fitness or energy function of the entire system, governing the probability that a proposed change is accepted, which we capture by a Markov network that encodes the fitness of different states. We show that the fitness distribution is also the stationary distribution of the Markov process, so that this representation provides a characterization of a temporal process whose stationary distribution has a compact graphical representation. We describe the semantics of the representation, its basic properties, and how it compares to CTBNs. We also provide an algorithm for learning such models from data, and demonstrate its potential benefit over other learning approaches.", "creator": "TeX"}}}