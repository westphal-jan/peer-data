{"id": "1206.6844", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "From influence diagrams to multi-operator cluster DAGs", "abstract": "there exist several architectures to solve influence diagrams using local computations, such as the shenoy - shafer, the hugin, or the lazy propagation architectures. they only all extend while usual variable elimination algorithms thanks to the use of so - called'potentials '. in using this paper, we introduce a new architecture, called : the multi - operator cluster dag architecture, which can produce decompositions with an improved constrained induced - network width, and therefore induce any potentially exponential gains. its principle problem is to benefit from the composite implementation nature of influence diagrams, instead method of actively using uniform binding potentials, basically in order to better analyze the problem structure.", "histories": [["v1", "Wed, 27 Jun 2012 16:21:20 GMT  (169kb)", "http://arxiv.org/abs/1206.6844v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["cedric pralet", "thomas schiex", "gerard verfaillie"], "accepted": false, "id": "1206.6844"}, "pdf": {"name": "1206.6844.pdf", "metadata": {"source": "CRF", "title": "From influence diagrams to multi-operator cluster DAGs", "authors": ["C\u00e9dric Pralet", "Thomas Schiex"], "emails": [], "sections": [{"heading": null, "text": "There exist several architectures to solve influence diagrams using local computations, such as the Shenoy-Shafer, the HUGIN, or the Lazy Propagation architectures. They all extend usual variable elimination algorithms thanks to the use of so-called \u201cpotentials\u201d. In this paper, we introduce a new architecture, called the Multi-operator Cluster DAG architecture, which can produce decompositions with an improved constrained inducedwidth, and therefore induce potentially exponential gains. Its principle is to benefit from the composite nature of influence diagrams, instead of using uniform potentials, in order to better analyze the problem structure."}, {"heading": "1 INTRODUCTION", "text": "Since the first algorithms based on decision trees or arc-reversal operations [Shachter, 1986], several exact methods have been proposed to solve influence diagrams using local computations, such as the ones based on the Shenoy-Shafer, the HUGIN, or the Lazy Propagation architectures [Shenoy, 1992; Jensen et al., 1994; Madsen and Jensen, 1999]. These methods have successfully adapted classical Variable Elimination (VE) techniques (which are basically designed to compute one type of marginalization on a combination of local functions with only one type of combination operator), in order to handle the multiple types of information (probabilities and utilities), the multiple types of marginalizations (sum and max), and the multiple types of combination (\u00d7 for probabilities, + for utilities) involved in an influence diagram. The key mechanism used for such an extension consists in using elements known as potentials [Ndilikilikesha, 1994].\nIn this paper, we define a new architecture, called the Multi-operator Cluster DAG (MCDAG) architecture,\nwhich does not use potentials, but still relies on VE. Compared to existing schemes, MCDAGs actively exploit the composite nature of influence diagrams. We first present the potential-based approach and motivate the need for a new architecture (Section 2). Then, MCDAGs are introduced (Section 3) and a VE algorithm is defined (Section 4). Finally, this work is compared with existing approaches (Section 5) and extended to other frameworks (Section 6). All proofs are available in [Pralet et al., 2006b]."}, {"heading": "2 MOTIVATIONS", "text": "Notations and definitions An influence diagram [Howard and Matheson, 1984] is a composite graphical model defined on three sets of variables organized in a Directed Acyclic Graph (DAG) G: (1) a set C of chance variables x \u2208 C, for each of which a conditional probability distribution Px | pa(x) on x given its parents in G is specified; (2) a set D = {D1, . . . , Dq} (indices represent the order in which decisions are made) of decision variables x \u2208 D, for each of which pa(x) is the set of variables observed before decision x is made; (3) a set \u0393 of utility variables u \u2208 \u0393, each of which is associated with a utility function Upa(u) on pa(u) (and utility variables are leaves in the DAG).\nWe consider influence diagrams where the parents of a decision variable are parents of all subsequent decision variables (no-forgetting). The set of conditional probability distributions (one for each x \u2208 C) is denoted P and the set of utility functions (one for each u \u2208 \u0393) is denoted U . Each function \u03c6 \u2208 P \u222aU holds on a set of variables sc(\u03c6) called its scope, and is consequently called a scoped function (sc(Px | pa(x)) = {x} \u222a pa(x) and sc(Upa(u)) = pa(u)). The set of chance variables observed before the first decision is denoted I0, the set of chance variables observed between decisions Dk and Dk+1 is denoted Ik, and the set of chance variables unobserved before the last decision is denoted Iq . We use dom(x) to denote the domain of a vari-\nable x \u2208 C \u222a D, and by extension, for W \u2282 C \u222a D, dom(W ) = \u220f\nx\u2208W dom(x).\nThe usual problem associated with an influence diagram is to find decision rules maximizing the expected utility (a decision rule for a decision Dk is a function associating a value in dom(Dk) with any assignment of the variables observed before making decision Dk) As shown in [Jensen et al., 1994], this is equivalent to computing optimal decision rules for the quantity\n\u2211\nI0\nmax D1\n. . . \u2211\nIq\u22121\nmax Dq\n\u2211\nIq\n((\n\u220f\nPi\u2208P\nPi\n)\n\u00d7\n(\n\u2211\nUi\u2208U\nUi\n))\n(1)"}, {"heading": "2.1 THE \u201cPOTENTIAL\u201d APPROACH", "text": "With this approach, Equation 1 is reformulated using so-called potentials in order to use only one combination and one marginalization operator. A potential on a set of variables W is a pair \u03c0W = (pW , uW ), where pW and uW are respectively a nonnegative real function and a real function, whose scopes are included in W . The initial conditional probability distributions Pi \u2208 P are transformed into potentials (Pi, 0), whereas the initial utility functions Ui \u2208 U are transformed into potentials (1, Ui). On these potentials, a combination operation \u2297 and a marginalization (or elimination) operation \u2191 are defined:\n\u2022 the combination of \u03c0W1 = (pW1 , uW1) and \u03c0W2 = (pW2 , uW2) is the potential on W1 \u222aW2 given by \u03c0W1 \u2297 \u03c0W2 = (pW1 \u00d7 pW2 , uW1 + uW2);\n\u2022 the marginalization of \u03c0W = (pW , uW ) overW1 \u2282\nC equals \u03c0\u2191W1W = ( \u2211 W1 pW ,\nP\nW1 pWuW\nP\nW1 pW\n)\n(with\nthe convention 0/0 = 0), whereas the marginalization of \u03c0W = (pW , uW ) over W1 \u2282 D is given by \u03c0\u2191W1W = (pW ,maxW1 uW ).\nSolving the problem associated with an influence diagram is then equivalent to computing \u03b2 = ((\u00b7 \u00b7 \u00b7 ((\u03c0C\u222aD\n\u2191Iq )\u2191Dq )\u2191Iq\u22121 \u00b7 \u00b7 \u00b7 )\u2191D1)\u2191I0 , where \u03c0C\u222aD = (\u2297Pi\u2208P (Pi, 0))\u2297 (\u2297Ui\u2208U (1, Ui)) is the combination of the initial potentials. As \u2297 and \u2191 satisfy the ShenoyShafer axioms defined in [Shenoy, 1990], \u03b2 can be computed using usual VE algorithms [Jensen et al., 1994]. This explains why existing architectures like ShenoyShafer, HUGIN, or Lazy Propagation (LP1) use potentials to solve influence diagrams."}, {"heading": "2.2 QUANTIFYING THE COMPLEXITY", "text": "In the case of influence diagrams, the alternation of sum and max marginalizations, which do not gener-\n1The LP architecture actually uses potentials defined as pairs of set of functions (instead of pairs of functions).\nally commute, prevents from eliminating variables in any order. The complexity of VE can then be quantified using constrained induced-width [Jensen et al., 1994; Park and Darwiche, 2004] (instead of inducedwidth [Dechter and Fattah, 2001]).\nDefinition 1. Let G = (VG, HG) be a hypergraph 2 and let be a partial order on VG. The constrained induced-width of G with constraints on the elimination order given by (\u201cx \u227a y\u201d stands for \u201cy must be eliminated before x\u201d) is a parameter denoted wG( ). It is defined as wG( ) = mino\u2208lin( ) wG(o), lin( ) being the set of linearizations of to a total order on VG and wG(o) being the induced-width of G for the elimination order o (i.e. the size of the largest hyperedge created when eliminating variables in the order given by o).\nThe constrained induced-width can be used to give an upper bound on the complexity of existing potentialbased VE algorithms. Let Gp = (C \u222a D, {sc(\u03c6)|\u03c6 \u2208 P \u222aU}) be the hypergraph corresponding to the \u201cuntyped\u201d influence diagram. Let p be the partial order defined by I0 \u227ap D1, (Ik 6= \u2205) \u2192 (Dk \u227ap Ik \u227ap Dk+1), and Dq \u227ap Iq . Finally, let d be the maximum size of the variables domains. Then, with classical approaches based on potentials and strong junction trees [Jensen et al., 1994], which are junction trees with constraints on the marginalization order, the theoretical complexity is O(|P \u222aU | \u00b7 d1+wGp ( p)) (the number of elements of a finite set E is denoted |E|)."}, {"heading": "2.3 DECREASING THE CONSTRAINED INDUCED-WIDTH", "text": "The constrained-induced width is a guideline to show how the complexity can be decreased. In this direction, one can work on the two parameters on which it depends: the partial order , and the hypergraph G.\nWeakening the partial order\nProposition 1. Let G = (VG, HG) be a hypergraph and let 1, 2 be two partial orders on VG such that \u2200(x, y) \u2208 VG\u00d7VG, (x 2 y) \u2192 (x 1 y) ( 2 is weaker than 1). Then, wG( 1) \u2265 wG( 2).\nProposition 1 means that if one weakens , i.e. if one reveals some extra freedoms in the elimination order (e.g. by proving that some marginalizations with sum and max can commute), then the theoretical complexity may decrease. Though such a technique is known to be useless in contexts like Maximum A Posteriori hypothesis [Park and Darwiche, 2004], where there is only one alternation of max and sum marginalizations,\n2This means that VG is the set of variables (or vertices), and HG is a set of hyperedges on VG, i.e. a subset of 2 VG .\nit can lead to an exponential gain as soon as there are more than two levels of alternation.\nIndeed, assume that one wants to compute maxx1,...,xn \u2211 y maxxn+1 Py(Ux1,y + \u2211\n1\u2264i\u2264n Uxi,xn+1). On one hand, using 1 defined by {x1, . . . , xn} \u227a1 y \u227a1 xn+1 provides us with the constrained inducedwidth wG( 1) = n, since xn+1 is then necessarily eliminated first. On the other hand, the scopes of the functions involved enable us to infer that with 2 defined by x1 \u227a2 y, one is guaranteed to compute the same value, since y is \u201clinked\u201d only with x1. The constrained induced-width is then wG( 2) = 1, e.g. with the elimination order x1 \u227a y \u227a xn+1 \u227a xn \u227a . . . \u227a x2. Therefore, the theoretical complexity decreases from O((n + 2) \u00b7 dn+1) to O((n + 2) \u00b7 d2), thanks to the weakening of the partial order (the (n + 2) factor corresponds to the number of scoped functions).\nWorking on the hypergraph The second possible mechanism is to work on the hypergraph G, either by eliminating so-called \u201cbarren\u201d variables (computing \u2211\nx Px | pa(x) is useless because of normalization), or by better decomposing the problem. To illustrate the latter, assume that one wants to compute maxx1,...,xn \u2211\ny Py \u00b7 (Uy,x1 + \u00b7 \u00b7 \u00b7 + Uy,xn). The basic hypergraph G1 = ({x1, . . . , xn, y}, {{y, x1}, . . . , {y, xn}}), together with 1 defined by {x1, . . . , xn} \u227a1 y, gives a theoretical complexity O((n+1) \u00b7dwG1 ( 1)+1) = O((n+1) \u00b7dn+1). However, one can write:\nmaxx1,...,xn \u2211 y Py \u00b7 (Uy,x1 + \u00b7 \u00b7 \u00b7 + Uy,xn) = (maxx1 \u2211 y Py \u00b7 Uy,x1) + \u00b7 \u00b7 \u00b7 + (maxxn \u2211 y Py \u00b7 Uy,xn)\nThus, an implicit duplication of y makes the complexity decrease to O((n + 1)d2) = O((n + 1)d1+wG2 ( 2)), where G2 is the hypergraph defined by the variables {x1, . . . , xn, y(1), . . . , y(n)} and by the hyperedges {{x1, y(1)}, . . . , {xn, y(n)}}, and where 2 is given by x1 \u227a2 y(1), . . . , xn \u227a2 y(n). This method, which uses the property \u2211\nS (U1 + U2) = ( \u2211 S U1)+( \u2211\nS U2), duplicates variables \u201cquantified\u201d with \u2211\n, so that computations become more local. Proposition 2 shows the possible exponential gain obtained by duplication.\nProposition 2. Let \u03c6x,Si be a scoped function of scope {x} \u222a Si for any i \u2208 [1,m]. The direct computation of \u2211\nx (\u03c6x,S1 + \u00b7 \u00b7 \u00b7 + \u03c6x,Sm) always requires more sums than the direct computation of ( \u2211\nx \u03c6x,S1) + \u00b7 \u00b7 \u00b7 + ( \u2211\nx \u03c6x,Sm). Moreover, the computation of \u2211\nx (\u03c6x,S1 + \u00b7 \u00b7 \u00b7 + \u03c6x,Sm) results in a complexity O(m \u00b7 d1+|S1\u222a...\u222aSm|), whereas the computation of the m quantities in the set { \u2211\nx \u03c6x,Si | 1 \u2264 i \u2264 m} results in a complexity O(m \u00b7 d1+maxi\u2208[1,m] |Si|).\nWhy not use potentials? Though weakening the constraints on the elimination order could be done\nwith potentials, the duplication mechanism cannot be used if potentials are. Indeed, one cannot write (\u03c0W1 \u2297 \u03c0W2) \u2191W3 = (\u03c0\u2191W3W1 ) \u2297 (\u03c0 \u2191W3 W2\n) even if W3 \u2282 C. The duplication mechanism has actually already been proposed in the influence diagram litterature [Dechter, 2000] where it was applied \u201don the fly\u201d during elimination. In this paper, the duplication is exploited in a global preliminary analysis which may reveal new degrees of freedom in the elimination order, in synergism with the application of other mechanisms. The new architecture we introduce, which does not use potentials to solve influence diagrams, is called the Multi-operator Cluster DAG (MCDAG) architecture."}, {"heading": "3 THE MCDAG ARCHITECTURE", "text": ""}, {"heading": "3.1 MACROSTRUCTURING AN INFLUENCE DIAGRAM", "text": "The first step to build the MCDAG architecture is to analyze the macrostructure of the influence diagram, by detecting the possible reordering freedoms in the elimination order, while using the duplication technique and the normalization conditions on conditional probability distributions. This macrostructure is represented with a DAG of computation nodes.\nDefinition 2. An atomic computation node n is a scoped function \u03c6 in P \u222a U . In this case, the value of n is val(n) = \u03c6, and its scope is sc(n) = sc(\u03c6). A computation node is either an atomic computation node or a triple n = (Sov,~, N), where Sov is a sequence of operator-variables pairs, ~ is an associative and commutative operator with an identity, and where N is a set of computation nodes. In the latter, the value of n is given by val(n) = Sov (~n\u2032\u2208N val(n\n\u2032)), and its scope is given by sc(n) = (\u222an\u2032\u2208N sc(n\u2032)) \u2212 {x | opx \u2208 Sov}.\nInformally, a computation node (Sov,~, N) defines a sequence of marginalizations on a combination of computation nodes with a specific operator. It can be represented as in Figure 1. Given a set of computation nodes N , we define N+x (resp. N\u2212x) as the set of nodes of N whose scope contains x (resp. does not contain x): N+x = {n \u2208 N |x \u2208 sc(n)} (resp. N\u2212x = {n \u2208 N |x /\u2208 sc(n)})."}, {"heading": "3.1.1 From influence diagrams to computation nodes", "text": "Without loss of generality, we assume that U 6= \u2205 (if this is not the case, one can add U0 = 0 to U).\nProposition 3. Let Sov0 be the initial sequence \u2211\nI0 maxD1 . . .\n\u2211\nIq\u22121 maxDq\n\u2211\nIq of operator-variables\npairs defined by the influence diagram. The value of Equation 1 is equal to the value of the computation node n0 = (Sov0,+, {(\u2205,\u00d7, P \u222a {Ui}), Ui \u2208 U}).\nFor the influence diagram associated with the computation of maxd \u2211\nr2,r1 Pr1 \u00b7Pr2|r1 \u00b7(Ud,r1+Ud,r2+Ud), n0\ncorresponds to the first computation node in Figure 2."}, {"heading": "3.1.2 Macrostructuring the initial node", "text": "In order to exhibit the macrostructure of the influence diagram, we analyze the sequence of computations performed by n0. To do so, we successively consider the eliminations in Sov0 from the right to the left and use three types of rewriting rules, preserving nodes values, to make the macrostructure explicit: (1) decomposition rules, which decompose the structure using namely the duplication technique; (2) recomposition rules, which reveal freedoms in the elimination order; (3) simplification rules, which remove useless computations from the architecture, by using normalization conditions. Rewriting rules are presented first for the case of sum-marginalizations, and then for the case of max-marginalizations. A rewriting rule may be preceded by preconditions restricting its applicability.\nRewriting rules for \u2211\nx When a sum-marginalization must be performed, a decomposition rule D\u03a3, a recomposition rule R\u03a3, and two simplification rules S1\u03a3 and S 2 \u03a3 are used. These are illustrated in Figure 2, which corresponds to the influence diagram example introduced in 3.1.1.\nD\u03a3 (Sov. \u2211 x,+, {(\u2205,\u00d7, N) , N \u2208 N})\n( Sov,+, {( \u2205,\u00d7, N\u2212x \u222a {( \u2211 x,\u00d7, N +x )}) , N \u2208 N })\nR\u03a3 [ Prec.: (S \u2032 \u2229 (S \u222a sc(N1)) = \u2205) \u2227 (N1 \u2229N2 = \u2205)] ( \u2211\nS ,\u00d7, N1 \u222a {( \u2211 S\u2032 ,\u00d7, N2)}) ( \u2211 S\u222aS\u2032 ,\u00d7, N1 \u222aN2)\nS1\u03a3 [ Prec.: x /\u2208 S \u222a sc(N) ]\n( \u2211 {x}\u222aS ,\u00d7, N \u222a { Px | pa(x) } ) ( \u2211 S ,\u00d7, N)\nS2\u03a3 ( \u2205,\u00d7, N \u222a {( \u2211 \u2205,\u00d7, \u2205 )})\n(\u2205,\u00d7, N)\nExample In the example of Figure 2, the first rule to be applied is the decomposition rule D\u03a3, which treats\n\u2205 Pr1 Pr2|r1\n\u00d7\n\u00d7\u2205\nPr2|r1 UdUd,r2Ud,r1 \u2205\u00d7 Pr1 Pr1 Pr2|r1\nUd,r1\nD\u03a3\nR\u03a3\nS1 \u03a3 + S2 \u03a3\nD\u03a3\nP\nr2 \u00d7 Ud,r2\nP\nr2\n\u00d7 Pr1P\nr1 Pr2|r1 Pr2|r1\n\u2205 \u00d7 Ud\u2205 \u00d7\n\u00d7\u00d7 P\nr2\n\u00d7 P\nr1\nPr1\nUd,r1\nUd,r1 P r1,r2 Pr1 Pr2|r1 \u00d7 P r1,r2 \u00d7 P r1,r2 Pr1 Pr2|r1 Ud,r2\n+maxd\n\u2205 \u00d7 \u2205 \u00d7 Ud\nPr1\n+maxd\n+maxd\n\u2205 \u00d7 Ud\u2205 \u00d7 \u2205 \u00d7\n\u00d7 Pr2|r1\n\u00d7 Ud\u2205 \u00d7\n\u00d7 Pr1 \u00d7 P\nr1\nP\nr1Pr2|r1\nPr1 Pr2|r1\n+maxd P\nr2,r1\n\u2205 \u00d7\n\u2205\n\u2205 \u00d7\n\u00d7 P\nr1 Pr1 Ud,r1 \u00d7\nP\nr1,r2 Pr1 Pr2|r1 Ud,r2\n+maxd P\nr2\n\u2205 \u00d7 Ud,r2\nFigure 2: Application of rewriting rules for \u2211 .\nthe operator-variable pair \u2211\nr1 .Such a rule uses the\nduplication mechanism and the distributivity property of \u00d7 over +. It provides us with a DAG of computation nodes. It is a DAG since common computation nodes are merged (and it is not hard to detect such nodes when applying the rules). Then, D\u03a3 can be applied again for \u2211\nr2 . One can infer from the ob-\ntained architecture that there is no reason for r1 to be eliminated before r2. Using the recomposition rule R\u03a3 makes this clear in the structure. Basically, R\u03a3 uses the distributivity of \u00d7 over +. Last, applying S1\u03a3 and S2\u03a3, which use the normalization of conditional probability distributions, simplifies some nodes in the architecture. In the end, no computation involves more than two variables if one eliminates r1 first in the node ( \u2211\nr1,r2 ,\u00d7, {Pr1 , Pr2|r1 , Ud,r2}), whereas with a poten-\ntial-based approach, it would be necessary to process three variables simultaneously (since r1 would be involved in the potentials (Pr1 , 0), (Pr2|r1 , 0), (1, Ud,r1) if eliminated first, and r2 would be involved in the potentials (Pr2|r1 , 0), (1, Ud,r2) if eliminated first).\nRewriting rules for maxx When a max-marginalization must be performed, a decomposition rule Dmax and a recomposition rule Rmax are used (there is no simplification rule since there is no normalization condition to use for decision variables). These rules are a bit more complex than the previous ones and are illustrated in Figure 3, which corresponds to the influence diagram maxd1 \u2211 r2 maxd2 \u2211 r1 maxd3 Pr1 \u00b7Pr2|r1 \u00b7 (Ud1 + Ud2,d3 + Ur2,d1,d3 + Ur1,d2).\nDmax [ Prec.: \u2200N \u2208 N+x \u2200n \u2208 N\u2212x, val(n) \u2265 0 ]\n(Sov.maxx,+, {(\u2205,\u00d7, N) , N \u2208 N})\n\n\n\n(Sov,+, {(\u2205,\u00d7, N) , N \u2208 N}) if N+x = \u2205 (Sov,+, {(\u2205,\u00d7, N) , N \u2208 N\u2212x}\n\u222a {(\u2205,\u00d7, N1 \u222a {(maxx,+, N2)})}) otherwise\nwhere\n{\nN1 = \u2229N\u2208N+xN \u2212x N2 = {(\u2205,\u00d7, N \u2212N1) , N \u2208 N +x}\nRmax [ Prec.: (S \u2032\u2229(S\u222asc(N1)\u222asc(N2)) = \u2205)\u2227(\u2200N3 \u2208\nN, N2 \u2229N3 = \u2205) \u2227 (\u2200n \u2208 N2, val(n) \u2265 0) ]\n(maxS ,+, N1\u222a\n{(\u2205,\u00d7, N2 \u222a {(maxS\u2032 ,+, {(\u2205,\u00d7, N3), N3 \u2208 N})})})\n(maxS\u222aS\u2032 ,+, N1 \u222a {(\u2205,\u00d7, N2 \u222aN3) , N3 \u2208 N})\nExample In the example of Figure 3, one first applies the decomposition ruleDmax, in order to treat the operator-variable pair maxd3 . Such a rule uses first the monotonicity of + (max(a+ b, a+ c) = a+ max(b, c)), and then both the distributivity of \u00d7 over + and the monotonicity of \u00d7 (so as to write things like maxd3((Pr1 \u00b7Pr2|r1 \u00b7Ud2,d3)+ (Pr1 \u00b7Pr2|r1 \u00b7Ur2,d1,d3)) = Pr1 \u00b7Pr2|r1 \u00b7maxd3(Ud2,d3+Ur2,d1,d3)). Then, D\u03a3 can be used for \u2211\nr1 , and Dmax can be used for maxd2 . After\nthose steps, the recomposition rule Rmax, which uses the monotonicities of \u00d7 and +, reveals that the elimination order between d2 and d3 is actually free. This was not obvious from the initial Sov sequence. The approach using potentials is unable to make such freedoms explicit, which may induce exponential increase in complexity as shown in 2.3.\nRule application order A chaotic iteration of the rules does not converge, since e.g., rules Dmax and Rmax may be infinitely alternately applied. Hence, we specify an order in which we apply rules to converge to a unique final DAG of computation nodes (we have\nPr1\nUd1\n\u00d7 Pr2|r1\u2205\nPr1\nUd1\n\u00d7 Pr2|r1\u2205\n\u00d7\u2205\n\u2205\nUr1,d2\nUd2,d3\n\u00d7 maxd2 +\n\u00d7 Ud1\n\u2205\nP\nr1 Pr2|r1\nPr1 +\nPr1Ur1,d2P r1\nD\u03a3\nDmax\n+maxd1 P\nr2\nmaxd3 \u00d7Pr2|r1 \u00d7\n\u2205 \u00d7\nUr2,d1,d3\nPr2|r1\u00d7\nDmax\n+maxd1 P r2 maxd2 P r1 maxd3\nUd2,d3\n\u2205\n\u00d7 Ur1,d2 Pr2|r1\nPr1 \u2205Pr2|r1 Pr1 \u00d7\u2205 Pr1 \u2205 \u00d7 Pr2|r1\nPr1\n\u00d7\n\u2205\n\u2205 \u00d7\u2205\nmaxd1 P r2 maxd2 +\n\u00d7 Ud1\n\u00d7 Pr2|r1 Pr1P r1\n\u00d7 P\nr1\nUr1,d2Pr1 Pr2|r1\nRmax\n+maxd1 P r2 maxd2 P r1\n\u2205\n\u00d7 Ur2,d1,d3\n\u2205 \u00d7\n\u2205 \u00d7 Ud2,d3 \u2205 \u00d7 Ur2,d1,d3\n+maxd1 P\nr2\n\u2205\n\u00d7 Ud2,d3\n\u2205 \u00d7 Pr1 Pr2|r1\n+maxd3\n\u2205 \u00d7 Ur2,d1,d3\u2205 \u00d7 Ud2,d3\n\u2205 \u00d7\n+\nUr2,d1,d3\u00d7\u2205\nmaxd3 +\nmaxd2,d3\n\u2205 \u00d7\n\u00d7 P\nr1\nUr1,d2Pr1 Pr2|r1\n\u2205 \u00d7 Ud1\n\u00d7 Pr2|r1 Pr1P r1\nFigure 3: Application of rewriting rules for max (the application of the rules may create nodes looking like (\u2205,\u00d7, {n}), which perform no computations; these nodes can be eliminated at a final step).\nused this order in the previous examples). We successively consider each operator-variable pair of the initial sequence Sov0 from the right to the left (marginalizations like \u2211\nx1,...,xn can be split into\n\u2211 x1 \u00b7 \u00b7 \u00b7 \u2211 xn ).\nIf the rightmost marginalization in the Sov sequence of the root node is \u2211\nx, then rule D\u03a3 is applied once. It creates new grandchildren nodes for the root, for each\nof which, we try to apply rule R\u03a3 in order to reveal freedoms in the elimination order. If R\u03a3 is applied, this creates new computation nodes, on each of which simplification rules S1\u03a3 and then S 2 \u03a3 are applied (until they cannot be applied anymore).\nIf the rightmost marginalization in the Sov sequence of the root node is maxx, then rule Dmax is applied once. This creates a new child and a new grandchild for the root. For the created grandchild, we try to weaken constraints on the elimination order using Rmax.\nTherefore, the rewriting rules are applied in a deterministic order, except from the freedom left when choosing the next variable in S to consider for marginalizations like \u2211\nS or maxS . It can be shown that this freedom does not change the final structure. The soundness of the macrostructure obtained is provided by the soundness of the rewriting rules:\nProposition 4. Rewriting rules D\u03a3, R\u03a3, S 1 \u03a3, S 2 \u03a3, Dmax and Rmax are sound, i.e. for any of these rules n1 n2, if the preconditions are satisfied, then val(n1) = val(n2) holds. Moreover, rules Dmax and Rmax leave the set of optimal decision rules unchanged.\nComplexity issues An architecture is usable only if it is reasonable to build it. Proposition 5 makes it possible to save some tests during the application of the rewriting rules, and Proposition 6 gives upper bounds on the complexity.\nProposition 5. Except for S1\u03a3, the preconditions of the rewriting rules are always satisfied.\nProposition 6. The time and space complexity of the application of the rewriting rules are O(|C \u222aD| \u00b7 |U | \u00b7 (1 + |P |)2) and O(|C \u222aD| \u00b7 (|U | + |P |)) respectively."}, {"heading": "3.2 TOWARDS MCDAGS", "text": "The rewriting rules enable us to transform the initial multi-operator computation node n0 into a DAG of mono-operator computation nodes looking like (maxS ,+, N), ( \u2211\nS ,\u00d7, N), (\u2205,\u00d7, N), or \u03c6 \u2208 P \u222a U . For nodes (maxS ,+, N) or ( \u2211\nS ,\u00d7, N), it is time to use freedoms in the elimination order. To do so, usual junction tree construction techniques can be used, since on one hand, (R,max,+) and (R,+,\u00d7) are commutative semirings, and since on the other hand, there are no constraints on the elimination order inside each of these nodes (the only slight difference with usual junction trees is that only a subset of the variables involved in a computation node may have to be eliminated, but it is quite easy to cope with this).\nTo obtain a good decomposition for nodes n like (maxS ,+, N) or ( \u2211\nS ,\u00d7, N), one can build a junction tree to eliminate S from the hypergraph G =\n(sc(N), {sc(n\u2032) |n\u2032 \u2208 N}). The optimal induced-width which can be obtained for n is w(n) = wG,S , the induced-width of G for the elimination of the variables in S.3 The induced-width of the MCDAG architecture is then defined by wmcdag = maxn\u2208N w(n), where N is the set of nodes looking like (maxS ,+, N) or ( \u2211\nS ,\u00d7, N).\nAfter the decomposition of each mono-operator computation node, one obtains a Multi-operator Cluster DAG.\nDefinition 3. A Multi-operator Cluster DAG is a DAG where every vertex c (called a cluster) is labeled with four elements: a set of variables V (c), a set of scoped functions \u03a8(c) taking values in a set E, a set of son clusters Sons(c), and a couple (\u2295c,\u2297c) of operators on E s.t. (E,\u2295c,\u2297c) is a commutative semiring.\nDefinition 4. The value of a cluster c of a MCDAG is given by val(c) = \u2295cV (c)\u2212V (pa(c)) (( \u2297c\u03c8\u2208\u03a8(c) \u03c8 ) \u2297c ( \u2297cs\u2208Sons(c) val(s) ))\n. The value of a MCDAG is the value of its root node.\nThanks to Proposition 7, working on MCDAGs is sufficient to solve influence diagrams.\nProposition 7. The value of the MCDAG obtained after having decomposed the macrostructure is equal to the maximal expected utility. Moreover, for any decision variable Dk, the set of optimal decision rules for Dk in the influence diagram is equal to the set of optimal decision rules for Dk in the MCDAG."}, {"heading": "3.3 MERGING SOME COMPUTATIONS", "text": "There may exist MCDAG clusters performing exactly the same computations, even if the computation nodes they come from are distinct. For instance, a computation node n1 = ( \u2211\nx,y,\u00d7, {Px, Py|x, Uy,z) may be decomposed into clusters c1 = ({x}, {Px, Py|x}, \u2205, (+,\u00d7)) and c\u20321 = ({y}, {Uy,z}, {c \u2032 1}, (+,\u00d7)). A computation node n2 = ( \u2211\nx,y,\u00d7, {Px, Py|x, Uy,t) may be decomposed into clusters c2 = ({x}, {Px, Py|x}, \u2205, (+,\u00d7)) and c\u20322 = ({y}, {Uy,t}, {c \u2032 2}, (+,\u00d7)). As c1 = c2, some computations can be saved by merging clusters c1 and c2 in the MCDAG. Detecting common clusters is not as easy as detecting common computation nodes.\n3For (maxS, +, N) nodes, which actually always look like (maxS, +, {(\u2205,\u00d7, N\n\u2032), N \u2032 \u2208 N}), better decompositions can be obtained by using another hypergraph. In fact, for each N \u2032 \u2208 N, there exists a unique n \u2208 N \u2032, denoted N \u2032[u], s.t. n or its children involve a utility function. It is then better to consider the hypergraph (sc(N), {sc(N \u2032[u]) |N \u2032 \u2208 N}). This enables to figure out that e.g. only two variables (x and y) must be considered if one eliminates x first in a node like (maxxy, +, N) = (maxxy, +, {(\u2205,\u00d7, Uy,z), (\u2205,\u00d7, {nz , Ux,y}), (\u2205,\u00d7, {nz, Ux})}), since nz is a factor of both Ux,y and Ux. We do not further develop this technical point.\nTo sum up, there are three steps to build the architecture. First, the initial multi-operator computation node is transformed into a DAG of mono-operator computation nodes (via sound rewriting rules). Then, each computation node is decomposed with a usual junction tree construction. It provides us with a MCDAG, in which some clusters can finally be merged."}, {"heading": "4 VE ALGORITHM ON MCDAGs", "text": "Defining a VE algorithm on a MCDAG is simple. The only difference with existing VE algorithms is the multi-operator aspect for both the marginalization and the combination operators used. As in usual architectures, nodes send messages to their parents. Whenever a node c has received all messages val(s) from its children, c can compute its own value val(c) = \u2295cV (c)\u2212V (pa(c)) (( \u2297c\u03c8\u2208\u03a8(c) \u03c8 ) \u2297c ( \u2297cs\u2208Sons(c) val(s) )) and send it to its parents. As a result, messages go from leaves to root, and the value computed by the root is the maximal expected utility."}, {"heading": "5 COMPARISON WITH EXISTING ARCHITECTURES", "text": "Compared to existing architectures on influence diagrams, MCDAGs can be exponentially more efficient by strongly decreasing the constrained induced-width (cf Section 2.3), thanks to (1) the duplication technique, (2) the analysis of extra reordering freedoms, and (3) the use of normalizations conditions. One can compare these three points with existing works:\n\u2022 The idea behind duplication is to use all the decompositions (independences) available in influence diagrams. An influence diagram actually expresses independences on one hand on the global probability distribution PC |D, and on the other hand on the global utility function. MCDAGs separately use these two kinds of independences, whereas a potential-based approach uses a kind of weaker \u201cmixed\u201d independence relation. Using the duplication mechanism during the MCDAG building is better, in terms of induced-width, than using it \u201con the fly\u201d as in [Dechter, 2000].4\n\u2022 Weakening constraints on the elimination order can be linked with the usual notion of relevant information for decision variables. With MCDAGs,\n4E.g., for the quite simple influence diagram introduced in Section 3.1.1, the algorithm in [Dechter, 2000] gives 2 as an induced-width, whereas MCDAGs give an inducedwidth 1. The reason is that MCDAGs allow to eliminate both x1 before x2 in the subproblem corresponding to Ud,x2 and x2 before x1 in the subproblem corresponding to Ud,x1 .\nthis notion is not used only for decision rules conciseness reasons: it is also used to reveal reordering freedoms, which can decrease the time complexity. Note that some of the ordering freedom here is obtained by synergism with the duplication.\n\u2022 Thanks to simplification rule S1\u03a3, the normalization conditions enable us not only to avoid useless computations, but also to improve the architecture structure (S1\u03a3 may indirectly weaken some constraints on the elimination order). This is stronger than Lazy Propagation architectures [Madsen and Jensen, 1999], which use the first point only, during the message passing phase. Note that with MCDAGs, once the DAG of computation nodes is built, there are no remaining normalization conditions to be used.\nCompared to existing architectures, MCDAGs actually always produce the best decomposition in terms of constrained induced-width, as Theorem 1 shows.\nTheorem 1. Let wGp( p) be the constrained inducedwidth associated with the potential-based approach (cf Section 2.2). Let wmcdag be the induced-width associated with the MCDAG (cf Section 3.2). Then, wmcdag \u2264 wGp( p).\nLast, the MCDAG architecture contradicts a common belief that using division operations is necessary to solve influence diagrams with VE algorithms."}, {"heading": "6 POSSIBLE EXTENSIONS", "text": "The MCDAG architecture has actually been developed in a generic algebraic framework which subsumes influence diagrams. This framework, called the Plausibility-Feasibility-Utility networks (PFUs) framework [Pralet et al., 2006a], is a generic framework for sequential decision making with possibly uncertainties (plausibility part), asymmetries in the decision process (feasibility part), and utilities. PFUs subsume formalisms from quantified boolean formulas or Bayesian networks to stochastic constraint satisfaction problems, and even define new frameworks like possibilistic influence diagrams. This subsumption is possible because the questions raised in many existing formalisms often reduce to a sequence of marginalizations on a combination of scoped functions. Such sequences, a particular case of which is Equation 1, can be structured using rewriting rules as the ones previously presented, which actively exploit the algebraic properties of the operators at stake.\nThanks to the generic nature of PFUs, extending the previous work to a possibilistic version of influence diagrams is trivial. If one uses the possibilistic\npessimistic expected utility [Dubois and Prade, 1995], the optimal utility can be defined by (the probability distributions Pi become possibility distributions, and the utilities Ui become preference degrees in [0, 1]):\nmin I0 max D1 . . .min Iq\u22121 max Dq min Iq max\n(\nmax Pi\u2208P (1 \u2212 Pi), min Ui\u2208U U\n)\n.\nThese eliminations can be structured via a MCDAG. The only difference in the rewriting rules is that \u00d7 becomes max and + becomes min. The computation nodes then look like (min,max, N), (max,min, N), or (\u2205,max, N), and the MCDAG clusters use (\u2295c,\u2297c) = (min,max), (max,min), or (\u2205,max)."}, {"heading": "7 CONCLUSION", "text": "To solve influence diagrams, using potentials allows one to reuse existing VE schemes, but may be exponentially sub-optimal. The key point is that taking advantage of the composite nature of graphical models such as influence diagrams, and namely of the algebraic properties of the elimination and combination operators at stake, is essential to obtain an efficient architecture for local computations. The direct handling of several elimination and combination operators in a kind of composite architecture is the key mechanism which allows MCDAGs to always produce the best constrained induced-width when compared to potential-based schemes.\nThe authors are currently working to obtain experimental results on MCDAGs in the context of the PFU framework (the construction of MCDAG architectures is currently implemented). Future directions could be first to adapt the MCDAG architecture to the case of Limited Memory Influence Diagrams (LIMIDs) [Lauritzen and Nilsson, 2001], and then to use the MCDAG architecture in the context of approximate resolution."}, {"heading": "Acknowledgments", "text": "We would like to thank the reviewers of this article for their helpful comments on related works. This work was partially conducted within the EU IP COGNIRON (\u201cThe Cognitive Companion\u201d) funded by the European Commission Division FP6-IST Future and Emerging Technologies under Contract FP6-002020."}], "references": [{"title": "Topological Parameters for Time-Space Tradeoff", "author": ["R. Dechter", "Y. El Fattah"], "venue": "Artificial Intelligence, 125(1-2):93\u2013118", "citeRegEx": "Dechter and Fattah. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Possibility Theory as a Basis for Qualitative Decision Theory", "author": ["D. Dubois", "H. Prade"], "venue": "Proc. of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95), pages 1925\u20131930", "citeRegEx": "Dubois and Prade. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Influence Diagrams", "author": ["R. Howard", "J. Matheson"], "venue": "Readings on the Principles and Applications of Decision Analysis, pages 721\u2013762", "citeRegEx": "Howard and Matheson. 1984", "shortCiteRegEx": null, "year": 1984}, {"title": "From Influence Diagrams to Junction Trees", "author": ["F. Jensen", "F.V. Jensen", "S. Dittmer"], "venue": "Proc. of the 10th International Conference on Uncertainty in Artificial Intelligence (UAI-94), pages 367\u2013373", "citeRegEx": "Jensen et al.. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Representing and Solving Decision Problems with Limited Information", "author": ["S. Lauritzen", "D. Nilsson"], "venue": "Management Science, 47(9):1235\u20131251", "citeRegEx": "Lauritzen and Nilsson. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Lazy Evaluation of Symmetric Bayesian Decision Problems", "author": ["A. Madsen", "F.V. Jensen"], "venue": "Proc. of the 15th International Conference on Uncertainty in Artificial Intelligence (UAI-99), pages 382\u2013390", "citeRegEx": "Madsen and Jensen. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Potential Influence Diagrams", "author": ["P. Ndilikilikesha"], "venue": "International Journal of Approximated Reasoning, 10:251\u2013285", "citeRegEx": "Ndilikilikesha. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Complexity Results and Approximation Strategies for MAP Explanations", "author": ["J. Park", "A. Darwiche"], "venue": "Journal of Artificial Intelligence Research, 21:101\u2013133", "citeRegEx": "Park and Darwiche. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Decision with Uncertainties", "author": ["C. Pralet", "G. Verfaillie", "T. Schiex"], "venue": "Feasibilities, and Utilities: Towards a Unified Algebraic Framework. In Proc. of the 17th European Conference on Artificial Intelligence (ECAI-06)", "citeRegEx": "Pralet et al.. 2006a", "shortCiteRegEx": null, "year": 2006}, {"title": "From Influence Diagrams to MCDAGs: Extended version", "author": ["C. Pralet", "G. Verfaillie", "T. Schiex"], "venue": "Technical report, LAAS-CNRS,", "citeRegEx": "Pralet et al.. 2006b", "shortCiteRegEx": null, "year": 2006}, {"title": "Evaluating Influence Diagrams", "author": ["R. Shachter"], "venue": "Operations Research, 34(6):871\u2013882", "citeRegEx": "Shachter. 1986", "shortCiteRegEx": null, "year": 1986}, {"title": "Valuation-based Systems for Discrete Optimization", "author": ["P. Shenoy"], "venue": "Proc. of the 6th International Conference on Uncertainty in Artificial Intelligence (UAI-90), pages 385\u2013400", "citeRegEx": "Shenoy. 1990", "shortCiteRegEx": null, "year": 1990}, {"title": "Valuation-based Systems for Bayesian Decision Analysis", "author": ["P. Shenoy"], "venue": "Operations Research, 40(3):463\u2013484", "citeRegEx": "Shenoy. 1992", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 10, "context": "Since the first algorithms based on decision trees or arc-reversal operations [Shachter, 1986], several exact methods have been proposed to solve influence diagrams using local computations, such as the ones based on the Shenoy-Shafer, the HUGIN, or the Lazy Propagation architectures [Shenoy, 1992; Jensen et al.", "startOffset": 78, "endOffset": 94}, {"referenceID": 12, "context": "Since the first algorithms based on decision trees or arc-reversal operations [Shachter, 1986], several exact methods have been proposed to solve influence diagrams using local computations, such as the ones based on the Shenoy-Shafer, the HUGIN, or the Lazy Propagation architectures [Shenoy, 1992; Jensen et al., 1994; Madsen and Jensen, 1999].", "startOffset": 285, "endOffset": 345}, {"referenceID": 3, "context": "Since the first algorithms based on decision trees or arc-reversal operations [Shachter, 1986], several exact methods have been proposed to solve influence diagrams using local computations, such as the ones based on the Shenoy-Shafer, the HUGIN, or the Lazy Propagation architectures [Shenoy, 1992; Jensen et al., 1994; Madsen and Jensen, 1999].", "startOffset": 285, "endOffset": 345}, {"referenceID": 5, "context": "Since the first algorithms based on decision trees or arc-reversal operations [Shachter, 1986], several exact methods have been proposed to solve influence diagrams using local computations, such as the ones based on the Shenoy-Shafer, the HUGIN, or the Lazy Propagation architectures [Shenoy, 1992; Jensen et al., 1994; Madsen and Jensen, 1999].", "startOffset": 285, "endOffset": 345}, {"referenceID": 6, "context": "The key mechanism used for such an extension consists in using elements known as potentials [Ndilikilikesha, 1994].", "startOffset": 92, "endOffset": 114}, {"referenceID": 9, "context": "All proofs are available in [Pralet et al., 2006b].", "startOffset": 28, "endOffset": 50}, {"referenceID": 2, "context": "Notations and definitions An influence diagram [Howard and Matheson, 1984] is a composite graphical model defined on three sets of variables organized in a Directed Acyclic Graph (DAG) G: (1) a set C of chance variables x \u2208 C, for each of which a conditional probability distribution Px | pa(x) on x given its parents in G is specified; (2) a set D = {D1, .", "startOffset": 47, "endOffset": 74}, {"referenceID": 3, "context": "The usual problem associated with an influence diagram is to find decision rules maximizing the expected utility (a decision rule for a decision Dk is a function associating a value in dom(Dk) with any assignment of the variables observed before making decision Dk) As shown in [Jensen et al., 1994], this is equivalent to computing optimal decision rules for the quantity", "startOffset": 278, "endOffset": 299}, {"referenceID": 11, "context": "As \u2297 and \u2191 satisfy the ShenoyShafer axioms defined in [Shenoy, 1990], \u03b2 can be computed using usual VE algorithms [Jensen et al.", "startOffset": 54, "endOffset": 68}, {"referenceID": 3, "context": "As \u2297 and \u2191 satisfy the ShenoyShafer axioms defined in [Shenoy, 1990], \u03b2 can be computed using usual VE algorithms [Jensen et al., 1994].", "startOffset": 114, "endOffset": 135}, {"referenceID": 3, "context": "The complexity of VE can then be quantified using constrained induced-width [Jensen et al., 1994; Park and Darwiche, 2004] (instead of inducedwidth [Dechter and Fattah, 2001]).", "startOffset": 76, "endOffset": 122}, {"referenceID": 7, "context": "The complexity of VE can then be quantified using constrained induced-width [Jensen et al., 1994; Park and Darwiche, 2004] (instead of inducedwidth [Dechter and Fattah, 2001]).", "startOffset": 76, "endOffset": 122}, {"referenceID": 0, "context": ", 1994; Park and Darwiche, 2004] (instead of inducedwidth [Dechter and Fattah, 2001]).", "startOffset": 58, "endOffset": 84}, {"referenceID": 3, "context": "Then, with classical approaches based on potentials and strong junction trees [Jensen et al., 1994], which are junction trees with constraints on the marginalization order, the theoretical complexity is O(|P \u222aU | \u00b7 dGp ( p) (the number of elements of a finite set E is denoted |E|).", "startOffset": 78, "endOffset": 99}, {"referenceID": 7, "context": "Though such a technique is known to be useless in contexts like Maximum A Posteriori hypothesis [Park and Darwiche, 2004], where there is only one alternation of max and sum marginalizations, This means that VG is the set of variables (or vertices), and HG is a set of hyperedges on VG, i.", "startOffset": 96, "endOffset": 121}, {"referenceID": 5, "context": "This is stronger than Lazy Propagation architectures [Madsen and Jensen, 1999], which use the first point only, during the message passing phase.", "startOffset": 53, "endOffset": 78}, {"referenceID": 8, "context": "This framework, called the Plausibility-Feasibility-Utility networks (PFUs) framework [Pralet et al., 2006a], is a generic framework for sequential decision making with possibly uncertainties (plausibility part), asymmetries in the decision process (feasibility part), and utilities.", "startOffset": 86, "endOffset": 108}, {"referenceID": 1, "context": "pessimistic expected utility [Dubois and Prade, 1995], the optimal utility can be defined by (the probability distributions Pi become possibility distributions, and the utilities Ui become preference degrees in [0, 1]): min I0 max D1 .", "startOffset": 29, "endOffset": 53}, {"referenceID": 4, "context": "Future directions could be first to adapt the MCDAG architecture to the case of Limited Memory Influence Diagrams (LIMIDs) [Lauritzen and Nilsson, 2001], and then to use the MCDAG architecture in the context of approximate resolution.", "startOffset": 123, "endOffset": 152}], "year": 0, "abstractText": "There exist several architectures to solve influence diagrams using local computations, such as the Shenoy-Shafer, the HUGIN, or the Lazy Propagation architectures. They all extend usual variable elimination algorithms thanks to the use of so-called \u201cpotentials\u201d. In this paper, we introduce a new architecture, called the Multi-operator Cluster DAG architecture, which can produce decompositions with an improved constrained inducedwidth, and therefore induce potentially exponential gains. Its principle is to benefit from the composite nature of influence diagrams, instead of using uniform potentials, in order to better analyze the problem structure.", "creator": null}}}