{"id": "1206.6849", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "General-Purpose MCMC Inference over Relational Structures", "abstract": "tasks such as record linkage and multi - target tracking, which notably involve carefully reconstructing the set of objects that underlie some observed data, perhaps are particularly challenging for probabilistic inference. recent work has achieved efficient and accurate inference on such problems using markov chain monte'carlo ( mcmc ) techniques with customized proposal distributions. currently, implementing such a system requires coding mcmc state representations and acceptance probability calculations that are specific to a particular application. an increasingly alternative approach, which we pursue in this paper, is to uniformly use a general - purpose probabilistic modeling language ( such as blog ) and a generic metropolis - hastings sequential mcmc algorithm that supports user - supplied proposal distributions. our algorithm gains flexibility by using mcmc states that are only partial descriptions of possible worlds ; we provide conditions under which mcmc over partial worlds yields plausible correct answers to queries. we also show how to use a context - specific bayes net to closely identify the factors in the acceptance probability that need to be computed for a given proposed move. experimental results on a citation matching task show that our overall general - purpose mcmc engine compares favorably with an application - specific system.", "histories": [["v1", "Wed, 27 Jun 2012 16:24:15 GMT  (135kb)", "http://arxiv.org/abs/1206.6849v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["brian milch", "stuart russell"], "accepted": false, "id": "1206.6849"}, "pdf": {"name": "1206.6849.pdf", "metadata": {"source": "CRF", "title": "General-Purpose MCMC Inference over Relational Structures", "authors": ["Brian Milch"], "emails": ["milch@cs.berkeley.edu", "russell@cs.berkeley.edu"], "sections": [{"heading": null, "text": "Tasks such as record linkage and multi-target tracking, which involve reconstructing the set of objects that underlie some observed data, are particularly challenging for probabilistic inference. Recent work has achieved efficient and accurate inference on such problems using Markov chain Monte Carlo (MCMC) techniques with customized proposal distributions. Currently, implementing such a system requires coding MCMC state representations and acceptance probability calculations that are specific to a particular application. An alternative approach, which we pursue in this paper, is to use a general-purpose probabilistic modeling language (such as BLOG) and a generic Metropolis-Hastings MCMC algorithm that supports user-supplied proposal distributions. Our algorithm gains flexibility by using MCMC states that are only partial descriptions of possible worlds; we provide conditions under which MCMC over partial worlds yields correct answers to queries. We also show how to use a context-specific Bayes net to identify the factors in the acceptance probability that need to be computed for a given proposed move. Experimental results on a citation matching task show that our general-purpose MCMC engine compares favorably with an application-specific system."}, {"heading": "1 INTRODUCTION", "text": "Many probabilistic reasoning problems involve reconstructing the set of (possibly related) real-world objects that underlie some observed data: for instance, the publications and authors that are mentioned in a set of bibliographic citations, or the aircraft that underlie a set of observed radar blips. Because the number of possible ways to map a set of observations to underlying objects is huge, these problems are extremely challenging for standard proba-\nbilistic inference algorithms. Specialized algorithms have been developed in the fields of record linkage [Fellegi and Sunter, 1969] and data association [Bar-Shalom and Fortmann, 1988] that yield approximate solutions for particular classes of models.\nRecently, Pasula et al. [2003] and Oh et al. [2004] have achieved state-of-the-art results on these problems using Markov chain Monte Carlo (MCMC) methods. Given a probability distribution p on an outcome space \u2126, an MCMC algorithm approximates the probability of a query event Q given an evidence event E by generating a sequence of samples s1, s2, . . . , sN from a Markov chain over \u2126. This Markov chain is chosen so that it only visits outcomes consistent with E, and its stationary distribution is proportional to p(s). The desired probability p(Q|E) is then approximated as the fraction of s1, . . . , sN that are in Q. If the Markov chain is ergodic, then this approximation converges to the correct posterior probability as N \u2192\u221e.\nPasula et al. [2003] and Oh et al. [2004] use the MetropolisHastings (M-H) algorithm [Metropolis et al., 1953; Hastings, 1970] to construct a Markov chain whose stationary distribution is proportional to p(s). In the M-H algorithm, when the current state of the chain is sn, a new state is sampled from a proposal distribution q(s\u2032|sn). The acceptance probability for this proposed move is:\n\u03b1(sn, s \u2032) , max ( 1, p(s\u2032)q(sn|s \u2032)\np(sn)q(s\u2032|sn)\n) (1)\nWith probability \u03b1(sn, s\u2032), the move is accepted and sn+1 is set to s\u2032; otherwise, sn+1 = sn. The proposal distribution can incorporate domain-specific heuristics that help the chain to move quickly among reasonably probable hypotheses. As long as this Markov chain is ergodic, the stationary distribution will be proportional to p(s) regardless of the proposal distribution.\nThe recent successes of the M-H algorithm suggest that it could be applied usefully to other relational inference tasks, such as resolving coreference among noun phrases in text. However, to recycle a comment that Gilks et al. made about Gibbs sampling in 1994, \u201cUntil now all existing implemen-\ntations have involved writing one-off computer code in low or intermediate level languages such as C or Fortran.\u201d Although perhaps Matlab has replaced Fortran, this is still true about M-H implementations today: the data structures that represent MCMC states and the algorithms that compute acceptance probabilities are application-specific. Tackling a new application \u2014 or even adding a variable to an existing model \u2014 requires rewriting portions of the state representation and acceptance probability code, in addition to modifying the proposal distribution.\nWe would prefer to have a general approximate inference system that computed answers to queries based on a usersupplied probability model and proposal distribution. This paper presents a prototype for such a system. To represent the model, we use Bayesian logic (BLOG) [Milch et al., 2005a], a language that allows us to express uncertainty about the number of latent objects and the relations among objects and observations. Many of our results would also be applicable to other probabilistic modeling languages [Pfeffer, 2001; Jaeger, 2001; Laskey and da Costa, 2005; Richardson and Domingos, 2006; Mjolsness, 2005]. Ideally, we would also like to have a declarative language for specifying proposal distributions. However, at this point we are only beginning to understand what constructs and features such a language would need to have. Thus, we assume that proposal distributions are specified procedurally, as Java classes that implement a certain interface.\nThis paper focuses on the interface between the proposal distribution and the main M-H engine \u2014 specifically, the representation of MCMC states \u2014 and on how the engine can efficiently compute acceptance probabilities for arbitrary proposals. We begin in Sec. 2 by describing the probability model and M-H implementation used by [Pasula et al., 2003] to resolve coreference among citations. We then briefly review the BLOG language of [Milch et al., 2005a]. Sec. 3 introduces the architecture of our generic M-H system. Our main contributions begin in Sec. 4, which discusses the state space of our M-H algorithm. The key point here is that it is impractical to use MCMC states that correspond to single possible worlds; instead, states are represented with partial descriptions that denote whole sets of worlds. We provide conditions under which MCMC over sets of worlds yields asymptotically correct answers to queries. Taking advantage of this theorem, we use state descriptions that are partial in two ways: they do not instantiate irrelevant variables, and they abstract away the numbering of interchangeable objects.\nSec. 5 discusses the data structures and algorithms necessary to make generic M-H efficient. The goal here is to avoid having the time required to compute the acceptance probability and update the MCMC state grow with the number of hypothesized objects or the number of instantiated variables. We use data structures that represent a proposed world as a set of differences with respect to the\ncurrent world (Sec. 5.1). More interestingly, we can determine which factors in the acceptance probability need to be recomputed by maintaining a context-specific Bayes net graph over the instantiated variables (Sec. 5.2). Sec. 6 presents experimental results on the citation matching task, showing that our generic M-H system supports the same proposal distribution that was used in a hand-coded implementation, and has a running time of the same order of magnitude."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 CITATION MATCHING", "text": "Pasula et al. [2003] employ an M-H algorithm to cluster citations into groups that refer to the same publication. They use a generative model to define a distribution over worlds containing publications, researchers, and citations. This model includes prior distributions for researcher names and publication titles. The authors of each publication are chosen uniformly from the set of researchers, and the publication cited by each citation is chosen uniformly from the set of publications. The text of a citation is generated from the author names and title of the publication it cites, through an observation model that allows abbreviations, errors, and various styles of formating. Given the text of some observed citations, the goal is to infer which citations co-refer.\nThe proposal distribution of [Pasula et al., 2003] is based on moves that split and merge clusters of co-referring citations. In a preprocessing step, the citations are grouped into canopies: overlapping groups of citations that are similar enough (according to a rough heuristic) that they have a non-negligible chance of co-referring [McCallum et al., 2000]. For each move, the proposal distribution randomly chooses a canopy and two citations c1, c2 in that canopy. If c1 and c2 refer to the same publication p1, then a new publication p2 is added and the citations of p1 are split randomly among p1 and p2. If c1 and c2 refer to different publications, then the two publications may be merged. New titles and authors are proposed for the affected publications, based on the text of citations that now refer to them. Finally, any publications and researchers that are no longer connected to citations are removed from the MCMC state.\nThe resulting M-H algorithm recovers between 93% and 97% of the true co-referring clusters exactly. This accuracy is significantly better than that reported by the developers of Citeseer [Lawrence et al., 1999], and is competitive with more recent discriminative methods [Wellner et al., 2004]."}, {"heading": "2.2 BAYESIAN LOGIC", "text": "Bayesian logic (BLOG) [Milch et al., 2005a] is a language for defining probability distributions over structures that can contain varying numbers of objects with varying re-"}, {"heading": "1 type Res; type Pub; type Cit;", "text": ""}, {"heading": "2 guaranteed Cit Cit1, Cit2, Cit3, Cit4;", "text": ""}, {"heading": "3 #Res \u223c NumResearchersPrior;", "text": ""}, {"heading": "4 random String Name(Res r) \u223c NamePrior;", "text": ""}, {"heading": "5 #Pub \u223c NumPublicationsPrior;", "text": ""}, {"heading": "6 random String Title(Pub p) \u223c TitlePrior;", "text": ""}, {"heading": "7 random NaturalNum NumAuthors(Pub p)", "text": ""}, {"heading": "8 \u223c NumAuthorsPrior;", "text": ""}, {"heading": "9 random Res NthAuthor(Pub p, NaturalNum n)", "text": "lations among them. These \u201cpossible worlds\u201d are represented formally as model structures of a typed first-order logical language. A typed first-order language includes a set of types, such as String, NaturalNum, Citation, and Publication, and a set of function symbols1, such as Name, Title, and PubCited. A model structure specifies a set of objects for each type, and for each function symbol, it specifies a mapping from argument tuples to values.\nA BLOG model specifies a generative process for constructing such possible worlds. Fig. 1 shows a simplified version of the BLOG model that we use for citation matching. The model begins by declaring object types (line 1) and introducing some guaranteed objects that exist in all possible worlds. The rest of the model consists of two kinds of statements. Number statements, appearing on lines 3 and 5, describe generative steps that add new objects to the world. Dependency statements describe steps that set the value of a function on a tuple of arguments. For instance, the statement on lines 16\u201320 specifies a distribution for the observed author name NthAuthorText(c, n), conditioning on the true name Name(NthAuthor(PubCited(c), n)).\nThe objects that exist in a possible world include built-in objects (of types NaturalNum, String, etc.), guaranteed objects introduced in the BLOG model, and non-guaranteed\n1We treat predicates as function symbols with return type Boolean, and constant symbols as zero-ary function symbols.\nobjects generated by number statements. Specifically, if a number statement for type \u03c4 generates N objects, these objects are consecutively numbered pairs (\u03c4, 1), . . . , (\u03c4,N).\nA possible world is fully specified by the values of certain basic random variables. For a given BLOG model, the basic variables include a number variable for each number statement,2 and a function application variable Vf [o1, . . . , ok] for each k-ary random function f and each tuple of appropriately typed objects o1, . . . , ok that exist in any possible world. A BLOG model implicitly defines a Bayes net (BN) over its basic random variables. This BN may be infinite, and it is typically contingent: edges are active only in certain contexts [Milch et al., 2005b]. For instance, TitleText(Cit1) depends on Title((Pub, 7)) only in the context where PubCited(Cit1) = (Pub, 7)."}, {"heading": "3 METROPOLIS-HASTINGS ARCHITECTURE", "text": "Our general-purpose M-H system is implemented in Java as part of the BLOG inference engine, which is available for download from http://www.cs.berkeley.edu/\u02dcmilch/blog. Proposal distributions are written as Java classes that implement an interface called Proposer. More precisely, a class implementing the Proposer interface defines a family of proposal distributions: an object of this class defines a specific distribution q(s\u2032|sn) once it has been initialized with a particular BLOG model, observed evidence, and list of queries. Useful Proposer classes may be specialized to a particular model and type of query: for instance, a proposer for citation matching might only support BLOG models identical to that in Fig. 1 (except for variations in the number of citations); evidence that consists of observed text for each citation; and queries for whether two citations refer to the same publication (i.e., have the same PubCited value).\nTo begin the MCMC chain, the M-H engine calls a method on the Proposer object that generates an initial state s0 consistent with the evidence. Then on each iteration, the engine calls the following Proposer method:\ndouble proposeNextState(MCMCState state);\nThe state object passed in is a copy of the current state sn. The method changes the values of certain basic random variables in state so that it represents a proposed state s\u2032 chosen from q(s\u2032|sn). The proposer must ensure that s\u2032 satisfies the evidence and is complete enough to answer the queries specified at initialization. The method returns the log proposal ratio ln (q(sn|s\u2032)/q(s\u2032|sn)).\n2In models where objects generate other objects, there is a number variable for each application of a number statement to a tuple of generating objects (see [Milch et al., 2005a]).\nThe general-purpose M-H engine then computes the probability ratio p(s\u2032)/p(sn), and uses this along with the log proposal ratio to compute the acceptance probability given in Eq. 1. If it accepts the proposal, it sets sn+1 equal to the state object that was modified by the proposer; otherwise, it sets sn+1 equal to a saved copy of sn."}, {"heading": "4 MCMC STATES", "text": "MCMC states serve as the interface between the generalpurpose and application-specific parts of the generic M-H system. The obvious way to apply MCMC to a BLOG model is to let the MCMC states be possible worlds. However, the proposal distribution of [Pasula et al., 2003] does not propose complete possible worlds. A full possible world typically contains many publications that are not cited, that is, are not the value of PubCited(c) for any citation c. The world must specify the values of the Title and NthAuthor functions on all the publications that exist. But the proposal distribution discussed in Sec. 2.1 never proposes titles or authors for uncited publications.\nThe contingent BN for the citation matching model makes it clear that attributes of uncited publications are irrelevant: in a world where a publication p is uncited, the Title and NthAuthor variables defined on p are not active ancestors of query or evidence variables. Proposing values for these variables would be a waste of time. In fact, in some BLOG models \u2014 such as a model for aircraft tracking with variables State(a, t) for every aircraft a and natural number t \u2014 each possible world assigns non-null values to infinitely many variables. In such models, proposing and storing full possible worlds would require infinite time and space."}, {"heading": "4.1 EVENTS AS MCMC STATES", "text": "Our generic MCMC architecture circumvents these difficulties by allowing proposal distributions to use partial descriptions of possible worlds. For instance, the proposer for citation matching specifies the values of the PubCited function on all citations, and specifies attributes for the cited publications and their authors. Such a partial specification can be thought of as an event: a set of full possible worlds that satisfy the specification.\nThus, our system runs a Markov chain over a set \u03a3 of events, which are subsets of the outcome space \u2126. The following theorem gives conditions under which a Markov chain over \u03a3 will yield correct answers to queries.\nTheorem 1. Let p be a probability distribution over a set \u2126, E and Q be subsets of \u2126, and \u03a3 be a set of subsets of \u2126. Suppose s1, s2, . . . , sN are samples from an ergodic Markov chain over \u03a3 with stationary distribution proportional to p(s). If:\n1. \u03a3 is a partition of E; and\n2. for each s \u2208 \u03a3, either s \u2286 Q or s \u2229 Q = \u2205,\nthen 1 N \u2211N n=1 1(sn \u2286 Q) converges to p(Q|E).\nProof. Let \u03c0 be the stationary distribution of the Markov chain over \u03a3, and let Q\u0303 be the set of states {s \u2208 \u03a3 : s \u2286 Q}. Then by standard results about ergodic Markov chains, 1 N \u2211N n=1 1(sn \u2286 Q) converges to \u03c0(Q\u0303) as N \u2192 \u221e. So it suffices to show that \u03c0(Q\u0303) = p(Q|E). By definition, \u03c0(Q\u0303) = \u2211 s\u2208 eQ\n\u03c0(s). Now since \u03c0(s) is proportional to p(s):\n\u03c0(Q\u0303) =\n\u2211 s\u2208 eQ\np(s) \u2211\ns\u2208\u03a3 p(s) (2)\nBy the assumption that \u03a3 is a partition of E, we know\u2211 s\u2208\u03a3 p(s) = p(E). We now argue that the set of events Q\u0303 is a partition of Q\u2229E. To see this, consider any \u03c9 \u2208Q\u2229E. Because \u03a3 is a partition of E, there is exactly one set s \u2208 \u03a3 such that \u03c9 \u2208 s. Given that \u03c9 \u2208 s \u2229 Q, it follows by assumption 2 that s \u2286 Q. Therefore s \u2208 Q\u0303. Thus, since Q\u0303 \u2286 \u03a3, there is exactly one s\u2208 Q\u0303 containing \u03c9. So Q\u0303 is a partition of Q \u2229 E and \u2211 s\u2208 eQ p(s) = p(Q \u2229 E). Plugging into Eq. 2, we find that \u03c0(Q\u0303) = p(Q\u2229E) p(E) = p(Q|E).\nThe next section discusses a way to choose the event set \u03a3."}, {"heading": "4.2 PARTIAL INSTANTIATIONS", "text": "The most straightforward events to use as MCMC states are those corresponding to partial instantiations of the basic random variables. To satisfy Thm. 1, these partial instantiations must instantiate the evidence variables to their observed values, instantiate the query variables, and define a partition of the worlds consistent with the evidence.\nFurthermore, to compute the acceptance probability given in Eq. 1, the system must be able to compute the ratio p(s\u2032)/p(sn) for events sn, s\u2032 \u2208\u03a3. In general, it is not easy to compute the probability of a partial instantiation: for instance, if the instantiation just includes the evidence variables, then computing its probability involves summing out all the hidden variables. In some cases it is possible to sum out uninstantiated variables analytically, but our generic MCMC system currently cannot do so.\nInstead, we limit ourselves to partial instantiations whose probabilities are given by simple product expressions. These are the self-supporting instantiations: those that include all the active parents of the variables they instantiate. To say this formally, we need a bit more background on contingent BNs (see [Milch et al., 2005b] for details). In a contingent BN, the conditional probability distribution (CPD) for a variable V is given by a tree where each internal node is labeled with a parent variable U , edges out of a node are labeled with values of U , and each leaf is labeled with a probability distribution over V . A particular parent variable may occur on some paths through\nthe tree and not on others: for instance, in the tree for TitleText(Cit1), the root is labeled with PubCited(Cit1), and the variable Title((Pub, 7)) occurs only in the subtree where PubCited(Cit1) = (Pub, 7). An instantiation \u03c3 supports V if it is complete enough so that only one path through the tree is consistent with \u03c3. This path leads to a leaf with some distribution over V ; we write pV (v|\u03c3) for the probability of the value v under this distribution.\nAn instantiation is self-supporting if it supports every variable that it instantiates. By the semantics of a contingent BN, if \u03c3 is a finite, self-supporting instantiation, then:\np(\u03c3) = \u220f\nV \u2208vars(\u03c3)\npV (\u03c3(V )|\u03c3) (3)\nwhere \u03c3(V ) is the value that \u03c3 assigns to V . Thus, if we use self-supporting partial instantiations as our MCMC states, we can compute p(s\u2032)/p(sn) with no summations.\nTo satisfy the conditions of Thm. 1, we need to use selfsupporting instantiations that form a partition of E. In particular, we need to ensure that these instantiations are mutually exclusive: if some of them define overlapping events, then worlds occurring in several events will be overcounted. The following result ensures that we can avoid overlaps by using \u201cminimal\u201d instantiations.\nDefinition 1. Let V be a set of random variables, and \u03c3 be a self-supporting instantiation that instantiates V. Then \u03c3 is minimal beyond V if no sub-instantiation of \u03c3 that instantiates V is self-supporting.\nProposition 2. Let V be a set of random variables in a contingent BN. The self-supporting instantiations that are minimal beyond V are mutually contradictory.\nProof. Assume for contradiction that two distinct selfsupporting instantiations \u03c3 and \u03c4 that are minimal beyond V are both satisfied by some world \u03c9. By definition, neither \u03c3 nor \u03c4 can be a sub-instantiation of the other. Therefore \u03c3 instantiates a variable, call it X\u2217, that \u03c4 does not instantiate. Consider a graph over vars (\u03c3) where there is an edge from X to Y if the path through Y \u2019s CPD tree that is consistent with \u03c9 contains a node labeled with X . Since \u03c3 is minimal beyond V, there must be a directed path in this graph from X\u2217 to V; otherwise the sub-instantiation obtained by removing X\u2217 and all its descendents would still instantiate V and be self-supporting. But since \u03c4 is also consistent with \u03c9, \u03c4 must instantiate all the variables along this directed path in order to be self-supporting. This contradicts the assumption that \u03c4 does not instantiate X\u2217.\nWe have now identified a set of partial instantiations that satisfy the conditions of Thm. 1 and have probabilities that are easy to compute. If the evidence variables are VE and the query variables are VQ, we use the set of selfsupporting instantiations that assign the observed values to VE and are miminal beyond VE \u222a VQ."}, {"heading": "4.3 OBJECT IDENTIFIERS", "text": ""}, {"heading": "4.3.1 Proposers and interchangeable objects", "text": "Recall that in a BLOG model, the objects that satisfy a given number statement are numbered. For instance, in worlds where there are 10 publications, the publication objects are (Pub, 1), . . . , (Pub, 10). The BLOG model in Fig. 1 specifies that the value of each PubCited variable is chosen uniformly from these publication objects.\nHowever, our description of the Pasula et al. proposal distribution in Sec. 2.1 does not say how it chooses publication objects to serve as the values for PubCited variables.3 For instance, consider an MCMC state where #Pub =1000, but only 200 distinct publication objects currently serve as values for PubCited variables. When the proposer performs a split move \u2014 taking, say (Pub, 7), and splitting off some of its citations to join a new publication \u2014 which of the 800 previously uncited publications is used as the PubCited value for these citations?\nOne answer that seems reasonable is to choose the lowestnumbered uncited publication. However, in order to have non-zero acceptance probabilities, all our MCMC moves must be reversible: the reverse proposal probability q(sn|s\u2032) must be positive. Under the policy just described, a move that merges, say, (Pub, 7) into (Pub, 3) is not reversible when (Pub, 1) happens to be uncited. Any split move in the resulting state would assign citations to (Pub, 1), not (Pub, 7).\nSuch reversibility problems can be avoided by choosing the new PubCited value randomly from the publications that are uncited in the current partial instantiation. But it seems that it should not be necessary to spend time invoking the pseudo-random number generator to choose a publication, since all the uncited publications are interchangeable. Furthermore, if the proposer samples publications randomly, then it must include these sampling probabilities in the forward and backward proposal probabilities; this increases the amount of bookkeeping involved in writing a proposal distribution."}, {"heading": "4.3.2 Abstract partial instantiations", "text": "Our general MCMC system includes an additional layer of abstraction that makes it easier to write proposal distributions involving interchangeable objects. The idea is to specify MCMC states using abstract partial instantiations, in which unnumbered object identifiers can be used as both arguments and values for basic random variables. For instance, an abstract partial instantiation using the identifier Pub@A3F could say: PubCited(Cit1)= Pub@A3F, Title(Pub@A3F)= \u201cfoo\u201d. We will refer to guaranteed and\n3The original formulation of this proposer in [Pasula et al., 2003] does not include PubCited variables; the MCMC states just specify a partition of the citations into co-referring groups.\nnon-guaranteed objects that exist in possible worlds as concrete objects, to distinguish them from object identifiers.\nDefinition 2. An abstract function application variable has the form Af [o1, . . . , ok] where f is a k-ary function symbol and o1, . . . , ok are concrete objects or object identifiers. An abstract partial instantiation \u03c3 consists of a set of number variables4 and abstract function application variables, denoted vars (\u03c3), and a function that maps each element of vars (\u03c3) to a concrete object or object identifier. For each type, an abstract partial instantiation uses either object identifiers or concrete objects to represent the nonguaranteed objects, not both.\nSemantically, object identifiers can be thought of as existentially quantified logical variables. The abstract partial instantiation used as an example above is equivalent to \u2203x((PubCited(Cit1) = x) \u2227 (Title(x) = \u201cfoo\u201d)). When an abstract instantiation uses several object identifiers, they are also asserted to be distinct.\nDefinition 3. A partial instantiation \u03b3 is a concrete version of an abstract partial instantiation \u03c3 if there is a one-toone function h from object identifiers used in \u03c3 to concrete objects that exist in some world consistent with \u03b3, such that \u03c3 instantiates Af [o1, . . . , ok] if and only if \u03b3 instantiates Vf [h(o1), . . . , h(ok)], and h(\u03c3(Af [o1, . . . , ok])) = \u03b3 (Vf [h(o1), . . . , h(ok)]). A world satisfies an abstract partial instantiation \u03c3 if and only if it satisfies some concrete version of \u03c3.\nFor instance, the abstract partial instantiation:\n#Pub = 3, PubCited(Cit1) = Pub@A3F, Tit(Pub@A3F) = \u201cfoo\u201d\nhas three concrete versions:\n#Pub = 3, PubCited(Cit1) = (Pub, 1), Tit((Pub, 1)) = \u201cfoo\u201d #Pub = 3, PubCited(Cit1) = (Pub, 2), Tit((Pub, 2)) = \u201cfoo\u201d #Pub = 3, PubCited(Cit1) = (Pub, 3), Tit((Pub, 3)) = \u201cfoo\u201d"}, {"heading": "4.3.3 Probabilities of abstract instantiations", "text": "Each abstract instantiation corresponds to an event, namely the set of possible worlds that satisfy it (note that two instantiations using different object identifiers may represent the same event). But how do we compute the probability of this event?\nLemma 3. Let \u03c3 be an abstract partial instantiation for a BLOG model. Then all concrete versions of \u03c3 have the same probability, which we will call pc(\u03c3). Also, if any concrete version of \u03c3 is self-supporting, then they all are.\nThe proof of this lemma relies on the stipulation in Def. 2 that an abstract instantiation cannot use both concrete objects and object identifiers for the same type. If such mixing were allowed, then some concrete versions might have\n4In models where objects generate other objects, number variables can also be abstract.\ndifferent probabilities than others, depending on whether certain object identifiers were mapped to concrete objects used elsewhere in the instantiation.\nIf an abstract instantiation \u03c3 contains an instantiated number variable asserting that there are n objects of a given type, and \u03c3 uses m object identifiers of that type, then there are nPm , n!(n\u2212m)! distinct functions that could play the role of h in Def. 3. However, this observation does not lead to a general formula for the probability of \u03c3. The problem is that the concrete versions produced by different h functions may correspond to overlapping events. For instance, two concrete versions of the abstract instantiation (Title(Pub@A3F)= \u201cfoo\u201d) are (Title((Pub, 3))= \u201cfoo\u201d) and (Title((Pub, 7))= \u201cfoo\u201d). There are many worlds that satisfy both these concrete instantiations. Two h functions may even yield exactly the same concrete instantiation. For example, suppose \u03c3 = (Title(Pub@A3F)= \u201cfoo\u201d,Title(Pub@B46)= \u201cfoo\u201d). Here an h function that maps Pub@A3F to (Pub, 1) and Pub@B46 to (Pub, 2) yields the same concrete instantiation as one that does the opposite, since \u03c3 makes the same assertion about Pub@A3F and Pub@B46.\nThe difficulty in this last example is that \u03c3 has a non-trivial automorphism: interchanging Pub@A3F and Pub@B46 yields \u03c3 itself. In general, the number of distinct concrete versions of an abstract instantiation with a automorphisms is 1\na (nPm). But if the instantiation specifies re-\nlations among the non-guaranteed objects \u2014 for instance, publications citing one another \u2014 then counting automorphisms becomes difficult. Indeed, counting the number of automorphisms of an undirected graph is polynomially equivalent to determining whether two graphs are isomorphic [Mathon, 1979], a problem for which no polynomialtime algorithm is known. This issue of automorphisms does not just arise because we are trying to use abstract partial instantiations as MCMC states: if we required the proposer to choose non-guaranteed objects randomly, its proposal probability calculations would also need to determine how many different choices would yield the same proposal."}, {"heading": "4.3.4 A useful special case", "text": "Fortunately, for many models of practical interest, there is a simple way to avoid this issue. The abstract partial instantiations that we use for citation matching only make assertions about cited publications. That is, if \u03c3 uses an object identifier such as Pub@A3F, then \u03c3 asserts PubCited(c)= Pub@A3F for some citation c. If we apply two h functions that yield different concrete values for Pub@A3F, say (Pub, 1) and (Pub, 2), then the resulting concrete versions define disjoint events: one asserts PubCited(c)= (Pub, 1) and the other asserts PubCited(c)= (Pub, 2). In general:\nDefinition 4. An object identifier i is grounded in an ab-\nstract partial instantiation \u03c3 if there is a logical ground term ti such that every mapping function h (as in Def. 3) yields a concrete instantiation where h(i) is the value of ti.\nProposition 4. Suppose \u03c3 is an abstract partial instantiation whose concrete versions are self-supporting instantiations having probability pc(\u03c3). Let T be the set of types for which \u03c3 uses identifiers, and assume that for every type \u03c4 \u2208 T , \u03c3 instantiates a number variable asserting that there are n\u03c4 non-guaranteed objects of type \u03c4 . If every identifier used in \u03c3 is grounded, then:\np(\u03c3) = pc(\u03c3) \u220f\n\u03c4\u2208T\nn\u03c4 Pm\u03c4 (4)\nwhere m\u03c4 is the number of identifiers of type \u03c4 used in \u03c3.5\nProof. Given our discussion above, it suffices to show that the mapping functions h all yield disjoint events when applied to \u03c3. Consider any two distinct mapping functions h1 and h2, and let \u03b31 and \u03b32 be the corresponding concrete versions of \u03c3. Let i be any identifier used in \u03c3 such that h1(i) 6= h2(i). Because i is grounded in \u03c3, Def. 4 implies that there is a logical ground term ti such that ti evaluates to h1(i) in every world satisfying \u03b31, and ti evaluates to h2(i) in every world satisfying \u03b32. Since h1(i) 6= h2(i), this implies \u03b31 and \u03b32 are disjoint.\nLogical ground terms include not just expressions such as PubCited(Cit1), but also nested expressions such as NthAuthor(PubCited(Cit1), 1). The requirement that object identifiers be grounded is not burdensome in scenarios \u2014 such as citation matching \u2014 where the relevant objects are those connected to guaranteed objects by some chains of function applications. In BLOG models that involve weighted sampling or aggregation, non-guaranteed objects that do not serve as function values may become relevant. In such cases, the proposer would need to represent such objects concretely.\nIn cases where Prop. 4 applies, we can compute the probability of an abstract instantiation by just computing the probability of one of its concrete versions and then multiplying in an adjustment factor that is a product of factorials. In fact, the ratio of these adjustment factors in the acceptance probability is the same as the ratio of adjustments to the backward and forward proposal probabilities that would emerge if we required the proposal distribution to choose a distinct non-guaranteed object for each identifier randomly. But we have avoided the need to actually do this random sampling, and shifted this computation from the application-specific proposal distribution to general-purpose code.\n5This result can be extended to cases where objects generate objects; then the product is not over types, but over applications of number statements to tuples of generating objects. Abstract instantiations must be extended to specify the generating objects for each object identifier."}, {"heading": "5 PERFORMING M-H STEPS EFFICIENTLY", "text": "Our overall goal is to compute the probability ratio and update the MCMC state in time that does not grow with the number of existing objects or the number of instantiated variables. This is not always possible, but applicationspecific implementations exploit various forms of structure to do these computations in constant time. We are able to exploit some of the same structure in our generic system."}, {"heading": "5.1 DIFFERENCE DATA STRUCTURES", "text": "We said in Sec. 3 that the M-H engine saves a copy of the current state sn before passing a modifiable copy to the proposer. But making a full copy of sn would take time linear in the number of instantiated variables. Thus, our implementation does something more subtle. The state object passed to proposeNextState is actually a difference structure or \u201cpatch\u201d built on top of the current state sn. This difference structure contains a hash table that maps changed or newly instantiated basic variables to their new values, as well as a list of newly uninstantiated variables. The proposer actually just changes this patch; the underlying copy of sn is left unchanged. However, the difference structure supports all the same access methods as an ordinary MCMCState data structure: if a client asks for the value of a variable that has not been changed, the request is just passed through to the original state.\nIf the proposal is rejected, the patch is simply discarded, and sn+1 is set equal to sn. If the proposal is accepted, then sn+1 is obtained by applying the patch to sn: that is, changing the underlying state so it reflects the changes made in the patch. This operation takes time linear in the number of changed variables. The patch is then cleared, leaving it free to accept modifications from the next call to proposeNextState."}, {"heading": "5.2 COMPUTING THE ACCEPTANCE PROBABILITY", "text": "Besides maintaining the MCMC state, the main task for our general-purpose code is to compute the acceptance probability. The proposal distribution provides q(sn|s\n\u2032)/q(s\u2032|sn), so we must compute the probability ratio p(s\u2032)/p(sn). If sn and s\u2032 are represented as selfsupporting partial instantiations \u03c3n and \u03c3\u2032, Eq. 3 tells us that this ratio is:\np(\u03c3\u2032) p(\u03c3n) =\n\u220f V \u2208vars(\u03c3\u2032) pV (\u03c3\n\u2032(V )|\u03c3\u2032) \u220f\nV \u2208vars(\u03c3n) pV (\u03c3n(V )|\u03c3n)\n(5)\nComputing this ratio naively would require time proportional to the number of instantiated variables in \u03c3\u2032 and \u03c3n. But fortunately, many of the factors in the numerator and denominator may cancel.\nDefinition 5. If a partial instantiation \u03c3 supports a variable V , then the active parents of V in \u03c3 are those variables that occur as labels on nodes in V \u2019s CPD tree on the unique path that is consistent with \u03c3.\nProposition 5. Suppose two partial instantiations \u03c3 and \u03c3\u2032 agree on a variable V and on all the variables that are active parents of V in \u03c3. Then pV (\u03c3\u2032(V )|\u03c3\u2032) = pV (\u03c3(V )|\u03c3). Also, V has the same active parents in \u03c3\u2032 as in \u03c3.\nThus, we only need to compute the factors for variables that are newly instantiated, uninstantiated, or changed in \u03c3\u2032, or whose active parents have changed values. Because we are explicitly representing the differences between \u03c3\u2032 and \u03c3n (see Sec. 5.1), we can identify the changed variables efficiently. However, it is not so easy to identify variables whose active parents have changed. We can enumerate V \u2019s active parents in \u03c3n by walking through V \u2019s CPD tree. But if only a few variables have changed, we don\u2019t want to iterate over all variables, seeing which ones happen to have a changed variable as an active parent.\nTo avoid this iteration, we maintain a graph over the instantiated variables in \u03c3n; this graph contains those edges from the BLOG model\u2019s contingent BN that are active in \u03c3n. Each variable has pointers to its children, that is, the variables of which it is an active parent in \u03c3n. Given this data structure, we can efficiently enumerate the children of all variables that are changed in \u03c3\u2032. The graph is constructed on the initial state, and then updated after each accepted proposal to reflect newly active or inactive parent relationships in \u03c3\u2032. Conveniently, by Prop. 5, we only need to update a variable\u2019s active parent set if one of its active parents in \u03c3n has changed \u2013 and we are enumerating these variables anyway to recompute their probability factors.\nIf we use abstract partial instantiations, the probability ratio includes the factorial adjustment factors given in Eq. 4. Again, computing these factorials naively would take time linear in the magnitudes of the number variables. But if the proposal makes small changes to the values of number variables and the number of used identifiers, then most of the factors inside the factorials cancel out.\nThe calculation techniques presented here do have some limitations. One is that a variable\u2019s child set may grow linearly with the number of objects. In the citation matching model, where the probability that a PubCited variable takes on any particular value in a world with N publications is 1/N , the #Pub variable is always an active parent of all PubCited variables. So the time required to compute the acceptance probability for a proposal that changes the number of publications grows linearly with the number of citations. This slowdown could be avoided by recognizing that every PubCited variable makes the same contribution the probability ratio, so we can compute this contribution once and raise it to the power of the number of citations. However, our current implementation does not detect when\nthis can be done. Conversely, a variable\u2019s active parent set may grow linearly with the number of hypothesized objects: this happens in cases of weighted sampling or aggregation. Finally, our approach does not allow the system to detect cancellations between the p(s\u2032) and q(s\u2032|sn) factors, such as occur in Gibbs sampling [Gelman, 1992]."}, {"heading": "6 EXPERIMENTS", "text": "We have developed a BLOG model and proposal distribution for the citation matching domain. Earlier work on applying M-H to this task [Pasula et al., 2003] used an implementation hand-coded in Lisp. Unfortunately, we do not know all the details of the model and proposal distribution used in that implementation, nor do we have data on its running time. However, we do have all these details for an application-specific Java system that we implemented in the summer of 2003. Our BLOG implementation almost exactly reproduces the model and proposal distribution used in this hand-coded Java system, which thus serves as our reference for speed and accuracy comparisons.\nThe BLOG model we use is an elaboration of the one shown in Fig. 1. The prior distributions for author names and titles are n-gram models learned from the author and title fields of a large BibTeX file; some parameters of the citation formating model are estimated from a set of handsegmented citations. Other parameters, such as typo probabilities, are set by hand; the exact values of these parameters have little influence on the accuracy results. The proposal distribution uses split-merge moves of the kind described in [Jain and Neal, 2004].\nTable 1 shows results on four sets of about 300\u2013500 unparsed citations that were collected by [Lawrence et al., 1999]. The files are annotated with the true clustering of citations into co-referring groups; the accuracy metric is the fraction of true clusters recovered exactly. All of the M-H implementations achieve better accuracy than the [Lawrence et al., 1999] technique, with the [Pasula et al., 2003] implementation doing best by a significant margin. The [Pasula et al., 2003] implementation outperforms the others because it uses more sophisticated prior distributions for author names and citation formats, and more finely tuned heuristics for proposing parses of citations. There is little difference in accuracy between the handcoded Java implementation and the general-purpose BLOG engine; this is to be expected, since they implement approximately the same model and proposal distribution.\nThe timing results in Table 1 reflect the time required to initialize the system and run MCMC for 10,000 samples. Both systems display significant variation in run time across data sets; this reflects differences in the average number of citations affected by split-merge moves (the data sets have different ratios of citations to publications) and differences in the fraction of proposals that are accepted. However,\nthe BLOG engine consistently takes 5 times as long as the hand-coded Java implementation.\nThere are three main reasons for this difference. First, in the hand-coded implementation, an MCMC state is represented as a collection of Java objects of application-specific classes such as Publication and Citation. The current values of functions such as Title and PubCited are stored in fields on those objects. By contrast, the general BLOG engine does not include specialized Java classes for the citation domain; it uses a hash table that maps functions and argument tuples to values. Thus, accessing and updating the state is considerably slower in the BLOG implementation. Second, the hand-coded implementation includes special code for determining which variables are affected by moves that are proposed by its specific proposal distribution. In order to support arbitrary proposals, the BLOG engine must look at the list of variables changed by the proposal, find their children in the current BN graph, and (if the proposal is accepted) update the BN graph to reflect dependencies that are active in the new world. Finally, computing the probabilities of variables given their parents is slower in the BLOG implementation. The BLOG engine must interpret if\u2013then clauses that occur in the BLOG model (e.g., lines 10 and 23 of Fig. 1) and explicitly store values in the MCMC state for intermediate variables, such as AuthListSuffix in Fig. 1. In the hand-coded implementation, if-statements and local variables can be written into the Java code, allowing faster execution.\nAs a result, while the hand-coded implementation does 10,000 samples in 12\u201320 seconds, the BLOG engine takes 60\u2013100 seconds. However, our notes from summer of 2003 indicate that with the computers and Java runtime environment we had then, the hand-coded implementation ran in about 120 seconds. In other words, our computing infrastructure has improved enough that a general system runs faster than a hand-coded system did three years ago."}, {"heading": "7 CONCLUSIONS", "text": "We have described a general MCMC inference system that just requires the user to provide a BLOG model and a proposal distribution. Our main contribution is a semantics for MCMC states that do not fully specify a possible world. By allowing partial world descriptions, we support proposal distributions that do not instantiate irrelevant variables or assign numbers to interchangeable objects. We also show how to use a context-specific Bayes net graph to determine efficiently what factors in the acceptance probability need to be computed for a given proposal.\nOur current system still requires that the user implement a proposal distribution, which can be a significant undertaking. Other sampling-based approaches to approximate inference require less customization, and allow correspondingly less flexibility. For instance, the widely used BUGS system [Gilks et al., 1994] allows users to run Gibbs sampling on a wide range of graphical models with no additional programming. However, Gibbs sampling falls short in scenarios where it is difficult to move between highprobability hypotheses by changing one variable at a time. In such cases, M-H algorithms can explore the posterior distribution more efficiently. There has been some recent work on adding generic M-H capabilities to BUGS using adaptive proposal distributions [Lunn et al., 2005].\nAnother approach to automatic approximate inference is forward sampling: using the model\u2019s CPDs to sample variables given their parents. Milch et al. [2005b] use forward sampling in a general likelihood weighting algorithm for contingent BNs. Jaeger [2006] explores several variations on forward sampling for relational Bayesian networks, including a version where values are also propagated up from evidence nodes through deterministic dependencies. Angelopoulos and Cussens [2001; 2005], on the other hand, use forward sampling in a proposal distribution within an\nM-H algorithm. Their models are represented as stochastic logic programs, which define distributions over Prolog proof trees; the proposal distribution resamples a sub-tree of the current proof tree. This algorithm has been successful on several applications. However, it seems that more data-driven proposal distributions are needed for applications such as citation matching, where forward sampling has a negligible probability of yielding author names and publication titles consistent with the observed citations.\nClearly there is more work to be done on general-purpose inference for relational probabilistic models. In the citation matching domain, we are extending our BLOG model and proposal distribution to simultaneously reconstruct the publications, researchers, and venues mentioned in a set of citations. We also plan to develop BLOG models and proposal distributions for other tasks, such as resolving coreference among names and pronouns in newswire articles. We hope that through these efforts, we will come to understand some common principles that underlie effective proposal distributions for various tasks. This understanding should lead toward the development of a library of (possibly adaptive) proposal distribution modules, which can be combined to yield effective proposal distributions for new tasks with little or no programming."}, {"heading": "Acknowledgements", "text": "This work was supported by DARPA IPTO under the CALO project (03\u2013000219) and the Effective Bayesian Transfer Learning project (FA8750\u201305\u20132\u20130249). B. Milch was also supported by a Siebel Scholarship."}], "references": [{"title": "Markov chain Monte Carlo using tree-based priors on model structure", "author": ["N. Angelopoulos", "J. Cussens"], "venue": "Proc. 17th UAI, pages 16\u201323", "citeRegEx": "Angelopoulos and Cussens. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Exploiting informative priors for Bayesian classification and regression trees", "author": ["N. Angelopoulos", "J. Cussens"], "venue": "Proc. 19th IJCAI", "citeRegEx": "Angelopoulos and Cussens. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Tracking and Data Association", "author": ["Y. Bar-Shalom", "T.E. Fortmann"], "venue": "Academic Press, Boston", "citeRegEx": "Bar.Shalom and Fortmann. 1988", "shortCiteRegEx": null, "year": 1988}, {"title": "A theory for record linkage", "author": ["I. Fellegi", "A. Sunter"], "venue": "JASA, 64:1183\u20131210", "citeRegEx": "Fellegi and Sunter. 1969", "shortCiteRegEx": null, "year": 1969}, {"title": "Iterative and non-iterative sampling algorithms", "author": ["A. Gelman"], "venue": "Comput. Sci. and Stat., 24:433\u2013438", "citeRegEx": "Gelman. 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "A language and program for complex Bayesian modelling", "author": ["W.R. Gilks", "A. Thomas", "D.J. Spiegelhalter"], "venue": "The Statistician, 43(1):169\u2013177", "citeRegEx": "Gilks et al.. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Monte Carlo sampling methods using Markov chains and their applications", "author": ["W.K. Hastings"], "venue": "Biometrika, 57:97\u2013 109", "citeRegEx": "Hastings. 1970", "shortCiteRegEx": null, "year": 1970}, {"title": "Complex probabilistic modeling with recursive relational Bayesian networks", "author": ["M. Jaeger"], "venue": "Annals of Math and AI, 32:179\u2013220", "citeRegEx": "Jaeger. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Importance sampling on relational Bayesian networks", "author": ["M. Jaeger"], "venue": "Probabilistic, Logical and Relational Learning\u2014Towards a Synthesis, number 05051 in Dagstuhl Seminar Proceedings", "citeRegEx": "Jaeger. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model", "author": ["S. Jain", "R.M. Neal"], "venue": "J. Computational and Graphical Statistics, 13:158\u2013182", "citeRegEx": "Jain and Neal. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Of starships and Klingons: First-order Bayesian logic for the 23rd century", "author": ["K.B. Laskey", "P.C.G. da Costa"], "venue": "Proc. 21st UAI, pages 346\u2013353,", "citeRegEx": "Laskey and da Costa. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Autonomous citation matching", "author": ["S. Lawrence", "C.L. Giles", "K.D. Bollacker"], "venue": "Proc. 3rd Int\u2019l Conf. on Autonomous Agents, pages 392\u2013393", "citeRegEx": "Lawrence et al.. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "Generic reversible jump MCMC using graphical models", "author": ["D.J. Lunn", "N. Best", "J. Whittaker"], "venue": "Tech Report EPH-2005-01, Dept. of Epidemiology and Public Health, Imperial College London", "citeRegEx": "Lunn et al.. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "A note on the graph isomorphism counting problem", "author": ["R. Mathon"], "venue": "Inform. Process. Lett., 8(3):131\u2013132", "citeRegEx": "Mathon. 1979", "shortCiteRegEx": null, "year": 1979}, {"title": "Efficient clustering of high-dimensional data sets with application to reference matching", "author": ["A. McCallum", "K. Nigam", "L.H. Ungar"], "venue": "Proc. 6th KDD", "citeRegEx": "McCallum et al.. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Equations of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "J. Chemical Physics, 21:1087\u2013 1092", "citeRegEx": "Metropolis et al.. 1953", "shortCiteRegEx": null, "year": 1953}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "Proc. 19th IJCAI", "citeRegEx": "Milch et al.. 2005a", "shortCiteRegEx": null, "year": 2005}, {"title": "Approximate inference for infinite contingent Bayesian networks", "author": ["B. Milch", "B. Marthi", "D. Sontag", "S. Russell", "D.L. Ong", "A. Kolobov"], "venue": "Proc. 10th AISTATS", "citeRegEx": "Milch et al.. 2005b", "shortCiteRegEx": null, "year": 2005}, {"title": "Variable-structure systems from graphs and grammars", "author": ["E. Mjolsness"], "venue": "Technical Report 05-09, School of Info. and Comp. Sci., UC Irvine", "citeRegEx": "Mjolsness. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Markov chain Monte Carlo data association for general multiple-target tracking problems", "author": ["S. Oh", "S. Russell", "S. Sastry"], "venue": "Proc. 43rd IEEE Conf. on Decision and Control, pages 735\u2013742", "citeRegEx": "Oh et al.. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "Identity uncertainty and citation matching", "author": ["H. Pasula", "B. Marthi", "B. Milch", "S. Russell", "I. Shpitser"], "venue": "NIPS 15. MIT Press, Cambridge, MA", "citeRegEx": "Pasula et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "IBAL: A probabilistic rational programming language", "author": ["A. Pfeffer"], "venue": "Proc. 17th IJCAI", "citeRegEx": "Pfeffer. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "MLJ, 62:107\u2013136", "citeRegEx": "Richardson and Domingos. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "An integrated", "author": ["B. Wellner", "A. McCallum", "F. Peng", "M. Hay"], "venue": "conditional model of information extraction and coreference with application to citation matching. In Proc. 20th UAI", "citeRegEx": "Wellner et al.. 2004", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "Specialized algorithms have been developed in the fields of record linkage [Fellegi and Sunter, 1969] and data association [Bar-Shalom and Fortmann, 1988] that yield approximate solutions for particular classes of models.", "startOffset": 75, "endOffset": 101}, {"referenceID": 2, "context": "Specialized algorithms have been developed in the fields of record linkage [Fellegi and Sunter, 1969] and data association [Bar-Shalom and Fortmann, 1988] that yield approximate solutions for particular classes of models.", "startOffset": 123, "endOffset": 154}, {"referenceID": 15, "context": "[2004] use the MetropolisHastings (M-H) algorithm [Metropolis et al., 1953; Hastings, 1970] to construct a Markov chain whose stationary distribution is proportional to p(s).", "startOffset": 50, "endOffset": 91}, {"referenceID": 6, "context": "[2004] use the MetropolisHastings (M-H) algorithm [Metropolis et al., 1953; Hastings, 1970] to construct a Markov chain whose stationary distribution is proportional to p(s).", "startOffset": 50, "endOffset": 91}, {"referenceID": 16, "context": "To represent the model, we use Bayesian logic (BLOG) [Milch et al., 2005a], a language that allows us to express uncertainty about the number of latent objects and the relations among objects and observations.", "startOffset": 53, "endOffset": 74}, {"referenceID": 21, "context": "Many of our results would also be applicable to other probabilistic modeling languages [Pfeffer, 2001; Jaeger, 2001; Laskey and da Costa, 2005; Richardson and Domingos, 2006; Mjolsness, 2005].", "startOffset": 87, "endOffset": 191}, {"referenceID": 7, "context": "Many of our results would also be applicable to other probabilistic modeling languages [Pfeffer, 2001; Jaeger, 2001; Laskey and da Costa, 2005; Richardson and Domingos, 2006; Mjolsness, 2005].", "startOffset": 87, "endOffset": 191}, {"referenceID": 10, "context": "Many of our results would also be applicable to other probabilistic modeling languages [Pfeffer, 2001; Jaeger, 2001; Laskey and da Costa, 2005; Richardson and Domingos, 2006; Mjolsness, 2005].", "startOffset": 87, "endOffset": 191}, {"referenceID": 22, "context": "Many of our results would also be applicable to other probabilistic modeling languages [Pfeffer, 2001; Jaeger, 2001; Laskey and da Costa, 2005; Richardson and Domingos, 2006; Mjolsness, 2005].", "startOffset": 87, "endOffset": 191}, {"referenceID": 18, "context": "Many of our results would also be applicable to other probabilistic modeling languages [Pfeffer, 2001; Jaeger, 2001; Laskey and da Costa, 2005; Richardson and Domingos, 2006; Mjolsness, 2005].", "startOffset": 87, "endOffset": 191}, {"referenceID": 20, "context": "2 by describing the probability model and M-H implementation used by [Pasula et al., 2003] to resolve coreference among citations.", "startOffset": 69, "endOffset": 90}, {"referenceID": 16, "context": "We then briefly review the BLOG language of [Milch et al., 2005a].", "startOffset": 44, "endOffset": 65}, {"referenceID": 20, "context": "The proposal distribution of [Pasula et al., 2003] is based on moves that split and merge clusters of co-referring citations.", "startOffset": 29, "endOffset": 50}, {"referenceID": 14, "context": "In a preprocessing step, the citations are grouped into canopies: overlapping groups of citations that are similar enough (according to a rough heuristic) that they have a non-negligible chance of co-referring [McCallum et al., 2000].", "startOffset": 210, "endOffset": 233}, {"referenceID": 11, "context": "This accuracy is significantly better than that reported by the developers of Citeseer [Lawrence et al., 1999], and is competitive with more recent discriminative methods [Wellner et al.", "startOffset": 87, "endOffset": 110}, {"referenceID": 23, "context": ", 1999], and is competitive with more recent discriminative methods [Wellner et al., 2004].", "startOffset": 68, "endOffset": 90}, {"referenceID": 16, "context": "Bayesian logic (BLOG) [Milch et al., 2005a] is a language for defining probability distributions over structures that can contain varying numbers of objects with varying re-", "startOffset": 22, "endOffset": 43}, {"referenceID": 17, "context": "This BN may be infinite, and it is typically contingent: edges are active only in certain contexts [Milch et al., 2005b].", "startOffset": 99, "endOffset": 120}, {"referenceID": 16, "context": "In models where objects generate other objects, there is a number variable for each application of a number statement to a tuple of generating objects (see [Milch et al., 2005a]).", "startOffset": 156, "endOffset": 177}, {"referenceID": 20, "context": "However, the proposal distribution of [Pasula et al., 2003] does not propose complete possible worlds.", "startOffset": 38, "endOffset": 59}, {"referenceID": 17, "context": "To say this formally, we need a bit more background on contingent BNs (see [Milch et al., 2005b] for details).", "startOffset": 75, "endOffset": 96}, {"referenceID": 20, "context": "The original formulation of this proposer in [Pasula et al., 2003] does not include PubCited variables; the MCMC states just specify a partition of the citations into co-referring groups.", "startOffset": 45, "endOffset": 66}, {"referenceID": 13, "context": "Indeed, counting the number of automorphisms of an undirected graph is polynomially equivalent to determining whether two graphs are isomorphic [Mathon, 1979], a problem for which no polynomialtime algorithm is known.", "startOffset": 144, "endOffset": 158}, {"referenceID": 4, "context": "Finally, our approach does not allow the system to detect cancellations between the p(s) and q(s\u2032|sn) factors, such as occur in Gibbs sampling [Gelman, 1992].", "startOffset": 143, "endOffset": 157}, {"referenceID": 20, "context": "Earlier work on applying M-H to this task [Pasula et al., 2003] used an implementation hand-coded in Lisp.", "startOffset": 42, "endOffset": 63}, {"referenceID": 9, "context": "The proposal distribution uses split-merge moves of the kind described in [Jain and Neal, 2004].", "startOffset": 74, "endOffset": 95}, {"referenceID": 11, "context": "Table 1 shows results on four sets of about 300\u2013500 unparsed citations that were collected by [Lawrence et al., 1999].", "startOffset": 94, "endOffset": 117}, {"referenceID": 11, "context": "All of the M-H implementations achieve better accuracy than the [Lawrence et al., 1999] technique, with the [Pasula et al.", "startOffset": 64, "endOffset": 87}, {"referenceID": 20, "context": ", 1999] technique, with the [Pasula et al., 2003] implementation doing best by a significant margin.", "startOffset": 28, "endOffset": 49}, {"referenceID": 20, "context": "The [Pasula et al., 2003] implementation outperforms the others because it uses more sophisticated prior distributions for author names and citation formats, and more finely tuned heuristics for proposing parses of citations.", "startOffset": 4, "endOffset": 25}, {"referenceID": 11, "context": "Table 1: Citation matching results for the phrase matching algorithm of [Lawrence et al., 1999], the hand-coded M-H implementation used by [Pasula et al.", "startOffset": 72, "endOffset": 95}, {"referenceID": 20, "context": ", 1999], the hand-coded M-H implementation used by [Pasula et al., 2003], a simpler M-H implementation hand-coded in Java, and the BLOG inference engine.", "startOffset": 51, "endOffset": 72}, {"referenceID": 5, "context": "For instance, the widely used BUGS system [Gilks et al., 1994] allows users to run Gibbs sam-", "startOffset": 42, "endOffset": 62}, {"referenceID": 12, "context": "There has been some recent work on adding generic M-H capabilities to BUGS using adaptive proposal distributions [Lunn et al., 2005].", "startOffset": 113, "endOffset": 132}], "year": 0, "abstractText": "Tasks such as record linkage and multi-target tracking, which involve reconstructing the set of objects that underlie some observed data, are particularly challenging for probabilistic inference. Recent work has achieved efficient and accurate inference on such problems using Markov chain Monte Carlo (MCMC) techniques with customized proposal distributions. Currently, implementing such a system requires coding MCMC state representations and acceptance probability calculations that are specific to a particular application. An alternative approach, which we pursue in this paper, is to use a general-purpose probabilistic modeling language (such as BLOG) and a generic Metropolis-Hastings MCMC algorithm that supports user-supplied proposal distributions. Our algorithm gains flexibility by using MCMC states that are only partial descriptions of possible worlds; we provide conditions under which MCMC over partial worlds yields correct answers to queries. We also show how to use a context-specific Bayes net to identify the factors in the acceptance probability that need to be computed for a given proposed move. Experimental results on a citation matching task show that our general-purpose MCMC engine compares favorably with an application-specific system.", "creator": null}}}