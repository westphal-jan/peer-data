{"id": "1209.3352", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2012", "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs", "abstract": "thompson sampling is one of the oldest heuristics technologies for multi - agent armed bandit problems. it is a randomized algorithm based on related bayesian ideas, observed and has recently generated sustained significant field interest after several studies demonstrated it tends to have better empirical performance compared relative to the state of the art methods. however, extremely many questions regarding its theoretical performance remained open. in this paper, we design and analyze shannon thompson sampling algorithm for the contextual periodic multi - armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. this is perhaps the most important and hence widely still studied version of the contextual bandits problem. we prove a high probability regret bound of $ \\ tilde { l o } ( \\ & frac { 1 } { \\ sqrt { \\ epsilon } } \\ sqrt { t ^ { 1 + \\ epsilon } } d ) $ run in time $ t $ for any $ 0 & lt ; \\ epsilon & lt ; 1 $, where $ * d $ is the dimension condition of each context vector \u03c3 and $ \\ epsilon $ is a parameter used by the algorithm. coincidentally our results provide the first theoretical guarantees for the contextual version of thompson trouble sampling, and are close close to the empirical lower bound of $ \\ * omega ( \\ sqrt { td } ) $ for this problem. this essentially solves the colt open problem of chapelle and li [ colt 2012 ] regarding regret bounds for minimum thompson sampling for robust contextual bandits adversary problem.", "histories": [["v1", "Sat, 15 Sep 2012 03:27:11 GMT  (23kb)", "http://arxiv.org/abs/1209.3352v1", null], ["v2", "Fri, 22 Feb 2013 18:35:56 GMT  (30kb)", "http://arxiv.org/abs/1209.3352v2", null], ["v3", "Thu, 30 Jan 2014 07:00:54 GMT  (30kb)", "http://arxiv.org/abs/1209.3352v3", "Improvements from previous version: (1) dependence on d improved from d^2 to d^{3/2} (2) Simpler and more modular proof techniques"], ["v4", "Mon, 3 Feb 2014 07:09:03 GMT  (30kb)", "http://arxiv.org/abs/1209.3352v4", "Improvements from previous version: (1) dependence on d improved from d^2 to d^{3/2} (2) Simpler and more modular proof techniques (3) bounds in terms of log(N) added"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS stat.ML", "authors": ["shipra agrawal", "navin goyal"], "accepted": true, "id": "1209.3352"}, "pdf": {"name": "1209.3352.pdf", "metadata": {"source": "CRF", "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs", "authors": ["Shipra Agrawal"], "emails": ["shipra@microsoft.com", "navingo@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 9.\n33 52\nv1 [\ncs .L\nG ]\n1 5\nSe p\n20 12\n\u01eb\n\u221a T 1+\u01ebd) in time\nT for any 0 < \u01eb < 1, where d is the dimension of each context vector and \u01eb is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of \u2126( \u221a Td) for this problem. This essentially solves the COLT open problem of Chapelle and Li [COLT 2012] regarding regret bounds for Thompson Sampling for contextual bandits problem.\nOur version of Thompson sampling uses Gaussian prior and Gaussian likelihood function. Our novel martingale-based analysis techniques also allow easy extensions to the use of more general distributions, satisfying certain general conditions.\n0"}, {"heading": "1 Introduction", "text": "Multi-armed bandit (MAB) problems model the exploration/exploitation trade-off inherent in many sequential decision problems. There are many versions of multi-armed bandit problems; a particularly useful version is the contextual multi-armed bandit problem. In this problem, on each of T rounds, a learner is presented with the choice of taking one of N actions, referred to as N arms. Before making the choice of which arm to play, the learner sees a d-dimensional feature vector bi, referred to as \u201ccontext\u201d, associated with each arm i. The learner uses these feature vectors along with the feature vectors and rewards of arms played by her in the past to make the choice of arm. Over time, the learner\u2019s aim is gather enough information about how the feature vectors and rewards are related to each other, so that she can predict, with some certainty, which arm will give the best reward by looking at the feature vectors. The learner competes with a class of predictors, in which each predictor takes in the feature vectors and predicts which arm will give the best reward. If the learner can guarantee to do nearly as well as the predictions of the best predictor in hindsight (to have low regret), the learner is said to successfully compete with that class.\nIn the contextual bandits setting with linear payoff functions, the learner competes with the class of all \u201clinear\u201d predictors on the feature vectors. That is, a predictor is defined by N d-dimensional parameters {\u00b5\u0303i}Ni=1, and the predictor ranks the arms according to bTi \u00b5\u0303i. We consider the contextual bandit problem under the linear realizability assumption, that is, we assume that there are unknown underlying parameters {\u00b5i}Ni=1 such that the expected reward for each arm i, given context bi, is bTi \u00b5i. Under this realizability assumption, the linear predictor corresponding to {\u00b5i}Ni=1 is in fact the best predictor and the learner\u2019s aim is to learn these underlying parameters. This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].\nIn this paper, we analyze Thompson Sampling (TS) algorithm for the contextual bandits problem with linear payoffs. Thompson Sampling is one the earliest heuristics for the multi-armed bandit problems. The first version of this Bayesian heuristic is around 80 years old, dating to Thompson (1933) [25]. Since then, it got rediscovered numerous times independently in the context of reinforcement learning, e.g., in [27, 20, 24]. It is a member of the family of randomized probability matching algorithms. The basic idea is to assume a simple prior distribution on the underlying parameters of the reward distribution of every arm, and at every time step, play an arm according to its posterior probability of being the best arm. The general structure of Thompson sampling for the contextual bandits problem involves the following elements:\n1. a set \u0398 of parameters \u00b5\u0303;\n2. an assumed prior distribution P (\u00b5\u0303) on these parameters; 3. past observation D consisting of (context b, reward r) for the past time steps; 4. an assumed likelihood function P (r|b, \u00b5\u0303), which gives the probability of reward given a context\nb and a parameter \u00b5\u0303;\n5. a posterior distribution P (\u00b5\u0303|D) \u221d P (D|\u00b5\u0303)P (\u00b5\u0303), where P (D|\u00b5\u0303) is the likelihood function.\nIn each round, TS plays an arm according to its posterior probability of maximizing the expected reward. A simple way to achieve that is to produce a sample of reward for each arm, using the posterior distributions, and play the arm that produces the largest sample. We emphasize that although TS algorithm is a Bayesian approach, the description of the algorithm and our analysis apply to the prior-free stochastic MAB model, and are directly comparable to the UCB family of\nalgorithms which are a frequentist approach to the same problem. One could interpret the Bayesian priors used by the TS algorithm as a way of capturing the current knowledge about the arms.\nRecently, TS has attracted considerable attention. Several studies (e.g., [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques. Chapelle and Li [7] demonstrate that for the basic stochastic MAB problem, empirically TS achieves regret comparable to the lower bound of [16]; and in applications like display advertising and news article recommendation modeled by the contextual bandits problem, it is competitive to or better than the other methods such as UCB. In their experiments, TS is also more robust to delayed or batched feedback than the other methods. TS has also been used in an industrial scale application for CTR prediction of search ads on search engines [12]. Kaufmann et al. [15] do a thorough comparison of TS with the best known versions of UCB, and show that TS has the lowest regret in the long run.\nDespite being easy to implement and being competitive to the state of the art methods, the theoretical understanding of TS algorithm is limited. [13, 18] provided weak guarantees, namely, a bound of o(T ) on expected regret in time T . More recently, some significant progress was made by [3, 15], who provided near-optimal problem-dependent bounds on the expected regret of TS for the basic (i.e. without contexts) version of the stochastic MAB problem. However, many questions regarding theoretical analysis of TS remained open, including near-optimal problem-independent regret bounds, high probability regret bounds, and regret bounds for the more general contextual bandits setting. Some of these questions were formally raised as a COLT 2012 open problem [8]. In this paper, we use novel and simple martingale-based analysis techniques to demonstrate that TS achieves high probability, near-optimal problem independent regret bounds for contextual bandits with linear payoffs. To our knowledge, ours are the first non-trivial regret bounds for TS for the contextual bandits problem. Additionally, our results are the first high probability regret bounds for TS, even in the case of basic MAB problem. This essentially solves the COLT 2012 open problem [8] for linear contextual bandits.\nThe contextual MAB problem does not seem easily amenable to the techniques used so far for analyzing the basic MAB problem by [3, 15]. In Section 2.3, we describe some of the challenges, and our martingale-based solution ideas to handle them. Our version of Thompson Sampling algorithm, described formally in Section 2.2, uses Gaussian prior and Gaussian likelihood functions. As we discuss towards the end of the paper in Section 4, our techniques are easily extensible to the use of other prior distributions, satisfying certain conditions."}, {"heading": "1.1 Our Results", "text": "The formal problem statement appears in Sec. 2.1.\nTheorem 1. For the contextual bandit problem with linear payoffs, with probability 1 \u2212 \u03b4, the total regret in time T for Thompson Sampling is bounded by O ( d \u221a NT 1+\u01eb\n\u01eb lnN lnT ln 1 \u03b4\n)\n, for any\n0 < \u01eb < 1. Here, \u01eb is a parameter used by the Thompson Sampling algorithm.\nTheorem 2. When \u00b51 = \u00b52 \u00b7 \u00b7 \u00b7 = \u00b5N = \u00b5, i.e. there is a single underlying d-dimensional parameter \u00b5, then with probability 1 \u2212 \u03b4, the total regret in time T for Thompson Sampling is bounded by O ( d \u221a T 1+\u01eb\n\u01eb lnN lnT ln 1 \u03b4\n)\n.\nRemark 1. Here 0 < \u01eb < 1 can be chosen to be any constant. If T is known, one could choose \u01eb = 1lnT , to get O\u0303(d \u221a NT ) (and O\u0303(d \u221a T )) regret bound.\nRemark 2. Note that Theorem 2 has only a logarithmic dependence on the number of arms N , which makes it particularly useful when the number of arms N is very large, but there is a single underlying d-dimensional parameter \u00b5, with d being much smaller than N . One could also recover the setting with different \u00b5is from the setting with a single \u00b5, by letting \u00b5 be an Nd-dimensional vector formed by appending all the \u00b5is, and letting context bi(t) be an Nd-dimensional vector which is 0 in all but d positions corresponding to arm i. However, a direct application of Theorem 2 would then give a slightly weaker bound of O\u0303( 1\u221a\n\u01eb Nd\n\u221a T 1+\u01eb) compared to Theorem 1.\nWe will mainly describe the algorithm and regret analysis for the setting of single parameter \u00b5 = \u00b51 = \u00b7 \u00b7 \u00b7\u00b5N , i.e. the proof of Theorem 2. The algorithm and analysis for the setting with different \u00b5is is similar. In Section 4, we describe the changes required for the latter setting in order to get the result of Theorem 1."}, {"heading": "1.2 Related Work", "text": "The contextual bandit problem with linear payoffs is a widely studied problem in statistics and machine learning often under different names as mentioned by Chu et al. [9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5]. The name contextual bandits was coined in Langford and Zhang [17].\nChu et al. [9] show that for any algorithm the regret is \u2126( \u221a Td) for d2 \u2264 T for the N -armed contextual bandits problem with linear payoffs and single parameter. Auer [4] and Chu et al. [9] SupLinUCB, a complicated algorithm using UCB as a subroutine, for this problem. Chu et al. achieve a regret bound of O( \u221a\nTd ln3(NT ln(T )/\u03b4)) with probability at least 1\u2212 \u03b4 (Auer [4] proves similar results). Let us compare these results with ours. Our bounds have a factor of d compared to \u221a d in the bounds just mentioned. As can be observed in our regret analysis, the extra \u221a d factor in our bounds appears because we will use a concentration inequality that provides only a concentration of O( \u221a\nd ln 1\u03b4 ) for the empirical estimate of the mean rewards around the actual mean.\nThe advantage of using this (weaker though more generally applicable) concentration inequality is that it allows for statistical dependence between the samples used in the estimates of the mean rewards, which could be due to the dependence between the past rewards and the future choice of arms, or because the contexts are generated by an adaptive adversary. By contrast, in the analysis of SupLinUCB, [4, 9] consider only oblivious adversary, and achieve statistical independence of samples by using a complicated master procedure SupLin on top of the basic UCB style algorithm. This allows them to use a stronger concentration of O( \u221a\nln 1\u03b4 ) given by the Azuma-Hoeffding\ninequality. We do not use any such master algorithm. A closely related setting is that of linear stochastic bandits problem, e.g. [10, 1]. In linear stochastic bandits problem, every arm i is associated with a known fixed vector bi, and the expected reward of the arm, when played, is bTi \u00b5 for some common unknown underlying parameter \u00b5. AbbasiYadkori et al. [1] analyze a UCB-style algorithm for that problem. When adapted to our setting, their regret bound is O(d log (T ) \u221a T + \u221a\ndT log (T/\u03b4)). Note that their regret bound does not depend on N , and thus can even be applied to infinite set of arms, for example, when the set\nof arms is specified as those corresponding to all vectors in a d-dimensional polytope. The lower bound for this setting was given by Dani et al. [10] as \u2126(d \u221a T ). The state-of-the-art bounds for linear bandits problem in case of finite N are given by [6]. They provide an algorithm based on exponential weights, with regret of order \u221a dT logN for any finite set of N actions. However, their setting is slightly different from ours. They consider a non-stochastic (adversarial) bandit setting where the reward at time t for arm i is bTi \u00b5t with \u00b5t chosen by an adversary. The set of arms and the associated bi vectors are non-adaptive and fixed in advance.\nWhile the results in this paper do not claim to provide regret bounds for Thompson Sampling algorithm that match or better the best available bounds of this extensively studied problem, our bounds for this natural and efficient heuristic are close to the best bounds. Our bounds are essentially within a factor of \u221a d lnT of the best bounds for finite N (those for UCB1 by [9, 4],\nand for Exp2 algorithm by [6]), and within \u221a lnN factor of the best bounds that do not depend on N (by Abbasi-Yadkori et al. [1]). The main contribution of this paper is to provide tools for the analysis of Thompson Sampling algorithm for contextual bandits, which despite of being popular and empirically attractive, has eluded theoretical analysis. While significant recent progress was made in analyzing it for basic MAB [3, 15], it was not clear how to extend that to contextual bandits problem, for which no regret bounds were available. There were considerable difficulties in extending the existing techniques to this case, some of which were also pointed out in [8]. We believe the techniques used in this paper will provide useful insights into the workings of this Bayesian algorithm, and may be useful for further improvements and extensions."}, {"heading": "2 Problem setting and algorithm description", "text": ""}, {"heading": "2.1 Problem setting", "text": "There are N arms. At time t = 1, 2, . . ., a context vector bi(t) \u2208 Rd, ||bi(t)|| \u2264 1, is revealed for every arm i. These context vectors are chosen by an adversary in an adaptive manner after observing the arms played and their rewards up to time t\u2212 1, i.e. history Ht\u22121,\nHt\u22121 = {i(w), ri(w)(w), bi(w), i = 1, . . . , N,w = 1, . . . , t\u2212 1}, where i(t) denotes the arm played at time t.\nGiven bi(t), reward for arm i at time t is generated from an (unknown) distribution with mean bi(t)\nT\u00b5i, where \u00b5i \u2208 Rd, ||\u00b5i|| \u2264 1 are fixed but unknown parameters. Also, given history Ht\u22121, and bi(t), i = 1, . . . , N , reward for arms i, i\n\u2032, i 6= i\u2032 are independent of each other. E [\nri(t) {bi(t)}Ni=1,Ht\u22121 ] = E [ri(t) bi(t)] = bi(t) T\u00b5i\nFurthermore, we assume that \u03b7i,t = ri(t) \u2212 bi(t)T\u00b5i is conditionally R-sub-Gaussian for constant R \u2265 0, i.e.,\n\u2200\u03bb \u2208 R,E[e\u03bb\u03b7i,t |{bi(t)}Ni=1,Ht\u22121] \u2264 exp ( \u03bb2R2 2 ) .\nThis assumption is satisfied if ri(t) \u2208 [bi(t)T\u00b5i \u2212 R, bi(t)T\u00b5i + R] (refer to Remark 1 in Appendix A.1 of [11]). Note that this assumption is weaker than assuming ri(t) is bounded.\nAn algorithm for the contextual bandit problem needs to choose, at every time t, an arm i(t) to play, using history Ht\u22121 and current contexts bi(t), i = 1, . . . , N . Let i\u2217(t) denote the optimal arm at time t, i.e. i\u2217(t) = argmaxi bi(t)T\u00b5i. Then the regret at time t,\nregret(t) = bi\u2217(t)(t) T\u00b5i\u2217(t) \u2212 bi(t)(t)T\u00b5i(t).\nThe objective is to minimize the total regret R(T ) = \u2211Tt=1 regret(t) in time T . The time horizon T is finite but possibly unknown.\nRemark 3. An alternative definition of regret that appears in literature is R(T ) = \u2211Tt=1 ri\u2217(t)(t)\u2212 ri(t)(t). When the reward ri(t) at all time steps is bounded as |ri(t)| \u2264 R for some constant R, for all i, then we can obtain the same results as in Theorem 1 and Theorem 2 for this definition of regret. The details are provided in Section 3.1."}, {"heading": "2.2 Thompson Sampling algorithm", "text": "Here, we describe the algorithm for the setting of single parameter \u00b5 = \u00b51 = \u00b7 \u00b7 \u00b7\u00b5N . (For the case of N different parameters, see Section 4.) Since there is a single underlying parameter, TS will maintain a common prior distribution over this parameter.\nWe use Gaussian likelihood function and Gaussian prior in our version of Thompson Sampling algorithm. More precisely, we assume that the likelihood of reward ri(t) at time t, given context bi(t) and parameter \u00b5, is given by the pdf of Gaussian distribution N (bi(t)T\u00b5, v2). Here, v = R \u221a\n6 \u01ebd ln( 1 \u03b4 ), with \u01eb \u2208 (0, 1) which parameterizes our algorithm. Let\nB(t) = Id + \u2211t\u22121 w=1 bi(w)(w)bi(w)(w) T , \u00b5\u0302(t) = B(t)\u22121\n(\n\u2211t\u22121 w=1 bi(w)(w)ri(w)(w)\n)\n.\nThen, assuming that the prior for \u00b5 at time t is given by N (\u00b5\u0302(t), v2B(t)\u22121), it easy to compute the posterior distribution Pr(\u00b5\u0303|ri(t)) \u221d Pr(ri(t)|bi(t)T \u00b5\u0303) Pr(\u00b5\u0303) as N (\u00b5\u0302(t+1), v2B(t+ 1)\u22121) (details of this computation are in Appendix A). Or, equivalently, the posterior distribution of the mean reward bi(t+1)\nT \u00b5\u0302(t+1) for arm i is N (bi(t+1)T \u00b5\u0302(t+1), v2bi(t+ 1)TB(t+1)\u22121bi(t+1)). In our Thompson Sampling algorithm, for each arm i, we will generate an independent sample \u03b8i(t) from the distribution N (bi(t)T \u00b5\u0302(t), v2bi(t)TB(t)\u22121bi(t)) at time t. The arm with maximum value of \u03b8i(t) will be played.\nAlgorithm 1: Thompson Sampling for Contextual bandits\nSet B = Id, \u00b5\u0302 = 0d, f = 0d. foreach t = 1, 2, . . . , do\nFor each arm i = 1, . . . , N , sample \u03b8i(t) independently from distribution N (bi(t)T \u00b5\u0302, v2bi(t)TB\u22121bi(t)). Play arm i(t) := argmaxi \u03b8i(t) and observe reward rt. Update B = B + bi(t)(t)bi(t)(t) T , f = f + bi(t)(t)rt, \u00b5\u0302 = B \u22121f .\nend\nRemark 4. Note that in the case of a single underlying parameter \u00b5, one could alternatively first generate a single \u00b5\u0303 from distribution N (\u00b5\u0302(t+1), v2B(t+1)\u22121), and then generate \u03b8i(t) as bi(t)T \u00b5\u0303. This alternative algorithm could be more efficient if N is large, but there is an efficient way to compute maxi bi(t)\nT \u00b5\u0303. While in this alternative algorithm, the marginal distribution of each \u03b8i(t) remains N (bi(t+1)T \u00b5\u0302(t+1), v2bi(t+1)TB(t+1)\u22121bi(t+1)), \u03b8i(t)s are not independent anymore. In our proof, we utilize the independence of \u03b8i(t)s (in Lemma 2), and it is not clear to us at this point whether the algorithm with dependent \u03b8is will have the same regret. For the case of N different parameters, this distinction is not important, as a separate \u00b5\u0303i has to be generated for every i."}, {"heading": "2.3 Challenges and solution outline", "text": "The contextual version of the multi-armed bandit problem presents new challenges for the analysis of TS algorithm, and the techniques used so far for analyzing the basic multi-armed bandit problem by [3, 15] do not seem directly applicable. Let us describe some of these difficulties and our novel solution ideas to resolve them.\nIn the basic MAB problem there are N arms, each with mean reward \u00b5i, and the regret for playing a suboptimal arm i is \u00b5i\u2217 \u2212\u00b5i, where i\u2217 is the arm with highest mean. Let us compare this to a 1-dimensional contextual MAB problem, where each arm i is associated with a parameter \u00b5i, but in addition, at every time t, it is associated with a context bi(t), so that mean reward is bi(t)\u00b5i, the best arm i\u2217(t) at time t is the arm with the highest mean at time t, and the regret for playing arm i is bi\u2217(t)(t)\u00b5i\u2217(t) \u2212 bi(t)\u00b5i.\nIn general, the basis of regret analysis for stochastic MAB is to prove that the variance of empirical estimates for all arms decreases fast enough, so that the regret incurred until the variance becomes small enough, is small. In the basic MAB, the variance of the empirical mean is inversely proportional to the number of plays ki(t) of arm i at time t. Thus, every time a suboptimal arm is played, we know that even though a regret of \u00b5i\u2217 \u2212 \u00b5i \u2264 1 in incurred, there is also an improvement of exactly 1 in the number of plays of that arm, and hence, corresponding decrease in the variance. The techniques for analyzing basic MAB rely on this observation to precisely quantify the exploration-exploitation tradeoff. On the other hand, the variance of empirical mean for contextual case is given by inverse of Bi(t) = \u2211t u=1 bu(t)\n2. When a suboptimal arm i is played, if bi(t) is small, the regret bi\u2217(t)(t)\u00b5i\u2217(t) \u2212 bi(t)\u00b5i could be much higher than the improvement bi(t) in Bi(t).\nIn our solution, we overcome this difficulty by bounding the expected regret at every step by a function of the probability of playing the optimal arm at that step. So, a high expected regret would mean large expected number of plays of optimal arm, in turn implying that regret is small. More precisely, we prove that, for \u201cmost histories\u201d Ft\u22121,\n1 ( \u221a 4 ln(NT ) v+\u2113(T )) E[regret(t)|Ft\u22121] . 1p Pr (i(t) = i\u2217(t) Ft\u22121) st,i\u2217(t) + st,i(t).\nThis inequality will form the basis for establishing our super-martingale process. Here filtration Ft\u22121 will be defined as the union of history Ht\u22121 and the contexts bi(t), i = 1, . . . , N at time t. And, p = 1\n2e \u221a \u03c0T \u01eb\n, \u2113(T ) = R \u221a d ln(T 3\n\u03b4 ) + 1, st,i = \u221a bi(t)TB(t)\u22121bi(t).\nThe main idea behind proving the above inequality is to divide the arms into two groups at any given time :\n\u2022 unsaturated arms defined as those with \u2206i(t) := bi\u2217(t)(t)T\u00b5 \u2212 bi(t)T\u00b5 \u2264 ( \u221a\n4 ln(NT ) v + \u2113(T ))st,i,\n\u2022 saturated arms defined as those with \u2206i(t) > ( \u221a 4 ln(NT ) v + \u2113(T ))st,i.\nNote that st,i gives the standard deviation of the estimate bi(t) T \u00b5\u0302(t) and of \u03b8i(t). Thus, intuitively saturated arms are arms with the property that the estimates of the means constructed so far in the direction of their contexts are good, making the deviations st,is small enough\u2014significantly smaller than their current \u2206i(t).\nIf an unsaturated arm is played at time t, then regret is at most \u2206i(t)(t) \u2264 ( \u221a\n4 ln(NT ) v + \u2113(T ))st,i(t). For saturated arms, the regret can be large, but on the other hand, since their deviation st,i is small, the concentration of \u03b8i(t) and bi(t) T \u00b5\u0302(t) will ensure that with reasonable probability, the algorithm is able to distinguish between them and the optimal arm. In particular, we prove\nthat the probability of playing a saturated arm is within p of the probability of playing the optimal arm. Further, using concentration bounds for \u03b8i(t) and \u00b5\u0302(t), the regret at any time can be bounded by ( \u221a\n4 ln(NT ) v + \u2113(T ))(st,i\u2217(t) + st,i(t)) to get the desired inequality. Then, using the Azuma-Hoeffding inequality for super-martingales, it will follow that with high\nprobability,\nR(T ) = \u2211Tt=1 regret(t) . ( \u221a\n4 ln(NT ) v + \u2113(T )) (\n1 p \u2211T t=1 I(i(t) = i \u2217(t))st,i\u2217(t) + \u2211 t st,i(t)\n)\n\u2264 ( \u221a 4 ln(NT ) v + \u2113(T )) (\n1 p\n\u2211 t st,i(t) + \u2211 t st,i(t)\n)\nThen, we will use the inequality \u2211 t st,i(t) = O( \u221a Td) (derived along the lines of [4]), to get the desired regret bound."}, {"heading": "3 Regret Analysis: Proof of Theorem 2", "text": "Definition 1. Define \u2113(T ) = R \u221a d ln(T 3 \u03b4 ) + 1, v = R \u221a 6 \u01ebd ln( 1 \u03b4 ). And for all i, define st,i = \u221a\nbi(t)TB(t)\u22121bi(t), \u2206i(t) = bi\u2217(t)(t) T\u00b5 \u2212 bi(t)T\u00b5. Also define filtration Ft as the union of history\nuntil time t, and the contexts at time t+ 1, i.e., Ft = {Ht, bi(t+ 1), i = 1, . . . , N}.\nDefinition 2. An arm i is called saturated at time t if \u2206i(t) > ( \u221a\n4 ln(NT ) v + \u2113(T ))st,i, and unsaturated otherwise. Let C(t) denote the set of saturated arms at time t. Note that the optimal arm at time t is always unsaturated at time t, i.e., i\u2217(t) /\u2208 C(t), and an arm may keep shifting from saturated to unsaturated and vice-versa over time.\nDefinition 3. Define E(t) and E\u0303(t) as the events that bi(t) T \u00b5\u0302(t) and \u03b8i(t) are concentrated around their respective means. More precisely, define E(t) as the event that \u2200i : |bi(t)T \u00b5\u0302(t)\u2212 bi(t)T\u00b5| \u2264 \u2113(T )st,i. Define E\u0303i(t) as the event that\n|\u03b8i(t)\u2212 bi(t)T \u00b5\u0302(t)| \u2264 \u221a 4 ln(NT ) vst,i,\nand E\u0303(t) as the event that \u2200i, E\u0303i(t) holds.\nLemma 1. For all t, 0 < \u03b4 < 1, Pr(E(t)) \u2265 1 \u2212 \u03b4 T 2 . And, for all possible filtrations Ft\u22121, \u2200i,Pr(E\u0303i(t)|Ft\u22121) \u2265 1\u2212 1NT 2 , and Pr(E\u0303(t)|Ft\u22121) \u2265 1\u2212 1T 2 .\nProof. The complete proof of this lemma appears in Appendix B.2. The probability bound for E(t) will be proven using the concentration inequality given by Theorem 1 in [1]). The probability bound for E\u0303i(t) will be proven using a concentration inequality for Gaussian random variables from [2] stated as Lemma 4 in Appendix B.1 .\nDefinition 4. Recall that regret(t) was defined as the regret at time t, regret(t) = bi\u2217(t)(t) T\u00b5 \u2212 bi(t)(t) T\u00b5. Define regret\u2032(t) = regret(t)\u2212 I(E(t)).\nNext, we establish a super-martingale process that will form the basis of proving our highprobability regret bound.\nDefinition 5. Let\nXt := 1 ( \u221a 4 ln(NT ) v+\u2113(T )) regret\u2032(t)\u2212 1pI(i(t) = i\u2217(t))st,i\u2217(t) \u2212 st,i(t) \u2212 5pT 2 ,\nand Yt := \u2211t w=1Xw, where p = 1 2e \u221a \u03c0T \u01eb .\nLemma 2. (Yt; t \u2265 0) is a super-martingale process with respect to filtration Ft.\nProof. We need to prove that for all t \u2208 [1, T ],\n1\n( \u221a 4 ln(NT ) v + \u2113(T )) E[regret\u2032(t)|Ft\u22121] \u2264 Pr (i(t) = i\u2217(t) Ft\u22121) p st,i\u2217(t) + E [ st,i(t) Ft\u22121 ] + 5 pT 2 .\nLet g(T ) = ( \u221a 4 ln(NT ) v + \u2113(T )). Note that whether E(t) is true or not is completely determined by Ft\u22121. If Ft\u22121 is such that E(t) does not hold, then regret\u2032(t) = regret(t)\u2212 I(E(t)) \u2264 0, and the lemma holds trivially. So, we will prove the above lemma while assuming we are given an Ft\u22121 such that E(t) holds.\nLet Es(t) denote the event that some (by definition suboptimal) saturated arm i in C(t) exceeds all the suboptimal unsaturated arms at time t, i.e., Es(t) : \u2203i \u2208 C(t), such that \u2200j /\u2208 C(t), j 6= i\u2217(t), \u03b8i(t) \u2265 \u03b8j(t). We prove the following lower bound on the probability of playing the optimal arm,\nPr (i(t) = i\u2217(t) Ft\u22121) \u2265 pPr ( Es(t) E\u0303(t),Ft\u22121 ) \u2212 2 T 2 .\nAnd we prove that\n1 g(T )E[regret \u2032(t)|Ft\u22121] \u2264 Pr ( Es(t) E\u0303(t),Ft\u22121 ) st,i\u2217(t) + E [ st,i(t) Ft\u22121 ] + 3 T 2 ,\nto get the desired inequality. For the lower bound on Pr (i(t) = i\u2217(t) Ft\u22121),\nPr (i(t) = i\u2217(t) Ft\u22121) \u2265 Pr(i(t) = i\u2217(t), Es(t), E\u0303(t)|Ft\u22121) = Pr(i(t) = i\u2217(t) Es(t), E\u0303(t),Ft\u22121) \u00b7 Pr(Es(t) E\u0303(t),Ft\u22121) \u00b7 Pr(E\u0303(t) Ft\u22121)\n(1)\nNow, given E\u0303(t), and Ft\u22121 such that E(t) is true, using the definition of saturated arms, it holds that for all i \u2208 C(t), \u03b8i(t) \u2264 bi(t)T\u00b5+ g(T )st,i \u2264 bi(t)T\u00b5+\u2206i(t) \u2264 bi\u2217(t)(t)T\u00b5, and given Es(t), it holds that there exists j \u2208 C(t) with maxi/\u2208C(t) \u03b8i(t) \u2264 \u03b8j(t) \u2264 bj(t)T\u00b5+\u2206j(t) \u2264 bi\u2217(t)(t)T\u00b5, so that for all arms i, \u03b8i(t) \u2264 bi\u2217(t)(t)T\u00b5. Therefore,\nPr(i(t) = i\u2217(t) Es(t), E\u0303(t),Ft\u22121) \u2265 Pr(\u03b8i\u2217(t)(t) \u2265 bi\u2217(t)(t)T\u00b5 Es(t), E\u0303(t),Ft\u22121) = Pr(\u03b8i\u2217(t)(t) \u2265 bi\u2217(t)(t)T\u00b5 E\u0303i\u2217(t)(t),Ft\u22121) \u2265 Pr(\u03b8i\u2217(t)(t) \u2265 bi\u2217(t)(t)T\u00b5 Ft\u22121)\u2212 1T 2\nThe equality in above holds because events Es(t) and E\u0303i(t),\u2200i 6= i\u2217(t) do not concern the optimal arm, and given Ft\u22121 (and hence i\u2217(t), bi\u2217(t)(t), \u00b5\u0302(t), and B(t)), \u03b8i\u2217(t)(t) is independent of these events. For the last inequality, we use that for any two events A,B, Pr(A) \u2264 Pr(A|B) + Pr(B).\nIn Lemma 3, we prove a lower bound of p on the probability of \u03b8i\u2217(t)(t) to exceed the optimal mean reward bi\u2217(t)(t) T\u00b5i\u2217(t) given Ft\u22121 such that E(t) holds. This will be proven using concentration provided by E(t) and anti-concentration of Gaussian random variable \u03b8i\u2217(t)(t). Using Lemma 3,\nPr(i(t) = i\u2217(t) Es(t), E\u0303(t),Ft\u22121) \u2265 p\u2212 1T 2 Substituting this along with Pr ( E\u0303(t) Ft\u22121 ) \u2265 1\u2212 1T 2 in Equation (1), we get\nPr (i(t) = i\u2217(t) Ft\u22121) \u2265 pPr ( Es(t) E\u0303i(t),Ft\u22121 ) \u2212 2 T 2 . (2)\nFor the regret upper bound, we observe that given E\u0303(t), and Ft\u22121 such that E(t) holds, if an arm i is played at time t, then \u2206i(t) \u2264 g(T )(st,i + st,i\u2217(t)). This holds because if an arm i is played at time t, then it must be true that \u03b8i(t) \u2265 \u03b8i\u2217(t)(t). And, given E\u0303(t) and E(t),\nbi(t) T\u00b5 \u2265 \u03b8i(t)\u2212 g(T )st,i\n\u2265 \u03b8i\u2217(t)(t)\u2212 g(T )st,i \u2265 bi\u2217(t)(t)T\u00b5\u2212 g(T )st,i\u2217(t) \u2212 g(T )st,i.\nAlso, by definition of unsaturated arms, for every unsaturated arm i, \u2206i(t) \u2264 g(T )st,i. Therefore, E [regret\u2032(t) Ft\u22121] \u2264 E [ \u2211 i\u2208C(t) \u2206i(t)I(i = i(t)) Ft\u22121 ] + E [ \u2211 i/\u2208C(t),i 6=i\u2217(t) g(T )st,iI(i = i(t)) Ft\u22121 ]\n(\u2217) \u2264 E [ \u2211 i\u2208C(t) \u2206i(t)I(i = i(t)) E\u0303(t),Ft\u22121 ] + 1 T 2 + g(T )E [ st,i(t)I(i(t) /\u2208 C(t)) Ft\u22121 ]\n\u2264 E [ ( g(T )st,i\u2217(t) + g(T )st,i(t) ) I(i(t) \u2208 C(t)) E\u0303(t),Ft\u22121 ] + 1 T 2\n+g(T )E [ st,i(t)I(i(t) /\u2208 C(t)) Ft\u22121 ]\n(\u2217\u2217) \u2264 g(T )st,i\u2217(t) \u00b7 E [ I(i(t) \u2208 C(t)) E\u0303(t),Ft\u22121 ] + 1 T 2 + g(T )E [ st,i(t) Ft\u22121 ] ( 1\u2212 1 T 2 )\u22121\n\u2264 ( g(T )st,i\u2217(t) ) Pr ( Es E\u0303(t),Ft\u22121 ) + 1 T 2 + g(T )E [ st,i(t) Ft\u22121 ] (1 + 2 T 2 )\n\u2264 ( g(T )st,i\u2217(t) ) Pr ( Es E\u0303(t),Ft\u22121 ) + 1 T 2 + g(T )E [ st,i(t) Ft\u22121 ] + 2g(T ) T 2\n(3)\nFor the inequality marked (\u2217), we use that for any random variable A \u2264 1, event B, and F , E[A|F ] \u2264 E[A|B,F ] + 1 \u2212 Pr(B|F ). We use this with A = \u2211i\u2208C(t) \u2206i(t)I(i = i(t)) \u2264 \u2211\ni\u2208C(t) ||bi\u2217(t)(t)|| \u00b7 ||\u00b5i\u2217(t)||I(i = i(t)) \u2264 1, B = E\u0303(t),F = Ft\u22121. For the inequality marked (\u2217\u2217), we use that for any events A,B,F , Pr(A|F ) \u2265 Pr(A|B,F ) Pr(B|F ). We use this with A = I(i(t) /\u2208 C(t)),B = E\u0303(t), F = Ft\u22121. For the last inequality, we use that st,i(t) \u2264 ||bi(t)(t)|| \u2264 1.\nThe next lemma lower bounds the probability that the sample \u03b8i\u2217(t)(t) of the optimal arm at time t will exceed its mean reward.\nLemma 3. For any filtration Ft\u22121 such that E(t) is true, Pr ( \u03b8i\u2217(t)(t) \u2265 bi\u2217(t)(t)T\u00b5 Ft\u22121 )\n\u2265 1 2e \u221a \u03c0T \u01eb .\nProof. Given event E(t), |bi\u2217(t)(t)T \u00b5\u0302(t) \u2212 bi\u2217(t)(t)T\u00b5| \u2264 \u2113(T )st,i\u2217(t). And, since Gaussian random variable \u03b8i\u2217(t)(t) has mean bi\u2217(t)(t) T \u00b5\u0302(t) and standard deviation vst,i\u2217(t), using anti-concentration inequality in Lemma 4, we will prove that with probability at least 1 2e \u221a \u03c0T \u01eb , \u03b8i\u2217(t)(t) will exceed bi\u2217(t)(t) T \u00b5\u0302(t) + ( ln T\u01eb ) vst,i\u2217(t). Then, the proof will follow from observing that bi\u2217(t)(t) T \u00b5\u0302(t) + (\nln T\u01eb ) vst,i\u2217(t) \u2265 bi\u2217(t)(t)T \u00b5\u0302(t) + \u2113(T )st,i\u2217(t) \u2265 bi\u2217(t)(t)T\u00b5. The details of the proof are in Appendix C.\nNow, we are ready to prove Theorem 2."}, {"heading": "3.1 Proof of Theorem 2", "text": "Note that for super-martingale Yt,\n|Yt \u2212 Yt\u22121| = |Xt| = | 1 ( \u221a 4 ln(NT ) v+\u2113(T )) regret\u2032(t)\u2212 1pI(i(t) = i\u2217(t))st,i\u2217(t) \u2212 st,i(t) \u2212 5pT 2 | \u2264 7p .\nThe last inequality holds because for any i, st,i = \u221a bi(t)TB\u22121(t)bi(t) \u2264 ||bi(t)||2 \u2264 1. Therefore, by Azuma-Hoeffding inequality,\nPr (\nYT \u2212 Y0 > 7p \u221a\nT ln(2/\u03b4) ) \u2264 exp ( \u2212 ln(2/\u03b4)T T ) \u2264 \u03b4/2.\nTherefore with probability 1\u2212 \u03b42 , \u2211T\nt=1\n(\n1 ( \u221a 4 ln(NT ) v+\u2113(T )) regret\u2032(t)\u2212 1pI(i(t) = i\u2217(t))st,i\u2217(t) \u2212 st,i(t) \u2212 5pT 2\n)\n\u2264 7p \u221a T ln(2/\u03b4).\nAlso, 1 p \u2211T t=1 I(i(t) = i \u2217(t))st,i\u2217(t) + \u2211T t=1 st,i(t) + 5 pT = 1 p \u2211 t:i(t)=i\u2217(t) st,i\u2217(t) + \u2211T t=1 st,i(t) + 5 pT\n\u2264 1p \u2211T t=1 st,i(t) + \u2211T t=1 st,i(t) + 5 pT = O( \u221a T \u01eb \u221a Td lnT )\nFor the last inequality, we use that \u2211T t=1 st,i(t) \u2264 5 \u221a dT lnT , which can be derived along the lines of Lemma 3 of [9] using Lemma 11 of [4]. Details are in Appendix B.3. Therefore, with probability 1\u2212 \u03b42 ,\n\u2211T t=1 regret \u2032(t) \u2264 ( \u221a\n4 ln(NT ) v + \u2113(T )) \u00b7 ( O( \u221a T \u01eb \u221a Td ln T ) + 7p \u221a T ln(2\u03b4 ) ) =\nO\n(\nd \u221a T 1+\u01eb\n\u01eb lnN lnT ln 1 \u03b4\n)\nAlso, because E(t) holds for all t with probability at least 1 \u2212 \u03b42 (refer to Lemma 1), regret\u2032(t) = regret(t) for all t with probability at least 1\u2212 \u03b42 . Hence, with probability 1\u2212 \u03b4,\nR(T ) = \u2211Tt=1 regret(t) = O ( d \u221a T 1+\u01eb \u01eb lnN lnT ln 1 \u03b4 ) .\nTo obtain bounds for the other definition of regret in Remark 3, observe that the expected regret for this definition is the same as before,\nE[regret(t)] = E[ri\u2217(t)(t)\u2212ri(t)(t)] = E[E[ri\u2217(t)(t)|i\u2217(t)]]\u2212E[E[ri(t)(t)|i(t)]] = E[bi\u2217(t)(t)T\u00b5\u2212bi(t)(t)T\u00b5].\nTherefore, Lemma 2 holds as it is, and Yt defined in Definition 5 is a super-martingale with respect to this new definition of regret(t) as well. Now, if |ri(t)| \u2264 R for all i, then |regret\u2032(t)| \u2264 R and |Yt \u2212 Yt\u22121| \u2264 7p +R, and we can apply Azuma-Hoeffding inequality exactly as in this subsection to obtain regret bounds of the same order as Theorem 2 for the new definition."}, {"heading": "4 Extensions", "text": ""}, {"heading": "4.1 N different parameters", "text": "Theorem 1 considers the setting where each arm i is associated with a parameter \u00b5i, where possibly \u00b5i 6= \u00b5i\u2032 for two different arms i and i\u2032. In this case, Thompson Sampling would maintain a separate estimate of mean \u00b5\u0302i(t), and Bi(t) for each arm i which would be updated only at the time instances when i is played. The statements of Lemma 1, and the super-martingale property established by Lemma 2 will hold as it is for the new definitions. The only difference will appear in the bound for \u2211\nt st,i(t) used in the proof of Theorem 2. For the case of N different parameters, we will get a\nbound of O( \u221a NTd lnT ) on this quantity instead of O( \u221a Td lnT ), leading to the extra \u221a N factor in the bound in Theorem 1 compared to Theorem 2. The details of the algorithm for the case of N different parameters, and the changes in the analysis required for proving Theorem 1 are provided in Appendix D."}, {"heading": "4.2 General distributions", "text": "In the algorithm in this paper, \u03b8i(t) is generated from a Gaussian distribution. However, the analysis techniques in this paper are easily extendable to an algorithm that uses a posterior distribution other than the Gaussian distribution. The only distribution specific properties we have used in the analysis are the concentration and anti-concentration inequalities for Gaussian distributed random variables mentioned in Lemma 4. The concentration inequality was used to prove that E\u0303(t) happens with high probability in Lemma 1, and the anti-concentration inequality was used to lower bound the probability that Gaussian distributed random variable \u03b8i\u2217(t)(t) exceeds its mean by some factors of its standard deviation in Lemma 3. If any other distribution provides similar tail inequalities, these inequalities can be used as a black box in the analysis, and the regret bounds can be reproduced for that distribution."}, {"heading": "5 Conclusions", "text": "We provided a theoretical analysis of Thompson Sampling for the contextual bandits problem with linear payoffs. Our results resolve many open questions regarding the theoretical guarantees for Thompson Sampling, and establish that even for the contextual version of the stochastic MAB problem, TS achieves regret bounds comparable to the state-of-the-art methods. We used novel martingale-based analysis techniques which are simpler than those in the past work on TS [3, 15], and amenable to extensions. In fact, the techniques introduced in this paper could also be used to provide a simpler proof for the optimal expected regret bounds for TS for the basic MAB problem studied in [3, 15]. The proof of this claim will appear elsewhere.\nSeveral questions remain open. A tighter analysis that can remove the dependence on \u01eb is desirable. We believe that our techniques would adapt to provide such bounds for the expected regret. Other avenues to explore are contextual bandits with generalized linear models considered in [11], the setting with delayed and batched feedbacks, and the agnostic case of contextual bandits with linear payoffs. The agnostic case refers to the setting which does not make the realizability assumption that there exists a vector \u00b5i for each i for which E[ri(t)|bi(t)] = bi(t)T\u00b5i. To our knowledge, no existing algorithm has been shown to have non-trivial regret bounds for the agnostic case."}, {"heading": "A Posterior distribution computation", "text": "Pr(\u00b5\u0303|ri(t)) \u221d Pr(ri(t)|bi(t)T \u00b5\u0303) Pr(\u00b5\u0303) \u221d exp{\u2212 1\n2v2 ((ri(t)\u2212 \u00b5\u0303T bi(t))2 + (\u00b5\u0303\u2212 \u00b5\u0302(t))TB(t)(\u00b5\u0303\u2212 \u00b5\u0302(t))}\n\u221d exp{\u2212 1 2v2 (ri(t) 2 + \u00b5\u0303T bi(t)bi(t) T \u00b5\u0303+ \u00b5\u0303TB(t)\u00b5\u0303\u2212 2\u00b5\u0303T bi(t)ri(t)\u2212 2\u00b5\u0303TB(t)\u00b5\u0302(t))} \u221d exp{\u2212 1 2v2 (\u00b5\u0303TB(t+ 1)\u00b5\u0303 \u2212 2\u00b5\u0303TB(t+ 1)\u00b5\u0302(t+ 1))} \u221d exp{\u2212 1 2v2 (\u00b5\u0303\u2212 \u00b5\u0302(t+ 1))TB(t+ 1)(\u00b5\u0303 \u2212 \u00b5\u0302(t+ 1))} \u221d N (\u00b5\u0302(t+ 1), v2B(t+ 1)\u22121)\nTherefore, the posterior distribution of \u00b5 at time t+ 1 is N (\u00b5\u0302(t+ 1), v2B(t+ 1)\u22121),"}, {"heading": "B Proof of Theorem 2", "text": "B.1 Gaussian concentration\nFormula 7.1.13 from [2] can be used to derive the following concentration and anti-concentration inequalities for Gaussian distributed random variables.\nLemma 4. [2] For a Gaussian distributed random variable Z with mean m and variance \u03c32, for any z \u2265 1,\n1\n2 \u221a \u03c0z\ne\u2212z 2/2 \u2264 Pr(|Z \u2212m| > z\u03c3) \u2264 1\u221a\n\u03c0z e\u2212z 2/2.\nB.2 Proof of Lemma 1\nWe will use the following lemma (implied by Theorem 1 in [1]):\nLemma 5. [1] Let (F \u2032t ; t \u2265 0) be a filtration, (mt; t \u2265 1) be an Rd-valued stochastic process such that mt is (F \u2032t\u22121) measurable, (\u03b7t; t \u2265 1) be a real-valued martingale difference process such that \u03b7t is (F \u2032t) measurable and For t \u2265 0, define \u03bet = \u2211t u=1mu\u03b7u and Mt = Id + \u2211t u=1 mum T u , where Id is the d-dimensional identity matrix. Assume \u03b7t is conditionally R-sub-Gaussian. Then, for any \u03b4\u2032 > 0, t \u2265 0, with probability at least 1\u2212 \u03b4\u2032,\n||\u03bet||M\u22121t \u2264 R \u221a d ln ( t+ 1 \u03b4\u2032 ) .\nWe use the above lemma with mt = bi(t)(t), \u03b7t = ri(t) \u2212 bi(t)(t)T\u00b5, F \u2032t = (mu+1, \u03b7u : u \u2264 t). (Note that effectively, F \u2032t can be imagined to have all the information including the arms played until time t+ 1, except for the reward of the arm played at time t+ 1). By definition of F \u2032t , mt is F \u2032t\u22121 measurable, and \u03b7t is F \u2032t measurable. And, \u03b7t is a martingale difference process:\nE [ \u03b7t|F \u2032t\u22121 ] = E[ri(t)|bi(t)(t), i(t)] \u2212 bi(t)(t)T\u00b5 = 0. Also, this makes\nMt = Id + t \u2211\nu=1\nmum T u = Id +\nt \u2211\nu=1\nbi(u)(u)bi(u)(u) T ,\n\u03bet =\nt \u2211\nu=1\nmu\u03b7u =\nt \u2211\nu=1\nbi(u)(u)(ri(u) \u2212 bi(u)(u)T\u00b5).\nNote that B(t) = Mt\u22121, and \u00b5\u0302(t)\u2212 \u00b5 = M\u22121t\u22121(\u03bet\u22121 \u2212 \u00b5), so that\n|bi(t)T \u00b5\u0302(t)\u2212bi(t)T\u00b5| = |bi(t)TM\u22121t\u22121(\u03bet\u22121\u2212\u00b5)| \u2264 ||bi(t)||M\u22121t\u22121 ||\u03bet\u22121\u2212\u00b5||M\u22121t\u22121 = ||bi(t)||B(t)\u22121 ||\u03bet\u22121\u2212\u00b5||M\u22121t\u22121 . The inequality holds because M\u22121t\u22121 is a positive definite matrix. Using the above lemma, for any \u03b4\u2032 > 0, t \u2265 1, with probability at least 1\u2212 \u03b4\u2032,\n||\u03bet\u22121||M\u22121t\u22121 \u2264 R \u221a d ln ( t \u03b4\u2032 ) .\nTherefore, ||\u03bet\u22121 \u2212 \u00b5||M\u22121t\u22121 \u2264 R \u221a d ln ( t \u03b4\u2032 ) + ||\u00b5||M\u22121t\u22121 \u2264 R \u221a d ln ( t \u03b4\u2032 ) + 1. Substituting \u03b4\u2032 = \u03b4 T 2 , we get that with probability 1\u2212 \u03b4 T 2 , for all i,\n|bi(t)T \u00b5\u0302(t)\u2212 bi(t)T\u00b5| \u2264 st,i \u00b7 ( R \u221a d ln( T 3\n\u03b4 ) + 1\n)\n\u2264 \u2113(T )st,i\nThis proves the bound on the probability of E(t). To prove bound on probability of E\u0303i(t), we use the fact that since \u03b8i(t) is distributed as N (bi(t)T \u00b5\u0302(t), v2bi(t)TB(t)\u22121bi(t)), therefore, using concentration inequalities for Gaussian random variables,\nPr(|\u03b8i(t)\u2212 bi(t)T \u00b5\u0302(t)| > z v \u221a bi(t)TB(t)\u22121bi(t)|Ft\u22121) \u2264 1\u221a \u03c0z e\u2212z 2/2\nSubstituting z = \u221a 4 ln(NT ) , we get the desired bound.\nB.3 Bound on the sum of st,i(t)\nWe will use the following result, implied by the referred lemma in [4]\nLemma 6. [[4], Lemma 11]. Let A\u2032 = A+xxT , where x \u2208 Rd, A,A\u2032 \u2208 Rd\u00d7d, and all the eigenvalues \u03bbj , j = 1, . . . , d of A are greater than or equal to 1. Then, the eigenvalues \u03bb \u2032 j, j = 1, . . . , d of A\n\u2032 can be arranged so that \u03bbj \u2264 \u03bb\u2032j for all j, and\nxTA\u22121x \u2264 10 d \u2211\nj=1\n\u03bb\u2032j \u2212 \u03bbj \u03bbj\nLet \u03bbj,t denote the eigenvalues of B(t). Note that B(t + 1) = B(t) + bi(t)(t)bi(t)(t) T , and\n\u03bbj,t \u2265 1,\u2200j. Therefore, above implies\ns2t,i(t) \u2264 10 d \u2211\nj=1\n\u03bbj,t+1 \u2212 \u03bbj,t \u03bbj,t .\nThis allows us to derive the following along the lines of Lemma 3 of [9].\nT \u2211\nt=1\nst,i(t) \u2264 5 \u221a dT lnT ."}, {"heading": "C Proof of Lemma 3", "text": "Given event E(t), |bi\u2217(t)(t)T \u00b5\u0302(t) \u2212 bi\u2217(t)(t)T\u00b5| \u2264 \u2113(T )st,i\u2217(t). And, since Gaussian random variable \u03b8i\u2217(t)(t) has mean bi\u2217(t)(t)\nT \u00b5\u0302(t) and standard deviation vst,i\u2217(t), using anti-concentration inequality in Lemma 4,\nPr ( \u03b8i\u2217(t)(t) \u2265 bi\u2217(t)(t)T\u00b5 Ft\u22121 ) = Pr\n(\n\u03b8i\u2217(t)(t)\u2212 bi\u2217(t)(t)T \u00b5\u0302(t) vst,i\u2217(t) \u2265 bi \u2217(t)(t) T\u00b5\u2212 bi\u2217(t)(t)T \u00b5\u0302(t) vst,i\u2217(t) Ft\u22121 )\n\u2265 1 2 \u221a \u03c0 e\u2212Z 2 t\nwhere\n|Zt| = \u2223 \u2223 \u2223 \u2223\n\u2223 bi\u2217(t)(t) T\u00b5\u2212 bi\u2217(t)(t)T \u00b5\u0302(t) vst,i\u2217(t)\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 \u2113(T )st,i\u2217(t)\nvst,i\u2217(t)\n\u2264 R \u221a d ln(T 3 \u03b4 ) + 1\nR \u221a\n6 \u01ebd ln( 1 \u03b4 )\n\u2264 \u221a \u01eb\n2 (lnT + 1)\nPr ( \u03b8i\u2217(t)(t) \u2265 bi\u2217(t)(t)T\u00b5 Ft\u22121 ) \u2265 1 2 \u221a \u03c0 e\u2212 \u01eb 2 (lnT+1) =\n1\n2e \u221a \u03c0T \u01eb 2"}, {"heading": "D N different parameters: Proof of Theorem 1", "text": "Theorem 1 considers the setting where each arm i is associated with a parameter \u00b5i, where possibly \u00b5i 6= \u00b5i\u2032 for two different arms i and i\u2032. In this case, Thompson Sampling would maintain a separate estimate of mean \u00b5\u0302i(t), and Bi(t) for each arm i which would be updated only at the time instances when i is played.\nBi(t) = Id + t\u22121 \u2211\nu=1:i(u)=i\nbi(u)bi(u) T\n\u00b5\u0302i(t) = Bi(t) \u22121\n\n\nt\u22121 \u2211\nu=1:i(u)=i\nbi(u)ri(u)\n\n\nst,i = \u221a bi(t)TBi(t)\u22121bi(t)\nThe posterior distribution for each arm i at time t would beN (bi(t)T \u00b5\u0302i(t), v2 bi(t)TBi(t)\u22121bi(t)). Algorithm 2: Thompson Sampling for Contextual bandits with N parameters\nSet Bi = Id, \u00b5\u0302i = 0d, i = 1, . . . , N , fi = 0d. foreach t = 1, 2, . . . , do\nFor each arm i = 1, . . . , N , sample \u03b8i(t) independently from distribution N (bi(t)T \u00b5\u0302i, v2 bi(t)TB\u22121i bi(t)). Play arm i(t) := argmaxi \u03b8i(t) and observe reward rt. Update Bi(t) = Bi(t) + bi(t)(t)bi(t)(t) T , fi(t) = fi(t) + bi(t)(t)rt, \u00b5\u0302i(t) = B \u22121 i(t)fi(t).\nend\nIn the regret analysis, the events E(t) will now be defined with respect to concentration of all \u00b5\u0302i(t) around their respective means. That is,\nE(t) : \u2200i, bi(t)T \u00b5\u0302i(t) \u2208 [bi(t)T\u00b5i \u2212 \u2113(T )st,i, bi(t)T\u00b5i + \u2113(T )st,i]\nSimilarly, E\u0303i(t) will be the event that\n\u03b8i(t) \u2208 [bi(t)T \u00b5\u0302i(t)\u2212 \u221a 4 ln(NT ) vst,i, bi(t) T \u00b5\u0302i(t) + \u221a 4 ln(NT ) vst,i],\nand E\u0303(t) will be the event that \u2200i, E\u0303i(t) holds. It is easy to observe that the statements of Lemma 1 and the super-martingale property established by Lemma 2 will hold as it is for these new definitions. The only difference will appear in the bound for \u2211\nt st,i(t) used in the proof of Theorem\n2. For the case of N different parameters, we will get a bound of O( \u221a NTd ln T ) on this quantity.\nLet ni(T ) be the number of times arm i is played by time T . Then using Lemma 6, for two consequent time steps t, t\u2032 at which arm i is played\ns2t,i(t) \u2264 10 d \u2211\nj=1\n\u03bbj,t\u2032 \u2212 \u03bbj,t \u03bbj,t .\nThis allows us to derive the following lemma along the lines of Lemma 3 of [9].\nLemma 7. [[9], Lemma 3] For i = 1, . . . , N ,\nT \u2211\nt=1:i(t)=i\nst,i(t) \u2264 5 \u221a dni(T ) ln(ni(T )).\nUsing above lemma,\nT \u2211\nt=1\nst,i(t) =\nN \u2211\ni=1\nT \u2211\nt=1:i(t)=i\nst,i(t) \u2264 N \u2211\ni=1\n5 \u221a ni(T )d lnT \u2264 5 \u221a N \u221a \u2211\ni\nni(T ) \u221a d lnT = 5 \u221a NTd ln T .\nTherefore, following the same lines as proof of Theorem 2, we will get a regret bound ofO(d \u221a T \u01eb\n\u01eb \u221a NT lnN lnT ln 1\u03b4 )."}], "references": [{"title": "Improved Algorithms for Linear Stochastic Bandits", "author": ["Yasin Abbasi-Yadkori", "D\u00e1vid P\u00e1l", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables", "author": ["Milton Abramowitz", "Irene A. Stegun"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1964}, {"title": "Analysis of Thompson Sampling for the Multi-armed Bandit Problem", "author": ["Shipra Agrawal", "Navin Goyal"], "venue": "In COLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Using Confidence Bounds for Exploitation-Exploration Trade-offs", "author": ["Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "The Nonstochastic Multiarmed Bandit Problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi", "Sham M. Kakade"], "venue": "Proceedings of the 25th Conference on Learning Theory (COLT),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "An Empirical Evaluation of Thompson Sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Open Problem: Regret Bounds for Thompson Sampling", "author": ["Olivier Chapelle", "Lihong Li"], "venue": "In COLT,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Contextual Bandits with Linear Payoff Functions", "author": ["Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E. Schapire"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Stochastic Linear Optimization under Bandit Feedback", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Parametric Bandits: The Generalized Linear Case", "author": ["Sarah Filippi", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Web-Scale Bayesian Click-Through rate Prediction for Sponsored Search Advertising in Microsoft\u2019s Bing Search Engine", "author": ["Thore Graepel", "Joaquin Qui\u00f1onero Candela", "Thomas Borchert", "Ralf Herbrich"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Solving Two-Armed Bernoulli Bandit Problems Using a Bayesian Learning Automaton", "author": ["O.-C. Granmo"], "venue": "International Journal of Intelligent Computing and Cybernetics (IJICC),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Associative Reinforcement Learning: Functions in k-DNF", "author": ["Leslie Pack Kaelbling"], "venue": "Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1985}, {"title": "The Epoch-Greedy Algorithm for Multi-armed Bandits with Side Information", "author": ["John Langford", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Linearly Parametrized Bandits", "author": ["Pedro A. Ortega", "Daniel A. Braun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "One-armed badit problem with covariates", "author": ["Jyotirmoy Sarkar"], "venue": "The Annals of Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1991}, {"title": "A modern Bayesian look at the multi-armed bandit", "author": ["S. Scott"], "venue": "Applied Stochastic Models in Business and Industry,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Experienceefficient learning in associative bandit problems", "author": ["Alexander L. Strehl", "Chris Mesterharm", "Michael L. Littman", "Haym Hirsh"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "A Bayesian Framework for Reinforcement Learning", "author": ["Malcolm J.A. Strens"], "venue": "In ICML, pages 943\u2013950,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["William R. Thompson"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1933}, {"title": "A one-armed bandit problem with a concomitant variable", "author": ["Michael Woodroofe"], "venue": "Journal of the American Statistics Association,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1979}], "referenceMentions": [{"referenceID": 3, "context": "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].", "startOffset": 103, "endOffset": 116}, {"referenceID": 10, "context": "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].", "startOffset": 103, "endOffset": 116}, {"referenceID": 8, "context": "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].", "startOffset": 103, "endOffset": 116}, {"referenceID": 0, "context": "This realizability assumption is standard in the existing literature on contextual multi-armed bandits [4, 11, 9, 1].", "startOffset": 103, "endOffset": 116}, {"referenceID": 21, "context": "The first version of this Bayesian heuristic is around 80 years old, dating to Thompson (1933) [25].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": ", in [27, 20, 24].", "startOffset": 5, "endOffset": 17}, {"referenceID": 20, "context": ", in [27, 20, 24].", "startOffset": 5, "endOffset": 17}, {"referenceID": 12, "context": ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.", "startOffset": 2, "endOffset": 25}, {"referenceID": 18, "context": ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.", "startOffset": 2, "endOffset": 25}, {"referenceID": 11, "context": ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.", "startOffset": 2, "endOffset": 25}, {"referenceID": 6, "context": ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.", "startOffset": 2, "endOffset": 25}, {"referenceID": 18, "context": ", [13, 22, 12, 7, 19, 15]) have empirically demonstrated the efficacy of TS: Scott [22] provides a detailed discussion of probability matching techniques in many general settings along with favorable empirical comparisons with other techniques.", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "Chapelle and Li [7] demonstrate that for the basic stochastic MAB problem, empirically TS achieves regret comparable to the lower bound of [16]; and in applications like display advertising and news article recommendation modeled by the contextual bandits problem, it is competitive to or better than the other methods such as UCB.", "startOffset": 16, "endOffset": 19}, {"referenceID": 14, "context": "Chapelle and Li [7] demonstrate that for the basic stochastic MAB problem, empirically TS achieves regret comparable to the lower bound of [16]; and in applications like display advertising and news article recommendation modeled by the contextual bandits problem, it is competitive to or better than the other methods such as UCB.", "startOffset": 139, "endOffset": 143}, {"referenceID": 11, "context": "TS has also been used in an industrial scale application for CTR prediction of search ads on search engines [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "[13, 18] provided weak guarantees, namely, a bound of o(T ) on expected regret in time T .", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "More recently, some significant progress was made by [3, 15], who provided near-optimal problem-dependent bounds on the expected regret of TS for the basic (i.", "startOffset": 53, "endOffset": 60}, {"referenceID": 7, "context": "Some of these questions were formally raised as a COLT 2012 open problem [8].", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "This essentially solves the COLT 2012 open problem [8] for linear contextual bandits.", "startOffset": 51, "endOffset": 54}, {"referenceID": 2, "context": "The contextual MAB problem does not seem easily amenable to the techniques used so far for analyzing the basic MAB problem by [3, 15].", "startOffset": 126, "endOffset": 133}, {"referenceID": 8, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 37, "endOffset": 45}, {"referenceID": 17, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 37, "endOffset": 45}, {"referenceID": 13, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 116, "endOffset": 123}, {"referenceID": 19, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 116, "endOffset": 123}, {"referenceID": 4, "context": "[9]: bandit problems with covariates [26, 21], associative reinforcement learning [14], associative bandit problems [4, 23], and bandit problems with expert advice [5].", "startOffset": 164, "endOffset": 167}, {"referenceID": 15, "context": "The name contextual bandits was coined in Langford and Zhang [17].", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "[9] show that for any algorithm the regret is \u03a9( \u221a Td) for d2 \u2264 T for the N -armed contextual bandits problem with linear payoffs and single parameter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Auer [4] and Chu et al.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "[9] SupLinUCB, a complicated algorithm using UCB as a subroutine, for this problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Td ln(NT ln(T )/\u03b4)) with probability at least 1\u2212 \u03b4 (Auer [4] proves similar results).", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "By contrast, in the analysis of SupLinUCB, [4, 9] consider only oblivious adversary, and achieve statistical independence of samples by using a complicated master procedure SupLin on top of the basic UCB style algorithm.", "startOffset": 43, "endOffset": 49}, {"referenceID": 8, "context": "By contrast, in the analysis of SupLinUCB, [4, 9] consider only oblivious adversary, and achieve statistical independence of samples by using a complicated master procedure SupLin on top of the basic UCB style algorithm.", "startOffset": 43, "endOffset": 49}, {"referenceID": 9, "context": "[10, 1].", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "[10, 1].", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "[1] analyze a UCB-style algorithm for that problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] as \u03a9(d \u221a T ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "The state-of-the-art bounds for linear bandits problem in case of finite N are given by [6].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "Our bounds are essentially within a factor of \u221a d lnT of the best bounds for finite N (those for UCB1 by [9, 4], and for Exp2 algorithm by [6]), and within \u221a lnN factor of the best bounds that do not depend on N (by Abbasi-Yadkori et al.", "startOffset": 105, "endOffset": 111}, {"referenceID": 3, "context": "Our bounds are essentially within a factor of \u221a d lnT of the best bounds for finite N (those for UCB1 by [9, 4], and for Exp2 algorithm by [6]), and within \u221a lnN factor of the best bounds that do not depend on N (by Abbasi-Yadkori et al.", "startOffset": 105, "endOffset": 111}, {"referenceID": 5, "context": "Our bounds are essentially within a factor of \u221a d lnT of the best bounds for finite N (those for UCB1 by [9, 4], and for Exp2 algorithm by [6]), and within \u221a lnN factor of the best bounds that do not depend on N (by Abbasi-Yadkori et al.", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "While significant recent progress was made in analyzing it for basic MAB [3, 15], it was not clear how to extend that to contextual bandits problem, for which no regret bounds were available.", "startOffset": 73, "endOffset": 80}, {"referenceID": 7, "context": "There were considerable difficulties in extending the existing techniques to this case, some of which were also pointed out in [8].", "startOffset": 127, "endOffset": 130}, {"referenceID": 10, "context": "1 of [11]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "3 Challenges and solution outline The contextual version of the multi-armed bandit problem presents new challenges for the analysis of TS algorithm, and the techniques used so far for analyzing the basic multi-armed bandit problem by [3, 15] do not seem directly applicable.", "startOffset": 234, "endOffset": 241}, {"referenceID": 3, "context": "t st,i(t) = O( \u221a Td) (derived along the lines of [4]), to get the desired regret bound.", "startOffset": 49, "endOffset": 52}, {"referenceID": 0, "context": "The probability bound for E(t) will be proven using the concentration inequality given by Theorem 1 in [1]).", "startOffset": 103, "endOffset": 106}, {"referenceID": 1, "context": "The probability bound for \u1ebci(t) will be proven using a concentration inequality for Gaussian random variables from [2] stated as Lemma 4 in Appendix B.", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "Also, 1 p \u2211T t=1 I(i(t) = i (t))st,i\u2217(t) + \u2211T t=1 st,i(t) + 5 pT = 1 p \u2211 t:i(t)=i(t) st,i\u2217(t) + \u2211T t=1 st,i(t) + 5 pT \u2264 1 p \u2211T t=1 st,i(t) + \u2211T t=1 st,i(t) + 5 pT = O( \u221a T \u01eb \u221a Td lnT ) For the last inequality, we use that \u2211T t=1 st,i(t) \u2264 5 \u221a dT lnT , which can be derived along the lines of Lemma 3 of [9] using Lemma 11 of [4].", "startOffset": 303, "endOffset": 306}, {"referenceID": 3, "context": "Also, 1 p \u2211T t=1 I(i(t) = i (t))st,i\u2217(t) + \u2211T t=1 st,i(t) + 5 pT = 1 p \u2211 t:i(t)=i(t) st,i\u2217(t) + \u2211T t=1 st,i(t) + 5 pT \u2264 1 p \u2211T t=1 st,i(t) + \u2211T t=1 st,i(t) + 5 pT = O( \u221a T \u01eb \u221a Td lnT ) For the last inequality, we use that \u2211T t=1 st,i(t) \u2264 5 \u221a dT lnT , which can be derived along the lines of Lemma 3 of [9] using Lemma 11 of [4].", "startOffset": 325, "endOffset": 328}, {"referenceID": 2, "context": "We used novel martingale-based analysis techniques which are simpler than those in the past work on TS [3, 15], and amenable to extensions.", "startOffset": 103, "endOffset": 110}, {"referenceID": 2, "context": "In fact, the techniques introduced in this paper could also be used to provide a simpler proof for the optimal expected regret bounds for TS for the basic MAB problem studied in [3, 15].", "startOffset": 178, "endOffset": 185}, {"referenceID": 10, "context": "Other avenues to explore are contextual bandits with generalized linear models considered in [11], the setting with delayed and batched feedbacks, and the agnostic case of contextual bandits with linear payoffs.", "startOffset": 93, "endOffset": 97}], "year": 2016, "abstractText": "Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to the state of the art methods. However, many questions regarding its theoretical performance remained open. In this paper, we design and analyze Thompson Sampling algorithm for the contextual multi-armed bandit problem with linear payoff functions, when the contexts are provided by an adaptive adversary. This is perhaps the most important and widely studied version of the contextual bandits problem. We prove a high probability regret bound of \u00d5( 1 \u221a \u01eb \u221a T d) in time T for any 0 < \u01eb < 1, where d is the dimension of each context vector and \u01eb is a parameter used by the algorithm. Our results provide the first theoretical guarantees for the contextual version of Thompson Sampling, and are close to the lower bound of \u03a9( \u221a Td) for this problem. This essentially solves the COLT open problem of Chapelle and Li [COLT 2012] regarding regret bounds for Thompson Sampling for contextual bandits problem. Our version of Thompson sampling uses Gaussian prior and Gaussian likelihood function. Our novel martingale-based analysis techniques also allow easy extensions to the use of more general distributions, satisfying certain general conditions.", "creator": "LaTeX with hyperref package"}}}