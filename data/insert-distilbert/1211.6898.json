{"id": "1211.6898", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2012", "title": "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes", "abstract": "we consider developing infinite - horizon stationary $ \\ gamma $ - discounted markov decision processes, for assuming which it is known that there exists exists a stationary optimal policy. using value and policy iteration with some error $ \\ epsilon $ at after each iteration, it is well - known that one can compute on stationary policies that are $ \\ frac { when 2 \\ gamma } { ( 1 - \\ gamma ) ^ 2 } \\ epsilon $ - optimal. after originally arguing that this optimal guarantee is tight, we develop multiple variations of value and policy iteration for computing non - stationary policies that can be up to $ \\ frac { 2 \\ gamma } { 1 - \\ gamma } \\ epsilon $ - optimal, which constitutes a significant improvement in the usual situation when $ \\ gamma $ is close to 1. surprisingly, this shows that the problem of \" computing near - optimal non - stationary policies \" is substantially much simpler than that of \" computing near - optimal stationary policies \".", "histories": [["v1", "Thu, 29 Nov 2012 12:54:58 GMT  (16kb)", "http://arxiv.org/abs/1211.6898v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["bruno scherrer", "boris lesner"], "accepted": true, "id": "1211.6898"}, "pdf": {"name": "1211.6898.pdf", "metadata": {"source": "CRF", "title": "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes", "authors": ["Bruno Scherrer", "Boris Lesner"], "emails": ["bruno.scherrer@inria.fr", "boris.lesner@inria.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n21 1.\n68 98\nv1 [\ncs .L\nG ]\n2 9\nN ov"}, {"heading": "1 Introduction", "text": "Given an infinite-horizon stationary \u03b3-discounted Markov Decision Process [24, 4], we consider approximate versions of the standard Dynamic Programming algorithms, Policy and Value Iteration, that build sequences of value functions vk and policies \u03c0k as follows\nApproximate Value Iteration (AVI): vk+1 \u2190 Tvk + \u01ebk+1 (1)\nApproximate Policy Iteration (API):\n{\nvk \u2190 v\u03c0k + \u01ebk \u03c0k+1 \u2190 any element of G(vk)\n(2)\nwhere v0 and \u03c00 are arbitrary, T is the Bellman optimality operator, v\u03c0k is the value of policy \u03c0k and G(vk) is the set of policies that are greedy with respect to vk. At each iteration k, the term \u01ebk accounts for a possible approximation of the Bellman operator (for AVI) or the evaluation of v\u03c0k (for API). Throughout the paper, we will assume that error terms \u01ebk satisfy for all k, \u2016\u01ebk\u2016\u221e \u2264 \u01eb for some \u01eb \u2265 0. Under this assumption, it is well-known that both algorithms share the following performance bound (see [25, 11, 4] for AVI and [4] for API):\nTheorem 1. For API (resp. AVI), the loss due to running policy \u03c0k (resp. any policy \u03c0k in G(vk\u22121)) instead of the optimal policy \u03c0\u2217 satisfies\nlim sup k\u2192\u221e\n\u2016v\u2217 \u2212 v\u03c0k\u2016\u221e \u2264 2\u03b3\n(1\u2212 \u03b3)2 \u01eb.\nThe constant 2\u03b3(1\u2212\u03b3)2 can be very big, in particular when \u03b3 is close to 1, and consequently the above bound is commonly believed to be conservative for practical applications. Interestingly, this very constant 2\u03b3(1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved. Indeed, the bound (and the 2\u03b3(1\u2212\u03b3)2 constant) are tight for API [4, Example 6.4], and we will show in Section 3 \u2013 to our knowledge, this has never been argued in the literature \u2013 that it is also tight for AVI.\nEven though the theory of optimal control states that there exists a stationary policy that is optimal, the main contribution of our paper is to show that looking for a non-stationary policy (instead of a stationary one) may lead to a much better performance bound. In Section 4, we will show how to deduce such a non-stationary policy from a run of AVI. In Section 5, we will describe two original policy iteration variations that compute non-stationary policies. For all these algorithms, we will prove that we have a performance bound that can be reduced down to 2\u03b31\u2212\u03b3 \u01eb. This is a factor 1 1\u2212\u03b3 better than the standard bound of Theorem 1, which is significant when \u03b3 is close to 1. Surprisingly, this will show that the problem of \u201ccomputing near-optimal non-stationary policies\u201d is much simpler than that of \u201ccomputing near-optimal stationary policies\u201d. Before we present these contributions, the next section begins by precisely describing our setting."}, {"heading": "2 Background", "text": "We consider an infinite-horizon discounted Markov Decision Process [24, 4] (S,A, P, r, \u03b3), whereS is a possibly infinite state space, A is a finite action space, P (ds\u2032|s, a), for all (s, a), is a probability kernel on S, r : S \u00d7 A \u2192 R is a reward function bounded in max-norm by Rmax, and \u03b3 \u2208 (0, 1) is a discount factor. A stationary deterministic policy \u03c0 : S \u2192 A maps states to actions. We write r\u03c0(s) = r(s, \u03c0(s)) and P\u03c0(ds\u2032|s) = P (ds\u2032|s, \u03c0(s)) for the immediate reward and the stochastic kernel associated to policy \u03c0. The value v\u03c0 of a policy \u03c0 is a function mapping states to the expected discounted sum of rewards received when following \u03c0 from any state: for all s \u2208 S,\nv\u03c0(s) = E\n[\n\u221e \u2211\nt=0\n\u03b3tr\u03c0(st)\n\u2223 \u2223 \u2223 \u2223 \u2223 s0 = s, st+1 \u223c P\u03c0(\u00b7|st) ] .\nThe value v\u03c0 is clearly bounded by Vmax = Rmax/(1 \u2212 \u03b3). It is well-known that v\u03c0 can be characterized as the unique fixed point of the linear Bellman operator associated to a policy \u03c0: T\u03c0 : v 7\u2192 r\u03c0 + \u03b3P\u03c0v. Similarly, the Bellman optimality operator T : v 7\u2192 max\u03c0 T\u03c0v has as unique fixed point the optimal value v\u2217 = max\u03c0 v\u03c0. A policy \u03c0 is greedy w.r.t. a value function v if T\u03c0v = Tv, the set of such greedy policies is written G(v). Finally, a policy \u03c0\u2217 is optimal, with value v\u03c0\u2217 = v\u2217, iff \u03c0\u2217 \u2208 G(v\u2217), or equivalently T\u03c0\u2217v\u2217 = v\u2217.\nThough it is known [24, 4] that there always exists a deterministic stationary policy that is optimal, we will, in this article, consider non-stationary policies and now introduce related notations. Given a sequence \u03c01, \u03c02, . . . , \u03c0k of k stationary policies (this sequence will be clear in the context we describe later), and for any 1 \u2264 m \u2264 k, we will denote \u03c0k,m the periodic non-stationary policy that takes the first action according to \u03c0k, the second according to \u03c0k\u22121, . . . , the mth according to \u03c0k\u2212m+1 and then starts again. Formally, this can be written as\n\u03c0k,m = \u03c0k \u03c0k\u22121 \u00b7 \u00b7 \u00b7 \u03c0k\u2212m+1 \u03c0k \u03c0k\u22121 \u00b7 \u00b7 \u00b7\u03c0k\u2212m+1 \u00b7 \u00b7 \u00b7\nIt is straightforward to show that the value v\u03c0k,m of this periodic non-stationary policy \u03c0k,m is the unique fixed point of the following operator:\nTk,m = T\u03c0k T\u03c0k\u22121 \u00b7 \u00b7 \u00b7 T\u03c0k\u2212m+1 .\nFinally, it will be convenient to introduce the following discounted kernel:\n\u0393k,m = (\u03b3P\u03c0k)(\u03b3P\u03c0k\u22121) \u00b7 \u00b7 \u00b7 (\u03b3P\u03c0k\u2212m+1).\nIn particular, for any pair of values v and v\u2032, it can easily be seen that Tk,mv\u2212Tk,mv\u2032 = \u0393k,m(v\u2212v\u2032)."}, {"heading": "3 Tightness of the performance bound of Theorem 1", "text": "The bound of Theorem 1 is tight for API in the sense that there exists an MDP [4, Example 6.4] for which the bound is reached. To the best of our knowledge, a similar argument has never been provided for AVI in the literature. It turns out that the MDP that is used for showing the tightness for API also applies to AVI. This is what we show in this section.\nExample 1. Consider the \u03b3-discounted deterministic MDP from [4, Example 6.4] depicted on Figure 1. It involves states 1, 2, . . . . In state 1 there is only one self-loop action with zero reward, for each state i > 1 there are two possible choices: either move to state i \u2212 1 with zero reward or stay\nwith reward ri = \u22122 \u03b3\u2212\u03b3i\n1\u2212\u03b3 \u01eb with \u01eb \u2265 0. Clearly the optimal policy in all states i > 1 is to move to i\u2212 1 and the optimal value function v\u2217 is 0 in all states.\nStarting with v0 = v\u2217, we are going to show that for all iterations k \u2265 1 it is possible to have a policy \u03c0k+1 \u2208 G(vk) which moves in every state but k + 1 and thus is such that v\u03c0k+1(k + 1) = rk+1 1\u2212\u03b3 = \u22122 \u03b3\u2212\u03b3k+1 (1\u2212\u03b3)2 \u01eb, which meets the bound of Theorem 1 when k tends to infinity.\nTo do so, we assume that the following approximation errors are made at each iteration k > 0:\n\u01ebk(i) =\n{\n\u2212\u01eb if i = k \u01eb if i = k + 1 0 otherwise .\nWith this error, we are now going to prove by induction on k that for all k \u2265 1,\nvk(i) =\n\n \n  \u2212\u03b3k\u22121\u01eb if i < k rk/2\u2212 \u01eb if i = k \u2212(rk/2\u2212 \u01eb) if i = k + 1 0 otherwise .\nSince v0 = 0 the best action is clearly to move in every state i \u2265 2 which gives v1 = v0 + \u01eb1 = \u01eb1 which establishes the claim for k = 1.\nAssuming that our induction claim holds for k, we now show that it also holds for k + 1.\nFor the move action, write qmk its action-value function. For all i > 1 we have q m k (i) = 0+ \u03b3vk(i\u2212 1), hence\nqmk (i) =\n\n \n  \u03b3(\u2212\u03b3k\u22121\u01eb) = \u2212\u03b3k\u01eb if i = 2, . . . , k \u03b3(rk/2\u2212 \u01eb) = rk+1/2 if i = k + 1 \u2212\u03b3(rk/2\u2212 \u01eb) = \u2212rk+1/2 if i = k + 2 0 otherwise .\nFor the stay action, write qsk its action-value function. For all i > 0 we have q s k(i) = ri + \u03b3vk(i), hence\nqsk(i) =\n\n   \n   \nri + \u03b3(\u2212\u03b3 k\u22121\u01eb) = ri \u2212 \u03b3 k\u01eb if i = 1, . . . , k \u2212 1 rk + \u03b3(rk/2\u2212 \u01eb) = rk + rk+1/2 if i = k rk+1 \u2212 rk+1/2 = rk+1/2 if i = k + 1 rk+2 + \u03b30 = rk+2 if i = k + 2 0 otherwise .\nFirst, only the stay action is available in state 1, hence, since r0 = 0 and \u01ebk+1(1) = 0, we have vk+1(1) = q s k(1) + \u01ebk+1(1) = \u2212\u03b3\nk\u01eb, as desired. Second, since ri < 0 for all i > 1 we have qmk (i) > q s k(i) for all these states but k+1 where q m k (k+1) = q s k(k+1) = rk+1/2. Using the fact that vk+1 = max(qmk , q s k) + \u01ebk+1 gives the result for vk+1.\nThe fact that for i > 1 we have qmk (i) \u2265 q s k(i) with equality only at i = k+1 implies that there exists a policy \u03c0k+1 greedy for vk which takes the optimal move action in all states but k + 1 where the stay action has the same value, leaving the algorithm the possibility of choosing the suboptimal stay action in this state, yielding a value v\u03c0k+1(k + 1), matching the upper bound as k goes to infinity.\nSince Example 1 shows that the bound of Theorem 1 is tight, improving performance bounds imply to modify the algorithms. The following sections of the paper shows that considering non-stationary policies instead of stationary policies is an interesting path to follow."}, {"heading": "4 Deducing a non-stationary policy from AVI", "text": "While AVI (Equation (1)) is usually considered as generating a sequence of values v0, v1, . . . , vk\u22121, it also implicitely produces a sequence1 of policies \u03c01, \u03c02, . . . , \u03c0k, where for i = 0, . . . , k \u2212 1, \u03c0i+1 \u2208 G(vi). Instead of outputing only the last policy \u03c0k, we here simply propose to output the periodic non-stationary policy \u03c0k,m that loops over the last m generated policies. The following theorem shows that it is indeed a good idea.\nTheorem 2. For all iteration k and m such that 1 \u2264 m \u2264 k, the loss of running the non-stationary policy \u03c0k,m instead of the optimal policy \u03c0\u2217 satisfies:\n\u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e \u2264 2\n1\u2212 \u03b3m\n(\n\u03b3 \u2212 \u03b3k\n1\u2212 \u03b3 \u01eb+ \u03b3k\u2016v\u2217 \u2212 v0\u2016\u221e\n)\n.\nWhen m = 1 and k tends to infinity, one exactly recovers the result of Theorem 1. For general m, this new bound is a factor 1\u2212\u03b3 m\n1\u2212\u03b3 better than the standard bound of Theorem 1. The choice that optimizes the bound, m = k, and which consists in looping over all the policies generated from the very start, leads to the following bound:\n\u2016v\u2217 \u2212 v\u03c0k,k\u2016\u221e \u2264 2\n(\n\u03b3\n1\u2212 \u03b3 \u2212\n\u03b3k\n1\u2212 \u03b3k\n)\n\u01eb+ 2\u03b3k\n1\u2212 \u03b3k \u2016v\u2217 \u2212 v0\u2016\u221e,\nthat tends to 2\u03b31\u2212\u03b3 \u01eb when k tends to \u221e.\nThe rest of the section is devoted to the proof of Theorem 2. An important step of our proof lies in the following lemma, that implies that for sufficiently big m, vk = Tvk\u22121 + \u01ebk is a rather good approximation (of the order \u01eb1\u2212\u03b3 ) of the value v\u03c0k,m of the non-stationary policy \u03c0k,m (whereas in general, it is a much poorer approximation of the value v\u03c0k of the last stationary policy \u03c0k).\nLemma 1. For all m and k such that 1 \u2264 m \u2264 k,\n\u2016Tvk\u22121 \u2212 v\u03c0k,m\u2016\u221e \u2264 \u03b3 m\u2016vk\u2212m \u2212 v\u03c0k,m\u2016\u221e +\n\u03b3 \u2212 \u03b3m\n1\u2212 \u03b3 \u01eb.\nProof of Lemma 1. The value of \u03c0k,m satisfies:\nv\u03c0k,m = T\u03c0kT\u03c0k\u22121 \u00b7 \u00b7 \u00b7T\u03c0k\u2212m+1v\u03c0k,m . (3)\nBy induction, it can be shown that the sequence of values generated by AVI satisfies:\nT\u03c0kvk\u22121 = T\u03c0kT\u03c0k\u22121 \u00b7 \u00b7 \u00b7T\u03c0k\u2212m+1vk\u2212m +\nm\u22121 \u2211\ni=1\n\u0393k,i\u01ebk\u2212i. (4)\nBy substracting Equations (4) and (3), one obtains:\nTvk\u22121 \u2212 v\u03c0k,m = T\u03c0kvk\u22121 \u2212 v\u03c0k,m = \u0393k,m(vk\u2212m \u2212 v\u03c0k,m) +\nm\u22121 \u2211\ni=1\n\u0393k,i\u01ebk\u2212i\nand the result follows by taking the norm and using the fact that for all i, \u2016\u0393k,i\u2016\u221e = \u03b3i.\nWe are now ready to prove the main result of this section.\nProof of Theorem 2. Using the fact that T is a contraction in max-norm, we have:\n\u2016v\u2217 \u2212 vk\u2016\u221e = \u2016v\u2217 \u2212 Tvk\u22121 + \u01ebk\u2016\u221e \u2264 \u2016Tv\u2217 \u2212 Tvk\u22121\u2016\u221e + \u01eb\n\u2264 \u03b3\u2016v\u2217 \u2212 vk\u22121\u2016\u221e + \u01eb.\n1A given sequence of value functions may induce many sequences of policies since more than one greedy policy may exist for one particular value function. Our results holds for all such possible choices of greedy policies.\nThen, by induction on k, we have that for all k \u2265 1,\n\u2016v\u2217 \u2212 vk\u2016\u221e \u2264 \u03b3 k\u2016v\u2217 \u2212 v0\u2016\u221e +\n1\u2212 \u03b3k\n1\u2212 \u03b3 \u01eb. (5)\nUsing Lemma 1 and Equation (5) twice, we can conclude by observing that\n\u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e \u2264 \u2016Tv\u2217 \u2212 Tvk\u22121\u2016\u221e + \u2016Tvk\u22121 \u2212 v\u03c0k,m\u2016\u221e\n\u2264 \u03b3\u2016v\u2217 \u2212 vk\u22121\u2016\u221e + \u03b3 m\u2016vk\u2212m \u2212 v\u03c0k,m\u2016\u221e +\n\u03b3 \u2212 \u03b3m\n1\u2212 \u03b3 \u01eb\n\u2264 \u03b3\n(\n\u03b3k\u22121\u2016v\u2217 \u2212 v0\u2016\u221e + 1\u2212 \u03b3k\u22121\n1\u2212 \u03b3 \u01eb\n)\n+ \u03b3m ( \u2016vk\u2212m \u2212 v\u2217\u2016\u221e + \u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e )\n+ \u03b3 \u2212 \u03b3m\n1\u2212 \u03b3 \u01eb\n\u2264 \u03b3k\u2016v\u2217 \u2212 v0\u2016\u221e + \u03b3 \u2212 \u03b3k\n1\u2212 \u03b3 \u01eb\n+ \u03b3m ( \u03b3k\u2212m\u2016v\u2217 \u2212 v0\u2016\u221e + 1\u2212 \u03b3k\u2212m\n1\u2212 \u03b3 \u01eb + \u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e\n)\n+ \u03b3 \u2212 \u03b3m\n1\u2212 \u03b3 \u01eb\n= \u03b3m\u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e + 2\u03b3 k\u2016v\u2217 \u2212 v0\u2016\u221e +\n2(\u03b3 \u2212 \u03b3k)\n1\u2212 \u03b3 \u01eb\n\u2264 2\n1\u2212 \u03b3m\n(\n\u03b3 \u2212 \u03b3k\n1\u2212 \u03b3 \u01eb+ \u03b3k\u2016v\u2217 \u2212 v0\u2016\u221e\n)\n."}, {"heading": "5 API algorithms for computing non-stationary policies", "text": "We now present similar results that have a Policy Iteration flavour. Unlike in the previous section where only the output of AVI needed to be changed, improving the bound for an API-like algorithm is slightly more involved. In this section, we describe and analyze two API algorithms that output non-stationary policies with improved performance bounds.\nAPI with a non-stationary policy of growing period Following our findings on non-stationary policies AVI, we consider the following variation of API, where at each iteration, instead of computing the value of the last stationary policy \u03c0k, we compute that of the periodic non-stationary policy \u03c0k,k that loops over all the policies \u03c01, . . . , \u03c0k generated from the very start:\nvk \u2190 v\u03c0k,k + \u01ebk\n\u03c0k+1 \u2190 any element of G(vk)\nwhere the initial (stationary) policy \u03c01,1 is chosen arbitrarily. Thus, iteration after iteration, the nonstationary policy \u03c0k,k is made of more and more stationary policies, and this is why we refer to it as having a growing period. We can prove the following performance bound for this algorithm:\nTheorem 3. After k iterations, the loss of running the non-stationary policy \u03c0k,k instead of the optimal policy \u03c0\u2217 satisfies:\n\u2016v\u2217 \u2212 v\u03c0k,k\u2016\u221e \u2264 2(\u03b3 \u2212 \u03b3k)\n1\u2212 \u03b3 \u01eb+ \u03b3k\u22121\u2016v\u2217 \u2212 v\u03c01,1\u2016\u221e + 2(k \u2212 1)\u03b3 kVmax.\nWhen k tends to infinity, this bound tends to 2\u03b31\u2212\u03b3 \u01eb, and is thus again a factor 1\n1\u2212\u03b3 better than the original API bound.\nProof of Theorem 3. Using the facts that Tk+1,k+1v\u03c0k,k = T\u03c0k+1Tk,kv\u03c0k,k = T\u03c0k+1v\u03c0k,k and T\u03c0k+1vk \u2265 T\u03c0\u2217vk (since \u03c0k+1 \u2208 G(vk)), we have:\nv\u2217 \u2212 v\u03c0k+1,k+1\n= T\u03c0\u2217v\u2217 \u2212 Tk+1,k+1v\u03c0k+1,k+1\n= T\u03c0\u2217v\u2217 \u2212 T\u03c0\u2217v\u03c0k,k + T\u03c0\u2217v\u03c0k,k \u2212 Tk+1,k+1v\u03c0k,k + Tk+1,k+1v\u03c0k,k \u2212 Tk+1,k+1v\u03c0k+1,k+1\n= \u03b3P\u03c0\u2217(v\u2217 \u2212 v\u03c0k,k) + T\u03c0\u2217v\u03c0k,k \u2212 T\u03c0k+1v\u03c0k,k + \u0393k+1,k+1(v\u03c0k,k \u2212 v\u03c0k+1,k+1)\n= \u03b3P\u03c0\u2217(v\u2217 \u2212 v\u03c0k,k) + T\u03c0\u2217vk \u2212 T\u03c0k+1vk + \u03b3(P\u03c0k+1 \u2212 P\u03c0\u2217)\u01ebk + \u0393k+1,k+1(v\u03c0k,k \u2212 v\u03c0k+1,k+1)\n\u2264 \u03b3P\u03c0\u2217(v\u2217 \u2212 v\u03c0k,k) + \u03b3(P\u03c0k+1 \u2212 P\u03c0\u2217)\u01ebk + \u0393k+1,k+1(v\u03c0k,k \u2212 v\u03c0k+1,k+1).\nBy taking the norm, and using the facts that \u2016v\u03c0k,k\u2016\u221e \u2264 Vmax, \u2016v\u03c0k+1,k+1\u2016\u221e \u2264 Vmax, and \u2016\u0393k+1,k+1\u2016\u221e = \u03b3 k+1, we get:\n\u2016v\u2217 \u2212 v\u03c0k+1,k+1\u2016\u221e \u2264 \u03b3\u2016v\u2217 \u2212 v\u03c0k,k\u2016\u221e + 2\u03b3\u01eb+ 2\u03b3 k+1Vmax.\nFinally, by induction on k, we obtain:\n\u2016v\u2217 \u2212 v\u03c0k,k\u2016\u221e \u2264 2(\u03b3 \u2212 \u03b3k)\n1\u2212 \u03b3 \u01eb+ \u03b3k\u22121\u2016v\u2217 \u2212 v\u03c01,1\u2016\u221e + 2(k \u2212 1)\u03b3 kVmax.\nThough it has an improved asymptotic performance bound, the API algorithm we have just described has two (related) drawbacks: 1) its finite iteration bound has a somewhat unsatisfactory term of the form 2(k \u2212 1)\u03b3kVmax, and 2) even when there is no error (when \u01eb = 0), we cannot guarantee that, similarly to standard Policy Iteration, it generates a sequence of policies of increasing values (it is easy to see that in general, we do not have v\u03c0k+1,k+1 \u2265 v\u03c0k,k ). These two points motivate the introduction of another API algorithm.\nAPI with a non-stationary policy of fixed period We consider now another variation of API parameterized by m \u2265 1, that iterates as follows for k \u2265 m:\nvk \u2190 v\u03c0k,m + \u01ebk\n\u03c0k+1 \u2190 any element of G(vk)\nwhere the initial non-stationary policy \u03c0m,m is built from a sequence of m arbitrary stationary policies \u03c01, \u03c02, \u00b7 \u00b7 \u00b7 , \u03c0m. Unlike the previous API algorithm, the non-stationary policy \u03c0k,m here only involves the last m greedy stationary policies instead of all of them, and is thus of fixed period. This is a strict generalization of the standard API algorithm, with which it coincides when m = 1. For this algorithm, we can prove the following performance bound: Theorem 4. For all m, for all k \u2265 m, the loss of running the non-stationary policy \u03c0k,m instead of the optimal policy \u03c0\u2217 satisfies:\n\u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e \u2264 \u03b3 k\u2212m\u2016v\u2217 \u2212 v\u03c0m,m\u2016\u221e +\n2(\u03b3 \u2212 \u03b3k+1\u2212m)\n(1\u2212 \u03b3)(1\u2212 \u03b3m) \u01eb.\nWhen m = 1 and k tends to infinity, we recover exactly the bound of Theorem 1. When m > 1 and k tends to infinity, this bound coincides with that of Theorem 2 for our non-stationary version of AVI: it is a factor 1\u2212\u03b3 m\n1\u2212\u03b3 better than the standard bound of Theorem 1.\nThe rest of this section develops the proof of this performance bound. A central argument of our proof is the following lemma, which shows that similarly to the standard API, our new algorithm has an (approximate) policy improvement property. Lemma 2. At each iteration of the algorithm, the value v\u03c0k+1,m of the non-stationary policy\n\u03c0k+1,m = \u03c0k+1 \u03c0k . . . \u03c0k+2\u2212m \u03c0k+1 \u03c0k . . . \u03c0k\u2212m+2 . . .\ncannot be much worse than the value v\u03c0\u2032 k,m of the non-stationary policy\n\u03c0\u2032k,m = \u03c0k\u2212m+1 \u03c0k . . . \u03c0k+2\u2212m \u03c0k\u2212m+1 \u03c0k . . . \u03c0k\u2212m+2 . . .\nin the precise following sense:\nv\u03c0k+1,m \u2265 v\u03c0\u2032k,m \u2212 2\u03b3\n1\u2212 \u03b3m \u01eb.\nThe policy \u03c0\u2032k,m differs from \u03c0k+1,m in that every m steps, it chooses the oldest policy \u03c0k\u2212m+1 instead of the newest one \u03c0k+1. Also \u03c0\u2032k,m is related to \u03c0k,m as follows: \u03c0 \u2032\nk,m takes the first action according to \u03c0k\u2212m+1 and then runs \u03c0k,m; equivalently, since \u03c0k,m loops over \u03c0k\u03c0k\u22121 . . . \u03c0k\u2212m+1, \u03c0\u2032k,m = \u03c0k\u2212m+1\u03c0k,m can be seen as a 1-step right rotation of \u03c0k,m. When there is no error (when \u01eb = 0), this shows that the new policy \u03c0k+1,m is better than a \u201crotation\u201d of \u03c0k,m. When m = 1, \u03c0k+1,m = \u03c0k+1 and \u03c0\u2032k,m = \u03c0k and we thus recover the well-known (approximate) policy improvement theorem for standard API (see for instance [4, Lemma 6.1]).\nProof of Lemma 2. Since \u03c0\u2032k,m takes the first action with respect to \u03c0k\u2212m+1 and then runs \u03c0k,m, we have v\u03c0\u2032\nk,m = T\u03c0k\u2212m+1v\u03c0k,m . Now, since \u03c0k+1 \u2208 G(vk), we have T\u03c0k+1vk \u2265 T\u03c0k\u2212m+1vk and\nv\u03c0\u2032 k,m \u2212 v\u03c0k+1,m = T\u03c0k\u2212m+1v\u03c0k,m \u2212 v\u03c0k+1,m\n= T\u03c0k\u2212m+1vk \u2212 \u03b3P\u03c0k\u2212m+1\u01ebk \u2212 v\u03c0k+1,m\n\u2264 T\u03c0k+1vk \u2212 \u03b3P\u03c0k\u2212m+1\u01ebk \u2212 v\u03c0k+1,m\n= T\u03c0k+1v\u03c0k,m + \u03b3(P\u03c0k+1 \u2212 P\u03c0k\u2212m+1)\u01ebk \u2212 v\u03c0k+1,m\n= T\u03c0k+1Tk,mv\u03c0k,m \u2212 Tk+1,mv\u03c0k+1,m + \u03b3(P\u03c0k+1 \u2212 P\u03c0k\u2212m+1)\u01ebk\n= Tk+1,mT\u03c0k\u2212m+1v\u03c0k,m \u2212 Tk+1,mv\u03c0k+1,m + \u03b3(P\u03c0k+1 \u2212 P\u03c0k\u2212m+1)\u01ebk\n= \u0393k+1,m(T\u03c0k\u2212m+1v\u03c0k,m \u2212 v\u03c0k+1,m) + \u03b3(P\u03c0k+1 \u2212 P\u03c0k\u2212m+1)\u01ebk\n= \u0393k+1,m(v\u03c0\u2032 k,m \u2212 v\u03c0k+1,m) + \u03b3(P\u03c0k+1 \u2212 P\u03c0k\u2212m+1)\u01ebk.\nfrom which we deduce that:\nv\u03c0\u2032 k,m \u2212 v\u03c0k+1,m \u2264 (I \u2212 \u0393k+1,m) \u22121\u03b3(P\u03c0k+1 \u2212 P\u03c0k\u2212m+1)\u01ebk\nand the result follows by using the facts that \u2016\u01ebk\u2016\u221e \u2264 \u01eb and \u2016(I \u2212 \u0393k+1,m)\u22121\u2016\u221e = 11\u2212\u03b3m .\nWe are now ready to prove the main result of this section.\nProof of Theorem 4. Using the facts that 1) Tk+1,m+1v\u03c0k,m = T\u03c0k+1Tk,mv\u03c0k,m = T\u03c0k+1v\u03c0k,m and 2) T\u03c0k+1vk \u2265 T\u03c0\u2217vk (since \u03c0k+1 \u2208 G(vk)), we have for k \u2265 m,\nv\u2217 \u2212 v\u03c0k+1,m\n= T\u03c0\u2217v\u2217 \u2212 Tk+1,mv\u03c0k+1,m\n= T\u03c0\u2217v\u2217 \u2212 T\u03c0\u2217v\u03c0k,m + T\u03c0\u2217v\u03c0k,m \u2212 Tk+1,m+1v\u03c0k,m + Tk+1,m+1v\u03c0k,m \u2212 Tk+1,mv\u03c0k+1,m\n= \u03b3P\u03c0\u2217(v\u2217 \u2212 v\u03c0k,m) + T\u03c0\u2217v\u03c0k,m \u2212 T\u03c0k+1v\u03c0k,m + \u0393k+1,m(T\u03c0k\u2212m+1v\u03c0k,m \u2212 v\u03c0k+1,m)\n\u2264 \u03b3P\u03c0\u2217(v\u2217 \u2212 v\u03c0k,m) + T\u03c0\u2217vk \u2212 T\u03c0k+1vk + \u03b3(P\u03c0k+1 \u2212 P\u03c0\u2217)\u01ebk + \u0393k+1,m(T\u03c0k\u2212m+1v\u03c0k,m \u2212 v\u03c0k+1,m)\n\u2264 \u03b3P\u03c0\u2217(v\u2217 \u2212 v\u03c0k,m) + \u03b3(P\u03c0k+1 \u2212 P\u03c0\u2217)\u01ebk + \u0393k+1,m(T\u03c0k\u2212m+1v\u03c0k,m \u2212 v\u03c0k+1,m). (6)\nConsider the policy \u03c0\u2032k,m defined in Lemma 2. Observing as in the beginning of the proof of Lemma 2 that T\u03c0k\u2212m+1v\u03c0k,m = v\u03c0\u2032k,m , Equation (6) can be rewritten as follows:\nv\u2217 \u2212 v\u03c0k+1,m \u2264 \u03b3P\u03c0\u2217(v\u2217 \u2212 v\u03c0k,m) + \u03b3(P\u03c0k+1 \u2212 P\u03c0\u2217)\u01ebk + \u0393k+1,m(v\u03c0\u2032k,m \u2212 v\u03c0k+1,m).\nBy using the facts that v\u2217 \u2265 v\u03c0k,m , v\u2217 \u2265 v\u03c0k+1,m and Lemma 2, we get\n\u2016v\u2217 \u2212 v\u03c0k+1,m\u2016\u221e \u2264 \u03b3\u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e + 2\u03b3\u01eb+ \u03b3m(2\u03b3\u01eb)\n1\u2212 \u03b3m\n= \u03b3\u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e + 2\u03b3\n1\u2212 \u03b3m \u01eb.\nFinally, we obtain by induction that for all k \u2265 m,\n\u2016v\u2217 \u2212 v\u03c0k,m\u2016\u221e \u2264 \u03b3 k\u2212m\u2016v\u2217 \u2212 v\u03c0m,m\u2016\u221e +\n2(\u03b3 \u2212 \u03b3k+1\u2212m)\n(1\u2212 \u03b3)(1\u2212 \u03b3m) \u01eb."}, {"heading": "6 Discussion, conclusion and future work", "text": "We recalled in Theorem 1 the standard performance bound when computing an approximately optimal stationary policy with the standard AVI and API algorithms. After arguing that this bound is tight \u2013 in particular by providing an original argument for AVI \u2013 we proposed three new dynamic programming algorithms (one based on AVI and two on API) that output non-stationary policies for which the performance bound can be significantly reduced (by a factor 11\u2212\u03b3 ).\nFrom a bibliographical point of view, it is the work of [14] that made us think that non-stationary policies may lead to better performance bounds. In that work, the author considers problems with a finite-horizon T for which one computes non-stationary policies with performance bounds in O(T \u01eb), and infinite-horizon problems for which one computes stationary policies with performance bounds in O( \u01eb(1\u2212\u03b3)2 ). Using the informal equivalence of the horizons T \u2243 1 1\u2212\u03b3 one sees that non-stationary policies look better than stationary policies. In [14], non-stationary policies are only computed in the context of finite-horizon (and thus non-stationary) problems; the fact that nonstationary policies can also be useful in an infinite-horizon stationary context is to our knowledge completely new.\nThe best performance improvements are obtained when our algorithms consider periodic nonstationary policies of which the period grows to infinity, and thus require an infinite memory, which may look like a practical limitation. However, in two of the proposed algorithm, a parameter m allows to make a trade-off between the quality of approximation 2\u03b3(1\u2212\u03b3m)(1\u2212\u03b3)\u01eb and the amount of memory O(m) required. In practice, it is easy to see that by choosing m = \u2308\n1 1\u2212\u03b3\n\u2309\n, that is a\nmemory that scales linearly with the horizon (and thus the difficulty) of the problem, one can get a performance bound of2 2\u03b3(1\u2212e\u22121)(1\u2212\u03b3)\u01eb \u2264 3.164\u03b3 1\u2212\u03b3 \u01eb.\nWe conjecture that our asymptotic bound of 2\u03b31\u2212\u03b3 \u01eb, and the non-asymptotic bounds of Theorems 2 and 4 are tight. The actual proof of this conjecture is left for future work. Important recent works of the literature involve studying performance bounds when the errors are controlled in Lp norms instead of max-norm [19, 20, 21, 1, 8, 18, 17] which is natural when supervised learning algorithms are used to approximate the evaluation steps of AVI and API. Since our proof are based on componentwise bounds like those of the pioneer works in this topic [19, 20], we believe that the extension of our analysis to Lp norm analysis is straightforward. Last but not least, an important research direction that we plan to follow consists in revisiting the many implementations of AVI and API for building stationary policies (see the list in the introduction), turn them into algorithms that look for non-stationary policies and study them precisely analytically as well as empirically."}], "references": [{"title": "Cs", "author": ["A. Antos"], "venue": "Szepesv\u00e1ri, and R. Munos. Learning near-optimal policies with Bellmanresidual minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89\u2013129", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Dynamic Policy Programming with Function Approximation", "author": ["M. Gheshlaghi Azar", "V. Gmez", "H.J. Kappen"], "venue": "14th International Conference on Artificial Intelligence and Statistics (AISTATS), volume 15, Fort Lauderdale, FL, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Approximate policy iteration: a survey and some new methods", "author": ["D.P. Bertsekas"], "venue": "Journal of Control Theory and Applications, 9:310\u2013335", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Leastsquares methods for Policy Iteration", "author": ["L. Busoniu", "A. Lazaric", "M. Ghavamzadeh", "R. Munos", "R. Babuska", "B. De Schutter"], "venue": "M. Wiering and M. van Otterlo, editors, Reinforcement Learning: State of the Art. Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "Journal of Machine Learning Research (JMLR), 6", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Planning in pomdps using multiplicity automata", "author": ["E. Even-dar"], "venue": "Uncertainty in Artificial Intelligence (UAI, pages 185\u2013192", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Cs", "author": ["A.M. Farahmand", "M. Ghavamzadeh"], "venue": "Szepesv\u00e1ri, and S. Mannor. Regularized policy iteration. Advances in Neural Information Processing Systems, 21:441\u2013448", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Error propagation for approximate policy and value iteration (extended version)", "author": ["A.M. Farahmand", "R. Munos", "Cs. Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Classification-based Policy Iteration with a Critic", "author": ["V. Gabillon", "A. Lazaric", "M. Ghavamzadeh", "B. Scherrer"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Stable Function Approximation in Dynamic Programming", "author": ["G.J. Gordon"], "venue": "ICML, pages 261\u2013268", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Max-norm projections for factored MDPs", "author": ["C. Guestrin", "D. Koller", "R. Parr"], "venue": "International Joint Conference on Artificial Intelligence, volume 17-1, pages 673\u2013682", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient Solution Algorithms for Factored MDPs", "author": ["C. Guestrin", "D. Koller", "R. Parr", "S. Venkataraman"], "venue": "Journal of Artificial Intelligence Research (JAIR), 19:399\u2013468", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["S.M. Kakade"], "venue": "PhD thesis, University College London", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Approximately Optimal Approximate Reinforcement Learning", "author": ["S.M. Kakade", "J. Langford"], "venue": "International Conference on Machine Learning (ICML), pages 267\u2013274", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research (JMLR), 4:1107\u20131149", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Finite-Sample Analysis of Least-Squares Policy Iteration", "author": ["A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "To appear in Journal of Machine learning Research (JMLR)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Finite Sample Analysis of Bellman Residual Minimization", "author": ["O.A. Maillard", "R. Munos", "A. Lazaric", "M. Ghavamzadeh"], "venue": "Masashi Sugiyama and Qiang Yang, editors, Asian Conference on Machine Learpning. JMLR: Workshop and Conference Proceedings, volume 13, pages 309\u2013 324", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Error Bounds for Approximate Policy Iteration", "author": ["R. Munos"], "venue": "International Conference on Machine Learning (ICML), pages 560\u2013567", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Performance Bounds in Lp norm for Approximate Value Iteration", "author": ["R. Munos"], "venue": "SIAM J. Control and Optimization", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Finite time bounds for sampling based fitted value iteration", "author": ["R. Munos", "Cs. Szepesv\u00e1ri"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Biasing Approximate Dynamic Programming with a Lower Discount Factor", "author": ["M. Petrik", "B. Scherrer"], "venue": "Twenty-Second Annual Conference on Neural Information Processing Systems -NIPS 2008, Vancouver, Canada", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G.J. Gordon", "S. Thrun"], "venue": "International Joint Conference on Artificial Intelligence, volume 18, pages 1025\u20131032", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Markov Decision Processes", "author": ["M. Puterman"], "venue": "Wiley, New York", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "An Upper Bound on the Loss from Approximate Optimal-Value Functions", "author": ["S. Singh", "R. Yee"], "venue": "Machine Learning, 16-3:227\u2013233", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1994}, {"title": "Least-Squares \u03bb Policy Iteration: Bias-Variance Trade-off in Control Problems", "author": ["C. Thiery", "B. Scherrer"], "venue": "International Conference on Machine Learning, Haifa, Israel", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Feature-Based Methods for Large Scale Dynamic Programming", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "Machine Learning, 22(1-3):59\u201394", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 23, "context": "1 Introduction Given an infinite-horizon stationary \u03b3-discounted Markov Decision Process [24, 4], we consider approximate versions of the standard Dynamic Programming algorithms, Policy and Value Iteration, that build sequences of value functions vk and policies \u03c0k as follows Approximate Value Iteration (AVI): vk+1 \u2190 Tvk + \u01ebk+1 (1) Approximate Policy Iteration (API): {", "startOffset": 89, "endOffset": 96}, {"referenceID": 3, "context": "1 Introduction Given an infinite-horizon stationary \u03b3-discounted Markov Decision Process [24, 4], we consider approximate versions of the standard Dynamic Programming algorithms, Policy and Value Iteration, that build sequences of value functions vk and policies \u03c0k as follows Approximate Value Iteration (AVI): vk+1 \u2190 Tvk + \u01ebk+1 (1) Approximate Policy Iteration (API): {", "startOffset": 89, "endOffset": 96}, {"referenceID": 24, "context": "Under this assumption, it is well-known that both algorithms share the following performance bound (see [25, 11, 4] for AVI and [4] for API): Theorem 1.", "startOffset": 104, "endOffset": 115}, {"referenceID": 10, "context": "Under this assumption, it is well-known that both algorithms share the following performance bound (see [25, 11, 4] for AVI and [4] for API): Theorem 1.", "startOffset": 104, "endOffset": 115}, {"referenceID": 3, "context": "Under this assumption, it is well-known that both algorithms share the following performance bound (see [25, 11, 4] for AVI and [4] for API): Theorem 1.", "startOffset": 104, "endOffset": 115}, {"referenceID": 3, "context": "Under this assumption, it is well-known that both algorithms share the following performance bound (see [25, 11, 4] for AVI and [4] for API): Theorem 1.", "startOffset": 128, "endOffset": 131}, {"referenceID": 24, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 10, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 26, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 11, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 12, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 22, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 6, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 5, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 19, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 20, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 21, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 8, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 91, "endOffset": 136}, {"referenceID": 14, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 18, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 15, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 0, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 7, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 17, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 4, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 16, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 9, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 2, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 8, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 1, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 153, "endOffset": 195}, {"referenceID": 25, "context": "Interestingly, this very constant 2\u03b3 (1\u2212\u03b3)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.", "startOffset": 231, "endOffset": 235}, {"referenceID": 23, "context": "2 Background We consider an infinite-horizon discounted Markov Decision Process [24, 4] (S,A, P, r, \u03b3), whereS is a possibly infinite state space, A is a finite action space, P (ds|s, a), for all (s, a), is a probability kernel on S, r : S \u00d7 A \u2192 R is a reward function bounded in max-norm by Rmax, and \u03b3 \u2208 (0, 1) is a discount factor.", "startOffset": 80, "endOffset": 87}, {"referenceID": 3, "context": "2 Background We consider an infinite-horizon discounted Markov Decision Process [24, 4] (S,A, P, r, \u03b3), whereS is a possibly infinite state space, A is a finite action space, P (ds|s, a), for all (s, a), is a probability kernel on S, r : S \u00d7 A \u2192 R is a reward function bounded in max-norm by Rmax, and \u03b3 \u2208 (0, 1) is a discount factor.", "startOffset": 80, "endOffset": 87}, {"referenceID": 23, "context": "Though it is known [24, 4] that there always exists a deterministic stationary policy that is optimal, we will, in this article, consider non-stationary policies and now introduce related notations.", "startOffset": 19, "endOffset": 26}, {"referenceID": 3, "context": "Though it is known [24, 4] that there always exists a deterministic stationary policy that is optimal, we will, in this article, consider non-stationary policies and now introduce related notations.", "startOffset": 19, "endOffset": 26}, {"referenceID": 13, "context": "From a bibliographical point of view, it is the work of [14] that made us think that non-stationary policies may lead to better performance bounds.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "In [14], non-stationary policies are only computed in the context of finite-horizon (and thus non-stationary) problems; the fact that nonstationary policies can also be useful in an infinite-horizon stationary context is to our knowledge completely new.", "startOffset": 3, "endOffset": 7}], "year": 2012, "abstractText": "We consider infinite-horizon stationary \u03b3-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. Using Value and Policy Iteration with some error \u01eb at each iteration, it is well-known that one can compute stationary policies that are 2\u03b3 (1\u2212\u03b3)2 \u01eb-optimal. After arguing that this guarantee is tight, we develop variations of Value and Policy Iteration for computing non-stationary policies that can be up to 2\u03b3 1\u2212\u03b3 \u01eb-optimal, which constitutes a significant improvement in the usual situation when \u03b3 is close to 1. Surprisingly, this shows that the problem of \u201ccomputing near-optimal non-stationary policies\u201d is much simpler than that of \u201ccomputing near-optimal stationary policies\u201d.", "creator": "LaTeX with hyperref package"}}}