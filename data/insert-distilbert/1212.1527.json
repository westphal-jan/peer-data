{"id": "1212.1527", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2012", "title": "Learning Mixtures of Arbitrary Distributions over Large Discrete Domains", "abstract": "we give an algorithm for learning a mixture of unstructured distributions. this problem arises in various unsupervised learning scenarios, observed for example in learning topic models from which a corpus collection of relational documents spanning several topics. traditionally we show someone how to learn the constituents ( or the topic distributions and the mixture weights ) of a mixture of $ k $ ( constant ) linear arbitrary empirical distributions over a large discrete domain $ [ n ] = { 1, 2,..., n } $, using $ o ( n \\ polylog n ) $ y samples.", "histories": [["v1", "Fri, 7 Dec 2012 04:03:06 GMT  (34kb)", "https://arxiv.org/abs/1212.1527v1", null], ["v2", "Tue, 9 Apr 2013 18:41:14 GMT  (39kb)", "http://arxiv.org/abs/1212.1527v2", "Update of previous version, which includes aperture and sample-size lower bounds"], ["v3", "Wed, 18 Sep 2013 04:18:49 GMT  (40kb)", "http://arxiv.org/abs/1212.1527v3", "Update of previous version with improved aperture and sample-size lower bounds"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["yuval rabani", "leonard schulman", "chaitanya swamy"], "accepted": false, "id": "1212.1527"}, "pdf": {"name": "1212.1527.pdf", "metadata": {"source": "CRF", "title": "Learning Mixtures of Arbitrary Distributions over Large Discrete Domains", "authors": ["Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "emails": ["yrabani@cs.huji.ac.il.", "schulman@caltech.edu."], "sections": [{"heading": null, "text": "ar X\niv :1\n21 2.\n15 27\nv3 [\ncs .L\nG ]\n1 8\nSe p\n20 13\nThis task is information-theoretically impossible for k > 1 under the usual sampling process from a mixture distribution. However, there are situations (such as the above-mentioned topic model case) in which each sample point consists of several observations from the same mixture constituent. This number of observations, which we call the \u201csampling aperture\u201d, is a crucial parameter of the problem.\nWe obtain the first bounds for this mixture-learning problem without imposing any assumptions on the mixture constituents. We show that efficient learning is possible exactly at the information-theoretically least-possible aperture of 2k \u2212 1. Thus, we achieve near-optimal dependence on n and optimal aperture. While the sample-size required by our algorithm depends exponentially on k, we prove that such a dependence is unavoidable when one considers general mixtures.\nA sequence of tools contribute to the algorithm, such as concentration results for random matrices, dimension reduction, moment estimations, and sensitivity analysis."}, {"heading": "1 Introduction", "text": "We give an algorithm for learning a mixture of unstructured distributions. More specifically, we consider the problem of learning a mixture of k arbitrary distributions over a large finite domain [n] = {1, 2, . . . , n}. This finds applications in various unsupervised learning scenarios including collaborative filtering [29], and learning topic models from a corpus of documents spanning several topics [39, 11], which is often used as the prototypical motivating example for this problem. Our goal is to learn the probabilistic model that is hypothesized to generate the observed data. In particular, we learn the constituents of the mixture and their weights in the mixture. (In the topic models application, the mixture constituents are the topic distributions.)\nIt is information-theoretically impossible to reconstruct the mixture model from single-snapshot samples. Thus, our work relies on multi-snapshot samples. To illustrate, in the (pure documents)\n\u2217The Rachel and Selim Benin School of Computer Science and Engineering and the Center of Excellence on Algorithms, The Hebrew University of Jerusalem, Jerusalem 91904, Israel. Email: yrabani@cs.huji.ac.il.\n\u2020Caltech, Pasadena, CA 91125, USA. Supported in part by NSF CCF-1038578, NSF CCF-0515342, NSA H9823006-1-0074, and NSF ITR CCR-0326554. Email: schulman@caltech.edu.\n\u2021Dept. of Combinatorics and Optimization, Univ. Waterloo, Waterloo, ON N2L 3G1, Canada. Supported in part by NSERC grant 32760-06, an NSERC Discovery Accelerator Supplement Award, and an Ontario Early Researcher Award. Email: cswamy@math.uwaterloo.ca.\ntopic model introduced in [39], each document consists of a bag of words generated by selecting a topic with probability proportional to its mixture weight and then taking independent samples from this topic\u2019s distribution (over words); so n is the size of the vocabulary and k is the number of topics. Notice that typically n will be quite large, and substantially larger than k. Also, clearly, if very long documents are available, the problem becomes easy, as each document already provides a very good sample for the distribution of its topic. Thus, it is desirable to keep the dependence of the sample size on n as low as possible, while at the same time minimize what we call the aperture, which is the number of snapshots per sample point (i.e., words per document). These parameters govern both the applicability of an algorithm and its computational complexity.\nOur results. We provide the first bounds for the mixture-learning problem without making any limiting assumptions on the mixture constituents. Let probability distributions p1, . . . , pk \u2208 \u2206n\u22121 denote the k-mixture constituents, where \u2206n\u22121 is the (n \u2212 1)-simplex, and w1, . . . , wk denote the mixture weights. Our algorithm uses\nO\n(\nk3n lnn\n\u01eb6\n)\n+O\n(\nk2n ln6 n ln ( k \u01eb )\n\u01eb4\n)\n+O\n(\nk\n\u01eb\n)O(k2)\n(1)\ndocuments (i.e., samples) and reconstructs with high probability (see Theorem 4.1) each mixture constituent up to \u21131-error \u01eb, and each mixture weight up to additive error \u01eb. We make no assumptions on the constituents. The asymptotic notation hides factors that are polynomial in wmin := mint wt and the \u201cwidth\u201d of the mixture (which intuitively measures the minimum variation distance between any two constituents). The three terms in (1) correspond to the requirements for the number of 1-, 2-, and (2k \u2212 1)-snapshots respectively. So we need aperture 2k \u2212 1 only for a small part of the sample (and this is necessary).\nNotably, we achieve near-optimal dependence on n and optimal aperture. To see this, and put our bounds in perspective, notice importantly that we recover the mixture constituents within \u21131distance \u01eb. One needs \u2126 ( n/\u01eb2 )\nsamples to learn even a single arbitrary distribution over [n] (i.e., k = 1) within \u21131-error \u01eb; for larger k but fixed aperture (independent of n), a sample size of \u2126(n) is necessary to recover even the expectation of the mixture distribution with constant \u21131-error. On the other hand, aperture \u2126 ( (n + k2) log nk )\nis sufficient for algorithmically trivial recovery of the model with constant \u2113\u221e error using few samples. Restricting the aperture to 2k \u2212 2 makes recovery impossible to arbitrary accuracy (without additional assumptions): we show that there are two far-apart k-mixtures that generate exactly the same aperture-(2k\u2212 2) sample distribution; moreover, we prove that with O(k) aperture, an exponential in k sample size is necessary for arbitrary-accuracy reconstruction. These lower bounds hold even for n = 2, and hence apply to arbitrary mixtures even if we allow O(k log n) aperture. Also, they apply even if we only want to construct a k-mixture source that is close in transportation distance to the true k-mixture source (as opposed to recovering the parameters of the true mixture). Section 6 presents these lower bounds. (Interestingly, an exponential in k sample-size lower bound is also known for the problem of learning a mixture of k Gaussians [36], but this lower bound applies for the parameter-recovery problem and not for reconstructing a mixture that is close to the true Gaussian mixture.)\nOur work yields new insights into the mixture-learning problem that nicely complements the recent interesting work of [4, 3, 2]. These papers posit certain assumptions on the mixture constituents, use constant aperture, and obtain incomparable sample-size bounds: they recover the constituents up to \u21132 or \u2113\u221e error using sample size that is poly(k) and sublinear in (or independent of) n. An important new insight revealed by our work is that such bounds of constant aperture and poly(k) sample size are impossible to achieve for arbitrary mixtures. Moreover, if we seek to\nachieve \u21131-error \u01eb, there are inputs for which their sample size is \u2126(n 3) (or worse, again ignoring dependence on wmin and \u201cwidth\u201d; see Appendix B). This is a significantly poorer dependence on n compared to our near-linear dependence (so our bounds are better when n is large but k is small). To appreciate a key distinction between our work and [4, 3, 2], observe that with \u2126(n3) samples, the entire distribution on 3-snapshots can be estimated fairly accurately; the challenge in [4, 3, 2] is therefore to recover the model from this relatively noiseless data. In contrast, a major challenge for achieving \u21131-reconstruction with O(n polylog n) samples is to ensure that the error remains bounded despite the presence of very noisy data due to the small sample size, and we develop suitable machinery to achieve this.\nWe now give a rough sketch of our algorithm (see Section 3) and the ideas behind its analysis (Section 4). Let P = (p1, . . . , pk), r = \u2211\nt wtp t be the expectation of the mixture, and k\u2032 =\nrank(p1 \u2212 r, . . . , pk \u2212 r). We first argue that it suffices to focus on isotropic mixtures (Lemma 3.3). Our algorithm reduces the problem to the problem of learning one-dimensional mixtures. Note that this is a special case of the general learning problem that we need to be able to solve (since we do not make any assumptions about the rank of P ). We choose k\u2032 random lines that are close to the affine hull, aff(P ), of P and \u201cproject\u201d the mixture on to these k\u2032 lines. We learn each projected mixture, which is a one-dimensional mixture-learning problem, and combine the inferred projections on these k\u2032 lines to obtain k points that are close to aff(P ). Finally, we project these k\u2032 points on to \u2206n\u22121 to obtain k distributions over [n], which we argue are close (in \u21131-distance) to p1, . . . , pk.\nVarious difficulties arise in implementing this plan. We first learn a good approximation to aff(P ) using spectral techniques and 2-snapshots. We use ideas similar to [35, 6, 34], but our challenge is to show that the covariance matrix A = \u2211\ntwt(p t\u2212r)(pt\u2212r)\u2020 can be well-approximated\nby the empirical covariance matrix with only O(n ln6 n) 2-snapshots. A random orthonormal basis of the learned affine space supplies the k\u2032 lines on which we project our mixture. Of course, we do not know P , so \u201cprojecting\u201d on to a basis vector b actually means that we project snapshots from P on to b by mapping item i to bi. For this to be meaningful, we need to ensure that if the mixture constituents are far apart in variation distance then their projections (b\u2020pt)t\u2208[k] are also well separated relative to the spread of the support {b1, . . . bn} of the one-dimensional distribution. In general, we can only claim a relative separation of \u0398 (\n1\u221a n\n) (since mint6=t\u2032 \u2016pt \u2212 pt \u2032\u20162 may be\n\u0398 ( 1\u221a n ) ). We avoid this via a careful balancing act: we prove (Lemma 4.3) that the \u2113\u221e norm of unit vectors in aff(P ) is O (\n1\u221a n\n)\n, and argue that this isotropy property suffices since b is close to\naff(P ). Finally, a key ingredient of our algorithm (see Section 5) is to show how to solve the onedimensional mixture-learning problem and learn the real projections (b\u2020pt)t\u2208[k] from the projected snapshots. This is technically the most difficult step and the one that requires aperture 2k \u2212 1 (the smallest aperture at which this is possible). We show that the projected snapshots on b yield empirical moments of a related distribution and use this to learn the projections and the mixture weights via a method of moments (see, e.g., [25, 24, 31, 10, 36, 3]). One technical difficulty is that variation distance in \u2206n\u22121 translates to transportation distance [42] in the one-dimensional projection. We use a combination of convex programming and numerical-analysis techniques to learn the projections from the empirical \u201cdirectional\u201d moments. In the process, we establish some novel properties about the moment curve\u2014an object that plays a central role in convex and polyhedral geometry [8]\u2014that may be of independent interest.\nRelated work. The past decade has witnessed tremendous progress in the theory of learning statistical mixture models. The most striking example is that of learning mixtures of high dimen-\nsional Gaussians. Starting with Dasgupta\u2019s groundbreaking paper [20], a long sequence of improvements [21, 5, 41, 32, 1, 24, 13] culminated in the recent results [31, 10, 36] that essentially resolve the problem in its general form. In this vein, other highly structured mixture models, such as mixtures of discrete product distributions [33, 26, 18, 25, 14, 16] and similar models [18, 9, 37, 32, 19, 15, 22], have been studied intensively. One important difference between this line of work and ours is that the structure of those mixtures enables learning using single-snapshot samples, whereas this is impossible in our case. Another interesting difference between our setting and the work on structured models (and this is typical of most results on PAC-style learning) is that the amount of information in each sample point is roughly in the same ballpark as the information needed to describe the model. In our setting, the amount of information in each sample point is exponentially sparser than the information needed to describe the model to good accuracy. Thus, the topic-model learning problem motivates the natural question of inference from sparse samples. This issue is also encountered in collaborative filtering; see [34] for some related theoretical problems.\nRecently and independently, [4, 3, 2] have considered much the same question as ours.1 They make certain assumptions about the mixture constituents which makes it possible to learn the mixture with constant aperture and poly(n, k) sample size (for \u21131-error). In comparison with our work, their sample bounds are attractive in terms of k but come at the expense of added assumptions (which are necessary), and have a worse dependence on n.\nThe assumptions in [4, 3, 2] impose some limitations on the applicability of their algorithms. To understand this, it is illuminating to consider the case where all the pts lie on a line-segment in \u2206n\u22121 as an illustration. This poses no problems for our algorithm: we recover the pts along with their mixture weights. However, as we show below, the algorithms in [4, 3, 2] all fail to reconstruct this mixture. Anandkumar et al. [3] solve the same problem that we consider, under the assumption that P (viewed as an n \u00d7 k matrix) has rank k. This is clearly violated here, rendering their algorithm inapplicable. The other two papers [4, 2] consider the setting where each multi-snapshot is generated from a combination of mixture constituents [39, 28]: first a convex combination \u03bb \u2208 \u2206k\u22121 is sampled from a mixture distribution T on \u2206k\u22121, then the snapshot is generated by sampling from the distribution\n\u2211k t=1 \u03bbtp t. The goal is to learn the mixture constituents and the mixture distribution. (The problem we consider is the special case where T places weight wt on the t-th vertex of \u2206k\u22121.) [4] posits a \u03c1-separability assumption on the mixture constituents, wherein each pt has a unique \u201canchor word\u201d i such that pti \u2265 \u03c1 and pt \u2032 i = 0 for every t \u2032 6= t, whereas [2] weakens this to the requirement that P has rank k. Both papers handle the case where T is the Dirichlet distribution (which gives the latent Dirichlet model [12]); [4] obtains results for other mixture distributions as well.\nIn order to apply these algorithms, we can view the input as being specified by two constituents, x and y, which are the end points of the line segment; T then places weight wt on the convex combination (\u03bbt, 1\u2212 \u03bbt)\u2020, where pt = \u03bbtx+(1\u2212\u03bbt)y. This T is far from the Dirichlet distribution, so [3] does not apply here. Suppose that x and y satisfy the \u03c1-separability condition. (Note that \u03c1 may only be O (\n1 n\n)\n, even if x and y have disjoint supports.) We can then apply the algorithm of Arora et al. [4]. But this does not recover T ; it returns the \u201ctopic correlation\u201d matrix ET [\u03bb\u03bb\u2020], which does not reconstruct the mixture (w,P ).\nThis limitation should not be surprising since [4] uses constant aperture. Indeed, [4] notes that it is impossible to reconstruct T with arbitrary accuracy (with any constant aperture) even if one knows the constituents x and y. In this context, we remark that our earlier work [40] uses the approach presented in this paper and solves the problem for arbitrary mixtures of two distributions,\n1An earlier stage of this work, including the case k = 2 as well as some other results that are not subsumed by this paper, dates to 2007. The last version of that phase has been posted since May 2008 at [40]. The extension to arbitrary k is from last year.\nyielding a crisp statement about the tradeoff between the sampling aperture and the accuracy with which T can be learnt.\nOur methods bear some resemblance with the recent independent work of Gravin et al. [27] who consider the problem of recovering the vertices of a polytope from its directional moments. [27] solves this problem for a polynomial density function assuming that exact directional moments are available; they do not perform any sensitivity analysis for measuring the error in their output if one has noisy information. In contrast, we solve this problem given only noisy empirical moment statistics and using much smaller aperture, albeit when the polytope is a subset of the (n \u2212 1)- simplex and the distribution is concentrated on its vertices.\nFinally, it is also pertinent to compare our mixture-learning problem with the problem of learning a mixture of product distributions (e.g., [25]). Multi-snapshot samples can be thought of as single-snapshot samples from the power distribution on [n]K , where K is the aperture. The product distribution literature typically deals with samples spaces that are the product of many small cardinality components, whereas our problem deals with samples spaces that are the product of few large cardinality components."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Mixture sources, snapshots, and projections", "text": "Let [n] denote {1, 2, . . . , n}, and \u2206n\u22121 denote the (n \u2212 1)-simplex {x \u2208 Rn\u22650 : \u2211 i xi = 1}. A k-mixture source (w,P ) on [n] consists of k mixture constituents P = (p1, . . . , pk), where pt has support [n] for all t \u2208 [k], along with the corresponding mixture weights w = (w1, . . . , wk) \u2208 \u2206k\u22121. An m-snapshot from (w,P ) is obtained by choosing t \u2208 [k] according to the distribution w, and then choosing i \u2208 [n] m times independently according to the distribution pt. The probability distribution on m-snapshots is thus a mixture of k power distributions on the product space [n]m. We also consider mixture sources whose constituents are distributions on R. A k-mixture source (w,P ) on R consists of k mixture constituents P = (p1, p2, . . . , pk), where each pt is a probability distribution on R, along with corresponding mixture weights w = (w1, . . . , wk) \u2208 \u2206k\u22121.\nGiven a distribution p on [n] and a vector x \u2208 Rn, we define the projection of p on x, denoted \u03c0x(p), to be the discrete distribution on R that assigns probability mass \u2211\ni:xi=\u03b2 pi to \u03b2 \u2208 R. (Thus,\n\u03c0x(p) has support {x1, . . . , xn} and E[\u03c0x(p)] = x\u2020p.) Given a k-mixture source (w,P ) on [n], we define the projected k-mixture source (w, \u03c0x(P )) on R to be the k-mixture source on R given by (\nw, (\u03c0x(p 1), . . . , \u03c0x(p k)) )\n. We also denote by (w,E[\u03c0x(P )]) the distribution that assigns probability mass wt to E[\u03c0x(p\nt)] = x\u2020pt for all t \u2208 [k]. This is an example of what we call a k-spike distribution, which is a distribution on R that assigns positive probability mass to k points in R."}, {"heading": "2.2 Transportation distance for mixtures", "text": "Let ( w, (p1, . . . , pk) ) and ( w\u0303, (p\u03031, . . . , p\u0303\u2113) )\nbe k- and \u2113- mixture sources on [n] respectively. The transportation distance (with respect to the total variation distance 12\u2016x\u2212y\u20161 on measures on \u2206n\u22121) between these two sources, denoted by Tran(w,P ; w\u0303, P\u0303 ), is the optimum value of the following linear program (LP).\nmin k \u2211\ni=1\n\u2113 \u2211\nj=1\nxij \u00b7 1\n2 \u2016pi \u2212 p\u0303j\u20161 s.t.\n\u2113 \u2211\nj=1\nxij = wi \u2200i \u2208 [k], k \u2211\ni=1\nxij = w\u0303j \u2200j \u2208 [\u2113], x \u2265 0.\nThe transportation distance Tran(w,\u03b1; w\u0303, \u03b1\u0303) between a k-spike distribution ( w,\u03b1 = (\u03b11, . . . , \u03b1k) ) and an \u2113-spike distribution ( w\u0303, \u03b1\u0303 = (\u03b1\u03031, . . . , \u03b1\u0303\u2113) )\nis defined as the optimum value of the above LP with the objective function replaced by \u2211\ni\u2208[k],j\u2208[\u2113] xij |\u03b1i \u2212 \u03b1\u0303j |."}, {"heading": "2.3 Perturbation results and operator norm of random matrices", "text": "Definition 2.1. The operator norm of A (induced by the \u21132 norm) is defined by \u2016A\u2016op = maxx 6=0 \u2016Ax\u20162 \u2016x\u20162 . The Frobenius norm of A = (Ai,j) is defined by \u2016A\u2016F = \u221a \u2211 i,j A 2 i,j .\nLemma 2.2 (Weyl; see Theorem 4.3.1 in [30]). Let A and B be n\u00d7n matrices such that \u2016A\u2212B\u2016op \u2264 \u03c1. Let \u03bb1(A) \u2265 . . . \u2265 \u03bbn(A), and \u03bb1(B) \u2265 . . . \u2265 \u03bbn(B) be the sorted list of eigenvalues of A and B respectively. Then |\u03bbi(A)\u2212 \u03bbi(B)| \u2264 \u03c1 for all i = 1, . . . , n.\nLemma 2.3. Let A,B be n \u00d7 n positive semi-definite (PSD) matrices whose nonzero eigenvalues are at least \u03b5 > 0. Let \u03a0A and \u03a0B be the projection operators onto the column spaces of A and B respectively. Let \u2016A\u2212B\u2016op \u2264 \u03c1. Then \u2016\u03a0A \u2212\u03a0B\u2016op \u2264 \u221a 4\u03c1/\u03b5.\nProof. Note that A\u03a0A = A, \u03a0 2 A = \u03a0A, B\u03a0B = B, and \u03a0 2 B = \u03a0B . Let x be a unit vector. Since \u2016(A \u2212 B)\u2016op \u2264 \u03c1 and \u03a0B is a contraction, \u2016(A \u2212 B)\u03a0Bx\u2016 \u2264 \u03c1\u2016\u03a0Bx\u2016 \u2264 \u03c1. Now note that (A \u2212 B)\u03a0Bx = A\u03a0Bx \u2212 Bx so by the triangle inequality, we have \u2016A\u03a0Bx \u2212 Ax\u2016 \u2264 2\u03c1. Now we can also write A\u03a0Bx\u2212 Ax = A(\u03a0B \u2212 I)x = A(\u03a0A\u03a0B \u2212 \u03a0A)x. Since A here is acting on a vector that has already been projected down by \u03a0A, we can conclude\n2\u03c1 \u2265 \u2016A\u03a0Bx\u2212Ax\u2016 = \u2016A(\u03a0A\u03a0B \u2212\u03a0A)x\u2016 \u2265 \u03b5\u2016(\u03a0A\u03a0B \u2212\u03a0A)x\u2016.\nThus, 2\u03c1/\u03b5 \u2265 \u2016(\u03a0A \u2212 \u03a0A\u03a0B)x\u2016. By the symmetric argument we also can write 2\u03c1/\u03b5 \u2265 \u2016(\u03a0B \u2212 \u03a0B\u03a0A)x\u2016. Adding these and applying the triangle inequality we have\n4\u03c1/\u03b5 \u2265 \u2016(\u03a0A \u2212\u03a0A\u03a0B +\u03a0B \u2212\u03a0B\u03a0A)x\u2016 = \u2016(\u03a02A \u2212\u03a0A\u03a0B \u2212\u03a0B\u03a0A +\u03a02B)x\u2016 = \u2016(\u03a0A \u2212\u03a0B)2x\u2016.\nTheorem 2.4 ([43]). For every \u00b5 > 0, there is a constant \u03ba = \u03ba(\u00b5) = O(\u00b5) > 0 such that the following holds. Let Xi,j , 1 \u2264 i \u2264 j \u2264 n be independent random variables with |Xij | \u2264 K, E[Xi,j ] = 0, and Var(Xi,j) \u2264 \u03c32 for all i, j \u2208 [n], where \u03c3 \u2265 \u03ba2n\u22121/2K ln2 n. Let A be the symmetric matrix with entries Ai,j = Xmin(i,j),max(i,j) for all i, j \u2208 [n]. Then, Pr [ \u2016A\u2016op \u2264 2\u03c3 \u221a n+ \u03ba(K\u03c3)1/2n1/4 lnn ] \u2265 1\u2212 n\u2212\u00b5."}, {"heading": "3 Our algorithm", "text": "We now describe our algorithm that uses 1-, 2-, and (2k \u2212 1)-snapshots from the mixture source (w,P ). Given a matrix Z, we use Span(Z) to denote the column space of Z. Let r =\n\u2211k t=1 wtp t\ndenote the 1-snapshot distribution of (w,P ). Let M be the n\u00d7n symmetric matrix representing the 2-snapshot distribution of (w,P ); so Mi,j is the probability of obtaining the 2-snapshot (i, j) \u2208 [n]2. Let R = rr\u2020.\nProposition 3.1. M = \u2211k t=1 wtp tpt\u2020 = R+A, where A = \u2211k t=1 wt(p t \u2212 r)(pt \u2212 r)\u2020.\nNote that M and A are both PSD. We say that (w,P ) is \u03b6-wide if (i) \u2016p \u2212 q\u20162 \u2265 \u03b6\u221an for any two distinct p, q \u2208 P ; and (ii) the smallest non-zero eigenvalue of A is at least \u03b62\u2016r\u2016\u221e \u2265 \u03b6 2\nn . We assume that wmin := mint wt > 0. Let k\n\u2032 = rank(A) \u2264 k\u22121. It is easy to estimate r using Chernoff bounds (Lemma A.1).\nLemma 3.2. For every \u00b5 \u2208 N and every \u03c3 > 0, if we use N \u2265 8(\u00b5+2) \u03c33 \u00b7n lnn independent 1-snapshots and set r\u0303i to be the frequency of i in these 1-snapshots for all i \u2208 [n], then with probability at least 1\u2212 n\u2212\u00b5 the following hold.\n(1\u2212 \u03c3)ri \u2264 r\u0303i \u2264 (1 + \u03c3)ri \u2200i with ri \u2265 \u03c3\n2n , r\u0303i \u2264 (1 + \u03c3)\u03c3/2n \u2200i with ri <\n\u03c3\n2n . (2)\nIt will be convenient in the sequel to assume that our mixture source (w,P ) is isotropic, by which we mean that 12n \u2264 ri \u2264 2n for all i \u2208 [n]; notice that this implies that pti \u2264 2wminn for all i \u2208 [n]. We show below that this can be assumed at the expense of a small additive error.\nLemma 3.3. Suppose that we can learn, with probability 1 \u2212 1\u03c9 , the constituents of an isotropic \u03b6-wide k-mixture source on [n] to within transportation distance \u01eb using N1(n; \u03b6, \u03c9, \u01eb), N2(n; \u03b6, \u03c9, \u01eb), and N2k\u22121(n; \u03b6, \u03c9, \u01eb) 1-, 2-, and (2k\u22121)-snapshots respectively. Then, we can learn, with probability 1\u2212O (\n1 \u03c9\n)\n, the constituents of an arbitrary \u03b6-wide k-mixture source (w,P ) on [n] to within transporta-\ntion distance 2\u01eb using O ( ln\u03c9 \u03c33 \u00b7 n lnn ) + 6\u03c9N1 ( n \u03c3 , \u03b6 2 , \u03c9, \u01eb ) , 6\u03c9N2 ( n \u03c3 , \u03b6 2 , \u03c9, \u01eb ) , and 6\u03c9N2k\u22121 ( n \u03c3 , \u03b6 2 , \u03c9, \u01eb ) 1-, 2-, and (2k \u2212 1)-snapshots respectively, where \u03c3 = \u01eb\u03b6232kwmin .\nProof. Given (w,P ), we first compute an estimate r\u0303 satisfying (2), where \u00b5 = 2 + ln\u03c9, using O (\nln\u03c9 \u03c33 \u00b7 n lnn\n)\n1-snapshots. We assume in the sequel that (2) holds. Consider the following modification of the mixture constituents. We eliminate items i such that r\u0303i < 2\u03c3 n . Each remaining item i is \u201csplit\u201d into ni = \u230anr\u0303i/\u03c3\u230b items, and the probability of i is split equally among its copies. The mixture weights are unchanged. From (2), we have that ri < 4\u03c3 n if i is eliminated. So the total weight of eliminated items is at most 4\u03c3. Let n\u2032 = \u2211\ni:r\u0303i\u22652\u03c3/n ni \u2264 n \u03c3 be the number of new items.\nLet P\u0302 = (p\u03021, . . . , p\u0302k) denote the modified mixture constituents, and r\u0302 denote the distribution of the modified 1-snapshots. We prove below that the modified mixture (w, P\u0302 ) is isotropic and \u03b6/2-wide.\nWe use the algorithm for isotropic mixture sources to learn (w, P\u0302 ) within transportation distance \u01eb, using the following procedure to sample m-snapshots from (w, P\u0302 ). We obtain an m-snapshot from (w,P ). We eliminate this snapshot if it includes an eliminated item; otherwise, each item i in the snapshot is replaced by one of its ni copies, chosen uniformly at random (and independently of previous such choices). From the inferred modified mixture source, we can obtain an estimate of the original mixture source by aggregating, for each inferred mixture constituent, the probabilities of the items that we split, and setting the probability of each eliminated item to 0. This degrades the quality of the solution by the weight of the eliminated items, which is at most an additive 4\u03c3 \u2264 \u01eb term in the transportation distance.\nThe probability that anm-snapshot from (w,P ) survives is at least (1\u22124\u03c3)m \u2265 12 form \u2264 2k\u22121. Therefore, with probability at least 1\u2212 13\u03c9 , we need at most 6\u03c9N m-snapshots from (w,P ) to obtain N m-snapshots from (w, P\u0302 ). (If we violate this bound, we declare failure.) Thus, we use at most the stated number of 1-, 2-, and (2k\u22121)-snapshots from (w,P ) and succeed with probability 1\u2212O (\n1 \u03c9\n)\n.\nWe conclude by showing that (w, P\u0302 ) is isotropic and \u03b6/2-wide. Let S = {i \u2208 [n] : r\u0303i < 2\u03c3/n} denote the set of eliminated items. Recall that r\u0303 satisfies (2). So we have 3132 \u2264 r\u0303iri \u2264 33 32 for every non-eliminated item. We use i\u2113, where \u2113 = 1, . . . , ni, to denote a new item obtained by splitting item i. Define ni = 0 if i is eliminated.\nThe number n\u2032 of new items is at most n\u03c3 and at least \u2211 i/\u2208S 2 3 \u00b7 nr\u0303i\u03c3 \u2265 23 \u00b7 n\u03c3 \u00b7 (1 \u2212 2\u03c3) \u2265 5n8\u03c3 .\nLet K = \u2211 i/\u2208S ri \u2265 1 \u2212 4\u03c3 \u2265 7/8. For every new item i\u2113, we have r\u0302i\u2113 \u2265 rinr\u0303i/\u03c3 \u2265 32\u03c3 33n \u2265 12n\u2032 and r\u0302i\u2113 \u2264 1K \u00b7 32 \u00b7 rinr\u0303i/\u03c3 \u2264 384\u03c3 217n \u2264 2n\u2032 . Thus, (w, P\u0302 ) is isotropic.\nNow consider the width of (w, P\u0302 ). For t = 1, . . . , k, define p\u2032t \u2208 Rn to be the vector where p\u2032ti = 0 if i \u2208 S, and p\u2032ti = pti otherwise. For any distinct t, t\u2032 \u2208 [k], we have \u2016p\u0302t \u2212 p\u0302t \u2032\u20162 \u2265 \u2016p\u0302 t\u2212p\u0302t\u2032\u20161\u221a\nn\u2032\nand\n\u2016p\u0302t \u2212 p\u0302t\u2032\u20161 = \u2016p\u2032t \u2212 p\u2032t\u2032\u20161\nK \u2265 \u2016pt \u2212 pt\u2032\u20161 \u2212\n\u2211 i\u2208S max{pti, pt \u2032 i } \u2265 \u03b6 \u2212 n \u00b7 4\u03c3 wminn \u2265 \u03b6/2.\nLet A\u0302 = \u2211k t=1 wt(p\u0302 t \u2212 r\u0302)(p\u0302t \u2212 r\u0302)\u2020, which is an n\u2032 \u00d7 n\u2032 matrix. We need to prove that the smallest non-zero eigenvalue of A\u0302 is at least \u03b6 2\n4 \u00b7 \u2016r\u0302\u2016\u221e. It will be convenient to define the following matrices. Let B \u2208 R([n]\\S)\u00d7([n]\\S) be the matrix defined by setting Bi,j = Ai,j for all i, j /\u2208 S. Define A\u2032 to be the n \u00d7 n matrix obtained by padding B with 0s: set A\u2032i,j = Ai,j = Bi,j if i, j /\u2208 S, and equal to 0 otherwise. It is easy to see that the non-zero eigenvalues of A\u2032 coincide with the non-zero eigenvalues of B. Define X \u2208 Rn\u2032\u00d7([n]\\S) as follows. Letting {i\u2113}i/\u2208S,\u2113=1,...,ni index the rows of X, we set Xi\u2113,j = 1 Kni if j = i, and 0 otherwise. Notice that A\u0302 = XBX\u2020. To see this, it is convenient to define a padded version Y \u2208 Rn\u2032\u00d7[n] of X by setting Yi\u2113,j = Xi\u2113,j if j /\u2208 S and 0 otherwise. Then, we have p\u0302t = Y pt for all t \u2208 [k], and hence, A\u0302 = Y AY \u2020 = XBX\u2020.\nNote that rank(A\u2032) \u2264 rank(A) = k\u2032. Consider A \u2212 A\u2032. Suppose i \u2208 S, so pti \u2264 4\u03c3wminn for all t \u2208 [k]. Then,\n|(A\u2212A\u2032)i,j | = |Ai,j | = |Mi,j \u2212Ri,j| \u2264 max {\nk \u2211\nt=1\nwtp t ip t j, rirj\n} \u2264 4\u03c3 wminn \u00b7 rj \u2264 4\u03c3 wminn \u00b7 \u2016r\u2016\u221e.\nHence, \u2016A\u2212A\u2032\u2016op \u2264 \u2016A\u2212A\u2032\u2016F \u2264 8\u03c3wmin \u00b7 \u2016r\u2016\u221e. By Lemma 2.2, this implies that\n\u03bbk\u2032(B) = \u03bbk\u2032(A \u2032) \u2265 \u03bbk\u2032(A)\u2212 \u2016A\u2212A\u2032\u2016op \u2265\n( \u03b62 \u2212 8\u03c3 wmin ) \u2016r\u2016\u221e \u2265 \u03b62 2 \u00b7 \u2016r\u2016\u221e.\nWe now argue that \u03bbk\u2032(XBX \u2020) \u2265 \u03bbk\u2032(B)/(maxi ni). By the Courant-Fischer theorem (see, e.g., Theorem 4.2.11 in [30]), this is equivalent to showing that there exist vectors y1, . . . , yk \u2032 \u2208 Rn\u2032 , such that for every unit vector v \u2208 Span(y1, . . . , yk\u2032), we have v\u2020(XBX\u2020)v \u2265 \u03bbk\u2032 (B)maxi ni . We know that there are vectors u1, . . . , uk \u2032 \u2208 R[n]\\S such that zBz\u2020 \u2265 \u03bbk\u2032(B)\u2016z\u20162 for every z \u2208 Span(u1, . . . , uk \u2032 ). Set yti\u2113 = u t i for every copy i\u2113 of item i \u2208 [n] \\S, and every t \u2208 [k\u2032]. Consider any v \u2208 Span(y1, . . . , yk \u2032 ). We have that z = X\u2020v \u2208 Span(u1, . . . , uk\u2032), and since vi\u2113 = zi for every copy i\u2113 of item i \u2208 [n] \\ S we have that \u2016v\u201622 \u2264 (maxi ni)\u2016z\u201622. Therefore, if v is a unit vector, we have v\u2020XBX\u2020v = z\u2020Bz \u2265 \u03bbk\u2032(B)\u2016z\u201622 \u2265 \u03bbk\u2032 (B) maxi ni .\nPutting everything together, we have that \u03bbk\u2032(A\u0302) \u2265 \u03b6 2\u2016r\u2016\u221e\n2maxi ni . Note that \u2016r\u2016\u221e \u2265 3233\u2016r\u0303\u2016\u221e and\n\u2016r\u0303\u2016\u221e maxi ni \u2265 \u03c3n \u2265 217384\u2016r\u0302\u2016\u221e. So the smallest non-zero eigenvalue of A\u0302 is \u03bbk\u2032(A\u0302) \u2265 \u03b62 4 \u2016r\u0302\u2016\u221e.\nAlgorithm overview. Our algorithm for learning an isotropic k-mixture source on [n] takes three parameters: \u03b6 \u2264 1 such that (w,P ) is \u03b6-wide, \u03c9 \u2208 N, which controls the success probability of the algorithm, and \u03b4 \u2208 (0, 1), which controls the statistical distance between the constituents of the learnt model and the constituents of the correct model. For convenience, we assume that \u03b4 is sufficiently small. The output of the algorithm is a k-mixture source (w\u0303, P\u0303 ) such that with probability 1\u2212O (\n1 \u03c9\n)\n, \u2016w\u2212 w\u0303\u2016\u221e and \u2016pt\u2212 p\u0303t\u20161 for all t \u2208 [k] tend to 0 as \u03b4 \u2192 0 (see Theorem 4.1). The algorithm (see Algorithm 1) consists of three stages. First, we reduce the dimensionality of the problem from n to k\u2032 using only 1- and 2-snapshots. By Lemma 3.2, we have an estimate r\u0303 that is component-wise close to r. Thus, R\u0303 = r\u0303r\u0303\u2020 is close in operator norm to R. So we focus on learning the column space of A for which we employ spectral techniques. Leveraging Theorem 2.4, we argue (Lemma 4.2) that by using O(n ln6 n) 2-snapshots, one can compute (with high probability) a good enough estimate M\u0303 of M , and hence obtain a PSD matrix A\u0303 such that \u2016A\u2212 A\u0303\u2016op is small.\nThe remaining task is to learn the projection of P on the affine space r\u0303 + Span(A\u0303), and the mixture weights, which then yields the desired k-mixture source (w\u0303, P\u0303 ). We divide this into two steps. We choose a random orthonormal basis {b1, . . . , bk\u2032} of Span(A\u0303). For each bj , we consider the projected k-mixture source (w, \u03c0bj (P )) on R. In Section 5, we devise a procedure to learn the corresponding k-spike distribution (w,E[\u03c0bj (P )]) using (2k\u2212 1)-snapshots from (w, \u03c0bj (P )) (which we can obtain using (2k \u2212 1)-snapshots from (w,P )). Applying this procedure (see Lemma 4.7), we obtain weights w\u0303j1, . . . , w\u0303 j k and k (distinct) values \u03b1 j 1, . . . , \u03b1 j k such that each true spike (wt, b \u2020 jp t) maps to a distinct inferred spike (w\u0303j \u03c3j(t) , \u03b1j \u03c3j (t) ).\nFinally, we match up \u03c3j and \u03c3k\u2032 for all j \u2208 [k\u2032 \u2212 1] to obtain k points in r\u0303 + Span(A\u0303) that are close to the projection of P on r\u0303 + Span(A\u0303). For every j \u2208 [k\u2032 \u2212 1], we generate a random unit \u201ctest vector\u201d zj in Span(bj , bk\u2032) and learn the projections {z\u2020jpt}t\u2208[k]. Since (w,P ) is \u03b6-wide, results about random projections and the guarantees obtained from our k-spike learning procedure imply that z\u2020j (\u03b1 j t1bj + \u03b1 k\u2032 t2bk\u2032) is close to some value in {z \u2020 jp t}t\u2208[k] iff there is some t such that \u03b1jt1 and \u03b1k \u2032 t2 are close respectively to b\u2020jpt and b \u2020 k\u2032p t (Lemma 4.8). Thus, we can use the learned projections of {z\u2020jpt}t\u2208[k] to match up {\u03b1 j t}t\u2208[k] and {\u03b1k \u2032 t }t\u2208[k].\nAlgorithm 1. Input: an isotropic \u03b6-wide k-mixture source (w,P ) on [n], and parameters \u03c9 > 1 and \u03b4 > 0. Output: a k-mixture source (w\u0303, P\u0303 ) on [n] that is \u201cclose\u201d to (w,P ).\nDefine T = 3\u03c9k4, H = 4 w2 min \u03b6 \u221a n and L = \u03b6 64\u03c91.5k4 \u221a n . We assume that \u03b4 \u2264 w\n3 min \u03b64\n229\u03c95k16 . Let \u03ba = \u03ba(2 + ln\u03c9)\nbe given by Theorem 2.4; we assume \u03ba \u2265 1 for convenience. Define c = 6400\u03ba2 w2\nmin \u03b42\n\u00b7 ln ( 1 \u03b4 ) . We assume that\nw2min \u2265 240\u03ba ln 2.5 n\u221a n .\nA1. Dimension reduction.\nA1.1 Use Lemma 3.2 with \u00b5 = 2 + ln\u03c9 and \u03c3 = \u03b448 to compute an estimate r\u0303 of r. Set R\u0303 = r\u0303r\u0303 \u2020. A1.2 Independent of all other random variables, choose a Poisson random variable N2 with expectation E[N2] = cn ln\n6 n. Choose N2 independent 2-snapshots and construct a symmetric n \u00d7 n matrix M\u0303 as follows: set M\u0303i,i = frequency of the 2-snapshot (i, i) in the sample for all i \u2208 [n], and M\u0303i,j = M\u0303j,i = half the combined frequency of 2-snapshots (i, j) and (j, i) in the sample, for all i, j \u2208 [n], i 6= j.\nA1.3 Compute the spectral decomposition M\u0303 \u2212 R\u0303 = \u2211ni=1 \u03bbiviv \u2020 i , where \u03bb1 \u2265 . . . \u2265 \u03bbn. A1.4 Set A\u0303 = \u2211\ni:\u03bbi\u2265\u03b62/2n \u03bbiviv \u2020 i . Note that A\u0303 is PSD.\nA2. Learning projections of (w, P ) on random vectors in Span(A\u0303).\nA2.1 Pick an orthonormal basis B = {b1, . . . , bk\u2032} for Span(A\u0303) uniformly at random. A2.2 Set (w\u0303j , \u03b1j) \u2190 Learn ( bj, \u03b4, 1\n6\u03c9k\n) for all j = 1, . . . , k\u2032.\nA3. Combining the projections to obtain (w\u0303, P\u0303 ).\nA3.1 Pick \u03b8 \u2208 [0, 2\u03c0] uniformly at random. A3.2 For each j = 1, . . . , k\u2032 \u2212 1, we do the following.\n\u2013 Let zj = bj cos \u03b8 + bk\u2032 sin \u03b8. \u2013 Set (w\u0302j , \u03b1\u0302j) \u2190 Learn ( zj , \u03b4, 1\n6\u03c9k\n)\n.\n\u2013 For each t1, t2 \u2208 [k], if there exists t \u2208 [k] such that \u2223 \u2223(\u03b1jt1bj+\u03b1 k\u2032 t2bk\u2032) \u2020zj\u2212 \u03b1\u0302jt \u2223\n\u2223 \u2264 ( \u221a 2+1)L/(2+\n5T ) then set \u033aj(t2) = t1.\nA3.3 Define \u033ak \u2032 (t) = t for all t \u2208 [k]. A3.4 For every t \u2208 [k]: set w\u0303t = ( \u2211k\u2032 j=1 w\u0303 j \u033aj(t) ) /k\u2032, p\u0302t = r\u0303 + \u2211k\u2032 j=1 ( \u03b1j\u033aj(t) \u2212 b \u2020 j r\u0303 ) bj , and p\u0303 t =\nargminx\u2208\u2206n\u22121 \u2016x\u2212 p\u0302t\u20161, which can be computed by solving an LP. Return ( w\u0303, P\u0303 = (p\u03031, . . . , p\u0303k) ) .\nAlgorithm Learn(v, \u03c2, \u03b5) Input: a unit vector v \u2208 Span(A\u0303), and parameters \u03c2 > 0, \u03b5 > 0. We assume that (a) |v\u2020(p\u2212 q)| \u2265 L for all distinct p, q \u2208 P ; and (b) 1024k\u03c2 < wminL16H . Output: a k-spike distribution ( w\u0304, (\u03b31, . . . , \u03b3k) ) close to (w,E[\u03c0v(P )]).\nL1. Solve the following convex program:\nmin \u2016x\u2016\u221e s.t. v\u2020x \u2265 1\u2212 4\u03b4\n\u03b62 , \u2016x\u201622 \u2264 1 (Qv)\nto obtain x\u2217; set a = x \u2217 \u2016x\u2217\u20162 . We prove in Lemma 4.4 that \u2016a\u2016\u221e \u2264 H and |a \u2020(p \u2212 q)| \u2265 L2 for all\np, q \u2208 P, p 6= q. L2. Let s = \u03c24k. Apply the procedure in Section 5 leading to Theorem 5.1 for ( w, \u03c0a/2H(P ) )\nto infer a k-spike distribution (w\u0304, \u03b2) that, with probability at least 1 \u2212 \u03b5, is within transportation distance O ( s\u2126(1/k) ) from ( w,E[\u03c0a/2H(P )] )\n. This uses a sample of (2k\u2212 1)-snapshots of size 3k24ks\u22124k ln(4k/\u03b5). L3. For every t \u2208 [k], set \u03b3t = (2H\u03b2t)(a\u2020v). Return (w\u0304, \u03b3). Remark 3.4. We cannot compute the spectral decomposition in step A1.3 exactly, or solve (Qv) exactly in step L1, since the output may be irrational. However, one can obtain a decomposition such that \u2016M\u0303 \u2212 R\u0303\u2212 \u2211n\ni=1 \u03bbiviv \u2020 i \u2016op = O ( \u03b4 n )\nand compute a 2-approximate solution to (Qv) in polytime, and this suffices: slightly modifying the constants H and c makes the entire analysis go through. We have chosen the presentation above to keep exposition simple."}, {"heading": "4 Analysis", "text": "Theorem 4.1. Algorithm 1 uses O ( ln\u03c9 \u03b43 \u00b7 n lnn ) 1-snapshots, O ( ln2 \u03c9 ln(1/\u03b4)\n\u03b42w2 min\n\u00b7 n ln6 n ) 2-snapshots,\nand O ( k24k \u03b416k2 \u00b7 ln(24\u03c9k2) ) (2k \u2212 1)-snapshots, and computes a k-mixture source (w\u0303, P\u0303 ) on [n] such that with probability 1\u2212O (\n1 \u03c9\n)\n, there is a permutation \u03c3 : [k] 7\u2192 [k] such that for all t = 1, . . . , k,\n|wt \u2212 w\u0303\u03c3(t)| = O (\u03b4\u03c91.5k5\nw2min\u03b6 2\n)\nand \u2016pt \u2212 p\u0303\u03c3(t)\u20161 = O (\n\u221a k\u03b4\nw1.5min\u03b6\n)\n.\nHence, Tran(w,P ; w\u0303, P\u0303 ) = O (\n\u221a k\u03b4\nw1.5min\u03b6\n)\n. The running time is polynomial in the sample size.\nThe roadmap of the proof is as follows. By Lemma 3.2, with probability at least 1 \u2212 1 \u03c9n2 , (\n1\u2212 \u03b448 ) ri \u2264 r\u0303i \u2264 ( 1+ \u03b448 ) ri for all i \u2208 [n]. We assume that this holds in the sequel. In Lemma 4.2, we prove that the matrix A\u0303 computed after step A1 is a good estimate of A. In Lemma 4.3, we derive some properties of the column space of A. Lemma 4.4 then uses these properties to show that algorithm Learn returns a good approximation to (w,E[\u03c0v(P )]). Claim 4.5 and Lemma 4.6 prove that the projections of the mixture constituents on the bjs and the zjs are well-separated. Combining this with Lemma 4.4, we prove in Lemma 4.7 that with suitably large probability, every true spike (wt, b \u2020 jp\nt) maps to a distinct nearby inferred spike on every bj, j \u2208 [k\u2032], and similarly every true spike (wt, z \u2020 jp\nt) maps to a distinct nearby inferred spike on every zj , j \u2208 [k\u2032 \u2212 1]. Lemma 4.8 shows that one can then match up the spikes on the different bjs. This yields k points in Span(A\u0303) that are close to the projection of P on Span(A\u0303). Finally, we argue that this can be mapped to a k-mixture source (w\u0303, P\u0303 ) that is close to (w,P ).\nLemma 4.2. With probability at least 1 \u2212 1n\u03c9 , the matrix A\u0303 computed after step A1 satisfies rank(A\u0303) = k\u2032 = rank(A) and \u2016A\u2212 A\u0303\u2016op \u2264 \u03b4n .\nProof. Recall that k\u2032 = rank(A). Let B = M\u0303 \u2212 R\u0303 = \u2211ni=1 \u03bbiviv \u2020 i , where \u03bb1 \u2265 . . . \u2265 \u03bbn. We prove below that with probability at least 1\u2212 1n\u03c9 , we have \u2016M \u2212 M\u0303\u2016op \u2264 \u03b44n and \u2016R\u2212 R\u0303\u2016op \u2264 \u03b44n . This implies that \u2016A\u2212B\u2016op \u2264 \u2016M\u2212M\u0303\u2016op+\u2016R\u2212R\u0303\u2016op \u2264 \u03b42n . Hence, by Lemma 2.2, it follows that by the \u03b6-wide assumption, \u03bbk\u2032 \u2265 \u03b6 2 n \u2212 \u03b42n \u2265 3\u03b62 4n , and |\u03bbi| \u2264 \u03b42n \u2264 \u03b62 4n for all i > k \u2032. Thus, we include exactly k\u2032 eigenvectors when defining A\u0303, so rank(A\u0303) = k\u2032. Since A\u0303 is the closest rank-k\u2032 approximation in operator norm to B, we have \u2016A\u2212 A\u0303\u2016op \u2264 \u2016A\u2212B\u2016op + \u2016B \u2212 A\u0303\u2016op \u2264 2\u2016A\u2212B\u2016op \u2264 \u03b4n .\nIt is easy to see that |R\u0303i,j \u2212 Ri,j | \u2264 3\u03c3ri,j, where \u03c3 = \u03b4/48, and so \u2016R \u2212 R\u0303\u2016op \u2264 \u2016R \u2212 R\u0303\u2016F \u2264 \u03b4 4n . Bounding \u2016M \u2212 M\u0303\u2016op is more challenging. We carefully define a matrix whose entries are independent random variables with bounded variance, and then apply Theorem 2.4.\nNote that Mi,j \u2264 min { 2 n , 4 wminn2 } due to isotropy. Let K = 4 ln(1/\u03b4)\u03b4 and K \u2032 = 5 ln(1/\u03b4)\u03b4 . Let\nD = N2 \u00b7 ( M\u0303 \u2212M ) . Let X\u2113i,i = 1 if the \u2113-th snapshot is (i, i), for i \u2208 [n], and for i, j \u2208 [n], i 6= j, let X\u2113i,j = X \u2113 j,i = 1 2 if the \u2113-th 2-snapshot is (i, j) or (j, i), and 0 otherwise. Let Y \u2113 i,j = X \u2113 i,j \u2212Mi,j = X\u2113i,j \u2212 E[X\u2113i,j ]; so Di,j = \u2211N2 \u2113=1 Y \u2113 i,j for all i, j \u2208 [n]. We have \u03c32(n2) := Var[Di,j |N2 = n2] = n2Var[X 1 i,j ] \u2264 n2 E[(X1i,j)2] \u2264 n2Mi,j. For n2 \u2264 2cn ln6 n, we have \u03c32(n2) \u2264 8c ln 6 n w2\nmin n\n\u2264 lnn ln(1/\u03b4) \u03b42\n(since w4min \u2265 57600\u03ba 2 ln5 n n ). So by Bernstein\u2019s inequality (Lemma A.2),\nPr[|Di,j | > K lnn|N2 = n2] \u2264 2 exp ( \u2212 K 2 ln2 n\n2 ( \u03c32(n2) +K lnn/3 )\n)\n\u2264 2max { exp (\n\u2212K2 ln2 n 4\u03c32(n2) ) , exp ( \u22123K lnn4 )\n} \u2264 2\u03b4 n3 .\nSince Pr[N2 > 2c ln 6 n] \u2264 n\u22123, we can say that with probability at least 1 \u2212 2n\u22122, we have |Di,j | \u2264 K lnn for every i, j \u2208 [n] and N2 \u2264 2c ln6 n. Define a matrix D\u2032 by putting, for every i, j \u2208 [n], D\u2032i,j = sign(Di,j) \u00b7min { |Di,j |,K lnn } . Put D\u2032\u2032 = D\u2032\u2212E[D\u2032]. Clearly, E[D\u2032\u2032i,j ] = 0 for every i, j \u2208 [n]. We prove below that \u2223 \u2223E[D\u2032i,j ] \u2223 \u2223 \u2264 3\u03b4c ln6 n n2\n\u2264 lnn ln(1/\u03b4)\n\u03b4 ; therefore, |D\u2032\u2032i,j | \u2264 K \u2032 lnn. The entries of D are independent random variables as N2 is a Poisson random variable; hence, the entries of D\u2032\u2032 are also independent random variables. Also Var[D\u2032\u2032i,j] \u2264 Var[Di,j ] since censoring a random variable to an interval can only reduce the variance. Note that Di,j = \u2211N2 \u2113=1 Y \u2113 i,j follows the compound Poisson distribution. So we have\nVar[Di,j ] = E[N2] \u00b7 E[(Y 1i,j)2] = E[N2] \u00b7Var[X1i,j] \u2264 E[N2]Mi,j \u2264 4c ln6 n w2min \u00b7 n \u2264 c\u0302 2K \u20322 ln6 n n\nwhere c\u0302 = max { 2\n\u221a c\nwminK \u2032 , \u03ba2\n}\n. Thus, by Theorem 2.4, the constant \u03ba = \u03ba(2 + ln\u03c9) > 0 is such that\nwith probability at least 1\u2212 1 n2\u03c9\n\u2016D\u2032\u2032\u2016op \u2264 2 \u00b7 c\u0302K \u2032 ln3 n\u221a n \u00b7 \u221an+ \u03ba\n\u221a\nK \u2032 lnn \u00b7 c\u0302K \u2032 ln3 n\u221a n \u00b7 4\u221an \u00b7 lnn \u2264 ( 2K \u2032c\u0302+ \u03baK \u2032 \u221a c\u0302 ) ln3 n. (3)\nWe have Pr [ N2 \u2265 12 E[N2] ] \u2265 1\u2212 n\u22122, Thus, with probability at least 1 \u2212 1n\u03c9 , we have that N2 \u2265 1 2 E[N2], D\n\u2032 = D, and \u2016D\u2032\u2032\u2016op is bounded by (3). We show below that 2\u2016E[D\u2032]\u2016op/E[N2] \u2264 6\u03b4n\u22122 \u2264 \u03b4/20n. One can verify that 4K \u2032c\u0302/c \u2264 \u03b4/10 and 2\u03baK \u2032 \u221a c\u0302/c \u2264 \u03b4/10. Therefore, with probability at least 1\u2212 1n\u03c9 , we have that \u2016M \u2212 M\u0303\u2016op = 1N2 \u00b7 \u2016D\u2016op \u2264 2 E[N2] \u00b7 (\u2016D\u2032\u2032\u2016op + \u2016E[D\u2032]\u2016op) \u2264 \u03b44n .\nFinally, we bound \u2016E[D\u2032]\u2016op. We have \u2016E[D\u2032]\u2016op \u2264 \u2016E[D\u2032]\u2016F \u2264 n \u00b7 maxi,j \u2223 \u2223E[D\u2032i,j ] \u2223 \u2223. Let\n\u00b5 = cn ln6 n = E[N2]. Fix any i, j. We have \u2223 \u2223E[D\u2032i,j ] \u2223 \u2223 = \u2223 \u2223E[D\u2032i,j \u2212 Di,j ] \u2223 \u2223 \u2264 E [ |D\u2032i,j \u2212 Di,j| ] . For\nany n2 \u2264 2 ln(1/\u03b4)\u00b5, we have Var[Di,j |N2 = n2] \u2264 n2Mi,j \u2264 8c ln(1/\u03b4) ln 6 n\nw2 min\nn . So by Bernstein\u2019s\ninequality, we have that Pr[|Di,j | > K lnn|N2 \u2264 2 ln(1/\u03b4)\u00b5] < 2\u03b4n\u22123. Also, |D\u2032i,j \u2212 Di,j | \u2264 N2 always. Therefore,\nE [ |D\u2032i,j \u2212Di,j| \u2223 \u2223N2 = n2 ]\n\u2264 { 2\u03b4n\u22123n2 if n2 \u2264 2 ln ( 1 \u03b4 ) \u00b5;\nn2 otherwise\nand E [ |D\u2032i,j \u2212 Di,j | ] \u2264 \u00b5 \u2212 Pr[N2 \u2264 2 ln(1/\u03b4)\u00b5] E[N2|N2 \u2264 2 ln(1/\u03b4)\u00b5](1 \u2212 2\u03b4n\u22123). Since N2 is Poisson distributed, we have\nPr[N2 \u2264 2 ln(1/\u03b4)\u00b5] E[N2|N2 \u2264 2 ln(1/\u03b4)\u00b5] = \u230a2 ln(1/\u03b4)\u00b5\u230b \u2211\n\u2113=0\n\u2113 \u00b7 \u00b5 \u2113e\u2212\u00b5\n\u2113! = \u00b5\n\u230a2 ln(1/\u03b4)\u00b5\u230b\u22121 \u2211\n\u2113=0\n\u00b5\u2113e\u2212\u00b5\n\u2113!\n\u2265 \u00b5Pr[N2 \u2264 ln(1/\u03b4)\u00b5] \u2265 \u00b5(1\u2212 \u03b4n\u22123).\nThus, E [ |D\u2032i,j\u2212Di,j| ] \u2264 \u00b5\u2212\u00b5(1\u2212\u03b4n\u22123)(1\u22122\u03b4n\u22123) \u2264 3\u03b4n\u22123\u00b5, and 2\u2016E[D\u2032]\u2016op/E[N2] \u2264 6\u03b4n\u22122.\nWe assume in the sequel that the high-probability event stated in Lemma 4.2 happens. Thus,\nLemma 2.3 implies that \u2016\u03a0A \u2212\u03a0A\u0303\u2016op \u2264 2 \u221a \u03b4 \u03b6 .\nLemma 4.3. For every unit vector b \u2208 Span(A), \u2016b\u2016\u221e \u2264 2w2 min \u03b6 \u221a n .\nProof. Recall that A = \u2211k t=1 wt(p t \u2212 r)(pt \u2212 r)\u2020, and the smallest non-zero eigenvalue of A is at least \u03b62/n. Note that Span(A) = Span{p1 \u2212 r, . . . , pk \u2212 r}. Let Z = conv(P ). If r + b \u2208 Z, then \u2016r + b\u2016\u221e \u2264 2wminn , r + b \u2265 0, and \u2016r\u2016\u221e \u2264 2 n imply that \u2016b\u2016\u221e \u2264 2wminn . Otherwise, let the line segment [r, r + b] intersect the boundary of Z at some point b\u2032. We show that \u2016r \u2212 b\u2032\u201622 \u2265 \u03b62w2 min n . The lemma then follows since b = (b\u2032 \u2212 r)/\u2016b\u2032 \u2212 r\u20162 and so \u2016b\u2016\u221e = \u2016b \u2032\u2212r\u2016\u221e\n\u2016b\u2032\u2212r\u20162 \u2264 2\nw2 min\n\u03b6 \u221a n .\nLet S be a facet of Z such that b\u2032 \u2208 S, r /\u2208 S (note that r is in the strict interior of P ). Since Z \u2286 Span(A), one can find a unit vector v \u2208 Span(A) such that S is exactly the set of points that minimize v\u2020x over x \u2208 Z. Let dL = v\u2020r \u2212minx\u2208Z v\u2020x = v\u2020(r \u2212 b\u2032). We lower bound \u2016r \u2212 b\u2032\u20162 by dL. Note that dL > 0. Clearly, v\u2020(pt \u2212 r) \u2265 \u2212dL for all t \u2208 [k]. Projecting P onto v, we have that (a)\n\u2211k t=1 wtv \u2020(pt \u2212 r) = 0; and (b) vTAv = \u2211kt=1 wt ( v\u2020(pt \u2212 r) )2 \u2265 \u03b62n since\nv \u2208 Span(A) and (w,P ) is \u03b6-wide. Let WL = \u2211 t:v\u2020(pt\u2212r)\u22640 wt, let WR = 1 \u2212WL \u2265 wmin, and let dR = maxt{v\u2020(pt \u2212 r)}. Then, 0 = \u2211k t=1wtv\n\u2020(pt \u2212 r) \u2265 WL(\u2212dL) + wmindR, so dR \u2264 dL \u00b7 WLwmin , Also \u03b6 2\nn \u2264 \u2211k t=1 wt ( v\u2020(pt \u2212 r) )2 \u2264 WL \u00b7 d2L + WR \u00b7 d2R \u2264 WL \u00b7 d2L + WR \u00b7 d2L \u00b7 W 2L w2\nmin\n\u2264 d 2 L\nw2 min\n. So,\nd2L \u2265 \u03b62w2 min n .\nLemma 4.4. If the assumptions stated in Algorithm Learn are satisfied, then: (i) the vector a computed in Learn satisfies \u2016a\u2016\u221e \u2264 H, and |a\u2020(p\u2212q)| \u2265 L/2 for every two mixture constituents p, q \u2208 P ; (ii) with probability at least 1\u2212 \u03b5, the output (w\u0304, \u03b3) of Learn satisfies the following: there is a permutation \u03c3 : [k] 7\u2192 [k] such that for all t = 1, . . . , k,\n|wt \u2212 w\u0304\u03c3(t)| = O (\u03c2\u03c91.5k5\nw2min\u03b6 2\n) , |v\u2020pt \u2212 \u03b3\u03c3(t)| \u2264 2048kH\u03c2\nwmin +\n8 \u221a 2\u03b4\nwmin\u03b6 \u221a n \u2264 2048kH\u03c2 wmin + L 8T .\nProof. We have v\u2020\u03a0A(v) = 1 \u2212 \u2016v \u2212 \u03a0A(v)\u201622 = 1 \u2212 \u2016(\u03a0A\u0303 \u2212 \u03a0A)v\u201622 \u2265 1 \u2212 4\u03b4\u03b62 . Thus, \u03a0A(v) is feasible to (Qv), and since \u2016\u03a0A(v)\u20162 \u2264 1, by Lemma 4.3, the optimal solution x\u2217 to (Qv) satisfies\n\u2016x\u2217\u2016\u221e \u2264 2\u2016\u03a0A(v)\u2016\u221e \u2264 H/2. Also \u2016x\u2217\u201622 \u2265 v\u2020x\u2217 \u2265 1 \u2212 4\u03b4\u03b62 \u2265 14 , so \u2016a\u2016\u221e \u2264 H. Note that \u2016v \u2212 a\u201622 = 2(1\u2212 v\u2020a) \u2264 2(1\u2212 v\u2020x\u2217) \u2264 8\u03b4\u03b62 . It follows that for any two mixture constituents p, q, we have\n|a\u2020(p \u2212 q)| \u2265 |v\u2020(p \u2212 q)| \u2212 |(v \u2212 a)\u2020(p\u2212 q)| \u2265 |v\u2020(p\u2212 q)| \u2212 2 \u221a 2\u03b4\n\u03b6 \u2016p \u2212 q\u20162\n\u2265 |v\u2020(p\u2212 q)| \u2212 8 \u221a 2\u03b4\nwmin\u03b6 \u221a n \u2265 |v\u2020(p\u2212 q)| \u2212 L 2 \u2265 L 2 .\nThis proves part (i). For part (ii), we note that any two spikes in the k-spike mixture (\nw,E[\u03c0a/2H (P )] ) are separated by a distance of at least L/4H. Since s < L/4H, Theorem 5.1 guarantees that with a sample of (2k \u2212 1)-snapshots of size 3k24ks\u22124k log(4k/\u03b5), with probability at least 1 \u2212 \u03b5, the learned k-spike distribution (w\u0304, \u03b2) satisfies Tran ( w,E[\u03c0a/2H (P )]; w\u0304, \u03b2 )\n\u2264 1024ks1/(4k) = 1024k\u03c2 < Lwmin8H . Notice that this implies that there is a permutation \u03c3 : [k] 7\u2192 [k] such that \u2200t = 1, . . . , k:\n|(a/2H)\u2020pt \u2212 \u03b2\u03c3(t)| \u2264 1024k\u03c2\nwmin <\nL\n8H , |wt \u2212 w\u0304\u03c3(t)| = O\n( k\u03c2\nL/8H\n) = O (\u03c2\u03c91.5k5\nw2min\u03b6 2\n)\n. (4)\nFix some t \u2208 [k]. Let t\u2032 = \u03c3(t). From (4), we know that |a\u2020pt \u2212 2H \u00b7 \u03b2t\u2032 | = 2048kH\u03c2wmin . We bound |v\u2020pt\u2212a\u2020pt| and |2H\u03b2t\u2032 \u2212 \u03b3t\u2032 |, which together with the above will complete the proof of the lemma. We have |(v \u2212 a)\u2020pt| \u2264 \u2016v \u2212 a\u20162\u2016pt\u20162 \u2264 4 \u221a 2\u03b4\nwmin\u03b6 \u221a n . Since \u03b3t\u2032 = (2H\u03b2t\u2032)a \u2020v and |\u03b2t\u2032 | \u2264 12 , we have |2H\u03b2t\u2032 \u2212\u03b3t\u2032 | \u2264 H\u00b74\u03b4\u03b62 \u2264 16\u03b4w2\nmin \u03b63\n\u221a n . It follows that |v\u2020pt\u2212\u03b3t\u2032 | \u2264 2048kH\u03c2wmin +\n8 \u221a 2\u03b4\nwmin\u03b6 \u221a n \u2264 2048kH\u03c2wmin + L 8T .\nClaim 4.5. Let Z be a random unit vector in Span(A\u0303) and v \u2208 Span(A\u0303). Pr [ |Z\u2020v| < \u2016v\u2016232\u03c91.5k4 ]\n< 1\n3\u03c9k\u2032k2 .\nProof. One way of choosing the random unit vector Z is as follows. Fix an orthonormal basis {u1, . . . , uk\u2032} for Span(A\u0303). We choose independent N(0, 1) random variables Xi for i \u2208 [k\u2032]. Define C = \u2211k\u2032\ni=1Xiui and set Z = C/\u2016C\u20162. Set a1 = \u03c032\u03c92k\u20322k4 and a2 = 2 + 4 ln(12\u03c9k\u2032k2) k\u2032 \u2264 96\u03c9k. Note that C\u2020v/\u2016v\u20162 is distributed as N(0, 1). Therefore, Pr [ |C\u2020v| \u2264 \u2016v\u20162 \u221a a1 ] \u2264 \u221a 2a1 \u03c0 \u2264\n1 4\u03c9k\u2032k2 . Also, \u2016C\u201622 = \u2211k\u2032 i=1 X 2 i follows the \u03c7 2 k\u2032 distribution. So\nPr[\u2016C\u201622 > a2k\u2032] < ( a2e 1\u2212a2 )k\u2032/2 < exp ( (1\u2212 a2/2)k\u2032/2 ) < 1\n12\u03c9k\u2032k2 .\nObserve that \u221a\na1 a2k\u2032 \u2265 1 32\u03c91.5k4 . So if the \u201cbad\u201d event stated in the lemma happens, then |C\u2020v| \u2264 \u2016v\u20162 \u221a a1 or \u2016C\u201622 \u2265 a2k\u2032 happens; the probability of this is at most 13\u03c9k\u2032k2 .\nLemma 4.6. With probability at least 1\u2212 13\u03c9 , for every pair p, q \u2208 P , we have (i) |b \u2020 j(p \u2212 q)| \u2265 L for every j \u2208 [k\u2032] and (ii) |z\u2020j (p\u2212 q)| \u2265 L for every j \u2208 [k\u2032 \u2212 1].\nProof. Define p\u0303 = \u03a0A\u0303(p) for a mixture constituent p. Clearly, for any v \u2208 Span(A\u0303), v\u2020p\u0303 = v\u2020p. Recall that \u2016\u03a0A \u2212\u03a0A\u0303| \u2264 2 \u221a \u03b4\n\u03b6 . So for every p, q \u2208 P , \u2016p\u0303\u2212 q\u0303\u201622 \u2265 \u2016p\u2212 q\u201622 \u2212\u2016(\u03a0A \u2212\u03a0A\u0303)(p\u2212 q)\u201622 \u2265 \u2016p \u2212 q\u20162/4; hence, \u2016p\u0303 \u2212 q\u0303\u20162 \u2265 \u03b62\u221an . Notice that the zj vectors are also random unit vectors in Span(A\u0303). Applying Claim 4.5 to each event involving one of the {bj}j\u2208[k\u2032], {zj}j\u2208[k\u2032\u22121] random unit vectors, and one of the\n(k 2 ) vectors \u2016p\u0303 \u2212 q\u0303\u2016 for p\u0303, q\u0303 \u2208 \u03a0A\u0303(P ), and taking the union bound over the at most k\u2032k2 such events completes the proof.\nLemma 4.7. With probability at least 1 \u2212 23\u03c9 , the k-spike distributions obtained in steps A2 and A3 satisfy: (i) For every j \u2208 [k\u2032], there is a permutation \u03c3j : [k] 7\u2192 [k] such that for all t \u2208 [k],\n|wt \u2212 w\u0303j\u03c3j(t)| = O (\u03b4\u03c91.5k5\nw2min\u03b6 2\n)\n, |b\u2020jpt \u2212 \u03b1 j \u03c3j(t) | = O (\n\u221a \u03b4\nw1.5min\u03b6 \u221a n\n) and is at most L\n2 + 5T .\nHence, |\u03b1jt1 \u2212 \u03b1 j t2 | \u2265 L\u2212 2L2+5T = L1+0.4/T for all distinct t1, t2 \u2208 [k]. (ii) For every j \u2208 [k\u2032 \u2212 1], for every t \u2208 [k], there is a distinct t\u2032 such that\n|wt \u2212 w\u0302jt\u2032 | = O (\u03b4\u03c91.5k5\nw2min\u03b6 2\n)\n, |z\u2020jpt \u2212 \u03b1\u0302 j t\u2032 | = O\n(\n\u221a \u03b4\nw1.5min\u03b6 \u221a n\n) and is at most L\n2 + 5T .\nProof. Assume that the event stated in Lemma 4.6 happens. Then the inputs to Learn in steps A2 and A3 are \u201cvalid\u201d, i.e., satisfy the assumptions stated in Algorithm Learn. Plug in \u03c2 = \u03b4 and \u03b5 = 16\u03c9k in Lemma 4.4. Taking the union bound over all the bjs and the zjs, we obtain that the probability that Learn fails on some input, when all the bjs and zjs are valid is at most 1 3\u03c9 . The lemma follows from Lemma 4.4 by noting that 2048kH\u03b4wmin = O ( \u221a \u03b4 w1.5min\u03b6 \u221a n ) and is at most L24T , and L/24T + L/8T \u2264 L/(2 + 5T ).\nLemma 4.8. With probability at least 1\u2212 1\u03c9 , for every j = 1, . . . , k\u2032\u22121 \u033aj is a well-defined function and \u033aj(\u03c3k \u2032 (t)) = \u03c3j(t) for every t \u2208 [k].\nProof. Lemma 4.8 Assume that the events in Lemmas 4.6 and 4.7 occur. Fix j \u2208 [k\u2032 \u2212 1]. We call a point \u03b1jt1bj + \u03b1 k\u2032 t2bk\u2032 a grid-j point. Call this grid point \u201cgenuine\u201d if there exists t \u2208 [k] such that \u03c3j(t) = t1 and \u03c3 k\u2032(t) = t2, and \u201cfake\u201d otherwise. The distance between any two grid-j points is at least L/(1 + 0.4/T ) (by Lemma 4.7). So the probability there is a pair of genuine and fake grid-j points whose projections on zj are less than L/(T + 0.4) away is at most k 3 \u00b7 2\u03c0 arcsin ( 1 T )\n\u2264 k3 \u00b7 2\u03c0 \u00b7 65T \u2264 13\u03c9k . Therefore, with probability at least 1\u2212\u03c9, the events in Lemma 4.6 and Lemma 4.7 happen, and for all j \u2208 [k\u2032 \u2212 1], every pair of genuine and fake grid-j points project to points on zj that are at least L/(T + 0.4) apart. We condition on this in the sequel.\nNow fix j \u2208 [k\u2032 \u2212 1] and consider any pair t1, t2 \u2208 [k]2. Let g be the grid-j point bj\u03b1jt1 + bk\u2032\u03b1k \u2032\nt2 We show that \u033aj(t2) = t1 iff g is a genuine grid-j point. If g is genuine, let t be such that \u03c3j(t) = t1, \u03c3 k\u2032(t) = t2. Let p \u2032 be the projection of pt on Span(bj , bk\u2032). By Lemma 4.7, we have that \u2016p\u2032 \u2212 g\u20162 \u2264 \u221a 2L 2+5T . Also, there exists t \u2032 \u2208 [k] such that |\u03b1\u0302jt\u2032 \u2212 z \u2020 jp t| \u2264 L2+5T . Since z \u2020 jp \u2032 = z\u2020jp t, this implies that |z\u2020jg \u2212 \u03b1\u0302 j t\u2032 | \u2264 |\u03b1\u0302 j t\u2032 \u2212 z \u2020 jp t|+ |z\u2020j (p\u2032 \u2212 g)| \u2264 ( \u221a 2+1)L 2+5T and so \u033a j(t2) = t1.\nNow suppose g is fake but |z\u2020jg \u2212 \u03b1\u0302 j t\u2032 | \u2264 (\n\u221a 2 + 1)L/(2 + 5T ) for some t\u2032 \u2208 [k]. Let t \u2208 [k] be\nsuch that |\u03b1\u0302jt\u2032 \u2212 z \u2020 jp t| \u2264 L2+5T . Let g\u2032 be the genuine grid point bj\u03b1 j \u03c3j(t) + bk\u2032\u03b1 k\u2032 \u03c3k\u2032 (t) . So |z\u2020jg\u2032\u2212 \u03b1\u0302 j t\u2032 | \u2264 ( \u221a 2 + 1)L/(2 + 5T ), and hence |z\u2020j (g \u2212 g\u2032)| \u2264 2( \u221a 2+1)L 2+5T < L 0.4+T which is a contradiction.\nProof of Theorem 4.1. We condition on the fact that all the \u201cgood\u201d events stated in Lemmas 3.2, 4.2, 4.6, 4.7, and 4.8 happen. The probability of success is thus 1\u2212O (\n1 \u03c9\n)\n. The sample-size bounds\nfollow from the description of the algorithm. For notational simplicity, let \u03c3k \u2032 be the identity permutation, i.e., \u03c3k \u2032 (t) = t for all t \u2208 [k]. So by Lemma 4.8, we have \u033aj(t) = \u03c3j(t) for every j \u2208 [k\u2032 \u2212 1] and t \u2208 k.\nFor t = 1, 2, . . . , k, define p\u0304t = r\u0303 + \u2211k\u2032 j=1 b \u2020 j(p t \u2212 r\u0303)bj = r\u0303 +\u03a0A\u0303(pt \u2212 r\u0303). Fix t \u2208 [k]. Then\n\u2016pt \u2212 p\u0303t\u20161 \u2264 \u2016pt \u2212 p\u0302t\u20161 + \u2016p\u0302t \u2212 p\u0303t\u20161 \u2264 2\u2016pt \u2212 p\u0302t\u20161 \u2264 2 ( \u2016pt \u2212 p\u0304t\u20161 + \u2016p\u0304t \u2212 p\u0302t\u20161 ) .\nWe have \u2016pt \u2212 p\u0304t\u20162 = \u2225 \u2225 \u2225r \u2212 r\u0303 + (pt \u2212 r)\u2212 k\u2032 \u2211\nj=1\nb\u2020j(p t \u2212 r\u0303)bj\n\u2225 \u2225 \u2225\n2\n= \u2225 \u2225r \u2212 r\u0303 +\u03a0A\u0303(r\u0303 \u2212 r) + (pt \u2212 r)\u2212\u03a0A\u0303(pt \u2212 r) \u2225 \u2225 2\n\u2264 2 \u00b7 \u2016r \u2212 r\u0303\u20162 + \u00b7\u2016\u03a0A \u2212\u03a0A\u0303\u2016op \u00b7 \u2016pt \u2212 r\u20162 \u2264 \u03b4\n12 \u221a n +\n8 \u221a 2\u03b4\nwmin\u03b6 \u221a n .\nAlso \u2016p\u0304t \u2212 p\u0302t\u20162 \u2264 \u2225 \u2225 \u2225\nk\u2032 \u2211\nj=1\n( b\u2020jp t \u2212 \u03b1j \u03c3j(t) ) bj\n\u2225 \u2225 \u2225\n2 = O\n( \u221a k\u03b4\nw1.5min\u03b6 \u221a n\n)\nwhere the last equality follows from Lemma 4.7. Thus, \u2016pt \u2212 p\u0303t\u20161 = O (\n\u221a k\u03b4 w1.5 min \u03b6 ) . Also, we have\n|wt \u2212 w\u0303t| = O ( \u03b4\u03c91.5k5\nw2 min \u03b62\n)\nby Lemma 4.7. Finally, note that\nTran(w,P ; w\u0303, P\u0303 ) \u2264 1 2 (\nk \u2211\nt=1\nmin{wt, w\u0303t}max t \u2016pt \u2212 p\u0303t\u20161 + \u2016w \u2212 w\u0303\u20161 max t6=t\u2032\n\u2016pt \u2212 p\u0303t\u2032\u20161 )\n\u2264 1 2 (\nk \u2211\nt=1\nmin{wt, w\u0303t}max t \u2016pt \u2212 p\u0303t\u20161 + \u2016w \u2212 w\u0303\u20161 ( max t6=t\u2032 \u2016pt \u2212 pt\u2032\u20161 +max t \u2016pt \u2212 p\u0303t\u20161 )\n)\n\u2264 max t\n\u2016pt \u2212 p\u0303t\u20161 + \u2016w \u2212 w\u0303\u20161 \u00b7 2\nwmin = O\n( \u221a k\u03b4\nw1.5min\u03b6\n)\n.\nThe running time is dominated by the time required to compute the spectral decomposition in step A1.3, the calls to Learn in steps A2.2 and A3.2, and the time to compute p\u0303t in step A3.4. The other steps are clearly polytime. As noted in Remark 3.4, it suffices to compute a decomposition such that \u2016M\u0303 \u2212 R\u0303\u2212\u2211ni=1 \u03bbiviv \u2020 i \u2016 = O ( \u03b4 n ) ; this takes time poly ( n, ln(n/\u03b4) )\n. The LP used in step A3.4 is of polynomial size, and hence can be solved in polytime. Procedure Learn requires solving (Qv); again, an approximate solution suffices and can be computed in polytime. Theorem 5.1 proves that the one-dimensional learning problem can be solved in polytime; hence, Learn takes polytime."}, {"heading": "5 The one-dimensional problem: learning mixture sources on [0,1]", "text": "In this section, we supply the key subroutine called upon in step L2 of Algorithm Learn, which will complete the description of Algorithm 1. We are given a k-mixture source ( w, \u03c0x(P ) ) on [ \u221212 , 12 ]\n. (Recall that Learn invokes the procedure for the mixture ( w, \u03c0a/2H (P ) )\nwhere \u2016a\u2016\u221e \u2264 H.) It is clear that we cannot in general reconstruct this mixture source with an aperture size that is independent of n, let alone aperture 2k\u22121. However, our goal is somewhat different and more modest. We seek to reconstruct the k-spike distribution ( w,E[\u03c0x(P )] )\n, and we show that this can be achieved with aperture 2k\u2212 1 (which is the smallest aperture at which this is information-theoretically possible).\nIt is easy to obtain a (2k \u2212 1)-snapshot from (w, \u03c0x(P )) given a (2k \u2212 1)-snapshot from (w,P ) by simply replacing each item i \u2208 [n] that appears in the snapshot by xi. We will assume in the sequel that every constituent \u03c0x(p\nt) is supported on [0, 1], which is simply a translation by 12 . To simplify notation, we use \u03b8 = ( \u03d1, (q1, . . . , qk) )\nto denote the k-mixture source on [0, 1], and (\n\u03d1, \u03b1 = (\u03b11, . . . , \u03b1k) ) to denote the corresponding k-spike distribution, where \u03b1i \u2208 [0, 1] is the\nexpectation of qi for all i \u2208 [k]. We equivalently view (\u03d1, \u03b1) as a k-mixture source ( \u03d1, (f1, . . . , fk) ) on {0, 1}: each f i is a \u201ccoin\u201d whose bias is f i1 = \u03b1i. In Section 5.1, we describe how to learn such a binary mixture source from its (2k \u2212 1)-snapshots (see Algorithm 2 and Theorem 5.3). Thus, if we can obtain (2k \u2212 1)-snapshots from the binary source ( \u03d1, (f1, . . . , fk) )\n(although our input is \u03b8) then Theorem 5.3 would yield the the desired result. We show that this is indeed possible, and hence, obtain the following result (whose proof appears at the end of Section 5.1).\nTheorem 5.1. Let \u03b8 = ( \u03d1, (q1, . . . , qk) )\nbe a k-mixture source on [0, 1], and (\u03d1, \u03b1) be the corresponding k-spike distribution. Let \u03c4 = minj 6=j\u2032 |\u03b1j \u2212 \u03b1j\u2032 |. For any s < \u03c4 and \u03c8 > 0, using 3k24ks\u22124k ln(4k/\u03c8) (2k \u2212 1)-snapshots from source \u03b8, one can compute in polytime a k-spike distribution (\u03d1\u0303, \u03b1\u0303) on [0, 1] such that Tran(\u03d1, \u03b1; \u03d1\u0303, \u03b1\u0303) \u2264 1024ks1/(4k) with probability at least 1\u2212 \u03c8.\n5.1 Learning a binary k-mixture source\nRecall that ( \u03d1, (f1, . . . , fk) ) denotes the binary k-mixture source, and \u03b1i = f i 1 is the bias of the i-th \u201ccoin\u201d. We can collect from each (2k\u22121)-snapshot a random variable 0 \u2264 X \u2264 2k\u22121 denoting the number of times the outcome \u201c1\u201d occurs in the snapshot. Thus, Pr[X = i] =\n(2k\u22121 i ) \u2211k j=1 \u03d1j\u03b1 i j(1\u2212\n\u03b1j) 2k\u22121\u2212i. Our objective is to use these statistics to reconstruct, in transportation distance (see Section 2.2), the binary source (i.e., the mixture weights and the k biases). Now consider the equivalent k-spike distribution (\u03d1, \u03b1). The i-th moment, and (what we call) the i-th normalized binomial moment (NBM) of this distribution are\ngi(\u03d1, \u03b1) = k \u2211\nj=1\n\u03d1j\u03b1 i j , \u03bdi(\u03d1, \u03b1) =\nk \u2211\nj=1\n\u03d1j\u03b1 i j(1\u2212 \u03b1j)2k\u22121\u2212i (5)\nUp to the factors (2k\u22121\ni\n)\nthe NBMs are precisely the statistics of the random variable X and so our objective in this section can be restated as: use the empirical NBMs to reconstruct the k-spike distribution (\u03d1, \u03b1).\nLet g(\u03d1, \u03b1) = ( gi(\u03d1, \u03b1) )2k\u22121 i=0 and \u03bd(\u03d1, \u03b1) = ( \u03bdi(\u03d1, \u03b1) )2k\u22121 i=0\ndenote the row-vectors of the first 2k\u22121 moments and NBMs respectively of (\u03d1, \u03b1). For an integer b > 0 and a vector \u03b2 = (\u03b21, . . . , \u03b2\u2113), let Ab(\u03b2) be the \u2113 \u00d7 b matrix (Ab(\u03b2))ij = (1 \u2212 \u03b2i)b\u22121\u2212j\u03b2ji (with 1 \u2264 i \u2264 \u2113 and 0 \u2264 j \u2264 b \u2212 1). Analogously, let Vb(\u03b2) be the \u2113 \u00d7 b \u201cVandermonde\u201d matrix (Vb(\u03b2))ij = \u03b2ji (with 1 \u2264 i \u2264 \u2113 and 0 \u2264 j \u2264 b \u2212 1). Let Pas be the 2k \u00d7 2k lower-triangular \u201cPascal triangle\u201d matrix with non-zero entries Pasij = ( 2k\u22121\u2212j i\u2212j )\nfor 0 \u2264 j \u2264 2k \u2212 1 and j \u2264 i \u2264 2k \u2212 1. Then V2k(\u03b1) = A2k(\u03b1)Pas, \u03bd(\u03d1, \u03b1) = \u03d1A2k(\u03b1), and g(\u03d1, \u03b1) = \u03d1V2k(\u03b1) = \u03bd(\u03d1, \u03b1)Pas.\nIn our algorithm it is convenient to use the empirical ordinary moments, but what we obtain are actually the empirical NBMs, so we need the following lemma. Lemma 5.2. \u2016Pas\u2016op \u2264 4k/ \u221a 3.\nProof. The non-zero entries in column j of Pas are (m \u2113 ) for \u2113 = 0, . . . ,m = 2k \u2212 1 \u2212 j. Therefore, \u2016Pas\u2016op \u2264 \u2016Pas\u2016F = \u221a \u22112k\u22121 m=0 \u2211m \u2113=0 (m \u2113 )2 = \u221a \u22112k\u22121 m=0 (2m m ) \u2264 \u221a \u22112k\u22121 m=0 2 2m.\nOur algorithm uses two input parameters \u03c4 and \u03be as input, and the empirical NBM vector \u03bd\u0303, which we convert to an empirical moment vector g\u0303 by multiplying by Pas. Since we infer (in the sampling limit) the locations of the k spikes exactly, there is a singularity in the process when spikes coincide. So we assume a minimum separation between spikes: \u03c4 = minj 6=j\u2032 |\u03b1j \u2212 \u03b1j\u2032 |. (It is of course possible to simply run a doubling search for sufficiently small \u03c4 , but the required accuracy in the moments, and hence sample size, does increase as \u03c4 decreases.) We also assume a bound \u03be\non the accuracy of our empirical statistics. (When we utilize Theorem 5.3 to obtain Theorem 5.1, \u03be is a consequence, and not an input parameter). We require that\n\u2016\u03bd\u0303 \u2212 \u03bd(\u03d1, \u03b1)\u20162 \u2264 \u03be4\u2212k \u221a 3, \u03be \u2264 \u03c42k (6)\nTheorem 5.3. There is a polytime algorithm that receives as input \u03c4, \u03be, an empirical NBM vector \u03bd\u0303 \u2208 R2k satisfying (6), and outputs a k-spike distribution (\u03d1\u0303, \u03b1\u0303) on [0, 1] such that Tran(\u03d1, \u03b1; \u03d1\u0303, \u03b1\u0303) \u2264 O(\u03be\u2126(1/k 2)).\nWe first show the information-theoretic feasibility of Theorem 5.3: the transportation distance between two probability measures on [0, 1] is upper bounded by (a moderately-growing function of) the Euclidean distance between their moment maps. (To use Lemma 5.4 to prove Theorem 5.3, we have to also show how to compute \u03d1\u0303 and \u03b1\u0303 from g\u0303 such that \u2016g\u0303 \u2212 g(\u03d1\u0303, \u03b1\u0303)\u20162, and hence, \u2016g(\u03d1, \u03b1)\u2212 g(\u03d1\u0303, \u03b1\u0303)\u20162 is small.)\nLemma 5.4. For any two (at most) k-spike distributions (\u03d1, \u03b1) (\u03d1\u0303, \u03b1\u0303) on [0, 1],\n\u2016g(\u03d1, \u03b1) \u2212 g(\u03d1\u0303, \u03b1\u0303)\u20162 \u2265 1 (2k \u2212 1)4k28k\u22125 \u00b7 ( Tran(\u03d1, \u03b1; \u03d1\u0303, \u03b1\u0303) )4k\u22122 .\nLemma 5.4 can be geometrically interpreted as follows. The point g(\u03d1, \u03b1) is in the convex hull of the moment curve and is therefore, by Caratheodory\u2019s theorem, expressible as a convex combination of 2k points on the curve. However, this point is special in that it belongs to the collection of points expressible as a convex combination of merely k points of the curve. Lemma 5.4 shows that g(\u03d1, \u03b1) is in fact uniquely expressible in this way, and that moreover this combination is stable: any nearby point in this collection can only be expressed as a very similar convex combination. We utilize the following lemma, which can be understood as a global curvature property of the moment curve; we defer its proof to Section 5.2. We prove a partial converse of Lemma 5.4 in Section 6, and hence obtain a sample-size lower bound that is exponential in k. The moment curve plays a central role in convex and polyhedral geometry [8], but as far as we know Lemmas 5.4 and 5.5 are new, and may be of independent interest.\nLemma 5.5. Let 0 \u2264 \u03b21 < . . . < \u03b2\u03ba+1 \u2264 1, \u2113 \u2208 [\u03ba], and s = \u03b2\u2113+1 \u2212 \u03b2\u2113. Let \u03b3(x) = \u2211\u03ba i=0 \u03b3ix i be a real polynomial of degree \u03ba evaluating to 1 at the points \u03b21, . . . , \u03b2\u2113 and evaluating to 0 at the points \u03b2\u2113+1, . . . , \u03b2\u03ba+1. Then \u2211\u03ba i=0 \u03b3 2 i \u2264 \u03ba224\u03ba\u22121s\u22122\u03ba.\nProof of Lemma 5.4. Denote {\u03b11, . . . , \u03b1k} \u222a {\u03b1\u03031, . . . , \u03b1\u0303k} by \u03b1 = {\u03b11, . . . , \u03b1K} where \u03b11 < . . . < \u03b1K . Define \u03d1i = \u2211\nj:\u03b1j=\u03b1i \u03d1j\u2212\n\u2211\nj:\u03b1\u0303j=\u03b1i \u03d1\u0303j for i \u2208 [K]. Let \u03d1 \u2208 RK be the row vector (\u03d11, . . . , \u03d1K).\nLet \u03b7 = Tran(\u03d1, \u03b1; \u03d1\u0303, \u03b1\u0303). So we need to show that \u2016\u03d1V2k(\u03b1)\u20162 \u2265 1(2k\u22121)4k28k\u22125 \u00b7 \u03b74k\u22122. It suffices to show that \u2016\u03d1VK(\u03b1)\u20162 \u2265 1(K\u22121)2K24K\u22125 \u00b7 \u03b72K\u22122. There is an 1 \u2264 \u2113 < K such that \u2223 \u2223 \u2211\u2113 i=1 \u03d1i \u2223\n\u2223 \u00b7 (\u03b1\u2113+1 \u2212 \u03b1\u2113) \u2265 \u03b7/(K \u2212 1). Let \u03b4 = \u2211\u2113 i=1 \u03d1i; without loss of generality \u03b4 \u2265 0, and note that \u03b4 \u2264 1. Let s = \u03b1\u2113+1 \u2212 \u03b1\u2113, so (K \u2212 1)\u03b4s \u2265 \u03b7. Denote row i of a matrix Z by Zi\u2217 and column j by Z\u2217j . We lower bound \u2016\u03d1VK(\u03b1)\u20162, by considering its minimum value under the constraints \u2211\u2113 i=1 \u03d1i = \u03b4 and \u2211K i=1 \u03d1i = 0. A vector y\u2020 = \u03d1VK(\u03b1) minimizing \u2016y\u20162 must be orthogonal to VK(\u03b1)i\u2217 \u2212 VK(\u03b1)i\u2032\u2217 if 1 \u2264 i < i\u2032 \u2264 \u2113 or if \u2113 + 1 \u2264 i < i\u2032 \u2264 K. This means that there are scalars c and d such that VK(\u03b1)y = c( \u2211\u2113\nj=1 ej) + d( \u2211K j=\u2113+1 ej), where vector ej \u2208 RK has a 1 in the j-th position and 0 everywhere else. Therefore, y = c\u03b3 + d\u03b3\u2032, where \u03b3 =\n\u2211\u2113 j=1(VK(\u03b1) \u22121)\u2217j and \u03b3\u2032 = \u2211K j=\u2113+1(VK(\u03b1) \u22121)\u2217j . At the\nsame time\n\u03b4 =\n\u2113 \u2211\ni=1\n\u03d1i = \u03d1VK(\u03b1)\u03b3 = y \u2020\u03b3 = c\u2016\u03b3\u201622+d\u03b3\u2032\u2020\u03b3 \u2212\u03b4 =\nK \u2211\ni=\u2113+1\n\u03d1i = \u03d1VK(\u03b1)\u03b3 \u2032 = y\u2020\u03b3\u2032 = c\u03b3\u2020\u03b3\u2032+d\u2016\u03b3\u2032\u201622\nand hence, \u2016y\u201622 = y \u00b7 (c\u03b3 + d\u03b3\u2032) = (c\u2212 d)\u03b4. Solving for c, d gives c\u2212 d = \u03b4\u2016\u03b3+\u03b3\u2032\u201622\n\u2016\u03b3\u20162 2 \u00b7\u2016\u03b3\u2032\u20162 2 \u2212(\u03b3\u2020\u00b7\u03b3\u2032)2 .\nFirst we examine the numerator of c \u2212 d. Like any combination of the columns of VK(\u03b1)\u22121, \u03b3 + \u03b3\u2032 is the list of coefficients of a polynomial of degree K \u2212 1, in the basis 1, x, . . . , xK\u22121. By definition, \u03b3 + \u03b3\u2032 = \u2211\nj(VK(\u03b1) \u22121)\u2217j , which is to say that for every i, VK(\u03b1)i\u2217 \u00b7 (\u03b3 + \u03b3\u2032) = 1. So the\npolynomial \u03b3 + \u03b3\u2032 evaluates to 1 at every \u03b1i. It can therefore only be the constant polynomial 1; this means that (\u03b3 + \u03b3\u2032)i = 1 if i = 0, and (\u03b3 + \u03b3\u2032)i = 0 otherwise. Thus \u2016\u03b3 + \u03b3\u2032\u201622 = 1.\nNext we examine the denominator, which we upper bound by \u2016\u03b3\u201622 \u00b7 \u2016\u03b3\u2032\u201622. When interpreted as a polynomial, \u03b3 takes the value 1 on a nonempty set of points \u03b11, . . . , \u03b1\u2113 separated by the positive distance s = \u03b1\u2113+1 \u2212 \u03b1\u2113 from another nonempty set of points \u03b1\u2113+1, . . . , \u03b1K upon which it takes the value 0. Observe that if the polynomial was required to change value by a large amount within a short interval, it would have to have large coefficients. A converse to this is the inequality stated in Lemma 5.5. Using this to bound \u2016\u03b3\u201622 and \u2016\u03b3\u2032\u201622, and since \u03b4s \u2265 \u03b7/(K \u2212 1), we obtain that\n\u2016y\u201622 = (c\u2212 d)\u03b4 \u2265 \u03b42 \u2016\u03b3\u201622 \u00b7 \u2016\u03b3\u2032\u201622 \u2265 \u03b4 2 ((K \u2212 1)224K\u22125s\u22122K+2)2 \u2265 \u03b74K\u22124 (K \u2212 1)4K28K\u221210 .\nWe now define the algorithm promised by Theorem 5.3. To give some intuition, suppose first that we are given the true moment vector g(\u03d1, \u03b1) = \u03d1V2k(\u03b1). Observe that there is a common vector \u03bb = (\u03bb0, . . . , \u03bbk)\n\u2020 of length k + 1 that is a dependency among every k + 1 adjacent columns of V2k(\u03b1). In other words, letting \u039b = \u039b(\u03bb) denote the 2k \u00d7 k matrix with \u039bij = \u03bbi\u2212j (for 0 \u2264 i < 2k, 0 \u2264 j < k and with the understanding \u03bb\u2113 = 0 for \u2113 /\u2208 {0, . . . , k}), V2k(\u03b1)\u039b = 0. Thus g(\u03d1, \u03b1)\u039b = \u03d1V2k(\u03b1)\u039b = 0. Overtly this is a system of 2k equations to determine \u03bb. But we eliminate the redundancy in \u039b by forming the k \u00d7 (k + 1) matrix G = G(g(\u03d1, \u03b1)) defined by Gij = g(\u03d1, \u03b1)i+j for i = 0, . . . , k\u2212 1 and j = 0, . . . , k, and then solve the system of linear equations G\u03bb = 0 to obtain \u03bb. This system does not have a unique solution, so in the sequel \u03bb will denote a solution with \u03bbk = 1. For each i = 1, . . . , k, we have ( V2k(\u03b1)\u039b )\ni,1 = \u2211k \u2113=0 \u03bb\u2113\u03b1i \u2113 = 0. This implies\nthat we can obtain the \u03b1i values by computing the roots of the polynomial P\u03bb(x) := \u2211k \u2113=0 \u03bb\u2113x \u2113. Once we have the \u03b1i\u2019s, we can compute \u03d1 by solving the linear system yV2k(\u03b1) = g(\u03d1, \u03b1) for y. Of course, we are actually given g\u0303 rather than the true vector g(\u03d1, \u03b1). So we need to control\nthe error in estimating first \u03b1 and then \u03d1. The learning algorithm is as follows.\nAlgorithm 2. Input: parameters \u03be, \u03c4 and empirical moments g\u0303 such that \u2016g\u0303 \u2212 g(\u03d1, \u03b1)\u20162 \u2264 \u03be. Output: a k-spike distribution (\u03d1\u0303, \u03b1\u0303)\nB1. Solve the minimization problem:\nminimize \u2016x\u20161 s.t. \u2016G(g\u0303)x\u20161 \u2264 2kk\u03be, xk = 1 (P)\nwhich can be encoded as a linear program and hence solved in polytime, to obtain a solution \u03bb\u0303. Observe that since G(g\u0303) has k + 1 columns and k rows, there is always a feasible solution.\nB2. Let \u03b1\u03041, . . . , \u03b1\u0304k be the (possibly complex) roots of the polynomial P\u03bb\u0303. Thus, we have V2k(\u03b1\u0304)\u039b(\u03bb\u0303) = 0. We map the roots to values in [0, 1] as follows. Let \u01eb = 4\u03c4 (2k\u03be)\n1/k. First we compute \u03b1\u03021, . . . , \u03b1\u0302k such that |\u03b1\u0302i \u2212 \u03b1\u0304i| \u2264 \u01eb for all i, in time poly ( log(1\u01eb ) )\n, using Pan\u2019s algorithm [38, Theorem 1.1]2. We now set \u03b1\u0303i = max{0,min{Re(\u03b1\u0302i), 1}}.\n2The theorem requires that the complex roots lie within the unit circle and that the coefficient of the highest-degree term is 1; but the discussion following it in [38] shows that this is essentially without loss of generality.\nB3. Finally, we set \u03d1\u0303 to be the row-vector y that minimizes \u2016yV2k(\u03b1\u0303)\u2212 g\u0303\u201622 subject to \u2016y\u20161 = 1, y \u2265 0. Note that this is a convex quadratic program that can be solved exactly in polytime [17].\nWe now analyze Algorithm 2 and justify Theorem 5.3. Recall that \u03c4 = minj 6=j\u2032 |\u03b1j \u2212 \u03b1j\u2032 |. We need the following lemma, whose proof appears in Section 5.2.\nLemma 5.6. The weights \u03d1\u0303 satisfy \u2016\u03d1\u0303V2k(\u03b1\u0303)\u2212 g\u0303\u20162 \u2264 \u2016g(\u03d1, \u03b1) \u2212 g\u0303\u20162 + (8k) 5/2 \u03c4 \u00b7 (2k\u03be)1/k.\nProof of Theorem 5.3. We call Algorithm 2 with g\u0303 = \u03bd\u0303Pas. By Lemma 5.2, we obtain that \u2016g\u0303 \u2212 g(\u03d1, \u03b1)\u20162 \u2264 \u03be, and by Lemma 5.6, we have that \u2016g(\u03d1, \u03b1) \u2212 \u03d1\u0303V2k(\u03b1\u0303)\u20162 \u2264 2\u2016g(\u03d1, \u03b1) \u2212 g\u0303\u20162 + 8\u03c4 \u00b7 (8k)3/2 ( 2k\u03be )1/k . Coupled with Lemma 5.4 and since \u03be \u2264 \u03c42k, we obtain that\nTran(\u03d1, \u03b1; \u03d1\u0303, \u03b1\u0303) \u2264 [ (2k \u2212 1)4k28k\u22125\u2016g(\u03d1, \u03b1) \u2212 g(\u03d1\u0303, \u03b1\u0303)\u20162 ] 1 4k\u22122\n\u2264 [ (2k \u2212 1)4k28k\u22125 ( 2\u03be + (8k) 5 2\n\u03c4\n( 2k\u03be )\n1 k\n)\n] 1\n4k\u22122\n\u2264 [ (2k \u2212 1)4k28k\u22125 ( 2\u03be + (8k)5/2 ( 2k \u221a \u03be )1/2k\n)] 1\n4k\u22122 \u2264 1024 \u00b7 k\u03be 1 8k2 .\nProof of Theorem 5.1. We convert \u03b8 to the corresponding binary source ( \u03d1, (f1, . . . , fk) )\nby randomized rounding. Given a (2k \u2212 1)-snapshot z = (z1, . . . , z2k\u22121) \u2208 [0, 1]2k\u22121 from \u03b8, we obtain a (2k \u2212 1)-snapshot from the binary source as follows. We choose 2k \u2212 1 independent values a1, . . . , a2k\u22121 uniformly at random from [0, 1] and set Xi = 1 if zi \u2265 ai and 0 otherwise for all i \u2208 [2k \u2212 1]. Note that if qj is the constituent generating the (2k \u2212 1)-snapshot z, then Pr[Xi = 1|qj ] = E[Xi|qj] = \u03b1j, and so X1, . . . ,X2k\u22121 is a random (2k\u22121)-snapshot from the above binary source.\nNow we apply Theorem 5.3, setting \u03be = s2k. Let \u03bd\u0303 be the empirical NBM-vector obtained from\nthe (2k \u2212 1)-snapshots of the above binary source (i.e., \u03bd\u0303i = (2k\u22121\ni )\u22121\u00b7 (frequency with which the (2k \u2212 1)-snapshot has exactly i 1s)). The stated sample size ensures, via a Chernoff bound, that Pr [ |\u03bd\u0303i \u2212 \u03bd(\u03d1, \u03b1)i| \u2265 \u03be4 \u2212k\n\u221a 6k\n]\n< \u03c82k for all i = 0, . . . , 2k \u2212 1. Hence, with probability at least 1\u2212 \u03c8, we have \u2016\u03bd\u0303 \u2212 \u03bd(\u03d1, \u03b1)\u20162 \u2264 \u221a 2k \u00b7 \u2016\u03bd\u0303 \u2212 \u03bd(\u03d1, \u03b1)\u2016\u221e \u2264 \u03be4\u2212k/ \u221a 3."}, {"heading": "5.2 Proofs of Lemma 5.5 and Lemma 5.6", "text": "Proof of Lemma 5.5. There are two easy cases to dismiss before we reach the more subtle part of this lemma. The first easy case is \u2113 = 1. In this case \u03b3 is a single Lagrange interpolant: \u03b3(x) =\n\u220f\u03ba+1 j=2 x\u2212\u03b2j \u03b21\u2212\u03b2j . For 0 \u2264 i \u2264 \u03ba let e \u03ba i (\u03b22, . . . , \u03b2\u03ba+1) be the i\u2019th elementary symmetric mean,\ne\u03bai (\u03b22, . . . , \u03b2\u03ba+1) = 1 (\n\u03ba i\n)\n\u2211\nS\u2286{2,...,\u03ba+1}:|S|=i\n\u220f j\u2208S \u03b2j\nand observe that for all i, 0 \u2264 e\u03bai (\u03b22, . . . , \u03b2\u03ba+1) \u2264 1. Now\n\u03b3(x) = (\n\u03ba+1 \u220f\nj=2\n1\n\u03b21 \u2212 \u03b2j\n) \u03ba \u2211\ni=0\n(\u22121)\u03ba\u2212i ( \u03ba\ni\n)\ne\u03ba\u03ba\u2212i(\u03b22, . . . , \u03b2\u03ba+1)x i\nSo \u2211\u03ba i=0 \u03b3 2 i =\n(\n\u220f\u03ba+1 j=2 1 \u03b21\u2212\u03b2j\n)2 \u2211\u03ba\ni=0 ((\u03ba i ) e\u03bai (\u03b22, . . . , \u03b2\u03ba+1) )2 \u2264 s\u22122\u03ba\u2211\u03bai=0 (\u03ba i )2 = (2\u03ba \u03ba ) s\u22122\u03ba.\nThe second easy case is \u2113 = \u03ba; this is almost as simple. Merely note that the above argument applies to the polynomial 1 \u2212 \u03b3, so that we have only to allow for the possible increase of |\u03b30| by 1. Hence\n\u2211\u03ba i=0 \u03b3 2 i \u2264 4\n(\n2\u03ba \u03ba\n) s\u22122\u03ba. We now consider the less trivial case of 1 < \u2113 < \u03ba. The difficulty here is that the La-\ngrange interpolants of \u03b3 may have very large coefficients, particularly if among \u03b21, . . . , \u03b2\u2113 or among \u03b2\u2113+1, . . . , \u03b2\u03ba+1 there are closely spaced roots, as well there may be. We must show that these large coefficients cancel out in \u03b3.\nThe trick is to examine not \u03b3 but \u2202\u03b3/\u2202x. The roots of the derivative interlace the two sets on which \u03b3 is constant, which is to say, with \u03b2\u20321 \u2264 . . . \u2264 \u03b2\u2032\u03ba\u22121 denoting the roots of \u2202\u03b3/\u2202x, that for j < \u2113, \u03b2j \u2264 \u03b2\u2032j \u2264 \u03b2j+1, and for j \u2265 \u2113, \u03b2j+1 \u2264 \u03b2\u2032j \u2264 \u03b2j+2. In particular, none of the roots fall in the interval (\u03b2\u2113, \u03b2\u2113+1). For some constant C we can write \u2202\u03b3/\u2202x = C \u220f\u03ba\u22121 j=0 (x\u2212\u03b2\u2032j) (with sign(C) = (\u22121)1+\u03ba\u2212\u2113). Observe that \u222b \u03b2\u2113+1 \u03b2\u2113 \u2202\u03b3 \u2202x(x) dx = \u22121. So (\u22121)1+\u03ba\u2212\u2113/C = \u222b \u03b2\u2113+1 \u03b2\u2113\n(\u22121)\u03ba\u2212\u2113\u220f\u03ba\u22121j=0 (x \u2212 \u03b2\u2032j) dx. Observe that if for any j < \u2113, \u03b2\u2032j is increased, or if for any j \u2265 \u2113, \u03b2\u2032j is decreased, then the integral decreases. So (\u22121)1+\u03ba\u2212\u2113/C \u2265\n\u222b \u03b2\u2113+1 \u03b2\u2113 (\u22121)\u03ba\u2212\u2113(x \u2212 \u03b2\u2113)\u2113\u22121(x \u2212 \u03b2\u2113+1)\u03ba\u2212\u2113 dx. This is a definite integral that can be evaluated in closed form:\n\u222b \u03b2\u2113+1\n\u03b2\u2113\n(\u22121)\u03ba\u2212\u2113(x\u2212 \u03b2\u2113)\u2113\u22121(x\u2212 \u03b2\u2113+1)\u03ba\u2212\u2113 dx = (\u03b2\u2113+1 \u2212 \u03b2\u2113)\u03ba(\u2113\u2212 1)!(\u03ba \u2212 \u2113)!/\u03ba! .\nHence, (\u22121)1+\u03ba\u2212\u2113C \u2264 \u03ba!s\u03ba(\u2113\u22121)!(\u03ba\u2212\u2113)! . The sum of squares of coefficients of \u2202\u03b3 \u2202x is C2 \u2211\u03ba\u22121\ni=0\n( \u03ba\u22121 i )2 (e\u03ba\u22121i (\u03b2 \u2032 1, . . . , \u03b2 \u2032 \u03ba\u22121)) 2 \u2264 C2 ( 2\u03ba\u22122 \u03ba\u22121 )\n. Integration only decreases the magnitude of the coefficients, so the same bound applies to \u03b3, with the exception of the constant coefficient. The constant coefficient can be bounded by the fact that \u03b3 has a root in (0, 1), and that in that interval the derivative is bounded in magnitude by C\n\u2211\u03ba\u22121 i=0 (\u03ba\u22121 i ) = C \u00b7 2\u03ba. So |\u03b30| \u2264 C \u00b7 2\u03ba. Consequently, \u2211\u03ba\ni=0 \u03b3 2 i is at most\nC2 [( 2\u03ba\u2212 2 \u03ba\u2212 1 ) + 22\u03ba ] \u2264 (2\u03ba\u22122 \u03ba\u22121 ) + 22\u03ba s2\u03ba \u00b7 (\n\u03ba!\n(\u2113\u2212 1)!(\u03ba \u2212 \u2113)!\n)2 \u2264 5\u03ba 222\u03ba\u22122 s2\u03ba \u00b7 ( \u03ba\u2212 1 \u2113\u2212 1 )2 \u2264 5\u03ba 224\u03ba\u22124 s2\u03ba ,\nwhich completes the proof of the lemma.\nProof of Lemma 5.6. Recall that G = G(g(\u03d1, \u03b1)) is the k\u00d7(k+1) matrix defined byGij = g(\u03d1, \u03b1)i+j for i = 0, . . . , k \u2212 1 and j = 0, . . . , k; \u03bb is such that G\u03bb = 0 and \u03bbk = 1; \u039b = \u039b(\u03bb) is the 2k \u00d7 k matrix with \u039bij = \u03bbi\u2212j (for 0 \u2264 i < 2k, 0 \u2264 j < k with the understanding \u03bb\u2113 = 0 for \u2113 /\u2208 {0, . . . , k}; and P\u03bb(x) is the polynomial \u2211k \u2113=0 \u03bb\u2113x\n\u2113. We use Vk, V2k, to denote Vk(\u03b1), V2k(\u03b1) respectively, and V\u0303k, V\u03032k, G\u0303, \u039b\u0303 to denote Vk(\u03b1\u0303), V2k(\u03b1\u0303), G(g\u0303),\u039b(\u03bb\u0303) respectively. We abbreviate g(\u03d1, \u03b1) to g.\nLemma 5.7. If \u2016g\u0303 \u2212 g\u20162 \u2264 \u03be, then \u2016G\u03bb\u0303\u20161 \u2264 2k+1k\u03be.\nProof. First, observe that G\u0303\u03bb = G\u03bb+(G\u0303\u2212G)\u03bb = (G\u0303\u2212G)\u03bb. Also \u2016\u03bb\u20162 \u2264 \u2016\u03bb\u20161 = \u220fk i=1(1+\u03b1i) \u2264 2k. The last two inequalities follows since P\u03bb(x) = \u220fk i=1(x\u2212\u03b1i), and P\u03bb(\u22121) = (\u22121)k\u2016\u03bb\u20161. So for any i = 1, . . . , k, \u2223 \u2223(G \u2212 G\u0303)i \u00b7 \u03bb \u2223\n\u2223 \u2264 \u2016\u03bb\u20162\u2016Gi \u2212 G\u0303i\u20162 \u2264 2k\u03be. Thus, \u03bb is a feasible solution to (P), which implies that \u2016\u03bb\u0303\u20161 \u2264 2k. We have \u2016G\u03bb\u0303\u20161 \u2264 \u2016G\u0303\u03bb\u0303\u20161 + \u2016(G \u2212 G\u0303)\u03bb\u0303\u20161 \u2264 2kk\u03be + \u2016(G \u2212 G\u0303)\u03bb\u0303\u20161. For any i = 1, . . . , k, \u2223 \u2223(G\u2212 G\u0303)i \u00b7 \u03bb\u0303 \u2223 \u2223 \u2264 \u2016Gi \u2212 G\u0303i\u20162\u2016\u03bb\u0303\u20162 \u2264 2k\u03be, so \u2016G\u03bb\u0303\u20161 \u2264 2k+1k\u03be.\nLemma 5.8. For every \u03b1i, i = 1, . . . , k, there exists a \u03c3(i) \u2208 {1, . . . , k} such that \u03d1i|\u03b1i \u2212 \u03b1\u0303\u03c3(i)| \u2264 8 \u03c4 (2k\u03be) 1/k.\nProof. Since \u2016G\u03bb\u0303\u20162 \u2264 2k+1k\u03be (by Lemma 5.7), we have equivalently that the \u2016.\u20162 norm of g\u039b\u0303 = \u03d1V2k\u039b\u0303 is at most 2 k+1k\u03be. We may write \u03d1V2k\u039b\u0303 as\n\u03d1V2k\u039b\u0303 = \u03d1\n\n   \nP\u03bb\u0303(\u03b11) \u03b11P\u03bb\u0303(\u03b11) \u00b7 \u00b7 \u00b7 \u03b1k\u221211 P\u03bb\u0303(\u03b11) P\u03bb\u0303(\u03b12) \u03b12P\u03bb\u0303(\u03b12) \u00b7 \u00b7 \u00b7 \u03b1k\u221212 P\u03bb\u0303(\u03b12)\n... ...\n. . . ...\nP\u03bb\u0303(\u03b1k) \u03b1kP\u03bb\u0303(\u03b1k) \u00b7 \u00b7 \u00b7 \u03b1k\u22121k P\u03bb\u0303(\u03b1k)\n\n   \nwhich is equal to \u03d1\u2032Vk(\u03b1) where \u03d1\u2032 = ( \u03d11P\u03bb\u0303(\u03b11), \u00b7 \u00b7 \u00b7 , \u03d1kP\u03bb\u0303(\u03b1k) ) . Thus, we are given that \u2016\u03d1\u2032Vk\u20162 \u2264 2k+1k\u03be.\nLet (\u03b3i)\u2020 = ( argminy\u2208Rk:yi=1 \u2016yVk\u20162 ) Vk. Then, we also have \u2016\u03d1\u2032Vk\u20162 \u2265 maxi |\u03d1\u2032i|\u2016\u03b3i\u20162. Note that \u03b3i must be orthogonal to (Vk)j\u2217 for all j 6= i, and (Vk)i\u2217\u03b3i = \u2016\u03b3i\u201622. (Recall that Zi\u2217 denotes row i of a matrix Z.) Let Qi(x) = \u2211k\u22121 \u2113=0 \u03b3 i \u2113x \u2113. Then, Qi(x) = \u2016\u03b3i\u201622 \u220f j 6=i x\u2212\u03b1j \u03b1i\u2212\u03b1j . Also, since the coefficients of Qi(x) have alternating signs, we have\n|Qi(\u22121)| = \u2016\u03b3i\u20161 = \u2016\u03b3i\u201622 \u220f\nj 6=i\n1 + \u03b1j |\u03b1i \u2212 \u03b1j| .\nHence, \u2016\u03b3i\u20162 \u2265 \u220f j 6=i |\u03b1i\u2212\u03b1j | 1+\u03b1j . So we obtain the lower bound\n\u2016\u03d1\u2032Vk\u20162 \u2265 max i\n(\n|\u03d1\u2032i|\u00b7 \u220f\nj 6=i\n|\u03b1i \u2212 \u03b1j | 1 + \u03b1j ) \u2265 max i ( \u03d1i (\u03c4 2\n)k\u22121 k\u220f\nj=1\n|\u03b1i\u2212\u03b1\u0304j| )\n\u2265 max i\n(\n\u03d1i\n(\u03c4\n2\n)k\u22121 k\u220f\nj=1\n|\u03b1i\u2212Re(\u03b1\u0304j)| ) .\nThe last inequality follows since complex roots occur in conjugate pairs, so if \u03b1\u0304\u2113 = a+bi is complex, then there must be some \u2113\u2032 such that \u03b1\u0304\u2113\u2032 = a\u2212 bi and therefore,\n\u220f\nj\n|\u03b1i \u2212 \u03b1\u0304j| = ( (\u03b1i \u2212 a)2 + b2 ) \u00b7 \u220f j 6=\u2113,\u2113\u2032 |\u03b1i \u2212 \u03b1\u0304j | \u2265 (\u03b1i \u2212 a)2 \u00b7 \u220f j 6=\u2113,\u2113\u2032 |\u03b1i \u2212 \u03b1\u0304j |.\nNow, we claim that |\u03b1i\u2212Re(\u03b1\u0304j)| \u2265 \u2223 \u2223|\u03b1i\u2212 \u03b1\u0303j|\u2212\u01eb \u2223\n\u2223 for every j. If both Re(\u03b1\u0304j) and Re(\u03b1\u0302j) lie in [0, 1], or both of them are less than 0, or both are greater than 1, then this follows since |\u03b1\u0304j \u2212 \u03b1\u0302j| \u2264 \u01eb and \u03b1i \u2208 [0, 1]. If Re(\u03b1\u0304j) /\u2208 [0, 1] but Re(\u03b1\u0302j) \u2208 [0, 1], or if Re(\u03b1\u0304j) \u2208 [0, 1] but Re(\u03b1\u0302j) /\u2208 [0, 1], then this again follows since |\u03b1\u0304j \u2212 \u03b1\u0302j | \u2264 \u01eb. Combining everything, we get that\n2k(2k\u03be) \u2265 \u2016\u03d1\u2032Vk\u20162 \u2265 max i\n(\n\u03d1i\n(\u03c4\n2\n)k\u22121 k\u220f\nj=1\n\u2223 \u2223|\u03b1i \u2212 \u03b1\u0303j | \u2212 \u01eb \u2223 \u2223\n)\n.\nThis implies that for every i = 1, . . . , k, there exists \u03c3(i) \u2208 {1, . . . , k} such that \u03d1i|\u03b1i \u2212 \u03b1\u0303\u03c3(i)| \u2264 4 \u03c4 \u00b7 ( 2k\u03be )1/k + \u01eb.\nWe can now wrap up the proof of Lemma 5.6. Let \u03b7 = 8\u03c4 \u00b7 ( 2k\u03be )1/k . We will bound \u2016\u03d1\u0303V\u03032k \u2212 g\u0303\u20162 by exhibiting a solution y \u2208 [0, 1]k , \u2016y\u20161 = 1 such that \u2016yV\u03032k \u2212 g\u0303\u20162 \u2264 \u2016g \u2212 g\u0303\u2016 + k(8k)3/2\u03b7. Let \u03c3 be the function whose existence is proved in Lemma 5.8. For j = 1, . . . , k, set yj = \u2211 i:\u03c3(i)=j \u03d1i (if \u03c3\u22121(j) = \u2205, then yj = 0). We have \u2016yV\u03032k \u2212 g\u0303\u20162 \u2264 \u2016g \u2212 g\u0303\u20162 + \u2016g \u2212 yV\u03032k\u20162. We expand g \u2212 yV\u03032k = \u03d1V2k \u2212 yV\u03032k = \u2211k i=1 \u03d1i ( (V2k)i\u2217 \u2212 (V\u03032k)\u03c3(i)\u2217 ) For every i,\n\u03d12i \u2016(V2k)i\u2217 \u2212 (V\u03032k)\u03c3(i)\u2217\u201622 = \u03d12i 2k\u22121 \u2211\n\u2113=0\n(\u03b1\u2113i \u2212 \u03b1\u0303\u2113\u03c3(i))2 \u2264 \u03d12i \u00b7 8k3 \u00b7 \u03b72.\nTherefore, \u2016g \u2212 yV\u03032k\u20162 \u2264 k(8k)3/2\u03b7."}, {"heading": "6 Lower bounds", "text": "In this section, we prove sample-size and aperture lower bounds that apply even to the setting where we have k-mixture sources on {0, 1} (so n = 2). Recall that a k-mixture source on {0, 1} may be equivalently viewed as a k-spike distribution supported on [0, 1]; in the sequel, we therefore focus on k-spike distributions. The separation of a k-spike distribution (or the equivalent k-mixture source) is the minimum separation between its spikes. Theorem 6.2 proves that 2k\u22121 is the smallest aperture at which it becomes possible to reconstruct a k-spike distribution. We emphasize that this is an information-theoretic lower bound. We show (Theorem 6.2) that there are two k-spike distributions supported on [0, 1] having separation \u2126 (\n1 k\n) and transportation distance \u2126 ( 1 k )\nthat yield exactly the same first 2k \u2212 2 moments. Moreover, for any b \u2265 2k \u2212 1, by adjusting the constant in the \u2126(.)s, one can ensure that the (2k \u2212 1)-th, . . . , b-th moments of these two k-spike distributions are exponentially close.\nIt follows immediately that even with infinite sample size it is impossible to reconstruct a kmixture source (with arbitrarily small error) if we limit the aperture to 2k \u2212 2. Furthermore, we leverage the exponential closeness of the moments to show that for any aperture b \u2265 2k \u2212 1, there exists \u03c4 = \u2126 (\n1 k\n)\nsuch that reconstructing a k-mixture source on {0, 1} having separation \u03c4 to within transportation distance \u03c44 requires exponential in k sample size (Theorem 6.1). In fact, since n = 2, this means that with arbitrary mixtures, the exponential dependence of the sample size on k remains even with aperture O(k log n), and more generally, even with aperture O ( k \u00b7\u03ba(n) ) for any function \u03ba(.). (To place this in perspective, observe that with separation \u03c4 = \u2126 (\n1 k\n)\n, if we have \u2126(k2 log k) aperture, then O(k3) samples suffice to reconstruct the given mixture source to within transportation distance \u03c44 . This is because with, with high probability, we will see every {0, 1} source or \u201ccoin\u201d with weight \u03d1i \u2265 1\u03c42 , and we can estimate its bias within additive error, say \u03c48 , with probability 1 \u2212 1poly(k) since we have \u2126 ( log k \u03c42 ) coin tosses available. The unseen coins contribute O(\u03c4) to the transportation distance, so we infer k-mixture source within transportation distance \u03c44 .)\nTheorem 6.1. (i) With aperture 2k \u2212 2, it is impossible to reconstruct a k-mixture source having separation at least 12k to within transportation distance 1 8k even with infinite sample size. (ii) For any \u03c8 \u2208 (0, 1), and any constants cA \u2265 1, cE \u2265 0, there exists \u03c4 = \u2126 ( 1 k )\nsuch that reconstructing a k-mixture source having separation \u03c4 to within transportation distance \u03c44 with probability at least 1\u2212 \u03c8 using aperture cA(2k \u2212 1) requires \u2126 ( 3cEk ln( 1\u03c8 ) ) samples.\nOur approach for proving Theorem 6.1 is as follows. To prove the existence of two suitable k-spike distributions (Theorem 6.2), we fix some spike locations ensuring the required separation and transportation-distance bounds, and search for suitable probability weights to place on these locations so as to obtain the desired closeness of moments for the two k-spike distributions. Since moments are linear functions of the weights (and the spike locations are fixed), this search problem can be encoded as a minimization LP (P1). To upper bound the optimum, we move to the dual LP (D1), which can be interpreted as finding a polynomial satisfying certain conditions on its coefficients and roots, to maximize the variation between its values at certain spike locations. We upper bound the variation possible by such a polynomial using elementary properties of polynomials. Finally, the closeness of moments of the two k-spike distributions obtained this way implies that the distributions of b-snapshots of these two distributions have exponentially small variation distance (Lemma 6.3), and this yields the sample-size lower bound in Theorem 6.1.\nTheorem 6.2. Let b \u2265 2k \u2212 1, and \u03c1 \u2265 2. There are two k-spike distributions (y, \u03b1) and (z, \u03b2) on [0, 1] with separation 2(2k\u22121)\u03c1 and Tran(y, \u03b1; z, \u03b2) \u2265 1(2k\u22121)\u03c1 such that g\u2113(y, \u03b1) = g\u2113(z, \u03b2) for all\n\u2113 = 0, . . . , 2k \u2212 2, and \u2211b\u2113=2k\u22121 ( b \u2113 ) 2\u2113|g\u2113(y, \u03b1) \u2212 g\u2113(z, \u03b2)| \u2264 4 \u00b7 3 b \u03c12k\u22121 .\nProof. Let \u01eb = 1\u03c1 . We set \u03b1i = \u01eb \u00b7 2(i\u22121) 2k\u22121 , and \u03b2i = \u01eb \u00b7 2i\u221212k\u22121 = \u03b1i+ \u01eb2k\u22121 for all i = 1, . . . , k. Note that for any mixture weights y1, . . . , yk, and z1, . . . , zk, the separation of (y, \u03b1) and (z, \u03b2) is 2\n(2k\u22121)\u03c1 and\nTran(y, \u03b1; z, \u03b2) \u2265 1(2k\u22121)\u03c1 . We obtain y and z by solving the following linear program (P1), whose optimal value we show is at most 4 \u00b7 3b\n\u03c12k\u22121 .\nmin b \u2211\n\u2113=2k\u22121\n(\nb\n\u2113\n)\n2\u2113\u03bb\u2113 (P1)\ns.t.\nk \u2211\ni=1\n(\nzi\u03b2 \u2113 i \u2212 yi\u03b1\u2113i) = 0 \u2200\u2113 = 0, . . . , 2k \u2212 2 (7)\nk \u2211\ni=1\n(\nzi\u03b2 \u2113 i \u2212 yi\u03b1\u2113i) \u2264 \u03bb\u2113 \u2200\u2113 = 2k \u2212 1, . . . , b (8)\nk \u2211\ni=1\n(\nyi\u03b1 \u2113 i \u2212 zi\u03b2\u2113i ) \u2264 \u03bb\u2113 \u2200\u2113 = 2k \u2212 1, . . . , b (9)\nk \u2211\ni=1\nyi = 1, yi, zi \u2265 0 \u2200i = 1, . . . , k.\nmax c (D1)\ns.t. c\u2212 2k\u22122 \u2211\n\u2113=0\n\u03b3\u2113\u03b1 \u2113 i \u2212\nb \u2211\n\u2113=2k\u22121 (\u03b3\u2113 \u2212 \u03b8\u2113)\u03b1\u2113i \u2264 0\n\u2200i = 1, . . . , k 2k\u22122 \u2211\n\u2113=0\n\u03b3\u2113\u03b2 \u2113 i +\nb \u2211\n\u2113=2k\u22121 (\u03b3\u2113 \u2212 \u03b8\u2113)\u03b2\u2113i \u2264 0\n\u2200i = 1, . . . , k\n\u03b3\u2113 + \u03b8\u2113 \u2264 ( b\n\u2113\n)\n2\u2113 \u2200\u2113 = 2k \u2212 1, . . . , b\n\u03b3\u2113, \u03b8\u2113 \u2265 0 \u2200\u2113.\n(D1) is the dual of (P1). The dual variable c corresponds to \u2211\ni yi = 1, variables \u03b3\u2113 for \u2113 = 0, . . . , b correspond to (7) and (9), and variables \u03b8\u2113 for \u2113 = 2k \u2212 1, . . . , b correspond to (8). Given a feasible solution to (D1), if we set \u03b3\u2032\u2113 = \u03b3\u2113 \u2212 min{\u03b3\u2113, \u03b8\u2113}, \u03b8\u2032\u2113 = \u03b8\u2113 \u2212 min{\u03b3\u2113, \u03b8\u2113} for all \u2113 = 2k\u22121, . . . , b, then we obtain another feasible solution to (D1), where \u03b3\u2032\u2113+\u03b8\u2032\u2113 = |\u03b3\u2032\u2113\u2212\u03b8\u2032\u2113| = |\u03b3\u2113\u2212\u03b8\u2113|. Thus, an optimal solution to (D1) can be interpreted as a polynomial f(x) =\n\u2211b \u2113=0 f\u2113x \u2113 satisfying\n|f\u2113| \u2264 (b \u2113 ) 2\u2113 for all \u2113 = 2k \u2212 1, . . . , b, and f(\u03b1i) \u2265 c > 0 \u2265 f(\u03b2i) for all i = 1, . . . , k (where c > 0 follows from Lemma 5.4).\nLet c\u2032 = 3b \u00b7 \u03c1/(\u03c1\u22121) \u03c12k\u22121 \u2264 2 \u00b7 3b \u03c12k\u22121 . Suppose that c > 2c\u2032. Observe that for any x \u2208 [0, \u01eb], by the Cauchy-Schwarz inequality and since the \u21132-norm is at most the \u21131 norm, we have\n\u2223 \u2223 \u2223\nb \u2211\n\u2113=2k\u22121 f\u2113x\n\u2113 \u2223 \u2223 \u2223 \u2264 (\nb \u2211\n\u2113=2k\u22121 |f\u2113|\n)( b \u2211\n\u2113=2k\u22121 x\u2113 )\n\u2264 b \u2211\n\u2113=2k\u22121\n(\nb\n\u2113\n) 2\u2113 \u00b7 \u03c1/(\u03c1\u2212 1) \u03c12k\u22121 \u2264 c\u2032. (10)\nLet h(x) = \u22112k\u22122 \u2113=0 f\u2113x \u2113 \u2212 c/2. Then, due to (10), we have f(x)\u2212 c/2\u2212 c\u2032 \u2264 h(x) \u2264 f(x)\u2212 c/2 + c\u2032 for all x \u2208 [0, \u01eb], so h(\u03b1i) > 0 > h(\u03b2i) for all i = 1, . . . , k. But then h(x) has 2k \u2212 1 roots\u2014one in every (\u03b1i, \u03b2i) and (\u03b2i, \u03b1i+1) interval\u2014which is impossible since h(x) is a polynomial of degree 2k \u2212 2.\nGiven a k-spike distribution ( \u03d1, \u03b1 = (\u03b11, . . . , \u03b1k) )\non [0, 1], we abuse notation and denote the equivalent k-mixture source on {0, 1} also by (\u03d1, \u03b1); that is, \u03b8 = (\u03d1, \u03b1) represents a mixture of k \u201ccoins\u201d, where coin i has bias \u03b1i and is chosen with weight \u03d1i. Let g(\u03d1, \u03b1) = ( gi(\u03d1, \u03b1) )2k\u22121 i=0 . We use D\u03b8 (viewed as a vector in R {0,1}2k\u22121 \u22650 ) to denote the distribution of (2k \u2212 1)-snapshots induced by \u03b8 on {0, 1}2k\u22121. The total variation distance dTV(Dy,Dz) between two such distributions is defined to be 12\u2016Dy \u2212Dz\u20161.\nLemma 6.3. Let b \u2265 2k\u22121. Given two k-mixture sources y = (y, \u03b1) and z = (z, \u03b2) on {0, 1} with identical first 2k \u2212 2 moments, we have dTV(Dy,b,Dz,b) = 12 \u2211b \u2113=2k\u22121 (b \u2113 ) 2\u2113|g\u2113(y, \u03b1) \u2212 g\u2113(z, \u03b2)|.\nProof. For any s \u2208 {0, 1}b with i 1s, we have Dy,bs = \u03bdi(y, \u03b1) and Dz,bs = \u03bdi(z, \u03b2). Therefore, dTV(D y,b,Dz,b) = 12 \u2211b i=0 (b i )\n|\u03bdi(y, \u03b1)\u2212\u03bdi(z, \u03b2)|. Let B be the (b+1)\u00d7 (b+1) diagonal matrix with Bii = ( b i ) for 0 \u2264 i \u2264 b. Then, ( b i ) \u03bdi(y, \u03b1) = ( gb(y, \u03b1)Pas\u22121b+1B ) i . Let \u2206gb := gb(y, \u03b1) \u2212 gb(z, \u03b2). So dTV(D y,b,Dz,b) = 12\u2016(\u2206gb)(Pas\u22121b+1B)\u20161. We prove below that Pas\u22121b+1 is the lower triangular matrix with entries (Pas\u22121b+1)ij = (\u22121)i\u2212j (b\u2212j i\u2212j )\nfor 0 \u2264 j \u2264 i \u2264 b (and 0 otherwise). Let Zi\u2217 denote row i of matrix Z. Since (\u2206gb)i = 0 for i = 0, . . . , 2k \u2212 2, we have that dTV(Dy,b,Dz,v) is at most\n1\n2\nb \u2211\n\u2113=2k\u22121 |(\u2206gb)\u2113| \u00b7 \u2016(Pas\u22121b+1B)\u2113,\u2217\u20161 =\n1\n2\nb \u2211\n\u2113=2k\u22121 |(\u2206gb)\u2113|\n\u2113 \u2211\nj=0\n(\nb\u2212 j \u2113\u2212 j\n)(\nb\nj\n)\n= 1\n2\nb \u2211\n\u2113=2k\u22121 |(\u2206gb)\u2113|\n(\nb\n\u2113\n) \u2113 \u2211\nj=0\n(\n\u2113\nj\n)\n= 1\n2\nb \u2211\n\u2113=2k\u22121 |(\u2206gb)\u2113|\n(\nb\n\u2113\n)\n2\u2113.\nTo see the claim about Pas\u22121b+1, let Q be the claimed inverse matrix; so Qij = (\u22121)i\u2212j (b\u2212j i\u2212j ) for 0 \u2264 j \u2264 i \u2264 b. Then (Pasb+1Q)ij = 0 for j > i, and is equal to \u2211i\n\u2113=j\n( b\u2212\u2113 i\u2212\u2113 ) (\u22121)\u2113\u2212j ( b\u2212j \u2113\u2212j ) otherwise.\nThe latter expression evaluates to ( b\u2212j i\u2212j ) \u2211i \u2113=j(\u22121)\u2113\u2212j ( i\u2212j \u2113\u2212j ) = ( b\u2212j i\u2212j ) (1 \u2212 1)i\u2212j , which is 0 if i > j, and 1 if i = j.\nProof of Theorem 6.1. For part (i), take \u03c1 = 2 and b = 2k\u2212 1. Consider the two k-mixture sources y = (y, \u03b1) and z = (z, \u03b2) given by Theorem 6.2, which have separation 12k\u22121 and transportation distance 12(2k\u22121) . For any b \u2032 \u2264 2k \u2212 2, the distributions Dy,b\u2032 and Dz,b\u2032 are identical and hence indistinguishable even with infinitely many samples, but the stated reconstruction task would allow us to do precisely this.\nFor part (ii), set \u03c1 = 3cE+cA , b = cA(2k \u2212 1), and \u03c4 = 2(2k\u22121)\u03c1 . Let y = (y, \u03b1) and z = (z, \u03b2) be as given by Theorem 6.2 (for this b, \u03c1), which satisfy the required separation property. Suppose that we can perform the stated reconstruction task using N b-snapshots. Then, we can distinguish between y and z with probability at least 1 \u2212 \u03c8. But this probability is also upper bounded by [\n1 + dTV ( (Dy,b)N , (Dz,b)N )] /2, where (Dy,b)N and (Dz,b)N are the N -fold products of Dy,b and Dz,b respectively. Thus, dTV ( (Dy,b)N , (Dz,b)N ) \u2265 1\u2212 2\u03c8. By Proposition 11 and Lemma 12 in [7]\nN \u2265 1 4dTV(Dy,b,Dz,b) ln\n(\n1 1\u2212 (1\u2212 2\u03c8)2 ) \u2265 \u03c1 2k\u22121 8 \u00b7 3b ln ( 1 4\u03c8 ) = \u2126 ( 3cE(2k\u22121) ln ( 1 \u03c8 ) )\nwhere the second inequality follows from Theorem 6.2 and Lemma 6.3."}, {"heading": "A Probability background", "text": "We use the following large-deviation bounds in our analysis.\nLemma A.1 (Chernoff bound; see Theorem 1.1 in [23]). Let X1, . . . ,XN be independent random variables with Xi \u2208 [0, 1] for all i, and \u00b5 = ( \u2211 i EXi ) /N . Then, Pr [\u2223 \u2223 1 N \u2211 iXi\u2212\u00b5 \u2223 \u2223 > \u01eb ] \u2264 2e\u22122\u01eb2N .\nLemma A.2 (Bernstein\u2019s inequality; see Theorem 1.2 in [23]). Let X1, . . . ,XN be independent random variables with |Xi| \u2264 b, E[Xi] = 0 for all i, and let \u03c32 = \u2211 i Var[Xi]. Then, Pr [\n|\u2211iXi| > t ] \u2264 2 exp (\n\u2212 t2 2(\u03c32+bt/3) ) ."}, {"heading": "B Sample-size dependence of [2, 3, 4] on n for \u21131-reconstruction", "text": "We view P = (p1, . . . , pk) as an n \u00d7 k matrix. Recall that r = \u2211kt=1 wtptp \u2020 t , A = \u2211k t=1 wt(p t \u2212 r)(pt \u2212 r)\u2020, and M = rr\u2020 + A. Let wmax := maxt wt. We consider isotropic k-mixture sources, which is justified by Lemma 3.3. So 12n \u2264 ri \u2264 2n for all i \u2208 [n]. Note that \u2016r\u20161, \u2016r\u201622, and \u2016r\u2016\u221e are all \u0398 (\n1 n\n)\n. It will be convenient to split the width parameter \u03b6 into two parameters. Let (i) \u03b61\u221a n = minp,q\u2208P,p 6=q \u2016p \u2212 q\u20162; and (ii) \u03b622\u2016r\u2016\u221e be the smallest non-zero eigenvalue of A. Then, the width of (w,P ) is \u03b6 = min{\u03b61, \u03b62}. We use \u03c3i(Z) to denote the i-th largest singular value of a matrix Z. If Z has rank \u2113, its condition number is given by \u03ba(Z) := \u03c31(Z)/\u03c3\u2113(Z). For a square matrix Z with real eigenvalues, we use \u03bbi(Z) to denote the i-th largest eigenvalue of Z. Note that if Z is an n \u00d7 k matrix, then \u03c3i(Z)2 = \u03bbi(ZZ\u2020) = \u03bbi(Z\u2020Z) for all i = 1, . . . , k. Also the singular values of ZZ\u2020 coincide with its eigenvalues, and the same holds for Z\u2020Z.\nWe now proceed to evaluate the sample-size dependence of [2, 3, 4] on n for reconstructing the mixture constituents within \u21131-distance \u01eb. Since these papers use different parameters than we do, in order to obtain a meaningful comparison, we relate their bounds to our parameters \u03b61, \u03b62; we keep track of the resulting dependence on n but ignore the (polynomial) dependence on other quantities. We show that the sample size needed is at least \u2126 ( n4\n\u01eb2\n)\n, with the exception of Algorithm\nB in [3], which needs \u2126 ( n3\n\u01eb2\n)\nsamples. As required by [2, 3, 4], we assume that P has full column rank. It follows that M has rank k and A has rank k \u2212 1. The following inequality will be useful. Proposition B.1. Let D = diag(d1, . . . , dk) where d1 \u2265 d2 \u2265 . . . \u2265 dk > 0. Then \u03bbk(PDP \u2020) \u2265 dk\u03bbk(PP \u2020) = dk\u03c3k(P )2.\nComparison with [4]. The algorithm in [4] requires also that P be \u03c1-separable. This means that for every t \u2208 [k], there is some i \u2208 [n] such that pti \u2265 \u03c1 and pt \u2032 i = 0 for all t \u2032 6= t. This has the\nfollowing implications. For any t, t\u2032 \u2208 [k], t 6= t\u2032, we have \u2016pt \u2212 pt\u2032\u20162 \u2265 \u221a 2\u03c1, so \u03b61\u221a\nn \u2265\n\u221a 2\u03c1. We can\nwrite P \u2020P = Y + Z, where Y is a PSD matrix, and Z is a diagonal matrix whose diagonal entries are at least \u03c12. So \u03bbk(P \u2020P ) = \u03bbk(PP \u2020) \u2265 \u03c12. Therefore,\n\u03b622\u2016r\u2016\u221e + \u2016r\u201622 = \u03bbk(A) + \u2016r\u201622 \u2265 \u03bbk(M) \u2265 wmin \u00b7 \u03c12\nwhere the first inequality follows from Lemma 2.2, and the second from Proposition B.1. It follows that \u03c1 = O (\n1\u221a n\n)\n. The bound in [4] to obtain \u2113\u221e error \u03b5 is (ignoring dependence on other quantities)\n\u2126 ( 1 \u03b52\u03c16 ) . So setting \u03b5 = \u01ebn to guarantee \u21131-error at most \u01eb and plugging in the above upper bounds on \u03c1, we obtain that the sample size is \u2126 ( n5\n\u01eb2\n)\n.\nComparison with [2]. The sample size required by [2] for the latent Dirichlet model for obtaining \u21132 error \u03b5 is \u2126 (\n1 \u03b52\u03c3k(P )6\n)\n. Proposition B.1 yields \u03bbk(M) \u2265 wmin \u00b7 \u03c3k(P )2 and as argued above, \u03bbk(M) \u2264 \u03bbk(A) + \u2016r\u201622 = O ( 1 n ) . So \u03c3k(P ) 6 = O ( 1 n3 ) . Setting \u03b5 = \u01eb\u221a n for \u21131 error \u01eb, this yields a bound of \u2126 ( n4\n\u01eb2\n)\n.\nComparison with [3]. Algorithm A in [3] requires sample size \u2126 ( 1 \u03c3k(P )8\u03c3k(M)4\u03b52 ) to recover each pt to within \u21132-distance \u03b5maxp\u2208P \u2016p\u20162. Since maxp\u2208P \u2016p\u20162 \u2264 2wmin\u221an due to isotropy, we can set \u03b5 = \u01ebwmin2 to obtain \u21131-error \u01eb. Since \u03c3k(P ) 2 and \u03c3k(M) = \u03bbk(M) are both O ( 1 n ) , we obtain a bound of \u2126 ( n8\n\u01eb2\n)\n.\nAlgorithm B in [3] uses sample size \u2126 ( \u03ba(P )8/ ( \u03b62 1\nn \u00b7 \u03c3k(M)2\u03b52 )\n)\nto recover each pt to within \u21132-\ndistance \u03b5maxp\u2208P \u2016p\u20162. Clearly \u03ba(P ) \u2265 1. Again, setting \u03b5 = \u01ebwmin2 , this yields a sample size of \u2126 ( n3\n\u01eb2\n)\nfor \u21131 error \u01eb."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>We give an algorithm for learning a mixture of unstructured distributions. This problem<lb>arises in various unsupervised learning scenarios, for example in learning topic models from<lb>a corpus of documents spanning several topics. We show how to learn the constituents of<lb>a mixture of k arbitrary distributions over a large discrete domain [n] = {1, 2, . . . , n} and the<lb>mixture weights, using O(n polylogn) samples. (In the topic-model learning setting, the mixture<lb>constituents correspond to the topic distributions.)<lb>This task is information-theoretically impossible for k > 1 under the usual sampling process<lb>from a mixture distribution. However, there are situations (such as the above-mentioned topic<lb>model case) in which each sample point consists of several observations from the same mixture<lb>constituent. This number of observations, which we call the \u201csampling aperture\u201d, is a crucial<lb>parameter of the problem.<lb>We obtain the first bounds for this mixture-learning problem without imposing any assump-<lb>tions on the mixture constituents. We show that efficient learning is possible exactly at the<lb>information-theoretically least-possible aperture of 2k \u2212 1. Thus, we achieve near-optimal de-<lb>pendence on n and optimal aperture. While the sample-size required by our algorithm depends<lb>exponentially on k, we prove that such a dependence is unavoidable when one considers general<lb>mixtures.<lb>A sequence of tools contribute to the algorithm, such as concentration results for random<lb>matrices, dimension reduction, moment estimations, and sensitivity analysis.", "creator": "LaTeX with hyperref package"}}}