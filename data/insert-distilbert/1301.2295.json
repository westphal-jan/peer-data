{"id": "1301.2295", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2013", "title": "Recognition Networks for Approximate Inference in BN20 Networks", "abstract": "we also propose using recognition networks for identifying approximate inference inbayesian networks ( bns ). often a recognition network is literally a multilayerperception ( in mlp ) trained periodically to predict posterior marginals among given observedevidence in a particular comparison bn. the incoming input to the mlp is a vector vector of two thestates of supplying the correlated evidential nodes. the activity of fitting an expert output unit isinterpreted as a prediction of the posterior marginal of thecorresponding variable. the mlp inference is trained using samples generated fromthe corresponding bn. we normally evaluate a recognition network that was professionally trained to do inference ina large bayesian network, reasonably similar in structure and complexity to thequick medical reference, decision theoretic ( qmr - dt ). our networkis a binary, two - layer, noisy - or network containing over 4000 potentially observable nodes and over 600 unobservable, apparently hidden nodes. inreal medical diagnosis, most observables are unavailable, and there isa complex and unknown bias signal that selects indicates which ones are provided. weincorporate a very basic type of selection bias in our network : a knownpreference that available observables are wholly positive rather than totally negative. even this simple bias has a significant effect on the posterior. we efficiently compare the prior performance of establishing our recognition network tostate - of - the - art approximate inference algorithms on a large set oftest cases. in order to physically evaluate the effect of our simplistic modelof the selection bias, we evaluate algorithms using a higher variety ofincorrectly modeled local observation matching biases. recognition networks performwell using both correct and locally incorrect observation biases.", "histories": [["v1", "Thu, 10 Jan 2013 16:25:25 GMT  (888kb)", "http://arxiv.org/abs/1301.2295v1", "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)"]], "COMMENTS": "Appears in Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI2001)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["quaid morris"], "accepted": false, "id": "1301.2295"}, "pdf": {"name": "1301.2295.pdf", "metadata": {"source": "CRF", "title": "Recognition Networks for Approximate Inference in BN20 Networks", "authors": ["Quaid Morris"], "emails": ["quaid@gatsby.ucl.ac.uk"], "sections": null, "references": [{"title": "AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks", "author": ["Cheng", "Druzdzel", "J. 2000] Cheng", "M.J. Druzdzel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cheng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2000}, {"title": "Sequentially fitting \"in\u00ad clusive\" trees for inference in noisy-OR networks", "author": ["Frey et al", "B.J. 2001] Frey", "R. Patrascu", "T.S. Jaakkola", "J. Moran"], "venue": "In Advances in Neural Information Processing Sys\u00ad tems,", "citeRegEx": "al. et al\\.,? \\Q2001\\E", "shortCiteRegEx": "al. et al\\.", "year": 2001}, {"title": "Prob\u00ad abilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base: II. Evalua\u00ad", "author": ["Middleton et al", "B. 1991] Middleton", "M.A. Shwe", "D.E. Beckerman", "M. Henrion", "E.J. Horvitz", "H.P. Lehmann", "G.F. Cooper"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1991\\E", "shortCiteRegEx": "al. et al\\.", "year": 1991}], "referenceMentions": [], "year": 2011, "abstractText": "A recognition network is a multilayer per\u00ad ception (MLP) trained to predict posterior marginals given observed evidence in a par\u00ad ticular Bayesian network. The input to the MLP is a vector of the states of the eviden\u00ad tial nodes. The activity of an output unit is interpreted as a prediction of the posterior marginal of the corresponding variable. The MLP is trained using samples generated from the corresponding Bayesian network. We evaluate a recognition network that was trained to do inference in a large Bayesian network, similar in structure and complex\u00ad ity to the Quick Medical Reference, Decision Theoretic (QMR-DT) network. Our network is a binary, two-layer, noisy-OR (BN20) net\u00ad work containing over 4000 potentially observ\u00ad able nodes and over 600 unobservable, hidden nodes. In real medical diagnosis, most ob\u00ad servables are unavailable, and there is a com\u00ad plex and unknown process that selects which ones are provided. We incorporate a very ba\u00ad sic type of selection bias in our network: a known preference that available observables are positive rather than negative. Even this simple bias has a significant effect on the pos\u00ad terior. We compare the performance of our recogni\u00ad tion network to state-of-the-art approximate inference algorithms on a large set of test cases. In order to evaluate the effect of our simplistic model of the selection bias, we eval\u00ad uate algorithms using a variety of incorrectly modelled selection biases. Recognition net\u00ad works perform well using both correct and incorrect selection biases. \u2022 also affiliated with Department of Brain and Cogni\u00ad tive Sciences at MIT", "creator": "pdftk 1.41 - www.pdftk.com"}}}