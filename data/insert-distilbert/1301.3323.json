{"id": "1301.3323", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "abstract": "learning image invariant representations from visible images is one of the hardest creative challenges facing computer vision. spatial pooling is widely used to repeatedly create invariance to spatial shifting, but it is commonly restricted to convolutional models. in this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. it is trained to improve the temporal coherence of features, while keeping the information loss at minimum. our method does not use spatial information, so it can be used informally with non - convolutional models too. experiments on images extracted from natural videos showed that our method can cluster similar features together. traditionally when trained exactly by convolutional features, auto - pooling outperformed traditional spatial pooling on an image classification coordination task, even though it does not use the spatial topology of geographical features.", "histories": [["v1", "Tue, 15 Jan 2013 12:47:39 GMT  (856kb)", "https://arxiv.org/abs/1301.3323v1", "9 pages, 10 figures"], ["v2", "Wed, 16 Jan 2013 06:05:10 GMT  (856kb)", "http://arxiv.org/abs/1301.3323v2", "9 pages, 10 figures"], ["v3", "Wed, 23 Jan 2013 16:39:30 GMT  (860kb)", "http://arxiv.org/abs/1301.3323v3", "9 pages, 10 figures. Submission for ICLR 2013"], ["v4", "Mon, 18 Mar 2013 07:19:31 GMT  (1034kb)", "http://arxiv.org/abs/1301.3323v4", "9 pages, 10 figures. Submission for ICLR 2013"]], "COMMENTS": "9 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["sainbayar sukhbaatar", "takaki makino", "kazuyuki aihara"], "accepted": false, "id": "1301.3323"}, "pdf": {"name": "1301.3323.pdf", "metadata": {"source": "CRF", "title": "Auto-pooling: Learning to Improve Invariance of Image Features from Image Sequences", "authors": ["Sainbayar Sukhbaatar"], "emails": ["sainaa@sat.t.u-tokyo.ac.jp", "mak@sat.t.u-tokyo.ac.jp", "aihara@sat.t.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 1.\n33 23\nv4 [\ncs .C\nV ]\n1 8\nLearning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spatial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images extracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed traditional spatial pooling on an image classification task, even though it does not use the spatial topology of features."}, {"heading": "1 Introduction", "text": "The main difficulty of object recognition is that the appearance of an object can change in complex ways. To build a robust computer vision, one needs representations that are invariant to various transformations. The concept of invariant features dates back to Hubel and Wiesel\u2019s seminal work [7], in which cells in a cat\u2019s visual cortex are studied. They found two types of cells: simple cells that responded to a specific pattern at a specific location, and complex cells that showed more invariance to location and orientation.\nInspired by simple and complex cells, the spatial pooling step is introduced to computer vision architectures along with the convolution step [4, 12, 13]. In the convolution step, the same feature is applied to different locations. Then in the pooling step, responses from nearby locations are pooled together (typically with a sum or a max operation) to create invariance to small spatial shifting. However, spatial pooling only works with convolutional models. Also, spatial pooling only improves the invariance to spatial shifting.\nAn ideal pooling should make features invariant to major types of transformations that appear in the nature. For example, to distinguish people from other objects, one need a representation that is invariant to various transformations of the human body. The complexity of such transformations creates the necessity of more adaptive pooling that does not rely on manual fixed configurations. One promising way to obtain such an adaptive pooling is an unsupervised learning approach.\nIn recent years, more adaptive spatial pooling methods have been proposed. Jia et al. [9] showed that it is possible to learn custom pooling regions specialized for a given classification task. The training starts with many pooling region candidates, but only a few of them are used in the final classification. This selection of pooling regions is achieved by supervised learning incorporated into the training of a classifier. Although this method learns pooling regions from data, it is still restricted\nto spatial shifting. Further, it is not suited for deep learning, where lower layers are trained in an unsupervised way.\nAnother method that improves spatial pooling is proposed by Coates and Ng [3], in which local features are clustered by their similarity. A similarity metric between features is defined from their energy correlations. Then, nearby features from the same cluster are pooled together to create rotation invariance. However, the invariance to spatial shifting is still achieved through the same spatial pooling, which restricts this model to convolutional models.\nBeside from spatial pooling, there are methods [8, 10, 16] that create invariance by placing features on a two-dimensional topographic map. During training, nearby features are constrained to be similar to each other. Then, invariant representations are achieved by clustering features in a local neighborhood. However, those methods fix clustering manually, which restricts clusters from having adaptive sizes that depend on the nature of their features. Also, we cannot guarantee that the optimal feature clustering can be mapped into two-dimensional space. For example, edge detectors have at least four dimensions of variation, so an ideal clustering can be achieved by placing edge detectors in a four-dimensional space and grouping nearby features. It will be difficult to approximate such a clustering with a two-dimensional map. Moreover, those methods cannot be used with features already learned by another model.\nSlowness has been used in many methods as a criterion for invariant features [1, 15, 2, 6]. The intuition is that if a feature is invariant to various transformations, then its activation should change slowly when presented with an image sequence containing those transformations. Mobahi et al. [15] incorporated unsupervised slowness learning with supervised back-propagation learning, which improved the classification rate. However, our focus is a simple unsupervised method that can make features invariant without changing them, so it can easily replace and improve spatial pooling in any application.\nIn this paper, we propose auto-pooling, a novel pooling method that learns soft clustering of features from image sequences in an unsupervised way. Our method improves the invariance of features using temporal coherence of image sequences. Two consecutive frames are likely to contain the same object, so auto-pooling minimizes the distance between their pooled representations. At the same time, the information loss due to pooling is also minimized. This is done by minimizing the reconstruction error in the same way as auto-encoders [17, 18]. Through experiments, we show that our method can pool similar features and increase accuracy of an image classification task.\nThere are several advantages in auto-pooling over traditional spatial pooling (see Figure 1). First, it produces invariance to all types of transformations that present in natural videos. Second, autopooling is a more biologically plausible model for complex cells because its parameters are learned from image sequences rather than being manually defined. Third, auto-pooling can be used with non-convolutional models because it does not use spatial information."}, {"heading": "2 Auto-pooling", "text": "Auto-pooling is a pooling method that learns transformations appeared in image sequences. It is trained by image sequences in an unsupervised way to make features more invariant. The goal of training is to cluster similar features together, so that small transformations would not affect pooled representations. Two features are considered similar if they are traversable by a small transformation such as shift or rotation. We use natural videos as the resource for learning similarity between\nfeatures, because they are rich in various complex transformations. Moreover, image sequences are available to animals and humans as early as their birth, so it is biologically plausible to use them in learning of complex-cell-like invariant features.\nWe believe that there are two desirable properties in good pooling methods. The first property is that if two images show the same object, then their pooled representations should be the same. Auto-pooling tries to meet this invariance property by minimizing the distance between pooled representations of consecutive frames, which are likely to contain the same object. The second desirable property is that the information loss due to pooling should be minimal. This is the same as maximizing the cross entropy between inputs and their pooled representations. This entropy property could be obtained by minimizing the error between inputs and reconstructions from their pooled representations.\nInstead of image sequences, it is convenient to use image pairs taken from consecutive video frames as training data. Such image pairs can be written as\nX = {x1,x \u2032 1,x2,x \u2032 2, ...,xN ,x \u2032 N}.\nIf an object is present in image xi, then it is likely to be present in x\u2032i too, because frames xi and x \u2032 i have very small time difference (only 33ms for videos with 30 frames per second). Let us assume that the low-level feature extraction is done by\nyi = f(xi), y \u2032 i = f(x \u2032 i).\nHere, f can be any function as long as yi, y\u2032i are non-negative.\nIn auto-pooling, clustering of features is parameterized by a pooling matrix P . We require all elements of P to be non-negative because they represent the associations between features and clusters. Pij = 0 means that j-th feature does not belong to i-th cluster. On the other hand, large Pij indicates that i-th cluster contains j-th feature. Then, pooling is done by a simple matrix multiplication, that is\nzi = Pyi, z \u2032 i = Py \u2032 i.\nIf the dimension of feature vectors yi,y\u2032i is M , and the dimension of pooled representations zi, z \u2032 i is K , then P is a K\u00d7N matrix. While in spatial pooling, elements of P are fixed to 0 or 1 based on the topology of feature maps, auto-pooling generalizes it by allowing Pij to take any non-negative value.\nOur main goal is to learn pooling parameters Pij from data, without using the explicit spatial information. Training of auto-pooling is driven by two cost functions. The first cost function\nJ1 = 1\nN\nN\u2211\ni=1\n1 2 \u2016zi \u2212 z \u2032 i\u2016 2 2\nis for the invariance property, and minimizes the distance between pooled representations zi and z\u2032i. However, there is a trivial solution of P = 0 if we use only this cost function.\nThe second cost function corresponds to the entropy property, and encourages pooled representations to be more informative of their inputs. Input yi and y\u2032i are reconstructed from their pooled\nrepresentations by y\u0302i = P T zi, y\u0302 \u2032 i = P T z \u2032 i\nusing the same parameters as the pooling step. Then, the reconstruction error is minimized by the cost function of\nJ2 = 1\nN\nN\u2211\ni=1\n1 2 (\u2016yi \u2212 y\u0302i\u2016 2 2 + \u2016y \u2032 i \u2212 y\u0302 \u2032 i\u2016 2 2).\nThis prevents auto-pooling from throwing away too much information for the sake of invariance.\nAuto-pooling is similar to auto-encoders, which are used in feature learning. In auto-pooling, the input is reconstructed in the same way as auto-encoders. Also, the reconstruction cost function J2 is exactly same as the cost function of auto-encoders. However, there are several important differences between them. First, parameters of auto-pooling are restricted to non-negative values. Second, activation functions of auto-pooling are linear and have no biases. Third, auto-pooling has an additional cost function for temporal coherence.\nThe final cost function of auto-pooling is\nJ = \u03bbJ1 + J2,\nwhere the parameter \u03bb \u2265 0 controls the weight of the invariance cost function. Larger \u03bb will make features more invariant by discarding more information. The training is done by minimizing the cost function with a simple gradient descent algorithm."}, {"heading": "2.1 Invariance Score", "text": "For evaluating our pooling model, we define a score for measuring invariance of features (a similar score is also introduced in [5]). A simple measurement of feature\u2019s invariance is its average activation change between two neighboring frames, which is\nG = 1\nN\nN\u2211\nn=1\n\u2016g(xn)\u2212 g(x \u2032 n)\u20162.\nHere, g(x) := f(x) if we are measuring invariance of raw features, and g(x) := Pf(x) if we are measuring invariance of pooled representations.\nFor invariant features, G should be small. However, features can cheat by simply making its activation constant to reduce G , which is obviously not useful. An ideal invariant feature should take the same value only if the stimuli are from consecutive frames. For frames chosen from random timings, an invariant feature should have different activities because it is likely that the inputs contain different objects. Therefore, the average distance between two random frames\nH = 1\nN\nN\u2211\nn=1\n\u2016g(xn)\u2212 g(x \u2032 \u03c3(n))\u20162\nshould be large for invariant features. Here \u03c3 is a random permutation of {1, 2, ..., N}. The invariance score is defined as\nF = H\nG , (1)\nwhich will be large only if a feature is truly invariant."}, {"heading": "3 Experiments", "text": "We will show the effectiveness of our method with two types of experiments. In the first experiment, we train an auto-pooling model with non-convolutional features. The goal of this experiment is to see whether similar features are being pooled together. We also measured the invariance score of features before and after pooling. In the second experiment, we compared our method with traditional spatial pooling on an image classification task."}, {"heading": "3.1 Clustering of Image Features", "text": "The goal of this set of experiments is to analyze feature clusters learned by auto-pooling. We prepared a dataset of 16\u00d716 gray-scale patch pairs from natural videos1. Patch pairs are extracted from random locations of consecutive frames. Some of the patch pairs are shown in Figure 3.\nWe used a sparse auto-encoder to learn 400 features from patches. Then, we trained an auto-pooling model on those features. Since auto-pooling performs soft clustering, it is hard to visualize the clustering result. For simplicity, we used a small threshold to show some of the learned clusters in Figure 4, where each column represents a single cluster. For i-th cluster, we showed features with Pij > \u03b5. It is evident that similar features are clustered together. Also, one can see that the size of a cluster vary depending on the nature of its features.\nTo display clusters more clearly, some clusters of edge detectors are shown in more detail in Figure 5, in which edge detectors are replaced by corresponding thin lines. This allows us to see the diversity of edge detectors inside each cluster. The important thing is that there is variance in orientations\n1We used 44 short videos. All videos are obtained from http://www.vimeo.com and had the Creative Commons license. Although we tried to include videos with the same objects as CIFAR10 (a labeled image dataset used in the next experiment), image patches extracted from the videos were qualitatively different than images from CIFAR10. Even if a video contains a desired object, not all frames show the object. Also, most patches only included a part of an object, because the patch size was much smaller than the frame size,\nas well as in locations, which means that auto-pooling can create representations invariant to small rotations.\nNext, we analyzed the effect of pooling on image features using our invariance score. Figure 6 shows invariance scores measured at various values of \u03bb, which controls the weight of the invariance cost. The invariance score is significantly improved by the pooling, especially for large \u03bb values. It is not surprising because larger \u03bb puts more importance on the invariance cost, thus makes pooled representations less likely to change between consecutive frames. As a result, G in equation 1 becomes smaller and increases the invariance cost.\nAt the same time, however, increase of \u03bb diminishes the role of the reconstruction cost, which was preventing the loss of too much information. Too large \u03bb makes pooled representations overinvariant, having constant values all the time. This results in a small H and decreases the invariance cost. This side effect of large \u03bb can be observed from Figure 6, where the invariance score stopped its increase at large \u03bb."}, {"heading": "3.2 Image Classification", "text": "Next, we tested the effectiveness of our pooling method by applying it to an image classification task. We used two datasets. For image classification, we used CIFAR10 dataset [11], which contains 50 thousand labeled images from ten categories. All images were 32\u00d732 pixels in size, and had threecolor channels. For training of an auto-pooling model, we prepared a patch pair dataset in the same way as the previous experiment, except patches were 32\u00d732 color images to match CIFAR10 images. Some samples from the patch pair dataset are shown in Figure 7.\nIn the feature extraction step, we used a convolutional model. We trained a sparse auto-encoder with 100 hidden units on 6\u00d76 small patches extracted from CIFAR10 images. Learned local features are shown in Figure 8. Then, those 6\u00d7 6 local features are duplicated to all possible locations of 32\u00d732 images, resulting in 100 feature maps of 27\u00d7 27.\nThe convolutional feature extraction produced very large (exceeding 100 gigabytes) training data for auto-pooling. Luckily, the training took only few hours because we implemented our algorithm on a graphic card (Tesla K20c) using CUDAMat library [14]. Some of the learned feature clusters are visualized in Figure 9. For each cluster, we showed 15 feature maps with the largest pooling area (i.e. maxk( \u2211 j\u2208Sk\nPij), where Sk is the set of features in k-th feature map). Pij = 0 is shown in gray and large Pij is shown in white. Local features corresponding to the feature maps are also shown in Figure 9.\nUnlike spatial pooling, each cluster learned by auto-pooling extended to multiple feature maps. Pooling regions (i.e., white areas in Figure 9) of those maps usually have the same continues spatial distribution, which will create spatial invariance in the same way as spatial pooling. If we observe those pooling regions carefully, however, we can see the small variance in their locations. This location variance is inversely related to the location variance of corresponding local features. For example, if there is a edge detector in the lower part of a 6\u00d7 6 local feature, corresponding pooling regions will have upper position in 27\u00d7 27 feature maps.\nBeside from pooling by spatial areas, auto-pooling also succeeded in clustering similar local features. In some clusters, edge detectors of similar orientations are grouped together. This will make pooled representations invariant to small rotations, which is a clear advantage over traditional spatial pooling. In addition, clustering of local features only differing in their locations will reduce the redundancy created by convolutional feature extraction.\nWe compared our pooling method with traditional spatial pooling on a classification task, in which a supervised classifier is trained by pooled representations of labeled images. For auto-pooling, we varied the number of clusters from 400 to 2500. For spatial pooling, we can only change the grid size. However, it is possible to use multiple spatial pooling at once [12] to produce better results. We denote a spatial pooling that used 2\u00d7 2 and 3\u00d7 3 grids by 2\u00d7 2 + 3\u00d7 3.\nIn classification, we trained a linear SVM with pooled representations. The results are shown in Table 1. We trained the classifier with two training data: a full data with 5000 examples per class, and a smaller one with 1000 examples per class. Since the number of features is an important factor in classification, we plotted the accuracy of the two pooling methods against the number of clusters in Figure 10. Auto-pooling outperformed traditional spatial pooling for most of the time. Especially for small training data, the difference between the two pooling methods was substantial. This indicates that auto-pooling is better at generalization, which is the main goal of invariant features. The spatial pooling, on other hand, shows the sign of over-fitting when its pooling regions are increased."}, {"heading": "4 Conclusions", "text": "In this paper, we introduced auto-pooling, a novel pooling method that can generalize traditional spatial pooling to transformations other than spatial shifting. Auto-pooling tries to make features more temporally coherent, having slow changing activation when presented with a continues image sequence. The information loss due to pooling is kept minimum using the same cost function as autoencoders. The main advantage of our method is that it learns to cluster features from data, rather than relying on manual heuristic spatial divisions. Therefore, auto-pooling is a more biologically plausible model for complex cells.\nWhen trained by image pairs extracted from natural videos, auto-pooling successfully clustered similar features together. We showed that such clustering could significantly improve the invariance of features. Also, our pooling model was more effective than traditional spatial pooling when it was used in a real-world classification task, where spatial pooling had the advantage of using spatial information of features.\nIn our experiments, the advantage of auto-pooling over spatial pooling was mainly restricted to learning of rotation invariance. This is because auto-pooling is applied to low-level features, which were mostly edge detectors with the size. Therefore, the only possible variance beside spatial shifting was rotation. We believe that if we use auto-pooling instead of spatial pooling in deep architectures, we can create invariance to more complex transformations such as three-dimensional rotations and distortions."}, {"heading": "Acknowledgments", "text": "This research is supported by the Aihara Innovative Mathematical Modelling Project, the Japan Society for the Promotion of Science (JSPS) through the \u201cFunding Program for World-Leading\nInnovative R&D on Science and Technology (FIRST Program),\u201d initiated by the Council for Science and Technology Policy (CSTP)."}], "references": [{"title": "Slow feature analysis yields a rich repertoire of complex cell properties", "author": ["P. Berkes", "L. Wiskott"], "venue": "Journal of Vision,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Learning transformational invariants from natural movies", "author": ["C. Cadieu", "B. Olshausen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1980}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "H. Lee", "A. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Emergence of complex-like cells in a temporal product network with local receptive fields", "author": ["K. Gregor", "Y. LeCun"], "venue": "arXiv preprint arXiv:1006.0448,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Receptive fields, binocular interaction and functional architecture in the cat\u2019s visual cortex", "author": ["D. Hubel", "T. Wiesel"], "venue": "The Journal of Physiology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1962}, {"title": "Topographic independent component analysis", "author": ["A. Hyv\u00e4rinen", "P. Hoyer", "M. Inki"], "venue": "Neural Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Beyond spatial pyramids: Receptive field learning for pooled image features", "author": ["Y. Jia", "C. Huang", "T. Darrell"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Learning invariant features through topographic filter maps", "author": ["K. Kavukcuoglu", "M. Ranzato", "R. Fergus", "Y. LeCun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Master\u2019s thesis,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "CUDAMat: a CUDA-based matrix class for Python", "author": ["V. Mnih"], "venue": "Technical report, Department of Computer Science, University of Toronto,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Deep learning from temporal coherence in video", "author": ["H. Mobahi", "R. Collobert", "J. Weston"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Topographic product models applied to natural scene statistics", "author": ["S. Osindero", "M. Welling", "G. Hinton"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["M. Ranzato", "S. Chopra", "Y. LeCun"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "The concept of invariant features dates back to Hubel and Wiesel\u2019s seminal work [7], in which cells in a cat\u2019s visual cortex are studied.", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "Inspired by simple and complex cells, the spatial pooling step is introduced to computer vision architectures along with the convolution step [4, 12, 13].", "startOffset": 142, "endOffset": 153}, {"referenceID": 11, "context": "Inspired by simple and complex cells, the spatial pooling step is introduced to computer vision architectures along with the convolution step [4, 12, 13].", "startOffset": 142, "endOffset": 153}, {"referenceID": 12, "context": "Inspired by simple and complex cells, the spatial pooling step is introduced to computer vision architectures along with the convolution step [4, 12, 13].", "startOffset": 142, "endOffset": 153}, {"referenceID": 8, "context": "[9] showed that it is possible to learn custom pooling regions specialized for a given classification task.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Another method that improves spatial pooling is proposed by Coates and Ng [3], in which local features are clustered by their similarity.", "startOffset": 74, "endOffset": 77}, {"referenceID": 7, "context": "Beside from spatial pooling, there are methods [8, 10, 16] that create invariance by placing features on a two-dimensional topographic map.", "startOffset": 47, "endOffset": 58}, {"referenceID": 9, "context": "Beside from spatial pooling, there are methods [8, 10, 16] that create invariance by placing features on a two-dimensional topographic map.", "startOffset": 47, "endOffset": 58}, {"referenceID": 15, "context": "Beside from spatial pooling, there are methods [8, 10, 16] that create invariance by placing features on a two-dimensional topographic map.", "startOffset": 47, "endOffset": 58}, {"referenceID": 0, "context": "Slowness has been used in many methods as a criterion for invariant features [1, 15, 2, 6].", "startOffset": 77, "endOffset": 90}, {"referenceID": 14, "context": "Slowness has been used in many methods as a criterion for invariant features [1, 15, 2, 6].", "startOffset": 77, "endOffset": 90}, {"referenceID": 1, "context": "Slowness has been used in many methods as a criterion for invariant features [1, 15, 2, 6].", "startOffset": 77, "endOffset": 90}, {"referenceID": 5, "context": "Slowness has been used in many methods as a criterion for invariant features [1, 15, 2, 6].", "startOffset": 77, "endOffset": 90}, {"referenceID": 14, "context": "[15] incorporated unsupervised slowness learning with supervised back-propagation learning, which improved the classification rate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "This is done by minimizing the reconstruction error in the same way as auto-encoders [17, 18].", "startOffset": 85, "endOffset": 93}, {"referenceID": 17, "context": "This is done by minimizing the reconstruction error in the same way as auto-encoders [17, 18].", "startOffset": 85, "endOffset": 93}, {"referenceID": 4, "context": "For evaluating our pooling model, we define a score for measuring invariance of features (a similar score is also introduced in [5]).", "startOffset": 128, "endOffset": 131}, {"referenceID": 10, "context": "For image classification, we used CIFAR10 dataset [11], which contains 50 thousand labeled images from ten categories.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "Luckily, the training took only few hours because we implemented our algorithm on a graphic card (Tesla K20c) using CUDAMat library [14].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "However, it is possible to use multiple spatial pooling at once [12] to produce better results.", "startOffset": 64, "endOffset": 68}], "year": 2013, "abstractText": "Learning invariant representations from images is one of the hardest challenges facing computer vision. Spatial pooling is widely used to create invariance to spatial shifting, but it is restricted to convolutional models. In this paper, we propose a novel pooling method that can learn soft clustering of features from image sequences. It is trained to improve the temporal coherence of features, while keeping the information loss at minimum. Our method does not use spatial information, so it can be used with non-convolutional models too. Experiments on images extracted from natural videos showed that our method can cluster similar features together. When trained by convolutional features, auto-pooling outperformed traditional spatial pooling on an image classification task, even though it does not use the spatial topology of features.", "creator": "LaTeX with hyperref package"}}}