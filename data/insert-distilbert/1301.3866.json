{"id": "1301.3866", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Marginalization in Composed Probabilistic Models", "abstract": "composition of low - dimensional distributions, instruments whose earlier foundations were laid in the papaer published in the proceeding of uai'97 ( jirousek 1997 ), appeared immediately to be an alternative apparatus to describe multidimensional probabilistic models. in contrast to graphical markov models, which define uniformly multidomensinoal distributions in a declarative way, using this approach paradigm is rather procedural. sequential ordering of low - dimensional generalized distributions into a proper sequence fully defines the resepctive computational procedure ; therefore, a stury distinction of different type of generating sequences is one task fo the central problems in undertaking this field. thus, it appears however that an important role is played by special sequences that are called perfect. their main characterization theorems are generally presetned in this theorem paper. however, the main main result of this paper typically is a solution to eliminate the problem of margnialization for general sequences. ideally the main theorem describes a way to obtain a convex generating sequence that defines the model corresponding to the marginal of the distribution defined by an arbitrary genearting sequence. from this theorem the data reader can see to what extent these comutations are local ; i. e., selecting the sequence approximation consists of marginal initial distributions whose computation must specifically be made by summing up over the values of the variable eliminated ( the paper deals with weak finite model ).", "histories": [["v1", "Wed, 16 Jan 2013 15:50:54 GMT  (254kb)", "http://arxiv.org/abs/1301.3866v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["radim jirousek"], "accepted": false, "id": "1301.3866"}, "pdf": {"name": "1301.3866.pdf", "metadata": {"source": "CRF", "title": "Marginalization in Composed Probabilistic Models", "authors": ["Radim Jirousek"], "emails": ["radim@vse.cz"], "sections": [{"heading": null, "text": "Composition of low-dimensional distribu tions, whose foundations were laid in the pa per published in the Proceedings of UAI'97 (Jirousek 1997), appeared to be an alterna tive apparatus to describe multidimensional probabilistic models. In contrast to Graphi cal Markov Models, which define multidimen sional distributions in a declarative way, this approach is rather procedural. Ordering of low-dimensional distributions into a proper sequence fully defines the respective compu tational procedure; therefore, a study of dif ferent types of generating sequences is one of the central problems in this field. Thus, it ap pears that an important role is played by spe cial sequences that are called perfect. Their main characterization theorems are presented in this paper. However, the main result of this paper is a solution to the problem of marginalization for general sequences. The main theorem describes a way to obtain a generating sequence that defines the model corresponding to the marginal of the distri bution defined by an arbitrary generating se quence. From this theorem the reader can see to what extent these computations are lo cal; i.e., the sequence consists of marginal dis tributions whose computation must be made by summing up over the values of the vari able eliminated (the paper deals with a finite model) .\n1 INTRODUCTION\nBayesian networks, perhaps the most famous model for representation of multidimensional probability dis tributions, belong to a wider class of models that are most often called Graphical Markov models. All of\nthese models are proposed to represent distributions of high dimensionality (hundreds or even thousands di mensions), which cannot generally be handled because of the exponential growth of the number of necessary parameters. What is common to all these models is the fact that they can represent distributions with spe cial dependence structures (namely, this feature makes it possible to define the distribution with the aid of a moderate number of parameters), and that these struc tures are described by graphs. The approach presented herein keeps the former property, abandoning the lat ter.\nInstead of representing the dependence structure of a modeled distribution, the presented approach de scribes the computational process that defines the mul tidimensional distribution. Naturally, one can see the dependence structure from this process. But it is not the main goal of this apparatus.\nOur approach is based on the operators of composition, used for construction of the distribution of variables (Xi)iEK1uK2 from two low-dimensional distributions, Pt (Xi)iEK1 and P2 (Xi)iEK2\u2022 These operators, as well as their basic properties, are introduced in the next section. The third section describes the main idea, namely, generating the multidimensional distributions by iterative application of these operators; and the most important generating sequences, called perfect sequences, are characterized. The fourth section is de voted to the main focus of the paper: marginalization of multidimensional distributions defined by generat ing sequences.\n2 NOTATION AND BASIC PROPERTIES\nIn this paper, we will deal with probability distribu tions P ( (Xi)iEK), where both the index set K and all sets of values of variables Xi are assumed to be finite. Different distributions can be (and usually are) defined for different sets of variables. To simplify the notation\n302 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nas much as possible, distributions P1, P2, ... , Pn will always be defined for variables whose indices lie in sets K1, Kz, ... , Kn, respectively. In other words, we are going to consider distributions\nHaving a distribution P (i.e. P((X;)iEK)), we will often consider its marginal distributions. For L C K, the marginal distribution P((X;)iEL) will be denoted by using set L as an upper index in round parentheses: p (\nL). Considering a general L (i.e., Lis not necessarily a part of K), the symbol p(L) will denote the marginal distribution P((X;)iEKn\u00a3). Two distributions P1 and P2 are called consistent if\nP(KtnK2) _ P.(K1nK2) 1 - 2 .\nThe main theorem of this paper deals with marginal izing one variable out, i.e., it describes a form of the marginal distribution p(K\\{f}) forCE K. To simplify the notation, for this type of a marginal distribution we shall use the symbol p[tJ.\nThe most important concept of this contribution is that of the composition operators. To make it clear from the very beginning, let us stress that it is noth ing else but a generalization of the idea of comput ing a three-dimensional distribution from two two dimensional ones by introducing the conditional inde pendence of variables X1 and X3 given X2:\nP1(X1,Xz)Pz(Xz,X3) Pz(X2) = P1 (X1, X2)P2(X3IX2).\nConsider two probability distributions P1 ( (X;)iEKJ and P2((X;)iEK2). A right composition of probability distributions P1 and P2 is defined by the formula\notherwise,\n(P2((X; = x;)iE(K1nK2)) = 0 => PI((X; = x;)iE(K1nK2J) = 0) \u00b7\nTaking 00\u00b0 = 0, the operation of composition, if de fined, results in a probability distribution\n(P1 1> Pz)((X;)iEK1uK2)\nand its marginal distribution (P1 1> P2)((X;)iEKJ (or, using the symbol more often utilized in the sequel,\n(PI 1> Pz)(Kt)) equals P1. If K1 n Kz = 0, then P1 1> Pz degenerates to a simple product of P1 and P2.\nAnalogously, we can also introduce the operator of left composition:\nP1P2 P{KtnK2) undefined otherwise,\nThese operators, when applied iteratively, construct multidimensional distributions from a set of low dimensional ones. In this paper we will primarily con centrate on sequences connected by the operator of right composition:\nP1 I> P2 I> \u2022 \u00b7\u00b7I> Pn.\nThis formula, if it is defined, determines the distribu tion of variables (X;)iEK1uK2u ... uKn. Regarding the fact (see Jirousek 1997) that the operator 1> is neither commutative nor associative, we must stress just this once that we always apply the operators from left to right:\nP1 1> P2 1> ... 1> Pn = ( ... ((P1 1> P2) 1> ?3) 1> ... 1> Pn)\u00b7\nAs can already be seen from the above formula tions, when speaking about properties of generating sequences we often have to distinguish between the situations in which the respective formulae are or are not defined. Describing singular situations with unde fined formulae is, from the point of view of this paper, quite uninteresting. To avoid the necessity of repeat ing technical exercises on each occasion, let us assume that all the formulae are well defined. It can, for ex ample, be guaranteed by an assumption that all the distributions Pk are positive.\nNow, let us introduce a couple of assertions that will be necessary in the next sections. The first two lemmata were proven in (Jirousek 1997).\nLemma 1 P1 and P2 are consistent iff\nLemma 2\nLemma 3 then\nIf K1 :2 (K2 n K3) then\nP1 I> Pz I> P3 = P1 I> P3 1> Pz .\nLet L be such that K1 n K2\n(L} P1 I> P2 = P1 I> P2 I> Pz .\n0\n0\nc L c K2;\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 303\nProof. The assertion follows immediately from the def inition of the composition operator 1>:\nplpJL) p2 P.(LnKI) p(K2n(K,UL)) 2 2\npl pJLl p2 P.(K,nK2) p((K2nK,)UL) 2 2 P1PJL) P2 P.(K,nK2) p(L) 2 2 plp2 \ufffd-=--___,..,....,...) = P1 1> P2. p(K,nK2 2 0\nThe following theorem is of ultimate importance for the assertions from Section 4. There are two additional reasons for our presentation of its proof, despite its having already been proven in (Jirousek 1997). First, the proof presented here is more transparent than the original one, and second, there is a certain license in defining the operator @K, which appears in the as sertion. This arbitrariness, which will be discussed in more detail below, can be seen from the proof.\nTheorem 1\nwhere\nProof.\nTherefore\n=\n(PJ(K, \\K2)nKa) P2) I> p3 P.((K,\\K2)nKa)p. p. 3 2 3\np((K,uK2)nKa) 3\np<(K1uK2)nKa) 3\nwhere the second modification is feasible because\nNotice here that the last modification is just an elim. . d\" .b . p((K, \\K2)nKal \u00b7 ination of the aux1hary 1stn ut10n 3 mtroduced in the definition of the operator \u00aeK,.\nLet us focus our attention on the denomina tor of the last fraction. It is a marginal of a product of P2 with a conditional distribution P3((Xi)iEK3\\(K,uK2JI(Xi)iEKan(K,uK2))\u00b7 When com puting this marginal, we have to sum up over all combi nations of values of variables (Xi)iE(K2uKa)\\K1\u2022 In the following computations we will separate these variables into two groups: (Xi)iEK2\\K1 and (Xi)iEK3\\(K1uK2)\u00b7 Let XK2\\K, and XKa\\(K,uK2) be the sets whose elements are all combinations of values of vari ables (Xi)iEK2\\K, and (Xi)iEKa\\(K,uK2), respectively. x E XK2\\K, is thus a vector of values of variables (Xi)iEK2\\K,, with xi denoting the coordinate which corresponds to the value of variable Xi. Analogously, y E XKa\\(K,uK2) is a vector of values of variables (Xi)iEKa\\(K,uK2) and Yi again denotes the correspond ing value of variable Xi. Using this notation, we can compute:\nxEXK2\\K1 yEXKa\\(K1uK2) P2((Xi)iEK2nK1, (Xi= Xi)iEK2\\K,) P3((Xi = Yi)iEK3\\(K,uK2) I\n(X; = X;)iE(K3nK2)\\K1, (X;)iEKanK,)\n= P2((X;)iEK2nK,) L xEXK2\\Kt\nP2((X; = x;)iEK2\\K1I(X;);EK2nK,)\nL P3((X; = Yi)iEK3\\(K1uK21 yEXKa\\(K1 uK2)\n(X; = Xi)iE(K3nK2)\\K1, (Xi)iEKanK,) = P2((X;)iEK2nK,)\nSubstituting this result back into the denominator of the fraction, we get\nP.(K2nK!) p((K,uK2)nKa) 2 3 = P1 1> P2 1> P3 .\nwhich completes the proof. 0\n304 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nAs we mentioned previously, we could define the @K operator with the aid of an (almost) arbitrary distri bution R\nFor example, an arbitrary positive distribution which is defined for the respective variables will serve well. For the sake of simplicity, it seems reasonable to con sider a uniform distribution. The specific purpose of this distribution is simply to introduce the neces sary conditional independence that would otherwise be omitted. To illustrate the point, let us consider the following trivial example:\nIf we used the operator 1> instead of @K, , we would get\nP1 (XI) 1> (P2 (X2) 1> P3 (X1, X2)) P1 (X1) (P2 (X2)P3 (X1IX2))\n- L P2 (X2)P3 (X1 = xiX2)' xEX1\nwhich evidently differs from P1 (XI)P2 (X2) because P1 1> (P2 1> P3) inherits the dependence of variables X1 and X2 from P3. Nevertheless, considering\nP1 (X1) 1> (P2 (X2) @{l}P3 (X1,X2))\n= P1 (XI) 1> (P3 (X1)P2 (X2) 1> P3(X1, X2))\n= P1 (XI) 1> P3 (XI)P2 (X2)\n= P1 (XI)P2 (X2)\ngives the desired result.\n3 GENERATING SEQUENCES\nUsing operators of composition, we can construct multidimensional distributions from a system of low dimensional ones. As a rule, we consider constructions that apply one of the two introduced operators itera tively. This means we consider either distributions\nor\nSince these formulae generally define different distribu tions, it is reasonable to study both of them. However, though it is perhaps not evident at first sight, these two expressions substantially differ from each other, namely, from the computational point of view. Con sider an index k E { 1, 2, ... , n - 1} which is close to\nn - 1. Application of the k-th operator means the computation of either\n(P1 1> \u2022 \u2022 \u2022 1> Pk)Pk+l (P1 1> \u00b7 \u00b7 \u00b7 I> Pk) I> Pk+l = p(Kk+1n(K1u ... uKk))\nk+l\nfor the application of the operator 1>; or\n- (P1 <1 \u2022 \u2022 \u2022 <1 Pk)(Kk+1n(K,u ... uKk))\nin the latter case. Though the numerators are al most equivalent, and both of the denominators rep resent computation of a I (Kk+l n (K1 U ... U Kk))l dimensional marginal distribution, there is a compu tational difference between these expressions. While in the first case the denominator represents compu tation of a marginal from distribution Pk+l, which is assumed to be low-dimensional, in the latter case one has to marginalize the distribution (P1 <1 \u2022 \u2022 \u2022 <1 Pk), whose dimension can be rather high; more precisely, it is I (Kl U ... UKk)l-dimensional. In practical situations, when the goal is to construct a distribution with di mensionality of several hundreds, these computations become generally intractable (more precisely, no effec tive algorithms have been found). Therefore, we will concentrate mainly on applications of the operator 1>. Nevertheless, there are sequences of distributions for which\nholds true. Among such sequences, an important role is played by those that are called perfect (this no tion was already introduced in (Jirousek 1997)). A sequence of probability distributions P1, P2, ... , Pn is called perfect if for all k = 2, ... , n the equality\nholds true.\nIt is not difficult to show that the class of Bayesian networks is equivalent to the class of perfect sequences in the following sense:\n1. If P1, ... , Pn is perfect then there exists a Bayesian network representing the distribution P1 1> \u2022 \u2022 \u2022 1> Pn such that for each variable Xj there exists k E {1, ... , n} such that\n2. For each Bayesian network one can construct a perfect sequence P1, ... , Pn such that each {Xi}iEKk equals some cl (Xj) = {Xj} U pa (Xj) and P1 1> \u2022 \u2022 \u2022 1> Pn equals the distribution repre sented by the Bayesian network.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 305\nIn other words, there are simple procedures trans forming an arbitrary Bayesian network into a perfect sequence and vice versa; and the distributions defin ing both structures (i.e., respective conditional distri butions defining the Bayesian network and distribu tions from the generating sequence) are of the same dimensionality. An algorithm for reconstruction of a Bayesian network from a perfect sequence can be found in (Jirousek et al. 2000). In fact, this algorithm trans forms any sequence PI, ... , Pn into a Bayesain network representing the distribution Pir> ... r>Pn. What is more important, from our point of view, is the fact that any Bayesian network can be viewed at as a structure con structed from a perfect sequence of low-dimensional distributions. The exact meaning and importance of this statement can be seen from the following charac terization theorem.\nTheorem 2 A sequence of distributions PI, P2 , .. . ,Pn is perfect iff all the distributions from this sequence are marginals of the distribution (PI r> P2 r> . . . r> Pn).\nProof. The fact that all distributions Pk from a perfect sequence are marginals of (PI r>P2r> ... r>Pn) was already stated in Theorem 4 in (Jirousek 1997). It follows from the fact that (PI r> ... r> Pk) is marginal to (PI r> ... r> P n) and Pk is marginal to (PI <J \u2022 \u2022 \u2022 <J Pk).\nSuppose that for all k = 1, . . . , n, Pk are marginal distributions of (PI r> ... r> Pn). Then PI and P2 are consistent, and due to Lemma 1\nSince PI r> P2 is also marginal to (PI r> ... r> Pn), it must be consistent with P3, too. Using Lemma 1 again, we get\nPI r> p2 r> p3 = PI <l p2 <l p3 .\nHowever, PI r> P2 r> P3 being marginal to (PI r> .. . r> Pn) must also be consistent with P4 and we can continue in this manner until we achieve that for all k = 2, . .. , n\n0\nWhat is the most important message conveyed by the previous characterization theorem? A distribution de fined by a perfect sequence is unique, regardless of which of the two operators ( <J or r>) is used. More over, considering that low-dimensional distributions Pk are carriers of local information, the constructed multidimensional distribution represents global infor mation, faithfully reflecting all of the local input. The reader can visualize the situation with an analogy to\na jigsaw puzzle, whose pieces correspond to individual low-dimensional distributions Pk and whose completed picture corresponds to the distribution P1 r> . . . r> Pn. In this case, if the picture is properly assembled, each local piece is fully utilized, no piece of information is lost, and no information that is not included in any Pk is added.\nThere is still another moment worth mentioning to readers who are familiar with the famous Iterative Proportional Fitting Procedure (Deming and Stephan 1940, Csiszar 1975). Since the operator <l describes exactly what is computed by this procedure at each step, (PI <l \u2022 \u2022 \u2022 <l Pn) is the distribution computed by the first cycle (n iterative steps) when the procedure starts with the uniform distribution. Moreover, due to the fact that, for perfect sequences, all distributions Pk are marginal to (PI <l \u2022 \u2022 \u2022 <JPn), the iterative process terminates after the n-th step. Therefore, for perfect sequences the IPFP terminates after the first cycle.\n4 MARGINALIZATION\nWe believe that the apparatus based on composition of distributions from generating sequences is not only an elegant way how to describe multidimensional dis tributions but we hope it will enable us also to de scribe necessary computational procedures. These consist mainly from steps performing conditioning and marginalization. Therefore, in this paper we start studying problems connected with marginalization. The goal, however, is not to describe algorithms per forming this type of computations (it can be done by any of the famous marginalization procedures pro posed for Bayesian networks, see e.g. (Shenoy and Shafer 1990, Shafer and Shenoy 1990)) but to find for mulae based on operators of composition describing the resulting marginal distribution.\nIt is easy to show that generally\nTo see it, consider a simple example of composition of two two-dimensional distributions\nthat yields, generally, a dependence of variables XI and X3. Therefore\n(PI (XI, X2) r> P2 (X2, X3)) ({I, 3})\n=f. (PI (XI, X2)) ({I}) r> (P2 (X2, X3)) ({3}) =PI (XI)P2 (X3) .\nNevertheless, for special situations the following sim ple assertion (Lemma 2 in (Jirousek 1997)) presents\n306 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nsufficient conditions under which equality in the above expression holds true.\nLemma 4 If L 2 KIn K2 then\n(PI t> P2) (L) = pi( L) t> Pi L ) 0\nD\nIn the sequel we will primarily concentrate on the sim plest case: marginalization of one variable out. From this point of view, the following assertion - an imme diate consequence of iterative application of Lemma 1 - is rather interesting:\nLemma 5 Iff! E K; for some i E {1, 2, . . . , n} and f! r/. Kj for all j =/:- i then\n(PIt> P2t> . . . t> Pn)[t] = P1 t> ... t>Pi-1 t> pJRl t> P;+I t> ... t> Pn.\nD\nHowever, situations in which the variable f! that is to be eliminated is contained in several distributions, are much more complicated. The solution to this problem is in fact given by the following theorem, which ex presses the distribution P1 t> . . . t> Pn with the aid of its marginal (P1 t> . .. t> Pn)[\u00a31.\nTheorem 3 Let P1, P2, . . . , Pn be a generating se quence and f! E K;1 n Ki2 n ... n Ki\ufffd for some\n(assuming (it < i2 < . . . < im)) such that f! rf. Kj for all j E {1,2, . . . ,n} \\ {i1,i2, ... ,im}\u00b7 Then\nQ;\ufffd =(Pit @L,2-l P;2 @Li3-l 0 0 0 @L\u00b7\ufffd-1 P;\ufffd)[f], Qn+1 = (P;I @L,2-I P;2 @L,3-I ... @Li\ufffd-I P;\ufffd),\nand L;k-1 = (Kt U K2 U . . . U K;k-t) \\ { \u00a3} . Proof. Let us start proving the theorem for m = 1. Since K I U .. . U K;1 -I does not contain f!, we can apply Lemma 3, which yields\n(PI t> . .. t> P;1-1) t> PJ:l t> P;1\nQ1 t> . . . t> Q;1 t> P;1\nDistribution (QI t> . . . t> Q;1) is defined for (X;)iE(K1u ... uK,1)\\{l}\nand (KI U . . . U K;1) \\{\u00a3} contains K;1 n Kj for all j = ii + 1, . . . , n, because none of these Kj contain \u00a3. Therefore, applying Lemma 2 (n - ii)-times, we get\nP1 t> ... t> P;1 t> P;1+I t> .. . t> Pn\n= Q1 t> . . . t> Q;1 t> P;1 t> P;1+I t> . .. t> Pn\n= Ql t> .. . t> Q;1 t> P;1+I t> ... t> Pn t> P;1\n= Ql t> ... t> Qn+l\u00b7\nNow, assuming the assertion has been proven for m- 1, let us prove it for m. In the following computations we will first use Lemma 3, then Theorem 1, and finally (n - im)-times Lemma 2.\nPI t> ... t> P;\ufffd -1 t> P;\ufffd t> ... t> P;n\n= QI t> \u2022 \u2022 \u2022 t> Qim-I\nt>(P;l @Li2-1 \u2022 \u2022 \u2022 @L;\ufffd-1-1 P;m-1)\nt>P;\ufffd t> . . . t> P;n\n= Ql t> 0 0 0 t> Q;\ufffd-1\nt>(P;l @Li2-1 0 0 0 @L\u00b7\ufffd-1-1 P;\ufffd-1 )[f]\nt>(P;1 @\u00a3,2-1 0 0 0 @L\u00b7\ufffd-t-1 P;\ufffd-1) t> P;\ufffd\nt> P;\ufffd +1 t> ... t> P;n\n= QI t> 0 0 0 t> Q;\ufffd\nt>(P;t @\u00a3,2-1 . . . @L,\ufffd-1-1P;\ufffd-1 @L,\ufffd-1P;\ufffd)\nr>P;\ufffd +1 t> \u2022 . \u2022 t> P;n\nQI t> ... t> Q;\ufffd t> P;\ufffd +1 t> .. . t> P;n\nt>(P;l @\u00a3,2-1 0 0 0 @L\u00b7\ufffd-1 P;\ufffd)\n= Ql t> \u2022 \u2022 \u2022 t> Qin+l\nD\nTheorem 4 Let P1, P2, ... , Pn be a generating se quence and f! E K;1 n K;2 n ... n K;m for some\n{il,i2, .. . ,im} \ufffd {1,2, . . . ,n}\n(assuming (ii < i2 < . . . < im)) such that f! rf. Kj for allj E {1,2, . . . ,n}\\{ii,i2,\u00b7\u00b7\u00b7,im} then\n(Pl t> P2 t> ... t> Pn)[l] = Q1 t> Q2 t> . .. t> Qn,\nwhere the distributions Q1, ... , Qn are defined as in Theorem 3.\nProof. In fact, this assertion is a direct conse quence of the preceding Theorem, which claims that QI t> ... t> Qn is marginal to P1 t> . .. t> Pn. The fact that it is the marginal distribution for vari ables (X;);E(K1u ... uKn)\\{f}\nimmediately follows from the definitions of distributions Q k since Qn+l is the only distribution in whose domain f! occurs.\nD\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 307\nWe will conclude this section with a simple example, illustrating the marginalization formula of Theorem 4. Consider the following generating sequence:\nwhich is, generally, not perfect. (Nevertheless, it can be perfect if P3 (XI, X2, X3) = PI (XI, X3)P2 (X2).) The goal of the following computation is to eliminate variable X I .\nwhich was to be expected.\nLet us stress once more that this is something quite different from\npjil t> P2 t> (PIt> P3 Y IJ\nwhich equals\n( ) ({2,3})\" PI(XI, X3)P3 (X4, X2!XI, X3)\n5 CONCLUSIONS\nWe have presented a contribution to a new appa ratus for representations of multidimensional proba bility distributions, based on composition operators. Although the two basic operators, <1 and t>, are de fined by almost identical formulae, they substantially differ when used iteratively to constitute multidi mensional probabilistic distributions. The difference mostly manifests in the computational complexity of the respective processes.\nWithin this framework, different generating sequences of low-dimensional probability distributions can be studied (Jirousek, 1998). In this paper we defined and characterized only the most important class, that of perfect sequences. The main result of this paper, the orem on marginalization for generating sequences, was, however, formulated for general sequences.\nLet us conclude the paper with two comments con cerning research in the related fields.\nWe made a rather great deal of effort to characterize sequences that define multidimensional models corre sponding to decomposable models. Up to now, we have not received satisfactory results. Naturally, it would be possible to choose perfect sequences PI, ... , Pn for which sets KI, . . . , Kn can be ordered to meet the so called running intersection property (introduced in (Kellerer, 1964)). This would, however, exclude some situations we want to address. For example, if\nis perfect, then the distribution\nis decomposable, because"}, {"heading": "PI (XI, X2) t> P2 (X2, X3) t> P3 (X3, X4) t> P4 (XI, X4)", "text": "=PI (XI, X2) t> P2 (X2, X3) t> P3 (X3, X4),\n308 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nin spite of the fact that the respective sets {1, 2},{2, 3},{3, 4},{1, 4} corresponding to the original sequence P1, P2, P3, P4 cannot be ordered to achieve the running intersection property. Another situation we want to address occurs when all the distributions from a generating sequence P1, ... , Pn are uniform. Then the result, the uniform multidimensional distri bution, is also decomposable regardless of whether the respective sets K 1, ... , Kn can be ordered to meet the running intersection property or not. Thus, the prob lem of how to specify sequences corresponding to de composable models is still open.\nThe second comment goes beyond the probability theory. The operators of composition were also de fined for possibilistic distributions (Vejnarova, 1998). Corresponding to the conditioning introduced in (de Cooman, 1997a - 1997c) they are parameterized by a t-norm, nevertheless they manifest a lot of properties which also hold true for probabilistic operators. These properties, for example, make a definition of perfect se quences possible, which can thus be understood as a definition of a possibilistic counterpart of Bayesian net works (Jirousek et al., 2000). Although a large number of properties are yet to be proven, a chance exists that the composition operators may prove to be tools that will enable us to study multidimensional distributions in both probability and possibility theories, within the framework of a uniform approach.\nAcknowledgments\nThis work was supported by the grants: Ministry of Education of CR no. VS96008 and GACR no. 201/98/1487.\nReferences\nde Cooman, G. (1997a) Possibility theory 1: The measure- and integral-theoretic groundwork. Int. J. of General Systems, 25, pp. 291-323.\nde Cooman, G. (1997b) Possibility theory II: Con ditional possibility. Int. J. of General Systems 25, pp. 325-351.\nde Cooman, G. (1997c) Possibility theory Ill: Possi bilistic independence. Int. J. of General Systems, 25, pp.; 353-371.\nCsiszar, I. (1975). !-divergence geometry of proba bility distributions and minimization problems. Ann. Probab., 3, pp. 146-158.\nDeming, W.E. and Stephan, F.F. (1940). On a least\nsquare adjustment of a sampled frequency table when the expected marginal totals are known. Ann. Math. Stat., 11, pp 427-444.\nHajek, P., Havranek, T. and Jirousek R. (1992). Un certain Information Processing in Expert Systems. CRC Press, Inc., Boca Raton.\nJensen, F.V . (1996). Introduction to Bayesian Net work. UCL Press, London.\nJirousek, R. (1997). Composition of Probability Mea sures on Finite Spaces. In Proc. of the 13th Conf. Un certainty in Artificial Intelligence UAI'g7, Providence, RI, August 1997 (D. Geiger, P. P. Shenoy, eds.). Mor gan Kaufmann Pub!. San Francisco, California, 1997, pp. 274-281.\nJirousek, R. (1998). Graph Modelling without Graphs. In Proc. of the 17th Int. Conf. on Information Processing and Management of Uncer tainty in Knowledge-based Systems IPMU'98, Paris (B. Bouchon-Meunier, R,R, Yager, eds.). Editions E.D.K. Paris, pp. 809-816.\nJirousek, R., Vejnarova J. and Gemela, J. (2000). Pos sibilistic Belief Network Constructed by Operators of Composition and its Application to Financial Analy sis. Submitted.\nKellerer, H.G. (1964). Verteilungsfunktionen mit gegeben Marginalverteilungen. Z. Warhsch. Verw. Gebiete, 3, pp. 247- 270.\nLauritzen, S.L. (1996). Graphical Models. Clarendon Press, Oxford.\nShafer, G. (1996). Probabilistic Expert Systems. SIAM, Philadelphia, PA.\nShafer, G. and Shenoy, P.P. (1990). Probability Prop agation. Annals of Mathematics and Artificial Intelli gence , 2, pp. 327-352.\nShenoy, P.P. and Shafer, G. (1990). Axioms for Prob ability and Belief Functions Propagation. In Uncer tainty in Aritificial Intelligence, 4, pp. 169-198.\nVejnarova, J. (1998). Composition of possibility mea sures on finite spaces: preliminary results. In Proc. of the 7th Conf. on Information Processing and Man agement of Uncertainty in Knowledge-based Systems IPMU'98, Paris, France, (B. Bouchon-Meunier, R. R. Yager, eds.). E.D.K., Paris, pp. 25-30."}], "references": [{"title": "Possibility theory 1: The measure- and integral-theoretic groundwork", "author": ["G. de Cooman"], "venue": "Int. J. of General Systems,", "citeRegEx": "Cooman,? \\Q1997\\E", "shortCiteRegEx": "Cooman", "year": 1997}, {"title": "Possibility theory Ill: Possi\u00ad bilistic independence", "author": ["G. de Cooman"], "venue": "Int. J. of General Systems,", "citeRegEx": "Cooman,? \\Q1997\\E", "shortCiteRegEx": "Cooman", "year": 1997}, {"title": "-divergence geometry of proba\u00ad bility distributions and minimization problems", "author": ["I. Csiszar"], "venue": "Ann. Probab., 3, pp. 146-158.", "citeRegEx": "Csiszar,? 1975", "shortCiteRegEx": "Csiszar", "year": 1975}, {"title": "On a least", "author": ["W.E. Deming", "F.F. Stephan"], "venue": null, "citeRegEx": "Deming and Stephan,? \\Q1940\\E", "shortCiteRegEx": "Deming and Stephan", "year": 1940}, {"title": "Un\u00ad certain Information Processing in Expert Systems", "author": ["P. Hajek", "T. Havranek", "Jirousek R."], "venue": "CRC Press, Inc., Boca Raton.", "citeRegEx": "Hajek et al\\.,? 1992", "shortCiteRegEx": "Hajek et al\\.", "year": 1992}, {"title": "Introduction to Bayesian Net\u00ad work", "author": ["Jensen", "F.V ."], "venue": "UCL Press, London.", "citeRegEx": "Jensen and .,? 1996", "shortCiteRegEx": "Jensen and .", "year": 1996}, {"title": "Composition of Probability Mea\u00ad sures on Finite Spaces", "author": ["R. Jirousek"], "venue": "Proc. of the 13th Conf. Un\u00ad certainty in Artificial Intelligence UAI'g7, Providence, RI, August 1997 (D. Geiger, P. P. Shenoy, eds.). Mor\u00ad gan Kaufmann Pub!. San Francisco, California, 1997,", "citeRegEx": "Jirousek,? 1997", "shortCiteRegEx": "Jirousek", "year": 1997}, {"title": "Graph Modelling without Graphs", "author": ["R. Jirousek"], "venue": "Proc. of the 17th Int. Conf. on Information Processing and Management of Uncer\u00ad tainty in Knowledge-based Systems IPMU'98, Paris (B. Bouchon-Meunier, R,R, Yager, eds.). Editions", "citeRegEx": "Jirousek,? 1998", "shortCiteRegEx": "Jirousek", "year": 1998}, {"title": "Pos\u00ad sibilistic Belief Network Constructed by Operators of Composition and its Application to Financial Analy\u00ad", "author": ["R. Jirousek", "Vejnarova J", "J. Gemela"], "venue": "E.D.K. Paris,", "citeRegEx": "Jirousek et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Jirousek et al\\.", "year": 2000}, {"title": "Verteilungsfunktionen mit gegeben Marginalverteilungen", "author": ["H.G. Kellerer"], "venue": "Z. Warhsch. Verw. Gebiete, 3, pp. 247- 270.", "citeRegEx": "Kellerer,? 1964", "shortCiteRegEx": "Kellerer", "year": 1964}, {"title": "Graphical Models", "author": ["S.L. Lauritzen"], "venue": "Clarendon Press, Oxford.", "citeRegEx": "Lauritzen,? 1996", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "Probabilistic Expert Systems", "author": ["G. Shafer"], "venue": "SIAM, Philadelphia, PA.", "citeRegEx": "Shafer,? 1996", "shortCiteRegEx": "Shafer", "year": 1996}, {"title": "Probability Prop\u00ad agation", "author": ["G. Shafer", "P.P. Shenoy"], "venue": "Annals of Mathematics and Artificial Intelli\u00ad gence , 2, pp. 327-352.", "citeRegEx": "Shafer and Shenoy,? 1990", "shortCiteRegEx": "Shafer and Shenoy", "year": 1990}, {"title": "Axioms for Prob\u00ad ability and Belief Functions Propagation", "author": ["P.P. Shenoy", "G. Shafer"], "venue": "Uncer\u00ad tainty in Aritificial Intelligence, 4, pp. 169-198.", "citeRegEx": "Shenoy and Shafer,? 1990", "shortCiteRegEx": "Shenoy and Shafer", "year": 1990}, {"title": "Composition of possibility mea\u00ad sures on finite spaces: preliminary results", "author": ["J. Vejnarova"], "venue": "Proc. of the 7th Conf. on Information Processing and Man\u00ad agement of Uncertainty in Knowledge-based Systems IPMU'98, Paris, France, (B. Bouchon-Meunier, R. R.", "citeRegEx": "Vejnarova,? 1998", "shortCiteRegEx": "Vejnarova", "year": 1998}], "referenceMentions": [{"referenceID": 6, "context": "Composition of low-dimensional distribu\u00ad tions, whose foundations were laid in the pa\u00ad per published in the Proceedings of UAI'97 (Jirousek 1997), appeared to be an alterna\u00ad tive apparatus to describe multidimensional probabilistic models.", "startOffset": 130, "endOffset": 145}, {"referenceID": 6, "context": "The first two lemmata were proven in (Jirousek 1997).", "startOffset": 37, "endOffset": 52}, {"referenceID": 6, "context": "There are two additional reasons for our presentation of its proof, despite its having already been proven in (Jirousek 1997).", "startOffset": 110, "endOffset": 125}, {"referenceID": 6, "context": "Among such sequences, an important role is played by those that are called perfect (this no\u00ad tion was already introduced in (Jirousek 1997)).", "startOffset": 124, "endOffset": 139}, {"referenceID": 8, "context": "An algorithm for reconstruction of a Bayesian network from a perfect sequence can be found in (Jirousek et al. 2000).", "startOffset": 94, "endOffset": 116}, {"referenceID": 6, "context": "r>Pn) was already stated in Theorem 4 in (Jirousek 1997).", "startOffset": 41, "endOffset": 56}, {"referenceID": 6, "context": "Nevertheless, for special situations the following sim\u00ad ple assertion (Lemma 2 in (Jirousek 1997)) presents", "startOffset": 82, "endOffset": 97}, {"referenceID": 7, "context": "Within this framework, different generating sequences of low-dimensional probability distributions can be studied (Jirousek, 1998).", "startOffset": 114, "endOffset": 130}, {"referenceID": 9, "context": ", Kn can be ordered to meet the so called running intersection property (introduced in (Kellerer, 1964)).", "startOffset": 87, "endOffset": 103}, {"referenceID": 14, "context": "The operators of composition were also de\u00ad fined for possibilistic distributions (Vejnarova, 1998).", "startOffset": 81, "endOffset": 98}, {"referenceID": 8, "context": "These properties, for example, make a definition of perfect se\u00ad quences possible, which can thus be understood as a definition of a possibilistic counterpart of Bayesian net\u00ad works (Jirousek et al., 2000).", "startOffset": 181, "endOffset": 204}], "year": 2011, "abstractText": "Composition of low-dimensional distribu\u00ad tions, whose foundations were laid in the pa\u00ad per published in the Proceedings of UAI'97 (Jirousek 1997), appeared to be an alterna\u00ad tive apparatus to describe multidimensional probabilistic models. In contrast to Graphi\u00ad cal Markov Models, which define multidimen\u00ad sional distributions in a declarative way, this approach is rather procedural. Ordering of low-dimensional distributions into a proper sequence fully defines the respective compu\u00ad tational procedure; therefore, a study of dif\u00ad ferent types of generating sequences is one of the central problems in this field. Thus, it ap\u00ad pears that an important role is played by spe\u00ad cial sequences that are called perfect. Their main characterization theorems are presented in this paper. However, the main result of this paper is a solution to the problem of marginalization for general sequences. The main theorem describes a way to obtain a generating sequence that defines the model corresponding to the marginal of the distri\u00ad bution defined by an arbitrary generating se\u00ad quence. From this theorem the reader can see to what extent these computations are lo\u00ad cal; i.e., the sequence consists of marginal dis\u00ad tributions whose computation must be made by summing up over the values of the vari\u00ad able eliminated (the paper deals with a finite model) .", "creator": "pdftk 1.41 - www.pdftk.com"}}}