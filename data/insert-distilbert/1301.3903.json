{"id": "1301.3903", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Exploiting Qualitative Knowledge in the Learning of Conditional Probabilities of Bayesian Networks", "abstract": "learning algorithms for learning the conditional probabilities of bayesian networks with hidden variables typically operate within a secure high - dimensional search space and yield only locally optimal solutions. one way of limiting expanding the search space and avoiding isolated local parameters optima is to impose qualitative constraints that are based on background knowledge concerning designing the domain. we present first a method for integrating formal statements of qualitative constraints into two learning algorithms, apn and em. in our experiments with synthetic data, this method yielded networks that satisfied the constraints almost perfectly. the accuracy index of the learned networks was consistently superior to that of corresponding networks learned fully without constraints. the exploitation of qualitative constraints therefore appears to be a promising way to increase both the interpretability and the accuracy of universally learned bayesian networks with known structure.", "histories": [["v1", "Wed, 16 Jan 2013 15:53:24 GMT  (378kb)", "http://arxiv.org/abs/1301.3903v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["frank wittig", "anthony jameson"], "accepted": false, "id": "1301.3903"}, "pdf": {"name": "1301.3903.pdf", "metadata": {"source": "CRF", "title": "Exploiting Qualitative Knowledge in the Learning of Conditional Probabilities of Bayesian Networks", "authors": ["Frank Wittig", "Anthony Jameson"], "emails": ["jameson}@cs.uni-sb.de"], "sections": [{"heading": null, "text": "Algorithms for learning the conditional proba bilities of Bayesian networks with hidden vari ables typically operate within a high-dimensional search space and yield only locally optimal so lutions. One way of limiting the search space and avoiding local optima is to impose quali tative constraints that are based on background knowledge concerning the domain. We present a method for integrating formal statements of qual itative constraints into two learning algorithms, APN and EM. In our experiments with synthetic data, this method yielded networks that satisfied the constraints almost perfectly. The accuracy of the learned networks was consistently superior to that of corresponding networks learned without constraints. The exploitation of qualitative con straints therefore appears to be a promising way to increase both the interpretability and the accu racy of learned Bayesian networks with known structure."}, {"heading": "If you don 't know where you're going, you might", "text": "wind up someplace else. -Yogi Berra\n1 INTRODUCTION\nThe following problem often arises when we use standard learning algorithms to learn the conditional probabilities of a Bayesian network (BN) with known structure and one or more hidden variables: 1 We have some fairly clear ideas about the qualitative relationships that must exist among the variables in the BN, and we are mainly interested in determining the quantitative relationships. But the learning algorithm uses no knowledge of qualitative relationships, so it \"winds up someplace else\": It produces a BN that may\n1For an overview of the general problem of learning Bayesian networks from data, see, e.g., Heckerman (1998).\nfit the data fairly well but that noticeably fails to exhibit the qualitative relationships that we expected. Such a network can be awkward to use and to explain to others, since it regularly violates natural expectations. 2 Moreover, we may suspect that a more accurate network could have been found which did fulfill our qualitative expectations.\nBinder, Koller, Russell, and Kanazawa ( 1997, Section 7), after introducing the APN algorithm for learning BNs with hidden variables, proposed that it ought to be possible to guide the learning process by specifying qualitative con straints that the resulting network should satisfy. For ex ample, a domain expert might state: \"If the value of a vari able A increases, then the value of its child variable B also increases\". The purpose of the present paper is to work out and implement this proposal.\nOur basic method for exploiting such constraints is to de fine a term within the network scoring function that reflects the overall extent to which qualitative constraints are vio lated by an intermediate learned network. If such a network rates poorly according to this criterion, the learning algo rithm should tend to move on to alternative networks that rate better.\nAnother related approach, which goes a step further in the same direction as our approach, is to specify in advance specific types of functions describing the nature of the ( un certain) relationships between particular variables in the BN; and to apply learning techniques that are appropriate for these functions. For example, a linear relationship be tween a child and its parents could be learned using linear regression (see, e.g., Musick, 1996; Binder et al., 1997). But in many cases, it is not clear what specific functional relationship holds, even though it is clear that there must be a monotonic relationship of the sort mentioned above.\n2 Some discrepancies may simply be due to the fact that the learning algorithm has in effect labeled the states of a variable differently than we expected, e.g., assigning to the state \"On\" the probabilities that we would expect for \"Off'', and vice-versa. These deviations from expectations may be easily correctible; but in practice they do not account for all of the violations of qualita tive expectations.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 645\nThe paper is organized as follows: After briefly review ing the basic concepts and notation involved in the learning of Bayesian networks, we present a general conceptualiza tion of how qualitative constraints can figure in the learning process. In Section 3 we then show how qualitative con straints can be suitably formalized. Section 4 shows how these ideas can be applied to two important learning algo rithms for BNs, the APN and EM algorithms. Section 5 presents and discusses the results of tests of our approach in which synthetic data were used.\n2 LEARNING BAYESIAN NETWORKS\nWITH KNOWN STRUCTURE\n2.1 BAYESIAN NETWORKS\nFormally, a BN B = (G, 9) consists of two components.3 The first one is a directed acyclic graph G that represents the causal independencies that hold in the domain to be modeled by B. Nodes represent random variables and di rected links between nodes are commonly interpreted as causal influences between these variables. We restrict our attention in this article to BNs in which all of the variables are discrete.\nBNs are characterized by the following independence as sumption: Given the states of its parents, a node is inde pendent of all its non-descendents in the BN. The second component of a BN is a vector 9 of conditional probability tables (CPTs) 9; that represent the (uncertain) relationships between nodes and their parents. A node's CPT consists of conditional probabilities for each state of the node condi tioned on its parents' state configuration. A BN B repre sents a joint probability distribution P(X1 , . . \u2022 , Xn) over the states of its variables X = {X1, . .. , Xn}\u00b7 Exploiting the independence assumption of BNs, the joint probability distribution decomposes into a product of local conditional probabilities:\nn n\ni=l i=l\nThe term pa(X;) represents the set of all configurations of X; 's parents, while 0; is the CPT belonging to node\nX;. Therefore, 0 = (81, . .. , On) \u00b7 Bijk = P(X; = Xij !pa(X;) = pak(X;)) = P(xii !Pak (Xi)) stands for the entry corresponding to the jth state of Xi in Oi when its parents take on their kth configurationpak(X;).\n2.2 THE LEARNING PROBLEM\nThe general problem of learning the conditional probabili ties 0 of a BN B with known structure can be formulated\n3Detailed treatments of the theoretical and mathematical basis of BNs are given by, among others, Pearl ( 1988) and Castillo, Gutierrez, and Hadi ( 1997).\nas follows: Given a data set D = { D1, . . . , D 8} of s train ing cases Di, find a set of CPTs 9 that optimizes a certain scoring function that describes the fitness of B with respect to D. Every training case Di is an assignment of states for a subset of B's nodes. A common scoring function is the likelihood P(D!O) of D with respect to 0 for a given structure G. For computational convenience, the logarithm of this function is commonly used:\n8 lnP(D!O) = L)n P(D i!O) . (2)\ni=l\nThe vector 9 that maximizes this function (locally) repre sents a (locally) optimal set of CPTs, leaving out of con sideration any prior qualitative knowledge about the CPTs. When a BN includes one or more hidden variables, it is in general infeasible to compute exact solutions to this prob lem (see, e.g., Heckerman, 1998). Therefore, approxima tive algorithms such as the ones discussed in Section 4 are used.\n2.3 THE ROLE OF QUALITATIVE CONSTRAINTS\nSuppose now that we have asked a domain expert whether the CPTs of the to-be-learned BN satisfy a particular set of qualitative constraints C and that the expert has answered \"Yes\". How can we take this fact into account within the framework implied by Equation 2?\nOne conceptualization would be that the expert has hereby specified a prior probability distribution over the possible values of 9. But within the maximum-likelihood frame work, it is more appropriate to think in terms of the like lihood that the expert would answer \"Yes\" given various possible states of reality-i.e., various extents to which the constraints are really satisfied.\nConcretely, suppose that we have defined a function violation(O, C) that indexes the extent to which the CPTs 0 violate the constraints C: violation takes the value 0 if there is no violation at all and some positive value other wise which increases with the seriousness of the violation.\nLet us consider the likelihood that the expert answers \"Yes\" as a function ofviolation(O, C) . This likelihood should be equal (or close) to 1 if there is in reality no violation; and it should move toward 0 as the value of violation(O, C) increases from its minimum ofO.\nA computationally convenient function that meets these re quirements is the following one:\nP(answer = yesjO, C) = exp( -w \u00b7 violation(O, C)) . (3)\nHere, the positive weight w determines how quickly the\n646 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nprobability decreases from its maximum of 1 as the extent of constraint violations increases from its minimum of 0.\nWe can now view the expert's statement as a single but especially significant-\"observation\" that can be taken into account along with the normal observations D in the dataset. Accordingly, we can add the log-likelihood of this \"observation\" to the right-hand side of Equation 2 to obtain a modified likelihood of all observations:\nIn P(DI9)- w \u00b7 violation(9, C) . (4)\nThe aim is now of course to find a (local) maximum for this log-likelihood. The term violation(9, C) can be viewed as a penalty term which will cause the search algorithm to avoid solutions that violate the constraints, and the constant w can be viewed as the weight of this penalty term.\nThe empirical results presented in Section 5 suggest that, when the constraints C are in fact satisfied by the model that generates the data, a solution will typically be found for which the value of this term is (close to) 0.\nIt is clear that the function specified in Equation 3 is partly arbitrary. Indeed, determining the actual probabilistic re lationship between constraint violations and expert judg ments would require empirical research, and it might be impossible to find a useful domain-independent formula tion. The above account can therefore be seen as specifying a possible scenario in which the penalty term in (4) can be given a probabilistic interpretation. Its intent is to clarify the relationship between the roles of the empirical data and the expert's judgment in guiding the search for a solution.\nIn order to be able to make use of (4), we have to answer two main questions:\nI. How can the violation function best be defined and motivated for some useful class of constraints? 2. W hat algorithms can be used to find a (local) maxi mum of the scoring function of ( 4 )?\nThe first question will be addressed in the next section and the second question in the subsequent sections.\n3 FORMALIZING QUALITATIVE CONSTRAINTS\n3.1 QUALITATIVE INFLUENCES\nWithin the framework of qualitative probabilistic networks (Wellman, 1990), Druzdzel and van der Gaag ( 1995) give formal probabilistic definitions of several types of quali tative relationships that can hold between nodes in a BN. The authors of the latter work employed these definitions in a method for combining different types of knowledge for the specification of the CPTs for BNs. They did not employ standard BN learning methods like the EM algo rithm or gradient-based methods. Our method can be seen\nas an integration of parts of the method ofDruzdzel and van der Gaag ( 1995) with standard BN learning algorithms. In this paper, we focus on the simple relationships that these authors call qualitative influences; but our method can be applied analogously to the more complex relationships that they also deal with.\nThe concept of a qualitative influence is only applicable if there is an ordering on the states of the nodes involved. Without loss of generality, we define x;1 < X;2 < ... < Xin; for every node X; with n; discrete states that is in volved in a qualitative influence. A qualitative influence is denoted by S7(Xw,Xz), where? E {+,-}describes the quality ( + or -) of a monotonic relationship between a variable Xw and one of its children Xz. Two kinds of qualitative influences exist: If a positive one holds, an in crease in the state of Xw causes an increase (or at least no decrease) in the state of X z. If the relationship is negative, an increase in Xw 's state causes a decrease (or at least no increase) in Xz's state.\nSomewhat more formally, a positive qualitative influence s+(Xw, Xz) can be defined as follows: For any given value of Xz, an increase in the value of Xw will not de crease the probability that the value of X z is equal to or greater than that given value.\nFormally (cf. Druzdzel & van der Gaag, 1995): For all states x zm of X z with m > 1 and all distinct pairs of states Xwi,Xwj of Xw such that i > j and for all possible state configurations y of Xz 's parents other than Xw, the fol lowing inequality must hold:\nIn terms of the conditional probabilities for individual states of Xz, this definition yields a set of inequalities of the following form:\nn. \ufffd \"2::: P(xzdXw;, y) 2:: \"2::: P(xzdXwj, y). l=m l=m\n(6)\nThere exists one such inequality for each combination of an Xzm such that m > 1, a pair Xwi and Xwj such that i > j, and a configuration y of the states of Xz 's parents other thanXw.4\nNegative qualitative influences are defined analogously.\n4 Actually, for the unambiguous specification of a constraint, we require only the inequalities that involve adjacent values of Xw, i.e., where i = j + 1, since the other inequalities are implied by the transitivity of the relation \ufffd- But in cases where a con straint has been violated, the redundant inequalities allow us to identify all of the values that are involved in the violation. As will become clear below, it is then possible to adjust all of these values simultaneously so as to eliminate the violation more quickly.\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 647\n3.2 DEFINITION OF A VIOLATION TERM\nWe can now see how to define a suitable index of the over all extent to which a given set of CPTs () violates a given set of constraints C, when these constraints concern quali tative influences-i.e., how to define a violation function of the type introduced in Section 2.3. Consider Inequality 6, which is part of the mathematical description of a positive qualitative influence of Xw on Xz. We can write this in equality more generally as follows:\nn% nz L P(xzdXwi' y) - L P(xzdXwj, y) 2:: 0. l=m l=m (7)\nFor every violated positive constraint, there has to exist at least one such inequality that is not satisfied-i.e., where the difference on the left-hand side of the equation is neg ative. Analogously, violations of negative constraints lead to values greater than 0.\nA partial violation term corresponding to a single inequal ity can be defined as follows:\n\"f?- d '?wz 0 , 1 \u2022 + an cmijy < , \"f?- d '?wz 0 , 1 . - an Cmijy > ,\n, otherwise. (8)\nThe total violation term violation((), C) is defined as the sum of all of the relevant partial violation terms:\nviolation((), C) : = m,i,j,y,w,z\n?wz Cmijy\u2022 (9)\nwhere ? stands for the quality ( + or -) of the constraint corresponding tow and z. Note that, for each combination of variables corresponding to the indices w and z, only one quality ? can exist, since it makes no sense to specify both a negative and a positive influence for these two variables.\n4 USING CONSTRAINTS IN LEARNING ALGORITHMS\nHaving seen how to define the violation term required by (4), we can address the problem of finding a (local) maxi mum of the right-hand side of that equation. An analytic solution is not in general available, but various iterative search methods have been proposed. We will first discuss the use of the APN method ( Binder et a!., 1997; Russell, Binder, Koller, & Kanazawa, 1995), which can deal with the addition of the constraint violation term in a straightfor ward way. We will then turn to the EM method, with which dealing with the constraint violation term is less straightfor ward.\n4.1 BASIC APN\nThe adaptive probabilistic networks method (APN) is a gradient-based algorithm for the learning problem formu lated in Equation 2.\nThe computation of new values ()' for the CPT entries is done by taking (small) steps in the direction that is deter mined by the gradient V In P( D 16) of the log-likelihood function that constitutes the right-hand side of Equation 2:\n6' = 6 + aVInP(DI6), ( 10)\nwhere a is a step-size parameter.\nAs was shown by Binder et a!. ( 1997, Section 5. 1), the par tial derivatives of the log-likelihood function can be com puted as follows:\n( 1 1)\nwhere the superscript \" indicates that the gradient is still unprojected; that is, it has to be projected onto the con straint surface defined by Ej e:jk = 1, so that the new values (}\ufffdjk will continue to obey this fundamental constraint on the entries of any CPT. After the projection has been performed (as described by Binder et a!., 1997, Section 4), the resulting gradient vector V In P(D 16) can be used in Equation 10. Binder et a!. ( 1997, Section 5.3) and Russell et a!. (1995, Section 7) present empirical results concern ing the effectiveness of this method for learning the CPTs of BNs with hidden variables.\n4.2 TAKING CONSTRAINTS INTO ACCOUNT\nWITH APN\nTo use APN with the extended scoring function of ( 4 ), we need to compute a slightly more complex gradient:\nV In P(DI6) - Vw \u00b7violation((), C). ( 12)\nThe partial derivatives for the first term are of course the ones specified in Equation 1 1. For the second term, we can write:\n\\7ijk w \u00b7violation((), C) = w \u00b7 Vijk (6, C). ( 13)\nEach Vijk ( (), C) is the partial derivative of the violation function with respect to the CPT entry (}ijk\u00b7 This partial derivative is easy to compute, as can be seen from Inequal ity 7: Each partial violation term is a linear function of CPT entries, with each entry occurring at most once and having a coefficient of either + 1 or - 1 . The only partial violation terms that contribute to the total violation term are the ones that correspond to inequalities that are not fulfilled at the current point () in the search space.\n648 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nIt is straightforward to show that Vijk(O, C) can be com puted as follows:\nwhere vijk (0, C) is the number of unfulfilled inequalities which suggest that Bijk ought to be lower and vtk ((},C) is the number of such inequalities that suggest that (}iik ought to be higher. It is therefore intuitively plausible, as well as mathematically sound, to add the gradient specified by Equation 13 to the one specified by Equation I I, as follows:\nAs with normal APN, it is still necessary to project this unprojected gradient onto the constraint surface defined by Lj B\ufffdik = 1 before using it to compute the next step.5 We will now see how the log-likelihood functions in (2) and (4) can also be handled with the powerful and frequently used EM algorithm.\n4.3 BASIC EM\nThe Expectation Maximization or EM algorithm (Demp ster, Laird, & Rubin, 1977), when applied to our problem of maximizing the log-likelihood in Equation 2, proceeds as follows: After(} has been initialized to some vector of start ing values, the algorithm performs two steps iteratively: The first step, called the expectation- or E-step, computes, for each Di in D, an expectation for the value(s) of the hid den variable(s) in Di. (This computation involves, for each Di, instantiating the observable nodes of a BN with CPTs corresponding to (} and evaluating this BN to derive a be lief about each hidden variable.) The result of this step is a hypothetical dataset n' which includes, in addition to the observed values of the observable variables, expectations concerning the hidden variables.\nThe second step, which is called the maximization- or M step, computes new CPT values o' which (locally) maxi mize the total log-likelihood of this hypothetical dataset-a task that is much easier than maximizing the log-likelihood of the real dataset (cf. Equation 2). These new values o' always yield a log-likelihood of the real dataset that is at least as high as the one produced by the previous values (}.\nFor our particular problem of learning the CPTs of BNs with hidden variables, the E-step and the M-step taken to gether yield a simple update rule (see, e.g., Castillo et al.,\n5In addition to the normalization mentioned above, these methods have to take into account that o;jk E [0, 1]. This can be done by not allowing the learning algorithm to leave this search space. This may lead to situations where the learning procedures tend to follow the boundaries of the search space.\nI997, p. 5I5):\n(I6)\nHere, EB[Nik] is the expectation of the number of obser vations in the dataset in which the parents of the node xi take on their kth configuration pak(Xi)\u00b7 EB[Nijk] is the expectation of the number of such observations for which Xi takes on its jth value Xii. The latter expectation can be computed according to Equation I7:\n8 EB[Nijk] = LP(xii\u2022Pak(Xi)IDl,O), (I7)\nl=l\nwhile the former expectation is found through summation over j.\nThe update rule in Equation I6 is applied repeatedly until it converges on a (local) optimum for Equation 2, a solution which represents a maximum-likelihood estimate of the set of CPT entries.\n4.4 TAKING CONSTRAINTS INTO ACCOUNT\nWITH EM\nHow can EM be used to maximize the extended scoring function (4) instead of the simple log-likelihood of the ob served data? The most elegant approach would be to apply the expectation and maximization steps directly to the scor ing function ( 4) so as to derive a modified update rule that could be used instead of ( 16). Unfortunately, the general EM approach is not equally easy to apply to all possible scoring functions; in particular, an attempt to apply it to the scoring function in (4) yields an interrelated set of non linear equations for which we did not find an analytic solu tion. The further pursuit of this approach therefore remains a matter for future research, which might, for example, con sider the use of somewhat different scoring functions.\nBecause of the generally desirable properties of the EM ap proach, it seems worthwhile to pursue a theoretically less elegant modification of it which is capable of dealing with the extended scoring function of (4), albeit in a heuristic manner. Similarly, other researchers have developed vari ants of EM which are theoretically less justifiable than the pure form but which can be shown in practice to perform well, at least for some types of problems (see, e.g., Ortiz & Kaelbling, 1999; Bauer, Koller, & Singer, I997). Like some of these methods, our approach combines EM with gradient ascent.\nThe basic idea is to alternate between two types of updates of the vector (} of CPT entries:\nI. the standard EM update given by ( 16), which moves to an intermediate solution which yields a higher log likelihood for the observations;\nUNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000 649\n2. a gradient-based update which uses the gradient spec ified in Equation 13 and which aims only to decrease the extent of constraint violations that are found at the solution ()' that results from the EM update.\nThis results of applying this method are theoretically less predictable than the results for normal EM--or indeed for many of the variants of EM-, since the quality of the so lution cannot be guaranteed to increase with each iteration: In principle, an EM update that increases the log-likelihood slightly can lead to a drastic decline in the extent to which the constraints are satisfied; and vice versa for a gradient based update that slightly reduces constraint violations. On the other hand, if the specified constraints really do hold, the two goals of maximizing log-likelihood and maximiz ing constraint fulfillment are generally compatible; hence we would not expect the two types of update to work con tinually at cross-purposes.\nThe performance of this hybrid algorithm in practice will be investigated in the next section.\n5 TEST OF THE METHOD\nWe conducted empirical tests using both of the two pro cedures described in the previous section. Since the initial results were qualitatively roughly similar, we conducted the most systematic tests for the modified EM procedure, be cause this procedure is more in need of empirical valida tion, given its partly heuristic nature.\nTwo network structures were used for the tests. Since theo retical interpretability is one of the motivations for the use of qualitative constraints, the first network structure comes from a domain in which the interpretability of a single hid den variable is important. The second-abstract--example demonstrates the feasibility of our approach for network structures involving more than one hidden variable.\n5.1 EXAMPLE NETWORKS\nOur first example BN, shown in the top part of Figure 1, could be used as a basis for an influence diagram for a hypothetical assistance system S that presents sequences of spoken instructions to the user U (as, for example, a speech-based help system might do).6\nAn adaptation decision that S has to make is whether to present a given set of instructions in a stepwise manner (i.e., S presents its instructions one by one, allowing U to exe cute each one before presenting the next one) or in a bun dled manner (all instructions are presented at once before U starts to execute the first one). One result of an experiment\n6Explanations of the individual variables, along with the raw data from an experiment in this domain that involved 24 subjects, are available from http://w5.cs.uni-sb.de/\"'ready/. See also Jame son, GroBmann-Hutter, March, and Rummer (2000).\nNetwork Structure 1\nNetwork Structure 2\n\ufffd)!l\ufffd D 0 E 0 (3) (3)\nFigure 1. Structures of BNs Used for Testing.\n(Nodes within dashed boxes correspond to hidden variables. For each of the arrows labeled+ (or-), a positive (or negative) qual itative influence was postulated. The number in parentheses for each node is the number of states of that node.)\nin this domain was that stepwise presentation reduced the number of errors U made but led to longer execution times.\nWe specified four qualitative influences involving the hid den variable COGNITIVE LOAD: Both psychological research and common sense led us to expect that the three parent variables of COGNITIVE LOAD would influence it positively. Moreover, higher COGNITIVE LOAD should increase the like lihood of ERROR IN PRIMARY TASK?.\nThe lower half of Figure 1 shows a second example BN. We offer no theoretical interpretation for it, but it enables us to test the effectiveness of the learning methods for network structures that include more than one hidden variable.\n5.2 PROCEDURE\nSpecification of original ENs. We first manually specified a plausible BN for each structure just described, each of which satisfied the specified qualitative constraints. These original ENs were assumed for the rest of the evaluation to model the true causal relationships perfectly.\nGeneration of synthetic learning data. We then gener ated a sample of 1,000 learning cases using the first net work structure and a sample of 200 cases with the second\n650 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\none. (Of course, the values generated for the hidden vari ables were not recorded.) We chose the numbers of 1,000 and 200 because (a) in the first domain a number of 1,000 seems to represent a realistic number of observations that one might be able to obtain and (b) we wanted to demon strate the feasibility of the proposed method in the case of sparse data.\nLearning, starting with different initial BNs. For each of the two structures, we generated ten BNs with randomly as signed initial values for the CPT entries. Each of these BNs was used as a starting point for two learning tasks: one us ing the standard EM algorithm and one using the extended algorithm that took the specified qualitative constraints into account. 7 In all cases, the learning procedure was termi nated after I 00 iterations.\nEvaluating the learned BNs. To evaluate the accuracy of the learned BNs, we used the two original BNs to gener ate two sets of 10,000 and 5,000 test cases, respectively (again, without recording any values for the hidden vari ables). Each of the learned BNs was evaluated with respect to its average negative log-likelihood per case. In addition, the criterion of average quadratic loss per case was applied to the nodes ERROR IN PRIMARY TASK? and G in the first and second structures, respectively. (For both quality measures, lower values indicate better results.)\n5.3 OVERALL RESULTS\nThe left-hand side of Figure 2 shows the main results for each of the 20 BNs that were learned for each network structure.\nViolation of constraints. The narrow bars show that all of the BNs learned without constraints exhibited substantial violations even after 100 iterations. (The maximum possi ble values of violation were 63 and 54 for Network Struc tures 1 and 2, respectively.)8 By contrast, the violation scores for the constrained BNs are mostly invisible in the graph because they are essentially zero.\nFit to the test data (overall). In the larger histogram for each network structure, the baseline shows the fit to the test data of the original BN that generated the test data. As one would expect, the fit of the learned BNs to the test data was worse than this baseline in every case; the fit is shown by the bars that project above the baseline. Looking at first only at these bars above the baseline and comparing the black bars with the gray ones, we see that the constrained BNs came closer to the baseline in 8 of the 10 cases with\n7For the latter procedure, the violation weight w (Equation 13) was set to 2.0. Moreover, the vector ofvijkS used in the gradient step was rescaled so that the absolute size of its largest component was equal to the absolute size of the largest component of the preceding EM step.\n8These quantitative values need to be interpreted with some caution, for the reason mentioned in Footnote 2.\nNetwork Structure I (9% closer in terms of the average dif ference). With Network Structure 2, the constrained BNs were closer to the baseline in all I 0 cases, the distance be ing 57% shorter on the average.\nFit to the test data (selected variables). With regard to the quadratic loss for the variables ERROR IN PRIMARY TASK? and G (not shown in Figure 2), the constrained BNs likewise fit the data consistently better than the unconstrained BNs. In fact, for the two network structures they come 50% and 66% closer to the relevant baseline, respectively, than the corresponding unconstrained BNs. Note that the criterion variables used here are directly involved in the specified constraints.\nOveifitting. The fit of the learned BNs to the learning data indicates how much overfitting occurred during the learn ing process. Each of the learned BNs fit the learning data better than the original generating BN did. These results are shown by the bars that project below the baseline. We see that the unconstrained BNs overfit the learning data to a consistently greater extent than the constrained BNs. Thus, one advantage of using constraints seems to be that they offer a natural way to limit overfitting.\n5.4 THE TIME COURSE OF LEARNING\nTo get a clearer picture of the reasons for the success of the learning procedure with constraints, we can examine the time course of the learning process for one typical BN for each network structure (see the right-hand side of Figure 2).\nElimination of constraint violations. In both cases, the procedure with constraints essentially eliminated the con straint violations within the first few iterations. (The val ues of the violation variable are not shown for the individ ual unconstrained BNs, since there was no typical pattern, aside from the fact that there were almost always substan tial violations.)\nEvolution of the fit to the test data. Looking at the two uppermost curves in each graph, we can distinguish two phases of the learning process. In the first phase, which typ ically lasts less than 10 iterations, the unconstrained BNs fit the test data better. Then, after a crossover point, the con strained BNs show a consistently better fit. This pattern can be understood in terms of the basic properties of the mod ified EM algorithm (Section 4.4): Initially, when there are substantial constraint violations, the normal EM steps alter nate with gradient-based steps that serve solely to reduce constraint violations, perhaps at the expense of fit to the data. It is only after reaching a region of the search space in which the constraints are fulfilled that the algorithm can perform updates that are determined primarily by the goal of improving the fit to the data.\nAvoidance of oveifitting as a key advantage. With the two individual BNs shown on the right-hand side of Fig-\n(Black and gray bars and curves show results for BNs learned with and without constraints, respectively. The two \ufffdppermost learning curves in the graphs on the right show the fit to the test data, while the two curves below them show the fit to the learnmg data. The other aspects of the figure are explained in the text.)\nure 2, the optimal termination point for the learning pro cedure occurred somewhere between the 5th and the 1Oth iterations-whether constraints were used or not. In fact, with Network Structure 2, the unconstrained EM algorithm could have attained the best fit of all if it had known ex actly where to terminate. On the other hand, terminating too early or too late without constraints could lead to signif icant loss of accuracy. In sum, the advantage of constraints in terms of fitting the data may not be that they yield a de gree of fit that is unattainable without the specification of constraints. Rather, they appear to ensure a roughly equally good fit to the test data no matter when the learning process is terminated, as long as it is not terminated very early.\nA certain amount of overfitting does seem to occur even with constraints, as is shown by the uppermost black curve for Network Structure 1. It will be interesting to inves tigate whether this overfitting mainly concerns the CPTs of variables for which no qualitative constraints have been\nspecified.\n6 CONCLUSIONS\nWhile fitting test data is an important goal, it should be remembered that the elimination of constraint violations would in itself constitute an adequate motivation for the use of procedures such as the ones proposed here. Theoret ical interpretability is a key goal in the application of BN learning techniques in many real-world systems that make use of BNs with hidden variables. Indeed, theoretical in terpretability and explainability are important strengths of Bayesian networks generally.\nSystems that use theoretically interpretable BN s may in some situations be better accepted by users. In particular, such a system's reasoning and decision making can be ex plained to users without the risk that the system will behave in a way that is incompatible with the explanation given. In\n652 UNCERTAINTY IN ARTIFICIAL INTELLIGENCE PROCEEDINGS 2000\nA general conceptualization of the problem of incorporating prior qualitative knowledge into the process of searching for a Bayesian network that fits a given dataset of observations.\nA definition of a quantitative index of the extent to which a given BN violates specified qualitative constraints, based on work of Druzdzel and van der Gaag ( 1995).\nSpecification and justification of adaptations of the basic APN and EM methods in accordance with the above contributions.\nDemonstration, using two quite different network structures and synthetic data, that the modified EM algorithm can learn BNs that fulfill the constraints (almost) perfectly while fitting the data bet ter than the BNs learned by the unmodified algorithm.\nthe domain of our first network structure, we might for mulate an explanation like \"The presence of a secondary task increases the user's cognitive load; and higher cogni tive load makes it more likely that the user will make an error.\"\nTable 1 lists the contributions of the present paper and some corresponding possible extensions. Although there is clearly much that remains to be done, the results presented here seem to indicate that this work is worth doing.\nAcknowledgments\nThis research was supported by the German Science Foun dation (DFG) in its Collaborative Research Center on Resource-Adaptive Cognitive Processes, SFB 378, Project B2 (READY). The implementations were done using HUGIN API from Hugin Expert A/S under a PhD license granted to the first author and with assistance from Bjorn Decker. We thank the anonymous reviewers for insightful comments.\nReferences\nBauer, E., Koller, D., & Singer, Y. (1997). Update rules for pa rameter estimation in Bayesian networks. In D. Geiger & P. P. Shenoy (Eds.), Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference (pp. 3-13). San Francisco: Morgan Kaufmann.\nBinder, J., Koller, D., Russell, S., & Kanazawa, K. ( 1997). Adap tive probabilistic networks with hidden variables. Machine Learning, 29, 213-244.\nCastillo, E., Gutierrez, J. M., & Hadi, A. S. ( 1997). Expert sys tems and probabilistic network models. Berlin: Springer.\nDempster, A. P., Laird, N. M., & Rubin, D. B. ( 1977). Max imum likelihood from incomplete data via the EM algo rithm. Journal of the Royal Statistical Society, 39, 1-38.\nDruzdzel, M. J., & van der Gaag, L. C. (1995). Elicitation of\nApplication of this conceptualization to other types of learning problems.\nSimilar definitions for other types of qualitative constraints.\nDerivation of a more theoretically justifiable adaptation of EM; adaptation of other search procedures, such as ELQ (Greiner, Grove, & Schuurmans, 1997).\nSimilar tests using larger networks and/or empirically collected data; systematic manipulation of parameters such as the step size, the weight of the violation term (Equation 4), and the proportion of nodes for which constraints are specified.\nprobabilities for belief networks: Combining qualitative and quantitative information. In P. Besnard & S. Hanks (Eds.), Uncertainty in Artificial Intelligence: Proceedings of the Eleventh Conference (pp. 141-148). San Francisco: Morgan Kaufmann.\nGreiner, R., Grove, A. J., & Schuurmans, D. ( 1997). Learn ing Bayesian nets that perform well. In D. Geiger & P. P. Shenoy (Eds.), Uncertainty in Artificial Intelligence: Pro ceedings of the Thirteenth Conference (pp. 198-207). San Francisco: Morgan Kaufmann.\nBeckerman, D. ( 1998). A tutorial on learning with Bayesian networks. In M. I. Jordan (Ed.), Learning in graphical models. Cambridge, MA: MIT Press.\nJameson, A., GroBmann-Hutter, B., March, L., & Rummer, R. (2000). Creating an empirical basis for adaptation deci sions. In H. Lieberman (Ed.), IU/2000: International Con ference on Intelligent User Interfaces (pp. 149-156). New York: ACM.\nMusick, R. ( 1996). Rethinking the learning of belief network probabilities. In E. Simoudis, J. W. Han, & U. Fayyad (Eds. ), Proceedings of the Second International Confer ence on Knowledge Discovery and Data Mining (pp. 120- 125). Menlo Park, CA: AAAI Press.\nOrtiz, L. E., & Kaelbling, L. P. ( 1999). Accelerating EM: An empirical study. InK. B. Laskey & H. Prade (Eds.), Un certainty in Artificial Intelligence: Proceedings of the Fif teenth Conference (pp. 512-521). San Francisco: Morgan Kaufmann. Pearl, J. ( 1988). Probabilistic reasoning in intelligent systems:\nNetworks of plausible iJiference. San Mateo, CA: Morgan Kaufmann.\nRussell, S., Binder, J., Koller, D., & Kanazawa, K. ( 1995). Local learning in probabilistic networks with hidden variables. In C. S. Mellish (Ed.), Proceedings of the Fourteenth In ternational Joint Conference on Artificial Intelligence (pp. 1 146-- 1 152). San Mateo, CA: Morgan Kaufmann.\nWellman, M. P. ( 1990). Fundamental concepts of qualitative probabilistic networks. Artificial Intelligence, 44, 257- 303."}], "references": [{"title": "Update rules for pa\u00ad rameter estimation in Bayesian networks", "author": ["D. Koller", "Y. Singer"], "venue": "Uncertainty in Artificial Intelligence: Proceedings of the Thirteenth Conference (pp. 3-13)", "citeRegEx": "Koller and Singer,? \\Q1997\\E", "shortCiteRegEx": "Koller and Singer", "year": 1997}, {"title": "Adap\u00ad tive probabilistic networks with hidden variables", "author": [], "venue": "Machine Learning,", "citeRegEx": "K.,? \\Q1997\\E", "shortCiteRegEx": "K.", "year": 1997}, {"title": "Expert sys\u00ad tems and probabilistic network models", "author": ["E. Castillo", "J.M. Gutierrez", "A.S. Hadi"], "venue": null, "citeRegEx": "Castillo et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Castillo et al\\.", "year": 1997}, {"title": "Max\u00ad imum likelihood from incomplete data via the EM algo\u00ad rithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Learn\u00ad ing Bayesian nets that perform well", "author": ["R. Greiner", "A.J. Grove", "D. Schuurmans"], "venue": "Uncertainty in Artificial Intelligence: Pro\u00ad ceedings of the Thirteenth Conference (pp. 198-207)", "citeRegEx": "Greiner et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Greiner et al\\.", "year": 1997}, {"title": "A tutorial on learning with Bayesian networks", "author": ["D. Beckerman"], "venue": null, "citeRegEx": "Beckerman,? \\Q1998\\E", "shortCiteRegEx": "Beckerman", "year": 1998}, {"title": "Creating an empirical basis for adaptation deci\u00ad sions", "author": ["A. Jameson", "B. GroBmann-Hutter", "L. March", "R. Rummer"], "venue": "In H. Lieberman (Ed.), IU/2000: International Con\u00ad ference on Intelligent User Interfaces (pp. 149-156)", "citeRegEx": "Jameson et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Jameson et al\\.", "year": 2000}, {"title": "Rethinking the learning of belief network probabilities", "author": ["R. Musick"], "venue": "Proceedings of the Second International Confer\u00ad ence on Knowledge Discovery and Data Mining (pp. 120125)", "citeRegEx": "Musick,? \\Q1996\\E", "shortCiteRegEx": "Musick", "year": 1996}, {"title": "Accelerating EM: An empirical study", "author": ["L.E. Ortiz", "L.P. Kaelbling"], "venue": null, "citeRegEx": "Ortiz and Kaelbling,? \\Q1999\\E", "shortCiteRegEx": "Ortiz and Kaelbling", "year": 1999}, {"title": "Probabilistic reasoning in intelligent systems: Networks of plausible iJiference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Local learning in probabilistic networks with hidden variables", "author": [], "venue": "Proceedings of the Fourteenth In\u00ad ternational Joint Conference on Artificial Intelligence (pp", "citeRegEx": "K.,? \\Q1995\\E", "shortCiteRegEx": "K.", "year": 1995}, {"title": "Fundamental concepts of qualitative probabilistic networks", "author": ["M.P. Wellman"], "venue": "Artificial Intelligence,", "citeRegEx": "Wellman,? \\Q1990\\E", "shortCiteRegEx": "Wellman", "year": 1990}], "referenceMentions": [{"referenceID": 11, "context": "Within the framework of qualitative probabilistic networks (Wellman, 1990), Druzdzel and van der Gaag ( 1995) give formal probabilistic definitions of several types of quali\u00ad tative relationships that can hold between nodes in a BN.", "startOffset": 59, "endOffset": 74}], "year": 2011, "abstractText": "Algorithms for learning the conditional proba\u00ad bilities of Bayesian networks with hidden vari\u00ad ables typically operate within a high-dimensional search space and yield only locally optimal so\u00ad lutions. One way of limiting the search space and avoiding local optima is to impose quali\u00ad tative constraints that are based on background knowledge concerning the domain. We present a method for integrating formal statements of qual\u00ad itative constraints into two learning algorithms, APN and EM. In our experiments with synthetic data, this method yielded networks that satisfied the constraints almost perfectly. The accuracy of the learned networks was consistently superior to that of corresponding networks learned without constraints. The exploitation of qualitative con\u00ad straints therefore appears to be a promising way to increase both the interpretability and the accu\u00ad racy of learned Bayesian networks with known structure. If you don 't know where you're going, you might wind up someplace else. -Yogi Berra", "creator": "pdftk 1.41 - www.pdftk.com"}}}