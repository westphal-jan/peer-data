{"id": "1301.6744", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Mixture Approximations to Bayesian Networks", "abstract": "structure inputs and parameters in a uniform bayesian network uniquely specify the probability distribution of of the modeled population domain. the locality of both structure and probabilistic information are the primary great benefits of bayesian networks and require the modeler to only specify local mathematical information. seeing on the other hand this locality of spatial information might prevent the modeler - and subsequently even more any other person - from obtaining a general overview of the important relationships within the domain. the goal of the proposed work literature presented in evaluating this paper is to provide an \" alternative \" view on the knowledge encoded in a bayesian network which might sometimes be very helpful for providing insights into the precise underlying domain. the basic idea is to calculate a mixture approximation to observe the probability distribution represented by the bayesian network. the mixture component densities can be thought of as approximate representing typical scenarios implied by doing the statistical bayesian wiener model, providing intuition about the basic relationships. as an additional benefit, performing inference in assuming the approximate model is very simple interactive and intuitive and can provide additional valuable insights. the computational processing complexity for the calculation of the mixture approximations criticaly depends on the measure which defines the distance between the probability distribution represented by the bayesian network and the approximate distribution. both the kl - divergence and the backward kl - divergence lead to inefficient algorithms. incidentally, the latter is used in recent work on modeling mixtures of special mean field solutions to which query the work presented, here is closely related. we show, however, that formulation using inverse a mean squared error exponential cost tool function leads to update equations which both can be solved using the junction tree algorithm. we conclude finally that the mean squared error cost function can be used for bayesian networks in defining which inference based on the junction tree is tractable. for large networks, however, one may have to rely on special mean k field approximations.", "histories": [["v1", "Wed, 23 Jan 2013 16:01:18 GMT  (280kb)", "http://arxiv.org/abs/1301.6744v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["volker tresp", "michael haft", "reimar hofmann"], "accepted": false, "id": "1301.6744"}, "pdf": {"name": "1301.6744.pdf", "metadata": {"source": "CRF", "title": "Mixture Approximations to Bayesian Networks", "authors": ["Volker Tresp", "Michael Haft", "Reimar Hofmann"], "emails": [], "sections": [{"heading": null, "text": "Structure and parameters in a Bayesian net work uniquely specify the probability distri bution of the modeled domain. The locality of both structure and probabilistic informa tion are the great benefits of Bayesian net works and require the modeler to only spec ify local information. On the other hand this locality of information might prevent the modeler -and even more any other person from obtaining a general overview of the im portant relationships within the domain. The goal of the work presented in this paper is to provide an \"alternative\" view on the knowl edge encoded in a Bayesian network which might sometimes be very helpful for pro viding insights into the underlying domain. The basic idea is to calculate a mixture ap proximation to the probability distribution represented by the Bayesian network. The mixture component densities can be thought of as representing typical scenarios implied by the Bayesian model, providing intuition about the basic relationships. As an addi tional benefit, performing inference in the approximate model is very simple and intu itive and can provide additional insights. The computational complexity for the calculation of the mixture approximations critically de pends on the measure which defines the dis tance between the probability distribution represented by the Bayesian network and the approximate distribution. Both the KL divergence and the backward KL-divergence lead to inefficient algorithms. Incidentally, the latter is used in recent work on mixtures of mean field solutions to which the work pre sented here is closely related. We show, how ever, that using a mean squared error cost function leads to update equations which can\nbe solved using the junction tree algorithm. We conclude that the mean squared error cost function can be used for Bayesian net works in which inference based on the junc tion tree is tractable. For large networks, however, one may have to rely on mean field approximations.\n1 Introduction\nOne of the appealing aspects of Bayesian networks is their modularity; the modeler has to specify only local information and may thus generate a complex model step by step. As a drawback of local modeling one might very soon loose the overview of the relation ships in the joint model. The goal of the work pre sented in this paper is to provide an \"alternative\" view on the domain knowledge encoded in a Bayesian net work which might sometimes be very helpful for pro viding insights into the underlying domain. The basic idea is to calculate a mixture approximation to the probability distribution represented by the Bayesian network. The mixture component densities can be thought of as representing typical scenarios as com ponents of the joint distribution thus providing intu ition about the basic relationships in the domain. It can be argued that reasoning in terms of cases or sce narios is very close to the human approach to deal ing with complex domains. This idea was explored by Druzdzel and Henrion (1990) who used as sce narios the most likely configurations given some ev idence (although they didn't provide efficient algo rithms for finding those). A mixture of 'scenarios' can be computed both for the joint distribution encoded by the Bayesian net and for a conditional distribu tion given evidence. In the latter case our approach can illustrate the relationships in the joint distribution of the unknown variables given the evidence which is not straightforward in the standard evidence propaga tion algorithms which primarily only calculate simple\n640 Tresp, Haft, and Hofmann\nmarginals. The challenge in our approach lies in cal culating the optimal parameters in the mixture mod els. It turns out that the computational complexity for the calculation of the mixture approximations crit ically depends on the measure which is used for calcu lating the distance between the probability distribu tion represented by the Bayesian network and the ap proximate distribution. We show that finding the best approximation using the most natural metric, i.e. the Kullback-Leibler (KL-) distance is computationally in feasible. The same is true if the backward KL-distance is used. Haft, Hofmann and Tresp (1997, 1999) have shown that for the latter case, a reasonable approxima tion can be found by first finding the local minima of the backward KL-distance between the Bayesian net work model and one mixture component (which cor responds to solving the mean field equations) and by forming the mixture model in a second step by using a small overlap assumption. A new approach pursued in this paper is to replace the KL-distance as a distance measure by the mean squared distance between the probability distribution described by the Bayesian net work model and the mixture approximation. We can show that if either the mean squared distance or the expected mean squared distance is used, the resulting equations can be solved using the junction tree algo rithm (Lauritzen and Spiegelhalter, 1988, Jensen, Lau ritzen and Olsen, 1990). The algorithms are therefore exactly then efficient, when inference in the Bayesian network itself is efficient. The ideas here are devel oped for binary Bayesian networks but can easily be generalized to general graphical models, discrete mod els and Gaussian graphical models. In the next Sec tion we develop and discuss the different algorithms for obtaining mixture approximations in the joint proba bility distribution. In the following Section we show how evidence can be taken into account. In Section 4 we perform a mixture analysis of the well-known chest clinic example. We demonstrate how typical scenarios can be extracted, how inference can be performed in the approximate model and how simple probabilistic rules can be extracted. Finally, in Section 5 we present conclusions.\n2 Theory\nAssume a Bayesian network with N variables with probability distribution P(x) which factorizes as\nN P(x) = II P(xjiiij)\u00b7\nj=l\nAs stated in the introduction our goal is to find a mix ture approximation to P(x) of the form\nM M P(x)RjQ(x) =LQ(i,x) =Lq;Q(xli). (1)\ni=l i=l For simplicity, we focus in this paper on the case that x = (xi, ... , XN )' is a vector of binary variables Xj E {0, 1} and that the mixture component distributions factorize into binomial distributions\nN Q(xli) = II q7J (1- Qij)!-x;. (2) j=l\nThe goal is now to determine the model parameters {q;j}t;1;=I and {q; }t;1 such that we obtain a good approximation to P(x). First, we need to define a dis tance measure which specifies what exactly we mean by a \"good\" approximation. In the next sections we will define different distance measures and we will show that the complexity of the resulting update equations very much depends on the distance measure which is selected.\n2.1 KL-divergence\nThe KL-divergence has the form\nKL(P(x)IIQ(x)) = -L P(x) log \ufffdi:i \u00b7 X\nThis cost function might be considered the most nat ural cost function since it corresponds to drawing an infinite number of samples from P(x) and to then do a maximum likelihood mixture modeling approach of the data. The K \u00a3-distance can be minimized using an EM algorithm. The E-step calculates Vi and V configu rations of x\nand the M-steps update Vi, j\nand Vi\nI:x,x;=! P(x)Q(ilx) Qij = I:x P(x)Q(ilx)\nI:x P(x)Q(ilx) q; = I:x I;\ufffd P(x)Q(ilx) .\nSince the summations in the update equations are over exponentially many states of x and since Q( ilx) can not be easily decomposed, the update equations are infeasible for large networks. The only exception is if\nMixture Approximations to Bayesian Networks 641\nM = 1, since then the M-step reduces to q1j = P(xj) and simply calculates the marginal distributions which can be calculated efficiently using the junction tree al gorithm. A simple approximate solution can be obtained by generating a large number of samples from a given Bayesian network and by then calculating the model which maximizes the likelihood w.r.t. the data us ing the corresponding EM-algorithm. In this case, the sums in the previous EM-equations contain only as many terms as data are generated. Although this ap proach should be feasible in most cases, if some mix ture components only obtain small probabilities, one might have to generate a large number of samples to obtain good parameter estimates.\n2.2 The Backward KL-divergence\nThere is a clear asymmetry in the KL-distance w.r.t the two distributions P(x) and Q(x). It is therefore also common to optimize the \"backward\" KL-distance defined as\nBKL(P(x)IIQ(x)) = KL(Q(x)IIP(x))\n\"\"\"\"' P(x) =- L.. Q(x) log Q(x)\" X Note that the role of Q(x) and P(x) is interchanged and the expectation is calculated with respect to the simpler approximate distribution Q(x). For M = 1 (i.e. only one component), minima of BK L can be found by iterating the associated mean field equations which for the binary case ( 2) read\nThe previous equation is iterated repeatedly for all j until convergence where Mj C x denotes the elements in the Markov blanket of Xj and\nsig(x) = [1+ exp(-x)t1. The previous update equation is efficient since it re quires the summation only over the elements in the Markov blanket of x j and therefore only involves lo cal computations. Haft, Hofmann and Tresp (1999) have shown that for Bayesian networks the update equations can further be simplified. Note, that -as for example in the mean field approximation to the Boltzmann machine- the sigmoid transfer function is a result of using the mean field approximation and was not postulated as in the work on sigmoid belief networks (Saul, Jaakkola and Jordan, 1996).\nAlthough the previous mean field update equations can be used to find local optima in the B K L-distance, calculating the optimal best mixture approximation as for the KL-distance- involves the summation over all states of the Bayesian network. Haft, Hofmann and Tresp (1999) have therefore suggested to find M local optima of the BKL-distance and use those as compo nents in the mixture model. The mixture weights can then be calculated using a small overlap assumption as\nwhere C normalizes the Qj and Q(xjli) = q7j(1Qij )1-xi . Note, that again only local computations are required. Further approximate solutions have been derived by Bishop, Lawrence, Jaakkola and Jordan (1998), and Lawrence, Bishop and Jordan (1998) for special graph ical models such as Boltzmann machines and sigmoid belief networks.\n2.3 The Mean Squared Error\nLet's consider the squared error cost function\nSE(P(x)IIQ(x)) = L(P(x) - Q(x))2 X\nM M = L P(x)2 + L L L q;qkQ(xli)Q(xlk)\nX\nM -2 L L P(x) q;Q(xli).\ni=l X (3)\nThe advantage now is that the cost function is a sim ple quadratic expression in the the parameters of the approximating mixture distribution (1). The SE cost function can be motivated by considering a Taylor ex pansion to the KL-distance which yields as a distance measure l::x(P(x)- Q(x)j2 / P(x). By taking a closer look at Equation 3 we notice that all sums over x have the form\nN LP(x)A IT fi(xj)\nX j:l (4)\nwhere A is an integer and /j is a function of x j only, Vj.\nWe make the following observation:\nObservation 1 All summations of the form of Equa tion 4 can be calculated efficiently using, e.g., the junc tion tree algorithm, as long as the distribution P itself can be handled efficiently.\n642 Tresp, Haft, and Hofmann\nTo see this it is sufficient to note that the structure of the sum ( 4) is still that of the original Bayesian network and therefore can be factorized the same way. To be explicit,\nN N LP(x)A ITfi(xi) =LIT P(xiiiii)A/j(xi)\u00b7 (5)\nj=1 X j=l Formally, fi ( x j) assumes the role of soft evidence in a Bayesian network defined by the potentially un normalized conditional probabilities P( x j I IIi )A (Ap pendix 6.1). We can calculate update equations by setting the derivatives of the cost function with respect to the pa rameters to zero. We obtain\n1 q;j = 2 2 \"' Q(. \\ )2 X (6) qi L....x\\xi z, X Xj\n(L(2xj -1)P(x)Q(i,x\\ xj) + L Q(i,x \\xj)2 X x\\Xj\n+ 2::(1- 2qtj) L Q(xill) Q(i, x \\ Xj) Q(j, x \\ Xj)) l,l;ti x\\xi\nwhere Q( i, X \\ Xj) = q; nf=1,k;t!j qft (1 - q;k )1-x\u2022. All summations can be calculated efficiently by using Ob servation 1. Updating the parameters using Equa tion 6 performs component wise minimization of the cost function. The optimization of the component weights q; is a low dimensional quadratic zPtimization problem with con straints q; > 0 and L:;=1 q; = 1. The gradient of SE with respect to a mixture component can also be cal culated efficiently using Observation 1. Note that since we can evaluate the distance measure SE efficiently, we can easily decide if we supplied a sufficient number of mixture components for obtaining a good approximation.\n2.4 The Expected Mean Squared Error\nAlternatively, we might consider the expected mean squared error\nESE(P(x)IIQ(x)) = LP(x)(P(x)- Q(x))2\u2022\nSimilar as in the previous section, we obtain the fixed point equations for the q;j,\n1 % = Ex P(x)Q(i, X \\ Xj)2 X\n(7)\n(L(2xj-1)P(x)2Q(i, x\\xj)2+ L P(x)Q(i, x\\xj)2 x x,xj=O\n+ L L(l-2xj)P(x) Q(xjll) Q(i,x\\xi) Q(l,x\\xj)) l,lt;i X\nAll sums can be calculated efficiently using Observa tion 1.\n3 Evidence and Inference\nIf evidence is entered into the Bayesian network we can obtain a mixture approximation to the conditional dis tribution of the remaining variables given the evidence in the same way as described in the previous section. The mixture distribution for the unknown nodes may then be viewed as defining possible scenarios given our state of information. Having already computed a mixture approximation to the joint distribution there is of course a much simpler way to obtain a mixture model for the conditional dis tribution. Rather than propagating the evidence in the exact model and then reapproximating the conditional distribution with new mixture components and mixing weights, one can perform approximate inference in the mixture model directly:\nwith Q(eli) = II q\ufffdj(l- %)1-xi x;Ee\nand Q(uli) = II q\ufffdj(l- %)1-xi x;Eu where e C x are the variables with evidence and u = x \\ e are the remaining variables. The above equa tion shows that the conditional distribution Q(ule) is still a mixture distribution composed of the same 'un conditional scenarios', but the new mixture weights are proportional to\nq;Q(eli). That is, the effect of evidence is just a reweighting of the known scenarios, which makes the effects of the evidence clearly visible. Each mixture component is weighted by the product of q; (as before) and the prob ability of the evidence Q(eli) for that mixture compo nent.\n4 Experiments\n4.1 Modeling\nThe goal of our experiments was to test the quality of the mixture solutions found by the different cost\nfunctions and algorithms. As a test case we used the well known chest clinic example. The chest clinic is a well documented Bayesian network (Lauritzen and Spiegelhalter, 1988) consisting of the 8 variables visit to asia (1), smoker (2), tuberculosis (3), lung cancer (4), bronchitis (5), tuberculosis or lung cancer (6), pos itive x-rays (7) and dyspnoea (8) . In this domain with only eight variables the algorithms for all cost func tions could be executed in reasonable time. The net work structure is defined in Appendix 6.2. In the first experiment the EM-algorithm using the KL-distance measure as described in Section 2.1 was used. Figure 1 shows the KL-distance between the true probability distribution and the approximate distribu tion using different number of components. A small KL-divergence is reached for only 4 mixture compo nents. If more components are used, the description is further refined but the KL-divergence does not im prove significantly. In the next experiment, the mixture parameters were calculated by minimizing the KL-distance and the SE and ESE distance measures using four and five mixture components. Table 1 and 2 show that for four or five mixture components only three mixture components obtain weights of more than 1% for all approaches. Figure 2 and 3 plot q;; (the probability for a positive finding of node j in scenario i) for the solutions ob tained by the three algorithms. Tables 1 and 2 show the mixture weights and the KL-distance of the mix ture approximations to the true probability distribu tion. We see that approximately half of the weight is assigned to a scenario describing healthy patients with a very low probability of any symptom and with ap proximately 30% probability of being a smoker. This mixture component really models two scenarios: one in which the healthy patient is a smoker and one where he or she is not a smoker. Approximately 40% prob ability mass is assigned to a group of patients with very high probability of bronchitis, high probability of dyspnoea and above average probability of being a smoker. Similarly as for the healthy patient, this mix ture component can be thought of as modeling four patient groups all of which have bronchitis but vary in the four possible configurations of being smokers and having dyspnoea.1 Approximately 5% probability mass is assigned to a group of patients with a very high probability of a positive x-ray, dyspnoea, a very high probability of lung cancer and a high probability of having bronchitis. In the model with five mixture\n'Note, that given the states of the remaining variables in this configuration, smoker and dyspnoea are likely to be independent; otherwise the mixture approximation would have had the tendency to assign more than one component to the bronchitis scenario.\nMixture Approximations to Bayesian Networks 643\ncomponents about 1% or less probability mass is as signed to a group of persons with tuberculosis, slightly higher than normal probability of having visited asia, and with a high probability of having a positive x-ray and dyspnoea. In the four component model, both the KL-distance model and the SE model have also converged to this solution. Interestingly, the mixture components really \"make up their mind\" and converge to clearly identifiable sce narios. The solutions from the different approaches all agree in the scenarios which have obtained consid erable probability mass. In terms of KL-divergence (recall that KL-divergence is only minimized for one approach) the SE solution performs very well whereas the ESE-solution is considerably worse. This might be explained by the fact that ESE does not tend to penalize errors for states with small probabilities. Based on these experiments we might conclude that the SE-approach can provide solutions very simi lar both qualitatively and quantitatively to the KL solution. Figure 4 displays the results using the mixture of mean field solutions approach (Haft, Hofmann and Tresp, 1999). First of all it is interesting to note that there are exactly three optima of the mean field equations. Approximately 91% of the probability mass is here as signed to the \"normal\" patient group with an average probability for smoking and bronchitis. The main dif ference to the first scenario of Figure 2 is that here the probability for bronchitis is higher. The second com ponent with approximately 7% of probability mass is almost identical to the third component in the previ ous solutions. Finally, the third solution with approx imately 1% of probability mass is very similar to the fourth component in the previous five component mod els. It appears that the main difference to the previous solutions is that the first component of the mean field solution approximates the first two components of the mixture approximations in the previous experiments.\n4.2 Evidence and Inference\nWhen we enter evidence as described in Section 3 the scenarios are correspondingly re-weighted. Table 3 de scribes the new mixture weights when the evidence\n644 Tresp, Haft, and Hofmann\nFigure 1: The mixture approximation using the KL distance. The x-axis indicates the number of mixture components and the y-axis the KL-distance between the mixture model and the Bayesian network.\nTable 2: KL-divergence and component weights for the five component model.\nI Appr I KL-dist I q1 I q2 I q3 I q4 I q5\nKL 0.0020 0.517 0.404 0.055 0.010 0.014 SE 0.0056 0.522 0.415 0.043 0.009 0.012 ESE 0.0360 0.513 0.422 0.052 0.005 0.008\n\"dyspnoea=yes\" and \"smoker=yes\" is entered. One can see, for example, that for the SE metric the first scenario (healthy person) has lost and the second (per son with bronchitis) and third (sick smoker) scenarios have gained weight as a result of the evidence - if compared to unconditional weights in Table 1. Ta ble 4 shows the marginal posterior probabilities com puted from the different mixture models. As could be expected from the low KL-distances between the ex act and all mixture models, the estimated probabilities are very close to the true probabilities. The KL-metric shows the best and the ESE-metric the worst approxi mation, but all three are within around one percentage point from the exact posteriors for all variables. Table 5 and Table 6 show the same experiment for the unlikely evidence \"visit to Asia=yes\" and \"x ray=positive\" which has a probability of only 0.15% in the model. Even though the KL-distance between the exact (unconditioned) model and the mixture models is low, the KL-distance of the corresponding condi tioned models can be large, because a small absolute error in the estimated probability of an unlikely event causes only a small KL-distance, but a large relative er ror in the probability of that event. This, however, can lead to a large absolute error in the probabilities con ditioned on that event. We therefore expect stronger deviations in the probabilities for this second example. Table 6 shows indeed stronger deviations than Table 4. The KL-model is still extremely good, the SE-model shows deviations in the range of 5% percentage points, whereas the ESE-model is completely wrong in its es-\nI\u00b7L_ \ufffd _:_ \ufffd \ufffd _:_ \ufffd- \ufffdOJ\nH\ufffd \u2022\ufffd -\ufffd _:_ - _: __ :_ m1 H \ufffd lil : \u2022 \ufffd \ufffd- \ufffdl i\u00b7[ \ufffd-[ \ufffd- \ufffd l d_ \ufffdI\nvisit asia smoker tuberc. cancer brooch. lor c. x-ray dyspnoea\nFigure 2: The component parameters qij for the mix ture model with four components i = 1, 2, 3, 4. The filled bars show the result using the KL-divergence, the gray bars using the SE error function, and the empty bars using the ESE-error function.\nIt \ufffd : : : : \ufffd J l\u00b7L_ iiJ _:_ _:_I_:_ \ufffd Ill It_ I \ufffd-- \ufffd --\u00b7l lt It\ufffd \ufffdI_:_\ufffd\ufffd- \ufffdl d_ _:_1 \ufffdD 1 j_ \ufffd\ufffd\ufffd\nvisit asia smoker tubeoc. cancer bronch. lor\ufffd x-ray dyspooea\nFigure 3: Same as in Figure 2 but with five compo nents.\nMixture Approximations to Bayesian Networks 645\nFigure 4: The solution obtained by the mean field ap proximation. There are exactly three solutions. The mixture weights are 0.919 (component 1), 0.069 (com ponent 2) and 0.012 (component 3). The KL-distance between the true probability distribution and the mix ture approximation is 0.304.\ntimation of the probability of TBC. This is probably due to the fact that deviations from the exact model in regions of low probability have a particularly low weight in the ESE-metric. So both the KL- and the SE-model still give useful results even in this case of unlikely evidence.\n5 Conclusions\nWe have demonstrated that a mixture approximation can give interesting new insights into the relationships between the variables in a probability domain mod eled by a Bayesian network. Furthermore, we have shown that the complexity of the computational cost for calculating the parameters in the mixture mod els depends critically on the distance measure. For a quadratic distance measure, parameter optimization can be executed efficiently using the junction tree al gorithm. We have show that experimentally, the so-\nlutions found by minimizing the KL-distance (which is computationally infeasible for large networks) and a squared error cost function are basically identical. The significant components in each mixture model clearly make up their mind and are very different. We have compared these results to those obtained using multiple mean field solutions. Here, maxima in the backward KL-distance are found and form the mix ture components. The mean field solutions have been used recently for approximate inference and learning in various large intractable networks, for example by Pe terson and Anderson (1987), Saul, Jaakkola and Jor dan (1996), Kappen and Rodriguez (1997), Bishop, Lawrence, Jaakkola and Jordan (1998), Lawrence, Bishop and Jordan (1998) and Haft, Hofmann and Tresp (1999). For those large networks in which the junction tree algorithm is computationally infeasible, mean field approximations are still the only viable op tion for inference, besides Monte Carlo methods. For smaller networks in which the junction tree algorithm can be used, the algorithms presented here are applica ble. Finally, we have shown how the mixture approxi mation can be used to obtain both a mixture model for the unknown nodes composed of reweighted scenarios and approximations to conditional probabilities. The inference based on the mixture approximation appears to be reasonable precise, as long as the inference does not depend on a very accurate model of the underlying probability distribution; the latter is the case when the evidence entered into the model is very unlikely.\nAcknowledgements\nThe comments of the three anonymous reviewers were very helpful.\nReferences\n[1] C. M. Bishop, N. D. Lawrence, T. Jaakkola, and M. I. Jordan. Approximating posterior distribu tions in belief networks using mixtures. In M. I. Jordan, M. J. Kearns, and S. A. Solla, editors, Advances in Neural Information Processing Sys tems 10, pages 416-422. MIT Press, Cambridge MA, 1998.\n[2] J. Druzdzel, M. and M. Henrion. Using scenarios to explain probabilistic inference. In Workshop\n646 Tresp, Haft, and Hofmann\non Explanation, pages 133-141. American Asso ciation for Artificial Intelligence, 1990.\n[3] H. Haft, R. Hofmann, and V. Tresp. Model independent mean-field theory as a local method for approximate propagation of information. Net work, 10(1):93-105, 1999.\n[4] M. Haft, R. Hofmann, and V. Tresp. Model-independent mean field theory as a local method for approx imate propagation of information. Technical Re port, http:/ jwww7.informatik.tu-muenchen.de/\ufffd hofmannr/mf-abstr.html, 1997.\n[5] F. V. Jensen, S. L. Lauritzen, and K. G. Olsen. Bayesian updating in causal probabilistic net works by local computations. Computational Statistics Quaterly, 4:269-282, 1990.\n[6] H. J. Kappen and F. B. Rodriguez. Mean field ap proach to learning in boltzmann machines. Pat tern Recogntion Letters, 18:1317-1322, 1997.\n[7] S. L. Lauritzen and D. J. Spiegelhalter. Lo cal computations with probabilities on graphical structures and their application to expert sys tems. J. Roy. Statist. Soc. B, 50:154-227, 1988.\n[8] N. D. Lawrence, C. M. Bishop, and M. I. Jor dan. Mixture representations for inference and learning in boltzmann machines. In G. F. Cooper and S. Moral, editors, Uncertainty in Artificial Intelligence: Proceedings of the Fourteenth Con ference, pages 320-327. Morgan Kaufmann, San Francisco, CA, 1998.\n[9] C. Peterson and J. R. Anderson. A mean field the ory learning algorithm for neural networks. Com plex Systems, 1:995-1019, 1987.\n[10] K. L. Saul, T. Jaakkola, and M. I. Jordan. Mean field theory for sigmoid belief networks. Journal of Artificial Intelligence Research, 4:61-76, 1996.\n6 Appendix\n6.1 Interpretation of fJ (xj) as Soft Evidence\nConsider the last term in Equation 3. If network we add soft evidence nodes ej with conditional densities P(ejlxJ) ex q;Q(xJii) to the original we obtain for the joint probability distribution P(x) IJj P(eilxJ)\u00b7 The likelihood of the evidence is"}], "references": [{"title": "Approximating posterior distribu\u00ad tions in belief networks using mixtures", "author": ["C.M. Bishop", "N.D. Lawrence", "T. Jaakkola", "M.I. Jordan"], "venue": "Advances in Neural Information Processing Sys\u00ad", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Using scenarios to explain probabilistic inference", "author": ["M.J. Druzdzel", "M. Henrion"], "venue": "American Asso\u00ad ciation for Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Model\u00ad independent mean-field theory as a local method for approximate propagation of information", "author": ["H. Haft", "R. Hofmann", "V. Tresp"], "venue": "Net\u00ad work,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Model-independent mean field theory as a local method for approx\u00ad imate propagation of information", "author": ["M. Haft", "R. Hofmann", "V. Tresp"], "venue": "Technical Re\u00ad port,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Bayesian updating in causal probabilistic net\u00ad works by local computations", "author": ["F.V. Jensen", "S.L. Lauritzen", "K.G. Olsen"], "venue": "Computational Statistics Quaterly,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Mean field ap\u00ad proach to learning in boltzmann machines", "author": ["H.J. Kappen", "F.B. Rodriguez"], "venue": "Pat\u00ad tern Recogntion Letters,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Lo\u00ad cal computations with probabilities on graphical structures and their application to expert sys\u00ad tems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "J. Roy. Statist. Soc. B,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1988}, {"title": "Mixture representations for inference and learning in boltzmann machines", "author": ["N.D. Lawrence", "C.M. Bishop", "M.I. Jor\u00ad dan"], "venue": "Uncertainty in Artificial Intelligence: Proceedings of the Fourteenth Con\u00ad ference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "A mean field the\u00ad ory learning algorithm for neural networks", "author": ["C. Peterson", "J.R. Anderson"], "venue": "Com\u00ad plex Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1987}, {"title": "Mean field theory for sigmoid belief networks", "author": ["K.L. Saul", "T. Jaakkola", "M.I. Jordan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}], "referenceMentions": [], "year": 2011, "abstractText": "Structure and parameters in a Bayesian net\u00ad work uniquely specify the probability distri\u00ad bution of the modeled domain. The locality of both structure and probabilistic informa\u00ad tion are the great benefits of Bayesian net\u00ad works and require the modeler to only spec\u00ad ify local information. On the other hand this locality of information might prevent the modeler -and even more any other person\u00ad from obtaining a general overview of the im\u00ad portant relationships within the domain. The goal of the work presented in this paper is to provide an \"alternative\" view on the knowl\u00ad edge encoded in a Bayesian network which might sometimes be very helpful for pro\u00ad viding insights into the underlying domain. The basic idea is to calculate a mixture ap\u00ad proximation to the probability distribution represented by the Bayesian network. The mixture component densities can be thought of as representing typical scenarios implied by the Bayesian model, providing intuition about the basic relationships. As an addi\u00ad tional benefit, performing inference in the approximate model is very simple and intu\u00ad itive and can provide additional insights. The computational complexity for the calculation of the mixture approximations critically de\u00ad pends on the measure which defines the dis\u00ad tance between the probability distribution represented by the Bayesian network and the approximate distribution. Both the KL\u00ad divergence and the backward KL-divergence lead to inefficient algorithms. Incidentally, the latter is used in recent work on mixtures of mean field solutions to which the work pre\u00ad sented here is closely related. We show, how\u00ad ever, that using a mean squared error cost function leads to update equations which can be solved using the junction tree algorithm. We conclude that the mean squared error cost function can be used for Bayesian net\u00ad works in which inference based on the junc\u00ad tion tree is tractable. For large networks, however, one may have to rely on mean field approximations.", "creator": "pdftk 1.41 - www.pdftk.com"}}}