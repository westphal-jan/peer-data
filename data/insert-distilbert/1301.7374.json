{"id": "1301.7374", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Learning the Structure of Dynamic Probabilistic Networks", "abstract": "dynamic probabilistic networks are a small compact representation of complex stochastic processes. in this paper we examine how to learn describing the structure of a dpn filter from data. after we extend structure scoring rules for standard probabilistic optimization networks to the dynamic case, and show how to search for structure when some of quite the variables are hidden. finally, afterwards we examine two applications where such a technology might genuinely be useful : predicting and classifying dynamic behaviors, and learning causal orderings in biological processes. we provide many empirical results exactly that demonstrate the applicability thesis of our methods in both domains.", "histories": [["v1", "Wed, 30 Jan 2013 15:03:42 GMT  (457kb)", "http://arxiv.org/abs/1301.7374v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["nir friedman", "kevin murphy", "stuart russell"], "accepted": false, "id": "1301.7374"}, "pdf": {"name": "1301.7374.pdf", "metadata": {"source": "CRF", "title": "Learning the Structure of Dynamic Probabilistic Networks", "authors": ["Nir Friedman", "Kevin Murphy", "Stuart Russell"], "emails": ["nir@cs.berkeley.edu", "rnurphyk@cs.berkeley.edu", "russell@cs.berkeley.edu"], "sections": [{"heading": null, "text": "1 INTRODUCTION Probabilistic networks (PNs), also known as Bayesian net works or belief networks, are already well-established as representations of domains involving uncertain relations among several random variables. Somewhat less well established, but perhaps of equal importance, are dynamic probabilistic networks (DPNs), which model the stochastic evolution of a set of random variables over time [5]. DPNs have significant advantages over competing representations such as Kalman filters, which handle only unimodal pos terior distributions and linear models, and hidden Markov models (HMMs), whose parameterization grows exponen tially with the number of state variables. For example, [31] show that DPNs can outperform HMMs on standard speech recognition tasks.\nPNs and DPNs are defined by a graphical structure and a set of parameters, which together specify a joint distribu tion over the random variables. Algorithms for learning the parameters of PNs [1, 21] and DPNs [1, 14] are becoming widely used. These algorithms typically use either gradi ent methods or EM, and can handle hidden variables and missing values.\nAlgorithms for learning the graphical structure, on the other hand, have until recently been restricted to networks with complete data, i.e., where the value\ufffd of all variables are specified in each training case [ 4, 15]. Friedman [ 10, 11] has developed the Structural EM (SEM) algorithm for learning PN structure from data with hidden variables and missing values. SEM combines structural and parametric modifi cation within a single EM process, and appears to be sub stantially more effective than previous approaches based\non parametric EM operating within an outer-loop structural search. SEM can be shown to find local optima defined by a scoring function that combines the likelihood of the data with a structural penalty that discourages overly complex networks. This property holds for both the Bayesian Infor mation Criterion (BIC) score [28], a variant of Minimum Description Length (MDL) scoring, and the BDe score [ 15], a Bayesian metric that uses an explicit prior over networks.\nIn this paper, we extend the BIC and BDe scores to han dle the problem of learning DPN structure from complete data. More importantly, we extend the SEM algorithm to learn DPNs from incomplete data with both scores. Given partial observations of a set of random variables over time, the algorithm constructs a DPN (possibly including addi tional hidden variables) that fits the observations as well as possible. The addition of hidden variables in DPNs is particularly important as many processes-human de cision making, speech generation, disease processes, for example-are only partially observable.\nWe begin with formal definitions of PNs and DPNs. Sec tion 3 discusses the complete-data case, developing scores for DPNs so that existing algorithms for learning PN struc tures can be applied directly. In Section 4, we handle the case of incomplete data and show how to extend SEM to DPNs. Computation of the necessary sufficient statistics is highlighted as a principal bottleneck. Section 5 describes two applications: learning simple models of human driving behavior from videotapes, and learning models of biologi cal processes from very sparse observations.\n2 PRELIMINARIES\nWe start with a short review of probabilistic networks and dynamic probabilistic networks.\nWe will be concerned with distributions over sets of dis crete random variables, where each variable Xi may take on values from a finite set, denoted by Val( Xi)\u00b7 We denote the size of Val( Xi) by IIXill\u00b7 We use capital letters, such as X, Y, Z, for variable names and lowercase letters x, y, z to denote specific values taken by those variables. Sets of vari ables are denoted by boldface capital letters X, Y, Z, with sets of values denoted by boldface lowercase letters x, y, z. For a given network, we will use X = { X1, \u2022 . \u2022 , Xn} to de note the variables of the network in topological order, and P to denote a joint probability distribution over the variables\nin X. A probabilistic network (PN) is an annotated directed acyclic graph that encodes a joint probability distribution over X. Formally, a PN for X is a pair B = (G,E>). The first component, G, is a directed acyclic graph whose vertices correspond to the random variables Xt, ... ,Xn that encodes the following set of conditional independence assumptions: each variable Xi is independent of its non descendants given its parents Pa(Xi) in G. The second component, e, represents the set of parameters that quan tifies the network. In the simplest case, it contains a pa rameter Oi,J;,k; = Pr(Xi = kiiPa(Xi) = ji) for each pos sible value ki of Xi and each possible set of values ji of Pa(Xi). Each conditional probability distribution can be represented as a table, called a CPT (conditional probabil ity table). Representations which require fewer parameters, such as noisy-ORs [27] or trees [12], are also possible indeed, we use them in the experimental results section - but, for simplicity of notation, we shall stick to the CPT case.\nGiven G and 0, a PN B defines a unique joint probability distribution over X given by:\nPB(Xt, ... 'Xn) = n\ufffd=l PB(xilpa(Xi)) A PN describes a probability distribution over a fixed set of variables. DPNs extend this representation to model temporal processes. For simplicity, we assume that changes occur between discrete time points that are indexed by the non-negative integers. We assume that X = { Xt, ... , Xn} is a set of attributes that the process changes. Xi[t] is the random variable that denotes the value of the attribute xi at timet, and X[t] is the set of random variables Xi [t].\nTo represent beliefs about the possible trajectories of the process, we need a probability distribution over the random variables X[O] U X[l] U X[2] U.. .. Of course, such a distribution can be extremely complex. In this paper, we assume the process is Markovian in X, i.e., P(X[t + 1] I X[O], ... ,X[t]) = P(X[t + 1] I X[t]). We also assume that the process is stationary, i.e., that the tran sition probability P(X[t + 1]1 X[t]) is independent oft.\nGiven these assumptions, a DPN that represents the joint distribution over all possible trajectories of a process con sists of two parts:\n\u2022 a prior network B0 that specifies a distribution over initial states X[O]; and\n\u2022 a transition network B--+ over the variables X[O] U\nPB .... (x(1JI x(O]) = TI\ufffd=tPB .... (xi(1)lpa(Xi(1])).\nA DPN defined by a pair (Eo, B--+) corresponds to a semi-infinite network over the variables X[O], ... , X[oo]. In practice, we reason only about a finite interval 0, . . . , T. To do this, we can notionally \"unroll\" the DPN structure into a PN over X[O], ... ,X(T]. In slice 0, the parents of Xi[O] are those specified in the prior network Bo; in slice\nt + 1, the parents of Xi[t + 1] are those nodes in slices t and t + 1 corresponding to the parents of Xi[l] in B+ We copy the conditional distributions for these variables in a similar manner. Figure 1 (b) shows the result of unrolling the network in Figure 1(a) for 3 time slices. Given a DPN model, the joint distribution over X[O), ... , X[T) is\nPB(x(O], ... , x[T]) = PB0(x[O])TI[::(/ PB .... (x(t + 1] 1 x[t]) (1) where PB .... (x[t + 1] 1 x[t]) is obtained in the obvious way from the transition model.\n3 LEARNING from Complete Data In this section we develop the theory for learning DPNs from complete data. We begin with a brief review of how one learns standard PNs. Then we derive the details of the BIC and BDe score for DPNs and finally we discuss how these are optimized in the search for good structures. The upshot of this section is that learning DPNs from complete data uses the same techniques as learning PNs from complete data. Learning DPNs is not quite the same as applying PN methods to the unrolled network, however, due to the constraint of repeated structure and repeated parameters.\n3.1 LEARNING PNS The problem of learning a probabilistic network is stated as follows. Given a training set D of instances of X, find a network B = ( G, 0) that best matches D. The notion of \"best match\" is defined using a scoring function. Sev eral different scoring functions have been proposed in the literature. The most frequently used are the Bayesian Infor mation Criterion [28] and the BDe score [ 15]. Both of these\n1 Some authors define DPNs using just the transition network, assuming that all \"slices,\" including slice 0, have the same struc ture. In general, however, the prior distribution on X[O] can have a quite different independence structure from other slices, since it may represent either the way in which the process is initialized or the dependencies induced among the variables in X[O] by the unobserved portion of the process before t = 0. For example, if t = 0 represents an arbitrary starting point taken from an infinite process, th\ufffdn it is reasonable to use a dependency structure for X[ OJ that reflects the stationary distribution of the Markov chain defined by the transition network. One can combine the prior and transition networks into a single two-slice network, but, be cause the learning processes for the prior and transition models are distinct, we have chosen to represent them separately.\nLearning the Structure of Dynamic Probabilistic Networks 141\nscores combine the likelihood of the data according to the network, L(B : D) = log Pr(D I B), with some penalty relating to the complexity of the network. When learning the structure of PN s, a complexity penalty is essential since the maximum-likelihood network is usually the completely connected network.\nThe BIC and BDe scores are derived from the posterior probability of the network structure. Let the random vari able G range over the possible network structures that might in fact obtain in the real world. Then, using Bayes' rule, the posterior distribution over G is\nPr( G l D) ex: Pr(D I G) Pr( G) where the likelihood of the data given a network structure can be computed by conditioning on the associated network parameters:\nPr(D 1 G) = f Pr(D 1 G, e) Pr(e 1 G) de. (2) Obviously, specifying the parameter priors and evaluating this integral can be difficult.\nOne approach to avoiding full computation of the inte gral in (2) is to examine the asymptotic behavior of this term. Given a large number of data points, the posterior probability is insensitive to the choice of prior (assuming that the prior does not give probability zero to any event). Schwarz [28] derives the following asymptotic estimate for well-behaved priors:\nlogPr(D I G) = logPr(D I G, Ba)- lo\ufffdN #G + 0(1), (3) where E>a are the parameter settings for G that maximize the likelihood of the data, N is the number of training instances, #G is the dimension of G (which in the case of complete data is just the number of parameters), and 0 ( 1) is a constant term which is independent of N and G. The BIC score uses E quation (3) to rank candidate network structures. Notice that it obviates the need for parameter priors, and the prior on structures is reduced to counting parameters.2\nAn alternative approach is to evaluate (2) in closed form given a restricted family of priors [3, 4, 15]. Roughly speaking, the prior on the parameters for a given structure G is assumed to factor into independent priors over the pa rameters for each conditional distribution P(Xi I pa(Xi) ) . When the data is complete, this implies that the posterior factors in the same way. Thus, each conditional distribu tion can be updated and scored separately. The score can be computed in closed form if we assume in addition that the prior for each conditional distribution is from a conju gate family, such as the Dirichlet distribution. The details appear below.\nSince we might examine a large number of possible net work structures, we would like to avoid having to assign a prior distribution over parameters for each possible struc ture. [15] provide a set of assumptions that allow the pa rameter prior for all structures to be specified using a single network with Dirichlet priors, together with a single \"vir tual data count\" that describes the confidence in that prior.\n2 A similar formula arises from the Minimum Description Length (MDL) principle [20].\nThis approach has the desirable property that the scores of two networks that are equivalent (i.e., describe the same set of independence assumptions) are the same. Finally, to compute the full BDe score, a simple description-length penalty corresponding to Pr( G) is added in.\n3.2 BIC SCORE FOR DPNS\nWe now describe the BIC score for DPNs. Unsurprisingly, the results here mirror the results for PNs.\nThroughout this section, we assume that we are given a training set D consisting of Nseq complete observation se quences. The \u00a3th such sequence has length Nt and specifies values for the variables xi [OJ, . . . , xl [Ne]. Such a dataset give us Nseq instances of initial slices, from which we can train Bo, and N = l:e Nt instances of transitions, from which we can train B-t.\nWe start by introducing some notation. Let us define\nB\ufffdJ:,k: = Pr(Xi[O] = k\ufffdIPa(Xi[O]) = ji) and similarly B;j;,k; = Pr(Xt[t] = kiiPa(Xi[t]) = Ji) for t = 1 , ... , T. We will also need some notation for the sufficient statistics for each family,\nNLJ:,k: = l:ei(Xi[O] = k\ufffd,Pa(Xi[O]) =ji;xl) and\nNiJ;,k; = l:el:ti(Xi[t] = ki,Pa(Xi[t]) =ji;xt) where I(\u00b7; xl) is an indicator function which takes on value 1 if the event \u00b7 occurs in sequence xi, and 0 otherwise.\nUsing (1), and rearranging terms, we find that the likeli hood function decomposes according to the structure of the DPN, just as with PNs:\nN(O) Pr(D IG,ea ) = n n n (e(O) ) i,j:,k: X i j\ufffd k\ufffd i,j\ufffd ,k\ufffd\n( ) N-:-*. k n. n. n e-:+. t,J;, i (4) t Ji k; >,J; ,k;\nand hence the log-likelihood is given by\nL(B: D) = I:i l:i: l:k: NLJ:,k: log B\ufffdJ:,k: + l:i l:i; l:k; NiJ,,k, log B;j,,k; (5)\nThis decomposition facilitates the computation of the BIC and BDe scores in several ways. First, note that the like lihood is expressed as a sum of terms, where each term depends only on the conditional probability of a variable given a particular assignment to its parents. Thus, if we want to find the maximum likelihood parameters, we can maximize within each family independently. Second, the decomposition implies that we can learn Bo independently of B-+. Finally, we can learn B-+ in exactly the same manner as learning a PN for a set of samples of transitions.\nWe now make these arguments more precise. Using the standard maximum likelihood estimate for multinomial dis tributions, we immediately get the following expression for E>a.\n142 Friedman, Murphy, and Russell\nand similarly for the transition case. In the case of CPTs, the number of parameters in the network is given by\n#G =#Go+ #G ..... where\n#Go= Li LpEPa(X;[o]) I IXpll x (IIXi[O] I I )- 1) and similarly for the transition case.\nFinally, substituting into (3), we find that the BIC score is given by\nBIC(G: D)= BICo + BIC-} (6) where\nBICo = Li Lj: Lk: NPJ: ,k: logB\ufffd\ufffd; ,k: - log\ufffdseq#Go and\nBIC-} = Li Lj; Lk; NiJ;,k; logBij;,k;- logt #G-t Note that the penalty for families in the original time slice is a function of the number of sequences Nseq\u2022 since we only observe that many examples for this part of the model; whereas the penalty for the transition model is a function of N, the total number of transitions observed. 3.3 BDe SCORE FOR DPNS\nRecall that to compute the BDe score, we need to evaluate the following integral.\nPr(D I G) = IPr(D I G,ea)Pr(ea I G)dea The first term inside the integral decomposes as in Equa tion 4. If we assume that the prior over each conditional probability is independent of the rest, then the prior term also decomposes as\nPr(ea I G)= rt Tij: Pr(O\ufffd\ufffd]; ,k:) X [li [lj; Pr(Oi,j;,kJ\nInserting this into the preceding equation, we see that the entire expression consists of an integral over the product of independent terms. Hence Pr(D I G) can be written as a product of two integrals,\nN(O) TI TI I TI (n(O) ) i,f ,k'. p (n{O) ) dn{O) . ., k' u . . , k' ' ' x r v . . , k' x u . . , k' t Ji i t,Ji, i '''i' i 'I.,Ji' i\nand similarly for the transition case. In order to obtain a closed-form solution, we assume Dirichlet priors [6]. A Dirichlet prior for a multinomial distribution of a variable X is specified by a set of hyper parameters { N' x : x E Val( X)} as follows: Pr(8x) = Dirichlet({N'x: X E Val(X)}) <X II o\u00a31'.,-l,\nX Intuitively, the hyperparameters can be thought of as \"pseudo counts\", since they play a similar role to the actual counts we derive from the data. Under a Dirichlet prior, the probability of observing a sequence of values of X with counts N (x) is\nI Tix o:<x) Pr(ex I G)dex = r<L: N'.,) x\u00b7n rcN'.,+N(x)) r(L:.,<N' .,+N(x))) X rcN' .,) '\nwhere r(x) = Iooo tx-le-tdt is the Gamma function that satisfies the properties r(1) = 1 and r(x + 1) = xr(x).\nReturning to the BDe score, let us assume that for each structure G, we have chosen the hyperparameters N'Y:J\ufffd ,k'\n-} ..\nand N' i,j; ,k;. Then we can rewrite Pr( D I G) as a product of two terms,\nand similarly for the transition case. This still requires us to supply the Dirichlet hyperparame\nters for each candidate DPN structure. Since the number of possible DPN structures is large, these prior estimates might be hard to asses in practice. Following [15], we can assign all of these given a prior DPN B' = ( B(o), B\ufffd) and two equivalent sample sizes if(O) and N-}. Given these compo nents, we assign the Dirichlet weights for G = (Go, G->) as follows:\nN'\ufffd\ufffd];,k; = if(O) x PB\ufffd(Xi[O] = k\ufffd I Pa ( Xi[O]) = jD N'id;,k; N-} x PB\ufffd (Xi[1) = ki I Pa ( Xi[l]) = ji) (Note that the choice of parents here is based on G, and might differ from the structure of B'.) Intuitively, we can consider the belief in B' as equivalent to having previously experienced if(O) sequences with N-} transitions.\nIt is easy to verify that choosing priors in this manner preserves the main property we require: two DPN structures that imply identical independence assumptions will receive the same score. Thus, we claim that our definition is the natural extension of the BDe score to the case of dynamic probabilistic networks.\n3.4 LEARNING IN PRACTICE\nBoth scores we considered so far have two important prop erties. First, the score of a DPN can be written as a sum of terms, (for the BDe case we look at the logarithm of Pr( G I D)), where each term determines the score of a particular choice of parents for a particular variable. Thus, a local change to one family (e.g., arc addition or removal) affects only one of these terms. Second, the term that eval uates Xi[t] given its parents is a function of the appropriate counts (either N(0)(\u00b7) or N->(\u00b7)) for that family. Thus, by caching these counts we can efficiently evaluate many families.\nThese two properties can be exploited by hill-climbing search procedures [3, 15] that gradually improve a candidate structure by applying the best arc addition, deletion, or reversal. In the case of DPNs, as opposed to static PNs, we have the additional constraint that the network structure must repeat over time. This reduces the number of options at each point in the search. Moreover, we can search for the best structure for Bo independently of the search for the best structure for B-}.\nLearning the Structure of Dynamic Probabilistic Networks 143\n4 LEARNING FROM INCOMPLETE DATA\nWe now examine how to learn DPNs from incomplete data. Incomplete data is crucial since in most real life applications since we do not have complete observability of the process we want to model. This means that even if the process is a stationary Markov process, the partial observations we have are not Markovian. As an example, suppose we are tracking a car moving down the highway and we are ob serving the car's speed, lane, relative distance to other cars, and so on. This example is clearly not Markovian, given the observations. On the other hand, a Markovian model might be reasonable provided we include as part of the state information the driver's goals, e.g., get to the left lane, exit at the next off ramp, etc. By learning hidden variables, we can capture state information about the process. This allows the model to \"remember\" additional information about the past and to make better predictions of the future.\nThe main difficulty with learning from partial observa tions is that we no longer have the score decomposition properties of (4). This means that the optimal parameter choice in one part of the network depends on the parameter choices in other parts of the network. This problem can be understood better if we notice that once we have partial observability we can no longer talk about counts from the training data. For most events of interest, the counts are not defined, since we do not know the exact value of the variables in questions.\nThe most commonly used method to alleviate this problem is the Expectation-Maximization (EM) algorithm [7, 21]. The E -step of EM uses the currently estimated parameters to complete the data by computing the ex pected counts. TheM-step then re-estimates the maximum likelihood parameter values as if the expected counts were true observed counts. The central theorem underlying EM's behavior is that each EM cycle is guaranteed to improve the likelihood of the data given the model until it reaches a local maximum.\nEM has been traditionally viewed as a method for ad justing the parameters of a fixed model structure. How ever, the underlying theorem can be generalized to apply to structural as well as parametric modifications. Friedman's Structural EM (SEM) algorithm [10] has the same E -step as EM, completing the data by computing expected counts based on the current structure and parameters. In addition to re-estimating parameters, the M-step of SEM can use the expected counts according to the current structure to evalu ate any other candidate structure-essentially performing a complete-data structural search in the inner loop. Friedman shows that for a large family of scorings rules, including the BIC score and BDe score [11], the resulting network must have a higher score than the original. This is true even though the expected counts used in evaluating the new structure are computed using the old structure.\nWe can extend Friedman's results to the DPN case in the following way. Let E[BIC((Bb, B\ufffd) : n+) : D, (B0,R.7)] be the expected BIC score of a DPN ( Bh, B\ufffd) based on all possible completions n+ of the data, i.e., an assignment of values to the hidden variables. The ex pectation is taken with respect to Pr(D+ I D, (B(o), B-+ )),\ni.e., the probability assigned to this completion based on the old DPN. Using Theorem 3.1 of [10], we can prove the following:\nTheorem 4.1:\nBIC((Bb,B\ufffd): D) -BIC((Bo,B-+): D)> E[BIC((Bb,B\ufffd): n+): D,(Bo,B-+)] E[BIC((Bo,B-+): n+): D,(Bo,B-t)]\nThat is, if we choose a new DPN that has a higher expected score than the expected score of the old DPN, then the true score of new DPN will also be higher than the true score of the old DPN. Moreover, the difference in the expected scores is a lower bound on the improvement in terms of the actual score we are trying to optimize.\nThe crucial property of the expected BIC score is that it, too, decomposes into a sum of local terms, as follows. By the linearity of expectation, we can \"push\" the E opera tor through the summation signs implicit in (6) to get an equation which involves terms of the following form:\nE[NiJ,,k.] L\":tl:tE[I(Xi[t] = ki,Pa(Xi[t]) =ji;r)] = L\":t L\":t Pr(Xi[t] = ki, Pa(Xi[t]) = iilxi)\nand similarly for E[Ni(,\ufffdLk:J\u00b7 These are called the expected sufficient statistics. Hence the key requirement to apply the SEM algorithm in the incomplete data case is the ability to compute the probabilities of all the families Pr(Xi[t] = ki,Pa(Xi[t]) = jilr) of all the networks which we wish to evaluate, i.e., the current network and the \"neighboring\" networks in the search space.\nTo efficiently compute the probabilities of the families, we can convert the DPN to a join tree and use a two-pass dy namic programming algorithm [19], similar to the forwards backwards algorithm used in HMMs [29]. To efficiently compute the probability of a set of nodes that is not con tained in any of the join tree nodes, we need to use more sophisticated techniques [30]. A simpler approach, which is the one we currently use, is to connect together all the variables in a single timeslice into a single node (i.e., to convert the DPN to a Markov chain), compute the expected number of transitions between every pair of consecutive states in this chain, and then marginalize these counts to get the counts for each family. We also maintain a cache of the expected counts computed so far based on the current completion model, and use this cache to avoid recomputing expected counts. Whatever technique we use, computing exact expected sufficient statistics for all models of interest is likely to remain computationally challenging. In the dis cussion section we mention some approximation techniques that may allow us to learn larger models.\nIn summary, the SEM procedure is as follows. procedure DPN-SEM:\nChoose (B8, B\ufffd) (possibly randomly). Loop for n = 0, 1, . . . until convergence\nImprove the parameters of ( B(j, B\ufffd) using EM Search over DPN structures\n(using expected counts computed with (Eli, B\ufffd)) Let the best scoring DPN seen be (B;+t, B\ufffd+1) if (B;+t, B\ufffd+1) = (B[j, B\ufffd)\nreturn (B;+t, B\ufffd+1)\n144 Friedman, Murphy, and Russell\nThe above discussion was for the application of SEM for the BIC score. In [11], Friedman shows how to extend the SEM procedure to learn with the BDe score. The details are more involved, since the BDe score is not linear. Nonethe less, Friedman shows that it is a reasonable approximation to use the BDe score on the expected counts inside the SEM loop.\n5 APPLICATIONS In this section we describe two preliminary investigations that attempt to evaluate the usefulness of the DPN technol ogy we develop here for real-life applications.\n5.1 INFERRING DRIVER BEHAVIOR\nIn many tracking domains, we would like to learn a predic tive model of the behavior of the object being tracked. The more accurate the model, the more robust the tracking sys tem and the more useful its predictions will be in decision making. For objects with hidden state, it is our hope that DPNs can be learned that correctly reflect the unobserved process governing the behavior. The learned model may also provide insight into how the behavior is generated.\nIn this section, we describe some experiments we car ried out using a simulated driving domain [9]. The data is an idealization of what cameras mounted on the side of the road can collect. In particular, at each time step of the simulation, we get a report on cars that are within the\ncamera's range. The report for each car has the following attributes: position and velocity (relative to the camera's reference frame), relative speed and distance to the car in front, and whether there is a car immediately to the left or right. From this data, we want to learn models of typical classes of driving behavior. Such models can be useful for several tasks. A prime example arises in the BATmobile autonomous car project [8]. The BATmobile's autonomous controller attempts to predict the behavior of neighboring car. For example, it would be useful to know that someone who has just driven across two lanes might be attempting to leave the freeway, and consequently is likely to cut in front of you. Since tracking information from real cam eras is readily available [24], it is reasonable to hope that realistic models of human drivers can be obtained. In ad dition to their use in autonomous vehicles, such models are of paramount importance in so-called \"microscopic\" traffic models used in freeway design and construction planning and also in safety studies.\nIn our experiments, we generate a variety of simulated traffic patterns consisting of populations of vehicles with a mixture of different driving tendencies (e.g., trucks, sports cars, Sunday drivers, etc.). We generated 3500 cars and tracked their behavior over sequences of roughly 40-70 time steps. The observed data were discretized using fixed-sized bins. We then trained networks using a dataset consisting of the first 250, 500, 1000, and 1500 sequences, and tested these networks on the last 2000 sequences.\nLearning the Structure of Dynamic Probabilistic Networks 145\n(a) (b)\nFigure 4: (a) A simple pathway model with five vertices. Each vertex represents a site in the genome, and each arc is a possible triggering pathway. (b) A DPN model that is equivalent to the pathway model shown in (a).\nWe learned networks with 0, 1, and 2 hidden variables using decision tree CPTs [12] with the BIC and BDe score. The initial network structure in each case had each hidden variable (if any) dependent on its value in the previous time slice, and each observable variable dependent only on the hidden variables in the same time slice. (Thus, we did not put in any persistence arcs in the initial model.) This struc ture connects each hidden variable to all the other variables, allowing that variable to carry forward information about previous time slices.\nDPNs are a powerful representation language for describ ing causal models of stochastic processes. Such models are useful in many areas of science, including molecular biol ogy, where one is often interested in inferring the structure of regulatory pathways. McAdams and Shapiro [26] model part of the genetic circuit of the lambda bacteriophage in terms of a sequential logic circuit; and attempts have even be made to automatically infer the form of such Boolean circuits from data [22]. However, it is well known that the abstraction of binary-valued signals together with deter ministic switches often breaks down, and one must model\n3For comparison, the g zip utility compresses the observations to approximately 5.6 bits per time slice.\nthe continuous nature and inherent noise in the underlying system to get accurate predictions [25]. Also, one must be able to deal with noisy and incomplete observations of the system.\nWe believe that DPNs provide a good tool for model ing such noisy, causal systems, and furthermore that SEM provides a good way to learn these models automatically from noisy, incomplete data. Here we describe some initial experiments using DPNs to learn small artificial examples typical of the causal processes involved in genetic regula tion. We generate data from models of known structure, learn DPN models from the data in a variety of settings, and compare these with the original models. The main purpose of these experiments is to understand how well DPNs can represent such processes, how many observations are re quired, and what sorts of observations are most useful. We refrain from describing any particular biological process, since we do not yet have sufficient real data on the processes we are studying to learn a scientifically useful model.\nSimple genetic systems are commonly described by a pathway model-a graph in which vertices represent genes (or larger chromosomal regions) and arcs represent causal pathways (Figure 4(a)). A vertex can either be \"off/normal\" (state 0) or \"on/abnormal\" (state 1). The system starts in a state which is all Os, and vertices can \"spontaneously\" turn on (due to unmodelled external causes) with some probabil ity per unit time. Once a vertex is turned on, it stays on, but may trigger other neighboring vertices to turn on as well again, with a certain probability per unit time. The arcs on the graph are usually annotated with the \"half-life\" param eter of the triggering process. Note that pathway models, unlike PNs, can contain directed cycles. For many impor tant biological processes, the structure and parameters of this graph are completely unknown; their discovery would constitute a major advance in scientific understanding.\nPathway models have a very natural representation as DPNs: each vertex becomes a state variable, and the trig gering arcs are represented as links in the transition network of the DPN. The tendency of a vertex to stay \"on\" once triggered is represented by persistence links in the DPN. Figure 4(b) shows a DPN representation of the five-vertex pathway model in Figure 4(a). The nature of the problem suggests that noisy-ORs (or noisy-ANDs) should provide a parsimonious representation of the conditional density function at each node. To specify a noisy-OR for a node with k parents, we use parameters q1, ... , qk, where qi is the probability the child node will be in state 0 if the ith par ent is in state 1. (Thus we need only k parameters instead of 2k for a full CPT.) In the five-vertex DPN model that we used in the experiments reported below, all the q parame ters (except for the persistence arcs) have value 0.2. For a strict persistence model (vertices stay on once triggered), q parameters for persistence arcs are fixed at 0. To learn such noisy-OR distributions, we follow the technique suggested in [27]; this entails introducing a new hidden node for each arc in the network, which is a noisy version of its parent, and replacing each noisy-OR gate with a deterministic-OR gate. We also tried using gradient descent, following [1], but en countered difficulties with convergence in cases where the optimal parameter values were close to the boundaries (0\nor 1). In all our experiments, we enforced the presence of the persistence arcs in the network structure. We used two alternative initial topologies: one that has only persistence arcs (so the system must learn to add arcs) and one that is fully interconnected (so the system must learn to delete arcs). Performance in the two cases was very similar.\nWe experimented with three observation regimes that cor respond to realistic settings:\n\u2022 The complete state of the system is observed at every time step.\n\u2022 Entire time slices are hidden uniformly at random with probability h, corresponding to intermittent observa tion.\n\u2022 Only two observations are made, one before the pro cess begins and another at some unknown time tobs after the process is initiated by some external or spon taneous event. This might be the case with some dis ease processes, where the DNA of a diseased cell can be observed but the elapsed time since the disease pro cess began is not known. (The \"initial\" observation is of the DNA of some other, healthy cell from the same individual.)\nThis last case, which obtains in many realistic situations, raises a new challenge for machine learning. We resolve it as follows: we supply the network with the \"diseased\" observation at time slice T, where T is with high prob ability larger than the actual elapsed time tobs since the process began.4 We also augment the DPN model with a hidden \"switch'' variable S that is initially off, but can come on spontaneously. When the switch is off, the system evolves according to its normal transition model\n4With the q parameters set to 0.2 in the true network, the actual system state is all-ls with high probability after about T = 20, so this is the length used in training and testing.\nP(X[t + 1]1 X[t], S =0), which is to be determined from the data. Once the switch turns on, however, the state of the system is frozen-that is, the conditional probabil ity distribution P(X[t + 1] I X[t], S = 1) is fixed so that X[t + 1] = X[t] with probability 1. The persistence param eter for S determines a probability distribution over tobs; by fixing this parameter such that (a priori) tobs < T with high probability, we effectively fix a scale for time, which would otherwise be arbitrary. The learned network will, nonetheless, imply a more constrained distribution for tobs given a pair of observations.\nWe consider two measures of performance. One is the number of different edges in the learned network compared to the generating network, i.e., the Hamming distance be tween their adjacency matrices. The second is the difference in the logloss of the learned model compared to the gen erating model, measured on an independent test set. Our results for the five-vertex model of Figure 4(a) are shown in Figure 5. We see that noisy-ORs perform much better than tabular CPTs when the amount of missing data is high. Even with 40% missing slices, the exact structure is learned from only 30 examples by the noisy-OR network. However, when all-but-two slices are hidden, the system cannot learn effectively with a reasonable amount of data (results not shown). The case in which we do not even know the time at which the second observation is made (which we modeled with the switching variable) is even harder to learn (results not shown). Obviously, the application of prior knowledge will be very important in reducing the data requirements.\n6 DISCUSSION\nIn this paper we addressed the question of learning the structure and parameters of DPNs from complete and in complete data. To the best of our knowledge, we are the first to examine this problem. As our experiments show,\nLearning the Structure of Dynamic Probabilistic Networks 147\nwe can learn non-trivial structures from synthetic data and \"realistic\" data from a nontrivial simulator.\nThe main bottleneck in the application of our procedures is inference, which is necessary to compute the expected sufficient statistics. Unlike the case of PNs, a sparse DPN structure does not necessarily ensure fast inference-the minimum size of the posterior distribution for a slice is generally exponential in the number of variables that have parents in the previous slice. There are various approxi mations that we could use to speed up inference: (1) The method proposed by Boyen and Koller [2], which approxi mates posterior probabilities in the DPN in a factored form; this should be particularly appropriate for the biological models we are investigating. (2) Stochastic simulation-for example, the ER/SOF algorithm ofKanazawa et al. [ 18]. (3) Variational approximations, e.g., Jaakkola and Jorden [17] and Ghahramani and Jordan [14]. (4) Methods based on multilevel abstraction hierarchy to detect which variables are related to each other.\nWe are currently extending SEM to learn the structure of linear Gaussian DPNs; we hope this will prove com petitive with traditional techniques of system identification [23]. One advantage of the Gaussian case over the discrete case is that marginalizing the posterior over two slices is an efficient operation. Ultimately we wish to tackle the case of hybrid DPNs, with both discrete and continuous variables. The advantage of hybrid DPNs over switching state space models [16] is that the state variables can be represented in factored form. For example, in the driving domain, we can have separate variables for the continuous observations (such as speed and position) and for the dis crete hidden states (such as \"want to change lane\" or \"want to overtake\"). The question of how to know when to add hidden variables is a very interesting one which we are also currently investigating.\nAcknowledgments\nWe thank Jeff Forbes and Nikunj Oza for their help in getting the training data for the driving domain. This work was supported in part by ARO under the MURI program \"Integrated Approach to Intelligent Systems\", grant number DAAH04-96-1-0341.\nReferences\n[1] J. Binder, D. Koller, S. Russell, and K. Kanazawa. Adaptive probabilistic networks with hidden variables. Mach. Learn ing, 29:213-244, 1997. [2] X. Boyen and D. Koller. Tractable Inference for Complex Stochastic Processes. In UAI, 1998. [3] W. Buntine. Theory refinement on Bayesian networks. In UA/-91, pp. 52-60. 1991. [4] G. Cooper and E. Herskovits. A Bayesian method for the induction of probabilistic networks from data. Mach. Learn ing, 9:309-347, 1992. [5] T. Dean and K. Kanazawa. Probabilistic temporal reasoning. InAAAI-88, pp. 524-528, 1988.\n[6] M. DeGroot. Optimal Statistical Decisions. 1970. [7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi\nmum likelihood from incomplete data via the EM algorithm. J. Royal Stat. Soc., B 39:1-39, 1977.\n[8] J. Forbes, T. Huang, K. Kanazawa, and S. Russell. The BATmobile: Towards a Bayesian automated taxi. In /JCAI95, 1995. [9] J. Forbes, N. Oza, R. Parr, and S. Russell. Feasibility study of fully automated traffic using decision-theoretic control. Cal. PATH Research Report UCB-I TS-PRR-97-18, lnst. of Transportation Studies, U. C. Berkeley, 1997.\n[ 10] N. Friedman. Learning belief networks in the presence of missing values and hidden variables. In JCML-97, 1997.\n[ 1 1] N. Friedman. The Bayesian Structural EM Algorithm. In UA/-98, 1998. [ 12] N. Friedman and M. Goldszmidt. Learning Bayesian net works with local structure. In M. I. Jordan, ed., Learning in Graphical Models, 1998. A preliminary version appeared in UAI '96 .. [ 13] D. Geiger and D. Heckerman. Learning Gaussian Networks. In UAI, 1994.\n[ 14] Z. Ghahramani and M. I. Jordan. Factorial hidden Markov models. Mach. Learning, 29:245-274, 1997.\n[ 15] D. Heckerman, D. Geiger, and D. M. Chickering. Learn ing Bayesian networks: The combination of knowledge and statistical data. Mach. Learning, 20: 197-243, 1995.\n[ 16] Z. Ghahramani and G. Hinton. Switching State-Space Mod els. Submitted for publication, 1998. [ 17] T.S. Jaakkola and M.l. Jordan. Recursive algorithms for approximating probabilities in graphical models. In NIPS 9, pp. 487-93, 1997.\n[ 18] K. Kanazawa, D. Koller, and S. Russell. Stochastic sim ulation algorithms for dynamic probabilistic networks. In UA/-95, pp. 346-351, 1995. [ 19] U. Kjaerulff. A computational scheme for reasoning in dy namic probabilistic networks. In UAI-92, 1992. [20] W. Lam and F. Bacchus. Learning Bayesian belief networks: An approach based on the MDL principle. Comp. Intel., 10:269-293, 1994. [21] S. L. Lauritzen. The EM algorithm for graphical association models with missing data. Comp. Stat. and Data Anal., 19:191-201, 1995.\n[22] S. Liang, S. Fuhrman, and R. Somogyi. Reveal, a general reverse engineering algorithm for inference of genetic net work architectures. In Pacific Symp. on Biocomputing, vol. 3, pp. 18-29, 1998.\n[23] L. Ljung. System Identification: Theory for the User. Pren tice Hall, 1987. [24] J. Malik and S. Russell. Traffic surveillance and detec tion technology development: New sensor technology final report. Research Report UCB-I TS-PRR-97-6, Cal. PATH Program, 1997.\n[25] H. H. McAdams and A. Arkin. Stochastic mechanisms in gene expression. Proc. of the Nat. Acad. of Sci., 94:814-819, 1997.\n[26] H. H. McAdams and L. Shapiro. Circuit simulation of ge netic networks. Science, 269:650-656, 1995.\n[27] C. Meek and D. Heckerman. Structure and parameter learn ing for causal independence and causal interaction models. In UA/-97, pp. 366-375, 1997. [28] G. Schwarz. Estimating the dimension of a model. Ann. Stat., 6:46 1-464, 1978.\n[29] P. Smyth, D. Heckerman, and M. Jordan. Probabilistic inde pendence networks for hidden Markov probability models. Neural Computation, 9(2):227-269, 1997. [30] H. Xu. Computing marginals for arbitrary subsets from marginal representations in Markov trees. Art. Intel., 74:177- 189, 1995.\n[31] G. Zweig and S. Russell. Speech recognition with dynamic Bayesian networks. In AAAI-98, 1998."}], "references": [{"title": "Adaptive probabilistic networks with hidden variables", "author": ["J. Binder", "D. Koller", "S. Russell", "K. Kanazawa"], "venue": "Mach. Learn\u00ad ing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Tractable Inference for Complex Stochastic Processes", "author": ["X. Boyen", "D. Koller"], "venue": "In UAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Theory refinement on Bayesian networks", "author": ["W. Buntine"], "venue": "In UA/-91,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G. Cooper", "E. Herskovits"], "venue": "Mach. Learn\u00ad ing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Probabilistic temporal reasoning", "author": ["T. Dean", "K. Kanazawa"], "venue": "InAAAI-88, pp. 524-528,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Optimal Statistical Decisions", "author": ["M. DeGroot"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1970}, {"title": "Maxi\u00ad mum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J. Royal Stat. Soc., B", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "The BATmobile: Towards a Bayesian automated taxi", "author": ["J. Forbes", "T. Huang", "K. Kanazawa", "S. Russell"], "venue": "In /JCAI-", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}], "referenceMentions": [{"referenceID": 4, "context": "Somewhat less well\u00ad established, but perhaps of equal importance, are dynamic probabilistic networks (DPNs), which model the stochastic evolution of a set of random variables over time [5].", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "Algorithms for learning the parameters of PNs [1, 21] and DPNs [1, 14] are becoming widely used.", "startOffset": 46, "endOffset": 53}, {"referenceID": 0, "context": "Algorithms for learning the parameters of PNs [1, 21] and DPNs [1, 14] are becoming widely used.", "startOffset": 63, "endOffset": 70}, {"referenceID": 3, "context": ", where the value\ufffd of all variables are specified in each training case [ 4, 15].", "startOffset": 72, "endOffset": 80}, {"referenceID": 1, "context": "To represent beliefs about the possible trajectories of the process, we need a probability distribution over the random variables X[O] U X[l] U X[2] U.", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "\u2022 a prior network B0 that specifies a distribution over initial states X[O]; and \u2022 a transition network B--+ over the variables X[O] U X[1] that is taken to specify the transition probability P(X[t + 1] 1 X[t]) for all t.", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "2 An alternative approach is to evaluate (2) in closed form given a restricted family of priors [3, 4, 15].", "startOffset": 96, "endOffset": 106}, {"referenceID": 3, "context": "2 An alternative approach is to evaluate (2) in closed form given a restricted family of priors [3, 4, 15].", "startOffset": 96, "endOffset": 106}, {"referenceID": 5, "context": "In order to obtain a closed-form solution, we assume Dirichlet priors [6].", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "These two properties can be exploited by hill-climbing search procedures [3, 15] that gradually improve a candidate structure by applying the best arc addition, deletion, or reversal.", "startOffset": 73, "endOffset": 80}, {"referenceID": 6, "context": "The most commonly used method to alleviate this problem is the Expectation-Maximization (EM) algorithm [7, 21].", "startOffset": 103, "endOffset": 110}, {"referenceID": 7, "context": "A prime example arises in the BATmobile autonomous car project [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "We also tried using gradient descent, following [1], but en\u00ad countered difficulties with convergence in cases where the optimal parameter values were close to the boundaries (0", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "There are various approxi\u00ad mations that we could use to speed up inference: (1) The method proposed by Boyen and Koller [2], which approxi\u00ad mates posterior probabilities in the DPN in a factored form; this should be particularly appropriate for the biological models we are investigating.", "startOffset": 120, "endOffset": 123}], "year": 2011, "abstractText": "Dynamic probabilistic networks are a compact repre\u00ad sentation of complex stochastic processes. In this pa\u00ad per we examine how to learn the structure of a DPN from data. We extend structure scoring rules for stan\u00ad dard probabilistic networks to the dynamic case, and show how to search for structure when some of the vari\u00ad ables are hidden. Finally, we examine two applications where such a technology might be useful: predicting and classifying dynamic behaviors, and learning causal orderings in biological processes. We provide empir\u00ad ical results that demonstrate the applicability of our methods in both domains.", "creator": "pdftk 1.41 - www.pdftk.com"}}}