{"id": "1302.1550", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2013", "title": "Relational Bayesian Networks", "abstract": "a new method commonly is developed to represent robust probabilistic relations on multiple random events. similarly where previously knowledge bases containing standard probabilistic rules were used for this similar purpose, here a probability distribution over the relations is directly represented by a bayesian network. by using a powerful way of specifying conditional probability distributions in these networks, the resulting formalism is more expressive than the respective previous ones. particularly, it technically provides freedom for constraints on complex equalities of events, functions and it allows to define complex, nested combination functions.", "histories": [["v1", "Wed, 6 Feb 2013 15:57:05 GMT  (744kb)", "http://arxiv.org/abs/1302.1550v1", "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)"]], "COMMENTS": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (UAI1997)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["manfred jaeger"], "accepted": false, "id": "1302.1550"}, "pdf": {"name": "1302.1550.pdf", "metadata": {"source": "CRF", "title": "Relational Bayesian Networks", "authors": ["Manfred Jaeger"], "emails": ["jaeger@robotics.stanford.edu"], "sections": [{"heading": null, "text": "A new method is developed to represent prob abilistic relations on multiple random events. Where previously knowledge bases containing probabilistic rules were used for this purpose, here a probability distribution over the relations is directly represented by a Bayesian network. By using a powerful way of specifying conditional probability distributions in these networks, the resulting formalism is more expressive than the previous ones. Particularly, it provides for con straints on equalities of events, and it allows to define complex, nested combination functions.\n1 INTRODUCTION\nIn a standard Bayesian network, nodes are labeled with ran dom variables (r.v.s) X that take values in some finite set { x1, . . . , Xn}. A network with r.v.s ( earth)quake, burglary, and alarm, each with possible values {true ,false}, for in stance, then defines a joint probability distribution for these r.v.s.\nEvidence, E, is a set of instantiations of some of the r. v.s. A query asks for the probability of a specific value x of some r. v. X, given the instantiations in the evidence. The answer to this query is the conditional probability P( X = x I E) in the distribution P defined by the network.\nThe implicit underlying assumption we here make is that the value assignments in the evidence and the query in stantiate the attributes of one single random event, or object, that has been sampled (observed) according to the distribution of the network. If, for instance, E = {quake = true, alarm = true}, then both instantiations are assumed to refer to one single observed state of the world w, and not the facts that there was an earthquake in 1906, and the alarm bell is ringing right now.\n\u2022 On leave from: Max-Pianck-Institut fur Informatik, Im Stadtwald, D-66123 Saarbriicken, Gennany\nIn case we indeed have evidence about several ob served events, e.g. quake(w1) = true, alarm(w1) = true, burglary(w2) = false, then, for the purpose of answer ing a query X ( w) = x about one of these events, all evidence about other events can be ignored, and only P(X(w) = x I E(w)) needs to be computed. For each of these computations the same Bayesian network can be used.\nThings become much different when we also want to model relations that may hold between two different random events. Suppose, for instance, we also want to say some thing about the probability that one earthquake was stronger than another. For this we use the binary relation stronger, and would like to relate the probability of stronger(w1, w2) to, say, alarm(w1) and alarm(w2). Evidence may now contain instantiations of stronger for many different pairs of states: {stronger(w1, w2), ... , stronger(w1, Wn)}, and a query may be alarm(wt). In evaluating this query, we no longer can ignore information about the other events w2, . . . , Wn. This means, however, that if we do not want to impose an a priori restriction on the number of events we can have evidence for, no single fixed Bayesian network with finite-range r.v.s will be sufficient to evaluate queries for arbitrary evidence sets.\nNevertheless, the probabilistic information that we would like to encode about relations between an arbitrary number of different events may very well be expressible by some fi nite set of laws, applicable to an arbitrary number of events. One way of expressing such laws, which has been explored in the past ((Breese 1992),(Poole I993),(Haddawy 1994)), is to use probabilistic rules such as\nstronger ( u, v) \ufffd quake( u) i\\ quake( v) 1\\alarm(u) 1\\ -,a[arm(v). (1)\nThe intended meaning here is: for all states of the world w1 and w2, given that quake(w1) 1\\ ... i\\ -,alamz(w2) is true, the probability that w1 is stronger than w2 is 0.8. A rule-base containing expressions of this form then can be used to construct, for each specific evidence and query,\na Bayesian network over binary r.v.s stronger(w1, w2), stronger(w1, w3),quake(w3), . .. , in which the answer to the query subsequently is computed using standard Bayesian\nnetwork inference.\nIn all the above mentioned approaches, quite strong syn tactic and/or semantic restrictions are imposed in the for\nmalism that severely limit its expressiveness. Poole (1993) does not allow the general expressiveness of rules like ( 1 ), but only combines deterministic rules with the specification\nof certain unconditional probabilities. Haddawy( 1994) al\nlows only rules in which the antecedent does not contain free variables that do not appear in the consequent. As\npointed out by Glesner and Koller ( 1995), this is a severe\nlimitation. For instance, we can then not express by a rule like aids( x) .!!-contact( x, y) that the probability of person x having aids depends on any other person y, with whom x had sexual contact. When we do permit an additional free variable y in this manner, it also has to be defined how the probability of the consequent is affected when there exist multiple instantiations of y that make the antecedent true (this question also arises when several rules with the same consequent are permitted in the rule base ). In (Glesner & Koller 1995)and (Ngo, Haddawy & Helwig 1995) therefore a combination rule is added to the rule-base, which defines how the conditional probabilities arising from different in\nstantiations, or rules, are to be combined. If the different\ncausal relationships described by the rules are understood\nto be independent, then the combination rule typically will\nbe noisy-or.\nThe specification of a single combination rule applied to\nall sets of instantiations of applicable rules, again, does\nnot permit us to describe certain important distinctions. If, for instance, we have a rule that relates aids( x) to the re lation contact(x, y), and another rule that relates aids(x) to the relation donor(x, y), standing for the fact that x has received a blood transfusion from donor y, then the probability computed for aids( a) , using a simple combina tion rule, will depend only on the number of instantiations for contact( a, y) and for donor( a, y). Particularly, we are not able to make special provisions for the two rules to be instantiated by the same element b, even though the case contact( a, b) 1\\ donor( a, b) clearly has to be distin guished from the case contact( a, b) 1\\ donor( a, c) , or even contact( a, b) 1\\ donor( a, a) .\nIn this paper a representation formalism is developed that incorporates constraints on the equality of instantiating el\nements, and thereby allows us to define different probabil\nities in situations only distinguished by equalities between instantiating elements.\nFurthermore, our representation method will allow us to specify hierarchical, or nested, combination rules. As an illustrations of what this means, consider the unary predicate cancer( x), representing that person x will de-\nRelational Bayesian Networks 267\nvelop cancer at some time, and the three placed rela tion exposed(x,y,z), representing that organ y of per son x was exposed to radiation at time z (by the taking of an x-ray, intake of radioactively contaminated food, etc). Suppose, now, that for person x we have evidence E = {exposed(x,yi,Zj) I i = l, . . . ,k;j = 1, .. . ,1}, where Yi = Yi' for some i, i', and Zj = zr for some j, j'. Assume that for any specific organ y, multiple ex posures of y to radiation have a cumulative effect on the risk of developing cancer of y, so that noisy-or is not the adequate rule to model the combined effect of instances exposed( x, y, Zj) on the probability of developing cancer of y. On the other hand, developing cancer at any of the various organs y can be viewed as independent causes for developing cancer at alL Thus, a single rule of the form cancer( x) .!!-exposed( x, y, z) together with a \"fiat\" combi nation rule is not sufficient to model the true probabilistic\nrelationships. Instead, we need to use one rule to first com bine for every fixed y the instances given by different z, and then use another rule (here noisy-or) to combine the effect of the different y's.\nTo permit constraints on the equality of instantiating ele\nments, and to allow for hierarchical definitions of combina\ntion functions, in this paper we depart from the method of representing our information in a knowledge base contain\ning different types of rules. Instead, we here use Bayesian\nnetworks with a node for every relation symbol r of some vocabulary S, which is seen as a r.v. whose values are pos sible interpretations of r in some specific domain D. The state space of these relational Bayesian networks therefore can be identified with the set of all S-structures over D, and its semantics is a probability distribution over S-structures,\nas were used by Halpern( 1990) to interpret first-order proba bilistic logic. Halpern and Koller( 1996) have used Markov\nnetworks labeled with relation symbols for representing conditional independencies in probability distributions over S-structures. This can be seen as a qualitative analog to the\nquantitative relational Bayesian networks described here.\n2 THE BASIC FRAMEWORK\nIn medical example domains it is often natural to make the domain closure assumption, i.e. to assume that the domain\nunder consideration consists just of those objects mentioned\nin the knowledge base. The following example highlights\na different kind of situation, where a definite domain of objects is given over which the free variables are to range,\nyet there is no evidence about most of these objects.\nExample 2.1 Robot TBayesO.l moves in an environment consisting of n distinct locations. TBayesO.l can make di rect moves from any location x to any location y unless the (directed) path x __,. y is blocked. This happens to be the case with probability p0 for all x f. y. At each time step TBayesO.l, as well as a certain number of other robots op-\n268 Jaeger\nerating in this domain, make one move along an unblocked path x ____, y, x # y. TBayesO.l just has completed the task it was assigned to do, and is now in search of new instruc\ntions. It can receive these instructions, either by reaching a tenninal-location from where a central task assigning com puter can be accessed, or by meeting another robot that will assign TBayesO. l a subtask of its own job. Unfortunately, TBayesO. l only has the vaguest idea of where the terminal locations are, or where the other robots are headed. The best model of its environment that it can come up with, is that every location x is a terminal location with proba bility p1, and that any unblocked path x ___, y is likely to be taken by at least one robot at any given time step with probability p2. In order to plan its next move, TBayesO.l tries to evaluate for every location x the probability that going to x leads to success, defined as either getting in structions at x directly, or being able to access a terminal location in one more move from x. Hence, the probability of s(uccess)(x) is I if t(enninal)(x) is true, or ift (z ) and --,b(locked)(x, z) holds for some z. Otherwise, there still is a chance of s( x) being true, determined by the number of incoming paths z --+ x, each of which is likely to be taken by another robot with probability p2\u2022 Assuming a fairly large number of other robots, the event that z - x is taken by some robot can be viewed as independent from z' ____, x being taken by a robot, so that the overall probability that another robot will reach location xis given by 1- (1-p2 ) k, wherek =I {z I z \"# x,--,b(z,x)} 1. i.e. by combining the individual probabilities via noisy-or.\nThe foregoing example gives an informal description of how the probability of s( x) is evaluated, given the predicates b and t. Also, the probabilities forb and t are given. Piecing all this information together (and assuming independence\nwhenever no dependence has been mentioned explicitly), we obtain for every finite domain D of locations a proba bility distribution P for the { b, t, s }-structures over D. Our aim now is to represent this class of probability distri\nbutions in compact form as a Bayesian network with nodes b, t, and s. Given the description of the dependencies in the example, it is clear that this network should have two edges: one leading from b to s, and one leading from t to s.\nThe more interesting problem is how to specify the condi\ntional probability of the possible values of each node (i.e.\nthe possible interpretations of the symbol at that node), given the values of its parent nodes. For the two parentless nodes in our example this is accomplished very easily: for a given domain D, and for all locations x, y E D we have\nP(b(x, y))\nP(t(x)) PI\u00b7\nif X -:j; y if X = y (2)\n(3)\nHere P( b( x, y)) stands for the probability that ( x, y) be longs to the interpretation of b. Similarly for P( t(x) ). Since\nb(x, y) and b(x', y') for (x, y) # (x', y'), respectively t(x) and t(x') for x \"# x', were assumed to be mutually indepen dent, this defines a probability distribution over the possible interpretations in D of the two predicates. For example, the probability that I c:;; D X D is the interpretation of b is 0 if (x, x) E I for some x E D, and Pb11(1- po)n(n-l)-III else.\nNext, we have to define the probability of interpretations of s. Given interpretations of b and t, the events s( x) and s( x') are independent for x \"# x'. Also, example 2. 1 contains a bight level description of how the probability of s( x) is to be computed. Our aim now is to formalize this computation rule in such a manner, that P(s(x)) can be computed by evaluating a single functional expression , in the same manner as P(b(x, y)) and P(t(x)) are given by (2) and (3).\nSince P(s(x)) depends on the interpretations of band t, we begin with functional expressions that access these in\nterpretations. This is done by using indicator functions 1I(b)(x, y) and l J(t)(x). 1/(b)(x, y), for example, evalu ates to 1 if (x,y) is in the given interpretation I(b) of b, and to 0 otherwise. Though the function 1/(b)(x, y) has to be distinguished from the logical expression b(x, y), for the benefit of greater readability, in the sequel the simpler notation will be used for both. Thus, b( x, y) stands for the function 11( b) ( x, y) whenever it appears within a functional expression .\nIn order to find a suitable functional expression F, ( x) for P(s(x)), assume first that t(x) is true. Since t(x) implies s( x ), in this case we need to obtain F, ( x) = 1. In the case -.t(x), the probability of s(x) is computed by considering all locations z # x for which either --,b( x, z) or --,b( z, x). Any such z that satisfies -.b( x, z) /\\ t( z) again makes s( x) true with probability 1. If only --,b( z, x) holds, then the location z merely \"contributes\" a probability p2 toP( s( x)). Thus, for any z, the contribution of z to P(s(x)) is given by max{t(z)( l- b(x, z)),p2(1- b(z, x))} . Combining all the relevant z via noisy-or, we obtain the formula\nF,(x) = n-o{max{t(z)(l- b(x, z)),p2(1- b(z, x))} lz;zfx} (4)\nfor x with --,t(x) . Abbreviating the functional expression on the right-hand side of ( 4) by H ( x ) , we can finally put the two cases t ( x) and --,t( x) together, defining\nF,(x) = t(x) + (1- t(x))H(x). (5)\nWe now give a general definition of a representation lan guage for forming functional expressions in the style of (5). We begin by describing the general class of combination functions, instances of which are the functions n-o and max used above.\nDefinition 2.2 A combination function is any function that maps every finite multiset (i.e. a set possibly containing multiple copies of the same element) with elements from [0,1] into [0,1].\nExcept n-o and max, examples of combination functions are min, the arithmetic mean of the arguments, etc. Each combination function must include a sensible definition for its result on the empty set. For example, we here use the conventions n-o 0 = max 0 = 0, min 0 = L\nIn the following, we use bold type to denote tuples of vari abies: x = (x1, ... , xn) for some n. The number of ele ments in tuple x is denoted by I x 1- An equality constraint c( x) for x is a quantifier free formula over the empty vocab ulary, i.e., a formula only containing atomic subformulas of the form x; = Xj.\nDefinition 2.3 The class of probability formulas over the relational vocabulary 5 is inductively defined as follows.\n(i) (Constants) Each rational number q E [01 1] is a proba bility formula.\n(ii) (Indicator functions) For every n-ary symbol r E S, and every n-tuple x of variables, r( x) is a probability formula.\n(iii) (Convex combinations) When F1, F21 F3 are probabil ity formulas, then so is F1F2 + (1- Ft)Fs.\n(iv) (Combination functions) When F11 \u2022 \u2022 \u2022 1 Fk are prob ability formulas, comb is any combination function, x,z are tuples of variables, and c(x, z) is an equal ity constraint, then comb{F1, .. . , Fk I z; c(x, z)} is a probability formula.\nNote that special cases of (iii) are multiplication {F3 = 0) and \"inversion\" (F2 = 0, F3 = 1). The set of free variables of a probability formula is defined in the canonical way. The free variables of comb{ . .. } are the union of the free variables of the F;, minus the variables in z.\nA probability formula F over S in the free variables x = (xt, ... , Xn) defines for every S-structure \u00a3iJ over a domain D a mapping Dn \ufffd----+ [0, 1]. The value F(d) ford E Dn is defined inductively over the structure of F. We here give the details only for case (iv).\nLet F(x) be of the form comb{Ft(x, z), . .. 1 Fk(x, z) I z; c(x, z)} (where not necessarily all the variables in x and z actually appear in all the F; and in c). In order to define F( d), we must specify the multiset represented by\n{Ft(d,z), ... ,Fk(d,z) I z;c(d,z)}. (6)\nLet E \ufffd Dlzl be the set {d' I c(d, d')}. For each d' E E and each i E { 1, . . . , k}, by induction hypothe sis, F; ( d, d') E [0, 1]. The multiset represented by (6) now\nRelational Bayesian Networks 269\nis defined as containing as many copies of p E [0, 1] as there are representations p = F;(d, d') with different i or d'- Note that F;(d1 d') and F;(d, d\") count as different representations even in the case that the variables for which d' and d\" substitute different elements do not actually ap pear in F;. The multi set { r( d) I z; z = z}, for instance, contains as many copies of the indicator r( d), as there are elements in the domain over which it is evaluated.\nFor any tautological constraint like z = z, in the sequel we simply write T.\nAnother borderline case that needs clarification is the case where z is empty. Here our definition degenerates to: if c(d) holds, then the multiset {F1(d), ... , Fk(d) 10; c(d)} contains as many copies of p E [0, 1] as there are represen tations p = F; (d); it is empty if c( d) does not hold.\nBy using indicator functions r(x), the value of F(d) is being defined in terms of the validity in \u00a3iJ of atomic formu las r( d'). A natural generalization of probability formulas might therefore be considered, in which not only the truth values of atomic formulas are used, but indicator functions for arbitrary first-order formulas are allowed. As the fol lowing lemma shows, this provides no real generalization.\nLemma 2.4 Let\u00a2;( x) be a first-order formula over the rela tional vocabulary S. Then there exists a probability formula F q, ( x) over S, using max as the only combination function, s.t. for every finite S-structure \u00a3iJ, and every d E D\ufffd;q: Fq,(d) = 1 iff \u00a2(d) holds in \u00a3iJ, and Fq,(d) = 0 else.\nProof: By induction on the structure of\u00a2. If\u00a2 = r(x) for some r E S, then Fq,(x) = r(x). For\u00a2= x1 = x2, let Fq,(x1, x2) = max{ I 10; x1 = x2}. Conjunction and negation are handled by multiplication and in version, respectively, of probability formulas. For \u00a2 = 3y1j; ( x, y) the corresponding probability formula is Fq,(x) = max{F\u00a2(x,y) I y;r}. D\nDefinition 2.5 A relational Bayesian network for the (re lational) vocabulary S is given by a directed acyclic graph containing one node for every r E S. The node for an n-ary r E S is labeled with a probability formula F,.(x1, .. . , Xn) over the symbols in the parent nodes of r, denoted by Pa(r ).\nThe definition for the probability of b( x, y) in (2) does not seem to quite match definition 2.5, because it contains a distinction by cases not accounted for in definition 2.5. However, this distinction by cases can be incorporated into a single probability formula. If, for instance, c1 ( x) and c 2(x) are two mutually exclusive and exhaustive equality constraints, then\nF(x) := max{max{Ft(x) l0;cl(x)},\nmax{F2(x) 10;c2(x)} l0;r} (7)\n270 Jaeger\nevaluates to F1(x) for x with ct(x), and to F2(x) for x with c2(x).\nLet N now be a relational Bayesian network overS. Let r be (the label of) a node inN with arity n, and let \ufffdbe a Pa(r ) structure over domain D. For every d E D\", Fr (d) E [0, 1] then is defined. Thus, for every interpretation I( r) of r in D\" we can define\nP(I(r)) := II Fr(d) II (1- Fr(d)), dEJ(r) dii(r)\nwhich gives a probability distribution over interpretations of r, given the interpretations of Pa( r ). Given a fixed do main D, a relational Bayesian network thus defines a joint probability distribution P over the interpretations in D of the symbols in S, or, equivalently, a probability measure on S-structures over D. Hence, semantically, relational Bayesian networks are mappings of finite domains D into probability measures on S-structures over D.\nExample 2.6 Reconsider the relations cancer and exposed as described in the introduction. Assume that 1 : N ---> [0, 1] is the probability distribution that for any fixed organ y gives the probability that y develops cancer after the nth exposure to radiation. Let f(n) := I:7=o 1(n) be the cor responding distribution function. Then r can be used to de fine a combination function combr by letting for a multiset A: combrA := f(n), where n is the number of nonzero el ements in A (counting multiplicities). Using combr we ob tain the probability formulacombr{ exposed(x, y, z) I z; T} for the contribution of organ y to the cancer risk of x. Com bining for all y, then"}, {"heading": "Fcancer(x) = n-o{combr{exposed(x, y, z) I z; r} I y; r}", "text": "is a probability formula defining the risk of cancer for x, given the relation exposed.\nIn the preceding example we have tacitly assumed a multi sorted domain, so that the variables x, y, z range over dif ferent sets \"people\", \"organs\", \"times\", respectively. We here do not introduce an extra formalization for dealing with many sorted domains. It is clear that this can be done easily, but would introduce an extra load of notation.\n3 INFERENCE\nThe inference problem we would like to solve is: given a relational Bayesian network N for S, a finite domain D = { d1, .. . , dn}, an evidence set of ground literals E = {rt(di), ... , rk(dk), ''k+t(dk+l), ... , 'rm(dm)} with r; E S (not necessarily distinct), d; \ufffd D (not neces sarily distinct) fori= 1, ... , m, and a ground atom r0(d0) (ro E S, do \ufffd D), what is the probability of r0(d0) given rt(di), ... , -wm(dm)? More precisely: in the probabil ity measure P defined by N on the S- structures over\nD, what is the conditional probability P(r0(do) I E) of a structure satisfying r0 (do), given that it satisfies rt(dt), ... , --,rm(dm)?\nSince for any given finite domain a relational Bayesian network can be seen as an ordinary Bayesian network for variables with finitely many possible values, in principle, any inference algorithm for standard Bayesian networks can be used.\nUnfortunately, however, direct application of any such algo rithm will be inefficient, because they include a summation over all possible values of a node, and the number of pos sible values here is exponential in the size of the domain. For this reason, it will often be more efficient to follow the approach used in inference from rule-base encodings of probabilistic knowledge, and to construct for every spe cific inference task an auxiliary Bayesian network whose nodes are ground atoms in the symbols from S, each of which with the two possible values true and false ( cf.(Breese 1992),(Ngo et a!. 1995)).\nThe reason why we here can do the same is that in the query ro (do) we do not ask for the probability of any spe cific interpretation of r0, but only for the probability of all interpretations containing d0. For the computation of this probability, in turn, it is irrelevant to know the exact inter pretations of parent nodes r' of r. Instead, we only need to know which of those tuples d' belong tor', whose indicator r1 ( d') is needed in the computation of Fr0( do). In order to construct such an auxiliary network, we have to compute for some given atom r( d) the list of atoms r' ( d') on whose truth value Fr (d) depends. One way of doing this is to just go through a recursive evaluation of Fr (d), and list all the ground atoms encountered in this evaluation. However, rather than doing this, it is useful to compute for every relation symbol r E S, and each parent relation r1 of r, an explicit description of the tuples y, such that Fr ( x) depends on r'(y). Such an explicit description can be given in form of a first -order formula parr' ( x 1 y) over the empty vocabulary.\nTo demonstrate the general method for the computation of these formulas, we show how to obtain pa.b(x 1 Yl 1 Y2) for F,(x) as defined in (5). By induction on the structure of F,, we compute formulas paGb ( x 1 Yl 1 Y2) that define for a subformula G(x) of F. the set of (y1, y2) s.t. G(x) depends on b(y1, y2). In the end, then, pa,b(x, Yt1 Y2) :=: paF,b(x, Y1, Y2)\u00b7\nThe two subformulas t ( x) and ( 1 - t ( x)) of F. do not depend on bat all; therefore we can let pat(x)b(x, Yl 1 Y2 )= pa(l-t(x))b(x1 Y1 1 Y2)= E, where E is some unsatisfiable for mula.\nTo obtain pan(x)b(x1 y1, y2) we begin with the atomic subformulas b(x,z) and b(z1x) of H(x), which yield pab(x,z)b(x,z,yt,Y2) = Yl x 1\\ Y2 = z and\npab(z,x)o(x, z, Yt, Yz) = Yt = z 1\\ Yz = x respectively. The remaining atomic subformulas t(z),l, and p2 appear ing within the max combination function again only yield the unsatisfiable c. Skipping one trivial step where the for mulas for the two arguments of M ( x, z) : = max{ . . . } are computed, we next obtain the formula\npaM(x,z)o(x, z, Yt, Y2) = (Yt = x 1\\ Yz = z) V (Yt = z 1\\ Y2 = x)\n(after deleting some meaningless c-disjuncts). H ( x) n-o{ M ( x, z) I z; z ::j:. x} depends on all b(y1, y2) for which there exist some z ::j:. x s.t. paM(x,z)o( x, z, Yt, Y2 ). Hence,\npaH(x)o(x, Yt, Y2) = 3z((Yt = x 1\\ Yz = z) V (Yt = z 1\\ Y2 = x)) , (8)\nwhich is already the same as paF,(x)b(x, y1, y2). Finally, we can simplify (8), and obtain\npa,b(x, Yt, Yz) := (Yt=xl\\y2::j:.x)V(Yt::f:.xl\\y2=x). (9)\nIn general, the formulas parr'(x, y) are existential 0- formulas. It is not always possible to completely elimi nate the existential quantifiers as in the preceding example. However, it is always possible to transform parr' ( x, y) into a formula so that quantifiers only appear in subformulas of the form 3;::-n xx = x, postulating the existence of at least n elements. This means that for every formula parr'( x, y), and tuples d, d' \ufffd D, it can be checked in time independent of the size of D whether parr'(d, d') holds.\nThe formula parr' ( x, y) enables us to find for every tuple d the parents r' ( d') of r( d) in the auxiliary network. More over, we can take this one step further: suppose that in the original network N there is a path of length two from node r\" via a node r' tor. Then, in the auxiliary network, there is a path of length two from a node r\" ( d\") via a node r' ( d') to r( d) iff the formula\npar11-r'-r(x, y) :=: 3z(parr'(x, z) 1\\par'ru(z, y)). (10) is satisfied for x = d, andy= d\". Taking the disjunction of all formulas of the form ( 1 0) for all paths in N leading from r\" to r then yields a formula pa;rll ( x, y) defining all predecessors r\" ( d\") of a node r( d) in the auxiliary network.\nUsing the parr' and pa;ru, we can for given evidence and query construct the auxiliary network needed to answer the query: we begin with a node r0 ( d0) for the query. For all nodes r( d) added to the network, we add all parents r' ( d') of r( d), as defined by parr'. If r( d) is not instantiated in E, using the formulas pa;,r, we check whether the subgraph rooted at r( d) contains a node instantiated in E. If this is the case, we add all successors of r( d) that lie on a path\nRelational Bayesian Networks 271\nfrom r( d) to an instantiated node (these are again given by the formulas pa;,r). Thus, we can construct directly the minimal network needed to answer the query, without first backward chaining from every atom in E, and pruning afterwards.\nAuxiliary networks as described here still encode finer dis tinctions in the instantiations of the nodes of N than is actually needed to solve our inference problem. Consider, for example, the case where the domain in example 2.1 con sists of ten locations {It, ... , Ito}, there is no evidence, and the query is s( 11). According to (9), the auxiliary network will contain nodes b(l1, li ), b( I;, 11) for all i = 2, . .. , 10. In applying standard inference techniques on this network, we distinguish e.g. the case where b(/1,/2), b(/2, It) are true and b(/1, /3), b(/3, It) are false from the case where b(l1,lz),b(l2,1d are false and b(l1,13),b(l3,11) are true, and all other b(/1,/;),b(/i,/1) have the same truth value. However, for the given inference problem, this distinction really is unnecessary, because the identity of locations men tioned neither in evidence nor query is immaterial. Future work will therefore be directed towards finding inference techniques for relational Bayesian networks that distinguish instantiations of the relations in the network at a higher level of abstraction than the current auxiliary networks, and thereby reduce the complexity of inference in terms of the size of the underlying domain.\n4 RECURSIVE NETWORKS\nIn the distributions defined by relational Bayesian networks of definition 2.5, the events r( a) and r( a') with a ::j:. a' are conditionally independent, given the interpretation of the parent relations of r. This is a rather strong limitation of the expressiveness of these networks. For instance, using these networks, we can not model a variation of example 2.1 in which the predicate blocked is symmetric: b(x, y) being independent from b(y, x), b(x, y) {::} b(y, x) can not be enforced.\nThere are other interesting things that we are not able to model so far. Among them are random functions (the main concern of (Haddawy 1994)), and a recursive temporal de pendence of a relation on itself (addressed both in (Ngo et a!. 1995) and (Glesner & Koller 1995)). In this sec tion we define a straightforward generalization of relational Bayesian networks that allows us to treat all these issues in a uniform way.\nWe can identify a recursive dependence of a relation on itself as the general underlying mechanism we have to model. In the case of symmetric relations, this is a dependence of r( x, y) on r(y, x ) . In the case of a temporal development, this is the dependence of a predicate r(t, x ), having a time variable as its first argument, on r( t - 1, x ) . Functions can be seen as special relations r( x, y ) , where for every :c there exists exactly one y, s.t. r(x, y) is true. Thus, for every x,\n272 Jaeger\nr( x, y) depends on all r( x, y') in that exactly one of these atoms must be true.\nIt is clear that there is no fundamental problem in model\ning such recursive dependencies within a Bayesian network framework, as long as the recursive dependency of r( x) on r(y1), ... r(y1,J does not produce any cycles. Most ob viously, in the case of a temporal dependency, the use of r( t - 1, x) in a definition of the probability of r ( t, x) does not pose a problem, as long as a non-recursive definition of the probability of r( 0, x) is provided.\nTo make the recursive dependency of r(x, y) on r(y, x ) in a symmetric relation similarly well-founded, we can use\na total order :=; on the domain. Then we can generate a random symmetric relation by first defining the probability of r(x, y) with x :=; y, and then the (0,1-valued) probability of r(y, x) given r(x, y). Now consider the case of a random function r(x,y) with possible values y E {v1, \u2022 . . ,vk}. Here, too, we can make the interdependence of the different r( x, y ) acyclic by using a total order on { v1 , . . . , v k}, and assigning a truth value to r( x, v; ) by taking into account the already defined truth values of r(x, Vj) for all Vj that precede v; in that order.\nFrom these examples we see that what we essentially need,\nin order to extend our framework to cover a great vari ety of interesting specific forms of probability distributions\nover 5-structures, are well-founded orderings on tuples of domain elements. These well-founded orderings can be\nsupplied via rigid relations on the domain, i.e. fixed, prede\ntermined relations that are not generated probabilistically. Indeed, one such relation we already have used throughout:\nthe equality relation. It is therefore natural to extend our framework by allowing additional relations that are to be\nused in the same way as the equality predicate has been em\nployed, namely, in constraints for combination functions.\nAlso, fixed constants will be needed as the possible values\nof random functions.\nFor the case of a binary symmetric relation r(x, y), assume, as above, that we are given a total (non-strict) order :=; on the domain. A probability formula that defines a proba\nbility distribution concentrated on symmetric relations, and making r( x 1, x2) true with probability p for all ( x 1, x2), then is\nFr(x1, x2)=max{max{p I 0; x1 :S: xz}, (11)\nmax{r(xz,xl) 10;-.xl :S: x2} 10;;}.\nAs in (7), here a nested max{ ... }-function is used in order to model a distinction by cases. The first inner max-function evaluates top if x1 :=; x2, and to 0 else. The second max function is equal to r(xz, xi) if x1 > x2, and 0 else.\nFor the temporal example, assume that the domain contains n + 1 time points t0, \u2022 \u2022 \u2022 , tn, and a successor relation s = {(t;,t;+l) I 0 :S: i :S: n-1 } on thet;'s. Assume that r (t , x) is a relation with a time parameter as the first argument ,\nand that r(t0, x) shall hold with probability p0 for all x, whereas r(ti+l, x) has probability p1 if r( t;, x) holds, and probability p2 else. In order to define the probability of r(t, x) by a probability formula, the case t = to must be distinguished from the case t = t;, i 2: 1. For this we use the probability formula F0 ( t ) = max{ 1 I t'; s( t', t)}, which evaluates to 0 fort = to, and to 1 fort = t 1, ... , tn. We can now use the formula\nFr(t, x) = (1- Fo(t))Po + Fa(t)max{r(t',x)pl + (1-r(t',x))pz\nIt'; s(t', t)}\nto define the probability of r(t, x ).\nFinally, for a functional relation r(x, y), suppose that we are given a domain, together with the interpretations of n constant symbols v1, .. . , Vn, and a strict total order <, s.t. v1 < v2 < ... < Vn. Now consider the probability formula\nFr(x, y) = (1- max{r(x, z ) I z; z < y}) \u00b7 max{max{pt I 0; y = vt}, ... ,\nmax{pn 10; Y = Vn} 10; r}\nThe first factor in this formula tests whether r( x, z) al ready is true for some possible value v; < y. If this is the case, then the probability of r( x, y) given by Fr ( x, y) is 0. Otherwise, the probability of r(x , y) is p; iffy = v;. T he probability that by this procedure the argument x is as signed the valuev; then is (l-pt)(l-p2) ... (1-p; _ t)p ; . By a suitable choice of the p; any probability distribution over the v; can be generated.\nThe given examples motivate a generalization of relational Bayesian networks. For this, let R be a vocabulary contain ing relation and constant symbols, S a relational vocabulary with RnS = 0. An R-constraintc(x )forx is a quantifier freeR-formula. Define the class of R-probabilityformulas over 5 precisely as in definition 2.3, with \"equality con straint\" replaced by \"R-constraint\".\nDefinition 4.1 Let R, 5 be as above. A recursive relational Bayesian network for 5 with R-constraints is given by a directed acyclic graph containing one node for every r E 5. The node for an n-ary r E 5 is labeled with an R probability formula Fr(x1, ... , xn) over Pa(r) U {r }.\nThe semantics of a recursive relational Bayesian network is a bit more complicated than that of relational Bayesian\nnetworks. T he latter defined a mapping of domains D into probability measures on S-structures over D. Re cursive relational Bayesian networks essentially define a mapping of R-structures \ufffd into probability measures on\n5-expansions of\ufffd- This mapping, however, is only defined for R-structures whose interpretations of the symbols in R lead to well-founded recursive definitions of the probabil ities for the r-atoms (r E 5). If, for instance, R = {::;},\nand q! is an R-structure in which there exist two elements dt , d2 , s.t. neither d1 ::; d2 , nor d2 ::; d1 , then ( I I ) does not define a probability measure on { r} -expansions of q!, because the probability of 1\u00b7( d1 , d2) gets defined in terms of r( d2 , d ! ) , and vice versa. As i n section 3, for every r ' E Pa( r) U { r} a formula parr' ( x , y) can be computed that defines for an R-structure q! and d c;;; D the tuples d' c;;; D, s.t. Fr (d) depends on r' ( d'). While in section 3 existential formulas over the empty vocabulary were obtained, for recursive relational networks the parr' are existential formulas over R.\nThe definitions of the probabilities Fr (d) are well-founded for d c;;; D iff the relation q!(parr ) := {(d, t!') I parr(d, t!') holds in P} is acyclic. A recursive relational Bayesian network N thus defines a probability measure on S'-expansions of those R-structures q!, for which the rela tion (g(parr ) is acyclic for all r E 5.\nThe discussion of i nference procedures for relational Bayesian networks i n section 3 applies with few modifi cations to recursive networks as well. Again, we can con struct an auxiliary network with nodes for ground atoms, using formulas parr' and pa;r'. The complexity of this construction, however, increases on two accounts: first, the existential quantifications in the parr' , pa;r' can no longer be reduced to mere cardinality constraints. Therefore, the complexity of deciding whether pa\ufffd;!( d, d') holds for given d, d' c;;; D is no longer guaranteed to be independent of the size of the domain D. Second, to obtain the formulas pa;r' we may have to build much larger disjunctions: it is no longer sufficient to take the disj unction over all possible paths from r' to r in the network structure of N. In ad dition, for every relation r on these paths, the disjunction over all possible paths within q!(pan) has to be taken. This amounts to determining the length I of the longest path in q!(parr ) , and then taking the disjunction over all formulas pa\ufffdr (x , y) : =: 3zl , . . . , Zi (Parr(x , Zt) A . . . Aparr(z; , y)) with i < l. As a consequence, the formulas pa;r' are no longer independent of the structure q! under consideration.\n5 CONCLUSION\nIn this paper we have presented a new approach to deal with rule-like probability statements for nondeterministic relations on the elements of some domain of discourse. De viating from previous proposals for formalizing such rules with a logic programming style framework, we here have associated with every relation symbol r\u00b7 a single probabil ity formula that directly defines the probability distribution over i nterpretations of r within a Bayesian network. The re sulting framework is both more expressive and semantically more transparent than previous ones. It is more expressive, because it introduces the tools to restrict the i nstantiations of certain rules to tuples satisfying certain equality con-\nRelational Bayesian Networks 273\nstraints, and to specify complex combinations and nestings of combination functions. It is semantically more transpar ent, because a relational Bayesian network directly defines a unique probability distribution over S-structures, whereas the semantics of a probabilistic rule base usually are only implicitly defined through a transformation into an auxiliary Bayesian network.\nInference from relational Bayesian networks by auxiliary network construction is as efficient as inference (by essen tially the same method) in rule based formalisms. It may be hoped that in the case where this inference procedure seems unsatisfactory, namely, for large domains most of whose elements are not mentioned in the evidence, our new representation paradigm will lead to more efficient infer ence techniques.\nAcknowledgments\nI have benefited from discussions with Daphne Koller who also provided the original motivation for this work. This work was funded in part by DARPA contract DACA 76-93- C-0025, under subcontract to Information Extraction and Transport, Inc.\nReferences\nBreese, J. S. ( 1 992), \"Construction of belief and decision networks\", Computational Intelligence.\nGlesner, S. & Koller, D. ( 1 995), Constructing flexible dy namic belief networks from first-order probabilistic knowledge bases, in \"Proceedings of ECSQARU\" , Lecture Notes in Artificial Intelligence, Springer Ver lag.\nHaddawy, P. ( 1 994), Generating bayesian networks from probability logic knowledge bases, in \"Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence\".\nHalpern, J. ( 1 990), \"An analysis of first-order logics of probability\", Artificial Intelligence 46, 3 1 1 -350.\nKoller, D. & Halpern, J. Y. ( 1 996), Irrelevance and con ditioning in first-order probabilistic logic, in \"Pro ceedins of the 1 3th National Conference on Artificial Intelligence (AAAI)\" , pp. 569-576.\nNgo, L., Haddawy, P. & Helwig, J. ( 1 995), A theoretical framework for context-sensitive temporal probability model construction with application to plan projec tion, in \"Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence\", pp. 4 1 9-426.\nPoole, D. ( 1 993), \"Probabilistic hom abduction and bayesian networks\", Artificial Intelligence 64, 8 1 - 1 29."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "A new method is developed to represent prob\u00ad abilistic relations on multiple random events. Where previously knowledge bases containing probabilistic rules were used for this purpose, here a probability distribution over the relations is directly represented by a Bayesian network. By using a powerful way of specifying conditional probability distributions in these networks, the resulting formalism is more expressive than the previous ones. Particularly, it provides for con\u00ad straints on equalities of events, and it allows to define complex, nested combination functions.", "creator": "pdftk 1.41 - www.pdftk.com"}}}