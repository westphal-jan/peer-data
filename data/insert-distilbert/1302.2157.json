{"id": "1302.2157", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2013", "title": "Passive Learning with Target Risk", "abstract": "in this paper we consider learning in passive setting but with surprisingly a slight modification. we assume that determining the immediate target expected loss, also referred to as target risk, is provided in advance for learner as prior knowledge. unlike most studies in the learning utility theory that only randomly incorporate the principal prior knowledge into the generalization bounds, we are able immediately to explicitly utilize the target risk assumptions in the learning process. our analysis reveals a surprising result on the possible sample complexity of learning : by exploiting the target risk in the learning algorithm, we show that when the loss sum function is both strongly convex to and properly smooth, creating the sample complexity \u03b4 reduces to $ \\ o ( \\ log ( \\ frac { 1 } { \\ epsilon } ) ) $, an exponential improvement compared according to including the sample complexity $ \\ o ( \\ frac { 1 } { \\ epsilon } ) $ for learning with strongly convex loss length functions. furthermore, our proof is constructive and is based uniformly on a computationally efficient stochastic functional optimization algorithm for considering such settings which demonstrate that the proposed algorithm is practically useful.", "histories": [["v1", "Fri, 8 Feb 2013 21:18:24 GMT  (20kb)", "https://arxiv.org/abs/1302.2157v1", null], ["v2", "Sun, 19 May 2013 00:39:52 GMT  (20kb)", "http://arxiv.org/abs/1302.2157v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mehrdad mahdavi", "rong jin"], "accepted": false, "id": "1302.2157"}, "pdf": {"name": "1302.2157.pdf", "metadata": {"source": "CRF", "title": "Passive Learning with Target Risk", "authors": ["Mehrdad Mahdavi", "Rong Jin"], "emails": ["mahdavim@cse.msu.edu", "rongjin@cse.msu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 2.\n21 57\nv2 [\ncs .L\nG ]\n1 9\n( 1\n\u01eb\n) ), an exponential improvement compared to\nthe sample complexity O( 1 \u01eb ) for learning with strongly convex loss functions. Furthermore, our proof is constructive and is based on a computationally efficient stochastic optimization algorithm for such settings which demonstrate that the proposed algorithm is practically useful."}, {"heading": "1 Introduction", "text": "In the standard passive supervised learning setting, the learning algorithm is given a set of labeled examples S = ((x1, y1), \u00b7 \u00b7 \u00b7 , (xn, yn)) drawn i.i.d. from a fixed but unknown distribution D. The goal, with the help of labeled examples, is to output a classifier h from a predefined hypothesis class H that does well on unseen examples coming from the same distribution. The sample complexity of an algorithm is the number of examples which is sufficient to ensure that, with probability at least 1\u2212 \u03b4 (w.r.t. the random choice of S), the algorithm picks a hypothesis with an error that is at most \u01eb from the optimal one. Sample complexity of passive learning is well established and goes back to early works in the learning theory where the lower bounds \u2126 ( 1 \u01eb (log 1 \u01eb + log 1 \u03b4 ) ) and \u2126 ( 1 \u01eb2 (log 1 \u01eb + log 1 \u03b4 ) ) were obtained in classic PAC and general agnostic PAC settings, respectively [9, 5, 1]. In light of no free lunch theorem, learning is impossible unless we make assumptions regarding the nature of the problem at hand. Therefore, when approaching a particular learning problem, it is desirable to take into account some prior knowledge we might have about our problem and use a specialized algorithm that exploits this knowledge into a learning process or theoretical analysis. A key issue in this regard is the formalization of prior knowledge. Such prior knowledge can be expressed by restricting our hypothesis class, making assumptions on the nature of unknown distribution D or formalization of the data space, analytical\nproperties of the loss function being used to evaluate the performance, sparsity, and margin\u2013 to name a few.\nThere has been an upsurge of interest over the last decade in finding tight upper bounds on the sample complexity by utilizing prior knowledge on the analytical properties of the loss function, that led to stronger generalization bounds in agnostic PAC setting. In [17] fast rates obtained for squared loss, exploiting the strong convexity of this loss function, which only holds under pseudo-dimensionality assumption. With the recent development in online strongly convex optimization [11], fast rates approaching O(1\n\u01eb log 1 \u03b4 ) for convex Lipschitz\nstrongly convex loss functions has been obtained in [29, 15]. For smooth non-negative loss functions, [27] improved the sample complexity to optimistic rates\nO ( 1\n\u01eb\n( \u01ebopt + \u01eb\n\u01eb\n)( log3 1\n\u01eb + log\n1\n\u03b4\n))\nfor non-parametric learning using the notion of local Rademacher complexity [3], where \u01ebopt is the optimal risk.\nIn this work, we consider a slightly different setup for passive learning. We assume that before the start of the learning process, the learner has in mind a target expected loss, also referred to as target risk, denoted by \u01ebprior\n1, and tries to learn a classifier with the expected risk of O(\u01ebprior) by labeling a small number of training examples. We further assume the target risk \u01ebprior is feasible, i.e., \u01ebprior \u2265 \u01ebopt. To address this problem, we develop an efficient algorithm, based on stochastic optimization, for passive learning with target risk. The most surprising property of the proposed algorithm is that when the loss function is both smooth and strongly convex, it only needs O(d log(1/\u01ebprior)) labeled examples to find a classifier with the expected risk of O(\u01ebprior), where d is the dimension of data. This is a significant improvement compared to the sample complexity for empirical risk minimization.\nThe key intuition behind our algorithm is that by knowing target risk as prior knowledge, the learner has better control over the variance in stochastic gradients, which contributes mostly to the slow convergence in stochastic optimization and consequentially large sample complexity in passive learning. The trick is to run the stochastic optimization in multistages with a fixed size and decrease the variance of stochastically perturbed gradients at each iteration by a properly designed mechanism. Another crucial feature of the proposed algorithm is to utilize the target risk \u01ebprior to gradually refine the hypothesis space as the algorithm proceeds. Our algorithm differs significantly from standard stochastic optimization algorithms and is able to achieve a geometric convergence rate with the knowledge of target risk \u01ebprior.\nWe note that our work does not contradict the lower bound in [27] because a feasible target risk \u01ebprior is given in our learning setup and is fully exploited by the proposed algorithm. Knowing that the target risk \u01ebprior is feasible makes it possible to improve the sample complexity from O(1/\u01ebprior) to O(log(1/\u01ebprior)). We also note that although the logarithmic sample complexity is known for active learning [10, 2], we are unaware of any existing passive learning algorithm that is able to achieve a logarithmic sample complexity by incorporating any kind of prior knowledge.\n1We use \u01ebprior instead of \u01eb to emphasize the fact that this parameter is known to the learner in advance."}, {"heading": "1.1 More Related Work", "text": "Stochastic Optimization and Learnability Our work is related to the recent studies that examined the learnability from the viewpoint of stochastic convex optimization. In [28, 26], the authors presented learning problems that are learnable by stochastic convex optimization but not by empirical risk minimization (ERM). Our work follows this line of research. The proposed algorithm achieves the sample complexity of O(d log(1/\u01ebprior)) by explicitly incorporating the target expected risk \u01ebprior into the stochastic convex optimization algorithm. It is however difficult to incorporate such knowledge into the framework of ERM. Furthermore, it is worth noting that in [23, 28, 22, 4], the authors explored the connection between online optimization and statistical learning in the opposite direction. This was done by exploring the complexity measures developed in statistical learning for the learnability of online learning.\nOnline and Stochastic Optimization The proposed algorithm is closely related to the recent works that stated O(1/n) is the optimal convergence rate for stochastic optimization when the objective function is strongly convex [14, 12, 21]. In contrast, the proposed algorithm is able to achieve a geometric convergence rate for a target optimization error. Similar to the previous argument, our result does not contradict the lower bound given in [12] because of the knowledge of a feasible optimization error. Moreover, in contrast to the multistage algorithm in [12] where the size of stages increases exponentially, in our algorithm, the size of each stage is fixed to be a constant.\nOutline The remainder of the paper is organized as follows: In Section 2, we set up notation, describe the setting, and discuss the assumptions on which our algorithm relies. Section 3 motivates the problem and discusses the main intuition of our algorithm. The proposed algorithm and main result are discussed in Section 4. We prove the main result in Section 5. Section 6 concludes the paper and the appendix contains the omitted proofs."}, {"heading": "2 Preliminaries", "text": "As usual in the framework of statistical learning theory, we consider a domain Z := X \u00d7 Y where X \u2286 Rd is the space for instances and Y is the set of labels, and H is a hypothesis class. We assume that the domain space Z is endowed with an unknown Borel probability measure D. We measure the performance of a specific hypothesis h by defining a nonnegative loss function \u2113 : H\u00d7Z \u2192 R+. We denote the risk of a hypothesis h by L(h) = Ez\u223cD[\u2113(h, z)]. Given a sample S = (z1, \u00b7 \u00b7 \u00b7 , zn) = ((x1, y1), \u00b7 \u00b7 \u00b7 , (xn, yn)) \u223c Dn, the goal of a learning algorithm is to pick a hypothesis h : X \u2192 Y from H in such a way that its risk L(h) is close to the minimum possible risk of a hypothesis in H.\nThroughout this paper we pursue stochastic optimization viewpoint for risk minimization as detailed in Section 3. Precisely, we focus on the convex learning problems for which we assume that the hypothesis class H is a parametrized convex set H = {hw : x 7\u2192 \u3008w,x\u3009 : w \u2208 Rd, \u2016w\u2016 \u2264 R} and for all z = (x, y) \u2208 Z, the loss function \u2113(\u00b7, z) is a non-negative convex function. Thus, in the remainder we simply use vector w to represent hw, rather than working with hypothesis hw. We will assume throughout that X \u2286 Rd is the unit ball so\nthat \u2016x\u2016 \u2264 1. Finally, the conditions under which we can get the desired result on sample complexity depend on analytic properties of the loss function. In particular, we assume that the loss function is strongly convex and smooth [20].\nDefinition 1 (Strong convexity). A loss function \u2113(w) is said to be \u03b1-strongly convex w.r.t a norm \u2016 \u00b7 \u20162, if there exists a constant \u03b1 > 0 (often called the modulus of strong convexity) such that, for any \u03bb \u2208 [0, 1] and for all w1,w2 \u2208 H, it holds that\n\u2113(\u03bbw1 + (1\u2212 \u03bb)w2) \u2264 \u03b1\u2113(w1) + (1 \u2212 \u03bb)\u2113(w2)\u2212 1\n2 \u03bb(1\u2212 \u03bb)\u03b1\u2016w1 \u2212w2\u20162.\nWhen \u2113(w) is differentiable, the strong convexity is equivalent to\n\u2113(w1) \u2265 \u2113(w2) + \u3008\u2207\u2113(w2),w1 \u2212w2\u3009+ \u03b1\n2 \u2016w1 \u2212w2\u20162, \u2200 w1,w2 \u2208 H.\nWe would like to emphasize that in our setting, we only need that the expected loss function L(w) be strongly convex, without having to assume strong convexity for individual loss functions. Another property of loss function that underline our analysis is its smoothness. Smooth functions arise, for instance, in logistic and least-squares regression, and in general for learning linear predictors where the loss function has a Lipschitz-continuous gradient.\nDefinition 2 (Smoothness). A differentiable loss function \u2113(w) is said to be \u03b2-smooth with respect to a norm \u2016 \u00b7 \u2016, if it holds that\n\u2113(w1) \u2264 \u2113(w2) + \u3008\u2207\u2113(w2),w1 \u2212w2\u3009+ \u03b2\n2 \u2016w1 \u2212w2\u20162, \u2200 w1,w2 \u2208 H. (1)"}, {"heading": "3 The Curse of Stochastic Oracle", "text": "We begin by discussing stochastic optimization for risk minimization, convex learnability, and then the main intuition that motivates this work.\nMost existing learning algorithms follow the framework of empirical risk minimizer (ERM) or regularized ERM, which was developed to great extent by Vapnik and Chervonenkis [30]. Essentially, ERM methods use the empirical loss over S, i.e., L\u0302(w) = 1 n \u2211n i=1 \u2113(w, zi), as a criterion to pick a hypothesis. In regularized ERM methods, the learner picks a hypothesis that jointly minimizes L\u0302(w) and a regularization function over w. We note that ERM resembles the widely used Sample Average Approximation (SAA) method in the optimization community when the hypothesis space and the loss function are convex. If uniform convergence holds, then the empirical risk minimizer is consistent, i.e., the population risk of the ERM converges to the optimal population risk, and the problem is learnable using ERM.\nA rather different paradigm for risk minimization is stochastic optimization. Recall that the goal of learning is to approximately minimize the risk L(w) = Ez\u223cD [\u2113(w, z)]. However, since the distribution D is unknown to the learner, we can not utilize standard gradient methods to minimize the expected loss. Stochastic optimization methods circumvent this problem by allowing the optimization method to take a step which is only in expectation along the\n2Throughout this paper, we only consider the \u21132-norm.\nnegative of the gradient. To motivate stochastic optimization as an alternative to the ERM method, [25, 24] challenged the ERM method and showed that there is a real gap between learnability and uniform convergence by investigating non-trivial problems where no uniform convergence holds, but they are still learnable using Stochastic Gradient Descent (SGD) algorithm [18]. These results uncovered an important relationship between learnability and stability, and showed that stability together with approximate empirical risk minimization, assures learnability [26]. We note that Lipschitzness or smoothness of loss function is necessary for an algorithm to be stable, and boundedness and convexity alone are not sufficient for ensuring that the convex learning problem is learnable.\nTo directly solve minw\u2208H L(w) = Ez\u223cD[\u2113(w, z)], a typical stochastic optimization algorithm initially picks some point in the feasible set H and iteratively updates these points based on first order perturbed gradient information about the function at those points. For instance, the widely used SGD algorithm starts with w0 = 0; at each iteration t, it queries the stochastic oracle (SO) at wt to obtain a perturbed but unbiased gradient g\u0302t and updates the current solution by wt+1 = \u03a0H (wt \u2212 \u03b7tg\u0302t) , where \u03a0H(w) projects the solution w into the domain H. To capture the efficiency of optimization procedures in a general sense, one can use oracle complexity of the algorithm which, roughly speaking, is the minimum number of calls to any oracle needed by any method to achieve desired accuracy [20]. We note that the oracle complexity corresponds to the sample complexity of learning from the stochastic optimization viewpoint previously discussed. The following theorem states a lower bound on the sample complexity of stochastic optimization algorithms [19].\nTheorem 3 (Lower Bound on Oracle Complexity). Suppose L(w) = Ez\u223cD[\u2113(w, z)] is \u03b1strongly and \u03b2-smooth convex function defined over convex domain H. Let SO be a stochastic oracle that for any point w \u2208 H returns an unbiased estimate g\u0302, i.e., E[g\u0302] = \u2207L(w), such that E [ \u2016g\u0302\u2212\u2207L(w)\u20162 ] \u2264 \u03c32 holds. Then for any stochastic optimization algorithm A to find a solution w\u0302 with \u01eb accuracy respect to the optimal solution w\u2217, i.e., E [L(w\u0302)\u2212 L(w\u2217)] \u2264 \u01eb, the number of calls to SO is lower bounded by\nO(1) (\u221a \u03b2\n\u03b1 log\n( \u03b2\u2016w0 \u2212w\u2217\u20162\n\u01eb\n) + \u03c32\n\u03b1\u01eb\n) . (2)\nThe first term in (2) comes from deterministic oracle complexity and the second term is due to noisy gradient information provided by SO. As indicated in (2), the slow convergence rate for stochastic optimization is due to the variance in stochastic gradients, leading to at least O ( \u03c32/\u01eb ) queries to be issued. We note that the idea of mini-batch [7, 8], although it reduces the variance in stochastic gradients, does not reduce the oracle complexity. We close this section by informally presenting why logarithmic sample complexity is, in principle, possible, under the assumption that target risk is known to the learner A. To this end, consider the setting of Theorem 3 and assume that the learner A is given the prior accuracy \u01ebprior and is asked to find an \u01ebprior-accurate solution. If it happens that the variance of SO has the same magnitude as \u01ebprior, i.e., E [ \u2016g\u0302\u2212\u2207L(w)\u20162 ] \u2264 \u01ebprior, then from (2) it\nfollows that the second term vanishes and the learner A needs to issue only O (log 1/\u01ebprior) queries to find the solution. But, since there is no control on SO, except that the variance of stochastic gradients are bounded, A needs a mechanism to manage the variance of perturbed gradients at each iteration in order to alleviate the influence of noisy gradients. One strategy is to replace the unbiased estimate of gradient with a biased one, which unfortunately may yield loose bounds. To overcome this problem, we introduce a strategy that shrinks the solution space with respect to the target risk \u01ebprior to control the damage caused by biased estimates."}, {"heading": "4 Algorithm and Main Result", "text": "In this section we proceed to describe the proposed algorithm and state the main result on its sample complexity."}, {"heading": "4.1 Description of Algorithm", "text": "We now turn to describing our algorithm. Interestingly, our algorithm is quite dissimilar to the classic stochastic optimization methods. It proceeds by running the algorithm online on fixed chunks of examples, and using the intermediate hypotheses and target risk \u01ebprior to gradually refine the hypothesis space. As mentioned above, we assume in our setting that the target expected risk \u01ebprior is provided to the learner a priori. We further assume the target risk \u01ebprior is feasible for the solution within the domain H, i.e., \u01ebprior \u2265 \u01ebopt. The proposed algorithm explicitly takes advantage of the knowledge of expected risk \u01ebprior to attain an O (log(1/\u01ebprior)) sample complexity.\nThroughout we shall consider linear predictors of form \u3008w,x\u3009 and assume that the loss function of interest \u2113(\u3008w,x\u3009, y) is \u03b2-smooth. It is straightforward to see that L(w) = E(x,y)\u223cD [\u2113(\u3008w,x\u3009, y)] is also \u03b2-smooth. In addition to the smoothness of the loss function, we also assume that L(w) to be \u03b1-strongly convex. We denote by w\u2217 the optimal solution that minimizes L(w), i.e., w\u2217 = argminw\u2208H L(w), and denote its optimal value by \u01ebopt.\nLet (xt, yt), t = 1, . . . , T be a sequence of i.i.d. training examples. The proposed algorithm divides the T iterations into the m stages, where each stage consists of T1 training examples, i.e., T = mT1. Let (x t k, y t k) be the t-th training example received at stage k, and let \u03b7 be the step size used by all the stages. At the beginning of each stage k, we initialize the solution w by the average solution w\u0302k obtained from the last stage, i.e.,\nw\u0302k = 1\nT1\nT1\u2211\nt=1\nw\u0302tk, (3)\nwhere w\u0302tk denotes the tth solution at stage k. Another feature of the proposed algorithm is a domain shrinking strategy that adjusts the domain as the algorithm proceeds using intermediate hypotheses and target risk. We define the domain Hk used at stage k as\nHk = {w \u2208 H : \u2016w\u2212 w\u0302k\u2016 \u2264 \u2206k} , (4)\nwhere \u2206k is the domain size, whose value will be discussed later. Similar to the SGD method, at each iteration of stage k, we receive a training example (xtk, y t k), and compute the gradient\nAlgorithm 1 Convex Learning with Target Risk\n1: Input: step size \u03b7, stage size T1, number of stages m, target expected risk \u01ebprior, parameters \u03b5 \u2208 (0, 1) and \u03c4 \u2208 (0, 1) used for updating domain size \u2206k, and parameter \u03be \u2265 1 used to clip the gradients 2: Initialization: w\u03021 = 0, \u22061 = R, and H1 = H 3: for k = 1, . . . ,m do 4: Set wtk = w\u0302k and \u03b3k = 2\u03be\u03b2\u2206k 5: for t = 1, . . . , T1 do 6: Receive training example (xt, yt) 7: Compute the gradient g\u0302tk and the clipped version of the gradient v t k using Eq. (5) 8: Update the solution wtk using Eq. (6). 9: end for 10: Update \u2206k using Eq. (7). 11: Compute the average solution w\u0302k+1 according to Eq. (3), and update the domain Hk+1 using the expression in (4). 12: end for\ng\u0302tk = \u2113 \u2032 (\u3008wtk,xtk\u3009, yt)xtk. Instead of using the gradient directly, following [13], a clipped version of the gradient, denoted by vtk = clip (\u03b3k, g\u0302 t k), will be used for updating the solution. More specifically, the clipped vector vtk \u2208 Rd is defined as\n[vtk]i = clip ( \u03b3k, [ g\u0302tk ] i ) = sign ([ g\u0302tk ] i ) min ( \u03b3k, \u2223\u2223[g\u0302tk ] i \u2223\u2223) , i = 1, . . . , d (5)\nwhere \u03b3k = 2\u03be\u03b2\u2206k with \u03be \u2265 1. Given the clipped gradient vtk, we follow the standard framework of stochastic gradient descent, and update the solution by\nwt+1k = \u03a0Hk ( wtk \u2212 \u03b7vtk ) . (6)\nThe purpose of introducing the clipped version of the gradient is to effectively control the variance in stochastic gradients, an important step toward achieving the geometric convergence rate. At the end of each stage, we will update the domain size by explicitly exploiting the target expected risk \u01ebprior as\n\u2206k+1 = \u221a \u03b5\u22062k + \u03c4\u01ebprior , (7)\nwhere \u03b5 \u2208 (0, 1) and \u03c4 \u2208 (0, 1) are two parameters, both of which will be discussed later. Algorithm 1 gives the detailed steps for the proposed method. The three important aspects of Algorithm 1, all crucial to achieve a geometric convergence rate, are highlighted as follows:\n\u2022 Each stage of the proposed algorithm is comprised of the same number of training examples. This is in contrast to the epoch gradient algorithm [12] which divides m iterations into exponentially increasing epochs, and runs SGD with averaging on each epoch. Also, in our case the learning rate is fixed for all iterations.\n\u2022 The proposed algorithm uses a clipped gradient for updating the solution in order to better control the variance in stochastic gradients; this stands in contrast to the SGD method, which uses original gradients to update the solution.\n\u2022 The proposed algorithm takes into account the targeted expected risk and intermediate hypotheses when updating the domain size at each stage. The purpose of domain shrinking is to reduce the damage caused by biased gradients that resulted from clipping operation."}, {"heading": "4.2 Main Result on Sample Complexity", "text": "The main theoretical result of Algorithm 1 is given in the following theorem.\nTheorem 4 (Convergence Rate). Assume that the hypothesis space H is compact and the loss function \u2113 is \u03b1-strongly convex and \u03b2-smooth. Let T = mT1 be the size of the sample and \u01ebprior be the target expected loss given to the learner in advance such that \u01ebopt \u2264 \u01ebprior holds. Given \u03b5 \u2208 (0, 1) and \u03c4 \u2208 (0, 1), set \u03be, \u03b7, and T1 as\n\u03be = 4\u03b2\n\u03b1\u03c4 , T1 = 4max\n{ \u03be3\u03b2d+ 2\u03be\u03b2 \u221a d\n\u03b5\u03b1 ln\nms \u03b4 , 16\u03be2\u03b22 \u03b12\u03b52\n} , \u03b7 =\n1\n2\u03be\u03b2 \u221a T1 ,\nwhere\ns = \u2308 log2 \u03be\u03b2R2\n\u01ebprior\n\u2309 . (8)\nAfter running Algorithm 1 over m stages, we have, with a probability 1\u2212 \u03b4,\nL(w\u0302m+1) \u2264 \u03b2R2\n2 \u03b5m +\n( 1 + \u03c4\n1\u2212 \u03b5\n) \u01ebprior,\nimplying that only O(d log[1/\u01ebprior]) training examples are needed in order to achieve a risk of O(\u01ebprior).\nWe note that comparing to the bound in Theorem 3, for Algorithm 1 the level of error to which the linear convergence holds is not determined by the noise level in stochastic gradients, but by the target risk. In other words, the algorithm is able to tolerate the noise by knowing the target risk as prior knowledge and achieves a linear convergence to the level of the target risk even when the variance of stochastic gradients is much larger than the target risk. In addition, although the result given in Theorem 4 assumes a bounded domain with \u2016w\u2016 \u2264 R, however, this assumption can be lifted by effectively exploring the strong convexity of the loss function and further assuming that the loss function is Lipschitz continuous with constant G, i.e., |L(w1) \u2212 L(w2)| \u2264 G\u2016w1 \u2212 w2\u2016, \u2200 w1,w2 \u2208 H. More specifically, the fact that the L(w) is \u03b1-strongly convex with first order optimality condition, for the optimal solution w\u2217 = argminw\u2208H L(w), we have\nL(w) \u2212 L(w\u2217) \u2265 \u03b1\n2 \u2016w\u2212w\u2217\u20162, \u2200w \u2208 H.\nThis inequality combined with Lipschitz continuous assumption implies that for any w \u2208 H the inequality \u2016w \u2212 w\u2217\u2016 \u2264 R\u2217 := 2G/\u03b1 holds, and therefore we can simply set R = R\u2217. We also note that this dependency can be resolved with a weaker assumption than Lipschitz continuity, which only depends on the gradient of loss function at origin. To this end, we define |\u2113\u2032(0, y)| = G. Using the fact that L(w) is \u03b1-strongly, it is easy to verify that \u03b12 \u2016w\u2217\u20162\u2212 G\u2016w\u2217\u2016 \u2264 0, leading to \u2016w\u2217\u2016 \u2264 R\u2217 := 2\u03b1G and, therefore, we can simply set R = R\u2217.\nWe now use our analysis of Algorithm 1 to obtain a sample complexity analysis for learning smooth strongly convex problems with a bounded hypothesis class. To make it easier to parse, we only keep the dependency on the main parameters d, \u03b1, \u03b2, T , and \u01ebprior and hide the dependency on other constants in O(\u00b7) notation. Let w\u0302 denote the output of Algorithm 1. By setting \u03b5 = 0.5 and letting c = O(\u03c4) to be an arbitrary small number, Theorem 4 yields the following:\nCorollary 5 (Sample Complexity). Under the same conditions as Theorem 4, by running Algorithm 1 for minimizing L(w) with a number of iterations (i.e., number of training examples) T , if it holds that,\nT \u2265 O ( d\u03ba4 ( log 1\n\u01ebprior log log\n1\n\u01ebprior + log\n1\n\u03b4\n))\nwhere \u03ba = \u03b2/\u03b1 denotes the condition number of the loss function and d is the dimension of data, then with a probability 1\u2212 \u03b4, w\u0302 attains a risk of O(\u01ebprior), i.e., L(w\u0302) \u2264 (1 + c)\u01ebprior.\nAs an example of a concrete problem that may be put into the setting of the present work is the regression problem with squared loss. It is easy to show that average square loss function is Lipschitz continuous with a Lipschitz constant \u03b2 = \u03bbmax(X\n\u22a4X) which denotes the largest eigenvalue of matrix X\u22a4X where X is the data matrix. The strong convexity is guaranteed as long as the population data covariance matrix is not rank-deficient and its minimum eigenvalue is lower bounded by a constant \u03b1 > 0. For this problem, the optimal minimax sample complexity is known to be O(1\n\u01eb ), but as it implies from Corollary 5, by the knowledge\nof target risk \u01ebprior, it is possible to reduce the sample complexity to O(log(1/\u01ebprior)).\nRemark 6. It is indeed remarkable that the sample complexity of Theorem 4 has \u03ba4 = (\u03b2/\u03b1)4 dependency on the condition number of the loss function, which is worse than the \u221a \u03b2/\u03b1 dependency in the lower bound in (2). Also, the explicit dependency of sample complexity on dimension d makes the proposed algorithm inappropriate for non-parametric settings."}, {"heading": "5 Analysis", "text": "Now we turn to proving the main theorem. The proof will be given in a series of lemmas and theorems where the proof of few are given in the appendix. The proof makes use of the Bernstein inequality for martingales, idea of peeling process, self-bounding property of smooth loss functions, standard analysis of stochastic optimization, and novel ideas to derive the claimed sample complexity for the proposed algorithm.\nThe proof of Theorem 4 is by induction and we start with the key step given in the following theorem.\nTheorem 7. Assume \u01ebprior \u2265 \u01ebopt. For a fixed stage k, if \u2016w\u0302k \u2212 w\u2217\u2016 \u2264 \u2206k, then, with a probability 1\u2212 \u03b4, we have \u2016w\u0302k+1 \u2212w\u2217\u20162 \u2264 a\u22062k + b\u01ebprior where\na = 2\n\u03b1T1\n( 2\u03be\u03b2 \u221a T1 + [ \u03be3\u03b2d+ 2\u03be\u03b2 \u221a d ] ln s\n\u03b4\n) , b = 8\n\u03b1\u03be (9)\nand s is given in (8), provided that \u03be \u2265 16\u03b2/\u03b1 and \u03b7 = 1/(2\u03be\u03b2 \u221a T1) hold.\nTaking this statement as given for the moment, we proceed with the proof of Theorem 4, returning later to establish the claim stated in Theorem 7.\nof Theorem 4. By setting a and b in (9) in Theorem 7 as a \u2264 \u03b5 and b \u2264 2\u03c4/\u03b2, we have \u03be \u2265 4\u03b2/(\u03b1\u03c4) and\nT1 \u2264 2\n\u03b1\u03b5\n( 2\u03be\u03b2 \u221a T1 + [ \u03be3\u03b2d+ 2\u03be\u03b2 \u221a d ] ln s\n\u03b4\n)\nimplying that\nT1 \u2265 4max { \u03be3\u03b2d+ 2\u03be\u03b2 \u221a d\n\u03b5\u03b1 ln\ns \u03b4 , 16\u03be2\u03b22 \u03b12\u03b52\n} .\nThus, using Theorem 7 and the definition of \u03be and T1, we have, with a probability 1\u2212 \u03b4,\n\u22062k+1 \u2264 \u03b5\u22062k + 2\u03c4\n\u03b2 \u01ebprior.\nAfter m stages, with a probability 1\u2212m\u03b4, we have\n\u22062m+1 \u2264 \u03b5m\u220621 + 2\u03c4\n\u03b2 \u01ebprior\nm\u22121\u2211\ni=0\n\u03b5i \u2264 \u03b5m\u220621 + 2\u03c4\n\u03b2(1\u2212 \u03b5)\u01ebprior.\nBy the \u03b2-smoothness of L(w), it implies that\nL(w\u0302m+1)\u2212 L(w\u2217) \u2264 \u03b2\n2 \u2016w\u0302m+1 \u2212w\u2217\u20162 \u2264\n\u03b2 2 \u03b5m\u220621 + \u03c4 1\u2212 \u03b5\u01ebprior,\n\u2264 \u03b2R 2\n2 \u03b5m +\n\u03c4\n1\u2212 \u03b5\u01ebprior,\nwhere the last inequality follows from \u22061 \u2264 R. The bound stated in the theorem follows the assumption that L(w\u2217) = \u01ebopt \u2264 \u01ebprior."}, {"heading": "5.1 Proof of Theorem 7", "text": "To bound \u2016w\u0302k+1\u2212w\u2217\u2016 in terms of \u2206k, we start with the standard analysis of online learning. In particular, from the strong convexity assumption of L(w) and updating rule (6) we have,\nL(wtk)\u2212 L(w\u2217) \u2264 \u3008\u2207L(wtk),wtk \u2212w\u2217\u3009 \u2212 \u03b1\n2 \u2016wtk \u2212w\u2217\u20162\n= \u3008vtk,wtk \u2212w\u2217\u3009+ \u3008\u2207L(wtk)\u2212 vtk,wtk \u2212w\u2217\u3009 \u2212 \u03b1\n2 \u2016wt \u2212w\u2217\u20162\n\u2264 \u2016w t+1 k \u2212w\u2217\u20162 \u2212 \u2016wt+1k \u2212w\u2217\u20162\n2\u03b7 +\n\u03b7d\n2 \u03b32k\n+ \u3008\u2207L(wtk)\u2212 vtk,wtk \u2212w\u2217\u3009\ufe38 \ufe37\ufe37 \ufe38 ,vt\nk\n\u2212\u03b1 2 \u2016wt \u2212w\u2217\u20162, (10)\nwhere the last step follows from \u2016vtk\u2016 \u2264 \u03b3k \u221a d. By adding all the inequalities of (10) at stage k, we have\nT1\u2211\nt=1\nL(wtk)\u2212 L(w\u2217) \u2264 \u2016w\u0302k \u2212w\u2217\u20162\n2\u03b7 +\nd\u03b7\n2 \u03b32kT1 +\nT1\u2211\nt=1\nvtk \u2212 \u03b1\n2\nT1\u2211\nt=1\n\u2016wt \u2212w\u2217\u20162\n\u2264 \u2206 2 k\n2\u03b7 +\nd\u03b7\n2 \u03b32kT1 + Vk \u2212\n\u03b1 2 Wk, (11)\nwhere Vk and Wk are defined as Vk = \u2211T1 t=1 v t k and Wk = \u2211T1 t=1 \u2016wtk \u2212w\u2217\u20162, respectively. In order to bound Vk, using the fact that \u2207L(wtk) = Et[g\u0302tk], we rewrite Vk as\nVk =\nT1\u2211\nt=1 \u3008\u2212vtk + Et[vtk],wtk \u2212w\u2217\u3009\ufe38 \ufe37\ufe37 \ufe38 ,dt\nk\n+\nT1\u2211\nt=1\n\u3008Et [ g\u0302tk ] \u2212 Et[vtk],wtk \u2212w\u2217\u3009\ufe38 \ufe37\ufe37 \ufe38\n,et k\n= Dk + Ek,\nwhere Dk = \u2211T1 t=1 d t k and Ek = \u2211T1 t=1 e t k which represent the variance and bias of the clipped gradient vtk, respectively. We now turn to separately upper bound each term. The following lemma bounds the variance term Dk using the Bernstein inequality for martingale. Its proof can be found in Appendix A.\nLemma 1. For any L > 0 and \u00b5 > 0, we have\nPr ( Wk \u2264\n\u01ebpriorT1 2\u00b5\u03b2\n) + Pr ( Dk \u2264 1\nL Wk +\n( L\u03b32kd+ \u03b3k\u2206k \u221a d ) ln s\n\u03b4\n) \u2265 1\u2212 \u03b4\nwhere s is given by\ns = \u2308 log2 8\u03b2\u00b5R2\n\u01ebprior\n\u2309 .\nThe following lemma bounds Ek using the self-bounding property of smooth functions and the proof is deferred to Appendix B.\nLemma 2.\nEk \u2264 4T1 \u03be \u01ebopt + 4\u03b2 \u03be Wk \u2264 4T1 \u03be \u01ebprior + 4\u03b2 \u03be Wk.\nNote that without the knowledge of \u01ebprior, we have to bound \u01ebopt by \u2126(1), resulting in a very loose bound for the bias term Ek. It is knowledge of the target expected risk \u01ebprior that allows us to come up with a significantly more accurate bound for the bias term Ek, which consequentially leads to a geometric convergence rate.\nWe now proceed to bound \u2211T1\nt=1 L(wtk) \u2212 L(w\u2217) using the two bounds in Lemma 1 and 2. To this end, based on the result obtained in Lemma 1, we consider two scenarios. In the first scenario, we assume\nWk \u2264 \u01ebpriorT1 2\u00b5\u03b2\n(12)\nIn this case, we have\nT1\u2211\nt=1\nL(wtk)\u2212 L(w\u2217) \u2264 \u03b2\n2 Wk \u2264 \u01ebprior 2\u00b5 T1. (13)\nIn the second scenario, we assume\nDk \u2264 1\nL WT +\n( L\u03b32kd+ \u03b3k\u2206k \u221a d ) ln s\n\u03b4 . (14)\nIn this case, by combining the bounds for Dk and Ek and setting L = \u03be 4\u03b2 , we have\nVk \u2264 8\u03b2\n\u03be Wk +\n( \u03bed\n4\u03b2 \u03b32k + \u03b3k\u2206k\n\u221a d ) ln s\n\u03b4 + 4T1 \u03be \u01ebprior\n= 8\u03b2\n\u03be Wk +\n( \u03be3\u03b2d+ 2\u03be\u03b2 \u221a d ) \u22062k ln s\n\u03b4 + 4T1 \u03be \u01ebprior,\nwhere the last equality follows from the fact \u03b3k = 2\u03be\u03b2\u2206k. If we choose \u03be such that 8\u03b2 \u03be \u2264 \u03b12 or \u03be \u2265 16\u03b2\n\u03b1 > 1 holds, we get\nVk \u2264 \u03b1\n2 Wk +\n( \u03be3\u03b2d+ 2\u03be\u03b2 \u221a d ) \u22062k ln s\n\u03b4 + 4T1 \u03be \u01ebprior\nSubstituting the above bound for Vk into the inequality of (11), we have\nT1\u2211\nt=1\nL(wtk)\u2212 L(w\u2217) \u2264 \u22062k 2\u03b7 + \u03b7 2 \u03b32kT1 +\n( \u03be3\u03b2d+ 2\u03be\u03b2 \u221a d ) \u22062k ln s\n\u03b4 + 4T1 \u03be \u01ebprior\nBy choosing \u03b7 as \u03b7 = \u2206k \u03b3k \u221a T1 = 1 2\u03be\u03b2 \u221a T1 , we have\nL(w\u0302k+1)\u2212 L(w\u2217) \u2264 1\nT1\n( 2\u03be\u03b2 \u221a T1 + [ \u03be3\u03b2d+ 2\u03be\u03b2 \u221a d ] ln s\n\u03b4\n) \u22062k + 4\n\u03be \u01ebprior. (15)\nBy combining the bounds in (13) and (15), under the assumption that at least one of the two conditions in (12) and (14) is true, by setting \u00b5 = B/8, we have\nL(w\u0302k+1)\u2212 L(w\u2217) \u2264 1\nT1\n( 2\u03be\u03b2 \u221a T1 + [ \u03be3\u03b2d+ 2\u03be\u03b2 \u221a d ] ln s\n\u03b4\n) \u22062k + 4\n\u03be \u01ebprior,\nimplying\n\u2016w\u0302k+1 \u2212w\u2217\u2016 \u2264 2\n\u03b1T1\n( 2\u03be\u03b2 \u221a T1 + [ \u03be3\u03b2d+ 2\u03be\u03b2 \u221a d ] ln s\n\u03b4\n) \u22062k + 8\n\u03b1\u03be \u01ebprior.\nWe complete the proof by using Lemma 1, which states that the probability for either of the two conditions hold is no less than 1\u2212 \u03b4."}, {"heading": "6 Conclusions", "text": "In this paper, we have studied the sample complexity of passive learning when the target expected risk is given to the learner as prior knowledge. The crucial fact about target risk assumption is that, it can be fully exploited by the learning algorithm and stands in contrast to most common types of prior knowledges that usually enter into the generalization bounds and are often perceived as a rather crude way to incorporate such assumptions. We showed that by explicitly employing the target risk \u01ebprior in a properly designed stochastic optimization algorithm, it is possible to attain the given target risk \u01ebprior with a logarithmic\nsample complexity log (\n1 \u01ebprior\n) , under the assumption that the loss function is both strongly\nconvex and smooth. There are various directions for future research. The current study is restricted to the parametric setting where the hypothesis space is of finite dimension. It would be interesting to see how to achieve a logarithmic sample complexity in a non-parametric setting where hypotheses lie in a functional space of infinite dimension. Evidently, it is impossible to extend the current algorithm for the non-parametric setting; therefore additional analysis tools are needed to address the challenge of infinite dimension arising from the non-parametric setting. It is also an interesting problem to relate target risk assumption we made here to the low noise margin condition which is often made in active learning for binary classification since both settings appear to share the same sample complexity. However it is currently unclear how to derive a connection between these two settings. We believe this issue is worthy of further exploration and leave it as an open problem."}, {"heading": "A Proof of Lemma 1", "text": "The proof is based on the Bernstein inequality for martingales (see, e.g., [6]).\nLemma 3. (Bernstein inequality for martingales). Let X1, . . . , Xn be a bounded martingale difference sequence with respect to the filtration F = (Fi)1\u2264i\u2264n and with \u2016Xi\u2016 \u2264 M . Let Si = \u2211i j=1 Xj be the associated martingale. Denote the sum of the conditional variances by\n\u03a32n =\nn\u2211\nt=1\nE [ X2t |Ft\u22121 ]\nThen for all constants \u03ba, \u03bd > 0,\nPr [ max\ni=1,...,n Si > \u03c1 and \u03a3\n2 n \u2264 \u03bd\n] \u2264 exp ( \u2212 \u03c1 2\n2(\u03bd +M\u03c1/3)\n)\nand therefore,\nPr [ max\ni=1,...,n Si >\n\u221a 2\u03bd\u03c1+\n\u221a 2\n3 M\u03c1 and \u03a32n \u2264 \u03bd\n] \u2264 e\u2212\u03c1.\nof Lemma 1. Define martingale difference dtk = \u3008wtk \u2212w\u2217,Et[vtk]\u2212 vtk\u3009 and martingale Dk =\u2211T1 t=1 d t k. Let \u03a3 2 T denote the conditional variance as\n\u03a32T =\nT1\u2211\nt=1\nEt [ (dtk) 2 ] \u2264 T1\u2211\nt=1\nEt [\u2225\u2225Et[vtk]\u2212 vtk \u2225\u22252 ] \u2016wtk \u2212w\u2217\u20162\n\u2264 T\u2211\nt=1\nd\u03b32k\u2016wtk \u2212w\u20162 = d\u03b32kWk,\nwhich follows from the Cauchy\u2019s Inequality and the definition of clipping. Define M = max\nt |dtk| \u2264 2\n\u221a d\u03b3k\u2206k. To prove the inequality in Lemma 1, we follow the idea of peeling\nprocess [16]. Since Wk \u2264 4R2T1, we have Pr ( Dk \u2265 2\u03b3k \u221a Wkd\u03c1+ \u221a 2M\u03c1/3 )\n= Pr ( Dk \u2265 2\u03b3k \u221a Wkd\u03c1+ \u221a 2M\u03c1/3,Wk \u2264 4R2T1 ) = Pr ( Dk \u2265 2\u03b3k \u221a Wkd\u03c1+ \u221a 2M\u03c1/3,\u03a32T \u2264 \u03b32kdWk,Wk \u2264 4R2T1 ) \u2264 Pr ( Dk \u2265 2\u03b3k \u221a Wkd\u03c1+ \u221a 2M\u03c1/3,\u03a32T \u2264 \u03b32kdWk,Wk \u2264 \u01ebpriorT1/(2\u03b2\u00b5) )\n+\ns\u2211\ni=1\nPr ( Dk \u2265 2\u03b3k \u221a Wkd\u03c1+ \u221a 2M\u03c1/3,\u03a32T \u2264 \u03b32kdWk, \u01ebprior2 i\u22121T1\n2\u03b2\u00b5 < Wk \u2264\n\u01ebprior2 iT1\n2\u03b2\u00b5\n)\n\u2264 Pr ( Wk \u2264\n\u01ebpriorT1 2\u03b2\u00b5\n) + s\u2211\ni=1\nPr  Dk \u2265 \u221a \u01ebprior2i+1T1\u03b32kd\n2\u03b2\u00b5 \u03c1+\n\u221a 2\n3 M\u03c1,\u03a32T \u2264\n\u01ebprior2 iT1\u03b3 2 kd\n2\u03b2\u00b5\n \n\u2264 Pr ( Wk \u2264\n\u01ebpriorT1 2\u03b2\u00b5\n) + se\u2212\u03c1,\nwhere s is given by\ns = \u2308 log2 8\u03b2\u00b5R2\n\u01ebprior\n\u2309 .\nThe last step follows the Bernstein inequality for martingales. We complete the proof by setting \u03c1 = ln(s/\u03b4) and using the fact that\n2\u03b3k \u221a Wk\u03c1d \u2264 1\nL Wk + \u03b3\n2 k\u03c1dL."}, {"heading": "B Proof of Lemma 2", "text": "To bound Ek, we need the following two lemmas. The first lemma bounds the deviation of the expected value of a clipped random variable from the original variable, in terms of its variance (Lemma A.2 from [13]).\nLemma 4. Let X be a random variable, let X\u0303 = clip(X,C) and assume that |E[X ]| \u2264 C/2 for some C > 0. Then\n|E[X\u0303 ]\u2212 E[X ]| \u2264 2 C |Var[X]|\nAnother key observation used for bounding Ek is the fact that for any non-negative \u03b2smooth convex function, we have the following self-bounding property. We note that this self-bounding property has been used in [27] to get better (optimistic) rates of convergence for non-negative smooth losses. Lemma 5. For any \u03b2-smooth non-negative function f : R \u2192 R, we have |f \u2032(w)| \u2264 \u221a 4\u03b2f(w)\nAs a simple proof, first from the smoothness assumption, by setting w1 = w2 \u2212 1\u03b2 f \u2032(w2) in (1) and rearranging the terms we obtain f(w2)\u2212 f(w1) \u2265 12\u03b2 |f \u2032(w2)|2. On the other hand, from the convexity of loss function we have f(w1) \u2265 f \u2032(w2) + \u3008f \u2032(w1), w1 \u2212 w2\u3009. Combining these inequalities and considering the fact that the function is non-negative gives the desired inequality.\nof Lemma 2. To apply the above lemmas, we write etk as\netk =\nd\u2211\ni=1\nEt [ \u2113\u2032(\u3008wtk,xtk\u3009, yt)[xtk]i \u2212 clip ( \u03b3k, \u2113 \u2032(\u3008wtk,xtk\u3009, yt)[xtk]i )] [wtk \u2212w\u2217]i\nIn order to apply Lemma 4, we check if the following condition holds\n\u03b3k \u2265 2 \u2223\u2223Et [ \u2113\u2032 ( \u3008wtk,xtk\u3009, yt ) [xtk]i ]\u2223\u2223 (16)\nSince \u2223\u2223Et [ \u2113\u2032 ( \u3008wtk,xtk\u3009, yt ) [xtk]i\n]\u2223\u2223 \u2264 \u2223\u2223Et [{ \u2113\u2032 ( \u3008wtk,xtk\u3009, yt ) \u2212 \u2113\u2032 ( \u3008w\u2217,xtk\u3009, yt )} [xtk]i ]\u2223\u2223+ \u2223\u2223Et [ \u2113\u2032 ( \u3008w\u2217,xtk\u3009, yt ) [xtk]i\n]\u2223\u2223 \u2264 \u03b2\u2016wtk \u2212w\u2217\u2016 \u2264 \u03b2\u2206k\nwhere the last inequality follows from Et [\u2113 \u2032 (\u3008w\u2217,xtk\u3009, yt) [xtk]i] = 0 since w\u2217 is the minimizer of L(w), we thus have\n\u03b3k = 2\u03be\u03b2\u2206k \u2265 2\u03b2\u2206k \u2265 2 \u2223\u2223Et [ \u2113\u2032 ( \u3008wtk,xtk\u3009, yt ) [xtk]i ]\u2223\u2223\nwhere \u03be \u2265 1, implying that the condition in (16) holds. Thus, using Lemma 4, we have\netk \u2264 d\u2211\ni=1\n\u2223\u2223[wtk \u2212w\u2217]i \u2223\u2223 1 \u03b3k Et [( \u2113\u2032(\u3008wtk,xtk\u3009, yt)[xtk]i )2]\n\u2264 2\u2016w t k \u2212w\u2217\u2016\u221e \u03b3k Et\n[( \u2113\u2032(\u3008wtk,xtk\u3009, yt) )2]\nUsing Lemma 5 to upper bound the right hand side, we further simplify the above bound for etk as\netk \u2264 8\u03b2\u2016wtk \u2212w\u2217\u2016\u221e\n\u03b3k Et\n[ \u2113 ( \u3008wtk,xtk\u3009, yt )]\n= 8\u03b2\u2016wtk \u2212w\u2217\u2016\u221e\n\u03b3k L(wtk)\n\u2264 8\u03b2\u2206k \u03b3k L(wtk) = 4\n\u03be L(wtk)\nwhere the second inequality follows from \u2016wtk \u2212 w\u2217\u2016\u221e \u2264 \u2016wtk \u2212 w\u2217\u2016 \u2264 \u2206k. Therefore we obtain\nEk =\nT1\u2211\nt=1\netk \u2264 4\n\u03be\nT1\u2211\nt=1\nL(wtk) = 4\n\u03be\nT1\u2211\nt=1\nL(w\u2217) + 4\n\u03be\nT1\u2211\nt=1\nL(wtk)\u2212 L(w\u2217)\n\u2264 4T1 \u03be L(w\u2217) + 4\u03b2 \u03be\nT1\u2211\nt=1\n\u2016wtk \u2212w\u2217\u20162\n= 4T1 \u03be L(w\u2217) + 4\u03b2 \u03be Wk,\nwhere the second inequality follows from the smoothness assumption of L(w)."}], "references": [{"title": "Neural network learning: Theoretical foundations", "author": ["M. Anthony", "P.L. Bartlett"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "The true sample complexity of active learning", "author": ["Maria-Florina Balcan", "Steve Hanneke", "Jennifer Wortman Vaughan"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Local rademacher complexities", "author": ["P.L. Bartlett", "O. Bousquet", "S. Mendelson"], "venue": "The Annals of Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Agnostic online learning", "author": ["Shai Ben-David", "David Pal", "Shai Shalev-Shwartz"], "venue": "In COLT,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Learnability and the vapnik-chervonenkis dimension", "author": ["Anselm Blumer", "A. Ehrenfeucht", "David Haussler", "Manfred K. Warmuth"], "venue": "J. ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Randomized smoothing for stochastic optimization", "author": ["John C. Duchi", "Peter L. Bartlett", "Martin J. Wainwright"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "A general lower bound on the number of examples needed for learning", "author": ["A. Ehrenfeucht", "D. Haussler", "M. Kearns", "L. Valiant"], "venue": "Information and Computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Theoretical Foundations of Active Learning", "author": ["Steve Hanneke"], "venue": "PhD thesis,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Adam Kalai", "Satyen Kale", "Amit Agarwal"], "venue": "In COLT,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Optimal algorithms for ridge and lasso regression with partially observed", "author": ["Elad Hazan", "Tomer Koren"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Primal-dual subgradient methods for minimizing uniformly convex functions", "author": ["Anatoli Iouditski", "Yuri Nesterov"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "author": ["Sham M. Kakade", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems", "author": ["Vladimir Koltchinskii"], "venue": "Lecture Notes in mathematics. Springer,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "The importance of convexity in learning with squared loss", "author": ["Wee Sun Lee", "Peter L. Bartlett", "Robert C. Williamson"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1980}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Problem complexity and method efficiency in optimization", "author": ["A.S. Nemirovsky", "D.B. Yudin"], "venue": "Wiley Interscience Series in Discrete Mathematics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1983}, {"title": "Introductory Lectures on Convex Optimization: A Basic Course", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In ICML,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Online learning: Random averages, combinatorial parameters, and learnability", "author": ["Alexander Rakhlin", "Karthik Sridharan", "Ambuj Tewari"], "venue": "CoRR, abs/1006.1138,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Optimal stochastic convex optimization through the lens of active learning", "author": ["Aaditya Ramdas", "Aarti Singh"], "venue": "In ICML,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Learnability and stability in the general learning setting", "author": ["S. Shalev-Shwartz", "O. Shamir", "K. Sridharan", "N. Srebro"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Stochastic convex optimization", "author": ["S. Shalev-Shwartz", "O. Shamir", "K. Sridharan", "N. Srebro"], "venue": "COLT,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Learnability, stability and uniform convergence", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Smoothness, low noise and fast rates", "author": ["Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In NIPS,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Learning from an optimization viewpoint", "author": ["Karthik Sridharan"], "venue": "PhD Thesis,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Fast rates for regularized objectives", "author": ["Karthik Sridharan", "Shai Shalev-Shwartz", "Nathan Srebro"], "venue": "In NIPS,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V.N. Vapnik", "A.Y. Chervonenkis"], "venue": "Theory of Probability and Its Applications,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1971}], "referenceMentions": [{"referenceID": 8, "context": "Sample complexity of passive learning is well established and goes back to early works in the learning theory where the lower bounds \u03a9 ( 1 \u01eb (log 1 \u01eb + log 1 \u03b4 ) ) and \u03a9 ( 1 \u01eb2 (log 1 \u01eb + log 1 \u03b4 ) ) were obtained in classic PAC and general agnostic PAC settings, respectively [9, 5, 1].", "startOffset": 277, "endOffset": 286}, {"referenceID": 4, "context": "Sample complexity of passive learning is well established and goes back to early works in the learning theory where the lower bounds \u03a9 ( 1 \u01eb (log 1 \u01eb + log 1 \u03b4 ) ) and \u03a9 ( 1 \u01eb2 (log 1 \u01eb + log 1 \u03b4 ) ) were obtained in classic PAC and general agnostic PAC settings, respectively [9, 5, 1].", "startOffset": 277, "endOffset": 286}, {"referenceID": 0, "context": "Sample complexity of passive learning is well established and goes back to early works in the learning theory where the lower bounds \u03a9 ( 1 \u01eb (log 1 \u01eb + log 1 \u03b4 ) ) and \u03a9 ( 1 \u01eb2 (log 1 \u01eb + log 1 \u03b4 ) ) were obtained in classic PAC and general agnostic PAC settings, respectively [9, 5, 1].", "startOffset": 277, "endOffset": 286}, {"referenceID": 16, "context": "In [17] fast rates obtained for squared loss, exploiting the strong convexity of this loss function, which only holds under pseudo-dimensionality assumption.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "With the recent development in online strongly convex optimization [11], fast rates approaching O( \u01eb log 1 \u03b4 ) for convex Lipschitz strongly convex loss functions has been obtained in [29, 15].", "startOffset": 67, "endOffset": 71}, {"referenceID": 28, "context": "With the recent development in online strongly convex optimization [11], fast rates approaching O( \u01eb log 1 \u03b4 ) for convex Lipschitz strongly convex loss functions has been obtained in [29, 15].", "startOffset": 184, "endOffset": 192}, {"referenceID": 14, "context": "With the recent development in online strongly convex optimization [11], fast rates approaching O( \u01eb log 1 \u03b4 ) for convex Lipschitz strongly convex loss functions has been obtained in [29, 15].", "startOffset": 184, "endOffset": 192}, {"referenceID": 26, "context": "For smooth non-negative loss functions, [27] improved the sample complexity to optimistic rates", "startOffset": 40, "endOffset": 44}, {"referenceID": 2, "context": "for non-parametric learning using the notion of local Rademacher complexity [3], where \u01ebopt is the optimal risk.", "startOffset": 76, "endOffset": 79}, {"referenceID": 26, "context": "We note that our work does not contradict the lower bound in [27] because a feasible target risk \u01ebprior is given in our learning setup and is fully exploited by the proposed algorithm.", "startOffset": 61, "endOffset": 65}, {"referenceID": 9, "context": "We also note that although the logarithmic sample complexity is known for active learning [10, 2], we are unaware of any existing passive learning algorithm that is able to achieve a logarithmic sample complexity by incorporating any kind of prior knowledge.", "startOffset": 90, "endOffset": 97}, {"referenceID": 1, "context": "We also note that although the logarithmic sample complexity is known for active learning [10, 2], we are unaware of any existing passive learning algorithm that is able to achieve a logarithmic sample complexity by incorporating any kind of prior knowledge.", "startOffset": 90, "endOffset": 97}, {"referenceID": 27, "context": "In [28, 26], the authors presented learning problems that are learnable by stochastic convex optimization but not by empirical risk minimization (ERM).", "startOffset": 3, "endOffset": 11}, {"referenceID": 25, "context": "In [28, 26], the authors presented learning problems that are learnable by stochastic convex optimization but not by empirical risk minimization (ERM).", "startOffset": 3, "endOffset": 11}, {"referenceID": 22, "context": "Furthermore, it is worth noting that in [23, 28, 22, 4], the authors explored the connection between online optimization and statistical learning in the opposite direction.", "startOffset": 40, "endOffset": 55}, {"referenceID": 27, "context": "Furthermore, it is worth noting that in [23, 28, 22, 4], the authors explored the connection between online optimization and statistical learning in the opposite direction.", "startOffset": 40, "endOffset": 55}, {"referenceID": 21, "context": "Furthermore, it is worth noting that in [23, 28, 22, 4], the authors explored the connection between online optimization and statistical learning in the opposite direction.", "startOffset": 40, "endOffset": 55}, {"referenceID": 3, "context": "Furthermore, it is worth noting that in [23, 28, 22, 4], the authors explored the connection between online optimization and statistical learning in the opposite direction.", "startOffset": 40, "endOffset": 55}, {"referenceID": 13, "context": "Online and Stochastic Optimization The proposed algorithm is closely related to the recent works that stated O(1/n) is the optimal convergence rate for stochastic optimization when the objective function is strongly convex [14, 12, 21].", "startOffset": 223, "endOffset": 235}, {"referenceID": 11, "context": "Online and Stochastic Optimization The proposed algorithm is closely related to the recent works that stated O(1/n) is the optimal convergence rate for stochastic optimization when the objective function is strongly convex [14, 12, 21].", "startOffset": 223, "endOffset": 235}, {"referenceID": 20, "context": "Online and Stochastic Optimization The proposed algorithm is closely related to the recent works that stated O(1/n) is the optimal convergence rate for stochastic optimization when the objective function is strongly convex [14, 12, 21].", "startOffset": 223, "endOffset": 235}, {"referenceID": 11, "context": "Similar to the previous argument, our result does not contradict the lower bound given in [12] because of the knowledge of a feasible optimization error.", "startOffset": 90, "endOffset": 94}, {"referenceID": 11, "context": "Moreover, in contrast to the multistage algorithm in [12] where the size of stages increases exponentially, in our algorithm, the size of each stage is fixed to be a constant.", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "In particular, we assume that the loss function is strongly convex and smooth [20].", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "t a norm \u2016 \u00b7 \u20162, if there exists a constant \u03b1 > 0 (often called the modulus of strong convexity) such that, for any \u03bb \u2208 [0, 1] and for all w1,w2 \u2208 H, it holds that l(\u03bbw1 + (1\u2212 \u03bb)w2) \u2264 \u03b1l(w1) + (1 \u2212 \u03bb)l(w2)\u2212 1 2 \u03bb(1\u2212 \u03bb)\u03b1\u2016w1 \u2212w2\u2016.", "startOffset": 120, "endOffset": 126}, {"referenceID": 29, "context": "Most existing learning algorithms follow the framework of empirical risk minimizer (ERM) or regularized ERM, which was developed to great extent by Vapnik and Chervonenkis [30].", "startOffset": 172, "endOffset": 176}, {"referenceID": 24, "context": "To motivate stochastic optimization as an alternative to the ERM method, [25, 24] challenged the ERM method and showed that there is a real gap between learnability and uniform convergence by investigating non-trivial problems where no uniform convergence holds, but they are still learnable using Stochastic Gradient Descent (SGD) algorithm [18].", "startOffset": 73, "endOffset": 81}, {"referenceID": 23, "context": "To motivate stochastic optimization as an alternative to the ERM method, [25, 24] challenged the ERM method and showed that there is a real gap between learnability and uniform convergence by investigating non-trivial problems where no uniform convergence holds, but they are still learnable using Stochastic Gradient Descent (SGD) algorithm [18].", "startOffset": 73, "endOffset": 81}, {"referenceID": 17, "context": "To motivate stochastic optimization as an alternative to the ERM method, [25, 24] challenged the ERM method and showed that there is a real gap between learnability and uniform convergence by investigating non-trivial problems where no uniform convergence holds, but they are still learnable using Stochastic Gradient Descent (SGD) algorithm [18].", "startOffset": 342, "endOffset": 346}, {"referenceID": 25, "context": "These results uncovered an important relationship between learnability and stability, and showed that stability together with approximate empirical risk minimization, assures learnability [26].", "startOffset": 188, "endOffset": 192}, {"referenceID": 19, "context": "To capture the efficiency of optimization procedures in a general sense, one can use oracle complexity of the algorithm which, roughly speaking, is the minimum number of calls to any oracle needed by any method to achieve desired accuracy [20].", "startOffset": 239, "endOffset": 243}, {"referenceID": 18, "context": "The following theorem states a lower bound on the sample complexity of stochastic optimization algorithms [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "We note that the idea of mini-batch [7, 8], although it reduces the variance in stochastic gradients, does not reduce the oracle complexity.", "startOffset": 36, "endOffset": 42}, {"referenceID": 7, "context": "We note that the idea of mini-batch [7, 8], although it reduces the variance in stochastic gradients, does not reduce the oracle complexity.", "startOffset": 36, "endOffset": 42}, {"referenceID": 12, "context": "Instead of using the gradient directly, following [13], a clipped version of the gradient, denoted by vtk = clip (\u03b3k, \u011d t k), will be used for updating the solution.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "This is in contrast to the epoch gradient algorithm [12] which divides m iterations into exponentially increasing epochs, and runs SGD with averaging on each epoch.", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": ", [6]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 15, "context": "To prove the inequality in Lemma 1, we follow the idea of peeling process [16].", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "2 from [13]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 26, "context": "We note that this self-bounding property has been used in [27] to get better (optimistic) rates of convergence for non-negative smooth losses.", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "In this paper we consider learning in passive setting but with a slight modification. We assume that the target expected loss, also referred to as target risk, is provided in advance for learner as prior knowledge. Unlike most studies in the learning theory that only incorporate the prior knowledge into the generalization bounds, we are able to explicitly utilize the target risk in the learning process. Our analysis reveals a surprising result on the sample complexity of learning: by exploiting the target risk in the learning algorithm, we show that when the loss function is both strongly convex and smooth, the sample complexity reduces to O(log ( 1 \u01eb ) ), an exponential improvement compared to the sample complexity O( 1 \u01eb ) for learning with strongly convex loss functions. Furthermore, our proof is constructive and is based on a computationally efficient stochastic optimization algorithm for such settings which demonstrate that the proposed algorithm is practically useful.", "creator": "LaTeX with hyperref package"}}}