{"id": "1302.2176", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2013", "title": "Minimax Optimal Algorithms for Unconstrained Linear Optimization", "abstract": "we design and analyze minimax - optimal algorithms for online linear optimization games where the player's choice is unconstrained. the player strives to minimize judgment regret, the price difference between his loss and the loss of a post - hoc benchmark strategy. the standard benchmark is the loss of the best strategy chosen from a bounded comparator set. when the the comparison mapping set and the adversary's gradients satisfy proper l _ infinity bounds, we give forth the characteristic value constraint of the game in closed form and and prove it naturally approaches euclidean sqrt ( 2t / pi ) as is t - & gt ; infinity.", "histories": [["v1", "Fri, 8 Feb 2013 23:16:04 GMT  (17kb)", "http://arxiv.org/abs/1302.2176v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["h brendan mcmahan"], "accepted": true, "id": "1302.2176"}, "pdf": {"name": "1302.2176.pdf", "metadata": {"source": "CRF", "title": "Minimax Optimal Algorithms for Unconstrained Linear Optimization", "authors": ["H. Brendan McMahan"], "emails": ["mcmahan@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 2.\n21 76\nv1 [\ncs .L\nG ]\n8 F\neb 2\n\u221a\n2T/\u03c0 as T \u2192 \u221e. Interesting algorithms result when we consider soft constraints on the comparator, rather than restricting it to a bounded set. As a warmup, we analyze the game with a quadratic penalty. The value of this game is exactly T/2, and this value is achieved by perhaps the simplest online algorithm of all: unprojected gradient descent with a constant learning rate. We then derive a minimax-optimal algorithm for a much softer penalty function. This algorithm achieves good bounds under the standard notion of regret for any comparator point, without needing to specify the comparator set in advance. The value of this game converges to \u221a e as T \u2192 \u221e; we give a closed-form for the exact value as a function of T . The resulting algorithm is natural in unconstrained investment or betting scenarios, since it guarantees at worst constant loss, while allowing for exponential reward against an \u201ceasy\u201d adversary."}, {"heading": "1 Introduction", "text": "Minimax analysis has recently been shown to be a powerful tool for the construction of online learning algorithms [Rakhlin et al., 2012]. Generally, these results use bounds on the value of the game (often based on the sequential Rademacher complexity) in order to construct efficient algorithms. In this work, we show that when the learner is unconstrained, it is often possible to efficiently compute an exact minimax strategy.\nWe consider a game where on each round t = 1, . . . , T , first the learner selects xt \u2208 Rn, and then an adversary chooses gt \u2208 G \u2282 Rn, and the learner suffers loss gt \u00b7 xt. The goal of the learner is to minimize regret, that is, loss in excess of that achieved by a benchmark strategy. We define\nRegret = Loss\u2212 (Benchmark Loss) = T \u2211\nt=1\ngt \u00b7 xt \u2212 L(g1, . . . , gT ) (1)\nas the regret with respect to benchmark performance L (the L intended will be clear from context). Letting I(x \u2208 X ) = 0 for x \u2208 X and \u221e otherwise, the standard definition of regret arises from the choice\nL(g1, . . . , gT ) = inf x\u2208Rn\ng1:T \u00b7 x+ I(x \u2208 X ), (2)\nthe loss of the best strategy in a bounded convex set X (we write g1:t = \u2211t\ns=1 gs for a sum of scalars or vectors). When L depends only on the sum G \u2261 g1:T we write L(G). We will be able to interpret the alternative benchmarks L we consider as penalties \u03a8 on comparator points, so L(G) = argminxG \u00b7 x+\u03a8(x), where \u03a8(x) has replaced I(x \u2208 X ) in Eq. (2).\nWe view this interaction as a sequential zero-sum game played over T rounds, where the player strives to minimize Eq. (1), and the adversary attempts to maximize it. We study the value of this game, V T , and design minimax optimal algorithms for the player; formal definitions are given below. Some results are more naturally stated in terms of rewards rather than losses, and so we define Reward = \u2212Loss = \u2212\u2211Tt=1 gtxt.\nOutline and Summary of Results Section 2 provides motivation for the consideration of alternative benchmarks L. Section 3 then develops several theoretical tools for analyzing unconstrained games with concave benchmark functions L. Section 4 applies this theory to three particular instances; Figure 1 summarizes the results from this section. These games exhibit a strong combinatorial structure, which leads to interesting algorithms and perhaps surprising game values.\nSection 4.1 serves as a warmup, where we show that constant step-size gradient descent is in fact minimax optimal for a natural choice of L, which can be though of as replacing the hard feasible set X in Eq. (2) with a quadratic penalty function on comparator points. Section 4.2 provides results analogous to those of Abernethy et al. [2008]; we consider regret compared to the best x\u030a where \u2016x\u030a\u2016\u221e \u2264 1 against an adversary constrained to play \u2016gt\u2016\u221e \u2264 1, while Abernethy et al. considered \u2016gt\u20162 \u2264 1 and \u2016x\u030a\u20162 \u2264 1 for n \u2265 3 dimensions. Interestingly, while we prove results for the unconstrained player, we show the optimal strategy in fact always plays points from X = {x | \u2016x\u2016\u221e \u2264 1}, and so applies to the constrained case as well. Our results hold for the n = 1 case (where L2 and L\u221e coincide), showing that the value of the game approaches \u221a 2T/\u03c0 as T \u2192 \u221e, as opposed to \u221a T as one might extrapolate from the results of Abernethy. This indicates an interesting change in the geometry of the L2 game between n = 1 and n = 3. Finally, Section 4.3 gives a minimax optimal algorithm for the setting introduced by Streeter and McMahan [2012]. Following their work, our algorithm obtains standard regret at most O(R \u221a T log ((1 +R)T )) simultaneously for any comparator x\u030a with |\u030ax| = R, without needing to choose R in advance. However, we emphasize a slightly different interpretation of this setting, discussed in Section 2. It is worth noting that the regret (relative the the respective L) of these algorithms is O(T ), O( \u221a T ), and O(1), respectively, though all three are minimax algorithms.\nThe Minimax Value of the Game Given a benchmark function L, the minimax value of the game is\nV T =\n\u2329\ninf xt\u2208Rn sup gt\u2208G\n\u232aT\nt=1\n(\nT \u2211\nt=1\ngt \u00b7 xt \u2212 L(g1, . . . , gT ) )\n(3)\nwhere \u2329 infxt supgt \u232aT t=1 is a shorthand notation for infx1 supg1 . . . infxT supgT . Against a worstcase adversary, any algorithm must incur regret at least V T , and the minimax optimal algorithm will incur regret at most V T against any adversary. Since in this work we study minimax algorithms, we will often use the value of the game V T as an upper bound on Regret (as defined in Eq. (1)). Generally we will not assume our adversaries are minimax optimal.\nWe are also concerned with the conditional value of the game, Vt, given x1, . . . xt and g1, . . . gt have already been played. That is, the Regret when we fix the plays on the first t rounds, and then assume minimax optimal play for rounds t + 1 through T . However, following the approach of Rakhlin et al. [2012], we omit the terms\n\u2211t s=1 xs \u00b7 gs from Eq. (3).\nWe can view this as cost that the learner has already payed, and neither that cost nor the specific previous plays of the learner impact the value of the remaining terms in Eq. (1). Thus, we define\nVt(g1, . . . , gt) =\n\u2329\ninf xs\u2208Rn sup gs\u2208G\n\u232aT\ns=t+1\n(\nT \u2211\ns=t+1\ngs \u00b7 xs \u2212 L(g1, . . . , gT ) ) . (4)\nNote the conditional value of the game before anything has been played, V0(), is exactly V T .\nRelated Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction. The analysis of alternative notions of regret is also not new. In the expert setting, there has been much work on tracking a shifting sequence of experts rather than the single best expert; see Koolen et al. [2012] and references therein. Zinkevich [2003] considers drifting comparators in an online convex optimization framework. This notion can be expressed by an appropriate L(g1, . . . , gT ), but now the order of the gradients matters, unlike the benchmarks L considered in this work. Merhav et al. [2006] and Dekel et al. [2012] consider the stronger notion of policy regret in the online experts and bandit settings, respectively. For investing scenarios, Agarwal et al. [2006] and Hazan and Kale [2009] consider regret with respect to the best constant-rebalanced portoflio.\nMore recently, the field has seen minimax approaches to online learning. Abernethy and Warmuth [2010] give a minimax strategy for several zero-sum games against a budgeted adversary. Section 4.2 studies the online linear game of Abernethy et al. [2008] under different assumptions, and we adapt some techniques from Abernethy et al. [2009]. Rakhlin et al. [2012] takes powerful tools for non-constructive analysis of online learning problems and shows they can be used to design algorithms; our work differs in that we focus on cases where the exact minimax strategy can be computed."}, {"heading": "2 Alternative Notions of Regret", "text": "One of our contributions is showing that that interesting results can be obtained by choosing L differently than in Eq. (2); in particular, we obtain minimax optimal algorithms for the problem considered by Streeter and McMahan [2012] by analyzing an appropriate choice of L.\nOne could choose L(G) = 0, but this leads to an uninteresting game: the adversary has no long-term constrains, and so can simply pick gt to maximize gtxt for whatever xt the player selected. Thus, the player can do no better than always picking xt = 0. This is exactly the reason for studying the standard notion of regret: we do not require that we do well in absolute terms, but rather relative to the best strategy from a fixed set.\nThat is, interesting games result when the player accepts the fact that it is impossible to do well in terms of the absolute loss \u2211\nt gt \u00b7xt for all sequences g1, . . . , gT . However, the player can do better on some sequences at the expense of doing worse on others. The benchmark function L makes this notion precise: sequences for which L(g1, . . . , gT ) is large and negative are those on which the player desires good performance,1 at the expense of allowing more loss (in absolute terms) on sequences where L(g1, . . . , gT ) is large and positive. The value of the game V T tells us to what extent any online algorithm can hope to match the benchmark performance L. It follows by definition that if we add a constant k to L (making L easier to achieve), we decrease the minimax value of the game by k, without changing the minimax optimal strategy.\nWe can use these ideas to derive algorithms for a setting that is quite different from typical online convex optimization. On each round t, the world (possibly adversarial, possibly not) offers the player a betting opportunity on a binary outcome; the player can take either side of the bet. The player begins with $1, but on later rounds can wager up to whatever amount he currently has (based on previous wins and losses). The player selects an amount xt to bet, and then the world reveals whether the bet was won or lost; if the player won the bet, he receives xt dollars; otherwise, he loses xt dollars. The players net winnings are \u2212 \u2211\nt gtxt, where gt \u2208 {\u22121, 1}; the player wins the bet when sign(xt) 6= gt (thus, the player strives to minimize \u2211\nt gtxt). How should the player bet in this game? Clearly if the world is adversarial, we cannot do better than always betting xt = 0. But, we might have reason to believe the world is not fully adversarial; if we knew gt = 1 with a fixed probability p, then following a Kelly betting scheme [Kelly Jr, 1956] might be appropriate, but knowing p is often unrealistic in practice.\nIf the player is familiar with online linear optimization, he might try projected online\n1It can be useful to think about \u2212L(G) as the benchmark reward for the sequence with gradient sum G.\ngradient descent [Zinkevich, 2003] with a constant step size.2 If we restrict our bets to the feasible set [\u2212B,B], letting G = g1:T , this algorithm guarantees Regret = Loss + B|G| \u2264 2B \u221a T . Then Winnings = \u2212Loss \u2265 B|G| \u2212 2B \u221a T . Thus in the best case (when |G| = T ) the player ends up with a little less than BT ; but he can lose up to B \u221a T when G = 0. Thus, to ensure he loses no more than the $1 he has on hand, he must choose B = 1/ \u221a T . With this restriction, in the best case the player wins less than \u221a T dollars. However, the post-hoc optimal strategy would have been to bet everything every round, netting winnings of 2T . Despite the theoretical guarantees, the player certainly might feel regret at having won only\u221a T in this situation! One might also hope to use online algorithms for portfolio management, for example those of Hazan and Kale [2009] and Agarwal et al. [2006]. However, these algorithms require the assumption that you always retain at least an \u03b1 > 0 fraction of your bet, which is directly violated in our game.\nBy carefully crafting a suitable benchmark function L, we can provide the player with a more satisfying algorithm. Ideally, we would like an L that satisfies three properties: 1) there exists an algorithm where regret is bounded by a constant \u01eb (for any T ) with respect to L, 2) \u2212L(G) \u2265 0, and 3) \u2212L(G) grows exponential in |G|. Properties 1) and 2) ensure the player never loses more than \u01eb running this algorithm; by scaling the bets the algorithm suggests by 1/\u01eb, he can ensure he never loses more than his starting $1. Property 3 implies that for \u201ceasy\u201d sequences, we get exponential reward; in fact, given 1) and 2) we would like \u2212L(G) to grow as quickly as possible.\nOf course, if the adversary chooses gt uniformly at random from {\u22121, 1} each round, we expect to frequently see |G| \u2265 \u221a T , and so intuitively we will not be able to guarantee exponential winnings. This suggests the best we might hope for is a function like L(G) = \u2212 exp (\n|G|\u221a T\n)\n. In fact, in Section 4.3 we show that constant regret against such a benchmark\nfunction is possible, and we derive a minimax algorithm.\nA Comparator Set Interpretation The classic definition of regret defines L indirectly as the loss of the best strategy from a fixed class X in hindsight, Eq. (2). As this work shows, it can be advantageous to state L as an explicit function of G; however, useful intuition can be gained by interpreting L as a penalty function on comparator points x\u030a. That is, we wish to find a \u03a8 such that\nL(G) = argmin x Gx+\u03a8(x).\nFor the benchmark functions L we consider, we also derive the corresponding penalty functions \u03a8 using convex conjugates. These are summarized in our results in Figure 1.\nThe standard notion of regret correspond to a hard penalty \u03a8(x) = I(x \u2208 X ). Such a definition makes sense when the player by definition must select a strategy from some bounded set, for example a probability from the n-dimensional simplex, or a distribution on paths in a graph. For such problems, standard regret is really comparing the player\u2019s performance to that of any fixed feasible strategy chosen with knowledge of g1, . . . , gT ; by putting an equal penalty on each of them, we do not indicate any prior belief that some strategies are more likely to be optimal than others.\n2Any other algorithm that provides a bound on standard regret of O(B \u221a T ) will behave similarly.\nHowever, in contexts such as machine learning where any x \u2208 Rn corresponds to a valid model, such a hard constraint is difficult to justify; while any x \u2208 Rn is technically feasible, in order to prove regret bounds we compare to a much more restrictive set. As an alternative, in Sections 4.1 and 4.3 we propose soft penalty functions that encode the belief that points near the origin are more likely to be optimal (we can always re-center the problem to match our beliefs in this reguard), but do not rule out any x \u2208 Rn a priori."}, {"heading": "3 General Unconstrained Linear Optimization", "text": "In this section we prove a theorem that greatly simplifies the task of computing minimax values and deriving algorithms for the games we consider. We prove this result in the onedimensional case; Corollary 2 then extends the result to n-dimensions.\nTheorem 1. Consider the one-dimensional unconstrained game where the player selects xt \u2208 R and the adversary chooses gt \u2208 G = [\u22121, 1], and L is concave in each of its arguments and bounded below on GT . Then,\nV T = E gt\u223c{\u22121,1}\n[ \u2212 L(g1, . . . , gT ) ] .\nwhere the expectation is over each gt chosen independently and uniformly from {\u22121, 1} (that is, the gt are Rademacher random variables). Further, the conditional value of the game is\nVt(g1, . . . , gt) = E gt+1,...,gT\u223c{\u22121,1}\n[ \u2212 L(g1, . . . , gt, gt+1, . . . gT ) ] . (5)\nProof. We argue by backwards induction (from t = T to t = 1) on the conditional value of the game, with the induction hypothesis that\nVt(g1, . . . , gt) = E gt+1,...,gT\u223c{\u22121,1}\n[\u2212L(g1, . . . , gT )], (6)\nand further that Vt is convex in each of its arguments and bounded above on GT . The induction hypothesis holds trivially for T = t, using the assumption that L is concave and bounded below for the second part. Now, suppose the induction hypothesis holds for t. We then have (by the definition of Vt)\nVt\u22121(g1, . . . , gt\u22121) = inf xt sup gt gtxt + Vt(g1, . . . , gt\u22121, gt).\nNote Vt\u22121 must be convex in each of it\u2019s arguments, using the induction hypothesis on Vt. Let M(g, x) = gx + Vt(g1, . . . , gt\u22121, g). We would like to appeal to the minimax theorem to switch the inf and sup, but since M is convex in g (using the induction hypothesis) rather than concave, we cannot do so immediately. However, because we are choosing gt \u2208 [\u22121, 1], it follows from the convexity of M that the supremum is obtained at either \u22121 or +1. Thus, we can write\nVt\u22121(g1, . . . , gt\u22121) = inf xt sup gt\u2208[\u22121,1] M(gt, xt)\n= inf xt sup gt\u2208{\u22121,1} M(gt, xt)\n= inf xt sup pt\u2208\u2206({\u22121,1}) E gt\u223cpt [M(gt, xt)],\nwhere pt \u2208 [0, 1] is the probability the adversary chooses gt = +1 (otherwise, gt = \u22121). Now Egt\u223cpt[M(gt, xt)] is linear in both pt and xt, and so we can apply the minimax theorem (e.g., Theorem 7.1 from Cesa-Bianchi and Lugosi [2006]), which gives\nVt\u22121(g1, . . . , gt\u22121) = sup pt\u2208\u2206({\u22121,1}) inf xt E gt\u223cpt [gtxt + Vt(g1, . . . , gt\u22121, g)]\n= sup pt\u2208\u2206({\u22121,1}) inf xt E gt\u223cpt [gtxt] + E gt\u223cpt [Vt(g1, . . . , gt\u22121, gt)].\nNow, the adversary (sup player) must choose pt = 0.5 so E[gt] = 0, or otherwise the player can choose xt to drive the value to \u2212\u221e (since Vt is bounded above). Thus, the first expectation term disappears, and the choice of the player becomes irrelevant, giving\nVt\u22121(g1, . . . , gt\u22121) = E gt [Vt(g1, . . . , gt\u22121, gt)],\nwhere now the expectation is on gt drawn i.i.d. from {\u22121, 1}. Applying the induction hypothesis completes the proof, since then iterated expectation yields Eq. (6) for Vt\u22121, and boundedness is immediate.\nThe use of randomization to allow the application of the minimax theorem is similar to the technique used by Abernethy et al. [2009].\nA key insight from the proof is that an optimal adversary can always select from {\u22121, 1}. With this knowledge, we can view the game as a binary tree of height T . An algorithm for the player simply assigns a play x \u2208 R to each node, and the adversary chooses which outgoing edge to take: if the adversary chooses the left edge, the player suffers loss x, otherwise the player wins x (suffers loss -x). Finally, when leaf \u2113 is reached, the adversary pays the player some amount L(\u2113). Theorem 1 implies the value of the game is then simply the average value of \u2212L(\u2113).\nGiven Theorem 1, and the fact that the functions L of interest will generally depend only on g1:T , it will be useful to define BT to be the distribution of g1:T when each gt is drawn independently and uniformly from {\u22121, 1} (that is, the sum of T Rademacher random variables).\nTheorem 1 immediately yields bounds for games in n-dimensions where the adversary is constrained to play \u2016gt\u2016\u221e \u2264 1: Corollary 2. Consider the game where the player chooses xt \u2208 Rn, and the adversary chooses gt \u2208 [\u22121, 1]n, and the total payoff is\nT \u2211\nt=1\ngt \u00b7 xt \u2212 n \u2211\ni=1\nL(g1:T,i)\nfor a concave function L. Then, the value of the game is\nV T = n E G\u223cBT\n[ \u2212 L(G) ] ,\nFurther, the conditional value of the game is\nVt(g1, . . . , gt) =\nn \u2211\ni=1\nE Gi\u223cBT\u2212t\n[ \u2212 L(g1:t,i +Gi) ] .\nProof sketch. The proof follows by noting the constraints on both players\u2019 strategies and the value of the game fully decompose on a per-coordinate basis.\nA recipe for minimax optimal algorithms in one dimension For any function L,\nE G\u223cBT\n[L(G)] = 1\n2T\nT \u2211\ni=0\n(\nT\ni\n)\nL(2i\u2212 T ), (7)\nsince 2\u2212T ( T i )\nis the binomial probability of getting exactly i gradients of +1 over T rounds, which implies T \u2212 i gradients of \u22121, so G = i\u2212 (T \u2212 i) = 2i\u2212 T .\nSince Eq. (5) gives the minimax value of the game if both players play optimally from round t+ 1 forward, a minimax strategy for the learner on round t+ 1 must be\nxt+1 = argmin x\u2208R max g\u2208{\u22121,1}\ng \u00b7 x+ Vt+1(g1, . . . , gt, g)\n= 1\n2\n( Vt+1(g1, . . . , gt,\u22121) \u2212 Vt+1(g1, . . . , gt,+1) ) . (8)\nThe second line follows because the argmin is simply over the max of two intersecting linear functions, which we can compute in closed form as the point of intersection. Thus, if we can derive a closed form for Vt(g1, . . . , gt), we will have an efficient minimax-optimal algorithm. In the next section, we explore cases where this is possible.\nWhen L depends only on G = g1:T , we may be able to run the minimax algorithm efficiently even if Vt does not have a convenient closed form: if \u03c4 = T \u2212 t, the number of rounds remaining, is small, then we can compute Vt exactly by using the appropriate binomial probabilities (following Eq. (5) and Eq. (7)). On the other hand, if \u03c4 is large, then applying the Gaussian approximation to the binomial distribution may be sufficient."}, {"heading": "4 Deriving Minimax Optimal Algorithms", "text": "In this sections, we explore three applications of the tools from the previous section. We begin with a relatively simple but interesting example which illustrates the technique."}, {"heading": "4.1 Constant step-size gradient descent can be minimax optimal", "text": "Suppose we use a \u201csoft\u201d feasible set for the benchmark,\nL(G) = min x\nGx+ \u03c3 2 x2 = \u2212 1 2\u03c3 G2, (9)\nfor a constant \u03c3 > 0. Does a no-regret algorithm against this comparison class exist? Unfortunately, the general answer is no, as shown in the next theorem:\nTheorem 3. The value of this game is V T = EG\u223cBT\n[\n1 2\u03c3G\n2 ]\n= T2\u03c3 .\nProof. Starting from Eq. (7),\nE G\u223cBT\n[G2] = 1\n2T\nT \u2211\ni=0\n(\nT\ni\n)\n(2i\u2212 T )2 Eq. (7)\n= 1\n2T\n(\n4\nT \u2211\ni=0\n(\nT\ni\n) i2 \u2212 4T T \u2211\ni=0\n(\nT\ni\n) i+ T 2 T \u2211\ni=0\n(\nT\ni\n)\n)\nand since \u2211T\nt=0\n(\nT t\n)\n= 2T , \u2211T\nt=0\n(\nT t\n) t = T2T\u22121, \u2211T\nt=0\n(\nT t\n) t2 = (T + T 2)2T\u22122,\n= 1\n2T\n( 4(T + T 2)2T\u22122 \u2212 4T (T2T\u22121) + T 22T )\n= (T + T 2)\u2212 2T 2 + T 2 = T.\nThe result then follows from linearity of expectation.\nThis implies Reward \u2265 \u2212L(G)\u2212Regret = 12\u03c3 ( G2\u2212T ), a fact noted by Streeter and McMahan [2012, Lemma 2].\nThus, for a fixed \u03c3, we cannot have no a regret algorithm with respect to this L. However, if T is known in advance, we could choose \u03c3 = \u221a T in order to claim no-regret. But this is a bit arbitrary: if the player could pick \u03c3, and cares purely about Regret, obviously he would like to play the game where \u03c3 \u2192 \u221e, as that makes the value of the game (Regret) as small as possible. However, this choice also drives Reward to zero. If the lower-bound on reward is what matters, then the player should choose based on how he expects G2 to relate to T .\nTo derive the minimax optimal algorithm, we can compute conditional values (using similar techniques to Theorem 3),\nVt(g1, . . . , gt) = E G\u223cBT\u2212t\n[ 1\n2\u03c3 (g1:t +G)\n2 ] = 1\n2\u03c3\n(\n(g1:t) 2 + (T \u2212 t)\n)\n,\nand so following Eq. (8) the minimax-optimal algorithm must use\nxt+1 = 1\n4\u03c3\n(( (g1:t \u2212 1)2 + (T \u2212 t\u2212 1) ) \u2212 ((g1:t + 1)2 + (T \u2212 t\u2212 1)) )\n= 1\n4\u03c3 (\u22124g1:t) = \u2212\n1 \u03c3 g1:t\nThus, a minimax-optimal algorithm is simply constant-learning-rate gradient descent with learning rate 1\n\u03c3 . Note that for a fixed \u03c3, this is the optimal algorithm independent of T ; this\nis atypical, as usually the minimax optimal algorithm depends on the horizon (as we will see in the next two cases)."}, {"heading": "4.2 Optimal regret against hypercube adversaries", "text": "Abernethy et al. [2008] gives a minimax optimal algorithm when the player\u2019s xt and the comparator x\u030a are constrained to an L2 ball, and the adversary must also select gt from an L2 ball, for n \u2265 3 dimensions.3 In contrast, we consider regret compared to the best x\u030a\n3Their results are actually more general than this, allowing the constraint on \u2016gt\u20162 to vary on a per-round basis. Our work could also be extended in that manner.\nconstrained to the unit L\u221e ball, but allow the player to select any xt \u2208 Rn; our adversary is constrained to select gt from the unit L\u221e ball (the generalization to arbitrary hyper-rectangles is straightforward). Perhaps surprisingly, the optimal strategy for the player always plays from the unit L\u221e ball as well, so our results immediately apply to the case of the constrained player.\nSince we consider L\u221e constraints on both the comparator and adversary, Corollary 2 implies it is sufficient to study the one-dimensional case. We consider the standard notion of regret, taking L(G) = \u2212|G| following Eq. (2). Our main result is the following:\nTheorem 4. Consider the game between an adversary who chooses loss functions gt \u2208 [\u22121, 1], and a player who chooses xt \u2208 R. For a given sequence of plays, x1, g1, x2, g2, . . . , xT , gT , the value to the adversary is\n\u2211T t=1 gtxt\u2212|g1:T |. Then, when T is even with T = 2M , the minimax\nvalue of this game is given by\nVT = 2 \u2212T 2M T ! (T \u2212M)!M ! \u2264 \u221a 2T \u03c0 .\nFurther, as T \u2192 \u221e, VT \u2192 \u221a 2T \u03c0 .\nProof. Letting T = 2M and working from Eq. (7),\nV T = \u2212 E G\u223cBT\n[L(G)] = 2\n2T\nT \u2211\ni=0\n(\nT\ni\n)\n|i\u2212M | = 2M 2T\n(\n2M\nM\n)\n= 2\u2212T 2M T !\n(T \u2212M)!M ! , (10)\nwhere we have applied a classic formula of de Moivre [1718] for the mean absolute deviation of the binomial distribution (see also Diaconis and Zabell [1991]). Using a standard bound on the central binomial coefficient (based on Stirling\u2019s formula),\n(\n2M\nM\n)\n= 4M\u221a \u03c0M ( 1\u2212 cM M )\n(11)\nwhere 19 < cM < 1 8 for all M \u2265 1, we have\nV T \u2264 2M 1\u221a \u03c0M =\n\u221a\n2T\n\u03c0 .\nAs implied by Eq. (11), this inequality quickly becomes tight as T \u2192 \u221e.\nThe minimax algorithm (for the constrained player, too!) In order to compute the minimax algorithm, we would like a closed form for\nVt(Gt) = \u2212 E G\u03c4\u223cB\u03c4\n[ L(Gt +G \u03c4 ) ] ,\nwhere Gt = g1:t is the sum of the gradients so far, \u03c4 = T \u2212 t is the number of rounds to go, and and G\u03c4 = gt+1:T is a random variable giving the sum of the remaining gradients. Unfortunately, the structure of the binomial coefficients exploited by de Moivre and used in Eq. (10) does not apply given an arbitrary offset G\u03c4 . Nevertheless, we will be able to derive\na formula for the update that is readily computable. Writing Pr\u03c4 (b) for the probability a random draw from B\u03c4 has value b, the update of Eq. (8) becomes\nxt+1 = 1\n2\n\u03c4 \u2211\nb=\u2212\u03c4 Pr\u03c4 (b)\n( |Gt + b\u2212 1| \u2212 |Gt + b+ 1| ) .\nWhenever Gt + b \u2265 1, the difference in absolute values is \u22122, and whenever Gt + b \u2264 1, the difference is 2. When Gt + b = 0, the difference is zero. Thus,\nxt+1 = 1\n2 (Pr\u03c4 (b > \u2212G)(\u22122) + Pr\u03c4 (b < \u2212G)(2))\n= Pr\u03c4 (b < \u2212G)\u2212 Pr\u03c4 (b > \u2212G). (12)\nWhile this update does not have a closed form, it can be efficiently computed numerically.4 It follows from this expression that even though we allow the player to select xt+1 \u2208 R, the minimax optimal algorithm always selects points from [\u22121, 1]. Thus, we have the following Corollary:\nCorollary 5. Consider the game of Theorem 4, but suppose now we also constrain the player to choose xt \u2208 [\u22121, 1]. This does not change the value of the game, as the minimax algorithm for the unconstrained case always plays from [\u22121, 1] regardless.\nAbernethy et al. [2008] shows that for the linear game with n \u2265 3 where both the learner and adversary select vectors from the unit sphere, the minimax value is exactly \u221a T . Interestingly, in the n = 1 case (where L2 and L\u221e coincide), the value of the game is lower, about 0.8 \u221a T rather than \u221a T . This indicates a fundamental difference in the geometry of the n = 1 space and n \u2265 3. We conjecture the minimax value for the L2 game with n = 2 lies somewhere in between."}, {"heading": "4.3 Non-stochastic betting and No-regret for all feasible sets simultaneously", "text": "We derive a minimax optimal approach to the betting problem presented in Section 2, which also corresponds to the setting introduced by Streeter and McMahan [2012]. Again, it is sufficient to consider the one-dimensional case. In that work, the goal was to prove bounds like Regret \u2264 O(R \u221a T log ((1 +R)T )) simultaneously for any comparator x\u030a with |\u030ax| = R. Stating their Theorem 1 in terms of losses, this bound is achieved by any algorithm that guarantees\nLoss =\nT \u2211\nt=1\ngtxt \u2264 \u2212 exp ( |g1:T |\u221a\nT\n)\n+O(1). (13)\nNote that whenever |g1:T | is large compared to \u221a T the algorithm must achieve significantly negative loss (positive reward).\n4The CDF of the binomial distribution can be computed numerically using the regularized incomplete beta function, from which Pr\u03c4 (b \u2264 \u2212G) can be derived. Then, Pr\u03c4 (b = \u2212G) can be computed from the appropriate binomial coefficient, leading to both needed probabilities.\nWe initially study the game where\nL(G) = \u2212 exp ( G\u221a T )\n(14)\n(note G = g1:T \u2208 [\u2212T, T ] can be positive or negative). We prove the minimax algorithm achieves\n\u2211T t=1 gtxt \u2212 L(g1:T ) \u2264 \u221a e, implying Reward = \u2212\u2211Tt=1 gtxt \u2265 exp ( G\u221a T ) \u2212 \u221ae. Thus, this algorithm guarantees large reward whenever the gradient sum G is large and positive. In order to satisfy Eq. (13), we must also achieve large reward when G is large and negative. Since L(g1:t)+L(\u2212g1:t) \u2264 \u2212 exp (\n|g1:T |\u221a T\n)\n, this can be accomplished by running\ntwo copies of the minimax algorithm simultaneously, switching the signs of the gradients and plays of the second copy. We formalize this in Appendix A.\nInterpretation as a soft feasible set Before developing an algorithm it is worth noting an alternative characterization of this benchmark function. One can show, that for a \u2265 0,\nmin x\u2208R\u2212\n(Gx\u2212 ax log(\u2212ax) + ax) = \u2212 exp ( G\na\n)\nThus, if we take \u03a8(x) = \u2212ax log(ax) + ax+ I(x \u2264 0), we have\nmin x\u2208R\u2212\ng1:Tx+\u03a8(x) = \u2212 exp ( G\na\n)\n.\nSince this algorithm needs large Reward when G is large and positive, we might expect that the minimax optimal algorithm only plays xt \u2264 0. Another intuition for this is that the algorithm should not need to play any point x\u030a to which \u03a8 assigns an infinite penalty. This intuition is confirmed by the analysis of this \u201cone-sided\u201d algorithm:\nTheorem 6. Consider the game with benchmark L as defined in Eq. (14). The minimax value of this game is exactly\nV T =\n( 1 + exp (\n2\u221a T\n))T\n2T exp ( \u221a T ) \u2264\n\u221a e,\nand further limT\u2192\u221e V T = \u221a e. Letting \u03c4 = T \u2212 t be the number of rounds left to be played, and defining Gt = g1:t, the conditional value of the game is\nVt(Gt) = 2 \u2212\u03c4 exp\n(\nGt \u2212 \u03c4\u221a T\n)\n(\n1 + exp ( 2/ \u221a T ) )\u03c4 ,\nwhich leads to the minimax optimal algorithm5 for the player\nxt+1 = \u22122\u2212\u03c4 exp ( Gt \u2212 \u03c4 \u2212 1\u221a T )( exp ( 2\u221a T ) \u2212 1 )( exp ( 2\u221a T ) + 1 )\u03c4 \u2264 0. (15)\n5When computing the player\u2019s strategy via Eq. (15), it is numerically preferable to do the calculation in log-space, and then exponentiate to get the final play.\nProof. First, we compute the value of the game:\nV T = E G\u223cBT\n[ \u2212 L(G) ] = 2\u2212T T \u2211\ni=0\n(\nT\ni\n)\nexp\n(\n2i\u2212 T\u221a T\n)\n= 2\u2212T exp ( \u2212 \u221a T ) T \u2211\ni=0\n(\nT\ni\n)\n(\nexp ( 2/ \u221a T ) )i\n= 2\u2212T exp ( \u2212 \u221a T ) ( 1 + exp ( 2/ \u221a T ) )T ,\nwhere we have used the ordinary generating function, \u2211T\ni=0\n(\nT i\n)\nxi = (1 + x)T . Manipulating\nthe above expression for the value of the game, we arrive at V T = cosh(1/ \u221a T )T . Using the series expansion for cosh leads to the upper bound cosh(x) \u2264 exp(x2/2), from which we conclude\nVT = ( cosh ( 1/ \u221a T ) )T \u2264 exp ( 1\n2T\n)T = \u221a e.\nUsing similar techniques, we can derive the conditional value of the game, letting \u03c4 = T\u2212t be the number of rounds left to be played:\nVt(Gt) = 2 \u2212\u03c4\n\u03c4 \u2211\ni=0\n(\n\u03c4\ni\n)\nexp\n(\nGt + 2i\u2212 \u03c4\u221a T\n)\n= 2\u2212\u03c4 exp\n(\nGt \u2212 \u03c4\u221a T\n)\n(\n1 + exp ( 2/ \u221a T ) )\u03c4 .\nFollowing Eq. (8) and simplifying leads to the update of Eq. (15). It remains to show limT\u2192\u221e VT = \u221a e. Using the change of variable x = 1/ \u221a T , equivalently we have limx\u21920 cosh(x) 1\nx2 . Examining the log of this function,\nlim x\u21920\nlog ( cosh(x) 1 x2 )\n= lim x\u21920\n1\nx2 log cosh(x) = lim x\u21920 1 x2\n(\nx2\n2 \u2212 x\n4\n12 +\nx6\n45 \u2212 17x\n8\n2520 + . . .\n)\n= 1\n2 ,\nwhere we have taken the Maclaurin series of log cosh(x). Using the continuity of exp, we have\nlim x\u21920\n(\ncosh(x) 1 x2\n) = exp (\nlim x\u21920\nlog ( cosh(x) 1 x2 )) = \u221a e.\nA strong lower-bound Recall from Section 2 that as long as \u2212L(G) \u2265 0 and we get constant regret with respect to L, we can scale our bets so that we never risk losing more than a constant starting budget. This holds for any number of rounds T against any adversary. Given that constraint, we would like \u2212L(G) to grow as fast as possible, so it is natural to consider the generalizing Eq. (14) as\nL\u03b1(G) = \u2212 exp ( G\nT\u03b1\n)\nfor \u03b1 \u2208 (0, 12 ]. Following the techniques used in the preceding proof, we can show for this game\nV T\u03b1 = E G [L(G)] = 2\u2212T exp ( \u2212T 1\u2212\u03b1 ) ( 1 + exp ( 2T\u2212\u03b1 ))T = cosh ( T\u2212\u03b1 )T .\nBy taking the first term in the series for log cosh x, namely x2/2, and plugging in x = 1/T\u03b1 \u2264 1, we get a good upper bound on the value of the game:\nV T = exp ( T log cosh(T\u2212\u03b1) ) \u2264 exp ( T 1\n2T 2\u03b1\n)\n= exp\n(\n1 2 T 1\u22122\u03b1\n)\nThis implies that, for any \u03b1 < 1/2, no algorithm can provide constant loss (that is, \u2211T t=1 gtxt \u2264 k for a constant k \u2265 0) for any sequence while also guaranteeing\nReward = \u2212 T \u2211\nt=1\ngtxt = \u2126\n(\nexp\n(\nG\nT\u03b1\n))\n(16)\nfor any \u03b1 < 1/2. In fact, for \u03b1 < 1/2, no algorithm can guarantee even linear loss in the worst case while making the reward guarantee of Eq. (16)."}, {"heading": "A A Symmetric Betting Algorithm", "text": "The one-sided algorithm of Theorem 6 has\nLoss = V T + L(G) \u2264 \u2212 exp ( G\u221a T ) + \u221a e.\nIn order to do well when g1:T is large and negative, we can run a copy of the algorithm on \u2212g1, . . . ,\u2212gT , switching the signs of each xt it suggests. The combined algorithm then satisfies\nLoss \u2264 \u2212 exp ( G\u221a T ) \u2212 exp (\u2212G\u221a T ) + 2 \u221a e\n\u2264 \u2212 exp ( |G|\u221a\nT\n) + 2 \u221a e,\nand so following Eq. (13) and Theorem 1 of Streeter and McMahan [2012], we obtain the desired regret bounds. The following theorem implies the symmetric algorithm is in fact minimax optimal with respect to the combined benchmark\nLC(G) = \u2212 exp ( G\u221a T ) \u2212 exp (\u2212G\u221a T ) .\nTheorem 7. Consider two 1-D games where the adversary plays from [\u22121, 1], defined by concave functions L1 and L2 respectively. Let x 1 t and x 2 t be minimax-optimal plays for L1 and L2 respectively, given that g1, . . . gt\u22121 have been played so far in both games. Then x1 + x2 is also minimax optimal for the combined game that uses the benchmark LC(G) = L1(G) + L2(G).\nProof. First, taking \u03c4 = T \u2212 t and using Theorem 1 three times, we have\nV C(g1, . . . , gt) = \u2212 E G\u03c4\u223cB\u03c4\n[\nL1(g1:t +G \u03c4 ) + L2(g1:t +G \u03c4 ) ]\n= \u2212 E G\u03c4\u223cB\u03c4\n[ L1(g1:t +G \u03c4 ) ] \u2212 E G\u03c4\u223cB\u03c4 [ L2(g1:t +G \u03c4 ) ]\n= V 1(g1, . . . , gt) + V 2(g1, . . . , gt),\nusing linearity of expectation. Then, using Eq. (8) for each of the three games, we have\nxCt = argmin x max g gx+ VC(g1, . . . , gt\u22121, g)\n= 1\n2\n( VC(g1, . . . , gt\u22121,\u22121)\u2212 VC(g1, . . . , gt\u22121,+1) )\n= 1\n2\n( V1(g1, . . . , gt\u22121,\u22121) + V2(g1, . . . , gt\u22121,\u22121)\u2212 V1(g1, . . . , gt\u22121,1 )\u2212 V2(g1, . . . , gt\u22121,+1) )\n= x1t + x 2 t ,"}], "references": [{"title": "Repeated games against budgeted adversaries", "author": ["Jacob Abernethy", "Manfred K. Warmuth"], "venue": "In NIPS,", "citeRegEx": "Abernethy and Warmuth.,? \\Q2010\\E", "shortCiteRegEx": "Abernethy and Warmuth.", "year": 2010}, {"title": "Optimal strategies and minimax lower bounds for online convex games", "author": ["Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "A stochastic view of optimal regret through minimax duality", "author": ["Jacob Abernethy", "Alekh Agarwal", "Peter Bartlett", "Alexander Rakhlin"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2009}, {"title": "Algorithms for portfolio management based on the Newton method", "author": ["Amit Agarwal", "Elad Hazan", "Satyen Kale", "Robert E. Schapire"], "venue": "In ICML,", "citeRegEx": "Agarwal et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2006}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Online bandit learning against an adaptive adversary: from regret to policy regret", "author": ["Ofer Dekel", "Ambuj Tewari", "Raman Arora"], "venue": "In ICML,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "Closed form summation for classical distributions: Variations on a theme of de Moivre", "author": ["Persi Diaconis", "Sandy Zabell"], "venue": "Statistical Science,", "citeRegEx": "Diaconis and Zabell.,? \\Q1991\\E", "shortCiteRegEx": "Diaconis and Zabell.", "year": 1991}, {"title": "On stochastic and worst-case models for investing", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In NIPS", "citeRegEx": "Hazan and Kale.,? \\Q2009\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2009}, {"title": "A new interpretation of information rate", "author": ["J.L. Kelly Jr."], "venue": "Bell System Technical Journal,", "citeRegEx": "Jr.,? \\Q1956\\E", "shortCiteRegEx": "Jr.", "year": 1956}, {"title": "Putting bayes to sleep", "author": ["Wouter Koolen", "Dmitry Adamskiy", "Manfred Warmuth"], "venue": "In NIPS", "citeRegEx": "Koolen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Koolen et al\\.", "year": 2012}, {"title": "On sequential strategies for loss functions with memory", "author": ["N. Merhav", "E. Ordentlich", "G. Seroussi", "M.J. Weinberger"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "Merhav et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Merhav et al\\.", "year": 2006}, {"title": "Relax and randomize: From value to algorithms", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In NIPS,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2012}, {"title": "No-regret algorithms for unconstrained online convex optimization", "author": ["Matthew Streeter", "H. Brendan McMahan"], "venue": "In NIPS,", "citeRegEx": "Streeter and McMahan.,? \\Q2012\\E", "shortCiteRegEx": "Streeter and McMahan.", "year": 2012}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}, {"title": "Theorem 1 of Streeter and McMahan [2012], we obtain the desired regret bounds. The following theorem implies the symmetric algorithm is in fact minimax optimal with respect to the combined benchmark", "author": ["\u221a e"], "venue": null, "citeRegEx": "e,? \\Q2012\\E", "shortCiteRegEx": "e", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "1 Introduction Minimax analysis has recently been shown to be a powerful tool for the construction of online learning algorithms [Rakhlin et al., 2012].", "startOffset": 129, "endOffset": 151}, {"referenceID": 1, "context": "2 provides results analogous to those of Abernethy et al. [2008]; we consider regret compared to the best x\u030a where \u2016x\u030a\u2016\u221e \u2264 1 against an adversary constrained to play \u2016gt\u2016\u221e \u2264 1, while Abernethy et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 1, "context": "2 provides results analogous to those of Abernethy et al. [2008]; we consider regret compared to the best x\u030a where \u2016x\u030a\u2016\u221e \u2264 1 against an adversary constrained to play \u2016gt\u2016\u221e \u2264 1, while Abernethy et al. considered \u2016gt\u20162 \u2264 1 and \u2016x\u030a\u20162 \u2264 1 for n \u2265 3 dimensions. Interestingly, while we prove results for the unconstrained player, we show the optimal strategy in fact always plays points from X = {x | \u2016x\u2016\u221e \u2264 1}, and so applies to the constrained case as well. Our results hold for the n = 1 case (where L2 and L\u221e coincide), showing that the value of the game approaches \u221a 2T/\u03c0 as T \u2192 \u221e, as opposed to \u221a T as one might extrapolate from the results of Abernethy. This indicates an interesting change in the geometry of the L2 game between n = 1 and n = 3. Finally, Section 4.3 gives a minimax optimal algorithm for the setting introduced by Streeter and McMahan [2012]. Following their work, our algorithm obtains standard regret at most O(R \u221a T log ((1 +R)T )) simultaneously for any comparator x\u030a with |\u030ax| = R, without needing to choose R in advance.", "startOffset": 41, "endOffset": 862}, {"referenceID": 11, "context": "However, following the approach of Rakhlin et al. [2012], we omit the terms \u2211t s=1 xs \u00b7 gs from Eq.", "startOffset": 35, "endOffset": 57}, {"referenceID": 6, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction.", "startOffset": 89, "endOffset": 111}, {"referenceID": 3, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction.", "startOffset": 115, "endOffset": 146}, {"referenceID": 3, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction. The analysis of alternative notions of regret is also not new. In the expert setting, there has been much work on tracking a shifting sequence of experts rather than the single best expert; see Koolen et al. [2012] and references therein.", "startOffset": 115, "endOffset": 382}, {"referenceID": 3, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction. The analysis of alternative notions of regret is also not new. In the expert setting, there has been much work on tracking a shifting sequence of experts rather than the single best expert; see Koolen et al. [2012] and references therein. Zinkevich [2003] considers drifting comparators in an online convex optimization framework.", "startOffset": 115, "endOffset": 423}, {"referenceID": 3, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction. The analysis of alternative notions of regret is also not new. In the expert setting, there has been much work on tracking a shifting sequence of experts rather than the single best expert; see Koolen et al. [2012] and references therein. Zinkevich [2003] considers drifting comparators in an online convex optimization framework. This notion can be expressed by an appropriate L(g1, . . . , gT ), but now the order of the gradients matters, unlike the benchmarks L considered in this work. Merhav et al. [2006] and Dekel et al.", "startOffset": 115, "endOffset": 679}, {"referenceID": 3, "context": "Related Work Regret-based analysis has received extensive attention in recent years; see Shalev-Shwartz [2012] and Cesa-Bianchi and Lugosi [2006] for an introduction. The analysis of alternative notions of regret is also not new. In the expert setting, there has been much work on tracking a shifting sequence of experts rather than the single best expert; see Koolen et al. [2012] and references therein. Zinkevich [2003] considers drifting comparators in an online convex optimization framework. This notion can be expressed by an appropriate L(g1, . . . , gT ), but now the order of the gradients matters, unlike the benchmarks L considered in this work. Merhav et al. [2006] and Dekel et al. [2012] consider the stronger notion of policy regret in the online experts and bandit settings, respectively.", "startOffset": 115, "endOffset": 703}, {"referenceID": 3, "context": "For investing scenarios, Agarwal et al. [2006] and Hazan and Kale [2009] consider regret with respect to the best constant-rebalanced portoflio.", "startOffset": 25, "endOffset": 47}, {"referenceID": 3, "context": "For investing scenarios, Agarwal et al. [2006] and Hazan and Kale [2009] consider regret with respect to the best constant-rebalanced portoflio.", "startOffset": 25, "endOffset": 73}, {"referenceID": 0, "context": "Abernethy and Warmuth [2010] give a minimax strategy for several zero-sum games against a budgeted adversary.", "startOffset": 0, "endOffset": 29}, {"referenceID": 0, "context": "Abernethy and Warmuth [2010] give a minimax strategy for several zero-sum games against a budgeted adversary. Section 4.2 studies the online linear game of Abernethy et al. [2008] under different assumptions, and we adapt some techniques from Abernethy et al.", "startOffset": 0, "endOffset": 180}, {"referenceID": 0, "context": "Abernethy and Warmuth [2010] give a minimax strategy for several zero-sum games against a budgeted adversary. Section 4.2 studies the online linear game of Abernethy et al. [2008] under different assumptions, and we adapt some techniques from Abernethy et al. [2009]. Rakhlin et al.", "startOffset": 0, "endOffset": 267}, {"referenceID": 0, "context": "Abernethy and Warmuth [2010] give a minimax strategy for several zero-sum games against a budgeted adversary. Section 4.2 studies the online linear game of Abernethy et al. [2008] under different assumptions, and we adapt some techniques from Abernethy et al. [2009]. Rakhlin et al. [2012] takes powerful tools for non-constructive analysis of online learning problems and shows they can be used to design algorithms; our work differs in that we focus on cases where the exact minimax strategy can be computed.", "startOffset": 0, "endOffset": 290}, {"referenceID": 0, "context": "Abernethy and Warmuth [2010] give a minimax strategy for several zero-sum games against a budgeted adversary. Section 4.2 studies the online linear game of Abernethy et al. [2008] under different assumptions, and we adapt some techniques from Abernethy et al. [2009]. Rakhlin et al. [2012] takes powerful tools for non-constructive analysis of online learning problems and shows they can be used to design algorithms; our work differs in that we focus on cases where the exact minimax strategy can be computed. 2 Alternative Notions of Regret One of our contributions is showing that that interesting results can be obtained by choosing L differently than in Eq. (2); in particular, we obtain minimax optimal algorithms for the problem considered by Streeter and McMahan [2012] by analyzing an appropriate choice of L.", "startOffset": 0, "endOffset": 778}, {"referenceID": 14, "context": "gradient descent [Zinkevich, 2003] with a constant step size.", "startOffset": 17, "endOffset": 34}, {"referenceID": 6, "context": "Despite the theoretical guarantees, the player certainly might feel regret at having won only \u221a T in this situation! One might also hope to use online algorithms for portfolio management, for example those of Hazan and Kale [2009] and Agarwal et al.", "startOffset": 209, "endOffset": 231}, {"referenceID": 3, "context": "Despite the theoretical guarantees, the player certainly might feel regret at having won only \u221a T in this situation! One might also hope to use online algorithms for portfolio management, for example those of Hazan and Kale [2009] and Agarwal et al. [2006]. However, these algorithms require the assumption that you always retain at least an \u03b1 > 0 fraction of your bet, which is directly violated in our game.", "startOffset": 235, "endOffset": 257}, {"referenceID": 2, "context": "1 from Cesa-Bianchi and Lugosi [2006]), which gives Vt\u22121(g1, .", "startOffset": 7, "endOffset": 38}, {"referenceID": 1, "context": "The use of randomization to allow the application of the minimax theorem is similar to the technique used by Abernethy et al. [2009]. A key insight from the proof is that an optimal adversary can always select from {\u22121, 1}.", "startOffset": 109, "endOffset": 133}, {"referenceID": 1, "context": "2 Optimal regret against hypercube adversaries Abernethy et al. [2008] gives a minimax optimal algorithm when the player\u2019s xt and the comparator x\u030a are constrained to an L2 ball, and the adversary must also select gt from an L2 ball, for n \u2265 3 dimensions.", "startOffset": 47, "endOffset": 71}, {"referenceID": 14, "context": "where we have applied a classic formula of de Moivre [1718] for the mean absolute deviation of the binomial distribution (see also Diaconis and Zabell [1991]).", "startOffset": 2, "endOffset": 60}, {"referenceID": 6, "context": "where we have applied a classic formula of de Moivre [1718] for the mean absolute deviation of the binomial distribution (see also Diaconis and Zabell [1991]).", "startOffset": 131, "endOffset": 158}, {"referenceID": 1, "context": "Abernethy et al. [2008] shows that for the linear game with n \u2265 3 where both the learner and adversary select vectors from the unit sphere, the minimax value is exactly \u221a T .", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Abernethy et al. [2008] shows that for the linear game with n \u2265 3 where both the learner and adversary select vectors from the unit sphere, the minimax value is exactly \u221a T . Interestingly, in the n = 1 case (where L2 and L\u221e coincide), the value of the game is lower, about 0.8 \u221a T rather than \u221a T . This indicates a fundamental difference in the geometry of the n = 1 space and n \u2265 3. We conjecture the minimax value for the L2 game with n = 2 lies somewhere in between. 4.3 Non-stochastic betting and No-regret for all feasible sets simultaneously We derive a minimax optimal approach to the betting problem presented in Section 2, which also corresponds to the setting introduced by Streeter and McMahan [2012]. Again, it is sufficient to consider the one-dimensional case.", "startOffset": 0, "endOffset": 714}], "year": 2013, "abstractText": "We design and analyze minimax-optimal algorithms for online linear optimization games where the player\u2019s choice is unconstrained. The player strives to minimize regret, the difference between his loss and the loss of a post-hoc benchmark strategy. The standard benchmark is the loss of the best strategy chosen from a bounded comparator set. When the the comparison set and the adversary\u2019s gradients satisfy L\u221e bounds, we give the value of the game in closed form and prove it approaches \u221a 2T/\u03c0 as T \u2192 \u221e. Interesting algorithms result when we consider soft constraints on the comparator, rather than restricting it to a bounded set. As a warmup, we analyze the game with a quadratic penalty. The value of this game is exactly T/2, and this value is achieved by perhaps the simplest online algorithm of all: unprojected gradient descent with a constant learning rate. We then derive a minimax-optimal algorithm for a much softer penalty function. This algorithm achieves good bounds under the standard notion of regret for any comparator point, without needing to specify the comparator set in advance. The value of this game converges to \u221a e as T \u2192 \u221e; we give a closed-form for the exact value as a function of T . The resulting algorithm is natural in unconstrained investment or betting scenarios, since it guarantees at worst constant loss, while allowing for exponential reward against an \u201ceasy\u201d adversary.", "creator": "LaTeX with hyperref package"}}}