{"id": "1302.4245", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2013", "title": "Gaussian Process Kernels for Pattern Discovery and Extrapolation", "abstract": "fuzzy gaussian processes are rich distributions over functions, which provide a bayesian nonparametric approach to smoothing smoothing and interpolation. we introduce simple closed form kernels that can be used with natural gaussian processes to discover transient patterns and enable extrapolation. these kernels are derived by modelling a spectral density - - the fourier transform of a kernel - - periodic with a gaussian mixture. the proposed kernels support a broad orthogonal class of stationary covariances, consistently but gaussian process inference remains simple and analytic. we ultimately demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric co2 spatial trends and airline passenger data. we finally also show that we can reconstruct standard covariances within our framework.", "histories": [["v1", "Mon, 18 Feb 2013 12:41:50 GMT  (448kb,D)", "http://arxiv.org/abs/1302.4245v1", "15 pages, 5 figures, 1 table. Submitted for publication"], ["v2", "Tue, 19 Feb 2013 12:52:04 GMT  (448kb,D)", "http://arxiv.org/abs/1302.4245v2", "15 pages, 5 figures, 1 table. Submitted for publication"], ["v3", "Tue, 31 Dec 2013 16:41:30 GMT  (456kb,D)", "http://arxiv.org/abs/1302.4245v3", "10 pages, 5 figures, 1 table. Minor edits and titled changed from \"Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation\" to \"Gaussian Process Kernels for Pattern Discovery and Extrapolation\". Appears at the International Conference on Machine Learning (ICML), JMLR W&amp;CP 28(3):1067-1075, 2013"]], "COMMENTS": "15 pages, 5 figures, 1 table. Submitted for publication", "reviews": [], "SUBJECTS": "stat.ML cs.AI stat.ME", "authors": ["andrew gordon wilson", "ryan prescott adams"], "accepted": true, "id": "1302.4245"}, "pdf": {"name": "1302.4245.pdf", "metadata": {"source": "CRF", "title": "Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation", "authors": ["Andrew Gordon Wilson", "Ryan Prescott Adams"], "emails": ["agw38@cam.ac.uk", "rpa@seas.harvard.edu"], "sections": [{"heading": "1 Introduction", "text": "Machine learning is fundamentally about pattern discovery. The first machine learning models, such as the perceptron (Rosenblatt, 1962), were based on a simple model of a neuron (McCulloch and Pitts, 1943). Papers such as Rumelhart et al. (1986) inspired hope that it would be possible to develop intelligent agents with models like neural networks, which could automatically discover hidden representations in data. Indeed, machine learning aims not only to equip humans with tools to analyze data, but to fully automate the learning and decision making process.\nResearch on Gaussian processes (GPs) within the machine learning community developed out of neural networks research, triggered by Neal (1996), who observed that Bayesian neural networks became Gaussian processes as the number of hidden units approached infinity. Neal (1996) conjectured that \u201cthere may be simpler ways to do inference in this case\u201d.\nThese simple inference techniques became the cornerstone of subsequent Gaussian process models for machine learning (Rasmussen and Williams, 2006). These models construct a prior directly over functions, rather than parameters. Assuming Gaussian noise, one can analytically infer a posterior distribution over these functions, given data. Gaussian process models have become popular for non-linear regression and classification (Rasmussen and Williams, 2006), and often have impressive empirical performance (Rasmussen, 1996).\nThe properties of likely functions under a GP, e.g., smoothness, periodicity, etc., are controlled by a positive definite covariance kernel1, an operator which determines the similarity between pairs of points in the domain of the random function. The choice of kernel profoundly affects the performance of a Gaussian\n\u2217http://mlg.eng.cam.ac.uk/andrew \u2020http://people.seas.harvard.edu/~rpa/ 1The terms covariance kernel, covariance function, kernel function, and kernel are used interchangeably.\n1\nar X\niv :1\n30 2.\n42 45\nv1 [\nst at\n.M L\n] 1\n8 Fe\nprocess on a given task \u2013 as much as the choice of architecture, activation functions, learning rate, etc., can affect the performance of a neural network.\nGaussian processes are sometimes used as expressive statistical tools, where the pattern discovery is performed by a human, and then hard coded into parametric kernels. Often, however, the squared exponential (Gaussian) kernel is used by default. In either case, Gaussian processes are used as smoothing interpolators with a fixed (albeit infinite) set of basis functions. Such simple smoothing devices are not a realistic replacement for neural networks, which were envisaged as intelligent agents that could discover hidden features in data2 via adaptive basis functions (MacKay, 1998).\nHowever, Bayesian nonparametrics can help build automated intelligent systems that reason and make decisions. It has been suggested that the human ability for inductive reasoning \u2013 concept generalization with remarkably few examples \u2013 could derive from a prior combined with Bayesian inference (Yuille and Kersten, 2006; Tenenbaum et al., 2011; Steyvers et al., 2006). Bayesian nonparametric models, and Gaussian processes in particular, are an expressive way to encode prior knowledge, and also reflect the belief that the real world is infinitely complex (Neal, 1996).\nWith more expressive kernels, one could use Gaussian processes to learn hidden representations in data. Expressive kernels have been developed by combining Gaussian processes in a type of Bayesian neural network structure (Salakhutdinov and Hinton, 2008; Wilson et al., 2012; Damianou and Lawrence, 2012). However, these approaches, while promising, typically 1) are designed to model specific types of structure (e.g., input-dependent correlations between different tasks); 2) make use of component GPs with simple interpolating kernels; 3) indirectly induce complicated kernels that do not have a closed form and are difficult to interpret; and 4) require sophisticated approximate inference techniques that are much more demanding than that required by simple analytic kernels.\nSophisticated kernels are most often achieved by composing together a few standard kernel functions (Archambeau and Bach, 2011; Durrande et al., 2011; Go\u0308nen and Alpayd\u0131n, 2011; Rasmussen and Williams, 2006). Tight restrictions are typically enforced on these compositions and they are hand-crafted for specialized applications. Without such restrictions, complicated compositions of kernels can lead to overfitting and unmanageable hyperparameter inference. Moreover, while some compositions (e.g., addition) have an easily interpretable effect on the resulting sample paths, many other operations change the distribution over functions in ways that are difficult to identify. It is difficult, therefore, to construct an effective inductive bias for kernel composition that leads to automatic discovery of the appropriate statistical structure, without human intervention.\nThis difficulty is exacerbated by the fact that it is challenging to say anything about the covariance function of a stochastic process from a single draw if no assumptions are made. If we allow the covariance between any two points in the input space to arise from any positive definite function, then we gain essentially no information from a single realization. Most commonly one assumes a restriction to stationary kernels, meaning that covariances are invariant to translations in the input space.\nIn this paper, we explore flexible classes of kernels that go beyond composition of simple analytic forms, while maintaining the useful inductive bias of stationarity. We propose new kernels which can be used to automatically discover patterns and extrapolate far beyond the available data. This class of kernels contains many stationary kernels, but has a simple closed form that leads to straightforward analytic inference. The simplicity of these kernels is one of their strongest qualities. In many cases, these kernels can be used as a drop in replacement for the popular squared exponential kernel, with benefits in performance and expressiveness. By learning features in data, we not only improve predictive performance, but we can more deeply understand the structure of the problem at hand \u2013 greenhouse gases, air travel, heart physiology, brain activity, etc.\nAfter a brief review of Gaussian processes in Section 2, we derive the new kernels in Section 3 by modelling a spectral density with a mixture of Gaussians. We focus our experiments in Section 4 on elucidating\n2In this paper, we refer to representations, features and patterns interchangeably. In other contexts, the term features sometimes specifically means low dimensional representations of data, like neurons in a neural network.\nthe fundamental differences between the proposed kernels and the popular alternatives in Rasmussen and Williams (2006). In particular, we show how the proposed kernels can automatically discover patterns and extrapolate on the CO2 dataset in Rasmussen and Williams (2006), on a synthetic dataset with strong negative covariances, on a difficult synthetic sinc pattern, and on airline passenger data. We also use our framework to reconstruct several popular standard kernels."}, {"heading": "2 Gaussian Processes", "text": "A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution. Using a Gaussian process, we can define a distribution over functions f(x),\nf(x) \u223c GP(m(x), k(x, x\u2032)) , (1)\nwhere x \u2208 RP is an arbitrary input variable, and the mean function m(x) and covariance kernel k(x, x\u2032) are defined as\nm(x) = E[f(x)] , (2) k(x, x\u2032) = cov(f(x), f(x\u2032)) . (3)\nAny collection of function values has a joint Gaussian distribution\n[f(x1), f(x2), . . . , f(xN )] > \u223c N (\u00b5,K) , (4)\nwhere the N \u00d7N covariance matrix K has entries Kij = k(xi, xj), and the mean \u00b5 has entries \u00b5i = m(xi). The properties of the functions \u2013 smoothness, periodicity, etc. \u2013 are determined by the kernel function.\nThe popular squared exponential (SE) kernel has the form\nkSE(x, x \u2032) = exp(\u22120.5||x\u2212 x\u2032||2/`2) . (5)\nFunctions drawn from a Gaussian process with this kernel function are infinitely differentiable, and can display long range trends. Gaussian processes with a squared exponential kernel are simply smoothing devices: the only covariance structure that can be learned from data is the length-scale `, which determines how quickly a Gaussian process function varies with x.\nAssuming Gaussian noise, one can analytically infer a posterior predictive distribution over Gaussian process functions, and analytically derive a marginal likelihood of the observed function values y given only hyperparameters \u03b8, and the input locations {xn}Nn=1, p(y|\u03b8, {xn}Nn=1). This marginal likelihood can be optimised to estimate hyperparameters such as `, or used to integrate out the hyperparameters via Markov chain Monte Carlo (Murray and Adams, 2010). Detailed Gaussian process references include Rasmussen and Williams (2006), Stein (1999), and Cressie (1993)."}, {"heading": "3 Kernels for Pattern Discovery", "text": "In this section we introduce a class of kernels that can discover patterns, extrapolate, and model negative covariances. This class contains a large set of stationary kernels. Roughly, a kernel measures the similarity between data points. As in Equation (3), the covariance kernel of a GP determines how the associated random functions will tend to vary with inputs (predictors) x \u2208 RP . A stationary kernel is a function of \u03c4 = x\u2212 x\u2032, i.e., it is invariant to translation of the inputs.\nAny stationary kernel (aka covariance function) can be expressed as an integral using Bochner\u2019s theorem (Bochner, 1959; Stein, 1999; Gikhman and Skorokhod, 2004):\nTheorem 3.1 (Bochner) A complex-valued function k on RP is the covariance function of a weakly stationary mean square continuous complex-valued random process on RP if and only if it can be represented as\nk(\u03c4) = \u222b RP e2\u03c0is >\u03c4\u03c8(ds) , (6)\nwhere \u03c8 is a positive finite measure.\nIf \u03c8 has a density S(s), then S is called the spectral density or power spectrum of k, and k and S are Fourier duals (Chatfield, 1989):\nk(\u03c4) = \u222b S(s)e2\u03c0is >\u03c4ds , (7)\nS(s) = \u222b k(\u03c4)e\u22122\u03c0is >\u03c4d\u03c4 . (8)\nIn other words, a spectral density entirely determines the properties of a stationary kernel. Substituting the squared exponential kernel of (5) into (8), we find its spectral density is SSE(s) = (2\u03c0`\n2)P/2 exp(\u22122\u03c02`2s2). Therefore SE kernels, and mixtures of SE kernels, are a very small corner of the set of possible stationary kernels, as they correspond only to Gaussian spectral densities centered on the origin.\nHowever, by using a mixture of Gaussians that have non-zero means, one can achieve a much wider range of spectral densities. Indeed, mixtures of Gaussians are dense in the set of all distribution functions (Kostantinos, 2000). Therefore, the dual of this set is also dense in stationary covariances. That is, we can approximate any stationary covariance kernel to arbitrary precision, given enough mixture components in the spectral representation. This observation motivates our approach, which is to model GP covariance functions via spectral densities that are scale-location mixtures of Gaussians.\nWe first consider a simple case, where\n\u03c6(s ;\u00b5, \u03c32) = 1\u221a 2\u03c0\u03c32 exp{\u2212 1 2\u03c32 (s\u2212 \u00b5)2}, and (9)\nS(s) = [\u03c6(s) + \u03c6(\u2212s)]/2 , (10)\nnoting that spectral densities are real and symmetric about s = 0 (since k(\u03c4) is real and symmetric about \u03c4 = 0) (e.g. Ho\u0308rmander (1990)).\nSubstituting S(s) into equation (7), we find\nk(\u03c4) = exp{\u22122\u03c02\u03c42\u03c32} cos(2\u03c0\u03c4\u00b5) . (11)\nIf \u03c6(s) is instead a mixture ofQGaussians on RP , where the qth component has mean vector \u00b5q = (\u00b5(1)q , . . . , \u00b5(P )q ) and covariance matrix Mq = diag(v (1) q , . . . , v (P ) q ), and \u03c4p is the p\nth component of the P dimensional vector \u03c4 = x\u2212 x\u2032, then\nk(\u03c4) = Q\u2211 q=1 wq P\u220f p=1 exp{\u22122\u03c02\u03c42p v(p)q } cos(2\u03c0\u03c4p\u00b5(p)q ). (12)\nThe integral in (7) is tractable even when the spectral density is an arbitrary Gaussian mixture, allowing us to derive3 the exact closed form expressions in Eqs. (11) and (12), and to perform analytic inference with Gaussian processes. Moreover, this class of kernels is expressive \u2013 containing many stationary kernels \u2013 but nevertheless has a simple form.\nThese kernels are easy to interpret, and provide drop-in replacements for kernels in Rasmussen and Williams (2006). The weights wq specify the relative contribution of each mixture component. The\n3Detailed derivations of Eqs. (11) and (12) are in the Appendix.\ninverse means 1/\u00b5q are the component periods, and the inverse standard deviations 1/ \u221a vq are lengthscales, determining how quickly a component varies with the inputs x. The kernel in Eq. (12) can also be interpreted through its associated spectral density. In Section 4, we use the learned spectral density to interpret the number of discovered patterns in the data and how these patterns generalize. Henceforth, we refer to the kernel in Eq. (12) as a spectral mixture (SM) kernel."}, {"heading": "4 Experiments", "text": "We show how the SM kernel in Eq. (12) can be used to discover patterns, extrapolate, and model negative covariances. We contrast the SM kernel with popular kernels in, e.g., Rasmussen and Williams (2006) and Abrahamsen (1997), which typically only provide smooth interpolation. Although the SM kernel generally improves predictive likelihood over popular alternatives, we focus on clearly visualizing the learned kernels and spectral densities, examining patterns and predictions, and discovering structure in the data. Our objective is to elucidate the fundamental differences between the proposed SM kernel and the alternatives.\nIn all experiments, Gaussian noise is assumed, so that marginalization over the unknown function can be performed in closed form. Kernel hyperparameters are trained using nonlinear conjugate gradients to optimize the marginal likelihood p(y|\u03b8, {xn}Nn=1) of the data y given hyperparameters \u03b8, as described in Section 2, assuming a zero mean GP. Automatic relevance determination (MacKay et al., 1994; Tipping, 2004) takes place during training, removing extraneous components from the proposed model, through the complexity penalty in the marginal likelihood (Rasmussen and Williams, 2006). For a fully Bayesian treatment, the spectral density could alternatively be integrated out using Markov chain Monte Carlo (Murray and Adams, 2010), rather than choosing a point estimate. However, we wish to emphasize that the SM kernel can be successfully used in the same way as other popular kernels, without additional inference efforts.\nWe compare with the popular squared exponential (SE), Mate\u0301rn (MA), rational quadratic (RQ), and periodic (PE) kernels. In each comparison, we attempt to give these alternative kernels fair treatment: we initialise hyperparameters at values that give high marginal likelihoods and which are well suited to the datasets, based on features we can already see in the data. Conversely, we randomly initialise the parameters for the SM kernel. Training runtimes are on the order of minutes for all tested kernels.\n4.1 Extrapolating Atmospheric CO2\nKeeling and Whorf (2004) recorded monthly average atmospheric CO2 concentrations at the Mauna Loa Observatory, Hawaii. The data are shown in Figure 1. The first 200 months are used for training (in blue), and the remaining 301 months (\u2248 25 years) are used for testing (in green).\nThis dataset was used in Rasmussen and Williams (2006), and is frequently used in Gaussian process tutorials, to show how GPs are flexible statistical tools: a human can look at the data, recognize patterns, and then hard code those patterns into covariance kernels. Rasmussen and Williams (2006) identify, by looking at the blue and green curves in Figure 1a, a long term rising trend, seasonal variation with possible decay away from periodicity, medium term irregularities, and noise, and hard code a stationary covariance kernel to represent each of these features.\nHowever, in this view of GP modelling, all of the interesting pattern discovery is done by the human user, and the Gaussian process is used simply as a smoothing device, with the flexibility to incorporate human inferences in the prior. Our contention is that such pattern recognition can also be performed algorithmically. To discover these patterns without encoding them a priori into the GP, we use the spectral mixture kernel in Eq. (12), with Q = 10. The results are shown in Figure 1a.\nIn the training region, predictions using each kernel are essentially equivalent, and entirely overlap with the training data. However, unlike the other kernels, the SM kernel (in black) is able to discover patterns in the training data and accurately extrapolate over a long range. The 95% high predictive density (HPD) region contains the true CO2 measurements for the duration of the measurements.\nWe can see the structure discovered by the SM kernel in the learned log spectral density in Figure 1b. Of the Q = 10 components, only seven were used. This is an example of automatic relevance determination (ARD) occurring through the mixture weights. There is a peak at a frequency near 0.07, which corresponds to a period of about 14 months, roughly a year, which is in line with seasonal variations. However, there are departures away from periodicity, and accordingly, there is a large variance about this peak, reflecting the uncertainty in this learned feature. We see other, sharper peaks corresponding to periods of 6 months, 4 months, 3 months and 1 month.\nIn red, we show the spectral density for the learned SE kernel, which covers some of the peaks in the SM spectral density, but assigns mass to many different frequencies, while missing the important frequency component at 1. All SE kernels have a Gaussian spectral density centred on zero. Since a mixture of Gaussians centred on zero is a poor approximation to many more general density functions, combinations of SE kernels have limited expressive power. Indeed the spectral density learned by the SM kernel in Figure 1b is highly non-Gaussian. The test predictive performance using the SM, SE, MA, RQ, and PE kernels is given in Table 1, under the heading CO2."}, {"heading": "4.2 Recovering Popular Kernels", "text": "The SM class of kernels contains many stationary kernels, since mixtures of Gaussians can be used to construct a wide range of spectral densities. Even with a small number of mixture components, e.g.,Q \u2264 10, the SM kernel can closely recover the most popular stationary kernels catalogued in Rasmussen and Williams (2006).\nAs an example, we start by sampling 100 points from a one-dimensional GP with a Mate\u0301rn kernel with\ndegrees of freedom \u03bd = 3/2:\nkMA(\u03c4) = a(1 +\n\u221a 3\u03c4\n` ) exp(\u2212\n\u221a 3\u03c4\n` ) , (13)\nwhere ` = 5 and a = 4. Sample functions from a Gaussian process with this Mate\u0301rn kernel are far less smooth (only once-differentiable) than Gaussian process functions with a squared exponential kernel.\nWe attempt to reconstruct the kernel underlying the data by training an SM kernel with Q = 10. After ARD in training, only Q = 4 components are used. The log marginal likelihood of the data \u2013 having integrated away the Gaussian process \u2013 using the trained SM kernel is \u2212133, compared to \u2212138 for the Mate\u0301rn kernel that generated the data. Training the squared exponential kernel in (5) gives a log marginal likelihood of \u2212140.\nFigure 2a shows the learned SM correlation function4, compared to the generating Mate\u0301rn correlation function, the empirical autocorrelation function, and learned squared exponential correlation function. Although often used in geostatistics to guide choices of Gaussian process kernel functions (and parameters) (Cressie, 1993), the empirical autocorrelation function tends to be unreliable, particularly with a small amount of data (e.g., N < 1000), and at high lags (for \u03c4 > 10). In Figure 2a, the empirical autocorrelation function is erratic and does not resemble the Mate\u0301rn kernel for \u03c4 > 10. Moreover, the squared exponential kernel cannot capture the heavy tails of the Mate\u0301rn kernel, no matter what length-scale it has. The learned SM kernel more closely matches the Mate\u0301rn kernel.\nNext, we reconstruct a mixture of the rational quadratic (RQ) and periodic kernels (PE) in Rasmussen and Williams (2006):\nkRQ(\u03c4) = (1 + \u03c42\n2\u03b1 `2RQ )\u2212\u03b1 , (14)\nkPE(\u03c4) = exp(\u22122 sin2(\u03c0 \u03c4 \u03c9)/`2PE) . (15)\nThe rational quadratic kernel in (14) is derived as a scale mixture of squared exponential kernels with different length-scales. The standard periodic kernel in (15) is derived by mapping the two dimensional\n4A correlation function c(x, x\u2032) is a normalised covariance kernel k(x, x\u2032), such that c(x, x\u2032) = k(x, x\u2032)/ \u221a\nk(x, x)k(x\u2032, x\u2032) and c(x, x) = 1.\nvariable u(x) = (cos(x), sin(x)) through the squared exponential kernel in (5). Derivations for both the RQ and PE kernels in (14) and (15) are in Rasmussen and Williams (2006). Rational quadratic and Mate\u0301rn kernels are also discussed in Abrahamsen (1997). We sample 100 points from a Gaussian process with kernel 10kRQ + 4kPE, with \u03b1 = 2, \u03c9 = 1/20, `RQ = 40, `PE = 1.\nWe reconstruct the kernel function of the sampled process using an SM kernel with Q = 4, with the results shown in Figure 2b. The heavy tails of the RQ kernel are modelled by two components with large periods (essentially aperiodic): one of these components has a short length-scale, while the other has a large length-scale. The third component has a relatively short length-scale, and a period of 20. There is not enough information in the 100 sample points to justify using more than Q = 3 components, and so \u2013 through automatic relevance determination in training \u2013 the fourth component in the SM kernel moves towards zero. The empirical autocorrelation function somewhat captures the periodicity in the data, but significantly underestimates the correlations. The squared exponential kernel learns a long length-scale: since the SE kernel is significantly misspecified with the true generating kernel, the data are explained as noise."}, {"heading": "4.3 Negative Covariances", "text": "All of the stationary covariance functions in the standard machine learning Gaussian process reference Rasmussen and Williams (2006) are everywhere positive, including the periodic kernel, k(\u03c4) = exp(\u22122 sin2(\u03c0 \u03c4 \u03c9)/`2). While positive covariances are often suitable for interpolation, capturing negative covariances can be essential for extrapolating patterns: for example, linear trends have long-range negative covariances. We test the ability of the SM kernel to learn a negative covariance function, by sampling 400 points from a\nsimple AR(1) discrete time Gaussian process:\ny(x+ 1) = \u2212e\u22120.01y(x) + \u03c3 (x) , (16) (x) \u223c N (0, 1) , (17)\nwhich has kernel\nk(x, x\u2032) = \u03c32(\u2212e\u2212.01)|x\u2212x \u2032|/(1\u2212 e\u2212.02) . (18)\nThe process in Eq. (16) is shown in Figure 3a. This process follows an oscillatory pattern, systematically switching states every x = 1 unit, but is not periodic and has long range covariances: if we were to only view every second data point, the resulting process would vary rather slowly and smoothly.\nWe see in Figure 3b that the learned SM covariance function accurately reconstructs the true covariance function. The spectral density in Figure 3c) shows a sharp peak at a frequency of 0.5, or a period of 2. This feature represents the tendency for the process to oscillate from positive to negative every x = 1 unit. In this case Q = 4, but after automatic relevance determination in training, only 3 components were used. Using the SM kernel, we forecast 20 units ahead and compare to other kernels in Table 1 (NEG COV)."}, {"heading": "4.4 Discovering the Sinc Pattern", "text": "The sinc function is defined as sinc(x) = sin(\u03c0x)/(\u03c0x). We created a pattern by combining three sinc functions:\ny(x) = sinc(x+ 10) + sinc(x) + sinc(x\u2212 10) . (19)\nThis is a complex oscillatory pattern. Given only the points shown in Figure 4a, we wish to complete the pattern for x \u2208 [\u22124.5, 4.5]. Unlike the CO2 example in Section 4.1, it is perhaps even difficult for a human to extrapolate the missing pattern from the training data. It is an interesting exercise to focus on this figure, identify features, and fill in the missing part.\nNotice that there is complete symmetry about the origin x = 0, peaks at x = \u221210 and x = 10, and destructive interference on each side of the peaks facing the origin. We therefore might expect a peak at x = 0 and a symmetric pattern around x = 0.\nAs shown in Figure 4b, the SM kernel with Q = 10 reconstructs the pattern in the region x \u2208 [\u22124.5, 4.5] almost perfectly from the 700 training points in blue. Moreover, using the SM kernel, 95% of the posterior predictive mass entirely contains the true pattern in x \u2208 [\u22124.5, 4.5]. GPs using Mate\u0301rn, squared exponential, rational quadratic, and periodic kernels are able to predict reasonably within x = 0.5 units of the training data, but entirely miss the pattern in x \u2208 [\u22124.5, 4.5].\nFigure 4c shows the learned SM correlation function (normalised kernel). For \u03c4 \u2208 [0, 10] there is a local pattern, roughly representing the behaviour of a single sinc function. For \u03c4 > 10 there is a repetitive pattern representing a new sinc function every 10 units \u2013 an extrapolation a human might make. With a sinc functions centred at x = \u221210, 0, 10, we might expect more sinc patterns every 10 units. The learned Mate\u0301rn correlation function is shown in red in Figure 4c \u2013 unable to discover complex patterns in the data, it simply assigns high correlation to nearby points.\nFigure 4d shows the (highly non-Gaussian) spectral density of the SM kernel, with peaks at 0.003, 0.1, 0.2, 0.3, 0.415, 0.424, and 0.492. In this case, only 7 of the Q = 10 components are used. The peak at 0.1 represents a period of 10: every 10 units, a sinc function is repeated. The variance of this peak is small, meaning the method will extrapolate this structure over long distances. By contrast, the squared exponential spectral density simply has a broad peak, centred at the origin. The predictive performance for recovering the missing 300 test points (in green) is given in Table 1 (SINC)."}, {"heading": "4.5 Airline Passenger Data", "text": "Figure 5a shows airline passenger numbers, recorded monthly, from 1949 to 1961 (Hyndman, 2005). Based on only the first 96 monthly measurements, in blue, we wish to forecast airline passenger numbers for the next 48 months (4 years); the corresponding 48 test measurements are in green. Companies wish to make such long range forecasts to decide upon expensive long term investments, such as large passenger jets, which can cost hundreds of millions of dollars.\nThere are several features apparent in these data: short seasonal variations, a long term rising trend, and an absence of white noise artifacts. Many popular kernels are forced to make one of two choices: 1) Model the short term variations and ignore the long term trend, at the expense of extrapolation. 2) Model the long term trend and treat the shorter variations as noise, at the expense of interpolation.\nAs seen in Figure 5a, the Mate\u0301rn kernel is more inclined to model the short term trends than the smoother SE or RQ kernels, resulting in sensible interpolation (predicting almost identical values to the training data in the training region), but poor extrapolation \u2013 moving quickly to the prior mean, having learned no structure to generalise beyond the data. The SE kernel interpolates somewhat sensibly, but appears to underestimate the magnitudes of peaks and troughs, and treats repetitive patterns in the data as noise. Extrapolation using the SE kernel is poor. The RQ kernel, which is a scale mixture of SE kernels, is more able to manage different length-scales in the data, and generalizes the long term trend better than the SE kernel, but interpolates poorly.\nBy contrast, the SM kernel interpolates nicely (overlapping with the data in the training region), and is able to successfully extrapolate complex patterns far beyond the data, capturing the true airline passenger numbers for years after the data ends, within a small band containing 95% of the predictive probability mass.\nOf the Q = 10 initial components specified for the SM kernel, 7 were used after ARD in training. The learned spectral density in Figure 5b shows a large sharp low frequency peak (at about 0.00148). This\npeak corresponds to the rising trend, which generalises well beyond the data (small variance peak), is smooth (low frequency), and is important for describing the data (large relative weighting). The next largest peak corresponds to the yearly trend of 12 months, which again generalises, but not to the extent of the smooth rising trend, since the variance of this peak is larger than for the peak near the origin. The higher frequency peak at x = 0.34 (period of 3 months) corresponds to the beginnings of new seasons, which can explain the effect of seasonal holidays on air traffic. We could interpret the peak at x = 0.25 as relating to seasonal holidays as well, since there is a major holiday every 4 months. A more detailed study of these features and their properties \u2013 frequencies, variances, weightings, etc. \u2013 could isolate less obvious non-seasonal effects important for modelling airline passenger numbers. Table 1 (AIRLINE) shows comparative predictive performance for forecasting 48 months of airline passenger numbers."}, {"heading": "5 Discussion", "text": "We have derived expressive closed form kernels and we have shown that these kernels, when used with Gaussian processes, can discover patterns in data and extrapolate over long ranges. The simplicity of these kernels is one of their strongest properties: they can be used as drop-in replacements for popular kernels such as the squared exponential kernel, with major benefits in expressiveness and performance, while retaining simple training and inference procedures.\nGaussian processes have proven themselves as powerful smoothing interpolators. We believe that pattern discovery and extrapolation is an exciting new direction for Bayesian nonparametric approaches, which can capture rich variations in data. Here we have shown how Bayesian nonparametric models can naturally be used to generalise a pattern from a small number of examples.\nWe have only begun to explore what could be done with such pattern discovery methods. In future work, one could integrate away a spectral density, using recently developed efficient MCMC for GP hyperparameters (Murray and Adams, 2010). Moreover, spectral representations of the proposed models suggest recent Toeplitz methods (Saatchi, 2011) could be applied for significant speedup in inference and predictions."}, {"heading": "Acknowledgements", "text": "We thank Richard E. Turner, Carl Edward Rasmussen, and Neil D. Lawrence, for interesting discussions."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 Derivations for Spectral Mixture Kernels", "text": "A stationary kernel k(x, x\u2032) is the inverse Fourier transform of its spectral density S(s),\nk(\u03c4) = \u222b S(s)e2\u03c0is >\u03c4ds , (20)\nwhere \u03c4 = x\u2212 x\u2032. First suppose\nS(s) = 1\u221a 2\u03c0\u03c32 exp{\u2212 1 2\u03c32 (s\u2212 \u00b5)2} , (21)\nwhere s, \u00b5, \u03c3 and \u03c4 = x\u2212 x\u2032 are scalars. Substituting (21) into (20),\nk(x, x\u2032) = \u222b exp(2\u03c0is(x\u2212 x\u2032)) 1\u221a\n2\u03c0\u03c32 exp(\u2212 1 2\u03c32 (s\u2212 \u00b5)2)ds (22)\nlet \u03c4 = x\u2212 x\u2032\n= 1\u221a\n2\u03c0\u03c32\n\u222b exp[2\u03c0i\u03c4 \u2212 1\n2\u03c32 (s2 \u2212 2\u00b5s+ \u00b52)]ds (23)\n= 1\u221a\n2\u03c0\u03c32\n\u222b exp[\u2212 1\n2\u03c32 s2 + (2\u03c0i\u03c4 +\n\u00b5 \u03c32 )s\u2212 \u00b5\n2\n\u03c32 ]ds (24)\nlet a = 1\n2\u03c32 , b = 2\u03c0i\u03c4 +\n\u00b5 \u03c32 , c = \u2212 \u00b5\n2\n2\u03c32\n= 1\u221a\n2\u03c0\u03c32\n\u222b exp(\u2212a(s\u2212 b\n2a )2) exp(\nb2 4a + c)ds (25)\n= exp[(2\u03c0i\u03c4 + \u00b5 \u03c32 )2 \u03c32 2 \u2212 \u00b5 2 2\u03c32 ] (26) = exp[(\u22124\u03c02\u03c42 + 4\u03c0i\u03c4 \u00b5 \u03c32 + \u00b52 \u03c34 ) \u03c32 2 \u2212 \u00b5 2 2\u03c32 ] (27) = exp[\u22122\u03c02(x\u2212 x\u2032)2\u03c32][cos(2\u03c0(x\u2212 x\u2032)\u00b5) + i sin(2\u03c0(x\u2212 x\u2032)\u00b5))] . (28)\nNoting that the spectral density S(s) must be symmetric about s = 0, we let\n\u03c6(s ;\u00b5, \u03c32) = 1\u221a 2\u03c0\u03c32 exp{\u2212 1 2\u03c32 (s\u2212 \u00b5)2}, and (29)\nS(s) = [\u03c6(s) + \u03c6(\u2212s)]/2 . (30)\nClosely following the above derivation, substituting (30) into (20) gives\nk(\u03c4) = exp{\u22122\u03c02\u03c42\u03c32} cos(2\u03c0\u03c4\u00b5) . (31)\nIf \u03c6(s) is instead a mixture ofQGaussians on RP , where the qth component has mean vector \u00b5q = (\u00b5(1)q , . . . , \u00b5(P )q ) and covariance matrix Mq = diag(v (1) q , . . . , v (P ) q ), and \u03c4p is the p\nth component of the P dimensional vector \u03c4 = x\u2212 x\u2032, then the integral in (20) becomes a sum of a product of the one dimensional integrals we encountered to derive (31), from which it follows that\nk(\u03c4) = Q\u2211 q=1 wq P\u220f p=1 exp{\u22122\u03c02\u03c42p v(p)q } cos(2\u03c0\u03c4p\u00b5(p)q ). (32)"}, {"heading": "6.2 Comment on Training Hyperparameters", "text": "Generally, we have had success naively training kernel hyperparameters using conjugate gradients (we use Carl Rasmussen\u2019s 2010 version of minimize.m) to maximize the marginal likelihood p(y|\u03b8) of the data y given hypers \u03b8, having analytically integrated away a zero mean Gaussian process. We have found subtracting an empirical mean from the data prior to training hyperparameters (with conjugate gradients) undesirable, sometimes leading to local optima with lower marginal likelihoods, particularly on small datasets with a rising trend."}], "references": [{"title": "A review of Gaussian random fields and correlation functions", "author": ["P. Abrahamsen"], "venue": "Norweigan Computing Center Technical report.", "citeRegEx": "Abrahamsen,? 1997", "shortCiteRegEx": "Abrahamsen", "year": 1997}, {"title": "Multiple Gaussian process models", "author": ["C. Archambeau", "F. Bach"], "venue": "arXiv preprint arXiv:1110.5238.", "citeRegEx": "Archambeau and Bach,? 2011", "shortCiteRegEx": "Archambeau and Bach", "year": 2011}, {"title": "Time Series Analysis: An Introduction", "author": ["C. Chatfield"], "venue": "London: Chapman and Hall.", "citeRegEx": "Chatfield,? 1989", "shortCiteRegEx": "Chatfield", "year": 1989}, {"title": "Statistics for Spatial Data (Wiley Series in Probability and Statistics)", "author": ["N. Cressie"], "venue": "WileyInterscience.", "citeRegEx": "Cressie,? 1993", "shortCiteRegEx": "Cressie", "year": 1993}, {"title": "Deep Gaussian processes", "author": ["A. Damianou", "N. Lawrence"], "venue": "arXiv preprint arXiv:1211.0358.", "citeRegEx": "Damianou and Lawrence,? 2012", "shortCiteRegEx": "Damianou and Lawrence", "year": 2012}, {"title": "Additive kernels for Gaussian process modeling", "author": ["N. Durrande", "D. Ginsbourger", "O. Roustant"], "venue": "arXiv preprint arXiv:1103.4023.", "citeRegEx": "Durrande et al\\.,? 2011", "shortCiteRegEx": "Durrande et al\\.", "year": 2011}, {"title": "The Theory of Stochastic Processes, volume 2", "author": ["I. Gikhman", "A. Skorokhod"], "venue": "Springer Verlag.", "citeRegEx": "Gikhman and Skorokhod,? 2004", "shortCiteRegEx": "Gikhman and Skorokhod", "year": 2004}, {"title": "Multiple kernel learning algorithms", "author": ["M. G\u00f6nen", "E. Alpayd\u0131n"], "venue": "Journal of Machine Learning Research, 12:2211\u20132268.", "citeRegEx": "G\u00f6nen and Alpayd\u0131n,? 2011", "shortCiteRegEx": "G\u00f6nen and Alpayd\u0131n", "year": 2011}, {"title": "The Analysis of Linear Partial Differential Operators I, Distribution Theory and Fourier Analysis", "author": ["L. H\u00f6rmander"], "venue": "Springer-Verlag.", "citeRegEx": "H\u00f6rmander,? 1990", "shortCiteRegEx": "H\u00f6rmander", "year": 1990}, {"title": "Time series data library", "author": ["R. Hyndman"], "venue": "http://www-personal.buseco.monash.edu.au/ ~hyndman/TSDL/. Keeling, C. D. and Whorf, T. P. (2004). Atmospheric CO2 records from sites in the SIO air sampling network. Trends: A Compendium of Data on Global Change. Carbon Dioxide Information Analysis Center.", "citeRegEx": "Hyndman,? 2005", "shortCiteRegEx": "Hyndman", "year": 2005}, {"title": "Gaussian mixtures and their applications to signal processing", "author": ["N. Kostantinos"], "venue": "Advanced Signal Processing Handbook: Theory and Implementation for Radar, Sonar, and Medical Imaging Real Time Systems.", "citeRegEx": "Kostantinos,? 2000", "shortCiteRegEx": "Kostantinos", "year": 2000}, {"title": "Bayesian nonlinear modeling for the prediction competition", "author": ["D MacKay"], "venue": "Ashrae Transactions, 100(2):1053\u20131062.", "citeRegEx": "MacKay,? 1994", "shortCiteRegEx": "MacKay", "year": 1994}, {"title": "Introduction to Gaussian processes", "author": ["D.J. MacKay"], "venue": "Christopher M. Bishop, e., editor, Neural Networks and Machine Learning, chapter 11, pages 133\u2013165. Springer-Verlag.", "citeRegEx": "MacKay,? 1998", "shortCiteRegEx": "MacKay", "year": 1998}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W. McCulloch", "W. Pitts"], "venue": "Bulletin of mathematical biology, 5(4):115\u2013133.", "citeRegEx": "McCulloch and Pitts,? 1943", "shortCiteRegEx": "McCulloch and Pitts", "year": 1943}, {"title": "Slice sampling covariance hyperparameters in latent Gaussian models", "author": ["I. Murray", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems 23.", "citeRegEx": "Murray and Adams,? 2010", "shortCiteRegEx": "Murray and Adams", "year": 2010}, {"title": "Bayesian Learning for Neural Networks", "author": ["R. Neal"], "venue": "Springer Verlag.", "citeRegEx": "Neal,? 1996", "shortCiteRegEx": "Neal", "year": 1996}, {"title": "Evaluation of Gaussian Processes and Other Methods for Non-linear Regression", "author": ["C.E. Rasmussen"], "venue": "PhD thesis, University of Toronto.", "citeRegEx": "Rasmussen,? 1996", "shortCiteRegEx": "Rasmussen", "year": 1996}, {"title": "Gaussian processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K. Williams"], "venue": "The MIT Press.", "citeRegEx": "Rasmussen and Williams,? 2006", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Principles of Neurodynamics", "author": ["F. Rosenblatt"], "venue": "Spartan Book.", "citeRegEx": "Rosenblatt,? 1962", "shortCiteRegEx": "Rosenblatt", "year": 1962}, {"title": "Learning representations by back-propagating errors", "author": ["D. Rumelhart", "G. Hinton", "R. Williams"], "venue": "Nature, 323(6088):533\u2013536.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Scalable Inference for Structured Gaussian Process Models", "author": ["Y. Saatchi"], "venue": "PhD thesis, University of Cambridge.", "citeRegEx": "Saatchi,? 2011", "shortCiteRegEx": "Saatchi", "year": 2011}, {"title": "Using deep belief nets to learn covariance kernels for Gaussian processes", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems, 20:1249\u20131256.", "citeRegEx": "Salakhutdinov and Hinton,? 2008", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2008}, {"title": "Interpolation of Spatial Data: Some Theory for Kriging", "author": ["M. Stein"], "venue": "Springer Verlag.", "citeRegEx": "Stein,? 1999", "shortCiteRegEx": "Stein", "year": 1999}, {"title": "Probabilistic inference in human semantic memory", "author": ["M. Steyvers", "T. Griffiths", "S. Dennis"], "venue": "Trends in Cognitive Sciences, 10(7):327\u2013334.", "citeRegEx": "Steyvers et al\\.,? 2006", "shortCiteRegEx": "Steyvers et al\\.", "year": 2006}, {"title": "How to grow a mind: Statistics, structure, and abstraction", "author": ["J. Tenenbaum", "C. Kemp", "T. Griffiths", "N. Goodman"], "venue": "Science, 331(6022):1279\u20131285.", "citeRegEx": "Tenenbaum et al\\.,? 2011", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2011}, {"title": "Bayesian inference: An introduction to principles and practice in machine learning", "author": ["M. Tipping"], "venue": "Advanced Lectures on Machine Learning, pages 41\u201362.", "citeRegEx": "Tipping,? 2004", "shortCiteRegEx": "Tipping", "year": 2004}, {"title": "Gaussian process regression networks", "author": ["A.G. Wilson", "D.A. Knowles", "Z. Ghahramani"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML).", "citeRegEx": "Wilson et al\\.,? 2012", "shortCiteRegEx": "Wilson et al\\.", "year": 2012}, {"title": "Vision as Bayesian inference: analysis by synthesis", "author": ["A. Yuille", "D. Kersten"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "Yuille and Kersten,? \\Q2006\\E", "shortCiteRegEx": "Yuille and Kersten", "year": 2006}], "referenceMentions": [{"referenceID": 18, "context": "The first machine learning models, such as the perceptron (Rosenblatt, 1962), were based on a simple model of a neuron (McCulloch and Pitts, 1943).", "startOffset": 58, "endOffset": 76}, {"referenceID": 13, "context": "The first machine learning models, such as the perceptron (Rosenblatt, 1962), were based on a simple model of a neuron (McCulloch and Pitts, 1943).", "startOffset": 119, "endOffset": 146}, {"referenceID": 17, "context": "These simple inference techniques became the cornerstone of subsequent Gaussian process models for machine learning (Rasmussen and Williams, 2006).", "startOffset": 116, "endOffset": 146}, {"referenceID": 17, "context": "Gaussian process models have become popular for non-linear regression and classification (Rasmussen and Williams, 2006), and often have impressive empirical performance (Rasmussen, 1996).", "startOffset": 89, "endOffset": 119}, {"referenceID": 16, "context": "Gaussian process models have become popular for non-linear regression and classification (Rasmussen and Williams, 2006), and often have impressive empirical performance (Rasmussen, 1996).", "startOffset": 169, "endOffset": 186}, {"referenceID": 13, "context": "The first machine learning models, such as the perceptron (Rosenblatt, 1962), were based on a simple model of a neuron (McCulloch and Pitts, 1943). Papers such as Rumelhart et al. (1986) inspired hope that it would be possible to develop intelligent agents with models like neural networks, which could automatically discover hidden representations in data.", "startOffset": 120, "endOffset": 187}, {"referenceID": 13, "context": "The first machine learning models, such as the perceptron (Rosenblatt, 1962), were based on a simple model of a neuron (McCulloch and Pitts, 1943). Papers such as Rumelhart et al. (1986) inspired hope that it would be possible to develop intelligent agents with models like neural networks, which could automatically discover hidden representations in data. Indeed, machine learning aims not only to equip humans with tools to analyze data, but to fully automate the learning and decision making process. Research on Gaussian processes (GPs) within the machine learning community developed out of neural networks research, triggered by Neal (1996), who observed that Bayesian neural networks became Gaussian processes as the number of hidden units approached infinity.", "startOffset": 120, "endOffset": 648}, {"referenceID": 13, "context": "The first machine learning models, such as the perceptron (Rosenblatt, 1962), were based on a simple model of a neuron (McCulloch and Pitts, 1943). Papers such as Rumelhart et al. (1986) inspired hope that it would be possible to develop intelligent agents with models like neural networks, which could automatically discover hidden representations in data. Indeed, machine learning aims not only to equip humans with tools to analyze data, but to fully automate the learning and decision making process. Research on Gaussian processes (GPs) within the machine learning community developed out of neural networks research, triggered by Neal (1996), who observed that Bayesian neural networks became Gaussian processes as the number of hidden units approached infinity. Neal (1996) conjectured that \u201cthere may be simpler ways to do inference in this case\u201d.", "startOffset": 120, "endOffset": 781}, {"referenceID": 12, "context": "Such simple smoothing devices are not a realistic replacement for neural networks, which were envisaged as intelligent agents that could discover hidden features in data via adaptive basis functions (MacKay, 1998).", "startOffset": 199, "endOffset": 213}, {"referenceID": 27, "context": "It has been suggested that the human ability for inductive reasoning \u2013 concept generalization with remarkably few examples \u2013 could derive from a prior combined with Bayesian inference (Yuille and Kersten, 2006; Tenenbaum et al., 2011; Steyvers et al., 2006).", "startOffset": 184, "endOffset": 257}, {"referenceID": 24, "context": "It has been suggested that the human ability for inductive reasoning \u2013 concept generalization with remarkably few examples \u2013 could derive from a prior combined with Bayesian inference (Yuille and Kersten, 2006; Tenenbaum et al., 2011; Steyvers et al., 2006).", "startOffset": 184, "endOffset": 257}, {"referenceID": 23, "context": "It has been suggested that the human ability for inductive reasoning \u2013 concept generalization with remarkably few examples \u2013 could derive from a prior combined with Bayesian inference (Yuille and Kersten, 2006; Tenenbaum et al., 2011; Steyvers et al., 2006).", "startOffset": 184, "endOffset": 257}, {"referenceID": 15, "context": "Bayesian nonparametric models, and Gaussian processes in particular, are an expressive way to encode prior knowledge, and also reflect the belief that the real world is infinitely complex (Neal, 1996).", "startOffset": 188, "endOffset": 200}, {"referenceID": 21, "context": "Expressive kernels have been developed by combining Gaussian processes in a type of Bayesian neural network structure (Salakhutdinov and Hinton, 2008; Wilson et al., 2012; Damianou and Lawrence, 2012).", "startOffset": 118, "endOffset": 200}, {"referenceID": 26, "context": "Expressive kernels have been developed by combining Gaussian processes in a type of Bayesian neural network structure (Salakhutdinov and Hinton, 2008; Wilson et al., 2012; Damianou and Lawrence, 2012).", "startOffset": 118, "endOffset": 200}, {"referenceID": 4, "context": "Expressive kernels have been developed by combining Gaussian processes in a type of Bayesian neural network structure (Salakhutdinov and Hinton, 2008; Wilson et al., 2012; Damianou and Lawrence, 2012).", "startOffset": 118, "endOffset": 200}, {"referenceID": 1, "context": "Sophisticated kernels are most often achieved by composing together a few standard kernel functions (Archambeau and Bach, 2011; Durrande et al., 2011; G\u00f6nen and Alpayd\u0131n, 2011; Rasmussen and Williams, 2006).", "startOffset": 100, "endOffset": 206}, {"referenceID": 5, "context": "Sophisticated kernels are most often achieved by composing together a few standard kernel functions (Archambeau and Bach, 2011; Durrande et al., 2011; G\u00f6nen and Alpayd\u0131n, 2011; Rasmussen and Williams, 2006).", "startOffset": 100, "endOffset": 206}, {"referenceID": 7, "context": "Sophisticated kernels are most often achieved by composing together a few standard kernel functions (Archambeau and Bach, 2011; Durrande et al., 2011; G\u00f6nen and Alpayd\u0131n, 2011; Rasmussen and Williams, 2006).", "startOffset": 100, "endOffset": 206}, {"referenceID": 17, "context": "Sophisticated kernels are most often achieved by composing together a few standard kernel functions (Archambeau and Bach, 2011; Durrande et al., 2011; G\u00f6nen and Alpayd\u0131n, 2011; Rasmussen and Williams, 2006).", "startOffset": 100, "endOffset": 206}, {"referenceID": 16, "context": "the fundamental differences between the proposed kernels and the popular alternatives in Rasmussen and Williams (2006). In particular, we show how the proposed kernels can automatically discover patterns and extrapolate on the CO2 dataset in Rasmussen and Williams (2006), on a synthetic dataset with strong negative covariances, on a difficult synthetic sinc pattern, and on airline passenger data.", "startOffset": 89, "endOffset": 119}, {"referenceID": 16, "context": "the fundamental differences between the proposed kernels and the popular alternatives in Rasmussen and Williams (2006). In particular, we show how the proposed kernels can automatically discover patterns and extrapolate on the CO2 dataset in Rasmussen and Williams (2006), on a synthetic dataset with strong negative covariances, on a difficult synthetic sinc pattern, and on airline passenger data.", "startOffset": 89, "endOffset": 272}, {"referenceID": 14, "context": "This marginal likelihood can be optimised to estimate hyperparameters such as `, or used to integrate out the hyperparameters via Markov chain Monte Carlo (Murray and Adams, 2010).", "startOffset": 155, "endOffset": 179}, {"referenceID": 13, "context": "This marginal likelihood can be optimised to estimate hyperparameters such as `, or used to integrate out the hyperparameters via Markov chain Monte Carlo (Murray and Adams, 2010). Detailed Gaussian process references include Rasmussen and Williams (2006), Stein (1999), and Cressie (1993).", "startOffset": 156, "endOffset": 256}, {"referenceID": 13, "context": "This marginal likelihood can be optimised to estimate hyperparameters such as `, or used to integrate out the hyperparameters via Markov chain Monte Carlo (Murray and Adams, 2010). Detailed Gaussian process references include Rasmussen and Williams (2006), Stein (1999), and Cressie (1993).", "startOffset": 156, "endOffset": 270}, {"referenceID": 3, "context": "Detailed Gaussian process references include Rasmussen and Williams (2006), Stein (1999), and Cressie (1993).", "startOffset": 94, "endOffset": 109}, {"referenceID": 22, "context": "Any stationary kernel (aka covariance function) can be expressed as an integral using Bochner\u2019s theorem (Bochner, 1959; Stein, 1999; Gikhman and Skorokhod, 2004):", "startOffset": 104, "endOffset": 161}, {"referenceID": 6, "context": "Any stationary kernel (aka covariance function) can be expressed as an integral using Bochner\u2019s theorem (Bochner, 1959; Stein, 1999; Gikhman and Skorokhod, 2004):", "startOffset": 104, "endOffset": 161}, {"referenceID": 2, "context": "If \u03c8 has a density S(s), then S is called the spectral density or power spectrum of k, and k and S are Fourier duals (Chatfield, 1989):", "startOffset": 117, "endOffset": 134}, {"referenceID": 10, "context": "Indeed, mixtures of Gaussians are dense in the set of all distribution functions (Kostantinos, 2000).", "startOffset": 81, "endOffset": 100}, {"referenceID": 8, "context": "H\u00f6rmander (1990)).", "startOffset": 0, "endOffset": 17}, {"referenceID": 16, "context": "These kernels are easy to interpret, and provide drop-in replacements for kernels in Rasmussen and Williams (2006). The weights wq specify the relative contribution of each mixture component.", "startOffset": 85, "endOffset": 115}, {"referenceID": 15, "context": ", Rasmussen and Williams (2006) and Abrahamsen (1997), which typically only provide smooth interpolation.", "startOffset": 2, "endOffset": 32}, {"referenceID": 0, "context": ", Rasmussen and Williams (2006) and Abrahamsen (1997), which typically only provide smooth interpolation.", "startOffset": 36, "endOffset": 54}, {"referenceID": 25, "context": "Automatic relevance determination (MacKay et al., 1994; Tipping, 2004) takes place during training, removing extraneous components from the proposed model, through the complexity penalty in the marginal likelihood (Rasmussen and Williams, 2006).", "startOffset": 34, "endOffset": 70}, {"referenceID": 17, "context": ", 1994; Tipping, 2004) takes place during training, removing extraneous components from the proposed model, through the complexity penalty in the marginal likelihood (Rasmussen and Williams, 2006).", "startOffset": 166, "endOffset": 196}, {"referenceID": 14, "context": "For a fully Bayesian treatment, the spectral density could alternatively be integrated out using Markov chain Monte Carlo (Murray and Adams, 2010), rather than choosing a point estimate.", "startOffset": 122, "endOffset": 146}, {"referenceID": 16, "context": "This dataset was used in Rasmussen and Williams (2006), and is frequently used in Gaussian process tutorials, to show how GPs are flexible statistical tools: a human can look at the data, recognize patterns, and then hard code those patterns into covariance kernels.", "startOffset": 25, "endOffset": 55}, {"referenceID": 16, "context": "This dataset was used in Rasmussen and Williams (2006), and is frequently used in Gaussian process tutorials, to show how GPs are flexible statistical tools: a human can look at the data, recognize patterns, and then hard code those patterns into covariance kernels. Rasmussen and Williams (2006) identify, by looking at the blue and green curves in Figure 1a, a long term rising trend, seasonal variation with possible decay away from periodicity, medium term irregularities, and noise, and hard code a stationary covariance kernel to represent each of these features.", "startOffset": 25, "endOffset": 297}, {"referenceID": 16, "context": ",Q \u2264 10, the SM kernel can closely recover the most popular stationary kernels catalogued in Rasmussen and Williams (2006).", "startOffset": 93, "endOffset": 123}, {"referenceID": 3, "context": "Although often used in geostatistics to guide choices of Gaussian process kernel functions (and parameters) (Cressie, 1993), the empirical autocorrelation function tends to be unreliable, particularly with a small amount of data (e.", "startOffset": 108, "endOffset": 123}, {"referenceID": 3, "context": "Although often used in geostatistics to guide choices of Gaussian process kernel functions (and parameters) (Cressie, 1993), the empirical autocorrelation function tends to be unreliable, particularly with a small amount of data (e.g., N < 1000), and at high lags (for \u03c4 > 10). In Figure 2a, the empirical autocorrelation function is erratic and does not resemble the Mat\u00e9rn kernel for \u03c4 > 10. Moreover, the squared exponential kernel cannot capture the heavy tails of the Mat\u00e9rn kernel, no matter what length-scale it has. The learned SM kernel more closely matches the Mat\u00e9rn kernel. Next, we reconstruct a mixture of the rational quadratic (RQ) and periodic kernels (PE) in Rasmussen and Williams (2006):", "startOffset": 109, "endOffset": 707}, {"referenceID": 15, "context": "Derivations for both the RQ and PE kernels in (14) and (15) are in Rasmussen and Williams (2006). Rational quadratic and Mat\u00e9rn kernels are also discussed in Abrahamsen (1997).", "startOffset": 67, "endOffset": 97}, {"referenceID": 0, "context": "Rational quadratic and Mat\u00e9rn kernels are also discussed in Abrahamsen (1997). We sample 100 points from a Gaussian process with kernel 10kRQ + 4kPE, with \u03b1 = 2, \u03c9 = 1/20, `RQ = 40, `PE = 1.", "startOffset": 60, "endOffset": 78}, {"referenceID": 16, "context": "All of the stationary covariance functions in the standard machine learning Gaussian process reference Rasmussen and Williams (2006) are everywhere positive, including the periodic kernel, k(\u03c4) = exp(\u22122 sin(\u03c0 \u03c4 \u03c9)/`).", "startOffset": 103, "endOffset": 133}, {"referenceID": 9, "context": "Figure 5a shows airline passenger numbers, recorded monthly, from 1949 to 1961 (Hyndman, 2005).", "startOffset": 79, "endOffset": 94}, {"referenceID": 14, "context": "In future work, one could integrate away a spectral density, using recently developed efficient MCMC for GP hyperparameters (Murray and Adams, 2010).", "startOffset": 124, "endOffset": 148}, {"referenceID": 20, "context": "Moreover, spectral representations of the proposed models suggest recent Toeplitz methods (Saatchi, 2011) could be applied for significant speedup in inference and predictions.", "startOffset": 90, "endOffset": 105}], "year": 2017, "abstractText": "Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density \u2013 the Fourier transform of a kernel \u2013 with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that we can reconstruct standard covariances within our framework.", "creator": "LaTeX with hyperref package"}}}