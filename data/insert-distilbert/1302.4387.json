{"id": "1302.4387", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2013", "title": "Online Learning with Switching Costs and Other Adaptive Adversaries", "abstract": "we study the power of different types of adaptive ( nonoblivious ) adversaries in the setting of prediction with expert advice, under both full information and bandit feedback. we measure the player's performance using a new notion of regret, also known as policy regret, which better captures the adversary's adaptiveness to the player'# s behavior. in a setting where losses are allowed to drift, we characterize - - - in a simple nearly complete manner - - - the power of adaptive adversaries with bounded memories and switching costs. in particular, we show that with switching costs, the attainable rate with bandit feedback is $ \\ theta ( t ^ { 2 / 3 } ) $. interestingly, this rate comparison is significantly worse than than the $ \\ theta ( \\ sqrt { t } ) $ rate attainable with switching costs in the full information case. via a novel reduction from experts to bandits, we also show that a bounded memory adversary can necessarily force $ \\ theta ( t ^ { 2 / 3 } ) $ regret ( even in the full large information case, proving that switching costs are easier to control than their bounded memory adversaries. our lower security bounds significantly rely on a new stochastic adversary strategy that generates loss processes with strong dependencies.", "histories": [["v1", "Mon, 18 Feb 2013 18:46:37 GMT  (154kb)", "https://arxiv.org/abs/1302.4387v1", null], ["v2", "Sat, 1 Jun 2013 09:35:15 GMT  (25kb)", "http://arxiv.org/abs/1302.4387v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["nicol\u00f2 cesa-bianchi", "ofer dekel", "ohad shamir"], "accepted": true, "id": "1302.4387"}, "pdf": {"name": "1302.4387.pdf", "metadata": {"source": "CRF", "title": "Online Learning with Switching Costs and Other Adaptive Adversaries", "authors": ["Nicol\u00f2 Cesa-Bianchi"], "emails": ["nicolo.cesa-bianchi@unimi.it", "oferd@microsoft.com", "ohad.shamir@weizmann.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 2.\n43 87\nv2 [\ncs .L\nG ]\n\u221a\nT ) rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also show that a bounded memory adversary can force \u0398\u0303(T 2/3) regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies."}, {"heading": "1 Introduction", "text": "An important instance of the framework of prediction with expert advice \u2014see, e.g., Cesa-Bianchi and Lugosi [2006]\u2014 is defined as the following repeated game, between a randomized player with a finite and fixed set of available actions and an adversary. At the beginning of each round of the game, the adversary assigns a loss to each action. Next, the player defines a probability distribution over the actions, draws an action from this distribution, and suffers the loss associated with that action. The player\u2019s goal is to accumulate loss at the smallest possible rate, as the game progresses. Two versions of this game are typically considered: in the full-information feedback version of the game, at the end of each round, the player observes the adversary\u2019s assignment of loss values to each action. In the bandit feedback version, the player only observes the loss associated with his chosen action, but not the loss values of other actions.\nWe assume that the adversary is adaptive (also called nonoblivious by Cesa-Bianchi and Lugosi [2006] or reactive by Maillard and Munos [2010]), which means that the adversary chooses the loss values on round t based on the player\u2019s actions on rounds 1 . . . t\u2212 1. We also assume that the adversary is deterministic and has unlimited computational power. These assumptions imply that the adversary can specify his entire strategy before the game begins. In other words, the adversary can perform all of the calculations needed to specify, in advance, how he plans to react on each round to any sequence of actions chosen by the player.\nMore formally, let A denote the finite set of actions and let Xt denote the player\u2019s random action on round t. We adopt the notation X1:t as shorthand for the sequence X1 . . . Xt. We assume that the adversary defines, in advance, a sequence of history-dependent loss functions f1, f2, . . .. The input to each loss function ft is the entire history of\nthe player\u2019s actions so far, therefore the player\u2019s loss on round t is ft(X1:t). Note that the player doesn\u2019t observe the functions ft, only the losses that result from his past actions. Specifically, in the bandit feedback model, the player observes ft(X1:t) on round t, whereas in the full-information model, the player observes ft(X1:t\u22121, x) for all x \u2208 A.\nOn any round T , we evaluate the player\u2019s performance so far using the notion of regret, which compares his cumulative loss on the first T rounds to the cumulative loss of the best fixed action in hindsight. Formally, the player\u2019s regret on round T is defined as\nRT =\nT\u2211\nt=1\nft(X1:t)\u2212min x\u2208A\nT\u2211\nt=1\nft(x . . . x) . (1)\nRT is a random variable, as it depends on the randomized action sequence X1:t. Therefore, we also consider the expected regret E[RT ]. This definition is the same as the one used in Merhav et al. [2002] and Arora et al. [2012] (in the latter, it is called policy regret), but differs from the more common definition of expected regret\nE\n[ T\u2211\nt=1\nft(X1:t)\u2212min x\u2208A\nT\u2211\nt=1\nft(X1:t\u22121, x)\n] . (2)\nThe definition in Eq. (2) is more common in the literature (e.g., Auer et al. [2002], McMahan and Blum [2004], Dani and Hayes [2006], Maillard and Munos [2010]), but is clearly inadequate for measuring a player\u2019s performance against an adaptive adversary. Indeed, if the adversary is adaptive, the quantity ft(X1:t\u22121, x)is hardly interpretable \u2014see Arora et al. [2012] for a more detailed discussion.\nIn general, we seek algorithms for which E[RT ] can be bounded by a sublinear function of T , implying that the per-round expected regret, E[RT ]/T , tends to zero. Unfortunately, Arora et al. [2012] shows that arbitrary adaptive adversaries can easily force the regret to grow linearly. Thus, we need to focus on (reasonably) weaker adversaries, which have constraints on the loss functions they can generate.\nThe weakest adversary we discuss is the oblivious adversary, which determines the loss on round t based only on the current action Xt. In other words, this adversary is oblivious to the player\u2019s past actions. Formally, the oblivious adversary is constrained to choose a sequence of loss functions that satisfies \u2200t, \u2200x1:t \u2208 At, and \u2200x\u20321:t\u22121 \u2208 At\u22121,\nft(x1:t) = ft(x \u2032 1:t\u22121, xt) . (3)\nThe majority of previous work in online learning focuses on oblivious adversaries. When dealing with oblivious adversaries, we denote the loss function by \u2113t and omit the first t\u2212 1 arguments. With this notation, the loss at time t is simply written as \u2113t(Xt).\nFor example, imagine an investor that invests in a single stock at a time. On each trading day he invests in one stock and suffers losses accordingly. In this example, the investor is the player and the stock market is the adversary. If the investment amount is small, the investor\u2019s actions will have no measurable effect on the market, so the market is oblivious to the investor\u2019s actions. Also note that this example relates to the full-information feedback version of the game, as the investor can see the performance of each stock at the end of each trading day.\nA stronger adversary is the oblivious adversary with switching costs. This adversary is similar to the oblivious adversary defined above, but charges the player an additional switching cost of 1 whenever Xt 6= Xt\u22121. More formally, this adversary defines his sequence of loss functions in two steps: first he chooses an oblivious sequence of loss functions, \u21131, \u21132 . . ., which satisfies the constraint in Eq. (3). Then, he sets f1(x) = \u21131(x), and\n\u2200 t \u2265 2, ft(x1:t) = \u2113t(xt) + I{xt 6=xt\u22121} . (4)\nThis is a very natural setting. For example, let us consider again the single-stock investor, but now assume that each trade has a fixed commission cost. If the investor keeps his position in a stock for multiple trading days, he is exempt from any additional fees, but when he sells one stock and buys another, he incurs a fixed commission. More generally, this setting (or simple generalizations of it) allows us to capture any situation where choosing a different action involves a costly change of state. In the paper, we will also discuss a special case of this adversary, where the loss function \u2113t(x) for each action is sampled i.i.d. from a fixed distribution.\nThe switching costs adversary defines the loss on round t as a function of Xt and Xt\u22121, and is therefore a special case of a more general adversary called an adaptive adversary with a memory of 1. This adversary is constrained to choose loss functions that satisfy \u2200t, \u2200x1:t \u2208 At, and \u2200x\u20321:t\u22122 \u2208 At\u22122,\nft(x1:t) = ft(x \u2032 1:t\u22122, xt\u22121, xt) . (5)\nThis adversary is more general than the switching costs adversary because his loss functions can depend on the previous action in an arbitrary way. We can further strengthen this adversary and define the bounded memory adaptive adversary, which has a bounded memory of an arbitrary size. In other words, this adversary is allowed to set his loss function based on the player\u2019s m most recent past actions, where m is a predefined parameter. Formally, the bounded memory adversary must choose loss functions that satisfy, \u2200t, \u2200x1:t \u2208 At, and \u2200x\u20321:t\u2212m\u22121 \u2208 At\u2212m\u22121,\nft(x1:t) = ft(x \u2032 1:t\u2212m\u22121, xt\u2212m:t) .\nIn the information theory literature, this setting is called individual sequence prediction against loss functions with memory Merhav et al. [2002].\nIn addition to the adversary types described above, the bounded memory adaptive adversary has additional interesting special cases. One of them is the delayed feedback oblivious adversary of Mesterharm [2005], which defines an oblivious loss sequence, but reveals each loss value with a delay of m rounds. Since the loss at time t depends on the player\u2019s action at time t\u2212m, this adversary is a special case of a bounded memory adversary with a memory of size m. The delayed feedback adversary is not a focus of our work, and we present it merely as an interesting special case.\nSo far, we have defined a succession of adversaries of different strengths. This paper\u2019s goal is to understand the upper and lower bounds on the player\u2019s regret when he faces these adversaries. Specifically, we focus on how the expected regret depends on the number of rounds, T , with either full-information or bandit feedback."}, {"heading": "1.1 The Current State of the Art", "text": "Different aspects of this problem have been previously studied and the known results are surveyed below and summarized in Table 1. Most of these previous results rely on the additional assumption that the range of the loss functions is bounded in a fixed interval, say [0, C]. We explicitly make note of this because our new results require us to slightly generalize this assumption.\nAs mentioned above, the oblivious adversary has been studied extensively and is the best understood of all the adversaries discussed in this paper. With full-information feedback, both the Hedge algorithm Littlestone and Warmuth [1994], Freund and Schapire [1997] and the follow the perturbed leader (FPL) algorithm Kalai and Vempala [2005] guarantee a regret of O( \u221a T ), with a matching lower bound of \u2126( \u221a T ) \u2014see, e.g., Cesa-Bianchi and Lugosi [2006]. Analyses of Hedge in settings where the loss range may vary over time have also been considered \u2014see, e.g., Cesa-Bianchi et al. [2007]. The oblivious setting with bandit feedback, where the player only observes the incurred loss ft(X1:t), is called the nonstochastic (or adversarial) multi-armed bandit problem. In this setting, the Exp3 algorithm of Auer et al. [2002] guarantees the same regret O( \u221a T ) as the full-information setting, and clearly the full-information lower bound \u2126( \u221a T ) still applies.\nThe follow the lazy leader (FLL) algorithm of Kalai and Vempala [2005] is designed for the switching costs setting with full-information feedback. The analysis of FLL guarantees that the oblivious component of the player\u2019s expected regret (without counting the switching costs), as well as the expected number of switches, is upper bounded byO( \u221a T ), implying an expected regret of at most O( \u221a T ).\nThe algorithm of Merhav et al. [2002] focuses on the bounded memory adversary with full-information feedback, referring to this problem as loss functions with memory, and guaranteeing a regret ofO(T 2/3). The work of Arora et al. [2012] extends this result to the bandit feedback case, maintaining the same regret bound.\nLearning with bandit feedback and switching costs has mostly been considered in the economics literature, using a different setting than ours and with prior knowledge assumptions (see Jun [2004] for an overview). The setting of stochastic oblivious adversaries (i.e., oblivious loss functions sampled i.i.d. from a fixed distribution) was first studied by Agrawal et al. [1988], where they show that O(log T ) switches are sufficient to asymptotically guarantee logarithmic regret. The paper Ortner [2010] achieves logarithmic regret nonasymptotically with O(logT ) switches.\nSeveral other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood. For example, even the Exp3 algorithm of Auer et al. [2002] has extensions to the \u201cadaptive\u201d adversary case, with a regret upper bound of O( \u221a T ). This bound doesn\u2019t contradict the \u2126(T ) lower bound for general adaptive adversaries mentioned earlier, since these papers use the regret defined in Eq. (2) rather than the regret used in our work, defined in Eq. (1).\nAnother related body of work lies in the field of competitive analysis \u2014see Borodin and El-Yaniv [1998], which also deals with loss functions that depend on the player\u2019s past actions, and the adversary\u2019s memory may even be unbounded. However, obtaining sublinear regret is generally impossible in this case. Therefore, competitive analysis studies much weaker performance metrics such as the competitive ratio, making it orthogonal to our work."}, {"heading": "1.2 Our Contribution", "text": "In this paper, we make the following contributions (see Table 1):\n\u2022 Our main technical contribution is a new lower bound on regret that matches the existing upper bounds in several of the settings discussed above. Specifically, our lower bound applies to the switching costs adversary with bandit feedback and to all strictly stronger adversaries.\n\u2022 Building on this lower bound, we prove another regret lower bound in the bounded memory setting with fullinformation feedback, again matching the known upper bound.\n\u2022 We confirm that existing upper bounds on regret hold in our setting and match the lower bounds up to logarithmic factors.\n\u2022 Despite the lower bound, we show that for switching costs and bandit feedback, if we also assume stochastic i.i.d. losses, then one can get a distribution-free regret bound of O( \u221a T log log logT ) for finite action sets, with only\nO(log logT ) switches. This result uses ideas from Cesa-Bianchi et al. [2013], and is deferred to Appendix A. Our new lower bound is a significant step towards a complete understanding of adaptive adversaries; observe that the upper and lower bounds in Table 1 essentially match in all but one of the settings.\nOur results have two important consequences. First, observe that the optimal regret against the switching costs adversary is \u0398 (\u221a T ) with full-information feedback, versus \u0398 ( T 2/3 ) with bandit feedback. To the best of our knowledge, this is the first theoretical confirmation that learning with bandit feedback is strictly harder than learning with full-information, even on a small finite action set and even in terms of the dependence on T (previous gaps we are aware of were either in terms of the number of actions Auer et al. [2002], or required large or continuous action spaces \u2014see, e.g., Bubeck et al. [2011], Shamir [2012]). Moreover, recall the regret bound of O (\u221a T log log log T ) against the stochastic i.i.d. adversary with switching costs and bandit feedback. This demonstrates that dependencies in the loss process must play a crucial role in controlling the power of the switching costs adversary. Indeed, the \u2126 ( T 2/3 ) lower bound proven in the next section heavily relies on such dependencies. Second, observe that in the full-information feedback case, the optimal regret against a switching costs adversary is \u0398( \u221a T ), whereas the optimal regret against the more general bounded memory adversary is \u2126(T 2/3). This is\nsomewhat surprising given the ideas presented in Merhav et al. [2002] and later extended in Arora et al. [2012]: The main technique used in these papers is to take an algorithm originally designed for oblivious adversaries, forcefully prevent it from switching actions very often, and obtain a new algorithm that guarantees a regret of O(T 2/3) against bounded memory adversaries. This would seem to imply that a small number of switches is the key to dealing with general bounded memory adversaries. Our result contradicts this intuition by showing that controlling the number of switches is easier then dealing with a general bounded memory adversary.\nAs noted above, our lower bounds require us to slightly weaken the standard technical assumption that loss values lie in a fixed interval [0, C]. We replace it with the following two assumptions:\n1. Bounded range. We assume that the loss values on each individual round are bounded in an interval of constant size C, but we allow this interval to drift from round to round. Formally, \u2200t, \u2200x1:t \u2208 At and \u2200x\u20321:t \u2208 At,\n\u2223\u2223ft(x1:t)\u2212 ft(x\u20321:t) \u2223\u2223 \u2264 C . (6)\n2. Bounded drift. We also assume that the drift of each individual action from round to round is contained in a bounded interval of size Dt, where Dt may grow slowly, as O (\u221a log(t) ) . Formally, \u2200t and \u2200x1:t \u2208 At,\n\u2223\u2223ft(x1:t)\u2212 ft+1(x1:t, xt) \u2223\u2223 \u2264 Dt . (7)\nSince these assumptions are a relaxation of the standard assumption, all of the known lower bounds on regret automatically extend to our relaxed setting. For our results to be consistent with the current state of the art, we must also prove that all of the known upper bounds continue to hold after the relaxation, up to logarithmic factors."}, {"heading": "2 Lower Bounds", "text": "In this section, we prove lower bounds on the player\u2019s expected regret in various settings.\n2.1 \u2126(T 2/3) with Switching Costs and Bandit Feedback\nWe begin with a \u2126(T 2/3) regret lower bound against an oblivious adversary with switching costs, when the player receives bandit feedback. It is enough to consider a very simple setting, with only two actions, labeled 1 and 2. Using the notation introduced earlier, we use \u21131, \u21132, . . . to denote the oblivious sequence of loss functions chosen by the adversary before adding the switching cost.\nTheorem 1. For any player strategy that relies on bandit feedback and for any number of rounds T , there exist loss functions f1, . . . , fT that are oblivious with switching costs, with a range bounded by C = 2, and a drift bounded by Dt = \u221a 3 log(t) + 16, such that E[RT ] \u2265 140T 2/3.\nThe full proof is given in Appendix B, and here we give an informal proof sketch. We begin by constructing a randomized adversarial strategy, where the loss functions \u21131, . . . , \u2113T are an instantiation of random variables Lt, . . . , LT defined as follows. Let \u03be1, . . . , \u03beT be i.i.d. standard Gaussian random variables (with zero mean and unit variance) and let Z be a random variable that equals \u22121 or 1 with equal probability. Using these random variables, define for all t = 1 . . . T\nLt(1) =\nt\u2211\ns=1\n\u03bes ,\nLt(2) = Lt(1) + ZT \u22121/3 . (8)\nIn words, {Lt(1)}Tt=1 is simply a Gaussian random walk and {Lt(2)}Tt=1 is the same random walk, slightly shifted up or down \u2014see figure 1 for an illustration. It is straightforward to confirm that this loss sequence has a bounded range, as required by the theorem: by construction we have |\u2113t(1) \u2212 \u2113t(2)| = T\u22121/3 \u2264 1 for all t, and since the switching cost can add at most 1 to the loss on each round, we conclude that |ft(1) \u2212 ft(2)| \u2264 2 for all t. Next, we\nshow that the expected regret of any player against this random loss sequence is \u2126(T 2/3), where expectation is taken over the randomization of both the adversary and the player. The intuition is that the player can only gain information about which action is better by switching between them. Otherwise, if he stays on the same action, he only observes a random walk, and gets no further information. Since the gap between the two losses on each round is T\u22121/3, the player must perform \u2126(T 2/3) switches before he can identify the better action. If the player performs that many switches, the total regret incurred due to the switching costs is \u2126(T 2/3). Alternatively, if the player performs o(T 2/3) switches, he can\u2019t identify the better action; as a result he suffers an expected regret of \u2126(T\u22121/3) on each round and a total regret of \u2126(T 2/3).\nSince the randomized loss sequence defined in Eq. (8), plus a switching cost, achieves an expected regret of \u2126(T 2/3), there must exist at least one deterministic loss sequence \u21131 . . . \u2113T with a regret of \u2126(T 2/3). In our proof, we show that there exists such \u21131 . . . \u2113T with bounded drift.\n2.2 \u2126(T 2/3) with Bounded Memory and Full-Information Feedback\nWe build on Thm. 1 and prove a \u2126(T 2/3) regret lower bound in the full-information setting, where we get to see the entire loss vector on every round. To get this strong result, we need to give the adversary a little bit of extra power: memory of size 2 instead of size 1 as in the case of switching costs. To show this result, we again consider a simple setting with two actions.\nTheorem 2. For any player strategy that relies on full-information feedback and for any number of rounds T \u2265 2, there exist loss functions f1, . . . , fT , each with a memory of size m = 2, a range bounded by C = 2, and a drift bounded by Dt = \u221a 3 log(t) + 18, such that E[RT ] \u2265 140 (T \u2212 1)2/3.\nThe formal proof is deferred to Appendix C and a proof sketch is given here. The proof is based on a reduction from full-information to bandit feedback that might be of independent interest. We construct the adversarial loss sequence as follows: on each round, the adversary assigns the same loss to both actions. Namely, the value of the loss depends only on the player\u2019s previous two actions, and not on his action on the current round. Recall that even in the full-information version of the game, the player doesn\u2019t know what the losses would have been had he chosen different actions in the past. Therefore, we have made the full-information game as difficult as the bandit game.\nSpecifically, we construct an oblivious loss sequence \u21131 . . . \u2113T as in Thm. 1 and define\nft(x1:t) = \u2113t\u22121(xt\u22121) + I{xt\u22121 6=xt\u22122} . (9)\nIn words, we define the loss on round t of the full-information game to be equal to the loss on round t\u2212 1 of a banditswith-switching-costs game in which the player chooses the same sequence of actions. This can be done with a memory of size 2, since the loss in Eq. (9) is fully specified by the player\u2019s choices on rounds t, t \u2212 1, t \u2212 2. Therefore, the \u2126(T 2/3) lower bound for switching costs and bandit feedback extends to the full-information setting with a memory of size at least 2."}, {"heading": "3 Upper Bounds", "text": "In this section, we show that the known upper bounds on regret, originally proved for bounded losses, can be extended to the case of losses with bounded range and bounded drift. Specifically, of the upper bounds that appear in Table 1, we prove the following:\n\u2022 O( \u221a T ) for an oblivious adversary with switching costs, with full-information feedback. \u2022 O\u0303( \u221a T ) for an oblivious adversary with bandit feedback (where O\u0303 hides logarithmic factors).\n\u2022 O\u0303(T 2/3) for a bounded memory adversary with bandit feedback. The remaining upper bounds in Table 1 are either trivial or follow from the principle that an upper bound still holds if we weaken the adversary or provide a more informative feedback.\n3.1 O( \u221a T ) with Switching Costs and Full-Information Feedback\nIn this setting, ft(x1:t) = \u2113t(xt)+ I{xt 6=xt\u22121}. If the oblivious losses \u21131 . . . \u2113T (without the additional switching costs) were all bounded in [0, 1], the Follow the Lazy Leader (FLL) algorithm of Kalai and Vempala [2005] would guarantee a regret of O( \u221a T ) with respect to these losses (again, without the additional switching costs). Additionally, FLL guarantees that its expected number of switches is O( \u221a T ). We use a simple reduction to extend these guarantees to loss functions with a range bounded in an interval of size C and with an arbitrary drift. On round t, after choosing an action and receiving the loss function \u2113t, the player defines the modified loss \u2113\u2032t(x) =\n1 C\u22121 ( \u2113t(x) \u2212miny \u2113t(y) ) and feeds it to the FLL algorithm. The FLL algorithm then chooses the next action.\nTheorem 3. If each of the loss functions f1, f2, . . . is oblivious with switching costs and has a range bounded by C then the player strategy described above attains O(C \u221a T ) expected regret.\nThe formal proof is given in Appendix D but the proof technique is quite straightforward. We first show that each \u2113\u2032t is bounded in [0, 1] and therefore the standard regret bound for FLL holds with respect to the sequence of modified loss functions \u2113\u20321, \u2113 \u2032 2, . . .. Then we show that the guarantees provided for FLL imply a regret of O( \u221a T ) with respect to the original loss sequence f1, f2, . . ..\n3.2 O\u0303( \u221a T ) with an Oblivious Adversary and Bandit Feedback\nIn this setting, ft(x1:t) simply equals \u2113t(xt). The reduction described in the previous subsection cannot be used in the bandit setting, since minx \u2113t(x) is unknown to the player, and a different reduction is needed. The player sets a fixed horizon T and focuses on controlling his regret at time T ; he can then use a standard doubling trick Cesa-Bianchi and Lugosi [2006] to handle an infinite horizon. The player uses the fact that each ft has a range bounded by C. Additionally, he defines D = maxt\u2264T Dt and on each round he defines the modified loss\nf \u2032t(x1:t) = 1\n2(C +D)\n( \u2113t(xt)\u2212 \u2113t\u22121(xt\u22121) ) + 1\n2 . (10)\nNote that f \u2032t(X1:t) can be computed by the player using only bandit feedback. The player then feeds f \u2032 t(X1:t) to\nan algorithm that guarantees a O( \u221a T ) standard regret (see definition in Eq. (2)) against an adaptive adversary. The Exp3.P algorithm, due to Auer et al. [2002], is such an algorithm. The player chooses his actions according to the choices made by Exp3.P. The following theorem states that this reduction results in a bandit algorithm that guarantees a regret of O\u0303( \u221a T ) against oblivious adversaries.\nTheorem 4. If each of the loss functions f1 . . . fT is oblivious with a range bounded by C and a drift bounded by Dt = O (\u221a log(t) ) then the player strategy described above attains O\u0303(C \u221a T ) expected regret.\nThe full proof is given in Appendix D. In a nutshell, we show that each f \u2032t is an adaptive loss function bounded in [0, 1] and therefore the analysis of Exp3.P guarantees a regret of O( \u221a T ) with respect to the loss sequence f \u20321 . . . f \u2032 T .\nThen, we show that this guarantee implies a regret of (C +D)O( \u221a T ) = O\u0303(C \u221a T ) with respect to the original loss sequence f1 . . . fT .\n3.3 O\u0303(T 2/3) with Bounded Memory and Bandit Feedback Proving an upper bound against an adversary with a memory of size m, with bandit feedback, requires a more delicate reduction. As in the previous section, we assume a finite horizon T and we let D = maxt Dt. Let K = |A| be the number of actions available to the player.\nSince fT (x1:t) depends only on the last m + 1 actions in x1:t, we slightly overload our notation and define ft(xt\u2212m:t) to mean the same as ft(x1:t). To define the reduction, the player fixes a base action x0 \u2208 A and for each t > m he defines the loss function\nf\u0302t(xt\u2212m:t) = 1\n2 ( C + (m+ 1)D\n)(ft(xt\u2212m:t)\u2212 ft\u2212m\u22121(x0 . . . x0) ) + 1\n2 .\nNext, he divides the T rounds into J consecutive epochs of equal length, where J = \u0398(T 2/3). We assume that the epoch length T/J is at least 2K(m+ 1), which is true when T is sufficiently large. At the beginning of each epoch, the player plans his action sequence for the entire epoch. He uses some of the rounds in the epoch for exploration and the rest for exploitation. For each action in A, the player chooses an exploration interval of 2(m + 1) consecutive rounds within the epoch. These K intervals are chosen randomly, but they are not allowed to overlap, giving a total of 2K(m+ 1) exploration rounds in the epoch. The details of how these intervals are drawn appears in our analysis, in Appendix D. The remaining T/J \u2212 2K(m+ 1) rounds are used for exploitation.\nThe player runs the Hedge algorithm Freund and Schapire [1997] in the background, invoking it only at the beginning of each epoch and using it to choose one exploitation action that will be played consistently on all of the exploitation rounds in the epoch. In the exploration interval for action x, the player first plays m + 1 rounds of the base action x0 followed by m + 1 rounds of the action x. Letting tx denote the first round in this interval, the player uses the observed losses ftx+m(x0 . . . x0) and ftx+2m+1(x . . . x) to compute f\u0302tx+2m+1(x . . . x). In our analysis, we show that the latter is an unbiased estimate of the average value of f\u0302t(x . . . x) over t in the epoch. At the end of the epoch, the K estimates are fed as feedback to the Hedge algorithm.\nWe prove the following regret bound, with the proof deferred to Appendix D.\nTheorem 5. If each of the loss functions f1 . . . fT is has a memory of size m, a range bounded by C, and a drift bounded by Dt = O (\u221a log(t) ) then the player strategy described above attains O\u0303(T 2/3) expected regret."}, {"heading": "4 Discussion", "text": "In this paper, we studied the problem of prediction with expert advice against different types of adversaries, ranging from the oblivious adversary to the general adaptive adversary. We proved upper and lower bounds on the player\u2019s regret against each of these adversary types, in both the full-information and the bandit feedback models. Our lower bounds essentially matched our upper bounds in all but one case: the adaptive adversary with a unit memory in the full-information setting, where we only know that regret is \u2126( \u221a T ) and O(T 2/3).\nOur new bounds have two important consequences. First, we characterize the regret attainable with switching costs, and show a setting where predicting with bandit feedback is strictly more difficult than predicting with fullinformation feedback \u2014even in terms of the dependence on T , and even on small finite action sets. Second, in the full-information setting, we show that predicting against a switching costs adversary is strictly easier than predicting against an arbitrary adversary with a bounded memory.\nTo obtain our results, we had to slightly relax the standard assumption that loss values are bounded in [0, 1]. Re-introducing this assumption and proving similar lower bounds remains an elusive open problem. Many other questions remain unanswered. Can we characterize the dependence of the regret on the size of the action set A? Can we strengthen any of our expected regret bounds to bounds that hold with high probability? Can any of our results be generalized to more sophisticated notions of regret, such as shifting regret and swap regret, as in Arora et al. [2012]?\nIn addition to the adversary types discussed in this paper, there are other interesting classes of adversaries that lie between the oblivious and the adaptive. One of these is the oblivious adversary with delayed feedback, briefly mentioned in the introduction. While some results for this adversary exist (see Mesterharm [2005]), the attainable regret, especially in the bandit feedback case, is not clear. Another interesting case is the family of deterministically\nadaptive adversaries, which includes adversaries that adapt to the player\u2019s actions (so they are not oblivious) in a known deterministic way, rather than in a secret malicious way. For example, imagine playing a multi-armed bandit game where the loss values are initially oblivious, but whenever the player chooses an arm with zero loss, the loss of the same arm on the next round is deterministically changed to zero. In other words, whenever the player suffers a zero loss, he knows that choosing the same arm again guarantees another zero loss. This is an important setting because many real-world online prediction scenarios are indeed deterministically adaptive."}, {"heading": "A Distribution-free regret bound for bandits with switching costs", "text": "In this appendix we adapt results of Cesa-Bianchi et al. [2013] to show a strategy that achieves O (\u221a T log log logT ) regret against any i.i.d. oblivious adversary in the bandit setting with switching costs, assuming a finite action set A = {1 . . .K}. The strategy used by this stochastic adversary is specified by a probability distribution over oblivious loss functions. The oblivious loss function for each step t = 1, 2, . . . is the realization on an independent draw Lt from this distribution. The regret of a player choosing actions X0 = X1, X2, . . . is defined by\nRT =\nT\u2211\nt=1\nEt [ Lt(Xt) + I{Xt 6=Xt\u22121} ] \u2212min\nx\u2208A\nT\u2211\nt=1\nE [ Lt(x) ]\nwhere the expectation E is over the random draw of each Lt and the possible randomization of the player, and the expectation Et is conditioned over X1, L1(X1), . . . , Xt\u22121, Lt\u22121(Xt\u22121).\nOur result focuses on loss distributions such that the law of each marginal L1(x) is subgaussian. A random variable Z is subgaussian if there exist constants b, c such that for any a > 0 P ( Z > EZ + a ) \u2264 be\u2212ca2 and P ( Z < EZ \u2212 a ) \u2264 be\u2212ca2 . One can then show that, for any i.i.d. sequence Z1, . . . , ZT of subgaussian random variables,\nP (\u2223\u2223\u2223\u2223\u2223 1 T T\u2211\nt=1\nZt \u2212 EZ1 \u2223\u2223\u2223\u2223\u2223 > \u221a 112b cT ln 1 \u03b4 ) \u2264 \u03b4 . (11)\nIn the following, we use the notation E [ Lt(x) ] = \u00b5(x) and \u00b5\u2217 = min\nx\u2208A \u00b5(x) .\nTheorem 6. Consider a finite action set A = {1 . . .K}. Then for each T there exists a deterministic player strategy for the bandit game with i.i.d. oblivious adversaries and switching costs, whose regret after T steps is O (\u221a T log log logT )\nwith high probability, provided the distribution of L1(x) is sugaussian for each x \u2208 A. Proof. Consider the following player that proceeds in stages. At each stage s = 1, 2, . . . , S, the player maintains a set As \u2286 A of active actions. Each action is played Ts/|As| times in a round-robin fashion, where Ts = T 1\u22122 \u2212s\nis the total number of plays in stage s and T is the known horizon. Note that the overall number of switches is at most KS, where\nS = min { j \u2208 N : j\u2211\ns=1\nTs \u2265 T } = O ( ln lnT ) .\nLet \u00b5\u0302s(x) the sample mean of losses for action x in stage s, and define\nx\u0302s = argmin x\u2208As \u00b5\u0302s(x)\nthe best empirical action in stage s. The sets As of active actions are defined as follows: A1 = A and\nAs = { x \u2208 Ai\u22121 : \u00b5\u0302s\u22121(x) \u2264 \u00b5\u0302s\u22121(x\u0302s\u22121) + 2Cs\u22121 }\nwhere\nCs =\n\u221a 112(b/c) K\nTs ln\nKS\n\u03b4 .\nNote that AS \u2286 \u00b7 \u00b7 \u00b7 \u2286 A1 by construction. Also, using (11) and the union bound we have that\nmax x\u2208As\n\u2223\u2223\u00b5\u0302s(x) \u2212 \u00b5(x) \u2223\u2223 \u2264 Cs (12)\nsimultaneously for all s = 1, . . . , S with probability at least 1\u2212 \u03b4. We claim the following.\nClaim 1. With probability at least 1\u2212 \u03b4,\nx\u2217 \u2208 S\u22c2\ns=1\nAs and 0 \u2264 \u00b5\u0302s(x\u2217)\u2212 \u00b5\u0302s(x\u0302s) \u2264 2Cs for all s = 1, . . . , S.\nProof of Claim. We prove the lemma by induction on s = 1, . . . , S. We first show that the base case s = 1 holds with probability at least 1\u2212 \u03b4/S. Then we show that if the claim holds for s\u2212 1, then it holds for s with probability at least 1 \u2212 \u03b4/S over all random events in stage s. Therefore, using a union bound over s = 1, . . . , S we get that the claim holds simultaneously for all s with probability at least 1\u2212 \u03b4.\nFor the base case s = 1 note that x\u2217 \u2208 A1 by definition, and thus \u00b5\u03021(x\u03021) \u2264 \u00b5\u03021(x\u2217) holds. Moreover, using (12) we obtain that\n\u00b5\u03021(x \u2217)\u2212 \u00b5(x\u2217) \u2264 C1 and \u00b5(x\u03021)\u2212 \u00b5\u03021(x\u03021) \u2264 C1\nholds with probability at least 1\u2212 \u03b4/S. Since \u00b5(x\u2217)\u2212 \u00b5(x\u03021) \u2264 0 by definition of x\u2217, we obtain\n0 \u2264 \u00b5\u03021(x\u2217)\u2212 \u00b5\u03021(x\u03021) \u2264 2C1 as required. We now prove the claim for s > 1 using the inductive assumption\nx\u2217 \u2208 As\u22121 and 0 \u2264 \u00b5\u0302s\u22121(x\u2217)\u2212 \u00b5\u0302s\u22121(x\u0302s\u22121) \u2264 2Cs\u22121 .\nThe inductive assumption directly implies that x\u2217 \u2208 As. Thus we have \u00b5\u0302i(x\u0302s) \u2264 \u00b5\u0302s(x\u2217), because x\u0302s minimizes \u00b5\u0302s over a set that contains x\u2217. The rest of the proof of the claim closely follows that of the base case s = 1.\nNow, for any s = 1, . . . , S and for any x \u2208 As we have that\n\u00b5(x)\u2212 \u00b5(x\u2217) \u2264 \u00b5\u0302s\u22121(x)\u2212 \u00b5(x\u2217) + Cs\u22121 by (12) \u2264 \u00b5\u0302s\u22121(x\u0302s\u22121)\u2212 \u00b5(x\u2217) + 3Cs\u22121 by definition of As\u22121, since x \u2208 As \u2286 As\u22121 \u2264 \u00b5\u0302s\u22121(x\u2217)\u2212 \u00b5(x\u2217) + 3Cs\u22121 since x\u0302s\u22121 minimizes \u00b5\u0302s\u22121 in As\u22121 \u2264 4Cs\u22121 by (12)\nholds with probability at least 1\u2212 \u03b4/S. Hence, recalling that T\u2211\nt=1\nI{Xt 6=Xt\u22121} \u2264 KS\nholds deterministically, the regret of the player over the T plays can be bounded as follows\nKS +\nT\u2211\nt=1\n( \u00b5(Xt)\u2212 \u00b5\u2217 ) = KS + S\u2211\ns=1\nTs |As|\n\u2211\nx\u2208As\n( \u00b5(x)\u2212 \u00b5\u2217 )\n= KS + T1 K\nK\u2211\ni=1\n( \u00b5(x) \u2212 \u00b5\u2217 ) + S\u2211\ns=2\nTs |As|\n\u2211\nx\u2208As\n( \u00b5(x) \u2212 \u00b5\u2217 )\n\u2264 KS + T1\u00b5\u2217 + S\u2211\ni=2\n4Ts\n\u221a 112(b/c) K\nTs ln\nKS\n\u03b4\n= KS + T1\u00b5 \u2217 + 4 \u221a 112(b/c)K ln KS\n\u03b4\nS\u2211\ns=2\nTs\u221a Ts\u22121\nNow, since T1 = \u221a T , Ts/ \u221a Ts\u22121 = \u221a T and S = O ( ln lnT ) , we obtain that with probability at least 1\u2212 \u03b4 the regret is at most of order\nK ln lnT + \u00b5\u2217 \u221a T + \u221a KT ( ln K\n\u03b4 + ln ln lnT\n)\nas desired."}, {"heading": "B Proof of Thm. 1", "text": "As mentioned in the text, we first consider the player\u2019s expected regret against a randomized adversary. Specifically, we define\n\u2200t Lt(1) = t\u2211\ns=1\n\u03bes and Lt(2) = Lt(1) + Z\u01eb ,\nwhere \u03be1 . . . \u03beT are independent standard Gaussians, Z equals \u22121 or 1 with equal probability, and \u01eb is the gap between the losses of the two actions (which will later be set to \u01eb = T\u22121/3).\nNext, we assume for now, without loss of generality, that the player is deterministic. A deterministic player chooses each actionXt as a deterministic function of the random losses suffered on the previous rounds,L1(X1) . . . Lt\u22121(Xt\u22121). We can make this assumption because any randomized player strategy can be seen as a distribution over deterministic player strategies, and since the randomization used by the adversary is independent of the player\u2019s strategy.\nIn the results below, P denotes the distribution of the randomized adversary. We also introduce the conditional distributions S = P(\u00b7 | Z > 0) (i.e., 1 is the better action) and Q = P(\u00b7 | Z < 0) (i.e., 2 is the better action). Since Z has an equal probability of being negative or positive, it holds that P = 12 (S+Q).\nWe begin with the following technical lemma.\nLemma 1. Let I{xt\u22121 6=xt} indicate whether the player switched actions on round t (and 1 for t = 1). Then for any event A,\n\u2223\u2223S(A)\u2212Q(A) \u2223\u2223 \u2264 \u01eb \u221a\u221a\u221a\u221aE [ T\u2211\nt=1\nI{Xt 6=Xt\u22121}\n]\nwhere the expectation in the right-hand side is with respect to P.\nProof. To show this, we use the chain rule for relative entropy, which implies\nDKL ( S \u2225\u2225 Q ) = T\u2211\nt=1\nDKL ( St\u22121 \u2225\u2225 Qt\u22121 )\n(13)\nwhere St\u22121 and Qt\u22121 denote the distributions of the player\u2019s loss Lt(xt) conditioned on L1, . . . , Lt\u22121, when the joint distribution of L1, . . . , LT is, respectively, S and Q.\nLet us focus on a particular termDKL ( St\u22121 \u2225\u2225Qt\u22121 )\nand a particular realization of the random losses L1, . . . , Lt\u22121. Since we assume a deterministic player strategy, for any such realization the player\u2019s choices x1:t are all determined, and we deterministically have that the player either switched or not at time t. If he did not switch, then Lt(xt) is distributed as Lt\u22121(xt\u22121)+\u03bet under both measures St\u22121 and Qt\u22121, so the relative entropy between them is zero. If he did switch, then Lt(xt) is distributed as Lt\u22121(xt\u22121)\u2212 \u01eb+ \u03be under St\u22121 (where the switch is towards the best action), and as Lt\u22121(xt\u22121) + \u01eb + \u03be under Qt\u22121 (where the switch is towards the worst action). Hence, the relative entropy is the same as two standard Gaussians whose means are shifted by 2\u01eb, namely 2\u01eb2. So overall, we can upper bound Eq. (13) by\n2\u01eb2 E\n[ T\u2211\nt=1\nI{Xt 6=Xt\u22121} \u2223\u2223\u2223\u2223Z > 0 ] . (14)\nUsing a similar argument, we also show that DKL ( Q \u2225\u2225 S ) is upper bounded by Eq. (14) in which the conditioning on Z > 0 is replaced by Z < 0. Then, Pinsker\u2019s inequality implies that \u2223\u2223S(A) \u2212Q(A) \u2223\u22232 is at most\n\u01eb2\n2\n( E [ T\u2211\nt=1\nI{Xt 6=Xt\u22121} \u2223\u2223\u2223\u2223Z > 0 ] + E [ T\u2211\nt=1\nI{Xt 6=Xt\u22121} \u2223\u2223\u2223\u2223Z < 0 ]) = \u01eb2E [ T\u2211\nt=1\nI{Xt 6=Xt\u22121}\n]\nwhich gives the desired bound.\nWith this lemma, we can prove a lower bound on the expected regret for randomized adversaries.\nLemma 2. By picking \u01eb = T\u22121/3, the expected regret of any deterministic player strategy, over the randomness of the adversary, is at least 110T 2/3.\nProof. Let A be the event that the worst action (action 2 if Z > 0, and 1 if Z < 0) was picked by the player at least T/2 times. Also, let ST = \u2211T t=1 I{Xt 6=Xt\u22121} be the number of switches the player performs. Then\nE[RT ] \u2265 E [ max { ST , \u01ebT\n2 I{A}\n}] \u2265 E [ 1\n2\n( ST + \u01ebT\n2 I{A}\n)] = 1\n2 E[ST ] +\n\u01ebT\n4 P(A) .\nMoreover, letting A1 denote the event that the player chose action 1 at least T/2 times, and letting A2 denote the event that the player chose action 2 at least T/2 times, we have P(A) = 12 ( S(A2) +Q(A1) ) . Substituting this, we get\n1 2 E[ST ] + \u01ebT 8\n( S(A2) +Q(A1) ) .\nUsing Lemma 1 to lower bound Q(A1) via S(A1), we get a lower bound of\n1 2 E[ST ] + \u01ebT 8\n( S(A2) + S(A1)\u2212 \u01eb \u221a E[ST ] ) \u2265 1\n2 E[ST ] +\n\u01ebT\n8\n( S(A1 \u222a A2)\u2212 \u01eb \u221a E[ST ] )\n= 1\n2 E[ST ] +\n\u01ebT\n8\n( 1\u2212 \u01eb \u221a E[ST ] ) = 1\n2 E[ST ]\u2212\n\u01eb2T\n8\n\u221a E[ST ] + \u01ebT\n8 ,\nwhere we used a union bound and the fact that either A1 or A2 always holds. This is a quadratic function of \u221a E[ST ], and it is easily verified that the lowest possible value it can attain (for any value of E[ST ]) is\n\u01ebT 8 \u2212 \u01eb\n4T 2 128 .\nPicking \u01eb = T\u22121/3, this equals ( 1 8 \u2212 1128 ) T 2/3 > 110T 2/3.\nThe lemma above tells us that for the randomized adversary strategy we have devised, the expected regret for any deterministic player is at least 110T\n2/3. This implies that there exist some deterministic adversarial strategy, for which the expected regret of any possibly randomized player is at least 110T\n2/3. However, we are not done yet, since this strategy doesn\u2019t guarantee that the losses have bounded drift: In our case, the variation is governed by a potentially unbounded Gaussian random variable, so the deterministic adversary strategy that we picked might have an arbitrarily large drift. So now, our goal will be to show that there exists some deterministic adversarial strategy for which the expected regret is large, and the variation is bounded. To do this, the plan is to show that the probabilities (over the adversary\u2019s strategy) of the two events are large, summing to a number larger than one. This means there is some realization of the losses such that both events occur. We first state and prove two auxiliary lemmas, and then provide two more fundamental lemmas which together give us the required result.\nLemma 3. Let Y be a random variable in [\u2212b, b] (where b > 0), and E[Y ] \u2265 c for some c \u2208 [0, b/2]. Then we have\nP (Y \u2265 c/2) \u2265 c 2b\u2212 c \u2265 c 2b .\nProof.\nc \u2264 E[Y ] = P(Y \u2265 c/2)E[Y | Y \u2265 c/2] + P(Y < c/2)E[Y | Y < c/2] \u2264 P(Y \u2265 c/2)b+ ( 1\u2212 P(Y \u2265 c/2) ) c/2\nSolving for P(Y \u2265 c/2) gives the desired result.\nLemma 4. Let \u03be1, \u03be2, . . . be an infinite sequence of independent standard Gaussian random variables. Then for any \u03b4 \u2208 (0, 1)\nP ( \u2203t : |\u03bet| \u2265 \u221a 3 log(2t/\u03b4) ) \u2264 \u03b4.\nProof. By a standard Gaussian tail bound, we have that P(|\u03bet| > x) \u2264 exp(\u2212x2/2) for any x \u2265 0. This implies that\nP(|\u03bet| \u2265 \u221a 3 log(2t/\u03b4)) \u2264\n( \u03b4\n2t\n)3/2 .\nBy a union bound, we get that\nP ( \u2203t : |\u03bet| \u2265 \u221a 3 log(2t/\u03b4) ) \u2264 \u221e\u2211\nt=1\n( \u03b4\n2t\n)3/2 \u2264 \u03b43/2 < \u03b4.\nLemma 5. For any (possibly randomized) player strategy, it holds that\nP ( Eplayer[RT ] \u2265 1\n40 T 2/3\n) \u2265 1\n40 ,\nwhere P is over the adversary\u2019s randomization, and Eplayer[RT ] is the player\u2019s expected regret (over the player\u2019s randomization).\nProof. By Lemma 2, we already know that\nE [ Eplayer[RT ] ] \u2265 1\n10 T 2/3, (15)\nsince if we have a T 2/3/10 lower bound on the regret for any deterministic player strategy, the same holds for any randomized player strategy. Our approach is to apply Lemma 3 in order to convert this into a probability lower bound as in the lemma statement. However, we cannot apply Lemma 3 as-is, since Eplayer[RT ] can be as large as \u2126(T ), and the resulting bound is too weak. Instead, we show that there exists a different player strategy, with expected regret E\np\u0303layer [RT ], such that |Ep\u0303layer[RT ]| is always at most 2T 2/3 and\nE p\u0303layer\n[RT ] \u2264 2 Eplayer[RT ] (16)\nfor any realization of the adversary\u2019s random strategy. Also, analogous to Eq. (15), we have E[Ep\u0303layer[RT ]] \u2265 1 10T 2/3 by Lemma 2. Therefore, using Eq. (16) and Lemma 3, we get that\nP ( Eplayer[RT ] \u2265 1\n40 T 2/3\n) \u2265 P ( E\np\u0303layer [RT ] \u2265\n1\n20 T 2/3\n) \u2265 1\n40\nas required. The new player strategy we consider depends on the horizon T , and is very simple: It is identical to the original player strategy, but whenever the number of action switches reaches \u230aT 2/3\u230b, the player \u201cfreezes\u201d in its current action, and keeps playing the same action till T rounds are elapsed. Clearly, the number of switches with this strategy can never be more than T 2/3, and since the regret in terms of the loss \u2113t at each round is either 0 or T\u22121/3, we get that the total regret RT can never be more than T 2/3 + T \u2217 T\u22121/3 = 2T 2/3.\nTo prove Eq. (16), we consider some instantiation of the adversary\u2019s random strategy, and note that for any realization of the player\u2019s random coin tosses, the regret can only differ between the two strategies if ST (the total number of switches) is at least \u230aT 2/3\u230b. Therefore, we have Pplayer ( ST < \u230aT 2/3\u230b ) = P\np\u0303layer\n( ST < \u230aT 2/3\u230b ) ,\nPplayer ( ST \u2265 \u230aT 2/3\u230b ) = Pp\u0303layer ( ST \u2265 \u230aT 2/3\u230b ) and Eplayer[RT |ST < \u230aT 2/3\u230b] = Ep\u0303layer[RT |ST < \u230aT 2/3\u230b]. Also, we recall that RT \u2265 0 with the adversary strategy that we consider (since one action is always worse than the other action at all rounds). Finally, we note that if ST \u2265 \u230aT 2/3\u230b, then the regret for both strategies is at least \u230aT 2/3\u230b (since with the\nadversary strategy that we consider, the number of switches is a lower bound on the regret). Using these observations, we have\nEp\u0303layer[RT ]\n= Pp\u0303layer(ST < \u230aT 2/3\u230b)Ep\u0303layer[RT |ST < \u230aT 2/3\u230b] + Pp\u0303layer(ST \u2265 \u230aT 2/3\u230b)Ep\u0303layer[RT |ST \u2265 \u230aT 2/3\u230b] \u2264 P\np\u0303layer (ST < \u230aT 2/3\u230b)Ep\u0303layer[RT |ST < \u230aT 2/3\u230b] + P p\u0303layer (ST \u2265 \u230aT 2/3\u230b)2T 2/3\n= Pplayer(ST < \u230aT 2/3\u230b)Eplayer[RT |ST < \u230aT 2/3\u230b] + Pplayer(ST \u2265 \u230aT 2/3\u230b)2T 2/3 \u2264 2 ( Pplayer(ST < \u230aT 2/3\u230b)Eplayer[RT |ST < \u230aT 2/3\u230b] + Pplayer(ST \u2265 \u230aT 2/3\u230b)T 2/3 ) \u2264 2 ( Pplayer(ST < \u230aT 2/3\u230b)Eplayer[RT |ST < \u230aT 2/3\u230b] + Pplayer(ST \u2265 \u230aT 2/3\u230b)Eplayer[RT |ST \u2265 \u230aT 2/3\u230b] )\n= 2 Eplayer[RT ],\nwhere in the second-to-last step we used the fact that if ST \u2265 \u230aT 2/3\u230b, then the regret is at least \u230aT 2/3\u230b, plus we must have picked the worst action (worst by T\u22121/3 than the best action) at least \u2126(T 2/3) times, hence the total regret is certainly at least T 2/3.\nFinally, we use Lemma 4 with \u03b4 = 1/80, to get that with probability at least 1 \u2212 1/80, the drift factor Dt of the adversarial strategy is at most \u221a 3 log(160t) \u2264 \u221a 3 log(t) + 16 for all t. Moreover, Lemma 5 tells us that Eplayer[RT ] is at least 140T 2/3 with probability at least 1/40. This implies that the intersection of the two events is non-empty, and\nthere exists some deterministic adversarial strategy, such that the drift Dt \u2264 \u221a 3 log(t) + 16 for all t, and the expected regret is at least 140T 2/3 as required."}, {"heading": "C Proof of Thm. 2", "text": "Thm. 1 guarantees that given any player\u2019s strategy, there is some deterministic adversary strategy with a lower bound on the regret. However, as part of proving Thm. 1, we actually showed that there exists some randomized adversary strategy {f\u0302t}Tt=1 with memory size 1, such that for any (possibly randomized) player strategy x1:t,\nE\n[ T\u2211\nt=1\nf\u0302t(Xt\u22121, Xt)\u2212min x\u2208A\nT\u2211\nt=1\nf\u0302t(x, x)\n] \u2265 1\n10 T 2/3 (17)\n(see Lemma 2). We now use this strategy to define a randomized adversary strategy for our setting (with memory size 2), for a game of T + 1 rounds. We let f1(x1) = 0 for any x1, f2(x1, x2) = f\u03021(x1), and for every t = 3 . . . T + 1,\nft(xt\u22122, xt\u22121, xt) = f\u0302t\u22121(xt\u22122, xt\u22121) . (18)\nNow, suppose we had some (possibly randomized) player strategy X1 . . . XT+1, so that in expectation over the player and adversary strategies, we have\nE\n[ T+1\u2211\nt=1\nft(Xt\u22122, Xt\u22121, Xt)\u2212min x\u2208A\nT+1\u2211\nt=1\nft(x, x, x)\n] < 1\n10 T 2/3.\nIn particular, since f1 is always 0, it would imply that\nE\n[ T+1\u2211\nt=2\nft(Xt\u22122, Xt\u22121, Xt)\u2212min x\u2208A\nT+1\u2211\nt=2\nft(x, x, x)\n] < 1\n10 T 2/3 .\nBy Eq. (18), this implies\nE\n[ T\u2211\nt=1\nf\u0302t(Xt\u22121, Xt)\u2212min x\u2208A\nT\u2211\nt=1\nf\u0302t(x, x)\n] < 1\n10 T 2/3 .\nThus, if we could implement the player strategy X1 . . . XT in the bandits-with-switching-costs setting, it will contradict Eq. (17). To see that this indeed can happen, note that each Xt is a (possibly randomized) function of X1:t\u22121 as well as {f\u03c4 (X\u03c4\u22122, X\u03c4\u22121, X\u03c4 )}t\u22121\u03c4=1. But again, due to Eq. (18) and the fact that f1 is always 0, Xt can in fact be defined using X1:t\u22121 and\n{ f\u03c4 (X\u03c4\u22122, X\u03c4\u22121, X\u03c4 ) }t\u22121 \u03c4=2 = { f\u0302\u03c4\u22121(X\u03c4\u22122, X\u03c4\u22121) }t\u22121 \u03c4=2 .\nThe right hand side is an observable quantity in the bandit setting: In each round t, we know what are the set of losses {f\u0302\u03c4\u22121(X\u03c4\u22122, X\u03c4\u22121)}t\u22121\u03c4=2 that we obtained. Thus, we can simulate the strategy x1:t in the bandit-with-switching-costs setting, and get an expected regret smaller than 110T\n2/3, contradicting Eq. (17). Thus, the expected regret (for a game of T + 1 rounds) must be at least 110T\n2/3. Substituting T instead of T + 1, we get that the expected regret for a game with T rounds is at least 110 (T \u2212 1)2/3.\nThe regret bound we just now obtained is in expectation over the randomized adversary strategy, and holds for any player\u2019s strategy. We now use the same line of argument as in the last part of Thm. 1\u2019s proof, to show that for any (possibly randomized) player\u2019s strategy, there exists some deterministic adversary strategy, with a similar expected regret bound, and with losses of bounded drift. Specifically, a result completely analogous to Lemma 5 implies that\nP ( Eplayer[RT ] \u2265 1\n40 (T \u2212 1)2/3\n) \u2265 1\n40 ( T \u2212 1 T )2/3 ,\nwhich is at least 1/80 for any T > 1 (if T = 1 the bound in the theorem is trivial from the non-negativity of RT for the adversary strategy that we consider). Moreover, using Eq. (4) as in the proof of Thm. 1, the probability of the loss drift being at most \u221a 3 log(320t) \u2264 \u221a 3 log(t) + 18 is at least 1 \u2212 1/160. Thus, the intersection of the two events is not empty, and this implies that there exists some deterministic adversary strategy causing expected regret \u2265 140 (T \u2212 1)2/3, and loss drift at most \u221a 3 log(t) + 18 for all t."}, {"heading": "D Proofs of Upper Bounds", "text": "Proof of Thm. 3. Each loss functions equals ft(x1:t) = \u2113(xt) + I{xt 6=xt\u22121}, where \u2113t is an oblivious loss function. Since the range of ft is contained in an interval of size C, the range of \u2113t must be contained in an interval of size C \u2212 1. In other words,\n\u2200x \u2208 A \u2113t(x)\u2212min y \u2113t(y) \u2264 C \u2212 1 .\nTherefore, by definition, the range of \u2113\u2032t is contained in the interval [0, 1], and the analysis of the FLL algorithm holds. Namely, if X1, X2, . . . is the sequence of actions chosen by FLL, then, for any T\nE\n[ T\u2211\nt=1\n\u2113\u2032t(Xt)\n] \u2212min\nx\u2208A\nT\u2211\nt=1\n\u2113\u2032t(x) = O( \u221a T ) , (19)\nand\nE\n[ T\u2211\nt=1\nI{Xt 6=Xt\u22121}\n] = O( \u221a T ) . (20)\nPlugging the definition of \u2113\u2032t into Eq. (19) and rearranging terms, we get\nE\n[ T\u2211\nt=1\n\u2113t(Xt)\n] \u2212min\nx\u2208A \u2113t(x) = (C \u2212 1)O(\n\u221a T ) .\nSumming the above with Eq. (20) gives\nE\n[ T\u2211\nt=1\nft(X1:t)\n] \u2212min\nx\u2208A ft(x . . . x) = O(C\n\u221a T ) .\nProof of Thm. 4. Recall that ft(x1:t) = 12(C+D) (\u2113t(xt)\u2212 \u2113t\u22121(xt\u22121)) + 12 , and note that our assumptions imply that\n|\u2113t(xt)\u2212 \u2113t\u22121(xt\u22121)| = |\u2113t(xt)\u2212 \u2113t\u22121(xt) + \u2113t\u22121(xt)\u2212 \u2113t\u22121(xt\u22121)| \u2264 |\u2113t(xt)\u2212 \u2113t\u22121(xt)|+ |\u2113t\u22121(xt)\u2212 \u2113t\u22121(xt\u22121)| \u2264 D + C .\nTherefore, f \u2032t(x1:t) is always bounded in [0, 1] and the analysis of Exp3.P holds. Although f \u2032 t is not an oblivious loss, the standard regret bounds for Exp3.P holds against adaptive adversaries. Namely, if X1:T is the sequence of actions chosen by Exp3.P, then\nE\n[ T\u2211\nt=1\nf \u2032t(X1:t)\u2212min x\u2208A\nT\u2211\nt=1\nf \u2032t(X1:t\u22121, x)\n] = O( \u221a T ) .\nUsing the definition if f \u2032t , the left hand side above can be rewritten as\n1\n2(C +D) E\n[ T\u2211\nt=1\n( \u2113t(Xt)\u2212 \u2113t\u22121(Xt\u22121) ) \u2212min\nx\u2208A\nT\u2211\nt=1\n( \u2113t(x) \u2212 \u2113t\u22121(Xt\u22121)\n) ]\n= 1\n2(C +D) E\n[ T\u2211\nt=1\n\u2113t(Xt)\u2212min x\u2208A\nT\u2211\nt=1\n\u2113t(x)\n] .\nTherefore,\nE[RT ] = E\n[ T\u2211\nt=1\n\u2113t(Xt)\u2212min x\u2208A\nT\u2211\nt=1\n\u2113t(x)\n] = 2(C +D)O( \u221a T ) .\nUsing the assumption that Dt = O (\u221a log(T ) ) , we conclude that E[RT ] = O\u0303(C \u221a T ).\nProof of Thm. 5. First, note that, due to the bounded range and drift assumptions, f\u0302t \u2208 [0, 1]. Also note that\nft(xt\u2212m:t)\u2212 ft(x . . . x) = 2 ( C + (m+ 1)D )( f\u0302t(xt\u2212m:t)\u2212 f\u0302t(x . . . x) ) .\nAs previously mentioned, we divide the T rounds into J consecutive epochs of the same length T/J , where T/J \u2265 2K(m+ 1), plus an additional final epoch of length at most T/J . We let tj denote the index of the first round in the j-th epoch. We run a mini-batched version of the Hedge algorithm Freund and Schapire [1997] over the epochs: at the beginning of each epoch j, Hedge draws an action Xj \u2208 A which is played consistently throughout the epoch. Now assume that at the end of each epoch j, loss estimates gj(x) \u2208 [0, 1] for each action x are available such that\nE [ gj(x) ] =\n1\nT/J \u2212 2m\u2212 1\ntj+1\u22121\u2211\nt=tj+2m+1\nf\u0302t(x . . . x)\nwhere the randomness used to compute each gj is independent of that used by Hedge to draw Xj . At the end of epoch\nj, we feed loss estimates gj(x) for each x \u2208 A to Hedge. The resulting regret can be bounded as follows,\nT\u2211\nt=1\nE [ ft(Xt\u2212m:t)\u2212 ft(x . . . x) ]\n\u2264 J\u2211\nj=1\ntj+1\u22121\u2211\nt=tj\nE [ ft(Xt\u2212m:t)\u2212 ft(x . . . x) ] + CT\nJ\n= 2 ( C + (m+ 1)D ) J\u2211\nj=1\ntj+2m\u2211\nt=tj\nE [ f\u0302t(Xt\u2212m:t)\u2212 f\u0302t(x . . . x) ]\n+ 2 ( C + (m+ 1)D ) J\u2211\nj=1\ntj+1\u22121\u2211\nt=tj+2m+1\nE [ f\u0302t(Xj . . .Xj)\u2212 f\u0302t(x . . . x) ] + CT\nJ\n\u2264 2 ( C + (m+ 1)D ) (2m+ 1)J\n+ 2 ( C + (m+ 1)D )T J E\n  J\u2211\nj=1\nE [ gj(Xj)\u2212 gj(x) \u2223\u2223\u2223Xj ]  + CT\nJ\n= 2 ( C + (m+ 1)D ) (2m+ 1)J\n+ 2 ( C + (m+ 1)D )T J E\n  J\u2211\nj=1\n( gj(Xj)\u2212 gj(x) )  + CT\nJ\n\u2264 2 ( C + (m+ 1)D ) (2m+ 1)J + 4 ( C + (m+ 1)D )T J \u221a J lnK + CT J .\nIn the last step we applied the known upper bound on the regret of Hedge with respect to losses gj \u2208 [0, 1], where K is the number of actions. This is valid if, in particular, losses gj are oblivious. We now explain how to obtain oblivious estimates gj with the desired properties. At the beginning of each epoch j, we use the independent randomization to draw K exploration steps {tx : x \u2208 A} from the set Tj = {tj , . . . , tj+1 \u2212 2m \u2212 2} with the property that these steps are well separated. Namely, between any two tx and tx\u2032 there are at least 2m + 1 consecutive free time steps in Tj . During epoch j, when we arrive at step tx we freeze Hedge and play action x0 for m + 1 time steps, then we play action x for m+ 1 more time steps. We use the two observed losses ftx+m(x0 . . . x0) and ftx+2m+1(x . . . x) to compute f\u0302tx+2m+1(x . . . x). Because the tx are well separated, the exploration steps do not interfere with each other. Suppose now that we can draw these points such that the marginal of each tx is uniform in Tj . Then\nE [ f\u0302tx+2m+1(x . . . x) ] =\n1\nT/J \u2212 2m\u2212 1\ntj+1\u22122m\u22122\u2211\nt=tj\nf\u0302t+2m+1(x . . . x)\n= 1\nT/J \u2212 2m\u2212 1\ntj+1\u22121\u2211\nt=tj+2m+1\nf\u0302t(x . . . x) .\nThis shows that f\u0302tx+2m+1(x . . . x) is a valid estimate gj(x). Moreover, for each x \u2208 A the quantity f\u0302tx+2m+1(x . . . x) does not depend on Hedge\u2019s action Xj for the current epoch j. It does not even depend on Hedge\u2019s past actions. Hence, Hedge is indeed run on a set of oblivious losses and the standard regret bound applies.\nThe last thing to prove is that we can draw {tx : x \u2208 A} \u2282 Tj such that the marginal of each tx is uniform in Tj . Note that giving equal probability to all well separated configurations of {tx : x \u2208 A} does not work, because the times steps closer to the beginning and to the end of Tj appear in more configurations (for example, check the case |Tj | = 8 and m = 1). This problem can be fixed simply by arranging the points of Tj on a circle, so that the first point tj follows the last point tj+1 \u2212 2m \u2212 2, and then enforcing well-separatedness on the circle. This makes the sample space completely symmetric, excluding those configurations of exploration points that exploited border effects.\nThe additional regret due to the computation of the K exploration points is 2(m + 1)CK per epoch. The final regret, including these additional costs, is then bounded by\n2 ( C + (m+ 1)D ) (2m+ 1)J + 4 ( C + (m+ 1)D )T J \u221a J lnK + CT J + 2(m+ 1)CKJ .\nChoosing J of order T 2/3 concludes the proof."}], "references": [{"title": "Asymptotically efficient adaptive allocation rules for the multiarmed bandit problem with switching cost", "author": ["R. Agrawal", "M.V. Hedge", "D. Teneketzis"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Agrawal et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 1988}, {"title": "Online bandit learning against an adaptive adversary: from regret to policy regret", "author": ["R. Arora", "O. Dekel", "A. Tewari"], "venue": "In Proceedings of the Twenty-Ninth International Conference on Machine Learning,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Online computation and competitive analysis", "author": ["A. Borodin", "R. El-Yaniv"], "venue": null, "citeRegEx": "Borodin and El.Yaniv.,? \\Q1998\\E", "shortCiteRegEx": "Borodin and El.Yaniv.", "year": 1998}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz"], "venue": "Machine Learning,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2007}, {"title": "Regret minimization for reserve prices in second-price auctions", "author": ["N. Cesa-Bianchi", "C. Gentile", "Y. Mansour"], "venue": "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2013}, {"title": "Robbing the bandit: Less regret in online geometric optimization against an adaptive adversary", "author": ["V. Dani", "T.P. Hayes"], "venue": "In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Dani and Hayes.,? \\Q2006\\E", "shortCiteRegEx": "Dani and Hayes.", "year": 2006}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "A survey on the bandit problem with switching costs", "author": ["T. Jun"], "venue": "De Economist,", "citeRegEx": "Jun.,? \\Q2004\\E", "shortCiteRegEx": "Jun.", "year": 2004}, {"title": "Efficient algorithms for online decision problems", "author": ["A. Kalai", "S. Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Adaptive bandits: Towards the best history-dependent strategy", "author": ["O. Maillard", "R. Munos"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Maillard and Munos.,? \\Q2010\\E", "shortCiteRegEx": "Maillard and Munos.", "year": 2010}, {"title": "Online geometric optimization in the bandit setting against an adaptive adversary", "author": ["H.B. McMahan", "A. Blum"], "venue": "In Proceedings of the Seventeenth Annual Conference on Learning Theory,", "citeRegEx": "McMahan and Blum.,? \\Q2004\\E", "shortCiteRegEx": "McMahan and Blum.", "year": 2004}, {"title": "Sequential strategies for loss functions with memory", "author": ["N. Merhav", "E. Ordentlich", "G. Seroussi", "M.J. Weinberger"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Merhav et al\\.,? \\Q1947\\E", "shortCiteRegEx": "Merhav et al\\.", "year": 1947}, {"title": "Online learning with delayed label feedback", "author": ["C. Mesterharm"], "venue": "In Proceedings of the Sixteenth International Conference on Algorithmic Learning Theory,", "citeRegEx": "Mesterharm.,? \\Q2005\\E", "shortCiteRegEx": "Mesterharm.", "year": 2005}, {"title": "Online regret bounds for Markov decision processes with deterministic transitions", "author": ["R. Ortner"], "venue": "Theoretical Computer Science,", "citeRegEx": "Ortner.,? \\Q2010\\E", "shortCiteRegEx": "Ortner.", "year": 2010}, {"title": "On the complexity of bandit and derivative-free stochastic convex optimization", "author": ["O. Shamir"], "venue": "CoRR, abs/1209.2388,", "citeRegEx": "Shamir.,? \\Q2012\\E", "shortCiteRegEx": "Shamir.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": ", Cesa-Bianchi and Lugosi [2006]\u2014 is defined as the following repeated game, between a randomized player with a finite and fixed set of available actions and an adversary.", "startOffset": 2, "endOffset": 33}, {"referenceID": 4, "context": ", Cesa-Bianchi and Lugosi [2006]\u2014 is defined as the following repeated game, between a randomized player with a finite and fixed set of available actions and an adversary. At the beginning of each round of the game, the adversary assigns a loss to each action. Next, the player defines a probability distribution over the actions, draws an action from this distribution, and suffers the loss associated with that action. The player\u2019s goal is to accumulate loss at the smallest possible rate, as the game progresses. Two versions of this game are typically considered: in the full-information feedback version of the game, at the end of each round, the player observes the adversary\u2019s assignment of loss values to each action. In the bandit feedback version, the player only observes the loss associated with his chosen action, but not the loss values of other actions. We assume that the adversary is adaptive (also called nonoblivious by Cesa-Bianchi and Lugosi [2006] or reactive by Maillard and Munos [2010]), which means that the adversary chooses the loss values on round t based on the player\u2019s actions on rounds 1 .", "startOffset": 2, "endOffset": 970}, {"referenceID": 4, "context": ", Cesa-Bianchi and Lugosi [2006]\u2014 is defined as the following repeated game, between a randomized player with a finite and fixed set of available actions and an adversary. At the beginning of each round of the game, the adversary assigns a loss to each action. Next, the player defines a probability distribution over the actions, draws an action from this distribution, and suffers the loss associated with that action. The player\u2019s goal is to accumulate loss at the smallest possible rate, as the game progresses. Two versions of this game are typically considered: in the full-information feedback version of the game, at the end of each round, the player observes the adversary\u2019s assignment of loss values to each action. In the bandit feedback version, the player only observes the loss associated with his chosen action, but not the loss values of other actions. We assume that the adversary is adaptive (also called nonoblivious by Cesa-Bianchi and Lugosi [2006] or reactive by Maillard and Munos [2010]), which means that the adversary chooses the loss values on round t based on the player\u2019s actions on rounds 1 .", "startOffset": 2, "endOffset": 1011}, {"referenceID": 13, "context": "This definition is the same as the one used in Merhav et al. [2002] and Arora et al.", "startOffset": 47, "endOffset": 68}, {"referenceID": 1, "context": "[2002] and Arora et al. [2012] (in the latter, it is called policy regret), but differs from the more common definition of expected regret", "startOffset": 11, "endOffset": 31}, {"referenceID": 1, "context": ", Auer et al. [2002], McMahan and Blum [2004], Dani and Hayes [2006], Maillard and Munos [2010]), but is clearly inadequate for measuring a player\u2019s performance against an adaptive adversary.", "startOffset": 2, "endOffset": 21}, {"referenceID": 1, "context": ", Auer et al. [2002], McMahan and Blum [2004], Dani and Hayes [2006], Maillard and Munos [2010]), but is clearly inadequate for measuring a player\u2019s performance against an adaptive adversary.", "startOffset": 2, "endOffset": 46}, {"referenceID": 1, "context": ", Auer et al. [2002], McMahan and Blum [2004], Dani and Hayes [2006], Maillard and Munos [2010]), but is clearly inadequate for measuring a player\u2019s performance against an adaptive adversary.", "startOffset": 2, "endOffset": 69}, {"referenceID": 1, "context": ", Auer et al. [2002], McMahan and Blum [2004], Dani and Hayes [2006], Maillard and Munos [2010]), but is clearly inadequate for measuring a player\u2019s performance against an adaptive adversary.", "startOffset": 2, "endOffset": 96}, {"referenceID": 1, "context": "Indeed, if the adversary is adaptive, the quantity ft(X1:t\u22121, x)is hardly interpretable \u2014see Arora et al. [2012] for a more detailed discussion.", "startOffset": 93, "endOffset": 113}, {"referenceID": 1, "context": "Indeed, if the adversary is adaptive, the quantity ft(X1:t\u22121, x)is hardly interpretable \u2014see Arora et al. [2012] for a more detailed discussion. In general, we seek algorithms for which E[RT ] can be bounded by a sublinear function of T , implying that the per-round expected regret, E[RT ]/T , tends to zero. Unfortunately, Arora et al. [2012] shows that arbitrary adaptive adversaries can easily force the regret to grow linearly.", "startOffset": 93, "endOffset": 345}, {"referenceID": 14, "context": "In the information theory literature, this setting is called individual sequence prediction against loss functions with memory Merhav et al. [2002]. In addition to the adversary types described above, the bounded memory adaptive adversary has additional interesting special cases.", "startOffset": 127, "endOffset": 148}, {"referenceID": 14, "context": "In the information theory literature, this setting is called individual sequence prediction against loss functions with memory Merhav et al. [2002]. In addition to the adversary types described above, the bounded memory adaptive adversary has additional interesting special cases. One of them is the delayed feedback oblivious adversary of Mesterharm [2005], which defines an oblivious loss sequence, but reveals each loss value with a delay of m rounds.", "startOffset": 127, "endOffset": 358}, {"referenceID": 2, "context": "With full-information feedback, both the Hedge algorithm Littlestone and Warmuth [1994], Freund and Schapire [1997] and the follow the perturbed leader (FPL) algorithm Kalai and Vempala [2005] guarantee a regret of O( \u221a T ), with a matching lower bound of \u03a9( \u221a T ) \u2014see, e.", "startOffset": 57, "endOffset": 88}, {"referenceID": 2, "context": "With full-information feedback, both the Hedge algorithm Littlestone and Warmuth [1994], Freund and Schapire [1997] and the follow the perturbed leader (FPL) algorithm Kalai and Vempala [2005] guarantee a regret of O( \u221a T ), with a matching lower bound of \u03a9( \u221a T ) \u2014see, e.", "startOffset": 89, "endOffset": 116}, {"referenceID": 2, "context": "With full-information feedback, both the Hedge algorithm Littlestone and Warmuth [1994], Freund and Schapire [1997] and the follow the perturbed leader (FPL) algorithm Kalai and Vempala [2005] guarantee a regret of O( \u221a T ), with a matching lower bound of \u03a9( \u221a T ) \u2014see, e.", "startOffset": 89, "endOffset": 193}, {"referenceID": 1, "context": ", Cesa-Bianchi and Lugosi [2006]. Analyses of Hedge in settings where the loss range may vary over time have also been considered \u2014see, e.", "startOffset": 2, "endOffset": 33}, {"referenceID": 1, "context": ", Cesa-Bianchi and Lugosi [2006]. Analyses of Hedge in settings where the loss range may vary over time have also been considered \u2014see, e.g., Cesa-Bianchi et al. [2007]. The oblivious setting with bandit feedback, where the player only observes the incurred loss ft(X1:t), is called the nonstochastic (or adversarial) multi-armed bandit problem.", "startOffset": 2, "endOffset": 169}, {"referenceID": 0, "context": "In this setting, the Exp3 algorithm of Auer et al. [2002] guarantees the same regret O( \u221a T ) as the full-information setting, and clearly the full-information lower bound \u03a9( \u221a T ) still applies.", "startOffset": 39, "endOffset": 58}, {"referenceID": 0, "context": "In this setting, the Exp3 algorithm of Auer et al. [2002] guarantees the same regret O( \u221a T ) as the full-information setting, and clearly the full-information lower bound \u03a9( \u221a T ) still applies. The follow the lazy leader (FLL) algorithm of Kalai and Vempala [2005] is designed for the switching costs setting with full-information feedback.", "startOffset": 39, "endOffset": 267}, {"referenceID": 0, "context": "In this setting, the Exp3 algorithm of Auer et al. [2002] guarantees the same regret O( \u221a T ) as the full-information setting, and clearly the full-information lower bound \u03a9( \u221a T ) still applies. The follow the lazy leader (FLL) algorithm of Kalai and Vempala [2005] is designed for the switching costs setting with full-information feedback. The analysis of FLL guarantees that the oblivious component of the player\u2019s expected regret (without counting the switching costs), as well as the expected number of switches, is upper bounded byO( \u221a T ), implying an expected regret of at most O( \u221a T ). The algorithm of Merhav et al. [2002] focuses on the bounded memory adversary with full-information feedback, referring to this problem as loss functions with memory, and guaranteeing a regret ofO(T ).", "startOffset": 39, "endOffset": 635}, {"referenceID": 0, "context": "The work of Arora et al. [2012] extends this result to the bandit feedback case, maintaining the same regret bound.", "startOffset": 12, "endOffset": 32}, {"referenceID": 0, "context": "The work of Arora et al. [2012] extends this result to the bandit feedback case, maintaining the same regret bound. Learning with bandit feedback and switching costs has mostly been considered in the economics literature, using a different setting than ours and with prior knowledge assumptions (see Jun [2004] for an overview).", "startOffset": 12, "endOffset": 311}, {"referenceID": 0, "context": "from a fixed distribution) was first studied by Agrawal et al. [1988], where they show that O(log T ) switches are sufficient to asymptotically guarantee logarithmic regret.", "startOffset": 48, "endOffset": 70}, {"referenceID": 0, "context": "from a fixed distribution) was first studied by Agrawal et al. [1988], where they show that O(log T ) switches are sufficient to asymptotically guarantee logarithmic regret. The paper Ortner [2010] achieves logarithmic regret nonasymptotically with O(logT ) switches.", "startOffset": 48, "endOffset": 198}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood.", "startOffset": 76, "endOffset": 95}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood.", "startOffset": 76, "endOffset": 118}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood.", "startOffset": 76, "endOffset": 145}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood.", "startOffset": 76, "endOffset": 170}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood. For example, even the Exp3 algorithm of Auer et al. [2002] has extensions to the \u201cadaptive\u201d adversary case, with a regret upper bound of O( \u221a T ).", "startOffset": 76, "endOffset": 310}, {"referenceID": 2, "context": "Several other papers discuss online learning against \u201cadaptive\u201d adversaries Auer et al. [2002], Dani and Hayes [2006], Maillard and Munos [2010], McMahan and Blum [2004], but these results are not relevant to our work and can be easily misunderstood. For example, even the Exp3 algorithm of Auer et al. [2002] has extensions to the \u201cadaptive\u201d adversary case, with a regret upper bound of O( \u221a T ). This bound doesn\u2019t contradict the \u03a9(T ) lower bound for general adaptive adversaries mentioned earlier, since these papers use the regret defined in Eq. (2) rather than the regret used in our work, defined in Eq. (1). Another related body of work lies in the field of competitive analysis \u2014see Borodin and El-Yaniv [1998], which also deals with loss functions that depend on the player\u2019s past actions, and the adversary\u2019s memory may even be unbounded.", "startOffset": 76, "endOffset": 720}, {"referenceID": 4, "context": "This result uses ideas from Cesa-Bianchi et al. [2013], and is deferred to Appendix A.", "startOffset": 28, "endOffset": 55}, {"referenceID": 2, "context": "To the best of our knowledge, this is the first theoretical confirmation that learning with bandit feedback is strictly harder than learning with full-information, even on a small finite action set and even in terms of the dependence on T (previous gaps we are aware of were either in terms of the number of actions Auer et al. [2002], or required large or continuous action spaces \u2014see, e.", "startOffset": 316, "endOffset": 335}, {"referenceID": 2, "context": "To the best of our knowledge, this is the first theoretical confirmation that learning with bandit feedback is strictly harder than learning with full-information, even on a small finite action set and even in terms of the dependence on T (previous gaps we are aware of were either in terms of the number of actions Auer et al. [2002], or required large or continuous action spaces \u2014see, e.g., Bubeck et al. [2011], Shamir [2012]).", "startOffset": 316, "endOffset": 415}, {"referenceID": 2, "context": "To the best of our knowledge, this is the first theoretical confirmation that learning with bandit feedback is strictly harder than learning with full-information, even on a small finite action set and even in terms of the dependence on T (previous gaps we are aware of were either in terms of the number of actions Auer et al. [2002], or required large or continuous action spaces \u2014see, e.g., Bubeck et al. [2011], Shamir [2012]).", "startOffset": 316, "endOffset": 430}, {"referenceID": 13, "context": "somewhat surprising given the ideas presented in Merhav et al. [2002] and later extended in Arora et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 1, "context": "[2002] and later extended in Arora et al. [2012]: The main technique used in these papers is to take an algorithm originally designed for oblivious adversaries, forcefully prevent it from switching actions very often, and obtain a new algorithm that guarantees a regret of O(T ) against bounded memory adversaries.", "startOffset": 29, "endOffset": 49}, {"referenceID": 9, "context": "lT (without the additional switching costs) were all bounded in [0, 1], the Follow the Lazy Leader (FLL) algorithm of Kalai and Vempala [2005] would guarantee a regret of O( \u221a T ) with respect to these losses (again, without the additional switching costs).", "startOffset": 118, "endOffset": 143}, {"referenceID": 4, "context": "The player sets a fixed horizon T and focuses on controlling his regret at time T ; he can then use a standard doubling trick Cesa-Bianchi and Lugosi [2006] to handle an infinite horizon.", "startOffset": 126, "endOffset": 157}, {"referenceID": 2, "context": "P algorithm, due to Auer et al. [2002], is such an algorithm.", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": "The player runs the Hedge algorithm Freund and Schapire [1997] in the background, invoking it only at the beginning of each epoch and using it to choose one exploitation action that will be played consistently on all of the exploitation rounds in the epoch.", "startOffset": 36, "endOffset": 63}, {"referenceID": 1, "context": "Can we characterize the dependence of the regret on the size of the action set A? Can we strengthen any of our expected regret bounds to bounds that hold with high probability? Can any of our results be generalized to more sophisticated notions of regret, such as shifting regret and swap regret, as in Arora et al. [2012]? In addition to the adversary types discussed in this paper, there are other interesting classes of adversaries that lie between the oblivious and the adaptive.", "startOffset": 303, "endOffset": 323}, {"referenceID": 1, "context": "Can we characterize the dependence of the regret on the size of the action set A? Can we strengthen any of our expected regret bounds to bounds that hold with high probability? Can any of our results be generalized to more sophisticated notions of regret, such as shifting regret and swap regret, as in Arora et al. [2012]? In addition to the adversary types discussed in this paper, there are other interesting classes of adversaries that lie between the oblivious and the adaptive. One of these is the oblivious adversary with delayed feedback, briefly mentioned in the introduction. While some results for this adversary exist (see Mesterharm [2005]), the attainable regret, especially in the bandit feedback case, is not clear.", "startOffset": 303, "endOffset": 653}, {"referenceID": 5, "context": "A Distribution-free regret bound for bandits with switching costs In this appendix we adapt results of Cesa-Bianchi et al. [2013] to show a strategy that achieves O (\u221a T log log logT )", "startOffset": 103, "endOffset": 130}, {"referenceID": 8, "context": "We run a mini-batched version of the Hedge algorithm Freund and Schapire [1997] over the epochs: at the beginning of each epoch j, Hedge draws an action Xj \u2208 A which is played consistently throughout the epoch.", "startOffset": 53, "endOffset": 80}], "year": 2013, "abstractText": "We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback. We measure the player\u2019s performance using a new notion of regret, also known as policy regret, which better captures the adversary\u2019s adaptiveness to the player\u2019s behavior. In a setting where losses are allowed to drift, we characterize \u2014in a nearly complete manner\u2014 the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switching costs, the attainable rate with bandit feedback is \u0398\u0303(T ). Interestingly, this rate is significantly worse than the \u0398( \u221a T ) rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also show that a bounded memory adversary can force \u0398\u0303(T ) regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies.", "creator": "LaTeX with hyperref package"}}}