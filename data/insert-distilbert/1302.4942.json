{"id": "1302.4942", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "Implementation of Continuous Bayesian Networks Using Sums of Weighted Gaussians", "abstract": "adaptive bayesian networks provide a method of representing conditional data independence between random variables and computing the random probability distributions associated with these random variables. in this paper, we extend bayesian network structures to essentially compute probability density functions for smooth continuous random variables. we make convenient this extension by approximating prior and rounding conditional densities using successive sums of weighted gaussian initial distributions and then finding the propagation rules for updating the densities in terms of these weights. we present a simple example that illustrates the bayesian network inequality for continuous variables ; this example shows the effect of the network structure and approximation mapping errors on the computation of densities for variables in describing the network.", "histories": [["v1", "Wed, 20 Feb 2013 15:20:02 GMT  (339kb)", "http://arxiv.org/abs/1302.4942v1", "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)"]], "COMMENTS": "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eric driver", "darryl morrell"], "accepted": false, "id": "1302.4942"}, "pdf": {"name": "1302.4942.pdf", "metadata": {"source": "CRF", "title": "Implementation of Continuous Bayesian Networks Using Sums of Weighted Gaussians", "authors": ["Eric Driver", "Darryl Morrell"], "emails": [], "sections": [{"heading": null, "text": "1 INTRODUCTION\nBayesian networks provide a method of represent ing conditional independence relationships between random variables and of computing the probabil ity distributions associated with these random vari ables. Bayesian networks were originally developed for discrete-valued random variables; there is an increas ing interest in extending this approach to continuous valued random variables. Previous work on networks of continuous-valued variables has required that the variables have Gaussian density functions and that the relationships between these variables be linear(Pearl 1988, Shachter 1989). Theoretical issues involving the representation of relationships between random vari ables using a network (directed or undirected graph) consisting of both continuous and discrete random variables, including tests for conditional independence, are addressed in (Lauritzen 1989, Lauritzen 1990); an approach to updating conditional probabilities in which a single Gaussian is used to approximate a mix ture of Gaussian densities is given in (Spiegelhalter 1990). In this paper, we extend Bayesian networks\nFigure 1: Fragment of a singly-connected tree.\nto include random variables with arbitrary distribu tions. We make this extension by approximating prior and conditional densities using sums of weighted Gaus sian distributions; to our knowledge, this is the first time this approximation technique has been used in Bayesian networks. Using this technique, we find prop agation rules for updating the densities of the network variables; information propagates through the network in the form of messages consisting of weight, mean, and variance updates.\n2 PRELIMINARIES\nWe implement Bayesian networks for continuous vari ables by approximating both prior and conditional density functions by sums of weighted Gaussian den sities. A density so approximated is represented in terms of a set of weights, a set of means, and a set of variances. Computation of these approximations is a hard problem which we discuss in Section 5.\nFigure 1 shows a fragment of a Bayesian network. For our implementation, we assume that the network is singly connected (i.e. at most one path connects any two variables) and we assume that X is related to its\nparent nodes U1, U2, \u00b7 \u00b7 \u00b7, Un by (1)\nwhere g is an arbitrary function and Wx is a noise term assumed to be Gaussian with zero mean and uncorre lated with any other noise term or node in the network. In section 3.5 we consider the case when g is linear. From ( 1), we have:\nwhere N(x;u2,J.L) is the Gaussian distribution defined by: [ ( )2 ] def 1 - X- f-L N(x;u2,J.L) = --- exp \ufffd 2u2 (3) We approximate f(xJu1, . . . , un) by a weighted sum of Gaussians as follows:\nM f(xJul, . . . ,un) \ufffd LciN(x;a},f-L\ufffd))\u00b7\nj=l\n\u00b7 N(u1\u00b7u\ufffd f-L(j))\u00b7\u00b7\u00b7N(u \u00b7u\ufffd f-L(j)) (4) ' 1' Ut n' 1' Un where M is the number of Gaussians used to approxi mate f (xJut, . . . , un). Note that the approximation in ( 4) is capable of representing conditional density func tions that do not embody the relationship between X and U1, U2, . . . , Un given in (1); thus, this approach can be used in more general settings than considered here. Again, we refer to Section 5 for a discussion of how the weights { Cj}, means { f-L\ufffdj}, and variances { uJ} can be determined to approximate a given con ditional density function.\nUsing this approximation, we show that all belief func tions and messages can be represented as sums of weighted Gaussians. Before proceeding, we state with out proof the following identity which will be useful in the upcoming derivations:\n(5)\nwhere a = N(J.Ll; u12 + u22, f..L2) is a constant with re spect to x.\n3 THE PROPAGATION RULES\nWe first consider an arbitrary node X shown in Fig ure 1. The parents of X are U1, U2, . . . , Un and the children of X are Y1, Y2, . .. , Ym. We assume to begin that X has not been instantiated. The case when X has been instantiated and also the case when X has no parents or no children is considered in Section 3.4.\n3.1 COMPUTATION OF BEL(X)\nLet e1 denote the evidence in the subnetwork above X and let ex denote the evidence in the subnet work below X. Also, let et (i = 1,2, .. . ,n) de note the evidence coming to X via node U;, and let ej(j = 1,2, .. . ,m) denote the evidence coming to X via node }j. We assume X has received messages 7rx(u;)(i = 1 , 2, ... ,n) from its parents; each message is a sum of weighted Gaussians:\nk;=l (6)\nwhere M; is the number of Gaussian densities in the representation for 1r x ( u;), { aL } is a set of real valued weights, { (u\ufffd.k,r} is a set of real valued vari-\nances, and {f-L\ufffd,k;} is a set of real valued means. Sim ilarly, we assume X has received messages Ay;(x)(j = 1, 2, ... , m) from its children:\n( ) def ( J ) AY; x = f ej x\n= { P; Lf{N (x;(a{,1/,f-LtiJ 1;=1 1 if ej :/= 0 if ej = 0\n(7)\nwhere Pi is the number of Gaussian densities in the representation for Ay; ( x), {,ef;} is a set of real valued weights, { ( a{,l;) 2 } is a set of real valued variances,\nand {f-L{.I;} is a set of real valued means.\nThe belief function for X is computed as follows:\nBEL(x) def f(xJe) f(xJe1,ei) af(xJe1 )f(ex Jx) a1r(x)..\\(x), (8)\nwhere a = f(e)(llef) is a normalization constant. We compute 1r( x) as follows: 7r(x) def f(xJe1)\n1 \u00b7 \u00b7 \u00b71 f(xJe:k, u1, . . . , un) \u00b7 U1 Un. -f(ul, . .. , unJe1 )du1 \u00b7 \u00b7 \u00b7dun\n1 \u00b7\u00b7\u00b71 f(xJul, . . . ,un) \u00b7 U1 Un n \u00b7IT f( u; Jet )du1 \u00b7 \u00b7 \u00b7dun i=l\nM -1. \u00b7 \u00b7 \u00b71. '\"\"' c \u00b7 N(x \u00b7 (f\ufffd uU))N(u1\u00b7 (f\ufffd \"(j)) L..J J ' J 'rx ' J 'ru1 Ut Un j=1 n M;\n... N(u \u00b7 (f\ufffd \"(j)) \u00b7 IT '\"\"' a'k . . \u00b7 n, J ' ,.....Un L..J 1 i=1 k;=1 \u00b7N ( u;; ((f\ufffd,ky, J-l\ufffd,k.) du1 \u00b7 \u00b7 \u00b7 dun\nM M, Mn = LciN (x;(fJ,J-tV)) L \u00b7 \u00b7 \u00b7 L j=1 kt=1 kn=1\nM\nft at 1 N ( Ui; (ff, J-l\ufffd.>) \u00b7 i=l Ui \u00b7N ( u;; ((f\ufffd,ky, J-l\ufffd,kJ du;\n- '\"\"' \"'. N (x\u00b7 (f\ufffd \ufffd\ufffd(j)) L..J 1) ' J 'rx ' j=1 (9)\nwhere we have defined:\n'Yi def M 1 M n IT n\n1. ( ) Cj L \u00b7 \u00b7 \u00b7 'L: . ai, \" N u;; (fJ, J-l\ufffd,> kt=1 kn=1\u2022=1 '\n= n M; IT'\"' iN ( (i). 2+( i )2 i ) Cj L..J ak, J-l.,,,(fj <T1r,k; ,J-l1r,k, i=1 k;=1 (10)\nWe now compute A(x), but first we make some def initions. Let C denote the set of child nodes of X, let R = {j E Clej -::J 0}, and let r be the number of elements in R. Next, we relabel the child nodes so that nodes Y1 through Yr correspond to the nodes with ej =j:. 0. If r = 0 then A(x) \ufffdf 1. If r = 1 then A(x) \ufffdf Ayr(x). For r 2:: 2, we compute A(x) as follows:\nr\nA(x) def f(e:Xix) =IT f(ejlx) j=1\nr Pj IT L 1{ N ( x; ( o1.,t)2' J-t{,IJ j=11j=1\n= r\n. ITN (x;(o{,t)2,J-t{,IJ (1l) j=1 Using the fact that a product of Gaussians is propor tional to a Gaussian we can see that (11) is indeed\na weighted sum of Gaussians. We refer the reader to (Driver 1995) for a complete derivation. We write A(x) in the following form:\nMo A( X) = L aj0N ( x; ( 17>.,j0)2, J-lio) (12) io=1\nwhere Mo = IJ\ufffd=l Pi, { aio} is a set of real valued weights, {(17>.,j0)2} is a set of real valued variances, and {J-tio} is a set of real valued means. We can now compute BEL(x) by forming the product of (12) and (9). BEL(x) can then be written as a weighted sum of Gaussians by using (5):\nBEL(x)\nwhere\nM Mo aLL 7Ji.io \u00b7 j=l io=1\n( (f\ufffd ((f\\ . )2 11. (f\ufffd + ll(j)(O\">. . )2) N . J -\",Jo rJo J ,-x ,Jo X' 2 ( )2 ' 2 ( )2 17j + (f>.,io (fj + (f>.,jo\n(13)\n7Jj,jo = 'Yj a ioN (J-tV); (fJ + ( O\">.,jo )2' J-lio) (14) and a is a normalization constant chosen so that J BEL(x)dx = 1. Integrating (13) with respect to x, it follows that\na= ( ) -1 M Mo {; j?; 7}j,jo (15)\n3.2 TOP DOWN PROPAGATION\nThe message 1ry1(x) that node X sends to its jth child (j = 1, 2, ... , m) is formed as follows:\ndef f(xie- ej) = BEL(xjej = 0)\na?r(x)A(x)i>.y (x)=1 J (16) So 1ry1 ( x) can be computed by the method of the last section with the assumption that AyAx) = 1. 3.3 BOTTOM UP PROPAGATION\nThe message Ax ( u;) that node X sends to its ith par ent ( i = 1, 2, .. . , n) is formed as follows: Ax(u;)\nf(e- e(ju1, ... , Un, x) .J(u1, ... , Ui-1, Ui+l> ... , Un, xju; ) -dxdu1 \u00b7 \u00b7 \u00b7 du;-1du;+l \u00b7 \u00b7 \u00b7 dun\n(17)\nConsider the first distribution in the integrand:\nf( e- et lu1, ... , Un, X) - !( - - + + + +I - e1 , ... ,em,e1 , ... ,ei-1>ei+1, ... ,en\nU1, .. . , Un, x) = f(e1, ... ,e;;.lx) \u00b7\nf( e{, ... , et_1, et+1, ... , e\ufffd lu1, ... , Un, x) n = A(x) II f(etiuk) k=t kf;i = A(x) IT f(ukiet)f(et) k=1 f(uk) kf;i n n 1 = CA(x) II 1rx(uk) II f(uk) k=l k=l kf;i kf;i (18)\nWhere C = f1\ufffd=1 f( et) is constant for a given set of kf;i evidence. Next, consider the second distribution in the integrand:\nf(u1, ... , Ui-1, Ui+1, . .. , Un, xiu;) = f(xiu1, ... , Un)f(u1, ... , Ui-1, u;+l, ... , uniu;) n = f(xiu1, ... , Un) II f(uk) (19) k=1 kf;i\nSubstituting (18) and (19) into (17), we have:\nC 11 .. \u00b71 ,_1 1 ,+1 .. \u00b71 .. 1 A(x) n \u00b7II 7rx( Uk)f(xiu1, ... , un) k=t kf;i \u00b7dxdu1 \u00b7 \u00b7 \u00b7 du;-1dui+1 \u00b7\u00b7 \u00b7dun (20)\nThe constant C will get absorbed during the normalization of BEL(x), so we can ignore it. J(xiu1, . .. ,un), A(x), and 7rx(uk) are given by (4) , (12), and (6) respectively. Substituting these into (20) we have:\nAx(u;) n Mk\n\u00b7N (x; (o-;>,,jo)2' 11-io) II L ajk k=1 ik=1 kf;i m \u00b7N( uk; (cr\ufffd.ik)2, Jl-\ufffd.ik) L Cj N(x; crJ, p,\ufffd)) i=1 \u00b7 N(u1\u00b7 cr\ufffd \"(j )) \u00b7 \u00b7 \u00b7 N(u \u00b7 cr\ufffd \"(j)) ' J, ru1 n' J 'run \u00b7dxdu1 \u00b7 \u00b7 \u00b7du;-1dui+1 \u00b7 \u00b7 \u00b7dun\n\u00b7 (rr ajk) N(u;;crJ,JJ-\ufffd}) 1 \u00b7 \u00b7\u00b71 k=t Ut u,_t kf;i 1 . . \u00b71 r N (x; ( CT>.,jo)2' Jl-jo) Ui+t Un lx n \u00b7N(x; crJ, p,\ufffd)) II N( Uk; (cr\ufffd.ik)2, 11-\ufffd.iJ k=t kf;i \u00b7N(uk;crJ,JJ-\ufffd})dxdu1\u00b7 \u00b7 -du;-1du;+1\u00b7 \u00b7 \u00b7dun m\n- \"\"' \u00b71\u2022 \u2022 N(u \u00b7 \u00b7 cr\ufffd \"(j)) - L...J '+'J \u2022' J > ru; j=1 where we have defined: Mo M1 M;-1 Mi+1 M,. 1/!j def Cj L L \u00b7\u00b7 \u00b7 L L \u00b7 \u00b7 \u00b7 L C\u00a5jo io=1 h=1 ii-1 =1 j;+1 =1 j,.=1\n\u00b71 N (x; (cr>.,j0)2, P,j0) N(x; UJ, p,\ufffd))dx n \u00b7IIaJk 1 N(uk;crJ,p,\ufffd}) k=l UJc kf;i \u00b7N (uk; (cr\ufffd.iJ2, 11-\ufffd.iJ duk\nMo M1 Mi-1 M;+1 M,. Cj L L \u00b7 \u00b7 \u00b7 L L \u00b7 \u00b7 \u00b7 L C\u00a5jo\n\u00b7N (P,j0; UJ + (cr>.,j0)2, P,\ufffd))\n(21)\n. (IIn a\ufffd N (\"(j). cr\ufffd + (crk . )2 \"k . )) ]k ruk' J 'lr,Jk ' r'lr,Jk k=1 k\u00a2i Cj (\ufffd C\u00a5joN (11-io;crJ + (cr>.,j0)2,p,\ufffd))) Jo=1\nn Mk \u00b7II L ajkN (11-\ufffd};crJ + (cr\ufffd,jk)2,Jl-\ufffd.ik) \u00b7 k=1 ik=1 k\u00a2i\n(22) Note that if A(x) is constant then from (20) we have:\nAx(u;) = c1 . . \u00b71 1 .. \u00b71 { Ut Ui-1 Ui+t Un lx n\n\u00b7II 7rx(uk)f(xiu1, . .. , Un) k=1 k\u00a2i\n138 Driver and Morrell\n\u00b7dxdu1 \u00b7 \u00b7 \u00b7 du;-1dui+1 \u00b7\u00b7\u00b7dun cfr.J1Tx(uk)duk\nk=l Uk k;ti constant\nHence, if A(x) = 1 then Ax(u;) = 1 for each i. So just like with discrete variables, evidence gathered at a node does not affect any spousal nodes until their common child node obtains evidence.\n3.4 BOUNDARY CONDITIONS\nIf X is a root node (i.e. a node with no parents) that has not been instantiated, then we set 1r( x) equal to the prior density function f( x). This prior distribution is approximated by a sum of weighted Gaussians.\nIf X is a leaf node (i.e. a node with no children) that has not been instantiated, then we set A( x) = 1. This implies that BEL(x) = 1r(x). If X is an evidence node, say X = x0, then we set A(x) = c5(x- x0) = N(x; 0, xo) regardless of the in coming A-messages. This implies that BEL(x) = N(x; 0, x0) as we would expect. Furthermore, for each j, 1Tyi(x) = N(x; 0, xo) is the message that node X sends to its children (each child gets the same message in this case). The messages that node X sends to its parents are still weighted sums of Gaussians.\n3.5 SPECIAL CASE: LINEARITY\nIn this section, we consider the special case when the function g of Equation ( 1) has the form g(U1, U2, \u00b7 \u00b7 \u00b7 , Un) = b1 U1 + b2U2 + \u00b7 \u00b7 \u00b7 + bnUn (23) where the b; 's are real numbers. This relationship was suggested in (Pearl 1988). With this we have\nf(xJul, . . . ,un) = N (x;u\ufffdx'tb;u;). (24) \u2022=1 We note that only the computations of 1r( x) and Ax(u;) involve the distribution f(xJul, . . . , un), so all other computations are the same as before. Further more, we can obtain closed form solutions for 1r( x) and Ax ( u;) without invoking the approximation given by (4). To do so, we use the following identity:\n11 \u00b7\u00b7\u00b71n (J]N(x;;u;2,Jti)). \u00b7N (tb;x;;u2,p) dx1\u00b7 \u00b7 \u00b7dxn \u2022=1\nN (p; o-2 + t b7ul, t b;p;) (25) For completeness we state the final result but omit the proof; we refer the interested reader to (Driver 1995):\nNote that the means and variances of the Normal dis tributions in (26) and (27) have the same form as those in Pearl's result. The only real difference here is that our result is a sum of weighted Gaussians and Pearl's result is a single Gaussian.\n4 EXAMPLE\nIn this section, we present a simple example that illus trates the characteristics of this approach to continu ous Bayesian networks. The example uses the network of Figure 2. Nodes X andY are independent and uni formly distributed on [0, 1]. Node Z is related to X and Y by the following equation:\nZ =X + Y +wz, where wz is a Gaussian random variable with zero mean and variance 0.01 ; wz is independent of X and\nY.\nFigure 3 shows a graph of our approximation to the prior distributions of nodes X and Y; this approxima tion consists of 20 Gaussians with uniformly spaced\nmeans, equal variances, and equal weights. The abso lute error in this approximation is 9%. Other approxi mations could be found that provide a better represen tation of the densities with fewer Gaussians; potential methods by which this can be done are discussed in Section 5.\nPerforming the updating algorithm with no evidence in the network, we obtain the distribution for node Z. This is shown in Figure 4. From probability theory, we know that the distribution of a sum of two independent random varibles is the convolution of the individual distributions. So theoretically, the distribution of Z should be the triangular function on the interval [0,2]. Taking into account the effects of the noise wz and the approximation error in the prior distributions, this is exactly what we have in Figure 4.\nNext, we assume that node X is instantiated at a value of 1.0. Performing the updating algorithm for this evidence, we obtain the belief function for node Z. This is shown in Figure 5. We can explain this as follows. Given X = 1.0, Z is just the sum of the constant 1.0 and a uniform distribution (plus the noise term wz ). Hence, Z is a uniform distribution on [1, 2]. This is approximately what we have in Figure 5. Node\nY of course remains unchanged since it is independent of X. Now we reinitialize the network and assume we have\nobserved node Z = 2.0. Performing the updating algo rithm for this evidence, we obtain the belief function for node X. This is shown in Figure 6. The only way that Z can equal 2.0 is if both X andY are 1.0, so the distributions for X and Y should be delta functions centered at 1.0. From Figure 6, we see that our results are a reasonable approximation to a delta function, es pecially when we consider the effects of the noise and the approximation error in the prior density functions.\n5 GAUSSIAN SUM APPROXIMATIONS\nIn this section, we discuss briefly some of the issues involved in choosing sum-of-Gaussian approximations to probability density functions. We first discuss the existence of good approximations. We then mention several techniques that can be used to obtain these approximations.\nLetS be a compact subset ofnn and consider the set:\n9s {g E C[S] lg(x) = E c; exp [ -\ufffd (x -IL;)T i=l 2cr,\n(x - It;)] ; mEN; c;, CT; En; It; E nn} (28)\n140 Driver and Morrell\nwhere C[S] is the set of all continuous functions from S ton. By a simple application of the Stone-Weierstrass theorem, it can be shown (Girosi 1990) that gs is dense in C[S]. That is, given any f E C[S] and any c > 0, there exists g E gs such that fs Jg(x) - f(x)JdV < t. (Note: here we have used the L1 norm, in fact the result holds for any Lp norm, 1 \ufffd p < oo.) In other words, any arbitrary function in C[S] can be approx imated arbitrarily well by a finite sum of weighted Gaussians.\nIn this paper, the functions we are approximating are probability density functions. Since density functions die off at infinity, as an approximation we may assume that they are defined on compact subsets of nn ( i.e. we set them to zero outside some compact subset of\nnn). Hence, by the above results it is reasonable to assume that any density function can be approximated arbitrarily well by a finite sum of Gaussians.\nWe now turn our attention to actually finding such approximations. One approach to obtaining approxi mations is the use of neural networks (Poggio 1990). Another method (Klopfenstein 1983) approximates an arbitrary function by uniformly spaced Gaussians; us ing Fourier transform techniques, error estimates can be obtained. Other methods include simmulated an nealing and gradient descent algorithms. Our best re sults have been with the gradient descent algorithm, especially when approximating functions with a small number of variables.\nAcknowledgements\nThis work supported by the United States Army Re search Office, grant number DAAH04-93-G-0218\nReferences\nJ. Pearl, Probabilistic Reasoning In Intelligent Sys tems. San Mateo, CA: Morgan Kaufmann Publishers, 1988.\nR. D. Shachter and C. R. Kenley, \"Gaussian Influence Diagrams,\" Management Science, Vol. 35, No.5, May 1989, pp. 527-550.\nS. L. Lauritzen and N. Wermuth, \"Graphical Models for Associations Between Variables, Some of Which are Qualitative and Some Quantitative,\" The Annals of Statistics, Vol. 17, No. 1, 1989, pp. 31-57.\nS. L. Lauritzen, A. P. Dawid, B. N. Larsen, and H. G. Leimer, \"Independence Properties of Directed Markov Fields,\" Networks, Vol. 20, Iss. 5, Aug 1990, pp. 491-505.\nD. J. Spiegelhalter and S. L. Lauritzen, \"Sequen tial Updating of Conditional Probabilities on Directed Graphical Structures,\" Networks, Vol. 20, Iss. 5, Aug 1990, pp. 579-605.\nE. Driver and D. Morrell, \"Implementation of Continu ous Bayesian Networks Using Sums of Weighted Gaus-\nsians,\" Technical Report TRC-SP-DRM-9501, Electri cal Engineering Department, ASU, May 1995.\nF. Girosi and T. Poggio, \"Networks and the Best Approximaton Property,\" Biological Cybernetics, Vol. 63, 1990, pp. 169-176.\nT. Poggio and F. Girosi, \"Networks for Approxima tion and Learning,\" Proceedings of the IEEE, Vol. 78, No. 9, Sep. 1990,pp. 1481-1497.\nR. Klopfenstein and R. Sverdlove, \"Approximation by Uniformly Spaced Gaussian Functions,\" in Approxi mation Theory IV, (C. K. Chui, L. L. Schumaker, and J.D. Ward, eds.), pp. 575-580, Academic Press, 1983."}], "references": [{"title": "Probabilistic Reasoning In Intelligent Sys\u00ad tems", "author": ["J. Pearl"], "venue": "San Mateo, CA: Morgan Kaufmann Publishers,", "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "F", "author": ["ASU", "May"], "venue": "Girosi and T. Poggio, \"Networks and the Best Approximaton Property,\" Biological Cybernetics, Vol. 63, 1990, pp. 169-176. T. Poggio and F. Girosi, \"Networks for Approxima\u00ad", "citeRegEx": "ASU and May,? 1995", "shortCiteRegEx": "ASU and May", "year": 1995}, {"title": "Sverdlove, \"Approximation by Uniformly Spaced Gaussian Functions,\" in Approxi\u00ad mation Theory IV, (C", "author": ["R.R. Klopfenstein"], "venue": "Vol. 78,", "citeRegEx": "Klopfenstein,? \\Q1990\\E", "shortCiteRegEx": "Klopfenstein", "year": 1990}], "referenceMentions": [{"referenceID": 0, "context": "This relationship was suggested in (Pearl 1988).", "startOffset": 35, "endOffset": 47}], "year": 2011, "abstractText": "Bayesian networks provide a method of rep\u00ad resenting conditional independence between random variables and computing the prob\u00ad ability distributions associated with these random variables. In this paper, we ex\u00ad tend Bayesian network structures to compute probability density functions for continuous random variables. We make this extension by approximating prior and conditional den\u00ad sities using sums of weighted Gaussian dis\u00ad tributions and then finding the propagation rules for updating the densities in terms of these weights. We present a simple exam\u00ad ple that illustrates the Bayesian network for continuous variables; this example shows the effect of the network structure and approxi\u00ad mation errors on the computation of densities for variables in the network.", "creator": "pdftk 1.41 - www.pdftk.com"}}}