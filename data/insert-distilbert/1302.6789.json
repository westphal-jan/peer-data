{"id": "1302.6789", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2013", "title": "Exploratory Model Building", "abstract": "some instances of creative thinking require an agent to build interpretations and test other hypothetical theories. such conditions a reasoner needs to initially explore clearly the space of not only those situations that have occurred in the past, but also those that are rationally conceivable. in approaching this paper we deliberately present a formalism for exploring especially the space of conceivable situation - models for those domains in which available the knowledge is primarily probabilistic in nature. the formalism seeks to construct consistent, minimal, simple and desirable situation - descriptions by selecting suitable domain - type attributes and dependency relationships from the available domain knowledge.", "histories": [["v1", "Wed, 27 Feb 2013 14:14:14 GMT  (745kb)", "http://arxiv.org/abs/1302.6789v1", "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)"]], "COMMENTS": "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["raj bhatnagar"], "accepted": false, "id": "1302.6789"}, "pdf": {"name": "1302.6789.pdf", "metadata": {"source": "CRF", "title": "Exploratory Model Building", "authors": ["Raj Bhatnagar"], "emails": ["bhatnagr@ucunix.san."], "sections": [{"heading": null, "text": "Some instances of creative thinking require an agent to build and test hypothetical the ories. Such a reasoner needs to explore the space of not only those situations that have occurred in the past, but also those that are rationally conceivable. In this paper we present a formalism for exploring the space of conceivable situation-models for those do mains in which the knowledge is primarily probabilistic in nature. The formalism seeks to construct consistent, minimal , and desir able situation-descriptions by selecting suit able domain-attributes and dependency rela tionships from the available domain knowl edge.\n1 Introduction\nIn this paper we describe a formalism for exploring the space of those situation-descriptions (also called models, scenarios, or theories) that may be consid ered rationally conceivable in a domain. This exer cise is the same as that of a controlled imagination process. Many aspects of such theory building activ ity, in the context of default logics, have been pre sented in [12]. Probabilistic knowledge derived from available databases of a domain has traditionally been used only for performing probabilistic reasoning. Our focus in this paper is to appropriately represent and use the probabilistic knowledge of a domain for per forming exploratory theory-building exercises. In the following sections we first make precise our notions for the following concepts : 1. The process of imag ination; 2. The form in which the probabilistic and qualitative domain knowledge are represented; 3. The requirements for a situation-description to qualify as an acceptable imagined scenariofcontezt; and 4. Some types of objectives that an agent may be pursuing dur ing the imagination process. We then discuss the com putational aspects of the construction of consistent, minimal and optimal scenarios by an imagining agent.\n2 The Scenario-Building Process\nThe scenario building (or the imagination) process has been extensively examined and discussed by many philosophers and they have examined this activity in widely varying contexts such as science, literature, phi losophy, music, and painting. The scope of our for malism is limited to the imagination in the context of developing formal scientific (or other) theories.\n2.1 A Philosophical Perspective\nIn his book The Origins of Knowledge and Imagina tion [1] Jacob Bronowski states \". . . every act of imagination is the discovery of likenesses between two things which were thought unlike\". He gives the ex ample of Newton's thinking of the likeness between a thrown apple and the moon sailing in the sky. He fur ther states : . . .A ll act! of imagination are of that kind. . . . They take the closed system, . . . open the system up, they introduce new likenesses, whether it is Shake!peare saying, \"My Mistres eyes are nothing like the Sunne\" or it is Newton saying that the moon in essence is like a. thrown apple. In his view the acts of imagination in the context of scientific discovery pro ceed as follows. An initial theory exists in the form of a closed system consisting of some domain attributes and some causal dependencies that inter-connect the domain attributes. An investigation is triggered by ei ther an observation contradicting the theory or by a desire to enhance the scope of the theory by includ ing in it more attributes from the environment of the theory's domain. The investigator then needs to dis cover and include in the theory a new and satisfactory causal dependency. The task of investigator's imagi nation is to provide the candidate causal dependencies and it is the role of his critical judgment to select one dependency from among the imagined candidates. In his lecture Imagination and Science [6] J. H. Van't Hoff describes this mechanism in the words : \"The so called occurring to mind results from a requisite survey of the possible case8 in one's mind and a definite 8e lection therefrom, i.e. combined efforts of imagination with the power of critical judgment are required.\"\n78 Bhatnagar\nThe imagination process, as described above, gener ates candidate causal dependencies for building up a theory. A major source from which the investigator ob tains the candidate dependencies is the analogies and likenesses gathered by him from all the domains and theories known to him. A number of interesting sci entific theories owe their birth to such analogy based imagination process. In [4] Donald Crosby, while dis cussing the evolution of Maxwell's theory of electrical and magnetic fields, states : \"A nalogie\" with fluid flow, ela,tic nb6tance6, and whirling vortices had helped to bring the idea of the field into being, but once that idea had been given a firm mathematical description, the6e particular analogie\" tended to drop into the back ground,\" and also, \"Mazwell'\" equation\" faced back ward to Newton'\" vi6ion of mechanical interaction\" in a material medium and forward to belief in the con cept of the field.\" This analogy with the mechanics of fluid flow had helped Maxwell formulate the field theory which completely replaced the then prevalent and completely different notion of \"actions at a dis tance\" for explaining the effects of what we now know as electrical and magnetic fields.\nIn the discussions by the above quoted authors the main focus has been on incrementally correcting or extending a theory by discovering appropriate causal dependencies among the attributes of a domain. Our computational formalism follows the spirit of perform ing imagination by discovering appropriate causal de pendencies and also generalizes the above incremental notion and seeks to hypothesize all the causal depen dencies for building a hypothetical - imagined- theory. Since we are working with probabilistic knowledge, we equate the concept of a specific theory with that of a specific probability distribution, also called a specific contezt by us. One main consideration for the formal ism is to decide on what dependencies can be extracted from probabilistic knowledge and then used as causal dependencies.\nWhile seeking to build a hypothetical theory the imag ining agent is constrained by the need to remain consis tent with the body of observed evidence and is guided by a wish to make some preferred and desirable infer ence in the imagined theory. The intere6tingneu of an imagined scenario to the agent, therefore, is deter mined by:\n1. A set E of constraining evenb. The occurrence of these events must be possible in the context of the hypothesized scenarios.\n2. A de6ired event, d. The probability of occurrence of the event d conditioned on the occurrence of the constraining events E in the hypothesized context S, written by us as P[d I E (S)], should be very high/low (as desired by the agent).\nThe interestingneu criterion for focusing the imagina tion process is based on the perception that an imag ining agent is driven by the question - \"what are tho6e possible contezts and scenario\" in which the desired\nevent 1d' would be very-highly/very-leu likely to oc cur?\", irrespective of the probability of occurrence of the imagined contezt or 6cenario itself. Having imag ined the interesting contezts or scenarios, if the agent desires, he may take actions to alter the real envi ronment in a way to make the interesting scenarios more/less likely to occur. A scientist tries to imag ine those possible scenarios in which his experimental observations are explained and the result-proposition conjectured by him has a high probability of being true. A scenario similar to mechanical fluid flows was imagined to explain the observations in electrical and magnetic fields, and Maxwell's goal of having a set of equations as true in this imagined scenario is an ex ample of this type of imagination.\nA scientist, when in an exploratory mood, may not be interested in the mo6t reasonable explanation of his ex perimental observations but may seek those, possibly less reasonable, explanations in which hi\" conjecture is most likely to be true. Imagination of such scenarios would guide him towards designing those experiments and seeking those observations which would turn his imagined scenario into the most reasonable one given all the observations. This kind of imagination pro cess is a precursor to seeking relevant evidence in the larger process of creative thinking. Such imagination process can be characterized only be a theory-building paradigm and not by a paradigm for reasoning in the context of a given theory.\n2.2 Abduction\nAbductive inference is easily understood through the following simple example given by Peirce in (11].\n\u2022 The surprising fact, C, is observed.\n\u2022 But if A were true, C would be a matter of course.\n\u2022 Hence, there is reason to suspect that A is true.\nHere C is the observed fact and the second sentence states the dependency relationship, possibly causal, (and available from the domain knowledge) that the presence of A explains the presence of C. In the third statement, A is an abductively inferred hypothesis. The content of the inference is the premise \"If A were true, C would be a matter of course.\" Given a G, an A which explains the occurrence of C must be discovered by the abductive reasoner. Many A's, each explaining the C, may exist in the reasoner's mid and he would have to choose one from all the possible candidates.\nThe focus of many abductive reasoners [10, 13] has been on determining the most reasonable, that is, the par6imoniou6 or the mo6t probable or the lea6t cost [2, 14] hypotheses A, given the observed events G. The imagining agent is not necessarily seeking the most rea6onable scenarios. He is guided by the inter e6tingne66 of the candidate scenarios, while requiring only some minimal reasonableness from the hypothe ses. That is, an imagining agent seeks those A's in\nwhose contexts Cis at least minimally explained (can possibly occur), but the intere.stingness of the hypoth esis is maximum.\nInterestingneu as a preference criterion is more gen eral than the most reasonable criterion. The probabil ity of an event of interest d inferred in the context of an imagined scenario is one possible criterion of inter estingness which can not be simulated by using either the probability of occurrence of a scenario or the costs associated with the components of a scenario. Size of a scenario is another possible criterion of interesting ness.\nIn the following discussion the imagining abductive reasoner is given the interestingness criterion in terms of the probability of occurrence of a desired event d, conditioned on the occurrence of events in E, com puted in an imagined context S, and written as P[d I E (S)] by us. The agent's task is to build those hy pothetical contexts S in which P[d I E (S)] is very high/low irrespective of the probability of occurrence of the context S itself. The contexts and the scenar ios contained in them, however, must satisfy the con straints of consistency and should be minimal in size so as to exclude all irrelevant information.\n3 Probabilistic Knowledge\nFor most domains the available knowledge falls into the following two categories : 1. A number of databases, each consisting of cases recorded in various contexts of the domain, and 2. Some qualitative or probabilistic domain dependencies acquired possibly from a theo retical understanding of the domain. Typically, the data part of a domain's knowledge is available in the form of a number of databases, each representing a different context of the domain. For example, in the domain of lung-diseases, databases may be available corresponding to different age-groups, different popu lation groups, different specialized disease classes, and different treatment strategies. Each database repre sents a specialized context, reflecting the conditions under which it was recorded.\nWe denote by H = (h1, h2, \u2022 \u2022 \u2022 h,.) the set of all the known and relevant attributes for a domain. We define a &ituation-description {scenario) of a domain to be a set T of attributes, along with their values such that T C H and T is non-null. By contezt we refer to a subset of some scenarios of the domain. In the lung disease example, each individual database mentioned above represents a specialized contezt of the domain of lung diseases, and approximates the joint probability distribution only for that contezt.\nOne way in which a database of cases serves as a source for domain-knowledge is that we can extract from it the important inter-attribute dependencies. These de pendencies may take the form of conditional prob ability functions or the likelihood& used in Bayesian inference. Let us say that the set P contains suffi-\nExploratory Model Building 79\ncient number of conditional probability functions (de pendency relationships) derived from a database hav ing T as its set of attributes so that the pair < T, P > approximates the probability distribution re flected by the database. A different pair < T, P > for each contextual database represents the proba bility distribution approximated by its corresponding database. An example of dependencies learnt from a contextual database S is the set of probabilities P[sympt07'ni I disease; (S)]. Bayesian reasoning systems treat these likelihood dependencies as the in variants of the context S for performing probabilis tic inference. That is, the conditional probability P[symptom; I diseasei (S)J remains constant in the contextS, and is used as a dependency relation be tween the probabilities of the symptom and the disease events. The problem of compacting a joint probability distribution into the set P of important dependency re lationships has been extensively researched and some of the popular graph based representation methods have been presented in [3, 7, 8, 10).\n3.0.1 Abduction with Probabilistic Knowledge\nOne type of abductive reasoning in a context S : < T, P > has been described in [8, 10] as follows. A set of observed attributes corresponding to attributes E1, E2, . . \u2022 E,., (contained in set E where E C T) are given. That is, the instantiations E1 = eii have been fixed by the observations. The set A = (T - E) = (At, A2 ... , Am) represents all the unobserved at tributes of the context under consideration. The ob jective of the abductive exercise is to find an assign ment for each unobserved attribute A, such that :\nP[Al :::= a10, A2 :::= a2; . . . Am = am,.. I E1 = e101 \u2022 \u2022 \u2022 E,. = e\ufffd:r (S)j (1)\nis maximum among all possible sets of assignments to the attributes in set A. We say that Ah. refers to that set of assignments which results in the maximum value for the above probability. the content of the abductive exercise (in a manner parallel to Peirce's formulation) can be summarized as follows :\n\u2022 In context S the surprising events of the set E are observed.\n\u2022 But if assignments Ah were true, E would be a matter of course.\n\u2022 In the context S there is no other set of assign ments for the attributes of set A which is more likely to occur along with the occurrence of the observed events contained in set E.\n\u2022 Hence, there is reason to suspect that Ah is true about the observed situation.\nThe third statement above says that the Ah selected is the most probable set of assignments, given the ob served evidence. This type of reasoning is in the spirit\n80 Bhatnagar\nof inductive reasoning where we seek the most reason able hypothesis given the observed events. The imag ining agent needs to perform a more widespread explo ration directed at the most interesting scenarios and not restricted to only the most reasonable explanations of the observations.\n3.1 Probabilistic Knowledge - \"Imagination\" Perspective\nOne basic way in which the imagining agent's view of the probabilistic knowledge and reasoning differs from the traditional reasoning methods is as follows. The traditional methods consider a domain's probability distribution as a unified whole within which all rea soning activity is performed. The context invariant conditional probability functions is only a way of rep resenting the complete distribution. We consider an imagining agent who possesses the probability distri bution and the causal dependency ordering of all the attributes in the distribution. The latter knowledge may be available from a deeper understanding of the domain to which the database pertains. He orders the attributes according to their causal (may be partial) order and determines the context invariant dependen cies such that in the dependency P[a I B (S)J B is a set of attributes which are strictly causal predecessors of event a. For the imagination agent these causal con text invariant dependencies is all the knowledge that is needed. For the imagining agent the domain knowledge is rep resented by n contextual databases, < T1, P1 >,< T2, P2 > . .. ... < Tn, Pn > where the distribution < Tt, P, > represents the i1h context s, for the domain. It is possible that an attribute h E H may appear in more than one context and in such a case there may exist a dependency of the type P[h I a.su.bset.of.T; (Si)] in the set P; of every context S, that includes h. The various P,'s, therefore, represent the in\"Variant dependencies for different contezt8 and are useful for probabilistic inference only within their respective con texts.\n3.1.1 Difference in Perspectives\nThe main difference between the imagination agent and the traditional probabilistic inference is the fol lowing:\n\u2022 For traditional reasoning the only meaningful knowledge is a probability distribution < T, P > within which the instances of reasoning are per formed.\n\u2022 The imagining agent considers each invariant causal dependency, learnt from a particular con text, as a basic entity of the domain knowledge. As discussed in the section on philosophical per spective of imagination, the imagining agent feels\nfree to use causal likenesses learnt from one con text to build and test other interesting hypotheti cal contexts. The context invariant causal depen dencies, therefore, are the basic units which the imagination agent considers as models of contex tual causal phenomena that can be used as build ing blocks for building the descriptions of other hypothetical contexts.\nThe conte.ztu.al causal dependencies learnt from indi vidual contexts are reflective of the underlying causal processes for the context and therefore are viewed as possible causal building blocks from which other possibly not yet encountered, contexts may be con\ufffd structed. The causal dependencies learnt from a com plete domain's probability distribution can be seen as weighted accumulations of the causal dependencies of individual contextual databases. Therefore, an in stance of abductive reasoning with probabilistic knowl edge in the traditional sense would not be following the process of selecting some causal dependencies and leav ing out the other. It would be aggregating over all the known causal dependencies that affect any particular attribute. The analogical thought as practiced by the imagin ing agent is similar to the capacity of a thinker to visualize a concept in a hypothetical context, different from the one in which it was learnt. In the context of probabilistic knowledge, the invariant dependencies characterizing a context are placed in a different hypo thetical contezt by the imagining agent to imagine the descriptions of hypothetical scenarios. This capabil ity is, arguably, the foundation of imagination process as discussed earlier, which in turn is the foundation of creativity. Theory building using defaults and default logic has been presented in [12] and in our case the causal context invariant dependencies may be seen as analogous to the defaults which may or may not be applicable in any particular situation. When we mix and match the contextual dependencies learnt from disparate contexts to hypothesize new con texts we don't have enough information to determine the probability of occurrence of this new concocted context. This is because the marginals represented by the contextual databases are not sufficient to con struct the complete joint distribution from which the probability of occurrence of a particular context may be determined. Since our formulation of an imagin ing agent, as explained in the previous section, does not need to determine the probabilities of occurrence of various scenarios, the above shortcoming is not of much significance. The abduction exercise that the imagining agent needs to perform can be stated as follows.\n\u2022 The surprising events contained in the set E are observed.\n\u2022 But if the context were to be Sh, represented by < Th, Ph > then E would be a matter of course.\n\u2022 There is no other context S in which P[d I E (S)] is greater than what is obtained in the contextS\".\n\u2022 Hence, there is reason to suspect that S\" is the most interesting context and the various scenar ios obtained from s\ufffd (by making attribute assign ments) are the most interesting scenarios.\nThe main difference between this abduction formula tion and the one described in section 3.0.1 can be seen in terms of the second ab.d the third statements of the abduction formulation. The imagining agent is seek ing to hypothesize that context in which interesting inferences can be made and the traditional reasoner is trying to hypothesize the most probable scenario in a given context. The computational problem of the tra ditional reasoner's abduction, as presented in [8, 10], is to find the most suitable assignment A\" in a given contextS. The corresponding problem for the imagin ing agent's abduction task is to construct the context s\" by selecting an appropriate subset TIL from the set of domain attributes H and an appropriate subset P,. from the set of all the known contextual invariant de pendencies, such that the probability of occurrence of din sk meets the interestingness criterion.\n4 The Dependency Relation\nFor each contextual database of a domain, we can rep resent the joint probability distribution by the pair < T, P > where the set P may contain either all the Chow dependencies (3], or all the Bayesian dependen cies (8, 10], or all the qualitative dependencies known from a theoretical understanding of the domain.\nWe define a relation D for the complete domain, in cluding information from all its contextual databases, such that\nn-1 D \ufffd U {H;xH}where H; = HxHx ... xH, i times\ni=l (2)\nwhere H is the set of all the attributes in all the contextual databases, domain{D} = U:,;1\n1 {H;} and range{D) =H. This relation has the same character as the set P of a joint probability distribution. The dif ference is that D contains all the dependencies learnt from all the contextual databases or the qualitative knowledge of the domain and it does not necessarily constitute a consistent description of a joint probabil ity distribution.\nTo construct the dependency relation D for the com plete domain we repeat the following with each avail able contextual database. We first order (or partial order, if sufficient knowledge is not available) the at tributes of the database in such a way that each at tribute is followed in the sequence by only those that can possibly causally influence it. Given this order ing of attributes, we determine the sets of Bayesian\nExploratory Model Building 81\nnetwork dependencies, and the known qualitative de pendencies for the database. A union of all these sets is the contribution of this contextual database to the dependency relation D of the complete domain. The relation D therefore contains all the dependencies de rived from each available contextual database. From the perspective of traditional probabilistic reasoning D is an unnatural and meaningless medley of conditional probability functions but for the imagining agent the relation Dis the source from which to derive the candi date causal dependencies to build a hypothetical the ory. The issue of maintaining some consistency has been addressed in a later section.\nFor the imagining agent the elements of set D are in dependent entities in the sense that a particular de pendency can be used without worrying about the probabilistic dependence of its consequent attribute on some other attributes of the context not included in the dependency. This is because in a Bayesian de pendency a consequent node is in fact probabilistically independent given the antecedent attributes of the de pendency. Therefore, by using these dependencies the agent is not making any assumption about any prob abilistic independences. The independence among de pendencies is a characteristic of the way the Bayesian or other dependencies are constructed.\n5 Structure of an Imagined Context\nThe task of the imagining agent is to hypothesize a contextS\" such that P[d IE (S\")J is in accordance with the interestingness criterion. The interestingness criterion may seek this probability value to be mini mum, maximum, or satisfy some specified constraint. The description of the context S\" consists of the pair < T\ufffd., Ph >, specifying some joint probability distribu tion. It is an arbitrary, imagined distribution but must be consistent as a description of a distribution. Imag ining a contezt, therefore, is the same as construct ing a relevant, complete and consistently specified ap proximation of a probability distribution by using the known dependencies as building blocks. An analogy with building consistent theories using defaults in de fault logic [12] can be drawn here. Our context build ing task is accomplished by selecting an appropriate set of dependencies from the domain dependency re lation D. However, any arbitrary choice for the set Pk may not be an acceptable description of a context. The constraints that a hypothesized context S\" must follow are the following:\n1. Sufficiency : A hypothesized context < T,., Pk > is considered sufficient if all the constraining at tributes (E) and the event of interest d are in cluded in the set Th. The objective of sufficiency criterion is to ensure that in the imagined contexts the probability of occurrence for each constraining event can be computed. Only those hypothesized contexts are of interest to the imagining agent in which scenarios with non-zero probabilities for all\n82 Bhatnagar\nthe constraining events can be constructed.\n2. Consistency : A hypothesized context is consid ered consistent if it is a complete and consistent description of a joint probability distribution - even though a completely hypothetical joint prob ability distribution.\n3. Minimality : A hypothesized context is considered minimal if it contains only those attributes and dependencies that are needed to show the possi bility of occurrence of the constraining events and the high/low probability of the occurrence of the desired event.\n5.1 A Sufficiently Large Context\nA hypothesized context that is sufficiently large in de scription should : 1. Include the attribute d corre sponding to the event of interest . 2. Include the con straining attributes contained in the set E. 3. Include a subset R of the dependency relation D such that their associated conditional probability functions de fine a complete joint probability distribution. 4. Be an Ezplanation for the occurrence of the desired event d and the constraining events E.\nIn the abduction example given by Peirce in [11] the abductive hypothesis A is considered an explanation of the observed event C. In this same spirit, w our notion of an explanation is as follows :\n5.1.1 A Contezt as an Explanation\nEach conditional probability function in D, say derived from a Bayesian network, P(node.n J parents.of(node.n) (S)] represents a dependency o\u00a3 the context represented by the Bayesian network. The dependencies selected for constructing a hypoth esized context can be arranged in a graphical network to show the unique Bayesian Network that they repre sent. We define a path in the graphical representation of a context as follows : Definition : A path from a node (attribute) 111 to a node VA: in a context < T, R > is defined as a sequence {vt, Wt1 va, wa . . . Wk-1, v,_}, where each w, E R, each v, E T, and for each w;\n1. either v; +1 or u; is its consequent node and the other is its antecedent node;\n2. no v; is the consequent node of both Wj-1 and w1; and\n3. no v0 or wi is repeated in the sequence; 4. VA: is not an antecedent node of any w, included\nin the sequence.\n(end-definition)\nThe conditions in the above definition ensure that each path : 1. is a connected sequence of causal dependen cies; 2. is non-cyclical; and 3. connects vertices 111 and\nv\ufffd; without containing a vertex which is a causal de scendent of both Vt and v\ufffd;. A context description can also be viewed as a collection of various such paths.\nWe consider Peirce's description of abduction again and replace in it the fact A above by a path of depen dencies, the observed event C by a pair of attributes, and the explanation A by a the path. The resulting reformulation of Peirce's example can be stated as fol lows:\n\u2022 The surprising events, e, and e; are observed to occur\n\u2022 If the path of dependencies A between these two observed events were to be active in a context then e.; and e; would have occurred as a matter of course.\n\u2022 Hence, there is reason to suspect that the path of causal. dependencies A is active in the situation.\nThe content of the above inference is the premise \"If the path of dependencies A were active, the events e, and e; would occur as a matter of course.\" The path A, therefore, is a possible explanation for the occurrence of the two events.\nThe above definition of an explanation restricts the contexts that can be hypothesized by the imagin ing agent. The agent is restricted to explain each constraining event in relation to another constrain ing event, or the event of interest. A single attribute, with a single dependency containing the attribute as its consequent node, may also be considered as a pos sible explanation of an event corresponding to the at tribute. This latter definition, however, results in a large number of trivial explanations in which no con straining event is connected either to another con straining event or to the desired attribute. Imposing the above more restrictive, definition of an explana-, . tion forces the imagining agent towards those imagmed contezts in which the attributes are relatively much more connected to each other. The contezb with the more liberal definition of an explanation are also valid from the imagination perspective but are more discon nected as hypotheses and add more complexity to the computational process of constructing the interesting contezts.\n5.1.2 Contents of a Context\nA 111.jficiently large context Sh. is one in which the pos terior probability P[d IE (Sh.)] and the probabilities P[( e, E E) (Sh )] can be computed and it is an e\ufffd\ufffdla nation in the above described sense of the constrrumng and the desired events.\nWe say each element\ufffd of D has the form (z, y) where y is the consequent attribute of d, and z is the set of antecedent attributes of d;. We define the func tions conseq(d,), antec(d;), conseq\u2022(r), and antec\u2022(r) as follows:\n\u2022 conseq(d,) is the attribute yin the pair (x, y) of dt.\n\u2022 antec(dt) is the set of attributes :z: in the pair (:z:, y) of dt.\n\u2022 conseq* (R), where R c;; D, is the set of attributes containing conseq( di) for each di E R, and con tains no other attributes.\n\u2022 antec*(R), where R c;; D, is the set of attributes containing antec(di ) for each di E R, and contains no other attributes.\nA Sufficient description of a context ts the pair < T, R > such that :\n1. T c;; H, E s; T, dE T 2. R\ufffd D,\n3. d E conseq* ( R) 4. E C conseq*(R)\n5. There is a path from each e, to either the attribute d or another constraining attribute e;, and there is a path from d to at least one ej.\nThe third and the fourth conditions stated above im ply that a hypothesized context must include depen dencies that have d and all the members of E as con sequent nodes. That is, we must include in the hy pothetical context those dependencies which can be viewed as the causes for the occurrence of d and the members of E. The fifth condition states that in a suf ficiently large context each event, in conjunction with some other event of interest has been explained. This condition would cause many nodes, other than d and the constraining events, to be included in set T. A sufficiently large pair < T, R > selected as above would be an acceptable context only if the dependen cies in R constitute a consistent description of a joint probability distribution.\n5.2 A Consistent Context\nThe notion of consistency of a hypothesized context is derived from the perspective of a context being the same as a probability distribution. That is, a context < T, R > is consistent only if the dependencies in R completely describe a joint probability distribution for the attributes in T. This consistency condition ensures that the resulting context is neither under-specified nor over-specified. Using the chain rule and deleting those conditioning attributes which are not a part of the dependency relationship, the joint probability dis tribution for the attributes of the set T can be written as :\nP[tlt t2, . .. tm] = P[tl I Gt] * P[t2 I G2] * ... *P(tm-1 I Gm-1] * P(tm] where(3)\nG, c;; { ti+11 ti+2\u2022 . . . , tm}. The similarity between the above expansion and a context being hypothesized is\nExploratory Model Building 83\nthat each product term on the right hand side is a dependency in R which helps constitute the hypothet ical context. Keeping in mind such an expansion for a probability distribution, we should 'construct a con text < T, R > by including in R dependencies from the domain dependency set D such that :\n1. there is one and only one dependency in R corre sponding to each product term of equation 3;\n2. each product term and the corresponding depen dency in R have exactly the same attributes and in the same relative positions; and\n3. set R includes no other dependencies;\nA pair < T, R > that follows the above correspondence completely and consistently describes some hypotheti cal joint probability distribution without either under or over specifying it. For some given d and E it is possible to select a number of different subsets R from the set D such that each of these choices consistently describes a joint probability distribution. The objective of the imagining agent is to prefer those contexts in which the probability of the desired event is maximum/minimum (as desired).\n5.3 The Minimal Context\nA hypothesized context < T, R > is considered mini mal if the following are true :\n1. Removal of any dependency r E R from the con text would disrupt a path between either attribute d and an attribute e0 E E, or between attributes e, E E and e3 E E.\n2. Every t E T is included in at least one r E R.\nA path is the abductively hypothesized explanation for all the consequent nodes included in the path. The above definition of minimality implies that we don't want any such dependencies in the context that are not playing a role in explaining the occurrence of ei ther the attributes d or a constraining attributes. It should, however, be noted that a contezt in the above defined senses of sufficiency, consistency, and minimal ity can be a set of disjoint directed acyclic graphs. Each Bayesian Network is a directed acyclic graph [10] if we assume a direction in each dependency from the antecedent nodes to the consequent node. With the constraints on an acceptable Context spec ified, we now examine the computational framework for constructing the contexts in which scenarios with desired probabilities values for the events of interest can be constructed.\n6 Constructing Preferred Contexts\nThe computational task of constructing the optimal contexts is formulated by us as a state-space search\n84 Bhatnagar\nguided by an admissible heuristic function. The AI search algorithm A \u2022 given in [9] has been directly em ployed in our implementation. Each 8tate is a partially constructed context < t, r > where t C T and r C R of some completely described context < T, R >. In the start state the set t contains the attribute d and all the constraining attributes contained in the set E. To maintain the minimality and the consistency of the resulting context-description, a. dependency d E D can be added to the set r of a. state < t, r > only if the following hold :\n\u2022 con8eq( d) E ( t U antec\u2022 ( r)) , and \u2022 conseq(d) \ufffd conseq*(r).\nThat is, only that dependency can possibly be added to a. state whose consequent node already exists in the state and there is no other dependency in the state with the same consequent node. The successor states of a search state are all those states that are obtained by adding one dependency to the parent state. It can be seen with little reasoning that each possible con text description that is consistent and minimal can be built using the above incremental step of adding one dependency at a time. Also, all context descriptions that are built using the above constrained incremental process are consistent. The minimality is guaranteed by stopping the incremental steps as soon a sufficiently large context has been created. The goal state S for the search algorithm is that com pletely described context in which each constraining attribute e, is connected by a path to either the at tribute d or another constraining attribute e1. That is, the the sufficiency requirements for a context as discussed in section 5.1 are satisfied. Tha A \u2022 search algorithm can yield the best, the sec ond best, etc. hypotheses until all possible hypothet ical contexts have been generated. The capability to output multiply hypothetical contexts, rank ordered according to their interestingness, is a very desirable aspect of an imagining agent. Some computational formalisms [10] can only compute the best or the best and the second best hypotheses and this would be an undesirable restriction on an imagining agent. The next important aspect of the A \u2022 search algorithm is the heuristic function used for ordering the partially constructed contexts. If the Merit Function whose value must be maximized by the desired context is given by\nF(S) = (P[d = dr IE (S)]) (4)\nthen in order for the search to be admissible, we need an estimating function W(ss) such that\n\u2022 u is a partially constructed context, and \u2022 the value W(ss) \ufffd F(S) where Sis any complete\nconte:z;t that can be constructed by adding more\ndependencies to ss.\nWith an admissible estimating function we are guaran teed to get the optimal contezt as its first output from the A \u2022 search. Further contezts, in order of decreas ing merit value can also be obtained by continuing the search process. For the Merit function F(S) specified above, an esti mating function W(ss) can be specified as follows:\nW(ss)=(max P(d=drla,b,c, ... g (h)]) (5) ,,,,k ... n where h is that dependency which has d as its con sequent node, and a, b, c etc. are those antecedent attributes of h that lie on a path which can possibly exist in an extended version of the state ss. We can show by simple arithmetic that W(ss) is indeed an upper bound on F(S) as required above. Proposition :Considering that S refers to a com pleted contezt, and ss is a partially constructed con text of S, the estimating function W(ss) as described above computes a value that is not less than P[d = dr I E (S)] for all those S that can be generated from ss by addition of more dependencies. Proof: Since S is a consistent context, the attribute d in it can be the consequent node of only one depen dency (according to the conditions of consistency of a context). Let us say this dependency is h and a, b, c ... g etc. are its antecedent nodes. If E is the set of constraining nodes included in S then we can say that\nP[d = dr I E (S)] = L (P[d = dr I ai 1\\ b,. 1\\ .. gr (S)] j,k . .l\n(6) The first product term on the right hand side above represents the conditional probability values for the dependency h and the second term represents the probability computed for the antecedent attributes based on the constraining events. The right hand side of the above expression can be rewritten as :\nj,k .. l (7) This expression can be viewed as\nr\nwhere a.: 's correspond to the first terms and :c;'s to the second terms of the product. Since the sum of the probability values for all the events in a consistent context must be one, we can say that\nConsidering this constraint, we can say that\nma:c (L a.:* :c.:) = max( a. ) . \u2022 i\n(8)\nThat is, for the Merit function under consideration we can say that\nmax( P [d = dz I E (S)] ) = maxj,L.I (P [d = dz I ai 1\\ bk 1\\ . . 9l (h)]) . (9)\n(End-Proof) The above described search process would therefore stop when a pair < T, R > has been constructed in which the probability of occurrence of the event d = dz is the maximum among all possible contezt& that can be constructed given the constraining events, desired event , and the storehouse of dependencies, the rela tion D, from which the imagining agent can select the causal dependencies.\nThe selected merit function is a very simple interest ingness criterion and the suggested estimating func tion is also a very weak upper bound on the merit function. More intelligent estimating functions, that are stronger upper bounds on F(S) can be designed. The imagining agent can also attempt variations of the merit functions to achieve different types of objectives for the imagination exercise.\nThe imagining agent, thus, can produce hypothetical situation descriptions that are sufficient, consistent , and minimal for an imagination task. The imagined contexts need not be possible in the real world but they are only conceivable by an imagining mind. The actions that should be taken based on the imagination of a conceivable situation descriptions is the next step after the imagination task.\n7 Conclusion\nIntelligent systems have tended to use the probabilis tic knowledge of a domain for traditional probabilistic reasoning alone. The task of constructing the conceiv able contexts and scenarios of a domain requires that we move away from the most probable, the most likely, or the least cost focus of the inductive probabilistic in ference. We have presented in this paper a formalism for using the probabilistic knowledge available in a do main, by methods different from those of traditional probabilistic reasoning. We have defined the nature of sufficient, consistent, and minimal hypothetical con texts that can be conceived by an imagining agent and have briefly outlined an AI search based computational formalism for arriving at the interesting imagined hy potheses. We have presented justifications of this for malism from the perspective of an imagination process and have highlighted the differences between an imag ination formalism and a reasoning formalism using the probabilistic knowledge.\nAcknowledgment\nThis research was supported by National Science Foundation Grant Number IRI-9308868. This is a\nExploratory Model Building 85\nsmaller version of a complete report which can be ob tained from the author. We are greatful to the review ers for providing very useful comments on the earlier version of this paper.\nReferences\n[1] Jacob Bronowski. The Origin& of Knowledge and Imagination, Yale University Press, 1978.\n[2] Eugene Charniak and Solomon E. Shimony. Prob abilistic Semantics for Cost Based Abduction, Proceedings of the 1990 National Conference on Artificial Intelligence. pp 106- 1 1 1 .\n[3] C. K. Chow and C. N . Liu. Approximating Dis crete Probability Distributions with Dependence Trees. IEEE Transaction& on Information The ory, vol. IT-14 , No. 3, May 1968\n[4] Donald A. Crosby and Ron G. Williams. Cre ative Problem Solving in Physics, Philosophy, and Painting : Three case Studies, Creativity and the Imagination edited by Mark Amsler, University of Delaware Press, 1987, pp 168-214.\n[5] J ohan de Kleer and Brian C . Williams. Diagnosis With Behavioral Modes, Proceeding& of the IJ CAI, 1 989 Morgan Kaufmann, pp 1324-1330.\n[6] J . H . Van't Hoff. Imagination in Science, Springer Verlag New York Inc., 1967.\n[7] S. L. Lauritzen and D. J . Spiegelhalter, Local Computations with Probabilities on Graphical Structures and their Application to Expert Sys tems, The Journal of Royal Statistical Society, Se ries B, vol. 50, No. 2, pp 157-224, 1988.\n[8] Richard E. Neapolitan Probabili&tic Rea&oning in Ezpert Systems, John Wiley and Sons, Inc. 1990.\n[9] Nils Nilsson. Principle& of Artificial Intelligence, Tioga Press, 1980.\n[10] J . Pearl. Probabili&tic Rea&oning in Ezpert Sys tem& : Networks of Plausible Inference. Morgan Kaufmann, San Mateo, CA, 1988 .\n[ 11] Charles S. Peirce. The Philosophy of Peirce - Se lected Writings, Edited by Justus Buchler, Har court , Brace and Company, 1940.\n[12] David Poole. A Logical Framework for Default Reasoning, Artificial Intelligence, vol. 36, pp 27- 47, 1988.\n[13] Yun Peng and James A . Reggia. Abductive In ference Models for Diagnostic Problem-Solving. Springer Verlag 1990.\n[14] Eugene Santos Jr. On the Generation of Alter native Explanations with Implications for Belief Revision. Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence, pp 339- 347."}], "references": [{"title": "The Origin& of Knowledge and Imagination, Yale", "author": ["Jacob Bronowski"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1978}, {"title": "Prob\u00ad abilistic Semantics for Cost Based Abduction", "author": ["Eugene Charniak", "Solomon E. Shimony"], "venue": "Proceedings of the 1990 National Conference on Artificial Intelligence", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Approximating Dis\u00ad crete Probability Distributions with Dependence Trees", "author": ["C.K. Chow", "C. N . Liu"], "venue": "IEEE Transaction& on Information The\u00ad ory, vol. IT-14 ,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1968}, {"title": "Cre\u00ad ative Problem Solving in Physics, Philosophy, and Painting : Three case Studies, Creativity and the Imagination edited by Mark Amsler", "author": ["Donald A. Crosby", "Ron G. Williams"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1987}, {"title": "Imagination in Science", "author": ["J . H . Van't Hoff"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1967}, {"title": "Local Computations with Probabilities on Graphical Structures and their Application to Expert Sys\u00ad tems, The Journal of Royal Statistical Society, Se\u00ad", "author": ["S.L. Lauritzen", "D. J . Spiegelhalter"], "venue": "ries B,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1988}, {"title": "Neapolitan Probabili&tic Rea&oning in Ezpert", "author": ["E. Richard"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1990}, {"title": "Principle& of Artificial Intelligence", "author": ["Nils Nilsson"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1980}, {"title": "Probabili&tic Rea&oning in Ezpert Sys\u00ad tem& : Networks of Plausible Inference", "author": ["J . Pearl"], "venue": "Brace and Company,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1988}], "referenceMentions": [{"referenceID": 0, "context": "In his book The Origins of Knowledge and Imagina\u00ad tion [1] Jacob Bronowski states \".", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "In his lecture Imagination and Science [6] J.", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "In [4] Donald Crosby, while dis\u00ad", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "The focus of many abductive reasoners [10, 13] has been on determining the most reasonable, that is, the par6imoniou6 or the mo6t probable or the lea6t cost [2, 14] hypotheses A, given the observed events G.", "startOffset": 38, "endOffset": 46}, {"referenceID": 1, "context": "The focus of many abductive reasoners [10, 13] has been on determining the most reasonable, that is, the par6imoniou6 or the mo6t probable or the lea6t cost [2, 14] hypotheses A, given the observed events G.", "startOffset": 157, "endOffset": 164}, {"referenceID": 6, "context": "One type of abductive reasoning in a context S : < T, P > has been described in [8, 10] as follows.", "startOffset": 80, "endOffset": 87}, {"referenceID": 8, "context": "One type of abductive reasoning in a context S : < T, P > has been described in [8, 10] as follows.", "startOffset": 80, "endOffset": 87}, {"referenceID": 6, "context": "The computational problem of the tra\u00ad ditional reasoner's abduction, as presented in [8, 10], is to find the most suitable assignment A\" in a given contextS.", "startOffset": 85, "endOffset": 92}, {"referenceID": 8, "context": "The computational problem of the tra\u00ad ditional reasoner's abduction, as presented in [8, 10], is to find the most suitable assignment A\" in a given contextS.", "startOffset": 85, "endOffset": 92}, {"referenceID": 8, "context": "Each Bayesian Network is a directed acyclic graph [10] if we assume a direction in each dependency from the antecedent nodes to the consequent node.", "startOffset": 50, "endOffset": 54}, {"referenceID": 7, "context": "The AI search algorithm A \u2022 given in [9] has been directly em\u00ad ployed in our implementation.", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "Some computational formalisms [10] can only compute the best or the best and the second best hypotheses and this would be an undesirable restriction on an imagining agent.", "startOffset": 30, "endOffset": 34}], "year": 2011, "abstractText": "Some instances of creative thinking require an agent to build and test hypothetical the\u00ad ories. Such a reasoner needs to explore the space of not only those situations that have occurred in the past, but also those that are rationally conceivable. In this paper we present a formalism for exploring the space of conceivable situation-models for those do\u00ad mains in which the knowledge is primarily probabilistic in nature. The formalism seeks to construct consistent, minimal , and desir\u00ad able situation-descriptions by selecting suit\u00ad able domain-attributes and dependency rela\u00ad tionships from the available domain knowl\u00ad", "creator": "pdftk 1.41 - www.pdftk.com"}}}