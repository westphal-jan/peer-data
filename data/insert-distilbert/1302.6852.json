{"id": "1302.6852", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2013", "title": "Generating Graphoids from Generalised Conditional Probability", "abstract": "we take a general approach to uncertainty on product spaces, and give those sufficient conditions for the independence structures of uncertainty measures to satisfy all graphoid properties. since these conditions are arguably more intuitive than some of the discrete graphoid properties, they essentially can be viewed as explanations why probability and certain other formalisms generate logical graphoids. the conditions cited include a sufficient activation condition for the intersection property which can still essentially apply even if there is a strong or logical relations hip between the variables. we indicate how these results can be used to produce theories of qualitative conditional probability which technically are semi - synonymous graphoids and graphoids.", "histories": [["v1", "Wed, 27 Feb 2013 14:20:29 GMT  (1170kb)", "http://arxiv.org/abs/1302.6852v1", "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)"]], "COMMENTS": "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nic wilson"], "accepted": false, "id": "1302.6852"}, "pdf": {"name": "1302.6852.pdf", "metadata": {"source": "CRF", "title": "Generating Graphoids from Generalised Conditional Probability", "authors": ["Nic Wilson"], "emails": ["nic@dcs.qmw"], "sections": [{"heading": null, "text": "Keywords: Graphoids, Conditional Inde pendence, Qualitative P robability.\n1 INTRODUCTION\nThe importance of the qualitative features of prob abilistic reasoning has often been emphasised in the recent AI literature, especially by Judea Pearl. An important qualitative aspect of probability is given by the graphoid properties, defined in [Pearl, 88] (see also [Dawid, 79; Smith, 90]) which sum up many of the properties of probabilistic conditional independence. In this paper we look at the reasons why probabil ity obeys these properties, with an eye to generating other uncertainty theories which share much of the same structure as probability, but represent different types of information, perhaps of a more qualitative nature.\nA fairly general family of uncertainty calculi on prod uct spaces is introduced, which we call Generalised Conditional Probability on Product Spaces (GCPP), and define two different types of conditional indepen dence for GCPPs. We show that under simple (and apparently fairly weak) conditions, conditional inde pendence for GCPPs satisfies the graphoid properties.\nThis means that then independence assumptions can be propagated using the graphoid inference rules, and can be represented and propagated using the (both directed and undirected) graphical methods of [Pearl, 88].\nIn section 2 we define independence structures, semi graphoids and graphoids. GCPPs are defined in sec tion 3, with examples and we show how they give rise to independence structures. Section 4 considers the Intersection property. In the literature it seems to be generally assumed that this only holds for probability distributions which are always non-zero; we show here that it holds much more generally, a sufficient con dition being a connectivity property on the non-zero values of the probability; exactly the same condition is sufficient for other GCPPs to satisfy Intersection. Section 5 considers different sufficient conditions for GCPPs to give rise to semi-graphoids. These are use ful for constructing uncertainty calculi which generate graphoids and might also be used for showing that an uncertainty calculus gives rise to a graphoid.\nIn section 6 we consider another view of GCPPs: as qualitative conditional probabilities. This view allows graphoids to be constructed from qualitative compar ative judgements of probability. Section 7 briefly con siders computation of GCPPs, and section 8 highlights some areas for further study."}, {"heading": "2 INDEPENDENCE STRUCTURES", "text": "Let U be a finite set. An independence structure I on U is defined to be a set of triples (X, Z, Y) where X, Y and Z are disjoint1 subsets of U. We write I(X, Z, Y) for (X, Z, Y) E I. For disjoint subsets X, Y \ufffd U, their union XU Y will be written XY.\nU is intended to be a set of variables, and I(X, Z, Y) is intended to mean that variables X are independent of variables Y given we know the values of the variables z.\n1 A collection A of sets is sa.id to be disjoint if for ea.ch X, Y E A, X n Y = 0.\n584 Wilson\nThe Graphoid Properties of Independence Structures\nI(X, Z, 0) (Trivial Independence) If I( X, Z, Y) then I(Y, Z, X) (Symmetry) If I(X, Z, YW) then I(X, Z, Y) (Decomposition)\nIf I(X, Z, YW) then I(X, ZY, W) (Weak Union)\nIf I(X,ZY, W) and I(X,Z, Y) then I(X,Z, YW) (Contraction)\nIf I{ X, ZY, W) and I( X, ZW, Y) then I( X, Z, YW) (Intersection)\nwhere W, X, Y, Z are arbitrary disjoint subsets of U (so, for example, I satisfies symmetry if and only if the above property holds for all disjoint X, Y and Z).\nIf an independence structure satisfies all these proper ties then it is said to be a graphoid; if it satisfies the first five (i.e, all except Intersection) then it is said to be a semi-graphoid. As we shall see in section 5, prob abilistic conditional independence is a semi-graphoid, and in certain situations a graphoid .\nThe definitions given here for semi-graphoid and graphoid differ from that given in [Pearl, 88], in that we require Trivial Independence to hold. However, our definition seems to be what Pearl intended2; it is not implied by other properties (consider the empty inde pendence structure) and it is satisfied by probabilis tic conditional independence so it is a (rather triv ial) counter-example to the Completeness Conjecture3 [Pearl, 88]; also without Trivial Independence, Markov boundaries don't necessarily exist (consider the empty independence structure again) which makes Theorem 4 of [Pearl, 88] incorrect.\nThe intersection of a family of (semi-)graphoids is a (semi-)graphoid. Hence, for any independence struc ture I, there is a unique smallest (semi-)graphoid con taining I.\n3 GENERALISED CONDITIONAL PROBABILITY ON PROD UCT SPACES (GCPPs)\nUncertainty measures are usually defined on boolean algebras. However, for our purposes of studying in dependence structures generated by the uncertainty measure, a different domain is natural.\n3.1 THE B ASIC DEFINITIONS\nU = {X1, ... , Xn} is said to be a set of variables if associated with each variable X; E U is a finite set of values X;. For X\ufffd U define X to be Ilx,EX X;. For\n2See, for example, the sentence before Theorem 4, p97 of [Pearl, 88].\n3Fatal counter-examples are given in [Studeny, 92].\ndisjoint X, Y \ufffd U, an element of XY may be written xy for x EX, y E Y. For disjoint X, Y \ufffd U we define XIY to be the set of all pairs {xly : x EX, y E Y}. The set ]1 is defined to be a singleton {T}. An element. xiT of X I\ufffd will usually be abbreviated to x (we are identifying Xl]l. with X).\nThe set u\u00b7 is defined to be u X IY. X,YCU XnY-=\ufffd\nA GCPP p over set of variables U is defined to be a function p: u\u2022 _. D for some set D containing differ ent distinguished elements 0 and oo such that for any disjoint X, Y \ufffd U and x EX,\n(i) p(x) = 0 if and only if for all y E Y, p(xy) = 0, and\n(ii) for any y E Y, p(xlv) = oo if and only if p(y) = 0.\nGCPPs will be viewed as measures of uncertainty; p(xly) may be thought of as some sort of measure of how plausible it is that the composite variable X takes the value x , given that Y takes the value y. The assignment p( x IY) = 0 is intended to mean that :c is impossible given y, i.e., X cannot take the value :1: if Y takes the value y. The inclusion of element oo in D is not strictly necessary; it is used as a notational convenience, and can be read as 'undefined'. We re quire (i) because: x is possible if and only if there is some value of Y which is possible when X takes the value x. We require (ii) because: if y is impossible then conditioning on y doesn't make much sense.\nNote that no structure on Dis assumed; for example, we do not assume that D \\ { oo} \ufffd IR, or even that D has an ordering on it.\nThe definition implies that p(T) ::}; 0, oo and that for any X\ufffd U and x EX, p(J.;)::}; oo .\nDefinition\nFor GCPP p over U and disjoint X, Y \ufffd U define pXIY to be p restricted to X IY and define px to be pXI0. For Z \ufffd U and z E Z define Pz: (U \\ Z)\" -> D by, for disjoint X, Y \ufffd U \\ Z, x E X, y E Y, Pz(xly) = p(xlyz). For disjoint X, Y \ufffd U \\ Z, p;'JY is defined to be Pz restricted to X IY, and P7 is defined to be p;'10. GCPP p over U is said to be a full GCPP over U if for every Z \ufffd U and z E Z such that p(z)::}; 0, p, is a GCPP over U \\ Z.\nThe function P.z may be thought of as p conditioned on Z = z. It turns out that, for GCPP p, p is a full GCPP if and only if for all disjoint X, Y \ufffd U, x E X, y E Y , [p(xly) = 0 \u00a2::=::> p(xy) = 0 and p(y)::}; 0].\n3.2 EXAMPLES OF GCPPS\nA probability function over set of variables U is de fined to be a function p: u\u00b7 - [ 0, 1] u { 00} such that\nGenerating Graphoids from Generalised Conditional Probability 585\nfor xJy E U*, (i) P(xJy) (ii) if P(y) # 0, P(xJy) P(x)::: LweU\\X P(xw).\noo \u00a2::::::} P(y) == 0; P(xy)/P(y); and (iii)\nThe definition implies that P is a full GCPP over U and P{T) = 1. The latter follows since P(T) is equal, by definition, to P(TJT) so by (i) above, P(T) = 0 if and only if P(T) = oo, which implies that P(T) is neither 0 or oo. We can now apply (ii) to get P(T) = P(TIT) = P(T)/P(T) which implies that P(T) = 1 as required.\nFor any (finite) set of variables U, there is a one-to-one correspondence between probability functions P over U and probability functions f on U, i.e., functions f:U--> [0,1] such that LuEuf(u) = 1; P restricted to U is a probability function on U, and conversely, a probability function f on U extends uniquely to a probability function over U using (i), (ii) and (iii).\nA Dempster possibility function over set of variables U is defined to be a function 11\": u\u00b7 -+ [ 0, 1) u { 00} such that for xJy E U*, {i) 1r(xly) = oo <==:> 1r(y) = 0; (ii) if 1r(y) # 0, 1r(xly) = 1r(xy)/1r(y); and (iii) 1r(x) = maxwEU\\X 1r(xw).\nAgain, the definition implies that 1r is a full GCPP over U and 1r(T) = 1. Dempster possibility functions are essentially Zadeh's possibility measures [Zadeh,78; Dubois and Prade, 88] and consonant plausibility func tions [Shafer, 76]; the definition of conditional possibil ity is obtained from Dempster's rule, and is not the one most commonly used, partly because it means that the range of 1T cannot be viewed as an ordinal scale (and most justifications of possibility theory require this).\nA special case of Dempster possibility functions are consistency functions over U, where 1T only takes the values 0, 1 and oo. 1r(xjy) = 1 is then intended to mean that, given that y is the true value of variables Y, it is possible that x is the true value of variables X. Every full GCPP p over U gives rise to a consistency function p* over U defined, for 1/; E U* , by p*('l/;) = p('I/J) if p(\"l/J) = 0 or oo, and p*(l/J) = 1 otherwise. Consistency functions appear in the theory of relational databases [Fagin, 77}, and also in [Shafer et a/., 87].\nA kappa function over set of variables U is defined to be a function K.:U*-+ {0,1, 2 , .. . ,oo,oon } (where oo n is different from the other elements) such that for xiy E U*, (i) K.(xly) = DOn {::::::::} \ufffd\ufffd:(y) = oo; (ii) if K(y) # oo, K(xjy) = x:(xy) - K(y); and (iii) x:(x) = minwEU\\X K(xw).\nThe definition implies that K.(T) = 0 and ,.., is a full GCPP over U (however, the labelling of the elements in the range of \"' is confusing: the zero of D in the definition of a GCPP is oo and the element meaning 'undefined' is oo n not oo) . Kappa functions are based on Spohn's Ordinal Conditional Functions [Spohn, 88]. An important difference is that the range of kappa functions has a maximum element oo; Spohn did not\ninclude a maximal element in the range of OCFs be cause he desired belief change to be reversible [Spohn, 88, p130}.\nKappa function x: can be transformed into a Demp ster possibility function 1r\" by 1r\"( \u00a2) == oo \u00a2::::::} .\ufffde(1/;) = oon and 1r\"(\"l/J) = 2-\ufffd<(;J;) otherwise, for 1/J E U*, where 2-oo is taken to be 0. For 1/J, \u00a2 E U*, x:( 1/;) == x:( \u00a2) \u00a2::::::} 1r\" ( 1/;) = 1r\"' ( \u00a2). This means that, for our purposes, kappa functions can be viewed as special cases of Dempster possibility functions.\nShafer's plausibility functions [Shafer, 76], also give full GCPPs; their dual functions, belief functions, and necessity functions, the dual of possibility functions, do not give GCPPs, since, for these, a value of 0 means a lack of evidence , rather than 'impossible'.\n3.3 INDEPENDENCE STRUCTURES OF GCPPS\nFor GCPP p over set of variables U, independence structures Ip and I\ufffd are defined as follows. Let X, Y and Z be disjoint subsets of U .\nIp(X, Z, Y) i f and only if p(:I:jyz) = p(J:Iz) for all x E X, y E Y, z E Z such that p(yz) # 0.\nI\ufffd(X, Z, Y) if and only if p(:z:Jyz) = p(xly'z) for all x EX, y,y' E Y, z E Z s11ch that p(yz) =P 0 and. p(y'z)#O.\nFor set S and functions g, h: S -+ D write g =00 h if they are equal when they are both defined, i.e., if for all s E S, [g(s) = h(s) or g(s) = oo or h(s) = oo] . This gives a simpler way of expressing; the two independence structures. For disjoint. subsets X, Y and Z of U,\nIp(X, Z, Y) if and only if for ally E Y, p;jiZ =\"\" pXIZ, and\nI\ufffd(X,Z,Y) if and only iffor all y,y' EY, XIZ 00 XIZ py == Py'\nTo understand the definitions, first consider the case when Z == 0. Then Ip(X, Z, Y) if the degree of plau sibility of x, p(x), does not change when we condition by any (possible) value y of Y. Thus our uncertainty about variable X does not change by learning the value of variable Y I\ufffd(X, Z, Y) holds if the degree of plau sibility of x given y, p(xiy) does not depend on the choice of (possible) value y of Y. The same 1\u00b7emarks apply for general Z, except that. now we must consider the degrees of plausibility conditional on each value z of Z.\nWe shall see in section 5 that for any GCPP p, lp satisfies Trivial Independence and Contraction, and, if Ip = I;, it satisfies Decomposition and Weak Union also .\n586 Wilson\n4 THE INTERSECTION PROPERTY\nThis is the only one of the graphoid properties that does not hold for all probability functions. [Pearl, 88, p87] appears to suggest that it only holds for proba bility functions which are every where non-zero. This turns out not to be the case, and we will see that a sufficient condition for Intersection to hold sometimes allows very strong logical dependencies between the variables.\nSet n is said t o be connected under relation R \ufffd n X n if the smallest equivalence relation on 0 containing R is the relation 0 X 0.\nLet p be a GCPP over set of variables U and let Y, W and Z be disjoint subsets of U. For z E Z.. define (YW)t,z = {yw E YW : p(ywz) =/; 0}. We say that (Y, W) is p, Z-connected4 if for all z E \u00a3.., (YW)t,,. is connected under the relation R defined by yw R y' w' {::::::::} y = y1 or w = w'. For GCPP p over set of variables U, we say that U if p-connected if for all disjoint subsets Y, W, Z of U, the pair {Y, W) is p, Z-connected.\nNote that these properties only depend on the set of elements of U for which pis zero (that is, those which are known to be impossible).\nThe above concepts are not quite as obscure as they appear at first sight. (YW)\ufffd. is the set of yw which are not known to be impossible when we know that Z = z. If we label Y as Y1, ... , Ym and W as W1, ... , Wn then YW = Y x W can be viewed as the squares of am x n chess board. Then y;Wj R Yi'Wj' iff i = i' or j = j', i.e., iff the rook chesspiece could move between the squares (i,j) and (i',j'). Let N, be the set of squares corresponding to (YW)t,.. We there fore have that (Y, W) is p, Z-connected iff for all z, it is possible to move between any two elements of N, us ing a sequence of rook moves, where each intermediate square is also in Nz.\nProposition 1\nLet p be a GCPP over a set of variables U.\n(i) For disjoint subsets X, Y, Z and W of U, suppose that {Y, W) is p, Z-connected and also that 1;(x, ZY, W) and I;(x, ZW, Y). Then I\ufffd(X, Z, YW) holds.\n(ii) If GCPP p over set of variables U is such that U if p-connec.ted then I\ufffd satisfies Intersection. 5\n4Interestingly, a very similar concept is important in Dempster-Shafer theory: in [Moral and Wilson, 94) it guar antees the convergence of Markov Chain Monte-Carlo algo rithms, and in (Wilson, 93) it is relevant to the justification of Dempster's rule.\n5Milan Studeny (89) has found a similar result (for the case of probability functions).\nIf GCPP p is non-zero on U then, trivially, U IS p connected, and so I; satisfies Intersection.\nExample\nLet U = {S, H1, H2}. Variable S ranges over shoe sizes, and the correct value is the shoe size of the (un known) next person to walk into my office. H1 and H2 both take integer values between 0 and 3000. The cor rect value of H 1 is the height in millimetres rounded down of this unknown person and the correct value of H2 is their height to the nearest millimetre.\nLet P be a Bayesian probability function on U, rep resenting our Bayesian beliefs about the variables. As described above, P extends uniquely to a GCPP over U. Now, P(ij) = 0 unless i = j or i = j- 1, where ij means H1 = i and H2 = j. Despite the very strong logical relationship between H1 and H2, ({HI},{H2}) is P, 0-connected, and so if we considered S to be logically independent of { H 1 , H 2}, in the sense that P(sh1h2) = 0 if and only if P(s) = 0 or P(h1h2) = 0, then U would be P-connected. This implies that Ip ( = If, by the results of the next section) would satisfy the Intersection axiom, and so would be a graphoid.\nIn any case, given knowledge of height to the near est millimetre, one will learn almost nothing more about shoe size by learning height in millimetres rounded down, so one might be tempted to make the subjective conditional independence judgement /p({S}, {H2}, {HI}). (Alternatively, if one did not know the precise meaning of H1 and H2, but had the values of the variables for previous visitors then it would take a vast amount of data to detect a de pendency.) Similarly one might be tempted to say /p( {S}, {HI}, {H2}). But these lead, using the last proposition, to Ip( {S}, 0, {H1, H2}) which is certainly unreasonable since there is a definite dependency be tween shoe size and height.\nThis example illustrates how careful one must be with subjective independence judgements for Bayesian probability (or any other GCPP). It also seems to sug gest that GCPPs, with definition 1;, cannot represent 'approximate independence' .\n5 SUFFICIENT CONDITIONS FOR GCPPS TO GENERATE GRAPHOIDS\nHere we consider simple sufficient conditions on GCPPs for its associated independence structures t.o satisfy semi-graphoid properties. Since probability functions, Dempster possibility functions, kappa func tions and consistency functions satisfy these proper ties with the exception of the conditions of proposi tion 4(v), these could be viewed as explanations for why they are semi-graphoids.\nGenerating Graphoids from Generalised Conditional Probability 587\n5.1 CONDITIONAL COHERENCE"}, {"heading": "It is easy to see that for any GCPP p, Ip \ufffd I\ufffd, i.e.,", "text": "if Ip(X, Z, Y) holds then I\ufffd( X , Z, Y) must hold. Fur thermore if p is inten de d to represent a generalised form of probability then a natural constraint is that Ip = I\ufffd. T his is because a sufficient condition for Ip = I\ufffd is the apparently reasonable condition: Conditional-Coherence: For any disjoint subsets X,Y,Z of U, x EX and z E Z, if for all y,y' E Y , p(xiyz) = p(xly'z) (i.e., p(xlyz) does not vary with y) then p(xlz) = p(xlyz) (i.e., p(xlz) is equal to that constant v alue).\nWe say t hat pis weakly conditional-coherent if Ip = I\ufffd.\nConsider conditioning on a fixed z E Z. The idea behind conditional coherence is that if the degree of plausibility of x given y (i.e, Pz(xiy)) is not dependent on which value y of Y we use, then one might expect that the degree of plausibility of x ( i.e . , Pz(x)) wou ld be equal to that constant value. The conditionals and marginal then cohere in a particular sense.\nConditional-Coherence is a re stri cted version of the S andwich Prin cip le [Pearl , 90; IJAR, 92]. Pl ausi bi l ity /belief functions and upper /lower probability func tions have good reason not to obey conditional coher ence: see e.g., [Wilson, 92; Chunhai and Arasta, 94). It is satisfied by probability and Dempster possibility functions and hence by con si stency and kappa func ti ons.\nProposition 2\nFor any GCPP p over set of variables U the indepen dence s tru cture IP satisfies Trivial Independence and Contraction, and lp satisfies Decomposition if and only if it satisfies Weak Union.\nNow suppose tha t p is weakly conditional-coherent. We then have\n(i) lp s atisfies Decomposition and Weak Union. Therefore if Ip satisfies Symmetry then it is a semi graphoid.\n(ii) If U is p-connected then Ip satisfies Intersection. Therefore if lp satisfies Symmetry then it is a graphoid.\nPerhaps the only one of t he grap hoid properties, with the exception of Trivial Independence, which imme diately seems natural is Symmetry. Surprisingly, it seems to be harde r to find natural sufficient conditions on p for Ip t o satisfy Symmetry. The follo wi ng result gives a fairly strong condition.\nProposition 3\nSuppose that p is a full GCPP over U such that for all Z \ufffd U and z E Z, there exists a fu ncti on o: R -+ D for some R \ufffd D x D such that\n(i) for al l xly E (U\\Z)*, Pz(xy) = p,.(xly)<>pz(Y), and (ii) if a o b = c <>a and a =f 0 then b = c. Then Ip satisfies Sy mme try.\n5.2 DETERMINISTIC RELATIONS BETWEEN JOINTS, CONDITIONALS AND MARGIN ALS\nIf I is an independence structure let its reflection IR be defined by IR(Y, Z, X) {::::} I(X, Z, Y). Clearly, I satisfies Symmetry if and only if I = I R. Let I5 = In JR be the symmetric part of I, so that for disjoint subsets X, Y, Z of U, I5 (X, Z, Y) if and only if I(X, Z, Y) and I(Y, Z, X).\nProposition 4\nSuppose p is a GCPP o ve r set of variables U. (i) If p(Tix) = p(T) for all X \ufffd U and x E X such\nthat p(x) =f 0, t hen (I\ufffd)R s ati sfies Trivial In depen  d ence.\n(ii) If for all disjoint W, Y, X \ufffd U there exists a func tion M su ch that for al l x E X, M(p!;YY) = p\"; then (I\ufffd)R satisfies De com positi on .\n(iii) If for all disjoint W, Y, X \ufffd U t here exists a fun c tion C s uch that for all x E X, C{p!;YY) = p!;YIY then (I\ufffd)R satisfies Weak Union.\n(iv) If pi s a full GCPP and for all disjoint W, Y, X \ufffd U there exists a function J such that (a) for all x E X, J(p!;YIY, p;;) = p!;YY and (b) J(g, h) = J(g',h) when for all wand y, [g(wiY) = g' (wly)\nor h(y) = 0], for functions g, g': WIY -+ D and h:Y-+ D wit h (g,h) and (g',h) in the domain of J. The n (I\ufffd)R sati sfies Contraction.\n( v) If p is non-zero on U and for all disjoint W, Y, X \ufffd U there exists a function S s uch that for all x E X, S(p!;YIY, p\ufffdIW) = p!;YY then (I\ufffd)R satisfies Inter section.\nThe functi on M in (ii) can be thought of as a marginal isation operator, and C in (ii i) as a conditioning op erator. J in (iv) gives a way of calculating the joint. dist rib ution from the conditional and marginal distri butions; condition (b) in (iv) can be omitted if p is non-ze ro on U; J is e ssentia lly just pointwise multipli cation for probability and Dempster possibility.\nThe existence of M in (ii) means that for each x, the joint distribution of Px on WY determines the marginal distribution (of Px on Y). To see how this condition is used , suppose I\ufffd(WY, 0, X) and p(x), p(x') =f 0; then p!;YY = p!;I:Y sop\ufffd = p\ufffd which leads to I\ufffd(Y, 0, X). Similar considerations apply to (iii), (iv) and (v).\nProp osi tion 4 implies that if p is a full GCPP, satisfy ing the conditions of (i), (ii), (iii) and (iv) above, an d\n588 Wilson"}, {"heading": "1; is symmetric then I; is a semi-graphoid; further more if U is p-connected then J\ufffd is a graphoid.", "text": "The result also leads to a way of constructing a semi graphoid from a GCPP even if I, and I\ufffd are not sym metric.\nProposition 5\nIf GCPP p over U is weakly conditional-coherent, and satisfies the conditions of (i), (ii), (iii) and (iv) of proposition 4 then If is a semi-graphoid. If, in addition, p satisfies the conditions of (v) then If IS a graphoid.\n6 QUALITATIVE COND ITIONAL PROBABILITY\nAnother way of viewing GCPPs is as Qualitative Con ditional Probabilities on Product spaces (QCPPs).\nA Symmetric QCPP (abbreviated to SQCPP) \ufffdover set of variables U is an equivalence relation on u\u00b7 U {O,oo} satisfying, for disjoint X, Y <:;;: U and x EX, (i) p(x) \ufffd 0 if and only if for ally E Y, p(xy) \ufffd 0,\nand\n(ii) for any y E Y, p(xly) \ufffd oo if and only if p(y) \ufffd 0. \ufffd is said to be consistent if 0 f. oo.\nIndependence structures I:: and 1{:, on U are defined analogously to the definitions for GCPPs: Ir:::(X, Z, Y) -\u00a2:::::::} xlyz \ufffd xly for all x, y, z such that yz \u00a2 0, and I\ufffd(X, Z, Y) -\u00a2:::::::} xlyz \ufffd xly' z for all x, y, y1, z such that yz f. 0 f. y' z. The framework of consistent SQCPPs is essentially equivalent to the framework of GCPPs. For any GCPP p over U we can define a consistent SQCPP \ufffdP over U, by first extending p toW U{O, oo} by defining p(O) = 0 and p(oo) = oo, and then, for \u00a2,1/; E U\"' U {O,oo}, defining t/J \ufffdP \u00a2 -\u00a2:::::::} p(t/;) = p(\u00a2). We then have Ir:::. = Ip and I!.. = Ip'. p -p Conversely, for consistent SQCPP \ufffd we can define GCPP Pr:::. by letting D = (u\u2022u{O, oo})/\ufffd, calling the equivalence classes of 0 and oo by 0 and oo respectively and, for xly E U*, defining Pr:::.(xly) = d-\u00a2:::::::} d 3 xly. We have I,, = Ir:::. and I\ufffd .. = I\ufffd- Also, for any SQCPP \ufffd. we have \ufffd(p..,)= \ufffd-\nRelation \ufffd is said to be a QCPP over set of variables"}, {"heading": "U i f it is a reflexive transitive relation on U* U {0, oo}", "text": "such that its symmetric part \ufffd (the intersection of \ufffd and \ufffd) is a SQCPP over U.\nQCPPs might be thought of as probability functions without the numbers; a statement such as xlz \ufffd yiz could mean 'given z, value y is at least as probable as x'. QCPPs generate independence structures via their\nassociated SQCPPs. The correspondences between GCPPs and SQCPPs mean that the sufficient con ditions for independence structures to satisfy the graphoid properties given in section 5 can be trans lated into sufficient conditions for SQCPPs (and hence QCPPs) to generate independence structures with those properties.\nIf consistent \ufffd is a full SQCPP (i.e, for all xly E u\u00b7 I xly \ufffd 0 -\u00a2:::::::} xy \ufffd 0 and y ';j:; 0), a sufficient condi tion for I\ufffd to satisfy Symmetry is the following 'cross multiplication' condition:\nFor all disjoint X, Y, Z <:;;: U, x E X, y E Y, z E \ufffd\ufffd if xz \u00a2 0 and xyzlyz \ufffd xzlz then xyzlxz \ufffd yziz, where we have trivially extended \ufffd to elements :r:yly of XYIY (for disjoint X, Y <:;;: U), by placing xyiy in the same \ufffd-equivalence class as xly.\nConstructing QCPPs\nWe will often want to construct a QCPP from a num ber of different types of information:\n(i) some qualitative probability relationships we ex pect always to hold, such as, perhaps , 0 \ufffd xl11 for all xiy E U*;\n(ii) some desirable properties of=:::;, such as the above sufficient condition for Symmetry of Ir:::., and other conditions that imply graphoid properties;\n(iii) an agent's comparative probability judgements, e.g., statements of the form xlz =:::; x or xlz \ufffd yjz;\n(iv) an agent's conditional independence judgements.\nThe obvious way to at tempting to construct a QCPP for a particular situation is to treat (i) and (iii) as sets of axioms and (ii) and (iv) as sets of inference rules, and generate the QCPP from these. However, there is a technical problem: because of the condi tions yz \u00a2 0, the conditional independence assump tions cannot quite be viewed as sets of inference rules. We can solve this by requiring that the user gives (ex plicitly or implicitly) a II the values u of U such that u \ufffd 0 (that is, the set of all u which are considered im possible); the key point here is that the application of the rules must not lead to any more zero values of U. The conditional independence assumptions can now be viewed as inference rules, since they are now closed un der intersection. For the same reason, we require the properties in (ii) also to be closed under intersection, once the zeros of U are determined. Naturally, if we have included in (ii) properties which imply that Ir:::. is a semi-graphoid, then we can propa gate conditional independence assumptions using the semi-graphoid properties, or using the graphical meth ods described in (Pearl, 88].\nGenerating Graphoids from Generalised Conditional Probability 589\n7 COMPUTATION OF GCPPS\nHere we only consider computation of values of the joint distribution of a GCPP, leaving other aspects of computation for future work.\nLet I be an independence structure on U. For X; E U, and W \ufffd U \\{Xi} the set B \ufffd W is said to be an /-Markov boundary of X; with respect to W if B is minimal such that I( {Xi}, B, W \\B).\nIf I satisfies Trivial Independence then there is at least one /-Markov boundary of X; with respect toW, and if I satisfies Weak Union and Intersection then there is at most one.\nProposition 6\nLet p be a GCPP over U, which is labelled X 1, .. . , Xn. S uppose there exists a function6 o: D x D - D such that for all xjy E U*, p(xy) = p(xjy) o p(y). Then for any x; E \ufffd, i = 1, . . . , n,\nwhere the repeated application of o is performed right-to-left, b; is x1 \u00b7 \u00b7 \u00b7 Xn projected onto B;, and B; is an Ip-Markov boundary of X; with respect to {Xt, ... ,X;_t}.\nThe Boundary Directed Acyclic Graph [Pearl, 88] is formed by letting the parents of each X; be B;. The a bove result shows that, just like for probability, the values of the joint distribut ion (i.e, p on U) can be calculated using this DAG together with, for each X;, the matrix giving the values of p on X; conditional on the values of the parents of X;. If Ip is a semi-graphoid, then a result in [Verma, 86] (also Theorem 9 of [Pearl, 88]) implies that the Bound ary DAG is a minimal I-map for IP so that conditional independence properties of Ip can be derived by test ing for d-separation in the DAG.\n8 D ISCUSSION\nThe sufficient conditions we have found for GCPPs to generate semi-graphoids seem natural and fairly weak. However, we clearly we need to look for more (sensible) examples of GCPPs that generate semi-graphoids, via our sufficient conditions. For example, it would be desirable to find simple such uncertainty formalisms which take values which are not totally ordered.\nFor Qualitative Conditional Probability, we need to consider appropriate extra axioms and inference rules to add to the system. The relationship between Quali tative Conditional Probability and relations on condi tional objects [Goodman et al, 91] would be interesting\n6The existence of this function is very closely related to one of the axioms in a justification of Bayesian probability [Cox, 46].\nto explore, as would its relationship with comparative probability [Walley and Fine, 79]. There may well also be connections between GCPPs and the framework of [Shenoy, 92] which uses a pro duct definition of inde pendence.\nAcknowledgements\nThanks to Serafin Moral and Luis de Campos for some useful discussions, and to Milan Studeny and the ref erees for their helpful comments. The author is sup ported by a SERC postdoctoral fellowship. I am also grateful for the use of the computing facilities of the school of Computing and Mathematical Sciences, Ox ford Brookes University.\nReferences\nChunhai, Y., and Arasta, F ., 94, On Conditional Belief Functions, International Journal of Appr\u00b7oxirnate Reasoning, 10: 155-172.\nCox, R., 46, Probability, Frequency and Reasonable Expectation , American Journal of Physics 14:1, January-February 1-13. Reprinted in Readings in Uncertain Reasoning, G. Shafer and J. Pearl (eds.), Morgan Kaufmann, San Mateo, California, 1990.\nDubois, D. and Prade, H., 88, Possibility Theory: An Approach to Computerized Processing and Uncer tainty, Plenum Press, New York.\nFagin, R., 77, Multivalued Dependencies and a New Form for Relational Databases, A CM Trans. on Databases 2 (no. 3): 262-278.\nGoodman, I. R., Nguyen, H. T., and Walker, E. A., 91, Conditional Inference and Logic for Intelligent Systems: A Theory of M eas1tre-Free Conditioning, North-Holland , Amsterdam.\nIJ AR, 92, International Journal of Approximate Rea soning, 6, No. 3 [special issue].\nMoral, S., and Wilson, N . , 94, Markov Chain Monte Carlo Algorithms for the Calculation of Dempster Shafer Belief, submitted to AAAI-94.\nPearl, J ., 88, Probabilistic Reasoning in Intelligent Systems: Networks of Plausibl e Inference, Morgan Kaufmann P ublishers Inc .\nPearl, J., 90, Reasoning with Belief Functions: An Analysis of Compatibility, International Journal of Approximate Reasoning, 4(5/6), 363-390.\nShafer, G., 76, A Mathematical Theor\u00b7y of Evidence, Princeton University Press, Princeton, NJ.\nShafer, G., Shenoy, P. P., and Mellouli, K., 87, Prop agating Belief Functions in Qualitative Markov Trees, I nte rnational Journal of Approximate Rea soning 1 No. 4: 349-400.\nShenoy, P. P., Conditional Independence in Uncer tainty Theories, Proc. Eig h th Conference on Un-\n590 Wilson\ncertainty in Artificial Intelligence, Morgan Kauf mann, 284-291.\nSmith, J. Q., 90, Statistical Principles on Graphs, 89- 120, in Influence Diagrams, Belief Nets and Deci sion Analysis, R. M. Oliver and J. Q. Smith (eds.), John Wiley and Sons.\nSpohn, W., 88, Ordinal conditional functions: a dy  namic theory of epistemic states. In: Causation in Decision, Belief Change and Statistics (W. Harper, B. Skyrms, eds.), 105-134.\nStudeny, M., 89, Multiiinformation and Conditional Independence I., Research Report n. 1619, In stitute of Information Theory and Automation, Prague, October 1989.\nStudeny, M., 92, Conditional In dependence Rela tions have no F inite Complete Characterization, in Transactions of the 11th Prague Conference on Information Theory, Statistical Decision Functions and Random Processes, vol B, Academia, Prague, 377-396.\nVerma, T. S., 86, Causal Networks: Semantics and Expressiveness, Technical Report R-65, Cognitive Systems Laboratory, University of California, Los Angeles.\nWalley, P. and Fine, T. L., 79, Varieties of Modal (Classificatory) and Comparative Probability, Syn these 41: 321-374.\nW ilson, N., 92, How Much Do You Believe?, Interna tional Journal of Approximate Reasoning, 6, No. 3, 345-366.\nWilson, N., 93, The Assumptions Behind Demp ster's Rule, Proceedings of the Ninth Conference of Uncertainty in Artificial Intelligence {UAI93}, David Heckerman and Abe Mamdani (eds.), Mor gan Kaufmann Publishers, San Mateo, California, 527-534.\nZadeh, 78, Fuzzy Sets as a Basis for a Theory of Pos sibility, Fuzzy Sets and Systems, 1: 3-28."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "We take a general approach to uncertainty on product spaces, and give sufficient condi\u00ad tions for the independence structures of un\u00ad certainty measures to satisfy graphoid prop\u00ad erties. Since these conditions are arguably more intuitive than some of the graphoid properties, they can be viewed as explana\u00ad tions why probability and certain other for\u00ad malisms generate graphoids. The conditions include a sufficient condition for the Inter\u00ad section property which can still apply even if there is a strong logical relationship between the variables. We indicate how these results can be used to produce theories of qualita\u00ad tive conditional probability which are semi\u00ad graphoids and graphoids.", "creator": "pdftk 1.41 - www.pdftk.com"}}}