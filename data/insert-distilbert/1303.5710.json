{"id": "1303.5710", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Combination of Upper and Lower Probabilities", "abstract": "in this paper, we consider clearly several predominant types of information and those methods of combination rule associated with incomplete probabilistic systems. we discriminate between'a priori'and evidential information. the predominant former one is as a description of the whole population, the latest is a restriction based on trait observations for a particular case. then, we propose different combination methods for each one of them. we also consider conditioning information as the heterogeneous combination of'a priori'and for evidential trait information. the evidential information is represented as a convex set of likelihood functions. these will thereafter have an obviously associated possibility distribution with behavior according to classical possibility theory.", "histories": [["v1", "Wed, 20 Mar 2013 15:30:02 GMT  (369kb)", "http://arxiv.org/abs/1303.5710v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jose e cano", "serafin moral", "juan f verdegay-lopez"], "accepted": false, "id": "1303.5710"}, "pdf": {"name": "1303.5710.pdf", "metadata": {"source": "CRF", "title": "COMBINATION OF UPPER AND LOWER PROBABILITIES", "authors": ["Jose E. Cano", "Serafin Moral", "Juan F. Verdegay-Lopez"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we consider several types of in formation and methods of combination asso ciated with incomplete probabilistic systems. \\Ve discriminate between 'a priori' and evi dential information. The former one is a de scription of the whole population, the latest is a restriction based on observations for a particular case. Then, we proposse different combination methods for each one of them. 'We also consider conditioning as the hetero geneous combination of 'a priori' and eviden tial information. The evidential information is represented as a convex set of likelihood func tions. These will have an associated possi bility distribution with behavior according to classical Possibility Theory.\n1 INTRODUCTION\nIn Probability Theory the main method of incorpo rating new information is through conditioning. In general, there is no doubt about how to represent ini tial information and how to update it in the light of new observations. However, when we work with pro babilistic intervals, there is a bit of a mess. Several formulas of combining and conditioning are available, the main problem being which formula or method to use in a particular case. The reason for this is the lack of a firm semantic basis.\nIn this paper we present a method of tackling incom plete probabilistic information, which tries to avoid\nthis kind of ambiguity. The main feature is the clear distinction between evidential and 'a priori' informa tion. Evidential information will be represented as a convex set of likelihood functions with an associated possibility distribution. The interpretation of these possibilities will have a probabilistic basis. However their behavior will be very similar to classical possibil ity distributions (Dubois, Prade 1988; Zadeh 1978).\nWe shall discriminate between the combination of ev idential information and the combination of 'a priori' information. Conditioning will be a kind of heteroge neous combination: 'a priori' and evidential informa tion. The result is called 'a posteriori' information. This 'a posteriori' information is different from that resulting from applying known formulas of calculat ing conditional information: Dempster conditioning (Dempster 1967) and upper and lower conditioning (Dempster 1967; Campos, Lamata, llforal 1990; Fa gin, Halpern 1990).\nThe problem of applying Dempster-Shafer Theory of Evidence (Dempster 1967; Shafer 1976) to upper and lower probabilities is that there is only one way of com bining information, the so called Dempster rule of com bination. Furthermore, the most used conditioning is Dempster conditioning (Dempster 1967; Moral, Cam pos 1990) which is a particular case of Dempster rule. So all information is combined in an homogeneous way. This gives rise to cases in which Dempster rule seems rea\ufffdonable and cases in which the results are not very intuitive. In general, Dempster conditioning produces very narrow intervals, if upper and lower probabilities interpretation is considered (Pearl 1989).\n62 Cano, Moral, and I.6pez\n2 'A PRIORI' INFORMATION\nLet X be a variable taking values on a finite set U = { u1, ... , Um}. An 'a priori' information about X is a convex set of probabilities,\nwhere { P1, P2, ... P,} is a finite set of probabilities on U. In general, this information is applicable under a determined set of conditions Co under which X takes its values. The meaning is that one element of 1i. is the true probability distribution associated with X, under conditions Co. Given a set of possible probabilities C = {PI, P2, ... Pn}, we may associate a convex set of probabilities with it, its convex hull\n{ n n } C= \ufffda;P; i\ufffda;=l\nC and C may be considered for the same experiment with different interpretations. For example, let us con sider that we have two urns, ul and u2. ul has 99 red balls and one black. U2 has one red ball and 99 black ones. If we pick up a ball randomly from one of the two urns then, we have two possible probabilities, C = { P1, P2 }, for the color of the ball, one for each urn. However, if the experiment is to select an urn and then pick a ball, the frequencies of the colors will be given by one probability aP1 + (1- a)P2, where Cl' is the probability of selecting ul , and (1- a) the probability of selecting U2. As a is unknown, we have a convex set of probabilities C. We will always consider the second interpretation. That is, that we have a pre vious experiment consisting in randomly selecting one of the possible probability distributions. Therefore a set of probability distributions will be equivalent. t.o its convex hull.\nAbove assumption may have some problems . For ex ample, assume that we have two variables, X 1 and X2 , which are known to be independent. For the first vari able we have the convex set of possible probabilities C1 = {P1, Q1}, and for the second variable, the convex set C2 = {P2, Q2}\u00b7 Given these conditions, the possi ble set of probabilities for the variable X= (X1,X2) IS\nHowever, this set Cis not nccccsarily convex. \\Ve may have:\n\u2022 Sometimes the convex combination\nis not equal to a product such as\nthat is, an element from C. This has been considered also by B. Tessem (1989), in a slightly different problem: The induced set of prob abilities in X 2 , from a con vex set of 'a priori' distri butions on X, and a convex set of conditionals. It is shown that this set is not necessarily convex.\nWe shall always represent uncertainty as a convex set. of probabilities. Then, in situations like this, we shall do an approximation of the possible set of probabili ties calculating its convex hull. From a practical point of view, this approximation is equivalent to assuming that the selection of probabilities for X 1 and X 2 may be done in a dependent way. That is, with probabil ity a we may choose P1 for X1 and P2 for X2 ; and with probability 1 - a it is possible to choose Q1 for X1 and Q2 for X2 . This does not imply that the variables X1 and X2 are dependent. In fact, the in dividual probabilities are combined by multiplication (P1.P2 and QJ.Q2). Only the probability distributions are selected on a dependent way. In a particular case, it may occurs that the selection of probabilities is in dependent. But then, we are adding some extra prob abilities. From this point of view, we are losing some information, but we lose it for the sake of simplicity. Convex sets are more manageabl\"' than general sets.\nThe combination of 'a priori' convex sets of prob ability distributions has been considered in Campos, Lamata, Moral (1988). If we have two 'a priori' con vex sets of probabilities cl and c:J given for t.he same set of conditions, then the conjunction will be the in tersection of the convex hulls, cl IIC2. The disjunction will be the convex hull of t.he union: cl u c2. This kind of combination is the one applied in situa tions like the following: We know that if we pick up a ball from this urn the probability of being red is be tween 0.75 and 0.85, and from other source we obtain that this probability is between 0.8 and 0.9. Then we may apply this combination (conjunction in this case) and deduce that the probability is between 0.8 and 0.85.\nIf C1 and C2 are convex sets of probabilities, but rel ative to different contexts Co1 and Co2, then no 'a priori' information may be deduced in the context Co1 U Co2 (both conditions are verified), except if one of the probabilities is degenerated. The following sit uation is perfectly possible:\n\u2022 C1 ={pi} in conditions Co1, where\n\u2022 C2 = {p2} in conditions Co2, where\n\u2022 C3 = {p3} in conditions Co1 u Co2, where\nHowever, if we have the following information,\n\u2022 Il: \"Most of Computer Science students (CS) are single (S)\"\n\u2022 12: \"Most of young people (Y) are single (S)\"\nWhere I1 and 12 may be translated in the probabilistic information,\n\u2022 Under conditions Co1 == { CS}\nP1(S) == 0.99 Pl(-,5) == 0.01\n\u2022 Under conditions Co2== {Y}\nPz(S) == 0.99 pz( \u2022S) == 0.01\nand nothing is known about the probability of being single UtHler conditions {CS,Y}, then common sense says that in this case it would not be wrong to assume that \"Most young people stu ding Computer Science are single\". It migth occur that young Computer Sci ence students are a rare combination and most of them are married. But if nothing is said about this, then it may be considered that there are not strange interac tions and that under conditions Co1 and Co2 we may use c3 == cl n c2 == {pi} == {pz}.\nIn short, to do the combination of 'a priori' proba bilitic convex sets is neccesary to determine whether they are given in the same context. In shuch a case, we calculate the conjunction by the intersection of con vex sets and the disjunction by the convex hull of the union. If the sets of probabilities are given under two different contexts, then nothing can be said about the combination. However, when there is no more avail able information, then the former combination could be considered by default, but taking into account that this is an additional assumption we make about the problem.\nVery often, 'a priori' information is given by means of probability intervals or probability envelopes. A probability envelope is a pair of applications\nI, u : P(U) _____. [0, 1]\nCombination of Upper and Lower H:obabilities 63\nsuch that there exist a family P of probability mea sures verifying\nI( A)== inf P(A) u(A) ==sup P(A) pEP pEP\nIt is clear that given a set of probabilities, C1, we may associate with it a probability envelope. However a probability envelope (I, u) may be defined from differ ent sets of probabilities. But, in every case, there is always a ma.ximal family given by\nP == {Pii(A) \ufffd P(A) \ufffd u(A),VA <;:; U}\nIf C is a set of probabilities and we calculate the asso ciated envelope (I, u), this envelope is equivalent to a maximal family P. Always, we have C <;:; P. Then, if we transform a set C on an envelope we may consider that some information is lost (there are more proba bilities being possible).\n3 EVIDENTIAL INFORMATION\n3.1 LIKELIHOOD FUNCTIONS\nAssume that we do not have an 'a priori' information about X, but we have observed 0, and we have a family of conditional probabilities,\nP(OIX ==a;), a; E U.\nTaking into account that nothing is known about 'a priori' probabilities of a; are not known then nothing can be said about 'a posteriori' probabilities of a; after observing 0, with the exception that if P(Oia;) == 0 then we can conclude that a; is impossible. Cosider the following example: We have U == {a1, a2} and\nP(O\\aJ) == 1, P(O!a2) == 0.001\nThen after observing 0, we may have p(a2IO) == 1 (p(a2IO) == 0) if the unknown 'a priori' probability was p(a2) == 1 (p(az) == 0). However, it is clear that after observing 0, a2 should be considered less possible than a1. In conclussion the information provided by 0 can not be represented by probabilities or interval probabilities. We shall do as in Clasical Non-Bayesian Statistics and say that 0 defines a likelihood function, 10, on U, which is a mapping from U on the interval [0, 1], given by\nlo(a;) == P(OIX ==a;), a; E U.\nThis likelihood may be interpreted as a possibility distribution, 11'0, which is not neccesarily normalized (Smets 1982).\n64 Cano, Moral, and I.6pez\nThe possibility measure associated with 1r 0 is defined (Zadeh 1978) as a mapping\ngiven by\nITo : P( U) ___, [0, 1]\nITo( A)= max 7r0(a) aEA\nThe following proposition relates a possibility measure with probability bounds.\nProposition.- Given a possibility measure ITo on U, then\nITo(A) 2: P(O n A)\nand these bounds are optimal under information 0.\nProof.-\nP(OnA)= L P(On{ai})= a,EA\nL P(Oia;).p(a;) = L 'lro(a;).p(a;)\nNow, taking into account that,\nL p(a;) = P(A)::; 1 , aiEA\nwe get the required inequality,\nP(O n A) ::0 max 7r0(a;) =ITo( A). a,EA\nThe bounds are optimal in the sense that being\nthen, if\nwe may consider the 'a priori' probability\n- p(a;) = 0, othenvise\nWith this 'a priori' probability,\nP(O n A)= p(ak)P(Oiak) = ITo(A),\nthat is, equality is given. \u2022\nThese bounds are not associated with conditional probabilities, P(.IO), but with probabilities of consis tency with information 0. To consider real conditional probabilities we have to divide by P(O), but this prob ability is unknown, and we only have an upper bound\nIT0(U). The normalization by this value may be con sidered as an upper relative degree of consistency,\n, (A) = ITo(A) go ITo(U)\nFrom this upper value, we may define the lower limit as\ngo,(A) = 1- go'(A).\n3.2 CONVEX SETS OF LIKELIHOOD FUNCTIONS\nOn the other hand, it is possible that the exact val ues of conditional probabilities are not known. For example, we only have probability intervals\nb; ::; P(OIX =a;) ::; c;\nIn this case, observation 0 does not define only one likelihood function, but a convex set of likelihoods, those verifying a; ::=; l( a;) ::=; c; . This convex set will be called the evidential information associated with 0 and denote it by E0\u2022\nAn evidential information also has an associated pos sibility distribution,\n7r0(a;) = max l(a;) lEE.\nThis possibility distribution also verifies a similar pro position to the above, relative to probability bounds. In the same way, we may associate the pair of lower upper measures (go, go'). With the same reasoning as in 'a priori' information, it will be considered that a set of likelihood functions, E, is equivalent to its convex hull, E. A special likelihood function is the null likelihood, IN , defined as\nThis likelihood function comes from an observation, 0, for which P(Oia;) = 0 are possible conditional probabilities (there may be an other possible conditional probabilities defining other likelihood functions associated with observation 0).\nIt is clear that after observing 0, these conditional probabilities are impossible, because we have\nP(O) = l:P(Oia;)p(a;) = 0\nThen IN has to be considered as an impossible like lihood. It could be thought that when IN appears it\nwould be better to remove it. In fact, the removing of something impossible must not change our state of mind. But for the same reason the inclussion of IN should not have any effect in our final results. This will happen in our model: never the final probability intervals will chage because of the inclussion or elimi nation of IN.\nTaking the above reasons into account we shall extend our original equivalence relation among sets of likeli hood functions, considering that two sets, E1 and E2, are equivalent if and only if E1 U {IN} = E2 U {IN}. That is, if previously including the null linkelihood their convex hulls are equal. The effect of this equiv alence relation will be that if we have a likelihood, I, we do not have to consider any likelihood ex .I (where ex ::; 1). This is not strange. Having I, the likelihood ad (ex::; 1) defines the same relative possibilities, but with a lower norma.lization factor.\nIn the following, the convex set E U {IN} will be de noted as C( E). The disjunction and conjunction of evidential informa tion are defined in an analogous way to the disjunction and conjunction of 'a priori' information.\nIf E1 and E2 are two sets of evidential information, the disjunction, E1 V E2 , is defined as C(E1 U E2). The conjunction, E1 1\\ E2 , is defined as the intersection of the couvex hulls: C(E1) II C(Ez). Here is important to distinguish between the conjunc tion of evidential information, C(E1) II C(E2) and the evidential observation associated with the conjunc tion of two observations 01 and 02, Eo,AO,\u00b7 The first is applied when we know that E1 and E2 are two convex sets of likelihood functions associated with the same observation and is performed calculating C(E1) n C(E2). The last, when we have two sets of likelihoods corresponding to two observations, 01 and 02, and we want to calculate the evidential informa tion associated with 01 1\\ 02. The same is applied for the disjunction.\nFor the calculus of evidential information Eo, AO, from the evidential information Eo, and Eo,, we have to calculate the possible values for the probability P( 01/\\ Ozlai) from the values of P(01Ia;) and P(02Ia;). The only thing we can say is that\nmax {0, P(01Ia;) + P(02Ia;)- 1}::; P(01 /\\ 02la;)::;\n::; min {P(01Iai), P(02Ia;)} then we have to consider in Eo, 1\\0, all the likelihood functions, I, verifying\nCombination of Upper and Lower ftobabilities 65\nwhere 11 E Eo,,l2 E Eo,. For the associated possibility distribution, we get\nJro,Ao,(a;) =min {1ro, (a;), Jro,(a;)}\nthat is, the same formula as in classical Possibility Theory (Dubois, Prade 1988), but without assuming any additional assumption of coherence or compatibil ity of observations.\nIf we assume that 01 and 02 are conditionally inde pendent given the value of X then we get:\nJro,Ao,(a;) = Jro,(a;).Jro,(a;).\nAnother assumptions may be make to obtain different combination formulas for operations on observations.\n4 COMBINATION OF 'A PRIORI' AND EVIDENTIAL INFORMA TION\nHere, it is considered the combination of a con vex set of 'a priori' probabilities, C, with an evidential infor mation, E. The method is a generalization of Bayes Theorem and is based on the formula of conditioning in Moral, Campos ( 1990). The generalization given here is different from the one given by Smets (1978, 1981). The main difference comes from the fact that we assume that an observation defines a consonant ev idence (a possibility) and Smets works with general evidential information. Also, in our approach, we dis tinguish between evidential and 'a priori' information using different methods according to the particular sit uation.\nThe combination of a probability measure p about the values of X and a possibility associated with obser vation 0 7r0(a;) = p(OIX = a;), is given by the function,\nh(a;) = p(a;).7r0(a;).\nWe shall denote this function h by p x 1!'0\u2022 After normalization, h determines the values of conditional probability,\nThe normalization factor may be considered as the likelihood of the 'a priori' probability given 0. A very small likelihood of h may make us suspect the initial values of probability and therefore, the resulting con ditional probabilities. Furthermore, in this case these\n66 Cano, Moral, and IDpez\nvalues will be very sensitive to the lack of accuracy of initial probabilities.\nThe above expression is precisely Bayes formula, de veloped in two stages: first combination and after nor malization. In a similar way, we shall define the com bination of an 'a priori' convex set of probability dis tributions and an evidential information Eo as the set,\nH = C({p x 1rJp E C, 1r E Eo})= C0Eo\nAs before, and by analogous reasons we shall assume that two combination sets, Hi and H2, are equivalent if C(Hi) = C(H2).\nTo assign probability intervals (Moral, Campos 1990) to the set H we select the extreme points of H, hi, ... , hn. Each hk different from the null function can define a probability value, normalizing it by its likelihood,\nWe could now calculate the upper and lower probabil ity values by means of the expression,\nto *(AID)= max Pk(AIO), k\nto.(AJO) = min Pk(AJO) k\nThis is equivalent to upper-lower conditioning (Demp ster 1967; Fagin, Halpern 1990). But in this method there is some missing information. In effect, given that hk is the true combination, then in this case, the prob ability of observation 0 is Lj hk(aj ). According to our definition of possibility, this defines a possibility about the combination functions and the corresponding con ditional probabilities, given by\n7r(Pk(.JO)) = I>k(Xj) j\nThat is, we do not only have a set of conditional prob\nabilities, we also have a possibility about them. This possibility also defines upper and lower probabilities: If D <;::: {pi(.JO), ... ,pm(.JO)}, then\n*(D) _ II(D)\n90 - maxk 7r(Pk(.JO)) '\nNow, we define the upper and lower conditional inter vals in the following way. The upper and lower values of B given observation 0, are calculated by means of\nChoquet integral (Choquet 1953) of conditional prob abilities with respect to the measures g\ufffd and 9o., re spectively:\nP;(B) = I(Pk(BIO)/g/),\nPo.(B) = I(Pk(BJO)/go.),\nthese integrals being defined in the following way,\nl(hjg) = fooo g(Ha)da where\n- h is a function h : U --+ Rt\n- Ha = {x E UJh(x) 2: a}\n- g is a monotone fuzzy measure (non necessarily additive (Sugeno 1974)).\nIt is important to point out that the result of the com bination is the set H, not the intervals. For exam ple, we may have the same intervals coming from pure possibilistic information or a convex set of probability distributions. However, after combining each one of them with new information, the intervals may become very different. The following example illustrates these ideas.\nExample.- Let us consider a set U = { 1, 2, 3} and an obsevation Oi such that\nThe intervals defined by this observation are\n0 --+ (0, OJ {2} --+ (0, 0.5]\n{1,2} __, [0.8, I] {2, 3} --+ [0, 0.5)\n{1} --+ (0.5, 1) {3} --+ (0, 0.2)\n{1,3}- [0.5,1) {1, 2, 3}- [1, 1)\nNow assume that we have a convex set of probability distributions, C, with extreme points\n1 2 3 Pi 1 0 0 P2 0.5 0.5 0 P3 0.5 0.3 0.2 P4 0.8 0 0.2\nThe intervals associated with it are the same as before. However the information is different and it is combined in a different way. Assume now that we have observa tion 02 and\np(02Jl) = 0.1, p(02l2) = 0.6, p(0213) = 1\nIf we assume that 01 and 02 are conditionally inde pendent given the value of X, then the conjunction of these two observations gives rise to the following possibility and intervals.\n0 ---. [0, 0] {2}---. [0.33, 1]\n{1, 2}---. [0.33, 1] {2,3}---. [0.67, 1]\n{1}---. [0, 0.33] {3}---. [0,0.67]\n{ 1, 3} ---. [0, 0.67] {1,2,3}-+[1,1]\nIf we combine observation 02 with the convex set C, we get the convex set H with extreme points\n1 2 3 hi 0 0 0 h2 0.1 0 0 h3 0.05 0.3 0 h4 0.05 0.18 0.2 h5 0.08 0 0.2\nThe corresponding normalized probabilities and possi bilities are\n1 2 3 71' undefined undefined undefined 0\n1 0 0 0.1 0.14 0.86 0 0.35 0.12 0.42 0.46 0.43 0.29 0 0.71 0.28\nThe intervals, calculated using Choquet integral are,\n0 ---. [0, 0] {2}- [0.15, 0.78]\n{1,2}---+ [0.37,0.91] {2, 3} ---+ [0.60, 0.88]\n{ 1} ---+ [0.12, 0.40] {3} ---+ [0.09, 0.63]\n{1, 3} ---+ [0.22, 0.85] {1, 2, 3}- [1, 1]\nwhich are really different to the corresponding to the combination of 01 and 02.\nThe most important thing to remark in this combina tion method is that it is a mixture of Classical Statis tics based on likelihood functions and Bayesian Statis tics. When we have a probabilistic 'a priori' infor mation then Bayes Theorem is obtained. When we do not have 'a priori' information a likelihood function or its corresponding possibility is considered. When we have an 'a priori' information consisting on a con vex set of probabilities, then Bayes Theorem is applied to each individual probability but, at the same time, it is defined a likelihood about the possible probabili ties. Then we are using at the same time Bayes The orem and likelihood functions, the first is applied to\nCombination of Upper and Lower frob abilities 67\ntransform probabilities, the second to change our be lief about what is the true probability. The following example illustrates these ideas.\nExample.- Assume as above that we have two urns U1 and U2 with red and black balls\u00b7.\nU1 99 1\u00b7ed 1 black U2 1 red 99 black\nConsider that we pick up two balls with replacement from the same urn and the events are denoted as fol lows:\n- B1 The first ball is black\n- R1 The first ball is red\n- B2 The second ball is black\n- R2 The second ball is red\nFor the color of the two balls we have an 'a priori' in formation with two extreme probabilities, one for each urn:\nR1nR2 PI 0.9801 P2 0.0001 R1 n B2 0.0099 0.0099 B1 n R2 0.0099 0.0099 B1 n B2 0.0001 0.9801\nAssume now that we observe the colour of the first ball: red. This defines the following likelihood\nR1 n R2 R1 n B2 B 1 n R2 1 1 0 B1 n B2 0\nThe combination of 'a priori' information and the like lihood is\nR1nR2 hl 0.9801 h2 0.0001 R1 n B2 0.099 0.099 B1 nR2 0 0 B1 n B2 0 0\nIf we normalize the probabilities calculating the corre sponding possibilities we get\nPJ(.JR1) P2(.JR1) R2 B2 71' 0.99 0.01 0.99 0.01 0.99 0.01\nObserve as we have transformed each probability dis tribution according to Bayes rule. But also, the com bination defines a possibility about which is the true probability (or what is equivalent: which is the true urn) . Note also that here the probabilities for the sec ond ball are the same as before the first ball is ob served. However, knowing that the first ball is red\n68 Cano, Moral, and l.6pez\ndefines a likelihood about which is the true urn, that changes our belief about the colour of the second ball. The integration of conditional probabilities and possi bilities by using Choquet integral produces the follow ing intervals:"}, {"heading": "R2 [0.9801, 0.9900]", "text": ""}, {"heading": "B2 [0.0100, 0.0199]", "text": "These intervals incorporate not only the changes on conditonal probabilities but also our chages on belief about the urns, that is, the bayesian updating and the likelihood information.\nAcknowledgments\nWe are indebted to L.l\\1. de Campos, M. Delgado and M.T. Lamata for their help in completing this work. We are also very grateful to Ph. Smets by his valuable and useful comments.\nThis research has been supported by the Commission of European Communities under Project DRUMS (Es prit B.R.A. 3085).\nReferences\nCampos L.M. de, Lamata M.T., Moral S. (1988) Log ical connectives for combining fuzzy measures. In: Methodologies for Intelligent Systems (Z.W. Ras, L. Saitta, eds.) Elsevier (New York) 11 ,18.\nCampos L.M. de, Lamata M.T., 1\\Ioral S.(1990) The concept of conditional fuzzy measure. International Journal of Intelligent Systems 5, 237-246.\nChoquet G. (1953/54) Theorie of capacities. Ann. Inst. Fourier 5, 131-292.\nDempster A.P. (1967) Upper and lower probabilities induced by a multivalued mapping. Ann. Math. Statist. 38, 325-339.\nDubois D., Prade H. (1988). Possibility Theory. An Approach to Computerized Processing of Information. Plenum Press (New York).\nFagin R., Halpern J .Y. (1990) A new approach to up dating beliefs. Research Report RJ 7222. IBM Al maden Research Center.\nMoral S., Campos L.M. de (1990) Updating uncer tain information. Proceedings 3rd. IPMU Conference, Paris 1990, 452-454.\nPearl J. (1989) Reasoning with belief functions: a crit ical assessment. Tech. Rep. R-136. University of California, Los Angeles.\nShafer G. ( 1976) A Mathematical Theory of Evidence. Princeton University Press (Princeton).\nSmets Ph. (1982) Possibilistic Inference from Statisti cal Data. Proceedings of the Second World Conference on Mathematics at the Service of Man (A. Ballester, eds.) 611-613.\nSmets Ph. ( 1978) Un Modele Mathematico-Statistique Simulant le Processus du Diagnostic Medical. Doc toral Disertation. Universite Libre de Bruxelles, Brux elles.\nSmets Ph. (1981) Medical Diagnosis: Fuzzy Sets and Degrees of Belief. Fuzzy Sets and Systems 5, 259-266.\nSugeno M. (1974) Theory of Fuzzy Integrals and its Applications. Ph. D. Thesis, Tokyo Institute of Tech nology.\nTessen B. (1989) Interval Representation of Uncer tainty in Artificial Intelligence. Ph. D. Thesis, Depar tament of Informatics, University of Bergen, Norway.\nZadeh L.A. (1978) Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets and Systems 1, 3-28."}], "references": [{"title": "Log\u00ad ical connectives for combining fuzzy measures. In: Methodologies for Intelligent Systems (Z.W. Ras, L", "author": ["de Campos L.M", "M.T. Lamata", "S. Moral"], "venue": "Elsevier (New York)", "citeRegEx": "L.M. et al\\.,? \\Q1988\\E", "shortCiteRegEx": "L.M. et al\\.", "year": 1988}, {"title": "1\\Ioral S.(1990) The concept of conditional fuzzy measure", "author": ["Campos L.M. de", "Lamata M.T"], "venue": "International Journal of Intelligent Systems", "citeRegEx": "de and M.T.,? \\Q1990\\E", "shortCiteRegEx": "de and M.T.", "year": 1990}, {"title": "Upper and lower probabilities induced by a multivalued mapping", "author": ["A.P. Dempster"], "venue": "Ann. Math. Statist", "citeRegEx": "Dempster,? \\Q1967\\E", "shortCiteRegEx": "Dempster", "year": 1967}, {"title": "Possibility Theory. An Approach to Computerized Processing of Information", "author": ["D. Dubois", "H. Prade"], "venue": null, "citeRegEx": "Dubois and Prade,? \\Q1988\\E", "shortCiteRegEx": "Dubois and Prade", "year": 1988}, {"title": "A new approach to up\u00ad dating beliefs", "author": ["Fagin R", "Halpern J .Y"], "venue": "Research Report RJ 7222. IBM Al\u00ad maden Research Center", "citeRegEx": "R. and .Y.,? \\Q1990\\E", "shortCiteRegEx": "R. and .Y.", "year": 1990}, {"title": "Updating uncer\u00ad tain information", "author": ["S. Moral", "L.M. Campos"], "venue": "Proceedings 3rd. IPMU Conference,", "citeRegEx": "Moral and Campos,? \\Q1990\\E", "shortCiteRegEx": "Moral and Campos", "year": 1990}, {"title": "Reasoning with belief functions: a crit\u00ad ical assessment", "author": ["J. Pearl"], "venue": "Tech. Rep. R-136", "citeRegEx": "Pearl,? \\Q1989\\E", "shortCiteRegEx": "Pearl", "year": 1989}, {"title": "A Mathematical Theory of Evidence", "author": ["G. Shafer"], "venue": null, "citeRegEx": "Shafer,? \\Q1976\\E", "shortCiteRegEx": "Shafer", "year": 1976}, {"title": "Possibilistic Inference from Statisti\u00ad cal Data", "author": ["Smets Ph"], "venue": "Proceedings of the Second World Conference on Mathematics at the Service of Man (A. Ballester,", "citeRegEx": "Ph.,? \\Q1982\\E", "shortCiteRegEx": "Ph.", "year": 1982}, {"title": "Un Modele Mathematico-Statistique Simulant le Processus du Diagnostic Medical", "author": ["Smets Ph"], "venue": "Doc\u00ad toral Disertation. Universite Libre de Bruxelles, Brux\u00ad elles", "citeRegEx": "Ph.,? \\Q1978\\E", "shortCiteRegEx": "Ph.", "year": 1978}, {"title": "Medical Diagnosis: Fuzzy Sets and Degrees of Belief", "author": ["Smets Ph"], "venue": "Fuzzy Sets and Systems", "citeRegEx": "Ph.,? \\Q1981\\E", "shortCiteRegEx": "Ph.", "year": 1981}, {"title": "Theory of Fuzzy Integrals and its Applications", "author": ["M. Sugeno"], "venue": "Ph. D. Thesis,", "citeRegEx": "Sugeno,? \\Q1974\\E", "shortCiteRegEx": "Sugeno", "year": 1974}, {"title": "Interval Representation of Uncer\u00ad tainty in Artificial Intelligence", "author": ["B. Tessen"], "venue": "Ph. D. Thesis, Depar\u00ad tament of Informatics,", "citeRegEx": "Tessen,? \\Q1989\\E", "shortCiteRegEx": "Tessen", "year": 1989}, {"title": "Fuzzy sets as a basis for a theory of possibility", "author": ["L.A. Zadeh"], "venue": "Fuzzy Sets and Systems", "citeRegEx": "Zadeh,? \\Q1978\\E", "shortCiteRegEx": "Zadeh", "year": 1978}], "referenceMentions": [{"referenceID": 13, "context": "However their behavior will be very similar to classical possibil\u00ad ity distributions (Dubois, Prade 1988; Zadeh 1978).", "startOffset": 85, "endOffset": 117}, {"referenceID": 2, "context": "This 'a posteriori' information is different from that resulting from applying known formulas of calculat\u00ad ing conditional information: Dempster conditioning (Dempster 1967) and upper and lower conditioning (Dempster 1967; Campos, Lamata, llforal 1990; Fa\u00ad gin, Halpern 1990).", "startOffset": 158, "endOffset": 173}, {"referenceID": 2, "context": "This 'a posteriori' information is different from that resulting from applying known formulas of calculat\u00ad ing conditional information: Dempster conditioning (Dempster 1967) and upper and lower conditioning (Dempster 1967; Campos, Lamata, llforal 1990; Fa\u00ad gin, Halpern 1990).", "startOffset": 207, "endOffset": 275}, {"referenceID": 2, "context": "The problem of applying Dempster-Shafer Theory of Evidence (Dempster 1967; Shafer 1976) to upper and lower probabilities is that there is only one way of com\u00ad bining information, the so called Dempster rule of com\u00ad bination.", "startOffset": 59, "endOffset": 87}, {"referenceID": 7, "context": "The problem of applying Dempster-Shafer Theory of Evidence (Dempster 1967; Shafer 1976) to upper and lower probabilities is that there is only one way of com\u00ad bining information, the so called Dempster rule of com\u00ad bination.", "startOffset": 59, "endOffset": 87}, {"referenceID": 2, "context": "Furthermore, the most used conditioning is Dempster conditioning (Dempster 1967; Moral, Cam\u00ad pos 1990) which is a particular case of Dempster rule.", "startOffset": 65, "endOffset": 102}, {"referenceID": 6, "context": "In general, Dempster conditioning produces very narrow intervals, if upper and lower probabilities interpretation is considered (Pearl 1989).", "startOffset": 128, "endOffset": 140}, {"referenceID": 13, "context": "The possibility measure associated with 1r 0 is defined (Zadeh 1978) as a mapping", "startOffset": 56, "endOffset": 68}, {"referenceID": 11, "context": "- h is a function h : U --+ Rt - Ha = {x E UJh(x) 2: a} - g is a monotone fuzzy measure (non necessarily additive (Sugeno 1974)).", "startOffset": 114, "endOffset": 127}], "year": 2011, "abstractText": "In this paper, we consider several types of in\u00ad formation and methods of combination asso\u00ad ciated with incomplete probabilistic systems. \\Ve discriminate between 'a priori' and evi\u00ad dential information. The former one is a de\u00ad scription of the whole population, the latest is a restriction based on observations for a particular case. Then, we proposse different combination methods for each one of them. 'We also consider conditioning as the hetero\u00ad geneous combination of 'a priori' and eviden\u00ad tial information. The evidential information is represented as a convex set of likelihood func\u00ad tions. These will have an associated possi\u00ad bility distribution with behavior according to classical Possibility Theory.", "creator": "pdftk 1.41 - www.pdftk.com"}}}