{"id": "1303.5726", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Reasoning with Mass Distributions", "abstract": "the concept of movable evidence masses that flow from supersets to subsets randomly as specified by experts represents a suitable framework for reasoning independently under uncertainty. usually the mass flow is controlled by specialization matrices. new evidence is integrated progressively into proving the frame of discernment by conditioning or revision ( dempster's rule of conditioning ), for which special specialization matrices exist. even some aspects of non - certain monotonic reasoning can be represented automatically by certain higher specialization matrices.", "histories": [["v1", "Wed, 20 Mar 2013 15:31:20 GMT  (307kb)", "http://arxiv.org/abs/1303.5726v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["rudolf kruse", "detlef nauck", "frank klawonn"], "accepted": false, "id": "1303.5726"}, "pdf": {"name": "1303.5726.pdf", "metadata": {"source": "CRF", "title": "Reasoning with Mass Distributions", "authors": ["Rudolf Kruse", "Detlef Nauck", "Frank Klawonn"], "emails": [], "sections": [{"heading": null, "text": "The concept of movable evidence masses that flow from supersets to subsets as specified by experts represents a suitable framework for reasoning under uncertainty. The mass flow is controlled by specialization matrices. New evidence is integrated\ninto the frame of discernment by conditioning or revision (Dempster's rule of conditioning), for\nwhich special specialization matrices exist. Even some aspects of non-monotonic reasoning can be\nrepresented by certain specialization matrices.\n1 INTRODUCTION\nIn this paper we present a suitable theoretical model for handling uncertainty, which is an important problem in the range of knowledge based systems. Uncertainty corre sponds to the valuation of some datum, reflecting the faith\nor doubt in the respective source. So we have to deal with statements being not just simply true or false but with a validity which is a matter of degree. This is caused by the fact that the actual state of the world is not completely\ndetermined and we have to rely on a human experts sub jective preferences among different possibilities.\nThroughout this paper we will restrict ourselves to the treatment of subjective valuations of evidence which re quires the use of belief functions measuring the credibility of information although our concept of specialization is very general and can be applied to probabilities as well as possibility measures.\nLet n, a finite nonempty set be our frame of discernment. We assume n to be a product space QM !f nj X ... X nm with m characteristiCS JC.l! E Q1, ... , JC.m! E Qm where Q, (i = 1, . .. , m) is a finite nonempty set. The partial know ledge is encoded through evidence masses attached to subsets of n. Specialization matrices quantify the flow of\nmasses, the concept we prefer to Dempster's rule of con ditioning (Shafer 1976).\nA mass distribution is considered here as the condensed representation of a (possibly unknown) random set, for other semantics see (Kruse, Schwecke and Heinsohn 1991).\nSection 2 provides an overview about mass distributions and belief functions. In section 3 we present our main concept: the flow of evidence masses given by a speciali zation matrix (Kruse and Schwecke 1990). In section 4 we consider specialization matrices which can be applied to conditioning and revision, and discuss certain aspects\nof non-monotonic reasoning.\n2 REPRESENTING KNOWLEDGE WITH MASS DISTRIBUTIONS\nBelief functions aim to model a human decision maker's subjective valuation of evidence. For this purpose we consider an inaccessible, finite probability space e of sen sors or experts and a sample space n containing the pos sible events. The sensors or experts choose subsets of n which they believe to contain the actual state of the world. This means we consider multivalued mappings\ndefined on a probability space, here called random sets (Matheron 1975). With respect to the probability distribu tion on the sensor space e one unit of ''belief' which we conceive as movable \"evidence mass\" is distributed among the elements of n, attributing a greater amount to the more likely elements (the elements chosen by the most or most reliable sensors or experts). That means a mass distribution m (basic probability assignment (Shafer 1976)) is specified, which is a mapping from 2\u00b0 to the unit interval!.\nDefinition 1: Each junction m : 2\u00b0 \ufffd [0, 1) is called a mass distribution, whenever\n(i) m(0) = 0,\n(ii) L m(A) = 1 A:Ac n\nhold.\nThe mass m(A) is understood to be the measure of \"be lief' that is committed exactly to A and corresponds to the support given to A but not to any strict subset of A. Those sets A with m(A) > 0 are called focal elements. To obtain the total measure of belief committed to some set A, we have to sum up the quantities m(B) for all B \ufffdA.\nDefinition 2: If m is a mass distribution on 2\u00b0, then the function Belm : 2\u00b0 -t [0, 1],\nBelm(A) 4 L m(B), B:Bo;:A\nis called the belief function induced by m.\nBel .. ( A) represents the degree to which the actual evidence supports A, i.e. it measures the credibility of A. We are also able to calculate the degree to which the evidence fails to refute A, i.e. the degree to which A remains plau sible:\nd -PLm(A) = 1 - Belm(A)\nWe have\n= 1 - L m(B) L m(B). a: Be. A B:An s\ufffd0\nBel .. (A) <::; Pl .. (A), and Bel,.( A) + Bel .. ( A) <::; 1\nfor all A\ufffd Q.\nTo measure the evidence mass that can freely move to any element or subset of A we use the concept of commo nality junctions. Let m be a mass distribution defined on 2\u00b0. l11e function\nQ .. (A) 4 L m(B) B:Ac. 8\nmeasures the evidence mass which is attached to supersets of A and can move to A or to any of its subsets. Obvio usly Q.,(A) = 0 indicates that there is no mass \"above\" A, i.e. A cannot receive more evidence mass from its supersets.\nTo represent total ignorance about the domain under consideration, we set m(Q) = 1 and m(A) = 0 for all A \ufffd Q and we obtain Bel.,(Q) = 1 and Bel.,( A) = 0 for all A \ufffd n. This belief function is called the vacuous belief function. On the other hand setting m({xJ) = p,, x, E n = {x1, \u2022 \u2022 \u2022 , x,} and m(A) = 0 for all non-elementary sets A leads to a Bayesian belief junction or, in terms of the probability theory, a discrete probability distribution. We can imagine \"belief' as partially movable evidence mass, where m( A) is that amount of mass which can, in the light of new information, move to every subset of A but not to\nReasoning with Mass Distributions 183\nsets with elements outside of A.\nThe concepts of conditioning and revision are based on this idea. When we obtain the information that ''the truth'' is within some set E with certainty, all elements of E become impossible. The two concepts differ in their treat ments of sets which have a nonempty intersection with E. Conditioning a mass distribution m defined on Q with respect to a set E \ufffd Q means to neglect the evidence mass which is inconsistent with the new information. All masses not attached to subsets of E are omitted and the remaining masses are normalized.\nDefmition 3: Let m be a mass distribution on 7? and E be a subset ofil with Bel.,(E) > 0. The mass distribu tion\nd \ufffd m(A) if A c E m(. I E): 2\u00b0 \ufffd [0, 1]; m(A IE) = Belm(E) - 0 otherwise is called conditional1 mass distribution.\nThe concept of revision is directly based on the idea of partially movable evidence mass. All masses attached to subsets A of Q float to the sets A n E after revision with respect to the set E. Definition 4: Let m be a mass distribution on 7? and E be a subset of n with Bel.,( E) > 0. The mass distribu tion\nd j D\u2022\ufffd\u2022A m(D)\nmiA) = Plm(E)\n0 is called revised 2 mass distribution.\nif A\ufffd 0\notherwise\nContrary to conditioning revision does not omit the evi dence mass attached to sets lying just partially in E. Revi sing m on E yields the belief function\nBelm(A U B) - Belm(B) Bel (A) = , A \ufffd Q m,\n1 - Belm(B) and the plausibility function\nPI (A) = Plm(A n B) \u2022\nA \ufffd Q. m, Plm(B)\nRemembering our idea of experts or sensors choosing\n11bis concept is also called strong conditioning (Dubois and Prade 1986a) or geometric conditioning.\n2 This concept is also know as Dempster's rule of cond itioning (Shafer 1976).\n184 Kruse, Nauck, and Klawonn\nsubsets of n the differences between the two concepts conditioning and revision can be made clear quite easily. Conditioning is a very strict treatment of experts whose valuations are inconsistent with the new information E. These experts are now considered as totally unreliable and the evidence mass distributed due to their statements has to be redistributed under the subsets A c;;; E chosen by the reliable experts.\nRevision induces a more optimistic treatment of the ex perts. The idea is that the valuations which are only parti ally inconsistent with the new information (A IZ: E but A n E * 0) are now treated as if the expert meant A n E and not A. The expert just was not able to express this situation because he had not enough information. So he is still considered to be reliable and the evidence mass atta ched to A flows completely to the intersection with E. Only those experts whose valuations are totally inconsi stent with E are treated as in the case of conditioning.\n3 THE CONCEPT OF SPECIALIZATION\nIn order to compare different frames of discermnent we introduce the notion of a refinement (Shafer 1976).\nDefinition 5: A set Q' is a refinement of n if there is a mapping fi : 2\u00b0 -t 2\u00b0' such that\n(i)\n(ii)\n(iii)\n(iv)\nfi({x}) * 0 for all X E Q,\nfi({x}) n fi({x1}) \"' 0, if X #- X1,\nU { fi({x}) I x e n } \"' n' and\nfi(A) \"' u { n({x}) I X E A }.\nfi is called a refmement mapping. If such a mapping exists, the sets n and Q' are compatible, where the refi ned space Q' is able to carry more information than its quotient space Q. In order to decide for each ro E Q whether information concerning some set A' c;;; Q' may be of relevance for the valuation of ro or not we define the mapping II.\nDefinition 6: Let Q' be a refinement of n where fi : 2\u00b0 -t 2\u00b0' is the respective refinement mapping. The mapping\nII : 2a -t 2\u00b0, II(A1 4 {roE n I tr({ro}) n A'* d\nis called the outer reduction induced by fi.\nII(A ') contains those ro e n which have one or more\nelements ro' e fi ( { ro}) within A'. Note that II essentially is a projection that attaches to each element ro' e n that element ro with ro' e fi ( { ro }). The projection of a mass distribution m' defined on 2\u00b0' can be obtained by\nIT(m') : 2\u00b0 -t [0, 1); II(m')(A) 4 L m1(A '). A'\ufffd 0': n(A\ufffd\u00b7A\nIf there is a mass distribution m' defined on 2\u00b0' and a projection II(m') of m' on\ufffd. then m' is a refinement of IT(m'). The formulation of a mass distribution m on n in terms of the refmed space n\u00b7 is defined by\nfi(m) : 2a -t [0, 1);\nfi(m)(A') g,l m(A), if A1 \"'. IT(A), 0 otherwzse and is denoted as the vacuous extension of m. From the definition it is obvious, that each vacuous extension of a mass distribution is its refinement. In contrast to the pro jection which generally means a loss of information, the vacuous extension preserves the information borne by the original mass distribution.\nThe main issue of this chapter is to define the concept of specialization. The intuitive idea of a specialization is the projection of a revision.\nDefinition 7: Let s, t be two mass distributions defined on 2\u00b0. We call s a specialization of t (s 1:: t), if and only if there are two mass distributions s' and t' on a refine ment Q' of n where s' and t' are refinements of s and t, respectively, and if there is an event E' c;;; Q' such that s'(B') = t\ufffd.(B') holds for each B' c;;; Q'.\nThis definition tells us that we will get all specializations of a given mass distribution on n by considering all pos sible revisions in a refined space n'. Relating now the concept of specialization with Dempster's rule of combi nation we can see, that specialization is bound to the idea of updating and not to aggregation. Dempster's rule com bines two mass distributions (basic probability assign ments) which are defined on the same sample space but based on different bodies of evidence. This is a concept of aggregating different expert views.\nThe change from a mass distribution m to a specialization of m is a different concept, and it is due to an updating of the refinement of m in a refinement of the sample space n. We use revision as the updating rule which causes a change of data in the refined space. Those observations A of the experts which are not completely covered by the new evidence E are changed to become A n E instead without loosing any evidence mass.\nIn addition to the definition above the following theorem gives two equivalent characterizations of the specialization relationship. The first one allows to check easily whether s c: t is valid or not. The second one reflects our intuitive idea of floating evidence masses describing the flow of the mass t(A) onto the subsets of A.\nTheorem 1: Let s, t be two mass distributions on n. The following three statements are equivalent:\n(i) s 1:: t,\n(ii) \\;fA \\::: n : ( Q,(A) \ufffd 0 => Q,(A) \ufffd 0 ). (iii) For every A \\::: n there are functions\nhA : 2\u00b0 \ufffd [0, 1] such that\na) E hiB) \ufffd t(A), B:Bc. 0 b) hA(B) -F 0 => B \\::: A, for all B \\::: n, and E hiB) c) s(B) \ufffd A'A\" 0 for all 0 -FB \\::: n. - E hi0)\nA:Ao;; 0\nhi B) specifies that amount of \"belief' comitted to A that in the course of refining m to m' floats to the set B. Con dition (iii.a) of Theorem 1 assures that no evidence mass is lost, condition (iii. b) requires that the masses flow only to subsets. Those masses floating to the empty set repre sent partial contradictions, thus have to be neglected and the remaining portions have to be normalized as pointed out in condition (iii.c).\nThe normalization in condition (iii.c) is due to our treat ment of experts whose observations are totally inconsi stent with the new evidence (see sect. 2). They are now considered to be unreliable and so the evidence mass bound to their observations has to be redistributed under the consistent observations. Note that we also use a closed world assumption. Smets (Smets 1988) considers an open world assumption and allows the empty set to bear evi dence mass. In this case there is no normalization of the remaining masses because the evidence mass on the emp ty set is supposed to indicate the belief that the actual state of the world cannot be represented in the chosen frame of discernment. Our perception of the empty set is a different one. The evidence mass that flows to the emp ty set indicates from our point of view the inconsistency of expert observations at the beginning of the updating process and is not characterizing the current situation. So a normalization has to be made because we don't want to weaken the belief in the consistent observations. Using an open world assumption means that an expert cannot be wrong in spite of inconsistencies due to new information. From our point of view inconsistency arises because of\nReasoning with Mass Distributions 185\nerrors made by some of the experts.\nA similar concept to the specialization relation is the idea of a containment of \"bodies of evidence\" introduced in (Yager 1986). A body of evidence is a pair (F,m), where m is a mass distribution defined on n and F contains the focal elements of m. A definition of \"strong inclusion\" can be found in (Dubois and Prade 1986b):\n(F,m) -< (F',m') if and only if\n(I) \\;f B E F, 3 A' E F1, B \\::: A1 (ii) \\;f A1 E F1, 3 B E F, B \\::: A1 (iii) There exist W8A, E [0, 1 ], for all B, A' such that\nWBA' > 0 => B \\:::A', E WBA' \ufffd 1, and A'.B \\;f B E F, m(B) \ufffd E WBA' , A':B C. A1 \\;f A' E F', m'(A ') \ufffd E WBA' . B:Bc.A'\nSpecialization is more general than strong inclusion. We have (F,m) -< (F', m') => m c: m' but not vice versa. The W8A. are identical to the values hA.(B), but there is no normalization. From considering the definition above and our idea of floating evidence masses, it is obvious that in the case of strong inclusion there is no mass flow to the empty set and that no mass is lost (LWBA' = 1), so a nor malization is not necessary.\n4 SPECIALIZATION MATRICES\nIn order to compute a specialization of a mass distribution m we characterize m as a vector and the respective specia lization-relation by a matrix V : 2\u00b0 x 2\u00b0 \ufffd [0, 1] and obtain the more specific mass distribution m' by \"multi plying\" the vector m with the matrix V. In the following we use square brackets to indicate that we conceive the respective functions as vectors or matrices.\nDefinition 8: Let !l be the frame of discernment. (i) A matrix V: 2\u00b0 x 2\u00b0 \ufffd [0, I] is called a speciali zation matrix, if and only if\n(a) E V[A,B] \ufffd 1 for all A \\::: n B:Bc. 0\n(b) B \u00a2 A => V[A,B] \ufffd 0. (ii) Let V be a specialization matrix and let m be a mass distribution on 2\u00b0. If\nc g, E E m[A] \u2022 V[A,B] > 0 A:A:r;; 0 8:8\ufffd0\nthen the mass distribution m 0 V is defined by\n186 Kruse, Nauck, and Klawonn\nd \ufffd2. \u00b7 L m[A] \u00b7 V[A,B] if B \"- 0 (m 0 V)[B] \"' ' A'A\" o for all B,;;; Q. 0 otherwise\nIn contrast to the mass flow functions hA, A ,;;; Q, specia lization matrices do not assign absolute portions but rela tive amounts of mass.\nTheorem 2: Let m and m' be two mass distributions defined on2\u00b0. We have m' \ufffd;;: m <=? 3V: m' = m 0 V, where V is a specialization matrix.\nThe processes of conditioning and revision, i.e. the change from a mass distribution m to the conditional mass dis tribution m( \u00b7I E) or to the revised mass distribution mE respectively, are special cases of specialization and can therefore be described by special specialization matrices.\nRecall that conditioning with respect to the set E ,;;; Q means that those masses bound to sets A ,;;; E remain where they are, while those bound to sets A a; E have to be neglected.\nDefinition 9: Let Q be the frame of discernment and let E ,;;; Q be a nrn-empty set. The conditional matrix C(E) : 2\u00b0 x 2\u00b0 \ufffd [0, I] is defined by\nll ifA\ufffdtE and B=0 C(E )[A,B] g l if A ,;;; E and B \"' A\n0 otherwise We obtain m( \u00b7 IE) = m 0 C(E). Revision with respect to the set E means that the masses attached to sets A \"-0 float to A n E. Masses attached to sets with A n E = 0 have to be neglected since they represent (partial) contradictions of the information E and the mass distribution m.\nDefinition 10: Let Q be the frame of discernment and let E ,;;; Q be a non-empty set. The revision matrix R(E) : 2\u00b0 x 2\u00b0 \ufffd [0, l] is defined by R(E )[A,B] g { ol if B \"'An E otherwise We obtain mE = m 0 R(E).\nThe use of specialization matrices leads to a new inter esting concept. Some specialization matrix V represents a piece of \"structural knowledge\". Multiplying a mass distribution m with V means to split the evidence masses in the light of knowledge encoded by V. A rather strict requirement is that the \"application\" of V to a more specific mass distribution m' should yield a more specific result.\nDefinition 11: Let V: 2\u00b0 x 2\u00b0 \ufffd [0, l] be a speciali zation matrix. V is called monotonic, if and only if s\ufffd;;t \ufffd s0V\ufffd;;t0V holds for all mass distribution s, t : 2\u00b0 \ufffd [0, 1].\nThe next theorem provides a simple possibility to check whether a given specialization matrix is monotonic or not. It relies on a test, if there is no such set A whose mass flow is completely \"outrun\" by one of its supersets mass flow.\nTheorem 3: Let V: 2\u00b0 x 2\u00b0 \ufffd [0, l] be a speciali zation matrix. V is monotonic, if and only if for all sets A, B ,;;; Q with V[A,B] > 0, and for all C ::2 A there is a set D ::2 B with V[C,D] > 0.\nTheorem 4: Let s,t be two mass distributions defined on 2\u00b0 and s \ufffd;;: t. Then there is always a specialization matrix V : 2\u00b0 x 2\u00b0 \ufffd [0, l] and V is monotonic, such that s = t 0 v. We want to show in the sequel that also aspects of non monotonic reasoning can be handled with specialization matrices. From Theorem 3 it is clear that a specialization matrix V is non-monotonic, if there exist sets B \ufffd A \ufffd C such that there is a mass flow from A to B and no mass flow from C to supersets of B.\nFirst we want to compare non-monotonic specialization matrices with Yager's non-monotonic compatibility rela tions (Yager 1988). Yager defines a (type ID compatibility relation on two sets X and Y as a relation R on 2x\u00b7 x Y such that for each T E 2x\u00b7 there exists at least one y E Y such that (T,y) E R, where 2X' is the power set of X minus the empty set. R(T,y) implies that (x,y), for all x E T, are possible states of the world.\nLet W = {yl R(T,y)} be the subset of Y that contains the y E Y which are related to any x E T. W is called the \"associated set\" in Y of T, denoted T \ufffd W. A compatibi lity relation R is called \"irregular\" if there exists a triple TI \ufffd WI, T2 \ufffd W2 and T3 \ufffd W3 with T3 = TI u T2 such that W3 is strictly contained in WI u W2, W3 c WI u W2. Yager has proven that every irregular (type II) compati bility relation is non-montonic. That means if we have two mass distributions s, t and we have s -< t (strong inclusion) this does not imply s o R -< t o R.\nBecause the concept of specialization matrices is more general than compatibility relations, a non-monotonic compatibility relation R can be easily expressed with a non-monotonic specialization matrix. Let S be a subset of\nX x Y, let D5 \"' {x I 3 y, (x,y) E S, R (x, y) } , and let W 0 be the associated set of D5. A (type II) compatibility ' relation R can be expressed with a specialization matrix VR: 2XxY X 2XxY with\nv [S S'] = l l , if S' = s n {Ds X wD,l R ' 0, otherwise If the relation R is non-monotonic, the same is true for the specialization matrix V R- If we express any (type II) compatibility relation R with a specialization matrix V R> and VR is non-monotonic, the same holds for R.\nNow let us take a look at the well known example of the bird Tweety who is not able to fly because he is a pen guin. Let n = nl X n2 be our frame of discememt where 01 = {birds, fish} and 02 = (fly, not fly}. Now the rule \"All birds fly\" can be expressed by a specialization matrix V with\nV[A,B] g, l l if B = A - {(birds, not fly)}\n0 otherwise\nThe rule \"Penguins don't fly\" can only be represented in a refined space, e.g. Q' = 01' x 02, where 01' ={eagles, penguins, fish}. In our refined space the two (partially contradicting) rules \"All birds fly\" and \"Penguins don't fly\" are expressed by the following specialization matrix v\u00b7.\nV'[A,B] 4\nif A ;:;2 !eagles, penguins) x !not fly) := H and B = A - H, if (penguins, fly) E A and B = A - {(penguinsJly)},\nl if A :tl H and (penguins, fly) \ufffd A, 0 otherwise\nThe two rules force the mass attached to the set C = {eagles, penguins} x (fly, not fly} to float to the set D = {eagles, penguins} x (fly} and the masses attached to A = {penguins} x (fly, not fly} to B = (penguin} x {not fly}. We have C ;:;2 A but D :tl B. That means the speciali zation matrix V' is non-monotonic.\n5 CONCLUSIONS\nWith the calculus of mass distributions we presented a suitable theoretical tool for reasoning under uncertainty. We showed that the flow of evidence masses can be con veniently handled by specialization matrices. For the concepts of conditioning and revision (Dempster's rule of conditioning) there exist special specialization matrices. We also demonstrated that certain aspects of non-monoto nic reasoning, especially partially contradicting statements can be expressed by non-monotonic specialization matri ces. In cooperation with Domier GmbH the method of\nReasoning with Mass Distributions 187\nreasoning with mass distributions was implemented on a TI-Explorer under KEE.\nReferences\nD. Dubois, H. Prade (l986a). 'On the Unicity of Demp ster's Rule of Combination'. Int. J. Intelligent Systems, 1, 133-142. D. Dubois, H. Prade (l986b). 'A Set Theoretic View of Belief Functions'. Int. J. General Systems, 12, 193-226. F. Klawonn, R. Kruse, E. Schwecke (1990). 'Belief Func tions and Non-monotonic Reasoning'. Proc. of the 1st DRUMS Workshop on Non-monotonic Reasoning, Mar seille, February 1990. R. Kruse, E. Schwecke (1990). 'Specialization - A New Concept for Uncertainty Handling with Belief Functions', to appear in: Int. J. General Systems. R. Kruse, E. Schwecke, J. Heinsohn (1991). Uncertainty Handling in Knowledge Based Systems: Numerical Methods, Series Artificial Intelligence, Springer, Heidel berg. G. Matheron (1975). Random Sets and Integral Geometry, Wiley, New York. G. Shafer (1976). A Mathematical Theory of Evidence, Princeton University Press, Princeton.\nP. Smets (1988). 'Belief Functions'. In P. Smets, E.H. Mamdani, D. Dubois, H. Prade, Non-Standard Logics for Automated Reasoning, Academic Press, London, 253-286. R. R. Yager (1986). 'The entaihnent principle for Demp ster-Shafer granules'. Int. J. Intelligent Systems, 1, 247-262. R. R. Yager (1988). 'Non-monotonic Compatibility Rela tions in the Theory of Evidence'. Int. J. Man-Machine Studies, 29, 517-537."}], "references": [{"title": "Belief Func\u00ad tions and Non-monotonic Reasoning", "author": ["F. Klawonn", "R. Kruse", "E. Schwecke"], "venue": "Proc. of the 1st DRUMS Workshop on Non-monotonic Reasoning, Mar\u00ad seille, February", "citeRegEx": "Klawonn et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Klawonn et al\\.", "year": 1990}, {"title": "Belief Functions", "author": ["P. Smets"], "venue": "Int. J. Intelligent Systems,", "citeRegEx": "Smets,? \\Q1988\\E", "shortCiteRegEx": "Smets", "year": 1988}, {"title": "Non-monotonic Compatibility Rela\u00ad tions in the Theory of Evidence", "author": ["R.R. Yager"], "venue": "Int. J. Man-Machine Studies,", "citeRegEx": "Yager,? \\Q1988\\E", "shortCiteRegEx": "Yager", "year": 1988}], "referenceMentions": [{"referenceID": 1, "context": "Smets (Smets 1988) considers an open world assumption and allows the empty set to bear evi\u00ad dence mass.", "startOffset": 6, "endOffset": 18}, {"referenceID": 2, "context": "First we want to compare non-monotonic specialization matrices with Yager's non-monotonic compatibility rela\u00ad tions (Yager 1988).", "startOffset": 116, "endOffset": 128}], "year": 2011, "abstractText": "The concept of movable evidence masses that flow from supersets to subsets as specified by experts represents a suitable framework for reasoning under uncertainty. The mass flow is controlled by specialization matrices. New evidence is integrated into the frame of discernment by conditioning or revision (Dempster's rule of conditioning), for which special specialization matrices exist. Even some aspects of non-monotonic reasoning can be represented by certain specialization matrices.", "creator": "pdftk 1.41 - www.pdftk.com"}}}