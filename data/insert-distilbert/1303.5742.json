{"id": "1303.5742", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Deliberation and its Role in the Formation of Intentions", "abstract": "deliberation plays an important role in the design of rational intelligent agents embedded in the real - world. in particular, deliberation leads to the formation sense of intentions, about i. w e., plans of action that the agent is committed to achieving. in this summary paper, we present a branching time possible - worlds model for representing and reasoning about, beliefs, purchasing goals, current intentions, time, actions, probabilities, and payoffs. we compare this possible - worlds approach with the more traditional decision tree representation and provide a transformation map from decision trees to possible worlds. finally, we illustrate how an agent scientist can collectively perform deliberation using a decision - tree representation and finally then use internally a possible - worlds model calculated to change form and reason about his intentions.", "histories": [["v1", "Wed, 20 Mar 2013 15:32:42 GMT  (362kb)", "http://arxiv.org/abs/1303.5742v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["anand s rao", "michael p georgeff"], "accepted": false, "id": "1303.5742"}, "pdf": {"name": "1303.5742.pdf", "metadata": {"source": "CRF", "title": "Deliberation and its Role in the Formation of Intentions*", "authors": ["Anand S. Rao", "Michael P. Georgeff"], "emails": ["anand@aaii.oz.au", "georgeff@aaii.oz.au"], "sections": [{"heading": null, "text": "1 INTRODUCTION\nThe design of rational agents, situated in a dynamic world and operating effectively under real-time con straints and resource limitations, has been of great interest to researchers in philosophy, artificial intel ligence, and computer science [1, 2, 7]. Such ratio nal agents have to balance the time taken thinking against the time needed for acting. In particular, they must balance the frequency of reassessment of options against continuing commitment to previously chosen plans.\nClassical planning addresses only one aspect of the above problem; namely, means-end reasoning. Means end reasoning involves finding a sequence of actions that satisfy a certain end goal (or goals). However,\n'This research was in part supported by a Generic In dustry Research and Development Gmnt from the Depart ment of Industry, Technology and Commerce, Australia and in part by the Australian Civil Aviation Authority.\nsimplifying assumptions are made about the capabili ties of the reasoning agents and the worlds they occupy that limit the use of these techniques to essentially static domains.\nClassical decision theory, on the other hand, addresses the problem of weighing alternative courses of action and choosing the best plan of action according to some well-defined criteria, such as maximizing expected util ity. However, this theory presupposes an ideal agent who can consider and weigh all possible alternative courses of action before making a decision. In real sit uations, such an assumption is rarely valid-not only does the world undergo continuous change, even as the agent is deliberating, but the agent may not be capable of enumerating all the alternatives.\nWhat is required for the design of rational agents is a combination of symbolic means-end reasoning and numeric decision-theoretic analysis that takes into account the resource-boundedness of rational agents [6]. One such design is provided by the belief-desire intention (BDI) architecture [2]. This architecture gives primary importance to the attitude of intentions. While most philosophical accounts of rational agency treat intentions as being reducible to beliefs and de sires, Bratman [1] argues convincingly that intentions, especially future-directed intentions, play a significant and distinct role in resource-bounded reasoning.\nBratman treats intentions as plans of action that the agent is committed to achieving. Prior intentions con strain the search for possible means for achieving the current intention and thus focus the means-end reason ing process. The notion of commitment, which lends a certain sense of stability to means-end reasoning, is balanced against the notion of reconsideration of inten tions, which lends a certain sense of reactivity. This rational balance between commitment and reconsider ation is essential for effective means-end reasoning in dynamic domains.1\n1 Some interesting experimental work has recently been done in this area [11].\nDeliberation and its Role in the Formation of Intentions 301\nIntentions play two important roles in resource bounded decision-theoretic analysis or deliberation [1). First, prior intentions pose problems for further de liberation, i.e., prior intentions produce the decision problems that the agent needs to consider. Second, prior intentions constrain the deliberation process be cause they rule out options that conflict with existing intentions. Under this view, the deliberation process is a continuous resource-bounded activity rather than a one-off exhaustive decision-theoretic analysis.\nSo far, we have discussed the role of intentions in means-ends reasoning and deliberation. However, we have not discussed how the agent arrives at his in tentions. Prior intentions, means-ends reasoning, and deliberation are all involved in the formation of in tentions. By means-ends reasoning, a prior intention towards an end results in the agent enumerating all the alternative or means of achieving this end; the agent by deliberating on all these alternatives then chooses the best one and commits to it by forming an intention.\nWe have previously provided (14, 15) a logical frame work that describes the role of intentions in means-end reasoning. In this paper, we illustrate how the process of deliberation can lead to the formation of intentions.\n2 O VERVIEW\nBDI-architectures are formalized by defining notions such as beliefs, goals, intentions, actions, and the inter-relationships between them. We have previ ously shown how this can be accomplished using a branching-time possible-worlds logic (15).\nBriefly, the structure of our logic is as follows: Each world is a temporal structure with a branching time fu ture and a single past called a time tree (4). A particu lar time point in a particular world is called a situation. Event types transform one time point into another. For each situation we associate a set of belief-accessible, goal-accessible, and intention-accessible worlds; intu itively, those worlds that the agent believes to be pos sible, desires to bring about, and commits to achiev ing, respectively. Multiple possible worlds result from the agent's lack of knowledge about the state of the world. But within each of these possible worlds, the branching future represents the choice of actions avail able to the agent. Moving from belief to goal to inten tion worlds amounts to successively pruning the paths of the time tree; intuitively, to making increasingly selective choices about one's future actions. This is captured semantically by requiring that for each belief accessible world there exists a sub-world which is goal accessible and, in turn, for each goal-accessible world there exists a sub-world which is intention-accessible (see Figure 1).\nIn this paper, we extend the expressive power of the above logic to model the process of deliberation by\nintroducing subjective probabilities and subjective pay offs. For the former, we adopt the formalism of Fa gin and Halpern (5) and extend it to a branching time model. For the latter, we introduce a payoff function that associates numeric values (or payoffs) with certain paths in a time tree. Intuitively, an agent at each situation has a probability distribution on his belief-accessible worlds. He then chooses sub worlds of these that he considers are worth pursu ing and associates a payoff value with each path in these sub-worlds. These sub-worlds are considered to be the agent's goal-accessible worlds. By making use of the probability distribution on his belief-accessible worlds and the payoff distribution on the paths in his goal-accessible worlds, the agent determines the best plan(s) of action for different scenarios. This process will be called Possible- Worlds(P W) deliberation. The result of PW-deliberation is a set of sub-worlds of the goal-accessible worlds; namely, the ones that the agent considers best. These sub-worlds are taken to be the intention-accessible worlds that the agent commits to achieving.\nIn contrast to this approach, decision theory represents the problem as a decision tree (or, equivalently, as a payoff matrix or influence diagram). A decision tree consists of three types of nodes: (a) decision nodes, which represent the choice of actions; (b) chance nodes, which represent the state of uncertainty in the world; and (c) terminal nodes, which represent the value of outcomes. Based on the category of decision making - namely, certainty, risk or uncertainty - a particular decision rule is adopted for selecting the best plan(s) of action. We shall refer to this process as decision-tree (DT) deliberation.\nThe main thrust of this paper is to show how decision tree deliberation can be utilized within a framework that is suited to resource-bounded reasoning in dy namic domains. We first describe the possible-worlds model and the decision tree representation formally. We then provide a transformation from decision trees to the possible-worlds model. From the possible worlds viewpoint, this provides a concrete method for obtaining the probability and payoff distribution on the worlds. From a decision theory viewpoint, the transformation facilitates symbolic manipulation of decision-theoretic entities. Finally, we describe clas sical decision-tree deliberation and show how it can be used to determine the formation of intentions.\n3 POSSIBLE WORLDS MODEL\nIn our earlier work (15) we extended the propositional branching-time logic CTL \u2022 (4) to a possible-worlds framework and introduced modal operators for beliefs, goals, and intentions. In this section, we enhance this logic by introducing operators for probability (similar to that of Fagin and Halpern (5]) and payoffs.\n302 Rao and George\u00a3\u00a3\n3.1 SYNTAX AND SEMANTICS\nSimilar to CTL \u2022, we have two types of formulas in our logic: state formulas (which are true in a specific world at a particular time point) and path formulas (which are true along a specific path). A state formula is defined as follows: (a) any propositional formula is a state formula; (b) if <h, . . . , c/Jk are state formulas, 1/J1, . .. , 1/Jk are path formulas, and 01, ... , Ok, a are real numbers, then 01PROB(\u00a2I) + . . . + OkPROB(</>k) \ufffd a and 01 PAYOFF( 1/;1) + . . . + Ok PAYOFF( 1/Jk) \ufffd a are also state formulas; (c) if \u00a21 and <1>2 are state formulas, and 1/J is a path formula, then -,4>1, \u00a21 V \u00a22, BEL(\u00a2!), GOAL(\u00a2!), lNTEND(</>I) and OPTIONAL(l/;) are state formulas. A path formula can be defined as follows: (a) any state formula is a path formula; (b) if e is an event type then done( e) is a path formula; (c) ifl/;1 and 1/;2 are path formulas, then ..,1/;1, 1/J1 V 1/;2, and Ol/;1, are path formulas. Event types include primitive event types, e1;e2, and?\u00a2.\nWe now define formally the notion of an interpretation in our language.\nDefinition 1 : An interpretation M = <W, E, T, -<, B, 9, I, PA, OA, <1>>. W is a set of worlds, E is a set of primitive event types, T is a set of time points, -< a binary relation on time points, 2 and <I> is a truth assignment of primitive propositions for any given world and time point. A situation is a world, say w, at a particular time point, say t, and is denoted by w,. The relations, B, 9, and I map the agent's current situation to her belief, goal, and intention-accessible worlds, respectively. More formally, B \ufffd W x T x W and similarly for 9 and I. PA is a probability assignment function that assigns to each time point t and world w a probability function 11r . Each 11r is a discrete probability function on the set of worlds W. OA is a payoff assignment function that assigns to each time point t and world w a payoff function p'f. Each p'f is a partial mapping from paths to real-valued numbers.\nDefinition 2 : Each world w of W is a tuple <Tw, Aw, Ow>, where Tw \ufffd T, Aw \ufffd Tw X Tw , and Ow: Tw X Tw >--+ E. Intuitively, Ow is an arc function that is a partial mapping from time points to an event and signifies the occurrence of an event. Also, Aw obeys the ordering of -(. Such worlds are called time trees. A fullpath in a world w is an infinite sequence of time points (to, tJ, ... ) such that Vi (t;, ti+l) E Aw. We use the notation ( w,0, w,, ... ) to make the world of a particular fullpath explicit.\nThe semantics of the language with interpretation M is as follows:\n2We require that the binary relation be total, transitive and backward-linear to enforce a single past and branching future.\nM, Wt0 F PROB(\u00a2) \ufffda iff 11:';. ( { w' E 8:';, I M, w' to F 4>}) \ufffd a M, Wt0 F PAYOFF(l/;) \ufffda iff \\fw' E 9;\" and Vx; such that M, x; f: 1/;, where x; is a fullpath (w',0, w',.,, . . . ), it is the case that pf. ( x;) \ufffd a M, Wt0 F OPTIONAL(l/;) iff there exists a full path in w, ( w,0, w,, , . . . ) such that M, (w,0, w,, . . . ) F 1/J M, Wto F R(\u00a2) iff\\/ w' E nro' we have M, w',o F </>. M, (w,0, w,, .. . ) F </>iff M, Wt0 F </>. M, (w,0, Wt, .. . ) F 0\u00a2 iff 3k, k>O such that M, (w,., . . . ) f: 4> M, (w,, . . . ) f: done(e) iff there exists to such that e E Ow(to, ti) M, (w,,, . . . ) f: done(e1;e2) iff\nthere exists to such that e2 E Ow(to, t1) and M, (w,0, \u2022 \u2022 \u2022 ) f: done(ei)\nM, (w,, . . . ) f: done(?\u00a2) iff M, w,, f: </>.\nR and 'R indicate the modal operators and relations, respectively, of belief, goal, and intention. We use the abbreviation nr to denote all the worlds 'R-accessible from watt.\nThe semantics of temporal and modal operators is rel atively straightforward. The probability of a formula 4> is greater than or equal to a if and only if the proba bility distribution of all the belief-accessible worlds in which 4> is true is greater than a. The payoff of a for mula 1/J is greater than or equal to a if and only if the payoff function assigns a value greater than or equal to a to all paths where 1/J is true in all goal-accessible worlds.\nINEVITABLE(\u00a2) is defined as ..,OPTIONAL(..,\u00a2); D\u00a2 as ..,o..,\u00a2. Additionally, the conditional probability PROB(\u00a21 14>2) \ufffda can be represented as PROB(</>1 1\\ <1>2) \ufffd a.PROB(\u00a22) [8].\nWe shall illustrate the belief- and goal-accessible worlds of an agent using a simple example. Phil, who is currently in the House of Representatives, believes that he can stand for the House of representatives (Rep), switch to the Senate and stand for a Senate seat (Sen), or retire from politics (Ret) [10]. He does not consider the option of retiring seriously and is sure to retain his House seat. He has to make a decision re garding conducting or not conducting an opinion poll, based upon which he has to decide to stand for the House or the Senate. The results of the poll would be either a majority approving his switch to the Senate (yes) or a majority disapproving of his switch (no).\nConsider the current situation to be w,. The four belief-accessible worlds of w,, shown in Figure 1, corre spond to Phil winning or losing the Senate seat based on the majority answering yes or no in the poll. The probabilities of these worlds are shown in the top right hand corner of each world. The propositions win, loss, yes, and no are true at the situations shown.\nDeliberation and its Role in the Formation of Intentions 303\nBelief Worlds:\nTO\n0.24\nwin\nwin ..........................................................\nGoal Worlds: 200 I 300 l \ufffdn\ufffd 2001 300 l ................................................... \ufffdi-\ufffd_1\n200 ! 100 1 lo\ufffd 200 i 100 i los\ufffd\nTO\n200\n300 wi\n200\n300 win\nlos\nloss\n200\n100 loss\n200\n100 loss\nSome of the formulas that are satisfiable at w1 are BEL(OPTIONAL(<>done(Sen))), i.e., Phil believes that he has the option of eventually standing for the Sen ate, and PROB(OPTIONAL(<>yes)) = 0.42, i.e., the probability of eventually achieving a yes response is 0.42.\nThe goal worlds are also shown in Figure 1 (for clarity, we have omitted the time points, which are the same as in the belief worlds). The values at the end of the paths (100, 200, and 300) signify the value of losing a Sen ate seat, winning a House seat, and winning a Senate seat. This can be expressed as PAYOFF(<>(done(Sen) 1\\ loss))= 100, PAYOFF(<>(done(Sen) 1\\ win))= 300, etc. Other formulas can state other properties of the agent's goals. For example, the goal of the agent to re tain his option to eventually stand for a Senate seat is expressed as GOAL(OPTIONAL(<>done(Sen))). Note that the option of retiring from politics exists only in belief worlds, not in goal worlds, i.e., Phil believes that retiring is an option, but does not have any goal to wards retiring.\n3.2 SEMANTIC CONDITIONS\nIn this section, we give an informal description of some of the semantic conditions that can be imposed on our possible worlds model. Some of these conditions re quire the definition of a sub-world. We define a world to be a sub-world of another (denoted by \ufffd) if and only if the time points in one are a subset of the other, they\nshare the same history, and everything else is identical. The formal definition of sub-worlds and the axioms corresponding to the following semantic conditions are given elsewhere [13].\nIf the belief-accessible worlds represent chance, then no level of introspection can change the chance. Thus we require that all belief-accessible worlds have identi cal probability distributions (Semantic Condition C1). We also require that the probability distribution over belief-accessible worlds add up to one (C2). From C2 we also have that beliefs about inevitable facts have probability one.\nWe introduce a constraint on our belief, goal, and intention-accessible worlds, called strong realism [15]. Strong realism requires that for every belief-accessible world there exists a sub-world which is a goal accessible world, and for every goal-accessible world there exists a sub-world which is an intention accessible world (C3). The same restriction can be applied in the reverse direction also (C4). These two conditions essentially ensure that, if the agent intends an option, he has the goal towards that option and also believes in that option. These semantic condi tions are very strong and have a significant impact on the inter-relationships between beliefs, goals, and intentions. We show elsewhere [14, 15] how these con ditions can be relaxed to solve some of the problems associated with possible-world representations of be liefs, goals, and intentions [3].\n304 Rao and George\u00a3\u00a3\nMore formally, the semantic conditions Cl to C4 can be stated as follows:\n(Cl) Vw' E Bf', pf\" = p( \u00b7 (C2) pf'(Bl\") = 1. (C3) Vw' E Bw 3w\" E gw such that w\" C: w' and t t - Vw' E 9f' 3w\" E Bf' such that w\" !:;;; ul. (C4) Vw' E gw 3w\" E zw such that w\" C: w' and t t - Vw' E I/\" 3w\" E 9/\" such that w\" !:;;; w'. In the remainder of this paper, we shall use this possible-worlds BDI model as a basis for deliberation.\n4 DECISION TREES AND GOAL\nWORLDS\nIn this section, we give a formal description of a deci sion tree and show how one can transform a decision tree into a set of goal-accessible worlds. Intuitively, both decision trees and goal-accessible worlds capture the desirable ends or outcomes of the decision prob lem, the different alternatives or choices available to the agent to achieve those ends, and the chance events controlled by nature. Note that this intuitive map ping is possible only because we have chosen to rep resent each possible world as a branching-time struc ture, rather than the more traditional model where each possible world is a linear-time structure [3]. Al though one may be able to define a transformation from decision trees to linear-time models, we believe that such a mapping would be less intuitive than the one illustrated here.\nThe decision tree for our running example is given on the left hand side of Figure 3. Decision nodes are de noted by boxes and chance nodes by circles. The for mal definition of a decision tree is as follows:\nDefinition 3 : A decision tree DT = <N, \u00a3, S, PS, P, U, <I>, lji, E> . ./11, is the union of all decision nodes V, all chance nodes C, and all terminal nodes T. <I> is the set of all propositional formulas, IJi is the set of all probabilistic state formulas (which includes the condi tional probability operator in addition to the standard logical operators), and E is the set of all primitive event types. \u00a3 <; V x ./11 x E is an event relation. S <; C x (V U T) x <I> is a chance relation. PS <; C x (V U T) x IJi is a probabilistic state relation. P: IJi ...... \ufffd is a probability function that maps probabilistic states to real numbers. U: T --> \ufffdis the payoff function that assigns to a terminal node a real number.\nNow we consider the transformation from decision trees to possible worlds. Given a decision tree, we start from the root node and traverse each arc. For each unique state labeled on an arc emanating from a chance node,3 we create a new decision tree that\n3Note that the decision tree is split with respect to the\nis identical to the original tree except that (a) the chance node is removed and (b) the arc incident on the chance node is connected to the successor of the chance node. This process is carried out recursively until there are no chance nodes left. Each of the deci sion trees so obtained consists of only decision nodes and terminal nodes. Each one of these decision trees is then transformed into a possible world structure by appropriately renaming the relations. The payoff func tion is assigned to paths in a straightforward way, thus yielding a set of goal-accessible worlds.\nWe obtain the probability distribution over the corre sponding belief worlds by associating with each de cision tree that is created a value o, which will fi nally correspond to the probability of a goal world. This probability is essentially the weighted product of all the chance nodes that a particular world repre sents. This probability distribution is finally passed back onto the corresponding belief-accessible worlds.\nThe transformation is performed by two functions, create and remove, which are defined in Figure 2. We have assumed in the function remove that the chance node is connected by an arc from a decision node. This is true in all cases except when the chance node is the root node of the decision tree. We have also assumed that the chance states are named uniquely.\nThe create function, when called with a given decision tree, its root node, and a probability value of one, will result in a set of decision trees with appropriate prob abilities. The final transformation from these multiple decision trees with no chance nodes to possible worlds is trivial and is given elsewhere [13]. Figure 3 gives the transformation for the running example.\nThe possible worlds so formed are goal-accessible worlds. The probabilities associated with these worlds are the same as the probabilities of the decision trees\nchance states and not with respect to the chance nodes. This is important to avoid invalid goal worlds.\nDeliberation and its Role in the Formation of Intentions 305\na=l\nP(Wioi)'C') = 0.571 P(Lo.oi)'C') = 0.429\nP(Winlno) = 0.276 P(Lonlno) = 0.724\na= 0.42\na=O.S8\no.t ................................... iOO'j winl 300: 2001\nno winj ........................ \ufffd.\ufffd ........ \ufffd,; o\u00b7\nT \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7;o.;\n\ufffd ! no too < \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022 1.\u2022A \u2022\u2022\u2022\u2022\u2022\u2022\u2022...\nFigure 3: Transformation from Decision Trees to Goal Worlds\nfrom which they are derived. Given our semantic con dition earlier that all goal-accessible worlds have corre sponding belief-accessible worlds, the probability dis tribution flows backwards to belief-accessible worlds. This transformation yields the following proposition:\nProposition 1 : Given a decision tree DT we can create a possible worlds interpretation M such that the information given by the decision tree DT is satisfiable for a particular world y and time t in M. We shall denote this by transform(DT,<M,y,t>).\n5 DELIBERATION AND\nINTENTIONS\nGiven a decision tree and the above transformation, an agent can make use of standard decision-theoretic techniques such as maximin or maximizing expected value to deliberate and decide the best plan of action. This best plan of action is what the agent commits to and adopts as an intention.\nTo capture the process of decision theory deliberation, we introduce two generic functions, the value function, denoted by V, and the deliberation function, denoted by 8. The value function assigns a real-valued number to every node in the decision tree and the deliberation function chooses one or more best sequences of actions to perform at a given node. Both these functions will be parameterised on the particular deliberation proce-\ndure used; i.e., maximin deliberation, maximizing the expected utility, or any other deliberation procedure. We shall use the operator ';' to denote sequencing of actions.\nFirst we consider the maximin approach. The value and deliberation functions for this approach is given in Figure 4. For the running example, the maximin deliberation function returns the set {No Poii;Rep, Poii;Rep}.\nNext we examine the principle of maximizing expected value. For decision nodes and terminal nodes, the value function and the deliberation function using the maxexpval principle are identical to the ones under the maximin principle. For chance nodes, the value and deliberation functions are defined as: V(maxexpval, n;) = L:{n,JPS(n,,n;,p;)} P(pj).V(nj) 8(maxexpval, n;) = { Sj ?; 8( maxexpval, ni) IS( n;, ni, Sj)} For the running example the above deliberation func tion returns { Poll;yes?;Sen, Poll;no ?;Rep}. Actions yes? and no? are used as conditional tests.\nModifying the modal operator INTEND with a sub script that indicates the decision procedure used, we can state the following theorem which allows an agent to form intentions based on his deliberation.\nTheorem 1 : If an agent with a decision tree DT with\n306 Rao and George\u00a3\u00a3\n{ min{n;IS(n,,n;,s;)} V(n;) V(maximin, n;) = maX{n;IC(n,,n;,e;)} V(n;)\nU(n;)\nif n; E C if n; E 'D if n; E T\n6(maximin, ni) = { {6(maximin, n;) I S(n;,n;,s;) and V(n;) = V(n;)} {e;;6(maximin,n;) I e(n;,n;,e;) and V(n;) = V(n;)} nil if n; E C if n; E 'D if n; E T\nFigure 4: Value and deliberation functions for maximin\nroot node n and deliberation procedure d chooses as a best plan a sequence of actions a, i.e., a E 6{d, n), and the transformation is transform(DT, <M,y,t>) then M, y, t f: INTENDd(OPTIONAL(<>done(a))).\nTo prove the above theorem, we need to define the process of deliberation within a possible-worlds model that generates intention-accessible worlds from a given set of goal-accessible and belief-accessible worlds. This definition is given elsewhere [13]. The set of intention accessible worlds generated by maxexpval deliberation is shown in F igure 1. The possible-worlds delibera tion can be shown to be equivalent to the decision tree deliberation [13]. This equivalence together with Proposition 1 establishes the above theorem.\nNote that the maximin deliberation function always commits to a particular branch emanating from a chance node; namely, the branch that leads to a min imum value node. This means that 6 for maximin can always return a sequence of actions without being conditional on any state information. However, this is not true in the case of maxexpval deliberation because each chance node is a weighted sum of all its branches. The actions thus have to be conditional on the state of the world. Thus maximin deliberation yields uncon ditional intentions and maxexpval deliberation results in conditional intentions.\nFor the example under discussion, we have the fol lowing unconditional maximin intentions and condi tional maxexpval intentions: (a) the agent intends by maximin that, in all future paths he will stand for the House of representatives, i.e., INTENDmaximin (IN EVITABLE (<>done(Rep))); (b) the agent intends by maxexpval that, in all future paths in which he has carried out a poll and the majority have answered yes, he would stand for the Senate, i.e., INTENDmaxexpval (INEVITABLE(<> (done(Po/0 A yes ::J <>done(Sen)))); and (c) the agent intends by maxexpval that, in all future paths inwhich he has carried out a poll and the majority have answered no, he would stand for the House, i.e., INTENDmaxexpval (INEVITABLE(<> (done(Po/0 1\\ no ::J <)done(Rep)))).\nSo far, we have discussed how deliberation leads to the formation of conditional and unconditional intentions. As discussed in the introduction, intentions play the\ntwo important roles of posing decision problems for deliberation and constraining the options open for de liberation. The decision tree and the decision prob lem that we until now have taken for granted could have been generated because of a top-level intention of Phil to be rich and famous. In other words, a prior intention of the form INTEND(OPTION AL<>(rich A famous)) followed by means-end reasoning could have resulted in the decision tree to conduct a poll and run for the House or Senate seat. Also, if Phil had the prior intention to stand for a Senate seat, i.e., if INTEND(INEVITABLE<>done(Sen)) were true, then the decision tree would be one without any alternatives for standing for the House. Thus, future-directed in tentions constrain the decision problem that has to be considered.\nWe believe that the formalism we have presented here is general enough to cover a wide range of decision problems. The transformation and equivalence estab lished in this paper should help one to choose the ap propriate representation for the appropriate purpose, making use of the results of one representation within the other. For example, one could operate within a possible-worlds BDI framework for reasoning about the interaction of beliefs, goals, and intentions, and how they change with time [15], shift to a decision tree representation for deliberation, and then come back to the possible-worlds framework for reasoning about the intentions so formed.\n6 CONCLUSIONS\nThis paper examines one of the important philosoph ical aspects of Bratman's theory of rational agency; namely, that deliberation leads to the formation of intentions. We have presented a powerful branching time possible-worlds model for reasoning about beliefs, goals, intentions, actions, time, probabilities, and pay offs, and provided a transformation from decision trees to structures in this model. We have also shown how the deliberation procedure used determines the inten tions adopted by the agent. This formal model of de liberation within a BDI-architecture is one of the main contributions of this paper. Previous work on formal izations of BDI-architectures [3, 12, 15] does not ad-\nDeliberation and its Role in the Formation of Intentions 307\ndress this issue.\nRecent work in real-time reasoning has vigorously pur sued the use of decision-theoretic techniques. Russell and Wefald [16] treat computations themselves as ac tions, with appropriate utilities. These computations have to be chosen from among a number of different alternatives and decision theory is used to choose the best action or computation. This facilitates meta-level reasoning. Haddawy and Hanks [9] explore the rela tionships between symbolic goals and numeric utili ties. In particular, they address the problem of build ing utility functions. However, neither approach con siders the role of decision theory in the formation of intentions, which is the primary focus of this paper.\nFagin and Halpern [5] combine reasoning about knowl edge and probabilities by explicitly introducing prob ability formulas. Haddawy [8] introduces reasoning about probabilities in a branching time model by con sidering a world to be a future path. The work pre sented here deals with both future paths in a branching time model and different possible worlds in the epis temic sense. It also introduces explicit reasoning about payoffs and treats payoffs as values the agent places on his future paths within a goal-accessible world. Thus it builds on the existing tradition by combining a possible-worlds BDI framework with decision-theoretic deliberation and explicit reasoning about probabilities and payoffs.\nOur future work in this area will focus on the role of intentions in deliberation and reconsideration. We aim to analyze the need for rational agents to recon sider intentions and when they should carry out such reconsideration.\nReferences\n[1] M. E. Bratman. Intentions, Plans, and Practical Reason. Harvard University Press, Massachusetts, 1987.\n[2] M. E. Bratman, D. Israel, and M. E. Pollack. Plans and resource-bounded practical reasoning. Computational Intelligence, 4:349-355, 1988.\n[3] P. R. Cohen and II. J. Levesque. Intention is choice with commitment. Artificial Intelligence, 42(3), 1990.\n[4] E. A. Emerson and J. Srinivasan. Btanching time temporal logic. In J. W. de Bakker, W. P. de Roever, and G. Rozen berg, editors, Linear Time, Branching Time and Partial Order in Log ics and Models for Concurrency, pages 123-172. Springer-Verlag, Berlin, 1989.\n[5] R. Fagin and J. Y. Halpern. Reasoning about knowledge and probability: Preliminary report. In M. Y. Vardi, editor, Second Conference on Theoretical Aspects of Reasoning About Knowl edge. Morgan Kaufmann Publishers, 1988.\n[6] M. P. Georgeff and Pollack. M. E. Rational agency project. CSLI Monthly, 2(3), 1986.\n[7] M.P. Georgeff and F.F. Ingrand. Decision-making in an embedded reasoning system. In Proceedings of the International Joint Conference on Artificial Intelligence, Detroit, MI, 1989.\n[8] P. Haddawy. Time, chance, and action. In Proceedings of the Sixth Conference on Un certainty in Artificial Intelligence, Cambridge, Mass., U.S.A, July 1990.\n[9] P. Haddawy and S. Hanks. Issues in decision theoretic planning: Symbolic goals and numeric utilities. In Proceedings of the DARPA Workshop on Innovative Approaches to Planning, Schedul ing, and Control, 1990.\n[10] J. M. Jones. Introduction to Decision Theory. Richard D. Irwin, Inc., Homewood, Illinois, 1977.\n[11] D. Kinny and M. P. Georgeff. Commitment and effectiveness of situated agents. In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence {IJCAI-91}, 1991.\n[12] K. Konolige and M. Pollack. A representational ist theory of intention. Technical Report (to be published), SRI International, Menlo Park, Cali fornia, 1990.\n[13] A. S. Rao and M. P. Georgeff. Deliberation and the formation of intentions. Technical Report 10, Australian Artificial Intelligence Institute, Carl ton, Australia, 1990.\n[14] A. S. Rao and M. P. Georgeff. Asymmetry the sis and side-effect problems in linear time and branching time intention logics. In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI-91}, 1991.\n[15] A. S. Rao and M. P. Georgeff. Modeling ratio nal agents within a BDI-architecture. In J. Allen, R. Fikes, and E. Sandewall, editors, Proceedings of the Second International Conference on Princi ples of Knowledge Representation and Reasoning. Morgan Kaufmann Publishers, San Mateo, 1991.\n[16] S. Russell and E. Wefald. Principles of metarea soning. In Proceedings of the First International Conference on Principles of Knowledge Represen tation and Reasoning, Toronto, 1989."}], "references": [{"title": "Intentions, Plans, and Practical Reason", "author": ["M.E. Bratman"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "Plans and resource-bounded practical reasoning", "author": ["M.E. Bratman", "D. Israel", "M.E. Pollack"], "venue": "Computational Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "Intention is choice with commitment", "author": ["P.R. Cohen", "II.J. Levesque"], "venue": "Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "Btanching time temporal logic", "author": ["E.A. Emerson", "J. Srinivasan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "Reasoning about knowledge and probability: Preliminary report", "author": ["R. Fagin", "J.Y. Halpern"], "venue": "Second Conference on Theoretical Aspects of Reasoning About Knowl\u00ad edge. Morgan Kaufmann Publishers,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Decision-making in an embedded reasoning system", "author": ["M.P. Georgeff", "F.F. Ingrand"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Time, chance, and action", "author": ["P. Haddawy"], "venue": "In Proceedings of the Sixth Conference on Un\u00ad certainty in Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1990}, {"title": "Issues in decision\u00ad theoretic planning: Symbolic goals and numeric utilities", "author": ["P. Haddawy", "S. Hanks"], "venue": "In Proceedings of the DARPA Workshop on Innovative Approaches to Planning, Schedul\u00ad ing, and Control,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1990}, {"title": "Introduction to Decision Theory", "author": ["J.M. Jones"], "venue": "Richard D. Irwin, Inc., Homewood, Illinois,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1977}, {"title": "Commitment and effectiveness of situated agents", "author": ["D. Kinny", "M.P. Georgeff"], "venue": "In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence {IJCAI-91},", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "A representational\u00ad ist theory of intention", "author": ["K. Konolige", "M. Pollack"], "venue": "Technical Report (to be published),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1990}, {"title": "Deliberation and the formation of intentions", "author": ["A.S. Rao", "M.P. Georgeff"], "venue": "Technical Report 10, Australian Artificial Intelligence Institute, Carl\u00ad ton, Australia,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}, {"title": "Asymmetry the\u00ad sis and side-effect problems in linear time and branching time intention logics", "author": ["A.S. Rao", "M.P. Georgeff"], "venue": "In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI-91},", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1991}, {"title": "Modeling ratio\u00ad nal agents within a BDI-architecture", "author": ["A.S. Rao", "M.P. Georgeff"], "venue": "Proceedings of the Second International Conference on Princi\u00ad ples of Knowledge Representation and Reasoning", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1991}, {"title": "Principles of metarea\u00ad soning", "author": ["S. Russell", "E. Wefald"], "venue": "In Proceedings of the First International Conference on Principles of Knowledge Represen\u00ad tation and Reasoning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1989}], "referenceMentions": [{"referenceID": 0, "context": "The design of rational agents, situated in a dynamic world and operating effectively under real-time con\u00ad straints and resource limitations, has been of great interest to researchers in philosophy, artificial intel\u00ad ligence, and computer science [1, 2, 7].", "startOffset": 246, "endOffset": 255}, {"referenceID": 1, "context": "The design of rational agents, situated in a dynamic world and operating effectively under real-time con\u00ad straints and resource limitations, has been of great interest to researchers in philosophy, artificial intel\u00ad ligence, and computer science [1, 2, 7].", "startOffset": 246, "endOffset": 255}, {"referenceID": 5, "context": "The design of rational agents, situated in a dynamic world and operating effectively under real-time con\u00ad straints and resource limitations, has been of great interest to researchers in philosophy, artificial intel\u00ad ligence, and computer science [1, 2, 7].", "startOffset": 246, "endOffset": 255}, {"referenceID": 1, "context": "One such design is provided by the belief-desire\u00ad intention (BDI) architecture [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "While most philosophical accounts of rational agency treat intentions as being reducible to beliefs and de\u00ad sires, Bratman [1] argues convincingly that intentions, especially future-directed intentions, play a significant and distinct role in resource-bounded reasoning.", "startOffset": 123, "endOffset": 126}, {"referenceID": 9, "context": "1 Some interesting experimental work has recently been done in this area [11].", "startOffset": 73, "endOffset": 77}, {"referenceID": 6, "context": "PROB(\u00a22) [8].", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "Phil, who is currently in the House of Representatives, believes that he can stand for the House of representatives (Rep), switch to the Senate and stand for a Senate seat (Sen), or retire from politics (Ret) [10].", "startOffset": 209, "endOffset": 213}, {"referenceID": 11, "context": "The formal definition of sub-worlds and the axioms corresponding to the following semantic conditions are given elsewhere [13].", "startOffset": 122, "endOffset": 126}, {"referenceID": 13, "context": "We introduce a constraint on our belief, goal, and intention-accessible worlds, called strong realism [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "We show elsewhere [14, 15] how these con\u00ad ditions can be relaxed to solve some of the problems associated with possible-world representations of be\u00ad liefs, goals, and intentions [3].", "startOffset": 18, "endOffset": 26}, {"referenceID": 13, "context": "We show elsewhere [14, 15] how these con\u00ad ditions can be relaxed to solve some of the problems associated with possible-world representations of be\u00ad liefs, goals, and intentions [3].", "startOffset": 18, "endOffset": 26}, {"referenceID": 2, "context": "We show elsewhere [14, 15] how these con\u00ad ditions can be relaxed to solve some of the problems associated with possible-world representations of be\u00ad liefs, goals, and intentions [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 2, "context": "Note that this intuitive map\u00ad ping is possible only because we have chosen to rep\u00ad resent each possible world as a branching-time struc\u00ad ture, rather than the more traditional model where each possible world is a linear-time structure [3].", "startOffset": 235, "endOffset": 238}, {"referenceID": 11, "context": "The final transformation from these multiple decision trees with no chance nodes to possible worlds is trivial and is given elsewhere [13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 11, "context": "This definition is given elsewhere [13].", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "The possible-worlds delibera\u00ad tion can be shown to be equivalent to the decision tree deliberation [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "For example, one could operate within a possible-worlds BDI framework for reasoning about the interaction of beliefs, goals, and intentions, and how they change with time [15], shift to a decision tree representation for deliberation, and then come back to the possible-worlds framework for reasoning about the intentions so formed.", "startOffset": 171, "endOffset": 175}, {"referenceID": 2, "context": "Previous work on formal\u00ad izations of BDI-architectures [3, 12, 15] does not ad-", "startOffset": 55, "endOffset": 66}, {"referenceID": 10, "context": "Previous work on formal\u00ad izations of BDI-architectures [3, 12, 15] does not ad-", "startOffset": 55, "endOffset": 66}, {"referenceID": 13, "context": "Previous work on formal\u00ad izations of BDI-architectures [3, 12, 15] does not ad-", "startOffset": 55, "endOffset": 66}, {"referenceID": 14, "context": "Russell and Wefald [16] treat computations themselves as ac\u00ad tions, with appropriate utilities.", "startOffset": 19, "endOffset": 23}, {"referenceID": 7, "context": "Haddawy and Hanks [9] explore the rela\u00ad tionships between symbolic goals and numeric utili\u00ad ties.", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "Fagin and Halpern [5] combine reasoning about knowl\u00ad edge and probabilities by explicitly introducing prob\u00ad ability formulas.", "startOffset": 18, "endOffset": 21}, {"referenceID": 6, "context": "Haddawy [8] introduces reasoning about probabilities in a branching time model by con\u00ad sidering a world to be a future path.", "startOffset": 8, "endOffset": 11}], "year": 2011, "abstractText": "Deliberation plays an important role in the design of rational agents embedded in the real-world. In particular, deliberation leads to the formation of intentions, i.e., plans of action that the agent is committed to achiev\u00ad ing. In this paper, we present a branching\u00ad time possible-worlds model for representing and reasoning about, beliefs, goals, inten\u00ad tions, time, actions, probabilities, and pay\u00ad offs. We compare this possible-worlds ap\u00ad proach with the more traditional decision\u00ad tree representation and provide a transfor\u00ad mation from decision trees to possible worlds. Finally, we illustrate how an agent can per\u00ad form deliberation using a decision-tree rep\u00ad resentation and then use a possible-worlds model to form and reason about his inten\u00ad tions.", "creator": "pdftk 1.41 - www.pdftk.com"}}}