{"id": "1304.0740", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Apr-2013", "title": "O(logT) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions", "abstract": "traditional algorithms for stochastic optimization require projecting the solution at each iteration window into a given domain to ensure its feasibility. when facing complex domains, such as positive semi - definite cones, the projection operation can be expensive, leading to a high computational implementation cost scenario per iteration. in this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. the proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini - steep batch, extra - finite gradient, convex and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. we show, both in initial expectation and with a high probability, that when the objective function \u03b1 is declared both relatively smooth and strongly robust convex, the earliest proposed algorithm achieves exceeding the optimal $ o ( 1 / 25 t ) $ rate of convergence comparing with only $ o ( \\ log t ) $ h projections. our empirical study initially verifies the first theoretical result.", "histories": [["v1", "Tue, 2 Apr 2013 19:11:23 GMT  (37kb)", "http://arxiv.org/abs/1304.0740v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lijun zhang 0005", "tianbao yang", "rong jin", "xiaofei he"], "accepted": true, "id": "1304.0740"}, "pdf": {"name": "1304.0740.pdf", "metadata": {"source": "CRF", "title": "O(logT ) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions", "authors": ["Lijun Zhang", "Tianbao Yang", "Rong Jin", "Xiaofei He", "Zhang Yang", "Jin He"], "emails": ["zhanglij@msu.edu", "tyang@ge.com", "rongjin@cse.msu.edu", "xiaofeihe@cad.zju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 4.\n07 40\nv1 [\nKeywords: Epoch gradient descent, extra-gradient descent, mini-batch, strongly convex, smooth"}, {"heading": "1. Introduction", "text": "The goal of stochastic optimization is to solve the optimization problem\nmin w\u2208D F (w),\nusing only the stochastic gradients of F (w). In particular, we assume there exists a gradient oracle, which for any point w \u2208 D, returns a random vector g\u0302(w) that gives an unbiased estimate of the subgradient of F (\u00b7) at w. A special case of stochastic optimization is the risk minimization problem, whose objective function is given by\nF (w) = E(x,y) [\u2113(w; (x, y))] ,\nc\u00a9 2013 L. Zhang, T. Yang, R. Jin & X. He.\nwhere (x, y) is an instance-label pair, \u2113 is a convex loss function that measures the prediction error, and the expectation is taken oven the unknown joint distribution of (x, y) (Zhang, 2004; Shalev-Shwartz et al., 2009; Hu et al., 2009). The performance of stochastic optimization algorithms is typically characterized by the excess risk\nF (wT )\u2212 min w\u2208D F (w),\nwhere T is the number of iterations and wT is the solution obtained after making T calls to the gradient oracle.\nFor general Lipschitz continuous convex functions, stochastic gradient descent exhibits the unimprovable O(1/ \u221a T ) rate of convergence (Nemirovski and Yudin, 1983; Nemirovski et al., 2009). For strongly-convex functions, the algorithms proposed in very recent works (Juditsky and Nesterov, 2010; Hazan and Kale, 2011; Rakhlin et al., 2012; Chen et al., 2012) achieve the optimal O(1/T ) rate (Agarwal et al., 2012). Although these convergence rates are significantly worse than the results in deterministic optimization, stochastic optimization is appealing due to its low per-iteration complexity. However, this is not the case when the domain D is complex. This is because most stochastic optimization algorithms require projecting the solution at each iteration into domain D to ensure its feasibility, an expensive operation when the domain is complex. In this paper, we show that if the objective function is smooth and strongly convex, it is possible to reduce the number of projections dramatically without affecting the convergence rate.\nOur work is motivated by the difference in convergence rates between stochastic and deterministic optimization. When the objective function is smooth and convex, under the first-order oracle assumption, Nesterov\u2019s accelerated gradient method enjoys the optimal O(1/T 2) rate (Nesterov, 2004, 2005). Thus, for deterministic optimization of smooth and convex functions, we can achieve an O(1/ \u221a T ) rate by only performing O(T 1/4) updating. When the objective function is smooth and strongly convex, the optimal rate for first-order algorithms is O(1/\u03b1k), for some constant \u03b1 > 1 (Nesterov, 2004, 2007). In other words, for deterministic optimization of smooth and strongly convex functions, we can achieve an O(1/T ) rate by only performing O(log T ) updating. The above observations inspire us to consider the following questions.\n1. For Stochastic Optimization of Smooth and Convex functions (SOSC), is it possible to maintain the optimal O(1/ \u221a T ) rate by performing O(T 1/4) projections? 2. For Stochastic Optimization of Smooth and Strongly Convex functions (SOS2C), is it possible to maintain the optimal O(1/T ) rate by performing O(log T ) projections?\nFor the 1st question, we have found a positive answer from literature. By combining mini-batches (Roux et al., 2008) with the accelerated stochastic approximation (Lan, 2012), we can achieve the optimal O(1/ \u221a T ) rate by performing O(T 1/4) projections (Cotter et al., 2011). However, a naive application of mini-batches does not lead to the desired O(log T ) complexity for SOS2C. The main contribution of this paper is a novel stochastic optimization algorithm that answers the 2nd question positively. Our theoretical analysis reveals, both in expectation and with a high probability, that the proposed algorithm achieves the optimal O(1/T ) rate by only performing O(log T ) projections."}, {"heading": "2. Related Work", "text": "In this section, we provide a brief review of the existing approaches for avoiding projections."}, {"heading": "2.1. Mini-batch based algorithms", "text": "Instead of updating the solution after each call to the gradient oracle, mini-batch based algorithms use the average gradient over multiple calls to update the solution (Roux et al., 2008; Shalev-Shwartz et al., 2011; Dekel et al., 2011). For a fixed batch size B, the number of updates (and projections) is reduced from O(T ) to O(T/B), and the variance of the stochastic gradient is reduced from \u03c3 to \u03c3/ \u221a B. By appropriately balancing between the loss cased by a smaller number of updates and the reduction in the variance of stochastic gradients, it is able to maintain the optimal rate of convergence.\nThe idea of mini-batches can be incorporated into any stochastic optimization algorithm that uses gradient-based updating rules. When the objective function is smooth and convex, combining mini-batches with the accelerated stochastic approximation (Lan, 2012) leads to\nO\n(\nB2 T 2 + 1\u221a T\n)\nrate of convergence (Cotter et al., 2011). By setting B = T 3/4, we achieve the optimal O(1/ \u221a T ) rate with only O(T 1/4) projections. When the target function is smooth and strongly convex, we can apply mini-batches to the optimal algorithms for strongly convex functions (Hu et al., 2009; Ghadimi and Lan, 2012), leading to\nO\n(\nB2 T 2 + 1 T\n)\nrate of convergence (Dekel et al., 2012). In order to maintain the optimal O(1/T ) rate, the value of B cannot be larger than \u221a T , implying at least O( \u221a T ) projections are required. In contrast, the algorithm proposed in this paper achieves an O(1/T ) rate with only O(log T ) projections."}, {"heading": "2.2. Projection free algorithms", "text": "Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al., 2013). At each iteration of the Frank-Wolfe algorithm, instead of performing a projection that requires solving a constrained quadratic programming problem, it solves a constrained linear programming problem. For many domains of interest, including the positive semidefinite cone and the trace norm ball, the constrained linear problem can be solved more efficiently than a projection problem (Jaggi, 2013), making this kind of methods attractive for large-scale optimization.\nIn a recent work (Hazan and Kale, 2012), an online variant of the Frank-Wolfe algorithm is proposed. Although the online Frank-Wolfe algorithm exhibits an O(1/ \u221a T ) convergence rate for smooth functions, it is unable to achieve the optimal O(1/T ) rate for strongly convex functions. Besides, the memory complexity of this algorithm is O(T ), making it\nunsuitable for large-scale optimization problems. Another related work is the stochastic gradient descent with only one projection (Mahdavi et al., 2012). This algorithm is built upon the assumption that the solution domain can be characterized by an inequality constraint g(w) \u2264 0 and the gradient of g(\u00b7) can be evaluated efficiently. Unfortunately, this assumption does not hold for some commonly used domain (e.g., the trace norm ball). Compared to the projection free algorithms, our proposed method is more general because it make no assumption about the solution domain."}, {"heading": "3. Stochastic Optimization of Smooth and Strongly Convex Functions", "text": ""}, {"heading": "3.1. Preliminaries", "text": "We first define smoothness and strongly convexity.\nDefinition 1 A function f : D \u2192 R is L-smooth w.r.t. a norm \u2016 \u00b7 \u2016 if f is everywhere differentiable and\n\u2016\u2207f(w)\u2212\u2207f(w\u2032)\u2016\u2217 \u2264 L\u2016w \u2212w\u2032\u2016, \u2200w,w\u2032 \u2208 D.\nwhere \u2016 \u00b7 \u2016\u2217 is the dual norm.\nDefinition 2 A smooth function f : D \u2192 R is \u03bb-strongly convex w.r.t. a norm \u2016 \u00b7 \u2016, if f is everywhere differentiable and\n\u2016\u2207f(w)\u2212\u2207f(w\u2032)\u2016\u2217 \u2265 \u03bb\u2016w \u2212w\u2032\u2016, \u2200w,w\u2032 \u2208 D.\nTo simplify our analysis, we assume that both \u2016 \u00b7 \u2016 and \u2016 \u00b7 \u2016\u2217 are the vector \u21132 norm in the following discussion.\nFollowing (Hazan and Kale, 2011), we make the following assumptions about the gradient oracle.\n\u2022 There is a gradient oracle, which, for a given input point w returns a stochastic gradient g\u0302(w) whose expectation is the gradient of F (w) at w, i.e.,\nE[g\u0302(w)] = \u2207F (w).\nWe further assume the stochastic gradients obtained by calling the oracle are independent.\n\u2022 The gradient oracle is G-bounded, i.e.,\n\u2016g\u0302(w)\u2016 \u2264 G, \u2200w \u2208 D.\nWe note that this assumption may be relaxed by assuming the orlicz norm of g\u0302(w) to be bounded (Lan, 2012), i.e., E[exp(\u2016g\u0302(w)\u20162/G2)] \u2264 exp(1). Although our theoretical result holds even under the assumption of bounded orlicz norm, we choose the Gbounded gradient for simplicity. Define w\u2217 as the optimal solution that minimizes F (w), i.e., w\u2217 = argminw\u2208D F (w). Using the strongly convexity of F (w), we have (Hazan and Kale, 2011)\n\u03bb 2 \u2016w \u2212w\u2217\u20162 \u2264 F (w)\u2212 F (w\u2217) \u2264\n2G2\n\u03bb ,\u2200 w \u2208 D. (1)"}, {"heading": "3.2. The Algorithm", "text": "Algorithm 1 shows the proposed method for Stochastic Optimization of Smooth and Strongly Convex functions (SOS2C), that achieves the optimal O(1/T ) rate of convergence by performing O(log T ) projections. The inputs of the algorithm are: (1) \u03b7, the step size, (2) M , the fixed number of updates per epoch/stage, (3) B1, the initial batch size, and (4) T , the total number of calls to the gradient oracle. With a slight abuse of notation, we use g\u0302(w, i) to denote the stochastic gradient at w obtained after making the i-th call to the oracle. We denote the projection of w onto the domain D by \u03a0D(w).\nSimilar to the epoch gradient descent algorithm (Hazan and Kale, 2011), the proposed algorithm consists of two layers of loops. It uses the outer (while) loop to divide the learning process into a sequence of epochs (Step 5 to Step 12). Similar to (Hazan and Kale, 2011), the number of calls to the gradient oracle made by Algorithm 1 increases exponentially over the epoches, a key that allows us to achieve the optimal O(1/T ) convergence rate for strongly convex functions. We note that other techniques, such as the \u03b1-suffix averaging (Rakhlin et al., 2012), can also be used as an alternative.\nIn the inner (for) loop of each epoch, we combine the idea of mini-batches (Dekel et al., 2011) with extra-gradient descent (Nemirovski, 2005; Juditsky et al., 2011). We choose extra-gradient descent because it allows us to replace in the excess risk bound E[\u2016g\u0302(w)\u20162] with E[\u2016g\u0302(w)\u2212 E[g\u0302(w)]\u20162], the variance of the stochastic gradient g\u0302(w), thus opening the door to fully exploring the capacity of mini-batches in variance reduction.\nTo be more specific, in the k-th epoch, we maintain two sequences of solutions {wkt }Mt=1 and {zkt }Mt=1, where zkt is an auxiliary solution that allows us to effectively explore the smoothness of the loss function. At each iteration t of the k-th epoch, we calculate the average gradients g\u0304kt and f\u0304 k t by calling the gradient oracle B\nk times (Steps 6 and 8), and update the solutions wkt and z k t using the average gradients (Steps 7 and 9). The batch size Bk is fixed inside each epoch but doubles from epoch to epoch (Step 11). This is in contrast to most mini-batch based algorithms that have a fixed batch size. This difference is critical for achieving O(1/T ) convergence rate with only O(log T ) updates."}, {"heading": "3.3. The main results", "text": "The following theorem bounds the expected excess risk of the solution return by Algorithm 1 and the number of projections.\nTheorem 1 Set the parameters in Algorithm 1 as\n\u03b7 = 1\u221a 6L , M = 4 \u03b7\u03bb and B1 = 12\u03b7\u03bb.\nThe final point wk1 returned by Algorithm 1 makes at most T calls to the gradient oracle, and has its excess risk bounded by\nE[F (wk1)\u2212 F (w\u2217)] \u2264 384G2\n\u03bbT = O\n(\n1\nT\n)\n,\nand the total number of projections bounded by\n8 \u221a 6L\n\u03bb\n\u230a\nlog2\n(\nT 96 + 1\n)\u230b\n= O (log T ) .\nAlgorithm 1 log T Projections for SOS2C\n1: Input: parameters \u03b7, M , B1 and T 2: Initialize w11 \u2208 D arbitrarily 3: Set k = 1 4: while 2M\n\u2211k i=1 B i \u2264 T do 5: for t = 1 to M do 6: Compute the average gradient at wkt over B k calls to the gradient oracle\ng\u0304kt = 1\nBk\nBk \u2211\ni=1\ng\u0302(wkt , i)\n7: Update\nzkt = \u03a0D\n( wkt \u2212 \u03b7g\u0304kt )\n8: Compute the average gradient at zkt over B k calls to the gradient oracle\nf\u0304kt = 1\nBk\nBk \u2211\ni=1\ng\u0302(zkt , i)\n9: Update\nwkt+1 = \u03a0D\n( wkt \u2212 \u03b7f\u0304kt )\n10: end for 11: wk+11 = 1 M \u2211M t=1 z k t , and B k+1 = 2Bk 12: k = k + 1 13: end while 14: Return: wk1\nTheorem 1 shows that in expectation, Algorithm 1 achieve an O(1/T ) convergence with O(log T ) updates. The following theorem gives a high probability bound of the excess risk for Algorithm 1.\nTheorem 2 Set the parameters in Algorithm 1 as\n\u03b7 = 1\u221a 6L , M = 4 \u03b7\u03bb and B1 = \u03b1\u03b7\u03bb,\nwhere \u03b1 is defined below. For any 0 < \u03b4 < 1, let\n\u03b4\u0303 = \u03b4\nk\u2020 ,\nk\u2020 =\n\u230a\nlog2\n(\nT\n8\u03b1 + 1\n)\u230b\n= O(log T ), (2)\n\u03b1 =max\n{\n400 log2 8M\n\u03b4\u0303 , 1 + 64 log2\n8M\n\u03b4\u0303\n(\nlog 4N\n\u03b4\u0303 +\n4 9 log2 4N\n\u03b4\u0303\n)}\n(3)\n=O\n[\n(\nlog log T + log 1\n\u03b4\n)4 ]\n,\nN =\n\u2308\nlog2 4MT\n\u03b7\u03bb\n\u2309\n= O(log T ). (4)\nThe final point wk1 returned by Algorithm 1 makes at most T calls to the gradient oracles, performs\n8 \u221a 6L\n\u03bb\n\u230a\nlog2\n(\nT\n8\u03b1 + 1\n)\u230b\n= O (log T )\nprojections, and with a probability at least 1\u2212 \u03b4, has its excess risk bounded by\nF (wk1)\u2212 F (w\u2217) \u2264 32\u03b1G2\n\u03bbT = O\n(\n(log log T + log 1/\u03b4)4\nT\n)\n.\nRemark: It is worth noting that we achieve the high probability bound without making any modifications to Algorithm 1. This is in contrast to the epoch gradient descent algorithm (Hazan and Kale, 2011) that needs to shrink the domain size in order to obtain the desirable high probability bound, which could potentially lead to an additional computational cost in performing projection. We remove the shrinking step by effectively exploring the peeling technique (Bartlett et al., 2005).\nThe number of projections required by Algorithm 1, according to Theorem 2, exhibits a linear dependence on the conditional number L/\u03bb, which can be very large when dealing with ill-conditioned optimization problems. In the deterministic setting, the convergence rate only depends on the square root of the conditional number (Nesterov, 2004, 2007). Thus, we conjecture that it may be possible to improve the dependence on the conditional number to its square root in the stochastic setting, a problem that will be examined in the future."}, {"heading": "4. Analysis", "text": "We here present the proofs of main theorems. The omitted proofs are provided in the supplementary material."}, {"heading": "4.1. Proof of Theorem 1", "text": "Since we make use of the the multi-stage learning strategy, the proof provided below is similar to the proof in (Hazan and Kale, 2011). We begin by analyzing the property of the inner loop in Algorithm 1, which is a combination of mini-batches and the extra-gradient descent. To this end, we have the following lemma.\nLemma 3 Let \u03b7 = 1/[ \u221a 6L] in Algorithm 1. Then, we have\nF\n(\n1\nM\nM \u2211\nt=1\nzkt\n)\n\u2212 F (w\u2217) \u2264 \u2016wk1 \u2212w\u2217\u20162 2M\u03b7 \u2212 \u03bb 2M\nM \u2211\nt=1\n\u2016zkt \u2212w\u2217\u20162\n+ 3\u03b7\nM\n(\nM \u2211\nt=1\n\u2016g\u0304kt \u2212 gkt \u20162 + M \u2211\nt=1\n\u2016f\u0304kt \u2212 fkt \u20162 )\n(5)\n+ 1\nM\nM \u2211\nt=1\n\u3008fkt \u2212 f\u0304kt , zkt \u2212w\u2217\u3009 (6)\nwhere gkt = \u2207F (wkt ) and fkt = \u2207F (zkt ).\nTaking the conditional expectation of the inequality, we have\nEk\u22121\n[\nF\n(\n1\nM\nM \u2211\nt=1\nzkt\n)]\n\u2212 F (w\u2217) \u2264 \u2016wk1 \u2212w\u2217\u20162\n2M\u03b7 +\n6\u03b7G2\nBk .\nwhere Ek\u22121[\u00b7] denotes the expectation conditioned on all the randomness up to epoch k \u2212 1.\nThe quantity in (5) illustrates the advantage of the extra-gradient descent, i.e., it is able to produce variance-dependent upper bound when applied to stochastic optimization. Because of mini-batches, the expectations of \u2016g\u0304kt \u2212 gkt \u20162 and \u2016f\u0304kt \u2212 fkt \u20162 are smaller than G2/Bk, which leads to the tight upper bound in the second inequality.\nBased on Lemma 3, we get the following lemma that bounds the expected excess risk in each epoch.\nLemma 4 Define \u2206k = F (w k 1)\u2212 F (w\u2217). Set the parameters \u03b7 = 1/[ \u221a 6L], M = 4/[\u03b7\u03bb] and B1 = 12\u03b7\u03bb in Algorithm 1. For any k, we have\nE[\u2206k] \u2264 Vk = G2\n\u03bb2k\u22122 .\nProof It is straightforward to check that\nBk = 12\u03b7\u03bb2k\u22121 = 24\u03b7G2\nVk . (7)\nWe prove this lemma by induction on k. When k = 1, we know that\n\u22061 = F (w 1 1)\u2212 F (w\u2217)\n(1) \u2264 2G 2\n\u03bb =\nG2\n\u03bb21\u22122 = V1.\nAssume that E[\u2206k] \u2264 Vk for some k \u2265 1, and we prove the inequality for k + 1. From Lemma 3, we have\nEk\u22121\n[ F (\nwk+11\n)] \u2212 F (w\u2217) \u2264 \u2016wk1 \u2212w\u2217\u20162\n2M\u03b7 +\n6\u03b7G2\nBk .\nThus\nE [ F (\nwk+11\n)]\n\u2212 F (w\u2217)\n\u2264 E[\u2016w k 1 \u2212w\u2217\u20162] 2M\u03b7 + 6\u03b7G2 Bk\n(1) \u2264 E[2(F (w k 1)\u2212 F (w\u2217))/\u03bb] 2M\u03b7 + 6\u03b7G2 Bk\n(7) =\nE[\u2206k] M\u03b7\u03bb + Vk 4 \u2264 Vk 4 + Vk 4 = Vk+1.\nWe are now at the position to prove Theorem 1. Proof [Proof of Theorem 1] From the stopping criterion of the outer loop in Algorithm 1, we know that the number of the epochs is given by the largest value of k such that\n2M\nk \u2211\ni=1\nBi \u2264 T.\nSince\n2M\nk \u2211\ni=1\nBi = 24M\u03b7\u03bb\nk \u2211\ni=1\n2i\u22121 = 96(2k \u2212 1),\nthe final epoch is given by\nk\u2020 =\n\u230a\nlog2\n(\nT 96 + 1\n)\u230b\n,\nand the final output is wk \u2020+1\n1 . From Lemma 4, we have\nE[F (wk \u2020+1 1 )]\u2212 F (w\u2217) \u2264 Vk\u2020+1 = G2 2k\u2020\u22121\u03bb \u2264 384G 2 \u03bbT ,\nwhere we use the fact\n2k \u2020 \u2265 1\n2\n(\nT 96 + 1\n)\n\u2265 T 192 .\nThe total number of projections is\n2Mk\u2020 = 8 \u221a 6L\n\u03bb\n\u230a\nlog2\n(\nT 96 + 1\n)\u230b\n."}, {"heading": "4.2. Proof of Theorem 2", "text": "Compared to the proof of Theorem 1, the main difference here is that we need a high probability version of Lemma 3. Specifically, we need to provide high probability bounds for the quantities in (5) and (6).\nTo bound the variances given in (5), we need the following norm concentration inequality in Hilbert Space (Smale and Zhou, 2009).\nLemma 5 Let H be a Hilbert Space and let \u03be be a random variable on (Z, \u03c1) with values in H. Assume \u2016\u03be\u2016 \u2264 B < \u221e almost surely. Let {\u03bei}mi=1 be independent random drawers of \u03c1. For any 0 < \u03b4 < 1, with a probability at least 1\u2212 \u03b4,\n\u2225 \u2225 \u2225 \u2225 \u2225 1 m m \u2211\ni=1\n(\u03bei \u2212 E[\u03bei]) \u2225 \u2225 \u2225 \u2225\n\u2225 \u2264 4B\u221a m log 2 \u03b4 .\nBased on Lemma 5, it is straightforward to prove the following lemma.\nLemma 6 With a probability at least 1\u2212 \u03b4\u0303/2, we have\n\u2016g\u0304kt \u2212 gkt \u2016 \u2264 4G\u221a Bk log 4M \u03b4\u0303 , \u2200 t = 1, . . . ,M. (8)\nSimilarly, with a probability at least 1\u2212 \u03b4\u0303/4, we have\n\u2016f\u0304kt \u2212 fkt \u2016 \u2264 4G\u221a Bk log 8M \u03b4\u0303 , \u2200 t = 1, . . . ,M. (9)\nWe define the Martingale difference sequence:\nZkt = \u3008fkt \u2212 f\u0304kt , zkt \u2212w\u2217\u3009.\nIn order to bound the summation of Zkt in (6), we make use of the Berstein inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique described in (Bartlett et al., 2005), leading to the following Lemma.\nLemma 7 We use E1 to denote the event that all the inequalities in (9) hold. On event E1, with a probability at least 1\u2212 \u03b4\u0303/4, we have\nM \u2211\nt=1\nZkt \u2264 4G2\u03b7M\nBk log2\n8M\n\u03b4\u0303 +\nG2\n\u03bbBk\n[\n1 + 64 log2 8M\n\u03b4\u0303\n(\nlog 4n\n\u03b4\u0303 +\n4 9 log2 4n\n\u03b4\u0303\n)]\n+ \u03bb\n2\nM \u2211\nt=1\n\u2016zkt \u2212w\u2217\u20162,\nwhere\nn =\n\u2308\nlog2 4MBk\n\u03b7\u03bb\n\u2309\n. (10)\nSubstituting the results in Lemmas 6 and 7 into Lemma 3, we obtain the lemma below.\nLemma 8 For any 0 < \u03b4\u0303 < 1, with a probability at least 1\u2212 \u03b4\u0303, we have\nF\n(\n1\nM\nM \u2211\nt=1\nzkt\n)\n\u2212 F (w\u2217) \u2264 \u2016wk1 \u2212w\u2217\u20162\n2M\u03b7 +\n100G2\u03b7\nBk log2\n8M\n\u03b4\u0303\n+ G2\n\u03bbBkM\n[\n1 + 64 log2 8M\n\u03b4\u0303\n(\nlog 4n\n\u03b4\u0303 +\n4 9 log2 4n\n\u03b4\u0303\n)]\n,\nwhere n is given in (10).\nBased on Lemma 8, we provide a high probability version of Lemma 4, that bounds the excess risk in each epoch with a high probability. Lemma 9 Set the parameters \u03b7 = 1/[ \u221a 6L], M = 4/[\u03b7\u03bb] and B1 = \u03b1\u03b7\u03bb in Algorithm 1, where \u03b1 is defined in (3). For any k, with a probability at least (1\u2212 \u03b4\u0303)k\u22121, we have\n\u2206k = F (w k 1)\u2212 F (w\u2217) \u2264 Vk =\nG2\n\u03bb2k\u22122 .\nProof We follow the logic used in the proof of Lemma 4. It is straightforward to check that\nBk = \u03b1\u03b7\u03bb2k\u22121 = 2\u03b1\u03b7G2\nVk .\nWhen k = 1, with a probability (1\u2212 \u03b4\u0303)1\u22121 = 1, we have\n\u22061 = F (w 1 1)\u2212 F (w\u2217)\n(1) \u2264 2G 2\n\u03bb =\nG2\n\u03bb21\u22122 = V1.\nAssume that with a probability at least (1\u2212 \u03b4\u0303)k\u22121, \u2206k \u2264 Vk for some k \u2265 1. We now prove the case for k + 1. Notice that N defined in (4) is larger than n defined in (10). From Lemma 8, with a probability at least 1\u2212 \u03b4\u0303, we have\n\u2206k+1 = F (w k+1 1 )\u2212 F (w\u2217)\n\u2264\u2016w k 1 \u2212w\u2217\u20162 2M\u03b7 + 100G2\u03b7 Bk log2 8M \u03b4\u0303 + G2 \u03bbBkM\n[\n1 + 64 log2 8M\n\u03b4\u0303\n(\nlog 4N\n\u03b4\u0303 +\n4 9 log2 4N\n\u03b4\u0303\n)]\n\u2264\u2206k 4 + 400 \u03b1 log2 8M\n\u03b4\u0303\nVk 8 + 1 \u03b1\n[\n1 + 64 log2 8M\n\u03b4\u0303\n(\nlog 4N\n\u03b4\u0303 +\n4 9 log2 4N\n\u03b4\u0303\n)]\nVk 8 .\nUsing the definition of \u03b1 in (3), with a probability at least (1\u2212 \u03b4\u0303)k we have,\n\u2206k+1 \u2264 1\n4 Vk +\n1 8 Vk + 1 8 Vk = 1 2 Vk = Vk+1.\nNow, we provide the proof of Theorem 2.\nProof [Proof of Theorem 2] The number of epochs made is given by the largest value of k satisfying 2M\n\u2211k i=1B i \u2264 T . Since\n2M k \u2211\ni=1\nBi = 2M\u03b1\u03bb\u03b7 k \u2211\ni=1\n2i\u22121 = 8\u03b1(2k \u2212 1),\nk\u2020 defined in (2) is the value of the final epoch, and the final output is wk \u2020+1\n1 . From\nLemma 9, we have with a probability at least (1\u2212 \u03b4\u0303)k\u2020\nF (wk \u2020+1 1 )\u2212 F (w\u2217) = \u2206k\u2020+1 \u2264 Vk\u2020+1 = G2\n2k\u2020\u22121\u03bb =\n2G2\n2k\u2020\u03bb \u2264 32\u03b1G\n2\n\u03bbT ,\nwhere we use the fact\n2k \u2020 \u2265 1\n2\n(\nT\n8\u03b1 + 1\n)\n\u2265 T 16\u03b1 .\nWe complete the proof by using the property that (1 \u2212 1x)x is an increasing function when x > 1, which implies\n(1\u2212 \u03b4\u0303)k\u2020 = ( 1\u2212 \u03b4 k\u2020\n)k\u2020\n=\n(\n(\n1\u2212 1 k\u2020/\u03b4\n)k\u2020/\u03b4 )\u03b4 \u2265 ( (\n1\u2212 1 1/\u03b4\n)1/\u03b4 )\u03b4\n= 1\u2212 \u03b4."}, {"heading": "5. Experiments", "text": "In this section, we present numerical experiments to support our theoretical analysis. We studied the following algorithms:\n1. log T : the proposed algorithm that is optimal for SOS2C but only needs log(T ) projections; 2. EP GD: the epoch gradient descent developed in (Hazan and Kale, 2011), which is also optimal for SOS2C but needs O(T ) projections; 3. SGD: the stochastic gradient descent with step size \u03b7t = 1/(\u03bbt) (Shalev-Shwartz et al., 2011), which achieves O(log T/T ) rate of convergence for general SOS2C and needs O(T ) projections. We first consider the a simple stochastic optimization problem adapted from (Rakhlin et al., 2012), which is both smooth and strongly convex. The objective function is F (W ) = 12\u2016W\u20162F and the domain is the 5 \u00d7 5 dimensional positive semidefinite (PSD) cone. The stochastic gradient oracle, given a pointW , returns the stochastic gradientW+Z where Z is uniformly distributed in [\u22121, 1]5\u00d75. Because of the noise matrix Z, all the immediate solutions are not PDS and we need to project them back to the PSD cone. To ensure the eigendecomposition only involving real numbers, we further require Z to be symmetric. Notice that for this problem we know W\u2217 = argminW o F (W ) = 0 5\u00d75. Since the gradient of W\u2217 is 0 5\u00d75, it can be shown that SGD also achieves the optimal O(1/T ) rate of convergence on this problem (Rakhlin et al., 2012).\nLet WT be the solution returned after making T calls to the gradient oracle. To verify if the proposed algorithm achieves an O(1/T ) convergence, we measure (F (WT )\u2212 F (W\u2217))\u00d7 T versus T , which is given in Fig. 1(a). We observe that when T is sufficiently large, quantity (F (WT ) \u2212 F (W\u2217)) \u00d7 T essentially becomes a constant for all three algorithms, implying O(1/T ) convergence rates for all the algorithms. We also observe that the constant achieved by the proposed algorithm is slightly larger than the two competitors, which can be attributed to the term (log log T )4 in our bound in Theorem 2. To demonstrate the advantage of our algorithm, we plot the value of the objective function versus the number of projections P in Fig. 1(b). We observe that using our algorithm, the objective function is reduced significantly faster than other algorithms w.r.t. the number of projections.\nIn the second experiment, we apply our algorithm to the regularized distance metric learning (Jin et al., 2009). The goal is to solve the following problem\nmin W 0\nE(xi,yi),(xj ,yj)[\u2113(yij(1\u2212 \u2016xi \u2212 xj\u20162M ))] + \u03bb\n2 \u2016W\u20162F ,\nwhere xi is the instance, and yi is xi\u2019s label, yij is derived from labels yi and yj (i.e., yij = 1 if yi = yj and \u22121 otherwise), \u2016x\u20162M = x\u22a4Mx, and \u2113(z) = log(1 + exp(\u2212z)) is the logit loss. During the optimization process, the call to the gradient oracle corresponds to generate a training pair {(xi, yi), (xj , yj)} randomly. To estimate the value of objective function, we evaluate the average empirical loss on 104 testing pairs, which are also generated randomly. Fig. 2 shows the value of the objective function versus the number of projections P . Again, this result validates that the proposed algorithm log T is able to reduce the number of projections dramatically without hurting the performance."}, {"heading": "6. Conclusion", "text": "In this paper, we study the problem of reducing the number of projections in stochastic optimization by exploring the property of smoothness. When the target function is smooth\nand strongly convex, we propose a novel algorithm that achieves the optimal O(1/T ) rate of convergence by only performing O(log T ) projections.\nAn open question is how to extend our results to stochastic composite optimization (Lan, 2012), where the objective function is a combination of non-smooth and smooth stochastic components. We plan to explore the composite gradient mapping technique, introduced in (Nesterov, 2007), to see if we can achieve an O(1/T ) convergence rate with only O(log T ) projections."}, {"heading": "Appendix A. Proof of Lemma 3", "text": "We need the following lemma that characterizes the property of the extra-gradient descent.\nLemma 10 (Lemma 3.1 in (Nemirovski, 2005)) Let Z be a convex compact set in Euclidean space E with inner product \u3008\u00b7, \u00b7\u3009, let \u2016 \u00b7 \u2016 be a norm on E and \u2016 \u00b7 \u2016\u2217 be its dual norm, and let \u03c9(z) : Z 7\u2192 R be a \u03b1-strongly convex function with respect to \u2016 \u00b7 \u2016. The Bregman distance associated with \u03c9 for points z,w \u2208 Z is defined as\nB\u03c9(z,w) = \u03c9(z)\u2212 \u03c9(w)\u2212 \u3008z\u2212w,\u2207\u03c9(w)\u3009.\nLet U be a convex and closed subset of Z, and let z\u2212 \u2208 Z, let \u03be,\u03b7 \u2208 E, and let \u03b3 > 0. Consider the points\nw = argmin y\u2208U\n{\u3008\u03b3\u03be \u2212\u2207\u03c9(z\u2212),y\u3009 + \u03c9(y)},\nz+ = argmin y\u2208U\n{\u3008\u03b3\u03b7 \u2212\u2207\u03c9(z\u2212),y\u3009 + \u03c9(y)}.\nThen for all z \u2208 U one has\n\u3008w \u2212 z, \u03b3\u03b7\u3009 \u2264 B\u03c9(z, z\u2212)\u2212B\u03c9(z, z+) + \u03b32\n\u03b1 \u2016\u03b7 \u2212 \u03be\u20162\u2217 \u2212\n\u03b1 2 {\u2016w \u2212 z\u2212\u20162 + \u2016z+ \u2212w\u20162}.\nProof [Proof of Lemma 3] We first state the inner loop in Algorithm 1 below.\nfor t = 1 to M do Compute the average gradient at wkt over B k calls to the gradient oracle\ng\u0304kt = 1\nBk\nBk \u2211\ni=1\ng\u0302(wkt , i)\nUpdate\nzkt = \u03a0D\n( wkt \u2212 \u03b7g\u0304kt )\nCompute the average gradient at zkt over B k calls to the gradient oracle\nf\u0304kt = 1\nBk\nBk \u2211\ni=1\ng\u0302(zkt , i)\nUpdate\nwkt+1 = \u03a0D\n( wkt \u2212 \u03b7f\u0304kt )\nend for\nTo simplify the notation, we define\ngkt = \u2207F (wkt ) and fkt = \u2207F (zkt ).\nLet the two norms \u2016 \u00b7 \u2016 and \u2016 \u00b7 \u2016\u2217 in Lemma 10 be the vector \u21132 norm. Each iteration in the inner loop satisfies the conditions in Lemma 10 by doing the mappings below:\nU = Z = E \u2190 D, \u03c9(z) \u2190 1 2 \u2016z\u20162, \u03b1 \u2190 1, \u03b3 \u2190 \u03b7,\nz\u2212 \u2190 wkt , \u03be \u2190 g\u0304kt , \u03b7 \u2190 f\u0304kt , w \u2190 zkt , z+ \u2190 wkt+1, z \u2190 w\u2217.\nFollowing Lemma 10, we have\n\u3008zkt \u2212w\u2217, \u03b7f\u0304kt \u3009\n\u2264\u2016w k t \u2212w\u2217\u20162 2 \u2212 \u2016w k t+1 \u2212w\u2217\u20162 2 + \u03b72\u2016g\u0304kt \u2212 f\u0304kt \u20162 \u2212 1 2 \u2016wkt \u2212 zkt \u20162 \u2264\u2016w k t \u2212w\u2217\u20162\n2 \u2212 \u2016w k t+1 \u2212w\u2217\u20162 2 + 3\u03b72 ( \u2016g\u0304kt \u2212 gkt \u20162 + \u2016f\u0304kt \u2212 fkt \u20162 + \u2016gkt \u2212 fkt \u20162 )\n\u2212 1 2 \u2016wkt \u2212 zkt \u20162\n\u2264\u2016w k t \u2212w\u2217\u20162 2 \u2212 \u2016w k t+1 \u2212w\u2217\u20162 2 + 3\u03b72 ( \u2016g\u0304kt \u2212 gkt \u20162 + \u2016f\u0304kt \u2212 fkt \u20162 )\n+ 3\u03b72\u2016gkt \u2212 fkt \u20162 \u2212 1\n2 \u2016wkt \u2212 zkt \u20162\n\u2264\u2016w k t \u2212w\u2217\u20162 2 \u2212 \u2016w k t+1 \u2212w\u2217\u20162 2 + 3\u03b72 ( \u2016g\u0304kt \u2212 gkt \u20162 + \u2016f\u0304kt \u2212 fkt \u20162 )\n+ 3\u03b72L2\u2016wkt \u2212 zkt \u20162 \u2212 1\n2 \u2016wkt \u2212 zkt \u20162\n\u2264\u2016w k t \u2212w\u2217\u20162 2 \u2212 \u2016w k t+1 \u2212w\u2217\u20162 2 + 3\u03b72 ( \u2016g\u0304kt \u2212 gkt \u20162 + \u2016f\u0304kt \u2212 fkt \u20162 ) ,\n(11)\nwhere in the fifth line we use the smoothness assumption\n\u2016gkt \u2212 fkt \u2016 = \u2016\u2207F (wkt )\u2212\u2207F (zkt )\u2016 \u2264 L\u2016wkt \u2212 zkt \u2016.\nFrom the property of \u03bb-strongly convex function and (11), we obtain\nF (zkt )\u2212 F (w\u2217)\n\u2264\u3008fkt , zkt \u2212w\u2217\u3009 \u2212 \u03bb\n2 \u2016zkt \u2212w\u2217\u20162\n=\u3008f\u0304kt , zkt \u2212w\u2217\u3009+ \u3008fkt \u2212 f\u0304kt , zkt \u2212w\u2217\u3009 \u2212 \u03bb\n2 \u2016zkt \u2212w\u2217\u20162\n\u2264\u2016w k t \u2212w\u2217\u20162 2\u03b7 \u2212 \u2016w k t+1 \u2212w\u2217\u20162 2\u03b7 + 3\u03b7 ( \u2016g\u0304kt \u2212 gkt \u20162 + \u2016f\u0304kt \u2212 fkt \u20162 )\n+ \u3008fkt \u2212 f\u0304kt , zkt \u2212w\u2217\u3009 \u2212 \u03bb\n2 \u2016zkt \u2212w\u2217\u20162.\nSumming up over all t = 1, 2, . . . ,M , we have\nM \u2211\nt=1\nF (zkt )\u2212MF (w\u2217)\n\u2264\u2016w k 1 \u2212w\u2217\u20162 2\u03b7 + 3\u03b7\n(\nM \u2211\nt=1\n\u2016g\u0304kt \u2212 gkt \u20162 + M \u2211\nt=1\n\u2016f\u0304kt \u2212 fkt \u20162 )\n+\nM \u2211\nt=1\n\u3008fkt \u2212 f\u0304kt , zkt \u2212w\u2217\u3009 \u2212 \u03bb\n2\nM \u2211\nt=1\n\u2016zkt \u2212w\u2217\u20162.\nDividing both sides by M and following Jensen\u2019s inequality, we have\nF\n(\n1\nM\nM \u2211\nt=1\nzkt\n)\n\u2212 F (w\u2217)\n\u2264 1 M\nM \u2211\nt=1\nF (zkt )\u2212 F (w\u2217)\n\u2264\u2016w k 1 \u2212w\u2217\u20162 2M\u03b7 + 3\u03b7 M\n(\nM \u2211\nt=1\n\u2016g\u0304kt \u2212 gkt \u20162 + M \u2211\nt=1\n\u2016f\u0304kt \u2212 fkt \u20162 ) +\n1\nM\nM \u2211\nt=1\n\u3008fkt \u2212 f\u0304kt , zkt \u2212w\u2217\u3009 \u2212 \u03bb\n2M\nM \u2211\nt=1\n\u2016zkt \u2212w\u2217\u20162.\n(12)\nwhich gives the first inequality in Lemma 3. Let Ek\u22121[\u00b7] denote the expectation conditioned on all the randomness up to epoch k\u2212 1 and Et\u22121k [\u00b7] denote the expectation conditioned on all the randomness up to the t \u2212 1-th iteration in the k-th epoch. Taking the conditional expectation of (12), we have\nEk\u22121\n[\nF\n(\n1\nM\nM \u2211\nt=1\nzkt\n)]\n\u2212 F (w\u2217)\n\u2264\u2016w k 1 \u2212w\u2217\u20162 2M\u03b7 + 3\u03b7 M\n(\nM \u2211\nt=1\nEk\u22121\n[ \u2016g\u0304kt \u2212 gkt \u20162 ] + M \u2211\nt=1\nEk\u22121\n[ \u2016f\u0304kt \u2212 fkt \u20162 ]\n)\n+ 1\nM\nM \u2211\nt=1\nEk\u22121\n[ \u3008fkt \u2212 f\u0304kt , zkt \u2212w\u2217\u3009 ] ,\n(13)\nwhere we drop the last term, since it is negative. To bound Ek\u22121 [ \u2016g\u0304kt \u2212 gkt \u20162 ] , we have\nEk\u22121\n[ \u2016g\u0304kt \u2212 gkt \u20162 ] = Ek\u22121\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 Bk Bk \u2211\ni=1\ng\u0302(wkt , i)\u2212 gkt\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2 \n=Ek\u22121\n\n\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 Bk Bk \u2211\ni=1\n( g\u0302(wkt , i)\u2212 gkt )\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2 \n= 1\n[Bk]2\nBk \u2211\ni=1\nEk\u22121\n[\n\u2225 \u2225 \u2225g\u0302(wkt , i)\u2212 gkt \u2225 \u2225 \u2225\n2 ]\n+ 1\n[Bk]2 Ek\u22121\n\n\n\u2211\ni 6=j\n\u2329\nEt\u22121k\n[ g\u0302(wkt , i)\u2212 gkt ] ,Et\u22121k [ g\u0302(wkt , j)\u2212 gkt ]\u232a\n\n\n= 1\n[Bk]2\n\n\nBk \u2211\ni=1\nEk\u22121\n[\n\u2225 \u2225 \u2225 g\u0302(wkt , i) \u2212 gkt \u2225 \u2225 \u2225\n2 ]\n  \u2264 G 2\nBk ,\n(14)\nwhere we make use of the facts g\u0302(wkt , i) and g\u0302(w k t , j) are independent when i 6= j, and\nEt\u22121k\n[ g\u0302(wkt , i) \u2212 gkt ] = 0, Et\u22121k [ \u2016g\u0302(wkt , i)\u2212 gkt \u20162 ] \u2264 Et\u22121k [ \u2016g\u0302(wkt , i)\u20162 ] \u2264 G2, \u2200i = 1, . . . , Bk.\nSimilarly, we also have\nEk\u22121\n[ \u2016f\u0304kt \u2212 fkt \u20162 ] \u2264 G 2\nBk . (15)\nNotice that f\u0304kt is an unbiased estimate of f k t , thus\nEk\u22121\n[ \u3008fkt \u2212 f\u0304kt , zkt \u2212w\u2217\u3009 ] = Ek\u22121 [ \u3008Et\u22121k [ fkt \u2212 f\u0304kt ] , zkt \u2212w\u2217\u3009 ] = 0. (16)\nSubstituting (14), (15), and (16) into (13), we get the second inequality in Lemma 3."}, {"heading": "Appendix B. Proof of Lemma 6", "text": "Proof Recall that g\u0304kt = 1 Bk\n\u2211Bk\ni=1 g\u0302(w k t , i), thus\n\u2016g\u0304kt \u2212 gkt \u2016 =\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 1 Bk Bk \u2211\ni=1\ng\u0302(wkt , i)\u2212 gkt\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2225 .\nSince \u2016g\u0302(wkt , i)\u2016 \u2264 G, and E[g\u0302(wkt , i)] = gkt , we have with a probability at least 1\u2212 \u03b4\n\u2016g\u0304kt \u2212 gkt \u2016 \u2264 4G\u221a Bk log 2 \u03b4 .\nWe obtain (8) by the union bound and setting \u03b4\u0303/2 = M\u03b4. The inequality in (9) can be proved in the same way."}, {"heading": "Appendix C. Proof of Lemma 7", "text": "We first state the Berstein inequality for martingales (Cesa-Bianchi and Lugosi, 2006), which is used in the proof below.\nTheorem 3 (Bernstein\u2019s inequality for martingales). Let X1, . . . ,Xn be a bounded martingale difference sequence with respect to the filtration F = (Fi)1\u2264i\u2264n and with |Xi| \u2264 K. Let\nSi =\ni \u2211\nj=1\nXj\nbe the associated martingale. Denote the sum of the conditional variances by\n\u03a32n =\nn \u2211\nt=1\nE [ X2t |Ft\u22121 ] .\nThen for all constants t, \u03bd > 0,\nPr\n[\nmax i=1,...,n\nSi > t and \u03a3 2 n \u2264 \u03bd\n] \u2264 exp ( \u2212 t 2\n2(\u03bd +Kt/3)\n)\n,\nand therefore,\nPr\n[\nmax i=1,...,n\nSi > \u221a 2\u03bdt+ 2\n3 Kt and \u03a32n \u2264 \u03bd\n]\n\u2264 e\u2212t.\nTo simplify the notation, we define\nA =\nM \u2211\ni=1\n\u2016zkt \u2212w\u2217\u20162 \u2264 4MG2\n\u03bb2 ,\nC = 4G\u221a Bk log 8M \u03b4\u0303 .\nIn the analysis below, we consider two different scenarios, i.e., A \u2264 \u03b7G2/[\u03bbBk] and A > \u03b7G2/[\u03bbBk].\nC.1. A \u2264 \u03b7G2/[\u03bbBk] On event E1, we can bound\nZkt \u2264 \u2016fkt \u2212 f\u0304kt \u2016\u2016zkt \u2212w\u2217\u2016 \u2264 \u03b7\n4 \u2016fkt \u2212 f\u0304kt \u20162 +\n1 \u03b7 \u2016zkt \u2212w\u2217\u20162 \u2264 \u03b7 4 C2 + 1 \u03b7 \u2016zkt \u2212w\u2217\u20162.\nSumming up over all t = 1, 2, . . . ,M ,\nM \u2211\nt=1\nZkt \u2264 \u03b7MC2\n4 +\n1\n\u03b7\nM \u2211\nt=1\n\u2016zkt \u2212w\u2217\u20162 \u2264 \u03b7MC2\n4 +\nG2\n\u03bbBk . (17)\nC.2. A > \u03b7G2/[\u03bbBk]\nSimilar to the above proof, on event E1, we bound\n|Zkt | \u2264 \u2016fkt \u2212 f\u0304kt \u2016\u2016zkt \u2212w\u2217\u2016 \u2264 1\n\u03b8 \u2016fkt \u2212 f\u0304kt \u20162 +\n\u03b8 4 \u2016zkt \u2212w\u2217\u20162 \u2264\nC2\n\u03b8 +\n\u03b8A\n4 ,\nwhere \u03b8 can be any nonnegative real number. Denote the sum of conditional variances by\n\u03a32M =\nM \u2211\nt=1\nEt\u22121k\n[ [Zkt ] 2 ] \u2264 C2 M \u2211\nt=1\n\u2016zt \u2212w\u2217\u20162 = C2A,\nwhere Et\u22121k [\u00b7] denote the expectation conditioned on all the randomness up to the t\u2212 1-th iteration in the k-th epoch.\nNotice that A in the upper bound for |Zkt | and \u03a32M is a random variable, thus we cannot directly apply Theorem 3. To address this challenge, we make use of the peeling technique described in (Bartlett et al., 2005), and have\nPr\n(\nM \u2211\nt=1\nZkt \u2265 2 \u221a C2A\u03c4 + 4\n3\n(\nC2\n\u03b8 +\n\u03b8A\n4\n)\n\u03c4\n)\n=Pr\n(\nM \u2211\nt=1\nZkt \u2265 2 \u221a C2A\u03c4 + 4\n3\n(\nC2\n\u03b8 +\n\u03b8A\n4\n)\n\u03c4, \u03b7G2 \u03bbBk < A \u2264 4MG 2 \u03bb2\n)\n=Pr\n(\nM \u2211\nt=1\nZkt \u2265 2 \u221a C2A\u03c4 + 4\n3\n(\nC2\n\u03b8 +\n\u03b8A\n4\n)\n\u03c4,\nmax t\n|Zkt | \u2264 C2\n\u03b8 +\n\u03b8A\n4 ,\u03a32M \u2264 C2A,\n\u03b7G2\n\u03bbBk < A \u2264 4MG\n2\n\u03bb2\n)\n\u2264 n \u2211\ni=1\nPr\n(\nM \u2211\nt=1\nZkt \u2265 2 \u221a C2A\u03c4 + 4\n3\n(\nC2\n\u03b8 +\n\u03b8A\n4\n)\n\u03c4,\nmax t\n|Zkt | \u2264 C2\n\u03b8 +\n\u03b8A\n4 ,\u03a32M \u2264 C2A,\n\u03b7G2\n\u03bbBk 2i\u22121 < A \u2264 \u03b7G\n2 \u03bbBk 2i )\n\u2264 n \u2211\ni=1\nPr\n(\nM \u2211\nt=1\nZkt \u2265 2 \u221a ( C2 \u03b7G2\n\u03bbBk 2i\u22121\n)\n\u03c4 + 4\n3\n(\nC2\n\u03b8 +\n\u03b8\n4\n\u03b7G2 \u03bbBk 2i\u22121\n)\n\u03c4,\nmax t\n|Zkt | \u2264 C2\n\u03b8 +\n\u03b8\n4\n\u03b7G2 \u03bbBk 2i,\u03a32M \u2264 C2 \u03b7G2 \u03bbBk 2i )\n\u2264 n \u2211\ni=1\nPr\n(\nM \u2211\nt=1\nZkt \u2265 \u221a 2 ( C2 \u03b7G2 \u03bbBk 2i ) \u03c4 + 2 3 ( C2 \u03b8 + \u03b8 4 \u03b7G2 \u03bbBk 2i ) \u03c4,\nmax t\n|Zkt | \u2264 C2\n\u03b8 +\n\u03b8\n4\n\u03b7G2 \u03bbBk 2i,\u03a32M \u2264 C2 \u03b7G2 \u03bbBk 2i )\n\u2264ne\u2212\u03c4 , where\nn =\n\u2308\nlog2 4MBk\n\u03b7\u03bb\n\u2309\n,\nand the last step follows the Bernstein inequality for martingales in Theorem 3. Setting\n\u03b8 = 3\u03bb\n4\u03c4 , and \u03c4 = log\n4n\n\u03b4\u0303 ,\nwith a probability at least 1\u2212 \u03b4\u0303/4 we have\nM \u2211\nt=1\nZkt\n\u22642 \u221a C2A\u03c4 + 4\n3\n(\nC2\n\u03b8 +\n\u03b8A\n4\n) \u03c4 = 2 \u221a C2A\u03c4 + 16C2\n9\u03bb \u03c42 +\n\u03bbA\n4\n\u2264 4 \u03bb C2\u03c4 + \u03bbA 4 +\n16C2\n9\u03bb \u03c42 +\n\u03bbA\n4 =\n4C2\n\u03bb\n(\nlog 4n\n\u03b4\u0303 +\n4 9 log2 4n\n\u03b4\u0303\n)\n+ \u03bbA\n2 .\n(18)\nWe complete the proof by combining (17) and (18)."}], "references": [{"title": "Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization", "author": ["Alekh Agarwal", "Peter L. Bartlett", "Pradeep Ravikumar", "Martin J. Wainwright"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "Local rademacher complexities", "author": ["Peter L. Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "The Annals of Statistics,", "citeRegEx": "Bartlett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2005}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Optimal regularized dual averaging methods for stochastic optimization", "author": ["Xi Chen", "Qihang Lin", "Javier Pena"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Coresets, sparse greedy approximation, and the frank-wolfe algorithm", "author": ["Kenneth L. Clarkson"], "venue": "ACM Transactions on Algorithms,", "citeRegEx": "Clarkson.,? \\Q2010\\E", "shortCiteRegEx": "Clarkson.", "year": 2010}, {"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Cotter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cotter et al\\.", "year": 2011}, {"title": "Optimal distributed online prediction", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Dekel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2011}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dekel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2012}, {"title": "An algorithm for quadratic programming", "author": ["Marguerite Frank", "Philip Wolfe"], "venue": "Naval Research Logistics Quarterly,", "citeRegEx": "Frank and Wolfe.,? \\Q1956\\E", "shortCiteRegEx": "Frank and Wolfe.", "year": 1956}, {"title": "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework", "author": ["Saeed Ghadimi", "Guanghui Lan"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Ghadimi and Lan.,? \\Q2012\\E", "shortCiteRegEx": "Ghadimi and Lan.", "year": 2012}, {"title": "Sparse approximate solutions to semidefinite programs", "author": ["Elad Hazan"], "venue": "In Proceedings of the 8th Latin American conference on Theoretical informatics,", "citeRegEx": "Hazan.,? \\Q2008\\E", "shortCiteRegEx": "Hazan.", "year": 2008}, {"title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In Proceedings of the 24th Annual Conference on Learning Theory,", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Projection-free online learning", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Hazan and Kale.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2012}, {"title": "Accelerated gradient methods for stochastic optimization and online learning", "author": ["Chonghai Hu", "James Kwok", "Weike Pan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "author": ["Martin Jaggi"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Jaggi.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi.", "year": 2013}, {"title": "Regularized distance metric learning: Theory and algorithm", "author": ["Rong Jin", "Shijun Wang", "Yang Zhou"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2009}, {"title": "Primal-dual subgradient methods for minimizing uniformly convex functions", "author": ["Anatoli Juditsky", "Yuri Nesterov"], "venue": "Technical report,", "citeRegEx": "Juditsky and Nesterov.,? \\Q2010\\E", "shortCiteRegEx": "Juditsky and Nesterov.", "year": 2010}, {"title": "Solving variational inequalities with stochastic mirror-prox algorithm", "author": ["Anatoli Juditsky", "Arkadi Nemirovski", "Claire Tauvel"], "venue": "Stochastic Systems,", "citeRegEx": "Juditsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Juditsky et al\\.", "year": 2011}, {"title": "Block-coordinate frank-wolfe optimization for structural svm", "author": ["Simon Lacoste-Julien", "Martin Jaggi", "Mark Schmidt", "Patrick Pletscher"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2013}, {"title": "An optimal method for stochastic composite optimization", "author": ["Guanghui Lan"], "venue": "Mathematical Programming,", "citeRegEx": "Lan.,? \\Q2012\\E", "shortCiteRegEx": "Lan.", "year": 2012}, {"title": "Constrained minimization methods", "author": ["Evgenij S Levitin", "Boris T Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "Levitin and Polyak.,? \\Q1966\\E", "shortCiteRegEx": "Levitin and Polyak.", "year": 1966}, {"title": "Stochastic gradient descent with only one projection", "author": ["Mehrdad Mahdavi", "Tianbao Yang", "Rong Jin", "Shenghuo Zhu", "Jinfeng Yi"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mahdavi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mahdavi et al\\.", "year": 2012}, {"title": "Problem complexity and method efficiency in optimization", "author": ["A. Nemirovski", "D.B. Yudin"], "venue": null, "citeRegEx": "Nemirovski and Yudin.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovski and Yudin.", "year": 1983}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["Arkadi Nemirovski"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Nemirovski.,? \\Q2005\\E", "shortCiteRegEx": "Nemirovski.", "year": 2005}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yu. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov.,? \\Q2005\\E", "shortCiteRegEx": "Nesterov.", "year": 2005}, {"title": "Introductory lectures on convex optimization: a basic course, volume 87 of Applied optimization", "author": ["Yurii Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Yurii Nesterov"], "venue": "Core discussion papers,", "citeRegEx": "Nesterov.,? \\Q2007\\E", "shortCiteRegEx": "Nesterov.", "year": 2007}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["Nicolas Le Roux", "Pierre-Antoine Manzagol", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Roux et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2008}, {"title": "Stochastic convex optimization", "author": ["Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Pegasos: primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Geometry on probability spaces", "author": ["Steve Smale", "Ding-Xuan Zhou"], "venue": "Constructive Approximation,", "citeRegEx": "Smale and Zhou.,? \\Q2009\\E", "shortCiteRegEx": "Smale and Zhou.", "year": 2009}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "Zhang.,? \\Q2004\\E", "shortCiteRegEx": "Zhang.", "year": 2004}], "referenceMentions": [{"referenceID": 33, "context": "where (x, y) is an instance-label pair, l is a convex loss function that measures the prediction error, and the expectation is taken oven the unknown joint distribution of (x, y) (Zhang, 2004; Shalev-Shwartz et al., 2009; Hu et al., 2009).", "startOffset": 179, "endOffset": 238}, {"referenceID": 30, "context": "where (x, y) is an instance-label pair, l is a convex loss function that measures the prediction error, and the expectation is taken oven the unknown joint distribution of (x, y) (Zhang, 2004; Shalev-Shwartz et al., 2009; Hu et al., 2009).", "startOffset": 179, "endOffset": 238}, {"referenceID": 13, "context": "where (x, y) is an instance-label pair, l is a convex loss function that measures the prediction error, and the expectation is taken oven the unknown joint distribution of (x, y) (Zhang, 2004; Shalev-Shwartz et al., 2009; Hu et al., 2009).", "startOffset": 179, "endOffset": 238}, {"referenceID": 22, "context": "For general Lipschitz continuous convex functions, stochastic gradient descent exhibits the unimprovable O(1/ \u221a T ) rate of convergence (Nemirovski and Yudin, 1983; Nemirovski et al., 2009).", "startOffset": 136, "endOffset": 189}, {"referenceID": 23, "context": "For general Lipschitz continuous convex functions, stochastic gradient descent exhibits the unimprovable O(1/ \u221a T ) rate of convergence (Nemirovski and Yudin, 1983; Nemirovski et al., 2009).", "startOffset": 136, "endOffset": 189}, {"referenceID": 16, "context": "For strongly-convex functions, the algorithms proposed in very recent works (Juditsky and Nesterov, 2010; Hazan and Kale, 2011; Rakhlin et al., 2012; Chen et al., 2012) achieve the optimal O(1/T ) rate (Agarwal et al.", "startOffset": 76, "endOffset": 168}, {"referenceID": 11, "context": "For strongly-convex functions, the algorithms proposed in very recent works (Juditsky and Nesterov, 2010; Hazan and Kale, 2011; Rakhlin et al., 2012; Chen et al., 2012) achieve the optimal O(1/T ) rate (Agarwal et al.", "startOffset": 76, "endOffset": 168}, {"referenceID": 28, "context": "For strongly-convex functions, the algorithms proposed in very recent works (Juditsky and Nesterov, 2010; Hazan and Kale, 2011; Rakhlin et al., 2012; Chen et al., 2012) achieve the optimal O(1/T ) rate (Agarwal et al.", "startOffset": 76, "endOffset": 168}, {"referenceID": 3, "context": "For strongly-convex functions, the algorithms proposed in very recent works (Juditsky and Nesterov, 2010; Hazan and Kale, 2011; Rakhlin et al., 2012; Chen et al., 2012) achieve the optimal O(1/T ) rate (Agarwal et al.", "startOffset": 76, "endOffset": 168}, {"referenceID": 0, "context": ", 2012) achieve the optimal O(1/T ) rate (Agarwal et al., 2012).", "startOffset": 41, "endOffset": 63}, {"referenceID": 29, "context": "By combining mini-batches (Roux et al., 2008) with the accelerated stochastic approximation (Lan, 2012), we can achieve the optimal O(1/ \u221a T ) rate by performing O(T 1/4) projections (Cotter et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 19, "context": ", 2008) with the accelerated stochastic approximation (Lan, 2012), we can achieve the optimal O(1/ \u221a T ) rate by performing O(T 1/4) projections (Cotter et al.", "startOffset": 54, "endOffset": 65}, {"referenceID": 5, "context": ", 2008) with the accelerated stochastic approximation (Lan, 2012), we can achieve the optimal O(1/ \u221a T ) rate by performing O(T 1/4) projections (Cotter et al., 2011).", "startOffset": 145, "endOffset": 166}, {"referenceID": 29, "context": "Mini-batch based algorithms Instead of updating the solution after each call to the gradient oracle, mini-batch based algorithms use the average gradient over multiple calls to update the solution (Roux et al., 2008; Shalev-Shwartz et al., 2011; Dekel et al., 2011).", "startOffset": 197, "endOffset": 265}, {"referenceID": 31, "context": "Mini-batch based algorithms Instead of updating the solution after each call to the gradient oracle, mini-batch based algorithms use the average gradient over multiple calls to update the solution (Roux et al., 2008; Shalev-Shwartz et al., 2011; Dekel et al., 2011).", "startOffset": 197, "endOffset": 265}, {"referenceID": 6, "context": "Mini-batch based algorithms Instead of updating the solution after each call to the gradient oracle, mini-batch based algorithms use the average gradient over multiple calls to update the solution (Roux et al., 2008; Shalev-Shwartz et al., 2011; Dekel et al., 2011).", "startOffset": 197, "endOffset": 265}, {"referenceID": 19, "context": "When the objective function is smooth and convex, combining mini-batches with the accelerated stochastic approximation (Lan, 2012) leads to O ( B2 T 2 + 1 \u221a T )", "startOffset": 119, "endOffset": 130}, {"referenceID": 5, "context": "rate of convergence (Cotter et al., 2011).", "startOffset": 20, "endOffset": 41}, {"referenceID": 13, "context": "When the target function is smooth and strongly convex, we can apply mini-batches to the optimal algorithms for strongly convex functions (Hu et al., 2009; Ghadimi and Lan, 2012), leading to O ( B2 T 2 + 1 T )", "startOffset": 138, "endOffset": 178}, {"referenceID": 9, "context": "When the target function is smooth and strongly convex, we can apply mini-batches to the optimal algorithms for strongly convex functions (Hu et al., 2009; Ghadimi and Lan, 2012), leading to O ( B2 T 2 + 1 T )", "startOffset": 138, "endOffset": 178}, {"referenceID": 7, "context": "rate of convergence (Dekel et al., 2012).", "startOffset": 20, "endOffset": 40}, {"referenceID": 8, "context": "Projection free algorithms Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al.", "startOffset": 80, "endOffset": 103}, {"referenceID": 20, "context": "Projection free algorithms Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al.", "startOffset": 135, "endOffset": 161}, {"referenceID": 10, "context": "Projection free algorithms Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al., 2013).", "startOffset": 218, "endOffset": 276}, {"referenceID": 4, "context": "Projection free algorithms Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al., 2013).", "startOffset": 218, "endOffset": 276}, {"referenceID": 18, "context": "Projection free algorithms Due to the low iteration cost, Frank-Wolfe algorithm (Frank and Wolfe, 1956) or conditional gradient method (Levitin and Polyak, 1966) has seen a recent surge of interest in machine learning (Hazan, 2008; Clarkson, 2010; Lacoste-Julien et al., 2013).", "startOffset": 218, "endOffset": 276}, {"referenceID": 14, "context": "For many domains of interest, including the positive semidefinite cone and the trace norm ball, the constrained linear problem can be solved more efficiently than a projection problem (Jaggi, 2013), making this kind of methods attractive for large-scale optimization.", "startOffset": 184, "endOffset": 197}, {"referenceID": 12, "context": "In a recent work (Hazan and Kale, 2012), an online variant of the Frank-Wolfe algorithm is proposed.", "startOffset": 17, "endOffset": 39}, {"referenceID": 21, "context": "Another related work is the stochastic gradient descent with only one projection (Mahdavi et al., 2012).", "startOffset": 81, "endOffset": 103}, {"referenceID": 11, "context": "Following (Hazan and Kale, 2011), we make the following assumptions about the gradient oracle.", "startOffset": 10, "endOffset": 32}, {"referenceID": 19, "context": "We note that this assumption may be relaxed by assuming the orlicz norm of \u011d(w) to be bounded (Lan, 2012), i.", "startOffset": 94, "endOffset": 105}, {"referenceID": 11, "context": "Using the strongly convexity of F (w), we have (Hazan and Kale, 2011) \u03bb 2 \u2016w \u2212w\u2217\u2016 \u2264 F (w)\u2212 F (w\u2217) \u2264 2G2 \u03bb ,\u2200 w \u2208 D.", "startOffset": 47, "endOffset": 69}, {"referenceID": 11, "context": "Similar to the epoch gradient descent algorithm (Hazan and Kale, 2011), the proposed algorithm consists of two layers of loops.", "startOffset": 48, "endOffset": 70}, {"referenceID": 11, "context": "Similar to (Hazan and Kale, 2011), the number of calls to the gradient oracle made by Algorithm 1 increases exponentially over the epoches, a key that allows us to achieve the optimal O(1/T ) convergence rate for strongly convex functions.", "startOffset": 11, "endOffset": 33}, {"referenceID": 28, "context": "We note that other techniques, such as the \u03b1-suffix averaging (Rakhlin et al., 2012), can also be used as an alternative.", "startOffset": 62, "endOffset": 84}, {"referenceID": 6, "context": "In the inner (for) loop of each epoch, we combine the idea of mini-batches (Dekel et al., 2011) with extra-gradient descent (Nemirovski, 2005; Juditsky et al.", "startOffset": 75, "endOffset": 95}, {"referenceID": 24, "context": ", 2011) with extra-gradient descent (Nemirovski, 2005; Juditsky et al., 2011).", "startOffset": 36, "endOffset": 77}, {"referenceID": 17, "context": ", 2011) with extra-gradient descent (Nemirovski, 2005; Juditsky et al., 2011).", "startOffset": 36, "endOffset": 77}, {"referenceID": 11, "context": "This is in contrast to the epoch gradient descent algorithm (Hazan and Kale, 2011) that needs to shrink the domain size in order to obtain the desirable high probability bound, which could potentially lead to an additional computational cost in performing projection.", "startOffset": 60, "endOffset": 82}, {"referenceID": 1, "context": "We remove the shrinking step by effectively exploring the peeling technique (Bartlett et al., 2005).", "startOffset": 76, "endOffset": 99}, {"referenceID": 11, "context": "Proof of Theorem 1 Since we make use of the the multi-stage learning strategy, the proof provided below is similar to the proof in (Hazan and Kale, 2011).", "startOffset": 131, "endOffset": 153}, {"referenceID": 32, "context": "To bound the variances given in (5), we need the following norm concentration inequality in Hilbert Space (Smale and Zhou, 2009).", "startOffset": 106, "endOffset": 128}, {"referenceID": 2, "context": "In order to bound the summation of Zk t in (6), we make use of the Berstein inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique described in (Bartlett et al.", "startOffset": 103, "endOffset": 134}, {"referenceID": 1, "context": "In order to bound the summation of Zk t in (6), we make use of the Berstein inequality for martingales (Cesa-Bianchi and Lugosi, 2006) and the peeling technique described in (Bartlett et al., 2005), leading to the following Lemma.", "startOffset": 174, "endOffset": 197}, {"referenceID": 11, "context": "EP GD: the epoch gradient descent developed in (Hazan and Kale, 2011), which is also optimal for SOS2C but needs O(T ) projections; 3.", "startOffset": 47, "endOffset": 69}, {"referenceID": 31, "context": "SGD: the stochastic gradient descent with step size \u03b7t = 1/(\u03bbt) (Shalev-Shwartz et al., 2011), which achieves O(log T/T ) rate of convergence for general SOS2C and needs O(T ) projections.", "startOffset": 64, "endOffset": 93}, {"referenceID": 28, "context": "We first consider the a simple stochastic optimization problem adapted from (Rakhlin et al., 2012), which is both smooth and strongly convex.", "startOffset": 76, "endOffset": 98}, {"referenceID": 28, "context": "Since the gradient of W\u2217 is 0 5\u00d75, it can be shown that SGD also achieves the optimal O(1/T ) rate of convergence on this problem (Rakhlin et al., 2012).", "startOffset": 130, "endOffset": 152}, {"referenceID": 15, "context": "In the second experiment, we apply our algorithm to the regularized distance metric learning (Jin et al., 2009).", "startOffset": 93, "endOffset": 111}], "year": 2013, "abstractText": "Traditional algorithms for stochastic optimization require projecting the solution at each iteration into a given domain to ensure its feasibility. When facing complex domains, such as positive semi-definite cones, the projection operation can be expensive, leading to a high computational cost per iteration. In this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. The proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini-batch, extra-gradient, and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. We show, both in expectation and with a high probability, that when the objective function is both smooth and strongly convex, the proposed algorithm achieves the optimal O(1/T ) rate of convergence with only O(log T ) projections. Our empirical study verifies the theoretical result.", "creator": "LaTeX with hyperref package"}}}