{"id": "1304.5758", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2013", "title": "Prior-free and prior-dependent regret bounds for Thompson Sampling", "abstract": "we consider the stochastic multi - goal armed bandit problem with a prior distribution on the reward distributions. we later show that for any prior distribution, the thompson sampling strategy achieves a bayesian regret bounded from looking above entirely by $ 14 \\ sqrt { n k } $. this smoothing result is unimprovable in the sense that there exists a prior distribution such such that hence any algorithm generally has a bayesian regret bounded solely from below by $ 1 / 20 \\ sqrt { n k } $.", "histories": [["v1", "Sun, 21 Apr 2013 15:58:56 GMT  (6kb)", "https://arxiv.org/abs/1304.5758v1", null], ["v2", "Thu, 3 Oct 2013 00:48:53 GMT  (26kb)", "http://arxiv.org/abs/1304.5758v2", "A previous version appeared under the title 'A note on the Bayesian regret of Thompson Sampling with an arbitrary prior'"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["s\u00e9bastien bubeck", "che-yu liu"], "accepted": true, "id": "1304.5758"}, "pdf": {"name": "1304.5758.pdf", "metadata": {"source": "CRF", "title": "Prior-free and prior-dependent regret bounds for Thompson Sampling", "authors": ["S\u00e9bastien Bubeck", "Che-Yu Liu"], "emails": ["sbubeck@princeton.edu,", "cheliu@princeton.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 4.\n57 58\nv2 [\nst at\n.M L\n\u221a nK. This result is unimprovable in the sense that there exists a prior distribution\nsuch that any algorithm has a Bayesian regret bounded from below by 120 \u221a nK. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.\n1 Introduction\nIn this paper we are interested in the Bayesian multi-armed bandit problem which can be described as follows. Let \u03c00 be a known distribution over some set \u0398, and let \u03b8 be a random variable distributed according to \u03c00. For i \u2208 [K], let (Xi,s)s\u22651 be identically distributed random variables taking values in [0, 1] and which are independent conditionally on \u03b8. Denote \u00b5i(\u03b8) := E(Xi,1|\u03b8). Consider now an agent facing K actions (or arms). At each time step t = 1, . . . n, the agent pulls an arm It \u2208 [K]. The agent receives the reward Xi,s when he pulls arm i for the sth time. The arm selection is based only on past observed rewards and potentially on an external source of randomness. More formally, let (Us)s\u22651 be an i.i.d. sequence of random variables uniformly distributed on [0, 1], and let Ti(s) = \u2211s t=1 1It=i, then It is a random variable measurable with respect to \u03c3(I1, X1,1, . . . , It\u22121, XIt\u22121,TIt\u22121(t\u22121), Ut). We measure the performance of the agent through the\nBayesian regret defined as\nBRn = E n\u2211\nt=1\n( max i\u2208[K] \u00b5i(\u03b8)\u2212 \u00b5It(\u03b8) ) ,\nwhere the expectation is taken with respect to the parameter \u03b8, the rewards (Xi,s)s\u22651, and the external source of randomness (Us)s\u22651. We will also be interested in the individual regret Rn(\u03b8) which is defined similarly except that \u03b8 is fixed (instead of being integrated over \u03c00). When it is clear from the context we drop the dependency on \u03b8 in the various quantities defined above.\nGiven a prior \u03c00 the problem of finding an optimal strategy to minimize the Bayesian regret BRn is a well defined optimization problem and as such it is merely a computational problem. On the other hand the point of view initially developed in Robbins [1952] leads to a learning problem. In this latter view the agent\u2019s strategy must have a low regret Rn(\u03b8) for any \u03b8 \u2208 \u0398. Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior distribution takes a product form an optimal strategy is given by the Gittins indices (which are relatively easy to compute). The product assumption on the prior means that the reward processes (Xi,s)s\u22651 are independent across arms. In the present paper we are precisely interested in the situations where this assumption is not satisfied. Indeed we believe that one of the strength of the Bayesian setting is that one can incorporate prior knowledge on the arms in very transparent way. A prototypical example that we shall consider later on in this paper is when one knows the distributions of the arms up to a permutation, in which case the reward processes are strongly dependent.\nIn general without the product assumption on the prior it seems hopeless (from a computational perspective) to look for the optimal Bayesian strategy. Thus, despite being in a Bayesian setting, it makes sense to view it as a learning problem and to evaluate the agent\u2019s performance through its Bayesian regret. In this paper we are particularly interested in studying the Thompson Sampling strategy which was proposed in the very first paper on the multi-armed bandit problem Thompson [1933]. This strategy can be described very succinctly: let \u03c0t be the posterior distribution on \u03b8 given the history Ht = (I1, X1,1, . . . , It\u22121, XIt\u22121,TIt\u22121 (t\u22121)) of the algorithm up to the beginning of round t. Then Thompson Sampling first draws a parameter \u03b8(t) from \u03c0t (independently from the past given \u03c0t) and it pulls It \u2208 argmaxi\u2208[K] \u00b5i(\u03b8(t)).\nRecently there has been a surge of interest in this simple policy, mainly because of its flexibility to incorporate prior knowledge on the arms, see for example Chapelle and Li [2011]. For a long time the theoretical properties of Thompson Sampling remained elusive. The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al. [2012], Agrawal and Goyal [2012b]. However as we pointed out above here we are interested in proving regret bounds for the more realistic scenario where one runs Thompson Sampling with a hand-tuned prior distribution, possibly very different from a Beta prior. The first result in this spirit was obtained very recently by Russo and Roy [2013] who showed that for any prior distribution \u03c00 Thompson Sampling always satisfies BRn \u2264 5 \u221a nK logn. A similar\nbound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1. Our first contribution is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009].\nOur second contribution is to show that Thompson Sampling can take advantage of the properties of some non-trivial priors to attain much better regret guarantees. More precisely in Section 2 and 3 we consider the setting of Bubeck et al. [2013] (which we call the BPR setting) where \u00b5\u2217 and \u03b5 > 0 are known values such that for any \u03b8 \u2208 \u0398, first there is a unique best arm {i\u2217(\u03b8)} = argmaxi\u2208[K] \u00b5i(\u03b8), and furthermore\n\u00b5i\u2217(\u03b8)(\u03b8) = \u00b5 \u2217, and \u2206i(\u03b8) := \u00b5i\u2217(\u03b8)(\u03b8)\u2212 \u00b5i(\u03b8) \u2265 \u03b5, \u2200i 6= i\u2217(\u03b8).\nIn other words the value of the best arm is known as well as a non-trivial lower bound on the gap between the values of the best and second best arms. For this problem a new algorithm was proposed in Bubeck et al. [2013] (which we call the BPR policy), and it was shown that the BPR policy satisfies\nRn(\u03b8) = O\n  \u2211\ni 6=i\u2217(\u03b8)\nlog(\u2206i(\u03b8)/\u03b5)\n\u2206i(\u03b8) log log(1/\u03b5)\n  , \u2200\u03b8 \u2208 \u0398, \u2200n \u2265 1.\nThus the BPR policy attains a regret uniformly bounded over time in the BPR setting, a feature that standard bandit algorithms such as UCB of Auer et al. [2002] cannot achieve. It is natural to view the assumptions of the BPR setting as a prior over the reward distributions and to ask what regret guarantees attains Thompson Sampling in that situation. More precisely we consider Thompson Sampling with Gaussian reward distributions and uniform prior over the possible range of parameters. We then prove individual regret bounds for any sub-Gaussian distributions (similarly to Bubeck et al. [2013]). We obtain that Thompson Sampling uses optimally the prior information in the sense that it also attains uniformly bounded over time regret. Furthermore as an added bonus we remove the extraneous log-log factor of the BPR policy\u2019s regret bound.\nThe results presented in Section 3 and 4 can be viewed as a first step towards a better understanding of prior-dependent regret bounds for Thompson Sampling. Generalizing these results to arbitrary priors is a challenging open problem which is beyond the scope of our current techniques.\n2 Optimal prior-free regret bound for Thompson Sampling\nIn this section we prove the following result.\nTheorem 1 For any prior distribution \u03c00 over reward distributions in [0, 1], Thompson Sampling satisfies\nBRn \u2264 14 \u221a nK.\nRemark that the above result is unimprovable in the sense that there exist prior distributions \u03c00 such that for any algorithm one has Rn \u2265 120 \u221a nK (see e.g. [Theorem 3.5, Bubeck and Cesa-Bianchi\n1Note however that the result of Agrawal and Goyal [2012b] applies to the individual regret Rn(\u03b8) while the result of Russo and Roy [2013] only applies to the integrated Bayesian regret BRn.\n[2012]]). This theorem also implies an optimal rate of identification for the best arm, see Bubeck et al. [2009] for more details on this.\nProof We decompose the proof into three steps. We denote i\u2217(\u03b8) \u2208 argmaxi\u2208[K] \u00b5i(\u03b8), in particular one has It = i\u2217(\u03b8(t)).\nStep 1: rewriting of the Bayesian regret in terms of upper confidence bounds. This step is given by [Proposition 1, Russo and Roy [2013]] which we reprove for the sake of completeness. Let Bi,t be a random variable measurable with respect to \u03c3(Ht). Note that by definition \u03b8(t) and \u03b8 are identically distributed conditionally on Ht. This implies by the tower rule:\nEBi\u2217(\u03b8),t = EBi\u2217(\u03b8(t)),t = EBIt,t.\nThus we obtain:\nE ( \u00b5i\u2217(\u03b8)(\u03b8)\u2212 \u00b5It(\u03b8) ) = E ( \u00b5i\u2217(\u03b8)(\u03b8)\u2212 Bi\u2217(\u03b8),t ) + E (BIt,t \u2212 \u00b5It(\u03b8)) .\nInspired by the MOSS strategy of Audibert and Bubeck [2009] we will now take\nBi,t = \u00b5\u0302i,Ti(t\u22121) +\n\u221a\u221a\u221a\u221a log+ ( n KTi(t\u22121) )\nTi(t\u2212 1) ,\nwhere \u00b5\u0302i,s = 1s \u2211s t=1Xi,t, and log+(x) = log(x)1x\u22651. In the following we denote \u03b40 = 2 \u221a K n\n. From now on we work conditionally on \u03b8 and thus we drop all the dependency on \u03b8.\nStep 2: control of E ( \u00b5i\u2217(\u03b8)(\u03b8)\u2212Bi\u2217(\u03b8),t|\u03b8 ) . By a simple integration of the deviations one has\nE (\u00b5i\u2217 \u2212Bi\u2217,t) \u2264 \u03b40 + \u222b 1\n\u03b40\nP(\u00b5i\u2217 \u2212Bi\u2217,t \u2265 u)du.\nNext we extract the following inequality from Audibert and Bubeck [2010] (see p2683\u20132684), for any i \u2208 [K],\nP(\u00b5i \u2212Bi,t \u2265 u) \u2264 4K\nnu2 log\n(\u221a n\nK u\n) +\n1\nnu2/K \u2212 1 .\nNow an elementary integration gives\n\u222b 1\n\u03b40\n4K nu2 log\n(\u221a n\nK u\n) du = [ \u22124K\nnu log\n( e \u221a n\nK u\n)]1\n\u03b40\n\u2264 4K n\u03b40 log\n( e \u221a n\nK \u03b40\n) = 2(1+log 2) \u221a K\nn ,\nand\n\u222b 1\n\u03b40\n1 nu2/K \u2212 1du = [ \u22121 2 \u221a K n log (\u221a n K u+ 1\u221a\nn K u\u2212 1\n)]1\n\u03b40\n\u2264 1 2\n\u221a K\nn log\n(\u221a n K \u03b40 + 1\u221a\nn K \u03b40 \u2212 1\n) = log 3\n2\n\u221a K\nn .\nThus we proved: E ( \u00b5i\u2217(\u03b8)(\u03b8)\u2212 Bi\u2217(\u03b8),t|\u03b8 ) \u2264 ( 2 + 2(1 + log 2) + log 3\n2 )\u221a K n \u2264 6 \u221a K n .\nStep 3: control of \u2211n\nt=1 E (BIt,t \u2212 \u00b5It(\u03b8)|\u03b8). We start again by integrating the deviations:\nE\nn\u2211\nt=1\n(BIt,t \u2212 \u00b5It) \u2264 \u03b40n+ \u222b +\u221e\n\u03b40\nn\u2211\nt=1\nP(BIt,t \u2212 \u00b5It \u2265 u)du.\nNext we use the following simple inequality:\nn\u2211\nt=1\n1{BIt,t \u2212 \u00b5It \u2265 u} \u2264 n\u2211\ns=1\nK\u2211\ni=1\n1   \u00b5\u0302i,s + \u221a log+ ( n Ks ) s \u2212 \u00b5i \u2265 u    ,\nwhich implies\nn\u2211\nt=1\nP(BIt,t \u2212 \u00b5It \u2265 u) \u2264 K\u2211\ni=1\nn\u2211\ns=1\nP  \u00b5\u0302i,s + \u221a log+ ( n Ks )\ns \u2212 \u00b5i \u2265 u\n  .\nNow for u \u2265 \u03b40 let s(u) = \u23083 log ( nu2\nK\n) /u2\u2309 where \u2308x\u2309 is the smallest integer large than x. Let\nc = 1\u2212 1\u221a 3 . It is easy to see that one has:\nn\u2211\ns=1\nP  \u00b5\u0302i,s + \u221a log+ ( n Ks )\ns \u2212 \u00b5i \u2265 u\n  \u2264 3 log ( nu2 K )\nu2 +\nn\u2211\ns=s(u)\nP (\u00b5\u0302i,s \u2212 \u00b5i \u2265 cu) .\nUsing an integration already done in Step 2 we have\n\u222b +\u221e\n\u03b40\n3 log ( nu2\nK\n)\nu2 \u2264 3(1 + log(2))\n\u221a n\nK \u2264 5.1\n\u221a n\nK .\nNext using Hoeffding\u2019s inequality and the fact that the rewards are in [0, 1] one has for u \u2265 \u03b40 n\u2211\ns=s(u)\nP (\u00b5\u0302i,s \u2212 \u00b5i \u2265 cu) \u2264 n\u2211\ns=s(u)\nexp(\u22122sc2u2)1u\u22641/c \u2264 exp(\u221212c2 log 2) 1\u2212 exp(\u22122c2u2)1u\u22641/c.\nNow using that 1\u2212 exp(\u2212x) \u2265 x\u2212 x2/2 for x \u2265 0 one obtains \u222b 1/c\n\u03b40\n1 1\u2212 exp(\u22122c2u2)du = \u222b 1/(2c)\n\u03b40\n1 1\u2212 exp(\u22122c2u2)du+ \u222b 1/c\n1/(2c)\n1\n1\u2212 exp(\u22122c2u2)du\n\u2264 \u222b 1/(2c)\n\u03b40\n1 2c2u2 \u2212 2c4u4du+ 1 2c(1\u2212 exp(\u22121/2))\n\u2264 \u222b 1/(2c)\n\u03b40\n2\n3c2u2 du+\n1\n2c(1\u2212 exp(\u22121/2))\n= 2 3c2\u03b40 \u2212 4 3c +\n1\n2c(1\u2212 exp(\u22121/2))\n\u2264 1.9 \u221a n\nK .\nPutting the pieces together we proved\nE\nn\u2211\nt=1\n(BIt,t \u2212 \u00b5It) \u2264 7.6 \u221a nK,\nwhich concludes the proof together with the results of Step 1 and Step 2.\n3 Thompson Sampling in the two-armed BPR setting\nFollowing [Section 2, Bubeck et al. [2013]] we consider here the two-armed bandit problem with sub-Gaussian reward distributions (that is they satisfy Ee\u03bb(X\u2212\u00b5) \u2264 e\u03bb2/2 for all \u03bb \u2208 R) and such that one reward distribution has mean \u00b5\u2217 and the other one has mean \u00b5\u2217 \u2212\u2206 where \u00b5\u2217 and \u2206 are known values.\nIn order to derive the Thompson Sampling strategy for this problem we further assume that the reward distributions are in fact Gaussian with variance 1. In other words let \u0398 = {\u03b81, \u03b82}, \u03c00(\u03b81) = \u03c00(\u03b82) = 1/2, and under \u03b81 one has X1,s \u223c N (\u00b5\u2217, 1) and X2,s \u223c N (\u00b5\u2217 \u2212 \u2206, 1) while under \u03b82 one has X2,s \u223c N (\u00b5\u2217, 1) and X1,s \u223c N (\u00b5\u2217 \u2212\u2206, 1). Then a straightforward computation (using Bayes rule and induction) shows that one has for some normalizing constant c > 0:\n\u03c0t(\u03b81) = c exp\n \u22121\n2\nT1(t\u22121)\u2211\ns=1\n(\u00b5\u2217 \u2212X1,s)2 \u2212 1\n2\nT2(t\u22121)\u2211\ns=1\n(\u00b5\u2217 \u2212\u2206\u2212X2,s)2   ,\n\u03c0t(\u03b82) = c exp\n \u22121\n2\nT1(t\u22121)\u2211\ns=1\n(\u00b5\u2217 \u2212\u2206\u2212X1,s)2 \u2212 1\n2\nT2(t\u22121)\u2211\ns=1\n(\u00b5\u2217 \u2212X2,s)2   .\nRecall that Thompson Sampling draws \u03b8(t) from \u03c0t and then pulls the best arm for the environment \u03b8(t). Observe that under \u03b81 the best arm is arm 1 and under \u03b82 the best arm is arm 2. In other words Thompson Sampling draws It at random with the probabilities given by the posterior \u03c0t. This leads to a general algorithm for the two-armed BPR setting with sub-Gaussian reward distributions that we summarize in Figure 1. The next result shows that it attains optimal performances in this setting up to a numerical constant (see Bubeck et al. [2013] for lower bounds), for any subGaussian reward distribution (not necessarily Gaussian) with largest mean \u00b5\u2217 and gap \u2206.\nTheorem 2 The policy of Figure 1 has regret bounded as Rn \u2264 \u2206+ 578\u2206 , uniformly in n.\nNote that we did not try to optimize the numerical constant in the above bound. Figure 2 shows an empirical comparison of the policy of Figure 1 with Policy 1 of Bubeck et al. [2013]. Note in particular that a regret bound of order 16/\u2206 was proved for the latter algorithm and the (limited) numerical simulation presented here suggests that Thompson Sampling outperforms this strategy. Proof Without loss of generality we assume that arm 1 is the optimal arm, that is \u00b51 = \u00b5\u2217 and \u00b52 = \u00b5 \u2217 \u2212 \u2206. Let \u00b5\u0302i,s = 1s \u2211s\nt=1Xi,t, \u03b3\u03021,s = \u00b51 \u2212 \u00b5\u03021,s and \u03b3\u03022,s = \u00b5\u03022,s \u2212 \u00b52. Note that large (positive) values of \u03b3\u03021,s or \u03b3\u03022,s might mislead the algorithm into bad decisions, and we will need\nto control what happens in various regimes for these \u03b3 coefficients. We decompose the proof into three steps.\nStep 1. This first step will be useful in the rest of the analysis, it shows how the probability ratio of a bad pull over a good pull evolves as a function of the \u03b3 coefficients introduced above. One has:\npt(2) pt(1) = exp\n \u22121\n2\nT1(t\u22121)\u2211\ns=1\n[ (\u00b52 \u2212X1,s)2 \u2212 (\u00b51 \u2212X1,s)2 ] \u2212 1\n2\nT2(t\u22121)\u2211\ns=1\n[ (\u00b51 \u2212X2,s)2 \u2212 (\u00b52 \u2212X2,s)2 ] \n= exp ( \u2212T1(t\u2212 1)\n2\n[ \u00b522 \u2212 \u00b521 \u2212 2(\u00b52 \u2212 \u00b51)\u00b5\u03021,T1(t\u22121) ] \u2212 T2(t\u2212 1)\n2\n[ \u00b521 \u2212 \u00b522 \u2212 2(\u00b51 \u2212 \u00b52)\u00b5\u03022,T2(t\u22121) ])\n= exp ( \u2212T1(t\u2212 1)\n2\n[ \u22062 \u2212 2\u2206(\u00b51 \u2212 \u00b5\u03021,T1(t\u22121)) ] \u2212 T2(t\u2212 1)\n2\n[ \u22062 \u2212 2\u2206(\u00b5\u03022,T2(t\u22121) \u2212 \u00b52) ])\n= exp ( \u2212t\u2206 2\n2 + T1(t\u2212 1)\u2206\u03b3\u03021,T1(t\u22121) + T2(t\u2212 1)\u2206\u03b3\u03022,T2(t\u22121)\n) .\nStep 2. We decompose the regret Rn as follows:\nRn \u2206 = 1 + E\nn\u2211\nt=3\n1{It = 2}\n= 1 + E n\u2211\nt=3\n1 { \u03b3\u03022,T2(t\u22121) > \u2206\n4 , It = 2\n} + E n\u2211\nt=3\n1 { \u03b3\u03022,T2(t\u22121) \u2264 \u2206\n4 , \u03b3\u03021,T1(t\u22121) \u2264\n\u2206 4 , It = 2\n}\n+E n\u2211\nt=3\n1 { \u03b3\u03022,T2(t\u22121) \u2264 \u2206\n4 , \u03b3\u03021,T1(t\u22121) >\n\u2206 4 , It = 2\n} .\n\u00b5* = 0, \u2206 = 0.2\n\u00b5* = 0, \u2206 = 0.05\nWe use Hoeffding\u2019s inequality to control the first term:\nE\nn\u2211\nt=3\n1 { \u03b3\u03022,T2(t\u22121) > \u2206\n4 , It = 2\n} \u2264 E n\u2211\ns=1\n1 { \u03b3\u03022,s > \u2206\n4\n} \u2264 n\u2211\ns=1\nexp ( \u2212s\u2206 2\n32\n) \u2264 32\n\u22062 .\nFor the second term, using the rewriting of Step 1 as an upper bound on pt(2), one obtains:\nE\nn\u2211\nt=3\n1 { \u03b3\u03022,T2(t\u22121) \u2264 \u2206\n4 , \u03b3\u03021,T1(t\u22121) \u2264\n\u2206 4 , It = 2\n} = n\u2211\nt=3\nE ( pt(2)1 { \u03b3\u03022,T2(t\u22121) \u2264 \u2206\n4 , \u03b3\u03021,T1(t\u22121) \u2264\n\u2206\n4\n})\n\u2264 n\u2211\nt=3\nexp ( \u2212t\u2206 2\n4\n) \u2264 4\n\u22062 .\nThe third term is more difficult to control, and we further decompose the corresponding event as follows:\n{ \u03b3\u03022,T2(t\u22121) \u2264 \u2206\n4 , \u03b3\u03021,T1(t\u22121) >\n\u2206 4 , It = 2\n}\n\u2282 { \u03b3\u03021,T1(t\u22121) > \u2206\n4 , T1(t\u2212 1) > t/4\n} \u222a { \u03b3\u03022,T2(t\u22121) \u2264 \u2206\n4 , It = 2, T1(t\u2212 1) \u2264 t/4\n} .\nThe cumulative probability of the first event in the above decomposition is easy to control thanks to Hoeffding\u2019s maximal inequality2 which states that for any m \u2265 1 and x > 0 one has\nP(\u2203 1 \u2264 s \u2264 m s.t. s \u03b3\u03021,s \u2265 x) \u2264 exp ( \u2212 x 2\n2m\n) .\n2It is an easy exercise to verify that Azuma-Hoeffding holds for martingale differences with sub-Gaussian increments, which implies Hoeffding\u2019s maximal inequality for sub-Gaussian distributions.\nIndeed this implies\nP ( \u03b3\u03021,T1(t\u22121) > \u2206\n4 , T1(t\u2212 1) > t/4\n) \u2264 P ( \u2203 1 \u2264 s \u2264 t s.t. s \u03b3\u03021,s > \u2206t\n16\n) \u2264 exp ( \u2212t\u2206 2\n512\n) ,\nand thus\nE\nn\u2211\nt=3\n1 { \u03b3\u03021,T1(t\u22121) > \u2206\n4 , T1(t\u2212 1) > t/4\n} \u2264 512\n\u22062 .\nIt only remains to control the term\nE\nn\u2211\nt=3\n1 { \u03b3\u03022,T2(t\u22121) \u2264 \u2206\n4 , It = 2, T1(t\u2212 1) \u2264 t/4\n} = n\u2211\nt=3\nE ( pt(2)1 { \u03b3\u03022,T2(t\u22121) \u2264 \u2206\n4 , T1(t\u2212 1) \u2264 t/4\n})\n\u2264 n\u2211\nt=3\nE exp ( \u2212t\u2206 2\n4 + \u2206 max 1\u2264s\u2264t/4 s\u03b3\u03021,s\n) ,\nwhere the last inequality follows from Step 1. The last step is devoted to bounding from above this last term.\nStep 3. By integrating the deviations and using again Hoeffding\u2019s maximal inequality one obtains\nE exp ( \u2206 max\n1\u2264s\u2264t/4 s\u03b3\u03021,s\n) \u2264 1+ \u222b +\u221e\n1\nP ( max 1\u2264s\u2264 t\n4\ns\u03b3\u03021,s \u2265 log x\n\u2206\n) dx \u2264 1+ \u222b +\u221e\n1\nexp ( \u22122(log x) 2\n\u22062t\n) dx.\nNow, straightforward computation gives n\u2211\nt=3\nexp ( \u2212t\u2206 2\n4\n)( 1 + \u222b +\u221e\n1\nexp ( \u22122(log x) 2\n\u22062t\n) dx ) \u2264 n\u2211\nt=3\nexp ( \u2212t\u2206 2\n4\n)( 1 + \u221a \u03c0\u22062t\n2 exp\n( t\u22062\n8\n))\n\u2264 4 \u22062 +\n\u222b +\u221e\n0\n\u221a \u03c0\u22062t\n2 exp\n( \u2212t\u2206 2\n8\n) dt\n\u2264 4 \u22062\n+ 16 \u221a \u03c0\n\u22062\n\u222b +\u221e\n0\n\u221a u exp(\u2212u) du\n\u2264 30 \u22062 .\nwhich concludes the proof by putting this together with the results of the previous step.\n4 Optimal strategy for the BPR setting inspired by Thompson Sampling\nIn this section we consider the general BPR setting. That is the reward distributions are subGaussian (they satisfy Ee\u03bb(X\u2212\u00b5) \u2264 e\u03bb2/2 for all \u03bb \u2208 R), one reward distribution has mean \u00b5\u2217, and all the other means are smaller than \u00b5\u2217 \u2212 \u03b5 where \u00b5\u2217 and \u03b5 are known values.\nSimilarly to the previous section we assume that the reward distributions are Gaussian with variance 1 for the derivation of the Thompson Sampling strategy (but we do not make this assumption for the analysis of the resulting algorithm). Then the set of possible parameters is described as follows:\n\u0398 = \u222aKi=1\u0398i where \u0398i = {\u03b8 \u2208 RK s.t. \u03b8i = \u00b5\u2217 and \u03b8j \u2264 \u00b5\u2217 \u2212 \u03b5 for all j 6= i}.\nAssuming a uniform prior over the index of the best arm, and a prior \u03bb over the mean of a suboptimal arm one obtains by Bayes rule that the probability density function of the posterior is given by:\nd\u03c0t(\u03b8) \u221d exp  \u22121\n2\nK\u2211\nj=1\nTj(t\u22121)\u2211\ns=1\n(Xj,s \u2212 \u03b8j)2  \nK\u220f\nj=1,j 6=i\u2217(\u03b8) d\u03bb(\u03b8j).\nNow remark that with Thompson Sampling arm i is played at time t if and only if \u03b8(t) \u2208 \u0398i. In other words It is played at random from probability pt where\npt(i) = \u03c0t(\u0398i) \u221d exp  \u22121\n2\nTi(t\u22121)\u2211\ns=1\n(Xi,s \u2212 \u00b5\u2217)2  \u220f\nj 6=i\n  \u222b \u00b5\u2217\u2212\u03b5\n\u2212\u221e exp\n \u22121\n2\nTj(t\u22121)\u2211\ns=1\n(Xj,s \u2212 v)2   d\u03bb(v)  \n\u221d exp\n( \u22121\n2 \u2211Ti(t\u22121) s=1 (Xi,s \u2212 \u00b5\u2217)2\n)\n\u222b \u00b5\u2217\u2212\u03b5 \u2212\u221e exp ( \u22121 2 \u2211Ti(t\u22121) s=1 (Xi,s \u2212 v)2 ) d\u03bb(v) .\nTaking inspiration from the above calculation we consider the following policy, where \u03bb is the Lebesgue measure and we assume a slightly larger value for the variance (this is necessary for the proof).\nThe following theorem shows that this policy attains the best known performance for the BPR setting, shaving off a log-log term in the regret bound of the BPR policy.\nTheorem 3 The policy of Figure 3 has regret bounded as Rn \u2264 \u2211\ni:\u2206i>0\n( \u2206i +\n80+log(\u2206i/\u03b5) \u2206i\n) ,\nuniformly in n.\nProof The general structure of the proof is superficially similar to the proof of Theorem 2 but many details are different. Without loss of generality we assume that arm 1 is the optimal arm, that is \u00b51 = \u00b5\u2217 and \u2200i \u2265 2, \u00b5i = \u00b5\u2217 \u2212 \u2206i. Let \u03b3\u03021,s = \u00b51 \u2212 \u00b5\u03021,s and \u03b3\u0302i,s = \u00b5\u0302i,s \u2212 \u00b5i for i \u2265 2. We decompose the proof into four steps.\nStep 1: Rewriting of the ratio pi,t p1,t . Let i \u2265 2, the following rewriting will be useful in the rest of the proof:\npt(i) pt(1) =\n\u222b \u00b51\u2212\u03b5 \u2212\u221e exp ( \u22121 3 \u2211T1(t\u22121) s=1 (X1,s \u2212 v)2 \u2212 (X1,s \u2212 \u00b51)2 ) dv \u222b \u00b51\u2212\u03b5 \u2212\u221e exp ( \u22121 3 \u2211Ti(t\u22121) s=1 (Xi,s \u2212 v)2 \u2212 (Xi,s \u2212 \u00b51)2 ) dv\n=\n\u222b \u00b51\u2212\u03b5 \u2212\u221e exp ( \u2212T1(t\u22121) 3 (\u00b5\u03021,T1(t\u22121) \u2212 v)2 \u2212 (\u00b5\u03021,T1(t\u22121) \u2212 \u00b51)2 ) dv\n\u222b \u00b51\u2212\u03b5 \u2212\u221e exp ( \u2212Ti(t\u22121) 3 (\u00b5\u0302i,Ti(t\u22121) \u2212 v)2 \u2212 (\u00b5\u0302i,Ti(t\u22121) \u2212 \u00b51)2 ) dv\n=\n\u222b +\u221e \u2212\u03b3\u03021,T1(t\u22121)+\u03b5 exp ( \u2212T1(t\u22121) 3 (v2 \u2212 \u03b3\u030221,T1(t\u22121)) ) dv\n\u222b +\u221e \u03b3\u0302i,Ti(t\u22121)\u2212\u2206i+\u03b5 exp ( \u2212Ti(t\u22121) 3 (v2 \u2212 (\u03b3\u0302i,Ti(t\u22121) \u2212\u2206i)2) ) dv ,\nwhere the last step follows by a simple change of variable.\nStep 2: Decomposition of Rn. For i \u2265 2. Let Ai = \u2308 6\u22062i log( e6\u2206i \u03b5 )\u2309 where \u2308x\u2309 is the smallest integer larger than x. We decompose the regret Rn as follows.\nRn =\nK\u2211\ni=2\n( \u2206i +\u2206iE n\u2211\nt=K+1\n1{It = i} )\n\u2264 K\u2211\ni=2\n\u2206i ( Ai + E n\u2211\nt=K+1\n1{Ti(t\u2212 1) \u2265 Ai, It = i} )\n= K\u2211\ni=2\n\u2206i ( Ai + E n\u2211\nt=K+1\n1 { \u03b3\u0302i,Ti(t\u22121) >\n\u2206i 4 , Ti(t\u2212 1) \u2265 Ai, It = i\n}\n+ E n\u2211\nt=K+1\n1 { \u03b3\u0302i,Ti(t\u22121) \u2264\n\u2206i 4 , Ti(t\u2212 1) \u2265 Ai, It = i\n}) .\nThe first expectation can be bounded by using Hoeffding\u2019s inequality.\nE\nn\u2211\nt=K+1\n1 { \u03b3\u0302i,Ti(t\u22121) >\n\u2206i 4 , Ti(t\u2212 1) \u2265 Ai, It = i\n} \u2264 E n\u2211\ns=1\n1 { \u03b3\u0302i,s >\n\u2206i 4\n} \u2264 n\u2211\ns=1\nexp ( \u2212s\u2206 2 i\n32\n) \u2264 32\n\u22062i .\nThe second expectation is more difficult to bound from above and the next two steps are dedicated to this task.\nStep 3: Analysis of \u2211n t=K+1 E 1 { \u03b3\u0302i,Ti(t\u22121) \u2264 \u2206i4 , Ti(t\u2212 1) \u2265 Ai, It = i } . Clearly by definition of the policy one has\nn\u2211\nt=K+1\nE 1 { \u03b3\u0302i,Ti(t\u22121) \u2264\n\u2206i 4 , Ti(t\u2212 1) \u2265 Ai, It = i\n} = n\u2211\nt=K+1\nE\n[ pt(i)\npt(1) 1\n{ \u03b3\u0302i,Ti(t\u22121) \u2264\n\u2206i 4 , Ti(t\u2212 1) \u2265 Ai, It = 1\nWe have now to control the term pt(i) pt(1) on the event {\u03b3\u0302i,Ti(t\u22121) \u2264 \u2206i4 , Ti(t\u22121) \u2265 Ai}. The following bounds on the tail of the standard Gaussian distribution will be useful, for any x > 0 one has\n1 x e\u2212 1 2 x2 \u2265\n\u222b +\u221e\nx\ne\u2212 1 2 v2 dv \u2265 1\nx\n( 1\u2212 1\nx2\n) e\u2212 1 2 x2 .\nNow one has \u222b +\u221e\n\u03b3\u0302i,Ti(t\u22121)\u2212\u2206i+\u03b5 e\u2212 1 3 Ti(t\u22121)(v2\u2212(\u03b3\u0302i,Ti(t\u22121)\u2212\u2206i) 2) dv\n= e 1 3 Ti(t\u22121)(\u03b3\u0302i,Ti(t\u22121)\u2212\u2206i) 2\n\u222b +\u221e\n\u03b3\u0302i,Ti(t\u22121)\u2212\u2206i+\u03b5 e\u2212 1 3 Ti(t\u22121)v2 dv\n\u2265 e 316Ti(t\u22121)\u22062i \u222b +\u221e\n\u2206i 4\ne\u2212 1 3 Ti(t\u22121)v2 dv\n= e 3 16 Ti(t\u22121)\u22062i \u00b7 \u221a\n3 2Ti(t\u2212 1) \u00b7 \u222b +\u221e\n\u2206i 4\n\u221a 2Ti(t\u22121)\n3\ne\u2212 1 2 v2 dv\n\u2265 e 316Ti(t\u22121)\u22062i \u00b7 6 \u2206iTi(t\u2212 1)\n( 1\u2212 24\n\u22062iTi(t\u2212 1)\n) e\u2212 1 48 Ti(t\u22121)\u22062i\n\u2265 e 16Ti(t\u22121)\u22062i \u00b7 2 \u2206iTi(t\u2212 1) ,\nwhere the last step follows from\nTi(t\u2212 1) \u2265 Ai \u2265 6\n\u22062i log ( e6\u2206i \u03b5 ) \u2265 36 \u22062i .\nNext, using the fact that the function x \u2192 1 x e 1 6 x\u22062i is increasing on [ 6 \u22062i ,+\u221e), we get (\u222b +\u221e\n\u03b3\u0302i,Ti(t\u22121)\u2212\u2206i+\u03b5 e\u2212 1 3 Ti(t\u22121)(v2\u2212(\u03b3\u0302i,Ti(t\u22121)\u2212\u2206) 2) dv\n)\u22121 \u2264 ( e 1 6 Ti(t\u22121)\u22062 \u00b7 2\n\u2206iTi(t\u2212 1)\n)\u22121\n\u2264 e\u2212 1 6 \u22062i\n( 6\n\u22062 i\nlog\n( e6\u2206i\n\u03b5 )) \u2206i 2 6 \u22062i log ( e6\u2206 \u03b5 )\n= 3 e6 \u03b5 \u22062i log ( e6\u2206i \u03b5 ) .\nPlugging into the expression of pt(i) pt(1) , we obtain\nn\u2211\nt=K+1\nE 1{\u03b3\u0302i,Ti(t\u22121)\u2264 \u2206i 4 ,Ti(t\u22121)\u2265Ai,It=i}\n=\nn\u2211\nt=K+1\nE [ pi,t p1,t 1{\u03b3\u0302i,Ti(t\u22121)\u2264 \u2206i 4 ,Ti(t\u22121)\u2265Ai,It=1} ]\n\u2264 ( n\u2211\nt=K+1\nE\n[\u222b +\u221e\n\u2212\u03b3\u03021,T1(t\u22121)+\u03b5 e \u2212 1 3 T1(t\u22121)(v2\u2212\u03b3\u030221,T1(t\u22121)) dv1{It=1}\n]) 3\ne6 \u03b5 \u22062i log ( e6\u2206i \u03b5 ) .\n\u2264 ( +\u221e\u2211\nt=1\nE\n[\u222b +\u221e\n\u2212\u03b3\u03021,t+\u03b5 e\u2212 1 3 t(v2\u2212\u03b3\u030221,t) dv\n]) 3\ne6 \u03b5 \u22062i log ( e6\u2206i \u03b5 ) .\nStep 4: Control of \u2211+\u221e\nt=1 E [\u222b +\u221e \u2212\u03b3\u03021,t+\u03b5 e \u2212 1 3 t(v2\u2212\u03b3\u030221,t) dv ] .\nFirst, observe that\n+\u221e\u2211\nt=1\nE\n[\u222b +\u221e\n\u2212\u03b3\u03021,t+\u03b5 e\u2212 1 3 t(v2\u2212\u03b3\u030221,t) dv\n]\n\u2264 +\u221e\u2211\nt=1\nE\n[\u222b +\u221e\n\u2212\u03b3\u03021,t+\u03b5 e\u2212 1 3 tv2 dv \u00b7 e 13 t\u03b3\u030221,t1{|\u03b3\u03021,t|\u2264 \u03b53}\n] + +\u221e\u2211\nt=1\nE\n[\u222b +\u221e\n\u2212\u03b3\u03021,t+\u03b5 e\u2212 1 3 tv2 dv \u00b7 e 13 t\u03b3\u030221,t1{|\u03b3\u03021,t|\u2265 \u03b53}\n]\n\u2264 +\u221e\u2211\nt=1 \u222b +\u221e 2 3 \u03b5 e\u2212 1 3 tv2 dv \u00b7 e 127 t\u03b52 + +\u221e\u2211 t=1 \u222b +\u221e \u2212\u221e e\u2212 1 3 tv2 dv \u00b7 E [ e 1 3 t\u03b3\u030221,t1{|\u03b3\u03021,t|\u2265 \u03b53} ] .\nThe first term is straightforward to compute:\n+\u221e\u2211\nt=1 \u222b +\u221e 2 3 \u03b5 e\u2212 1 3 tv2 dv \u00b7 e 127 t\u03b52 = \u222b +\u221e 2 3 \u03b5 +\u221e\u2211 t=1 e\u2212 1 3 t(v2\u2212 1 9 \u03b52) dv\n\u2264 \u222b +\u221e\n2 3 \u03b5\n3\nv2 \u2212 1 9 \u03b52\ndv \u2264 9 log 3 2\u03b5\nFor the second term, we first integrate the deviations and we use Hoeffding\u2019s inequality to obtain\nE [ e\n1 3 t\u03b3\u030221,t1{|\u03b3\u03021,t|\u2265 \u03b53} ] \u2264 e 13 t( \u03b53 )2P(|\u03b3\u03021,t| \u2265 \u03b5\n3 ) +\n\u222b +\u221e\ne 1 3 t( \u03b5 3 )\n2 P(e\n1 3 t\u03b3\u030221,t \u2265 x) dx\n\u2264 2e\u2212 154 t\u03b52 + \u222b +\u221e\ne 1 27 t\u03b5\n2 P\n( |\u03b3\u03021,t| \u2265 \u221a 3 log x\nt\n) dx\n\u2264 2e\u2212 154 t\u03b52 + 2 \u222b +\u221e\ne 1 27 t\u03b5\n2 e\u2212 3 2 log x dx\n\u2264 6e\u2212 154 t\u03b52 ,\nwhich yields\n+\u221e\u2211\nt=1\n\u222b +\u221e\n\u2212\u221e e\u2212 1 3 tv2 dv \u00b7 E\n[ e\n1 3 t\u03b3\u030221,t1{|\u03b3\u03021,t|\u2265 \u03b53}\n] \u2264\n\u222b +\u221e\n\u2212\u221e 6\n+\u221e\u2211\nt=1\ne\u2212 1 3 t(v2+ 1 18 \u03b52) dv\n\u2264 \u222b +\u221e\n\u2212\u221e 18\n1\nv2 + 1 18 \u03b52\ndv = 54 \u221a 2\u03c0\n\u03b5 .\nPutting together all the steps finishes the proof.\nReferences\nS. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. In Proceedings of the 25th Annual Conference on Learning Theory (COLT), 2012a.\nS. Agrawal and N. Goyal. Further optimal regret bounds for thompson sampling, 2012b. arXiv:1209.3353.\nJ.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In Proceedings of the 22nd Annual Conference on Learning Theory (COLT), 2009.\nJ.-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitoring. Journal of Machine Learning Research, 11:2635\u20132686, 2010.\nP. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning Journal, 47(2-3):235\u2013256, 2002.\nS. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1\u2013122, 2012.\nS. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT), 2009.\nS. Bubeck, V. Perchet, and P. Rigollet. Bounded regret in stochastic multi-armed bandits. In Proceedings of the 26th Annual Conference on Learning Theory (COLT), 2013.\nO. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Advances in Neural Information Processing Systems (NIPS), 2011.\nJ.C. Gittins. Bandit processes and dynamic allocation indices. Journal Royal Statistical Society Series B, 14:148\u2013167, 1979.\nE. Kaufmann, N. Korda, and R. Munos. Thompson sampling: an asymptotically optimal finitetime analysis. In Proceedings of the 23rd International Conference on Algorithmic Learning Theory (ALT), 2012.\nH. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematics Society, 58:527\u2013535, 1952.\nD. Russo and B. Van Roy. Learning to optimize via posterior sampling, 2013. arXiv:1301.2609.\nW. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Bulletin of the American Mathematics Society, 25:285\u2013294, 1933."}], "references": [{"title": "Analysis of Thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Further optimal regret bounds for thompson sampling, 2012b. arXiv:1209.3353", "author": ["S. Agrawal", "N. Goyal"], "venue": null, "citeRegEx": "Agrawal and Goyal.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal and Goyal.", "year": 2012}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["J.-Y. Audibert", "S. Bubeck"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory (COLT),", "citeRegEx": "Audibert and Bubeck.,? \\Q2009\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2009}, {"title": "Regret bounds and minimax policies under partial monitoring", "author": ["J.-Y. Audibert", "S. Bubeck"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Audibert and Bubeck.,? \\Q2010\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2010}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning Journal,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In Proceedings of the 20th International Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "Bubeck et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2009}, {"title": "Bounded regret in stochastic multi-armed bandits", "author": ["S. Bubeck", "V. Perchet", "P. Rigollet"], "venue": "In Proceedings of the 26th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "An empirical evaluation of Thompson sampling", "author": ["O. Chapelle", "L. Li"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Chapelle and Li.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Li.", "year": 2011}, {"title": "Bandit processes and dynamic allocation indices", "author": ["J.C. Gittins"], "venue": "Journal Royal Statistical Society Series B,", "citeRegEx": "Gittins.,? \\Q1979\\E", "shortCiteRegEx": "Gittins.", "year": 1979}, {"title": "Thompson sampling: an asymptotically optimal finitetime analysis", "author": ["E. Kaufmann", "N. Korda", "R. Munos"], "venue": "In Proceedings of the 23rd International Conference on Algorithmic Learning Theory (ALT),", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Mathematics Society,", "citeRegEx": "Robbins.,? \\Q1952\\E", "shortCiteRegEx": "Robbins.", "year": 1952}, {"title": "Learning to optimize via posterior sampling", "author": ["D. Russo", "B. Van Roy"], "venue": null, "citeRegEx": "Russo and Roy.,? \\Q2013\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2013}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W. Thompson"], "venue": "Bulletin of the American Mathematics Society,", "citeRegEx": "Thompson.,? \\Q1933\\E", "shortCiteRegEx": "Thompson.", "year": 1933}], "referenceMentions": [{"referenceID": 2, "context": "Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 \u221a nK.", "startOffset": 30, "endOffset": 57}, {"referenceID": 2, "context": "Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 \u221a nK.", "startOffset": 30, "endOffset": 82}, {"referenceID": 2, "context": "Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 \u221a nK. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by 1 20 \u221a nK. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.", "startOffset": 30, "endOffset": 486}, {"referenceID": 5, "context": "On the other hand the point of view initially developed in Robbins [1952] leads to a learning problem.", "startOffset": 59, "endOffset": 74}, {"referenceID": 3, "context": "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting.", "startOffset": 91, "endOffset": 122}, {"referenceID": 3, "context": "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior distribution takes a product form an optimal strategy is given by the Gittins indices (which are relatively easy to compute).", "startOffset": 91, "endOffset": 271}, {"referenceID": 3, "context": "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior distribution takes a product form an optimal strategy is given by the Gittins indices (which are relatively easy to compute). The product assumption on the prior means that the reward processes (Xi,s)s\u22651 are independent across arms. In the present paper we are precisely interested in the situations where this assumption is not satisfied. Indeed we believe that one of the strength of the Bayesian setting is that one can incorporate prior knowledge on the arms in very transparent way. A prototypical example that we shall consider later on in this paper is when one knows the distributions of the arms up to a permutation, in which case the reward processes are strongly dependent. In general without the product assumption on the prior it seems hopeless (from a computational perspective) to look for the optimal Bayesian strategy. Thus, despite being in a Bayesian setting, it makes sense to view it as a learning problem and to evaluate the agent\u2019s performance through its Bayesian regret. In this paper we are particularly interested in studying the Thompson Sampling strategy which was proposed in the very first paper on the multi-armed bandit problem Thompson [1933]. This strategy can be described very succinctly: let \u03c0t be the posterior distribution on \u03b8 given the history Ht = (I1, X1,1, .", "startOffset": 91, "endOffset": 1487}, {"referenceID": 3, "context": "Both formulations of the problem have a long history and we refer the interested reader to Bubeck and Cesa-Bianchi [2012] for a survey of the extensive recent literature on the learning setting. In the Bayesian setting a major breakthrough was achieved in Gittins [1979] where it was shown that when the prior distribution takes a product form an optimal strategy is given by the Gittins indices (which are relatively easy to compute). The product assumption on the prior means that the reward processes (Xi,s)s\u22651 are independent across arms. In the present paper we are precisely interested in the situations where this assumption is not satisfied. Indeed we believe that one of the strength of the Bayesian setting is that one can incorporate prior knowledge on the arms in very transparent way. A prototypical example that we shall consider later on in this paper is when one knows the distributions of the arms up to a permutation, in which case the reward processes are strongly dependent. In general without the product assumption on the prior it seems hopeless (from a computational perspective) to look for the optimal Bayesian strategy. Thus, despite being in a Bayesian setting, it makes sense to view it as a learning problem and to evaluate the agent\u2019s performance through its Bayesian regret. In this paper we are particularly interested in studying the Thompson Sampling strategy which was proposed in the very first paper on the multi-armed bandit problem Thompson [1933]. This strategy can be described very succinctly: let \u03c0t be the posterior distribution on \u03b8 given the history Ht = (I1, X1,1, . . . , It\u22121, XIt\u22121,TIt\u22121 (t\u22121)) of the algorithm up to the beginning of round t. Then Thompson Sampling first draws a parameter \u03b8 from \u03c0t (independently from the past given \u03c0t) and it pulls It \u2208 argmaxi\u2208[K] \u03bci(\u03b8). Recently there has been a surge of interest in this simple policy, mainly because of its flexibility to incorporate prior knowledge on the arms, see for example Chapelle and Li [2011]. For a long time the theoretical properties of Thompson Sampling remained elusive.", "startOffset": 91, "endOffset": 2011}, {"referenceID": 0, "context": "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al.", "startOffset": 103, "endOffset": 129}, {"referenceID": 0, "context": "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al. [2012], Agrawal and Goyal [2012b].", "startOffset": 103, "endOffset": 153}, {"referenceID": 0, "context": "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al. [2012], Agrawal and Goyal [2012b]. However as we pointed out above here we are interested in proving regret bounds for the more realistic scenario where one runs Thompson Sampling with a hand-tuned prior distribution, possibly very different from a Beta prior.", "startOffset": 103, "endOffset": 180}, {"referenceID": 0, "context": "The specific case of binary rewards with a Beta prior is now very well understood thanks to the papers Agrawal and Goyal [2012a], Kaufmann et al. [2012], Agrawal and Goyal [2012b]. However as we pointed out above here we are interested in proving regret bounds for the more realistic scenario where one runs Thompson Sampling with a hand-tuned prior distribution, possibly very different from a Beta prior. The first result in this spirit was obtained very recently by Russo and Roy [2013] who showed that for any prior distribution \u03c00 Thompson Sampling always satisfies BRn \u2264 5 \u221a nK logn.", "startOffset": 103, "endOffset": 490}, {"referenceID": 0, "context": "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1.", "startOffset": 20, "endOffset": 46}, {"referenceID": 0, "context": "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1. Our first contribution is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009]. Our second contribution is to show that Thompson Sampling can take advantage of the properties of some non-trivial priors to attain much better regret guarantees.", "startOffset": 20, "endOffset": 280}, {"referenceID": 0, "context": "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1. Our first contribution is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009]. Our second contribution is to show that Thompson Sampling can take advantage of the properties of some non-trivial priors to attain much better regret guarantees. More precisely in Section 2 and 3 we consider the setting of Bubeck et al. [2013] (which we call the BPR setting) where \u03bc\u2217 and \u03b5 > 0 are known values such that for any \u03b8 \u2208 \u0398, first there is a unique best arm {i\u2217(\u03b8)} = argmaxi\u2208[K] \u03bci(\u03b8), and furthermore \u03bci\u2217(\u03b8)(\u03b8) = \u03bc \u2217, and \u2206i(\u03b8) := \u03bci\u2217(\u03b8)(\u03b8)\u2212 \u03bci(\u03b8) \u2265 \u03b5, \u2200i 6= i\u2217(\u03b8).", "startOffset": 20, "endOffset": 526}, {"referenceID": 0, "context": "bound was proved in Agrawal and Goyal [2012b] for the specific case of Beta prior1. Our first contribution is to show in Section 2 that the extraneous logarithmic factor in these bounds can be removed by using ideas reminiscent of the MOSS algorithm of Audibert and Bubeck [2009]. Our second contribution is to show that Thompson Sampling can take advantage of the properties of some non-trivial priors to attain much better regret guarantees. More precisely in Section 2 and 3 we consider the setting of Bubeck et al. [2013] (which we call the BPR setting) where \u03bc\u2217 and \u03b5 > 0 are known values such that for any \u03b8 \u2208 \u0398, first there is a unique best arm {i\u2217(\u03b8)} = argmaxi\u2208[K] \u03bci(\u03b8), and furthermore \u03bci\u2217(\u03b8)(\u03b8) = \u03bc \u2217, and \u2206i(\u03b8) := \u03bci\u2217(\u03b8)(\u03b8)\u2212 \u03bci(\u03b8) \u2265 \u03b5, \u2200i 6= i\u2217(\u03b8). In other words the value of the best arm is known as well as a non-trivial lower bound on the gap between the values of the best and second best arms. For this problem a new algorithm was proposed in Bubeck et al. [2013] (which we call the BPR policy), and it was shown that the BPR policy satisfies", "startOffset": 20, "endOffset": 983}, {"referenceID": 4, "context": "Thus the BPR policy attains a regret uniformly bounded over time in the BPR setting, a feature that standard bandit algorithms such as UCB of Auer et al. [2002] cannot achieve.", "startOffset": 142, "endOffset": 161}, {"referenceID": 4, "context": "Thus the BPR policy attains a regret uniformly bounded over time in the BPR setting, a feature that standard bandit algorithms such as UCB of Auer et al. [2002] cannot achieve. It is natural to view the assumptions of the BPR setting as a prior over the reward distributions and to ask what regret guarantees attains Thompson Sampling in that situation. More precisely we consider Thompson Sampling with Gaussian reward distributions and uniform prior over the possible range of parameters. We then prove individual regret bounds for any sub-Gaussian distributions (similarly to Bubeck et al. [2013]).", "startOffset": 142, "endOffset": 600}, {"referenceID": 0, "context": "5, Bubeck and Cesa-Bianchi 1Note however that the result of Agrawal and Goyal [2012b] applies to the individual regret Rn(\u03b8) while the result of Russo and Roy [2013] only applies to the integrated Bayesian regret BRn.", "startOffset": 60, "endOffset": 86}, {"referenceID": 0, "context": "5, Bubeck and Cesa-Bianchi 1Note however that the result of Agrawal and Goyal [2012b] applies to the individual regret Rn(\u03b8) while the result of Russo and Roy [2013] only applies to the integrated Bayesian regret BRn.", "startOffset": 60, "endOffset": 166}, {"referenceID": 6, "context": "This theorem also implies an optimal rate of identification for the best arm, see Bubeck et al. [2009] for more details on this.", "startOffset": 82, "endOffset": 103}, {"referenceID": 6, "context": "This theorem also implies an optimal rate of identification for the best arm, see Bubeck et al. [2009] for more details on this. Proof We decompose the proof into three steps. We denote i\u2217(\u03b8) \u2208 argmaxi\u2208[K] \u03bci(\u03b8), in particular one has It = i\u2217(\u03b8(t)). Step 1: rewriting of the Bayesian regret in terms of upper confidence bounds. This step is given by [Proposition 1, Russo and Roy [2013]] which we reprove for the sake of completeness.", "startOffset": 82, "endOffset": 387}, {"referenceID": 2, "context": "Inspired by the MOSS strategy of Audibert and Bubeck [2009] we will now take", "startOffset": 33, "endOffset": 60}, {"referenceID": 2, "context": "Next we extract the following inequality from Audibert and Bubeck [2010] (see p2683\u20132684), for any i \u2208 [K], P(\u03bci \u2212Bi,t \u2265 u) \u2264 4K nu2 log (\u221a n K u ) + 1 nu2/K \u2212 1 .", "startOffset": 46, "endOffset": 73}, {"referenceID": 6, "context": "3 Thompson Sampling in the two-armed BPR setting Following [Section 2, Bubeck et al. [2013]] we consider here the two-armed bandit problem with sub-Gaussian reward distributions (that is they satisfy Ee\u03bb(X\u2212\u03bc) \u2264 e\u03bb2/2 for all \u03bb \u2208 R) and such that one reward distribution has mean \u03bc\u2217 and the other one has mean \u03bc\u2217 \u2212\u2206 where \u03bc\u2217 and \u2206 are known values.", "startOffset": 71, "endOffset": 92}, {"referenceID": 6, "context": "The next result shows that it attains optimal performances in this setting up to a numerical constant (see Bubeck et al. [2013] for lower bounds), for any subGaussian reward distribution (not necessarily Gaussian) with largest mean \u03bc\u2217 and gap \u2206.", "startOffset": 107, "endOffset": 128}, {"referenceID": 6, "context": "The next result shows that it attains optimal performances in this setting up to a numerical constant (see Bubeck et al. [2013] for lower bounds), for any subGaussian reward distribution (not necessarily Gaussian) with largest mean \u03bc\u2217 and gap \u2206. Theorem 2 The policy of Figure 1 has regret bounded as Rn \u2264 \u2206+ 578 \u2206 , uniformly in n. Note that we did not try to optimize the numerical constant in the above bound. Figure 2 shows an empirical comparison of the policy of Figure 1 with Policy 1 of Bubeck et al. [2013]. Note in particular that a regret bound of order 16/\u2206 was proved for the latter algorithm and the (limited) numerical simulation presented here suggests that Thompson Sampling outperforms this strategy.", "startOffset": 107, "endOffset": 516}, {"referenceID": 6, "context": "Policy 1 from Bubeck et al.[2013] Policy of Figure 1", "startOffset": 14, "endOffset": 34}, {"referenceID": 6, "context": "Policy 1 from Bubeck et al.[2013] Policy of Figure 1", "startOffset": 14, "endOffset": 34}, {"referenceID": 6, "context": "Figure 2: Empirical comparison of the policy of Figure 1 and Policy 1 of Bubeck et al. [2013] on Gaussian reward distributions with variance 1.", "startOffset": 73, "endOffset": 94}], "year": 2013, "abstractText": "We consider the stochastic multi-armed bandit problem with a prior distribution on the reward distributions. We are interested in studying prior-free and prior-dependent regret bounds, very much in the same spirit as the usual distribution-free and distribution-dependent bounds for the non-Bayesian stochastic bandit. Building on the techniques of Audibert and Bubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling attains an optimal prior-free bound in the sense that for any prior distribution its Bayesian regret is bounded from above by 14 \u221a nK. This result is unimprovable in the sense that there exists a prior distribution such that any algorithm has a Bayesian regret bounded from below by 1 20 \u221a nK. We also study the case of priors for the setting of Bubeck et al. [2013] (where the optimal mean is known as well as a lower bound on the smallest gap) and we show that in this case the regret of Thompson Sampling is in fact uniformly bounded over time, thus showing that Thompson Sampling can greatly take advantage of the nice properties of these priors.", "creator": "LaTeX with hyperref package"}}}