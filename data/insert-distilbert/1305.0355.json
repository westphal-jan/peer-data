{"id": "1305.0355", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2013", "title": "Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition", "abstract": "in the high - dimensional regression programming model a response linear variable is precisely linearly related to $ p $ covariates, but the sample size $ n $ is smaller than $ p $. alternately we assume that only obtaining a small subset of covariates is ` active'( i. for e., that the corresponding coefficients normally are non - zero ), and clearly consider the model - oriented selection problem of identifying the active covariates. a popular approach is to commonly estimate the regression coefficients through the lasso ( $ \\ ell _ k 1 $ - regularized least squares ). this is known to correctly identify the active set only if the many irrelevant independent covariates are roughly orthogonal to the relevant ones, as clearly quantified through the so called ` irrepresentability'condition. in this paper we study define the ` gauss - lasso'selector, a simple two - stage method that first solves directly the lasso, periodically and then performs ordinary least squares restricted to the entire lasso n active set. we firstly formulate ` generalized irrepresentability condition'( gic ), an assumption that is substantially similarly weaker than irrepresentability. we prove that, under affirmative gic, because the gauss - lasso correctly recovers the active set.", "histories": [["v1", "Thu, 2 May 2013 07:25:52 GMT  (2565kb,D)", "http://arxiv.org/abs/1305.0355v1", "32 pages, 3 figures"]], "COMMENTS": "32 pages, 3 figures", "reviews": [], "SUBJECTS": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "authors": ["adel javanmard", "andrea montanari"], "accepted": true, "id": "1305.0355"}, "pdf": {"name": "1305.0355.pdf", "metadata": {"source": "CRF", "title": "Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition", "authors": ["Adel Javanmard", "Andrea Montanari"], "emails": ["adelj@stanford.edu"], "sections": [{"heading": null, "text": "A popular approach is to estimate the regression coefficients through the Lasso (`1-regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called \u2018irrepresentability\u2019 condition. In this paper we study the \u2018Gauss-Lasso\u2019 selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set.\nWe formulate \u2018generalized irrepresentability condition\u2019 (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set.\nContents"}, {"heading": "1 Introduction 2", "text": "1.1 An example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Further related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.3 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7"}, {"heading": "2 Deterministic designs 7", "text": "2.1 Zero-noise problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.2 Noisy problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9"}, {"heading": "3 Random Gaussian designs 10", "text": "3.1 The n =\u221e problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 The high-dimensional problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12"}, {"heading": "4 UCI communities and crimes data example 14", "text": "\u2217Department of Electrical Engineering, Stanford University. Email: adelj@stanford.edu\n\u2020Department of Electrical Engineering and Department of Statistics, Stanford University. Email: montanar@ stanford.edu\nar X\niv :1\n30 5.\n03 55\nv1 [\nm at\nh. ST\n] 2\nM ay\n2 01"}, {"heading": "5 Proof of Theorems 2.5 and 2.7 15", "text": "5.1 Proof of Theorem 2.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5.2 Proof of Theorem 2.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17"}, {"heading": "6 Proof of Theorems 3.4 and 3.7 18", "text": ""}, {"heading": "A Proof of technical lemmas 23", "text": ""}, {"heading": "B Generalized irrepresentability vs. irrepresentability 28", "text": "References 32"}, {"heading": "1 Introduction", "text": "In linear regression, we wish to estimate an unknown but fixed vector of parameters \u03b80 \u2208 Rp from n pairs (Y1, X1), (Y2, X2), . . . , (Yn, Xn), with vectors Xi taking values in Rp and response variables Yi given by\nYi = \u3008\u03b80, Xi\u3009+Wi , Wi \u223c N(0, \u03c32) , (1)\nwhere \u3008 \u00b7 , \u00b7 \u3009 is the standard scalar product. In matrix form, letting Y = (Y1, . . . , Yn)\nT and denoting by X the design matrix with rows XT1 , . . . , X T n , we have\nY = X \u03b80 +W , W \u223c N(0, \u03c32In\u00d7n) . (2)\nIn this paper, we consider the high-dimensional setting in which the number of parameters exceeds the sample size, i.e., p > n, but the number of non-zero entries of \u03b80 is smaller than p. We denote by S \u2261 supp(\u03b80) \u2286 [p] the support of \u03b80, and let s0 \u2261 |S|. We are interested in the \u2018model selection\u2019 problem, namely in the problem of identifying S from data Y , X.\nIn words, there exists a \u2018true\u2019 low dimensional linear model that explains the data. We want to identify the set S of covariates that are \u2018active\u2019 within this model. This problem has motivated a large body of research, because of its relevance to several modern data analysis tasks, ranging from signal processing [Don06, CRT06] to genomics [PZB+10, SK03]. A crucial step forward has been the development of model-selection techniques based on convex optimization formulations [Tib96, CD95, CT07]. These formulations have lead to computationally efficient algorithms that can be applied to large scale problems. Such developments pose the following theoretical question: For which vectors \u03b80, designs X, and noise levels \u03c3, the support S can be identified, with high probability, through computationally efficient procedures? The same question can be asked for random designs X and, in this case, \u2018high probability\u2019 will refer both to the noise realization W , and to the design realization X. In the rest of this introduction we shall focus \u2013for the sake of simplicity\u2013 on the deterministic settings, and refer to Section 3 for a treatment of Gaussian random designs.\nThe analysis of computationally efficient methods has largely focused on `1-regularized least squares, a.k.a. the Lasso [Tib96]. The Lasso estimator is defined by\n\u03b8\u0302n(Y,X;\u03bb) \u2261 arg min \u03b8\u2208Rp { 1 2n \u2016Y \u2212X\u03b8\u201622 + \u03bb\u2016\u03b8\u20161 } . (3)\nIn case the right hand side has more than one minimizer, one of them can be selected arbitrarily for our purposes. We will often omit the arguments Y , X, as they are clear from the context. (A closely related method is the so-called Dantzig selector [CT07]: it would be interesting to explore whether our results can be generalized to that approach.)\nIt was understood early on that, even in the large-sample, low-dimensional limit n \u2192 \u221e at p constant, supp(\u03b8\u0302n) 6= S unless the columns of X with index in S are roughly orthogonal to the ones with index outside S [KF00]. This assumption is formalized by the so-called \u2018irrepresentability condition\u2019, that can be stated in terms of the empirical covariance matrix \u03a3\u0302 = (XTX/n). Letting \u03a3\u0302A,B be the submatrix (\u03a3\u0302i,j)i\u2208A,j\u2208B, irrepresentability requires\n\u2016\u03a3\u0302Sc,S\u03a3\u0302\u22121S,S sign(\u03b80,S)\u2016\u221e \u2264 1\u2212 \u03b7 , (4)\nfor some \u03b7 > 0 (here sign(u)i = +1, 0, \u22121 if, respectively, ui > 0, = 0, < 0). In an early breakthrough, Zhao and Yu [ZY06] proved that, if this condition holds with \u03b7 uniformly bounded away from 0, it guarantees correct model selection also in the high-dimensional regime p n. Meinshausen and Bu\u0308lmann [MB06] independently established the same result for random Gaussian designs, with applications to learning Gaussian graphical models. These papers applied to very sparse models, requiring in particular s0 = O(n\nc), c < 1, and parameter vectors with large coefficients. Namely, scaling the columns of X such that \u03a3\u0302i,i \u2264 1, for i \u2208 [p], they require \u03b8min \u2261 mini\u2208S |\u03b80,i| \u2265 c \u221a s0/n.\nWainwright [Wai09] strengthened considerably these results by allowing for general scalings of s0, p, n and proving that much smaller non-zero coefficients can be detected. Namely, he proved that for a broad class of empirical covariances it is only necessary that \u03b8min \u2265 c\u03c3 \u221a (log p)/n. This scaling of the minimum non-zero entry is optimal up to constants. Also, for a specific classes of random Gaussian designs (including X with i.i.d. standard Gaussian entries), the analysis of [Wai09] provides tight bounds on the minimum sample size for correct model selection. Namely, there exists c`, cu > 0 such that the Lasso fails with high probability if n < c` s0 log p and succeeds with high probability if n \u2265 cu s0 log p.\nWhile, thanks to these recent works [ZY06, MB06, Wai09], we understand reasonably well model selection via the Lasso, it is fundamentally unknown what model-selection performances can be achieved with general computationally practical methods. Two aspects of of the above theory cannot be improved substantially: (i) The non-zero entries must satisfy the condition \u03b8min \u2265 c\u03c3/ \u221a n to be detected with high probability. Even if n = p and the measurement directions Xi are orthogonal, e.g., X = \u221a nIn\u00d7n, one would need |\u03b80,i| \u2265 c\u03c3/ \u221a n to distinguish the i-th entry from noise. For instance, in [JM13], the present authors prove a general upper bound on the minimax power of tests for hypotheses H0,i = {\u03b80,i = 0}. Specializing this bound to the case of standard Gaussian designs, the analysis of [JM13] shows formally that no test can detect \u03b80,i 6= 0, with a fixed degree of confidence, unless |\u03b80,i| \u2265 c\u03c3/ \u221a n. (ii) The sample size must satisfy n \u2265 s0. Indeed, if this is not the case, for each \u03b80 with support of size |S| = s0, there is a one parameter family {\u03b80(t) = \u03b80 + t v}t\u2208R with supp(\u03b80(t)) \u2286 S, X\u03b80(t) = X\u03b80 and, for specific values of t, the support of \u03b80(t) is strictly contained in S.\nOn the other hand, there is no fundamental reason to assume the irrepresentability condition (4). This follows from the requirement that a specific method (the Lasso) succeeds, but is unclear why it should be necessary in general. The situation is very different for estimation consistency, e.g., for characterizing the `2 error \u2016\u03b8\u0302\u2212 \u03b80\u20162. In that case the restricted isometry property (RIP) [CT05] (or one of its relaxations [BRT09, vdGB09]) is sufficient and \u2013essentially\u2013 necessary.\nGauss-Lasso selector: Model selector for high dimensional problems\nInput: Measurement vector y, design model X, regularization parameter \u03bb, support size s0. Output: Estimated support S\u0302. 1: Let T = supp(\u03b8\u0302n) be the support of Lasso estimator \u03b8\u0302n = \u03b8\u0302n(y,X, \u03bb) given by\n\u03b8\u0302n(Y,X;\u03bb) \u2261 arg min \u03b8\u2208Rp { 1 2n \u2016Y \u2212X\u03b8\u201622 + \u03bb\u2016\u03b8\u20161 } .\n2: Construct the estimator \u03b8\u0302GL as follows:\n\u03b8\u0302GLT = (X T TXT ) \u22121XTT y , \u03b8\u0302 GL T c = 0 .\n3: Find s0-th largest entry (in modulus) of \u03b8\u0302 GL T , denoted by \u03b8\u0302 GL (s0) , and let\nS\u0302 \u2261 { i \u2208 [p] : |\u03b8\u0302GLi | \u2265 |\u03b8\u0302GL(s0)| } .\nIn this paper we prove that the Gauss-Lasso selector has nearly optimal model selection properties under a condition that is strictly weaker than irrepresentability. We call this condition the generalized irrepresentability condition (GIC). The Gauss-Lasso procedure uses the Lasso estimator to estimate a first model T \u2286 {1, . . . , p}. It then constructs a new estimator by ordinary least squares regression of the data Y onto the model T .\nWe prove that the estimated model is, with high probability, correct (i.e., S\u0302 = S) under conditions comparable to the ones assumed in [MB06, ZY06, Wai09], while replacing irrepresentability by the weaker generalized irrepresentability condition. In the case of random Gaussian designs, our analysis further assumes the restricted eigenvalue property in order to establish a nearly optimal scaling of the sample size n with the sparsity parameter s0.\nIn order to build some intuition about the difference between irrepresentability and generalized irrepresentability, it is convenient to consider the Lasso cost function at \u2018zero noise\u2019:\nG(\u03b8; \u03be) \u2261 1 2n \u2016X(\u03b8 \u2212 \u03b80)\u201622 + \u03be\u2016\u03b8\u20161\n= 1\n2 \u3008(\u03b8 \u2212 \u03b80), \u03a3\u0302(\u03b8 \u2212 \u03b80)\u3009+ \u03be\u2016\u03b8\u20161 .\nLet \u03b8\u0302ZN(\u03be) be the minimizer of G( \u00b7 ; \u03be) and v \u2261 lim\u03be\u21920+ sign(\u03b8\u0302ZN(\u03be)). The limit is well defined by Lemma 2.2 below. The KKT conditions for \u03b8\u0302ZN imply, for T \u2261 supp(v),\n\u2016\u03a3\u0302T c,T \u03a3\u0302\u22121T,T vT \u2016\u221e \u2264 1 .\nSince G( \u00b7 ; \u03be) has always at least one minimizer, this condition is always satisfied. Generalized irrepresentability requires that the above inequality holds with some small slack \u03b7 > 0 bounded away from zero, i.e.,\n\u2016\u03a3\u0302T c,T \u03a3\u0302\u22121T,T vT \u2016\u221e \u2264 1\u2212 \u03b7 .\nNotice that this assumption reduces to standard irrepresentability cf. Eq. (4) if, in addition, we ask that v = sign(\u03b80). In other words, earlier work [MB06, ZY06, Wai09] required generalized irrepresentability plus sign-consistency in zero noise, and established sign consistency in non-zero noise. In this paper the former condition is shown to be sufficient.\nFrom a different point of view, GIC demands that irrepresentability holds for a superset of the true support S. It was indeed argued in the literature that such a relaxation of irrepresentability allows to cover a significantly broader set of cases (see for instance [BvdG11, Section 7.7.6]). However, it was never clarified why such a superset irrepresentability condition should be significantly more general than simple irrepresentability. Further, no precise prescription existed for the superset of the true support.\nOur contributions can therefore be summarized as follows:\n1. By tying it to the KKT condition for the zero-noise problem, we justify the expectation that generalized irrepresentability should hold for a broad class of design matrices.\n2. We thus provide a specific formulation of superset irrepresentability, prescribing both the superset T and the sign vector vT , that is \u2013by itself\u2013 significantly more general than simple irrepresentability.\n3. We show that, under GIC, exact support recovery can be guaranteed using the Gauss-Lasso, and formulate the appropriate \u2018minimum coefficient\u2019 conditions that guarantee this.\nAs a side remark, even when simple irrepresentability holds, our results strengthen somewhat the estimates of [Wai09] (see below for details).\nThe paper is organized as follows. In the rest of the introduction we illustrate the range of applicability of GIC through a simple example and we discuss further related work. We finally introduce the basic notations to be used throughout the paper.\nSection 2 treats the case of deterministic designs X, and develops our main results on the basis of the GIC. Section 3 extends our analysis to the case of random designs. In this case GIC is required to hold for the population covariance, and the analysis is more technical as it requires to control the randomness of the design matrix. The proofs of our main results can be found in Sections 5 and 6, with several technical steps deferred to the Appendices."}, {"heading": "1.1 An example", "text": "In order to illustrate the range of new cases covered by our results, it is instructive to consider a simple example. A detailed discussion of this calculation can be found in Appendix B. The example corresponds to a Gaussian random design, i.e., the rows XT1 , . . .X T n are i.i.d. realizations of a pvariate normal distribution with mean zero. We write Xi = (Xi,1, Xi,2, . . . , Xi,p) T for the components of Xi. The response variable is linearly related to the first s0 covariates\nYi = \u03b80,1Xi,1 + \u03b80,2Xi,2 + \u00b7 \u00b7 \u00b7+ \u03b80,s0Xi,s0 +Wi ,\nwhere Wi \u223c N(0, \u03c32) and we assume \u03b80,i > 0 for all i \u2264 s0. In particular S = {1, . . . , s0}. As for the design matrix, first p\u2212 1 covariates are orthogonal at the population level, i.e., Xi,j \u223c N(0, 1) are independent for 1 \u2264 j \u2264 p\u2212 1 (and 1 \u2264 i \u2264 n). However the p-th covariate is correlated\nto the s0 relevant ones:\nXi,p = aXi,1 + aXi,2 + \u00b7 \u00b7 \u00b7+ aXi,s0 + b X\u0303i,p .\nHere X\u0303i,p \u223c N(0, 1) is independent from {Xi,1, . . . , Xi,p\u22121} and represents the orthogonal component of the p-th covariate. We choose the coefficients a, b \u2265 0 such that s0a2 +b2 = 1, whence E{X2i,p} = 1 and hence the p-th covariate is normalized as the first (p \u2212 1) ones. In other words, the rows of X are i.i.d. Gaussian Xi \u223c N(0,\u03a3) with covariance given by\n\u03a3ij =  1 if i = j,\na if i = p, j \u2208 S or i \u2208 S, j = p, 0 otherwise.\nFor a = 0, this is the standard i.i.d. design and irrepresentability holds. The Lasso correctly recovers the support S from n \u2265 c s0 log p samples, provided \u03b8min \u2265 c\u2032 \u221a (log p)/n. It follows from [Wai09] that this remains true as long as a \u2264 (1 \u2212 \u03b7)/s0 for some \u03b7 > 0 bounded away from 0. However, as soon as a > 1/s0, the Lasso includes the p-th covariate in the estimated model, with high probability (see Appendix B).\nAs it is shown in Appendix B, the Gauss-Lasso is successful for a significantly larger set of values of a. Namely, if\na \u2208 [ 0,\n1\u2212 \u03b7 s0\n] \u222a ( 1\ns0 , 1\u2212 \u03b7 \u221a s0\n] ,\nthen it recovers S from n \u2265 c s0 log p samples, provided \u03b8min \u2265 c\u2032 \u221a\n(log p)/n. While the interval ((1\u2212\u03b7)/s0, 1/s0] is not covered by this result, we expect this to be due to the proof technique rather than to an intrinsic limitation of the Gauss-Lasso selector."}, {"heading": "1.2 Further related work", "text": "The restricted isometry property [CT05, CT07] (or the related restricted eigenvalue [BRT09] or compatibility conditions [vdGB09]) have been used to establish guarantees on the estimation and model selection errors of the Lasso or similar approaches. In particular, Bickel, Ritov and Tsybakov [BRT09] show that, under such conditions, with high probability,\n\u2016\u03b8\u0302 \u2212 \u03b80\u201622 \u2264 C\u03c32 s0 log p\nn .\nThe same conditions can be used to prove model-selection guarantees. In particular, Zhou [Zho10] studies a multi-step thresholding procedure whose first steps coincide with the Gauss-Lasso. While the main objective of this work is to prove high-dimensional `2 consistency with a sparse estimated model, the author also proves partial model selection guarantees. Namely, the method correctly recovers a subset of large coefficients SL \u2286 S, provided |\u03b80,i| \u2265 c\u03c3 \u221a s0(log p)/n, for i \u2208 SL. This means that the coefficients that are guaranteed to be detected must be a factor \u221a s0 larger than what is required by our results. Also related to model selection is the recent line of work on hypothesis testing in high-dimensional regression [ZZ11, Bu\u0308h12]. These papers propose methods for testing hypotheses of the form H0,i =\n{\u03b80,i = 0}. In order to achieve a given significance level, they require \u2013again\u2013 large coefficients, namely |\u03b80,i| \u2265 c\u03c3 \u221a s0(log p)/n (see [JM13] for a discussion of this point). In [JM13], we investigate a hypothesis testing method that achieves any given significance level \u03b1 for |\u03b80,i| \u2265 c\u03c3/ \u221a n, with c a constant that depends on \u03b1. Although the testing procedure can be used for general setting, the guarantee on its statistical power is provided only for some random Gaussian designs in an asymptotic sense. A very recent paper by van de Geer, Bu\u0308hlmann and Ritov [vdGBR13] proposes a similar procedure and gives conditions under which the procedure achieves the semiparametric efficiency bound. Their analysis allows for general Gaussian and sub-Gaussian designs. However, it requires a sample size n \u2265 C(s0 log p)2, namely the square of the optimal sample size.\nLet us finally mention that an alternative approach to establishing model-selection guarantees assumes a suitable mutual incoherence conditions. Lounici [Lou08] proves correct model selection under the assumption maxi 6=j |\u03a3\u0302ij | = O(1/s0). This assumption is however stronger than irrepresentability [vdGB09]. Cande\u0301s and Plan [CP09] also assume mutual incoherence, albeit with a much weaker requirement, namely maxi 6=j |\u03a3\u0302ij | = O(1/(log p)). Under this condition, they establish model selection guarantees for an ideal scaling of the non-zero coefficients \u03b8min \u2265 c\u03c3 \u221a (log p)/n. However, this result only holds with high probability for a \u2018random signal model\u2019 in which the non-zero coefficients \u03b80,i have uniformly random signs.\nFinally, model selection consistency can be obtained without irrepresentability through other methods. For instance [Zou06] develops the adaptive Lasso, using a data-dependent weighted `1 regularization, and [Bac08] proposes the Bolasso, a resampling-based techniques. Unfortunately, both of these approaches are only guaranteed to succeed in the low-dimensional regime of p fixed, and n\u2192\u221e."}, {"heading": "1.3 Notations", "text": "We provide a brief summary of the notations used throughout the paper. For a matrix A and set of indices I, J , we let AJ denote the submatrix containing just the columns in J and AI,J denote the submatrix formed by the rows in I and columns in J . Likewise, for a vector v, vI is the restriction of v to indices in I. Further, the notation A\u22121I,I represents the inverse of AI,I , i.e., A \u22121 I,I = (AI,I)\n\u22121. The maximum and the minimum singular values of A are respectively denoted by \u03c3max(A) and \u03c3min(A). We write \u2016v\u2016p for the standard `p norm of a vector v. Specifically, \u2016v\u20160 denotes the number of nonzero entries in v. Also, \u2016A\u2016p refers to the induced operator norm on a matrix A. We use ei to refer to the i-th standard basis element, e.g., e1 = (1, 0, . . . , 0). For a vector v, supp(v) represents the positions of nonzero entries of v. Throughout, we denote the rows of the design matrix X by X1, . . . , Xn \u2208 Rp and denote its columns by x1, . . . , xp \u2208 Rn. Further, for a vector v, sign(v) is the vector with entries sign(v)i = +1 if vi > 0, sign(v)i = \u22121 if vi < 0, and sign(v)i = 0 otherwise."}, {"heading": "2 Deterministic designs", "text": "An outline of this section is given below:\n1. We first consider the zero-noise problem W = 0, and prove several useful properties of the Lasso estimator in this case. In particular, we show that there exists a threshold for the regularization parameter below which the support of the Lasso estimator remains the same and contains supp(\u03b80). Moreover, the Lasso estimator support is not much larger than supp(\u03b80).\n2. We then turn to the noisy problem, and introduce the generalized irrepresentability condition (GIC) that is motivated by the properties of the Lasso in the zero-noise case. We prove that under GIC (and other technical conditions), with high probability, the signed support of the Lasso estimator is the same as that in the zero-noise problem.\n3. We show that the Gauss-Lasso selector correctly recovers the signed support of \u03b80."}, {"heading": "2.1 Zero-noise problem", "text": "Recall that \u03a3\u0302 \u2261 (XTX/n) denotes the empirical covariance of the rows of the design matrix. Given \u03a3\u0302 \u2208 Rp\u00d7p, \u03a3\u0302 0, \u03b80 \u2208 Rp and \u03be \u2208 R+, we define the zero-noise Lasso estimator as\n\u03b8\u0302ZN(\u03be) \u2261 arg min \u03b8\u2208Rp { 1 2n \u3008(\u03b8 \u2212 \u03b80), \u03a3\u0302(\u03b8 \u2212 \u03b80)\u3009+ \u03be\u2016\u03b8\u20161 } . (5)\nNote that \u03b8\u0302ZN(\u03be) is obtained by letting Y = X\u03b80 in the definition of \u03b8\u0302 n(Y,X; \u03be).\nFollowing [BRT09], we introduce a restricted eigenvalue constant for the empirical covariance matrix \u03a3\u0302:\n\u03ba\u0302(s, c0) \u2261 min J\u2286[p] |J |\u2264s\nmin u\u2208Rp\n\u2016uJc\u20161\u2264c0\u2016uJ\u20161\n\u3008u, \u03a3\u0302u\u3009 \u2016u\u201622 . (6)\nOur first result states that the support of \u03b8\u0302ZN(\u03be) is not much larger than the support of \u03b80, for any \u03be > 0.\nLemma 2.1. Let \u03b8\u0302ZN = \u03b8\u0302ZN(\u03be) be defined as per Eq. (17), with \u03be > 0. Then, if s0 = \u2016\u03b80\u20160,\n\u2016\u03b8\u0302ZN\u20160 \u2264 ( 1 +\n4\u2016\u03a3\u0302\u20162 \u03ba\u0302(s0, 1)\n) s0 . (7)\nThe proof of this lemma is deferred to Section A.1.\nLemma 2.2. Let \u03b8\u0302ZN = \u03b8\u0302ZN(\u03be) be defined as per Eq. (5), with \u03be > 0. Then there exist \u03be0 = \u03be0(\u03a3\u0302, S, \u03b80) > 0, T0 \u2286 [p], v0 \u2208 {\u22121, 0,+1}p, such that the following happens. For all \u03be \u2208 (0, \u03be0), sign(\u03b8\u0302ZN(\u03be)) = v0 and supp(\u03b8\u0302\nZN(\u03be)) = supp(v0) = T0. Further T0 \u2287 S, v0,S = sign(\u03b80,S) and \u03be0 = mini\u2208S |\u03b80,i/[\u03a3\u0302\u22121T0,T0v0,T0 ]i|.\nProof of Lemma 2.2 can be found in Section A.2. Finally we have the following standard characterization of the solution of the zero-noise problem.\nLemma 2.3. Let \u03b8\u0302ZN = \u03b8\u0302ZN(\u03be) be defined as per Eq. (5), with \u03be > 0. Let T \u2287 S and v \u2208 {+1, 0,\u22121}p be such that supp(v) = T . Then sign(\u03b8\u0302ZN) = v if and only if\u2225\u2225\u2225\u03a3\u0302T c,T \u03a3\u0302\u22121T,T vT\u2225\u2225\u2225\u221e \u2264 1 , (8)\nvT = sign ( \u03b80,T \u2212 \u03be\u03a3\u0302\u22121T,T vT ) . (9)\nFurther, if the above holds, \u03b8\u0302ZN is given by \u03b8\u0302ZNT c = 0 and\n\u03b8\u0302ZNT = \u03b80,T \u2212 \u03be\u03a3\u0302\u22121T,T vT .\nLemma 2.3 is proved in Appendix A.3. Motivated by this result, we introduce the generalized irrepresentability condition (GIC) for\ndeterministic designs.\nGeneralized irrepresentability (deterministic designs). The pair (\u03a3\u0302, \u03b80), \u03a3\u0302 \u2208 Rp\u00d7p, \u03b80 \u2208 Rp satisfy the generalized irrepresentability condition with parameter \u03b7 > 0 if the following happens. Let v0, T0 be defined as per Lemma 2.2. Then\u2225\u2225\u2225\u03a3\u0302T c0 ,T0\u03a3\u0302\u22121T0,T0v0,T0\u2225\u2225\u2225\u221e \u2264 1\u2212 \u03b7 . (10)\nIn other words we require the dual feasibility condition (8) \u2013which always holds\u2013 to hold with a positive slack \u03b7."}, {"heading": "2.2 Noisy problem", "text": "Consider the noisy linear observation model as described in (2), and let r\u0302 \u2261 (XTW/n). We begin with a standard characterization of sign(\u03b8\u0302n), the signed support of the Lasso estimator (3).\nLemma 2.4. Let \u03b8\u0302n = \u03b8\u0302n(y,X;\u03bb) be defined as per Eq. (3), and let z \u2208 {+1, 0,\u22121}p with supp(z) = T . Further assume T \u2287 S. Then the signed support of the Lasso estimator is given by sign(\u03b8\u0302n) = z if and only if \u2225\u2225\u2225\u03a3\u0302T c,T \u03a3\u0302\u22121T,T zT + 1\u03bb(r\u0302T c \u2212 \u03a3\u0302T c,T \u03a3\u0302\u22121T,T r\u0302T )\u2225\u2225\u2225\u221e \u2264 1 , (11)\nzT = sign ( \u03b80,T \u2212 \u03a3\u0302\u22121T,T (\u03bbzT \u2212 r\u0302T ) ) . (12)\nLemma 2.4 is proved in Appendix A.4.\nTheorem 2.5. Consider the deterministic design model with empirical covariance matrix \u03a3\u0302 \u2261 (XTX)/n, and assume that \u03a3\u0302i,i \u2264 1 for i \u2208 [p]. Let T0 \u2286 [p], v0 \u2208 {+1, 0,\u22121}p be the set and vector defined in Lemma 2.2, and t0 \u2261 |T0|. Assume that\n(i) We have \u03c3min(\u03a3\u0302T0,T0) \u2265 Cmin > 0.\n(ii) The pair (\u03a3\u0302, \u03b80) satisfies the generalized irrepresentability condition with parameter \u03b7.\nConsider the Lasso estimator \u03b8\u0302n = \u03b8\u0302n(y,X;\u03bb) defined as per Eq. (3), with regularization parameter\n\u03bb = \u03c3\n\u03b7\n\u221a 2c1 log p\nn , (13)\nfor some constant c1 > 1, and suppose that\n(iii) For some c2 > 0:\n|\u03b80,i| \u2265 c2\u03bb+ \u03bb \u2223\u2223[\u03a3\u0302\u22121T0,T0v0,T0 ]i\u2223\u2223 for all i \u2208 S, (14)\u2223\u2223[\u03a3\u0302\u22121T0,T0v0,T0 ]i\u2223\u2223 \u2265 c2 for all i \u2208 T0 \\ S. (15)\nWe further assume, without loss of generality, \u03b7 \u2264 c2 \u221a Cmin. Then the following holds true:\nP { sign(\u03b8\u0302n(\u03bb)) = v0 } \u2265 1\u2212 4p1\u2212c1 . (16)\nTheorem 2.5 is proved in Section 5.1. Note that, even in the case standard irrepresentability holds (and hence T0 = S), this result improves over [Wai09, Theorem 1.(b)], in that the required lower bound for |\u03b80,i|, i \u2208 S, does not depend on \u2016\u03a3\u0302S,S\u2016\u221e. More precisely, Theorem 2.5 assumes |\u03b80,i| \u2265 \u03bb(c2 + |[\u03a3\u0302\u22121S,Sv0,S ]i|), for i \u2208 S, which is weaker than the assumption of Theorem1.(b)[Wai09], namely, |\u03b80,i| \u2265 \u03bb(c+ \u2016\u03a3\u0302\u22121S,S\u2016\u221e), since \u2016v0,S\u2016\u221e \u2264 1.\nRemark 2.6. Condition (i) in Theorem 2.5 requires the submatrix \u03a3\u0302T0,T0 to have minimum singular value bounded away form zero. Assuming \u03a3\u0302S,S to be non-singular is necessary for identifiability. Requiring the minimum singular value of \u03a3\u0302T0,T0 to be bounded away from zero is not much more restrictive since T0 is comparable in size with S, as stated in Lemma 2.1.\nWe next show that the Gauss-Lasso selector correctly recovers the support of \u03b80.\nTheorem 2.7. Consider the deterministic design model with empirical covariance matrix \u03a3\u0302 \u2261 (XTX)/n, and assume that \u03a3\u0302i,i \u2264 1 for i \u2208 [p]. Under the assumptions of Theorem 2.5,\nP ( \u2016\u03b8\u0302GL \u2212 \u03b80\u2016\u221e \u2265 \u00b5 ) \u2264 4p1\u2212c1 + 2pe\u2212nCmin\u00b52/2\u03c32 .\nIn particular, if S\u0302 is the model selected by the Gauss-Lasso, we have\nP(S\u0302 = S) \u2265 1\u2212 6 p1\u2212c1/4 .\nThe proof of Theorem 2.7 is given in Section 5.2."}, {"heading": "3 Random Gaussian designs", "text": "In the previous section, we studied the case of deterministic design models which allowed for a straightforward analysis. Here, we consider the random design model which needs a more involved analysis. Within the random Gaussian design model, the rows Xi are distributed as Xi \u223c N(0,\u03a3) for some (unknown) covariance matrix \u03a3 0.\nIn order to study the performance of Gauss-Lasso selector in this case, we first define the population-level estimator. Given \u03a3 \u2208 Rp\u00d7p, \u03a3 0, \u03b80 \u2208 Rp and \u03be \u2208 R+, the population-level estimator \u03b8\u0302\u221e(\u03be) = \u03b8\u0302\u221e(\u03be; \u03b80,\u03a3) is defined as\n\u03b8\u0302\u221e(\u03be) \u2261 arg min \u03b8\u2208Rp {1 2 \u3008(\u03b8 \u2212 \u03b80),\u03a3(\u03b8 \u2212 \u03b80)\u3009+ \u03be\u2016\u03b8\u20161 } . (17)\nNotice that the minimizer is unique because \u03a3 is strictly positive definite and hence the cost function on the right-hand side is strongly convex. In fact, the population-level estimator is obtained by assuming that the response vector Y is noiseless and n =\u221e, hence replacing the empirical covariance (XTX/n) with the exact covariance \u03a3 in the lasso optimization problem (3).\nNotice that the population-level estimator \u03b8\u0302\u221e is deterministic, albeit X is a random design. We show that under some conditions on the covariance \u03a3 and vector \u03b80, T \u2261 supp(\u03b8\u0302n) = supp(\u03b8\u0302\u221e), i.e.,\nthe population-level estimator and the Lasso estimator share the same (signed) support. Further T \u2287 S. Since \u03b8\u0302\u221e (and hence T ) is deterministic, XT is a Gaussian matrix with rows drawn independently from N(0,\u03a3T,T ). This observation allows for a simple analysis of the Gauss-Lasso selector \u03b8\u0302\nGL. An outline of the section is given below:\n1. We begin with proving several properties of the population-level estimator. Similar to the zero-noise problem in Section 2.1, we show that there exists a threshold \u03be0, such that for all \u03be \u2208 (0, \u03be0), supp(\u03b8\u0302\u221e(\u03be)) remains the same and contains supp(\u03b80). Moreover, supp(\u03b8\u0302\u221e(\u03be)) is not much larger than supp(\u03b80).\n2. We show that under GIC for covariance matrix \u03a3 (and other sufficient conditions), with high probability, the signed support of the Lasso estimator is the same as the signed support of the population-level estimator.\n3. Following the previous steps, we show that the Gauss-Lasso selector correctly recovers the signed support of \u03b80."}, {"heading": "3.1 The n =\u221e problem", "text": "In this section we derive several useful properties of the population-level problem (17). Comparing Eqs. (5) and (17), the estimators \u03b8\u0302ZN(\u03be) and \u03b8\u0302\u221e(\u03be) are defined in a very similar manner (the former is defined with respect to \u03a3\u0302 and the latter is defined with respect to \u03a3), and as we will see \u03b8\u0302\u221e also possesses the properties stated in Section 2.1.\nLet \u03ba\u221e(s, c0) be the restricted eigenvalue constant for the covariance matrix \u03a3:\n\u03ba(s, c0) \u2261 min J\u2286[p] |J |\u2264s\nmin u\u2208Rp\n\u2016uJc\u20161\u2264c0\u2016uJ\u20161\n\u3008u,\u03a3u\u3009 \u2016u\u201622 . (18)\nThe proofs of the following Lemmas are very similar to the corresponding ones in Section 2.1, and are omitted.\nLemma 3.1. Let \u03b8\u0302\u221e = \u03b8\u0302\u221e(\u03be) be defined as per Eq. (17), with \u03be > 0. Then, if s0 = \u2016\u03b80\u20160, \u2016\u03b8\u0302\u221e\u20160 \u2264 (\n1 + 4\u2016\u03a3\u20162 \u03ba(s0, 1)\n) s0 . (19)\nLemma 3.2. Let \u03b8\u0302\u221e = \u03b8\u0302\u221e(\u03be) be defined as per Eq. (17), with \u03be > 0. Then there exist \u03be0 = \u03be0(\u03a3, S, \u03b80) > 0, T0 \u2286 [p], v0 \u2208 {\u22121, 0,+1}p, such that the following happens. For all \u03be \u2208 (0, \u03be0), sign(\u03b8\u0302\u221e(\u03be)) = v0 and supp(\u03b8\u0302\n\u221e(\u03be)) = supp(v0) = T0. Further T0 \u2287 S, v0,S = sign(\u03b80,S) and \u03be0 = mini\u2208S |\u03b80,i/[\u03a3\u22121T0,T0v0,T0 ]i|.\nFinally we have the following standard characterization of the solution of the n = \u221e problem (17).\nLemma 3.3. Let \u03b8\u0302\u221e = \u03b8\u0302\u221e(\u03be) be defined as per Eq. (17), with \u03be > 0. Let T \u2287 S and v \u2208 {+1, 0,\u22121}p be such that supp(v) = T . Then sign(\u03b8\u0302\u221e) = v if and only if\u2225\u2225\u2225\u03a3T c,T\u03a3\u22121T,T vT\u2225\u2225\u2225\u221e \u2264 1 ,\nvT = sign ( \u03b80,T \u2212 \u03be\u03a3\u22121T,T vT ) .\nFurther, if the above holds, \u03b8\u0302\u221e is given by \u03b8\u0302\u221eT c = 0 and\n\u03b8\u0302\u221eT = \u03b80,T \u2212 \u03be\u03a3\u22121T,T vT .\nMotivated by this result, we introduce the following assumption.\nGeneralized irrepresentability (random designs). The pair (\u03a3, \u03b80), \u03a3 \u2208 Rp\u00d7p, \u03b80 \u2208 Rp satisfy the generalized irrepresentability condition with parameter \u03b7 > 0 if the following happens. Let v0, T0 be defined as per Lemma 3.2. Then\u2225\u2225\u2225\u03a3T c0 ,T0\u03a3\u22121T0,T0v0,T0\u2225\u2225\u2225\u221e \u2264 1\u2212 \u03b7 , (20)"}, {"heading": "3.2 The high-dimensional problem", "text": "We now consider the Lasso estimator (3). Recall the notations\n\u03a3\u0302 \u2261 1 n XTX , r\u0302 \u2261 1 n XTW .\nNote that \u03a3\u0302 \u2208 Rp\u00d7p, r\u0302 \u2208 Rp are both random quantities in the case of random designs.\nTheorem 3.4. Consider the Gaussian random design model with covariance matrix \u03a3 0, and assume that \u03a3i,i \u2264 1 for i \u2208 [p]. Let T0 \u2286 [p], v0 \u2208 {+1, 0,\u22121}p be the deterministic set and vector defined in Lemma 3.2, and t0 \u2261 |T0|. Assume that\n(i) We have \u03c3min(\u03a3T0,T0) \u2265 Cmin > 0.\n(ii) The pair (\u03a3, \u03b80) satisfies the generalized irrepresentability condition with parameter \u03b7.\nConsider the Lasso estimator \u03b8\u0302n = \u03b8\u0302n(y,X;\u03bb) defined as per Eq. (3), with regularization parameter\n\u03bb = 4\u03c3\n\u03b7\n\u221a c1 log p\nn , (21)\nfor some constant c1 > 1, and suppose that\n(iii) For some c2 > 0:\n|\u03b80,i| \u2265 c2\u03bb+ 3 2 \u03bb \u2223\u2223[\u03a3\u22121T0,T0v0,T0 ]i\u2223\u2223 for all i \u2208 S, (22)\u2223\u2223[\u03a3\u22121T0,T0v0,T0 ]i\u2223\u2223 \u2265 2c2 for all i \u2208 T0 \\ S. (23)\nWe further assume, without loss of generality, \u03b7 \u2264 c2 \u221a Cmin.\nIf n \u2265 max(M1,M3)t0 log p with\nM1 \u2261 74c1 \u03b72Cmin , M3 \u2261 322c1 c22C 2 min ,\nthen the following holds true: P { sign(\u03b8\u0302n(\u03bb)) = v0 } \u2265 1\u2212 pe\u2212 n 10 \u2212 6e\u2212 t0 2 \u2212 8p1\u2212c1 . (24)\nUnder standard irrepresentability, this result improves over [Wai09, Theorem 3.(ii)], in that the required lower bound for |\u03b80,i|, i \u2208 S, does not depend on \u2016\u03a3\u22121/2S,S \u2016\u221e. More precisely, Theorem 2.5 assumes |\u03b80,i| \u2265 \u03bb(c2 + 1.5|[\u03a3\u22121S,Sv0,S ]i|), for i \u2208 S, while Theorem 3.(ii)[Wai09] requires |\u03b80,i| \u2265 c\u03bb\u2016\u03a3\u22121/2S,S \u20162\u221e, for i \u2208 S. Note that |[\u03a3 \u22121 S,Sv0,S ]i| \u2264 \u2016\u03a3 \u22121 S,S\u2016\u221e \u2264 \u2016\u03a3 \u22121/2 S,S \u20162\u221e.\nWhile being closely analogous to Theorem 2.5, the last theorem has somewhat worse constants. Indeed in the present case we need to control the randomness of the design matrix X in addition to the one of the noise.\nRemark 3.5. Condition (i) follows readily from the restricted eigenvalue constraint as in Eq. (18), i.e., \u03ba\u221e(t0, 0) > 0. This is a reasonable assumption since T0 is not much larger than S0, as stated in Lemma 3.1.\nCorollary 3.6. Under the assumptions of Theorem 3.4, if n \u2265 max(M\u03031, M\u03033)s0 log p, with\nM\u03031 = ( 1 + 4\u2016\u03a3\u20162\n\u03ba\u221e(s0, 1)\n) M1 , M\u03033 = ( 1 +\n4\u2016\u03a3\u20162 \u03ba\u221e(s0, 1)\n) M3 ,\nthen the following holds: P { sign(\u03b8\u0302n(\u03bb)) = v0 } \u2265 1\u2212 pe\u2212 n 10 \u2212 6e\u2212 s0 2 \u2212 8p1\u2212c1 .\nProof (Corollary 3.6). The result follows readily from Theorem 3.4, noting that s0 \u2264 t0 since S0 \u2286 T0, and t0 \u2264 (1 + 4\u2016\u03a3\u20162/\u03ba\u221e(s0, 1))s0 as per Lemma 3.1.\nBelow, we show that the Gauss-Lasso selector correctly recovers the signed support of \u03b80.\nTheorem 3.7. Consider the random Gaussian design model with covariance matrix \u03a3 0, and assume that \u03a3i,i \u2264 1 for i \u2208 [p]. Under the assumptions of Theorem 3.4, and for n \u2265 max(M\u03031, M\u03033)s0 log p, we have\nP ( \u2016\u03b8\u0302GL \u2212 \u03b80\u2016\u221e \u2265 \u00b5 ) \u2264 pe\u2212 n 10 + 6e\u2212 s0 2 + 8p1\u2212c1 + 2pe\u2212nCmin\u00b5 2/2\u03c32 .\nMoreover, letting S\u0302 be the model returned by the Gauss-Lasso selector, we have\nP(S\u0302 = S) \u2265 1\u2212 p e\u2212 n 10 \u2212 6 e\u2212 s0 2 \u2212 10 p1\u2212c1 .\nThe proof of Theorem 3.7 is deferred to Section 6.4.\nRemark 3.8. [Detection level] Let \u03b8min \u2261 mini\u2208S |\u03b80,i| be the minimum magnitude of the nonzero entries of vector \u03b80. By Theorem 3.7, Gauss-Lasso selector correctly recovers supp(\u03b80), with probability greater than 1\u2212 p e\u2212 n 10 \u2212 6 e\u2212 s0 2 \u2212 10 p1\u2212c1, if n \u2265 max(M\u03031, M\u03033)s0 log p, and\n\u03b8min \u2265 C\u03c3 \u221a log p\nn\n( 1 + \u2016\u03a3\u22121T0,T0\u2016\u221e ) , (25)\nwhere C = C(c1, c2, \u03b7) is a constant depending on c1, c2, and \u03b7. Eq. (25) stems from the condition (iii) in Theorem 3.4.\nWe can further generalize this result. Define\nS1 = { i \u2208 S : |\u03b80,i| \u2265 C\u03c3 \u221a log p\nn\n( 1 + \u2016\u03a3\u22121T0,T0\u2016\u221e )} ,\nand S2 = S\\S1. By a very similar argument to the proof of Theorem 3.4, the Gauss-Lasso selector can recover S1, if \u2016\u03b80,S2\u2016 = O(\u03c3 \u221a log p/n). More precisely, letting W\u0303 = X\u03b80,S2 + W , the response vector Y can be recast as Y = X\u03b80,S1 + W\u0303 and the Gauss-Lasso selector treats the small entries \u03b80,S2 as noise."}, {"heading": "4 UCI communities and crimes data example", "text": "We consider a problem about predicting the rate of violent crimes in different communities within US, based on other demographic attributes of the communities. We evaluate the performance of the Gauss-Lasso selector on the UCI communities and crimes dataset [FA10]. The dataset consists of a univariate response variable and 122 predictive attributes for 1994 communities. The response variable is the total number of violent crimes per 100K population. Covariates are quantitative, including e.g., the average family income, the fraction of unemployed population, and the police operating budget. We consider a linear model as in (2) and perform model selection using GaussLasso selector and Lasso estimator.\nWe do the following preprocessing steps: (i) Each missing value is replaced by the mean of the non-missing values of that attribute for other communities; (ii) We eliminate 16 attributes to make the ensemble of the attribute vectors linearly independent; (iii) We normalize the columns to have mean zero and `2 norm \u221a n. Thus we obtain a design matrix Xtot \u2208 Rntot\u00d7p with ntot = 1994 and p = 106. For the sake of performance evaluation, we need to know the true model, i.e., the true significant covariates. We let \u03b80 = (X T totXtot)\n\u22121XTtoty be the least square solution obtained from the whole dataset Xtot. The entries of \u03b80 are shown in Fig. 1. Clearly only a few of them are non negligible,\ncorresponding to the true model. We treat the entries with magnitude larger than 0.04 as truly active and the others as truly inactive. The number of active covariates according to this criterion is s0 = 13.\nWe choose random subsamples of size n = 85 from the communities and normalize each column of the resulting design matrix to have mean zero and `2 norm \u221a n. We use Gauss-Lasso selector and Lasso for model selection based on this design. Figures 2 and 3 respectively show the solution path for Gauss-Lasso and Lasso as the parameter \u03bb changes form \u03bb = 0.001 to \u03bb = 1. The paths corresponding to the truly active set are in black and the paths corresponding to the truly inactive variables are in red. At \u03bb = 1, the solutions \u03b8\u0302GL(\u03bb) and \u03b8\u0302n(\u03bb) have no active variables; for decreasing \u03bb, each knot \u03bbk marks the entry or removal of some variables from the current active set of the Lasso solution. Therefore, the support of the Lasso solution T remains constant in between knots. Since Gauss-Lasso selector performs ordinary least squares restricted to T , its coordinate paths are constant in between knots. However, the Lasso paths are linear with respect to \u03bb, with changes in slope at the knots (see e.g., [EHJT04] for a discussion).\nIt is clear from Figure 3 that the Lasso support either misses a large fraction of the truly active covariates, or includes many false positives. For instance at \u03bb = 0.08, we get 4 true positives out of 13 and 4 false positives. On the other hand, for a smaller value of the regularization parameter, \u03bb = 0.01, we get 10 true positives out of 13 and 8 false positives.1\nIf we consider on the other hand the Gauss-Lasso, any \u03bb \u2264 0.02 produces a set of coefficients with a gap between large ones, that are mostly true positives, and small ones, that are mostly true negatives."}, {"heading": "5 Proof of Theorems 2.5 and 2.7", "text": "In this section we prove Theorems 2.5 and 2.7 using Lemmas 2.1 to 2.4. The latter are proved in the appendices."}, {"heading": "5.1 Proof of Theorem 2.5", "text": "By the condition (iii) in the statement of the theorem, we have\n\u03bb < min i\u2208S \u2223\u2223\u2223\u2223\u2223 \u03b80,i[\u03a3\u0302\u22121T0,T0v0,T0 ]i \u2223\u2223\u2223\u2223\u2223 = \u03be0 ,\nwhere the equality holds because of Lemma 2.2. By Lemma 2.2, we know that sign(\u03b8\u0302ZN(\u03bb)) = v0 and that supp(v0) = T0 contains the true support S. Applying Lemma 2.3, Eq. (9) and using the generalized irrepresentability assumption (10), we obtain\u2225\u2225\u2225\u03a3\u0302T0c,T0\u03a3\u0302\u22121T0,T0v0,T0\u2225\u2225\u2225\u221e \u2264 1\u2212 \u03b7 , (26)\nv0,T0 = sign ( \u03b80,T0 \u2212 \u03bb\u03a3\u0302\u22121T0,T0v0,T0 ) . (27)\n1We treat the entries of the Lasso solution with magnitude less than 0.005 as zero.\nGauss-Lasso\nLasso\nAlso, by Lemma 2.4, sign(\u03b8\u0302n) = v0 if Eqs. (11) and (12) hold with z = v0 and T = T0, namely, if\u2225\u2225\u2225\u03a3\u0302T0c,T0\u03a3\u0302\u22121T0,T0v0,T0 + 1\u03bb(r\u0302T0c \u2212 \u03a3\u0302T0c,T0\u03a3\u0302\u22121T0,T0 r\u0302T0)\u2225\u2225\u2225\u221e \u2264 1 , (28) v0,T0 = sign ( \u03b80,T \u2212 \u03a3\u0302\u22121T0,T0(\u03bbv0,T0 \u2212 r\u0302T0) ) . (29)16\nIn the sequel, we show that these equations are satisfied, with probability lower bounded as per Eq. (16).\nWe begin with proving Eq. (28). Let T = (1/\u03bb)(r\u0302T0c \u2212 \u03a3\u0302T0c,T0\u03a3\u0302 \u22121 T0,T0 r\u0302T0). We need to show that\n\u2016T \u2016\u221e \u2264 \u03b7. Plugging for r\u0302, we get T \u2261 XT c0 \u03a0X\u22a5T0 W/(n\u03bb), where \u03a0X\u22a5T0 = I \u2212XT0(XTT0XT0) \u22121XTT0 is the orthogonal projection onto the orthogonal complement of the column space of XT0 . Since W \u223c N(0, \u03c32In\u00d7n), the variable Tj = xTj \u03a0X\u22a5T0 W/(n\u03bb) is normal with variance at most\n( \u03c3 n\u03bb )2 \u2016\u03a0X\u22a5T0 xj\u201622 \u2264 ( \u03c3 n\u03bb )2 \u2016xj\u201622 \u2264 \u03c32 n\u03bb2 ,\nwhere we used the fact that \u2016xj\u20162 \u2264 n, as \u03a3\u0302i,i \u2264 1. By the Gaussian tail bound with union bound over j \u2208 T c0 , we obtain\nP(\u2016T \u2016\u221e \u2264 \u03b7) \u2265 1\u2212 2pe\u2212 n\u03bb2\u03b72 2\u03c32 = 1\u2212 2p1\u2212c1 . (30)\nWe next prove Eq. (29). Given Eq. (27), we need to show sign ( \u03b80,T0 \u2212 \u03bb\u03a3\u0302\u22121T0,T0v0,T0 ) = sign ( \u03b80,T0 \u2212 \u03a3\u0302\u22121T0,T0(\u03bbv0,T0 \u2212 r\u0302T0) ) .\nLet u \u2261 \u03b80,T0 \u2212 \u03bb\u03a3\u0302\u22121T0,T0v0,T0 , and u\u0302 \u2261 \u03b80,T0 \u2212 \u03a3\u0302 \u22121 T0,T0 (\u03bbv0,T0 \u2212 r\u0302T0). By condition (iii), we have, for all i \u2208 S, |ui| \u2265 |\u03b80,i| \u2212 \u03bb|[\u03a3\u0302\u22121T0,T0v0,T0 ]i| \u2265 c2\u03bb. Further, for all i \u2208 T0 \\ S, we have |ui| = \u03bb|[\u03a3\u0302\u22121T0,T0v0,T0 ]i| \u2265 c2\u03bb. Summarizing, for all i \u2208 T0, we have |ui| \u2265 c2\u03bb. We will show that \u2016u\u2212 u\u0302\u2016\u221e = \u2016\u03a3\u0302\u22121T0,T0 r\u0302T0\u2016\u221e < c2\u03bb, with high probability, thus implying sign(uT0) = sign(u\u0302T0) as desired.\nLemma 5.1. The following holds true.\nP ( \u2016\u03a3\u0302\u22121T0,T0 r\u0302T0\u2016\u221e \u2265 \u03c3 \u221a 2c1 log p n \u2016\u03a3\u0302\u22121T0,T0\u2016 1/2 2 ) \u2264 2p1\u2212c1 . (31)\nLemma 5.1 is proved by noting that conditioned on XT0 , \u03a3\u0302 \u22121 T0,T0\nr\u0302T0 is a Gaussian vector and then applying standard tail bound inequality. The details are deferred to Section A.5.\nUsing Lemma 5.1 and the assumption \u03b7 \u2264 c2 \u221a Cmin, we get \u2016u\u2212 u\u0302\u2016\u221e < c2\u03bb, with probability at least 1\u2212 2p1\u2212c1 . Putting all this together, Eqs. (28) and (29) hold simultaneously, with probability at least 1 \u2212 4p1\u2212c1 . This implies the thesis."}, {"heading": "5.2 Proof of Theorem 2.7", "text": "Recall that T = supp(\u03b8\u0302n). On the event E \u2261 {T = T0}, we have\n\u03b8\u0302GLT = (X T TXT ) \u22121XTT (XT \u03b80,T +W ) = \u03b80,T + (X T TXT ) \u22121XTTW ,\nwhere the first equality holds since T = T0 \u2287 S and thus \u03b80,T c = 0. Further note that \u03b8\u0302GLi \u2212 \u03b80,i, for i \u2208 T , is a zero mean Gaussian vector with variance\n\u03c32\u2016eTi (XTTXT )\u22121XTT \u20162 \u2264 \u03c32\u2016\u03a3\u0302\u22121T,T \u20162/n \u2264 \u03c3 2/(nCmin) .\nUsing tail bound inequality along with union bounding over i \u2208 [p], we get\nP ( \u2016\u03b8\u0302GLT \u2212 \u03b80,T \u2016\u221e \u2265 \u00b5; E ) \u2264 2e\u2212nCmin\u00b52/2\u03c32 .\nAlso, under the assumptions of Theorem 2.5, P(E) \u2265 1\u2212 4p1\u2212c1 . Hence\nP ( \u2016\u03b8\u0302GLT \u2212 \u03b80,T \u2016\u221e \u2265 \u00b5 ) \u2264 P ( \u2016\u03b8\u0302GLT \u2212 \u03b80,T \u2016\u221e \u2265 \u00b5; E ) + P(Ec) \u2264 2e\u2212nCmin\u00b52/2\u03c32 + 4p1\u2212c1 .\nSince \u03b8\u0302GLT c = \u03b80,T c = 0, we get \u2016\u03b8\u0302GL\u2212\u03b80\u2016\u221e < \u00b5, with probability at least 1\u22124p1\u2212c1\u22122e\u2212nCmin\u00b5 2/2\u03c32 .\nMoreover, if \u2016\u03b8\u0302GL \u2212 \u03b80\u2016 < \u03b8min/2, then |\u03b8\u0302GLi | > \u03b8min/2 for i \u2208 S and |\u03b8\u0302GLi | < \u03b8min/2, for i \u2208 Sc. Hence, the s0 top entries of \u03b8\u0302\nGL (in modulus), returned by the Gauss-Lasso selector, correspond to the true support S. Therefore,\nP(S\u0302 = S) \u2265 P(\u2016\u03b8\u0302GL \u2212 \u03b80\u2016\u221e < \u03b8min/2)\n\u2265 1\u2212 4p1\u2212c1 \u2212 2pe\u2212nCmin\u03b82min/8\u03c32 \u2265 1\u2212 6p1\u2212c1/4 ,\nwhere the last inequality follows from the facts \u03b8min \u2265 c2\u03bb, and \u03b7 \u2264 c2 \u221a Cmin."}, {"heading": "6 Proof of Theorems 3.4 and 3.7", "text": "By the condition (iii) in the statement of the theorem, we have\n\u03bb \u2264 2 3 min i\u2208S \u2223\u2223\u2223\u2223\u2223 \u03b80,i[\u03a3\u22121T0,T0v0,T0 ]i \u2223\u2223\u2223\u2223\u2223 < \u03be0 ,\nwhere the second inequality holds because of Lemma 3.2. Therefore, as a result of Lemma 3.2, we have sign(\u03b8\u0302\u221e(\u03bb)) = v0 and that supp(v0) = T0 contains the true support S. Applying Lemma 3.3 and using the generalized irrepresentability assumption, we have\u2225\u2225\u2225\u03a3T0c,T0\u03a3\u22121T0,T0v0,T0\u2225\u2225\u2225\u221e \u2264 1\u2212 \u03b7 , (32)\nv0,T0 = sign ( \u03b80,T0 \u2212 \u03bb\u03a3\u22121T0,T0v0,T0 ) . (33)\nMoreover, by Lemma 2.4, sign(\u03b8\u0302n) = v0 if Eqs. (11) and (12) hold with z = v0 and T = T0, namely,\u2225\u2225\u2225\u03a3\u0302T0c,T0\u03a3\u0302\u22121T0,T0v0,T0 + 1\u03bb(r\u0302T0c \u2212 \u03a3\u0302T0c,T0\u03a3\u0302\u22121T0,T0 r\u0302T0)\u2225\u2225\u2225\u221e \u2264 1 , (34) v0,T0 = sign ( \u03b80,T \u2212 \u03a3\u0302\u22121T0,T0(\u03bbv0,T0 \u2212 r\u0302T0) ) . (35)\nThe rest of the proof is devoted to show the validity of these equations, with probability lower bounded as per Eq. (24)."}, {"heading": "6.1 Proof of Eq. (34)", "text": "It is immediate to see that Eq. (34) holds if the followings hold true:\nT1 \u2261 \u2225\u2225\u03a3\u0302T0c,T0\u03a3\u0302\u22121T0,T0v0,T0\u2225\u2225\u221e \u2264 1\u2212 \u03b72 , (36) T2 \u2261 1\n\u03bb \u2225\u2225r\u0302T0c \u2212 \u03a3\u0302T0c,T0\u03a3\u0302\u22121T0,T0 r\u0302T0\u2225\u2225\u221e \u2264 \u03b72 . (37) In order to prove inequalities (36) and (37), it is useful to recall the following proposition from\nrandom matrix theory.\nProposition 6.1 ([DS01, Wai09, Ver12]). For k \u2264 n, let X \u2208 Rn\u00d7k be a random matrix with i.i.d rows drawn from N(0,\u03a3). Then the following hold true for all t \u2265 1 and \u03c4 \u2261 2( \u221a k n + t) + ( \u221a k n + t) 2 .\n(a) If \u03a3 has maximum eigenvalue \u03c3max <\u221e, then P ( \u2016 1 n XTX\u2212 \u03a3\u20162 \u2265 \u03c3max \u03c4 ) \u2264 2e\u2212nt2/2 .\n(b) If \u03a3 has minimum eigenvalue \u03c3min > 0, then P ( \u2016( 1 n XTX)\u22121 \u2212 \u03a3\u22121\u20162 \u2265 \u03c3\u22121min \u03c4 ) \u2264 2e\u2212nt2/2 .\nWe consider the particular choice of t = \u221a k/n which is useful for future reference. Since k/n \u2264 1,\nwe get \u03c4 \u2264 8 \u221a k/n and therefore the specialized version of Proposition 6.1 reads:\nP ( \u2016 1 n XTX\u2212 \u03a3\u20162 \u2265 8 \u221a k/n\u03c3max ) \u2264 2e\u2212k/2 , (38)\nP ( \u2016( 1 n XTX)\u22121 \u2212 \u03a3\u22121\u20162 \u2265 8 \u221a k/n\u03c3\u22121min ) \u2264 2e\u2212k/2 . (39)\nWe define the event E1 as E1 \u2261 { \u2016(\u03a3\u0302T0,T0)\u22121 \u2212 \u03a3\u22121T0,T0\u20162 \u2264 8 \u221a t0/nC \u22121 min } .\nApplying Eqs. (38), (39) to XT0 , we conclude that\nP(Ec1) \u2264 2e\u2212t0/2 . (40)\nWe now have in place all we need to bound the terms T1 and T2."}, {"heading": "6.1.1 Bounding T1", "text": "To bound T1, we employ similar techniques to those used in [Wai09, Theorem 3] to verify strict dual feasibility. The argument in [Wai09] works under the irrepresentability condition (see Eq. (26) therein) and we modify it to apply to the current setting, i.e., the generalized irrepresentability condition.\nWe begin by conditioning on XT0 . For j \u2208 T0c, xj is a zero mean Gaussian vector and we can decompose it into a linear correlated part plus an uncorrelated part as\nxTj = \u03a3j,T0\u03a3 \u22121 T0,T0 XTT0 + T j ,\nwhere j \u2208 Rn has i.i.d. entries distributed as ji \u223c N(0,\u03a3j,j \u2212 \u03a3j,T0\u03a3\u22121T0,T0\u03a3T0,j). Letting u = \u03a3\u0302T0c,T0\u03a3\u0302 \u22121 T0,T0 v0,T0 , we write\nuj = x T j XT0(X T T0XT0) \u22121v0,T0\n= \u03a3j,T0(\u03a3T0,T0) \u22121v0,T0 + T j XT0(X T T0XT0) \u22121v0,T0 . (41)\nThe first term is bounded as |\u03a3j,T0(\u03a3T0,T0)\u22121v0,T0 | \u2264 1\u2212\u03b7 as per Eq. (32). Letmj = Tj XT0(XTT0XT0) \u22121v0,T0 . Since Var( ji) \u2264 \u03a3j,j \u2264 1, conditioned on XT0 , mj is zero mean Gaussian with variance at most\nVar(mj) \u2264 \u2016XT0(XTT0XT0) \u22121v0,T0\u201622\n\u2264 1 n vT0,T0 (XTT0XT0 n )\u22121 v0,T0 \u2264 1 n \u2016\u03a3\u0302\u22121T0,T0\u20162 \u2016v0,T0\u2016 2 . (42)\nUnder the event E1, we have\n\u2016\u03a3\u0302\u22121T0,T0\u20162 \u2264 \u2016\u03a3 \u22121 T0,T0 \u20162 + \u2016\u03a3\u0302\u22121T0,T0 \u2212 \u03a3 \u22121 T0,T0 \u20162 \u2264 (1 + 8 \u221a t0/n)C \u22121 min \u2264 9C \u22121 min , (43)\nand hence, Var(mj) \u2264 9t0/(nCmin). We now define the event E as\nE \u2261 {\nmax j\u2208T c |mj | \u2265\n\u221a 18c1 t0 log p\nnCmin\n} .\nBy the total probability rule, we have\nP(E) \u2264 P(E ; E1) + P(Ec1) .\nUsing Gaussian tail bound and union bounding over j \u2208 T0c, we obtain P(E ; E1) \u2264 2p1\u2212c1 . Using the bound P(Ec1) \u2264 2e\u2212t0/2, we arrive at:\nP ( max j\u2208T c |mj | > \u221a 18c1 t0 log p\nnCmin\n) \u2264 2p1\u2212c1 + 2e\u2212 t0 2 . (44)\nUsing this, together with Eq. (32), in Eq. (41), we obtain that the following holds true with probability at least 1\u2212 2p1\u2212c1 \u2212 2e\u2212t0/2:\nT1 \u2264 1\u2212 \u03b7 + \u221a 18c1 t0 log p\nnCmin . (45)\nIt is easy to check that the this implies T1 < 1 \u2212 \u03b7/2, for \u03bb as claimed in Eq. (21) provided n \u2265 M1t0 log p."}, {"heading": "6.1.2 Bounding T2", "text": "We bound T2 by the same technique used in proving Eq. (28). Let m = (1/\u03bb)(r\u0302T0c\u2212\u03a3\u0302T0c,T0\u03a3\u0302 \u22121 T0,T0 r\u0302T0). Plugging for r\u0302, we get m \u2261 XT c0 \u03a0X\u22a5T0 W/(n\u03bb). Since W \u223c N(0, \u03c32In\u00d7n), conditioned on X, the variable mj = x T j \u03a0X\u22a5T0 W/(n\u03bb) is normal with variance at most\n( \u03c3\nn\u03bb )2\u2016\u03a0X\u22a5T0\nxj\u201622 \u2264 ( \u03c3\nn\u03bb )2\u2016xj\u20162 ,\nwhere we used the contraction property of orthogonal projections. Now, define the event E as follows. E \u2261 { \u2016xj\u20162 < 2n,\u2200j \u2208 [p] } .\nNote that \u2016xj\u20162 d = \u03a3j,jZ, where Z is a chi-squared random variable with n degrees of freedom. Using the standard chi-squared tail bounds [Joh01], for a fixed j, we have \u2016xj\u20162 < 2\u03a3j,j n \u2264 2n, with probability at least 1\u2212 e\u2212n/10. Union bounding over j \u2208 [p], we obtain P(Ec) \u2264 pe\u2212n/10.\nUnder the event E , we have Var(mj) \u2264 2\u03c32/(n\u03bb2). Employing the standard Gaussian tail bound along with union bounding over j \u2208 T c0 , we obtain\nP(T2 \u2265 \u03b7/2; E) \u2264 2pe\u2212 n\u03bb2\u03b72 16\u03c32 = 2p1\u2212c1 . (46)\nHence,\nP(T2 \u2265 \u03b7/2) \u2264 P(T2 \u2265 \u03b7/2; E) + P(Ec) \u2264 2p1\u2212c1 + pe\u2212 n 10 . (47)"}, {"heading": "6.2 Proof of Eq. (35)", "text": "We next prove Eq. (35). Given Eq. (33), we need to show sign ( \u03b80,T0 \u2212 \u03bb\u03a3\u22121T0,T0v0,T0 ) = sign ( \u03b80,T0 \u2212 \u03a3\u0302\u22121T0,T0(\u03bbv0,T0 \u2212 r\u0302T0) ) .\nLet u \u2261 \u03b80,T0 \u2212 \u03bb\u03a3\u22121T0,T0v0,T0 , and u\u0302 \u2261 \u03b80,T0 \u2212 \u03a3\u0302 \u22121 T0,T0 (\u03bbv0,T0 \u2212 r\u0302T0). By condition (iii), we have, for all i \u2208 S, |ui| \u2265 |\u03b80,i|\u2212\u03bb|[\u03a3\u22121T0,T0v0,T0 ]i| \u2265 c2\u03bb+(1/2)\u03bb|[\u03a3 \u22121 T0,T0\nv0,T0 ]i|. Further, for all i \u2208 T0\\S, we have |ui| = \u03bb|[\u03a3\u22121T0,T0v0,T0 ]i| \u2265 c2\u03bb+(1/2)\u03bb|[\u03a3 \u22121 T0,T0\nv0,T0 ]i|. Summarizing, for all i \u2208 T0, we have\n|ui| \u2265 c2\u03bb+ 1 2 \u03bb|[\u03a3\u22121T0,T0v0,T0 ]i| .\nWe will show that |ui \u2212 u\u0302i| < c2\u03bb + (1/2)\u03bb|[\u03a3\u22121T0,T0v0,T0 ]i| for all i \u2208 T0, with high probability, thus implying sign(uT0) = sign(u\u0302T0) as desired. Since |ui\u2212 u\u0302i| \u2264 \u03bb|[(\u03a3\u0302\u22121T0,T0\u2212\u03a3 \u22121 T0,T0\n)v0,T0 ]i|+ |[\u03a3\u0302\u22121T0,T0 r\u0302T0 ]i|, it suffices to show that\nT3(i) \u2261 \u03bb|[(\u03a3\u0302\u22121T0,T0 \u2212 \u03a3 \u22121 T0,T0 )v0,T0 ]i \u2223\u2223 < 1 2 \u03bb|[\u03a3\u22121T0,T0v0,T0 ]i| for all i \u2208 T0, (48) T4 \u2261 \u2016\u03a3\u0302\u22121T0,T0 r\u0302T0\u2016\u221e < c2\u03bb . (49)\nIn the sequel, we provide probabilistic bounds on T3(i) and T4.\n6.2.1 Bounding T3(i)\nLemma 6.2. Under the assumptions of Theorem 3.4, for any c\u2032 > 1, t0 \u2265 4, we have\nP { \u2203i \u2208 T0 s.t. \u2223\u2223[(\u03a3\u0302\u22121T0,T0 \u2212 \u03a3\u22121T0,T0)v0,T0 ]i\u2223\u2223 \u2265 16 \u221a c\u2032c\u2217 t0 log p\nn\n\u2223\u2223[\u03a3\u22121T0,T0v0,T0 ]i\u2223\u2223 } \u2264 2e\u2212 t0 2 + 2p1\u2212c \u2032 ,\nwhere c\u2217 \u2261 (c2Cmin)\u22122.\nThe proof of Lemma 6.2 is presented in Section A.6. Applying this lemma, with probability at least 1\u22122e\u2212t0/2\u22122p1\u2212c1 , we have T3(i) < (1/2)\u03bb|[\u03a3\u22121T0,T0v0,T0 ]i|\nprovided\n16\n\u221a c1c\u2217 t0 log p\nn \u2264 1 2 .\ni.e., for n \u2265M3t0 log p."}, {"heading": "6.2.2 Bounding T4", "text": "Lemma 6.3. The following holds true.\nP ( T4 \u2264 3\u03c3 \u221a 2c1 log p\nnCmin\n) \u2265 1\u2212 2e\u2212 t0 2 \u2212 2p1\u2212c1 . (50)\nLemma 6.3 is proved in Section A.7. From the last lemma, it follows that Eq. (49) holds with probability at least 1\u2212 2e\u2212 t0 2 \u2212 2p1\u2212c1 ,\nprovided\n3\u03c3\n\u221a 2c1 log p\nnCmin \u2264 c2\u03bb .\nChoosing \u03bb as per Eq. (21), the latter is easily shown to follow from \u03b7 \u2264 c2 \u221a Cmin."}, {"heading": "6.3 Summary: Proof of Theorem 3.4", "text": "Now combining the bounds on T1,. . . T4, we get that for n \u2265 max(M1,M3) t0 log p, Eqs. (34) and (35) hold simultaneously, with probability at least 1\u2212pe\u2212n/10\u22126e\u2212t0/2\u22128p1\u2212c1 . This implies sign(\u03b8\u0302n(\u03bb)) = v0."}, {"heading": "6.4 Proof of Theorem 3.7", "text": "Note that the matrix XT0 is a random Gaussian matrix with rows drawn independently form N(0,\u03a3T0,T0) (recall that T0 is a deterministic set determined by the population-level problem). Therefore, \u2016\u03a3\u0302\u22121T0,T0\u20162 \u2264 9\u2016\u03a3 \u22121 T0,T0 \u20162 \u2264 9C\u22121min. Using Theorem 3.4 to bound the probability that T 6= T0, the proof proceeds along the same lines as the proof of Theorem 2.7."}, {"heading": "Acknowledgements", "text": "A.J. is supported by a Caroline and Fabian Pease Stanford Graduate Fellowship. This work was partially supported by the NSF CAREER award CCF-0743978, the NSF grant DMS-0806211, and the grants AFOSR/DARPA FA9550-12-1-0411 and FA9550-13-1-0036."}, {"heading": "A Proof of technical lemmas", "text": "A.1 Proof of Lemma 2.1\nBy a change of variables, it is easy to see that \u03b8\u0302ZN(\u03be) = \u03b80 +\u03be u\u0302(\u03be), where u\u0302(\u03be) = arg minu\u2208Rp F (u; \u03be) and\nF (u; \u03be) \u2261 1 2 \u3008u, \u03a3\u0302u\u3009+ \u2016uSc\u20161 +\n( \u2016\u03be\u22121\u03b80,S + uS\u20161 \u2212 \u2016\u03be\u22121\u03b80,S\u20161 ) .\nThe rest of the proof is analogous to an argument in [BRT09]. Since, by definition, F (u\u0302; \u03be) \u2264 F (0; \u03be), we have\n1 2 \u3008u\u0302, \u03a3\u0302u\u0302\u3009+ \u2016u\u0302Sc\u20161 \u2212 \u2016u\u0302S\u20161 \u2264 0 (51)\nand hence \u2016u\u0302Sc\u20161 \u2264 \u2016u\u0302S\u20161. Using the definition of \u03ba\u0302, with J = S, c0 = 1, we have\n0 \u2265 1 2 \u03ba\u0302(s0, 1)\u2016u\u0302\u201622 + \u2016u\u0302Sc\u20161 \u2212 \u2016u\u0302S\u20161\n\u2265 1 2 \u03ba\u0302(s0, 1)\u2016u\u0302S\u201622 \u2212 \u2016u\u0302S\u20161 ,\nand since \u2016u\u0302S\u201622 \u2265 \u2016u\u0302S\u201621/s0, we deduce that\n\u2016u\u0302S\u20161 \u2264 2s0\n\u03ba\u0302(s0, 1) .\nBy Eq. (51), this implies in turn\n\u3008u\u0302, \u03a3\u0302u\u0302\u3009 \u2264 4s0 \u03ba\u0302(s0, 1) . (52)\nNow, consider the stationarity conditions of F . These imply\n(\u03a3\u0302u\u0302)i = \u2212sign(u\u0302i) , for i \u2208 T \\ S.\nWe therefore have\n|T \\ S| \u2264 \u2211 i\u2208T\\S (\u03a3\u0302u\u0302)2i \u2264 \u2016\u03a3\u0302u\u0302\u201622 \u2264 \u2016\u03a3\u0302\u20162\u3008u\u0302, \u03a3\u0302u\u0302\u3009 ,\nand our claim follows by substituting Eq. (52) in the latter equation.\nA.2 Proof of Lemma 2.2\nBy a change of variables, it is easy to see that \u03b8\u0302ZN(\u03be) = \u03b80 +\u03be u\u0302(\u03be), where u\u0302(\u03be) = arg minu\u2208Rp F (u; \u03be) and\nF (u; \u03be) \u2261 1 2 \u3008u, \u03a3\u0302u\u3009+ \u2016uSc\u20161 +\n( \u2016\u03be\u22121\u03b80,S + uS\u20161 \u2212 \u2016\u03be\u22121\u03b80,S\u20161 ) .\nNotice that, for any u \u2208 Rp, lim\u03be\u21920 F (u; \u03be) = F0(u), where\nF0(u) \u2261 1\n2 \u3008u, \u03a3\u0302u\u3009+ \u2016uSc\u20161 + \u3008sign(\u03b80,S), uS\u3009 .\nIndeed F (u; \u03be) = F0(u) provided \u03be \u2264 mini\u2208S |\u03b80,i/ui|. Further, F (u; \u03be) \u2265 F0(u) for all u. Let u0 \u2261 arg minu\u2208Rp F0(u), and set \u03be0 \u2261 mini\u2208S |\u03b80,i/u0,i|. Then, for any u 6= u0, and all \u03be \u2208 (0, \u03be0), we have\nF (u; \u03be) \u2265 F0(u) > F0(u0) = F (u0; \u03be) .\nHence u0 is the unique minimizer of F (u; \u03be), i.e., u\u0302(\u03be) = u0 for all \u03be \u2208 (0, \u03be0). It follows that \u03b8\u0302ZN(\u03be) = \u03b80+\u03be u0 for all \u03be \u2208 (0, \u03be0) and hence sign(\u03b8\u0302ZN(\u03be)) = v0 and supp(\u03b8\u0302ZN(\u03be)) = T0 where we set\nv0,S \u2261 sign(\u03b80,S) , v0,Sc \u2261 sign(u0,Sc) , T0 \u2261 S \u222a supp(u0) .\nFinally, the zero subgradient condition for u0 reads \u03a3\u0302u0 + z = 0, with zS = sign(\u03b80,S) and zSc \u2208 \u2202\u2016u0,Sc\u20161. In particular, zT0 = v0,T0 and therefore u0,T0 = \u2212\u03a3\u0302\u22121T0,T0vT0 . This implies\n\u03be0 \u2261 min i\u2208S \u2223\u2223\u2223\u2223 \u03b80,iu0,i \u2223\u2223\u2223\u2223 = mini\u2208S \u2223\u2223\u2223\u2223\u2223 \u03b80,i[\u03a3\u0302\u22121T0,T0v0,T0 ]i \u2223\u2223\u2223\u2223\u2223 .\nA.3 Proof of Lemma 2.3\nWriting the zero-subgradient conditions for problem (5), we have\n\u03a3\u0302(\u03b8\u0302ZN \u2212 \u03b80) = \u2212\u03beu, u \u2208 \u2202\u2016\u03b8\u0302ZN\u20161.\nGiven that T \u2287 S, we have \u03b80,T c = 0, and thus\n\u03a3\u0302T,T (\u03b8\u0302 ZN T \u2212 \u03b80,T ) = \u2212\u03beuT ,\n\u03a3\u0302T c,T (\u03b8\u0302 ZN T \u2212 \u03b80,T ) = \u2212\u03beuT c .\nSolving for \u03b8\u0302ZNT \u2212 \u03b80,T in terms of uT , we obtain\n\u03a3\u0302T c,T \u03a3\u0302 \u22121 T,TuT = uT c , \u03b8\u0302ZNT = \u03b80,T \u2212 \u03be\u03a3\u0302\u22121T,TuT .\nThis proves the \u2018only if\u2019 part noting that uT = sign(\u03b8\u0302 ZN T ) = vT , and \u2016uT c\u2016\u221e \u2264 1 since u \u2208 \u2202\u2016\u03b8\u0302ZN\u20161.\nNow suppose that Eqs. (8) and (9) hold true. Let \u03b8\u0303T = \u03b80,T \u2212 \u03be\u03a3\u0302\u22121T,T vT , and \u03b8\u0303T c = 0. We prove that \u03b8\u0303 = \u03b8\u0302ZN, by showing that it satisfies the zero-subgradient condition. By Eq. (9), vT = sign(\u03b8\u0303T ). Define u \u2208 Rp by letting uT = vT and uT c = \u03a3\u0302T c,T \u03a3\u0302 \u22121 T,T vT . Note that \u2016uT c\u2016\u221e \u2264 1 by Eq. (8), and so u \u2208 \u2202\u2016\u03b8\u0303\u20161. Moreover,\n\u03a3\u0302T,T (\u03b8\u0303T \u2212 \u03b80,T ) = \u2212\u03beuT \u03a3\u0302T c,T (\u03b8\u0303T \u2212 \u03b80,T ) = \u2212\u03beuT c ,\nCombining the above two equations, we get the zero-subgradient condition for (\u03b8\u0303, u). Therefore, \u03b8\u0303 = \u03b8\u0302ZN, and v = sign(\u03b8\u0302ZN).\nA.4 Proof of Lemma 2.4\nThe proof proceeds along the same lines as the proof of Lemma 2.3. We begin with proving the \u2018only if\u2019 part. The zero-subgradient condition for Problem 3 reads:\n\u2212 1 n XT(Y \u2212X\u03b8\u0302n) + \u03bbu = 0 , u \u2208 \u2202\u2016\u03b8\u0302n\u20161 .\nPlugging for Y = X\u03b80 +W and r\u0302 = (X TW/n) in the above equation, we arrive at:\n\u03a3\u0302(\u03b8\u0302n \u2212 \u03b80) = r\u0302 \u2212 \u03bbu .\nSince T \u2287 S, \u03b80,T c = 0, and writing the above equation for indices in T and T c separately, we obtain\n\u03a3\u0302T c,T (\u03b8\u0302 n T \u2212 \u03b80,T ) = r\u0302T c \u2212 \u03bbuT c ,\n\u03a3\u0302T,T (\u03b8\u0302 n T \u2212 \u03b80,T ) = r\u0302T \u2212 \u03bbuT .\nSolving for \u03b8\u0302nT \u2212 \u03b80,T from the second equation, we get\n\u03a3\u0302T c,T \u03a3\u0302 \u22121 T,TuT +\n1 \u03bb (r\u0302T c \u2212 \u03a3\u0302T c,T \u03a3\u0302\u22121T,T r\u0302T ) = uT c ,\n\u03b8\u0302nT = \u03b80,T \u2212 \u03a3\u0302\u22121T,T (\u03bbuT \u2212 r\u0302T ) .\nThis proves Eqs. (11) and (12), since uT = sign(\u03b8\u0302 n T ) = zT and \u2016uT c\u2016\u221e \u2264 1.\nWe next prove the other direction. Suppose that Eqs. (11) and (12) hold true. Let \u03b8\u0303T = \u03b80,T \u2212 \u03a3\u0302\u22121T,T (\u03bbzT \u2212 r\u0302T ), and \u03b8\u0303T c = 0. We prove that \u03b8\u0303 = \u03b8\u0302n, by showing that it satisfies the zero-subgradient condition. By Eq. (12), zT = sign(\u03b8\u0303T ). Define u \u2208 Rp by letting uT = zT and uT c = \u03a3\u0302T c,T \u03a3\u0302 \u22121 T,T zT + (r\u0302T c \u2212 \u03a3\u0302T c,T \u03a3\u0302 \u22121 T,T r\u0302T )/\u03bb. Note that \u2016uT c\u2016\u221e \u2264 1 by Eq. (12), and so u \u2208 \u2202\u2016\u03b8\u0303\u20161. Moreover,\n\u03a3\u0302T,T (\u03b8\u0303T \u2212 \u03b80,T ) = \u2212(\u03bbuT \u2212 r\u0302T )\n\u03a3\u0302T c,T (\u03b8\u0303T \u2212 \u03b80,T ) = \u2212(\u03bbuT c \u2212 r\u0302T c) ,\nCombining the above two equations, we get the zero-subgradeint condition for (\u03b8\u0303, u). Therefore, \u03b8\u0303 = \u03b8\u0302n, and z = sign(\u03b8\u0302n).\nA.5 Proof of Lemma 5.1\nLet m = \u03a3\u0302\u22121T0,T0 r\u0302T0 = (X T T0 XT0) \u22121XTT0W . Conditioned on XT0 , mi is a zero mean Gaussian vector with variance \u03c32\u2016eTi (XT0XT0)\u22121XTT0\u2016 2. By a Gaussian tail bound, we get\nP ( |mi| \u2265 \u221a 2c1 log p \u03c3\u2016eTi (XTT0XT0) \u22121XTT0\u2016 ) \u2264 2p\u2212c1 .\nFurther, notice that \u2016eTi (XTT0XT0) \u22121XTT0\u2016 2 \u2264 \u2016\u03a3\u0302\u22121T0,T0\u20162/n. By union bounding over i = 1, . . . , p, we have\nP ( \u2016m\u2016\u221e \u2265 \u03c3 \u221a 2c1 log p\nn \u2016\u03a3\u0302\u22121T0,T0\u2016 1/2 2\n) \u2264 2p1\u2212c1 .\nA.6 Proof of Lemma 6.2\nWe begin by stating and proving a lemma that is similar to Lemma 5 in [Wai09], but provides a stronger control.\nLemma A.1. Let Z \u2208 Rn\u00d7k be a random matrix with i.i.d. Gaussian rows with zero mean and covariance \u03a3, with k \u2265 4. Further let a1, . . . , aM \u2208 Rk and b1, . . . , bM \u2208 Rk be non-random vectors. Then, letting \u03a3\u0302Z \u2261 ZTZ/n, we have, for all \u2206 > 0:\nP { \u2203i \u2208 [M ] s.t. \u2223\u2223\u2223\u3008ai, (\u03a3\u0302\u22121Z \u2212 \u03a3\u22121)bi\u3009\u2223\u2223\u2223 \u2265 8 \u221a k\nn |\u3008ai,\u03a3\u22121bi\u3009|+ \u2206 \u2016\u03a3\u22121/2ai\u20162\u2016\u03a3\u22121/2bi\u20162\n}\n\u2264 2e\u2212 k 2 + 2M exp { \u2212 n\u2206 2\n256\n} . (53)\nProof. First notice that Z = Z\u0303\u03a31/2 with Z\u0303 \u2208 Rn\u00d7k a random matrix with i.i.d. standard Gaussian entries Zij \u223c N(0, 1). By substituting in the statement of the theorem, it is easy to check that we only need to prove our claim in the case \u03a3 = Ik\u00d7k (i.e., for Z with i.i.d. entries), which we shall assume hereafter.\nDefining the event E\u2217 = {\u2016\u03a3\u0302\u22121 \u2212 I\u20162 \u2264 8 \u221a k/n}, we have, by Eq. (39) and the union bound,\nP { \u2203i \u2208 [M ] s.t. \u2223\u2223\u2223\u3008ai, (\u03a3\u0302\u22121 \u2212 I)bi\u3009\u2223\u2223\u2223 \u2265 8\u221ak n |\u3008ai, bi\u3009|+ \u2206 \u2016ai\u20162\u2016bi\u20162 } \u2264\n2 e\u2212k/2 +M max i\u2208[M ] P {\u2223\u2223\u3008ai, (\u03a3\u0302\u22121 \u2212 I)bi\u3009\u2223\u2223 \u2265 8\u221ak n |\u3008ai, bi\u3009|+ \u2206 \u2016ai\u20162\u2016bi\u20162; E\u2217 }\nWe can now concentrate on the last probability. Let \u03b1 \u2261 |\u3008ai, bi\u3009| and \u03b2 \u2261 (\u2016ai\u201622\u2016bi\u201622\u2212\u3008ai, bi\u30092)1/2. Since \u03a3\u0302 is distributed as R\u03a3\u0302RT for any orthogonal matrix R, we have\n\u3008ai, (\u03a3\u0302\u22121 \u2212 I)bi\u3009 d = \u03b1\u3008e1, (\u03a3\u0302\u22121 \u2212 I)e1\u3009+ \u03b2\u3008e1, (\u03a3\u0302\u22121 \u2212 I)e2\u3009 ,\nwhere d = denotes equality in distribution. Under the event E\u2217, we have |\u03b1\u3008e1, (\u03a3\u0302\u22121\u2212I)e1\u3009| \u2264 8\u03b1 \u221a k/n. Further (\u03a3\u0302\u22121 \u2212 I) = UDUT with U a uniformly random orthogonal matrix (with respect to Haar\nmeasure on the manifold of orthogonal matrices). Letting u1, u2 denote the first two rows of U we then have\nP {\u2223\u2223\u3008ai, (\u03a3\u0302\u22121 \u2212 I)bi\u3009\u2223\u2223 \u2265 8\u221ak n |\u3008ai, bi\u3009|+ \u2206 \u2016ai\u20162\u2016bi\u20162; E\u2217 } \u2264 P{|\u3008u1, Du2\u3009| \u2265 \u2206; E\u2217} .\nNotice that conditioned on u2 and D, u1 is uniformly random on a (k \u2212 1)-dimensional sphere. Further, letting v2 = Du2, we have \u2016v2\u20162 \u2264 8 \u221a k/n. Hence, by isoperimetric inequalities on the sphere [Led01], we obtain\nP{|\u3008u1, Du2\u3009| \u2265 \u2206; E\u2217} \u2264 sup \u2016v2\u2016\u22648 \u221a k/n P{|\u3008u1, v2\u3009| \u2265 \u2206| v2}\n\u2264 2 exp { \u2212 (k \u2212 2)\u2206 2\n128k/n\n} \u2264 2 exp { \u2212 n\u2206 2\n256\n} ,\nwhere the last inequality holds for all k \u2265 4. The proof is completed by substituting this inequality in the expressions above.\nWe are now in position to prove Lemma 6.2.\nProof (Lemma 6.2). We apply Lemma A.1 to \u03a3\u0302 = \u03a3\u0302T0,T0 , M = t0, ai = ei and bi = v0,T0 for i \u2208 {1, . . . , t0}. We get\nP { \u2203i \u2208 T0 s.t. \u2223\u2223[(\u03a3\u0302\u22121T0,T0 \u2212 \u03a3\u22121T0,T0)v0,T0 ]i\u2223\u2223 \u2265 8 \u221a t0 n \u2223\u2223[\u03a3\u22121T0,T0v0,T0 ]i\u2223\u2223+ \u2206\u2016\u03a3\u22121/2T0,T0ei\u20162\u2016\u03a3\u22121/2T0,T0v0,T0\u20162 } \u2264\n2 e\u2212t0/2 + 2t0 exp { \u2212 n\u2206 2\n256\n} .\nNote that \u2016\u03a3\u22121/2T0,T0ei\u20162\u2016\u03a3 \u22121/2 T0,T0 v0,T0\u20162 \u2264 C\u22121min\u2016ei\u20162\u2016v0,T0\u20162 = C \u22121 min \u221a t0. Further |[\u03a3\u22121T0,T0v0,T0 ]i \u2223\u2223 \u2265 2c2, and hence \u2016\u03a3\u22121/2T0,T0ei\u20162\u2016\u03a3 \u22121/2 T0,T0 v0,T0\u20162 \u2264 (1/2) \u221a c\u2217t0 |[\u03a3\u22121T0,T0v0,T0 ]i\n\u2223\u2223. We therefore get P { \u2203i \u2208 T0 s.t. \u2223\u2223[(\u03a3\u0302\u22121T0,T0 \u2212 \u03a3\u22121T0,T0)v0,T0 ]i\u2223\u2223 \u2265 (8 \u221a t0 n + \u2206 2 \u221a c\u2217t0 )\u2223\u2223[\u03a3\u22121T0,T0v0,T0 ]i\u2223\u2223 } \u2264\n2 e\u2212t0/2 + 2t0 exp { \u2212 n\u2206 2\n256\n} .\nThe proof is completed by taking \u2206 = 16 \u221a (c\u2032 log p)/n.\nA.7 Proof of Lemma 6.3\nBy Lemma 5.1, we have\nP ( \u2016\u03a3\u0302\u22121T0,T0 r\u0302T0\u2016\u221e \u2265 \u03c3 \u221a 2c1 log p n \u2016\u03a3\u0302\u22121T0,T0\u2016 1/2 2 ) \u2264 2p1\u2212c1 .\nRecalling Eq. (43), under the event E1 we have \u2016\u03a3\u0302\u22121T0,T0\u20162 \u2264 9C \u22121 min. Since P(Ec1) \u2264 2e\u2212t0/2, we arrive at:\nP ( \u2016\u03a3\u0302\u22121T0,T0 r\u0302T0\u2016\u221e \u2265 3\u03c3 \u221a 2c1 log p\nnCmin\n) \u2264 2p1\u2212c1 + 2e\u2212 t0 2 ."}, {"heading": "B Generalized irrepresentability vs. irrepresentability", "text": "In this appendix we discuss the example provided in Section 1.1 in more details. The objective is to develop some intuition on the domain of validity of generalized irrepresentability, and compare it with the standard irrepresentability condition.\nAs explained in Section 1.1, let S = supp(\u03b80) = {1, . . . , s0} and consider the following covariance matrix:\n\u03a3ij =  1 if i = j,\na if i = p, j \u2208 S or i \u2208 S, j = p, 0 otherwise.\nEquivalently,\n\u03a3 = Ip\u00d7p + a ( epu T S + uSe T p ) ,\nwhere uS is the vector with entries (uS)i = 1 for i \u2208 S and (uS)i = 0 for i 6\u2208 S. It is easy to check that \u03a3 is strictly positive definite for a \u2208 (\u22121/\u221as0,+1/ \u221a s0). By redefining the p-th covariate, we can assume, without loss of generality, a \u2208 [0,+1/\u221as0). We will further assume sign(\u03b80,i) = +1 for all i \u2208 S.\nThis example captures the case of a single confounding variable, i.e., of an irrelevant covariate that correlates strongly with the relevant covariates, and with the response variable.\nWe will show that the Gauss-Lasso has a significantly broader domain of validity with respect to the simple Lasso.\nClaim B.1. Consider the Gaussian design defined above, and suppose that a > 1/s0. Then for any regularization parameter \u03bb and for any sample size n, the probability of correct signed support recovery with Lasso is at most 1/2. (and is not guaranteed with high probability unless a \u2208 [0, (1\u2212 \u03b7)/s0], for some constant \u03b7 > 0.\nOn the other hand, Theorem 3.7 implies correct support recovery with the Gauss-Lasso from n = \u2126(s0 log p) samples, for any\na \u2208 [ 0,\n1\u2212 \u03b7 s0\n] \u222a ( 1\ns0 , 1\u2212 \u03b7 \u221a s0\n] . (54)\nProof. In order to prove that Gauss-Lasso correctly recovers the support of \u03b80, we will show that all the conditions of Theorem 3.4 and Theorem 3.7 hold with constants of order one, provided Eq. (54) holds. Vice versa, the irrepresentability condition does not hold unless a \u2208 [0, 1/s0), and hence the simple Lasso fails outside this regime.\nWe now proceed to check the assumptions of Theorems 3.4 and 3.7, while showing that irrepresentability does not hold for a \u2265 1/s0. Restricted eigenvalues. We have \u03bbmin(\u03a3) = 1\u2212 a \u221a s0. In particular, for any set T \u2286 [p], we have \u03bbmin(\u03a3T,T ) \u2265 1\u2212 a \u221a s0 \u2265 \u03b7. Also, for any constant c0 \u2265 0, \u03ba(s0, c0) \u2265 1\u2212 a \u221a s0 \u2265 \u03b7. Irrepresentability condition. We have \u03a3SS = Is0\u00d7s0 and hence \u2016\u03a3ScS\u03a3\u22121SS\u2016\u221e = \u2016\u03a3p,S\u20161 = as0. Hence the irrepresentability condition holds only if a \u2208 [0, 1/s0). The corresponding irrepresentability parameter is \u03b7 = 1\u2212 as0.\nFor large s0, the condition is only satisfied for a small interval in a, compared to the interval for which \u03a3 is positive definite.\nGeneralized irrepresentability condition. In order to check this condition, we need to compute T0 and v0 defined as per Lemma 3.2. We have \u03b8\u0302 \u221e(\u03be) = arg min\u03b8\u2208Rp G(\u03b8; \u03be) where\nG(\u03b8; \u03be) \u2261 1 2 \u3008(\u03b8 \u2212 \u03b80),\u03a3(\u03b8 \u2212 \u03b80)\u3009+ \u03be\u2016\u03b8\u20161\n= 1\n2 \u2016\u03b8 \u2212 \u03b80\u201622 + a\u3008uS , (\u03b8S \u2212 \u03b80,S)\u3009\u03b8p + \u03be\u2016\u03b8\u20161 .\nFrom this expression, it is immediate to see that \u03b8\u0302\u221ei (\u03be) = 0 for i 6\u2208 S\u222a{p}. Further \u03b8\u0302\u221eS\u222a{p}(\u03be) satisfies\n\u03b8S \u2212 \u03b80,S + a\u03b8puS + \u03bevS = 0 , (55) \u03b8p + a\u3008uS , (\u03b8S \u2212 \u03b80,S)\u3009+ \u03bevp = 0 , (56)\nwith vS \u2208 \u2202\u2016\u03b8S\u20161 and vp \u2208 \u2202|\u03b8p|. Since \u03b80,S > 0, we have, from Eq. (55),\n\u03b8\u0302\u221eS = \u03b80,S \u2212 (a\u03b8\u0302\u221ep + \u03be)uS ,\nprovided (a\u03b8\u0302\u221ep + \u03be) \u2264 \u03b8min. Substituting in Eq. (56) and solving for \u03b8p, we get\n\u03b8\u0302\u221ep (\u03be) = { 0 if a \u2208 [0, 1/s0)( as0\u22121 1\u2212a2s0 ) \u03be if a \u2208 [1/s0, 1/ \u221a s0).\nThis holds provided (a\u03b8\u0302\u221ep + \u03be) \u2264 \u03b8min, i.e., if \u03be \u2264 \u03be\u2217 \u2261 min(1, (1\u2212 a2s0)/(1\u2212 a)) \u03b8min. Using the definition in Lemma 3.2, we have\nT0 = { S if a \u2208 [0, 1/s0) S \u222a {p} if a \u2208 [1/s0, 1/ \u221a s0),\nand v0,T0 = uT0 . We can now check the generalized irrepresentability condition. For a \u2208 [0, 1/s0) we have \u2016\u03a3T c0 ,T0\u03a3 \u22121 T0,T0\nv0,T0\u2016\u221e = \u2016\u03a3Sc,S\u03a3\u22121S,SuS\u2016\u221e = as0, and therefore the generalized irrepresentability condition is satisfied with parameter \u03b7 = 1\u2212as0. For a \u2208 [1/s0, 1/ \u221a s0), we have \u2016\u03a3T c0 ,T0\u03a3 \u22121 T0,T0\nv0,T0\u2016\u221e = 0.\nWe therefore conclude that, for any fixed \u03b7 \u2208 (0, 1], the generalized irrepresentability condition with parameter \u03b7 is satisfied for\na \u2208 [ 0,\n1\u2212 \u03b7 s0 ] \u222a [ 1 s0 , 1 \u221a s0 ) ,\na significant larger domain than for simple irrepresentability.\nMinimum entry condition. For a \u2208 [0, 1/s0), we have T0 = S and it is therefore only necessary to check Eq. (22). Since [\u03a3\u22121T0,T0v0,T0 ]i = 1, this reads\n|\u03b80,i| \u2265 ( c2 + 3\n2\n) \u03bb = C\u03c3 \u221a log p\nn ,\nwith C a constant. For a \u2208 (1/s0, (1\u2212 \u03b7)/\n\u221a s0], we have T0 = S \u222a {p}. A straightforward calculation shows that\u2223\u2223[\u03a3\u22121T0,T0v0,T0 ]i\u2223\u2223 = 1\u2212 a1\u2212 a2s0 , for i \u2208 S ,\u2223\u2223[\u03a3\u22121T0,T0v0,T0 ]p\u2223\u2223 = as0 \u2212 11\u2212 a2s0 .\nIt is not hard to show for all a satisfying Eq. (54), we have\u2223\u2223[\u03a3\u22121T0,T0v0,T0 ]i\u2223\u2223 \u2264 11\u2212 (1\u2212 \u03b7)2 for i \u2208 S, \u2223\u2223[\u03a3\u22121T0,T0v0,T0 ]p\u2223\u2223 \u2265 C , for some constant C > 0. It therefore follows that condition (22) holds if |\u03b80,i| \u2265 C \u2032\u03c3 \u221a log p/n and condition (23) holds for c2 = C/2."}], "references": [{"title": "Bolasso: model consistent lasso estimation through the bootstrap", "author": ["Francis R Bach"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "Statistical significance in high-dimensional linear models, arXiv:1202.1377", "author": ["P. B\u00fchlmann"], "venue": null, "citeRegEx": "B\u00fchlmann,? \\Q2012\\E", "shortCiteRegEx": "B\u00fchlmann", "year": 2012}, {"title": "Statistics for high-dimensional data, SpringerVerlag", "author": ["Peter B\u00fchlmann", "Sara van de Geer"], "venue": null, "citeRegEx": "B\u00fchlmann and Geer,? \\Q2011\\E", "shortCiteRegEx": "B\u00fchlmann and Geer", "year": 2011}, {"title": "Examples of basis pursuit", "author": ["S.S. Chen", "D.L. Donoho"], "venue": "Proceedings of Wavelet Applications in Signal and Image Processing III (San Diego, CA),", "citeRegEx": "Chen and Donoho,? \\Q1995\\E", "shortCiteRegEx": "Chen and Donoho", "year": 1995}, {"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "author": ["E. Candes", "J.K. Romberg", "T. Tao"], "venue": "IEEE Trans. on Inform. Theory", "citeRegEx": "Candes et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2006}, {"title": "Decoding by linear programming", "author": ["E.J. Cand\u00e9s", "T. Tao"], "venue": "IEEE Trans. on Inform. Theory", "citeRegEx": "Cand\u00e9s and Tao,? \\Q2005\\E", "shortCiteRegEx": "Cand\u00e9s and Tao", "year": 2005}, {"title": "The Dantzig selector: statistical estimation when p is much larger than n", "author": ["E. Cand\u00e9s", "T. Tao"], "venue": "Annals of Statistics", "citeRegEx": "Cand\u00e9s and Tao,? \\Q2007\\E", "shortCiteRegEx": "Cand\u00e9s and Tao", "year": 2007}, {"title": "Local operator theory, random matrices and Banach spaces", "author": ["K.R. Davidson", "S.J. Szarek"], "venue": "Handbook on the Geometry of Banach spaces,", "citeRegEx": "Davidson and Szarek,? \\Q2001\\E", "shortCiteRegEx": "Davidson and Szarek", "year": 2001}, {"title": "Least angle regression", "author": ["Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani"], "venue": "Annals of Statistics", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "UCI machine learning repository (communities and crime data set), http://archive.ics.uci.edu/ml, 2010, University of California, Irvine, School of Information and Computer Sciences", "author": ["A. Frank", "A. Asuncion"], "venue": null, "citeRegEx": "Frank and Asuncion,? \\Q2010\\E", "shortCiteRegEx": "Frank and Asuncion", "year": 2010}, {"title": "Hypothesis testing in high-dimensional regression under the gaussian random design model: Asymptotic theory", "author": ["Adel Javanmard", "Andrea Montanari"], "venue": "arXiv preprint arXiv:1301.4240,", "citeRegEx": "Javanmard and Montanari,? \\Q2013\\E", "shortCiteRegEx": "Javanmard and Montanari", "year": 2013}, {"title": "Chi-squared oracle inequalities, State of the Art in Probability and Statistics (M", "author": ["I. Johnstone"], "venue": "IMS Lecture Notes, Institute of Mathematical Statistics,", "citeRegEx": "Johnstone,? \\Q2001\\E", "shortCiteRegEx": "Johnstone", "year": 2001}, {"title": "Asymptotics for lasso-type estimators", "author": ["K. Knight", "W. Fu"], "venue": "Annals of Statistics", "citeRegEx": "Knight and Fu,? \\Q2000\\E", "shortCiteRegEx": "Knight and Fu", "year": 2000}, {"title": "The concentration of measure phenomenon", "author": ["M. Ledoux"], "venue": "Mathematical Surveys and Monographs,", "citeRegEx": "Ledoux,? \\Q2001\\E", "shortCiteRegEx": "Ledoux", "year": 2001}, {"title": "Sup-norm convergence rate and sign concentration property of lasso and dantzig estimators", "author": ["Karim Lounici"], "venue": "Electronic Journal of statistics", "citeRegEx": "Lounici,? \\Q2008\\E", "shortCiteRegEx": "Lounici", "year": 2008}, {"title": "Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer", "author": ["Jie Peng", "Ji Zhu", "Anna Bergamaschi", "Wonshik Han", "Dong-Young Noh", "Jonathan R Pollack", "Pei Wang"], "venue": "The Annals of Applied Statistics", "citeRegEx": "Peng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2010}, {"title": "A simple and efficient algorithm for gene selection using sparse logistic regression, Bioinformatics", "author": ["Shirish Krishnaj Shevade", "S. Sathiya Keerthi"], "venue": null, "citeRegEx": "Shevade and Keerthi,? \\Q2003\\E", "shortCiteRegEx": "Shevade and Keerthi", "year": 2003}, {"title": "Regression shrinkage and selection with the Lasso", "author": ["R. Tibshirani"], "venue": "J. Royal. Statist. Soc B", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "On asymptotically optimal confidence regions and tests for high-dimensional models, arXiv:1303.0518", "author": ["S. van de Geer", "P. B\u00fchlmann", "Y. Ritov"], "venue": null, "citeRegEx": "Geer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Geer et al\\.", "year": 2013}, {"title": "Introduction to the non-asymptotic analysis of random matrices, Compressed Sensing: Theory and Applications (Y.C", "author": ["R. Vershynin"], "venue": "Eldar and G. Kutyniok, eds.),", "citeRegEx": "Vershynin,? \\Q2012\\E", "shortCiteRegEx": "Vershynin", "year": 2012}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming", "author": ["M.J. Wainwright"], "venue": "IEEE Trans. on Inform. Theory", "citeRegEx": "Wainwright,? \\Q2009\\E", "shortCiteRegEx": "Wainwright", "year": 2009}, {"title": "Thresholded Lasso for high dimensional variable selection and statistical estimation, arXiv:1002.1583v2", "author": ["S. Zhou"], "venue": null, "citeRegEx": "Zhou,? \\Q2010\\E", "shortCiteRegEx": "Zhou", "year": 2010}, {"title": "The adaptive lasso and its oracle properties", "author": ["H. Zou"], "venue": "Journal of the American Statistical Association", "citeRegEx": "Zou,? \\Q2006\\E", "shortCiteRegEx": "Zou", "year": 2006}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Zhao and Yu,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu", "year": 2006}, {"title": "Confidence Intervals for Low-Dimensional Parameters in High-Dimensional", "author": ["C.-H. Zhang", "S.S. Zhang"], "venue": "Linear Models,", "citeRegEx": "Zhang and Zhang,? \\Q2011\\E", "shortCiteRegEx": "Zhang and Zhang", "year": 2011}], "referenceMentions": [], "year": 2013, "abstractText": "In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is \u2018active\u2019 (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefficients through the Lasso (`1-regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called \u2018irrepresentability\u2019 condition. In this paper we study the \u2018Gauss-Lasso\u2019 selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate \u2018generalized irrepresentability condition\u2019 (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set.", "creator": "LaTeX with hyperref package"}}}