{"id": "1305.1319", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2013", "title": "New Alignment Methods for Discriminative Book Summarization", "abstract": "we consider the unsupervised alignment of the standard full text of a book with a human - written translation summary. this practice presents challenges not seen in other text alignment problems, including a disparity in citation length and, consequent succession to this, a violation of the expectation that individual words and phrases should align, since large cumulative passages accumulated and countless chapters can be distilled into a single summary phrase. we thus present two new methods, based on hidden markov models, specifically targeted to this elimination problem, and demonstrate gains on an abstract extractive book summarization task. so while there is still much room for improvement, unsupervised alignment holds greatly intrinsic value in offering insight into what features of illustrating a book are deemed worthy of summarization.", "histories": [["v1", "Mon, 6 May 2013 20:27:55 GMT  (210kb,D)", "http://arxiv.org/abs/1305.1319v1", "This paper reflects work in progress"]], "COMMENTS": "This paper reflects work in progress", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david bamman", "noah a smith"], "accepted": false, "id": "1305.1319"}, "pdf": {"name": "1305.1319.pdf", "metadata": {"source": "CRF", "title": "New Alignment Methods for Discriminative Book Summarization Work in Progress", "authors": ["David Bamman", "Noah A. Smith"], "emails": ["dbamman@cs.cmu.edu", "nasmith@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "The task of extractive summarization is to select a subset of sentences from a source document to present as a summary. Supervised approaches to this problem make use of training data in the form of source documents paired with existing summaries (Marcu, 1999; Osborne, 2002; Jing and McKeown, 1999; Ceylan and Mihalcea, 2009). These methods learn what features of a source sentence are likely to result in that sentence appearing in the summary; for news articles, for example, strong predictive features include the position of a sentence in a document (earlier is better), the sentence length (shorter is better), and the number of words in a sentence that are among the most frequent in the document.\nSupervised discriminative summarization relies on an alignment between a source document and\nits summary. For short texts and training pairs where a one-to-one alignment between source and abstract sentences can be expected, standard techniques from machine translation can be applied, including word-level alignment (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003) and longer phrasal alignment (Daum\u00e9 and Marcu, 2005), especially as adapted to the monolingual setting (Quirk et al., 2004). For longer texts where inference over all possible word alignments becomes intractable, effective approximations can be made, such as restricting the space of the available target alignments to only those that match the identity of the source word (Jing and McKeown, 1999).\nThe use of alignment techniques for book summarization, however, challenges some of these assumptions. The first is the disparity between the length of the source document and that of a summary. While the ratio between abstracts and source documents in the benchmark Ziff-Davis corpus of newswire (Marcu, 1999) is approximately 12% (133 words vs. 1,066 words), the length of a full-text book greatly overshadows the length of a simple summary. Figure 1 illustrates this with a dataset comprised of books from Project Gutenberg paired with plot summaries extracted from Wikipedia for a set of 439 books (described more fully in \u00a74.1 below). The average ratio between a summary and its corresponding book is 1.2%.\nThis disparity in size leads to a potential violation of a second assumption: that we expect words and phrases in the source document to align with words and phrases in the target. When the disparity is so great, we might rather expect that an entire paragraph, page, or even chapter in a book aligns to a single summary sentence.\nar X\niv :1\n30 5.\n13 19\nv1 [\ncs .C\nL ]\n6 M\nay 2\n01 3\nTo help adapt existing methods of supervised document summarization to books, we present two alignment techniques that are specifically adapted to the problem of book alignment, one that aligns passages of varying size in the source document to sentences in the summary, guided by the unigram language model probability of the sentence under that passage; and one that generalizes the HMM alignment model of Och and Ney (2003) to the case of long but sparsely aligned documents."}, {"heading": "2 Related Work", "text": "This work builds on a long history of unsupervised word and phrase alignment originating in the machine translation literature, both for the task of learning alignments across parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al., 2004) and comparable corpora (Barzilay and Elhadad, 2003). For the related task of document/abstract alignment, we draw on work in document summarization (Marcu, 1999; Osborne, 2002; Daum\u00e9 and Marcu, 2005). Past approaches to fictional summarization, including both short stories (Kazantseva and Szpakowicz, 2010) and books (Mihalcea and Ceylan, 2007), have tended toward nondiscriminative methods; one notable exception is Ceylan (2011), which applies the Viterbi alignment\nmethod of Jing and McKeown (1999) to a set of 31 literary novels."}, {"heading": "3 Methods", "text": "We present two methods, both of which involve estimating the parameters of a hidden Markov model (HMM). The HMMs differ in their definitions of states, observations, and parameterizations of the emission distributions. We present a generic HMM first, then instantiate it with each of our two models, discussing their respective inference and learning algorithms in turn.\nLet S be the set of hidden states andK = |S|. An observation sequence t = \u3008t1, . . . , tn\u3009, each t` \u2208 V , is assigned probability:\np(t | n) = \u2211 z\u2208Sn \u03c0z1\n( n\u220f\n`=1\n\u03b7z`,t`\u03b3z`,z`+1\n) (1)\nwhere z is the sequence of hidden states, \u03c0 \u2208 \u2206K is the distribution over start states, and for all s \u2208 S, \u03b7s \u2208 \u2206|V| and \u03b3s \u2208 \u2206K are s\u2019s emission and transition distributions, respectively. Note that we avoid stopping probabilities by always conditioning on the sequence length."}, {"heading": "3.1 Passage Model", "text": "In the passage model, each HMM state corresponds to a contiguous passage in the source document. The intuition behind this approach is the following: while word and phrasal alignment attempts to capture fine-grained correspondences between a source and target document, longer documents that are distilled into comparatively short summaries may instead have long, topically coherent passages that are summarized into a single sentence. For example, the following summary sentence in a Wikipedia plot synopsis summarizes several long episodic passages in The Adventures of Tom Sawyer:\nAfter playing hooky from school on Friday and dirtying his clothes in a fight, Tom is made to whitewash the fence as punishment all of the next day.\nOur aim is to find the sequence of passages in the source document that aligns to the sequence of summary sentences. Therefore, we identify each HMM\nstate in s \u2208 S with source document positions is and js. When a summary sentence t` = \u3008t`,1, . . . , t`,T`\u3009 is sampled from state s, its emission probability is defined as follows:\n\u03b7s,t` = T\u220f\u0300 k=1 p\u0302unigram(t`,k | bis:js) (2)\nwhere bis:js is the passage in the source document from position is to position js; again, we avoid a stop symbol by implicitly assuming lengths are fixed exogenously. The unigram distribution p\u0302unigram(\u00b7 | bis:js) is estimated directly from the source document passage bis:js .\nThe transition distribution from state s \u2208 S, \u03b3s is operationalized following the HMM word alignment formulation of Vogel et al. (1996). The transition events between ordered pairs of states are binned by the difference in two passages\u2019 ranks within the source document.1 We give the formula for relative frequency estimation of the transition distributions:\n\u03b3s,s\u2032 = c(s\u2032 \u2212 s)\u2211\ns\u2032\u2032\u2208S c(s\u2212 s\u2032\u2032) (3)\nwhere c(\u00b7) denotes the count of jumps of a particular length, measured as the distance between the rank order of two passages within a document; the count of a jump between passage 10 and passage 13 is the same as that between passage 21 and 24; namely, c(3). Note that this distance is signed, so that the distance of a backwards jump from passage 13 to passage 10 (\u22123) is not the same as a jump from 10 to 13 (3).\nThe HMM states\u2019 spans are constrained not to overlap with each other, and they need not cover the source document. Because we do not know\n1These ranks are fixed; our inference procedure does not allow passages to overlap or to \u201cleapfrog\u201d over each other across iterations.\nthe boundary positions for states in advance, we must estimate them alongside the traditional HMM parameters. Figure 2 illustrates this scenario with a sequence of 17 words in the source document ([1 . . . 17]) and 4 sentences in the target summary ({a, b, c, d}). In this case, the states correspond to [1 . . . 4], [9 . . . 13], and [15 . . . 17]."}, {"heading": "3.1.1 Inference", "text": "Given a source document b and a target summary t, our aim is to infer the most likely passage z` for each sentence t`. This depends on the parameters (\u03c0, \u03b7, and \u03b3) and the passages associated with each state, so we estimate those as well, seeking to maximize likelihood. Our approach is an EM-like algorithm (Dempster et al., 1977); after initialization, it iterates among three steps:\n\u2022 E-step. Calculate p(t) and the posterior distributions q(zk | t) for each sentence tk. This is done using the forward-backward algorithm.\n\u2022 M-step. Estimate \u03c0 and \u03b3 from the posteriors, using the usual HMM M-step.\n\u2022 S-step. Sample new passages for each state. The sampling distribution considers, for each state s, moving is subject to the no-overlapping constraint and js, and then moving js subject to the no-overlapping constraint and is (DeNero et al., 2008). (See \u00a73.1.2 for more details.) The emission distribution \u03b7s is updated whenever is and js change, through Equation 2.\nFor the experiments described in section 4, each source document is initially divided into K equallength passages (K = 100), from which initial emission probabilities are defined; \u03c0 and \u03b3 are both initialized to uniform distribution. Boundary samples are collected once for each iteration, after one E step and one M step, for a total of 500 iterations."}, {"heading": "3.1.2 Sampling chunk boundaries", "text": "During the S-step, we sample the boundaries of each HMM state\u2019s passage, favoring (stochastically) those boundaries that make the observations more likely. We expect that, early on, most chunks will be radically reduced to smaller spans that match closely the target sentences aligned to them with high probability. Over subsequent iterations, longer spans should be favored when adding words at a boundary offsets the cost of adding the non-essential words between the old and new boundary.\nA greedy step\u2014analogous to the M-step use to estimate parameters\u2014is one way to do this: we could, on each S-step, move each span\u2019s boundaries to the positions that maximize likelihood under the revised language model. Good local choices, however, may lead to suboptimal global results, so we turn instead to sampling. Note that, if our model defined a marginal distribution over passage boundary positions in the source document, this sampling step could be interpreted as part of a Markov Chain Monte Carlo EM algorithm (Wei and Tanner, 1990). As it is, we do not have such a distribution; this equates to a fixed uniform distribution over all valid (non-overlapping) passage boundaries.\nThe implication is that the probability of a particular state s\u2019s passage\u2019s start- or end-position is proportional to the probability of the observations generated given that span. Following any E-step, the assignment of observations to swill be fractional. This means that the likelihood, as a function of particular values of is and js, depends on all of the sentences\nin the summary:\nL(is, js) = n\u220f `=1 \u03b7 q(z`=s|t) s,t`\n(4)\n= n\u220f\n`=1 ( T\u220f\u0300 k=1 p\u0302unigram(t`,k | bis:js) )q(z`=s|t)\nFor example, in Figure 2, the start position of the second span (word 9) might move anywhere from word 5 (just past the end of the previous span) to word 12 (just before the end of its own span, js = 12). Each of the values should be sampled with probability proportional to Equation 4, so that the sampling distribution is:\n1\u221112 i=5 L(i, 12) \u3008L(5, 12), L(6, 12), . . . , L(12, 12)\u3009\nCalculating L for different boundaries requires recalculating the emission probabilities \u03b7s,t` as the language model changes. We can do this efficiently (in linear time) by decomposing the language model probability. Here we represent a state s by its boundary positions in the source document, i : j, and we use the relative frequency estimate for p\u0302unigram .\nlog \u03b7i:j,t` = T\u2211\u0300 k=1 log freq(t`,k; bi:j) j \u2212 i+ 1 (5)\n= \u2212T` log(j \u2212 i+ 1) + T\u2211\u0300 k=1 log freq(t`,k; bi:j)\n(6)\nNow consider the change if we remove the first word from s\u2019s passage, so that its boundaries are [i+1, j].\nLet bi denote the source document\u2019s word at position i. log \u03b7i+1:j,t` =\n\u2212T` log(j \u2212 i) + T\u2211\u0300 k=1 log freq(t`,k; bi+1:j)\n= log \u03b7i:j,t` + freq(bi; t`) log freq(bi; bi:j)\u2212 1 freq(bi; bi:j)\n+ T` log j \u2212 i+ 1 j \u2212 i\n(7)\nThis recurrence is easy to solve for all possible left boundaries (respecting the no-overlap constraints) if we keep track of the word frequencies in each span of the source document\u2014something we must do anyway to calculate p\u0302unigram . A similar recurrence holds for the right boundary of a passage.\nFigure 3 illustrates the result of this sampling procedure on the start and end positions for a single source passage in Heart of Darkness. After 500 iterations, the samples can be seen to fluctuate over a span of approximately 600 words; however, the modes are relatively peaked, with the most likely start position at 1613, and the most likely end position at 1660 (yielding a span of 47 words)."}, {"heading": "3.2 Token Model", "text": "Jing and McKeown (1999) introduced an HMM whose states correspond to tokens in the source doc-\nument. The observation is the sequence of target summary tokens (restricting to those types found in the source document). The emission probabilities are fixed to be one if the source and target words match, zero if they do not. Hence each instance of v \u2208 V in the target summary is assumed to be aligned to an instance of v in the source. The transition parameters were fixed manually to simulate a ranked set of transition types (e.g., transitions within the same sentence are more likely than transitions between sentences). No parameter estimation is used; the Viterbi algorithm is used to find the most probable alignment. The allowable transition space is bounded by F 2, where F is the frequency of the most common token in the source document. The resulting model is scalable to large source documents (Ceylan and Mihalcea, 2009; Ceylan, 2011).\nOne potential issue with this model is that it lacks the concept of a null source, not articulated in the original HMM alignment model of Vogel et al. (1996) but added by Och and Ney (2003). Without such a null source, every word in the summary must be generated by some word in the source document. The consequence of this decision is that a Viterbi alignment over the summary must pick a perhaps distant, low-probability word in the source document if no closer word is available. Additionally, while the choice to enforce lexical identity constrains the state space, it also limits the range of lexical variation captured.\nOur second model extends Jing\u2019s approach in three ways.\nFirst, we introduce parameter inference to learn the values of start probabilities and transitions that maximize the likelihood of the data, using the EM algorithm. We operationalize the transition probabilities again following Vogel et al. (1996), but constrain the state space by only measuring transititions between fixed bucket lengths, rather than between the absolute position of each source word. The relative frequency estimator for transitions is:\n\u03b3s,s\u2032 = c(b(s\u2032 \u2212 s))\u2211\ns\u2032\u2032\u2208S c(b(s \u2032\u2032 \u2212 s))\n(8)\nAs above, c(\u00b7) denotes the count of an event, and here b(\u00b7) is a function that transforms the difference between two token positions into a coarser set of bins (for example, b may transform a distance of 0\ninto its own bin, a distance of +1 into a different bin, a distance in the range [+2,+10] into a third bin, a difference of [\u221210,\u22122] into a fourth, etc.). Future work may include dynamically learning optimizal bin sizes, much as boundaries are learned in the passage HMM.\nSecond, we introduce the concept of a null source that can generate words in the target sentence. In the sentence-to-sentence translation setting, for a source sentence that is m words long, Och and Ney (2003) add m corresponding NULL tokens, one for each source word position, to be able to adequately model transitions to, from and between NULL tokens in an alignment. For a source document that is ca. 100,000 words long, this is clearly infeasible (since the complexity of even a single round of forward-backward inference is O(m2n), where n is the number of words in the target summary t). However, we can solve this problem by noting that the transition probability as defined above is not measured between individual words, but rather between the positions of coarser-grained chunks that contain each word; by coarsing the transitions to model the jump between a fixed set ofB bins (whereB m), we effectively only need to add B null tokens, making inference tractable. As a final restriction, we disallow transitions between source state positions i and j where |i\u2212 j| > \u03c4 . In the experiments described in section 4, \u03c4 = 1000.\nThird, we expand the emission probabilities to allow the translation of a source word into a fixed set of synonyms (e.g., as derived from Roget\u2019s Thesaurus.2) This expands the coverage of important lexical variants while still constraining the allowable emission space to a reasonable size. All synonyms of a word are available as potential \u201ctranslations\u201d; the exact translation probability (e.g., \u03b7purchase,buy) is learned during inference."}, {"heading": "4 Experiments", "text": "To evaluate these two alignment methods and compare with past work, we evaluate on the downstream task of extractive book summarization.\n2http://www.gutenberg.org/ebooks/10681"}, {"heading": "4.1 Data", "text": "The available data includes 14,120 book plot summaries extracted from the November 2, 2012 dump of English-language Wikipedia3 and 31,393 English-language books from Project Gutenberg.4. We restrict the book/summary pairs to only those where the full text of the book contains at least 10,000 words and the paired abstract contains at least 100 words (stopwords and punctuation excluded). This results in a dataset of 439 book/summary pairs, where the average book length is 43,223 words, and the average summary length is 369 words (again, not counting stopwords and punctuation).\nThe ratio between summaries and full books in this dataset is approximately 1.2%, much smaller than that used in previous work for any domain, even for past work involving literary novels: Ceylan (2009) makes use of a collection of 31 books paired with relatively long summaries from SparkNotes, CliffsNotes and GradeSaver, where the average summary length is 6,800 words. We focus instead on the more concise case, targeting summaries that distill an entire book into approximately 500 words."}, {"heading": "4.2 Discriminative summarization", "text": "We follow a standard approach to discriminative summarization. All experiments described below use 10-fold cross validation, in which we partition the data into ten disjoint sets, train on nine of them and then test on the remaining held-out partition. Ten evaluations are conducted in total, with the reported accuracy being the average across all ten sets. First, all source books and paired summaries in the training set are aligned using one of the three unsupervised methods described above (Passage HMM, Token HMM, Jing 1999).\nNext, all of the sentences in the source side of the book/summary pairs are featurized; all sentences that have been aligned to a sentence in the summary are assiged a label of 1 (appearing in summary) and 0 otherwise (not appearing in summary). Using this featurized representation, we then train a binary logistic regression classifier with `2 regularization on the training data to learn which features are the most\n3http://dumps.wikimedia.org/enwiki/ 4http://www.gutenberg.org\nindicative of a source sentence appearing in a summary. Following previous work, we devise sentencelevel features that can be readily computed in comparison both with the document in which the sentence in found, and in comparison with the collection of documents as whole (Yeh et al., 2005; Shen et al., 2007). All feature values are binary:\n\u2022 Sentence position within document, discretized into membership in each of ten deciles. (10 features.)\n\u2022 Sentence contains a salient name. We operationalize \u201csalient name\u201d as the 100 capitalized words in a document with the highest TF-IDF score in comparison with the rest of the data; only non-sentence-initial tokens are used for calculate counts. (100 features.)\n\u2022 Contains lexical item x (x \u2208 most frequent 10,000 words). This captures the tendency for some actions, such as kills, dies to be more likely to appear in a summary. (10,000 features.)\n\u2022 Contains the first mention of lexical item x (x \u2208most frequent 10,000 words). (10,000 features.)\n\u2022 Contains a word that is among the top [1,10], [1,100], [1,1000] words having the highest TF/IDF scores for that book. (3 features.)\nWith a trained model and learned weights for all features, we next featurize each sentence in a test book according to the same set of features described above and predict whether or not it will appear in the summary. Sentences are then ranked by probability and the top sentences are chosen to create a summary of 1,000 words. To create a summary, sentences are then ordered according to their position in the source document."}, {"heading": "5 Evaluation", "text": "Document summarization has a standard (if imperfect) evaluation in the ROUGE score (Lin and Hovy, 2003), which, as an n-gram recall measure, stresses the ability of the candidate summary to recover the words in the reference. To evaluate the automatically generated summary, we calculate the ROUGE\nscore between the generated summary and the heldout reference summary from Wikipedia for each book. We consider both ROUGE-1, which measures the overlap of unigrams, and ROUGE-2, which measures bigram overlap. For the case of a single reference translation, ROUGE-N is calculated as the following (where w ranges over all unigrams or bigrams in the reference summary, depending on N , and c(\u00b7) is the count of the n-gram in the text).\n\u2211 w\u2208ref min(c(wref ), c(whyp))\u2211\nw\u2208ref c(wref ) (9)\nFigure 2 lists the results of a 10-fold test on the 439 available book/summary pairs. Both alignment models described above show a moderate improvement over the method of Jing et al. For comparison, we also present a baseline of simply choosing the first 1,000 words in the book as the summary.\nHow well does this method actually work in practice, however, at the task of generating summaries? Manually inspecting the generated summaries reveals that automatic summarization of books still has great room for improvement, for all alignment methods involved. Appendix A shows the sentences extracted as a summary for Heart of Darkness.\nIndependent of the quality of the generated summaries on held-out test data, one practical benefit of training binary log-linear models is that the resulting feature weights are interpretable, providing a datadriven glimpse into the qualities of a sentence that make it conducive to appearing in human-created summary. Table 3 lists the 25 strongest features predicting inclusion in the summary (rank-averaged over all ten training splits). The presence of a name in a sentence is highly predictive, as is its position at the beginning of a book (decile 0) or at the very end (decile 8 and 9). The strongest lexical features illustrate the importance of a character\u2019s persona, particularly in their relation with others (father, son,\netc.), as well as the natural importance of major life events (death). The importance of these features in the generated summary of Heart of Darkness is clear \u2013 nearly every sentence contains one name, and the most important plot point captured is indeed one such life event (\u201cMistah Kurtz \u2013 he dead.\u201d)."}, {"heading": "6 Conclusion", "text": "We present here two new methods optimized for aligning the full text of books with comparatively much shorter summaries, where the assumptions of the possibility of an exact word or phrase alignment may not always hold. While these methods perform competitively in a downstream evaluation, book summarization clearly remains a challenging task. Nevertheless, improved book/summary alignments hold intrinsic value in shedding light on what features of a work are deemed \u201csummarizable\u201d by human editors, and may potentially be exploited by tasks beyond summarization as well.\nA Generated summary for Heart of Darkness\n\u2022 \" And this also , \" said Marlow suddenly , \" has been one of the dark places of the earth . \" He was the only man of us who still \" followed the sea . \" The worst that could be said of him was that he did not represent his class .\n\u2022 No one took the trouble to grunt even ; and presently he said , very slow \u2013 \" I was thinking of very old times , when the Romans first came here , nineteen hundred years ago \u2013 the other day .... Light came out of this river since \u2013 you say Knights ?\n\u2022 We looked on , waiting patiently \u2013 there was nothing else to do till the end of the flood ; but it was only after a long silence , when he said , in a hesitating voice , \" I suppose you fellows remember I did once turn fresh - water sailor for a bit , \" that we knew we were fated , before the ebb began to run , to hear about one of Marlow \u2019 s inconclusive experiences .\n\u2022 I know the wife of a very high personage in the Administration , and also a man who has lots of influence with , \u2019 etc . She was determined to make no end of fuss to get me appointed skipper of a river steamboat , if such was my fancy .\n\u2022 He shook hands , I fancy , murmured vaguely , was satisfied with my French .\n\u2022 I found nothing else to do but to offer him one of my good Swede \u2019 s\n\u2022 Kurtz was ... I felt weary and irritable . \u2022 Kurtz was the best agent he had , an exceptional man ,\nof the greatest importance to the Company ; therefore I could understand his anxiety .\n\u2022 I heard the name of Kurtz pronounced , then the words , \u2019 take advantage of this unfortunate accident . \u2019 One of the men was the manager .\n\u2022 Kurtz , \u2019 I continued , severely , \u2019 is General Manager , you won \u2019 t have the opportunity . \u2019 \" He blew the candle out suddenly , and we went outside .\n\u2022 The approach to this Kurtz grubbing for ivory in the wretched bush was beset by as many dangers as though he had been an enchanted princess sleeping in a fabulous castle .\n\u2022 In a moment he came up again with a jump , possessed himself of both my hands , shook them continuously , while he gabbled : \u2019 Brother sailor ... honour ... pleasure ... delight ... introduce myself ... Russian ... son of an arch - priest ... Government of Tambov ... What ?\n\u2022 Where \u2019 s a sailor that does not smoke ? \" \" The pipe soothed him , and gradually I made out he had run\naway from school , had gone to sea in a Russian ship ; ran away again ; served some time in English ships ; was now reconciled with the arch - priest .\n\u2022 \" He informed me , lowering his voice , that it was Kurtz who had ordered the attack to be made on the steamer .\n\u2022 \" We had carried Kurtz into the pilot - house : there was more air there .\n\u2022 Suddenly the manager \u2019 s boy put his insolent black head in the doorway , and said in a tone of scathing contempt : \" \u2019 Mistah Kurtz \u2013 he dead . \u2019 \" All the pilgrims rushed out to see .\n\u2022 That is why I have remained loyal to Kurtz to the last , and even beyond , when a long time after I heard once more , not his own voice , but the echo of his magnificent eloquence thrown to me from a soul as translucently pure as a cliff of crystal .\n\u2022 Kurtz \u2019 s knowledge of unexplored regions must have been necessarily extensive and peculiar \u2013 owing to his great abilities and to the deplorable circumstances in which he had been placed : therefore \u2013 \u2019 I assured him Mr .\n\u2022 \u2019 There are only private letters . \u2019 He withdrew upon some threat of legal proceedings , and I saw him no more ; but another fellow , calling himself Kurtz \u2019 s cousin , appeared two days later , and was anxious to hear all the details about his dear relative \u2019 s last moments .\n\u2022 Incidentally he gave me to understand that Kurtz had been essentially a great musician .\n\u2022 I had no reason to doubt his statement ; and to this day I am unable to say what was Kurtz \u2019 s profession , whether he ever had any \u2013 which was the greatest of his talents .\n\u2022 This visitor informed me Kurtz \u2019 s proper sphere ought to have been politics \u2019 on the popular side . \u2019 He had furry straight eyebrows , bristly hair cropped short , an eyeglass on a broad ribbon , and , becoming expansive , confessed his opinion that Kurtz really couldn \u2019 t write a bit \u2013 \u2019 but heavens ! how that man could talk .\n\u2022 All that had been Kurtz \u2019 s had passed out of my hands : his soul , his body , his station , his plans , his ivory , his career .\n\u2022 And , by Jove ! the impression was so powerful that for me , too , he seemed to have died only yesterday \u2013 nay , this very minute .\n\u2022 He had given me some reason to infer that it was his impatience of comparative poverty that drove him out there . \" \u2019 ... Who was not his friend who had heard him speak once ? \u2019 she was saying .\n\u2022 Would they have fallen , I wonder , if I had rendered Kurtz that justice which was his due ?"}], "references": [{"title": "Sentence alignment for monolingual comparable corpora", "author": ["Barzilay", "Elhadad2003] Regina Barzilay", "Noemie Elhadad"], "venue": "In Proceedings of the 2003 conference on Empirical methods in natural language processing,", "citeRegEx": "Barzilay et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2003}, {"title": "A statistical approach to machine translation", "author": ["Brown et al.1990] Peter F. Brown", "John Cocke", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Fredrick Jelinek", "John D. Lafferty", "Robert L. Mercer", "Paul S. Roossin"], "venue": "Comput. Linguist.,", "citeRegEx": "Brown et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1990}, {"title": "The decomposition of human-written book summaries", "author": ["Ceylan", "Mihalcea2009] Hakan Ceylan", "Rada Mihalcea"], "venue": "In CICLing\u201909,", "citeRegEx": "Ceylan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ceylan et al\\.", "year": 2009}, {"title": "Investigating the Extractive Summarization of Literary Novels", "author": ["Hakan Ceylan"], "venue": "Ph.D. thesis,", "citeRegEx": "Ceylan.,? \\Q2011\\E", "shortCiteRegEx": "Ceylan.", "year": 2011}, {"title": "Induction of word and phrase alignments for automatic document summarization", "author": ["Daum\u00e9", "III Marcu2005] Hal Daum\u00e9", "Daniel Marcu"], "venue": "Comput. Linguist.,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2005}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["M.N. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Sampling alignment structure under a bayesian translation model", "author": ["DeNero et al.2008] John DeNero", "Alexandre BouchardC\u00f4t\u00e9", "Dan Klein"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "DeNero et al\\.,? \\Q2008\\E", "shortCiteRegEx": "DeNero et al\\.", "year": 2008}, {"title": "The decomposition of humanwritten summary sentences", "author": ["Jing", "McKeown1999] Hongyan Jing", "Kathleen R. McKeown"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Jing et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jing et al\\.", "year": 1999}, {"title": "Summarizing short stories", "author": ["Kazantseva", "Szpakowicz2010] Anna Kazantseva", "Stan Szpakowicz"], "venue": "Computational Linguistics,", "citeRegEx": "Kazantseva et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kazantseva et al\\.", "year": 2010}, {"title": "Automatic evaluation of summaries using ngram co-occurrence statistics", "author": ["Lin", "Hovy2003] Chin-Yew Lin", "Eduard Hovy"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "The automatic construction of large-scale corpora for summarization research", "author": ["Daniel Marcu"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Marcu.,? \\Q1999\\E", "shortCiteRegEx": "Marcu.", "year": 1999}, {"title": "Explorations in automatic book summarization", "author": ["Mihalcea", "Ceylan2007] Rada Mihalcea", "Hakan Ceylan"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learn-", "citeRegEx": "Mihalcea et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2007}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Comput. Linguist.,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Using maximum entropy for sentence extraction", "author": ["Miles Osborne"], "venue": "In Proceedings of the ACL-02 Workshop on Automatic Summarization - Volume", "citeRegEx": "Osborne.,? \\Q2002\\E", "shortCiteRegEx": "Osborne.", "year": 2002}, {"title": "Monolingual machine translation for paraphrase generation", "author": ["Quirk et al.2004] Chris Quirk", "Chris Brockett", "William Dolan"], "venue": "Proceedings of EMNLP", "citeRegEx": "Quirk et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 2004}, {"title": "Document summarization using conditional random fields", "author": ["Shen et al.2007] Dou Shen", "Jian-Tao Sun", "Hua Li", "Qiang Yang", "Zheng Chen"], "venue": "In Proceedings of the 20th international joint conference on Artifical intelligence,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Hmm-based word alignment in statistical translation", "author": ["Vogel et al.1996] Stephan Vogel", "Hermann Ney", "Christoph Tillmann"], "venue": "In Proceedings of the 16th conference on Computational linguistics - Volume", "citeRegEx": "Vogel et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 1996}, {"title": "A Monte Carlo Implementation of the EM Algorithm and the Poor Man\u2019s Data Augmentation Algorithms", "author": ["Wei", "Tanner1990] Greg C.G. Wei", "Martin A. Tanner"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Wei et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Wei et al\\.", "year": 1990}, {"title": "Text summarization using a trainable summarizer and latent semantic analysis", "author": ["Yeh et al.2005] Jen-Yuan Yeh", "Hao-Ren Ke", "Wei-Pang Yang", "I-Heng Meng"], "venue": "Inf. Process. Manage.,", "citeRegEx": "Yeh et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yeh et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 10, "context": "Supervised approaches to this problem make use of training data in the form of source documents paired with existing summaries (Marcu, 1999; Osborne, 2002; Jing and McKeown, 1999; Ceylan and Mihalcea, 2009).", "startOffset": 127, "endOffset": 206}, {"referenceID": 13, "context": "Supervised approaches to this problem make use of training data in the form of source documents paired with existing summaries (Marcu, 1999; Osborne, 2002; Jing and McKeown, 1999; Ceylan and Mihalcea, 2009).", "startOffset": 127, "endOffset": 206}, {"referenceID": 1, "context": "For short texts and training pairs where a one-to-one alignment between source and abstract sentences can be expected, standard techniques from machine translation can be applied, including word-level alignment (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003) and longer phrasal alignment (Daum\u00e9 and Marcu, 2005), especially as adapted to the monolingual setting (Quirk et al.", "startOffset": 211, "endOffset": 270}, {"referenceID": 16, "context": "For short texts and training pairs where a one-to-one alignment between source and abstract sentences can be expected, standard techniques from machine translation can be applied, including word-level alignment (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003) and longer phrasal alignment (Daum\u00e9 and Marcu, 2005), especially as adapted to the monolingual setting (Quirk et al.", "startOffset": 211, "endOffset": 270}, {"referenceID": 14, "context": ", 1996; Och and Ney, 2003) and longer phrasal alignment (Daum\u00e9 and Marcu, 2005), especially as adapted to the monolingual setting (Quirk et al., 2004).", "startOffset": 130, "endOffset": 150}, {"referenceID": 10, "context": "While the ratio between abstracts and source documents in the benchmark Ziff-Davis corpus of newswire (Marcu, 1999) is approximately 12% (133 words vs.", "startOffset": 102, "endOffset": 115}, {"referenceID": 1, "context": "This work builds on a long history of unsupervised word and phrase alignment originating in the machine translation literature, both for the task of learning alignments across parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al.", "startOffset": 190, "endOffset": 270}, {"referenceID": 16, "context": "This work builds on a long history of unsupervised word and phrase alignment originating in the machine translation literature, both for the task of learning alignments across parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al.", "startOffset": 190, "endOffset": 270}, {"referenceID": 6, "context": "This work builds on a long history of unsupervised word and phrase alignment originating in the machine translation literature, both for the task of learning alignments across parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al.", "startOffset": 190, "endOffset": 270}, {"referenceID": 14, "context": ", 2008) and between monolingual (Quirk et al., 2004) and comparable corpora (Barzilay and Elhadad, 2003).", "startOffset": 32, "endOffset": 52}, {"referenceID": 10, "context": "For the related task of document/abstract alignment, we draw on work in document summarization (Marcu, 1999; Osborne, 2002; Daum\u00e9 and Marcu, 2005).", "startOffset": 95, "endOffset": 146}, {"referenceID": 13, "context": "For the related task of document/abstract alignment, we draw on work in document summarization (Marcu, 1999; Osborne, 2002; Daum\u00e9 and Marcu, 2005).", "startOffset": 95, "endOffset": 146}, {"referenceID": 1, "context": "This work builds on a long history of unsupervised word and phrase alignment originating in the machine translation literature, both for the task of learning alignments across parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al., 2004) and comparable corpora (Barzilay and Elhadad, 2003). For the related task of document/abstract alignment, we draw on work in document summarization (Marcu, 1999; Osborne, 2002; Daum\u00e9 and Marcu, 2005). Past approaches to fictional summarization, including both short stories (Kazantseva and Szpakowicz, 2010) and books (Mihalcea and Ceylan, 2007), have tended toward nondiscriminative methods; one notable exception is Ceylan (2011), which applies the Viterbi alignment method of Jing and McKeown (1999) to a set of 31 literary novels.", "startOffset": 191, "endOffset": 748}, {"referenceID": 1, "context": "This work builds on a long history of unsupervised word and phrase alignment originating in the machine translation literature, both for the task of learning alignments across parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al., 2004) and comparable corpora (Barzilay and Elhadad, 2003). For the related task of document/abstract alignment, we draw on work in document summarization (Marcu, 1999; Osborne, 2002; Daum\u00e9 and Marcu, 2005). Past approaches to fictional summarization, including both short stories (Kazantseva and Szpakowicz, 2010) and books (Mihalcea and Ceylan, 2007), have tended toward nondiscriminative methods; one notable exception is Ceylan (2011), which applies the Viterbi alignment method of Jing and McKeown (1999) to a set of 31 literary novels.", "startOffset": 191, "endOffset": 819}, {"referenceID": 16, "context": "The transition distribution from state s \u2208 S, \u03b3s is operationalized following the HMM word alignment formulation of Vogel et al. (1996). The transition events between ordered pairs of states are binned by the difference in two passages\u2019 ranks within the source document.", "startOffset": 116, "endOffset": 136}, {"referenceID": 5, "context": "Our approach is an EM-like algorithm (Dempster et al., 1977); after initialization, it iterates among three steps:", "startOffset": 37, "endOffset": 60}, {"referenceID": 6, "context": "The sampling distribution considers, for each state s, moving is subject to the no-overlapping constraint and js, and then moving js subject to the no-overlapping constraint and is (DeNero et al., 2008).", "startOffset": 181, "endOffset": 202}, {"referenceID": 3, "context": "The resulting model is scalable to large source documents (Ceylan and Mihalcea, 2009; Ceylan, 2011).", "startOffset": 58, "endOffset": 99}, {"referenceID": 3, "context": "The resulting model is scalable to large source documents (Ceylan and Mihalcea, 2009; Ceylan, 2011). One potential issue with this model is that it lacks the concept of a null source, not articulated in the original HMM alignment model of Vogel et al. (1996) but added by Och and Ney (2003).", "startOffset": 59, "endOffset": 259}, {"referenceID": 3, "context": "The resulting model is scalable to large source documents (Ceylan and Mihalcea, 2009; Ceylan, 2011). One potential issue with this model is that it lacks the concept of a null source, not articulated in the original HMM alignment model of Vogel et al. (1996) but added by Och and Ney (2003). Without such a null source, every word in the summary must be generated by some word in the source document.", "startOffset": 59, "endOffset": 291}, {"referenceID": 3, "context": "The resulting model is scalable to large source documents (Ceylan and Mihalcea, 2009; Ceylan, 2011). One potential issue with this model is that it lacks the concept of a null source, not articulated in the original HMM alignment model of Vogel et al. (1996) but added by Och and Ney (2003). Without such a null source, every word in the summary must be generated by some word in the source document. The consequence of this decision is that a Viterbi alignment over the summary must pick a perhaps distant, low-probability word in the source document if no closer word is available. Additionally, while the choice to enforce lexical identity constrains the state space, it also limits the range of lexical variation captured. Our second model extends Jing\u2019s approach in three ways. First, we introduce parameter inference to learn the values of start probabilities and transitions that maximize the likelihood of the data, using the EM algorithm. We operationalize the transition probabilities again following Vogel et al. (1996), but constrain the state space by only measuring transititions between fixed bucket lengths, rather than between the absolute position of each source word.", "startOffset": 59, "endOffset": 1031}, {"referenceID": 3, "context": "2%, much smaller than that used in previous work for any domain, even for past work involving literary novels: Ceylan (2009) makes use of a collection of 31 books paired with relatively long summaries from SparkNotes, CliffsNotes and GradeSaver, where the average summary length is 6,800 words.", "startOffset": 111, "endOffset": 125}, {"referenceID": 18, "context": "Following previous work, we devise sentencelevel features that can be readily computed in comparison both with the document in which the sentence in found, and in comparison with the collection of documents as whole (Yeh et al., 2005; Shen et al., 2007).", "startOffset": 216, "endOffset": 253}, {"referenceID": 15, "context": "Following previous work, we devise sentencelevel features that can be readily computed in comparison both with the document in which the sentence in found, and in comparison with the collection of documents as whole (Yeh et al., 2005; Shen et al., 2007).", "startOffset": 216, "endOffset": 253}], "year": 2013, "abstractText": "We consider the unsupervised alignment of the full text of a book with a human-written summary. This presents challenges not seen in other text alignment problems, including a disparity in length and, consequent to this, a violation of the expectation that individual words and phrases should align, since large passages and chapters can be distilled into a single summary phrase. We present two new methods, based on hidden Markov models, specifically targeted to this problem, and demonstrate gains on an extractive book summarization task. While there is still much room for improvement, unsupervised alignment holds intrinsic value in offering insight into what features of a book are deemed worthy of summarization.", "creator": "LaTeX with hyperref package"}}}