{"id": "1305.2254", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2013", "title": "Programming with Personalized PageRank: A Locally Groundable First-Order Probabilistic Logic", "abstract": "in many probabilistic first - gen order representation systems, inference retrieval is performed typically by \" grounding \" - - - i. e., mapping it to a propositional representation, using and then performing propositional inference. with a large database of semantic facts, groundings can be chosen very large, avoiding making inference and learning computationally expensive. here we present a first - order probabilistic language which always is well - suited to approximate \" local \" symbolic grounding : every query $ q $ can be approximately grounded with completing a small graph. the language programming is an extension of stochastic logic programs where inference is performed just by modeling a variant of personalized pagerank. experimentally, we show that the approach paradigm performs well without weight learning on an entity resolution task ; that supervised weight - learning learning improves accuracy ; and again that grounding time execution is independent of db size. we also show well that order - of - magnitude speedups are possible by parallelizing learning.", "histories": [["v1", "Fri, 10 May 2013 04:16:15 GMT  (321kb,D)", "http://arxiv.org/abs/1305.2254v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["william yang wang", "kathryn mazaitis", "william w cohen"], "accepted": false, "id": "1305.2254"}, "pdf": {"name": "1305.2254.pdf", "metadata": {"source": "CRF", "title": "Programming with Personalized PageRank: A Locally Groundable First-Order Probabilistic Logic", "authors": ["William Y. Wang", "Kathryn Mazaitis"], "emails": [], "sections": [{"heading": null, "text": "In many probabilistic first-order representation systems, inference is performed by \u201cgrounding\u201d\u2014i.e., mapping it to a propositional representation, and then performing propositional inference. With a large database of facts, groundings can be very large, making inference and learning computationally expensive. Here we present a firstorder probabilistic language which is wellsuited to approximate \u201clocal\u201d grounding: every query Q can be approximately grounded with a small graph. The language is an extension of stochastic logic programs where inference is performed by a variant of personalized PageRank. Experimentally, we show that the approach performs well without weight learning on an entity resolution task; that supervised weight-learning improves accuracy; and that grounding time is independent of DB size. We also show that order-of-magnitude speedups are possible by parallelizing learning."}, {"heading": "1 INTRODUCTION", "text": "In many probabilistic first-order representation systems, including Markov Logic Networks [13] and Probabilistic Similarity Logic [3], inference is performed by mapping a first-order program to a propositional representation, and performing inference in that propositional representation. This mapping is often called grounding. For example, Figure 1 shows a simple MLN.1 As is often the case, this MLN has two parts: the rules R1, R2, which are weighted first-order clauses; and the database DB, which consists of facts (unit clauses) of the form links(a,b) for constants a, b.\n1This MLN does a very simple sort of label-propagation through hyperlinks.\nFigure 1: A Markov logic network program and its grounding. (Dotted lines are clique potentials associated with rule R2, solid lines with rule R1.)\nThe figure also shows the the grounded version of this MLN, which is an ordinary Markov network: the DB facts become constraints on node values, and the clauses become clique potentials.\nGrounding a first-order program can be an expensive operation. For a realistic hyperlink graph, a Markov network with size even linear in the number of facts in the database, |DB|, is impractically large for inference. Superficially, it would seem that groundings must inheritly be o(|DB|) for some programs: in the example, for instance, the probability of aboutSport(x) must depends to some extent on the entire hyperlink graph (if it is fully connected). However, it also seems intuitive that if we are interested in inferring information about a specific page\u2014say, the probability of aboutSport(d1)\u2013then the parts of the network only distantly connected to d1 are likely to have a small influence. This suggests that an approximate grounding strategy might be feasible, in which a query such as aboutSport(d1) would be grounded by constructing a small subgraph of the full network, followed by inference on this small \u201clocally grounded\u201d subgraph. Likewise, consider learning (e.g., from a set of queries Q with their desired truth values). Learning might proar X\niv :1\n30 5.\n22 54\nv1 [\ncs .A\nI] 1\n0 M\nay 2\nceed by locally-grounding every query goal, allowing learning to also take less than O(|DB|) time.\nIn this paper, we present a first-order probabilistic language which is well-suited to approximate \u201clocal\u201d grounding. We present an extension to stochastic logic programs (SLP) [5] that is biased towards short derivations, and show that this is related to personalized PageRank (PPR) [12, 4] on a linearized version of the proof space. Based on the connection to PPR, we develop a proveably-correct approximate inference scheme, and an associated proveably-correct approximate grounding scheme: specifically, we show that it is possible to prove a query, or to build a graph which contains the information necessary for weightlearning, in time O( 1\u03b1 ), where \u03b1 is a reset parameter associated with the bias towards short derivations, and is the worst-case approximation error across all intermediate stages of the proof. This means that both inference and learning can be approximated in time independent of the size of the underlying database\u2014a surprising and important result.\nThe ability to locally ground queries has another important consequence: it is possible to decompose the problem of weight-learning to a number of moderatesize subtasks (in fact, tasks of size O( 1\u03b1 ) or less) which are weakly coupled. Based on this we outline a parallelization scheme, which in our initial implementation provides a order-of-magnitude speedup in learning time.\nBelow, we will first introduce our formalism, and then describe our weight-learning algorithm. We will then present experimental results on a prototypical inference task, and compare the scalability of our method to Markov logic networks. We finally discuss related work and conclude."}, {"heading": "2 Programming with Personalized PageRank (PROPPR)", "text": ""}, {"heading": "2.1 LOGIC PROGRAM INFERENCE AS GRAPH SEARCH", "text": "We will now describe our \u201clocally groundable\u201d firstorder probabilistic language, which we call ProPPR. Inference for ProPPR is based on a personalized PageRank process over the proof constructed by Prolog\u2019s Selective Linear Definite (SLD) theorem-prover. To define the semantics we will use notation from logic programming [8]. Let LP be a program which contains a set of definite clauses c1, . . . , cn, and consider a conjunctive query Q over the predicates appearing in LP . A traditional Prolog interpreter can be viewed as having the following actions. First, construct a \u201croot vertex\u201d v0 which is a pair (Q,Q) and add it to an\notherwise-empty graph G\u2032Q,LP . (For brevity, we will use drop the subscripts of G\u2032 where possible.) Then recursively add to G\u2032 new vertices and edges as follows: if u is a vertex of the form (Q, (R1, . . . , Rk)), and c is a clause in LP of the form R\u2032 \u2190 S\u20321, . . . , S\u2032`, and R1 and R\n\u2032 have a most general unifier \u03b8 = mgu(R1, R\n\u2032), then add to G\u2032 a new edge u\u2192 v where v = (Q\u03b8, (S\u20321, . . . , S \u2032 `, R2, . . . , Rk)\u03b8). Let us call Q\u03b8 the transformed query and (S\u20321, . . . , S \u2032 `, R2, . . . , Rk)\u03b8 the associated subgoal list. If a subgoal list is empty, we will denote it by 2.\nG\u2032 is often large or infinite so it is not constructed explicitly. Instead Prolog performs a depth-first search on G\u2032 to find the first solution vertex v\u2014i.e., a vertex with an empty subgoal list\u2014and if one is found, returns the transformed query from v as an answer to Q. Table 1 and Figure 2 show a simple Prolog program and a proof graph for it.2 Given the query Q = about(a,Z), Prolog\u2019s depth-first search would return Q = about(a,fashion).\nNote that in this proof formulation, the nodes are conjunctions of literals, and the structure is, in general, a digraph (rather than a tree). Also note that the proof is encoded as a graph, not a hypergraph, even if the predicates in the LP are not binary: the edges represent a step in the proof that reduces one conjunction to another, not a binary relation between entities."}, {"heading": "2.2 FROM STOCHASTIC LOGIC PROGRAMS TO PROPPR", "text": "In stochastic logic programs (SLPs) [5], one defines a randomized procedure for traversing the graph G\u2032 which thus defines a probability distribution over vertices v, and hence (by selecting only solution vertices) a distribution over transformed queries (i.e. answers) Q\u03b8. The randomized procedure thus produces a distribution over possible answers, which can be tuned\n2The annotations after the hashmarks and the edge labels in the proof graph will be described below. For conciseness, only R1, . . . , Rk is shown in each node u = (Q, (R1, . . . , Rk)).\nby learning to upweight desired (correct) answers and downweight others.\nIn past work, the randomized traversal of G\u2032 was defined by a probabilistic choice, at each node, of which clause to apply, based on a weight for each clause. We propose two extensions. First, we will introduce a new way of computing clause weights, which allows for a potentially richer parameterization of the traversal process. We will associate with each edge u\u2192 v in the graph a feature vector \u03c6u\u2192v. This edge is produced indirectly, by associating with every clause c \u2208 LP a function \u03a6c(\u03b8), which produces the \u03c6 associated with an application of c using mgu \u03b8. This feature vector is computed during theorem-proving, and used to annotate the edge u\u2192 v in G\u2032 created by applying c with mgu \u03b8. Finally, an edge u\u2192 v will be traversed with probability Pr(v|u) \u221d f(w, \u03c6u\u2192v) where w is a parameter vector and where f(w, \u03c6) is a weighting function\u2014e.g., f(w, \u03c6) = exp(wi \u00b7\u03c6). This weighting function now determines the probability of a transition, in theorem-proving, from u to v: specifically, Prw(v|u) \u221d f(w, \u03c6u\u2192v). Weights in w default to 1.0, and learning consists of tuning these.\nThe second and more fundamental extension is to add edges in G\u2032 from every solution vertex to itself, and also add an edge from every vertex to the start vertex v0. We will call this augmented graph GQ,LP below (or just G if the subscripts are clear from context). These links make SLP\u2019s graph traversal a personalized PageRank (PPR) procedure, sometimes known as random-walk-with-restart [18]. These links are annotated by another feature vector function \u03a6restart(R), which is a applied of the leftmost literal of the subgoal list for u to annotate the edge u\u2192 v.\nThese links back to the start vertex bias will the traversal of the proof graph to upweight the results of short proofs. To see this, note that if the restart probability P (v0|u) = \u03b1 for every node u, then the probability of reaching any node at depth d is bounded by (1\u2212 \u03b1)d.\nTo summarize, if u is a node of the search graph, u = (Q\u03b8, (R1, . . . , Rk)), then the transitions from u, and their respective probabilities, are defined as follows, where Z is an appropriate normalizing constant:\n\u2022 If v = (Q\u03b8\u03c3, (S\u20321, . . . , S\u2032`, R2, . . . , Rk)\u03b8\u03c3) is a state derived by applying the clause c (with mgu \u03c3), then\nPr w (v|u) = 1 Z f(w,\u03a6c(\u03b8 \u25e6 \u03c3))\n\u2022 If v = v0 = (Q,Q) is the initial state in G, then\nPr w (v|u) = 1 Z f(w,\u03a6restart(R1\u03b8))\n\u2022 If v is any other node, then Pr(v|u) = 0.\nFinally we must specify the functions \u03a6c and \u03a6restart. For clauses in LP , the feature-vector producing function \u03a6c(\u03b8) for a clause is specified by annotating c as follows: every clause c = (R \u2190 S1, . . . , Sk) can be annotated with an additional conjunction of \u201cfeature literals\u201d F1, . . . , F`, which are written at the end of the clause after the special marker \u201c#\u201d. The function \u03a6c(\u03b8) then returns a vector \u03c6 = {F1\u03b8, . . . , F`\u03b8}, where every Fi\u03b8 must be ground.\nThe requirement3 that edge features Fi\u03b8 are ground is the reason for introducing the apparently unnecessary\n3The requirement that the feature literals returned by \u03c6c(\u03b8) must be ground in \u03b8 is not strictly necessary for cor-\npredicate linkedBy(X,Y,W) into the program of Table 1: adding the feature literal by(W) to the second clause for sim would result in a non-ground feature by(W), since W is a free variable when \u03a6c is called. Notice also that the weight on the by(W) features are meaningful, even though there is only one clause in the definition of linkedBy, as the weight for applying this clause competes with the weight assigned to the restart edges.\nIt would be cumbersome to annotate every database fact, and difficult to learn weights for so many features. Thus, if c is the unit clause that corresponds to a database fact, then \u03a6c(\u03b8) returns a default value \u03c6 = {db}, where db is a special feature indicating that a database predicate was used.4\nThe function \u03a6restart(R) depends on the functor and arity of R. If R is defined by clauses in LP , then \u03a6restart(R) returns a unit vector \u03c6 = {defRestart}. If R is a database predicate (e.g., hasWord(doc1,W)) then we follow a slightly different procedure, which is designed to ensure that the restart link has a reasonably large weight even with unit feature weights: we compute n, the number of possible bindings for R, and set \u03c6[defRestart] = n \u00b7 \u03b11\u2212\u03b1 , where \u03b1 is a global parameter. This means that with unit weights, after normalization, the probability of following the restart link will be \u03b1.\nPutting this all together with the standard iterative approach to computing personalized PageRank over a graph [12], we arrive at the following inference algorithm for answering a query Q, using a weight vector w. Below, we let Nv0(u) denote the neighbors of u\u2014 i.e., the set of nodes v where Pr(v|u) > 0 (including the restart node v = v0). We also let W be a matrix such that W[u, v] = Prw(v|u), and in our discussion, we use ppr(v0) to denote the personalized PageRank vector for v0.\n1. Let v0 = (Q,Q) be the start node of the search graph. Let G be a graph containing just v0. Let v0 = {v0}.\n2. For t = 1, . . . , T (i.e., until convergence):\n(a) Let vt be an all-zeros vector. (b) For each u with non-zero weight in vt\u22121, and each v \u2208 Nu+0(u), add (u, v, \u03c6u\u2192v) to G with weight Prw(v|u), and set vt = W \u00b7 vt\u22121\nrectness. However, in developing ProPPR programs we noted than non-ground features were usually not what the programmer intended.\n4If a non-database clause c has no annotation, then the default vector is \u03c6 = {id(c)}, where c is an identifier for the clause c.\n3. At this point vT \u2248 ppr(v0). Let S be the set of nodes (Q\u03b8,2) that have empty subgoal lists and non-zero weight in vT , and let Z = \u2211 u\u2208S v\nT [u]. The final probability for the literal L = Q\u03b8 is found by extracting these solution nodes S, and renormalizing:\nPr w (L) \u2261 1 Z vT [(L,2)]\nFor example, given the query Q = about(a,Z) and the program of Table 1, this procedure would give assign a non-zero probability to the literals about(a,sport) and about(a,fashion), concurrently building the graph of Figure 2."}, {"heading": "2.3 LOCALLY GROUNDING A QUERY", "text": "Note that this procedure both performs inference (by computing a distribution over literals Q\u03b8) and \u201cgrounds\u201d the query, by constructing a graph G. ProPPR inference for this query can be re-done efficiently, by running an ordinary PPR process on G. This is useful for faster weight learning. Unfortunately, the grounding G can be very large: it need not include the entire database, but if T is the number of iterations until convergence for the sample program of Table 1 on the query Q = about(d, Y ), G will include a node for every page within T hyperlinks of d.\nTo construct a more compact local grounding graph G, we adapt an approximate personalized PageRank method called PageRank-Nibble [1]. This method has been used for the problem of local partitioning : in local partitioning, the goal is to find a small, lowconductance5 component of a large graph G that contains a given node v.\nThe PageRank-Nibble-Prove algorithm is shown in Table 2. It maintains two vectors: p, an approximation to the personalized PageRank vector associated with node v0, and r, a vector of \u201cresidual errors\u201d in p. Initially, p = \u2205 and r = {v0}. The algorithm repeatedly picks a node u with a large residual error r[u], and reduces this error by distributing a fraction \u03b1\u2032 of it to p[u], and the remaining fraction back to r[u] and r[v1], . . . , r[vn], where the vi\u2019s are the neighbors of u. The order in which nodes u are picked does not matter for the analysis (in our implementation, we follow Prolog\u2019s usual depth-first search as much as possible.) Relative to PageRank-Nibble, the main differences are the the use of a lower-bound on \u03b1 rather than a fixed restart weight and the construction of the graph G\u0302.\n5For small subgraphs GS , conductance of GS is the ratio of the weight of all edges exiting GS to the weight of all edges incident on a node in GS .\nTable 2: The PageRank-Nibble-Prove algorithm for inference in ProPPR. \u03b1\u2032 is a lower-bound on Pr(v0|u) for any node u to be added to the graph G\u0302, and is the desired degree of approximation.\ndefine PageRank-Nibble-Prove(Q): let v =PageRank-Nibble((Q,Q), \u03b1\u2032, ) let S = {u : p[u] > u and u = (Q\u03b8,2)} let Z = \u2211 u\u2208S p[u]\ndefine Prw(L) \u2261 1Zv[(L,2)] end\ndefine PageRank-Nibble(v0, \u03b1 \u2032, ):\nlet p = r = 0, let r[v0] = 1, and let G\u0302 = \u2205 while \u2203u : r(u)/|N(u)| > do: push(u) return p\nend\ndefine push(u):\ncomment: this modifies p, r, and G\u0302 p[u] = p[u] + \u03b1\u2032 \u00b7 r[u] r[u] = r[u] \u00b7 (1\u2212 \u03b1\u2032) for v \u2208 N(u):\nadd the edge (u, v, \u03c6u\u2192v) to G\u0302 if v = v0 then r[v] = r[v] + Pr(v|u)r[u] else r[v] = r[v] + (Pr(v|u)\u2212 \u03b1\u2032)r[u]\nendfor end\nFollowing the proof technique of Andersen et al, it can be shown that after each push, p + r = ppr(v0). It is also clear than when PageRank-Nibble terminates, then for any u, the error ppr(v0)[u]\u2212p[u] is bounded by N(u): hence, in any graph where N(u) is bounded, a good approximation can be obtained. It can also be shown [1] that the subgraph G\u0302 (of the full proof space) is in some sense a \u201cuseful\u201d subset: for an appropriate setting of , if there is a low-conductance subgraph G\u2217 of the full graph that contains v0, then G\u2217 will be contained in G\u0302: thus if there is a subgraph G\u2217 containing v0 that approximates the full graph well, PageRank-Nibble will find (a supergraph of) G\u2217.\nFinally, we have the following efficiency bound:\nTheorem 1 (Andersen,Chung,Lang) Let ui be the i-th node pushed by PageRank-Nibble-Prove. Then\u2211 i |N(ui)| < 1 \u03b1\u2032 .\nThis can be proved by noting that initially |r|1 = 1, and also that |r|1 decreases by at least \u03b1\u2032 |N(ui)| on the i-th push. As a direct consequence we have the following:\nCorollary 1 The number of edges in the graph G\u0302 produced by PageRank-Nibble-Prove is no more than 1\u03b1\u2032 .\nImportantly, the bound holds independent of the size of the full database of facts. The bound also holds regardless of the size or loopiness of the full proof graph, so this inference procedure will work for recursive logic programs.\nTo summarize, we have outlined an efficient approximate proof procedure, which is closely related to personalized PageRank. As a side-effect of inference for a query Q, this procedure will create a ground graph G\u0302Q on which personalized PageRank can be run di-\nrectly, without any (relatively expensive) manipulation of first-order theorem-proving constructs such as clauses or logical variables. As we will see, this \u201clocally grounded\u201d graph will be very useful in learning weights w to assign to the features of a ProPPR program.\nAs an illustration of the sorts of ProPPR programs that are possible, some small sample programs are shown in Figure 3. Clauses c1 and c2 are, together, a bag-of-words classifier: each proof of predictedClass(D,Y) adds some evidence for D having class Y , with the weight of this evidence depending on the weight given to c2\u2019s use in establishing related(w,y), where w and y are a specific word in D and y is a possible class label. In turn, c2\u2019s weight depends on the weight assigned to the r(w, y) feature by w, relative to the weight of the restart link.6 Adding c3 and c4 to this program implements label propagation, and adding c5 and c6 implements a sequential classifier.\nIn spite of its efficient inference procedure, and its limitation to only definite clauses, ProPPR appears to have much of the expressive power of MLNs [6], in that many useful heuristics can apparently be encoded."}, {"heading": "2.4 LEARNING FOR PROPPR", "text": "As noted above, inference for a query Q in ProPPR is based on a personalized PageRank process over the graph associated with the SLD proof of a query goal G. More specifically, the edges u\u2192 v of the graph G are annotated with feature vectors \u03c6u\u2192v, and from these feature vectors, weights are computed using a parameter vector w, and finally normalized to form a\n6The existence of the restart link thus has another important role in this program, as it avoids a sort of \u201clabel bias problem\u201d in which local decisions are difficult to adjust.\nprobability distribution over the neighbors of u. The \u201cgrounded\u201d version of inference is thus a personalized PageRank process over a graph with feature-vector annotated edges.\nIn prior work, Backstrom and Leskovic [2] outlined a family of supervised learning procedures for this sort of annotated graph. In the simpler case of their learning procedure, an example is a triple (v0, u, y) where v0 is a query node, u is a node in in the personalized PageRank vector pv0 for v0, y is a target value, and a loss `(v0, u, y) is incurred if pv0 [u] 6= y. In the more complex case of \u201clearning to rank\u201d, an example is a triple (v0, u+, u\u2212) where v0 is a query node, u+ and u\u2212 are nodes in in the personalized PageRank vector pv0 for v0, and a loss is incurred unless pv0 [u+] \u2265 pv0 [u\u2212]. The core of Backstrom and Leskovic\u2019s result is a method for computing the gradient of the loss on an example, given a differentiable feature-weighting function f(w, \u03c6) and a differentiable loss function `. The gradient computation is broadly similar to the power-iteration method for computation of the personalized PageRank vector for v0. Given the gradient, a number of optimization methods can be used to compute a local optimum.\nWe adopt this learning for ProPPR, with some modifications. The training data D is a set of triples {(Q1, P 1, N1), . . . , (Qm, Pm, Nm)} where each Qk is a query, P k = \u3008Q\u03b81+, . . . , Q\u03b8I+\u3009 is a list of correct answers, and Nk is a list \u3008Q\u03b81\u2212, . . . , Q\u03b8J\u2212\u3009 incorrect answers. Each such triple is then locally grounded using the PageRank-Nibble-Prove method and used to produce a set I \u2217 J of \u201clearning-to-order\u201d triples of the form (vk0 , u k,i + , u k,j \u2212 ) where v k 0 corresponds to Q k, and\nthe u\u2019s are the nodes in G\u0302QK that correspond to the (in)correct answers for QK . We use a squared loss on the difference of scores h = pv0 [u+]\u2212 pv0 [u\u2212], i.e.,\n`(v0, u+, u\u2212) \u2261 { h2 if h < 0 0 else\nand L2 regularization of the parameter weights. Hence the final function to be optimized is\u2211\nk \u2211 i,j `(vk0 , u k,i + , u k,j \u2212 ) + \u00b5||w||22\nTo optimize this loss, we use stochastic gradient descent (SGD), rather than the quasi-Newton method of Backstrom and Leskovic. Weights are initialized to 1.0 + \u03b4, where \u03b4 is randomly drawn from [0, 0.01]. We set the learning rate \u03b2 of SGD to be \u03b2 = \u03b7epoch2 where epoch is the current epoch in SGD, and \u03b7, the initial learning rate, defaults to 1.0.\nWe implemented SGD because it is fast and has been adapted to parallel learning tasks [20, 11]. Local grounding means that learning for ProPPR is quite well-suited to parallelization. The step of locally grounding each Qi is \u201cembarassingly\u201d parallel, as every grounding can be done independently. To parallelize the weight-learning stage, we use multiple threads, each of which computes the gradient over a single grounding G\u0302Qk , and all of which accesses a single shared parameter vector w. Although the shared parameter vector is a potential bottleneck [19], it is not a severe one, as the gradient computation dominates the learning cost.7\n7This is not the case when learning a linear classifier, where gradient computations are much cheaper.\nTable 4: ProPPR program used for entity resolution.\nsamebib(BC1,BC2) :- author(BC1,A1),sameauthor(A1,A2),authorinverse(A2,BC2) # author. samebib(BC1,BC2) :- title(BC1,A1),sametitle(A1,A2),titleinverse(A2,BC2) # title. samebib(BC1,BC2) :- venue(BC1,A1),samevenue(A1,A2),venueinverse(A2,BC2) # venue. samebib(BC1,BC2) :- samebib(BC1,BC3),samebib(BC3,BC2) # tcbib. sameauthor(A1,A2) :- haswordauthor(A1,W),haswordauthorinverse(W,A2),keyauthorword(W) # authorword. sameauthor(A1,A2) :- sameauthor(A1,A3),sameauthor(A3,A2) # tcauthor. sametitle(A1,A2) :- haswordtitle(A1,W),haswordtitleinverse(W,A2),keytitleword(W) # titleword. sametitle(A1,A2) :- sametitle(A1,A3),sametitle(A3,A2) # tctitle. samevenue(A1,A2) :- haswordvenue(A1,W),haswordvenueinverse(W,A2),keyvenueword(W) # venueword. samevenue(A1,A2) :- samevenue(A1,A3),samevenue(A3,A2) # tcvenue. keyauthorword(W) :- true # authorWord(W). keytitleword(W) :- true # titleWord(W). keyvenueword(W) :- true # venueWord(W).\nTable 5: Performance of the approximate PageRankNibble-Prove method, compared to the grounding by running personalized PageRank to convergence. In all cases \u03b1\u2032 = 0.1.\nMAP Time(sec) 0.0001 0.30 28 0.00005 0.40 39 0.00002 0.53 75 0.00001 0.54 116 0.000005 0.54 216 power iteration 0.54 819"}, {"heading": "3 EXPERIMENTS", "text": ""}, {"heading": "3.1 A SAMPLE TASK", "text": "To evaluate this method, we use data from an entity resolution task previously studied as a test case for MLNs [15]. The program we use in the experiments is shown in Table 4: it is approximately the same as the MLN(B+T) approach from Singla and Domingos.8 To evaluate accuracy, we use the CORA dataset, a collection of 1295 bibliography citations that refer to 132 distinct papers. Throughout the experiments, we set the regularization coefficient \u00b5 to 0.001, the total number of epochs to 5, and learning rate parameter \u03b7 to 1. A standard log loss function was used in our objective function."}, {"heading": "3.2 RESULTS", "text": "We first consider the cost of the PageRank-NibbleProve inference/grounding technique. Table 5 shows the time required for inference (with uniform weights) for a set of 52 randomly chosen entity-resolution tasks from the CORA dataset, using a Python implemention of the theorem-prover. We report the time in seconds\n8The principle difference is that we do not include tests on the absence of words in a field in our clauses.\nFigure 3: Run-time for inference in ProPPR (with a single thread) as a function of the number of entities in the database. The base of the log is 2.\nfor all 52 tasks, as well as the mean average precision (MAP) of the scoring for each query. It is clear that PageRank-Nibble-Prove offers a substantial speedup on these problems with little loss in accuracy: on these problems, the same level of accuracy is achieved in less than a tenth of the time.\nWhile the speedup in inference time is desirable, the more important advantages of the local grounding approach are that (1) grounding time, and hence inference, need not grow with the database size and (2) learning can be performed in parallel, by using multiple threads for parallel computations of gradients in SGD. Figure 3 illustrates the first of these points: the scalability of the PageRank-Nibble-Prove method as database size increases. For comparison, we also show the inference time for MLNs with three wellpublished inference methods: Gibbs refers to Gibbs sampling, and Lifted BP is the lifted belief propagation method. We also compare with the maximum a\nposteriori (MAP) inference approach, which does not return probabilistic estimates of the specified queries. In each case the performance task is inference over 16 test queries.\nNote that ProPPR\u2019s runtime is constant, independent of the database size: it takes essentially the same time for 28 = 256 entities as for 24 = 16. In contrast, lifted belief propagation is around 1000 times slower on the larger database.\nFigure 4 explores the speedup in learning (from grounded examples) due to multi-threading. The weight-learning is using a Java implementation of the algorithm which runs over ground graphs. The full CORA dataset was used in this experiment. As can be seen, the speedup that is obtained is nearly optimal, even with 16 threads running concurrently.\nWe finally consider the effectiveness of weight learning. We train on the first four sections of the CORA dataset, and report results on the fifth. Following Singla and Domingos [15] we report performance as area under the ROC curve (AUC). Table 6 shows AUC\non the test set used by Singla and Domingos for several methods. The line for MLN(Fig 1) shows results obtained by an MLN version of the program of Figure 1. The line MLN(S&D) shows analogous results for the best-performing MLN from [15]. Compared to these methods, ProPPR does quite well even before training (with unit feature weights, w=1); the improvement here is likely due to the ProPPR\u2019s bias towards short proofs, and the tendency of the PPR method to put more weight on shared words that are rare (and hence have lower fanout in the graph walk.) Training ProPPR improves performance on three of the four tasks, and gives the most improvement on citation-matching, the most complex task.\nThe results in Table 6 all use the same data and evaluation procedure, and the MLNs were trained with the state-of-the-art Alchemy system using the recommended commands for this data (which is distributed with Alchemy9). However, we should note that the MLN results reproduced here are not identical to previous-reported ones [15]. Singla and Domingos used a number of complex heuristics that are difficult to reproduce\u2014e.g., one of these was combining MLNs with a heuristic, TFIDF-based matching procedure based on canopies [9]. While the trained ProPPR model outperformed the reproduced MLN model in all prediction tasks, it outperforms the reported results from Singla and Domingos only on venue, and does less well than the reported results on citation and author10."}, {"heading": "4 RELATED WORK", "text": "Although we have chosen here to compare experimentally to MLNs [13, 15], ProPPR represents a rather different philosophy toward language design: rather than beginning with a highly-expressive but intractible logical core, we begin with a limited logical inference scheme and add to it a minimal set of extensions that allow probabilistic reasoning, while maintaining stable, efficient inference and learning. While ProPPR is less expressive than MLNs (for instance, it is limited to definite clause theories) it is also much more efficient. This philosophy is similar to that illustrated by probabilistic similarity logic (PSL) [3]; however, unlike ProPPR, PSL does not include a \u201clocal\u201d grounding procedure, which leads to small inference problems, even for large databases.\nTechnically, ProPPR is most similar to stochastic logic programs (SLPs) [5]. The key innovation is the integration of a restart into the random-walk process,\n9http://alchemy.cs.washington.edu 10Performance on title matching is not reported by\nSingla and Domingos.\nwhich, as we have seen, leads to very different computational properties.\nThere has been some prior work on reducing the cost of grounding probabilistic logics: noteably, Shavlik et al [14] describe a preprocessing algorithm called FROG that uses various heuristics to greatly reduce grounding size and inference cost, and Niu et al [10] describe a more efficient bottom-up grounding procedure that uses an RDBMS. Other methods that reduce grounding cost and memory usage include \u201clifted\u201d inference methods (e.g., [17]) and \u201clazy\u201d inference methods (e.g., [16]); in fact, the LazySAT inference scheme for Markov networks is broadly similar algorithmically to PageRank-Nibble-Prove, in that it incrementally extends a network in the course of theorem-proving. However, there is no theoretical analysis of the complexity of these methods, and experiments with FROG and LazySAT suggest that they still lead to a groundings that grow with DB size, albeit more slowly.\nProPPR is also closely related to the Path Ranking Algorithm (PRA), learning algorithm for link prediction [7]. Like ProPPR, PRA uses random-walk methods to approximate logical inference. However, the set of \u201cinference rules\u201d learned by PRA corresponds roughly to a logic program in a particular form\u2014namely, the form\np(S, T )\u2190 r1,1S,X1), . . . , r1,k1(Xk1\u22121, T ). p(S, T )\u2190 r2,1(S,X1), . . . , r2,k2(Xk2\u22121, T ). ...\nProPPR allows much more general logic programs. However, unlike PRA, we do not consider the task of searching for new logic program clauses."}, {"heading": "5 CONCLUSIONS", "text": "We described a new probabilistic first-order language which is designed with the goal of highly efficient inference and rapid learning. ProPPR takes Prolog\u2019s SLD theorem-proving, extends it with a probabilistic proof procedure, and then limits this procedure further, by including a \u201crestart\u201d step which biases the system to short proofs. This means that ProPPR has a simple polynomial-time proof procedure, based on the wellstudied personalized PageRank (PPR) method.\nFollowing prior work on PPR-like methods, we designed a local grounding procedure for ProPPR, based on local partitioning methods [1], which leads to an inference scheme that is an order of magnitude faster that the conventional power-iteration approach to computing PPR, takes time O( 1 \u03b1\u2032 ), independent of database size. This ability to \u201clocally ground\u201d a query also makes it possible to partition the weight\nlearning task into many separate gradient computations, one for each training example, leading to a weight-learning method that can be easily parallelized. In our current implementation, an additional orderof-magnitude speedup in learning is made possible by parallelization. Experimentally, we showed that ProPPR performs well, even without weight learning, on an entity resolution task, and that supervised weight-learning improves accuracy."}], "references": [{"title": "Local partitioning for directed graphs using pagerank", "author": ["Reid Andersen", "Fan R.K. Chung", "Kevin J. Lang"], "venue": "Internet Mathematics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Group formation in large social networks: membership, growth, and evolution", "author": ["Lars Backstrom", "Dan Huttenlocher", "Jon Kleinberg", "Xiangyang Lan"], "venue": "In KDD \u201906: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Probabilistic similarity logic", "author": ["Matthias Brocheler", "Lilyana Mihalkova", "Lise Getoor"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Dynamic personalized PageRank in entity-relation graphs", "author": ["Soumen Chakrabarti"], "venue": "In WWW \u201907: Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Parameter estimation in stochastic logic programs", "author": ["James Cussens"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Markov Logic: An Interface Layer for Artificial Intelligence", "author": ["Pedro Domingos", "Daniel Lowd"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["Ni Lao", "William W. Cohen"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Foundations of Logic Programming: Second Edition", "author": ["J.W. Lloyd"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "Efficient clustering of high-dimensional data sets with application to reference matching", "author": ["Andrew McCallum", "Kamal Nigam", "Lyle H. Ungar"], "venue": "In Knowledge Discovery and Data Mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Tuffy: Scaling up statistical inference in markov logic networks using an RDBMS", "author": ["Feng Niu", "Christopher R\u00e9", "AnHai Doan", "Jude Shavlik"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Feng Niu", "Benjamin Recht", "Christopher R\u00e9", "Stephen J Wright"], "venue": "arXiv preprint arXiv:1106.5730,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The PageRank citation ranking: Bringing order to the web", "author": ["Larry Page", "Sergey Brin", "R. Motwani", "T. Winograd"], "venue": "In Technical Report,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Speeding up inference in markov logic networks by preprocessing to reduce the size of the resulting grounded network", "author": ["Jude Shavlik", "Sriraam Natarajan"], "venue": "In Proceedings of the Twentyfirst International Joint Conference on Artificial Intelligence (IJCAI-09),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Entity resolution with markov logic", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In Data Mining,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Memoryefficient inference in relational domains", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In Proceedings of the national conference on Artificial intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Lifted firstorder belief propagation", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In Proceedings of the 23rd national conference on Artificial intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Fast random walk with restart and its applications", "author": ["Hanghang Tong", "Christos Faloutsos", "Jia-Yu Pan"], "venue": "In ICDM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Slow learners are fast", "author": ["Martin Zinkevich", "Alex Smola", "John Langford"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Alex Smola", "Lihong Li"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "In many probabilistic first-order representation systems, including Markov Logic Networks [13] and Probabilistic Similarity Logic [3], inference is performed by mapping a first-order program to a propositional representation, and performing inference in that propositional representation.", "startOffset": 130, "endOffset": 133}, {"referenceID": 4, "context": "We present an extension to stochastic logic programs (SLP) [5] that is biased towards short derivations, and show that this is related to personalized PageRank (PPR) [12, 4] on a linearized version of", "startOffset": 59, "endOffset": 62}, {"referenceID": 11, "context": "We present an extension to stochastic logic programs (SLP) [5] that is biased towards short derivations, and show that this is related to personalized PageRank (PPR) [12, 4] on a linearized version of", "startOffset": 166, "endOffset": 173}, {"referenceID": 3, "context": "We present an extension to stochastic logic programs (SLP) [5] that is biased towards short derivations, and show that this is related to personalized PageRank (PPR) [12, 4] on a linearized version of", "startOffset": 166, "endOffset": 173}, {"referenceID": 7, "context": "To define the semantics we will use notation from logic programming [8].", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "In stochastic logic programs (SLPs) [5], one defines a randomized procedure for traversing the graph G\u2032 which thus defines a probability distribution over vertices v, and hence (by selecting only solution vertices) a distribution over transformed queries (i.", "startOffset": 36, "endOffset": 39}, {"referenceID": 16, "context": "These links make SLP\u2019s graph traversal a personalized PageRank (PPR) procedure, sometimes known as random-walk-with-restart [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "Putting this all together with the standard iterative approach to computing personalized PageRank over a graph [12], we arrive at the following inference algorithm for answering a query Q, using a weight vector", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "To construct a more compact local grounding graph G, we adapt an approximate personalized PageRank method called PageRank-Nibble [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "shown [1] that the subgraph \u011c (of the full proof space) is in some sense a \u201cuseful\u201d subset: for an appropriate setting of , if there is a low-conductance subgraph G\u2217 of the full graph that contains v0, then G\u2217 will be contained in \u011c: thus if there is a subgraph G\u2217 containing v0 that approximates the full graph well, PageRank-Nibble will find (a supergraph of) G\u2217.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "In spite of its efficient inference procedure, and its limitation to only definite clauses, ProPPR appears to have much of the expressive power of MLNs [6], in that many useful heuristics can apparently be encoded.", "startOffset": 152, "endOffset": 155}, {"referenceID": 1, "context": "In prior work, Backstrom and Leskovic [2] outlined a family of supervised learning procedures for this sort of annotated graph.", "startOffset": 38, "endOffset": 41}, {"referenceID": 18, "context": "We implemented SGD because it is fast and has been adapted to parallel learning tasks [20, 11].", "startOffset": 86, "endOffset": 94}, {"referenceID": 10, "context": "We implemented SGD because it is fast and has been adapted to parallel learning tasks [20, 11].", "startOffset": 86, "endOffset": 94}, {"referenceID": 17, "context": "Although the shared parameter vector is a potential bottleneck [19], it is not a severe one, as the gradient computation dominates the learning cost.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "To evaluate this method, we use data from an entity resolution task previously studied as a test case for MLNs [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "Following Singla and Domingos [15] we report performance as area under the ROC curve (AUC).", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "The line MLN(S&D) shows analogous results for the best-performing MLN from [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 13, "context": "that the MLN results reproduced here are not identical to previous-reported ones [15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": ", one of these was combining MLNs with a heuristic, TFIDF-based matching procedure based on canopies [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 13, "context": "Although we have chosen here to compare experimentally to MLNs [13, 15], ProPPR represents a rather", "startOffset": 63, "endOffset": 71}, {"referenceID": 2, "context": "This philosophy is similar to that illustrated by probabilistic similarity logic (PSL) [3]; however, unlike ProPPR, PSL does not include a \u201clocal\u201d grounding procedure, which leads to small inference problems, even for large databases.", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "Technically, ProPPR is most similar to stochastic logic programs (SLPs) [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 12, "context": "There has been some prior work on reducing the cost of grounding probabilistic logics: noteably, Shavlik et al [14] describe a preprocessing algorithm called FROG that uses various heuristics to greatly reduce grounding size and inference cost, and Niu et al [10] describe a more efficient bottom-up grounding proce-", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "There has been some prior work on reducing the cost of grounding probabilistic logics: noteably, Shavlik et al [14] describe a preprocessing algorithm called FROG that uses various heuristics to greatly reduce grounding size and inference cost, and Niu et al [10] describe a more efficient bottom-up grounding proce-", "startOffset": 259, "endOffset": 263}, {"referenceID": 15, "context": ", [17]) and \u201clazy\u201d inference methods (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": ", [16]); in fact, the LazySAT inference scheme for Markov networks is broadly similar algorithmically", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": "ProPPR is also closely related to the Path Ranking Algorithm (PRA), learning algorithm for link prediction [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "Following prior work on PPR-like methods, we designed a local grounding procedure for ProPPR, based on local partitioning methods [1], which leads to an inference scheme that is an order of magnitude faster that the conventional power-iteration approach to computing PPR, takes time O( 1 \u03b1\u2032 ), independent of database size.", "startOffset": 130, "endOffset": 133}], "year": 2013, "abstractText": "In many probabilistic first-order representation systems, inference is performed by \u201cgrounding\u201d\u2014i.e., mapping it to a propositional representation, and then performing propositional inference. With a large database of facts, groundings can be very large, making inference and learning computationally expensive. Here we present a firstorder probabilistic language which is wellsuited to approximate \u201clocal\u201d grounding: every query Q can be approximately grounded with a small graph. The language is an extension of stochastic logic programs where inference is performed by a variant of personalized PageRank. Experimentally, we show that the approach performs well without weight learning on an entity resolution task; that supervised weight-learning improves accuracy; and that grounding time is independent of DB size. We also show that order-of-magnitude speedups are possible by parallelizing learning.", "creator": "LaTeX with hyperref package"}}}