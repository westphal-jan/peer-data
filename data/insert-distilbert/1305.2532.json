{"id": "1305.2532", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2013", "title": "Learning Policies for Contextual Submodular Prediction", "abstract": "many prediction learning domains, such as ad placement, recommendation, trajectory prediction, and document summarization, require predicting a set or list of options. such lists are often evaluated using submodular reward functions that measure both quality and diversity. we propose a simple, rather efficient, and provably near - optimal trajectory approach to optimizing such prediction problems based on no - regret learning. our method leverages a surprising result from existing online strategy submodular optimization : a single no - regret driven online learner can compete with an optimal balanced sequence of predictions. compared to previous work, which either learn a uniform sequence of classifiers or rely on stronger assumptions such as target realizability, suggests we ensure both data - efficiency as well as performance guarantees in the fully agnostic setting. experiments validate the efficiency and applicability of the approach on a wide range of problems including manipulator trajectory optimization, news recommendation and document summarization.", "histories": [["v1", "Sat, 11 May 2013 18:09:52 GMT  (169kb,D)", "http://arxiv.org/abs/1305.2532v1", "13 pages. To appear in proceedings of the International Conference on Machine Learning (ICML), 2013"]], "COMMENTS": "13 pages. To appear in proceedings of the International Conference on Machine Learning (ICML), 2013", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["st\u00e9phane ross", "jiaji zhou", "yisong yue", "debadeepta dey", "drew bagnell"], "accepted": true, "id": "1305.2532"}, "pdf": {"name": "1305.2532.pdf", "metadata": {"source": "META", "title": "Learning Policies for Contextual Submodular Prediction", "authors": ["Stephane Ross", "Jiaji Zhou", "Yisong Yue", "Andrew Bagnell"], "emails": ["stephaneross@cmu.edu", "jiajiz@andrew.cmu.edu", "yisongyue@cmu.edu", "debadeep@cs.cmu.edu", "dbagnell@ri.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Many problem domains, ranging from web applications such as ad placement or content recommendation to identifying successful robotic grasp trajectories require predicting lists of items. Such applications are often budget-limited and the goal is to choose the best list of items, from a large set of possible items, with maximal utility. In ad placement, we must pick a small set of ads with high click-through rate. For robotic ma-\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nnipulation, we must pick a small set of initial grasp trajectories to maximize the chance of finding a successful trajectory via more extensive evaluation or simulation.\nIn all of these problems, the predicted list of items should be both relevant and diverse. For example, recommending a diverse set of news articles increases the chance that a user would like at least one article (Radlinski et al., 2008). As such, recommending multiple redundant articles on the same topic would do little to increase this chance. This notion of diminishing returns due to redundancy is often captured formally using submodularity (Guestrin & Krause).\nExact submodular function optimization is intractable, but simple greedy selection is known to have strong near-optimal performance guarantees and typically works very well in practice (Guestrin & Krause). Given access to the submodular reward function, one could simply employ greedy to construct good lists.\nIn this paper, we study the general supervised learning problem of training a policy to maximize a submodular reward function. We assume that the submodular reward function is only directly measured on a finite training set, and our goal is to learn to make good predictions on new test examples where the reward function is not directly measurable.\nWe develop a novel agnostic learning approach based on new analysis showing that a single no-regret learner can produce a near-optimal list of predictions.1 We use a reduction approach to \u201clift\u201d this result to contextual hypothesis classes that map features to predictions, and bound performance relative to the optimal sequence of hypotheses in the class. In contrast to previous work, our approach ensures both dataefficiency as well as performance guarantees in the fully\n1This result may seem surprising given that previous approaches (Streeter & Golovin, 2008) require a sequence of online learners \u2013 one for each position in the list.\nar X\niv :1\n30 5.\n25 32\nv1 [\ncs .L\nG ]\n1 1\nM ay\n2 01\nagnostic setting. Moreover, our approach is simple to implement and easily integrates with conventional offthe-shelf learning algorithms. Empirical evaluations show our approach to be competitive with or exceed the state-of-the-art performance on a variety of problems, ranging from trajectory prediction in robotics to extractive document summarization."}, {"heading": "2. Related Work", "text": "The problem of learning to optimize submodular reward functions from data, both with and without contextual features, has become increasingly important in machine learning due to its diverse application areas. Broadly speaking, there are two main approaches for this setting. The first aims to identify a model within a parametric family of submodular functions and then use the resulting model for new predictions. The second attempts to learn a strategy to directly predict a list of elements by decomposing the overall problem into multiple simpler learning tasks.\nThe first approach (Yue & Joachims, 2008; Yue & Guestrin, 2011; Lin & Bilmes, 2012; Raman et al., 2012) involves identifying the parameterization that best matches the submodular rewards of the training instances. These methods are largely limited to learning non-negative linear combinations of features that are themselves submodular, which often restricts their expressiveness. Furthermore, while good sample complexity results are known, these guarantees only hold under strong realizability assumptions where submodular rewards can be modeled exactly by such linear combinations (Yue & Guestrin, 2011; Raman et al., 2012). Recent work on Determinental Point Processes (DPPs) (Kulesza & Taskar, 2011) provide a probabilistic model of sets, which can be useful for the tasks that we consider. These approaches, while appealing, solve a potentially unnecessarily hard problem in first learning a holistic list evaluation model, and thus may compound errors by first approximating the submodular function and then approximately optimizing it.\nThe second, a learning reduction approach, by contrast, decomposes list prediction into a sequence of simpler learning tasks that attempts to mimic the greedy strategy (Streeter & Golovin, 2008; Radlinski et al., 2008; Streeter et al., 2009; Dey et al., 2012). In (Dey et al., 2012), this strategy was extended to the contextual setting by a reduction to cost-sensitive classification. Essentially, each learning problem aims to best predict an item to add to the list, given features, so as to maximize the expected marginal utility. This approach is flexible, in that it can be used with most common hypothesis classes and arbitrary features. Be-\ncause of this decomposition, the full model class (all possible sequences of predictors) is often quite expressive, and allows for agnostic learning guarantees.2 This generality comes at the expense of being significantly less data-efficient than methods that make realizability assumptions such as (Yue & Guestrin, 2011; Raman et al., 2012), as the existing approach learns a different classifier for each position in the list.\nCompared with related work, our approach enjoys the benefits of being both data-efficient while ensuring strong agnostic performance guarantees. We do so by developing new analysis for online submodular optimization which yields agnostic learning guarantees while learning a single data-efficient policy."}, {"heading": "3. Background", "text": "Let S denote the set of possible items to choose from (e.g. ads, sentences, grasps). Our objective is to pick a list of items L \u2286 S to maximize a reward function f that obeys the following properties:3\n1. Monotonicity: For any lists L1, L2, f(L1) \u2264 f(L1 \u2295 L2) and f(L2) \u2264 f(L1 \u2295 L2)\n2. Submodularity: For any lists L1, L2 and item s \u2208 S, f(L1 \u2295 s) \u2212 f(L1) \u2265 f(L1 \u2295 L2 \u2295 s) \u2212 f(L1 \u2295 L2).\nHere, \u2295 denotes the concatenation operator. Intuitively, monotonicity implies that adding more elements never hurts, and submodularity captures the notion of diminishing returns (i.e. adding an item to a long list increases the objective less than when adding it to a shorter sublist). We further assume for simplicity that f takes values in [0, 1], and that f(\u2205) = 0 where \u2205 denotes the empty list. We will also use the shorthand b(s|L) = f(L \u2295 s) \u2212 f(L) to denote the marginal benefit of adding the item s to list L.\nA simple example submodular function that repeatedly arises in many domains is one that takes value 0 until a suitable instance is found, and then takes on value 1 thereafter. Examples include the notion of \u201cmultiple choice\u201d learning as in (Dey et al., 2012; Guzman-Rivera et al., 2012) where a predicted set of options is considered successful if any predicted item is deemed correct, and abandonment in ad placement (Radlinski et al., 2008) where success is measured by\n2This first strategy of learning the parameters of a submodular function can be seen as a special case of this second approach (see section 5.1).\n3\u201cLists\u201d generalize the notion of \u201cset\u201d more commonly used in submodular optimization, and enables reasoning about item order and repeated items (Streeter & Golovin, 2008). One may consider sets where appropriate.\nwhether any predicted advertisement is clicked on.\nWe consider reward functions that may depend on some underlying state x \u2208 X (e.g. a user, environment of the robot, a document, etc.). Let fx denote the reward function for state x, and assume that fx is monotone submodular for all x."}, {"heading": "3.1. Learning Problem", "text": "Our task consists in learning to construct good lists of pre-specified length k under some unknown distribution of states D (e.g. distribution of users or documents we have to summarize). We consider two cases: context-free and contextual.\nContext-Free. In the context-free case, we have no side-information about the current state (i.e. we do not observe anything about x). We quantify the performance of any list L by its expected value:\nF (L) = Ex\u223cD[fx(L)].\nNote that F (L) is also monotone submodular. Thus the clairvoyant greedy algorithm with perfect knowledge of D can find a list L\u0302k such that F (L\u0302k) \u2265 (1 \u2212 1/e)F (L\u2217k), were L\u2217k = argmaxL:|L|=k F (L). Although D is unknown, we assume that we observe samples of the objective fx during training. Our goal is thus to develop a learning approach that efficiently converges, both computationally and statistically, to the performance of the clairvoyant greedy algorithm.\nContextual. In the contextual case, we observe sideinformation in the form of features regarding the state of the world. We \u201clift\u201d this problem to a hypothesis space of policies (i.e. multi-class predictors) that map features to items.\nLet \u03a0 denote our policy class, and let \u03c0(x) denote the prediction of policy \u03c0 \u2208 \u03a0 given side-information describing state x. Let L\u03c0,k = (\u03c01, \u03c02, . . . , \u03c0k) denote a list of policies. In state x, this list of policies will predict L\u03c0,k(x) = (\u03c01(x), \u03c02(x), . . . , \u03c0k(x)). We quantify performance using the expected value:\nF (L\u03c0) = Ex\u223cD[fx(L\u03c0(x))].\nIt can be shown that F obeys both monotonicity and submodularity with respect to appending policies (Dey et al., 2012). Thus, a clairvoyant greedy algorithm that sequentially picks the policy with highest expected benefit will construct a list L\u0302\u03c0,k such that F (L\u0302\u03c0,k) \u2265 (1 \u2212 1/e)F (L\u2217\u03c0,k), where L\u2217\u03c0,k = argmaxL\u03c0 :|L\u03c0|=k F (L\u03c0). As before, our goal is to develop a learning approach (for learning a list of policies) that efficiently competes with the performance of the clairvoyant greedy algorithm.\nAlgorithm 1 Submodular Contextual Policy (SCP) Algorithm in context-free setting.\nInput: Set of items S, length m of list to construct, length k of best list to compete against, online learner Predict and Update functions. for t = 1 to T do\nCall online learner Predict()m times to construct list Lt. (e.g. by sampling m times from online learner\u2019s internal distribution over items). Evaluate list Lt on a sampled state xt \u223c D. For all s \u2208 S, define its discounted cumulative benefit: rt(s) = \u2211m i=1(1\u2212 1/k)\nm\u2212ib(s|Lt,i\u22121, xt). For all s \u2208 S: define loss `t(s) = maxs\u2032\u2208S rt(s\u2032)\u2212rt(s) Call online learner update with loss `t: Update(`t)\nend for"}, {"heading": "4. Context-free List Optimization", "text": "We first consider the context-free setting. Our algorithm, called Submodular Contextual Policy (SCP), is described in Algorithm 1. SCP requires an online learning algorithm subroutine (denoted by Update) that is no-regret with respect to a bounded positive loss function,4 maintains an internal distribution over items for prediction, and can be queried for multiple predictions (i.e. multiple samples).5 In contrast to prior work (Streeter & Golovin, 2008), SCP employs only a single online learning in the inner loop.\nSCP proceeds by training over a sequence of states x1, x2, . . . , xT . At each iteration, SCP queries the online learner to generate a list of m items (via Predict, e.g. by sampling from its internal distribution over items), evaluates a weighted cumulative benefit of each item on the sampled list to define a loss related to each item, and then uses the online learner (via Update) to update its internal distribution.\nDuring training, we allow the algorithm to construct lists of length m, rather than k. In its simplest form, one may simply choose m = k. However, it may be beneficial to choose m differently than k, as is shown later in the theoretical analysis.\nPerhaps the most unusual aspect is how loss is defined using the weighted cumulative benefits of each item:\nrt(s) = m\u2211 i=1 (1\u2212 1/k)m\u2212ib(s|Lt,i\u22121, xt), (1)\nwhere Lt,i\u22121 denotes the first i\u2212 1 items in Lt, and\nb(s|Lt,i\u22121, xt) = fxt(Lt,i\u22121 \u2295 s)\u2212 fxt(Lt,i\u22121). (2) 4See Section 4.1 and (3) for a definition of no-regret. 5Algorithms that meet these requirements include Randomized Weighted Majority (Littlestone & Warmuth, 1994), Follow the Leader (Kalai & Vempala, 2005), EXP3 (Auer et al., 2003), and many others.\nIntuitively, (1) represents the weighted sum of benefits of item s in state xt had we added it at any intermediate stage in Lt. The benefits at different positions are weighed differently, where position i is adjusted by a factor (1\u22121/k)m\u2212i. These weights are derived via our theoretical analysis, and indicate that benefits in early positions should be more discounted than benefits in later positions. Intuitively, this weighting has the effect of rebalancing the benefits so that each position contributes more equally to the overall loss.6\nSCP requires the ability to directly measure fx in each training instance xt. Directly measuring fxt enables us to obtain loss measurements `t(s) for any s \u2208 S. For example, in document summarization fx corresponds to the ROUGE score (Lin, 2004), which can be evaluated for any generated summary given expert annotations which are only available for training instances.\nIn principle, SCP can also be applied in partial feedback settings, e.g. ad placement where the value fxt is only observed for some items (e.g. only the displayed ads), by using bandit learning algorithms instead (e.g. EXP3 (Auer et al., 2003)).7 As this is an orthogonal issue, most of our focus is on the full information case."}, {"heading": "4.1. Theoretical Guarantees", "text": "We now show that Algorithm 1 is no-regret with respect to the clairvoyant greedy algorithm\u2019s expected performance over the training instances. Our main theoretical result provides a reduction to an online learning problem and directly relates the performance of our algorithm on the submodular list optimization problem to the standard online learning regret incurred by the subroutine.\nAlthough Algorithm 1 uses only a single instance of an online learner subroutine, it achieves the same performance guarantee as prior work (Streeter & Golovin, 2008; Dey et al., 2012) that employ k separate instances of an online learner. This leads to a surprising fact: it is possible to sample from a stationary distribution over items to construct a list that achieves the same guarantee as the clairvoyant greedy algorithm. 8\n6We also consider a similar algorithm in the min-sum cover setting, where the theory also requires reweighting benefits, but instead weights earlier benefits more highly (by a factor m \u2212 i, rather than (1 \u2212 1/k)m\u2212i). We omit discussing this variant for brevity.\n7Partial information settings arise, e.g., when f is derived using real-world trials that preclude the ability to evaluate b(s|L, x) (2) for every possible s \u2208 S.\n8This fact can also be seen as a special case of a more general result proven in prior related work that analyzed randomized set selection strategies to optimize submodular functions (Feige et al., 2011).\nFor a sequence of training states {xt}Tt=1, let the sequence of loss functions {`t}Tt=1 defined in Algorithm 1 correspond to the sequence of losses incurred in the reduction to the online learning problem. The expected regret of the online learning algorithm is\nE[R] = T\u2211 t=1 Es\u2032\u223cpt [`t(s\u2032)]\u2212min s\u2208S T\u2211 t=1 `t(s), (3)\nwhere pt is the internal distribution of the online learner used to construct list Lt. Note that an online learner is called no-regret if R is sublinear in T .\nLet F (p,m) = ELm\u223cp[Ex\u223cD[fx(Lm)]] denote the expected value of constructing lists by sampling (with replacement) m elements from distribution p, and let p\u0302 = arg maxt\u2208{1,2,...,T} F (pt,m) denote the best distribution found by the algorithm.\nWe define a mixture distribution p over lists that constructs a list as follows: sample an index t uniformly in {1, 2, . . . , T}, then sample m elements (with replacement) from pt. Note that F (p,m) = 1 T \u2211T t=1 F (pt,m) and F (p\u0302,m) \u2265 F (p,m). Thus it suffices to show that F (p,m) has good guarantees. We show that in expectation p (and thus p\u0302) constructs lists with performance guarantees close to the clairvoyant greedy algorithm:9\nTheorem 1. Let \u03b1 = exp(\u2212m/k) and k\u2032 = min(m, k). For any \u03b4 \u2208 (0, 1), with probability \u2265 1\u2212\u03b4:\nF (p,m) \u2265 (1\u2212 \u03b1)F (L\u2217k)\u2212 E[R] T \u2212 3\n\u221a 2k\u2032 ln(2/\u03b4)\nT\nCorollary 1. If a no-regret algorithm is used on the sequence of loss `t, then as T \u2192\u221e, E[R]T \u2192 0, and:\nlim T\u2192\u221e\nF (p,m) \u2265 (1\u2212 \u03b1)F (L\u2217k)\nTheorem 1 provides a general approximation ratio to the best list of size k when constructing a list of a different size m. For m = k, we obtain the typical (1\u22121/e) approximation ratio (Guestrin & Krause). As m increases, this provides approximation ratios that converge exponentially closer to 1.\nNaively, one might expect regret E[R]/T to scale linearly in k\u2032 as it involves loss in [0, k\u2032]. However, we show that regret actually scales as O( \u221a k\u2032) (e.g. using Weighted Majority (Kalai & Vempala, 2005)). Our result matches the best known results for this setting\n9Additionally, if the distributions pt converge, then the last distribution pT+1 must have performance arbitrarily close to p as T \u2192 \u221e. In particular, we can expect this to occur when the examples are randomly drawn from a fixed distribution that does not change over time.\nAlgorithm 2 Submodular Contextual Policy (SCP) Algorithm.\nInput: Set of items S, policy class \u03a0\u0303, length m of list we construct, length k of best list we compete against. Pick initial policy \u03c01 (or distribution over policies) for t = 1 to T do\nObserve features of a sampled state xt \u223c D (e.g. features of user/document) Construct list Lt of m items using \u03c0t with features of xt (or by sampling a policy for each position if \u03c0t is a distribution over policies). Define m new cost-sensitive classification examples {(vti, cti, wti)}mi=1 where:\n1. vti is the feature vector of state xt and list Lt,i\u22121\n2. cti is the cost vector such that \u2200s \u2208 S: cti(s) = maxs\u2032\u2208S b(s \u2032|Lt,i\u22121, xt)\u2212 b(s|Lt,i\u22121, xt)\n3. wti = (1\u2212 1/k)m\u2212i is the weight of this example\n\u03c0t+1 = Update(\u03c0t, {(vti, cti, wti)}mi=1) end for return \u03c0T+1\n(Streeter & Golovin, 2008) while using a single online learner, and is especially beneficial in the contextual setting due to improved generalization (see Section 5).\nCorollary 2. Using weighted majority with the optimal learning rate guarantees with probability \u2265 1\u2212 \u03b4:\nF (p,m) \u2265 (1\u2212\u03b1)F (L\u2217k)\u2212O\n(\u221a k\u2032 log(1/\u03b4)\nT +\n\u221a k\u2032 log |S|\nT\n) ."}, {"heading": "5. Contextual List Optimization with Stationary Policies", "text": "We now consider the contextual setting where features of each state xt are observed before choosing the list. As mentioned, our goal here is to compete with the best list of policies (\u03c01, \u03c02, . . . , \u03c0k) from a hypothesis class \u03a0. Each of these policies are assumed to choose an item solely based on features of the state xt.\nWe consider embedding \u03a0 within a larger class, \u03a0 \u2286 \u03a0\u0303, where policies \u03a0\u0303 are functions of both state and a partially chosen list. Then for any \u03c0 \u2208 \u03a0\u0303, \u03c0(x, L) corresponds to the item that policy \u03c0 selects to append to list L given state x. We will learn a policy, or distribution of policies, from \u03a0\u0303 that attempts to generalize list construction across multiple positions.10\nWe present an extension of SCP to the contextual set-\n10Competing against the best list of policies in \u03a0\u0303 is difficult in general as it violates submodularity: policies can perform better when added later in the list (due to list features). Nevertheless, we can still learn from class \u03a0\u0303 and compete against the best list of policies in \u03a0.\nting (Algorithm 2). At each iteration, SCP constructs a list Lt for the state xt (using its current policy or by sampling policies from its distribution over policies).\nAnalogous to the context-free setting, we define a loss function for the learner subroutine (Update). We represent the loss using weighted cost-sensitive classification examples {(vti, cti, wti)}mi=1, where vti denotes features of the state xt and list Lt,i\u22121, wti = (1\u22121/k)m\u2212i is the weight associated to this example, and cti is the cost vector specifying the cost of each item s \u2208 S\ncti(s) = max s\u2032\u2208S\nb(s\u2032|Lt,i\u22121, xt)\u2212 b(s|Lt,i\u22121, xt). (4)\nThe loss incurred by any policy \u03c0 is defined by its loss on this set of cost-sensitive classification examples, i.e.\n`t(\u03c0) = m\u2211 i=1 wticti(\u03c0(vti)).\nThese new examples are then used to update the policy (or distribution over policies) using a no-regret algorithm (Update). This reduction effectively transforms the task of learning a policy for this submodular list optimization problem into a standard online cost-sensitive classification problem.11 Analogous to the context-free setting, we can also extend to partial feedback settings where f is only partially measurable by using contextual bandit algorithms such as EXP4 (Auer et al., 2003) as the online learner (Update).12"}, {"heading": "5.1. No-Regret Cost-Sensitive Classification", "text": "Having transformed our problem into online costsensitive classification, we now present approaches that can be used to achieve no-regret on such tasks. For finite policy classes \u03a0\u0303, one can again leverage any no-regret online algorithm such as Weighted Majority (Kalai & Vempala, 2005). Weighted Majority maintains a distribution over policies in \u03a0\u0303 based on the loss `t(\u03c0) of each \u03c0, and achieves regret at a rate of\nR = \u221a k\u2032 log |\u03a0\u0303|/T ,\nfor k\u2032 = min(m, k). In fact, the context-free setting can be seen as a special case, where \u03a0 = \u03a0\u0303 = {\u03c0s|s \u2208 S} and \u03c0s(v) = s for any v.\n11This is similar to DAgger (Ross et al., 2011a;b; Ross & Bagnell, 2012) developed for sequential prediction problems like imitation learning. Our work can be seen as a specialization of DAgger for submodular list optimization, and ensures that we learn policies that pick good items under the lists they construct. Unlike prior work, our analysis leverages submodularity, leading to several modifications, and improved global optimality guarantees.\n12Analogous to the context-free setting, partial information arises when cti (4) is not measurable for every s \u2208 S.\nHowever, achieving no-regret for infinite policy classes is in general not tractable. A more practical approach is to employ existing reductions of cost-sensitive classification problems to convex optimization problems, for which we can efficiently run no-regret convex optimization (e.g. gradient descent). These reductions effectively upper bound the cost-sensitive loss by a convex loss, and thus bound the original loss of the list prediction problem. We briefly describe two such reductions from (Beygelzimer et al., 2005):\nReduction to Regression We transform costsensitive classification into a regression problem of predicting the costs of each item s \u2208 S. Afterwards, the policy chooses the item with lowest predicted cost. We convert each weighted cost-sensitive example (v, c, w) into |S| weighted regression examples.\nFor example, if we use least-squares linear regression, the weighted squared loss for a particular example (v, c, w) and policy h would be:\n`(h) = w \u2211 s\u2208S (h>v(s)\u2212 c(s))2.\nReduction to Ranking Another useful reduction transforms the problem into a \u201dranking\u201d problem that penalizes ranking an item s above another better item s\u2032. In our experiments, we employ a weighted hinge loss, and so the penalty is proportional to the difference in cost of the misranked pair. For each costsensitive example (v, c, w), we generate |S|(|S| \u2212 1)/2 ranking examples for every distinct pair of items (s, s\u2032), where we must predict the best item among (s, s\u2032) (potentially by a margin), with a weight of w|c(s)\u2212c(s\u2032)|.\nFor example, if we train a linear SVM (Joachims, 2005), we obtain a weighted hinge loss of the form:\nw|\u03b4s,s\u2032 |max(0, 1\u2212 h>(v(s)\u2212 v(s\u2032))sign(\u03b4s,s\u2032)),\nwhere \u03b4s,s\u2032 = c(s)\u2212 c(s\u2032) and h is the linear policy. At prediction time, we simply predict the item s\u2217 with highest score, s\u2217 = argmaxs\u2208S h\n>v(s). This reduction proves advantageous whenever it is easier to predict pairwise rankings rather than the actual cost."}, {"heading": "5.2. Theoretical Guarantees", "text": "We now present contextual performance guarantees for SCP that relate performance on the submodular list optimization task to the regret of the corresponding online cost-sensitive classification task. Let `t : \u03a0\u0303\u2192 R compute the loss of each policy \u03c0 on the cost-sensitive classification examples {vti, cti, wti}mi=1 collected in Algorithm 2 for state xt. We use {`t}Tt=1 as the sequence of losses for the online learning problem.\nFor a deterministic online algorithm that picks the sequence of policies {\u03c0t}Tt=1, the regret is\nR = T\u2211 t=1 `t(\u03c0t)\u2212min \u03c0\u2208\u03a0\u0303 T\u2211 t=1 `t(\u03c0).\nFor a randomized online learner, let \u03c0t be the distribution over policies at iteration t, with expected regret\nE[R] = T\u2211 t=1 E\u03c0\u2032t\u223c\u03c0t [`t(\u03c0 \u2032 t)]\u2212min \u03c0\u2208\u03a0\u0303 T\u2211 t=1 `t(\u03c0).\nLet F (\u03c0,m) = EL\u03c0,m\u223c\u03c0[Ex\u223cD[fx(L\u03c0,m(x))]] denote the expected value of constructing lists by sampling (with replacement) m policies from distribution \u03c0 (if \u03c0 is a deterministic policy, then this means we use the same policy at each position in the list). Let \u03c0\u0302 = argmaxt\u2208{1,2,...,T} F (\u03c0t,m) denote the best distribution found by the algorithm in hindsight.\nWe use a mixture distribution \u03c0 over policies to construct a list as follows: sample an index t uniformly in {1, 2, . . . , T}, then sample m policies from \u03c0t to construct the list. As before, we note that F (\u03c0,m) = 1T \u2211T t=1 F (\u03c0t,m), and F (\u03c0\u0302,m) \u2265 F (\u03c0,m). As such, we again focus on proving good guarantees for F (\u03c0,m), as shown by the following theorem. Theorem 2. Let \u03b1 = exp(\u2212m/k), k\u2032 = min(m, k) and pick any \u03b4 \u2208 (0, 1). After T iterations, for deterministic online algorithms, we have that with probability at least 1\u2212 \u03b4:\nF (\u03c0,m) \u2265 (1\u2212 \u03b1)F (L\u2217\u03c0,k)\u2212 R\nT \u2212 2\n\u221a 2 ln(1/\u03b4)\nT .\nSimilarly, for randomized online algorithms, with probability at least 1\u2212 \u03b4:\nF (\u03c0,m) \u2265 (1\u2212 \u03b1)F (L\u2217\u03c0,k)\u2212 E[R] T \u2212 3\n\u221a 2k\u2032 ln(2/\u03b4)\nT .\nThus, as in the previous section, a no-regret algorithm must achieve F (\u03c0,m) \u2265 (1 \u2212 \u03b1)F (L\u2217\u03c0,k) with high probability as T \u2192 \u221e. This matches similar guarantees provided in (Dey et al., 2012). Despite having similar guarantees, we intuitively expect SCP to outperform (Dey et al., 2012) in practice because SCP can use all data to train a single predictor, instead of being split to train k separate ones. We empirically verify this intuition in Section 6.\nWhen using surrogate convex loss functions (such as regression or ranking loss), we provide a general result that applies if the online learner uses any convex upper bound of the cost-sensitive loss. An extra penalty term is introduced that relates the gap between the convex upper bound and the original cost-sensitive loss:\nCorollary 3. Let \u03b1 = exp(\u2212m/k) and k\u2032 = min(m, k). If we run an online learning algorithm on the sequence of convex loss Ct instead of `t, then after T iterations, for any \u03b4 \u2208 (0, 1), we have that with probability at least 1\u2212 \u03b4:\nF (\u03c0,m) \u2265 (1\u2212 \u03b1)F (L\u2217\u03c0,k)\u2212 R\u0303\nT \u2212 2\n\u221a 2 ln(1/\u03b4)\nT \u2212 G\nwhere R\u0303 is the regret on the sequence of convex loss Ct, and G is defined as\n1 T [ T\u2211 t=1 (`t(\u03c0t)\u2212 Ct(\u03c0t)) + min \u03c0\u2208\u03a0\u0303 T\u2211 t=1 Ct(\u03c0)\u2212 min \u03c0\u2032\u2208\u03a0\u0303 T\u2211 t=1 `t(\u03c0 \u2032) ]\nand denotes the \u201cconvex optimization gap\u201d that measures how close the surrogate Ct is to minimizing `t.\nThis result implies that using a good surrogate convex loss for no-regret convex optimization will lead to a policy that has a good performance relative to the optimal list of policies. Note that the gap G often may be small or non-existent. For instance, in the case of the reduction to regression or ranking, G = 0 in realizable settings where there exists a \u201cperfect\u201d predictor in the class. Similarly, in cases where the problem is near-realizable we would expect G to be small.13"}, {"heading": "6. Experimental Results", "text": ""}, {"heading": "6.1. Robotic Manipulation Planning", "text": "We applied SCP to a manipulation planning task for a 7 degree-of-freedom robot manipulator. The goal is to predict a set of initial trajectories so as to maximize the chance that one of them leads to a collision-free trajectory. We use local trajectory optimization techniques such as CHOMP (Ratliff et al., 2009), which have proven effective in quickly finding collision-free trajectories using local perturbations of an initial trajectory. Note that selecting a diverse set of initial trajectories is important since local techniques such as CHOMP often get stuck in local optima.14\nWe use the dataset from (Dey et al., 2012). It consists of 310 training and 212 test environments of random obstacle configurations around a target object, and 30 initial seed trajectories. In each environment, each seed trajectory has 17 features describing the spatial properties of the trajectory relative to obstacles.15\n13We conjecture that this gap term G is not specific to our particular scenario, but rather is (implicitly) always present whenever one attempts to optimize classification accuracy via surrogate convex optimization.\n14I.e., similar or redundant inital trajectories will lead to the same local optima.\n15In addition to the base features, we add features of the\nFollowing (Dey et al., 2012), we employ a reduction of cost-sensitive classification to regression as explained in Section 5.1. We compare SCP to ConSeqOpt (Dey et al., 2012) (which learns k separate predictors), and Regression (regress success rate from features to sort seeds; this accounts for relevance but not diversity).\nFigure 1 (left) shows the failure probability over the test environments versus the number of training environments. ConSeqOpt employs a reduction to k classifiers. As a consequence, ConSeqOpt faces data starvation issues for small training sizes, as there is little data available for training predictors lower in the list.16 In contrast, SCP has no data starvation issue and outperforms both ConSeqOpt and Regression."}, {"heading": "6.2. Personalized News Recommendation", "text": "We built a stochastic user simulation based on 75 user preferences derived from a user study in (Yue & Guestrin, 2011). Using this simulation as a training oracle, our goal is to learn to recommend articles to any user (depending on their contextual features) to minimize the failure case where the user does not like any of the recommendations.17\nArticles are represented by features, and user preferences by linear weights. We derived user contexts by soft-clustering users into groups, and using corrupted group memberships as contexts.\nWe perform five-fold cross validation. In each fold, we train SCP and ConSeqOpt on 40 users\u2019 preferences, use 20 users for validation, and then test on the heldout 15 users. Training, validation and testing are all performed via simulation. Figure 1 (middle) shows the results, where we see the recommendations made by SCP achieves significantly lower failure rate as the number of recommendations is increased from 1 to 5."}, {"heading": "6.3. Document Summarization", "text": "In the extractive multi-document summarization task, the goal is to extract sentences (with character budget B) to maximize coverage of human-annotated summaries. Following the experimental setup from (Lin & Bilmes, 2010) and (Kulesza & Taskar, 2011), we use\ncurrent list w.r.t. each initial trajectory. We use the per feature minimum absolute distance and average absolute value of the distance to the features of initial trajectories in the list. We also use a bias feature always set to 1, and an indicator feature which is 1 when selecting the element in the first position, 0 otherwise.\n16When a successful seed is found, benefits at later positions are 0. This effectively discards training environments for training classifiers lower in the list in ConSeqOpt.\n17Also known as abandonment (Radlinski et al., 2008).\ndata from the Document Understanding Conference (DUC) 2003 and 2004 (Task 2) (Dang, 2005). Each training or test instance corresponds to a cluster of documents, and contains approximately 10 documents belonging to the same topic and four human reference summaries. We train on the 2003 data (30 clusters) and test on the 2004 data (50 clusters). The budget is B = 665 bytes, including spaces.\nWe use the ROUGE (Lin, 2004) unigram statistics (ROUGE-1R, ROUGE-1P, ROUGE-1F) for performance evaluation. Our method directly attempts to optimize the ROUGE-1R objective with respect to the reference summaries, which can be easily shown to be monotone submodular (Lin & Bilmes, 2011).\nWe aim to predict sentences that are both short and informative. Therefore we maximize the normalized marginal benefit,\nb\u2032(s|Lt,i\u22121) = b(s|Lt,i\u22121)/l(s), (5)\nwhere l(s) is the length of the sentence s.18 We use a reduction to ranking as described in Section 5.1 using (5). While not performance-optimized, our approach takes less than 15 minutes to train.\nFollowing (Kulesza & Taskar, 2011), we consider features fi for each sentence consisting of quality features qi and similarity features \u03c6i (fi = [q T i , \u03c6 T i ] T ). The quality features, attempt to capture the representativeness for a single sentence. Similarity features qi for sentence si as we construct the list Lt measure a notion of distance of a proposed sentence to sentences already included in the set.19\n18This results in a knapsack constrained optimization problem. We expect our approach to perform well in this setting, but defer a formal analysis for future work.\n19 A variety of similarity features were considered, with\nTable 1 shows the performance (Rouge unigram statistics) comparing SCP with existing algorithms. We observe that SCP outperforms existing state-of-the-art approaches, which we denote SubMod (Lin & Bilmes, 2010) and DPP (Kulesza & Taskar, 2011). \u201cGreedy (Oracle)\u201d corresponds to the clairvoyant oracle that directly optimizes the test Rouge score and thus serves as an upper bound on this class of techniques. Figure 1 (right) plots Rouge-1R performance as a function of the size of training data, suggesting SCP\u2019s superior data-efficiency compared to ConSeqOpt."}, {"heading": "Acknowledgements", "text": "This research was supported in part by NSF NRI Purposeful Prediction project and ONR MURIs Decentralized Reasoning in Reduced Information Spaces and Provably Stable Vision-Based Control. Yisong Yue was also supported in part by ONR (PECASE) N000141010672 and ONR Young Investigator Program N00014-08-1-0752. We gratefully thank Martial Hebert for valuable discussions and support.\nthe simplest being average squared distance of tf-idf vectors. Performance was very stable across different features. The experiments presented use three types: 1) following the idea in (Kulesza & Taskar, 2011) of similarity as a volume metric, we compute the squared volume of the parallelopiped spanned by the TF-IDF vectors of sentences in the set Lt,k \u222asi; 2) the product between det(GLt,k\u222asi) and the quality features; 3) the minimum absolute distance of quality features between si and each element in Lt,k."}, {"heading": "A. Proofs of Theoretical Results", "text": "This appendix contains the proofs of the various theoretical results presented in this paper.\nA.1. Preliminaries\nWe begin by proving a number of lemmas about monotone submodular functions, which will be useful to prove our main results.\nLemma 1. Let S be a set and f be a monotone submodular function defined on list of items from S. For any lists A,B, we have that:\nf(A\u2295B)\u2212 f(A) \u2264 |B|(Es\u223cU(B)[f(A\u2295 s)]\u2212 f(A))\nfor U(B) the uniform distribution on items in B.\nProof. For any list A and B, let Bi denote the list of the first i items in B, and bi the i\nth item in B. We have that:\nf(A\u2295B)\u2212 f(A) = \u2211|B| i=1 f(A\u2295Bi)\u2212 f(A\u2295Bi\u22121)\n\u2264 \u2211|B| i=1 f(A\u2295 bi)\u2212 f(A) = |B|(Eb\u223cU(B)[f(A\u2295 b)]\u2212 f(A))\nwhere the inequality follows from the submodularity property of f .\nLemma 2. Let S be a set, and f a monotone submodular function defined on lists of items in S. Let A,B be any lists of items from S. Denote Aj the list of the first j items in A, U(B) the uniform distribution on items in B and define j = Es\u223cU(B)[f(Aj\u22121 \u2295 s)] \u2212 f(Aj), the additive error term in competing with the average marginal benefits of the items in B when picking the jth item in A (which could be positive or negative). Then: f(A) \u2265 (1\u2212(1\u22121/|B|)|A|)f(B)\u2212 |A|\u2211 i=1 (1\u22121/|B|)|A|\u2212i i\nIn particular if |A| = |B| = k, then:\nf(A) \u2265 (1\u2212 1/e)f(B)\u2212 k\u2211 i=1 (1\u2212 1/k)k\u2212i i\nand for \u03b1 = exp(\u2212|A|/|B|) (i.e. |A| = |B| log(1/\u03b1)):\nf(A) \u2265 (1\u2212 \u03b1)f(B)\u2212 |A|\u2211 i=1 (1\u2212 1/|B|)|A|\u2212i i\nProof. Using the monotone property and previous lemma 1, we must have that: f(B) \u2212 f(A) \u2264 f(A \u2295 B)\u2212 f(A) \u2264 |B|(Eb\u223cU(B)[f(A\u2295 b)]\u2212 f(A)).\nNow let \u2206j = f(B) \u2212 f(Aj). By the above we have that\n\u2206j \u2264 |B|[Es\u223cU(B)[f(Aj \u2295 s)]\u2212 f(Aj)] = |B|[Es\u223cU(B)[f(Aj \u2295 s)]\u2212 f(Aj+1)\n+f(Aj+1)\u2212 f(B) + f(B)\u2212 f(Aj)] = |B|[ j+1 + \u2206j \u2212\u2206j+1]\nRearranging terms, this implies that \u2206j+1 \u2264 (1 \u2212 1/|B|)\u2206j + j+1. Recursively expanding this recurrence from \u2206|A|, we obtain:\n\u2206|A| \u2264 (1\u2212 1/|B|)|A|\u22060 + |A|\u2211 i=1 (1\u2212 1/|B|)|A|\u2212i i\nUsing the definition of \u2206|A| and rearranging terms, we obtain f(A) \u2265 (1 \u2212 (1 \u2212 1/|B|)|A|)f(B) \u2212 \u2211|A| i=1(1 \u2212 1/|B|)|A|\u2212i i. This proves the first statement of the theorem. The following two statements follow from the observations that (1\u2212 1/|B|)|A| = exp(|A| log(1\u2212 1/|B|)) \u2264 exp(\u2212|A|/|B|) = \u03b1. Hence (1 \u2212 (1 \u2212 1/|B|)|A|)f(B) \u2265 (1 \u2212 \u03b1)f(B). When |A| = |B|, \u03b1 = 1/e and this proves the special case where |A| = |B|.\nFor the greedy list construction strategy, the j in the last lemma are always \u2264 0, such that Lemma 2 implies that if we construct a list of size k with greedy, it must achieve at least 63% of the value of the optimal list of size k, but also that it must achieve at least 95% of the value of the optimal list of size bk/3c, and at least 99.9% of the value of the optimal list of size bk/7c.\nA more surprising fact that follows from the last lemma is that constructing a list stochastically, by sampling items from a particular fixed distribution, can provide the same guarantee as greedy:\nLemma 3. Let S be a set, and f a monotone submodular function defined on lists of items in S. Let B be any list of items from S and U(B) the uniform distribution on elements in B. Suppose we construct the list A by sampling k items randomly from U(B) (with replacement). Denote Aj the list obtained after j samples, and Pj the distribution over lists obtained after j samples. Then:\nEA\u223cPk [f(A)] \u2265 (1\u2212 (1\u2212 1/|B|)k)f(B)"}, {"heading": "In particular, for \u03b1 = exp(\u2212k/|B|):", "text": "EA\u223cPk [f(A)] \u2265 (1\u2212 \u03b1)f(B)\nProof. The proof follows a similar proof to the previous lemma. Recall that by the monotone property and\nlemma 1, we have that for any list A: f(B)\u2212 f(A) \u2264 f(A \u2295 B) \u2212 f(A) \u2264 |B|(Eb\u223cU(B)[f(A \u2295 b)] \u2212 f(A)). Because this holds for all lists, we must also have that for any distribution P over lists A, f(B) \u2212 EA\u223cP [f(A)] \u2264 |B|EA\u223cP [Eb\u223cU(B)[f(A \u2295 b)] \u2212 f(A)]. Also note that by the way we construct sets, we have that EAj+1\u223cPj+1 [f(Aj+1)] = EAj\u223cPj [Es\u223cU(B)[f(Aj \u2295 s)]]\nNow let \u2206j = f(B)\u2212EAj\u223cPj [f(Aj)]. By the above we have that:\n\u2206j \u2264 |B|EAj\u223cPj [Es\u223cU(B)[f(Aj \u2295 s)]\u2212 f(Aj)] = |B|EAj\u223cPj [Es\u223cU(B)[f(Aj \u2295 s)]\u2212 f(B)\n+f(B)\u2212 f(Aj)] = |B|(EAj+1\u223cPj+1 [f(Aj+1)]\u2212 f(B)\n+f(B)\u2212 EAj\u223cPj [f(Aj)]) = |B|[\u2206j \u2212\u2206j+1]\nRearranging terms, this implies that \u2206j+1 \u2264 (1 \u2212 1/|B|)\u2206j . Recursively expanding this recurrence from \u2206k, we obtain:\n\u2206k \u2264 (1\u2212 1/|B|)k\u22060\nUsing the definition of \u2206k and rearranging terms we obtain EA\u223cPk [f(A)] \u2265 (1 \u2212 (1 \u2212 1/|B|)k)f(B). The second statement follows again from the fact that (1\u2212 (1\u2212 1/|B|)k)f(B) \u2265 (1\u2212 \u03b1)f(B)\nCorollary 4. There exists a distribution that when sampled k times to construct a list, achieves an approximation ratio of (1\u22121/e) of the optimal list of size k in expectation. In particular, if A\u2217 is an optimal list of size k, sampling k times from U(A\u2217) achieves this approximation ratio. Additionally, for any \u03b1 \u2208 (0, 1], sampling dk log(1/\u03b1)e times must construct a list that achieves an approximation ratio of (1\u2212\u03b1) in expectation.\nProof. Follows from the last lemma using B = A\u2217.\nThis surprising result can also be seen as a special case of a more general result proven in prior related work that analyzed randomized set selection strategies to optimize submodular functions (lemma 2.2 in (Feige et al., 2011)).\nA.2. Proofs of Main Results\nWe now provide the proofs of the main results in this paper. We provide the proofs for the more general contextual case where we learn over a policy class \u03a0\u0303. All the results for the context-free case can be seen as special cases of these results when \u03a0 = \u03a0\u0303 = {\u03c0s|s \u2208 S} and \u03c0s(x, L) = s for any state x and list L.\nWe refer the reader to the notation defined in section ?? and 5 for the definitions of the various terms used.\nTheorem 2. Let \u03b1 = exp(\u2212m/k) and k\u2032 = min(m, k). After T iterations, for any \u03b4, \u03b4\u2032 \u2208 (0, 1), we have that with probability at least 1\u2212 \u03b4:\nF (\u03c0,m) \u2265 (1\u2212 \u03b1)F (L\u2217\u03c0,k)\u2212 R\nT \u2212 2\n\u221a 2 ln(1/\u03b4)\nT\nand similarly, with probability at least 1\u2212 \u03b4 \u2212 \u03b4\u2032:\nF (\u03c0,m) \u2265 (1\u2212 \u03b1)F (L\u2217\u03c0,k)\u2212 E[R] T \u2212\n\u221a 2k\u2032 ln(1/\u03b4\u2032)\nT \u22122 \u221a\n2 ln(1/\u03b4) T\nProof.\nF (\u03c0,m) = 1T \u2211T t=1 F (\u03c0t,m)\n= 1T \u2211T t=1 EL\u03c0,m\u223c\u03c0t [Ex\u223cD[fx(L\u03c0,m(x))]] = (1\u2212 \u03b1)Ex\u223cD[fx(L\u2217\u03c0,k(x))] \u2212[(1\u2212 \u03b1)Ex\u223cD[fx(L\u2217\u03c0,k(x))] \u2212 1T \u2211T t=1 EL\u03c0,m\u223c\u03c0t [Ex\u223cD[fx(L\u03c0,m(x))]]]\nNow consider the sampled states {xt}Tt=1 and the policies \u03c0t,i sampled i.i.d. from \u03c0t to construct the lists {Lt}Tt=1 and denote the random variables Xt = (1 \u2212 \u03b1)(Ex\u223cD[fx(L\u2217\u03c0,k(x))] \u2212 fxt(L\u2217\u03c0,k(xt))) \u2212 EL\u03c0,m\u223c\u03c0t [Ex\u223cD[fx(L\u03c0,m(x))]]\u2212fxt(Lt)]. If \u03c0t is deterministic, then simply consider all \u03c0t,i = \u03c0t. Because the xt are i.i.d. from D, and the distribution of policies used to construct Lt only depends on {x\u03c4}t\u22121\u03c4=1 and {L\u03c4}t\u22121\u03c4=1, then the Xt conditioned on {X\u03c4} t\u22121 \u03c4=1 have expectation 0, and because fx \u2208 [0, 1] for all state x \u2208 X , Xt can vary in a range r \u2286 [\u22122, 2]. Thus the sequence of random variables Yt = \u2211t i=1Xi, for t =1 to T , forms a martingale where |Yt \u2212 Yt+1| \u2264 2. By the Azuma-Hoeffding\u2019s inequality, we have that P (YT /T \u2265 ) \u2264 exp(\u2212 2T/8). Hence for any \u03b4 \u2208 (0, 1), we have that with probability at least 1 \u2212 \u03b4, YT /T \u2264 2 \u221a 2 ln(1/\u03b4)\nT . Hence we have that with proba-\nbility at least 1\u2212 \u03b4:\nF (\u03c0,m) = (1\u2212 \u03b1)Ex\u223cD[fx(L\u2217\u03c0,k(x))] \u2212[(1\u2212 \u03b1)Ex\u223cD[fx(L\u2217\u03c0,k(x))] \u2212 1T \u2211T t=1 EL\u03c0,m\u223c\u03c0t [Ex\u223cD[fx(L\u03c0,m(x))]]] = (1\u2212 \u03b1)Ex\u223cD[fx(L\u2217\u03c0,k(x))] \u2212[(1\u2212 \u03b1) 1T \u2211T t=1 fxt(L \u2217 \u03c0,k(xt))\n\u2212 1T \u2211T t=1 fxt(Lt)]\u2212 YT /T = (1\u2212 \u03b1)Ex\u223cD[fx(L\u2217\u03c0,k(x))] \u2212[(1\u2212 \u03b1) 1T \u2211T t=1 fxt(L \u2217 \u03c0,k(xt))\n\u2212 1T \u2211T t=1 fxt(Lt)]\u2212 2 \u221a 2 ln(1/\u03b4) T\nLet wi = (1\u2212 1/k)m\u2212i. From Lemma 2, we have:\n(1\u2212 \u03b1) 1T \u2211T t=1 fxt(L \u2217 \u03c0,k(xt))\u2212 1T \u2211T t=1 fxt(Lt)\n\u2264 1T \u2211T t=1 \u2211m i=1 wi(E\u03c0\u223cU(L\u2217\u03c0,k)[fxt(Lt,i\u22121 \u2295 \u03c0(xt))]\n\u2212fxt(Lt,i)) = E\u03c0\u223cU(L\u2217\u03c0,k)[ 1 T \u2211T t=1 \u2211m i=1 wi(fxt(Lt,i\u22121 \u2295 \u03c0(xt))\n\u2212fxt(Lt,i))] \u2264 max\u03c0\u2208\u03a0[ 1T \u2211T t=1 \u2211m i=1 wi(fxt(Lt,i\u22121 \u2295 \u03c0(xt))\n\u2212fxt(Lt,i))] \u2264 max\u03c0\u2208\u03a0\u0303[ 1 T \u2211T t=1 \u2211m i=1 wi(f(Lt,i\u22121 \u2295 \u03c0(xt))\n\u2212fxt(Lt,i))] = R/T\nHence combining with the previous result proves the first part of the theorem.\nAdditionally, for the sampled environments {xt}Tt=1 and the policies \u03c0t,i, consider the random variables Qm(t\u22121)+i = wiE\u03c0\u223c\u03c0t [fxt(Lt,i\u22121 \u2295 \u03c0(xt, Lt,i\u22121))] \u2212 wifxt(Lt,i). Because each draw of \u03c0t,i is i.i.d. from \u03c0t, we have that again the sequence of random variables Zj = \u2211j i=1Qi, for j = 1 to Tm forms a martingale and because each Qi can take values in a range [\u2212wj , wj ] for j = 1 + mod(i \u2212 1,m), we have |Zi \u2212 Zi\u22121| \u2264 wj . Since \u2211Tm i=1 |Zi \u2212 Zi\u22121|2 \u2264 T \u2211m i=1(1 \u2212 1/k)2(m\u2212i) \u2264 T min(k,m) = Tk\u2032, by Azuma-Hoeffding\u2019s inequality, we must have that P (ZTm \u2265 ) \u2264 exp(\u2212 2/2Tk\u2032). Thus for any \u03b4\u2032 \u2208 (0, 1), with probability at least 1\u2212\u03b4\u2032, ZTm \u2264 \u221a 2Tk\u2032 ln(1/\u03b4). Hence combining with the previous result, it must be the case that with probabil-\nity at least 1 \u2212 \u03b4 \u2212 \u03b4\u2032, both YT /T \u2264 2 \u221a 2 ln(1/\u03b4) T and\nZTm \u2264 \u221a 2Tk\u2032 ln(1/\u03b4\u2032) holds.\nNow note that:\nmax\u03c0\u2208\u03a0\u0303[ 1 T \u2211T t=1 \u2211m i=1 wi(f(Lt,i\u22121 \u2295 \u03c0(xt))\u2212 fxt(Lt,i))] = max\u03c0\u2208\u03a0\u0303[ 1 T \u2211T t=1 \u2211m i=1 wi(fxt(Lt,i\u22121 \u2295 \u03c0(xt))\n\u2212E\u03c0\u2032\u223c\u03c0t [f(Lt,i\u22121 \u2295 \u03c0\u2032(xt, Lt,i\u22121))])] + ZTm/T = E[R]/T + ZTm/T\nUsing this additional fact, and combining with previous results we must have that with probability at least 1\u2212 \u03b4 \u2212 \u03b4\u2032:\nF (\u03c0,m) \u2265 (1\u2212 \u03b1)F (L\u2217\u03c0,k)\u2212 [(1\u2212 \u03b1) 1T \u2211T t=1 fxt(L \u2217 \u03c0,k(xt))\n\u2212 1T \u2211T t=1 fxt(Lt)]\u2212 2 \u221a 2 ln(1/\u03b4) T\n\u2265 (1\u2212 \u03b1)F (L\u2217\u03c0,k)\u2212 E[R]/T \u2212 ZTm/T \u2212 2 \u221a 2 ln(1/\u03b4) T\n\u2265 (1\u2212 \u03b1)F (L\u2217\u03c0,k)\u2212 E[R]/T \u2212 \u221a 2k\u2032 ln(1/\u03b4\u2032) T\n\u22122 \u221a\n2 ln(1/\u03b4) T\nWe now show that the expected regret must grow with\u221a k\u2032 and not k\u2032, hen using Weighted Majority with the optimal learning rate (or with the doubling trick).\nCorollary 2. Under the event where Theorem 2 holds (the event that occurs w.p. 1\u2212\u03b4\u2212\u03b4\u2032), if \u03a0\u0303 is a finite set of policies, using Weighted Majority with the optimal learning rate guarantees that after T iterations:\nE[R]/T \u2264 4k \u2032 ln |\u03a0\u0303| T + 2\n\u221a k\u2032 ln |\u03a0\u0303|\nT +29/4(k\u2032/T )3/4(ln(1/\u03b4\u2032))1/4 \u221a ln |\u03a0\u0303|\nFor large enough T in \u2126(k\u2032(ln |\u03a0\u0303|+ ln(1/\u03b4\u2032))), we obtain that:\nE[R]/T \u2264 O( \u221a k\u2032 ln |\u03a0\u0303| T )\nProof. We use a similar argument to Streeter & Golovin Lemma 4 (Streeter & Golovin, 2007) to bound E[R] in the result of theorem 2. Consider the sum of the benefits accumulated by the learning algorithm at position i in the list, for i \u2208 1, 2, . . . ,m, i.e. let yi = \u2211T t=1 b(\u03c0t,i(xt, Lt,i\u22121)|xt, Lt,i\u22121), where \u03c0t,i corresponds to the particular sampled policy by Weighted Majority for choosing the item at position i, when constructing the list Lt for state xt. Note that\u2211m i=1(1 \u2212 1/k)m\u2212iyi \u2264 \u2211m i=1 yi \u2264 T by the fact that the monotone submodular function fx is bounded in [0, 1] for all state x. Now consider the sum of the benefits you could have accumulated at position i, had you chosen the best fixed policy in hindsight to construct all list, keeping the policy fixed as the policy is constructed, i.e. let zi = \u2211T t=1 b(\u03c0\n\u2217(xt, Lt,i\u22121)|xt, Lt,i\u22121), for \u03c0\u2217 = arg max\u03c0\u2208\u03a0\u0303 \u2211m i=1(1 \u2212\n1/k)m\u2212i \u2211T t=1 b(\u03c0\n\u2217(xt, Lt,i\u22121)|xt, Lt,i\u22121) and let ri = zi \u2212 yi. Now denote Z = \u221a\u2211m i=1(1\u2212 1/k)m\u2212izi.\nWe have Z2 = \u2211m i=1(1 \u2212 1/k)m\u2212izi =\u2211m\ni=1(1 \u2212 1/k)m\u2212i(yi + ri) \u2264 T + R, where R is the sample regret incurred by the learning algorithm. Under the event where theorem 2 holds (i.e. the event that occurs with probability at least 1\u2212 \u03b4\u2212 \u03b4\u2032), we had already shown that R \u2264 E[R] + ZTm, for ZTm \u2264 \u221a 2Tk\u2032 ln(1/\u03b4\u2032), in the second part of the proof of theorem 2. Thus when theorem 2 holds, we have Z2 \u2264 T + \u221a 2Tk\u2032 ln(1/\u03b4\u2032) + E[R]. Now using the generalized version of weighted majority with rewards (i.e. using directly the benefits as rewards) (Arora et al., 2012), since the rewards at each update are in [0, k\u2032], we have that with the best learning\nrate in hindsight 20: E[R] \u2264 2Z \u221a k\u2032 ln |\u03a0\u0303|. Thus we\n20if not a doubling trick can be used to get the same regret bound within a small constant factor (Cesa-Bianchi et al., 1997)\nLearning Policies for Contextual Submodular Prediction\nobtain Z2 \u2264 T + \u221a 2Tk\u2032 ln(1/\u03b4\u2032) + 2Z \u221a k\u2032 ln |\u03a0\u0303|.\nThis is a quadratic inequality of the form Z2\u2212 2Z \u221a k\u2032 ln |\u03a0\u0303| \u2212T \u2212 \u221a 2Tk\u2032 ln(1/\u03b4\u2032) \u2264 0, with the additional constraint Z \u2265 0. This implies Z is less than or equal to the largest non-negative root of the\npolynomial Z2 \u2212 2Z \u221a k\u2032 ln |\u03a0\u0303| \u2212 T \u2212 \u221a 2Tk\u2032 ln(1/\u03b4\u2032).\nSolving for the roots, we obtain Z \u2264 \u221a k\u2032 ln |\u03a0\u0303|+ \u221a k\u2032 ln |\u03a0\u0303|+ T + \u221a 2Tk\u2032 ln(1/\u03b4\u2032)\n\u2264 2 \u221a k\u2032 ln |\u03a0\u0303|+ \u221a T + (2Tk\u2032 ln(1/\u03b4\u2032))1/4\nPlugging back Z into the expression E[R] \u2264 2Z \u221a k\u2032 ln |\u03a0\u0303|, we obtain:\nE[R] \u2264 4k\u2032 ln |\u03a0\u0303|+ 2 \u221a Tk\u2032 ln |\u03a0\u0303|\n+2(2T ln(1/\u03b4\u2032))1/4(k\u2032)3/4 \u221a ln |\u03a0\u0303|\nThus the average regret:\nE[R] T \u2264 4k\u2032 ln |\u03a0\u0303| T + 2\n\u221a k\u2032 ln |\u03a0\u0303|\nT +29/4(k\u2032/T )3/4(ln(1/\u03b4\u2032))1/4 \u221a ln |\u03a0\u0303|\nFor T in \u2126(k\u2032(ln \u03a0\u0303 + ln(1/\u03b4\u2032))), the dominant term is\n2\n\u221a k\u2032 ln |\u03a0\u0303|\nT , and thus E[R] T is O(\n\u221a k\u2032 ln |\u03a0\u0303|\nT ).\nCorollary 3. Let \u03b1 = exp(\u2212m/k) and k\u2032 = min(m, k). If we run an online learning algorithm on the sequence of convex loss Ct instead of `t, then after T iterations, for any \u03b4 \u2208 (0, 1), we have that with probability at least 1\u2212 \u03b4:\nF (\u03c0,m) \u2265 (1\u2212 \u03b1)F (L\u2217\u03c0,k)\u2212 R\u0303\nT \u2212 2\n\u221a 2 ln(1/\u03b4)\nT \u2212 G\nwhere R\u0303 is the regret on the sequence of convex loss Ct, and G = 1T [ \u2211T t=1(`t(\u03c0t) \u2212 Ct(\u03c0t)) +\nmin\u03c0\u2208\u03a0\u0303 \u2211T t=1 Ct(\u03c0) \u2212 min\u03c0\u2032\u2208\u03a0\u0303 \u2211T t=1 `t(\u03c0\n\u2032)] is the \u201cconvex optimization gap\u201d that measures how close the surrogate losses Ct is to minimizing the cost-sensitive losses `t.\nProof. Follows immediately from Theorem 2 using the definition of R, R\u0303 and G, since G = R\u2212R\u0303T"}], "references": [{"title": "The multiplicative weights update method: A meta-algorithm and applications", "author": ["S. Arora", "E. Hazan", "S. Kale"], "venue": "Theory of Computing,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicol\u00f3", "Freund", "Yoav", "Schapire", "Robert"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2003}, {"title": "Error limiting reductions between classification tasks", "author": ["Beygelzimer", "Alina", "Dani", "Varsha", "Hayes", "Thomas", "Langford", "John", "Zadrozny", "Bianca"], "venue": "In ICML. ACM,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2005}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "Overview of duc 2005", "author": ["Dang", "Hoa Trang"], "venue": "In DUC,", "citeRegEx": "Dang and Trang.,? \\Q2005\\E", "shortCiteRegEx": "Dang and Trang.", "year": 2005}, {"title": "Contextual sequence optimization with application to control library optimization", "author": ["Dey", "Debadeepta", "Liu", "Tian Yu", "Hebert", "Martial", "Bagnell", "J. Andrew (Drew"], "venue": "In RSS,", "citeRegEx": "Dey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dey et al\\.", "year": 2012}, {"title": "Maximizing non-monotone submodular functions", "author": ["U. Feige", "V.S. Mirrokni", "J. Vondrak"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Feige et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Feige et al\\.", "year": 2011}, {"title": "Multiple choice learning: Learning to produce multiple structured outputs", "author": ["Guzman-Rivera", "Abner", "Batra", "Dhruv", "Kohli", "Pushmeet"], "venue": "In NIPS,", "citeRegEx": "Guzman.Rivera et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Guzman.Rivera et al\\.", "year": 2012}, {"title": "A support vector method for multivariate performance measures", "author": ["Joachims", "Thorsten"], "venue": "In ICML. ACM,", "citeRegEx": "Joachims and Thorsten.,? \\Q2005\\E", "shortCiteRegEx": "Joachims and Thorsten.", "year": 2005}, {"title": "Efficient algorithms for online decision problems", "author": ["Kalai", "Adam", "Vempala", "Santosh"], "venue": null, "citeRegEx": "Kalai et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2005}, {"title": "Learning determinantal point processes", "author": ["Kulesza", "Alex", "Taskar", "Ben"], "venue": "In UAI,", "citeRegEx": "Kulesza et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulesza et al\\.", "year": 2011}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Lin", "Chin-Yew"], "venue": "In Text Summarization Branches Out: ACL-04 Workshop,", "citeRegEx": "Lin and Chin.Yew.,? \\Q2004\\E", "shortCiteRegEx": "Lin and Chin.Yew.", "year": 2004}, {"title": "A class of submodular functions for document summarization", "author": ["Lin", "Hui", "Bilmes", "Jeff"], "venue": "In ACL-HLT,", "citeRegEx": "Lin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["Lin", "Hui", "Bilmes", "Jeff"], "venue": "In UAI,", "citeRegEx": "Lin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2012}, {"title": "The Weighted Majority Algorithm", "author": ["Littlestone", "Nick", "Warmuth", "Manfred"], "venue": "INFORMATION AND COMPUTATION,", "citeRegEx": "Littlestone et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone et al\\.", "year": 1994}, {"title": "Learning diverse rankings with multiarmed bandits", "author": ["Radlinski", "Filip", "Kleinberg", "Robert", "Joachims", "Thorsten"], "venue": "In ICML,", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "Online learning to diversify from implicit feedback", "author": ["Raman", "Karthik", "Shivaswamy", "Pannaga", "Joachims", "Thorsten"], "venue": "In KDD,", "citeRegEx": "Raman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Raman et al\\.", "year": 2012}, {"title": "Chomp: Gradient optimization techniques for efficient motion planning", "author": ["Ratliff", "Nathan", "Zucker", "Matt", "Bagnell", "J. Andrew", "Srinivasa", "Siddhartha"], "venue": null, "citeRegEx": "Ratliff et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2009}, {"title": "Agnostic system identification for model-based reinforcement learning", "author": ["Ross", "Stephane", "Bagnell", "J. Andrew"], "venue": "In ICML,", "citeRegEx": "Ross et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2012}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "Stephane", "Gordon", "Geoff", "Bagnell", "J. Andrew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Learning message-passing inference machines for structured prediction", "author": ["Ross", "Stephane", "Munoz", "Daniel", "Bagnell", "J. Andrew", "Hebert", "Martial"], "venue": "In CVPR,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "An online algorithm for maximizing submodular functions", "author": ["M. Streeter", "D. Golovin"], "venue": "In NIPS,", "citeRegEx": "Streeter and Golovin,? \\Q2008\\E", "shortCiteRegEx": "Streeter and Golovin", "year": 2008}, {"title": "An online algorithm for maximizing submodular functions", "author": ["Streeter", "Matthew", "Golovin", "Daniel"], "venue": "Technical Report CMU-CS-07-171,", "citeRegEx": "Streeter et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Streeter et al\\.", "year": 2007}, {"title": "Online learning of assignments", "author": ["Streeter", "Matthew", "Golovin", "Daniel", "Krause", "Andreas"], "venue": "In NIPS,", "citeRegEx": "Streeter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Streeter et al\\.", "year": 2009}, {"title": "Linear submodular bandits and their application to diversified retrieval", "author": ["Yue", "Yisong", "Guestrin", "Carlos"], "venue": "In NIPS,", "citeRegEx": "Yue et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2011}, {"title": "Predicting diverse subsets using structural svms", "author": ["Yue", "Yisong", "Joachims", "Thorsten"], "venue": "In ICML,", "citeRegEx": "Yue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 15, "context": "For example, recommending a diverse set of news articles increases the chance that a user would like at least one article (Radlinski et al., 2008).", "startOffset": 122, "endOffset": 146}, {"referenceID": 16, "context": "The first approach (Yue & Joachims, 2008; Yue & Guestrin, 2011; Lin & Bilmes, 2012; Raman et al., 2012) involves identifying the parameterization that best matches the submodular rewards of the training instances.", "startOffset": 19, "endOffset": 103}, {"referenceID": 16, "context": "Furthermore, while good sample complexity results are known, these guarantees only hold under strong realizability assumptions where submodular rewards can be modeled exactly by such linear combinations (Yue & Guestrin, 2011; Raman et al., 2012).", "startOffset": 203, "endOffset": 245}, {"referenceID": 15, "context": "The second, a learning reduction approach, by contrast, decomposes list prediction into a sequence of simpler learning tasks that attempts to mimic the greedy strategy (Streeter & Golovin, 2008; Radlinski et al., 2008; Streeter et al., 2009; Dey et al., 2012).", "startOffset": 168, "endOffset": 259}, {"referenceID": 23, "context": "The second, a learning reduction approach, by contrast, decomposes list prediction into a sequence of simpler learning tasks that attempts to mimic the greedy strategy (Streeter & Golovin, 2008; Radlinski et al., 2008; Streeter et al., 2009; Dey et al., 2012).", "startOffset": 168, "endOffset": 259}, {"referenceID": 5, "context": "The second, a learning reduction approach, by contrast, decomposes list prediction into a sequence of simpler learning tasks that attempts to mimic the greedy strategy (Streeter & Golovin, 2008; Radlinski et al., 2008; Streeter et al., 2009; Dey et al., 2012).", "startOffset": 168, "endOffset": 259}, {"referenceID": 5, "context": "In (Dey et al., 2012), this strategy was extended to the contextual setting by a reduction to cost-sensitive classification.", "startOffset": 3, "endOffset": 21}, {"referenceID": 16, "context": "This generality comes at the expense of being significantly less data-efficient than methods that make realizability assumptions such as (Yue & Guestrin, 2011; Raman et al., 2012), as the existing approach learns a different classifier for each position in the list.", "startOffset": 137, "endOffset": 179}, {"referenceID": 5, "context": "Examples include the notion of \u201cmultiple choice\u201d learning as in (Dey et al., 2012; Guzman-Rivera et al., 2012) where a predicted set of options is considered successful if any predicted item is deemed correct, and abandonment in ad placement (Radlinski et al.", "startOffset": 64, "endOffset": 110}, {"referenceID": 7, "context": "Examples include the notion of \u201cmultiple choice\u201d learning as in (Dey et al., 2012; Guzman-Rivera et al., 2012) where a predicted set of options is considered successful if any predicted item is deemed correct, and abandonment in ad placement (Radlinski et al.", "startOffset": 64, "endOffset": 110}, {"referenceID": 15, "context": ", 2012) where a predicted set of options is considered successful if any predicted item is deemed correct, and abandonment in ad placement (Radlinski et al., 2008) where success is measured by", "startOffset": 139, "endOffset": 163}, {"referenceID": 5, "context": "It can be shown that F obeys both monotonicity and submodularity with respect to appending policies (Dey et al., 2012).", "startOffset": 100, "endOffset": 118}, {"referenceID": 1, "context": "Algorithms that meet these requirements include Randomized Weighted Majority (Littlestone & Warmuth, 1994), Follow the Leader (Kalai & Vempala, 2005), EXP3 (Auer et al., 2003), and many others.", "startOffset": 156, "endOffset": 175}, {"referenceID": 1, "context": "EXP3 (Auer et al., 2003)).", "startOffset": 5, "endOffset": 24}, {"referenceID": 5, "context": "Although Algorithm 1 uses only a single instance of an online learner subroutine, it achieves the same performance guarantee as prior work (Streeter & Golovin, 2008; Dey et al., 2012) that employ k separate instances of an online learner.", "startOffset": 139, "endOffset": 183}, {"referenceID": 6, "context": "This fact can also be seen as a special case of a more general result proven in prior related work that analyzed randomized set selection strategies to optimize submodular functions (Feige et al., 2011).", "startOffset": 182, "endOffset": 202}, {"referenceID": 1, "context": "Analogous to the context-free setting, we can also extend to partial feedback settings where f is only partially measurable by using contextual bandit algorithms such as EXP4 (Auer et al., 2003) as the online learner (Update).", "startOffset": 175, "endOffset": 194}, {"referenceID": 2, "context": "We briefly describe two such reductions from (Beygelzimer et al., 2005):", "startOffset": 45, "endOffset": 71}, {"referenceID": 5, "context": "This matches similar guarantees provided in (Dey et al., 2012).", "startOffset": 44, "endOffset": 62}, {"referenceID": 5, "context": "Despite having similar guarantees, we intuitively expect SCP to outperform (Dey et al., 2012) in practice because SCP can use all data to train a single predictor, instead of being split to train k separate ones.", "startOffset": 75, "endOffset": 93}, {"referenceID": 17, "context": "We use local trajectory optimization techniques such as CHOMP (Ratliff et al., 2009), which have proven effective in quickly finding collision-free trajectories using local perturbations of an initial trajectory.", "startOffset": 62, "endOffset": 84}, {"referenceID": 5, "context": "We use the dataset from (Dey et al., 2012).", "startOffset": 24, "endOffset": 42}, {"referenceID": 5, "context": "In addition to the base features, we add features of the Following (Dey et al., 2012), we employ a reduction of cost-sensitive classification to regression as explained in Section 5.", "startOffset": 67, "endOffset": 85}, {"referenceID": 5, "context": "We compare SCP to ConSeqOpt (Dey et al., 2012) (which learns k separate predictors), and Regression (regress success rate from features to sort seeds; this accounts for relevance but not diversity).", "startOffset": 28, "endOffset": 46}, {"referenceID": 15, "context": "Also known as abandonment (Radlinski et al., 2008).", "startOffset": 26, "endOffset": 50}, {"referenceID": 6, "context": "2 in (Feige et al., 2011)).", "startOffset": 5, "endOffset": 25}, {"referenceID": 0, "context": "using directly the benefits as rewards) (Arora et al., 2012), since the rewards at each update are in [0, k\u2032], we have that with the best learning rate in hindsight : E[R] \u2264 2Z \u221a k\u2032 ln |\u03a0\u0303|.", "startOffset": 40, "endOffset": 60}, {"referenceID": 3, "context": "if not a doubling trick can be used to get the same regret bound within a small constant factor (Cesa-Bianchi et al., 1997)", "startOffset": 96, "endOffset": 123}], "year": 2013, "abstractText": "Many prediction domains, such as ad placement, recommendation, trajectory prediction, and document summarization, require predicting a set or list of options. Such lists are often evaluated using submodular reward functions that measure both quality and diversity. We propose a simple, efficient, and provably near-optimal approach to optimizing such prediction problems based on noregret learning. Our method leverages a surprising result from online submodular optimization: a single no-regret online learner can compete with an optimal sequence of predictions. Compared to previous work, which either learn a sequence of classifiers or rely on stronger assumptions such as realizability, we ensure both data-efficiency as well as performance guarantees in the fully agnostic setting. Experiments validate the efficiency and applicability of the approach on a wide range of problems including manipulator trajectory optimization, news recommendation and document summarization.", "creator": "LaTeX with hyperref package"}}}