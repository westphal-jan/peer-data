{"id": "1305.3321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2013", "title": "A Mining-Based Compression Approach for Constraint Satisfaction Problems", "abstract": "explicitly in this paper, we propose an extension of our mining for sat framework to constraint satisfaction problem ( csp ). we strongly consider n - ary extensional constraints ( table constraints ). our approach immediately aims to reduce the size of the csp by exploiting the structure of the constraints graph and of its associated mathematical microstructure. more roughly precisely, we should apply itemset mining techniques to search for closed frequent itemsets operating on these different two representation. using concurrent tseitin extension, we rewrite the whole csp to another compressed / csp equivalent with respect prior to satisfiability. our numerical approach contrast with previous proposed approach by katsirelos and walsh, as we don't change the initial structure of integrating the constraints.", "histories": [["v1", "Tue, 14 May 2013 23:17:49 GMT  (33kb)", "http://arxiv.org/abs/1305.3321v1", "arXiv admin note: substantial text overlap witharXiv:1304.4415"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1304.4415", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["said jabbour", "lakhdar sais", "yakoub salhi"], "accepted": false, "id": "1305.3321"}, "pdf": {"name": "1305.3321.pdf", "metadata": {"source": "CRF", "title": "A Mining-Based Compression Approach for Constraint Satisfaction Problems", "authors": ["Said Jabbour", "Yakoub Salhi"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n30 5.\n33 21\nv1 [\ncs .A\nI] 1\n4 M\nay 2"}, {"heading": "1 Introduction", "text": "The table constraint is considered for a long time as particularly important in constraint satisfaction problems (CSP). Indeed, on of the most used formulation of CSP consists in expressing the each constraint in extension or as a relation among variables with associated finite domains. Many research work, consider table constraints as the standard representation. Indeed, any constraint can be expressed using a set of allowed or forbidden tuples. However, the size of these kind of extensional constraints might be exponential in the worst case. In [6], Katsirelos and Walsh proposed for the first time a compression algorithm for large arity extensional constraints. The proposed algorithm attempts to capture the structure that may exist in a table constraint. The authors proposed an alternative representation of the set of tuples of a given relation by a set of compressed tuples. The proposed representation may lead to an exponential reduction in space complexity. However, the compressed tuples may be larger than the arity of the original constraint. Consequently, the obtained CSP do not follow the standard representation of the table constraint. The authors use decision trees to derive a set of compressed tuples.\nIn this paper, we present a new compression algorithm that combines both itemset mining techniques and Tseitin extension principles to derive a new compact representation of the table constraints. First, we show our previous Mining for SAT approach can be extended to deal with the CSP by considering the constraint graph as a transaction database, where the transactions corresponds to the constraints and items to the variables of the CSP. The closed frequent itemsets corresponds to subset of variables shared most often by the different\nconstraint of the CSP. Secondly, using extension (auxiliary variables) we show how such constraints can be rewritten while preserving satisfiability. Secondly, we consider each table constraint individually, we derive a new transaction database made of a sequence of tuples i.e. a set of indexed tuples. More precisely, each value of a tuple is indexed with its position in the constraint. By enumerating closed frequent itemsets on such transaction database, we are able to search for the largest rectangle in the table constraint. Similarly, with extension principle, we show how such constraint can be compressed while preserving the traditional representation."}, {"heading": "2 Technical background and preliminary definitions", "text": ""}, {"heading": "2.1 Frequent Itemset Mining Problem", "text": "Let I be a set of items. A set I \u2286 I is called an itemset. A transaction is a couple (tid, I) where tid is the transaction identifier and I is an itemset. A transaction database D is a finite set of transactions over I where for all two different transactions, they do not have the same transaction identifier. We say that a transaction (tid, I) supports an itemset J if J \u2286 I.\nThe cover of an itemset I in a transaction database D is the set of identifiers of transactions in D supporting I: C(I,D) = {tid | (tid, J) \u2208 D and I \u2286 J}. The support of an itemset I in D is defined by: S(I,D) =| C(I,D) |. Moreover, the frequency of I in D is defined by: F(I,D) = S(I,D)|D| .\nFor example, let us consider the transaction database in Table 1. Each transaction corresponds to the favorite writers of a library member. For instance, we have S({Hemingway,Melville},D) = |{002, 004}|= 2 and F({Hemingway, Melville},D) = 13 .\nLet D be a transaction database over I and \u03bb a minimal support threshold. The frequent itemset mining problem consists of computing the following set: FIM(D, \u03bb) = {I \u2286 I | S(I,D) > \u03bb}.\nThe problem of computing the number of frequent itemsets is #P -hard [3]. The complexity class #P corresponds to the set of counting problems associated with a decision problems in NP . For example, counting the number of models satisfying a CNF formula is a #P problem.\nLet us now define two condensed representations of the set of all frequent itemsets: maximal and closed frequent itemsets.\nDefinition 1 (Maximal Frequent Itemset). Let D be a transaction database, \u03bb a minimal support threshold and I \u2208 FIM(D, \u03bb). I is called maximal when for all I \u2032 \u2283 I, I \u2032 /\u2208 FIM(D, \u03bb) (I \u2032 is not a frequent itemset).\nWe denote by MAX (D, \u03bb) the set of all maximal frequent itemsets in D with \u03bb as a minimal support threshold. For instance, in the previous example, we have MAX (D, 2) = {{Joyce, Proust}, {Hemingway,Melville}}.\nDefinition 2 (Closed Frequent Itemset). Let D be a transaction database, \u03bb a minimal support threshold and I \u2208 FIM(D, \u03bb). I is called closed when for all I \u2032 \u2283 I, C(I,D) 6= C(I \u2032,D).\nWe denote by CLO(D, \u03bb) the set of all closed frequent itemsets in D with \u03bb as a minimal support threshold. For instance, we have CLO(D, 2) = {{Hemingway}, {Joyce, Proust}, {Hemingway,Melville}}. In particular, let us note that we have C({Hemingway},D) = {002, 004, 006} and C({Hemingway,Melville},D) = {002, 004}. That explains why {Hemingway} and {Hemingway,Melville} are both closed. One can easily see that if all the closed (resp. maximal) frequent itemsets are computed, then all the frequent itemsets can be computed without using the corresponding database. Indeed, the frequent itemsets correspond to all the subsets of the closed (resp. maximal) frequent itemsets.\nClearly, the number of maximal (resp. closed) frequent itemsets is significantly smaller than the number of frequent itemsets. Nonetheless, this number is not always polynomial in the size of the database [9]. In particular, the problem of counting the number of maximal frequent itemsets is #P -complete (see also [9]).\nMany algorithm has been proposed for enumerating frequent closed itemsets. One can cite Apriori-like algorithm, originally proposed in [1] for mining frequent itemsets for association rules. It proceeds by a level-wise search of the elements of FIM(D, \u03bb). Indeed, it starts by computing the elements of FIM(D, \u03bb) of size one. Then, assuming the element of FIM(D, \u03bb) of size n is known, it computes a set of candidates of size n+1 so that I is a candidate if and only if all its subsets are in FIM(D, \u03bb). This procedure is iterated until no more candidates are found. Obviously, this basic procedure is enhanced using some properties such as the anti-monotonicity property that allow us to reduce the search space. Indeed, if I /\u2208 FIM(D, \u03bb), then I \u2032 /\u2208 FIM(D, \u03bb) for all I \u2032 \u2287 I. In our experiments, we consider one of the state-of-the-art algorithm LCM for mining frequent closed itemsets proposed by Takeaki Uno et al. in [8]. In theory, the authors prove that LCM exactly enumerates the set of frequent closed itemsets within polynomial time per closed itemset in the total input size. Let us mention that LCM algorithm obtained the best implementation award of FIMI\u20192004 (Frequent Itemset Mining Implementations)."}, {"heading": "2.2 Constraint Satisfaction Problems: Preliminary definitions and notations", "text": "A constraint network is defined as a tuple P = \u3008X ,D, C\u3009. X is a finite set of n variables {x1, x2, . . . , xn} and D is a function mapping a variable xi \u2208 X to a domain of values D(xi) = {vi1 , vi2 . . . vidi }. We note d = max{di|1 6 i 6 n} the maximum size of the domains, and V = \u222ax\u2208XD(x) the set of all values. C is a finite set of m constraints {c1, c2, . . . , cm}. Each constraint ci \u2208 C of arity k is defined as a couple \u3008scope(ci), Rci\u3009 where scope(ci) = {xi1 , . . . , xik} \u2286 X is the set of variables involved in ci and Rci \u2286 D(xi1 ) \u00d7 . . .\u00d7 D(xik ) the set of allowed tuples i.e. t \u2208 Rci iff the tuple t satisfies the constraint ci. We define the size of the constraint network P as |P| = \u2211 c\u2208C |Rc| where |Rc| = \u2211 t\u2208Rc\n|t| and |t| = |scope(c)|. A solution to the constraint network P is an assignment of all the variables satisfying all the constraints in C. A CSP (Constraint Satisfaction Problem) is the problem of deciding if a constraint network P admits a solution or not.\nWe denote t[x] the value of the variable x in the tuple t. Let t1 = (v1, . . . , vk) and t2 = (w1, . . . , wl) be two tuples (of values or variables), we define the noncommutative operator \u2295 by t1 \u2295 t2 = (v1, . . . , vk, w1, . . . , wl). Let P = \u3008X ,D, C\u3009 be a CSP instance, c = \u3008scope(c), Rc\u3009 \u2208 C a constraint and s = (x1, . . . , xk) a sequence of variables such that V ar(s) \u2286 scope(c) where V ar(s) is the set of variables of s. We denote by Rc[s] the following set of tuples:\nRc[s] = {(t[x1], . . . , t[xk]) | t \u2208 Rc}"}, {"heading": "2.3 Tseitin\u2019s Extension principle", "text": "To explain the Tseitin principles [7] at the basis of linear transformation of general Boolean formulas to a formula in conjunctive normal form (CNF), let us introduce some necessary definitions and notations. A CNF formula \u03a6 is a conjunction of clauses, where a clause is a disjunction of literals. A literal is a positive (p) or negated (\u00acp) propositional variable. The two literals p and \u00acp are called complementary. A CNF formula can also be seen as a set of clauses, and a clause as a set of literals. The size of the CNF formula \u03a6 is defined as |\u03a6| = \u2211 c\u2208\u03a6 |c|, where |c| is equal to the number of literals in c.\nTseitin\u2019s encoding consists in introducing fresh variables to represent subformulae in order to represent their truth values. Let us consider the following DNF formula (Disjunctive Normal Form: a disjunction of conjunctions):\n(x1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 xl) \u2228 (y1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 ym) \u2228 (z1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 zn)\nA naive way of converting such a formula to a CNF formula consists in using the distributivity of disjunction over conjunction (A\u2228 (B\u2227C) \u2194 (A\u2228B)\u2227 (A\u2228C)):\n(x1 \u2228 y1 \u2228 z1) \u2227 (x1 \u2228 y1 \u2228 z2) \u2227 \u00b7 \u00b7 \u00b7 \u2227 (xl \u2228 ym \u2228 zn)\nSuch a naive approach is clearly exponential in the worst case. In Tseitin\u2019s transformation, fresh propositional variables are introduced to prevent such combinatorial explosion, mainly caused by the distributivity of disjunction over conjunction and vice versa. With additional variables, the obtained CNF formula is linear in the size of the original formula. However the equivalence is only preserved w.r.t satisfiability:\n(t1 \u2228 t2 \u2228 t3) \u2227 (t1 \u2192 (x1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 xl)) \u2227 (t2 \u2192 (y1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 ym))\n\u2227(t3 \u2192 (z1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 zn))"}, {"heading": "3 Compressing Table Constraints Networks", "text": "In this section, we proposed two compression rules for table constraints networks. The first one is based on the constraint graph aims to reduce the size of the constraint network by rewriting the constraints using the most shared variables. The second compression technique based on the microstructure of the constraint network aims to reduce the size of table constraints by exploiting common subtuples."}, {"heading": "3.1 Constraint graph Based Compression", "text": "CSP instance as transactions database: We describe the transactions database that we associate to a given constraints network. It is obtained by considering the set of variables as a set of items.\nDefinition 3. Let P = \u3008X ,D, C\u3009 be a constraints network. The transactions database associated to P, denoted T DP , is defined over the set of items X as follows:\nT DP = {(tidc, scope(c)) | c \u2208 C}\nConstraints Graph Rewriting Rule (CGR): We provide a rewriting rule for reducing the size of a constraints network. It is mainly based on introducing new variables using Tseitin extension principle.\nDefinition 4 (CGR rule). Let P = \u3008X ,D, C\u3009 be a constraints network, s = (x1, . . . , xk) a tuple of variables and {c1, c2, . . . , cn} \u2286 C a subset of n constraints of C such that V(s) \u2286 scope(ci) for 1 6 i 6 n. In order to rewrite P, we introduce a new variable y /\u2208 X and a set N of l new values such that V \u2229 N = \u2205 and l = | \u22c2n i=1 Rci [s]|. Let f be a bijection from \u22c2n i=1 Rci [s] to N . We denote by P g the constraint network \u3008X g,Dg, Cg\u3009 obtained by rewriting P with respect to s and f :\n\u2013 X g = X \u222a {y}; \u2013 Dg is a domain function defined as follows: Dg(x) = D(x) if x \u2208 X , and\nDg(y) = N .\n\u2013 Cg = C \\ {c1, . . . , cn} \u222a C\u2032 , where C\u2032 = {c0, c\u20321, . . . , c \u2032 n} such that:\n\u2022 c0 = \u3008(y, x1, . . . , xk), {(f(a1, . . . , ak), a1, . . . , ak)|(a1, . . . , ak) \u2208 \u22c2n\ni=1 Rci [s]}\u3009 \u2022 c\u2032i = \u3008(scope(ci) \u2212 s) \u2295 (y), {t[scope(ci) \u2212 s] \u2295 (f(t[s]))|t \u2208 Rci , t[s] \u2208\u22c2n\nj=1 Rcj [s]}\u3009\nIt is important to note that our rewriting rule, achieve a weak form of pairwise consistency [5]. A constraint network is pairwise consistent (PWC) iff it has nonempty relations and any consistent tuple of a constraint c can be consistently extended to any other constraint that intersects with c.\nDefinition 5 (Pairwise consistency). [2,5] Let P = \u3008X ,D, C\u3009 be a constraints network. P is pairwise consistent if and only if \u2200ci, \u2200cj \u2208 C, Rci[scope(ci)\u2229 scope(cj)] = Rcj [scope(ci) \u2229 scope(cj)] and \u2200c \u2208 C, Rc 6= \u2205.\nAs pairwise consistency deletes tuples from a constraint relation, some values can be eliminated when they have lost all their supports. Consequently, domains can be filtered if generalized arc consistency (GAC) is applied in a second step.\nAs a side effect, our CGR rewriting rule maintains some weak form of PWC. Indeed, in Definition 4, when a sub-tuple t[s] /\u2208 \u22c2n j=1 Rcj [s], the tuple t is then deleted and do not belong to the new constraint c\u2032i.\nExample 1. Let P = \u3008X ,D, C\u3009 be a constraints network, where X = {x1, . . . , x4}, D(x1) =, . . . ,= D(x4) = {a, b} and C = {c1, c2} where c1 = \u3008{x1, x2, x3}, {(b, a, a), (a, a, b), (a, b, a)}\u3009 and c1 = \u3008{x2, x3, x4}, {(a, b, a), (b, a, a), (b, a, b)}\u3009. Let s = (x2, x3) be a tuple of variables such that s \u2282 scope(c1) and s \u2282 scope(c2).By applying the CGR rule on P , we obtain Pg = \u3008X g ,Dg, Cg\u3009 such that:\n\u2013 X g = X \u222a {y} \u2013 \u2200i(1 6 i 6 4),Dg(xi) = {a, b}. We have \u22c22\nj=1 Rcj [s] = {(a, b), (b, a)}. We define f((a, b)) = c, f((b, a)) = d. Then Dg(y) = {c, d}.\n\u2013 Cg = {c0, c\u20321, c \u2032 2}\n\u2022 c0 = \u3008{y, x2, x3}, {(c, a, b), (d, b, a)}\u3009; \u2022 c\u20321 = \u3008{x1, y}, {(a, c), (a, d)}\u3009 and c \u2032 2 = \u3008{x4, y}, {(a, c), (a, d), (b, d)}\u3009\nIn this simple example, using one additional variable, we reduce the size of the constraint network from |P| = 18 to |Pg| = 16. As we can observe, the value b can be eliminated by GAC from the domain of x1.\nNecessary and sufficient condition for size reduction Let P = \u3008X ,D, C\u3009 be a constraints network, and s = (x1, . . . , xn) \u2286 X be a sub-tuple of variables corresponding to a frequent itemset Is of Pg where the minimal support threshold is greater or equal to k. Let {c1, . . . , ck} \u2286 C be the set of constraints such that s \u2286 scope(ci) for 1 6 i 6 k. Suppose that the constraints network P is pairwise consistent, in such a case, all the relations associated to each ci for 1 6 i 6 k contain the same number p of tuples. Under such worst case hypothesis, the size of P can be reduced by at least r = (n\u00d7p\u00d7k\u2212(p\u00d7k+n\u00d7p+p). Let us consider\nagain the example 1. The reduction is at least r = (2\u00d73\u00d72)\u2212(3\u00d72+2\u00d73+3) = 12\u2212 15 = \u22123. If we consider, the tuple (b, a, b) \u2208 Rc1 eliminated by the application of the CGR rule. This results in subtracting 5 from the second term of r. Consequently, we obtain a reduction of 2.\nRegarding the value of k, one can see that the compression is interesting when r > 0 i.e. k > n+1\nn\u22121 . Indeed, if n < 2 then there is no reduction. Thus, there are three cases : if n = 2, then k > 4, else if n = 3 then k > 3, k > 2 otherwise. Therefore, the constraint network is always reduced when k > 4. We obtain exactly the same condition as in our mining based compression approach of Propositional CNF formula [4]. This is not surprising, as CGR rule is an extension of our Mining4SAT approach [4] to CSP.\nClosed vs. Maximal: In [4], we introduced two condensed representations of the frequent itemsets: closed and maximal. We know that the set of maximal frequent itemsets is included in that of the closed ones. Thus, a small number of fresh variables and new clauses are introduced using the maximal frequent itemsets. However, there are cases where the use of the closed frequent itemsets is more suitable. The example given in [4], show the benefit that can be obtained by considering frequent closed itemsets. In our Mining for CSP approach we search for frequent closed itemsets.\nCompression algorithm: Given a constraint network P , we first search for closed frequent itemsets (set of variables) on T DP and then we apply the above rewriting rule on the constraint network using the discovered itemsets of variables. For more details on our algorithm, we refer the reader to the Mining4SAT greedy algorithm [4], where the overlap notion between itemsets are considered. The general compression problem can be stated as follows: given a set of frequent closed itemsets (sub-sequence of variables) and a constraints network, the question is to find an ordered sequence of operations (application of the CGR rule) leading to a CSP of minimal size."}, {"heading": "3.2 Microstructure Based Compression", "text": "In this section, we describe our compression based approach of Table constraints. First, we show how a Table constraint c can be translated to a transaction database T Dc. Secondly, we show how to compress c using itemset mining techniques.\nTable constraint as transactions database: Obviously, a table constraint c can be translated in a naive way to a transaction database T Dc. Indeed, one can define the set of items as the union of the domains of the variables in the scope of c (I = \u222ax\u2208scope(c)D(x)) and a transaction (tid, t) as the set of values involved in the tuple t \u2208 Rc. This naive representation is difficult to exploit in our context. Let I = {a, b, c} be a frequent itemset of T Dc. As the variables in\neach transaction (or tuple) associated to the values in I are different, it is difficult to compress the the constraint while using both classical tuples and compressed tuples [6]. To overcome this difficulty, we consider tuples as sequence, where each value is indexed by its position in the tuple.\nDefinition 6 (Indexed tuples). Let P = \u3008X ,D, C\u3009 be a constraint network, and ci \u2208 C a table constraint such that scope(ci) = (xi1 , xi2 , . . . , xini ). Let t \u2208 Rci a tuple of ci. We define indexed(t) = (t[xi1 ] 1, t[xi2 ] 2, . . . , t[xini ]\nni) as an indexed tuple associated to t i.e. each value of the tuple is indexed with its position in the tuple.\nDefinition 7 (Inclusion, index). Let c be a table constraint with scope(c) = {x1, . . . , xn} and t = (v1, . . . , vn) \u2208 Rc a tuple of c. We say that s = (w1, . . . , wk) is a sub-tuple of t, denoted s \u2286 t, if \u22031 6 i1 < i2 < \u00b7 \u00b7 \u00b7 < ik 6 n such that w1 = vi1 , . . . , wk = vik . We define index(t) = {1, . . . , n}, while index(w) = {i1, . . . , ik}. We also define vars(index(t)) = scope(c) and vars(index(w)) = {xi1 , . . . , xik}.\nDefinition 8. Let P = \u3008X ,D, C\u3009 be a constraints network, and c \u2208 C a table constraint where scope(c) = {x1, . . . , xn}. The transaction database associated to c, denoted T Dc, is defined over the set of items I = \u22c3 t\u2208Rc\n{t[x1]1, . . . , t[xn]n} as follows:\nT Dc = {(tidt, indexed(t))|t \u2208 Rc}\nExample 2. Let P = \u3008X ,D, C\u3009 be a constraints network, where X = {x1, x2, x3, x4}, D(x) = D(y) = D(z) = D(t) = {a, b}. Let c \u2208 C a table constraint, such that scope(c) = {x1, x2, x3, x4} and Rc = {(a, b, b, a), (a, a, b, b), (a, b, a, a), (b, b, a, a), (b, b, b, a)}. The transaction database T Dc associated to c is defined as follows:\nLet I = {b2, a4} be an itemset of T Dc. We have S(I, T Dc) = |{001, 003, 004, 005}|= 2, index(I) = {2, 4} and vars(index(I)) = {x2, x4}.\nMicrostructure Rewriting Rule (MRR): We now provide a rewriting rule for reducing the size of a table constraint.\nDefinition 9 (MRR rule). Let P = \u3008X ,D, C\u3009 be a constraints network and c \u2208 C be a table constraint with scope(c) = {x1, x2, . . . xn} and |Rc| = m. Let I = {vi11 , . . . , v ik k } be an itemset of T Dc and Y = vars(index(I)) = {xi1 , . . . , xik}. In order to rewrite c using I, we introduce a new variable z /\u2208 X and a set N of l new values such that V \u2229 N = \u2205 and l = |\n\u22c3 t\u2208Rc\nt[Y ]|. Let f be a bijection from\n\u22c3 t\u2208Rc\nt[Y ] to N . We denote by Pm the constraints network \u3008Xm,Dm, Cm\u3009 obtained by rewriting c with respect to I and f :\n\u2013 Xm = X \u222a {z}; \u2013 Dm is a domain function defined as follows: Dm(x) = D(x) if x \u2208 X , and\nDm(z) = N . \u2013 Cm = C \\ {c} \u222aC\u2032 , where C\u2032 = {c0, c\u2032} such that:\n\u2022 c0 = \u3008(z, Y ), {(f(a1, . . . , ak), a1, . . . , ak)|(a1, . . . , ak) \u2208 \u22c3\nt\u2208Rc t[Y ]}\u3009\n\u2022 c\u2032 = \u3008(scope(c)\u2212 Y )\u2295 (z), {t[scope(c)\u2212 Y ]\u2295 (f(t[Y ]))|t \u2208 Rc}\u3009\nExample 3. Let us consider again the example 2. Applying the MR rewriting rule to c with respect to I = {b2, a4}, and f where f((b, a)) = c1 and f((a, b)) = c2, we obtain the following two constraints:\n\u2013 c0 = \u3008{z, x2, x4}, {(c1, b, a), (c2, a, b)}\u3009; \u2013 c\u2032 = \u3008{x1, x3, z}, {(a, b, c1), (a, b, c2), (a, a, c1), (b, a, c1), (b, b, c1)}\u3009\nIt is easy to see that in example 3, applying MRR rule leads to a constraint of greater size. In what follows, we introduce a necessary and sufficient condition for reducing the size of the table constraint.\nNecessary and sufficient condition for size reduction Let c be a table constraint, p the number of tuples in Rc, and s = (v1, . . . , vn) \u2286 X be a subtuple of values corresponding to a frequent itemset Is of T D\nc where the minimal support threshold is greater or equal to k. Let {t1, . . . , tk} be the set of tuples such that ti[vars(index(s))] = s for 1 6 i 6 k. The size of Rc can be reduced by at least r = (n \u00d7 k \u2212 (p + 1 + n + (p \u2212 k)). Let us consider again the example 3. The reduction is at least r = (2 \u00d7 4 \u2212 (5 + 1 + 2 + (5 \u2212 4)) = 8 \u2212 9 = \u22121. In this example, we increase the size of c by one value. Indeed, |Rc| = 20 and |Rc0 |+ |Rc\u2032 | = 6 + 15 = 21.\nRegarding the value of k, one can see that applying MRR rule is interesting when r > 0 i.e. k > 2\u00d7p+n+1\nn+1 . In the previous example, no reduction is obtained\nas 4 > 2\u00d75+2+12+1 . (4 > 4, the condition is not satisfied).\nCompression algorithm of a table constraint: Given a constraint network P , and c a constraint table of P , we first search for closed frequent itemsets (subtuple of values) on T Dc and then we apply the above rewriting rule on the table constraint using the discovered itemsets of values. Similarly to the constraint graph based compression algorithm, our microstructure based compression algorithm can be derived from the one defined in [4].\nAs a summary, to compress general CSP, our approach first apply constraint graph based compression algorithm followed by the microstructure based compression algorithm."}, {"heading": "4 Conclusion and Future Works", "text": "In this paper, we propose a data-mining approach, called Mining4CSP, for reducing the size of constraints satisfaction problems when constraints are represented in extension. It can be seen as a preprocessing step that aims to discover hidden structural knowledge that are used to decrease the size of table constraints. Mining4CSP combines both frequent itemset mining techniques for discovering interesting substructures, and Tseitin-based approach for a compact representation of Table constraints using these substructures. Our approach is able to compact a CSP by considering both its associated constraint graph and microstructure. This allows us to define a two step algorithm. The first step, named coarse-grained compression, allows to compact the constraint graph using patterns representing subsets of variables. The second step, named fine-grained compression allows us to compact a given set of tuples of a given table constraint using patterns representing subset of values. Finally, an experimental evaluation on CSP instances is short term perspective."}, {"heading": "1. R. Agrawal, T. Imielinski, and A. N. Swami. Mining association rules between", "text": "sets of items in large databases. In ACM SIGMOD International Conference on Management of Data, pages 207\u2013216, Baltimore, 1993. ACM Press. 2. C. Beeri, R. Fagin, D. Maier, and M. Yannakakis. On the desirability of acyclic database schemes. Journal of the ACM, 30:479\u2013513, 1983. 3. Dimitrios Gunopulos, Roni Khardon, Heikki Mannila, Sanjeev Saluja, Hannu Toivonen, and Ram Sewak Sharma. Discovering all most specific sentences. ACM Trans. Database Syst., 28(2):140\u2013174, June 2003. 4. Sa\u0308\u0131d Jabbour, Lakhdar Sais, and Yakoub Salhi. Mining to compact cnf propositional formulae. CoRR, abs/1304.4415, 2013. 5. P. Janssen, P. Je\u0301gou, B. Nouguier, and M.C. Vilarem. A filtering process for general constraint satisfaction problems: Achieving pairwise consis- tency using an associated binary representation. In Proceedings of IEEE Workshop on Tools for Artificial Intelligence, pages 420\u2013427, 1989. 6. George Katsirelos and Toby Walsh. A compression algorithm for large arity extensional constraints. In Proceedings of the 13th International Conference on Principles and Practice of Constraint Programming - CP 2007, volume 4741 of Lecture Notes in Computer Science, pages 379\u2013393. Springer, 2007. 7. G.S. Tseitin. On the complexity of derivations in the propositional calculus. In H.A.O. Slesenko, editor, Structures in Constructives Mathematics and Mathematical Logic, Part II, pages 115\u2013125, 1968. 8. Takeaki Uno, Masashi Kiyomi, and Hiroki Arimura. Lcm ver. 2: Efficient mining algorithms for frequent/closed/maximal itemsets. In Roberto J. Bayardo Jr., Bart Goethals, and Mohammed Javeed Zaki, editors, FIMI, volume 126 of CEUR Workshop Proceedings. CEUR-WS.org, 2004. 9. Guizhen Yang. The complexity of mining maximal frequent itemsets and maximal frequent patterns. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD \u201904, pages 344\u2013353, New York, NY, USA, 2004. ACM."}], "references": [{"title": "Mining association rules between sets of items in large databases", "author": ["R. Agrawal", "T. Imielinski", "A.N. Swami"], "venue": "In ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "On the desirability of acyclic database schemes", "author": ["C. Beeri", "R. Fagin", "D. Maier", "M. Yannakakis"], "venue": "Journal of the ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1983}, {"title": "Discovering all most specific sentences", "author": ["Dimitrios Gunopulos", "Roni Khardon", "Heikki Mannila", "Sanjeev Saluja", "Hannu Toivonen", "Ram Sewak Sharma"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Mining to compact cnf propositional formulae", "author": ["S\u00e4\u0131d Jabbour", "Lakhdar Sais", "Yakoub Salhi"], "venue": "CoRR, abs/1304.4415,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "A filtering process for general constraint satisfaction problems: Achieving pairwise consis- tency using an associated binary representation", "author": ["P. Janssen", "P. J\u00e9gou", "B. Nouguier", "M.C. Vilarem"], "venue": "In Proceedings of IEEE Workshop on Tools for Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "A compression algorithm for large arity extensional constraints", "author": ["George Katsirelos", "Toby Walsh"], "venue": "In Proceedings of the 13th International Conference on Principles and Practice of Constraint Programming - CP 2007,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "On the complexity of derivations in the propositional calculus", "author": ["G.S. Tseitin"], "venue": "In H.A.O. Slesenko, editor, Structures in Constructives Mathematics and Mathematical Logic, Part II,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1968}, {"title": "Lcm ver. 2: Efficient mining algorithms for frequent/closed/maximal itemsets", "author": ["Takeaki Uno", "Masashi Kiyomi", "Hiroki Arimura"], "venue": "FIMI, volume 126 of CEUR Workshop Proceedings. CEUR-WS.org,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "The complexity of mining maximal frequent itemsets and maximal frequent patterns", "author": ["Guizhen Yang"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}], "referenceMentions": [{"referenceID": 5, "context": "In [6], Katsirelos and Walsh proposed for the first time a compression algorithm for large arity extensional constraints.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "The problem of computing the number of frequent itemsets is #P -hard [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "Nonetheless, this number is not always polynomial in the size of the database [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "In particular, the problem of counting the number of maximal frequent itemsets is #P -complete (see also [9]).", "startOffset": 105, "endOffset": 108}, {"referenceID": 0, "context": "One can cite Apriori-like algorithm, originally proposed in [1] for mining frequent itemsets for association rules.", "startOffset": 60, "endOffset": 63}, {"referenceID": 7, "context": "in [8].", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "To explain the Tseitin principles [7] at the basis of linear transformation of general Boolean formulas to a formula in conjunctive normal form (CNF), let us introduce some necessary definitions and notations.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "It is important to note that our rewriting rule, achieve a weak form of pairwise consistency [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 1, "context": "[2,5] Let P = \u3008X ,D, C\u3009 be a constraints network.", "startOffset": 0, "endOffset": 5}, {"referenceID": 4, "context": "[2,5] Let P = \u3008X ,D, C\u3009 be a constraints network.", "startOffset": 0, "endOffset": 5}, {"referenceID": 3, "context": "We obtain exactly the same condition as in our mining based compression approach of Propositional CNF formula [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "This is not surprising, as CGR rule is an extension of our Mining4SAT approach [4] to CSP.", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "Maximal: In [4], we introduced two condensed representations of the frequent itemsets: closed and maximal.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "The example given in [4], show the benefit that can be obtained by considering frequent closed itemsets.", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "For more details on our algorithm, we refer the reader to the Mining4SAT greedy algorithm [4], where the overlap notion between itemsets are considered.", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "each transaction (or tuple) associated to the values in I are different, it is difficult to compress the the constraint while using both classical tuples and compressed tuples [6].", "startOffset": 176, "endOffset": 179}, {"referenceID": 3, "context": "Similarly to the constraint graph based compression algorithm, our microstructure based compression algorithm can be derived from the one defined in [4].", "startOffset": 149, "endOffset": 152}], "year": 2013, "abstractText": "In this paper, we propose an extension of our Mining for SAT framework to Constraint satisfaction Problem (CSP). We consider n-ary extensional constraints (table constraints). Our approach aims to reduce the size of the CSP by exploiting the structure of the constraints graph and of its associated microstructure. More precisely, we apply itemset mining techniques to search for closed frequent itemsets on these two representation. Using Tseitin extension, we rewrite the whole CSP to another compressed CSP equivalent with respect to satisfiability. Our approach contrast with previous proposed approach by Katsirelos and Walsh, as we do not change the structure of the constraints.", "creator": "LaTeX with hyperref package"}}}