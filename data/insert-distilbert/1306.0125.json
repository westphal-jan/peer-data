{"id": "1306.0125", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2013", "title": "Understanding ACT-R - an Outsider's Perspective", "abstract": "the act - r theory of cognition developed by john anderson and colleagues endeavors to explain how humans recall chunks representation of information and how they solve problems. act - r also serves as a theoretical basis for \" cognitive comprehension tutors \", i. e., automatic tutoring solving systems that help students learn primary mathematics, computer programming, and other cognitive subjects. the official act - r definition is distributed across a large body of literature spanning across many articles and monographs, and the hence it is difficult for each an \" outsider \" to learn the most important aspects of the theory. this paper aims to someday provide a tutorial to the core components in of the act - r theory.", "histories": [["v1", "Sat, 1 Jun 2013 15:48:58 GMT  (100kb,D)", "http://arxiv.org/abs/1306.0125v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jacob whitehill"], "accepted": false, "id": "1306.0125"}, "pdf": {"name": "1306.0125.pdf", "metadata": {"source": "CRF", "title": "Understanding ACT-R \u2013 an Outsider\u2019s Perspective", "authors": ["Jacob Whitehill"], "emails": ["jake@mplab.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "The Adaptive Character of Thought - Rational (ACT-R) is a theory of cognition developed principally by John Anderson at Carnegie-Mellon University [4]. ACT-R models how humans recall \u201cchunks\u201d of information from memory and how they solve problems by breaking them down into subgoals and applying knowledge from working memory as needed.\nACT-R is the latest in a series of cognitive models in the ACT family; it succeeds ACT, ACTE, and ACT*. With ACT* [2], Anderson endeavored to describe the human memory and reasoning faculties mechanistically \u2013 i.e., to describe the mechanisms through which memory recall and cognition take place \u2013 by describing how goals are broken down into subgoals using \u201cproduction rules.\u201d Later, in 1990, Anderson took a step back from the existing ACT* and asked how cognitive processes such as memory, categorization, causal inference, and problem solving could be cast as optimal solutions to the tasks that humans commonly encounter. The underlying belief was that humans, from an evolutionary perspective, represent a local maximum of adaptedness to their environment; therefore, their basic memory retrieval and decision-making processes should also be somehow optimal. Optimality severely constrains the possible mechanisms of human cognition and thus helps to reduce the search space for the true underlying mechanism.\nWith his new-found optimization approach, Anderson returned in 1993 to the ACT production rulebased framework and revised ACT* to create ACT-R. The \u201cR\u201d (rational) implies that the human mind is behaving \u201crationally,\u201d in the sense that it wishes to solve problems to maximize reward and minimize cost. This formulation of cognition (with costs and rewards) is somewhat reminiscent of stochastic optimal control theory.\nUnfortunately, in contrast to the precisely defined mathematics of stochastic optimal control, the ACT-R literature is marred by a lack of specificity and consistency. The ACT-R definition is distributed across a vast collection of journal articles and monographs, and no single text is sufficient to provide a complete definition. Important parts of the ACT-R definition vary from source to source, with no explanation as to why the change was made, or even an acknowledgement that the change had occurred at all. (An example of this is the decay rate of a learning event for estimating its effect on activation.) Mathematically precise terminology such as \u201clog odds of X\u201d (for some event X) is used injudiciously, without any proof that the associated quantity equals what it should. The reader must sometimes guess what the author meant to say in order for the equations to hold true.\nThis tutorial endeavors to describe clearly the main ideas of ACT-R from the top-level decisionmaking processes down to the level of strengthening the presence of knowledge chunks in memory. The goal of the document is to provide clarity where the original ACT-R literature was vague or inconsistent, and to summarize in one relatively short document the \u201ccomplete picture\u201d of ACT-R (or at least the main points) that are scattered throughout the vast ACT-R literature corpus."}, {"heading": "1.1 Roadmap", "text": "We first introduce the crucial distinction between declarative knowledge and procedural knowledge in Section 2. The document then proceeds in a top-down fashion: Under the assumption that the agent (a human, or possibly a computer) already has all of the knowledge he/she needs, we examine in Section 3 how the decision-making process is made on a rational basis under ACT-R. In particular, we describe\nar X\niv :1\n30 6.\n01 25\nv1 [\ncs .L\nG ]\n1 J\nun 2\n01 3\nthe mechanism by which a particular \u201cproduction rule,\u201d corresponding to the \u201cactions\u201d of ACT-R, is chosen out of many possible alternatives. In Section 4, we remove the assumption that knowledge is already available, and describe the ACT-R processes by which new knowledge is acquired. This includes both the creation of new memories, as well as the strengthening (and decay) of existing ones. Finally, in Sections 6 and 7, we discuss how ACT-R partially models the Spacing Effect and the Power Laws of Learning/Forgetting."}, {"heading": "2 Declarative versus Procedural Knowledge", "text": "Under ACT-R, human knowledge is divided into two disjoint but related sets of knowledge \u2013 declarative and procedural. Declarative knowledge comprises many knowledge chunks, which are the current set of facts that are known and goals that are active. Two examples of chunks are \u201cThe bank is closed on Sundays,\u201d and \u201cThe current goal is to run up a hill.\u201d Notice that each chunk may refer to other chunks. For instance, our first example chunk refers to the concepts of \u201cbank,\u201d \u201cclosed,\u201d and \u201cSunday,\u201d which presumably are themselves all chunks in their own right. When a chunk i refers to, or is referred to by, another chunk j, then chunk i is said to be connected to chunk j. This relationship is not clearly defined in ACT-R \u2013 for instance, whether the relationship is always symmetrical, or whether it can be reflexive (i.e., a chunk referring to itself in recursive fashion), is not specified.\nProcedural knowledge is the set of production rules \u2013 if/then statements that specify how a particular goal can be achieved when a specified pre-condition is met \u2013 that the agent currently knows. A production rule might state, for instance, \u201cIf I am hungry, then eat.\u201d For the domain of intelligent tutoring systems, for which ACT* and ACT-R were partly conceived, a more typical rule might be, \u201cIf the goal is to prove triangle similarity, then prove that any two pairs of angles are congruent.\u201d\nThe human memory contains many declarative knowledge chunks and production rules. At any point in time, when a person is trying to complete some task, a production rule, indicating the next step to take in order to solve the problem, may \u201cfire\u201d if the rule\u2019s pre-condition, which is a conjunction of logical propositions that must hold true according to the current state of declarative memory, is fulfilled. Since the currently available set of knowledge chunks may fulfill the pre-conditions of multiple production rules, a competition exists among production rules to select the one that will actually fire. (This competition will be described later.) Whichever rule ends up firing may result either in the goal being achieved, or in the creation of new knowledge chunks in working memory, which may then trigger more production rules, and so on."}, {"heading": "2.1 Example", "text": "Let us consider a concrete setting in order to make the above ideas more concrete. ACT-R most readily lends itself to domains in which tasks can be decomposed into well-defined component operations. Suppose the current task the person is working on is to add two multi-digit numbers. Table 1 proposes a plausible set of productions with which this problem can be solved. Let us assume for now that the agent (a math student, presumably) already possesses the productions given in Table 1) in his/her procedural memory, and also that he/she can perform single-digit addition. This latter assumption could be fulfilled in two ways: either the student has memorized these basic addition facts by rote, in which case they would be stored in declarative memory; or he/she knows some counting procedure (e.g., using the fingers) to implement simple addition. This procedure would correspond to another production rule in its own right, which for brevity we have omitted.\nGiven the current goal to add two multi-digit numbers \u2013 in the present example, 36 and 23 \u2013 the student\u2019s mind, under the ACT-R model, implicitly matches the available declarative memory chunks to the pre-condition of all productions. In the example shown, based on the contents of declarative memory, there is always only one possible production rule that can be applied at each step, but ACT-R allows for more general scenarios and specifies how to select from multiple production rules whose pre-conditions are all fulfilled.\nWe briefly describe the execution of the first few steps. At the onset, the only contents of memory relevant to the addition task is that the goal is to add two numbers. This \u201cchunk\u201d of information \u2013 a goal chunk, in this case \u2013 matches the pre-condition of only a single production, P1. After some small amount of memory retrieval time (latency of production matching will be described in Section 4), the\nproduction will \u201cfire,\u201d meaning that its action \u2013 embedded in the \u201cThen\u201d part of the production rule \u2013 will be executed. In the case of P1, a new chunk is written to declarative memory storing the information k=0, and a subgoal \u201cProcess columns\u201d is pushed onto the \u201cgoal stack.\u201d\nThe above paragraph describes one complete step of execution within the decision-making framework of ACT-R. At this point, a new goal chunk sits at the top of the goal stack, and declarative memory contains a new chunk (k=0). The process then proceeds as before \u2013 the set of declarative memory chunks is matched to all the productions\u2019 pre-conditions, causing one of these productions to fire. At later points of execution, instead of \u201cpushing\u201d a subgoal onto the goal stack, a \u201csubroutine\u201d of execution will have completed and a production will instead \u201cpop\u201d a goal chunk off the stack.\nIt should be noted that ACT-R only supports a single goal stack; how multiple conflicting or completing goals (which are ubiquitous in real life) are handled is not explored. A theory of goal pre-emption \u2013 the process whereby a more important goal interrupts decision-making on behalf of a less important one \u2013 is also not developed.\nThe entire production queue from start to finish for the given addition problem (36 + 23) is shown in Figure 1, along with the contents of working memory and the state of the environment at each time-step. In the next section, we describe the general decision-making process under ACT-R in which multiple productions must compete for execution."}, {"heading": "3 Decision Making in ACT-R", "text": "At any moment in time, the knowledge chunks in declarative memory may fulfill, or match, a part of the pre-condition of a production rule (or of multiple production rules). When all parts of a production rule\u2019s pre-condition are fulfilled (recall that a pre-condition is a logical conjunction), then production rule itself is said to match/be matched. Given multiple production rules that are matched, a decision must still be made as to which production rule will actually fire (be executed). This decision is made on the basis of the expected value V of each production that matches, as well as the time t when the matching occurs.\nThe expected value V is learned by the agent through experience, and the time of match t is dictated by the latency of matching, which decreases with learning. We will discuss learning in Section 4; for now, however, assume that the time of matching t, and the expected value V , of each production is already known by the agent."}, {"heading": "3.1 Flowchart", "text": "The decision process of choosing which production to fire is shown in Figure 2. Suppose production si had matched at time t and that the expected value of si is Vi = piG\u2212Ci, where G is the reward of achieving the goal, p is the probability of achieving the goal if production si fires, and Ci is the expected cost (both immediate and future) of si. Note that Ci refers to \u201creal-world\u201d costs of performing the production\u2019s action (e.g., the cost of gasoline if, for instance, driving a car is involved), not memory retrieval costs (which will be discussed momentarily). According to ACT-R, the agent must decide whether to choose (fire) production si, or instead to wait for a better production sj (with higher value Vj). This decision is made by assessing whether the expected gain of waiting exceeds the expected cost \u2013 we call this \u03c4 .\nIn ACT-R, the cost of waiting is modelled by a single constant, designed to reflect the memory retrieval cost of matching another production sj in the future. A constant valued cost can only approximate the true cost for two reasons: first, several higher-valued productions may in fact end up matching (if each is better than the last) and thus being retrieved, and hence the cost should actually vary with the number of productions whose value exceeds Vi. Second, each production rule may refer to a different number of declarative memory chunks; presumably, retrieving each of these takes some energy as well. Nevertheless, a constant-valued \u03c4 provides at least some approximation of waiting costs.\nIt is difficult to infer from [4] the exact process by which the above rational decision is made. Two alternative interpretations are possible:\n1. At time t (when production si fired), the agent implicitly computes a fixed amount of time T during which it will wait for higher-valued productions. If no higher-valued production matches during this interval, then si is fired; if production sj matches such that Vj > Vi, then the time t of the last-matched production is updated (to the time sj matched), and the decision process starts over. 2. At each moment in time t\u2032 \u2265 t (until infinity), the agent will decide, using some binary decision rule D(t), whether to accept si as the best choice, or to wait for a possibly higher-valued production to match. If at time t\u2032 another production sj matches such that Vj > Vi, then the decision process starts over, with Vj the new value-to-beat.\nIn fact, these two varying formulations may be reconciliable, if D(t) is chosen somehow to be consistent with T . In ACT-R, the decision rule D(t) is:\nFire if \u222b G Vi\n(x\u2212 Vi)Zt(x;Vi)dx \u2264 \u03c4 Wait otherwise\n(1)\nZt(x;Vi) is the probability distribution that a production with value x will ever match, some time between t and infinity, and \u03c4 is the cost of waiting (retrieving a future production), described above. Zt is defined as (see [4], p. 62, and [3], p. 215):\nZt(x;Vi) = 1 t(G\u2212 Vi) e \u2212 1 t(G\u2212Vi) (G\u2212x)\nThis distribution integrates to 1 iff x \u2208 [\u2212\u221e, G], i.e., if future productions are bounded above by the value of G.1 Note that the probability of a production with value x occurring decreases with time; this was intended by Anderson to model the idea that higher-valued productions should match earlier than lower-valued productions [4]. It is important to realize that, according to this definition of Zt, t is not the time when the production with value x fires.\nApplying this decision rule D(t) at each time t\u2032 \u2265 t implicitly results in a time-window T for which the agent will end up waiting after production si fired. Hence, the two alternative formulations posed above are somehow equivalent. However, how to calculate T from D(t) is unclear."}, {"heading": "3.2 Selecting from Multiple Productions", "text": "Each production rule has an associated expected value:\nV = pG\u2212 C (2)\nwhere G is the reward earned by reaching the current goal, p is the probability that the goal will be reached if the production is selected, and C is the expected immediate and future costs of selecting the production.\nSuppose a production i with value Vi is matched at time t, and suppose V \u2217 is the highest value production that can ever match the current set of declarative knowledge chunks. (Recall that productions have a latency before they match.) Then production i will fire (be selected) iff:\nE[V \u2217 \u2212 Vi] \u2264 \u03c4 1Note that, under this distribution, higher -valued productions are given a higher probability of occurring. It is unclear\nwhether this was intended.\nfor a fixed threshold \u03c4 . This expectation is computed over the distribution Zt(V \u2217) which is the probability of a production firing with value V \u2217. Under ACT-R, Z is dependent on t in order to \u201callow for the possibility that later instantiations are less likely to be as good as earlier instantiations\u201d ([4], p. 62). Hence, production i will fire iff: \u222b G\nV\n(V \u2217 \u2212 V )Zt(V \u2217)dV \u2217 \u2264 \u03c4\n(Note that G is the maximum value a production could have since p \u2208 [0, 1] and C > 0.)"}, {"heading": "4 Learning in ACT-R", "text": "In ACT-R, learning consists of creating new knowledge chunks and production rules in memory, and in strengthening these memories through use. We discuss each category of learning below."}, {"heading": "4.1 Creating New Knowledge Chunks", "text": "Very little is said about how ACT-R models the initial creation of declarative memory: Either a new chunk is \u201ccreated as the encoding of external events\u201d ([4], p. 70), or a chunk is written to memory as the result of an executed production rule. The letter case is exemplified in Figure 1, where the knowledge chunk (variable k) that contains the column currently being processed is updated (k is incremented by 1 in P2)."}, {"heading": "4.2 Creating New Production Rules", "text": "The initial creation of productions is somewhat more developed. New production rules are formed through several processes, including: proceduralization, composition, generalization, and analogy."}, {"heading": "4.2.1 Proceduralization", "text": "Production rules often contain variables to refer to particular values on which the production operates. An example was shown in Figure 1 for the addition of multi-digit numbers. After performing the same exact task many times, new productions can arise in procedural memory which contain the parameter values \u201chard-coded\u201d into them. For instance, if the addition 36 + 23 is performed repeatedly, then the laborious application of the six production rules could be skipped by creating a new production: \u201cIf the goal is to add 36 and 23, then write 59.\u201d This is known as proceduralization and is a form of compilation under ACT-R [1]. However, ACT-R proposes no model of when or under what conditions such a proceduralization will occur."}, {"heading": "4.2.2 Composition", "text": "If two productions perform problem-solving steps in sequence, then the productions can be combined. Since applying each production requires some cost, performing both productions in one step can potentially save computational resources such as time and/or effort. For an example of composition, if P1 (not the one in Table 1) has the effect of multiplying both sides of the equation 4\n5 x = 1 by 5, and P2\ndivides both sides by 4 (to solve for x), then a new production \u2013 call it P3 \u2013 could combine these steps and multiply both sides by 5\n4 ."}, {"heading": "4.2.3 Generalization", "text": "Generalization occurs when many similar production rules have already been stored, from which a more general rule can be learned inductively. For example, a child may already know that the plural of \u201ccat\u201d is \u201ccats,\u201d and that the plural of \u201cdog\u201d is \u201ddogs.\u201d From these rules, he/she may infer that a plural can be formed by appending an \u201ds\u201d to the singular. The astute reader will note that there are many exceptions to this rule \u2013 for instance, \u201cox\u201d and \u201coxen\u201d, or \u201coctopus\u201d and \u201coctopodes.\u201d2 This necessitates what is called discrimination, in which additional production rules are learned to handle the exceptions.\n2The proper English plural form of \u201coctopus\u201d is either \u201coctopodes\u201d or \u201coctopuses.\u201d"}, {"heading": "4.2.4 Analogy", "text": "When presented with a new problem for which no solution (in the form of production rules) is known, a human can reason by analogy from previously seen example solutions to similar problems. For example, having encountered the programming problem in which a variable is to be increased by 4, whose solution (in C) is x += 4, he/she may reason that the solution to the similar task of multiplying a variable by 4 might be x *= 4, assuming the syntax for multiplication (*) was already known. Analogy-based learning requires that the learner bew able to create a mapping from the known example to the present task. In the case above, the mapping was from addition to multiplication. The new production rule is created by applying the same mapping to the solution production of the example problem, i.e., \u201c...Then write x += 4\u201d is mapped to \u201c...Then write x *= 4.\u201d\nAnalogy may also occur even if (perhaps inefficient) production rules already exist which can solve the current problem. According to Anderson, this could potentially be modeled under ACT-R by assuming the existence of a meta-production \u2013 analogy \u2013 which models the value of learning new productions for future use."}, {"heading": "4.3 Strengthening Existing Chunks", "text": "In ACT-R, the strength in memory of a knowledge chunk is called activation. A chunk can gain activation through use: a chunk is used when it matches against some production rule that fires. This definition is also consistent with the idea of memory strength increasing through practice by introducing a trivial \u201crecall\u201d production: by defining the current goal to be \u201cPractice i,\u201d and defining a production as \u201cIf the goal is to practice i, then recall i.\u201d\nAnderson posits that the activation A(t) of a knowledge chunk is the log-odds that, at time t, it will match to some production rule that ends up firing. 3 The activation A(t) of a particular chunk is defined as:\nA(t) = B(t) + \u2211 j wjsij (3)\nwhere B(t) is the base activation of the knowledge chunk at time t, and the summation is the associative strength of the chunk dependent on related chunks, defined as \u201cthe elements in the current goal chunk and the elements currently being processed in the perceptual field\u201d ([4], p. 51). Each wj \u2208 [0, 1] \u201creflects the salience or validity of [related] element j.\u201d The sij \u2208 [\u2212\u221e,+\u221e] are \u201cthe strengths of association to i from elements j in the current context.\u201d\nThe base activation B(t) is increased whenever it is \u201cused,\u201d either through practice (\u201clearning events\u201d) or when matched to a production rule.\nB(t) = ln \u2211 k t\u2212dk +B (4)\nThe decay rate d is defined in the ACT-R literature in various ways:\n\u2022 In [4], the decay rate d is a constant for all k. \u2022 In [6], the decay rate of activation for the particular learning event k is defined as dk = max{d1, b(tk\u22121\u2212 tk) \u2212d1}4 where d1 (a constant) is the decay rate of the first learning event for the chunk, and b is\na constant. The difference tk\u22121 \u2212 tk expresses the spacing effect \u2013 the notion that tight spacing of learning events results in lesser activation gains than longer spacing.\n\u2022 In [8], dk = cemk\u22121 + \u03b1, where c and \u03b1 are constants, and mk\u22121 is the activation of the chunk at the time of the previous learning event (k \u2212 1). This definition also expresses the spacing effect since, if the activation was already high at the previous learning event k \u2212 1, then the decay rate of the kth learning event will also be high.\n3In fact, he defines activation in two conflicting ways: in [4], p. 64, chunk activation is the log-odds that the chunk matches a production that fires; in [4], p. 50, however, activation is merely the log-odds of matching any production, not necessarily the one that fires. This distinction is crucial.\n4In [6], the base was actually tk \u2212 tk\u22121, but since older events have longer decay times t, this would result in a negative base, and hence a complex number for the decay, which was presumably unintended."}, {"heading": "4.4 Learning Associative Weights", "text": "The activation Ai(t) of chunk i is also afffected by the presence of other chunks in the current context. The notion of context in ACT-R is not clearly defined, but it is described roughly as \u201cthe elements in the current goal chunk and the elements currently being processed in the perceptual field\u201d ([4], p. 51). Presumably, the elements of the goal chunk refer to those chunks to which the goal chunk is connected (in the sense of Section 2).\nSuppose chunk j is in the current context. Then its effect on Ai(t) is as follows: if i is connected to j, then the log-odds of chunk i matching to a production that fires (i.e., i\u2019s activation) is increased by wjsij . The wj are barely defined at all, except that each wj \u2208 [0, 1], and that it \u201creflects the salience or validity of [related] element j.\u201d [7] suggests that the wj express \u201cattentional weighting, i.e., the variable degree of attention different individuals are able to dedicate to the elements of the current context.\u201d\nThe sij \u2208 [\u2212\u221e,+\u221e] are \u201cthe strengths of association to i from elements j in the current context\u201d ([4], p. 51). They are weights that are learned based on the history of chunk i matching a production that fires with or without the accompanying presence of chunk j in the current context. More precisely, sij represents the log-likelihood ratio term\nlog p(Cj | Ni) p(Cj | Ni)\nwhere Cj is the event that chunk j is in the current context and Ni is the event that i matches a production that fires. sij can be approximated as\nsij = log p(Cj | Ni) p(Cj | Ni) \u2248 log p(Cj | Ni) p(Cj) = log p(Ni | Cj) p(Ni)\nThe justification for this approximation given in [4] is that knowing the outcome ofNi cannot substantially affect the probability of event Ni since there are presumably very many chunks in all of declarative memory. The two terms in the fraction, p(Ni | Cj) and p(Ni), can be estimated empirically based on the history of productions that fired, and the chunks that matched them, and the context at each moment a production fired. In [4], a prior probability for both of these terms was also employed."}, {"heading": "4.5 Learning Production Success Probability and Production Costs", "text": "In order to estimate the value of a production V = pG\u2212C, the terms p and C must both be computed, where p is the probability that the production will lead to the goal being achieved, and C is the expected immediate and future cost of firing this production. Probability p can be decomposed into the probability that the production s itself will succeed, i.e., have its intended affect, multiplied by the probability that the goal will be reached conditional on production s succeeding. The first probability, which is called q in [4], can be estimated by updating a Beta distribution every time that a production either succeeds or fails. The second quantity, called r, is estimated by ACT-R heuristically in one of two different ways: from the cost already incurred in achieving the goal (the assumption is that, the higher the total cost already spent, the lower the probability should be), or from the similarity between the current state and the goal state.\nThe notion of a production \u201csucceeding\u201d and the real-world \u201ccosts\u201d involved in a production are not thoroughly developed in ACT-R. As stated by the author himself in [3], the proposed methods of estimating p and C are not optimal, but rather only plausible. We do not review them in detail in this tutorial. More conventional methods, from the perspective of the machine learning community, for estimating these quantities are available in the reinforcement learning literature for Markov processes."}, {"heading": "4.6 Strengthening Existing Productions", "text": "Analagous to activation for knowledge chunks, the measure of memory strength of a production rule is the production strength, S. In [4], S is supposed to be the log-odds that the production rule will fire, and it is defined as:\nS(t) = ln \u2211 k t\u2212dk +B (5)\nwhere the k indexes events in which the production fires ([4], p. 293), and B is a constant. (It is unclear from [4] if the B in 5 is the same B as in 4.)"}, {"heading": "4.6.1 Cautionary Note", "text": "Equation 3 supposedly models the log-odds that a particular chunk will match the production that fires, and 5 models the log-odds of a particular production rule firing. However, these probabilities are not independent: for instance, if chunk i is the sole chunk referenced by production rule s, and if Ai(t) = \u2212\u221e (i.e., the chunk has no chance of being matched to the rule that fires), then S(t) must also equal \u2212\u221e. Moreover, since production firing is modelled as a competition between multiple production rules that fired, the log-odds of a particular production firing cannot be calculated independently of other productions. For both the above reasons, the associated equations cannot truly equal the log-odds that they are claimed to represent."}, {"heading": "4.7 Latency of Production Rule Matching", "text": "Learning can also decrease the latency involved in a production rule matching. The latency of a production rule s is defined as the period between the first moment that all of the knowledge chunks in the pre-condition of s were in memory, to the time when s actually matches. This latency is modeled as:\nL(t) = \u2211 i Be\u2212b(Ai(t)+S(t)) (6)\nwhere the summation is over the required knowledge chunks for the production, Ai(t) is the \u201cactivation\u201d (a measure of strength in memory, described below) of the knowledge chunk at time t, and S(t) is the strength of the production itself at time t. B and b are constants."}, {"heading": "5 Memory", "text": ""}, {"heading": "5.1 Relationship of Latency to Probability of Recall", "text": "The probability of recalling a knowledge chunk is\nP (t) = 1\n1 + e\u2212(A(t)\u2212\u03c4)/s\nwhich means that a knowledge chunk will tend not to be remembered if its activation A(t) is below \u03c4 . This is consistent with ACT-R because certain productions will never fire if their latency is too long."}, {"heading": "6 The Spacing Effect", "text": "The spacing effect is a psychology phenomenon whereby probability of remembering an item at test-time can be increased by scheduling practice sessions such that a substantial time difference exists between successive sessions. Intuitively, if one had just drilled a particular item a few seconds ago, then it will make little difference to the person\u2019s memory if he/she immediately drills it again. The spacing effect can be modeled in ACT-R in at least two different ways, as explained in Section 4.3."}, {"heading": "7 The Power Laws of Learning", "text": ""}, {"heading": "7.1 Latency", "text": "Though much touted for supposedly modeling the power law of learning, ACT-R in fact exhibits only a tenuous relationship with this phenomenon. One form of the power law of learning is that the latency of a correctly retrieved declarative memory chunk L decays as a power function, which has the general form L(t) = At\u2212k for constants A and k assuming that the number of practice sessions grows with t. However, in ACT-R, in order for this relationship to hold, one must assume that the decay rate d of multiple practice sessions is constant (which then destroys the modeling of the spacing effect), and also that the time between practice sessions tk\u22121 \u2212 tk is likewise some constant \u2206t. One must also consider the latency of a knowledge chunk in isolation from any particular production, as in [9] \u2013 in the ACT-R\ndefinition in [4], however, latency is only explicitly defined for the instantiation of a production rule (matching of its pre-condition to chunks); hence, strictly speaking, this too is an approximation.\nUnder these assumptions, the expected latency of recall of a knowledge chunk is approximately modeled by a power function. (The proof below is adapted from [5]). The latency of recall of a particular chunk as defined in [9] is:\nL(t) = Fe\u2212A(t) + C\nwhere A(t) is the activation of the chunk at time t, and F and C are constants, such that C represents some fixed time cost of retrieval of the chunk. Since the only component of A(t) that depends on t is B(t), let us ignore these terms as well (since they can be folded into F ) \u2013 these terms are the associative strength term and the constant B term in Equation 4. Then,\nL(t) = Fe\u2212B(t) = Fe \u2212 ln \u2211K k=0 t\u2212d k\n= F 1\u2211K\nk=0 t\u2212dk\nNow, since the learning events indexed by k were assumed to be equally spaced in time, we can rewrite this as:\nL(t) = F 1\u2211K\nk=0 ((K \u2212 k)\u2206t)\u2212d\n= F (\u2206t)d\u2211K\nk=0 (K \u2212 k)\u2212d\n= F (\u2206t)d\u2211K k=0 k\u2212d \u2248 F (\u2206t) d\u222b K\nk=0 k\u2212ddk\n= F (\u2206t)d K1\u2212d/(1\u2212 d) = F (\u2206t)d(1\u2212 d)Kd\u22121\nThe number of learning events K is a function of t: K(t) = t \u2206t . Hence:\nL(t) = F (\u2206t)d(1\u2212 d) ( t\n\u2206t )d\u22121 = Gtd\u22121\nfor some constant G."}], "references": [{"title": "Acquisition of cognitive skill", "author": ["J.R. Anderson"], "venue": "Psychological Review, 89(4):369\u2013406", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1982}, {"title": "The Architecture of Cognition", "author": ["J.R. Anderson"], "venue": "Harvard University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1983}, {"title": "Adaptive Character of Thought", "author": ["J.R. Anderson"], "venue": "Lawrence Erlbaum Associates", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1993}, {"title": "Rules of the Mind", "author": ["J.R. Anderson"], "venue": "Lawrence Erlbaum Associates", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "Practice and retention: A unifying analysis", "author": ["J.R. Anderson", "J.M. Fincham", "S. Douglass"], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition, 25(5):1120\u20131136", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Reflections of the environment in memory", "author": ["J.R. Anderson", "L.J. Schooler"], "venue": "Psychological Science, 2:396\u2013408", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "Memory", "author": ["D. Fum", "A. Stocco"], "venue": "emotion, and rationality: An ACT-R interpretation for gambling task results. In Proceedings of the sixth International Conference on Cognitive Modeling", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Using a model to compute the optimal schedule of practice", "author": ["P.I. Pavlik", "J.R. Anderson"], "venue": "Journal of Experimental Psychology: Applied, 14(2):101\u2013117", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimizing knowledge component learning using a dynamic structural model of practice", "author": ["P.I. Pavlik", "N. Presson", "J.R. Anderson"], "venue": "Proceedings of the Eighth International Conference of Cognitive Modeling", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "The Adaptive Character of Thought - Rational (ACT-R) is a theory of cognition developed principally by John Anderson at Carnegie-Mellon University [4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "With ACT* [2], Anderson endeavored to describe the human memory and reasoning faculties mechanistically \u2013 i.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "It is difficult to infer from [4] the exact process by which the above rational decision is made.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "Zt is defined as (see [4], p.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "62, and [3], p.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "Note that the probability of a production with value x occurring decreases with time; this was intended by Anderson to model the idea that higher-valued productions should match earlier than lower-valued productions [4].", "startOffset": 216, "endOffset": 219}, {"referenceID": 3, "context": "Under ACT-R, Z is dependent on t in order to \u201callow for the possibility that later instantiations are less likely to be as good as earlier instantiations\u201d ([4], p.", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "V (V \u2217 \u2212 V )Zt(V )dV \u2217 \u2264 \u03c4 (Note that G is the maximum value a production could have since p \u2208 [0, 1] and C > 0.", "startOffset": 95, "endOffset": 101}, {"referenceID": 3, "context": "Very little is said about how ACT-R models the initial creation of declarative memory: Either a new chunk is \u201ccreated as the encoding of external events\u201d ([4], p.", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "\u201d This is known as proceduralization and is a form of compilation under ACT-R [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "where B(t) is the base activation of the knowledge chunk at time t, and the summation is the associative strength of the chunk dependent on related chunks, defined as \u201cthe elements in the current goal chunk and the elements currently being processed in the perceptual field\u201d ([4], p.", "startOffset": 276, "endOffset": 279}, {"referenceID": 0, "context": "Each wj \u2208 [0, 1] \u201creflects the salience or validity of [related] element j.", "startOffset": 10, "endOffset": 16}, {"referenceID": 3, "context": "The decay rate d is defined in the ACT-R literature in various ways: \u2022 In [4], the decay rate d is a constant for all k.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "\u2022 In [6], the decay rate of activation for the particular learning event k is defined as dk = max{d1, b(tk\u22121\u2212 tk) \u2212d1}4 where d1 (a constant) is the decay rate of the first learning event for the chunk, and b is a constant.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "\u2022 In [8], dk = cek\u22121 + \u03b1, where c and \u03b1 are constants, and mk\u22121 is the activation of the chunk at the time of the previous learning event (k \u2212 1).", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "3In fact, he defines activation in two conflicting ways: in [4], p.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "64, chunk activation is the log-odds that the chunk matches a production that fires; in [4], p.", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "4In [6], the base was actually tk \u2212 tk\u22121, but since older events have longer decay times t, this would result in a negative base, and hence a complex number for the decay, which was presumably unintended.", "startOffset": 4, "endOffset": 7}, {"referenceID": 3, "context": "The notion of context in ACT-R is not clearly defined, but it is described roughly as \u201cthe elements in the current goal chunk and the elements currently being processed in the perceptual field\u201d ([4], p.", "startOffset": 195, "endOffset": 198}, {"referenceID": 0, "context": "The wj are barely defined at all, except that each wj \u2208 [0, 1], and that it \u201creflects the salience or validity of [related] element j.", "startOffset": 56, "endOffset": 62}, {"referenceID": 6, "context": "\u201d [7] suggests that the wj express \u201cattentional weighting, i.", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "\u201d The sij \u2208 [\u2212\u221e,+\u221e] are \u201cthe strengths of association to i from elements j in the current context\u201d ([4], p.", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "sij = log p(Cj | Ni) p(Cj | Ni) \u2248 log p(Cj | Ni) p(Cj) = log p(Ni | Cj) p(Ni) The justification for this approximation given in [4] is that knowing the outcome ofNi cannot substantially affect the probability of event Ni since there are presumably very many chunks in all of declarative memory.", "startOffset": 128, "endOffset": 131}, {"referenceID": 3, "context": "In [4], a prior probability for both of these terms was also employed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "The first probability, which is called q in [4], can be estimated by updating a Beta distribution every time that a production either succeeds or fails.", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "As stated by the author himself in [3], the proposed methods of estimating p and C are not optimal, but rather only plausible.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "In [4], S is supposed to be the log-odds that the production rule will fire, and it is defined as: S(t) = ln \u2211", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "where the k indexes events in which the production fires ([4], p.", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "(It is unclear from [4] if the B in 5 is the same B as in 4.", "startOffset": 20, "endOffset": 23}, {"referenceID": 8, "context": "One must also consider the latency of a knowledge chunk in isolation from any particular production, as in [9] \u2013 in the ACT-R", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "definition in [4], however, latency is only explicitly defined for the instantiation of a production rule (matching of its pre-condition to chunks); hence, strictly speaking, this too is an approximation.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "(The proof below is adapted from [5]).", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "The latency of recall of a particular chunk as defined in [9] is: L(t) = Fe + C", "startOffset": 58, "endOffset": 61}], "year": 2013, "abstractText": "The ACT-R theory of cognition developed by John Anderson and colleagues endeavors to explain how humans recall chunks of information and how they solve problems. ACT-R also serves as a theoretical basis for \u201ccognitive tutors\u201d, i.e., automatic tutoring systems that help students learn mathematics, computer programming, and other subjects. The official ACT-R definition is distributed across a large body of literature spanning many articles and monographs, and hence it is difficult for an \u201coutsider\u201d to learn the most important aspects of the theory. This paper aims to provide a tutorial to the core components of the ACT-R theory.", "creator": "LaTeX with hyperref package"}}}