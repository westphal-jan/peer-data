{"id": "1306.1185", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2013", "title": "Multiclass Total Variation Clustering", "abstract": "ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely generally on the concept of total effect variation. while these algorithms perform well for bi - partitioning tasks, then their recursive extensions yield unimpressive statistical results for multiclass data clustering analysis tasks. this paper presents a general framework for multiclass total variation clustering that does not rely on recursion. evidently the results thereby greatly outperform previous total variation algorithms models and compare well with state - product of - the - art nmf approaches.", "histories": [["v1", "Wed, 5 Jun 2013 17:42:57 GMT  (47kb,D)", "http://arxiv.org/abs/1306.1185v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC", "authors": ["xavier bresson", "thomas laurent 0001", "david uminsky", "james h von brecht"], "accepted": true, "id": "1306.1185"}, "pdf": {"name": "1306.1185.pdf", "metadata": {"source": "CRF", "title": "Multiclass Total Variation Clustering", "authors": ["Xavier Bresson", "Thomas Laurent", "David Uminsky", "James H. von Brecht"], "emails": ["(xbresson@cityu.edu.hk).", "(laurent@math.ucr.edu)", "(duminsky@usfca.edu)", "(jub@math.ucla.edu)"], "sections": [{"heading": "1 Introduction", "text": "Many clustering models rely on the minimization of an energy over possible partitions of the data set. These discrete optimizations usually pose NP-hard problems, however. A natural resolution of this issue involves relaxing the discrete minimization space into a continuous one to obtain an easier minimization procedure. Many current algorithms, such as spectral clustering methods or non-negative matrix factorization (NMF) methods, follow this relaxation approach.\nA fundamental problem arises when using this approach, however; in general the solution of the relaxed continuous problem and that of the discrete NP-hard problem can differ substantially. In other words, the relaxation is too loose. A tight relaxation, on the other hand, has a solution that closely matches the solution of the original discrete NP-hard problem. Ideas from the image processing literature have recently motivated a new set of algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those used by NMF and spectral clustering. These new algorithms all rely on the concept of total variation. Total variation techniques promote the formation of sharp indicator functions in the continuous relaxation. These functions equal one on a subset of the graph, zero elsewhere and exhibit a non-smooth jump between these two regions. In contrast to the relaxations employed by spectral clustering and NMF, total variation techniques therefore lead to quasi-discrete solutions that closely resemble the discrete solution of the original NP-hard problem. They provide a promising set of clustering tools for precisely this reason.\n\u2217Department of Computer Science, City University of Hong Kong, Hong Kong (xbresson@cityu.edu.hk). \u2020Department of Mathematics, University of California Riverside, Riverside CA 92521 (laurent@math.ucr.edu) \u2021Department of Mathematics, University of San Francisco, San Francisco, CA 94117 (duminsky@usfca.edu) \u00a7Department of Mathematics, University of California Los Angeles, Los Angeles CA 90095 (jub@math.ucla.edu)\nar X\niv :1\n30 6.\n11 85\nv1 [\nst at\n.M L\n] 5\nJ un\n2 01\nPrevious total variation algorithms obtain excellent results for two class partitioning problems [18, 11, 12, 3] . Until now, total variation techniques have relied upon a recursive bi-partitioning procedure to handle more than two classes. Unfortunately, these recursive extensions have yet to produce state-of-the art results. This paper presents a general framework for multiclass total variation clustering that does not rely on a recursive procedure. Specifically, we introduce a new discrete multiclass clustering model, its corresponding continuous relaxation and a new algorithm for optimizing the relaxation. Our approach also easily adapts to handle either unsupervised or transductive clustering tasks. The results significantly outperform previous total variation algorithms and compare well against state-of-the-art approaches [19, 20, 1]. We name our approach Multiclass Total Variation clustering (MTV-clustering)."}, {"heading": "2 The Multiclass Balanced-Cut Model", "text": "Given a weighted graph G = (V,W ) we let V = {x1, . . . ,xN} denote the vertex set and W := {wi,j}1\u2264i,j\u2264N denote the non-negative, symmetric similarity matrix. Each entry wij of W encodes the similarity, or lack thereof, between a pair of vertices. The classical balanced-cut (or, Cheeger cut) [7, 8] asks for a partition of V = A \u222a Ac into two disjoint sets that minimizes the set energy\nBal(A) := Cut(A,Ac)\nmin{|A|, |Ac|} =\n\u2211 xi\u2208A,xj\u2208Ac wij\nmin{|A|, |Ac|} . (1)\nA simple rationale motivates this model: clusters should exhibit similarity between data points, which is reflected by small values of Cut(A,Ac), and also form an approximately equal sized partition of the vertex set. Note that min{|A|, |Ac|} attains its maximum when |A| = |Ac| = N/2, so that for a given value of Cut(A,Ac) the minimum occurs when A and Ac have approximately equal size.\nWe generalize this model to the multiclass setting by pursuing the same rationale. For a given number of classes R we formulate our generalized balanced-cut problem as\nMinimize R\u2211 r=1\nCut(Ar, A c r)\nmin{\u03bb|Ar|, |Acr|}\nover all disjoint partitions Ar \u2229As = \u2205, A1 \u222a \u00b7 \u00b7 \u00b7 \u222aAR = V of the vertex set.  (P) In this model the parameter \u03bb controls the sizes of the sets Ar in the partition. Previous work [4] has used \u03bb = 1 to obtain a multiclass energy by a straightforward sum of the twoclass balanced-cut terms (1). While this follows the usual practice, it erroneously attempts to enforce that each set in the partition occupy half of the total number of vertices in the graph. We instead select the parameter \u03bb to ensure that each of the classes approximately occupy the appropriate fraction 1/R of the total number of vertices. As the maximum of min{\u03bb|Ar|, |Acr|} occurs when \u03bb|Ar| = |Acr| = N \u2212 |Ar|, we see that \u03bb = R\u2212 1 is the proper choice.\nThis general framework also easily incorporates a priori known information, such as a set of labels for transductive learning. If Lr \u2282 V denotes a set of data points that are a priori known to belong to class r then we simply enforce Lr \u2282 Ar in the definition of an allowable partition of the vertex set. In other words, any allowable disjoint partition Ar \u2229As = \u2205, A1 \u222a \u00b7 \u00b7 \u00b7 \u222aAR = V must also respect the given set of labels."}, {"heading": "3 Total Variation and a Tight Continuous Relaxation", "text": "We derive our continuous optimization by relaxing the set energy (P) to the continuous energy\nE(F ) = R\u2211 r=1 \u2016fr\u2016TV \u2016fr \u2212med\u03bb(fr)\u2016 1,\u03bb . (2)\nHere F := [f1, . . . , fR] \u2208 MN\u00d7R([0, 1]) denotes the N \u00d7 R matrix that contains in its columns the relaxed optimization variables associated to the R clusters. A few definitions will help clarify the meaning of this formula. The total variation \u2016f\u2016TV of a vertex function f : V \u2192 R is defined by:\n\u2016f\u2016TV = n\u2211 i=1 wij |f(xi)\u2212 f(xj)|. (3)\nAlternatively, if we view a vertex function f as a vector (f(x1), . . . , f(xN )) t \u2208 RN then we can write \u2016f\u2016TV := \u2016Kf\u20161. (4)\nHere K \u2208MM\u00d7N (R) denotes the gradient matrix of a graph with M edges and N vertices. Each row of K corresponds to an edge and each column corresponds to a vertex. For any edge (i, j) in the graph the corresponding row in the matrix K has an entry wij in the column corresponding to the ith vertex, an entry \u2212wij in the column corresponding to the jth vertex and zeros otherwise.\nTo make sense of the remainder of (2) we must introduce the asymmetric `1-norm, which\n\u2016f\u20161,\u03bb = n\u2211 i=1 |f(xi)|\u03bb where |t|\u03bb = { \u03bbt if t \u2265 0 \u2212t if t < 0.\n(5)\nFinally we define the \u03bb-median (or quantile), denoted med\u03bb(f), as:\nmed\u03bb(f) = the (k + 1) st largest value in the range of f , where k = bN/(\u03bb+ 1)c. (6)\nThese definitions, as well as the relaxation (2) itself, were motivated by the following theorem. Its proof, found in the Appendix, relies only on using the three preceeding definitions and some simple algebra.\nTheorem 1. If f = 1A is the indicator function of a subset A \u2282 V then\n\u2016f\u2016TV \u2016f \u2212med\u03bb(f)\u2016 1,\u03bb = 2 Cut(A,Ac) min {\u03bb|A|, |Ac|} .\nThe preceding theorem allows us to restate the original set optimization problem (P) in the equivalent discrete form\nMinimize R\u2211 r=1 \u2016fr\u2016TV \u2016fr \u2212med\u03bb(fr)\u2016 1,\u03bb\nover non-zero functions f1, . . . , fR : V \u2192 {0, 1} such that f1 + . . .+ fR = 1V .  (P\u2019) Indeed, since the non-zero functions fr can take only two values, zero or one, they must define indicator functions of some nonempty set. The simplex constraint f1 + . . .+ fR = 1V\nthen guarantees that the sets Ar := {xi \u2208 V : fr(xi) = 1} form a partition of the vertex set. We obtain the relaxed version (P-rlx) of (P\u2019) in the usual manner by allowing fr \u2208 [0, 1] to have a continuous range. This yields\nMinimize R\u2211 r=1 \u2016fr\u2016TV \u2016fr \u2212med\u03bb(fr)\u2016 1,\u03bb\nover functions f1, . . . , fR : V \u2192 [0, 1] such that f1 + . . .+ fR = 1V .  (P-rlx) The following two points form the foundation on which total variation clustering relies: 1 \u2014 As the next subsection details, the total variation terms give rise to quasi-indicator functions. That is, the relaxed solutions [f1, . . . , fR] of (P-rlx) mostly take values near zero or one and exhibit a sharp, non-smooth transition between these two regions. Since these quasi-indicator functions essentially take values in the discrete set {0, 1} rather than the continuous interval [0, 1], solving (P-rlx) is almost equivalent to solving either (P) or (P\u2019). In other words, (P-rlx) is a tight relaxation of (P).\n2 \u2014 Both functions f 7\u2192 \u2016f\u2016TV and f 7\u2192 \u2016f \u2212 med\u03bb(f)\u20161,\u03bb are convex. The simplex constraint in (P-rlx) is also convex. Therefore solving (P-rlx) amounts to minimizing a sum of ratios of convex functions with convex constraints. As the next section details, this fact allows us to use machinery from convex analysis to develop an efficient, novel algorithm for such problems."}, {"heading": "3.1 The Role of Total Variation in the Formation of Quasi-Indicator Functions", "text": "To elucidate the precise role that the total variation itself plays in the formation of quasiindicator functions, it proves useful to consider a version of (P-rlx) that uses a spectral relaxation in place of the total variation:\nMinimize R\u2211 r=1 \u2016fr\u2016Lap \u2016fr \u2212med\u03bb(fr)\u2016 1,\u03bb\nover functions f1, . . . , fR : V \u2192 [0, 1] such that f1 + . . .+ fR = 1V  (P-rlx2) Here \u2016f\u20162Lap = \u2211n i=1 wij |f(xi) \u2212 f(xj)|2 denotes the spectral relaxation of Cut(A,Ac); it equals \u3008f, Lf\u3009 if L denotes the unnormalized graph Laplacian matrix. Thus problem (Prlx2) relates to spectral clustering (and therefore NMF [9]) with a positivity constraint. Note that the only difference between (P-rlx2) and (P-rlx) is that the exponent 2 appears in \u2016 \u00b7 \u2016Lap while the exponent 1 appears in the total variation. This simple difference of exponent has an important consequence for the tightness of the relaxations. Figure 1 presents a simple example that illuminates this difference. If we bi-partition the depicted graph, i.e. a line with 20 vertices and edge weights wij = 1, then the optimal cut lies between vertex 10 and vertex 11 since this gives a perfectly balanced cut. Figure 1(a) shows the vertex function f1 generated by (P-rlx) while figure 1(b) shows the one generated by (P-rlx2). Observe that the solution of the total variation model coincides with the indicator function of the desired cut whereas the the spectral model prefers its smoothed version. Note that both functions in figure 1a) and 1b) have exactly the same total variation \u2016f\u2016TV = |f(x1)\u2212f(x2)|+ \u00b7 \u00b7 \u00b7+ |f(x19)\u2212f(x20)| = f(x1)\u2212f(x20) = 1 since both functions are monotonic. The total variation model will therefore prefer the sharp indicator function\nsince it differs more from its \u03bb-median than the smooth indicator function. Indeed, the denominator \u2016fr \u2212 med\u03bb(fr)\u20161,\u03bb is larger for the sharp indicator function than for the smooth one. A different scenario occurs when we replace the exponent one in \u2016 \u00b7 \u2016TV by an exponent two, however. As \u2016f\u20162Lap = |f(x1)\u2212 f(x2)|2 + \u00b7 \u00b7 \u00b7+ |f(x19)\u2212 f(x20)|2 and t2 < t when t < 1 it follows that\u2016f\u2016Lap is much smaller for the smooth function than for the sharp one. Thus the spectral model will prefer the smooth indicator function despite the fact that it differs less from its \u03bb-median. We therefore recognize the total variation as the driving force behind the formation of sharp indicator functions.\nThis heuristic explanation on a simple, two-class example generalizes to the multiclass case and to real data sets (see figure 2). In simple terms, quasi-indicator functions arise due to the fact that the total variation of a sharp indicator function equals the total variation of a smoothed version of the same indicator function. The denominator \u2016fr\u2212med\u03bb(fr)\u20161,\u03bb then measures the deviation of these functions from their \u03bb-median. A sharp indicator function deviates more from its median than does its smoothed version since most of its values concentrate around zero and one. The energy is therefore much smaller for a sharp indicator function than for a smooth indicator function, and consequently the total variation clustering energy always prefers sharp indicator functions to smooth ones. For bi-partitioning problems this fact is well-known. Several previous works have proven that the relaxation is exact in the two-class case; that is, the total variation solution coincides with the solution of the original NP-hard problem [8, 18, 3, 5].\nFigure 2 illustrates the result of the difference between total variation and NMF relaxations on the data set OPTDIGITS, which contains 5620 images of handwritten numerical digits. Figure 2(a) shows the quasi-indicator function f4 obtained by our MTV algorithm while 2(b) shows the function f4 obtained from the NMF algorithm of [1]. We extract the portion of each function corresponding to the digits four and nine, then sort and plot the result. The MTV relaxation leads a sharp transition between the fours and the nines while the NMF relaxation leads to a smooth transition."}, {"heading": "3.2 Transductive Framework", "text": "From a modeling point-of-view, the presence of transductive labels poses no additional difficulty. In addition to the simplex constraint\nF \u2208 \u03a3 := { F \u2208MN\u00d7R([0, 1]) : fr(xi) \u2265 0,\nR\u2211 r=1 fr(xi) = 1\n} (7)\nrequired for unsupervised clustering we also impose the set of labels as a hard constraint. If L1, . . . , LR denote the R vertex subsets representing the labeled points, so that xi \u2208 Lr means xi belongs to class r, then we may enforce these labels by restricting F to lie in the subset\nF \u2208 \u039b := {F \u2208MN\u00d7R([0, 1]) : \u2200r, (f1(xi), . . . , fR(xi)) = er \u2200 xi \u2208 Lr } . (8)\nHere er denotes the row vector containing a one in the r th location and zeros elsewhere. Our model for transductive classification then aims to solve the problem\nMinimize R\u2211 r=1 \u2016fr\u2016TV \u2016fr \u2212med\u03bb(fr)\u2016 1,\u03bb over matrices F \u2208 \u03a3 \u2229 \u039b.\n} (P-trans)\nNote that \u03a3 \u2229 \u039b also defines a convex set, so this minimization remains a sum of ratios of convex functions subject to a convex constraint. Transductive classification therefore poses no additional algorithmic difficulty, either. In particular, we may use the proximal splitting algorithm detailed in the next section for both unsupervised and transductive classification tasks."}, {"heading": "4 Proximal Splitting Algorithm", "text": "This section details our proximal splitting algorithm for finding local minimizers of a sum of ratios of convex functions subject to a convex constraint. We start by showing in the first subsection that the functions\nT (f) := \u2016f\u2016TV and B(f) := \u2016f \u2212med\u03bb(f)1\u20161,\u03bb (9)\ninvolved in (P-rlx) or (P-trans) are indeed convex. We also give an explicit formula for a subdifferential of B since our proximal splitting algorithm requires this in explicit form. We then summarize a few properties of proximal operators before presenting the algorithm."}, {"heading": "4.1 Convexity, Subgradients and Proximal Operators", "text": "Recall that we may view each function f : V \u2192 R as a vector in RN with f(xi) as the ith component of the vector. We may then view T and B as functions from RN to R. The next theorem states that both B and T define convex functions on RN and furnishes an element v \u2208 \u2202B(f) by means of an easily computable formula. The formula for the subdifferential generalizes a related result for the symmetric case [11] to the asymmetric setting.\nTheorem 2. The functions B and T are convex. Moreover, given f \u2208 RN the vector v \u2208 RN defined by\nv(xi) =  \u03bb if f(xi) > med\u03bb(f) n\u2212\u2212\u03bbn+ n0 if f(xi) = med\u03bb(f)\n\u22121 if f(xi) < med\u03bb(f) where  n0 = |{xi \u2208 V : f(xi) = med\u03bb(f)}| n\u2212 = |{xi \u2208 V : f(xi) < med\u03bb(f)}| n+ = |{xi \u2208 V : f(xi) > med\u03bb(f)}|\nbelongs to \u2202B(f).\nIn the above theorem \u2202B(f) denotes the subdifferential of B at f and v \u2208 \u2202B(f) denotes a subgradient. The proof of Theroem 2 can be found in the Appendix. Given a convex function A : RN \u2192 R, the proximal operator of A is defined by\nproxA(g) := argmin f\u2208RN\nA(f) + 1\n2 ||f \u2212 g||22. (10)\nIf we let \u03b4C denote the barrier function of the convex set C,\n\u03b4C(f) := { 0 if f \u2208 C +\u221e if f /\u2208 C,\n(11)\nthen we easily see that prox\u03b4C is simply the least-squares projection on C:\nprox\u03b4C (f) = projC(f) := argmin g\u2208C\n1 2 ||f \u2212 g||22. (12)\nIn this manner the proximal operator defines a mapping from RN to RN that generalizes the least-squares projection onto a convex set."}, {"heading": "4.2 The Algorithm", "text": "We can rewrite the problem (P-rlx) or (P-trans) as\nMinimize \u03b4C(F ) + R\u2211 r=1 E(fr) over all matrices F = [f1, . . . , fr] \u2208MN\u00d7R (13)\nwhere E(fr) = T (fr)/B(fr) denotes the energy of the quasi-indicator function of the r th cluster. The set C = \u03a3 or C = \u03a3 \u2229 \u039b is the convex subset of MN\u00d7R that encodes the simplex constraint (7) or the simplex constraint with labels. The corresponding function \u03b4C(F ), defined in (11), is the barrier function of the desired set. Beginning from an initial iterate F 0 \u2208 C we propose the following proximal splitting algorithm:\nF k+1 := proxT k+\u03b4C (F k + \u2202Bk(F k)). (14)\nHere T k(F ) and Bk(F ) denote the convex functions\nT k(F ) := R\u2211 r=1 ckr T (fr) Bk(F ) := R\u2211 r=1 dkr B(fr),\nthe constants (ckr , d k r ) are computed using the previous iterate\nckr = \u2206k\nB(fkr ) and dkr =\n\u2206kE(fkr )\nB(fkr )\nand \u2206k denotes the timestep for the current iteration. This choice of the constants (ckr , d k r ) yields Bk(F k) = T k(F k), and this fundamental property allows us to derive the energy descent estimate:\nTheorem 3 (Estimate of the energy descent). Each of the F k belongs to C, and if Bkr 6= 0 then\nR\u2211 r=1 Bk+1r Bkr ( Ekr \u2212 Ek+1r ) \u2265 \u2016F k \u2212 F k+1\u20162 \u2206k (15)\nwhere Bkr , E k r stand for B(f k r ), E(f k r ).\nInequality (18) states that the energies of the quasi-indicator functions (as a weighted sum) decrease at every step of the algorithm. It also gives a lower bound for how much these energies decrease. As the algorithm progress and the iterates stabilize the ratio Bk+1r /B k r converges to 1, in which case the sum, rather than a weighted sum, of the individual cluster energies decreases. The proof of Theorem 3 can be found in the Appendix.\nOur proximal splitting algorithm (14) requires two steps. The first step requires computing Gk = F k + \u2202Bk(F k), and this is straightforward since theorem 5 provides the subdifferential of B, and therefore of Bk, through an explicit formula. The second step requires computing proxT k+\u03b4C (G\nk), which seems daunting at a first glance. Fortunately, minimization problems of this form play an important role in the image processing literature. Recent years have therefore produced several fast and accurate algorithms for computing the proximal operator of the total variation. As T k+\u03b4C consists of a weighted sum of total variation terms subject to a convex constraint, we can readily adapt these algorithms to compute the second step of our algorithm efficiently. In this work we use the primal-dual algorithm of [6] with acceleration. This relies on a proper uniformly convex formulation of the proximal minimization, which we detail completely in the Appendix.\nThe primal-dual algorithm we use to compute proxT k+\u03b4C (G k) produces a sequence of approximate solutions by means of an iterative procedure. A stopping criterion is therefore needed to indicate when the current iterate approximates the actual solution proxT k+\u03b4C (G\nk) sufficiently. Ideally, we would like to terminate F k+1 \u2248 proxT k+\u03b4C (G\nk) in such a manner so that the energy descent property (18) still holds and F k+1 always satisfies the required constraints. In theory we cannot guarantee that the energy estimate holds for an inexact solution. We may note, however, that a slightly weaker version of the energy estimate (18)\nR\u2211 r=1 Bk+1r Bkr ( Ekr \u2212 Ek+1r ) \u2265 (1\u2212 )\u2016F k \u2212 F k+1\u20162F \u2206k\n(16)\nholds after a finite number of iterations of the inner minimization. Moreover, this weaker version still guarantees that the energies of the quasi-indicator functions decrease as a weighted\nsum in exactly the same manner as before. In this way we can terminate the inner loop adaptively: we solve F k+1 \u2248 proxT k+\u03b4C (G\nk) less precisely when F k+1 lies far from a minimum and more precisely as the sequence {F k} progresses. Overall this leads to a substantial increase in efficiency of the full algorithm.\nOur implementation of the proximal splitting algorithm also guarantees that F k+1 always satisfies the required constraints. We accomplish this task by implementing the primal-dual algorithm in such a way that each inner iteration always satisfies the constraints. This requires computing the projection projC(F ) exactly at each inner iteration. The overall algorithm remains efficient provided we can compute this projection quickly. When C = \u03a3 the algorithm [14] performs the required projection in at most R steps. When C = \u03a3 \u2229 \u039b the computational effort actually decreases, since in this case the projection consists of a simplex projection on the unlabeled points and straightforward assignment on the labeled points.\nWe may now summarize the full algorithm, including the proximal operator computation. In practice we find the choices \u2206k = max{Bk1 , . . . , BkR} and any small work well, so we present the algorithm with these choices. Recall the matrix K in (4) denotes the gradient matrix of the graph.\nAlgorithm 1 Proximal Splitting Algorithm\nInput: F \u2208 C,P = 0, L = ||K||2, \u03c4 = L\u22121, = 10\u22123 while loop not converged do\n//Perform outer step Gk = F k + \u2202Bk(F k) \u2206 = maxr B(fr) \u22060 = minr B(fr) \u03c3 = \u2206 2 0(\u03c4\u2206 2L2)\u22121 F\u0304 = F\nDE = diag [ E(f1) B(f1) , . . . , E(fR)B(fR) ] DB = diag [ \u2206 B(f1) , . . . , \u2206B(fR) ] V = \u2206[\u2202B(f1), . . . , \u2202B(fR)]DE (using theorem 5) G = F + V //Perform F k+1 \u2248 proxT k+\u03b4C (G\nk) until energy estimate holds while (16) fails do P\u0303 = P + \u03c3KF\u0304DB P = P\u0303 /max{|P\u0303 |, 1} (both operations entriwise) Fold = F F\u0303 = F \u2212 \u03c4KtPDB F = (F\u0303 + \u03c4G)/(1 + \u03c4) F = projC(F ) \u03b8 = 1/ \u221a 1 + 2\u03c4 \u03c4 = \u03b8\u03c4 \u03c3 = \u03c3/\u03b8 F\u0304 = (1 + \u03b8)F \u2212 \u03b8Fold\nend while end while"}, {"heading": "5 Numerical Experiments", "text": "We now demonstrate the MTV algorithm for unsupervised and transductive clustering tasks. We selected six standard, large-scale data sets as a basis of comparison. We obtained the first data set (4MOONS) and its similarity matrix from [4] and the remaining five data sets and matrices (WEBKB4, OPTDIGITS, PENDIGITS, 20NEWS, MNIST) from [19]. The 4MOONS data set contains 4K points while the remaining five contain 4.2K, 5.6K, 11K, 20K and 70K points, respectively.\nOur first set of experiments compares our MTV algorithm against other unsupervised approaches. We compare against two previous total variation algorithms [11, 3], which rely on recursive bi-partitioning, and two top NMF algorithms [1, 19]. We use the normalized Cheeger cut versions of [11] and [3] with default parameters. We used the code available\nfrom [19] to test each NMF algorithm. All non-recursive algorithms (LSD [1], NMFR [19], MTV) received two types of initial data: (a) the deterministic data used in [19]; (b) a random procedure leveraging normalized-cut [16]. Procedure (b) first selects one data point uniformly at random from each computed NCut cluster, then sets fr equal to one at the data point drawn from the rth cluster and zero otherwise. We then propagate this initial stage by replacing each fr with (I + L)\n\u22121fr where L denotes the unnormalized graph Laplacian. Finally, to aid the NMF algorithms, we add a small constant 0.2 to the result (each performed better than without adding this constant). For MTV we use (a) and 30 random trials of (b) then report the cluster purity of the solution with the lowest discrete energy (P). We then use each NMF with exactly the same initial conditions and report simply the highest purity achieved over all 31 runs. This biases the results in favor of the NMF algorithms. Due to the non-convex nature of these algorithms, the random initialization gave the best results and significantly improved on previously reported results of LSD in particular. We allowed each non-recursive algorithm 10000 iterations using initial condition (a) while each trial of (b) performed 2000 iterations. The following table reports the results. Our next\nAlg/Data 4MOONS WEBKB4 OPTDIGITS PENDIGITS 20NEWS MNIST NCC-TV [3] 88.75 51.76 95.91 73.25 23.20 88.80 1SPEC [11] 73.92 39.68 88.65 82.42 11.49 88.17\nLSD [1] 99.40 54.50 97.94 88.44 41.25 95.67 NMFR [19] 77.80 64.32 97.92 91.21 63.93 96.99\nMTV 99.53 59.15 98.29 89.06 39.40 97.60\nset of experiments demonstrate our algorithm in a transductive setting. For each data set we randomly sample either one label per class or a percentage of labels per class from the ground truth. We then run ten trials of initial condition (b) (propagating all labels instead of one) and report the purity of the lowest energy solution as before along with the average computational time (for simple MATLAB code running on a standard desktop) of the ten runs. We terminate the algorithm once the relative change in energy falls below 10\u22124 between outer steps of algorithm 1. The table below reports the results. Note that for well-constructed graphs (such as MNIST), our algorithm performs remarkably well with only one label per class.\nLabels 4MOONS WEBKB4 OPTDIGITS PENDIGITS 20NEWS MNIST 1 99.55/ 3.0s 56.58/ 1.8s 98.29/ 7s 89.17/ 14s 50.07/ 52s 97.53/ 98s\n1% 99.55/ 3.1s 58.75/ 2.0s 98.29/ 4s 93.73/ 9s 61.70/ 54s 97.59/ 54s 2.5% 99.55/ 1.9s 57.01/ 1.7s 98.35/ 3s 95.83/ 7s 67.61/ 42s 97.72/ 39s 5% 99.53/ 1.2s 58.34/ 1.3s 98.38/ 2s 97.98/ 5s 70.51/ 32s 97.79/ 31s 10% 99.55/ 0.8s 62.01/ 1.2s 98.45/ 2s 98.22/ 4s 73.97/ 25s 98.05/ 25s\nOur non-recursive MTV algorithm vastly outperforms the two previous recursive total variation approaches and also comparse well with state-of-the-art NMF approaches. Each of MTV, LSD and NMFR perform well on manifold data sets such as MNIST but NMFR tends to perform best on noisy, non-manifold data sets. This results from the fact that NMFR uses a costly graph smoothing technique while our algorithm and LSD do not. We plan to incorporate such improvements into the total variation framework in future work. Lastly, we found procedure (b) can help overcome the lack of convexity inherent in many clustering approaches. We plan to pursue a more principled and efficient initialization along\nthese lines in the future as well. Overall, our total variation framework therefore presents a promising alternative to NMF methods due to its strong mathematical foundation and tight relaxation."}, {"heading": "6 Appendix", "text": ""}, {"heading": "6.1 Proofs of Theorems", "text": "Theorem 4. If f = 1A is the indicator function of a subset A \u2282 V then\n\u2016f\u2016TV \u2016f \u2212med\u03bb(f)\u2016 1,\u03bb = 2 Cut(A,Ac) min {\u03bb|A|, |Ac|} .\nProof. The fact that \u2016f\u2016TV = 2 Cut(A,Ac) follows directly from the definition of the total variation. Indeed, a straightforward computation shows\n\u2016f\u2016TV = \u2211 xi\u2208A N\u2211 j=1 wij |1\u2212 f(xj)|+ \u2211 xi\u2208Ac N\u2211 j=1 wij |f(xj)| = \u2211 xi\u2208A \u2211 xj\u2208Ac wij + \u2211 xi\u2208Ac \u2211 xj\u2208A wij .\nThus \u2016f\u2016TV = 2 Cut(A,Ac) as W is symmetric. Let B(f) := \u2016f \u2212med\u03bb(f)\u20161,\u03bb. To show that B(f) = min {\u03bb|A|, |Ac|}, suppose first that \u03bb|A| \u2264 |Ac|. This inequality implies \u03bb|A| \u2264 N \u2212 |A|, or equivalently that |A| \u2264 N/(1 + \u03bb). Thus |A| \u2264 k := bN/(1 + \u03bb)c, and since f = 1A for |A| \u2264 k it follows immediately that the (k + 1)st largest entry in the range of f equals zero. Thus med\u03bb(f) = 0 by definition. A direct computation then yields that B(f) = \u2211 i\u2208V |f(xi)|\u03bb = \u03bb|A|. In the converse case, the fact that |Ac| < \u03bb|A| implies |A| > N/(1 + \u03bb) \u2265 k. Thus |A| \u2265 k + 1 and med\u03bb(f) = 1. Direct computation then shows that B(f) = \u2211 i\u2208V |f(xi)\u2212 1|\u03bb = |Ac| as claimed.\nLemma 1. Let h \u2208 RN and suppose v \u2208 RN satisfies\nv(xi) \u2208  \u03bb if h(xi) > 0\n[\u22121, \u03bb] if h(xi) = 0 \u22121 if h(xi) < 0.\n(17)\nThen v \u2208 \u2202\u2016h\u20161,\u03bb.\nProof. Note that |h(xi)|\u03bb = v(xi)h(xi) for each xi, so that for arbitrary g \u2208 RN and each xi the inequality\n|g(xi)|\u03bb \u2212 |h(xi)|\u03bb \u2265 v(xi) (g(xi)\u2212 h(xi))\nholds. Summing both sides over all xi \u2208 V then gives the claim.\nTheorem 5. The functions B and T are convex. Moreover, given f \u2208 RN the vector v \u2208 RN defined by\nv(xi) =  \u03bb if f(xi) > med\u03bb(f) n\u2212\u2212\u03bbn+ n0 if f(xi) = med\u03bb(f)\n\u22121 if f(xi) < med\u03bb(f) where  n0 = |{xi \u2208 V : f(xi) = med\u03bb(f)}| n\u2212 = |{xi \u2208 V : f(xi) < med\u03bb(f)}| n+ = |{xi \u2208 V : f(xi) > med\u03bb(f)}|\nbelongs to \u2202B(f).\nProof. The convexity of T (f) follows directly from its definition and a straightforward computation using the definition of convexity. Due to the continuity B(f), to show convexity it suffices to establish the existence of a subdifferential at every point.\nTo this end note that med\u03bb(f) \u2208 range(f), so that in particular n0 \u2265 1 by definition. Let 1 \u2264 k := bN/(1 + \u03bb)c < N denote that entry of f so that f(xk) = med\u03bb(f). By definition of med\u03bb(f) there exist at most k elements of f larger than med\u03bb(f), so that n\n+ \u2264 k \u2264 N/(1 + \u03bb). As N = n\u2212 + n0 + n+ this implies \u03bbn\n+\u2212n\u2212 n0 \u2264 1. Similarly there exist at most\nN \u2212 (k+1) elements of f smaller than med\u03bb(f), so that n\u2212 \u2264 N \u2212 (k+1) \u2264 N \u2212N/(1+\u03bb). The fact that N = n\u2212 + n0 + n+ then implies n\n\u2212\u2212\u03bbn+ n0 \u2264 \u03bb. Combining this with the\nprevious inequality yields \u22121 \u2264 n \u2212\u2212\u03bbn+ n0 \u2264 \u03bb.\nPut h := f \u2212med\u03bb(f)1, and note that the vector v defined above satisfies v \u2208 \u2202\u2016h\u20161,\u03bb by the preceeding lemma. Thus for any g \u2208 RN it holds that\n||g \u2212med\u03bb(g)1||1,\u03bb \u2212 ||f \u2212med\u03bb(f)1||1,\u03bb \u2265 \u3008v, g \u2212 f + (med\u03bb(f)\u2212med\u03bb(g))1\u3009\nby definition of the subdifferential. Note also that \u3008v,1\u3009 = 0, so that in fact\nB(g)\u2212B(f) = ||g \u2212med\u03bb(g)1||1,\u03bb \u2212 ||f \u2212med\u03bb(f)1||1,\u03bb \u2265 \u3008v, g \u2212 f\u3009\nfor g \u2208 RN arbitrary. Thus v \u2208 \u2202B(f) by definition of the subdifferential. In particular \u2202B(f) is always non-empty, so B(f) is convex.\nTheorem 6 (Estimate of the energy descent). Each of the F k belongs to C, and if Bkr 6= 0 then\nR\u2211 r=1 Bk+1r Bkr ( Ekr \u2212 Ek+1r ) \u2265 \u2016F k \u2212 F k+1\u20162 \u2206k (18)\nwhere Bkr , E k r stand for B(f k r ), E(f k r ).\nProof. Let V k \u2208 \u2202Bk(F k). Then by definition of the subdifferential it follows that\nBk(F k+1) \u2265 Bk(F k) + \u3008F k+1 \u2212 F k, V k\u3009. (19)\nAs F k+1 = proxT k+\u03b4C (F k+V k) the definition of the proximal operator implies that F k+1 \u2208 C and that also F k + V k \u2212 F k+1 \u2208 \u2202(T k + \u03b4C)(F k+1). The definition of the subdifferential and the fact that \u03b4C(F k) = \u03b4C(F\nk+1) = 0 then combine to imply\nT k(F k) \u2265 T k(F k+1) + \u3008F k \u2212 F k+1, F k + V k \u2212 F k+1\u3009 = T k(F k+1) + \u2016F k \u2212 F k+1\u20162 + \u3008F k \u2212 F k+1, V k\u3009 (20)\nAdding (19) and (20) yields\nT k(F k) + Bk(F k+1) \u2265 T k(F k+1) + Bk(F k) + \u2016F k \u2212 F k+1\u20162,\nor equivalently that Bk(F k+1) \u2265 T k(F k+1) + \u2016F k \u2212 F k+1\u20162 since Bk(F k) = T k(F k) by construction. Expanding this last inequality shows\nR\u2211 r=1 \u2206k Bkr ( EkrB k+1 r \u2212 T k+1r ) \u2265 \u2016F k \u2212 F k+1\u20162,\nwhich yields the claim after by Bk+1r in each term of the summation."}, {"heading": "6.2 Primal-Dual Formulation", "text": "Consider the minimization F k+1 := proxT k+\u03b4C (G k).\nWe may write this as the saddle-point problem\nmin u\u2208RNR max p\u2208RMR\n\u3008p,Ku\u3009+G(u)\u2212 F \u2217(p).\nHere the vector u = (f1, . . . , fR) t is a \u201cvectorized\u201d version of F and the matrix K denotes the block diagonal matrix\nK := blkdiag ( \u2206k\nBk1 K, . . . ,\n\u2206k BkR K ) where K is the gradient matrix of the graph. We define the convex function G(u) as\nG(u) := 1\n2 R\u2211 r=1 ||fr \u2212 gkr ||2 + \u03b4C(u),\nwhere \u03b4C denotes the barrier function of the convex set C (either the simplex or simplex with labels) as before. The convex function F \u2217(p) denotes the barrier function of the l\u221e unit ball, so that\nF \u2217(p) = { 0 if |pi| \u2264 1 \u2200 1 \u2264 i \u2264MR +\u221e otherwise.\nNote also that G(u) is uniformly convex, in that if v \u2208 \u2202G(u) denotes any subdifferential then for any u\u2032 \u2208 RNR the inequality\nG(u\u2032)\u2212G(u) \u2265 \u3008v, u\u2032 \u2212 u\u3009+ 1 2 ||u\u2212 u\u2032||2\nholds. We may therefore apply algorithm 2 of [6] with \u03b3 = 1 with to solve the saddle-point problem. This algorithm consists in the iterations\npn+1 = prox\u03c3nF\u2217(p n + \u03c3nKu\u0304n) un+1 = prox\u03c4nG(u n \u2212 \u03c4nKtpn+1)\n\u03b8n = 1\u221a\n1 + 2\u03c4n \u03c4n+1 = \u03b8n\u03c4n \u03c3n+1 = \u03c3n/\u03b8n\nu\u0304n+1 = un+1 + \u03b8n(un+1 \u2212 un)\nand converges provided the inequality \u03c30 \u2264 (\u03c40||K||22)\u22121 holds for the initial timesteps. We may compute the inner proximal operators analytically to find\n(prox\u03c3nF\u2217(z))i = zi/max{1, |zi|} \u2200 1 \u2264 i \u2264MR,\nand by completing the square that\nprox\u03c4nG(z) = projC\n( z + \u03c4ng\n1 + \u03c4n\n) ,\nwhere g = (gk1 , . . . , g k R) t denotes Gk in vectorized form. The inner loop of algorithm 1 then follows by re-writing these computations in matrix form."}], "references": [{"title": "Clustering by left-stochastic matrix factorization", "author": ["Raman Arora", "M Gupta", "Amol Kapila", "Maryam Fazel"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Diffuse Interface Models on Graphs for Classification of High Dimensional Data", "author": ["A. Bertozzi", "A. Flenner"], "venue": "Multiscale Modeling and Simulation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Convergence and energy landscape for cheeger cut clustering", "author": ["X. Bresson", "T. Laurent", "D. Uminsky", "J. von Brecht"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Multi-Class Transductive Learning based on ` Relaxations of Cheeger Cut and Mumford-Shah-Potts Model", "author": ["X. Bresson", "X.-C. Tai", "T.F. Chan", "A. Szlam"], "venue": "UCLA CAM Report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Spectral Clustering Based on the Graph p-Laplacian", "author": ["T. B\u00fchler", "M. Hein"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging", "author": ["A. Chambolle", "T. Pock"], "venue": "Journal of Mathematical Imaging and Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "A Lower Bound for the Smallest Eigenvalue of the Laplacian", "author": ["J. Cheeger"], "venue": "Problems in Analysis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1970}, {"title": "Spectral Graph Theory, volume 92 of CBMS Regional Conference Series in Mathematics", "author": ["F.R.K. Chung"], "venue": "Published for the Conference Board of the Mathematical Sciences,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "On the equivalence of nonnegative matrix factorization and spectral clustering", "author": ["Chris Ding", "Xiaofeng He", "Horst D Simon"], "venue": "In Proc. SIAM Data Mining Conf,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Fast multiclass segmentation using diffuse interface methods on", "author": ["C. Garcia-Cardona", "E. Merkurjev", "A.L. Bertozzi", "A. Flenner", "A.G. Percus"], "venue": "graphs. Submitted,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA", "author": ["M. Hein", "T. B\u00fchler"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Beyond Spectral Clustering - Tight Relaxations of Balanced Graph Cuts", "author": ["M. Hein", "S. Setzer"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "An mbo scheme on graphs for segmentation and image processing", "author": ["E. Merkurjev", "T. Kostic", "A. Bertozzi"], "venue": "UCLA CAM Report 12-46,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "A Finite Algorithm for Finding the Projection of a Point onto the Canonical Simplex of Rn", "author": ["C. Michelot"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1986}, {"title": "Constrained 1-Spectral Clustering", "author": ["S. Rangapuram", "M. Hein"], "venue": "In International conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Normalized Cuts and Image Segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "A total variation-based graph clustering algorithm for cheeger ratio cuts", "author": ["A. Szlam", "X. Bresson"], "venue": "UCLA CAM Report 09-68,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Total variation and cheeger cuts", "author": ["A. Szlam", "X. Bresson"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Clustering by nonnegative matrix factorization using graph random walk", "author": ["Zhirong Yang", "Tele Hao", "Onur Dikmen", "Xi Chen", "Erkki Oja"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Clustering by low-rank doubly stochastic matrix decomposition", "author": ["Zhirong Yang", "Erkki Oja"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}], "referenceMentions": [{"referenceID": 16, "context": "Ideas from the image processing literature have recently motivated a new set of algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those used by NMF and spectral clustering.", "startOffset": 91, "endOffset": 128}, {"referenceID": 17, "context": "Ideas from the image processing literature have recently motivated a new set of algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those used by NMF and spectral clustering.", "startOffset": 91, "endOffset": 128}, {"referenceID": 10, "context": "Ideas from the image processing literature have recently motivated a new set of algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those used by NMF and spectral clustering.", "startOffset": 91, "endOffset": 128}, {"referenceID": 11, "context": "Ideas from the image processing literature have recently motivated a new set of algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those used by NMF and spectral clustering.", "startOffset": 91, "endOffset": 128}, {"referenceID": 3, "context": "Ideas from the image processing literature have recently motivated a new set of algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those used by NMF and spectral clustering.", "startOffset": 91, "endOffset": 128}, {"referenceID": 14, "context": "Ideas from the image processing literature have recently motivated a new set of algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those used by NMF and spectral clustering.", "startOffset": 91, "endOffset": 128}, {"referenceID": 2, "context": "Ideas from the image processing literature have recently motivated a new set of algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those used by NMF and spectral clustering.", "startOffset": 91, "endOffset": 128}, {"referenceID": 1, "context": "Ideas from the image processing literature have recently motivated a new set of algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those used by NMF and spectral clustering.", "startOffset": 91, "endOffset": 128}, {"referenceID": 12, "context": "Ideas from the image processing literature have recently motivated a new set of algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those used by NMF and spectral clustering.", "startOffset": 91, "endOffset": 128}, {"referenceID": 9, "context": "Ideas from the image processing literature have recently motivated a new set of algorithms [17, 18, 11, 12, 4, 15, 3, 2, 13, 10] that can obtain tighter relaxations than those used by NMF and spectral clustering.", "startOffset": 91, "endOffset": 128}, {"referenceID": 17, "context": "Previous total variation algorithms obtain excellent results for two class partitioning problems [18, 11, 12, 3] .", "startOffset": 97, "endOffset": 112}, {"referenceID": 10, "context": "Previous total variation algorithms obtain excellent results for two class partitioning problems [18, 11, 12, 3] .", "startOffset": 97, "endOffset": 112}, {"referenceID": 11, "context": "Previous total variation algorithms obtain excellent results for two class partitioning problems [18, 11, 12, 3] .", "startOffset": 97, "endOffset": 112}, {"referenceID": 2, "context": "Previous total variation algorithms obtain excellent results for two class partitioning problems [18, 11, 12, 3] .", "startOffset": 97, "endOffset": 112}, {"referenceID": 18, "context": "The results significantly outperform previous total variation algorithms and compare well against state-of-the-art approaches [19, 20, 1].", "startOffset": 126, "endOffset": 137}, {"referenceID": 19, "context": "The results significantly outperform previous total variation algorithms and compare well against state-of-the-art approaches [19, 20, 1].", "startOffset": 126, "endOffset": 137}, {"referenceID": 0, "context": "The results significantly outperform previous total variation algorithms and compare well against state-of-the-art approaches [19, 20, 1].", "startOffset": 126, "endOffset": 137}, {"referenceID": 6, "context": "The classical balanced-cut (or, Cheeger cut) [7, 8] asks for a partition of V = A \u222a A into two disjoint sets that minimizes the set energy", "startOffset": 45, "endOffset": 51}, {"referenceID": 7, "context": "The classical balanced-cut (or, Cheeger cut) [7, 8] asks for a partition of V = A \u222a A into two disjoint sets that minimizes the set energy", "startOffset": 45, "endOffset": 51}, {"referenceID": 3, "context": "Previous work [4] has used \u03bb = 1 to obtain a multiclass energy by a straightforward sum of the twoclass balanced-cut terms (1).", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": ", fR] \u2208 MN\u00d7R([0, 1]) denotes the N \u00d7 R matrix that contains in its columns the relaxed optimization variables associated to the R clusters.", "startOffset": 13, "endOffset": 19}, {"referenceID": 0, "context": "We obtain the relaxed version (P-rlx) of (P\u2019) in the usual manner by allowing fr \u2208 [0, 1] to have a continuous range.", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": ", fR : V \u2192 [0, 1] such that f1 + .", "startOffset": 11, "endOffset": 17}, {"referenceID": 0, "context": "Since these quasi-indicator functions essentially take values in the discrete set {0, 1} rather than the continuous interval [0, 1], solving (P-rlx) is almost equivalent to solving either (P) or (P\u2019).", "startOffset": 125, "endOffset": 131}, {"referenceID": 0, "context": ", fR : V \u2192 [0, 1] such that f1 + .", "startOffset": 11, "endOffset": 17}, {"referenceID": 8, "context": "Thus problem (Prlx2) relates to spectral clustering (and therefore NMF [9]) with a positivity constraint.", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "Several previous works have proven that the relaxation is exact in the two-class case; that is, the total variation solution coincides with the solution of the original NP-hard problem [8, 18, 3, 5].", "startOffset": 185, "endOffset": 198}, {"referenceID": 17, "context": "Several previous works have proven that the relaxation is exact in the two-class case; that is, the total variation solution coincides with the solution of the original NP-hard problem [8, 18, 3, 5].", "startOffset": 185, "endOffset": 198}, {"referenceID": 2, "context": "Several previous works have proven that the relaxation is exact in the two-class case; that is, the total variation solution coincides with the solution of the original NP-hard problem [8, 18, 3, 5].", "startOffset": 185, "endOffset": 198}, {"referenceID": 4, "context": "Several previous works have proven that the relaxation is exact in the two-class case; that is, the total variation solution coincides with the solution of the original NP-hard problem [8, 18, 3, 5].", "startOffset": 185, "endOffset": 198}, {"referenceID": 0, "context": "Figure 2(a) shows the quasi-indicator function f4 obtained by our MTV algorithm while 2(b) shows the function f4 obtained from the NMF algorithm of [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "Right: Solution f4 from LSD [1] plotted over the fours and nines.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "F \u2208 \u03a3 := { F \u2208MN\u00d7R([0, 1]) : fr(xi) \u2265 0, R \u2211", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": ", LR denote the R vertex subsets representing the labeled points, so that xi \u2208 Lr means xi belongs to class r, then we may enforce these labels by restricting F to lie in the subset F \u2208 \u039b := {F \u2208MN\u00d7R([0, 1]) : \u2200r, (f1(xi), .", "startOffset": 200, "endOffset": 206}, {"referenceID": 10, "context": "The formula for the subdifferential generalizes a related result for the symmetric case [11] to the asymmetric setting.", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "In this work we use the primal-dual algorithm of [6] with acceleration.", "startOffset": 49, "endOffset": 52}, {"referenceID": 13, "context": "When C = \u03a3 the algorithm [14] performs the required projection in at most R steps.", "startOffset": 25, "endOffset": 29}, {"referenceID": 3, "context": "We obtained the first data set (4MOONS) and its similarity matrix from [4] and the remaining five data sets and matrices (WEBKB4, OPTDIGITS, PENDIGITS, 20NEWS, MNIST) from [19].", "startOffset": 71, "endOffset": 74}, {"referenceID": 18, "context": "We obtained the first data set (4MOONS) and its similarity matrix from [4] and the remaining five data sets and matrices (WEBKB4, OPTDIGITS, PENDIGITS, 20NEWS, MNIST) from [19].", "startOffset": 172, "endOffset": 176}, {"referenceID": 10, "context": "We compare against two previous total variation algorithms [11, 3], which rely on recursive bi-partitioning, and two top NMF algorithms [1, 19].", "startOffset": 59, "endOffset": 66}, {"referenceID": 2, "context": "We compare against two previous total variation algorithms [11, 3], which rely on recursive bi-partitioning, and two top NMF algorithms [1, 19].", "startOffset": 59, "endOffset": 66}, {"referenceID": 0, "context": "We compare against two previous total variation algorithms [11, 3], which rely on recursive bi-partitioning, and two top NMF algorithms [1, 19].", "startOffset": 136, "endOffset": 143}, {"referenceID": 18, "context": "We compare against two previous total variation algorithms [11, 3], which rely on recursive bi-partitioning, and two top NMF algorithms [1, 19].", "startOffset": 136, "endOffset": 143}, {"referenceID": 10, "context": "We use the normalized Cheeger cut versions of [11] and [3] with default parameters.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "We use the normalized Cheeger cut versions of [11] and [3] with default parameters.", "startOffset": 55, "endOffset": 58}, {"referenceID": 18, "context": "from [19] to test each NMF algorithm.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "All non-recursive algorithms (LSD [1], NMFR [19], MTV) received two types of initial data: (a) the deterministic data used in [19]; (b) a random procedure leveraging normalized-cut [16].", "startOffset": 34, "endOffset": 37}, {"referenceID": 18, "context": "All non-recursive algorithms (LSD [1], NMFR [19], MTV) received two types of initial data: (a) the deterministic data used in [19]; (b) a random procedure leveraging normalized-cut [16].", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "All non-recursive algorithms (LSD [1], NMFR [19], MTV) received two types of initial data: (a) the deterministic data used in [19]; (b) a random procedure leveraging normalized-cut [16].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "All non-recursive algorithms (LSD [1], NMFR [19], MTV) received two types of initial data: (a) the deterministic data used in [19]; (b) a random procedure leveraging normalized-cut [16].", "startOffset": 181, "endOffset": 185}, {"referenceID": 2, "context": "Alg/Data 4MOONS WEBKB4 OPTDIGITS PENDIGITS 20NEWS MNIST NCC-TV [3] 88.", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "80 1SPEC [11] 73.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "17 LSD [1] 99.", "startOffset": 7, "endOffset": 10}, {"referenceID": 18, "context": "67 NMFR [19] 77.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "We may therefore apply algorithm 2 of [6] with \u03b3 = 1 with to solve the saddle-point problem.", "startOffset": 38, "endOffset": 41}], "year": 2013, "abstractText": "Ideas from the image processing literature have recently motivated a new set of clustering algorithms that rely on the concept of total variation. While these algorithms perform well for bi-partitioning tasks, their recursive extensions yield unimpressive results for multiclass clustering tasks. This paper presents a general framework for multiclass total variation clustering that does not rely on recursion. The results greatly outperform previous total variation algorithms and compare well with state-of-the-art NMF approaches.", "creator": "LaTeX with hyperref package"}}}