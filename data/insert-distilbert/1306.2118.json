{"id": "1306.2118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2013", "title": "A Novel Approach for Single Gene Selection Using Clustering and Dimensionality Reduction", "abstract": "we must extend the standard rough simple set - based approach specifically to deal with huge trace amounts of numeric attributes versus small amount of available objects. here, a novel approach of clustering along with dimensionality reduction ; hybrid fuzzy c derived means - quick reduct ( fcmqr ) algorithm is proposed for single gene selection. gene selection is merely a process conducted to select genes which are more informative. it is one of the important steps in knowledge discovery. the problem is that all genes interacting are not quite important in gene expression data. some of the genes may be redundant, and usually others may be irrelevant and noisy. in this straightforward study, designing the entire dataset cluster is successfully divided in proper grouping of similar genes by applying fuzzy c means ( fcm ) algorithm. a high class discriminated genes has been selected based on their degree of dependence by applying quick reduct methods algorithm based on rough set theory to all rank the resultant clusters. average relative correlation value ( acv ) is calculated for the related high intensity class discriminated genes. the clusters which have the acv value like a s 1 is determined as significant clusters, whose classification accuracy actually will be equal or high when comparing to assess the accuracy of the entire dataset. the proposed algorithm is evaluated using weka classifiers and compares compared. finally, experimental results actually related to the leukemia brain cancer data confirm that our approach is quite promising, though it surely requires much further valuable research.", "histories": [["v1", "Mon, 10 Jun 2013 07:28:51 GMT  (648kb)", "http://arxiv.org/abs/1306.2118v1", "6 pages, 4 figures. arXiv admin note: text overlap witharXiv:1306.1323"]], "COMMENTS": "6 pages, 4 figures. arXiv admin note: text overlap witharXiv:1306.1323", "reviews": [], "SUBJECTS": "cs.CE cs.LG", "authors": ["e n sathishkumar", "k thangavel", "t chandrasekhar"], "accepted": false, "id": "1306.2118"}, "pdf": {"name": "1306.2118.pdf", "metadata": {"source": "META", "title": "A Novel Approach for Single Gene Selection Using Clustering and Dimensionality Reduction", "authors": [], "emails": [], "sections": [{"heading": null, "text": "available objects. Here, a novel approach of clustering along with dimensionality reduction; Hybrid Fuzzy C Means-Quick Reduct (FCMQR) algorithm is proposed for single gene selection. Gene selection is a process to select genes which are more informative. It is one of the important steps in knowledge discovery. The problem is that all genes are not important in gene expression data. Some of the genes may be redundant, and others may be irrelevant and noisy. In this study, the entire dataset is divided in proper grouping of similar genes by applying Fuzzy C Means (FCM) algorithm. A high class discriminated genes has been selected based on their degree of dependenc e by applying Quick Reduct algorithm based on Rough Set Theory to all the resultant clusters. Average Correlation Value (ACV) is calculated f or the high class discriminated genes. The clusters which have the ACV value a s 1 is determined as significant clusters, whose classification accuracy will be equal or high when comparing to the accuracy of the entire dataset. The proposed algorithm is evaluated using WEKA classifiers and compared. Finally, experimental results related to the leukemia cancer data confirm that our approach is quite promising, though it surely requires further research.\nIndex Terms\u2014 Clustering, Feature Selection, Fuzzy C-Means, FCMQR, Gene Expression Data, Gene Selection, Rough Sets.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014  \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014"}, {"heading": "1 INTRODUCTION", "text": "HE DNA microarray technology provides enormous quantities of biological information about genetically conditioned susceptibility to diseases. The data sets acquired from microarrays refer to genes via their expression levels. Microarray production starts with preparing two samples of mRNA, as illustrated by Fig. 1. The sample of interest is paired with a healthy control sample. The fluorescent red/green labels are applied to both samples. The procedure of samples mixing is repeated for each of thousands of genes on the slide. Fluorescence of red/green colors indicates to what extent the genes are expressed. The gene expressions can be then stored in numeric attributes, coupled with, e.g., clinical information about the patients. Given thousands of genes-attributes against hundreds of objects, we face a \u2015few-objects-manyattributes\u2016 problem [19]. Dimensionality reduction in gene expression data can be critical for a number of reasons. First, for large number of genes or feature set, the processing of all available genes may be computationally infeasible. Second, many of the available features may be redundant and noisedominated or irrelevant to the classification task at hand. Third, high-dimensionality is also a problem if the number of variables is much larger than the number of data points available. In such a scenario, dimensionality reduction is crucial in order to overcome the curse of dimensionality [7],[11]and allow for meaningful data analysis. For the above reasons, feature selection is important for gene expression data analysis.\nA problem with gene expression analysis is often the selection of significant variables (feature selection) within the data set that would enable accurate classification of the data to some output classes. These variables may be potential diagnostic markers too. There are good reasons for reducing the large number of variables: First, an opportunity to scrutinize individual genes for further medical treatment and drug development. Second, dimension reduction to reduce the computational cost. Third, reducing the number of redundant and unnecessary variables can improve inference and classification. Fourth, more interpretable features or characteristics that can help identify and monitor the target diseases or functions types.\nIn this paper, gene or features will be selected from a group, such that the genes in a group will be similar. Gene clustering identifies groups of genes that exhibit similar expression profiles across samples. Clustering is a widely used technique for analysis of gene expression data. Most clustering methods\nT\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n E.N. Sathishkumar is currently pursuing Ph.D., in Computer Science in Periyar University, Salem, India. E-mail: en.sathishkumar@yahoo.in\n K. Thangavel is currently working as Professor and Head, Department of Computer Science in Periyar University, Salem, India. E-mail: drktvelu@yahoo.com\n T. Chandrasekhar is currently pursuing Ph.D., in Computer Science in Bharthiyar University, India. E-mail: ch_ansekh80@rediffmail.com\ngroup genes based on the distances, while few methods group according to the similarities of the distributions of the gene expression levels. Clustering is the process of finding groups of objects such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups. A good clustering method will produce high quality clusters with high intra-cluster similarity and low inter-cluster similarity. The quality of a clustering result depends on both the similarity measure used by the method and its implementation and also by its ability to discover some and all of the hidden patterns [16].\nFeature Selection algorithm aims at finding out a subset of the most representative features according to some objective function in discrete space. The algorithms of FS are always greedy. Our feature selection will be based on rough set; The Rough set approach to feature selection consists in selecting a subset of features which can predict the classes as well as the original set of features. The optimal criterion for Rough set feature selection is to find shortest or minimal reducts while obtaining high quality classifiers based on the selected features. In this paper, we introduce a novel method of using Fuzzy CMeans clustering along with rough set attribute reduction (Quick Reduct) for single gene selection. The attribute selection method is perform for all clusters which are obtained by the applying FCM algorithm.\nThis paper is organized as follows. The next section describes about various methods and in section 3 briefs the proposed gene selection algorithm. In section 4, experimental results are listed. The discussions of these results are given. Section 5 briefs about WEKA classification and its classification results. Finally, this paper is concluded in section 6."}, {"heading": "2 METHODS", "text": ""}, {"heading": "2.1 Fuzzy C-Means Clustering", "text": "Fuzzy clustering allows each feature vector to belong to more than one cluster with different membership degrees (between 0 and 1) and vague or fuzzy boundaries between clusters. Fuzzy C-Means (FCM) is a method of clustering which allows one piece of data to belong to two or more clusters[2],[14]. This method (developed by Dunn in 1973 and improved by Bezdek in 1981) is frequently used in pattern recognition. The Fuzzy C Means algorithm is given below:\nAlgorithm 1: Fuzzy C-Means clustering algorithm [14]\nStep-1: Randomly initialize the membership matrix using this equation,\n\ud835\udf07\ud835\udc57 \ud835\udc36 \ud835\udc57=1 (\ud835\udc65\ud835\udc56) = 1 i = 1, 2\u2026k\nStep-2: Calculate the Centroid using equation,\n\ud835\udc36\ud835\udc57 = [\ud835\udf07\ud835\udc57 (\ud835\udc65\ud835\udc56)] \ud835\udc5a\ud835\udc65\ud835\udc56\ud835\udc56 [\ud835\udf07\ud835\udc57 (\ud835\udc65\ud835\udc56)] \ud835\udc5a \ud835\udc56\nStep-3: Calculate dissimilarly between the data points and\nCentroid using the Euclidean distance.\n\ud835\udc37\ud835\udc56 = (\ud835\udc652 \u2212 \ud835\udc651 ) 2 + (\ud835\udc662 \u2212 \ud835\udc661 ) 2\nStep-4: Update the New membership matrix using the equation,\n\ud835\udf07\ud835\udc57 (\ud835\udc65\ud835\udc56) =\n[ 1 \ud835\udc51\ud835\udc57\ud835\udc56 ]1 \ud835\udc5a\u22121\n[ 1 \ud835\udc51\ud835\udc58\ud835\udc56 ]1 \ud835\udc5a\u22121 \ud835\udc50\ud835\udc58=1\nHere m is a fuzzification parameter. The range m is always [1.25, 2] Step -5: Go back to Step 2, unless the centroids are not changing."}, {"heading": "2.2 K-Means Discretization", "text": "Many data mining techniques often require that the attributes of the data sets are discrete. Given that most of the experimental data are continuous, not discrete, the discretization of the continuous attributes is an important issue. The goal of discretization is to reduce the number of possible values a continuous attribute takes by partitioning them into a number of intervals. K-Means discretization method is used in this paper, in gene expression data set each gene attribute are clustered with K-Means, replaces the attribute values with the cluster membership labels. These labels will be act as discrete values for gene expression data set."}, {"heading": "2.3 Quick Reduct Algorithm", "text": "Rough set theory (Pawlak, 1991) is a formal mathematical tool that can be applied to reducing the dimensionality of dataset. The rough set attribute reduction method removes redundant input attributes from dataset of discrete values, all the while making sure that no information is lost. The reduction of attributes is achieved by comparing equivalence relations generated by sets of attributes. Attributes are removed so that the reduced set provides the same quality of classification as the original. A reduct is defined as a subset R of the conditional attribute set C such that \u03b3R \ud835\udc37 = \u03b3C \ud835\udc37 . A given dataset may have many attribute reduct sets, so the set R of all reducts is defined as:\nRall = {X |X \u2286C, \u03b3X \ud835\udc37 = \u03b3C \ud835\udc37 ;\n\u03b3X\u2212{a} \ud835\udc37 \u2260 \u03b3X \ud835\udc37 ) ,\u2200a \u2208 X}. (1)\nThe intersection of all the sets in Rall is called the core, the elements of which are those attributes that cannot be eliminated without introducing more contradictions to the representation of the dataset. For many tasks (for example, feature selection), a reduct of minimal cardinality is ideally searched for a single element of the reduct set Rmin\u2286Rall:\nRmin = {X |X \u2208Rall, \u2200 Y\u2208Rall,|X| \u2264 |Y|}. (2) The Quick Reduct algorithm shown below, it searches for a minimal subset without exhaustively generating all possible subsets. The search begins with an empty subset; attributes which result in the greatest increase in the rough set dependency value that is added iteratively. This process continues until the search produces its maximum possible dependency value for that data set \u03b3C \ud835\udc37 . Note that this type of search does\nnot guarantee a minimal subset and may only discover a local minimum. Such techniques may be found in [1],[7],[8],[14],[17]\nAlgorithm 2: Quick Reduct (C, D)\nC, the set of all conditional features; D, the set of decision features. (a) R \u2190 {} (b) Do (c) T \u2190 R (d) \u2200 x \u2208(C-R) (e) if \u03b3R\u222a x \ud835\udc37 > \u03b3T \ud835\udc37\nWhere \u03b3R \ud835\udc37 = \ud835\udc50\ud835\udc4e\ud835\udc5f\ud835\udc51 (POS R \ud835\udc37 )\n\ud835\udc50\ud835\udc4e\ud835\udc5f\ud835\udc51 (\ud835\udc48)\n(f) T \u2190 R\u222a{x} (g) R \u2190 T (h) until \u03b3R \ud835\udc37 == \u03b3C \ud835\udc37 (i) return R"}, {"heading": "2.4 Average Correlation Value", "text": "Average Correlation Value is used to evaluate the homogeneity of a cluster. Matrix A= (Aij) has the ACV which is defined by the following function,\nACV (A) = \ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc36_\ud835\udc5f\ud835\udc5c\ud835\udc64 \ud835\udc56\ud835\udc57 \u2212\ud835\udc5a\n\ud835\udc5a \ud835\udc57=1 \ud835\udc5a \ud835\udc56=1\n\ud835\udc5a2\u2212\ud835\udc5a , \ud835\udc36_\ud835\udc50\ud835\udc5c\ud835\udc59 \ud835\udc5d\ud835\udc5e \u2212\ud835\udc5b\n\ud835\udc5b \ud835\udc5e=1 \ud835\udc5b \ud835\udc5d=1\n\ud835\udc5b2\u2212\ud835\udc5b (3)\nWhere C_rowij \u2013 is the correlation coefficient between rows i and j, C_colpq is the correlation coefficient between columns p and q, ACV approaching 1 denote a significant cluster. Such technique may be found in [12]."}, {"heading": "3 HYBRID FUZZY C-MEAN-QUICKREDUCT (FCMQR) ALGORITHM", "text": "The proposed FCMQR algorithm logically consists of three steps: (i) grouping the similar genes, (ii) feature selection from group, (iii) finding ACV and selecting representative features. The purpose of the algorithm is to select a subset of features R = {RG1, RG2,\u2026,RGr} from the original gene set G = {G1, G2,\u2026,Gn} where n is the dimension of gene feature vectors and r<n is the number of selected features that having ACV =1. A feature Gbest is included in the subset R, if for this Gbest, the subset R gives the highest classification accuracy. The algorithm of FCMQR method is described as follows.\nAlgorithm 3: Hybrid Fuzzy C-Mean-QuickReduct (FCMQR)\nInputs: Gene expression data contains n genes and a m class variable, G = {G1, G2\u2026 Gn} and D= {D1, D2\u2026 Dm} Output: Gbest \u2013 Selected gene Step 1: set k=5 and Gbest \u2190 {} Step 2: Do, Gene wise cluster using FCM\ni) Random membership initialization\n\ud835\udf07\ud835\udc57\n\ud835\udc36\n\ud835\udc57=1\n(\ud835\udc65\ud835\udc56) = 1\nii) Calculate the Centroid \ud835\udc36\ud835\udc57 = [\ud835\udf07\ud835\udc57 (\ud835\udc65\ud835\udc56)]\n\ud835\udc5a \ud835\udc65\ud835\udc56\ud835\udc56\n[\ud835\udf07\ud835\udc57 (\ud835\udc65\ud835\udc56)] \ud835\udc5a \ud835\udc56\niii) Calculate dissimilarly between the data points and Centroid using the Euclidean distance. iv) Update new membership matrix\n\ud835\udf07\ud835\udc57 (\ud835\udc65\ud835\udc56) =\n[ 1 \ud835\udc51\ud835\udc57\ud835\udc56 ]1 \ud835\udc5a\u22121\n[ 1 \ud835\udc51\ud835\udc58\ud835\udc56 ]1 \ud835\udc5a\u22121 \ud835\udc50\ud835\udc58=1\nv) Go back to ii, unless the centroids are not changing. Step 3: Get GCi= {g1, g2\u2026 gq} from Step 2. Step 4: Discretize the GCi by applying K-means Discretization Step 5: for i= 1 to k Do, Quick Reduct(GCi,D) to select RCi= {R1, R2\u2026Rr} according to Alg.2 where RCi \u2286 GCi End End Step 6: Compute ACV for all refined RCi according to Eqs.3 Step 7: Collect all the genes from clusters, where ACV=1 Rk = {RCi / ACV (RCi) = 1} = {RG1, RG2,\u2026,RGr} where r = no. of genes in acv=1 clusters Step 8: Repeat step 2 to 6, for k = 7 and etc. where k= no. of clusters we need. Step 9: Let Gbest = \ud835\udc45\ud835\udc58\ud835\udf16\ud835\udc45\ud835\udc58 k Step 10: Return Gbest"}, {"heading": "4 EXPERIMENTAL RESULTS", "text": ""}, {"heading": "4.1 Data Set", "text": "We use leukemia data set which is available in the website: http://datam.i2r.a-star.edu.sg/ datasets/krbd/ [15]. Our initial leukemia data set consisted of 38 bone marrow samples (27 ALL, 11 AML) obtained from acute leukemia patients at the time of diagnosis. RNA prepared from bone marrow mononuclear cells was hybridized to high-density oligonucleotide microarrays, produced by Affymetrix and containing probes over 7129 from 6817 human genes [18]."}, {"heading": "4.2 Cluster Analysis and Gene Selection", "text": "Gene clustering identifies group of genes that exhibit similar expression profiles across samples. Fuzzy C-Means clustering is used to cluster the similar characteristics of genes GCi. Before clustering, need to specify the number of clusters. The optimal number of clusters is difficult to determine, because it may depend on different sets of genes under investigation. In this study, the number of clusters is chosen to be five and seven (k=5, 7), then leukemia data set will divide k number of groups using Fuzzy C-Means clustering techniques. After clustering, features or genes will be selected from a similar gene cluster GCi. Rough Quick Reduct has been used as feature selection method. The data studied by rough set are mainly organized in the form of decision tables. One decision table can be represented as S = (U, A=GCi U D), where U is the set of samples in cluster GCi (i=1 to k), GCi the condition attribute set and D the decision attribute set. We can represent every gene expression data with the decision table like Table 1.\nTABLE 1\nROUGH SET DATA DECISION TABLE\nSam ples\nCluster GCi (Condition attributes) Decision attributes\nGene1 Gene 2 \u2026 Gene n Class label\n1 g(1,1) g(1,2) \u2026 g(1,n) Class(1)\n2 g(2,1) g(2,2) \u2026 g(2,n) Class(2)\n\u2026 \u2026 \u2026 \u2026 \u2026 \u2026\np g(m,1) g(m,2) \u2026 g(m,n) Class(m)\nIn the decision table, there are m samples and n genes in cluster GCi. Every sample is assigned to one class label. Each gene is a condition attribute and each class is a decision attribute. g(m, n) signifies the expression level of gene n in sample m [15]. Before applying feature selection algorithm all the conditional attributes (samples) are discretized using K-Means discretization. After feature selection, to evaluate the Average Correlation Value for selected genes from each cluster. ACV approaching 1 denote a significant cluster and it is evaluating the homogeneity of a cluster. Table 2 and 3 shows the selected genes from particular cluster by applying Quick Reduct and Average Correlation Value for that genes.\nTable 2, depict the similar expression genes when k=5, and shows selected genes (RCi) after applied Quick Reduct. Based on Average Correlation Values, we determine cluster 2 and 4 are significant clusters Rk. In that cluster genes having high classification accuracy compare to other genes.\nTable 3, shows similar expression genes when k=7 and depict selected genes (RCi) after Quick Reduct. Based on Average Correlation Values, we determine Cluster 2, 4 and 7 are significant clusters."}, {"heading": "5 WEKA CLASSIFICATION", "text": "The classifier tool WEKA [1],[11],[13] is open source java based machine-learning. It brings together many machine learning algorithm and tools under a common frame work. The WEKA is a well known package of data mining tools which provides a variety of known, well maintained classifying algorithms. This allows us to do experiments with several kinds of classifiers quickly and easily. The tool is used to perform benchmark experiment. Some of the classifiers we used in our experiment are bayes.NaiveBayes, trees.J48, rules.Decision Table and lazy.K-Star.\nFig. 2, Denotes classification accuracy for leukemia data when cluster k=5. We obtained thirteen genes from entire genes by applying Quick Reduct. Out of the thirteen genes which genes having ACV=1 that five genes are selected as best Rk. The classification accuracy for those five; #154, #3252, #930, #1765 and #5711 genes is equal or high when compared to entire genes (7129) and Quick Reduct selected genes.\nFig. 3, Denotes classification accuracy of leukemia data when k=7. We obtained twenty genes from entire genes by applying QuickReduct. Out of the twenty genes which genes having ACV=1 that six genes are selected as high class discriminated genes Rk. The classification accuracy for those six; #29, #1884, #79, #3252, #1674 and #2288 genes is equal or high when compared to entire genes and QuickReduct selected genes.\nTABLE 6 MARKER GENE SELECTED FROM SIGNIFICANT CLUSTERS GENES\nCluster Rk Genes Selected\ngene\nCluster k=5 #154, #3252, #930, #1765,\n#5711 #3252 (Gbest)\nCluster k=7 #29, #1884, #79, #3252,\n#1674, #2288 Table 6 shows, In the leukemia dataset, when cluster k=5, gene #154, #3252, #930, #1765 and #5711 are identified; when k=7, gene #29, #1884, #3252, #79, #1674 and #2288 are identified. Among the significant clusters (ACV=1) genes have the classification accuracy higher than 88.2353%. Finally, we get Gbest = \ud835\udc45\ud835\udc58\ud835\udf16\ud835\udc45\ud835\udc58 k is gene #3252 has 97.0588% accuracy and which is common to all experiment. We denote the expression level of gene x by g(x). Two decision rules induced by gene #3252 are:\nIf g (#3252) > 643, then AML; If g (#3252) \u2264 643, then ALL."}, {"heading": "5.1 Comparison of Classification Results", "text": "The leukemia dataset has been well studied by many researchers [3], [4], [5], [6], [8], [9], [10]. Although there are a few reports on the use of a single gene to distinguish the AML from the ALL, a majority of investigators conduct the classification with more than one gene, even tens or hundreds. In[15], the authors present the classification outcomes of 31 out of 34 samples correctly classified with one common gene (#4847): X.Wang & O.Gotoh, we correctly classify 33 out of 34 samples using a selected gene (#3252). Classification accuracy for existing and proposed method selected single genes shown in table 7.\nTABLE 7\nCOMPARISON BETWEEN EXISTING AND PROPOSED METHOD\nClassifier Existing Method gene #4847 Proposed FCMQR gene #3252\nNa\u00efve 91.1765 97.0588 D.Table 85.2941 91.1765\nJ48 88.2353 91.1765 K-Star 91.1765 97.0588\nRegarding leukemia datasets, the best classification results reported in our and existing works are shown in Table 7 and Fig 4, respectively. These tables demonstrate that our FCMQR algorithm perform comparatively well in leukemia dataset."}, {"heading": "6 CONCLUSION", "text": "The work has been proposed for improving the gene selection method in a simple and efficient way. Here a novel approach combining clustering and rough set attribute reduction method FCMQR has been proposed for gene expression data. Informative genes are selected using their classification accuracy. Fuzzy C-Means clustering, Rough Quick Reduct and Average Correlation Value methods are studied and implemented successfully for gene selection. The proposed work gives sparse and interpretable classification accuracy, compared to other gene selection method on leukemia gene expression data set. The classification accuracy of genes has been done using WEKA classifier."}, {"heading": "ACKNOWLEDGMENT", "text": "The present work is supported by Special Assistance Programme of University Grants Commission, New Delhi, India (Grant No. F.3-50/2011(SAP-II).\nThe first author immensely acknowledges the partial financial assistance under University Research Fellowship, Periyar University, Salem, Tamilnadu, India."}], "references": [{"title": "Unsupervised Quick Reduct Algorithm using Rough Set Theory", "author": ["C. Velayutham", "K. Thangavel"], "venue": "Journal of Electronic Science and Technology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Image Analysis: Bio- Inspired Computational Approach", "author": ["K Thangavel", "C Velayutham", "\"Mammogram"], "venue": "\"Proceedings of the International Conf. on SocProS", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Support vector machine classification and validation of cancer tissue samples using microarray expression", "author": ["Furey", "T.-S", "N. Cristianini", "N. Duffy", "Bednarski", "D.-W", "M. Schummer", "D. Haussler"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Molecular classification of cancer: class discovery and class prediction by gene expression", "author": ["Golub", "T.-R", "Slonim", "D.-K", "P. Tamayo", "C. Huard", "M. Gaasenbeek", "Mesirov J.P", "H. Coller", "Loh M.-L", "Downing", "J.-R", "Caligiuri", "M.-A"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Coding polymorphisms in CD33 and response to gemtuzumab ozogamicin in pediatric patients with AML: a pilot study. Leukemia", "author": ["Lamba", "J.-K", "S. Pounds", "X. Cao", "Downing J.-R", "D. Campana", "Ribeiro", "R.-C", "C.H. Pui", "Rubnitz", "J.-E"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Gene Selection with Rough Sets for Cancer Classification", "author": ["Lijun Sun", "Duoqian Miao", "Hongyun Zhang"], "venue": "Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Gene selection using rough set theory", "author": ["D. Li", "W. Zhang"], "venue": "Proc. 1st International Conference on Rough Sets and Knowledge Technology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Identifying good diagnostic gene groups from gene expression profiles using the concept of emerging patterns, Bioinformatics", "author": ["J. Li", "L. Wong"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Reduct generation and classification of gene expression data", "author": ["Momin", "B.-F", "S. Mitra"], "venue": "Proc. 1st International Conference on Hybrid Information Technology,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Feature selection for classification", "author": ["M. Dashand H. Liu"], "venue": "Intelligent Data Analysis,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "K.Thangavel and J.Bagyamani, \u2015Evolutionary Biclustering of Clickstream Data", "author": ["R. Rathipriya", "Dr"], "venue": "International Journal of Computer Science Issues,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Ensemble machine learning on gene expression data for cancer", "author": ["Tan", "A.-C", "D. Gilbert"], "venue": "classification, Appl Bioinformatics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Sathishkumar, \u2015Verdict Accuracy of Quick Reduct Algorithm using Clustering and Classification Techniques for Gene Expression Data", "author": ["T. Chandrasekhar", "E.N.K. Thangavel"], "venue": "International Journal of Computer Science Issues,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Survey of clustering algorithms", "author": ["R. Xu", "D. Wunsch"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Molecular Classification of cancer: Class Discovery and Class Prediction by Gene Expression Monitoring\u2016, www.sciencemag.org", "author": ["T.R. Golub et.al"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Rough Sets and Few-Objects-Many-Attributes Problem: The Case Study of Analysis of Gene Expression Data Sets", "author": ["Dominik Slezak"], "venue": "Frontiers in the Convergence of Bioscience and Information Technologies,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}], "referenceMentions": [{"referenceID": 15, "context": "Given thousands of genes-attributes against hundreds of objects, we face a \u2015few-objects-manyattributes\u2016 problem [19].", "startOffset": 112, "endOffset": 116}, {"referenceID": 5, "context": "In such a scenario, dimensionality reduction is crucial in order to overcome the curse of dimensionality [7],[11]and allow for meaningful data analysis.", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "In such a scenario, dimensionality reduction is crucial in order to overcome the curse of dimensionality [7],[11]and allow for meaningful data analysis.", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "The quality of a clustering result depends on both the similarity measure used by the method and its implementation and also by its ability to discover some and all of the hidden patterns [16].", "startOffset": 188, "endOffset": 192}, {"referenceID": 1, "context": "Fuzzy C-Means (FCM) is a method of clustering which allows one piece of data to belong to two or more clusters[2],[14].", "startOffset": 110, "endOffset": 113}, {"referenceID": 12, "context": "Fuzzy C-Means (FCM) is a method of clustering which allows one piece of data to belong to two or more clusters[2],[14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "Algorithm 1: Fuzzy C-Means clustering algorithm [14]", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "Such techniques may be found in [1],[7],[8],[14],[17]", "startOffset": 32, "endOffset": 35}, {"referenceID": 5, "context": "Such techniques may be found in [1],[7],[8],[14],[17]", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "Such techniques may be found in [1],[7],[8],[14],[17]", "startOffset": 40, "endOffset": 43}, {"referenceID": 12, "context": "Such techniques may be found in [1],[7],[8],[14],[17]", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Such technique may be found in [12].", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "RNA prepared from bone marrow mononuclear cells was hybridized to high-density oligonucleotide microarrays, produced by Affymetrix and containing probes over 7129 from 6817 human genes [18].", "startOffset": 185, "endOffset": 189}, {"referenceID": 0, "context": "The classifier tool WEKA [1],[11],[13] is open source java based machine-learning.", "startOffset": 25, "endOffset": 28}, {"referenceID": 9, "context": "The classifier tool WEKA [1],[11],[13] is open source java based machine-learning.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "The classifier tool WEKA [1],[11],[13] is open source java based machine-learning.", "startOffset": 34, "endOffset": 38}, {"referenceID": 2, "context": "1 Comparison of Classification Results The leukemia dataset has been well studied by many researchers [3], [4], [5], [6], [8], [9], [10].", "startOffset": 102, "endOffset": 105}, {"referenceID": 3, "context": "1 Comparison of Classification Results The leukemia dataset has been well studied by many researchers [3], [4], [5], [6], [8], [9], [10].", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "1 Comparison of Classification Results The leukemia dataset has been well studied by many researchers [3], [4], [5], [6], [8], [9], [10].", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "1 Comparison of Classification Results The leukemia dataset has been well studied by many researchers [3], [4], [5], [6], [8], [9], [10].", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "1 Comparison of Classification Results The leukemia dataset has been well studied by many researchers [3], [4], [5], [6], [8], [9], [10].", "startOffset": 127, "endOffset": 130}, {"referenceID": 8, "context": "1 Comparison of Classification Results The leukemia dataset has been well studied by many researchers [3], [4], [5], [6], [8], [9], [10].", "startOffset": 132, "endOffset": 136}], "year": 2013, "abstractText": "We extend the standard rough set-based approach to deal with huge amounts of numeric attributes versus small amount of available objects. Here, a novel approach of clustering along with dimensionality reduction; Hybrid Fuzzy C Means-Quick Reduct (FCMQR) algorithm is proposed for single gene selection. Gene selection is a process to select genes which are more informative. It is one of the important steps in knowledge discovery. The problem is that all genes are not important in gene expression data. Some of the genes may be redundant, and others may be irrelevant and noisy. In this study, the entire dataset is divided in proper grouping of similar genes by applying Fuzzy C Means (FCM) algorithm. A high class discriminated genes has been selected based on their degree of dependenc e by applying Quick Reduct algorithm based on Rough Set Theory to all the resultant clusters. Average Correlation Value (ACV) is calculated f or the high class discriminated genes. The clusters which have the ACV value a s 1 is determined as significant clusters, whose classification accuracy will be equal or high when comparing to the accuracy of the entire dataset. The proposed algorithm is evaluated using WEKA classifiers and compared. Finally, experimental results related to the leukemia cancer data confirm that our approach is quite promising, though it surely requires further research.", "creator": "Microsoft\u00ae Office Word 2007"}}}