{"id": "1307.0060", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2013", "title": "Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs", "abstract": "the idea introduction of computer vision as the bayesian inverse problem to computer graphics has a long history and exhibits an appealing elegance, but it has proved moderately difficult to directly implement. instead, most vision tasks are approached via complex computational bottom - up processing pipelines. here we show that it is possible to write short, simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real - world images. generative computation probabilistic graphics programs consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking mainly the renderer's output and the data, and latent variables that adjust the fidelity of the renderer and the tolerance of solving the likelihood model. representations and algorithms from computer graphics, originally designed to produce high - quality images, are instead used as the deterministic backbone for highly approximate and stochastic generative models. this formulation combines probabilistic programming, computer graphics, algorithms and approximate bayesian computation, and depends only on general - purpose, automatic inference techniques. we describe two applications : reading sequences of degraded and adversarially obscured alphanumeric characters, combining and inferring 3d road models from vehicle - mounted camera images. each of the probabilistic computer graphics programs we present relies on under 20 lines of probabilistic code, and supports accurate, approximately bayesian inferences about ambiguous real - world images.", "histories": [["v1", "Sat, 29 Jun 2013 02:36:45 GMT  (16122kb,D)", "http://arxiv.org/abs/1307.0060v1", "The first two authors contributed equally to this work"]], "COMMENTS": "The first two authors contributed equally to this work", "reviews": [], "SUBJECTS": "cs.AI cs.CV stat.ML", "authors": ["vikash k mansinghka", "tejas d kulkarni", "yura n perov", "joshua b tenenbaum"], "accepted": true, "id": "1307.0060"}, "pdf": {"name": "1307.0060.pdf", "metadata": {"source": "CRF", "title": "Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs", "authors": ["Vikash K. Mansinghka", "Tejas D. Kulkarni", "Yura N. Perov", "Joshua B. Tenenbaum"], "emails": ["jbt)@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Computer vision has historically been formulated as the problem of producing symbolic descriptions of scenes from input images [9]. This is usually done by building bottom-up processing pipelines that isolate the portions of the image associated with each scene element and extract features that are indicative of its identity. Many pattern recognition and learning techniques can then be used to build classifiers for individual scene elements, and sometimes to learn the features themselves [7, 6].\nThis approach has been remarkably successful, especially on problems of recognition. Bottom-up pipelines that combine image processing and machine learning can identify written characters with high accuracy and recognize objects from large sets of possibilities. However, the resulting systems typically require large training corpuses to achieve reasonable levels of accuracy, and are difficult both to build and modify. For example, the Tesseract system [15] for optical character recognition is over 10, 000 lines of C++. Small changes to the underlying assumptions frequently necessitates end-to-end retraining and/or redesign.\n* The first two authors contributed equally to this work. * (vkm, kulkarni, perov, jbt)@mit.edu\nar X\niv :1\n30 7.\n00 60\nv1 [\ncs .A\nI] 2\n9 Ju\nn 20\nGenerative models for a range of image parsing tasks are also being explored [16, 3, 17, 21, 19]. These provide an appealing avenue for integrating top-down constraints with bottom-up processing, and provide an inspiration for the approach we take in this paper. But like traditional bottom-up pipelines for vision, these approaches have relied on considerable problem-specific engineering, chiefly to design and/or learn custom inference strategies, such as MCMC proposals [17, 21] that incorporate bottom-up cues. Other combinations of top-down knowledge with bottom up processing have been remarkably powerful. For example, [8] has shown that global, 3D geometric information can significantly improve the performance of bottom-up object detectors.\nIn this paper, we propose a novel formulation of image interpretation problems, called generative probabilstic graphics programming (GPGP). Generative probabilistic graphics programs share a common template: a stochastic scene generator, an approximate renderer based on existing graphics software, a highly stochastic likelihood model for comparing the renderer\u2019s output with the observed data, and latent variables that control the fidelity of the renderer and the tolerance of the image likelihood. Our probabilistic graphics programs are written in a variant of the Church probabilistic programming language [5]. Each model we introduce requires less than 20 lines of probabilistic code. The renderers and likelihoods for each are based on standard templates written as short Python programs. Unlike typical generative models for scene parsing, inverting our probabilistic graphics programs requires no custom inference algorithm design. Instead, we rely on the automatic Metropolis-Hastings transition operators provided by our probabilistic programming system. The approximations and stochasticity in our renderer, scene generator and likelihood models serve to implement a variant of approximate Bayesian computation [18, 10]. This combination can produce a kind of self-tuning analogue of annealing that facilities reliable convergence.\nTo the best of our knowledge, our GPGP framework is the first real-world image interpretation formulation to combine probabilistic programming, automatic inference, computer graphics, and approximate Bayesian computation; this constitutes our main contribution. Our second contribution is to provide demonstrations of the efficacy of this approach on two image interpretation problems: reading snippets of degraded and adversarially obscured alphanumeric characters, and inferring 3D road models from vehicle mounted cameras. In both cases we quantitatively report the accuracy of our approach on representative test datasets, as compared to standard bottom-up baselines that have been extensively engineered."}, {"heading": "2 Generative Probabilistic Graphics Programs and Approximate Bayesian Inference.", "text": "Generative probabilistic graphics programs define generative models for images by combining four components. The first is a stochastic scene generator written as probabilistic code that makes random choices for the location and configuration of the main elements in the scene. The second is an approximate renderer based on existing graphics software that maps a scene S and control variables X to an image IR = f(S,X). The third is a stochastic likelihood model for image data ID that enables scoring of rendered scenes given the control variables. The fourth is a set of latent variablesX that control the fidelity of the renderer and/or the tolerance in the stochastic likelihood model. These components are described schematically in Figure 1.\nWe formulate image interpretation tasks in terms of sampling (approximately) from the posterior distribution over images:\nP (S|ID) \u221d P (S)P (X)\u03b4f(S,X)(IR)P (ID|IR, X)\nWe perform inference over execution histories of our probabilistic graphics programs using a uniform mixture of generic, single-variable Metropolis-Hastings transitions, without any custom, bottom-up proposals. We first give a general description of the generative model and inference algorithm induced by our probabilistic graphics programs; in later sections, we describe specific details for each application.\nLet S = {Si} be a decomposition of the scene S into parts Si with independent priors P (Si). For example, in our text application, the Sis include binary indicators for the presence or absence of each glyph, along with its identity (\u201cA\u201c through \u201dZ\u201d, plus digits 0-9), and parameters including location,"}, {"heading": "Stochastic Scene Generator", "text": ""}, {"heading": "Approximate Renderer", "text": ""}, {"heading": "Stochastic", "text": "size and rotation. Also let X = {Xj} be a decomposition of the control variables X into parts Xj with priors P (Xj), such as the bandwidths of per-glyph Gaussian spatial blur kernels, the variance of a Gaussian image likelihood, and so on. Our proposals modify single elements of the scene and control variables at a time, as follows:\nP (S) = \u220f i P (Si) qi(S \u2032 i, Si) = P (S \u2032 i) P (X) = \u220f j P (Xj) qj(X \u2032 j , Xj) = P (X \u2032 j)\nNow let K = |{Si}| + |{Xj}| be the total number of random variables in each execution. For simplicity, we describe the case where this number can be bounded above beforehand, i.e. total a priori scene complexity is limited. At each inference step, we choose a random variable index k < K uniformly at random. If k corresponds to a scene variable i, then we propose from qi(S\u2032i, Si), so our overall proposal kernel q((S,X) \u2192 (S\u2032, X \u2032)) = \u03b4S\u2212i(S\u2032)P (S\u2032i)\u03b4X(X \u2032). If k corresponds to a control variable j, we propose from qj(X \u2032j , Xj). In both cases we re-render the scene I \u2032 R = f(S\n\u2032, X \u2032). We then run the kernel associated with this variable, and accept or reject via the Metropolis-Hastings equation: \u03b1MH((S,X)\u2192 (S\u2032, X \u2032)) = min ( 1, P (ID|f(S\u2032, X \u2032), X \u2032)P (S\u2032)P (X \u2032)q((S\u2032, X \u2032)\u2192 (S,X)) P (ID|f(S,X), X)P (S)P (X)q((S,X)\u2192 (S\u2032, X \u2032))\n) We implement our probabilistic graphics programs in a variant of the Church probabilistic programming language. The Metropolis-Hastings inference algorithm we use is provided by default in this system; no custom inference code is required. In the context of our generative probabilistic graphics formulation, this algorithm makes implicit use of ideas from approximate Bayesian computation (ABC). ABC methods approximate Bayesian inference over complex generative processes by using an exogenous distance function to compare sampled outputs with observed data. In the original rejection sampling formulation, samples are accepted only if they match the data within a hard threshold. Subsequently, combinations of ABC and MCMC were proposed [10], including variants with inference over the threshold value [14]. Most recently, extensions have been introduced where the hard cutoff is replaced with a stochastic likelihood model [18]. Our formulation incorporates a combination of these insights: rendered scenes are only approximately constrained to match the observed image, with the tightness of the match mediated by inference over factors such as the fidelity of the rendering and the stochasticity in the likelihood. This allows image variability that is unnecessary or even undesirable to model to be treated in a principled fashion."}, {"heading": "3 Generative Probabilistic Graphics in 2D for Reading Degraded Text.", "text": "We developed a probabilistic graphics program for reading short snippets of degraded text consisting of arbitrary digits and letters. See Figure 2 for representative inputs and outputs. In this program, the latent scene S = {Si} contains a bank of variables for each glyph, including whether a potential letter is present or absent from the scene, what its spatial coordinates and size are, what its identity is, and how it is rotated:\nP (Spresi = 1) = 0.5 P (S x i = x) = { 1/w 0 \u2264 x \u2264 w 0 otherwise P (Syi = y) = { 1/h 0 \u2264 x \u2264 h 0 otherwise\nP (Sglyph idi = g) = { 1/G 0 \u2264 Sglyph idi < G 0 otherwise P (S\u03b8i = g) = { 1/2\u03b8max \u2212\u03b8max \u2264 S\u03b8i < \u03b8max 0 otherwise\nOur renderer rasterizes each letter independently, applies a spatial blur to each image, composites the letters, and then blurs the result. We also applied global blur to the original training image before applying the stochastic likelihood model on the blurred original and rendered images. The stochastic likelihood model is a multivariate Gaussian whose mean is the blurry rendering; formally, ID \u223c N(IR;\u03c3). The control variables X = {Xj} for the renderer and likelihood consist of per-letter Gaussian spatial blur bandwidths Xij \u223c \u03bb \u00b7 Beta(1, 2), a global image blur on the rendered image Xblur rendered \u223c \u03b2 \u00b7 Beta(1, 2), a global image blur on the original test image Xblur test \u223c \u03b3 \u00b7 Beta(1, 2), and the standard deviation of the Gaussian likelihood \u03c3 \u223c Gamma(1, 1) (with \u03bb, \u03b3 and \u03b2 set to favor small bandwidths). To make hard classification decisions, we use the sample with lowest pixel reconstruction error from a set of 5 approximate posterior samples. We also experimented with enabling enumerative (griddy) Gibbs sampling for uniform discrete variables with 10% probability. The probabilistic code for this model is shown in Figure 4.\nTo assess the accuracy of our approach on adversarially obscured text, we developed a CAPTCHA corpus consisting of over 40 images from widely used websites such as TurboTax, E-Trade, and AOL, plus additional challenging synthetic CAPTCHAs with high degrees of letter overlap and superimposed distractors. Each source of text violates the underlying assumptions of our probabilistic graphics program in different ways. TurboTax CAPTCHAs incorporate occlusions that break strokes within\nletters, while AOL CAPTCHAs include per-letter warping. These CAPTCHAs all involve arbitrary digits and letters, and as a result lack cues from word identity that the best published CAPTCHA breaking systems depend on [11]. We observe robust character recognition given enough inference, with an overall character detection rate of 70.6%. To calibrate the difficulty of our corpus, we also ran the Tesseract optical character recognition engine [15] on our corpus; its character detection rate was 37.7%.\nWe have found that the dynamically-adjustable fidelity of our approximate renderer and the high stochasticity of our generative model are necessary for inference to robustly converge to accurate results. This aspect of our formulation can be viewed as a kind of self-tuning, stochastic Bayesian analogue of annealing, and significantly improves the robustness of our approach. See Figure 3 for an illustration of these dynamics."}, {"heading": "4 Generative Probabilistic Graphics in 3D: Road Finding.", "text": "We have also developed a generative probabilistic graphics program for localizing roads in 3D from single images. This is an important problem in autonomous driving. As with many perception problems in robotics, there is clear scene structure to exploit, but also considerable uncertainty about the scene, as well as substantial image-to-image variability that needs to be robustly ignored. See Figure 5b for example inputs.\nThe probabilistic graphics program we use for this problem is shown in Figure 7. The latent scene S is comprised of the height of the roadway from the ground plane, the road\u2019s width and lane size, and the 3D offset of the corner of the road from the (arbitrary) camera location. The prior encodes assumption that the lanes are small relative to the road, and that the road has two lanes and is very likely to be visible (but may not be centered). This scene is then rendered to produce a surface-based segmentation image, that assigns each input pixel to one of 4 regions r \u2208 R = {left offroad, right offroad, road, lane}. Rendering is done for each scene element sepa-\nrately, followed by compositing, as with our 2D text program. See Figure 5a for random surface-based segmentation images drawn from this prior. Extensions to richer road and ground geometries are an interesting direction for future work.\nIn our experiments, we used k-means (with k = 20) to cluster RGB values from a randomly chosen training image. We used these clusters to build a compact appearance model based on cluster-center histograms, by assigning text image pixels to their nearest cluster. Our stochastic likelihood incorporates these histograms, by multiplying together the appearance probabilities for each image region r \u2208 R. These probabilities, denoted ~thetar, are smoothed by pseudo-counts drawn from a Gamma distribution. Let Zr be the per-region normalizing constant, and ID(x,y) be the quantized pixel at coordinates (x, y) in the input image. Then our likelihood model is:\nP (ID|IR, ) = \u220f r\u2208R \u220f x,y s.t. IR=r \u03b8 ID(x,y) r + Zr\nFigure 5f shows appearance model histograms from one random training frame. Figure 5c shows the extremely noisy lane/non-lane classifications that result from the appearance model on its own, without our scene prior; accuracy is extremely low. Other, richer appearance models, such as Gaussian mixtures over RGB values (which could be either hand specified or learned), are compatible with our formulation; our simple, quantized model was chosen primarily for simplicity. We use the same generic Metropolis-Hastings strategy for inference in this problem as in our text application. Although deterministic search strategies for MAP inference could be developed for this particular program, it is less clear how to build a single deterministic search algorithm that could work on both of the generative probabilistic graphics programs we present.\nIn Table 1, we report the accuracy of our approach on one road dataset from the KITTI Vision Benchmark Suite. To focus on accuracy in the face of visual variability, we do not exploit temporal correspondences. We test on every 5th frame for a total of 80. We report lane/non-lane accuracy results for maximum likelihood classification over 10 appearance models (from 10 randomly chosen training images), as well as for the single best appearance model from this set. We use 10 posterior samples per frame for both. For reference, we include the performance of a sophisticated bottom-up baseline system from [1]. This baseline system requires significant 3D a priori knowledge, including the intrinsic and extrinsic parameters of the camera, and a rough intial segmentation of each test image. In contrast, our approach has to infer these aspects of the scene from the image data. We also show\nexamples of the uncertainty estimates that result from approximate Bayesian inference in Figure 6. Our probabilistic graphics program for this problem requires under 20 lines of probabilistic code."}, {"heading": "5 Discussion", "text": "We have shown that it is possible to write short probabilistic graphics programs that use simple 2D and 3D computer graphics techniques as the backbone for highly approximate generative models. Approximate Bayesian inference over the execution histories of these probabilistic graphics programs \u2014 automatically implemented via generic, single-variable Metropolis-Hastings transitions, using existing rendering libraries and simple likelihoods \u2014 then implements a new variation on analysis by synthesis [20]. We have also shown that this approach can yield accurate, globally consistent interpretations of real-world images, and can coherently report posterior uncertainty over latent scenes when appropriate. Our core contributions are the introduction of this conceptual framework and two initial demonstrations of its efficacy.\nTo scale our inference approach to handle more complex scenes, it will likely be important to consider more complex forms of automatic inference, beyond the single-variable Metropolis-Hastings proposals we currently use. For example, discriminatively trained proposals could help, and in fact could be trained based on forward executions of the probabilistic graphics program. Appearance models derived from modern image features and texture descriptors [13, 12] \u2014 going beyond the simple quantizations we currently use \u2014 could also reduce the burden on inference and improve the generalizability of individual programs. It is important to note that the high dimensionality involved in probabilistic graphics programming does not necessarily mean inference (and even automatic inference) is impossible. For example, approximate inference in models with probabilities bounded away from 0 and 1 can sometimes be provably tractable via sampling techniques, with runtimes that depend\non factors other than dimensionality [2]. In fact, preliminary experiments with our 2D text program (not shown) appear to show flat convergence times with up to 30 unknown letters. Exploring the role of stochasticity in facilitating tractability is an important avenue for future work.\nThe most interesting potential of generative probabilistic graphics programming is the avenue it provides for bringing powerful graphics representations and algorithms to bear on the hard modeling and inference problems in vision. For example, to avoid global re-rendering after each inference step, we need to represent and exploit the conditional independencies in the rendering process. Lifting graphics data structures such as z-buffers into the probabilistic program itself might enable this. Real-time, high-resolution graphics software contains solutions to many hard technical problems in image synthesis. Long term, we hope probabilistic extensions of these ideas ultimately become part of analogous solutions for image analysis."}, {"heading": "Acknowledgments", "text": "We are grateful to Keith Bonawitz and Eric Jonas for preliminary work exploring the feasibility of CAPTCHA breaking in Church, and to Seth Teller, Bill Freeman, Ted Adelson, Michael James and Max Siegel for helpful discussions."}], "references": [{"title": "Real time detection of lane markers in urban streets", "author": ["Mohamed Aly"], "venue": "Intelligent Vehicles Symposium,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "An optimal approximation algorithm for Bayesian inference", "author": ["Paul Dagum", "Michael Luby"], "venue": "Artificial Intelligence", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Bayesian geometric modeling of indoor scenes", "author": ["L Del Pero"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Are we ready for autonomous driving? The KITTI vision benchmark suite", "author": ["Andreas Geiger", "Philip Lenz", "Raquel Urtasun"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Church: A language for generative models", "author": ["Noah Goodman", "Vikash Mansinghka", "Daniel Roy", "Keith Bonawitz", "Joshua Tenenbaum"], "venue": "UAI", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Generative models for discovering sparse distributed representations", "author": ["Geoffrey E Hinton", "Zoubin Ghahramani"], "venue": "Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Putting objects in perspective", "author": ["Derek Hoiem", "Alexei A Efros", "Martial Hebert"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Markov chain Monte Carlo without likelihoods", "author": ["Paul Marjoram", "John Molitor", "Vincent Plagnol", "Simon Tavar\u00e9"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Recognizing objects in adversarial clutter: Breaking a visual CAPTCHA", "author": ["Greg Mori", "Jitendra Malik"], "venue": "Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["Aude Oliva", "Antonio Torralba"], "venue": "In: International journal of computer vision", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "A parametric texture model based on joint statistics of complex wavelet coefficients", "author": ["Javier Portilla", "Eero P Simoncelli"], "venue": "In: International Journal of Computer Vision", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Model criticism based on likelihood-free inference, with an application to protein network evolution", "author": ["Oliver Ratmann", "Christophe Andrieu", "Carsten Wiuf", "Sylvia Richardson"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "An overview of the Tesseract OCR engine", "author": ["Ray Smith"], "venue": "Ninth International Conference on Document Analysis and Recognition", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Image parsing: Unifying segmentation, detection, and recognition", "author": ["Zhuowen Tu", "Xiangrong Chen", "Alan L Yuille", "Song-Chun Zhu"], "venue": "In: International Journal of Computer Vision", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Image Segmentation by Data-Driven Markov Chain Monte Carlo", "author": ["Zhuowen Tu", "Song-Chun Zhu"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Approximate Bayesian computation (ABC) gives exact results under the assumption of model error", "author": ["Richard D Wilkinson"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Nonstandard interpretations of probabilistic programs for efficient inference", "author": ["David Wingate", "Noah D Goodman", "A Stuhlmueller", "J Siskind"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Vision as Bayesian inference: analysis by synthesis?", "author": ["Alan Yuille", "Daniel Kersten"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Image Parsing via Stochastic Scene Grammar", "author": ["Yibiao Zhao", "Song-Chun Zhu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}], "referenceMentions": [{"referenceID": 6, "context": "Many pattern recognition and learning techniques can then be used to build classifiers for individual scene elements, and sometimes to learn the features themselves [7, 6].", "startOffset": 165, "endOffset": 171}, {"referenceID": 5, "context": "Many pattern recognition and learning techniques can then be used to build classifiers for individual scene elements, and sometimes to learn the features themselves [7, 6].", "startOffset": 165, "endOffset": 171}, {"referenceID": 13, "context": "For example, the Tesseract system [15] for optical character recognition is over 10, 000 lines of C++.", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": "Generative models for a range of image parsing tasks are also being explored [16, 3, 17, 21, 19].", "startOffset": 77, "endOffset": 96}, {"referenceID": 2, "context": "Generative models for a range of image parsing tasks are also being explored [16, 3, 17, 21, 19].", "startOffset": 77, "endOffset": 96}, {"referenceID": 15, "context": "Generative models for a range of image parsing tasks are also being explored [16, 3, 17, 21, 19].", "startOffset": 77, "endOffset": 96}, {"referenceID": 19, "context": "Generative models for a range of image parsing tasks are also being explored [16, 3, 17, 21, 19].", "startOffset": 77, "endOffset": 96}, {"referenceID": 17, "context": "Generative models for a range of image parsing tasks are also being explored [16, 3, 17, 21, 19].", "startOffset": 77, "endOffset": 96}, {"referenceID": 15, "context": "But like traditional bottom-up pipelines for vision, these approaches have relied on considerable problem-specific engineering, chiefly to design and/or learn custom inference strategies, such as MCMC proposals [17, 21] that incorporate bottom-up cues.", "startOffset": 211, "endOffset": 219}, {"referenceID": 19, "context": "But like traditional bottom-up pipelines for vision, these approaches have relied on considerable problem-specific engineering, chiefly to design and/or learn custom inference strategies, such as MCMC proposals [17, 21] that incorporate bottom-up cues.", "startOffset": 211, "endOffset": 219}, {"referenceID": 7, "context": "For example, [8] has shown that global, 3D geometric information can significantly improve the performance of bottom-up object detectors.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Our probabilistic graphics programs are written in a variant of the Church probabilistic programming language [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 16, "context": "The approximations and stochasticity in our renderer, scene generator and likelihood models serve to implement a variant of approximate Bayesian computation [18, 10].", "startOffset": 157, "endOffset": 165}, {"referenceID": 8, "context": "The approximations and stochasticity in our renderer, scene generator and likelihood models serve to implement a variant of approximate Bayesian computation [18, 10].", "startOffset": 157, "endOffset": 165}, {"referenceID": 8, "context": "Subsequently, combinations of ABC and MCMC were proposed [10], including variants with inference over the threshold value [14].", "startOffset": 57, "endOffset": 61}, {"referenceID": 12, "context": "Subsequently, combinations of ABC and MCMC were proposed [10], including variants with inference over the threshold value [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "Most recently, extensions have been introduced where the hard cutoff is replaced with a stochastic likelihood model [18].", "startOffset": 116, "endOffset": 120}, {"referenceID": 9, "context": "These CAPTCHAs all involve arbitrary digits and letters, and as a result lack cues from word identity that the best published CAPTCHA breaking systems depend on [11].", "startOffset": 161, "endOffset": 165}, {"referenceID": 13, "context": "To calibrate the difficulty of our corpus, we also ran the Tesseract optical character recognition engine [15] on our corpus; its character detection rate was 37.", "startOffset": 106, "endOffset": 110}, {"referenceID": 3, "context": "(b) Representative test frames from the KITTI dataset [4].", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "(d) Results from [1].", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "For reference, we include the performance of a sophisticated bottom-up baseline system from [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "Aly et al [1] 68.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Table 1: Quantitative results for lane detection accuracy on one of the road datasets in the KITTI Vision Benchmark Suite [4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 18, "context": "Approximate Bayesian inference over the execution histories of these probabilistic graphics programs \u2014 automatically implemented via generic, single-variable Metropolis-Hastings transitions, using existing rendering libraries and simple likelihoods \u2014 then implements a new variation on analysis by synthesis [20].", "startOffset": 308, "endOffset": 312}, {"referenceID": 11, "context": "Appearance models derived from modern image features and texture descriptors [13, 12] \u2014 going beyond the simple quantizations we currently use \u2014 could also reduce the burden on inference and improve the generalizability of individual programs.", "startOffset": 77, "endOffset": 85}, {"referenceID": 10, "context": "Appearance models derived from modern image features and texture descriptors [13, 12] \u2014 going beyond the simple quantizations we currently use \u2014 could also reduce the burden on inference and improve the generalizability of individual programs.", "startOffset": 77, "endOffset": 85}, {"referenceID": 1, "context": "on factors other than dimensionality [2].", "startOffset": 37, "endOffset": 40}], "year": 2013, "abstractText": "The idea of computer vision as the Bayesian inverse problem to computer graphics has a long history and an appealing elegance, but it has proved difficult to directly implement. Instead, most vision tasks are approached via complex bottom-up processing pipelines. Here we show that it is possible to write short, simple probabilistic graphics programs that define flexible generative models and to automatically invert them to interpret real-world images. Generative probabilistic graphics programs consist of a stochastic scene generator, a renderer based on graphics software, a stochastic likelihood model linking the renderer\u2019s output and the data, and latent variables that adjust the fidelity of the renderer and the tolerance of the likelihood model. Representations and algorithms from computer graphics, originally designed to produce high-quality images, are instead used as the deterministic backbone for highly approximate and stochastic generative models. This formulation combines probabilistic programming, computer graphics, and approximate Bayesian computation, and depends only on general-purpose, automatic inference techniques. We describe two applications: reading sequences of degraded and adversarially obscured alphanumeric characters, and inferring 3D road models from vehicle-mounted camera images. Each of the probabilistic graphics programs we present relies on under 20 lines of probabilistic code, and supports accurate, approximately Bayesian inferences about ambiguous real-world images.", "creator": "LaTeX with hyperref package"}}}