{"id": "1307.2118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2013", "title": "A PAC-Bayesian Tutorial with A Dropout Bound", "abstract": "this tutorial gives a concise overview of existing pac - bayesian theory focusing on three generalization bounds. the first is an occam stat bound which handles rules with guaranteed finite precision parameters and which states that generalization loss is near training loss when the number of constituent bits needed to write the rule is similarly small compared to the sample size. the second is a pac - bayesian bound providing a generalization guarantee for posterior distributions unknown rather than for individual rules. the pac - bayesian bound naturally handles infinite precision rule defining parameters, $ l _ 2 $ regularization, { \\ em ^ provides a bound for dropout training }, and defines a natural notion of a single distinguished pac - bayesian posterior distribution. the third bound is a training - variance bound - - - a kind of hidden bias - variance analysis but with bias replaced by expected training loss. the training - variance bound dominates the other bounds but is more difficult problems to interpret. it seems to suggest variance reduction methods such as bagging validation and may ultimately provide a more meaningful analysis of dropouts.", "histories": [["v1", "Mon, 8 Jul 2013 15:03:03 GMT  (13kb,D)", "http://arxiv.org/abs/1307.2118v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["david mcallester"], "accepted": false, "id": "1307.2118"}, "pdf": {"name": "1307.2118.pdf", "metadata": {"source": "CRF", "title": "A PAC-Bayesian Tutorial with A Dropout Bound", "authors": ["David McAllester"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "PAC-Bayesian theory blends Bayesian and frequentist approaches to the theory of machine learning. PAC-Bayesians theory assumes a probability distribution on \u201csituations\u201d occurring in nature and a prior weighting on \u201crules\u201d expressing a learners preference for some rules over others. There is no assumed relationship between the learner\u2019s bias on rules and nature\u2019s distribution on situations. This is different from Bayesian inference where the starting point is a (perhaps subjective) joint distribution on rules and situations inducing a conditional distribution on rules given situations. The acronym PAC stands for Probably Approximately Correct and is borrowed from Valiant\u2019s notion of PAC learnability [13]. PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution. The performance guarantee involves the learner\u2019s bias and an (unrelated) sample of situations.\nThis tutorial provides a concise overview of existing PAC-Bayesian theory focusing on three bounds. The first is an Occam bound. An Occam bound assumes a discrete (countable) set of rules and bounds the loss of an individual rule. The Occam bound immediately yields guarantees for rules\nar X\niv :1\n30 7.\n21 18\nv1 [\ncs .L\nG ]\n8 J\nul 2\nwith sparse finite precision parameters. The second is a PAC-Bayesian bound governing the loss of a stochastic process which draws rules from a PAC-Bayesian rule posterior. The PAC-Bayesian bound easily handles L2 regularization of infinite-precision parameters producing bounds closely related to support vector machines. It also provides bounds for a form of dropout learning [5].\nThe third bound is a training-variance bound similar to a bias-variance analysis but with bias replaced by expected training loss. This bound assumes a given learning algorithm and provides an upper bound on the expected generalization loss in terms of the expected training loss and a measure of the variance of the output of the learning algorithm. While the training-variance bound is clearly tighter than the PAC-Bayesian bound, the training-variance bound is difficult to interpret. The training-variance bounds seems to suggest variance-reduction methods such as bagging [3].\nUnbounded loss functions, such as square loss or log loss, can lead to unstable learning algorithms. Learning algorithms that minimize training loss for an unbounded loss function tend to be overly sensitive to outliers \u2014 training points of very high loss. Learning algorithms based on bounded loss functions tend to be more robust (stable). An unbounded loss function can be converted to a bounded loss by selecting an \u201coutlier threshold\u201d Lmax and replacing the unbounded loss L by min(L,Lmax).\nPAC-Bayesian bounds apply only to bounded loss functions. We assume the loss is bounded to the interval [0, Lmax]. It is of course possible to rescale any bounded loss function into the interval [0, 1]. However, this obscures the significance of the choice of Lmax in the design of a robust versions of square loss or log loss. For this reason we leave Lmax explicit in the statement of the bounds.\nThis tutorial also discusses two improvements or clarifications of the three bounds mentioned above. The first applies the training-variance bound to the learning algorithm defined by the PAC-Bayesian posterior. Unfortunately the results suffer from looseness in the analysis and remain difficult to interpret. The second tightens the Occam bound by incorporating the loss variance into the bound. We show that the improvements achievable in this way a fundamentally limited."}, {"heading": "2 An Occam Bound", "text": "Let H be a set of \u201crules\u201d, S be a set of \u201csituations\u201d, Lmax > 0 be a real number, and L be a loss function such that for a rule h \u2208 H and a situation s \u2208 S we have that L(h, s) \u2208 [0, Lmax]. We let D be a probability distribution (measure) on S and let P be a distribution (measure) on H. We think of D as a distribution on situations occurring in nature and P as learner bias on rules. There is no assumed relationship between D and P .1 We are interested in drawing a sequence S of N situations IID from D (S \u223c DN ) and then selecting h based on S so as to minimize the \u201cgeneralization loss\u201d L(h) = Es\u223cD [L(h, s)]. When the sample S is clear from context we will write L\u0302(h) for 1\nN \u2211 s\u2208S L(h, s).\n1We should assume that the loss function L is measurable with respect to D and P . Here we will avoid this level of rigor.\nFor Occam bounds we consider the case where H is discrete (countable). An Occam bound states that with probability at least 1\u2212\u03b4 over the draw of the sample S \u223c DN we have L(h) \u2264 B(P (h), S, \u03b4) simultaneously for all h where B(P (h), S, \u03b4) is different in different bounds. While various Occam bounds have appeared in the literature, here we will consider only the following.\nTheorem 1. With probability at least 1\u2212 \u03b4 over the draw of S \u223c DN we have that the following holds simultaneously for all h.\nL(h) \u2264 inf \u03bb> 1\n2\n1\n1\u2212 1 2\u03bb\n( L\u0302(h) +\n\u03bbLmax N\n( ln 1\nP (h) + ln\n1\n\u03b4\n)) (1)\nProof. We consider the case of Lmax = 1, the case for general Lmax follows by rescaling the loss function. Define (h) by\n(h) = \u221a\u221a\u221a\u221a2L(h)(ln 1P (h) + ln 1\u03b4) N .\nFor a given h \u2208 H the relative Chernoff bound [1] states that\nPS\u223cDN ( L\u0302(h) \u2264 L(h)\u2212 (h) ) \u2264 e\u2212N (h)2 2L(h) = \u03b4P (h).\nHence, for a fixed h the probability of L(h) > L\u0302(h) + (h) is at most P (h)\u03b4. By the union bound the probability that there exists an h with L(h) > L\u0302(h) + (h) is at most the sum over h of P (h)\u03b4 which equals \u03b4. Hence we get that with probability at least 1 \u2212 \u03b4 over the draw of the sample the following holds simultaneously for all h.\nL(h) \u2264 L\u0302(h) + \u221a\u221a\u221a\u221a\u221aL(h) 2 ( ln 1 P (h) + ln 1 \u03b4 ) N  Using\n\u221a ab = inf\n\u03bb>0\na 2\u03bb + \u03bbb 2\nwe get\nL(h) \u2264 L\u0302(h) + L(h) 2\u03bb\n+ \u03bb ( ln 1 P (h) + ln 1 \u03b4 ) N\nSolving for L(h) yields the result.\nAn important observation for (1) is that there is no point in taking \u03bb to be large. Restricting \u03bb to be less than \u03bbmax increases the bound by a factor of at most 1/(1 \u2212 1/2\u03bbmax). For example, restricting \u03bb to be less than 10 increases the bound by a factor of at most 20/19. So for practical purposes we can assume that \u03bb is no larger than 10."}, {"heading": "2.1 Finite Precision Bounds", "text": "As an example application of (1) we can consider rules of the form h\u0398 for some parameter vector \u0398 \u2208 Rd where each component of the vector \u0398 is represented with a b-bit finite precision representation. In this case we can take the prior P to be uniform on the 2bd possible rules and (1) then gives that with probability at least 1\u2212 \u03b4 over the draw of the sample we have that the following holds simultaneously for all such \u0398.\nL(h\u0398) \u2264 inf \u03bb> 1\n2\n1\n1\u2212 1 2\u03bb\n( L\u0302(h\u0398) +\n\u03bbLmax N\n( (ln 2)bd+ ln 1\n\u03b4 )) We can also consider sparse representations. For \u0398 \u2208 Rd we say that \u0398 has sparsity level s if at most s components of \u0398 are non-zero. We can then represent a sparse vector by first specifying the sparsity s and then listing s pairs each of which specifies a non-zero component and its value. Intuitively we can write a rule by first using log2 d bits to specify s plus (log2 d)b bits for each pair of a component index and b-bit parameter value representation. The probability of a rule h can always be taken to be 2\u2212|h| where h is the number of bits needed to name h. Formally we avoid coding and instead defining a probability distribution where we first select s uniformly from 1 to d and then select s pairs with indices drawn uniformly form 1 to d and a parameter representation drawn uniformly from all 2b bit strings. In this case we get that with probability at least 1\u2212\u03b4 over the draw of the sample of situations we have the following holds simultaneously for all sparsity levels s and \u0398 with sparsity s and with b-bit representations for the non-zero components of \u0398.\nL(h\u0398) \u2264 inf \u03bb> 1\n2\n1\n1\u2212 1 2\u03bb\n( L\u0302(h\u0398) +\n\u03bbLmax N\n( ln d+ s(ln d+ (ln 2)b) + ln 1\n\u03b4 )) More sophisticated codings of classifiers are possible. For example, variable precision codes for real numbers can be useful when the error rate is insensitive to the precision. However, the PAC-Bayesian theorem stated in section 3 handles infinite precision parameters and seems generally preferable. The Occam bound is included here primarily because of its conceptual simplicity and the intuitive value of its proof."}, {"heading": "3 A PAC-Bayesian Bound", "text": "Let H, S, Lmax, L, D and P be defined as in section 2. We now allow the rule set H to be continuous (uncountable). Let Q be a variable ranging over distributions (measures) on the rule space H. For s \u2208 S we define the loss L(Q, s) to be Eh\u223cQ [L(h, s)]. We have that L(Q, s) is the loss of a stochastic process that selects the hypothesis h according to distribution Q. We define L(Q) to be Es\u223cD [L(Q, s)]. Given a sample S = {s1, . . . , sN} we define L\u0302(Q) to be 1\nN \u2211 s\u2208S L(Q, s). Finally we will write D(Q,P ) for\nthe Kullback-Leibler divergence from Q to P . D(Q,P ) = Eh\u223cQ [ ln Q(h)\nP (h)\n]\nA PAC-Bayesian theorem uniformly bounds L(Q) in terms of L\u0302(Q) and D(Q,P ). The first PAC-Bayesian theorem was given in [11]. Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6]. Here we will focus on the following PAC-Bayesian version of the Occam bound (1) which can be derived as a corollary of statements by Catoni [4]. A proof is included here in appendix A. Theorem 2. For \u03bb > 1 2\nselected before the draw of the sample (for any fixed \u03bb > 1/2) we have that, with probability at least 1 \u2212 \u03b4 over the draw of the sample, the following holds simultaneously for all distributions Q on H.\nL(Q) \u2264 1 1\u2212 1\n2\u03bb\n( L\u0302(Q) +\n\u03bbLmax N\n( D(Q,P ) + ln 1\n\u03b4\n)) (2)\nAs with (1), there is no point in taking \u03bb in (2) to be large \u2014 we can in practice assume that \u03bb is no larger than 10. Second, although (2) is not uniform in \u03bb we can select k different values \u03bb1, . . ., \u03bbk (all of which are selected before the draw of the sample) and by a simple union bound over these values derive that with probability at least 1\u2212 \u03b4 over the draw of the sample the following holds simultaneously for all Q.\nL(Q) \u2264 min 1\u2264i\u2264k\n1\n1\u2212 1 2\u03bbi\n( L\u0302(Q) +\n\u03bbiLmax N\n( D(Q,P ) + ln k\n\u03b4\n)) (3)\nFor minimizing the bound we can assume 1/2 \u2264 \u03bb \u2264 10 and a small number of values of \u03bb should suffice. While it is possible to give a version of this theorem that is uniform over all \u03bb > 1/2, achieving this uniformity increases the complexity of the proof."}, {"heading": "3.1 An Infinite Precision L2 Bound", "text": "As in section 2.1 we consider rules h\u03c9 with \u03c9 \u2208 Rd. Here we also assume that the rule is scale-invariant \u2014 that the rule h\u03c9 depends only on the direction of the vector \u03c9. For example linear predictors of the form\nh\u03c9(x) = argmax y\n\u03c9\u1d40\u03a6(x, y)\nare scale-invariant. For scale-invariant rules it is natural to consider the uniform distribution over the directions of \u03c9. This uniform distribution can be formalized as an isotropic unit-variance prior P = N (0, 1)d where N (0, 1) is the zero mean unit-variance Gaussian distribution. For \u0398 \u2208 Rd we define the distribution Q\u0398 to be the isotropic unit-variance Gaussian centered on \u0398. Since only the direction of \u03c9 matters, we should think of P as the uniform distribution over directions and think of Q\u0398 as a non-uniform distribution over directions. We then have the following.\nL(Q\u0398) = E \u223cN (0,1)d [L(f\u0398+ )] L\u0302(Q\u0398) = E \u223cN (0,1)d [ L\u0302(f\u0398+ ) ] D(Q\u0398, P ) = 1\n2 ||\u0398||2\nThe PAC-Bayesian bound (2) then gives that with probability at least 1 \u2212 \u03b4 over the draw of the sample the following holds simultaneously for all \u0398.\nL(Q\u0398) \u2264 1\n1\u2212 1 2\u03bb\n( L\u0302(Q\u0398) +\n\u03bbLmax N\n( 1\n2 ||\u0398||2 + ln 1 \u03b4\n)) (4)"}, {"heading": "3.2 Binary and Multi-Class classification", "text": "As an example we can consider linear binary classification. In this case we have that each situation is a pair (x, y) with y \u2208 {\u22121, 1} and we have\nh\u03c9(x) = sign(\u03c9 \u1d40\u03a6(x))\nwhere \u03a6 is a feature map such that \u03a6(x) \u2208 Rd. We also use 0-1 loss\nL(h, (x, y)) = 1h(x)6=y.\nWe then have\nL(Q\u0398, (x, y)) = P\u03c9\u223cQ\u0398 [h\u03c9(x) 6= y] = P \u223cN (0,1)( > y\u0398 \u1d40\u03a6(x)/||\u03a6(x)||).\nIn this case we have that (4) is very similar to the objective defining a support vector machine but where the hinge loss is replaced by a (non-convex) sigmoidal loss function (the cumulative of a Gaussian). In practice the rule h\u0398 is used at test time noting that \u0398 is the mean of the distribution Q\u0398.\nAs another example we can consider expected loss for multi-class classification. In this case each situation is a pair (x, y) with x \u2208 X and y \u2208 Y and where Y is small enough to be feasibly enumerated. We assume a feature map \u03a6 with \u03a6(x, y) \u2208 Rd and a loss function L\u0303 with L\u0303(y\u0302, y) \u2208 [0, Lmax]. For \u03b2 > 0 we then have the following definitions.\nh\u03c9(x, y) = \u03c9\u1d40\u03a6(x, y)\n||\u03c9|| ||\u03a6(x, y)||\nP\u03b2,\u03c9(y\u0302|x) = 1\nZ\u03b2,\u03c9,x e\u03b2h\u03c9(x,y\u0302)\nZ\u03b2,\u03c9,x = \u2211 y\u0302\u2208Y e\u03b2h\u03c9(x,y\u0302)\nL\u03b2(h\u03c9, (x, y)) = Ey\u0302\u223cP\u03b2,\u03c9(\u00b7|x)\n[ L\u0303(y\u0302, y) ] This particular formulation has the property that h\u03c9 is scale invariant (depends only on the direction of \u03c9) and \u03b2 is a parameter of the loss function. This formulation also has the property that L\u03b2(h\u03c9, (x, y)) is differentiable in \u03c9. We can then optimize the right hand side of (4) by stochastic gradient descent using\n\u2207\u0398 L\u0302(Q\u0398) = 1\nN N\u2211 i=1 E \u223cN (0,1)d [\u2207\u0398 L(h\u0398+ , si)] ."}, {"heading": "3.3 Dropouts", "text": "We now present a dropout bound inspired by the recent success of dropout training in deep neural networks [5]. This dropout bound is the only original contribution of this tutorial.\nFor a given dropout rate \u03b1 \u2208 [0, 1] and vector \u0398 \u2208 Rd we can stochastically generate a vector w \u2208 Rd by selecting, for each coordinate wi, the value 0 with probability \u03b1 (dropping the coordinate \u03c9i) or with probability 1 \u2212 \u03b1 setting \u03c9i = \u0398i + with \u223c N (0, 1). We let Q\u03b1,\u0398 denote the distribution on vectors defined by this generation process. To apply the PAC-Bayesian bound we will take Q\u03b1,0 as the prior distribution and Q\u03b1,\u0398 as the posterior distribution. The PAC-Bayesian theorem then implies that for a dropout rate \u03b1 selected before the draw of the sample we have that with probability at least 1\u2212 \u03b4 over the draw of the sample the following holds simultaneously for all \u0398.\nL(Q\u03b1,\u0398) \u2264 1\n1\u2212 1 2\u03bb\n( L\u0302(Q\u03b1,\u0398) +\n\u03bbLmax N\n( D(Q\u03b1,\u0398, Q\u03b1,0) + ln 1\n\u03b4 )) To clarify formal notation we first consider the Boolean d-cube B which is the set of vector s \u2208 Rd such that si \u2208 {0, 1} for all 1 \u2264 i \u2264 d. We will call vectors s \u2208 B \u201csparsity patterns\u201d. We let S\u03b1 be the distribution on the d-cube B (the distribution on sparsity patterns) generated by selecting each si independently with the probability of si = 0 being \u03b1. For a sparsity pattern s and for \u03c9 \u2208 Rd we will write s \u25e6 \u03c9 for the Hadamard product defined by (s \u25e6 \u03c9)i = si\u03c9i. We then have that a draw from Q\u03b1,\u0398 can be made by first drawing a sparsity pattern s \u223c S\u03b1 and a noise vector \u223c N (0, 1)d and then constructing s \u25e6 (\u0398 + ). More formally we have the following.\nE\u03c9\u223cQ\u03b1,\u0398 [f(\u03c9)] = Es\u223cS\u03b1, \u223cN (0,1)d [f(s \u25e6 (\u0398 + ))]\nWe then have\nD(Q\u03b1,\u0398, Q\u03b1,0) = Es\u223cS\u03b1, \u223cN (0,1)d\n[ ln S\u03b1(s)e \u2212 1 2 ||s\u25e6 ||2\nS\u03b1(s)e \u2212 1 2 ||s\u25e6(\u0398+ )||2\n]\n= Es\u223cS\u03b1\n[ 1\n2 ||s \u25e6\u0398||2\n]\n= 1\u2212 \u03b1\n2 ||\u0398||2\nThe PAC-Bayesian bound then gives that, for a dropout rate \u03b1 selected before the draw of the sample, with probability at least 1\u2212\u03b4 over the draw of the sample the following holds simultaneously for all \u0398.\nL(Q\u03b1,\u0398) \u2264 1\n1\u2212 1 2\u03bb\n( L\u0302(Q\u03b1,\u0398) +\n\u03bbLmax N\n( 1\u2212 \u03b1\n2 ||\u0398||2 + ln 1 \u03b4\n)) (5)\nComparing (5) with (4) we see that a dropout rate of \u03b1 reduces the complexity cost by a factor of 1 \u2212 \u03b1. However, for \u03b1 very small we expect\nL\u0302(Q\u03b1,\u0398) to be large. We can optimize the right hand side of this bound by stochastic gradient descent using the following.\n\u2207\u0398 L\u0302(Q\u03b1,\u0398) = 1\nN N\u2211 i=1 Es\u223cS\u03b1, \u223cN (0,1)d [ \u2207\u0398L(hs\u25e6(\u0398+ ), si) ]"}, {"heading": "3.4 The PAC-Bayesian Posterior", "text": "It is important to note that (2) has a closed-form solution for the distribution Q minimizing the bound.\nQ\u2217 = argmin Q L\u0302(Q) + \u03bbLmax N D(Q,P )\nIn the case where the rule space H is finite we have the constraint that\u2211 h\u2208HQ(h) = 1 and a straightforward application of the KTT conditions yields the following.\nQ\u2217(h) = Q\u03bb(h) = 1\nZ\u03bb P (h)e\n\u2212NL\u0302(h) \u03bbLmax\nZ\u03bb = Eh\u223cP [ e \u2212 N \u03bbLmax L\u0302(h) ]\nHere we can think of Q\u03bb as \u201cthe\u201d PAC-Bayesian posterior distribution for regularization parameter \u03bb. It is important to note that the choice of Lmax strongly influences the posterior distribution. In the case of (3) we can optimize over \u03bb as follows.\nQ\u2217 = Q\u03bbi\u2217 i \u2217 = argmin\n1\u2264i\u2264k L\u0302(Q\u03bbi) + \u03bbLmax N D(Q\u03bbi , P )\nL(Q\u2217) \u2264 min 1\u2264i\u2264k\n1\n1\u2212 1 2\u03bbi\n( L\u0302(Q\u03bbi) +\n\u03bbiLmax N\n( D(Q\u03bbi , P ) + ln k\n\u03b4\n))"}, {"heading": "4 A Training-Variance Bound", "text": "We now consider a fixed learning algorithm A which takes as input a sample S \u223c DN and returns a rule distribution QA(S). For a given learning algorithm A we now consider the expected loss ES\u223cDN [L(QA(S))] and the expected posterior\nQ\u0304A(h) = ES\u223cDN [QA(S)(h)] .\nThe training-variance bound is the following where we will write ES [f(S)] for ES\u223cDN [f(S)]. Theorem 3. For any fixed learning algorithm A and for \u03bb > 1 2 we have\nES [L(QA(S))] \u2264 11\u2212 1 2\u03bb\n( ES [ L\u0302(QA(S)) ] + \u03bbLmax N ES [ D(QA(S), Q\u0304A) ]) . (6)\nWe can think of ES [ D(QA(S), Q\u0304A) ] as a measure of the variation of QA(S) over the draw of S. For the PAC-Bayesian bound (2) we have a closed form solution for the optimal posterior. But for the trainingvariance bound (6) we do not have a solution for the optimal algorithm. The training-variance bound seems to motivate variance-reduction methods such as bagging [3].\nThe training-variance bound is an immediate corollary of the following more general theorem which is implicit in Catoni [4] and which is proved in appendix B.\nTheorem 4. For any rule distribution P , learning algorithm A, and for \u03bb > 1\n2 , we have\nES [L(QA(S))] \u2264 11\u2212 1 2\u03bb\n( ES [ L\u0302(QA(S)) ] + \u03bbLmax\nN ES [D(QA(S), P )]\n) . (7)\nIt was observed by Langford [8] that the rule distribution P minimizing ES [D(QA(S), P )] is Q\u0304A. This can be shown as follows.\nES [D(QA(S), P )] = ES, h\u223cQA(S) [ ln QA(S)(h)\nP (h)\n]\n= ES, h\u223cQA(S)\n[ ln QA(S)(h)\nQ\u0304A(h)\n] + Eh\u223cQ\u0304A [ ln Q\u0304(h)\nP (h) ] = ES [ D(QA(S), Q\u0304A) ] +D(Q\u0304A, P )\nThis shows that (6) dominates (7) and is much better when D(Q\u0304A, P ) is large.\nFor a given learning algorithm A we can insert Q\u0304A for P in the PACBayesian bound (2) yielding the following high confidence version of (6).\nTheorem 5. For any given learning algorithm A and \u03bb > 1 2\nwe have the following with probability at least 1\u2212 \u03b4 over the draw of the sample.\nL(QA(S)) \u2264 11\u2212 1 2\u03bb\n( L\u0302(QA(S)) +\n\u03bbLmax N ( D(QA(S), Q\u0304A) + ln 1\u03b4 )) (8)"}, {"heading": "5 Applying the Training-Variance Bound", "text": "to the PAC-Bayesian Posterior\nWe now consider the learning algorithm that maps a sample S to the PAC-Bayesian posterior Q\u03bb(S). Here \u03bb is a parameter of the learning algorithm. We should note that, although Q\u03bb is the posterior optimizing the PAC-Bayesian bound (2), it seems unlikely that Q\u03bb is the algorithm optimizing the training-variance bound (6). Also, as we will see below, the analysis given here is somewhat loose.\nFollowing Catoni [4] and Lever et al. [9] we approximate Q\u0304\u03bb with the following.\nQ\u0308\u03bb(h) = 1\nZ\u0308\u03bb P (h)e\n\u2212NL(h) \u03bbLmax\nZ\u0308\u03bb = Eh\u223cP [ e \u2212NL(h) \u03bbLmax ]\nInserting Q\u0308\u03bb for P in (7) gives that for \u03b3 > 1 2 we have the following.\nES [L(Q\u03bb(S))] \u2264 11\u2212 1 2\u03b3\n( ES [ L\u0302(Q\u03bb(S)) ] + \u03b3Lmax\nN ES\n[ D(Q\u03bb(S), Q\u0308\u03bb) ]) (9)\nNote that we allow \u03b3 to be different from \u03bb. We now have the following bound from Catoni [4] and whose proof is given in appendix C.\nES [ D(Q\u03bb(S), Q\u0308\u03bb) ] \u2264 N \u03bbLmax ( ES [L(Q\u03bb(S))]\u2212 ES [ L\u0302(Q\u03bb(S)) ]) (10)\nBy inserting (10) into (9), setting \u03b3 = 1 2 \u03bb and solving for ES [L(Q\u03bb(S))] one can derive the following for \u03bb > 2.\nES [L(Q\u03bb(S))] \u2264 11\u2212 2 \u03bb ES\n[ L\u0302(Q\u03bb(S)) ] (11)\nNote that 1 2\u03bb\nin the Occam and PAC-Bayesian bound has been replaced with 2\n\u03bb . Also note that Lmax appears in the definition of Q\u03bb(S).\nTo get a corresponding high-confidence bound we first note that by inserting Q\u0308\u03bb for P in the PAC-Bayesian bound (2) we get that, for \u03b3 >\n1 2 ,\nwith probability at least 1\u2212 \u03b4 over the draw of the sample we have\nL(Q\u03bb(S)) \u2264 11\u2212 1 2\u03b3\n( L\u0302(Q\u03bb(S)) + \u03b3Lmax N ( D(Q\u03bb(S), Q\u0308\u03bb) + ln 1\u03b4 )) . (12)\nThis can be combined with the following whose proof is given in appendix C.\nLemma 1. For \u03bb > 0, with probability at least 1\u2212 \u03b4 over the draw of the sample we have\nD(Q\u03bb(S), Q\u0308\u03bb) \u2264 N\n\u03bbLmax\n( L(Q\u03bb(S))\u2212 L\u0302(Q\u03bb(S)) ) + N\n\u03bb\n\u221a ln 1\n\u03b4\n2N . (13)\nTaking a union bound over (13) and (12) so that both are true simultaneously, then inserting (13) into (12), setting \u03b3 = 1\n2 \u03bb, and solving for\nL(Q\u03bb(S)), yields that with probability at least 1\u2212 \u03b4 over the draw of the sample we have\nL(Q\u03bb(S)) \u2264 1\n1\u2212 2 \u03bb\nL\u0302(Q\u03bb(S)) + Lmax \u221a ln 2 \u03b4\n2N + \u03bbLmax ln\n2 \u03b4\nN  (14) Improvements in these bounds should be possible. To see this consider\nthe PAC-Bayesian bound (2) for which Q\u03bb(S) is the optimal posterior.\nL(Q\u03bb(S)) \u2264 1\n1\u2212 1 2\u03bb\n( L\u0302(Q\u03bb(S)) +\n\u03bbLmax N\n( D(Q\u03bb(S), P ) + ln 1\n\u03b4 )) Replacing P by Q\u0304\u03bb should significantly improve this bound. However, replacing P by Q\u0308\u03bb and then inserting (13) makes the bound vacuous."}, {"heading": "6 Incorporating Empirical Loss Variance", "text": "For a given rule h and sample S = {s1, . . . , sn} one can measure an empirical loss variance.\n\u03c3\u03022(h) = 1\nN \u2212 1 N\u2211 i=1 (L(h, si)\u2212 L\u0302(h))2\nIt is natural to ask whether tighter bounds are possible if we allow the bounds to involve \u03c3\u03022(h). Audibert, Munos and Szepesvari [2] give a bound motivated by this question. Consider a random variable x \u2208 [0, Lmax] with expectation \u00b5 and an IID sample {x1, . . . , xn} with empirical mean \u00b5\u0302 and empirical variance \u03c3\u03022. Audibert, Munos and Szepesvari prove that the following holds with probability at least 1 \u2212 \u03b4 over the draw of the sample.\n\u00b5 \u2264 \u00b5\u0302+\n\u221a 2\u03c3\u03022 ln 3\n\u03b4\nN +\n3Lmax ln 3 \u03b4\nN\nTaking a union bound over a prior P we get that with probability at least 1\u2212 \u03b4 the following holds for all h \u2208 H.\nL(h) \u2264 L\u0302(h) + \u221a\u221a\u221a\u221a2\u03c3\u03022(h)(ln 1P (h) + ln 3\u03b4) N + 3Lmax ( ln 1 P (h) + ln 3 \u03b4 ) N\nTo show the limitations of these bounds we consider the best possible case where \u03c3\u03022(h) = 0. For this case we have the following theorem whose proof is given in appendix D.\nTheorem 6. With probability at least 1\u2212 \u03b4 over the draw of the sample we have that the following holds for all h such that \u03c3\u03022(h) = 0.\nL(h) \u2264 L\u0302(h) + Lmax\n( ln 1\nP (h) + ln 1 \u03b4 ) N \u2212 1 (15)\nThe inequality (15) is essentially the best that can be done using a union bound over P (h). The basic idea is that even if \u03c3\u03022(h) = 0 one cannot rule out the possibility of outliers which happened not to occur in the data. The probability of outliers cannot be bounded to be less than (ln 1\nP (h) + ln 1 \u03b4 )/N and an outlier can have loss Lmax so (15) is the best that can be done. But (15) is not significantly tighter than the general Occam bound (1). In particular, by taking \u03bb = 1 in (1) we get a bound that is only a factor of 2 worse than (15). If (15) is dominated by L\u0302(h) then we can take \u03bb in the Occam bound (1) to be large and the two bounds are essentially the same."}, {"heading": "7 Conclusion", "text": "This paper focuses on three generalization bounds \u2014 an Occam bound, a PAC-Bayesian bound, and a training-variance bound. The Occam\nbound and PAC-Bayesian bound seem to be important primarily because they provide the conceptual foundation required for the proof of the training-variance bound which dominates the other two. While the PAC-Bayesian posterior defines a learning algorithm optimizing the PACBayesian bound, there is no known analogous optimal algorithm for the training-variance bound. The bound seems to suggest variance reduction methods such as boosting. There is clearly room for improved theoretical understanding of the consequences of the training-variance bound."}, {"heading": "A Proof of Theorem 2", "text": "All proofs in these appendices are adapted from Catoni [4] except for the proof of theorem 6 which is straightforward.\nThe theorem states that for \u03bb > 1 2\nselected before the draw of the sample (for any fixed \u03bb > 1/2) we have that, with probability at least 1\u2212 \u03b4 over the draw of the sample, the following holds simultaneously for all distributions Q on H.\nL(Q) \u2264 1 1\u2212 1\n2\u03bb\n( L\u0302(Q) +\n\u03bbLmax N\n( D(Q,P ) + ln 1\n\u03b4 )) We will consider the case of Lmax = 1, the general case follows by rescaling the loss function. For real numbers p, q \u2208 [0, 1] we define D(q, p) to be the divergence from a Bernoulli variable with bias q to a Bernoulli variable with bias p.\nD(q, p) = q ln q p + (1\u2212 q) ln 1\u2212 p 1\u2212 q\nFor a real number \u03b3 we define\nD\u03b3(q, p) = \u03b3q \u2212 ln (1\u2212 p+ pe\u03b3) .\nBy a straightforward optimization over \u03b3 one can show\nD(q, p) = sup \u03b3 D\u03b3(q, p). (16)\nNow consider a random variable x with x \u2208 [0, 1] and with mean \u00b5. Let \u00b5\u0302 be the mean of N independent draws of x. We first show that for any fixed \u03b3 we have\nE [ eND\u03b3(\u00b5\u0302,\u00b5) ] \u2264 1. (17)\nTo see this note that E [ eN\u03b3\u00b5\u0302 ] = (E [e\u03b3x])N . For x \u2208 [0, 1] we note that the convexity of the exponential function implies e\u03b3x \u2264 1\u2212 x+ xe\u03b3 . This gives E [ eN\u03b3\u00b5\u0302 ] \u2264 (1 \u2212 \u00b5 + \u00b5e\u03b3)N . Dividing by the right hand side gives\nE [ eN(\u03b3\u00b5\u0302\u2212ln(1\u2212\u00b5+\u00b5e \u03b3)) ] \u2264 1 which is the same as (17). It is interesting to\nnote that ES\u223cDN [ eND(\u00b5\u0302,\u00b5) ] = ES\u223cDN [ sup \u03b3 eND\u03b3(\u00b5\u0302,\u00b5) ]\n\u2265 sup \u03b3\nES\u223cDN [ eND\u03b3(\u00b5\u0302,\u00b5) ] \u2264 1\nFor h fixed (17) implies the following. ES\u223cDN [ eND\u03b3(L\u0302(h),L(h)) ] \u2264 1\nEh\u223cP [ ES\u223cDN [ eND\u03b3(L\u0302(h),L(h)) ]] \u2264 1\nES\u223cDN [ Eh\u223cP [ eND\u03b3(L\u0302(h),L(h)) ]] \u2264 1 (18)\nApplying Markov\u2019s inequality to (18) we get that with probability at least 1\u2212 \u03b4 over the draw of S we have\nEh\u223cP [ eND\u03b3(L\u0302(h),L(h,P )) ] \u2264 1 \u03b4 . (19)\nNext we observe the shift of measure lemma Eh\u223cQ [f(h)] \u2264 D(Q,P ) + ln Eh\u223cP [ ef(h) ] (20)\nwhich can be derived as follows. Eh\u223cQ [f(h)] = Eh\u223cQ [ ln ef(h) ] = Eh\u223cQ [ ln P (h)\nQ(h) ef(h) + ln\nQ(h)\nP (h) ] \u2264 ln Eh\u223cQ [ P (h)\nQ(h) ef(h)\n] +D(Q,P )\n= D(Q,P ) + ln Eh\u223cP [ ef(h) ] Setting f(h) = ND\u03b3(L\u0302(h), L(h)) in (20) and using (19) we get\nEh\u223cQ [ ND\u03b3(L\u0302(h), L(h)) ] \u2264 D(Q,P ) + ln 1\n\u03b4 .\nNoting that D\u03b3(q, p) is jointly convex in q and p we get\nD\u03b3(L\u0302(Q), L(Q)) \u2264 1\nN\n( D(Q,P ) + ln 1\n\u03b4\n) . (21)\nTheorem 2 is now implied by the following lemma.\nLemma 2. For \u03bb > 1 2\n, if D\u2212 1 \u03bb (p, q) \u2264 c then p \u2264 1 1\u2212 1\n2\u03bb\n(q + \u03bbc).\nProof. Let \u03b3 abbreviate \u2212 1 \u03bb . We are given q\u03b3\u2212ln (1\u2212 p+ pe\u03b3) \u2264 c. Since \u03bb > 1\n2 we have \u03b3 \u2208 (\u22122, 0). We then get\np \u2264 1\u2212 e \u03b3q\u2212c\n1\u2212 e\u03b3 .\nApplying e\u03b3 \u2265 1 + \u03b3 in the numerator and e\u03b3 \u2264 1 + \u03b3 + 1 2 \u03b32 \u2264 1 for \u03b3 \u2208 (\u22122, 0) in the denominator we get\np \u2264 \u2212\u03b3q + c \u2212\u03b3 \u2212 1\n2 \u03b32\n= q \u2212 c \u03b3\n1 + 1 2 \u03b3\nReplacing \u03b3 by \u22121/\u03bb proves the lemma."}, {"heading": "B Proof of Theorem 4", "text": "The theorem states that for distribution P on rules, any algorithm A, and for \u03bb > 1\n2 , we have\nES [L(QA(S))] \u2264 11\u2212 1 2\u03bb\n( ES [ L\u0302(QA(S)) ] + \u03bbLmax\nN ES [D(QA(S), P )]\n) .\nProof. The proof is a slight modification of the proof of theorem 2 given in section A. By the shift of measure lemma (20) we have the following for any fixed sample S.\nEh\u223cQA(S)\n[ ND\u03b3(L\u0302(h), L(h)) ] \u2264 D(QA(S), P )+ln Eh\u223cP [ eND\u03b3(L\u0302(h),L(h)) ] .\nBy the joint convexity of D\u03b3 we then have\nD\u03b3(L\u0302(QA(S)), L(QA(S))) \u2264 1\nN\n( D(QA(S), P ) + ln Eh\u223cP [ eND\u03b3(L\u0302(h),L(h)) ]) .\nTaking the expectation of both sides with respect to S and using the convexity of D\u03b3 and the concavity of ln we get\nD\u03b3 ( ES [ L\u0302(QA(S)) ] ,ES [L(QA(S))] ) \u2264 1\nN\n( ES [D(QA(S), P )] + ln Eh\u223cP,S\u223cDN [ eND\u03b3(L\u0302(h),L(h)) ]) .\nTheorem 4 now follows from (17) and lemma 2."}, {"heading": "C Proof of (10) and (13)", "text": "(10) is the following.\nES [ D(Q\u03bb(S), Q\u0308\u03bb) ] \u2264 N \u03bbLmax ( ES [L(Q\u03bb(S))]\u2212 ES [ L\u0302(Q\u03bb(S)) ]) Proof.\nES [ D(Q\u03bb(S), Q\u0308\u03bb) ] = ES,h\u223cQ\u03bb(S) [ ln Q\u03bb(S)(h)\nQ\u0308\u03bb(h)\n]\n= ES,h\u223cQ\u03bb(S)\n[ N\n\u03bbLmax L(h)\u2212 N \u03bbLmax L\u0302(h)) ] \u2212ES [lnZ\u03bb(S)] + ln Z\u0308\u03bb\n= N\n\u03bbLmax\n( ES [L(Q\u03bb(S))]\u2212 ES [ L\u0302(Q\u03bb(S)) ]) \u2212ES [lnZ\u03bb(S)] + ln Z\u0308\u03bb\nBut the log partition function is convex in energy which gives ES [lnZ\u03bb(S)] = ES [ ln Eh\u223cP [ e \u2212 N \u03bbLmax L\u0302(h) ]]\n\u2265 ln Eh\u223cP [ e \u2212 N \u03bbLmax ES[L\u0302(h)] ] = ln Eh\u223cP [ e \u2212 N \u03bbLmax L(h) ]\n= Z\u0308\u03bb\n(13) states that with probability at least 1\u2212 \u03b4 we have\nD(Q\u03bb(S), Q\u0308\u03bb) \u2264 N\n\u03bbLmax\n( ES [ L(Q\u03bb(S))\u2212 L\u0302(Q\u03bb(S)) ]) + N\n\u03bb\n\u221a ln 1\n\u03b4\n2N .\nProof. D(Q\u03bb(S), Q\u0308\u03bb) = Eh\u223cQ\u03bb(S) [ ln Q\u03bb(S)(h)\nQ\u0308\u03bb(h)\n]\n= Eh\u223cQ\u03bb(S)\n[ N\n\u03bbLmax L(h)\u2212 N \u03bbLmax L\u0302(h)) ] \u2212 lnZ\u03bb(S) + ln Z\u0308\u03bb\n= N\n\u03bbLmax\n( L(Q\u03bb(S))\u2212 L\u0302(Q\u03bb(S))) ) \u2212 lnZ\u03bb(S) + ln Z\u0308\u03bb\nlnZ\u03bb(S) = ln Eh\u223cP [ e \u2212 N \u03bbLmax L\u0302(h) ]\n= ln Eh\u223cQ\u0308\u03bb\n[ P (h)\nQ\u0308\u03bb(h) e \u2212 N \u03bbLmax L\u0302(h)\n]\n\u2265 Eh\u223cQ\u0308\u03bb\n[ ln P (h)\nQ\u0308\u03bb(h) \u2212 N \u03bbLmax L\u0302(h)\n]\n= ln Z\u0308\u03bb + N( L(Q\u0308\u03bb)\u2212 L\u0302(Q\u0308\u03bb) )\n\u03bbLmax\nSince \u03bb is selected before the draw of the sample, a Hoeffding bound can be used to bound L(Q\u0308\u03bb) \u2212 L\u0302(Q\u0308\u03bb) yielding that with probability at least 1\u2212 \u03b4 over the draw of the sample we have\nlnZ\u03bb(S) \u2265 ln Z\u0308\u03bb \u2212 N\n\u03bb\n\u221a ln 1\n\u03b4\n2N . (22)"}, {"heading": "D Proof of Theorem 6", "text": "The theorem states that with probability at least 1\u2212\u03b4 over the draw of the sample we we have that the following holds for all h such that \u03c3\u03022(h) = 0.\nL(h) \u2264 L\u0302(h) + Lmax\n( ln 1\nP (h) + ln 1 \u03b4 ) N \u2212 1\nProof. Consider a sample S = {s1, . . . , sn}. We let the sample s1 define a target loss value for each rule. We consider a sample si for i > 1 to be an \u201coutlier\u201d for rule h if L(h, si) 6= L(h, s1). We can then use the standard \u201crealizable\u201d analysis over the sample {s2, . . . , sN} to bound the outlier rate. More specifically, let \u00b5(h) be the probability that a new draw of a situation from D is an outlier for h.\n\u00b5(h) = Ps\u223cD(L(h, s) 6= L(h, s1))\nWe will first show that with probability at least 1 \u2212 \u03b4 over the draw of {s2, . . . , sN} we have that the following holds simultaneously for all h such that \u03c3\u03022(h) = 0.\n\u00b5(h) \u2264 ln 1 P (h) + ln 1 \u03b4\nN \u2212 1 (23)\nThe probability over the draw of {s2, . . . , sN} \u223c DN\u22121 that \u03c3\u03022(h) = 0 equals (1 \u2212 \u00b5(h))N\u22121 \u2264 e\u2212(N\u22121)\u00b5(h). So if h violates (23) then the probability that \u03c32(h) = 0 is at most P (h)\u03b4. By the union bound the probability that there exists an h with \u03c3\u03022(h) = 0 and violating (23) is at most \u2211 h P (h)\u03b4 = \u03b4 and thus with high probability (23) holds for all h. The theorem then follows from the observation that if \u03c3\u03022(h) = 0 then L\u0302(h) = L(h, s1) and L(h) \u2264 L(h, s1) + Lmax\u00b5(h)."}], "references": [{"title": "Fast probabilistic algorithms for hamiltonian circuits and matchings", "author": ["Dana Angluin", "Leslie G Valiant"], "venue": "In Proceedings of the ninth annual ACM symposium on Theory of computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1977}, {"title": "Use of variance estimation in the multi-armed bandit problem", "author": ["Jean-Yves Audibert", "R\u00e9mi Munos", "Csaba Szepesvari"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Pac-bayesian supervised classification: the thermodynamics of statistical learning", "author": ["Olivier Catoni"], "venue": "arXiv preprint arXiv:0712.0248,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "New types of deep neural network learning for speech recognition and related applications: An overview", "author": ["Li Deng", "Geoffrey Hinton", "Brian Kingsbury"], "venue": "Proc. ICASSP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Pac-bayesian learning of linear classifiers", "author": ["Pascal Germain", "Alexandre Lacasse", "Fran\u00e7ois Laviolette", "Mario Marchand"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Tutorial on practical prediction theory for classification", "author": ["John Langford"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Microchoice bounds and self bounding learning algorithms", "author": ["John Langford", "Avrim Blum"], "venue": "In Proceedings of the twelfth annual conference on Computational learning theory,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Distribution-dependent pac-bayes priors", "author": ["Guy Lever", "Fran\u00e7ois Laviolette", "John Shawe-Taylor"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "A note on the pac-bayesian theorem", "author": ["Andreas Maurer"], "venue": "arXiv preprint cs/0411099,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Pac-bayesian model averaging", "author": ["David A. McAllester"], "venue": "In COLT, pages 164\u2013170,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Pac-bayesian generalisation error bounds for gaussian process classification", "author": ["Matthias Seeger"], "venue": "J. Mach. Learn. Res,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 10, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 8, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 5, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 2, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 4, "context": "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a \u201cposterior\u201d distribution.", "startOffset": 35, "endOffset": 56}, {"referenceID": 3, "context": "It also provides bounds for a form of dropout learning [5].", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "It is of course possible to rescale any bounded loss function into the interval [0, 1].", "startOffset": 80, "endOffset": 86}, {"referenceID": 0, "context": "For a given h \u2208 H the relative Chernoff bound [1] states that", "startOffset": 46, "endOffset": 49}, {"referenceID": 9, "context": "The first PAC-Bayesian theorem was given in [11].", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].", "startOffset": 64, "endOffset": 81}, {"referenceID": 8, "context": "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].", "startOffset": 64, "endOffset": 81}, {"referenceID": 5, "context": "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].", "startOffset": 64, "endOffset": 81}, {"referenceID": 2, "context": "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].", "startOffset": 64, "endOffset": 81}, {"referenceID": 4, "context": "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].", "startOffset": 64, "endOffset": 81}, {"referenceID": 2, "context": "Here we will focus on the following PAC-Bayesian version of the Occam bound (1) which can be derived as a corollary of statements by Catoni [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "We now present a dropout bound inspired by the recent success of dropout training in deep neural networks [5].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "For a given dropout rate \u03b1 \u2208 [0, 1] and vector \u0398 \u2208 R we can stochastically generate a vector w \u2208 R by selecting, for each coordinate wi, the value 0 with probability \u03b1 (dropping the coordinate \u03c9i) or with probability 1 \u2212 \u03b1 setting \u03c9i = \u0398i + with \u223c N (0, 1).", "startOffset": 29, "endOffset": 35}, {"referenceID": 2, "context": "The training-variance bound is an immediate corollary of the following more general theorem which is implicit in Catoni [4] and which is proved in appendix B.", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "It was observed by Langford [8] that the rule distribution P minimizing ES [D(QA(S), P )] is Q\u0304A.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "Following Catoni [4] and Lever et al.", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "[9] we approximate Q\u0304\u03bb with the following.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "We now have the following bound from Catoni [4] and whose proof is given in appendix C.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "Audibert, Munos and Szepesvari [2] give a bound motivated by this question.", "startOffset": 31, "endOffset": 34}], "year": 2013, "abstractText": "This tutorial gives a concise overview of existing PAC-Bayesian theory focusing on three generalization bounds. The first is an Occam bound which handles rules with finite precision parameters and which states that generalization loss is near training loss when the number of bits needed to write the rule is small compared to the sample size. The second is a PAC-Bayesian bound providing a generalization guarantee for posterior distributions rather than for individual rules. The PAC-Bayesian bound naturally handles infinite precision rule parameters, L2 regularization, provides a bound for dropout training, and defines a natural notion of a single distinguished PAC-Bayesian posterior distribution. The third bound is a training-variance bound \u2014 a kind of bias-variance analysis but with bias replaced by expected training loss. The training-variance bound dominates the other bounds but is more difficult to interpret. It seems to suggest variance reduction methods such as bagging and may ultimately provide a more meaningful analysis of dropouts.", "creator": "LaTeX with hyperref package"}}}