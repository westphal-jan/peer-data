{"id": "1307.2191", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2013", "title": "A Knowledge-based Treatment of Human-Automation Systems", "abstract": "in evaluating a supervisory control system the true human agent knowledge of past, or current, and future system behavior is critical for system performance. being able to reason better about that knowledge in a often precise and structured manner is central to effective system design. in this paper we introduce the application of a well - established formal education approach to reasoning about knowledge to the modeling and analysis of complex human - automation systems. an intuitive notion of knowledge in human - automation systems is sketched and then cast as standard a formal learning model. we present both a case study in which the approach is used to model and reason about a classic problem from the human - automation systems literature ; the results of our concept analysis provide evidence perhaps for the validity and value of dynamic reasoning about complex systems in terms specific of governing the knowledge of the system agents. to conclude, we discuss research design directions underway that will extend this approach, include and note several systems in the aviation and human - robot team building domains that are of particular interest.", "histories": [["v1", "Mon, 8 Jul 2013 18:07:31 GMT  (581kb)", "http://arxiv.org/abs/1307.2191v1", "39 pages, 1 figure"]], "COMMENTS": "39 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.HC cs.AI", "authors": ["yoram moses", "marcia k shamo technion - israel institute of technology)"], "accepted": false, "id": "1307.2191"}, "pdf": {"name": "1307.2191.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Marcia K. Shamo"], "emails": [], "sections": [{"heading": null, "text": "behavior is critical for system performance. Being able to reason about that knowledge in a precise and structured manner is central to effective system design. In this paper we introduce the application of a wellestablished formal approach to reasoning about knowledge to the modeling and analysis of complex humanautomation systems. An intuitive notion of knowledge in human-automation systems is sketched and then cast as a formal model. We present a case study in which the approach is used to model and reason about a classic problem from the human-automation systems literature; the results of our analysis provide evidence for the validity and value of reasoning about complex systems in terms of the knowledge of the system\u2019s agents. To conclude, we discuss research directions that will extend this approach, and note several systems in the aviation and human-robot team domains that are of particular interest.\nIndex Terms\u2014Formal specifications, knowledge in multi-agent systems, man-machine systems.\nI. INTRODUCTION\nhuman operator\u2019s knowledge regarding a complex system\u2019s past, current, and future behavior is elemental to\nsystem performance. Indeed, issues of the existence, content, and validity of operator knowledge in a complex human-automation system underlie many major threads of research in the human-machine systems community.\nThese research threads include broad and active domains such as situation awareness (briefly, an operator\u2019s knowledge of the relevant elements in the environment), mode awareness (operator knowledge of a system\u2019s current\nA\noperational state), and mental models (knowledge and beliefs regarding a system\u2019s components, possible behaviors, and interdependencies). In a basic sense, all these domains are concerned with aspects of the operator\u2019s knowledge and aim to define both theoretical frameworks and evaluative criteria for determining whether that knowledge is satisfactory for system performance.\nSince human operator knowledge is a fundamental element of system performance, we argue that it is important to reason about that knowledge, and its role in controlling system performance, in a precise and structured manner. Moreover, as increasingly complex human-automation and human-robot systems are developed, the need for tools that support the design and evaluation of these systems from the perspective of the agents' knowledge becomes an imperative. Consider, for instance, the knowledge possessed by the human and automation agents in advanced flight deck information systems, in systems of human supervisory controllers and multiple autonomous vehicles, or in robot-assisted search and rescue teams. Sophisticated tools are required to effectively analyze the complex and subtle interactions between what the human and non-human agents know in these and similar systems.\nTo be useful, a reasoning approach must satisfy a number of modeling and analysis requirements. In particular,\nknowledge in a system must be a well-defined entity or resource that can be directly represented, manipulated, and analyzed. Properties of knowledge unique to human agents must be expressible. Since our focus is the correct performance of the complete system, and since non-human agents play equally important albeit different roles, we will in many cases need to be able to capture and reason about the 'knowledge' of the non-human agents in the system. Finally, since systems are dynamic, it will be necessary to represent the evolution of knowledge over time, and how that knowledge both influences and is influenced by system behavior.\nIn this paper we introduce a formal approach to reasoning about knowledge in human-automation systems that is\nbased on a model of knowledge developed by Halpern and Moses and their colleagues [1]-[3]. Their methodology combines the formal rigor of epistemic logic with intuitive and direct notions of knowledge and action in multiagent systems, and thus provides a means of reasoning about systems at the level of the agents' knowledge. To date, existing applications of the approach focus primarily on systems in which all agents are non-human (e.g., communication protocols for distributed computer systems [1], [4], robot motion planning [5], adding notions of knowledge and communication to discrete event control systems [6], [7]). We argue that this formalism can be equally valuable for reasoning about knowledge in systems in which one or more agents are human, if properties of knowledge that are unique to human agents can be expressively captured.\nThe value of reasoning about knowledge in the analysis of socio-technical systems has already been noted in the\nliterature [8], [9]. In the papers cited, the authors discuss an approach combining knowledge, timed automata and Activity Theory, and propose a case study of an aviation accident. From the brief description presented, it appears that their work focuses on the temporal aspect and does not model knowledge explicitly. Our approach makes use of a fine-grained analysis of knowledge based on a careful model of the different entities' local states, along the lines of Fagin et al.[1]. This enables us to establish rigorous claims regarding the roles of knowledge, and the lack of knowledge, in human-automation interaction and supervisory control of complex systems. Our contribution is thus to propose a cohesive formal framework for the modeling and analysis of knowledge in these systems.\nIn this article, we present the theoretical foundations of our approach, and sketch a case study. The article is\nstructured as follows. Within the domain of complex human-automation systems in which the human\u2019s role is that of supervisory controller, we first define more precisely a notion of human knowledge in a complex system. We also put forth basic criteria for what it means for a human operator's knowledge to be satisfactory for system performance. We then describe the main concepts and elements of the knowledge formalism as developed by Fagin et al., and present a variation that provides a more expressive language for modeling human-automation systems. Following, we consider a case study in which our formalism is used to model and reason about the Therac-25 device problem, a classic problem in the human-automation systems literature [10]. We provide a brief overview of the relevant aspects of the problem, and construct a knowledge-based model. We then use the model to evaluate the knowledge in the system against formally defined criteria, and show how a solution can be derived and then validated. To conclude, we present our views regarding the potential value of the approach for reasoning about knowledge in a variety of human-automation and human-robot systems, and outline a number of research directions that will expand the value of the approach as a means of modeling complex systems."}, {"heading": "II. HUMAN KNOWLEDGE IN COMPLEX SYSTEMS", "text": "Human knowledge is a significant and intriguing topic for study in many domains. Philosophers from ancient times have considered fundamental questions such as the nature of knowledge and the origin of its existence, and began the development of logics and formal modeling in order to more precisely reason about these complex and often subtle concepts [11]. In more contemporary times, scholars in fields such as Artificial Intelligence and Cognitive Science have variously interpreted and re-interpreted the concept of knowledge in order to explore their\nparticular domains, focusing for example on notions such as knowledge representation and knowledge-based agents that are fundamental to AI. Valuable outgrowths of work in these and other important fields include both strong support for the critical value of knowledge as a formal construct, and richly descriptive tools for precisely describing and reasoning about knowledge [12]-[15].\nOur own goal is to draw on this formal approach as a means of reasoning precisely about complex human-\nautomation systems. More specifically, we aim to use this well-structured notion of knowledge to model and analyze the design of complex systems for which the human\u2019s knowledge is required for supervisory control. The ability to describe in exact terms what a human operator must know in order to control a system, coupled with the ability to describe, in like terms, what that human can be expected to know as a function of the interface, offers a rigorous approach to the analysis of complex human-automation system design efficacy.\nA. A Notion of Operator Knowledge\nSo that we may reason formally about operator knowledge in the context of complex systems, we first need to\nconceptualize operator knowledge in supervisory control in a manner that will be amenable to capture within a formal framework. In this section we discuss types of knowledge that an operator may need to control a complex system, and then examine several additional characteristics of operator knowledge that should also be captured in our framework.\n1) Knowledge Types\nThe concept of knowledge in the literature is often partitioned and variously classified, with each research domain categorizing knowledge in the way that is most suited for its theories and constructs. Since our focus at present is the knowledge of a human supervisory controller within the context of a complex system, we need to cast our notion of knowledge accordingly. What is the knowledge that is required for supervisory control?\nSheridan notes that to function as a supervisory controller, the human agent must be able to command the system\nin accordance with defined specifications, monitor the system\u2019s behavior to ensure that it performs as required, and identify and correct anomalous system behaviors [16], [17]. It is straightforward to interpret these supervisory control tasks in terms of knowledge: the effective human operator needs both knowledge of the current state and behaviors of the system, and knowledge of the rules defined by the system specifications. To represent these knowledge requirements correctly, our approach will need to capture the knowledge of current system state available from the automated system\u2019s interface (e.g. the displays) as well as the knowledge that the human operator\nis expected to possess regarding the rules of possible and acceptable system behavior.\na) Explicit knowledge\nThe information available from the human-automation interface at any given point in time is the source of the\noperator\u2019s explicit knowledge. 1 We wish to say, for example, that a competent human driver explicitly knows that the current speed of the car she is driving is 40 miles per hour since that value is displayed on the speedometer. While the relationship between displayed information and useful knowledge may depend on non-trivial factors such as the agent correctly perceiving the display and understanding its meaning, the compatibility of the display format to the task, and so forth [18], we assume here that the display type, quality, and information saliency are adequate for the task at hand. Explicit knowledge is needed for supervisory control tasks such as monitoring system health and status; it is also necessary for tasks such as synchronizing the actions of interacting or otherwise interdependent system components (e.g., not opening a tank cover until the internal pressure of the tank has dropped below some pre-defined level).\nb) Mental model knowledge\nA second type of knowledge needed for human-supervisory control is the 'knowledge-in-the-head' that the human agent possesses regarding the global behaviors and properties of the physical system with which he is interacting [19]-[21]. This mental model knowledge can be thought of as a collection of inferences that the operator knows to make about the system. Mental model knowledge is required both for reasoning about and interpreting current interface information as well as for future-oriented supervisory control tasks such as planning and scheduling, troubleshooting, and decision-making [16]-[18]. We assume that the operator\u2019s mental model knowledge also includes what we shall call ground knowledge \u2013 essential domain-relevant knowledge that the human agent may be assumed to possess, such as basic logical and computational skills. We suggest that this knowledge is \u2018automatic\u2019 in nature and thus requires almost no effortful thinking or reasoning action [18], [22]. For instance, it seems correct to assume that a human agent who is sufficiently skilled to act as supervisory controller of a complex system, should be able to automatically compare two displayed integers and determine which of them is greater.\nMental model knowledge is normally based in fact (e.g. through training). However, since it evolves over time as a result of repeated interactions, mental model knowledge may grow to include some proportion of unproven (and\n1 It is worth mentioning that the human-automation interface need not be visual; our intent is any interface that provides the operator with\ninformation relevant to system behavior.\nperhaps incorrect) beliefs [23]. For any typically-sized complex system the human operator\u2019s mental model knowledge is clearly incomplete, as the number of potential system behaviors under all possible conditions is too large to be known.\n2) Characteristics of Human Knowledge\nIn order to develop a representation that allows us to reason accurately as well as expressively about human supervisory control knowledge within the context of complex systems, we need to consider several additional aspects or characteristics of operator knowledge.\na) Reasoning and knowledge\nIn a practical sense, what the operator knows at any given moment is the result of interpreting the explicit knowledge available using the mental model knowledge. Precisely how humans reason, the rules of inference that guide their reasoning activities, or indeed whether humans use any form of mental logic at all are on-going and strongly debated questions in the literature [24]. The goal of this paper is not to add to that debate, nor to make claims regarding the actual processes by which humans reason, or the specific rules of logical inference they may use. To capture a notion of reasoning useful for our purposes, we will assume that the human supervisory controller has the ability to reach at least a particular set of conclusions that are generated by a small set of deductions that are part of her mental model.\nFor example, consider that at a given moment in time the driver agent explicitly knows that the allowable speed is\n25 mph (she just passed the speed limit sign), and that her current actual speed is 40 mph as displayed on the speedometer. Further, assume that the set of facts that comprise her mental model includes the inference that an actual speed greater than an allowable speed may result in a speeding ticket. The inference will then allow the human agent to deduce the (quite important) conclusion that she is currently in danger of receiving a ticket.\nb) Human computational ability\nA related characteristic of human reasoning ability and the knowledge that results is the question of the operator\u2019s computational power \u2013 even if the operator only reaches conclusions based on the inferences in her mental model, how many conclusions can she reach in a bounded period of time? If a conclusion requires conclusions from other inferences as inputs or antecedents, how long can this chain of inferences be before the human operator is overwhelmed? As research shows, human computational power is quite limited [18], [22], [25] and even more so in\nthe time- or safety-critical environments within which most complex systems function [26]. In order to capture some reasonable facsimile of the knowledge of a supervisory controller, our approach will have to limit the number of inferences that the operator can make at a given moment."}, {"heading": "B. When Does An Operator Know Enough?", "text": "If our aim is to provide a practically useful methodology and tool set for the design and analysis of complex systems from the perspective of agent knowledge, we must provide not only a formal means of describing knowledge in these systems, but also formal metrics against which knowledge in systems can be rigorously evaluated. Though the human agent's knowledge of a complex system's possible behavior is incomplete by definition due to the size of the state space, we propose that at a minimum, the human agent\u2019s knowledge must be sound and adequate. We describe these properties next, and note that if they hold, we shall say that the human agent's knowledge is satisfactory.\n1) Soundness\nWe will say that the human agent's knowledge is sound at a given system state if the facts that the agent is said to\nknow and the inferences that the agent can make there are true. As we discuss in following sections, soundness is a fundamental property of knowledge that is formalized as the Knowledge Axiom, stating that only what is true can be known [1]. A formal definition of knowledge for human agents is sound if the Knowledge Axiom holds at all states of the system of interest.\n2) Adequacy\nWhile an agent\u2019s knowledge in a complex system will necessarily be incomplete, the knowledge must be adequate; in particular, it must guarantee her ability to identify anomalous or 'illegal' behaviors or states (i.e. not in accordance with system specifications). In other words, though the agent will not know, a priori, all the possible behaviors that a system may exhibit, she must always be able to determine when the current system behavior is 'bad'.\nHaving discussed basic notions related to human knowledge in supervisory control, we next provide a brief\nintroduction to the basic elements of the knowledge formalism in order to recast our model."}, {"heading": "III. REASONING ABOUT KNOWLEDGE \u2013 AN INTRODUCTION", "text": ""}, {"heading": "A. Elements of the Framework", "text": "The knowledge formalism was originally adapted to distributed and multi-agent systems by Halpern and Moses\nand was expanded on in numerous writings [1], [2], [5], [27]-[30]. It includes structures to represent the entities or agents in the system and the knowledge they can be said to possess. The formalism also includes a means for describing the interaction of knowledge and action and the behavior of the system. It thus provides a rigorous methodology for reasoning about knowledge in a dynamic multi-agent system, and for analyzing and proving properties of that knowledge. Here we focus on the elements of the formalism that are relevant to the present work, and describe how knowledge is defined using these elements. The primary references for this part are [1]-[3]. Once the foundations of the formalism have been presented, we introduce a number of concepts that will enable more expressive representation of knowledge properties of human agents.\n1) Agents\nThe formalism considers agents and systems of multiple interacting agents, where an agent might be a robot, a\nprocessor, a human, a physical object, or any other entity of interest in the system. The approach includes the external environment as an agent, albeit a special type of agent, whose behavior is not under the control of the other agents in the system. In general, the environment\u2019s role is to represent all that is relevant to the system that is not captured by the other system agents. In this paper we focus on systems involving a single human agent h, (intuitively, this is the operator), a single \u201cautomaton\u201d agent a \u2013 the complex system that h operates, and the environment agent e. In modeling the speedy driver, for example, we could capture the driver as human agent h, the car as the automaton agent a, and the outside world (including, for instance, traffic lights and whether or not there is a policeman watching) as the environment e.\n2) Local states, global states, and agent knowledge\nWhen reasoning about knowledge, we are typically interested in modeling a dynamic situation in which the world\nevolves, and with it the state of knowledge of the agents. In this section we consider a natural way to model these.\nWe think of every agent at any given instant as being in a well-defined local state. The local state in our setting captures all of the information that is available to the agent at a given instant. This is the information available to the agent when it determines its next action. The precise structure and contents of this local state depends on the system that is being modeled.\nA global state corresponds to a snapshot of the state of the agents in the world frozen at an instant. In our threeagent setting, a global state would be modeled by a triple (se, sh, sa), where si is agent i\u2019s local state, for i=e,h,a. Intuitively, an agent\u2019s local state captures exactly what is visible to the agent at the current point - the agent\u2019s\nknowledge depends solely on its local state. The agent is able to distinguish between two global states exactly if its local state in one is different from its local state in the other. For example, consider a car in which there is an indicator light for low brake fluid. As long as the indicator light is off, the driver cannot distinguish whether or not the brake fluid is leaking. All the driver knows is that the fluid level is not below the critical level. Indeed, if the indicator is not fully reliable, then the driver cannot distinguish a situation in which the indicator is working and the fluid is sufficient, from one in which the fluid level is critical but the indicator is malfunctioning. The formalism thus allows us to accurately model and reason about situations in which agents have only partial knowledge of system behavior. The state se is called the local state of the environment, and it accounts for all else that is relevant to the analysis, possibly keeping track of aspects of the world that are not part of any agent\u2019s local state (e.g., a traffic light).\n3) Dynamic system behavior\nThe evolution of a world over time produces a history, which in our terminology will be called a run. Formally, a\nrun r is a function from time to global states, assigning to every time instant m a global state r(m). If r(m) = (se, sh, sa), then we denote by ri(m) the local state si, for i=h,a,e.\nIt is often convenient to identify time with the natural numbers, in which case the run is identified with the sequence r = r(0), r(1), . . .. Facts, represented in our framework by formulas of L, are considered true or false at a point (r,m), consisting of a run r at a time m. The truth of non-epistemic facts (ones that do not involve knowledge) at (r,m) can usually be determined based on the global state r(m) at that point.\nWe choose to model the types of human-automation systems that we are interested in as consisting of three agents; the human agent h, the automation agent a, and the environment agent e. With this construction we can focus on the human \u2018knower\u2019 h, and capture how h\u2019s knowledge influences and is influenced by the behavior of the complete system. Thus, at any given time each agent i, for i = e, h, a, will be in a local state li, representing all the information that i has available at that time. We denote by Li the set of i's possible local states. As in the general case noted above, the tuple of all the agents\u2019 local states is a global state g. The set G of all global states is then the Cartesian product G = Le x Lh x La of the sets of local states. In a given application, there can be dependencies between elements in the agents\u2019 local states (e.g. shared data) and the set of global states that will appear in a relevant set of runs R will usually be a subset of G.\nThe dynamic behavior of the system is represented by the set of runs R is generated by the joint actions of the\nenvironment, automation, and human agents.\n4) The logical language\nWe need a language in order to rigorously reason about knowledge in complex systems, and so now formally define a logical language for this purpose. In any given application, there are a (typically small) number of basic facts of interest. These can be the status of the traffic light, the speed of the car, the direction it is driven at, etc. We\ncall such basic facts primitive propositions. In each application or system being considered, we assume that a set \nof the relevant primitive propositions is given. These will be the basic formulas of our logical language. We create more complex formulas inductively by applying logical connectives to simple formulas. Formally, we will be working with a logical language L , which is the set of formulas defined as follows\n Every primitive proposition p   is a formula,\n If  and  are formulas then so are\n  (standing for not )\n    (standing for  and ), and\n Ki (standing for agent i  {h,a } knows ).\nWe define only the two Boolean operators  and  because all other Boolean operators are definable using these\ntwo [31]. (For example,    corresponding to \u201c implies \u201d, can be defined as shorthand for (  ).) The\nlanguage L allows us to express rich and complex statements such as   Kh   which reads that \u201cif  is true and agent h does not know , then  must be the case.\u201d Such a formula would be true, for example, if  stands for\nthe brake fluid level being critical, and  stands for the fact that the indicator is malfunctioning. In fact, while we\nwill not expand into a discussion of one agent\u2019s knowledge of another agent\u2019s knowledge in this paper, our language allows us to talk about what the human knows about the automaton's knowledge, what the automaton knows about the human's knowledge, etc.\nThe propositions p,q,\u2026  and indeed all formulas of L are initially strings that we may intend to ascribe\nmeaning to. We consider that the set of propositions  represents the basic facts about the system that may be\nderived from the design specification. The truth of a primitive proposition p   is defined by way of an\ninterpretation  that maps every  and g to True or False. Thus, if (p)(g)=True then proposition p is true at the\nglobal state g.\nKnowledge changes over time as an agent learns new facts and forgets some that are known. Thus a formula is considered true or false at a time m in a history (or run) r in a system or model R. We can think of a world as\nconsisting of a tuple (R,,r,m) where R is a system,  is the interpretation for the propositions of  at states of R, r is\na history (or run) in R, and m is the point in time at which we are evaluating truth of formulas. The pair (r,m) is a\npoint, and if r is in R, then it is a point of R. We denote the fact that a formula  is true, or satisfied at a world\n(R,,r,m) by (R,,r,m) |=  . The satisfaction relation |= is formally defined by induction on the structure of .\nPrimitive propositions p   form the base of the induction, and their truth is determined according to the\ninterpretation :\n(R,,r,m) |= p (for a primitive proposition p  ) iff (p)(r(m)) = True . (1)\nNegations and conjunctions are handled in the standard way:\n(R,,r,m) |= \u00ac iff (R,,r,m) |= . (2)\n(R,,r,m) |=   \u2019 iff both (R,,r,m) |=  and (R,,r,m) |= \u2019. (3)\nFor a formula Ki , the clause (R,,r,m) |= Ki states that Ki (i knows ) is satisfied at the point (r,m) in system\nR. We will now discuss when this may hold.\n5) Knowledge as truth in all possible worlds\nThe formal definition of knowledge is traditionally framed within the notion of possible worlds [13], [14]. Two points (r,m) and (r\u2019,m\u2019) are considered to be indistinguishable for agent i, which we denote by (r,m)~i(r\u2019,m\u2019), if i has the same local state in the global state r(m) as in r'(m').\nIntuitively, if (r,m)~i(r\u2019,m\u2019) then at (r,m) agent i can equally imagine that the actual state is (r\u2019,m\u2019). Hence (r\u2019,m\u2019)\nis considered a possible world for i at (r,m).\nThe possible-worlds definition of knowledge states that an agent knows a fact precisely if this fact is true in every state or world that the agent considers possible. If two points (r,m) and (r\u2019,m\u2019) are indistinguishable to an agent, and\nthe fact  is false at (r\u2019,m\u2019), then at (r,m) the agent cannot be said to know that  is true. More formally,\n(R,,r,m) |= Ki holds iff (R,,r',m') |=  holds for all points (r',m') with r\u2019R (4)\nthat satisfy (r,m) ~i(r',m').\nObserve that the four clauses (1)\u2013(4) above defining the satisfaction relation `|=' enable us to determine, for every\nformula L and every world (R,,r,m), whether  is or is not satisfied at the world.\nThis definition of knowledge is a powerful and useful tool for the description and analysis of properties of multiagent systems, and enables formal proof of agent knowledge or, alternately, the impossibility of that knowledge. However, several fundamental properties of the approach make it problematic for modeling human agents in humanautomation systems. We briefly mention these properties here prior to discussing the modifications to the definition that will allow a more appropriate and expressive representation of the knowledge of human agents (cf.[13]).\n6) Logical omniscience\nOne of the most important difficulties with applying the traditional possible-worlds definition of knowledge to human agents is that it implies a notion of logical omniscience. Intuitively, logical omniscience refers to the fact that if an agent knows a set of facts it will also know all logical consequences of that set. In our context, logical omniscience suggests that in any state an agent will know all the facts that follow from the information available in that state. That is, the agent\u2019s knowledge in this case is closed under logical inference. Though this may be a fair assumption for an agent with a powerful capacity for reasoning, the typical human agent clearly does not fall into this category. 2 The knowledge that can be ascribed to a human agent using the traditional model is thus far greater than what the agent can actually be expected to know. In order to better fit the task of analyzing systems with human agents, our modifications must provide a more realistic measure of the human agent\u2019s limited ability to infer and to reason, and the limitations on human knowledge that result.\n7) Representing false knowledge\nAs we have seen, the traditional possible-worlds definition of knowledge entails the Knowledge Axiom, stating\nthat an agent knows only true facts, Ki   . In the case of an analysis of automated agents for whom knowledge is in any event an externally ascribed notion this does not appear to be troublesome. For human agents, however, false knowledge is a common state of affairs and a particularly relevant issue in human-automation interaction \u2013 we would wish to capture, for instance, aspects of the interface design that mislead the human operator into holding false beliefs.\n2 This point is most succinctly emphasized by Konolige [37] who notes that if humans were capable of unlimited reasoning, a chess player\nwould know the outcome of a chess match immediately following the first move."}, {"heading": "IV. DEFINING BOUNDED KNOWLEDGE IN A COMPLEX HUMAN-AUTOMATION SYSTEM", "text": ""}, {"heading": "A. An Appropriate 3-Tiered Syntactic Model of Human Knowledge", "text": "We now propose a formalism in which the assumptions about the computational aspects and contents of an agent\u2019s knowledge are limited, in order to more appropriately represent the bounded character of human knowledge. Rather than using possible-worlds semantics as the primary component, we will present a syntactic model in which the known facts will be obtained based on a restricted amount of reasoning, and in which what is boundedly known may not be true (for related approaches to syntactic knowledge see [13], [32]).\nWe model the human's knowledge as a three-tiered structure. Tier 1 - Explicit knowledge\nAt the base is the agent's explicit knowledge, which we intuitively think of as the raw information available from the system interface: clock readings, indicators, aural warnings, and so forth.\nTier 2 - Automatic knowledge\nBased on the agent's explicit knowledge, we allow a set of ground (or \u201cautomatic\u201d) conclusions, in which various propositions that may not be explicitly represented in the state are immediately observed. For example, the local state may contain two 5-digit numbers, and the agent may be expected to know immediately which of them is larger. Similarly, a driver in a residential zone that sees the odometer reading 70 mph (as part of his explicit knowledge) would automatically know that \u201cI am speeding\u201d is true.\nTier 3 \u2013 Bounded Knowledge via deductions\nOn top of the explicit knowledge and the ground conclusions it affords, we think of the agent as being able to perform a finite set of deductions. Each of these deductions will use the fact that a set of propositions appear at the ground and explicit knowledge tiers, to conclude that the agent knows a formula of interest. This is a strictly limited process however, in that knowledge that is deduced at this level cannot be used for further rounds of deduction.\nWe now present the formal definition of this syntactic framework. First, we define the logical language L+,\nextending the language L by adding a \u201cbounded knowledge\u201d operator \u02c6 hK :\n All formulas of L are formulas of L+,\n if  is a formula of L+ then so is \u02c6 hK \u03c6 (thus, L + is closed under \u02c6 hK ), and\n L+ is closed under , , and Ki.\nNext, if  is the set of propositions, we define the set  c =   {p: p} containing all propositions and their\nnegations. We model the human agent's explicit knowledge by way of a function fh: Lh  c 2 specifying which elements of the set  c\nthe agent explicitly knows to be true at any given local state lh Lh of the human agent.\nObserve that explicit knowledge will satisfy the Knowledge Axiom only if the function fh ensures that all explicit knowledge is true.\nThe agent's ground knowledge, which we think of as being derived automatically from its explicit knowledge, is\ncaptured by way of a function ah: c 2  c 2 that, when applied to a subset T c produces another such subset ah(T) = T \u2019 . In our setting, the function ah will be applied to sets T describing the agent's explicit knowledge. For ease of exposition, we think of the ground knowledge as extending the explicit knowledge, so that we formally require\nthat ah be monotonic, so that T ah(T).\nFinally, we consider a deduction phase determined by a set Dh (typically finite) of implications of the form\np1 p2\u2026 pk  \u02c6 hK , where pi   c for i=1,2,\u2026,k.\nWe denote by dh(T) the application of the implications in Dh to a subset T  c of propositional formulas.\nFormally,\ndh (T) = { \u02c6 hK p |p T} U { \u02c6 hK  | (p1 p2\u2026 pk  \u02c6 hK )  Dh and p1, p2,\u2026,pk  T}.\nIn words, the agent's bounded knowledge is obtained from applying a fixed set of deductions to its explicit and its ground knowledge. The latter, in turn is based on the explicitly available knowledge consisting, for example, of data presented on the man-machine interface in a control room. The fact that the deduction phase consists of a single step in which a finite number of deductions are applied enforces the boundedness of this type of knowledge, avoiding forms of logical omniscience.\nPutting these elements together, we define the knowledge of the human agent h by way of an epistemic setup h = (fh, ah, Dh). We think of the epistemic setup as encoding a 3-step process by which h\u2019s structured knowledge is obtained as a function of its local state l:\nStep 1. The function fh is applied to the local state to obtain the agent's explicit knowledge.\nStep 2. The function ah is applied to the result to account for the immediate conclusions that the agent draws\nautomatically from its explicit knowledge.\nStep 3. The set of inferences available in the agent's mental model, captured by the Dh component, is applied once\nusing the function dh and generates the agent's knowledge at l.\nFormally, applying h to a local state l yields the set h(l) = dh(ah(fh(l))) consisting of knowledge formulas \u02c6 hK \nthat the human agent is taken to know at local state lh.\nAn example will help to clarify. Let lh be the local state of a human agent h piloting an aircraft, and let time instant m at r(m) be the final approach just prior to landing. Among the many indicators in the cockpit is an indicator for the status of the flaps (trailing edges of the wings \u2013 full flaps is typically the normal status for optimized landing safety and efficiency) and an indicator for the status of the landing gear, which may be up, transitioning, or down and locked.\nThe pilot\u2019s local state lh at any time will consist of the status of these two indicators; for instance lh = ((flaps.not.full, landing.gear.up) describes the pilot\u2019s local state when the flaps are not at full extension and the landing gear is folded up into the airplane. In this local state the set of propositions that are true \u2013 the pilot\u2019s explicit knowledge \u2013 are that the flaps are not fully extended and that the wheels are up. We\u2019ll call these propositions p1 and p2, respectively.\nBy applying the automatic ground knowledge function ah on this set of propositions the pilot also knows that the airplane is not configured for landing, and this immediate conclusion, call it p3, is added to the set of propositions that the pilot knows. For the third step in the process, we\u2019ll assume that a trained pilot\u2019s mental model includes an inference d that captures the rule \u2018if on final approach then full flaps and landing gear down and locked\u2019. Applying this inference to the set of propositions p1, p2, and p3 for time instant m at r(m) = final approach generates the pilot agent\u2019s knowledge of the formula  at local state l where  is \u2018 on final approach and aircraft not configured for\nfinal approach so go-around (return to altitude in order to safely configure aircraft for landing)\u2019. Thus, \u02c6 hK \ndescribes the pilot\u2019s bounded knowledge at l."}, {"heading": "B. Knowledge of Bad System Behavior", "text": "A supervisory controller must at all times know whether the behavior of the system is within specified and acceptable bounds. In addition to system\u2013specific knowledge formulas that will be part of the human agent\u2019s\nepistemic setup h, the agent is expected to know, at each local state l in which the current global state is not in accordance with system specifications, that there is a problem.. We define pbad as a proposition standing for \u2018the current behavior of the system is not acceptable\u2019 where the notion of \u201cacceptable\u201d would depend on the application.\nMoreover, we require that \u02c6 hK pbad be included in the set h(l) at any state of the system that is not guaranteed to be\nwithin acceptable bounds.\nIn the example above, if we assume there is nothing that precludes the go-around maneuver then the current\nbehavior of the system is acceptable and the formula \u02c6 hK pbad, is not part of the pilot\u2019s knowledge at this local state l.\nAlternately, if the pilot in this situation cannot execute the go-around maneuver then a bad state exists and \u02c6 hK pbad would be included in the pilot\u2019s local state.\nAt present we make the simplifying assumption that the human knows all the formulas in her epistemic setup\nh(l) from the instant that she is in l, and do not address the (more realistic) possibility that drawing even a simple set of inferences may require some time. While we can easily extend our approach to capture this distinction, the resulting definitions are somewhat cumbersome, and are beyond the scope of this paper. (See [33] for definitions of knowledge that take computational complexity into account.)"}, {"heading": "C. The Epistemic System Model", "text": "The mathematical model in which we can both define truth of formulas and generate the human agent\u2019s\nknowledge will be an epistemic system E = (R, , h), where R is the set of possible runs,  is the interpretation of primitive propositions at the global states of R, and h is the epistemic setup of the human agent as described above. Truth of formulas in L + can now be defined with respect to E = (R, , h). At a given time m in a run r, we define:\n(E,r,m) |= p (for p) iff (p)(r(m)) = True\n(E,r,m) |=    iff both (E,r,m) |=  and (E,r,m) |= \n(E,r,m) |=  iff it is not the case that (E,r,m) |= \n(E,r,m) |= \u02c6 hK  iff \u02c6 hK   h(rh(m))\nFinally, since the system R and the interpretation  are components of the epistemic system E= (R, , h), we can\nalso define the truth of standard (possible-worlds) knowledge with respect to E. For i=a,h,e:\n(E,r,m) |= Ki iff (E,r\u2019,m\u2019) |=  for all points (r\u2019,m\u2019)~i(r,m)\nThe latter clause will enable us to consider possible-worlds knowledge and bounded knowledge within the same\nframework. Recall that \u02c6 hK represents a syntactic, rather than semantic, notion of knowledge. That is, what the human agent \u201cknows\u201d in each local state is a set of knowledge formulas that is determined by the epistemic setup\nh.. There is no a priori guarantee that this knowledge is true, or even consistent.\nGiven an epistemic system E, we write E |=  (and say that  is valid, or tautologically true in E) if (E,r,m) |= \nholds for all points (r,m) of R."}, {"heading": "D. Criteria for Satisfactory Bounded Knowledge", "text": "We now formalize the soundness and adequacy properties to enable the analysis of whether or not the human\nagent's bounded knowledge in an epistemic system E = (R, , h) is satisfactory for supervisory control.\n1) Soundness\nWe say that an epistemic system E = (R, , h) is sound if the following two statements are true: 1. For every (r, m) with r  R, if q  ah(fh(rh(m))) then (E,r,m) |= q. In words, for every point (r,m) in a run of R,\nif q is in the human agent\u2019s explicit or automatic knowledge, then q is true at (r,m) in the epistemic system E. Moreover,\n2. E |= (p1  p2 \u2026 pk)   holds for every implication p1 \u2026 pk  \u02c6 hK  in Dh.\nIntuitively, soundness guarantees that E |= \u02c6 hK    holds, meaning that every conclusion that the human agent\ndraws using its mental model is true at every point (r,m) in the epistemic system E. We will prove this fact formally in Corollary 1 below.\nThe importance of soundness in this context is that it ensures that what the human agent is said to know according\nto \u02c6 hK is in fact true. If the epistemic system E does not satisfy soundness, so that E |\u2260 \u02c6 hK   , then there is one\npoint at which there is at least one formula  known to the agent according to the epistemic system E is false. In\nthis case, either an element of the agent\u2019s explicit or automatic knowledge is incorrect, or an implication in Dh is incorrect (possibly both).\nThe notion of soundness allows us to consider situations in which the knowledge available for supervisory control is incorrect. The faulty indicator light for low brake fluid mentioned previously is an example \u2013 if the indicator is malfunctioning and providing the explicit information \u2018brake fluid indicator light off\u2019, the driver may boundedly\nknow that the brake fluid is sufficient while in fact it is critically low. This is in comparison with a lack of required knowledge which is addressed by the adequacy criterion we consider next.\n2) Adequacy\nThough a human operator's knowledge of a complex system is incomplete by definition, we will say that the human\u2019s knowledge is adequate for supervisory control only if the operator can always recognize unsafe system states and avoid anomalous system states and behaviors. 3 That is, the agent must be able to use her own local state to determine whether the current state is acceptable. For example, assume that a well-intentioned but somewhat confused cost-cutting designer concludes that since wheels are either up or down, having multiple indicators for wheel position is wasteful and that one \u2018wheels up\u2019 indicator should be sufficient. At any given time, the epistemic\nsetup of the pilot of this low-cost airplane is comprised solely of fh(l) = pup: \u2018wheels are up\u2019 or its negation pup: \u2018not (wheels are up)\u2019, or more familiarly, \u2018wheels are not up\u2019. Landing of course is allowed only if wheels have completed transitioning and are locked in the 'down' position. However, the pilot's display does not allow her\ndistinguish between the wheels being in transition, pup, and the wheels being locked in the down position and so the information available is not adequate for her to determine whether indeed the aircraft is configured for landing.\nTo formalize adequacy in epistemic systems, recall that we defined pbad as a proposition standing for \u2018the current state of the system is not acceptable.\u2019 Moreover, we required that \u02c6 hK pbad hold at all points at which the global state is not in accordance with the system specification. We then say that an epistemic system E is adequate if the following hold:\n1. \u02c6 hK pbad  h((rh(m)) for all states of the system such that (E,r,m) |= pbad , thus pbad  \u02c6 hK pbad is valid in E.\nSoundness must also hold so that the operator\u2019s bounded knowledge of a bad state is true:\n2. E is sound, so that, in particular, E |= \u02c6 hK pbad  pbad"}, {"heading": "E. Human Knowledge and Possible Worlds Knowledge", "text": "Our syntactic representation of bounded human knowledge via an epistemic system E makes no assumptions regarding logical omniscience, consistency of reasoning, or the actual truth value of the human agent\u2019s knowledge. By using this approach we are able to capture an expressive and reasonable description of a system with human\n3 We refer here to significantly anomalous states and behaviors that impact overall performance or safety, and not the transient anomalies that\nare automatically corrected and are part of any complex system.\nagents while at the same time we avoid the problems inherent in the application of the possible-worlds definition. However, is there a price to be paid for this useful tool set? Is it less formal, less precise, or does it limit the conclusions that can be reached regarding what the human agent knows?\nWhile additional work is needed to fully determine the limitations of our framework, we do claim that our approach precludes any arbitrary or specious claims to agent knowledge. As the following theorem and corollaries\nshow, any proposition  that is known by human agent h in a sound system E is also known when knowledge is\ndefined as truth in all possible worlds:\nTheorem 1: Let E = (R, , h) be an epistemic system and let L + . If E is sound, then E |= \u02c6 hK   Kh  .\nIn words, this theorem states that any proposition  that the human agent (boundedly) knows in a sound epistemic\nsystem E is also known by that agent in the possible-worlds sense.\nProof: Suppose that E = (R, ,h) is sound. We need to show that if (E,r,m) |= \u02c6 hK  then (E,r,m) |= Kh  holds as\nwell. By definition, (E,r,m) |= Kh  holds if (E,r\u2019,m\u2019) |=  holds at all points (r\u2019,m\u2019) satisfying (r\u2019,m\u2019)~h(r,m). So let (E,r,m) |= \u02c6 hK  and (r\u2019,m\u2019)~h(r,m). Denoting l= rh(m), we have that r\u2019h(m\u2019)= l as well. By definition of (E,r,m) |=\n\u02c6 hK  we have that \u02c6 hK   h(l) = dh(ah(fh(l))). By definition of h and dh this means that either (a)  = p\nah(fh(l)) or (b) there is an implication p1 \u2026 pk  \u02c6 hK  in Dh with pi  ah(fh(l)) every i=1,\u2026,k. In case (a) we\nhave by clause (1) of soundness that (E,r\u2019,m\u2019) |=  . In case (b) we have by clause (2) of soundness that E |= (p1  p2 \u2026 pk)  . Since we also have by clause (1) that (E,r\u2019,m\u2019) |= pi for every i, it follows that (E,r\u2019,m\u2019) |= p1  p2 \u2026 pk, and thus (E,r\u2019,m\u2019) |= , as desired. QED\nSince (r,m)~h(r,m) for all points (r,m), by definition, we have that standard (possible-worlds) knowledge satisfies the Knowledge Axiom (or knowledge property) E |= Kh   . Given Theorem 1, it immediately follows that so does human agent knowledge:\nCorollary 1: If E is sound, then E |= \u02c6 hK    holds for all L + .\nCorollary 1 says that when a system E is sound, then every formula  in L + that the human agent boundedly\nknows to be true according to E is indeed true. A second corollary consists of the equivalent, contrapositive, form of Theorem 1:\nCorollary 2: If E is sound then E |=  Kh    \u02c6 hK  .\nIn other words, if human agent h does not know a proposition  in a system (R, ) according to the possible\nworlds definition of knowledge, then h will not know  in any sound epistemic system E extending the system (R,\n).\nThe relationship between knowledge in an epistemic system and the classical possible worlds interpretation of knowledge demonstrates that our approach is fundamentally grounded, with Theorem 1 and its corollaries collectively providing the formal support.\nAssume, for instance, that there is an epistemic system E that models the knowledge of the agents in a complex human-automation system S. In E there is a local state l of the human agent at which h boundedly knows a proposition q. According to Theorem 1, if E is sound, then h must also know q when at l according to the possible worlds approach. Thus if E is sound, any proposition that the human agent knows in our modified approach is indeed true.\nThis relationship between the two approaches allows us also to demonstrate when the design of a given system R makes satisfactory knowledge impossible. If for example we show that for a state in R satisfying pbad the human agent does not know pbad at that state, then by Corollary 2 no epistemic system for R can be adequate and sound, and the design of R is fundamentally flawed. The design must be changed - perhaps by adding explicit information to the human agent states - before an adequate epistemic setting can be obtained.\nEpistemic systems are a framework for reasoning about knowledge in complex human-automation systems that allows us to expressively describe the knowledge a human agent has available for supervisory control. At the same time, the framework allows us to formally determine if that knowledge enables effective system performance.\nIn the remainder of the paper we discuss a representative case study in which our approach is used to model and analyze the classic Therac-25 problem [10]. We use the formalism to describe the knowledge available to the human operator as a function of the system\u2019s design, to prove that the existing design precludes effective supervisory\ncontrol, and to identify the knowledge that is lacking. Following, we show that once missing knowledge is made available (by a change in the interface in this case), effective supervisory control is enabled. In this manner we establish the viability, value, and correctness of thinking formally about human-automation systems at the level of the knowledge in the system, and what agents know."}, {"heading": "V. THE THERAC-25 DEVICE PROBLEM  OVERVIEW", "text": "The case study is presented at two levels of detail. The first, immediately following, is a high-level overview discussion that aims to enhance an intuitive understanding of our approach and its value for reasoning about complex human-automation systems. The second, included in the Appendix, is a complete formal representation of the problem and the solution."}, {"heading": "A. Problem Description", "text": "\u201cBetween June 1985 and January 1987, a computer-controlled radiation therapy machine, called the Therac-25, massively overdosed six people. These accidents have been described as the worst in the [then] 35-year history of medical accelerators\u201d [10].\nDuring investigation of the Therac-25 failures, a common factor in two of the accidents was the device\u2019s operator. In both cases, the operator had started and completed the patient data entry task, and then had gone back to edit one or more values before starting the treatment.\nAnalysis showed that the operator\u2019s data entry speed was the cause of the machine\u2019s behavior \u2013 while editing data was an allowable function, the operator was able to edit the data and return to a \u2018data entry complete\u2019 state so quickly that the machine did not record, and hence was never aware, that the data editing had occurred. The operator then initiated treatment believing that dosage would be in accordance with the newly edited parameter values displayed on the interface, while the actual dosage was given to the patient according to the original data values.\nSeveral flaws in the system\u2019s design were implicated in the accidents. The faulty design introduced a system state during which the treatment values could be edited, and the machine activated, without the new values actually being processed. Furthermore, the interface provided almost no information to the operator about the internal behavior of the machine, and the operator thus had no way of knowing whether the machine had accepted her data editing."}, {"heading": "B. Modeling the System", "text": "In order to reason more clearly about the problem and potential solutions we first need to generate a more formal\nrepresentation of the significant elements of the Therac-25 problem.\n1) Agents\nThere are two agents that must be included in the model, the human operator h, and the Therac-25 device, denoted a. (Reference to the environment is omitted in this example, as the device is deterministic and so the environment plays no essential role.)\n2) Local states\nThe human operator\u2019s local states in this example capture the information that is available via the Therac-25 device\u2019s interface. For instance, the operator\u2019s local states will include a variable representing the status of data entry or modification, and a variable for the \u2018ready-to-treat\u2019 status of the device, as presented to the operator by the interface. The machine\u2019s local states consist of variables indicating whether there is complete data entry, whether the entered data has been modified, and whether the system is in treat mode.\n3) Actions and joint actions\nIn each state, the operator h and the machine a perform actions that may change the global state. The operator of the Therac-25 device can input data, modify the data entered, initiate treatment, or do nothing. The device can process data that has been entered, treat, or do nothing.\n4) Dynamic system behavior\nWe represent the dynamic behavior of the combined operator-device system by defining the possible transitions between global states as a function of the operator\u2019s and the device\u2019s joint actions. For example, consider the global state g0 as the initial state in which the operator has not yet entered any treatment data. A possible joint action in this state would be for the operator to enter treatment data and the Therac device to do nothing. This joint action would cause the system to transition to a global state g1 in which the treatment data has been entered but not yet processed. By defining all possible transitions between states we can obtain a complete model of the system\u2019s set of possible behaviors (which will form our set of possible runs R). A crucial task in modeling the behavior of the system is to identify behaviors that are unacceptably anomalous, so that the set of points (r,m) in R at which pbad is true may be clearly defined.\n5) Modeling the human\u2019s knowledge\nThe next step in development of the model is to define and formally represent the operator\u2019s knowledge in each of\nthe states of the system; this is the knowledge, the epistemic setup h, that the operator has in order to control the system.\nWe start by generating the set of primitive propositions or facts that are relevant to the complete system\u2019s dynamic behavior. For instance, facts such as \u2018no treatment data is entered\u2019 and \u2018device is ready to treat\u2019 would be part of this set. The truth value of these facts at a given time will be a function of the actions of the operator and the machine, and the physical dynamics of the complete system. Thus, for example, the proposition \u2018no treatment data is entered\u2019 will be true only if the operator has not yet entered any treatment data.\nWe then add new propositions of ground or automatic knowledge such as \u2018data is entered accurately\u2019. As the final step in modeling the operator\u2019s bounded knowledge we define the implications that would comprise the operator\u2019s mental model knowledge used to interpret the explicit knowledge in her local state. The Therac operator\u2019s mental model might include the implication \u2018if data entry is complete and the device is ready to treat and the entered data is correct then treatment can be initiated\u2019; in a local state in which the propositions \u2018data entry is complete\u2019, \u2018the device is ready to treat\u2019, and \u2018entered data is correct\u2019 are true, the operator would know that treatment can be initiated. Critically, the operator\u2019s mental model must also include the implications that enable knowledge of anomalous system behaviors, so that the agent knows when pbad is true."}, {"heading": "C. Satisfactory Knowledge and Possible-Worlds Knowledge in the Therac-25 Problem", "text": "Once the system\u2019s behavior and the operator\u2019s bounded knowledge have been modeled, we use our definitions of soundness and adequacy to analyze the system and demonstrate that the Therac-25 operator\u2019s bounded knowledge as modeled is not satisfactory for supervisory control. Examining the model within the possible worlds framework shows further that the design of the Therac-25 is flawed and so renders satisfactory knowledge impossible.\nTo begin, recall that the operator of the Therac device boundedly knows that treatment data values can be modified prior to initiating the patient\u2019s treatment. We assume that in a given run r the operator modifies previously entered data, the device processes the data and displays its \u2018ready to treat\u2019 status, and the operator initiates treatment. In this run r the device functions as expected and treats the patient in accordance with the modified data; the operator knows the system behavior is acceptable.\nNow we consider another run r\u2019 in which the operator\u2019s actions are the same as in run r but the device does not process the new data prior to treatment being initiated. Thus pbad holds once treatment is initiated. Since the operator\u2019s local states in r\u2019 are the same as in r she will have the same knowledge as in r, and so boundedly knows that the system behavior is acceptable. There is no indication in her local states that once treatment is initiated, pbad holds.\n In both runs r and r\u2019, at the point m at which treatment data has been edited, the proposition p\n= \u2018device is ready to treat\u2019 is part of the operator\u2019s explicit knowledge, (E,r,m) |= \u02c6 hK p and\n(E,r\u2019,m\u2019) |= \u02c6 hK p . However, since in run r\u2019 at (r\u2019,m) the edited data has not yet been processed by the device then p is false in (r\u2019,m\u2019), as is the operator\u2019s knowledge of p. Soundness, a necessary property of satisfactory knowledge, therefore does not hold.\n In run r\u2019, though pbad holds after initiation of treatment, pbad is not in the operator\u2019s local state\nand so adequacy does not hold.\n While at point (r,m) treatment data has been edited and processed by the device, there is\nanother point (r',m') at which the human operator has the same state as at (r,m), so that (r,m)~h(r\u2019,m\u2019), but at which the new data has not been processed by the device. Thus, while the operator boundedly knows p at both (r,m) and at (r\u2019,m\u2019), p is true at one point and false at another. Since (r,m)~h(r\u2019,m\u2019), Khp (stated in terms of possible-world knowledge) does not hold. Corollary 2 implies that no epistemic system for this device can be adequate: there is guaranteed to be a bad state that the operator will be unable to detect as such.\nThe impossibility of satisfactory knowledge for supervisory control of the Therac-25 system is thus shown. This\nis discussed further and formally proved in the Appendix."}, {"heading": "D. Problem Resolution", "text": "In addition to supporting a fine-grained analysis of a complex human-automation system, our approach provides clear direction for problem resolution. In the case of the Therac-25 problem, the knowledge-based model and analysis suggests the need for a design modification that provides the operator with the information needed to truly\nknow that the device has processed the operator\u2019s edited data values and so is indeed ready to treat. 4 This modification would allow the operator to easily determine the true \u2018ready to treat\u2019 status of the device and so should preclude the possibility of initiating treatment with incorrect treatment data."}, {"heading": "E. Supporting Design Modification Via Bounded Knowledge", "text": "It remains to demonstrate, using our formal framework, that this modification does indeed result in a system in which the knowledge available is satisfactory for supervisory control. Again assume that there is a state g in which treatment data has been edited and processed by the device, and a state g\u2019 in which treatment data has been edited but not processed by the device.\nLet E m be the epistemic system that models the knowledge of the operator and device in the Therac-25 system\nmodified as described in section 5.4. In the local state l, the operator\u2019s epistemic setup h(l) = dh(ah(fh(l))) now contains a knowledge formula \u02c6 hK  representing the operator\u2019s bounded knowledge of the global system\u2019s\nreadiness to treat.\nFor this system E m to be sound, i.e., for what the operator boundedly knows to indeed be true, the operator must know that the system is ready to treat within the possible worlds framework, as well. It is easy to see that this is so. In any global state g\u2019 in which treatment data had been edited but not yet processed by the device, the modified device would provide an indication to the operator that the system is not ready to treat. The operator would thus know that the system is ready to treat only in states in which the system truly is ready to treat. That is, in E m the proposition \u2018ready to treat\u2019 is true in any state or world in which the operator boundedly knows it to be true. This demonstrates that the suggested modification generates a sound epistemic system E m .\nTheorem 4 in the appendix states this in more formal terms, and provides proof that the knowledge available with\nthis modification is satisfactory for supervisory control."}, {"heading": "F. Summary", "text": "The goal of this section was to provide an initial intuitive example of how our approach may be applied to a real system. Though for most complex systems of interest neither the model nor the solution will be nearly as easy to define, this Therac-25 example shows how a \u2018real-life\u2019 system may be modeled, and its human-automation interface\n4 We do not suggest that adding display elements for each fact the human needs to know is a desirable or even viable solution option. Display\noptimization for the knowledge that is identified as necessary is (far) outside the scope of this research.\ndesign evaluated, using the knowledge of its operator. For the interested reader the complete detailed model and analysis is included in the Appendix."}, {"heading": "VI. DISCUSSION AND DIRECTIONS FOR FUTURE RESEARCH", "text": "In this article we presented an initial attempt to utilize a well-established formal theory of knowledge and action in multi-agent systems in order to conduct a knowledge-based analysis of a complex human-automation system. This approach allows us to reason cleanly and rigorously about the design and performance of a system as a function of one of its most fundamental resources \u2013 the knowledge of the agents. The Therac-25 analysis revealed the existence of design flaws in the system that precluded the human agent from being an effective supervisory controller, and identified the knowledge that was missing. No additional theories of performance or behavior were required, and the human and automation agents were depicted using an expressive common construct without losing important and unique properties of either of these distinct agents. The example demonstrated that our approach offers a formal, expressive, and parsimonious methodology for the design and analysis of complex systems, and we believe this to be a significant contribution. As an initial step in a new direction, the work discussed here raises many intriguing points for investigation; we briefly discuss several that we believe particularly worthy of further pursuit."}, {"heading": "A. A Richer Notion of Human Knowledge", "text": "A first A first direction for further research is to expand on and more completely define a notion of human knowledge within our formal framework. For instance, we may wish to consider a classification of human knowledge in terms of its contents in addition to the type-based taxonomy defined in the current paper, such as the declarative / procedural dimension often used in cognitive modeling [34]-[36]. In this manner we may gain a more multi-dimensional model of human knowledge that would render our approach useful in a wider range of problem domains.\nAnother aspect of human knowledge that is important in supervisory control is the distinction between knowledge and belief. By capturing the important distinction between a human operator formally knowing a fact p and believing p, we can more precisely investigate the impact of incorrect human belief on system performance. Belief has been formally represented using a number of techniques [30], [37], and a valuable goal is to identify and build on the technique most appropriate for modeling a notion of human belief in domains of interest.\nAn additional element of human knowledge and reasoning that is particularly important in the supervisory control context is the concept of counterfactual reasoning (\u2018if p were to hold then q would be true\u2019) [38]. When an operator plans future actions, especially error recovery actions, counterfactual reasoning supports the operator\u2019s consideration of conditional alternatives and the outcomes of hypothetical scenarios [39], [40]. The ability to reason about and identify the knowledge needed for effective counterfactual reasoning will be useful in the design of more robust systems that provide the information needed for an operator to successfully 'think through' novel, perhaps safetycritical situations. Preliminary work suggests that incorporating an existing formalization of counterfactual reasoning [27] into our approach will provide designers with a means to do so.\nOnce these elements of human knowledge are added to our framework, the approach will support the modeling and analysis of a wider and more realistic set of complex systems. For example, a designer could more accurately evaluate the potential failure conditions of supervisory control inherent in a system that executes in a highly ambiguous environment by limiting the human agent's knowledge of important automation and environment agent behaviors. We may be able to capture differences in system performance that result from differences in the amount or quality of knowledge possessed by the human controller. If qualitative differences in knowledge distinguish between novice and expert human operators [41], [42], for example, this would allow a designer to gauge the vulnerability of the system to novice supervisory control and might make salient required emphases in training.\nThere are, no doubt, many additional aspects of human agent knowledge that are relevant to our problem domain and that are amenable to formal representation in our framework; it is an interesting extension of our work to identify them. Again, the goal should not be to draw a true and faithful picture of human knowledge, but rather to develop a representation that is epistemically adequate [43] and that allows us to reason expressively and formally about important properties of human knowledge within the context of complex systems."}, {"heading": "B. Extending the Formal Model", "text": "Our formalization of human knowledge has drawn on various approaches in epistemic logic to create a framework that is expressive enough to begin capturing the unique properties of human agent knowledge without sacrificing the rigor of formal logic. Our approach can, as well, formally represent different types of reasoning that a human agent might do, perhaps in different circumstances or in different systems. Previously we noted that our current definition of a 'one-shot' round of reasoning captures an intuitive notion of how a human agent might reason in a supervisory control setting. That is, since complex systems are normally dynamic systems in which control decisions need to be\nmade and implemented in a timely manner, the operator\u2019s reasoning activities cannot require multiple rounds of reasoning. One-shot reasoning captures the resource bounds that would be expected due both to the human's inherently limited reasoning abilities as well as to the requirements of a context in which control decisions need to be made and implemented in a timely manner.\nIt will be interesting to expand on this initial work and identify additional patterns of inference that can be used to express human reasoning, the systems in which they may be appropriate, and the means by which they should be formalized. More precise bounds on reasoning may also be identified. For instance, within the one-shot model, how many independent instances of simple deductive inference can a human do? A system's design might require a human to infer, in the same round, n separate conclusions. Can this requirement be satisfied by typical human cognitive resources? Can we augment the one-shot model with a multi-step (i.e. algorithmic) representation of more complex human reasoning? What might be the natural resource bounds for this type of reasoning actions?"}, {"heading": "C. Knowledge of Groups of Agents in Human-Automation Systems", "text": "One of the most significant contributions of the knowledge formalism has been its ability to express notions of the knowledge of groups of agents, such as agents\u2019 knowledge of other agents\u2019 knowledge, distributed knowledge (knowledge is distributed between the agents in the system), and common knowledge (that is, all the agents know a fact p, and know that all agents know p, and so on\u2026) [3]. This expressiveness supports analysis of centrally important system properties such as the need for an agent to know what another agent knows, the additional knowledge made available by a fact being commonly known, the potential performance cost when system failure precludes a needed fact from being common knowledge, and so forth.\nThe importance of being able to reason formally about the knowledge of groups of agents in the design and analysis of human-automation systems is significant, as noted in [44]-[46]. For a simple example, recall the searchand-rescue robot team mentioned previously. Not only must the designer consider what the human and robot agents need to know in any state of the system, but since the robot is normally remotely operated (e.g. it is deep in a collapsed building looking for survivors) the designer must be able to capture what the human and robot agents can know about each other's knowledge at any point in time in order to determine whether the knowledge of the agents will be sufficient for task performance.\nIt is important to explore the knowledge of groups of agents within the context of human-automation systems. Our work will consider these notions both from a conceptual perspective \u2013 for instance, what does it really mean to\nsay that human and automation agents have common knowledge of a fact? \u2013 and in terms of formal modeling considerations.\nWhat are the theoretical issues of shared knowledge relevant to a human-automation system? It is natural to say that an operator may need to know what the automation knows, but when is it useful (and meaningful) to talk about an automation agent knowing what a human agent knows or what other automation agents know? Considering various forms of human-robot teams, for example, the need for a robot to know what the human agent knows seems clear in the case of search and rescue, personal assistant, or physical therapy robots [47]. Can we define general taxonomies of human-automation and human-robot systems in which specific types of group knowledge are required for system performance?\nThe knowledge formalism provides a complete formal semantics and structures for the modeling and analysis of various types of group knowledge. We thus have the tools to represent and reason about the knowledge of every agent in the group, EG, distributed knowledge in the group, DG, and common knowledge, CG.,. An important direction for our work is to further investigate the notion of group knowledge for human-automation systems. As systems grow in size (number of agents), in heterogeneity (types of agents), and in the criticality of the mission, a common concept that binds all agents and that provides a rigorous method for evaluation will become an imperative. We believe this last direction for our research program to be particularly significant."}, {"heading": "D. Applications", "text": "The final test of any formal approach for modeling and analysis is its applicability to real-world problems in the intended domains. The Therac-25 device problem discussed in this article served as a benchmark for demonstrating that our approach (1) can expressively capture important properties of human and non-human agents, (2) can enable us to answer queries about what human and non-human agents know and don\u2019t know, and (3) can be used to draw significant conclusions regarding a complex system's design in spite of the dissimilarity of its agents. While these results offer an important initial \u2018proof-of-concept\u2019, the Therac-25 problem is a cleanly bounded and thoroughly researched scenario with faults in design identified a priori. What is the value of our formalism as a modeling and analysis tool for systems that are not as neatly defined or for which answers are not as clear?\nThe state-of-the-art in human-automation system design will provide a valuable test-bed of systems for evaluating our approach \u2018in real-life\u2019; we are particularly interested in systems in the aviation and human-robot teams domains. In the aviation domain, current directions in integrated flight deck systems design, aviation information\nmanagement, and the evolving role of the pilot as supervisory controller underscore the inherent relevance of a knowledge-based approach. Will our formalism be able to provide useful insight regarding the design of these highly complex and sophisticated systems?\nThe application of our formalism to the modeling and analysis of human-robot teams will serve as another challenging and rigorous test of its viability. On the one hand, the importance of capturing notions of agent knowledge and common knowledge, and the need for a formal method for reasoning about knowledge in this domain, have already been noted in the literature [45], [48], and so the potential value of our approach is clear. On the other hand, these systems have unique properties that make accurate and useful representation exceptionally difficult. A knowledge-based model of a human-robot team will need to capture the (albeit artificial) intelligence of the non-human robotic agents and the often hostile environment within which these systems operate (e.g. the collapsed buildings environment of search and rescue teams). Too, our approach must be able to represent and reason usefully about the complex and dynamic patterns of communication and knowledge distribution between multiple human and non-human agents that are to be found in this domain. Consider that in a team of humans and unmanned aerial vehicles (UAVs) the multiple UAVs may communicate among themselves to maintain formation and ensure surveillance coverage while information regarding potential targets is transmitted to the human operator. Or the operator may communicate different commands to different UAVs, and receive different responses, which may be a function of the UAV\u2019s reasoning ability and knowledge, rather than just data. How best to capture the role of the knowledge of all the agents in this and similar systems? What insights can we gain here using our knowledgebased approach?"}, {"heading": "VII. CONCLUSION", "text": "As noted in the introduction to this work, the critical nature of many human-automation systems impose a clear\nneed for rigorous design and analysis tools that are practically useful. This is well-recognized, and the development of methods and tools has been an active area of research for many years. Unfortunately the highly complex nature of many of the tools too often results in their isolation in academic and scientific arenas. Subsequently, practical system design remains to a large extent a somewhat ad hoc process.\nTowards that end we have introduced and described the initial development of a novel approach to modeling and\nreasoning about these systems that is intended to both satisfy the need for formal rigor and be sufficiently intuitive\nfor practical use. One of the most significant aspects of this framework is that agent knowledge is ascribed and analyzed with respect to the automaton representing the complete given human-automation system. Our initial results suggest that reasoning about these systems from the perspective of agent knowledge is in fact a viable and valuable approach. conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions.\nAPPENDIX"}, {"heading": "VIII. THE THERAC-25 DEVICE PROBLEM \u2013 DETAILED ANALYSIS", "text": ""}, {"heading": "A. Problem Description", "text": "\u201cBetween June 1985 and January 1987, a computer-controlled radiation therapy machine, called the Therac-25, massively overdosed six people. These accidents have been described as the worst in the [then] 35-year history of medical accelerators\u201d [10].\nDuring investigation of the Therac-25 failures, a common factor in two of the accidents was the device\u2019s operator. In both cases, the operator had started and completed the patient data entry task, and then had gone back to edit one or more values before starting the treatment.\nAnalysis showed that the operator\u2019s data entry speed was the cause of the machine\u2019s behavior \u2013 while editing data was an allowable function, the operator was able to edit the data and return to a \u2018data entry complete\u2019 state so quickly that the machine was never aware that the data editing had occurred. The operator then initiated treatment believing that dosage would be in accordance with the newly edited parameter values displayed on the interface, while the actual dosage given to the patient was as defined by the original data values.\nSeveral flaws in the system\u2019s design were implicated in the accidents. The faulty design introduced a system state during which the treatment values could be edited, and the machine activated, without the new values actually being processed. Further, the interface provided almost no information to the operator about the internal behavior of the machine, and the operator thus had no way of knowing whether the machine had accepted her data editing."}, {"heading": "B. Modeling the System", "text": "1) Agents\nThe two agents in our model of the system are the human operator, denoted h, and the Therac-25 device, denoted\na. (Including the environment in this model does not alter the analysis and so we do not add it here.)\n2) Local states\nThe local states of each agent consist of the variables or information the agent has available in that state. The human operator\u2019s local states, which capture the information available via the interface, include a variable representing the status of data entry or modification, and a variable for the \u2018ready-to-treat\u2019 status of the device. Lh is the set of possible states for the human agent which have the form lh = (data.entry, system.status), where the possible values for each variable are:\ndata.entry  {, data.in, new.data.in}, corresponding to no data entered (i.e. treatment set-up has not yet begun),\ninitial data entry complete, modified data entry complete, respectively. Note that by design the status of data entry is complete only when all required fields are filled and the cursor is in a specified field. This signifies to the system that data entry is complete, and normally triggers the system\u2019s processing of the entered data.\nsystem.status  {sys.ready.no, sys.ready.yes, treat}, indicating whether the system is not ready to treat, ready to\ntreat, or treating.\nThe machine\u2019s local states consist of variables indicating whether there is complete data entry, whether the entered data has been modified, and whether the system is treating. La is the set of possible states for the automation device which have the form la = (data, new.data, status); the possible values for each variable are\ndata  {, treat.data},\nnew.data  {new.no, new.yes},\nstatus  {ready.no, ready.yes, treat}.\n3) Actions and joint actions\nThe operator of the Therac-25 device can input data, modify the data entered, initiate treatment, or do nothing (a\nnull action, denoted ); ACTh = {input.data, modify.data, press.treat, }.\nThe device can process data, treat, or do nothing; ACTa = {process.data, treat, }. The joint actions of the human and automation agents are thus:\n(input.data, )\n(, process.data)\n(modify.data, )\n(modify.data, process.data) (press.treat, treat)\n(press.treat, )\n(,)\n4) Local states and protocols\nThe operator\u2019s local states Lh and the protocol ACTh defining the actions allowable in each local state lh are listed\nbelow.\nThe human operator\u2019s local states are:\nLh = {(, sys.ready.no), (data.in, sys.ready.no), (data.in, sys.ready.yes), (new.data.in, sys.ready.no), (new.data.in,\nsys.ready.yes), (data.in, treating), (new.data.in, treating)}\nThe operator\u2019s protocolACTh is:\nACTh(, sys.ready.no) = {input.data}  {} ACTh(data.in, sys.ready.no) = {modify.data}  {} ACTh(data.in,sys.ready.yes) = {press.treat}  {modify.data}  {} ACTh(new.data.in, sys.ready.no) = {modify.data}  {} ACTh(new.data.in, sys.ready.yes) = {press.treat}  {modify.data}  {} ACTh(data.in, treating) =  ACTh(new.data.in, treating) = \nThe Therac-25 device\u2019s local states La and the protocol ACTh defining the actions allowable in each local state la\nare listed below.\nTherac-25 local states:\nLa = {(, new.no, ready.no), (treat.data, new.no, ready.no), (treat.data, new.no, ready.yes), (treat.data, new.yes,\nready.no), (treat.data, new.yes, ready.yes), (treat.data, new.no, treat), (treat.data, new.yes, treat)}\nTherac-25 actions ACTa\nACTa(, new.no, ready.no) =  ACTa(treat.data, new.no, ready.no) = process.data ACTa(treat.data, new.no, ready.yes) = {treat}  {} ACTa(treat.data, new.yes, ready.no) = process.data ACTa(treat.data, new.yes, ready.yes) = {treat}  {} ACTa(treat.data, new.no, treat) = treat ACTa(treat.data, new.yes, treat) = treat\n5) Dynamic system behavior\nThe system\u2019s dynamic behavior is represented as the set of runs R that result from the operator's protocol above and the automaton's protocol, in which the transitions from one global state to the next are in accordance with a\ntransition function ; the complete set of possible runs is defined to be all executions of the automaton in Fig. 1.\nNote that we assume a correctly operating device and so anomalous behaviors that might arise should the device malfunction are not included. We also bound the scope of the model by defining the states in which treatment has been initiated as end states. Finally, in the interest of readability the automaton does not display the self-loops that\nresult from joint null actions (,) or from actions that have no effect (e.g. the operator action treat when the\nmachine is \u2018not.ready\u2019.)\nC. Impossibility of a Sound Epistemic Setup for the Therac Example\nA logical treatment of the Therac example would start defining a set  of primitive propositions or facts that are\nrelevant to the system\u2019s dynamic behavior; For example, we may have  = {p1,\u2026,p6} where\np1 = \u2018no treatment data is entered\u2019 p2 = \u2018initial data entry is complete\u2019 p3 = \u2018modified data entry is complete\u2019 p4 = \u2018modified data entry has been processed\u2019 p5 = \u2018device is ready to treat\u2019 p6 = \u2018device is treating\u2019\n is derived from the Therac 25 system specification; it represents system design assumptions regarding the\nknowledge an operator is required to have. Recall that, in general, the proposition pbad should capture the statement \u2018the current state of the system is not acceptable\u2019. In every particular system, pbad will correspond to a property\nspecific to that system. In the Therac-25 example, we identify pbad with p4  p6 : A state in which the operator has modified the data and the device is treating but the treatment is not in accordance with the defined values.\nAt any given time in system behavior, the truth value of each proposition in  is well-defined based on its\ndescription in words above. This truth value is a function of the agents\u2019 actions and the physical dynamics of the complete system. For instance, the proposition p3 standing for \u2018modified data entry complete\u2019 will be true only if the operator has indeed completed data entry and returned the cursor to the correct field; the proposition p4 , \u2018modified data entry has been processed,\u2019 holds only when the Therac device has recognized that the data has been\nmodified and has reset its treatment parameters. More specifically, the interpretation  that maps the propositions pi   and the global states g to True or False captures the behavior of the Therac human-automation system as\ndesigned.\nIn this post hoc analysis, we can straightforwardly show that no sound epistemic system exists for the design\ndepicted in Figure 1.\nTheorem 2: There can be no adequate epistemic setup E for the existing design of the Therac-25\nProof: As suggested in the main text, we prove this result via the connection between possible-worlds knowledge\nand bounded knowledge. Let R be the set of runs of the automaton depicted in Figure 1. Let rR be a run whose first\nfour global states are g0, g1, g4 and g5, and similarly let r' be a run with prefix g0, g1, g2, g3, g6, g7. Observe that r(4)=g5, while r'(6)=g7. In the run r the operator enters initial data, and then enters a modified version of the data, while the device processes the initial data, and signals that data entry is complete. Once the system ready indication is displayed the operator initiates treatment. Treatment is not, however, in accordance with the new data and thus(R,r,4)|= pbad . In the run r', the operator enters initial data, this data is processed by the device, the operator enters modified data, and the new data is also processed. In this case, the processed data coincides with the latest\nentered data, and so (R,r',6)|= pbad. The crucial point is that (r,4) ~h (r',6). It follows that (R,r,4)|= pbad  Kh pbad . By Corollary 2, we have that (E,r,4)|= pbad   \u02c6 hK pbad for every epistemic setup E for the system R. It follows that\nthere can be no adequate epistemic setup for R."}, {"heading": "D. Problem Resolution", "text": "Our approach not only identifies problems with agent knowledge, but provides insight into problem resolution and the means to determine whether the resolution is adequate. For the Therac-25 system, the modeling and analysis process suggests possible and provably correct solutions. In the situation described above it was shown that the operator had no way of determining that the device had processed the data, nor was she aware of the bad state once treatment was initiated prior to data processing. A straightforward design solution would be to modify the interface so that information regarding the device\u2019s data processing status would be always available. For instance, the operator\u2019s modification to the entered data might disable further system action and cause the system ready indicator to turn red. Only once new data had been completely entered and then processed by the device would the device display a data.ready signal to the control panel, in the operator's local view. In this manner, the human operator would be able to know when treatment could be initiated safely.\nA sound and adequate epistemic setting for this modified design would have the following features. We add\nanother proposition p7 to , standing for \"the value of data.ready is no\". The operator's bounded knowledge of system readiness to treat after modification of treatment values would then be based on the explicit information available on the interface display. It is straightforward to see that this change eliminates the design flaw that precluded satisfactory operator knowledge."}, {"heading": "E. Conclusion", "text": "This Therac-25 problem model and its suggested solution were simplified in order to serve as a straightforward example of how our approach may be applied. Note that multiple iterations of the analysis and solution definition process will derive a system design that is provably satisfactory for supervisory control. Though for most complex systems of interest neither the model nor the solution will be nearly as easy to define 5 , the Therac-25 example has shown how a \u2018real-life\u2019 system may be modeled, and its human-automation interface design evaluated, using the knowledge of its operator.\n5 We also do not suggest that adding display elements for each fact the human needs to know is a desirable or even viable solution option.\nDisplay optimization for the knowledge that is identified as necessary is (far) outside the scope of this research."}, {"heading": "IX. REFERENCES", "text": "[1] R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi, \u201cKnowledge-based programs\u201d, presented at PODC 95, Ottawa, 1995. [2] J. Y. Halpern and R. Fagin, \u201cModelling knowledge and action in distributed systems,\u201d Distributed Computing, vol. 3, pp. 159-\n179, 1989.\n[3] J. Y. Halpern and Y. Moses, \u201cKnowledge and common knowledge in a distributed environment.,\u201d Journal of the ACM, vol. 37, pp. 549-587, 1990. [4] C. Dwork and Y. Moses, \u201cKnowledge and common knowledge in a byzantine environment: crash failures,\u201d Information and Computation, vol. 88, pp. 156-186, 1990. [5] R. I. Brafman, J.-C. Latombe, Y. Moses, and Y. Shoham, \u201cKnowledge as a tool in motion planning under uncertainty,\u201d presented at Theoretical Aspects of Reasoning about Knowledge: Proc. Fifth Conference, San Francisco, California, 1994. [6] S. L. Ricker and K. Rudie, \u201cKnow means no: Incorporating knowledge into discrete-event control systems,\u201d IEEE Transactions on Automatic Control, vol. 45, pp. 1656-1668, 2000. [7] K. Rudie, S. Lafortune, and F. Lin, \u201cMinimal communication in a distributed discrete-event system,\u201d IEEE Transactions on Automatic Control, vol. 48, pp. 957-975, 2003. [8] S. Anderson and J. K. Filipe, \u201cGuaranteeing temporal validity with a real-time logic of knowledge,\u201d presented at Proceedings of the 23rd IEEE International Conference on Distributed Computing Systems Workshops, Providence, Rhode Island, 2003. [9] J. K. Filipe, M. Felici, and S. Anderson, \u201cTimed knowledge-based modelling and analysis: on the dependability of sociotechnical systems,\u201d presented at Proceedings of the 8th International Conference on Human Aspects of Advanced\nManufacturing: Agility & Hybrid Automation, Rome, Italy, 2003.\n[10] N. Leveson, Safeware: System Safety and Computers: Addison-Wesley, 1995. [11] J. F. Sowa, Knowledge Representation: Logical, Philosophical, and Computational Foundations. Pacific Grove: Brooks/Cole\nThomson Learning, 2000.\n[12] E. Davis, Representations of Commonsense Knowledge. San Mateo CA: Morgan Kaufmann, 1990. [13] R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi, Reasoning about Knowledge. Cambridge, Massachusetts: MIT Press, 2003. [14] J. Hintikka, Knowledge and Belief: An Introduction to the Logic of the Two Notions. Ithaca: Cornell University Press, 1962. [15] R. C. Moore, \u201cA formal theory of knowledge and action,\u201d in Formal Theories of the Commonsense World, J. R. Hobbs and R. C.\nMoore, Eds. Norwood New Jersey: Ablex, 1985.\n[16] T. B. Sheridan, Humans and Automation: System Design and Research Issues. Santa Monica: John Wiley & Sons, Inc., 2002. [17] T. B. Sheridan, \u201cHuman supervisory control,\u201d in Handbook of Systems Engineering and Management, A. P. Sage and W. B.\nRouse, Eds. New York: John Wiley & Sons, Inc., 1999.\n[18] C. D. Wickens and J. G. Hollands, Engineering Psychology and Human Performance, Third Edition ed. Upper Saddle River, New Jersey: Prentice Hall, 2000. [19] J. M. Carroll and J. R. Olson, \u201cMental models in human-computer interaction,\u201d in Handbook of Human-Computer Interaction, M. Helander, Ed. Amsterdam: Elsevier, 1988. [20] S. Gentner and A. L. Stevens, Mental Models. NY: Erlbaum, 1983. [21] D. A. Norman, \u201cSome observations on mental models,\u201d in Mental Models, D. Gentner and A. Stevens, Eds. Hillsdale, New\nJersey: Lawrence Erlbaum Associates, 1983.\n[22] D. Gopher and E. Donchin, \u201cWorkload: An examination of the concept,\u201d in Handbook of Perception and Performance, vol. 2, K. Boff, L. Kauffman, and J. Thomas, Eds. New York: Wiley, 1986, pp. 41-1 to 41-49. [23] D. Besnard, D. Greathead, and G. Baxter, \u201cWhen mental models go wrong: co-occurrences in dynamic, critical systems,\u201d International Journal of Human-Computer Studies, vol. 60, pp. 117-128, 2004. [24] L. J. Rips, The Psychology of Proof: Deductive Reasoning in Human Thinking. Cambridge: MIT Press, 1994. [25] C. D. Wickens, \u201cProcessing resources in attention,\u201d in Varieties of Attention, R. Parasuraman and R. Davies, Eds. New York:\nAcademic, 1984, pp. 63-98. [26] C. D. Wickens, S. Dixon, and D. Chang, \u201cUsing interference models to predict performance in a multiple-task UAV\nenvironment - 2 UAVs,\u201d Aviation Human Factors Division Institute of Aviation, Savoy, Illinois, Technical Report AHFD-039/MAAD-03-1, April 2003 2003. [27] J. Y. Halpern and Y. Moses, \u201cUsing counterfactuals in knowledge-based programming,\u201d Distributed Computing, vol. 17, pp. 91- 106, 2004. [28] Y. Moses, \u201cResource-bounded knowledge (extended abstract),\u201d presented at Proc. Second Conference on Theoretical Aspects of Reasoning About Knowledge, San Francisco, Calif., 1988. [29] Y. Moses, \u201cKnowledge and communication (a tutorial). ,\u201d presented at Theoretical Aspects of Reasoning About Knowledge: Proc. Fourth Conference, San Francisco: Calif., 1992. [30] Y. Moses and Y. Shoham, \u201cBelief as defeasible knowledge,\u201d Artificial Intelligence, vol. 64, pp. 299-322, 1993. [31] E. Mendelson, Introduction to Mathematical Logic, Fourth Edition ed. Boca Raton: Chapman & Hall, CRC Press LLC, 1997. [32] J. R. Hobbs and R. C. Moore, \u201cFormal Theories of the Commonsense World,\u201d in Ablex Series in Artificial Intelligence, Ablex,\nEd. Norwood, New Jersey: Ablex Publishing Company, 1985.\n[33] J. Y. Halpern, Y. Moses, and M. Y. Vardi, \u201cAlgorithmic knowledge,\u201d presented at Theoretical Aspects of Reasoning About Knowledge: Proceedings of the Fifth Conference, San Francisco, Calif, 1994. [34] J. R. Anderson and C. Lebiere, The Atomic Components of Thought. Mahwah, NJ: Lawrence Erlbaum Associates, 1998. [35] M. D. Byrne and A. Kirlik, \u201cUsing computational cognitive modeling to diagnose possible sources of aviation error,\u201d Aviation\nHuman Factors Division, Institute of Aviation, University of Illinois Technical Report AHFD-03-14/NASA-03-4, 2003.\n[36] R. Stout, J. A. Cannon-Bowers, and E. Salas, \u201cThe role of shared mental models in developing team situation awareness: implications for training,\u201d Training Research Journal, vol. 2, pp. 85-116, 1996. [37] K. Konolige, \u201cBelief and incompleteness,\u201d in Formal Theories of the Commonsense World, J. R. Hobbs and R. C. Moore, Eds. Norwood NJ: Ablex, 1985.\n[38] R. M. J. Bryne, \u201cMental models and counterfactual thoughts about what might have been,\u201d Trends in Cognitive Science, vol. 6, pp. 426-431, 2002. [39] R. M. J. Byrne and S. M. Egan, \u201cCounterfactual and prefactual conditionals,\u201d Canadian Journal of Experimental Psychology, vol. 58, pp. 113-120, 2004. [40] V. A. Thompson and R. M. J. Byrne, \u201cReasoning counterfactually: making inferences about things that didn't happen,\u201d Journal of Experimental Psychology: Learning, Memory, and Cognition, vol. 28, pp. 1154-1170, 2002. [41] L. Bainbridge, \u201cTypes of representation,\u201d in Tasks, Errors and Mental Models, L. P. Goodstein, H. B. Anderson, and S. E. Olsen, Eds. London: Taylor and Francis Ltd., 1988. [42] D. D. Woods and E. M. Roth, \u201cCognitive systems engineering,\u201d in Handbook of Human-Computer Interaction, M. Helander, Ed. New York: North-Holland, 1988. [43] . McCarthy and P. J. Hayes, \u201cSome philosophical problems from the standpoint of artificial intelligence,\u201d Machine Intelligence, vol. 6, 1969. [44] K. Christoffersen and D. D. Woods, \u201cHow to make automated systems team players,\u201d in Advances in Human Performance and Cognitive Engineering Research, vol. 2: Elsevier Science Ltd., 2002, pp. 1-12. [45] S. Kiesler, \u201cFostering common ground in human-robot interaction,\u201d presented at Proceedings of the IEEE International Workshop on Robots and Human Interactive Communication (RO-MAN), 2005. [46] G. Klein, P. J. Feltovich, J. M. Bradshaw, and D. D. Woods, \u201cCommon ground and coordination in joint activity,\u201d in Organizational Simulation, W. R. Rouse and K. B. Boff, Eds. New York City, NY: John Wiley, 2004, pp. (pp. in press). [47] J. L. Burke, R. R. Murphy, E. Rogers, V. J. Lumelsky, and J. Scholtz, \u201cFinal report for the DARPA/NSF interdisciplinary study on human-robot interaction,\u201d IEEE Transactions on Systems, Man, and Cybernetics-Part C: Applications and Reviews, vol. 34,\npp. 103-112, 2004.\n[48] R. R. Murphy, \u201cHuman-robot interaction in rescue robotics,\u201d IEEE Transactions on Systems, Man, and Cybernetics, vol. 34, pp. 138-153, 2004."}], "references": [{"title": "Modelling knowledge and action in distributed systems", "author": ["J.Y. Halpern", "R. Fagin"], "venue": "Distributed Computing, vol. 3, pp. 159- 179, 1989.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Knowledge and common knowledge in a distributed environment", "author": ["J.Y. Halpern", "Y. Moses"], "venue": "Journal of the ACM, vol. 37, pp. 549-587, 1990.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "Knowledge and common knowledge in a byzantine environment: crash failures", "author": ["C. Dwork", "Y. Moses"], "venue": "Information and Computation, vol. 88, pp. 156-186, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Knowledge as a tool in motion planning under uncertainty", "author": ["R.I. Brafman", "J.-C. Latombe", "Y. Moses", "Y. Shoham"], "venue": "presented at Theoretical Aspects of Reasoning about Knowledge: Proc. Fifth Conference, San Francisco, California, 1994.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Know means no: Incorporating knowledge into discrete-event control systems", "author": ["S.L. Ricker", "K. Rudie"], "venue": "IEEE Transactions on Automatic Control, vol. 45, pp. 1656-1668, 2000.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Minimal communication in a distributed discrete-event system", "author": ["K. Rudie", "S. Lafortune", "F. Lin"], "venue": "IEEE Transactions on Automatic Control, vol. 48, pp. 957-975, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Guaranteeing temporal validity with a real-time logic of knowledge", "author": ["S. Anderson", "J.K. Filipe"], "venue": "presented at Proceedings of the 23rd IEEE International Conference on Distributed Computing Systems Workshops, Providence, Rhode Island, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Timed knowledge-based modelling and analysis: on the dependability of sociotechnical systems", "author": ["J.K. Filipe", "M. Felici", "S. Anderson"], "venue": "presented at Proceedings of the 8th International Conference on Human Aspects of Advanced Manufacturing: Agility & Hybrid Automation, Rome, Italy, 2003.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Knowledge Representation: Logical, Philosophical, and Computational Foundations", "author": ["J.F. Sowa"], "venue": "Pacific Grove: Brooks/Cole Thomson Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Representations of Commonsense Knowledge", "author": ["E. Davis"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1990}, {"title": "Knowledge and Belief: An Introduction to the Logic of the Two Notions", "author": ["J. Hintikka"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1962}, {"title": "A formal theory of knowledge and action", "author": ["R.C. Moore"], "venue": "Formal Theories of the Commonsense World, J. R. Hobbs and R. C. Moore, Eds. Norwood New Jersey: Ablex, 1985.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1985}, {"title": "Humans and Automation: System Design and Research Issues", "author": ["T.B. Sheridan"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Human supervisory control", "author": ["T.B. Sheridan"], "venue": "Handbook of Systems Engineering and Management, A. P. Sage and W. B. Rouse, Eds. New York: John Wiley & Sons, Inc., 1999.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Engineering Psychology and Human Performance, Third Edition ed", "author": ["C.D. Wickens", "J.G. Hollands"], "venue": "Upper Saddle River,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Mental models in human-computer interaction", "author": ["J.M. Carroll", "J.R. Olson"], "venue": "Handbook of Human-Computer Interaction, M. Helander, Ed. Amsterdam: Elsevier, 1988.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "Some observations on mental models", "author": ["D.A. Norman"], "venue": "Mental Models, D. Gentner and A. Stevens, Eds. Hillsdale, New Jersey: Lawrence Erlbaum Associates, 1983.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1983}, {"title": "Workload: An examination of the concept", "author": ["D. Gopher", "E. Donchin"], "venue": "Handbook of Perception and Performance, vol. 2, K. Boff, L. Kauffman, and J. Thomas, Eds. New York: Wiley, 1986, pp. 41-1 to 41-49.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1986}, {"title": "When mental models go wrong: co-occurrences in dynamic, critical systems", "author": ["D. Besnard", "D. Greathead", "G. Baxter"], "venue": "International Journal of Human-Computer Studies, vol. 60, pp. 117-128, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "The Psychology of Proof: Deductive Reasoning in Human Thinking", "author": ["L.J. Rips"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Processing resources in attention", "author": ["C.D. Wickens"], "venue": "Varieties of Attention, R. Parasuraman and R. Davies, Eds. New York: Academic, 1984, pp. 63-98.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1984}, {"title": "Using interference models to predict performance in a multiple-task UAV environment - 2 UAVs", "author": ["C.D. Wickens", "S. Dixon", "D. Chang"], "venue": "Aviation Human Factors Division Institute of Aviation, Savoy, Illinois, Technical Report AHFD-03- 9/MAAD-03-1, April 2003 2003.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "Using counterfactuals in knowledge-based programming", "author": ["J.Y. Halpern", "Y. Moses"], "venue": "Distributed Computing, vol. 17, pp. 91- 106, 2004.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Resource-bounded knowledge (extended abstract)", "author": ["Y. Moses"], "venue": "presented at Proc. Second Conference on Theoretical Aspects of Reasoning About Knowledge, San Francisco, Calif., 1988.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1988}, {"title": "Knowledge and communication (a tutorial)", "author": ["Y. Moses"], "venue": "presented at Theoretical Aspects of Reasoning About Knowledge: Proc. Fourth Conference, San Francisco: Calif., 1992.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1992}, {"title": "Belief as defeasible knowledge", "author": ["Y. Moses", "Y. Shoham"], "venue": "Artificial Intelligence, vol. 64, pp. 299-322, 1993.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1993}, {"title": "Introduction to Mathematical Logic, Fourth Edition ed", "author": ["E. Mendelson"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1997}, {"title": "Formal Theories of the Commonsense World", "author": ["J.R. Hobbs", "R.C. Moore"], "venue": "Ablex Series in Artificial Intelligence, Ablex, Ed. Norwood, New Jersey: Ablex Publishing Company, 1985.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1985}, {"title": "Algorithmic knowledge", "author": ["J.Y. Halpern", "Y. Moses", "M.Y. Vardi"], "venue": "presented at Theoretical Aspects of Reasoning About Knowledge: Proceedings of the Fifth Conference, San Francisco, Calif, 1994.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1994}, {"title": "The Atomic Components of Thought", "author": ["J.R. Anderson", "C. Lebiere"], "venue": "Mahwah, NJ: Lawrence Erlbaum Associates,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Using computational cognitive modeling to diagnose possible sources of aviation error", "author": ["M.D. Byrne", "A. Kirlik"], "venue": "Aviation Human Factors Division, Institute of Aviation, University of Illinois Technical Report AHFD-03-14/NASA-03-4, 2003.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2003}, {"title": "The role of shared mental models in developing team situation awareness: implications for training", "author": ["R. Stout", "J.A. Cannon-Bowers", "E. Salas"], "venue": "Training Research Journal, vol. 2, pp. 85-116, 1996.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1996}, {"title": "Belief and incompleteness", "author": ["K. Konolige"], "venue": "Formal Theories of the Commonsense World, J. R. Hobbs and R. C. Moore, Eds. Norwood NJ: Ablex, 1985.  39", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1985}, {"title": "Mental models and counterfactual thoughts about what might have been", "author": ["R.M.J. Bryne"], "venue": "Trends in Cognitive Science, vol. 6, pp. 426-431, 2002.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "Counterfactual and prefactual conditionals", "author": ["R.M.J. Byrne", "S.M. Egan"], "venue": "Canadian Journal of Experimental Psychology, vol. 58, pp. 113-120, 2004.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Reasoning counterfactually: making inferences about things that didn't happen", "author": ["V.A. Thompson", "R.M.J. Byrne"], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition, vol. 28, pp. 1154-1170, 2002.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2002}, {"title": "Types of representation", "author": ["L. Bainbridge"], "venue": "Tasks, Errors and Mental Models, L. P. Goodstein, H. B. Anderson, and S. E. Olsen, Eds. London: Taylor and Francis Ltd., 1988.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1988}, {"title": "Cognitive systems engineering", "author": ["D.D. Woods", "E.M. Roth"], "venue": "Handbook of Human-Computer Interaction, M. Helander, Ed. New York: North-Holland, 1988.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1988}, {"title": "Some philosophical problems from the standpoint of artificial intelligence", "author": ["McCarthy", "P.J. Hayes"], "venue": "Machine Intelligence, vol. 6, 1969.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1969}, {"title": "How to make automated systems team players", "author": ["K. Christoffersen", "D.D. Woods"], "venue": "Advances in Human Performance and Cognitive Engineering Research, vol. 2: Elsevier Science Ltd., 2002, pp. 1-12.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "Fostering common ground in human-robot interaction", "author": ["S. Kiesler"], "venue": "presented at Proceedings of the IEEE International Workshop on Robots and Human Interactive Communication (RO-MAN), 2005.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2005}, {"title": "Common ground and coordination in joint activity", "author": ["G. Klein", "P.J. Feltovich", "J.M. Bradshaw", "D.D. Woods"], "venue": "Organizational Simulation, W. R. Rouse and K. B. Boff, Eds. New York City, NY: John Wiley, 2004, pp. (pp. in press).", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2004}, {"title": "Final report for the DARPA/NSF interdisciplinary study on human-robot interaction", "author": ["J.L. Burke", "R.R. Murphy", "E. Rogers", "V.J. Lumelsky", "J. Scholtz"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics-Part C: Applications and Reviews, vol. 34, pp. 103-112, 2004.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2004}, {"title": "Human-robot interaction in rescue robotics", "author": ["R.R. Murphy"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, vol. 34, pp. 138-153, 2004.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "In this paper we introduce a formal approach to reasoning about knowledge in human-automation systems that is based on a model of knowledge developed by Halpern and Moses and their colleagues [1]-[3].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": ", communication protocols for distributed computer systems [1], [4], robot motion planning [5], adding notions of knowledge and communication to discrete event control systems [6], [7]).", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": ", communication protocols for distributed computer systems [1], [4], robot motion planning [5], adding notions of knowledge and communication to discrete event control systems [6], [7]).", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": ", communication protocols for distributed computer systems [1], [4], robot motion planning [5], adding notions of knowledge and communication to discrete event control systems [6], [7]).", "startOffset": 176, "endOffset": 179}, {"referenceID": 5, "context": ", communication protocols for distributed computer systems [1], [4], robot motion planning [5], adding notions of knowledge and communication to discrete event control systems [6], [7]).", "startOffset": 181, "endOffset": 184}, {"referenceID": 6, "context": "The value of reasoning about knowledge in the analysis of socio-technical systems has already been noted in the literature [8], [9].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "The value of reasoning about knowledge in the analysis of socio-technical systems has already been noted in the literature [8], [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "Philosophers from ancient times have considered fundamental questions such as the nature of knowledge and the origin of its existence, and began the development of logics and formal modeling in order to more precisely reason about these complex and often subtle concepts [11].", "startOffset": 271, "endOffset": 275}, {"referenceID": 9, "context": "Valuable outgrowths of work in these and other important fields include both strong support for the critical value of knowledge as a formal construct, and richly descriptive tools for precisely describing and reasoning about knowledge [12]-[15].", "startOffset": 235, "endOffset": 239}, {"referenceID": 11, "context": "Valuable outgrowths of work in these and other important fields include both strong support for the critical value of knowledge as a formal construct, and richly descriptive tools for precisely describing and reasoning about knowledge [12]-[15].", "startOffset": 240, "endOffset": 244}, {"referenceID": 12, "context": "What is the knowledge that is required for supervisory control? Sheridan notes that to function as a supervisory controller, the human agent must be able to command the system in accordance with defined specifications, monitor the system\u2019s behavior to ensure that it performs as required, and identify and correct anomalous system behaviors [16], [17].", "startOffset": 341, "endOffset": 345}, {"referenceID": 13, "context": "What is the knowledge that is required for supervisory control? Sheridan notes that to function as a supervisory controller, the human agent must be able to command the system in accordance with defined specifications, monitor the system\u2019s behavior to ensure that it performs as required, and identify and correct anomalous system behaviors [16], [17].", "startOffset": 347, "endOffset": 351}, {"referenceID": 14, "context": "While the relationship between displayed information and useful knowledge may depend on non-trivial factors such as the agent correctly perceiving the display and understanding its meaning, the compatibility of the display format to the task, and so forth [18], we assume here that the display type, quality, and information saliency are adequate for the task at hand.", "startOffset": 256, "endOffset": 260}, {"referenceID": 15, "context": "A second type of knowledge needed for human-supervisory control is the 'knowledge-in-the-head' that the human agent possesses regarding the global behaviors and properties of the physical system with which he is interacting [19]-[21].", "startOffset": 224, "endOffset": 228}, {"referenceID": 16, "context": "A second type of knowledge needed for human-supervisory control is the 'knowledge-in-the-head' that the human agent possesses regarding the global behaviors and properties of the physical system with which he is interacting [19]-[21].", "startOffset": 229, "endOffset": 233}, {"referenceID": 12, "context": "Mental model knowledge is required both for reasoning about and interpreting current interface information as well as for future-oriented supervisory control tasks such as planning and scheduling, troubleshooting, and decision-making [16]-[18].", "startOffset": 234, "endOffset": 238}, {"referenceID": 14, "context": "Mental model knowledge is required both for reasoning about and interpreting current interface information as well as for future-oriented supervisory control tasks such as planning and scheduling, troubleshooting, and decision-making [16]-[18].", "startOffset": 239, "endOffset": 243}, {"referenceID": 14, "context": "We suggest that this knowledge is \u2018automatic\u2019 in nature and thus requires almost no effortful thinking or reasoning action [18], [22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "We suggest that this knowledge is \u2018automatic\u2019 in nature and thus requires almost no effortful thinking or reasoning action [18], [22].", "startOffset": 129, "endOffset": 133}, {"referenceID": 18, "context": "perhaps incorrect) beliefs [23].", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "Precisely how humans reason, the rules of inference that guide their reasoning activities, or indeed whether humans use any form of mental logic at all are on-going and strongly debated questions in the literature [24].", "startOffset": 214, "endOffset": 218}, {"referenceID": 14, "context": "A related characteristic of human reasoning ability and the knowledge that results is the question of the operator\u2019s computational power \u2013 even if the operator only reaches conclusions based on the inferences in her mental model, how many conclusions can she reach in a bounded period of time? If a conclusion requires conclusions from other inferences as inputs or antecedents, how long can this chain of inferences be before the human operator is overwhelmed? As research shows, human computational power is quite limited [18], [22], [25] and even more so in", "startOffset": 524, "endOffset": 528}, {"referenceID": 17, "context": "A related characteristic of human reasoning ability and the knowledge that results is the question of the operator\u2019s computational power \u2013 even if the operator only reaches conclusions based on the inferences in her mental model, how many conclusions can she reach in a bounded period of time? If a conclusion requires conclusions from other inferences as inputs or antecedents, how long can this chain of inferences be before the human operator is overwhelmed? As research shows, human computational power is quite limited [18], [22], [25] and even more so in", "startOffset": 530, "endOffset": 534}, {"referenceID": 20, "context": "A related characteristic of human reasoning ability and the knowledge that results is the question of the operator\u2019s computational power \u2013 even if the operator only reaches conclusions based on the inferences in her mental model, how many conclusions can she reach in a bounded period of time? If a conclusion requires conclusions from other inferences as inputs or antecedents, how long can this chain of inferences be before the human operator is overwhelmed? As research shows, human computational power is quite limited [18], [22], [25] and even more so in", "startOffset": 536, "endOffset": 540}, {"referenceID": 21, "context": "the time- or safety-critical environments within which most complex systems function [26].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "and was expanded on in numerous writings [1], [2], [5], [27]-[30].", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "and was expanded on in numerous writings [1], [2], [5], [27]-[30].", "startOffset": 51, "endOffset": 54}, {"referenceID": 22, "context": "and was expanded on in numerous writings [1], [2], [5], [27]-[30].", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "and was expanded on in numerous writings [1], [2], [5], [27]-[30].", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": "The primary references for this part are [1]-[3].", "startOffset": 45, "endOffset": 48}, {"referenceID": 26, "context": "two [31].", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "5) Knowledge as truth in all possible worlds The formal definition of knowledge is traditionally framed within the notion of possible worlds [13], [14].", "startOffset": 147, "endOffset": 151}, {"referenceID": 32, "context": "2 This point is most succinctly emphasized by Konolige [37] who notes that if humans were capable of unlimited reasoning, a chess player would know the outcome of a chess match immediately following the first move.", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "Rather than using possible-worlds semantics as the primary component, we will present a syntactic model in which the known facts will be obtained based on a restricted amount of reasoning, and in which what is boundedly known may not be true (for related approaches to syntactic knowledge see [13], [32]).", "startOffset": 299, "endOffset": 303}, {"referenceID": 28, "context": "(See [33] for definitions of knowledge that take computational complexity into account.", "startOffset": 5, "endOffset": 9}, {"referenceID": 29, "context": "For instance, we may wish to consider a classification of human knowledge in terms of its contents in addition to the type-based taxonomy defined in the current paper, such as the declarative / procedural dimension often used in cognitive modeling [34]-[36].", "startOffset": 248, "endOffset": 252}, {"referenceID": 31, "context": "For instance, we may wish to consider a classification of human knowledge in terms of its contents in addition to the type-based taxonomy defined in the current paper, such as the declarative / procedural dimension often used in cognitive modeling [34]-[36].", "startOffset": 253, "endOffset": 257}, {"referenceID": 25, "context": "Belief has been formally represented using a number of techniques [30], [37], and a valuable goal is to identify and build on the technique most appropriate for modeling a notion of human belief in domains of interest.", "startOffset": 66, "endOffset": 70}, {"referenceID": 32, "context": "Belief has been formally represented using a number of techniques [30], [37], and a valuable goal is to identify and build on the technique most appropriate for modeling a notion of human belief in domains of interest.", "startOffset": 72, "endOffset": 76}, {"referenceID": 33, "context": "An additional element of human knowledge and reasoning that is particularly important in the supervisory control context is the concept of counterfactual reasoning (\u2018if p were to hold then q would be true\u2019) [38].", "startOffset": 207, "endOffset": 211}, {"referenceID": 34, "context": "When an operator plans future actions, especially error recovery actions, counterfactual reasoning supports the operator\u2019s consideration of conditional alternatives and the outcomes of hypothetical scenarios [39], [40].", "startOffset": 208, "endOffset": 212}, {"referenceID": 35, "context": "When an operator plans future actions, especially error recovery actions, counterfactual reasoning supports the operator\u2019s consideration of conditional alternatives and the outcomes of hypothetical scenarios [39], [40].", "startOffset": 214, "endOffset": 218}, {"referenceID": 22, "context": "Preliminary work suggests that incorporating an existing formalization of counterfactual reasoning [27] into our approach will provide designers with a means to do so.", "startOffset": 99, "endOffset": 103}, {"referenceID": 36, "context": "If qualitative differences in knowledge distinguish between novice and expert human operators [41], [42], for example, this would allow a designer to gauge the vulnerability of the system to novice supervisory control and might make salient required emphases in training.", "startOffset": 94, "endOffset": 98}, {"referenceID": 37, "context": "If qualitative differences in knowledge distinguish between novice and expert human operators [41], [42], for example, this would allow a designer to gauge the vulnerability of the system to novice supervisory control and might make salient required emphases in training.", "startOffset": 100, "endOffset": 104}, {"referenceID": 38, "context": "Again, the goal should not be to draw a true and faithful picture of human knowledge, but rather to develop a representation that is epistemically adequate [43] and that allows us to reason expressively and formally about important properties of human knowledge within the context of complex systems.", "startOffset": 156, "endOffset": 160}, {"referenceID": 1, "context": ") [3].", "startOffset": 2, "endOffset": 5}, {"referenceID": 39, "context": "The importance of being able to reason formally about the knowledge of groups of agents in the design and analysis of human-automation systems is significant, as noted in [44]-[46].", "startOffset": 171, "endOffset": 175}, {"referenceID": 41, "context": "The importance of being able to reason formally about the knowledge of groups of agents in the design and analysis of human-automation systems is significant, as noted in [44]-[46].", "startOffset": 176, "endOffset": 180}, {"referenceID": 42, "context": "What are the theoretical issues of shared knowledge relevant to a human-automation system? It is natural to say that an operator may need to know what the automation knows, but when is it useful (and meaningful) to talk about an automation agent knowing what a human agent knows or what other automation agents know? Considering various forms of human-robot teams, for example, the need for a robot to know what the human agent knows seems clear in the case of search and rescue, personal assistant, or physical therapy robots [47].", "startOffset": 527, "endOffset": 531}, {"referenceID": 40, "context": "On the one hand, the importance of capturing notions of agent knowledge and common knowledge, and the need for a formal method for reasoning about knowledge in this domain, have already been noted in the literature [45], [48], and so the potential value of our approach is clear.", "startOffset": 215, "endOffset": 219}, {"referenceID": 43, "context": "On the one hand, the importance of capturing notions of agent knowledge and common knowledge, and the need for a formal method for reasoning about knowledge in this domain, have already been noted in the literature [45], [48], and so the potential value of our approach is clear.", "startOffset": 221, "endOffset": 225}], "year": 2013, "abstractText": "In a supervisory control system the human agent\u2019s knowledge of past, current, and future system behavior is critical for system performance. Being able to reason about that knowledge in a precise and structured manner is central to effective system design. In this paper we introduce the application of a wellestablished formal approach to reasoning about knowledge to the modeling and analysis of complex humanautomation systems. An intuitive notion of knowledge in human-automation systems is sketched and then cast as a formal model. We present a case study in which the approach is used to model and reason about a classic problem from the human-automation systems literature; the results of our analysis provide evidence for the validity and value of reasoning about complex systems in terms of the knowledge of the system\u2019s agents. To conclude, we discuss research directions that will extend this approach, and note several systems in the aviation and human-robot team domains that are of particular interest.", "creator": "Microsoft\u00ae Word 2010"}}}