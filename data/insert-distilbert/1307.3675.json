{"id": "1307.3675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jul-2013", "title": "Minimum Error Rate Training and the Convex Hull Semiring", "abstract": "we describe the probability line search used in the minimum error rate training algorithm mert as the \" symmetric inside score \" of a weighted proof forest under a semiring defined in terms of purely well - understood operations from computational geometry. this conception leads to a straightforward complexity analysis of the pure dynamic programming mert algorithms of maurice macherey et al. ( 2008 ) and kala kumar et al. ( 2009 ) and practical performance approaches to robust implementation.", "histories": [["v1", "Sat, 13 Jul 2013 19:38:09 GMT  (17kb,D)", "http://arxiv.org/abs/1307.3675v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chris dyer"], "accepted": false, "id": "1307.3675"}, "pdf": {"name": "1307.3675.pdf", "metadata": {"source": "CRF", "title": "Minimum Error Rate Training and the Convex Hull Semiring\u2217", "authors": ["Chris Dyer"], "emails": ["cdyer@cs.cmu.edu"], "sections": [{"heading": null, "text": "We describe the line search used in the minimum error rate training algorithm (Och, 2003) as the \u201cinside score\u201d of a weighted proof forest under a semiring defined in terms of wellunderstood operations from computational geometry. This conception leads to a straightforward complexity analysis of the dynamic programming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009) and practical approaches to implementation."}, {"heading": "1 Introduction", "text": "Och\u2019s (2003) algorithm for minimum error rate training (MERT) is widely used in the direct loss minimization of linear translation models. It is based on an efficient and optimal line search and can optimize non-differentiable, corpus-level loss functions. While the original algorithm used n-best hypothesis lists to learn from, more recent work has developed dynamic programming variants that leverage much larger sets of hypotheses encoded in finite-state lattices and context-free hypergraphs (Macherey et al., 2008; Kumar et al., 2009; Sokolov and Yvon, 2011). Although MERT has several attractive properties (\u00a72) and is widely used in MT, previous work has failed to explicate its close relationship to more familiar inference algorithms, and, as a result, it is less well understood than many other optimization algorithms.\n\u2217While preparing these notes, I discovered the work by Sokolov and Yvon (2011), who elucidated the semiring properties of the MERT line search computation. Because they did not discuss the polynomial bounds on the growth of the values while running the inside algorithm, I have posted this as an unpublished manuscript.\nIn this paper, we show that the both the original (Och, 2003) and newer dynamic programming algorithms given by Macherey et al. (2008) and Kumar et al. (2009) can be understood as weighted logical deductions (Goodman, 1999; Lopez, 2009; Eisner and Filardo, 2011) using weights from a previously undescribed semiring, which we call the convex hull semiring (\u00a73). Our description of the algorithm in terms of semiring computations has both theoretical and practical benefits: we are able to provide a straightforward complexity analysis and an improved DP algorithm with better asymptotic and observed run-time (\u00a74). More practically still, since many tools for structured prediction over discrete sequences support generic semiring-weighted inference (Allauzen et al., 2007; Li et al., 2009; Dyer et al., 2010; Eisner and Filardo, 2011), our analysis makes it is possible to add dynamic programming MERT to them with little effort."}, {"heading": "2 Minimum error rate training", "text": "The goal of MERT is to find a weight vector w\u2217 \u2208 Rd that minimizes a corpus-level loss L (with respect to a development set D) incurred by a decoder that selects the most highly-weighted output of a linear structured prediction model parameterized by feature vector function H:\nw\u2217 = argmin w L({y\u0302wi },D)\n{y\u0302wi } = arg maxy\u2208Y(xi) w>H(xi, y) \u2200(xi, ygoldi ) \u2208 D\nWe assume that the loss L is computed using a vector error count function \u03b4(y\u0302, y) \u2192 Rm and a loss\nar X\niv :1\n30 7.\n36 75\nv1 [\ncs .L\nG ]\n1 3\nJu l 2\n01 3\nscalarizer L : Rm \u2192 R, and that the error count decomposes linearly across examples:1\nL({y\u0302wi },D) = L  |D|\u2211 i=1 \u03b4(y\u0302i, y gold i ) \nAt each iteration of the optimization algorithm, MERT choses a starting weight vector w0 and a search direction vector v (both \u2208 Rd) and determines which candidate in a set has the highest model score for all weight vectors w\u2032 = \u03b7v + w0, as \u03b7 sweeps from \u2212\u221e to +\u221e.2\nTo understand why this is potentially tractable, consider any (finite) set of outputs {yj} \u2286 Y(x) for an input x (e.g., an n-best list, a list of n random samples, or the complete proof forest of a weighted deduction). Each output yj has a corresponding feature vector H(x, yj), which means that the model score for each hypothesis, together with \u03b7, form a line in R2:\ns(\u03b7) = (\u03b7v + w0)>H(x, yj)\n= \u03b7 v>H(x, yj)\ufe38 \ufe37\ufe37 \ufe38 slope +w>0 H(x, yj)\ufe38 \ufe37\ufe37 \ufe38 y-intercept .\nThe upper part of Figure 1 illustrates how the model scores (y-axis) of each output in an example hypothesis set vary with \u03b7 (x-axis). The lower part shows how this induces a piecewise constant error surface (i.e., \u03b4(y\u0302\u03b7v+w0 , ygold)). Note that y3 has a model score that is always strictly less than the score of some other output at all values of \u03b7. Detecting such \u201cobscured\u201d lines is useful because it is unnecessary to compute their error counts. There is simply no setting of \u03b7 that will yield weights for which y3 will be ranked highest by the decoder.3\n1Nearly every evaluation metric used in NLP and MT fulfills these criteria, including F-measure, BLEU, METEOR, TER, AER, and WER. Unlike many dynamic programming optimization algorithms, the error count function \u03b4 is not required to decompose with the structure of the model.\n2Several strategies have been proposed for selecting v and w0. For an overview, refer to Galley and Quirk (2011) and references therein.\n3Since \u03b4 need only be evaluated for the (often small) subset of candidates that can obtain the highest model score at some \u03b7, it is possible to use relatively computationally expensive loss\nBy summing the error surfaces for each sentence in the development set, a corpus-level error surface is created. Then, by traversing this from left to right and selecting best scoring segment (transforming each segment\u2019s corpus level error count to a loss with L), the optimal \u03b7 for updating w0 can be determined.4"}, {"heading": "2.1 Point-line duality", "text": "The set of line segments corresponding to the maximum model score at every \u03b7 form an upper envelope. To determine which lines (and corresponding hypotheses) these are, we turn to standard algorithms from computational geometry. While algorithms for directly computing the upper envelop of a set of lines do exist, we proceed by noting that computing the upper envelope has as a dual problem that can be solved instead: finding the lower convex hull of a set of points (de Berg et al., 2010). The dual representation of a line of the form y = mx+b is the point (m,\u2212b). This, for a given output, w0, v, and feature vector H, the line showing how the model score of the output hypothesis varies with \u03b7 can simply be represented by the point (v>H,\u2212w>0 H).\nfunctions. Zaidan and Callison-Burch (2009) exploit this and find that it is even feasible to solicit human judgments while evaluating \u03b4!\n4Macherey et al. (2008) recommend selecting the midpoint of the segment with the best loss, but Cer et al. (2008) suggest other strategies.\nFigure 2 illustrates the line-point duality and the relationship between the primal upper envelope and dual lower convex hull. Usefully, the \u03b7 coordinates (along the x-axis in the primal form) where upperenvelope lines intersect and the error count changes are simply the slopes of the lines connecting the corresponding points in the dual."}, {"heading": "3 The Convex Hull Semiring", "text": "Definition 1. A semiring K is a quintuple \u3008K,\u2295,\u2297, 0, 1\u3009 consisting of a set K, an addition operator \u2295 that is associative and commutative, a multiplication operator \u2297 that is associative, and the values 0 and 1 in K, which are the additive and multiplicative identities, respectively. \u2297 must distribute over \u2295 from the left or right (or both), i.e., a \u2297 (b \u2295 c) = (a \u2297 b) \u2295 (a \u2297 c) or (b \u2295 c) \u2297 a = (b\u2297a)\u2295 (c\u2297a). Additionally, 0\u2297u = 0 must hold for any u \u2208 K. If a semiring K has a commutative \u2297 operator, the semiring is said to be commutative. If K has an idempotent \u2295 operator (i.e., a \u2295 a = a for all a \u2208 K), then K is said to be idempotent. Definition 2. The Convex Hull Semiring. Let (K,\u2295,\u2297, 0, 1) be defined as follows:\nK A set of points in the plane that are the extreme points of a convex hull.\nA\u2295B conv [A \u222aB] A\u2297B convex hull of the Minkowski sum, i.e.,\nconv{(a1 + b1, a2 + b2) | (a1, a2) \u2208 A \u2227 (b1, b2) \u2208 B}\n0 \u2205 1 {(0, 0)}\nTheorem 1. The Convex Hull Semiring fulfills the semiring axioms and is commutative and idempotent.\nProof. To show that this is a semiring, we need only to demonstrate that commutativity and associativity hold for both addition and multiplication, from which distributivity follows. Commutativity (A \u00b7 B = B \u00b7 A) follows straightforwardly from the definitions of addition and multiplication, as do the identities. Proving associativity is a bit more subtle on account of the conv operator. For multiplication, we rely on results of Krein and S\u030cmulian (1940), who show that\nconv [A+Mink. B] = conv [conv A+Mink. conv B] .\nFor addition, we make an informal argument that a context hull circumscribes a set of points, and convexification removes the interior ones. Thus, addition continually expands the circumscribed sets, regardless of what their interiors were, so order does not matter. Finally, addition is idempotent since conv [A \u222aA] = A."}, {"heading": "4 Complexity", "text": "Shared structures such as finite-state automata and context-free grammars encode an exponential number of different derivations in polynomial space. Since the values of the convex hull semiring are themselves sets, it is important to understand how their sizes grow. Fortunately, we can state the following tight bounds, which guarantee that growth will be worst case linear in the size of the input grammar: Theorem 2. |A\u2295B| \u2264 |A|+ |B|. Theorem 3. |A\u2297B| \u2264 |A|+ |B|. The latter fact is particularly surprising, since multiplication appears to have a bound of |A| \u00d7 |B|. The linear (rather than multiplicative) complexity bound for Minkowski addition is the result of Theorem 13.5 in de Berg et al. (2010). From these inequalities, it follows straightforwardly that the number of points in a derivation forest\u2019s total convex hull is upper bounded by |E|.5"}, {"heading": "Acknowledgements", "text": "We thank David Mount for suggesting the point-line duality and pointing us to the relevant literature in computational geometry and Adam Lopez for the TikZ MERT figures."}], "references": [{"title": "OpenFst: A general and efficient weighted finite-state transducer library", "author": ["Allauzen et al.2007] C. Allauzen", "M. Riley", "J. Schalkwyk", "W. Skut", "M. Mohri"], "venue": "In Proc. of CIAA,", "citeRegEx": "Allauzen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Allauzen et al\\.", "year": 2007}, {"title": "Regularization and search for minimum error rate training", "author": ["Cer et al.2008] D. Cer", "D. Jurafsky", "C.D. Manning"], "venue": "In Proc. ACL", "citeRegEx": "Cer et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cer et al\\.", "year": 2008}, {"title": "Computational Geometry: Algorithms and Applications", "author": ["de Berg et al.2010] M. de Berg", "M. van Kreveld", "M. Overmars", "O. Schwarzkopf"], "venue": null, "citeRegEx": "Berg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2010}, {"title": "cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models", "author": ["Dyer et al.2010] C. Dyer", "A. Lopez", "J. Ganitkevitch", "J. Weese", "F. Ture", "P. Blunsom", "H. Setiawan", "V. Eidelman", "P. Resnik"], "venue": "In Proc. of ACL", "citeRegEx": "Dyer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2010}, {"title": "Datalog 2.0, chapter Dyna: Extending Datalog", "author": ["Eisner", "Filardo2011] J. Eisner", "N.W. Filardo"], "venue": null, "citeRegEx": "Eisner et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eisner et al\\.", "year": 2011}, {"title": "Optimal search for minimum error rate training", "author": ["Galley", "Quirk2011] M. Galley", "C. Quirk"], "venue": "In Proc. of EMNLP", "citeRegEx": "Galley et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2011}, {"title": "Efficient minimum error rate training and minimum Bayes-risk decoding for translation hypergraphs and lattices", "author": ["S. Kumar", "W. Macherey", "C. Dyer", "F. Och"], "venue": "In Proc. of ACL-IJCNLP", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Joshua: An open source toolkit for parsing-based machine translation", "author": ["Z. Li", "C. Callison-Burch", "C. Dyer", "S. Khudanpur", "L. Schwartz", "W. Thornton", "J. Weese", "O. Zaidan"], "venue": "In Proc. of the Fourth Workshop on Statistical Machine Translation", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Translation as weighted deduction", "author": ["A. Lopez"], "venue": "In Proc. of EACL", "citeRegEx": "Lopez.,? \\Q2009\\E", "shortCiteRegEx": "Lopez.", "year": 2009}, {"title": "Lattice-based minimum error rate training for statistical machine translation", "author": ["Macherey et al.2008] W. Macherey", "F.J. Och", "I. Thayer", "J. Uszkoreit"], "venue": "In Proc. of EMNLP", "citeRegEx": "Macherey et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Macherey et al\\.", "year": 2008}, {"title": "Minimum error rate training in statistical machine translation", "author": ["F.J. Och"], "venue": "In Proc. of ACL", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Minimum error rate training semiring", "author": ["Sokolov", "Yvon2011] A. Sokolov", "F. Yvon"], "venue": "In Proc. of AMTA", "citeRegEx": "Sokolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sokolov et al\\.", "year": 2011}, {"title": "Feasibility of human-inthe-loop minimum error rate training", "author": ["Zaidan", "Callison-Burch2009] O.F. Zaidan", "C. Callison-Burch"], "venue": "In Proc. of EMNLP", "citeRegEx": "Zaidan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zaidan et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 10, "context": "We describe the line search used in the minimum error rate training algorithm (Och, 2003) as the \u201cinside score\u201d of a weighted proof forest under a semiring defined in terms of wellunderstood operations from computational geometry.", "startOffset": 78, "endOffset": 89}, {"referenceID": 8, "context": "This conception leads to a straightforward complexity analysis of the dynamic programming MERT algorithms of Macherey et al. (2008) and Kumar et al.", "startOffset": 109, "endOffset": 132}, {"referenceID": 6, "context": "(2008) and Kumar et al. (2009) and practical approaches to implementation.", "startOffset": 11, "endOffset": 31}, {"referenceID": 9, "context": "While the original algorithm used n-best hypothesis lists to learn from, more recent work has developed dynamic programming variants that leverage much larger sets of hypotheses encoded in finite-state lattices and context-free hypergraphs (Macherey et al., 2008; Kumar et al., 2009; Sokolov and Yvon, 2011).", "startOffset": 240, "endOffset": 307}, {"referenceID": 6, "context": "While the original algorithm used n-best hypothesis lists to learn from, more recent work has developed dynamic programming variants that leverage much larger sets of hypotheses encoded in finite-state lattices and context-free hypergraphs (Macherey et al., 2008; Kumar et al., 2009; Sokolov and Yvon, 2011).", "startOffset": 240, "endOffset": 307}, {"referenceID": 10, "context": "In this paper, we show that the both the original (Och, 2003) and newer dynamic programming algorithms given by Macherey et al.", "startOffset": 50, "endOffset": 61}, {"referenceID": 8, "context": "(2009) can be understood as weighted logical deductions (Goodman, 1999; Lopez, 2009; Eisner and Filardo, 2011) using weights from a previously undescribed semiring, which we call the convex hull semiring (\u00a73).", "startOffset": 56, "endOffset": 110}, {"referenceID": 0, "context": "More practically still, since many tools for structured prediction over discrete sequences support generic semiring-weighted inference (Allauzen et al., 2007; Li et al., 2009; Dyer et al., 2010; Eisner and Filardo, 2011), our analysis makes it is possible to add dynamic programming MERT to them with little effort.", "startOffset": 135, "endOffset": 220}, {"referenceID": 7, "context": "More practically still, since many tools for structured prediction over discrete sequences support generic semiring-weighted inference (Allauzen et al., 2007; Li et al., 2009; Dyer et al., 2010; Eisner and Filardo, 2011), our analysis makes it is possible to add dynamic programming MERT to them with little effort.", "startOffset": 135, "endOffset": 220}, {"referenceID": 3, "context": "More practically still, since many tools for structured prediction over discrete sequences support generic semiring-weighted inference (Allauzen et al., 2007; Li et al., 2009; Dyer et al., 2010; Eisner and Filardo, 2011), our analysis makes it is possible to add dynamic programming MERT to them with little effort.", "startOffset": 135, "endOffset": 220}, {"referenceID": 4, "context": "In this paper, we show that the both the original (Och, 2003) and newer dynamic programming algorithms given by Macherey et al. (2008) and Kumar et al.", "startOffset": 112, "endOffset": 135}, {"referenceID": 4, "context": "(2008) and Kumar et al. (2009) can be understood as weighted logical deductions (Goodman, 1999; Lopez, 2009; Eisner and Filardo, 2011) using weights from a previously undescribed semiring, which we call the convex hull semiring (\u00a73).", "startOffset": 11, "endOffset": 31}, {"referenceID": 8, "context": "Zaidan and Callison-Burch (2009) exploit this and find that it is even feasible to solicit human judgments while evaluating \u03b4! Macherey et al. (2008) recommend selecting the midpoint of the segment with the best loss, but Cer et al.", "startOffset": 127, "endOffset": 150}, {"referenceID": 1, "context": "(2008) recommend selecting the midpoint of the segment with the best loss, but Cer et al. (2008) suggest other strategies.", "startOffset": 79, "endOffset": 97}, {"referenceID": 2, "context": "5 in de Berg et al. (2010). From these inequalities, it follows straightforwardly that the number of points in a derivation forest\u2019s total convex hull is upper bounded by |E|.", "startOffset": 8, "endOffset": 27}], "year": 2013, "abstractText": "We describe the line search used in the minimum error rate training algorithm (Och, 2003) as the \u201cinside score\u201d of a weighted proof forest under a semiring defined in terms of wellunderstood operations from computational geometry. This conception leads to a straightforward complexity analysis of the dynamic programming MERT algorithms of Macherey et al. (2008) and Kumar et al. (2009) and practical approaches to implementation.", "creator": "LaTeX with hyperref package"}}}