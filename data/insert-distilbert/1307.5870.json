{"id": "1307.5870", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jul-2013", "title": "Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery", "abstract": "recovering a low - rank tensor from incomplete information analysis is a recurring problem researched in signal processing and machine learning. the most popular convex relaxation of this specific problem and minimizes the sum of the nuclear norms of the unfoldings of the tensor. we show that this approach can be substantially suboptimal : reliably recovering a k - way tensor of length n and tucker rank r from gaussian measurements requires $ \\ omega ( rn ^ { k - 1 } ) $ observations. in contrast, a certain ( intractable ) nonconvex formulation needs only $ o ( r ^ k + \u2264 nrk ) $ observations. we introduce a very simple, new convex static relaxation, which partially bridges this gap. our new formulation succeeds with $ o ( r ^ { \\ lfloor k / 2 \\ rfloor } n ^ { \\ lceil k / 2 \\ rceil } ) $ observations. while these results pertain to gaussian measurements, simulations strongly suggest that the new norm also outperforms the sum of nuclear norms for tensor completion from a random subset of entries. our lower bounds derive for the sum - of - factors nuclear - norm model follow from taking a typical new result on simultaneously structured models, which may be of independent interest for matrix and vector recovery problems.", "histories": [["v1", "Mon, 22 Jul 2013 20:23:29 GMT  (36kb,D)", "http://arxiv.org/abs/1307.5870v1", null], ["v2", "Thu, 15 Aug 2013 05:59:52 GMT  (33kb,D)", "http://arxiv.org/abs/1307.5870v2", "Slight modifications are made in this second version (mainly, Lemma 5)"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["cun mu", "bo huang", "john wright", "donald goldfarb"], "accepted": true, "id": "1307.5870"}, "pdf": {"name": "1307.5870.pdf", "metadata": {"source": "CRF", "title": "Square Deal: Lower Bounds and Improved Relaxations for Tensor Recovery", "authors": ["Cun Mu", "Bo Huang", "John Wright", "Donald Goldfarb"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Tensors arise naturally in problems where the goal is to estimate a multi-dimensional object whose entries are indexed by several continuous or discrete variables. For example, a video is indexed by two spatial variables and one temporal variable; a hyperspectral datacube is indexed by two spatial variables and a frequency/wavelength variable. While tensors often reside in extremely highdimensional data spaces, in many applications, the tensor of interest is low-rank, or approximately so [KB09], and hence has much lower-dimensional structure. The general problem of estimating a low-rank tensor has applications in many different areas, both theoretical and applied: e.g., estimating latent variable graphical models [AGH+12], classifying audio [MSS06], mining text [CC12], processing radar signals [DN10], to name a few.\nIn this paper, we consider the problem of recovering aK-way tensor X \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7nK from linear measurements z = G[X ] \u2208 Rm. Typically, m N = \u220fK i=1 ni, and so the problem of recovering X from z is ill-posed. In the past few years, tremendous progress has been made in understanding how to exploit structural assumptions such as sparsity for vectors [CRT] or low-rankness for matrices [RFP10] to develop computationally tractable methods for tackling ill-posed inverse problems. In many situations, convex optimization can estimate a structured object from near-minimal sets of\nar X\niv :1\n30 7.\n58 70\nv1 [\nst at\n.M L\n] 2\nobservations [NRWY12, CRPW12, ALMT13]. For example, an n \u00d7 n matrix of rank r can, with high probability, be exactly recovered from Cnr generic linear measurements, by minimizing the nuclear norm \u2016X\u2016\u2217 = \u2211 i \u03c3i(X). Since a generic rank r matrix has r(2n \u2212 r) degrees of freedom, this is nearly optimal. In contrast, the correct generalization of these results to low-rank tensors is not obvious. The numerical algebra of tensors is fraught with hardness results [HL09]. For example, even computing a tensor\u2019s (CP) rank,\nrankcp(X ) = min { r | X =\nr\u2211 i=1 a (i) 1 \u25e6 a (i) 2 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 a (i) K\n} , (1.1)\nis NP-hard in general. The nuclear norm of a tensor is also intractable, and so we cannot simply follow the formula that has worked for vectors and matrices.\nWith an eye towards numerical computation, many researchers have studied how to estimate or recover tensors of small Tucker rank [Tuc66]. The Tucker rank of a K-way tensor X is a Kdimensional vector whose i-th entry is the (matrix) rank of the mode-i unfolding X (i) of X :\nranktc(X ) := ( rank(X (1)), rank(X (2)), \u00b7 \u00b7 \u00b7 , rank(X (K)) ) . (1.2)\nHere, the matrix X (i) \u2208 Rni\u00d7 \u220f\nj 6=i nj is obtained by concatenating all the mode-i fibers of X as column vectors. Each mode-i fiber is an ni-dimensional vector obtained by fixing every index of X but the i-th one. The Tucker rank of X can be computed efficiently using the (matrix) singular value decomposition. For this reason, we focus on tensors of low Tucker rank. However, we will see that our proposed regularization strategy also automatically adapts to recover tensors of low CP rank, with some reduction in the required number of measurements.\nThe definition (1.2) suggests a very natural, tractable convex approach to recovering low-rank tensors: seek the X that minimizes \u2211 i \u03bbi \u2225\u2225X (i)\u2225\u2225\u2217 out of all X satisfying G[X ] = z. We will refer to this as the sum-of-nuclear-norms (SNN) model. Originally, proposed in [LMWY09], this approach has been widely studied [GRY11, SDS10, THK10, TSHK11, STDLS13] and applied to various datasets in imaging [SVdPDMS11, SHKM13, KS13, LL10, GEK13, LYZY10].\nPerhaps surprisingly, we show that this natural approach can be substantially suboptimal, and introduce a simple new convex regularizer with provably better performance. For ease of stating results, suppose that n1 = \u00b7 \u00b7 \u00b7 = nK = n, and ranktc(X ) (r, r, \u00b7 \u00b7 \u00b7 , r). Let Tr denote the set of all such tensors. We will consider the problem of estimating an element X 0 of Tr from Gaussian measurements G (i.e., zi = \u3008Gi,X \u3009, where Gi has i.i.d. standard normal entries). To describe a generic tensor in Tr, we need at most r\nK + rnK parameters. Section 2 shows that a certain nonconvex strategy can recover all X \u2208 Tr exactly when m > (2r)K + 2nrK. In contrast, the best known theoretical guarantee for SNN minimization, due to Tomioka et. al. [TSHK11], shows that X 0 \u2208 Tr can be recovered (or accurately estimated) from Gaussian measurements G, provided m = \u2126(rnK\u22121). In Section 3, we prove that this number of measurements is also necessary: accurate recovery is unlikely unless m = \u2126(rnK\u22121). Thus, there is a substantial gap between an ideal nonconvex approach and the best known tractable surrogate. In Section 4, we introduce a simple alternative, which we call the square norm model, which reduces the required number of measurements to O(rbK/2cndK/2e). For K > 3, this improves by a multiplicative factor polynomial in n.\nOur theoretical results pertain to Gaussian operators G. The motivation for studying Gaussian measurements is twofold. First, Gaussian measurements may be of interest for compressed sensing recovery [Don06], either directly as a measurement strategy, or indirectly due to universality phenomena [DT09, BLM12]. Second, the available theoretical tools for Gaussian measurements are very sharp, allowing us to rigorously investigate the efficacy of various regularization schemes, and prove both upper and lower bonds on the number of observations required. In simulation, our qualitative conclusions carry over to more realistic measurement models, such as random subsampling [LMWY09] (see Section 5). We expect our results to be of interest for a wide range of problems in tensor completion [LMWY09], robust tensor recovery / decomposition [LYZY10, GQ12] and sensing.\nOur technical methodology draws on, and enriches, the literature on general structured model recovery. The surprisingly poor behavior of the SNN model is an example of a phenomenon first discovered by Oymak et. al. [OJF+12]: for recovering objects with multiple structures, a combination of structure-inducing norms is often not significantly more powerful than the best individual structure-inducing norm. Our lower bound for the SNN model follows from a general result of this nature, which we prove using the geometric framework of [ALMT13]. Compared to [OJF+12], our result pertains to a more general family of regularizers, and gives sharper constants."}, {"heading": "2 Bounds for Non-Convex Recovery", "text": "In this section, we introduce a non-convex model for tensor recovery, and show that it recovers low-rank tensors from near-minimal numbers of measurements. While our nonconvex formulation is computationally intractable, it gives a baseline for evaluating tractable (convex) approaches.\nFor a tensor of low Tucker rank, the matrix unfolding along each mode is low-rank. Suppose we observe G[X 0] \u2208 Rm. We would like to attempt to recover X 0 by minimizing some combination of the ranks of the unfoldings, over all tensors X that are consistent with our observations. This suggests a vector optimization problem (e.g. [BV04] Chapter 4.7):\nminimize(w.r.t. RK+ ) ranktc(X ) subject to G[X ] = G[X 0]. (2.1)\nIn vector optimization, a feasible point is called Pareto optimal if no other feasible point dominates it in every criterion. In a similar vein, we say that (2.1) recovers X 0 if there does not exist any other tensor X that is consistent with the observations and has no larger rank along each mode:\nDefinition 1. We call X 0 recoverable by (2.1) if the set\n{X \u2032 6= X 0 | G[X \u2032] = G[X 0], ranktc(X \u2032) RK+ ranktc(X 0)} = \u2205.\nThis is equivalent to saying that X 0 is the unique optimal solution to the scalar optimization:\nminimizeX max i { rank(X (i)) rank(X 0(i)) } subject to G[X ] = G[X 0]. (2.2)\nThe problems (2.1)-(2.2) are not tractable. However, they do serve as a baseline for understanding how many generic measurements are required to recover X 0. The recovery performance of program (2.1) depends heavily on the properties of G. Suppose (2.1) fails to recover X 0 \u2208 Tr. Then there exists another X \u2032 \u2208 Tr such that G[X \u2032] = G[X 0]. So, to guarantee that (2.1) recovers any X 0 \u2208 Tr, a necessary and sufficient condition is that G is injective on Tr, which can be implied by the condition\nnull(G) \u2229 T2r = {0}. Consequently, if null(G) \u2229 T2r = {0}, (2.1) will recover any X 0 \u2208 Tr. We expect this to occur when the number of measurements significantly exceeds the number of intrinsic degrees of freedom of a generic element of Tr, which is O(r\nK + nrK). The following theorem shows that when m is approximately twice this number, with probability one, G is injective on Tr:\nTheorem 1. Whenever m \u2265 (2r)K + 2nrK + 1, with probability one, null(G) \u2229 T2r = {0}, and hence (2.1) recovers every X 0 \u2208 Tr.\nThe proof of Theorem 1 follows from a covering argument, which we establish in several steps. Let\nS2r = {D | D \u2208 T2r, \u2016D\u2016F = 1} . (2.3)\nThe following lemma shows that the required number of measurements can be bounded in terms of the exponent of the covering number for S2r, which can be considered as a proxy for dimensionality:\nLemma 1. Suppose that the covering number for S2r with respect to Frobenius norm, satisfies\nN(S2r, \u2016\u00b7\u2016F , \u03b5) \u2264 (\u03b2/\u03b5) d , (2.4)\nfor some integer d and scalar \u03b2 that does not depend on \u03b5. Then if m \u2265 d+ 1, with probability one null (G) \u2229S2r = \u2205, which implies that null (G) \u2229 T2r = {0}.\nIt just remains to find the covering number of S2r. We use the following lemma, which uses the triangle inequality to control the effect of perturbations in the factors of the Tucker decomposition\n[[C;U1,U2, \u00b7 \u00b7 \u00b7 ,UK ]] := C \u00d71 U1 \u00d72 U2 \u00d73 \u00b7 \u00b7 \u00b7 \u00d7K UK , (2.5)\nwhere the mode-i (matrix) product of tensor A with matrix B of compatible size, denoted as A\u00d7iB, outputs a tensor C such that C(i) = BA(i).\nLemma 2. Let C,C\u2032 \u2208 Rr1,...,rK , and U1,U \u20321 \u2208 Rn1\u00d7r1 , . . . ,UK ,U \u2032 K \u2208 RnK\u00d7rK with U \u2217 iU i = U \u2032i \u2217 U \u2032i = I, and \u2016C\u2016F = \u2225\u2225C\u2032\u2225\u2225 F = 1. Then\n\u2225\u2225[[C;U1, . . . ,UK ]]\u2212 [[C\u2032;U \u20321, . . . ,U \u2032K ]]\u2225\u2225F \u2264 \u2225\u2225C \u2212 C\u2032\u2225\u2225F + K\u2211 i=1 \u2225\u2225U i \u2212U \u2032i\u2225\u2225. (2.6) Using this result, we construct an \u03b5-net for S2r by building \u03b5/(K + 1)-nets for each of the K + 1\nfactors C, (U i). The total size of the resulting \u03b5 net is bounded by the following lemma:\nLemma 3. N(S2r, \u2016\u00b7\u2016F , \u03b5) \u2264 (3(K + 1)/\u03b5) (2r)K+2nrK\nWith these observations in hand, Theorem 1 follows immediately."}, {"heading": "3 Convexification: Sum of Nuclear Norms?", "text": "Since the nonconvex problem (2.1) is NP-hard for general G, it is tempting to seek a convex surrogate. In matrix recovery problems, the nuclear norm is often an excellent convex surrogate for the rank\n[Faz02, RFP10, Gro11]. It seems natural, then, to replace the ranks in (2.1) with nuclear norms, and solve\nminimize h(X ) := (\u2225\u2225X (1)\u2225\u2225\u2217 ,\u2225\u2225X (2)\u2225\u2225\u2217 , \u00b7 \u00b7 \u00b7 ,\u2225\u2225X (K)\u2225\u2225\u2217) subject to G[X ] = G[X 0]. (3.1)\nSince \u2225\u2225X (i)\u2225\u2225\u2217 is a convex function, the set H := \u22c3X :G[X ]=G[X 0] {Y | h(Y) h(X )} is convex. For any pareto optimal point X\u0302 , there is a hyperplane supporting H passing through h(X\u0302 ), with normal vector \u03bb \u2265 0. Therefore, X\u0302 is an optimal solution to the following scalar optimization:\nminimize K\u2211 i=1 \u03bbi\u2016X (i)\u2016\u2217 subject to G[X ] = G[X 0]. (3.2)\nThe optimization (3.2) was first introduced by [LMWY09] and has been used successfully in applications in imaging [SVdPDMS11, SHKM13, KS13, LL10, GEK13, LYZY10]. Similar convex relaxations have been considered in a number of theoretical and algorithmic works [GRY11, SDS10, THK10, TSHK11, STDLS13]. It is not too surprising, then, that (3.2) provably recovers the underlying tensor X 0, when the number of measurements m is sufficiently large. For example, the following is a (simplified) corollary of results of Tomioka et. al. [THK10]:1\nCorollary 2 (of [THK10], Theorem 3). Suppose that X 0 has Tucker rank (r, . . . , r), and m \u2265 CrnK\u22121. With high probability, X 0 is an optimal solution to (3.2), with each \u03bbi = 1. Here, C is numerical.\nThis result shows that there is a range in which (3.2) succeeds: loosely, when we undersample by at most a factor of m/N \u223c r/n. However, the number of observations m \u223c rnK\u22121 is significantly larger than the number of degrees of freedom in X 0, which is on the order of rK + nrK. Is it possible to prove a better bound for this model? Unfortunately, we show that in general O(rnK\u22121) measurements are also necessary for reliable recovery using (3.2):\nTheorem 3. Let X 0 \u2208 Tr be nonzero. Set \u03ba = mini {\u2225\u2225(X 0)(i)\u2225\u22252\u2217 / \u2016X 0\u20162F} \u00d7 nK\u22121. Then if the number of measurements m \u2264 \u03ba\u2212 1, X 0 is not the unique solution to (3.2), with probability at least 1\u2212 4 exp(\u2212 (\u03ba\u2212m\u22121) 2\n16(\u03ba\u22121) ). Moreover, there exists X 0 \u2208 Tr for which \u03ba = rn K\u22121.\nThis implies that Corollary 2 (and other results of [THK10]) is essentially tight. Unfortunately, it has negative implications for the efficacy of the sum of nuclear norms in (3.2): although a generic element X 0 of Tr can be described using at most rK + nrK real numbers, we require \u2126(rnK\u22121) observations to recover it using (3.2).\nRecovering objects with multiple structures. The poor behavior of (3.2) is actually an instance of a much more general phenomenon, first discovered by Oymak et. al. [OJF+12]. Our target tensor X 0 has multiple low-dimensional structures simultaneously: it is low-rank along each of the K modes. In practical applications, many other such simultaneously structured objects may be of interest \u2013 for example, matrices that are simultaneously sparse and low-rank [RSV12, OJF+12]. To recover such a simultaneously structured object, it is tempting to build a convex relaxation by\n1Tomioka et. al. also show noise stability when m = \u2126(rnK\u22121) and give extensions to the case where the ranktc (X 0) = (r1, . . . , rK) differs from mode to mode.\ncombining the convex relaxations for each of the individual structures. In the tensor case, this yields (3.2). Surprisingly, this combination is often not significantly more powerful than the best single regularizer [OJF+12]. We obtain Theorem 3 as a consquence of a new, general result of this nature, using a geometric framework introduced in [ALMT13]. Compared to the proof strategy in [OJF+12], this approach has a clearer geometric intuition, covers a more general class of regularizers and yields sharper bounds.\nConsider a signal x0 \u2208 Rn having K low-dimensional structures simultaneously (e.g. sparsity, low-rank, etc.). Let \u2016\u00b7\u2016(i) be the penalty norms corresponding to the i-th structure (e.g. `1, nuclear norm). Consider the composite norm optimization\nmin x\u2208Rn f(x) := \u03bb1 \u2016x\u2016(1) + \u03bb2 \u2016x\u2016(2) + \u00b7 \u00b7 \u00b7+ \u03bbK \u2016x\u2016(K) subject to G[x] = G[x0], (3.3)\nwhere G[\u00b7] is a Gaussian measurement operator, and \u03bb > 0. Is x0 the unique optimal solution to (3.3)? Recall that the descent cone of a function f at a point x0 is defined as\nC(f,x0) = cone {v | f(x0 + v) \u2264 f(x0)} , (3.4)\nwhich, in short, will be denoted as C. Then x0 is the unique optimal solution if and only if null(G)\u2229 C = {0}. Conversely, recovery fails if null(G) has nontrivial intersection with C. If G is a Gaussian operator, null(G) is a uniformly oriented random subspace of dimension n\u2212m. This random subspace is more likely to have nontrivial intersection with C if C is \u201clarge,\u201d in a sense we will make precise. The polar of C is Co = cone ( \u2202f(x0) ) . Because polarity reverses inclusion, we expect that C will be \u201clarge\u201d whenever Co is \u201csmall\u201d. Figure 1 visualizes this geometry. To control the size of Co, first consider a single norm \u2016\u00b7\u2016 , with dual norm \u2016\u00b7\u2016 \u2217 . Suppose that \u2016\u00b7\u2016 is L-Lipschitz: \u2016x\u2016 \u2264 L \u2016x\u20162 for all x. Then \u2016x\u20162 \u2264 L \u2016x\u2016 \u2217 for all x as well. Noting that\n\u2202 \u2016\u00b7\u2016 (x) = { v | \u3008v,x\u3009 = \u2016x\u2016 , \u2016v\u2016 \u2217 \u2264 1 } ,\nfor any v \u2208 \u2202 \u2016\u00b7\u2016 (x0), we have\n\u3008v,x0\u3009 \u2016v\u20162 \u2016x0\u20162 \u2265 \u2016x0\u2016 L \u2016v\u2016\u2217 \u2016x0\u20162 \u2265 \u2016x0\u2016 L \u2016x0\u20162 . (3.5)\nA more geometric way of summarizing this is as follows: for x 6= 0, let\ncirc(x, \u03b8) = {z | \u2220(z,x) \u2264 \u03b8} , (3.6)\nand denote the circular cone with axis x and angle \u03b8. Then if x0 6= 0, and \u03b8 = cos\u22121(\u2016x0\u2016 /L \u2016x0\u20162),\n\u2202 \u2016\u00b7\u2016 (x0) \u2286 circ (x0, \u03b8) . (3.7)\nTable 1 describes the angle parameters \u03b8 for various structure inducing norms. Notice that in general, more complicated x0 leads to smaller angles \u03b8. For example, if x0 is a k-sparse vectors with entries all of the same magnitude, and \u2016\u00b7\u2016 the `1 norm, cos2 \u03b8 = k/n. As x0 becomes more dense, \u2202 \u2016\u00b7\u2016 is contained in smaller and smaller circular cones.\nFor f = \u2211 i \u03bbi \u2016\u00b7\u2016(i), notice that every element of \u2202f(x0) is a conic combination of elements of the \u2202 \u2016\u00b7\u2016(i) (x0). Since each of the \u2202 \u2016\u00b7\u2016(i) (x0) is contained in a circular cone with axis x0, \u2202f(x0) is also contained in a circular cone:\nLemma 4. Suppose that \u2016\u00b7\u2016(i) is Li-Lipschitz. For x0 6= 0, set \u03b8i = cos\u22121 ( \u2016x0\u2016(i) /L \u2016x0\u20162 ) . Then \u2202f(x0) \u2286 circ ( x0, max\ni=1...K \u03b8i\n) . (3.8)\nSo, the subdifferential of our combined regularizer f is contained in a circular cone whose angle is given by the largest of the \u03b8i.\nHow does this behavior affect the recoverability of x0 via (3.3)? The informal reasoning above suggests that as \u03b8 becomes smaller, the descent cone C becomes larger, and we require more measurements to recover x0. This can be made precise using an elegant framework introduced by Amelunxen et. al. [ALMT13]. They define the statistical dimension of the convex cone C to be the expected norm of the projection of a standard Gaussian vector onto C:\n\u03b4(C) .= Eg\u223ci.i.d.N (0,1) [ \u2016PC(g)\u201622 ] . (3.9)\nUsing tools from spherical integral geometry, [ALMT13] shows that for linear inverse problems with Gaussian measurements, a sharp phase transition in recoverability occurs around m = \u03b4(C). We will need only one side of their result; for more details see [ALMT13]. We state a slight variant here:\nCorollary 4. Let G : Rn \u2192 Rm be a Gaussian operator, and C a convex cone. Then if m \u2264 \u03b4(C), P [ C \u2229 null(G) = {0} ] \u2264 4 exp ( \u2212 (\u03b4(C)\u2212m) 2\n16\u03b4(C)\n) . (3.10)\nTo apply this result to our problem, we lower bound the statistical dimension \u03b4(C), of the descent cone C of f at x0. Using the Pythagorean theorem, monotonicity of \u03b4(\u00b7), and Lemma 4, we calculate\n\u03b4(C) = n\u2212 \u03b4(Co) = n\u2212 \u03b4 (cone(\u2202f(x0))) \u2265 n\u2212 \u03b4(circ(x0,max i \u03b8i)).\nMoreover, using several results established in [ALMT13], we can show that \u03b4(circ(x0, \u03b8)) \u2264 n sin2 \u03b8+ 1, whose proof is included in the appendix. So, finally, \u03b4(C) \u2265 nmini cos2 \u03b8i\u2212 1. Using Corollary 4, we obtain\nTheorem 5. Let x0 6= 0. Suppose that for each i, \u2016\u00b7\u2016(i) is Li-Lipschitz. Set\n\u03bai = n \u2016x0\u20162(i) L2i \u2016x0\u2016 2 2 = n cos2(\u03b8i),\nand \u03ba = mini \u03bai. Then if m \u2264 \u03ba\u2212 1, P [x0 is the unique optimal solution to (3.3) ] \u2264 4 exp ( \u2212 (\u03ba\u2212m\u2212 1) 2\n16 (\u03ba\u2212 1)\n) . (3.11)\nThus, for reliable recovery, the number of measurements needs to be at least proportional to \u03ba.2 Notice that \u03ba = mini \u03bai is determined by only the best of the structures. Per Table 1, \u03bai is often on the order of the number of degrees of freedom in a generic object of the i-th structure. For example, for a k-sparse vector whose nonzeros are all of the same magnitude, \u03ba = k.\nTheorem 5 together with Table 1 leads us to the phenomenon that recently discovered by Oymak et. al. [OJF+12]: for recovering objects with multiple structures, a combination of structure-inducing norms tends to be not significantly more powerful than the best individual structure-inducing norm. As we demonstrate, this general behavior follows a clear geometric interpretation that the subdifferential of a norm at x0 is contained in a relatively small circular cone with central axis x0.\nWe can specialize Theorem 5 to low-rank tensors as follows: if X is a K-mode n \u00d7 n \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 n tensor of Tucker rank (r, r, . . . , r), then for each i, \u2016X\u2016(i) . = \u2225\u2225X (i)\u2225\u2225\u2217 is L = \u221an-Lipschitz. Hence,\n\u03ba = min i {\u2225\u2225X (i)\u2225\u22252\u2217 / \u2016X\u20162F} nK\u22121. (3.12) 2E.g., if m = (\u03ba\u2212 1)/2, the probability of success is at most 4 exp(\u2212(\u03ba\u2212 1)/64).\nThe term in brackets lies between 1 and r, inclusive. For example, if X = [[C,U1, . . . ,UK ]], with UTi U i = I and C supersymmetric (Ci1...iK = 1i1=i2=\u00b7\u00b7\u00b7=iK ), then this term is equal to r."}, {"heading": "4 A Better Convexification: Square Norm", "text": "The number of measurements promised by Corollary 2 and Theorem 3 is actually the same (up to constants) as the number of measurements required to recover a tensor X 0 which is low-rank along just one mode. Since matrix nuclear norm minimization correctly recovers a n1\u00d7 n2 matrix of rank r when m \u2265 Cr(n1 + n2) [CRPW12], solving\nminimize \u2016X (1)\u2016\u2217 subject to G[X ] = G[X 0] (4.1)\nalso exactly recovers X 0 with high probability when m \u2265 CrnK\u22121. This suggests a more mundane explanation for the difficulty with (3.2): the term rnK\u22121 comes from the need to reconstruct the right singular vectors of the n \u00d7 nK\u22121 matrix X (1). If we had some way of matricizing a tensor that produced a more balanced (square) matrix and also preserved the low-rank property, we could substantially reduce this effect, and reduce the overall sampling requirement. In fact, this is possible when the order K of X 0 is four or larger.\nFor A \u2208 Rm1\u00d7n1 , and integers m2 and n2 satisfying m1n1 = m2n2, the reshaping operator reshape(A,m2, n2) returns a m2 \u00d7 n2 matrix whose elements are taken columnwise from A. This operator rearranges elements in A and leads to a matrix of different shape. In the following, we reshape matrix X (1) to a more square matrix while preserving the low-rank property. Let X \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nK . Select j \u2208 [K] := {1, 2, \u00b7 \u00b7 \u00b7 , K}. Then we define matrix X [j] as\nX [j] = reshape ( X (1), j\u220f i=1 ni, K\u220f i=j+1 ni ) .\nWe can view X [j] as a natural generalization of the standard tensor matricization. When j = 1, X [j] is nothing but X (1). However, when some j > 1 is selected, X [j] becomes a more balanced matrix. This reshaping also preserves some of the algebraic structures of X . In particular, we will see that if X is a low-rank tensor (in either the CP or Tucker sense), X [j] will be a low-rank matrix.\nLemma 5. (1) If X has CP decomposition X = \u2211r i=1 \u03bbia (1) i \u25e6 a (2) i \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 a (K) i , then\nX [j] = r\u2211 i=1 \u03bbi(a (j) i \u2297 a (j\u22121) i \u2297 \u00b7 \u00b7 \u00b7 \u2297 a (1) i ) \u25e6 (a (K) i \u2297 a (K\u22121) i \u00b7 \u00b7 \u00b7 \u2297 a (j+1) i ). (4.2)\n(2) If X has Tucker decomposition X = C \u00d71 U1 \u00d72 U2 \u00d73 \u00b7 \u00b7 \u00b7 \u00d7K UK , then\nX [j] = (U j \u2297U j\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297U1)C[j] (UK \u2297UK\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297U j+1)\u2217. (4.3)\nUsing Lemma 5 and the fact that rank(A\u2297B) = rank(A) rank(B), we obtain:\nLemma 6. Let ranktc (X ) = (r1, r2, \u00b7 \u00b7 \u00b7 , rK), and rankcp (X ) = rcp. Then rank(X [j]) \u2264 rcp, and rank(X [j]) \u2264 min { \u220fj i=1 ri, \u220fK i=j+1 ri } .\nThus, X [j] is not only more balanced but also maintains the low-rank property of tensor X . In the following, we show how this new matricization can lead to better relaxations for tensor recovery. For ease of discussion, we assume X has the same length (say n) along each mode and has Tucker rank (r, r, \u00b7 \u00b7 \u00b7 , r). We write X\n. = X [bK2 c] and call \u2016X\u2016 := \u2016X \u2016? the square norm of tensor X .\nSince X is low-rank, we can attempt to recover X by solving\nminimize \u2016X\u2016 subject to G[X ] = G[X 0]. (4.4)\nUsing Lemma 6 and Proposition 3.11 of [CRPW12], we can prove that this relaxation exactly recovers X 0, when the number of measurements is sufficienly large:\nTheorem 6. (1) If X 0 has CP rank r, using (4.4), m \u2265 Crnd K 2 e is sufficient to recover X 0 with high probability. (2) If X 0 has Tucker rank (r, r, \u00b7 \u00b7 \u00b7 , r), using (4.4), m \u2265 Crb K 2 cnd K 2 e is sufficient to recover X 0 with high probability."}, {"heading": "5 Simulation Results for Tensor Completion", "text": "Tensor completion attempts to reconstruct the low-rank tensor X 0 based on observations over a subset of its entries \u2126. By imposing appropriate incoherence conditions (and modifying slightly arguments in [Gro11]), it is possible to prove recovery guarantees for each of the following programs:\nminimize K\u2211 i=1 \u03bbi\u2016X (i)\u2016\u2217 subject to P\u2126[X ] = P\u2126[X 0]; (5.1)\nminimize \u2016X\u2016 subject to P\u2126[X ] = P\u2126[X 0]. (5.2)\nUnlike the recovery problem under Gaussian random measurements, due to the lack of sharp upper bounds, we have no proof that our square norm formulation outperforms the SNN model here. However, our simulation result below will strongly suggest that (5.2) also performs much better than (5.1) for tensor completion case.\nOur experiment is set up as follows. We generate a 4-way tensor, X 0 \u2208 Rn\u00d7n\u00d7n\u00d7n, as\nX 0 = [[C0;U1,U2,U3,U4]] = C0 \u00d71 U1 \u00d72 U2 \u00d73 U3 \u00d74 U4,\nwhere the core tensor C0 \u2208 R1\u00d71\u00d72\u00d72 has i.i.d. standard Gaussian entries, and matrices U1, U2 \u2208 Rn\u00d71 and matrices U3, U4 \u2208 Rn\u00d72, satisfying U\u2217iU i = I, are drawn uniformly at random (by the command orth(randn(\u00b7, \u00b7)) in Matlab). The observed entries are chosen uniformly with ratio \u03c1. We increase the problem size n from 10 to 30 with increment 1, and the observation ratio \u03c1 from 0.01 to 0.2 with increment 0.01. For each (\u03c1, n)-pair, we simulate 5 test instances and declare a trial to be successful if the recovered X ? satisfies \u2016X ?\u2212X 0\u2016F \u2016X 0\u2016F\n\u2264 10\u22122. The optimization problems are solved using efficient first-order methods. Since (5.2) is in the form of standard matrix completion, we use the Augmented Lagrangian Method (ALM) in [LCM10] to solve it. For the sum of nuclear norms minimization (5.1) with \u03bbi = 1, we implement the accelerated linearized Bregman algorithm [HMG13], of which we include a detailed discussion in the appendix. Figure 2 plots the fraction of correct recovery for each pair (black = 0% and white = 100%). Clearly much larger white region is produced by square norm, which empirically suggests that (5.2) outperforms (5.1) for tensor completion problem."}, {"heading": "6 Conclusion", "text": "In this paper, we establish several theoretical bounds for the problem of low-rank tensor recovery using random Gaussian measurements. For the nonconvex model (2.1), we show that ( (2r)K +\n2nrK + 1 )\nmeasurements are sufficient to recover any X 0 \u2208 Tr almost surely. We highlight that though the nonconvex recovery program is NP-hard in general, it does serve a baseline for evaluating tractable (convex) approaches. For the conventional convex surrogate sum-of-nuclear-norms (SNN) model (3.2), we prove a necessary condition that \u2126(rnK\u22121) Gaussian measurements are required for reliable recovery. This lower bound is derived from our general study on multi-structured object recovery, which may also be of interest for other problems. To narrow the gap between the non-convex model and the SNN model, we unfold the tensor into a more balanced matrix while preserving its lowrank property, leading to our square-norm model (4.4). We prove that O(rb K 2 cnd K 2 e) measurements are sufficient to recover a tensor X 0 \u2208 Tr with high probability. Though the theoretical results only pertain to Gaussian measurements, our simulation result for tensor completion also suggests that square-norm model outperforms the SNN model.\nCompared with \u2126(rnK\u22121) measurements required by the sum-of-nuclear-norms model, the sam-\nple complexity, O(rb K 2 cnd K 2 e), required by the square reshaping (4.4), is always within a constant of it, much better for small r and K \u2265 4 \u2013 e.g., by a multiplicative factor of nbK/2c\u22121 when r is a constant. This is a significant improvement. However, there are also two clear limitations. First,\nno improvement is obtained for the case K = 3. Second, the improved sample complexity in Theorem 6 is still suboptimal compared to the nonconvex model (2.1). Obtaining near-optimal convex relaxations remains an open problem for all K > 2.\nMore broadly speaking, to recover objects with multiple structures, regularizing with a combination of individual structure-inducing norms is proven to be substantially suboptimal (Theorem 5 and also [OJF+12]). The resulting sample requirements tend to be much larger than the intrinsic degrees of freedom of the low-dimensional manifold that the structured signal lies in. Our squarenorm model for the low-rank tensor recovery demonstrates the possibility that a better exploitation can significantly reduce this sample complexity. However, there are still no clear clues on how to intelligently utilize several simultaneous structures generally, and moreover how to design tractable method to recover multi-structured objects with near minimal number of measurements. These problems are definitely worth pursuing in future study."}, {"heading": "A Proofs for Section 2", "text": ""}, {"heading": "Proof of Lemma 1.", "text": "Proof. The arguments we used below are primarily adapted from [ENP11], where their interest is to establish the number of Gaussian measurements required to recover a low rank matrix by rank minimization.\nNotice that every D \u2208 S2r, and every i, \u3008Gi,D\u3009 is a standard Gaussian random variable, and so\n\u2200 t > 0, P [ |\u3008Gi,D\u3009| < t ] < 2t \u00b7 1\u221a 2\u03c0 = t\n\u221a 2\n\u03c0 . (A.1)\nLet N be an \u03b5-net for S2r in terms of \u2016\u00b7\u2016F . Because the measurements are independent, for any fixed D\u0304 \u2208 S2r, P [ \u2225\u2225G[D\u0304]\u2225\u2225\u221e < t ] < (t\u221a2/\u03c0)m . (A.2)\nMoreover, for any D \u2208 S2r, we have \u2016G[D]\u2016\u221e \u2265 maxD\u0304\u2208N { \u2225\u2225G[D\u0304]\u2225\u2225\u221e \u2212 \u2016G\u2016F\u2192\u221e \u2225\u2225D\u0304 \u2212D\u2225\u2225F } (A.3)\n\u2265 min D\u0304\u2208N {\u2225\u2225G[D\u0304]\u2225\u2225\u221e}\u2212 \u03b5 \u2016G\u2016F\u2192\u221e . (A.4) Hence,\nP [\ninf D\u2208S2r\n\u2016G[D]\u2016\u221e < \u03b5 log(1/\u03b5) ]\n\u2264 P [\nmin D\u2208N\n\u2016G[D]\u2016\u221e < 2\u03b5 log(1/\u03b5) ] + P [ \u2016G\u2016F\u2192\u221e > log(1/\u03b5) ]\n\u2264 #N\u00d7 ( 2 \u221a 2/\u03c0 \u00d7 \u03b5 log(1/\u03b5) )m\n+ P [ \u2016G\u2016F\u2192\u221e > log(1/\u03b5) ] \u2264 \u03b2d(2 \u221a\n2/\u03c0)m\u03b5m\u2212d log(1/\u03b5)m + P [ \u2016G\u2016F\u2192\u221e > log(1/\u03b5) ] . (A.5)\nSince m \u2265 d+ 1, (A.5) goes to zero as \u03b5\u2198 0. Hence, taking a sequence of decreasing \u03b5, we can show that P [ infD\u2208S2r \u2016G[D]\u2016\u221e = 0 ] \u2264 t for every positive t, establishing the result."}, {"heading": "Proof of Lemma 2.", "text": "Proof. This follows from the basic fact that for any tensor X and matrix U of compatible size,\n\u2016X \u00d7k U\u2016F \u2264 \u2016U\u2016 \u2016X\u2016F , (A.6)\nwhich can be established by direct calculation. Write\u2225\u2225[[C;U1, . . . ,UK ]]\u2212 [[C\u2032;U \u20321, . . . ,U \u2032K ]]\u2225\u2225F \u2264\n\u2225\u2225[[C;U1, . . . ,UK ]]\u2212 [[C\u2032;U1, . . . ,UK ]]\u2225\u2225F +\n\u2225\u2225\u2225\u2225\u2225 K\u2211 i=1 [[C\u2032;U \u20321, . . . ,U \u2032 i,U i+1, . . .Uk]]\u2212 [[C \u2032;U \u20321, . . . ,U \u2032 i\u22121,U i, . . .UK ]] \u2225\u2225\u2225\u2225\u2225 F\n\u2264 \u2225\u2225C \u2212 C\u2032\u2225\u2225\nF + K\u2211 i=1 \u2225\u2225U i \u2212U \u2032i\u2225\u2225, where the first inequality follows from triangle inequality and the second inequality follows from the fact that \u2016C\u2016F = 1, \u2016U j\u2016 = 1 and U \u2217 iU i = I."}, {"heading": "Proof of Lemma 3.", "text": "Proof. The idea of this proof is to construct a net for each component of the Tucker decomposition and then combine those nets to form a compound net with the desired cardinality.\nDenote C = {C \u2208 R2r\u00d72r\u00d7\u00b7\u00b7\u00b7\u00d72r | \u2016C\u2016F = 1} and O = {U \u2208 Rn\u00d7r | U \u2217U = I}. Clearly, for any C \u2208 C, \u2016C\u2016F = 1, and for any U \u2208 O, \u2016U\u2016 = 1. Thus by Proposition 4 in [Ver07] and Lemma 5.2 in [Ver10], there exists an \u03b5K+1 -net C \u2032 covering C with respect to the Frobenius norm such that #C\u2032 \u2264 ( 3(K+1)\u03b5 ) (2r)K , and there exists an \u03b5K+1 -net O\n\u2032 covering O with respect to the operator norm such that #O\u2032 \u2264 ( 3(K+1)\u03b5 ) 2nr. Construct\nS\u20322r = {[[C \u2032;U \u20321, . . . ,U \u2032 K ]] | C \u2032 \u2208 C\u2032, U \u2032i \u2208 O\u2032}.\nClearly #S\u20322r \u2264 ( 3(K+1) \u03b5 )(2r)K+2nrK . The rest is to show that S\u20322r is indeed an \u03b5-net covering S2r with respect to the Frobenius norm. For any fixed D = [[C;U1, \u00b7 \u00b7 \u00b7 ,UK ]] \u2208 S2r where C \u2208 C and U i \u2208 O, by our constructions\nabove, there exist C\u2032 \u2208 C\u2032 and U \u2032i \u2208 O\u2032 such that \u2225\u2225C \u2212 C\u2032\u2225\u2225 F \u2264 3(K+1)\u03b5 and \u2225\u2225U i \u2212U \u2032i\u2225\u2225 \u2264 3(K+1)\u03b5 . Then D\u2032 = [[C\u2032;U \u20321, \u00b7 \u00b7 \u00b7 ,U \u2032 K ]] \u2208 S\u20322r is within \u03b5-distance from D, since by the triangle inequality derived in Lemma 2, we have\n\u2225\u2225D \u2212D\u2032\u2225\u2225 F = \u2225\u2225[[C;U1, . . . ,UK ]]\u2212 [[C\u2032;U \u20321, . . . ,U \u2032K ]]\u2225\u2225F \u2264 \u2225\u2225C \u2212 C\u2032\u2225\u2225F + K\u2211\ni=1 \u2225\u2225U i \u2212U \u2032i\u2225\u2225 \u2264 \u03b5. This completes the proof."}, {"heading": "B Proofs for Section 3", "text": "Proof of the upper bound for the statistical dimension of circular cone: \u03b4 ( circ(x0, \u03b8) ) \u2264 n sin2 \u03b8 + 1.\nProof. Denote circ(en, \u03b8) as circn(\u03b8), where en is the nth standard basis for Rn. Since \u03b4 ( circ(x0, \u03b8) ) =\n\u03b4 ( circ(en, \u03b8) ) , it is sufficient to prove \u03b4 ( circn(\u03b8) ) \u2264 n sin2 \u03b8 + 1, which will be proved using several results established in [ALMT13]. Let us first consider the case where n is even. Define a discrete random variable V supporting on {0, 1, 2, \u00b7 \u00b7 \u00b7 , n} with probability mass function P [V = k ] = vk. Here vk denotes the k-th intrinsic volumes of circn(\u03b8), which has been specified in (C.2) of [ALMT13]:\nvk =\n{ 1 2 ( 1 2 (n\u22122) 1 2 (k\u22121) ) sink\u22121(\u03b8) cosn\u2212k\u22121(\u03b8) for k = 1, 2, \u00b7 \u00b7 \u00b7 , n\u2212 1;\n0 for k = 0 and n.\nBy Proposition 5.11 in [ALMT13], we know that\n\u03b4 ( circn(\u03b8) ) = E [V ] = n\u2211 k=1 P [V \u2265 k ] .\nMoreover, by the interlacing result from Proposition 5.6 of [ALMT13], we have\nP [V \u2265 1 ] \u2264 2P [V = 1 ] + 2P [V = 3 ] + \u00b7 \u00b7 \u00b7+ 2P [V = n\u2212 1 ] , P [V \u2265 2 ] \u2264 2P [V = 1 ] + 2P [V = 3 ] + \u00b7 \u00b7 \u00b7+ 2P [V = n\u2212 1 ] ;\nP [V \u2265 3 ] \u2264 2P [V = 3 ] + 2P [V = 5 ] + \u00b7 \u00b7 \u00b7+ 2P [V = n\u2212 1 ] , P [V \u2265 5 ] \u2264 2P [V = 3 ] + 2P [V = 5 ] + \u00b7 \u00b7 \u00b7+ 2P [V = n\u2212 1 ] ;\n... ... ...\nP [V \u2265 n\u2212 1 ] \u2264 2P [V = n\u2212 1 ] , P [V \u2265 n ] \u2264 2P [V = n\u2212 1 ] .\nSumming up the above inequalities, we have\nE [V ] = n\u2211 k=1 P [V \u2265 k ]\n\u2264 \u2211\nk=1,3,\u00b7\u00b7\u00b7 ,n\u22121\n2(k \u2212 1)vk + \u2211\nk=1,3,\u00b7\u00b7\u00b7 ,n\u22121\n2vk\n\u2264 (n\u2212 2) sin2 \u03b8 + n\u2211 k=0 vk \u2264 (n\u2212 2) sin2 \u03b8 + 1 = n sin2 \u03b8 + (cos 2\u03b8),\nwhere the second last inequality follows observations that \u2211 k=1,3,\u00b7\u00b7\u00b7 ,n\u22121(k\u22121)vk = E [ Bin(n\u221222 , sin 2 \u03b8) ]\nand \u2211n k=0 vk \u2265 \u2211 k=1,3,\u00b7\u00b7\u00b7 ,n\u22121 2vk by the interlacing result in Proposition 5.6 of [ALMT13].\nSuppose n is odd. Since the intersection of circn+1(\u03b8) with any n-dimensional linear subspace containing en+1 is an isometric image of circn(\u03b8), by Proposition 4.1 in [ALMT13], we have\n\u03b4(circn(\u03b8)) = \u03b4(circn(\u03b8)\u00d7 {0}) \u2264 \u03b4(circn+1(\u03b8)) \u2264 (n+ 1) sin2 \u03b8 + cos(2\u03b8) \u2264 n sin2 \u03b8 + cos2 \u03b8.\nThus, taking both cases (d is even and d is odd) into consideration, we have \u03b4 ( circn(\u03b8) ) \u2264 n sin2 \u03b8 + 1."}, {"heading": "Proof of Corollary 4.", "text": "Proof. Denote \u03bb = \u03b4(C)\u2212m. Then following Theorem 7.1 in [ALMT13], we have P [C \u2229 null(G) = {0} ] \u2264 exp ( \u2212 \u03bb 2/8\nmin{\u03b4(C), \u03b4(C\u25e6)}+ \u03bb ) \u2264 exp ( \u2212 \u03bb 2/8\n\u03b4(C) + \u03bb ) \u2264 4 exp ( \u2212 (\u03b4(C)\u2212m) 2\n16\u03b4(C)\n) ."}, {"heading": "Proof of Theorem 5.", "text": "Proof. Notice that for any fixed m > 0, the function f : t \u2192 4 exp ( \u2212 (t\u2212m) 2\n16t\n) is decreasing for\nt \u2265 m. Then due to Corollary 4 and the fact that \u03b4(C) \u2265 \u03ba\u2212 1 \u2265 m, we have\nP [x0 is the unique optimal solution to (3.3) ] = P [C \u2229 null(G) = {0} ] \u2264 4 exp ( \u2212 (\u03b4(C)\u2212m) 2\n16\u03b4(C) ) \u2264 4 exp ( \u2212 (\u03ba\u2212m\u2212 1) 2\n16 (\u03ba\u2212 1)\n) ."}, {"heading": "C Proofs for Section 4", "text": ""}, {"heading": "Proof of Lemma 5.", "text": "Proof. (1) By the definition of X [j], it is sufficient to prove that the vectorization of the right hand side of (4.2) equals vec(X (1)).\nSince X = \u2211r i=1 \u03bbia (1) i \u25e6 a (2) i \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 a (K) i , we have\nvec(X (1)) = vec ( r\u2211 i=1 \u03bbia (1) i \u25e6 (a (K) i \u2297 a (K\u22121) i \u2297 \u00b7 \u00b7 \u00b7 \u2297 a (2) i ) )\n= r\u2211 i=1 \u03bbivec ( a (1) i \u25e6 (a (K) i \u2297 a (K\u22121) i \u2297 \u00b7 \u00b7 \u00b7 \u2297 a (2) i ) )\n= r\u2211 i=1 \u03bbi(a (K) i \u2297 a (K\u22121) i \u2297 \u00b7 \u00b7 \u00b7 \u2297 a (2) i \u2297 a (1) i ),\nwhere the last equality follows from the fact that vec(a \u25e6 b) = b\u2297 a. Similarly, we can derive that the vectorization of the right hand side of (4.2),\nvec( r\u2211 i=1 \u03bbi(a (j) i \u2297 a (j\u22121) i \u2297 \u00b7 \u00b7 \u00b7 \u2297 a (1) i ) \u25e6 (a (K) i \u2297 a (K\u22121) i \u00b7 \u00b7 \u00b7 \u2297 a (j+1) i ))\n= r\u2211 i=1 \u03bbivec ( (a (j) i \u2297 a (j\u22121) i \u2297 \u00b7 \u00b7 \u00b7 \u2297 a (1) i ) \u25e6 (a (K) i \u2297 a (K\u22121) i \u00b7 \u00b7 \u00b7 \u2297 a (j+1) i ) ) =\nr\u2211 i=1 \u03bbi(a (K) i \u2297 a (K\u22121) i \u2297 \u00b7 \u00b7 \u00b7 \u2297 a (2) i \u2297 a (1) i )\n= vec(X (1)).\nThus, equation (4.2) is valid.\n(2) The above argument can be easily adapted to prove the second claim. Since X = C \u00d71 U1 \u00d72 U2 \u00d73 \u00b7 \u00b7 \u00b7 \u00d7K UK , we have\nvec(X (1)) = vec ( U1 C(1) (UK \u2297UK\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297U2)\u2217 ) = (UK \u2297UK\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297U1) vec(C(1)),\nwhere the last equality follows from the fact that vec(ABC) = (C\u2217 \u2297A)vec(B). Similarly, we can derive that the vectorization of the right hand side of (4.3),\nvec ( (U j \u2297U j\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297U1) C[j] (UK \u2297UK\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297U j+1)\u2217 ) = (UK \u2297UK\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297U1) vec(C[j]) = (UK \u2297UK\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297U1) vec(C(1)) = vec(X (1)).\nThus, equation (4.3) is valid."}, {"heading": "D Algorithms for Section 5", "text": "In this section, we will discuss in detail our implementation of accelerated linearized Bregman algorithm for the following problem:\nminimizeX K\u2211 i=1 \u2016X (i)\u2016\u2217 subject to P\u2126[X ] = P\u2126[X 0]. (D.1)\nBy introducing auxiliary variable W and splitting X into X 1, X 2, \u00b7 \u00b7 \u00b7 , XK , it can be easily\nverified that problem (D.1) is equivalent to\nmin({X i},W) K\u2211 i=1 \u2016(X i)(i)\u2016\u2217\ns.t. X i = W , i = 1, 2, \u00b7 \u00b7 \u00b7 ,K, (D.2) P\u2126[W ] = P\u2126[X 0],\nwhose objective function is now separable. The accelerated linearized Bregman (ALB) algorithm, proposed in [HMG13], is an efficient firstorder method designed for solving convex optimization problems with nonsmooth objective functions and linear constraints. It has been successfully applied to solve `1 and nuclear norm minimization problems [HMG13]. The ALB algorithm solves nonsmooth problem by firstly smoothing the objective function (e.g. adding a small l2 perturbation), and then exploiting Nesterov\u2019s accelerated scheme [Nes83] to the dual problem, which can be verified to be unconstrained and Lipschitz differentiable. In Algorithm 1, we describe our ALM algorithm adapted to problem (D.2). Algorithm 1 solves exactly the smoothed version of problem (D.2):\nmin({X i},W) K\u2211 i=1 ( \u2016(X i)(i)\u2016\u2217 + 1 2\u00b5 \u2016(X i)(i)\u20162F ) + 1 2\u00b5 \u2016W\u20162F\ns.t. X i = W , i = 1, 2, \u00b7 \u00b7 \u00b7 ,K, (D.3) P\u2126[W ] = P\u2126[X 0],\nwhere we denote Yi as the dual variable for the constraint X i = W and denote Z as the dual variable for the last constraint P\u2126[W ] = P\u2126[X 0]. Since the objective function in (D.3) is separable, each setup of the ALB algorithm is easy to solve as we can see from Algorithm 1 3.\nAlgorithm 1: accelerated linearized Bregman algorithm for SNN model (D.1)\n1 Initialization: Y0i = Y\u0303 0 i = 0 for each i \u2208 [K], Z 0 = Z\u03030 = 0, \u00b5 > 0, \u03c4 > 0, t0 = 1; 2 for k = 0, 1, 2, \u00b7 \u00b7 \u00b7 do 3 for i = 1, 2, \u00b7 \u00b7 \u00b7 , K do 4 X k+1i = \u00b5 \u00b7 Shrinkage(Y k i , 1) ;\n5 Wk+1 = \u00b5 \u00b7 ( P\u2126 [ Zk ] \u2212 \u2211 iY k i ) ; 6 for i = 1, 2, \u00b7 \u00b7 \u00b7 , K do 7 Y\u0303ki = Y k i \u2212 \u03c4 \u00b7 ( X k+1i \u2212W k+1 ) ;\n8 Z\u0303k = Zk \u2212 \u03c4 \u00b7 P\u2126 [ Wk+1 \u2212X 0 ] ; 9 tk+1 = 1+ \u221a 1+4t2k 2 ;\n10 for i = 1, 2, \u00b7 \u00b7 \u00b7 , K do 11 Yk+1i = Y\u0303 k i + tk\u22121 tk+1 ( Y\u0303ki \u2212 Y\u0303 k\u22121 i ) ;\n12 Zk+1 = Z\u0303k + tk\u22121tk+1 ( Z\u0303k \u2212 Z\u0303k\u22121 ) ;\n3 The Shrinkage operator in line 4 of Algorithm 1 performs the regular shrinkage on the singular values of the ith unfolding matrix of Yki , i.e. (Yki )(i), and then folds the resulting matrix back into tensor.\nFor our numerical experiment (K = 4), we choose smoothing parameter \u00b5 = 50\u2016X 0\u2016F and step size \u03c4 = 15\u00b5 . Empirically, we observe that larger values of \u00b5 do not result in a better recovery performance. This is consistent with the theoretical results established in [LY12, ZCCZ11]."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Recovering a low-rank tensor from incomplete information is a recurring problem in signal processing and machine learning. The most popular convex relaxation of this problem minimizes the sum of the nuclear norms of the unfoldings of the tensor. We show that this approach can be substantially suboptimal: reliably recovering a K-way tensor of length n and Tucker rank r from Gaussian measurements requires \u03a9(rnK\u22121) observations. In contrast, a certain (intractable) nonconvex formulation needs only O(r +nrK) observations. We introduce a very simple, new convex relaxation, which partially bridges this gap. Our new formulation succeeds with O(rbK/2cndK/2e) observations. While these results pertain to Gaussian measurements, simulations strongly suggest that the new norm also outperforms the sum of nuclear norms for tensor completion from a random subset of entries. Our lower bounds for the sum-of-nuclearnorm model follow from a new result on simultaneously structured models, which may be of independent interest for matrix and vector recovery problems.", "creator": "LaTeX with hyperref package"}}}