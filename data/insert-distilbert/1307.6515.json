{"id": "1307.6515", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jul-2013", "title": "Cluster Trees on Manifolds", "abstract": "in this paper we investigate the problem results of estimating the cluster tree for a density $ f $ h supported manifold on or near of a smooth $ d $ - dimensional manifold $ m $ isometrically embedded in $ \\ mathbb { r } ^ d $. we analyze a modified version of a $ gt k $ - dimensions nearest neighbor based algorithm recently informally proposed by chaudhuri and dasgupta. the main results of this paper show that under mild assumptions on $ f $ and $ q m $, though we easily obtain rates of convergence that better depend on $ d $ only but not on the ambient dimension $ d $. we also show shown that essentially similar ( an albeit non - algorithmic ) results can jointly be obtained for kernel density estimators. we sketch a construction of a conditional sample complexity lower bound instance for a natural class of metric manifold oblivious clustering algorithms. we further briefly consider the weakly known manifold case and show that in this case a spatially adaptive algorithm achieves better rates.", "histories": [["v1", "Wed, 24 Jul 2013 18:17:53 GMT  (80kb)", "http://arxiv.org/abs/1307.6515v1", "28 pages, 3 figures"]], "COMMENTS": "28 pages, 3 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["sivaraman balakrishnan", "srivatsan narayanan", "alessandro rinaldo", "aarti singh", "larry a wasserman"], "accepted": true, "id": "1307.6515"}, "pdf": {"name": "1307.6515.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n30 7.\n65 15\nv1 [\nst at\n.M L\n] 2\n4 Ju\nl 2 01\n3"}, {"heading": "CLUSTER TREES ON MANIFOLDS", "text": ""}, {"heading": "By Sivaraman Balakrishnan, Srivatsan Narayanan, Alessandro Rinaldo, Aarti", "text": "Singh and Larry Wasserman\nSchool of Computer Science and Statistics Department Carnegie Mellon University\nIn this paper we investigate the problem of estimating the cluster tree for a density f supported on or near a smooth d-dimensional manifold M isometrically embedded in RD. We analyze a modified version of a k-nearest neighbor based algorithm recently proposed by Chaudhuri and Dasgupta [1]. The main results of this paper show that under mild assumptions on f and M , we obtain rates of convergence that depend on d only but not on the ambient dimension D. We also show that similar (albeit non-algorithmic) results can be obtained for kernel density estimators. We sketch a construction of a sample complexity lower bound instance for a natural class of manifold oblivious clustering algorithms. We further briefly consider the known manifold case and show that in this case a spatially adaptive algorithm achieves better rates.\n1. Introduction. In this paper, we study the problem of estimating the cluster tree of a density when the density is supported on or near a manifold. Let X := {X1, . . . ,Xn} be a sample drawn i.i.d. from a distribution P with density f . The connected components Cf (\u03bb) of the upper level set {x : f(x) \u2265 \u03bb} are called density clusters. The collection C = {Cf (\u03bb) : \u03bb \u2265 0} of all such clusters is called the cluster tree and estimating this cluster tree is referred to as density clustering.\nThe density clustering paradigm is attractive for various reasons. One of the main difficulties of clustering is that often the true goals of clustering are not clear and this makes clusters, and clustering as a task seem poorly defined. Density clustering however is estimating a well defined population quantity, making its goal, consistent recovery of the population density clusters, clear. Typically only mild assumptions are made on the density f and this allows extremely general shapes and numbers of clusters at each level. Finally, the cluster tree is an inherently hierarchical object and thus density clustering algorithms typically do not require specification of the \u201cright\u201d level, rather they capture a summary of the density across all levels.\nThe search for a simple, statistically consistent estimator of the cluster tree has a long history. Hartigan [2] showed that the popular single-linkage algorithm is not consistent for a sample from R D, with D > 1. Recently, Chaudhuri and Dasgupta [1] analyzed an algorithm which is both simple and consistent. The algorithm finds the connected components of a sequence of carefully constructed neighborhood graphs. They showed that, as long as the parameters of the algorithm are chosen appropriately, the resulting collection of connected components correctly estimates the cluster tree with high probability.\nIn this paper, we are concerned with the problem of estimating the cluster tree when the density f is supported on or near a low dimensional manifold. The motivation for this work stems from the problem of devising and analyzing clustering algorithms with provable performance that can be used in high dimensional applications. When data live in high dimensions, clustering (as well as other statistical tasks) generally become prohibitively difficult due to the curse of dimensionality,\nwhich demands a very large sample size. In many high dimensional applications however data is not spread uniformly but rather concentrates around a low dimensional set. This so-called manifold hypothesis motivates the study of data generated on or near low dimensional manifolds and the study of procedures that can adapt effectively to the intrinsic dimensionality of this data.\n1.1. Contributions. Here is a brief summary of the main contributions of this paper:\n1. We show that the simple algorithm studied in [1] is consistent and has fast rates of convergence for data on or near a low dimensional manifold M . The algorithm does not require the user to first estimate M (which is a difficult problem). In other words, the algorithm adapts to the (unknown) manifold. 2. We show that the sample complexity for identifying salient clusters is independent of the ambient dimension. 3. We sketch a construction of a sample complexity lower bound instance for a natural class of clustering algorithms that we study in this paper. 4. We show that in the known manifold case a modified spatially adaptive algorithm achieves better rates, similar to the near minimax-optimal rates of [1]. 5. We introduce a framework for studying consistency of clustering when the distribution is not supported on a manifold but rather, is concentrated near a manifold. The generative model in this case is that the data are first sampled from a distribution on a manifold and then noise is added. The original data are latent (unobserved). We show that for certain noise models we can still efficiently recover the cluster tree on the latent samples. 6. We show similar statistical results for the level sets of kernel density estimates for an appropriately chosen bandwidth. Computing the level sets of the kernel density estimate is however a challenging problem that we do not address in this paper. 7. We present some simulations to confirm our theoretical results.\n1.2. Related Work. The idea of using probability density functions for clustering dates back to Wishart [3]. [2] expanded on this idea and formalized the notions of high-density clustering, of the cluster tree and of consistency and fractional consistency of clustering algorithms. In particular, [2] showed that single linkage clustering is consistent when D = 1 but is only fractionally consistent when D > 1. [4] and [5] have also proposed procedures for recovering the cluster tree. None of these procedures however, come with the theoretical guarantees given by [1], which demonstrated that a generalization of Wishart\u2019s algorithm allows one to estimate parts of the cluster tree for distributions with full-dimensional support near-optimally under rather mild assumptions. This paper forms the starting point for our work and is reviewed in more detail in the next section.\nIn the last two decades, much of the research effort involving the use of nonparametric density estimators for clustering has focused on the more specialized problems of optimal estimation of the support of the distribution or of a fixed level set. However, consistency of estimators of a fixed level set does not imply cluster tree consistency, and extending the techniques and analyses mentioned above to hold simultaneously over a variety of density levels is non-trivial. See, e.g., [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and references therein. Estimating the cluster tree has more recently been considered in [16] which also gives a simple pruning procedure for removing spurious clusters. [17, 18] propose procedures for determining recursively the lowest split in the cluster tree and give conditions for asymptotic consistency with minimal assumptions on the density.\n2. Background and Assumptions. Let P be a distribution supported on an unknown ddimensional manifold M . We assume that the manifold M is a d-dimensional Riemannian manifold without boundary embedded in a compact set X \u2282 RD with d < D. We further assume that the volume of the manifold is bounded from above by a constant, i.e., vold(M) \u2264 C. The main regularity condition we impose on M is that its condition number be not too large. The condition number of M is 1/\u03c4 , where \u03c4 is the largest number such that the open normal bundle about M of radius r is imbedded in RD for every r < \u03c4 . The condition number controls the curvature of M and prevents it from being too close to being self-intersecting (see [19] for a detailed treatment).\nThe Euclidean norm is denoted by \u2016 \u00b7 \u2016 and vd denotes the volume of the d-dimensional unit ball in R d. B(x, r) denotes the full-dimensional ball of radius r centered at x and BM (x, r) ..= B(x, r)\u2229M . For Z \u2282 Rd and \u03c3 > 0, define Z\u03c3 = Z + B(0, \u03c3) and ZM,\u03c3 = (Z + B(0, \u03c3)) \u2229M . Note that Z\u03c3 is full dimensional, while if Z \u2286 M then ZM,\u03c3 is d-dimensional.\nLet f be the density of P with respect to the uniform measure on M . For \u03bb \u2265 0, let Cf (\u03bb) be the collection of connected components of the level set {x \u2208 X : f(x) \u2265 \u03bb} and define the cluster tree of f to be the hierarchy C = {Cf (\u03bb) : \u03bb \u2265 0}. For a fixed \u03bb, any member of Cf (\u03bb) is a cluster. For a cluster C its restriction to the sample X is defined to be C[X] = C \u2229X. The restriction of the cluster tree C to X is defined to be C[X] = {C \u2229X : C \u2208 C}. Informally, this restriction is a dendrogram-like hierarchical partition of X.\nTo give finite sample results, following [1], we define the notion of salient clusters. Our definitions are slight modifications of those in [1] to take into account the manifold assumption.\nDefinition 1. Clusters A and A\u2032 are (\u03c3, \u01eb) separated if there exists a nonempty S \u2282 M such that:\n1. Any path along M from A to A\u2032 intersects S. 2. supx\u2208SM,\u03c3 f(x) < (1\u2212 \u01eb) infx\u2208AM,\u03c3\u222aA\u2032M,\u03c3 f(x).\nChaudhuri and Dasgupta [1] analyze a robust single linkage (RSL) algorithm (in Figure 1). An RSL algorithm estimates the connected components at a level \u03bb in two stages. In the first stage, the sample is cleaned by thresholding the k-nearest neighbor distance of the sample points at a radius r and then, in the second stage, the cleaned sample is connected at a connection radius R. The connected components of the resulting graph give an estimate of the restriction Cf (\u03bb)[X]. In Section 4 we prove a sample complexity lower bound for the class of RSL algorithms which we now define.\nDefinition 2. The class of RSL algorithms refers to any algorithm that is of the form described in the algorithm in Figure 1 and relying on Euclidean balls, with any choice of k, r and R.\nWe define two notions of consistency for an estimator C\u0302 of the cluster tree:\nDefinition 3 (Hartigan consistency). For any sets A, A\u2032 \u2282 X , let An (resp., A\u2032n) denote the smallest cluster of C\u0302 containing A \u2229X (resp, A\u2032 \u2229X). We say Cn is consistent if, whenever A and A\u2032 are different connected components of {x : f(x) \u2265 \u03bb} (for some \u03bb > 0), the probability that An is disconnected from A\u2032n approaches 1 as n \u2192 \u221e.\n1. For each Xi, rk(Xi) := inf{r : B(Xi, r) contains k data points}.\n2. As r grows from 0 to \u221e:\n(a) Construct a graph Gr,R with nodes {Xi : rk(Xi) \u2264 r} and edges (Xi, Xj) if \u2016Xi \u2212Xj\u2016 \u2264 R.\n(b) Let C(r) be the connected components of Gr,R.\n3. Denote C\u0302 = {C(r) : r \u2208 [0,\u221e)} and return C\u0302.\nFig 1. Robust Single Linkage (RSL) Algorithm\nDefinition 4 ((\u03c3, \u01eb) consistency). For any sets A, A\u2032 \u2282 X such that A and A\u2032 are (\u03c3, \u01eb) separated, let An (resp., A \u2032 n) denote the smallest cluster of C\u0302 containing A \u2229X (resp, A\u2032 \u2229X). We say Cn is consistent if, whenever A and A\u2032 are different connected components of {x : f(x) \u2265 \u03bb} (for some \u03bb > 0), the probability that An is disconnected from A \u2032 n approaches 1 as n \u2192 \u221e.\nThe notion of (\u03c3, \u01eb) consistency is similar that of Hartigan consistency except restricted to (\u03c3, \u01eb) separated clusters A and A\u2032, and typically associated with a finite sample of size n. [1] prove the following theorem, establishing finite sample bounds for a particular RSL algorithm. In this theorem there is no manifold and f is a density with respect to the Lebesgue measure on RD.\nTheorem 5. There is a constant C such that the following holds. Suppose that we run the algorithm in Figure 1 with\nR = \u221a 2r and k = C\n( D log n\n\u01eb2\n) log2(1/\u03b4)\nthen with probability at least 1\u2212 \u03b4, the algorithm output C\u0302 is (\u03c3, \u01eb) consistent provided\n\u03bb \u2265 1 vD(\u03c3/2)D k n\n( 1 + \u01eb\n2\n) .\nThe theorem as stated does not explicitly give a sample complexity bound but it is straightforward to obtain one by plugging in the value for k and solving for n in the inequality that restricts \u03bb to be large enough (as a function of n).\nIn particular, notice that if\nn \u2265 O (\nD\n\u03bb\u01eb2vD(\u03c3/2)D log\nD\n\u03bb\u01eb2vD(\u03c3/2)D\n)\nthen we can resolve any pair of (\u03c3, \u01eb) clusters at level at least \u03bb. It is important to note that this theorem does not apply to the setting when distributions are supported on a lower dimensional set for at least two reasons: (1) the density f is singular with respect to the Lebesgue measure on X and so the cluster tree is trivial, and (2) the definitions of saliency with respect to X are typically not satisfied when f has a lower dimensional support.\n3. Clustering on Manifolds. In this section we show that the RSL algorithm can be adapted to recover the cluster tree of a distribution supported on a manifold of dimension d < D with the rates depending only on d. In place of the cluster salience parameter \u03c3, our rates involve a new parameter \u03c1\n\u03c1 := min\n( 3\u03c3\n16 , \u01eb\u03c4 72d , \u03c4 16\n) .\nThe precise reason for this definition of \u03c1 will be clear from the proofs (particularly of Lemma 8) but for now notice that in addition to \u03c3 it is dependent on the condition number 1/\u03c4 and deteriorates as the condition number increases. Finally, to succinctly present our results we use \u00b5 := log n+ d log(1/\u03c1).\nTheorem 6. There are universal constants C1 and C2 such that the following holds. For any \u03b4 > 0, 0 < \u01eb < 1/2, run the algorithm in Figure 1 on a sample X drawn from f , where the parameters are set according to the equations\nR = 4\u03c1 and k = C1 log 2(1/\u03b4)(\u00b5/\u01eb2).\nThen with probability at least 1\u2212\u03b4, C\u0302 is (\u03c3, \u01eb) consistent. In particular, the clusters containing A[X] and A\u2032[X], where A and A\u2032 are (\u03c3, \u01eb) separated, are internally connected and mutually disconnected in C(r) for r defined by\nvdr d\u03bb =\n1\n1\u2212 \u01eb/6\n( k\nn +\nC2 log(1/\u03b4)\nn\n\u221a k\u00b5\n)\nprovided\n\u03bb \u2265 2 vd\u03c1d k n .\nBefore we prove this theorem a few remarks are in order:\n1. To obtain an explicit sample complexity, as in Theorem 5, we plug in the value of k and solve for n from the inequality restricting \u03bb. The sample complexity of the RSL algorithm for recovering (\u03c3, \u01eb) clusters at level at least \u03bb on a manifold M with condition number at most 1/\u03c4 is\nn = O\n( d\n\u03bb\u01eb2vd\u03c1d log\nd\n\u03bb\u01eb2vd\u03c1d\n)\nwhere \u03c1 = Cmin (\u03c3, \u01eb\u03c4/d, \u03c4). Ignoring constants that depend on d the main difference between this result and the result of [1] (Theorem 5) is that our results only depend on the manifold dimension d and not the ambient dimension D (typically D \u226b d). There is also a dependence of our result on 1/(\u01eb\u03c4)d, for \u01eb\u03c4 \u226a \u03c3. In Section 4 we sketch the construction of an instance that suggests that this dependence is not an artifact of our analysis and that the sample complexity of the class of RSL algorithms is at least n \u2265 1/(\u01eb\u03c4)\u2126(d). 2. Another aspect is that our choice of the connection radius R depends on the (typically) unknown \u03c1, while for comparison, the connection radius in [1] is chosen to be \u221a 2r. Under\nthe mild assumption that \u03bb \u2264 nO(1) (which is satisfied for instance, if the density on M is bounded from above), we show in Section 9.8 that an identical theorem holds for R = 4r. k is the only real tuning parameter of this algorithm whose choice depends on \u01eb and an unknown leading constant.\n3. It is easy to see that this theorem also establishes consistency for recovering the entire cluster tree by selecting an appropriate schedule on \u03c3n, \u01ebn and kn that ensures that all clusters are distinguished for n large enough (see [1] for a formal proof).\nOur proofs structurally mirror those in [1]. We begin with a few technical results in 3.1. In Section 3.2 we establish (\u03c3, \u01eb) consistency by showing that the clusters are mutually disjoint and internally connected. The main technical challenge is that the curvature of the manifold, modulated by its condition number 1/\u03c4 , limits our ability to resolve the density level sets from a finite sample, by limiting the maximum cleaning and connection radii the algorithm can use. In what follows, we carefully analyze this effect and show that somewhat surprisingly, despite this curvature, essentially the same algorithm is able to adapt to the unknown manifold and produce a consistent estimate of the entire cluster tree. Similar manifold adaptivity results have been shown in classification [20] and in non-parametric regression [21, 22].\n3.1. Technical results. In our proof, we use the uniform convergence of the empirical mass of Euclidean balls to their true mass. In the full dimensional setting of [1], this follows from standard VC inequalities. To the best of our knowledge however sharp (ambient dimension independent) inequalities for manifolds are unknown. We get around this obstacle by using the insight that, in order to analyze the RSL algorithms, uniform convergence for Euclidean balls around the sample points and around a fixed minimum s-net N of M (for an appropriately chosen s) suffice to analyze the RSL algorithm.\nRecall, an s-net N \u2286 M is such that every point of M is at a distance at most s from some point in N . Let\nBn,N := { B(z, s) : z \u2208 N \u222aX, s \u2265 0 }\nbe the collection of balls whose centers are sample or net points. We are ready to state our uniform convergence lemma. The proof is in Section 9.3.\nLemma 7 (Uniform Convergence). Assume k \u2265 \u00b5. Then there exists a constant C0 such that the following holds. For every \u03b4 > 0, with probability > 1\u2212 \u03b4, for all B \u2208 Bn,N , we have:\nP (B) \u2265 C\u03b4\u00b5 n =\u21d2 Pn(B) > 0\nP (B) \u2265 k n + C\u03b4 n\n\u221a k\u00b5 =\u21d2 Pn(B) \u2265 k\nn\nP (B) \u2264 k n \u2212 C\u03b4 n\n\u221a k\u00b5 =\u21d2 Pn(B) < k\nn ,\nwhere C\u03b4 := 2C0 log(2/\u03b4), and \u00b5 := 1 + log n + log |N | = Cd + log n + d log(1/s). Here Pn(B) = |X \u2229B|/n denotes the empirical probability measure of B, and C is a universal constant.\nNext we provide a tight estimate of the volume of a small ball intersected with M . This bounds the distortion of the apparent density due to the curvature of the manifold and is central to many of our arguments. Intuitively, the claim states that the volume is approximately that of a d-dimensional Euclidean ball, provided that its radius is small enough compared to \u03c4 . The lower bound is based on Lemma 5.3 of [19] while the upper bound is based on a modification of the main result of [23].\nLemma 8 (Ball volumes). Assume r < \u03c4/2. Define S := B(x, r) \u2229M for a point x \u2208 M . Then ( 1\u2212 r 2\n4\u03c42\n)d/2 vdr d \u2264 vold(S) \u2264 vd (\n\u03c4\n\u03c4 \u2212 2r1\n)d rd1 ,\nwhere r1 = \u03c4 \u2212 \u03c4 \u221a 1\u2212 2r/\u03c4 . In particular, if r \u2264 \u01eb\u03c4/72d for 0 \u2264 \u01eb < 1, then\nvdr d(1\u2212 \u01eb/6) \u2264 vold(S) \u2264 vdrd(1 + \u01eb/6).\n3.2. Separation and Connectedness.\nLemma 9 (Separation). Assume that we pick k, r and R to satisfy the conditions:\nr \u2264 \u03c1 R = 4\u03c1\nvdr d(1\u2212 \u01eb/6)\u03bb \u2265 k\nn + C\u03b4 n\n\u221a k\u00b5\nvdr d(1 + \u01eb/6)\u03bb(1 \u2212 \u01eb) \u2264 k n \u2212 C\u03b4 n\n\u221a k\u00b5.\nThen with probability 1\u2212 \u03b4, we have:\n1. All points in A\u03c3\u2212r and A \u2032 \u03c3\u2212r are kept, and all points in S\u03c3\u2212r are removed. 2. The two point sets A \u2229X and A\u2032 \u2229X are disconnected in Gr,R.\nProof. The proof is analogous to the separation proof of [1] with several modifications. Most importantly, we need to ensure that despite the curvature of the manifold we can still resolve the density well enough to guarantee that we can identify and eliminate points in the region of separation.\nThroughout the proof, we will assume that the good event in Lemma 7 (uniform convergence for Bn,N ) occurs. Since r \u2264 \u01eb\u03c4/72d, by Lemma 8 vol(BM (x, r)) is between vdrd(1 \u2212 \u01eb/6) and vdr\nd(1+ \u01eb/6), for any x \u2208 M . So if Xi \u2208 A\u222aA\u2032, then BM (Xi, r) has mass at least vdrd(1\u2212 \u01eb/6) \u00b7\u03bb. Since this is \u2265 kn + C\u03b4 n \u221a k\u00b5 by assumption, this ball contains at least k sample points, and hence Xi is kept.\nOn the other hand, ifXi \u2208 S\u03c3\u2212r, then the set BM (Xi, r) contains mass at most vdrd(1+\u01eb/6)\u00b7\u03bb(1\u2212\u01eb). This is \u2264 kn \u2212 C\u03b4 n \u221a k\u00b5. Thus by Lemma 7 BM (Xi, r) contains fewer than k sample points, and hence Xi is removed.\nTo prove the graph is disconnected, we first need a bound on the geodesic distance between two points that are at most R apart in Euclidean distance. Such an estimate follows from Proposition 6.3 in [19] who show that if \u2016p\u2212 q\u2016 = R \u2264 \u03c4/2, then the geodesic distance\ndM (p, q) \u2264 \u03c4 \u2212 \u03c4 \u221a\n1\u2212 2R \u03c4 .\nIn particular, if R \u2264 \u03c4/4, then dM (p, q) < R ( 1 + 4R\u03c4 ) \u2264 2R. Now, notice that if the graph is connected there must be an edge that connects two points that are at a geodesic distance of at\nleast 2(\u03c3 \u2212 r). Any path between a point in A and a point in A\u2032 along M must pass through S\u03c3\u2212r and must have a geodesic length of at least 2(\u03c3 \u2212 r). This is impossible if the connection radius satisfies 2R < 2(\u03c3 \u2212 r), which follows by the assumptions on r and R.\nAll the conditions in Lemma 9 can be simultaneously satisfied by setting k := 16C2\u03b4 (\u00b5/\u01eb 2), and\n(1) vdr d(1\u2212 \u01eb/6) \u00b7 \u03bb = k\nn + C\u03b4 n\n\u221a k\u00b5.\nThe condition on r is satisfied since\n\u03bb \u2265 2 vd\u03c1d k n\nand the condition on R is satisfied by its definition.\nLemma 10 (Connectedness). Assume that the parameters k, r and R satisfy the separation conditions (in Lemma 9). Then, with probability at least 1\u2212 \u03b4, A[X] is connected in Gr,R.\nProof. Let us show that any two points in A \u2229X are connected in Gr,R. Consider y, y\u2032 \u2208 A \u2229X. Since A is connected, there is a path P between y, y\u2032 lying entirely inside A, i.e., a continuous map P : [0, 1] \u2192 A such that P (0) = y and P (1) = y\u2032. We can find a sequence of points y0, . . . , yt \u2208 P such that y0 = y, yt = y\n\u2032, and the geodesic distance on M (and hence the Euclidean distance) between yi\u22121 and yi is at most \u03b7, for an arbitrarily small constant \u03b7.\nLet N be minimal R/4-net of M . There exist zi \u2208 N such that \u2016yi \u2212 zi\u2016 \u2264 R/4. Since yi \u2208 A, we have zi \u2208 AM,R/4, and hence the ball BM (zi, R/4) lies completely inside AM,R/2 \u2286 AM,\u03c3\u2212r. In particular, the density inside the ball is at least \u03bb everywhere, and hence the mass inside it is at least\nvd(R/4) d(1\u2212 \u01eb/6)\u03bb \u2265 C\u03b4\u00b5\nn .\nObserve that R \u2265 4r and so this condition is satisfied as a consequence of satisfying Equation 1. Thus Lemma 7 guarantees that the ball BM (zi, R/4) contains at least one sample point, say xi. (Without loss of generality, we may assume x0 = y and xt = y\n\u2032.) Since the ball lies completely in AM,\u03c3\u2212r, the sample point xi is not removed in the cleaning step (Lemma 9).\nFinally, we bound d(xi\u22121, xi) by considering the sequence of points (xi\u22121, zi\u22121, yi\u22121, yi, zi, xi). The pair (yi\u22121, yi) are at most s apart and the other successive pairs at most R/4 apart, hence d(xi\u22121, xi) \u2264 4(R/4) + \u03b7 = R+ \u03b7. The claim follows by letting \u03b7 \u2192 0.\n4. A lower bound instance for the class of RSL algorithms. Recall that the sample complexity in Theorem 6 scales as\nn = O\n( d\n\u03bb\u01eb2vd\u03c1d log\nd\n\u03bb\u01eb2vd\u03c1d\n)\nwhere \u03c1 = Cmin (\u03c3, \u01eb\u03c4/d, \u03c4). For full dimensional densities, [1] showed the information theoretic lower bound\nn = \u2126\n( 1\n\u03bb\u01eb2vD\u03c3D log\n1\n\u03bb\u01eb2vD\u03c3D\n) .\nTheir construction can be straightforwardly modified to a d-dimensional instance on a smooth manifold. Ignoring constants that depend on d, these upper and lower bounds can still differ by a factor of 1/(\u01eb\u03c4)d, for \u01eb\u03c4 \u226a \u03c3. In this section we provide an informal sketch of a hard instance for the class of RSL algorithms (see Definition 2) that suggests a sample complexity lower bound of n \u2265 1/(\u01eb\u03c4)\u2126(d).\nWe first describe our lower bound instance. The manifold M consists of two disjoint components, C and C \u2032. The component C in turn contains three parts, which we call \u2018top\u2019, \u2018middle\u2019, and \u2018bottom\u2019 respectively. The middle part, denoted M2, is the portion of the standard d-dimensional unit sphere S d(0, 1) between the planes x1 = + \u221a 1\u2212 4\u03c42 and x1 = \u2212 \u221a 1\u2212 4\u03c42. The top part, denoted M1, is\nthe upper hemisphere of radius 2\u03c4 centered at (+ \u221a 1\u2212 4\u03c42, 0, 0, . . . , 0). The bottom part, denoted\nM3, is a symmetric hemisphere centered at (\u2212 \u221a 1\u2212 4\u03c42, 0, 0, . . . , 0). Thus C is obtained by gluing a portion of the unit sphere with two (small) hemispherical caps. C as described does not have a condition number at most 1/\u03c4 because of the \u201ccorners\u201d at the intersection of M2 and M1 \u222a M3. This can be fixed without affecting the essence of the construction by smoothing this intersection by rolling a ball of radius \u03c4 around it (a similar construction is made rigorous in Theorem 6 of [25]). Finally, the component C \u2032 is a sphere far away from C whose function ensure that f integrates to 1.\nLet P be the distribution on M whose density over C is \u03bb if |x1| > 1/2, and \u03bb(1\u2212 \u01eb) if |x1| \u2264 1/2, where \u03bb is chosen small enough such that \u03bb vold(C) \u2264 1. The density over C \u2032 is chosen such that the total mass of the manifold is 1. Now M1 and M3 are (\u03c3, \u01eb) separated at level \u03bb for \u03c3 = \u2126(1). The separator set S is the equator of M2 in the plane x1 = 0.\nWe now provide some intuition for why RSL algorithms will require n \u2265 1/(\u01eb\u03c4)\u2126(d) to succeed on this instance. We focus our discussion on RSL algorithms with k > 2, i.e. on algorithms that do in fact use a cleaning step, ignoring the single linkage algorithm which is known to be inconsistent for full dimensional densities.\nIntuitively, because of the curvature of the described instance, the mass of a sufficiently large Euclidean ball in the separator set is larger than the mass of a corresponding ball in the true clusters. This means that any algorithm that uses large balls cannot reliably clean the sample and this restricts the size of the balls that can be used. Now if points in the regions of high density are to survive then there must be k sample points in the small ball around any point in the true clusters and this gives us a lower bound on the necessary sample size.\nThe RSL algorithms work by counting the number of sample points inside the balls B(x, r) centered at the sample points x, for some radius r. In order for the algorithm to reliably resolve (\u03c3, \u01eb) clusters, it should distinguish points in the separator set S \u2282 M2 from those in the level \u03bb clusters M1\u222aM3. A necessary condition for this is that the mass of a ball B(x, r) for x \u2208 S\u03c3\u2212r should be strictly smaller than the mass inside B(y, r) for y \u2208 M1 \u222aM3. In Section 9.4, we show that this condition restricts the radius r to be at most O(\u03c4 \u221a \u01eb/d).\nNow, consider any sample point x0 in M1 \u222aM3 (such an x exists with high probability). Since x0 should not be removed during the cleaning step, the ball B(x0, r) must contain some other sample point (indeed, it must contain at least k\u2212 1 more sample points). By a union bound, this happens with probability at most\n(n\u2212 1)vdrd\u03bb \u2264 O(d\u2212d/2n\u03c4d\u01ebd/2\u03bb).\nIf we want the algorithm to succeed with probability at least 1/2 (say) then\nn \u2265 \u2126 ( dd/2\n\u03c4d\u03bb\u01ebd/2\n) .\n5. A modified algorithm for the known manifold case. In this section we consider the case when the manifold is known. In particular, we assume that we have an oracle that given as input a point x \u2208 M and a number V returns us a radius rx such that vold(BM (x, rx)) = V . We call the ball B(x, rx) the V -ball around x, and the oracle a V -ball oracle.\nGiven access to the V -ball oracle we show that a modified spatially adaptive RSL algorithm achieves the rate\nn \u2265 O (\n1\n\u03bbvd\u03c1d\u01eb2 log\n1\n\u03bbvd\u03c1d\u01eb2\n)\nwhere \u03c1 ..= min { \u03c3 10 , \u03c4 16 }\nIn particular, \u03c1 no longer depends on \u01eb\u03c4 and for the case of \u03c4 fixed (ignoring constants depending on d) the algorithm achieves the near minimax optimal rates of [1], in the manifold setting with d replacing D.\nThe modified algorithm is in Figure 2 and it uses two parameters, k and V , to be specified shortly.\nWe begin with a preliminary lemma which is a straightforward consequence of Lemma 8.\nLemma 11. If V = vdr d, then rl \u2264 rx \u2264 ru, where\nrl := r\n( 1\u2212 6r\n\u03c4\n) and ru := r ( 1 + 6r\n\u03c4\n) .\nTheorem 12. There are universal constants C1 and C2 such that the following holds. For any \u03b4 > 0, 0 < \u01eb < 1/2, run the algorithm in Figure 2 on a sample X drawn from f , where the parameters are set according to the equations\nR = 4ru = r\n( 1 + 6r\n\u03c4\n) and k = C1 log 2(1/\u03b4)(\u00b5/\u01eb2).\nfor r defined by\nvdr d\u03bb =\nk n + C2 log(1/\u03b4) n\n\u221a k\u00b5.\nThen with probability at least 1\u2212\u03b4, C\u0302 is (\u03c3, \u01eb) consistent. In particular, the clusters containing A[X] and A\u2032[X], where A and A\u2032 are (\u03c3, \u01eb) separated, are internally connected and mutually disconnected in C(r) provided\n\u03bb \u2265 2 vd\u03c1d k n .\nProof. The theorem is a straightforward consequence of the following lemma.\nLemma 13 (Separation and Connectedness). For the parameter choices prescribed in the theorem, provided we satisfy the following\n5ru \u2264 \u03c3 and R \u2264 \u03c4/2\nV \u03bb \u2265 k n + C\u03b4 n\n\u221a k\u00b5\nV \u03bb(1\u2212 \u01eb) \u2264 k n \u2212 C\u03b4 n\n\u221a k\u00b5.\nthe following properties hold w.p. at least 1\u2212 \u03b4:\n1. All points in A\u03c3\u2212ru and A \u2032 \u03c3\u2212ru are kept, and all points in S\u03c3\u2212ru are removed. 2. The two point sets A[X] and A\u2032[X] are disconnected in the graph Gr,R. 3. A[X] and A\u2032[X] are internally connected.\nProof. The proof is similar to that of Theorem 6 and we only highlight the differences.\n1. The V -ball around any point x in the manifold has volume exactly V by definition, and hence part (1) is true under the good event described in Lemma 7. In particular notice that using V -balls removes the necessity for estimating the ball volumes. 2. We show part (2) by contradiction. Assume that the graph connects a pair of points from A and A\u2032. Then the connection step guarantees that every edge of the path from A to A\u2032 is of Euclidean distance \u2264 R \u2264 \u03c4/2, and hence geodesic distance \u2264 2R. Therefore, by part (1), there must be an edge of (geodesic) length 2(\u03c3 \u2212 ru). This gives us a contradiction, provided 2R \u2264 2(\u03c3 \u2212 ru). 3. For part (3) note that R = 4ru \u2265 4rx, and hence an R/4-ball around any net point in AM,R/4 contains at least one sample point. The rest of the proof is unchanged.\nAs in the proof of Theorem 6, we set the parameters according to k = C2\u03b4 (\u00b5/\u01eb 2), and\nvdr d\u03bb =\nk n + C\u03b4 n\n\u221a k\u00b5.\nBy our assumption on \u03c1 and \u03bb, we can see that r \u2264 \u03c1, and that\nru = r\n( 1 + 6r\n\u03c4\n) \u2264 \u03c1 ( 1 + 6\u03c1\n\u03c4\n) \u2264 2\u03c1.\nNow, setting R = 4ru, we find that the requirements R \u2264 \u03c4/2 and R + r \u2264 \u03c3 are automatically satisfied. Similarly, the final requirement\nvdr d\u03bb(1\u2212 \u01eb) \u2264 k n \u2212 C\u03b4 n\n\u221a k\u00b5\nis also satisfied because of our choices of r and k.\n6. Cluster tree recovery in the presence of noise. So far we have considered the problem of recovering the cluster tree given samples from a density supported on a lower dimensional manifold. In this section we extend these results to the more general situation when we have noisy samples concentrated near a lower dimensional manifold. Indeed it can be argued that the manifold + noise model is a natural and general model for high-dimensional data.\nIn the noisy setting, it is clear that we can infer the cluster tree of the noisy density in a straightforward way. A stronger requirement would be consistency with respect to the underlying latent sample. Following the literature on manifold estimation ([26, 25]) we consider two main noise models. For both of them, we specify a distribution Q for the noisy sample.\n1. Clutter Noise: We observe data Y1, . . . , Yn from the mixture\nQ := (1\u2212 \u03c0)U + \u03c0P\nwhere 0 < \u03c0 \u2264 1 and U is a uniform distribution on X .\nDenote the samples drawn from P in this mixture\nX = {X1, . . . ,Xm}.\nThe points drawn from U are called background clutter. In this case, we can show:\nTheorem 14. There are universal constants C1 and C2 such that the following holds. For any \u03b4 > 0, 0 < \u01eb < 1/2, run the algorithm in Figure 1 on a sample {Y1, . . . , Yn}, with parameters\nR := 4\u03c1 k := C1 log 2(1/\u03b4)(\u00b5/\u01eb2).\nThen with probability at least 1\u2212\u03b4, C\u0302 is (\u03c3, \u01eb) consistent. In particular, the clusters containing A[X] and A\u2032[X] are internally connected and mutually disconnected in C(r) for r defined by\n\u03c0vdr d\u03bb =\n1\n1\u2212 \u01eb/6\n( k\nn +\nC2 log(1/\u03b4)\nn\n\u221a k\u00b5\n)\nprovided\n\u03bb \u2265 max { 2\nvd\u03c1d k n , 2v d/D D (1\u2212 \u03c0)d/D vd\u01ebd/D\u03c0\n( k\nn\n)1\u2212d/D}\nwhere \u03c1 is now slightly modified (in constants), i.e., \u03c1 := min ( \u03c3 7 , \u01eb\u03c4 72d , \u03c4 24 ) .\n2. Additive Noise: The data are of the form Yi = Xi+\u03b7i where X1, . . . ,Xn \u223c P ,and \u03b71, . . . , \u03b7n are a sample from any bounded noise distribution \u03a6, with \u03b7i \u2208 B(0, \u03b8). Note that Q is the convolution of P and \u03a6, Q = P \u22c6 \u03a6.\nTheorem 15. There are universal constants C1 and C2 such that the following holds. For any \u03b4 > 0, 0 < \u01eb < 1/2, run the algorithm in Figure 1 on the sample {Y1, . . . , Yn} with parameters\nR := 5\u03c1 k := C1 log 2(1/\u03b4)(\u00b5/\u01eb2).\nThen with probability at least 1\u2212 \u03b4, C\u0302 is (\u03c3, \u01eb) consistent for \u03b8 \u2264 \u03c1\u01eb/24d. In particular, the clusters containing {Yi : Xi \u2208 A} and {Yi : Xi \u2208 A\u2032} are internally connected and mutually disconnected in C(r) for r defined by\nvdr d(1\u2212 \u01eb/12)(1 \u2212 \u01eb/6)\u03bb = k\nn + C\u03b4 n\n\u221a k\u00b5\nif\n\u03bb \u2265 2 vd\u03c1d k n\nand \u03b8 \u2264 \u03c1\u01eb/24d, where \u03c1 := min (\u03c3 7 , \u03c4 24 , \u01eb\u03c4 144d ) .\nThe proofs for both Theorems 14 and 15 appear in Section 9.5. Notice that in each case we receive samples from a full D-dimensional distribution but are still able to achieve rates independent of D because these distributions are concentrated around the lower dimensional M . For the clutter noise case we produce a tree that is consistent for samples drawn from P (which are exactly on M), while in the additive noise case we produce a tree on the observed Yis which is (\u03c3, \u01eb) consistent for the latent Xis (for \u03b8 small enough). It is worth noting that in the case of clutter noise we can still consistently recover the entire cluster tree. Intuitively, this is because the k-NN distances for points on M are much smaller than for clutter points that are far away from M . As a result the clutter noise only affects a vanishingly low level set of the cluster tree. In the case of additive noise with small variance, it is possible to recover well-separated clusters at ambient dimension independent rates. It is also possible to recover the cluster tree in the presence of general additive noise distributions via deconvolution [27, 26] but we do not pursue this approach here.\n7. Kernel Density Estimators. The results of the previous sections have used k-nearest neighbors based density estimators. However, similar (albeit non-algorithmic) results can be obtained for kernel density estimators.\nFor the full dimensional cases we consider the usual kernel density estimators\nf\u0302h(x) = 1\nnhD\nn\u2211\ni=1\nK\n( x\u2212Xi\nh\n) .\nFor the manifold case we consider the following estimator (notice that unlike the usual kernel density estimate it does not integrate to 1),\nf\u0302h(x) = 1\nnhd\nn\u2211\ni=1\nK\n( x\u2212Xi\nh\n) .\nIn each case, K : RD \u2192 R is a kernel. In each case, there is an associated population quantity that will be useful. In the full dimensional case\nfh(x) = 1\nhD EX\u223cfK\n( x\u2212X\nh\n)\nand in the manifold case\nfh(x) = 1\nhd EX\u223cfK\n( x\u2212X\nh\n)\nAs before C(f\u0302h) denotes the cluster tree of the kernel density estimate.\n7.1. Assumptions and preliminaries. We will make one of the following assumptions on the kernel:\nAssumption 1 (Bounded support).\n[1A] For the case of full-dimensional densities we will assume the kernel has bounded support and integrates to 1, i.e.\n{x : K(x) > 0} \u2286 B(0, 1) and \u222b\nx\u2208RD K(\u2016x\u2016) = 1\nFollowing [24], we will further assume that the class of functions\nF = { K ( x\u2212 \u25e6 h ) , x \u2208 RD, h > 0 }\nsatisfies, for some positive number A and v\nsup P\nN (Fh, L2(P ), \u01eb\u2016F\u2016L2(P )) \u2264 ( A\n\u01eb\n)v\nwhere N (T, d, \u01eb) denotes the \u01eb-covering number of the metric space (T, d), F is the envelope function of F and the supremum is taken over the set of all probability measures on RD. A and v are called the VC characteristics of the kernel.\n[1B] For the case of densities supported on lower-dimensional manifolds we will assume a particular form for the kernel\nK(x) = I(x \u2264 1)\nvd\nObserve that this kernel also satisfies the VC assumption above.\nThe first assumption is quite mild and can be further relaxed to include kernels with an appropriate tail decay, albeit at the cost of more complicated proofs. The second assumption allows us to avoid dealing with integrals over the manifold but can also be similarly relaxed.\nAssumption 2 (Bandwidth regularity: BR(m)). For some c > 0,\nhn \u0581 0, nhmn | log hn| \u2192 \u221e | log hn| log log n \u2192 \u221e and hmn \u2264 chm2n\nWe will first state two preliminary results showing the uniform consistency of the kernel density estimate.\nThe first Lemma appears in a similar form in [14] (Proposition 9) and is a modification of a result of [24] (Corollary 2.2). The proof is omitted.\nLemma 16 (Full dimensional density). Given n samples from a distribution which has a bounded density f with respect to the Lebesgue measure on RD\n1. For n \u2265 n0, where n0 is a constant depending only on the VC characteristics of K, \u2016K\u2016\u221e, \u2016K\u20162 and fmax, and fixed h \u2264 h0 depending only on \u2016K\u2016\u221e and fmax there is a constant C depending on K such that\nP ( \u2016f\u0302h \u2212 fh\u2016\u221e \u2265 C \u2032 \u00b7 C \u221a fmax log(1/h)\nnhD\n) \u2264 ( 1\nh\n)C\u2032\nfor any large enough constant C \u2032 depending on K and fmax of our choice. 2. For any sequence hn \u2264 h0 as before, satisfying Assumption 2, BR(D), for all n \u2265 n0 as before\nP ( \u2016f\u0302hn \u2212 fhn\u2016\u221e \u2265 C \u2032 \u00b7 C \u221a fmax log(1/hn)\nnhDn\n) \u2264 ( 1\nh\n)C\u2032\nFor the ball kernel of Assumption 1 a similar result holds for densities supported on a lower dimensional manifold.\nLemma 17 (Manifold case). Given n samples from a distribution supported on a smooth Riemannian manifold M with condition number at most 1/\u03c4 with bounded density f with respect to the uniform measure on M\n1. For n \u2265 n0, where n0 is a constant depending only on the VC characteristics of K, \u2016K\u2016\u221e, \u2016K\u20162 and \u2016f\u2016\u221e, and fixed h \u2264 min( \u03c48 , h0) where h0 depends only on \u2016K\u2016\u221e and \u2016f\u2016\u221e there is a constant C\u03b4 depending on \u03b4 and n0 such that\nP ( \u2016f\u0302h \u2212 fh\u2016\u221e \u2265 C \u2032 \u00b7 C \u221a fmax log(1/h)\nnhd\n) \u2264 ( 1\nh\n)C\u2032\n2. For any sequence hn \u2264 min( \u03c48 , h0) as before, satisfying Assumption 2, BR(d), for all n \u2265 n0 as before\nP ( \u2016f\u0302hn \u2212 fhn\u2016\u221e \u2265 C \u2032 \u00b7 C \u221a fmax log(1/hn)\nnhdn\n) \u2264 ( 1\nh\n)C\u2032\nProof. The proof follows along the lines of those in [14, 24]. The main modification to achieve d rates involves a more careful calculation of the variance.\nTo apply Talagrand\u2019s inequality in the proof of [24] we need to bound\nsup g\u2208F Varfg\nF is the set of kernel functions with various bandwidths, and centers anywhere on M .\nLet us show how to bound supg\u2208Fh Varfg for a single bandwidth h.\nVarX\u223cp\n( K ( x\u2212X\nh\n)) = EX [ K ( x\u2212X\nh\n) \u2212 EXK ( x\u2212X\nh\n)]2\n\u2264 [ EXK 2 ( x\u2212X\nh\n)]\n=\n\u222b X\u2208M K2 ( x\u2212X h ) f(X)dX\n\u2264 \u2016K\u20162\u221e \u222b I(X \u2208 B(x, h))f(X)dX\n\u2264 hdCd\u2016K\u20162\u221e\u2016f\u2016\u221e The last step follows if h \u2264 \u03c48 by the ball volume Lemma 8. Notice that the variance does not depend on x and so the bound holds uniformly over all x on M .\nReplacing this bound on the variance in the proof of [24] we obtain the desired result.\n7.2. Rates of convergence for the cluster tree. Our first result mirrors the main result of [1].\nTheorem 18 (Full dimensional cluster tree). There is a constant C\u03b4 depending on the VC characteristics of the kernel, \u2016K\u2016\u221e, \u2016K\u20162, \u2016f\u2016\u221e and \u03b4 such that the following holds with probability at least 1\u2212 \u03b4, C(p\u0302\u03c3) is (\u03c3, \u01eb) consistent for any pair of clusters A, A\u2032 at level at least \u03bb for\nn \u2265 C\u03b4 \u03c3D\u03bb2\u01eb2 log\n( 1\n\u03c3\n)\nNotice, in particular that while for the k-nearest neighbor based algorithm the choice of k depends on \u01eb for the kernel density estimate the optimal choice of bandwidth depends on \u03c3. Also notice unlike the result of [1] this result requires the density to be uniformly upper bounded.\nProof. To prove this theorem it suffices to show that the regions A and A\u2032 are internally connected and mutually separated.\nLet us first show that \u03c3-clusters A and A\u2032 (for any \u03bb, \u01eb > 0) are connected and separated in C(f\u03c3). Consider any point x \u2208 A \u222aA\u2032,\nf\u03c3(x) =\n\u222b\ny\u2208B(x,\u03c3) K ( y \u2212 x h ) f(y)dy \u2265 \u03bb \u222b y\u2208B(x,\u03c3) K ( y \u2212 x h ) dy \u2265 \u03bb\nSimilarly, we can see that for any point in the separator S, f\u03c3(x) < \u03bb(1\u2212\u01eb). In particular, \u03c3-clusters A and A\u2032 are distinguished in C(f\u03c3) at level \u03bb as desired.\nNow, we use Lemma 16. Notice for a constant C\u03b4\nn \u2265 C\u03b4 \u03c3D\u03bb2\u01eb2 log\n( 1\n\u03c3\n)\nwe have\n\u2016f\u0302\u03c3 \u2212 f\u03c3\u2016\u221e \u2264 \u03bb\u01eb\n2\nwith probability 1\u2212 \u03b4. Let E1 denote the event {\u2016f\u0302\u03c3 \u2212 f\u03c3\u2016\u221e \u2264 \u03bb\u01eb2 }.\nNow, let us consider the cluster tree of f\u0302\u03c3 at level \u03bb\u2212 \u03bb\u01eb2 . On E1, for any point x \u2208 A\u222aA\u2032 we know f\u03c3 \u2265 \u03bb and thus f\u0302\u03c3 \u2265 \u03bb\u2212 \u03bb\u01eb2 . Similarly for x \u2208 S we have f\u0302\u03c3 < \u03bb \u2212 \u03bb\u01eb2 . These together show that on E1 A and A\u2032 are distinguished in C(f\u0302\u03c3) at level \u03bb\u2212 \u03bb\u01eb2 . This establishes the theorem.\nTo establish Hartigan consistency we select a schedule hn satisfying Assumption 2. Under mild conditions connected components of any level set at \u03bb, are (\u03c3, \u01eb) separated for some \u03c3, \u01eb > 0 and are distinguished for n large enough.\nWe can similarly give a manifold version of this result. Define\n\u03c1 = min ( \u03c3, \u03c4\n8 , \u01eb\u03c4 72d\n)\nTheorem 19 (Cluster tree on manifolds). There is a constant C\u03b4 depending on the VC characteristics of the kernel, \u2016K\u2016\u221e, \u2016K\u20162, \u2016f\u2016\u221e and \u03b4 such that the following holds with probability at least 1\u2212 \u03b4, for all \u01eb \u2264 1/2 C(p\u0302\u03c1) is (\u03c3, \u01eb) consistent for any pair of clusters A, A\u2032 at level at least \u03bb for\nn \u2265 C\u03b4 \u03c1D\u03bb2\u01eb2 log\n( 1\n\u03c1\n)\nProof. Let us again consider f\u03c1. For any point x \u2208 A \u222aA\u2032,\nf\u03c1(x) = 1\nhd EX\u223cfK\n( x\u2212X\nh\n) = 1\nvd\u03c1d\n\u222b\nX\u2208BM (x,h) dX \u2265 \u03bb\n( 1\u2212 \u01eb\n6\n)\nwhere the second equality follows from the assumed form of the kernel, and the inequality follows from Lemma 8 under the assumption on \u03c1. Similarly, for any point in S we have\nf\u03c1(x) < \u03bb (1\u2212 \u01eb) ( 1 + \u01eb\n6\n)\nThe gap between these is at least \u03bb\u01eb/2, and hence A and \u2032 are distinguished in f\u03c1 at level \u03bb(1\u2212\u01eb/6).\nThe proof that these clusters are distinguished in f\u0302\u03c1 follows from an identical argument to the one in the proof of Theorem 18, replacing the use of Lemma 16 with Lemma 17.\n8. Simulations. Figure 3 depicts the results of simulations we performed to test our main theoretical predictions. For Figure 3(B) we sample data from a mixture distribution on a unit d-sphere. The mixture has 10 salient clusters (with a total mixture weight of 0.7) mixed with uniform samples on the sphere with mixture weight 0.3. Finally, we mix samples from this density withD-dimensional clutter noise with \u03c0 = 0.8. A sample is shown in Figure 3(A) for d = 2,D = 3 and n = 1000. For Figures 3(C)-(H) we simulate data from the lower bound instance described in Section 4.\nIn Figure 3(B), we plot the probability of successfully recovering the 10 clusters in the cluster tree as a function of sample size. The figure confirms that the sample size is independent of the ambient dimension D but (typically) gets worse with the manifold dimension d. In particular, the figure shows that for D = {20, 40, 60, 80, 100} (in the same color) sample complexities are nearly unchanged. Figures 3(C)-(H), shows the effect on sample size of (\u01eb, d) for the lower bound instance. Notice, that for a fixed \u01eb and n the probability of success decays rapidly with increasing d and that for a fixed d and n the probability of success grows with \u01eb, in agreement with our 1/\u01eb\u2126(d) prediction and in contrast to the 1/\u01eb2 scaling predicted by [1] for recovering a full-dimensional cluster tree.\n9. Additional proofs. In this section we first prove some technical lemmas before giving full proofs of various claims made in the paper.\n9.1. Volume estimates for small balls on manifolds.\nTheorem 20. If\nr \u2264 \u01eb\u03c4 12d\nfor 0 \u2264 \u01eb < 1 then vdr d (1\u2212 \u01eb) \u2264 vol(S) \u2264 vdrd (1 + \u01eb)\nProof. The lower bound follows from [19] (Lemma 5.3) who show that\nLemma 21. For r < \u03c42\nvol(S) \u2265 ( 1\u2212 r 2\n4\u03c42\n)d/2 vdr d\nThe upper bound follows from [23] who shows that\nLemma 22. For r < \u03c42\nvol(S) \u2264 vd ( \u03c4\n\u03c4 \u2212 2\u03b1\n)d \u03b1d\nwhere\n\u03b1 = \u03c4 \u2212 \u03c4 \u221a\n1\u2212 2r \u03c4\nTo produce the result of the theorem we will need some careful manipulation of these two lemmas. In particular, we need the following estimates\nLemma 23. f(x) = (1\u2212 x)1/2 \u2265 1\u2212 x\n2 \u2212 x2\nif 0 \u2264 x \u2264 12 . f(x) = (1 + x)n \u2264 1 + 2nx\nif 0 \u2264 x \u2264 12n . f(x) = (1\u2212 x)\u22121 \u2264 1 + 2x\nif 0 \u2264 x \u2264 1/2 f(x) = (1\u2212 x)n \u2265 1\u2212 2nx\nif 0 \u2264 x \u2264 12n\nThe proof of this lemma is straightforward based on approximations via Taylor\u2019s series and we omit them.\nUsing Lemma 23 we have\n\u03b1 \u2264 r ( 1 + 4r\n\u03c4\n)\nif r \u2264 \u03c44 . Now, using this also notice that\n\u03c4 \u03c4 \u2212 2\u03b1 \u2264 1\n1\u2212 2r\u03c4 ( 1 + 4r\u03c4\n) \u2264 1 + 4r \u03c4\n( 1 + 4r\n\u03c4\n)\nwhere the second inequality follows from Lemma 23 if r \u2264 \u03c4/8.\nCombining these we have the following:\nFor all r \u2264 \u03c48 vdr d ( 1\u2212 r 2\n4\u03c42\n)d/2 \u2264 vol(S) \u2264 vdrd ( 1 + 6r\n\u03c4\n)d\nThe final result now follows another application of Lemma 23 on either side of this inequality.\n9.2. Bound on covering number. We need the following bound on the covering number of a manifold. See [19] (p. 16) for a proof.\nLemma 24. For s \u2264 2\u03c4 , the s-covering number of M is at most\nvold(M)\ncosd(arcsin(s/4\u03c4))vd(s/2)d \u2264 O\n( vold(M)c d\nvdsd\n)\nfor an absolute constant c. In particular, if vold(M) is bounded above by a constant, the s-covering number of M is at most O(cd/(vds d)).\nProof. We prove only the second claim. For s \u2264 2\u03c4 , we have arcsin(s/4\u03c4) \u2264 \u03c0/6, and hence cos(arcsin(s/4\u03c4)) \u2265 \u221a 3/2. Plugging this in the bound, we get\n|N | \u2264 vold(M)(2/ \u221a 3)d\nvd(s/2)d ,\nwhich gives the claim with c = 4/ \u221a 3.\n9.3. Uniform convergence. In this subsection, we prove uniform convergence for balls centered on sample and net points (Lemma 7). Consider the family of balls centered at a fixed point z, Bz := { B(z, s) : s \u2265 0 } . This collection has VC dimension 1. Thus with probability 1 \u2212 \u03b4\u2032, it holds that for every B \u2208 Bz, we have\nmax {P (B)\u2212 Pn(B)\u221a\nP (B) , P (B)\u2212 Pn(B)\u221a\nPn(B)\n} \u2264 2 \u221a log(2n) + log(4/\u03b4\u2032)\nn ,\nwhere P (B) is the true mass of B, and Pn(B) = |X \u2229 B|/n is its empirical measure. By a union bound over all z \u2208 N , setting \u03b4\u2032 := \u03b4/(2|N |), the following holds uniformly for every z \u2208 N and every B \u2208 Bz with probability 1\u2212 \u03b4/2:\nmax {P (B)\u2212 Pn(B)\u221a\nP (B) , P (B)\u2212 Pn(B)\u221a\nPn(B)\n} \u2264 2 \u221a\nlog(2n) + log(8|N |/\u03b4) n .\nTo provide a similar uniform convergence result for balls centered at a sample point Xi, we consider the (n\u22121)-subsampleXn\u22121i ofX obtained by deletingXi from the sample. Let Pn\u22121i be the empirical probability measure of this subsample:\nPn\u22121(B) := 1 n\u2212 1 \u2211\nj 6=i\nI[Xi \u2208 B].\nIt is easy to check that Pn\u22121 is uniformly close to Pn. In particular, for every set B containing Xi, we have\n(2) Pn\u22121(B) \u2264 Pn(B) \u2264 Pn\u22121(B) + 1\nn .\nNow, with probability at least 1\u2212 \u03b4/(2n), for any ball B centered at Xi,\nP (B)\u2212 Pn\u22121(B) \u2264 2 \u221a\nlog(2n\u2212 2) + log 8n/\u03b4 n\u2212 1 \u00b7 \u221a P (B),\nPn\u22121(B)\u2212 P (B) \u2264 2 \u221a\nlog(2n\u2212 2) + log 8n/\u03b4 n\u2212 1 \u00b7 \u221a Pn\u22121(B).\nUsing (2), we get\nP (B)\u2212 Pn(B) \u2264 2 \u221a\nlog(2n\u2212 2) + log 8n/\u03b4 n\u2212 1 \u00b7 \u221a P (B),\nPn(B)\u2212 P (B) \u2264 2 \u221a\nlog(2n\u2212 2) + log 8n/\u03b4 n\u2212 1 \u00b7 \u221a Pn(B) + 1 n .\nBy a union bound over all Xi \u2208 X, we get the claimed inequalities for all sample points with probability 1\u2212 \u03b4/2.\nPutting together our bounds for balls around sample and net points, with probability at least 1\u2212\u03b4, it holds that for all B \u2208 Bn,N , we have\nP (B)\u2212 Pn(B) \u2264 O (\u221a\u00b5+ log(1/\u03b4)\nn\n) \u00b7 \u221a P (B),\nPn(B)\u2212 P (B) \u2264 O (\u221a\u00b5+ log(1/\u03b4)\nn\n) \u00b7 \u221a Pn(B) + 1\nn .\nfor \u00b5 = 1 + log n+ log |N | = O(d) + log n+ d log(1/s) (using Lemma 24). The lemma now follows using simple manipulations of these inequalities (see [1] for details).\n9.4. Sketch of the lower bound instance. The following lemma gives an estimate of the volume of the intersection of a small ball with a sphere.\nLemma 25 (Volume of a spherical cap). Suppose Sd is a d-dimensional sphere of radius \u03c4 (embedded in Rd+1), and let x \u2208 Sd. Then, for small enough r, it holds that\nvold(B(x, r) \u2229 Sd) = vdrd ( 1\u2212 cd r2\n\u03c42 +Od ( r4 \u03c44\n))\nwhere cd := d(d\u22122) 8(d+2) . Note that c1 < 0, c2 = 0, and cd > 0 for all d \u2265 3.\nIn this section, we prove Lemma 25. The height h of the cap can be easily checked to be equal to h = r2/2\u03c4 . Now, the volume of the cap is given by the formula\nvcap = \u03c0(d+1)/2\u03c4d\n\u0393((d+ 1)/2) I\u03b1(d/2, 1/2)\nwhere the parameter \u03b1 is defined by\n\u03b1 := 2\u03c4h\u2212 h2\n\u03c4 =\nr2\n\u03c42 (1\u2212 r\n2\n4\u03c42 ).\nFurther I\u03b1(\u00b7, \u00b7) represents the incomplete beta function:\nI\u03b1(z, w) = B(\u03b1; z, w)\nB(z, w)\n=\n\u222b \u03b1 0 u\nz\u22121(1\u2212 u)w\u22121du B(z, w)\n= \u0393(z + w)\n\u0393(z)\u0393(w)\n\u222b \u03b1\n0 uz\u22121(1\u2212 u)w\u22121du\nThus,\nvcap = \u03c0(d+1)/2\u03c4d \u0393((d+ 1)/2) \u00b7 \u0393((d+ 1)/2)) \u0393(d/2)\u0393(1/2)\n\u00b7 \u222b \u03b1\n0 ud/2\u22121(1\u2212 u)\u22121/2du\n= \u03c0d/2\u03c4d\n\u0393(d/2)\n\u222b \u03b1\n0 ud/2\u22121(1\u2212 u)\u22121/2du\n= dvd\u03c4\nd\n2\n\u222b \u03b1\n0 ud/2\u22121(1\u2212 u)\u22121/2du.\nSince \u03b1 \u2192 0 as r \u2192 0, we can approximate the integral by expanding the integrand as a Taylor series around 0:\nvcap = dvd\u03c4\nd\n2\n\u222b \u03b1\n0 ud/2\u22121\n( 1 + u/2 +O(u2) ) du\n= dvd\u03c4\nd\n2\n( \u03b1d/2\nd/2 +\n1\n2\n\u03b1d/2+1\nd/2 + 1 +O(\u03b1d/2+2)\n)\n= vd\u03c4 d\u03b1d/2 ( 1 +\nd\n2(d+ 2) \u03b1+O(\u03b12))\n)\nFinally, using \u03b1 := r 2 \u03c42 (1\u2212 r2 \u03c42 ), we get\nvcap = vdr d ( 1\u2212 r 2\n4\u03c42\n)d/2 ( 1 +\ndr2\n2(d+ 2)\u03c42 +O ( r4 \u03c44\n))\n= vdr d \u00b7 ( 1\u2212 dr 2\n8\u03c42 +\ndr2\n2(d+ 2)\u03c42 +Od ( r4 \u03c44 )) ,\nwhich simplifies to the claimed estimate. We now show that it must be the case that r \u2264 O(\u03c4 \u221a\n\u01eb/d). We argued that for the algorithm to reliably resolve the (\u03c3, \u01eb) separated clusters M1 and M3, an r-ball around a sample point in S\u03c3\u2212r must have mass appreciably smaller than those around points in M1. By the previous lemma, the two kinds of balls have volumes\nvdr d ( 1\u2212 cd r2\n12 +Od (r4 14 )) = vdr d ( 1\u2212 cdr2 +Od(r4) )\nand\nvdr d ( 1\u2212 cd r2\n4\u03c42 +Od ( r4 16\u03c44 )) = vdr d ( 1\u2212 cd r2 4\u03c42 +Od ( r4 \u03c44 )) .\nThus we must have\nvdr dvdr d ( 1\u2212 cdr2 +Od(r4) ) \u00b7 \u03bb(1\u2212 \u01eb) \u2264 vdrd ( 1\u2212 cd r2\n4\u03c42 +Od ( r4 \u03c44 )) \u00b7 \u03bb.\nThis implies that r2 \u2264 O (\n4\u03c42\u01eb (1\u22124\u03c42)cd\n) . Hence if \u03c4 \u2264 1/4, we have r \u2264 \u03c4 \u221a \u01eb/cd. Plugging in cd = \u2126(d)\ngives us the claim.\n9.5. Clustering with noisy samples.\n9.6. Proof of Theorem 14. As before we begin by showing separation followed by a proof of connectivity. Recall that \u03c1 := min ( \u03c3 7 , \u01eb\u03c4 72d , \u03c4 24 ) .\nLemma 26 (Separation). Assume that we pick k, r and R to satisfy the conditions:\nr \u2264 \u03c1, R = 4\u03c1\n\u03c0 \u00b7 vdrd(1\u2212 \u01eb/6) \u00b7 \u03bb \u2265 k\nn + C\u03b4 n\n\u221a k\u00b5,\n\u03c0 \u00b7 vdrd(1 + \u01eb/6) \u00b7 \u03bb(1\u2212 \u01eb) + (1\u2212 \u03c0) \u00b7 vDrD \u2264 k n \u2212 C\u03b4 n\n\u221a k\u00b5.\nThen with probability 1\u2212 \u03b4, it holds that:\n1. All points in AM,\u03c3\u2212r and A \u2032 M,\u03c3\u2212r are kept, and all points in X \\Mr and S\u03c3\u2212r are removed.\nHere, Mr is the tubular region around M of width r. 2. The two point sets A[X] and A\u2032[X] are disconnected in the graph Gr,R.\nProof. The proof of the first claim is similar to the noiseless setting, except that the probability mass inside a ball now has contributions from both the manifold and the background clutter. For x \u2208 S\u03c3\u2212r, the probability mass of the ball B(x, r) underQ is at most \u03c0vdr\nd(1+\u01eb/6)\u00b7\u03bb(1\u2212\u01eb)+(1\u2212\u03c0)vDrD, which is at most kn \u2212 C\u03b4 n \u221a k\u00b5. Thus x is removed during the cleaning step. Similarly, if x /\u2208 Mr, the ball B(x, r) does not intersect the manifold, and hence its mass is at most (1\u2212 \u03c0)vDrD. Hence all points outside Mr are removed. Finally, if x \u2208 (AM,\u03c3\u2212r \u222a A\u2032M,\u03c3\u2212r) \u2229 X, then the mass of the ball BM (x, r) is at least vdr\nd(1 \u2212 \u01eb/6)\u03bb (ignoring the contribution of the noise). This is at least k n + C\u03b4 n \u221a k\u00b5, and hence x is kept.\nTo prove the second claim, suppose that sets A\u2229X and A\u2032 \u2229X are connected in Gr,R. Then there exists a sequence of sample points y0, y1, . . . , yt such that y0 \u2208 A, yt \u2208 A\u2032 and d(yi\u22121, yi) \u2264 R for all 1 \u2264 i \u2264 t. Let xi be the projection of yi on M , i.e., xi is the point of M closest to yi. We have already showed that each yi lies inside the tube Mr, so d(xi, yi) \u2264 r, and hence by triangle inequality, we have d(xi\u22121, xi) \u2264 R+2r \u2264 \u03c4/4. Hence, the geodesic distance between xi\u22121 and xi is < 2(R+ 2r). Now, by an argument analogous to the noiseless setting, there exists a pair (xi\u22121, xi) which are at a (geodesic) distance at least 2(\u03c3 \u2212 r). This is a contradiction since our parameter setting implies that 2(\u03c3 \u2212 r) \u2265 2(R + 2r).\nLemma 27 (Connectedness). Assume that the parameters k, r and R satisfy the separation conditions (in Lemma 26). Then, with probability at least 1\u2212 \u03b4, A \u2229Y is connected in Gr,R.\nProof. The proof of this lemma is identical to Lemma 10 and is omitted.\nWe now show how to pick the parameters to satisfy the conditions in Lemma 26. Set k := 144C2\u03b4 (\u00b5/\u01eb 2), and define r by\n\u03c0vdr d(1\u2212 \u01eb/6) \u00b7 \u03bb = k\nn + C\u03b4 n\n\u221a k\u00b5.\nIt is easy to check that this setting satisfies all our requirements, provided that the term (1\u2212\u03c0)vDrD arising from the clutter noise satisfies the additional constraint\n(1\u2212 \u03c0)vDrD \u2264 (\u01eb/2) \u00d7 \u03c0vdrd\u03bb.\nThe definition of r implies that r is upper bounded by (\n2k n\u03bb\u03c0vd\n)1/d . Thus it suffices to ensure that\n(1\u2212 \u03c0)vD ( 2k\nn\u03bb\u03c0vd\n)D/d \u2264 (\u01eb/2) \u00b7 2k\nn =\nk\u01eb n .\nThis is equivalent to the condition\n\u03bb \u2265 2v d/D D vd\u01ebd/D \u00b7 (1\u2212 \u03c0) d/D \u03c0 \u00b7 ( k n )1\u2212d/D ,\nwhich is assumed by Theorem 14.\n9.7. Proof of Theorem 15. Let P be a distribution on a manifold M with density f . Let X = (X1, . . . ,Xn) be the latent sample from P , and let Y = (Y1, . . . , Yn) be the observed sample. The only fact that we use about the observed sample is that it is close to the corresponding latent sample point: d(Yi,Xi) \u2264 \u03b8, where \u03b8 is the noise radius. We show that we can adapt the RSL algorithm to resolve (\u03c3, \u01eb) separated clusters (A,A\u2032), provided that \u03b8 is sufficiently small compared to both \u03c3 and \u01eb.\nAgain, we will pick values for k, r,R based on a parameter \u03c1, defined as \u03c1 := min(\u03c37 , \u03c4 24 , \u01eb\u03c4 144d ).\nLemma 28 (Separation). Suppose k, r,R are chosen to satisfy\n\u03b8 \u2264 r/2 r \u2264 \u03c1 R := 5\u03c1,\nvd(r \u2212 2\u03b8)d(1\u2212 \u01eb/6) \u00b7 \u03bb \u2265 k\nn + C\u03b4 n\n\u221a k\u00b5,\nvd(r + 2\u03b8) d(1 + \u01eb/6) \u00b7 \u03bb(1\u2212 \u01eb) \u2264 k n \u2212 C\u03b4 n\n\u221a k\u00b5,\nThen, with probability 1\u2212 \u03b4, the following holds uniformly over all (\u03c3, \u01eb) separated clusters (A,A\u2032):\n1. If a latent sample point Xi \u2208 AM,\u03c3\u2212r+2\u03b8 \u222aA\u2032M,\u03c3\u2212r+2\u03b8, then the corresponding sample point Yi is kept during the cleaning step. If Xi \u2208 SM,\u03c3\u2212r\u22122\u03b8, then Yi is removed.\n2. The sets {Yi : Xi \u2208 A} and {Yi : Xi \u2208 A\u2032} are disconnected in the graph Gr,R.\nProof. To prove the first claim, supposeXi \u2208 A\u03c3\u2212r+2\u03b8\u222aA\u2032\u03c3\u2212r+2\u03b8. Consider the ball BM (Xi, r\u22122\u03b8). It is completely inside AM,\u03c3 \u222a A\u2032M,\u03c3, hence the density f inside it is at least \u03bb. Moreover, if Xj is in BM (Xi, r \u2212 2\u03b8), then by triangle inequality, we have\nd(Yj , Yi) \u2264 d(Xj , Yj) + d(Xj ,Xi) + d(Yi,Xi) \u2264 r.\nHence the ball B(Xi, r) contains at least k sample points, provided BM (Xi, r\u22122\u03b8) contains at least k points from X. Finally, the true mass of the set BM (Xi, r \u2212 2\u03b8) is at least\nvd(r \u2212 2\u03b8)d(1\u2212 \u01eb/6) \u00b7 \u03bb \u2265 k\nn + C\u03b4 n\n\u221a k\u00b5.\nHence it contains at least k latent sample points, and we are done.\nSimilarly, suppose Xi \u2208 S\u03c3\u2212r\u22122\u03b8, and consider the ball BM (Xi, r + 2\u03b8). It is completely contained inside SM,\u03c3 and hence the density inside the ball is at most \u03bb(1\u2212 \u01eb). Moreover, if Xj is outside the set, then d(Yj , Yi) \u2265 d(Xj ,Xj)\u2212 d(Xi, Yi)\u2212 d(Xj , Yj) > r. Hence the ball B(Yi, r) contains fewer than k sample points, provided BM (Xi, r+2\u03b8) contains fewer than k points from X. The true mass of the ball BM (Xi, r + 2\u03b8) is at most\nvd(r + 2\u03b8) d(1 + \u01eb/6) \u00b7 \u03bb(1\u2212 \u01eb) \u2264 k n \u2212 C\u03b4 n\n\u221a k\u00b5.\nHence the ball contains fewer than k latent sample points, and we are done.\nWe now prove that the graph Gr,R is disconnected. Suppose not. Then there must exist a sequence of latent sample points x0, x1, . . . , xt \u2208 Y and a corresponding sequence of noisy sample points y0, . . . , yt \u2208 X such that x0 \u2208 A, xt \u2208 A\u2032, and d(yi\u22121, yi) \u2264 R. Clearly d(xi\u22121, xi) \u2264 R + 2\u03b8 \u2264 \u03c4/4. Thus the geodesic distance between xi\u22121 and xi is less than 2(R + 2\u03b8). However, by the (\u03c3, \u01eb) separation condition, we must have a successive pair (xi\u22121, xi) whose geodesic distance is at least 2(\u03c3\u2212r). This is a contradiction since we have set our parameters such that 2(\u03c3\u2212r) \u2265 2(R+2\u03b8).\nLemma 29 (Connectedness). Assume that the conditions of Lemma 28 are satisfied. Then, with probability at least 1 \u2212 \u03b4, the following holds uniformly over all A: if infx\u2208AM,\u03c3 f(x) \u2265 \u03bb, then {Yi : Xi \u2208 A} is connected in Gr,R.\nProof. The proof is similar to that of Lemma 10, so we indicate only the necessary modifications, omitting the details. We now use a net of radius (R \u2212 2\u03b8)/4, and the condition that R \u2265 4r is replaced by R \u2212 2\u03b8 \u2265 4r. Finally, the xi\u2019s defined in the proof are latent sample points, whereas the algorithm observes an arbitrary point yi in a \u03b8-ball around the xi. Thus, the distance between yi\u22121 and yi is at most\n4 \u00b7 R\u2212 2\u03b8 4 + d(yi, xi) + d(yi\u22121, xi\u22121) \u2264 R.\nIn order to satisfy the conditions stated in Lemma 28, we need the assumption that \u03b8 is small compared to r. More precisely, we will assume that \u03b8 \u2264 r\u01eb/24d. Under this assumption, we can satisfy the above conditions by ensuring that\nvdr d(1\u2212 \u01eb/12)(1 \u2212 \u01eb/6) \u00b7 \u03bb \u2265 k\nn + C\u03b4 n\n\u221a k\u00b5,\nvdr d(1 + \u01eb/6)(1 + \u01eb/6) \u00b7 \u03bb(1\u2212 \u01eb) \u2264 k n \u2212 C\u03b4 n\n\u221a k\u00b5\nAs before, we can satisfy these equations by setting k := O(C2\u03b4\u00b5/\u01eb 2), and r according to\nvdr d(1\u2212 \u01eb/12)(1 \u2212 \u01eb/6) \u00b7 \u03bb = k\nn + C\u03b4 n\n\u221a k\u00b5.\n9.8. Connection radius for polynomially bounded densities. In this section, we prove that in our algorithm (Figure 1), we can pick the connection radius R to be R := 4r, independent of the other parameters, provided that the density level satisfies \u03bb \u2264 nA for some absolute constant A. (Our original setting picked R = 4\u03c1 and r \u2264 \u03c1.)\nMore precisely, we will argue that the parameter \u00b5 in the algorithm can be safely replaced by a related parameter \u00b5\u0303 := 2A log n without affecting the performance of the algorithm. Pick k = O(C2\u03b4 \u00b5\u0303/\u01eb 2), and set r,R by the equations\nvdr d\u03bb =\n1\n1\u2212 \u01eb/6\n( k\nn +\nC2 log(1/\u03b4)\nn\n\u221a k\u00b5\u0303 ) ,\nR = 4r.\nThe crucial ingredient in the analysis of our algorithm is the uniform convergence property of balls centered at the sample points and net points (Lemma 7), so we first verify that this statement remains true. Note that by our choice of r, we have\nvdr d\u03bb \u2265 k n \u2265 1 n ,\nso that 1/rd \u2264 vdn\u03bb \u2264 vdnA+1 \u2264 nA+1 (since vd < 1 for sufficiently large d). As before, we consider a net N of radius R/4 (i.e., r); by Lemma 24, size of this net is at most cd/rd for some absolute constant c > 0. Thus by Lemma 7, we have the uniform convergence property, provided the parameter \u00b5 is replaced by\nlog n+ log |N | = log n+ log(1/rd) +O(1) = (A+ 2) log n+O(1).\nNotice that \u00b5\u0303 is picked to be a safe upper bound on this quantity, hence the lemma holds when \u00b5 is replaced by \u00b5\u0303.\nFinally, it is easy to check that our choice of parameters satisfies all the conditions given in the separation lemma. Hence the separation and connectedness guarantees (Lemmas 9 and 10), together with their proofs, remain unaffected.\n10. Discussion. In this paper we have shown that simple non-parametric estimators based on k nearest neighbors and kernel density estimates are manifold adaptive estimators of the cluster tree. We have also introduced the problem of cluster tree recovery in the presence of noise. Many open questions remain, particularly regarding the minimax optimal rates of convergence and rates of convergence in the tubular noise case which we hope to address in future work.\nOne of the main advantages of the k nearest neighbors based estimator is its easy computability. In the case of known manifolds we have shown a more general spatially adaptive algorithm achieves better rates and in current work we are trying to understand the extent to which spatially adaptive estimators can help when the manifold is unknown.\nFinally, simple modifications of these simple non-parametric estimators can also be used as estimators of various geometric properties of the level sets of the density. We are currently working on these extensions."}], "references": [{"title": "Rates of convergence for the cluster", "author": ["Kamalika Chaudhuri", "Sanjoy Dasgupta"], "venue": "tree. NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Consistency of single linkage for high-density clusters", "author": ["J.A. Hartigan"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1981}, {"title": "Mode analysis: a generalization of nearest neighbor which reduces chaining", "author": ["D. Wishart"], "venue": "In Proceedings of the Colloquium on Numerical Taxonomy held in the University of St. Andrews,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1969}, {"title": "A generalized single linkage method for estimating the cluster tree of a density", "author": ["W. Stuetzle", "Nugent. R"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Estimating the cluster tree of a density by analyzing the minimal spanning tree of a sample", "author": ["Werner Stuetzle"], "venue": "J. Classification,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Measuring mass concentrations and estimating density contour clusters: an excess mass approach", "author": ["W. Polonik"], "venue": "Annals of Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "On nonparametric estimation of density level", "author": ["A B Tsybakov"], "venue": "sets. Ann. Statist.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Granulometric smoothing", "author": ["G. Walther"], "venue": "Annals of Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "A plug-in approach to support estimation", "author": ["Antonio Cuevas", "Ricardo Fraiman"], "venue": "Annals of Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Rod\u0155\u0131guez-Casal. Plug-in estimation of general level", "author": ["Antonio Cuevas", "Wenceslao Gonz\u00e1lez-Manteiga", "Alberto"], "venue": "sets. Aust. N. Z. J. Stat.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Fast rates for plug-in estimators of density level", "author": ["P. Rigollet", "R. Vert"], "venue": "sets. Bernoulli,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Optimal construction of k-nearest-neighbor graphs for identifying noisy clusters", "author": ["Markus Maier", "Matthias Hein", "Ulrike von Luxburg"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Adaptive {H}ausdorff estimation of density level", "author": ["Aarti Singh", "Clayton Scott", "Robert Nowak"], "venue": "sets. Ann. Statist.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Generalized density clustering", "author": ["Alessandro Rinaldo", "Larry Wasserman"], "venue": "The Annals of Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Stability of density-based clustering", "author": ["Alessandro Rinaldo", "Aarti Singh", "Rebecca Nugent", "Larry Wasserman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Pruning nearest neighbor cluster trees", "author": ["Samory Kpotufe", "Ulrike von Luxburg"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Adaptive density level set clustering", "author": ["Ingo Steinwart"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Consistency and rates for clustering with dbscan", "author": ["Bharath K. Sriperumbudur", "Ingo Steinwart"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Finding the homology of submanifolds with high confidence from random", "author": ["P Niyogi", "S Smale", "S Weinberger"], "venue": "Discrete and Computational Geometry,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Random projection trees and low dimensional manifolds", "author": ["Sanjoy Dasgupta", "Yoav Freund"], "venue": "In STOC,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "A tree-based regressor that adapts to intrinsic dimension", "author": ["Samory Kpotufe", "Sanjoy Dasgupta"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Local polynomial regression on unknown manifolds", "author": ["Peter Bickel", "Bo Li"], "venue": "In Technical report, Department of Statistics, UC Berkeley,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "An upper bound for the volume of geodesic balls in submanifolds of euclidean spaces", "author": ["Frederic Chazal"], "venue": "Personal Communication, available at http://geometrica.saclay.inria.fr/team/Fred.Chazal/BallVolumeJan2013.pdf,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Rates of strong uniform consistency for multivariate kernel density estimators", "author": ["Evarist Gin\u00e9", "Armelle Guillou"], "venue": "In Annales de l\u2019Institut Henri Poincare (B) Probability and Statistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Minimax manifold estimation", "author": ["Christopher R. Genovese", "Marco Perone-Pacifico", "Isabella Verdinelli", "Larry Wasserman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Minimax rates for homology inference", "author": ["Sivaraman Balakrishnan", "Alessandro Rinaldo", "Don Sheehy", "Aarti Singh", "Larry Wasserman"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Empirical geometry of multivariate data: a deconvolution approach", "author": ["V.I. Koltchinskii"], "venue": "Ann. Statist.,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "We analyze a modified version of a k-nearest neighbor based algorithm recently proposed by Chaudhuri and Dasgupta [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "Hartigan [2] showed that the popular single-linkage algorithm is not consistent for a sample from R D, with D > 1.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "Recently, Chaudhuri and Dasgupta [1] analyzed an algorithm which is both simple and consistent.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "We show that the simple algorithm studied in [1] is consistent and has fast rates of convergence for data on or near a low dimensional manifold M .", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "We show that in the known manifold case a modified spatially adaptive algorithm achieves better rates, similar to the near minimax-optimal rates of [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 2, "context": "The idea of using probability density functions for clustering dates back to Wishart [3].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "[2] expanded on this idea and formalized the notions of high-density clustering, of the cluster tree and of consistency and fractional consistency of clustering algorithms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "In particular, [2] showed that single linkage clustering is consistent when D = 1 but is only fractionally consistent when D > 1.", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "[4] and [5] have also proposed procedures for recovering the cluster tree.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4] and [5] have also proposed procedures for recovering the cluster tree.", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "None of these procedures however, come with the theoretical guarantees given by [1], which demonstrated that a generalization of Wishart\u2019s algorithm allows one to estimate parts of the cluster tree for distributions with full-dimensional support near-optimally under rather mild assumptions.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": ", [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and references therein.", "startOffset": 2, "endOffset": 38}, {"referenceID": 6, "context": ", [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and references therein.", "startOffset": 2, "endOffset": 38}, {"referenceID": 7, "context": ", [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and references therein.", "startOffset": 2, "endOffset": 38}, {"referenceID": 8, "context": ", [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and references therein.", "startOffset": 2, "endOffset": 38}, {"referenceID": 9, "context": ", [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and references therein.", "startOffset": 2, "endOffset": 38}, {"referenceID": 10, "context": ", [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and references therein.", "startOffset": 2, "endOffset": 38}, {"referenceID": 11, "context": ", [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and references therein.", "startOffset": 2, "endOffset": 38}, {"referenceID": 12, "context": ", [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and references therein.", "startOffset": 2, "endOffset": 38}, {"referenceID": 13, "context": ", [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and references therein.", "startOffset": 2, "endOffset": 38}, {"referenceID": 14, "context": ", [6, 7, 8, 9, 10, 11, 12, 13, 14, 15], and references therein.", "startOffset": 2, "endOffset": 38}, {"referenceID": 15, "context": "Estimating the cluster tree has more recently been considered in [16] which also gives a simple pruning procedure for removing spurious clusters.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "[17, 18] propose procedures for determining recursively the lowest split in the cluster tree and give conditions for asymptotic consistency with minimal assumptions on the density.", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "[17, 18] propose procedures for determining recursively the lowest split in the cluster tree and give conditions for asymptotic consistency with minimal assumptions on the density.", "startOffset": 0, "endOffset": 8}, {"referenceID": 18, "context": "The condition number controls the curvature of M and prevents it from being too close to being self-intersecting (see [19] for a detailed treatment).", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "To give finite sample results, following [1], we define the notion of salient clusters.", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "Our definitions are slight modifications of those in [1] to take into account the manifold assumption.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "Chaudhuri and Dasgupta [1] analyze a robust single linkage (RSL) algorithm (in Figure 1).", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "[1] prove the following theorem, establishing finite sample bounds for a particular RSL algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Ignoring constants that depend on d the main difference between this result and the result of [1] (Theorem 5) is that our results only depend on the manifold dimension d and not the ambient dimension D (typically D \u226b d).", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "Another aspect is that our choice of the connection radius R depends on the (typically) unknown \u03c1, while for comparison, the connection radius in [1] is chosen to be \u221a 2r.", "startOffset": 146, "endOffset": 149}, {"referenceID": 0, "context": "It is easy to see that this theorem also establishes consistency for recovering the entire cluster tree by selecting an appropriate schedule on \u03c3n, \u01ebn and kn that ensures that all clusters are distinguished for n large enough (see [1] for a formal proof).", "startOffset": 231, "endOffset": 234}, {"referenceID": 0, "context": "Our proofs structurally mirror those in [1].", "startOffset": 40, "endOffset": 43}, {"referenceID": 19, "context": "Similar manifold adaptivity results have been shown in classification [20] and in non-parametric regression [21, 22].", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": "Similar manifold adaptivity results have been shown in classification [20] and in non-parametric regression [21, 22].", "startOffset": 108, "endOffset": 116}, {"referenceID": 21, "context": "Similar manifold adaptivity results have been shown in classification [20] and in non-parametric regression [21, 22].", "startOffset": 108, "endOffset": 116}, {"referenceID": 0, "context": "In the full dimensional setting of [1], this follows from standard VC inequalities.", "startOffset": 35, "endOffset": 38}, {"referenceID": 18, "context": "3 of [19] while the upper bound is based on a modification of the main result of [23].", "startOffset": 5, "endOffset": 9}, {"referenceID": 22, "context": "3 of [19] while the upper bound is based on a modification of the main result of [23].", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "The proof is analogous to the separation proof of [1] with several modifications.", "startOffset": 50, "endOffset": 53}, {"referenceID": 18, "context": "3 in [19] who show that if \u2016p\u2212 q\u2016 = R \u2264 \u03c4/2, then the geodesic distance dM (p, q) \u2264 \u03c4 \u2212 \u03c4 \u221a 1\u2212 2R \u03c4 .", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": ", a continuous map P : [0, 1] \u2192 A such that P (0) = y and P (1) = y\u2032.", "startOffset": 23, "endOffset": 29}, {"referenceID": 0, "context": "For full dimensional densities, [1] showed the information theoretic lower bound n = \u03a9 ( 1 \u03bb\u01ebvD\u03c3 log 1 \u03bb\u01ebvD\u03c3 ) .", "startOffset": 32, "endOffset": 35}, {"referenceID": 24, "context": "This can be fixed without affecting the essence of the construction by smoothing this intersection by rolling a ball of radius \u03c4 around it (a similar construction is made rigorous in Theorem 6 of [25]).", "startOffset": 196, "endOffset": 200}, {"referenceID": 0, "context": "In particular, \u03c1 no longer depends on \u01eb\u03c4 and for the case of \u03c4 fixed (ignoring constants depending on d) the algorithm achieves the near minimax optimal rates of [1], in the manifold setting with d replacing D.", "startOffset": 162, "endOffset": 165}, {"referenceID": 25, "context": "Following the literature on manifold estimation ([26, 25]) we consider two main noise models.", "startOffset": 49, "endOffset": 57}, {"referenceID": 24, "context": "Following the literature on manifold estimation ([26, 25]) we consider two main noise models.", "startOffset": 49, "endOffset": 57}, {"referenceID": 26, "context": "It is also possible to recover the cluster tree in the presence of general additive noise distributions via deconvolution [27, 26] but we do not pursue this approach here.", "startOffset": 122, "endOffset": 130}, {"referenceID": 25, "context": "It is also possible to recover the cluster tree in the presence of general additive noise distributions via deconvolution [27, 26] but we do not pursue this approach here.", "startOffset": 122, "endOffset": 130}, {"referenceID": 23, "context": "Following [24], we will further assume that the class of functions", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "The first Lemma appears in a similar form in [14] (Proposition 9) and is a modification of a result of [24] (Corollary 2.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "The first Lemma appears in a similar form in [14] (Proposition 9) and is a modification of a result of [24] (Corollary 2.", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "The proof follows along the lines of those in [14, 24].", "startOffset": 46, "endOffset": 54}, {"referenceID": 23, "context": "The proof follows along the lines of those in [14, 24].", "startOffset": 46, "endOffset": 54}, {"referenceID": 23, "context": "To apply Talagrand\u2019s inequality in the proof of [24] we need to bound", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "Replacing this bound on the variance in the proof of [24] we obtain the desired result.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "Our first result mirrors the main result of [1].", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "Also notice unlike the result of [1] this result requires the density to be uniformly upper bounded.", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "Notice, that for a fixed \u01eb and n the probability of success decays rapidly with increasing d and that for a fixed d and n the probability of success grows with \u01eb, in agreement with our 1/\u01eb\u03a9(d) prediction and in contrast to the 1/\u01eb2 scaling predicted by [1] for recovering a full-dimensional cluster tree.", "startOffset": 253, "endOffset": 256}, {"referenceID": 18, "context": "The lower bound follows from [19] (Lemma 5.", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "The upper bound follows from [23] who shows that", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "See [19] (p.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "The lemma now follows using simple manipulations of these inequalities (see [1] for details).", "startOffset": 76, "endOffset": 79}], "year": 2013, "abstractText": "In this paper we investigate the problem of estimating the cluster tree for a density f supported on or near a smooth d-dimensional manifold M isometrically embedded in R. We analyze a modified version of a k-nearest neighbor based algorithm recently proposed by Chaudhuri and Dasgupta [1]. The main results of this paper show that under mild assumptions on f and M , we obtain rates of convergence that depend on d only but not on the ambient dimension D. We also show that similar (albeit non-algorithmic) results can be obtained for kernel density estimators. We sketch a construction of a sample complexity lower bound instance for a natural class of manifold oblivious clustering algorithms. We further briefly consider the known manifold case and show that in this case a spatially adaptive algorithm achieves better rates.", "creator": "LaTeX with hyperref package"}}}