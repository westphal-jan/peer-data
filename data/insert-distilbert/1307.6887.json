{"id": "1307.6887", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jul-2013", "title": "Sequential Transfer in Multi-armed Bandit with Finite Set of Models", "abstract": "constantly learning from prior tasks forgetting and transferring that experience to improve future performance - is critical for building lifelong learning agents. although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer fatigue is focused on batch explicit learning tasks. in this paper we study primarily the problem of \\ textit { sequential transfer in online learning }, notably in the multi - armed bandit framework, : where \u00ab the fundamental objective is to minimize decreasing the cumulative regret over a sequence of tasks by incrementally transferring knowledge from prior tasks. we introduce a novel bandit algorithm based on a method - of - moments approach promising for the rapid estimation of the possible desired tasks and derive regret bounds for it.", "histories": [["v1", "Thu, 25 Jul 2013 22:17:12 GMT  (53kb)", "http://arxiv.org/abs/1307.6887v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["mohammad gheshlaghi azar", "alessandro lazaric", "emma brunskill"], "accepted": true, "id": "1307.6887"}, "pdf": {"name": "1307.6887.pdf", "metadata": {"source": "CRF", "title": "Sequential Transfer in Multi-armed Bandit with Finite Set of Models", "authors": ["Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill"], "emails": ["mazar@cs.cmu.edu", "alessandro.lazaric@inria.fr", "ebrun@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n30 7.\n68 87\nv1 [\nst at\n.M L\n] 2\n5 Ju"}, {"heading": "1 Introduction", "text": "Learning from prior tasks and transferring that experience to improve future performance is a key aspect of intelligence, and is critical for building lifelong learning agents. Recently, multi-task and transfer learning received much attention in the supervised and reinforcement learning (RL) setting with both empirical and theoretical encouraging results (see recent surveys by Pan and Yang, 2010; Lazaric, 2011). Most of these works focused on scenarios where the tasks are batch learning problems, in which a training set is directly provided to the learner. On the other hand, the online learning setting (Cesa-Bianchi and Lugosi, 2006), where the learner is presented with samples in a sequential fashion, has been rarely considered (see Mann and Choe (2012) for an example in RL and Sec. E in the supplementary material for a discussion on related settings).\nThe multi\u2013arm bandit (MAB) (Robbins, 1952) is a simple yet powerful framework formalizing the online learning with partial feedback problem, which encompasses a large number of applications, such as clinical trials, online advertisements and adaptive routing. In this paper we take a step towards understanding and providing formal bounds on transfer in stochastic MABs. We focus on a sequential transfer scenario where an (online) learner is acting in a series of tasks drawn from a stationary distribution over a finite set of MABs. Prior to learning, the model parameters of each bandit problem are not known to the learner, nor does it know the distribution probability over the bandit problems. Also, we assume that the learner is not provided with the identity of the task. This setting is sufficient to model a number of interesting problems, including: a tutoring system working to help a sequence of students to learn, by finding the right type of education program for them, where each student may be a remedial, normal or honors student but which is unknown; an online advertisement site that wishes to run a sequence of ads with maximum expected click for a sequence of webpages based on the type of the users of each webpage, which is unknown to the system. To act efficiently in this setting, it is crucial to define a mechanism for transferring knowledge across tasks. In fact the learner may encounter the same bandit problem over and over throughout the learning, and an efficient algorithm should be able to reuse (transfer) the knowledge obtained in previous tasks, when it is presented with the same problem again. This can be achieved by modeling the reward distribution of the whole process as a latent variable model (LVM), where the observed variables are the rewards of pulling the arms and the latent variable is the identity of the bandit. If we can accurately estimate this LVM, we show that an extension of the UCB algorithm (Auer et al., 2002) is able to exploit this prior knowledge to reduce the regret through tasks (Sec. 3).\nIn this paper we rely on a new variant of method-of-moments (Anandkumar et al., 2012a,c), the robust tensor power method (RTP) (Anandkumar et al., 2012b), to estimate the LVM associated with the sequential-bandit problem. RTP relies on decomposing the eigenvalues/eigenvectors of certain tensors for estimating the model means (Anandkumar et al., 2012b). We prove that RTP provides a consistent estimate of the means of all arms for every bandit problem as long as they are pulled at least three times per task (Sec. 4.2). This guarantees that once RTP is paired with an efficient bandit algorithm able to exploit the transferred\nknowledge about the models (Sec. 4.3), we obtain a bandit algorithm, called tUCB, guaranteed to perform as well as UCB in early episodes, thus avoiding any negative transfer effect, and then to approach the performance of the ideal case when the set of bandit problems is known in advance (Sec. 4.4). Finally, we report some preliminary results on synthetic data confirming the theoretical findings (Sec. 5)."}, {"heading": "2 Preliminaries", "text": "We consider a stochastic MAB problem defined by a set of arms A = {1, . . . ,K}, |A| = K , where each i \u2208 A is characterized by a distribution \u03bdi and the samples observed from each arm are independent and identically distributed. We focus on the setting where there exists a set of models \u0398 = {\u03b8 = (\u03bd1, . . . , \u03bdK)}, |\u0398| = m, which contains all the possible bandit problems. We denote the mean of an arm i, the best arm, and the best value of a model \u03b8 \u2208 \u0398 respectively by \u00b5i(\u03b8), i\u2217(\u03b8), \u00b5\u2217(\u03b8). We define the arm gap of an arm i for a model \u03b8 as \u2206i(\u03b8) = \u00b5\u2217(\u03b8) \u2212 \u00b5i, while the model gap for an arm i between two models \u03b8 and \u03b8\u2032 is defined as \u0393i(\u03b8, \u03b8\u2032) = |\u00b5i(\u03b8) \u2212 \u00b5i(\u03b8\u2032)|.\nWe also introduce some tensor notation. Let X \u2208 RK be a random realization of all the arms from a random model. All the realizations are i.i.d. conditional on a model \u03b8\u0304 and E[X |\u03b8 = \u03b8\u0304] = \u00b5(\u03b8), where the i-th component of \u00b5(\u03b8) \u2208 RK is [\u00b5(\u03b8)]i = \u00b5i(\u03b8). Given two realizations X1 and X2, we define the second moment matrix M2 = E[X1 \u2297 X2] such that [M2]i,j = E[X1i X 2 j ] and the third moment tensor M3 = E[X\n1\u2297X2\u2297X3]. Since the realizations are conditionally independent, we have that E[X1 \u2297 X2|\u03b8 = \u03b8\u0304] = E[X1|\u03b8 = \u03b8\u0304] \u2297 E[X2|\u03b8 = \u03b8\u0304] = \u00b5(\u03b8) \u2297 \u00b5(\u03b8) and this allows us to rewrite the second and third moments as M2 = \u2211 \u03b8 \u03c1(\u03b8)\u00b5(\u03b8) \u22972,M3 = \u2211 \u03b8 \u03c1(\u03b8)\u00b5(\u03b8) \u22973 (Anandkumar et al., 2012c), where v\u2297p = v \u2297 v \u2297 \u00b7 \u00b7 \u00b7 v is the p-th tensor power. Let A be a 3rd order member of the tensor product of the Euclidean space RK (as M3), then we define the multilinear map as follows. For a set of three matrices {Vi \u2208 RK\u00d7m}1\u2264i\u22643 , the (i1, i2, i3) entry in the 3-way array representation of A(V1, V2, V3) \u2208 Rm\u00d7m\u00d7m is [A(V1, V2, V3)]i1,i2,i3 := \u2211 1\u2264j1,j2,j3\u2264n\nAj1,j2,j3 [V1]j1,i1 [V2]j2,i2 [V3]j3,i3 . We also use different norms: the Euclidean norm \u2016 \u00b7 \u2016; the Frobenius norm \u2016 \u00b7 \u2016F ; the matrix max-norm \u2016A\u2016max = maxij |[A]ij |.\nWe consider the sequential transfer setting where at each episode j the learner interacts with a task \u03b8\u0304j , drawn from a distribution \u03c1 over \u0398, for n steps. The objective is to minimize the (pseudo-)regret over J episodes measured as the difference between the rewards obtained by the optimal arms i\u2217(\u03b8\u0304j) and the rewards achieved by the learner. More formally, the regret is defined as\nRJ = \u2211J\nj=1 Rjn = \u2211J j=1 \u2211 i6=i\u2217 T ji,n\u2206i(\u03b8\u0304 j), (1)\nwhere T ji,n is the number of pulls to arm i after n steps of episode j. The only information available to the learner is the number of models m, number of episodes J and number of steps n per task."}, {"heading": "3 Mult-armed Bandit with Finite Models", "text": "Before considering the transfer problem, we show that a simple variation to UCB allows to effectively exploit the knowledge of \u0398 and obtain a significant reduction in the regret. The mUCB (model-UCB) algorithm in Fig. 1 takes as input a set of models \u0398 including the current (unknown) model \u03b8\u0304. At each step t, the algorithm computes a subset \u0398t \u2286 \u0398 containing only the models whose means \u00b5i(\u03b8) are compatible with the current estimates \u00b5\u0302i,t of the means \u00b5i(\u03b8\u0304) of the current model, obtained averaging Ti,t pulls, and their uncer-\ntainty \u03b5i,t (see Eq. 2 for an explicit definition of this term). Notice that it is enough that one arm does not satisfy the compatibility condition to discard a model \u03b8. Among all the models in \u0398t, mUCB first selects the model with the largest optimal value and then it pulls its corresponding optimal arm. This choice is coherent with the optimism in the face of uncertainty principle used in UCB-based algorithms, since mUCB always pulls the optimal arm corresponding to the optimistic model compatible with the current estimates \u00b5\u0302i,t. We show that mUCB incurs a regret which is never worse than UCB and it is often significantly smaller.\nWe denote the set of arms which are optimal for at least a model in a set \u0398\u2032 as A\u2217(\u0398\u2032) = {i \u2208 A : \u2203\u03b8 \u2208 \u0398\u2032 : i\u2217(\u03b8) = i}. The set of models for which the arms in A\u2032 are optimal is \u0398(A\u2032) = {\u03b8 \u2208 \u0398 : \u2203i \u2208 A\u2032 : i\u2217(\u03b8) = i}. The set of optimistic models for a given model \u03b8\u0304 is \u0398+ = {\u03b8 \u2208 \u0398 : \u00b5\u2217(\u03b8) \u2265 \u00b5\u2217(\u03b8\u0304)}, and their corresponding optimal arms A+ = A\u2217(\u0398+). The following theorem bounds the expected regret (similar bounds hold in high probability). The lemmas and proofs (using standard tools from the bandit literature) are available in Sec. B of the supplementary material.\nTheorem 1. If mUCB is run with \u03b4 = 1/n, a set of m models \u0398 such that the \u03b8\u0304 \u2208 \u0398 and\n\u03b5i,t = \u221a log(mn2/\u03b4)/(2Ti,t\u22121), (2)\nwhere Ti,t\u22121 is the number of pulls to arm i at the beginning of step t, then its expected regret is\nE[Rn] \u2264 K + \u2211\ni\u2208A+\n2\u2206i(\u03b8\u0304) log ( mn3 )\nmin\u03b8\u2208\u0398+,i \u0393i(\u03b8, \u03b8\u0304) 2 \u2264 K + \u2211 i\u2208A+\n2 log ( mn3 )\nmin\u03b8\u2208\u0398+,i \u0393i(\u03b8, \u03b8\u0304) , (3)\nwhere A+ = A\u2217(\u0398+) is the set of arms which are optimal for at least one optimistic model\u0398+ and\u0398+,i = {\u03b8 \u2208 \u0398+ : i\u2217(\u03b8) = i} is the set of optimistic models for which i is the optimal arm.\nRemark (comparison to UCB). The UCB algorithm incurs a regret\nE[Rn(UCB)] \u2264 O (\u2211\ni\u2208A\nlogn\n\u2206i(\u03b8\u0304)\n) \u2264 O ( K logn\nmini\u2206i(\u03b8\u0304)\n) .\nWe see that mUCB displays two major improvements. The regret in Eq. 3 can be written as\nE[Rn(mUCB)] \u2264 O (\u2211\ni\u2208A+\nlogn\nmin\u03b8\u2208\u0398+,i \u0393i(\u03b8, \u03b8\u0304)\n) \u2264 O ( |A+|\nlogn\nmini min\u03b8\u2208\u0398+,i \u0393i(\u03b8, \u03b8\u0304)\n) .\nThis result suggests that mUCB tends to discard all the models in \u0398+ from the most optimistic down to the actual model \u03b8\u0304 which, with high-probability, is never discarded. As a result, even if other models are still in \u0398t, the optimal arm of \u03b8\u0304 is pulled until the end. This significantly reduces the set of arms which are actually pulled by mUCB and the previous bound only depend on the number of arms in A+, which is |A+| \u2264 |A\u2217(\u0398)| \u2264 K . Furthermore, it is possible to show that for all arms i, the minimum gap min\u03b8\u2208\u0398+,i \u0393i(\u03b8, \u03b8\u0304) is guaranteed to be larger than the arm gap \u2206i(\u03b8\u0304) (see Lem. 4 in Sec. B), thus further improving the performance of mUCB w.r.t. UCB."}, {"heading": "4 Online Transfer with Unknown Models", "text": "We now consider the case when the set of models is unknown and the regret is cumulated over multiple tasks drawn from \u03c1 (Eq. 1). We introduce tUCB (transfer-UCB) which transfers estimates of \u0398, whose accuracy is improved through episodes using a method-of-moments approach."}, {"heading": "4.1 The transfer-UCB Bandit Algorithm", "text": "Require: number of arms K, number of models m, constant C(\u03b8). Initialize estimated models \u03981 = {\u00b5\u03021i (\u03b8)}i,\u03b8 , samples R \u2208 RJ\u00d7K\u00d7n\nfor j = 1, 2, . . . , J do Run Rj = umUCB(\u0398j , n) Run \u0398j+1 = RTP(R,m,K, j, \u03b4) end for\nFigure 2: The tUCB algorithm.\nRequire: set of models \u0398j , num. steps n Pull each arm three times for t = 3K + 1, . . . , n do\nBuild \u0398jt = {\u03b8 : \u2200i, |\u00b5\u0302 j i (\u03b8)\u2212 \u00b5\u0302i,t| \u2264 \u03b5i,t + \u03b5 j} Compute Bjt (i; \u03b8) = min { (\u00b5\u0302ji (\u03b8) + \u03b5 j), (\u00b5\u0302i,t + \u03b5i,t) } Compute \u03b8jt = argmax\u03b8\u2208\u0398jt maxi B j t (i; \u03b8) Pull arm It = argmaxi B j t (i; \u03b8 j t )\nObserve sample R(It, Ti,t) = xIt and update end for return Samples R\nFigure 3: The umUCB algorithm.\nFig. 2 outlines the structure of our online transfer bandit algorithm tUCB (transfer-UCB). The algorithm uses two subalgorithms, the bandit algorithm umUCB (uncertain model-UCB), whose objective is to minimize the regret at each episode, and RTP (robust tensor power method) which at each episode j computes an estimate {\u00b5\u0302ji (\u03b8)} of the arm means of all the models. The bandit algorithm umUCB in Fig. 3 is an extension of the mUCB algorithm. It first computes a set of models \u0398jt whose means \u00b5\u0302i(\u03b8) are compatible with the current estimates \u00b5\u0302i,t. However, unlike the case where the exact models are available, here the models themselves are estimated and the uncertainty \u03b5j in their means (provided as input to umUCB) is taken into account in the definition of \u0398jt . Once the active set is computed, the algorithm computes an upper-confidence bound on the value of each arm i\nfor each model \u03b8 and returns the best arm for the most optimistic model. Unlike in mUCB, due to the uncertainty over the model estimates, a model \u03b8 might have more than one optimal arm, and an upper-confidence bound on the mean of the arms \u00b5\u0302i(\u03b8) + \u03b5j is used together with the upper-confidence bound \u00b5\u0302i,t+ \u03b5i,t, which is directly derived from the samples observed so far from arm i. This guarantees that the B-values are always consistent with the samples generated from the actual model \u03b8\u0304j . Once umUCB terminates, RTP (Fig. 4) updates the estimates of the model means \u00b5\u0302j(\u03b8) = {\u00b5\u0302ji (\u03b8)}i \u2208 RK using the samples obtained from each arm i. At the beginning of each task umUCB pulls all the arms 3 times, since RTP needs at least 3 samples from each arm to accurately estimate the 2nd and 3rd moments (Anandkumar et al., 2012b). More precisely, RTP uses all the reward samples generated up to episode j to estimate the 2nd and 3rd moments (see Sec. 2) as\nM\u03022 = j \u22121 \u2211j l=1 \u00b51l \u2297 \u00b52l, and M\u03023 = j\u22121 \u2211j l=1 \u00b51l \u2297 \u00b52l \u2297 \u00b53l, (4)\nwhere the vectors \u00b51l, \u00b52l, \u00b53l \u2208 RK are obtained by dividing the T li,n samples observed from arm i in episode l in three batches and taking their average (e.g., [\u00b51l]i is the average of the first T l i,n/3 samples). 1 Since \u00b51l, \u00b52l, \u00b53l are independent estimates of \u00b5(\u03b8\u0304l), M\u03022 and M\u03023 are consistent estimates of the second and third moments M2 and M3. RTP relies on the fact that the model means \u00b5(\u03b8) can be recovered from the spectral decomposition of the symmetric tensor T = M3(W,W,W ), where W is a whitening matrix for M2, i.e., M2(W,W ) = Im\u00d7m (see Sec. 2 for the definition of the mapping A(V1, V2, V3)). Anandkumar et al. (2012b) (Thm. 4.3) have shown that under some mild assumption (see later Assumption 1) the model means {\u00b5(\u03b8)}, can be obtained as \u00b5(\u03b8) = \u03bb(\u03b8)Bv(\u03b8), where (\u03bb(\u03b8), v(\u03b8)) is a pair of eigenvector/eigenvalue for the tensor T and B := (WT)+.Thus the RTP algorithm estimates the eigenvectors v\u0302(\u03b8) and the eigenvalues \u03bb\u0302(\u03b8), of the m\u00d7m\u00d7m tensor T\u0302 := M\u03023(W\u0302 , W\u0302 , W\u0302 ).2 Once v\u0302(\u03b8) and \u03bb\u0302(\u03b8) are computed, the estimated mean vector \u00b5\u0302j(\u03b8) is obtained by the inverse transformation \u00b5\u0302j(\u03b8) = \u03bb\u0302(\u03b8)B\u0302v\u0302(\u03b8), where B\u0302 is the pseudo inverse of W\u0302T(for a detailed description of RTP algorithm see Anandkumar et al., 2012b)."}, {"heading": "4.2 Sample Complexity of the Robust Tensor Power Method", "text": "umUCB requires as input \u03b5j , i.e., the uncertainty of the model estimates. Therefore we need finite sample complexity bounds on the accuracy of {\u00b5\u0302i(\u03b8)} computed by RTP. The performance of RTP is directly affected by the error of the estimates M\u03022 and M\u03023 w.r.t. the true moments. In Thm. 2 we prove that, as the number of tasks j grows, this error rapidly decreases with the rate of\u221a 1/j. This result provides us with an upper-bound on the error \u03b5j needed for building the confidence intervals in umUCB. The following definition and assumption are required for our result.\nDefinition 1. Let \u03a3M2 = {\u03c31, \u03c32, . . . , \u03c3m} be the set of m largest eigenvalues of the matrix M2. Define \u03c3min := min\u03c3\u2208\u03a3M2 \u03c3, \u03c3max := min\u03c3\u2208\u03a3M2 \u03c3 and \u03bbmax := max\u03b8 \u03bb(\u03b8). Define the minimum gap between the distinct eigenvalues of M2 as \u0393\u03c3 := min\u03c3i 6=\u03c3l(|\u03c3i \u2212 \u03c3l|).\nAssumption 1. The mean vectors {\u00b5(\u03b8)}\u03b8 are linear independent and \u03c1(\u03b8) > 0 for all \u03b8 \u2208 \u0398.\nWe now state our main result which is in the form of a high probability bound on the estimation error of mean reward vector of every model \u03b8 \u2208 \u0398.\n1Notice that 1/3([\u00b51l]i + [\u00b52l]i + [\u00b51l]i) = \u00b5\u0302 l i,n, the empirical mean of arm i at the end of episode l.\n2The matrix W\u0302 \u2208 RK\u00d7m is such that M\u03022(W\u0302 , W\u0302 ) = Im\u00d7m, i.e., W\u0302 is the whitening matrix of M\u03022. In general W\u0302 is not unique. Here, we choose W\u0302 = U\u0302D\u0302\u22121/2, where D\u0302 \u2208 Rm\u00d7m is a diagonal matrix consisting of the m largest eigenvalues of M\u03022 and U\u0302 \u2208 RK\u00d7m has the corresponding eigenvectors as its columns.\nTheorem 2. Pick \u03b4 \u2208 (0, 1). Let C(\u0398) := C3\u03bbmax \u221a\n\u03c3max \u03c33 min ( \u03c3max \u0393\u03c3 + 1\u03c3min + 1 \u03c3max ) , where C3 > 0 is a universal constant. Then\nunder Assumption 1 there exist constants C4 > 0 and a permutation \u03c0 on \u0398 such that after j tasks\nmax \u03b8\n\u2016\u00b5(\u03b8)\u2212 \u00b5\u0302j(\u03c0(\u03b8))\u2016 \u2264 C(\u0398)K2.5m2 \u221a log(K/\u03b4)\nj ,\nw.p. 1\u2212 \u03b4, given that j \u2265 C4m\n5K6 log(K/\u03b4) min(\u03c3min,\u0393\u03c3)2\u03c33min\u03bb 2 min . (5)\nRemark (comparison with the previous bounds). This bound improves on the previous bounds of Anandkumar et al. (2012c,a) moving from a dependency on the number of models of order O(m5) to a milder quadratic dependency on m.3 4 Although the dependency on \u03c3min is a bit worse in our bounds in comparison to those of Anandkumar et al. (2012c,a), here we have the advantage that there is no dependency on the smallest singular value of the matrix {\u00b5(\u03b8) : \u03b8 \u2208 \u0398}, whereas those results scale polynomially with this factor.\nRemark (computation of C(\u0398)). As illustrated in Fig. 3, umUCB relies on the estimates \u00b5\u0302j(\u03b8) and on their accuracy \u03b5j . Although the bound reported in Thm. 2 provides an upper confidence bound on the error of the estimates, it contains terms which are not computable in general (e.g., \u03c3min). In practice, C(\u0398) should be considered as a parameter of the algorithm.5 This is not dissimilar from to the parameter usually introduced in the definition of \u03b5i,t in front of the square-root term in UCB.\n4.3 Regret Analysis of umUCB We now analyze the regret of umUCB when an estimated set of models \u0398j is provided as input. At episode j, for each model \u03b8 we define the set of non-dominated arms (i.e., potentially optimal arms) as Aj\u2217(\u03b8) = {i \u2208 A : \u2204i\u2032, \u00b5\u0302ji (\u03b8)+\u03b5j < \u00b5\u0302ji\u2032(\u03b8)\u2212\u03b5j}. Among the non-dominated arms, when the actual model is \u03b8\u0304j , the set of optimistic arms is Aj+(\u03b8; \u03b8\u0304j) = {i \u2208 Aj\u2217(\u03b8) : \u00b5\u0302ji (\u03b8) + \u03b5j \u2265 \u00b5\u2217(\u03b8\u0304j)}. As a result, the set of optimistic models is \u0398j+(\u03b8\u0304j) = {\u03b8 \u2208 \u0398 : Aj+(\u03b8; \u03b8\u0304j) 6= \u2205}. In some cases, because of the uncertainty in the model estimates, unlike in mUCB, not all the models \u03b8 6= \u03b8\u0304j can be discarded, not even at the end of a very long episode. Among the optimistic models, the set of models that cannot be discarded is defined as \u0398\u0303j+(\u03b8\u0304\nj) = {\u03b8 \u2208 \u0398j+(\u03b8\u0304j) : \u2200i \u2208 Aj+(\u03b8; \u03b8\u0304j), |\u00b5\u0302ji (\u03b8)\u2212 \u00b5i(\u03b8\u0304j)| \u2264 \u03b5j}. Finally, when we want to apply the previous definitions to a set of models \u0398\u2032 instead of single model we have, e.g., Aj\u2217(\u0398\u2032; \u03b8\u0304j) = \u22c3 \u03b8\u2208\u0398\u2032 A j \u2217(\u03b8; \u03b8\u0304j).\nThe proof of the following results are available in Sec. D of the supplementary material, here we only report the number of pulls, and the corresponding regret bound.\nCorollary 1. If at episode j umUCB is run with \u03b5i,t as in Eq. 2 and \u03b5j as in Eq. 2 with a parameter \u03b4\u2032 = \u03b4/2K , then for any arm i \u2208 A, i 6= i\u2217(\u03b8\u0304j) is pulled Ti,n times such that\n \n\nTi,n \u2264 min\n{ 2 log ( 2mKn2/\u03b4 )\n\u2206i(\u03b8\u0304j)2 ,\nlog ( 2mKn2/\u03b4 )\n2min \u03b8\u2208\u0398\nj i,+\n(\u03b8\u0304j) \u0393\u0302i(\u03b8; \u03b8\u0304j)2\n} + 1 if i \u2208 Aj1\nTi,n \u2264 2 log ( 2mKn2/\u03b4 ) /(\u2206i(\u03b8\u0304\nj)2) + 1 if i \u2208 Aj2 Ti,n = 0 otherwise\nw.p. 1 \u2212 \u03b4, where \u0398ji,+(\u03b8\u0304j) = {\u03b8 \u2208 \u0398j+(\u03b8\u0304j) : i \u2208 A+(\u03b8; \u03b8\u0304j)} is the set of models for which i is among theirs optimistic non-dominated arms, \u0393\u0302i(\u03b8; \u03b8\u0304j) = \u0393i(\u03b8, \u03b8\u0304j)/2\u2212\u03b5j, Aj1 = Aj+(\u0398j+(\u03b8\u0304j); \u03b8\u0304j)\u2212Aj+(\u0398\u0303j+(\u03b8\u0304j); \u03b8\u0304j) (i.e., set of arms only proposed by models that can be discarded), and Aj2 = Aj+(\u0398\u0303j+(\u03b8\u0304j); \u03b8\u0304j) (i.e., set of arms only proposed by models that cannot be discarded).\nThe previous corollary states that arms which cannot be optimal for any optimistic model (i.e., the optimistic non-dominated arms) are never pulled by umUCB, which focuses only on arms in i \u2208 Aj+(\u0398j+(\u03b8\u0304j); \u03b8\u0304j). Among these arms, those that may help to remove a model from the active set (i.e., i \u2208 Aj1) are potentially pulled less than UCB, while the remaining arms, which are optimal for the models that cannot be discarded (i.e., i \u2208 Aj2), are simply pulled according to a UCB strategy. Similar to mUCB, umUCB first pulls the arms that are more optimistic until either the active set \u0398jt changes or they are no longer optimistic (because of the evidence from the actual samples). We are now ready to derive the per-episode regret of umUCB.\n3Note that the improvement is mainly due to accuracy of the orthogonal tensor decomposition obtained via the tensor power method relative to the previously cited works. This is a direct consequence of the perturbation bound of Anandkumar et al. (2012b, Thm. 5.1), which is at the core of our sample complexity bound.\n4The result of Anandkumar et al. (2012a) has the explicit dependency of order m3 on the number of model as well as implicit dependency of order m2\nthrough the parameter \u03b10. 5One may also estimate the constant C(\u0398) in an online fashion using doubling trick (Audibert et al., 2012).\nTheorem 3. If umUCB is run for n steps on the set of models \u0398j estimated by RTP after j episodes with \u03b4 = 1/n, and the actual model is \u03b8\u0304j , then its expected regret (w.r.t. the random realization in episode j and conditional on \u03b8\u0304j) is\nE[Rjn] \u2264 K+ \u2211\ni\u2208A j 1\nmin\n{ 2 log ( 2mKn3 )\n\u2206i(\u03b8\u0304j)2 ,\nlog ( 2mKn3 )\n2min \u03b8\u2208\u0398\nj i,+\n(\u03b8\u0304j) \u0393\u0302i(\u03b8; \u03b8\u0304j)2\n} \u2206i(\u03b8\u0304 j) + \u2211\ni\u2208A j 2\n2 log ( 2mKn3 )\n\u2206i(\u03b8\u0304j) .\nRemark (negative transfer). The transfer of knowledge introduces a bias in the learning process which is often beneficial. Nonetheless, in many cases transfer may result in a bias towards wrong solutions and a worse learning performance, a phenomenon often referred to as negative transfer. The first interesting aspect of the previous theorem is that umUCB is guaranteed to never perform worse than UCB itself. This implies that tUCB never suffers from negative transfer, even when the set \u0398j contains highly uncertain models and might bias umUCB to pull suboptimal arms. Remark (improvement over UCB). In Sec. 3 we showed that mUCB exploits the knowledge of \u0398 to focus on a restricted set of arms which are pulled less than UCB. In umUCB this improvement is not as clear, since the models in \u0398 are not known but are estimated online through episodes. Yet, similar to mUCB, umUCB has the two main sources of potential improvement w.r.t. to UCB. As illustrated by the regret bound in Thm. 3, umUCB focuses on arms in Aj1 \u222aAj2 which is potentially a smaller set than A. Furthermore, the number of pulls to arms in Aj1 is smaller than for UCB whenever the estimated model gap \u0393\u0302i(\u03b8; \u03b8\u0304j) is bigger than \u2206i(\u03b8\u0304j). Eventually, umUCB reaches the same performance (and improvement over UCB) as mUCB when j is big enough. In fact, the set of optimistic models reduces to the one used in mUCB (i.e., \u0398j+(\u03b8\u0304\nj) \u2261 \u0398+(\u03b8\u0304j)) and all the optimistic models have only optimal arms (i.e., for any \u03b8 \u2208 \u0398+ the set of non-dominated optimistic arms is A+(\u03b8; \u03b8\u0304j) = {i\u2217(\u03b8)}), which corresponds to Aj1 \u2261 A\u2217(\u0398+(\u03b8\u0304j)) and Aj2 \u2261 {i\u2217(\u03b8\u0304j)}, which matches the condition of mUCB. For instance, for any model \u03b8, to have A\u2217(\u03b8) = {i\u2217(\u03b8)} we need for any arm i 6= i\u2217(\u03b8) that \u00b5\u0302ji (\u03b8) + \u03b5j \u2264 \u00b5\u0302ji\u2217(\u03b8)(\u03b8)\u2212 \u03b5\nj . As a result j \u2265 2C(\u0398)/min \u03b8\u0304\u2208\u0398 min \u03b8\u2208\u0398+(\u03b8\u0304) mini\u2206i(\u03b8) 2 + 1\nepisodes are needed in order for all the optimistic models to have only one optimal arm independently from the actual identity of the model \u03b8\u0304j . Although this condition may seem restrictive, in practice umUCB starts improving over UCB much earlier, as illustrated in the numerical simulation in Sec. 5.\n4.4 Regret Analysis of tUCB Given the previous results, we derive the bound on the cumulative regret over J episodes (Eq. 1).\nTheorem 4. If tUCB is run over J episodes of n steps in which the tasks \u03b8\u0304j are drawn from a fixed distribution \u03c1 over a set of models \u0398, then its cumulative regret is\nRJ \u2264 JK + J\u2211\nj=1\n\u2211\ni\u2208A j 1\nmin\n{ 2 log ( 2mKn2/\u03b4 )\n\u2206i(\u03b8\u0304j)2 ,\nlog ( 2mKn2/\u03b4 )\n2 min \u03b8\u2208\u0398\nj i,+ (\u03b8\u0304j)\n\u0393\u0302ji (\u03b8; \u03b8\u0304 j)2\n} \u2206i(\u03b8\u0304 j) + J\u2211\nj=1\n\u2211\ni\u2208A j 2\n2 log ( 2mKn2/\u03b4 )\n\u2206i(\u03b8\u0304j) ,\nw.p. 1\u2212 \u03b4 w.r.t. the randomization over tasks and the realizations of the arms in each episode.\nThis result immediately follows from Thm. 3 and it shows a linear dependency on the number of episodes J . This dependency is the price to pay for not knowing the identity of the current task \u03b8\u0304j . If the task was revealed at the beginning of the task, a bandit algorithm could simply cluster all the samples coming from the same task and incur a much smaller cumulative regret with a logarithmic dependency on episodes and steps, i.e., log(nJ). Nonetheless, as discussed in the previous section, the cumulative regret of tUCB is never worse than for UCB and as the number of tasks increases it approaches the performance of mUCB, which fully exploits the prior knowledge of \u0398."}, {"heading": "5 Numerical Simulations", "text": "In this section we report preliminary results of tUCB on synthetic data. The objective is to illustrate and support the previous theoretical findings. We define a set \u0398 of m = 5 MAB problems with K = 7 arms each, whose means {\u00b5i(\u03b8)}i,\u03b8 are reported in Fig. 5 (see Sect. F in the supplementary material for the actual values), where each model has a different color and squares correspond to optimal arms (e.g., arm 2 is optimal for model \u03b82). This set of models is chosen to be challenging and illustrate some interesting cases useful to understand the functioning of the algorithm.6 Models \u03b81 and \u03b82 only differ in their optimal arms and this makes it difficult to distinguish them. For arm 3 (which is optimal for model \u03b83 and thus potentially selected by mUCB), all the models share exactly the same mean value. This implies that no model can be discarded by pulling it. Although this might\n6Notice that although \u0398 satisfies Assumption 1, the smallest singular value \u03c3min = 0.0039 and \u0393\u03c3 = 0.0038, thus making the estimation of the models difficult.\n0 1000 2000 3000 4000 5000\n30\nNumber of Tasks (J)\nUCB UCB+ mUCB tUCB\nFigure 6: Complexity over tasks.\n0 5000 10000 15000 50\n100\n150\n200\n250\n300\n350\nNumber of Steps (n)\nR eg\nre t\nUCB UCB+ mUCB tUCB (J=1000) tUCB (J=2000) tUCB (J=5000)\nFigure 7: Regret of UCB, UCB+, mUCB, and tUCB (avg. over episodes) vs episode length. 0 1000 2000 3000 4000 5000\n60\n80\n100\n120\n140\n160\n180\n200\n220\n240\n260\nNumber of Tasks (J)\nP er\n\u2212 ep\nis od\ne R\neg re\nt\nUCB UCB+ mUCB tUCB\nFigure 8: Per-episode regret of tUCB.\nsuggest that mUCB gets stuck in pulling arm 3, we showed in Thm. 1 that this is not the case. Models \u03b81 and \u03b85 are challenging for UCB since they have small minimum gap. Only 5 out of the 7 arms are actually optimal for a model in \u0398. Thus, we also report the performance of UCB+ which, under the assumption that \u0398 is known, immediately discards all the arms which are not optimal (i /\u2208 A\u2217) and performs UCB on the remaining arms. The model distribution is uniform, i.e., \u03c1(\u03b8) = 1/m.\nBefore discussing the transfer results, we compare UCB, UCB+, and mUCB, to illustrate the advantage of the prior knowledge of \u0398 w.r.t. UCB. Fig. 7 reports the per-episode regret of the three algorithms for episodes of different length n (the performance of tUCB is discussed later). The results are averaged over all the models in \u0398 and over 200 runs each. All the algorithms use the same confidence bound \u03b5i,t. The performance of mUCB is significantly better than both UCB, and UCB+, thus showing that mUCB makes an efficient use of the prior of knowledge of \u0398. Furthermore, in Fig. 6 the horizontal lines correspond to the value of the regret bounds up to the n dependent terms and constants7 for the different models in \u0398 averaged w.r.t. \u03c1 for the three algorithms (the actual values for the different models are in the supplementary material). These values show that the improvement observed in practice is accurately predicated by the upper-bounds derived in Thm. 1.\nWe now move to analyze the performance of tUCB. In Fig. 8 we show how the per-episode regret changes through episodes for a transfer problem with J = 5000 tasks of length n = 5000. In tUCB we used \u03b5j as in Eq.2 with C(\u0398) = 2. As discussed in Thm. 3, UCB and mUCB define the boundaries of the performance of tUCB. In fact, at the beginning tUCB selects arms according to a UCB strategy, since no prior information about the models \u0398 is available. On the other hand, as more tasks are observed, tUCB is able to transfer the knowledge acquired through episodes and build an increasingly accurate estimate of the models, thus approaching the behavior of mUCB. This is also confirmed by Fig. 6 where we show how the complexity of tUCB changes through episodes. In both cases (regret and complexity) we see that tUCB does not reach the same performance of mUCB. This is due to the fact that some models have relatively small gaps and thus the number of episodes to have an accurate enough estimate of the models to reach the performance of mUCB is much larger than 5000 (see also the Remarks of Thm. 3). Since the final objective is to achieve a small global regret (Eq. 1), in Fig. 7 we report the cumulative regret averaged over the total number of\n7For instance, for UCB we compute \u2211\ni 1/\u2206i.\ntasks (J) for different values of J and n. Again, this graph shows that tUCB outperforms UCB and that it tends to approach the performance of mUCB as J increases, for any value of n."}, {"heading": "6 Conclusions and Open Questions", "text": "In this paper we introduce the transfer problem in the multi-armed bandit framework when a tasks are drawn from a finite set of bandit problems. We first introduced the bandit algorithm mUCB and we showed that it is able to fully exploit the prior knowledge on the set of bandit problems \u0398 and reduce the regret w.r.t. UCB. When the set of models is unknown we define a method-ofmoments variant (RTP) which consistently estimates the means of the models in \u0398 from the samples collected through episodes. This knowledge is then transferred to umUCB which never performs worse than UCB and tends to approach the performance of mUCB. For these algorithms we derive regret and sample complexity bounds, and we show preliminary numerical simulations. To the best of our knowledge, this is the first work studying the problem of transfer in multi-armed bandit and it opens a series of interesting questions.\nOptimality of mUCB. In some cases, mUCB may miss the opportunity to explore arms that could be useful in discarding models. For instance, an arm i /\u2208 A\u2217(\u0398) may correspond to very large gaps \u0393i(\u03b8, \u03b8\u0304) and few pulls to it, although leading to large regret, may be enough to discard many models, thus guaranteeing a very small regret in the following. This observation rises the question whether the optimistic approach in this case still guarantees an optimal tradeoff between exploration and exploitation. Since the focus of this paper is on transfer and mUCB is already guaranteed to perform better than UCB, we left this question for future work.\nOptimality of tUCB. At each episode, tUCB transfers the knowledge about \u0398 acquired from previous tasks to achieve a small per-episode regret using umUCB. Although this strategy guarantees that the per-episode regret of tUCB is never worse than UCB, it may not be the optimal strategy in terms of the cumulative regret through episodes. In fact, if J is large, it could be preferable to run a model identification algorithm instead of umUCB in earlier episodes so as to improve the quality of the estimates \u00b5\u0302i(\u03b8). Although such an algorithm would incur a much larger regret in earlier tasks (up to linear), it could approach the performance of mUCB in later episodes much faster than done by tUCB. This trade-off between identification of the models and transfer of knowledge resembles the exploration-exploitation trade-off in the single-task problem and it may suggest that different algorithms than tUCB are possible."}, {"heading": "A Table of Notation", "text": "Symbol Explanation\nA Set of arms \u0398 Set of models K Number of arms m Number of models J Number of episodes n Number of steps per episode t Time step \u03b8\u0304 Current model \u0398t Active set of models at time t \u03bdi Distribution of arm i\n\u00b5i(\u03b8) Mean of arm i for model \u03b8 \u00b5(\u03b8) Vector of means of model \u03b8 \u00b5\u0302i,t Estimate of \u00b5i(\u03b8\u0304) at time t \u00b5\u0302ji (\u03b8) Estimate of \u00b5i(\u03b8) by RTP for model \u03b8 and arm i at episode j \u00b5\u0302j(\u03b8) Estimate of \u00b5(\u03b8) by RTP for model \u03b8 at episode j \u0398j Estimated model of RTP after j episode \u03b5j Uncertainty of the estimated model by RTP at episode j \u03b5i,t Model uncertainty at time t \u03b4 Probability of failure i\u2217(\u03b8) Best arm of model \u03b8 \u00b5\u2217(\u03b8) Optimal value of model \u03b8 \u2206i(\u03b8) Arm gap of an arm i for a model \u03b8\n\u0393i(\u03b8, \u03b8 \u2032) Model gap for an arm i between two models \u03b8 and \u03b8\u2032\nM2 2 nd-order moment M3 3 rd-order moment\nM\u03022 Empirical 2nd-order moment M\u03023 Empirical 3rd-order moment \u2016 \u00b7 \u2016 Euclidean norm \u2016 \u00b7 \u2016F Frobenius norm \u2016 \u00b7 \u2016max Matrix max-norm RJ Pseudo-regret T ji,n The number of pulls to arm i after n steps of episode j A\u2217(\u0398 \u2032) Set of arms which are optimal for at least a model in a set \u0398\u2032\n\u0398(A\u2032) Set of models for which the arms in A\u2032 are optimal \u0398+ Set of optimistic models for a given model \u03b8\u0304 A+ Set of optimal arms corresponds to \u0398+ W Whitening matrix of M2 W\u0302 Empirical whitening matrix T M2 under the linear transformation W T\u0302 M\u03022 under the linear transformation W\u0302 D Diagonal matrix consisting of the m largest eigenvalues of M2 D\u0302 Diagonal matrix consisting of the m largest eigenvalues of M\u03022 U K \u00d7m matrix with the corresponding eigenvectors of D as its columns U\u0302 K \u00d7m matrix with the corresponding eigenvectors of D\u0302 as its columns\n\u03bb(\u03b8) Eigenvalue of T associated with \u03b8 v(\u03b8) Eigenvector of T associated with \u03b8 \u03bb\u0302(\u03b8) Eigenvalue of T\u0302 associated with \u03b8 v\u0302(\u03b8) Eigenvector of T\u0302 associated with \u03b8 \u03a3M2 Set of m largest eigenvalues of the matrix M2 \u03c3min Minimum eigenvalue of M2 among the m-largest \u03c3max Maximum eigenvalue of M2 \u03bbmax Maximum eigenvalue of T \u0393\u03c3 Minimum gap between the eigenvalues of M2\nC(\u0398) O ( \u03bbmax\n\u221a \u03c3max \u03c33 min ( \u03c3max \u0393\u03c3 + 1 \u03c3min + 1 \u03c3max\n))\n\u03c0(\u03b8) Permutation on \u03b8 Aj\u2217(\u03b8) Set of non-dominated arms for model \u03b8 at episode j \u0398\u0303j+ Set of models that cannot be discarded at episode j \u0398ji,+ Set of models for which i is among the optimistic non-dominated arms at episode j 11"}, {"heading": "B Proofs of Section 3", "text": "Lemma 1. mUCB never pulls arms which are not optimal for at least one model, that is \u2200i /\u2208 A\u2217(\u0398), Ti,n = 0 with probability 1. Notice also that |A\u2217(\u0398)| \u2264 |\u0398|.\nLemma 2. The actual model \u03b8\u0304 is never discarded with high-probability. Formally, the event E = {\u2200t = 1, . . . , n, \u03b8\u0304 \u2208 \u0398t} holds with probability P[E ] \u2265 1\u2212 \u03b4 if\n\u03b5i,t =\n\u221a 1\n2Ti,t\u22121 log\n( mn2\n\u03b4\n) ,\nwhere Ti,t\u22121 is the number of pulls to arm i at the beginning of step t and m = |\u0398|.\nIn the previous lemma we implicitly assumed that |\u0398| = m \u2264 K . In general, the best choice in the definition of \u03b5i,t has a logarithmic factor with min{|\u0398|,K}.\nLemma 3. On event E , all the arms i /\u2208 A\u2217(\u0398+), i.e., arms which are not optimal for any of the optimistic models, are never pulled, i.e., Ti,n = 0 with probability 1\u2212 \u03b4.\nThe previous lemma suggests that mUCB tends to discard all the models in \u0398+ from the most optimistic down to the actual model \u03b8\u0304 which, on event E , is never discarded. As a result, even if other models are still in \u0398t, the optimal arm of \u03b8\u0304 is pulled until the end. Finally, we show that the model gaps of interest (see Thm. 1) are always bigger than the arm gaps.\nLemma 4. For any model \u03b8 \u2208 \u0398+, \u0393i\u2217(\u03b8)(\u03b8, \u03b8\u0304) \u2265 \u2206i\u2217(\u03b8)(\u03b8\u0304).\nProof of Lem. 1. From the definition of the algorithm we notice that It can only correspond to the optimal arm i\u2217 of one model in the set \u0398t. Since \u0398t can at most contain all the models in \u0398, all the arms which are not optimal are never pulled.\nProof of Lem. 2. We compute the probability of the complementary event EC , that is that event on which there exist at least one step t = 1, . . . , n where the true model \u03b8\u0304 is not in \u0398t. By definition of \u0398t, we have that\nE = {\u2200t, \u03b8\u0304 \u2208 \u0398t} = {\u2200t, \u2200i \u2208 A, |\u00b5i \u2212 \u00b5\u0302i,t| \u2264 \u03b5i,t},\nthen\nP[EC ] = P[\u2203t, i, |\u00b5i \u2212 \u00b5\u0302i,t| \u2265 \u03b5i,t] \u2264 n\u2211\nt=1\n\u2211\ni\u2208A\nP[|\u00b5i \u2212 \u00b5\u0302i,t| \u2265 \u03b5i,t] = n\u2211\nt=1\n\u2211\ni\u2208A\u2217(\u0398)\nP[|\u00b5i \u2212 \u00b5\u0302i,t| \u2265 \u03b5i,t]\nwhere the upper-bounding is a simple union bound and the last passage comes from the fact that the probability for the arms which are never pulled is always 0 according to Lem. 1. At time t, \u00b5\u0302i,t is the empirical average of the Ti,t\u22121 samples observed from arm i up to the beginning of round t. We define the confidence \u03b5i,t as\n\u03b5i,t =\n\u221a 1\n2Ti,t\u22121 log ( |\u0398|n\u03b1 \u03b4 ) ,\nwhere \u03b4 \u2208 (0, 1) and \u03b1 is a constant chosen later. Since Ti,t\u22121 is a random variable, we need to take an additional union bound over Ti,t\u22121 = 1, . . . , t\u2212 1 thus obtaining\nP[EC ] \u2264 n\u2211\nt=1\n\u2211\ni\u2208A\u2217(\u0398)\nt\u22121\u2211\nTi,t\u22121=1\nP[|\u00b5i \u2212 \u00b5\u0302i,t| \u2265 \u03b5i,t]\n\u2264 n\u2211\nt=1\n\u2211\ni\u2208A\u2217(\u0398)\nt\u22121\u2211\nTi,t\u22121=1\n2 exp ( \u2212 2Ti,t\u22121\u03b52i,t ) \u2264 n(n\u2212 1) |A \u2217(\u0398)|\u03b4 |\u0398|n\u03b1 .\nSince |A\u2217(\u0398)| < |\u0398| (see Lem. 1) and by taking \u03b1 = 2 we finally have P[EC ] \u2264 \u03b4.\nProof of Lem. 3. On event E , \u0398t always contains the true model \u03b8\u0304, thus only models with larger optimal value could be selected as the optimistic model \u03b8t = argmax\u03b8\u2208\u0398t \u00b5\u2217(\u03b8), thus restricting the focus of the algorithm only to the models in \u0398+ and their respective optimal arms.\nProof of Lem. 4. By definition of \u0398+ we have \u00b5i\u2217(\u03b8)(\u03b8) = \u00b5\u2217(\u03b8) > \u00b5\u2217(\u03b8\u0304) and by definition of optimal arm we have \u00b5\u2217(\u03b8\u0304) > \u00b5i\u2217(\u03b8)(\u03b8\u0304), hence \u00b5\u2217(\u03b8) > \u00b5i\u2217(\u03b8)(\u03b8\u0304). Recalling the definition of model gap, we have \u0393i\u2217(\u03b8)(\u03b8) = |\u00b5i\u2217(\u03b8)(\u03b8) \u2212 \u00b5i\u2217(\u03b8)(\u03b8\u0304)| = \u00b5\u2217(\u03b8) \u2212 \u00b5i\u2217(\u03b8)(\u03b8\u0304), where we used the definition of \u00b5\u2217(\u03b8) and the previous inequality. Using the definition of arm gap \u2206i, we obtain\n\u0393i\u2217(\u03b8)(\u03b8, \u03b8\u0304) = \u00b5\u2217(\u03b8)\u2212 \u00b5i\u2217(\u03b8)(\u03b8\u0304) \u2265 \u00b5\u2217(\u03b8\u0304)\u2212 \u00b5i\u2217(\u03b8)(\u03b8\u0304) = \u2206i\u2217(\u03b8)(\u03b8\u0304), which proves the statement.\nProof of Thm. 1. We decompose the expected regret as\nE[Rn] = \u2211\ni\u2208A\n\u2206iE[Ti,n] = \u2211\ni\u2208A\u2217(\u0398)\n\u2206iE[Ti,n] \u2264 nP{EC}+ \u2211\ni\u2208A+\n\u2206iE[Ti,n|E ],\nwhere the refinement on the sum over arms follows from Lem. 1 and 3 and the high probability event E . In the following we drop the dependency on \u03b8\u0304 and we write \u00b5i(\u03b8\u0304) = \u00b5i.\nWe now bound the regret when the correct model is always included in \u0398t. On event E , only the restricted set of optimistic models \u0398+ = {\u03b8 \u2208 \u0398 : \u00b5\u2217(\u03b8) \u2265 \u00b5\u2217} is actually used by the algorithm. Thus we need to compute the number of pulls to the suboptimal arms before all the models in \u0398+ are discarded from \u0398t. We first compute the number of pulls to an arm i needed to discard a model \u03b8 on event E . We notice that\n\u03b8 \u2208 \u0398t \u21d4 {\u2200i \u2208 A, |\u00b5i(\u03b8)\u2212 \u00b5\u0302i,t| \u2264 \u03b5i,t}, which means that a model \u03b8 is included only when all its means are compatible with the current estimates. Since we consider event E , |\u00b5i \u2212 \u00b5\u0302i,t| \u2264 \u03b5i,t, thus \u03b8 \u2208 \u0398t only if for all i \u2208 A\n2\u03b5i,t \u2265 \u0393i(\u03b8, \u03b8\u0304), which corresponds to\nTi,t\u22121 \u2264 2\n\u0393i(\u03b8, \u03b8\u0304)2 log ( |\u0398|n2 \u03b4 ) , (6)\nwhich implies that if there exists at least one arm i for which at time t the number of pulls Ti,t exceeds the previous quantity, then \u2200s > t we have \u03b8 /\u2208 \u0398t (with probability P(E)). To obtain the final bound on the regret, we recall that the algorithm first selects an optimistic model \u03b8t and then it pulls the corresponding optimal arm until the optimistic model is not discarded. Thus we need to compute the number of times the optimal arm of the optimistic model is pulled before the model is discarded. More formally, since we know that on event E we have that Ti,n = 0 for all i /\u2208 A+, the constraints of type (6) could only be applied to the arms i \u2208 A+. Let t be the last time arm i is pulled, which coincides, by definition of the algorithm, with the last time any of the models in \u0398+,i = {\u03b8 \u2208 \u0398+ : i\u2217(\u03b8) = i} (i.e., the optimistic models recommending i as the optimal arm) is included in \u0398t. Then we have that Ti,t\u22121 = Ti,n \u2212 1 and the fact that i is pulled corresponds to the fact the a model \u03b8i \u2208 \u0398+,i is such that\n\u03b8i \u2208 \u0398t \u2227 \u2200\u03b8\u2032 \u2208 \u0398t, \u00b5\u2217(\u03b8i) > \u00b5\u2217(\u03b8\u2032), which implies that (see Eq. 6)\nTi,n \u2264 2\nmin\u03b8\u2208\u0398+,i \u0393i(\u03b8, \u03b8\u0304) 2 log ( |\u0398|n2 \u03b4 ) + 1. (7)\nwhere the minimum over \u0398+,i guarantees that all the optimistic models with optimal arm i are actually discarded. Grouping all the conditions, we obtain the expected regret\nE[Rn] \u2264 K + \u2211\ni\u2208A+\n2\u2206i(\u03b8\u0304)\nmin\u03b8\u2208\u0398+,i \u0393i(\u03b8, \u03b8\u0304) 2 log\n( |\u0398|n3 )\nwith \u03b4 = 1/n. Finally we can apply Lem. 4 which guarantees that for any \u03b8 \u2208 \u0398+,i the gaps \u0393i(\u03b8, \u03b8\u0304) \u2265 \u2206i(\u03b8\u0304) and obtain the final statement.\nRemark (proof). The proof of the theorem considers a worst case. In fact, while pulling the optimal arm of the optimistic model i\u2217(\u03b8t) we do not consider that the algorithm might actually discard other models, thus reducing \u0398t before the optimistic model is actually discarded. More formally, we assume that for any \u03b8 \u2208 \u0398t not in \u0398+,i the number of steps needed to be discarded by pulling i\u2217(\u03b8t) is larger than the number of pulls needed to discard \u03b8t itself, which corresponds to\nmin \u03b8\u2208\u0398+,i \u03932i (\u03b8, \u03b8\u0304) \u2265 max \u03b8\u2208\u0398+\n\u03b8/\u2208\u0398+,i\n\u03932i (\u03b8, \u03b8\u0304).\nWhenever this condition is not satisfied, the analysis is suboptimal since it does not fully exploit the structure of the problem and mUCB is expected to perform better than predicted by the bound.\nRemark (comparison to UCB with hypothesis testing). An alternative strategy is to pair UCB with hypothesis testing of fixed confidence \u03b4. Let \u0393min(\u03b8\u0304) = minimin\u03b8 \u0393i(\u03b8, \u03b8\u0304), if at time t there exists an arm i such that Ti,t > 2 log(2/\u03b4)\u03932min, then all the models \u03b8 6= \u03b8\u0304 can be discarded with probability 1 \u2212 \u03b4. Since from the point of view of the hypothesis testing the exploration strategy is unknown, we can only assume that after \u03c4 steps we have Ti,\u03c4 \u2265 \u03c4/K for at least one arm i. Thus after \u03c4 > 2K log(2/\u03b4)/\u03932min steps, the hypothesis testing returns a model \u03b8\u0302 which coincides with \u03b8\u0304 with probability 1 \u2212 \u03b4. If \u03c4 \u2264 n, from time \u03c4 on, the algorithm always pulls It = i\u2217(\u03b8\u0302) and incurs a zero regret with high probability. If we assume \u03c4 \u2264 n, the expected regret is\nE[Rn(UCB+Hyp)] \u2264 O (\u2211\ni\u2208A\nlogn\u03c4\n\u2206i\n) \u2264 O ( K logn\u03c4\n\u2206\n) .\nWe notice that this algorithm only has a mild improvement w.r.t. standard UCB. In fact, in UCB the big-O notation hides the constants corresponding to the exponent of n in the logarithmic term. This suggests that whenever \u03c4 is much smaller than n, then there might be a significant improvement. On the other hand, since \u03c4 has an inverse dependency w.r.t. \u0393min, it is very easy to build model sets \u0398 where \u0393min = 0 and obtain an algorithm with exactly the same performance as UCB.\nC Sample Complexity Analysis of RTP\nIn this section we provide the full sample complexity analysis of the RTP algorithm. In our analysis we rely on some results of Anandkumar et al. (2012b). Anandkumar et al. (2012b) have provided perturbation bounds on the error of the orthonormal eigenvectors v\u0302(\u03b8) and the corresponding eigenvalues \u03bb\u0302(\u03b8) in terms of the perturbation error of the transformed tensor \u01eb = \u2016T\u2212T\u0302\u2016 (see Anandkumar et al., 2012b, Thm 5.1). However, this result does not provide us with the sample complexity bound on the estimation error of model means. Here we complete their analysis by proving a sample complexity bound on the \u21132-norm of the estimation error of the means \u2016\u00b5(\u03b8)\u2212 \u00b5\u0302(\u03b8)\u2016.\nWe follow the following steps in our proof: (i) we bound the error \u01eb in terms of the estimation errors \u01eb2 := \u2016M\u03022 \u2212M2\u2016 and \u01eb3 := \u2016M\u03023 \u2212 M3\u2016 (Lem. 6). (ii) we prove high probability bounds on the error \u01eb2 and \u01eb3 using some standard concentration inequality results (Lem. 7). The bounds on the errors of the estimates v\u0302(\u03b8) and \u03bb\u0302(\u03b8) immediately follow from combining the results of Lem. 6, Lem. 7 and Thm. 5. (iii) Based on these bounds we then prove our main result by bounding the estimation error associated with the inverse transformation \u00b5\u0302(\u03b8) = \u03bb\u0302(\u03b8)B\u0302v\u0302(\u03b8) in high probability.\nWe begin by recalling the perturbation bound of Anandkumar et al. (2012b):\nTheorem 5 (Anandkumar et al., 2012b). Pick \u03b7 \u2208 (0, 1). Define W := UD\u22121/2, where D \u2208 Rm\u00d7m is the diagonal matrix of the m largest eigenvalues of M2 and U \u2208 RK\u00d7m is the matrix with the eigenvectors associated with the m largest eigenvalues of M2 as its columns. Then W is a linear mapping which satisfies WTM2W = I. Let T\u0302 = T + E \u2208 Rm\u00d7m\u00d7m, where the 3rd order moment tensor T = M3(W,W,W ) is symmetric and orthogonally decomposable in the form of \u2211 \u03b8\u2208\u0398 \u03bb(\u03b8)v(\u03b8)\n\u22973, where each \u03bb(\u03b8) > 0 and {v(\u03b8)}\u03b8 is an orthonormal basis. Define \u01eb := \u2016E\u2016 and \u03bbmax = max\u03b8 \u03bb(\u03b8). Then there exist some constants C1, C2 > 0, some polynomial function f(\u00b7), and a permutation \u03c0 on \u0398 such that the following holds w.p. 1\u2212 \u03b7\n\u2016v(\u03b8)\u2212 v\u0302(\u03c0(\u03b8))\u2016 \u2264 8\u01eb/\u03bb(\u03b8), |\u03bb(\u03b8) \u2212 \u03bb\u0302(\u03c0(\u03b8))| \u2264 5\u01eb,\nfor \u01eb \u2264 C1 \u03bbminm , L > log(1/\u03b7)f(k) and N \u2265 C2(log(k) + log log(\u03bbmax/\u01eb)), where N and L are the internal parameters of RTP algorithm.\nFor ease of exposition we consider the RTP algorithm in asymptotic case, i.e., N,L \u2192 \u221e and \u03b7 \u2248 1. We now prove bounds on the perturbation error \u01eb in terms of the estimation error \u01eb2 and \u01eb3. This requires bounding the error between W = UD\u22121/2 and W\u0302 = U\u0302D\u0302\u22121/2 using the following perturbation bounds on \u2016U \u2212 U\u0302\u2016, \u2016D\u0302\u22121/2 \u2212D\u22121/2\u2016 and \u2016D\u03021/2 \u2212D1/2\u2016. Lemma 5. Assume that \u01eb2 \u2264 1/2min(\u0393\u03c3, \u03c3min), then we have\n\u2016D\u0302\u22121/2 \u2212D\u22121/2\u2016 \u2264 2\u01eb2 (\u03c3min)3/2 , and \u2016D\u03021/2 \u2212D1/2\u2016 \u2264 \u01eb2 \u03c3max , and \u2016U\u0302 \u2212 U\u2016 \u2264 2 \u221a m\u01eb2 \u0393\u03c3 .\nProof. Here we just prove bounds on \u2016D\u0302\u22121/2 \u2212D\u22121/2\u2016 and \u2016U\u0302 \u2212U\u2016. The bound on \u2016D\u0302\u22121/2 \u2212D\u22121/2\u2016 can be proven using a similar argument to that used for bounding \u2016D\u03021/2 \u2212D1/2\u2016. Let \u03a3\u0302m = {\u03c3\u03021, \u03c3\u03022, . . . , \u03c3\u0302m} be the set of m largest eigenvalues of the matrix M\u03022. We have\n\u2016D\u0302\u22121/2 \u2212D\u22121/2\u2016 (1)= max 1\u2264i\u2264m \u2223\u2223\u2223\u2223 \u221a 1 \u03c3i \u2212 \u221a 1 \u03c3\u0302i \u2223\u2223\u2223\u2223 = max1\u2264i\u2264m\n  \u2223\u2223\u2223 1\u03c3i \u2212 1 \u03c3\u0302i \u2223\u2223\u2223 \u221a\n1 \u03c3i\n+ \u221a\n1 \u03c3\u0302i\n \n\u2264 max 1\u2264i\u2264m\n(\u221a \u03c3i \u2223\u2223\u2223\u2223 1\n\u03c3i \u2212 1 \u03c3\u0302i\n\u2223\u2223\u2223\u2223 )\n\u2264 max 1\u2264i\u2264m\n\u2223\u2223\u2223\u2223 \u03c3i \u2212 \u03c3\u0302i\u221a\n\u03c3i\u03c3\u0302i\n\u2223\u2223\u2223\u2223 (2)\n\u2264 \u01eb2\u221a \u03c3min(\u03c3min \u2212 \u01eb2)\n(3) \u2264 2\u01eb2 (\u03c3min)3/2 ,\nwhere in (1) we use the fact that the spectral norm of matrix is its largest singular value, which in case of a diagonal matrix coincides with its biggest element, in (2) we rely on the result of Weyl (see Stewart and Sun, 1990, Thm. 4.11, p. 204) for bounding the difference between \u03c3i and \u03c3\u0302i, and in (3) we make use of the assumption that \u01eb2 \u2264 1/2\u03c3min.\nIn the case of \u2016U \u2212 U\u0302\u2016 we rely on the perturbation bound of Wedin (1972). This result guarantees that for any positive definite matrix A the difference between the eigenvectors of A and the perturbed A\u0302 (also positive definite) is small whenever there is a minimum gap between the eigenvalues of A\u0302 and A. More precisely, for any positive definite matrix A and A\u0302 such that ||A\u2212 A\u0302|| \u2264 \u01ebA, let the minimum eigengap be \u0393A\u2194A\u0302 := minj 6=i |\u03c3i \u2212 \u03c3\u0302j |, then we have\n\u2016ui \u2212 u\u0302i\u2016 \u2264 \u01ebA\n\u0393A\u2194A\u0302 , (8)\nwhere (ui, \u03c3i) is an eigenvalue/vector pair for the matrix A. Based on this result we now bound the error \u2016U \u2212 U\u0302\u2016\n\u2016U \u2212 U\u0302\u2016 \u2264 \u2016U \u2212 U\u0302\u2016F \u2264 \u221a\u2211\ni\n\u2016ui \u2212 u\u0302i\u20162 (1) \u2264 \u221a m\u01eb2\n\u0393 M2\u2194M\u03022\n(2) \u2264 \u221a m\u01eb2\n\u0393\u03c3 \u2212 \u01eb2 (3) \u2264 2 \u221a m\u01eb2 \u0393\u03c3 ,\nwhere in (1) we rely on Eq. 8 and in (2) we rely on the definition of the gap as well as Weyl\u2019s inequality. Finally, in (3) We rely on the fact that \u01eb2 \u2264 1/2\u0393\u03c3 for bounding denominator from below.\nOur result also holds for those cases where the multiplicity of some of the eigenvalues are greater than 1. Note that for any eigenvalue \u03bb with multiplicity l the linear combination of the corresponding eigenvectors {v1, v2, . . . , vl} is also an eigenvector of the matrix. Therefore, in this case it suffices to bound the difference between the eigenspaces of two matrix. The result of Wedin (1972) again applies to this case and bounds the difference between the eigenspaces in terms of the perturbation \u01eb2 and \u0393\u03c3 .\nWe now bound \u01eb in terms of \u01eb2 and \u01eb3.\nLemma 6. Let \u00b5max := max\u03b8 \u2016\u00b5(\u03b8)\u2016, if \u01eb2 \u2264 1/2min(\u0393\u03c3, \u03c3min), then the estimation error \u01eb is bounded as\n\u01eb \u2264 ( m\n\u03c3min\n)3/2 ( 10\u01eb2 ( 1\n\u0393\u03c3 +\n1\n\u03c3min\n)( \u01eb3 + \u00b5 3 max ) + \u01eb3 ) .\nProof. Based on the definitions of T and T\u0302 we have\n\u01eb = \u2016T \u2212 T\u0302\u2016 = \u2016M3(W,W,W )\u2212 M\u03023(W\u0302 , W\u0302 , W\u0302 )\u2016 \u2264 \u2016M3(W,W,W )\u2212 M\u03023(W,W,W )\u2016+ \u2016M\u03023(W,W,W )\u2212 M\u03023(W,W, W\u0302 )\u2016 + \u2016M\u03023(W,W, W\u0302 )\u2212 M\u03023(W, W\u0302 , W\u0302 )\u2016+ \u2016M\u03023(W, W\u0302 , W\u0302 )\u2212 M\u03023(W\u0302 , W\u0302 , W\u0302 )\u2016\n= \u2016EM3(W,W,W )\u2016 + \u2016M\u03023(W,W,W \u2212 W\u0302 )\u2016+ \u2016M\u03023(W,W \u2212 W\u0302 , W\u0302 )\u2016 + \u2016M\u03023(W \u2212 W\u0302 , W\u0302 , W\u0302 )\u2016,\n(9)\nwhere EM3 = M3 \u2212 M\u03023. We now bound the terms in the r.h.s. of Eq. 9 in terms of \u01eb3 and \u01eb2. We begin by bounding \u2016EM3(W,W,W )\u2016:\n\u2016EM3(W,W,W )\u2016 \u2264 \u2016EM3\u2016\u2016W\u20163 \u2264 \u2016EM3\u2016\u2016U\u20163\u2016D\u22121\u20163/2 \u2264 \u2016EM3\u2016\u2016U\u20163F\u2016D\u22121\u20163/2\n(1) =\n( m\n\u03c3min\n)3/2 \u2016EM3\u2016 \u2264 ( m\n\u03c3min\n)3/2 \u01eb3,\n(10)\nwhere in (1) we use the fact that U is an orthonormal matrix and D is diagonal. In the case of \u2016M\u03023(W,W,W \u2212 W\u0302 )\u2016 we have\n\u2016M\u03023(W,W,W \u2212 W\u0302 )\u2016 \u2264 \u2016W\u20162\u2016W \u2212 W\u0302\u2016\u2016M\u03023\u2016 \u2264 \u2016W\u20162\u2016W \u2212 W\u0302\u2016(\u2016M\u03023 \u2212M3\u2016+ \u2016M3\u2016) (1)\n\u2264 \u2016W\u20162\u2016W \u2212 W\u0302\u2016(\u01eb3 + \u00b53max) \u2264 \u2016W\u20162\u2016UD\u22121/2 \u2212 U\u0302D\u0302\u22121/2\u2016(\u01eb3 + \u00b53max) \u2264 \u2016W\u20162(\u2016(U \u2212 U\u0302)D\u22121/2\u2016+ \u2016U\u0302(D\u0302\u22121/2 \u2212D\u22121/2)\u2016) ( \u01eb3 + \u00b5 3 max ) \u2264 \u2016W\u20162 ( \u2016U \u2212 U\u0302\u2016\u221a\n\u03c3min + \u221a m\u2016D\u0302\u22121/2 \u2212D\u22121/2\u2016\n) ( \u01eb3 + \u00b5 3 max ) .\nwhere in (1) we use the definition of M3 as a linear combination of the tensor product of the means \u00b5(\u03b8). This result combined with the result of Lem. 5 and the fact that \u2016W\u2016 \u2264 \u221a m/\u03c3min (see Eq. 10) implies that\n\u2016M\u03023(W,W,W \u2212 W\u0302 )\u2016 \u2264 m\n\u03c3min\n( 2 \u221a m\u01eb2\n\u0393\u03c3 \u221a \u03c3min\n+ 2 \u221a m\u01eb2\n(\u03c3min)3/2\n)( \u01eb3 + \u00b5 3 max )\n\u2264 2\u01eb2 ( m\n\u03c3min\n)3/2 ( 1\n\u0393\u03c3 +\n1\n\u03c3min\n)( \u01eb3 + \u00b5 3 max ) .\n(11)\nLikewise one can prove the following perturbation bounds for M\u03023(W,W \u2212 W\u0302 , W\u0302 ) and M\u03023(W,W \u2212 W\u0302 , W\u0302 ):\n\u2016M\u03023(W,W \u2212 W\u0302 , W\u0302 )\u2016 \u2264 2 \u221a 2\u01eb2\n( m\n\u03c3min\n)3/2 ( 1\n\u0393\u03c3 +\n1\n\u03c3min\n)( \u01eb3 + \u00b5 3 max )\n\u2016M\u03023(W \u2212 W\u0302 , W\u0302 , W\u0302 )\u2016 \u2264 4\u01eb2 ( m\n\u03c3min\n)3/2 ( 1\n\u0393\u03c3 +\n1\n\u03c3min\n)( \u01eb3 + \u00b5 3 max ) .\n(12)\nThe result then follows by plugging the bounds of Eq. 10, Eq. 11 and Eq. 12 into Eq. 9.\nWe now prove high-probability bounds on \u01eb3 and \u01eb2 when M2 and M3 are estimated by sampling.\nLemma 7. For any \u03b4 \u2208 (0, 1), if M\u03022 and M\u03023 are computed with samples from j episodes, then we that with probability 1\u2212 \u03b4:\n\u01eb3 \u2264 K1.5 \u221a 6 log(2K/\u03b4)\nj and \u01eb2 \u2264 2K\n\u221a log(2K/\u03b4)\nj .\nProof. Using some norm inequalities for the tensors we obtain\n\u01eb3 = \u2016M3 \u2212 M\u03023\u2016 \u2264 K1.5\u2016M3 \u2212 M\u03023\u2016max = K1.5max i,j,x |[M3]i,j,x \u2212 [M\u03023]i,j,x|.\nA similar argument leads to the bound of Kmaxi,j |[M2]i,j\u2212 [M\u03022]i,j | on \u01eb2. One can easily show that, for every 1 \u2264 i, j, x \u2264 K , the term [M3]i,j,x \u2212 [M\u03023]i,j,x and [M3]i,j,x \u2212 [M\u03023]i,j,x can be expressed as a sum of martingale differences with the maximum value 1/j. The result then follows by applying the Azuma\u2019s inequality (e.g., see Cesa-Bianchi and Lugosi, 2006, appendix, pg. 361) and taking the union bound.\nWe now draw our attention to the proof of our main result.\nProof of Thm. 2. We begin by deriving the condition of Eq. 5. The assumption on \u01eb2 in Lem. 6 and the result of Lem. 7 hold at the same time, w.p. 1\u2212 \u03b4, if the following inequality holds\n2K\n\u221a log(2K/\u03b4)\nj \u2264 1/2min(\u0393\u03c3, \u03c3min).\nBy solving the bound w.r.t. j we obtain\nj \u2265 16K 2 log(2K/\u03b4)\nmin(\u0393\u03c3, \u03c3min)2 . (13)\nA similar argument applies in the case of the assumption on \u01eb in Thm. 5. The results of Thm. 5 and Lem. 6 hold at the same time if we have\n\u03b5 \u2264 ( mK\n\u03c3min\n)3/2 ( 20\u01eb2 ( 1\n\u0393\u03c3 +\n1\n\u03c3min\n) + \u01eb3 ) \u2264 C1\n\u03bbmin m ,\nwhere in the first inequality we used that \u03b53 \u2264 K3/2 and \u00b53max \u2264 K3/2 by their respective definitions. This combined with high probability bounds of Lem. 7 on \u01eb1 and \u01eb2 implies\n( m\n\u03c3min\n)1.5 ( 20K2.5 \u221a log(4K/\u03b4)\nj\n( 1\n\u0393\u03c3 +\n1\n\u03c3min\n) +K1.5\n\u221a 6 log(4K/\u03b4)\nj\n) \u2264 C1\n\u03bbmin m .\nBy solving this bound w.r.t. j (and some simplifications) we obtain w.p. 1\u2212 \u03b4\nj \u2265 43 2m5K6 log(4K/\u03b4)\nC1\u03c33min\u03bb 2 min\n( 1\n\u0393\u03c3 +\n1\n\u03c3min\n)2 .\nCombining this result with that of Eq.13 and taking the union bound leads to the bound of Eq. 5 on the minimum number of samples.\nWe now draw our attention to the main result of the theorem. We begin by bounding \u2016\u00b5(\u03b8)\u2212 \u00b5\u0302(\u03c0(\u03b8))\u2016 in terms of estimation error term \u01eb3 and \u01eb2:\n\u2016\u00b5(\u03b8)\u2212 \u00b5\u0302(\u03c0(\u03b8))\u2016 = \u2016\u03bb(\u03b8)Bv(\u03b8) \u2212 \u03bb\u0302(\u03c0(\u03b8))B\u0302v\u0302(\u03c0(\u03b8))\u2016 \u2264\u2016(\u03bb(\u03c0(\u03b8)) \u2212 \u03bb\u0302(\u03b8))Bv(\u03c0(\u03b8))\u2016 + \u2016\u03bb\u0302(\u03b8)(B \u2212 B\u0302)v(\u03c0(\u03b8))\u2016 + \u2016\u03bb\u0302(\u03b8)B\u0302(v(\u03c0(\u03b8)) \u2212 v\u0302(\u03b8))\u2016 \u2264|\u03bb(\u03b8) \u2212 \u03bb\u0302(\u03c0(\u03b8))|\u2016B\u2016 + \u03bb\u0302(\u03c0(\u03b8))\u2016B \u2212 B\u0302\u2016+ \u03bb\u0302(\u03c0(\u03b8))\u2016B\u0302\u2016\u2016v(\u03b8)\u2212 v\u0302(\u03c0(\u03b8))\u2016,\n(14)\nwhere in the last line we rely on the fact that both v(\u03b8) and v\u0302(\u03c0(\u03b8)) are normalized vectors. We first bound the term \u2016B \u2212 B\u0302\u2016: \u2016B \u2212 B\u0302\u2016 = \u2016UD1/2 \u2212 U\u0302D\u03021/2\u2016 \u2264 \u2016(U \u2212 U\u0302)D1/2\u2016+ \u2016U\u0302(D1/2 \u2212 D\u03021/2)\u2016\n(1) \u2264 2 \u221a m\u01eb2\u03c3max \u0393\u03c3 + \u221a m\u01eb2 \u03c3max \u2264 \u221am\u01eb2 ( 2\u03c3max \u0393\u03c3 + 1 \u03c3max ) ,\nwhere in (1) we make use of the result of Lem. 5. Furthermore, we have\n\u2016B\u0302\u2016 = \u2016U\u0302D\u03021/2\u2016 \u2264 \u221a m\u03c3\u0302max \u2264 \u221a m(\u03c31/2max + \u01eb 1/2 2 ) \u2264 \u221a m(\u03c31/2max + \u03c3 1/2 min) \u2264 \u221a 2m\u03c3max,\nwhere we used the condition on \u01eb2. This combined with Eq.14 and the result of Thm 5 and Lem. 6 implies\n\u2016\u00b5(\u03c0(\u03b8)) \u2212 \u00b5\u0302(\u03b8)\u2016 (1)\n\u2264 5\u221am\u03c3max\u01eb+ \u221a m\u01eb2 (\u03bb(\u03b8) + \u01eb) ( 2\u03c3max \u0393\u03c3 + 1 \u03c3max ) + 8\u01eb \u03bb(\u03b8) \u221a 2m\u03c3max (\u03bb(\u03b8) + \u01eb)\n(2) \u2264 5\u221am\u03c3max\u01eb+ \u221a m\u01eb2 ( \u03bb(\u03b8) + 5C1\n\u03c3min m )(2\u03c3max \u0393\u03c3 + 1 \u03c3max ) + 8 \u221a 2m\u03c3max ( 1 + 5C1 \u03c3min m ) \u01eb\n\u2264 5\u221am\u03c3max ( m\n\u03c3min\n)3/2 ( 10\u01eb2 ( 1\n\u0393\u03c3 +\n1\n\u03c3min\n)( \u01eb3 + \u00b5 3 max ) + \u01eb3 )\n+ \u221a m\u01eb2 ( \u03bb(\u03b8) + 5C1\n\u03c3min m )(2\u03c3max \u0393\u03c3 + 1 \u03c3max )\n+ 8 \u221a 2m\u03c3max ( 1 +\n5C1 m\n)( m\n\u03c3min\n)3/2 ( 10\u01eb2 ( 1\n\u0393\u03c3 +\n1\n\u03c3min\n)( \u01eb3 + \u00b5 3 max ) + \u01eb3 ) .\nwhere in (1) we used ||B|| \u2264 \u221am\u03c3max, the bound on \u03bb\u0302(\u03c0(\u03b8)) \u2264 \u03bb(\u03b8) + 5\u01eb, \u2016v(\u03b8) \u2212 v\u0302(\u03c0(\u03b8))\u2016 \u2264 8\u01eb/\u03bb(\u03b8), in (2) we used \u03bb(\u03b8) = 1/ \u221a \u03c1(\u03b8) \u2265 1 and the condition that \u03b5 \u2264 5C1\u03c3min/m. The result then follows by combining this bound with the high probability bound of Lem. 7 and taking union bound as well as collecting the terms."}, {"heading": "D Proofs of Section 4.3", "text": "Lemma 8. At episode j, the arms i /\u2208 Aj\u2217(\u0398; \u03b8\u0304j) are never pulled, i.e., Ti,n = 0.\nLemma 9. If umUCB is run with\n\u03b5i,t =\n\u221a 1\n2Ti,t\u22121 log\n( 2mKn2\n\u03b4\n) , \u03b5j = C(\u0398)\n\u221a 1\nj log\n( 2mKJ\n\u03b4\n) , (15)\nwhere C(\u0398) is defined in Thm. 2, then the event E = E1\u2229E2 is such that P[E ] \u2265 1\u2212 \u03b4 where E1 = {\u2200\u03b8, t, i, |\u00b5\u0302i,t\u2212\u00b5i(\u03b8)| \u2264 \u03b5i,t} and E2 = {\u2200j, \u03b8, i, |\u00b5\u0302ji (\u03b8)\u2212 \u00b5i(\u03b8)| \u2264 \u03b5j}.\nNotice that the event E implies that for any episode j and step t, the actual model is always in the active set, i.e., \u03b8\u0304j \u2208 \u0398jt .\nLemma 10. At episode j, all the arms i /\u2208 Aj+(\u0398j+(\u03b8\u0304j); \u03b8\u0304j) are never pulled on event E , i.e., Ti,n = 0 with probability 1\u2212 \u03b4.\nLemma 11. At episode j, the arms i \u2208 Aj+(\u0398j+(\u03b8\u0304j); \u03b8\u0304j) are never pulled more than with a UCB strategy, i.e.,\nT ji,n \u2264 2\n\u2206i(\u03b8\u0304j)2 log\n( 2mKn2\n\u03b4\n) + 1, (16)\nwith probability 1\u2212 \u03b4.\nNotice that for UCB the logarithmic term in the previous statement would be log(Kn2/\u03b4) which would represent a negligible constant fraction improvement w.r.t. umUCB whenever the number of models is of the same order of the number of arms.\nLemma 12. At episode j, for any model \u03b8 \u2208 (\u0398j+(\u03b8\u0304j)\u2212 \u0398\u0303j(\u03b8\u0304j)) (i.e., an optimistic model that can be discarded), the number of pulls to any arm i \u2208 Aj+(\u03b8; \u03b8\u0304j) needed before discarding \u03b8 is\nT ji,n \u2264 1\n2 ( \u0393i(\u03b8, \u03b8\u0304j)/2\u2212 \u03b5j\n)2 log ( 2mKn2\n\u03b4\n) + 1, (17)\nwith probability 1\u2212 \u03b4.\nProof of Lem. 8. We first notice that the algorithm only pulls arms recommended by a model \u03b8 \u2208 \u0398jt . Let i\u0302\u2217(\u03b8) = argmaxi Bjt (i; \u03b8) with \u03b8 \u2208 \u0398jt , and i \u2208 Aj\u2217(\u03b8; \u03b8\u0304j). According to the selection process, we have\nBjt (i; \u03b8) < B j t (\u0302i\u2217; \u03b8).\nSince \u03b8 \u2208 \u0398jt we have that for any i, |\u00b5\u0302i,t\u2212\u00b5\u0302ji (\u03b8)| \u2264 \u03b5i,t+\u03b5j which leads to \u00b5\u0302ji (\u03b8)\u2212\u03b5j \u2264 \u00b5\u0302i,t+\u03b5i,t. Since \u00b5\u0302ji (\u03b8)\u2212\u03b5j \u2264 \u00b5\u0302ji (\u03b8)+\u03b5j , then we have that\n\u00b5\u0302ji (\u03b8) \u2212 \u03b5j \u2264 min{\u00b5\u0302i,t + \u03b5i,t, \u00b5\u0302ji (\u03b8) + \u03b5j} = Bjt (i; \u03b8).\nFurthermore from the definition of the B-values we deduce that\nBjt (\u0302i\u2217; \u03b8) \u2264 \u00b5\u0302ji\u0302\u2217(\u03b8) + \u03b5 j.\nBringing together the previous inequalities, we obtain\n\u00b5\u0302ji (\u03b8) \u2212 \u03b5j \u2264 \u00b5\u0302ji\u0302\u2217(\u03b8) + \u03b5 j .\nwhich is a contradiction with the definition of non-dominated arms Aj\u2217(\u0398; \u03b8\u0304j).\nProof of Lem. 9. The probability of E1 is computed in Lem. 2 with the difference that now we need an extra union bound over all the models and that the union bound over the arms cannot be restricted to the number of models. The probability of E2 follows from Thm. 2.\nProof of Lem. 10. We first recall that on event E , at any episode j, the actual model \u03b8\u0304j is always in the active set \u0398jt . If an arm i is pulled, then according to the selection strategy, there exists a model \u03b8 \u2208 \u0398t such that\nBjt (i; \u03b8) \u2265 Bjt (\u0302i\u2217(\u03b8\u0304j); \u03b8\u0304j).\nSince i\u0302\u2217(\u03b8\u0304j) = argmaxiB j t (i; \u03b8\u0304 j), then Bjt (\u0302i\u2217(\u03b8\u0304 j); \u03b8\u0304j) \u2265 Bjt (i\u2217(\u03b8\u0304j); \u03b8\u0304j) where i\u2217(\u03b8\u0304j) is the true optimal arm of \u03b8\u0304j . By definition of B(i; \u03b8), on event E we have that Bjt (i\u2217(\u03b8\u0304j); \u03b8\u0304j) \u2265 \u00b5\u2217(\u03b8\u0304j) and that Bjt (i; \u03b8) \u2264 \u00b5\u0302ji (\u03b8) + \u03b5j . Grouping these inequalities we obtain\n\u00b5\u0302ji (\u03b8) + \u03b5 j \u2265 \u00b5\u2217(\u03b8\u0304j),\nwhich, together with Lem. 8, implies that i \u2208 Aj+(\u03b8; \u03b8\u0304j) and that this set is not empty, which corresponds to \u03b8 \u2208 \u0398j+(\u03b8\u0304j).\nProof of Lem. 11. Let t be the last time arm i is pulled (Ti,t\u22121 = Ti,n + 1), then according to the selection strategy we have\nBjt (i; \u03b8 j t ) \u2265 Bjt (\u0302i\u2217(\u03b8\u0304j); \u03b8\u0304j) \u2265 Bjt (i\u2217; \u03b8\u0304j),\nwhere i\u2217 = i\u2217(\u03b8\u0304j). Using the definition of B, we have that on event E\nBjt (i\u2217(\u03b8\u0304 j); \u03b8\u0304j) = min { (\u00b5\u0302ji\u2217(\u03b8\u0304 j) + \u03b5j); (\u00b5\u0302i\u2217,t + \u03b5i\u2217,t) } \u2265 \u00b5\u2217(\u03b8\u0304j)\nand\nBjt (i; \u03b8 j t ) \u2264 \u00b5\u0302i,t + \u03b5i,t \u2264 \u00b5i(\u03b8\u0304j) + 2\u03b5i,t.\nBringing the two conditions together we have\n\u00b5i(\u03b8\u0304 j) + 2\u03b5i,t \u2265 \u00b5\u2217(\u03b8\u0304j) \u21d2 2\u03b5i,t \u2265 \u2206i(\u03b8\u0304j),\nwhich coincides with the (high-probability) bound on the number of pulls for i using a UCB algorithm and leads to the statement by definition of \u03b5i,t.\nProof of Lem. 12. According to Lem. 10, a model \u03b8 can only propose arms in Aj+(\u03b8; \u03b8\u0304j). Similar to the analysis of mUCB, \u03b8 is discarded from \u0398jt with high probability after t steps and j episodes if\n2(\u03b5i,t + \u03b5 j) \u2264 \u0393i(\u03b8, \u03b8\u0304j).\nAt round j, if \u03b5j \u2265 \u0393i(\u03b8, \u03b8\u0304j)/2 then the algorithm will never be able to pull i enough to discard \u03b8 (i.e., the uncertainty on \u03b8 is too large), but since i \u2208 Aj\u2217(\u03b8; \u03b8\u0304j), this corresponds to the case when \u03b8 \u2208 \u0398\u0303j(\u03b8\u0304j). Thus, the condition on the number of pulls to i is derived from the inequality\n\u03b5i,t \u2264 \u0393i(\u03b8, \u03b8\u0304j)/2\u2212 \u03b5j ."}, {"heading": "E Related Work", "text": "As discussed in the introduction, transfer in online learning has been rarely studied. In this section we review possible alternatives and a series of settings which are related to the problem we consider in this paper. Models estimation. Although in tUCB we use RTP for the estimation of the model means, a wide number of other algorithms could be used, in particular those based on the method of moments (MoM). Recently a great deal of progress has been made regarding the problem of parameter estimation in LVM based on the method of moments approach (MoM) (Anandkumar et al., 2012c,a,b). The main idea of MoM is to match the empirical moments of the data with the model parameters that give rise\nto nearly the same corresponding population quantities. In general, matching the model parameters to the observed moments may require solving systems of high-order polynomial equations which is often computationally prohibitive. However, for a rich class of LVMs, it is possible to efficiently estimate the parameters only based on the low-order moments (up to the third order) (Anandkumar et al., 2012c). Prior to RTP various scenarios for MoM are considered in the literature for different classes of LVMs using different linear algebra techniques to deal with the empirical moments Anandkumar et al. (2012c,a). The variant introduced in (Anandkumar et al., 2012c, Algorithm B) recovers the matrix of the means {\u00b5(\u03b8)} up to a permutation in columns without any knowledge of \u03c1. Also, theoretical guarantees in the form of sample complexity bounds with polynomial dependency on the parameters of interest have been provided for this algorithm. The excess correlation analysis (ECA) (Alg. 5 in Anandkumar et al. (2012a)) generalizes the idea of the MoM to the case that \u03c1 is not fixed anymore but sampled from some Dirichlet distribution. The parameters of this Dirichlet distribution is not to be known by the learner.8 In this case again we can apply a variant of MoM to recover the models. Online Multi-task. In the online multi-task learning the task change at each step (n = 1) but at the end of each step both the true label (in the case of online binary classification) and the identity of the task are revealed. A number of works (Dekel et al., 2006; Saha et al., 2011; Cavallanti et al., 2010; Lugosi et al., 2009) focused on this setting and showed how the samples coming from different tasks can be used to perform multi-task learning and improve the worst-case performance of an online learning algorithm compared to using all the samples separately. Contextual Bandit. In contextual bandit (e.g., see Agarwal et al., 2012; Langford and Zhang, 2007), at each step the learner observes a context xt and has to choose the arm which is best for the context. The contexts belong to an arbitrary (finite or continuous) space and are drawn from a stationary distribution. This scenario resembles our setting where tasks arrive in a sequence and are drawn from a \u03c1. The main difference is that in our setting the learner does not observe explicitly the context and it repeatedly interact with that context for n steps. Furthermore, in general in contextual bandits some similarity between contexts is used, while here the models are completely independent. Non-stationary Bandit. When the learning algorithm does not know when the actual change in the task happens, then the problem reduces to learning in a piece-wise stationary environment. Garivier and Moulines (2011) introduces a modified version of UCB using either a sliding window or discounting to track the changing distributions and they show, when optimally tuned w.r.t. the number of switches R, it achieves a (worst-case) expected regret of order O( \u221a TR) over a total number of steps T and R switches. Notice that this could be also considered as a partial transfer algorithm. Even in the case when the switch is directly observed, if T is too short to learn from scratch and to identify similarity with other previous tasks, one option is just to transfer the averages computed before the switch. This clearly introduces a transfer bias that could be smaller than the regret cumulated in the attempt of learning from scratch. This is not surprising since transfer is usually employed whenever the number of samples that can be collected from the task at hand is relatively small. If we applied this algorithm to our setting T = nJ and R = J , the corresponding performance would be O(J \u221a n), which matches the worst-case performance of UCB (and tUCB as well) on J tasks. This result is not surprising since the advantage of knowing the switching points (every n steps) could always be removed by carefully choosing the worst possible tasks. Nonetheless, whenever we are not facing a worst case, the non-stationary UCB would have a much worse performance than tUCB."}, {"heading": "F Numerical Simulations", "text": "In Table 1 we report the actual values of the means of the arms of the models in \u0398, while in Table 2 we compare the complexity of UCB, UCB+, and mUCB, for all the different models and on average. Finally, the graphs in Fig. 9 are an extension up to J = 10000 of the performance of tUCB for n = 5000 reported in the main text.\n8 We only need to know sum of the parameters of the Dirichlet distribution \u03b10.\n00.050.1 0\n5\n10\n15\n20\n25\n30\nModel error\nC om\npl ex\nity"}], "references": [{"title": "Contextual bandit learning with predictable rewards", "author": ["A. Agarwal", "M. Dud\u00edk", "S. Kale", "J. Langford", "R.E. Schapire"], "venue": "In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS\u201912)", "citeRegEx": "Agarwal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2012}, {"title": "A spectral algorithm for latent dirichlet allocation", "author": ["A. Anandkumar", "D.P. Foster", "D. Hsu", "S. Kakade", "Liu", "Y.-K"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models. CoRR, abs/1210.7559", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": null, "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A method of moments for mixture models and hidden markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "In Proceeding of the 25th Annual Conference on Learning Theory (COLT\u201912),", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Optimization for Machine Learning, chapter Bandit View on Noisy Optimization", "author": ["Audibert", "J.-Y", "S. Bubeck", "R. Munos"], "venue": null, "citeRegEx": "Audibert et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2012}, {"title": "Finite-time analysis of the multi-armed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Linear algorithms for online multitask classification", "author": ["G. Cavallanti", "N. Cesa-Bianchi", "C. Gentile"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cavallanti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cavallanti et al\\.", "year": 2010}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Online multitask learning", "author": ["O. Dekel", "P.M. Long", "Y. Singer"], "venue": "In Proceedings of the 19th Annual Conference on Learning Theory (COLT\u201906),", "citeRegEx": "Dekel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2006}, {"title": "On upper-confidence bound policies for switching bandit problems", "author": ["A. Garivier", "E. Moulines"], "venue": "In Proceedings of the 22nd international conference on Algorithmic learning theory,", "citeRegEx": "Garivier and Moulines,? \\Q2011\\E", "shortCiteRegEx": "Garivier and Moulines", "year": 2011}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["J. Langford", "T. Zhang"], "venue": "In Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "Langford and Zhang,? \\Q2007\\E", "shortCiteRegEx": "Langford and Zhang", "year": 2007}, {"title": "Transfer in reinforcement learning: a framework and a survey", "author": ["A. Lazaric"], "venue": "Reinforcement Learning: State of the Art. Springer", "citeRegEx": "Lazaric,? \\Q2011\\E", "shortCiteRegEx": "Lazaric", "year": 2011}, {"title": "Online multi-task learning with hard constraints", "author": ["G. Lugosi", "O. Papaspiliopoulos", "G. Stoltz"], "venue": "In Proceedings of the 22nd Annual Conference on Learning Theory (COLT\u201909)", "citeRegEx": "Lugosi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lugosi et al\\.", "year": 2009}, {"title": "Directed exploration in reinforcement learning with transferred knowledge", "author": ["T.A. Mann", "Y. Choe"], "venue": "In Proceedings of the Tenth European Workshop on Reinforcement Learning (EWRL\u201912)", "citeRegEx": "Mann and Choe,? \\Q2012\\E", "shortCiteRegEx": "Mann and Choe", "year": 2012}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Pan and Yang,? \\Q2010\\E", "shortCiteRegEx": "Pan and Yang", "year": 2010}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the AMS,", "citeRegEx": "Robbins,? \\Q1952\\E", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "Online learning of multiple tasks and their relationships", "author": ["A. Saha", "P. Rai", "H. Daum\u00e9 III", "S. Venkatasubramanian"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS\u201911),", "citeRegEx": "Saha et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Saha et al\\.", "year": 2011}, {"title": "Matrix perturbation theory", "author": ["G.W. Stewart", "Sun", "J.-g"], "venue": null, "citeRegEx": "Stewart et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 1990}, {"title": "Perturbation bounds in connection with singular value decomposition", "author": ["P. Wedin"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "Wedin,? \\Q1972\\E", "shortCiteRegEx": "Wedin", "year": 1972}, {"title": "However, this result does not provide us with the sample complexity bound on the estimation error of model means. Here we complete their analysis by proving a sample complexity bound on the l2-norm of the estimation error of the means \u2016\u03bc(\u03b8)\u2212 \u03bc\u0302(\u03b8)", "author": ["Anandkumar et al", "2012b", "Thm"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Lem. 6). (ii) we prove high probability bounds on the error \u01eb2 and \u01eb3 using some standard concentration inequality results (Lem", "author": ["\u2212 M"], "venue": null, "citeRegEx": "M3.,? \\Q2012\\E", "shortCiteRegEx": "M3.", "year": 2012}, {"title": "Langford and Zhang, 2007), at each step the learner observes a context xt and has to choose the arm which is best for the context. The contexts belong to an arbitrary (finite or continuous) space and are drawn from a stationary distribution. This scenario resembles our setting where tasks arrive in a sequence", "author": ["Agarwal"], "venue": null, "citeRegEx": "Agarwal,? \\Q2012\\E", "shortCiteRegEx": "Agarwal", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "Recently, multi-task and transfer learning received much attention in the supervised and reinforcement learning (RL) setting with both empirical and theoretical encouraging results (see recent surveys by Pan and Yang, 2010; Lazaric, 2011).", "startOffset": 181, "endOffset": 238}, {"referenceID": 7, "context": "On the other hand, the online learning setting (Cesa-Bianchi and Lugosi, 2006), where the learner is presented with samples in a sequential fashion, has been rarely considered (see Mann and Choe (2012) for an example in RL and Sec.", "startOffset": 47, "endOffset": 78}, {"referenceID": 15, "context": "The multi\u2013arm bandit (MAB) (Robbins, 1952) is a simple yet powerful framework formalizing the online learning with partial feedback problem, which encompasses a large number of applications, such as clinical trials, online advertisements and adaptive routing.", "startOffset": 27, "endOffset": 42}, {"referenceID": 5, "context": "If we can accurately estimate this LVM, we show that an extension of the UCB algorithm (Auer et al., 2002) is able to exploit this prior knowledge to reduce the regret through tasks (Sec.", "startOffset": 87, "endOffset": 106}, {"referenceID": 3, "context": "On the other hand, the online learning setting (Cesa-Bianchi and Lugosi, 2006), where the learner is presented with samples in a sequential fashion, has been rarely considered (see Mann and Choe (2012) for an example in RL and Sec.", "startOffset": 48, "endOffset": 202}, {"referenceID": 1, "context": "1 of Anandkumar et al. (2012b) and compute eigen-vectors/values {v\u0302(\u03b8)}, {\u03bb\u0302(\u03b8)} Compute \u03bc\u0302(\u03b8) = \u03bb\u0302(\u03b8)(\u0174 )v\u0302(\u03b8) for all \u03b8 \u2208 \u0398 return \u0398 = {\u03bc\u0302(\u03b8) : \u03b8 \u2208 \u0398}", "startOffset": 5, "endOffset": 31}, {"referenceID": 1, "context": "Anandkumar et al. (2012b) (Thm.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "5One may also estimate the constant C(\u0398) in an online fashion using doubling trick (Audibert et al., 2012).", "startOffset": 83, "endOffset": 106}, {"referenceID": 1, "context": "This is a direct consequence of the perturbation bound of Anandkumar et al. (2012b, Thm. 5.1), which is at the core of our sample complexity bound. 4The result of Anandkumar et al. (2012a) has the explicit dependency of order m3 on the number of model as well as implicit dependency of order m2 through the parameter \u03b10.", "startOffset": 58, "endOffset": 189}], "year": 2013, "abstractText": "Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may significantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of sequential transfer in online learning, notably in the multi-armed bandit framework, where the objective is to minimize the cumulative regret over a sequence of tasks by incrementally transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for the estimation of the possible tasks and derive regret bounds for it.", "creator": "LaTeX with hyperref package"}}}