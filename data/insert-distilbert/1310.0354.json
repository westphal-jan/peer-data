{"id": "1310.0354", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2013", "title": "Deep and Wide Multiscale Recursive Networks for Robust Image Labeling", "abstract": "feedforward multilayer networks trained by constraint supervised learning have recently demonstrated state of the art performance on image labeling problems problems such as boundary prediction and scene parsing. as even very low error rates can limit practical usage of such systems, methods that not perform closer to human accuracy remain desirable. in this work, nowadays we propose a new type of network with the following properties that could address what we hypothesize to be limiting aspects of existing methods : ( 1 ) a ` wide'structure with thousands of features, ( 2 ) a large field of view, ( 3 ) recursive iterations that thoroughly exploit statistical dependencies in label space, and ( 4 ) a parallelizable research architecture that can be trained in lasting a fraction of the time compared to benchmark multilayer convolutional networks. for the specific image labeling problem of boundary prediction, we also introduce a novel example weighting algorithm that improves segmentation accuracy. experiments in the challenging domain of connectomic reconstruction of neural circuity from 3d electron microscopy data show that these \" deep and wide multiscale recursive \" ( dawmr ) networks lead to new levels of image labeling performance. the highest performing architecture has twelve layers, interwoven supervised and unsupervised stages, and uses an input field of view of 157, 464 voxels ( $ 54 ^ 3 $ ) to rapidly make a prediction at substantially each image location. we present an associated open source software package that enables the simple and flexible creation of dawmr networks.", "histories": [["v1", "Tue, 1 Oct 2013 15:42:54 GMT  (4068kb,D)", "https://arxiv.org/abs/1310.0354v1", null], ["v2", "Wed, 30 Oct 2013 21:16:45 GMT  (4068kb,D)", "http://arxiv.org/abs/1310.0354v2", null], ["v3", "Fri, 6 Dec 2013 17:00:03 GMT  (4069kb,D)", "http://arxiv.org/abs/1310.0354v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["gary b huang", "viren jain"], "accepted": true, "id": "1310.0354"}, "pdf": {"name": "1310.0354.pdf", "metadata": {"source": "META", "title": "Deep and Wide Multiscale Recursive Networks for Robust Image Labeling", "authors": ["Gary B. Huang"], "emails": ["jainv}@janelia.hhmi.org"], "sections": [{"heading": "1 Introduction", "text": "Image labeling tasks generate a pixel-wise field of predictions across an image space. In boundary prediction, for example, the goal is to predict whether each pixel in an image belongs to the interior or boundary of an object [24]; in scene parsing, the goal is to associate with each pixel a multidimensional vector that denotes the category of object to which that pixel belongs [9]. These types of tasks are distinguished from traditional object recognition, for which pixel-wise assigments are usually irrelevant and the goal is to produce a single global prediction about object identity.\nDensely-labeled pixel-wise ground truth data sets have recently been generated for image labeling tasks that were traditionally solved by entirely hand-designed methods [24]. This has enabled the use of learning methods that require extensive supervised parameter learning. As a result, a common class of methods, supervised multilayer neural networks, have recently been found to excel at image labeling and object recognition tasks. This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15]. Despite these improvements, for most practical applications even higher accuracy is required to achieve reliable automated image analysis. For example, in the main application studied in this paper, reconstruction of neurons from nanometer-resolution electron microscopy images of brain tissue, even small pixel-wise error rates can catastrophically deteriorate the utility of automated analysis [14].\nar X\niv :1\n31 0.\n03 54\nv3 [\ncs .C\nV ]\n6 D\nec 2\nIn this paper, we identify limitations in existing multilayer network architectures, propose a novel architecture that addresses these limitations, and then conduct detailed experiments in the domain of connectomic reconstruction of electron microscopy data. The primary contributions of our work are:\n1. A \u2018wide\u2019 and multiscale core architecture whose labeling accuracy exceeds a standard benchmark of a feedforward multilayer convolutional network. By exploiting parallel computing on both CPU clusters and GPUs, the core architecture can be trained in a day, compared with two weeks for a GPU implementation of the convolutional network. 2. A recursive pipeline consisting of repeated iterations of the core architecture. Through this recursion, the network is able to increase the field of view used to make a prediction at a given image location and exploit statistical structure in label space, resulting in substantial gains in accuracy. 3. A computationally efficient scheme for weighting training set examples in the specific image labeling problem of boundary prediction. This approach, which we refer to as \u2018local error density\u2019 (LED) weighting, is used to focus supervised learning on difficult and topologically relevant image locations, and leads to more useful boundary predictions results."}, {"heading": "2 Networks for Image Labeling: Prior Work and Desiderata", "text": "Multilayer networks for visual processing combine filtering, normalization, pooling, and subsampling operations to extract features from image data. Feature extraction is followed by additional processing layers that perform linear or nonlinear classification to generate the desired prediction variables [17]. Farabet et al. recently adapted convolutional networks to natural image scene labeling by training 2d networks that process the image at multiple scales [9]. Boundary prediction in largescale biological datasets has been investigated using 3d architectures that have five to six layers of processing [13, 33, 12, 32] and, in the work of Ciresan et al., 2d architectures with pooling operations and ensembles of multiple networks [3]. These studies have shown that multilayer networks often outperform alternative machine learning methods, such as MRFs and random forest classifiers. We hypothesize that image labeling accuracy could be further improved by a network architecture that simultaneously addresses all of the following issues:\nNarrow vs wide feature representations: The number of features in each layer of a network plays a major role in determining the overall capacity available to represent different aspects of the input space. Most multilayer models for image labeling have thus far been relatively \u2018narrow\u2019, i.e., containing a small number of features in each layer. For example, networks described in Jain et al. and Farabet et al. used respectively 12 and 16 features in the first layer, while those in Ciresan et al. used 48. We would like to transition to much wider architectures that utilize thousands of features.\nLarge field of view: Local ambiguity in an image interpretation task can be caused by noise, clutter, or intrinsic uncertainty regarding the interpretation of some local structures. Global image information can be used to resolve local ambiguity, and thus effective integration of image data over large fields of view is critical to solving an image labeling task. In multilayer visual processing architectures, there are a variety of factors that determine the effective size of the field of view used to compute a prediction for a specific pixel location: filter size, network depth, pooling structure, and multiscale processing pathways. Experiments in this work and others suggest that appropriate usage of all of these architectural components is likely to be necessary to achieve highly accurate image labeling.\nWhile the 2d architecture proposed for scene labeling in Farabet et al. is already multiscale, converting the architecture to utilize 3d filters lengthens training time into weeks or more. The 3d boundary prediction networks in Jain et al. and Turaga et al. have also been augmented with multiscale capabilities, but these modifications lengthen training times from weeks into months.\nModeling and exploiting statistical structure in labels: In the multilayer networks introduced thus far, predictions about neighboring image locations are nearly independent and become potentially correlated only due to a dependence on overlapping parts of the input image. However, in image labeling tasks there is usually a substantial amount of statistical structure among labels at neighboring image locations. This observation suggests that image labeling is a structured prediction problem in which statistics among output variables should be explicitly modeled [31]. Markov random field (MRF) image models are an example of a generative approach to structured prediction. These methods consist of an observation model p(X|Y), encoding the conditional distribution of the image X given\nthe labels Y, and a prior model p(Y), specifying the distribution over different label configurations. Given a noisy input image X , inference for p(Y|X) can thus involve both image-dependent aspects (to invert the observation model) as well as interactions among the random variables Y \u2208 Y that reflect prior statistics on valid label configurations [22].\nMultilayer network models for image analysis typically outperform MRFs, as the computational expense associated with probabilistic learning and inference substantially restricts the modeling capability of MRF models that are practical to work with [13, 15]. An alternative approach is pursued by Farabet et al., in which a multilayer network is augmented by a simple three-parameter CRF post-processing step designed to \u2018clean up\u2019 classifier predictions. In this work, we propose and investigate a recursive approach in which outputs from one network become input for a subsequent network, thereby allowing for explicit and powerful modeling of statistics among output predictions.\nReasonable training time: We regard it as critical that a network can be learned in a reasonable amount of wall-clock time (within a few days at most, but more ideally within hours). Many existing approaches could conceptually be scaled up to address the limitations that we discuss, but would then require weeks or more in order to train. Such long training times can prohibit certain usage scenarios (for example, interactively adding new labeled data based on rapid classifier retraining [30]). More generally, experimenting in the space of different cost functions, architectures, labeling strategies, etc., is only feasible if a single experiment can be performed in a reasonable duration of time. To achieve a reasonable training time, in this paper we assume access to both graphics processing units (GPUs) and multi-core CPUs or cluster computing environments. Many recent and notable results in machine learning would not have been possible without parallel computing on GPUs or CPUs [19, 4, 28]."}, {"heading": "3 Deep and Wide Multiscale Recursive Networks", "text": "We formalize the image labeling problem as follows: given an input image I , we want to predict at each location l \u2208 I a vector of labels Yl. For the main data set considered, I is a three dimensional volume of size on the order of 10003, and with each location is associated a vector of 3 labels (|Yl| = 3), indicating 3d neighbor connectivity (see Section 4.1.2 for details of the data and labels). For ease of notation we will treat the location l as a single dimension with regard to operations discussed later, such as pooling, but in practice such operations are applied in 3d.\nIn this section, we describe our proposed method for image labeling, Deep and Wide Multiscale Recursive (DAWMR) networks. DAWMR networks process images by recursive iteration of a core network architecture. Overall, a DAWMR network may have dozens of individual processing layers between the raw input image and final labeling output. A schematic overview of a typical DAWMR network is given in Figure 1."}, {"heading": "3.1 Single Iteration Core Architecture", "text": "The core network architecture in each iteration consists of two sequential processing modules: feature extraction and classification. These stages are conceptually distinct, learned using differing levels of supervision, and implemented using different parallel computing strategies.\nFeature Extraction: Given a location l, the feature extraction module produces hl, a representation of the input image centered at l. These representations are subsequently passed to the classifier to learn the specific image labeling that is encoded in the training data. (Generally hl is normalized such that each feature has zero mean and unit standard deviation prior to being passed to the classifier.)\nAt a high level, each feature extraction module consists of multiple processing layers of feature encoding using vector quantization (VQ), with intermediate layers that apply operations such as pooling, subsampling, and whitening. An entire set of processing layers can be replicated within a single module to process the image at multiple different downsampled scales (where downsampling is achieved by simple averaging). We take advantage of the recent observation that unsupervised clustering and dictionary learning techniques can be used to efficiently learn thousands of features for image recognition tasks [7, 5]. Following Coates and Ng [5], the core vector quantization component in the feature extraction module consists of a dictionary learned using a greedy variant of orthogonal matching pursuit (OMP-1) and encoding using soft-thresholding with reverse polarity.\nGiven a learned dictionary for performing vector quantization, we can produce an encoding fi centered at a location i. We consider two contrasting methods for forming a final hidden representation hl from these encoding fi. The first method uses the encoding itself as the representation, at various pixel locations centered at i. In what we call an m receptive field (RF) architecture, the hidden representation hl is formed by concatenating m features, hl = {fl\u2212m2 , . . . , fl+m2 }. This is similar to the convolutional network architectures used in [13, 16, 33]; in those networks, classification is based on input from all feature maps in the final hidden layer from feature map values within a 53 pixel window centered at the location being classified. The second method is a foveated representation that incorporates pooling operations. Given some neighborhood size m, we first perform max pooling over the neighborhood, (gl)j = max\nl+m2 i=l\u2212m2\n(hi)j . The foveated representation is then the concatenation of the feature encoding centered at l and the pooled feature, hl = {fl, gl}. We also experimented with average pooling but found max pooling to give better results in general.\nWe note that an m RF architecture and a foveated representation with a pooling neighborhood of size m have the same field of view of the data. However, if the dimensionality of the encoding is |fi| = k, then the dimensionality of the hidden representation using an m RF architecture is |hl| = mk, whereas with a foveated representation |hl| = 2k. Therefore, these two methods lie at opposite ends of the spectrum of \u2018narrow\u2019 versus \u2018wide\u2019 network architectures. Given a fixed hidden representation dimensionality |hl| = d, the m3 RF architecture will have a narrow VQ dictionary ( dm3 ) whereas the foveated representation will be able to support a wider VQ dictionary ( d 2 ).\nClassification: Following the feature extraction module, we have a standard supervised learning problem consisting of features hl and labels Yl. For the classification module, we use a multilayer perceptron (MLP) with a single hidden layer trained by mini-batch stochastic gradient descent [11]. In preliminary experiments, we found that an MLP outperformed a linear SVM. For image labeling problems that involved predicting multiple labels at each location l, we also found that using a single MLP with multiple output units outperformed an architecture with multiple single output MLPs.\nRecursive Application of Core Network: In recursive approaches to prediction, a classifier is repeatedly applied to an input (and previous classifier results) to build a structured interpretation of some data. Pioneering work established graph transformer networks for solving segmentation and recognition tasks arising in automated document processing [2]. More recently, recursive approaches have been revived for superpixel agglomeration [29, 16] and text parsing [8].\nIn image labeling tasks, each pixel in an input image generates a scalar or vector output that encodes predictions about the variables of interest. A straightforward way to directly model statistics similar to the labels is to use the output of an initial iteration of the architecture described in Section 3.1 and provide that \u201cnetwork iteration 1\u201d (N1) output as input to another instance of such an architecture\n(N2). The N1 output is accompanied by the raw image, and thus feature extraction and subsequent predictions from N2 are based upon structure in both the original image as well as the output representation from N1. Recursive construction of such classifiers is repeated for N3, ..., Nk, where k is as large as computation time permits or cross-validation performance justifies.\nEach additional recursive iteration also increases the overall field of view used to predict the output at a particular pixel location. Thus, recursive processing enables DAWMR networks to simultaneously model statistical regularities in label-space and use increasing amounts of image context, with the overall goal of refining image labeling predictions from one iteration to the next."}, {"heading": "4 Boundary Prediction Experiments", "text": "We performed detailed experiments in the domain of boundary prediction in electron microscopy images of neural tissue. This application has significant implications for the feasability of \u2018connectomics\u2019, an emerging endeavour in neurobiology to measure large-scale maps of neural circuitry at the resolution of single-synapse connectivity [23]. Reconstruction is currently the bottleneck in large-scale mapping projects due to the slow rate of purely manual reconstruction techniques [14]. Fully-automated methods for reconstruction would therefore be ideal. Current pipelines typically begin with a boundary prediction step, followed by oversegmentation of the resulting boundary map, and finally application of an agglomeration algorithm to piece together object fragments [1, 16]. Improvements in boundary prediction are desirable, as certain types of errors (such as subtle undersegmentation) can sometimes be difficult to correct during later steps of the reconstruction pipeline."}, {"heading": "4.1 Experimental Setup", "text": "Here we describe the details of the the image data and training/test sets. Experiments were run using our parallel computing software package, available online1; for more details see Section C.1."}, {"heading": "4.1.1 Image Acquisition", "text": "Neuropil from drosophila melanogaster was imaged using focused ion-beam scanning electron microscopy (FIB-SEM [18]) at a resolution of 8x8x8 nm. The tissue was prepared using highpressure freeze substitution and stained with heavy metals for contrast during electron microscopy. As compared to traditional electron microscopy methods such as serial-section transmission electron microscopy (ssTEM), FIB-SEM provides the ability to image tissue at very high resolution in all three spatial dimensions. Isotropic resolution at the sub-10nm scale is particularly advantageous in drosophila due to the small neurite size that is typical throughout the neuropil."}, {"heading": "4.1.2 Training, Test Sets", "text": "Two image volumes were obtained using the above acquisition process. The first volume was used for training and the second for both validation and testing. Initially, human annotators densely labeled subvolumes from both images. These labels form a preliminary training set, referred to in the sequel as the small training set (5.2 million labels), and the validation set (16 million labels), respectively. Afterward, an interactive procedure was used wherein human annotators \u2018proofread\u2019 machinegenerated segmentations by visually examining dense reconstructions within small subvolumes and correcting any mistakes. The proofreading interface enabled annotators to make pixel-level modifications. The proofread annotations were then added to the small training set and validation set to form the \u2018full\u2019 training set (120 million labels) and test set (46 million labels), respectively.2\nThe labels are binary and indicate connectivity of adjacent voxels, where positive labels indicate that two voxels belong to the same foreground object and negative labels indicate that two voxels belong to differing objects or both belong to the background. As the image volume is three dimensional, each location is associated with a vector of 3 labels in each direction. The set of such labels (or their inferred probabilistic values) over an image volume is referred to as the affinity graph. By specifying\n1http://sites.google.com/site/dawmrlib/ 2The validation set is a subset of the test set. Measuring segmentation accuracy requires large densely-labeled subvolumes, and due to the expense in obtaining such data, we believe that measuring final test results on a larger set of data is more valuable for evaluation than splitting off a subset to only be used as validation.\nconnectivity through an affinity graph it is possible to represent situations (such as directly adjacent, distinct objects) that would be impossible to represent with a more typical pixel-wise exterior/interior labeling [32]. Figure 2 shows a 2d slice of the test image data, affinity graph, and segmentation."}, {"heading": "4.1.3 Evaluation Measures", "text": "Given a densely-labeled ground truth segmentation, one can measure performance in two ways: classification metrics on binary representations of the segmentation (such as a boundary map or affinity graph), or segmentation-based measures that interpret the volume as a clustering of pixels. In this work, we report both types of measures.\nBoundary prediction performance is reported by treating affinity graph edge labeling as a standard binary classification task, where we compute results for each edge direction separately and then average the results over the three edge directions. As the ground truth has a class imbalance skewed toward positive (connected) edges, we report balanced class accuracy (bal-acc: 0.5 \u00b7 accuracy on positive edges + 0.5 \u00b7 accuracy on negative edges). We also compute area under the receiver operating characteristic curve, when varying the decision threshold for classifying positive/negative edges (AUC-edge).\nOne can also segment a ground truth affinity graph into clusters of connected voxels, forming a set of foreground objects and background. By segmenting an inferred affinity graph (whose labels may be real-valued) at a particular threshold, one can follow the same procedure to form an inferred clustering. We supplement the connected components segmentation by \u2018growing out\u2019 segmented objects until they touch each other, using a marker-based watershed algorithm adapted to affinity graphs. Segmentation performance is then measured by computing the Rand Index [34]. We report an area under the curve measure that integrates performance over different binarization thresholds (AUC-RI), as well as a maximum score (max RI)."}, {"heading": "4.2 Model Selection Experiments on a Validation Set", "text": "Like other deep, multilayer architectures, DAWMR networks have a number of model/architecture parameters that can be varied. In this section, we perform model selection experiments with the validation set. Unless explicitly stated otherwise, our experiments use the following set-up: the feature extraction modules produce a feature representation of dimension hul = 8000, individual filters use 3d 53 patches, and classification is performed using an MLP with a single hidden layer with 200 hidden units and trained with a balanced sampling of positive and negative training examples."}, {"heading": "4.2.1 Single-Iteration Classifiers and Comparison With Convolutional Networks", "text": "We begin by evaluating performance of single-iteration DAWMR classifiers and a supervised convolutional network. We consider five DAWMR architectures: 53 RF, single-scale vector quantization without pooling (SS), single-scale VQ with foveated representation (SS-FV), and multiscale VQ with foveated representation (MS-FV). We also test a version of the SS-FV architecture with 2d filters (other architectures use 3d filters). For both architectures using a foveated representation, we pool over a 53 neighborhood, and thus the 53 RF and SS-FV architectures have the same field of view. Table 1 provides an overview of the architectures.\nValidation performance of single iteration DAWMR networks using the above feature extraction architectures is shown in Table 2. The performance of a supervised convolutional network (CN) is also provided, and represents a strong baseline for comparison; identical types of networks have been extensively used in recent studies involving boundary prediction in 3d electron microscopy data [12, 33, 10]. The CN used in our experiment has 5 hidden layers, 16 feature maps per layer, all-to-all connectivity between hidden layers, and a filter size of 53. The CN was trained on a GPU with an implementation based on the CNS framework [26].\nThe multiscale foveated architecture (MS-FV) achieves slightly better results than the convolutional network on most metrics, for both the training and test set. The DAWMR classifier is also learned in an order of magnitude less time than the convolutional network. Adding drop-out regularization (MS-FV-DO) improves performance of the single iteration DAWMR classifier even further [11]."}, {"heading": "4.2.2 Recursive Multiscale Foveated Dropout (MS-FV-DO) Architecture", "text": "A specific core architecture can be recursively applied over multiple iterations, as discussed at the end of Section 3.1. In this section we experiment with this approach using the multiscale foveated dropout (MS-FV-DO) architecture. For the second and third iteration classifiers, which accept as input both an affinity graph as well as the original image, there is a model selection choice related to whether filters in the feature extraction stage receive input from only the image, only the affinity graph, or both. We found that dividing the set of features into an equal number which look exclusively at each type of input channel worked better than having all filters receive input from all input channels.\nTable 3 shows the results from recursive application of the MS-FV-DO architecture. Note that each iteration learns its own unsupervised and supervised parameters, thereby tripling the model parameters used to generate the final output, and that each iteration adds 183 to the total field of view used to generate an output prediction by the third iteration classifier (543). Recursive experiments were\nlimited to three iterations.3 We observe consistent improvements in classification and segmentation metrics as we recursively iterate the core MS-FV-DO architecture."}, {"heading": "4.2.3 Recursive MS-FV-DO Architecture with Local Error Density (LED) Weighting", "text": "Visual inspection of recursive output confirmed that boundary prediction generally improves over multiple iterations, but also revealed that predictions at certain rare image locations did not improve. These locations were characterized by a specific property: a high local density of boundary prediction errors present even in the first iteration output. Locally correlated boundary prediction errors are prone to causing mistakes in segmentation and are thus important to avoid. Yet because these locations are rare, they have a negligible impact on boundary prediction accuracy (the metric actually being optimized during training). Previous work has addressed this issue by proposing learning algorithms that directly optimize segmentation performance [32, 12]. These algorithms are computationally expensive, and can make convergence of online gradient descent sensitive to various parameter choices in the loss function and optimization procedure. Therefore we sought a simpler alternative.\nPrior to each recursive iteration we train a DAWMR classifier for 20% of the normal number of updates and compute affinity graph output on the training set. We then create a binary weighting mask with non-zero entries for each pixel location in which more than 50% of the affinity edge classifications in a 53 neighborhood are incorrect. This simple criteria proves effective in selectively identifying those rare locations where the failure mode occurred. The weighting mask is used during training of the full classifier by sampling weighted locations at a 10x higher rate than normal, and the mask is combined across iterations by \u2018or\u2019ing. Table 3 shows results from training a recursive MS-FV-DO architecture with this LED weighting. Weighting increased segmentation accuracy, particularly in the second and third recursive iterations. Boundary prediction classification accuracy was unaffected or even slightly diminished compared to non-weighted results. This is consistent with the idea that weighting alters the cost function to put greater emphasis on specific locations that influence segmentation accuracy, at the expense of overall boundary prediction performance.\n3Additional iterations would require a field of view so large that significant amounts of labeled data in the training and validation set would no longer be usable due to insufficient image support."}, {"heading": "4.3 Test Set Evaluation and Comparison", "text": "Based on the experiments performed on the validation set, we selected a few architectures for evaluation on the full test set. Details of the architectures are reviewed in Table 5, summary results are shown in Table 6, full plots of boundary prediction and segmentation performance are shown in Figure 4, and example 2d slices of the predicted affinity graphs are shown in Figure 3.\nThe test set results are consistent with the validation set experiments. Using a 5-million example training set (\u2018sm\u2019), the MS-FV-DO architecture outperforms the CN with far less training time. Switching to a larger training set (\u2018lg\u2019) improves MS-FV-DO boundary prediction performance.4 Recursive iterations and LED weighting further improves segmentation performance of the DAWMR architecture quite substantially.\nThe results also confirm previous observations that small differences in boundary prediction accuracy may be associated with large differences in segmentation accuracy [12]. For example, the non-recursive MS-FV-DO architecture outperforms the convolutional network only slightly when measured by AUC-edge, but much more substantially under Rand Index metrics. Visual inspection revealed that the convolutional network affinity graphs are more prone to generating undersegmentation errors due to false positive affinity edges between distinct objects."}, {"heading": "5 Discussion", "text": "Diverse strategies for exploiting image context: The DAWMR networks explored in this work use several different strategies for manipulating the size of the field of view: multiscale processing, pooling, and recursive iteration. It is likely that each strategy offers different modeling capabilities and benefits. For example, multiscale processing is an efficient way to model image features that appear at fundamentally different scales, while recursive processing of the image and affinity graph may be more effective for careful integration of high-frequency features (e.g., contour completion).\nIn the supplementary, Table 8 shows that a non-recursive architecture that achieves very large field of view in a single iteration performs worse compared to output of a third iteration recursive architecture with a smaller total field of view. As we lack an overall theory for the design of such networks, finding the architecture that makes optimal use of image context requires empirical model selection.\nA spectrum of weak vs fine tuning in feature learning schemes: We employ simple unsupervised learning algorithms to learn features in DAWMR networks. These features are likely to be only \u2018weakly\u2019 tuned for a specific prediction task, as compared to the \u2018finely\u2019 tuned features learned in a convolutional network trained by supervised backpropogation. The trade-off, which our empirical\n4Technical limitations in our GPU implementation of convolutional networks prevented us from being able to train the CN with the large (lg) training set.\nresults suggest are well worth it for the problem of boundary prediction, is in the size of the representation \u2013 DAWMR networks can quickly learn thousands of features, whereas for convolutional networks it is currently only practical to use a few dozen at most. Improvements in computing hardware, or fundamentally more parallel versions of stochastic gradient descent may enable larger convolutional network architectures in the future [27, 20].\nRecursive iterations and end-to-end learning: In recursive DAWMR networks, each iteration is learned without regard to future iterations. This is in contrast to true \u2018end-to-end\u2019 learning, in which each step is optimized based on updates back-propagated from the final output [21, 25]. While end-toend learning may lead to superior discriminative performance, the cost is twofold: a requirement for using processing stages that are (at least approximately) differentiable, as well as the computational expense of performing a \u2018forward\u2019 and \u2018backward\u2019 pass through all steps for each parameter update.\nWe avoid end-to-end learning primarily to minimize training time, but the freedom to use nondifferentiable processing steps in conjunction with intermediate affinity graphs presents interesting opportunities. For example, affinity graphs from intermediate iterations could be converted into segmentations from which object-level geometric and morphological features are computed. These features, which may be difficult to represent via differentiable operations such as filtering (e.g., geodesic and histogram-based measures), could be used as additional input to further recursive iterations that refine the affinity graph. This strategy for exploiting object-level representations is an alternative to superpixel-based approaches, and may more easily enable correction of labeling errors that lead to undersegmentation, which is difficult to address in superpixel approaches.\nAcknowledgements: We thank Zhiyuan Lu for sample preparation, Shan Xu and Harald Hess for FIB-SEM imaging, and Corey Fisher and Chris Ordish for data annotation."}, {"heading": "A Supplementary: Additional Model Selection", "text": "We report the results of additional model selection experiments for architectures and learning parameters that were less central to achieving the highest performance in the main presentation.\nA.1 Effect of Training Set Size\nAs noted in Section 4.1.2, two training sets were produced, a small training set (5.2M examples) and a full training set (120M examples), in order to examine how training set size affects DAWMR classification performance.\nWe augment the full training set by transforming the original data to create synthetic training examples. Specifically, we apply rotations and reflections to the original image data and labels, using a total of seven additional transformations to augment the full training set by a factor of eight. Given the large size of the augmented full training set (120M \u2217 8), we also subsample examples within each densely labeled subvolume in order to reduce computational load. Training examples that are nearby spatially are likely to have similar statistics, and thus we found that we can achieve comparable performance while using only a subset (10%) of the full training data.5\nThe augmented, subsampled version of the full training data constitutes the \u2018large\u2019 (lg) training set referenced in further experiments. Performance while varying the training set is shown in Table 7.\nExpanding the training set results in a significant increase in boundary prediction classification accuracy, but has a somewhat ambiguous impact on segmentation performance. These results suggest that, as training sets become large, improvements in segmentation accuracy may require additional model capacity or learning algorithms more explicitly focused on segmentation performance. We investigate both of these issues in subsequent experiments.\nA.2 Deeper Feature Extraction Stage\nIn the DAWMR architectures discussed in the main text, the unsupervised stage had a layer of filtering, followed by encoding and pooling. We experimented with adding a second set of filtering, encoding, and pooling steps. This modification adds the ability to learn higher-order image features from the data, and also dramatically increases the field of view of a single iteration architecture. We used a pairwise-similarity scheme to group first layer filters [6].\n5The training data consists of many densely labeled subvolumes; using only 10% of the subvolumes would lead to a much different and less informative training set as compared to the subsampling scheme we propose \u2013 using all labeled subvolumes and randomly sampling 10% of the examples within each.\nWe find that this deeper single-iteration architecture, MS-FV-DO-DFE, improves performance over the standard architecture (MS-FV-DO). However, performance of the recursive architecture is superior, even while using less image context, suggesting that immediately jumping to a large field of view based on deeper unsupervised feature extraction is not necessarily ideal. We also note that inference in the MS-FV-DO-DFE architecture is significantly more computational expensive than the MS-FV-DO architecture, due to the much larger number of filtering computations in the feature extraction stage.\nA.3 Varying Feature Dimensionality\nWe experimented with varying the dimensionality of the feature representation produced by the unsupervised feature extraction stage of the DAWMR networks, by varying the size of the dictionary used for vector quantization. The results, shown in Table 9, confirm our general hypothesis that wider networks produced by using a large dictionary yield increased performance.\nA.4 Varying the Number of MLP Hidden Units\nWe also experimented with varying the number of hidden units used in the supervised MLP classifier, with results shown in Table 10. All classifiers were trained using the same fixed number of updates. In general we found the results to not be especially sensitive to this parameter, and used 200 as a balance between sufficient capacity and faster training and convergence.\nA.5 Varying the Number of MLP Hidden Layers\nWe experimented with adding additional layers of hidden units in the supervised MLP classifier, with results shown in Table 11. The network was kept at a fixed with of 200 hidden units at each hidden layer, and a drop-out rate of 0.5 was used at each hidden layer. All classifiers were trained using the same fixed number of updates.\nA.6 Whitening\nPrevious work with vector quantization and deep learning architectures has noted the importance of whitening the data prior to dictionary learning [5]. We experimented with adding contrast normalization and ZCA whitening to the DAWMR networks. As shown in Table 12, we generally found that both contrast normalization and whitening generally decreased performance slightly. These results seem to indicate the importance of keeping information about intensity values relative to the\nglobal data rather than just a local patch, for this particular data set and in distinction to other data such as natural images.\nFor DAWMR networks with multiple feature encoding steps, as presented in Section A.2, we have had success with combining a small number of features produced by a single VQ step and no whitening with a larger number of features produced by multiple VQ steps and whitening.\nA.7 Orthogonal Matching Pursuit vs K-means\nIn initial experiments, we used K-means for dictionary learning and \u201ctriangle K-means\u201d for feature encoding [5]. This is compared with the Orthogonal Matching Pursuit that we used in the main presentation in Table 13. In general, we found both to give comparable results, with OMP allowing for faster feature encoding and seeming to be more amenable to multiple layers of feature encoding (Section A.2)."}, {"heading": "B Supplementary: Network Details", "text": "Here we present details and values of parameters used in our models.\nB.1 Convolutional Network\nThe convolutional network was trained in accordance with procedures outlined in previous work [13, 33]. We used sigmoid units and performed greedy layer-wise training of the architecture: 5e5 updates after adding each layer, 2e6 updates for the final architecture. Networks trained with significantly fewer iterations exhibited much worse training set performance. During training, we used a balanced sampling strategy that alternated between negative and positive edge locations and selected a 53 cube around each edge as a minibatch. Learning rates were set to 0.1, except for the last layer (set to 0.01). A square-square loss [33, 12] was optimized with a margin of 0.3.\nB.2 Multilayer Perceptron\nThe multilayer perceptrons in DAWMR architectures were trained using minibatch sizes of 40 with a balanced sampling of positive and negative edges. Learning rates were set to 0.02. We used sigmoid output units and rectified linear units in the hidden layer. For networks trained with dropout regularization, the drop-out rate was set to 0.5 for the hidden layer and 0 for the input layer. We performed 5e5 updates. Optimization was performing using a cross-entropy loss function. To regularize and prevent overfitting, we used an \u201cinverse margin\u201d of 0.1, meaning that target labels were set to 0.1/0.9 rather than 0/1, penalizing over-confident predictions."}, {"heading": "C Supplementary: DAWMR Implementation and Training Time", "text": "In this section we describe the code implementation details and training time analysis for DAWMR networks.\nC.1 Implementation\nThe design of DAWMR networks permits the use of parallel computing strategies that result in fast training time. A schematic illustration of our pipeline is given in Figure 5.\nUnsupervised feature learning is performed on a traditional multicore CPU. Next, features are extracted for potentially millions of locations distributed across a large 3d image volume with billions to trillions of voxels. In our experiments, this computation is spread across a CPU cluster comprising thousands of cores. Each worker loads image data for the locations it has been assigned, extracts features, and writes the final feature vector to a file or distributed database. Lastly, supervised learning is performed on a single GPU-equipped machine by repeatedly loading a random selection of feature vectors and performing online minibatch gradient updates with a GPU implementation of a multilayer perceptron.\nGradient-based supervised training of deep convolutional networks requires performing forward and backward pass computations through many layers of processing, and the intricate nature of these computations limits the extent of parallelism that can typically be achieved. By learning the feature representation via efficient unsupervised algorithms, DAWMR networks are able to \u2018pre-compute\u2019 feature vectors for each example in parallel across a large CPU cluster (in our experiments, this phase of computation can be accomplished in tens of minutes for even one-hundred million examples).\nAn open source Matlab/C software package that implements DAWMR networks is available online: http://sites.google.com/site/dawmrlib/.\nC.2 Training Time\nTraining DAWMR networks with parallel computation hardware (a CPU cluster and GPUs) results in training times on the order of a single day for a single iteration classifier, and multiple days for multiple recursive iterations. This compares favorably with purely supervised multilayer convolutional\nnetworks (typically on the order of weeks for GPU implementations with 3d filters, even without multiscale processing).\nAn analysis of our pipeline (Figure 5) reveals that the vast majority of time is spent training the multilayer perceptron (MLP). Moreover, during the GPU-based MLP training, most of the time is spent on I/O to retrieve feature vectors from the filesystem for each randomly constructed minibatch. In our experiments, the filesystem was a large-scale EMC Isilon installation accessed via 10-gigabit networking.\nSubstantial improvements in training time could thus be achieved by additional engineering that simply reduced the overhead associated with accessing feature vectors. In-memory databases, flash storage, and more efficient distributed filesystems are likely to enable such improvements."}], "references": [{"title": "3d segmentation of sbfsem images of neuropil by a graphical model over supervoxel boundaries", "author": ["B. Andres", "U. Koethe", "T. Kroeger", "M. Helmstaedter", "K.L. Briggman", "W. Denk", "F.A. Hamprecht"], "venue": "Medical image analysis, 16(4):796\u2013805,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["L. Bottou", "Y. Bengio", "Y. Le Cun"], "venue": "Computer Vision and Pattern Recognition, 1997. Proceedings., 1997 IEEE Computer Society Conference on, pages 489\u2013494. IEEE,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep neural networks segment neuronal membranes in electron microscopy images", "author": ["D. Ciresan", "A. Giusti", "J. Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Emergence of object-selective features in unsupervised feature learning", "author": ["A. Coates", "A. Karpathy", "A. Ng"], "venue": "Advances in Neural Information Processing Systems 25, pages 2690\u20132698,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 921\u2013928,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Selecting receptive fields in deep networks", "author": ["A. Coates", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 2528\u20132536,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "A.Y. Ng", "H. Lee"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 215\u2013223,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning for efficient discriminative parsing", "author": ["R. Collobert"], "venue": "International Conference on Artificial Intelligence and Statistics, pages 224\u2013232,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Scene parsing with multiscale feature learning, purity trees, and optimal covers", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "arXiv preprint arXiv:1202.2160,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectomic reconstruction of the inner plexiform layer in the mouse retina", "author": ["M. Helmstaedter", "K.L. Briggman", "S.C. Turaga", "V. Jain", "H.S. Seung", "W. Denk"], "venue": "Nature, 500(7461):168\u2013174,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Boundary Learning by Optimization with Topological Constraints", "author": ["V. Jain", "B. Bollmann", "M. Richardson", "D. Berger", "M. Helmstaedter", "K. Briggman", "W. Denk", "J. Bowden", "J. Mendenhall", "W. Abraham", "K. Harris", "N. Kasthuri", "K. Hayworth", "R. Schalek", "J. Tapia", "J. Lichtman", "H. Seung"], "venue": "Computer Vision and Pattern Recognition, IEEE Computer Society Conference on,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised learning of image restoration with convolutional networks", "author": ["V. Jain", "J.F. Murray", "F. Roth", "S.C. Turaga", "V. Zhigulin", "K.L. Briggman", "M.N. Helmstaedter", "W. Denk", "H.S. Seung"], "venue": "Computer Vision, IEEE International Conference on, 0:1\u20138,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Machines that learn to segment images: a crucial technology for connectomics", "author": ["V. Jain", "H. Seung", "S. Turaga"], "venue": "Current opinion in neurobiology,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural image denoising with convolutional networks", "author": ["V. Jain", "S. Seung"], "venue": "Advances in Neural Information Processing Systems, pages 769\u2013776,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to agglomerate superpixel hierarchies", "author": ["V. Jain", "S.C. Turaga", "K. Briggman", "M.N. Helmstaedter", "W. Denk", "H.S. Seung"], "venue": "Advances in Neural Information Processing Systems, pages 648\u2013656,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pages 2146\u20132153", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "IEEE,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Serial section scanning electron microscopy of adult brain tissue using focused ion beam milling", "author": ["G. Knott", "H. Marchman", "D. Wall", "B. Lich"], "venue": "Journal of Neuroscience, 28(12):2959,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pages 1106\u20131114,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q.V. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G.S. Corrado", "J. Dean", "A.Y. Ng"], "venue": "arXiv preprint arXiv:1112.6209,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Markov random field models in computer vision", "author": ["S. Li"], "venue": "Springer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1994}, {"title": "The big and the small: challenges of imaging the brain\u2019s circuits", "author": ["J.W. Lichtman", "W. Denk"], "venue": "Science, 334(6056):618\u2013623,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to detect natural image boundaries using local brightness, color, and texture cues", "author": ["D.R. Martin", "C.C. Fowlkes", "J. Malik"], "venue": "IEEE Trans. Patt. Anal. Mach. Intell., pages 530\u2013549,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Off-road obstacle avoidance through end-to-end learning", "author": ["U. Muller", "J. Ben", "E. Cosatto", "B. Flepp", "Y.L. Cun"], "venue": "Advances in neural information processing systems, pages 739\u2013746,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "CNS: a GPU-based framework for simulating cortically-organized networks", "author": ["J. Mutch", "U. Knoblich", "T. Poggio"], "venue": "Technical report, Massachussetts Institute of Technology,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. R\u00e9", "S.J. Wright"], "venue": "arXiv preprint arXiv:1106.5730,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "A high-throughput screening approach to discovering good forms of biologically inspired visual representation", "author": ["N. Pinto", "D. Doukhan", "J.J. DiCarlo", "D.D. Cox"], "venue": "PLoS computational biology, 5(11):e1000579,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "A. Ng", "C. Manning"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 129\u2013136,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Ilastik: Interactive learning and segmentation toolkit", "author": ["C. Sommer", "C. Straehle", "U. Kothe", "F.A. Hamprecht"], "venue": "Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium on, pages 230\u2013233. IEEE,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research, pages 1453\u20131484,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Maximin affinity learning of image segmentation", "author": ["S.C. Turaga", "K.L. Briggman", "M. Helmstaedter", "W. Denk", "H.S. Seung"], "venue": "NIPS,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Convolutional networks can learn to generate affinity graphs for image segmentation", "author": ["S.C. Turaga", "J.F. Murray", "V. Jain", "F. Roth", "M. Helmstaedter", "K. Briggman", "W. Denk", "H.S. Seung"], "venue": "Neural Computation, 22(2):511\u2013538,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Toward objective evaluation of image segmentation algorithms", "author": ["R. Unnikrishnan", "C. Pantofaru", "M. Hebert"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(6):929,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 23, "context": "In boundary prediction, for example, the goal is to predict whether each pixel in an image belongs to the interior or boundary of an object [24]; in scene parsing, the goal is to associate with each pixel a multidimensional vector that denotes the category of object to which that pixel belongs [9].", "startOffset": 140, "endOffset": 144}, {"referenceID": 8, "context": "In boundary prediction, for example, the goal is to predict whether each pixel in an image belongs to the interior or boundary of an object [24]; in scene parsing, the goal is to associate with each pixel a multidimensional vector that denotes the category of object to which that pixel belongs [9].", "startOffset": 295, "endOffset": 298}, {"referenceID": 23, "context": "Densely-labeled pixel-wise ground truth data sets have recently been generated for image labeling tasks that were traditionally solved by entirely hand-designed methods [24].", "startOffset": 169, "endOffset": 173}, {"referenceID": 8, "context": "This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15].", "startOffset": 130, "endOffset": 148}, {"referenceID": 18, "context": "This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15].", "startOffset": 130, "endOffset": 148}, {"referenceID": 11, "context": "This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15].", "startOffset": 130, "endOffset": 148}, {"referenceID": 2, "context": "This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15].", "startOffset": 130, "endOffset": 148}, {"referenceID": 14, "context": "This approach has led to state-of-the-art and in some cases breakthrough performance on a diverse array of problems and data sets [9, 19, 12, 3, 15].", "startOffset": 130, "endOffset": 148}, {"referenceID": 13, "context": "For example, in the main application studied in this paper, reconstruction of neurons from nanometer-resolution electron microscopy images of brain tissue, even small pixel-wise error rates can catastrophically deteriorate the utility of automated analysis [14].", "startOffset": 257, "endOffset": 261}, {"referenceID": 16, "context": "Feature extraction is followed by additional processing layers that perform linear or nonlinear classification to generate the desired prediction variables [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "recently adapted convolutional networks to natural image scene labeling by training 2d networks that process the image at multiple scales [9].", "startOffset": 138, "endOffset": 141}, {"referenceID": 12, "context": "Boundary prediction in largescale biological datasets has been investigated using 3d architectures that have five to six layers of processing [13, 33, 12, 32] and, in the work of Ciresan et al.", "startOffset": 142, "endOffset": 158}, {"referenceID": 32, "context": "Boundary prediction in largescale biological datasets has been investigated using 3d architectures that have five to six layers of processing [13, 33, 12, 32] and, in the work of Ciresan et al.", "startOffset": 142, "endOffset": 158}, {"referenceID": 11, "context": "Boundary prediction in largescale biological datasets has been investigated using 3d architectures that have five to six layers of processing [13, 33, 12, 32] and, in the work of Ciresan et al.", "startOffset": 142, "endOffset": 158}, {"referenceID": 31, "context": "Boundary prediction in largescale biological datasets has been investigated using 3d architectures that have five to six layers of processing [13, 33, 12, 32] and, in the work of Ciresan et al.", "startOffset": 142, "endOffset": 158}, {"referenceID": 2, "context": ", 2d architectures with pooling operations and ensembles of multiple networks [3].", "startOffset": 78, "endOffset": 81}, {"referenceID": 30, "context": "This observation suggests that image labeling is a structured prediction problem in which statistics among output variables should be explicitly modeled [31].", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "Given a noisy input image X , inference for p(Y|X) can thus involve both image-dependent aspects (to invert the observation model) as well as interactions among the random variables Y \u2208 Y that reflect prior statistics on valid label configurations [22].", "startOffset": 248, "endOffset": 252}, {"referenceID": 12, "context": "Multilayer network models for image analysis typically outperform MRFs, as the computational expense associated with probabilistic learning and inference substantially restricts the modeling capability of MRF models that are practical to work with [13, 15].", "startOffset": 248, "endOffset": 256}, {"referenceID": 14, "context": "Multilayer network models for image analysis typically outperform MRFs, as the computational expense associated with probabilistic learning and inference substantially restricts the modeling capability of MRF models that are practical to work with [13, 15].", "startOffset": 248, "endOffset": 256}, {"referenceID": 29, "context": "Such long training times can prohibit certain usage scenarios (for example, interactively adding new labeled data based on rapid classifier retraining [30]).", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "Many recent and notable results in machine learning would not have been possible without parallel computing on GPUs or CPUs [19, 4, 28].", "startOffset": 124, "endOffset": 135}, {"referenceID": 3, "context": "Many recent and notable results in machine learning would not have been possible without parallel computing on GPUs or CPUs [19, 4, 28].", "startOffset": 124, "endOffset": 135}, {"referenceID": 27, "context": "Many recent and notable results in machine learning would not have been possible without parallel computing on GPUs or CPUs [19, 4, 28].", "startOffset": 124, "endOffset": 135}, {"referenceID": 6, "context": "We take advantage of the recent observation that unsupervised clustering and dictionary learning techniques can be used to efficiently learn thousands of features for image recognition tasks [7, 5].", "startOffset": 191, "endOffset": 197}, {"referenceID": 4, "context": "We take advantage of the recent observation that unsupervised clustering and dictionary learning techniques can be used to efficiently learn thousands of features for image recognition tasks [7, 5].", "startOffset": 191, "endOffset": 197}, {"referenceID": 4, "context": "Following Coates and Ng [5], the core vector quantization component in the feature extraction module consists of a dictionary learned using a greedy variant of orthogonal matching pursuit (OMP-1) and encoding using soft-thresholding with reverse polarity.", "startOffset": 24, "endOffset": 27}, {"referenceID": 12, "context": "This is similar to the convolutional network architectures used in [13, 16, 33]; in those networks, classification is based on input from all feature maps in the final hidden layer from feature map values within a 5 pixel window centered at the location being classified.", "startOffset": 67, "endOffset": 79}, {"referenceID": 15, "context": "This is similar to the convolutional network architectures used in [13, 16, 33]; in those networks, classification is based on input from all feature maps in the final hidden layer from feature map values within a 5 pixel window centered at the location being classified.", "startOffset": 67, "endOffset": 79}, {"referenceID": 32, "context": "This is similar to the convolutional network architectures used in [13, 16, 33]; in those networks, classification is based on input from all feature maps in the final hidden layer from feature map values within a 5 pixel window centered at the location being classified.", "startOffset": 67, "endOffset": 79}, {"referenceID": 10, "context": "For the classification module, we use a multilayer perceptron (MLP) with a single hidden layer trained by mini-batch stochastic gradient descent [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 1, "context": "Pioneering work established graph transformer networks for solving segmentation and recognition tasks arising in automated document processing [2].", "startOffset": 143, "endOffset": 146}, {"referenceID": 28, "context": "More recently, recursive approaches have been revived for superpixel agglomeration [29, 16] and text parsing [8].", "startOffset": 83, "endOffset": 91}, {"referenceID": 15, "context": "More recently, recursive approaches have been revived for superpixel agglomeration [29, 16] and text parsing [8].", "startOffset": 83, "endOffset": 91}, {"referenceID": 7, "context": "More recently, recursive approaches have been revived for superpixel agglomeration [29, 16] and text parsing [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 22, "context": "This application has significant implications for the feasability of \u2018connectomics\u2019, an emerging endeavour in neurobiology to measure large-scale maps of neural circuitry at the resolution of single-synapse connectivity [23].", "startOffset": 220, "endOffset": 224}, {"referenceID": 13, "context": "Reconstruction is currently the bottleneck in large-scale mapping projects due to the slow rate of purely manual reconstruction techniques [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 0, "context": "Current pipelines typically begin with a boundary prediction step, followed by oversegmentation of the resulting boundary map, and finally application of an agglomeration algorithm to piece together object fragments [1, 16].", "startOffset": 216, "endOffset": 223}, {"referenceID": 15, "context": "Current pipelines typically begin with a boundary prediction step, followed by oversegmentation of the resulting boundary map, and finally application of an agglomeration algorithm to piece together object fragments [1, 16].", "startOffset": 216, "endOffset": 223}, {"referenceID": 17, "context": "Neuropil from drosophila melanogaster was imaged using focused ion-beam scanning electron microscopy (FIB-SEM [18]) at a resolution of 8x8x8 nm.", "startOffset": 110, "endOffset": 114}, {"referenceID": 31, "context": "connectivity through an affinity graph it is possible to represent situations (such as directly adjacent, distinct objects) that would be impossible to represent with a more typical pixel-wise exterior/interior labeling [32].", "startOffset": 220, "endOffset": 224}, {"referenceID": 33, "context": "Segmentation performance is then measured by computing the Rand Index [34].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "The performance of a supervised convolutional network (CN) is also provided, and represents a strong baseline for comparison; identical types of networks have been extensively used in recent studies involving boundary prediction in 3d electron microscopy data [12, 33, 10].", "startOffset": 260, "endOffset": 272}, {"referenceID": 32, "context": "The performance of a supervised convolutional network (CN) is also provided, and represents a strong baseline for comparison; identical types of networks have been extensively used in recent studies involving boundary prediction in 3d electron microscopy data [12, 33, 10].", "startOffset": 260, "endOffset": 272}, {"referenceID": 9, "context": "The performance of a supervised convolutional network (CN) is also provided, and represents a strong baseline for comparison; identical types of networks have been extensively used in recent studies involving boundary prediction in 3d electron microscopy data [12, 33, 10].", "startOffset": 260, "endOffset": 272}, {"referenceID": 25, "context": "The CN was trained on a GPU with an implementation based on the CNS framework [26].", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "Adding drop-out regularization (MS-FV-DO) improves performance of the single iteration DAWMR classifier even further [11].", "startOffset": 117, "endOffset": 121}, {"referenceID": 31, "context": "Previous work has addressed this issue by proposing learning algorithms that directly optimize segmentation performance [32, 12].", "startOffset": 120, "endOffset": 128}, {"referenceID": 11, "context": "Previous work has addressed this issue by proposing learning algorithms that directly optimize segmentation performance [32, 12].", "startOffset": 120, "endOffset": 128}, {"referenceID": 11, "context": "The results also confirm previous observations that small differences in boundary prediction accuracy may be associated with large differences in segmentation accuracy [12].", "startOffset": 168, "endOffset": 172}, {"referenceID": 26, "context": "Improvements in computing hardware, or fundamentally more parallel versions of stochastic gradient descent may enable larger convolutional network architectures in the future [27, 20].", "startOffset": 175, "endOffset": 183}, {"referenceID": 19, "context": "Improvements in computing hardware, or fundamentally more parallel versions of stochastic gradient descent may enable larger convolutional network architectures in the future [27, 20].", "startOffset": 175, "endOffset": 183}, {"referenceID": 20, "context": "This is in contrast to true \u2018end-to-end\u2019 learning, in which each step is optimized based on updates back-propagated from the final output [21, 25].", "startOffset": 138, "endOffset": 146}, {"referenceID": 24, "context": "This is in contrast to true \u2018end-to-end\u2019 learning, in which each step is optimized based on updates back-propagated from the final output [21, 25].", "startOffset": 138, "endOffset": 146}], "year": 2013, "abstractText": "Feedforward multilayer networks trained by supervised learning have recently demonstrated state of the art performance on image labeling problems such as boundary prediction and scene parsing. As even very low error rates can limit practical usage of such systems, methods that perform closer to human accuracy remain desirable. In this work, we propose a new type of network with the following properties that address what we hypothesize to be limiting aspects of existing methods: (1) a \u2018wide\u2019 structure with thousands of features, (2) a large field of view, (3) recursive iterations that exploit statistical dependencies in label space, and (4) a parallelizable architecture that can be trained in a fraction of the time compared to benchmark multilayer convolutional networks. For the specific image labeling problem of boundary prediction, we also introduce a novel example weighting algorithm that improves segmentation accuracy. Experiments in the challenging domain of connectomic reconstruction of neural circuity from 3d electron microscopy data show that these \u201cDeep And Wide Multiscale Recursive\u201d (DAWMR) networks lead to new levels of image labeling performance. The highest performing architecture has twelve layers, interwoven supervised and unsupervised stages, and uses an input field of view of 157,464 voxels (54) to make a prediction at each image location. We present an associated open source software package that enables the simple and flexible creation of DAWMR networks.", "creator": "LaTeX with hyperref package"}}}