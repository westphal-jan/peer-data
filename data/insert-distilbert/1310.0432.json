{"id": "1310.0432", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2013", "title": "Online Learning of Dynamic Parameters in Social Networks", "abstract": "this new paper addresses also the problem of online learning in a dynamic setting. we consider a social network in which each individual observes a private signal about the underlying invariant state of the world and communicates with her neighbors randomly at easter each time period. unlike many existing approaches, the underlying state is dynamic, and evolves according to a geometric random walk. we view the scenario as strictly an optimization problem where agents aim to learn the perfectly true observed state while suffering the smallest possible loss. based on the decomposition of the global loss reward function, we generally introduce two update mechanisms, representing each of which generates an estimate of the optimal true state. we establish a tight bound on the rate gradient of change of the underlying state, under which participant individuals can track the parameter output with a bounded variance. then, we characterize explicit expressions for the steady state mean - square deviation ( msd ) of the estimates from the truth, per measured individual. we observe likewise that only one of the estimators spontaneously recovers the optimal msd, which underscores the impact of increasing the objective function decomposition on the learning parameters quality. finally, we provide one an upper bound on the regret of the proposed methods, measured as solving an average of gibbs errors in estimating the parameter in a finite time.", "histories": [["v1", "Tue, 1 Oct 2013 19:08:04 GMT  (18kb)", "http://arxiv.org/abs/1310.0432v1", "12 pages, To appear in Neural Information Processing Systems (NIPS) 2013"]], "COMMENTS": "12 pages, To appear in Neural Information Processing Systems (NIPS) 2013", "reviews": [], "SUBJECTS": "math.OC cs.LG cs.SI stat.ML", "authors": ["shahin shahrampour", "alexander rakhlin", "ali jadbabaie"], "accepted": true, "id": "1310.0432"}, "pdf": {"name": "1310.0432.pdf", "metadata": {"source": "CRF", "title": "Online Learning of Dynamic Parameters in Social Networks", "authors": ["Shahin Shahrampour", "Alexander Rakhlin", "Ali Jadbabaie"], "emails": ["1shahin@seas.upenn.edu", "jadbabai@seas.upenn.edu", "rakhlin@wharton.upenn.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 0.\n04 32\nv1 [\nm at\nh. O\nC ]\n1 O"}, {"heading": "1 Introduction", "text": "In recent years, distributed estimation, learning and prediction has attracted a considerable attention in wide variety of disciplines with applications ranging from sensor networks to social and economic networks [1\u20136]. In this broad class of problems, agents aim to learn the true value of a parameter often called the underlying state of the world. The state could represent a product, an opinion, a vote, or a quantity of interest in a sensor network. Each agent observes a private signal about the underlying state at each time period, and communicates with her neighbors to augment her imperfect observations. Despite the wealth of research in this area when the underlying state is fixed (see e.g. [1\u20133, 7]), often the state is subject to some change over time(e.g. the price of stocks) [8\u201311]. Therefore, it is more realistic to study models which allow the parameter of interest to vary. In the non-distributed context, such models have been studied in the classical literature on time-series prediction, and, more recently, in the literature on online learning under relaxed assumptions about the nature of sequences [12]. In this paper we aim to study the sequential prediction problem in the context of a social network and noisy feedback to agents.\nWe consider a stochastic optimization framework to describe an online social learning problem when the underlying state of the world varies over time. Our motivation for the current study is the results of [8] and [9] where authors propose a social learning scheme in which the underlying state follows a simple random walk. However, unlike [8] and [9], we assume a geometric random walk evolution with an associated rate of change. This enables us to investigate the interplay of social learning, network structure, and the rate of state change, especially in the interesting case that the rate is\ngreater than unity. We then pose the social learning as an optimization problem in which individuals aim to suffer the smallest possible loss as they observe the stream of signals. Of particular relevance to this work is the work of Duchi et al. in [13] where the authors develop a distributed method based on dual averaging of sub-gradients to converge to the optimal solution. In this paper, we restrict our attention to quadratic loss functions regularized by a quadratic proximal function, but there is no fixed optimal solution as the underlying state is dynamic. In this direction, the key observation is the decomposition of the global loss function into local loss functions. We consider two decompositions for the global objective, each of which gives rise to a single-consensus-step belief update mechanism. The first method incorporates the averaged prior beliefs among neighbors with the new private observation, while the second one takes into account the observations in the neighborhood as well. In both scenarios, we establish that the estimates are eventually unbiased, and we characterize an explicit expression for the mean-square deviation(MSD) of the beliefs from the truth, per individual. Interestingly, this quantity relies on the whole spectrum of the communication matrix which exhibits the formidable role of the network structure in the asymptotic learning. We observe that the estimators outperform the upper bound provided for MSD in the previous work [8]. Furthermore, only one of the two proposed estimators can compete with the centralized optimal Kalman Filter [14] in certain circumstances. This fact underscores the dependence of optimality on decomposition of the global loss function. We further highlight the influence of connectivity on learning by quantifying the ratio of MSD for a complete versus a disconnected network. We see that this ratio is always less than unity and it can get arbitrarily close to zero under some constraints.\nOur next contribution is to provide an upper bound for regret of the proposed methods, defined as an average of errors in estimating the parameter up to a given time minus the long-run expected loss due to noise and dynamics alone. This finite-time regret analysis is based on the recently developed concentration inequalities for matrices and it complements the asymptotic statements about the behavior of MSD.\nFinally, we examine the trade-off between the network sparsity and learning quality in a microscopic level. Under mild technical constraints, we see that losing each connection has detrimental effect on learning as it monotonically increases the MSD. On the other hand, capturing agents communications with a graph, we introduce the notion of optimal edge as the edge whose addition has the most effect on learning in the sense of MSD reduction. We prove that such a friendship is likely to occur between a pair of individuals with high self-reliance that have the least common neighbors."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 State and Observation Model", "text": "We consider a network consisting of a finite number of agents V = {1, 2, ..., N}. The agents indexed by i \u2208 V seek the underlying state of the world, xt \u2208 R, which varies over time and evolves according to\nxt+1 = axt + rt, (1)\nwhere rt is a zero mean innovation, which is independent over time with finite variance E[r2t ] = \u03c3 2 r , and a \u2208 R is the expected rate of change of the state of the world, assumed to be available to all agents, and could potentially be greater than unity. We assume the initial state x0 is a finite random variable drawn independently by the nature. At time period t, each agent i receives a private signal yi,t \u2208 R, which is a noisy version of xt, and can be described by the linear equation\nyi,t = xt + wi,t, (2)\nwhere wi,t is a zero mean observation noise with finite variance E[w2i,t] = \u03c3 2 w, and it is assumed to be independent over time and agents, and uncorrelated to the innovation noise. Each agent i forms an estimate or a belief about the true value of xt at time t conforming to an update mechanism that will be discussed later. Much of the difficulty of this problem stems from the hardness of tracking a dynamic state with noisy observations, especially when |a| > 1, and communication mitigates the difficulty by virtue of reducing the effective noise."}, {"heading": "2.2 Communication Structure", "text": "Agents communicate with each other to update their beliefs about the underlying state of the world. The interaction between agents is captured by an undirected graph G = (V , E), where V is the set of agents, and if there is a link between agent i and agent j, then {i, j} \u2208 E . We let N\u0304i = {j \u2208 V : {i, j} \u2208 E} be the set of neighbors of agent i, and Ni = N\u0304i \u222a {i}. Each agent i can only communicate with her neighbors, and assigns a weight pij > 0 for any j \u2208 N\u0304i. We also let pii \u2265 0 denote the self-reliance of agent i.\nAssumption 1. The communication matrix P = [pij ] is symmetric and doubly stochastic, i.e., it satisfies\npij \u2265 0 , pij = pji , and \u2211\nj\u2208Ni\npij = N\u2211\nj=1\npij = 1.\nWe further assume the eigenvalues of P are in descending order and satisfy\n\u22121 < \u03bbN (P ) \u2264 ... \u2264 \u03bb2(P ) < \u03bb1(P ) = 1."}, {"heading": "2.3 Estimate Updates", "text": "The goal of agents is to learn xt in a collaborative manner by making sequential predictions. From optimization perspective, this can be cast as a quest for online minimization of the separable, global, time-varying cost function\nmin x\u0304\u2208R\nft(x\u0304) = 1\nN\nN\u2211\ni=1\n(\nf\u0302i,t(x\u0304) , 1 2 E ( yi,t \u2212 x\u0304\n)2 )\n= 1\nN\nN\u2211\ni=1\n(\nf\u0303i,t(x\u0304) ,\nN\u2211\nj=1\npij f\u0302j,t(x\u0304)\n)\n, (3)\nat each time period t. One approach to tackle the stochastic learning problem formulated above is to employ distributed dual averaging regularized by a quadratic proximal function [13]. To this end, if agent i exploits f\u0302i,t as the local loss function, she updates her belief as\nx\u0302i,t+1 = a\n( \u2211\nj\u2208Ni\npij x\u0302j,t\n\ufe38 \ufe37\ufe37 \ufe38 consensus update\n+\u03b1(yi,t \u2212 x\u0302i,t) \ufe38 \ufe37\ufe37 \ufe38\ninnovation update\n)\n, (4)\nwhile using f\u0303i,t as the local loss function results in the following update\nx\u0303i,t+1 = a\n( \u2211\nj\u2208Ni\npij x\u0303j,t\n\ufe38 \ufe37\ufe37 \ufe38 consensus update\n+\u03b1( \u2211\nj\u2208Ni pijyj,t \u2212 x\u0303i,t) \ufe38 \ufe37\ufe37 \ufe38\ninnovation update\n)\n, (5)\nwhere \u03b1 \u2208 (0, 1] is a constant step size that agents place for their innovation update, and we refer to it as signal weight. Equations (4) and (5) are distinct, single-consensus-step estimators differing in the choice of the local loss function with (4) using only private observations while (5) averaging observations over the neighborhood. We analyze both class of estimators noting that one might expect (5) to perform better than (4) due to more information availability.\nNote that the choice of constant step size provides an insight on the interplay of persistent innovation and learning abilities of the network. We remark that agents can easily learn the fixed rate of change a by taking ratios of observations, and we assume that this has been already performed by the agents in the past. The case of a changing a is beyond the scope of the present paper. We also point out that the real-valued (rather than vector-valued) nature of the state is a simplification that forms a clean playground for the study of the effects of social learning, effects of friendships, and other properties of the problem."}, {"heading": "2.4 Error Process", "text": "Defining the local error processes \u03be\u0302i,t and \u03be\u0303i,t, at time t for agent i, as\n\u03be\u0302i,t , x\u0302i,t \u2212 xt and \u03be\u0303i,t , x\u0303i,t \u2212 xt,\nand stacking the local errors in vectors \u03be\u0302t, \u03be\u0303t \u2208 RN , respectively, such that \u03be\u0302t , [\u03be\u03021,t, ..., \u03be\u0302N,t] T and \u03be\u0303t , [\u03be\u03031,t, ..., \u03be\u0303N,t]T, (6)\none can show that the aforementioned collective error processes could be described as a linear dynamical system.\nLemma 2. Given Assumption 1, the collective error processes \u03be\u0302t and \u03be\u0303t defined in (6) satisfy\n\u03be\u0302t+1 = Q\u03be\u0302t + s\u0302t and \u03be\u0303t+1 = Q\u03be\u0303t + s\u0303t, (7)\nrespectively, where\nQ = a(P \u2212 \u03b1IN ), (8) and\ns\u0302t = (\u03b1a)[w1,t, ..., wN,t] T \u2212 rt1N and s\u0303t = (\u03b1a)P [w1,t, ..., wN,t]T \u2212 rt1N , (9)\nwith 1N being vector of all ones.\nThroughout the paper, we let \u03c1(Q), denote the spectral radius of Q, which is equal to the largest singular value of Q due to symmetry."}, {"heading": "3 Social Learning: Convergence of Beliefs and Regret Analysis", "text": "In this section, we study the behavior of estimators (4) and (5) in the mean and mean-square sense, and we provide the regret analysis.\nIn the following proposition, we establish a tight bound for a, under which agents can achieve asymptotically unbiased estimates using proper signal weight. Proposition 3 (Unbiased Estimates). Given the network G with corresponding communication matrix P satisfying Assumption 1, the rate of change of the social network in (4) and (5) must respect the constraint\n|a| < 2 1\u2212 \u03bbN (P ) ,\nto allow agents to form asymptotically unbiased estimates of the underlying state.\nProposition 3 determines the trade-off between the rate of change and the network structure. In other words, changing less than the rate given in the statement of the proposition, individuals can always track xt with bounded variance by selecting an appropriate signal weight. However, the proposition does not make any statement on the learning quality. To capture that, we define the steady state Mean Square Deviation(MSD) of the network from the truth as follows. Definition 4 ((Steady State-)Mean Square Deviation). Given the network G with a rate of change which allows unbiased estimation, the steady state of the error processes in (7) is defined as follows\n\u03a3\u0302 , lim t\u2192\u221e E[\u03be\u0302t\u03be\u0302 T t ] and \u03a3\u0303 , limt\u2192\u221e E[\u03be\u0303t\u03be\u0303 T t ].\nHence, the (Steady State-)Mean Square Deviation of the network is the deviation from the truth in the mean-square sense, per individual, and it is defined as\n\u02c6MSD , 1\nN Tr(\u03a3\u0302) and \u02dcMSD ,\n1\nN Tr(\u03a3\u0303).\nTheorem 5 (MSD). Given the error processes (7) with \u03c1(Q) < 1, the steady state MSD for (4) and (5) is a function of the communication matrix P , and the signal weight \u03b1 as follows\n\u02c6MSD(P, \u03b1) = RMSD(\u03b1) + W\u0302MSD(P, \u03b1) \u02dcMSD(P, \u03b1) = RMSD(\u03b1) + W\u0303MSD(P, \u03b1), (10)\nwhere\nRMSD(\u03b1) , \u03c32r\n1\u2212 a2(1\u2212 \u03b1)2 , (11)\nand\nW\u0302MSD(P, \u03b1) , 1\nN\nN \u2211\ni=1\na2\u03b12\u03c32w\n1\u2212 a2(\u03bbi(P )\u2212 \u03b1)2 and W\u0303MSD(P, \u03b1) ,\n1\nN\nN \u2211\ni=1\na2\u03b12\u03c32w\u03bb 2 i (P )\n1\u2212 a2(\u03bbi(P )\u2212 \u03b1)2 . (12)\nTheorem 5 shows that the steady state MSD is governed by all eigenvalues of P contributing to WMSD pertaining to the observation noise, while RMSD is the penalty incurred due to the innovation noise. Moreover, (5) outperforms (4) due to richer information diffusion, which stresses the importance of global loss function decomposition.\nOne might advance a conjecture that a complete network, where all individuals can communicate with each other, achieves a lower steady state MSD in the learning process since it provides the most information diffusion among other networks. This intuitive idea is discussed in the following corollary beside a few examples.\nCorollary 6. Denoting the complete, star, and cycle graphs on N vertices by KN , SN , and CN , respectively, and denoting their corresponding Laplacians by LKN , LSN , and LCN , under conditions of Theorem 5,\n(a) For P = I \u2212 1\u2212\u03b1N LKN , we have\nlim N\u2192\u221e\n\u02c6MSDKN = RMSD(\u03b1) + a 2\u03b12\u03c32w. (13)\n(b) For P = I \u2212 1\u2212\u03b1N LSN , we have\nlim N\u2192\u221e\n\u02c6MSDSN = RMSD(\u03b1) + a2\u03b12\u03c32w\n1\u2212 a2(1\u2212 \u03b1)2 . (14)\n(c) For P = I \u2212 \u03b2LCN , where \u03b2 must preserve unbiasedness, we have\nlim N\u2192\u221e\n\u02c6MSDCN = RMSD(\u03b1) + \u222b 2\u03c0\n0\na2\u03b12\u03c32w 1\u2212 a2(1\u2212 \u03b2(2\u2212 2 cos(\u03c4)) \u2212 \u03b1)2 d\u03c4 2\u03c0 . (15)\n(d) For P = I \u2212 1NLKN , we have\nlim N\u2192\u221e\n\u02dcMSDKN = RMSD(\u03b1). (16)\nProof. Noting that the spectrum of LKN , LSN and LCN are, respectively [15], {\u03bbN = 0, \u03bbN\u22121 = N, ..., \u03bb1 = N}, {\u03bbN = 0, \u03bbN\u22121 = 1, ..., \u03bb2 = 1, \u03bb1 = N}, and {\u03bbi = 2 \u2212 2 cos(2\u03c0iN )}N\u22121i=0 , substituting each case in (10), and taking the limit over N , the proof follows immediately.\nTo study the effect of communication let us consider the estimator (4). Under purview of Theorem 5 and Corollary 6, the ratio of the steady state MSD for a complete network (13) versus a fully disconnected network(P = IN ) can be computed as\nlim N\u2192\u221e \u02c6MSDKN \u02c6MSDdisconnected = \u03c32r + a 2\u03b12\u03c32w(1\u2212 a2(1\u2212 \u03b1)2) \u03c32r + a 2\u03b12\u03c32w \u2248 1\u2212 a2(1\u2212 \u03b1)2,\nfor \u03c32r \u226a \u03c32w . The ratio above can get arbitrary close to zero which, indeed, highlights the influence of communication on the learning quality.\nWe now consider Kalman Filter(KF) [14] as the optimal centralized counterpart of (5). It is wellknown that the steady state KF satisfies a Riccati equation, and when the parameter of interest is scalar, the Riccati equation simplifies to a quadratic with the positive root\n\u03a3KF = a2\u03c32w \u2212 \u03c32w +N\u03c32r +\n\u221a\n(a2\u03c32w \u2212 \u03c32w +N\u03c32r)2 + 4N\u03c32w\u03c32r 2N .\nTherefore, comparing with the complete graph (16), we have\nlim N\u2192\u221e\n\u03a3KF = \u03c3 2 r \u2264 \u03c32r 1\u2212 a2(1\u2212 \u03b1)2 ,\nand the upper bound can be made tight by choosing \u03b1 = 1 for |a| < 1|\u03bbN (P )\u22121| . If |a| \u2265 1 |\u03bbN (P )\u22121| we should choose an \u03b1 < 1 to preserve unbiasedness as well.\nOn the other hand, to evaluate the performance of estimator (4), we consider the upper bound\nMSDBound = \u03c32r + \u03b1 2\u03c32w \u03b1 , (17)\nderived in [8], for a = 1 via a distributed estimation scheme. For simplicity, we assume \u03c32w = \u03c3 2 r = \u03c32, and let \u03b2 in (15) be any diminishing function of N . Optimizing (13), (14), (15), and (17) over \u03b1, we obtain\nlim N\u2192\u221e \u02c6MSDKN \u2248 1.55\u03c32 < lim N\u2192\u221e \u02c6MSDSN = lim N\u2192\u221e \u02c6MSDCN \u2248 1.62\u03c32 < MSDBound = 2\u03c32,\nwhich suggests a noticeable improvement in learning even in the star and cycle networks where the number of individuals and connections are in the same order.\nRegret Analysis\nWe now turn to finite-time regret analysis of our methods. The average loss of all agents in predicting the state, up until time T , is\n1\nT\nT\u2211\nt=1\n1\nN\nN\u2211\ni=1\n(x\u0302i,t \u2212 xt)2 = 1\nT\nT\u2211\nt=1\n1\nN Tr(\u03be\u0302t\u03be\u0302Tt ) .\nAs motivated earlier, it is not possible, in general, to drive this average loss to zero, and we need to subtract off the limit. We thus define regret as\nRT , 1\nT\nT\u2211\nt=1\n1\nN Tr(\u03be\u0302t\u03be\u0302Tt )\u2212\n1\nT\nT\u2211\nt=1\n1\nN Tr(\u03a3\u0302) =\n1\nN Tr\n(\n1\nT\nT\u2211\nt=1\n\u03be\u0302t\u03be\u0302 T t \u2212 \u03a3\u0302 ) ,\nwhere \u03a3\u0302 is from Definition 4. We then have for the spectral norm \u2016 \u00b7 \u2016 that\nRT \u2264 \u2225 \u2225 \u2225 \u2225 \u2225 1 T T\u2211\nt=1\n\u03bet\u03be T t \u2212 \u03a3 \u2225 \u2225 \u2225 \u2225 \u2225 , (18)\nwhere we dropped the distinguishing notation between the two estimators since the analysis works for both of them. We, first, state a technical lemma from [16] that we invoke later for bounding the quantity RT . For simplicity, we assume that magnitudes of both innovation and observation noise are bounded.\nLemma 7. Let {st}Tt=1 be an independent family of vector valued random variables, and let H be a function that maps T variables to a self-adjoint matrix of dimension N . Consider a sequence {At}Tt=1 of fixed self-adjoint matrices that satisfy\n( H(\u03c91, ..., \u03c9t, ..., \u03c9T )\u2212H(\u03c91, ..., \u03c9\u2032t, ..., \u03c9T ) )2 A2t ,\nwhere \u03c9i and \u03c9\u2032i range over all possible values of si for each index i. Letting Var = \u2016 \u2211T t=1 A 2 t\u2016, for all c \u2265 0, we have\nP { \u2225 \u2225H(s1, ..., sT )\u2212 E[H(s1, ..., sT )] \u2225 \u2225 \u2265 c } \u2264 Ne\u2212c2/8Var.\nTheorem 8. Under conditions of Theorem 5 together with boundedness of noise maxt\u2264T \u2016st\u2016 \u2264 s for some s > 0, the regret function defined in (18) satisfies\nRT \u2264 1\nT ( \u2016\u03be0\u20162 1\u2212 \u03c12(Q) ) + 1 T ( 2s\u2016\u03be0\u2016\n( 1\u2212 \u03c1(Q) )2\n)\n+ 1\nT\n( s2\n( 1\u2212 \u03c12(Q) )2\n)\n+ 1\u221a T\n8s2 \u221a\n2 log N\u03b4\n(1 \u2212 \u03c1(Q))2 ,\n(19)\nwith probability at least 1\u2212 \u03b4.\nWe mention that results that are similar in spirit have been studied for general unbounded stationary ergodic time series in [17\u201319] by employing techniques from the online learning literature. On the other hand, our problem has the network structure and the specific evolution of the hidden state, not present in the above works."}, {"heading": "4 The Impact of New Friendships on Social Learning", "text": "In the social learning model we proposed, agents are cooperative and they aim to accomplish a global objective. In this direction, the network structure contributes substantially to the learning process. In this section, we restrict our attention to estimator (5), and characterize the intuitive idea that making(losing) friendships can influence the quality of learning in the sense of decreasing(increasing) the steady state MSD of the network.\nTo commence, letting ei denote the i-th unit vector in the standard basis of RN , we exploit the negative semi-definite, edge function matrix\n\u2206P (i, j) , \u2212(ei \u2212 ej)(ei \u2212 ej)T, (20) for edge addition(removal) to(from) the graph. Essentially, if there is no connection between agents i and j,\nP\u01eb , P + \u01eb\u2206P (i, j), (21)\nfor \u01eb < min{pii, pjj}, corresponds to a new communication matrix adding the edge {i, j} with a weight \u01eb to the network G, and subtracting \u01eb from self-reliance of agents i and j. Proposition 9. Let G\u2212 be the network resulted by removing the bidirectional edge {i, j} with the weight \u01eb from the network G, so P\u2212\u01eb and P denote the communication matrices associated to G\u2212 and G, respectively. Given Assumption 1, for a fixed signal weight \u03b1 the following relationship holds\n\u02dcMSD(P, \u03b1) \u2264 \u02dcMSD(P\u2212\u01eb, \u03b1), (22) as long as P is positive semi-definite, and |a| < 1|\u03b1| .\nUnder a mild technical assumption, Proposition 9 suggests that losing connections monotonically increases the MSD, and individuals tend to maintain their friendships to obtain a lower MSD as a global objective. However, this does not elaborate on the existence of individuals with whom losing or making connections could have an immense impact on learning. We bring this concept to light in the following proposition with finding a so-called optimal edge which provides the most MSD reduction, in case it is added to the network graph.\nProposition 10. Given Assumption 1, a positive semi-definite P , and |a| < 1|\u03b1| , to find the optimal edge with a pre-assigned weight \u01eb \u226a 1 to add to the network G, we need to solve the following optimization problem\nmin {i,j}/\u2208E\nN\u2211\nk=1\n(\nhk(i, j) , zk(i, j)\n( 2(1\u2212 \u03b12a2)\u03bbk(P ) + 2a2\u03b1\u03bb2k(P ) )\n( 1\u2212 a2(\u03bbk(P )\u2212 \u03b1)2 )2\n)\n, (23)\nwhere\nzk(i, j) , (v T k\u2206P (i, j)vk)\u01eb, (24)\nand {vk}Nk=1 are the right eigenvectors of P . In addition, letting \u03b6max = maxk>1 |\u03bbk(P )\u2212 \u03b1|,\nmin {i,j}/\u2208E\nN\u2211\nk=1\nhk(i, j) \u2265 min {i,j}/\u2208E\n\u22122\u01eb ( (1\u2212 \u03b12a2)(pii + pjj) + a2\u03b1([P 2]ii + [P 2]jj \u2212 2[P 2]ij) )\n( 1\u2212 a2\u03b62max\n)2 .\n(25)\nProof. Representing the first order approximation of \u03bbk(P\u01eb) using definition of zk(i, j) in (24), we have \u03bbk(P\u01eb) \u2248 \u03bbk(P ) + zk(i, j) for \u01eb \u226a 1. Based on Theorem 5, we now derive\n\u02dcMSD(P\u01eb, \u03b1)\u2212 \u02dcMSD(P, \u03b1) \u221d N \u2211\nk=1\n( \u03bbk(P\u01eb)\u2212 \u03bbk(P ) )( (1\u2212 \u03b12a2)(\u03bbk(P\u01eb) + \u03bbk(P )) + 2a 2\u03b1\u03bbk(P )\u03bbk(P\u01eb) )\n( 1\u2212 a2(\u03bbk(P )\u2212 \u03b1)2 )( 1\u2212 a2(\u03bbk(P\u01eb)\u2212 \u03b1)2 )\n\u2248\nN \u2211\nk=1\nzk(i, j) ( 2(1\u2212 \u03b12a2)\u03bbk(P ) + 2a 2\u03b1\u03bb2k(P ) + (1\u2212 \u03b1 2a2 + 2a2\u03b1\u03bbk(P ))zk(i, j) )\n( 1\u2212 a2(\u03bbk(P )\u2212 \u03b1)2 )( 1\u2212 a2(\u03bbk(P )\u2212 \u03b1+ zk(i, j))2 )\n=\nN \u2211\nk=1\nzk(i, j) ( 2(1\u2212 \u03b12a2)\u03bbk(P ) + 2a 2\u03b1\u03bb2k(P ) )\n( 1\u2212 a2(\u03bbk(P )\u2212 \u03b1)2 )\n2 +O(\u01eb2),\nnoting that zk(i, j) is O(\u01eb) from the definition (24). Minimizing \u02dcMSD(P\u01eb, \u03b1) \u2212 \u02dcMSD(P, \u03b1) is, hence, equivalent to optimization (23) when \u01eb \u226a 1. Taking into account that P is positive semidefinite, zk(i, j) \u2264 0 for k \u2265 2, and v1 = 1N/ \u221a N which implies z1(i, j) = 0, we proceed to the lower bound proof using the definition of hk(i, j) and \u03b6max in the statement of the proposition, as follows\nN\u2211\nk=1\nhk(i, j) =\nN\u2211\nk=2\nzk(i, j) ( 2(1\u2212 \u03b12a2)\u03bbk(P ) + 2a2\u03b1\u03bb2k(P ) )\n( 1\u2212 a2(\u03bbk(P )\u2212 \u03b1)2 )2\n\u2265 1( 1\u2212 a2\u03b62max )2\nN\u2211\nk=2\nzk(i, j) ( 2(1\u2212 \u03b12a2)\u03bbk(P ) + 2a2\u03b1\u03bb2k(P ) ) .\nSubstituting zk(i, j) from (24) to above, we have\nN\u2211\nk=1\nhk(i, j) \u2265 2\u01eb\n( 1\u2212 a2\u03b62max )2\n( N\u2211\nk=1\n( vTk\u2206P (i, j)vk )( (1\u2212 \u03b12a2)\u03bbk(P ) + a2\u03b1\u03bb2k(P )\n) )\n= 2\u01eb\n( 1\u2212 a2\u03b62max\n)2 Tr\n(\n\u2206P (i, j)\nN\u2211\nk=1\n( (1 \u2212 \u03b12a2)\u03bbk(P ) + a2\u03b1\u03bb2k(P ) ) vkv T k\n)\n= 2\u01eb\n( 1\u2212 a2\u03b62max\n)2 Tr\n(\n\u2206P (i, j) ( (1\u2212 \u03b12a2)P + a2\u03b1P 2\n) )\n.\nUsing the facts that Tr(\u2206P (i, j)P ) = \u2212pii\u2212pjj+2pij and Tr(\u2206P (i, j)P 2) = \u2212[P 2]ii\u2212 [P 2]jj + 2[P 2]ij according to definition of \u2206P (i, j) in (20), and pij = 0 since we are adding a non-existent edge {i, j}, the lower bound (25) is derived.\nBeside posing the optimal edge problem as an optimization, Proposition 10 also provides an upper bound for the best improvement that making a friendship brings to the network. In view of (25), forming a connection between two agents with more self-reliance and less common neighbors, minimizes the lower bound, which offers the most maneuver for MSD reduction."}, {"heading": "5 Conclusion", "text": "We studied a distributed online learning problem over a social network. The goal of agents is to estimate the underlying state of the world which follows a geometric random walk. Each individual receives a noisy signal about the underlying state at each time period, so she communicates with her neighbors to recover the true state. We viewed the problem with an optimization lens where agents want to minimize a global loss function in a collaborative manner. To estimate the true state, we proposed two methodologies derived from a different decomposition of the global objective. Given the structure of the network, we provided a tight upper bound on the rate of change of the parameter which allows agents to follow the state with a bounded variance. Moreover, we computed the averaged, steady state, mean-square deviation of the estimates from the true state. The key observation was optimality of one of the estimators indicating the dependence of learning quality on the decomposition. Furthermore, defining the regret as the average of errors in the process of learning during a finite time T , we demonstrated that the regret function of the proposed algorithms decays with a rate O(1/ \u221a T ). Finally, under mild technical assumptions, we characterized the influence of network pattern on learning by observing that each connection brings a monotonic decrease in the MSD."}, {"heading": "Acknowledgments", "text": "We gratefully acknowledge the support of AFOSR MURI CHASE, ONR BRC Program on Decentralized, Online Optimization, NSF under grants CAREER DMS-0954737 and CCF-1116928, as well as Dean\u2019s Research Fund."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>This paper addresses the problem of online learning in a dynamic setting. We<lb>consider a social network in which each individual observes a private signal about<lb>the underlying state of the world and communicates with her neighbors at each<lb>time period. Unlike many existing approaches, the underlying state is dynamic,<lb>and evolves according to a geometric random walk. We view the scenario as an<lb>optimization problem where agents aim to learn the true state while suffering the<lb>smallest possible loss. Based on the decomposition of the global loss function, we<lb>introduce two update mechanisms, each of which generates an estimate of the true<lb>state. We establish a tight bound on the rate of change of the underlying state, un-<lb>der which individuals can track the parameter with a bounded variance. Then, we<lb>characterize explicit expressions for the steady state mean-square deviation(MSD)<lb>of the estimates from the truth, per individual. We observe that only one of the<lb>estimators recovers the optimal MSD, which underscores the impact of the objec-<lb>tive function decomposition on the learning quality. Finally, we provide an upper<lb>bound on the regret of the proposed methods, measured as an average of errors in<lb>estimating the parameter in a finite time.", "creator": "LaTeX with hyperref package"}}}