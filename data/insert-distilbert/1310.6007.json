{"id": "1310.6007", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Oct-2013", "title": "Efficient Optimization for Sparse Gaussian Process Regression", "abstract": "we propose an efficient optimization class algorithm for selecting a subset of training data to induce negative sparsity for gaussian process regression. the algorithm estimates using an aggregate inducing set and the hyperparameters using a single objective, either the marginal likelihood or thereby a variational free energy. the space and time expectation complexity are linear in training set size, and the algorithm can be applied to large regression involving problems on discrete or continuous economic domains. empirical evaluation shows state - of - art performance in discrete mathematical cases and is competitive results in the continuous case.", "histories": [["v1", "Tue, 22 Oct 2013 18:44:29 GMT  (217kb,D)", "https://arxiv.org/abs/1310.6007v1", "To appear in NIPS 2013"], ["v2", "Tue, 5 Nov 2013 05:13:30 GMT  (529kb,D)", "http://arxiv.org/abs/1310.6007v2", "To appear in NIPS 2013"], ["v3", "Mon, 11 Nov 2013 08:21:58 GMT  (529kb,D)", "http://arxiv.org/abs/1310.6007v3", "To appear in NIPS 2013"]], "COMMENTS": "To appear in NIPS 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yanshuai cao", "marcus a brubaker", "david j fleet", "aaron hertzmann"], "accepted": true, "id": "1310.6007"}, "pdf": {"name": "1310.6007.pdf", "metadata": {"source": "CRF", "title": "Efficient Optimization for Sparse Gaussian Process Regression", "authors": ["Yanshuai Cao", "Marcus A. Brubaker", "David J. Fleet", "Aaron Hertzmann"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Gaussian Process (GP) learning and inference are computationally prohibitive with large datasets, having time complexitiesO(n3) andO(n2), where n is the number of training points. Sparsification algorithms exist that scale linearly in the training set size (see [9] for a review). They construct a low-rank approximation to the GP covariance matrix over the full dataset using a small set of inducing points. Some approaches select inducing points from training points [6, 7, 11, 12]. But these methods select the inducing points using ad hoc criteria; i.e., they use different objective functions to select inducing points and to optimize GP hyperparameters. More powerful sparsification methods [13, 14, 15] use a single objective function and allow inducing points to move freely over the input domain which are learned via gradient descent. This continuous relaxation is not feasible, however, if the input domain is discrete, or if the kernel function is not differentiable in the input variables. As a result, there are problems in myraid domains, like bio-informatics, linguistics and computer vision where current sparse GP regression methods are inapplicable or ineffective.\nWe introduce an efficient sparsification algorithm for GP regression. The method optimizes a single objective for joint selection of inducing points and GP hyperparameters. Notably, it optimizes either the marginal likelihood, or a variational free energy [14], exploiting the QR factorization of a partial Cholesky decomposition to efficiently approximate the covariance matrix. Because it chooses inducing points from the training data, it is applicable to problems on discrete or continuous input domains. To our knowledge, it is the first method for selecting discrete inducing points and hyperparameters that optimizes a single objective, with linear space and time complexity. It is shown to outperform other methods on discrete datasets from bio-informatics and computer vision. On continuous domains it is competitive with the Pseudo-point GP [13] (SPGP)."}, {"heading": "1.1 Previous Work", "text": "Efficient state-of-the-art sparsification methods are O(m2n) in time and O(mn) in space for learning. They compute the predictive mean and variance in time O(m) and O(m2). Methods based on continuous relaxation, when applicable, entail learning O(md) continuous parameters, where d is the input dimension. In the discrete case, combinatorial optimization is required to select the inducing points, and this is, in general, intractable. Existing discrete sparsification methods therefore use other criteria to greedily select inducing points [6, 7, 11, 12]. Although their criteria are justified,\nar X\niv :1\n31 0.\n60 07\nv3 [\ncs .L\nG ]\n1 1\nN ov\neach in their own way (e.g., [7, 11] take an information theoretic perspective), they are greedy and do not use the same objective to select inducing points and to estimate GP hyperparameters.\nThe variational formulation of Titsias [14] treats inducing points as variational parameters, and gives a unified objective for discrete and continuous inducing point models. In the continuous case, it uses gradient-based optimization to find inducing points and hyperparameters. In the discrete case, our method optimizes the same variational objective of Titsias [14], but is a significant improvement over greedy forward selection using the variational objective as selection criteria, or some other criteria. In particular, given the cost of evaluating the variational objective on all training points, Titsias [14] evaluates the objective function on a small random subset of candidates at each iteration, and then select the best element from the subset. This approximation is often slow to achieve good results, as we explain and demonstrate below in Section 4.1. The approach in [14] also uses greedy forward selection, which provides no way to refine the inducing set after hyperparameter optimization, except to discard all previous inducing points and restart selection. Hence, the objective is not guaranteed to decrease after each restart. By comparison, our formulation considers all candidates at each step, and revisiting previous selections is efficient, and guaranteed to decrease the objective or terminate.\nOur low-rank decomposition is inspired by the Cholesky with Side Information (CSI) algorithm for kernel machines [1]. We extend that approach to GP regression. First, we alter the form of the lowrank matrix factorization in CSI to be suitable for GP regression with full-rank diagonal term in the covariance. Second, the CSI algorithm selects inducing points in a single greedy pass using an approximate objective. We propose an iterative optimization algorithm that swaps previously selected points with new candidates that are guaranteed to lower the objective. Finally, we perform inducing set selection jointly with gradient-based hyperparameter estimation instead of the grid search in CSI. Our algorithm selects inducing points in a principled fashion, optimizing the variational free energy or the log likelihood. It does so with time complexity O(m2n), and in practice provides an improved quality-speed trade-off over other discrete selection methods."}, {"heading": "2 Sparse GP Regression", "text": "Let y \u2208 R be the noisy output of a function, f , of input x. Let X = {xi}ni=1 denote n training inputs, each belonging to input space D, which is not necessarily Euclidean. Let y \u2208 Rn denote the corresponding vector of training outputs. Under a full zero-mean GP, with the covariance function\nE[yiyj ] = \u03ba(xi,xj) + \u03c321[i = j] , (1)\nwhere \u03ba is the kernel function, 1[\u00b7] is the usual indicator function, and \u03c32 is the variance of the observation noise, the predictive distribution over the output f? at a test point x? is normally distributed. The mean and variance of the predictive distribution can be expressed as\n\u00b5? = \u03ba(x?) T ( K + \u03c32In )\u22121 y\nv2? = \u03ba(x?,x?)\u2212 \u03ba(x?) T ( K + \u03c32In )\u22121 \u03ba(x?)\nwhere In is the n \u00d7 n identity matrix, K is the kernel matrix whose ijth element is \u03ba(xi,xj), and \u03ba(x?) is the column vector whose ith element is \u03ba(x?,xi).\nThe hyperparameters of a GP, denoted \u03b8, comprise the parameters of the kernel function, and the noise variance \u03c32. The natural objective for learning \u03b8 is the negative marginal log likelihood (NMLL) of the training data, \u2212 log (P (y|X,\u03b8)), given up to a constant by\nEfull(\u03b8) = (y >(K+\u03c32In)\u22121y + log |K+\u03c32In| ) / 2 . (2)\nThe computational bottleneck lies in the O(n2) storage and O(n3) inversion of the full covariance matrix, K + \u03c32In. To lower this cost with a sparse approximation, Csato\u0301 and Opper [4] and Seeger et al. [11] proposed the Projected Process (PP) model, wherein a set of m inducing points are used to construct a low-rank approximation of the kernel matrix. In the discrete case, where the inducing points are a subset of the training data, with indices I \u2282 {1, 2, ..., n}, this approach amounts to replacing the kernel matrix K with the following Nystro\u0308m approximation [10]:\nK ' K\u0302 = K[:, I]K[I, I]\u22121K[I, :] (3) where K[:, I] denotes the sub-matrix of K comprising columns indexed by I, and K[I, I] is the sub-matrix of K comprising rows and columns indexed by I. We assume the rank of K is m or\nhigher so we can always find such rank-m approximations. The PP NMLL is then algebraically equivalent to replacing K with K\u0302 in Eq. (2), i.e.,\nE(\u03b8, I) = ( ED(\u03b8, I) + EC(\u03b8, I) ) /2 , (4)\nwith data term ED(\u03b8, I) = y>(K\u0302+\u03c32In)\u22121y, and model complexity EC(\u03b8, I) = log |K\u0302+\u03c32In|.\nThe computational cost reduction from O(n3) to O(m2n) associated with the new likelihood is achieved by applying the Woodbury inversion identity to ED(\u03b8, I) and EC(\u03b8, I). The objective in (4) can be viewed as an approximate log likelihood for the full GP model, or as the exact log likelihood for an approximate model, called the Deterministically Trained Conditional [9].\nThe same PP model can also be obtained by a variational argument, as in [14], for which the variational free energy objective can be shown to be Eq. (4) plus one extra term; i.e.,\nF (\u03b8, I) = ( ED(\u03b8, I) + EC(\u03b8, I) + EV(\u03b8, I) ) / 2 , (5)\nwhere EV (\u03b8, I) = \u03c3\u22122 tr(K\u2212K\u0302) arises from the variational formulation. It effectively regularizes the trace norm of the approximation residual of the covariance matrix. The kernel machine of [1] also uses a regularizer of the form \u03bb tr(K\u2212K\u0302), however \u03bb is a free parameter that is set manually."}, {"heading": "3 Efficient optimization", "text": "We now outline our algorithm for optimizing the variational free energy (5) to select the inducing set I and the hyperparameters \u03b8. (The negative log-likelihood (4) is similarly minimized by simply discarding the EV term.) The algorithm is a form of hybrid coordinate descent that alternates between discrete optimization of inducing points, and continuous optimization of the hyperparameters. We first describe the algorithm to select inducing points, and then discuss continuous hyperparameter optimization and termination criteria in Sec. 3.4.\nFinding the optimal inducing set is a combinatorial problem; global optimization is intractable. Instead, the inducing set is initialized to a random subset of the training data, which is then refined by a fixed number of swap updates at each iteration.1 In a single swap update, a randomly chosen inducing point is considered for replacement. If swapping does not improve the objective, then the original point is retained. There are n \u2212m potential replacements for each each swap update; the key is to efficiently determine which will maximally improve the objective. With the techniques described below, the computation time required to approximately evaluate all possible candidates and swap an inducing point is O(mn). Swapping all inducing points once takes O(m2n) time."}, {"heading": "3.1 Factored representation", "text": "To support efficient evaluation of the objective and swapping, we use a factored representation of the kernel matrix. Given an inducing set I of k points, for any k \u2264 m, the low-rank Nystro\u0308m approximation to the kernel matrix (Eq. 3) can be expressed in terms of a partial Cholesky factorization:\nK\u0302 = K[:, I]K[I, I]\u22121K[I, :] = L(I)L(I)> , (6)\nwhere L(I) \u2208 Rn\u00d7k is, up to permutation of rows, lower trapezoidal matrix (i.e., has a k \u00d7 k top lower triangular block, again up to row permutation). The derivation of Eq. 6 follows from Proposition 1 in [1], and the fact that, given the ordered sequence of pivots I, the partial Cholesky factorization is unique.\nUsing this factorization and the Woodbury identities (dropping the dependence on \u03b8 and I for clarity), the terms of the negative marginal log-likelihood (4) and variational free energy (5) become\nED = \u03c3\u22122 ( y>y \u2212 y>L ( L>L+ \u03c32I )\u22121 L>y ) (7)\nEC = log ( (\u03c32)n\u2212k|L>L+ \u03c32I| ) (8)\nEV = \u03c3\u22122(tr(K)\u2212 tr(L>L)) (9)\n1The inducing set can be incrementally constructed, as in [1], however we found no benefit to this.\nWe can further simplify the data term by augmenting the factor matrix as L\u0303 = [L>, \u03c3Ik]>, where Ik is the k\u00d7k identity matrix, and y\u0303 = [yT,0Tk ] T is the y vector with k zeroes appended:\nED = \u03c3\u22122 ( y>y \u2212 y\u0303>L\u0303 (L\u0303>L\u0303)\u22121 L\u0303>y\u0303 ) (10)\nNow, let L\u0303 = QR be a QR factorization of L\u0303, where Q \u2208 R(n+k)\u00d7k has orthogonal columns and R \u2208 Rk\u00d7k is invertible. The first two terms in the objective simplify further to\nED = \u03c3\u22122 ( \u2016y\u20162 \u2212 \u2016Q>y\u0303\u20162 ) (11)\nEC = (n\u2212 k) log(\u03c32) + 2 log |R| . (12)"}, {"heading": "3.2 Factorization update", "text": "Here we present the mechanics of the swap update algorithm, see [3] for pseudo-code. Suppose we wish to swap inducing point i with candidate point j in Im, the inducing set of size m. We first modify the factor matrices in order to remove point i from Im, i.e. to downdate the factors. Then we update all the key terms using one step of Cholesky and QR factorization with the new point j.\nDowndating to remove inducing point i requires that we shift the corresponding columns/rows in the factorization to the right-most columns of L\u0303, Q, R and to the last row of R. We can then simply discard these last columns and rows, and modify related quantities. When permuting the order of the inducing points, the underlying GP model is invariant, but the matrices in the factored representation are not. If needed, any two points in Im, can be permuted, and the Cholesky or QR factors can be updated in time O(mn). This is done with the efficient pivot permutation presented in the Appendix of [1], with minor modifications to account for the augmented form of L\u0303. In this way, downdating and removing i take O(mn) time, as does the updating with point j.\nAfter downdating, we have factors L\u0303m\u22121,Qm\u22121, Rm\u22121, and inducing set Im\u22121. To add j to Im\u22121, and update the factors to rank m, one step of Cholesky factorization is performed with point j, for which, ideally, the new columnto append to L\u0303 is formed as\n`m = (K\u2212K\u0302m\u22121)[:, j] /\u221a (K\u2212K\u0302m\u22121)[j, j] (13)\nwhere K\u0302m\u22121 = Lm\u22121Lm\u22121T. Then, we set L\u0303m = [L\u0303m\u22121 \u02dc\u0300m], where \u02dc\u0300m is just `m augmented with \u03c3em = [0, 0, ..., \u03c3, ..., 0, 0]>. The final updates are Qm = [Qm\u22121 qm], where qm is given by Gram-Schmidt orthogonalization, qm = ((I\u2212Qm\u22121Q>m\u22121)\u02dc\u0300m) / \u2016(I \u2212Qm\u22121Q>m\u22121)\u02dc\u0300m\u2016, and Rm is updated from Rm\u22121 so that L\u0303m = QmRm."}, {"heading": "3.3 Evaluating candidates", "text": "Next we show how to select candidates for inclusion in the inducing set. We first derive the exact change in the objective due to adding an element to Im\u22121. Later we will provide an approximation to this objective change that can be computed efficiently.\nGiven an inducing set Im\u22121, and matrices L\u0303m\u22121, Qm\u22121, andRm\u22121, we wish to evaluate the change in Eq. 5 for Im =Im\u22121 \u222a j. That is, \u2206F \u2261 F (\u03b8, Im\u22121)\u2212F (\u03b8, Im) = (\u2206ED + \u2206EC + \u2206EV )/2, where, based on the mechanics of the incremental updates above, one can show that\n\u2206ED = \u03c3\u22122(y\u0303> ( I \u2212Qm\u22121Q>m\u22121 ) \u02dc\u0300 m) 2 / \u2016 ( I \u2212Qm\u22121Q>m\u22121 ) \u02dc\u0300 m\u20162 (14)\n\u2206EC = log ( \u03c32 ) \u2212 log \u2016(I \u2212Qm\u22121Q>m\u22121)\u02dc\u0300m\u20162 (15) \u2206EV = \u03c3\u22122\u2016`m\u20162 (16)\nThis gives the exact decrease in the objective function after adding point j. For a single point this evaluation is O(mn), so to evaluate all n\u2212m points would be O(mn2)."}, {"heading": "3.3.1 Fast approximate cost reduction", "text": "While O(mn2) is prohibitive, computing the exact change is not required. Rather, we only need a ranking of the best few candidates. Thus, instead of evaluating the change in the objective exactly, we use an efficient approximation based on a small number, z, of training points which provide\ninformation about the residual between the current low-rank covariance matrix (based on inducing points) and the full covariance matrix. After this approximation proposes a candidate, we use the actual objective to decide whether to include it. The techniques below reduce the complexity of evaluating all n\u2212m candidates to O(zn). To compute the change in objective for one candidate, we need the new column of the updated Cholesky factorization, `m. In Eq. (13) this vector is a (normalized) column of the residual K \u2212 K\u0302m\u22121 between the full kernel matrix and the Nystro\u0308m approximation. Now consider the full Cholesky decomposition of K = L\u2217L\u2217> where L\u2217 = [Lm\u22121, L(Jm\u22121)] is constructed with Im\u22121 as the first pivots and Jm\u22121 = {1, ..., n}\\Im\u22121 as the remaining pivots, so the residual becomes K\u2212 K\u0302m\u22121 = L(Jm\u22121)L(Jm\u22121)>. We approximate L(Jm\u22121) by a rank z n matrix, Lz , by taking z points from Jm\u22121 and performing a partial Cholesky factorization of K\u2212 K\u0302m\u22121 using these pivots. The residual approximation becomes K\u2212 K\u0302m\u22121 \u2248 LzL>z , and thus `m \u2248 (LzL>z )[:, j] /\u221a (LzL>z )[j, j]. The pivots used to construct Lz are called information\npivots; their selection is discussed in Sec. 3.3.2.\nThe approximations to \u2206EDk , \u2206E C k and \u2206E V k , Eqs. (14)-(16), for all candidate points, involve the following terms: diag(LzL>z LzL > z ), y >LzL > z , and (Qk\u22121[1 : n, :]) > LzL > z . The first term can be computed in time O(z2n), and the other two in O(zmn) with careful ordering of matrix multiplications.2 Computing Lz costs O(z2n), but can be avoided since information pivots change by at most one when an information pivots is added to the inducing set and needs to be replaced. The techniques in Sec. 3.2 bring the associated update cost to O(zn) by updating Lz rather than recomputing it. These z information pivots are equivalent to the \u201clook-ahead\u201d steps of Bach and Jordan\u2019s CSI algorithm, but as described in Sec. 3.3.2, there is a more effective way to select them."}, {"heading": "3.3.2 Ensuring a good approximation", "text": "Selection of the information pivots determines the approximate objective, and hence the candidate proposal. To ensure a good approximation, the CSI algorithm [1] greedily selects points to find an approximation of the residual K\u2212 K\u0302m\u22121 in Eq. (13) that is optimal in terms of a bound of the trace norm. The goal, however, is to approximate Eqs. (14)-(16) . By analyzing the role of the residual matrix, we see that the information pivots provide a low-rank approximation to the orthogonal complement of the space spanned by current inducing set. With a fixed set of information pivots, parts of that subspace may never be captured. This suggests that we might occasionally update the entire set of information pivots. Although information pivots are changed when one is moved into the inducing set, we find empirically that this is not insufficient. Instead, at regular intervals we replace the entire set of information pivots by random selection. We find this works better than optimizing the information pivots as in [1].\nFigure 1 compares the exact and approximate cost reduction for candidate inducing points (left), and their respective rankings (right). The approximation is shown to work well. It is also robust to changes in the number of information pivots and the frequency of updates. When bad candidates are proposed, they are rejected after evaluating the change in the true objective. We find that rejection rates are typically low during early iterations (< 20%), but increase as opti-\nmization nears convergence (to 30% or 40%). Rejection rates also increase for sparser models, where each inducing point plays a more critical role and is harder to replace."}, {"heading": "3.4 Hybrid optimization", "text": "The overall hybrid optimization procedure performs block coordinate descent in the inducing points and the continuous hyperparameters. It alternates between discrete and continuous phases until improvement in the objective is below a threshold or the computational time budget is exhausted.\nIn the discrete phase, inducing points are considered for swapping with the hyper-parameters fixed. With the factorization and efficient candidate evaluation above, swapping an inducing point i \u2208 Im\n2Both can be further reduced to O(zn) by appropriate caching during the updates of Q,R and L\u0303, and Lz\nproceeds as follows: (I) down-date the factorization matrices as in Sec. 3.2 to remove i; (II) compute the true objective function value Fm\u22121 over the down-dated model with Im\\{i}, using (11), (12) and (9); (III) select a replacement candidate using the fast approximate cost change from Sec. 3.3.1; (IV) evaluate the exact objective change, using (14), (15), and (16); (V) add the exact change to the true objective Fm\u22121 to get the objective value with the new candidate. If this improves, we include the candidate in I and update the matrices as in Sec. 3.2. Otherwise it is rejected and we revert to the factorization with i; (VI) if needed, update the information pivots as in Secs. 3.3.1 and 3.3.2.\nAfter each discrete optimization step we fix the inducing set I and optimize the hyperparameters using non-linear conjugate gradients (CG). The equivalence in (6) allows us to compute the gradient with respect to the hyperparameters analytically using the Nystro\u0308m form. In practice, because we alternate each phase for many training epochs, attempting to swap every inducing point in each epoch is unnecessary, just as there is no need to run hyperparameter optimization until convergence. As long as all inducing set points are eventually considered we find that optimized models can achieve similar performance with shorter learning times."}, {"heading": "4 Experiments and analysis", "text": "For the experiments that follow we jointly learn inducing points and hyperparameters, a more challenging task than learning inducing points with known hyperparameters [11, 13]. For all but the 1D example, the number of inducing points swapped per epoch is min(60,m). The maximum number of function evaluations per epoch in CG hyperparameter optimization ismin(20,max(15, 2d)), where d is the number of continuous hyperparameters. Empirically we find the algorithm is robust to changes in these limits. We use two performance measures, (a) standardized mean square error (SMSE), 1N \u03a3 N t=1(y\u0302t \u2212 yt)2/\u03c3\u03022\u2217, where \u03c3\u03022\u2217 is the sample variance of test outputs {yt}, and (2) standardized negative log probability (SNLP) defined in [10]."}, {"heading": "4.1 Discrete input domain", "text": "We first show results on two discrete datasets with kernels that are not differentiable in the input variable x. Because continuous relaxation methods are not applicable, we compare to discrete selection methods, namely, random selection as baseline (Random), greedy subset-optimal selection of Titsias [14] with either 16 or 512 candidates (Titsias-16 and Titsias-512), and Informative Vector Machine [7] (IVM). For learning continuous hyperparameters, each method optimizes the same objective using non-linear CG. Care is taken to ensure consist initialization and termination criteria [3]. For our algorithm we use z = 16 information pivots with random selection (CholQR-z16).\nLater, we show how variants of our algorithm trade-off speed and performance. Additionally, we also compare to least-square kernel regression using CSI (in Fig. 3(c)).\nThe first discrete dataset, from bindingdb.org, concerns the prediction of binding affinity for a target (Thrombin), from the 2D chemical structure of small molecules (represented as graphs). We do 50-fold random splits to 3660 training points and 192 test points for repeated runs. We use a compound kernel, comprising 14 different graph kernels, and 15 continuous hyperparameters (one noise variance and 14 data variances). In the second task, from [2], the task is to predict 3D human joint position from histograms of HoG image features [5]. Training and test sets have 4819 and 4811 data points. Because our goal is the general purpose sparsification method for GP regression, we make no attempt at the more difficult problem of modelling the multivariate output structure in the regression as in [2]. Instead, we predict the vertical position of joints independently, using a histogram intersection kernel [8], having four hyperparameters: one noise variance, and three data variances corresponding to the kernel evaluated over the HoG from each of three cameras. We select and show result on the representative left wrist here (see [3] for others joints, and more details about the datasets and kernels used).\nThe results in Fig. 2 and 3 show that CholQR-z16 outperforms the baseline methods in terms of test-time predictive power with significantly lower training time. Titsias-16 and Titsias-512 shows similar test performance, but they are two to four orders of magnitude slower than CholQR-z16 (see Figs. 3(d) and 3(e)). Indeed, Fig. 3(a) shows that the training time for CholQR-z16 is comparable to IVM and Random selection, but with much better performance. The poor performance of Random selection highlights the importance of selecting good inducing points, as no amount of hyperparameter optimization can correct for poor inducing points. Fig. 3(a) also shows IVM to be somewhat slower due to the increased number of iterations needed, even though per epoch, IVM is faster than CholQR. When stopped earlier, IVM test performance further degrades.\nFinally, Fig. 3(c) and 3(f) show the trade-off between the test SMSE and training time for variants of CholQR, with baselines and CSI kernel regression [1]. For CholQR we consider different numbers of information pivots (denoted z8, z16, z64 and z128), and different strategies for their selection including random selection, optimization as in [1] (denote OI) and adaptively growing the information pivot set (denoted AA, see [3] for details). These variants of CholQR trade-off speed and performance (3(f)), all significantly outperform the other methods (3(c)); CSI, which uses grid search to select hyper-parameters, is slow and exhibits higher SMSE."}, {"heading": "4.2 Continuous input domain", "text": "Although CholQR was developed for discrete input domains, it can be competitive on continuous domains. To that end, we compare to SPGP [13] and IVM [7], using RBF kernels with one lengthscale parameter per input dimension; \u03ba(xi,xj) = c exp(\u22120.5 \u2211d t=1 bt(x (t) i \u2212 x (t) j )\n2). We show results from both the PP log likelihood and variational objectives, suffixed by MLE and VAR.\nWe use the 1D toy dataset of [13] to show how the PP likelihood with gradient-based optimization of inducing points is easily trapped in local minima. Fig. 4(a) and 4(d) show that for this dataset our algorithm does not get trapped when initialization is poor (as in Fig. 1c of [13]). To simulate the sparsity of data in high-dimensional problems we also down-sample the dataset to 20 points (every 10th point). Here CholQR out-performs SPGP (see Fig. 4(b), 4(e), and 4(c)). By comparison, Fig. 4(f) shows SPGP learned with a more uniform initial distribution of inducing points avoids this local optima and achieves a better negative log likelihood of 11.34 compared to 14.54 in Fig. 4(c).\nFinally, we compare CholQR to SPGP [13] and IVM [7] on a large dataset. KIN40K concerns nonlinear forward kinematic prediction. It has 8D real-valued inputs and scalar outputs, with 10K training and 30K test points. We perform linear de-trending and re-scaling as pre-processing. For SPGP we use the implementation of [13]. Fig. 5 shows that CholQR-VAR outperforms IVM in terms of SMSE and SNLP. Both CholQR-VAR and CholQR-MLE outperform SPGP in terms of SMSE on KIN40K with largem, but SPGP exhibits better SNLP. This disparity between the SMSE and SNLP measures for CholQR-MLE is consistent with findings about the PP likelihood in [14]."}, {"heading": "5 Conclusion", "text": "We describe an algorithm for selecting inducing points for Gaussian Process sparsification. It optimizes principled objective functions, and is applicable to discrete domains and non-differentiable kernels. On such problems it is shown to be as good as or better than competing methods and, for methods whose predictive behavior is similar, our method is several orders of magnitude faster. On continuous domains the method is competitive. Extension to the SPGP form of covariance approximation would be interesting future research."}], "references": [{"title": "Predictive low-rank decomposition for kernel methods", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "ICML, pp. 33\u201340,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Twin gaussian processes for structured prediction", "author": ["L. Bo", "C. Sminchisescu"], "venue": "IJCV, 87:28\u2013", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Project page: supplementary material and software for efficient optimization for sparse gaussian process regression", "author": ["Y. Cao", "M.A. Brubaker", "D.J. Fleet", "A. Hertzmann"], "venue": "www.cs. toronto.edu/ \u0303caoy/opt_sgpr,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Sparse on-line gaussian processes", "author": ["L. Csat\u00f3", "M. Opper"], "venue": "Neural Comput.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "IEEE CVPR, pp", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "A matching pursuit approach to sparse gaussian process regression", "author": ["S.S. Keerthi", "W. Chu"], "venue": "NIPS 18,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Fast sparse gaussian process methods: The informative vector machine", "author": ["N.D. Lawrence", "M. Seeger", "R. Herbrich"], "venue": "NIPS 15,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Libpmk: A pyramid match toolkit", "author": ["J.J. Lee"], "venue": "TR: MIT-CSAIL-TR-2008-17, MIT CSAIL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "A unifying view of sparse approximate gaussian process regression", "author": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1939}, {"title": "Gaussian processes for machine learning. Adaptive computation and machine learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Fast forward selection to speed up sparse gaussian process regression", "author": ["M. Seeger", "C.K.I. Williams", "N.D. Lawrence", "S.S. Dp"], "venue": "AI & Stats", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Sparse greedy gaussian process regression", "author": ["A.J. Smola", "P. Bartlett"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Sparse gaussian processes using pseudo-inputs", "author": ["Snelson", "Edward", "Ghahramani", "Zoubin"], "venue": "NIPS 18,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Variational learning of inducing variables in sparse gaussian processes", "author": ["M.K. Titsias"], "venue": "JMLR, 5:567\u2013574,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Sparse multiscale gaussian process regression", "author": ["Christian Walder", "Kwang In Kim", "Bernhard Sch\u00f6lkopf"], "venue": "ICML pp. 1112\u20131119,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}], "referenceMentions": [{"referenceID": 8, "context": "Sparsification algorithms exist that scale linearly in the training set size (see [9] for a review).", "startOffset": 82, "endOffset": 85}, {"referenceID": 5, "context": "Some approaches select inducing points from training points [6, 7, 11, 12].", "startOffset": 60, "endOffset": 74}, {"referenceID": 6, "context": "Some approaches select inducing points from training points [6, 7, 11, 12].", "startOffset": 60, "endOffset": 74}, {"referenceID": 10, "context": "Some approaches select inducing points from training points [6, 7, 11, 12].", "startOffset": 60, "endOffset": 74}, {"referenceID": 11, "context": "Some approaches select inducing points from training points [6, 7, 11, 12].", "startOffset": 60, "endOffset": 74}, {"referenceID": 12, "context": "More powerful sparsification methods [13, 14, 15] use a single objective function and allow inducing points to move freely over the input domain which are learned via gradient descent.", "startOffset": 37, "endOffset": 49}, {"referenceID": 13, "context": "More powerful sparsification methods [13, 14, 15] use a single objective function and allow inducing points to move freely over the input domain which are learned via gradient descent.", "startOffset": 37, "endOffset": 49}, {"referenceID": 14, "context": "More powerful sparsification methods [13, 14, 15] use a single objective function and allow inducing points to move freely over the input domain which are learned via gradient descent.", "startOffset": 37, "endOffset": 49}, {"referenceID": 13, "context": "Notably, it optimizes either the marginal likelihood, or a variational free energy [14], exploiting the QR factorization of a partial Cholesky decomposition to efficiently approximate the covariance matrix.", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "On continuous domains it is competitive with the Pseudo-point GP [13] (SPGP).", "startOffset": 65, "endOffset": 69}, {"referenceID": 5, "context": "Existing discrete sparsification methods therefore use other criteria to greedily select inducing points [6, 7, 11, 12].", "startOffset": 105, "endOffset": 119}, {"referenceID": 6, "context": "Existing discrete sparsification methods therefore use other criteria to greedily select inducing points [6, 7, 11, 12].", "startOffset": 105, "endOffset": 119}, {"referenceID": 10, "context": "Existing discrete sparsification methods therefore use other criteria to greedily select inducing points [6, 7, 11, 12].", "startOffset": 105, "endOffset": 119}, {"referenceID": 11, "context": "Existing discrete sparsification methods therefore use other criteria to greedily select inducing points [6, 7, 11, 12].", "startOffset": 105, "endOffset": 119}, {"referenceID": 6, "context": ", [7, 11] take an information theoretic perspective), they are greedy and do not use the same objective to select inducing points and to estimate GP hyperparameters.", "startOffset": 2, "endOffset": 9}, {"referenceID": 10, "context": ", [7, 11] take an information theoretic perspective), they are greedy and do not use the same objective to select inducing points and to estimate GP hyperparameters.", "startOffset": 2, "endOffset": 9}, {"referenceID": 13, "context": "The variational formulation of Titsias [14] treats inducing points as variational parameters, and gives a unified objective for discrete and continuous inducing point models.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "In the discrete case, our method optimizes the same variational objective of Titsias [14], but is a significant improvement over greedy forward selection using the variational objective as selection criteria, or some other criteria.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "In particular, given the cost of evaluating the variational objective on all training points, Titsias [14] evaluates the objective function on a small random subset of candidates at each iteration, and then select the best element from the subset.", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "The approach in [14] also uses greedy forward selection, which provides no way to refine the inducing set after hyperparameter optimization, except to discard all previous inducing points and restart selection.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "Our low-rank decomposition is inspired by the Cholesky with Side Information (CSI) algorithm for kernel machines [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "To lower this cost with a sparse approximation, Csat\u00f3 and Opper [4] and Seeger et al.", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "[11] proposed the Projected Process (PP) model, wherein a set of m inducing points are used to construct a low-rank approximation of the kernel matrix.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": ", n}, this approach amounts to replacing the kernel matrix K with the following Nystr\u00f6m approximation [10]: K ' K\u0302 = K[:, I]K[I, I]\u22121K[I, :] (3) where K[:, I] denotes the sub-matrix of K comprising columns indexed by I, and K[I, I] is the sub-matrix of K comprising rows and columns indexed by I.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "The objective in (4) can be viewed as an approximate log likelihood for the full GP model, or as the exact log likelihood for an approximate model, called the Deterministically Trained Conditional [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 13, "context": "The same PP model can also be obtained by a variational argument, as in [14], for which the variational free energy objective can be shown to be Eq.", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "The kernel machine of [1] also uses a regularizer of the form \u03bb tr(K\u2212K\u0302), however \u03bb is a free parameter that is set manually.", "startOffset": 22, "endOffset": 25}, {"referenceID": 0, "context": "6 follows from Proposition 1 in [1], and the fact that, given the ordered sequence of pivots I, the partial Cholesky factorization is unique.", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "The inducing set can be incrementally constructed, as in [1], however we found no benefit to this.", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "2 Factorization update Here we present the mechanics of the swap update algorithm, see [3] for pseudo-code.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "This is done with the efficient pivot permutation presented in the Appendix of [1], with minor modifications to account for the augmented form of L\u0303.", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "To ensure a good approximation, the CSI algorithm [1] greedily selects points to find an approximation of the residual K\u2212 K\u0302m\u22121 in Eq.", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "We find this works better than optimizing the information pivots as in [1].", "startOffset": 71, "endOffset": 74}, {"referenceID": 10, "context": "For the experiments that follow we jointly learn inducing points and hyperparameters, a more challenging task than learning inducing points with known hyperparameters [11, 13].", "startOffset": 167, "endOffset": 175}, {"referenceID": 12, "context": "For the experiments that follow we jointly learn inducing points and hyperparameters, a more challenging task than learning inducing points with known hyperparameters [11, 13].", "startOffset": 167, "endOffset": 175}, {"referenceID": 9, "context": "We use two performance measures, (a) standardized mean square error (SMSE), 1 N \u03a3 N t=1(\u0177t \u2212 yt)/\u03c3\u0302 \u2217, where \u03c3\u0302 \u2217 is the sample variance of test outputs {yt}, and (2) standardized negative log probability (SNLP) defined in [10].", "startOffset": 223, "endOffset": 227}, {"referenceID": 13, "context": "Because continuous relaxation methods are not applicable, we compare to discrete selection methods, namely, random selection as baseline (Random), greedy subset-optimal selection of Titsias [14] with either 16 or 512 candidates (Titsias-16 and Titsias-512), and Informative Vector Machine [7] (IVM).", "startOffset": 190, "endOffset": 194}, {"referenceID": 6, "context": "Because continuous relaxation methods are not applicable, we compare to discrete selection methods, namely, random selection as baseline (Random), greedy subset-optimal selection of Titsias [14] with either 16 or 512 candidates (Titsias-16 and Titsias-512), and Informative Vector Machine [7] (IVM).", "startOffset": 289, "endOffset": 292}, {"referenceID": 2, "context": "Care is taken to ensure consist initialization and termination criteria [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "In the second task, from [2], the task is to predict 3D human joint position from histograms of HoG image features [5].", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "In the second task, from [2], the task is to predict 3D human joint position from histograms of HoG image features [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "Because our goal is the general purpose sparsification method for GP regression, we make no attempt at the more difficult problem of modelling the multivariate output structure in the regression as in [2].", "startOffset": 201, "endOffset": 204}, {"referenceID": 7, "context": "Instead, we predict the vertical position of joints independently, using a histogram intersection kernel [8], having four hyperparameters: one noise variance, and three data variances corresponding to the kernel evaluated over the HoG from each of three cameras.", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "We select and show result on the representative left wrist here (see [3] for others joints, and more details about the datasets and kernels used).", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "3(c) and 3(f) show the trade-off between the test SMSE and training time for variants of CholQR, with baselines and CSI kernel regression [1].", "startOffset": 138, "endOffset": 141}, {"referenceID": 0, "context": "For CholQR we consider different numbers of information pivots (denoted z8, z16, z64 and z128), and different strategies for their selection including random selection, optimization as in [1] (denote OI) and adaptively growing the information pivot set (denoted AA, see [3] for details).", "startOffset": 188, "endOffset": 191}, {"referenceID": 2, "context": "For CholQR we consider different numbers of information pivots (denoted z8, z16, z64 and z128), and different strategies for their selection including random selection, optimization as in [1] (denote OI) and adaptively growing the information pivot set (denoted AA, see [3] for details).", "startOffset": 270, "endOffset": 273}, {"referenceID": 12, "context": "To that end, we compare to SPGP [13] and IVM [7], using RBF kernels with one lengthscale parameter per input dimension; \u03ba(xi,xj) = c exp(\u22120.", "startOffset": 32, "endOffset": 36}, {"referenceID": 6, "context": "To that end, we compare to SPGP [13] and IVM [7], using RBF kernels with one lengthscale parameter per input dimension; \u03ba(xi,xj) = c exp(\u22120.", "startOffset": 45, "endOffset": 48}, {"referenceID": 12, "context": "We use the 1D toy dataset of [13] to show how the PP likelihood with gradient-based optimization of inducing points is easily trapped in local minima.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "1c of [13]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 12, "context": "Finally, we compare CholQR to SPGP [13] and IVM [7] on a large dataset.", "startOffset": 35, "endOffset": 39}, {"referenceID": 6, "context": "Finally, we compare CholQR to SPGP [13] and IVM [7] on a large dataset.", "startOffset": 48, "endOffset": 51}, {"referenceID": 12, "context": "For SPGP we use the implementation of [13].", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "This disparity between the SMSE and SNLP measures for CholQR-MLE is consistent with findings about the PP likelihood in [14].", "startOffset": 120, "endOffset": 124}], "year": 2013, "abstractText": "We propose an efficient optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates an inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy. The space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-ofart performance in discrete cases and competitive results in the continuous case.", "creator": "LaTeX with hyperref package"}}}