{"id": "1311.2110", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2013", "title": "Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions", "abstract": "we investigate three related and important problems connected to machine learning : ` approximating accepting a submodular function everywhere, learning a submodular function ( in theory a pac - like setting [ 53 ] ), and constrained minimization of submodular functions. \" we show that the complexity of all three problems depends on the'curvature'of the submodular function, and provide lower and warmer upper bounds measurements that refine and improve previous results [ 3, 16, 18, 52 ]. our proof techniques are fairly effectively generic. we either use a black - box transformation of the function ( for approximation and learning ), or a transformation of algorithms to use an appropriate surrogate function ( for minimization ). curiously, curvature has been known to influence approximations for submodular maximization [ 7, 55 ], but its effect on minimization, approximation and directed learning stability has hitherto been open. we complete this picture, and also support our theoretical claims by expanding empirical results.", "histories": [["v1", "Fri, 8 Nov 2013 23:42:34 GMT  (58kb,D)", "http://arxiv.org/abs/1311.2110v1", "21 pages. A shorter version appeared in Advances of NIPS-2013"]], "COMMENTS": "21 pages. A shorter version appeared in Advances of NIPS-2013", "reviews": [], "SUBJECTS": "cs.DS cs.DM cs.LG", "authors": ["rishabh k iyer", "stefanie jegelka", "jeff a bilmes"], "accepted": true, "id": "1311.2110"}, "pdf": {"name": "1311.2110.pdf", "metadata": {"source": "CRF", "title": "Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions", "authors": ["Rishabh Iyer", "Stefanie Jegelka", "Jeff Bilmes"], "emails": ["rkiyer@uw.edu,", "stefje@eecs.berkeley.edu,", "bilmes@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "Submodularity is a pervasive and important property in the areas of combinatorial optimization, economics, operations research, and game theory. In recent years, submodularity\u2019s use in machine learning has begun to proliferate as well. A set function f : 2V \u2192 R over a finite set V = {1, 2, . . . , n} is submodular if for all subsets S, T \u2286 V , it holds that f(S) + f(T ) \u2265 f(S \u222a T ) + f(S \u2229 T ). Given a set S \u2286 V , we define the gain of an element j /\u2208 S in the context S as f(j|S) , f(S \u222a j) \u2212 f(S). A function f is submodular if it satisfies diminishing marginal returns, namely f(j|S) \u2265 f(j|T ) for all S \u2286 T, j /\u2208 T , and is monotone if f(j|S) \u2265 0 for all j /\u2208 S, S \u2286 V . The search for optimal algorithms for submodular optimization has seen substantial progress [15, 24, 4] in recent years, but is still an ongoing endeavor. The first polynomial-time algorithm used the ellipsoid method [19, 20], and several combinatorial algorithms followed [27, 22, 14, 23, 26, 48]. For a detailed summary, see [24]. Unlike submodular minimization, submodular maximization is NP hard. However, maximization problems admit constant-factor approximations [46, 51, 12, 4], often even in the constrained case [51, 41, 42, 6, 13, 5]. While submodularity, like convexity, occurs naturally in a wide variety of problems, recent studies have shown that in the general case, many submodular problems of interest are very hard: the problems of learning a submodular function or of submodular minimization under constraints do not even admit constant or logarithmic approximation factors in polynomial time [3, 17, 18, 25, 52]. These rather pessimistic results however stand in sharp contrast to empirical observations, which suggest that these lower bounds are specific to rather contrived classes of functions, whereas much better results can be achieved in many practically relevant cases. Given the increasing importance of submodular functions in machine learning, these observations\nar X\niv :1\n31 1.\n21 10\nv1 [\ncs .D\nS] 8\nN ov\nbeg the question of qualifying and quantifying properties that make sub-classes of submodular functions more amenable to learning and optimization. Indeed, limited prior work has shown improved results for constrained minimization and learning of sub-classes of submodular functions, including symmetric functions [3, 49], concave functions [17, 37, 47], label cost or covering functions [21, 57]. In this paper, we take additional steps towards addressing the above problems and show how the generic notion of the curvature \u2013 the deviation from modularity\u2013 of a submodular function determines both upper and lower bounds on approximation factors for many learning and constrained optimization problems. In particular, our quantification tightens the generic, function-independent bounds in [18, 3, 52, 17, 25] for many practically relevant functions. Previously, the concept of curvature has been used to tighten bounds for submodular maximization problems [7, 55]. Hence, our results complete a unifying picture of the effect of curvature on submodular problems. By quantifying the influence of curvature on other problems, we improve previous bounds in [18, 3, 52, 17, 25] for many functions used in applications. Curvature, moreover, does not rely on a specific functional form but generically only on the marginal gains. It allows a smooth transition between the \u2018easy\u2019 functions and the \u2018really hard\u2019 subclasses of submodular functions."}, {"heading": "2 Problem statements, definitions and background", "text": "Before stating our main results, we provide some necessary definitions and introduce a new concept, the curve normalized version of a submodular function. Throughout this paper, we assume that the submodular function f is defined on a ground set V of n elements, that it is nonnegative and f(\u2205) = 0. We also use normalized modular (or additive) functions w : 2V \u2192 R which are those that can be written as a sum of weights, w(S) = \u2211 i\u2208S w(i). We are concerned with the following three problems:\nProblem 1. (Approximation [18]) Given a submodular function f in form of a value oracle, find an approximation f\u0302 (within polynomial time and representable within polynomial space), such that for all X \u2286 V , it holds that f\u0302(X) \u2264 f(X) \u2264 \u03b11(n)f\u0302(X) for a polynomial \u03b11(n).\nProblem 2. (PMAC-Learning [3]) Given i.i.d training samples {(Xi, f(Xi)}mi=1 from a distribution D, learn an approximation f\u0302(X) that is, with probability 1\u2212 \u03b4, within a multiplicative factor of \u03b12(n) from f . PMAC learning is defined like PAC learning with the added relaxation that the function is, with high probability, approximated within a factor of \u03b12(n).\nProblem 3. (Constrained optimization [52, 17, 25, 35]) Minimize a submodular function f over a family C of feasible sets, i.e., minX\u2208C f(X).\nIn its general form, the approximation problem was first studied by Goemans et al. [18], who approximate any monotone submodular function to within a factor of O( \u221a n log n), with a lower bound of \u03b11(n) = \u2126( \u221a n/ log n). Building on this result, Balcan and Harvey [3] show how to PMAC-learn a monotone submodular function within a factor of \u03b12(n) = O( \u221a n), and prove a lower bound of \u2126(n1/3) for the learning problem. Subsequent work extends these results to sub-additive and fractionally sub-additive functions [2, 1]. Better learning results are possible for the subclass of submodular shells [45] and Fourier sparse set functions [50]. Very recently Devanur et al [11] investigated a related problem of approximating one class of submodular functions with another and they show how many non-monotone submodular functions can be approximated with simple directed graph cuts within a factor of n2/4 which is tight. They also consider problems of approximating symmetric submodular functions and other subclasses of submodular functions. Both Problems 1 and 2 have numerous applications in algorithmic game theory and economics [3, 18] as well as machine learning [3, 44, 45, 50, 34]. For example, applications like bundle pricing, predicting prices of objects or growth rates etc. often have diminishing returns and a natural problem is to estimate these functions [3]. Similarly in machine learning, a number of problems involving sensor placement, summarization and others [44, 30] can be modeled through submodular functions. Often in these scenarios we would want\nto explicitly approximate or learn the true objective. For example in the case of document summarization, we are given the ROUGE scores. Since this function is submodular [44], a natural application is to learn these functions for summarization tasks. Constrained submodular minimization arises in applications such as power assignment or transportation problems [38, 39, 56, 32] . In machine learning, it occurs, for instance, in the form of MAP inference in high-order graphical models [10, 54, 36] or in size-constrained corpus extraction [43]. Recent results show that almost all constraints make it hard to solve the minimization even within a constant factor [52, 16, 35]. Here, we will focus on the constraint of imposing a lower bound on the cardinality, and on combinatorial constraints where C is the set of all s-t paths, s-t cuts, spanning trees, or perfect matchings in a graph."}, {"heading": "2.1 Curvature of a Submodular function", "text": "A central concept in this work is the total curvature \u03baf of a submodular function f and the curvature \u03baf (S) with respect to a set S \u2286 V , defined as [7, 55]\n\u03baf = 1\u2212min j\u2208V f(j | V \\ j) f(j) , \u03baf (S) = 1\u2212min j\u2208S f(j|S\\j) f(j) . (1)\nWithout loss of generality, assume that f(j) > 0 for all j \u2208 V . This follows since, if there exists an element j \u2208 V such that f(j) = 0, we can safely remove element j from the ground set, since for every set X, f(j|X) = 0 (from submodularity), and including or excluding j does not make any difference to the cost function. We also define two alternate notions of curvature. Define \u03ba\u0302f (S) and \u03ba\u0303f (S) as,\n\u03ba\u0302f (S) = 1\u2212 \u2211 j\u2208S f(j|S\\j)\u2211 j\u2208S f(j) , \u03ba\u0303f (S) = 1\u2212 min T\u2286V f(T |S) + \u2211 j\u2208S\u222aT f(j|S \u222a T\\j) f(T )\n(2)\nThese different forms of curvature are closely related.\nProposition 2.1. For any monotone submodular function and set S \u2286 V ,\n\u03ba\u0302f (S) \u2264 \u03baf (S) \u2264 \u03ba\u0303f (S) \u2264 \u03baf (3)\nProof. It is easy to see that \u03baf (S) \u2264 \u03baf (V ) = \u03baf , by the fact that \u03baf (S) is a monotone-decreasing set function. To show that \u03baf (S) \u2264 \u03ba\u0303f (S), note that,\n\u03ba\u0303f (S) = min T\u2286V f(T |S) + \u2211 j\u2208S\u222aT f(j|S \u222a T\\j) f(T )\n\u2265 min T\u2286V :|T |=1 f(T |S) + \u2211 j\u2208S\u222aT f(j|S \u222a T\\j) f(T )\n\u2265 min{min j\u2208S f(j|S\\j) f(j) ,min j /\u2208S f(j|S) f(j) }\n\u2265 min j\u2208S f(j|S\\j) f(j)\n\u2265 1\u2212 \u03baf (S) (4)\nWe finally prove that \u03ba\u0302f (S) \u2264 \u03baf (S). Note that,\n1\u2212 \u03baf (S) = min j\u2208S f(j|S\\j) f(j)\n\u2264 f(j|S\\j) f(j) ,\u2200j \u2208 S\nAlso notice that,\n1\u2212 \u03ba\u0302f (S) = \u2211 j\u2208S f(j|S\\j)\u2211 j\u2208S f(j)\n\u2265 \u2211 j\u2208S(1\u2212 \u03baf (S))f(j)\u2211\nj\u2208S f(j)\n\u2265 1\u2212 \u03baf (S)\nHence, \u03ba\u0302f (S) \u2264 \u03baf (S).\nHence \u03ba\u0302f (S) is the tightest notion of curvature. In this paper, we shall see these different notions of curvature coming up in different bounds. A modular function has curvature \u03baf = 0, and a matroid rank function has maximal curvature \u03baf = 1. Intuitively, \u03baf measures how far away f is from being modular. Conceptually, curvature is distinct from the recently proposed submodularity ratio [9] that measures how far a function is from being submodular. Curvature has served to tighten bounds for submodular maximization problems, e.g., from (1\u2212 1/e) to 1\u03baf (1\u2212 e \u2212\u03baf ) for monotone submodular maximization subject to a cardinality constraint [7] or matroid constraints [55], and these results are tight. In fact, [55] showed that this result for submodular maximization holds for the tighter version of curvature \u03ba\u0303f (S\n\u2217), where S\u2217 is the optimal solution. In other words, the bound for the greedy algorithm of [55] can be tightened to 1\u03ba\u0303f (S\u2217) (1\u2212 e \u2212\u03ba\u0303f (S\u2217)). For submodular minimization, learning, and approximation, however, the role of curvature has not yet been addressed (an exception are the upper bounds in [32] for minimization). In the following sections, we complete the picture of how curvature affects the complexity of submodular maximization and minimization, approximation, and learning. The above-cited lower bounds for Problems 1\u20133 were established with functions of maximal curvature (\u03baf = 1) which, as we will see, is the worst case. By contrast, many practically interesting functions have smaller curvature, and our analysis will provide an explanation for the good empirical results observed with such functions [32, 44, 33]. An example for functions with \u03baf < 1 is the class of concave over modular functions that have been used in speech processing [44] and computer vision [36]. This class comprises, for instance,\nfunctions of the form f(X) = \u2211k i=1(wi(X))\na, for some a \u2208 [0, 1] and a nonnegative weight vectors wi. Such functions may be defined over clusters Ci \u2286 V , in which case the weights wi(j) are nonzero only if j \u2208 Ci [44, 36, 30]. A related quantity distinct from curvature that has been introduced in the machine learning community is the submodularity ratio [9]:\n\u03b3U,k(f) = min L\u2286U,S:|S|\u2264k,S\u2229L=\u2205 \u2211 x\u2208S f(x|L) f(S|L)\n(5)\nThis parameter shows the decay of approximation bounds when an algorithm for submodular maximization is applied to non-submodular functions. The submodularity ratio measures how \u201cclose\u201d f is to submodularity, and helps characterize theoretical bounds for functions which are approximately submodular. Curvature, by contrast, measures how close a submodular function to being modular."}, {"heading": "2.2 The Curve-normalized Polymatroid function", "text": "To analyze Problems 1 \u2013 3, we introduce the concept of a curve-normalized polymatroid1. Specifically, we define the \u03baf -curve-normalized version of f as\nf\u03ba(X) = f(X)\u2212 (1\u2212 \u03baf )\n\u2211 j\u2208X f(j)\n\u03baf (6)\n1A polymatroid function is a monotone increasing, nonnegative, submodular function satisfying f(\u2205) = 0.\nIf \u03baf = 0, then we set f \u03ba \u2261 0. We call f\u03ba the curve-normalized version of f because its curvature is \u03baf\u03ba = 1. The function f \u03ba allows us to decompose a submodular function f into a \u201cdifficult\u201d polymatroid function and an \u201ceasy\u201d modular part as f(X) = fdifficult(X) +measy(X) where fdifficult(X) = \u03baff \u03ba(X) and\nmeasy(X) = (1 \u2212 \u03baf ) \u2211 j\u2208X f(j). Moreover, we may modulate the curvature of given any function g with \u03bag = 1, by constructing a function f(X) , cg(X) + (1\u2212 c)|X| with curvature \u03baf = c but otherwise the same polymatroidal structure as g. Our curvature-based decomposition is different from decompositions such as that into a totally normalized function and a modular function [8]. Indeed, the curve-normalized function has some specific properties that will be useful later on :\nLemma 2.1. If f is monotone submodular with \u03baf > 0, then f(X) \u2264 \u2211 j\u2208X f(j), f(X) \u2265 (1\u2212 \u03baf ) \u2211 j\u2208X f(j). (7)\nProof. The inequalities follow from submodularity and monotonicity of f . The first part follows from the subadditivity of f . The second inequality follows since f(X) \u2265 \u2211 j\u2208X f(j|V \\j) \u2265 (1\u2212 \u03baf ) \u2211 j\u2208X f(j), since \u2200j \u2208 X, f(j|V \\j) \u2265 (1\u2212 \u03baf )f(j) by definition of \u03baf .\nLemma 2.2. If f is monotone submodular, then f\u03ba(X) in Eqn. (6) is a monotone non-negative submodular function. Furthermore, f\u03ba(X) \u2264 \u2211 j\u2208X f(j).\nProof. Submodularity of f\u03ba is evident from the definition. To show the monotonicity, it suffices to show that f(X)\u2212 (1\u2212 \u03baf ) \u2211 j\u2208X f(j) is monotone non-decreasing and non-negative submodular. To show it is non-decreasing, notice that \u2200j /\u2208 X, f(j|V \\j) \u2212 (1 \u2212 \u03baf )f(j) \u2265 0, since (1 \u2212 \u03baf )f(j) \u2264 f(j|V \\j) by the definition of \u03baf . Non-negativity follows from monotonicity and the fact that f\n\u03ba(\u2205) = 0. To show the second part, notice that f(X)\u2212(1\u2212\u03baf ) \u2211 j\u2208X f(j)\n\u03baf \u2264\n\u2211 j\u2208X f(j)\u2212(1\u2212\u03baf ) \u2211 j\u2208X f(j) \u03baf = \u2211 j\u2208X f(j)."}, {"heading": "2.3 A framework for curvature-dependent lower bounds.", "text": "The function f\u03ba will be our tool for analyzing the hardness of submodular problems. Previous informationtheoretic lower bounds for Problems 1\u20133 [16, 18, 25, 52] are independent of curvature and use functions with \u03baf = 1. These curvature-independent bounds are proven by constructing two essentially indistinguishable matroid rank functions h and fR, one of which depends on a random set R \u2286 V . One then argues that any algorithm would need to make a super-polynomial number of queries to the functions for being able to distinguish h and fR with high enough probability. The lower bound will be the ratio maxX\u2208C h(X)/f\nR(X). We extend this proof technique to functions with a fixed given curvature. To this end, we define the functions\nfR\u03ba (X) = \u03baff R(X) + (1\u2212 \u03baf )|X| and h\u03ba(X) = \u03bafh(X) + (1\u2212 \u03baf )|X|. (8)\nBoth of these functions have curvature \u03baf . This construction enables us to explicitly introduce the effect of curvature into information-theoretic bounds for all monotone submodular functions.\nMain results. The curve normalization (6) leads to refined upper bounds for Problems 1\u20133, while the curvature modulation (8) provides matching lower bounds. The following are some of our main results: for approximating submodular functions (Problem 1), we replace the known bound of \u03b11(n) = O( \u221a n log n) [18] by an improved curvature-dependent O( \u221a n logn\n1+( \u221a n logn\u22121)(1\u2212\u03baf ) ). We complement this with a lower bound\nof \u2126\u0303( \u221a n\n1+( \u221a n\u22121)(1\u2212\u03baf ) ). For learning submodular functions (Problem 2), we refine the known bound of\n\u03b12(n) = O( \u221a n) [3] in the PMAC setting to a curvature dependent bound of O(\n\u221a n\n1+( \u221a n\u22121)(1\u2212\u03baf ) ), with a lower\nbound of \u2126\u0303( n 1/3\n1+(n1/3\u22121)(1\u2212\u03baf ) ). Finally, Table 1 summarizes our curvature-dependent approximation bounds for\nconstrained minimization (Problem 3). These bounds refine many of the results in [16, 52, 25, 35]. In general, our new curvature-dependent upper and lower bounds refine known theoretical results whenever \u03baf < 1, in many cases replacing known polynomial bounds by a curvature-dependent constant factor 1/(1 \u2212 \u03baf ). Besides making these bounds precise, the decomposition and the curve-normalized version (6) are the basis for constructing tight algorithms that (up to logarithmic factors) achieve the lower bounds."}, {"heading": "3 Approximating submodular functions everywhere", "text": "We first address improved bounds for the problem of approximating a monotone submodular function everywhere. Previous work established \u03b1-approximations g to a submodular function f satisfying g(S) \u2264 f(S) \u2264 \u03b1g(S) for all S \u2286 V [18]. We begin with a theorem showing how any algorithm computing such an approximation may be used to obtain a curvature-specific, improved approximation. Note that the curvature of a monotone submodular function can be obtained within 2n+ 1 queries to f . The key idea of Theorem 3.1 is to only approximate the curved part of f , and to retain the modular part exactly.\nTheorem 3.1. Given a polymatroid function f with \u03baf < 1, let f \u03ba be its curve-normalized version defined in Equation (6), and let f\u0302\u03ba be a submodular function satisfying f\u0302\u03ba(X) \u2264 f\u03ba(X) \u2264 \u03b1(n)f\u0302\u03ba(X), for some X \u2286 V . Then the function f\u0302(X) , \u03baf f\u0302\u03ba(X) + (1\u2212 \u03baf ) \u2211 j\u2208X f(j) satisfies\nf\u0302(X) \u2264 f(X) \u2264 \u03b1(n) 1 + (\u03b1(n)\u2212 1)(1\u2212 \u03baf ) f\u0302(X) \u2264 f\u0302(X) 1\u2212 \u03baf . (9)\nThe above inequalities hold, even if we use an upper bound \u03ba\u0304f instead of the actual curvature \u03baf .\nProof. The first inequality follows directly from definitions. To show the second inequality, note that\nf\u0302\u03ba(X) \u2265 f \u03ba(X) \u03b1(n) , and therefore\nf(X) \u03baf f\u0302\u03ba(X) + (1\u2212 \u03baf ) \u2211 j\u2208X f(j) = \u03baff\n\u03ba(X) + (1\u2212 \u03baf ) \u2211 j\u2208X f(j)\n\u03baf f\u0302\u03ba(X) + (1\u2212 \u03baf ) \u2211 j\u2208X f(j)\n(10)\n\u2264 \u03baff\n\u03ba(X) + (1\u2212 \u03baf ) \u2211 j\u2208X f(j)\n\u03baf f\u03ba \u03b1(n) + (1\u2212 \u03baf ) \u2211 j\u2208X f(j)\n(11)\n= \u03b1(n) \u03baff\n\u03ba(X) + (1\u2212 \u03baf ) \u2211 j\u2208X f(j)\n\u03baff\u03ba + (1\u2212 \u03baf )\u03b1(n) \u2211 j\u2208X f(j)\n(12)\n= \u03b1(n)\n1 + (\u03b1(n)\u22121)(1\u2212\u03baf )\n\u2211 j\u2208X f(j)\n\u03baff\u03ba(X)+(1\u2212\u03baf ) \u2211 j\u2208X f(j)\n(13)\n\u2264 \u03b1(n) 1 + (\u03b1(n)\u2212 1)(1\u2212 \u03baf )\n(14)\nThe last inequality follows since \u03baff \u03ba(X) + (1 \u2212 \u03baf ) \u2211 j\u2208X f(j) \u2264 \u2211 j\u2208X f(j). The other inequalities in Eqn. (9) follow directly from the definitions. It is also easy to see that all the above inequalities will hold using an upper bound \u03ba\u0304f > \u03baf instead of \u03baf in the definition of the curve-normalized function. The bound in that case would be,\nf\u0302(X) \u2264 f(X) \u2264 \u03b1(n) 1 + (\u03b1(n)\u2212 1)(1\u2212 \u03ba\u0304f ) f\u0302(X) \u2264 f\u0302(X) 1\u2212 \u03ba\u0304f\n(15)\nwhere, f\u0302(X) = \u03ba\u0304f f\u0302 \u00af\u03ba(X) + (1\u2212 \u03ba\u0304f ) \u2211 j\u2208X f(j), f\u0302\n\u00af\u03ba is an approximation of f \u03ba\u0304 satisfying f\u0302 \u00af\u03ba(X) \u2264 f\u03ba(X) \u2264 \u03b1(n)f\u0302\u03ba(X) and,\nf \u03ba\u0304(X) = f(X)\u2212 (1\u2212 \u03ba\u0304f )\n\u2211 j\u2208X f(j)\n\u03ba\u0304f (16)\nTheorem 3.1 may be directly applied to tighten recent results on approximating submodular functions everywhere. An algorithm by Goemans et al. [18] computes an approximation to a polymatroid function f in polynomial time by approximating the submodular polyhedron via an ellipsoid. This approximation (which we call the ellipsoidal approximation) satisfies \u03b1(n) = O( \u221a n log n), and has the form \u221a wf (X) for a certain weight vector wf .\nTheorem 3.2 ([18]). For any polymatroid rank function f , one can compute a weight vector wf and correspondingly an approximation \u221a wf (X) via a polynomial number of oracle queries such that \u221a wf (X) \u2264 f(X) \u2264 O( \u221a n log n) \u221a wf (X).\nThe weights wf are computed via an ellipsoidal approximation of the submodular polyhedron [18]. Corollary 3.3 states that a tighter approximation is possible for functions with \u03baf < 1.\nCorollary 3.3. Let f be a polymatroid function with \u03baf < 1, and let \u221a wf\u03ba(X) be the ellipsoidal approx-\nimation to the \u03ba-curve-normalized version f\u03ba(X) of f . Then the function fea(X) = \u03baf \u221a wf\u03ba(X) + (1 \u2212\n\u03baf ) \u2211 j\u2208X f(j) satisfies\nfea(X) \u2264 f(X) \u2264 O ( \u221a n log n\n1 + ( \u221a n log n\u2212 1)(1\u2212 \u03baf )\n) fea(X). (17)\nIf \u03baf = 0, then the approximation is exact. This is not surprising since a modular function can be inferred exactly within O(n) oracle calls.\nProof. To compute fea, construct the function f\u03ba as in Equation (6), and apply the algorithm in [18] to construct the approximation \u221a wf\u03ba(X) such that \u221a wf\u03ba(X) \u2264 f\u03ba(X) \u2264 O( \u221a n log n) \u221a wf\u03ba(X). Note that\u221a\nwf\u03ba(X) is an approximation of f\u03ba and not f . Then define fea(X) , \u03baf (X)fea(X)+(1\u2212\u03baf ) \u2211 j\u2208X f(j)\nThe following lower bound shows that Corollary 3.3 is tight up to logarithmic factors. It refines the lower bound in [18] to include \u03baf .\nTheorem 3.4. Given a submodular function f with curvature \u03baf , there does not exist a (possibly randomized) polynomial-time algorithm that computes an approximation to f within a factor of n 1/2\u2212\n1+(n1/2\u2212 \u22121)(1\u2212\u03baf ) , for\nany > 0.\nProof. The information-theoretic proof uses a construction and argumentation similar to that in [18, 52], but perturbs the functions to have the desired curvature. In the following let \u03baf = \u03ba. Define two monotone submodular functions h\n\u03ba(X) = \u03bamin{|X|, \u03b1}+ (1\u2212 \u03ba)|X| and fR\u03ba (X) = \u03bamin{\u03b2 + |X \u2229 R\u0304|, |X \u2229 R|, \u03b1}+ (1 \u2212 \u03ba)|X|, where R \u2286 V is a random set of cardinality \u03b1. Let \u03b1 and \u03b2 be an integer such that \u03b1 = x \u221a n/5 and \u03b2 = x2/5 for an x2 = \u03c9(log n). Both h\u03ba and fR\u03ba have curvature equal to \u03baf = \u03ba. Using a Chernoff bound, one can then show that any algorithm that uses a polynomial number of queries can distinguish h\u03ba and fR\u03ba with probability only n\n\u2212\u03c9(1), and therefore cannot reliably distinguish the functions with a polynomial number of queries [52]. Therefore, any such algorithm will, with high probability, approximate h\u03ba and fR\u03ba by the same function f\u0302 . Since the approximation must hold for both functions, the approximation factor must satisfy h\u03ba(R) \u2264 \u03b3f\u0302(R) \u2264 \u03b3fR\u03ba (R), and is therefore lower bounded by h \u03ba(R)/fR\u03ba (R). Given an arbitrary > 0, set x 2 = n2 = \u03c9(log n). Then\nh\u03ba(R) fR\u03ba (R) =\n\u03b1\n(1\u2212 \u03ba)\u03b1+ \u03ba\u03b2 (18)\n= n1/2+\n(1\u2212 \u03ba)n1/2+ + \u03ban2 (19)\n= n1/2\u2212\n1 + (n1/2\u2212 \u2212 1)(1\u2212 \u03ba) . (20)\nAssume there was an algorithm that generates an approximation f\u0302 \u2032 with approximation factor \u03b3\u2032 < \u03b3. This would imply that h\u03ba(R)/fR\u03ba (R) < \u03b3 \u2032, but this contradicts the above derivation.\nThe simplest alternative approximation to f one might conceive is the modular function f\u0302m(X) , \u2211 j\u2208X f(j) which can easily be computed by querying the n values f(j).\nLemma 3.1. Given a monotone submodular function f , it holds that\nf(X) \u2264 f\u0302m(X) = \u2211 j\u2208X f(j) \u2264 |X| 1 + (|X| \u2212 1)(1\u2212 \u03baf (X)) f(X) (21)\nMoreover, it also holds that,\nf(X) \u2264 f\u0302m(X) = \u2211 j\u2208X f(j) \u2264 |X| 1 + (|X| \u2212 1)(1\u2212 \u03ba\u0302f (X)) f(X) (22)\nProof. We first show the result for \u03ba\u0302f (X), and since it is a stronger notion of curvature, the bound will hold for \u03baf (X) as well. We shall use the following facts, which follow from the definitions of submodularity and curvature.\nFact 1: (1\u2212 \u03ba\u0302f (X)) \u2211 j\u2208X f(j) = \u2211 j\u2208X f(j|X\\j), (23)\nFact 2: f(X)\u2212 f(k) \u2265 \u2211 j\u2208X\\k f(j|X\\j),\u2200k \u2208 X. (24)\nSum the expressions from Fact 2, \u2200k \u2208 X, use Fact 1, and we obtain the following series of inequalities,\n|X|f(X)\u2212 \u2211 k\u2208X f(k) \u2265 \u2211 k\u2208X \u2211 j\u2208X\\k f(j|X\\j)\n\u2265 \u2211 k\u2208X \u2211 j\u2208X f(j|X\\j)\u2212 \u2211 k\u2208X f(j|X\\k)\n\u2265 (|X| \u2212 1) \u2211 j\u2208X\\k f(j|X\\j)\n\u2265 (|X| \u2212 1)(1\u2212 \u03ba\u0302f (X)) \u2211 k\u2208X f(k)\nHence we obtain that, \u2211 k\u2208X f(k) \u2264 |X| 1 + (|X| \u2212 1)(1\u2212 \u03ba\u0302f (X)) f(X)\nFrom the fact that 1\u2212 \u03ba\u0302f (X) \u2265 1\u2212 \u03baf (X), it immediately follows that,\u2211 k\u2208X f(k) \u2264 |X| 1 + (|X| \u2212 1)(1\u2212 \u03baf (X)) f(X)\nThe form of Lemma 3.1 is slightly different from Corollary 3.3. However, there is a straightforward correspondence: given f\u0302 such that f\u0302(X) \u2264 f(X) \u2264 \u03b1\u2032(n)f\u0302(X), by defining f\u0302 \u2032(X) = \u03b1\u2032(n)f\u0302(X), we get that f(X) \u2264 f\u0302 \u2032(X) \u2264 \u03b1\u2032(n)f(X). Lemma 3.1 for the modular approximation is complementary to Corollary 3.3: First, the modular approximation is better whenever |X| \u2264 \u221a n. Second, the bound in Lemma 3.1 depends on the curvature \u03baf (X) with respect to the set X, which is stronger than \u03baf . Third, f\u0302 m is extremely simple to compute. For sets of larger cardinality, however, the ellipsoidal approximation of Corollary 3.3 provides a better approximation, in fact, the best possible one (Theorem 3.4). In a similar manner, Lemma 3.1 is tight for any modular approximation to a submodular function:\nLemma 3.2. For any \u03ba > 0, there exists a monotone submodular function f with curvature \u03ba such that no modular upper bound on f ,f\u0302(X) = \u2211 j\u2208X w(j) \u2265 f(X),\u2200X \u2286 V , can approximate f(X) to a factor better than |X|1+(|X|\u22121)(1\u2212\u03baf ) .\nProof. Let f\u03ba(X) = \u03bamin{|X|, 1}+ (1 \u2212 \u03ba)|X|. Then f\u03ba(X) = \u03ba + (1 \u2212 \u03ba)|X| = 1 + (1 \u2212 \u03ba)(|X| \u2212 1) for all \u2205 \u2282 X \u2286 V . Since f\u0302m is an upper bound, it must satisfy f\u0302(j) = w(j) \u2265 1 for all j \u2208 V . Therefore, f\u0302m(X) = |X|, X 6= \u2205.\nThe improved curvature dependent bounds immediately imply better bounds for the class of concave over modular functions used in [44, 36, 30].\nCorollary 3.5. Given weight vectors w1, \u00b7 \u00b7 \u00b7 , wk \u2265 0, and a submodular function f(X) = \u2211k i=1 \u03bbi[wi(X)]\na, \u03bbi \u2265 0, for a \u2208 (0, 1), it holds that f(X) \u2264 \u2211 j\u2208X f(j) \u2264 |X|1\u2212af(X)\nProof. We first show this result independent of curvature, and then show how the curvature dependent bound also implies this improved bound. First define f(X) = [w(X)]a, for a \u2208 (0, 1] and w \u2265 0. since xa is a concave function for a \u2208 (0, 1], we have from Jensen\u2019s inequality that, given x1, x2, \u00b7 \u00b7 \u00b7 , xn \u2265 0,\u2211n\ni=1 x a i n \u2264 (\u2211n i=1 xi n )a Notice that, when f(X) = [w(X)]a, we have that f(i) = w(i)a. Hence \u2211 j\u2208X f(i) = \u2211 i\u2208X w(i)\na. Hence from the inequality above, it directly holds that,\u2211\ni\u2208X w(i) a\n|X| \u2264\n[ \u2211 i\u2208X w(i)] a\n|X|a\nand hence, \u2211 j\u2208X f(j) \u2264 |X|1\u2212af(X). This inequality also holds for a sum of concave over modular functions, since for each wi \u2265 0, we have \u2211 j\u2208X wi(j) a \u2264 |X|1\u2212awi(X)a. (25) Moreover, when f(X) = \u2211k i=1 \u03bbi[wi(X)] a, the modular upper bound \u2211 j\u2208X f(j) = \u2211 j\u2208X \u2211k i=1 \u03bbi[wi(j)]\na. Summing up eqn. (26) for all i, we have that,\nk\u2211 i=1 \u2211 j\u2208X \u03bbiwi(j) a \u2264 |X|1\u2212a k\u2211 i=1 \u03bbiwi(X) a. (26)\nWe next show that this result can also be seen from the curvature of the function. Lemma 3.3. Given weight vectors w1, \u00b7 \u00b7 \u00b7 , wk \u2265 0, and a submodular function f(X) = \u2211k i=1 \u03bbi[wi(X)]\na, \u03bbi \u2265 0, for a \u2208 (0, 1], it holds that,\n\u03ba\u0302f (X) \u2264 1\u2212 a\n|X|1\u2212a (27)\nProof. Again, let f(X) = [w(X)]a, for w \u2265 0 and a \u2208 (0, 1]. Then,\nf(j|X\\j) = f(X)\u2212 f(X\\j) = [w(X)]a \u2212 [w(X)\u2212 w(j)]a\n\u2265 aw(j) w(X)1\u2212a\nThe last inequality again holds due to concavity of g(x) = xa. In particular, for a concave function, g(y) \u2212 g(x) \u2264 g\u2032(x)(y \u2212 x), where g\u2032 is the derivative of g. Hence g(x) \u2265 g(y) + g\u2032(x)(x \u2212 y). Substitute y = w(X)\u2212 w(j), x = w(X) and g(x) = xa, and we get the above expression.\nHence we have,\n1\u2212 \u03ba\u0302f (X) = \u2211 j\u2208X f(j|X\\j)\u2211\nj\u2208X f(j) \u2265 a \u2211 j\u2208X w(j)w(X)\na\u22121\u2211 j\u2208X w(j) a\n\u2265 aw(X) a\u2211\nj\u2208X w(j) a\n\u2265 a |X|1\u2212a\nThe last inequality follows from the previous Lemma.\nHence from the curvature dependent bound, we obtain a slightly weaker bound, which still gives a O(|X|1\u2212a) bound for the modular upper bound. \u2211\nj\u2208X f(j) \u2264 1 1\u2212 \u03ba\u0302f (X) f(X)\n\u2264 |X| 1\u2212a\na f(X)\n\u2264 O(|X|1\u2212a)f(X)\nIn particular, when a = 1/2, the modular upper bound approximates the sum of square-root over modular functions by a factor of \u221a |X|."}, {"heading": "4 Learning Submodular functions", "text": "We next address the problem of learning submodular functions in a PMAC setting [3]. The PMAC (Probably Mostly Approximately Correct) framework is an extension of the PAC framework [53] to allow multiplicative errors in the function values from a fixed but unknown distribution D over 2V . We are given training samples {(Xi, f(Xi)}mi=1 drawn i.i.d. from D. The algorithm may take time polynomial in n, 1/ , 1/\u03b4 to compute a (polynomially-representable) function f\u0302 that is a good approximation to f with respect to D. Formally, f\u0302 must satisfy that\nPrX1,X2,\u00b7\u00b7\u00b7 ,Xm\u223cD [ PrX\u2208D[f\u0302(X) \u2264 f(X) \u2264 \u03b1(n)f\u0302(X)] \u2265 1\u2212 ] \u2265 1\u2212 \u03b4 (28)\nfor some approximation factor \u03b1(n). Balcan and Harvey [3] propose an algorithm that PMAC-learns any monotone, nonnegative submodular function within a factor \u03b1(n) = \u221a n+ 1 by reducing the problem to that of learning a binary classifier. If we assume that we have an upper bound on the curvature \u03baf , or that we can estimate it 2, and have access to the value of the singletons f(j), j \u2208 V , then we can obtain better learning results with non-maximal curvature:\nLemma 4.1. Let f be a monotone submodular function for which we know an upper bound on its curvature and the singleton weights f(j) for all j \u2208 V . For every , \u03b4 > 0 there is an algorithm that uses a polynomial number of training examples, runs in time polynomial in (n, 1/ , 1/\u03b4) and PMAC-learns f within a factor of\u221a n+1 1+( \u221a n+1\u22121)(1\u2212\u03baf ) . If D is a product distribution, then there exists an algorithm that PMAC-learns f within a factor of O( log 1\n1+(log 1 \u22121)(1\u2212\u03baf ) ).\n2note that \u03baf can be estimated from a set of 2n+ 1 samples {(j, f(j))}j\u2208V , {(V, f(V ))}, and {(V \\j, f(V \\j)}j\u2208V included in the training samples\nThe algorithm of Lemma 4.1 uses the reduction of Balcan and Harvey [3] to learn the \u03baf -curve-normalized version f\u03ba of f . From the learned function f\u0302\u03ba(X), we construct the final estimate f\u0302(X) , \u03baf f\u0302\u03ba(X) + (1\u2212 \u03baf ) \u2211 j\u2208X f(j). Theorem 3.1 implies Lemma 4.1 for this f\u0302(X).\nProof. The proof of this theorem directly follows from the results in [3] and those from section 3. The idea is that, we use the PMAC setting and algorithm from [3]. We use the same construction as section 3, and construct the function f\u03ba(X) which is the curve-normalized version of f . Let f\u0302\u03ba(X) be the function learn\nfrom f\u03ba using the algorithm from [3]. Then define f\u0302(X) = (1\u2212 \u03baf )f\u0302\u03ba(X) + \u03baf \u2211 j\u2208X f(j) and an analysis similar to that in section 3 conveys that the function f\u0302 is within a factor of \u221a n+1\n1+( \u221a n+1\u22121)(1\u2212\u03baf ) . Note that\nmoreover, whenever the bound f\u0302\u03ba(X) \u2264 f\u03ba(X) \u2264 \u221a n+ 1f\u0302\u03ba(X), the above curvature dependent bound will also hold. Hence the curvature dependent bound holds with high probability on a large measure of sets. The case for product distributions also follows from very similar lines and the results from [3].\nLemma 4.1 is tight as we show below.\nLemma 4.2. Given a class of submodular functions with curvature \u03baf , there does not exist a polynomial-time algorithm (which possibly even has information about \u03baf ) that is guaranteed to PMAC-learn f for every\n, \u03b4 > 0 within a factor of n 1/3\u2212 \u2032\n1+(n1/3\u2212 \u2032\u22121)(1\u2212\u03baf ) , for any \u2032 > 0.\nProof. Again, we use the same matroid functions used in [3]. Notice that the construction of [3], provides a family of matroids and a collection of sets B, with |A| = n1/3, such that f(A) = |A|, A \u2208 B and f(A) = \u03b2 = \u03c9(log n), A /\u2208 B. Again set \u03b2 = n \u2032 , and using a analysis and construction similar to the hardness proof of section 3 and Theorem 9 from [3] conveys that the lower bound for this problem is \u2126\u0303( n 1/3\n1+(n1/3\u22121)(1\u2212\u03baf ) )\nWe end this section by showing how we can learn with a construction analogous to that in Lemma 3.1.\nLemma 4.3. If f is a monotone submodular function with known curvature (or a known upper bound) \u03ba\u0302f (X),\u2200X \u2286 V , then for every , \u03b4 > 0 there is an algorithm that uses a polynomial number of training examples, runs in time polynomial in (n, 1/ , 1/\u03b4) and PMAC learns f(X) within a factor of 1+ |X|1+(|X|\u22121)(1\u2212\u03ba\u0302f (X)) .\nBefore proving this result, we compare this result to Lemma 4.1. Lemma 4.3 leads to better bounds for small sets, whereas Lemma 4.1 provides a better general bound. Moreover, in contrast to Lemma 4.1, here we only need an upper bound on the curvature and do not need to know the singleton weights {f(j), j \u2208 V }. Note also that, while \u03baf itself is an upper bound of \u03ba\u0302f (X), often one does have an upper bound on \u03ba\u0302f (X) if one knows the function class of f (for example, say concave over modular). In particular, an immediate corollary\nis that the class of concave over modular functions f(X) = \u2211k i=1 \u03bbi[wi(X)]\na, \u03bbi \u2265 0, for a \u2208 (0, 1) can be learnt within a factor of min{ \u221a n+ 1, 1 + |X|1\u2212a}.\nProof. To prove this result, we adapt Algorithm 2 in [3] to curvature and modular approximations. Following their arguments, we reduce the problem of learning a submodular function to that of learning a linear seperator, while separately handling the subset of instances where f is zero. We detail the parts where our proof deviates from [3]. We divide 2V into the support set S = {X \u2286 V | f(X) > 0} of f and its complement Z = {X \u2286 V | f(X) = 0}. Using samples from D\u2032, we generate new, binary labeled samples from a distribution D\u2032 on {0, 1}n \u00d7R that will be used to learn the linear separator. These samples differ slightly from those in [3]. Let\n\u03b1(X) = |X|\n1 + (|X| \u2212 1)(1\u2212 \u03ba\u0302f (X)) . (29)\nTo sample from D\u2032, we repeatedly sample from D until we obtain a set X \u2208 S. For each such X, we flip a fair coin and, with equal probability, generate a sample point from D\u2032 as\nx = (1X , f(X)) and label y = +1 if heads (30)\nx = (1X , (\u03b1(X) + 1)f(X)) and label y = \u22121 if tails. (31)\nWe observe that the generated positive and negative sample are linearly separable with the separator u = (w,\u22121), where w(j) = 0 if f(j) = 0, and w(j) = f(j)+\u03b4 if f(j) > 0, with \u03b4 such that 0 < \u03b4 < minj\u2208S f(Xj)/n:\nu>(1X , f(X)) = \u2211 j\u2208X f(j) + \u03b4|X| \u2212 f(X) > 0 (32)\nu>(1X , (\u03b1(X) + 1)f(X)) = \u2211 j\u2208X f(j) + \u03b4|X| \u2212 (\u03b1(X) + 1)f(X) < 0 (33)\nfor all X \u2286 V . The second inequality holds since \u2211 j\u2208X f(j) \u2264 \u03b1(X)f(X) and \u03b4|X| \u2264 \u03b4n < f(X). (For points in Z, we have that u>(1X , f(X)) = 0.) The final algorithm generates a sample from D\u2032 for each sample X \u2208 S from D. For each X \u2208 Z, it adds the constraint that w(j) = 0 for all j \u2208 X. We then find a linear separator u = (w,\u2212z) and output the function f\u0302(X) , w(X)/z. This is possible by the above arguments. This function satisfies the approximation constraints for the set Y of all training points X \u2208 S for which both generated samples are labeled correctly: the correct labelings w(X)\u2212zf(X) > 0 and w(X)\u2212z(\u03b1(X)+1)f(X) < 0 imply that\nf(X) \u2264 \u02c6f(X) = w(X) z \u2264 (\u03b1(X) + 1)f(X). (34)\nSimilarly, the constraints on w imply that the same holds for any subset of the union of the training samples in Z. It remains to show that for sufficiently many, i.e., ` = 16n log(\nn \u03b4 ) samples, the sets S \\Y and Z \\ \u22c3 Xi\u2208Z,i\u2264`Xi\nhave small (at most 1\u2212 2 ) measure. This follows from Claim 5 in [3]."}, {"heading": "5 Constrained submodular minimization", "text": "Next, we apply our results to the minimization of submodular functions under constraints. Most algorithms for constrained minimization use one of two strategies: they apply a convex relaxation [25, 35], or they\noptimize a surrogate function f\u0302 that should approximate f well [16, 18, 35]. We follow the second strategy and propose a new, widely applicable curvature-dependent choice for surrogate functions. A suitable selection of f\u0302 will ensure theoretically optimal results. Throughout this section, we refer to the optimal solution as X\u2217 \u2208 argminX\u2208C f(X).\nLemma 5.1. Given a submodular function f , let f\u03021 be an approximation of f such that f\u03021(X) \u2264 f(X) \u2264 \u03b1(n)f\u03021(X), for all X \u2286 V . Then any minimizer X\u03021 \u2208 argminX\u2208C f\u0302(X) of f\u0302 satisfies f(X\u0302) \u2264 \u03b1(n)f(X\u2217). Likewise, if an approximation of f is such that f(X) \u2264 f\u03022(X) \u2264 \u03b1(X)f(X) for a set-specific factor \u03b1(X), then its minimizer X\u03032 \u2208 argminX\u2208C f\u03022(X) satisfies f(X\u03022) \u2264 \u03b1(X\u2217)f(X\u2217). If only \u03b2-approximations3 are possible for minimizing f\u03021 or f\u03022 over C, then the final bounds are \u03b2\u03b1(n) and \u03b2\u03b1(X\u2217) respectively.\n3A \u03b2-approximation algorithm for minimizing a function g finds set X : g(X) \u2264 \u03b2minX\u2208C g(X)\nProof. We prove the first part and the second part similarly follows. Given that,\nf\u0302(X) \u2264 f(X) \u2264 \u03b1(n)f\u0302(X) (35)\nThen, if X\u0302 is the optimal solution for minimizing f\u0302 over C. We then have that,\nf(X\u0302) \u2264 \u03b1(n)f\u0302(X\u0302) \u2264 \u03b1(n)f\u0302(X\u2217) \u2264 \u03b1(n)f(X\u2217) (36)\nwhere X\u2217 is the optimal solution of f .\nFor Lemma 5.1 to be practically useful, it is essential that f\u03021 and f\u03022 be efficiently optimizable over C. We discuss two general curvature-dependent approximations that work for a large class of combinatorial constraints. In particular, we use Theorem 3.1: we decompose f into f\u03ba and a modular part fm, and then approximate f\u03ba while retaining fm, i.e., f\u0302 = f\u0302\u03ba + fm. The first approach uses a simple modular upper bound (MUB) and the second relies on the Ellipsoidal approximation (EA) we used in Section 3. MUB: The simplest approximation to a submodular function is the modular approximation f\u0302m(X) ,\u2211 j\u2208X f(j) \u2265 f(X). Since here, f\u0302\u03ba happens to be equivalent to fm, we obtain the overall approximation f\u0302 = f\u0302m. Lemmas 5.1 and 3.1 directly imply a set-dependent approximation factor for f\u0302m:\nCorollary 5.1. Let X\u0302 \u2208 C be a \u03b2-approximate solution for minimizing \u2211 j\u2208X f(j) over C, i.e. \u2211 j\u2208X\u0302 f(j) \u2264\n\u03b2minX\u2208C \u2211 j\u2208X f(j). Then\nf(X\u0302) \u2264 \u03b2|X \u2217|\n1 + (|X\u2217| \u2212 1)(1\u2212 \u03baf (X\u2217)) f(X\u2217). (37)\nCorollary 5.1 has also been shown in [32]. Thanks to Lemma 3.1 and the second part of Lemma 5.1, however, we can provide a much simpler proof. Similar to the algorithms in [32, 29, 31], MUB can be extended to an iterative algorithm yielding performance gains in practice. In particular, Corollary 5.1 implies improved approximation bounds for practically relevant concave over modular functions, such as those used in [36].\nFor instance, for f(X) = \u2211k i=1 \u221a\u2211 j\u2208X wi(j), we obtain a worst-case approximation bound of \u221a |X\u2217| \u2264 \u221a n. This is significantly better than the worst case factor of |X\u2217| for general submodular functions. EA: Instead of employing a modular upper bound, we can approximate f\u03ba using the construction by Goemans et al. [18], as in Corollary 3.3. In that case, f\u0302(X) = \u03baf \u221a wf\u03ba(X) + (1 \u2212 \u03baf )fm(X) has a special form: a weighted sum of a concave function and a modular function. Minimizing such a function over constraints C is harder than minimizing a merely modular function, but with the algorithm in [47] we obtain an FPTAS4 for minimizing f\u0302 over C whenever we can minimize a nonnegative linear function over C.\nCorollary 5.2. For a submodular function with curvature \u03baf < 1, algorithm EA will return a solution X\u0302 that satisfies f(X\u0302) \u2264 O ( \u221a n log n\n( \u221a n log n\u2212 1)(1\u2212 \u03baf ) + 1)\n) f(X\u2217). (38)\nProof. We use the important result from [47] where they show that any function of the form \u03bb1 \u221a m1(X) + \u03bb2m2(X) where \u03bb1 \u2265 0, \u03bb2 \u2265 0 and m1 and m2 are positive modular functions, has a FPTAS, provided a modular function can easily be optimized over C. Notice that our function is exactly of that form. Hence f\u0302(X) can be approximately optimized over C. This bound then translates into the approximation guarantee using Corollary 3.3 and the first part of Lemma 5.1.\n4The FPTAS will yield a \u03b2 = (1 + )-approximation through an algorithm polynomial in 1 .\nNext, we apply the results of this section to specific optimization problems, for which we show (mostly tight) curvature-dependent upper and lower bounds. Cardinality lower bounds (SLB). A simple constraint is a lower bound on the cardinality of the solution, i.e., C = {X \u2286 V : |X| \u2265 k}. Svitkina and Fleischer [52] prove that for monotone submodular functions of arbitrary curvature, it is impossible to find a polynomial-time algorithm with an approximation factor better than \u221a n/ log n. They show an algorithm which matches this approximation factor.\nObservation 5.1. For the SLB problem, Algorithm EA and MUB are guaranteed to be no worse than factors of O( \u221a n logn\n1+( \u221a n logn\u22121)(1\u2212\u03baf ) ) and k1+(k\u22121)(1\u2212\u03baf ) respectively.\nThe guarantee for MUB follows directly from Corollary 5.1, by observing that |X\u2217| = k. We also show a similar asymptotic hardness result, which is quite close to the bounds in observation 5.1. These bounds are improvements over the results of [52] whenever \u03baf < 1. Here, MUB is preferable to EA whenever k is small. The following theorem shows that the bound for EA is tight up to poly-log factors.\nTheorem 5.3. For \u03baf < 1 and any > 0, there exists submodular functions with curvature \u03baf such that no poly-time algorithm achieves an approx. factor of n 1/2\u2212\n1+(n1/2\u2212 \u22121)(1\u2212\u03baf ) for the SLB problem.\nProof. The proof of this theorem is analogous to that of theorem 3.4. Define two monotone submodular functions h\u03ba(X) = \u03baf min{|X|, \u03b1}+ (1\u2212 \u03baf )|X| and fR\u03ba (X) = \u03baf min{\u03b2 + |X| \u2229 R\u0304|, |X|, \u03b1}+ (1\u2212 \u03baf )|X|, where R \u2286 V is a random set of cardinality \u03b1. Let \u03b1 and \u03b2 be an integer such that \u03b1 = x \u221a n/5 and \u03b2 = x2/5 for an x2 = \u03c9(log n). Also we assume that k = \u03b1 in this case. Both h\u03ba and f R \u03ba have curvature \u03baf . Given an arbitrary > 0, set x2 = n2 = \u03c9(log n). Then the ratio between fR\u03ba and g \u03baf is n 1/2\u2212\n1+(n1/2\u2212 \u22121)(1\u2212\u03baf ) . Clearly\nthen if any algorithm can achieve better than this bound, it can distinguish between fR and g which is a contradiction.\nIn the following problems, our ground set V consists of the set of edges in a graph G = (V, E) with two distinct nodes s, t \u2208 V and n = |V|, m = |E|. The submodular function is f : 2E \u2192 R. Shortest submodular s-t path (SSP). Here, we aim to find an s-t path X of minimum (submodular) length f(X). Goel et al. [16] show a O(n2/3)-approximation with matching curvature-independent lower bound \u2126(n2/3). By Corollary 5.1, the curvature-dependent worst-case bound for MUB is n1+(n\u22121)(1\u2212\u03baf ) since any minimal s-t path has at most n edges. Similarly, the factor for EA is O( \u221a m logm\n1+( \u221a m logm\u22121)(1\u2212\u03baf ) ). The\nbound of EA will be tighter for sparse graphs while MUB provides better results for dense ones. We can also show the following curvature-dependent lower bound:\nTheorem 5.4. Given a submodular function with a curvature \u03baf > 0 and any > 0, no polynomial-time algorithm achieves an approximation factor better than n 2/3\u2212\n1+(n2/3\u2212 \u22121)(1\u2212\u03baf ) for the SSP problem.\nProof. The proof of this follows in very similar lines to the earlier lower bounds using our construction and the matroid constructions in [16]. The main idea is to use their multilevel graph, but define adjusted versions of their cost functions. In particular, define h(X) = \u03baf min{|X|, \u03b1} + (1 \u2212 \u03baf )|X| and fR(X) = \u03baf min{\u03b2 + |X| \u2229 R\u0304|, |X|, \u03b1}+ (1\u2212 \u03baf )|X|. In this context R is a randomly chosen s-t path of length n2/3 and \u03b1 = n2/3. Similarly the value of \u03b2 = n . The Chernoff bounds then show that the two functions above are indistinguishable (with high probability) and hence the ratio of the two functions h and fR then provides the hardness result.\nMinimum submodular s-t cut (SSC): This problem, also known as the cooperative cut problem [35, 36], asks to minimize a monotone submodular function f such that the solution X \u2286 E is a set of edges whose removal disconnects s from t in G. Using curvature refines the lower bound in [35]:\nTheorem 5.5. No polynomial-time algorithm can achieve an approximation factor better than n 1/2\u2212\n1+(n1/2\u2212 \u22121)(1\u2212\u03baf ) ,\nfor any > 0, for the SSC problem with a submodular function of curvature \u03baf .\nProof. This proof follows along the lines of the results shown above. It uses the construction from [35]. Corollary 5.1 implies an approximation factor of O( \u221a m logm\n( \u221a m logm\u22121)(1\u2212\u03baf )+1 ) for EA and a factor of m1+(m\u22121)(1\u2212\u03baf ) for MUB, where m = |E| is the number of edges in the graph. By Theorem 5.5, the factor for EA is tight for sparse graphs. Specifically for cut problems, there is yet another useful surrogate function that is exact on local neighborhoods. Jegelka and Bilmes [35] demonstrate how this approximation may be optimized via a generalized maximum flow algorithm that maximizes a polymatroidal network flow [40]. This algorithm still applies to the combination f\u0302 = \u03baf f\u0302 \u03ba + (1 \u2212 \u03baf )fm, where we only approximate f\u03ba. We refer to this approximation as Polymatroidal Network Approximation (PNA).\nCorollary 5.6. Algorithm PNA achieves a worst-case approximation factor of n2+(n\u22122)(1\u2212\u03baf ) for the cooperative cut problem.\nFor dense graphs, this factor is theoretically tighter than that of the EA approximation.\nProof. We use the polymatroidal network flow construction from [35], where the approximation f\u0302 is defined via a partition of the ground set, and is separable over groups of edges. This approximation can be solved efficiently via generalized flows in polynomial time [34, 35]. Moreover adding a modular term (for the modulation) does not increase the complexity of the problem. This approximation satisfies f\u03ba(X) \u2264 f\u0302\u03ba(X) \u2264 n2 f \u03ba(X) for all cuts X \u2208 C.We then convert this expression in the form of Theorem 3.1 as 2f\u0302 \u03ba(X) n \u2264 f \u03ba(X) \u2264 f\u0302\u03ba(X). Then define f\u0302(X) , \u03baf 2 \u02c6f\u03ba(X) n + (1\u2212 \u03baf ) \u2211 j\u2208X f(j), and using theorem 3.1, it implies that:\nf\u0302(X) \u2264 f(X) \u2264 n 2 + (n\u2212 2)(1\u2212 \u03baf ) f(X) (39)\nThen let X\u0302 be the minimizer of f\u0302(X) over C (using the generalized flows [35]). It then follows that (let \u03b1 = n2+(n\u22122)(1\u2212\u03baf ) ): f(X\u0302) \u2264 \u03b1f\u0302(X\u0302) \u2264 \u03b1f\u0302(X \u2217) \u2264 f(X\u2217) where X\u2217 is the optimal solution of f over C.\nMinimum submodular spanning tree (SST). Here, C is the family of all spanning trees in a given graph G. Such constraints occur for example in power assignment problems [56]. Goel et al. [16] show a curvature-independent optimal approximation factor of O(n) for this problem.\nObservation 5.2. For the minimum submodular spanning tree problem, algorithm MUB achieves an approximation guarantee, which is no worse than n\u2212r1+(n\u2212r\u22121)(1\u2212\u03baf ) , where r is the number of connected components of G.\nProof. This result follows directly from Corollary 5.1 and the fact that |X\u2217| = n\u2212 r.\nIn this case, Algorithm EA in fact provides slightly worse guarantees. Moreover the bound for MUB is optimal:\nTheorem 5.7. For the class of submodular functions with curvature \u03baf < 1, no poly-time algorithm can achieve an approximation factor of n 1\u22123\n1+(n1\u22123 \u22121)(1\u2212\u03baf )+\u03b4\u03baf for the SST problem for any , \u03b4 > 0.\nProof. In this case, we use the construction of [16], and define fR\u03ba (X) = \u03baf min{|X\u2229R\u0304|+min{|X\u2229R|, \u03b2}, \u03b1}+ (1\u2212 \u03baf )|X|, and g\u03baf (X) = \u03bafmin{|X|, \u03b1}+ (1\u2212 \u03baf )|X|, where \u03b1 = n1+ , \u03b2 = n3 (1 + \u03b4) and |R| = \u03b1. For the formal graph construction, see [16]. Then with high probability R is connected in the graph [16]. Since fR and g are indistinguishable with high probability, so are f R \u03ba and g \u03baf . Then notice that the minimum value\nof fR\u03ba and g \u03baf are \u03baf\u03b2 + (1\u2212 \u03baf )n and n respectively, and it is clear that the ratio between them is better than n 1\u22123\n1+(n1\u22123 \u22121)(1\u2212\u03baf )+\u03b4\u03baf . Hence if any algorithm performs better than this, it will be able to distinguish\nfR and g with high probability, which is a contradiction.\nAna analogous analysis applies to combinatorial constraints like Steiner trees [16]. Minimum submodular perfect matching (SPM): Here, we aim to find a perfect matching in a graph that minimizes a monotone submodular function. Corollary 5.1 implies that an MUB approximation will achieve an approximation factor of at most n2+(n\u22122)(1\u2212\u03baf ) . This bound is also tight:\nTheorem 5.8. Given a submodular function with a curvature \u03baf > 0 and any > 0, no polynomial-time algorithm achieves an approximation factor better than n 1\u22123\n2+(n1\u22123 \u22122)(1\u2212\u03baf )+2\u03b4\u03baf for the SPM problem.\nProof. We use the same submodular functions as the spanning tree case, and it can be shown [16] that with high probability the set R contains a perfect matching and the two functions are indistinguishable. Taking the ratio of g\u03baf and f\n\u03baf R , provides the above result.\nMinimum submodular edge cover: The minimum submodular edge cover involves finding an edge cover (subset of edges covering all vertices), with minimum submodular cost. This problem has been investigated in [25], and they show that this problem is O(n) hard. Algorithm MUB provides an approximation guarantee which is no worse than 2n2+(n\u22122)(1\u2212\u03baf ) . We can show a almost matching hardness lower bound for this problem.\nTheorem 5.9. Given a submodular function, with curvature coefficient \u03baf and any , \u03b4 > 0, there cannot exist a polynomial-time approximation algorithm, which achieves an approximation better than n 1\u22123 2+(n1\u22123 \u22122)(1\u2212\u03baf )+2\u03b4\u03baf for the minimum submodular edge cover problem.\nProof. We can use the construction of [25] to show this. However a simple observation shows that a perfect matching is also an edge cover, and hence the hardness of edge cover has to be at least as much as the hardness of perfect matchings."}, {"heading": "5.1 Experiments", "text": "We end this section by empirically demonstrating the performance of MUB and EA and their precise dependence on curvature. We focus on cardinality lower bound constraints, C = {X \u2286 V : |X| \u2265 \u03b1} and the \u201cworst-case\u201d class of functions that has been used throughout this paper to prove lower bounds,\nfR(X) = min{|X \u2229 R\u0304|+ \u03b2, |X|, \u03b1}, (40)\nwhere R\u0304 = V \\R and R \u2286 V is random set such that |R| = \u03b1. We adjust \u03b1 = n1/2+ and \u03b2 = n2 by a parameter . The smaller is, the harder the problem. The function (40) has curvature \u03baf = 1. To obtain a function with specific curvature \u03ba, we define\nfR\u03ba (X) = \u03baf(X) + (1\u2212 \u03ba)|X|. (41)\nIn all our experiments, we take the average over 20 random draws of R. We first set \u03ba = 1 and vary . Figure 1(a) shows the empirical approximation factors obtained using EA and MUB, and the theoretical bound. The empirical factors follow the theoretical results very closely. Empirically, we also see that the problem becomes harder as decreases. Next we fix = 0.1 and vary the curvature \u03ba in fR\u03ba . Figure 1(b) illustrates that the theoretical and empirical approximation factors improve significantly as \u03ba decreases. Hence, much better approximations than the previous theoretical lower bounds are possible if \u03ba is not too large. This observation can be very important in practice. Here, too, the empirical upper bounds follow the theoretical bounds very closely. Figures 1(c) and (d) show results for larger \u03b1 and \u03b2 = 1. In Figure 1(c), as \u03b1 increases, the empirical factors improve. In particular, as predicted by the theoretical bounds, EA outperforms MUB for large \u03b1 and, for \u03b1 \u2265 n2/3, EA finds the optimal solution. In addition, Figures 1(b) and (d) illustrate the theoretical and empirical effect of curvature: as n grows, the bounds saturate and approximate a constant 1/(1\u2212 \u03ba) \u2013 they do not grow polynomially in n. Overall, we see that the empirical results quite closely follow our theoretical results, and that, as the theory suggests, curvature significantly affects the approximation factors."}, {"heading": "6 Conclusion and Discussion", "text": "In this paper, we study the effect of curvature on the problems of approximating, learning and minimizing submodular functions under constraints. We prove tightened, curvature-dependent upper bounds with almost matching lower bounds. These results complement known results for submodular maximization [7, 55]. Moreover, in [28], we also consider the role of curvature in submodular optimization problems over a class of submodular constraints. Given that the functional form and effect of the submodularity ratio proposed in [9] is similar to that of curvature, an interesting extension is the question of whether there is a single unifying quantity for both of these terms. Another open question is whether a quantity similar to curvature can be defined for subadditive functions, thus refining the results in [2, 1] for learning subadditive functions. Finally it also seems that the techniques in this paper could be used to provide improved curvature-dependent regret bounds for constrained online submodular minimization [34]. Acknowledgments: Special thanks to Kai Wei for pointing out that Corollary 3.5 holds and for other discussions, to Bethany Herwaldt for reviewing an early draft of this manuscript, and to the anonymous reviewers. This material is based upon work supported by the National Science Foundation under Grant No. (IIS-1162606), a Google and a Microsoft award, and by the Intel Science and Technology Center for Pervasive Computing. Stefanie Jegelka\u2019s work is supported by the Office of Naval Research under contract/grant number N00014-11-1-0688, and gifts from Amazon Web Services, Google, SAP, Blue Goji, Cisco, Clearstory Data, Cloudera, Ericsson, Facebook, General Electric, Hortonworks, Intel, Microsoft, NetApp, Oracle, Samsung, Splunk, VMware and Yahoo!."}], "references": [{"title": "Sketching valuation functions", "author": ["A. Badanidiyuru", "S. Dobzinski", "H. Fu", "R. Kleinberg", "N. Nisan", "T. Roughgarden"], "venue": "In SODA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Learning valuation functions", "author": ["M.F. Balcan", "F. Constantin", "S. Iwata", "L. Wang"], "venue": "COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Submodular functions: Learnability, structure, and optimization", "author": ["N. Balcan", "N. Harvey"], "venue": "In Arxiv preprint,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "A tight (1/2) linear-time approximation to unconstrained submodular maximization", "author": ["N. Buchbinder", "M. Feldman", "J. Naor", "R. Schwartz"], "venue": "In FOCS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Maximizing a monotone submodular function under a matroid constraint", "author": ["G. Calinescu", "C. Chekuri", "M. Pal", "J. Vondr\u00e1k"], "venue": "IPCO,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Submodular function maximization via the multilinear relaxation and contention resolution schemes", "author": ["C. Chekuri", "J. Vondr\u00e1k", "R. Zenklusen"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Submodular set functions, matroids and the greedy algorithm: tight worstcase bounds and some generalizations of the Rado-Edmonds theorem", "author": ["M. Conforti", "G. Cornuejols"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1984}, {"title": "Decomposition of submodular functions", "author": ["W.H. Cunningham"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1983}, {"title": "Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection", "author": ["A. Das", "D. Kempe"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Minimizing sparse high-order energies by submodular vertex-cover", "author": ["A. Delong", "O. Veksler", "A. Osokin", "Y. Boykov"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "On the approximation of submodular functions", "author": ["N.R. Devanur", "S. Dughmi", "R. Schwartz", "A. Sharma", "M. Singh"], "venue": "arXiv preprint arXiv:1304.4948,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Maximizing non-monotone submodular functions", "author": ["U. Feige", "V. Mirrokni", "J. Vondr\u00e1k"], "venue": "SIAM J. COMPUT.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "A unified continuous greedy algorithm for submodular maximization", "author": ["M. Feldman", "J. Naor", "R. Schwartz"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "A push-relabel framework for submodular function minimization and applications to parametric optimization", "author": ["L. Fleischer", "S. Iwata"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Submodular functions and optimization, volume 58", "author": ["S. Fujishige"], "venue": "Elsevier Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Approximability of combinatorial problems with multi-agent submodular cost functions", "author": ["G. Goel", "C. Karande", "P. Tripathi", "L. Wang"], "venue": "In FOCS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Combinatorial problems with discounted price functions in multi-agent systems", "author": ["G. Goel", "P. Tripathi", "L. Wang"], "venue": "In FSTTCS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Approximating submodular functions everywhere", "author": ["M. Goemans", "N. Harvey", "S. Iwata", "V. Mirrokni"], "venue": "In SODA,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "The ellipsoid method and its consequences in combinatorial optimization", "author": ["M. Gr\u00f6tschel", "L. Lov\u00e1sz", "A. Schrijver"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1981}, {"title": "Geometric methods in combinatorial optimization", "author": ["M. Grotschel", "L. Lov\u00e1sz", "A. Schrijver"], "venue": "In Silver Jubilee Conf. on Combinatorics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1984}, {"title": "Approximation algorithms and hardness results for labeled connectivity problems", "author": ["R. Hassin", "J. Monnot", "D. Segev"], "venue": "J Combinatorial Optimization,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "A fully combinatorial algorithm for submodular function minimization", "author": ["S. Iwata"], "venue": "In Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "A faster scaling algorithm for minimizing submodular functions", "author": ["S. Iwata"], "venue": "SIAM Journal on Computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Submodular function minimization", "author": ["S. Iwata"], "venue": "Mathematical Programming,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Submodular function minimization under covering constraints", "author": ["S. Iwata", "K. Nagano"], "venue": "In In FOCS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "A simple combinatorial algorithm for submodular function minimization", "author": ["S. Iwata", "J. Orlin"], "venue": "In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "A combinatorial strongly polynomial algorithm for minimizing submodular functions", "author": ["S. Iwata", "L. Fleischer", "S. Fujishige"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Submodular optimization with submodular cover and submodular knapsack constraints", "author": ["R. Iyer", "J. Bilmes"], "venue": "In NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "The submodular Bregman and Lov\u00e1sz-Bregman divergences with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "In NIPS,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Algorithms for approximate minimization of the difference between submodular functions, with applications", "author": ["R. Iyer", "J. Bilmes"], "venue": "In UAI,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Mirror descent like algorithms for submodular optimization", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "NIPS Workshop on Discrete Optimization in Machine Learning (DISCML),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Fast semidifferential based submodular function optimization", "author": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Combinatorial Problems with submodular coupling in machine learning and computer vision", "author": ["S. Jegelka"], "venue": "PhD thesis, ETH Zurich,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Online submodular minimization for combinatorial structures", "author": ["S. Jegelka", "J. Bilmes"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Approximation bounds for inference using cooperative cuts", "author": ["S. Jegelka", "J.A. Bilmes"], "venue": "In ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "author": ["S. Jegelka", "J.A. Bilmes"], "venue": "In CVPR,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "A principled deep random field for image segmentation", "author": ["P. Kohli", "A. Osokin", "S. Jegelka"], "venue": "In CVPR,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["A. Krause", "C. Guestrin"], "venue": "In Proceedings of Uncertainity in Artificial Intelligence. UAI,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Near-optimal sensor placements: Maximizing information while minimizing communication cost", "author": ["A. Krause", "C. Guestrin", "A. Gupta", "J. Kleinberg"], "venue": "In IPSN,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Computing maximal \u201cpolymatroidal\u201d network flows", "author": ["E. Lawler", "C. Martel"], "venue": "Mathematics of Operations Research,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1982}, {"title": "Non-monotone submodular maximization under matroid and knapsack constraints", "author": ["J. Lee", "V. Mirrokni", "V. Nagarajan", "M. Sviridenko"], "venue": "In STOC,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Submodular maximization over multiple matroids via generalized exchange properties", "author": ["J. Lee", "M. Sviridenko", "J. Vondr\u00e1k"], "venue": "In APPROX,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Optimal selection of limited vocabulary speech corpora", "author": ["H. Lin", "J. Bilmes"], "venue": "In Interspeech,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "A class of submodular functions for document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "In The 49th Meeting of the Assoc. for Comp. Ling. Human Lang. Technologies", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Learning mixtures of submodular shells with application to document summarization", "author": ["H. Lin", "J. Bilmes"], "venue": "In UAI,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2012}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014i", "author": ["G. Nemhauser", "L. Wolsey", "M. Fisher"], "venue": "Mathematical Programming,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1978}, {"title": "Approximation algorithms for offline risk-averse combinatorial optimization", "author": ["E. Nikolova"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2010}, {"title": "A faster strongly polynomial time algorithm for submodular function minimization", "author": ["J. Orlin"], "venue": "Mathematical Programming,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2009}, {"title": "Symmetric submodular function minimization under hereditary family constraints", "author": ["J. Soto", "M. Goemans"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2010}, {"title": "Learning fourier sparse set functions", "author": ["P. Stobbe", "A. Krause"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2012}, {"title": "A note on maximizing a submodular set function subject to a knapsack constraint", "author": ["M. Sviridenko"], "venue": "Operations Research Letters,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2004}, {"title": "Submodular approximation: Sampling-based algorithms and lower bounds", "author": ["Z. Svitkina", "L. Fleischer"], "venue": "In FOCS,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2008}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1984}, {"title": "Graph cut based image segmentation with connectivity priors", "author": ["S. Vicente", "V. Kolmogorov", "C. Rother"], "venue": "In Proc. CVPR,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2008}, {"title": "Submodularity and curvature: the optimal algorithm", "author": ["J. Vondr\u00e1k"], "venue": "RIMS Kokyuroku Bessatsu,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2010}, {"title": "Minimum-energy broadcasting in static ad hoc wireless networks", "author": ["P.-J. Wan", "G. Calinescu", "X.-Y. Li", "O. Frieder"], "venue": "Wireless Networks,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2002}, {"title": "Approximation and hardness results for label cut and related problems", "author": ["P. Zhang", "J.-Y. Cai", "L.-Q. Tang", "W.-B. Zhao"], "venue": "Journal of Combinatorial Optimization,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2011}], "referenceMentions": [{"referenceID": 52, "context": "Abstract We investigate three related and important problems connected to machine learning: approximating a submodular function everywhere, learning a submodular function (in a PAC-like setting [53]), and constrained minimization of submodular functions.", "startOffset": 194, "endOffset": 198}, {"referenceID": 2, "context": "We show that the complexity of all three problems depends on the \u201ccurvature\u201d of the submodular function, and provide lower and upper bounds that refine and improve previous results [3, 16, 18, 52].", "startOffset": 181, "endOffset": 196}, {"referenceID": 15, "context": "We show that the complexity of all three problems depends on the \u201ccurvature\u201d of the submodular function, and provide lower and upper bounds that refine and improve previous results [3, 16, 18, 52].", "startOffset": 181, "endOffset": 196}, {"referenceID": 17, "context": "We show that the complexity of all three problems depends on the \u201ccurvature\u201d of the submodular function, and provide lower and upper bounds that refine and improve previous results [3, 16, 18, 52].", "startOffset": 181, "endOffset": 196}, {"referenceID": 51, "context": "We show that the complexity of all three problems depends on the \u201ccurvature\u201d of the submodular function, and provide lower and upper bounds that refine and improve previous results [3, 16, 18, 52].", "startOffset": 181, "endOffset": 196}, {"referenceID": 6, "context": "Curiously, curvature has been known to influence approximations for submodular maximization [7, 55], but its effect on minimization, approximation and learning has hitherto been open.", "startOffset": 92, "endOffset": 99}, {"referenceID": 54, "context": "Curiously, curvature has been known to influence approximations for submodular maximization [7, 55], but its effect on minimization, approximation and learning has hitherto been open.", "startOffset": 92, "endOffset": 99}, {"referenceID": 14, "context": "The search for optimal algorithms for submodular optimization has seen substantial progress [15, 24, 4] in recent years, but is still an ongoing endeavor.", "startOffset": 92, "endOffset": 103}, {"referenceID": 23, "context": "The search for optimal algorithms for submodular optimization has seen substantial progress [15, 24, 4] in recent years, but is still an ongoing endeavor.", "startOffset": 92, "endOffset": 103}, {"referenceID": 3, "context": "The search for optimal algorithms for submodular optimization has seen substantial progress [15, 24, 4] in recent years, but is still an ongoing endeavor.", "startOffset": 92, "endOffset": 103}, {"referenceID": 18, "context": "The first polynomial-time algorithm used the ellipsoid method [19, 20], and several combinatorial algorithms followed [27, 22, 14, 23, 26, 48].", "startOffset": 62, "endOffset": 70}, {"referenceID": 19, "context": "The first polynomial-time algorithm used the ellipsoid method [19, 20], and several combinatorial algorithms followed [27, 22, 14, 23, 26, 48].", "startOffset": 62, "endOffset": 70}, {"referenceID": 26, "context": "The first polynomial-time algorithm used the ellipsoid method [19, 20], and several combinatorial algorithms followed [27, 22, 14, 23, 26, 48].", "startOffset": 118, "endOffset": 142}, {"referenceID": 21, "context": "The first polynomial-time algorithm used the ellipsoid method [19, 20], and several combinatorial algorithms followed [27, 22, 14, 23, 26, 48].", "startOffset": 118, "endOffset": 142}, {"referenceID": 13, "context": "The first polynomial-time algorithm used the ellipsoid method [19, 20], and several combinatorial algorithms followed [27, 22, 14, 23, 26, 48].", "startOffset": 118, "endOffset": 142}, {"referenceID": 22, "context": "The first polynomial-time algorithm used the ellipsoid method [19, 20], and several combinatorial algorithms followed [27, 22, 14, 23, 26, 48].", "startOffset": 118, "endOffset": 142}, {"referenceID": 25, "context": "The first polynomial-time algorithm used the ellipsoid method [19, 20], and several combinatorial algorithms followed [27, 22, 14, 23, 26, 48].", "startOffset": 118, "endOffset": 142}, {"referenceID": 47, "context": "The first polynomial-time algorithm used the ellipsoid method [19, 20], and several combinatorial algorithms followed [27, 22, 14, 23, 26, 48].", "startOffset": 118, "endOffset": 142}, {"referenceID": 23, "context": "For a detailed summary, see [24].", "startOffset": 28, "endOffset": 32}, {"referenceID": 45, "context": "However, maximization problems admit constant-factor approximations [46, 51, 12, 4], often even in the constrained case [51, 41, 42, 6, 13, 5].", "startOffset": 68, "endOffset": 83}, {"referenceID": 50, "context": "However, maximization problems admit constant-factor approximations [46, 51, 12, 4], often even in the constrained case [51, 41, 42, 6, 13, 5].", "startOffset": 68, "endOffset": 83}, {"referenceID": 11, "context": "However, maximization problems admit constant-factor approximations [46, 51, 12, 4], often even in the constrained case [51, 41, 42, 6, 13, 5].", "startOffset": 68, "endOffset": 83}, {"referenceID": 3, "context": "However, maximization problems admit constant-factor approximations [46, 51, 12, 4], often even in the constrained case [51, 41, 42, 6, 13, 5].", "startOffset": 68, "endOffset": 83}, {"referenceID": 50, "context": "However, maximization problems admit constant-factor approximations [46, 51, 12, 4], often even in the constrained case [51, 41, 42, 6, 13, 5].", "startOffset": 120, "endOffset": 142}, {"referenceID": 40, "context": "However, maximization problems admit constant-factor approximations [46, 51, 12, 4], often even in the constrained case [51, 41, 42, 6, 13, 5].", "startOffset": 120, "endOffset": 142}, {"referenceID": 41, "context": "However, maximization problems admit constant-factor approximations [46, 51, 12, 4], often even in the constrained case [51, 41, 42, 6, 13, 5].", "startOffset": 120, "endOffset": 142}, {"referenceID": 5, "context": "However, maximization problems admit constant-factor approximations [46, 51, 12, 4], often even in the constrained case [51, 41, 42, 6, 13, 5].", "startOffset": 120, "endOffset": 142}, {"referenceID": 12, "context": "However, maximization problems admit constant-factor approximations [46, 51, 12, 4], often even in the constrained case [51, 41, 42, 6, 13, 5].", "startOffset": 120, "endOffset": 142}, {"referenceID": 4, "context": "However, maximization problems admit constant-factor approximations [46, 51, 12, 4], often even in the constrained case [51, 41, 42, 6, 13, 5].", "startOffset": 120, "endOffset": 142}, {"referenceID": 2, "context": "While submodularity, like convexity, occurs naturally in a wide variety of problems, recent studies have shown that in the general case, many submodular problems of interest are very hard: the problems of learning a submodular function or of submodular minimization under constraints do not even admit constant or logarithmic approximation factors in polynomial time [3, 17, 18, 25, 52].", "startOffset": 367, "endOffset": 386}, {"referenceID": 16, "context": "While submodularity, like convexity, occurs naturally in a wide variety of problems, recent studies have shown that in the general case, many submodular problems of interest are very hard: the problems of learning a submodular function or of submodular minimization under constraints do not even admit constant or logarithmic approximation factors in polynomial time [3, 17, 18, 25, 52].", "startOffset": 367, "endOffset": 386}, {"referenceID": 17, "context": "While submodularity, like convexity, occurs naturally in a wide variety of problems, recent studies have shown that in the general case, many submodular problems of interest are very hard: the problems of learning a submodular function or of submodular minimization under constraints do not even admit constant or logarithmic approximation factors in polynomial time [3, 17, 18, 25, 52].", "startOffset": 367, "endOffset": 386}, {"referenceID": 24, "context": "While submodularity, like convexity, occurs naturally in a wide variety of problems, recent studies have shown that in the general case, many submodular problems of interest are very hard: the problems of learning a submodular function or of submodular minimization under constraints do not even admit constant or logarithmic approximation factors in polynomial time [3, 17, 18, 25, 52].", "startOffset": 367, "endOffset": 386}, {"referenceID": 51, "context": "While submodularity, like convexity, occurs naturally in a wide variety of problems, recent studies have shown that in the general case, many submodular problems of interest are very hard: the problems of learning a submodular function or of submodular minimization under constraints do not even admit constant or logarithmic approximation factors in polynomial time [3, 17, 18, 25, 52].", "startOffset": 367, "endOffset": 386}, {"referenceID": 2, "context": "Indeed, limited prior work has shown improved results for constrained minimization and learning of sub-classes of submodular functions, including symmetric functions [3, 49], concave functions [17, 37, 47], label cost or covering functions [21, 57].", "startOffset": 166, "endOffset": 173}, {"referenceID": 48, "context": "Indeed, limited prior work has shown improved results for constrained minimization and learning of sub-classes of submodular functions, including symmetric functions [3, 49], concave functions [17, 37, 47], label cost or covering functions [21, 57].", "startOffset": 166, "endOffset": 173}, {"referenceID": 16, "context": "Indeed, limited prior work has shown improved results for constrained minimization and learning of sub-classes of submodular functions, including symmetric functions [3, 49], concave functions [17, 37, 47], label cost or covering functions [21, 57].", "startOffset": 193, "endOffset": 205}, {"referenceID": 36, "context": "Indeed, limited prior work has shown improved results for constrained minimization and learning of sub-classes of submodular functions, including symmetric functions [3, 49], concave functions [17, 37, 47], label cost or covering functions [21, 57].", "startOffset": 193, "endOffset": 205}, {"referenceID": 46, "context": "Indeed, limited prior work has shown improved results for constrained minimization and learning of sub-classes of submodular functions, including symmetric functions [3, 49], concave functions [17, 37, 47], label cost or covering functions [21, 57].", "startOffset": 193, "endOffset": 205}, {"referenceID": 20, "context": "Indeed, limited prior work has shown improved results for constrained minimization and learning of sub-classes of submodular functions, including symmetric functions [3, 49], concave functions [17, 37, 47], label cost or covering functions [21, 57].", "startOffset": 240, "endOffset": 248}, {"referenceID": 56, "context": "Indeed, limited prior work has shown improved results for constrained minimization and learning of sub-classes of submodular functions, including symmetric functions [3, 49], concave functions [17, 37, 47], label cost or covering functions [21, 57].", "startOffset": 240, "endOffset": 248}, {"referenceID": 17, "context": "In particular, our quantification tightens the generic, function-independent bounds in [18, 3, 52, 17, 25] for many practically relevant functions.", "startOffset": 87, "endOffset": 106}, {"referenceID": 2, "context": "In particular, our quantification tightens the generic, function-independent bounds in [18, 3, 52, 17, 25] for many practically relevant functions.", "startOffset": 87, "endOffset": 106}, {"referenceID": 51, "context": "In particular, our quantification tightens the generic, function-independent bounds in [18, 3, 52, 17, 25] for many practically relevant functions.", "startOffset": 87, "endOffset": 106}, {"referenceID": 16, "context": "In particular, our quantification tightens the generic, function-independent bounds in [18, 3, 52, 17, 25] for many practically relevant functions.", "startOffset": 87, "endOffset": 106}, {"referenceID": 24, "context": "In particular, our quantification tightens the generic, function-independent bounds in [18, 3, 52, 17, 25] for many practically relevant functions.", "startOffset": 87, "endOffset": 106}, {"referenceID": 6, "context": "Previously, the concept of curvature has been used to tighten bounds for submodular maximization problems [7, 55].", "startOffset": 106, "endOffset": 113}, {"referenceID": 54, "context": "Previously, the concept of curvature has been used to tighten bounds for submodular maximization problems [7, 55].", "startOffset": 106, "endOffset": 113}, {"referenceID": 17, "context": "By quantifying the influence of curvature on other problems, we improve previous bounds in [18, 3, 52, 17, 25] for many functions used in applications.", "startOffset": 91, "endOffset": 110}, {"referenceID": 2, "context": "By quantifying the influence of curvature on other problems, we improve previous bounds in [18, 3, 52, 17, 25] for many functions used in applications.", "startOffset": 91, "endOffset": 110}, {"referenceID": 51, "context": "By quantifying the influence of curvature on other problems, we improve previous bounds in [18, 3, 52, 17, 25] for many functions used in applications.", "startOffset": 91, "endOffset": 110}, {"referenceID": 16, "context": "By quantifying the influence of curvature on other problems, we improve previous bounds in [18, 3, 52, 17, 25] for many functions used in applications.", "startOffset": 91, "endOffset": 110}, {"referenceID": 24, "context": "By quantifying the influence of curvature on other problems, we improve previous bounds in [18, 3, 52, 17, 25] for many functions used in applications.", "startOffset": 91, "endOffset": 110}, {"referenceID": 17, "context": "(Approximation [18]) Given a submodular function f in form of a value oracle, find an approximation f\u0302 (within polynomial time and representable within polynomial space), such that for all X \u2286 V , it holds that f\u0302(X) \u2264 f(X) \u2264 \u03b11(n)f\u0302(X) for a polynomial \u03b11(n).", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "(PMAC-Learning [3]) Given i.", "startOffset": 15, "endOffset": 18}, {"referenceID": 51, "context": "(Constrained optimization [52, 17, 25, 35]) Minimize a submodular function f over a family C of feasible sets, i.", "startOffset": 26, "endOffset": 42}, {"referenceID": 16, "context": "(Constrained optimization [52, 17, 25, 35]) Minimize a submodular function f over a family C of feasible sets, i.", "startOffset": 26, "endOffset": 42}, {"referenceID": 24, "context": "(Constrained optimization [52, 17, 25, 35]) Minimize a submodular function f over a family C of feasible sets, i.", "startOffset": 26, "endOffset": 42}, {"referenceID": 34, "context": "(Constrained optimization [52, 17, 25, 35]) Minimize a submodular function f over a family C of feasible sets, i.", "startOffset": 26, "endOffset": 42}, {"referenceID": 17, "context": "[18], who approximate any monotone submodular function to within a factor of O( \u221a n log n), with a lower bound of \u03b11(n) = \u03a9( \u221a n/ log n).", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Building on this result, Balcan and Harvey [3] show how to PMAC-learn a monotone submodular function within a factor of \u03b12(n) = O( \u221a n), and prove a lower bound of \u03a9(n) for the learning problem.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "Subsequent work extends these results to sub-additive and fractionally sub-additive functions [2, 1].", "startOffset": 94, "endOffset": 100}, {"referenceID": 0, "context": "Subsequent work extends these results to sub-additive and fractionally sub-additive functions [2, 1].", "startOffset": 94, "endOffset": 100}, {"referenceID": 44, "context": "Better learning results are possible for the subclass of submodular shells [45] and Fourier sparse set functions [50].", "startOffset": 75, "endOffset": 79}, {"referenceID": 49, "context": "Better learning results are possible for the subclass of submodular shells [45] and Fourier sparse set functions [50].", "startOffset": 113, "endOffset": 117}, {"referenceID": 10, "context": "Very recently Devanur et al [11] investigated a related problem of approximating one class of submodular functions with another and they show how many non-monotone submodular functions can be approximated with simple directed graph cuts within a factor of n/4 which is tight.", "startOffset": 28, "endOffset": 32}, {"referenceID": 2, "context": "Both Problems 1 and 2 have numerous applications in algorithmic game theory and economics [3, 18] as well as machine learning [3, 44, 45, 50, 34].", "startOffset": 90, "endOffset": 97}, {"referenceID": 17, "context": "Both Problems 1 and 2 have numerous applications in algorithmic game theory and economics [3, 18] as well as machine learning [3, 44, 45, 50, 34].", "startOffset": 90, "endOffset": 97}, {"referenceID": 2, "context": "Both Problems 1 and 2 have numerous applications in algorithmic game theory and economics [3, 18] as well as machine learning [3, 44, 45, 50, 34].", "startOffset": 126, "endOffset": 145}, {"referenceID": 43, "context": "Both Problems 1 and 2 have numerous applications in algorithmic game theory and economics [3, 18] as well as machine learning [3, 44, 45, 50, 34].", "startOffset": 126, "endOffset": 145}, {"referenceID": 44, "context": "Both Problems 1 and 2 have numerous applications in algorithmic game theory and economics [3, 18] as well as machine learning [3, 44, 45, 50, 34].", "startOffset": 126, "endOffset": 145}, {"referenceID": 49, "context": "Both Problems 1 and 2 have numerous applications in algorithmic game theory and economics [3, 18] as well as machine learning [3, 44, 45, 50, 34].", "startOffset": 126, "endOffset": 145}, {"referenceID": 33, "context": "Both Problems 1 and 2 have numerous applications in algorithmic game theory and economics [3, 18] as well as machine learning [3, 44, 45, 50, 34].", "startOffset": 126, "endOffset": 145}, {"referenceID": 2, "context": "often have diminishing returns and a natural problem is to estimate these functions [3].", "startOffset": 84, "endOffset": 87}, {"referenceID": 43, "context": "Similarly in machine learning, a number of problems involving sensor placement, summarization and others [44, 30] can be modeled through submodular functions.", "startOffset": 105, "endOffset": 113}, {"referenceID": 29, "context": "Similarly in machine learning, a number of problems involving sensor placement, summarization and others [44, 30] can be modeled through submodular functions.", "startOffset": 105, "endOffset": 113}, {"referenceID": 43, "context": "Since this function is submodular [44], a natural application is to learn these functions for summarization tasks.", "startOffset": 34, "endOffset": 38}, {"referenceID": 37, "context": "Constrained submodular minimization arises in applications such as power assignment or transportation problems [38, 39, 56, 32] .", "startOffset": 111, "endOffset": 127}, {"referenceID": 38, "context": "Constrained submodular minimization arises in applications such as power assignment or transportation problems [38, 39, 56, 32] .", "startOffset": 111, "endOffset": 127}, {"referenceID": 55, "context": "Constrained submodular minimization arises in applications such as power assignment or transportation problems [38, 39, 56, 32] .", "startOffset": 111, "endOffset": 127}, {"referenceID": 31, "context": "Constrained submodular minimization arises in applications such as power assignment or transportation problems [38, 39, 56, 32] .", "startOffset": 111, "endOffset": 127}, {"referenceID": 9, "context": "In machine learning, it occurs, for instance, in the form of MAP inference in high-order graphical models [10, 54, 36] or in size-constrained corpus extraction [43].", "startOffset": 106, "endOffset": 118}, {"referenceID": 53, "context": "In machine learning, it occurs, for instance, in the form of MAP inference in high-order graphical models [10, 54, 36] or in size-constrained corpus extraction [43].", "startOffset": 106, "endOffset": 118}, {"referenceID": 35, "context": "In machine learning, it occurs, for instance, in the form of MAP inference in high-order graphical models [10, 54, 36] or in size-constrained corpus extraction [43].", "startOffset": 106, "endOffset": 118}, {"referenceID": 42, "context": "In machine learning, it occurs, for instance, in the form of MAP inference in high-order graphical models [10, 54, 36] or in size-constrained corpus extraction [43].", "startOffset": 160, "endOffset": 164}, {"referenceID": 51, "context": "Recent results show that almost all constraints make it hard to solve the minimization even within a constant factor [52, 16, 35].", "startOffset": 117, "endOffset": 129}, {"referenceID": 15, "context": "Recent results show that almost all constraints make it hard to solve the minimization even within a constant factor [52, 16, 35].", "startOffset": 117, "endOffset": 129}, {"referenceID": 34, "context": "Recent results show that almost all constraints make it hard to solve the minimization even within a constant factor [52, 16, 35].", "startOffset": 117, "endOffset": 129}, {"referenceID": 6, "context": "1 Curvature of a Submodular function A central concept in this work is the total curvature \u03baf of a submodular function f and the curvature \u03baf (S) with respect to a set S \u2286 V , defined as [7, 55] \u03baf = 1\u2212min j\u2208V f(j | V \\ j) f(j) , \u03baf (S) = 1\u2212min j\u2208S f(j|S\\j) f(j) .", "startOffset": 187, "endOffset": 194}, {"referenceID": 54, "context": "1 Curvature of a Submodular function A central concept in this work is the total curvature \u03baf of a submodular function f and the curvature \u03baf (S) with respect to a set S \u2286 V , defined as [7, 55] \u03baf = 1\u2212min j\u2208V f(j | V \\ j) f(j) , \u03baf (S) = 1\u2212min j\u2208S f(j|S\\j) f(j) .", "startOffset": 187, "endOffset": 194}, {"referenceID": 8, "context": "Conceptually, curvature is distinct from the recently proposed submodularity ratio [9] that measures how far a function is from being submodular.", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": ", from (1\u2212 1/e) to 1 \u03baf (1\u2212 e \u2212\u03baf ) for monotone submodular maximization subject to a cardinality constraint [7] or matroid constraints [55], and these results are tight.", "startOffset": 109, "endOffset": 112}, {"referenceID": 54, "context": ", from (1\u2212 1/e) to 1 \u03baf (1\u2212 e \u2212\u03baf ) for monotone submodular maximization subject to a cardinality constraint [7] or matroid constraints [55], and these results are tight.", "startOffset": 136, "endOffset": 140}, {"referenceID": 54, "context": "In fact, [55] showed that this result for submodular maximization holds for the tighter version of curvature \u03ba\u0303f (S \u2217), where S\u2217 is the optimal solution.", "startOffset": 9, "endOffset": 13}, {"referenceID": 54, "context": "In other words, the bound for the greedy algorithm of [55] can be tightened to 1 \u03ba\u0303f (S\u2217) (1\u2212 e \u2212\u03ba\u0303f (S)).", "startOffset": 54, "endOffset": 58}, {"referenceID": 31, "context": "For submodular minimization, learning, and approximation, however, the role of curvature has not yet been addressed (an exception are the upper bounds in [32] for minimization).", "startOffset": 154, "endOffset": 158}, {"referenceID": 31, "context": "By contrast, many practically interesting functions have smaller curvature, and our analysis will provide an explanation for the good empirical results observed with such functions [32, 44, 33].", "startOffset": 181, "endOffset": 193}, {"referenceID": 43, "context": "By contrast, many practically interesting functions have smaller curvature, and our analysis will provide an explanation for the good empirical results observed with such functions [32, 44, 33].", "startOffset": 181, "endOffset": 193}, {"referenceID": 32, "context": "By contrast, many practically interesting functions have smaller curvature, and our analysis will provide an explanation for the good empirical results observed with such functions [32, 44, 33].", "startOffset": 181, "endOffset": 193}, {"referenceID": 43, "context": "An example for functions with \u03baf < 1 is the class of concave over modular functions that have been used in speech processing [44] and computer vision [36].", "startOffset": 125, "endOffset": 129}, {"referenceID": 35, "context": "An example for functions with \u03baf < 1 is the class of concave over modular functions that have been used in speech processing [44] and computer vision [36].", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "This class comprises, for instance, functions of the form f(X) = \u2211k i=1(wi(X)) , for some a \u2208 [0, 1] and a nonnegative weight vectors wi.", "startOffset": 94, "endOffset": 100}, {"referenceID": 43, "context": "Such functions may be defined over clusters Ci \u2286 V , in which case the weights wi(j) are nonzero only if j \u2208 Ci [44, 36, 30].", "startOffset": 112, "endOffset": 124}, {"referenceID": 35, "context": "Such functions may be defined over clusters Ci \u2286 V , in which case the weights wi(j) are nonzero only if j \u2208 Ci [44, 36, 30].", "startOffset": 112, "endOffset": 124}, {"referenceID": 29, "context": "Such functions may be defined over clusters Ci \u2286 V , in which case the weights wi(j) are nonzero only if j \u2208 Ci [44, 36, 30].", "startOffset": 112, "endOffset": 124}, {"referenceID": 8, "context": "A related quantity distinct from curvature that has been introduced in the machine learning community is the submodularity ratio [9]:", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "Our curvature-based decomposition is different from decompositions such as that into a totally normalized function and a modular function [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 15, "context": "Previous informationtheoretic lower bounds for Problems 1\u20133 [16, 18, 25, 52] are independent of curvature and use functions with \u03baf = 1.", "startOffset": 60, "endOffset": 76}, {"referenceID": 17, "context": "Previous informationtheoretic lower bounds for Problems 1\u20133 [16, 18, 25, 52] are independent of curvature and use functions with \u03baf = 1.", "startOffset": 60, "endOffset": 76}, {"referenceID": 24, "context": "Previous informationtheoretic lower bounds for Problems 1\u20133 [16, 18, 25, 52] are independent of curvature and use functions with \u03baf = 1.", "startOffset": 60, "endOffset": 76}, {"referenceID": 51, "context": "Previous informationtheoretic lower bounds for Problems 1\u20133 [16, 18, 25, 52] are independent of curvature and use functions with \u03baf = 1.", "startOffset": 60, "endOffset": 76}, {"referenceID": 17, "context": "The following are some of our main results: for approximating submodular functions (Problem 1), we replace the known bound of \u03b11(n) = O( \u221a n log n) [18] by an improved curvature-dependent O( \u221a n logn 1+( \u221a n logn\u22121)(1\u2212\u03baf ) ).", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "For learning submodular functions (Problem 2), we refine the known bound of \u03b12(n) = O( \u221a n) [3] in the PMAC setting to a curvature dependent bound of O( \u221a n 1+( \u221a n\u22121)(1\u2212\u03baf ) ), with a lower bound of \u03a9\u0303( n 1/3 1+(n\u22121)(1\u2212\u03baf ) ).", "startOffset": 92, "endOffset": 95}, {"referenceID": 15, "context": "These bounds refine many of the results in [16, 52, 25, 35].", "startOffset": 43, "endOffset": 59}, {"referenceID": 51, "context": "These bounds refine many of the results in [16, 52, 25, 35].", "startOffset": 43, "endOffset": 59}, {"referenceID": 24, "context": "These bounds refine many of the results in [16, 52, 25, 35].", "startOffset": 43, "endOffset": 59}, {"referenceID": 34, "context": "These bounds refine many of the results in [16, 52, 25, 35].", "startOffset": 43, "endOffset": 59}, {"referenceID": 17, "context": "Previous work established \u03b1-approximations g to a submodular function f satisfying g(S) \u2264 f(S) \u2264 \u03b1g(S) for all S \u2286 V [18].", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "[18] computes an approximation to a polymatroid function f in polynomial time by approximating the submodular polyhedron via an ellipsoid.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "2 ([18]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "The weights w are computed via an ellipsoidal approximation of the submodular polyhedron [18].", "startOffset": 89, "endOffset": 93}, {"referenceID": 17, "context": "To compute f, construct the function f as in Equation (6), and apply the algorithm in [18] to construct the approximation \u221a wf(X) such that \u221a wf(X) \u2264 f(X) \u2264 O( \u221a n log n) \u221a wf(X).", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "It refines the lower bound in [18] to include \u03baf .", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "The information-theoretic proof uses a construction and argumentation similar to that in [18, 52], but perturbs the functions to have the desired curvature.", "startOffset": 89, "endOffset": 97}, {"referenceID": 51, "context": "The information-theoretic proof uses a construction and argumentation similar to that in [18, 52], but perturbs the functions to have the desired curvature.", "startOffset": 89, "endOffset": 97}, {"referenceID": 51, "context": "Using a Chernoff bound, one can then show that any algorithm that uses a polynomial number of queries can distinguish h and f \u03ba with probability only n \u2212\u03c9(1), and therefore cannot reliably distinguish the functions with a polynomial number of queries [52].", "startOffset": 251, "endOffset": 255}, {"referenceID": 43, "context": "The improved curvature dependent bounds immediately imply better bounds for the class of concave over modular functions used in [44, 36, 30].", "startOffset": 128, "endOffset": 140}, {"referenceID": 35, "context": "The improved curvature dependent bounds immediately imply better bounds for the class of concave over modular functions used in [44, 36, 30].", "startOffset": 128, "endOffset": 140}, {"referenceID": 29, "context": "The improved curvature dependent bounds immediately imply better bounds for the class of concave over modular functions used in [44, 36, 30].", "startOffset": 128, "endOffset": 140}, {"referenceID": 2, "context": "We next address the problem of learning submodular functions in a PMAC setting [3].", "startOffset": 79, "endOffset": 82}, {"referenceID": 52, "context": "The PMAC (Probably Mostly Approximately Correct) framework is an extension of the PAC framework [53] to allow multiplicative errors in the function values from a fixed but unknown distribution D over 2 .", "startOffset": 96, "endOffset": 100}, {"referenceID": 2, "context": "Balcan and Harvey [3] propose an algorithm that PMAC-learns any monotone, nonnegative submodular function within a factor \u03b1(n) = \u221a n+ 1 by reducing the problem to that of learning a binary classifier.", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "1 uses the reduction of Balcan and Harvey [3] to learn the \u03baf -curve-normalized version f of f .", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "The proof of this theorem directly follows from the results in [3] and those from section 3.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "The idea is that, we use the PMAC setting and algorithm from [3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "Let f\u0302\u03ba(X) be the function learn from f using the algorithm from [3].", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "The case for product distributions also follows from very similar lines and the results from [3].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "Again, we use the same matroid functions used in [3].", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "Notice that the construction of [3], provides a family of matroids and a collection of sets B, with |A| = n, such that f(A) = |A|, A \u2208 B and f(A) = \u03b2 = \u03c9(log n), A / \u2208 B.", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "Again set \u03b2 = n \u2032 , and using a analysis and construction similar to the hardness proof of section 3 and Theorem 9 from [3] conveys that the lower bound for this problem is \u03a9\u0303( n 1/3 1+(n\u22121)(1\u2212\u03baf ) )", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "To prove this result, we adapt Algorithm 2 in [3] to curvature and modular approximations.", "startOffset": 46, "endOffset": 49}, {"referenceID": 2, "context": "We detail the parts where our proof deviates from [3].", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "These samples differ slightly from those in [3].", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "This follows from Claim 5 in [3].", "startOffset": 29, "endOffset": 32}, {"referenceID": 24, "context": "Most algorithms for constrained minimization use one of two strategies: they apply a convex relaxation [25, 35], or they optimize a surrogate function f\u0302 that should approximate f well [16, 18, 35].", "startOffset": 103, "endOffset": 111}, {"referenceID": 34, "context": "Most algorithms for constrained minimization use one of two strategies: they apply a convex relaxation [25, 35], or they optimize a surrogate function f\u0302 that should approximate f well [16, 18, 35].", "startOffset": 103, "endOffset": 111}, {"referenceID": 15, "context": "Most algorithms for constrained minimization use one of two strategies: they apply a convex relaxation [25, 35], or they optimize a surrogate function f\u0302 that should approximate f well [16, 18, 35].", "startOffset": 185, "endOffset": 197}, {"referenceID": 17, "context": "Most algorithms for constrained minimization use one of two strategies: they apply a convex relaxation [25, 35], or they optimize a surrogate function f\u0302 that should approximate f well [16, 18, 35].", "startOffset": 185, "endOffset": 197}, {"referenceID": 34, "context": "Most algorithms for constrained minimization use one of two strategies: they apply a convex relaxation [25, 35], or they optimize a surrogate function f\u0302 that should approximate f well [16, 18, 35].", "startOffset": 185, "endOffset": 197}, {"referenceID": 31, "context": "1 has also been shown in [32].", "startOffset": 25, "endOffset": 29}, {"referenceID": 31, "context": "Similar to the algorithms in [32, 29, 31], MUB can be extended to an iterative algorithm yielding performance gains in practice.", "startOffset": 29, "endOffset": 41}, {"referenceID": 28, "context": "Similar to the algorithms in [32, 29, 31], MUB can be extended to an iterative algorithm yielding performance gains in practice.", "startOffset": 29, "endOffset": 41}, {"referenceID": 30, "context": "Similar to the algorithms in [32, 29, 31], MUB can be extended to an iterative algorithm yielding performance gains in practice.", "startOffset": 29, "endOffset": 41}, {"referenceID": 35, "context": "1 implies improved approximation bounds for practically relevant concave over modular functions, such as those used in [36].", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "[18], as in Corollary 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "Minimizing such a function over constraints C is harder than minimizing a merely modular function, but with the algorithm in [47] we obtain an FPTAS for minimizing f\u0302 over C whenever we can minimize a nonnegative linear function over C.", "startOffset": 125, "endOffset": 129}, {"referenceID": 46, "context": "We use the important result from [47] where they show that any function of the form \u03bb1 \u221a m1(X) + \u03bb2m2(X) where \u03bb1 \u2265 0, \u03bb2 \u2265 0 and m1 and m2 are positive modular functions, has a FPTAS, provided a modular function can easily be optimized over C.", "startOffset": 33, "endOffset": 37}, {"referenceID": 51, "context": "Svitkina and Fleischer [52] prove that for monotone submodular functions of arbitrary curvature, it is impossible to find a polynomial-time algorithm with an approximation factor better than \u221a n/ log n.", "startOffset": 23, "endOffset": 27}, {"referenceID": 51, "context": "These bounds are improvements over the results of [52] whenever \u03baf < 1.", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "[16] show a O(n)-approximation with matching curvature-independent lower bound \u03a9(n).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The proof of this follows in very similar lines to the earlier lower bounds using our construction and the matroid constructions in [16].", "startOffset": 132, "endOffset": 136}, {"referenceID": 34, "context": "Minimum submodular s-t cut (SSC): This problem, also known as the cooperative cut problem [35, 36], asks to minimize a monotone submodular function f such that the solution X \u2286 E is a set of edges whose removal disconnects s from t in G.", "startOffset": 90, "endOffset": 98}, {"referenceID": 35, "context": "Minimum submodular s-t cut (SSC): This problem, also known as the cooperative cut problem [35, 36], asks to minimize a monotone submodular function f such that the solution X \u2286 E is a set of edges whose removal disconnects s from t in G.", "startOffset": 90, "endOffset": 98}, {"referenceID": 34, "context": "Using curvature refines the lower bound in [35]:", "startOffset": 43, "endOffset": 47}, {"referenceID": 34, "context": "It uses the construction from [35].", "startOffset": 30, "endOffset": 34}, {"referenceID": 34, "context": "Jegelka and Bilmes [35] demonstrate how this approximation may be optimized via a generalized maximum flow algorithm that maximizes a polymatroidal network flow [40].", "startOffset": 19, "endOffset": 23}, {"referenceID": 39, "context": "Jegelka and Bilmes [35] demonstrate how this approximation may be optimized via a generalized maximum flow algorithm that maximizes a polymatroidal network flow [40].", "startOffset": 161, "endOffset": 165}, {"referenceID": 34, "context": "We use the polymatroidal network flow construction from [35], where the approximation f\u0302 is defined via a partition of the ground set, and is separable over groups of edges.", "startOffset": 56, "endOffset": 60}, {"referenceID": 33, "context": "This approximation can be solved efficiently via generalized flows in polynomial time [34, 35].", "startOffset": 86, "endOffset": 94}, {"referenceID": 34, "context": "This approximation can be solved efficiently via generalized flows in polynomial time [34, 35].", "startOffset": 86, "endOffset": 94}, {"referenceID": 34, "context": "Then let X\u0302 be the minimizer of f\u0302(X) over C (using the generalized flows [35]).", "startOffset": 74, "endOffset": 78}, {"referenceID": 55, "context": "Such constraints occur for example in power assignment problems [56].", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "[16] show a curvature-independent optimal approximation factor of O(n) for this problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In this case, we use the construction of [16], and define f \u03ba (X) = \u03baf min{|X\u2229R\u0304|+min{|X\u2229R|, \u03b2}, \u03b1}+ (1\u2212 \u03baf )|X|, and gf (X) = \u03bafmin{|X|, \u03b1}+ (1\u2212 \u03baf )|X|, where \u03b1 = n , \u03b2 = n (1 + \u03b4) and |R| = \u03b1.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "For the formal graph construction, see [16].", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "Then with high probability R is connected in the graph [16].", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "Ana analogous analysis applies to combinatorial constraints like Steiner trees [16].", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "We use the same submodular functions as the spanning tree case, and it can be shown [16] that with high probability the set R contains a perfect matching and the two functions are indistinguishable.", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "This problem has been investigated in [25], and they show that this problem is O(n) hard.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "We can use the construction of [25] to show this.", "startOffset": 31, "endOffset": 35}, {"referenceID": 6, "context": "These results complement known results for submodular maximization [7, 55].", "startOffset": 67, "endOffset": 74}, {"referenceID": 54, "context": "These results complement known results for submodular maximization [7, 55].", "startOffset": 67, "endOffset": 74}, {"referenceID": 27, "context": "Moreover, in [28], we also consider the role of curvature in submodular optimization problems over a class of submodular constraints.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "Given that the functional form and effect of the submodularity ratio proposed in [9] is similar to that of curvature, an interesting extension is the question of whether there is a single unifying quantity for both of these terms.", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "Another open question is whether a quantity similar to curvature can be defined for subadditive functions, thus refining the results in [2, 1] for learning subadditive functions.", "startOffset": 136, "endOffset": 142}, {"referenceID": 0, "context": "Another open question is whether a quantity similar to curvature can be defined for subadditive functions, thus refining the results in [2, 1] for learning subadditive functions.", "startOffset": 136, "endOffset": 142}, {"referenceID": 33, "context": "Finally it also seems that the techniques in this paper could be used to provide improved curvature-dependent regret bounds for constrained online submodular minimization [34].", "startOffset": 171, "endOffset": 175}], "year": 2013, "abstractText": "We investigate three related and important problems connected to machine learning: approximating a submodular function everywhere, learning a submodular function (in a PAC-like setting [53]), and constrained minimization of submodular functions. We show that the complexity of all three problems depends on the \u201ccurvature\u201d of the submodular function, and provide lower and upper bounds that refine and improve previous results [3, 16, 18, 52]. Our proof techniques are fairly generic. We either use a black-box transformation of the function (for approximation and learning), or a transformation of algorithms to use an appropriate surrogate function (for minimization). Curiously, curvature has been known to influence approximations for submodular maximization [7, 55], but its effect on minimization, approximation and learning has hitherto been open. We complete this picture, and also support our theoretical claims by empirical results.", "creator": "LaTeX with hyperref package"}}}