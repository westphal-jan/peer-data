{"id": "1311.2495", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Nov-2013", "title": "The Noisy Power Method: A Meta Algorithm with Applications", "abstract": "we provide a sufficiently new robust convergence complexity analysis of the previously well - known compact subspace iteration algorithm for steadily computing the dominant singular vectors of a matrix, also known as finite simultaneous iteration or power method. our own result characterizes the peak convergence behavior of the algorithm when a large inverse amount noise is introduced after each matrix - vector multiplication. while interesting in its own right, our main motivation comes from the problem of privacy - preserving orthogonal spectral analysis where noise is added in order to achieve the privacy guarantee known as differential privacy. our contributions here are twofold :", "histories": [["v1", "Mon, 11 Nov 2013 16:47:25 GMT  (22kb,D)", "https://arxiv.org/abs/1311.2495v1", null], ["v2", "Mon, 15 Sep 2014 19:17:32 GMT  (26kb,D)", "http://arxiv.org/abs/1311.2495v2", null], ["v3", "Mon, 8 Dec 2014 21:53:05 GMT  (26kb,D)", "http://arxiv.org/abs/1311.2495v3", "NIPS 2014"], ["v4", "Tue, 3 Feb 2015 23:43:37 GMT  (27kb,D)", "http://arxiv.org/abs/1311.2495v4", "NIPS 2014"]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["moritz hardt", "eric price"], "accepted": true, "id": "1311.2495"}, "pdf": {"name": "1311.2495.pdf", "metadata": {"source": "CRF", "title": "The Noisy Power Method: A Meta Algorithm with Applications", "authors": ["Moritz Hardt", "Eric Price"], "emails": ["mhardt@us.ibm.com", "ecprice@mit.edu"], "sections": [{"heading": null, "text": "Streaming PCA. A recent work of Mitliagkas et al. (NIPS 2013) gives a space-efficient algorithm for PCA in a streaming model where samples are drawn from a gaussian spiked covariance model. We give a simpler and more general analysis that applies to arbitrary distributions confirming experimental evidence of Mitliagkas et al. Moreover, even in the spiked covariance model our result gives quantitative improvements in a natural parameter regime. It is also notably simpler and follows easily from our general convergence analysis of the noisy power method together with a matrix Chernoff bound.\nPrivate PCA. We provide the first nearly-linear time algorithm for the problem of differentially private principal component analysis that achieves nearly tight worst-case error bounds. Complementing our worst-case bounds, we show that the error dependence of our algorithm on the matrix dimension can be replaced by an essentially tight dependence on the coherence of the matrix. This result resolves the main problem left open by Hardt and Roth (STOC 2013). The coherence is always bounded by the matrix dimension but often substantially smaller thus leading to strong average-case improvements over the optimal worst-case bound."}, {"heading": "1 Introduction", "text": "Computing the dominant singular vectors of a matrix is one of the most important algorithmic tasks underlying many applications including low-rank approximation, PCA, spectral clustering, dimensionality reduction, matrix completion and topic modeling. The classical problem is well-understood, but many recent applications in machine learning face the fundamental problem of approximately finding singular vectors in the presence of noise. Noise\n\u2217IBM Research Almaden. Email: mhardt@us.ibm.com \u2020IBM Research Almaden. Email: ecprice@mit.edu\nar X\niv :1\n31 1.\n24 95\nv4 [\ncs .D\nS] 3\nF eb\ncan enter the computation through a variety of sources including sampling error, missing entries, adversarial corruptions and privacy constraints. It is desirable to have one robust method for handling a variety of cases without the need for ad-hoc analyses. In this paper we consider the noisy power method, a fast general purpose method for computing the dominant singular vectors of a matrix when the target matrix can only be accessed through inaccurate matrix-vector products.\nFigure 1 describes the method when the target matrix A is a symmetric d \u00d7 d matrix\u2014a generalization to asymmetric matrices is straightforward. The algorithm starts from an initial matrix X0 \u2208 Rd\u00d7p and iteratively attempts to perform the update rule X` \u2192 AX`. However, each such matrix product is followed by a possibly adversarially and adaptively chosen perturbation G` leading to the update rule X`\u2192 AX` +G`. It will be convenient though not necessary to maintain that X` has orthonormal columns which can be achieved through a QR-factorization after each update.\nThe noisy power method is a meta algorithm that when instantiated with different settings of G` and X0 adapts to a variety of applications. In fact, there have been a number of recent surprising applications of the noisy power method:\n1. Jain et al. [JNS13, Har14] observe that the update rule of the well-known alternating least squares heuristic for matrix completion can be considered as an instance of NPM. This lead to the first provable convergence bounds for this important heuristic.\n2. Mitgliakas et al. [MCJ13] observe that NPM applies to a streaming model of principal component analysis (PCA) where it leads to a space-efficient and practical algorithm for PCA in settings where the covariance matrix is too large to process directly.\n3. Hardt and Roth [HR13] consider the power method in the context of privacy-preserving PCA where noise is added to achieve differential privacy.\nIn each setting there has so far only been an ad-hoc analysis of the noisy power method. In the first setting, only local convergence is argued, that is, X0 has to be cleverly chosen. In the second setting, the analysis only holds for the spiked covariance model of PCA. In the third application, only the case p = 1 was considered.\nIn this work we give a completely general analysis of the noisy power method that overcomes limitations of previous analyses. Our result characterizes the global convergence properties of the algorithm in terms of the noise G` and the initial subspace X0. We then consider the important case where X0 is a randomly chosen orthonormal basis. This case is rather delicate since the initial correlation between a random matrix X0 and the target subspace is vanishing in the dimension d for small p. Another important feature of the analysis\nis that it shows how X` converges towards the first k 6 p singular vectors. Choosing p to be larger than the target dimension leads to a quantitatively stronger result. Theorem 2.4 formally states our convergence bound. Here we highlight one useful corollary to illustrate our more general result.\nCorollary 1.1. Let k 6 p. Let U \u2208Rd\u00d7k represent the top k singular vectors of A and let \u03c31 > \u00b7 \u00b7 \u00b7 > \u03c3n > 0 denote its singular values. Suppose X0 is an orthonormal basis of a random p-dimensional subspace. Further suppose that at every step of NPM we have\n5\u2016G`\u2016 6 \u03b5(\u03c3k \u2212 \u03c3k+1) and 5\u2016U>G`\u2016 6 (\u03c3k \u2212 \u03c3k+1) \u221a p\u2212 \u221a k\u22121\n\u03c4 \u221a d\nfor some fixed parameter \u03c4 and \u03b5 < 1/2. Then with all but \u03c4\u2212\u2126(p+1\u2212k) + e\u2212\u2126(d) probability, there exists an L =O( \u03c3k\u03c3k\u2212\u03c3k+1 log(d\u03c4/\u03b5)) so that after L steps we have that \u2225\u2225\u2225(I \u2212XLX>L )U\u2225\u2225\u2225 6 \u03b5. The corollary shows that the algorithm converges in the strong sense that the entire spectral norm of U up to an \u03b5 error is contained in the space spanned by XL. To achieve this the result places two assumptions on the magnitude of the noise. The total spectral norm of G` must be bounded by \u03b5 times the separation between \u03c3k and \u03c3k+1. This dependence on the singular value separation arises even in the classical perturbation theory of Davis-Kahan [DK70]. The second condition is specific to the power method and requires that the noise term is proportionally smaller when projected onto the space spanned by the top k singular vectors. This condition ensures that the correlation between X` and U that is initially very small is not destroyed by the noise addition step. If the noise term has some spherical properties (e.g. a Gaussian matrix), we expect the projection onto U to be smaller by a factor of \u221a k/d, since the space U is k-dimensional. In the case where p = k +\u2126(k) this is precisely what the condition requires. When p = k the requirement is stronger by a factor of k. This phenomenon stems from the fact that the smallest singular value of a random p \u00d7 k gaussian matrix behaves differently in the square and the rectangular case.\nWe demonstrate the usefulness of our convergence bound with several novel results in some of the aforementioned applications."}, {"heading": "1.1 Application to memory-efficient streaming PCA", "text": "In the streaming PCA setting we receive a stream of samples z1, z2, . . . zn \u2208Rd drawn i.i.d. from an unknown distribution D over Rd . Our goal is to compute the dominant k eigenvectors of the covariance matrix A = Ez\u223cD zz>. The challenge is to do this in space linear in the output size, namely O(kd). Recently, Mitgliakas et al. [MCJ13] gave an algorithm for this problem based on the noisy power method. We analyze the same algorithm, which we restate here and call SPM:\nThe algorithm can be executed in space O(pd) since the update step can compute the d \u00d7 p matrix A`X`\u22121 incrementally without explicitly computing A`. The algorithm maps to our setting by defining G` = (A` \u2212A)X`\u22121. With this notation Y` = AX`\u22121 +G`. We can apply Corollary 1.1 directly once we have suitable bounds on \u2016G`\u2016 and \u2016U>G`\u2016.\nThe result of [MCJ13] is specific to the spiked covariance model. The spiked covariance model is defined by an orthonormal basis U \u2208 Rd\u00d7k and a diagonal matrix \u039b \u2208 Rk\u00d7k with diagonal entries \u03bb1 > \u03bb2 > \u00b7 \u00b7 \u00b7 > \u03bbk > 0. The distribution D(U,\u039b) is defined as the normal distribution N(0, (U\u039b2U> + \u03c32Idd\u00d7d)). Without loss of generality we can scale the examples\nsuch that \u03bb1 = 1. One corollary of our result shows that the algorithm outputs XL such that\u2225\u2225\u2225(I \u2212XLX>L )U\u2225\u2225\u2225 6 \u03b5 with probability 9/10 provided p = k +\u2126(k) and the number of samples satisfies\nn = \u0398 \u03c36 + 1\u03b52\u03bb6k kd  .\nPreviously, the same bound1 was known with a quadratic dependence on k in the case where p = k. Here we can strengthen the bound by increasing p slightly.\nWhile we can get some improvements even in the spiked covariance model, our result is substantially more general and applies to any distribution. The sample complexity bound we get varies according to a technical parameter of the distribution. Roughly speaking, we get a near linear sample complexity if the distribution is either \u201cround\u201d (as in the spiked covariance setting) or is very well approximated by a k dimensional subspace. To illustrate the latter condition, we have the following result without making any assumptions other than scaling the distribution:\nCorollary 1.2. Let D be any distribution scaled so that Pr {\u2016z\u2016 > t} 6 exp(\u2212t) for every t > 1. Let U represent the top k eigenvectors of the covariance matrix Ezz> and \u03c31 > \u00b7 \u00b7 \u00b7 > \u03c3d > 0 its eigenvalues. Then, SPM invoked with p = k + \u2126(k) outputs a matrix XL such with probability 9/10 we have\u2225\u2225\u2225(I \u2212XLX>L )U\u2225\u2225\u2225 6 \u03b5 provided SPM receives n samples where n satisfies n = O\u0303 ( \u03c3k\u03b52k(\u03c3k\u2212\u03c3k+1)3 \u00b7 d) .\nThe corollary establishes a sample complexity that\u2019s linear in d provided that the spectrum decays quickly, as is common in applications. For example, if the spectrum follows a power law so that \u03c3j \u2248 j\u2212c for a constant c > 1/2, the bound becomes n = O\u0303(k2c+2d/\u03b52)."}, {"heading": "1.2 Application to privacy-preserving spectral analysis", "text": "Many applications of singular vector computation are plagued by the fact that the underlying matrix contains sensitive information about individuals. A successful paradigm in privacypreserving data analysis rests on the notion of differential privacy which requires all access to the data set to be randomized in such a way that the presence or absence of a single data item is hidden. The notion of data item varies and could either refer to a single entry, a single row, or a rank-1 matrix of bounded norm. More formally, Differential Privacy requires that the output distribution of the algorithm changes only slightly with the addition or deletion of\n1That the bound stated in [MCJ13] has a \u03c36 dependence is not completely obvious. There is a O(\u03c34) in the numerator and log((\u03c32 + 0.75\u03bb2k )/(\u03c3 2 + 0.5\u03bb2k )) in the denominator which simplifies to O(1/\u03c3 2) for constant \u03bbk and \u03c32 > 1.\na single data item. This requirement often necessitates the introduction of significant levels of noise that make the computation of various objectives challenging. Differentially private singular vector computation has been studied actively in recent years [BDMN05, MM09, BBDS12, CSS12, KT13, HR12, HR13, DTTZ14]. There are two main objectives. The first is computational efficiency. The second objective is to minimize the amount of error that the algorithm introduces.\nIn this work, we give a fast algorithm for differentially private singular vector computation based on the noisy power method that leads to nearly optimal bounds in a number of settings that were considered in previous work. The algorithm is described in Figure 3. It\u2019s a simple instance of NPM in which each noise matrix G` is a gaussian random matrix scaled so that the algorithm achieves (\u03b5,\u03b4)-differential privacy (as formally defined in Definition 4.1). It is easy to see that the algorithm can be implemented in time nearly linear in the number of nonzero entries of the input matrix (input sparsity). This will later lead to strong improvements in running time compared with several previous works.\nWe first state a general purpose analysis of PPM that follows from Corollary 1.1.\nTheorem 1.3. Let k 6 p. Let U \u2208 Rd\u00d7k represent the top k singular vectors of A and let \u03c31 > \u00b7 \u00b7 \u00b7 > \u03c3d > 0 denote its singular values. Then, PPM satisfies (\u03b5,\u03b4)-differential privacy and after L =O( \u03c3k\u03c3k\u2212\u03c3k+1 log(d)) iterations we have with probability 9/10 that\u2225\u2225\u2225(I \u2212XLX>L )U\u2225\u2225\u2225 6O\u03c3max\u2016X`\u2016\u221e\u221ad logL\u03c3k \u2212 \u03c3k+1 \u00b7 \u221a p \u221a p \u2212 \u221a k \u2212 1\n . When p = k + \u2126(k) the trailing factor becomes a constant. If p = k it creates a factor k overhead. In the worst-case we can always bound \u2016X`\u2016\u221e by 1 since X` is an orthonormal basis. However, in principle we could hope that a much better bound holds provided that the target subspace U has small coordinates. Hardt and Roth [HR12, HR13] suggested a way to accomplish a stronger bound by considering a notion of coherence of A, denoted as \u00b5(A). Informally, the coherence is a well-studied parameter that varies between 1 and n, but is often observed to be small. Intuitively, the coherence measures the correlation between the singular vectors of the matrix with the standard basis. Low coherence means that the singular vectors have small coordinates in the standard basis. Many results on matrix completion and robust PCA crucially rely on the assumption that the underlying matrix has low coherence [CR09, CT10, CLMW11] (though the notion of coherence here will be somewhat different).\nTheorem 1.4. Under the assumptions of Theorem 1.3, we have the conclusion\n\u2225\u2225\u2225(I \u2212XLX>L )U\u2225\u2225\u2225 6O\u03c3\u221a\u00b5(A) logd logL\u03c3k \u2212 \u03c3k+1 \u00b7 \u221a p \u221a p \u2212 \u221a k \u2212 1  . Hardt and Roth proved this result for the case where p = 1. The extension to p > 1 lost a factor of \u221a d in general and therefore gave no improvement over Theorem 1.3. Our result resolves the main problem left open in their work. The strength of Theorem 1.4 is that the bound is essentially dimension-free under a natural assumption on the matrix and never worse than our worst-case result. It is also known that in general the dependence on d achieved in Theorem 1.3 is best possible in the worst case (see discussion in [HR13]) so that further progress requires making stronger assumptions. Coherence is a natural such assumption. The proof of Theorem 1.4 proceeds by showing that each iterate X` satisfies \u2016X`\u2016\u221e 6O( \u221a \u00b5(A) log(d)/d) and applying Theorem 1.3. To do this we exploit a non-trivial symmetry of the algorithm that we discuss in Section 4.3.\nOther objective functions and variants differential privacy. An important recent work by Dwork, Talwar, Thakurta and Zhang analyzes the mechanism of adding Gaussian noise to the covariance matrix and computing a truncated singular value decomposition of the noisy covariance matrix [DTTZ14]. Their objective function is a natural measure of how much variance of the data is captured by the resulting subspace. Our results are formally incomparable due to a different choice of objective function. We also do not know how to analyze the performance of the power method under their objective function. Indeed, this is an interesting question related to the content of Conjecture 1.6 that we will state shortly.\nOur discussion above applied to (\u03b5,\u03b4)-differential privacy under changing a single entry of the matrix. Several works consider other variants of differential privacy. It is generally easy to adapt the power method to these settings by changing the noise distribution or its scaling. To illustrate this aspect, we consider the problem of privacy-preserving principal component analysis as recently studied by [CSS12, KT13]. Both works consider an algorithm called exponential mechanism. The first work gives a heuristic implementation that may not converge, while the second work gives a provably polynomial time algorithm though the running time is more than cubic. Our algorithm gives strong improvements in running time while giving nearly optimal accuracy guarantees as it matches a lower bound of [KT13] up to a O\u0303( \u221a k) factor. We also improve the error dependence on k by polynomial factors compared to previous work. Moreover, we get an accuracy improvement of O( \u221a d) for the case of (\u03b5,\u03b4)-differential privacy, while these previous works only apply to (\u03b5,0)-differential privacy. Section 4.2 provides formal statements."}, {"heading": "1.3 Related Work", "text": "Numerical Analysis. One might expect that a suitable analysis of the noisy power method would have appeared in the numerical analysis literature. However, we are not aware of a reference and there are a number of points to consider. First, our noise model is adaptive thus setting it apart from the classical perturbation theory of the singular vector decomposition [DK70]. Second, we think of the perturbation at each step as large making it conceptually different from floating point errors. Third, research in numerical analysis over the past decades\nhas largely focused on faster Krylov subspace methods. There is some theory of inexact Krylov methods, e.g., [SS07] that captures the effect of noisy matrix-vector products in this context. Related to our work are also results on the perturbation stability of the QR-factorization since those could be used to obtain convergence bounds for subspace iteration. Such bounds, however, must depend on the condition number of the matrix that the QR-factorization is applied to. See Chapter 19.9 in [Hig02] and the references therein for background. Our proof strategy avoids this particular dependence on the condition number.\nStreaming PCA. PCA in the streaming model is related to a host of well-studied problems that we cannot survey completely here. We refer to [ACLS12, MCJ13] for a thorough discussion of prior work. Not mentioned therein is a recent work on incremental PCA [BDF13] that leads to space efficient algorithms computing the top singular vector; however, it\u2019s not clear how to extend their results to computing multiple singular vectors.\nPrivacy. There has been much work on differentially private spectral analysis starting with Blum et al. [BDMN05] who used an algorithm sometimes called Randomized Response, which adds a single noise matrix N either to the input matrix A or the covariance matrix AA>. This approach was used by McSherry and Mironov [MM09] for the purpose of a differentially private recommender system. Most recently, as discussed earlier, Dwork, Talwar, Thakurta and Zhang [DTTZ14] revisit (a variant of) the this algorithm and give matching upper and lower bounds under a natural objective function. While often suitable when AA> fits into memory, the approach can be difficult to apply when the dimension of AA> is huge as it requires computing a dense noise matrix N. The power method can be applied more easily to large sparse matrices, as well as in a streaming setting as shown by [MCJ13].\nChaudhuri et al. [CSS12] and Kapralov-Talwar [KT13] use the so-called exponential mechanism to sample approximate eigenvectors of the matrix. The sampling is done using a heuristic approach without convergence polynomial time convergence guarantees in the first case and using a polynomial time algorithm in the second. Both papers achieve a tight dependence on the matrix dimension d (though the dependence on k is suboptimal in general). Most closely related to our work are the results of Hardt and Roth [HR13, HR12] that introduced matrix coherence as a way to circumvent existing worst-case lower bounds on the error. They also analyzed a natural noisy variant of power iteration for the case of computing the dominant eigenvector of A. When multiple eigenvectors are needed, their algorithm uses the well-known deflation technique. However, this step loses control of the coherence of the original matrix and hence results in suboptimal bounds. In fact, a \u221a rank(A) factor is lost."}, {"heading": "1.4 Open Questions", "text": "We believe Corollary 1.1 to be a fairly precise characterization of the convergence of the noisy power method to the top k singular vectors when p = k. The main flaw is that the noise tolerance depends on the eigengap \u03c3k \u2212 \u03c3k+1, which could be very small. We have some conjectures for results that do not depend on this eigengap.\nFirst, when p > k, we think that Corollary 1.1 might hold using the gap \u03c3k \u2212 \u03c3p+1 instead of \u03c3k \u2212 \u03c3k+1. Unfortunately, our proof technique relies on the principal angle decreasing at each step, which does not necessarily hold with the larger level of noise. Nevertheless we expect the\nprincipal angle to decrease fairly fast on average, so that XL will contain a subspace very close to U . We are actually unaware of this sort of result even in the noiseless setting.\nConjecture 1.5. Let X0 be a random p-dimensional basis for p > k. Suppose at every step we have\n100\u2016G`\u2016 6 \u03b5(\u03c3k \u2212 \u03c3p+1) and 100\u2016UTG`\u2016 6 \u221a p \u2212 \u221a k \u2212 1\n\u221a d\nThen with high probability, after L =O( \u03c3k\u03c3k\u2212\u03c3p+1 log(d/\u03b5)) iterations we have\n\u2016(I \u2212XLX>L )U\u2016 6 \u03b5.\nThe second way of dealing with a small eigengap would be to relax our goal. Corollary 1.1 is quite stringent in that it requires XL to approximate the top k singular vectors U , which gets harder when the eigengap approaches zero and the kth through p+ 1st singular vectors are nearly indistinguishable. A relaxed goal would be for XL to spectrally approximate A, that is\n\u2016(I \u2212XLX>L )A\u2016 6 \u03c3k+1 + \u03b5. (1)\nThis weaker goal is known to be achievable in the noiseless setting without any eigengap at all. In particular, [HMT11] shows that (1) happens after L =O(\u03c3k+1\u03b5 logn) steps in the noiseless setting. A plausible extension to the noisy setting would be:\nConjecture 1.6. Let X0 be a random 2k-dimensional basis. Suppose at every step we have\n\u2016G`\u2016 6 \u03b5 and \u2016UTG`\u2016 6 \u03b5 \u221a k/d\nThen with high probability, after L =O(\u03c3k+1\u03b5 logd) iterations we have that\n\u2016(I \u2212XLX>L )A\u2016 6 \u03c3k+1 +O(\u03b5)."}, {"heading": "2 Convergence of the noisy power method", "text": "Figure 1 presents our basic algorithm that we analyze in this section. An important tool in our analysis are principal angles, which are useful in analyzing the convergence behavior of numerical eigenvalue methods. Roughly speaking, we will show that the tangent of the k-th principal angle between X and the top k eigenvectors of A decreases as \u03c3k+1/\u03c3k in each iteration of the noisy power method.\nDefinition 2.1 (Principal angles). Let X and Y be subspaces of Rd of dimension at least k. The principal angles 0 6 \u03b81 6 \u00b7 \u00b7 \u00b7 6 \u03b8k between X and Y and associated principal vectors x1, . . . ,xk and y1, . . . , yk are defined recursively via\n\u03b8i(X ,Y ) = min { arccos ( \u3008x,y\u3009 \u2016x\u20162\u2016y\u20162 ) : x \u2208 X , y \u2208 Y ,x \u22a5 xj , y \u22a5 yj for all j < i } and xi , yi are the x and y that give this value. For matrices X and Y , we use \u03b8k(X,Y ) to denote the kth principal angle between their ranges."}, {"heading": "2.1 Convergence argument", "text": "We will make use of a non-recursive expression for the principal angles, defined in terms of the set Pk of p \u00d7 p projection matrices \u03a0 from p dimensions to k dimensional subspaces:\nClaim 2.2. Let U \u2208Rd\u00d7k have orthonormal columns and X \u2208Rd\u00d7p have independent columns, for p > k. Then\ncos\u03b8k(U,X) = max \u03a0\u2208Pk min x\u2208range(X\u03a0) \u2016x\u20162=1 \u2016U>x\u2016 = max \u03a0\u2208Pk min \u2016w\u20162=1 \u03a0w=w\n\u2016U>Xw\u2016 \u2016Xw\u2016 .\nFor V =U\u22a5, we have\ntan\u03b8k(U,X) = min \u03a0\u2208Pk max x\u2208range(X\u03a0) \u2016V >x\u2016 \u2016U>x\u2016 = min \u03a0\u2208Pk\nmax \u2016w\u20162=1 \u03a0w=w\n\u2016V >Xw\u2016 \u2016U>Xw\u2016 .\nFix parameters 1 6 k 6 p 6 d. In this section we consider a symmetric d \u00d7 d matrix A with singular values \u03c31 > \u03c32 > \u00b7 \u00b7 \u00b7 > \u03c3d . We let U \u2208 Rd\u00d7k contain the first k eigenvectors of A. Our main lemma shows that tan\u03b8k(U,X) decreases multiplicatively in each step.\nLemma 2.3. Let U contain the largest k eigenvectors of a symmetric matrix A \u2208 Rd\u00d7d , and let X \u2208Rd\u00d7p with XtransX = Id for some p > k. Let G \u2208Rd\u00d7p satisfy\n4\u2016U>G\u2016 6 (\u03c3k \u2212 \u03c3k+1)cos\u03b8k(U,X) 4\u2016G\u2016 6 (\u03c3k \u2212 \u03c3k+1)\u03b5.\nfor some \u03b5 < 1. Then\ntan\u03b8k(U,AX +G) 6max \u03b5,max\u03b5,(\u03c3k+1\u03c3k )1/4 tan\u03b8k(U,X) .\nProof. Let \u03a0\u2217 be the matrix projecting onto the smallest k principal angles of X, so that\ntan\u03b8k(U,X) = max \u2016w\u20162=1 \u03a0\u2217w=w\n\u2016V >Xw\u2016 \u2016U>Xw\u2016 .\nWe have that\ntan\u03b8k(U,AX +G) = min \u03a0\u2208Pk max \u2016w\u20162=1 \u03a0w=w\n\u2016V >(AX +G)w\u2016 \u2016U>(AX +G)w\u2016\n6 max \u2016w\u20162=1 \u03a0\u2217w=w\n\u2016V >AXw\u2016+ \u2016V >Gw\u2016 \u2016U>AXw\u2016 \u2212 \u2016U>Gw\u2016\n6 max \u2016w\u20162=1 \u03a0\u2217w=w\n1 \u2016U>Xw\u2016 \u00b7 \u03c3k+1\u2016V >Xw\u2016+ \u2016V >Gw\u2016 \u03c3k \u2212 \u2016U>Gw\u2016/\u2016U>Xw\u2016 (2)\nDefine \u2206 = (\u03c3k \u2212 \u03c3k+1)/4. By the assumption on G,\nmax \u2016w\u20162=1 \u03a0\u2217w=w\n\u2016U>Gw\u2016 \u2016U>Xw\u2016 6 \u2016U>G\u2016/ cos\u03b8k(U,X) 6 (\u03c3k \u2212 \u03c3k+1)/4 = \u2206.\nSimilarly, and using that 1/ cos\u03b8 6 1 + tan\u03b8 for any angle \u03b8,\nmax \u2016w\u20162=1 \u03a0\u2217w=w\n\u2016V >Gw\u2016 \u2016U>Xw\u2016 6 \u2016G\u2016/ cos\u03b8k(U,X) 6 \u03b5\u2206(1 + tan\u03b8k(U,X)).\nPlugging back into (2) and using \u03c3k = \u03c3k+1 + 4\u2206,\ntan\u03b8k(U,AX +G) 6 max \u2016w\u20162=1 \u03a0\u2217w=w\n\u2016V >Xw\u2016 \u2016U>Xw\u2016 \u00b7 \u03c3k+1 \u03c3k+1 + 3\u2206 + \u03b5\u2206(1 + tan\u03b8k(U,X)) \u03c3k+1 + 3\u2206 .\n= \u03c3k+1 + \u03b5\u2206 \u03c3k+1 + 3\u2206 tan\u03b8k(U,X) + \u03b5\u2206 \u03c3k+1 + 3\u2206 = (1\u2212 \u2206 \u03c3k+1 + 3\u2206 ) \u03c3k+1 + \u03b5\u2206 \u03c3k+1 + 2\u2206 tan\u03b8k(U,X) + \u2206 \u03c3k+1 + 3\u2206 \u03b5 6max(\u03b5, \u03c3k+1 + \u03b5\u2206 \u03c3k+1 + 2\u2206 tan\u03b8k(U,X))\nwhere the last inequality uses that the weighted mean of two terms is less than their maximum. Finally, we have that\n\u03c3k+1 + \u03b5\u2206 \u03c3k+1 + 2\u2206 6max( \u03c3k+1 \u03c3k+1 +\u2206 , \u03b5)\nbecause the left hand side is a weighted mean of the components on the right. Since \u03c3k+1\u03c3k+1+\u2206 6 ( \u03c3k+1\u03c3k+1+4\u2206 ) 1/4 = (\u03c3k+1/\u03c3k)1/4, this gives the result.\nWe can inductively apply the previous lemma to get the following general convergence result.\nTheorem 2.4. Let U represent the top k eigenvectors of the matrix A and \u03b3 = 1\u2212 \u03c3k+1/\u03c3k . Suppose that the initial subspace X0 and noise G` is such that\n5\u2016U>G`\u2016 6 (\u03c3k \u2212 \u03c3k+1)cos\u03b8k(U,X0) 5\u2016G`\u2016 6 \u03b5(\u03c3k \u2212 \u03c3k+1)\nat every stage `, for some \u03b5 < 1/2. Then there exists an L . 1\u03b3 log ( tan\u03b8k(U,X0) \u03b5 ) such that for all ` > L we have tan\u03b8(U,XL) 6 \u03b5.\nProof of Theorem 2.4. We will see that at every stage ` of the algorithm,\ntan\u03b8k(U,X`) 6max(\u03b5, tan\u03b8k(U,X0))\nwhich implies for \u03b5 6 1/2 that\ncos\u03b8k(U,X`) >min(1\u2212 \u03b52/2,cos\u03b8k(U,X0)) > 7 8 cos\u03b8k(U,X0)\nso Lemma 2.3 applies at every stage. This means that\ntan\u03b8k(U,X`+1) = tan\u03b8k(U,AX` +G) 6max(\u03b5,\u03b4 tan\u03b8k(U,X`))\nfor \u03b4 = max(\u03b5, (\u03c3k+1/\u03c3k)1/4). After\nL = log1/\u03b4 tan\u03b8k(U,X0)\n\u03b5\niterations the tangent will reach \u03b5 and remain there. Observing that\nlog(1/\u03b4) &min(log(1/\u03b5), log(\u03c3k/\u03c3k+1)) >min(1, log 1\n1\u2212\u03b3 ) >min(1,\u03b3) = \u03b3\ngives the result."}, {"heading": "2.2 Random initialization", "text": "The next lemma essentially follows from bounds on the smallest singular value of gaussian random matrices [RV09].\nLemma 2.5. For an arbitrary orthonormal U and random subspace X, we have\ntan\u03b8k(U,X) 6 \u03c4\n\u221a d\n\u221a p \u2212 \u221a k \u2212 1\nwith all but \u03c4\u2212\u2126(p+1\u2212k) + e\u2212\u2126(d) probability.\nProof. Consider the singular value decomposition U>X = A\u03a3B> of U>X. Setting \u03a0 to be matrix projecting onto the first k columns of B, we have that\ntan\u03b8k(U,X) 6 max \u2016w\u20162=1 \u03a0w=w\n\u2016V >Xw\u2016 \u2016U>Xw\u2016\n6 \u2016V >X\u2016 max \u2016w\u20162=1 \u03a0w=w 1 \u2016\u03a3B>w\u2016 = \u2016V >X\u2016 max \u2016w\u20162=1 supp(w)\u2208[k] 1 \u2016\u03a3w\u2016 = \u2016V >X\u2016 \u03c3k(U>X) .\nLet X \u223cN (0, Id\u00d7p) represent the random subspace. Then Y :=U>X \u223cN (0, Ik\u00d7p). By [RV09], for any \u03b5, the smallest singular value of Y is at least ( \u221a p \u2212 \u221a k \u2212 1)/\u03c4 with all but \u03c4\u2212\u2126(p+1\u2212k) + e\u2212\u2126(p) probability. On the other hand, \u2016X\u2016 . \u221a d with all but e\u2212\u2126(d) probability. Hence\ntan\u03b8k(U,X) . \u03c4\n\u221a d\n\u221a p \u2212 \u221a k \u2212 1\nwith the desired probability. Rescaling \u03c4 gets the result.\nWith this lemma we can prove the corollary that we stated in the introduction.\nProof of Corollary 1.1. By Claim 2.5, with the desired probability we have tan\u03b8k(U,X0) 6 \u03c4 \u221a d\u221a\np\u2212 \u221a k\u22121\n. Hence cos\u03b8k(U,X0) > 1/(1 + tan\u03b8k(U,X0)) > \u221a p\u2212 \u221a k\u22121\n2\u00b7\u03c4 \u221a d . Rescale \u03c4 and apply Theo-\nrem 2.4 to get that tan\u03b8k(U,XL) 6 \u03b5. Then \u2016(I \u2212XLX>L )U\u2016 = sin\u03b8k(U,XL) 6 tan\u03b8k(U,XL) 6 \u03b5."}, {"heading": "3 Memory efficient streaming PCA", "text": "In the streaming PCA setting we receive a stream of samples z1, z2, \u00b7 \u00b7 \u00b7 \u2208 Rd . Each sample is drawn i.i.d. from an unknown distribution D over Rd . Our goal is to compute the dominant k eigenvectors of the covariance matrix A = Ez\u223cD zz>. The challenge is to do this with small space, so we cannot store the d2 entries of the sample covariance matrix. We would like to use O(dk) space, which is necessary even to store the output.\nThe streaming power method (Figure 2, introduced by [MCJ13]) is a natural algorithm that performs streaming PCA with O(dk) space. The question that arises is how many samples it requires to achieve a given level of accuracy, for various distributions D. Using our general analysis of the noisy power method, we show that the streaming power method requires fewer samples and applies to more distributions than was previously known.\nWe analyze a broad class of distributions:\nDefinition 3.1. A distribution D over Rd is (B,p)-round if for every p-dimensional projection P and all t > 1 we have Prz\u223cD {\u2016z\u2016 > t} 6 exp(\u2212t) and Prz\u223cD { \u2016P z\u2016 > t \u00b7 \u221a Bp/d } 6 exp(\u2212t) .\nThe first condition just corresponds to a normalization of the samples drawn from D. Assuming the first condition holds, the second condition always holds with B = d/p. For this reason our analysis in principle applies to any distribution, but the sample complexity will depend quadratically on B.\nLet us illustrate this definition through the example of the spiked covariance model studied by [MCJ13]. The spiked covariance model is defined by an orthonormal basis U \u2208Rd\u00d7k and a diagonal matrix \u039b \u2208Rk\u00d7k with diagonal entries \u03bb1 > \u03bb2 > \u00b7 \u00b7 \u00b7 > \u03bbk > 0. The distribution D(U,\u039b) is defined as the normal distribution N(0, (U\u039b2U> + \u03c32Idd\u00d7d)/D) where D = \u0398(d\u03c32 + \u2211 i \u03bb 2 i ) is a normalization factor chosen so that the distribution satisfies the norm bound. Note that the the i-th eigenvalue of the covariance matrix is \u03c3i = (\u03bb 2 i + \u03c3\n2)/D for 1 6 i 6 k and \u03c3i = \u03c32/D for i > k. We show in Lemma 3.6 that the spiked covariance model D(U,\u039b) is indeed (B,p)-round for B =O( \u03bb 2 1+\u03c3 2\ntr(\u039b)/d+\u03c32 ), which is constant for \u03c3 & \u03bb1. We have the following main theorem.\nTheorem 3.2. Let D be a (B,p)-round distribution over Rd with covariance matrix A whose eigenvalues are \u03c31 > \u03c32 > \u00b7 \u00b7 \u00b7 > \u03c3d > 0. Let U \u2208 Rd\u00d7k be an orthonormal basis for the eigenvectors corresponding to the first k eigenvalues of A. Then, the streaming power method SPM returns an orthonormal basis X \u2208 Rd\u00d7p such that tan\u03b8(U,X) 6 \u03b5 with probability 9/10 provided that SPM receives n samples from D for some n satisfying\nn 6 O\u0303\n( B2\u03c3kk log 2d\n\u03b52(\u03c3k \u2212 \u03c3k+1)3d ) if p = k +\u0398(k). More generally, for all p > k one can get the slightly stronger result\nn 6 O\u0303 Bp\u03c3kmax{1/\u03b52,Bp/(\u221ap \u2212 \u221a k \u2212 1)2} log2d\n(\u03c3k \u2212 \u03c3k+1)3d  . Instantiating with the spiked covariance model gives the following:\nCorollary 3.3. In the spiked covariance model D(U,\u039b) the conclusion of Theorem 3.2 holds for p = 2k with\nn = O\u0303  (\u03bb21 + \u03c32)2(\u03bb2k + \u03c32)\u03b52\u03bb6k dk  .\nWhen \u03bb1 =O(1) and \u03bbk = \u2126(1) this becomes n = O\u0303 ( \u03c36+1 \u03b52 \u00b7 dk ) .\nWe can apply Theorem 3.2 to all distributions that have exponentially concentrated norm by setting B = d/p. This gives the following result.\nCorollary 3.4. Let D be any distribution scaled such that Prz\u223cD[\u2016z\u2016 > t] 6 exp(\u2212t) for all t > 1. Then the conclusion of Theorem 3.2 holds for p = 2k with\nn = O\u0303 (\n\u03c3k \u03b52k(\u03c3k \u2212 \u03c3k+1)3\n\u00b7 d ) .\nIf the eigenvalues follow a power law, \u03c3j \u2248 j\u2212c for a constant c > 1/2, this gives an n = O\u0303(k2c+2d/\u03b52) bound on the sample complexity."}, {"heading": "3.1 Error term analysis", "text": "Fix an orthonormal basis X \u2208 Rd\u00d7k . Let z1, . . . , zn \u223c D be samples from a distribution D with covariance matrix A and consider the matrix\nG = ( A\u2212 A\u0302 ) X ,\nwhere A\u0302 = 1n \u2211n i=1 ziz > i is the empirical covariance matrix on n samples. Then, we have that A\u0302X = AX +G. In other words, one update step of the power method executed on A\u0302 can be expressed as an update step on A with noise matrix G. This simple observation allows us to apply our analysis of the noisy power method to this setting after obtaining suitable bounds on \u2016G\u2016 and \u2016U>G\u2016.\nLemma 3.5. Let D be a (B,p)-round distribution with covariance matrix M. Then with all but O(1/n2) probability,\n\u2016G\u2016 .\n\u221a Bp log4n logd\ndn + 1 n2 and \u2016U>G\u2016 .\n\u221a B2p2 log4n logd\nd2n + 1 n2\nProof. We will use a matrix Chernoff bound to show that 1. Pr { \u2016G\u2016 > Ct log(n)2 \u221a Bp/d +O(1/n2) } 6 d exp(\u2212t2n) + 1/n2\n2. Pr { \u2016U>G\u2016 > Ct log(n)2Bp/d +O(1/n2) } 6 d exp(\u2212t2n) + 1/n2\nsetting t = \u221a\n2 n logd gives the result. However, matrix Chernoff inequality requires the distri-\nbution to satisfy a norm bound with probability 1. We will therefore create a closely related distribution D\u0303 that satisfies such a norm constraint and is statistically indistinguishable up to small error on n samples. We can then work with D\u0303 instead of D. This truncation step is standard and works because of the concentration properties of D.\nIndeed, let D\u0303 be the distribution obtained from D be replacing a sample z with 0 if\n\u2016z\u2016 > C log(n) or \u2016U>z\u2016 > C log(n) \u221a Bp/d or \u2016z>X\u2016 > C log(n) \u221a Bp/d .\nFor sufficiently large constant C, it follows from the definition of (B,p)-round that the probability that one or more of n samples from D get zeroed out is at most 1/n2. In particular, the two product distributions D(n) and D\u0303(n) have total variation distance at most 1/n2. Furthermore, we claim that the covariance matrices of the two distributions are at most O(1/n2) apart in spectral norm. Formally,\u2225\u2225\u2225\u2225\u2225 Ez\u223cDzz> \u2212 Ez\u0303\u223cD\u0303 z\u0303z\u0303> \u2225\u2225\u2225\u2225\u2225 6 1n2 \u00b7O (\u222b t>1 C2t2 log2(n)exp(\u2212t)dt ) 6O(1/n2) .\nIn the first inequality we use the fact that z only gets zeroed out with probability 1/n2. Conditional on this event, the norm of z is larger than tC log(n) with probability at most n2 exp(\u221212 tC logn) 6 exp(\u2212t).Assuming the norm is at most tC log(n) we have \u2016zz\n>\u2016 6 t2C2 log2(n) and this bounds the contribution to the spectral norm of the difference.\nNow let G\u0303 be the error matrix defined as G except that we replace the samples z1, . . . , zn by n samples z\u03031, . . . , z\u0303n from the truncated distribution D\u0303. By our preceding discussion, it now suffices to show that\n1. Pr { \u2016G\u0303\u2016 > Ct log2(n) \u221a Bp/d } 6 d exp(\u2212t2n)\n2. Pr { \u2016U>G\u0303\u2016 > Ct log2(n)Bp/d } 6 d exp(\u2212t2n)\nTo see this, let Si = z\u0303i z\u0303 > i X. We have\n\u2016Si\u2016 6 \u2016z\u0303i\u2016 \u00b7 \u2225\u2225\u2225z\u0303>i X\u2225\u2225\u2225 6 C2 log2(n) \u00b7\u221aBp/d\nSimilarly, \u2225\u2225\u2225U>Si\u2225\u2225\u2225 6 \u2016U>z\u0303i\u2016 \u00b7 \u2225\u2225\u2225z\u0303>i X\u2225\u2225\u2225 6 C2 log2(n) \u00b7 Bpd . The claims now follow directly from the matrix Chernoff bound stated in Lemma A.4."}, {"heading": "3.2 Proof of Theorem 3.2", "text": "Given Lemma 3.5 we will choose n such that the error term in each iteration satisfies the assumptions of Theorem 2.4. Let G` denote the instance of the error term G arising in the `-th iteration of the algorithm. We can find an n satisfying\nn\nlog(n)4 =O Bpmax { 1/\u03b52,Bp/( \u221a p \u2212 \u221a k \u2212 1)2 } logd\n(\u03c3k \u2212 \u03c3k+1)2d  such that by Lemma 3.5 we have that with probability 1\u2212O(1/n2),\n\u2016G`\u2016 6 \u03b5(\u03c3k \u2212 \u03c3k+1)\n5 and \u2016U>G`\u2016 6 \u03c3k \u2212 \u03c3k+1 5\n\u221a p \u2212 \u221a k \u2212 1\n\u221a d\n.\nHere we used that by definition 1/n \u03b5 and 1/n \u03c3k\u2212\u03c3k+1 and so the 1/n2 term in Lemma 3.5 is of lower order.\nWith this bound, it follows from Theorem 2.4 that after L = O(log(d/\u03b5)/(1 \u2212 \u03c3k+1/\u03c3k)) iterations we have with probability 1\u2212max{1,L/n2} that tan\u03b8(U,XL) 6 \u03b5. The over all sample complexity is therefore\nLn = O\u0303 Bp\u03c3kmax { 1/\u03b52,Bp/( \u221a p \u2212 \u221a k \u2212 1)2 } log2d\n(\u03c3k \u2212 \u03c3k+1)3d  . Here we used that 1\u2212 \u03c3k+1/\u03c3k = (\u03c3k \u2212 \u03c3k+1)/\u03c3k . This concludes the proof of Theorem 3.2."}, {"heading": "3.3 Proof of Lemma 3.6 and Corollary 3.4", "text": "Lemma 3.6. The spiked covariance model D(U,\u039b) is (B,k)-round for B =O( \u03bb 2 1+\u03c3 2\ntr(\u039b)/d+\u03c32 ).\nProof. Note that an example z \u223c D(U,\u039b) is distributed as U\u039bg + g \u2032 where g \u223cN(0,1/D)k is a standard gaussian and g \u2032 \u223c N(0,\u03c32/D)d . is a noise term. Recall, that D is the normalization term. Let P be any projection operator onto a k-dimensional space. Then,\n\u2016P z\u2016 = \u2016PU\u039bg + P g \u2032\u2016 6 \u2016PU\u039bg\u2016+ \u2016P g \u2032\u2016 6 \u2016\u039bg\u2016+ \u2016P g \u2032\u2016 6 \u03bb1\u2016g\u2016+ \u2016P g \u2032\u2016 .\nBy rotational invariance of g \u2032, we may assume that P is the projection onto the first k coordinates. Hence, \u2016P g \u2032\u2016 is distributed like the norm of N(0,\u03c32/D)k . Using standard tail bounds for the norm of a gaussian random variables, we can see that \u2016P z\u20162 =O(t(k\u03bb21 + k\u03c32)/D) with probability 1\u2212 exp(\u2212t). On the other hand, D = \u0398( \u2211k i=1\u03bb 2 i + d\u03c3\n2). We can now solve for B by setting\n\u0398( k\u03bb21 + k\u03c3 2\u2211k i=1\u03bb 2 i + d\u03c3 2 ) = Bk d\n\u21d4 B = \u0398( \u03bb21 + \u03c3 2\n1 d \u2211k i=1\u03bb 2 i + \u03c3 2 ) .\nCorollary 3.4 follows by plugging in the bound on B and the eigenvalues of the covariance matrix into our main theorem.\nProof of Corollary 3.4. In the spiked covariance model D(U,\u039b) we have\nB = \u03bb21 + \u03c3 2\nD , \u03c3k =\n\u03bb2k + \u03c3 2\nD , \u03c3k+1 =\n\u03c32 D , D =O(tr(\u039b2) + d\u03c32) .\nHence, B2\u03c3k\n(\u03c3k \u2212 \u03c3k+1)3d =\n(\u03bb21 + \u03c3 2)2(\u03bb2k + \u03c3 2)\n\u03bb6kd 6\n(\u03bb21 + \u03c3 2)3\n\u03bb6kd\nPlugging this bound into Theorem 3.2 gives Corollary 3.4."}, {"heading": "4 Privacy-preserving singular vector computation", "text": "In this section we prove our results about privacy-preserving singular vector computation. We begin with a standard definition of differential privacy, sometimes referred to as entry-level differential privacy, as it hides the presence or absence of a single entry.\nDefinition 4.1 (Differential Privacy). A randomized algorithmM : Rd\u00d7d \u2032 \u2192 R (where R is some arbitrary abstract range) is (\u03b5,\u03b4)-differentially private if for all pairs of matrices A,A\u2032 \u2208 Rd\u00d7d\u2032 differing in only one entry by at most 1 in absolute value, we have that for all subsets of the range S \u2286 R, the algorithm satisfies: Pr {M(A) \u2208 S} 6 exp(\u03b5)Pr {M(A\u2032) \u2208 S}+ \u03b4 .\nThe definition is most meaningful when A has entries in [0,1] so that the above definition allows for a single entry to change arbitrarily within this range. However, this is not a requirement for us. The privacy guarantee can be strengthened by decreasing \u03b5 > 0.\nFor our choice of \u03c3 in Figure 3 the algorithm satisfies (\u03b5,\u03b4)-differential privacy as follows easily from properties of the Gaussian distribution. See, for example, [HR13] for a proof.\nClaim 4.2. PPM satisfies (\u03b5,\u03b4)-differential privacy.\nIt is straightforward to prove Theorem 1.3 by invoking our convergence analysis of the noisy power method together with suitable error bounds. The error bounds are readily available as the noise term is just gaussian.\nProof of Theorem 1.3. Let m = max\u2016X`\u2016\u221e. By Lemma A.2 the following bounds hold with probability 99/100:\n1. maxL`=1 \u2016G`\u2016 . \u03c3m \u221a d logL\n2. maxL`=1 \u2016U >G`\u2016 . \u03c3m\n\u221a k logL\nLet\n\u03b5\u2032 = \u03c3m\n\u221a d logL\n\u03c3k \u2212 \u03c3k+1 & 5maxL`=1 \u2016G`\u2016 \u03c3k \u2212 \u03c3k+1 .\nBy Corollary 1.1, if we also have that maxL`=1 \u2016U >G`\u2016 6 (\u03c3k \u2212 \u03c3k+1)\n\u221a p\u2212 \u221a k\u22121\n\u03c4 \u221a d for a sufficiently large constant \u03c4 , then we will have that\n\u2016(I \u2212XLX>L )U\u2016 6 \u03b5 \u2032 6\n\u03c3m \u221a d logL\n\u03c3k \u2212 \u03c3k+1\nafter the desired number of iterations, giving the theorem. Otherwise,\n(\u03c3k \u2212 \u03c3k+1) \u221a p \u2212 \u221a k \u2212 1\n\u03c4 \u221a d\n6 L max `=1 \u2016U>G`\u2016 . \u03b5\u2032(\u03c3k \u2212 \u03c3k+1)\n\u221a k/d,\nso it is trivially true that \u03c3m \u221a d logL\n\u03c3k \u2212 \u03c3k+1\n\u221a p\n\u221a p \u2212 \u221a k \u2212 1\n> \u03b5\u2032 \u221a k\n\u221a p \u2212 \u221a k \u2212 1\n& 1 > \u2016(I \u2212XLX>L )U\u2016."}, {"heading": "4.1 Low-rank approximation", "text": "Our results readily imply that we can compute accurate differentially private low-rank approximations. The main observation is that, assuming XL and U have the same dimension, tan\u03b8(U,XL) 6 \u03b1 implies that the matrix XL also leads to a good low-rank approximation for A in the spectral norm. In particular\n\u2016(I \u2212XLX>L )A\u2016 6 \u03c3k+1 +\u03b1\u03c31 . (3)\nMoreover the projection step of computing XLX > L A can be carried out easily in a privacypreserving manner. It is again the `\u221e-norm of the columns of XL that determine the magnitude of noise that is needed. Since A is symmetric, we have X>A = (AX)>. Hence, to obtain a good low-rank approximation it suffices to compute the product AXL privately as AXL +GL. This leads to the following corollary.\nCorollary 4.3. Let A \u2208 Rd\u00d7d be a symmetric matrix with singular values \u03c31 > . . . > \u03c3d and let \u03b3 = 1\u2212\u03c3k+1/\u03c3k . There is an (\u03b5,\u03b4)-differentially private algorithm that given A and k, outputs a rank 2k matrix B such that with probability 9/10,\n\u2016A\u2212B\u2016 6 \u03c3k+1 + O\u0303 \u03c31\u221a(k/\u03b3)d logd log(1/\u03b4)\u03b5(\u03c3k \u2212 \u03c3k+1)  . The O\u0303-notation hides the factor O (\u221a log(log(d)/\u03b3) ) .\nProof. Apply Theorem 1.3 with p = 2k and run the algorithm for L + 1 steps with L = O(\u03b3\u22121 logd). This gives the bound\n\u03b1 = \u2016(I \u2212XLX>L )A\u2016 6O \u221a(k/\u03b3)d logd log(log(d)/\u03b3) log(1/\u03b4)\u03b5(\u03c3k \u2212 \u03c3k+1)  . Moreover, the algorithm has computed YL+1 = AXL +GL and we have B = XLY > L+1 = XLX > L A+ XLG > L . Therefore\n\u2016A\u2212B\u2016 6 \u03c3k+1 +\u03b1\u03c31 + \u2225\u2225\u2225XLG>L \u2225\u2225\u2225\nwhere \u2225\u2225\u2225XLG>L \u2225\u2225\u2225 6 \u2016GL\u2016 . By definition of the algorithm and Lemma A.2, we have\n\u2016GL\u2016 6O (\u221a \u03c32d ) =O (1 \u03b5 \u221a (k/\u03b3)d log(d) log(1/\u03b4) ) .\nGiven that the \u03b1-term gets multiplied by \u03c31, this bound on \u2016GL\u2016 is of lower order and the corollary follows."}, {"heading": "4.2 Principal Component Analysis", "text": "Here we illustrate that our bounds directly imply results for the privacy notion studied by Kapralov and Talwar [KT13]. The notion is particularly relevant in a setting where we think of A as a sum of rank 1 matrices each of bounded spectral norm.\nDefinition 4.4. A randomized algorithm M : Rd\u00d7d \u2032 \u2192 R (where R is some arbitrary abstract range) is (\u03b5,\u03b4)-differentially private under unit spectral norm changes if for all pairs of matrices A,A\u2032 \u2208 Rd\u00d7d\u2032 satisfying \u2016A \u2212 A\u2032\u20162 6 1, we have that for all subsets of the range S \u2286 R, the algorithm satisfies: Pr {M(A) \u2208 S} 6 exp(\u03b5)Pr {M(A\u2032) \u2208 S}+ \u03b4 .\nLemma 4.5. If PPM is executed with each G` sampled independently as G` \u223c N (0,\u03c32)d\u00d7p with \u03c3 = \u03b5\u22121 \u221a 4pL log(1/\u03b4), then PPM satisfies (\u03b5,\u03b4)-differential privacy under unit spectral norm changes. If G` is sampled with i.i.d. Laplacian entries G` \u223c Lap(0,\u03bb)n\u00d7k where \u03bb = 10\u03b5\u22121pL \u221a d, then PPM satisfies (\u03b5,0)-differential privacy under unit spectral norm changes.\nProof. The first claim follows from the privacy proof in [HR12]. We sketch the argument here for completeness. LetD be any matrix with \u2016D\u20162 6 1 (thought of as A\u2212A\u2032 in Definition 4.4) and let \u2016x\u2016 = 1 be any unit vector which we think of as one of the columns of X = X`\u22121. Then, we have \u2016Dx\u2016 6 \u2016D\u2016\u00b7\u2016x\u2016 6 1, by definition of the spectral norm. This shows that the \u201c`2-sensitivity\u201d of one matrix-vector multiplication in our algorithm is bounded by 1. It is well-known that it suffices to add Gaussian noise scaled to the `2-sensitivity of the matrix-vector product in order to achieve differential privacy. Since there are kL matrix-vector multiplications in total we need to scale the noise by a factor of \u221a kL.\nThe second claim follows analogously. Here however we need to scale the noise magnitude to the \u201c`1-sensitivity\u201d of the matrix-vector product which be bound by \u221a n using CauchySchwarz. The claim then follows using standard properties of the Laplacian mechanism.\nGiven the previous lemma it is straightforward to derive the following corollaries.\nCorollary 4.6. Let A \u2208 Rd\u00d7d be a symmetric matrix with singular values \u03c31 > . . . > \u03c3d and let \u03b3 = 1\u2212 \u03c3k+1/\u03c3k . There is an algorithm that given a A and parameter k, preserves (\u03b5,\u03b4)-differentially privacy under unit spectral norm changes and outputs a rank 2k matrix B such that with probability 9/10,\n\u2016A\u2212B\u2016 6 \u03c3k+1 + O\u0303 \u03c31\u221a(k/\u03b3)d logd log(1/\u03b4)\u03b5(\u03c3k \u2212 \u03c3k+1)  . The O\u0303-notation hides the factor O (\u221a log(log(d)/\u03b3) ) .\nProof. The proof is analogous to the proof of Corollary 4.3.\nA similar corollary applies to (\u03b5,0)-differential privacy.\nCorollary 4.7. Let A \u2208 Rd\u00d7d be a symmetric matrix with singular values \u03c31 > . . . > \u03c3d and let \u03b3 = 1\u2212 \u03c3k+1/\u03c3k . There is an algorithm that given a A and parameter k, preserves (\u03b5,\u03b4)-differentially privacy under unit spectral norm changes and outputs a rank 2k matrix B such that with probability 9/10,\n\u2016A\u2212B\u2016 6 \u03c3k+1 + O\u0303 ( \u03c31k\n1.5d log(d) log(d/\u03b3) \u03b5\u03b3(\u03c3k \u2212 \u03c3k+1)\n) .\nProof. We invoke PPM with p = 2k and Laplacian noise with the scaling given by Lemma 4.5 so that the algorithm satisfies (\u03b5,0)-differential privacy. Specifically, G` \u223c Lap(0,\u03bb)d\u00d7p where \u03bb = 10\u03b5\u22121pL \u221a d. Lemma A.3. Indeed, with probability 99/100, we have\n1. maxL`=1 \u2016G`\u2016 6O ( \u03bb \u221a kd log(kdL) ) =O ( (1/\u03b5\u03b3)k1.5d log(d) log(kdL) ) 2. maxL`=1 \u2016U >G`\u2016 6O (\u03bbk log(kL)) =O ( (1/\u03b5\u03b3)k2 \u221a d log(d) log(kL) )\nWe can now plug these error bounds into Corollary 1.1 to obtain the bound\u2225\u2225\u2225(I \u2212XLX>L )U\u2225\u2225\u2225 6O (k1.5d log(d) log(d/\u03b3)\u03b5\u03b3(\u03c3k \u2212 \u03c3k+1) )\nRepeating the argument from the proof of Corollary 4.3 gives the stated guarantee for low-rank approximation.\nThe bound above matches a lower bound shown by Kapralov and Talwar [KT13] up to a factor of O\u0303( \u221a k). We believe that this factor can be eliminated from our bounds by using a quantitatively stronger version of Lemma A.3. Compared to the upper bound of [KT13] our algorithm is faster by a more than a quadratic factor in d. Moreover, previously only bounds for (\u03b5,0)-differential privacy were known for the spectral norm privacy notion, whereas our bounds strongly improve when going to (\u03b5,\u03b4)-differential privacy."}, {"heading": "4.3 Dimension-free bounds for incoherent matrices", "text": "The guarantee in Theorem 1.3 depends on the quantity \u2016X`\u2016\u221e which could in principle be as small as \u221a 1/d. Yet, in the above theorems, we use the trivial upper bound 1. This in turn resulted in a dependence on the dimensions of A in our theorems. Here, we show that the dependence on the dimension can be replaced by an essentially tight dependence on the coherence of the input matrix. In doing so, we resolve the main open problem left open by Hardt and Roth [HR13]. The definition of coherence that we will use is formally defined as follows.\nDefinition 4.8 (Matrix Coherence). We say that a matrix A \u2208Rd\u00d7d\u2032 with singular value decomposition A =U\u03a3V > has coherence\n\u00b5(A) def= { d\u2016U\u20162\u221e,d\u2032\u2016V \u20162\u221e } .\nHere \u2016U\u2016\u221e = maxij |Uij | denotes the largest entry of U in absolute value.\nOur goal is to show that the `\u221e-norm of the vectors arising in PPM is closely related to the coherence of the input matrix. We obtain a nearly tight connection between the coherence of the matrix and the `\u221e-norm of the vectors that PPM computes.\nTheorem 4.9. Let A \u2208Rd\u00d7d be symmetric. Suppose NPM is invoked on A, and L 6 n, with each G` sampled from N (0,\u03c32` ) d\u00d7p for some \u03c3` > 0. Then, with probability 1\u2212 1/n,\nL max `=1 \u2016X`\u20162\u221e 6O\n( \u00b5(A) log(d)\nd\n) .\nProof. Fix ` \u2208 [L]. Let A = \u2211n i=1\u03c3iuiu > i be given in its eigendecomposition. Note that\nB = d\nmax i=1 \u2016ui\u2016\u221e 6 \u221a \u00b5(A) d .\nWe may write any column x of X` as x = \u2211d i=1 si\u03b1iui where \u03b1i are non-negative scalars such\nthat \u2211d i=1\u03b1 2 i = 1, and si \u2208 {\u22121,1} where si = sign(\u3008x,ui\u3009). Hence, by Lemma 4.13 (shown below),\nthe signs (s1, . . . , sd) are distributed uniformly at random in {\u22121,1}d . Hence, by Lemma 4.14 (shown below), it follows that Pr { \u2016x\u2016\u221e > 4B \u221a logd } 6 1/n3 . By a union bound over all p 6 d\ncolumns it follows that Pr { \u2016X`\u2016\u221e > 4B \u221a logd } 6 1/d2 . Another union bound over all L 6 d steps completes the proof.\nThe previous theorem states that no matter what the scaling of the Gaussian noise is in each step of the algorithm, so long as it is Gaussian the algorithm will maintain that X` has small coordinates. We cannot hope to have coordinates smaller than \u221a \u00b5(A)/d, since eventually the algorithm will ideally converge to U. This result directly implies the theorem we stated in the introduction.\nProof of Theorem 1.4. The claim follows directly from Theorem 1.3 after applying Theorem 4.9 which shows that with probability 1\u2212 1/n,\nL max `=1 \u2016X`\u20162\u221e 6O\n( \u00b5(A) log(d)\nd\n) ."}, {"heading": "4.4 Proofs of supporting lemmas", "text": "We will now establish Lemma 4.13 and Lemma 4.14 that were needed in the proof of the previous theorem. For that purpose we need some basic symmetry properties of the QR-factorization. To establish these properties we recall the Gram-Schmidt algorithm for computing the QRfactorization.\nDefinition 4.10 (Gram-Schmidt). The Gram-Schmidt orthonormalization algorithm, denoted GS, is given an input matrix V \u2208 Rd\u00d7p with columns v1, . . . , vp and outputs an orthonormal matrixQ \u2208Rd\u00d7p with the same range as V . The columns q1, . . . , qp ofQ are computed as follows: For i = 1 to p do:\n\u2013 rii \u2190 \u2016vi\u2016 \u2013 qi \u2190 vi/rii \u2013 For j = i + 1 to p do:\n\u2013 rij \u2190 \u3008qi ,vj\u3009 \u2013 vj \u2190 vj \u2212 rijqi\nThe first states that the Gram-Schmidt operation commutes with an orthonormal transformation of the input.\nLemma 4.11. Let V \u2208 Rd\u00d7p and let O \u2208 Rd\u00d7d be an orthonormal matrix. Then, GS(OV ) = O \u00d7GS(V ).\nProof. Let {rij}ij\u2208[p] denote the scalars computed by the Gram-Schmidt algorithm as specified in Definition 4.10. Notice that each of the numbers {rij}ij\u2208[p] is invariant under an orthonormal transformation of the vectors v1, . . . , vp. This is because \u2016Ovi\u2016 = \u2016vi\u2016 and \u3008Ovi ,Ovj\u3009 = \u3008vi ,vj\u3009. Moreover, The output Q of Gram-Schmidt on input of V satisfies Q = VR, where R is an upper right triangular matrix which only depends on the numbers {rij}i,j\u2208[p]. Hence, the matrix R is identical when the input is OV . Thus, GS(OV ) =OVR =O \u00d7GS(V ).\nGiven i.i.d. Gaussian matrices G0,G1, . . . ,GL \u223cN (0,1)d\u00d7p, we can describe the behavior of our algorithm by a deterministic function f (G0,G1, . . . ,GL) which executes subspace iteration starting with G0 and then suitably scales G` in each step. The next lemma shows that this function is distributive with respect to orthonormal transformations.\nLemma 4.12. Let f : (Rd\u00d7p)L\u2192Rn\u00d7p denote the output of PPM on input of a matrix A \u2208Rn\u00d7n as a function of the noise matrices used by the algorithm as described above. Let O be an orthonormal matrix with the same eigenbasis as A. Then,\nf (OG0,OG1, . . . ,OGL) =O \u00d7 f (G0, . . . ,GL) . (4)\nProof. For ease of notation we will denote by X0, . . . ,XL the iterates of the algorithm when the noise matrices are G0, . . . ,GL, and we denote by Y0, . . . ,YL the iterates of the algorithm when the noise matrices are OG0, . . . ,OGL. In this notation, our goal is to show that YL =OXL.\nWe will prove the claim by induction on L. For L = 0, the base case follows from Lemma 4.11. Indeed,\nY0 = GS(OG0) =O \u00d7GS(G0) =OX0 .\nLet ` > 1. We assume the claim holds for ` \u2212 1 and show that it holds for `. We have,\nY` = GS(AY`\u22121 +OG`)\n= GS(AOX`\u22121 +OG`) (by induction hypothesis)\n= GS(O(AX`\u22121 +G`)) (A and O commute) =O \u00d7GS(AX`\u22121 +G`) (Lemma 4.11) =OX` .\nNote that A and O commute, since they share the same eigenbasis by the assumption of the lemma. This is what we needed to prove.\nThe previous lemmas lead to the following result characterizing the distribution of signs of inner products between the columns of X` and the eigenvectors of A.\nLemma 4.13 (Sign Symmetry). Let A be a symmetric matrix given in its eigendecomposition as A = \u2211d i=1\u03bbiuiu > i . Let ` > 0 and let x be any column of X`, where X` is the iterate of PPM on input of A. Put Si = sign(\u3008ui ,x\u3009) for i \u2208 [d]. Then (S1, . . . ,Sd) is uniformly distributed in {\u22121,1}d .\nProof. Let (z1, . . . , zd) \u2208 {\u22121,1}d be a uniformly random sign vector. Let O = \u2211d i=1 ziuiu > i . Note that O is an orthonormal transformation. Clearly, any column Ox of OX` satisfies the conclusion of the lemma, since \u3008ui ,Ox\u3009 = zi\u3008ui ,x\u3009. Since the Gaussian distribution is rotationally invariant, we have that OG` and G` follow the same distribution. In particular, denoting by Y` the matrix computed by the algorithm if OG0, . . . ,OG` were chosen, we have that Y` and X` are identically distributed. Finally, by Lemma 4.12, we have that Y` =OX`. By our previous observation this means that Y` satisfies the conclusion of the lemma. As Y` and X` are identically distributed, the claim also holds for X`.\nWe will use the previous lemma to bound the `\u221e-norm of the intermediate matrices X` arising in power iteration in terms of the coherence of the input matrix. We need the following large deviation bound.\nLemma 4.14. Let \u03b11, . . . ,\u03b1d be scalars such that \u2211d i=1\u03b1 2 i = 1 and u1, . . . ,ud are unit vectors in R n. Put B = maxdi=1 \u2016ui\u2016\u221e. Further let (s1, . . . , sd) be chosen uniformly at random in {\u22121,1} d . Then,\nPr  \u2225\u2225\u2225\u2225\u2225\u2225\u2225 d\u2211 i=1 si\u03b1iui \u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u221e > 4B \u221a logd  6 1/d3 . Proof. Let X = \u2211d i=1Xi where Xi = si\u03b1iui . We will bound the deviation of X in each entry and\nthen take a union bound over all entries. Consider Z = \u2211d i=1Zi where Zi is the first entry of Xi .\nThe argument is identical for all other entries of X. We have EZ = 0 and EZ2 = \u2211d i=1EZ 2 i 6\nB2 \u2211d i=1\u03b1 2 i = B 2. Hence, by Theorem A.1 (Chernoff bound),\nPr { |Z | > 4B \u221a log(d) } 6 exp ( \u221216B\n2 log(d) 4B2\n) 6 exp(\u22124log(d)) = 1\nd4 .\nThe claim follows by taking a union bound over all d entries of X."}, {"heading": "A Deferred Concentration Inequalities", "text": "Theorem A.1 (Chernoff bound). Let the random variables X1, . . . ,Xm be independent random variables such that for every i, Xi \u2208 [\u22121,1] almost surely. Let X = \u2211m i=1Xi and let \u03c3\n2 = V X. Then, for any t > 0, Pr {|X \u2212EX | > t} 6 exp ( \u2212 t24\u03c32 ) .\nThe next lemma follows from standard concentration properties of the Gaussian distribution.\nLemma A.2. Let U \u2208 Rd\u00d7k be a matrix with orthonormal columns. Let G1, . . . ,GL \u223c N (0,\u03c32)d\u00d7p with k 6 p 6 d and assume that L 6 d. Then, with probability 1\u2212 10\u22124,\nmax `\u2208[L] \u2016U>G`\u2016 6O\n( \u03c3 \u221a p+ logL ) .\nProof. By rotational invariance of G` the spectral norm \u2016U>G`\u2016 is distributed like largest singular value of a random draw from k\u00d7p gaussian matrix N(0,\u03c32)k\u00d7p. Since p > k, the largest singular value strongly concentrates around O(\u03c3 \u221a p) with a gaussian tail. By the gaussian concentration of Lipschitz functions of gaussians, taking the maximum over L gaussian matrices introduces an additive O(\u03c3 \u221a logL) term.\nWe also have an analogue of the previous lemma for the Laplacian distribution.\nLemma A.3. Let U \u2208Rn\u00d7k be a matrix with orthonormal columns. Let G1, . . . ,GL \u223c Lap(0,\u03bb)d\u00d7p with k 6 p 6 d and assume that L 6 d. Then, with probability 1\u2212 10\u22124,\nmax `\u2208[L] \u2016U>G`\u2016 6O\n( \u03bb \u221a pk log(Lpk) ) .\nProof. We claim that with probability 1\u221210\u22124 for every ` \u2208 [L], every entry ofU>G` is bounded by O(\u03bb log(Lpk)) in absolute value. This follows because each entry has variance \u03bb2 and is a weighted sum of n independent Laplacian random variables Lap(0,\u03bb). Assuming this event occurs, we have\nmax `\u2208[L] \u2016U>G`\u2016 6max `\u2208[L] \u2016U>G`\u2016F 6O\n( \u03bb \u221a pk log(Lpk) ) .\nLemma A.4 (Matrix Chernoff). Let X1, . . . ,Xn \u223c X be i.i.d. random matrices of maximum dimension d and mean \u00b5, uniformly bounded by \u2016X\u2016 6 R. Then for all t 6 1,\nPr {\u2225\u2225\u22251n\u2211iXi \u2212EX1\u2225\u2225\u2225 > tR} 6 de\u2212\u2126(mt2)"}, {"heading": "B Reduction to symmetric matrices", "text": "For all our purposes it suffices to consider symmetric n\u00d7n matrices. Given a non-symmetric m \u00d7 n matrix B we may always consider the (m + n) \u00d7 (m + n) matrix A = [0B |B>0]. This transformation preserves all the parameters that we are interested in as was argued in [HR13] more formally. This allows us to discuss symmetric eigendecompositions rather than singular vector decompositions and therefore simplify our presentation below."}], "references": [{"title": "Stochastic optimization for pca and pls", "author": ["Raman Arora", "Andrew Cotter", "Karen Livescu", "Nathan Srebro"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "The JohnsonLindenstrauss transform itself preserves differential privacy", "author": ["Jeremiah Blocki", "Avrim Blum", "Anupam Datta", "Or Sheffet"], "venue": "In Proc. 53rd Foundations of Computer Science (FOCS),", "citeRegEx": "Blocki et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blocki et al\\.", "year": 2012}, {"title": "The fast convergence of incremental PCA", "author": ["Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund"], "venue": "In Proc. 27th Neural Information Processing Systems (NIPS),", "citeRegEx": "Balsubramani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balsubramani et al\\.", "year": 2013}, {"title": "Practical privacy: the SuLQ framework", "author": ["Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim"], "venue": "In Proc. 24th PODS,", "citeRegEx": "Blum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2005}, {"title": "Robust principal component analysis", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Yi Ma", "John Wright"], "venue": "J. ACM,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J. Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computional Mathematics,", "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E", "shortCiteRegEx": "Cand\u00e8s and Recht.", "year": 2009}, {"title": "Near-optimal differentially private principal components", "author": ["Kamalika Chaudhuri", "Anand Sarwate", "Kaushik Sinha"], "venue": "In Proc. 26th Neural Information Processing Systems (NIPS),", "citeRegEx": "Chaudhuri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2012}, {"title": "The power of convex relaxation: nearoptimal matrix completion", "author": ["Emmanuel J. Cand\u00e8s", "Terence Tao"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2010\\E", "shortCiteRegEx": "Cand\u00e8s and Tao.", "year": 2010}, {"title": "The rotation of eigenvectors by a perturbation", "author": ["Chandler Davis", "W.M. Kahan"], "venue": "iii. SIAM J. Numer. Anal.,", "citeRegEx": "Davis and Kahan.,? \\Q1970\\E", "shortCiteRegEx": "Davis and Kahan.", "year": 1970}, {"title": "Analyze Gauss: optimal bounds for privacy-preserving principal component analysis", "author": ["Cynthia Dwork", "Kunal Talwar", "Abhradeep Thakurta", "Li Zhang"], "venue": "In Proc. 46th Symposium on Theory of Computing (STOC),", "citeRegEx": "Dwork et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2014}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": "In Proc. 55th Foundations of Computer Science (FOCS). IEEE,", "citeRegEx": "Hardt.,? \\Q2014\\E", "shortCiteRegEx": "Hardt.", "year": 2014}, {"title": "Accuracy and Stability of Numerical Algorithms", "author": ["Nicholas J. Higham"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "Higham.,? \\Q2002\\E", "shortCiteRegEx": "Higham.", "year": 2002}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Nathan Halko", "Per-Gunnar Martinsson", "Joel A. Tropp"], "venue": "SIAM Review,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Beating randomized response on incoherent matrices", "author": ["Moritz Hardt", "Aaron Roth"], "venue": "In Proc. 44th Symposium on Theory of Computing (STOC),", "citeRegEx": "Hardt and Roth.,? \\Q2012\\E", "shortCiteRegEx": "Hardt and Roth.", "year": 2012}, {"title": "Beyond worst-case analysis in private singular vector computation", "author": ["Moritz Hardt", "Aaron Roth"], "venue": "In Proc. 45th Symposium on Theory of Computing (STOC). ACM,", "citeRegEx": "Hardt and Roth.,? \\Q2013\\E", "shortCiteRegEx": "Hardt and Roth.", "year": 2013}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Proc. 45th Symposium on Theory of Computing (STOC),", "citeRegEx": "Jain et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2013}, {"title": "On differentially private low rank approximation", "author": ["Michael Kapralov", "Kunal Talwar"], "venue": "In Proc. 24rd Symposium on Discrete Algorithms (SODA). ACM-SIAM,", "citeRegEx": "Kapralov and Talwar.,? \\Q2013\\E", "shortCiteRegEx": "Kapralov and Talwar.", "year": 2013}, {"title": "Memory limited, streaming PCA", "author": ["Ioannis Mitliagkas", "Constantine Caramanis", "Prateek Jain"], "venue": "In Proc. 27th Neural Information Processing Systems (NIPS),", "citeRegEx": "Mitliagkas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mitliagkas et al\\.", "year": 2013}, {"title": "Differentially private recommender systems: building privacy into the net", "author": ["Frank McSherry", "Ilya Mironov"], "venue": "In Proc. 15th KDD,", "citeRegEx": "McSherry and Mironov.,? \\Q2009\\E", "shortCiteRegEx": "McSherry and Mironov.", "year": 2009}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Rudelson and Vershynin.,? \\Q2009\\E", "shortCiteRegEx": "Rudelson and Vershynin.", "year": 2009}, {"title": "Recent computational developments in krylov subspace methods for linear systems", "author": ["Valeria Simoncini", "Daniel B. Szyld"], "venue": "Numerical Linear Algebra With Applications,", "citeRegEx": "Simoncini and Szyld.,? \\Q2007\\E", "shortCiteRegEx": "Simoncini and Szyld.", "year": 2007}], "referenceMentions": [], "year": 2015, "abstractText": "We provide a new robust convergence analysis of the well-known power method for computing the dominant singular vectors of a matrix that we call the noisy power method. Our result characterizes the convergence behavior of the algorithm when a significant amount noise is introduced after each matrix-vector multiplication. The noisy power method can be seen as a meta-algorithm that has recently found a number of important applications in a broad range of machine learning problems including alternating minimization for matrix completion, streaming principal component analysis (PCA), and privacy-preserving spectral analysis. Our general analysis subsumes several existing adhoc convergence bounds and resolves a number of open problems in multiple applications: Streaming PCA. A recent work of Mitliagkas et al. (NIPS 2013) gives a space-efficient algorithm for PCA in a streaming model where samples are drawn from a gaussian spiked covariance model. We give a simpler and more general analysis that applies to arbitrary distributions confirming experimental evidence of Mitliagkas et al. Moreover, even in the spiked covariance model our result gives quantitative improvements in a natural parameter regime. It is also notably simpler and follows easily from our general convergence analysis of the noisy power method together with a matrix Chernoff bound. Private PCA. We provide the first nearly-linear time algorithm for the problem of differentially private principal component analysis that achieves nearly tight worst-case error bounds. Complementing our worst-case bounds, we show that the error dependence of our algorithm on the matrix dimension can be replaced by an essentially tight dependence on the coherence of the matrix. This result resolves the main problem left open by Hardt and Roth (STOC 2013). The coherence is always bounded by the matrix dimension but often substantially smaller thus leading to strong average-case improvements over the optimal worst-case bound.", "creator": "LaTeX with hyperref package"}}}