{"id": "1311.3287", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2013", "title": "Nonparametric Estimation of Multi-View Latent Variable Models", "abstract": "spectral smoothing methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. however, current spectral algorithms are largely restricted to mixtures resulting of discrete or gaussian distributions. in this paper, we effectively propose a kernel method for learning multi - view generic latent variable models, allowing each mixture component to basically be nonparametric. the key idea goal of the method is to embed the finite joint distribution of a multi - view latent continuous variable into entering a reproducing kernel hilbert space, and only then the correct latent parameters are recovered using a globally robust tensor power method. formally we establish that the sample complexity for the proposed method is quadratic in considering the given number of latent components and accordingly is a low order polynomial in preserving the other relevant parameters. thus, our non - parametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. moreover, the non - parametric tensor temporal power reconstruction method compares favorably accuracy to em algorithm and other existing spectral algorithms in our experiments.", "histories": [["v1", "Wed, 13 Nov 2013 20:42:21 GMT  (101kb,D)", "https://arxiv.org/abs/1311.3287v1", null], ["v2", "Sun, 8 Dec 2013 01:58:58 GMT  (99kb,D)", "http://arxiv.org/abs/1311.3287v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["le song", "animashree anandkumar", "bo dai", "bo xie 0002"], "accepted": true, "id": "1311.3287"}, "pdf": {"name": "1311.3287.pdf", "metadata": {"source": "CRF", "title": "Nonparametric Estimation of Multi-View Latent Variable Models", "authors": ["Le Song", "Animashree Anandkumar", "Bo Dai", "Bo Xie"], "emails": ["lsong@cc.gatech.edu", "a.anandkumar@uci.edu", "bodai@gatech.edu", "zixu1986@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Latent variable models have been used to address various machine learning problems, ranging from modeling temporal dynamics, to text document analysis and to social network analysis (Rabiner & Juang, 1986; Clark, 1990; Hoff et al., 2002; Blei et al., 2003). Recently, there is a surge of interest in designing spectral algorithms for estimating the parameters of latent variable models (Hsu et al., 2009; Parikh et al., 2011; Song et al., 2011; Foster et al., 2012; Anandkumar et al., 2012a,b; Kira\u0301ly, 2013). Compared to the Expectation-Maximization (EM) algorithm (Dempster et al., 1977) traditionally used for this task, spectral algorithms are better in terms of their computational efficiency and provable guarantees. Current spectral algorithms are largely restricted to mixture of discrete or Gaussian distributions, e.g. (Anandkumar et al., 2012a; Hsu & Kakade, 2013). When the mixture components are distributions other than these standard distributions, the theoretical guarantees for these algorithms are no longer applicable, and their empirical performance can be very poor.\nIn this paper, we propose a kernel method for estimating the parameters of multi-view latent variable models where the mixture components can be nonparametric. The key idea is to embed the joint distribution of such a model into a reproducing kernel Hilbert space, and exploit the low rank structure of the embedded distribution (or covariance operators). The key computation involves a kernel singular value decomposition of the two-view covariance operator, followed by a robust tensor power method on the three-view covariance operator. These standard matrix operations makes the algorithm very efficient and easy to deploy.\nThe kernel algorithm proposed in this paper is more general than the previous spectral algorithms which work only for distributions with parametric assumptions (Anandkumar et al., 2012a; Hsu & Kakade, 2013).\n\u2217Email: lsong@cc.gatech.edu \u2020Email: a.anandkumar@uci.edu \u2021Email: bodai@gatech.edu \u00a7Email: zixu1986@gmail.com\nar X\niv :1\n31 1.\n32 87\nv2 [\ncs .L\nG ]\n8 D\nec 2\nWhen we use delta kernel, our algorithm reduces to the spectral algorithm for discrete mixture components analyzed in (Anandkumar et al., 2012a). When we use universal kernels, such as Gaussian RBF kernel, our algorithm can recover Gaussian mixture components as well as mixture components with other distributions. In this sense, our work also provides a unifying framework for previous spectral algorithms. We prove sample complexity bounds for the nonparametric tensor power method and establish that the sample complexity is quadratic in the number of latent components, and is a low order polynomial in the other relevant parameters such as the lower bound on mixing weights. Thus, we propose a computational and sample efficient nonparametric approach to learning latent variable models.\nKernel methods have been previously applied to learning latent variable models. However, none of the previous works explicitly recovers the actual parameters of the models Song et al. (2011); Song & Dai (2013); Sgouritsa et al. (2013). Most of them estimate an (unknown) invertible transformation of the latent parameters, and it is not clear how one can recover the actual parameters based on these estimates. Furthermore, these works focused on predictive task: recover the marginal distribution of the observed variables by making use of the low rank structure of the latent variable models. It is significantly more challenging to design kernel algorithms for actual parameter recovery and analyze theoretical properties of these algorithms.\nWe compare our kernel algorithm to the EM algorithm and previous spectral algorithms. We show that when the model assumptions are correct for the EM algorithm and previous spectral algorithms, our algorithm converges in terms of estimation error to these competitors. In the opposite cases when the model assumptions are incorrect, our algorithm is able to adapt to the nonparametric mixture components and beating alternatives by a very large margin."}, {"heading": "2 Notation", "text": "We denote by X a random variable with domain X , and refer to instantiations of X by the lower case character, x. We endow X with some \u03c3-algebra A and denote a distributions (with respect to A ) on X by P(X). We also deal with multiple random variables, X1, X2, . . . , X`, with joint distribution P(X1, X2, . . . , X`). For simplicity of notation, we assume that the domains of all Xt, t \u2208 [`] are the same, but the methodology applies to the cases where they have different domains. Furthermore, we denote by H a hidden variable with domain H and distribution P(H).\nA reproducing kernel Hilbert space (RKHS) F on X with a kernel k(x, x\u2032) is a Hilbert space of functions f(\u00b7) : X 7\u2192 R with inner product \u3008\u00b7, \u00b7\u3009F . Its element k(x, \u00b7) satisfies the reproducing property: \u3008f(\u00b7), k(x, \u00b7)\u3009F = f(x), and consequently, \u3008k(x, \u00b7), k(x\u2032, \u00b7)\u3009F = k(x, x\u2032), meaning that we can view the evaluation of a function f at any point x \u2208 X as an inner product. Alternatively, k(x, \u00b7) can be viewed as an implicit feature map \u03c6(x) where k(x, x\u2032) = \u3008\u03c6(x), \u03c6(x\u2032)\u3009F . Popular kernel functions on Rn include the Gaussian RBF kernel k(x, x\u2032) = exp(\u2212s \u2016x\u2212 x\u2032\u20162) and the Laplace kernel exp(\u2212s\u2016x\u2212x\u2032\u2016). Kernel functions have also been defined on graphs, time series, dynamical systems, images and other structured objects Scho\u0308lkopf et al. (2004). Thus the methodology presented below can be readily generalized to a diverse range of data types as long as kernel functions are defined."}, {"heading": "3 Kernel Embedding of Distributions", "text": "We begin by providing an overview of kernel embeddings of distributions, which are implicit mappings of distributions into potentially infinite dimensional RKHS. The kernel embedding approach represents a distribution by an element in the RKHS associated with a kernel function Smola et al. (2007); Sriperumbudur et al. (2008),\n\u00b5X := EX [\u03c6(X)] = \u222b X \u03c6(x)P(dx), (1)\nwhere the distribution is mapped to its expected feature map, i.e., to a point in a potentially infinitedimensional and implicit feature space. The kernel embedding \u00b5X has the property that the expectation of\nany RKHS function f can be evaluated as an inner product in F , EX [f(X)] = \u3008\u00b5X , f\u3009F , \u2200f \u2208 F . Kernel embeddings can be readily generalized to joint distributions of two or more variables using tensor product feature maps. For instance, we can embed a joint distribution of two variables X1 and X2 into a tensor product feature space F \u00d7 F by\nCX1X2 := EX1X2 [\u03c6(X1)\u2297 \u03c6(X2)] (2)\n= \u222b X\u00d7X \u03c6(x1)\u2297 \u03c6(x2)P(dx1 \u00d7 dx2), (3)\nwhere the reproducing kernel for the tensor product features satisfies \u3008\u03c6(x1)\u2297 \u03c6(x2), \u03c6(x\u20321)\u2297 \u03c6(x\u20322)\u3009F\u00d7F = k(x1, x \u2032 1) k(x2, x \u2032 2). By analogy, we can also define CX1X2X3 := EX1X2X3 [\u03c6(X1)\u2297 \u03c6(X2)\u2297 \u03c6(X3)].\nKernel embedding of distributions has rich representational power. The mapping is injective for characteristic kernels Sriperumbudur et al. (2008). That is, if two distributions, P(X) and Q(X), are different, they are mapped to two distinct points in the RKHS. For domain Rd, many commonly used kernels are characteristic, such as the Gaussian RBF kernel and Laplace kernel. This injective property of kernel embeddings has been exploited to design state-of-the-art two-sample tests Gretton et al. (2012) and independence tests Gretton et al. (2008)."}, {"heading": "3.1 Kernel Embedding as Multi-Linear Operator", "text": "The joint embeddings can also be viewed as an uncentered covariance operator CX1X2 : F 7\u2192 F by the standard equivalence between a tensor product feature and a linear map. That is, given two functions f1, f2 \u2208 F , their covariance can be computed by EX1X2 [f1(X1)f2(X2)] = \u3008f1, CX1X2f2\u3009F , or equivalently \u3008f1 \u2297 f2, CX1X2\u3009F\u00d7F , where in the former we view CXY as an operator while in the latter we view it as an element in tensor product feature space. By analogy, CX1X2X3 can be regarded as a multi-linear operator from F \u00d7 F \u00d7 F to R. It will be clear from the context whether we use CXY as an operator between two spaces or as an element from a tensor product feature space. For generic introduction to tensor and tensor notation, please see Kolda & Bader (2009).\nThe operator CX1X2X3 (with shorthand CX1:3) is linear in each argument (mode) when fixing other arguments. Furthermore, the application of the operator to a set of elements {f1, f2, f3 \u2208 F} can be defined using the inner product from the tensor product feature space, i.e.,\nCX1:3 \u00d71 f1 \u00d72 \u00d73f3 := \u3008CX1:3 , f1 \u2297 f2 \u2297 f3\u3009F3\n= EX1X2X3 \u220f i\u2208[3] \u3008\u03c6(Xi), fi\u3009F  , where \u00d7i means applying fi to the i-th argument of CX1:3 . Furthermore, we can define the Hilbert-Schmidt norm \u2016\u00b7\u2016 of CX1:3 as\n\u2016CX1:3\u2016 2 = \u221e\u2211 i1=1 \u221e\u2211 i2=1 \u221e\u2211 i3=1 (CX1:3 \u00d71 ui1 \u00d72 ui2 \u00d73 ui3) 2\nusing three collections of orthonormal bases {ui1} \u221e i1=1 , {ui2} \u221e i2=1 , and {ui3} \u221e i3=1\n. We can also define the inner product for the space of such operator that \u2016CX1:3\u2016 <\u221e\u2329\nCX1:3 , C\u0303X1:3 \u232a = \u221e\u2211 i1=1 \u221e\u2211 i2=1 \u221e\u2211 i`=1 (CX1:` \u00d71 ui1 \u00d72 ui2 \u00d73 ui3)\n\u00b7 (C\u0303X1:` \u00d71 ui1 \u00d72 . . .\u00d7` ui`). The joint embedding, CX1X2 , is a 2nd order tensor, and we can essentially use notations and operations for matrices. For instance, we can perform singular value decomposition\nCX1X2 = \u221e\u2211 i=1 \u03c3i \u00b7 ui1 \u2297 ui2 ,\nwhere \u03c3i \u2208 R are singular values ordered in nonincreasing manner, and {ui1} \u221e i1=1 \u2282 F , {ui2} \u221e i2=1\n\u2282 F are singular vectors and orthonormal bases. The rank of CX1X2 is the smallest k such that \u03c3i = 0 for i > k."}, {"heading": "3.2 Finite Sample Estimate", "text": "While we rarely have access to the true underlying distribution, P(X), we can readily estimate its embedding using a finite sample average. Given a sample DX = { x1, . . . , xm } of size m drawn i.i.d. from P(X), the empirical kernel embedding is\n\u00b5\u0302X := 1\nm \u2211m i=1 \u03c6(xi). (4)\nThis empirical estimate converges to its population counterpart in RKHS norm, \u2016\u00b5\u0302X \u2212 \u00b5X\u2016F , with a rate of Op(m\n\u2212 12 ) Smola et al. (2007). The covariance operator can be estimated similarly using finite sample average. Given m pairs of training\nexamples DXY = { (xi1, x i 2) } i\u2208[m] drawn i.i.d. from P(X1, X2),\nC\u0302X1X2 = 1\nm m\u2211 i=1 \u03c6(xi1)\u2297 \u03c6(xi2). (5)\nSimilarly, given sample from distribution P(X1, X2, X3), one can estimate C\u0302X1:3 = 1m \u2211m i=1 \u03c6(x i 1)\u2297 \u03c6(xi2)\u2297 \u03c6(xi3). By virtue of the kernel trick, most of the computation required for subsequent statistical inference using kernel embeddings can be reduced to the Gram matrix manipulation. The entries in the Gram matrix K correspond to the kernel value between data points xi and xj , i.e., Kij = k(x\ni, xj), and therefore its size is determined by the number of data points in the sample. The size of the Gram matrix is in general much smaller than the dimension of the feature spaces (which can be infinite). This enables efficient nonparametric methods using the kernel embedding representation. If the sample size is large, the computation in kernel embedding methods may be expensive. In this case, a popular solution is to use a low-rank approximation of the Gram matrix, such as incomplete Cholesky factorization Fine & Scheinberg (2001), which is known to work very effectively in reducing computational cost of kernel methods, while maintaining the approximation accuracy."}, {"heading": "4 Multi-View Latent Variable Models", "text": "Multi-view latent variable models studied in this paper are a special class of Bayesian networks in which \u2022 observed variables X1, X2, . . . , X` are conditionally independent given a discrete latent variable H,\nand \u2022 the conditional distributions, P(Xt|H), of the Xt, t \u2208 [`] given the hidden variable H can be different. The conditional independent structure of a multi-view latent variable model is illustrated in Figure 1(a), and many complicated graphical models, such as the hidden Markov model in Figure 1(b), can be reduced to a multi-view latent variable model. For simplicity of exposition, we will explain our method using the model with symmetric view. That is the conditional distribution are the same for each view, i.e., P(X|h) = P(X1|h) = P(X2|h) = P(X3|h). In Appendix 5.3, we will show that multi-view models with different views can be reduced to ones with symmetric view."}, {"heading": "4.1 Conditional Embedding Operator", "text": "For simplicity of exposition, we focus on a simple model with three observed variables, i.e., ` = 3. Suppose H \u2208 [k], then we can embed each conditional distribution P(X|h) corresponding to a particular value of H = h into the RKHS as\n\u00b5X|h = \u222b X \u03c6(x)P(dx|h). (6)\nIf we vary the value of H, we obtain the kernel embedding for different P(X|h). Conceptually, we can collect these embeddings into a matrix (with potentially infinite number of rows)\nCX|H = ( \u00b5X|h=1, \u00b5X|h=2, . . . , \u00b5X|h=k ) , (7)\nwhich is called the conditional embedding operator. If we use the standard basis eh in Rk to represent each value of h, we can retrieve each \u00b5X|h from CX|H by \u00b5X|h = CX|Heh (8) Once we have the conditional embedding \u00b5X|h, we can estimate the density p(x|h) by performing an inner product p(x|h) = \u2329 \u03c6(x), \u00b5X|h \u232a ."}, {"heading": "4.2 Factorized Kernel Embedding", "text": "Then the distributions, P(X1, X2) and P(X1, X2, X3), can be factorized respectively P(dx1, dx2) = \u222b H P(dx1|h)P(dx2|h)P(dh), and\nP(dx1, dx2, dx3) = \u222b H P(dx1|h)P(dx2|h)P(dx3|h)P(dh).\nSince we assume the hidden variable H \u2208 [k] is discrete, we let \u03c0h := P(h). Furthermore, if we apply Kronecker delta kernel \u03b4(h, h\u2032) with feature map eh, then the embeddings for P(H)\nCHH = EH [eH \u2297 eH ] =  \u03c01 . . . 0... . . . ... 0 . . . \u03c0k  , and CHHH = EH [eH \u2297 eH \u2297 eH ]\n=  \u03c0h \u03b4(h, h\u2032) \u03b4(h\u2032, h\u2032\u2032)  h,h\u2032,h\u2032\u2032\u2208[k]\nare diagonal tensors. Making use of CHH and CHHH , and the factorization of the distributions P(X1, X2) and P(X1, X2, X3), we obtain the factorization of the embedding of P(X1, X2) (second order embedding)\nCX1X2\n= \u222b H (\u222b X \u03c6(x1)P(dx1|h) ) \u2297 (\u222b X \u03c6(x2)P(dx2|h) ) P(dh)\n= \u222b H ( CX|Heh ) \u2297 ( CX|Heh ) P(dh)\n= CX|H (\u222b H eh \u2297 eh P(dh) ) C>X|H\n= CX|H CHH C>X|H , (9) and that of P(X1, X2, X3) (third order embedding)\nCX1X2X3 = CHHH \u00d71 CX|H \u00d72 CX|H \u00d73 CX|H . (10)"}, {"heading": "4.3 Identifiability of Parameters", "text": "We note that CX|H = ( \u00b5X|h=1, \u00b5X|h=2, . . . , \u00b5X|h=k ) , and the kernel embeddings for CX1X2 and CX1X2X3 can be alternatively written as\nCX1X2 = \u2211 h\u2208[k] \u03c0h \u00b7 \u00b5X|h \u2297 \u00b5X|h, (11)\nCX1X2X3 = \u2211 h\u2208[k] \u03c0h \u00b7 \u00b5X|h \u2297 \u00b5X|h \u2297 \u00b5X|h (12)\nAllman et al. Allman et al. (2009) showed that, under mild conditions, a finite mixture of nonparametric product distributions is identifiable. The multi-view latent variable model in (12) has the same form as a finite mixture of nonparametric product distribution, and therefore we can adapt Allman\u2019s results to the current setting. Proposition 1 (Identifiability) Let P(X1, X2, X3) be a multi-view latent variable model, such that the conditional distributions {P(X|h)}h\u2208[k] are linearly independent. Then, the set of parameters { \u03c0h, \u00b5X|h } h\u2208[k] are identifiable from CX1X2X3 , up to label swapping of the hidden variable H. Example 1. The probability vector of a discrete variable X \u2208 [n], and the joint probability table of two discrete variables X1 \u2208 [n] and X2 \u2208 [n], are both kernel embeddings. To see this, let the kernel be the Kronecker delta kernel k(x, x\u2032) = \u03b4(x, x\u2032) whose feature map \u03c6(x) is the standard basis of ex in Rn. The x-th dimension of ex is 1 and 0 otherwise. Then\n\u00b5X = ( P(x = 1) . . . P(x = n) )> ,\nCX1X2 =  P(x1 = s, x2 = t)  s,t\u2208[n] .\nWe require that the conditional probability table {P (X|h)}h\u2208[k] to have full column rank for identifiability in this case.\nExample 2. Suppose we have a k-component mixture of one dimensional spherical Gaussian distributions. The Gaussian components have identical covariance \u03c32, but their mean values are distinct. Note that this model is not identifiable under the framework of Hsu & Kakade (2013) since the mean values are just scalars and therefore, rank deficient. However, if we embed the density functions using universal kernels such as Gaussian RBF kernel, it can be shown that the mixture model becomes identifiable. This is because we are working with the entire density function which are linearly independent from each other. Thus, under a non-parametric framework, we can incorporate overcomplete mixtures, where the number of components can exceed the observed dimensionality.\nFinally, we remark that the identifiability result in Proposition 1 can be extended to cases where the conditional distributions do not satisfy linear independence, e.g. Kruskal (1977); De Lathauwer et al. (2007); Anandkumar et al. (2013b). However, in general, it is not tractable to learn such models and we do not consider them here."}, {"heading": "5 Kernel Algorithm", "text": "We first design a kernel algorithm to recover the parameters, { \u03c0h, \u00b5X|h } h\u2208[k], of the multi-view latent variable model based on CX1X2 and CX1X2X3 . This can be easily extended to the sample versions and this is discussed in Section 5.2. For clarity of the presentation, we first present the symmetric view case, and then, extend to more general version."}, {"heading": "5.1 Population Case", "text": "We first derive the algorithm for the population case as if we could access the true operator CX1X2 and CX1X2X3 . Its finite sample counterpart will be presented in the next section. The algorithm can be thought\nof as a kernel generalization of the algorithm in Anandkumar et al. (2013a) using embedding representations. Step 1. We perform eigen-decomposition of CX1X2 ,\nCX1X2 = \u221e\u2211 i=1 \u03c3i \u00b7 ui \u2297 ui\nwhere the eigen-values are ordered in non-decreasing manner. According to the factorization in Eq. (9), CX1X2 has rank k. Let the leading eigenvectors corresponding to the largest k eigen-value be Uk := (u1, u2, . . . , uk), and the eigen-value matrix be Sk := diag(\u03c31, \u03c32, . . . , \u03c3k). We define the whitening operator W := UkS\u22121/2k which satisfies\nW>CX1X2W = (W>CX|HC 1/2 HH)(C 1/2 HHC > X|HW) = I,\nand M :=W>CX|HC 1/2 HH is an orthogonal matrix.\nStep 2. We apply the whiten operator to the 3rd order kernel embedding CX1X2X3 T := CX1X2X3 \u00d71 (W>)\u00d72 (W>)\u00d73 (W>).\nAccording to the factorization in Eq. (10),\nT = C\u22121/2HHH \u00d71 M \u00d72 M \u00d73 M, which is a tensor with orthogonal factors. Essentially, each column vi of M is an eigenvector of the tensor T .\nStep 3. We use tensor power method to find eigenvectors M for T Anandkumar et al. (2013a). We provide the method in the Appendix in Algorithm 2 for completeness.\nStep 4. We recover the conditional embedding operator by undoing the whitening step\nCX|H = (\u00b5X|h=1, \u00b5X|h=1, . . . , \u00b5X|h=k) = (W)\u2020M."}, {"heading": "5.2 Finite Sample Case", "text": "Given m observation DX1X2X3 = {(xi1, xi2, xi3)}i\u2208[m] drawn i.i.d. from a multi-view latent variable model P(X1, X2, X3), we now design a kernel algorithm to estimate the latent parameters from data. Although the empirical kernel embeddings can be infinite dimensional, we can carry out the decomposition using just the kernel matrices. We denote the implicit feature matrix by\n\u03a6 := (\u03c6(x11), . . . , \u03c6(x m 1 ), \u03c6(x 1 2), . . . , \u03c6(x m 2 )), \u03a8 := (\u03c6(x12), . . . , \u03c6(x m 2 ), \u03c6(x 1 1), . . . , \u03c6(x m 1 )),\nand the corresponding kernel matrix by K = \u03a6>\u03a6 and L = \u03a8>\u03a8 respectively. Then the steps in the population case can be mapped one-by-one into kernel operations.\nStep 1. We perform a kernel eigenvalue decomposition of the empirical 2nd order embedding\nC\u0302X1X2 := 1\n2m m\u2211 i=1 ( \u03c6(xi1)\u2297 \u03c6(xi2) + \u03c6(xi2)\u2297 \u03c6(xi1) ) ,\nwhich can be expressed succinctly as C\u0302X1X2 = 12m\u03a6\u03a8 >. Its leading k eigenvectors U\u0302k = (u\u03021, . . . , u\u0302k) lie in the span of the column of \u03a6, i.e., U\u0302k = \u03a6(\u03b21, . . . , \u03b2k) with \u03b2 \u2208 R2m. Then we can transform the eigen-value decomposition problem for an infinite dimensional matrix to a problem involving finite dimensional kernel matrices,\nC\u0302X1X2 C\u0302>X1X2 u = \u03c3\u0302 2 u \u21d2 1 4m2 \u03a6\u03a8>\u03a8\u03a6>\u03a6\u03b2 = \u03c3\u03022 \u03a6\u03b2\n\u21d2 1 4m2 KLK\u03b2 = \u03c3\u03022K\u03b2.\nAlgorithm 1 KernelSVD(K, L, k)\nOut: S\u0302k and (\u03b21, . . . , \u03b2k)\n1: Cholesky decomposition: K = R>R 2: Eigen-decomposition: 14m2RLR >\u03b2\u0303 = \u03c3\u03022 \u03b2\u0303 3: Use k leading eigenvalues: S\u0302k = diag(\u03c3\u03021, . . . , \u03c3\u0302k) 4: Use k leading eigenvectors: (\u03b2\u03031, . . . , \u03b2\u0303k) to compute: (\u03b21, . . . , \u03b2k) = R \u2020(\u03b2\u03031, . . . , \u03b2\u0303k)\nLet the Cholesky decomposition of K be R>R. Then by redefining \u03b2\u0303 = R\u03b2, and solving an eigenvalue problem\n1\n4m2 RLR>\u03b2\u0303 = \u03c3\u03022 \u03b2\u0303, and obtain \u03b2 = R\u2020\u03b2\u0303. (13)\nThe resulting eigenvectors satisfy u>i ui\u2032 = \u03b2 > i \u03a6 >\u03a6\u03b2i\u2032 = \u03b2 > i K\u03b2i\u2032 = \u03b2\u0303 > i \u03b2\u0303i\u2032 = \u03b4ii\u2032 . This step is summarized in Algorithm 1. Step 2. We whiten the empirical 3rd order embedding\nC\u0302X1X2X3 := 1\n3m m\u2211 i=1 (\u03c6(xi1)\u2297 \u03c6(xi2)\u2297 \u03c6(xi3)\n+ \u03c6(xi3)\u2297 \u03c6(xi1)\u2297 \u03c6(xi2) + \u03c6(xi2)\u2297 \u03c6(xi3)\u2297 \u03c6(xi1))\nusing W\u0302 := U\u0302kS\u0302\u22121/2k , and obtain\nT\u0302 := 1 3m m\u2211 i=1 (\u03be(xi1)\u2297 \u03be(xi2)\u2297 \u03be(xi3)\n+ \u03be(xi3)\u2297 \u03be(xi1)\u2297 \u03be(xi2) + \u03be(xi2)\u2297 \u03be(xi3)\u2297 \u03be(xi1)), where\n\u03be(xi1) := S\u0302 \u22121/2 k (\u03b21, . . . , \u03b2k) >\u03a6>\u03c6(xi1) \u2208 Rk. Step 3. We run tensor power method Anandkumar et al. (2013a) on the finite dimension tensor T\u0302 to\nobtain its leading k eigenvectors M\u0302 := (v\u03021, . . . , v\u0302k). Step 4. The estimates of the conditional embeddings are\nC\u0302X|H = (\u00b5\u0302X|h=1, . . . , \u00b5\u0302X|h=k) = \u03a6(\u03b21, . . . , \u03b2k)S\u0302 1/2 k M\u0302."}, {"heading": "5.3 Symmetrization", "text": "In this section, we will extend the algorithm to the general case where the conditional distributions for each view are different. Without loss of generality, we will consider recover the operator \u00b5X3|h for conditional distribution P(X3|h). The same strategy applies to other views. The idea is to reduce the multi-view case to the identical-view case based on a method by Anandkumar et al. (2012b).\nGiven the observations DX1X2X3 = {(xi1, xi2, xi3)}i\u2208[m] drawn i.i.d. from a multi-view latent variable model P(X1, X2, X3), let the kernel matrix associated with X1, X2 and X3 be K, L and G respectively and the corresponding feature map be \u03c6, \u03c8 and \u03c5 respectively. Furthermore, let the corresponding feature matrix be \u03a6\u0303 = (\u03c6(x11), . . . , \u03c6(x m 1 )), \u03a8\u0303 = (\u03c6(x 1 2), . . . , \u03c6(x m 2 )) and \u03a5\u0303 = (\u03c6(x 1 3), . . . , \u03c6(x m 3 )). Then, we have the empirical estimation of the second/third-order embedding as\nC\u0302X1X2 = 1\nm \u03a6\u0303\u03a8\u0303>, C\u0302X3X1 =\n1 m \u03a5\u0303\u03a6\u0303>, C\u0302X2X3 = 1 m \u03a8\u0303\u03a5\u0303>\nC\u0302X1X2X3 := 1\nm In \u00d71 \u03a6\u0303\u00d72 \u03a8\u0303\u00d73 \u03a5\u0303\nFind two arbitrary matrices A,B \u2208 Rk\u00d7\u221e, so that AC\u0302X1X2B> is invertible. Theoretically, we could randomly select k columns from \u03a6 and \u03a8 and set A = \u03a6>k ,B = \u03a8 > k . In practial, the first k leading\neigenvector directions of respect RKHS works better. Then, we have\nC\u0303X1X2 = 1\nm \u03a6\u0303>k \u03a6\u0303\u03a8\u0303\n>\u03a8\u0303k = 1\nm K>nkLnk\nC\u0303X3X1 = C\u0302X3X1\u03a6\u0303k = 1\nm \u03a5\u0303Knk\nC\u0303X3X2 = C\u0302X3X2\u03a8\u0303k = 1\nm \u03a5\u0303Lnk\nC\u0303X1X2X3 = C\u0302X1X2X3 \u00d71 \u03a6\u0303>k \u00d72 \u03a8\u0303>k = 1\nm In \u00d71 K>nk \u00d72 L>nk \u00d73 \u03a5\u0303\nBased on these matrices, we could reduce to a single view\nPair3 = C\u0303X3X1(C\u0303>X1X2) \u22121C\u0303X3X2\n= 1\nm \u03a5\u0303Knk(L\n> nkKnk) \u22121L>nk\u03a5\u0303 > =\n1 m \u03a5\u0303H\u03a5\u0303>\nwhere H = Knk(L>nkKnk)\u22121L>nk. Assume the leading k eigenvectors \u03bdk lie in the span of the column of \u03a5, i.e., \u03bdk = \u03a5\u03b2k where \u03b2k \u2208 Rm\u00d71\nPair3\u03bd = \u03bb\u03bd \u21d2 (Pair3)>Pair3\u03bd = \u03bb2\u03bd\n\u21d2 1 m2 \u03a5\u0303H>\u03a5\u0303>\u03a5\u0303H\u03a5\u0303>\u03bd = \u03bb2\u03bd \u21d2 1 m2 \u03a5\u0303H>GHG\u03b2 = \u03bb2\u03a5\u0303\u03b2 \u21d2 1 m2 GH>GHG\u03b2 = \u03bb2G\u03b2\nThen, we symmetrize and whiten the third-order embedding\nTriple3 = 1\nm C\u0303X1X2X3 \u00d71 [C\u0303X3X2 C\u0303\u22121X1X2 ]\u00d72 [C\u0303X3X1 C\u0303 \u22121 X2X1 ] (14)\nPlug C\u0303X3X2 C\u0303\u22121X1X2 = \u03a5\u0303Lnk(K > nkLnk) \u22121 and C\u0303X3X1 C\u0303\u22121X2X1 = \u03a5\u0303Knk(L > nkKnk) \u22121, we have\nTriple3 = 1\nm In \u00d71 \u03a5\u0303Lnk(K>nkLnk)\u22121K>nk \u00d72\u03a5\u0303Knk(L>nkKnk)\u22121L>nk \u00d73 \u03a5\nWe multiply each mode with \u03a5\u03b2S\u0302 \u2212 12 k to whitening the data and apply power method to decompose it\nT\u0302 = Triple3 \u00d71 S\u0302 \u2212 12 k \u03b2 >\u03a5\u0303> \u00d72 S\u0302 \u2212 12 k \u03b2 >\u03a5\u0303> \u00d73 S\u0302 \u2212 12 k \u03b2 >\u03a5\u0303>\n= 1\nm In \u00d71 S\u0302\n\u2212 12 k \u03b2 >GLnk(K>nkLnk)\u22121K>nk \u00d72\nS\u0302 \u2212 12 k \u03b2 >GKnk(L > nkKnk) \u22121L>nk \u00d73 S\u0302 \u2212 12 k \u03b2 >G\nApply the algorithm for symmetric case in previous section to T\u0302 , we could recover the conditional distribution operator."}, {"heading": "6 Sample Complexity", "text": "Let \u03c1 := supx\u2208X k(x, x), \u2016 \u00b7 \u2016 be the Hilbert-Schmidt norm, \u03c0min := mini\u2208[k] \u03c0i and \u03c3k(CX1X2) be the k-th largest singular value of CX1X2 .\nTheorem 2 (Sample Bounds) Pick any \u03b4 \u2208 (0, 1). When the number of samples m satisfies\nm > \u03b8\u03c12 log \u03b42 \u03c32k(CX1,X2) , \u03b8 := max\n( C3k 2\u03c1\n\u03c3k(CX1,X2) , C4k\n2/3\n\u03c0 1/3 min\n) ,\nfor some constants C3, C4 > 0, and the number of iterations N and the number of random initialization vectors L (drawn uniformly on the sphere Sk\u22121) satisfy\nN \u2265 C2 \u00b7 ( log(k) + log log ( 1\u221a\n\u03c0min T\n)) ,\nfor constant C2 > 0 and L = poly(k) log(1/\u03b4), the robust power method in Anandkumar et al. (2013a) yields eigen-pairs (\u03bb\u0302i, \u03c6\u0302i) such that there exists a permutation \u03b7, with probability 1\u2212 4\u03b4, we have\n\u2016\u03c0\u22121/2j \u00b5X|h=j \u2212 \u03c6\u0302\u03b7(j)\u2016 \u2264 8 T \u00b7 \u03c0 \u22121/2 j ,\n|\u03c0\u22121/2j \u2212 \u03bb\u0302\u03b7(j)| \u2264 5 T , \u2200j \u2208 [k], and \u2225\u2225\u2225\u2225T \u2212 k\u2211\nj=1\n\u03bb\u0302j \u03c6\u0302 \u22973 j \u2225\u2225\u2225\u2225 \u2264 55 T , where T is the tensor perturbation bound\nT := \u2016T\u0302 \u2212 T \u2016 \u2264 8\u03c11.5 \u221a log \u03b42\u221a\nm\u03c31.5k (CX1,X2) +\n512 \u221a 2\u03c13 ( log \u03b42 )1.5 m1.5 \u03c33k(CX1,X2) \u221a \u03c0min\nThus, the above result provides bounds on the estimated eigen-pairs using the robust tensor power method. The proof is in Appendix 9.\nRemarks: We note that the sample complexity is poly(k, \u03c1, 1/\u03c0min, 1/\u03c3k(CX1,X2)) of a low order, and in particular, it is O(k2), when the other parameters are fixed. For the special case of discrete measurements, where the kernel k(x, x\u2032) = \u03b4(x, x\u2032), we have \u03c1 = 1. Note that the sample complexity depends in this case only on the number of components k and not on the dimensionality of the observed state space. Thus, the robust tensor method has efficient sample and computational complexities for non-parametric latent variable estimation."}, {"heading": "7 Experiments", "text": "Methods. We compared our kernel nonparametric algorithm with three alternatives 1. The EM algorithm for mixture of Gaussians. The EM algorithm is not guaranteed to find the global\nsolution in each trial. Thus we randomly initialize it 10 times. 2. The spectral algorithm for mixture of spherical Gaussians (Hsu & Kakade, 2013). The assumption\nin Hsu & Kakade (2013) is very restrictive: the collection of spherical Gaussian centers need to span a k-dimension subpsace. 3. A discretization based spectral algorithm (Kasahara & Shimotsu, 2010). This algorithm approximates the joint distribution of the observed variables with histogram and then applies the spectral algorithm to recover the discretized conditional density. It is well-known that density estimation using histogram suffers from poor performance even for 3-dimension data. The error of this algorithm is typically 10 times larger than alternatives. To make the curves for other methods clearer, we did not plot the performance of Kasahara & Shimotsu (2010) algorithm in the figures. Our method has a hyper-parameter, kernel bandwidth, which we selected for each view separately using cross-validation."}, {"heading": "7.1 Synthetic Data", "text": ""}, {"heading": "7.1.1 General Case: Different Conditional Distributions", "text": "We generated three-dimensional synthetic data from various mixture models. The variables corresponding to the dimensions are independent given the latent component indicator. More specifically, we explored two\nsettings: 1. Gaussian conditional densities with different variances; 2. Mixture of Gaussian and shifted Gamma conditional densities. The shifted Gamma distribution has density\np(x\u2212 \u00b5) = (x\u2212 \u00b5) (d\u22121)e\u2212x/\u03b8\n\u03b8d\u0393(d) , x \u2265 \u00b5\nwhere we chose the shape parameter d \u2264 1 such that density is very skewed. Furthermore, we chose the mean and variance parameters of the Gaussian/Gamma density such that component pair-wise overlap is relatively small according to the Fisher ratio (\u00b51\u2212\u00b52) 2\n\u03c321+\u03c3 2 2\n.\nWe also varied the number of samples m from 50 to 10, 000, and experimented with k = 2, 3, 4 or 8 mixture components. The mixture proportion for the h-th component is set to be \u03c0h = 2h k(k+1) , \u2200h \u2208 [k] (unbalanced). It is worth noting that as k becomes larger, it is more difficult to recover parameters. This is because only a small number of data will be generated for the first several clusters. For every n, k in each setting, we randomly generated 10 sets of samples and reported the average results.\nError measure. We measured the performance of algorithms by the following weighted `2 norm difference\nMSE := k\u2211 h=1 \u03c0h \u221a\u221a\u221a\u221a m\u2032\u2211 j=1 (p(xj |h)\u2212 p\u0302(xj |h))2,\nwhere {xj}j\u2208[m] is a set of uniformly-spaced test points. Results. The results are plotted in Figure 2. It is clear that the kernel spectral method converges rapidly with the data increment in all experiment settings. In the mixture of Gaussians setting, the EM algorithm is best since the model is correctly specified. The spectral learning algorithm for spherical Gaussians does not perform well since the assumption of the method\nis too restricted. The performance of our kernel method converges to that of the EM algorithm. In the mixture of Gaussian and Gamma setting, our kernel spectral algorithm achieves superior results compared to other algorithms. These results demonstrate that our algorithm is able to automatically adapt to the shape of the density.\nWe also plotted the actual recovered conditional densities in Figure 3. The kernel spectral algorithm recovers nicely both the Gaussian and Gamma components, while the EM algorithm fails to fit one component.\nWe also note that the performance of EM degrades as the number of components increases, and our method outperforms EM in higher dimensions. This is the key advantage of our method in that it has favorable performance in higher dimensions, which agrees with the theoretical result in Theorem 2 that the sample complexity depends only quadratically in the number of components, when other parameters are held fixed."}, {"heading": "7.1.2 Symmetric Case: Same Conditional Distribution", "text": "We also did some experiments for three-dimensional synthetic data that each view has the same conditional distribution. We generated the data from two settings:\n1. Mixture of Gaussian conditional density;\n2. Mixture of Gaussian and shifted Gamma conditional density.\nThe mixture proportion and other experiment settings are exact same as the experiment in the main text. The only difference is that the conditional densities for each view here are the identical. We use the same measure to evaluate the performance. The empirical results are plotted in Figure 4.\nAs we expected, the behavior of the proposed method is similar to the results in different conditional densities case. In mixture of Gaussians, our algorithm converges to the EM GMM resuls. And in the mixture of Gaussian/shift Gamma, our algorithm consistently better to other alternatives."}, {"heading": "7.2 Flow Cytometry Data", "text": "Flow cytometry (FCM) data are multivariate measurements from flow cytometers that record light scatter and fluorescence emission properties of hundreds of thousands of individual cells. They are important to the studying of the cell structures of normal and abnormal cells and the diagnosis of human diseases.\nAghaeepour et al. (2013) introduced the FlowCAP-challenge whose main task is grouping the flow cytometry data automatically. Clustering on the FCM data is a difficult task because the distribution of the data is non-Gaussian and heavily skewed.\nWe used the DLBCL Lymphoma dataset collection from Aghaeepour et al. (2013) to compare our kernel algorithm with multi-view mixture of Gaussian model. This collection contains 30 datasets, and each dataset consists of tens of thousands of cell measurements in 5 dimensions. Each dataset is a separate clustering task, and we fit a multi-view model to each dataset separately and use the maximum-a-posteriori assignment for obtaining the cluster labels. All the cell measurements have been manually labeled, therefore we can evaluate the clustering performance using the f-score.\nWe split the 5 dimensional into three views: dimension 1 and 2 as the first view, 3 and 4 the second and 5 the third view. For each dataset, we select the best kernel bandwidth by 5-fold cross validation using log-likelihood. For EM algorithm for mixture of Gaussians (GMM) with diagonal covariances, we use a very generous 20 restarts. Figure 5 presents the results sorted by the number of clusters. Our method (kernel spectral) outperforms EM-GMM in a majority of datasets. However, there are also datasets where kernel spectral algorithm has a large gap in performance compared to GMM. These are the datasets where the multi-view assumptions are heavily violated. Obtaining improved performance in these datasets will be a subject of our future study where we plan to develop even more robust kernel spectral algorithms."}, {"heading": "Acknowledgements", "text": "L. Song is supported in part by NSF Award IIS-1218749 and NIH 1RO1GM108341-01. A. Anandkumar is supported in part by Microsoft Faculty Fellowship, NSF Career award CCF-1254106, NSF Award CCF1219234, and ARO YIP Award W911NF-13-1-0084."}, {"heading": "8 Robust Tensor Power Method", "text": "We recap the robust tensor power method for finding the tensor eigen-pairs in Algorithm 2, analyzed in detail in Anandkumar et al. (2013a) and Anandkumar et al. (2012a). The method computes the eigenvectors of a tensor through deflation, using a set of initialization vectors. Here, we employ random initialization vectors. This can be replaced with better initialization vectors, in certain settings, e.g. in the community model, the neighborhood vectors provide better initialization and lead to stronger guarantees Anandkumar et al. (2013a). Given the initialization vector, the method then runs a tensor power update, and runs for N iterations to obtain an eigenvector. The successive eigenvectors are obtained via deflation.\nAlgorithm 2 {\u03bb,\u03a6} \u2190TensorEigen(T, {vi}i\u2208[L], N) Input: Tensor T \u2208 Rk\u00d7k\u00d7k, set of L initialization vectors {vi}i\u2208L, number of iterations N . Output: the estimated eigenvalue/eigenvector pairs {\u03bb,\u03a6}, where \u03bb is the vector of eigenvalues and \u03a6 is\nthe matrix of eigenvectors. for i = 1 to k do\nfor \u03c4 = 1 to L do \u03b80 \u2190 v\u03c4 . for t = 1 to N do T\u0303 \u2190 T . for j = 1 to i\u2212 1 (when i > 1) do\nif |\u03bbj \u2329 \u03b8 (\u03c4) t , \u03c6j , | \u232a > \u03be then\nT\u0303 \u2190 T\u0303 \u2212 \u03bbj\u03c6\u22973j . end if\nend for Compute power iteration update \u03b8 (\u03c4) t := T\u0303 (I,\u03b8 (\u03c4) t\u22121,\u03b8 (\u03c4) t\u22121)\n\u2016T\u0303 (I,\u03b8(\u03c4)t\u22121,\u03b8 (\u03c4) t\u22121)\u2016\nend for end for Let \u03c4\u2217 := arg max\u03c4\u2208L{T\u0303 (\u03b8(\u03c4)N , \u03b8 (\u03c4) N , \u03b8 (\u03c4) N )}. Do N power iteration updates starting from \u03b8 (\u03c4\u2217) N to obtain eigenvector estimate \u03c6i, and set \u03bbi :=\nT\u0303 (\u03c6i, \u03c6i, \u03c6i). end for return the estimated eigenvalue/eigenvectors (\u03bb,\u03a6)."}, {"heading": "9 Proof of Theorem 2", "text": ""}, {"heading": "9.1 Recap of Perturbation Bounds for the Tensor Power Method", "text": "We now recap the result of Anandkumar et al. (2013a, Thm. 13) that establishes bounds on the eigenestimates under good initialization vectors for the above procedure. Let T = \u2211 i\u2208[k] \u03bbivi, where vi are orthonormal vectors and \u03bb1 \u2265 \u03bb2 \u2265 . . . \u03bbk. Let T\u0302 = T + E be the perturbed tensor with \u2016E\u2016 \u2264 T . Recall that N denotes the number of iterations of the tensor power method. We call an initialization vector u to\nbe (\u03b3,R0)-good if there exists vi such that \u3008u, vi\u3009 > R0 and | \u3008u, vi\u3009 | \u2212maxj<i | \u3008u, vj\u3009 | > \u03b3| \u3008u, vi\u3009 |. Choose \u03b3 = 1/100.\nTheorem 3 There exists universal constants C1, C2 > 0 such that the following holds. T \u2264 C1 \u00b7 \u03bbminR20, N \u2265 C2 \u00b7 ( log(k) + log log ( \u03bbmax T )) , (15) Assume there is at least one good initialization vector corresponding to each vi, i \u2208 [k]. The parameter \u03be for choosing deflation vectors in each iteration of the tensor power method in Procedure 2 is chosen as \u03be \u2265 25 T . We obtain eigenvalue-eigenvector pairs (\u03bb\u03021, v\u03021), (\u03bb\u03022, v\u03022), . . . , (\u03bb\u0302k, v\u0302k) such that there exists a permutation \u03b7 on [k] with \u2016v\u03b7(j) \u2212 v\u0302j\u2016 \u2264 8 T /\u03bb\u03b7(j), |\u03bb\u03b7(j) \u2212 \u03bb\u0302j | \u2264 5 T , \u2200j \u2208 [k], and \u2225\u2225\u2225\u2225\u2225\u2225T \u2212 k\u2211 j=1 \u03bb\u0302j v\u0302 \u22973 j\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2264 55 T . In the sequel, we establish concentration bounds that allows us to translate the above condition on tensor\nperturbation (15) to sample complexity bounds."}, {"heading": "9.2 Concentration Bounds", "text": ""}, {"heading": "9.2.1 Analysis of Whitening", "text": "Recall that we use the covariance operator CX1X2 for whitening the 3rd order embedding CX1,X2,X3 . We first analyze the perturbation in whitening when sample estimates are employed.\nLet C\u0302X1X2 denote the sample covariance operator between variables X1 and X2, and let\nB := 0.5(C\u0302X1X2 + C\u0302>X1X2) = U\u0302 S\u0302U\u0302 >\ndenote the SVD. Let U\u0302k and S\u0302k denote the restriction to top-k eigen-pairs, and let Bk := U\u0302kS\u0302kU\u0302>k . Recall that the whitening matrix is given by W\u0302 := U\u0302kS\u0302\u22121/2k . Now W\u0302 whitens Bk, i.e. W\u0302>BkW\u0302 = I.\nNow consider the SVD of\nW\u0302>CX1X2W\u0302 = ADA>, and define W := W\u0302AD\u22121/2A>, and W whitens CX1X2 since W>CX1X2W = I. Recall that by exchangeability assumption,\nCX1,X2 = k\u2211 j=1 \u03c0j \u00b7 \u00b5X|j \u2297 \u00b5X|j = M Diag(\u03c0)M>, (16)\nwhere the jth column of M , Mj = \u00b5X|j . We now establish the following perturbation bound on the whitening procedure. Recall from (26), pairs := \u2225\u2225\u2225CX1,X2 \u2212 C\u0302X1,X2\u2225\u2225\u2225. Let \u03c31(\u00b7) \u2265 \u03c32(\u00b7) . . . denote the singular values of an operator.\nLemma 4 (Whitening perturbation) Assuming that pairs < 0.5\u03c3k(CX1X2),\nW := \u2016Diag(\u03c0)1/2M>(W\u0302 \u2212W)\u2016 \u2264 4 pairs\n\u03c3k(CX1X2) (17)\nRemark: Note that \u03c3k(CX1X2) = \u03c32k(M).\nProof: The proof is along the lines of Lemma 16 of Anandkumar et al. (2013a), but adapted to whitening using the covariance operator here.\n\u2016Diag(\u03c0)1/2M>(W\u0302 \u2212W)\u2016 = \u2016Diag(\u03c0)1/2M>W (AD1/2A> \u2212 I)\u2016 \u2264 \u2016Diag(\u03c0)1/2M>W\u2016\u2016D1/2 \u2212 I\u2016.\nSinceW whitens CX1X2 = M Diag(\u03c0)M>, we have that \u2016Diag(\u03c0)1/2M>W\u2016 = 1. Now we control \u2016D1/2\u2212I\u2016. Let E\u0303 := CX1,X2 \u2212Bk, where recall that B = 0.5(C\u0302X1,X2 + C\u0302>X1X2) and Bk is its restriction to top-k singular values. Thus, we have \u2016E\u0303\u2016 \u2264 pairs + \u03c3k+1(B) \u2264 2 pairs. We now have\n\u2016D1/2 \u2212 I\u2016 \u2264 \u2016(D1/2 \u2212 I)(D1/2 + I)\u2016 \u2264 \u2016D \u2212 I\u2016\n= \u2016ADA> \u2212 I\u2016 = \u2016W\u0302>CX1X2W\u0302 \u2212 I\u2016\n= \u2016W\u0302>E\u0303W\u0302\u2016 \u2264 \u2016W\u0302\u20162(2 pairs). Now\n\u2016W\u03022\u2016 \u2264 1 \u03c3k(C\u0302X1X2) \u2264 2 \u03c3k(CX1X2) ,\nwhen pairs < 0.5\u03c3k(CX1X2). 2"}, {"heading": "9.2.2 Tensor Concentration Bounds", "text": "Recall that the whitened tensor from samples is given by T\u0302 := C\u0302X1X2X3 \u00d71 (W\u0302>)\u00d72 (W\u0302>)\u00d73 (W\u0302>). We want to establish its perturbation from the whitened tensor using exact statistics T := CX1X2X3 \u00d71 (W>)\u00d72 (W>)\u00d73 (W>). Further, we have\nCX1X2X3 = \u2211 h\u2208[k] \u03c0h \u00b7 \u00b5X|h \u2297 \u00b5X|h \u2297 \u00b5X|h (18)\nLet triples := \u2016C\u0302X1X2X3 \u2212 CX1X2X3\u2016. Let \u03c0min := minh\u2208[k] \u03c0h.\nLemma 5 (Tensor perturbation bound) Assuming that pairs < 0.5\u03c3k(CX1X2), we have\nT := \u2016T\u0302 \u2212 T \u2016 \u2264 2 \u221a\n2 triples \u03c3k(CX1X2)1.5 + 3W\u221a \u03c0min . (19)\nProof: Define intermediate tensor\nT\u0303 := CX1X2X3 \u00d71 (W\u0302>)\u00d72 (W\u0302>)\u00d73 (W\u0302>).\nWe will bound \u2016T\u0302 \u2212 T\u0303 \u2016 and \u2016T\u0302 \u2212 T \u2016 separately.\n\u2016T\u0302 \u2212 T\u0303 \u2016 \u2264 \u2016C\u0302X1,X2,X2 \u2212 CX1,X2,X3\u2016\u2016W\u0302\u20163 \u2264 2 \u221a\n2 triples \u03c3k(CX1X2)1.5 ,\nusing the bound on \u2016W\u0302\u2016 in Lemma 4. For the other term, first note that CX1,X2,X3 = \u2211 h\u2208[k] \u03c0h \u00b7Mh \u2297Mh \u2297Mh,\n\u2016T\u0302 \u2212 T \u2016 = \u2016CX1X2X3 \u00d71 (W\u0302 \u2212W)> \u00d72 (W\u0302 \u2212W)> \u00d73 (W\u0302 \u2212W)>\u2016\n\u2264 \u2016Diag(\u03c0) 1/2M>(W\u0302 \u2212W)\u20163 \u221a \u03c0min = 3W\u221a \u03c0min\n2 Proof of Theorem 2: We obtain a condition on the above perturbation T in (19) by applying Theorem 3 as T \u2264 C1\u03bbminR20. Here, we have \u03bbi = 1/ \u221a \u03c0i \u2265 1. For random initialization, we have that R0 \u223c 1/ \u221a k, with probability 1\u2212 \u03b4 using poly(k) poly(1/\u03b4) trials, see Thm. 5.1 in Anandkumar et al. (2012a). Thus, we require that T \u2264 C1k . Summarizing, we require for the following conditions to hold\npairs \u2264 0.5\u03c3k(CX1X2), T \u2264 C1 k . (20)\nWe now substitute for pairs and triples in (19) using Lemma 6 and Lemma 7. From Lemma 6, we have that\npairs 6 2 \u221a\n2\u03c1 \u221a\nlog \u03b42\u221a m ,\nwith probability 1\u2212 \u03b4. It is required that pairs < 0.5\u03c3k(CX1,X2), which yields that\nm > 32\u03c12 log \u03b42 \u03c32k(CX1,X2) . (21)\nFurther we require that T \u2264 C1/k, which implies that each of the terms in (19) is less than C/k, for some constant C. Thus, we have\n2 \u221a\n2 triples \u03c31.5k (CX1,X2) < C k \u21d2 m >\nC3k 2\u03c13 log \u03b42\n\u03c33k(CX1,X2) ,\nfor some constant C3 with probability 1\u2212 \u03b4 from Lemma 7. Similarly for the second term in (19), we have 3W\u221a \u03c0min < C k ,\nand from Lemma 4, this implies that\npairs \u2264 C \u2032\u03c0 1/6 min\u03c3k(CX1,X2)\nk1/3 ,\nThus, we have\nm > C4k\n2 3 \u03c12 log \u03b42\n\u03c0 1 3\nmin\u03c3 2 k(CX1,X2)\n,\nfor some other constant C4 with probability 1\u2212 \u03b4. Thus, we have the result in Theorem 2. 2"}, {"heading": "9.2.3 Concentration bounds for Empirical Operators", "text": "Concentration results for the singular value decomposition of empirical operators.\nLemma 6 (Concentration bounds for pairs) Let \u03c1 := supx\u2208\u2126 k(x, x), and \u2016 \u00b7 \u2016 be the Hilbert-Schmidt norm, we have for\npairs := \u2225\u2225\u2225CX1X2 \u2212 C\u0302X1X2\u2225\u2225\u2225 , (22)\nPr  pairs 6 2 \u221a 2\u03c1 \u221a log \u03b42\u221a m  > 1\u2212 \u03b4. (23) Proof We will use similar arguments as in Rosasco et al. (2010) which deals with symmetric operator. Let \u03bei be defined as\n\u03bei = \u03c6(x i 1)\u2297 \u03c6(xi2)\u2212 CX1,X2 . (24)\nIt is easy to see that E[\u03bei] = 0. Further, we have\nsup x1,x2 \u2016\u03c6(x1)\u2297 \u03c6(x2)\u20162 = sup x1,x2 k(x1, x1)k(x2, x2) 6 \u03c1 2, (25)\nwhich implies that \u2016CX1X2\u2016 6 \u03c1, and \u2016\u03bei\u2016 6 2\u03c1. The result then follows from the Hoeffding\u2019s inequality in Hilbert space.\nSimilarly, we have the concentration bound for 3rd order embedding.\nLemma 7 (Concentration bounds for triples) Let \u03c1 := supx\u2208\u2126 k(x, x), and \u2016 \u00b7\u2016 be the Hilbert-Schmidt norm, we have for\ntriples := \u2225\u2225\u2225CX1X2X3 \u2212 C\u0302X1X2X3\u2225\u2225\u2225 , (26)\nPr  triples 6 2 \u221a 2\u03c13/2 \u221a log \u03b42\u221a m  > 1\u2212 \u03b4. (27) Proof We will use similar arguments as in Rosasco et al. (2010) which deals with symmetric operator. Let \u03bei be defined as\n\u03bei = \u03c6(x i 1)\u2297 \u03c6(xi2)\u2297 \u03c6(xi3)\u2212 CX1X2X3 . (28)\nIt is easy to see that E[\u03bei] = 0. Further, we have\nsup x1,x2,x3 \u2016\u03c6(x1)\u2297 \u03c6(x2)\u2297 \u03c6(x3)\u20162 = sup x1,x2,x3 k(x1, x1)k(x2, x2)k(x3, x3) 6 \u03c1 3, (29)\nwhich implies that \u2016CX1X2X3\u2016 6 \u03c13/2, and \u2016\u03bei\u2016 6 2\u03c13/2. The result then follows from the Hoeffding\u2019s inequality in Hilbert space."}], "references": [{"title": "Critical assessment of automated flow cytometry data analysis techniques", "author": ["Aghaeepour", "Nima", "Finak", "Greg", "Consortium", "The FlowCAP", "The DREAM", "Hoos", "Holger", "Mosmann", "Tim R", "Brinkman", "Ryan", "Gottardo", "Raphael", "Scheuermann", "Richard H"], "venue": "Nature Methods,", "citeRegEx": "Aghaeepour et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Aghaeepour et al\\.", "year": 2013}, {"title": "Identifiability of parameters in latent structure models with many observed variables", "author": ["Allman", "Elizabeth", "Matias", "Catherine", "Rhodes", "John"], "venue": "The Annals of Statistics,", "citeRegEx": "Allman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Allman et al\\.", "year": 2009}, {"title": "Tensor Methods for Learning Latent Variable Models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "Available at arXiv:1210.7559,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A Tensor Spectral Approach to Learning Mixed Membership Community Models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade"], "venue": "ArXiv 1302.2684,", "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity", "author": ["A. Anandkumar", "D. Hsu", "M. Janzamin", "S.M. Kakade"], "venue": "ArXiv 1308.2853,", "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "A spectral algorithm for latent dirichlet allocation", "author": ["Anandkumar", "Animashree", "Foster", "Dean P", "Hsu", "Daniel", "Kakade", "Sham M", "Liu", "Yi-Kai"], "venue": "Available at arXiv:1204.6703,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Latent Dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Inference of haplotypes from PCR-amplified samples of diploid populations", "author": ["A. Clark"], "venue": "Molecular Biology and Evolution,", "citeRegEx": "Clark,? \\Q1990\\E", "shortCiteRegEx": "Clark", "year": 1990}, {"title": "Fourth-order cumulant-based blind identification of underdetermined mixtures", "author": ["L. De Lathauwer", "J. Castaing", "Cardoso", "J.-F"], "venue": "IEEE Tran. on Signal Processing,", "citeRegEx": "Lathauwer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lathauwer et al\\.", "year": 2007}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Efficient SVM training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fine and Scheinberg,? \\Q2001\\E", "shortCiteRegEx": "Fine and Scheinberg", "year": 2001}, {"title": "Spectral dimensionality reduction for hmms", "author": ["D.P. Foster", "J. Rodu", "L.H. Ungar"], "venue": "Arxiv preprint arXiv:1203.6130,", "citeRegEx": "Foster et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2012}, {"title": "A kernel statistical test of independence", "author": ["A. Gretton", "K. Fukumizu", "Teo", "C.-H", "L. Song", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gretton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2008}, {"title": "Latent space approaches to social network analysis", "author": ["Hoff", "Peter D", "Raftery", "Adrian E", "Handcock", "Mark S"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoff et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hoff et al\\.", "year": 2002}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["D. Hsu", "S. Kakade", "T. Zhang"], "venue": "In Proc. Annual Conf. Computational Learning Theory,", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "author": ["Hsu", "Daniel", "Kakade", "Sham M"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "citeRegEx": "Hsu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2013}, {"title": "Nonparametric identification of multivariate mixtures", "author": ["Kasahara", "Hiroyuki", "Shimotsu", "Katsumi"], "venue": "Journal of the Royal Statistical Society - Series B,", "citeRegEx": "Kasahara et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kasahara et al\\.", "year": 2010}, {"title": "Efficient orthogonal tensor decomposition, with an application to latent variable model learning", "author": ["Kir\u00e1ly", "Franz"], "venue": "Available at arXiv:1309.3233,", "citeRegEx": "Kir\u00e1ly and Franz.,? \\Q2013\\E", "shortCiteRegEx": "Kir\u00e1ly and Franz.", "year": 2013}, {"title": "Tensor decompositions and applications", "author": ["Kolda", "Tamara. G", "Bader", "Brett W"], "venue": "SIAM Review,", "citeRegEx": "Kolda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolda et al\\.", "year": 2009}, {"title": "Three-way arrays: Rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics", "author": ["J.B. Kruskal"], "venue": "Linear algebra and its applications,", "citeRegEx": "Kruskal,? \\Q1977\\E", "shortCiteRegEx": "Kruskal", "year": 1977}, {"title": "A spectral algorithm for latent tree graphical models", "author": ["A. Parikh", "L. Song", "E.P. Xing"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Parikh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2011}, {"title": "An introduction to hidden Markov models", "author": ["L.R. Rabiner", "B.H. Juang"], "venue": "IEEE ASSP Magazine,", "citeRegEx": "Rabiner and Juang,? \\Q1986\\E", "shortCiteRegEx": "Rabiner and Juang", "year": 1986}, {"title": "On learning with integral operators", "author": ["L. Rosasco", "M. Belkin", "E.D. Vito"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rosasco et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rosasco et al\\.", "year": 2010}, {"title": "Kernel Methods in Computational Biology", "author": ["B. Sch\u00f6lkopf", "K. Tsuda", "Vert", "J.-P"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2004}, {"title": "Identifying finite mixtures of nonparametric product distributions and causal inference of confounders", "author": ["Sgouritsa", "Eleni", "Janzing", "Dominik", "Peters", "Jonas", "Sch\u00f6lkopf", "Bernhard"], "venue": "In Conference on Uncertainty on Artificial Intelligence (UAI),", "citeRegEx": "Sgouritsa et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sgouritsa et al\\.", "year": 2013}, {"title": "A Hilbert space embedding for distributions", "author": ["A.J. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of the International Conference on Algorithmic Learning Theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Robust low rank kernel embedding of multivariate distributions", "author": ["L. Song", "B. Dai"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Song and Dai,? \\Q2013\\E", "shortCiteRegEx": "Song and Dai", "year": 2013}, {"title": "Kernel embeddings of latent tree graphical models", "author": ["L. Song", "A. Parikh", "E.P. Xing"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 7, "context": "Latent variable models have been used to address various machine learning problems, ranging from modeling temporal dynamics, to text document analysis and to social network analysis (Rabiner & Juang, 1986; Clark, 1990; Hoff et al., 2002; Blei et al., 2003).", "startOffset": 182, "endOffset": 256}, {"referenceID": 13, "context": "Latent variable models have been used to address various machine learning problems, ranging from modeling temporal dynamics, to text document analysis and to social network analysis (Rabiner & Juang, 1986; Clark, 1990; Hoff et al., 2002; Blei et al., 2003).", "startOffset": 182, "endOffset": 256}, {"referenceID": 6, "context": "Latent variable models have been used to address various machine learning problems, ranging from modeling temporal dynamics, to text document analysis and to social network analysis (Rabiner & Juang, 1986; Clark, 1990; Hoff et al., 2002; Blei et al., 2003).", "startOffset": 182, "endOffset": 256}, {"referenceID": 14, "context": "Recently, there is a surge of interest in designing spectral algorithms for estimating the parameters of latent variable models (Hsu et al., 2009; Parikh et al., 2011; Song et al., 2011; Foster et al., 2012; Anandkumar et al., 2012a,b; Kir\u00e1ly, 2013).", "startOffset": 128, "endOffset": 249}, {"referenceID": 20, "context": "Recently, there is a surge of interest in designing spectral algorithms for estimating the parameters of latent variable models (Hsu et al., 2009; Parikh et al., 2011; Song et al., 2011; Foster et al., 2012; Anandkumar et al., 2012a,b; Kir\u00e1ly, 2013).", "startOffset": 128, "endOffset": 249}, {"referenceID": 27, "context": "Recently, there is a surge of interest in designing spectral algorithms for estimating the parameters of latent variable models (Hsu et al., 2009; Parikh et al., 2011; Song et al., 2011; Foster et al., 2012; Anandkumar et al., 2012a,b; Kir\u00e1ly, 2013).", "startOffset": 128, "endOffset": 249}, {"referenceID": 11, "context": "Recently, there is a surge of interest in designing spectral algorithms for estimating the parameters of latent variable models (Hsu et al., 2009; Parikh et al., 2011; Song et al., 2011; Foster et al., 2012; Anandkumar et al., 2012a,b; Kir\u00e1ly, 2013).", "startOffset": 128, "endOffset": 249}, {"referenceID": 9, "context": "Compared to the Expectation-Maximization (EM) algorithm (Dempster et al., 1977) traditionally used for this task, spectral algorithms are better in terms of their computational efficiency and provable guarantees.", "startOffset": 56, "endOffset": 79}, {"referenceID": 2, "context": "When we use delta kernel, our algorithm reduces to the spectral algorithm for discrete mixture components analyzed in (Anandkumar et al., 2012a). When we use universal kernels, such as Gaussian RBF kernel, our algorithm can recover Gaussian mixture components as well as mixture components with other distributions. In this sense, our work also provides a unifying framework for previous spectral algorithms. We prove sample complexity bounds for the nonparametric tensor power method and establish that the sample complexity is quadratic in the number of latent components, and is a low order polynomial in the other relevant parameters such as the lower bound on mixing weights. Thus, we propose a computational and sample efficient nonparametric approach to learning latent variable models. Kernel methods have been previously applied to learning latent variable models. However, none of the previous works explicitly recovers the actual parameters of the models Song et al. (2011); Song & Dai (2013); Sgouritsa et al.", "startOffset": 119, "endOffset": 985}, {"referenceID": 2, "context": "When we use delta kernel, our algorithm reduces to the spectral algorithm for discrete mixture components analyzed in (Anandkumar et al., 2012a). When we use universal kernels, such as Gaussian RBF kernel, our algorithm can recover Gaussian mixture components as well as mixture components with other distributions. In this sense, our work also provides a unifying framework for previous spectral algorithms. We prove sample complexity bounds for the nonparametric tensor power method and establish that the sample complexity is quadratic in the number of latent components, and is a low order polynomial in the other relevant parameters such as the lower bound on mixing weights. Thus, we propose a computational and sample efficient nonparametric approach to learning latent variable models. Kernel methods have been previously applied to learning latent variable models. However, none of the previous works explicitly recovers the actual parameters of the models Song et al. (2011); Song & Dai (2013); Sgouritsa et al.", "startOffset": 119, "endOffset": 1004}, {"referenceID": 2, "context": "When we use delta kernel, our algorithm reduces to the spectral algorithm for discrete mixture components analyzed in (Anandkumar et al., 2012a). When we use universal kernels, such as Gaussian RBF kernel, our algorithm can recover Gaussian mixture components as well as mixture components with other distributions. In this sense, our work also provides a unifying framework for previous spectral algorithms. We prove sample complexity bounds for the nonparametric tensor power method and establish that the sample complexity is quadratic in the number of latent components, and is a low order polynomial in the other relevant parameters such as the lower bound on mixing weights. Thus, we propose a computational and sample efficient nonparametric approach to learning latent variable models. Kernel methods have been previously applied to learning latent variable models. However, none of the previous works explicitly recovers the actual parameters of the models Song et al. (2011); Song & Dai (2013); Sgouritsa et al. (2013). Most of them estimate an (unknown) invertible transformation of the latent parameters, and it is not clear how one can recover the actual parameters based on these estimates.", "startOffset": 119, "endOffset": 1029}, {"referenceID": 23, "context": "Kernel functions have also been defined on graphs, time series, dynamical systems, images and other structured objects Sch\u00f6lkopf et al. (2004). Thus the methodology presented below can be readily generalized to a diverse range of data types as long as kernel functions are defined.", "startOffset": 119, "endOffset": 143}, {"referenceID": 25, "context": "The kernel embedding approach represents a distribution by an element in the RKHS associated with a kernel function Smola et al. (2007); Sriperumbudur et al.", "startOffset": 116, "endOffset": 136}, {"referenceID": 25, "context": "The kernel embedding approach represents a distribution by an element in the RKHS associated with a kernel function Smola et al. (2007); Sriperumbudur et al. (2008),", "startOffset": 116, "endOffset": 165}, {"referenceID": 12, "context": "This injective property of kernel embeddings has been exploited to design state-of-the-art two-sample tests Gretton et al. (2012) and independence tests Gretton et al.", "startOffset": 108, "endOffset": 130}, {"referenceID": 12, "context": "This injective property of kernel embeddings has been exploited to design state-of-the-art two-sample tests Gretton et al. (2012) and independence tests Gretton et al. (2008).", "startOffset": 108, "endOffset": 175}, {"referenceID": 25, "context": "(4) This empirical estimate converges to its population counterpart in RKHS norm, \u2016\u03bc\u0302X \u2212 \u03bcX\u2016F , with a rate of Op(m \u2212 12 ) Smola et al. (2007). The covariance operator can be estimated similarly using finite sample average.", "startOffset": 123, "endOffset": 143}, {"referenceID": 14, "context": "Kruskal (1977); De Lathauwer et al.", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "Kruskal (1977); De Lathauwer et al. (2007); Anandkumar et al.", "startOffset": 19, "endOffset": 43}, {"referenceID": 2, "context": "(2007); Anandkumar et al. (2013b). However, in general, it is not tractable to learn such models and we do not consider them here.", "startOffset": 8, "endOffset": 34}, {"referenceID": 2, "context": "of as a kernel generalization of the algorithm in Anandkumar et al. (2013a) using embedding representations.", "startOffset": 50, "endOffset": 76}, {"referenceID": 2, "context": "We use tensor power method to find eigenvectors M for T Anandkumar et al. (2013a). We provide the method in the Appendix in Algorithm 2 for completeness.", "startOffset": 56, "endOffset": 82}, {"referenceID": 2, "context": "We run tensor power method Anandkumar et al. (2013a) on the finite dimension tensor T\u0302 to obtain its leading k eigenvectors M\u0302 := (v\u03021, .", "startOffset": 27, "endOffset": 53}, {"referenceID": 2, "context": "The idea is to reduce the multi-view case to the identical-view case based on a method by Anandkumar et al. (2012b). Given the observations DX1X2X3 = {(x1, x2, x3)}i\u2208[m] drawn i.", "startOffset": 90, "endOffset": 116}, {"referenceID": 2, "context": "for constant C2 > 0 and L = poly(k) log(1/\u03b4), the robust power method in Anandkumar et al. (2013a) yields eigen-pairs (\u03bb\u0302i, \u03c6\u0302i) such that there exists a permutation \u03b7, with probability 1\u2212 4\u03b4, we have \u2016\u03c0 j \u03bcX|h=j \u2212 \u03c6\u0302\u03b7(j)\u2016 \u2264 8 T \u00b7 \u03c0 \u22121/2 j , |\u03c0 j \u2212 \u03bb\u0302\u03b7(j)| \u2264 5 T , \u2200j \u2208 [k], and \u2225\u2225\u2225\u2225T \u2212 k \u2211", "startOffset": 73, "endOffset": 99}], "year": 2013, "abstractText": "Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixtures of discrete or Gaussian distributions. In this paper, we propose a kernel method for learning multi-view latent variable models, allowing each mixture component to be nonparametric. The key idea of the method is to embed the joint distribution of a multi-view latent variable into a reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. We establish that the sample complexity for the proposed method is quadratic in the number of latent components and is a low order polynomial in the other relevant parameters. Thus, our non-parametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. Moreover, the non-parametric tensor power method compares favorably to EM algorithm and other existing spectral algorithms in our experiments.", "creator": "LaTeX with hyperref package"}}}