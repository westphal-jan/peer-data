{"id": "1311.5022", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2013", "title": "Extended Formulations for Online Linear Bandit Optimization", "abstract": "on - under line algebraic linear optimization on combinatorial action sets ( d - dimensional actions ) with bandit feedback, is known to have complexity in the order of the dimension of the problem. the exponential weighted strategy achieves the best known regret bound that is of the order of $ d ^ { ` 2 } \\ sqrt { n } $ ( where $ d $ is the dimension of the problem, $ n $ is the time horizon ). however, such strategies are provably suboptimal or computationally inefficient. the complexity is attributed to the combinatorial structure of the action set and the dearth integral of efficient exploration strategies function of the set. mirror descent with entropic regularization function comes close to solving understanding this problem by enforcing a meticulous projection of weights with an inherent boundary condition. entropic regularization in mirror descent is the only known way of achieving a logarithmic dependence on the dimension. here, we argue otherwise and recover exactly the original intuition of proven exponential weighting by borrowing a technique from discrete optimization and approximation algorithms called ` extended weighted formulation '. such formulations appeal to the underlying geometry of the set with a guaranteed logarithmic dependence on the dimension underpinned by an information theoretic enabling entropic analysis. we show that with such formulations, exponential weighting can achieve logarithmic dependence on the dimension of the set.", "histories": [["v1", "Wed, 20 Nov 2013 11:39:26 GMT  (357kb,D)", "http://arxiv.org/abs/1311.5022v1", null], ["v2", "Tue, 26 Nov 2013 12:25:29 GMT  (336kb,D)", "http://arxiv.org/abs/1311.5022v2", null], ["v3", "Wed, 30 Sep 2015 16:43:29 GMT  (335kb,D)", "http://arxiv.org/abs/1311.5022v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["shaona ghosh", "adam prugel-bennett"], "accepted": false, "id": "1311.5022"}, "pdf": {"name": "1311.5022.pdf", "metadata": {"source": "META", "title": "Extended Formulations for Online Linear Bandit Optimization ", "authors": [], "emails": [], "sections": [{"heading": null, "text": "000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054\n055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109\n\u221a n\n(where d is the dimension of the problem, n is the time horizon ). However, such strategies are provably suboptimal or computationally inefficient. The complexity is attributed to the combinatorial structure of the action set and the dearth of efficient exploration strategies of the set. Mirror descent with entropic regularization function comes close to solving this problem by enforcing a meticulous projection of weights with an inherent boundary condition. Entropic regularization in mirror descent is the only known way of achieving a logarithmic dependence on the dimension. Here, we argue otherwise and recover the original intuition of exponential weighting by borrowing a technique from discrete optimization and approximation algorithms called \u2018extended formulation\u2019. Such formulations appeal to the underlying geometry of the set with a guaranteed logarithmic dependence on the dimension underpinned by an information theoretic entropic analysis. We show that with such formulations, exponential weighting can achieve logarithmic dependence on the dimension of the set."}, {"heading": "1. Introduction", "text": "Online linear optimization is a natural generalization of the the basic adversarial (non-stochastic) or worst case multiarm bandit framework (Auer et al., 2002), to the domain of convex optimization, where the set of actions is replaced by a compact action set A \u2282 Rd and the loss is a linear function on the action set A.\nDespite being a compelling and widely used framework, the problems become challenging to address when the fore-\nPreliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.\ncaster\u2019s decision set is the set of all possible overlapping actions, thus having a combinatorial structure. In this case, the forecaster\u2019s action at every round is an element of the combinatorial space. Examples of such problems are maximum weight cut in a graph (Goemans & Williamson, 1995), planted clique problem (Garey & Johnson, 1979) and others. For instance, in the spanning trees with Kclique problem, the number of spanning trees of a clique of K elements is exponential in K, hence the size of the all possible actions in the action set is also exponential in K (Cesa-Bianchi & Lugosi, 2012). Such combinatorial optimization problems have general tractability issues (Goemans & Williamson, 1995). Online linear optimization techniques prove good in such problems when the number of actions N is exponential in the natural parameters of the problem. However, with partial or bandit feedback, such techniques have suboptimal regret bounds. The best known bound is of the order d2 \u221a n (Bubeck et al., 2012) with a computationally hard John\u2019s exploration technique (Ball, 1997). Specifically, in the line of research of online combinatorial optimization, we address the following open questions posed by Bubeck et al. (2012). For the combinatorial bandit optimization where actions are d-dimensional, can we have an optimal regret bound with a computationally efficient strategy? Is there a natural way one can characterize the combinatorial action sets for which such optimal regret bounds are obtained under bandit feedback?\nWe address this question by using a notion from convex optimization that attributes the complexity of the problem in computing a complex set, to an efficient representation of the set. We employ techniques from non-negative and semi-definite linear programming to exploit the natural combinatorial structure of the problem. Particularly, we use a technique called \u2018extended formulation\u2019- a lift and projection technique where the complicated combinatorial convex set (action set) is approximated by a simpler convex set whose linear image is the original convex set (Gouveia et al., 2011). Particularly, we are interested in the approximation of the convex set based on the orthant or semidefinite/non-negative cone that has an associated natural barrier penalty (Nesterov et al., 1994; Chandrasekaran & Jordan, 2013). This technique shows that weaker approximations can lead to improved performance on the combinatorial optimization. Further, such hierarchy of approximations can naturally characterize the action sets for which ar X iv :1\n31 1.\n50 22\nv1 [\ncs .L\nG ]\n2 0\nN ov\n2 01\n3\n110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\noptimal regret bounds can be attained."}, {"heading": "1.1. Contribution", "text": "Specifically, our contributions are the following:\n\u2022 We present the \u201cextended\u201d exponential weighted algorithm. Our algorithm has an exponential weighting technique for the different actions; the weights in every round form the slack matrix (extended formulation). The minimum non-negative rank-r factorization, r d, (d is the dimensionality of the problem) of the slack matrix in every round guides the exploration of the set.\n\u2022 We introduce an inherent regularity measure by which the actions are weighted, that is a strikingly good indicator of the regret gap. Our method defines slack variables or inequalities given by the linear scalar loss at every round. The notion of non-negative \u2018slack\u2019 based regularization, measures the distance of each vertex (in geometric intuition, vertex corresponds to action) in the set from the origin, given the distance from the hyperplane (loss observed) and the vertex played in every iteration. The regularization penalizes for being too far away from either the loss or the last action played for exploitation while revealing boundary information (distance from origin) for exploration.\n\u2022 As an interesting consequence of the existence of the slack matrix, we show a minimum rank-r; r d sampling technique that guides the exploration and the unbiased loss estimator. In the setting of extended formulations, we analyse the notion of dimension complexity as the measure of complexity of matrices involved.\n\u2022 Our theoretical analysis is an indicator that entropic bounds exist beyond mirror descent based regularization. Contrary to the projection of the weights of the actions as in mirror descent, we use a counter-intuitive notion to lift and extend the complex action set itself to higher dimensions, take a simplified linear projection of this lift and work there. We confirm the computational advantage of the extended formulation over the existing exponential weighted techniques through empirical results on simulated and real dataset."}, {"heading": "2. Relation with Previous Work", "text": "Dani et al. (2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy. Their strategy explored the set of actions uniformly over a barycentric spanner by selecting actions that are separated by a maximum\ndistance. They also showed that without further assumptions, for \u221a dn log |A|, the bound is not improvable for A = [\u22121, 1]d. This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetric action sets |A| by using a bounded L\u221e loss assumption and using a uniform distribution over the actions, while proving that the regret bound of the order \u221a nd |A| is not improvable in general for concrete choices of A; but the bound is suboptimal in path planning problem. In both Dani et al. (2008) and Bianchi et al. (2012), the forecaster strategy is based on the probability distribution as in Exp3 by Auer (1995). However, for a set of k arms, Exp3 scales with \u221a nN lnn, and for discretised k into d as in the combinatorial setting, N possible actions is exponential in d. This work was completed by Bubeck et al. (2012), where an optimal exploration using John\u2019s theorem (Ball, 1997) from convex geometry is used on an adaptation of Exp3 called Exp2, to obtain optimal regret bound of \u221a dn logA for any set of finite actions.\nSimultaneously, there are other results in the same direction of research using gradient descent, mirror descent and perturbation approaches (Bubeck & Cesa-Bianchi, 2012). Using mirror descent for online linear optimization with bandit feedback on a computationally efficient strategy was provided by (Abernethy et al., 2008) using self concordant barriers. This approach enforced a natural barrier based on the local geometry of the convex set to determine the optimal distance of exploration inside the convex hull of the action set. However, this strategy results in a suboptimal dependency on the dimension d of the action set given by O(d2 \u221a n) with bounded scalar loss assumption. This result is improved by Bubeck (2012) using Exp2 strategy to attain O(d \u221a n). However, Audibert (2011) proved that Exp2 is a provably suboptimal strategy in the combinatorial setting. Their work showed that when the action set is combinatorial A \u2282 [0, 1]d and loss L = [0, 1]d, the minimax regret in the full information and semi-bandit case is of the order d \u221a n while with bandit feedback the order is the sub-optimal d3/2 \u221a n. The optimal regret bound is also obtained by the mirror descent strategies on the simplex and the Euclidean ball action sets. OSMD for the Euclidean ball in Bubeck (2012), (Ball, 1997), achieves regret of the order \u221a dn. For more information on combinatorial linear optimization, interested reader may refer (Bubeck & Cesa-Bianchi, 2012). Cesa Bianchi et al. have established in (2012), the unified analysis of mirror descent and fixed share of weights between experts in the online setting. Although, they have showed a logarithmic dependence on the dimension for the generalized case, our work is different in that we capture the loss estimation and show that in cases, the dependency is logarithmic on r where r d. However, our analysis of the oblivious adversary is close to their analysis for the generalized adversary on the simplex.\n220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273\n275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329\nAlthough, efficient in terms of playing the optimal strategy, the exponentially weighted techniques are computationally and provably sub-optimal given the complexity of the action set and the tools used for performing the exploration. One can question if the non-triviality is in reality much more fundamental and obvious than that. From the knowledge of convex optimization, one knows that the complexity of a convex set is associated to how well the set is represented. A sufficiently complex set can be efficiently represented as a combination of low complexity simpler sets. The added complexity associated with the simplification is offset by the low order complexity of the overall simplified problem. Our work here follows this intuition and line of research."}, {"heading": "2.1. Extended Formulations", "text": "An extended formulation (Conforti et al., 2010) is a way of representing the feasible set of solutions that is naturally exponential in the size of the data, to a formulation polynomial in the natural parameters of the problem by the introduction of a polynomial number of new variables. Where possible, the extended formulation or its approximation, simplifies the computing complexity associated with the problem in the linear programming paradigm. Yannakakis\u2019s (1991) seminal work naturally characterizes the size of the extended formulation by giving the smallest size of the extension that represents the original set well. The recent work of Gouveia et al. (2011) and Fiorini et al. (2012) generalizes the theorem from linear programming paradigm for convex optimization. The techniques are also known as lift-and-project (Balas et al., 1993) and approximation techniques (Chandrasekaran & Jordan, 2013). Our work employs this striking technique to the online linear optimization setting to exploit the underlying geometry in the hope of recovering some structure in the combinatorial action set. In the illustration shown in Figure 1, P is the original compact set and Q is the extension, such that P \u2286 Q."}, {"heading": "3. Extended Formulations in Linear Bandits", "text": ""}, {"heading": "3.1. Problem Setup", "text": "In the formal setting of the combinatorial bandit optimization problem, we consider an adversarial environment with limited feedback. The prediction game as described in the previous sections proceeds in a series of rounds t = 1, . . . , T . The action set forms a subset of a hypercube A \u2282 [0, 1]d that comprises set of all possible actions, where d is the dimension of the hypercube. When the feasible solutions of a combinatorial optimization problem are encoded as 0/1-points in Rd, they yield a convex hull polytope of the resulting points. The loss is as-\nO\nv3\nV2\nV1\nP Q\nAv \u2264 b2 2\nAv \u2264 b3 3\nAv \u2264 b1 1\n(a)\n(-1,-1)(-1,1)\n(1,1) (1,-1)\nP\nQ\nO\n(b)\nFigure 1. Illustration of extended formulation of convex compact set P (a) Q is the lifted or extended set, both P and Q are in the non-negative orthant cone. Vertices are denoted by vi, bi\u2019s are the distances of the hyperplanes from the origin (b) Toy example of the infinite polytope P and its extended formulation Q. Slack measures the distance from the origin O to the vertices.\nsumed to be non-negative L = [0, 1]d. At every round t, the forecaster chooses an action at \u2208 A; the action is represented by its incidence vector as corners of the hypercube. The cardinality of all possible actions is denoted by N . The adversary secretly selects the loss that is incurred by the forecaster and is defined as aTt lt. The forecaster\u2019s strategy at every round is to choose a probability distribution pt\u22121 (1) , . . . , pt\u22121 (N) over the set of actions such that pt\u22121(k) \u2265 0 for all k = 1, . . . , N . Note that\u2211N k=1 pt\u22121(k) = 1, and action at = k is drawn with probability pt\u22121(k). The objective of the forecaster is to minimize the pseudo regret defined in Equation 1. The expectation in 1 is with respect to the forecaster\u2019s internal randomization and possible adversarial randomization. Note that this problem formulation is identical to the adversarial bandit framework when d = N and the action selected at each round at forms the canonical basis.\nRT = E T\u2211\nt=1\naTt zt \u2212min a\u2208A\nE T\u2211\nt=1\naT zt (1)"}, {"heading": "3.2. Slack based Regularization", "text": "Definition 1. Let P = conv { a|a \u2208 A \u2282 {R}d } be the\nconvex hull of the action set with a non-empty interior, then for every loss estimate sampled randomly from the simplex l \u2208 {0, 1}d by the adversary, there is an extended set Q = {x|Ax \u2264 l}, where x is the action played, A is the action set, l is the loss observed. Both P and Q are defined on the positive orthant.\nThe above definition implies that, if P is the convex hull of all actions defined on the positive quadrant, let there be a set of hyperplanes given by the loss observed at every round; then Q defines a set of inequalities that define on which\n330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383\n385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439\nAlgorithm 1 Extended Exp Input: learning rate \u03b7 \u2265 0, mixing coefficient \u03b1 \u2265 0, action set dimensional rank \u03b4 = \u03c1 (A). repeat\nInitialize p1 = ( 1 \u03b4 , . . . , 1 \u03b4 ) \u2208 R|A|. Let non-negative rank r = \u03b4 for t = 1 to T do pi,t = \u03b1 r + (1\u2212 \u03b1)wi,t\nPlay action at from pt Observe loss aTt lt\nUpdate loss wi,t+1 = pi,te \u2212\u03b7|aTt lt\u2212aTt ai,t|\u2211N j=1 pj,te \u2212\u03b7|aTt lt\u2212aTt aj,t| Mt+1 = [wt,i]1\u2264t\u2264t+1,1\u2264i\u2264N Find minimum non-negative rank r such that M =\u2211r k=1 T\nkUk end for\nuntil Time horizon T or no regret\nside of the hyperplanes defined by the losses, the vertices lie.\nDefinition 2. The slack regularity for all action a \u2208 A, is the non-negative measure of how far the action is from the origin of the set P . It is the difference of the distance of the hyperplane from the origin defined by the loss observed l proportional to the distance of the action from the last action played: \u2223\u2223aTt lt \u2212 aTt a \u2223\u2223, where aTt lt defines the hyperplane for round t, at is the action played.\nThe slack regularity measure is the linear projection between P andQ, and defines a much simpler slack set called the slack matrix. The idea is the original complex set P is extended to Q by introducing inequalities and slack variables - the hyperplanes, then a linear projection technique is used to project the lift back. The linear projection defines the slack matrix.\nDefinition 3. The non-negative slack matrix defined by the slack regularity measure is the measure of how much any particular action is breaking the inequality or is far from the hyperplane proportional to the action played and is given byMt,i = \u2223\u2223aTt lt \u2212 aTt ai \u2223\u2223 for round t is a (t+ 1)\u00d7N matrix"}, {"heading": "4. Algorithms and Results", "text": "In Algorithm 1, an exponentially weighted technique is used to weight the actions based on the slack regularity measure defined in Definition 2. This defines the weight matrix or the slack matrix Mt+1 at each round that has a positive rank.\nDefinition 4. As long there is a positive rank for the weights matrixMt, there is a guaranteed non-negative factorization possible such that M = TU , where T and U\nAlgorithm 2 Extended Exp2 with Sampling Input: learning rate \u03b7 \u2265 0, mixing coefficient \u03b1 \u2265 0, action set dimensional rank \u03b4 = \u03c1 (A). repeat\nInitialize p1 = ( 1 \u03b4 , . . . , 1 \u03b4 ) \u2208 R|A|. Let non-negative rank r = \u03b4. Let initial covariance matrix P1 = cov (A) for t = 1 to T do pi,t = \u03b1 r \u03b3ri,t + (1\u2212 \u03b1)wi,t\nPlay action at from pt Observe loss aTt lt Estimate loss l\u0303t = P\u22121t ata T t lt\nUpdate loss wi,t+1 = pi,te \u2212\u03b7|aTt l\u0303t\u2212aTt ai,t|\u2211N j=1 pj,te \u2212\u03b7|aTt l\u0303t\u2212aTt aj,t| Mt+1 = [wt,i]1\u2264t\u2264t+1,1\u2264i\u2264N Find r non-negative rank-1 matrices Pk such that M = \u2211r k=1 Pk Sample index ri uniformly \u2200ri \u2208 [r] Let \u03b3ri,t =\nPri ,t\u2211r ri=1 Pri,t\nend for until Time horizon T or no regret\nare non-negative factors. In general, there is a minimum k, such that Mi,j = \u2211 k Ti,kUk,j\nThe Figure 2 illustrates the non-negative factorization of the slack matrix."}, {"heading": "4.1. Sampling of low rank approximations", "text": "Definition 4 directly leads to the technique of low rank approximations of the action set. Typically, in the linear optimization algorithms, the crucial step is the loss estimation l\u0303t. For the loss estimator to be an unbiased estimator, the actions correlation matrix is generally used. Here, instead of using the whole correlation matrix, we uniformly sample rank-one matrices from the distribution of r rank-one matrices.\nDefinition 5. Given the existence of a positive rank slack weight matrix Mt at every round t, that has a minimum r non-negative factorization possible, then M is the sum of r non-negative rank-one matrices as M =\n\u2211r ri=1\nPri , where Pri is the rank-1 matrix. At every round, the index ri is sampled from the distribution \u03b3ri,t uniformly. This index thus sampled generates a subset S \u2282 A for actions in S to be explored also such that the rank-|S| matrix Pri,t =\u2211 a\u2208S wt (a) aa T\nIt is important to note here that unlike in earlier approaches, we reduce the complexity of exploration by sampling from \u03b3ri,t that gives the bound that depends on the logarithm of r in our analysis, where r d. Another important point we would like to make here is that it can be shown that\n440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493\n495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549\nExtended Formulations for Online Linear Bandit Optimization October 3, 2013\nM =   0 0 1 2 2 1 1 0 0 1 2 2 2 1 0 0 1 2 2 2 1 0 0 1 1 2 2 1 0 0 0 1 2 2 1 0   =   1 0 1 0 0 1 0 0 0 1 0 0 0 1 2 0 1 0 0 1 0 1 1 0 0 0 0 2 1 0     0 0 0 1 2 1 1 2 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1  \n1\nFigure 2. Slack matrix factorization. Slack matrix for the regular hexagon M factored into non-negative matrices. Example taken from (Gouveia et al., 2011).\nthe Pri,t\u2019s compute the slack matrix M in expectation (see (Conforti et al., 2011)) over the randomization of the player and the adversary."}, {"heading": "4.2. Complexity of Extension", "text": "It turns out that the non-negative rank of the slack matrixM is the extension complexity of the polytope (hypercube in our case) of actions P . In reality, the non negative rank of a slack matrix S of the order log2 d is the extension complexity for dimensionality d. The rank of the slack matrix provides lower bounds to the complexity of the extension. The lower bound on the non-negative rank for a regular n-gon that has a Rr+ lift is given by r = O (log2 (d)) (Gouveia et al., 2011). Similarly, the minimum r-lift representation of the combinatorial d-dimensional hypercube is bounded by 2d."}, {"heading": "4.3. Information Theoretic - Entropic treatment", "text": "Although, we are in the exponential weights setting unlike the mirror descent algorithms, where an entropic regularization function is carefully selected to impose the boundary penalty, in our case, the regularization is inherent in the treatment of the problem. It has been shown that a randomized communication protocol can give a stronger lower bound in the order of base-2 logarithm of the non-negative rank of the slack matrix (communication matrix) computed in expectation (Faenza et al., 2012). Moreover, the uniform sampling of the non-negative rank index ri in our case, has an associated entropyH(ri) = log |S|. In fact, the uniform sampling of the low rank matrices Pri with bias wt has an entropyH(wt) = wt log 1wt+(1\u2212wt) log(1\u2212wt) which is similar to the entropic function in the mirror descent case."}, {"heading": "4.4. Analysis", "text": "Lemma 4.1. For all t \u2265 1, learning rate \u03b7 \u2265 0, for all a \u2208 A and oblivious adversary playing with the law qt \u2208 4d,\nd being the dimension of the problem, Algorithm 1 satisfies\nN\u2211\ni=1\npi,t ( aTt lt ) \u2264 ||at||1 ||ai,t||1 + a T t lt+\n1 \u03b7 log wi,t+1 pi,t + \u03b7 8\nProof. By definition of wi,t+1 we have,\nN\u2211\nj=1\npj,te \u2212\u03b7|\u3008at,lt\u3009\u2212\u3008at,aj,t\u3009| =\npi,te \u2212\u03b7|\u3008at,lt\u3009\u2212\u3008at,ai,t\u3009|\nwi,t+1\n(2)\nBy Hoeffding\u2019s inequality (Cesa-Bianchi, 2006)(Section A.1.1), with bounds on \u03b7 \u2282 [0, 1], taking logarithm on both sides of the inequality, for the right hand side of 2, we have,\nlog ( pi,te \u2212\u03b7|\u3008at,lt\u3009\u2212\u3008at,ai,t\u3009| ) \u2212 log (wi,t+1) \u2264 \u03b72\n8\nSimplifying the left hand side of the equation 2, reversing the inequality for negative terms by multiplying with \u22121, dividing by \u03b7 and ignoring the resultant negative term we have,\nN\u2211\ni=1\npi,t (\u3008at, lt\u3009 \u2212 \u3008at, ai,t\u3009) \u2264 \u3008at, lt\u3009+\n1 \u03b7 log wi,t+1 pi,t + \u03b7 8\nBy Cauchy-Schwarz Inequality for the inner product, exchanging the finite summation over t with expectation over randomization of the learning algorithm in playing an action and of the adversary in playing the corresponding loss for the L1 norm, we have,\nE ( aTt lt ) \u2264 ||at||1 ||ai,t||1 +a T t lt+ 1\n\u03b7 log wi,t+1 pi,t + \u03b7 8\nWe take a convex aggregation of lemma 4.1 with respect to an unknown probability distribution of the adversary qi,t on the simplex, and rewrite the bound in the lemma 4.1 to get,\nE [ (at \u2212 a)T lt ] \u2264 1 \u03b7 N\u2211\ni=1\nqi,t log wi,t+1 pi,t\n+ \u03b7\n8 + ||at||1 ||ai,t||1\n(3)\nLemma 4.2. For all t \u2265 1, learning rate \u03b7 \u2265 0, for all a \u2208 A and oblivious adversary playing with the law qt \u2208 4d, d being the dimension of the problem, Algorithm 1 satisfies\nE [ (at \u2212 a)T lt ] \u2264 1 \u03b7 N\u2211\ni=1\nqi,t log pi,t+1 pi,t\n+ \u03b7\n8 + ||at||1 ||ai,t||1 (4)\n550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603\n605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659\nProof. We use K-L divergence to rewrite the upper bound in 4.1. The K-L divergence of two probability distribution p and q is given byKL (p|q)) = \u2211 i pi log qi pi\n, is always positive. Using Pythagorean theorem for the K-L divergence, we get,\nE [ (at \u2212 a)T lt ] \u2264 1 \u03b7 N\u2211\ni=1\nqi,t log wi,t+1 pi,t + \u03b7 8\n+ ||at||1 ||ai,t||1\n\u2264 KL (qt,pt)\u2212KL (qt,wt+1) \u03b7 + \u03b7 8\n+ ||at||1 ||ai,t||1\n\u2264 KL (qt,pt)\u2212KL (qt,pt+1) \u03b7 + \u03b7 8\n+ ||at||1 ||ai,t||1\n= 1\n\u03b7\nN\u2211\ni=1\nqi,t log pi,t+1 pi,t + \u03b7 8\n+ ||at||1 ||ai,t||1\nTheorem 4.3. Algorithm 1 with the slack loss update, for \u03b7 > 0, mixing coefficient \u03b1 > 0, A the finite set of N actions with a dimensional complexity of \u03c1 (A), for r > 0 being the minimum non-negative rank of the slack matrix Mt of weights for different actions, for all t \u2265 1 and with \u03c1max (A) = \u03b4 , for all sequence of bounded losses, the algorithm satisfies for max \u2211T t=1 ||at||1 = BT\nT\u2211\nt=1\nE [ (at \u2212 a)T lt ] \u2264 1 \u03b7 log \u03b4 + 1 \u03b7 log r \u03b1 + T\u03b7 8 +BTB0\nProof. Rewriting the upper bound from Lemma 4.1 and summing over t = 2, . . . , T while taking Abel\u2019s transform of the first term, we get\nT\u2211\nt=1\nE [ (at \u2212 a)T lt ] \u2264 1 \u03b7 T\u2211\ni=1\nN\u2211\ni=1\nqi,t log pi,t+1 pi,t +\nT\u2211\ni=1\n\u03b7\n8\n+\nT\u2211\ni=1\n||at||1 ||ai,t||1\n\u2264 1 \u03b7\nN\u2211\ni=1\nqi,T log pi,T+1\n\ufe38 \ufe37\ufe37 \ufe38 0\n+ 1\n\u03b7\nN\u2211\ni=1\nqi,0 log 1\npi,1\n\u22121 \u03b7\nT\u22121\u2211\nt=1\nqi,t log pi,t+1\n\ufe38 \ufe37\ufe37 \ufe38 0\n+ 1\n\u03b7\nT\u22121\u2211\nt=1\nqi,t log 1\npi,t\n+\nT\u2211\ni=1\n\u03b7 8 +\nT\u2211\ni=1\n||at||1 ||ai,t||1\nWe use the fact that the initial exploration distribution is upper bounded by the dimensional complexity of the action set. Further, we know pi,t \u2265 \u03b1r , and 1 pi,t \u2264 r\u03b1 . \u2211N i=1 qi,1 = 1, \u2211N i=1 qi,T = 1 to get the required bound.\nLemma 4.4. The complexity of the non-negative slack matrix Mt in Algorithm1 in expectation is given by the dimensional complexity \u03c1 and is bounded by sr\u2212n, with r being the minimum non-negative rank of M , where M is of size s\u00d7 n\nE\u03c1(Mt) \u2265 sr \u2212 n r\nProof. We use the Sylvester\u2019s law for matrices Am\u00d7n and Bn\u00d7q\n\u03c1(A) + \u03c1(B)\u2212N \u2264 \u03c1(AB) \u2264 min{\u03c1(A), \u03c1(B)} (5)\nWe know from definition 5, M = \u2211r ri=1\nPri and M = T rUr for minimum non-negative r. Substituting in 5 and using the fact that Pri are non-negative rank-r matrices, we have,\ns\u2211\nri=1\n\u03c1(Pri)\u2212 n \u2264 \u03c1(Mt) \u2264 min{\u03c1(T ), \u03c1(U)}\nsr \u2212 n \u2264 \u03c1E\n( r\u2211\ni=1\nPri\n) \u2264 r\nIn general, for the particular Pri corresponding to minimum rank, the dimension complexity is lower bounded by\n\u03c1(Pri) \u2265 sr \u2212 n r\nIn the case of when an extended formulation of rank-r exists for M , \u03c1(Pri) \u2265 r\nProposition 4.5. Expectation over the randomization of the learning algorithm and the adversary\u2019s losses is the measure of the how well the algorithm played in expectation over the randomization of any action played\nProof. We use here, E [ l\u0303t ] = lt\nE \u2223\u2223\u2223aTt l\u0303t \u2212 aTt a \u2223\u2223\u2223 = EaTt \u2223\u2223\u2223l\u0303t \u2212 a \u2223\u2223\u2223\n= E \u2223\u2223\u2223\u2223aTt \u2223\u2223\u2223\u22232\naTt lt\n\u2223\u2223\u2223lTt l\u0303t \u2212 aT lt \u2223\u2223\u2223\n= E \u2223\u2223\u2223\u2223aTt \u2223\u2223\u2223\u22232\naTt lt\n\u2223\u2223\u2223E [ lTt l\u0303t ] \u2212 E [ aT lt ]\u2223\u2223\u2223\n= E \u2223\u2223\u2223\u2223aTt \u2223\u2223\u2223\u22232\naTt lt\n\u2223\u2223\u2223||lt||2 \u2212 E [ aT lt ]\u2223\u2223\u2223\n= E \u2223\u2223\u2223\u2223aTt lt \u2223\u2223\u2223\u2223\u2212 E [ aT lt ]\n660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713\n715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769\nTheorem 4.6. When r rank-one non-negative matrices are sampled from the slack matrix M of weights with size (t+ 1) \u00d7 N . for all t \u2265 0, for all actions and learning rate \u03b7 \u2265 0, mixing coefficient \u03b1 \u2265 0, for all sequence of bounded losses Algorithm 2 satisfies\nE T\u2211\nt=1\n[( aTt lt )] \u2212min a\u2208A E T\u2211\nt=1\n[( aT lt )] \u2264 1 \u03b7 (1 + \u03b1T )\n+ \u03b1 \u03b7r T log (r \u2212N) + r \u03b1 BT\nProof. With a similar analysis as in lemma 4.1, on simplifying the right hand side of the definition of wi,t+1 in Algorithm 2 , and from Proposition 4.5 we obtain,\nE [ aTt lt ] \u2212 E [ aT lt ] \u2264 1 \u03b7 log wi,t+1 pi,t + \u03b7 8\n\u2212 log ||at||1 ||ai,t||1\ufe38 \ufe37\ufe37 \ufe38 0 + log aTt l\u0303t\nLet us consider the fourth term of the right hand side. We know that a singular value decomposition of the pseudo inverse matrix P\u22121t is P \u22121 t = \u2211r i=1 1 \u03bbi viv T i , where 1 \u03bbi\n\u2019s are the singular values, with \u03bbis being the non-negative real eigenvalues and vi\u2019s are the orthonormal vectors. Hence, we have for log ( P\u22121t at ) = log ((\u2211r i=1 1 \u03bbi viv T i ) at ) . In order to maximize this, we consider the minimim singular value that is lower bounded by \u03bbi \u2265 \u03b1r from the definition of pi,t. With \u2211t t=1 ||at||1 = Bt, we get the required bound on the third term. Hence we have,\nE [ aTt lt ] \u2212 E [ aT lt ] \u2264 1 \u03b7 log wi,t+1 pi,t + \u03b7 8 + log ( r \u03b1 at )\n\u2264 1 \u03b7 logwi,t+1 \u2212 1 \u03b7 log pi,t + \u03b7 8 + log ( r \u03b1 at )\n\u2264 1 \u03b7 logwi,t+1 \u2212 1 \u03b7 log (\u03b1 r \u03b3i,t + (1\u2212 \u03b1)wi,t )\n+ log ( r \u03b1 at )\n\u2264 1 \u03b7 log (wi,t+1 \u2212 wi,t)\u2212 \u03b1 \u03b7r log Pri (t)\u2211r ri=1 Pri (t)\n+ \u03b1\n\u03b7 logwi,t + log ( r \u03b1 at )\nWe bound the complexity of Pi,t using lemma 4.4 and ignore negative terms. The obtained bounds are true for any action ai \u2208 A. Summing over t = 1, . . . , T , the first term telescopes. Substituting the obtained bounds, using\nlog (x) \u2264 x\u2212 1 and simplifying we get,\nE T\u2211\ni=1\n[ aTt lt ] \u2212 E T\u2211\ni=1\n[ aT lt ] \u2264 1 \u03b7\n( N\u2211\ni=1\nwi,T ) \u2212 1 \u03b7 ( N\u2211\ni=1\nwi,1\n)\n\ufe38 \ufe37\ufe37 \ufe38 0\n\u2212 \u03b1 \u03b7r\nT\u2211\nt=1\nlog \u03c1 ( Prj )\n\ufe38 \ufe37\ufe37 \ufe38 0\n+ \u03b1\n\u03b7r\nT\u2211\nt=1\nlog \u03c1\n( r\u2211\nri=1\nPri (t)\n)\n+ \u03b1\n\u03b7\nT\u2211\nt=1\nN\u2211\ni=1\nwi,t + r\n\u03b1 BT\nWe substitute the initial exploration distribution and the bound the dimensional complexity of the action set to get the required bound."}, {"heading": "5. Experiments", "text": "We compared the extended exponential weighted approach with the state-of-the-art exponential weighted algorithms in the adversarial linear optimization bandit setting."}, {"heading": "5.1. Simulations", "text": "In the first experiment, among a d-dimensional network of routes, the optimal route should be selected by the learning algorithm. Typically, we choose d to vary between 10 and 15. The environment is an oblivious adversary to the player\u2019s actions, simulated to choose fixed but unknown losses at each round. Losses are bounded and in the range [0, 1]. The learning algorithm is executed using our basic Extended Exp algorithm. Each action is represented a d-dimension incidence vector, with 1 indicating if an edge or path is present in the route or 0 otherwise. Figure 3 displays the results of the cases where d = 10, 15 and d = 20. The performance measure is the instantaneous regret over time; we use psuedo regret here. The number of iterations in the game are 10000 for d = 10 and 100 for d = 15. In both cases, the results are averaged over 100 runs of the games. We implemented Algorithm 1, Exp2 (Bubeck & Cesa-Bianchi, 2012), Exp3 (Auer et al., 2002), Exp3.P (Auer et al., 2002), and CombBand (Cesa-Bianchi & Lugosi, 2012). Our baseline is Exp2 which has the best known performance but provably sub-optimal. We could not compare with Exp2 with John\u2019s exploration (Bubeck et al., 2012) as the authors state its computational inefficiency. All the experiments are implemented using Matlab on a notebook with i7-2820QM CPU 2.30 GHz with 8 GB RAM. We see that Extended Exp 2 clearly beats the baseline comfortably against the oblivious adversary. We repeated the experiments with different configurations of the network and different dimensions. Each time, the complexity of the problem increases exponentially with the dimension. Extended Exp2 performs best\n770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823\n825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879\n0 2000 4000 6000 8000 10000\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nTime\nIn st\nan ta\nne ou\ns R\neg re\nt\ncombband exp2 extexp2 exp3 exp3.P\n(a)\n0 10 20 30 40 50 60 70 80 90 100 0.4\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\ntime\nre gr\net\ncombband exp2 extexp2 exp3 exp3.P\n(b)\nFigure 3. Simulation results with the network dataset with 10 dimensions. Results averaged over 100 runs. Extended Exp2(BLACK) beats the baseline Exp2 (RED) (a) Network dataset with 10 dimensions (b) Network dataset with 15 dimensions.\nin all our experiments, the results of the other trials are excluded for brevity."}, {"heading": "5.2. Empirical Datasets", "text": "The dataset is the Jester Online Joke Recommendation dataset (Goldberg, 2003) from the University of Berkeley, which is data collected from 24,983 users with ratings on 36 or more jokes. We consider the ratings of 24,983 users on 20 jokes, constituting the dense matrix. The ratings vary in the range [\u221210.00, 10.00], including not rated jokes. We scale and normalize the ratings in the range [0, 1]. For the purpose of our problem, each user represents a d dimensional decision problem, where d = 20. We do not make the ratings available to the algorithm, instead the ratings are provided by the environment based on the user. In other words, we assume the non-oblivious setting for the adversary, the loss changes with the user or action selected. The goal of the algorithm is to be able to identify the user who has rated the worst on all the jokes on average. As before we execute all the instances of the common exponential weighted algorithms and the basic version of Extended Exp2. Figure 4 displays the result on the dataset, all the plots are averaged over 100 runs of the games. We run each game for 10000 rounds. All the experiments are implemented on Matlab on a notebook with i7-2820QM CPU 2.30 GHz with 8 GB RAM. We observe that once again Extended Exp2 beats all the others. Quite surprisingly, Exp2 and Combband seem to perform equivalently in the nonoblivious setting. It will be interesting to compare with mirror descent based linear optimization, that is for future work.\n0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nTime\nIn st\nan ta\nne ou\ns R\neg re\nt\ncombband exp2 extexp2 exp3 exp3.P\nFigure 4. Dataset results with the Jester Online Joke Recommendation dataset with 20 dimensions. Results averaged over 100 runs. Extended Exp2(BLACK) beats the baseline Exp2 (RED)."}, {"heading": "6. Conclusion and Future Work", "text": "We have shown a novel slack based regularization approach to the online linear optimization with bandit information in the exponential weighted setting. We have proved a logarithmic dependence on the dimension complexity of the problem. We showed that in the exponential weighted setting, logarithmic dependence is achievable by a clever regularization, that measures how far an action in the set is, given the loss and the other actions using a trick called \u2019extended formulation\u2019. As future work, we would like to derive the lower bounds and prove that our results are unimprovable in general. We would also like to investigate how the extended formulation characterizes different action sets. Further, it would be interesting to derive an information theoretic entropic analysis of our method.\n880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933\n935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989"}], "references": [{"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["Abernethy", "Jacob", "Hazan", "Elad", "Rakhlin", "Alexander"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Minimax policies for combinatorial prediction games", "author": ["Audibert", "Jean-Yves", "Bubeck", "S\u00e9bastien", "Lugosi", "G\u00e1bor"], "venue": "arXiv preprint arXiv:1105.4871,", "citeRegEx": "Audibert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2011}, {"title": "Gambling in a rigged casino: The adversarial multi-armed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Auer et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Auer et al\\.", "year": 1995}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "A lift-and-project cutting plane algorithm for mixed 0\u20131 programs", "author": ["Balas", "Egon", "Ceria", "Sebasti\u00e1n", "Cornu\u00e9jols", "G\u00e9rard"], "venue": "Mathematical programming,", "citeRegEx": "Balas et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Balas et al\\.", "year": 1993}, {"title": "An elementary introduction to modern convex geometry", "author": ["Ball", "Keith"], "venue": "Flavors of geometry,", "citeRegEx": "Ball and Keith.,? \\Q1997\\E", "shortCiteRegEx": "Ball and Keith.", "year": 1997}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["Bubeck", "S\u00e9bastien", "Cesa-Bianchi", "Nicolo"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["Bubeck", "S\u00e9bastien", "Cesa-Bianchi", "Nicolo", "Kakade", "Sham M"], "venue": "JMLR Workshop and Conference Proceedings Volume", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Prediction, learning, and games", "author": ["Cesa-Bianchi", "Nicolo"], "venue": null, "citeRegEx": "Cesa.Bianchi and Nicolo.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Nicolo.", "year": 2006}, {"title": "Combinatorial bandits", "author": ["Cesa-Bianchi", "Nicolo", "Lugosi", "G\u00e1bor"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "Mirror descent meets fixed share (and feels no regret)", "author": ["Cesa-Bianchi", "Nicol\u00f2", "Gaillard", "Pierre", "Lugosi", "G\u00e1bor", "Stoltz", "Gilles"], "venue": "arXiv preprint arXiv:1202.3323,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "Computational and statistical tradeoffs via convex relaxation", "author": ["Chandrasekaran", "Venkat", "Jordan", "Michael I"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2013}, {"title": "Extended formulations in combinatorial optimization", "author": ["Conforti", "Michele", "Cornu\u00e9jols", "G\u00e9rard", "Zambelli", "Giacomo"], "venue": null, "citeRegEx": "Conforti et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Conforti et al\\.", "year": 2010}, {"title": "Extended formulations, non-negative factorizations and randomized communication protocols", "author": ["Conforti", "Michele", "Faenza", "Yuri", "Fiorini", "Samuel", "Grappe", "Roland", "Tiwary", "Hans Raj"], "venue": "arXiv preprint arXiv:1105.4127,", "citeRegEx": "Conforti et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Conforti et al\\.", "year": 2011}, {"title": "The price of bandit information for online optimization", "author": ["Dani", "Varsha", "Hayes", "Thomas", "Kakade", "Sham M"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Extended formulations, nonnegative factorizations, and randomized communication protocols", "author": ["Faenza", "Yuri", "Fiorini", "Samuel", "Grappe", "Roland", "Tiwary", "Hans Raj"], "venue": "In Combinatorial Optimization,", "citeRegEx": "Faenza et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Faenza et al\\.", "year": 2012}, {"title": "Linear vs. semidefinite extended formulations: exponential separation and strong lower bounds", "author": ["Fiorini", "Samuel", "Massar", "Serge", "Pokutta", "Sebastian", "Tiwary", "Hans Raj", "de Wolf", "Ronald"], "venue": "In Proceedings of the 44th symposium on Theory of Computing,", "citeRegEx": "Fiorini et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fiorini et al\\.", "year": 2012}, {"title": "Computers and intractability a guide to the theory of np-completeness", "author": ["M.R. Garey", "D.S. Johnson"], "venue": null, "citeRegEx": "Garey and Johnson,? \\Q1979\\E", "shortCiteRegEx": "Garey and Johnson", "year": 1979}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["Goemans", "Michel X", "Williamson", "David P"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Goemans et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Goemans et al\\.", "year": 1995}, {"title": "Anonymous Ratings from the Jester Online Joke", "author": ["Goldberg", "Ken"], "venue": "Recommender System. http://eigentaste. berkeley.edu/dataset/,", "citeRegEx": "Goldberg and Ken.,? \\Q2003\\E", "shortCiteRegEx": "Goldberg and Ken.", "year": 2003}, {"title": "Lifts of convex sets and cone factorizations", "author": ["Gouveia", "Joao", "Parrilo", "Pablo A", "Thomas", "Rekha"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Gouveia et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gouveia et al\\.", "year": 2011}, {"title": "Interior-point polynomial algorithms in convex programming, volume", "author": ["Nesterov", "Yurii", "Nemirovskii", "Arkadii Semenovich", "Ye", "Yinyu"], "venue": null, "citeRegEx": "Nesterov et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Nesterov et al\\.", "year": 1994}, {"title": "Expressing combinatorial optimization problems by linear programs", "author": ["Yannakakis", "Mihalis"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Yannakakis and Mihalis.,? \\Q1991\\E", "shortCiteRegEx": "Yannakakis and Mihalis.", "year": 1991}], "referenceMentions": [{"referenceID": 3, "context": "Online linear optimization is a natural generalization of the the basic adversarial (non-stochastic) or worst case multiarm bandit framework (Auer et al., 2002), to the domain of convex optimization, where the set of actions is replaced by a compact action set A \u2282 R and the loss is a linear function on the action set A.", "startOffset": 141, "endOffset": 160}, {"referenceID": 6, "context": "The best known bound is of the order d \u221a n (Bubeck et al., 2012) with a computationally hard John\u2019s exploration technique (Ball, 1997).", "startOffset": 43, "endOffset": 64}, {"referenceID": 6, "context": "The best known bound is of the order d \u221a n (Bubeck et al., 2012) with a computationally hard John\u2019s exploration technique (Ball, 1997). Specifically, in the line of research of online combinatorial optimization, we address the following open questions posed by Bubeck et al. (2012). For the combinatorial bandit optimization where actions are d-dimensional, can we have an optimal regret bound with a computationally efficient strategy? Is there a natural way one can characterize the combinatorial action sets for which such optimal regret bounds are obtained under bandit feedback?", "startOffset": 44, "endOffset": 282}, {"referenceID": 20, "context": "Particularly, we use a technique called \u2018extended formulation\u2019- a lift and projection technique where the complicated combinatorial convex set (action set) is approximated by a simpler convex set whose linear image is the original convex set (Gouveia et al., 2011).", "startOffset": 242, "endOffset": 264}, {"referenceID": 21, "context": "Particularly, we are interested in the approximation of the convex set based on the orthant or semidefinite/non-negative cone that has an associated natural barrier penalty (Nesterov et al., 1994; Chandrasekaran & Jordan, 2013).", "startOffset": 173, "endOffset": 227}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy.", "startOffset": 177, "endOffset": 196}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy. Their strategy explored the set of actions uniformly over a barycentric spanner by selecting actions that are separated by a maximum distance. They also showed that without further assumptions, for \u221a dn log |A|, the bound is not improvable for A = [\u22121, 1]. This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetric action sets |A| by using a bounded L\u221e loss assumption and using a uniform distribution over the actions, while proving that the regret bound of the order \u221a nd |A| is not improvable in general for concrete choices of A; but the bound is suboptimal in path planning problem.", "startOffset": 178, "endOffset": 533}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy. Their strategy explored the set of actions uniformly over a barycentric spanner by selecting actions that are separated by a maximum distance. They also showed that without further assumptions, for \u221a dn log |A|, the bound is not improvable for A = [\u22121, 1]. This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetric action sets |A| by using a bounded L\u221e loss assumption and using a uniform distribution over the actions, while proving that the regret bound of the order \u221a nd |A| is not improvable in general for concrete choices of A; but the bound is suboptimal in path planning problem. In both Dani et al. (2008) and Bianchi et al.", "startOffset": 178, "endOffset": 862}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy. Their strategy explored the set of actions uniformly over a barycentric spanner by selecting actions that are separated by a maximum distance. They also showed that without further assumptions, for \u221a dn log |A|, the bound is not improvable for A = [\u22121, 1]. This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetric action sets |A| by using a bounded L\u221e loss assumption and using a uniform distribution over the actions, while proving that the regret bound of the order \u221a nd |A| is not improvable in general for concrete choices of A; but the bound is suboptimal in path planning problem. In both Dani et al. (2008) and Bianchi et al. (2012), the forecaster strategy is based on the probability distribution as in Exp3 by Auer (1995).", "startOffset": 178, "endOffset": 888}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy. Their strategy explored the set of actions uniformly over a barycentric spanner by selecting actions that are separated by a maximum distance. They also showed that without further assumptions, for \u221a dn log |A|, the bound is not improvable for A = [\u22121, 1]. This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetric action sets |A| by using a bounded L\u221e loss assumption and using a uniform distribution over the actions, while proving that the regret bound of the order \u221a nd |A| is not improvable in general for concrete choices of A; but the bound is suboptimal in path planning problem. In both Dani et al. (2008) and Bianchi et al. (2012), the forecaster strategy is based on the probability distribution as in Exp3 by Auer (1995). However, for a set of k arms, Exp3 scales with \u221a nN lnn, and for discretised k into d as in the combinatorial setting, N possible actions is exponential in d.", "startOffset": 178, "endOffset": 980}, {"referenceID": 2, "context": "(2008) showed that optimal regret bounds of the order of O( \u221a n) (where n is the number of rounds) was first obtained by using a variant of the original adversarial bandit Exp3 (Auer et al., 1995) strategy. Their strategy explored the set of actions uniformly over a barycentric spanner by selecting actions that are separated by a maximum distance. They also showed that without further assumptions, for \u221a dn log |A|, the bound is not improvable for A = [\u22121, 1]. This exploration strategy was later refined by Bianchi et al. (2012), for combinatorial symmetric action sets |A| by using a bounded L\u221e loss assumption and using a uniform distribution over the actions, while proving that the regret bound of the order \u221a nd |A| is not improvable in general for concrete choices of A; but the bound is suboptimal in path planning problem. In both Dani et al. (2008) and Bianchi et al. (2012), the forecaster strategy is based on the probability distribution as in Exp3 by Auer (1995). However, for a set of k arms, Exp3 scales with \u221a nN lnn, and for discretised k into d as in the combinatorial setting, N possible actions is exponential in d. This work was completed by Bubeck et al. (2012), where an optimal exploration using John\u2019s theorem (Ball, 1997) from convex geometry is used on an adaptation of Exp3 called Exp2, to obtain optimal regret bound of \u221a dn logA for any set of finite actions.", "startOffset": 178, "endOffset": 1188}, {"referenceID": 0, "context": "Using mirror descent for online linear optimization with bandit feedback on a computationally efficient strategy was provided by (Abernethy et al., 2008) using self concordant barriers.", "startOffset": 129, "endOffset": 153}, {"referenceID": 0, "context": "Using mirror descent for online linear optimization with bandit feedback on a computationally efficient strategy was provided by (Abernethy et al., 2008) using self concordant barriers. This approach enforced a natural barrier based on the local geometry of the convex set to determine the optimal distance of exploration inside the convex hull of the action set. However, this strategy results in a suboptimal dependency on the dimension d of the action set given by O(d \u221a n) with bounded scalar loss assumption. This result is improved by Bubeck (2012) using Exp2 strategy to attain O(d \u221a n).", "startOffset": 130, "endOffset": 555}, {"referenceID": 0, "context": "Using mirror descent for online linear optimization with bandit feedback on a computationally efficient strategy was provided by (Abernethy et al., 2008) using self concordant barriers. This approach enforced a natural barrier based on the local geometry of the convex set to determine the optimal distance of exploration inside the convex hull of the action set. However, this strategy results in a suboptimal dependency on the dimension d of the action set given by O(d \u221a n) with bounded scalar loss assumption. This result is improved by Bubeck (2012) using Exp2 strategy to attain O(d \u221a n). However, Audibert (2011) proved that Exp2 is a provably suboptimal strategy in the combinatorial setting.", "startOffset": 130, "endOffset": 620}, {"referenceID": 0, "context": "Using mirror descent for online linear optimization with bandit feedback on a computationally efficient strategy was provided by (Abernethy et al., 2008) using self concordant barriers. This approach enforced a natural barrier based on the local geometry of the convex set to determine the optimal distance of exploration inside the convex hull of the action set. However, this strategy results in a suboptimal dependency on the dimension d of the action set given by O(d \u221a n) with bounded scalar loss assumption. This result is improved by Bubeck (2012) using Exp2 strategy to attain O(d \u221a n). However, Audibert (2011) proved that Exp2 is a provably suboptimal strategy in the combinatorial setting. Their work showed that when the action set is combinatorial A \u2282 [0, 1] and loss L = [0, 1], the minimax regret in the full information and semi-bandit case is of the order d \u221a n while with bandit feedback the order is the sub-optimal d \u221a n. The optimal regret bound is also obtained by the mirror descent strategies on the simplex and the Euclidean ball action sets. OSMD for the Euclidean ball in Bubeck (2012), (Ball, 1997), achieves regret of the order \u221a dn.", "startOffset": 130, "endOffset": 1113}, {"referenceID": 0, "context": "Using mirror descent for online linear optimization with bandit feedback on a computationally efficient strategy was provided by (Abernethy et al., 2008) using self concordant barriers. This approach enforced a natural barrier based on the local geometry of the convex set to determine the optimal distance of exploration inside the convex hull of the action set. However, this strategy results in a suboptimal dependency on the dimension d of the action set given by O(d \u221a n) with bounded scalar loss assumption. This result is improved by Bubeck (2012) using Exp2 strategy to attain O(d \u221a n). However, Audibert (2011) proved that Exp2 is a provably suboptimal strategy in the combinatorial setting. Their work showed that when the action set is combinatorial A \u2282 [0, 1] and loss L = [0, 1], the minimax regret in the full information and semi-bandit case is of the order d \u221a n while with bandit feedback the order is the sub-optimal d \u221a n. The optimal regret bound is also obtained by the mirror descent strategies on the simplex and the Euclidean ball action sets. OSMD for the Euclidean ball in Bubeck (2012), (Ball, 1997), achieves regret of the order \u221a dn. For more information on combinatorial linear optimization, interested reader may refer (Bubeck & Cesa-Bianchi, 2012). Cesa Bianchi et al. have established in (2012), the unified analysis of mirror descent and fixed share of weights between experts in the online setting.", "startOffset": 130, "endOffset": 1328}, {"referenceID": 12, "context": "An extended formulation (Conforti et al., 2010) is a way of representing the feasible set of solutions that is naturally exponential in the size of the data, to a formulation polynomial in the natural parameters of the problem by the introduction of a polynomial number of new variables.", "startOffset": 24, "endOffset": 47}, {"referenceID": 4, "context": "The techniques are also known as lift-and-project (Balas et al., 1993) and approximation techniques (Chandrasekaran & Jordan, 2013).", "startOffset": 50, "endOffset": 70}, {"referenceID": 11, "context": "An extended formulation (Conforti et al., 2010) is a way of representing the feasible set of solutions that is naturally exponential in the size of the data, to a formulation polynomial in the natural parameters of the problem by the introduction of a polynomial number of new variables. Where possible, the extended formulation or its approximation, simplifies the computing complexity associated with the problem in the linear programming paradigm. Yannakakis\u2019s (1991) seminal work naturally characterizes the size of the extended formulation by giving the smallest size of the extension that represents the original set well.", "startOffset": 25, "endOffset": 471}, {"referenceID": 11, "context": "An extended formulation (Conforti et al., 2010) is a way of representing the feasible set of solutions that is naturally exponential in the size of the data, to a formulation polynomial in the natural parameters of the problem by the introduction of a polynomial number of new variables. Where possible, the extended formulation or its approximation, simplifies the computing complexity associated with the problem in the linear programming paradigm. Yannakakis\u2019s (1991) seminal work naturally characterizes the size of the extended formulation by giving the smallest size of the extension that represents the original set well. The recent work of Gouveia et al. (2011) and Fiorini et al.", "startOffset": 25, "endOffset": 670}, {"referenceID": 11, "context": "An extended formulation (Conforti et al., 2010) is a way of representing the feasible set of solutions that is naturally exponential in the size of the data, to a formulation polynomial in the natural parameters of the problem by the introduction of a polynomial number of new variables. Where possible, the extended formulation or its approximation, simplifies the computing complexity associated with the problem in the linear programming paradigm. Yannakakis\u2019s (1991) seminal work naturally characterizes the size of the extended formulation by giving the smallest size of the extension that represents the original set well. The recent work of Gouveia et al. (2011) and Fiorini et al. (2012) generalizes the theorem from linear programming paradigm for convex optimization.", "startOffset": 25, "endOffset": 696}, {"referenceID": 20, "context": "from (Gouveia et al., 2011).", "startOffset": 5, "endOffset": 27}, {"referenceID": 13, "context": "the Pri,t\u2019s compute the slack matrix M in expectation (see (Conforti et al., 2011)) over the randomization of the player and the adversary.", "startOffset": 59, "endOffset": 82}, {"referenceID": 20, "context": "The lower bound on the non-negative rank for a regular n-gon that has a R+ lift is given by r = O (log2 (d)) (Gouveia et al., 2011).", "startOffset": 109, "endOffset": 131}, {"referenceID": 15, "context": "It has been shown that a randomized communication protocol can give a stronger lower bound in the order of base-2 logarithm of the non-negative rank of the slack matrix (communication matrix) computed in expectation (Faenza et al., 2012).", "startOffset": 216, "endOffset": 237}, {"referenceID": 3, "context": "We implemented Algorithm 1, Exp2 (Bubeck & Cesa-Bianchi, 2012), Exp3 (Auer et al., 2002), Exp3.", "startOffset": 69, "endOffset": 88}, {"referenceID": 3, "context": "P (Auer et al., 2002), and CombBand (Cesa-Bianchi & Lugosi, 2012).", "startOffset": 2, "endOffset": 21}, {"referenceID": 6, "context": "We could not compare with Exp2 with John\u2019s exploration (Bubeck et al., 2012) as the authors state its computational inefficiency.", "startOffset": 55, "endOffset": 76}], "year": 2017, "abstractText": "On-line linear optimization on combinatorial action sets (d-dimensional actions) with bandit feedback, is known to have complexity in the order of the dimension of the problem. The exponential weighted strategy achieves the best known regret bound that is of the order of d \u221a n (where d is the dimension of the problem, n is the time horizon ). However, such strategies are provably suboptimal or computationally inefficient. The complexity is attributed to the combinatorial structure of the action set and the dearth of efficient exploration strategies of the set. Mirror descent with entropic regularization function comes close to solving this problem by enforcing a meticulous projection of weights with an inherent boundary condition. Entropic regularization in mirror descent is the only known way of achieving a logarithmic dependence on the dimension. Here, we argue otherwise and recover the original intuition of exponential weighting by borrowing a technique from discrete optimization and approximation algorithms called \u2018extended formulation\u2019. Such formulations appeal to the underlying geometry of the set with a guaranteed logarithmic dependence on the dimension underpinned by an information theoretic entropic analysis. We show that with such formulations, exponential weighting can achieve logarithmic dependence on the dimension of the set.", "creator": "LaTeX with hyperref package"}}}