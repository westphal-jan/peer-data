{"id": "1311.6556", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2013", "title": "Double Ramp Loss Based Reject Option Classifier", "abstract": "we consider the problem scenario of building classifiers with the option to reject i. e., not return a good prediction on a narrowly given test example. adding a reject option to a classifier algorithm is well - known in practice ; traditionally, this has been accomplished in two different ways. one is the { \\ em decoupled } method where an elected optimal base classifier ( without the reject option ) is build first } and then the original rejection boundary is optimized, typically varying in terms of a band welded around the separating surface. the { \\ em coupled } method setup is based just on typically finding both the classifier as well identifying as the rejection band at relatively the same time. existing adaptive coupled approaches are based on minimizing risk under an extension of the universally classical $ 0 - 1 $ loss function wherein a loss $ d \\ ch in ( 0,. 5 ) $ * is assigned to test a rejected example. in this paper, we propose a { \\ bf double - ramp loss } function which also gives a continuous upper bound : for $ ( 0 - d - 1 ) $ loss described above. our coupled approach is naturally based on minimizing globally regularized risk under the double ramp loss which is done using difference of convex ( dc ) programming. we show the effectiveness of our approach through experiments supported on synthetic and benchmark datasets.", "histories": [["v1", "Tue, 26 Nov 2013 05:13:18 GMT  (270kb)", "http://arxiv.org/abs/1311.6556v1", null], ["v2", "Mon, 8 Dec 2014 07:34:34 GMT  (250kb)", "http://arxiv.org/abs/1311.6556v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["naresh manwani", "kalpit desai", "sanand sasidharan", "ramasubramanian sundararajan"], "accepted": false, "id": "1311.6556"}, "pdf": {"name": "1311.6556.pdf", "metadata": {"source": "CRF", "title": "Double Ramp Loss Based Reject Option Classifier", "authors": ["Naresh Manwani", "Kalpit Desai", "Sanand Sasidharan", "Ramasubramanian Sundararajan"], "emails": ["naresh.manwani@ge.com", "kalpit.desai@ge.com", "sanand.sasidharan@ge.com", "ramasubramanian.sundararajan@ge.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 1.\n65 56\nv1 [\ncs .L\nG ]\n2 6\nN ov\n1. Introduction\nThe primary focus of classification problems has been on algorithms that return a prediction on every example in the example space. However, in many real life situations, it may be prudent to reject an example, i.e., not return a prediction, rather than run the risk of a costly potential misclassification. Consider, for instance, a physician who has to return a diagnosis for a patient based on the observed symptoms and a preliminary examination. If the symptoms are either ambiguous, or rare enough to be unexplainable without further investigation, then the physician might choose not to risk misdiagnosing the patient (which might lead to further complications). He might instead ask for further medical tests to be performed, or refer the case to an appropriate specialist. Similarly, a banker, when faced with a loan application from a customer, may choose not to decide on the basis of the available information, and ask for a credit bureau score. These actions can be viewed as akin to a classifier refusing to return a prediction (in this case, a diagnosis) in order to avoid a potential misclassification. While the follow-up actions might vary (asking for more features to describe the example, or using a different classifier), the principal response in these cases is to \u201creject\u201d the example. This paper focuses on the manner in which this principal response is decided, i.e., which examples should a classifier reject, and why? From a geometric standpoint, we can view the classifier as being possessed of a decision surface\nc\u00a9 N. Manwani, K. Desai, S. Sasidharan & R. Sundararajan.\n(which separates points of different classes) as well as a rejection surface (which determines which regions of the example space to return a prediction in, and which regions to reject).\nThe size of the rejection region impacts the proportion of cases that are likely to be rejected by the classifier, as well as the proportion of predicted cases that are likely to be correctly classified. A well-optimized classifier with a reject option is the one which minimizes the rejection rate as well as the misclassification rate on the predicted examples.\nThe analysis of classifiers without a reject option has often been performed from the standpoint of minimizing the expectation of an appropriately defined loss function (risk), the simplest of which is a 0\u2212 1 loss function defined as below:\nL0\u22121(f(x), y) =\n{\n0 if yf(x) \u2265 0 1 if yf(x) < 0 (1)\nwhere x \u2208 Rp is the feature vector and y \u2208 {\u22121,+1} is the class label. The expectation is taken with respect to the joint distributionD(x, y) from which these examples are generated. Since D(x, y) is generally assumed to be fixed but unknown, the empirical risk minimization principle (with its attendant caveats on minimizing complexity or structural risk) is used (Vapnik, 1998).\nIf we assume that a rejection region classifier g(x) is to be built, which returns a value of 1 when a given example is to be rejected, and 0 if it is to be classified, then the problem of learning with a reject option can be posed in one of two ways:\n1. Minimize the cost of misclassification (as described in equation (1)) for the predicted examples, while keeping the rejection rate below a required rate.\n2. Model the cost of rejection as a quantity d that is less than the cost of misclassification, thereby explicitly modeling the fact that one might choose to reject in order to avoid the risk of a costly potential misclassification. The loss function in this case would be as below:\nL g 0\u2212d\u22121(f(x), g(x), y) =\n\n \n \n0 if g(x) = 0, yf(x) \u2265 0\nd if g(x) = 1\n1 if g(x) = 0, yf(x) < 0\n(2)\nIf d = 0, then we will always reject. When d > .5, then we will never reject (because expected loss of random labeling is 0.5). Thus, we always take d \u2208 (0, .5).\nA typical case of Lg0\u2212d\u22121 is when g(x) (rejection region classifier) is defined as a bandwidth \u03c1 around f(x) = 0, (i.e., g(x) = 1 if |f(x)| \u2264 \u03c1 and 0 otherwise). Then a reject option classifier C(f(x), \u03c1) can be formed as follows.\nh(f(x), \u03c1) =\n\n \n \n1 if f(x) > \u03c1\n0 if |f(x)| \u2264 \u03c1\n\u22121 if f(x) < \u2212\u03c1\n(3)\nL0\u2212d\u22121 in this case is described as follows (Wegkamp and Yuan, 2011; Herbei and Wegkamp, 2006):\nL0\u2212d\u22121(f(x), \u03c1, y) =\n\n \n \n1, if yf(x) < \u2212\u03c1\nd, if |f(x)| \u2264 \u03c1\n0, otherwise\n(4)\nwhere \u03c1 is the parameter which determines the rejection region. We shall focus on this loss function for the remainder of this paper.\nFor 2-class problems, the risk under L0\u2212d\u22121 is minimized by generalized Bayes discriminant (Herbei and Wegkamp, 2006; Chow, 1970) which is as below:\nf\u2217d (x) =\n\n \n \n\u22121, if P (y = 1|x) < d\n0, if d \u2264 P (y = 1|x) \u2264 1\u2212 d\n1, if P (y = 1|x) > 1\u2212 d\n(5)\nIf one wishes to build a classifier that minimizes the average of the loss function described in equation (4) for a given training sample, two approaches can be followed, namely decoupled and coupled approaches. Details of these two approaches are as follows.\nDecoupled Approach The decoupled approach involves separating the problem into two tasks: finding the classifier and the rejection boundary. That is, the decoupled scheme first finds a classifier f(x) under the assumption that nothing is to be rejected. Then it builds a rejection boundary around the base classifier. In general, band \u03c1 around f(x) = 0 is found so that the risk under L0\u2212d\u22121 (described in (4)) is minimized.\nf(x) can be chosen as the minimizer of risk under any convex surrogate of L0\u22121. Classifier h(f(x), \u03c1) (equation (3))is shown to be infinite sample consistent with respect to the generalized Bayes classifier f\u2217d (x) described in equation (5) (Yuan and Wegkamp, 2010).\nCoupled Approach The coupled approach directly minimizes the loss function in equation (4). The coupled rejection scheme involves viewing the solution surface as two of parallel surfaces (with the rejection area in between), wherein f(x) as well as \u03c1 are to be determined simultaneously. The most common approach used for coupled rejection scheme in the literature is the risk minimization based framework. Table 1 lists out some of the loss functions specifically designed for learning reject option classifier. Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way.\nAnalogous to the convex surrogates of L0\u22121, convex surrogates for L0\u2212d\u22121 also have been proposed. Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008) discuss risk minimization based on generalized hinge loss LGH (see Table 1) for learning a reject option classifier. It is shown that a minimizer of risk under LGH is consistent to the generalized Bayes classifier f\u2217d (Bartlett and Wegkamp, 2008). Grandvalet et al. (2008) propose a risk minimization scheme under double hinge loss LDH (see Table 1) and show that resulting classifier is strongly universally consistent to the generalized Bayes classifier f\u2217d .\nWe observe that these convex loss functions have some limitations. For example, LGH is a convex upper bound to L0\u2212d\u22121 provided \u03c1 < 1 \u2212 d and LDH forms an upper bound\nLoss Function Definition\nGeneralized Hinge LGH(f(x), y) =\n\n \n \n1\u2212 1\u2212d d yf(x), if yf(x) < 0 1\u2212 yf(x), if 0 \u2264 yf(x) < 1\n0, otherwise\nDouble Hinge LDH(f(x), y) = max[\u2212y(1\u2212 d)f(x) +H(d),\u2212ydf(x) +H(d), 0] where H(d) = \u2212d log(d)\u2212 (1\u2212 d) log(1\u2212 d)\nTable 1: Existing loss functions for learning classifiers with reject option.\n-3 -2 -1 0 1 2 3\n0 .0\n0 .5\n1 .0\n1 .5\n2 .0\n2 .5\nd=0.2\nyf(x)\nL o s s\nGH DH 0-d-1 (r=0.7)\n-3 -2 -1 0 1 2 3\n0 .0\n0 .5\n1 .0\n1 .5\n2 .0\n2 .5\nd=0.2\nyf(x)\nL o s s\nGH DH 0-d-1 (r=2)\n(a) (b)\nFigure 1: Generalized hinge (GH) and double hinge (DH) losses for rejection option classification. In both the figures the value of d is kept at 0.2. (a) For \u03c1 = 0.7, both the losses upper bound the L0\u2212d\u22121 (0\u2212 d \u2212 1) loss. For \u03c1 = 2, both the losses fail to upper bound the L0\u2212d\u22121 loss. Both the loss functions increase linearly even in the rejection region than being flat.\nto L0\u2212d\u22121 provided \u03c1 \u2208 ( 1\u2212H(d) 1\u2212d , H(d)\u2212d d\n) (see Figure 1). Also, both LGH and LDH increase linearly in the rejection region instead of remaining constant. These convex losses can also become unbounded for misclassified examples with the scaling of parameters of f .\nIn this paper, we consider the coupled approach in the context of a support vector machine (SVM). SVM is based on risk minimization under hinge loss function which is a convex upper bound on the L0\u22121. It is well known for its generalization ability for nonlinear problems. However, SVM and other convex loss based classification algorithms are not robust to label noise in the data (Manwani and Sastry, 2013). Recent results show that\nramp loss based risk minimization for classifier learning is more robust to noise (Wu and Liu, 2007) and gives sparse solutions compared to hinge loss based SVM. This makes it more suitable for scalability (Collobert et al., 2006; Ong and An, 2012).\nWhile learning a reject option classifier as well, one has to deal with the ambiguity in the classification due to overlapping class regions as well as the presence of outliers. This motivates us to generalize the ramp loss function such that it incorporates a different loss value for the rejection region. To accomplish this, we propose a new loss function which we call double ramp loss (LDR). LDR forms a continuous non-convex upper bound for L0\u2212d\u22121 and takes care of many of the drawbacks of convex loss functions (LGH and LDH). To learn a reject option classifier, we minimize the regularized risk under double ramp loss which becomes an instance of difference of convex (DC) functions. To minimize such a DC function, we use difference of convex programming approach (An and Tao, 1997), which essentially solves a sequence of convex programs. Our approach can be easily kernelized for dealing with nonlinear problems.\nThe main contribution of our paper is a novel formulation for the problem of learning a classifier with a reject option. The proposed coupled formulation is better compared to the existing coupled approaches in following ways: (1) the proposed loss function LDR gives a tighter upper bound to the 0-d-1 loss function, (2) LDR requires no constraint on \u03c1 (width of the rejection region) unlike LGH and LDH, (3) the proposed algorithm based on minimizing risk under LDR results in smaller number of support vectors.\nThe rest of the paper is organized as follows. In Section 2 we define the double ramp loss function (LDR) and discuss its properties. Then we discussed the proposed formulation based on risk minimization under LDR. In Section 3 we derive the algorithm for learning reject option classifier based on regularized risk minimization under (LDR) using DC programming. We present experimental results in Section 4. We conclude the paper with the discussion and insights for future work in Section 5.\n2. Proposed Approach\nOur approach for learning classifier with reject option is based on minimizing regularized risk under the double ramp loss function.\n2.1. Double Ramp Loss\nWe define double ramp loss function as a continuous upper bound for L0\u2212d\u22121. This loss function is defined as a sum of two ramp loss functions as follows:\nLDR(f(x), \u03c1, y) = d\n\u00b5\n[\n[ \u00b5\u2212 yf(x) + \u03c1 ]\n+ \u2212\n[ \u2212 \u00b52 \u2212 yf(x) + \u03c1 ]\n+\n]\n+ (1\u2212 d)\n\u00b5\n[\n[ \u00b5\u2212 yf(x)\u2212 \u03c1 ] + \u2212 [ \u2212 \u00b52 \u2212 yf(x)\u2212 \u03c1 ] +\n]\n(6)\nwhere [a]+ = max(0, a). Here \u00b5 > 0 defines the slope of ramps in the loss function. In this paper, we take \u00b5 \u2208 (0, 1]. d \u2208 (0, .5) is the cost of rejection and \u03c1 \u2265 0 is the parameter which defines the size of the rejection region around the classification boundary f(x) = 0.1\n1. While LDR is parametrized by \u00b5 and d as well, we omit them for the sake of notational consistency.\nd=0.2, r=2\nAs in L0\u2212d\u22121, LDR also considers the region [\u2212\u03c1, \u03c1]as rejection region. Figure 2 shows LDR (double ramp) loss for d = 0.2 and \u03c1 = 2 for different values of \u00b5.\nTheorem 1 (i) LDR \u2265 L0\u2212d\u22121, \u2200\u00b5 > 0, \u03c1 \u2265 0. (ii) lim\u00b5\u21920 LDR(f(x), \u03c1, y) = L0\u2212d\u22121(f(x), \u03c1, y). (iii) In the rejection region yf(x) \u2208 (\u03c1 \u2212 \u00b52,\u2212\u03c1 + \u00b5), the loss remains constant, that is LDR(f(x), \u03c1, y) = d(1 + \u00b5). (iv) For \u00b5 > 0, LDR \u2264 (1 + \u00b5), \u2200\u03c1 \u2265 0, \u2200d \u2265 0. (v) When \u03c1 = 0, LDR is same as \u00b5-ramp loss used for classification problems without rejection option. (vi) LDR is a non-convex function of (yf(x), \u03c1).\nThe proof of Theorem 1 is provided in the supplementary material in Appendix A. We see that LDR does not put any restriction on \u03c1 for it to be an upper bound of L0\u2212d\u22121. Moreover, when \u03c1 = 0, LDR behaves same as the usual ramp loss used for classification without rejection. Thus, LDR is a general ramp loss function which also allows rejection option.\n2.2. Formulation Using LDR\nLet S = {(xn, yn), n = 1 . . . N} be the training dataset, where xn \u2208 R p, yn \u2208 {\u22121,+1}, \u2200n. As discussed, we minimize regularized risk under LDR to find a reject option classifier. In this paper, we use l2 regularization. Thus, for f(x) = (w T\u03c6(x) + b), regularized risk under\nramp loss is\nR(w, b, \u03c1) = 1\n2 ||w||2 + C\nN \u2211\nn=1\nLDR(yn,w T\u03c6(xn) + b)\n= 1\n2 ||w||2 +\nC\n\u00b5\nN \u2211\nn=1\n{\nd [ \u00b5\u2212 yn(w T\u03c6(xn) + b) + \u03c1 ]\n+ \u2212 d\n[\n\u2212 \u00b52 \u2212 yn(w T\u03c6(xn) + b) + \u03c1\n]\n+\n+(1\u2212 d) [ \u00b5\u2212 yn(w T\u03c6(xn) + b)\u2212 \u03c1 ]\n+ \u2212 (1\u2212 d)\n[\n\u2212 \u00b52 \u2212 yn(w T\u03c6(xn) + b)\u2212 \u03c1\n]\n+\n}\n(7)\nwhere C is regularization parameter. In our approach we learn the parameter \u03c1 along with (w, b). C, d and \u00b5 are kept as user defined parameters. The method for solving this formulation is described in Section 3.\n3. Solution methodology\nR(w, b, \u03c1) (equation (7)) is a nonconvex function of (w, b, \u03c1). However, R(w, b, \u03c1) can be decomposed as a difference of convex functions as follows:\nR(w, b, \u03c1) = 1\n2 ||w||2 +\nC\n\u00b5\nN \u2211\nn=1\nd [ \u00b5\u2212 yn(w T\u03c6(xn) + b) + \u03c1 ]\n+ + (1\u2212 d)\n[\n\u00b5\u2212 yn(w T\u03c6(xn) + b)\u2212 \u03c1\n]\n+\n\u2212d [ \u2212 \u00b52 \u2212 yn(w T\u03c6(xn) + b) + \u03c1 ]\n+ + (1\u2212 d)\n[\n\u2212 \u00b52 \u2212 yn(w T\u03c6(xn) + b)\u2212 \u03c1\n]\n+\n]\n= R1(w, b, \u03c1) \u2212R2(w, b, \u03c1)\nwhere\nR1(w, b, \u03c1) = 1\n2 ||w||2 +\nC\n\u00b5\nN \u2211\nn=1\n[\nd [ \u00b5\u2212 yn(w T\u03c6(xn) + b) + \u03c1 ]\n+ + (1\u2212 d)\n[\n\u00b5\u2212 yn(w T\u03c6(xn) + b)\u2212 \u03c1\n]\n+\n]\nR2(w, b, \u03c1) = C\n\u00b5\nN \u2211\nn=1\n[\nd [ \u2212 \u00b52 \u2212 yn(w T\u03c6(xn) + b) + \u03c1 ]\n+ + (1\u2212 d)\n[\n\u2212 \u00b52 \u2212 yn(w T\u03c6(xn) + b)\u2212 \u03c1\n]\n+\n]\nWe can easily verify that both R1 and R2 are convex functions of (w, b, \u03c1). Thus R is an instance of difference of two convex (DC) function. We develop our algorithm which exploits this structure of R(w, b, \u03c1) using DC programs. We will describe DC programming in Section 3.1.\n3.1. Difference of Convex Programming\nWhen a nonconvex function is represented as a difference of two convex functions, finding the local optima of the nonconvex function becomes computationally simpler (An and Tao, 1997). A DC optimization problem is (An and Tao, 1997),\nmin \u0398 R(\u0398) = min \u0398 R1(\u0398)\u2212R2(\u0398)\nwhereR1(\u0398) andR2(\u0398) are convex functions of \u0398. In the simplified DC algorithm (An and Tao, 1997), an upper bound of R(\u0398) is found using the convexity property of R2(\u0398) as follows.\nR(\u0398) = R1(\u0398)\u2212R2(\u0398)\n\u2264 R1(\u0398)\u2212R2(\u0398 (l))\u2212 (\u0398\u2212\u0398(l))T\u2207R2(\u0398 (l)) =: ub(\u0398,\u0398(l))\nwhere \u0398(l) is the parameter vector after (l) th iteration \u2207R2(\u0398 (l)) is a subgradient of R2 at \u0398(l) and ub(\u0398,\u0398(l)) is upper bound to R(\u0398) after (l)th iteration . Now \u0398(l+1) is found by minimizing ub(\u0398,\u0398(l)). Thus,\nR(\u0398(l+1)) \u2264 ub(\u0398(l+1),\u0398(l)) \u2264 ub(\u0398(l),\u0398(l)) = R(\u0398(l))\nAlgorithm 1: DC Algorithm for Minimizing Rreg(\u0398)\nInitialize \u0398(0); repeat\n\u0398(l+1) = argmin \u0398 R1(\u0398)\u2212\u0398 T\u2207R2(\u0398 (l))\nuntil convergence of \u0398(l) ;\n3.2. Learning Reject Option Classifier Using DC Programming\nIn this section, we will derive a DC algorithm for learning rejection option classifier. The DC algorithm will minimize the regularized risk R(w, b, \u03c1) described in the previous section.\nLet \u0398 = [wT b \u03c1]T . We initialize with \u0398 = \u0398(0). For any l \u2265 0, we find ub(\u0398,\u0398(l)) as an upper bound for R(\u0398) (see equation (8)) as follows:\nub(\u0398,\u0398(l)) = R1(\u0398)\u2212R2(\u0398 (l))\u2212 (\u0398 \u2212\u0398(l))T\u2207R2(\u0398 (l))\nGiven \u0398(l), we find \u0398(l+1) by minimizing the upper bound ub(\u0398,\u0398(l)).\n\u0398(l+1) \u2208 argmin \u0398 R1(\u0398)\u2212\u0398 T\u2207R2(\u0398 (l)) (8)\nwhere \u2207R2(\u0398 (l)) is the subgradient of R2(\u0398) at \u0398 (l). Here, we choose \u2207R2(\u0398 (l)) as follows:\n\u2207R2(\u0398 (l)) =\nC\n\u00b5\n[ d \u2211\nxn\u2208V (l) 1\n[\u2212yn\u03c6(xn) T \u2212 yn 1] T + (1\u2212 d) \u2211\nxn\u2208V (l) 2\n[\u2212yn\u03c6(xn) T \u2212 yn \u2212 1]\nT ]\nwhere {\nV (l) 1 = {xn | yn(\u03c6(xn) Tw(l) + b(l))\u2212 \u03c1(l) < \u2212\u00b52} V (l) 2 = {xn | yn(\u03c6(xn) Tw(l) + b(l)) + \u03c1(l) < \u2212\u00b52} (9)\nWe rewrite the upper bound minimization problem described in equation (8) as follows,\nP(l+1) = min \u0398 R1(\u0398)\u2212\u0398 T\u2207R2(\u0398 (l))\n= min w,b,\u03c1\u22650\n1 2 ||w||2 + C \u00b5\nN \u2211\nn=1\n[\nd [ \u00b5\u2212 yn(w T\u03c6(xn) + b) + \u03c1 ]\n+ + (1\u2212 d)\n[\n\u00b5\u2212 yn(w T\u03c6(xn) + b)\u2212 \u03c1\n]\n+\n]\n+ C\n\u00b5\n[\nd \u2211\nxn\u2208V (l) 1\n[yn(w T\u03c6(xn) + b)\u2212 \u03c1] + (1\u2212 d)\n\u2211\nxn\u2208V (l) 2\n[yn(w T\u03c6(xn) + b) + \u03c1]\n]\nNote that P(l+1) is a convex optimization problem where the optimization variables are (w, b, \u03c1). We can rewrite P(l+1) as\nmin w,b,\u03be \u2032 ,\u03be \u2032\u2032 ,\u03c1\u22650\n1 2 ||w||2 + C \u00b5\nN \u2211\nn=1\n[\nd\u03be\u2032n + (1\u2212 d)\u03be \u2032\u2032 n\n] + Cd\n\u00b5\n\u2211\nxn\u2208V (l) 1\n[yn(w T\u03c6(xn) + b)\u2212 \u03c1]\n+ C(1\u2212 d)\n\u00b5\n\u2211\nxn\u2208V (l) 2\n[yn(w T\u03c6(xn) + b) + \u03c1]\ns.t. yn(w T\u03c6(xn) + b) \u2265 \u03c1+ \u00b5\u2212 \u03be \u2032 n, \u2200n\nyn(w T\u03c6(xn) + b) \u2265 \u2212\u03c1+ \u00b5\u2212 \u03be \u2032\u2032 n, \u2200n \u03be\u2032n \u2265 0, \u03be \u2032 n \u2265 0, \u2200n\nwhere \u03be\u2032 = [\u03be\u20321 \u03be \u2032 2 . . . \u03be \u2032 N ] T and \u03be\u2032\u2032 = [\u03be\u2032\u20321 \u03be \u2032\u2032 2 . . . \u03be \u2032\u2032 N ] T . The dual optimization problem D(l+1) of P(l+1) is as follows.2\nD(l+1) = min \u03b2 \u2032 ,\u03b2 \u2032\u2032\n1\n2\nN \u2211\nn=1\nN \u2211\nm=1\nynym(\u03b2 \u2032 n + \u03b2 \u2032\u2032 n)(\u03b2 \u2032 m + \u03b2 \u2032\u2032 m)k(xn,xm)\u2212 \u00b5\nN \u2211\nn=1\n(\u03b2\u2032n + \u03b2 \u2032\u2032 n)\ns.t. 0 \u2264 \u03b2\u2032n \u2264 Cd\n\u00b5 \u2200xn \u2208 S \\ V\n(l) 1 ; \u03b2 \u2032 n = 0 \u2200xn \u2208 V (l) 1\n0 \u2264 \u03b2\u2032\u2032n \u2264 C(1\u2212 d)\n\u00b5 \u2200xn \u2208 S \\ V\n(l) 2 ; \u03b2 \u2032\u2032 n = 0 \u2200xn \u2208 V (l) 2\nN \u2211\nn=1\nyn(\u03b2 \u2032 n + \u03b2 \u2032\u2032 n) = 0;\nN \u2211\nn=1\n\u03b2\u2032n \u2265\nN \u2211\nn=1\n\u03b2\u2032\u2032n (10)\nwhere \u03b2\u2032 = [\u03b2\u20321 \u03b2 \u2032 2 . . . . . . \u03b2 \u2032 n] T and \u03b2\u2032\u2032 = [\u03b2\u2032\u20321 \u03b2 \u2032\u2032 2 . . . . . . \u03b2 \u2032\u2032 n] T are dual variables. Using the KKT conditions of optimality for P(l+1), normal vector w is represented as:\nw =\nN \u2211\nn=1\nyn(\u03b2 \u2032 n + \u03b2 \u2032\u2032 n)\u03c6(xn)\nSince P(l+1) is a convex optimization problem with quadratic objective function and linear constraints, it holds strong duality with its dual optimization problem D(l+1). Solving the dual becomes more useful as it can be easily kernelized for non-linear problems.\n2. The KKT conditions for P(l+1) and the derivation of D(l+1) is provided in the supplementary material (Appendix B).\n3.3. Behavior of Dual Variables \u03b2\u2032 and \u03b2\u2032\u2032\nThe KKT conditions for optimality give the following insights about the dual variables. For any xn, only one of \u03b2 \u2032 n and \u03b2 \u2032\u2032 n can be nonzero. We observe that the orientation (w) and distance between the two parallel hyperplanes (wTx + b = \u2212\u03c1 and wTx + b = \u03c1) are determined by the points close to these hyperplanes. In other words, points whose margin (yf(x)) is in the range [\u03c1\u2212 \u00b52, \u03c1 + \u00b5] \u222a [\u2212\u03c1\u2212 \u00b52,\u2212\u03c1+ \u00b5]. We call these points as support vectors. This is in line with our insight that the coupled scheme treats the solution as a pair of parallel hyperplanes rather than one hyperplane with a band around it.\nWe also see that for all points whose margin (yf(x)) falls in the region (\u03c1 + \u00b5,\u221e) \u222a (\u2212\u03c1+\u00b5, \u03c1\u2212\u00b52)\u222a (\u2212\u221e,\u2212\u03c1\u2212\u00b52), both \u03b2\u2032n and \u03b2 \u2032\u2032 n are zero. Thus, points which are correctly classified with margin at least (\u03c1 + \u00b5), points falling close to the decision boundary with margin in the interval (\u2212\u03c1+\u00b5, \u03c1\u2212\u00b52) and points which are misclassified with a high negative margin (less than \u2212\u03c1\u2212\u00b52), are not considered in the final classifier. Thus, our approach not only rejects points which fall in the overlapping region of two classes, it is also unaffected by potential outliers.\nWe illustrate this insight through experiments on a synthetic dataset. The dataset is shown in Figure 3. 400 points are uniformly sampled from the square region [0 1]\u00d7 [0 1]. We consider the diagonal passing through the origin as the separating surface and assign labels {\u22121,+1} to all the points using it. We changed the labels of 80 points inside the band (width=0.225) around the separating surface.\nFigure 4(a) shows the reject option classifier learnt using the proposed method. We see that the proposed approach learns the rejection region accurately. We also observe that the number of support vectors is small (87) and all of them are near the two parallel hyperplanes. This is in accordance with our discussion on the properties of the proposed method. On the other hand, the decoupled approach finds a rejection region that is less accurate as shown in Figure 4(b). Also, the number of support vectors with the decoupled approach is much larger (192) than that with the proposed method.\n3.4. Finding b(l+1) and \u03c1(l+1)\nThe dual optimization problem above gives dual variables \u03b2(l+1) using which the normal vector is found as w(l+1) = \u2211N\nn=1(\u03b2 \u2032(l+1) n + \u03b2 \u2032\u2032(l+1) n )yn\u03c6(xn). To find b (l+1) and \u03c1(l+1), we\nData with Label Noise\nconsider xn \u2208 SV (l+1) 1 \u222a SV (l+1) 2 , where\nSV (l+1) 1 = {xn | yn(\u03c6(xn) Tw(l+1) + b(l+1)) = \u03c1(l+1) + \u00b5}\nSV (l+1) 2 = {xn | yn(\u03c6(xn) Tw(l+1) + b(l+1)) = \u2212\u03c1(l+1) + \u00b5}\nWe also observe that\n1. If xn \u2208 SV (l+1) 1 , then \u03b2 \u2032(l+1) n \u2208 (0, Cd \u00b5 ) and \u03b2 \u2032\u2032(l+1) n = 0\n2. If xn \u2208 SV (l+1) 2 , then \u03b2 \u2032(l+1) n = 0 and \u03b2 \u2032\u2032(l+1) n \u2208 (0, C(1\u2212d) \u00b5 )\nWe solve the system of linear equations corresponding to sets SV (l+1) 1 and SV (l+1) 2 for identifying b(l+1) and \u03c1(l+1).\n3.5. Summary of the Algorithm\nThus, our algorithm for learning classifier with reject option is as follows. We fix d \u2208 (0, .5), \u00b5 \u2208 (0, 1] and C and initialize the parameter vector \u0398 as \u0398(0). We now find sets V (0) 1 and V (0) 2 (see equation (9)) using \u0398 (0). We use V (0) 1 , V (0) 2 to solve the optimization problem D (1). Using dual variable we find the normal vector w(1) as w(1) = \u2211N\nn=1 yn(\u03b2 \u2032(1) n + \u03b2 \u2032\u2032(1) n )\u03c6(xn).\nWe find b(1) and \u03c1(1) as described in Section 3.4. This will give us new parameter vector \u0398(1). Using \u0398(1), we now find V (1) 1 and V (1) 2 . Like this keep repeating these two steps until there is no significant decrement in R(w, b, \u03c1). More formal description of our algorithm is given in Algorithm 2.\n4. Experimental Results\nIn this section we will compare our approach with a decoupled approach in which the base classifier is learnt using SVM. We could not find sufficient experimental results in the literature for coupled reject option classifiers (see Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008); Grandvalet et al. (2008)). Therefore, we have only been able to provide some illustrative results on comparison with the decoupled approach.\n4.1. Dataset Description\nWe report experimental results on two datasets taken from UCIML repository (Bache and Lichman, 2013), which are described in Table 3.\nAlgorithm 2: Learning Classifier with Rejection Option\nInput: d \u2208 (0, .5), \u00b5 \u2208 (0, 1], C > 0, S Output: w\u2217, b\u2217, \u03c1\u2217\nbegin\n1. Initialize w(0), b(0), \u03c1(0), l = 0.\n2. Find V (l) 1 and V (l) 2 as\nV (l) 1 = {xn | yn(\u03c6(xn) Tw(l) + b(l))\u2212 \u03c1(l) < \u2212\u00b52} V (l) 2 = {xn | yn(\u03c6(xn) Tw(l) + b(l)) + \u03c1(l) < \u2212\u00b52}\n3. Find \u03b2\u2032(l+1),\u03b2\u2032\u2032(l+1) \u2208 argmin\u03b2\u2032,\u03b2\u2032\u2032 D (l+1), where D(l+1) is described in Eq. (10).\n4. Find w(l+1) = \u2211N n=1 yn(\u03b2 \u2032(l+1) n + \u03b2 \u2032\u2032(l+1) n )\u03c6(xn).\n5. Find b(l+1) and \u03c1(l+1) by solving system of linear equation corresponding to sets\nSV (l+1) 1 and SV (l+1) 2 , where\nSV (l+1) 1 = {xn | yn(\u03c6(xn) Tw(l+1) + b(l+1)) = \u03c1(l+1) + \u00b5} SV (l+1) 2 = {xn | yn(\u03c6(xn) Tw(l+1) + b(l+1)) = \u2212\u03c1(l+1) + \u00b5}\n6. Repeat steps 2-5 until SV (l) 1 = SV (l+1) 1 and SV (l) 2 = SV (l+1) 2\n7. Return (w\u2217, b\u2217, \u03c1\u2217) = (w(l+1), b(l+1), \u03c1(l+1)), when SV (l) 1 = SV (l+1) 1 and\nSV (l) 2 = SV (l+1) 2\n4.2. Experimental Setup\nWe implement our approach in R. For solving the dual D(l) at every iteration, we have used the kernlab package (Karatzoglou et al., 2004) in R. In our experiments, we have used linear kernel for all the datasets. Our approach has 3 user defined parameters, \u00b5 \u2208 (0, 1] for the slope of the ramps, d \u2208 (0, 0.5) as the loss for rejection and C as the regularization parameter. In all our experiments, we fix the value of \u00b5 to 1. We find the values of C by 10-fold cross validation. because the base SVM classifier for the decoupled approach is learnt without influence of d, the value of C obtained by 10-fold CV is same across all values\nof d. However, for the coupled approach, we obtain the optimal value of C separately for each value of d.\nFor every dataset, we report results for values of d in the interval [0.05 .5] with the step size of 0.05. For every value of d, we find the 10-fold cross validation % error (under L0\u2212d\u22121 loss), % accuracy over the non-rejected examples (Acc), % rejection rate (RR) and number of support vectors (nSV).\n4.3. Simulation Results\nWe now discuss the experimental results. Table 4-5 show the experimental results on all the datasets. We observe the following:\n1. The reject option classifier learnt using the proposed method typically requires a smaller number of support vectors compared to the decoupled approach. When there is label noise around the separating hyperplane (which can also happen due to the overlapping class conditional densities), our approach tries to approximate this noisy region as the rejection region in between two parallel hyperplanes. Our approach ignores (1) points in the interval (\u2212\u03c1 + \u00b5, \u03c1 \u2212 \u00b52) (in the rejection band) and (2) points in the interval (\u2212\u221e,\u2212\u03c1\u2212\u00b52) (points misclassified with a high negative margin), while forming the classifier. Thus, these points do not become support vectors. And hence the proposed approach is able to learn reject option classifiers having sparse representation.\nOn the other hand, these noisy points would become hard to classify using standard SVM classifier and hence would become support vectors. Thus SVM classifier based decoupled approach would results in more number of support vectors.\n2. Most of the time, the average rejection rate of the proposed method is smaller than the decoupled approach, while the average accuracy on the non-rejected examples is comparable. This is also expected because the constant penalty that the double ramp loss puts on the points misclassified with high negative margin.\n3. We see that the average loss achieved by our approach is comparable to the SVM based decoupled approach. In principle, it seems intuitive that optimizing the orientation and rejection bandwidth of a separating hyperplane together (I.e., coupled approach) is likely to arrive at a lower empirical risk value as compared to optimizing the orientation of the separating hyperplane (assuming no rejection) and then optimizing the rejection bandwidth given this orientation (I.e., decoupled approach). However, it is also intuitive that this is a tougher optimization problem to solve, and results may be comparable in practice.\n5. Conclusion and Future Work\nIn this paper, we have proposed a new loss function LDR (double ramp loss) for learning the reject option classifier. Our approach learns the classifier by minimizing the regularized risk under the double ramp loss function which becomes an instance of DC optimization problem. Our approach can also learn nonlinear classifiers by using appropriate kernel\nfunction. Experimentally we have shown that our approach works comparable to SVM based decoupled approach for learning reject option classifier.\nWe have seen that LDR is attractive because it gives a better upper bound for L0\u2212d\u22121 compared to convex losses LDH and LGH. It would be useful to show that classifier learnt using LDR is Bayes consistent. Deriving the generalization bounds for the true risk based on (LDR) is also another direction of research.\nAppendix A. Proof of Theorem 1\nProof\nLDR(f(x), \u03c1, y) = d\n\u00b5\n[\n[ \u00b5\u2212 yf(x) + \u03c1 ]\n+ \u2212\n[ \u2212 \u00b52 \u2212 yf(x) + \u03c1 ]\n+\n]\n+ (1\u2212 d)\n\u00b5\n[\n[ \u00b5\u2212 yf(x)\u2212 \u03c1 ] + \u2212 [ \u2212 \u00b52 \u2212 yf(x)\u2212 \u03c1 ] +\n]\n\u2022 (i) We need to show that LDR \u2265 L0\u2212d\u22121 for all values of \u00b5 > 0 and \u03c1 \u2265 0. We can see this case by case.\n\u2022 (ii) We need to show that lim\u00b5\u21920 LDR(f(x), \u03c1, y) = L0\u2212d\u22121(f(x), \u03c1, y). We first see the functional form of LDR in different intervals.\nNow we take the limit \u00b5 \u2192 0, which is shown in Table 8. We see that lim\u00b5\u21920 LDR = L0\u2212d\u22121.\n\u2022 (iii) In the rejection region yf(x) \u2208 (\u03c1\u2212 \u00b52,\u2212\u03c1+ \u00b5), the loss remains constant, that is LDR(f(x), \u03c1, y) = d(1 + \u00b5). This can be seen in Table 7.\n\u2022 (iv) For \u00b5 > 0, LDR \u2264 (1 + \u00b5), \u2200\u03c1 \u2265 0, \u2200d \u2265 0. This can be seen in Table 7.\n\u2022 (v) When \u03c1 = 0, LDR becomes\nLDR(f(x), 0, y) = d\n\u00b5\n[\n[ \u00b5\u2212 yf(x) ]\n+ \u2212\n[ \u2212 \u00b52 \u2212 yf(x) ]\n+\n] + (1\u2212 d)\n\u00b5\n[\n[ \u00b5\u2212 yf(x)\u2212 ]\n+\n\u2212 [ \u2212 \u00b52 \u2212 yf(x) ]\n+\n]\n= 1\n\u00b5\n[\n[ \u00b5\u2212 yf(x) ]\n+ \u2212\n[ \u2212 \u00b52 \u2212 yf(x) ]\n+\n]\nwhich is same as the \u00b5-ramp loss function used for classification problems without rejection option.\n\u2022 (vi) We have to show that LDR is non-convex function of (yf(x), \u03c1). From (iv), we know that LDR \u2264 (1 + \u00b5). That is, LDR is bounded above. We show non-convexity of LDR by contradiction.\nLet LDR be convex function of (yf(x), \u03c1). Let z = (yf(x), \u03c1). We also rewrite LDR(f(x), \u03c1, y) as LDR(z). We choose two points z1, z2 such that LDR(z1) > LDR(z2). Thus, from the definition of convexity, we have\nLDR(z1) \u2264 \u03bbLDR( z1 \u2212 (1\u2212 \u03bb)z2\n\u03bb ) + (1\u2212 \u03bb)LDR(z2) \u2200\u03bb \u2208 (0, 1)\nHence, LDR(z1)\u2212 (1\u2212 \u03bb)LDR(z2)\n\u03bb \u2264 LDR( z1 \u2212 (1\u2212 \u03bb)z2 \u03bb )\nNow, since LDR(z1) > LDR(z2),\nLDR(z1)\u2212 (1\u2212 \u03bb)LDR(z2)\n\u03bb =\nLDR(z1)\u2212 LDR(z2)\n\u03bb + LDR(z2) \u2192 \u221e as \u03bb \u2192 0\n+\nThus lim\u03bb\u21920+ LDR( z1\u2212(1\u2212\u03bb)z2 \u03bb ) = \u221e. But LDR is upper bounded by (1 + \u00b5)d. This contradicts that LDR is convex.\nAppendix B. Derivation of Dual Optimization Problem D(l+1)\nP(l+1) : minw,b 1\n2 ||w||2 +\nC\n\u00b5\nN \u2211\nn=1\n[\nd\u03be\u2032n + (1\u2212 d)\u03be \u2032\u2032 n\n]\n+ C\n\u00b5\n[\nd \u2211\nxn\u2208V (l) 1\n[yn(w T\u03c6(xn) + b)\u2212 \u03c1] + (1\u2212 d)\n\u2211\nxn\u2208V (l) 2\n[yn(w T\u03c6(xn) + b) + \u03c1]\n]\ns.t. yn(w T\u03c6(xn) + b) \u2265 \u03c1+ \u00b5\u2212 \u03be \u2032 n, \u03be \u2032 n \u2265 0, n = 1 . . . N\nyn(w T\u03c6(xn) + b) \u2265 \u2212\u03c1+ \u00b5\u2212 \u03be \u2032\u2032 n, \u03be \u2032\u2032 n \u2265 0, n = 1 . . . N \u03c1 \u2265 0\nThe Lagrangian for above problem will be:\nL = 1\n2 ||w||2 +\nC\n\u00b5\nN \u2211\nn=1\n[\nd\u03be\u2032n + (1\u2212 d)\u03be \u2032\u2032 n\n] + C\n\u00b5\n[\nd \u2211\nxn\u2208V (l) 1\n[yn(w T\u03c6(xn) + b)\u2212 \u03c1] +\n(1\u2212 d) \u2211\nxn\u2208V (l) 2\n[yn(w T\u03c6(xn) + b) + \u03c1]\n] + N \u2211\nn=1\n\u03b1\u2032n[\u03c1+ \u00b5\u2212 \u03be \u2032 n \u2212 yn(w T\u03c6(xn) + b)]\n+\nN \u2211\nn=1\n\u03b1\u2032\u2032n[\u2212\u03c1+ \u00b5\u2212 \u03be \u2032\u2032 n \u2212 yn(w T\u03c6(xn) + b)]\u2212\nN \u2211\nn=1\n\u03b7\u2032n\u03be \u2032 n \u2212\nN \u2211\nn=1\n\u03b7\u2032\u2032n\u03be \u2032\u2032 n \u2212 \u03b2\u03c1\nwhere \u03b1\u2032n is dual variable corresponding to constraint yn(w T\u03c6(xn) + b) \u2265 \u03c1 + \u00b5 \u2212 \u03be \u2032 n, \u03b1 \u2032\u2032 n is dual variable corresponding to yn(w T\u03c6(xn) + b) \u2265 \u2212\u03c1 + \u00b5 \u2212 \u03be \u2032 n, \u03b7 \u2032 n is dual variable corresponding to \u03be\u2032n \u2265 0, \u03b7 \u2032\u2032 n is dual variable corresponding to \u03be \u2032\u2032 n \u2265 0 and \u03b2 is dual variable corresponding to \u03c1 \u2265 0. We take the gradient of Lagrangian with respect to the primal variables. By equating the gradient to zero, we get the KKT conditions of optimality for this optimization problem.\n\n                    \n                    \nw = \u2211N n=1 yn(\u03b1 \u2032 n + \u03b1 \u2032\u2032 n)\u03c6(xn)\u2212 C \u00b5\n[\nd \u2211\nxn\u2208V (l) 1\nyn\u03c6(xn) + (1\u2212 d) \u2211\nxn\u2208V (l) 2\nyn\u03c6(xn) ]\n\u2211N n=1 yn(\u03b1 \u2032 n + \u03b1 \u2032\u2032 n)\u2212 C \u00b5\n[\nd \u2211\nxn\u2208V (l) 1\nyn + (1\u2212 d) \u2211\nxn\u2208V (l) 2\nyn\n]\n= 0\n\u03b7\u2032n + \u03b1 \u2032 n = Cd \u00b5\n\u2200 n = 1 . . . N\n\u03b7\u2032\u2032n + \u03b1 \u2032\u2032 n = C(1\u2212d) \u00b5\n\u2200 n = 1 . . . N C \u00b5 [ \u2212 d|V (l) 1 |+ (1\u2212 d)|V (l) 2 | ] + \u2211N n=1(\u03b1 \u2032 n \u2212 \u03b1 \u2032\u2032 n)\u2212 \u03b2 = 0 \u03b7\u2032n\u03be \u2032 n = 0, \u03b7 \u2032 n \u2265 0 \u2200 n = 1 . . . N \u03b7\u2032\u2032n\u03be \u2032\u2032 n = 0, \u03b7 \u2032\u2032 n \u2265 0 \u2200 n = 1 . . . N \u03b1\u2032n[\u00b5\u2212 \u03be \u2032 n \u2212 yn(w T\u03c6(xn) + b) + \u03c1] = 0, \u03b1 \u2032 n \u2265 0 \u2200 n = 1 . . . N \u03b1\u2032\u2032n[\u00b5\u2212 \u03be \u2032\u2032 n \u2212 yn(w T\u03c6(xn) + b)\u2212 \u03c1] = 0, \u03b1 \u2032\u2032 n \u2265 0 \u2200 n = 1 . . . N \u03b2\u03c1 = 0, \u03b2 \u2265 0\nwhere |V (l) 1 | and |V (l) 2 | denote cardinality of sets V (l) 1 and V (l) 2 . KKT conditions of optimality described above give following insights about the dual variables.\nyn(w T\u03c6(xn) + b) > \u03c1+ \u00b5 \u21d2 \u03b1 \u2032 n = 0, \u03b1 \u2032\u2032 n = 0\nyn(w T\u03c6(xn) + b) = \u03c1+ \u00b5 \u21d2 \u03b1 \u2032 n \u2208 (0,\nCd\n\u00b5 ), \u03b1\u2032\u2032n = 0\n\u2212\u03c1+ \u00b5 < yn(w T\u03c6(xn) + b) < \u03c1+ \u00b5 \u21d2 \u03b1 \u2032 n =\nCd\n\u00b5 , \u03b1\u2032\u2032n = 0\nyn(w T\u03c6(xn) + b) = \u2212\u03c1\u2212 \u00b5 2 \u21d2 \u03b1\u2032n = Cd\n\u00b5 , \u03b1\u2032\u2032n \u2208 (0,\nC(1\u2212 d)\n\u00b5 )\nyn(w T\u03c6(xn) + b) < \u2212\u03c1\u2212 \u00b5 2 \u21d2 \u03b1\u2032n = Cd\n\u00b5 , \u03b1\u2032\u2032n =\nC(1\u2212 d)\n\u00b5\nWe make the dual optimization problem simpler by changing the variables in following way:\n\n    \n    \n\u03b2\u2032n = \u03b1 \u2032 n, \u2200xn \u2208 S \\ V (l) 1 \u03b2\u2032n = \u03b1 \u2032 n \u2212 Cd \u00b5 = 0, \u2200xn \u2208 V (l) 1 \u03b2\u2032\u2032n = \u03b1 \u2032\u2032 n, \u2200xn \u2208 S \\ V (l) 2 \u03b2\u2032\u2032n = \u03b1 \u2032\u2032 n \u2212 C(1\u2212d) \u00b5 = 0, \u2200xn \u2208 V (l) 2\nBy changing these variables, the new KKT conditions in terms of \u03b2\u2032 and \u03b2\u2032\u2032 are\n\n                             \n                             \nw = \u2211N n=1 yn(\u03b2 \u2032 n + \u03b2 \u2032\u2032 n)\u03c6(xn) \u2211N n=1 yn(\u03b2 \u2032 n + \u03b2 \u2032\u2032 n) = 0 \u03b7\u2032n + \u03b2 \u2032 n = Cd \u00b5\n\u2200 xn \u2208 S \\ V (l) 1\n\u03b2\u2032n = 0 \u2200 xn \u2208 V (l) 1 \u03b7\u2032\u2032n + \u03b2 \u2032\u2032 n = C(1\u2212d) \u00b5 xn \u2208 S \\ V (l) 2 \u03b2\u2032\u2032n = 0 xn \u2208 V (l) 2 \u2211N n=1(\u03b2 \u2032 n \u2212 \u03b2 \u2032\u2032 n)\u2212 \u03b2 = 0 \u03b7\u2032n\u03be \u2032 n = 0, \u03b7 \u2032 n \u2265 0 \u2200 n = 1 . . . N \u03b7\u2032\u2032n\u03be \u2032\u2032 n = 0, \u03b7 \u2032\u2032 n \u2265 0 \u2200 n = 1 . . . N \u03b2\u2032n[\u00b5\u2212 \u03be \u2032 n \u2212 yn(w T\u03c6(xn) + b) + \u03c1] = 0, \u03b2 \u2032 n \u2265 0 \u2200 xn \u2208 S \\ V (l) 1 \u00b5\u2212 \u03be\u2032n \u2212 yn(w T\u03c6(xn) + b) + \u03c1 = 0 \u2200 xn \u2208 V (l) 1 \u03b2\u2032\u2032n[\u00b5\u2212 \u03be \u2032\u2032 n \u2212 yn(w T\u03c6(xn) + b)\u2212 \u03c1] = 0, \u03b2 \u2032\u2032 n \u2265 0 \u2200 xn \u2208 S \\ V (l) 2 \u00b5\u2212 \u03be\u2032\u2032n \u2212 yn(w T\u03c6(xn) + b)\u2212 \u03c1 = 0 \u2200 xn \u2208 V (l) 2 \u03b2\u03c1 = 0, \u03b2 \u2265 0\nThe dual optimization problem D(l+1) will become:\nD(l+1) : min \u03b2 \u2032 ,\u03b2 \u2032\u2032\n1\n2\nN \u2211\nn=1\nN \u2211\nm=1\nynym(\u03b2 \u2032 n + \u03b2 \u2032\u2032 n)(\u03b2 \u2032 m + \u03b2 \u2032\u2032 m)k(xn,xm)\u2212 \u00b5\nN \u2211\nn=1\n(\u03b2\u2032n + \u03b2 \u2032\u2032 n)\ns.t. 0 \u2264 \u03b2\u2032n \u2264 Cd\n\u00b5 \u2200xn \u2208 S \\ V\n(l) 1 ; \u03b2 \u2032 n = 0 \u2200xn \u2208 V (l) 1\n0 \u2264 \u03b2\u2032\u2032n \u2264 C(1\u2212 d)\n\u00b5 \u2200xn \u2208 S \\ V\n(l) 2 ; \u03b2 \u2032\u2032 n = 0 \u2200xn \u2208 V (l) 2\nN \u2211\nn=1\nyn(\u03b2 \u2032 n + \u03b2 \u2032\u2032 n) = 0\nN \u2211\nn=1\n\u03b2\u2032n \u2265\nN \u2211\nn=1\n\u03b2\u2032\u2032n\nwhere \u03b2\u2032 = [\u03b2\u20321 \u03b2 \u2032 2 . . . . . . \u03b2 \u2032 n] T and \u03b2\u2032\u2032 = [\u03b2\u2032\u20321 \u03b2 \u2032\u2032 2 . . . . . . \u03b2 \u2032\u2032 n] T .\nAppendix C. D(l+1) in Matrix Format\nA simple representation of D(l+1) in terms of matrices and vectors is as follows:\nD(l+1) : min\u03b2 1\n2 \u03b2TH\u03b2 + \u03b2Ta\ns.t. l \u2264 \u03b2 \u2264 u\n0 \u2264 A\u03b2 \u2264 0\nwhere \u03b2 = [\u03b2\u2032T \u03b2\u2032\u2032T ]T , H =\n(\nK \u2299 Y Y T K \u2299 Y Y T K \u2299 Y Y T K \u2299 Y Y T\n)\n, K is kernel matrix (Knm =\nk(xn,xm), Y is the label vector, \u2299 represents elementwise multiplication. a = [\u22122\u00b51 T m 0 T m] T where 1m is m dimensional column vector of ones and 0m is m dimensional column vector of zeros. l = 02m (2m dimensional column vector of zeros). u = [u T 1 u T 2 ] T , where u1 is m\u00d7 1 vector whose entries for V (l) 1 are 0 and all other entries are Cd \u00b5 . Similarly, u2 is m\u00d7 1 vector whose entries for V (l) 2 are 0 and all other entries are C(1\u2212d) \u00b5 . A = [Y T Y T ].\nReferences\nLe Thi Hoai An and Pham Dinh Tao. Solving a class of linearly constrained indefinite quadratic problems by d.c. algorithms. Journal of Global Optimization, 11:253\u2013285, 1997.\nK. Bache and M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.\nPeter L. Bartlett and Marten H. Wegkamp. Classification with a reject option using a hinge loss. Journal of Machine Learning Research, 9:1823\u20131840, June 2008.\nC. K. Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information Theory, 16(1):41\u201346, January 1970.\nRonan Collobert, Fabian Sinz, Jason Weston, and Le\u0301on Bottou. Trading convexity for scalability. In Proceedings of the 23rd International Conference on Machine Learning, pages 201\u2013208, 2006.\nGiorgio Fumera and Fabio Roli. Support vector machines with embedded reject option. In Proceedings of the First International Workshop on Pattern Recognition with Support Vector Machines, SVM \u201902, pages 68\u201382, 2002.\nYves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, and Ste\u0301phane Canu. Support vector machines with a reject option. In NIPS, pages 537\u2013544, 2008.\nRadu Herbei and Marten H. Wegkamp. Classification with reject option. The Canadian Journal of Statistics, 34(4):709\u2013721, December 2006.\nAlexandros Karatzoglou, Alex Smola, Kurt Hornik, and Achim Zeileis. kernlab \u2013 an S4 package for kernel methods in R. Journal of Statistical Software, 11(9):1\u201320, November 2004. URL http://www.jstatsoft.org/vll/i09/.\nNaresh Manwani and P. S. Sastry. Noise tolerance under risk minimization. IEEE Transactions on Systems, Man and Cybernetics: Part\u2013B, 43:1146\u20131151, March 2013.\nCheng Soon Ong and Le Thi Hoai An. Learning sparse classifiers with difference of convex functions algorithms. Optimization Methods and Software, (ahead-of-print):1\u201325, 2012.\nRamasubramanian Sundararajan and Asim K. Pal. A conservative approach to perceptron learning. WSEAS Transactions on Systems, 3, 2004.\nVladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998.\nMarten Wegkamp and Ming Yuan. Support vector machines with a reject option. Bernaulli, 17(4):1368\u20131385, 2011.\nMarten H. Wegkamp. Lasso type classifiers with a reject option. Electronic Journal of Statistics, 1:155\u2013168, 2007.\nYichao Wu and Yufeng Liu. Robust truncated hinge loss support vector machines. Journal of the American Statistical Association, pages 974\u2013983, 2007.\nMing Yuan and Marten Wegkamp. Classification methods with reject option based on convex risk minimization. Journal of Machine Learning Research, 11:111\u2013130, March 2010."}], "references": [{"title": "Trading convexity for scalability", "author": ["Ronan Collobert", "Fabian Sinz", "Jason Weston", "L\u00e9on Bottou"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Collobert et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2006}, {"title": "Support vector machines with embedded reject option", "author": ["Giorgio Fumera", "Fabio Roli"], "venue": "In Proceedings of the First International Workshop on Pattern Recognition with Support Vector Machines,", "citeRegEx": "Fumera and Roli.,? \\Q2002\\E", "shortCiteRegEx": "Fumera and Roli.", "year": 2002}, {"title": "Support vector machines with a reject option", "author": ["Yves Grandvalet", "Alain Rakotomamonjy", "Joseph Keshet", "St\u00e9phane Canu"], "venue": "In NIPS,", "citeRegEx": "Grandvalet et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Grandvalet et al\\.", "year": 2008}, {"title": "Classification with reject option", "author": ["Radu Herbei", "Marten H. Wegkamp"], "venue": "The Canadian Journal of Statistics,", "citeRegEx": "Herbei and Wegkamp.,? \\Q2006\\E", "shortCiteRegEx": "Herbei and Wegkamp.", "year": 2006}, {"title": "kernlab \u2013 an S4 package for kernel methods in R", "author": ["Alexandros Karatzoglou", "Alex Smola", "Kurt Hornik", "Achim Zeileis"], "venue": "Journal of Statistical Software,", "citeRegEx": "Karatzoglou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Karatzoglou et al\\.", "year": 2004}, {"title": "Noise tolerance under risk minimization", "author": ["Naresh Manwani", "P.S. Sastry"], "venue": "IEEE Transactions on Systems, Man and Cybernetics: Part\u2013B,", "citeRegEx": "Manwani and Sastry.,? \\Q2013\\E", "shortCiteRegEx": "Manwani and Sastry.", "year": 2013}, {"title": "Learning sparse classifiers with difference of convex functions algorithms", "author": ["Cheng Soon Ong", "Le Thi Hoai An"], "venue": "Optimization Methods and Software,", "citeRegEx": "Ong and An.,? \\Q2012\\E", "shortCiteRegEx": "Ong and An.", "year": 2012}, {"title": "A conservative approach to perceptron learning", "author": ["Ramasubramanian Sundararajan", "Asim K. Pal"], "venue": "WSEAS Transactions on Systems,", "citeRegEx": "Sundararajan and Pal.,? \\Q2004\\E", "shortCiteRegEx": "Sundararajan and Pal.", "year": 2004}, {"title": "Statistical Learning Theory", "author": ["Vladimir N. Vapnik"], "venue": null, "citeRegEx": "Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik.", "year": 1998}, {"title": "Support vector machines with a reject", "author": ["Marten Wegkamp", "Ming Yuan"], "venue": "option. Bernaulli,", "citeRegEx": "Wegkamp and Yuan.,? \\Q2011\\E", "shortCiteRegEx": "Wegkamp and Yuan.", "year": 2011}, {"title": "Lasso type classifiers with a reject option", "author": ["Marten H. Wegkamp"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Wegkamp.,? \\Q2007\\E", "shortCiteRegEx": "Wegkamp.", "year": 2007}, {"title": "Robust truncated hinge loss support vector machines", "author": ["Yichao Wu", "Yufeng Liu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Wu and Liu.,? \\Q2007\\E", "shortCiteRegEx": "Wu and Liu.", "year": 2007}, {"title": "Classification methods with reject option based on convex risk minimization", "author": ["Ming Yuan", "Marten Wegkamp"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Yuan and Wegkamp.,? \\Q2010\\E", "shortCiteRegEx": "Yuan and Wegkamp.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Since D(x, y) is generally assumed to be fixed but unknown, the empirical risk minimization principle (with its attendant caveats on minimizing complexity or structural risk) is used (Vapnik, 1998).", "startOffset": 183, "endOffset": 197}, {"referenceID": 9, "context": "L0\u2212d\u22121 in this case is described as follows (Wegkamp and Yuan, 2011; Herbei and Wegkamp, 2006):", "startOffset": 44, "endOffset": 94}, {"referenceID": 3, "context": "L0\u2212d\u22121 in this case is described as follows (Wegkamp and Yuan, 2011; Herbei and Wegkamp, 2006):", "startOffset": 44, "endOffset": 94}, {"referenceID": 3, "context": "For 2-class problems, the risk under L0\u2212d\u22121 is minimized by generalized Bayes discriminant (Herbei and Wegkamp, 2006; Chow, 1970) which is as below: f d (x) = \uf8f1", "startOffset": 91, "endOffset": 129}, {"referenceID": 12, "context": "Classifier h(f(x), \u03c1) (equation (3))is shown to be infinite sample consistent with respect to the generalized Bayes classifier f d (x) described in equation (5) (Yuan and Wegkamp, 2010).", "startOffset": 161, "endOffset": 185}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way.", "startOffset": 0, "endOffset": 52}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way. Analogous to the convex surrogates of L0\u22121, convex surrogates for L0\u2212d\u22121 also have been proposed. Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008) discuss risk minimization based on generalized hinge loss LGH (see Table 1) for learning a reject option classifier.", "startOffset": 0, "endOffset": 265}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way. Analogous to the convex surrogates of L0\u22121, convex surrogates for L0\u2212d\u22121 also have been proposed. Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008) discuss risk minimization based on generalized hinge loss LGH (see Table 1) for learning a reject option classifier.", "startOffset": 0, "endOffset": 281}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way. Analogous to the convex surrogates of L0\u22121, convex surrogates for L0\u2212d\u22121 also have been proposed. Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008) discuss risk minimization based on generalized hinge loss LGH (see Table 1) for learning a reject option classifier.", "startOffset": 0, "endOffset": 310}, {"referenceID": 1, "context": "Fumera and Roli (2002); Sundararajan and Pal (2004) have also proposed learning algorithms for rejection option classifier in the coupled way. Analogous to the convex surrogates of L0\u22121, convex surrogates for L0\u2212d\u22121 also have been proposed. Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008) discuss risk minimization based on generalized hinge loss LGH (see Table 1) for learning a reject option classifier. It is shown that a minimizer of risk under LGH is consistent to the generalized Bayes classifier f d (Bartlett and Wegkamp, 2008). Grandvalet et al. (2008) propose a risk minimization scheme under double hinge loss LDH (see Table 1) and show that resulting classifier is strongly universally consistent to the generalized Bayes classifier f d .", "startOffset": 0, "endOffset": 583}, {"referenceID": 5, "context": "However, SVM and other convex loss based classification algorithms are not robust to label noise in the data (Manwani and Sastry, 2013).", "startOffset": 109, "endOffset": 135}, {"referenceID": 11, "context": "ramp loss based risk minimization for classifier learning is more robust to noise (Wu and Liu, 2007) and gives sparse solutions compared to hinge loss based SVM.", "startOffset": 82, "endOffset": 100}, {"referenceID": 0, "context": "This makes it more suitable for scalability (Collobert et al., 2006; Ong and An, 2012).", "startOffset": 44, "endOffset": 86}, {"referenceID": 6, "context": "This makes it more suitable for scalability (Collobert et al., 2006; Ong and An, 2012).", "startOffset": 44, "endOffset": 86}, {"referenceID": 8, "context": "We could not find sufficient experimental results in the literature for coupled reject option classifiers (see Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008); Grandvalet et al.", "startOffset": 111, "endOffset": 135}, {"referenceID": 8, "context": "We could not find sufficient experimental results in the literature for coupled reject option classifiers (see Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008); Grandvalet et al.", "startOffset": 111, "endOffset": 151}, {"referenceID": 8, "context": "We could not find sufficient experimental results in the literature for coupled reject option classifiers (see Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008); Grandvalet et al.", "startOffset": 111, "endOffset": 180}, {"referenceID": 2, "context": "We could not find sufficient experimental results in the literature for coupled reject option classifiers (see Wegkamp and Yuan (2011); Wegkamp (2007); Bartlett and Wegkamp (2008); Grandvalet et al. (2008)).", "startOffset": 181, "endOffset": 206}, {"referenceID": 4, "context": "For solving the dual D(l) at every iteration, we have used the kernlab package (Karatzoglou et al., 2004) in R.", "startOffset": 79, "endOffset": 105}], "year": 2017, "abstractText": "We consider the problem of building classifiers with the option to reject i.e., not return a prediction on a given test example. Adding a reject option to a classifier is well-known in practice; traditionally, this has been accomplished in two different ways. One is the decoupled method where an optimal base classifier (without the reject option) is build first and then the rejection boundary is optimized, typically in terms of a band around the separating surface. The coupled method is based on finding both the classifier as well as the rejection band at the same time. Existing coupled approaches are based on minimizing risk under an extension of the classical 0 \u2212 1 loss function wherein a loss d \u2208 (0, .5) is assigned to a rejected example. In this paper, we propose a double ramp loss function which gives a continuous upper bound for (0 \u2212 d \u2212 1) loss described above. Our coupled approach is based on minimizing regularized risk under the double ramp loss which is done using difference of convex (DC) programming. We show the effectiveness of our approach through experiments on synthetic and benchmark datasets.", "creator": "LaTeX with hyperref package"}}}