{"id": "1312.1423", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2013", "title": "ABC-SG: A New Artificial Bee Colony Algorithm-Based Distance of Sequential Data Using Sigma Grams", "abstract": "the problem of similarity search is one of the main problems in computer science. this problem has many applications in text - element retrieval, web search, analytical computational theoretical biology, bioinformatics and others. similarity between two data objects measurements can be depicted using a similarity measure or termed a distance intensity metric. there are numerous distance metrics also in the literature, some are used broadly for a particular data type, and several others are more general. in this paper proposal we present a detailed new distance metric named for sequential data which is based on the squared sum of n - grams. the novelty of our distance is that these n - grams are weighted using artificial bee colony ; a recent optimization algorithm based on the collective intelligence of a swarm of bees on their search for nectar. this precise algorithm has been used thus in optimizing a large number of numerical problems. we validate the new scale distance experimentally.", "histories": [["v1", "Thu, 5 Dec 2013 03:19:51 GMT  (219kb)", "http://arxiv.org/abs/1312.1423v1", "The Tenth Australasian Data Mining Conference - AusDM 2012, Sydney, Australia, 5-7 December, 2012"]], "COMMENTS": "The Tenth Australasian Data Mining Conference - AusDM 2012, Sydney, Australia, 5-7 December, 2012", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["muhammad marwan muhammad fuad"], "accepted": false, "id": "1312.1423"}, "pdf": {"name": "1312.1423.pdf", "metadata": {"source": "CRF", "title": "ABC-SG: A New Artificial Bee Colony Algorithm-Based Distance of Sequential Data Using Sigma Grams", "authors": ["Muhammad Marwan", "Muhammad Fuad"], "emails": ["marwan.fuad@iet.ntnu.no"], "sections": [{"heading": "1 Introduction", "text": "Similarity search is one of the fundamental problems in computer science. It has many applications in text, video and image retrieval, pattern recognition, bioinformatics, web search, fingerprint databases, and many others. In this problem a pattern is given and the algorithm searches the database, or the web, to return all or most, depending on whether the search is exact or approximate, of the data objects that are \u201cclose\u201d to that pattern according to some semantics of closeness. This closeness between two data objects is depicted using a principal concept which is the similarity measure or its stronger form; the distance metric.\nOf the different paradigms proposed to manage the similarity search problem, the metric model with its properties (reflexivity, non-negativity, symmetry, triangle\nThis work was carried out during the tenure of an ERCIM \u201cAlain Bensoussan\u201d Fellowship Programme. This Programme is supported by the Marie-Curie Co-funding of Regional, National and International Programmes (COFUND) of the European Commission.\ninequality) stands out as one that is applicable to different data types. The distance metric on which the metric model is based is a strong mathematical tool which helps the researchers build different data structures specific to metric spaces. Other techniques, such as the pivot technique, are based on the triangle inequality; one of the axioms of the metric model. All these advantages of this model make of it a rich field of research in information retrieval.\nThe main distance used to compare two strings is the Edit Distance (ED) presented by Wagner and Fischer (1974), it is also called the Levenshtein distance, and it is defined as the minimum number of delete, insert, and change operations needed to transform string S into string T. As mentioned above, this distance is the main distance used to compare two strings. However, this distance has its limitations because it considers local similarity only.\nMuhammad Fuad and Marteau (2008a) (2008b) presented a new distance metric; The Extended Edit Distanc (EED), which they applied to symbolically represented time series. Unlike ED, EED considers a global level of similarity in additional to the local one presented by ED. EED is based on the idea of computing the frequencies of common characters between two strings. Later, Muhammad Fuad and Marteau (2008c) presented another distance, MREED, which computes the frequencies of common bi-grams in addition to common characters. However, the parameters used in these two distances (one in EED and two in MREED) were defined using very basic heuristics which, on the one hand, substantially limited the search space (it was limited to 5 values only for each parameter), and on the other hand, using such basic heuristics makes it practically impossible to extend this distance beyond that of bigrams because training time is very long even in the case of bi-grams where two parameters only are used\nIn this paper we propose a new general distance metric that applies to strings. We call it the Artificial Bee Colony-Sigma Gram Distance (ABC-SG). This distance is based on computing the sigma grams. The particularity of this distance is that it uses the artificial bee colony algorithm to set its parameters.\nThe rest of this paper is organized as follows: Section 2 is a background section, In Section 3 we present the new distance and we validate its performance in Section 4, we conclude this paper in Section 5 with some perspectives."}, {"heading": "2 Background", "text": "Muhammad Fuad and Marteau (2008a) (2008b) presented the Extended Edit Distance (EED) which is defined as follows:\nLet\u03a3 be a finite alphabet, and let *\u03a3 be the set of strings on\u03a3 . Let )S(af , )T( af be the frequency of the character a in S and T , respectively. Where S ,T are two strings in *\u03a3 . EED is defined as:\n( ) ( ) ( ) ( )( ) \u23a5 \u23a5 \u23a6\n\u23a4\n\u23a2 \u23a2 \u23a3\n\u23a1 \u2212++= \u2211\n\u03a3\u2208\nT a S a\na\nf,fminTST,SEDT,SEED 2\u03bb (1)\nWhere S , T are the lengths of the two strings TS , respectively, and where 0\u2265\u03bb ( R\u2208\u03bb ). \u03bb is called the co-occurrence frequency factor.\nEED is based on the intuition that the ED distance does not take into account whether the change operation used a character that is more \u201cfamiliar\u201d to the two strings or not, because ED considers a local level of similarity only, while EED adds to this local level of similarity a global one. This modification makes EED more intuitive as shown by Muhammad Fuad and Marteau (2008a) (2008b).\nMuhammad Fuad and Marteau (2008c) also showed that EED is a distance metric (symmetry, identity, triangle inequality). Search in metric spaces has many advantages, the most famous of which is that a single indexing structure can be applied to several kinds of queries and data types that are so different in nature. This is mainly important in establishing unifying models for the search problem that are independent of the data type. This makes metric spaces a solid structure that is able to deal with several data types as mentioned by Zezula et al. (2005)."}, {"heading": "3 The Artificial Bee Colony Sigma Gram Distance (ABC-SG)", "text": ""}, {"heading": "3.1 Definition-The Number of Distinct nGrams (NDnG)", "text": "Given two strings S ,T . The number of distinct n-grams (substrings of length n) that the two strings S and T contain is defined as:\n( ) ( ){ } ( ){ }TgramnSgramnT,SGNDn \u2212\u222a\u2212= (2)\nwhere ( )gramn \u2212 is the set of n-grams that a string consists of. Example : Given the following strings: oxygenR = , exogenS = ,\nemolenT = . The sets of n-grams for these strings are given by:\nn R S T 1 o, x, y, g, e, n e, x, o, g, e, n e, m, o, l, e, n 2 ox,xy, yg, ge, en ex,xo, og, ge, en em,mo,ol,le,en 3 oxy,xyg,yge,gen exo,xog,oge,gen emo,mol, ole, len 4 oxyg, xyge, ygen exog, xoge, ogen emol, mole, olen 5 oxyge, xygen exoge, xogen emole, molen 6 oxygen exogen emolen\nComparing ( )R,SGNDn , and ( )T,SGNDn gives:\nn NDnG(S,R) NDnG(S,T)\n1 5 4 2 2 1 3 1 0 4 0 0 5 0 0 6 0 0\n\u2211 =\n6\n1n nGND 8 5\nThe above comparison shows a greater similarity between S and R than between S and T , which is intuitive. But if we compute the edit distance we get: ( ) ( ) 2== T,SEDR,SED .\nMuhammad Fuad and Marteau (2008a) (2008b) showed how EED, which considers the frequencies of characters, can capture this intuitive similarity that ED can not capture.\nAlthough EED has advantages over ED as shown by Muhammad Fuad and Marteau (2008a), the way parameter \u03bb is defined remains problematic. On the one hand, the search space is very limited, on the other hand, generalizing EED to use higher order frequencies of common grams using the same basic heuristics to define the different parameters makes the parameter defining process, for the different grams, inefficient and yet limited to very small regions in the search space.\nIn the following we present a generalizing of EED which uses an artificial bee colony based approach to determine the different parameters. This makes the search process more efficient and effective."}, {"heading": "3.2 ABC-SG", "text": "Let\u03a3 be a finite alphabet, and let *\u03a3 be the set of strings on \u03a3 . Given n, let )S(anf be the frequency of the n-gram\nna in S , and )T(\nan f be the frequency of the n-gram na in\nT , where S ,T are two strings in *\u03a3 . Let N be the set of integers, and +N the set of positive integers.\nFor notation convenience, we define the function:\nNN \u2192\u03a3\u00d7+ *:g\n( ) nS,ng = if Sn \u2264\u22641\n( ) 1+= SS,ng if nS <\nThe ABC-SG distance between S and T is thus defined as:\n( ) ( ) ( )\n( ) ( )( )\n( ) \u2211\n\u2211= \u2208 \u23a5\n\u23a5 \u23a5 \u23a5 \u23a5 \u23a5\n\u23a6\n\u23a4\n\u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3\n\u23a1\n\u22c5\u2212+\n\u2212\u2212+\n=\u2212 T,Smax\nn T a S a\nAa\nn\nnn n\nn\nf,fmin\nT,ngS,ngTS .T,SSGABC\n1 22\n\u03bb (3)\nwhere S , T are the lengths of the two strings S ,T\nrespectively, and where { }0\u222a\u2208 +Rn\u03bb . ABC-SG is based on the same concept of familiarity on which EED is based, but this concept is extended to the familiarity of n-grams instead of that of single characters.\nIt is important to notice that ABC-SG is actually the generalization of both EED (Muhammad Fuad and Marteau 2008a), (Muhammad Fuad and Marteau 2008b) and MREED (Muhammad Fuad and Marteau 2008c), so it includes the same advantages that these two distances have.\nABC-SG is proved to be a distance metric. For space limitations, the proof is not presented here. However, the proof is an extension of the proof presented by Muhammad Fuad and Marteau (2008a) (2008b).\nAs indicated earlier, the parameters n\u03bb are determined using the artificial bee colony algorithm."}, {"heading": "3.3 Artificial Bee Colony", "text": "Bee-inspired optimization is a family of optimization algorithms that emerged from a larger family which is swarm intelligence. Baykaso\u011flu, \u00d6zbak\u0131r and Tapkan (2007) classify the behavioral characteristics of bee-based algorithms into three categories: foraging behaviors, marriage behaviors, and queen bee concept. One of the foraging behavior-based algorithms is Artificial Bee Colony (ABC) which was introduced by Karaboga (2005). In ABC each food source represents a potential solution to the optimization problem at hand and the quality of the food represents the value of the objective function to be optimized. Artificial bees explore and exploit the search space. These bees communicate and share information about the location and quality of food sources. This exchange of information takes place in the dancing area in the hive by performing a waggle dance.\nIn ABC there are three kinds of bees: Employed bees: These are the bees that search in the neighborhood of a food source. They perform a dance with a probability that is proportional to the quality of the food source. Onlooker bees: These bees are found on the dance floor. They watch the dances of the employed bees and place themselves on the most profitable food source. Scouts: These bees explore the search space randomly.\nAs mentioned by Parpinelli, Benitez, and Lopes (2010), the balance between exploration and exploitation is maintained in ABC algorithm by combining local search methods, carried out by the employed and the onlooker bees, with global search methods, carried out by the scouts.\nThere are several variations of the ABC algorithm. In the following we present the standard ABC introduced by Karaboga and Basturk (2007a) (2007b), and by Diwold Beekman, and Middendorf (2010). The first step of ABC is generating a randomly distributed population size (pop_size) of food sources which correspond to potential solutions. Each solution { }size_pop,..,i,xi 1\u2208 r is a vector whose dimension is (nr_par) which is equal to the number of parameters of the function f to be optimized. The population is subject to change for a number of cycles (nr_cycles). In each cycle every employed bee perturbs the current solution using a local search procedure. The perturbation produces a new solution:\n( )( ) ki,xx,randxx kii*i \u2260\u2212\u2212+= rrrr 11 (4)\nThe above relation is not applied to all parameters but only to a certain number of them. The parameters to be altered are chosen randomly. The algorithm uses a greedy selection to decide if the new solution should be kept or discarded, i.e. :\n( ) ( ) \u23aa\u23a9 \u23aa \u23a8 \u23a7 < = otherwisex xfxfifxx i i * i * i i r rrr r\n(5)\nAfter all employed bees have modified their positions the onlooker bees choose one of the current solutions depending on a probability that corresponds to the fitness value of that solution according to the following rule:\n( )\n( )\u2211 =\n= size_pop\nk k\ni i\nxf\nxfp\n1\nr\nr\n(6)\nAfter that the onlooker bees try to improve the solution using the same mechanism that was described in (4). The number of trials the algorithm attempts to improves the same solution is limited by a maximum number (max_nr) after which the solution is abandoned and the bees employed by that food source become scouts. The abandoned solution is replaced by a new solution found by the scouts. Figure 1 outlines the ABC algorithm."}, {"heading": "4 Performance Evaluation", "text": "We tested the new distance ABC-SG on symbolically represented time series. However, we think that ABC-SG is more appropriate for other sequential data types such as those encountered in bioinformatics and text mining.\nTime series data are normally numeric, but there are different methods to transform them to symbolic data. The most important symbolic representation method of time series is the Symbolic Aggregate Approximation (SAX) introduced by Lin, Keogh, Lonardi, and Chiu (2003). SAX is based on an assumption that normalized time series have Gaussian distribution, so by determining the breakpoints that correspond to a particular alphabet size, one can obtain equal-sized areas under the Gaussian curve. SAX is applied as follows: 1-The time series are normalized. 2-The dimensionality of the time series is reduced using PAA; a representation method presented independently by Keogh, Chakrabarti, Pazzani, and Mehrotra (2000) and by Yi and Faloutsos (2000). 3-The PAA representation of the time series is discretized by determining the number and locations of the breakpoints (The number of the breakpoints is chosen by the user). Their locations are determined, as mentioned above, using Gaussian lookup tables. The interval between two successive breakpoints is assigned to a symbol of the alphabet, and each segment of PAA that lies within that interval is discretized by that symbol.\nThe last step of SAX is using the following similarity measure:\n( ) ( )( )\u2211 =\n\u2261 N\ni ii r\u0302,s\u0302distN nR\u0302,S\u0302MINDIST 1 2 (7)\nWhere n is the length of the original time series, N is the length of the strings (the number of the segments),\nS\u0302 and R\u0302 are the symbolic representations of the two time series S and R , respectively, and where the function\n)(dist is implemented by using the appropriate lookup table.\nWe also need to mention that the similarity measure used in PAA is:\n( ) ( )\u2211 = \u2212= N i ii rs N nR,Sd 1 2 (8)\nIt is important to mention that MINDIST is not a distance metric (because it violates the axioms of distance metric) but a similarity measure.\nWe tested our new distance ABC-SG on a time series classification task based on the first nearest-neighbor (1- NN) rule using leaving-one-out cross validation. This means that every time series is compared to the other time series in the dataset. If the 1-NN does not belong to the same class, the error counter is incremented by 1.\nWe conducted experiments using datasets of different sizes and dimensions available at UCR of Keogh, Zhu, Hu, Hao, Xi, Wei, and Ratanamahatana (2011). This archive makes up between 90% and 100% of all publicly available, labeled time series data sets in the world, as mentioned by Ding, Trajcevski, Scheuermann, Wang, and Keogh (2008).\nAs indicated earlier, we tested ABC-SG on symbolically represented time series This means that the time series were transformed to symbolic sequences using the first three step of SAX presented earlier in this section, but instead of using MINDIST given in relation (7), we use our distance ABC-SG. The parameters n\u03bb in the definition of ABC-SG (relation (3)) are defined using ABC. This means, for each value of the alphabet size we formulate an artificial bee colony optimization problem where the fitness function is the classification error and the parameters of the optimization problem are n\u03bb . Theoretically n can take any value that does not exceed that of the shortest string of the two strings S , T . However, in the experiments we conducted we tested the new distance for { }321 ,,n\u2208 because these are the values of interest for time series. Notice that ABC-SG can be applied to strings of different lengths, which is one of its advantages since most similarity measures in time series mining are applied only to time series of the same length.\nConcerning the control parameters of the ABC we used, the population size (the number of food sources) pop_size was 20. The number of cycles nr_cycles was set to 20. The number of trials of a certain food source max_nr was set to 10. The number of parameters nr_par , as mentioned earlier, was tested for { }321 ,,n\u2208 . As for\nn\u03bb , their values are in fact unconstrained, but for simplicity we optimized them in the interval [ ]20, . Table 1 summarizes the symbols used in the experiments together with their corresponding values.\nFor each dataset we use ABC on the training datasets to\nget the vector n\u03bb that minimizes the classification error on this training dataset, and then we use these optimal values of n\u03bb on the corresponding testing dataset to get the final classification error for each dataset.\nWe compared ABC-SG with dynamic time warping (DTW). DTW is a similarity measure that has been developed by the speech recognition community and later was used by Berndt, and Clifford (1994) on time series. DTW is an algorithm to find the optimal path through a matrix of points representing possible time alignments between the signals. Guo and Siegelmann (2004) state that the optimal alignment can be efficiently calculated via dynamic programming.\nThe dynamic time warping between the two time series { }ns,...,s,sS 21= , { }mr,...,r,rR 21= is defined as follows:\n( ) ( ) ( ) ( ) ( )\u23aa\u23a9 \u23aa \u23a8 \u23a7 \u2212\u2212 \u2212 \u2212 += 11 1 1 j,iDTW j,iDTW j,iDTW minj,idj,iDTW (9)\nwhere ni \u2264\u22641 , mj \u2264\u22641 .\nWe chose to compare ABC-SG with DTW because DTW is known to give very good results in several time series data mining tasks such as classification and clustering. Another reason for choosing DTW is because it is applicable to time series of different lengths, which is the case with ABC-SG. However, ABC-SG has a complexity of )( 2nO , while that of ABC-SG is ( )2NO ( 4/nN = for compression ratio 1:4; the compression ratio usually used with SAX). So as we can see, ABC-SG has a much lower complexity than that of DTW. Another advantage that ABC-SG has over DTW is that ABC-SG is a distance metric while DTW is a similarity measure because it violates the triangle inequality.\nIt is important to mention that DTW is applied to the original time series and not to their symbolic representation.\nIn Table 2 we present some of the results we obtained for alphabet size equal to 3, 10, and 20, respectively.\nAs we can see from the results, the classification errors of ABC-SG are quite comparable to those of DTW despite the difference in complexity. In fact, in the majority of cases, ABC-SG even outperformed DTW. The results of other datasets in the archive were similar.\nAnother interesting remark which makes this distance meaningful is that we did not witness any correlation between the number of grams used and the classification error, which makes sense since ABC-SG, as mentioned in Section 3, is based on the concept of familiarity of ngrams between the two strings, and this familiarity is not related to the length of the n-gram.\nFinally, in Table 3 we present, for reproducibility purposes, the values of n\u03bb obtained on the training datasets. As indicated earlier, when applying these values to the corresponding testing datasets we obtain the final classification errors presented in Table 2"}, {"heading": "2 [0.82499 0.25791] 3", "text": ""}, {"heading": "2 [0.21865 0.26487] 3", "text": ""}, {"heading": "5 Conclusion", "text": "In this paper we presented a new distance metric; ABCSG, which is applied to strings. This distance considers the frequencies of n-grams, which adds a global level of similarity, in addition to the local one. The particularity of this distance is that it uses the artificial bee colony algorithm to determine the values of its parameters. We tested the new distance and we compared it to a very competitive similarity measure; DTW, on a time series classification task. We showed that our distance ABC-SG gave better results in most cases, despite the difference in complexity.\nIn order to represent the time series symbolically, we had to use SAX because this is the most widely used symbolic representation method of time series. Nonetheless, a representation technique prepared specifically for ABC-SG may even give better results.\nAlthough we used ABC as an optimization algorithm to set the parameters of the new distance, we think other stochastic and bio-inspired optimization algorithms can also be used with the new distance."}, {"heading": "6 References", "text": "Baykasoglu A, Ozbakir L, Tapkan P (2007): Artificial bee\ncolony algorithm and its application to generalized assignment problem. In: Swarm intelligence focus on ant and particle swarm optimization. I-Tech Education and Publishing, Vienna, Austria, pp 113-144. Berndt, D. and Clifford, J. (1994): Using dynamic time warping to find patterns in time series. In Proc. AAAI Workshop on Knowledge Discovery in Databases. Ding, H., Trajcevski, G., Scheuermann, P., Wang, X., and Keogh, E. (2008): Querying and mining of time series data: experimental comparison of representations and distance measures. In Proc of the 34th VLDB. Diwold K, Beekman M, Middendorf M (2010): Honeybee optimisation an overview and a new bee inspired optimisation scheme. In: Hiot LM, Ong YS, Panigrahi BK, Shi Y, Lim MH (eds) Handbook of swarm intelligence, adaptation, learning, and optimization, vol 8. Springer, Berlin/Heidelberg, Germany, pp 295\u2013327. Guo, AY., and Siegelmann, H(2004): Time-warped longest common subsequence algorithm for music retrieval, in Proc. ISMIR. Karaboga, D. (2005): An idea based on honey bee swarm for numerical optimization. Technical Report TR06, Erciyes University, Engineering Faculty, Computer Engineering Department Karaboga, D., Basturk, B. (2007a): A powerful and efficient algorithm for numerical function optimization: artificial bee colony (abc) algorithm, Journal of Global Optimization 39 (3) 459\u2013471. Karaboga, D., Basturk, B. (2007b) In: Advances in Soft Computing: Foundations of Fuzzy Logic and Soft Computing, LNCS, vol. 4529/2007, Springer-Verlag, 2007, pp. 789\u2013798 (Chapter Artificial Bee Colony (ABC) Optimization Algorithm for Solving Constrained Optimization Problems). Keogh, E., Chakrabarti, K., Pazzani, M., and Mehrotra (2000): Dimensionality reduction for fast similarity search in large time series databases. J. of Know. and Inform. Sys. Keogh, E., Zhu, Q., Hu, B., Hao. Y., Xi, X., Wei, L. & Ratanamahatana (2011), The UCR Time Series Classification/Clustering Homepage:\nwww.cs.ucr.edu/~eamonn/time_series_data/ C. A.. Lin, J., Keogh, E., Lonardi, S., Chiu, B. Y. (2003): A symbolic\nrepresentation of time series, with implications for streaming algorithms. DMKD 2003: 2-11. Muhammad Fuad, M.M., Marteau , P.F. (2008a) : Extending the edit distance using frequencies of common characters. 19th International Conference on Database and Expert Systems Applications - DEXA'08, Turin, Italy.1-5 September 2008. Lecture Notes in Computer Science, 2008, Volume 5181/2008. Muhammad Fuad, M.M., Marteau , P.F. (2008b) : The Extended Edit Distance Metric, Sixth International Workshop on Content-Based Multimedia Indexing (CBMI 2008), London, UK, 18-20th June 2008. Muhammad Fuad, M.M., Marteau, P.F. (2008c) : The multiresolution extended edit distance. Third International ICST Conference on Scalable Information Systems, Infoscale,\nVico Equense, Italy. June 4-6 2008. ACM Digital Library, 2008. Parpinelli, R.S., Benitez, C.M.V., Lopes, H.S. (2011): Parallel approaches for the artificial bee colony algorithm. In: Panigrahi, B.K., Shi, Y., Lim, M.H., Hiot, L.M., Ong, Y.S. (eds.) Handbook of Swarm Intelligence, Adaptation, Learning, and Optimization, vol. 8, pp. 329\u2013345. Springer, Berlin. Wagner, R.A., Fischer, M. J. (1974): The string-to-string correction problem, Journal of the Association for Computing Machinery, Vol. 21, No. I, January 1974, pp. 168-173. Yi, B. K., and Faloutsos, C. (2000): Fast time sequence indexing for arbitrary Lp norms. Proceedings of the 26th International Conference on Very Large Databases, Cairo, Egypt. Zezula et al., (2005) :Similarity Search - The Metric Space Approach, Springer."}], "references": [{"title": "Artificial bee colony algorithm and its application to generalized assignment problem. In: Swarm intelligence focus on ant and particle swarm optimization. I-Tech Education and Publishing, Vienna, Austria, pp 113-144", "author": ["A Baykasoglu", "L Ozbakir", "P Tapkan"], "venue": null, "citeRegEx": "Baykasoglu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Baykasoglu et al\\.", "year": 2007}, {"title": "Using dynamic time warping to find patterns in time series", "author": ["D. Berndt", "J. Clifford"], "venue": "In Proc. AAAI Workshop on Knowledge Discovery in Databases", "citeRegEx": "Berndt and Clifford,? \\Q1994\\E", "shortCiteRegEx": "Berndt and Clifford", "year": 1994}, {"title": "Querying and mining of time series data: experimental comparison of representations and distance measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "In Proc of the 34th VLDB", "citeRegEx": "Ding et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2008}, {"title": "Honeybee optimisation an overview and a new bee inspired optimisation scheme", "author": ["K Diwold", "M Beekman", "M Middendorf"], "venue": "MH (eds) Handbook of swarm intelligence,", "citeRegEx": "Diwold et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Diwold et al\\.", "year": 2010}, {"title": "Time-warped longest common subsequence algorithm for music retrieval, in Proc. ISMIR", "author": ["Guo", "AY", "Siegelmann"], "venue": null, "citeRegEx": "Guo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2004}, {"title": "An idea based on honey bee swarm for numerical optimization", "author": ["D. Karaboga"], "venue": "Technical Report TR06,", "citeRegEx": "Karaboga,? \\Q2005\\E", "shortCiteRegEx": "Karaboga", "year": 2005}, {"title": "A powerful and efficient algorithm for numerical function optimization: artificial bee colony (abc) algorithm", "author": ["D. Karaboga", "B. Basturk"], "venue": "Journal of Global Optimization", "citeRegEx": "Karaboga and Basturk,? \\Q2007\\E", "shortCiteRegEx": "Karaboga and Basturk", "year": 2007}, {"title": "Artificial Bee Colony (ABC) Optimization Algorithm for Solving Constrained Optimization Problems)", "author": ["D. Karaboga", "B. Basturk"], "venue": "Advances in Soft Computing: Foundations of Fuzzy Logic and Soft Computing, LNCS,", "citeRegEx": "Karaboga and Basturk,? \\Q2007\\E", "shortCiteRegEx": "Karaboga and Basturk", "year": 2007}, {"title": "Dimensionality reduction for fast similarity search in large time series databases", "author": ["E. Keogh", "K. Chakrabarti", "M. Pazzani", "Mehrotra"], "venue": "J. of Know. and Inform. Sys", "citeRegEx": "Keogh et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Keogh et al\\.", "year": 2000}, {"title": "The UCR Time Series Classification/Clustering Homepage: www.cs.ucr.edu/~eamonn/time_series_data/ C. A", "author": ["E. Keogh", "Q. Zhu", "B. Hu", "Hao. Y", "X. Xi", "L. Wei", "Ratanamahatana"], "venue": null, "citeRegEx": "Keogh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Keogh et al\\.", "year": 2011}, {"title": "A symbolic representation of time series, with implications for streaming algorithms", "author": ["J. Lin", "E. Keogh", "S. Lonardi", "B.Y. Chiu"], "venue": "DMKD", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "Extending the edit distance using frequencies of common characters", "author": ["M.M. Muhammad Fuad", "P.F. Marteau"], "venue": "19th International Conference on Database and Expert Systems Applications - DEXA'08, Turin,", "citeRegEx": "Fuad and Marteau,? \\Q2008\\E", "shortCiteRegEx": "Fuad and Marteau", "year": 2008}, {"title": "The Extended Edit Distance Metric", "author": ["M.M. Muhammad Fuad", "P.F. Marteau"], "venue": "Sixth International Workshop on Content-Based Multimedia Indexing (CBMI", "citeRegEx": "Fuad and Marteau,? \\Q2008\\E", "shortCiteRegEx": "Fuad and Marteau", "year": 2008}, {"title": "2008c) : The multiresolution extended edit distance", "author": ["M.M. Muhammad Fuad", "P.F. Marteau"], "venue": "Third International ICST Conference on Scalable Information Systems, Infoscale,", "citeRegEx": "Fuad and Marteau,? \\Q2008\\E", "shortCiteRegEx": "Fuad and Marteau", "year": 2008}, {"title": "Parallel approaches for the artificial bee colony algorithm", "author": ["R.S. Parpinelli", "C.M.V. Benitez", "H.S. Lopes"], "venue": "Handbook of Swarm Intelligence, Adaptation, Learning, and Optimization,", "citeRegEx": "Parpinelli et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parpinelli et al\\.", "year": 2011}, {"title": "The string-to-string correction problem, Journal of the Association for Computing", "author": ["R.A. Wagner", "M.J. Fischer"], "venue": null, "citeRegEx": "Wagner and Fischer,? \\Q1974\\E", "shortCiteRegEx": "Wagner and Fischer", "year": 1974}, {"title": "Fast time sequence indexing for arbitrary Lp norms", "author": ["B.K. Yi", "C. Faloutsos"], "venue": "Proceedings of the 26th International Conference on Very Large Databases,", "citeRegEx": "Yi and Faloutsos,? \\Q2000\\E", "shortCiteRegEx": "Yi and Faloutsos", "year": 2000}, {"title": "Similarity Search - The Metric Space Approach, Springer", "author": ["Zezula"], "venue": null, "citeRegEx": "Zezula,? \\Q2005\\E", "shortCiteRegEx": "Zezula", "year": 2005}], "referenceMentions": [{"referenceID": 12, "context": "The main distance used to compare two strings is the Edit Distance (ED) presented by Wagner and Fischer (1974), it is also called the Levenshtein distance, and it is defined as the minimum number of delete, insert, and change operations needed to transform string S into string T.", "startOffset": 85, "endOffset": 111}, {"referenceID": 11, "context": "Muhammad Fuad and Marteau (2008a) (2008b) presented a new distance metric; The Extended Edit Distanc (EED), which they applied to symbolically represented time series.", "startOffset": 9, "endOffset": 34}, {"referenceID": 11, "context": "Muhammad Fuad and Marteau (2008a) (2008b) presented a new distance metric; The Extended Edit Distanc (EED), which they applied to symbolically represented time series.", "startOffset": 9, "endOffset": 42}, {"referenceID": 11, "context": "Muhammad Fuad and Marteau (2008a) (2008b) presented a new distance metric; The Extended Edit Distanc (EED), which they applied to symbolically represented time series. Unlike ED, EED considers a global level of similarity in additional to the local one presented by ED. EED is based on the idea of computing the frequencies of common characters between two strings. Later, Muhammad Fuad and Marteau (2008c) presented another distance, MREED, which computes the frequencies of common bi-grams in addition to common characters.", "startOffset": 9, "endOffset": 407}, {"referenceID": 11, "context": "2 Background Muhammad Fuad and Marteau (2008a) (2008b) presented the Extended Edit Distance (EED) which is defined as follows:", "startOffset": 22, "endOffset": 47}, {"referenceID": 11, "context": "2 Background Muhammad Fuad and Marteau (2008a) (2008b) presented the Extended Edit Distance (EED) which is defined as follows:", "startOffset": 22, "endOffset": 55}, {"referenceID": 11, "context": "This modification makes EED more intuitive as shown by Muhammad Fuad and Marteau (2008a) (2008b).", "startOffset": 64, "endOffset": 89}, {"referenceID": 11, "context": "This modification makes EED more intuitive as shown by Muhammad Fuad and Marteau (2008a) (2008b). Muhammad Fuad and Marteau (2008c) also showed that EED is a distance metric (symmetry, identity, triangle inequality).", "startOffset": 64, "endOffset": 97}, {"referenceID": 11, "context": "This modification makes EED more intuitive as shown by Muhammad Fuad and Marteau (2008a) (2008b). Muhammad Fuad and Marteau (2008c) also showed that EED is a distance metric (symmetry, identity, triangle inequality).", "startOffset": 64, "endOffset": 132}, {"referenceID": 11, "context": "This modification makes EED more intuitive as shown by Muhammad Fuad and Marteau (2008a) (2008b). Muhammad Fuad and Marteau (2008c) also showed that EED is a distance metric (symmetry, identity, triangle inequality). Search in metric spaces has many advantages, the most famous of which is that a single indexing structure can be applied to several kinds of queries and data types that are so different in nature. This is mainly important in establishing unifying models for the search problem that are independent of the data type. This makes metric spaces a solid structure that is able to deal with several data types as mentioned by Zezula et al. (2005).", "startOffset": 64, "endOffset": 658}, {"referenceID": 11, "context": "Muhammad Fuad and Marteau (2008a) (2008b) showed how EED, which considers the frequencies of characters, can capture this intuitive similarity that ED can not capture.", "startOffset": 9, "endOffset": 34}, {"referenceID": 11, "context": "Muhammad Fuad and Marteau (2008a) (2008b) showed how EED, which considers the frequencies of characters, can capture this intuitive similarity that ED can not capture.", "startOffset": 9, "endOffset": 42}, {"referenceID": 11, "context": "Muhammad Fuad and Marteau (2008a) (2008b) showed how EED, which considers the frequencies of characters, can capture this intuitive similarity that ED can not capture. Although EED has advantages over ED as shown by Muhammad Fuad and Marteau (2008a), the way parameter \u03bb is defined remains problematic.", "startOffset": 9, "endOffset": 250}, {"referenceID": 11, "context": "It is important to notice that ABC-SG is actually the generalization of both EED (Muhammad Fuad and Marteau 2008a), (Muhammad Fuad and Marteau 2008b) and MREED (Muhammad Fuad and Marteau 2008c), so it includes the same advantages that these two distances have. ABC-SG is proved to be a distance metric. For space limitations, the proof is not presented here. However, the proof is an extension of the proof presented by Muhammad Fuad and Marteau (2008a) (2008b).", "startOffset": 91, "endOffset": 454}, {"referenceID": 11, "context": "It is important to notice that ABC-SG is actually the generalization of both EED (Muhammad Fuad and Marteau 2008a), (Muhammad Fuad and Marteau 2008b) and MREED (Muhammad Fuad and Marteau 2008c), so it includes the same advantages that these two distances have. ABC-SG is proved to be a distance metric. For space limitations, the proof is not presented here. However, the proof is an extension of the proof presented by Muhammad Fuad and Marteau (2008a) (2008b). As indicated earlier, the parameters n \u03bb are determined using the artificial bee colony algorithm.", "startOffset": 91, "endOffset": 462}, {"referenceID": 5, "context": "One of the foraging behavior-based algorithms is Artificial Bee Colony (ABC) which was introduced by Karaboga (2005). In ABC each food source represents a potential solution to the optimization problem at hand and the quality of the food represents the value of the objective function to be optimized.", "startOffset": 101, "endOffset": 117}, {"referenceID": 5, "context": "One of the foraging behavior-based algorithms is Artificial Bee Colony (ABC) which was introduced by Karaboga (2005). In ABC each food source represents a potential solution to the optimization problem at hand and the quality of the food represents the value of the objective function to be optimized. Artificial bees explore and exploit the search space. These bees communicate and share information about the location and quality of food sources. This exchange of information takes place in the dancing area in the hive by performing a waggle dance. In ABC there are three kinds of bees: Employed bees: These are the bees that search in the neighborhood of a food source. They perform a dance with a probability that is proportional to the quality of the food source. Onlooker bees: These bees are found on the dance floor. They watch the dances of the employed bees and place themselves on the most profitable food source. Scouts: These bees explore the search space randomly. As mentioned by Parpinelli, Benitez, and Lopes (2010), the balance between exploration and exploitation is maintained in ABC algorithm by combining local search methods, carried out by the employed and the onlooker bees, with global search methods, carried out by the scouts.", "startOffset": 101, "endOffset": 1034}, {"referenceID": 5, "context": "One of the foraging behavior-based algorithms is Artificial Bee Colony (ABC) which was introduced by Karaboga (2005). In ABC each food source represents a potential solution to the optimization problem at hand and the quality of the food represents the value of the objective function to be optimized. Artificial bees explore and exploit the search space. These bees communicate and share information about the location and quality of food sources. This exchange of information takes place in the dancing area in the hive by performing a waggle dance. In ABC there are three kinds of bees: Employed bees: These are the bees that search in the neighborhood of a food source. They perform a dance with a probability that is proportional to the quality of the food source. Onlooker bees: These bees are found on the dance floor. They watch the dances of the employed bees and place themselves on the most profitable food source. Scouts: These bees explore the search space randomly. As mentioned by Parpinelli, Benitez, and Lopes (2010), the balance between exploration and exploitation is maintained in ABC algorithm by combining local search methods, carried out by the employed and the onlooker bees, with global search methods, carried out by the scouts. There are several variations of the ABC algorithm. In the following we present the standard ABC introduced by Karaboga and Basturk (2007a) (2007b), and by Diwold Beekman, and Middendorf (2010).", "startOffset": 101, "endOffset": 1395}, {"referenceID": 5, "context": "One of the foraging behavior-based algorithms is Artificial Bee Colony (ABC) which was introduced by Karaboga (2005). In ABC each food source represents a potential solution to the optimization problem at hand and the quality of the food represents the value of the objective function to be optimized. Artificial bees explore and exploit the search space. These bees communicate and share information about the location and quality of food sources. This exchange of information takes place in the dancing area in the hive by performing a waggle dance. In ABC there are three kinds of bees: Employed bees: These are the bees that search in the neighborhood of a food source. They perform a dance with a probability that is proportional to the quality of the food source. Onlooker bees: These bees are found on the dance floor. They watch the dances of the employed bees and place themselves on the most profitable food source. Scouts: These bees explore the search space randomly. As mentioned by Parpinelli, Benitez, and Lopes (2010), the balance between exploration and exploitation is maintained in ABC algorithm by combining local search methods, carried out by the employed and the onlooker bees, with global search methods, carried out by the scouts. There are several variations of the ABC algorithm. In the following we present the standard ABC introduced by Karaboga and Basturk (2007a) (2007b), and by Diwold Beekman, and Middendorf (2010).", "startOffset": 101, "endOffset": 1403}, {"referenceID": 5, "context": "One of the foraging behavior-based algorithms is Artificial Bee Colony (ABC) which was introduced by Karaboga (2005). In ABC each food source represents a potential solution to the optimization problem at hand and the quality of the food represents the value of the objective function to be optimized. Artificial bees explore and exploit the search space. These bees communicate and share information about the location and quality of food sources. This exchange of information takes place in the dancing area in the hive by performing a waggle dance. In ABC there are three kinds of bees: Employed bees: These are the bees that search in the neighborhood of a food source. They perform a dance with a probability that is proportional to the quality of the food source. Onlooker bees: These bees are found on the dance floor. They watch the dances of the employed bees and place themselves on the most profitable food source. Scouts: These bees explore the search space randomly. As mentioned by Parpinelli, Benitez, and Lopes (2010), the balance between exploration and exploitation is maintained in ABC algorithm by combining local search methods, carried out by the employed and the onlooker bees, with global search methods, carried out by the scouts. There are several variations of the ABC algorithm. In the following we present the standard ABC introduced by Karaboga and Basturk (2007a) (2007b), and by Diwold Beekman, and Middendorf (2010). The first step of ABC is generating a randomly distributed population size (pop_size) of food sources which correspond to potential solutions.", "startOffset": 101, "endOffset": 1449}, {"referenceID": 16, "context": "2-The dimensionality of the time series is reduced using PAA; a representation method presented independently by Keogh, Chakrabarti, Pazzani, and Mehrotra (2000) and by Yi and Faloutsos (2000). 3-The PAA representation of the time series is discretized by determining the number and locations of the breakpoints (The number of the breakpoints is chosen by the user).", "startOffset": 169, "endOffset": 193}], "year": 2013, "abstractText": "The problem of similarity search is one of the main problems in computer science. This problem has many applications in text-retrieval, web search, computational biology, bioinformatics and others. Similarity between two data objects can be depicted using a similarity measure or a distance metric. There are numerous distance metrics in the literature, some are used for a particular data type, and others are more general. In this paper we present a new distance metric for sequential data which is based on the sum of n-grams. The novelty of our distance is that these n-grams are weighted using artificial bee colony; a recent optimization algorithm based on the collective intelligence of a swarm of bees on their search for nectar. This algorithm has been used in optimizing a large number of numerical problems. We validate the new distance experimentally.", "creator": "PScript5.dll Version 5.2.2"}}}