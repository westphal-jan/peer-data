{"id": "1312.1737", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Dec-2013", "title": "Curriculum Learning for Handwritten Text Line Recognition", "abstract": "recurrent neural networks ( gps rnn ) have since recently achieved the best performance in automated off - line handwriting text recognition. at the same time, learning rnn by gradient descent leads to slow narrative convergence, and training times are particularly long when the training database consists of full lines of text. in this paper, we propose an easy way to accelerate stochastic gradient descent in this set - up, and in the general context of learning to recognize sequences. the principle implementation is called curriculum learning, or shaping. the idea is supposed to finally first learn to recognize multiple short sequences before training on all available training sequences. experiments on three different handwritten text databases ( rimes, iam, openhart ) show that a simple implementation of this evolving strategy can significantly speed up the training of rnn for text recognition, marking and even significantly improve performance in some cases.", "histories": [["v1", "Thu, 5 Dec 2013 23:53:45 GMT  (82kb,D)", "http://arxiv.org/abs/1312.1737v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["j\\'er\\^ome louradour", "christopher kermorvant"], "accepted": false, "id": "1312.1737"}, "pdf": {"name": "1312.1737.pdf", "metadata": {"source": "CRF", "title": "Curriculum Learning for Handwritten Text Line Recognition", "authors": ["J\u00e9r\u00f4me Louradour", "Christopher Kermorvant"], "emails": ["jl@a2ia.com", "ck@a2ia.com"], "sections": [{"heading": "1 Introduction", "text": "The application of interest in this paper is off-line Handwritten Text Recognition (HTR), on images of paper documents. At the time being, the most powerful models for this task are Recurrent Neural Networks (RNN) with several layers of multi-directional Long-Short Term Memory (LSTM) units [9, 17]. Gradient-based optimization of RNN, which cannot be guaranteed to converge to the optimal solution, is a particularly hard issue for two reasons:\nFirst, if we conceptually unfold the recurrences done in the spatial domain (2D, sometimes 1D), we can see RNN as deep models. Because of the numerous non-linear functions that are composed, they are exposed to the burden of exploding and vanishing gradient [3, 11, 18]. In practice, the use of LSTM units, which are carefully designed cells with multiplicative gates to store information over long periods and forget when needed, turned out to be a key ingredient to enable the learning of RNN with standard gradient descent despite the network deepness. There are other ways to efficiently learn RNN, namely enhanced optimization approaches such as second-order methods [15] or good initialization with momentum [23]. These methods are beyond the scope of this paper.\nSecondly, RNN are used here for an unconstrained \u201cTemporal Classification\u201d task [7, 8], where the length of the sequence to recognize is in general different from the length of the input sequence. In HTR, the goal is to detect occurrences of characters within a stream of image, without a priori segmentation, in other words without knowing the alignment between the pixels and the target characters. So the models must be optimized to solve two problems at the same time: localizing the characters and classifying them.\nBecause of all these aspects, training RNN takes a particularly long time. Here we propose to make the training process more effective by using the concept of Curriculum Learning, that has already been successfully applied in the context of deep models and Stochastic Gradient Descent [2]. The\nar X\niv :1\n31 2.\n17 37\nv1 [\ncs .L\nG ]\n5 D\nec 2\nkey idea is to guide the training by carefully choosing samples so as to start simple and progressively increase the complexity of training samples. The main motivation is to speed up the learning progression, without any loss of generality in the end. Gradually increasing the complexity of the task has been demonstrated to make learning faster and more robust in several scenarios. This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].\nIn this paper, we show how the Curriculum Learning concept can be naturally be applied to RNN in the context of Handwritten Text Recognition, using the text sequence length as a measure of its complexity. We give empirical evidences that our proposal significantly speeds up the learning progression. The principle is general enough to be applied to any sequence recognition task, and to any kind of model optimized using a gradient-based method."}, {"heading": "2 A curriculum for Text Recognition", "text": ""}, {"heading": "2.1 Two tasks when learning to recognize text: Localization and Classification", "text": "In text recognition, locating the characters is necessary to learn to recognize them. However, in many public database for Handwriting Recognition, the positions and the text content are given for each page or paragraph, not for characters. The localization of the lines is reasonably easy to obtain using automatic line segmentation. But locating the characters is a more difficult problem, particularly in the case of handwritten text where even humans can disagree on how to segment the characters. This is why a Connectionist Temporal Classification (CTC) approach as proposed by [7] is a very practical way to train RNN models without intensive labeling effort.\nIn their CTC approach, [7] efficiently compute and derive a cost function that is the Negative LogLikelihood (NLL), with the assumptions that all the frame probabilities are independent and that all possible alignments are equiprobable. Besides, an additional label is considered: the blank, which stands for \u201cno character\u201d but also for \u201czone in-between two characters\u201d, meaning that the blank label can be produced between any two different characters. To the best of our knowledge, taking into account all the possible alignments (including the blank) is the most effective approach in training RNN to detect characters. But it also unveils a vague localization of the characters, especially at the beginning of the training process, when the RNN gives quasi-random guesses for the posteriors of the labels (see the CTC Error Signal of Figure 4 in [7]).\nSeveral studies about Text Recognition have revealed that the training process of RNN is particularly long [21]. Not only because of the heavy computational complexity due to the recurrences, but also because the learning progression frequently starts with a plateau. A high number of model parameter updates is needed before the cost function starts to decrease. In some extreme cases, the learning seems to never start, as if the optimization process quickly got stuck in a poor local minimum."}, {"heading": "2.2 Building a suitable Curriculum", "text": "One of the reasons for this difficulty to start learning is the fact that when initializing with quasirandom model parameters, the RNN has little chance to produce a reasonable segmentation. Moreover, it is clear that the longer the sequences are, the more serious the problem is. In a nutshell, it is hard and inefficient to learn long sequences at first.\nThinking in the same spirit as [2], let us make a parallel with how to teach kids to read and write. A natural way is to do it step by step: first teach him to recognize characters by showing him isolated symbols, then teach him short words, before introducing longer words and sentences. A similar Curriculum Learning procedure can be done when optimizing neural networks by gradient-descent (e.g. RNN using CTC): First optimize on a database of isolated characters (if available), then on a database of isolated words, and finally on a database of lines1.\nSince having access to the positions of characters and/or words may be costly or impossible, we propose here to adapt this proposition to the case where only lines can be robustly extracted from the training database. Keeping in mind that the difficulty when starting to train RNN is related to\n1RNN cannot decode paragraphs, just single lines: the common RNN architectures collapse the 2D input image into a 1D signal just before aligning using CTC [9].\nthe length of the training sequences, a general way to build a Curriculum Learning for Text Line Recognition is to first train on short lines, before including long ones. Note that the last line in a paragraph can sometimes consist of a single word."}, {"heading": "2.3 Proposal: continuous curriculum", "text": "In practice, it is awkward to build a step-wise schedule by splitting a database with respect to the sequence lengths. Instead, we prefer to handle a probability to draw a sample line from the training database. The idea of defining such a probability for probing the training database has already been successfully applied in Active Learning [19, 4].\nIf (Xt, Yt) denotes a training sample (an image along with the corresponding target sequence of labels), we propose to draw this sample with the following probability parameterized by \u03bb:\nP\u03bb (train on (Xt, Yt)) = 1\nN\u03bb\n( shortness(Xt, Yt) )\u03bb (1)\nwhere \u2022 N\u03bb = \u2211 t (shortness(Xt, Yt))\n\u03bb is a normalization constant so that (1) defines a probability over the set of all the available training samples. \u2022 shortness \u2208 [0, 1] is a bounded value to represent how easy is a training sample. Here it\nis based on the sequence length. We discuss this quantity below. \u2022 \u03bb \u2265 0 is an hyper-parameter to tune how much the short words are favoured.\nThe particular setting \u03bb = 0 amounts to the baseline approach where samples are drawn randomly with flat probability and with replacement. So \u03bb can be tuned during the training process. In our experiments, we start with \u03bb = 3 and linearly decrease \u03bb until 0, during the equivalent of the first 5 epochs of training. And one epoch is about 10k to 100k different lines of text (see number of labelled lines in table 1).\nConcerning the shortness measure, we propose to use the following simple form:\nshortness(Xt, Yt) = 1\nmax(m, |Yt|) (2)\nwhere |Yt| is the length of the target sequence (number of characters), and m > 0 is a minimal length which stands as a clipping threshold. Using m = 1 is needed to avoid numerical problems when there are empty target sequences in the training set. Using more than 1 can be useful to avoid favouring too much on very small words, such as frequent pronouns, or punctuation marks when they are considered as a word with a single character. We used m = 5 in our experiments, as it is a common length for short words.\nNote that we could use in (2) the width of the input images |Xt| instead of (or along with) the sequence length |Yt|. It also makes sense and these two measures are actually correlated. But the target length |Yt| has the advantage of being independent of the resolution of the images, and is also a notion that can be used in other applications than Vision."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Databases", "text": "Three notated public handwriting datasets are used to evaluate our system:\n\u2022 the IAM database, a dataset containing pages of handwritten English text [16], \u2022 the Rimes database2, a dataset of handwritten French letters used in several ICDAR com-\npetitions (lastly ICDAR 2011 [10, 17]). \u2022 The OpenHaRT database, a dataset of handwritten Arabic pages, used in two NIST Open\nHandwriting Recognition and Translation Evaluation (lastly OpenHaRT 2013). 2http://www.rimes-database.fr\nFor all these databases, the localization of the words is available. So we could compare the continuous Curriculum strategy proposed in section 2.3 with the simple \u201cby-hand\u201d Curriculum which consists in first training RNN to recognize words and secondly to recognize lines.\nWe use distinct subsets of pages to train and to evaluate RNN models. In the case of the \u201cby-hand\u201d Curriculum, we carefully used the same subset of pages in the training set of words and in the training set of lines. Table 1 gives the number of data in each training set.\nThe resolution of images to feed the network is fixed to 300 dpi. Original OpenHaRT images (resp. Rimes images) are in 600 dpi (resp. 200 dpi) and they were rescaled with a factor 0.5 (resp. 1.5), using interpolation.\n3.2 Modeling and learning details\n2 2\nInput image Block: 2 x 2 MDLSTM Features: 2 Convolutional Input size: 2x4\nFeatures: 6\n2 6\n6\nSum & Tanh MDLSTM Features: 10 Convolutional Input: 2 x 4\nFeatures: 20\nSum & Tanh MDLSTM Features: 50 Fully-connected Features: N\nSum Collapse\n10 20\n20\n50 N\nN N\n..... N-way softmax\nCTC\nFigure 1: RNN topology used for all the experiments of this paper. The resolution of input images is 300 dpi. The size of the hidden layers is given as the number of features in each intermediate representation. N is the number of possible target characters including the blank (\u201c# different characters\u201d in Table 1, plus one).\nThe RNN topology we use is depicted in figure 1. It is the same as described in [9], except that the sizes of the filters have been adapted to images in 300 dpi: we used 2x2 input tiling, and 2x4 filters in the two sub-sampling hidden layers (which are convolutional layers without overlap between the filters). The LSTM layers scan the inputs in 4 directions, and the computations can be parallelized over the 4 directions.\nAll the models were optimized using Stochastic Gradient Descent [14]: a model update happens after each training sample (i.e. each line of characters) is visited. The learning rate is constant, and was fixed to 0.001 in all our experiments."}, {"heading": "3.3 Performance assessment", "text": "As we are interested in convergence speed, we plot convergence curves that represent the evolution of some costs with respect to a unit of progression of the training algorithm. In our case, we use Stochastic Gradient Descent [14] and the unit of progression could be for instance the number of updates, that is the number of training samples that have been browsed. Given that the sequence length of training samples is the measure of complexity here, we chose instead to represent the progression by the total number of targets (characters) that have been browsed. This unit is more representative of the computation time than the number of updates, because the inputs are sequences with variable-length, and we remind that the Curriculum strategies tend to process shorter sequences in the beginning of the learning process.\nWe remind that the cost optimized using CTC [7] is the Negative Log-Likelihood (NLL), which can be averaged over the number of training sequences. However, probabilities decrease exponentially with sequence length. For this reason, the NLL average costs are usually higher on databases with long sequences (e.g. lines) than on databases with short or middle-length sequences (e.g. words). That is why we chose a normalized NLL to monitor the performance of our systems:\nnormNLL ({ (Xt, Yt) }) = \u2211 t NLL(Yt|Xt)\u2211\nt |Yt| (3)\nAs a relevant but discrete cost to evaluate RNN optical models, we also monitor the Character Error Rate (CER) that is computed by an edit distance, normalized in a similar manner:\nCER = \u2211 tEditDistance(Yt, f\u0302(Xt))\u2211\nt |Yt| (4)\nwhere f\u0302(Xt) is the most probable sequence recognized by the RNN. The edit distance is a Levenshtein distance defined by the minimum number of insertions, substitutions and deletions required to change the target Yt into the model\u2019s prediction f\u0302(Xt)."}, {"heading": "3.4 Results and analysis", "text": "The convergence curves for learning Handwritten Text Recognition on the three languages, respectively IAM (English), Rimes (French) and OpenHaRT (Arabic), are shown in Figures 2, 3 and 4.\nThey show the progression of the costs on the validation dataset, and the vertical lines point out the best cost values achieved during all the learning process.\nAll the systems use exactly the same RNN topology and the same optimization procedure, the only difference is the way the training samples are drawn. The baseline system, represented by the black solid curves, consists in shuffling the dataset differently at each epoch, i.e. randomly drawing training samples with flat priors and without replacement. The dashed and dotted yellow curves represent the convergence obtained with a \u201cby-hand\u201d curriculum, starting to train on isolated words, and then training on lines (the switch was done when no more improvement is made by continuing training of words, looking at the performance on the validation set of lines). Finally, the red dashed curve represents the continuous Curriculum approach presented in section 2.3.\nIn the case of the IAM database, a great improvement is achieved by using Curriculum Learning, without any additional training time: the CER% is decreased from 22% to about 17%. In the case of the Rimes and the OpenHaRT databases, the improvement in performance is slight, but the rate of convergence speed up is remarkable: the whole learning process is roughly twice shorter.\nThe impact of the Curriculum strategy is visible at the beginning of the training, where the cost functions can be decreased very fast. However, after this initial fast training phase has been completed, and after a transitory phase, the convergence rates are suddenly particularly low whereas the training is not finished. Yet this difficulty to \u201cstop learning\u201d affects all the systems, and indicates that another strategy than the Curriculum should be used to speed up this last learning phase. For instance, techniques to compute a forced alignment [21].\nAdditional experiments show that, when adapting a RNN that has already been trained and that is able to recognize a good part of the characters, the Curriculum strategy does not improve over the purely random baseline strategy (neither in performance nor in speed), even in cases where the CER% was high on the new database on which to adapt. This confirms that implementing a Curriculum based on the sequence length can play a crucial role at the beginning of the learning process, but does not affect convergence speed any more after the RNN has learned to detect the positions of the characters.\nThe fact that Curriculum Learning can improve generalization performance supports one point mentioned by [6], namely that the networks optimized by stochastic gradient descent are greatly influenced by early training samples. By choosing these samples and modifying the initial learning steps, Curriculum learning is similar to other methods devoted to optimize deep models such as careful initialization [23] and unsupervised pre-training [1, 6]. However, it is complementary and can be used in combination with these methods."}, {"heading": "4 Conclusion", "text": "This paper describes an easy-to-implement strategy to speed up the learning process, that can also provide better performance in the end. The principle is to build a curriculum based on the lengths of the target sequences. Experimental results show that in the case of Recurrent Neural Network for text line recognition optimized by stochastic gradient descent, the first phase of the learning can be drastically shorten, and the generalization performance can be improved, especially when the training set is limited.\nAt the same time, the slowness of the last phase of the learning remains an issue, that has to be investigated in the future. Further research also includes to experiment our Curriculum Learning procedure in combination with more elaborated optimization methods [15, 23]."}, {"heading": "Acknowledgments", "text": "This work was supported by the French Research Agency under the contract Cognilego ANR 2010- CORD-013."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Curriculum learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston"], "venue": "In Proc. of the International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Active batch learning with stochastic query-by-forest", "author": ["Alexander Borisov", "Eugene Tuv", "George Runger"], "venue": "JMLR: Workshop and Conference Proceedings,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["Jeffrey L. Elman"], "venue": "Cognition, 48:781\u2013799,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pierre-Antoine Manzagol", "Pascal Vincent"], "venue": "Journal of Machine Learning Research, JMLR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fernandez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. of the International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fernandez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Icdar 2011: French handwriting recognition competition", "author": ["Emmanuele Grosicki", "Haikal El Abed"], "venue": "In Proc. of the Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Sepp Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "How do humans teach: On curriculum learning and teaching dimension", "author": ["Faisal Khan", "Xiaojin (Jerry) Zhu", "Bilge Mutlu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Flexible shaping: how learning in small steps helps", "author": ["Kai A. Krueger", "Peter Dayan"], "venue": "Cognition, 110:380\u2013394,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "Proc. of the International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "The iam-database: an english sentence database for offline handwriting recognition", "author": ["Urs-Viktor Marti", "Horst Bunke"], "venue": "International Journal on Document Analysis and Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "The A2iA French handwriting recognition system at the Rimes-ICDAR2011 competition", "author": ["Far\u00e8s Menasi", "J\u00e9r\u00f4me Louradour", "Anne-laure Bianne-bernard", "Christopher Kermorvant"], "venue": "In Document Recognition and Retrieval Conference,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research, JMLR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Active sampling for class probability estimation and ranking", "author": ["Maytal Saar-tsechansky", "Foster Provost"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Neural network learning control of robot manipulators using gradually increasing task difficulty", "author": ["T.D. Sanger"], "venue": "IEEE Trans. on Robotics and Automation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Stabilize sequence learning with recurrent neural networks by forced alignment", "author": ["Marc-Peter Schambach", "Sheikh Faisal Rashid"], "venue": "In Proc. of the Int. Conf. on Document Analysis and Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "From baby steps to leapfrog: How \u201cless is more\u201d in unsupervised dependency parsing", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky"], "venue": "In IN NAACL-HLT,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proc. of the International Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "On the utility of curricula in unsupervised learning of probabilistic grammars", "author": ["Kewei Tu", "Vasant Honavar"], "venue": "In Proc. of the Twenty-Second International Joint Conference on Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}], "referenceMentions": [{"referenceID": 8, "context": "At the time being, the most powerful models for this task are Recurrent Neural Networks (RNN) with several layers of multi-directional Long-Short Term Memory (LSTM) units [9, 17].", "startOffset": 171, "endOffset": 178}, {"referenceID": 16, "context": "At the time being, the most powerful models for this task are Recurrent Neural Networks (RNN) with several layers of multi-directional Long-Short Term Memory (LSTM) units [9, 17].", "startOffset": 171, "endOffset": 178}, {"referenceID": 2, "context": "Because of the numerous non-linear functions that are composed, they are exposed to the burden of exploding and vanishing gradient [3, 11, 18].", "startOffset": 131, "endOffset": 142}, {"referenceID": 10, "context": "Because of the numerous non-linear functions that are composed, they are exposed to the burden of exploding and vanishing gradient [3, 11, 18].", "startOffset": 131, "endOffset": 142}, {"referenceID": 17, "context": "Because of the numerous non-linear functions that are composed, they are exposed to the burden of exploding and vanishing gradient [3, 11, 18].", "startOffset": 131, "endOffset": 142}, {"referenceID": 14, "context": "There are other ways to efficiently learn RNN, namely enhanced optimization approaches such as second-order methods [15] or good initialization with momentum [23].", "startOffset": 116, "endOffset": 120}, {"referenceID": 22, "context": "There are other ways to efficiently learn RNN, namely enhanced optimization approaches such as second-order methods [15] or good initialization with momentum [23].", "startOffset": 158, "endOffset": 162}, {"referenceID": 6, "context": "Secondly, RNN are used here for an unconstrained \u201cTemporal Classification\u201d task [7, 8], where the length of the sequence to recognize is in general different from the length of the input sequence.", "startOffset": 80, "endOffset": 86}, {"referenceID": 7, "context": "Secondly, RNN are used here for an unconstrained \u201cTemporal Classification\u201d task [7, 8], where the length of the sequence to recognize is in general different from the length of the input sequence.", "startOffset": 80, "endOffset": 86}, {"referenceID": 1, "context": "Here we propose to make the training process more effective by using the concept of Curriculum Learning, that has already been successfully applied in the context of deep models and Stochastic Gradient Descent [2].", "startOffset": 210, "endOffset": 213}, {"referenceID": 1, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 70, "endOffset": 81}, {"referenceID": 21, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 70, "endOffset": 81}, {"referenceID": 23, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 70, "endOffset": 81}, {"referenceID": 19, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "This idea has been exploited in classification [2], grammar induction [5, 22, 24], robotics [20], cognitive science [13] and human teaching [12].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "This is why a Connectionist Temporal Classification (CTC) approach as proposed by [7] is a very practical way to train RNN models without intensive labeling effort.", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "In their CTC approach, [7] efficiently compute and derive a cost function that is the Negative LogLikelihood (NLL), with the assumptions that all the frame probabilities are independent and that all possible alignments are equiprobable.", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "But it also unveils a vague localization of the characters, especially at the beginning of the training process, when the RNN gives quasi-random guesses for the posteriors of the labels (see the CTC Error Signal of Figure 4 in [7]).", "startOffset": 227, "endOffset": 230}, {"referenceID": 20, "context": "Several studies about Text Recognition have revealed that the training process of RNN is particularly long [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 1, "context": "Thinking in the same spirit as [2], let us make a parallel with how to teach kids to read and write.", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "RNN cannot decode paragraphs, just single lines: the common RNN architectures collapse the 2D input image into a 1D signal just before aligning using CTC [9].", "startOffset": 154, "endOffset": 157}, {"referenceID": 18, "context": "The idea of defining such a probability for probing the training database has already been successfully applied in Active Learning [19, 4].", "startOffset": 131, "endOffset": 138}, {"referenceID": 3, "context": "The idea of defining such a probability for probing the training database has already been successfully applied in Active Learning [19, 4].", "startOffset": 131, "endOffset": 138}, {"referenceID": 0, "context": "\u2022 shortness \u2208 [0, 1] is a bounded value to represent how easy is a training sample.", "startOffset": 14, "endOffset": 20}, {"referenceID": 15, "context": "\u2022 the IAM database, a dataset containing pages of handwritten English text [16], \u2022 the Rimes database2, a dataset of handwritten French letters used in several ICDAR competitions (lastly ICDAR 2011 [10, 17]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "\u2022 the IAM database, a dataset containing pages of handwritten English text [16], \u2022 the Rimes database2, a dataset of handwritten French letters used in several ICDAR competitions (lastly ICDAR 2011 [10, 17]).", "startOffset": 198, "endOffset": 206}, {"referenceID": 16, "context": "\u2022 the IAM database, a dataset containing pages of handwritten English text [16], \u2022 the Rimes database2, a dataset of handwritten French letters used in several ICDAR competitions (lastly ICDAR 2011 [10, 17]).", "startOffset": 198, "endOffset": 206}, {"referenceID": 8, "context": "It is the same as described in [9], except that the sizes of the filters have been adapted to images in 300 dpi: we used 2x2 input tiling, and 2x4 filters in the two sub-sampling hidden layers (which are convolutional layers without overlap between the filters).", "startOffset": 31, "endOffset": 34}, {"referenceID": 13, "context": "All the models were optimized using Stochastic Gradient Descent [14]: a model update happens after each training sample (i.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "In our case, we use Stochastic Gradient Descent [14] and the unit of progression could be for instance the number of updates, that is the number of training samples that have been browsed.", "startOffset": 48, "endOffset": 52}, {"referenceID": 6, "context": "We remind that the cost optimized using CTC [7] is the Negative Log-Likelihood (NLL), which can be averaged over the number of training sequences.", "startOffset": 44, "endOffset": 47}, {"referenceID": 20, "context": "For instance, techniques to compute a forced alignment [21].", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": "The fact that Curriculum Learning can improve generalization performance supports one point mentioned by [6], namely that the networks optimized by stochastic gradient descent are greatly influenced by early training samples.", "startOffset": 105, "endOffset": 108}, {"referenceID": 22, "context": "By choosing these samples and modifying the initial learning steps, Curriculum learning is similar to other methods devoted to optimize deep models such as careful initialization [23] and unsupervised pre-training [1, 6].", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "By choosing these samples and modifying the initial learning steps, Curriculum learning is similar to other methods devoted to optimize deep models such as careful initialization [23] and unsupervised pre-training [1, 6].", "startOffset": 214, "endOffset": 220}, {"referenceID": 5, "context": "By choosing these samples and modifying the initial learning steps, Curriculum learning is similar to other methods devoted to optimize deep models such as careful initialization [23] and unsupervised pre-training [1, 6].", "startOffset": 214, "endOffset": 220}, {"referenceID": 14, "context": "Further research also includes to experiment our Curriculum Learning procedure in combination with more elaborated optimization methods [15, 23].", "startOffset": 136, "endOffset": 144}, {"referenceID": 22, "context": "Further research also includes to experiment our Curriculum Learning procedure in combination with more elaborated optimization methods [15, 23].", "startOffset": 136, "endOffset": 144}], "year": 2013, "abstractText": "Recurrent Neural Networks (RNN) have recently achieved the best performance in off-line Handwriting Text Recognition. At the same time, learning RNN by gradient descent leads to slow convergence, and training times are particularly long when the training database consists of full lines of text. In this paper, we propose an easy way to accelerate stochastic gradient descent in this set-up, and in the general context of learning to recognize sequences. The principle is called Curriculum Learning, or shaping. The idea is to first learn to recognize short sequences before training on all available training sequences. Experiments on three different handwritten text databases (Rimes, IAM, OpenHaRT) show that a simple implementation of this strategy can significantly speed up the training of RNN for Text Recognition, and even significantly improve performance in some cases.", "creator": "LaTeX with hyperref package"}}}