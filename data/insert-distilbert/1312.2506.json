{"id": "1312.2506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Dec-2013", "title": "An Application of Answer Set Programming to the Field of Second Language Acquisition", "abstract": "this paper explores the contributions of answer set programming ( np asp ) to the study instead of an established theory from the field of second language acquisition : input processing. moreover the theory describes default strategies that influence learners of a second language use in extracting meaning out translation of a text, based on their knowledge of the complete second language and their background knowledge about feeding the world. we gradually formalized this theory in asp, and as a result we were able to determine opportunities for teachers refining its natural language skills description, as well forth as directions suited for future theory development. we applied our model to automating the automated prediction of how learners of english would interpret sentences containing the passive voice. we present a system, dewey pias, that uses these predictions to assist basic language instructors in designing teaching materials. to appear in theory and classroom practice of logic programming ( tplp ).", "histories": [["v1", "Mon, 9 Dec 2013 16:37:22 GMT  (40kb)", "http://arxiv.org/abs/1312.2506v1", "17 pages, 3 tables, to appear in Theory and Practice of Logic Programming (TPLP)"]], "COMMENTS": "17 pages, 3 tables, to appear in Theory and Practice of Logic Programming (TPLP)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniela inclezan"], "accepted": false, "id": "1312.2506"}, "pdf": {"name": "1312.2506.pdf", "metadata": {"source": "CRF", "title": "An Application of Answer Set Programming to the Field of Second Language Acquisition", "authors": ["Daniela Inclezan"], "emails": ["inclezd@MiamiOH.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n25 06\nv1 [\ncs .A\nI] 9\nKEYWORDS: answer set programming, second language acquisition, qualitative scientific theories, natural language"}, {"heading": "1 Introduction", "text": "This paper extends a relatively new line of research that explores the contributions of Answer Set Programming (ASP) (Gelfond and Lifschitz 1991; Niemela\u0308 1998; Marek and Truszczynski 1999) to the study and refinement of qualitative scientific theories (Balduccini and Girotto 2010; Balduccini and Girotto 2011). As pointed out by Balduccini and Girotto (2010), qualitative theories tend to be formulated in natural language, often in the form of defaults. Modeling these theories in a precise mathematical language can assist scientists in analyzing their theories, or in designing experiments for testing their predictions. It was shown that ASP is a suitable tool for this task (Balduccini and Girotto 2010; Balduccini and Girotto 2011), as it provides means for an elegant and accurate representation of defaults, dynamic domains, and incomplete information, among others. In our work, we explore the applicability of ASP to the formalization and analysis of a theory from the field of Second Language Acquisition \u2014 a discipline that studies the processes by which people learn a second language.1\n1 In the field of Second Language Acquisition, the expression \u201csecond language\u201d denotes any language that is acquired after the first one.\nin ASP can benefit the future development of this theory. In particular, we will focus on contributions to (1) the refinement of this theory; (2) the automated testing of its statements; and (3) the development of practical applications for language teaching and testing.\nThe theory we consider is VanPatten\u2019s Input Processing theory (VanPatten 1984; VanPatten 2004). We chose it because it is an established theory in the field of Second Language Acquisition, with important consequences on foreign language education. It is specified in English in the form of a compact set of principles. Input Processing (IP) describes the default strategies that second language learners use to get meaning out of text written or spoken in the second language, during tasks focused on comprehension, given the learners\u2019 limitations in vocabulary, working memory, or internalized knowledge of grammatical structures. As a result of applying these strategies, even learners with limited grammatical expertise can often, but not always, interpret input sentences correctly. Once grammatical information is internalized, the default strategies are overridden by the always reliable grammatical knowledge. Hence, it can be said that IP describes an example of nonmonotonic reasoning.\nIP predicts that beginner learners of English reading the sentence \u201cThe cat was bitten by the dog\u201d would only be able to retrieve the meanings of the words \u201ccat\u201d, \u201cbitten\u201d, and \u201cdog\u201d and end up with something like the sequence of concepts CATBITE-DOG. Although they may notice the word \u201cwas\u201d or the ending \u201c-en\u201d of the verb \u201cbitten\u201d, they would not be able to process them (i.e., connect them with the function they serve, which is to indicate passive voice) because of limitations in processing resources. In this context, the expression processing resources (or simply resources) refers to the amount of information that a learner can hold and process in his/her working memory during real time comprehension of input sentences. Additionally, IP predicts that the sentence above, now mapped into the sequence of concepts CAT-BITE-DOG, would be incorrectly interpreted by these learners as \u201cThe cat bit the dog\u201d because of a hypothesized strategy of assigning agent status to the first noun of a sentence.\nIP, as described by VanPatten (2004), consists of two principles formulated as defaults. Each principle contains sub-principles that represent refinements of, or exceptions to, the original defaults. For example, a sub-principle of IP predicts that beginner learners of English would correctly interpret the sentence \u201cThe shoe was bitten by the dog\u201d because agent status cannot be assigned to the first noun, as a shoe cannot bite. This can happen even if the learner has not yet internalized the structure of the passive voice in English or did not have the resources to process it in the above sentence. Similarly for the sentence \u201cThe man was bitten by the dog\u201d because it is unlikely for a man to bite a dog. These strategies can also be applied to stories consisting of several sentences where information from previous sentences conditions the interpretation of latter ones. For example, the second sentence of the story: \u201cThe cat killed the dog. Then, the dog was pushed by the cat.\u201d would be interpreted correctly even by beginner learners, because a dead dog cannot push. IP was shown to be applicable to other grammatical forms (e.g., clitic pronouns,\npendently from the learners\u2019 native language (VanPatten 1984).\nASP is a natural choice for modeling the IP theory, first of all because defaults and their exceptions can be represented in ASP in an elegant and precise manner. Moreover, IP takes into consideration the learners\u2019 knowledge about the dynamics of the world (e.g., people know under what conditions a biting action can occur); in ASP, there is substantial research on how to represent actions and dynamic domains in which change is caused by actions (Gelfond and Lifschitz 1998; Balduccini and Gelfond 2003). All these features of ASP were useful in creating a formalization of IP, as shown in Section 2. We demonstrate how the process of modeling IP in ASP allowed us to analyze the theory\u2019s natural language description. As a result, we were able to notice some areas that need more clarification or could be further investigated. Next, we used our formalization of IP in making automated predictions about how learners would interpret simple sentences and paragraphs containing the passive voice in English. This contribution, described in Section 3, can facilitate the testing of the statements of IP or the tuning of its parameters. Based on these predictions, we created a system, PIas , that can assist language teachers in designing instructional materials, as discussed in Section 4. PIas relies on the guidelines of an established teaching method\u2014Processing Instruction (VanPatten 1993; VanPatten 2002)\u2014that is based on the principles of Input Processing. We end the paper with conclusions and directions for future work.\nThe current article extends a previous version of our work (Inclezan 2012). In\nthe remainder of the paper, we assume the reader\u2019s familiarity with ASP."}, {"heading": "2 An Analysis of IP Based on Its ASP Model", "text": "In this section, we describe our formalization of IP and demonstrate that using the precise language of ASP for this purpose can highlight opportunities for a future refinement and improvement of this theory."}, {"heading": "2.1 Logic Form Encoding of a Text", "text": "The IP theory assumes that a learner is given a text (called input in the enunciation of IP) \u2014 a paragraph with one or more sentences. Our logic form encoding of a text uses three sorts, words , sentences , and paragraphs , and two relations:\n\u2022 word of sent(K , S ,W ) \u2013 the K th word of sentence S is W ; \u2022 sent of par(K ,P , S ) \u2013 the K th sentence of paragraph P is S .\nFor example, the paragraph \u201cThe cat killed the dog. Then, the dog was pushed by the cat.\u201d in the introduction is encoded as: sent of par(1, p, s1)\u00b7\nsent of par(2, p, s2)\u00b7 word of sent(1, s1, \u201cthe\u201d) \u00b7 . . . word of sent(5, s1, \u201cdog\u201d)\u00b7 word of sent(1, s2, \u201cthen\u201d) \u00b7 . . . word of sent(8, s2, \u201ccat\u201d)\u00b7\nPrinciple 1 of IP describes how likely it is for words in a sentence to get processed by a learner engaged in a real time comprehension task, depending on the grammatical category to which words belong. In other words, given a sentence and a learner\u2019s knowledge of the second language, Principle 1 predicts a possibly partial mapping of words of this sentence into cognitive concepts.\nPrinciple 1 makes reference to certain linguistic terms: a lexical item is the basic unit of the mental vocabulary (e.g., \u201ccat\u201d, \u201clook for\u201d). Content words are those that carry the meaning of a sentence: nouns, verbs, adjectives, and adverbs. Forms, also called \u201cgrammatical structures\u201d, are inflections, articles, or particles (e.g., the thirdperson-singular marker \u201c-s\u201d attached to verbs as in \u201cmakes\u201d; the article \u201cthe\u201d). It is assumed that learners that have internalized a form fully also know, implicitly, whether that form is meaningful, which means that it contributes meaning to the overall comprehension of a sentence, or not.2 Similarly, they are able to distinguish between redundant and nonredundant meaningful forms, where a redundant form is one whose meaning can usually be retrieved from other parts of a sentence. Finally, the expression \u201cprocessing resources\u201d refers to resources available in the learner\u2019s working memory for holding and processing incoming information.\nPrinciple 1 is formulated by VanPatten (2004) as follows:\n1. The Primacy of Meaning Principle: Learners process input for meaning before they process it for form.\n1a. The Primacy of Content Words Principle: Learners process content words in\nthe input before anything else.\n1b. The Lexical Preference Principle: Learners will tend to rely on lexical items\nas opposed to grammatical form to get meaning when both encode the same semantic information. 1c. The Preference for Nonredundancy Principle: Learners are more likely to\nprocess nonredundant meaningful grammatical forms before they process redundant meaningful forms. 1d. The Meaning-Before-Nonmeaning Principle: Learners are more likely to pro-\ncess meaningful grammatical forms before nonmeaningful forms irrespective of redundancy. 1e. The Availability of Resources Principle: For learners to process either redun-\ndant meaningful grammatical forms or nonmeaningful forms, the processing of overall sentential meaning must not drain available processing resources. 1f. The Sentence Location Principle: Learners tend to process items in sentence\ninitial position before those in final position and these latter in turn before those in medial position (all other processing issues being equal).\n2 An example of a nonmeaningful form is grammatical gender inflection in Romance languages, which manifests itself on words associated with nouns, such as determiners. For instance, in Spanish, \u201cthe moon\u201d is feminine (\u201cla luna\u201d), while \u201cthe sun\u201d is masculine (\u201cel sol\u201d). The form is not meaningful because it does not reflect a \u201cbiological difference in the real world\u201d (VanPatten 2003), i.e., grammatical gender does not equal biological gender.\nLet us show what predictions Principle 1 makes about the processing of words from the sentence:\nS1. The cat was bitten by the dog.\nAccording to 1a, content words have the highest chance of getting processed, in this case: \u201ccat\u201d, \u201cbitten\u201d, and \u201cdog\u201d. Among them, based on 1f, \u201ccat\u201d has the highest chance as it is in sentence initial position, followed by \u201cdog\u201d in final position, and then by \u201cbitten\u201d in medial position.\nThe next chance belongs to meaningful forms, based on 1d, in this case: \u201cthe\u201d, \u201cwas\u201d, as well as \u201ccat\u201d as an indicator of third-person singular, and \u201cbitten\u201d (more precisely the suffix \u201c-en\u201d) as an indicator of passive voice. According to 1c, out of these forms, the nonredundant ones are more likely to get processed, in particular the definite article \u201cthe\u201d and the word \u201ccat\u201d as an indicator of third-person singular, both in initial position, followed by \u201cthe\u201d in final sentence position, and then by the forms \u201cwas\u201d as an indicator of past tense and \u201cbitten\u201d as an indicator of passive voice in medial position.\nPrinciple 1e says that the whole sentence has the next chance of getting processed, followed by the redundant form \u201cby\u201d. Finally, according to 1b, the redundant form \u201cwas\u201d (i.e., the suffix \u201c-s\u201d) as an indicator of third-person singular may or may not get processed, independently of available resources, because its meaning was already obtained from the word \u201ccat\u201d. Note that, how many words actually get processed depends on the resource capacity of a learner.\nEncoding a Learner\u2019s Knowledge of the Second Language The IP theory is supposed to be applicable independently from the mental model of a second language that is assumed (VanPatten 2004). This allows us to make the simplification of not considering inflections on a word (e.g., \u201c-s\u201d, \u201c-en\u201d) separately from the rest of the word. As a result, a word can be viewed as belonging to multiple categories. For instance, \u201cmakes\u201d can be viewed as a content word referring to the action of making something; it can also be perceived as a form indicating that the doer is not the speaker nor the addressee and that the action is occurring in the present (due to the ending \u201c-s\u201d).\nGiven the categories listed in Principle 1, we divide words into two subclasses,\ncontent words and forms ; forms are divided intom forms (meaningful) and nm forms (nonmeaningful), while m forms are further divided into r m forms (redundant) and nr m forms (nonredundant). The leaves of this hierarchy are denoted by a special sort, leaf ctg. All nodes of the hierarchy are denoted by the sort category. We introduce a new sort, concept , denoting language-independent cognitive concepts, such as entities, actions, or semantic concepts (e.g., the concepts of past tense and passive voice). Additionally, we specify a learner\u2019s knowledge of the second language using:\n\u2022 in(W ,Ctg) \u2013 word W belongs to category Ctg;\n\u2022 meaning(W ,Ctg,C ) \u2013 word W interpreted as a member of category Ctg has\nthe meaning C .\nAfter careful analysis, it is clear that Principle 1 specifies a partial order between words in a sentence, given the category they belong to and their sentential position. Greater elements in this ordering have more chances of being processed than lesser elements. It is important to note that we only realized that a partial order was described when attempting to formulate Principle 1 in ASP. The fact was not immediately obvious to us because the sub-principles specifying an order based on word categories (1a, 1c, 1d) and sentential position (1f) are not grouped together in the text of the theory.3 Hence, we can say that modeling IP in ASP led us to a better understanding of the theory.\nWe start formalizing Principle 1 by looking at its sub-principles 1a, 1c, and 1d, which describe a partial order on word categories. To model it, we define a relation is ml ctg on categories, where is ml ctg(Ctg1,Ctg2) says that words from category Ctg1 are more likely to get processed than words from category Ctg2. Based on Principles 1a, 1d, and 1c, respectively, we have the facts:\nis ml ctg(content words , forms)\u00b7 is ml ctg(m forms , nm forms)\u00b7 is ml ctg(nr m forms , r m forms)\u00b7\nNext, we look at Principle 1f, which describes a similar partial order on sentence positions. To specify the different possible sentence positions, we define a sort sentence position with three elements: initial , medial , and final . We use a relation is ml pos(Pos1,Pos2), which says that words in sentence position Pos1 are more likely to be processed than words in Pos2 (as long as they belong to the same word category). We encode Principle 1f via the facts:\nis ml pos(initial , final)\u00b7 is ml pos(final ,medial)\u00b7\nBy ml ctg and ml pos , respectively, we denote the transitive closures of the two relations above. In addition, we extend the relation ml ctg down to subclasses of categories, but not upwards to superclasses.\nBased on the two relations above, we can now define the partial relation between words, given their category and position. Our modeling process illuminated the fact that the IP theory does not say how many words starting from the beginning of a sentence are part of the \u201csentence initial position.\u201d This expression needs to be precisely defined in the future. For the moment, we define initial position as the first n words of a sentence, where n is a parameter of the encoding. Similarly for final positions. We use the relation pos(K , S ,Pos) to say that the K th word of sentence S is in Pos sentence position. We introduce a relation ml wrd(K1, S ,Ctg1,K2,Ctg2), which says that the K th 1 word of S is more likely to get processed for its interpretation as an element of the category Ctg1 than the K th 2 word of the same sentence\n3 Sub-principles 1b and 1e describe constraints that can further limit the chances of a word to get processed.\nml wrd(K1, S ,Ctg1,K2,Ctg2) \u2190 leaf ctg(Ctg1), leaf ctg(Ctg2),\nword of sent(K1, S ,W1), in(W1,Ctg1), word of sent(K2, S ,W2), in(W2,Ctg2), ml ctg(Ctg1,Ctg2)\u00b7\nml wrd(K1, S ,Ctg,K2,Ctg) \u2190 leaf ctg(Ctg),\nword of sent(K1, S ,W1), in(W1,Ctg), word of sent(K2, S ,W2), in(W2,Ctg), pos(K1, S ,Pos1), pos(K2, S ,Pos2), ml pos(Pos1,Pos2)\u00b7\nThe first rule relates to Principles 1a, 1c, and 1d, as it is based on the ordering of categories; the second rule is about Principle 1f, as it uses the sentence position ordering, for a given category of words. The effects of the ordering ml wrd on the processing of words of a sentence will be seen later.\nNext, we specify that, normally, a word will get processed (i.e., be mapped into a concept) if enough resources are available. We introduce a relation map(K , S , Ctg,C ), which says that the K th word of S was processed according to category Ctg and was mapped into concept C . We encode Principle 1 as:\nmap(K , S ,Ctg,C ) \u2190 word of sent(K , S ,W ), in(W ,Ctg),\nleaf ctg(Ctg), meaning(W ,Ctg,C ), enough resources available(K , S ,Ctg), not ab(dmap(K , S ,Ctg,C ))\u00b7\n(1)\nThe IP theory does not give any details about the initial resources in working memory available to a learner for processing a sentence, nor about how learners at different levels of proficiency consume those resources while attaching meaning to words. This is another aspect of the theory that needs more careful consideration. To solve this issue, we created a simple model of resources, in which we assume a fixed resource capacity available per sentence; this capacity decreases by one unit with each association of meaning to a word. We introduced a predicate resources consumed(N ,K , S ,Ctg), which says that N resources are consumed in processing those words that are more likely to get processed than the K th word of sentence S for category Ctg. The definition of this relation captures the implications of the ordering ml wrd on the processing of words:\nresources consumed(N ,K , S ,Ctg) \u2190\nword of sent(K , S ,W ), in(W ,Ctg), leaf ctg(Ctg), N = #count {ml wrd(K1, S ,Ctg1,K ,Ctg) :\nleaf ctg(Ctg1) }\u00b7\nAlthough this model does not reflect the complexities of working memory, it is enough for our purposes, as the IP theory only focuses on the expected end result of processing a sentence in working memory: certain word-to-concept associations will be made while others will not. To model Principle 1e, we assume that processing the whole sentence decreases available resources by one unit.\nvided by VanPatten (2004), its meaning is that a form that is normally redundant may not be processed at all if it is actually redundant in that sentence (i.e., its meaning was already extracted from some other word). We encode this knowledge as a possible weak exception to the default in the rule for predicate map, via a disjunctive rule:\nab(dmap(K , S ,Ctg,C )) or \u00acab(dmap(K , S ,Ctg,C )) \u2190\nword of sent(K , S ,W ), meaning(W ,Ctg,C ), word of sent(K1, S ,W1), K 6= K1, W 6= W1, map(K1, S ,Ctg1,C )\u00b7\n(2)\nThe informal reading of this axiom is that meanings that are actually redundant in a sentence may or may not be exceptions to the default for relation map."}, {"heading": "2.3 The Second Principle of Input Processing", "text": "Principle 2 describes the strategies that learners employ to understand the meaning of a sentence. The input of Principle 2 is the output of Principle 1 for a given sentence (i.e., a mapping of words to concepts), together with the learner\u2019s background knowledge about the world. Its output is an event denoting the meaning extracted by the learner from that sentence. When considering a story consisting of several sentences, the output of Principle 2 is a series of events that correspond to the sentences in that paragraph. For simplicity, we assume here that each sentence describes a single event, and that sentences of a story describe events in the order in which those events happened. Principle 2 is formulated by VanPatten (2004; 2002) as follows:\n2. The First Noun Principle (FNP): Learners tend to process the first noun or pronoun they encounter in a sentence as the agent.\n2a. The Lexical Semantics Principle: Learners may rely on lexical semantics,4\nwhere possible, instead of on word order to interpret sentences. 2b. The Event Probabilities Principle: Learners may rely on event probabilities,\nwhere possible, instead of on word order to interpret sentences. 2c. The Contextual Constraint Principle: Learners may rely less on the First\nNoun Principle if preceding context constrains the possible interpretation of\na clause or sentence. 2d. Prior Knowledge: Learners may rely on prior knowledge, where possible, to\ninterpret sentences. 2e. Grammatical Cues: Learners will adopt other processing strategies for gram-\nmatical role assignment only after their developing system5 has incorporated other cues.\n4 Lexical semantics refers to the meaning of lexical items. 5 Developing system refers to the representation of grammatical knowledge in the mind of the second language learner. This representation changes as the learner acquires more knowledge.\nWe illustrate the predictions made by Principle 2 for several sentences. First, we consider the case of beginner learners, who have limited resources and vocabulary, and can only process the content words out of a sentence. Based on Principle 1, beginners would map the words \u201ccat\u201d, \u201cbitten\u201d, and \u201cdog\u201d in sentence S1 from Example 1 into the concepts CAT, BITE, and DOG respectively and would not be able to process any other words. Principle 2 predicts that beginners would assign agent status to the first noun in S1 and hence interpret S1 incorrectly as \u201cThe cat bit the dog.\u201d\nBeginners are expected to correctly interpret the sentence: S2. The shoe was bitten by the dog.\nas a shoe cannot bite a dog (lexical semantics). Based on Principle 2a, lexical semantics override the assignment of agent status to the first noun. The sentence:\nS3. The man was bitten by the dog.\nis also supposed to be interpreted correctly by beginners because men normally do not bite animals (event probabilities and Principle 2b).\nPrinciple 2d predicts the correct interpretation of: S4. Holyfield was bitten by Tyson.\nassuming that learners have the prior knowledge that Tyson bit Holyfield.\nLet us now consider some short paragraphs:\nP1. (S5.) The cat pushed the dog. (S6.) Then, the dog was bitten by the cat.\nSentence S6 is supposed to be incorrectly interpreted by beginners because none of the Principles 2a-e applies. Instead, the second sentence of the paragraph:\nP2. (S7.) The cat killed the dog. (S8.) Then, the dog was pushed by the cat.\nwould be interpreted correctly due to lexical semantics in context, as predicted by Principles 2a and 2c together.\nFinally, let us consider advanced learners who possess enough resources and a large vocabulary, which allow them to map all words of a sentence into concepts. According to Principle 2e, these learners are expected to interpret all above sentences correctly, as they are able to detect the use of the active or passive voice and can rely on grammatical cues for sentence interpretation.\nEncoding a Learner\u2019s Background Knowledge about the World Learners are assumed to possess some background knowledge about the world and its dynamics. Three important types of information are supposed to be derivable from this knowledge base, and we capture them using the predicates:\n\u2022 impossible(Ev , I ) \u2013 event Ev is physically impossible to occur at step I of the\nnarrated story;\n\u2022 unlikely(Ev , I ) \u2013 event Ev is unlikely to occur at step I of the narrated story; \u2022 hpd(Ev) \u2013 event Ev is known to have happened in reality.\nTo model the background knowledge base of a learner, we use known methodologies for representing dynamic domains in ASP (Gelfond and Lifschitz 1998; Balduccini and Gelfond 2003). As a result, atoms of the type impossible(Ev , I ) are derived from axioms specifying preconditions for the execution of actions (i.e., executability conditions);\ntheir exceptions (Baral and Gelfond 1994); hpd(Ev) atoms are simply stored as a collection of facts.6 Note that our formalization of the IP theory is independent from the underlying model of the world and its dynamics. This means that other models could be easily coupled with our formalization of Principle 2, as long as the model can derive atoms of the three types mentioned above.\nOur ASP Model of Principle 2\nWe assume that each sentence in the input describes exactly one event, and that the N th sentence of a paragraph describes the N th occurring event.\nWe start by introducing some terminology. By the direct (reverse) meaning of a sentence we mean the action denoted by the verb of the sentence, and whose agent is the entity denoted by the first (second) noun appearing in the sentence. For instance the direct meaning of \u201cThe dog was bitten by the cat\u201d is the event of \u201cthe dog biting the cat,\u201d while its reverse meaning is the event of \u201cthe cat biting the dog.\u201d We use the predicate dir rev m(Dir ,Rev , S ) to say that Dir is the direct meaning and Rev is the reverse meaning of sentence S .\nPrinciple 2, also called the First Noun Principle (FNP), is a default statement and its sub-principles express exceptions to it. To encode Principle 2, we use a relation extr m(Ev , S , fnp) saying that the learner extracted the meaning Ev from sentence S by applying FNP:\nextr m(Dir , S , fnp) \u2190 not extr m(Rev , S , fnp),\ndir rev m(Dir ,Rev , S )\u00b7\nThe rule says that learners applying the FNP will extract the direct meaning from a sentence, unless they extract the reverse meaning.\nWe represent Principle 2a using the axiom:\nextr m(Rev , S , fnp) \u2190 impossible(Dir , I ),\nnot impossible(Rev , I ), dir rev m(Dir ,Rev , S ), sent of par(I ,P , S )\u00b7\nInformally, it says that learners will assign the reverse meaning to a sentence if this is a possible meaning, and the direct meaning is impossible.\n6 We considered using probabilistic ASP languages such as P-log (Baral et al. 2009) to model event probabilities. However, we decided against it because we believe that our naive model is closer to how humans record information in their minds, and because finding the exact probability of an event (e.g., how likely it is for a cat to bite a dog) is a complex task in itself.\nextr m(Rev , S , fnp) \u2190 not impossible(Dir , I ),\nunlikely(Dir , I ), not hpd(Dir), not impossible(Rev , I ), not unlikely(Rev , I ), dir rev m(Dir ,Rev , S ), sent of par(I ,P , S )\u00b7\nI.e., a sentence will be assigned its reverse meaning if the direct meaning is possible, but unlikely and not known to have actually happened, while the reverse meaning may hypothetically occur (i.e., it is possible and not unlikely).\nPrinciple 2d is encoded as follows:\nextr m(Rev , S , fnp) \u2190 hpd(Rev),\ndir rev m(Dir ,Rev , S ), sent of par(I ,P , S )\u00b7\nThis says that a learner using the FNP will extract the reverse meaning if he knows that this event actually happened.\nThe preference for grammatical cues when such cues can be interpreted (Principle\n2e) is encoded via the rules:\nextr m(Ev , S ) \u2190 extr m(Ev , S , grm cues)\u00b7 extr m(Ev , S ) \u2190 extr m(Ev , S , fnp),\nnot extr m by(S , grm cues)\u00b7\nextr m by(S ,X ) \u2190 extr m(Ev , S ,X )\u00b7\nwhere extr m(Ev , S ) says that Ev is the meaning extracted from S ; extr m(Ev , S , grm cues) \u2013 the meaning Ev was extracted from S based on grammatical cues (which vary for different grammatical forms); and extr m by(S ,X ) \u2013 the meaning of S was extracted based on strategy X . The definition of extr m(Ev , S , grm cues), not shown here, captures the fact that different grammatical forms have different grammatical cues. For instance, the cues for passive voice in English are the past participle (e.g., \u201cbitten\u201d) and the passive voice auxiliary (e.g., \u201cwas\u201d). An extr m(Ev , S , grm cues) atom belongs to an answer set if the learner was able to map the main grammatical form(s) in the sentence (in the case of passive voice, the past participle and the passive voice auxiliary) into the corresponding abstract concept (here, passive voice), and if Ev is the correct interpretation of sentence S .\nIn our formalization of FNP, Principle 2c was embedded in the representation of Principles 2a, 2b, and 2d. The one thing left for contextual constraints is to record the events corresponding to the meaning extracted from previous sentences of the story, assuming the first time step of the story is 1.\noccurs(Ev , I ) \u2190 extr m(Ev , S ),\nsent of par(I ,P , S )\u00b7\nNotice that Principle 2c specifies that preceding sentences in a paragraph constrain the interpretation of latter sentences, but does not mention a possible effect\ntially processed incorrectly. Also, Principle 2 in general does not address sentences that describe events which cannot physically take place in the real world, unless understood metaphorically (e.g., \u201cThe dog was bitten by the shoe.\u201d). These are interesting directions of research that the IP theory could address."}, {"heading": "3 Automating the Predictions of IP", "text": "We used our model of the IP theory to generate automated predictions about how sentences like the ones in Examples 1 and 2 would be interpreted by learners of English. We considered two different types of learners: advanced and beginners. They both shared the same background knowledge about the world, but had different knowledge about the second language. The advanced learner internalized the meaning of all content words and forms in our vocabulary, whereas beginners would only master the meaning of content words, but not forms. For each type of learner, we created a logic program \u03a0 (indexed by either adv or beg) by putting together the corresponding knowledge of the second language, the background knowledge about the world, and the formalizations of the two principles. For any text X, by lp(X ) we denote the logic form encoding of X as presented in Section 2.1. The answer set(s) of the program \u03a0 \u222a lp(X ) corresponds to predictions of the IP theory about how a learner would interpret X .\nWe first run tests for Principle 1 by using sentence S1, copied here with its words\nannotated by their sentential indices for a better understanding of the results:\nS1. The1 cat2 was3 bitten4 by5 the6 dog7.\nWe set the sentence position parameter n to 2, and run experiments for different resource capacities. A scientist working on a refinement of IP theory could easily change the values of these parameters and thus use our formalization of IP to fine-tune them.\nThe answer sets of the program \u03a0adv \u222a lp(S1), computed using the ASP solver claspD (Drescher et al. 2008), contained the map facts presented in Table 1. An atom map(k , s1, ctg, c) in the answer set for capacity m indicates that the k th word of S1 will get processed by an advanced learner with capacitym, and be mapped into the cognitive concept c. The atom map(3, s1, r m forms , third person singular) on the last line of the table is marked with an asterisk because two answer sets are generated for capacity 11, and this atom is part of one answer set but not the other. The non-determinism comes from the disjunctive rule (2) that, together with rule (1), encodes Principle 1b stating that learners tend to extract meaning from content words rather than from forms when they both encode the same meaning. Rule (2) specifies that learners may or may not obey this default. In the case of S1, the form \u201cwas\u201d indicates (among other things) that the event is about an entity other than the speaker and the addressee (i.e., third person singular). However, this meaning was already extracted by the learner from the content word \u201ccat\u201d (see atom map(2, s1, nr m forms , third person singular) for capacity 9), hence the form \u201cwas\u201d may or may not be processed for the same meaning. Another thing to\nCapacity Additional map Facts w.r.t. Answer Sets for Smaller Capacities\n0 \u2205\n1 map(2, s1, content words, cat)\n2 map(7, s1, content words, dog)\n3 map(4, s1, content words, bite)\n9 map(1, s1,nr m forms, definite) map(2, s1,nr m forms, third person singular) map(6, s1,nr m forms, definite) map(3, s1,nr m forms, passive voice) map(3, s1,nr m forms, past tense) map(4, s1,nr m forms, past participle)\n11 map(5, s1, r m forms, agency) map(3, s1, r m forms, third person singular) \u2217\nnote is that a beginner learner would not be able to process grammatical forms in the sentence even if s/he had a capacity exceeding value 11, just because s/he has not yet internalized forms.\nNext, we tested our predictions for Principle 2 for beginners and advanced learners. In both cases, we set the resource capacity to value 11. The relevant parts of the answer sets for the texts in Example 2 can be seen in Tables 2 and 3, where terms like ev(bite, cat , dog) are used to denote events, in this case \u201ca cat biting a dog\u201d. We do not show all the predictions for advanced learners, because they are expected to interpret all sentences and paragraphs correctly.\nX Answer Set of \u03a0adv \u222a lp(X ) contains Is X interpreted correctly?\nS1 extr m(ev(bite, dog , cat), s1) YES extr m by(s1, grm cues)\nOur automated predictions matched the ones in Examples 1 and 2, which suggests\nthat our model of IP is correct.\n4 The System PIas\nWe created a system, PIas , designed to assist instructors in preparing materials for the passive voice in English. PIas follows the guidelines of a successful teaching method called Processing Instruction (PI) (VanPatten 1993; VanPatten 2002; VanPatten 2003; Lee and VanPatten 2003), developed based on the principles of IP. For a sentence to be valuable in this approach, it must lead to an incorrect interpretation when grammatical cues are not used but the FNP is. In other words, learners must be made aware that their default strategies can sometimes be counterproductive, whereas grammatical cues are always reliable. S1 above is an example of a valuable sentence; S2, S3, and S4 are not.\nPIas has two functions. The first one is to specify whether sentences and paragraphs created by instructors are valuable or not. This is relevant because even instructors trained in PI happen to create bad materials.7 We define:\nvaluable(S ) \u2190 extr m(Ev1, S , grm cues),\nextr m(Ev2, S , fnp), Ev1 6= Ev2\u00b7\nWe create a module M containing this definition and its extension to paragraphs. PIas takes as an input a sentence or paragraph X in natural language, encodes it in its logic form lp(X ), and computes the answer sets of a program consisting of \u03a0adv , M, and lp(X ). X is valuable if the atom valuable(X ) belongs to all answer sets of the resulting program.\nThe second function of PIas is to generate all valuable sentences given a vocabulary and some simple grammar. This is important because PI requires to expose learners to a large number of valuable sentences.8 We add to M rules for sentence\n7 A non-valuable sentence crafted by a researcher from the Second Language Acquisition community (Qin 2008) is \u201cThe ball was pushed by the rabbit.\u201d This sentence is not valuable (VanPatten et al. 2009) because event probabilities ensure that even beginner learners will interpret this sentence correctly \u2013 it is more likely for a rabbit to push a ball than for a ball to push a rabbit. 8 In their study, VanPatten and Cadierno (1993) used 120 valuable sentences for a single grammatical form.\nsentence(s(\u201cThe\u201d,N1, \u201cwas\u201d,V , \u201cby\u201d, \u201cthe\u201d,N2)) \u2190\nschema(N1,V ,N2)\u00b7\nword of sent(1, s(\u201cThe\u201d,N1, \u201cwas\u201d,V , \u201cby\u201d, \u201cthe\u201d,N2), \u201cthe\u201d) \u2190\nschema(N1,V ,N2)\u00b7\nwhere schema(N1,V ,N2) is true if N1 and N2 are common nouns and V is a verb in the past participle form (e.g., \u201cbitten\u201d). Atoms of the type valuable(S ) in the answer set(s) of the program \u03a0adv \u222aM give all the valuable sentences that can be generated using our grammar and vocabulary.\nThe PIas system is currently just a proof-of-concept, as it only generates simple sentences, given a controlled grammar and a small vocabulary. Its evaluation was done by the author, who was previously trained in the Processing Instruction teaching method and was involved in research in the field of Second Language Acquisition (VanPatten et al. 2009). In the future, we plan to expand the system in order to make it capable of creating more complex sentences and stories containing the passive voice, as well as complete teaching and testing activities that interleave sentences containing the target grammatical form with sentences that do not contain it. Once PIas is capable of producing such activities, we plan to subject the system to a more rigorous evaluation."}, {"heading": "5 Conclusions and Future Work", "text": "This paper has shown three different directions in which modeling an important theory from the field of Second Language Acquisition can contribute to the development of this theory. First of all, we identified aspects in the text of the theory description that need refinement (i.e., the definition of \u201csentence initial position\u201d; the presentation of Principle 1, whose sub-principles could be ordered differently to facilitate a deeper understanding) and determined opportunities for future theory development (i.e., How are resources in working memory consumed during comprehension tasks? Do succeeding sentences in a narrative constrain the interpretation of previous sentences? How are non-sense sentences interpreted?) Second, we have shown how our ASP model can be used to design experiments for testing this theory and fine-tuning its parameters. Third, we described a system, PIas that assesses the quality of materials created by language instructors, and creates valuable sentences.\nWe hope that the application presented here, and its three main contributions, will help promote ASP as a tool for the study of qualitative theories, in different fields. To the best of our knowledge, the only other uses of ASP for the refinement of the natural language description of a cognitive theory are the papers of Balduccini and Girotto (2010; 2011) that inspired the current work. In the field of Applied Linguistics, we are aware of the use of computer models in simulating theory predictions (Dijkstra et al. 1998; Dijkstra and Van Heuven 2002). The mentioned computer models were created using procedural languages. In contrast to these approaches, our main focus is on facilitating the revision of the description of a theory by formalizing it in a mathematical language. ASP showed to be par-\nof defaults, uncertainty, and evolving domains. For us, the simulation of results and the automated testing of the theory\u2019s predictions is just a consequence of our primary goal.\nOur principal interest in expanding the work in this paper will be on improving the capabilities of the PIas system. We want PIas to use the valuable sentences it generates in creating complete exercises or activities suitable for teaching. We also intend to make PIas capable of producing valuable paragraphs. One difficult question to address here will be What makes a collection of sentences a story?"}, {"heading": "Acknowledgments", "text": "The author warmly thanks Michael Gelfond, Marcello Balduccini, and the anonymous reviewers for their valuable suggestions."}], "references": [{"title": "Diagnostic reasoning with A-Prolog", "author": ["M. Balduccini", "M. Gelfond"], "venue": "Journal of Theory and Practice of Logic Programming (TPLP) 3, 4\u20135, 425\u2013461.", "citeRegEx": "Balduccini and Gelfond,? 2003", "shortCiteRegEx": "Balduccini and Gelfond", "year": 2003}, {"title": "Formalization of psychological knowledge in Answer Set Programming and its application", "author": ["M. Balduccini", "S. Girotto"], "venue": "Journal of Theory and Practice of Logic Programming (TPLP) 10, 4\u20136, 725\u2013740.", "citeRegEx": "Balduccini and Girotto,? 2010", "shortCiteRegEx": "Balduccini and Girotto", "year": 2010}, {"title": "ASP as a cognitive modeling tool: Short-term memory and long-term memory", "author": ["M. Balduccini", "S. Girotto"], "venue": "Logic Programming, Knowledge Representation, and Nonmonotonic Reasoning, M. Balduccini and T. C. Son, Eds. Lecture Notes in Computer Science, vol. 6565. Springer, Berlin, 377\u2013397.", "citeRegEx": "Balduccini and Girotto,? 2011", "shortCiteRegEx": "Balduccini and Girotto", "year": 2011}, {"title": "Logic programming and knowledge representation", "author": ["C. Baral", "M. Gelfond"], "venue": "Journal of Logic Programming 19, 20, 73\u2013148.", "citeRegEx": "Baral and Gelfond,? 1994", "shortCiteRegEx": "Baral and Gelfond", "year": 1994}, {"title": "Probabilistic reasoning with answer sets", "author": ["C. Baral", "M. Gelfond", "N. Rushton"], "venue": "Journal of Theory and Practice of Logic Programming (TPLP) 9, 1, 57\u2013144.", "citeRegEx": "Baral et al\\.,? 2009", "shortCiteRegEx": "Baral et al\\.", "year": 2009}, {"title": "The architecture of the bilingual word recognition system: From identification to decision", "author": ["T. Dijkstra", "W. Van Heuven"], "venue": "Bilingualism: Language and Cognition 33, 600\u2013629.", "citeRegEx": "Dijkstra and Heuven,? 2002", "shortCiteRegEx": "Dijkstra and Heuven", "year": 2002}, {"title": "Simulating cross-language competition with the bilingual interactive activation model", "author": ["T. Dijkstra", "W. Van Heuven", "J. Grainger"], "venue": "Psychologica Belgica 38, 177\u2013196.", "citeRegEx": "Dijkstra et al\\.,? 1998", "shortCiteRegEx": "Dijkstra et al\\.", "year": 1998}, {"title": "Conflict-driven disjunctive answer set solving", "author": ["C. Drescher", "M. Gebser", "T. Grote", "B. Kaufmann", "A. K\u00f6nig", "M. Ostrowski", "T. Schaub"], "venue": "Proceedings of the Eleventh International Conference on Principles of Knowledge Representation and Reasoning (KR-08), G. Brewka and J. Lang, Eds. AAAI Press, 422\u2013432.", "citeRegEx": "Drescher et al\\.,? 2008", "shortCiteRegEx": "Drescher et al\\.", "year": 2008}, {"title": "Classical negation in logic programs and disjunctive databases", "author": ["M. Gelfond", "V. Lifschitz"], "venue": "New Generation Computing 9, 3/4, 365\u2013386.", "citeRegEx": "Gelfond and Lifschitz,? 1991", "shortCiteRegEx": "Gelfond and Lifschitz", "year": 1991}, {"title": "Action languages", "author": ["M. Gelfond", "V. Lifschitz"], "venue": "Electronic Transactions on AI 3, 16, 193\u2013210.", "citeRegEx": "Gelfond and Lifschitz,? 1998", "shortCiteRegEx": "Gelfond and Lifschitz", "year": 1998}, {"title": "Modeling a theory of Second Language Acquisition in ASP", "author": ["D. Inclezan"], "venue": "Proceedings of the 14th International Workshop on Non-Monotonic Reasoning (NMR12), R. Rosati and S. Woltran, Eds.", "citeRegEx": "Inclezan,? 2012", "shortCiteRegEx": "Inclezan", "year": 2012}, {"title": "Making Communicative Language Teaching Happen", "author": ["J.F. Lee", "B. VanPatten"], "venue": "McGraw-Hill, New York.", "citeRegEx": "Lee and VanPatten,? 2003", "shortCiteRegEx": "Lee and VanPatten", "year": 2003}, {"title": "Stable Models and an Alternative Logic Programming Paradigm", "author": ["V.W. Marek", "M. Truszczynski"], "venue": "The Logic Programming Paradigm: a 25-Year Perspective. Springer Verlag, Berlin, 375\u2013398.", "citeRegEx": "Marek and Truszczynski,? 1999", "shortCiteRegEx": "Marek and Truszczynski", "year": 1999}, {"title": "Logic programs with stable model semantics as a constraint programming paradigm", "author": ["I. Niemel\u00e4"], "venue": "Proceedings of the Workshop on Computational Aspects of Nonmonotonic Reasoning. 72\u201379.", "citeRegEx": "Niemel\u00e4,? 1998", "shortCiteRegEx": "Niemel\u00e4", "year": 1998}, {"title": "The effect of Processing Instruction and dictogloss tasks on acquisition of the English passive voice", "author": ["J. Qin"], "venue": "Language Teaching Research 12, 61\u201382.", "citeRegEx": "Qin,? 2008", "shortCiteRegEx": "Qin", "year": 2008}, {"title": "Learners\u2019 comprehension of clitic pronouns: More evidence for a word order strategy", "author": ["B. VanPatten"], "venue": "Hispanic Linguistics 1, 57\u201367.", "citeRegEx": "VanPatten,? 1984", "shortCiteRegEx": "VanPatten", "year": 1984}, {"title": "Grammar teaching for the acquisition-rich classroom", "author": ["B. VanPatten"], "venue": "Foreign Language Annals 26, 435\u2013450.", "citeRegEx": "VanPatten,? 1993", "shortCiteRegEx": "VanPatten", "year": 1993}, {"title": "Processing Instruction: An update", "author": ["B. VanPatten"], "venue": "Language Learning 52, 4, 755\u2013803.", "citeRegEx": "VanPatten,? 2002", "shortCiteRegEx": "VanPatten", "year": 2002}, {"title": "From Input to Output: A Teacher\u2019s Guide to Second Language Acquisition", "author": ["B. VanPatten"], "venue": "McGraw-Hill, New York.", "citeRegEx": "VanPatten,? 2003", "shortCiteRegEx": "VanPatten", "year": 2003}, {"title": "Input Processing in Second Language Acquisition", "author": ["B. VanPatten"], "venue": "Lawrence Erlbaum Associates, Mahwah, NJ, 5\u201332.", "citeRegEx": "VanPatten,? 2004", "shortCiteRegEx": "VanPatten", "year": 2004}, {"title": "Explicit Instruction and Input Processing", "author": ["B. VanPatten", "T. Cadierno"], "venue": "Studies in Second Language Acquisition 15, 225\u2013243.", "citeRegEx": "VanPatten and Cadierno,? 1993", "shortCiteRegEx": "VanPatten and Cadierno", "year": 1993}, {"title": "Processing Instruction and dictogloss: A study on object pronouns and word order in Spanish", "author": ["B. VanPatten", "D. Inclezan", "H. Salazar", "A.P. Farley"], "venue": "Foreign Language Annals 42, 3, 557\u2013575.", "citeRegEx": "VanPatten et al\\.,? 2009", "shortCiteRegEx": "VanPatten et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 8, "context": "This paper extends a relatively new line of research that explores the contributions of Answer Set Programming (ASP) (Gelfond and Lifschitz 1991; Niemel\u00e4 1998; Marek and Truszczynski 1999) to the study and refinement of qualitative scientific theories (Balduccini and Girotto 2010; Balduccini and Girotto 2011).", "startOffset": 117, "endOffset": 188}, {"referenceID": 13, "context": "This paper extends a relatively new line of research that explores the contributions of Answer Set Programming (ASP) (Gelfond and Lifschitz 1991; Niemel\u00e4 1998; Marek and Truszczynski 1999) to the study and refinement of qualitative scientific theories (Balduccini and Girotto 2010; Balduccini and Girotto 2011).", "startOffset": 117, "endOffset": 188}, {"referenceID": 12, "context": "This paper extends a relatively new line of research that explores the contributions of Answer Set Programming (ASP) (Gelfond and Lifschitz 1991; Niemel\u00e4 1998; Marek and Truszczynski 1999) to the study and refinement of qualitative scientific theories (Balduccini and Girotto 2010; Balduccini and Girotto 2011).", "startOffset": 117, "endOffset": 188}, {"referenceID": 1, "context": "This paper extends a relatively new line of research that explores the contributions of Answer Set Programming (ASP) (Gelfond and Lifschitz 1991; Niemel\u00e4 1998; Marek and Truszczynski 1999) to the study and refinement of qualitative scientific theories (Balduccini and Girotto 2010; Balduccini and Girotto 2011).", "startOffset": 252, "endOffset": 310}, {"referenceID": 2, "context": "This paper extends a relatively new line of research that explores the contributions of Answer Set Programming (ASP) (Gelfond and Lifschitz 1991; Niemel\u00e4 1998; Marek and Truszczynski 1999) to the study and refinement of qualitative scientific theories (Balduccini and Girotto 2010; Balduccini and Girotto 2011).", "startOffset": 252, "endOffset": 310}, {"referenceID": 1, "context": "It was shown that ASP is a suitable tool for this task (Balduccini and Girotto 2010; Balduccini and Girotto 2011), as it provides means for an elegant and accurate representation of defaults, dynamic domains, and incomplete information, among others.", "startOffset": 55, "endOffset": 113}, {"referenceID": 2, "context": "It was shown that ASP is a suitable tool for this task (Balduccini and Girotto 2010; Balduccini and Girotto 2011), as it provides means for an elegant and accurate representation of defaults, dynamic domains, and incomplete information, among others.", "startOffset": 55, "endOffset": 113}, {"referenceID": 1, "context": "This paper extends a relatively new line of research that explores the contributions of Answer Set Programming (ASP) (Gelfond and Lifschitz 1991; Niemel\u00e4 1998; Marek and Truszczynski 1999) to the study and refinement of qualitative scientific theories (Balduccini and Girotto 2010; Balduccini and Girotto 2011). As pointed out by Balduccini and Girotto (2010), qualitative theories tend to be formulated in natural language, often in the form of defaults.", "startOffset": 253, "endOffset": 360}, {"referenceID": 15, "context": "The theory we consider is VanPatten\u2019s Input Processing theory (VanPatten 1984; VanPatten 2004).", "startOffset": 62, "endOffset": 94}, {"referenceID": 19, "context": "The theory we consider is VanPatten\u2019s Input Processing theory (VanPatten 1984; VanPatten 2004).", "startOffset": 62, "endOffset": 94}, {"referenceID": 15, "context": "The theory we consider is VanPatten\u2019s Input Processing theory (VanPatten 1984; VanPatten 2004). We chose it because it is an established theory in the field of Second Language Acquisition, with important consequences on foreign language education. It is specified in English in the form of a compact set of principles. Input Processing (IP) describes the default strategies that second language learners use to get meaning out of text written or spoken in the second language, during tasks focused on comprehension, given the learners\u2019 limitations in vocabulary, working memory, or internalized knowledge of grammatical structures. As a result of applying these strategies, even learners with limited grammatical expertise can often, but not always, interpret input sentences correctly. Once grammatical information is internalized, the default strategies are overridden by the always reliable grammatical knowledge. Hence, it can be said that IP describes an example of nonmonotonic reasoning. IP predicts that beginner learners of English reading the sentence \u201cThe cat was bitten by the dog\u201d would only be able to retrieve the meanings of the words \u201ccat\u201d, \u201cbitten\u201d, and \u201cdog\u201d and end up with something like the sequence of concepts CATBITE-DOG. Although they may notice the word \u201cwas\u201d or the ending \u201c-en\u201d of the verb \u201cbitten\u201d, they would not be able to process them (i.e., connect them with the function they serve, which is to indicate passive voice) because of limitations in processing resources. In this context, the expression processing resources (or simply resources) refers to the amount of information that a learner can hold and process in his/her working memory during real time comprehension of input sentences. Additionally, IP predicts that the sentence above, now mapped into the sequence of concepts CAT-BITE-DOG, would be incorrectly interpreted by these learners as \u201cThe cat bit the dog\u201d because of a hypothesized strategy of assigning agent status to the first noun of a sentence. IP, as described by VanPatten (2004), consists of two principles formulated as defaults.", "startOffset": 26, "endOffset": 2039}, {"referenceID": 15, "context": ", Spanish, Italian, German, Chinese), independently from the learners\u2019 native language (VanPatten 1984).", "startOffset": 87, "endOffset": 103}, {"referenceID": 9, "context": ", people know under what conditions a biting action can occur); in ASP, there is substantial research on how to represent actions and dynamic domains in which change is caused by actions (Gelfond and Lifschitz 1998; Balduccini and Gelfond 2003).", "startOffset": 187, "endOffset": 244}, {"referenceID": 0, "context": ", people know under what conditions a biting action can occur); in ASP, there is substantial research on how to represent actions and dynamic domains in which change is caused by actions (Gelfond and Lifschitz 1998; Balduccini and Gelfond 2003).", "startOffset": 187, "endOffset": 244}, {"referenceID": 16, "context": "PIas relies on the guidelines of an established teaching method\u2014Processing Instruction (VanPatten 1993; VanPatten 2002)\u2014that is based on the principles of Input Processing.", "startOffset": 87, "endOffset": 119}, {"referenceID": 17, "context": "PIas relies on the guidelines of an established teaching method\u2014Processing Instruction (VanPatten 1993; VanPatten 2002)\u2014that is based on the principles of Input Processing.", "startOffset": 87, "endOffset": 119}, {"referenceID": 10, "context": "The current article extends a previous version of our work (Inclezan 2012).", "startOffset": 59, "endOffset": 74}, {"referenceID": 15, "context": "Principle 1 is formulated by VanPatten (2004) as follows:", "startOffset": 29, "endOffset": 46}, {"referenceID": 18, "context": "The form is not meaningful because it does not reflect a \u201cbiological difference in the real world\u201d (VanPatten 2003), i.", "startOffset": 99, "endOffset": 115}, {"referenceID": 19, "context": "The IP theory is supposed to be applicable independently from the mental model of a second language that is assumed (VanPatten 2004).", "startOffset": 116, "endOffset": 132}, {"referenceID": 15, "context": "Based on its accompanying explanation provided by VanPatten (2004), its meaning is that a form that is normally redundant may not be processed at all if it is actually redundant in that sentence (i.", "startOffset": 50, "endOffset": 67}, {"referenceID": 9, "context": "To model the background knowledge base of a learner, we use known methodologies for representing dynamic domains in ASP (Gelfond and Lifschitz 1998; Balduccini and Gelfond 2003).", "startOffset": 120, "endOffset": 177}, {"referenceID": 0, "context": "To model the background knowledge base of a learner, we use known methodologies for representing dynamic domains in ASP (Gelfond and Lifschitz 1998; Balduccini and Gelfond 2003).", "startOffset": 120, "endOffset": 177}, {"referenceID": 3, "context": "unlikely(Ev , I ) atoms are obtained from axioms encoding default statements and their exceptions (Baral and Gelfond 1994); hpd(Ev) atoms are simply stored as a collection of facts.", "startOffset": 98, "endOffset": 122}, {"referenceID": 4, "context": "6 We considered using probabilistic ASP languages such as P-log (Baral et al. 2009) to model event probabilities.", "startOffset": 64, "endOffset": 83}, {"referenceID": 7, "context": "The answer sets of the program \u03a0adv \u222a lp(S1), computed using the ASP solver claspD (Drescher et al. 2008), contained the map facts presented in Table 1.", "startOffset": 83, "endOffset": 105}, {"referenceID": 16, "context": "PIas follows the guidelines of a successful teaching method called Processing Instruction (PI) (VanPatten 1993; VanPatten 2002; VanPatten 2003; Lee and VanPatten 2003), developed based on the principles of IP.", "startOffset": 95, "endOffset": 167}, {"referenceID": 17, "context": "PIas follows the guidelines of a successful teaching method called Processing Instruction (PI) (VanPatten 1993; VanPatten 2002; VanPatten 2003; Lee and VanPatten 2003), developed based on the principles of IP.", "startOffset": 95, "endOffset": 167}, {"referenceID": 18, "context": "PIas follows the guidelines of a successful teaching method called Processing Instruction (PI) (VanPatten 1993; VanPatten 2002; VanPatten 2003; Lee and VanPatten 2003), developed based on the principles of IP.", "startOffset": 95, "endOffset": 167}, {"referenceID": 11, "context": "PIas follows the guidelines of a successful teaching method called Processing Instruction (PI) (VanPatten 1993; VanPatten 2002; VanPatten 2003; Lee and VanPatten 2003), developed based on the principles of IP.", "startOffset": 95, "endOffset": 167}, {"referenceID": 14, "context": "7 A non-valuable sentence crafted by a researcher from the Second Language Acquisition community (Qin 2008) is \u201cThe ball was pushed by the rabbit.", "startOffset": 97, "endOffset": 107}, {"referenceID": 21, "context": "\u201d This sentence is not valuable (VanPatten et al. 2009) because event probabilities ensure that even beginner learners will interpret this sentence correctly \u2013 it is more likely for a rabbit to push a ball than for a ball to push a rabbit.", "startOffset": 32, "endOffset": 55}, {"referenceID": 14, "context": "7 A non-valuable sentence crafted by a researcher from the Second Language Acquisition community (Qin 2008) is \u201cThe ball was pushed by the rabbit.\u201d This sentence is not valuable (VanPatten et al. 2009) because event probabilities ensure that even beginner learners will interpret this sentence correctly \u2013 it is more likely for a rabbit to push a ball than for a ball to push a rabbit. 8 In their study, VanPatten and Cadierno (1993) used 120 valuable sentences for a single grammatical form.", "startOffset": 98, "endOffset": 434}, {"referenceID": 21, "context": "Its evaluation was done by the author, who was previously trained in the Processing Instruction teaching method and was involved in research in the field of Second Language Acquisition (VanPatten et al. 2009).", "startOffset": 185, "endOffset": 208}, {"referenceID": 6, "context": "In the field of Applied Linguistics, we are aware of the use of computer models in simulating theory predictions (Dijkstra et al. 1998; Dijkstra and Van Heuven 2002).", "startOffset": 113, "endOffset": 165}], "year": 2013, "abstractText": "This paper explores the contributions of Answer Set Programming (ASP) to the study of an established theory from the field of Second Language Acquisition: Input Processing. The theory describes default strategies that learners of a second language use in extracting meaning out of a text, based on their knowledge of the second language and their background knowledge about the world. We formalized this theory in ASP, and as a result we were able to determine opportunities for refining its natural language description, as well as directions for future theory development. We applied our model to automating the prediction of how learners of English would interpret sentences containing the passive voice. We present a system, PIas, that uses these predictions to assist language instructors in designing teaching materials. To appear in Theory and Practice of Logic Programming (TPLP).", "creator": "LaTeX with hyperref package"}}}