{"id": "1312.2578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Dec-2013", "title": "Kernel-based Distance Metric Learning in the Output Space", "abstract": "clearly in this paper we present exactly two related, kernel - based distance metric learning ( dml ) approximation methods. their respective models non - linearly distributed map data from their original space to an output space, and subsequent distance measurements are performed in searching the output space locally via a mahalanobis metric. the dimensionality of the output space can be directly context controlled to facilitate the learning capability of measuring a low - rank metric. both methods allow for simultaneous inference of the associated metric problem and the mapping to the output space, which can be used to visualize the data, when the output space is 2 - or 3 - dimensional. experimental results for supporting a collection of classification tasks illustrate the advantages made of the proposed analytic methods over most other traditional and kernel - based dml approaches.", "histories": [["v1", "Mon, 9 Dec 2013 20:58:16 GMT  (245kb)", "http://arxiv.org/abs/1312.2578v1", "11 pages, 7 figures, accepted by 2013 International Joint Conference on Neural Networks (IJCNN)"], ["v2", "Mon, 28 Apr 2014 20:08:47 GMT  (245kb)", "http://arxiv.org/abs/1312.2578v2", "11 pages, 7 figures, appeared in the Proceedings of 2013 International Joint Conference on Neural Networks (IJCNN)"]], "COMMENTS": "11 pages, 7 figures, accepted by 2013 International Joint Conference on Neural Networks (IJCNN)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["cong li", "michael georgiopoulos", "georgios c anagnostopoulos"], "accepted": false, "id": "1312.2578"}, "pdf": {"name": "1312.2578.pdf", "metadata": {"source": "META", "title": "Output-Space Distance Metric Learning for k-NN Classification", "authors": ["Cong Li", "Michael Georgiopoulos", "Georgios C. Anagnostopoulos"], "emails": ["congli@eecs.ucf.edu,", "michaelg@ucf.edu", "georgio@fit.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n25 78\nv1 [\ncs .L\nG ]\n9 D\nec 2\nKeywords: Distance Metric Learning, Kernel Methods, Reproducing Kernel Hilbert Space for Vector-valued Functions"}, {"heading": "1 Introduction", "text": "Distance Metric Learning (DML) has become an active research area due to the fact that many machine learning models and algorithms depend on metric calculations. Considering plain Euclidean distances between samples may not be a suitable approach for some practical problems, e.g., for k-Nearest Neighbors (KNN) classification, where a metric other than the Euclidean may yield higher recognition rates. Hence, it may be important to learn an appropriate metric for the learning problem at hand. DML aims to address this problem, i.e., to infer a parameterized metric from the available training data that maximizes the performance of a model.\nMost of past DML research focuses specifically on learning a weighted Euclidean metric, also known as the Mahalanobis distance (e.g. see [13]), or generalizations of it, where the weights are inferred from the data. For elements x,x\u2032 of a finite-dimensional Euclidean space Rm, the Mahalanobis distance is defined as d(x,x\u2032) = \u2016x\u2212 x\u2032\u2016A , \u221a (x\u2212 x\u2032)TA(x\u2212 x\u2032), where A = AT 0, i.e. A \u2208 Rm\u00d7m is symmetric positive semi-definite matrix of weights to be determined. Note that when A is not strictly positive definite, it defines a pseudo-metric in Rm. An obvious DML approach is to learn this metric in the data\u2019s native space, which is tantamount to first linearly transforming the data via a matrix L, such that A = LTL, and then measuring distances using the standard Euclidean metric \u2016\u00b7\u20162.\nOne possible alternative worth exploring is to search for a non-linear transform prior to measuring Mahalanobis distances, so that performance may improve over the case, where a linear transformation is used. Towards this end, efforts have been recently made to develop kernel-based DML approaches. If X is the original (native) data space, most of these methods choose an appropriate (positive definite) scalar kernel k : X \u00d7 X \u2192 R, which gives rise to a Reproducing Kernel Hilbert Space (RKHS) H of functions f : X \u2192 R with inner product \u3008\u00b7, \u00b7\u3009H. This inner product satisfies the (reproducing) property that, for any x, x\n\u2032 \u2208 X , there are functions \u03c6x, \u03c6x\u2032 \u2208 H, such that \u3008\u03c6x, \u03c6x\u2032\u3009H = k(x, x \u2032). The mapping \u03c6 : x 7\u2192 \u03c6x is referred to\nas the feature map and H is referred to as the (transformed) feature space of X , both of which are implied by the chosen kernel. Notice that the feature map may be highly non-linear. Subsequently, these methods learn a metric in the feature space H: d(\u03c6x, \u03c6x\u2032) = \u221a \u3008(\u03c6x \u2212 \u03c6x\u2032), A(\u03c6x \u2212 \u03c6x\u2032)\u3009H, where A : H \u2192 H is a self-adjoint, bounded, positive-definite operator, preferably, of low rank. Since any element \u03c6x of H may be of infinite dimension, operator A may be described by an infinite number of parameters to be inferred from the data. Obviously, learning A is not feasible by following direct approaches and, therefore, needs to be learned in some indirect fashion. For example, the authors in [8] pointed out an equivalence between kernel learning and metric learning in the feature space. In specific, they showed that learning A in H is implicitly achieved by learning a finite-dimensional matrix.\nIn this paper, we propose a different DML kernelization strategy, according to which a kernel-based, nonlinear transform f maps X into a Euclidean output space Rm, in order to learn a Mahalanobis distance in that output space. This strategy gives rise to two new models that simultaneously learn both the mapping and the output space metric. Leveraged by the Representer Theorem proposed in [14], all computations of both methods involve only kernel calculations. Unlike previous kernel-based approaches, whose mapping from input to feature space H cannot be cast into an explicit form, the relevant mappings from input to output space are explicit for both of our methods. Thus, we can access the transformed data in the output space, and this feature can be even used to visualize the data [20], when the output space is 2- or 3-dimensional. Furthermore, by specifying the dimensionality of the output space, the rank of the learned metric can be easily controlled to facilitate dimensionality reduction of the original data.\nOur first approach uses an appropriate, but otherwise arbitrary, matrix-valued kernel function and, hence, provides maximum flexibility in specifying the mapping f . Furthermore, in this approach, Mahalanobis distances are explicitly parameterized by a weight matrix to be learned. Our second method is similar to the first one, but assumes a specific parameterized matrix-valued kernel function that can be inferred from the data. We show that the Mahalanobis distance is implicitly determined by the kernel function, and thus eliminates the need of learning a weight matrix for the Mahalanobis distances. To demonstrate the merit of our methods, we compare them to standard k-NN classification (without DML) and other recent kernelized DML algorithms, including Large Margin Nearest Neighbor (LMNN) [22], Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3]. The comparisons are drawn using eight UCI benchmark data sets in terms of recognition performance and show that the novel methods can achieve higher classification accuracy.\nRelated Work Several previous works have been focused on DML. Xing, et. al. [23] proposed an early DML method, which minimizes the distance between similar points, while enlarging the distance between dissimilar points. In [17], relative comparison constraints that involve three points at a time are considered. Neighborhood Components Analysis (NCA) [6] is proposed to learn a Mahalanobis distance for the k-NN classifier by maximizing the leave-one-out k-NN performance. [1] proposed a DML method for clustering. Large Margin Nearest Neighbor (LMNN) DML model [22] aims to produce a mapping, so that the k-nearest neighbors of any given sample belong to the same class, while samples from different classes are separated by large margins. Similarly, a Support Vector-based method is proposed in [15]. Also, LMNN is further extended to a Multi-Task Learning variation [16]. Another multi-task DML model is proposed in [25] that searches for task relationships. In [7], the authors proposed a general framework for sparse DML, such that several previous works are subsumed. Also, some other DML models can be extended to sparse versions by augmenting their formulations. Recently, an eigenvalue optimization framework for DML was developed an presented in [24]. Moreover, the connection between LMNN and Support Vector Machines (SVMs) was discussed in [5].\nBesides the problem of learning a metric in the original feature space, there has been increasing interest in kernelized DML methods. In the early work of [19], the Lagrange dual problem of the proposed DML formulation is derived, and the DML method is kernelized in the dual domain. Information-Theoretic Metric Learning (ITML) [4] is another kernelized method, which is based on minimizing the Kullback-Leibler divergence between two distributions. The kernelization of LMNN is discussed in [18] and [10]. Moreover, a Kernel Principal Component Analysis (KPCA)-based kernelized algorithm is developed in [3], such that many DML methods, such as LMNN, can be kernelized. In [12], the Mahalanobis matrix and kernel matrix are learned simultaneously. In [8] and its extended work [9], the authors proposed a framework that builds connections between kernel learning and DML in the kernel-induced feature space. Several kernelized models, such as ITML, are covered by this framework. Finally, Multiple Kernel Learning (MKL)-based metric DML\nis discussed in [21]."}, {"heading": "2 RKHS for Vector-Valued Functions", "text": "Before introducing our methods, in this section we will briefly review the concept of Reproducing Kernel Hilbert Space (RKHS) for vector-valued functions as presented in [14]. Let X be an arbitrary set, which we will refer to as input space, although it may not actually be a vector space per se. A matrix function K : X \u00d7X \u2192 Rm\u00d7m is called a positive-definite matrix-valued kernel, or simply matrix kernel, iff it satisfies the following conditions:\nK(x, x\u2032) = KT (x\u2032, x) \u2200 x, x\u2032 \u2208 X (1)\nK(x, x) 0 \u2200 x \u2208 X (2)\nK\u0304(X) 0 \u2200 X \u2286 X (3)\nwhere X = {xi} n i=1 and K\u0304(X) \u2208 R mn\u00d7mn is a n \u00d7 n block matrix, whose (i, j) block is given as K\u0304i,j = K(xi, xj) \u2208 R\nm\u00d7m, where i, j \u2208 {1, 2, . . . , n}. According to [14, Theorem 1], if K is a matrix kernel, then there exists a unique (up to an isometry) RKHS H of vector-valued functions f : X \u2192 Rm equipped with an inner product \u3008\u00b7, \u00b7\u3009H that admits K as its reproducing kernel, i.e. \u2200 x, x\n\u2032 \u2208 X and \u2200 y,y\u2032 \u2208 Rm, there are vector-valued functions Kxy, Kx\u2032y \u2032 \u2208 H that depend on x,y and x\u2032,y\u2032 respectively, such that it holds\n\u3008Kxy,Kx\u2032y \u2032\u3009H = y TK(x, x\u2032)y\u2032 (4)\nNote that Kx : R m \u2192 H is a bounded linear operator parameterized by x \u2208 X and that the function\nKxy \u2208 H is such that, when evaluated on x \u2032 \u2208 X , it yields\n(Kxy)(x \u2032) = K(x\u2032, x)y (5)"}, {"heading": "3 Fixed Matrix Kernel DML Formulation", "text": "In this section, we propose our first kernelized DML method based on a RKHS for vector-valued functions. Again, let X be an arbitrary set. Assume we are provided with a training set T = {(xi,yi)}i=1,\u00b7\u00b7\u00b7 ,n, where xi \u2208 X and yi \u2208 R\nm, and we are considering the supervised learning task that seeks to infer a distance metric in Rm along with a mapping f : X 7\u2192 Rm from T . In addition to T , we also assume that we are provided with a real-valued, symmetric similarity matrix S \u2208 Rn\u00d7n with entries si,j = s(yi,yj), where s : Rm \u00d7 Rm \u2192 R+ is such that 0 \u2264 si,j \u2264 si,i \u2200 i, j. Other than these constraints, the values si,j can be arbitrary and assigned appropriately with respect to a specific application context. Moreover, let K (x, x\u2032) be a matrix-valued kernel function (i.e., it satisfies Equation (1) through Equation (3)) on X of given form and let H be its associated RKHS of Rm-valued elements. Consider now the following DML formulation:\nmin f,L\n\u03b3\n2\n\u2211\ni,j\nsi,j\u2016L[f(xi)\u2212 f(xj)]\u2016 2 2 +\n\u03bb\n2\n\u2211\ni\n\u2016L[f(xi)\u2212 yi]\u2016 2 2 + \u03c1 tr(L) +\n1 2 \u2016f\u20162H (6)\nNotice that \u2016Ly\u20162 = \u2016y\u2016A, \u2200 y \u2208 R m, where A , LTL 0. In other words, the Euclidean norms of vector differences appearing in (6) are Mahalanobis distances for the output space. Note that if L is not full-rank, then A is not strictly positive definite, thus \u2016 \u00b7 \u2016A will be a pseudo-metric in R\nm. The rationale behind this formulation is as follows. The first term, the collocation term, forces similar (w.r.t. the similarity measure s) input samples to be mapped closely in the output space (unsupervised learning task). The second term, the regression term, forces samples to be mapped close to their target values (supervised learning task). In the context of classification tasks, the combination of these two terms aims to force data that belong to the same class to be mapped close to the same cluster. Closeness in the output space is measured via a Mahalanobis metric that is parameterized via L. The third term, as we will show later, controls the magnitude of matrix A and facilitates the derivation of our proposed algorithm. Finally, the fourth term is a regularization term\nand is penalizing the complexity of f . Eventually, one can simultaneously learn the output space distance metric and the mapping f through a joint minimization.\nThe functional of Problem (6) satisfies the conditions stipulated by the Representer Theorem for Hilbert spaces of vector-valued elements (Theorem 5 in [14]) and, therefore, for a fixed value of L, the unique\nminimizer f\u0302 is of the form:\nf\u0302 =\nn\u2211\ni=1\nKxici (7)\nwhere the m-dimensional vectors {ci} n i=1 are to be learned. Notice that, due to Equation (5), the explicit input-to-output mapping is given in Equation (8) and is, in general, non-linear in x, if X is a vector space over the reals.\nf\u0302(x) = n\u2211\ni=1\nK(x, xi)ci (8)\nProposition 1. Problem (6) is equivalent to the following minimization problem:\nmin c,L\n1 2 cT K\u0304c+ \u03b3 2\n\u2211\ni,j\nsij \u2016L\u0393ijc\u2016 2 2 +\n\u03bb\n2\n\u2211\ni\n\u2016L(K\u0304ic\u2212 yi)\u2016 2 2 + \u03c1 tr(L) (9)\nwhere c , [cT1 , \u00b7 \u00b7 \u00b7 , c T n ] T \u2208 Rmn, K\u0304 \u2208 Rmn\u00d7mn is the kernel matrix for the training set (as defined for Equation (3)), K\u0304i = K\u0304(xi) , [K(xi, x1), \u00b7 \u00b7 \u00b7 ,K(xi, xn)] \u2208 R m\u00d7mn, and \u0393ij = \u0393(xi, xj) , K\u0304i \u2212 K\u0304j.\nThe above proposition can be proved by directly substituting Equation (7) into Problem (6) and then using Equation (4). Given two samples x, x\u2032 \u2208 X , the inferred metric will be of the form\nd(x, x\u2032) = \u2016L\u0393(x, x\u2032)c\u20162 = \u2016\u0393(x, x \u2032)c\u2016A (10)\nwith A = LTL. Next, we state a result that facilitates the solution of Problem (9).\nProposition 2. Problem (9) is convex with respect to each of the two variables c and L individually.\nProof. The convexity of the objective function, denoted as Q (c,L), with respect to c is guaranteed by the positive semi-definiteness of the corresponding Hessian matrix of Q:\n\u22022Q(c,L)\n\u2202c\u2202cT = K\u0304 + \u03b3\n\u2211\ni,j\nsij\u0393 T ijL TL\u0393ij + \u03bb \u2211\ni\nK\u0304 T i L TLK\u0304i 0 (11)\nTo show the convexity with respect to L, we consider each term separately. The convexity of \u2016L\u0393ijc\u2016 2 2 stems from the conclusion in [2, p. 110], which states that \u2016Xz\u201622 is convex with respect to any matrix X for any z. For the same reason, \u2016L(K\u0304ic \u2212 yi)\u2016 2 2 is also convex. Finally, tr(L) is convex in L, as shown in [2, p. 109]. Thus, the objective function is also convex with respect to L.\nBased on Proposition 2, we can perform the joint minimization Problem (9) by block coordinate descent with respect to c and L. We set the partial derivatives of Q with respect to the two variables to zero and obtain\n\u2202Q(c,L)\n\u2202c = 0 \u21d2 c = \u03bb(\n\u22022Q(c,L) \u2202c\u2202cT )\u2020 \u2211\ni\nK\u0304 T i L TLyi (12)\n\u2202Q(c,L)\n\u2202L = 0 \u21d2 L = \u2212\u03c1(\u03b3\n\u2211\ni,j\nsij\u0393ijcc T\u0393Tij + \u03bb\n\u2211\ni\n(K\u0304ic\u2212 yi)(K\u0304ic \u2212 yi) T )\u2020 (13)\nwhere \u2020 stands for Moore-Penrose pseudo-inversion. One can update c via Equation (12) by holding L fixed to its current estimate and then update L via Equation (13) by using the most current value of c. Repeating these steps until convergence would constitute the basis for the block-coordinate descent to train this model.\nDue to the calculation of the pseudo-inverse, the time complexity of each iteration, in the worst case scenario, is O((mn)3).\nAs we can observe from Equation (13), since A = LTL, the parameter \u03c1 that appears in the term \u03c1tr(L) of Problem (6) directly controls the norm of A. Although other regularization terms on L may be utilized in place of \u03c1tr(L), they may not lead to a simple update equation for L, such as the one given in Equation (13). The potential appeal of this formulation stems from the simplicity of the training algorithm combined with the flexibility of choosing a matrix kernel function that is suitable to the application at hand."}, {"heading": "4 Parameterized Matrix Kernel DML Formulation", "text": "Our next formulation shares all assumptions with the previous one with the exception that the matrix kernel function K is now parameterized. We shall show that, even though the matrix kernel function is somewhat restricted, it has the property that is able to implicitly determine the output space Mahalanobis metric. To start, we assume a matrix kernel of the form:\nK(x, x\u2032) = k(x, x\u2032)B (14)\nwhere k is a scalar kernel function that is predetermined by the user and B \u2208 Rm\u00d7m is a symmetric, positive semi-definite matrix, which will be learned from T . Because of this facts, K satisfies Equation (1) through Equation (3) and, therefore is a legitimate matrix kernel function. The formulation for the alternative DML model reads\nmin f,B\n\u03b3\n2\n\u2211\ni,j\nsij\u2016f(xi)\u2212 f(xj)\u2016 2 2 +\n\u03bb\n2\n\u2211\ni\n\u2016f(xi)\u2212 yi\u2016 2 2 +\n\u03c1 2 \u2016B\u20162F + 1 2 \u2016f\u20162H (15)\nwhere \u2016B\u20162F , tr{B TB} = tr{B2} is the squared Frobenius norm of B and tr{\u00b7} is the matrix trace operator. Problem (15) differs from Problem (6) in a regularization term and in that the former seems to use Euclidean distances in the output space, while the latter uses Mahalanobis distances in the output space with weight matrix A = LTL. As was the case with the formulation of Section 3, the functional of Problem (15) also satisfies the conditions of the Representer Theorem for Hilbert spaces of vector-valued\nelements and, for fixed value of B, the unique minimizer f\u0302 has the same form as the one of Equation (7) and the explicit input-to-output mapping is given as\nf\u0302(x) =\nn\u2211\ni=1\nk(x, xi)Bci (16)\nwhich, in all but trivial cases, is again non-linear in x, if X is a vector space over the reals. In a derivation similar to the one found in Section 3, one can show that Problem (15) is equivalent to the following constrained joint minimization problem:\nmin C,B 0\n\u03b3 2 tr{CK\u0303\u2206C TB2}+ \u03bb 2 \u2016BCK\u0303 \u2212 Y \u20162F + \u03c1 2 \u2016B\u20162F + 1 2 tr{CTBCK\u0303} (17)\nwhere C , [c1, \u00b7 \u00b7 \u00b7 , cn] \u2208 R m\u00d7n, K\u0303 \u2208 Rn\u00d7n is the kernel matrix with k(xi, xj) as its (i, j) element, K\u0303\u2206 , K\u0303[diag{S1n} \u2212 S]K\u0303 \u2208 R n\u00d7n, where diag{\u00b7} is the operator producing a diagonal matrix with the same diagonal as the operator\u2019s argument, 1n \u2208 R n is the all-ones vector and Y , [y1, \u00b7 \u00b7 \u00b7 ,yn] \u2208 R\nm\u00d7n. The learned metric will be of the form\nd(x, x\u2032) = \u2016BC[k\u0303(x) \u2212 k\u0303(x\u2032)]\u20162 = \u2016C[k\u0303(x) \u2212 k\u0303(x \u2032)]\u2016A (18)\nwhere k\u0303(x) , [k(x, x1), . . . k(x, xn)] T and, in this case, A = B2. It is readily seen that the matrix B specifying the matrix kernel function also determines the Mahalanobis distance in the output space Rm. Therefore, this model implicitly learns the Mahalanobis distance by learning the B matrix in the kernel function.\nProposition 3. Problem (17) is convex with respect to each of the two variables C and B.\nProof. The proof is based on the following facts outlined in [2, sec. 3.6]: (a) A matrix-valued function g is matrix convex if and if for any z, zT gz is convex. (b) Suppose a matrix-valued function g is matrix convex and a real-valued function h is convex and non-decreasing. Then, h \u25e6 g is convex, where \u25e6 denotes function composition. (c) The function tr{WX} is convex and non-decreasing in X, if W 0. In what follows, we show convexity for each term in Problem (17). Since tr{CTBCK\u0303} = tr{CK\u0303CTB} and CK\u0303CT 0, therefore tr{CK\u0303CTB} is convex with respect to B based on facts (b) and (c). To show the convexity with respect to C, note that the matrix-valued function g(C) = CTBC is matrix convex with respect to C based\non B 0 and fact (a). Thus, with K\u0303 0 and fact (b) and (c), we achieve the convexity. The same method\nis employed to prove the convexity of the other three terms (note that K\u0303\u2206 0).\nBased on Proposition 3, we can again apply a block coordinate descent algorithm to solve Problem (17). If Q\u0303(C,B) is the relevant objective function, we set the partial derivative of Q\u0303(C,B) with respect to C zero and obtain:\n\u2202Q\u0303(C,B)\n\u2202C = 0 \u21d2 C + \u03b3BCK\u0303\u2206K\u0303\n\u22121 + \u03bbBCK\u0303 = \u03bbY (19)\nAs noted in [11], this matrix equation can be solved for C as follows:\nvec(C) = \u03bb(I + \u03b3(K\u0303\u2206K\u0303 \u22121 )\u2297B + \u03bbK\u0303 \u2297B)\u22121vec(Y ) (20)\nTo find the optimum B for fixed C, due to the constraint B 0, we use a projected gradient descent method. In each iteration, we update B using the traditional gradient descent rule: B \u2190 B\u2212\u03b1\u25bdB Q\u0303(C,B), where \u03b1 > 0 is the step length, followed by projecting the updated B onto the cone of positive semi-definite matrices. Since Q\u0303(C,B) is convex with respect to B for fixed C, this procedure is able to find the optimum solution for B. The gradient with respect to B is given as\n\u2202Q\u0303 (C,B)\n\u2202B = G+GT \u2212G\u2299 I (21)\nwhere \u2299 is the Hadamard matrix product and G is defined as\nG , B[C(\u03b3K\u0303\u2206 + \u03bbK 2)CT + \u03c1I]\u2212 (\u03bbY \u2212\n1 2 C)KCT (22)\nTherefore, for each iteration, the time complexity of updating C is O((mn)3), due to the calculation of a matrix inverse. When updating B, the time complexity is determined by the convergence speed of the projected gradient descent method."}, {"heading": "5 Experiments", "text": "In this section, we evaluate the performance of our two kernelized DML methods on classification problems. Towards this purpose, we opt to set yi = y\nk(i), \u2200i \u2208 {1, 2, . . . , n}, where k(i) \u2208 {1, 2, . . . , c} is the class label of the ith sample and yk is an appropriately chosen prototype target vector for the kth class. Additionally, we choose to evaluate the pair-wise sample similarities as si,j = [yi = yj ], where [predicate] denotes the result of the Iversonian bracket, i.e. it equals 1, if predicate evaluates to true, and 0, if otherwise. After training each of these models, we employ a KNN classifier to label samples in the range space of f ; the classifier uses the models\u2019 learned metrics (given by Equation (10) and Equation (18)) to establish nearest neighbors.\nWe compare our methods with several other approaches. The first one labels samples of the original feature space via the k-NN classification rule using Euclidean distances and, provides a baseline for the accuracy that can be achieved for each classification problem we considered. The second one relies on a popular DML method, namely the Large Margin Nearest Neighbor (LMNN) DML method [22]. We also selected two kernelized approaches for comparison, namely, Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3].\nWe evaluated all approaches on eight datasets from the UCI repository, namely, White Wine Quality (Wine), Wall-Following Robot Navigation (Robot), Statlog Vehicle Silhouettes (Vehicle), Molecular Biology\nSplice-junction Gene Sequences (Molecular), Waveform Database Generator Version 1 (Wave), Ionosphere (Iono), Cardiotocography (Cardio), Pima Indians Diabetes (Pima). For all datasets, each class was equally represented in number of samples. An exception is the original Wine dataset that has eleven classes, eight of which are poorly represented; for this dataset we only chose data from the other three classes.\nFor our model with general matrix kernel function K, we chose the diagonal matrix K(xi,xj) = diag{[k1(xi,xj), . . . , km(xi,xj)]\nT }, where k1 through km were Gaussian kernel functions with different spreads. For the second model, where K = k \u00b7 B, we also chose k to be a Gaussian kernel. During the test phase for all experiments, the parameters \u03b3, \u03bb, \u03c1, the output dimension m, the Gaussian kernel\u2019s spread parameter \u03c3 and the number of nearest neighbors \u03ba to be used by the KNN classifier are selected through cross-validation. Training of the models was performed using 10% and 50% of each data set. In the sequel, we provide the experimental results in figures, which display the average classification accuracies over 20 runs. Also, the error bars correspond to a 95% confidence interval of the estimated accuracies.\nWe first discuss the results in the case where we used only 10% of the training data; they are depicted in Figure 1. Our first model with general kernel function K is named as \u201cMethod 1\u201d, and the second model with specified kernel function K = k \u00b7B is called \u201cMethod 2\u201d. For almost all datasets, we observe that all five DML methods outperform the scheme involving no transformation of the original feature space (i.e., the output space coincided with the original feature space) and labeling samples via Euclidean-distance KNN classification. This remarkable fact underlines the potential benefits of DML methods. Moreover, we observe that kernelized methods usually outperform LMNN. This observation may partly justify the use of a nonlinear mapping for DML. Furthermore, we observe from the figure that both of our methods typically outperform the other four approaches. More specifically, the proposed two models achieve the highest accuracy across all datasets with the only exception on the Vehicle dataset, where ITML and KLMNN outperform slightly. It is worth mentioning that, for the Pima data set, none of the other three DML methods can enhance the performance compared to the baseline KNN classification, while our methods achieve significant improvements.\nSimilar conclusions can be drawn regarding the results generated by using 50% of the training data. These results are depicted in Figure 2. Our methods outperform all the other four methods for most datasets. An exception occurs for the Molecular dataset, where KLMNN achieves higher performance than ours. In the case of Robot and Cardio datasets, all methods perform similarly well. The reason might be that, with enough data, all of the models can be trained well enough to achieve close to optimal performance. For\nthe Pima data set, again, our methods achieve much better results than all other four methods. It is also important to note that, for our Method 1, despite the relatively simple form of the matrix kernel function we opted for, the resulting model demonstrated very competitive classification accuracy across all datasets. One would likely expect even better performance, if a more sophisticated matrix kernel function is used.\nFor the sake of visualizing the distribution of the transformed Robot data via our models in 2 dimensions, we provide Figure 3 and Figure 4. Similar to [8], we compare the produced mappings of our methods to Kernel Principal Component Analysis (KPCA). KPCA\u2019s 2-dimensional principal subspace was identified based on 10% of the available training data, i.e., 100 training patterns, and the test points were projected onto that subspace. The same training samples were also used for training our two models, which used a Gaussian kernel function and a spread parameter value \u03c3 that maximized KNN\u2019s classification accuracy.\nFrom Figure 3 we observe that KPCA\u2019s projection may only promote good discrimination between samples drawn from class 4 versus the rest. On the other hand, in Figure 4a and Figure 4b, all four classes are reasonably well-clustered in the output space obtained by our two methods. This may explain why our methods are able to achieve high classification accuracy, even when only 10% of the available data are used for training."}, {"heading": "6 Conclusions", "text": "In this paper, we proposed two new kernel-based Distance Metric Learning (DML) methods, which rely on Reproducing Kernel Hilbert Spaces (RKHSs) of vector-valued functions. Via a mapping f , the two methods map data from their original space to an output space, whose dimension can be directly controlled. Subsequent distance measurements are performed in the output space via a Mahalanobis metric. The first proposed model uses a general matrix kernel function and, thus, provides significant flexibility in modeling the input-to-output space mapping. On the other hand, the second proposed method uses a more restricted matrix kernel function, but has the advantage of implicitly determining the Mahalanobis metric. Furthermore, its matrix kernel function can be learned from data. Unlike previous kernel-based approaches, the relevant f mappings are explicit for both of our two methods. Combined with the fact that the output space dimensionality can be directly specified, the models can also be used for dimensionality reduction purposes, such as for visualizing the data in 2 or 3 dimensions. Experimental results on eight UCI benchmark data sets show that both of the proposed methods can achieve higher performance in comparison to other traditional and kernel-based DML techniques."}, {"heading": "Acknowledgements", "text": "C. Li acknowledges partial support from National Science Foundation (NSF) grant No. 0806931. Also, M. Georgiopoulos acknowledges partial support from NSF grants No. 0525429, No. 0963146, No. 1200566 and No. 1161228. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF."}], "references": [{"title": "Integrating constraints and metric learning in semi-supervised clustering", "author": ["Mikhail Bilenko", "Sugato Basu", "Raymond J. Mooney"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "A new kernelization framework for mahalanobis distance learning", "author": ["Ratthachat Chatpatanasiri", "Teesid Korsrilabutr", "Pasakorn Tangchanachaianan", "Boonserm Kijsirikul"], "venue": "algorithms. Neurocomputing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Information-theoretic metric learning", "author": ["Jason V. Davis", "Brian Kulis", "Prateek Jain", "Inderjit S. Dhillon"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "A metric learning perspective of svm: On the relation of LMNN and SVM", "author": ["Huyen Do", "Alexandros Kalousis", "Jun Wang", "Adam Woznica"], "venue": "In JMLR W&CP 5: Proceedings of Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Neighbourhood components analysis", "author": ["Jacob Goldberger", "Sam Roweis", "Geoff Hinton", "Ruslan Salakhutdinov"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Gsml: A unified framework for sparse metric learning", "author": ["Kaizhu Huang", "Yiming Ying", "Colin Campbell"], "venue": "In Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Inductive regularized learning of kernel functions", "author": ["Prateek Jain", "Brian Kulis"], "venue": "Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Metric and kernel learning using a linear transformation", "author": ["Prateek Jain", "Brian Kulis", "Jason V. Davis", "Inderjit S. Dhillon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Convex perturbations for scalable semidefinite programming", "author": ["Brian Kulis", "Suvrit Sra", "Dhillon", "Inderjit"], "venue": "In JMLR W&CP 5: Proceedings of Fifteenth International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Explicit solutions of linear matrix equations", "author": ["Peter Lancaster"], "venue": "SIAM Review, 12:pp", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1970}, {"title": "Geometry-aware metric learning", "author": ["Zhengdong Lu", "Prateek Jain", "Inderjit S. Dhillon"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "The mahalanobis distance", "author": ["R. De Maesschalck", "D. Jouan-Rimbaud", "D.L. Massart"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "On learning vector-valued functions", "author": ["Charles A. Micchelli", "Massimiliano Pontil"], "venue": "Neural Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Metric learning: A support vector approach", "author": ["Nam Nguyen", "Yunsong Guo"], "venue": "In Proceedings of the European conference on Machine Learning and Knowledge Discovery in Databases - Part II, ECML PKDD", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Large margin multi-task metric learning", "author": ["Shibin Parameswaran", "Kilian Q. Weinberger"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Learning a distance metric from relative comparisons", "author": ["Matthew Schultz", "Thorsten Joachims"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Large margin component analysis", "author": ["Lorenzo Torresani", "Kuang-chih Lee"], "venue": "Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Distance metric learning with kernels", "author": ["Ivor W. Tsang", "James T. Kwok"], "venue": "In Proceedings of International Conference on Artificial Neural Networks (ICANN),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Visualizing data using t-sne", "author": ["Laurens Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Metric learning with multiple kernels", "author": ["Jun Wang", "Huyen Do", "Adam Woznica", "Alexandros Kalousis"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q. Weinberger", "Laurence K. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["Eric P. Xing", "Andrew Y. Ng", "Michael I. Jordan", "Stuart Russell"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Distance metric learning with eigenvalue optimization", "author": ["Yiming Ying", "Peng Li"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Transfer metric learning by learning task relationships", "author": ["Yu Zhang", "Dit-Yan Yeung"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "see [13]), or generalizations of it, where the weights are inferred from the data.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "For example, the authors in [8] pointed out an equivalence between kernel learning and metric learning in the feature space.", "startOffset": 28, "endOffset": 31}, {"referenceID": 13, "context": "Leveraged by the Representer Theorem proposed in [14], all computations of both methods involve only kernel calculations.", "startOffset": 49, "endOffset": 53}, {"referenceID": 19, "context": "Thus, we can access the transformed data in the output space, and this feature can be even used to visualize the data [20], when the output space is 2- or 3-dimensional.", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "To demonstrate the merit of our methods, we compare them to standard k-NN classification (without DML) and other recent kernelized DML algorithms, including Large Margin Nearest Neighbor (LMNN) [22], Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3].", "startOffset": 194, "endOffset": 198}, {"referenceID": 3, "context": "To demonstrate the merit of our methods, we compare them to standard k-NN classification (without DML) and other recent kernelized DML algorithms, including Large Margin Nearest Neighbor (LMNN) [22], Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3].", "startOffset": 245, "endOffset": 248}, {"referenceID": 2, "context": "To demonstrate the merit of our methods, we compare them to standard k-NN classification (without DML) and other recent kernelized DML algorithms, including Large Margin Nearest Neighbor (LMNN) [22], Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3].", "startOffset": 277, "endOffset": 280}, {"referenceID": 22, "context": "[23] proposed an early DML method, which minimizes the distance between similar points, while enlarging the distance between dissimilar points.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In [17], relative comparison constraints that involve three points at a time are considered.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Neighborhood Components Analysis (NCA) [6] is proposed to learn a Mahalanobis distance for the k-NN classifier by maximizing the leave-one-out k-NN performance.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "[1] proposed a DML method for clustering.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "Large Margin Nearest Neighbor (LMNN) DML model [22] aims to produce a mapping, so that the k-nearest neighbors of any given sample belong to the same class, while samples from different classes are separated by large margins.", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "Similarly, a Support Vector-based method is proposed in [15].", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "Also, LMNN is further extended to a Multi-Task Learning variation [16].", "startOffset": 66, "endOffset": 70}, {"referenceID": 24, "context": "Another multi-task DML model is proposed in [25] that searches for task relationships.", "startOffset": 44, "endOffset": 48}, {"referenceID": 6, "context": "In [7], the authors proposed a general framework for sparse DML, such that several previous works are subsumed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 23, "context": "Recently, an eigenvalue optimization framework for DML was developed an presented in [24].", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "Moreover, the connection between LMNN and Support Vector Machines (SVMs) was discussed in [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 18, "context": "In the early work of [19], the Lagrange dual problem of the proposed DML formulation is derived, and the DML method is kernelized in the dual domain.", "startOffset": 21, "endOffset": 25}, {"referenceID": 3, "context": "Information-Theoretic Metric Learning (ITML) [4] is another kernelized method, which is based on minimizing the Kullback-Leibler divergence between two distributions.", "startOffset": 45, "endOffset": 48}, {"referenceID": 17, "context": "The kernelization of LMNN is discussed in [18] and [10].", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "The kernelization of LMNN is discussed in [18] and [10].", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "Moreover, a Kernel Principal Component Analysis (KPCA)-based kernelized algorithm is developed in [3], such that many DML methods, such as LMNN, can be kernelized.", "startOffset": 98, "endOffset": 101}, {"referenceID": 11, "context": "In [12], the Mahalanobis matrix and kernel matrix are learned simultaneously.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In [8] and its extended work [9], the authors proposed a framework that builds connections between kernel learning and DML in the kernel-induced feature space.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [8] and its extended work [9], the authors proposed a framework that builds connections between kernel learning and DML in the kernel-induced feature space.", "startOffset": 29, "endOffset": 32}, {"referenceID": 20, "context": "is discussed in [21].", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Before introducing our methods, in this section we will briefly review the concept of Reproducing Kernel Hilbert Space (RKHS) for vector-valued functions as presented in [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 13, "context": "The functional of Problem (6) satisfies the conditions stipulated by the Representer Theorem for Hilbert spaces of vector-valued elements (Theorem 5 in [14]) and, therefore, for a fixed value of L, the unique minimizer f\u0302 is of the form:", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "As noted in [11], this matrix equation can be solved for C as follows:", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "The second one relies on a popular DML method, namely the Large Margin Nearest Neighbor (LMNN) DML method [22].", "startOffset": 106, "endOffset": 110}, {"referenceID": 3, "context": "We also selected two kernelized approaches for comparison, namely, Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "We also selected two kernelized approaches for comparison, namely, Information-Theoretic Metric Learning (ITML) [4] and kernelized LMNN (KLMNN) [3].", "startOffset": 144, "endOffset": 147}, {"referenceID": 7, "context": "Similar to [8], we compare the produced mappings of our methods to Kernel Principal Component Analysis (KPCA).", "startOffset": 11, "endOffset": 14}], "year": 2017, "abstractText": "In this paper we present two related, kernel-based Distance Metric Learning (DML) methods. Their respective models non-linearly map data from their original space to an output space, and subsequent distance measurements are performed in the output space via a Mahalanobis metric. The dimensionality of the output space can be directly controlled to facilitate the learning of a low-rank metric. Both methods allow for simultaneous inference of the associated metric and the mapping to the output space, which can be used to visualize the data, when the output space is 2or 3-dimensional. Experimental results for a collection of classification tasks illustrate the advantages of the proposed methods over other traditional and kernel-based DML approaches.", "creator": "LaTeX with hyperref package"}}}