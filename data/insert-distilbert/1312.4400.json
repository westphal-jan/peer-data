{"id": "1312.4400", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Network In Network", "abstract": "we propose incorporating a novel network structure called \" network in network \" ( nin ) to enhance the model discriminability for local receptive fields. the conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. instead, we build micro neural networks with more complex structures planned to handle the variance of the local receptive fields. we instantiate the micro neural network region with a nonlinear multiple second layer structure which is a potent function frequency approximator. the feature maps are obtained by effectively sliding the micro networks over, the input in a similar transverse manner of cnn and then fed into the next layer. the deep nin is thus implemented as stacking of multiple sliding micro neural networks. we demonstrated state - of - the - art integrated classification performances with nin on cifar - 10 / 100, svhn and minst digital datasets.", "histories": [["v1", "Mon, 16 Dec 2013 15:34:13 GMT  (501kb,D)", "http://arxiv.org/abs/1312.4400v1", null], ["v2", "Wed, 18 Dec 2013 09:30:27 GMT  (509kb,D)", "http://arxiv.org/abs/1312.4400v2", "9 pages, 4 figures, for iclr2014"], ["v3", "Tue, 4 Mar 2014 05:15:42 GMT  (445kb,D)", "http://arxiv.org/abs/1312.4400v3", "10 pages, 4 figures, for iclr2014"]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["min lin", "qiang chen", "shuicheng yan"], "accepted": true, "id": "1312.4400"}, "pdf": {"name": "1312.4400.pdf", "metadata": {"source": "CRF", "title": "Network In Network", "authors": ["Min Lin", "Qiang Chen", "Shuicheng Yan"], "emails": ["linmin@nus.edu.sg", "chenqiang@nus.edu.sg", "eleyans@nus.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "Convolution neural networks (CNNs) [1] consist of alternating convolutional layers and pooling layers. Convolution layers take inner product of the linear filter and the underlying receptive field followed by a nonlinear activation function at every local portion of the input. The resulting outputs are called feature maps. The visualization of CNN [2] gives hint that the activations in the feature map correspond to some latent concepts, a bigger value in the feature map indicates a higher probability that the corresponding input patch contains the latent concept. Thus we can view the activations in the feature maps as confidence values of latent concepts. The convolutional filter can also be seen as a linear binary classifier for local patches, and the separation hyperplane is defined by the filter.\nThe above process suffices when the latent concepts are linearly separable. However, the confidence of a latent concept is often a highly nonlinear function of the input data[3], leading to complex separation hyperplanes. The conventional CNN utilizes nonlinear rectifier layer to achieve better overall discriminability in the whole architecture while leaves the underlying assumption that the local latent concepts are linearly separable. Here, we argue that more discriminative local latent concept modeling can be achieved by replacing the conventional convolutional operation with a nonlinear network structure which we call \u201cmicro network\u201d. In this work, we choose multilayer perceptron as the instantiation of the micro network, which is a universal function approximator and a neural network trainable by back-propagation.\nThe resulting structure compared with CNN is illustrated in Figure 1 which we call a mlpconv layer. Both the linear convolutional layer and the mlpconv layer map the local receptive field to a confidence value of the latent concept. The mlpconv consists of multiple fully-connected layers with nonlinear activation functions. The feature maps are obtained by sliding the multilayer perceptron over the input in a similar manner of CNN and are then fed into the next layer. The overall structure of our classification network is the stacking of multiple mlpconv layers. It is called \u201cNetwork In Network\u201d as we have micro networks to convolve the input of mlpconv layers which are composing\nar X\niv :1\n31 2.\n44 00\nv1 [\ncs .N\nE ]\n1 6\nD ec\n2 01\nelements of the overall deep network. Benefiting from more discriminative local micro network modeling, the feature maps obtained have much stronger semantic meaning. Therefore, instead of adopting the traditional fully connected layers for classification in CNN, we directly output the average of the feature maps from the last mlpconv layer as the confidence of categories via a global average pooling layer. The resulting vector is then softmaxed and used for classification."}, {"heading": "2 Convolutional Neural Networks", "text": "Classic convolutional neuron networks consist of alternatively stacked convolutional layers and spatial pooling layers. The convolutional layers generate feature maps by a linear convolutional filter followed by a nonlinear activation function (rectifier, sigmoid, tanh, etc.). Using linear rectifier as an example, the feature map can be calculated as in Equation 1.\nfi,j,k = relu(w T k xi,j) (1)\nHere i and j are the pixel indexes in the feature map, xij stands for the input patch centered at location (i, j), and k is used to index the channels of the feature map.\nThis linear convolution is sufficient when the instances of the latent concepts are linearly separable. However, the confidence of the latent concepts are generally highly nonlinear functions of the input data. In conventional CNN, this might be compensated by utilizing an over-complete set of filters [4] to cover all variations latent concepts. Namely, individual linear filters can be learned to detect different variations of a same concept. However, having too many filters for a single concept imposes extra burden on the next layer, which needs to consider all combinations of variations from the previous layer[5]. As in CNN, filters from higher layers map to larger regions in the original input. It generates a higher level concept by combining the lower level concept from the layer below. Therefore, we argue that it would be beneficial to do a better classification on each local patches, before combining them into higher level concepts.\nIn the recent maxout networks [6], the number of feature maps is reduced by maximum pooling over affine feature maps. Maximization over linear functions makes a piecewise linear approximator which is capable of approximating any convex functions. Compared to conventional convolutional layers which perform linear separation, maxout network is more potent as it can separate convex sets which is the super set of linear subspaces. This improvement endows maxout network with the best performances on several benchmark datasets.\nHowever, maxout network imposes the prior that instances of a latent concept lie within a convex set in the input space which does not necessarily hold. It would be necessary to employ a more general function approximator when the distributions of the latent concepts are more complex. We seek to achieve this by introducing the novel \u201cNetwork In Network\u201d structure, in which a micro network is introduced within each convolutional layer as a stronger classifier for local patches."}, {"heading": "3 Network In Network", "text": "We first highlight the key components of our proposed \u201cNetwork In Network\u201d structure. i.e. the MLP convolutional layer and the global averaging pooling layer. Then we detailed the overall NIN in Sec. 3.3."}, {"heading": "3.1 MLP Convolution Layers", "text": "Given no priors about the distributions of the latent concepts, it is desirable to use a universal function approximator for modeling of the local patches, as it is capable of approximating more general probability functions of the latent concepts. Radial basis network and multilayer perceptron are two well known universal function approximators. We choose multilayer perceptron in this work for two reasons: 1. Multilayer perceptron is compatible with the structure of convolutional neural networks, which is trained using back-propagation. 2. Multilayer perceptron can be a deep model itself, which is consistent with the spirit of feature re-use [3]. This new type of layer is called mlpconv in this paper, in which MLP replaces the linear filter and convolves over the input.\nFigure 1 illustrates the difference between linear convolutional layer and mlpconv layer. Note that in Figure 1 the generation of only one feature map is shown. When the mlpconv layer generates multiple feature maps (which is always the case), they can either each own a multilayer perceptron or share the hidden nodes. We choose the shared scheme because it requires less computation with the same number of parameters. The calculation performed by mlpconv layer is shown in Equation 2.\nf1i,j,k1 = relu(w 1 k1 T xi,j + bk1) ... fni,j,kn = relu(w n kn T fn\u22121i,j + bkn) (2)\nHere n is the number of layers in the multilayer perceptron. Rectified linear unit is used as the activation function in the multilayer perceptron.\nFrom cross channel(feature map) pooling point of view, Equation 2 is equivalent to multi step parametric cross channel pooling. Each pooling layer performs weighted linear recombination on the input feature maps, which then go through a rectifier linear unit. The cross pooled feature maps are cross pooled again and again in the next layers. This deep parametric cross channel pooling structure allows complex and learnable interactions of cross channel information.\nComparison to maxout layers: maxout layers in the maxout network performs max pooling across multiple affine feature maps [6]. The feature maps of maxout layers are calculated as in Equation 3.\nfi,j,k = max n\n(wTknxi,j) (3)\nMaxout over linear functions forms a piecewise linear function which is capable of modeling any convex function. For a convex function, samples with function values below a specific threshold form a convex set. Therefore, by approximating convex functions of the local patch, maxout has the capability of forming separation hyperplanes for concepts whose samples are within a convex set (i.e. l2 balls, convex cones). Mlpconv layer differs from maxout layer in that the convex function approximator is replaced by a universal function approximator, which has more capability in modeling various distributions of latent concepts."}, {"heading": "3.2 Global Average Pooling", "text": "Conventional convolutional neural networks perform convolution in the lower layers of the network. For classification, the feature maps of the last convolutional layer are vectorized and fed into fully connected layers followed by a softmax logistic regression layer [7] [6] [8]. This structure bridges the convolutional structure with traditional neural network classifiers. It treats the convolutional layers as feature extractors, and the resulting feature is classified in a traditional way.\nHowever, the fully connected layers are prone to overfitting, thus hampering the generalization ability of the overall network. Dropout is proposed by Hinton et al. [9] as a regularizer which randomly sets half of the activations from the fully connected layers to zero during training. It has successfully shown improvements on the generalization ability and largely prevents overfitting [7].\nIn this paper, we propose another strategy called global average pooling to improve the generalization ability of the network. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the softmax layer. A vectorized view of global average pooling is that the output of the last mlpconv layer is forced into orthogonal subspaces for different categories of inputs. However, for fully connected layers, it does not impose this constraint. Another advantage of the global average pooling over fully connected layers is that there is no parameter to optimize thus overfitting is avoided.\nWe can see global average pooling as a structural regularizer that explicitly enforces feature maps to be confidence maps of concepts (categories in the last mlpconv layer). This is made possible by the mlpconv layer, as the multilayer perceptron can better model the confidence of the concepts."}, {"heading": "3.3 Network In Network Structure", "text": "The overall structure of NIN is a stack of mlpconv layers, on top of which lies the global average pooling and the objective cost layer. Sub-sampling layers can be added in between the mlpconv layers as it is done in CNN and maxout networks. Figure 2 shows an NIN with three mlpconv layers, within each mlpconv layer, there is a three layer perceptron. The number of layers in either NIN or the micro networks is flexible and can be changed for specific tasks."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Overview", "text": "We evaluate NIN on four benchmark datasets: CIFAR-10, CIFAR-100, SVHN and MNIST. The networks used for the datasets all consist of three stacked mlpconv layers, the mlpconv layers in all the experiments are followed by a spatial max pooling layer which down samples the input image by a factor of two. As a regularizer, dropout is applied on the outputs of all but the last mlpconv layers. Unless stated specifically, all the networks used in the experiment section use global average pooling instead of fully connected layers at the top of the network. Another regularizer applied is weight decay as is used by Krizhevsky et al. [7]. Figure 2 illustrates the overall structure of NIN network used in this section. We implemente our network on the super fast cuda-convnet code developed by Alex Krizhevsky [7]. Preprocessing of the datasets, splitting of training and validation sets all follow Goodfellow et al. [6].\nWe adopt the training procedure used by Krizhevsky et al. [7]. Namely, we manually set proper initializations for the weights and the learning rates. The network is trained using mini-batches of size 128. The training process continues until the accuracy on the training set stops improving, then\nthe learning rate is lowered by a scale of 10. The learning rate is reduced to one tenth for the second time, after the training accuracy saturate again."}, {"heading": "4.2 CIFAR-10", "text": "The CIFAR-10 dataset [10] is composed of 10 classes of natural images with 50,000 images in total, and 10,000 testing images. Each image is an RGB image of size 32x32. For this dataset, we apply the same global contrast normalization and ZCA whitening as was used by Goodfellow et al. in the maxout network [6]. We use the last 10,000 images of the training set as validation data.\nThe network used for CIFAR-10 contains three mlpconv layers, each of which uses a three layer perceptron to convolve its input. The number of feature maps for each mlpconv layer is set to the same number as in the corresponding maxout network. Two hyper-parameters are tuned using the validation set, i.e. the local receptive field size and the weight decay. After that the hyperparameters are fixed and we re-train the network from scratch with both the training set and the validation set. The resulting model is used for testing. We obtain a test error of 10.41% on this dataset, which improves more than one percent compared to the state-of-the-art. A comparison with previous methods is shown in Table 1.\nTo be consistent with previous works, we also evaluate our method on the CIFAR-10 dataset with translation and horizontal flipping augmentation. We are able to achieve a test error of 8.81%, which sets the new state of art performance."}, {"heading": "4.3 CIFAR-100", "text": "The CIFAR-100 dataset [10] is the same in size and format as the CIFAR-10 dataset, but contains 100 classes. Thus the number of images in each class is only one tenth of the CIFAR-10 dataset. For CIFAR-100 we do not tune the hyper-parameters, but use the same setting as the CIFAR-10 dataset. The only difference is that the last mlpconv layer outputs 100 feature maps. A test error of 35.68% is obtained for CIFAR-100 which surpasses the current best performance without data augmentation by more than two percent. Details of the performance comparison are shown in Table 2."}, {"heading": "4.4 Street View House Numbers", "text": "The SVHN dataset [14] is composed of 630420 32x32 color images, divided into training set, testing set and an extra set. The task for this data set is to classify the digit located at the center of each\nimage. The training and testing procedure follow Goodfellow et al. [6]. Namely 400 samples per class selected from the training set and 200 samples per class from the extra set are used for validation. The remainder of the training set and the extra set are used for training. The validation set is only used as a guidance for hyper-parameter selection, but never used for training the model.\nPreprocessing of the dataset again follows Goodfellow et al. which was a local contrast normalization. The structure and parameters used in SVHN is similar to those used for CIFAR-10, which consists of three mlpconv layers followed by global average pooling. For this dataset, we obtain a test error rate of 2.35%, which sets the new state-of-the-art. We compare our result with methods that did not augment the data, the comparison is shown in Table 3."}, {"heading": "4.5 MNIST", "text": "The MNIST [1] dataset consists of hand written digits 0-9 which are 28x28 in size. There are 60,000 training images and 10,000 testing images in total. For this dataset, the same network structure as used for cifar10 is adopted. But the numbers of feature maps generated from each mlpconv layer are reduced. Because MNIST is a simpler dataset compared to CIFAR-10 and hence fewer parameters are needed. We test our method on this dataset without data augmentation. The result is compared with previous works that adopted convolutional structures, and are shown in Table 4\nWe do not achieve better performance than the current best for the MNIST dataset. However, since MNIST has been tuned to a very low error rate of 0.45%, we believe 0.47% is already a comparable result."}, {"heading": "4.6 Global Average Pooling as a Regularizer", "text": "Global average pooling layer is similar to fully connected layer in that they both perform linear transformations of the vectorized feature maps. The difference lies in the transformation matrix. For global average pooling, the transformation matrix is prefixed and it is non-zero only on block diagonal elements which share the same value. Fully connected layers can have dense transformation matrices and the values are subject to back-propagation optimization. To study the regularization effect of global average pooling, we replace the global average pooling layer with a fully connected layer, while the other parts of the model remain the same. We evaluated this model with and without dropout before the fully connected linear layer. Both models are tested on the CIFAR-10 dataset, a comparison of the performances are shown in Table 5.\nAs is shown in Table 5, fully connected layer without dropout regularization gave the worst performance (11.59%). It is expected as the fully connected layer overfits to the training data if no regularizer is applied. Adding dropout before the fully connected layer reduced the testing error (10.88%). Global average pooling has achieved the lowest testing error (10.41%) among the three.\nWe then explored whether the global average pooling had the same regularization effect for conventional CNNs. We instantiated a conventional CNN as described by Hinton et al. [9], which consists of three convolutional layers and one local connection layer. The local connection layer generated 16 feature maps which were fed to a fully connected layer with dropout. To make the comparison fair, we reduced the feature map number of the local connection layer from 16 to 10, since only one feature map is allowed for each category in the global average pooling scheme. A equivalent network with global average pooling is then created by replacing the dropout + fully connected layer with global average pooling. The performances were tested on the CIFAR-10 dataset.\nWe achieved a similar performance (15.99%) as reported by Hinton et al. [9] for convectional CNN with fully connected layer and dropout. Replacing the fully connected layer with global average pooling layer slightly hampered the performance (test error 16.46%). As the global average pooling enforces each feature map to be correspondence to one of the categories in the classification task, it work out in NIN because the multilayer perceptrons in mlpconv layers made the feature maps more semantic meaningful. But for linear convolutional layers this constraint might be too strong. Nevertheless, the regularization effect is still obvious as the error rate is 17.56% when using no regularizer at all."}, {"heading": "4.7 Visualization of NIN", "text": "We explicitly enforce feature maps in the last mlpconv layer of NIN to be confidence maps of the categories by means of global average pooling, which is possible only with stronger local receptive field modeling, e.g. mlpconv in NIN. To understand how much this purpose is accomplished, we extract and directly visualize the feature maps from the last mlpconv layer of the trained model for CIFAR-10.\nFigure 3 shows some examplar images and their corresponding feature maps for each of the ten categories selected from CIFAR-10 test set. It is expected that largest activations are observed in the feature map corresponding to the ground truth category of the input image; as this is enforced by global average pooling. Within the feature map of the ground truth category, we observed that the strongest activations appear roughly at the same region of the object in the original image. It is especially true for structured objects, for examples the car in the second row of Figure 3. Note that the feature maps for the categories are trained with only category information. Better results are expected if bounding boxes of the objects are used for fine grained labels. Another interesting observation in Figure 3 is the confusion between categories. For example, the car in the second row has some activations in the feature map for truck; the deer in the fifth row has some activations in the feature map for horse.\nThe visualization again demonstrates the effectiveness of NIN. It is achieved via a stronger local receptive field modeling using mlpconv layers, the global average pooling then enforces the learning of category level feature maps. Further exploration can be made towards general object detection. Detection results can be achieved based on the category level feature maps in the same flavor as in the scene labeling work of Farabet et al. [16]"}, {"heading": "5 Conclusion", "text": "We propose a novel structure called \u201cNetwork In Network\u201d (NIN) for classification tasks. This new structure consists of mlpconv layers which use multilayer perceptrons to convolve the input and a global average pooling layer as a replacement for the fully connected layers in conventional CNN. Mlpconv layers model the local patches better, and global average pooling act as a structural regularizer that prevents overfitting globally. With this two components of NIN we demonstrate\nstate-of-the-art the performance on CIFAR-10, CIFAR-100, SVHN and MNIST datasets. Through visualization of the feature maps, we demonstrate that feature maps from the last mlpconv layer of NIN are similar to confidence maps of the categories, and this shows a possibility of performing object detection in NIN."}], "references": [{"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Visualizing and understanding convolutional neural networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "arXiv preprint arXiv:1311.2901,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Y Bengio", "A Courville", "P Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Ica with reconstruction cost for efficient overcomplete feature learning", "author": ["Quoc V Le", "Alexandre Karpenko", "Jiquan Ngiam", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Piecewise linear multilayer perceptrons and dropout", "author": ["Ian J Goodfellow"], "venue": "arXiv preprint arXiv:1301.5088,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "arXiv preprint arXiv:1301.3557,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Master\u2019s thesis,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "venue": "arXiv preprint arXiv:1206.2944,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Learnable pooling regions for image classification", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "arXiv preprint arXiv:1301.3516,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Improving neural networks with dropout", "author": ["Nitish Srivastava"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Learning hierarchical features for scene labeling", "author": ["Clement Farabet", "Camille Couprie", "Laurent Najman", "Yann LeCun"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Convolution neural networks (CNNs) [1] consist of alternating convolutional layers and pooling layers.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "The visualization of CNN [2] gives hint that the activations in the feature map correspond to some latent concepts, a bigger value in the feature map indicates a higher probability that the corresponding input patch contains the latent concept.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "However, the confidence of a latent concept is often a highly nonlinear function of the input data[3], leading to complex separation hyperplanes.", "startOffset": 98, "endOffset": 101}, {"referenceID": 3, "context": "In conventional CNN, this might be compensated by utilizing an over-complete set of filters [4] to cover all variations latent concepts.", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": "However, having too many filters for a single concept imposes extra burden on the next layer, which needs to consider all combinations of variations from the previous layer[5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 2, "context": "Multilayer perceptron can be a deep model itself, which is consistent with the spirit of feature re-use [3].", "startOffset": 104, "endOffset": 107}, {"referenceID": 5, "context": "For classification, the feature maps of the last convolutional layer are vectorized and fed into fully connected layers followed by a softmax logistic regression layer [7] [6] [8].", "startOffset": 168, "endOffset": 171}, {"referenceID": 6, "context": "For classification, the feature maps of the last convolutional layer are vectorized and fed into fully connected layers followed by a softmax logistic regression layer [7] [6] [8].", "startOffset": 176, "endOffset": 179}, {"referenceID": 7, "context": "[9] as a regularizer which randomly sets half of the activations from the fully connected layers to zero during training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "It has successfully shown improvements on the generalization ability and largely prevents overfitting [7].", "startOffset": 102, "endOffset": 105}, {"referenceID": 5, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We implemente our network on the super fast cuda-convnet code developed by Alex Krizhevsky [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "The CIFAR-10 dataset [10] is composed of 10 classes of natural images with 50,000 images in total, and 10,000 testing images.", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "Method Test Error Stochastic Pooling [8] 15.", "startOffset": 37, "endOffset": 40}, {"referenceID": 9, "context": "13% CNN + Spearmint [11] 14.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "41% CNN + Spearmint + Data Augmentation [11] 9.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "38% DropConnect + 12 networks + Data Augmentation [12] 9.", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "The CIFAR-100 dataset [10] is the same in size and format as the CIFAR-10 dataset, but contains 100 classes.", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "Method Test Error Learned Pooling [13] 43.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "71% Stochastic Pooling [8] 42.", "startOffset": 23, "endOffset": 26}, {"referenceID": 12, "context": "The SVHN dataset [14] is composed of 630420 32x32 color images, divided into training set, testing set and an extra set.", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "Method Test Error Stochastic Pooling [8] 2.", "startOffset": 37, "endOffset": 40}, {"referenceID": 13, "context": "80% Rectifier + Dropout [15] 2.", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "78% Rectifier + Dropout + Synthetic Translation [15] 2.", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "The MNIST [1] dataset consists of hand written digits 0-9 which are 28x28 in size.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "Method Test Error 2-Layer CNN + 2-Layer NN [8] 0.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "53% Stochastic Pooling [8] 0.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "[9], which consists of three convolutional layers and one local connection layer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] for convectional CNN with fully connected layer and dropout.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[16]", "startOffset": 0, "endOffset": 4}], "year": 2013, "abstractText": "We propose a novel network structure called \u201cNetwork In Network\u201d(NIN) to enhance the model discriminability for local receptive fields. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to handle the variance of the local receptive fields. We instantiate the micro neural network with a nonlinear multiple layer structure which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner of CNN and then fed into the next layer. The deep NIN is thus implemented as stacking of multiple sliding micro neural networks. We demonstrated state-of-the-art classification performances with NIN on CIFAR 10/100, SVHN and MINST datasets.", "creator": "LaTeX with hyperref package"}}}