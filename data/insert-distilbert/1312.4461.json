{"id": "1312.4461", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2013", "title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep Neural Networks", "abstract": "assuming scalability properties of deep neural networks raise most key research questions, particularly as the problems seem considered become larger and more challenging. typically this paper expands on stating the idea of conditional computation introduced by bengio, et. al., where the nodes of a deep network program are augmented by a set algorithm of gating units that determine when a trusted node should be calculated. by factorizing the weight matrix representations into a statistical low - rank numerical approximation, an estimation of the sign of the pre - nonlinearity activation can be efficiently obtained. for networks using rectified - linear hidden units, this implies that the computation of a hidden fuzzy unit with an estimated negative pre - nonlinearity can be ommitted completely altogether, as its value will become zero when nonlinearity is applied. for sparse neural networks, this can partially result in gaining considerable speed rehearsal gains. experimental results using the mnist and svhn algorithm data sets with a fully - connected deep neural network demonstrate the performance robustness of the proposed scheme with close respect to the error introduced by the conditional filter computation process.", "histories": [["v1", "Mon, 16 Dec 2013 18:58:34 GMT  (70kb)", "http://arxiv.org/abs/1312.4461v1", "8 pages, 5 figures. Submitted to ICLR 2014"], ["v2", "Wed, 18 Dec 2013 18:11:21 GMT  (70kb)", "http://arxiv.org/abs/1312.4461v2", "8 pages, 5 figures. Submitted to ICLR 2014"], ["v3", "Sat, 21 Dec 2013 16:57:47 GMT  (70kb)", "http://arxiv.org/abs/1312.4461v3", "8 pages, 5 figures. Submitted to ICLR 2014"], ["v4", "Tue, 28 Jan 2014 22:29:55 GMT  (71kb)", "http://arxiv.org/abs/1312.4461v4", "10 pages, 5 figures. Submitted to ICLR 2014"]], "COMMENTS": "8 pages, 5 figures. Submitted to ICLR 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew davis", "itamar arel"], "accepted": false, "id": "1312.4461"}, "pdf": {"name": "1312.4461.pdf", "metadata": {"source": "CRF", "title": "Low-Rank Approximations for Conditional Feedforward Computation", "authors": ["Andrew S. Davis"], "emails": ["andrew.davis@utk.edu", "itamar@utk.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n44 61\nv1 [\ncs .L\nG ]"}, {"heading": "1 Introduction", "text": "In recent years, deep neural networks have redefined state-of-the-art in many application domains, notably in computer vision [11] and speech processing [14]. In order to scale to more challenging problems, however, neural networks must become larger, which implies an increase in computational resources. Shifting computation to highly parallel platforms such as GPUs has enabled the training of massive neural networks that would otherwise train too slowly on conventional CPUs. While the extremely high computational power used for the experiment performed in [12] (16,000 cores training for many days) was greatly reduced in [4] (3 servers training for many days), specialized high-performance platforms still require several machines and several days of processing time. However, there may exist more fundamental changes to the algorithms involved which can greatly assist in scaling neural networks.\nMany of these state-of-the-art networks have several common properties: the use of rectified-linear activation functions in the hidden neurons, and a high level of sparsity induced by dropout regularization or a sparsity-inducing penalty term on the loss function. Given that many of the activations are effectively zero, due to the combination of sparsity and the hard thresholding of rectified linear units, a large amount of computation is wasted on calculating values that are eventually truncated to zero and provide no contribution to the network outputs or error components. Here we focus on this\nkey observation in devising a scheme that can predict the zero-valued activations in a computationally cost-efficient manner."}, {"heading": "2 Conditional Computation in Deep Neural Networks", "text": ""}, {"heading": "2.1 Exploiting Redundancy in Deep Architectures", "text": "In [5], the authors made the observation that deep models tend to have a high degree of redundancy in their weight parameterization. The authors exploit this redundancy in order to train as few as 5% of the weights in a neural network while estimating the other 95% with the use of carefully constructed low-rank decompositions of the weight matrices. Such a reduction in the number of active training parameters can render optimization easier by reducing the number of variables to optimize over. Moreover, it can help address the problem of scalability by greatly reducing the communication overhead in a distributed system.\nAssuming there is a considerable amount of redundancy in the weight parameterization, a similar level of redundancy is likely found in the activation patterns of individual neurons. Therefore, given an input sample, the set of redundant activations in the network may be approximated. If a sufficiently accurate approximation can be obtained using low computational resources, activations for a subset of neurons in the network\u2019s hidden layers need not be calculated.\nIn [2] and [3], the authors propose the idea of conditional computation in neural networks, where the network is augmented by a gating model that turns activations on or off depending on the state of the network. If this gating model is able to reliably estimate which neurons need to be calculated for a particular input, great improvements in computational efficiency may be obtainable if the network is sufficiently sparse. Figure 2.1 illustrates a conditional computation unit augmenting a layer of a neural net by using some function f (U, V, al) to determine which hidden unit activations, a (i) l+1, in layer l + 1 should be computed given the activations al of layer l."}, {"heading": "2.2 Sparse Representations, Activation Functions, and Prediction", "text": "For many datasets considered in the literature, sparse representations have been shown to be superior to dense representations, particularly in the context of deep architectures [7]. However, sparse representations learned by neural networks with sigmoidal activations are not truly \u201csparse\u201d, as activations only approach zero in the limit towards negative infinity. A conditional computation model estimating the sparsity of a sigmoidal network would thus have to impose some threshold, beyond which the neuron is considered inactive. So-called \u201chard-threshold\u201d activation functions such as rectified-linear units, on the other hand, produce true zeros which can be used by conditional computation models without imposing additional hyperparameters."}, {"heading": "3 Problem Formulation", "text": ""}, {"heading": "3.1 Estimating Activation Sign via Low-Rank Approximation", "text": "Given the activation al of layer l of a neural network, the activation al+1 of layer l + 1 is given by:\nal+1 = \u03c3(alWl) (1)\nwhere \u03c3(\u00b7) denotes the function defining the neuron\u2019s non-linearity, al \u2208 Rn\u00d7h1 , al+1 \u2208 Rn\u00d7h2 , Wl \u2208 Rh1\u00d7h2 . If the weight matrix is highly redundant, as in [5], it can be well-approximated using a low-rank representation and we may rewrite (1) as\nal+1 = \u03c3(alUlVl) (2)\nwhere UlVl is the low-rank approximation of Wl, Ul \u2208 Rh1\u00d7k, Vl \u2208 Rk\u00d7h2 , k \u226a min(h1, h2). So long as k < h1h2\nh1+h2 , the low-rank multiplication alUlVl requires fewer arithmetic operations than\nthe full-rank multiplication alWl, assuming the multiplication alUl occurs first. When \u03c3(\u00b7) is the rectified-linear function,\n\u03c3(x) = max(0, x) (3)\nsuch that all negative elements of the linear transform alWl become zero, one only needs to estimate the sign of the elements of the linear transform in order to predict the zero-valued elements. Because the weights in a deep neural network can be well-approximated using a low-rank estimation, the small error in the low-rank estimation is of marginal relevance in the context of in recovering the sign of the operation. Figure 3.1 illustrates the error profile of a low-rank estimation compared with that of the activation sign estimator as a function of the rank (varied from one to full-rank). One can see that the error of the activation sign estimator diminishes far more quickly than the error of the low-rank activation.\nGiven a low-rank approximation Wl \u2248 W\u0302l, the estimated sign of al+1 is given by sgn(al+1) \u2248 sgn(alW\u0302l) = Sl (4)\nEach entry a(i) l+1 is given by a dot product between the row vector al and the column vector W (i) l . If sgn(alW\u0302 (i) l ) = \u22121, then the true activation a(i) l+1 is likely negative, and will likely become zero after the rectified-linear function is applied. Considerable speed gains are possible if we skip those dot products based on the prediction; such gains are especially substantial when the network is very sparse. The overall activations for a hidden layer l augmented by the activation estimator is given by \u03c3 (a (W \u00b7 S)), where \u00b7 denotes the element-wise product."}, {"heading": "3.2 SVD as a Low-Rank Approximation", "text": "The Singular Value Decomposition (SVD) is a common matrix decomposition technique that factorizes a matrix A \u2208 Rm\u00d7n into A = U\u03a3V T , U \u2208 Rm\u00d7m,\u03a3 \u2208 Rm\u00d7n, V \u2208 Rn\u00d7n. By [6], the matrix A can be approximated using a low rank matrix A\u0302r corresponding to the solution of the constrained optimization of\nmin A\u0302r\n\u2016A\u2212 A\u0302r\u2016F (5)\nwhere \u2016 \u00b7 \u2016F is the Frobenius norm, and A\u0302r is constrained to be of rank r < rank(A). The minimizer A\u0302r is given by taking the first r columns of U , the first r diagonal entries of \u03a3, and the first r columns of V . The resulting matrices Ur, \u03a3r, and Vr are multiplied, yielding A\u0302r = Ur\u03a3rV Tr . The low-rank approximation W\u0302 = UV is then defined such that W\u0302 = Ur(\u03a3rV Tr ), where U = Ur and V = \u03a3rV Tr .\nUnfortunately, calculating the SVD is an expensive operation, on the order of O(mn2), so recalculating the SVD upon the completion of every minibatch adds significant overhead to the training procedure. Given that we are uniquely interested in estimating in the sign of al+1 = alWl, we can opt to calculate the SVD less frequently than once per minibatch, assuming that the weights Wl do not change significantly over the course of a single epoch so as to corrupt the sign estimation."}, {"heading": "3.3 Encouraging Neural Network Sparsity", "text": "To overcome the additional overhead imposed by the conditional computation architecture, the neural network must have sparse activations. Without encouragement to settle on weights that result in sparse activations, such as penalties on the loss function, a neural network will not necessarily become sparse enough to be useful in the context of conditional computation. Therefore, an \u21131 penalty for the activation vector of each layer is applied to the overall loss function, such that\nJ(W,\u03bb) = L(W ) + \u03bb\nL \u2211\nl=1\n\u2016al\u20161 (6)\nSuch a penalty is commonly used in sparse dictionary learning algorithms and tends to push elements of al towards zero [13].\nDropout regularization [9] is another technique known to sparsify the hidden activations in a neural network. Dropout first sets the hidden activations al to zero with probability p. During training, the sparsity of the network is likely less than p for each minibatch. When the regularized network is running in the inference mode, dropout has been observed to have a sparsifying effect on the hidden activations [17]."}, {"heading": "3.4 Implementation Details", "text": "The neural network is built using Rasmus Berg Palm\u2019s Deep Learning Toolbox [16]. All hidden units are rectified-linear, and the output units are softmax trained with a negative log-likelihood loss function. The weights, w, are initialized as w \u223c N ( 0, \u03c32 )\nand biases b are set to 1 in order to encourage the neurons to operate in their non-saturated region once training begins. In all experiments, the dropout probability p is fixed to 0.5.\nThe learning rate \u03b3 is scheduled such that \u03b3n = \u03b30\u03bbn where \u03b3n is the learning rate for the nth epoch, \u03b30 is the initial learning rate, and \u03bb is a decay term slightly less than 1, eg., 0.995. The momentum term \u03bd is scheduled such that \u03bdn = max (\u03bdmax, \u03bd0\u03b2n) where \u03bdn is the momentum for the nth epoch, \u03bdmax is the maximum allowed momentum, \u03bd0 is the initial momentum, and \u03b2 is an incremental term slightly greater than 1, eg., 1.05.\nTo simplify prototyping, the feed-forward is calculated for a layer, and the activation estimator is immediately applied before the next layer activations are used. This is equivalent to bypassing the calculations for activations that are likely to produce zeros. In practice, re-calculating the SVD once per epoch for the activation estimator seems to be a decent tradeoff between activation estimation accuracy and computational efficiency, but this may not necessarily be true for other datasets."}, {"heading": "4 Experimental Results", "text": ""}, {"heading": "4.1 SVHN", "text": "Street View House Numbers (SVHN) [15] is a large image dataset containing over 600,000 labeled examples of digits taken from street signs. Each example is an RGB 32 \u00d7 32 (3072-dimensional) image. To pre-process the dataset, each image is transformed into the YUV colorspace. Next, local contrast normalization [10] followed by a histogram equalization is applied to the Y channel. The U and V channels are discarded, resulting in a 1024-dimensional vector per example. The dataset is then normalized for the neural network by subtracting out the mean and dividing by the square root of the variance for each variable.\nTo evaluate the sensitivity of the activation estimator, several parameterizations for the activation estimator are evaluated. Each network is trained with the hyperparameters in Table 4.1, and the results of seven parameterizations are shown in Figure 4.1. Each parameterization is described by the rank of each approximation, eg., \u201875-50-40-30\u2019 describes a network with an activation estimator using a 75-rank approximation for W1, a 50-rank approximation for W2, a 40-rank approximation for W3, and a 30-rank approximation for W4. Note that a low-rank approximation is not necessary for W5 (the weights connecting the last hidden layer to the output layer), as we do not want to approximate the activations for the output layer.\nSome runs, specifically 25-25-25-25 and 50-35-25-25 in Figure 4.1 exhibit an initial decrease in classification error, followed by a gradual increase in classification error as training progresses. In the initial epochs, the hidden layer activations are mostly positive because the weights are relatively small and the biases are very large. As a consequence, the activation estimation is a much simpler task for the initial epochs. However, as the pattern of the activation signs diversifies as the network continues to train, the lower-rank approximations begin to fail."}, {"heading": "100-75-50-25 9.96%", "text": "Table 4.2 summarizes the test set error for the control and activation estimation networks. W1 appears to be most sensitive, quickly reducing the test set error from 10.72% to 12.16% when the rank of W\u03021 is lowered from 75 to 50. The rank of W\u03024 appears to be the least sensitive, reducing the test set error from 9.96% to 10.01% as the rank is lowered from 25 to 15."}, {"heading": "4.2 MNIST", "text": "MNIST is a well-known dataset of hand-written digits containing 70,000 28 \u00d7 28 labeled images, and is generally split into 60,000 training and 10,000 testing examples. Very little pre-processing is required to achieve good results - each feature is transformed by xt = x\u221a\n\u03c32 max \u2212 0.5, where x is the input feature, \u03c32max is the maximum variance of all features, and 0.5 is a constant term to roughly center each feature. Several parameterizations for the activation estimator are evaluated for a neural network trained with the hyperparameters listed in Table 4.1 using the same approach as the SVHN experiment above. The results for the validation set plotted against the epoch number are shown in Figure 4.3, and the final test set accuracy is reported in Table 4.3.\nA neural network with a very low-rank weight matrix in the activation estimation can train surprisingly well on MNIST. Lowering the rank from 784-600-400 to 50-35-25 impacts performance negligibly. Ranks as low as 25-25-25 does not lessen performance too greatly, and ranks as low as 10-10-5 yield a classifier capable of 2.28% error."}, {"heading": "25-25-25 1.60%", "text": ""}, {"heading": "50-35-25 1.43%", "text": ""}, {"heading": "5 Discussion and Further Work", "text": "Low-rank estimations of weight matrices of a neural network obtained via once-per-epoch SVD work very well as efficient estimators of the sign of the activation for the next hidden layer. In the context of rectified-linear hidden units, computation time can be reduced greatly if this estimation is reliable and the hidden activations are sufficiently sparse. This approach is applicable to any hardthresholding activation function, such as the functions investigated in [8], and can be easily extended to be used with convolutional neural networks.\nWhile the activation estimation error does not tend to deviate too greatly inbetween minibatches over an epoch, as illustrated in Figure 5.1, this is not guaranteed. An online approach to the lowrank approximation would therefore be preferable to a once-per-epoch calculation. In addition, while the low-rank approximation given by SVD minimizes the objective function \u2016A \u2212 A\u0302r\u2016F , this is not necessarily the best objective function for an activation estimator, where we seek to minimize \u2016\u03c3 (aW )\u2212 \u03c3 (a (W \u00b7 S))\u2016, which is a much more difficult and non-convex objective function. Also, setting the hyperparameters for the activation estimator can be a tedious process involving expensive cross-validation when an adaptive algorithm could instead choose the rank based on the spectrum of the singular values. Therefore, developing a more suitable low-rank approximation algorithm could provide a promising future direction of research.\nIn [1], the authors propose a method called \u201cadaptive dropout\u201d by which the dropout probabilities are chosen by a function optimized by gradient descent instead of fixed to some value. This approach bears some resemblance to this paper, but with the key difference that the approach in [1] is motivated by improved regularization and this paper\u2019s method is motivated by computational efficiency. However, the authors introduce a biasing term that allows for greater sparsity that could be intro-\nduced into this paper\u2019s methodology. By modifying the conditional computation unit to compute sgn (aUV \u2212 b), where b is some bias, we can introduce a parameter that can tune the sparsity of the network, allowing for a more powerful trade-off between accuracy and computational efficiency."}], "references": [{"title": "Adaptive dropout for training deep neural networks", "author": ["Jimmy Ba", "Brendan Frey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Deep learning of representations: Looking forward", "author": ["Yoshua Bengio"], "venue": "Statistical Language and Speech Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["Yoshua Bengio", "Nicholas L\u00e9onard", "Aaron C. Courville"], "venue": "CoRR, abs/1308.3432,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Deep learning with cots hpc systems", "author": ["Adam Coates", "Brody Huval", "Tao Wang", "David Wu", "Bryan Catanzaro", "Ng Andrew"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "venue": "arXiv preprint arXiv:1306.0543,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "The approximation of one matrix by another of lower rank", "author": ["Carl Eckart", "Gale Young"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1936}, {"title": "Deep sparse rectifier networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Saturating auto-encoder", "author": ["Rostislav Goroshin", "Yann LeCun"], "venue": "arXiv preprint arXiv:1301.3577,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "What is the best multi-stage architecture for object recognition", "author": ["Kevin Jarrett", "Koray Kavukcuoglu", "Marc\u2019Aurelio Ranzato", "Yann LeCun"], "venue": "In Computer Vision,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc Le", "Marc\u2019Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg Corrado", "Jeff Dean", "Andrew Ng"], "venue": "In International Conference in Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Efficient sparse coding algorithms", "author": ["Honglak Lee", "Alexis Battle", "Rajat Raina", "Andrew Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Deep belief networks using discriminative features for phone recognition", "author": ["A Mohamed", "Tara N Sainath", "George Dahl", "Bhuvana Ramabhadran", "Geoffrey E Hinton", "Michael A Picheny"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Prediction as a candidate for learning deep hierarchical models of data", "author": ["R.B. Palm"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Improving neural networks with dropout", "author": ["Nitish Srivastava"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "This paper expands on the idea of conditional computation introduced in [2], where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated.", "startOffset": 72, "endOffset": 75}, {"referenceID": 10, "context": "In recent years, deep neural networks have redefined state-of-the-art in many application domains, notably in computer vision [11] and speech processing [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 13, "context": "In recent years, deep neural networks have redefined state-of-the-art in many application domains, notably in computer vision [11] and speech processing [14].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "While the extremely high computational power used for the experiment performed in [12] (16,000 cores training for many days) was greatly reduced in [4] (3 servers training for many days), specialized high-performance platforms still require several machines and several days of processing time.", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "While the extremely high computational power used for the experiment performed in [12] (16,000 cores training for many days) was greatly reduced in [4] (3 servers training for many days), specialized high-performance platforms still require several machines and several days of processing time.", "startOffset": 148, "endOffset": 151}, {"referenceID": 4, "context": "In [5], the authors made the observation that deep models tend to have a high degree of redundancy in their weight parameterization.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In [2] and [3], the authors propose the idea of conditional computation in neural networks, where the network is augmented by a gating model that turns activations on or off depending on the state of the network.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "In [2] and [3], the authors propose the idea of conditional computation in neural networks, where the network is augmented by a gating model that turns activations on or off depending on the state of the network.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "For many datasets considered in the literature, sparse representations have been shown to be superior to dense representations, particularly in the context of deep architectures [7].", "startOffset": 178, "endOffset": 181}, {"referenceID": 4, "context": "If the weight matrix is highly redundant, as in [5], it can be well-approximated using a low-rank representation and we may rewrite (1) as", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "By [6], the matrix A can be approximated using a low rank matrix \u00c2r corresponding to the solution of the constrained optimization of min \u00c2r \u2016A\u2212 \u00c2r\u2016F (5)", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "Such a penalty is commonly used in sparse dictionary learning algorithms and tends to push elements of al towards zero [13].", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "Dropout regularization [9] is another technique known to sparsify the hidden activations in a neural network.", "startOffset": 23, "endOffset": 26}, {"referenceID": 15, "context": "When the regularized network is running in the inference mode, dropout has been observed to have a sparsifying effect on the hidden activations [17].", "startOffset": 144, "endOffset": 148}, {"referenceID": 14, "context": "The neural network is built using Rasmus Berg Palm\u2019s Deep Learning Toolbox [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "Next, local contrast normalization [10] followed by a histogram equalization is applied to the Y channel.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "This approach is applicable to any hardthresholding activation function, such as the functions investigated in [8], and can be easily extended to be used with convolutional neural networks.", "startOffset": 111, "endOffset": 114}, {"referenceID": 0, "context": "In [1], the authors propose a method called \u201cadaptive dropout\u201d by which the dropout probabilities are chosen by a function optimized by gradient descent instead of fixed to some value.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "This approach bears some resemblance to this paper, but with the key difference that the approach in [1] is motivated by improved regularization and this paper\u2019s method is motivated by computational efficiency.", "startOffset": 101, "endOffset": 104}], "year": 2017, "abstractText": "Scalability properties of deep neural networks raise key research questions, particularly as the problems considered become larger and more challenging. This paper expands on the idea of conditional computation introduced in [2], where the nodes of a deep network are augmented by a set of gating units that determine when a node should be calculated. By factorizing the weight matrix into a low-rank approximation, an estimation of the sign of the pre-nonlinearity activation can be efficiently obtained. For networks using rectified-linear hidden units, this implies that the computation of a hidden unit with an estimated negative pre-nonlinearity can be ommitted altogether, as its value will become zero when nonlinearity is applied. For sparse neural networks, this can result in considerable speed gains. Experimental results using the MNIST and SVHN data sets with a fully-connected deep neural network demonstrate the performance robustness of the proposed scheme with respect to the error introduced by the conditional computation process.", "creator": "LaTeX with hyperref package"}}}