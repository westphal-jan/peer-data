{"id": "1312.5559", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "abstract": "there are two main approaches to the distributed representation of words : low - dimensional deep learning embeddings and high - dimensional distributional interaction models, in symbols which approximately each dimension domain corresponds to a context weighted word. illustrates in this paper, we combine these two approaches by learning distribution embeddings based on distributional - model vectors - as opposed to one - hot vectors as is standardly done in deep learning. we show that the inverse combined approach has better performance on a word relatedness judgment efficiency task.", "histories": [["v1", "Thu, 19 Dec 2013 14:18:14 GMT  (15kb)", "https://arxiv.org/abs/1312.5559v1", "4 pages, 1 table, ICLR Workshop"], ["v2", "Tue, 14 Jan 2014 17:33:49 GMT  (15kb)", "http://arxiv.org/abs/1312.5559v2", "4 pages, 1 table, ICLR Workshop; main experimental table was extended with more experimental results"], ["v3", "Tue, 18 Feb 2014 14:17:46 GMT  (15kb)", "http://arxiv.org/abs/1312.5559v3", "4 pages, 1 table, ICLR Workshop; main experimental table was extended with more experimental results; related word added"]], "COMMENTS": "4 pages, 1 table, ICLR Workshop", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["irina sergienya", "hinrich sch\\\"utze"], "accepted": false, "id": "1312.5559"}, "pdf": {"name": "1312.5559.pdf", "metadata": {"source": "CRF", "title": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds", "authors": ["Irina Sergienya"], "emails": ["irina@cis.lmu.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n55 59\nv3 [\ncs .C\nL ]\n1 8\nThere are two main approaches to the distributed representation of words: lowdimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributionalmodel vectors \u2013 as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task."}, {"heading": "1 Introduction", "text": "The standard approach to inducing deep learning embeddings is to represent each word of the input vocabulary as a one-hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013a)). There is no usable information available in this initial representation; in this sense, the standard way of inducing embedding is a form of learning from scratch.\nIn this paper, we explore the question of whether it can be advantageous to use a more informative initial representation for inducing embeddings. Specifically, we will test distributional-model representations for this purpose \u2013 where we define a distributional-model or distributional representation of a target word w as a vector of dimensionality |V |; the value on the dimension corresponding to v \u2208 V records a measure of the significance of the dimension word v for w. In the simplest case, this measure of significance is a weighted cooccurrence count of w and v (e.g., Schu\u0308tze (1992), Lund and Burgess (1996), Baroni and Lenci (2010)).\nDistributional representations have been successfully used for a wide variety of tasks in natural language processing, like synonym detection (Landauer and Dumais, 1997), concept categorization (Almuhareb and Poesio, 2004), metaphorical sense identification (Turney et al., 2011) and sentiment analysis (Turney and Littman, 2003). For this reason, it is natural to ask whether they can also improve the quality of embeddings. We will realize this idea by presenting distributional vectors to the neural network that learns embeddings, instead of presenting one-hot vectors. We will refer to these two modes of learning embeddings as distributional initialization and one-hot initialization.\nOur expectation is that distributional initialization will be particularly helpful for rare words that occur in only a few contexts. It is difficult for one-hot initialization to learn good embeddings for rare words. In contrast, distributional initialization should make the learning task easier. For example, the distributional signatures of two rare legal words will be similar since they occur in similar contexts. Thus, distributional initialization will make it easier to learn similar embeddings for them. In one-hot initialization, the embeddings for the two words are initialized randomly and it will be difficult for them to converge to close points in the embedding space during learning if each only occurs in a few contexts."}, {"heading": "2 Method", "text": "As we just discussed, we would expect distributional initialization to be beneficial mostly for rare words. Conversely, it is likely that distributional initialization will actually hurt the quality of embeddings learned for frequent words. The reason is that distributional initialization puts constraints on the relationship between different embeddings. This is a good thing for rare words as it enforces similarity of the embeddings of similar rare words. But it can be harmful for frequent words that often have idiosyncratic properties. For frequent words, it is better to use one-hot initialization, which in principle does not impose any constraints on the type of embedding that can be learned.\nBased on this motivation, we propose a hybrid initalization scheme: all words with a frequency f > \u03b8 are initialized with one-hot vectors, all words with a frequency f \u2264 \u03b8 are initialized with distributional vectors, where the frequency threshold \u03b8 is a parameter.\nWe test two versions of this hybrid initalization: separate and mixed. Let n be the dimensionality of the distributional vectors, i.e., the number of words that we use as dimension words, and k the number of words with frequency f > \u03b8. In the separate scheme, the input representation for a word is the concatentation of a k-dimensional vector and an n-dimensional vector. For a frequent word, the k-dimensional vector is a one-hot vector and the n-dimensional vector is zero. For a rare word, the k-dimensional vector is zero and the n-dimensional vector is its distributional vector. In the mixed scheme, the input representation for a word is an n-dimensional vector. It is a one-hot vector for a frequent word and a distributional vector for a rare word.\nIn addition to the two separate and mixed hybrid schemes, we also test non-hybrid distributional intialization. In that case, the input representation for a word is an n-dimensional distributional vector for frequent words as well as for rare words."}, {"heading": "3 Experimental setup", "text": "As training set for the word embeddings, we use parts 02 to 21 of the Wall Street Journal (Marcus et al., 1993), a corpus of about one million tokens and roughly 35,000 word types.\nWe used two word relatedness data sets for evaluation: MEN1 (Bruni et al., 2012) and WordSim3532 (Finkelstein et al., 2001). The two data sets contain pairs of words with human-assigned similarity scores. We only evaluate on the 2186 MEN pairs (of a total of 3000) and 303 WordSim353 pairs (of a total of 353) that are covered by our data set, i.e., both words occurred in WSJ.\nWe added to the continuous skip gram model (Mikolov et al., 2013a) of word2vec3 both one-hot and distributional initialization. We use hierarchical softmax, set the size of the context window to 11, min-count to 0 (do not discard words because of their low frequency), sample to 1e-3 (discard the words in the training set with probability P (w) = 1 \u2212 \u221a\nt/f(w), where f(w) is the frequency of word w and t is a chosen threshold (Mikolov et al., 2013b)) and embedding size to 100.\nWe use a simple binary distributional model: Entry 1 \u2264 i \u2264 n in the distributional vector of w is set to 1 iff vi and w cooccur at a distance of at most ten words in the corpus and to 0 otherwise. We test the methods for frequency thresholds \u03b8 \u2208 {1, 2, 5, 10, 20, 50, 100, 1000}.\nFor each initializaton condition, we train 10 models. We measure Spearman correlation of the gold standard \u2013 human-assigned similarity scores \u2013 with cosine similarity scores between word embeddings generated by our models, and report correlation averages for each initialization setup."}, {"heading": "4 Results and discussion", "text": "Table 1 gives averaged Spearman correlation coefficients between human and embedding-based similarity judgments on MEN and WordSim. Embeddings are produced by skip gram models with one-hot, hybrid (mixed or separate) and distributional initialization. The threshold is varied for the two hybrid models (column \u201c\u03b8\u201d). Correlation coefficients are given in the last two columns.\n1http://clic.cimec.unitn.it/ elia.bruni/MEN 2http://www.cs.technion.ac.il/ gabr/resources/data/wordsim353/wordsim353.html 3https://code.google.com/p/word2vec/\nThe main result is that the two hybrid initializations outperform one-hot initialization significantly4 on both MEN and WordSim data sets for low values of \u03b8. This result is evidence that a hybrid initialization scheme can be superior to one-hot initialization for words with very few occurrences.\nHybrid initialization only does well for low values of \u03b8. In general, as \u03b8 increases, performance goes down. This trend reaches its endpoint for distributional initialization, which can be interpreted as hybrid initialization with \u03b8 = \u221e. The correlations for distributional initialization are at the low end of the range of performance numbers and are lower than one-hot initialization.\nThe fact that distributional initialization performs worse than hybrid initialization confirms our initial hypothesis that frequent and rare words should be treated differently. Distributional initialization for all words \u2013 including frequent words \u2013 imposes harmful constraints on the embeddings of frequent words; it is probably also harmful because it links the embeddings of rare words to those of frequent words, which makes it harder for the skip gram model to learn embeddings for rare words."}, {"heading": "5 Related work", "text": "The problem of word embedding initialization was also addressed by Le et al. (2010). They propose three initialization schemes. Two of them, re-initialization and iterative re-initialization, use vectors from prediction space to initialize the context space during training. This approach is both more complex and less efficient than ours. The third initialization scheme, one vector initialization, initializes all word embeddings with the same random vector: this helps to keep rare words close to each other because vectors of rare words are rarely updated. However, this approach is also less efficient than ours since the initial embedding is much denser than in our approach."}, {"heading": "6 Conclusion", "text": "We have proposed to use a hybrid initialization for learning embeddings, an initialization that combines the standardly used one-hot initialization with a distributional initialization for rare words.\n4Student\u2019s t-test, two-tailed, p < .05\nExperimental results on a word relatedness task provide tentative evidence that hybrid initialization produces better embeddings than one-hot initialization.\nOur results are not directly comparable with prior research on modeling word relateness judgments, partly because the corpus we use has low coverage of the words in the evaluation sets. We also use a simple binary distributional vector representation, which is likely to have a negative effect on the performance of the embeddings.\nIn future work, we will test our models on larger corpora and look at a wider range of distributional models to produce results that are directly comparable with other work on word relatedness.\nAcknowledgments. We would like to thank Sebastian Ebert for his help with the code."}], "references": [{"title": "Attribute-based and value-based clustering: An evaluation", "author": ["A. Almuhareb", "M. Poesio"], "venue": null, "citeRegEx": "Almuhareb and Poesio,? \\Q2004\\E", "shortCiteRegEx": "Almuhareb and Poesio", "year": 2004}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["M. Baroni", "A. Lenci"], "venue": "Computational Linguistics", "citeRegEx": "Baroni and Lenci,? \\Q2010\\E", "shortCiteRegEx": "Baroni and Lenci", "year": 2010}, {"title": "Distributional semantics in technicolor", "author": ["E. Bruni", "G. Boleda", "M. Baroni", "N.K. Tran"], "venue": null, "citeRegEx": "Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": null, "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review", "citeRegEx": "Landauer and Dumais,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "Training continuous space language models: Some practical issues", "author": ["H.S. Le", "A. Allauzen", "G. Wisniewski", "F. Yvon"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Le et al\\.", "year": 2010}, {"title": "Producing high-dimensional semantic spaces from lexical cooccurrence", "author": ["K. Lund", "C. Burgess"], "venue": "Behavior Research Methods, Instruments, & Computers", "citeRegEx": "Lund and Burgess,? \\Q1996\\E", "shortCiteRegEx": "Lund and Burgess", "year": 1996}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational Linguistics", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Efficient estimation of word representations in vector space. In: Workshop at ICLR", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Dimensions of meaning", "author": ["H. Sch\u00fctze"], "venue": "ACM/IEEE Conference on Supercomputing", "citeRegEx": "Sch\u00fctze,? \\Q1992\\E", "shortCiteRegEx": "Sch\u00fctze", "year": 1992}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["J. Turian", "Ratinov", "L.-A", "Y. Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Measuring praise and criticism: Inference of semantic orientation from association", "author": ["P.D. Turney", "M.L. Littman"], "venue": "ACM TOIS", "citeRegEx": "Turney and Littman,? \\Q2003\\E", "shortCiteRegEx": "Turney and Littman", "year": 2003}, {"title": "Literal and metaphorical sense identification through concrete and abstract context", "author": ["P.D. Turney", "Y. Neuman", "D. Assaf", "Y. Cohen"], "venue": null, "citeRegEx": "Turney et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": ", Turian et al. (2010), Collobert et al.", "startOffset": 2, "endOffset": 23}, {"referenceID": 3, "context": "(2010), Collobert et al. (2011), Mikolov et al.", "startOffset": 8, "endOffset": 32}, {"referenceID": 3, "context": "(2010), Collobert et al. (2011), Mikolov et al. (2013a)).", "startOffset": 8, "endOffset": 56}, {"referenceID": 9, "context": ", Sch\u00fctze (1992), Lund and Burgess (1996), Baroni and Lenci (2010)).", "startOffset": 2, "endOffset": 17}, {"referenceID": 6, "context": ", Sch\u00fctze (1992), Lund and Burgess (1996), Baroni and Lenci (2010)).", "startOffset": 18, "endOffset": 42}, {"referenceID": 1, "context": ", Sch\u00fctze (1992), Lund and Burgess (1996), Baroni and Lenci (2010)).", "startOffset": 43, "endOffset": 67}, {"referenceID": 5, "context": "Distributional representations have been successfully used for a wide variety of tasks in natural language processing, like synonym detection (Landauer and Dumais, 1997), concept categorization (Almuhareb and Poesio, 2004), metaphorical sense identification (Turney et al.", "startOffset": 142, "endOffset": 169}, {"referenceID": 0, "context": "Distributional representations have been successfully used for a wide variety of tasks in natural language processing, like synonym detection (Landauer and Dumais, 1997), concept categorization (Almuhareb and Poesio, 2004), metaphorical sense identification (Turney et al.", "startOffset": 194, "endOffset": 222}, {"referenceID": 14, "context": "Distributional representations have been successfully used for a wide variety of tasks in natural language processing, like synonym detection (Landauer and Dumais, 1997), concept categorization (Almuhareb and Poesio, 2004), metaphorical sense identification (Turney et al., 2011) and sentiment analysis (Turney and Littman, 2003).", "startOffset": 258, "endOffset": 279}, {"referenceID": 13, "context": ", 2011) and sentiment analysis (Turney and Littman, 2003).", "startOffset": 31, "endOffset": 57}, {"referenceID": 8, "context": "As training set for the word embeddings, we use parts 02 to 21 of the Wall Street Journal (Marcus et al., 1993), a corpus of about one million tokens and roughly 35,000 word types.", "startOffset": 90, "endOffset": 111}, {"referenceID": 2, "context": "We used two word relatedness data sets for evaluation: MEN1 (Bruni et al., 2012) and WordSim3532 (Finkelstein et al.", "startOffset": 60, "endOffset": 80}, {"referenceID": 4, "context": ", 2012) and WordSim3532 (Finkelstein et al., 2001).", "startOffset": 24, "endOffset": 50}, {"referenceID": 6, "context": "The problem of word embedding initialization was also addressed by Le et al. (2010). They propose three initialization schemes.", "startOffset": 67, "endOffset": 84}], "year": 2014, "abstractText": "There are two main approaches to the distributed representation of words: lowdimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributionalmodel vectors \u2013 as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "creator": "LaTeX with hyperref package"}}}