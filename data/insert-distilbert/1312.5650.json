{"id": "1312.5650", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2013", "title": "Zero-Shot Learning by Convex Combination of Semantic Embeddings", "abstract": "several recent publications have proposed methods for potentially mapping images into continuous semantic embedding spaces. in some cases the semantic embedding space is trained jointly with solving the image transformation, while in other cases the semantic embedding space is established independently by a separate task, such as a natural learning language processing task on a text corpus, and then the image transformation into that space is learned in a second stage. proponents of these image embedding systems have stressed broadly their advantages over the traditional n - way classification framing of image understanding, particularly in convincing terms of the promise of zero - shot learning - - the ability to correctly annotate images of previously unseen object categories. returning here we propose a simple method for constructing an image embedding system from any existing n - way image classification mechanism and any existing semantic embedding space through which contains the n class labels in its vocabulary. our rendering method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional learning. we show that this simple and direct method confers many of the advantages associated with more efficient complex image embedding schemes, and indeed outperforms cognitive state of the art methods on the imagenet zero - shot learning task.", "histories": [["v1", "Thu, 19 Dec 2013 17:30:31 GMT  (18kb)", "http://arxiv.org/abs/1312.5650v1", null], ["v2", "Fri, 20 Dec 2013 23:30:47 GMT  (92kb,D)", "http://arxiv.org/abs/1312.5650v2", null], ["v3", "Fri, 21 Mar 2014 23:47:20 GMT  (93kb,D)", "http://arxiv.org/abs/1312.5650v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohammad norouzi", "tomas mikolov", "samy bengio", "yoram singer", "jonathon shlens", "andrea frome", "greg s corrado", "jeffrey dean"], "accepted": true, "id": "1312.5650"}, "pdf": {"name": "1312.5650.pdf", "metadata": {"source": "CRF", "title": "Zero-Shot Learning by Convex Combination of Semantic Embeddings", "authors": ["Mohammad Norouzi", "Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S. Corrado", "Jeffrey Dean"], "emails": ["norouzi@cs.toronto.edu,", "singer}@google.com", "jeff}@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n31 2.\n56 50\nv1 [\ncs .L\nG ]"}, {"heading": "1 Introduction", "text": "The classic machine learning approach to object recognition presupposes the existence of a large labeled training dataset to optimize the free parameters of an image classifier. There have been continued efforts in collecting larger image corpora with a broader coverage of object categories [3], thereby enabling image classification with many class labels. While annotating more object categories in images can lead to a finer granularity of image classification, arguably, creating high quality fine grained image annotations is prohibitively challenging, expensive, and time consuming. Moreover, as new visual entities emerge over time, the image annotations should be modified, and the classifiers should be re-trained.\nMotivated by the limitations of the standard machine learning framework for n-way classification, several recent papers have proposed methods for mapping images into continuous semantic embedding spaces [12, 4, 9, 6, 15]. In doing so, it is hoped that by resorting to nearest neighbor search in the continuous embedding space with respect to a set of word embedding vectors, one can address zero-shot learning \u2013 annotation of images with previously unseen object categories. Naturally, it is expected that a continuous embedding of images provides a more powerful representation than an\n\u2217Part of this work was done while Mohammad Norouzi was at Google.\nn-way discrete labeling. However, it has been unclear whether there exists a direct way to transform any existing probabilistic n-way image classifier into a continuous image embedding model. In this work, we propose a simple and direct method for constructing an image embedding system by combining any existing n-way image classifier and any existing word embedding model which contains the n class labels in its vocabulary. We show that our simple method confers many of the advantages associated with more complex image embedding schemes.\nOur model fits into the zero-shot learning framework [12], which recently received a growing amount of attention [13, 6, 15]. One key element of zero-shot learning is the use of a set of semantic embedding vectors associated with the class labels. These semantic embedding vectors might be obtained from supervised object attributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion [6, 15, 11], based on a separate natural language modeling task. Regardless of the way the label embedding vectors are obtained, previous work casts zero-shot learning as a regression problem from the input space into the semantic label embedding space. In contrast, given a pre-trained standard classifier, our method maps images into the semantic embedding space, deterministically, via convex combination of the class label embedding vectors. The classifier\u2019s prediction scores for different training labels are used to compute a weighted combination of the label embeddings in the semantic space. The convex combination of the label embedding vectors provides a continuous embedding of images into the semantic space, which is used for extrapolating a pre-trained classifier\u2019s predictions beyond the training labels, into a set of test labels.\nThe effectiveness of our method called \u201cconvex combination of semantic embeddings\u201d (ConSE) is evaluated on ImageNet zero-shot learning task. By employing a state-of-the-art convolutional neural network [7] trained only on 1000 object categories from ImageNet, the ConSE model is able to achieve 9.4% hit@1 and 24.7% hit@5 on 1600 unseen objects categories, which are omitted from the training dataset. When the test object classes are farther from the training classes in the ImageNet category hierarchy, the zero-shot classification results get worse, as expected, but still the ConSE model significantly outperforms the state-of-the-art model [6] applied to the same task."}, {"heading": "2 Formulation", "text": "Suppose a labeled training dataset of images D0 \u2261 {(xi, yi)}mi=1 is given, where each image is represented by a p-dimensional feature vector denoted xi \u2208 X \u2261 Rp, and there are n0 distinct class labels available for training, i.e., yi \u2208 Y0 \u2261 {1, . . . , n0}. In addition, a test dataset denoted D1 \u2261 {(x \u2032 j , y \u2032 j)} m\u2032 j=1 is provided, where x \u2032 j \u2208 X as above, but y \u2032 j \u2208 Y1 \u2261 {n0+1, . . . , n0+n1}. The test set contains n1 distinct class labels, which are omitted from the training set. Let n = n0+n1 denote the total number of labels in the training and test sets.\nThe goal of zero-shot learning is to train a classifier on the training set D0, which performs reasonably well on the unseen test set D1. Clearly, without any side information about the relationships between the labels in Y0 and Y1, zero-shot learning is intractable as Y0 \u222a Y1 = \u2205. However, to mitigate zero-shot learning, one typically assumes that each class label y (1 \u2264 y \u2264 m) is associated with a semantic embedding vector s(y) \u2208 S \u2261 Rq . The semantic embedding vectors are such that two labels y and y\u2032 are similar if and only if their semantic embeddings s(y) and s(y\u2032) are close in S, e.g., \u3008s(y), s(y\u2032)\u3009S is large. Clearly, given a joint embedding of training and test class labels in a semantic space i.e., {s(y); y \u2208 Y0 \u222a Y1}, the training and test labels become related, and one can hope to learn from the training labels to predict the test labels.\nPrevious work (e.g., [6, 15]) has addressed zero-shot classification by learning a mapping from input features to semantic label embedding vectors using a multivariate regression model. Accordingly, during training instead of learning an n0-way classifier from inputs to training labels (X \u2192 Y0), a regressor is learned from inputs to semantic embedding space (X \u2192 S). A training dataset of inputs paired with semantic embeddings, {(xi, s(yi)); (xi, yi) \u2208 D0}, is constructed to train a regression function f : X \u2192 S that maps xi to s(yi). Once f(\u00b7) is learned, it is applied to a test image x \u2032 j to obtain f(x \u2032 j), and this continuous semantic embedding for x \u2032 j is then compared with the test embedding vectors, {s(y\u2032); y\u2032 \u2208 Y1}, to find the most relevant test labels. In essence, instead of directly mapping from X \u2192 Y1, which seems impossible, we first learn a mapping X \u2192 S, and then a deterministic mapping such as k-nearest neighbor search in S is used to map S \u2192 Y1."}, {"heading": "3 Previous work", "text": "A key component of zero-shot learning is the way semantic embedding of the class labels is defined. In computer vision, there has been a body of work on the use of human-labeled attributes [4, 9] to help detecting unseen objects. Binary attributes are most commonly used to encode presence and absence of a set of visual characteristics within instances of an object category. Examples of the attributes include different types of materials, colors, textures, and object parts. The main issue with classification based on supervised attribute is its poor scalability to large-scale tasks. Clearly, annotating hundreds of attributes for tens of thousands of object classes is an ambiguous and challenging task itself, which limits the applicability of supervised attributes to large-scale zero-shot learning. There has been more recent work showing impressive zero-shot performance on visual recognition tasks [14, 10], but these methods also rely on the use of knowledge bases containing descriptive properties of object classes, and the WordNet hierarchy.\nA more scalable approach to semantic embeddings of class labels builds upon the recent advances in unsupervised neural language modeling [2]. In this approach, a set of multi-dimensional embedding vectors are learned for each word in a text corpus. The word embeddings are optimized to increase the predictability of each word given its context [11]. Essentially, the words that co-occur in similar contexts, are mapped onto similar embedding vectors. Frome et al. [6] and Socher et al. [15] exploit word embeddings to embed textual names of object class labels into a rich semantic space. In this work, we also use the skip-gram model [11] to learn class label embeddings.\nZero-shot learning is closely related to one-shot learning [5, 1, 8], where the goal is to learn classifiers for object categories based on a few labeled training exemplars. The key difference in zero-shot learning is that no training images are provided for a held out set of test categories. Thus, zero-shot learning is more challenging, and the use of side information is essential in this setting. Nevertheless, we expect that advances in zero-shot learning will benefit one-shot learning, and image classification in general, by providing better ways to incorporate prior knowledge about the relationships between the object categories."}, {"heading": "4 ConSE: Convex combination of semantic embeddings", "text": ""}, {"heading": "4.1 Model Description", "text": "In contrast to previous work which casts zero-shot learning as a regression problem from the input space to the semantic label embedding space, in this work, we do not explicitly learn a regression function f : X \u2192 S. Instead, we follow the classic machine learning approach, and learn a classifier from training inputs to training labels. To this end, a classifier p0 is trained on D0 to estimate the probability of an image x belonging to a class label y \u2208 Y0, denoted p0(y | x), where\u2211n0\ny=1 p0(y | x) = 1. Given p0, we propose a method to transfer the probabilistic predictions of the classifier beyond the training labels, to a set of test labels.\nLet y\u03020(x, 1) denote the most likely training label for an image x according to the classifier p0. Formally, we denote\ny\u03020(x, 1) \u2261 argmax y\u2208Y0 p0(y | x) . (1)\nAnalogously, let y\u03020(x, t) denote the tth most likely training label for x according to p0. In other words, p0(y\u03020(x, t) | x) is the tth largest value among {p0(y | x); y \u2208 Y0}. Given the top T predictions of p0 for an input x, our model deterministically predicts a semantic embedding vector f(x) for an input x, as the convex combination of the semantic embeddings {s(y\u03020(x, t))}Tt=1 weighted by their corresponding probabilities. More formally,\nf(x) = 1\nZ\nT\u2211\nt=1\np(y\u03020(x, t) | x) \u00b7 s(y\u03020(x, t)) , (2)\nwhere Z is a normalization factor given by Z = \u2211T\nt=1 p(y\u03020(x, t) | x), and T is a hyper-parameter controlling the maximum number of embedding vectors to be considered. If the classifier is very confident in its prediction of a label y for x, i.e., p0(y | x) \u2248 1, then f(x) \u2248 s(y). However, if the classifier is in doubt whether an image contains a \u201clion\u201d or a \u201ctiger\u201d, e.g., p0(lion | x) = 0.6 and\np0(tiger | x) = 0.4, then our predicted semantic embedding, f(x) = 0.6 \u00b7 s(lion) + 0.4 \u00b7 s(tiger), will be something between lion and tiger in the semantic space. Even though \u201cliger\u201d (a hybrid cross between a lion and a tiger) might not be among the training labels, because it is likely that s(liger) \u2248 1\n2 s(lion) + 1 2 s(tiger), then it is likely that f(x) \u2248 s(liger).\nGiven the predicted embedding of x in the semantic space, i.e., f(x), we perform zero-shot classification by finding the class labels with embeddings nearest to f(x) in the semantic space. The top prediction of our model for an image x from the test label set, denoted y\u03021(x, 1), is given by\ny\u03021(x, 1) \u2261 argmax y\u2032\u2208Y1\ncos(f(x), s(y\u2032)) , (3)\nwhere we use cosine similarity to rank the embedding vectors. Moreover, let y\u03021(x, k) denote the kth most likely test label predicted for x. Then, y\u03021(x, k) is defined as the label y\u2032 \u2208 Y1 with the kth largest value of cosine similarity in {cos(f(x), s(y\u2032)); y\u2032 \u2208 Y1}. Note that previous work on zero-shot learning also uses a similar k-nearest neighbor procedure in the semantic space to perform label extrapolation. The key difference in our work is that we define the embedding prediction f(x) based on a standard classifier as in Eq. (2), and not based on a learned regression model."}, {"heading": "4.2 Difference with DeViSE", "text": "The key difference between the ConSE and the DeViSE [6] models is that, the DeViSE model replaces the last layer of a convolutional neural network classifier [7], the Softmax layer, with a linear transformation layer. The new transformation layer is then trained using a ranking objective to map training inputs to continuous semantic embeddings for the correct labels. Subsequently, the lower layers of the convolutional neural network are fine-tuned using the ranking objective to produce better results. In contrast, the ConSE model keeps the Softmax layer of the convolutional net intact, and it does not train the neural network any further. Given a test image, the ConSE simply runs the classifier of Krizhevsky et al. [7], and considers the top T predictions of the model. Then, the convex combination of the corresponding T semantic embedding vectors in the semantic space (see Eq. (2)) is computed, which defines a deterministic transformation from the outputs of the Softmax classifier into the embedding space."}, {"heading": "5 Experiments", "text": "We compare our approach, \u201cconvex combination of semantic embedding\u201d (ConSE), with a stateof-the-art method called \u201cDeep Visual-Semantic Embedding\u201d (DeViSE) [6] on the ImageNet dataset [3]. Both of the models ConSE and DeViSE, use the same skipgram text model [11] to define the semantic label embedding space. The skipgram model was trained on 5.4 billion words from Wikipedia.org to construct 500 dimensional semantic embedding vectors. The embedding vectors are then normalized to have a unit norm. In both ConSE and DeViSE the convolutional neural network of Krizhevsky et al. [7] is used as the image classifier. This neural network is trained on ImageNet 2012 1K set with 1000 training labels. Because the image classifier component, and the semantic label embedding vectors are identical within the ConSE and DeViSE, we can perform a fair comparison between the two techniques.\nWe mirror the ImageNet zero-shot learning experiments of [6]. As in [6], we quantify the zero-shot generalization performance of the models on three test datasets with increasing degree of difficulty. The first zero-shot test dataset, called \u201c2-hops\u201d includes labels from the 2011 21K set which are visually and semantically similar to the training labels in the ImageNet 2012 1K set. This dataset only includes labels within 2 tree hops of the ImageNet 2012 1K labels. A more difficult dataset of labels within 3 hops of the training labels is created in a similar fashion, and referred to as \u201c3- hops\u201d. Finally, a dataset of all the labels in the ImageNet 2011 21K set is created. The three test datasets respectively include 1, 589, 7, 860, and 20, 900 labels. Note that these test datasets include no images with any of the 1000 training labels.\nExperimental results are reported in two metrics: \u201cflat\u201d hit@k and \u201chierarchical\u201d precision@k. Flat hit@k is the percentage of test images for which the model returns the one true label in its top k predictions. Hierarchical precision@k is based on the ImageNet category hierarchy, and it tends to penalize the predictions that are semantically far from the correct labels more than the predictions that are semantically close. Hierarchical precision@k measures on average what fraction of the\nmodel\u2019s top k predictions are among the k most relevant labels for a test image, where relevance of the labels is measure by their distance in the category hierarchy. For a detailed definition of hierarchical precision@k please refer to the supplementary material of [6]. Note that Hierarchical precision@1 is always equivalent to flat hit@1.\nTable 1 shows flat hit@k results for the DeViSE and three versions of the ConSE model. Recall that the ConSE model has a hyper-parameter T that controls the number of training labels used for the convex combination of semantic embeddings. We report the results for T = 1, 10, 1000 as ConSE (T ) in Table 1. Because there are only 1000 training labels, 1 \u2264 T \u2264 1000. Results are reported for three different test datasets with increasing degree of difficulty from top to bottom. For each dataset, we consider including and excluding the training labels within the candidate labels used for k-nearest neighbor label extrapolation (e.g., Y1 in Eq. (3)), although none of the images in the test set are labeled with training labels. This examines whether the models have a bias towards predicting the training labels more than unseen test labels. Datasets that include training labels in their candidate label set are marked by \u201c(+1K)\u201d. Table 1 clearly shows that the ConSE model consistently outperforms the DeViSE on all of the datasets. Among different versions of the ConSE, ConSE(10) performs the best. We do not directly compare the ConSE with the method of Socher et al. [15] because Frome et al. [6] reported that the ranking loss used in the DeViSE significantly outperforms the the squared loss used in [15].\nNot surprisingly, the performance of the models is best when training labels are excluded from the candidate label set. All of the models tend to predict training labels more than test labels, especially at their first few predictions. For example, when training labels are included, the performance of ConSE(10) drops from 9.4% hit@1 to 0.3% on the 2-hops dataset. This suggests that a procedure better than vanilla k-nearest neighbor search need to be employed in order to distinguish images that do not belong to the training labels. We note that the DeViSE has a slightly smaller bias towards training labels as the performance drop after inclusion of training labels is slightly smaller than the performance drop in the ConSE model.\nTable 2 shows hierarchical precision@k results for the Softmax baseline [7], DeViSE [6], and ConSE(10) on ImageNet zero-shot learning task. The results are only reported for ConSE (10) because T = 10 seems to perform the best among T = 1, 10, 1000. The hierarchical metric also confirms that the the ConSE significantly improves upon the DeViSE for zero-shot learning. We did not compare against the Softmax baseline [7] on the flat hit@k measure, because the Softmax model cannot predict any of the test labels. However, using the hierarchical metric, we can now compare with ConSE when the training labels are also included in the label candidate set (denoted by +1K). We find that the top k predictions of the ConSE outperform the Softmax baseline by a large margin.\nWhile the ConSE model is proposed for zero-shot learning, it is natural to ask how the ConSE compares with the DeViSE and the Softmax baseline on the standard classification task with the original 1000 labels, i.e., when the training and test labels are the same. Table 3 shows that the ConSE(10) model improves upon the Softmax baseline in hierarchical precision at 5, 10, and 20, suggesting that the mistakes made by the ConSE model are on average more semantically consistent with the correct class labels, than the Softmax baseline. This improvement is due to the use of label embedding vectors learned from Wikipedia articles. It is also evident the the ConSE(10) model underperforms the DeViSE model, presumably because the DeViSE model is trained with respect to a k-nearest neighbor objective on specifically the set of 1000 labels.\nFinally, Table 4 shows the flat hit@k rates for ConSE, DeViSE, and the Softmax baseline on the 1000-class learning task. Note that the ConSE does not compare well in this context. Our experiments clearly demonstrate that although the DeViSE model performs better than the ConSE on the original 1000-class learning tasks (Table 3, 4), it does not generalize as well as the ConSE model to the unseen zero-shot categories (Table 1, 2). Based on this observation, we conclude that a better k-nearest neighbor classification on the training labels, does not automatically translate into a better k-nearest neighbor classification on a zero-shot learning task. Accordingly, we believe that the DeViSE model is trapped into a variant of overfitting, where the model has learned a highly nonlinear and complex continuous embedding function for images. The DeViSE\u2019s complex embedding function is well suited for predicting the training label embeddings, but it does not generalize well to novel unseen label embedding vectors. In contrast, a simpler embedding model based on convex combination of semantic embeddings (ConSE) generalizes more reliably to unseen zero-shot classes, with little or no chance of overfitting."}, {"heading": "6 Conclusion", "text": "The ConSE approach to mapping images into a semantic embedding space is deceptively simple. Treating classifier scores as weights in a convex combination of word vectors is perhaps the most direct method imaginable for recasting an n-way image classification system as image embedding system. Yet this method outperforms more elaborate joint training approaches both on zero-short learning and on performance metrics which weight errors based on semantic quality. The success of this method undoubted lays is its ability to leverage the strengths inherent in the state-of-the-art image classifier and the state-of-the-art text embedding system from which it was constructed.\nWhile it draws from their strengths, we have no reason to believe that ConSE depends on the details the visual and text models from which it is constructed. In particular, though we used a deep convolutional network with a softmax classifier to generate the weights for our linear combination, any visual object classification system which produces relative scores over a set of classes is compatible with the ConSE framework. Similarly, though we used semantic embedding vectors which were the side product of an unsupervised natural language processing task, the ConSE framework is applicable to any alternative representation of text in which similar concepts are nearby in vector space.\nOne feature of the ConSE model which we did not exploit in our experiments is its natural representation of confidence. The norm of the vector ConSE assigns to an image is a implicit expression of the model\u2019s confidence in the embedding of that image. Label assignments about which the softmax classifier is uncertain be given lower scores, which naturally reduces the magnitude of the ConSE linear combination, particularly if softmax probabilities are used as weights without renormalization. Moreover, linear combinations of labels with disparate semantics under the text model will have a lower magnitude than linear combinations of the same number of closely related labels. These two effects combine such that ConSE only produces embeddings with an L2-norm near 1.0 for images which were either nearly completely unambiguous under the image model or which were assigned a small number of nearly synonymous text labels. We believe that this property could be fruitfully exploited in settings where confidence is a useful signal."}], "references": [{"title": "Cross-generalization: learning novel classes from a single example by feature replacement", "author": ["E. Bart", "S. Ullman"], "venue": "CVPR", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "J.-S. Sen\u00e9cal", "F. Morin", "J.-L. Gauvain"], "venue": "Innovations in Machine Learning", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "CVPR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "One-shot learning of object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "IEEE Trans. PAMI, 28:594\u2013 611", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "M. Ranzato", "T. Mikolov"], "venue": "NIPS", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "One shot learning of simple visual concepts", "author": ["B.M. Lake", "R. Salakhutdinov", "J. Gross", "J.B. Tenenbaum"], "venue": "CogSci", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "author": ["T. Mensink", "J. Verbeek", "F. Perronnin", "G. Csurka"], "venue": "ECCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Zero-shot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G.E. Hinton", "T.M. Mitchell"], "venue": "NIPS", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Evaluating knowledge transfer and zero-shot learning in a largescale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "CVPR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Evaluating knowledge transfer and zero-shot learning in a largescale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "CVPR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "H. Sridhar", "O. Bastani", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 2, "context": "There have been continued efforts in collecting larger image corpora with a broader coverage of object categories [3], thereby enabling image classification with many class labels.", "startOffset": 114, "endOffset": 117}, {"referenceID": 11, "context": "Motivated by the limitations of the standard machine learning framework for n-way classification, several recent papers have proposed methods for mapping images into continuous semantic embedding spaces [12, 4, 9, 6, 15].", "startOffset": 203, "endOffset": 220}, {"referenceID": 3, "context": "Motivated by the limitations of the standard machine learning framework for n-way classification, several recent papers have proposed methods for mapping images into continuous semantic embedding spaces [12, 4, 9, 6, 15].", "startOffset": 203, "endOffset": 220}, {"referenceID": 8, "context": "Motivated by the limitations of the standard machine learning framework for n-way classification, several recent papers have proposed methods for mapping images into continuous semantic embedding spaces [12, 4, 9, 6, 15].", "startOffset": 203, "endOffset": 220}, {"referenceID": 5, "context": "Motivated by the limitations of the standard machine learning framework for n-way classification, several recent papers have proposed methods for mapping images into continuous semantic embedding spaces [12, 4, 9, 6, 15].", "startOffset": 203, "endOffset": 220}, {"referenceID": 14, "context": "Motivated by the limitations of the standard machine learning framework for n-way classification, several recent papers have proposed methods for mapping images into continuous semantic embedding spaces [12, 4, 9, 6, 15].", "startOffset": 203, "endOffset": 220}, {"referenceID": 11, "context": "Our model fits into the zero-shot learning framework [12], which recently received a growing amount of attention [13, 6, 15].", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "Our model fits into the zero-shot learning framework [12], which recently received a growing amount of attention [13, 6, 15].", "startOffset": 113, "endOffset": 124}, {"referenceID": 5, "context": "Our model fits into the zero-shot learning framework [12], which recently received a growing amount of attention [13, 6, 15].", "startOffset": 113, "endOffset": 124}, {"referenceID": 14, "context": "Our model fits into the zero-shot learning framework [12], which recently received a growing amount of attention [13, 6, 15].", "startOffset": 113, "endOffset": 124}, {"referenceID": 3, "context": "These semantic embedding vectors might be obtained from supervised object attributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion [6, 15, 11], based on a separate natural language modeling task.", "startOffset": 85, "endOffset": 91}, {"referenceID": 8, "context": "These semantic embedding vectors might be obtained from supervised object attributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion [6, 15, 11], based on a separate natural language modeling task.", "startOffset": 85, "endOffset": 91}, {"referenceID": 5, "context": "These semantic embedding vectors might be obtained from supervised object attributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion [6, 15, 11], based on a separate natural language modeling task.", "startOffset": 164, "endOffset": 175}, {"referenceID": 14, "context": "These semantic embedding vectors might be obtained from supervised object attributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion [6, 15, 11], based on a separate natural language modeling task.", "startOffset": 164, "endOffset": 175}, {"referenceID": 10, "context": "These semantic embedding vectors might be obtained from supervised object attributes [4, 9], or they might be learned from a text corpus in an unsupervised fashion [6, 15, 11], based on a separate natural language modeling task.", "startOffset": 164, "endOffset": 175}, {"referenceID": 6, "context": "By employing a state-of-the-art convolutional neural network [7] trained only on 1000 object categories from ImageNet, the ConSE model is able to achieve 9.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "When the test object classes are farther from the training classes in the ImageNet category hierarchy, the zero-shot classification results get worse, as expected, but still the ConSE model significantly outperforms the state-of-the-art model [6] applied to the same task.", "startOffset": 243, "endOffset": 246}, {"referenceID": 5, "context": ", [6, 15]) has addressed zero-shot classification by learning a mapping from input features to semantic label embedding vectors using a multivariate regression model.", "startOffset": 2, "endOffset": 9}, {"referenceID": 14, "context": ", [6, 15]) has addressed zero-shot classification by learning a mapping from input features to semantic label embedding vectors using a multivariate regression model.", "startOffset": 2, "endOffset": 9}, {"referenceID": 3, "context": "In computer vision, there has been a body of work on the use of human-labeled attributes [4, 9] to help detecting unseen objects.", "startOffset": 89, "endOffset": 95}, {"referenceID": 8, "context": "In computer vision, there has been a body of work on the use of human-labeled attributes [4, 9] to help detecting unseen objects.", "startOffset": 89, "endOffset": 95}, {"referenceID": 13, "context": "There has been more recent work showing impressive zero-shot performance on visual recognition tasks [14, 10], but these methods also rely on the use of knowledge bases containing descriptive properties of object classes, and the WordNet hierarchy.", "startOffset": 101, "endOffset": 109}, {"referenceID": 9, "context": "There has been more recent work showing impressive zero-shot performance on visual recognition tasks [14, 10], but these methods also rely on the use of knowledge bases containing descriptive properties of object classes, and the WordNet hierarchy.", "startOffset": 101, "endOffset": 109}, {"referenceID": 1, "context": "A more scalable approach to semantic embeddings of class labels builds upon the recent advances in unsupervised neural language modeling [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 10, "context": "The word embeddings are optimized to increase the predictability of each word given its context [11].", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "[6] and Socher et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15] exploit word embeddings to embed textual names of object class labels into a rich semantic space.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "In this work, we also use the skip-gram model [11] to learn class label embeddings.", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "Zero-shot learning is closely related to one-shot learning [5, 1, 8], where the goal is to learn classifiers for object categories based on a few labeled training exemplars.", "startOffset": 59, "endOffset": 68}, {"referenceID": 0, "context": "Zero-shot learning is closely related to one-shot learning [5, 1, 8], where the goal is to learn classifiers for object categories based on a few labeled training exemplars.", "startOffset": 59, "endOffset": 68}, {"referenceID": 7, "context": "Zero-shot learning is closely related to one-shot learning [5, 1, 8], where the goal is to learn classifiers for object categories based on a few labeled training exemplars.", "startOffset": 59, "endOffset": 68}, {"referenceID": 5, "context": "The key difference between the ConSE and the DeViSE [6] models is that, the DeViSE model replaces the last layer of a convolutional neural network classifier [7], the Softmax layer, with a linear transformation layer.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "The key difference between the ConSE and the DeViSE [6] models is that, the DeViSE model replaces the last layer of a convolutional neural network classifier [7], the Softmax layer, with a linear transformation layer.", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "[7], and considers the top T predictions of the model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We compare our approach, \u201cconvex combination of semantic embedding\u201d (ConSE), with a stateof-the-art method called \u201cDeep Visual-Semantic Embedding\u201d (DeViSE) [6] on the ImageNet dataset [3].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "We compare our approach, \u201cconvex combination of semantic embedding\u201d (ConSE), with a stateof-the-art method called \u201cDeep Visual-Semantic Embedding\u201d (DeViSE) [6] on the ImageNet dataset [3].", "startOffset": 184, "endOffset": 187}, {"referenceID": 10, "context": "Both of the models ConSE and DeViSE, use the same skipgram text model [11] to define the semantic label embedding space.", "startOffset": 70, "endOffset": 74}, {"referenceID": 6, "context": "[7] is used as the image classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "We mirror the ImageNet zero-shot learning experiments of [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "As in [6], we quantify the zero-shot generalization performance of the models on three test datasets with increasing degree of difficulty.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Table 1: Flat hit@k performance of DeViSE [6] and ConSE (T ) for T = 1, 10, 1000 on ImageNet zero-shot learning task.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "Table 2: Hierarchical precision@k performance of Softmax baseline [7], DeViSE [6], and ConSE(10) on ImageNet zero-shot learning task.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Table 2: Hierarchical precision@k performance of Softmax baseline [7], DeViSE [6], and ConSE(10) on ImageNet zero-shot learning task.", "startOffset": 78, "endOffset": 81}, {"referenceID": 5, "context": "For a detailed definition of hierarchical precision@k please refer to the supplementary material of [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 14, "context": "[15] because Frome et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] reported that the ranking loss used in the DeViSE significantly outperforms the the squared loss used in [15].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[6] reported that the ranking loss used in the DeViSE significantly outperforms the the squared loss used in [15].", "startOffset": 109, "endOffset": 113}, {"referenceID": 6, "context": "Table 2 shows hierarchical precision@k results for the Softmax baseline [7], DeViSE [6], and ConSE(10) on ImageNet zero-shot learning task.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "Table 2 shows hierarchical precision@k results for the Softmax baseline [7], DeViSE [6], and ConSE(10) on ImageNet zero-shot learning task.", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "We did not compare against the Softmax baseline [7] on the flat hit@k measure, because the Softmax model cannot predict any of the test labels.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "Table 3: Hierarchical precision@k performance of Softmax baseline [7], DeViSE [6], and ConSE on ImageNet original 1000-class learning task.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Table 3: Hierarchical precision@k performance of Softmax baseline [7], DeViSE [6], and ConSE on ImageNet original 1000-class learning task.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Table 4: Flat hit@k performance of Softmax baseline [7], DeViSE [6], and ConSE on ImageNet original 1000-class learning task.", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "Table 4: Flat hit@k performance of Softmax baseline [7], DeViSE [6], and ConSE on ImageNet original 1000-class learning task.", "startOffset": 64, "endOffset": 67}], "year": 2017, "abstractText": "Several recent publications have proposed methods for mapping images into continuous semantic embedding spaces. In some cases the semantic embedding space is trained jointly with the image transformation, while in other cases the semantic embedding space is established independently by a separate task, such as a natural language processing task on a text corpus, and then the image transformation into that space is learned in a second stage. Proponents of these image embedding systems have stressed their advantages over the traditional n-way classification framing of image understanding, particularly in terms of the promise of zero-shot learning \u2013 the ability to correctly annotate images of previously unseen object categories. Here we propose a simple method for constructing an image embedding system from any existing n-way image classification mechanism and any existing semantic embedding space which contains the n class labels in its vocabulary. Our method maps images into the semantic embedding space via convex combination of the class label embedding vectors, and requires no additional learning. We show that this simple and direct method confers many of the advantages associated with more complex image embedding schemes, and indeed outperforms state of the art methods on the ImageNet zero-shot learning task.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}