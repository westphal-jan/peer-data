{"id": "1312.5783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Dec-2013", "title": "Unsupervised Feature Learning by Deep Sparse Coding", "abstract": "in this paper, we propose incorporating a new unsupervised feature learning framework, namely deep sparse coding ( deepsc ), that extends sparse coding to a standardized multi - layer architecture for visual object recognition sorting tasks. the main innovation of the framework is specifically that it connects the sparse - encoders from different layers described by a sparse - to - dense module. the sparse - to - dense module is a composition of a local spatial pooling step methodology and a low - dimensional embedding process, latter which takes advantage of the spatial region smoothness information in developing the image. as for a result, the new method is able to learn several levels of sparse representation of the image field which capture features at multiple a variety of abstraction levels and many simultaneously preserve the spatial distance smoothness between the neighboring image patches. combining the feature representations from implementing multiple layers, deepsc achieves the state - of - the - made art performance on multiple object metadata recognition tasks.", "histories": [["v1", "Fri, 20 Dec 2013 00:21:36 GMT  (306kb,D)", "http://arxiv.org/abs/1312.5783v1", "9 pages, submitted to ICLR"]], "COMMENTS": "9 pages, submitted to ICLR", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["yunlong he", "koray kavukcuoglu", "yun wang", "arthur szlam", "yanjun qi"], "accepted": false, "id": "1312.5783"}, "pdf": {"name": "1312.5783.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Feature Learning by Deep Sparse Coding", "authors": ["Yunlong He", "Koray Kavukcuoglu", "Yun Wang", "Arthur Szlam", "Yanjun Qi"], "emails": ["heyunlong@gatech.edu,", "koray@deepmind.com,", "yunwang@princeton.edu,", "aszlam@ccny.cuny.edu", "yanjun@virginia.edu,"], "sections": [{"heading": "1 Introduction", "text": "Visual object recognition is a major topic in computer vision and machine learning. In the past decade, people have realized that the central problem of object recognition is to learn meaningful representations (features) of the image/videos. A large amount of focus has been put on constructing effective learning architecture that combines modern machine learning methods and in the meanwhile considers the characteristics of image data and vision problems.\nIn this work, we combine the power of deep learning architecture and the bag-of-visual-words (BoV) pipeline to construct a new unsupervised feature learning architecture for learning image representations. Compared to the single-layer sparse coding (SC) framework, our method can extract feature hierarchies at the different levels of abstraction. The sparse codes at the same layer keeps the spatial smoothness across image patches and different SC hierarchies also capture different spatial scopes of the representation abstraction. As a result, the method has richer representation power and hence has better performance on object recognition tasks. Compared to deep learning methods, our method benefits from effective hand-crafted features, such as SIFT features, as the input. Each module of our architecture has sound explanation and can be formulated as explicit optimization problems with promising computational performance. The method shows superior performance over the state-ofthe-art methods in multiple experiments.\nIn the rest of this section, we review the technical background of the new framework, including the pipeline of using bag-of-visual-words for object recognition and a low-dimensional embedding method called DRLIM."}, {"heading": "1.1 Bag-of-visual-words pipeline for object recognition", "text": "We now review the bag-of-visual-words pipeline consisting of hand-crafted descriptor computing, bag-of-visual-words representation learning, spatial pyramid pooling and finally a classifier.\n\u2217heyunlong@gatech.edu, Georgia Institute of Technology \u2020koray@deepmind.com, DeepMind Technologies \u2021yunwang@princeton.edu, Princeton University \u00a7aszlam@ccny.cuny.edu ,The City College of New York \u00b6yanjun@virginia.edu, University of Virginia\nar X\niv :1\n31 2.\n57 83\nv1 [\ncs .L\nG ]\n2 0\nD ec\n2 01\nThe first step of the pipeline is to exact a set of overlapped image patches from each image with fixed patch size, while the spacing between the centers of two adjacent image patches is also fixed. Then a D-dimensional hand-crafted feature descriptor (e.g. 128-dimensional SIFT descriptor) is computed from each image patch. Now let X(i) denote the set of Mi feature descriptors, which are converted from Mi overlapped image patches extracted from the i-th image (e.g. size 300\u00d7 300), i.e.,\nX(i) = [x (i) 1 , \u00b7 \u00b7 \u00b7 , x (i) Mi ] \u2208 RD\u00d7Mi ,\nwhere x(i)j is the feature descriptor of the j-th patch in the i-th image.\nLet X = [X(1), X(2) \u00b7 \u00b7 \u00b7 , X(N)] \u2208 RD\u00d7M , where M = M1 + M2 + \u00b7 \u00b7 \u00b7 + MN , denote the set of all feature descriptors from all N training images. The second step of the pipeline consists of a dictionary learning process and a bag-of-visual-words representation learning process. In the case of using sparse coding to learn the bag-of-visual-words representation, the two processes can be unified as the following problem.\nmin V,Y \u2016X \u2212 V Y \u20162F + \u03b1\u2016Y \u20161,1 (1)\n= M\u2211 m=1 \u2016xm \u2212 V ym\u201622 + \u03b1\u2016ym\u20161\ns.t.\u2016vk\u2016 \u2264 1, \u2200k = 1, \u00b7 \u00b7 \u00b7 ,K\nwhere V = [v1, \u00b7 \u00b7 \u00b7 , vK ] \u2208 RD\u00d7K denotes the dictionary of visual-words, and columns of Y = [y1, \u00b7 \u00b7 \u00b7 , yM ] \u2208 RK\u00d7M are the learned sparse codes, and \u03b1 is the parameter that controls sparsity of the code. We should note, however, other sparse encoding methods such as vector quantization and LLC could be used to learn the sparse representations (see [6] for review and comparisons). Moreover, the dictionary learning process of finding V in (1) is often conducted in an online style [14] and then the feature descriptors of the i-th image stored in X(i) are encoded as the bag-of-visual-words representations stored in Y (i) = [y(i)1 , \u00b7 \u00b7 \u00b7 , y (i) Mi\n] in the K-dimensional space (K >> D). Intuitively speaking, the components of the bag-of-visual-words representation are less correlated compared to the components of dense descriptors. Therefore, compared to the dense feature descriptors, the high-dimensional sparse representations are more favorable for the classification tasks.\nIn the third stage of the pipeline, the sparse bag-of-visual-words representations of all image patches from each image are pooled together to obtain a single feature vector for the image based on the histogram statistics of the visual-words. To achieve this, each image is divided into three levels of pooling regions as suggested by the spatial pyramid matching (SPM) technique [13]. The first level of pooling region is the whole image. The second level is consist of 4 pooling regions which are 4 quadrants of the whole image. The third level consist of 16 pool regions which are quadrants of the second level pooling regions. In this way, we obtain 21 overlapped pooling regions. Then for each pooling region, a max-pooling operator is applied to all the sparse codes whose associating image patch center locates in this pooling region, and we obtain a single feature vector as the result. The max-pooling operator maps any number of vectors that have the same dimensionality to a single vector, whose components are the maximum value of the corresponding components in the mapped vectors. Formally, given the descriptors y1, \u00b7 \u00b7 \u00b7 , yn \u2208 RK that are in the same pooling region, we calculate y = opmax(y1, \u00b7 \u00b7 \u00b7 , yn) := max{y1, \u00b7 \u00b7 \u00b7 , yn} \u2208 RK , (2) where max is operated component-wisely. From the second stage of the framework, we know that the nonzero elements in a sparse code imply the appearance of corresponding visual-words in the image patch. Therefore, the max-pooling operator is actually equivalent to calculating the histogram statistics of the visual-words in a pooling region. Finally, the pooled bag-of-visual-words representations from 21 pooling regions are concatenated to obtain a single feature vector, which is regarded\nas the representation for the image and linear SVM is then used for training and testing on top of this representation. Since the labels of the training images are not used until the final training of SVM, the whole pipeline is regarded as an unsupervised method. For the rest of this paper, we focus on the version of the pipeline where the feature (bag-of-visual-words representation) learning part is performed by a sparse coding step as in (1)."}, {"heading": "1.2 Dimensionality reduction by learning an invariant mapping", "text": "We now review a method called dimensionality reduction by learning an invariant mapping (DRLIM, see [12]), which is the base model for our new method in Subsection 2.3. Different from traditional unsupervised dimensionality reduction methods, DRLIM relies not only on a set of training instances y1, y2, \u00b7 \u00b7 \u00b7 , yn \u2208 RK , but also on a set of binary labels {lij : (i, j) \u2208 I}, where I is the set of index pairs such that (i, j) \u2208 I if the label for the corresponding instance pair (yi, yj) is available. The binary label lij = 0 if the pair of training instances yi and yj are similar instances, and lij = 1 if yi and yj are known to be dissimilar. Notice that the similarity indicated by lij is usually from extra resource instead of the knowledge that can be learned from data instances y1, y2, \u00b7 \u00b7 \u00b7 , yn directly. DRLIM learns a parametric mapping\nA : y \u2208 RK 7\u2192 z \u2208 RD,\nsuch that the embeddings of similar instances attract each other in the low-dimensional space while the embeddings of dissimilar instances push each other away in the low-dimensional space. In this spirit, the exact loss function of DRLIM is as follows:\nL(A) = \u2211\n(i,j)\u2208I\n(1\u2212 lij) 1\n2 \u2016A(yi)\u2212A(yj)\u20162 (3)\n+ lij 1\n2 (max(0, \u03b2 \u2212 \u2016A(yi)\u2212A(yj)\u2016)2,\nwhere \u03b2 > 0 is the parameter for the contrastive loss term which decides the extent to which we want to push the dissimilar pairs apart. Since the parametric mappingA is assumed to be decided by some parameter. DRLIM learn the mapping A by minimizing the loss function in (3) with respect to the parameters of A. The mapping A could be either linear or nonlinear. For example, we can assume A is a two-layer fully connected neural network and then minimize the loss function (3) with respect to the weight. Finally, for any new data instance ynew, its low-dimensional embedding is represented by A(ynew) without knowing its relationship to the training instances."}, {"heading": "2 Deep sparse learning framework", "text": ""}, {"heading": "2.1 Overview", "text": "Recent progress in deep learning [2] has shown that the multi-layer architecture of deep learning system, such as that of deep belief networks, is helpful for learning feature hierarchies from data, where different layers of feature extractors are able to learn feature representations of different scopes. This results in more effective representations of data and benefits a lot of further tasks. The rich representation power of deep learning methods motivate us to combine deep learning with the bagof-visual-words pipeline to achieve better performance on object recognition tasks. In this section, we introduce a new learning framework, named as deep sparse coding (DeepSC), which is built of multiple layers of sparse coding.\nBefore we introduce the details of the DeepSC framework, we first identify two difficulties in designing such a multi-layer sparse coding architecture.\n\u2022 First of all, to build the feature hierarchies from bottom-level features, it is important to take advantage of the spatial information of image patches such that a higher-level feature is a composition of lower-level features. However, this issue is hardly addressed by simply stacking sparse encoders.\n\u2022 Second, it is well-known (see [16, 10]) that sparse coding is not \u201csmooth\u201d, which means a small variation in the original space might lead to a huge difference in the code space. For\ninstance, if two overlapped image patches have similar SIFT descriptors, their corresponding sparse codes can be very different. If another sparse encoder were applied to the two sparse codes, they would lost the affinity which was available in the SIFT descriptor stage. Therefore, stacking sparse encoders would only make the dimensionality of the feature higher and higher without gaining new informations.\nBased on the two observations above, we propose the deep sparse coding (DeepSC) framework as follows. The first layer of DeepSC framework is exactly the same as the bag-of-visual-words pipeline introduced in Subsection 1.1. Then in each of the following layer of the framework, there is a sparse-to-dense module which converts the sparse codes obtained from the last layer to dense codes, which is then followed by a sparse coding module. The output sparse code of the sparse coding module is the input of the next layer. Furthermore, the spatial pyramid pooling step is conducted at every layer such that the sparse codes of current layer are converted to a single feature vector for that layer. Finally, we concatenate the feature vectors from all layers as the input to the classifier. We summarize the DeepSC framework in Figure 2. It is important to emphasis that the whole framework is unsupervised until the final classifier.\nThe sparse-to-dense module is the key innovation of the DeepSC framework, where a \u201cpooling function\u201d is proposed to tackle the aforementioned two concerns. The pooling function is the composition of a local spatial pooling step and a low-dimensional embedding step, which are introduced in Subsection 2.2 and Subsection 2.3 respectively. On one hand, the local spatial pooling step ensures the higher-level features are learned from a collection of nearby lower-level features and hence exhibit larger scopes. On the other hand, the low-dimensional embedding process is designed to take into account the spatial affinities between neighboring image patches such that the spatial smoothness information is not lost during the dimension reduction process. As the combination of the two steps, the pooling function fills the gaps between the sparse coding modules, such that the power of sparse coding and spatial pyramid pooling can be fully expressed in a multi-layer fashion."}, {"heading": "2.2 Learning the pooling function", "text": "In this subsection, we introduce the details of designing the local spatial pooling step, which performs as the first part of the pooling function. First of all, we define the pooling function as a map from a set of sparse codes on a sampling grid to a set of dense codes on a new sampling grid. Assume that G is the sampling grid that includes M sampling points on a image, where the\nany two adjacent sampling points have fixed spacing (number of pixels) between them. As introduced in Subsection 1.1, each sampling point corresponds to the center of a image patch. Let Y = [y1, \u00b7 \u00b7 \u00b7 , yM ] \u2208 RK\u00d7M be the sparse codes on the sampling grid G, where each yi is associated with a sampling point on G according to its associated image patch. Mathematically, the pooling function is defined as the map:\nf : (Y,G) 7\u2192 (Z,G\u2032),\nwhere G\u2032 is the new sampling grid with M \u2032 sampling points and Z = [z1, \u00b7 \u00b7 \u00b7 , zM \u2032 ] \u2208 RD\u00d7M \u2032 stores the D-dimensional dense codes (D < K 1) associated with the sampling points on the new sampling grid G\u2032.\nAs the feature representations learned in the new layer are expected have larger scope than those in the previous layer, we enforce each of the sampling points on new grid G\u2032 to cover a larger area in the image. To achieve this, we take the center of 4 \u00d7 4 neighboring sampling points in G and let it be the new sampling points in G\u2032. By taking the center of every other 4 \u00d7 4 neighboring sampling points, the spacing between neighboring sampling points in G\u2032 is twice of that in G. As a result, we map G to a coarser grid G\u2032 such that M \u2032 \u2248M/4 (see Figure 3). Once the new sampling grid G\u2032 is determined, we finish the local spatial pooling step by applying the max-pooling operator (defined in (2)) to the subsets ofM sparse codes {y1, \u00b7 \u00b7 \u00b7 , yM} and obtain M \u2032 pooled sparse codes associated with the new sampling grid G\u2032. More specifically, let y\u0304i denote the pooled sparse codes associated with the i-th sampling point in G\u2032, where i \u2208 {1, \u00b7 \u00b7 \u00b7 ,M \u2032}. We have y\u0304i := opmax(yi1 , yi2 , \u00b7 \u00b7 \u00b7 , yi16), (4) where {i1, i2, \u00b7 \u00b7 \u00b7 , i16} are the indices of the 16 sampling points in G that are most close to the i-th sampling point in G\u2032."}, {"heading": "2.3 Dimensionality reduction with spatial information", "text": "In this subsection, we introduce the details of combining the DRLIM method [12] with the spatial information of image patches to learn a low-dimensional embedding A such that\nzi := A(y\u0304i). (5)\nAs the feature vector is transformed by A to lower-dimensional space, part of its information is discarded while some is preserved. As introduced in Subsection 1.2, DRLIM is trained on a collection of data instance pairs (y\u0304i, y\u0304j), each of which is associated with a binary label indicating their relationship. Therefore, it provides the option to incorporate prior knowledge in the dimensionality reduction process by determining the binary labels of training pairs based on the prior knowledge.\nIn the case of object recognition, the prior knowledge that we want to impose on the system is that if a image patch is shifted by a few pixels, it still contains the same object. Therefore, we constructed\n1For simplicity, we let D be the same as the dimensionality of SIFT features.\nthe collection of training pairs for DRLIM as follows. We extract training pairs such that there always exist overlapped pixels between the two corresponding patches. Let y\u0304i and y\u0304j be the pooled sparse codes corresponding to two image patches that have overlapped pixels and dij be the distance (in terms of pixels) between them, which is calculated based on the coordinate of the image patch centers. Given a thresholding \u03c3, we set\nlij = { 0 dij < \u03c3 1 dij > \u03c3\n(6)\nGenerated this way, lij = 0 indicates the two image patches are mostly overlapped, while lij = 1 indicates that the two image patch are only partially overlapped. This process of generating training pairs ensures that the training of the transformation A is focused on the most difficult pairs. Experiments shows that if we instead take the pooled sparse codes of far-apart image patches as the negative pairs (lij = 1), DRLIM suffers downgrading in performance. The sensitivity of the system to the thresholding parameter \u03c3 is demonstrated in Table 7.\nLet the linear transformation A be defined by the transformation matrix W \u2208 RD\u00d7K such that\nA(y\u0304i) = Wy\u0304i,\nand then the loss function with respect to the pair (y\u0304i, y\u0304j) is\nLij(W ) = (1\u2212 lij) 1\n2 \u2016Wy\u0304i \u2212Wy\u0304j\u20162 (7)\n+ lijmax(0, \u03b2 \u2212 \u2016Wy\u0304i \u2212Wy\u0304j\u2016)2.\nLet I be the set of index pairs for training pairs collected from all training images,W is then obtained by minimizing the loss with respect to all training pairs, i.e., solving\nmin W \u2211 (i,j)\u2208I Lij\ns.t. \u2016wk\u2016 \u2264 1, \u2200k = 1, \u00b7 \u00b7 \u00b7 ,K."}, {"heading": "3 Experiments", "text": "In this section, we evaluate the performance of DeepSC framework for image classification on three data sets: Caltech-101 [7] , Caltech-256 [11] and 15-Scene. Caltech-101 data set contains 9144 images belonging to 101 classes, with about 40 to 800 images per class. Most images of Caltech101 are with medium resolution, i.e., about 300\u00d7300. Caltech-256 data set contains 29, 780 images from 256 categories. The collection has higher intra-class variability and object location variability than Caltech-101. The images are of similar size to Caltech-101. 15-Scene data set is compiled by several researchers [8, 13, 15], contains a total of 4485 images falling into 15 categories, with the number of images per category ranging from 200 to 400. The categories include living room, bedroom, kitchen, highway, mountain, street and et al.\nFor each data set, the average per-class recognition accuracy is reported. Each reported number is the average of 10 repeated evaluations with random selected training and testing images. For each image, following [4], we sample 16 \u00d7 16 image patches with 4-pixel spacing and use 128 dimensional SIFT feature as the basic dense feature descriptors. The final step of classification is performed using one-vs-all SVM through LibSVM toolkit [5]. The parameters of DRLIM and the parameter to control sparsity in the sparse coding are selected layer by layer through crossvalidation. In the following, we present a comprehensive set of experimental results, and discuss the influence of each of the parameters independently. In the rest of this paper, DeepSC-2 indicates two-layer DeepSC system; DeepSC-3 represents three-layer DeepSC system, and SPM-SC means the one layer baseline, i.e. the BoV pipeline with sparse coding plus spatial pyramid pooling."}, {"heading": "3.1 Effects of Number of DeepSC Layers", "text": "As shown in Figure 2, the DeepSC framework utilizes multiple-layers of feature abstraction to get a better representation for images. Here we first check the effect of varying the number of layers\nutilized in our framework. Table 1 shows the average per-class recognition accuracy on three data sets when all using 1024 as dictionary size. The number of training images per class for the three data sets is set as 30 for Caltech-101, 60 for Caltech-256, and 100 for 15-Scene respectively. The second row shows the results when we have only one layer of the sparse coding, while the third row and the fourth row describe the results when we have two layers in DeepSC or three layers in DeepSC. Clearly the multi-layer structured DeepSC framework has superior performance on all three data sets compared to the single-layer SPM-SC system. Moreover, the classification accuracy improves as the number of layers increases."}, {"heading": "3.2 Effects of SC Dictionary Size", "text": "We examine how performance of the proposed DeepSC framework changes when varying the dictionary size of the sparse coding. On each of the three data sets, we consider three settings where the dimension of the sparse codes K is 1024, 2048 and 4096. The number of training images per class for these experiments is set as 30 for Caltech-101, 60 for Caltech-256, and 100 for 15-Scene respectively. We report the results for the three data sets in Table 2, Table 3 and Table 4 respectively. Clearly, when increasing the dictionary size of sparse coding K from 1024 to 4096, the accuracy of the system improves for all three data sets. We can observe that the performance of DeepSC is always improved with more layers, while in the case of K = 4096 the performance boost in term of accuracy is not so significant. This probably is due to that the parameter space in this case is already very large for the limited training data size. Another observation we made from Table 2, Table 3 and Table 4 is that DeepSC-2 (K=1024) always performs better than SPM-SC (K=2048), and DeepSC-2 (K=2048) always performs better than SPM-SC (K=4096). These two comparisons demonstrate that simply increasing the dimension of sparse codes doesn\u2019t give the same performance boost as increasing the number of layers, and therefore DeepSC framework indeed benefits from the feature hierarchies learned from the image."}, {"heading": "3.3 Effects of Varying Training Set Size", "text": "Furthermore, we check the performance change when varying the number of training images per class on two Caltech data sets. Here we fix the dimension of the sparse codes K as 2048. On Caltech-101, we compare two cases: randomly select 15 or 30 images per category respectively as training images and test on the rest. On Caltech-256, we randomly select 60, 30 and 15 images per category respectively as training images and test on the rest. Table 5 and Table 6 show that with the smaller set of training images, DeepSC framework still continues to improve the accuracy with more layers."}, {"heading": "3.4 Effects of varying parameters of DRLIM", "text": "In table 7, we report the performance variations when tuning the parameters for DRLIM. The parameter \u03c3 is the threshold for selecting positive and negative training pairs (see (6)) and the parameter \u03b2 in the hinge loss (see (7)) of DRLIM model is for controlling penalization for negative pairs. We can see that it is important to choose the proper thresholding parameter \u03c3 such that the transformation learned by DRLIM can differentiate mostly overlapped image pairs and partially overlapped image pairs."}, {"heading": "3.5 Comparison with other methods", "text": "We then compare our results with other algorithms in Table 8. The most direct baselines 2 for DeepSC to compare are the sparse coding plus SPM framework (ScSPM) [17], LLC[16], and SSC[1]. Table 8 shows the comparison of our DeepSC versus the ScSPM and SSC. We can see that our results are comparable to SSC, with a bit lower accuracy on the 15-Scene data (the std of SSC is much higher than ours). For the LLC method proposed from [16], it reported to achieve 73.44% for Caltech-101 when using K = 2048 and 47.68% when using K = 4096. Our DeepSC-3 has achieved 78.43% for Caltech-101 when using K = 2048 and 49.91% when using K = 4096. Overall our system achieves the state-of-the-art performance on all the three data sets.\n2We are also aware of that some works achieve very high accuracy based on adaptive pooling step [9] or multiple-path system that utilizes image patches of multiple sizes [3].\n\u03c3 \\ \u03b2 1 2 3 4 5 6 8 76.5 77.41 77.07 76.71 76.24 75.81\n16 74.93 76.55 76.87 76.97 76.43 75.83 24 73.95 75.43 76.18 76.42 76.53 76.45"}], "references": [{"title": "Smooth sparse coding via marginal regression for learning sparse representations", "author": ["K. Balasubramanian", "K. Yu", "G. Lebanon"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "arXiv preprint arXiv:1206.5538,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Multipath sparse coding using hierarchical matching pursuit", "author": ["L. Bo", "X. Ren", "D. Fox"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Learning mid-level features for recognition", "author": ["Y.-L. Boureau", "F. Bach", "Y. LeCun", "J. Ponce"], "venue": "In CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Libsvm: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "In CVPR, Workshop on Generative-Model Based Vision.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "In CVPR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Geometric p-norm feature pooling for image classification", "author": ["J. Feng", "B. Ni", "Q. Tian", "S. Yan"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Local features are not lonely\u2013laplacian sparse coding for image classification", "author": ["S. Gao", "I.W. Tsang", "L.-T. Chia", "P. Zhao"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In CVPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelop", "author": ["A. Oliva", "A. Torraba"], "venue": "In IJCV,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "We should note, however, other sparse encoding methods such as vector quantization and LLC could be used to learn the sparse representations (see [6] for review and comparisons).", "startOffset": 146, "endOffset": 149}, {"referenceID": 13, "context": "Moreover, the dictionary learning process of finding V in (1) is often conducted in an online style [14] and then the feature descriptors of the i-th image stored in X are encoded as the bag-of-visual-words representations stored in Y (i) = [y 1 , \u00b7 \u00b7 \u00b7 , y (i) Mi ] in the K-dimensional space (K >> D).", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "To achieve this, each image is divided into three levels of pooling regions as suggested by the spatial pyramid matching (SPM) technique [13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "We now review a method called dimensionality reduction by learning an invariant mapping (DRLIM, see [12]), which is the base model for our new method in Subsection 2.", "startOffset": 100, "endOffset": 104}, {"referenceID": 1, "context": "Recent progress in deep learning [2] has shown that the multi-layer architecture of deep learning system, such as that of deep belief networks, is helpful for learning feature hierarchies from data, where different layers of feature extractors are able to learn feature representations of different scopes.", "startOffset": 33, "endOffset": 36}, {"referenceID": 15, "context": "\u2022 Second, it is well-known (see [16, 10]) that sparse coding is not \u201csmooth\u201d, which means a small variation in the original space might lead to a huge difference in the code space.", "startOffset": 32, "endOffset": 40}, {"referenceID": 9, "context": "\u2022 Second, it is well-known (see [16, 10]) that sparse coding is not \u201csmooth\u201d, which means a small variation in the original space might lead to a huge difference in the code space.", "startOffset": 32, "endOffset": 40}, {"referenceID": 11, "context": "In this subsection, we introduce the details of combining the DRLIM method [12] with the spatial information of image patches to learn a low-dimensional embedding A such that", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "In this section, we evaluate the performance of DeepSC framework for image classification on three data sets: Caltech-101 [7] , Caltech-256 [11] and 15-Scene.", "startOffset": 122, "endOffset": 125}, {"referenceID": 10, "context": "In this section, we evaluate the performance of DeepSC framework for image classification on three data sets: Caltech-101 [7] , Caltech-256 [11] and 15-Scene.", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "15-Scene data set is compiled by several researchers [8, 13, 15], contains a total of 4485 images falling into 15 categories, with the number of images per category ranging from 200 to 400.", "startOffset": 53, "endOffset": 64}, {"referenceID": 12, "context": "15-Scene data set is compiled by several researchers [8, 13, 15], contains a total of 4485 images falling into 15 categories, with the number of images per category ranging from 200 to 400.", "startOffset": 53, "endOffset": 64}, {"referenceID": 14, "context": "15-Scene data set is compiled by several researchers [8, 13, 15], contains a total of 4485 images falling into 15 categories, with the number of images per category ranging from 200 to 400.", "startOffset": 53, "endOffset": 64}, {"referenceID": 3, "context": "For each image, following [4], we sample 16 \u00d7 16 image patches with 4-pixel spacing and use 128 dimensional SIFT feature as the basic dense feature descriptors.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "The final step of classification is performed using one-vs-all SVM through LibSVM toolkit [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 16, "context": "The most direct baselines 2 for DeepSC to compare are the sparse coding plus SPM framework (ScSPM) [17], LLC[16], and SSC[1].", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "The most direct baselines 2 for DeepSC to compare are the sparse coding plus SPM framework (ScSPM) [17], LLC[16], and SSC[1].", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "The most direct baselines 2 for DeepSC to compare are the sparse coding plus SPM framework (ScSPM) [17], LLC[16], and SSC[1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 15, "context": "For the LLC method proposed from [16], it reported to achieve 73.", "startOffset": 33, "endOffset": 37}, {"referenceID": 8, "context": "We are also aware of that some works achieve very high accuracy based on adaptive pooling step [9] or multiple-path system that utilizes image patches of multiple sizes [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "We are also aware of that some works achieve very high accuracy based on adaptive pooling step [9] or multiple-path system that utilizes image patches of multiple sizes [3].", "startOffset": 169, "endOffset": 172}, {"referenceID": 16, "context": "Table 8: Comparison of results with other image recognition algorithms: ScSPM[17], LLC[16], and SSC[1].", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "Table 8: Comparison of results with other image recognition algorithms: ScSPM[17], LLC[16], and SSC[1].", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "Table 8: Comparison of results with other image recognition algorithms: ScSPM[17], LLC[16], and SSC[1].", "startOffset": 99, "endOffset": 102}], "year": 2013, "abstractText": "In this paper, we propose a new unsupervised feature learning framework, namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer architecture for visual object recognition tasks. The main innovation of the framework is that it connects the sparse-encoders from different layers by a sparse-to-dense module. The sparse-to-dense module is a composition of a local spatial pooling step and a low-dimensional embedding process, which takes advantage of the spatial smoothness information in the image. As a result, the new method is able to learn several levels of sparse representation of the image which capture features at a variety of abstraction levels and simultaneously preserve the spatial smoothness between the neighboring image patches. Combining the feature representations from multiple layers, DeepSC achieves the state-of-the-art performance on multiple object recognition tasks.", "creator": "LaTeX with hyperref package"}}}