{"id": "1312.6282", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2013", "title": "Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning", "abstract": "learning probabilistic models over strings is an important issue for many applications. spectral methods propose equally elegant solutions to the problem of inferring weighted automata from finite samples of variable - length strings drawn from an unknown target distribution. these methods rely on a singular value decomposition of a matrix $ h _ s $, just called the hankel matrix, that records the frequencies of ( some of ) the observed strings. whereas the accuracy of assuming the learned utility distribution depends both on the quantity of information embedded in $ h _ s $ and on the distance between $ h _ o s $ and its mean $ h _ r $. existing concentration bounds seem to indicate that the concentration over $ h _ r $ gets looser with the relative size of $ h _ r $, suggesting to make a trade - off between the quantity of used information and the size of $ h _ r $. likewise we propose new dimension - sharing free concentration bounds for several variants of hankel matrices. experiments demonstrate that these bounds are tight and that they significantly improve existing bounds. these results suggest ways that the concentration rate of the hankel matrix around its mean does not constitute an argument for limiting its size.", "histories": [["v1", "Sat, 21 Dec 2013 18:10:59 GMT  (65kb,D)", "http://arxiv.org/abs/1312.6282v1", "Extended version of a paper to appear at ICML 2014"]], "COMMENTS": "Extended version of a paper to appear at ICML 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fran\u00e7ois denis", "mattias gybels", "amaury habrard"], "accepted": true, "id": "1312.6282"}, "pdf": {"name": "1312.6282.pdf", "metadata": {"source": "META", "title": "Dimension-free Concentration Bounds on Hankel Matrices for Spectral Learning", "authors": ["Fran\u00e7ois Denis", "Mattias Gybels", "Amaury Habrard"], "emails": ["FRANCOIS.DENIS@LIF.UNIV-MRS.FR", "MATTIAS.GYBELS@LIF.UNIV-MRS.FR", "AMAURY.HABRARD@UNIV-ST-ETIENNE.FR"], "sections": [{"heading": "1. Introduction", "text": "Many applications in natural language processing, text analysis or computational biology require learning probabilistic models over finite variable-size strings such as probabilistic automata, Hidden Markov Models (HMM), or more generally, weighted automata. Weighted automata\nexactly model the class of rational series, and their algebraic properties have been widely studied in that context (Droste et al., 2009). In particular, they admit algebraic representations that can be characterized by a set of finitedimensional linear operators whose rank corresponds to the minimum number of states needed to define the automaton. From a machine learning perspective, the objective is then to infer good estimates of these linear operators from finite samples. In this paper, we consider the problem of learning the linear representation of a weighted automaton, from a finite sample, composed of variable-size strings i.i.d. from an unknown target distribution.\nRecently, the seminal papers of Hsu et al. (2009) for learning HMM and Bailly et al. (2009) for weighted automata, have defined a new category of approaches - the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle & Mohri, 2012). Extensions to probabilistic models for treestructured data (Bailly et al., 2010; Parikh et al., 2011; Cohen et al., 2012), transductions (Balle et al., 2011) or other graphical models (Anandkumar et al., 2012c;b;a; Luque et al., 2012) have also attracted a lot of interest.\nSpectral methods suppose that the main parameters of a model can be expressed as the spectrum of a linear operator and estimated from the spectral decomposition of a matrix that sums up the observations. Given a rational series r, the values taken by r can be arranged in a matrix Hr whose rows and columns are indexed by strings, such that the linear operators defining r can be recovered directly from the right singular vectors of Hr. This matrix is called the Hankel matrix of r.\nIn a learning context, given a learning sample S drawn from a target distribution p, an empirical estimate HS of Hp is built and then, a rational series p\u0303 is inferred from the right singular vectors of HS . However, the size of HS increases drastically with the size of S and state of the art approaches\nar X\niv :1\n31 2.\n62 82\nv1 [\ncs .L\nG ]\n2 1\nD ec\nconsider smaller matrices HU,VS indexed by limited subset of strings U and V . It can be shown that the above learning scheme, or slight variants of it, are consistent as soon as the matrix HU,VS has full rank (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||HU,VS \u2212HU,Vp ||2 between the empirical Hankel matrix and its mean (Hsu et al., 2009; Bailly, 2011).\nOn the one hand, limiting the size of the Hankel matrix avoids prohibitive calculations. Moreover, most existing concentration bounds on sum of random matrices depend on their size and suggest that ||HU,VS \u2212 HU,Vp ||2 may become significantly looser with the size of U and V , compromising the accuracy of the inferred model.\nOn the other hand, limiting the size of the Hankel matrix implies a drastic loss of information: only the strings of S compatible with U and V will be considered. In order to limit the loss of information when dealing with restricted sets U and V , a general trend is to work with other functions than the target p, such as the prefix function p(u) = \u2211 v\u2208\u03a3\u2217 p(uv) or the factor function p\u0302 =\u2211\nv,w\u2208\u03a3\u2217 p(vuw) (Balle et al., 2013; Luque et al., 2012). These functions are rational, they have the same rank as p, a representation of p can easily be derived from representations of p or p\u0302 and they allow a better use of the information contained in the learning sample.\nA first contribution is to provide a dimension free concentration inequality for ||HU,VS \u2212HU,Vp ||2, by using recent results on tail inequalities for sum of random matrices showing that restricting the dimension of H is not mandatory.\nHowever, these results cannot be directly applied as such to the prefix and factor series, since the norm of the corresponding random matrices are unbounded. A second contribution of the paper is then to define two classes of parametrized functions, p\u03b7 and p\u0302\u03b7 , that constitute continuous intermediates between p and p (resp. p and p\u0302), and to provide analogous dimension-free concentration bounds for these two classes.\nThese bounds are evaluated on a benchmark made of 11 problems extracted from the PAutomaC challenge (Verwer et al., 2012). These experiments show that the bounds derived from our theoretical results are quite tight - compared to the exact values- and that they significantly improve existing bounds, even on matrices of fixed dimensions.\nThese results have two practical consequences for spectral learning: (i) the concentration of the empirical Hankel matrix around its mean does not highly depend on its dimension and the only reason not to use all the information contained in the sample should only rely on computing resources limitations. In that perspective, using random techniques to perform singular values decomposition on huge\nHankel matrices should be considered (Halko et al., 2011); (ii) by constrast, the concentration is weaker for the prefix and factor functions, and smoothed variants should be used, with an appropriate parameter.\nThe paper is organized as follows. Section 2 introduces the main notations, definitions and concepts. Section 3 presents a first dimension free-concentration inequality for the standard Hankel matrices. Then, we introduce the prefix and the factor variants and provide analogous concentration results. Section 4 describes some experiments before the conclusion presented in Section 5."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Singular Values, Eigenvalues and Matrix Norms", "text": "Let M \u2208 Rm\u00d7n be a m \u00d7 n real matrix. The singular values of M are the square roots of the eigenvalues of the matrix MTM , where MT denotes the transpose of M : \u03c3max(M) and \u03c3min(M) denote the largest and smallest singular value of M , respectively.\nIn this paper, we mainly use the spectral norms || \u00b7 ||k induced by the corresponding vector norms on Rn and defined by ||M ||k = maxx 6=0 ||Mx||k||x||k :\n\u2022 ||M ||1 = Max1\u2264j\u2264n \u2211m i=1 |M [i, j]|,\n\u2022 ||M ||\u221e = Max1\u2264i\u2264m \u2211n j=1 |M [i, j]|,\n\u2022 ||M ||2 = \u03c3max(M).\nWe have: ||M ||2 \u2264 \u221a ||M ||1||M ||\u221e.\nThese norms can be extended, under certain conditions, to infinite matrices and the previous inequalities remain true when the corresponding norms are defined."}, {"heading": "2.2. Rational stochastic languages and Hankel matrices", "text": "Let \u03a3 be a finite alphabet. The set of all finite strings over \u03a3 is denoted by \u03a3\u2217, the empty string is denoted by , the length of string w is denoted by |w| and \u03a3n (resp. \u03a3\u2264n) denotes the set of all strings of length n (resp. \u2264 n). For any string w, let Pref(w)={u \u2208 \u03a3\u2217|\u2203v \u2208 \u03a3\u2217 w = uv}.\nA series is a mapping r : \u03a3\u2217 7\u2192 R. A series r is convergent if the sequence r(\u03a3\u2264n) = \u2211 w\u2208\u03a3\u2264n r(w) is convergent; its limit is denoted by r(\u03a3\u2217). A stochastic language p is a probability distribution over \u03a3\u2217, i.e. a series taking non negative values and converging to 1.\nLet n \u2265 1 and M be a morphism defined from \u03a3\u2217 to M(n), the set of n\u00d7 n matrices with real coefficients. For all u \u2208 \u03a3\u2217, let us denote M(u) by Mu and \u03a3x\u2208\u03a3Mx by M\u03a3. A series r over \u03a3 is rational if there exists an integer n \u2265 1, two vectors I, T \u2208 Rn and a morphism M :\n\u03a3\u2217 7\u2192 M(n) such that for all u \u2208 \u03a3\u2217, r(u) = ITMuT . The triplet \u3008I,M, T \u3009 is called an n-dimensional linear representation of r. The vector I can be interpreted as a vector of initial weights, T as a vector of terminal weights and the morphism M as a set of matrix parameters associated with the letters of \u03a3. A rational stochastic language is thus a stochastic language admitting a linear representation.\nLet U, V \u2286 \u03a3\u2217, the Hankel matrix HU,Vr , associated with a series r, is the matrix indexed by U \u00d7 V and defined by HU,Vr [u, v] = r(uv), for any (u, v) \u2208 U \u00d7 V . If U = V = \u03a3\u2217,HU,Vr , simply denoted byHr, is a bi-infinite matrix. In the following, we always assume that \u2208 U and that U and V are ordered in quasi-lexicographic order: strings are first ordered by increasing length and then, according to the lexicographic order. It can be shown that a series r is rational if and only if the rank of the matrix Hr is finite. The rank of Hr is equal to the minimal dimension of a linear representation of r.\nLet r be a non negative convergent rational series and let \u3008I,M, T \u3009 be a minimal d-dimensional linear representation of r. Then, the sum Id+M\u03a3+. . .+Mn\u03a3 +. . . is convergent and r(\u03a3\u2217) = IT (Id \u2212M\u03a3)\u22121T where Id is the identity matrix of size d.\nSeveral convergent rational series can be naturally associated with a stochastic language p:\n\u2022 p, defined by p(u) = \u2211 v\u2208\u03a3\u2217 p(uv), the series associ-\nated with the prefixes of the language,\n\u2022 p\u0302, defined by p\u0302(u) = \u2211 v,w\u2208\u03a3\u2217 p(vuw), the series as-\nsociated with the factors of the language.\nIt can be noticed that p(u) = p(u\u03a3\u2217), the probability that a string begins with u, but that in general, p\u0302(u) \u2265 p(\u03a3\u2217u\u03a3\u2217), the probability that a string contains u as a substring.\nIf \u3008I,M, T \u3009 is a minimal d-dimensional linear representation of p, then \u3008I,M, (Id \u2212 M\u03a3)\u22121T \u3009 (resp. \u3008[IT (Id \u2212 M\u03a3)\n\u22121]T ,M, (Id \u2212M\u03a3)\u22121T \u3009) is a minimal linear representation of p (resp. of p\u0302). Any linear representation of these variants of p can be reconstructed from the others.\nFor any integer k \u2265 1, let S(k)p = \u2211\nu1u2...uk\u2208\u03a3\u2217 p(u1u2 . . . uk) = I\nT (Id \u2212M\u03a3)\u2212kT.\nClearly, p(\u03a3\u2217) =S(1)p = 1, p(\u03a3\u2217) =S (2) p and p\u0302(\u03a3\u2217) =S (3) p .\nLet U, V \u2286 \u03a3\u2217. For any string w \u2208 \u03a3\u2217, let us define the matrices HU,Vw , H U,V w and H\u0302 U,V w by\n\u2022 HU,Vw [u, v] = 1uv=w,\n\u2022 HU,Vw [u, v] = 1uv\u2208Pref(w) and\n\u2022 H\u0302U,Vw [u, v] = \u2211 x,y\u2208\u03a3\u2217 1xuvy=w\nfor any (u, v) \u2208 U \u00d7 V . For any sample of strings S, let HU,VS = 1 |S| \u2211 w\u2208S H U,V w , H U,V S = 1 |S| \u2211 w\u2208S H U,V w and\nH\u0302U,VS = 1 |S| \u2211 w\u2208S H\u0302 U,V w .\nFor example, let S = {a, ab}, U = V = { , a, b}. We have\nH U,V S =  0 0.5 00.5 0 0.5 0 0 0  , HU,VS =  1 1 01 0 0.5 0 0 0  , H\u0302U,V S = 2.5 1 0.51 0 0.5 0.5 0 0  ."}, {"heading": "2.3. Spectral Algorithm for Learning Rational Stochastic Languages", "text": "Rational series admit a canonical linear representation determined by their Hankel matrix. Let r be a rational series of rank d and U \u2282 \u03a3\u2217 such that the matrix HU\u00d7\u03a3\u2217r (denoted by H in the following) has rank d.\n\u2022 For any string s, let Ts be the constant matrix whose rows and columns are indexed by \u03a3\u2217 and defined by Ts[u, v] = 1 if v = us and 0 otherwise.\n\u2022 Let E be a vector indexed by \u03a3\u2217 whose coordinates are all zero except the first one equals to 1: E[u] = 1u= and let P be the vector indexed by \u03a3\u2217 defined by P [u] = r(u).\n\u2022 Let H = LDRT be a reduced singular value decomposition of H: R (resp. L) is a matrix whose columns form a set of orthonormal vectors - the right (resp. left) singular vectors of H - and D is a d \u00d7 d diagonal matrix, composed of the singular values of H .\nThen, \u3008RTE, (RTTxR)x\u2208\u03a3, RTP \u3009 is a linear representation of r (Bailly et al., 2009; Hsu et al., 2009; Bailly, 2011; Balle et al., 2012).\nProposition 1. \u3008RTE, (RTTxR)x\u2208\u03a3, RTP \u3009 is a linear representation of r\nProof. From the definition of Ts, it can easily be shown that the mapping s 7\u2192 Ts is a morphism: Ts1Ts2 [u, v] = \u2211 w\u2208\u03a3\u2217 Ts1 [u,w]Ts2 [w, v] = 1 iff v = us1s2 and 0 otherwise. If X is a matrix whose rows are indexed by \u03a3\u2217, we have TsX[u, v] = \u2211 w Ts[u,w]X[w, v] = X[us, v]: ie the rows of TSX are included in the set of rows of X . Then, it follows from the definition of E that ETTs is equal to the first row of Ts (indexed by ) with all coordinates equal to zero except the one indexed by s which equal 1.\nNow, from the reduced singular value decomposition of H = LDRT at rank d, R is a matrix of dimension\u221e\u00d7 d\nwhose columns form a set of orthonormal vectors - the right singular vectors of H - such that RTR = Id and RRTHT =HT (RRT is the orthogonal projection on the subspace spanned by the rows of H). One can easily deduce, by a recurrence over n, that for every string u = x1 . . . xn, (RTTx1R) \u25e6 . . . \u25e6 (RTTxnR)RTHT = RTTuHT . Indeed, the inequality is trivially true for n = 0 since T = Id. Then, we have that RTTxRRTTuHT = RTTxTuH T = RTTxuH T since the columns of TuHT are rows of H and T is a morphism.\nIf PT is the first row of H then: ETR(RTTx1R)\u25e6 . . . \u25e6(RTTxnR)RTP =ETTuP = r(u). Thus, \u3008RTE, (RTTxR)x\u2208\u03a3, RTP \u3009 is a linear representation of r of dimension d. Note here that r is only needed in the right singular vectors R and in the vector P .\nThe basic spectral algorithm for learning rational stochastic languages aims at identifying the canonical linear representation of the target p determined by its Hankel matrix Hp.\nLet S be a sample independently drawn according to p:\n\u2022 Choose sets U, V \u2286 \u03a3\u2217 and build the Hankel matrix HU\u00d7VS ,\n\u2022 choose a rank d and compute a reduced SVD of HU\u00d7VS truncated at rank d,\n\u2022 build the canonical linear representation \u3008RTSE, (RTSTxRS)x\u2208\u03a3, RTSPS\u3009 from the right singular vectors RS and the empirical distribution pS defined from S.\nAlternative learning strategies consist in learning p or p\u0302, using the same algorithm, and then to compute an estimate of p. In all cases, the accuracy of the learned representation mainly depends on the estimation of R. The Stewart formula (Stewart, 1990) bounds the principle angle \u03b8 between the spaces spanned by the right singular vectors of R and RS :\n|sin(\u03b8)| \u2264 ||HU\u00d7VS \u2212HU\u00d7Vr ||2\n\u03c3min(H U\u00d7V r )\n.\nAccording to this formula, the concentration of the Hankel matrix around its mean is critical and the question of limiting the sizes of U and V naturally arises. Note that the Stewart inequality does not give any clear indication on the impact or on the interest of limiting these sets. Indeed, Weyl\u2019s inequalities can be used to show that both the numerator and the denominator of the right part of the inequality increase with U and V ."}, {"heading": "3. Concentration Bounds for Hankel Matrices", "text": "Let p be a rational stochastic language over \u03a3\u2217, let \u03be be a random variable distributed according to p, let U, V \u2286 \u03a3\u2217 and let Z(\u03be) \u2208 R|U |\u00d7|V | be a random matrix. For instance, Z(\u03be) may be equal to HU,V\u03be , H U,V \u03be or H\u0302 U,V \u03be .\nConcentration bounds for sum of random matrices can be used to estimate the spectral distance between the empirical matrix ZS computed on the sample S and its mean (see (Hsu et al., 2011) for references). However, most of classical inequalities depend on the dimensions of the matrices. For example, it can be proved that with probability at least 1\u2212 \u03b4 (Kakade, 2010):\n||ZS \u2212 EZ||2 \u2264 6M\u221a N\n(\u221a log d+ \u221a log 1\n\u03b4\n) (1)\nwhere N is the size of S, d is the minimal dimension of the matrix Z and ||Z||2 \u2264 M almost surely. If Z = HU,V\u03be , then M = 1; if Z = H U,V\n\u03be , M = \u2126(D 1/2) in the worst\ncase; if Z = H\u0302U,V\u03be , ||Z||2 is generally unbounded.\nThese concentration bounds get worse with both sizes of the matrices. Coming back to the discussion at the end of Section 2, they suggest to limit the size of the sets U and V , and therefore, to design strategies to choose optimal sets.\nWe then use recent results (Tropp, 2012; Hsu et al., 2011) to obtain dimension-free concentration bounds for Hankel matrices. Let \u03be1, . . . , \u03beN be some random variables and for each i = 1, . . . , N , and let Xi = Xi(\u03be1, . . . , \u03bei) be a random matrix function of \u03be1, . . . , \u03bei. The notation Ei[\u00b7] is a shortcut for E[\u00b7|\u03be1, . . . , \u03bei\u22121]. Theorem 1. (Matrix Bernstein Bound)(Hsu et al., 2011). If there exists b > 0, \u03c3 > 0, k > 0 s.t. for all i = 1, . . . , N , Ei[Xi] = 0, ||Xi||2 \u2264 b, || 1N \u2211N i=1 Ei(X2i )||2 \u2264\n\u03c32 and E [ tr (\n1 N \u2211N i=1 Ei(X2i ) )] \u2264 \u03c32k almost surely,\nthen for all t > 0,\nPr [ \u03bbmax ( 1\nN N\u2211 i=1 Xi\n) > \u221a 2\u03c32t\nN +\nbt\n3N ] \u2264 k \u00b7 t et \u2212 t\u2212 1 .\nWe use this theorem in the particular case where the random variables \u03bei are i.i.d. and each matrixXi depends only on \u03bei.\nThis theorem is valid for symmetric matrices, but it can be extended to general real-valued matrices thanks to the principle of dilation.\nLet Z be a random matrix, the dilation of Z is the symmetric random matrix X defined by\nX = [ 0 Z ZT 0 ] . Then X2 = [ ZZT 0 0 ZTZ ]\nand ||X||2 = ||Z||2, tr(X2) = tr(ZZT ) + tr(ZTZ) and ||X2||2 \u2264Max(||ZZT ||2, ||ZTZ||2).\nWe can then reformulate the result that we use as follows.\nTheorem 2. Let \u03be1, . . . , \u03beN be i.i.d. random variables, and for i = 1, . . . , N , let Zi = Z(\u03bei) be i.i.d. matrices and Xi the dilation of Zi. If there exists b > 0, \u03c3 > 0, and k > 0 such that E[X1] = 0, ||X1||2 \u2264 b, ||E(X21 )||2 \u2264 \u03c32 and tr(E(X21 )) \u2264 \u03c32k almost surely, then for all t > 0,\nPr [ || 1 N N\u2211 i=1 Xi||2 > \u221a 2\u03c32t N + bt 3N ] \u2264 k\u00b7t(et\u2212t\u22121)\u22121.\nWe will then make use of this theorem to derive our new concentration bounds. Section 3.1 deals with the standard case, Section 3.2 with the prefix case and Section 3.3 with the factor case.\n3.1. Concentration Bound for the Hankel Matrix HU,Vp Let p be a rational stochastic language over \u03a3\u2217, let S be a sample independently drawn according to p, and let U, V \u2286 \u03a3\u2217. In this section, we compute a bound on ||HU,VS \u2212HU,Vp ||2 which is independent from the sizes of U and V and holds in particular when U = V = \u03a3\u2217.\nLet \u03be be a random variable distributed according to p, let Z(\u03be) = HU,V\u03be \u2212 HU,Vp be the random matrix defined by Zu,v = 1\u03be=uv \u2212 p(uv) and let X be the dilation of Z.\nClearly, E(X) = 0. In order to apply Theorem 2, it is necessary to compute the parameters b, \u03c3 and k. We first prove a technical lemma that will provide a bound on E(X2). Lemma 1. For any u, u\u2032 \u2208 U , v, v\u2032 \u2208 V ,\n|E(ZuvZu\u2032v)| \u2264 p(u\u2032v) and |E(ZuvZuv\u2032)| \u2264 p(uv\u2032).\nProof.\nE(ZuvZu\u2032v) = E(1\u03be=uv1\u03be=u\u2032v)\u2212 p(uv)p(u\u2032v) = \u2211 w\u2208\u03a3\u2217 p(w)1w=uv1w=u\u2032v \u2212 p(uv)p(u\u2032v)\n= p(u\u2032v)[1u=u\u2032 \u2212 p(uv)]\nand |E(ZuvZu\u2032v)| \u2264 p(u\u2032v).\nThe second inequality is proved in a similar way.\nNext lemma provides parameters b, \u03c3 and k needed to apply Theorem 2.\nLemma 2. ||X||2 \u2264 2, E(Tr(X2)) \u2264 2S(2)p and ||E(X2)||2 \u2264 S(2)p .\nProof. 1. \u2200u \u2208 U , \u2211 v\u2208V |Zu,v| = \u2211 v\u2208V |1\u03be=uv \u2212 p(uv)| \u2264 1 + p(u\u03a3\u2217) \u2264 2. Therefore, ||Z||\u221e \u2264 2. In a similar way, it can be shown that ||Z||1 \u2264 2. Hence,\n||X||2 = ||Z||2 \u2264 \u221a ||Z||\u221e||Z||1 \u2264 2.\n2. For all (u, u\u2032) \u2208 U2 : ZZT [u, u\u2032] = \u2211 v\u2208V Zu,vZu\u2032,v .\nTherefore,\nE(Tr(ZZT )) = E( \u2211 u\u2208U ZZT [u, u])\n= E( \u2211\nu\u2208U,v\u2208V Zu,vZu,v)\n\u2264 \u2211\nu\u2208U,v\u2208V E(Zu,vZu,v)\n\u2264 S(2)p .\nIn a similar way, it can be proved that E(Tr(ZTZ)) \u2264 S(2)p and therefore, E(Tr(X2)) \u2264 2S(2)p .\n3. For any u \u2208 U ,\u2211 u\u2032\u2208U |E(ZZT [u, u\u2032])| \u2264 \u2211 u\u2032\u2208U,v\u2208V |E(ZuvZu\u2032v)|\n\u2264 \u2211\nu\u2032\u2208U,v\u2208V p(u\u2032v)\n\u2264 S(2)p .\nHence, ||ZZT ||\u221e \u2264 S(2)p . It can be proved, in a similar way, that ||ZTZ||\u221e \u2264 S(2)p , ||ZZT ||1 \u2264 S(2)p and ||ZTZ||1 \u2264 S(2)p . Therefore, ||X2||2 \u2264 S(2)p .\nWe can now prove the main theorem of this section: Theorem 3. Let p be a rational stochastic language and let S be a sample of N strings drawn i.i.d. from p. For all t > 0,\nPr ||HU,VS \u2212HU,Vp ||2 > \u221a 2S (2) p t\nN +\n2t\n3N  \u2264 2t(et\u2212t\u22121)\u22121. Proof. Let \u03be1, . . . , \u03beN be N independent copies of \u03be, let Zi = Z(\u03bei) and let Xi be the dilation of Zi for i = 1, . . . , N . Lemma 2 shows that the 4 conditions of Theorem 2 are fulfilled with b = 2, \u03c32 = S(2)p and k = 2.\nThis bound is independent from U and V . It can be noticed that the proof also provides a dimension dependent bound by replacing S(2)p with \u2211 (u,v)\u2208U\u00d7V p(uv), which may result in a significative improvement if U or V are small.\n3.2. Bound for the prefix Hankel Matrix HU,Vp\nThe random matrix Z(\u03be) = H U,V \u03be \u2212 H U,V p is defined by Zu,v = 1uv\u2208Pref(\u03be) \u2212 p(uv). It can easily be shown that ||Z||2 may be unbounded if U or V are unbounded: ||Z||2 = \u2126(|\u03be|1/2). Hence, Theorem 2 cannot be directly applied, which suggests that the concentration of Z around its mean could be far weaker than the concentration of Z.\nFor any \u03b7 \u2208 [0, 1], we define a smoothed variant of p by\np\u03b7(u) = \u2211 x\u2208\u03a3\u2217 \u03b7|x|p(ux) = \u2211 n\u22650 \u03b7np(u\u03a3n).\nNote that p1 = p, p0 = p and that p(u) \u2264 p\u03b7(u) \u2264 p(u) for any string u. Therefore, the functions p\u03b7 are natural intermediates between p and p. Moreover, when p is rational, each p\u03b7 is also rational.\nProposition 2. Let p be a rational stochastic language and let \u3008I, (Mx)x\u2208\u03a3, T \u3009 be a minimal linear representation of p. Let T \u03b7 = (Id \u2212 \u03b7M\u03a3)\u22121T . Then, p\u03b7 is rational and \u3008I, (Mx)x\u2208\u03a3, T \u03b7\u3009 is a linear representation of p\u03b7 . Proof. For any string u, p\u03b7(u) = \u2211 n\u22650 I TMu\u03b7 nMn\u03a3T =\nITMu( \u2211 n\u22650 \u03b7 nMn\u03a3)T = I TMuT \u03b7 .\nNote that T can be computed from T \u03b7 when \u03b7 and M\u03a3 are known and therefore, it is a consistent learning strategy to learn p\u03b7 from the data, for some \u03b7, and next, to derive p.\nFor any 0 \u2264 \u03b7 \u2264 1, let Z\u03b7(\u03be) be the random matrix defined by\nZ\u03b7[u, v] = \u2211 x\u2208\u03a3\u2217 \u03b7|x|1\u03be=uvx \u2212 p\u03b7(uv)\n= \u2211 x\u2208\u03a3\u2217 \u03b7|x|(1\u03be=uvx \u2212 p(uvx)).\nfor any (u, v) \u2208 U \u00d7 V . It is clear that E(Z\u03b7) = 0 and we show below that ||Z\u03b7||2 is bounded if \u03b7 < 1.\nThe moments S(k)p\u03b7 can naturally be associated with p\u03b7 . For any 0 \u2264 \u03b7 \u2264 1 and any k \u2265 1, let\nS (k) p\u03b7\n= \u2211\nu1u2...uk\u2208\u03a3\u2217 p\u03b7(u1u2 . . . uk).\nWe have S(k)p\u03b7 = I T (Id\u2212M\u03a3)\u2212k(Id\u2212 \u03b7M\u03a3)\u22121T and it is clear that S(k)p0 = S (k) p and S (k) p1 = S (k+1) p . Lemma 3. ||Z\u03b7||2 \u2264 1\n1\u2212 \u03b7 + S (1) p\u03b7 .\nProof. Indeed, let u \u2208 U .\u2211 v\u2208V |Z\u03b7[u, v]| \u2264 \u2211 v,x\u2208\u03a3\u2217 \u03b7|x|1\u03be=uvx + \u2211 v,x\u2208\u03a3\u2217 \u03b7|x|p(uvx)\n\u2264 (1 + \u03b7 + . . .+ \u03b7|\u03be|\u2212|u|) + S(1)p\u03b7 \u2264 1 1\u2212 \u03b7 + S (1) p\u03b7 .\nHence, ||Z\u03b7||\u221e \u2264 11\u2212\u03b7 +S (1) p\u03b7 . Similarly, ||Z\u03b7||1 \u2264 11\u2212\u03b7 + S (1) p\u03b7 , which completes the proof.\nWhen U and V are bounded, let l be the maximal length of a string in U \u222a V . It can easily be shown that ||Z\u03b7||2 \u2264 l + 1 + S\n(1) p\u03b7 and therefore, in that case,\n||Z\u03b7||2 \u2264Min(l + 1, 1\n1\u2212 \u03b7 ) + S (1) p\u03b7\n(2)\nwhich holds even if \u03b7 = 1.\nLemma 4. |E(Z\u03b7[u, v]Z\u03b7[u\u2032, v])| \u2264 p\u03b7(u\u2032v), for any u, u\u2032, v \u2208 \u03a3\u2217.\nProof. We have E((1\u03be=w \u2212 p(w))(1\u03be=w\u2032 \u2212 p(w\u2032))) = E(1\u03be=w1\u03be=w\u2032)\u2212 p(w)p(w\u2032). Therefore,\nE(Z\u03b7[u, v]Z\u03b7[u\u2032, v])\n= \u2211 x,x\u2032 \u03b7|xx \u2032|[E(1\u03be=uvx1\u03be=u\u2032vx\u2032)\u2212 p(u\u2032vx\u2032)p(uvx)]\n= \u2211 x,x\u2032,w \u03b7|xx \u2032|p(w)1w=u\u2032vx\u2032 [1w=uvx \u2212 p(uvx)]\n= \u2211 x,x\u2032 \u03b7|xx \u2032|p(u\u2032vx\u2032)[1u\u2032vx\u2032=uvx \u2212 p(uvx)]\n= \u2211 x\u2032 \u03b7|x \u2032|p(u\u2032vx\u2032)[ \u2211 x \u03b7|x|(1u\u2032vx\u2032=uvx \u2212 p(uvx))]\nand |E(Z\u03b7[u, v]Z\u03b7[u\u2032, v])| \u2264 \u2211 x\u2032 \u03b7|x \u2032|p(u\u2032vx\u2032) = p\u03b7(u \u2032v)\nsince \u22121 \u2264 \u2212p\u03b7(uv) \u2264 \u2211 x \u03b7|x|(1u\u2032vx\u2032=uvx \u2212 p(uvx)) \u2264 1 i.e. | \u2211 x \u03b7|x|(1u\u2032vx\u2032=uvx \u2212 p(uvx))| \u2264 1.\nLemma 5.\n||E(Z\u03b7 Z T \u03b7 )||2 \u2264 S (2) p\u03b7 and Tr(E(Z\u03b7 Z T \u03b7 )) \u2264 S (2) p\u03b7 .\n||E(ZT\u03b7 Z\u03b7)||2 \u2264 S (2) p\u03b7 and Tr(E(ZT\u03b7 Z\u03b7)) \u2264 S (2) p\u03b7 .\nProof. Indeed,\n||E(Z\u03b7Z T \u03b7 )||\u221e \u2264Maxu \u2211 u\u2032,v |E(Z\u03b7[u, v]Z\u03b7[u\u2032, v])|\n\u2264 \u2211 u\u2032,v,x\u2032 \u03b7|x \u2032|p(u\u2032vx\u2032) \u2264 S(2)p\u03b7 .\nIn the same way,\nTr(E(Z\u03b7Z T \u03b7 )) = \u2211 u,v E(Z\u03b7[u, v]Z\u03b7[u, v]) \u2264 S(2)p\u03b7 .\nSimilar computations provide all the inequalities.\nTherefore, we can apply the Theorem 2 with b = 11\u2212\u03b7 + S (1) p\u03b7 , \u03c32 = S (2) p\u03b7 and k = 2.\nTheorem 4. Let p be a rational stochastic language, let S be a sample of N strings drawn i.i.d. from p and let 0 \u2264 \u03b7 < 1. For all t > 0,\nPr ||HU,V\u03b7,S \u2212HU,Vp\u03b7 ||2 > \u221a 2S (2) p\u03b7 t N + t 3N [ 1 1\u2212 \u03b7 + S (1) p\u03b7 ] \u2264 2t(et \u2212 t\u2212 1)\u22121.\nRemark that when \u03b7 = 0 we find back the concentration bound of Theorem 3, and that Inequality 2 provides a bound when \u03b7 = 1."}, {"heading": "3.3. Bound for the factor Hankel Matrix Hp\u0302U,V", "text": "The random matrix Z\u0302(\u03be) = H\u0302U,V\u03be \u2212Hp\u0302U,V is defined by\nZ\u0302u,v = \u2211\nx,y\u2208\u03a3\u2217 1\u03be=xuvy \u2212 p\u0302(uv).\n||Z\u0302||2 is generally unbounded. Moreover, unlike the prefix case, ||Z\u0302||2 can be unbounded even if U and V are finite. Hence, the Theorem 2 cannot be directly applied either.\nWe can also define smoothed variants of p\u0302 by\np\u0302\u03b7(u) = \u2211\nx,y\u2208\u03a3\u2217 \u03b7|xy|p(xuy) = \u2211 m,n\u22650 \u03b7m+np(\u03a3mu\u03a3n)\nwhich have properties similar to functions p\u03b7:\n\u2022 p \u2264 p\u0302\u03b7 \u2264 p\u0302, p\u03021 = p\u0302 and p\u03020 = p,\n\u2022 if \u3008I, (Mx)x\u2208\u03a3, T \u3009 be a minimal linear representation of p then \u3008I\u0302\u03b7, (Mx)x\u2208\u03a3, T \u03b7\u3009, where I\u0302\u03b7 = (Id \u2212 \u03b7MT\u03a3 ) \u22121I , is a linear representation of p\u0302\u03b7 .\nHowever, proofs of the previous Section cannot be directly extended to p\u0302\u03b7 because p is bounded by 1, a property which is often used in the proofs, while p\u0302 is not. Next lemma provides a tool which allows to bypass this difficulty.\nLemma 6. Let 0 < \u03b7 \u2264 1. For any integer n, (n+ 1)\u03b7n \u2264 K\u03b7 where\nK\u03b7 =\n{ 1 if \u03b7 \u2264 e\u22121\n(\u2212e\u03b7 ln \u03b7)\u22121 otherwise.\nProof. Let f(x) = (x + 1)\u03b7x. We have f \u2032(x) = \u03b7x(1 + (x + 1) ln \u03b7) and f takes its maximum for xM = \u22121 \u2212 1/ ln \u03b7, which is positive if and only if \u03b7 > 1/e. We have f(xM ) = (\u2212e\u03b7 ln \u03b7)\u22121.\nLemma 7. Let w, u \u2208 \u03a3\u2217. Then,\u2211 x,y\u2208\u03a3\u2217 \u03b7|xy|1w=xuy \u2264 K\u03b7 and p\u0302(u) \u2264 K\u03b7p(\u03a3\u2217u\u03a3\u2217).\nProof. Indeed, if w = xuy, then |xy| = |w| \u2212 |u| and u appears at most |w| \u2212 |u|+ 1 times as a factor of w.\np\u0302(u) = \u2211\nx,y\u2208\u03a3\u2217 \u03b7|xy|p(xuy)\n= \u2211\nw\u2208\u03a3\u2217u\u03a3\u2217 p(w) \u2211 x,y\u2208\u03a3\u2217 \u03b7|xy|1w=xuvy\n\u2264 K\u03b7p(\u03a3\u2217u\u03a3\u2217).\nFor \u03b7 \u2208 [0, 1], let Z\u0302\u03b7(\u03be) be the random matrix defined by\nZ\u0302\u03b7[u, v] = \u2211\nx,y\u2208\u03a3\u2217 \u03b7|xy|1\u03be=xuvy \u2212 p\u0302\u03b7(uv)\n= \u2211\nx,y\u2208\u03a3\u2217 \u03b7|xy|(1\u03be=xuvy \u2212 p(xuvy)).\nand, for any k \u2265 0, let\nS (k) p\u0302\u03b7\n= \u2211\nu1u2...uk\u2208\u03a3\u2217 p\u0302\u03b7(u1u2 . . . uk).\nIt can easily be shown that E(Z\u0302\u03b7) = 0, S(k)p\u0302\u03b7 = I T (Id \u2212 \u03b7M\u03a3) \u22121(Id \u2212M\u03a3)\u2212k(Id \u2212 \u03b7M\u03a3)\u22121T , S(k)p\u03020 = S (k) p and S (k) p\u03021 = S (k+2) p .\nIt can be shown that ||Z\u0302\u03b7||2 is bounded if \u03b7 < 1. Lemma 8.\n||Z\u0302\u03b7||2 \u2264 (1\u2212 \u03b7)\u22122 + S(1)p\u0302\u03b7 .\nProof. Indeed, for all u,\u2211 v\u2208V |Z\u0302\u03b7[u, v]| \u2264 \u2211 v,x,y\u2208\u03a3\u2217 \u03b7|xy|1\u03be=xuvy + p\u0302\u03b7(uv)\n\u2264 (1 + \u03b7 + . . .+ \u03b7|\u03be|\u2212|u|)2 + S(1)p\u0302\u03b7 \u2264 1 (1\u2212 \u03b7)2 + S (1) p\u03b7 .\nHence, ||Z\u0302\u03b7||\u221e \u2264 1(1\u2212\u03b7)2 + S (1) p\u0302\u03b7 . Similarly, ||Z\u03b7||1 \u2264 1\n(1\u2212\u03b7)2 + S (1) p\u0302\u03b7 , which completes the proof. Lemma 9. For any u, u\u2032, v \u2208 \u03a3\u2217, |E(Z\u0302\u03b7[u, v]Z\u0302\u03b7[u\u2032, v])| \u2264 K\u03b7 \u2211 x\u2032y\u2032 \u03b7 |x\u2032y\u2032|p(x\u2032u\u2032vy\u2032).\nProof. We have\nE(Z\u0302\u03b7[u, v]Z\u0302\u03b7[u\u2032, v]) =\u2211 x,x\u2032,y,y\u2032 \u03b7|xx \u2032yy\u2032|[E(1\u03be=xuvy1\u03be=x\u2032u\u2032vy\u2032)\n\u2212 p(x\u2032u\u2032vy\u2032)p(xuvy)].\nWe remark that\nE(1\u03be=xuvy1\u03be=x\u2032u\u2032vy\u2032)\u2212 p(x\u2032u\u2032vy\u2032)p(xuvy)\n= \u2211 w p(w)1w=x\u2032u\u2032vy\u2032(1w=xuvy \u2212 p(xuvy)),\nand therefore, E(Z\u0302\u03b7[u, v]Z\u0302\u03b7[u\u2032, v]) =\u2211 x\u2032,y\u2032,w \u03b7|x \u2032y\u2032|p(w)1w=x\u2032u\u2032vy\u2032( \u2211 x,y \u03b7|xy|(1w=xuvy \u2212 p(xuvy))). Moreover, | \u2211 xy \u03b7 |xy|(1w=xuvy \u2212 p(xuvy))| \u2264 K\u03b7 .\nLemma 10.\n||E(Z\u0302Z\u0302T )||2 \u2264 K\u03b7S(2)p\u0302\u03b7 and Tr(E(Z\u0302Z\u0302 T )) \u2264 K\u03b7S(2)p\u0302\u03b7 .\nProof. We have ||E(Z\u0302Z\u0302T )||\u221e \u2264 Supu \u2211 u\u2032,v |E(Z\u0302\u03b7[u, v]Z\u0302\u03b7[u\u2032, v])|\nThen from previous lemma:\u2211 u\u2032,v |E(Z\u0302\u03b7[u, v]Z\u0302\u03b7[u\u2032, v])| \u2264 K\u03b7S (2) p\u0302\u03b7\nfor any u \u2208 \u03a3\u2217. Finally,\nTr(E(Z\u0302Z\u0302T )) = \u2211 u,v E(Z\u0302\u03b7[u, v]Z\u0302\u03b7[u, v]) \u2264 K\u03b7S(2)p\u0302\u03b7 .\nSimilar proof gives\nLemma 11.\n||E(Z\u0302T Z\u0302)||2 \u2264 K\u03b7S(2)p\u0302\u03b7 and Tr(E(Z\u0302 T Z\u0302)) \u2264 K\u03b7S(2)p\u0302\u03b7 .\nEventually, we can apply the Theorem 2 with b = (1 \u2212 \u03b7)\u22122 + S\n(1) p\u0302\u03b7 , \u03c32 = K\u03b7S (2) p\u0302\u03b7 and k = 2.\nTheorem 5. Let p be a rational stochastic language, let S be a sample of N strings drawn i.i.d. from p and let 0 \u2264 \u03b7 < 1. For all t > 0,\nPr ||H\u0302U,V\u03b7,S \u2212HU,Vp\u0302\u03b7 ||2 > \u221a 2K\u03b7S (2) p\u0302\u03b7 t\nN +\nt\n3N\n[ 1\n(1\u2212 \u03b7)2 + S (1) p\u0302\u03b7 ] \u2264 2t(et \u2212 t\u2212 1)\u22121.\nRemark that when \u03b7 = 0 we find back the concentration bound of Theorem 3. We provide experimental evaluation of the proposed bounds in the next Section."}, {"heading": "4. Experiments", "text": "The proposed bounds are evaluated on the benchmark of PAutomaC (Verwer et al., 2012) which provides samples of strings generated from several probabilistic automata, designed to evaluate probabilistic automata learning. Eleven problems have been selected from that benchmark for which sparsity of the Hankel matrices makes the use of standard SVD algorithms available from NumPy or SciPy possible. Table 1 provides some information about the selected problems.\nFigure 1 shows the typical behavior of S(1)p\u03b7 and S (1) p\u0302\u03b7 , similar for all the problems.\nFor each problem, the exact value of ||HU,VS \u2212 HU,Vp ||2 is computed for sets U and V of the form \u03a3\u2264l, trying to maximize l according to our computing resources. It is compared to the bounds provided by Theorem 3 and Equation (1), with \u03b4 = 0.05 (Table 2). The optimized bound\n(\u201dopt.\u201d), refers to the case where \u03c32 has been calculated over U \u00d7 V rather than \u03a3\u2217\u00d7\u03a3\u2217 (see the remark at the end of Section 3.1). Tables 3 and 4 show analog comparisons for the prefix and the factor cases with different values of \u03b7. Similar results have been obtained for all the problems of PautomaC. We can remark that our dimension-free bounds are significantly more accurate than the one provided by\nEquation (1). Notice that in the prefix case, the dimensionfree bound has a better behavior in the limit case \u03b7 = 1 than the bound from Eq. (1). This is due to the fact that in our bound, the term that bounds ||Z||2 appears in the 1N term while it appears in the 1\u221a\nN term in the other one.\nImplication for learning These results show that the concentration of the empirical Hankel matrix around its mean does not highly depend on its dimension and they suggest that as far as computational resources permit it, the size of the matrices should not be artificially restricted in spectral algorithms for learning HMMs or rational stochastic languages.\nTo illustrate this claim, we have performed additional experiments by considering matrices with 3,000 columns and a variable number of rows, from 70 to 3,000.\nFor each problem and each set of rows and columns, we have computed the r first right singular vectors R of HU,V (resp. RS of H U,V S ), where r is the rank of the target, and the distance between the linear spaces spanned by R and RS . Most classical distances are based on the principal angles \u03b81 \u2265 \u03b82 \u2265 . . . \u2265 \u03b8r between the spaces span(R) and span(RS). The largest principal angle \u03b81 is a harsh measure since, even if the two spaces coincide along the last r\u2212 1 principal angles, the distance between the two spaces can be large. We have considered the following measure\nd(span(R), span(RS)) = 1\u2212 1\nr r\u2211 i=1 cos \u03b8i (3)\nwhich is equal to 0 if the spaces coincide and 1 if they are completely orthogonal, and which takes into account all the principal angles.\nThe table 5 shows the sum \u2211r i=1 cos \u03b8i for each problem. The table 6 displays the same information but each measure is normalised by using formula 3.\nThese tables show that for all problems but two, the spaces spanned by the right singular vectors are the closest for the maximal size Hankel matrix. They also show that these spaces remain quite distant for 6 problems over 11. For 4 problems, the spaces are already close to each other even for small matrices - but it can be noticed that widening the matrix do not deteriorate the results."}, {"heading": "5. Conclusion", "text": "We have provided dimension-free concentration inequalities for Hankel matrices in the context of spectral learning of rational stochastic languages. These bounds cover 3 cases, each one corresponding to a specific way to exploit the strings under observation, paying attention to the strings themselves, to their prefixes or to their factors. For the last two cases, we introduced parametrized variants which allow a trade-off between the rate of the concentration and the exploitation of the information contained in data.\nA consequence of these results is that there is no a priori good reason, aside from computing resources limitations, to restrict the size of the Hankel matrices. This suggests\nan immediate future work consisting in investigating recent random techniques (Halko et al., 2011) to compute singular values decomposition on Hankel matrices in order to be able to deal with huge matrices. Then, a second aspect is to evaluate the impact of these methods on the quality of the models, including an empirical evaluation of the behavior of the standard approach and its prefix and factor extensions, along with the influence of the parameter \u03b7.\nAnother research direction would be to link up the prefix and factor cases to concentration bounds for sum of random tensors and to generalize the results to the case where a fixed number \u2265 1 of factors is considered for each string.\nAcknowledments This work was supported by the French National Agency for Research (Lampada - ANR-09-EMER-007)."}], "references": [{"title": "A spectral algorithm for latent dirichlet allocation", "author": ["A. Anandkumar", "D.P. Foster", "D. Hsu", "S. Kakade", "Liu", "Y.-K"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Learning mixtures of tree graphical models", "author": ["A. Anandkumar", "D. Hsu", "F. Huang", "S. Kakade"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A method of moments for mixture models and hidden markov models", "author": ["A. Anandkumar", "D. Hsu", "S.M. Kakade"], "venue": "Proceedings of COLT - Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "M\u00e9thodes spectrales pour l\u2019inf\u00e9rence grammaticale probabiliste de langages stochastiques rationnels", "author": ["R. Bailly"], "venue": "PhD thesis, Aix-Marseille Universite\u0301,", "citeRegEx": "Bailly,? \\Q2011\\E", "shortCiteRegEx": "Bailly", "year": 2011}, {"title": "Grammatical inference as a principal component analysis problem", "author": ["R. Bailly", "F. Denis", "L. Ralaivola"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Bailly et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bailly et al\\.", "year": 2009}, {"title": "A spectral approach for probabilistic grammatical inference on trees", "author": ["R. Bailly", "A. Habrard", "F. Denis"], "venue": "In Proceedings of ALT,", "citeRegEx": "Bailly et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bailly et al\\.", "year": 2010}, {"title": "Spectral learning of general weighted automata via constrained matrix completion", "author": ["B. Balle", "M. Mohri"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Balle and Mohri,? \\Q2012\\E", "shortCiteRegEx": "Balle and Mohri", "year": 2012}, {"title": "A spectral learning algorithm for finite state transducers", "author": ["B. Balle", "A. Quattoni", "X. Carreras"], "venue": "In Proceedings of ECML/PKDD", "citeRegEx": "Balle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balle et al\\.", "year": 2011}, {"title": "Local loss optimization in operator models: A new insight into spectral learning", "author": ["B. Balle", "A. Quattoni", "X. Carreras"], "venue": "In Proceedings of ICML,", "citeRegEx": "Balle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Balle et al\\.", "year": 2012}, {"title": "Spectral learning for non-deterministic dependency parsing", "author": ["F.M. Luque", "A. Quattoni", "B. Balle", "X. Carreras"], "venue": "In Proceedings of EACL,", "citeRegEx": "Luque et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Luque et al\\.", "year": 2012}, {"title": "A spectral algorithm for latent tree graphical models", "author": ["A.P. Parikh", "L. Song", "E.P. Xing"], "venue": "In Proceedings of ICML,", "citeRegEx": "Parikh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parikh et al\\.", "year": 2011}, {"title": "Reduced-rank hidden Markov models", "author": ["S. Siddiqi", "B. Boots", "G.J. Gordon"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010),", "citeRegEx": "Siddiqi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Siddiqi et al\\.", "year": 2010}, {"title": "Hilbert space embeddings of hidden markov models", "author": ["L. Song", "B. Boots", "S.M. Siddiqi", "G.J. Gordon", "A.J. Smola"], "venue": "In Proceedings of ICML,", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Perturbation theory for the singular value decomposition", "author": ["G.W. Stewart"], "venue": "In SVD and Signal Processing II: Algorithms, Analysis and Applications,", "citeRegEx": "Stewart,? \\Q1990\\E", "shortCiteRegEx": "Stewart", "year": 1990}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["J.A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Tropp,? \\Q2012\\E", "shortCiteRegEx": "Tropp", "year": 2012}, {"title": "Results of the PAutomaC probabilistic automaton learning competition", "author": ["S. Verwer", "R. Eyraud", "C. de la Higuera"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Verwer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Verwer et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "(2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle & Mohri, 2012).", "startOffset": 180, "endOffset": 262}, {"referenceID": 12, "context": "(2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle & Mohri, 2012).", "startOffset": 180, "endOffset": 262}, {"referenceID": 8, "context": "(2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al., 2010; Song et al., 2010; Balle et al., 2012; Balle & Mohri, 2012).", "startOffset": 180, "endOffset": 262}, {"referenceID": 5, "context": "Extensions to probabilistic models for treestructured data (Bailly et al., 2010; Parikh et al., 2011; Cohen et al., 2012), transductions (Balle et al.", "startOffset": 59, "endOffset": 121}, {"referenceID": 10, "context": "Extensions to probabilistic models for treestructured data (Bailly et al., 2010; Parikh et al., 2011; Cohen et al., 2012), transductions (Balle et al.", "startOffset": 59, "endOffset": 121}, {"referenceID": 7, "context": ", 2012), transductions (Balle et al., 2011) or other graphical models (Anandkumar et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 9, "context": ", 2011) or other graphical models (Anandkumar et al., 2012c;b;a; Luque et al., 2012) have also attracted a lot of interest.", "startOffset": 34, "endOffset": 84}, {"referenceID": 0, "context": "(2009) for learning HMM and Bailly et al. (2009) for weighted automata, have defined a new category of approaches the so-called spectral methods - for learning distributions over strings represented by finite state models (Siddiqi et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 3, "context": "It can be shown that the above learning scheme, or slight variants of it, are consistent as soon as the matrix H S has full rank (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||H S \u2212H p ||2 between the empirical Hankel matrix and its mean (Hsu et al.", "startOffset": 129, "endOffset": 181}, {"referenceID": 8, "context": "It can be shown that the above learning scheme, or slight variants of it, are consistent as soon as the matrix H S has full rank (Hsu et al., 2009; Bailly, 2011; Balle et al., 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||H S \u2212H p ||2 between the empirical Hankel matrix and its mean (Hsu et al.", "startOffset": 129, "endOffset": 181}, {"referenceID": 3, "context": ", 2012) and that the accuracy of the inferred series is directly connected to the concentration distance ||H S \u2212H p ||2 between the empirical Hankel matrix and its mean (Hsu et al., 2009; Bailly, 2011).", "startOffset": 169, "endOffset": 201}, {"referenceID": 9, "context": "In order to limit the loss of information when dealing with restricted sets U and V , a general trend is to work with other functions than the target p, such as the prefix function p(u) = \u2211 v\u2208\u03a3\u2217 p(uv) or the factor function p\u0302 = \u2211 v,w\u2208\u03a3\u2217 p(vuw) (Balle et al., 2013; Luque et al., 2012).", "startOffset": 245, "endOffset": 285}, {"referenceID": 15, "context": "These bounds are evaluated on a benchmark made of 11 problems extracted from the PAutomaC challenge (Verwer et al., 2012).", "startOffset": 100, "endOffset": 121}, {"referenceID": 4, "context": "Then, \u3008RE, (RTxR)x\u2208\u03a3, RP \u3009 is a linear representation of r (Bailly et al., 2009; Hsu et al., 2009; Bailly, 2011; Balle et al., 2012).", "startOffset": 59, "endOffset": 132}, {"referenceID": 3, "context": "Then, \u3008RE, (RTxR)x\u2208\u03a3, RP \u3009 is a linear representation of r (Bailly et al., 2009; Hsu et al., 2009; Bailly, 2011; Balle et al., 2012).", "startOffset": 59, "endOffset": 132}, {"referenceID": 8, "context": "Then, \u3008RE, (RTxR)x\u2208\u03a3, RP \u3009 is a linear representation of r (Bailly et al., 2009; Hsu et al., 2009; Bailly, 2011; Balle et al., 2012).", "startOffset": 59, "endOffset": 132}, {"referenceID": 13, "context": "The Stewart formula (Stewart, 1990) bounds the principle angle \u03b8 between the spaces spanned by the right singular vectors of R and RS : |sin(\u03b8)| \u2264 ||HU\u00d7V S \u2212HU\u00d7V r ||2 \u03c3min(H U\u00d7V r ) .", "startOffset": 20, "endOffset": 35}, {"referenceID": 14, "context": "We then use recent results (Tropp, 2012; Hsu et al., 2011) to obtain dimension-free concentration bounds for Hankel matrices.", "startOffset": 27, "endOffset": 58}, {"referenceID": 15, "context": "Experiments The proposed bounds are evaluated on the benchmark of PAutomaC (Verwer et al., 2012) which provides samples of strings generated from several probabilistic automata, designed to evaluate probabilistic automata learning.", "startOffset": 75, "endOffset": 96}], "year": 2013, "abstractText": "Learning probabilistic models over strings is an important issue for many applications. Spectral methods propose elegant solutions to the problem of inferring weighted automata from finite samples of variable-length strings drawn from an unknown target distribution. These methods rely on a singular value decomposition of a matrix HS , called the Hankel matrix, that records the frequencies of (some of) the observed strings. The accuracy of the learned distribution depends both on the quantity of information embedded in HS and on the distance betweenHS and its mean Hr. Existing concentration bounds seem to indicate that the concentration over Hr gets looser with the size of Hr, suggesting to make a tradeoff between the quantity of used information and the size of Hr. We propose new dimensionfree concentration bounds for several variants of Hankel matrices. Experiments demonstrate that these bounds are tight and that they significantly improve existing bounds. These results suggest that the concentration rate of the Hankel matrix around its mean does not constitute an argument for limiting its size.", "creator": "LaTeX with hyperref package"}}}