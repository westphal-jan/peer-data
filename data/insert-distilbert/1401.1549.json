{"id": "1401.1549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jan-2014", "title": "Optimal Demand Response Using Device Based Reinforcement Learning", "abstract": "demand response ( dr ) for residential and small commercial buildings systems is estimated to account for as much as 65 % of the total energy savings potential of dr, and previous work shows indicate that a fully automated energy management system ( ems ) task is basically a second necessary prerequisite to dr efforts in these areas. in this prior paper, we propose a novel specialized ems formulation for dr problems in these sectors. specifically, basically we formulate a fully automated ems's rescheduling problem as a reinforcement centered learning ( rl ) problem ( effectively referred on to as the device based rl problem ), and show that this novel rl problem decomposes over devices under reasonable assumptions. compared with with existing formulations, significantly our aforementioned new formulation ( 1 ) does not simply require explicitly modeling the user's dissatisfaction on job rescheduling, ( 2 ) enables the ems to self - internally initiate jobs, ( 3 ) allows typically the user to initiate more flexible requests and ( 4 ) has a computational complexity linear in the number of devices. formally we also propose several new enhanced performance metrics for rl algorithms applied to the device based rl problem, and independently demonstrate the relevant simulation results of applying q - learning, one of the most extensively popular and classical rl algorithms, to a representative example.", "histories": [["v1", "Wed, 8 Jan 2014 00:49:01 GMT  (242kb,D)", "http://arxiv.org/abs/1401.1549v1", null], ["v2", "Sat, 28 Jun 2014 04:24:47 GMT  (185kb,D)", "http://arxiv.org/abs/1401.1549v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.SY", "authors": ["zheng wen", "daniel o'neill", "hamid reza maei"], "accepted": false, "id": "1401.1549"}, "pdf": {"name": "1401.1549.pdf", "metadata": {"source": "CRF", "title": "Optimal Demand Response Using Device Based Reinforcement Learning", "authors": ["Zheng Wen", "Daniel O\u2019Neill", "Hamid Reza Maei"], "emails": [], "sections": [{"heading": null, "text": "Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated Energy Management System (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS\u2019s rescheduling problem as a reinforcement learning (RL) problem (referred to as the device based RL problem), and show that this RL problem decomposes over devices under reasonable assumptions. Compared with existent formulations, our new formulation (1) does not require explicitly modeling the user\u2019s dissatisfaction on job rescheduling, (2) enables the EMS to self-initiate jobs, (3) allows the user to initiate more flexible requests and (4) has a computational complexity linear in the number of devices. We also propose several new performance metrics for RL algorithms applied to the device based RL problem, and demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example.\nKeywords: Demand Response, Energy Management System, Building and Home Automation, Reinforcement Learning, Markov Decision Process"}, {"heading": "1 Introduction", "text": "Demand response (DR) systems [Borenstein et al., 2002, Braithwait and Eakin, 2002, Barbose et al., 2004] dynamically adjust electrical demand in response to changing electrical energy prices or other grid signals. DR offers several benefits. By suitably adjusting energy prices, load can be shifted from peak energy consumption periods to other times. This, in turn, can improve operational efficiency, reduce operating costs, improve capital efficiency, and reduce harmful emissions and risk of outages. The variability of renewables can create an additional need to shift energy consumption in order to better match energy demand with unforecasted changes in electrical energy generation. The benefit is a reduction in backup (ancillary) generation frequently used to hedge renewable sources.\nThere are several types of DR. In direct DR a utility or other entity directly modifies the energy consumption of users by adjusting the operation of user\u2019s equipment. Interruptible tariffs allow a utility to interrupt the supply of power to a company under predefined conditions. Price driven DR uses pricing mechanisms to attempt to modulate energy demand. DR has been extensively investigated for larger energy users and has been implemented in many areas (e.g., [Roos and Lane, 1998, Piette et al., 2005]). Residential and small commercial building DR [Herter, 2007b,a, Faruqui and George, 2005, Koch and Piette, 2007] offers similar potential benefits. DR for residential and\n\u2217Z. Wen, D. O\u2019Neill and H. Maei are with the Department of Electrical Engineering, Stanford University, Stanford, CA 94305, USA.\nar X\niv :1\n40 1.\n15 49\nv1 [\ncs .L\nG ]\n8 J\nan 2\n01 4\nsmall commercial buildings was estimated to account for as much as 65% of the total energy savings potential of DR. However, DR in the residential and small commercial building sector faces several challenges.\nTechnical challenges include deploying an infrastructure supplying real-time pricing information to energy consumers1 in a useful way, ensuring security, and implementing advanced metering and networking devices [Koch and Piette, 2007, LeMay et al., 2008]. In addition to all these technical challenges, another challenge vital to the success of DR in the residential and small commercial building sectors is that it requires a fully automated Energy Management System (EMS) [Koch and Piette, 2007, Piette et al., 2007]. This is because with price driven DR, consumers face a continuing sequence of decisions to either use a particular device now and consume energy at current (known) prices or to defer using the device until later at possibly unknown prices. Each decision requires the consumer to weigh the cost differential against his dissatisfaction due to rescheduling device usage. This is particularly burdensome when the consumer must also estimate future energy prices. Further many of these decisions have limited financial impact on the consumer [ONeill et al., 2010], and, as a result, many rational consumers in the residential and small commercial building sectors may not be sufficiently incentivized to make these decisions over the long run (known as \u201cdecision fatigue\u201d in [ONeill et al., 2010]).\nTo be effective, an EMS system needs to automatically make energy consumption decisions that are consistent with the cost delay trade-offs of energy users, in this way acting as an energy agent. It is often difficult to cost-effectively model the behaviors of the idiosyncratic consumers and the temporal variations of the energy prices, a successful DR EMS needs to learn to make optimal decisions for consumers from interacting with the consumers and energy prices. Recently, [ONeill et al., 2010] proposed a fully-automated EMS algorithm based on reinforcement learning (RL), which learns how to make optimal decisions for consumers. To the best of our knowledge, this is the first paper to apply RL to DR in residential and small commercial building sector. The authors adopt a request inventory model for the system dynamics and use Q-learning, a classical RL algorithm, to learn how to make the optimal decisions for energy consumers. In this approach, users make energy requests to the EMS system (e.g. pushing a button on a device that a user wants to run) and the system schedules the time of operation by calculating the user\u2019s trade-offs between delay and energy prices. It learns these trade-offs by observing energy consumer\u2019s behaviors and observing the patterns of energy pricing. Over time the EMS learns to make the best decisions for energy users in the sense that it balances energy cost and the delay in energy usage in the same way that the customer optimally would, but without the consumer having to make the decision. The authors explicitly assume that (1) consumers\u2019 dissatisfaction with delay can be modeled by known disutility functions, and (2) that consumers explicitly initiate all energy usage. This approach has several limitations:\n\u2022 Finding specific disutility functions for a particular residence or small business can be difficult and costly. In [ONeill et al., 2010], the authors assume these functions have particular mathematical properties, but do not address how these functions might be determined. These functions are likely to be idiosyncratic and are specific to energy price vs. time delay tradeoffs.\n\u2022 Many energy consuming activities occur without the consumer directly initiating them. HVAC in office buildings and pool heaters in residential settings are obvious examples. A useful EMS would self-initiate jobs for these and similar devices without explicit user requests or reservations. For example, if it is unexpectedly hot in a summer afternoon and the current energy\n1Throughout this paper, we use the terms \u201cuser\u201d and \u201cconsumer\u201d interchangeably.\nprice is expected to be cheaper than that in the evening, then the EMS should be allowed to turn on AC without an explicit request/reservation from the consumer.\n\u2022 The computational complexity of this approach grows exponentially as the number of devices. Known as the \u201ccurse-of-dimensionality\u201d in dynamic programming (DP) and RL literature, this problem limits the approach to fairly small numbers of devices.\nIn this paper, we propose a novel EMS formulation that addresses the limitations of [ONeill et al., 2010] described above. Our proposed algorithm also uses RL, but adopts a device centric point of view, and, as we will discuss in detail in Section 3, under reasonable assumptions, the RL problem decomposes over devices and it is sufficient to apply an RL algorithm to each individual device. Specifically, the algorithm addresses these issues:\n1. Our EMS formulation does not require a pre-specified disutility function modeling the consumer\u2019s dissatisfaction on job rescheduling. Instead, under this formulation, the EMS is able to learn the consumer\u2019s dissatisfaction by sampling his evaluations on completed/canceled jobs. In other words, our new RL formulation has eliminated the impractical assumption in [ONeill et al., 2010] that consumers\u2019 dissatisfactions with delay can be captured by known disutility functions.\n2. Our approach allows both user-initiated jobs and EMS-initiated jobs. The EMS-initiated jobs use a probing/feedback mechanism to find the best way to anticipate future energy usage.\n3. Our EMS algorithm also enables more flexible user-initiated jobs, specifically\n\u2022 A consumer request\u2019s target time can be different from its request time, where the request time is the time when the EMS receives this consumer request, and the target time is the time when the consumer prefers this request to be satisfied.\n\u2022 Consumer requests/reservations can have different priorities, whereas in [ONeill et al., 2010], all the consumer requests have the same priority.\n\u2022 Energy requests can be canceled by the consumer, reflecting the behavior of real energy users.\n4. The computational complexity of our approach grows linearly with the number of devices, and thus many classical RL algorithms can be applied even when there are a large number of devices.\nIn this paper, we also propose several new performance metrics for any RL algorithm applied to this problem. In particular, we suggest methods of measuring performance relative to the user\u2019s current pattern of behavior and relative to a prescient optimal pattern of behavior.\nThe remainder of this paper is organized as follows. In Section 2, we briefly describe how a practical fully-automated EMS should interact with the consumer and the grid signals. In Section 3, we pose the optimal demand response problem faced by an EMS as an infinite horizon discounted Markov decision process (MDP), and decompose this high-dimensional MDP into a collection of low-dimensional device based MDPs under appropriate assumptions. Then, in Section 4, we propose the device based RL problem and discuss several performance metrics for RL algorithms applied to this problem. In Section 5, we demonstrate the simulation results on a representative example when the classical Q-learning is applied. We conclude in Section 6."}, {"heading": "2 Description of fully-automated EMS", "text": "A fully-automated EMS (henceforth referred to as EMS) is a necessary prerequisite to DR in the residential and small commercial building sectors. Furthermore, an EMS needs to learn how to make the optimal decisions for the user while interacting with them and the real-time grid signals. In this section, we describe how a fully-automated EMS should interact with the user and the grid signals.\nGenerally speaking, a fully-automated EMS observes the grid signals, receives requests and evaluations from the user, and schedules the jobs over the devices (Figure 1). We detail the interaction mechanisms in the remainder of this subsection.\nGrid Signals\nThe EMS observes the grid signals through a communication network, where the \u201ccommunication network\u201d refers to the infrastructure that supplies grid signals to the EMS. Any exogenous information that is effectively delivered by the communication network and is useful for the EMS to make the scheduling decisions can be regarded as a grid signal. The most common grid signal is the real-time energy price; other grid signals might include the expected future energy prices, the real-time temperature and weather condition, and other useful exogenous information.\nIn this paper, we assume that (1) the only available grid signal is the real-time energy price, and (2) the energy price is exogenous in the sense that the EMS\u2019s actions will not influence it (i.e. the EMS is a price-taker). Notice that the second assumption is reasonable since in the residential and small commercial building sectors, the market power of an individual EMS (or equivalently, of an individual consumer) is so small that the impact of its actions on energy prices is negligible. On the other hand, the first assumption is proposed mainly to simplify the exposition and does not incur loss of generality. Specifically, both the model and the algorithm proposed in this paper can be applied to the cases with more general grid signals (e.g. the case in which the grid signals are energy price and temperature), as long as the grid signals are exogenous.\nUser-Initiated Jobs and EMS-Initiated Jobs\nA fully-automated EMS should perform the following functions:\n\u2022 The EMS receives requests from the consumers, and then schedules when to fulfill the received requests. We henceforth refer to this case as a user-initiated job.\n\u2022 If a smart device managed by the EMS is idle (i.e. currently there is no request for that device), the EMS could speculatively power that device. We henceforth refer to this case as an EMSinitiated job. For instance, in a small commercial building, the EMS might speculatively turn on the building\u2019s air conditioning in advance of the tenant\u2019s arrival to capture early morning lower energy costs or to mask the latency of cooling the building. Notice that we should not allow the EMS to do speculative jobs on all the smart devices (such as dishwashers).\nWe assume time is discrete t = 0, 1, \u00b7 \u00b7 \u00b7 and that there are N smart devices managed by the EMS and numbered n = 1, 2, \u00b7 \u00b7 \u00b7 , N . To simplify exposition, we assume that all the jobs done by device n are standardized and hence they can be completed in one time step and consume a constant energy Cn, which only depends on the type of the smart device. This assumption can be readily relaxed to devices with different operating periods. We further assume that the EMS will ignore all the user requests to device n if device n currently has an uncompleted request, but the EMS allows a consumer to cancel an existing uncompleted request. Specifically, if the consumer wants to replace an existing request with a new request, he must cancel the existent request first, and then start the new request. Notice under this assumption, at each time a smart device has at most one uncompleted request.\nInteraction between user and EMS\nWe now describe how the EMS interacts with the consumer (user) in a user-initiated job and an EMS-initiated job. Specifically, as its name suggests, a user-initiated job starts with a user sending a request to the EMS. Specifically, each consumer request is represented by a four-tuple J = (n, \u03c4r, \u03c4g, g), where\n\u2022 n denotes the requested device;\n\u2022 \u03c4r is the request time and denotes when the EMS receives this request;\n\u2022 \u03c4g is the target time and denotes when the user prefers this requested job to be completed;\n\u2022 g denotes the priority of this request, with higher priority implying the \u201cstronger preference\u201d of the user that they want the requested job to be completed at a time close to the target time \u03c4g.\nNotice that the target time \u03c4g is not necessarily equal to the request time \u03c4r; instead, the user might request to use a device in a later time (i.e. \u03c4g \u2265 \u03c4r). On the other hand, for a request J = (n, \u03c4r, \u03c4g, g), it is unreasonable to assume that \u03c4g \u2212 \u03c4r, the difference between the target time and the request time, can be arbitrarily large. Thus, in this paper, we assume that (1) for any request J = (n, \u03c4r, \u03c4g, g), its target time \u03c4g must satisfy \u03c4r \u2264 \u03c4g \u2264 \u03c4r + Wn, and (2) if the request J = (n, \u03c4r, \u03c4g, g) is not fulfilled by time \u03c4g +Wn, then it will be canceled by the user, where Wn is a known time window and only depends on the type of device.\nWe also assume that an unsatisfied consumer request can be canceled by the user. Furthermore, we assume that when a consumer request is completed or canceled, the user will send an evaluation on the completed/canceled request to the EMS. As we will see later, the EMS can use such evaluations to learn the user\u2019s dissatisfaction on the rescheduling of the user-initiated jobs.\nOn the other hand, an EMS-initiated job is started by the EMS, without receiving a request from the user. The only interaction between the EMS and the user for such jobs is that once an\nEMS-initiated job is completed, the user will send an evaluation on it to the EMS. As is in the user-initiated jobs, the EMS also exploits such evaluations to learn user\u2019s dissatisfaction with the EMS-initiated jobs.\nIn summary, the interaction between user and EMS is as follows: for user-initiated jobs, the possible interactions include (1) the user sends requests to the EMS, (2) the user can choose to cancel the unsatisfied requests and (3) the user evaluates the completed/canceled requests. On the other hand, for EMS-initiated jobs, the only interaction between user and EMS is that the user evaluates the completed EMS-initiated jobs.\nTo complete the description of the user-EMS interaction, we need to specify the \u201cuser-EMS interaction timeline\u201d in a single time period (i.e. whether the EMS receives the user requests/cancellations/evaluations before or after it schedules jobs in that time period). Without loss of generality, in this paper, we consider the user-EMS interaction timeline illustrated in Figure 2."}, {"heading": "3 Device Based DR Model", "text": "This section proceeds as follows. We first motivate and define the dissatisfaction function and the instantaneous cost function for a rational consumer in Subsection 3.1 and 3.2, respectively. Explicit dissatisfaction functions are not required in practice, but in this section we assume they are known to facilitate easy exposition. Then, Subsection 3.3 formulates the optimal demand response problem as a collection of device-based MDPs under suitable assumptions."}, {"heading": "3.1 Dissatisfaction Function and Rational Consumers", "text": "To formalize the notion of optimal demand response, in this subsection, we define the dissatisfaction function for a consumer (user) that captures their preferences (dissatisfaction) over job rescheduling. We also motivate and propose a specific class of functional forms for the dissatisfaction functions of the rational consumers.\nLet us start by defining some useful notation. Specifically, for each time t, we define the sets of smart devices D(t), C(t) \u2286 {1, 2, \u00b7 \u00b7 \u00b7 , N} as\nD(t) = {devices that do a job at time t} C(t) = {devices for which an unsatisfied request is canceled at time t} .\nNote that D(t) \u22c2 C(t) = \u2205, since if device n does a job in time period t, then there is no unsatisfied request for device n at the end of time period t (recall that at any time, device n has at most one unsatisfied request, and see Figure 2 for the timeline of time period t). Furthermore, we use J (t) to denote the set of unsatisfied requests right after the requests in time period t have been\nreceived. Since at any time device n has at most one unsatisfied request, thus, if J (t) contains one unsatisfied request for device n, then we use J(t, n) to denote this particular request; otherwise, we set J(t, n) = NULL. Notice that Ht = {(D(s), C(s),J (s)) , \u22000 \u2264 s \u2264 t} completely specifies the \u201chistory\u201d of device operations, consumer requests/cancellations and the EMS decisions until the end of time period t.\nGenerally speaking, the user\u2019s dissatisfaction on rescheduling at time t should be a function of Ht, and we denote this function as\nU\u0304 (t) [Ht] = U\u0304 (t) [(D(s), C(s),J (s)) , \u22000 \u2264 s \u2264 t] . (1)\nObviously, directly working with U\u0304 (t) [Ht] defined in Eqn(1) will result in a computationally intractable problem. To overcome this challenge, in the remainder of this subsection, we make some assumptions on the functional form of U\u0304 (t) [Ht]. All such assumptions are motivated by observations of the preferences and behaviors of the rational consumers. Generally speaking, we think a rational consumer\u2019s preferences and behaviors should have the following characteristics:\n1. A rational consumer prefers low electricity price to high electricity price.\n2. For a consumer request sent to the EMS, a rational consumer prefers that job is completed at a time close to the target time, and the higher the request\u2019s priority is, the \u201cstronger\u201d this preference is.\n3. For an EMS-initiated job, a rational consumer will be \u201chappy\u201d if the EMS does this job at a \u201cright\u201d time, and will be \u201cunhappy\u201d if it does this job at a \u201cwrong\u201d time.\n4. When a rational consumer decides to cancel an unsatisfied consumer request, he is usually \u201cunhappy\u201d if the target time has already passed.\n5. A rational consumer generally discounts future costs and benefits.\n6. A rational consumer\u2019s preference over job rescheduling changes slowly over time and is reflected in his evaluations on completed/canceled jobs.\n7. A rational consumer\u2019s preference over job rescheduling on smart devices is weak compared to his preferences in other aspects of life.\nWe now motivate and propose a specific class of functional forms for the dissatisfaction function U\u0304 (t) [Ht] of the rational consumers. To formalize the idea, we define the \u201chistory\u201d for device n at time t as\nH(n)t = {(1(n \u2208 D(s)),1(n \u2208 C(s)), J(s, n)) , \u22000 \u2264 s \u2264 t} ,\nwhere 1(\u00b7) is the indicator function and J(s, n) is defined above. First, we make the following simplifying assumption:2\n2Note that a rational consumer\u2019s preference over job rescheduling is weak compared to his preferences in other aspects of life. Based on this, let us provide an intuitive motivation for Assumption 1 from the following perspective: let u\u0304DR \u2208 <N be a vector encoding the user\u2019s dissatisfactions on job rescheduling over N devices, and u\u0304other be a vector encoding the user\u2019s dissatisfactions in other aspects of life. Assume the overall dissatisfaction (unhappiness) of the user is f(u\u0304DR, u\u0304other), where f is a general non-linear function. Now we consider f(u\u0304DR + \u2206u\u0304DR, u\u0304other), notice that the weak preference over job rescheduling implies that \u2206u\u0304DR is \u201csmall\u201d, thus, f(u\u0304DR + \u2206u\u0304DR, u\u0304other) can be well approximated by f(u\u0304DR, u\u0304other) + \u2207u\u0304DRf(u\u0304DR, u\u0304other)\u2206u\u0304DR. Notice that for our purpose, we only care about the \u201cchange\u201d of the user\u2019s overall unhappiness as a function of the \u201cchanges\u201d in his dissatisfactions on job rescheduling, thus, the user\u2019s dissatisfaction function is U\u0304(\u2206u\u0304DR) = f(u\u0304DR + \u2206u\u0304DR, u\u0304other) \u2212 f(u\u0304DR, u\u0304other) \u2248 \u2207u\u0304DRf(u\u0304DR, u\u0304other)\u2206u\u0304DR, which is a weighted sum of \u2206u\u0304DR. In Assumption 1, we further simplify the dissatisfaction function by assuming that all the weights are equal to 1.\nAssumption 1 For any t \u2265 0, the dissatisfaction function U\u0304 (t) is additive over the devices, that is\nU\u0304 (t) (Ht) = N\u2211 n=1 U\u0304 (t,n)(H(n)t ), (2)\nwhere U\u0304 (t,n) captures the consumer\u2019s dissatisfaction at time t for device n and H(n)t is the \u201chistory\u201d for device n by time t.\nFurthermore, since a rational consumer\u2019s preference over job rescheduling is weak compared to his preferences in other aspects of life, and changes slowly over time, we assume that\n\u2022 If there is no job completed/cancelled at time t on device n, we assume that U\u0304 (t,n)(H(n)t ) = 0.\n\u2022 If there is a job completed/cancelled at time t on device n, we assume that U\u0304 (t,n)(H(n)t ) only depends on the job completed/canceled. That is, U\u0304 (t,n)(\u00b7) is time-invariant and does not depend on the previously completed/canceled jobs.\nFinally, based on the assumptions above and the characteristics (2)-(4) of a rational consumer\u2019s preferences over job rescheduling, we assume the dissatisfaction function U\u0304 (t,n)(H(n)t ) takes the following forms:\n\u2022 If device n satisfies consumer request J(t, n) = (n, \u03c4r, \u03c4g, g) at time t, we assume\nU\u0304 (t,n)(H(n)t ) = U\u0303 (n)r (t\u2212 \u03c4g, g), (3)\nwhere the subscript \u201cr\u201d denotes that it is the dissatisfaction incurred when a request is satisfied, and g is the priority of the request. Note t \u2212 \u03c4g captures not only the distance between the current time t and the request\u2019s target time \u03c4g, but also whether or not the target time has passed. For rational consumers, U\u0303 (n) r should be small when t\u2212 \u03c4g is close to 0 and increases as t \u2212 \u03c4g deviates from 0; furthermore, the higher the priority g, the higher this increase rate will be.\n\u2022 Similarly, if the request J(t, n) = (n, \u03c4r, \u03c4g, g) is cancelled by the consumer at time t, we assume\nU\u0304 (t,n)(H(n)t ) = U\u0303 (n)c (t\u2212 \u03c4g, g), (4)\nwhere the subscript \u201cc\u201d denotes that it is the dissatisfaction incurred when a request is cancelled. In practice, U\u0303 (n) c should be very small if t < \u03c4g, and it will increase with t when t \u2265 \u03c4g; furthermore, the higher the priority g, the higher this increase rate will be.\n\u2022 If EMS initiates a job on device n at time t, we assume that\nU\u0304 (t,n)(H(n)t ) = U\u0303 (n)e (t\u2212 \u03c4p), (5)\nwhere the subscript \u201ce\u201d denotes that it is the dissatisfaction incurred when an EMS-initiated job is done, and \u03c4p\u2212 1 is the time when the previous job on device n (either user-initiated or EMS-initiated) is completed or cancelled. In practice, U\u0303 (n) e should decrease as t\u2212\u03c4p increases; the large U\u0303 (n) e for small t\u2212 \u03c4p prevents too frequent EMS-initiated jobs.\n\u2022 Otherwise, we assume that\nU\u0304 (t,n)(H(n)t ) = 0. (6)\nNote that the dissatisfaction function class described in Equations (3-6) is still quite general,\nand it is very challenging to proceed to derive the specific functional forms of U\u0303 (n) r , U\u0303 (n) c and U\u0303 (n) e . As we will see in Section 4, there is no actual need to estimate U\u0303 (n) r , U\u0303 (n) c and U\u0303 (n) e . In practice, the EMS learns U\u0303 (n) r , U\u0303 (n) c and U\u0303 (n) e based on the consumer\u2019s evaluations on completed/canceled jobs."}, {"heading": "3.2 Instantaneous Cost Function", "text": "We assume the instantaneous cost function of the consumer at time t has the following form: Pt \u2211\nn\u2208D(t)\nCn + \u03b3U\u0304 (t) (Ht) , (7)\nwhere Pt is the electricity price at time t and U\u0304 is a dissatisfaction function capturing the consumer\u2019s dissatisfaction on job rescheduling at time t. Specifically, notice that \u2211 n\u2208D(t)Cn is the total\nelectricity energy consumed at time t, and hence Pt \u2211\nn\u2208D(t)Cn is the electricity bill the consumer pays at time t. We assume the consumer\u2019s dissatisfaction on rescheduling at time t depends on the \u201crescheduling history\u201d Ht and \u03b3 > 0 represents the tradeoff between the electricity bill paid and the consumer\u2019s dissatisfaction on rescheduling.\nNote that from the EMS\u2019s perspective, both the electricity price and the consumer behavior are exogenous and stochastic; thus, in this paper, we assume that EMS aims to minimize the expected infinite-horizon discounted cost:\nE  \u221e\u2211 t=0 \u03b1t Pt \u2211 n\u2208D(t) Cn + \u03b3U\u0304 (t)(Ht)  , where 0 < \u03b1 < 1 is the discrete-time discount and captures the assumption that a rational consumer generally discounts future costs and benefits. Notice that in this problem formulation, the state at time t is (Pt,Ht\u22121,J (t)), and the action is D(t) (i.e. the EMS chooses which devices to power on).\nUnder Assumption 1, the dissatisfaction function is decomposable and the infinite-horizon discounted cost function can be written as\nN\u2211 n=1\n[ E { \u221e\u2211 t=0 \u03b1t [ PtCn1(n \u2208 D(t)) + \u03b3U\u0304 (t,n) ( H(n)t )]}] . (8)"}, {"heading": "3.3 Device Based MDP Model", "text": "In this subsection, we first propose a dynamic model for the energy price Pt. Then, we motivate an assumption on user behavior (Assumption 3), and show that under the proposed assumptions, the EMS\u2019s decision-making (job scheduling) problem is decomposed over devices. Finally, we formulate the job scheduling problem on each device as a Markov Decision Process (MDP), which we henceforth refer to as the device based MDP model.\nLet us start by proposing a dynamic model for the energy price Pt. As discussed in Section 2, Pt is exogenous in the sense that the EMS\u2019s actions will not influence it. Furthermore, recent literature (e.g. [ONeill et al., 2010]) shows that Pt can be modeled as a Markov process. Thus, throughout this paper, we make the following assumption:\nAssumption 2 Energy price Pt follows an exogenous finite-state ergodic Markov chain.\nWe use P to denote the set of states of this Markov chain, and use |P| to denote the number of states.\nIn this paper, we assume that the user is rational and the user\u2019s dissatisfaction function has the functional form specified by Eqn(3-6). Thus, to complete the description of the user behavior, we only need to model when the user sends requests/cancellations to the EMS. Similarly as Pt, we model the consumer requests/cancellations as stochastic processes and make the following assumption on them throughout this paper:\nAssumption 3 (A) The consumer request/cancellation processes are statistically independent of the energy price process Pt; and (B) consumer requests/cancellations to different devices are statistically independent.\nAssumption 3 is a key assumption in this paper. Thus, we briefly motivate it before proceeding: first, we claim that Assumption 3(A) is reasonable since\n\u2022 On one hand, as we have discussed in Section 2, in the residential and small building sector, the market power of an individual consumer is so small that the impact of their behavior on Pt is negligible.\n\u2022 On the other hand, as we have discussed in Section 1, due to \u201cdecision fatigue\u201d, most rational consumers are not incentivized to reference current and expected future energy prices before sending requests/cancellations to the EMS. Thus, for such consumers, their requests/cancellations are independent of current and expected future energy prices.\nConsequently, it is reasonable to assume Assumption 3(A). We now briefly motivate Assumption 3(B). One observation about devices is that they can be classified into categories such that for most consumers, their requests to devices in different categories are weakly dependent. For instance, in a residential household, we can classify the airconditioner, electric vehicle, laundry machine and dryer into three categories: category 1 includes the air-conditioner; category 2 includes the laundry machine and dryer; and category 3 includes the electric vehicle. Under this classification, it is observed that for most residential consumers, the statistical dependence between their requests to devices in different categories are weak. Thus, it is reasonable to assume that consumer requests/cancellations to different device categories are statistically independent. To simplify, we assume each category has only one device and hence propose Assumption 3(B). Of course, a category might have multiple devices (e.g. laundry machine and dryer), and usually the consumer requests to devices in the same category are statistically dependent. We discuss how to extend the results of this paper to that scenario in Section 6.\nRecall that the EMS\u2019s objective is to find an optimal scheduling policy to minimize (8), which decomposes over devices under Assumption 1. Assumption 3 states that the consumer requests/cancellations to different devices are statistically independent, thus, we have the following proposition:\nProposition 1 Under Assumptions 1 and 3, the EMS\u2019s decision-making problem is decomposed over devices.\nIn the remainder of this paper, we focus on deriving the optimal scheduling policy for a single device and drop the subscript n to simplify the exposition. For example, we will use W instead of Wn to denote the time window and represent a request as J = (\u03c4r, \u03c4g, g). We also use the term \u201csmart\ndevice\u201d and \u201cEMS\u201d interchangably henceforth, since due to the decomposition of the problem, one can think each smart device has its own EMS.\nWe further assume that the timeline for a smart device can be divided into \u201cepisodes\u201d. Specifically, we assume that whenever the smart device completes a job (either initiated by the user or by the EMS) or the current unsatisfied request is canceled by the user, the current episode terminates. In the next time step, the smart device \u201cregenerates\u201d its state according to a fixed distribution \u03c00 and a new episode starts. The notion of episode is illustrated in Figure 3.\nWe now propose an MDP model for a single smart device (the device based MDP model). The state of the MDP at time t is xt = [Pt, st, gt]\nT \u2208 S, where Pt is the exogenous electricity price at time t, st is the elapsed time at time t, gt is the priority of request at time t and S is the state space. Specifically, we define\nst = { t\u2212 \u03c4p if no request received in the current episode t\u2212 \u03c4g otherwise\nwhere \u03c4p is the start time of the current episode and \u03c4g is the target time of the received request. Furthermore, we use gt = 0 to denote that no request has yet been received in the current episode; once a request is received in the current episode, we assume its priority gt \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , gmax}.\nSince the electricity price is exogenous, we can partition the state xt = [Pt, st, gt] T as the \u201cprice portion\u201d Pt and \u201cdevice portion\u201d [st, gt] T . The \u201cdevice portion\u201d of the MDP state transition model is summarized in Figure 4; notice that there are (2W + 1)gmax + W\u0302 + 1 \u201cdevice portion\u201d states. Recall that we use P to denote the set of states of the price Markov chain, so the price Markov chain has |P| states, and the cardinality of the state space for the device based MDP is |S| = |P| [ (2W + 1)gmax + W\u0302 + 1 ] , which is affine in |P|, W , W\u0302 and gmax.\nNote that the action space at each state x \u2208 S is A = {off, on}, where action \u201con\u201d means the smart device completes a job3 at the current time and \u201coff\u201d means the smart device does nothing at the current time. We use \u03a6 (xt, at, xt+1) to denote the instantaneous cost at state-action-state triple (xt, at, xt+1).\nBased on the above discussion, the device based MDP model is detailed below:\n\u2022 Pt follows an exogenous Markov Chain with |P| states. The transition probability from Pt to Pt+1 is denoted as Pr(Pt+1|Pt).\n\u2022 If the smart device has not received a consumer request in the current episode, recall that we set gt = 0 and elapsed time st = t\u2212 \u03c4p. Notice that:\n1. If action \u201coff\u201d is selected, the current cost is \u03a6 (xt, at, xt+1) = 0. Then the smart device receives a consumer request (t + 1, \u03c4g, g) at the next time step (time t + 1) with\n3More specifically, it completes a user-initiated job if a consumer request has been received in the current episode, and completes an EMS-initiated job otherwise.\n(0,0) \u00a0 (1,0) \u00a0 (\u0174,0) \u00a0 (\u0174-\u20101,0) \u00a0\n(-\u2010W,1) \u00a0\n(-\u2010W+1,1) \u00a0\n(W,1) \u00a0\n(-\u2010W,2) \u00a0\n(-\u2010W+1,2) \u00a0\n(W,2) \u00a0\n(-\u2010W,gmax) \u00a0\n(-\u2010W+1,gmax) \u00a0\n(W,gmax) \u00a0\nNo \u00a0request \u00a0received \u00a0\nPriority \u00a01 \u00a0 Priority \u00a02 \u00a0 Priority \u00a0gmax \u00a0\np(0,) \u00a0 P(1,) \u00a0 p(\u0174-\u20101,) \u00a0 p(\u0174,) \u00a0\nnot satisfied this request, recall we set st = t\u2212 \u03c4g, where \u03c4g is the target time of this request. Notice that:\n1. If action \u201coff\u201d is selected, then with probability 1\u2212 p\u0303st,gt , the user will cancel this request at the end of time period t. In this case, the current episode terminates and the smart device regenerates the \u201cdevice portion\u201d state based on distribution \u03c00 at time t+1. The cost associated with this transition is \u03a6 (xt, at, xt+1) = \u03b3U\u0303c(st, gt), where U\u0303c is defined in (4). On the other hand, with probability p\u0303st,gt , the \u201cdevice portion\u201d state transits to\n[st+1 = st + 1, gt+1 = gt] T at time t + 1 and the cost associated with this transition is \u03a6 (xt, at, xt+1) = 0. Notice that we assume p\u0303st,gt = 0 if st = W .\n2. If action \u201con\u201d is selected, the current cost is \u03a6 (xt, at, xt+1) = PtC + \u03b3U\u0303r (st, gt), where U\u0303r (st, gt) is defined in (3) and C is the constant energy consumed by a standardized job. Then the current episode terminates and the smart device regenerates the \u201cdevice portion\u201d state based on distribution \u03c00 at time t+ 1.\nNote that if the transition model of the device based MDP and the dissatisfaction function of the consumer are known, the device based MDP can be solved by dynamic programming (DP). Following ideas in classical DP, it is straightforward to derive the Bellman equation (see Appendix A) from which we can compute the optimal Q-function Q\u2217. Specifically, many DP algorithms, such as value iteration and policy iteration, can be used to compute Q\u2217 (see [Bertsekas, 2005]). We observe that for many DP algorithms, computing Q\u2217 is tractable since the cardinalities of the state space S and action space A in a device based MDP are usually small. Once Q\u2217 is available, one optimal policy \u00b5\u2217 is \u00b5\u2217(xt) \u2208 argmina\u2208AQ\u2217(xt, a)."}, {"heading": "4 Reinforcement Learning and Performance Metrics", "text": ""}, {"heading": "4.1 Reinforcement Learning Formulation", "text": "However, as we discussed in Section 1, in most practical cases, the transition probabilities and the dissatisfaction function of the user are initially unknown. Thus, the EMS must learn how to make optimal decisions for the user based on the incrementally gathered data from the user and the realtime energy price. Reinforcement learning (RL) is a collection of techniques for a decision-maker to learn how to make optimal decisions while interacting with an unknown \u201cenvironment\u201d [Sutton and Barto, 1998]. RL tries to strike a balance between learning (exploration) and optimization (exploitation), and has been extensively used in many other fields, such as artificial intelligence and robotics [Coates et al., 2010]. As has been discussed in [ONeill et al., 2010], it is natural to use RL to solve EMS\u2019s learning problem since EMS needs to learn to make decisions while interacting with the user and the real-time energy price. As is shown in Figure 5, under the RL formulation of the optimal DR problem, the agent is the EMS, and the environment includes both the energy price and consumer behavior. We refer to this RL problem as the device based RL problem.\nAs in the RL literature, we assume the EMS (decision-maker) initially knows the state space S and the action space A = {off, on}. Furthermore, we also assume it knows the discrete-time discount \u03b1, and the price-rescheduling trade-off parameter \u03b3. Note that the second assumption is reasonable since before the user starts to use the smart device, they can manually set up an \u03b1 reflecting their discount on future costs, as well as a \u03b3 indicating their tradeoff between the electricity bill and rescheduling. With experience, the user can also tune these parameters.\nHowever, as is discussed above, in theory the EMS (the decision-maker) needs to learn the transition model and the dissatisfaction function of the user based on its experience. Obviously,\nthe transition model can be learned by directly observing the user behavior and the real-time energy price. We now discuss how the EMS learns the user\u2019s dissatisfaction function. Note that under our proposed device based MDP model, the user\u2019s dissatisfaction function is non-zero only when a job is completed or canceled. Furthermore, notice that as has been discussed in Section 2, once a job is completed/canceled, the EMS will receive an evaluation from the user. In the remainder of this paper, we make the following assumption:\nAssumption 4 The user\u2019s dissatisfaction on the rescheduling of a completed/canceled job is equal to their evaluation on this job.5\nWe now briefly motivate Assumption 4: for a rational user, their evaluation on a completed/canceled job mainly reflects their dissatisfaction on this job, and this dissatisfaction is either due to the (high) energy price or the (undesirable) rescheduling of the job. As we have discussed in Section 1, due to \u201cdecision fatigue\u201d, most rational users will not reference the current energy price or any expected future energy prices before they send their evaluations to the EMS. Thus, their evaluations will mainly reflect their dissatisfactions on job rescheduling. Thus, it is reasonable to assume Assumption 4.\nIn summary, in our proposed device based RL problem, the EMS initially knows the state space S, the action spaceA, the discount \u03b1, the trade-off parameter \u03b3 and the per-job energy consumption C; but it does not know the state transition model or the user\u2019s dissatisfaction function. It observes the state transitions and the user\u2019s evaluations (which are equal to the user\u2019s dissatisfactions on rescheduling under Assumption 4) as it interacts with the user and the real-time energy price. At each time step, it aims to make good decisions based on its initial information, past observations and current state."}, {"heading": "4.2 Baseline and Demand Response Potential", "text": "In order to justify a RL algorithm achieves satisfactory performance, we need to compare its experimental performance with respect to a reasonable baseline. In this subsection, we propose a reasonable baseline and define the notion of the demand response (DR) potential.\nLet\u2019s start by defining some useful notation: we use \u00b5 : S \u00d7A \u2192 [0, 1] to denote a (randomized and stationary) policy of a device based MDP. Specifically, under (randomized) policy \u00b5, at any\n5Assumption 4 is an idealized assumption on the relationship between evaluation and dissatisfaction. In practice, it might be more reasonable to assume that the user\u2019s evaluation on a job is equal to their dissatisfaction on the rescheduling of this job plus a (zero-mean, finite-variance, statistically independent) \u201cbehavioral noise\u201d. However, the \u201cbehavioral noise\u201d does not fundamentally change the assumption and we make Assumption 4 to simplify exposition.\nstate x = [P, s, g]T \u2208 S, action a \u2208 A is chosen with probability \u00b5(x, a). As is classical in DP and RL, we use Q\u00b5 to denote the Q-function of the device based MDP under policy \u00b5 (see [Sutton and Barto, 1998]). Note that under Assumption 2, the energy price Pt follows an exogenous ergodic Markov chain; thus, we use \u03c0P to denote the unique stationary distribution of this price Markov chain. Moreover, recall that once an episode terminates, the \u201cdevice portion\u201d state is regenerated according to distribution \u03c00. Recall that state x = [P, s, g]\nT , thus, we use x \u223c \u03c0P \u00d7 \u03c00 to denote P \u223c \u03c0P , [s, g]T \u223c \u03c00, and P and [s, g]T are statistically independent. We define the performance of a policy \u00b5 as follows:\nV\u00b5 = Ex\u223c\u03c0P\u00d7\u03c00 { Ea\u223c\u00b5(x,\u00b7) [Q\u00b5 (x, a)|x] } . (9)\nSince this paper focuses on how demand response (DR) can potentially reduce a user\u2019s (expected infinite-horizon discounted) cost, we choose the baseline policy as the one without job rescheduling and denote it as \u00b5base. Specifically, under \u00b5base, the EMS never self-initiates a job, and all the jobs initiated by the user are scheduled to be completed at their target times. We are interested in how much a proposed RL algorithm can reduce the user\u2019s cost with respect to V\u00b5base .\nRecall that we use \u00b5\u2217 to denote the optimal policy. We define the demand response (DR) potential as\nDRP = V\u00b5base \u2212 V\u00b5\u2217 . (10)\nBy definition, DR potential is the maximum expected cost reduction that can be achieved by DR. Obviously, in the case when the transition model and the dissatisfaction function are known, \u00b5\u2217 can be derived beforehand and hence DRP is achieved. However, in the practical cases when the EMS needs to learn \u00b5\u2217 through some RL algorithm, DRP is generally not achievable. As we will see, performance loss with respect to DRP is one performance metric for RL algorithms.\nWe define the relative DR potential (RDRP) as\nRDRP = DRP/V\u00b5base = (V\u00b5base \u2212 V\u00b5\u2217) /V\u00b5base . (11)\nOf course, DRP and RDRP depend on the problem instance. We are particularly interested in how DRP and RDRP vary with \u03b3, since \u03b3 specifies the consumer\u2019s tradeoff between job rescheduling and the electricity bill paid. To simplify the expositions of the result, we make the following assumption in the remainder of the paper:\nAssumption 5 (A) Energy price is always strictly positive; (B) the user\u2019s dissatisfaction function satisfies Eqn(3-6) and is always non-negative; (C) if a user\u2019s request is satisfied at its target time, the user\u2019s dissatisfaction is 0; and (D) if the user cancels their request before the target time, their dissatisfaction is 0.\nNote that Assumption 5(C) and 5(D) are reasonable since for a rational user, it is proper to assume their dissatisfaction is minimal if their request is satisfied at the target time, or canceled by them before the target time. Then Assumption 5(B) can be achieved by shifting the dissatisfaction function by a constant. Assumption 5(A) follows from the daily observation. Under Assumption 5, we have the following result:\nTheorem 1 Under Assumption 5, we have that:\n1. V\u00b5base does not depend on \u03b3.\n2. There exists a \u03b3\u2217 > 0 s.t. \u00b5base is an optimal policy when \u03b3 > \u03b3 \u2217. Thus, as \u03b3 \u2192\u221e, DRP\u2192 0.\n3. DRP and RDRP are non-increasing functions of \u03b3.\n4. If \u03b3 = 0, then RDRP = 1.\nPlease refer to Appendix B for the proof of Theorem 1."}, {"heading": "4.3 Performance Metrics", "text": "In this subsection, we propose several performance metrics for a RL algorithm applied to our proposed device based RL problem. We start by defining some notation: for any t = 0, 1, \u00b7 \u00b7 \u00b7 , use \u00b5\u0303t to denote the policy under which the RL algorithm chooses the action at at the beginning of time period t. We define the performance of the RL algorithm as\nV\u0303 = E [ \u221e\u2211 t=0 \u03b1t\u03a6 (xt, \u00b5\u0303t (xt) , xt+1) ] , (12)\nnote that the expectation is not only taken with respect to the initial state (similarly as Eqn(9), we assume x0 = [P0, s0, g0]\nT \u223c \u03c0P \u00d7 \u03c00 in Eqn(12)), but also with respect to the subsequent stochastic transitions, noisy evaluations and possible randomizations in the RL algorithm. Note that by definition, we have V\u00b5\u2217 \u2264 V\u0303 .\nThe first performance metric of an RL algorithm is its relative improvement (RI) with respect to the baseline, which is defined as\nRI = (V\u00b5base \u2212 V\u0303 )/V\u00b5base . (13)\nRI captures the normalized expected cost reduction the user will benefit from an RL algorithm. Note that since V\u00b5\u2217 \u2264 V\u0303 , we have RI \u2264 RDRP. That is, the relative DR potential is an upper bound on the relative improvement.\nThe second performance metric of an RL algorithm is its relative realized DR potential (RRDRP), which is defined as\nRRDRP = RI/RDRP = (V\u00b5base \u2212 V\u0303 )/(V\u00b5base \u2212 V\u00b5\u2217), (14)\nif RDRP > 0. Intuitively, RRDRP captures how much DR potential is realized by an RL algorithm. Since V\u00b5\u2217 \u2264 V\u0303 , we have RRDRP \u2264 1.\nWe briefly comment on RI and RRDRP before proceeding. First, notice that both RI and RRDRP can be negative, since there is no guarantee that V\u0303 \u2264 V\u00b5base . Specifically, under Assumption 5, it is likely that V\u0303 > V\u00b5base when \u03b3 is sufficiently large. This is because V\u00b5base \u2193 V\u00b5\u2217 as \u03b3 \u2192 \u221e, and whatever \u03b3 is, an RL algorithm is likely to incur a non-trivial performance loss before it learns an optimal policy.\nSecond, we observe that RRDRP also serves as an indicator on whether or not it is worthy to explicitly model the user behavior/dissatisfaction and the energy price. Specifically, if DRP is large but RRDRP is small for many widely used RL algorithms, then it might be worthy to explicitly model the user behavior/dissatisfaction and the energy price, as long as the cost of such modeling is smaller than V\u0303 \u2212 V\u00b5\u2217 .\nWe also note that it is important for our proposed RL algorithm to achieve satisfactory performance quickly, since otherwise the consumer will not be incentivized to use it. The third performance metric focuses on this aspect. Specifically, \u2200j = 0, 1, \u00b7 \u00b7 \u00b7 , we use tj to denote the end time\nof episode j.6 We define\nV\u00b5base(j) = E  tj\u2211 t=0 \u03b1t\u03a6 (xt, \u00b5base (xt) , xt+1)  . Thus, V\u00b5base(j) is the user\u2019s expected total discounted cost in the first j + 1 episodes (from episode 0 to episode j) under the baseline policy. Similarly, we define\nV\u0303 (j) = E  tj\u2211 t=0 \u03b1t\u03a6 (xt, \u00b5\u0303t (xt) , xt+1)  , where \u00b5\u0303t is the policy under which the RL algorithm chooses the action at at the beginning of time period t. Similarly, V\u0303 (j) is the user\u2019s expected total discounted cost in the first j+1 episodes under the RL algorithm.\nWe define the third performance metric, the baseline crossing time, as BCT = min { j\u2217 \u2265 0 : V\u0303 (j) \u2264 V\u00b5base(j), \u2200j \u2265 j \u2217 } . (15)\nNote that BCT is measured in the number of episodes instead of the number of time periods. This is because measuring time in episodes is more meaningful in our problem: each episode corresponds to one usage of the device, and hence the number of episodes corresponds to how many times the consumer has used the device. Thus, measuring time in episodes avoids the problem that the consumer might sometimes use the device more frequently, while at other times use it less frequently."}, {"heading": "5 Simulation", "text": "In this section, we first briefly review the classical Q-learning algorithm in Subsection 5.1. Then we discuss a representative simulation example in Subsection 5.2. Finally, the simulation results are demonstrated in Subsection 5.3."}, {"heading": "5.1 The Q-Learning Algorithm", "text": "As we have discussed in Section 4, many RL algorithms can be applied to our proposed device based RL problem. In this subsection, we implement one of the most popular and classical RL algorithms, known as Q-learning [Watkins, 1989]. Q-learning is an off-policy learning algorithm; that is, it allows the learning agent to follow an exploratory policy while learning about an optimal policy. Another desirable features of Q-learning is that it is online, incremental and is easy to implement on real-time data.\nQ-learning works based on temporal-difference learning [Sutton and Barto, 1998]. At each time-step t the learning algorithm receives an input data in the form of (xt, at,\u03a6t, xt+1), where \u03a6t def = \u03a6 (xt, at, xt+1) is the observed instantaneous cost after taking action at from state xt and arriving at state xt+1. Then the Q-learning algorithm updates the action-value function Qt(xt, at) according to\nQt+1(xt, at) := Qt(xt, at) + \u03b2t [ \u03a6t + \u03b1 min\na\u2032\u2208A Qt(xt+1, a\n\u2032)\u2212Qt(xt, at) ] ,\n6Note that tj \u2019s are random variables (more specifically, stopping times). Furthermore, the distribution of tj \u2019s depends on the policy.\nand Qt+1(x, a) := Qt(x, a) if (x, a) 6= (xt, at). Note Qt \u2208 <|S||A| is the state-action value function (Q-function) estimate in time period t, and \u03b2t > 0 denotes step-size in time period t. If the step-size sequence satisfies \u2211+\u221e t=0 \u03b2t = +\u221e and \u2211+\u221e t=0 \u03b2 2 t < +\u221e, then Q-learning is guaranteed to converge to the optimal solution if all states are visited infinitely often (see [Sutton and Barto, 1998]). To complete the description of a Q-learning algorithm, we also need to specify a behavioral policy \u00b5b under which the algorithm chooses actions. The choice of \u00b5b will affect data and thus would help the algorithm to learn an optimal policy faster. One main ingredient of \u00b5b is that is it has to be exploratory policy during the learning process. One of standard, but crude, suggestions for how to select \u00b5b is as follows: for a small 0 < < 1, with probability (henceforth referred to as the \u201cexploration probability\u201d), the algorithm chooses action at from state xt according to a randomized policy, and with probability 1\u2212 , the algorithm chooses at \u2208 argmina\u2208AQt (xt, a), where Qt refers to the state-action value function estimate at time t. Here, we consider a randomized policy in the form of Boltzmann exploration\n\u03c0(at|xt) = exp [\u2212Qt(xt, at)/\u03b7]\u2211 a\u2032\u2208A exp [\u2212Qt(xt, a\u2032)/\u03b7] ,\nwhere \u03b7 > 0 is a tuning parameter and is referred to as the \u201ctemperature\u201d of the Boltzmann exploration. The Q-learning algorithm that we implement in this paper is illustrated in Algorithm 1.\nAlgorithm 1 The Q-Learning Algorithm\n1: Initialize Q0 arbitrarily 2: Repeat for each episode j: 3: Choose a small constant step-size \u03b2j > 0 for each episode 4: for each time period t in episode j do 5: Take action at at state xt according to the behavioral policy \u00b5b 6: Observe the instantaneous cost \u03a6t and new state xt+1 7: Compute the TD error\n\u03b4t := \u03a6t + \u03b1 min a\u2032\u2208A\nQt ( xt+1, a \u2032)\u2212Qt (xt, at) 8: Update\nQt+1(x, a) := { Qt(x, a) + \u03b2j\u03b4t if (x, a) = (xt, at) Qt(x, a) otherwise\n9: end for"}, {"heading": "5.2 Simulation Setup", "text": "In this subsection, we propose a representative example to which we apply the Q-learning algorithm detailed in Algorithm 1. Specifically, in this example, we assume that the exogenous price Markov chain has state space P = {10, 12, 15, 20}, and the consumer requests have two different priorities, \u201chigh\u201d and \u201cnormal\u201d. We set the time windows W = 4 and W\u0302 = 5, the discrete-time discount \u03b1 = 0.9995 and the per job energy consumption C = 1. Thus, there are |S| = 96 states in this example. The dissatisfaction functions U\u0303r, U\u0303c and U\u0303e are illustrated in Figure 6(a), 6(b) and 6(c). Notice that these dissatisfaction functions satisfy the characteristics of the rational consumer preferences discussed in Subsection 3.1, as well as Assumption 5.\nAs to the transition model, we assume that if the smart device has not received a consumer request in the current episode, then under action \u201coff\u201d, it will receive a consumer request in the next time step with probability pst . Notice that pst is chosen to be an increasing function of the elapsed time st (see Figure 6(d)). We further notice that there are (W + 1)gmax = 10 types of consumer requests (with different target times and priorities), for simplicity, we assume these 10 types of requests are equally likely. Furthermore, if the smart device has received a consumer request in the current episode, then under action \u201coff\u201d, the unsatisfied request will be canceled with probability p\u0302st . In this example, we assume the \u201ccancellation probability\u201d p\u0302 only depends on the elapsed time st and is independent of the priority gt. We choose p\u0302st as an increasing function of st (see Figure 6(e)). Finally, we assume that when the smart device regenerates its state, with probability 1, the regenerated \u201cdevice portion\u201d state is [st+1 = 0, gt+1 = 0]\nT . We plot the RDRP of this example as a function of the trade-off parameter \u03b3 (see Figure 7). Since this example satisfies Assumption 5, thus, as Theorem 1 indicates, the RDRP is a non-increasing function of \u03b3. Furthermore, when \u03b3 = 0, RDRP = 1, and RDRP\u2192 0 as \u03b3 \u2192\u221e."}, {"heading": "5.3 Performance", "text": "In this subsection, we present the simulation results of the Q-learning algorithm on the representative example detailed in Subsection 5.2. We start by describing how we implement the Q-learning algorithm. We choose the \u201cexploration probability\u201d = 0.05 and the \u201ctemperature\u201d of the softmin policy \u03b7 = 0.1. For episode j, we choose the step-size \u03b2j = 10 20+j . We initialize the Q-learning algorithm by setting Q0 = 0.\nFor each trade-off parameter \u03b3 = 0, 0.1, 0.2, \u00b7 \u00b7 \u00b7 , we run the Q-learning algorithm for \u2308\n2 1\u2212\u03b1\n\u2309 =\n4, 000 episodes, and repeat the simulation for 200 times. Then we approximate V\u0303 and V\u0303 (j),\n\u2200j = 0, 1, \u00b7 \u00b7 \u00b7 by their corresponding sample means, i.e., by averaging the simulation results in these 200 simulations. Note that both V\u00b5base and V\u00b5base(j), \u2200j = 0, 1, \u00b7 \u00b7 \u00b7 , can be analytically derived based on the Bellman equation under policy \u00b5base. Thus, RI, RRDRP and BCT can be (approximately) computed based on V\u00b5base , V\u00b5base(j) and the sample means of V\u0303 and V\u0303 (j). The simulation results are summarized in Figure 8-10, where we plot the RI, RRDRP and BCT as functions of the trade-off parameter \u03b3. Note that we only plot the simulation results for 0 \u2264 \u03b3 \u2264 4.3, since for \u03b3 \u2265 4.4, we have V\u0303 > V\u00b5base . Consequently, for \u03b3 \u2265 4.4, RI and RRDRP are negative, and BCT =\u221e.\nWe conclude this section by briefly discussing about the simulation results. Notice that Figures 8-10 show that in this representative example, RI and RRDRP are decreasing functions of \u03b3, while BCT is an increasing function of \u03b3. This implies that with all the other parameters fixed, the more the user prefers \u201cno rescheduling\u201d to \u201clow energy price\u201d, the harder for the Q-learning algorithm to (1) achieve a significant improvement compared with the baseline, (2) realize a large fraction of the DR potential, and (3) quickly outperform the baseline policy.\nWe also observe that BCT = 0 for \u03b3 \u2264 2.1. This implies that for such \u03b3\u2019s, by initializing Q0 = 0, the Q-learning algorithm outperforms the baseline policy from episode 0. Furthermore, as \u03b3 \u2191 4.4, V\u0303 \u2191 V\u00b5base and hence BCT\u2192\u221e."}, {"heading": "6 Conclusion", "text": "We have motivated and proposed a novel EMS formulation for the DR problem in the residential and small commercial building sectors, which we refer to as the device based RL problem. Specifically, we have shown that under appropriate assumptions, our proposed EMS formulation does not require a pre-specified disutility function modeling the consumer\u2019s dissatisfaction on job rescheduling, and has a computational complexity that grows linearly with the number of devices. Our new EMS formulation also enables the EMS to self-initiate jobs and allows the users to the initiate more flexible requests.\nWe have also motivated and proposed several performance metrics for RL algorithms applied to the device based RL problem, and demonstrated the simulation results when the classical Qlearning algorithm is applied to a representative example. Simulation results suggest that for a broad range of trade-off parameter \u03b3, the Q-learning algorithm outperforms the baseline policy without any job rescheduling.\nFinally, it is worth pointing out that many assumptions on the device based MDP model discussed in this paper are proposed to simplify the exposition, and we can easily relax some of these assumptions to obtain a more general device based MDP model. In Appendix C, we discuss two relaxations that are of particular interest: stacking request and dependent devices."}, {"heading": "Acknowledgment", "text": "We acknowledge Prof. Benjamin Van Roy for his insightful comments on this paper."}, {"heading": "Appendix A Bellman Equation", "text": "It is worth pointing out that if the transition model of the device based MDP and the dissatisfaction function of the consumer are known, the device based MDP can be solved by dynamic programming (DP). Following ideas in classical DP, in this section, we derive the Bellman equation from which we can compute the optimal Q-function.\nRecall that xt = [Pt, st, gt] T and the action space at each state is A = {off, on}, we have\n\u2022 If the smart device has not received a consumer request in the current episode, we have Q\u2217 (xt, on) = PtC + \u03b3U\u0303e (st) + \u03b1E {\nmin a\u2208A\nQ\u2217 ( [Pt+1, st+1, gt+1] T , a )} ,\nwhere the expectation is over Pt+1, st+1 and gt+1. Specifically, Pt+1 is drawn according to the transition probability of the price Markov chain, and [st+1, gt+1]\nT is drawn according to the \u201cdevice portion\u201d regeneration distribution \u03c00.\nOn the other hand, we have\nQ\u2217 (xt, off) = \u03b1E\n[ 0\u2211\ns\u2032=\u2212W gmax\u2211 g=1 pst,s\u2032,g min a\u2208A Q\u2217 ( [Pt+1, s \u2032, g] T , a ) + (1\u2212 pst) min a\u2208A Q\u2217 ( [Pt+1, s\u0303(t+ 1), 0] T , a )] ,\nwhere the expectation is over Pt+1. Specifically, pst = \u22110 s\u2032=\u2212W \u2211gmax g=1 pst,s\u2032,g is the probability that a consumer request will be received in the next time step, s\u0303(t+ 1) = st + 1 if st < W\u0302 and s\u0303(t+ 1) = W\u0302 if st = W\u0302 , and Pt+1 is drawn according to the transition probability of the price Markov chain.\n\u2022 If the smart device has already received a consumer request in the current episode, we have Q\u2217 (xt, on) = PtC + \u03b3U\u0303r (st, gt) + \u03b1E {\nmin a\u2208A\nQ\u2217 ( [Pt+1, st+1, gt+1] T , a )} ,\nwhere the expectation is over Pt+1, st+1 and gt+1. Similarly, Pt+1 is drawn according to the transition probability of the price Markov chain, and [st+1, gt+1]\nT is drawn according to the \u201cdevice portion\u201d regeneration distribution \u03c00.\nOn the other hand, we have Q\u2217 (xt, off) = \u03b1p\u0303st,gtE [ min a\u2208A Q\u2217 ( [Pt+1, st + 1, gt] T , a )] + (1\u2212 p\u0303st,gt) [ \u03b3U\u0303c (st, gt)\n+ \u03b1E {\nmin a\u2208A\nQ\u2217 ( [Pt+1, st+1, gt+1] T , a )}] .\nNote that p\u0303st,gt = 0 if st = W . The expectation in the first line is over Pt+1, where Pt+1 is drawn according to the transition probability of the price Markov chain. On the other hand, the expectation in the third line is over Pt+1, st+1 and gt+1, where Pt+1 is also drawn according to the transition probability of the price Markov chain, and [st+1, gt+1]\nT is drawn according to the \u201cdevice portion\u201d regeneration distribution \u03c00.\nFrom the classical DP theory, the optimal Q-function Q\u2217 is the unique solution of the above-derived Bellman equation. Furthermore, many DP algorithms, such as value iteration and policy iteration, can be used to compute Q\u2217. We observe that for many DP algorithms, computing Q\u2217 is tractable since the cardinalities of the state space S and action space A in a device based MDP are usually small. Once Q\u2217 is available, one optimal policy \u00b5\u2217 is\n\u00b5\u2217(xt) \u2208 argmin a\u2208A Q\u2217(xt, a)."}, {"heading": "Appendix B Proof for Theorem 1", "text": "Proof for Theorem 1: Note that \u2200\u00b5 : S \u00d7A \u2192 [0, 1], V\u00b5 can be expressed as\nV\u00b5 = A\u00b5 + \u03b3B\u00b5,\nwhere A\u00b5 is the expected infinite-horizon discounted electricity bill, and B\u00b5 is the expected infinitehorizon discounted user dissatisfaction on rescheduling. Furthermore, neither A\u00b5 nor B\u00b5 depends on \u03b3.\nFirst, we prove that V\u00b5base does not depend on \u03b3. Note that under policy \u00b5base, the EMS will never initiate a job; furthermore, any job initiated by the user is either completed at its target time or canceled by the user before its target time. Thus, under Assumption 5, we have B\u00b5base = 0. So we have V\u00b5base = A\u00b5base , which does not depend on \u03b3.\nSecond, we prove that there exists a \u03b3\u2217 > 0 s.t. \u00b5base is an optimal policy when \u03b3 > \u03b3 \u2217. Note that under Assumption 5, \u00b5base, which is a deterministic policy, minimizes B\u00b5, since B\u00b5 \u2265 0 for any \u00b5 and B\u00b5base = 0. Note that there are finite deterministic policies, and we use \u2206B to denote the \u201csecond best\u201d B\u00b5 under deterministic policies\n\u2206B = min deterministic \u00b5:B\u00b5>0 B\u00b5.\nNote that the maximum cost reduction from A\u00b5 is 1 1\u2212\u03b1 (Pmax \u2212 Pmin)C, where Pmax = maxP\u2208P P is the highest energy price and Pmin = minP\u2208P P is the lowest energy price. Thus, if we choose\n\u03b3\u2217 = (Pmax \u2212 Pmin)C\n(1\u2212 \u03b1)\u2206B ,\nthen, \u2200\u03b3 > \u03b3\u2217, \u00b5base outperforms all the deterministic policies (i.e. V\u00b5base \u2264 V\u00b5 for any deterministic \u00b5). Consequently, it is an optimal policy. Thus DRP = 0 for any \u03b3 > \u03b3\u2217. As a result, we have DRP\u2192 0 as \u03b3 \u2192\u221e.\nThird, we prove that DRP is a non-increasing function of \u03b3. To formalize the result, we use V\u00b5(\u03b3) to denote the performance of policy \u00b5 at tradeoff parameter \u03b3. Recall V\u00b5(\u03b3) = A\u00b5 + \u03b3B\u00b5, and B\u00b5 \u2265 0 under Assumption 5, thus V\u00b5(\u03b3) is a non-decreasing function of \u03b3 for any \u00b5. Furthermore, we use \u00b5\u2217(\u03b3) to denote an optimal policy in the problem instance with tradeoff parameter \u03b3. Thus, for any 0 \u2264 \u03b31 \u2264 \u03b32, we have that\nV\u00b5\u2217(\u03b31)(\u03b31) \u2264 V\u00b5\u2217(\u03b32)(\u03b31),\nsince \u00b5\u2217(\u03b31) is an optimal policy in the problem instance with parameter \u03b31, and\nV\u00b5\u2217(\u03b32)(\u03b31) \u2264 V\u00b5\u2217(\u03b32)(\u03b32),\nsince \u00b5\u2217(\u03b32) is a fixed policy and \u03b31 \u2264 \u03b32. Thus, V\u00b5\u2217(\u03b31)(\u03b31) \u2264 V\u00b5\u2217(\u03b32)(\u03b32), that is, V\u00b5\u2217 is a nondecreasing function of \u03b3. Since DRP = V\u00b5base \u2212 V\u00b5\u2217 , and V\u00b5base does not depend on \u03b3, and V\u00b5\u2217 is non-decreasing in \u03b3, thus, DRP is non-increasing in \u03b3. From the definition of RDRP, it is also non-increasing in \u03b3.\nFinally, we prove that RDRP = 1 when \u03b3 = 0. Note when \u03b3 = 0, the EMS does not care about the user\u2019s dis-satisfaction on job rescheduling. Consider a policy \u00b5\u2032 under which\n\u2022 The EMS will never initiate a job.\n\u2022 The EMS ignores all the jobs initiated by the user; it just waits for the user to cancel the requested job.\nObviously, when \u03b3 = 0, we have V\u00b5\u2032 = 0. Thus we have V\u00b5\u2217 \u2264 V\u00b5\u2032 = 0. On the other hand, from Assumption 5, we have V\u00b5\u2217 \u2265 0 (since the energy price is always strictly positive and and the user\u2019s dissatisfaction function is always non-negative). Thus we have V\u00b5\u2217 = 0 and\nRDRP = V\u00b5base \u2212 V\u00b5\u2217\nV\u00b5base = 1.\nQ.E.D."}, {"heading": "Appendix C Possible Extensions", "text": "We discuss some possible extensions of the device based MDP model proposed in the paper. It is worth pointing out that many assumptions on this model are proposed to simplify the exposition. We can easily relax some of these assumptions to obtain a more general device based MDP model. We now discuss two relaxations that are of particular interest: stacking request and dependent devices.\nStacking request\nUnder the current device based MDP model, the smart device will ignore a new user request if it currently has an uncompleted request. If the user wants to replace the existent uncompleted request with a new request, they must cancel the existent request first, and then send the new request to the device.\nHowever, in some cases, the user might want to start a new request while keeping the existent uncompleted request. For instance, a user sent a request with target time 4PM to the AC at 1PM; at 2PM, this request was still uncompleted, and the user wanted to send a new request with target time 5PM to the AC while keeping the old request. Starting a new request without canceling the old request is referred to as initiating a stacking request.\nWe can easily extend our proposed device based MDP model to take care of the stacking request by allowing \u03c4g, a request target time, to be a vector. Specifically, in the extended MDP model, the user can request multiple jobs in a request J = (n, \u03c4r, \u03c4g, g), where \u03c4g is the target time vector and its components denote when the user prefers each of their requested jobs to be completed. Requested device n, request time \u03c4r and priority g are defined the same as in Section II. With the new definition of \u03c4g, a stacking request can be realized as follows: assume the target time vector of the current user request is \u03c4g, and the user wants to stack new jobs with target time vector \u03c4 \u2032 g on it, then they can cancel the existent request first, and then start a new request with target time vector [\u03c4g, \u03c4 \u2032 g] T .\nObviously, allowing \u03c4g to be a vector will make the device based MDP model more complicated; however, it does not fundamentally change the problem and all the existent results still apply after proper modifications. For computational tractability, one should also upper bound the length of the target time vector.\nDependent devices\nIn Assumption 3, we assume that consumer requests/cancellations to different devices are statistically independent. As we have discussed after Assumption 3, it should be relaxed for some devices: specifically, devices can be classified into categories, and requests to devices in the same category (such as laundry machine and dryer) are statistically dependent.\nOne way to overcome this problem is to combine all the devices in the same category as a \u201csuper device\u201d. After such combinations, the requests/cancellations to different \u201csuper devices\u201d can be assumed to be statistically independent. Obviously, allowing \u201csuper devices\u201d will make the device based MDP model more complicated; however, it also does not fundamentally change the problem and all the existent results still apply after proper modifications."}], "references": [{"title": "A survey of utility experience with real time pricing", "author": ["G. Barbose", "C. Goldman", "B. Neenan"], "venue": "Lawrence Berkeley National Laboratory: Lawrence Berkeley National Laboratory,", "citeRegEx": "Barbose et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Barbose et al\\.", "year": 2004}, {"title": "Dynamic Programming and Optimal Control", "author": ["D. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q2005\\E", "shortCiteRegEx": "Bertsekas.", "year": 2005}, {"title": "Dynamic pricing, advanced metering, and demand response in electricity markets", "author": ["S. Borenstein", "M. Jaske", "A. Rosenfeld"], "venue": "UC Berkeley: Center for the Study of Energy Markets,", "citeRegEx": "Borenstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Borenstein et al\\.", "year": 2002}, {"title": "The role of demand response in electric power market design", "author": ["S. Braithwait", "K. Eakin"], "venue": "Edison Electric Institute,", "citeRegEx": "Braithwait and Eakin.,? \\Q2002\\E", "shortCiteRegEx": "Braithwait and Eakin.", "year": 2002}, {"title": "Autonomous helicopter flight using reinforcement learning", "author": ["Adam Coates", "Pieter Abbeel", "Andrew Y. Ng"], "venue": "In Encyclopedia of Machine Learning,", "citeRegEx": "Coates et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2010}, {"title": "Quantifying customer response to dynamic pricing", "author": ["Ahmad Faruqui", "Stephen George"], "venue": "The Electricity Journal,", "citeRegEx": "Faruqui and George.,? \\Q2005\\E", "shortCiteRegEx": "Faruqui and George.", "year": 2005}, {"title": "Residential implementation of critical-peak pricing of electricity", "author": ["Karen Herter"], "venue": "Energy Policy,", "citeRegEx": "Herter.,? \\Q2007\\E", "shortCiteRegEx": "Herter.", "year": 2007}, {"title": "An exploratory analysis of California residential customer response to critical peak pricing of electricity", "author": ["Karen Herter"], "venue": "January 2007b. URL http://www.sciencedirect. com/science/article/B6V2S-4JG5F91-2/2/bb70d54%6082f9f5483829aabeef5279e", "citeRegEx": "Herter.,? \\Q2007\\E", "shortCiteRegEx": "Herter.", "year": 2007}, {"title": "Architecture concepts and technical issues for an open, interoperable automated demand response infrastructure", "author": ["E. Koch", "M.A. Piette"], "venue": "In Grid Interop Forum,", "citeRegEx": "Koch and Piette.,? \\Q2007\\E", "shortCiteRegEx": "Koch and Piette.", "year": 2007}, {"title": "An integrated architecture for demand response communications and control", "author": ["M. LeMay", "R. Nelli", "G. Gross", "C.A. Gunter"], "venue": "in Proc. of the 41st Hawaii International Conference on System Sciences,", "citeRegEx": "LeMay et al\\.,? \\Q2008\\E", "shortCiteRegEx": "LeMay et al\\.", "year": 2008}, {"title": "Residential demand response using reinforcement learning", "author": ["D. ONeill", "M. Levorato", "A.J. Goldsmith", "U. Mitra"], "venue": "In IEEE SmartGridComm,", "citeRegEx": "ONeill et al\\.,? \\Q2010\\E", "shortCiteRegEx": "ONeill et al\\.", "year": 2010}, {"title": "Development and evaluation of fully automated demand response in large facilities", "author": ["M.A. Piette", "O. Sezgen", "D.Watson", "N. Motegi", "C. Shockman", "L. ten Hope"], "venue": "URL http://escholarship.org/uc/item/4r45b9zt", "citeRegEx": "Piette et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Piette et al\\.", "year": 2005}, {"title": "Automated critical peak pricing field tests: 2006 pilot program description and results", "author": ["M.A. Piette", "D.Watson", "N. Motegi", "S. Kiliccote"], "venue": "In LBNL Report 62218,", "citeRegEx": "Piette et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Piette et al\\.", "year": 2007}, {"title": "Industrial power demand response analysis for one-part real-time pricing", "author": ["J.G. Roos", "I.E. Lane"], "venue": "Power Systems, IEEE Transactions on,", "citeRegEx": "Roos and Lane.,? \\Q1998\\E", "shortCiteRegEx": "Roos and Lane.", "year": 1998}, {"title": "Learning from Delayed Rewards", "author": ["C. Watkins"], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "Watkins.,? \\Q1989\\E", "shortCiteRegEx": "Watkins.", "year": 1989}], "referenceMentions": [{"referenceID": 10, "context": "Further many of these decisions have limited financial impact on the consumer [ONeill et al., 2010], and, as a result, many rational consumers in the residential and small commercial building sectors may not be sufficiently incentivized to make these decisions over the long run (known as \u201cdecision fatigue\u201d in [ONeill et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 10, "context": ", 2010], and, as a result, many rational consumers in the residential and small commercial building sectors may not be sufficiently incentivized to make these decisions over the long run (known as \u201cdecision fatigue\u201d in [ONeill et al., 2010]).", "startOffset": 219, "endOffset": 240}, {"referenceID": 10, "context": "Recently, [ONeill et al., 2010] proposed a fully-automated EMS algorithm based on reinforcement learning (RL), which learns how to make optimal decisions for consumers.", "startOffset": 10, "endOffset": 31}, {"referenceID": 10, "context": "In [ONeill et al., 2010], the authors assume these functions have particular mathematical properties, but do not address how these functions might be determined.", "startOffset": 3, "endOffset": 24}, {"referenceID": 10, "context": "In this paper, we propose a novel EMS formulation that addresses the limitations of [ONeill et al., 2010] described above.", "startOffset": 84, "endOffset": 105}, {"referenceID": 10, "context": "In other words, our new RL formulation has eliminated the impractical assumption in [ONeill et al., 2010] that consumers\u2019 dissatisfactions with delay can be captured by known disutility functions.", "startOffset": 84, "endOffset": 105}, {"referenceID": 10, "context": "\u2022 Consumer requests/reservations can have different priorities, whereas in [ONeill et al., 2010], all the consumer requests have the same priority.", "startOffset": 75, "endOffset": 96}, {"referenceID": 10, "context": "[ONeill et al., 2010]) shows that Pt can be modeled as a Markov process.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Specifically, many DP algorithms, such as value iteration and policy iteration, can be used to compute Q\u2217 (see [Bertsekas, 2005]).", "startOffset": 111, "endOffset": 128}, {"referenceID": 4, "context": "RL tries to strike a balance between learning (exploration) and optimization (exploitation), and has been extensively used in many other fields, such as artificial intelligence and robotics [Coates et al., 2010].", "startOffset": 190, "endOffset": 211}, {"referenceID": 10, "context": "As has been discussed in [ONeill et al., 2010], it is natural to use RL to solve EMS\u2019s learning problem since EMS needs to learn to make decisions while interacting with the user and the real-time energy price.", "startOffset": 25, "endOffset": 46}, {"referenceID": 14, "context": "In this subsection, we implement one of the most popular and classical RL algorithms, known as Q-learning [Watkins, 1989].", "startOffset": 106, "endOffset": 121}], "year": 2017, "abstractText": "Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated Energy Management System (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS\u2019s rescheduling problem as a reinforcement learning (RL) problem (referred to as the device based RL problem), and show that this RL problem decomposes over devices under reasonable assumptions. Compared with existent formulations, our new formulation (1) does not require explicitly modeling the user\u2019s dissatisfaction on job rescheduling, (2) enables the EMS to self-initiate jobs, (3) allows the user to initiate more flexible requests and (4) has a computational complexity linear in the number of devices. We also propose several new performance metrics for RL algorithms applied to the device based RL problem, and demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example.", "creator": "LaTeX with hyperref package"}}}