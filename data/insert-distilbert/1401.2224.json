{"id": "1401.2224", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2014", "title": "A Comparative Study of Reservoir Computing for Temporal Signal Processing", "abstract": "reservoir computing ( rc ) is a novel approach to time series prediction using recurrent neural networks. in rc, an input signal perturbs the intrinsic dynamics of a medium called a reservoir. a readout layer is then highly trained to reconstruct a target output from the reservoir's state. the multitude of rc architectures and theoretical evaluation metrics poses a challenge to both practitioners and theorists who study the task - solving performance and computational power of rc. in addition, in contrast to highly traditional computation models, the real reservoir is usually a dynamical system in which computation and memory are inseparable, and therefore hard to analyze. here, we compare echo state networks ( esn ), a popular rc architecture, with tapped - delay lines ( dl ) loops and nonlinear autoregressive exogenous ( narx ) networks, which we use to model systems with limited computation and limited memory respectively. we compare the performance of the three systems while computing three common benchmark time series : h { \\'e } non map, narma10, and narma20. we find that the role of the reservoir in the reservoir computing paradigm goes completely beyond providing a memory of the past component inputs. the product dl and the narx network have higher precision memorization capability, but fall short of the generalization power of the esn.", "histories": [["v1", "Fri, 10 Jan 2014 03:39:28 GMT  (752kb,D)", "http://arxiv.org/abs/1401.2224v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["alireza goudarzi", "peter banda", "matthew r lakin", "christof teuscher", "darko stefanovic"], "accepted": false, "id": "1401.2224"}, "pdf": {"name": "1401.2224.pdf", "metadata": {"source": "CRF", "title": "A Comparative Study of Reservoir Computing for Temporal Signal Processing", "authors": ["Alireza Goudarzi", "Peter Banda", "Matthew R. Lakin", "Christof Teuscher", "Darko Stefanovic"], "emails": ["alirezag@cs.unm.edu."], "sections": [{"heading": null, "text": "Index Terms\u2014Reservoir computing, echo state networks, nonlinear autoregressive networks, time-delayed networks, time series computing\nI. INTRODUCTION Reservoir computing is a recent development in recurrent neural network research with applications to temporal pattern recognition [1]. RC\u2019s performance in time series processing tasks and its flexible implementation has made it an intriguing concept in machine learning and unconventional computing communities [2]\u2013[12]. In this paper, we functionally compare the performance of reservoir computing with linear and nonlinear autoregressive methods for temporal signal processing to develop a baseline for understanding memory and information processing in reservoir computing.\nIn reservoir computing, a high-dimensional dynamical core called a reservoir is perturbed with an external input. The reservoir states are then linearly combined to create the output. The readout parameters can be calculated by performing regression on the state of a teacher-driven reservoir and the expected teacher output. Figure 1 shows a sample RC architecture. Unlike other forms of neural computation, computation in RC takes place within the transient\n1 Department of Computer Science, University of New Mexico, Albuquerque, NM 87131, USA e-mail: alirezag@cs.unm.edu.\n2 Department of Computer Science, Portland State University, Portland, OR 97207, USA.\n3 Department of Electrical and Computer Engineering, Portland State University, Portland, OR 97207, USA.\ndynamics of the reservoir. The computational power of the reservoir is attributed to a short-term memory created by the reservoir [13] and the ability to preserve the temporal information from distinct signals over time [14], [15]. Several studies attributed this property to the dynamical regime of the reservoir and showed it to be optimal when the system operates in the critical dynamical regime\u2014a regime in which perturbations to the system\u2019s trajectory in its phase space neither spread nor die out [15]\u2013[19]. The reason for this observation remains unknown. Maass et al. [14] proved that given the two properties of separation and approximation, a reservoir system is capable of approximating any time series. The separation property ensures that the reservoir perturbations from distinct signals remain distinguishable whereas the approximation property ensures that the output layer can approximate any function of the reservoir states to an arbitrary degree of accuracy. Jaeger [20] proposed that an ideal reservoir needs to have the so-called echo state property (ESP), which means that the reservoir states asymptotically depend on the input and not the initial state of the reservoir. It has also been suggested that the reservoir dynamics acts like a spatiotemporal kernel, projecting the input signal onto a high-dimensional feature space [5], [21]. However, unlike in kernel methods, the reservoir explicitly computes the feature space.\nRC\u2019s robustness to the underlying implementation as well as its efficient training algorithm makes it a suitable choice for time series analysis [22]. However, despite more than a decade of research in RC and many success stories, its wide-spread adoption is still forthcoming for three main reasons: first, the lack of theoretical understanding of RC\u2019s working and its computational power, and second, the absence of a unifying implementation framework and performance analysis results, and thirdly, missing comparison with conventional methods."}, {"heading": "II. OBJECTIVES", "text": "Our main objective is to compare time series computing in the RC paradigm, in which memory and computation are integrated, with two basic time series computing methods: first, a device with perfect memory and no computational power, ordinary linear regression on tapped-delay line (DL); and second, a device with limited memory and arbitrary computational power, a nonlinear autoregressive exogenous (NARX) neural network. This is a first step toward a systematic investigation of topology, memory, computation, and dynamics in RC. In this article we restrict ourselves to ESNs with a fully connected reservoir and input\nar X\niv :1\n40 1.\n22 24\nv1 [\ncs .N\nE ]\n1 0\nJa n\n20 14\nlayer. We study the performance of ESN and autoregressive model on solving three time series problems: computing the 10th order NARMA time series [23], the 20th order NARMA time series [24], and the He\u0301non Map [25]. We also provide performance results using several variations of the mean squared error (MSE) and symmetric mean absolute percentage (SAMP) error to make our results accessible to the broader neural network and time series analysis research communities. Our systematic comparison between the ESN and autoregressive model provides solid evidence that the reservoir in the ESN performs non-trivial computation and is not just a memory device."}, {"heading": "III. A BRIEF SURVEY OF PREVIOUS WORK", "text": "The first conception of the RC paradigm in the recurrent neural network (RNN) community was Jaeger\u2019s echo state network (ESN) [26]. In this early ESN, the reservoir consisted of N fully interconnected sigmoidal nodes. The reservoir connectivity was represented by a weight matrix with elements sampled from a uniform distribution in the interval [\u22121,1]. The weight matrix was then rescaled to have a spectral radius of \u03bb < 1, a sufficient condition for ESP.\nThe input signal was connected to all the reservoir nodes and their weights were randomly assigned from the set {\u22121,1}. Later, Jaeger [20], [27] proposed that the sparsity of the connection weight matrix would improve performance and therefore only 20% of the connections were assigned weights from the set {\u221247,47}.Verstraeten et al. [28] used a 50% sparse reservoir and a normal distribution for the connection weights, and scaled the weight matrix posteriori to ensure the ESP; also, only 10% of the nodes were connected to the input. This study indicated that, contrary to the earlier report by Jaeger [26], the performance of the reservoir was sensitive to the spectral radius and showed optimality for \u03bb \u2248 1.1. Venayagamoorthy and Shishir [29] demonstrated experimentally that the spectral radius also affects training time, but, they did not study spectral radii larger than one. Gallicchio and Micheli [30] provided evidence that the sparsity of the reservoir has a negligible effect on ESN performance, but depending on the\ntask, input weight heterogeneity can significantly improve performance. Bu\u0308sing et al. [18] reported, from private communication with Jaeger, that different reservoir structures, such as the scale-free and the small-world topologies, do not have any significant effect on ESN performance. Song and Feng [31] demonstrated that in ESNs, with complex network reservoirs, high average path length and low clustering coefficient improved the performance. This finding is at odds with what has been observed in complex cortical circuits [32] and other studies of ESN [33].\nRodan and Tino [24] studied an ESN model with a very simple reservoir consisting of nodes that are interconnected in a cycle with homogeneous input weights and homogeneous reservoir weights, and showed that its performance can be made arbitrarily close to that of the classical ESN. This finding addressed for the first time concerns about the practical use of ESNs in embedded systems due to their complexity [2]. Massar and Massar [34] formulated a meanfield approximation to the ESN reservoir and demonstrated that the optimal standard deviation of a normally distributed weight matrix \u03c3w is an inverse power-law of the reservoir size N with exponent \u22120.5. However, this optimality is based on having critical dynamics and not task-solving performance."}, {"heading": "IV. MODELS", "text": "To understand reservoir computation, we compare its behavior with a system with perfect memory and no computational power and a system with limited memory and arbitrary computational power.\nWe choose delay line systems, NARX neural networks, and echo state networks as described below. We use U(t), Y (t), and Y\u0302 (t) to denote the time-dependent input signals, the time dependent output signal, and the time-dependent target signal, respectively."}, {"heading": "A. Delay line", "text": "A tapped-delay line (DL) is a simple system that allows us to access a delayed version of a signal. To compare the computation in a reservoir with the DL, we use a linear readout layer and connect it to read the states of\nthe DL. Figure 2(a) is a schematic for this architecture. Note that this architecture is different from the delay line used in [24] in that the input is only connected to a single unit in the delay line. The delay units do not perform any computation. The system is then fed with a teacher input and the weights are trained using an ordinary linear regression on the teacher output as follows:\nWout = (XT \u00b7X)\u22121 \u00b7XT \u00b7 Y\u0302, (1)\nwhere each row of X represents the state of the DL at a specific time X(t0) and the rows of Y\u0302 are the teacher output at for the corresponding time Y (t0). Note that the DL states are augmented with a bias unit with a constant value b = 1. Initially, all the DL states are set to zero."}, {"heading": "B. NARX Network", "text": "The NARX network is an autoregressive neural network architecture with a tapped delay input and one or more hidden layers. Both hidden layers and the output layer are provided with a bias input with constant value b = 1. We use tanh as the transfer function for the hidden layer and a linear transfer function for the output layer. The network is trained using the Marquardt algorithm [35]. This architecture performs a nonlinear regression on the teacher output using the previous values of the input accessible through the tapped delay line. A schematic of this architecture is given in Figure 2(b). Since we would like to study the effect of regression complexity on the performance, we fix the length of the tapped delay to 10 and vary the number of hidden nodes."}, {"heading": "C. Echo State Network", "text": "In our ESN, the reservoir consists of a fully connected network of N nodes extended with a constant bias node b = 1. The input and the output nodes are connected to all the reservoir nodes. The input weight matrix is an I\u00d7N matrix Win = [wini, j], where I is the number of input nodes and winj,i is the weight of the connection from input node i to reservoir node j. The connection weights inside the reservoir are represented by an N\u00d7N matrix Wres = [wresj,k], where wresj,k is the weight from node k to node j in the reservoir. The output weight matrix is an (N+1(\u00d7O matrix Wout = [woutl,k ], where O is the number of output nodes and\nwoutl,k is the weight of the connection from reservoir node k to output node l. All the weights are samples of i.i.d. random variables from a normal distribution with mean \u00b5w = 0 and standard deviation \u03c3w. We represent the time-varying input signal by an Ith order column vector U(t) = [ui(t)], the reservoir state by an Nth order column vector X(t)= [x j(t)], and the generated output by an Oth order column vector Y(t) = [yl(t)]. We compute the time evolution of each reservoir node in discrete time as:\nx j(t +1) = tanh(Wresj \u00b7X(t)+Win \u00b7U(t)), (2)\nwhere tanh is the nonlinear transfer function of the reservoir nodes and Wresj is the jth row of the reservoir weight matrix. The reservoir output is then given by:\nY(t) = Wout \u00b7X(t). (3)\nWe train the output weights to minimize the squared output error E = ||Y(t)\u2212 Y\u0302(t)||2 given the target output Y\u0302(t). As with DL, the output weights are calculated using the ordinary linear regression given in Equation 1."}, {"heading": "V. EVALUATION METHODOLOGY", "text": "The study of task-solving performance and analysis of computational power in RC is a major challenge because there are a variety of RC architectures, each with a unique set of parameters that potentially affect the performance. The optimal RC parameters are task-dependent and must be adjusted experimentally. Furthermore, not all studies use the same tasks and the same performance metrics to evaluate their results. In addition, in contrast to classical computation models in which a programmed automaton acts on a storage device, RC is a dynamical system in which memory and computing are inseparable parts of a single phenomenon. In other words, in RC the same dynamical process that performs computation also retains the memory of the previous results and inputs. Thus, it is not clear how much of the RC\u2019s performance can be attributed to its memory capacity and how much to its computational power. As a way of approaching this issue, we attempt to create a functional comparison between the ESN, DL, and NARX networks.\nWe choose three temporal tasks for our evaluation: the He\u0301non Map, the NARMA 10 time series, and the NARMA 20 time series. These tasks vary in increasing order in their\n4 time lag dependencies and the number of terms involved and thus let us compare the performance of our systems based on the memory and computational requirements for task solving. We measure the performance using three variations of MSE and a SAMP error measure to allow easy comparison with related work."}, {"heading": "A. Tasks", "text": "1) He\u0301non Map Time Series: This time series is generated by the following system:\nyt = 1\u22121.4y2t\u22121 +0.3yt\u22122 + zt , (4)\nwhere zt is a white noise term with standard deviation 0.001. This is an example of a task that requires limited computation and memory, and can therefore be used as a baseline to evaluate ESN performance.\n2) NARMA 10 Time Series: Nonlinear autoregressive moving average (NARMA) is a discrete-time temporal task with 10th-order time lag. To simplify the notation we use yt to denote y(t). The NARMA 10 time series is given by:\nyt = \u03b1yt\u22121 +\u03b2yt\u22121 n\n\u2211 i=1 yt\u2212i + \u03b3ut\u2212nut\u22121 +\u03b4 , (5)\nwhere n = 10, \u03b1 = 0.3,\u03b2 = 0.05,\u03b3 = 1.5,\u03b4 = 0.1. The input ut is drawn from a uniform distribution in the interval [0,0.5]. This task presents a challenging problem to any computational system because of its nonlinearity and dependence on long time lags. Calculating the task is trivial if one has access to a device capable of algorithmic programming and perfect memory of both the input and the outputs of up to 10 previous time steps. This task is often used to evaluate the memory capacity and computational power of ESN and other recurrent neural networks.\n3) NARMA 20 Time Series: NARMA 20 requires twice the memory and computation compared to NARMA 10 with an additional nonlinearity because of the saturation function tanh. This task is very unstable and the saturation function keeps its values bounded. NARMA 20 time series is given by:\nyt = tanh(\u03b1yt\u22121 +\u03b2yt\u22121 n\n\u2211 i=1 yt\u2212i + \u03b3ut\u2212nut\u22121 +\u03b4 ), (6)\nwhere n = 20, and the rest of the constants are set as in NARMA 10."}, {"heading": "B. Error Calculation", "text": "A challenge in comparing results across different studies is the way each study evaluates its results. In the case of time series analysis, each study may use a different error calculation to measure the performance of the presented methods. We present three different error calculations commonly used in the time series analysis literature. We use y to refer to the time-dependent output and y\u0302 to refer to the target output. The expectation operator \u3008\u00b7\u3009 refers to the time average of its operand.\n1) Root normalized mean squared error: The most commonly used measure is a root normalized mean squared error (RNMSE) calculated as follows:\nRNMSE =\n\u221a \u3008(y\u2212 y\u0302)2\u3009\n\u03c3y\u0302w2 . (7)\nHere \u03c32y\u0302 is the standard deviation of the target output over time. In some studies this calculation is used without taking the square root, in which case it is simply called a normalized mean squared error (NMSE).\n2) Normalized root mean squared error: A variant of the normalized error is the normalized root mean square error (NRMSE), also known as normalized root mean squared deviation (NRMSD). It is calculated as follows:\nNRMSE =\n\u221a \u3008(y\u2212 y\u0302)2\u3009\nmax(y\u0302)\u2212min(y\u0302) . (8)\nIn this variant, the error is normalized by the width of the range covered by the target signal. Both RNMSE and NRMSE attempt to normalize the error between 0 and 1. However, if the distance between the output and the target output is larger than the standard deviation of the target output or its range, they may produce an error value larger than 1.\n3) Symmetric absolute mean percentage error: The symmetric absolute mean percentage (SAMP) error, on the other hand, is guaranteed to produce an error value between 0% and 100%. SAMP is given by:\nSAMP = 100\u00d7 \u2329 |y\u2212 y\u0302| y+ y\u0302 \u232a . (9)\nThroughout the rest of the paper, we use the RNMSE error to produce the plots and make our comparison between the three systems. We have tabulated the results using the other metrics in the Appendix B."}, {"heading": "C. Reservoir optimization", "text": "Depending on the ESN architecture, its performance can be sensitive to some of the model\u2019s parameters. These parameters are optimized using offline cross-validation [24], [36] or online adaptation [19], [37]. This is a preliminary stage before the functional comparison. We are interested in the scaling of these parameters, which we study systematically. Figures 3(a)-3(c) shows the resulting error surface by averaging the result of the 10 runs of each \u03c3w-N combination. We observe that, as the nonlinearity of the task and its required memory increase, ESN performance becomes more sensitive to changes in \u03c3w and favors a more heterogeneous weight assignment (larger \u03c3w). We find the optimal standard deviation \u03c3\u2217w as a function of N for each task:\n\u03c3\u2217w(N) = arg min \u03c3w RNMSE(\u03c3w,N). (10)\nWe found experimentally that \u03c3\u2217w(N) is best fitted by a power-law curve. The bottom row of Figure 3 shows the result of this optimization and the power-law fit. The details of these fits are provided in Appendix A. For NARMA 10\nand NARMA 20 the power law is well behaved, except for the He\u0301non Map which is not sensitive to \u03c3w. We use \u03c3\u2217w = 0.02 for He\u0301non Map experiments. It is noteworthy that this power-law behavior is qualitatively consistent with what we expect from the theoretical result in [34], although the exact power-law coefficient is task-dependent."}, {"heading": "D. Functional Comparison", "text": "The division between memory and computation is not fully understood. Here, we attempt to compare the ESN size to the size of an equivalent device with only memory capacity and no computational power, and to a device with limited memory and theoretically arbitrary computational power. A DL of length n stores the perfect memory of the past n inputs and a NARX network with N hidden units can be a universal approximator of any time series. We would like to compare the performance of a linear readout with access to the reservoir state with a linear readout with access to the delay line states and also to the performance of the NARX network. It is clear that the ESN, the DL, and the NARX network are very different, i.e., the memory and computational power of ESN, DL, and NARX networks differ significantly even with identical N. Therefore, we perform a functional comparison in which we study the ESN, the DL, and the NARX network with equal RNMSE\non the same task. For ESN, we have two parameters: the standard deviation of the normal distribution used to create the weight matrix \u03c3w and the number of nodes N in the reservoir. We chose the \u03c3w that optimizes the performance of ESN for each N as described in Section V-C."}, {"heading": "E. Experimental Setup", "text": "In this section we describe the parameters and data sets used for our simulations. The training and testing for delay line and NARX networks are done by generating 10 time series of 4,000 time steps. We used 20 time seres of the same length for ESNs. We used the first 2,000 time steps of each time series to train the model and the second 2,000 steps to test the model. The training and testing performance metrics are then averaged over the 10 runs. The model specific setting are as follows:\n1) Delay line: A delay line is a fixed system with only one parameter N that defines the number of taps. For this study we limit ourselves to 1 \u2264 N \u2264 2,000. We have also experimented with 2,000\u2264N \u2264 3,000, but the performance of the delay line does not change for N > 2,000. We take N = 2,000 as the largest delay line for this study.\n2) NARX neural network: Since the training in the NARX network is sensitive to the random initial weights, we instantiate a new network for each time series. We fix\nthe number of input taps to 10 and the number of hidden layers to 1. We use the number of nodes in the hidden layer N as the control parameter, with 1\u2264 N \u2264 100.\n3) Echo state network: A single instantiation of ESN contains randomly assigned weights and the reservoir initial states. To average the ESN properties over these variations we instantiate five ESN to train and test for every time series. The error is then averaged over the five instances and then over 20 time series. The variable parameters for ESN are the number of reservoir nodes N and the standard deviation of the normal distribution used to generate the reservoir and the input weights. However, for each N we only use the \u03c3\u2217w as described in Section V-C. We study the ESN with the reservoir size 10\u2264 N \u2264 1,000."}, {"heading": "VI. RESULTS", "text": "Figures 4(a)-4(c) show the training performance measured in RNMSE as a function of the normalized system size. In all three tasks, the delay line shows a sharp decrease in error as soon as the system acquires enough capacity to hold all the required information for the task. This is N = 2 for He\u0301non Map, N = 10 for NARMA 10 task, and N = 20 for NARMA 20 task. After this point, the error decreases slowly until N = 2,000 where RNMSE \u2248 0.14 after a sharp drop. The decrease in error is due to the\n\u201cdimensionality curse\u201d: the fixed-length teacher time series are not representatives of the expanded state space of the delay line. This is expected to result in overfitting, which is reflected in the high testing errors on Figures 4(d)-4(f). Another expected behavior of overfitting to training data is that if the distribution of the data is wide, the error will be larger and for narrower distributions of data the error will be lower. This is why the delay line has the highest error for the simplest task, the He\u0301non Map, and the lowest error for the most difficult task, the NARMA 20 task. Note that to make the testing errors for all the system readable, we use logarithmic y-axes.\nThe NARX network behaves differently on the He\u0301non Map and the two NARMA tasks. For the He\u0301non Map, it shows the best training and testing performance around N = 5 and begins to overfits for N > 5. For both NARMA 10 and NARMA 20, the training error decreases gradually as the number of hidden nodes increases, but the system only memorizes the patterns and cannot generalize, which is characterized by increasing test RNMSE. For the NARMA 10 task, the observed training RNMSE for NARX network is comparable to the error of 0.17 reported by Atiya and Parlos [23] for the same network size N = 40. However, they used only 200 data points, which explains the slightly lower error. Atiya and Parlos focused on the convergence\n7 times of different algorithms and did not publish their testing error.\nFor the ESN, the training error on all three tasks decreases rapidly as the size of the reservoir increases and reaches a plateau for N \u2248 1,000. However, unlike the DL and the NARX network, the testing error also decreases sharply as the reservoir size increases. As expected, this decrease is sharper for easier tasks. The main different of the ESN performance is that the training error increases across all N as the difficulty of the task increases, which is a sign that the readout layer is not merely memorizing the training patterns. We have tabulated the error values for all three systems on all the tasks for a few system sizes, which can be found in the Appendix B. Our ESN testing results are similar to those reported by Rodan and Tino [24] for the same system size.\nNext, we compared the performance of the DL, the NARX network, and the ESN as described in Section V-D. Because the testing error of the systems do not overlap, we have to use the training error to perform a direct functional comparison between the three systems. This allows us to compare their memorization capabilities. Figure 5(a) shows the DL size as a function of the ESN size of identical training error for all three tasks. For the He\u0301non Map and the NARMA 10 tasks, the ESN achieves the same RNMSE as the delay line with significantly fewer nodes. For instance for the He\u0301non Map and NARMA 10, to achieve a training RNMSE of an ESN with 400 nodes, a delay line would need 1,990 and 1,810 nodes respectively. For NARMA 20, the delay line only needs 90 nodes to achieve the same RNMSE as an ESN with 400 nodes. The narrow distribution of the NARMA 20 time series allow the linear delay line to exploit the average case strategy to achieve a lower RNMSE. On the other hand, the ESN readout layer learns the task itself as in contrast to just memorizing patterns. The delay line use the same strategy for the easier tasks as well (NARMA 10 and He\u0301non Map), but ESN can fit to the the training data much better than the delay line and therefore requires much less resources to achieve the same error level.\nFigure 5(b) shows the functional comparison result between NARX networks and ESNs. A NARX network would need 10 hidden nodes to achieve the same RNMSE as a 400-node ESN on the He\u0301non Map task. The short time dependency and the simple form of this task make it very easy for the network to learn the system, which results in low training and testing RNMSE. For the NARMA 10 and the NARMA 20 tasks, the network requires 60 and 50 hidden nodes to be equivalent of the 400-node ESN. The strategy here is similar to the delay line where during learning the network tries to fit the training data on average, as best as it can. As expected this strategy will have two consequences: (1) the NARMA 20 task will be easier because of its distribution; (2) the network overfits the training data and cannot generalize to testing data."}, {"heading": "VII. DISCUSSION AND OPEN PROBLEMS", "text": "The reservoir in an ESN is a dynamical system in which memory and computation are inseparable. To understand this type of information processing we compared the ESN performance with a memory-only device, the DL, and a limited-memory but computationally powerful device, the NARX network. Our results illustrate that the performance of ESN is not only due to its memory capacity; ESN readout does not create an autoregression of the input, such as in the DL or the NARX network. The information processing that takes place inside a reservoir is fundamentally different from other types of neural networks. The exact mechanism of this process remains to be investigated. Studying reservoir computing usually takes place by analyzing the systems performance for task solving with different com-\n8 putational and memory requirements. To understand the details of information processing in a reservoir, we have to understand the effects of the reservoir\u2019s architecture on its fundamental memory and computational capacity. We also have to be able to define the classes of tasks that can be parametrically varied in memory requirement and nonlinearity. Our study reveals that although ESN cannot memorize patterns as well as a memory device or a neural network, it greatly outperforms them in generalizing to novel inputs. Also, increasing reservoir size in ESN improves the performance of generalization, whereas in the DL or the NARX network this will result in increased overfitting leading to poorer generalization. One solution would be to extend the receiver operation characteristic (ROC) and receiver error characteristic (REC) curve methods to decide on the quality of generalization in ESN [38]\u2013[40]. In the neural network community, methods based on pruning, regularization, cross-validation, and information criterion have been used to alleviate the overfitting problem [41]\u2013 [53]. Among these methods, regularization has been successfully used in ESNs [3]. However, these methods focus on increasing the neural network\u2019s performance and are not suitable to quantify overfitting or to study task hardness. Another area that requires more research is the amount of training that the ESN requires to guarantee a certain performance, as is described in probably approximately correct methods [54]\u2013[56]. To the best of our knowledge these problems have not been addressed in the case of highdimensional dynamical systems. A well developed theory of computation in reservoir computing needs to address all of these aspects. In future work, we will study some of these issues experimentally and based on our observations, we will attempt to develop theoretical understanding of\ncomputation in the reservoir computing paradigm."}, {"heading": "VIII. CONCLUSION", "text": "Reservoir computing is an approach to neural network training which has been successful in areas of machine learning, time series analysis, robot control, and sequence learning. There has been many studies aimed at understanding the working of RC and the factors that affect its performance. However, because of the complexity of the reservoir in RC, none of these studies have been completely satisfactory and have often resulted in contradictory conclusions. In this paper, we compared the performance of three approaches to time series analysis: the delay line, the NARX network, and the ESN. These methods vary in their memory capacity and computational power. The delay line retains a perfect memory of the past, but does not have any computational power. The NARX network only has limited memory of the past, but in principle can perform any computation. Finally, the ESN does not have an explicit access to past memory, but its reservoir carries out computation using the implicit memory of the past represented in its dynamics. Using a functional comparison we showed that for simple tasks with short time dependencies, the delay line requires more than four\ntimes as much resources that ESN requires to achieve the same error, while the NARX network requires 40 times less resources than ESN to achieve equivalent error. For tasks with long time dependencies and narrow distributions the delay line requires less than one fourth the resources of the ESN and the NARX network requires less then one fifth the same resources. However, neither a delay line nor a NARX network can achieve the generalization power of an ESN. Many theoretical aspects of reservoir computing, such as the memory-computation trade-off, and the relation between reservoir\u2019s structure, its dynamics, and its performance remain as open problems."}, {"heading": "ACKNOWLEDGMENT", "text": "The work was support by NSF grants #1028238 and #1028120. M.R.L. gratefully acknowledges support from the New Mexico Cancer Nanoscience and Microsystems Training Center."}, {"heading": "APPENDIX A", "text": "FITTING \u03c3w\nBefore we can fit \u03c3\u2217w, we have to interpolate the data points on the error surface with a linear fit. This allows us to use all values of N and \u03c3w and create a smooth fit. Table I shows the goodness of fit statistics for the linear fit to the error surface. Low SSE and high R2 statistics on this fit shows the the surface accurately represent the data points. We then calculate the \u03c3\u2217w corresponding to the\ntask SSE R2 He\u0301non Map 1.5486\u00d710\u221231 1 NARMA 10 2.989\u00d710\u221231 1 NARMA 20 7.3725\u00d710\u221231 1\nTable I GOODNESS OF FIT STATISTICS FOR THE INTERPOLANT FIT TO THE ERROR SURFACE (FIGURE 3) AS A FUNCTION OF RESERVOIR \u03c3w AND N FOR ALL THREE TASKS.\nstandard deviation of the weight matrix for each N that minimizes the error. We represent \u03c3\u2217w as a function of N and fit the power-law axb+c to it. The result of the fit and the goodness of fit statistics are given in Table II."}, {"heading": "APPENDIX B THE PERFORMANCE RESULTS", "text": "Tables III, IV, and V tabulate the average testing and training errors of optimal ESNs, delay lines, and NARX networks for the three different tasks using three different measures, i.e., RNMSE, NRMSE, and SAMP."}], "references": [{"title": "An overview of reservoir computing: theory, applications and implementations", "author": ["B. Schrauwen", "D. Verstraeten", "J.V. Campenhout"], "venue": "Proceedings of the 15th European Symposium on Artificial Neural Networks, 2007, pp. 471\u2013482.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "A comparative study of reservoir computing strategies for monthly time series prediction", "author": ["F. Wyffels", "B. Schrauwen"], "venue": "Neurocomputing, vol. 73, no. 10\u201312, pp. 1958\u20131964, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1958}, {"title": "Adaptive nonlinear system identification with echo state networks", "author": ["H. Jaeger"], "venue": "NIPS, 2002, pp. 593\u2013600.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Reservoir computing approaches to  10 H\u00e9non Map SSE 0.0011 R2 0.5670 a \u22125.733\u00d710\u22128\u00b12.9430\u00d710\u22127 b 1.795\u00b10.7450 c 0.02053\u00b10.0016 NARMA 10 SSE 2.9686\u00d710\u22124 R2 0.9926 a 0.306\u00b10.0095 b \u22120.2609\u00b10.0249 c  \u22120.02537\u00b10.0079 NARMA 20 SSE 5.8581\u00d710\u22124 R2 0.9909 a \u22120.03441\u00b10.0143 b 0.2156\u00b10.0408 c 0.1766\u00b10.0210 Table II GOODNESS OF FIT STATISTICS FOR THE POWER-LAW FIT axb + c TO \u03c3\u2217  w (FIGURE 3) AS A FUNCTION OF RESERVOIR SIZE N FOR ALL THREE TASKS. recurrent neural network training", "author": ["M. Luko\u0161evi\u010dius", "H. Jaeger"], "venue": "Computer Science Review, vol. 3, no. 3, pp. 127\u2013149, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "A theoretical and experimental study of neuromorphic atomic switch networks for reservoir computing", "author": ["H.O. Sillin", "R. Aguilera", "H.-H. Shieh", "A.V. Avizienis", "M. Aono", "A.Z. Stieg", "J.K. Gimzewski"], "venue": "Nanotechnology, vol. 24, no. 38, p. 384004, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamics of coupled cavities for optical reservoir computing", "author": ["M. Fiers", "B. Maes", "P. Bienstman"], "venue": "Proceedings of the 2009 Annual Symposium of the IEEE Photonics Benelux Chapter, S. Beri, P. Tassin, G. Craggs, X. Leijtens, and J. Danckaert, Eds. VUB Press, 2009, pp. 129\u2013132.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Integration of nanoscale memristor synapses in neuromorphic computing architectures", "author": ["G. Indiveri", "B. Linares-Barranco", "R. Legenstein", "G. Deligeorgis", "T. Prodromakis"], "venue": "Nanotechnology, vol. 24, no. 38, p. 384010, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Nano-scale reservoir computing", "author": ["O. Obst", "A. Trinchi", "S.G. Hardin", "M. Chadwick", "I. Cole", "T.H. Muster", "N. Hoschke", "D. Ostry", "D. Price", "K.N. Pham", "T. Wark"], "venue": "Nano Communication Networks, vol. 4, no. 4, pp. 189\u2013196, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Optoelectronic reservoir computing", "author": ["Y. Paquot", "F. Duport", "A. Smerieri", "J. Dambre", "B. Schrauwen", "M. Haelterman", "S. Massar"], "venue": "Scientific Reports, vol. 2, 02 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Pattern recognition in a bucket", "author": ["C. Fernando", "S. Sojakka"], "venue": "Advances in Artificial Life, ser. Lecture Notes in Computer Science, W. Banzhaf, J. Ziegler, T. Christaller, P. Dittrich, and J. Kim, Eds. Springer Berlin Heidelberg, 2003, vol. 2801, pp. 588\u2013597.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "DNA reservoir computing: A novel molecular computing approach", "author": ["A. Goudarzi", "M.R. Lakin", "D. Stefanovic"], "venue": "DNA Computing and Molecular Programming, ser. Lecture Notes in Computer Science, D. Soloveichik and B. Yurke, Eds. Springer International Publishing, 2013, vol. 8141, pp. 76\u201389.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication", "author": ["H. Jaeger", "H. Haas"], "venue": "Science, vol. 304, no. 5667, pp. 78\u201380, 2004.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Real-time computing without stable states: a new framework for neural computation based on perturbations", "author": ["W. Maass", "T. Natschl\u00e4ger", "H. Markram"], "venue": "Neural Computation, vol. 14, no. 11, pp. 2531\u201360, 2002.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Information dynamics and emergent computation in recurrent circuits of spiking neurons", "author": ["T. Natschl\u00e4ger", "W. Maass"], "venue": "Proc. of NIPS 2003, Advances in Neural Information Processing Systems, S. Thrun, L. Saul, and B. Schoelkpf, Eds., vol. 16. Cambridge: MIT Press, 2004, pp. 1255\u20131262.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Real-time computation at the edge of chaos in recurrent neural networks", "author": ["N. Bertschinger", "T. Natschl\u00e4ger"], "venue": "Neural Computation, vol. 16, no. 7, pp. 1413\u20131436, 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Computational capabilities of random automata networks for reservoir computing", "author": ["D. Snyder", "A. Goudarzi", "C. Teuscher"], "venue": "Phys. Rev. E, vol. 87, p. 042808, Apr 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Connectivity, dynamics, and memory in reservoir computing with binary and analog neurons.", "author": ["L. B\u00fcsing", "B. Schrauwen", "R. Legenstein"], "venue": "Neural Computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Initialization and self-organized optimization of recurrent neural network connectivity", "author": ["J. Boedecker", "O. Obst", "N.M. Mayer", "M. Asada"], "venue": "HFSP Journal, vol. 3, no. 5, pp. 340\u2013349, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the \u201c echo state network\u201d approach", "author": ["H. Jaeger"], "venue": "German National Research Center for Information Technology, St. Augustin- Germany, Tech. Rep. GMD Report 159, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Recurrent kernel machines: Computing with infinite echo state networks", "author": ["M. Hermans", "B. Schrauwen"], "venue": "Neural Computation, vol. 24, no. 1, pp. 104\u2013133, 2013/11/22 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Reservoir computing trends", "author": ["M. Luko\u0161evi\u010dius", "H. Jaeger", "B. Schrauwen"], "venue": "KI - K\u00fcnstliche Intelligenz, vol. 26, no. 4, pp. 365\u2013371, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "New results on recurrent network training: unifying the algorithms and accelerating convergence", "author": ["A. Atiya", "A. Parlos"], "venue": "Neural Networks, IEEE Transactions on, vol. 11, no. 3, pp. 697\u2013709, 2000.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Minimum complexity echo state network", "author": ["A. Rodan", "P. Tino"], "venue": "Neural Networks, IEEE Transactions on, vol. 22, no. 1, pp. 131\u2013144, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "A two-dimensional mapping with a strange attractor", "author": ["M. H\u00e9non"], "venue": "Communications in Mathematical Physics, vol. 50, no. 1, pp. 69\u201377, 1976.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1976}, {"title": "The \u201cecho state\u201d approach to analysing and training recurrent neural networks", "author": ["H. Jaeger"], "venue": "St. Augustin: German National Research Center for Information Technology, Tech. Rep. GMD Rep. 148, 2001.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Short term memory in echo state networks", "author": ["\u2014\u2014"], "venue": "GMD- Forschungszentrum Informationstechnik, Tech. Rep. GMD Report 152, 2002.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}, {"title": "An experimental unification of reservoir computing methods", "author": ["M.D.D. Verstraeten", "B. Schrauwen", "D. Stroobandt"], "venue": "Neural Networks, vol. 20, no. 3, pp. 391\u2013403, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Effects of spectral radius and settling time in the performance of echo state networks", "author": ["G.K. Venayagamoorthy", "B. Shishir"], "venue": "Neural Networks, vol. 22, no. 7, pp. 861 \u2013 863, 2009.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Architectural and markovian factors of echo state networks", "author": ["C. Gallicchio", "A. Micheli"], "venue": "Neural Networks, vol. 24, no. 5, pp. 440 \u2013 456, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Effects of connectivity structure of complex echo state network on its prediction performance for nonlinear time series", "author": ["Q. Song", "Z. Feng"], "venue": "Neurocomputing, vol. 73, no. 10\u201312, pp. 2177 \u2013 2185, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Complex brain networks: graph theoretical analysis of structural and functional systems", "author": ["E. Bullmore", "O. Sporns"], "venue": "Nat Rev Neurosci, vol. 10, no. 4, pp. 312\u2013312, 04 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Small world topology of dynamic reservoir for effective solution of memory guided tasks", "author": ["S. Dasgupta", "P. Manoonpong", "F. Woergoetter"], "venue": "Frontiers in Computational Neuroscience, no. 177.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 0}, {"title": "Mean-field theory of echo state networks", "author": ["M. Massar", "S. Massar"], "venue": "Phys. Rev. E, vol. 87, p. 042809, Apr 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Training feedforward networks with the marquardt algorithm", "author": ["M. Hagan", "M.-B. Menhaj"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 6, pp. 989\u2013993, 1994.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1994}, {"title": "Optimization and applications of echo state networks with leaky- integrator neurons", "author": ["H. Jaeger", "M. Luko\u0161evi\u010dius", "D. Popovici", "U. Siewert"], "venue": "Neural Networks, vol. 20, no. 3, pp. 335\u2013352, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Information theoretic self-organised adaptation in reservoirs for temporal memory tasks", "author": ["S. Dasgupta", "F. Wrgtter", "P. Manoonpong"], "venue": "Engineering Applications of Neural Networks, ser. Communications in Computer and Information Science, C. Jayne, S. Yue, and L. Iliadis, Eds. Springer Berlin Heidelberg, 2012, vol. 311, pp. 31\u201340.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Regression error characteristic curves.", "author": ["J. Bi", "K.P. Bennett"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "A comparison of different ROC measures for ordinal regression", "author": ["W. Waegeman", "B. De Baets", "L. Boullart"], "venue": "Proceedings of the 3rd International Workshop on ROC Analysis in Machine Learning., N. Lachiche, C. Ferri, and S. Macskassy, Eds., 2006, pp. 63\u201369.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "ROC analysis in ordinal regression learning", "author": ["\u2014\u2014"], "venue": "Pattern Recogn. Lett., vol. 29, no. 1, pp. 1\u20139, Jan. 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "A generalization error estimate for nonlinear systems", "author": ["J. Larsen"], "venue": " 11 in Neural Networks for Signal Processing, 1992. II., Proceedings of the 1992 IEEE Workshop on, 1992, pp. 29\u201338.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1992}, {"title": "Adaptive regularization in neural network modeling", "author": ["J. Larsen", "C. Svarer", "L. Andersen", "L. Hansen"], "venue": "Neural Networks: Tricks of the Trade, ser. Lecture Notes in Computer Science, G. Orr and K.-R. Mller, Eds. Springer Berlin Heidelberg, 1998, vol. 1524, pp. 113\u2013132.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1998}, {"title": "Design and regularization of neural networks: the optimal use of a validation set", "author": ["J. Larsen", "L. Hansen", "C. Svarer", "M. Ohlsson"], "venue": "Neural Networks for Signal Processing, 1996. VI. Proceedings of the 1996 IEEE Workshop on, 1996, pp. 62\u201371.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1996}, {"title": "Empirical generalization assessment of neural network models", "author": ["J. Larsen", "L. Hansen"], "venue": "Neural Networks for Signal Processing, 1995. V. Proceedings of the 1995 IEEE Workshop on, 1995, pp. 30\u201339.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1995}, {"title": "Generalization performance of regularized neural network models", "author": ["\u2014\u2014"], "venue": "Neural Networks for Signal Processing, 1994. IV. Proceedings of the 1994 IEEE Workshop on, 1994, pp. 42\u201351.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1994}, {"title": "Lessons in neural network training: Overfitting may be harder than expected", "author": ["S. Lawrence", "C.L. Giles", "A.C. Tsoi"], "venue": "In Proceedings of the Fourteenth National Conference on Artificial Intelligence, AAAI-97. AAAI Press, 1997, pp. 540\u2013545.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1997}, {"title": "Network information criterion-determining the number of hidden units for an artificial neural network model", "author": ["N. Murata", "S. Yoshizawa", "S.-I. Amari"], "venue": "Neural Networks, IEEE Transactions on, vol. 5, no. 6, pp. 865\u2013872, 1994.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1994}, {"title": "Prediction risk and architecture selection for neural networks", "author": ["J. Moody"], "venue": "From Statistics to Neural Networks, ser. NATO ASI Series, V. Cherkassky, J. Friedman, and H. Wechsler, Eds. Springer Berlin Heidelberg, 1994, vol. 136, pp. 147\u2013165.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1994}, {"title": "Pruning from adaptive regularization", "author": ["L.K. Hansen", "C.E. Rasmussen"], "venue": "Neural Computation, vol. 6, no. 6, pp. 1223\u20131232, 2013/12/11 1994.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2013}, {"title": "Quantifying multivariate classification performance: the problem of overfitting", "author": ["B.R. Stallard", "J.G. Taylor"], "venue": "pp. 426\u2013436, 1999.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1999}, {"title": "The problem of overfitting", "author": ["D.M. Hawkins"], "venue": "Journal of Chemical Information and Computer Sciences, vol. 44, no. 1, pp. 1\u201312, 2004.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2004}, {"title": "What size net gives valid generalization?", "author": ["E. Baum", "D. Haussler"], "venue": "Neural Computation,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1989}, {"title": "What size network is good for generalization of a specific task of interest?", "author": ["B. Amirikian", "H. Nishimura"], "venue": "Neural Networks,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1994}, {"title": "On the learnability of discrete distributions", "author": ["M. Kearns", "Y. Mansour", "D. Ron", "R. Rubinfeld", "R.E. Schapire", "L. Sellie"], "venue": "Proceedings of the Twenty-sixth Annual ACM Symposium on Theory of Computing, ser. STOC \u201994. New York, NY, USA: ACM, 1994, pp. 273\u2013282.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1994}, {"title": "Quantifying a critical training set size for generalization and overfitting using teacher neural networks", "author": ["R. Lange", "R. M\u00e4nner"], "venue": "ICANN \u201994, M. Marinaro and P. G. Morasso, Eds. Springer London, 1994, pp. 497\u2013500.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1994}, {"title": "A theory of the learnable", "author": ["L. Valiant"], "venue": "Communications of the ACM, vol. 27, no. 11, pp. 1134\u20131142, 1984.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1984}], "referenceMentions": [{"referenceID": 0, "context": "Reservoir computing is a recent development in recurrent neural network research with applications to temporal pattern recognition [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 10, "context": "RC\u2019s performance in time series processing tasks and its flexible implementation has made it an intriguing concept in machine learning and unconventional computing communities [2]\u2013[12].", "startOffset": 180, "endOffset": 184}, {"referenceID": 11, "context": "The computational power of the reservoir is attributed to a short-term memory created by the reservoir [13] and the ability to preserve the temporal information from distinct signals over time [14], [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "The computational power of the reservoir is attributed to a short-term memory created by the reservoir [13] and the ability to preserve the temporal information from distinct signals over time [14], [15].", "startOffset": 193, "endOffset": 197}, {"referenceID": 13, "context": "The computational power of the reservoir is attributed to a short-term memory created by the reservoir [13] and the ability to preserve the temporal information from distinct signals over time [14], [15].", "startOffset": 199, "endOffset": 203}, {"referenceID": 13, "context": "Several studies attributed this property to the dynamical regime of the reservoir and showed it to be optimal when the system operates in the critical dynamical regime\u2014a regime in which perturbations to the system\u2019s trajectory in its phase space neither spread nor die out [15]\u2013[19].", "startOffset": 273, "endOffset": 277}, {"referenceID": 17, "context": "Several studies attributed this property to the dynamical regime of the reservoir and showed it to be optimal when the system operates in the critical dynamical regime\u2014a regime in which perturbations to the system\u2019s trajectory in its phase space neither spread nor die out [15]\u2013[19].", "startOffset": 278, "endOffset": 282}, {"referenceID": 12, "context": "[14] proved that given the two properties of separation and approximation, a reservoir system is capable of approximating any time series.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Jaeger [20] proposed that an ideal reservoir needs to have the so-called echo state property (ESP), which means that the reservoir states asymptotically depend on the input and not the initial state of the reservoir.", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "It has also been suggested that the reservoir dynamics acts like a spatiotemporal kernel, projecting the input signal onto a high-dimensional feature space [5], [21].", "startOffset": 156, "endOffset": 159}, {"referenceID": 19, "context": "It has also been suggested that the reservoir dynamics acts like a spatiotemporal kernel, projecting the input signal onto a high-dimensional feature space [5], [21].", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "RC\u2019s robustness to the underlying implementation as well as its efficient training algorithm makes it a suitable choice for time series analysis [22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "We study the performance of ESN and autoregressive model on solving three time series problems: computing the 10th order NARMA time series [23], the 20th order NARMA time series [24], and the H\u00e9non Map [25].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "We study the performance of ESN and autoregressive model on solving three time series problems: computing the 10th order NARMA time series [23], the 20th order NARMA time series [24], and the H\u00e9non Map [25].", "startOffset": 178, "endOffset": 182}, {"referenceID": 23, "context": "We study the performance of ESN and autoregressive model on solving three time series problems: computing the 10th order NARMA time series [23], the 20th order NARMA time series [24], and the H\u00e9non Map [25].", "startOffset": 202, "endOffset": 206}, {"referenceID": 24, "context": "The first conception of the RC paradigm in the recurrent neural network (RNN) community was Jaeger\u2019s echo state network (ESN) [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 18, "context": "Later, Jaeger [20], [27] proposed that the sparsity of the connection weight matrix would improve performance and therefore only 20% of the connections were assigned weights from the set {\u221247,47}.", "startOffset": 14, "endOffset": 18}, {"referenceID": 25, "context": "Later, Jaeger [20], [27] proposed that the sparsity of the connection weight matrix would improve performance and therefore only 20% of the connections were assigned weights from the set {\u221247,47}.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "[28] used a 50% sparse reservoir and a normal distribution for the connection weights, and scaled the weight matrix posteriori to ensure the ESP; also, only 10% of the nodes were connected to the input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "This study indicated that, contrary to the earlier report by Jaeger [26], the performance of the reservoir was sensitive to the spectral radius and showed optimality for \u03bb \u2248 1.", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "Venayagamoorthy and Shishir [29] demonstrated experimentally that the spectral radius also affects training time, but, they did not study spectral radii larger than one.", "startOffset": 28, "endOffset": 32}, {"referenceID": 28, "context": "Gallicchio and Micheli [30] provided evidence that the sparsity of the reservoir has a negligible effect on ESN performance, but depending on the task, input weight heterogeneity can significantly improve performance.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "[18] reported, from private communication with Jaeger, that different reservoir structures, such as the scale-free and the small-world topologies, do not have any significant effect on ESN performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "Song and Feng [31] demonstrated that in ESNs, with complex network reservoirs, high average path length and low clustering coefficient improved the performance.", "startOffset": 14, "endOffset": 18}, {"referenceID": 30, "context": "This finding is at odds with what has been observed in complex cortical circuits [32] and other studies of ESN [33].", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "This finding is at odds with what has been observed in complex cortical circuits [32] and other studies of ESN [33].", "startOffset": 111, "endOffset": 115}, {"referenceID": 22, "context": "Rodan and Tino [24] studied an ESN model with a very simple reservoir consisting of nodes that are interconnected in a cycle with homogeneous input weights and homogeneous reservoir weights, and showed that its performance can be made arbitrarily close to that of the classical ESN.", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "Massar and Massar [34] formulated a meanfield approximation to the ESN reservoir and demonstrated that the optimal standard deviation of a normally distributed weight matrix \u03c3w is an inverse power-law of the reservoir size N with exponent \u22120.", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "Note that this architecture is different from the delay line used in [24] in that the input is only connected to a single unit in the delay line.", "startOffset": 69, "endOffset": 73}, {"referenceID": 33, "context": "The network is trained using the Marquardt algorithm [35].", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "These parameters are optimized using offline cross-validation [24], [36] or online adaptation [19], [37].", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": "These parameters are optimized using offline cross-validation [24], [36] or online adaptation [19], [37].", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "These parameters are optimized using offline cross-validation [24], [36] or online adaptation [19], [37].", "startOffset": 94, "endOffset": 98}, {"referenceID": 35, "context": "These parameters are optimized using offline cross-validation [24], [36] or online adaptation [19], [37].", "startOffset": 100, "endOffset": 104}, {"referenceID": 32, "context": "It is noteworthy that this power-law behavior is qualitatively consistent with what we expect from the theoretical result in [34], although the exact power-law coefficient is task-dependent.", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "17 reported by Atiya and Parlos [23] for the same network size N = 40.", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "Our ESN testing results are similar to those reported by Rodan and Tino [24] for the same system size.", "startOffset": 72, "endOffset": 76}, {"referenceID": 36, "context": "One solution would be to extend the receiver operation characteristic (ROC) and receiver error characteristic (REC) curve methods to decide on the quality of generalization in ESN [38]\u2013[40].", "startOffset": 180, "endOffset": 184}, {"referenceID": 38, "context": "One solution would be to extend the receiver operation characteristic (ROC) and receiver error characteristic (REC) curve methods to decide on the quality of generalization in ESN [38]\u2013[40].", "startOffset": 185, "endOffset": 189}, {"referenceID": 39, "context": "In the neural network community, methods based on pruning, regularization, cross-validation, and information criterion have been used to alleviate the overfitting problem [41]\u2013 [53].", "startOffset": 171, "endOffset": 175}, {"referenceID": 51, "context": "In the neural network community, methods based on pruning, regularization, cross-validation, and information criterion have been used to alleviate the overfitting problem [41]\u2013 [53].", "startOffset": 177, "endOffset": 181}, {"referenceID": 1, "context": "Among these methods, regularization has been successfully used in ESNs [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 52, "context": "Another area that requires more research is the amount of training that the ESN requires to guarantee a certain performance, as is described in probably approximately correct methods [54]\u2013[56].", "startOffset": 183, "endOffset": 187}, {"referenceID": 54, "context": "Another area that requires more research is the amount of training that the ESN requires to guarantee a certain performance, as is described in probably approximately correct methods [54]\u2013[56].", "startOffset": 188, "endOffset": 192}], "year": 2014, "abstractText": "Reservoir computing (RC) is a novel approach to time series prediction using recurrent neural networks. In RC, an input signal perturbs the intrinsic dynamics of a medium called a reservoir. A readout layer is then trained to reconstruct a target output from the reservoir\u2019s state. The multitude of RC architectures and evaluation metrics poses a challenge to both practitioners and theorists who study the task-solving performance and computational power of RC. In addition, in contrast to traditional computation models, the reservoir is a dynamical system in which computation and memory are inseparable, and therefore hard to analyze. Here, we compare echo state networks (ESN), a popular RC architecture, with tapped-delay lines (DL) and nonlinear autoregressive exogenous (NARX) networks, which we use to model systems with limited computation and limited memory respectively. We compare the performance of the three systems while computing three common benchmark time series: H\u00e9non Map, NARMA10, and NARMA20. We find that the role of the reservoir in the reservoir computing paradigm goes beyond providing a memory of the past inputs. The DL and the NARX network have higher memorization capability, but fall short of the generalization power of the ESN.", "creator": "LaTeX with hyperref package"}}}