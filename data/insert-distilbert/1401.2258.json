{"id": "1401.2258", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2014", "title": "Assessing Wikipedia-Based Cross-Language Retrieval Models", "abstract": "this first work compares concept models for cross - language retrieval : one first, we adapt probabilistic latent semantic analysis ( plsa ) for multilingual documents. experiments with different weighting schemes show that a weighting method favoring documents of similar length in both language sides gives best results. considering that both monolingual and multilingual latent dirichlet allocation ( lda ) behave alike when applied for such documents, we use a training corpus built on encyclopedia wikipedia where all documents are length - normalized and obtain improvements over previously reported scores for lda. another focus of our work is on model combination. for this end we include explicit semantic analysis ( esa ) in the experiments. we observe that esa is not competitive with lda in a query based retrieval task on clef 2000 data. the combination of machine translation with concept models increased performance by 21. 1 % map in comparison to machine translation alone. machine translation relies on parallel corpora, which may not be available for many language pairs. we further explore in how then much cross - lingual information can be carried over by a specific information source in wikipedia, namely linked text. the best results are essentially obtained using a language modeling approach, entirely without information from parallel corpora. the need for smoothing raises interesting questions on soundness and efficiency. link replacement models capture only a certain kind of information and suggest weighting schemes to emphasize particular words. for a combined model, another interesting question is therefore how to integrate different candidate weighting schemes. using a very pretty simple combination estimation scheme, we obtain results that compare favorably to previously reported results on the clef 2000 dataset.", "histories": [["v1", "Fri, 10 Jan 2014 08:50:54 GMT  (736kb)", "http://arxiv.org/abs/1401.2258v1", "74 pages; MSc thesis at Saarland University"]], "COMMENTS": "74 pages; MSc thesis at Saarland University", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["benjamin roth"], "accepted": false, "id": "1401.2258"}, "pdf": {"name": "1401.2258.pdf", "metadata": {"source": "CRF", "title": "Assessing Wikipedia-Based Cross-Language Retrieval Models", "authors": ["Dietrich Klakow", "Benjamin Roth", "Saeedeh Momtazi", "Grzegorz Chrupala", "Michael Wiegand"], "emails": ["beroth@CoLi.Uni-SB.de"], "sections": [{"heading": null, "text": "Universita\u0308t des Saarlandes Fachrichtung 4.7 Allgemeine Linguistik Computerlinguistik"}, {"heading": "Assessing Wikipedia-Based", "text": ""}, {"heading": "Cross-Language Retrieval Models", "text": ""}, {"heading": "Submitted for the degree of M.Sc.", "text": ""}, {"heading": "Supervisor: Dietrich Klakow", "text": "Benjamin Roth\nberoth@CoLi.Uni-SB.de\nSaarbru\u0308cken, 14 November 2009\nEidesstattliche Erkla\u0308rung\nHiermit erkla\u0308re ich, dass ich die vorliegende Arbeit selbsta\u0308ndig verfasst und keine\nanderen als die angegebenen Hilfsmittel und Quellen verwendet habe.\nSaarbru\u0308cken, den 14.11.2009\nBenjamin Roth\nDanksagung\nAn erster Stelle gilt mein Dank Herrn Prof. Dietrich Klakow als hilfreichem Betreuer, auf den ich immer za\u0308hlen konnte, und der es stets verstand, eine offene Atmospha\u0308re zu schaffen und zur Diskussion gestellten Ideen durch genaue und kritische Fragen zu konkreter Gestalt zu verhelfen. Von dieser Atmospha\u0308re war auch die gesamte Question-Answering-Gruppe getragen, aus ihrer Hilfsbereitschaft und Erfahrung zog ich reichlichen Gewinn. Ihren Mitgliedern, Saeedeh Momtazi, Grzegorz Chrupala, Michael Wiegand und Fang Xu, danke ich fu\u0308r ihre Anteilnahme an meinen Arbeitsfortschritten und den Gedankenaustausch in den Sitzungen. Dankbar bin ich besonders Matt Lease, mit dem ich einige Monate das Bu\u0308ro teilen durfte und dessen Begeisterung fu\u0308r Information Retrieval mich anspornte, manchmal sogar beim gemeinsamen Joggen. Last not least bedanke ich mich herzlich bei Yu Chen und Andreas Eisele: Sie haben mir durch ihre Hilfe bei den maschinellen U\u0308bersetzungen viel Zeit gespart."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Related Work and Ressources 4", "text": "2.1 Retrieval Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.1.1 Vector Space Retrieval . . . . . . . . . . . . . . . . . . . . . . . . 4 2.1.2 Language Modeling Retrieval . . . . . . . . . . . . . . . . . . . . 4 2.1.3 Latent Variable Models . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Using Cross-language Knowledge Sources . . . . . . . . . . . . . . . . . 6\n2.2.1 Comparable Corpora . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2.2 Parallel Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10"}, {"heading": "3 Models and Theory 12", "text": "3.1 Probabilistic Latent Semantic Analysis . . . . . . . . . . . . . . . . . . . 13\n3.1.1 Probabilistic Model . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.1.2 Theoretical Considerations . . . . . . . . . . . . . . . . . . . . . 17 3.1.3 Practical Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.1.4 Multilingual pLSA . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.2 Latent Dirichlet Allocation . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.2.1 Probabilistic Model . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.2.2 Sampling for LDA . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.2.3 Practical Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.2.4 Multilingual LDA . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.3 Explicit Semantic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.3.1 Formalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.3.2 Multilingual ESA . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3.3.3 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31"}, {"heading": "4 Experiments 33", "text": "4.1 Preliminary Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n4.1.1 Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . 34 4.1.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 4.1.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n1"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "4.2 Mate Retrieval on Multext JOC . . . . . . . . . . . . . . . . . . . . . . . 42 4.3 Query-Based Retrieval with CLEF2000 . . . . . . . . . . . . . . . . . . . 46\n4.3.1 Experiment Setup and Results . . . . . . . . . . . . . . . . . . . 46 4.3.2 Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49"}, {"heading": "5 Towards Retrieval Based on Only Wikipedia 51", "text": "5.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 5.2 Vector Space Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 55 5.3 Language Modelling Experiments . . . . . . . . . . . . . . . . . . . . . . 56\n5.3.1 Model Combination on Query Level . . . . . . . . . . . . . . . . 57 5.3.2 Model Combination on Word Level . . . . . . . . . . . . . . . . . 60\n5.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60"}, {"heading": "6 Conclusion 61", "text": "References 63\n2"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "1 Introduction", "text": "The task of cross-language information retrieval is to find documents relevant to a query, where query and documents do not belong to the same language. While many systems just translate either queries or documents and then perform monolingual retrieval, better performing or less ressource intensive approaches might integrate translation knowledge as an integral part of the retrieval model. Our focus will lie on integrating knowledge from the multilingual, freely available on-line encyclopedia Wikipedia1 as a cross-language bridge for retrieval.\nDimensionality reduction techniques have traditionally been of interest for information retrieval as a means of mitigating the word mismatch problem. Cross-language information retrieval can be viewed as an extreme case of word mismatch. More general than dimensionality reduction, the term concept model is used to denote a mapping from the word space to another representation. Such a representation may, for example, be obtained by matrix approximation [Deerwester et al., 1990], by probabilistic inference [Steyvers and Griffiths, 2007] or by techniques making use of the conceptual structure of corpora such as Wikipedia [Gabrilovich and Markovitch, 2007].\nWhile some work has been done on multilingual concept modeling [Dumais et al., 1997, Ni et al., 2009, Mimno et al., 2009, Potthast et al., 2008, Sorg and Cimiano, 2008, 2009], most often the focus is on one method and a comparison with other methods is missing or a particularly simple instantiation is used as a base-line. One reason for this might be that concept models require adaptations for multilinguality that do not seem to be easily implemented. We will show that the adaptations can in fact be minimal and on the data side only. Another question that has not been investigated so far is how different multilingual concept models interact with each other and how they can be combined with bag-of-word models, an approach that is standard for the monolingual case.\nThe subsequent parts of this thesis are structured as follows: In chapter 2 we give a broad survey over retrieval methods and knowledge sources used for cross-language retrieval. In chapter 3 we discuss three concept models included in our experiments and especially focus on their multilingual adaptations. In chapter 4 we report experiments on three corpora. We show that for one of our adaptations much better results can be observed than reported so far. In chapter 5 we explore whether machine translation\n1http://wikipedia.org/\n3"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "output can altogether be replaced by information from Wikipedia and show encouraging results. In chapter 6 we summarize our findings and point to promising directions for further research."}, {"heading": "2 Related Work and Ressources", "text": ""}, {"heading": "2.1 Retrieval Methods", "text": ""}, {"heading": "2.1.1 Vector Space Retrieval", "text": "Classical methods of IR follow the vector space model: documents and queries are vectors while similarity is measured by a distance funtion, most often by taking the cosine between their angles. The most simple models of that kind work directly on bag-ofword vectors and are prone to sparseness and missing word overlap, some backing-off can be achieved by dimensionality reduction techniques such as Latent Semantic Analysis [Deerwester et al., 1990] or term expansion as in the generalized vector space model [Wong et al., 1985]. In practice, many different features associated with documents can be expressed as vectors to form the basis of retrieval.\nWhile vector space retrieval models are set in a simple and precise mathematical theory, which make them immediately accessible to many other tasks such as document clustering or classification, the connection with the actual retrieval process is not very clearly represented, especially after applying term weighting functions like the empirically well-working tf.idf scheme [Salton and Buckley, 1987]. Similar starting points and theoretical properties exhibit tuned ranking functions like the popular Okapi-BM25 family [Robertson et al., 1995], which are still achieving state-of-the-art results [Armstrong et al., 2009]."}, {"heading": "2.1.2 Language Modeling Retrieval", "text": "Language model based retrieval is a newer paradigm [Ponte and Croft, 1998], where usually documents are treated as providing probabilistic models of generating the queries, and the document that provides the best model is regarded as most relevant. Language model retrieval systems are among the most popular today, as they model the retrieval process in a statistically sound and consistent way and, even in their most simple forms, exhibit performance comparable to or even better than fine\n4"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "tuned and tweaked vector space models. See [Liu and Croft, 2004] for an excellent overview. Usually the assumption is made that the probabilities of the query words are independent of each other conditioned on the model provided by the current document. The probabilistic models obtained from the documents D are in the simplest case the maximum likelihood estimates of the unigram word probabilities P (w|D). These estimates are zero for unseen words, which makes the probability of the query\nP (Q) = \ufffd\nw\u2208Q\nP (w|D)n(w,Q)\nzero if one query word has no evidence in the document.\nTwo different strategies are usually applied to overcome this problem. The first is to interpolate the document estimates with a naturally smoother distribution estimated from a big document collection [Hiemstra, 1998, Miller et al., 1999, Song and Croft, 1999]. The second is to smooth the estimates of either distribution by one of the many techniques originally developed for speech recognition [Chen and Goodman, 1999, Zhai and Lafferty, 2004]."}, {"heading": "2.1.3 Latent Variable Models", "text": "Latent variable modeling lies somewhere between vector space and language modeling retrieval: the underlying principle is to fit a probabilistic model to word\u2013document counts, word and documents are assumed to be conditionally independent on a set (of chosen size) of latent topics. Since such models perform a sort of dimensionality reduction (for every document or query they store only the distribution of the topics) parallels can be drawn to LSA. On the other hand, in their very nature they are probabilistic, even parts can be identified that correspond directly to generative language models. Indeed, both using the topic distributions (or other parameter vectors) directly for distance comparison (as in [Hofmann, 1999, 2001, Blei et al., 2003]) as well as using them as components in a generative model (as in [Azzopardi et al., 2004, Wei and Croft, 2006]) are common practices. The two most prominent latent variable models are probabilistic Latent Semantic Analysis [Hofmann, 2001], which implements the ideas of variable modeling in a straightforward way but does not define inference on documents not present in the training collection, and, developed some years later, Latent Dirichlet Allocation [Blei et al., 2003], which defines a fully generative model\n5"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "and is also favorable in terms of scalability [Wang et al., 2009]. Latent variable models applied for cross-language retrieval play a central role in this thesis. Probabilistic Latent Semantic Analysis and Latent Dirichlet Allocation are described in more detail in chapters 3.1 and 3.2 respectively."}, {"heading": "2.2 Using Cross-language Knowledge Sources", "text": "Knowledge sources to provide a connection between two or more languages can be manually edited lexica, parallel corpora, probabilistic lexica extracted from them and comparable corpora. Parallel corpora, such as the Europarl corpus of EU-Parliament proceedings [Koehn, 2005], provide translations of the same text in different languages, corresponding phrase pairs can be extracted automatically [Koehn et al., 2003]. Slightly different are comparable corpora, such as Wikipedia. They contain texts that have counterparts in other languages with which they are linked by dealing with the same topic. Here, it cannot in general be assumed to find corresponding phrase pairs, and it is an interesting research question how much this kind of information can contribute to effective retrieval. Also, comparable corpora are much easier to acquire, cheaper and available in more languages than parallel corpora."}, {"heading": "2.2.1 Comparable Corpora", "text": "Comparable corpora provide a promising knowledge source for information retrieval. As comparable corpora just provide connections on text level, the information provided by them should be appropriately captured by dimensionality reduction techniques aiming at modeling and smoothing document-term statistics. Indeed, dimensionality reduction for cross-language retrieval has been dealt with in several theoretical frameworks and is still a field of ongoing research. We will now present work relevant to using comparable corpora and outline strengths and weaknesses of them as well as possible extensions and modifications. The first approach [Dumais et al., 1997] to using comparable corpora and dimensionality reduction was done with Latent Semantic Analysis (LSA), a technique making use of lower-rank matrix approximation by singular value decomposition. As an effect, terms and documents are represented by lower-dimensional vectors, making them topically comparable [Deerwester et al., 1990]. For cross-lingual LSA, parallel or comparable documents are embedded and the weighted sum of embedded term vectors\n6"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "represents queries and documents on either side. While the method works well and is still used as a baseline [Cimiano et al., 2009], it lacks, as does monolingual LSA, a probabilistic interpretation and is hardly scalable, which becomes a major problem when it comes to embedding gigabyte-sized corpora such as Wikipedia. A more modern approach could be based on probabilistic Latent Semantic Analysis (pLSA Hofmann [2001]). The underlying assumption in this kind of model is that words and documents are independent conditioned on an unobserved class variable Z that can take on K different values zk \u2208 {z1 \u00b7 \u00b7 \u00b7 zK}. Therefore, in this model the following factorization of the joint probablity of a word wi occurring in document dj is possible:\nP (wi, dj) = \ufffd\nzk\nP (wi|zk)P (wj |zk)P (zk)\nAnother, equivalent factorization is\nP (wi, dj) = P (dj) \ufffd\nzk\nP (wi|zk)P (zk|dj)\nAn instance of the EM-Algorithm is used to estimate an optimal factorization. The result can be used in a probabilistic fashion or by taking the cosine of the distribution vectors p(Z|dj). No work is known to us that applies pLSA to embed multilingual documents for IR. In chapter 3.1.4 we present an adaptation of pLSA for cross-lingual IR that includes a language variable L taking on a language value ls and quantifying the amount of text in a particular language of a parallel document. Drawbacks of this method are theoretic concerns that the model is not fully generative and does therefore not include the possibility of estimating unseen documents (although ad-hoc techniques such as folding-in are used for that in practice). Another concern is that to us no large-scale implementations are known that showed positive scalability properties of pLSA to gigabyte sized collections like Wikipedia. Latent Dirichlet Allocation (LDA) overcomes some of the theoretical and practical limitations of LSA and pLSA. Still the likelihood of document\u2013word statistics is maximized, but the documents themselves (not only the terms within them) are assumed to be generated by a probabilistic process. This model allows to overcome overfitting to some degree and defines inference on unseen data in a theoretically sound way. Secondly, approximate inference by sampling is possible, making LDA scalable without\n7"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "losing considerable accuracy. Because of these properties LDA has gained remarkable popularity and many optimized implementations are available. LDA experiments for cross-lingual IR have been published [Cimiano et al., 2009] with disappointing outcomes compared to other techniques. These results do not convincingly show the inappropriateness of LDA, as the authors have embedded multilingual Wikipedia articles without any length normalization or model adaptation. For a method that tries to model characteristics of term statistics, the biggest increase in data-likelihood with limited model capacities should be expected by effectively capturing the predominant language vocabulary instead of doing topical modeling. An indicator that this effect might indeed be at work is that in another run of experiments, trained with in-domain data that have almost constant length ratio (because they are parallel), LDA performs as well as the other methods. Dealing with the supposed length ratio problem could be done either by processing the data accordingly or by adapting the model in a similar fashion as suggested by Ni et al. [2009] for web-page classification (see also [Mimno et al., 2009]). In chapter 3.2.4 we describe in more detail how LDA can be applied in a multilingual setting. A recently proposed similarity measure that is based on simple ideas and also uses wikipedia as a training corpus is Explicit Semantic Analysis (ESA) [Gabrilovich and Markovitch, 2007, Potthast et al., 2008, Sorg and Cimiano, 2009]). It can be applied in monolingual or cross-lingual settings. The main idea is that terms are represented as weighted vectors of documents in a training corpus. Most often, the column vectors of a tf.idf weighted document-term matrix constructed from the training corpus are used for that. Similarity between words is measured as a distance between such weight vectors. Documents and queries are represented by combining word vectors, e.g. by summing them up. The evaluation results on the method are inconsistent: while it has been found to work well on mate-retrieval tasks in a multilingual setting and is reported to surpass LDA there [Cimiano et al., 2009], in an actual query-based document retrieval setting it showed no competitive performance [Sorg and Cimiano, 2008]. These findings may be explained in different ways: for the first finding there might have been problems in how LDA was applied and compared, for the second finding, no interpolation with a word-vector based representation was undertaken, as it is usually done for dimensionality reduction techniques such as LSA and pLSA [Hofmann, 2001]. In contrast to LDA, which started off from a mathematical formulation, ESA\n8"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "was developed from plausibility considerations and tuned in experimental settings, which might render optimal parametrizations instable in different experiments. Some characteristics make ESA an interesting method to be included in our experiments: Conceptual simplicity, scalability, being a novel approach and being trained on a comparable corpus by default. In chapter 3.3 we discuss monolingual and multilingual ESA."}, {"heading": "2.2.2 Parallel Corpora", "text": "Besides beeing comparable, parallel corpora add the possibility of automatically extracting one-to-one correspondences between phrases and words in texts of different languages. In the following, possibilities of additionally including such more finegrained information are discussed. In [Bader and Chew, 2008] the fact is used that SVD of a matrix X can be computed by the Eigenvalue Decomposition of the composite matrix\nB =\n\ufffd\n0 X XT 0\n\ufffd\nThe authors include weighted word\u2013word alignment counts in the upper left block of the matrix (in case of a parallel document, the document vector may contain non-zero entries in term dimensions of more than one langauge). They evaluate their system on the performance of retrieving translations of Bible verses and show a modest but statistically significant improvement on just performing LSA without term alignments. A coherent conceptual interpretation of the overall embedded matrix is hard to give, the weighting scheme is found experimentally. Cohn and Hofmann [2001] use pLSA to include as an additional source of information links to a document (from another document). The two distributions that the model captures are a conventional term-document model as well as a model of inlinks cl and documents:\nP (wi, dj) = P (dj)\nK \ufffd\nk=1\nP (wi|zk)P (zk|dj) (1)\nP (cl, dj) = P (dj) K \ufffd\nk=1\nP (cl|zk)P (zk|dj) (2)\nThe two factorizations are connected via the topic distribution conditioned on a document, p(Z|dj). The overall likelihood function indicates how well the two factor-\n9"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "izations explain both kinds of (normalized) count data, weighted against each other by a factor \u03b1. In a similar fashion, word-translation links could be interpolated with word-document counts. However, we see two problems in such an approach: First, interpretability of the model parts would not be fully consistent for the different \u201contological\u201d status a word has as a translation from a lexicon or occurring in a document. Secondly, to us it is not evident that the EM-re-estimation equations in Cohn and Hofmann [2001], while looking reasonable at a first glance, really derive from the initial problem statement. From cursory calculations, without making additional non-trivial assumptions, complexity should increase from quadratic to cubic by adding an additional dimension of included information this way. An interesting alternative based on LDA has been proposed recently by Boyd-Graber and Blei [2009]. Here, multingual pairings of words are extracted from parallel or comparable corpora in an unsupervised way and assigned to a topic. To increase the quality of the pairs, pairing priors derived from e.g. edit-distances or manually edited lexica can be included. In spite of being theoretically interesting, the authors make reservations with respect to scalability issues and evaluate only on a very small training and testing basis with inconclusive results."}, {"heading": "2.3 Evaluation", "text": "Ideally, base-line systems for comparison in our experiments should be conceptually simple, widely used and trained on the same data. Evaluation tasks should be constructed in a way accepted by the research community and provide reproducible results that allow for direct comparison. General competitions like the Cross-Language Evaluation Forum (CLEF, [Peters and Braschler, 2001]) as well as datasets allowing for more specific comparisons are to be taken into consideration. Two evaluation scenarios are possible and common in the research community. The first are mate retrieval experiments, performed on parallel or comparable document collections with the task of finding the corresponding counterpart of a document in another language. The second scenario consists in classical retrieval settings with queries in one language and relevance-assessed documents in another. The advantage of a mate-retrieval approach to evaluation is that there are many such corpora available having been used for evaluation, starting with the Bible (as in [Chew and Abdelali, 2007, Bader and Chew, 2008]), Wikipedia (as in [Potthast\n10"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "et al., 2008]), translated news collections and parliament debates (as in [Boyd-Graber and Blei, 2009]), the Official Journal of the European Community and european law texts (as in [Potthast et al., 2008, Cimiano et al., 2009, Sorg and Cimiano, 2009, 2008]). Arguably, such settings also show a certain proximity to document clustering and recommendation (see also [Ni et al., 2009, Wu and Oard, 2008, Olsson et al., 2005]). On the other hand, it is rather an unrealistic setting to appear in real word information retrieval scenarios and a maybe too easy task as well. Also, to see that there is a lot of evaluation done on such corpora does not mean that the results are comparable. In fact, no comparable mate-retrieval results of two different research groups are known to us, unclear preprocessing and selection strategies even complicate the reproduction of experiments and results. A nicer setup is provided by IR challenges such as CLEF. Here, the evaluation is implemented in clear and replicable ways with hundreds of thousands of documents categorized by a two-stage process of pooling and human relevance-assessment. Comparison can be made immediately with state-of-the-art methods (although such an evaluation may possibly turn out unrewarding for experimental or novel approaches). Also, there are usually only a few, often just about 50 queries, making evaluation unstable to effects like missing vocabulary items. For our experiments, we evaluate on two datasets: To establish comparability with the findings on LDA and ESA in [Cimiano et al., 2009] and [Sorg and Cimiano, 2009], an evaluation on the Multext JOC corpus is undertaken. To establish comparability to generally applied retrieval methods and give a more realistic picture of performance in real world tasks an evaluation on a CLEF collection is carried out. One of the most accessible of these collections is the CLEF ad-hoc dataset from the year 2000. For the ease of processing and sanity checks, the experiments are done on the language pair German-English.\n11"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "3 Models and Theory", "text": "The models described in this chapter are all able to represent documents in vector representations that are not direct bag-of-words vectors of the vocabulary terms, and may differ from them drastically in dimensionality or other qualities. Because these representations can be made directly comparable for more than one language, such models are interesting for cross-language retrieval, especially when the training of them requires less strict properties of the training material than are required for machine translation. For all models introduced in this section comparable corpora are sufficient, while all common statistical machine translation systems need parallel training corpora.\nIn monolingual retrieval such models are usually justified by the existence two problems: The first is synonymy, the case that two words bear the same meaning while having different surface form, possibly leading to an underestimation of the similarity of two texts. The second is polysemy, possibly leading to overestimation of the similarity of documents in which the same word is used with different meanings.\nWe include three such models, two probabilistic ones, probabilistic Latent Semantic Analysis and Latent Dirichlet Allocation, and one explicit concept model based on weighting schemes (Explicit Semantic Analysis). Such techniques are often referred to under the term of dimensionality reduction. While for the probabilistic techniques the parameter space in which the documents are represented is in practice indeed of lower dimensionality than the vocabulary, for Explicit Semantic Analysis the new space might be even of bigger dimensionality than the original vocabulary space. Still, the vector representation is smoother than the original one based on words, providing a gradual similarity measure for non-matching terms. In that it is not so different from pseudo-relevance feedback techniques since the connection between terms is established via strongly associated (or highly ranked) documents.\nWhile the models are applicable to any collection of so-called two-mode date (discrete two-dimensional co-occurrence counts), we will consistently use a terminology that immediately draws the connection to modeling of text collections.\n12"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "3.1 Probabilistic Latent Semantic Analysis", "text": "Probabilistic Latent Semantic Analysis (pLSA) [Hofmann, 1999, 2001] is a latent variable model that starts with a straightforward model statement and is trained by maximizing the likelihood of the parameters for a training corpus. The most important assumption in pLSA is that the probability distribution of words in a document is only dependent on the distribution of topics in that document. This assumption is the same in more recently developed models. For several reasons we want to discuss pLSA: It is the first latent variable model widely applied to document clustering and retrieval. Its basic architecture was influential for its successors. It is easily understandable and derivable. It can be easily adapted and implemented. We will discuss to the shortcomings of pLSA in sections 3.1.2 and 3.2."}, {"heading": "3.1.1 Probabilistic Model", "text": "The pLSAmodel starts with a collection ofN documents over a vocabulary ofM words. The document collection is represented by co-occurrence counts n(di, wj), indicating how often document di contains word wj . Given such a document collection, the aim of pLSA is to find a probability distribution P (D,W ) (of document and word random variables that can take on values di and wj respectively) that maximizes the likelihood of n(d(\u00b7), w(\u00b7)). While stated in that way P (D,W ) would turn out just to be the relative frequencies, the additional assumption is made that wi and dj are independent conditioned on the value of a topic variable Z that can take on K different values which are denoted by zk.\nFrom this it follows that the joint probability can be factored, making use of the\nindependence assumption in the last step:\nP (di, wj) = P (di)P (wj |di)\n= P (di)\nK \ufffd\nk=1\nP (wj |zk, di)P (zk|di)\n= P (di)\nK \ufffd\nk=1\nP (wj |zk)P (zk|di)\nThis factorization is sometimes called asymmetric because the probability distributions involving words and documents are conditioned adversely with respect to the\n13"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "topic variable. The asymmetric factorization can also be explained as a generative process of the three steps:\n1. select a document di\n2. select a latent class zk|di\n3. generate a word wi|zk\nOne might call this process not fully generative since a document is picked from a preexisting set and not generated by a random process. The model is determined by KN + KM parameters, which is (assuming k \u226a M,N) considerably less than MN as in the case without the conditional independence assumption. The variable Z functions as bottleneck establishing the interdependence between documents and words. From the equivalent symmetric factorization given in equation 4 some parallels to Latent Semantic Analysis [Deerwester et al., 1990], based on truncated singular value decomposition, can be drawn: the values P (di|\u00b7) and P (wj |\u00b7) play a role similar to that of the left and right singular vectors: in both cases co-occurrence matrices can be constructed from them for each of the assumed topics; P (zk) corresponds to singular values by weighting the topical co-occurrence matrices.\nP (di, wj) =\nK \ufffd\nk=1\nP (di, wj |zk)P (zk) (3)\n= K \ufffd\nk=1\nP (di|zk)P (wj |zk)P (zk) (4)\n14"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "When the model is stated, the next step is to find a suitable parametrization, that is values for all p(di), P (wj |zk) and P (zk|di). Often one works with the asymmetric factorization because it characterizes the documents by their topic distribution. Parameters are estimated according to the maximum likelihood principle, so that those parameters are sought which give rise to the highest probability of the observed data. The likelihood function becomes:\nL =\nN,M \ufffd\ni=1,j=1\nP (di, wj) n(di,wj)\nFor ease of calculation, optimization is done on the log of this function, which gives\nL\u2217 =\nN,M \ufffd\ni=1,j=1\nn(di, wj) log p(di, wj)\n=\nN,M \ufffd\ni=1\nn(di) log P (di) +\nN,M \ufffd\ni=1,j=1\nn(di, wj) log\nK \ufffd\nk=1\nP (wj |zk)P (zk|di)\nwhere n(di) = \ufffdM j=1 n(di, wj) is the document length. Taking the derivative with respect to P (di) and setting to zero yields P (di) \u221d n(di). The other parameters cannot be determined analytically because they involve the variable Z for which no values can be observed from the data. For problems of this type the expectation-maximization (EM) algorithm offers a solution (see page 16 for a general introduction). In our case the steps of the algorithm become:\n1. E-step: Calculate P (Z|D,W ) .\n2. M-step: Find new P (W |Z) and P (Z|D) - as mentioned before, P (D) \u221d n(D)\ncan be estimated independently.\nThe expected complete data likelihood for pLSA is given by\nE [Lc] = const +\nN,M,K \ufffd\ni=1,j=1,k=1\nP (zk|di, wj) n(di, wj) log [P (wj |zk)P (zk|di)]\nwhere const is a quantity only dependent on observable counts. For standard pLSA, we refer to [Hofmann, 2001] for a full derivation. See section 3.1.4 for our multilingual adaptation of this model. The estimate of the hidden variable, given the values of the observed variables, can be\n15"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "The EM Algorithm [Dempster et al., 1977, Bilmes, 1998] aims at finding a maximumlikelihood solution for problems involving observable and non-observable (hidden) variables. Denote by X values the observable data have taken on and by Z one possible assignmet of values to the hidden variables (we discuss discrete variables only). A parametrization of a joint probability disbution over all variables is denoted by \u0398. We denote by P (X,Z|\u0398) the complete data likelihood, a function of \u0398 for a fixed assignment to both observed and unobserved variables. The straighforward optimization goal with respect to the observed data, obtained by marginalizing over all possible assignments to the hidden variables\n\u0398\u0302 = argmax \u0398 log\n\ufffd\n\ufffd\nZ\nP (X,Z|\u0398)\n\ufffd\n(5)\nis often analytically intractable. EM is motivated by the observation that the complete data log-likelihood\nLc = logP (X,Z|\u0398)\nis often easy to optimize. However, the assignment of Z is unknown. Therefore, the expectation of this value with respect to Z is used, while Z is taken to be distributed according to a previous parameter estimate \u0398i\u22121. The key finding for EM is that the iterative optimization for \u0398i of the expected complete-data log-likelihood\nE[Lc|X,\u0398i\u22121] = \ufffd\nZ\nP (Z|\u0398i\u22121) log P (X,Z|\u0398i) (6)\nnever decreases the observable data-likelihood [Dempster et al., 1977, Wu, 1983]. Often E[Lc] or Q(\u0398i,\u0398i\u22121) is written short for E[Lc|X,\u0398i\u22121]. Note, that in formula (6) the summation appears outside the logarithm which makes it analytically more appealing than formula (5). The algorithm can then be split up into two alternating steps:\n1. E-step: Estimate how often the hidden variables take on certain values. Use the\nlast parameters \u0398i\u22121.\n2. M-step: Find new parameters \u0398i that maximize the expected log-likelihood of\nboth observed and hidden variables, using the estimates of the last step.\nIn practice, the parameters are often initialized with random values.\n16"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "read off immediately from the original problem statement, making use of the independence assumption and Bayes\u2019 formula:\nP (zk|di, wj) = P (wj |zk)P (zk|di)\n\ufffdK k=1 P (wj |zk)P (zk|di, wj)\nThe optimal parameters are found by taking the derivative of the complete data likelihood function E [Lc] with respect to each parameter and setting to zero. Because the maximizing parameters have to form probability distributions, appropriate Lagrange multipliers have to be added for normalization. The resulting re-estimation equations are:\np(wj |zk) =\n\ufffdN i=1 n(di, wj)P (zk|di, wj)\n\ufffdN,M i=1,m=1 n(di, wm)P (zk|di, wm)\nP (zk|di) =\n\ufffdM j=1 n(di, wj)P (zk|di, wj)\nn(di)"}, {"heading": "3.1.2 Theoretical Considerations", "text": "Let us call a parametrization \u0398 (it comprises all the values found for p(di), P (wj |zk) and P (zk|di)) and the data D (it is the statistics for the documents in our training corpus). For the maximium likelihood estimate, we look for a \u0398\u0302 maximizing P (D|\u0398). This is called the maximum a posteriori estimate with an uninformative (uniform) prior p(\u0398) in Bayesian statistics:\n\u0398\u0302 = argmax \u0398 p(D|\u0398)p(\u0398)\nUsing this estimate has several disadvantages. The main criticism from a Bayesian point of view is that a particular choice of \u0398\u0302 might maximize the distribution over the parameters p(\u0398|D) but does not account for our uncertainty regarding this choice2 and entirely excludes other parametrizations that might be similarly reasonable but happen to have received a little less evidence from the data. The more practical argument is therefore that maximum likelihood estimates tend to overfit. In a fully Bayesian approach the probability of the data should account for different possible parametrizations and also include a non-uniform prior distribution p(\u0398|\u03bb) over the\n2it holds that p(D,\u0398) \u221d p(D,\u0398) p(D) = p(\u0398|D), since p(D) is unaffected by different \u0398\n17"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "model parameters, where all the intuitions about the problem are captured in a set of hyper-parameters \u03bb. We want to contrast the maximum likelihood (formula 7) and the Bayesian (formula 9) approach, both assigning a probability estimate to some new data items Dnew, given training data Dold. In both cases the assumption is made that the probability of the data is only dependent on the parametrization.\np(Dnew|Dold) = P (Dnew| argmax \u0398 p(Dold|\u0398)) (7)\n= P (Dnew|\u0398\u0302) (8)\np(Dnew|Dold, \u03bb) =\n\ufffd\nP (Dnew|Dold,\u0398, \u03bb)d\u0398 (9)\n=\n\ufffd\nP (Dnew|\u0398)p(\u0398|Dold, \u03bb)d\u0398 (10)\nMany problems naturally disappear when a Bayesian approach is undertaken, for example the problem that with pLSA p(d) distributes its probability mass only about documents present in the training corpus. Other problems come up, for example how to find a suitable \u03bb and how to find a solution for the parameters in a tractable way. In chapter 3.2 we will discuss a model that quite successfully offers a solution to most of these problems."}, {"heading": "3.1.3 Practical Issues", "text": "In the EM-algorithm the following equations have to be repeatedly evaluated:\np(zk|di, wj) = p(wj |zk)p(zk|di)\n\ufffdK k=1 p(wj |zk)p(zk|di)\np(wj |zk) =\n\ufffdN i=1 n(di, wj)p(zk|di, wj)\n\ufffdN,M i=1,m=1 n(di, wm)p(zk|di, wm)\np(zk|di) =\n\ufffdM j=1 n(di, wj)p(zk|di, wj)\nn(di)\nIn a naive implementation this would lead to space and time requirements ofO(NMK)\nbecause of the first of the three formulas. However, because p(zk|di, wj) is multiplied only with corresponding counts n(di, wj) only values have to be evaluated\n18"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "where these counts are non-zero. The complexity is therefore only O(fK), with f = \ufffdN,M\ni=1,m=1 1n(di,wm)>0. We have adapted matlabs vector multiplication for this\nend so that it evaluates the outcome only at specified positions of the result matrix.\nThe pLSA model has been reported to get stuck in local optima and to overfit. Two strategies have been applied to alleviate these issues. The first is tempering, an annealing method that tries to increase the entropy of the posterior distribution p(Z,D,W ) by taking it to the power of a decreasing 0 < \u03b2 \u2264 1 in the re-estimation equations (details can be found in [Hofmann et al., 1999, Ueda and Nakano, 1998]). Another often applied technique is to interpolate the parameters of different runs. For information retrieval, the pLSA model is usually interpolated with a weighted wordvector model."}, {"heading": "3.1.4 Multilingual pLSA", "text": "In this section we introduce our own pLSA model that is designed to capture the topical composition of multilingual documents. We want to assume a document that consists of parts in different languages which need not be equally long. For every word it is observable to which language part it belongs. In order to model multilinguality we introduce an additional variable L that can take on S different values indicating a particular language. We now have two bottleneck variables, Z and L, of which one is observable, the other not. We carry over the independence assumptions made for Z to the case with two such variables: The language and the topic that generate a word are independent conditioned on the document. Words and documents are independent conditioned on language and topic. The generative process is then:\n1. select a document di\n19"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "2. select a latent class zk|di\n3. select a language ls|di\n4. generate a word wj |zk, ls\nWe still have only one latent variable Z since we can observe the language variable.\nThe joint probability of a word wj occurring in a document di can be written as:\np(di, wj) = p(di)p(wj |di)\n= p(di) S \ufffd\ns=1\nK \ufffd\nk=1\np(wj |zk, ls, di)p(zk, ls|di)\nUsing the independence assumption and the fact that we can observe the language of a word, that means p(wj |zk, ls) = 0 for all languages ls other than the language s(wj) of the word considered, we get:\np(di, wj) = p(di, s(wj))\nK \ufffd\nk=1\np(wj |zk, s(wj))p(zk|di)\nFrom Bayes\u2019 law and using the knowledge about p(wj|zk, ls) as above, we get:\np(zk|di, wj) = p(wj |s(wj), zk)p(zk|di) \ufffd\nk\u2032 p(wj |s(wj), zk\u2032)p(zk\u2032 |di)\nSimilarly to the monolingual case one immediately gets p(di, ls) \u221d \ufffd\n{wj |s(wj)=ls} n(di, wj)\nby taking the derivative and setting to zero. For the parameters involving the latent variable one derives EM with the complete data likelihood (omitting the constant dependent on observable counts):\nE [Lc] =\nN,M,K \ufffd\ni=1,j=1,k=1\nn(di, wj)p(zk|di, wj) log [p(wj |zk, s(wj))p(zk|di)]\nAdding Lagrange multipliers to ensure that probabilities add up to one:\n20"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "H =\nN,M \ufffd\ni=1,j=1\nn(di, wj)\nK \ufffd\nk=1\np(zk|di, wj) log p(wj|zk, s(wj))\n+\nN,M \ufffd\ni=1,j=1\nn(di, wj)\nK \ufffd\nk=1\np(zk|di, wj) log p(zk|di)\n+\nK,S \ufffd\nk=1,s=1\n\u03c4k,s(1\u2212 \ufffd\n{wj |s(wj)=ls}\np(wj |zk, s(wj)))\n+ N \ufffd\ni=1\n\u03c1i(1\u2212 \ufffd\nzk\np(zk|di))\nAfter taking the derivative with respect to p(wj|zk, s(wj)) and setting to zero, one\ngets:\np(wj|zk, s(wj)) =\n\ufffdN i=1 n(di, wj)p(zk|di, wj)\n\u03c4k,s(wj)\nInserting this for p(wj|zk, s(wj)) we can optimize for the Lagrange multipliers \u03c4k,l,\nand get:\n\u03b4\n\u03b4\u03c4k,l H =\n\u03b4\n\u03b4\u03c4k,l\n\ufffd\ni,j\nn(di, wj) \ufffd\nk\np(zk|di, wj) ln\n\ufffd\ni n(di, wj)p(zk|di, wj)\n\u03c4l,s(wj)\n+ \u00b7 \u00b7 \u00b7+ \ufffd\nl,k\n\u03c4l,k\n 1\u2212 \ufffd\n{wj |s(wj)=ls}\n\ufffd\ni n(di, wj)p(zk|di, wj)\n\u03c4l,k\n\n\n= \u03b4\n\u03b4\u03c4l,k \u03c4l,k \u2212\n\ufffd\ni,{wj |s(wj)=ls}\nn(di, wj)p(zk|di, wj) ln \u03c4l,k\n= 1\u2212\n\ufffd\ni,{wj |s(wj)=ls} n(di, wj)p(zk|di, wj)\n\u03c4l,k = 0\nIn the first step, we omitted all summands that do not contain \u03c4k,l. Finally the values for \u03c4k,l can be reinserted. An analogous calculation for \u03c1i gives us all necessary re-estimation equations:\n21\nWikipedia-Based Cross-Language Retrieval Models Benjamin Roth\np(wj|zk, s(wj)) =\n\ufffdN i=1 n(di, wj)p(zk|di, wj)\n\ufffdN i,{wj |s(wj)=ls} n(di, wj)p(zk|di, wj)\np(zk|di) =\n\ufffd\nj n(di, wj)p(zk|di, wj)\nn(di)\nOur model can work on documents in any number of different languages, with arbitrary language proportions, including monolingual documents in a multilingual setting. Strict separation of content and language variables avoid estimation of the language by the topic variables, so that a unified comparison of the semantic content of multilingual documents is possible. In section 4.1 we apply this model in a retrieval task on multilingual Wikipedia documents.\n22"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "3.2 Latent Dirichlet Allocation", "text": ""}, {"heading": "3.2.1 Probabilistic Model", "text": "Latent Dirichlet Allocation (LDA) [Blei et al., 2003, Griffiths and Steyvers, 2004, Steyvers and Griffiths, 2007] is a latent variable model that gives a fully generative account for documents in a training corpus and for unseen documents. Each document is characterized by a topic distribution, words are emitted according to an emission probability dependent on a topic. The main difference to pLSA is that both topic distributions and word emission distributions are assumed to be generated by Dirichlet priors. A Dirichlet distribution with T parameters Dir(\u03b11 \u00b7 \u00b7 \u00b7\u03b1T ) (short Dir(\u03b1)) assigns a probability to a T -dimensional multinomial distribution Mult(p1 \u00b7 \u00b7 \u00b7 pT ) giving the density:\n\u0393( \ufffd\nj \u03b1j) \ufffd\nj \u0393(\u03b1j)\nT \ufffd\nj=1\np \u03b1j\u22121 j\nwhere \u0393 is the gamma function. The parameters of the Dirichlet distribution determine two properties of the multinomials drawn from it: first, the expected values pi are proportional to the paramters \u03b1i; secondly, the expected variances of the pi decrease when the sum of the \u03b1\u2019s gets bigger. When using the Dirichlet as a prior, it is common to assign the same value \u03b1 = \u03b11 = \u00b7 \u00b7 \u00b7 = \u03b1T to all parameters and basically to direct only the \u201cpeakiness\u201d of the multinomials drawn from it: values for \u03b1 bigger than 1 favor uniform multinomials (the mode of the Dirichlet is for a uniform multinomial), values smaller than 1 punish uniform multinomials (the Dirichlet has modes at multinomials that assign all probability mass to one event).\nThe LDA model describes the process of generating text in the following way:\n1. For all k topics generate multinomial distributions \u03c8(zk) = p(wj|zk) \u223c Dir(\u03b2).\n2. For every document d:\na) Generate a multinomial distribution \u03b8(d) = p(zk|d) \u223c Dir(\u03b1). b) Generate a document length, and topics zi \u223c \u03b8 (d) for every position i in the\ndocument.\nc) Generate words wi \u223c \u03c8 (zi) for every position in the document.\nUsually, no generative account for the length of the document is given, because it has no impact on the other parts of the model one is interested in. Figure 3 shows the\n23"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "generative model in plate notation."}, {"heading": "3.2.2 Sampling for LDA", "text": "The first approach to estimate such a model [Blei et al., 2003] was to represent and estimate \u03c8 and \u03b8 explicitly, resulting in different inference tasks to be solved and combined. Later approaches concentrate on getting a sample of the assignment of words to topics instead [Griffiths and Steyvers, 2004]. We now give a short survey of the conceptual steps involved in the topic sampling process. For a more detailed description of the theory behind sampling, we refer to general text books such as [Newman and Barkema, 1999, MacKay, 2003, Bishop, 2006].\nGibbs sampling is a simple technique based on the fact that when the outcome of one variable is sampled conditioned on the previously sampled outcomes of the other variables, the overall distribution converges, when sampled iteratively, to the underlying distribution.\nFor LDA, this sampling is particularly easy because the distributions involved are multinomials conditioned on Dirichlet priors. The Dirichlet distribution is conjugate to the multinomial distribution. This means that when the inital assumption about a multinomial is that it is drawn from a Dirichlet (prior) distribution with parameters \u03b1, then, after observations x(i) (vectors indicating the outcome) generated by the multinimial are available, the most likely underlying (posterior) distribution for having generated the multinomial is again a Dirichlet distribution with parameters\n\u03b1 \u2032 = \u03b1+\n\ufffd\ni\nx(i)\n24"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "One might want to regard the initial parameters \u03b1 as pseudo-counts, added before any actual data have been seen.\nThe expectation of a multinomial distribution p \u223c Dir(\u03b1) is\nE[pi] = \u03b1i \ufffd\nj \u03b1j\nTherefore, the estimates of the word emission probabilities \u03c8(zk) = p(wj|zk) and of the topic probability \u03b8(d) = p(zk|d), conditioned on the posterior estimate, as observations including samples for all positions, are:\n\u03c8\u0302 (w) j =\nn (w) j + \u03b2\nn (\u00b7) j +W\u03b2\n\u03b8\u0302 (d) j =\nn (d) j + \u03b1\nn (d) \u00b7 + T\u03b1\nThe sampling equation can easily be obtained by excluding the current position from\nthe observations:\nP (zi = j|z\u2212i,w) = P (zi = j,w|z\u2212i)\nP (w)\n= \u03c8\u0302 (wi) \u2212i,j \u03b8\u0302 (di) \u2212i,j\nP (w\u2212i|z\u2212i)\nP (w)\n\u221d n (wi) \u2212i,j + \u03b2\nn (\u00b7) \u2212i,j +W\u03b2\nn (di) \u2212i,j + \u03b1\nn (di) \u2212i,\u00b7 + T\u03b1\nHere, n\u2212i,j counts how often a topic j (using the dot \u00b7 for any topic) has been assigned to any position in the text collection, excluding the position i, for which a new topic is being sampled. The superscripts denote restrictions on the counts, for example (wi) only includes assignments to positions that have the same word assigned as at position i and (di) only includes assignments to positions that belong to the same document as position i. The dot (\u00b7) indicates no restriction.\nThe sampling distribution can be interpreted the following way: The left formula, corresponding to a smooth estimate of p(wi|zj), favors topics that have often generated the same word as at position i. The right formula, providing a smooth estimate of p(zj |d), favors topics that have already often occurred in the current document.\n25"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Sampling itself does not distinguish between training and inference documents. In practice, the topic counts are fixed for the training set after sampling is assumed to have reached a stable state. Sampling iterations for inference change the topic assignments only for single unseen documents."}, {"heading": "3.2.3 Practical Issues", "text": "To determine the similarity between two documents, one can compare either their sampled topic vectors or the probability vectors obtained from them [Griffiths and Steyvers, 2004]. When other variational EM estimation techniques are applied, also other parameter vectors might be available and used [Cimiano et al., 2009, Blei et al., 2003]. The comparison between these vectors can be done either by taking the cosine similarity of their angles or, in case the vector is indeed a probability distribution (as for \u03b8), by using probability divergence measures, such as the (symmetrized) Kullback Leibler divergence or the Jensen-Shannon divergence.\nFor language model based information retrieval, one is interested in the probability of a query, given a document p(q|di). Wei and Croft [2006] interpolate a language model based on LDA with a unigram language model directly estimated on the document. The LDA model gives smooth estimates for every word in the query by \ufffd\nj P (w|z =\nj)P (z = j|di). With this formulation, no latent topics are estimated for the query and therefore no \u03b8 exists that could disambiguate terms in the query based on other terms occurring in it (in contrast to topics having been sampled following a combination of word-based \u03c8 and query-based \u03b8).\nLDA can be easily parallelized [Newman et al., 2007, Wang et al., 2009]. Usually, the set of training documents is split and sampling is done independently in each iteration. However, the topic-per-word counts have to be updated for all processes in each iteration. Therefore, the speedup is linear for the size of the corpus, but no speedup is obtained for the vocabulary size. For large vocabularies and few documents communication costs may outweigh any speedup obtained.\nWhenever a sampling technique is used, one wants to be sure that the estimates are stable. This could be a problem when only one topic is sampled per position for short documents or queries. The most natural way to overcome this problem is to average the results of several sampling iterations.\n26"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "3.2.4 Multilingual LDA", "text": "LDA has been extended for several languages [Ni et al., 2009], see also [Mimno et al., 2009] for an investigation of the semantic clustering of this model. Most of the components remain the same, the main difference is that for each language ls a different word emission distribution \u03c8ls is assumed. Depending on the language of a position in a document, a word is generated conditioned on the corresponding distribution. The sampling equation becomes, using similar notation as before:\nP (zi = j|z\u2212i,w) \u221d n (wi,li) \u2212i,j + \u03b2li\nn (li) \u2212i,j +Wli\u03b2li\nn (di) \u2212i,j + \u03b1\nn (di) \u2212i,\u00b7 + T\u03b1\nFigure 3 shows the model in plate notation. This model allows to deal with multilingual topics in an elegant way. The model is truly multilingual in that it does not use the topic variable to estimate the language, as it could be the case for a monolingual model applied to a multilingual document without any adaptions on either the data or the model side. A theoretically sound model does not mean that it also provides a good bridging between two languages. It is crucial [Mimno et al., 2009] how many \u201cglue documents\u201d, i.e. documents that indeed have counterparts in all compared languages, are available. Although the topic variables may not estimate the language, the partition of the topic space might diverge. Consider, as an extreme example, a multilingual model trained on hundreds of thousands of documents, each available in exactly one of the covered languages, and only one document providing text in all of them. It becomes obvious that this is clearly too little to align the topic spaces between the languages.In our weighting experiments in section 4.1 we show evidence that this intuition can be generalized: not only should a large number of glue documents exist, good bridging documents should optimally be of equal length. We believe, therefore, that it is a more promising approach to normalize the data suitably to fit into a standard LDA model. Moreover, this has the advantage that one of the many optimized toolkits readily available for LDA can be used out of the box, without the need to adapt their sampling schemes.\nWe give an informal argument why a standard (monolingual) LDA model trained on a normalized multilingual document collection, with all language parts in a document being of equal length, is essentially equivalent to the multilingual LDA model applied to the same collection. Consider L languages, and the same prior parameters \u03b2 in all\n27"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "languages. We want to denote with\n\u03b6i,j = n (li) \u2212i,jL\nn (\u00b7) \u2212i,j\nthe proportion of topic occurrences in a language to occurrence counts in all languages. We assume that the words can be unequivocally identified as belonging to a particular language (this is in practice easily achieved by attaching suitable prefixes) so that n(wi,li) = n(wi). For simplicity, we also assume same vocabulary size in all languages (without this assumption smoothing differs per language). We start with the multilingual model:\nP (zi = j|z\u2212i,w) \u221d n (wi) \u2212i,j + \u03b2\n\u03b6i,j L n (\u00b7) \u2212i,j + 1 L W\u03b2\nn (di) \u2212i,j + \u03b1\nn (di) \u2212i,\u00b7 + T\u03b1\nFor \u03b6i,j = 1 it samples just as the monolingual model since the constant factor L can be ignored. For \u03b6i,j < 1 the denominator is decreased, making the topic (that is less frequent for the language at position i than for others) more likely to be sampled for this language. Likewise \u03b6i,j > 1 makes a more frequent topic less probable in the next iteration. One can argue that in the multilingual model on normalized data a stable situation occurs when the topic distribution of one language equals that of all languages. In this situation the multilingual model behaves like the standard model. The standard model can therefore be used on such data.\n28"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "3.3 Explicit Semantic Analysis", "text": "Explicit Semantic Analysis (ESA) [Gabrilovich and Markovitch, 2007, Potthast et al., 2008, Sorg and Cimiano, 2008, 2009] is another scheme to overcome the word-mismatch problem. In ESA, the association strength of words to the documents in a training collection is computed, and vectors of these asscociations are used to represent the words in the semantic space spanned by the document names. These word representations can be used to compare words, they can also be combined for a comparison of texts."}, {"heading": "3.3.1 Formalization", "text": "Several formalizations are possible in this setting. The fundamental ingredients that determine an implementation are:\n\u2022 Word vectors: For every word w a vector w, indicating its association strength\nto the documents is computed. Formally:\nw = \ufffdas(w, a1), \u00b7 \u00b7 \u00b7 , as(w, aN )\ufffd\nWhere as(w, an) indicates the association strength of w to training document an (mnemonic for article, since the most commonly used articles are Wikipedia documents) of N training documents in total.\n\u2022 Text vectors: For a new document (or query) d a representation d is computed\nfrom the word vectors:\nd = f({w|w \u2208 d}b)\nThe subscript b should indicate that in the general case one wants to be able to consider the multiset (\u201cbag\u201d) of word vectors, allowing for counts of the words in d to be considered.\n\u2022 Similarity function: Giving two documents d1 and d2 the similarity is com-\nputed using a similarity function on their text vectors.\nThe word vectors that were used in [Gabrilovich and Markovitch, 2007] and found to be optimal in [Sorg and Cimiano, 2009] are obtained by taking the respective columns of the tf.idf-weighted document-term matrix A of the training collection. In other words, this corresponds to an association strength function where:\nas(w, an) = An,w \ufffd\nw\u2032\u2208W An,w\u2032 log\nN \ufffdN\nn\u2032=1 1An\u2032,w>0 (11)\n29"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Here, 1An\u2032,w>0 is an indicator that equals to one if word w has appeared in document an\u2032 at least once, and that equals to zero otherwise. Notice that in this formulation the relative term frequency is used. In effect, this is a length normalization, making all documents contribute equally strongly. We use this choice of word vectors in our experiments.\nFor the text similarity, several settings have been proposed. In [Gabrilovich and Markovitch, 2007] a weigting of the word vectors is used, they are multiplied with scalars equal to, again, an tf.idf weighting of the terms, and then summed up. It is however not very clearly described which exact instantiation of the tf.idf function was used in the experiments. Sorg and Cimiano [2009] explore further combination schemes, including the sum of the elements of either the multiset (considering term frequency) or of the set (not considering term frequency) of word vectors. They find that the set combination works best, yielding preliminary text vectors of the form:\nd\u0302 = \ufffd\nw\u2208d\nw\nIt is beneficial to truncate the resulting vectors at a certain threshold. The thresholding that turned out to be most successful was to retain the c biggest non-zero values of the vectors d\u0302. Let t be the value of the (c+ 1)th biggest component of d\u0302.\ndi =\n \n\nd\u0302i if d\u0302i > t,\n0 otherwise.\nHere, di refers to the ith component of d. In Sorg and Cimiano [2009] it was found that a cut-off value of c = 10000 works best. Again, we use this parametrization in all following experiments that involve ESA. As a similarity function the cosine is suggested and used by us."}, {"heading": "3.3.2 Multilingual ESA", "text": "The application of this model in a multilingual setting is straightforward. For L languages consider document term matrices A(1) \u00b7 \u00b7 \u00b7A(L). Construct the matrices in a way that the document rows correspond. For all languages each of the rows A (\u00b7) n contains documents about the same topic across the languages. Therefore only documents can be included that are available in all of the considered languages. For each document\n30"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "the mapping to text vectors is performed using a monolingual matrix corresponding to its language. As the documents are aligned, similarities can be computed across languages. Because the relative frequency is used in the tf.idf-weighting, all documents are normalized and no bias occurs for documents longer in one language than in another.\nDifferent comparable text collections can be used. The most easily available and best performing one is Wikipedia, originally ESA was also tried on the Open Dictionary Project (ODP)3 data, with considerably worse results. Sorg and Cimiano [2008] observe that even when the ESA model is used to compare documents taken from two languages out of the set French, German and English it slightly improves retrieval performance to restrict the training corpus to documents available in all of the three languages. This finding is presented as a side-note rather than substantiated with further evaluation results. Since our work aims at finding best practices for cross-langual retrieval, the most general setting for us is to use training corpora in exactly those languages in which we want to do retrieval."}, {"heading": "3.3.3 Implementation", "text": "Wikipedia dumps can be bigger than the working memory current customary computers possess. The English Wikipedia dump for example comprises 27G of xml. From the\n3http://www.dmoz.org\n31"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "dump a sparse tf.idf-weighted document-term matrix is constructed, in a line by line process. This matrix has to be inverted to obtain the word vectors, while considering all documents per word. In the only open academic implementation known to us4, this invertation is done by using an indexer and a database. In our implementation we use an iterative blockwise invertation which is adaptive to different memory equipments and is usable without installing any external libraries. As our implementation has also put focus on efficient xml processing, we observe a speedup of more than factor 10 when processing the English Wikipedia dump as compared with research-esa.\n4 research-esa, http://code.google.com/p/research-esa/\n32"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "4 Experiments", "text": "As Table 1 shows, three corpora were used for our experiments: A Wikipedia subset assembled by us, the Multext JOC corpus and the German-English dataset of the CLEF 2000 ad-hoc track. A detailed discussion of the characteristics is given in the respective chapters."}, {"heading": "4.1 Preliminary Experiments", "text": "In the first series of experiments the models are explored on a small dataset. All three models, ESA, LDA and our adaptation of pLSA are compared. As for the latent variable models, it is of particular interest how data weighting and normalization affects retrieval performance. We will fix some decisions and will consider a smaller set of hypotheses in later large scale experiments.\nThe starting point of this chapter are hypotheses and questions that come up with the theoretical models we introduced and with experiments reported elsewhere. We will summarize them as follows:\n1. How does the multilingual pLSA model perform?\n2. How important are \u201cglue documents\u201d in this model (that is, truly multilingual\ndocuments)? What weighting schemes favor important documents?\n3. How does a monolingual LDA model perform on multilingual data\n\u2022 in the form reported in [Cimiano et al., 2009]?\n\u2022 when using a scheme that takes theoretical considerations into account?\n4. How do the latent variable models compare to ESA?\n5. How do combinations of them perform?\n33"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Weighting scheme experiments are conducted with the pLSA model, because it can handle real valued quantities which can be scaled up continuously. For the LDA model integer valued quantities are necessary because of the sampling process. In any case, weighting schemes are often ad-hoc. For that reason we want to use them only as indicators of important properties and as a justification for restricting our data in a sensible way."}, {"heading": "4.1.1 Experiment Setup", "text": "Our experiments were conducted using a bilingual dataset. We performed a mate retrieval-task, that means finding a document in a foreign language that corresponds to a document in the original language. For example in a collection of English Wikipedia articles, find the one dealing with \u201cfrench fries\u201d, given the German article corresponding to \u201cPommes Frites\u201d.\nOur dataset was constructed from a German and an English Wikipedia dump5. We randomly chose 2000 documents belonging to the German Wikipedia category \u201cPhilosophie\u201d for each of which an English Wikipedia article exists to which (1) the German article links and (2) which itself links to exactly this German article. We ignored certain Wikipedia pages that we do not expect to contain encyclopedic knowledge, like redirects or disambiguation pages. Tokenization was done by regarding all characters that are not in a Unicode letter block as delimiters, and all tokens that were only one letter long or were found in the Snowball stopword list were disregarded. We\n5we used the German snapshot of 2009-07-10 and the English snapshot of 2009-07-13\n34"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "also removed all words that occurred only once in this collection. The remaining words were stemmed using the Snowball6 stemmer.\nThe data are organized in a matrix of three blocks (see also Figure 5): One comparable block and two monolingual blocks. The comparable block comprises 1000 multilingual documents, each row representing the term statistics of both a German and an English Wikipedia article on the same topic. The German monolingual block contains the German term statistics of the remaining 1000 Wikipedia articles. Likewise, the English monolingual block contains 1000 articles, each with their counterpart in the German monolingual block.\nThe evaluation setup is as follows: For every document d(de,i) in the German monolingual block we rank the document in the English monolingual block according to the retrieval method applied and get a rank ri for the one corresponding to the German query document. A run is evaluated by its mean reciprocal rank (mrr), given by:\nmrr = 1\nN\nN \ufffd\ni=1\n1 ri\nFor the pLSA model, the whole matrix of three blocks is embedded, that is, the P (Z|di) of both the comparable and the monolingual articles were computed and used in the re-estimation iterations. This setting might be unrealistic in a real-word retrieval task, however as pLSA does theoretically not define inference for unseen documents it seems to us to be the most sound way. Note that in this scheme also term co-occurrences in the monolingual blocks influence the model estimation. For every training run, 100 EM iterations were performed.\nRanking was done by the cosine similarity of the parameter vectors of the multinomial distributions P (Z|di), a normalized dot product lying between 0 and 1 for vectors with all positive components:\nsim(d(1), d(2)) =\n\ufffdK k=1 P (zk|d (1))P (zk|d (2))\n\ufffd\n\ufffdK k=1 P (zk|d\n(1))2 \ufffd\n\ufffdK k=1 P (zk|d (2))2\nThis ranking scheme was applied and found to work best by Hofmann [2001] for a monolingual retrieval task and standard pLSA.\nSome experiments with pLSA involve a weighting of the data matrix. Only weighting schemes were applied that changed the length of the documents (to which p(di) is\n6http://snowball.tartarus.org/\n35"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "directly proportional) and therefore their influence. Such a scaling is theoretically admissible as it does not touch the meaning of the other concepts and can give hints which data provide the most useful cross-language bridge. For the ESA model we applied the parametrization described in 3.3 and used the comparable block as a training corpus. Note that the cut-off parameter is greater than the training corpus size and has therefore no influence. Again ranking is done by the cosine similarity. For the LDA model we estimated the parameters P (W |Z) with the comparable block. The highly efficient and parallel LDA implementation plda [Wang et al., 2009] was used. We used the Dirichlet parameter values suggested by Griffiths and Steyvers [2004], \u03b1 = 50 (number of topics) and \u03b2 = 0.1. The model was trained by doing 100 sampling iterations in order for the distributions to converge, and then averaging the sampling outcomes of further 50 iterations for the estimates of P (W |Z). For inference on unseen documents, 10 iterations were used for convergence and further 5 iterations for averaging the estimate. German and English terms were made distinguishable by adding the prefixes de and en respectively. Inference was done with this model on the documents in the monolingual blocks. Ranking was made, in the same way as for pLSA and ESA, by taking the cosine of vectors of the sampled topic counts per document."}, {"heading": "4.1.2 Results", "text": "In the first runs we tested the multilingual pLSA model. The glue documents are the only cross-language bridge and therefore of special importance. Instead of varying the number of glue documents (which is 1000), we gave them higher weight. Both single runs as well as combined runs with concatenated parameter vectors of different dimensionalities were evaluated. Combination was only done for runs with the same weighting.\nModel combination has a major impact on performance, which is explainable by overfitting issues and convergence to local maxima. The first applied weighting scheme multiplied the term counts in the comparable block by a constant weight (we tried values of 2,4,8 and 16). As Figure 6 and Table 2 show, this improved retrieval performance drastically compared with the original model. The higher the weights are chosen, the more the P (w|z) are influenced by the comparable documents, the monolingual doc-\n36\nWikipedia-Based Cross-Language Retrieval Models Benjamin Roth\numents are then rather \u201cpulled\u201d in the latent spaces instead of \u201cpulling\u201d themselves. However, this is only true for the P (w|z) that occur also in the comparable documents. The P (w|z) for words that occur only in monolingual documents provide some transitive clues that would not be present if the model was trained on comparable data only.\nThe other weighting scheme is based on the reasoning that, instead of uniformly weighting multingual documents higher, it could be beneficial to enforce documents that are more multilingual than others. One criterion to regard documents as more\n37\nWikipedia-Based Cross-Language Retrieval Models Benjamin Roth\nmultilingual is the degree to which they are similar in length in both language parts. Therefore we formalize the weight as the geometric mean, dependent on the length of the two language sides lde and len with a small floor count c = 0.001. We call such a document weighting \u201cdynamic\u201d. The dynamic weight yielded the best results for pLSA, it is formalized as:\nwdyn =\n\ufffd\n(1\u2212 c) lde\nmax(lde, len) + c \u00b7\n\ufffd\n(1\u2212 c) len\nmax(lde, len) + c\nWith the ESA model we got a score of mrr = 0.7548, which is clearly better than the pLSA model. In order to test whether the ESA model conveys distinct information to the pLSA model, we interpolated both models and multiplied the ESA vectors by a parameter \u03b1 and the dynamically weighted pLSA vectors by (1 \u2212 \u03b1) and used the concatenation of both. We got an improvement by 25.7% with mrr = 0.9489 for \u03b1 = 0.8. See Figure 7.\nFor LDA we performed three types of experiments: First, we sampled a model for the comparable documents with standard LDA without any length normalization, in the manner of [Cimiano et al., 2009]. In fact, it turned out that this produced the worst results of all runs conducted. To estimate whether the different length ratios were responsible for the negative effect, we constructed a new training set containing the same information, but with same length ratios. For this end we multiplied the counts of the shorter language side to match the counts of the longer one. As the\n38\nWikipedia-Based Cross-Language Retrieval Models Benjamin Roth\nsampling procedure needs integers, we rounded the new counts. Figure 8 and Table 3 show that this has a drastic impact on performance. This is in accordance with our theoretical assumptions. In another experiment on the stretched data, we multiplied (after inference was done) each sampled topic outcome with a weight according to the inverse document frequency of the word it was sampled for. The reasoning behind that was to increase the influence of discriminative terms. We found that this brought a slight improvement but it does not preserve the theoretical properties of the topic statistics.\nIn order to estimate how much different information the two latent variable models convey, we interpolated the dynamically weighted pLSA (\u03b1) model with the idf weighted stretched LDA model (1\u2212\u03b1). We got mrr = 0.7063 for \u03b1 = 0.4, which is an improvement of 8.0% with respect to the LDA run alone.\n39\nWikipedia-Based Cross-Language Retrieval Models Benjamin Roth\nFigure 9 shows topic distributions for the German and the English monolingual blocks for which inference was done with LDA models trained on either the stretched or the bare comparable block. It can never be expected that the inferred topics are the same for two text sets; in our case the monolingual documents might focus on different thematical aspects. However, for the unstretched model it is almost consistently the case that topics predominant in one language do rarely occur in the other language. We take this as evidence that such a model primarily estimates the language of a document.\nWe also tried runs that applied a tf.idf weighting to the document matrix in the pLSA setting. This weighting had a slightly negative effect on performance. Moreover, it does not integrate well in the theoretical models, therefore we will not discuss these results further."}, {"heading": "4.1.3 Conclusion", "text": "The initial experiments have been made on rather a small training and target collection. However, the number of query documents is high and therefore some interesting observations can be made. Obviously it is important to save model capacities for cross-language bridging. We can assume that performance improves\n\u2022 when the model does not try to structure the monolingual spaces. The indicator\n40"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "for this is the downweighting experiment for imbalanced documents in the pLSA model.\n\u2022 when the model does not try to guess the language. The upscaling experiment\nof the shorter language side points to this.\nFurthermore, performance increases drastically when different runs of latent variable\nmodels are combined and when latent variable models are interpolated with ESA.\n41"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "4.2 Mate Retrieval on Multext JOC", "text": "The Multext JOC corpus7 consists of 3500 questions to the European Parliament and of answers to these questions. As in Sorg and Cimiano [2008] we use the concatenation of a question together with its answer as a query in one language to search the collection of translations in another language for its counterpart. Our experiments were done with English as the query language and German as the target language. Only preprocessing steps that are clear and easy to reproduce were performed. Exactly those questions were retained that to which an answer was assigned and had the same id in English and German. This resulted in a set of 3212 texts in each language, 157 more than were used in Sorg and Cimiano [2008]8. Sequences of characters in Unicode letter blocks were considered words. Words with length = 1 or length > 64 and words contained in the Snowball stopword list were ignored. All other words were stemmed with the publicly available Snowball stemmer9. In contrast to Sorg and Cimiano [2008], no compound splitting was done.\nFor the training collection all pairs of Wikipedia articles10 were used that have bidirectional direct cross-language references. All markup was stripped off by using\n7http://www.lpl.univ-aix.fr/projects/multext 8the exact document selection criterion of their experiments is unknown to us 9http://snowball.tartarus.org/\n10we used the German snapshot of 2009-07-10 and the English snapshot of 2009-07-13\n42\nWikipedia-Based Cross-Language Retrieval Models Benjamin Roth\nthe same filter as in a publicly available ESA implementation11. Wikipedia articles of less than 100 words in either language were ignored and words with a Wikipedia document frequency of 1 were filtered out. The final training corpus consists of 320000 bilingual articles.\nPerformance of retrieval was measured in mean reciprocal rank (mrr). The ESA retrieval experiment was performed using the same parametrization as discribed before and the result of Sorg and Cimiano [2008] was reproduced to a difference of 1% (in our experiments we obtained a score of mrr = 0.77 compared with mrr = 0.78).\nAs for the LDA experiments, we were interested in the effect of length normalization of the training documents. We tried two methods: First, every document was cut off at a length of 100 words. Second, for the longer language side of an article only a random sample of size equal to the smaller language side was retained. A resizing with a scalar is not possible because the sampling process requires integer counts. We marked each word with a prefix indicating its language and retained a vocabulary size of roughly 770 thousand and 2.1 million for the cut-off method and for the downsampling method respectively. Both training collections were embedded with 125, 250 and 500 dimen-\n11http://code.google.com/p/research-esa/\n43\nWikipedia-Based Cross-Language Retrieval Models Benjamin Roth\nsions, and additionally with 1000 dimensions for the cut-off corpus (the vocabulary size was the limiting factor with respect to our computing facilities). The Google plda package Wang et al. [2009] was used with the suggested parameters (\u03b1 = 50#topics and \u03b2 = 0.01). With the trained model, topics were inferred for the monolingual Multext documents. In order to get a stable estimate, the statistics of 50 sampling iterations were averaged. Similarity in the LDA setup was measured by taking the cosine similarity between the sampling statistics.\nA drastic improvement over non-normalized LDA can be observed: while Sorg and Cimiano [2008] report a score of mrr = 0.16 for their 500-dimensional LDA model, we get mrr = 0.53 with the cut-off corpus. We suppose that the reason for this difference\n44"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "is that a non-multilingual LDA model applied to a comparable corpus estimates the predominant language of a document rather than its semantic content. Another improvement can be observed by combining the results of different models, a technique that is usually applied for pLSA Hofmann [2001]. In this case, the cosine scores of runs with different dimensional models were simply averaged (this corresponds to concatenating the L2-norm normalized sampling statistics vectors). This yielded a score of mrr = 0.68 for the cut-off model, showing performance in the same order of magnitude as ESA. Figure 11 and Table 4 give a survey of the results obtained with LDA. Scores significantly better than in the respective line above having p \u226a 0.005 in the paired t-test are marked with \u2217\u2217. (Of course we could not test against scores reported elsewhere, for lack of the original numerical data.)\nHow different are the ESA and the LDA models, how much can they contribute to each other? In order to answer this question, we combined the cosine scores of both models by different interpolation factors 0 \u2264 \u03b1 \u2264 1. A stable improvement in performance with maximummrr = 0.89 was achieved for giving the cut-off LDA model a weight of 0.4 and the ESA model a weight of 0.6. See Figure 12.\n45"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Average Precision measures for every document i relevant to a query q ranked at ri,q the precision up to its rank. The per-document score\nAPi,q = relevant documents j with 1 \u2264 rj,q \u2264 ri,q\nri,q\nis combined to a per-query score, by averaging over all scores of the set of relevant documents Relq:\nAPq = 1\n|Relq|\n\ufffd\ni\u2208Relq\nAPi,q\nMean Average Precision combines these scores for the set Q of all queries by their arithmetic mean:\nMAP = 1\n|Q|\n\ufffd\nq\u2208Q\nAPq\nGeometric Mean Average Precision uses the geometric mean instead:\nGMAP = |Q|\n\ufffd\n\ufffd\nq\u2208Q\nAPq"}, {"heading": "4.3 Query-Based Retrieval with CLEF2000", "text": ""}, {"heading": "4.3.1 Experiment Setup and Results", "text": "Mate retrieval experiments can be criticized as being an unrealistic retrieval scenario. Therefore, a second evaluation was done on the CLEF12 German-English ad-hoc track of the year 2000. The target corpus consists of about 110000 English newspaper articles together with 33 German queries for which relevant articles could be pooled. For our experiments the title and description fields of the queries were used and the narrative was ignored.\nA common strategy for cross-language retrieval is first to translate the query and then to perform monolingual retrieval. While the translation process would have taken prohibitively long for the Multext corpus, we performed query translation on the CLEF2000 queries with a standard Moses translation model trained on Europarl. Retrieval with the translated queries was done by comparing the cosine of the tf.idfweighted word-vectors; we used formula (11) on page 29 to compute this weighting.\nWe evaluated both the machine translation model and the concept models trained on\n12http://www.clef-campaign.org/\n46\nWikipedia-Based Cross-Language Retrieval Models Benjamin Roth\nWikipedia. In addition to the most commonly used mean average precision score (map) we also evaluated by geometric mean average precision (gmap, see page 46), which rewards stable results for hard queries. The ESA and the cut-off LDA models with dim = 500 perform equally well for map, while the combination of LDA dimensions gets a considerably better score. This is in contrast to the findings in the mate-retrieval setup. The reason for that, we suspect, may be that the parameters of ESA have been found in order to optimize such a setting. For gmap, LDA consistently outperforms ESA. For the combined LDA-ESA model no clear improvement could be observed using the combination with \u03b1 = .4 from the previous experiment. The machine translation model (map = .203) performed better than the concept models. When the different concept models were combined with the machine translation model by interpolating their cosine scores, all three models achieved improvements. The biggest and most stable improvement was achieved by the LDA-ESA concept model yielding a score up to map = .246 with equal weight for the concept model and the machine translation model. Figure 13 and Table 5 show an overview of the results. Scores significantly better than in the respective line above, having p < 0.05 and p < 0.005 in the paired t-test, are marked with \u2217 and \u2217\u2217 respectively.\n47"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "4.3.2 Error Analysis", "text": "A querywise error analysis is difficult because the inner workings of quantitative methods are often opaque to human analysis. However, the machine translation output is the most contributing source and it is accessible to examination. We sorted the machine translation output by how much it profited by the concept models in the best performing setting. On page 50 we report the score that is obtained by machine translation and the increase when combined with the concept models. We analyzed how often a word was obviously unknown by the machine translation system trained on Europarl and therefore wrongly just copied over. It would be possible to recognize this type of error automatically. In addition, for every translated query we counted how many words in it had no semantic meaning related to the purpose of the query and were therefore useless (these words are hence called junk words). Junk words are, for example, function words not filtered by the stopword list, machine translation errors of several kinds and artefacts from the query formulation, mainly from the description part (e.g. \u201crelevant documents contain information about...\u201d). The junk word error type would be more difficult to detect.\nAlthough the analyzed data basis is small, we conjecture that the concept model makes such queries more robust which induce one of the two errors, while it might be less useful where a good translation is present and the terms are weighted well: In the cases where the concept models contributed there were, on average, 0.53 unknown words for machine translation and 0.24% junk words, in contrast to 0.14 unknown words and 0.04% junk words in the cases where the concept model decreased the score. For future experiments it might be interesting to test whether a trade-off weighting\n48"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "between translation and concept model conditioned on a reliability score of the machine translation improves performance."}, {"heading": "4.4 Discussion", "text": "We have shown that a highly significantly contributing cross-language retrieval component can be obtained with LDA, since standard and multilingual LDA behave alike for length normalized data. The two very simple techniques we found most effective are document cut-off and model combination. Thus, we get an improvement of 325% mrr compared with a non-competitive score previously reported for LDA on the Multext corpus. The combination of LDA with ESA increases performance in mate-retrieval experiments by 15.6% mrr compared with ESA alone. In cross-language query-based retrieval on CLEF 2000, a monolingual scheme based on machine translation output can be improved by 21.1% map when combined with concept models. While ESA alone performs better on mate-retrieval, we find LDA superior in query-based retrieval, maybe because commonly used ESA-parameters have been tuned for direct translation finding.\n49\n50"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "5 Towards Retrieval Based on Only Wikipedia", "text": "Concept models trained on article level can contribute to cross-language retrieval. However, the main contribution comes from the statistical machine translation module trained on Europarl. Coarse thematical relationships alone can arguably not capture the specific meaning contained in a query, a word-to-word translation is necessary. But lexica and parallel corpora are often only accessible for some European language pairs. And even if they exist their vocabulary is inherently limited in contrast to an ever growing ressource like Wikipedia. The question therefore arises how parallel text or word-to-word mappings that are less dispersing than those obtained from documentlevel co-occurrences can be replaced by information extracted from freely available knowledge sources such as Wikipedia.\nSeveral approaches in this direction have been undertaken. Often, a Wikipedia title in the source language is associated with a word and the corresponding title in the target language is used as a translation [Wentland et al., 2008, Nguyen et al., 2009]. However, Sjobergh et al. [2008] note that the vocabulary distribution of titles has a skew to certain words: 33% are normal nouns, 66% proper nouns and only 0.03% verbs. Because the titles of Wikipedia articles are specifically tailored to be unique identifiers rather than representative text samples, translations of large classes of words might introduce artefacts. Also, titles are usually in a normal form which could be problematic in highly inflecting languages or when a variation is used. Another, more general approach is to construct a parallel corpus out of a comparable corpus by finding parallel sentences [Maeda et al., 2008, Adafre and de Rijke, 2006] which can then be used to enhance the training set of a machine translation system. However, such an extraction is complicated and still requires a parallel corpus to perform well.\nAn optimal translation model based on Wikipedia should therefore fulfill the follow-\ning demands:\n\u2022 It should be based on simple ideas and easily implemented.\n\u2022 It should be based on probabilistic principles to be usable in a language modeling\nsetup.\n\u2022 The model should be text-to-text rather than title-to-title.\n\u2022 The model should only translate text it is trained for.\n51"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "The method we use is based on the anchor text of links. In Wikipedia, text that is expected to be of particular interest to a reader is linked to another Wikipedia article if this adds to the understanding. The key policy as stated in the Wikipedia guidelines is:\nAsk yourself, \u201dHow likely is it that the reader will also want to read that other article?\u201d These links should be included where it is most likely that readers might want to use them; [...] Always link to the article on the most specific topic appropriate to the context from which you link: it will generally contain more focused information, as well as links to more general topics.13\nAny text can be linked to any page. For example, the German texts \u201cin Wa\u0308ldern gelegte Bra\u0308nde\u201d, \u201cBuschfeuer\u201d, \u201cBuschbrand\u201d and \u201cWaldbrand\u201d may be valid contexts to be linked to the article titled \u201cWaldbrand\u201d (with the English counterpart \u201cWildfire\u201d). The following three items of information are necessary to build a (potentially multilingual) probabilistic translation model based on linked text:\n\u2022 How likely is a text to be linked?\n\u2022 What are probable links, given a linked text?\n\u2022 What is a probable text, given there is a link to a certain article?"}, {"heading": "5.1 Model", "text": "If, for the sake of simplicity, a unigram model is used this amounts to the probablities P (l|w), P (a|w, l) and P (w|a, l). In a bilingual setting l is a variable indicating whether the source word is linked, a is the bilingual article the source word is linked to, and wE and wF are words in the source and the target languages respectively. It is:\nP (wF |wE) = P (wF |l = true, wE)P (l = true|wE)\n+P (wF |l = false, wE)P (l = false|wE)\nAssuming that the translation of linked words does only depend on the articles they\nare linked to, one gets:\n13http://en.wikipedia.org/wiki/Wikipedia:Linking retrieved on 13 October 2009\n52"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "P (wF |l = true, wE) = \ufffd\na\nP (wF |a, l = true)P (a|l = true, wE)\nIf Wikipedia links are the only cross-language bridge, for the unlinked case a general language model LMF could be used to estimate the probability, also other translation sources could be plugged in here.\nP (wF |l = false, wE) = P (wF |LMF )\nIn [Mihalcea and Csomai, 2007] several methods were tried to predict whether a word was linked or not. The most effective was based on the relative frequency of linked occurrences. We take this approach also for the other distributions and get\nP (l = true|wE) = n(l = true, wE)\nn(wE)\nP (a|l = true, wE) = n(a,wE)\nn(l = true, wE)\nP (wF |a, l = true) = n(wF , a)\nn(a)\nwhere n(\u00b7) denotes co-occurrence counts and we omitted redundant restrictions. Figure 14 shows some values for an example translation. Note that the translation distribution is not as clean as one could whish (the city name \u201cCanberra\u201d is linked as strongly associated with a specific event of a wildfire). One might hypothesise that problems of that kind are connected with the unigram assumption: If the anchor texts\n53"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "were considered as a whole, the specifity of the articles would correspond better to the specifity of texts.\nMany ways of how to exploit such a model are conceivable. For example, it can be used for query translation. First we note that the probability P (l = true|wE) can function as a term weighting, assuming that the importance of terms is correlated with their probability of being linked. We use this assumption and employ P (wF , l = true|wE) for a translation model. It is important to keep in mind that l indicates the linking of the source word. Additionally, translation and linking should be independent of the document, given a source word (but not independent of each other). The translation model of a document DE is the probability distribution that can be paraphrased as:\nThe probability that a word, chosen from D, is linked and has wF as translation.\nUsing the parts defined before and simple reordering gives:\nP (wF , l = true|DE) = \ufffd\nwE\nP (wF , l = true|wE)P (wE |DE)\n= \ufffd\na\nP (wF |a, l = true) \ufffd\nwE\nP (a|l = true, wE)P (l = true|wE)P (wE |DE)\nNote that the inner sum defines P (a, l = true|DE), which similarly to ESA associates a weighting over Wikipedia articles to documents but in addition has a probabilistic\n54\nWikipedia-Based Cross-Language Retrieval Models Benjamin Roth\nsemantics. By normalization, P (wF |l = true,DE) can be obtained easily."}, {"heading": "5.2 Vector Space Experiments", "text": "Experiments are done on the CLEF2000 dataset. The queries are translated from German into English by the following scheme: In the translation of the original query Qorig, words wtransl with P (wtransl|l = true, Qorig) \u2265 1 are retained. Counts are assigned to them proportionally to that probability. Retrieval is done with the translated query by tf.idf weighting and taking the cosine in the same way as it was done for queries translated with Europarl. Additionally, we interpolated the cosine scores of this run with the scores obtained by the other models trained on Wikipedia, which comprise the LDA model, the ESA model and the combination of LDA and ESA that worked best in the Multext experiments of section 4.2. In combination with the link translation model, LDA showed the best and most stable performance of up to map = 0.234. This is better than retrieval with the Moses translated queries (map = 0.203). However, it is slightly worse than the combination of machine translation output with LDA (0.237). ESA was clearly the least contributing component and could not improve the combination of the link model with LDA. One explanation may be that ESA captures similar information as the link model but does so less successfully. LDA and the link\n55"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "model, however, seem to be complementary, leading to a huge improvement when they are combined. As an alternative way to make use of the link model, similarly to the ESA model, the cosine similarity was taken for vectors with association strengths to Wikipedia articles. In the link model, we used the distribution vector P (a|l = true, Qorig) and P (a|l = true,D) to establish a similarity between query Q and document D. Using the article distribution vectors was a less successful way of employing the link model, but still better than the ESA model. Figure 16 and Table 7 give a survey of the results obtained with the link translation model."}, {"heading": "5.3 Language Modelling Experiments", "text": "In a query likelihood model [Ponte and Croft, 1998] a document provides a probabilistic model for a query. The ranking is usually done by:\nlogP (Q|D) = \ufffd\nw\u2208Q\nn(w,Q) log P (w|D)\nIn principle, all components are provided by the link model to perform retrieval in such a fashion. The probability that article a is the target when a linked word is picked from document D (making the same independence assumptions as before) is\nP (a|D, l) =\n\ufffd\nwE P (a|l = true, wE)P (l = true|wE)P (wE |D)\n\ufffd\na\u2032,wE P (a\u2032|l = true, wE)P (l = true|wE)P (wE |D)\nand P (wF |a, l) is the probability that, given a source word linked to article a, it is translated to word wF . Together the elements of the link model provide us with the\n56\nWikipedia-Based Cross-Language Retrieval Models Benjamin Roth\ndistribution:\nPlink(wF |D, l) = \ufffd\na\nP (wF |a, l)P (a|D, l)\nIt is practical to think of this distribution as combined in that way, because it separates the per-document estimates from the vocabulary estimates. Also, it is important to remember that the distribution is only conditioned on the words expected to be linked in the source document (indicated by the conditioning on l). If one wants to use the model as it is for a query likelyhood model, three problems immediately turn up:\n1. The zero-probability problem: Because the components of the model are\nestimated from relative frequencies, to many events zero probability is assigned. If one word in a query has zero probability, the whole query has also.\n2. The complexity problem: If the probability distributions are smoothed so\nthat they are never zero, summation might for every word go over all Wikipedia articles, which would be inhibitively expensive to compute.\n3. The training basis problem: The model only considers words likely to be\nlinked. Especially high frequency or function words could have skewed distributions."}, {"heading": "5.3.1 Model Combination on Query Level", "text": "The zero-probability problem is especially severe when\nPlink(wF |D, l) = \ufffd\na\nP (wF |a, l)P (a|D, l) = 0\nbecause this makes the log-probability undefined. This happens when P (wF |a, l) = 0 for the articles a that have non-zero probability for document D. However, smoothing a probability P (wF |a, l) > 0 for all word and document combinations would result in a full matrix with number of entries equal to vocabulary size multiplied by Wikipedia size. Moreover, smooth distributions P (wF |a, l) and P (a|D, l) would require full summation over all articles, while in the sparse case only the matching non-zeroes have to be considered. We employ a simpler, although slightly less principled strategy. We floor the probability Plink(wF |D, l) by the minimal value that could have been obtained\n57"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "by smoothing P (wF |a, l) with Laplace (add-one) smoothing.\nPlink(wF |D, l) = max\n\ufffd\n\ufffd\na\nP (wF |a, l)P (a|D, l), Pmin(wF |l)\n\ufffd\nbecause\n\ufffd\na\nPmin(wF |l)P (a|D, l) = Pmin(wF |l) \ufffd\na\nP (a|D, l)\n= Pmin(wF |l)\nwe set\nPmin(wF |l) = 1\nmaxa(n(a)) + V\nwhere maxa(n(a)) is the highest link count observed for an article and V is the vocabulary size. We use three language models. The first, Plink(Q|l,D), computes the likelihood of the query straighforward with the components as defined. The second, Plink\u2032(Q|l,D), tries to account for the training basis problem by weighting all counts in the query by multiplying them with the probability that they are linked. The third, PLDA(Q|D), estimates the probability according to the estimates obtained by the LDA models, in the same way as done in [Wei and Croft, 2006] for monolingual retrieval. For the LDA model we combined all runs from the cut-off corpus. We get:\nlog Plink(Q|l,D) = \ufffd\nw\u2208Q\nn(w,Q) log Plink(w|D, l)\nlogPlink\u2032(Q|l,D) = \ufffd\nw\u2208Q\nn(w,Q)P (l|w) log Plink(w|D, l)\nlog PLDA(Q|D) = \ufffd\nw\u2208Q\nn(w,Q) log PLDA(w|D)\nwhere\nPLDA(w|D) = \u03c8 (w)T \u03b8(D)\nis calculated as described in section 3.2.\nAs a simple form of model combination, we interpolated the query log probabilities of the LDA model linearly with those of each link model. So, for instance, the combination with the unweighted model becomes:\nlogP\u03b1(Q|D) = \u03b1 logPlink(Q|l,D) + (1\u2212 \u03b1) log PLDA(Q|D)\n58\nWikipedia-Based Cross-Language Retrieval Models Benjamin Roth\nFigure 17 and Table 8 give a survey of the results. One can see that the link model alone is already better than the tf.idf baseline based on Moses and Europarl. Interestingly, the LDA model gives worse results when applied in a language modeling setup than when used based on sampling statistics on both the query and the document side, as done in chapter 4.3. This might be so because sampling takes the whole context in the query into account, which may be seen as a way of disambiguation of the query words. By the combination of the link models with LDA in the language modeling setup we achieve better map scores on CLEF2000 than in the best vector retrieval setup, however, the results are worse for gmap compared to the vector retrieval setup."}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "5.3.2 Model Combination on Word Level", "text": "The second possible form of model combination is to interpolate word distributions given a document. Here, the question arises how to weight the query words, as a weighting according to link-probability seems reasonable and turned out to be successful for the link model, but is not justified for LDA. Hence, we used two model parameters \u03b1 and \u03b2 to interpolate distributions and weightings respectively. The query log-likelihood becomes:\nlogP\u03b1,\u03b2(Q|D) = \ufffd\nw\u2208Q\nn(w,Q) [\u03b2P (l|w) + (1\u2212 \u03b2)] log [\u03b1Plink(w|D, l) + (1\u2212 \u03b1)PLDA(w|D)]\nThe LDA-distributions are smooth by defininition, so for 0 \u2264 \u03b1 < 1 there is no zero-probability problem. Since the smoothing method applied in the query level combination (which was necessary there) is a little ad-hoc, we decide to use no such smoothing scheme in the combination on word level. However, this keeps us from calculating values for \u03b1 = 1. We performed a grid search for the parameter values of \u03b1 and \u03b2, using a step size of .05, and find the optimum at \u03b1 = .3, \u03b2 = .9 with map = .303. This value is better than any of those reported for this track in CLEF 2000 [Braschler, 2001, Hedlund et al., 2001]."}, {"heading": "5.4 Discussion", "text": "We explored three ways of using translation models based on Wikipedia links and evaluated on the CLEF2000 dataset. The first is to construct a bag-of-words representation and to do retrieval with a tf.idf-scheme, the second is to compare multinomial distribution vectors with association strengths to Wikipedia articles, and the third is based on a language modeling formulation.\nThe pure link-based bag-of-words approach could not reach the performance of vector space retrieval done with Moses machine translation output trained on Europarl. However, in combination with LDA vectors it performed better than that and almost as well as the machine translation output in the same combination. The distribution vectors showed poor performance, however, still better than the ESA vectors on the same dataset. The language modeling approach is clearly most promising, as there is much room for the exploration of different smoothing and combination schemes. We\n60"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "tried two different schemes of language model interpolation with LDA, one based on combination on query level, the other based on combination on word level. While the query level approach has fewer parameters to be set, we obtained better results with the word level approach. The language modeling setting did not only outperform all our results obtained with Moses machine translation, it also produced results that compare favorably against values reported for the CLEF 2000 ad-hoc track14."}, {"heading": "6 Conclusion", "text": "In this thesis, we compare three concept models for cross-language retrieval: First, we adapt probabilistic Latent Semantic Analysis (pLSA) for multilingual documents. Experiments with different weighting schemes show that a weighting method favoring bridging documents of equal length in both language sides gives best results. Together with the finding that both monolingual and multilingual Latent Dirichlet Allocation (LDA) behave alike when applied for such documents, we use a training corpus built on Wikipedia where all documents are length-normalized. Thus, we observe a considerable improvement over previously reported scores on the Multext JOC corpus for LDA.\nAnother focus of our work is on model combination. For this end we include Explicit Semantic Analysis (ESA) in the experiments. We observe that apart from direct translation finding, where it had been evaluated originally, ESA is not competitive with LDA in a query based retrieval task on the CLEF 2000 data set. All concept models perform significantly worse than a model based on machine translation output. However, the combination of machine translation output with concept model information increased performance by 21.1% map in comparison to machine translation alone.\nMachine translation relies on parallel corpora, which may not be available for many language pairs. Concept models like ESA or LDA only capture broad semantic relatedness on document level which might be to unspecific for proper names or other fine-grained distinctions. Therefore, we explore how much cross-lingual information can be carried over by a more specific information source in Wikipedia, namely linked text. We observe, that in a simple vector combination with LDA it already gives results comparable to, but not as good as the best result obtained in a combination run with machine translation.\n14For licensing reasons, we are not allowed to make a direct comparison, but see [Braschler, 2001] for\na survey.\n61"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "The best overall results are obtained using a language modeling approach, entirely without information from parallel corpora. The language modeling approach leaves much space for further experimentation, since combination of different models is not straightforward. The need for smoothing raises interesting questions on soundness and efficiency. Link models capture only a certain kind of information and suggest weighting schemes to emphasize particular words. For a combined model, another interesting question is therefore how to integrate different weighting schemes. Using a very simple combination scheme, we obtained values that compare favorably to results reported previously on the CLEF 2000 dataset.\nWe have shown how probabilistic techniques that work without parallel training texts can be applied to replace other cross-language retrieval methods. In principle, our methods can be employed wherever language barriers are to be overcome on text level. After all, it would be interesting to see whether our combination techniques, successful for cross-language retrieval, can also contribute to monolingual retrieval.\n62"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": ""}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "P. Cimiano, TU Delft, A. Schultz, S. Sizov, P. Sorg, and S. Staab. Explicit Versus\nLatent Concept Models for Cross-Language Information Retrieval. 2009.\nD. Cohn and T. Hofmann. The Missing Link \u2013 A Probabilistic Model of Document\nContent and Hypertext Connectivity. Advances in Neural Information Processing System, pages 430\u2013436, 2001.\nS. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. Indexing\nby latent semantic analysis. Journal of the American society for information science, 41(6):391\u2013407, 1990.\nA.P. Dempster, N.M. Laird, D.B. Rubin, et al. Maximum likelihood from incom-\nplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1\u201338, 1977.\nS.T. Dumais, T.A. Letsche, M.L. Littman, and T.K. Landauer. Automatic cross-\nlanguage retrieval using latent semantic indexing. AAAI Spring Symposuim on Cross-Language Text and Speech Retrieval, pages 115\u2013132, 1997.\nE. Gabrilovich and S. Markovitch. Computing semantic relatedness using wikipedia-\nbased explicit semantic analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 6\u201312, 2007.\nT.L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National\nAcademy of Sciences, 101(90001):5228\u20135235, 2004.\nT. Hedlund, H. Keskustalo, A. Pirkola, M. Sepponen, and K. Jarvelin. Bilingual tests\nwith Swedish, Finnish, and German queries: Dealing with morphology, compound words, and query structure. Lecture notes in computer science, pages 210\u2013223, 2001.\nD. Hiemstra. A linguistically motivated probabilistic model of information retrieval.\nLecture notes in computer science, pages 569\u2013584, 1998.\nT. Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual\ninternational ACM SIGIR conference on Research and development in information retrieval, pages 50\u201357. ACM New York, NY, USA, 1999.\nT. Hofmann. Unsupervised Learning by Probabilistic Latent Semantic Analysis. Ma-\nchine Learning, 42(1):177\u2013196, 2001.\n64"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "T. Hofmann, J. Puzicha, and MI Jordan. Unsupervised learning from dyadic data.\nAdvances in Neural Information Processing Systems, 11, 1999.\nP. Koehn. Europarl: A parallel corpus for statistical machine translation. In MT\nsummit, volume 5, 2005.\nP. Koehn, F.J. Och, and D. Marcu. Statistical phrase-based translation. In Proceed-\nings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 48\u201354. Association for Computational Linguistics Morristown, NJ, USA, 2003.\nX. Liu and W.B. Croft. Statistical language modeling for information retrieval. Annual\nReview of Information Science and Technology 2005: Volume 39, page 1, 2004.\nD.J.C. MacKay. Information theory, inference, and learning algorithms. Cambridge\nUniv Pr, 2003.\nK. Maeda, X. Ma, and S. Strassel. Creating Sentence-Aligned Parallel Text Corpora\nfrom a Large Archive of Potential Parallel Text using BITS and Champollion. In Proceedings of the Sixth Language Resources and Evaluation Conference, pages 26\u2013 30, 2008.\nR. Mihalcea and A. Csomai. Wikify!: linking documents to encyclopedic knowledge.\nIn Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 233\u2013242. ACM New York, NY, USA, 2007.\nD.R.H. Miller, T. Leek, and R.M. Schwartz. A hidden Markov model information\nretrieval system. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, page 221. ACM, 1999.\nDavid Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew\nMcCallum. Polylingual Topic Models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880\u2013889, 2009.\nD. Newman, A. Asuncion, P. Smyth, and M. Welling. Distributed inference for latent\ndirichlet allocation. Advances in Neural Information Processing Systems, 20:1081\u2013 1088, 2007.\n65"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "M.E.J. Newman and GT Barkema. Monte Carlo methods in statistical physics. Oxford\nUniversity Press, USA, 1999.\nD. Nguyen, A. Overwijk, C. Hauff, RB Trieschnigg, D. Hiemstra, and F.M.G. de Jong.\nWikiTranslate: Query Translation for Cross-lingual Information Retrieval using only Wikipedia. In Evaluating Systems for Multilingual and Multimodal Information Access: 9th Workshop of the Cross-Language Evaluation Forum, CLEF 2008, Aarhus, Denmark, September 17-19, 2008, Revised Selected Papers, page 58. Springer, 2009.\nX. Ni, J.T. Sun, J. Hu, and Z. Chen. Mining multilingual topics from wikipedia. In\nProceedings of the 18th international conference on World wide web, pages 1155\u2013 1156. ACM New York, NY, USA, 2009.\nJ.S. Olsson, D.W. Oard, and J. Hajic\u030c. Cross-language text classification. In Pro-\nceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 645\u2013646. ACM New York, NY, USA, 2005.\nC. Peters and M. Braschler. European Research Letter: cross-language system eval-\nuation: the CLEF campaigns. Journal of the American Society for Information Science and Technology, 52(12), 2001.\nJ.M. Ponte and W.B. Croft. A language modeling approach to information retrieval.\nIn Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, pages 275\u2013281. ACM New York, NY, USA, 1998.\nM. Potthast, B. Stein, and M. Anderka. A wikipedia-based multilingual retrieval\nmodel. Lecture Notes in Computer Science, 4956:522, 2008.\nSE Robertson, S. Walker, S. Jones, M. Gatford, MM Hancock-Beaulieu, N. Square,\nand E.C.V. London. Okapi at TREC-3. In Overview of the Third Text REtrieval Conference (TREC-3), page 109. Diane Pub Co, 1995.\nG. Salton and C. Buckley. Term weighting approaches in automatic text retrieval.\n1987.\nJ. Sjobergh, O. Sjobergh, and K. Araki. What Types of Translations Hide in\nWikipedia? Lecture Notes in Computer Science, 4938:59, 2008.\n66"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "F. Song and W.B. Croft. A general language model for information retrieval. In\nProceedings of the eighth international conference on Information and knowledge management, pages 316\u2013321. ACM New York, NY, USA, 1999.\nP. Sorg and P. Cimiano. Cross-lingual information retrieval with explicit semantic\nanalysis. In Working Notes of the Annual CLEF Meeting, 2008.\nP. Sorg and P. Cimiano. An Experimental Comparison of Explicit Semantic Analysis\nImplementations for Cross-Language Retrieval. In Proceedings of the 14th International Conference on Applications of Natural Language to Information Systems (NLDB\u201909), 2009.\nM. Steyvers and T. Griffiths. Probabilistic topic models. Handbook of Latent Semantic\nAnalysis, page 427, 2007.\nN. Ueda and R. Nakano. Deterministic annealing EM algorithm. Neural Networks, 11\n(2):271\u2013282, 1998.\nYi Wang, Hongjie Bai, Matt Stanton, Wen-Yen Chen, and Edward Y. Chang. Plda:\nParallel latent dirichlet allocation for large-scale applications. In Proc. of 5th International Conference on Algorithmic Aspects in Information and Management, 2009. Software available at http://code.google.com/p/plda.\nX. Wei and W.B. Croft. LDA-based document models for ad-hoc retrieval. In Pro-\nceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval, pages 178\u2013185. ACM New York, NY, USA, 2006.\nW. Wentland, J. Knopp, C. Silberer, and M. Hartung. Building a multilingual lexical\nresource for named entity disambiguation, translation and transliteration. Proc. of LREC08, 2008.\nSKMWong, W. Ziarko, and P.C.N. Wong. Generalized vector spaces model in informa-\ntion retrieval. In Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval, pages 18\u201325. ACM New York, NY, USA, 1985.\nC.F.J. Wu. On the convergence properties of the EM algorithm. The Annals of\nStatistics, 11(1):95\u2013103, 1983.\n67"}, {"heading": "Wikipedia-Based Cross-Language Retrieval Models Benjamin Roth", "text": "Y. Wu and D.W. Oard. Bilingual topic aspect classification with a few training ex-\namples. In Proceedings of the 31st annual international ACM SIGIR conference on research and development in information retrieval, pages 203\u2013210. ACM New York, NY, USA, 2008.\nC. Zhai and J. Lafferty. A study of smoothing methods for language models applied\nto information retrieval. ACM Transactions on Information Systems (TOIS), 22(2): 214, 2004.\n68"}], "references": [{"title": "Finding similar sentences across multiple languages in wikipedia", "author": ["S.F. Adafre", "M. de Rijke"], "venue": "In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Adafre and Rijke.,? \\Q2006\\E", "shortCiteRegEx": "Adafre and Rijke.", "year": 2006}, {"title": "Topic based language models for ad hoc information retrieval", "author": ["L. Azzopardi", "M. Girolami", "CJ Van Rijsbergen"], "venue": "IEEE International Joint Conference on Neural Networks,", "citeRegEx": "Azzopardi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Azzopardi et al\\.", "year": 2004}, {"title": "Enhancing Multilingual Latent Semantic Analysis with Term Alignment Information", "author": ["B.W. Bader", "P.A. Chew"], "venue": "Proceedings of COLING", "citeRegEx": "Bader and Chew.,? \\Q2008\\E", "shortCiteRegEx": "Bader and Chew.", "year": 2008}, {"title": "A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hiddenMarkov models", "author": ["J.A. Bilmes"], "venue": "International Computer Science Institute,", "citeRegEx": "Bilmes.,? \\Q1998\\E", "shortCiteRegEx": "Bilmes.", "year": 1998}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "Springer New York:,", "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Multilingual Topic Models for Unaligned Text", "author": ["J. Boyd-Graber", "D.M. Blei"], "venue": "The 25th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Boyd.Graber and Blei.,? \\Q2009\\E", "shortCiteRegEx": "Boyd.Graber and Blei.", "year": 2009}, {"title": "CLEF 2000-Overview of results. In Cross-language information retrieval and evaluation: workshop of the Cross-Language Evaluation Forum, CLEF", "author": ["M. Braschler"], "venue": null, "citeRegEx": "Braschler.,? \\Q2000\\E", "shortCiteRegEx": "Braschler.", "year": 2000}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Computer Speech and Language,", "citeRegEx": "Chen and Goodman.,? \\Q1999\\E", "shortCiteRegEx": "Chen and Goodman.", "year": 1999}, {"title": "Benefits of the Passively Parallel Rosetta Stone? CrossLanguage Information Retrieval with over 30 Languages", "author": ["P. Chew", "A. Abdelali"], "venue": "In ANNUAL MEETINGASSOCIATION FOR COMPUTATIONAL LINGUISTICS,", "citeRegEx": "Chew and Abdelali.,? \\Q2007\\E", "shortCiteRegEx": "Chew and Abdelali.", "year": 2007}, {"title": "Explicit Versus Latent Concept Models for Cross-Language Information Retrieval", "author": ["P. Cimiano", "TU Delft", "A. Schultz", "S. Sizov", "P. Sorg", "S. Staab"], "venue": null, "citeRegEx": "Cimiano et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cimiano et al\\.", "year": 2009}, {"title": "The Missing Link \u2013 A Probabilistic Model of Document Content and Hypertext Connectivity", "author": ["D. Cohn", "T. Hofmann"], "venue": "Advances in Neural Information Processing System,", "citeRegEx": "Cohn and Hofmann.,? \\Q2001\\E", "shortCiteRegEx": "Cohn and Hofmann.", "year": 2001}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Automatic crosslanguage retrieval using latent semantic indexing", "author": ["S.T. Dumais", "T.A. Letsche", "M.L. Littman", "T.K. Landauer"], "venue": "AAAI Spring Symposuim on Cross-Language Text and Speech Retrieval,", "citeRegEx": "Dumais et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 1997}, {"title": "Computing semantic relatedness using wikipediabased explicit semantic analysis", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Gabrilovich and Markovitch.,? \\Q2007\\E", "shortCiteRegEx": "Gabrilovich and Markovitch.", "year": 2007}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths and Steyvers.,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Bilingual tests with Swedish, Finnish, and German queries: Dealing with morphology, compound words, and query structure", "author": ["T. Hedlund", "H. Keskustalo", "A. Pirkola", "M. Sepponen", "K. Jarvelin"], "venue": "Lecture notes in computer science,", "citeRegEx": "Hedlund et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hedlund et al\\.", "year": 2001}, {"title": "A linguistically motivated probabilistic model of information retrieval", "author": ["D. Hiemstra"], "venue": "Lecture notes in computer science,", "citeRegEx": "Hiemstra.,? \\Q1998\\E", "shortCiteRegEx": "Hiemstra.", "year": 1998}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Unsupervised Learning by Probabilistic Latent Semantic Analysis", "author": ["T. Hofmann"], "venue": "Machine Learning,", "citeRegEx": "Hofmann.,? \\Q2001\\E", "shortCiteRegEx": "Hofmann.", "year": 2001}, {"title": "Unsupervised learning from dyadic data", "author": ["T. Hofmann", "J. Puzicha", "MI Jordan"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Hofmann et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 1999}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["P. Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Statistical language modeling for information retrieval", "author": ["X. Liu", "W.B. Croft"], "venue": "Annual Review of Information Science and Technology 2005: Volume 39,", "citeRegEx": "Liu and Croft.,? \\Q2004\\E", "shortCiteRegEx": "Liu and Croft.", "year": 2004}, {"title": "Information theory, inference, and learning algorithms", "author": ["D.J.C. MacKay"], "venue": "Cambridge Univ Pr,", "citeRegEx": "MacKay.,? \\Q2003\\E", "shortCiteRegEx": "MacKay.", "year": 2003}, {"title": "Creating Sentence-Aligned Parallel Text Corpora from a Large Archive of Potential Parallel Text using BITS and Champollion", "author": ["K. Maeda", "X. Ma", "S. Strassel"], "venue": "In Proceedings of the Sixth Language Resources and Evaluation Conference,", "citeRegEx": "Maeda et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maeda et al\\.", "year": 2008}, {"title": "Wikify!: linking documents to encyclopedic knowledge", "author": ["R. Mihalcea", "A. Csomai"], "venue": "In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,", "citeRegEx": "Mihalcea and Csomai.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea and Csomai.", "year": 2007}, {"title": "A hidden Markov model information retrieval system", "author": ["D.R.H. Miller", "T. Leek", "R.M. Schwartz"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Miller et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1999}, {"title": "Polylingual Topic Models", "author": ["David Mimno", "Hanna M. Wallach", "Jason Naradowsky", "David A. Smith", "Andrew McCallum"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mimno et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2009}, {"title": "Distributed inference for latent dirichlet allocation", "author": ["D. Newman", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Newman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2007}, {"title": "Monte Carlo methods in statistical physics", "author": ["M.E.J. Newman", "GT Barkema"], "venue": null, "citeRegEx": "Newman and Barkema.,? \\Q1999\\E", "shortCiteRegEx": "Newman and Barkema.", "year": 1999}, {"title": "WikiTranslate: Query Translation for Cross-lingual Information Retrieval using only Wikipedia", "author": ["D. Nguyen", "A. Overwijk", "C. Hauff", "RB Trieschnigg", "D. Hiemstra", "F.M.G. de Jong"], "venue": "Revised Selected Papers,", "citeRegEx": "Nguyen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2008}, {"title": "Mining multilingual topics from wikipedia", "author": ["X. Ni", "J.T. Sun", "J. Hu", "Z. Chen"], "venue": "In Proceedings of the 18th international conference on World wide web,", "citeRegEx": "Ni et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ni et al\\.", "year": 2009}, {"title": "Haji\u010d. Cross-language text classification", "author": ["J.S. Olsson", "D.W. Oard"], "venue": "In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Olsson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Olsson et al\\.", "year": 2005}, {"title": "European Research Letter: cross-language system evaluation: the CLEF campaigns", "author": ["C. Peters", "M. Braschler"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Peters and Braschler.,? \\Q2001\\E", "shortCiteRegEx": "Peters and Braschler.", "year": 2001}, {"title": "A language modeling approach to information retrieval", "author": ["J.M. Ponte", "W.B. Croft"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Ponte and Croft.,? \\Q1998\\E", "shortCiteRegEx": "Ponte and Croft.", "year": 1998}, {"title": "A wikipedia-based multilingual retrieval model", "author": ["M. Potthast", "B. Stein", "M. Anderka"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Potthast et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Potthast et al\\.", "year": 2008}, {"title": "Okapi at TREC-3", "author": ["SE Robertson", "S. Walker", "S. Jones", "M. Gatford", "MM Hancock-Beaulieu", "N. Square", "E.C.V. London"], "venue": "In Overview of the Third Text REtrieval Conference", "citeRegEx": "Robertson et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Robertson et al\\.", "year": 1995}, {"title": "Term weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": null, "citeRegEx": "Salton and Buckley.,? \\Q1987\\E", "shortCiteRegEx": "Salton and Buckley.", "year": 1987}, {"title": "What Types of Translations Hide in Wikipedia", "author": ["J. Sjobergh", "O. Sjobergh", "K. Araki"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Sjobergh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sjobergh et al\\.", "year": 2008}, {"title": "A general language model for information retrieval", "author": ["F. Song", "W.B. Croft"], "venue": "In Proceedings of the eighth international conference on Information and knowledge management,", "citeRegEx": "Song and Croft.,? \\Q1999\\E", "shortCiteRegEx": "Song and Croft.", "year": 1999}, {"title": "Cross-lingual information retrieval with explicit semantic analysis", "author": ["P. Sorg", "P. Cimiano"], "venue": "In Working Notes of the Annual CLEF Meeting,", "citeRegEx": "Sorg and Cimiano.,? \\Q2008\\E", "shortCiteRegEx": "Sorg and Cimiano.", "year": 2008}, {"title": "An Experimental Comparison of Explicit Semantic Analysis Implementations for Cross-Language Retrieval", "author": ["P. Sorg", "P. Cimiano"], "venue": "In Proceedings of the 14th International Conference on Applications of Natural Language to Information Systems (NLDB\u201909),", "citeRegEx": "Sorg and Cimiano.,? \\Q2009\\E", "shortCiteRegEx": "Sorg and Cimiano.", "year": 2009}, {"title": "Probabilistic topic models", "author": ["M. Steyvers", "T. Griffiths"], "venue": "Handbook of Latent Semantic Analysis,", "citeRegEx": "Steyvers and Griffiths.,? \\Q2007\\E", "shortCiteRegEx": "Steyvers and Griffiths.", "year": 2007}, {"title": "Deterministic annealing EM algorithm", "author": ["N. Ueda", "R. Nakano"], "venue": "Neural Networks,", "citeRegEx": "Ueda and Nakano.,? \\Q1998\\E", "shortCiteRegEx": "Ueda and Nakano.", "year": 1998}, {"title": "Plda: Parallel latent dirichlet allocation for large-scale applications", "author": ["Yi Wang", "Hongjie Bai", "Matt Stanton", "Wen-Yen Chen", "Edward Y. Chang"], "venue": "In Proc. of 5th International Conference on Algorithmic Aspects in Information and Management,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "LDA-based document models for ad-hoc retrieval", "author": ["X. Wei", "W.B. Croft"], "venue": "In Proceedings of the 29th annual international ACM SIGIR conference on research and development in information retrieval,", "citeRegEx": "Wei and Croft.,? \\Q2006\\E", "shortCiteRegEx": "Wei and Croft.", "year": 2006}, {"title": "Building a multilingual lexical resource for named entity disambiguation, translation and transliteration", "author": ["W. Wentland", "J. Knopp", "C. Silberer", "M. Hartung"], "venue": "Proc. of LREC08,", "citeRegEx": "Wentland et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wentland et al\\.", "year": 2008}, {"title": "Generalized vector spaces model in information retrieval", "author": ["SKMWong", "W. Ziarko", "P.C.N. Wong"], "venue": "In Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "SKMWong et al\\.,? \\Q1985\\E", "shortCiteRegEx": "SKMWong et al\\.", "year": 1985}, {"title": "On the convergence properties of the EM algorithm", "author": ["C.F.J. Wu"], "venue": "The Annals of Statistics,", "citeRegEx": "Wu.,? \\Q1983\\E", "shortCiteRegEx": "Wu.", "year": 1983}, {"title": "Bilingual topic aspect classification with a few training examples", "author": ["Y. Wu", "D.W. Oard"], "venue": "In Proceedings of the 31st annual international ACM SIGIR conference on research and development in information retrieval,", "citeRegEx": "Wu and Oard.,? \\Q2008\\E", "shortCiteRegEx": "Wu and Oard.", "year": 2008}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "Zhai and Lafferty.,? \\Q2004\\E", "shortCiteRegEx": "Zhai and Lafferty.", "year": 2004}], "referenceMentions": [{"referenceID": 12, "context": "Such a representation may, for example, be obtained by matrix approximation [Deerwester et al., 1990], by probabilistic inference [Steyvers and Griffiths, 2007] or by techniques making use of the conceptual structure of corpora such as Wikipedia [Gabrilovich and Markovitch, 2007].", "startOffset": 76, "endOffset": 101}, {"referenceID": 44, "context": ", 1990], by probabilistic inference [Steyvers and Griffiths, 2007] or by techniques making use of the conceptual structure of corpora such as Wikipedia [Gabrilovich and Markovitch, 2007].", "startOffset": 36, "endOffset": 66}, {"referenceID": 15, "context": ", 1990], by probabilistic inference [Steyvers and Griffiths, 2007] or by techniques making use of the conceptual structure of corpora such as Wikipedia [Gabrilovich and Markovitch, 2007].", "startOffset": 152, "endOffset": 186}, {"referenceID": 12, "context": "The most simple models of that kind work directly on bag-ofword vectors and are prone to sparseness and missing word overlap, some backing-off can be achieved by dimensionality reduction techniques such as Latent Semantic Analysis [Deerwester et al., 1990] or term expansion as in the generalized vector space model [Wong et al.", "startOffset": 231, "endOffset": 256}, {"referenceID": 39, "context": "idf scheme [Salton and Buckley, 1987].", "startOffset": 11, "endOffset": 37}, {"referenceID": 38, "context": "Similar starting points and theoretical properties exhibit tuned ranking functions like the popular Okapi-BM25 family [Robertson et al., 1995], which are still achieving state-of-the-art results [Armstrong et al.", "startOffset": 118, "endOffset": 142}, {"referenceID": 36, "context": "2 Language Modeling Retrieval Language model based retrieval is a newer paradigm [Ponte and Croft, 1998], where usually documents are treated as providing probabilistic models of generating the queries, and the document that provides the best model is regarded as most relevant.", "startOffset": 81, "endOffset": 104}, {"referenceID": 24, "context": "See [Liu and Croft, 2004] for an excellent overview.", "startOffset": 4, "endOffset": 25}, {"referenceID": 20, "context": "The two most prominent latent variable models are probabilistic Latent Semantic Analysis [Hofmann, 2001], which implements the ideas of variable modeling in a straightforward way but does not define inference on documents not present in the training collection, and, developed some years later, Latent Dirichlet Allocation [Blei et al.", "startOffset": 89, "endOffset": 104}, {"referenceID": 5, "context": "The two most prominent latent variable models are probabilistic Latent Semantic Analysis [Hofmann, 2001], which implements the ideas of variable modeling in a straightforward way but does not define inference on documents not present in the training collection, and, developed some years later, Latent Dirichlet Allocation [Blei et al., 2003], which defines a fully generative model", "startOffset": 323, "endOffset": 342}, {"referenceID": 46, "context": "and is also favorable in terms of scalability [Wang et al., 2009].", "startOffset": 46, "endOffset": 65}, {"referenceID": 22, "context": "Parallel corpora, such as the Europarl corpus of EU-Parliament proceedings [Koehn, 2005], provide translations of the same text in different languages, corresponding phrase pairs can be extracted automatically [Koehn et al.", "startOffset": 75, "endOffset": 88}, {"referenceID": 23, "context": "Parallel corpora, such as the Europarl corpus of EU-Parliament proceedings [Koehn, 2005], provide translations of the same text in different languages, corresponding phrase pairs can be extracted automatically [Koehn et al., 2003].", "startOffset": 210, "endOffset": 230}, {"referenceID": 14, "context": "The first approach [Dumais et al., 1997] to using comparable corpora and dimensionality reduction was done with Latent Semantic Analysis (LSA), a technique making use of lower-rank matrix approximation by singular value decomposition.", "startOffset": 19, "endOffset": 40}, {"referenceID": 12, "context": "As an effect, terms and documents are represented by lower-dimensional vectors, making them topically comparable [Deerwester et al., 1990].", "startOffset": 113, "endOffset": 138}, {"referenceID": 10, "context": "While the method works well and is still used as a baseline [Cimiano et al., 2009], it lacks, as does monolingual LSA, a probabilistic interpretation and is hardly scalable, which becomes a major problem when it comes to embedding gigabyte-sized corpora such as Wikipedia.", "startOffset": 60, "endOffset": 82}, {"referenceID": 10, "context": "While the method works well and is still used as a baseline [Cimiano et al., 2009], it lacks, as does monolingual LSA, a probabilistic interpretation and is hardly scalable, which becomes a major problem when it comes to embedding gigabyte-sized corpora such as Wikipedia. A more modern approach could be based on probabilistic Latent Semantic Analysis (pLSA Hofmann [2001]).", "startOffset": 61, "endOffset": 374}, {"referenceID": 10, "context": "LDA experiments for cross-lingual IR have been published [Cimiano et al., 2009] with disappointing outcomes compared to other techniques.", "startOffset": 57, "endOffset": 79}, {"referenceID": 29, "context": "[2009] for web-page classification (see also [Mimno et al., 2009]).", "startOffset": 45, "endOffset": 65}, {"referenceID": 10, "context": "The evaluation results on the method are inconsistent: while it has been found to work well on mate-retrieval tasks in a multilingual setting and is reported to surpass LDA there [Cimiano et al., 2009], in an actual query-based document retrieval setting it showed no competitive performance [Sorg and Cimiano, 2008].", "startOffset": 179, "endOffset": 201}, {"referenceID": 42, "context": ", 2009], in an actual query-based document retrieval setting it showed no competitive performance [Sorg and Cimiano, 2008].", "startOffset": 98, "endOffset": 122}, {"referenceID": 20, "context": "These findings may be explained in different ways: for the first finding there might have been problems in how LDA was applied and compared, for the second finding, no interpolation with a word-vector based representation was undertaken, as it is usually done for dimensionality reduction techniques such as LSA and pLSA [Hofmann, 2001].", "startOffset": 321, "endOffset": 336}, {"referenceID": 10, "context": "LDA experiments for cross-lingual IR have been published [Cimiano et al., 2009] with disappointing outcomes compared to other techniques. These results do not convincingly show the inappropriateness of LDA, as the authors have embedded multilingual Wikipedia articles without any length normalization or model adaptation. For a method that tries to model characteristics of term statistics, the biggest increase in data-likelihood with limited model capacities should be expected by effectively capturing the predominant language vocabulary instead of doing topical modeling. An indicator that this effect might indeed be at work is that in another run of experiments, trained with in-domain data that have almost constant length ratio (because they are parallel), LDA performs as well as the other methods. Dealing with the supposed length ratio problem could be done either by processing the data accordingly or by adapting the model in a similar fashion as suggested by Ni et al. [2009] for web-page classification (see also [Mimno et al.", "startOffset": 58, "endOffset": 990}, {"referenceID": 2, "context": "In [Bader and Chew, 2008] the fact is used that SVD of a matrix X can be computed by the Eigenvalue Decomposition of the composite matrix", "startOffset": 3, "endOffset": 25}, {"referenceID": 11, "context": "Cohn and Hofmann [2001] use pLSA to include as an additional source of information links to a document (from another document).", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "Secondly, to us it is not evident that the EM-re-estimation equations in Cohn and Hofmann [2001], while looking reasonable at a first glance, really derive from the initial problem statement.", "startOffset": 73, "endOffset": 97}, {"referenceID": 6, "context": "An interesting alternative based on LDA has been proposed recently by Boyd-Graber and Blei [2009]. Here, multingual pairings of words are extracted from parallel or comparable corpora in an unsupervised way and assigned to a topic.", "startOffset": 70, "endOffset": 98}, {"referenceID": 35, "context": "General competitions like the Cross-Language Evaluation Forum (CLEF, [Peters and Braschler, 2001]) as well as datasets allowing for more specific comparisons are to be taken into consideration.", "startOffset": 69, "endOffset": 97}, {"referenceID": 6, "context": ", 2008]), translated news collections and parliament debates (as in [Boyd-Graber and Blei, 2009]), the Official Journal of the European Community and european law texts (as in [Potthast et al.", "startOffset": 68, "endOffset": 96}, {"referenceID": 10, "context": "For our experiments, we evaluate on two datasets: To establish comparability with the findings on LDA and ESA in [Cimiano et al., 2009] and [Sorg and Cimiano, 2009], an evaluation on the Multext JOC corpus is undertaken.", "startOffset": 113, "endOffset": 135}, {"referenceID": 43, "context": ", 2009] and [Sorg and Cimiano, 2009], an evaluation on the Multext JOC corpus is undertaken.", "startOffset": 12, "endOffset": 36}, {"referenceID": 12, "context": "From the equivalent symmetric factorization given in equation 4 some parallels to Latent Semantic Analysis [Deerwester et al., 1990], based on truncated singular value decomposition, can be drawn: the values P (di|\u00b7) and P (wj |\u00b7) play a role similar to that of the left and right singular vectors: in both cases co-occurrence matrices can be constructed from them for each of the assumed topics; P (zk) corresponds to singular values by weighting the topical co-occurrence matrices.", "startOffset": 107, "endOffset": 132}, {"referenceID": 20, "context": "For standard pLSA, we refer to [Hofmann, 2001] for a full derivation.", "startOffset": 31, "endOffset": 46}, {"referenceID": 5, "context": "2 Sampling for LDA The first approach to estimate such a model [Blei et al., 2003] was to represent and estimate \u03c8 and \u03b8 explicitly, resulting in different inference tasks to be solved and combined.", "startOffset": 63, "endOffset": 82}, {"referenceID": 16, "context": "Later approaches concentrate on getting a sample of the assignment of words to topics instead [Griffiths and Steyvers, 2004].", "startOffset": 94, "endOffset": 124}, {"referenceID": 16, "context": "3 Practical Issues To determine the similarity between two documents, one can compare either their sampled topic vectors or the probability vectors obtained from them [Griffiths and Steyvers, 2004].", "startOffset": 167, "endOffset": 197}, {"referenceID": 5, "context": ", 2009, Blei et al., 2003]. The comparison between these vectors can be done either by taking the cosine similarity of their angles or, in case the vector is indeed a probability distribution (as for \u03b8), by using probability divergence measures, such as the (symmetrized) Kullback Leibler divergence or the Jensen-Shannon divergence. For language model based information retrieval, one is interested in the probability of a query, given a document p(q|di). Wei and Croft [2006] interpolate a language model based on LDA with a unigram language model directly estimated on the document.", "startOffset": 8, "endOffset": 478}, {"referenceID": 33, "context": "4 Multilingual LDA LDA has been extended for several languages [Ni et al., 2009], see also [Mimno et al.", "startOffset": 63, "endOffset": 80}, {"referenceID": 29, "context": ", 2009], see also [Mimno et al., 2009] for an investigation of the semantic clustering of this model.", "startOffset": 18, "endOffset": 38}, {"referenceID": 29, "context": "It is crucial [Mimno et al., 2009] how many \u201cglue documents\u201d, i.", "startOffset": 14, "endOffset": 34}, {"referenceID": 15, "context": "The word vectors that were used in [Gabrilovich and Markovitch, 2007] and found to be optimal in [Sorg and Cimiano, 2009] are obtained by taking the respective columns of the tf.", "startOffset": 35, "endOffset": 69}, {"referenceID": 43, "context": "The word vectors that were used in [Gabrilovich and Markovitch, 2007] and found to be optimal in [Sorg and Cimiano, 2009] are obtained by taking the respective columns of the tf.", "startOffset": 97, "endOffset": 121}, {"referenceID": 15, "context": "In [Gabrilovich and Markovitch, 2007] a weigting of the word vectors is used, they are multiplied with scalars equal to, again, an tf.", "startOffset": 3, "endOffset": 37}, {"referenceID": 15, "context": "In [Gabrilovich and Markovitch, 2007] a weigting of the word vectors is used, they are multiplied with scalars equal to, again, an tf.idf weighting of the terms, and then summed up. It is however not very clearly described which exact instantiation of the tf.idf function was used in the experiments. Sorg and Cimiano [2009] explore further combination schemes, including the sum of the elements of either the multiset (considering term frequency) or of the set (not considering term frequency) of word vectors.", "startOffset": 4, "endOffset": 325}, {"referenceID": 42, "context": "In Sorg and Cimiano [2009] it was found that a cut-off value of c = 10000 works best.", "startOffset": 3, "endOffset": 27}, {"referenceID": 42, "context": "Sorg and Cimiano [2008] observe that even when the ESA model is used to compare documents taken from two languages out of the set French, German and English it slightly improves retrieval performance to restrict the training corpus to documents available in all of the three languages.", "startOffset": 0, "endOffset": 24}, {"referenceID": 10, "context": "How does a monolingual LDA model perform on multilingual data \u2022 in the form reported in [Cimiano et al., 2009]? \u2022 when using a scheme that takes theoretical considerations into account? 4.", "startOffset": 88, "endOffset": 110}, {"referenceID": 19, "context": "\ufffdK k=1 P (zk|d (2))2 This ranking scheme was applied and found to work best by Hofmann [2001] for a monolingual retrieval task and standard pLSA.", "startOffset": 79, "endOffset": 94}, {"referenceID": 46, "context": "The highly efficient and parallel LDA implementation plda [Wang et al., 2009] was used.", "startOffset": 58, "endOffset": 77}, {"referenceID": 16, "context": "We used the Dirichlet parameter values suggested by Griffiths and Steyvers [2004], \u03b1 = 50 (number of topics) and \u03b2 = 0.", "startOffset": 52, "endOffset": 82}, {"referenceID": 10, "context": "For LDA we performed three types of experiments: First, we sampled a model for the comparable documents with standard LDA without any length normalization, in the manner of [Cimiano et al., 2009].", "startOffset": 173, "endOffset": 195}, {"referenceID": 42, "context": "As in Sorg and Cimiano [2008] we use the concatenation of a question together with its answer as a query in one language to search the collection of translations in another language for its counterpart.", "startOffset": 6, "endOffset": 30}, {"referenceID": 42, "context": "As in Sorg and Cimiano [2008] we use the concatenation of a question together with its answer as a query in one language to search the collection of translations in another language for its counterpart. Our experiments were done with English as the query language and German as the target language. Only preprocessing steps that are clear and easy to reproduce were performed. Exactly those questions were retained that to which an answer was assigned and had the same id in English and German. This resulted in a set of 3212 texts in each language, 157 more than were used in Sorg and Cimiano [2008]8.", "startOffset": 6, "endOffset": 601}, {"referenceID": 42, "context": "As in Sorg and Cimiano [2008] we use the concatenation of a question together with its answer as a query in one language to search the collection of translations in another language for its counterpart. Our experiments were done with English as the query language and German as the target language. Only preprocessing steps that are clear and easy to reproduce were performed. Exactly those questions were retained that to which an answer was assigned and had the same id in English and German. This resulted in a set of 3212 texts in each language, 157 more than were used in Sorg and Cimiano [2008]8. Sequences of characters in Unicode letter blocks were considered words. Words with length = 1 or length > 64 and words contained in the Snowball stopword list were ignored. All other words were stemmed with the publicly available Snowball stemmer9. In contrast to Sorg and Cimiano [2008], no compound splitting was done.", "startOffset": 6, "endOffset": 891}, {"referenceID": 42, "context": "The ESA retrieval experiment was performed using the same parametrization as discribed before and the result of Sorg and Cimiano [2008] was reproduced to a difference of 1% (in our experiments we obtained a score of mrr = 0.", "startOffset": 112, "endOffset": 136}, {"referenceID": 46, "context": "The Google plda package Wang et al. [2009] was used with the suggested parameters (\u03b1 = 50 #topics and \u03b2 = 0.", "startOffset": 24, "endOffset": 43}, {"referenceID": 42, "context": "A drastic improvement over non-normalized LDA can be observed: while Sorg and Cimiano [2008] report a score of mrr = 0.", "startOffset": 69, "endOffset": 93}, {"referenceID": 19, "context": "Another improvement can be observed by combining the results of different models, a technique that is usually applied for pLSA Hofmann [2001]. In this case, the cosine scores of runs with different dimensional models were simply averaged (this corresponds to concatenating the L2-norm normalized sampling statistics vectors).", "startOffset": 127, "endOffset": 142}, {"referenceID": 31, "context": ", 2008, Nguyen et al., 2009]. However, Sjobergh et al. [2008] note that the vocabulary distribution of titles has a skew to certain words: 33% are normal nouns, 66% proper nouns and only 0.", "startOffset": 8, "endOffset": 62}, {"referenceID": 27, "context": "P (wF |l = false, wE) = P (wF |LMF ) In [Mihalcea and Csomai, 2007] several methods were tried to predict whether a word was linked or not.", "startOffset": 40, "endOffset": 67}, {"referenceID": 36, "context": "3 Language Modelling Experiments In a query likelihood model [Ponte and Croft, 1998] a document provides a probabilistic model for a query.", "startOffset": 61, "endOffset": 84}, {"referenceID": 47, "context": "The third, PLDA(Q|D), estimates the probability according to the estimates obtained by the LDA models, in the same way as done in [Wei and Croft, 2006] for monolingual retrieval.", "startOffset": 130, "endOffset": 151}], "year": 0, "abstractText": null, "creator": "cairo 1.10.2 (http://cairographics.org)"}}}