{"id": "1401.3443", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Computational Logic Foundations of KGP Agents", "abstract": "this paper presents the computational logic foundations of a model of agency called the kgp ( knowledge, goals and functional plan model. this model allows the specification of heterogeneous agents that can interact with each other, and can exhibit both proactive and reactive behaviour allowing them to function in dynamic environments by adjusting their goals and plans when changes happen in such environments. kgp provides applications a highly modular agent architecture that integrates a typical collection of reasoning and physical capabilities, synthesised within transitions that update the agents state in response to reasoning, sensing and direct acting. transitions are orchestrated by cycle theories ) that specify the fundamental order in which functional transitions are executed while taking into account accurately the external dynamic context and agent strategy preferences, as well as selection operators for providing inputs to transitions.", "histories": [["v1", "Wed, 15 Jan 2014 04:54:59 GMT  (401kb)", "http://arxiv.org/abs/1401.3443v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["antonis kakas", "paolo mancarella", "fariba sadri", "kostas stathis", "francesca toni"], "accepted": false, "id": "1401.3443"}, "pdf": {"name": "1401.3443.pdf", "metadata": {"source": "CRF", "title": "Computational Logic Foundations of KGP Agents", "authors": ["Antonis Kakas", "Paolo Mancarella", "Fariba Sadri", "Kostas Stathis", "Francesca Toni"], "emails": ["antonis@ucy.ac.cy", "paolo.mancarella@unipi.it", "fs@doc.ic.ac.uk", "kostas@cs.rhul.ac.uk", "ft@doc.ic.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "It is widely acknowledged that the concept of agency provides a convenient and powerful abstraction to describe complex software entities acting with a certain degree of autonomy to accomplish tasks, often on behalf of a user (Wooldridge, 2002). An agent in this context is understood as a software component with capabilities such as reacting, planning and (inter) acting to achieve its goals in the environment in which it is situated. In this paper, we present a model of agency, called KGP (Knowledge, Goals and Plan). The model is hierarchical and highly modular, allowing independent specifications of a collection of reasoning and physical capabilities, used to equip an agent with intelligent decision making and adaptive behaviour. The model is particularly suited to open, dynamic environments where the agents have to adapt to changes in their environment and they have to function in circumstances where information is incomplete.\nc\u00a92008 AI Access Foundation. All rights reserved.\nThe development of the KGP model was originally motivated by the existing gap between modal logic specifications (Rao & Georgeff, 1991) of BDI agents (Bratman, Israel, & Pollack, 1988) and their implementation (for example see the issues raised by Rao, 1996). Another motivation for the development of KGP comes from our participation in the SOCS project (SOCS, 2007), where we had the need for an agent model that satisfies several requirements. More specifically, we aimed at an agent model that was rich enough to allow intelligent, adaptive and heterogeneous behaviour, formal so that it could lent itself well to formal analysis, and implementable in such a way that the implementation was sufficiently close to the formal specification to allow verification. Although several models of agency have been proposed, none satisfies all of the above requirements at once.\nTo bridge the gap between specification and implementation the KGP model is based on computational logic (CL). The focus of the work is to extend and synthesise a number of useful computational logic techniques to produce formal and executable specifications of agents. For this purpose, the model integrates abductive logic programming (ALP) (Kakas, Kowalski, & Toni, 1992), logic programming with priorities (Kakas, Mancarella, & Dung, 1994; Prakken & Sartor, 1997) and constraint logic programming (Jaffar & Maher, 1994). Each of these techniques has been explored in its own right, but their modular integration within the KGP model explores extensions of each, as well as providing the high level agent reasoning capabilities.\nThe KGP model provides a hierarchical architecture for agents. It specifies a collection of modular knowledge bases, each formalised in CL. These knowledge bases support a collection of reasoning capabilities, such as planning, reactivity, and goal decision, all of which are given formal specifications. The model also includes a specification of physical capabilities, comprising of sensing and actuating. The capabilities are utilised within transitions, that model how the state of the agent changes as a result of its reasoning, sensing and acting. Transitions use selection operators providing them with inputs. A control component, called cycle theory, also formalised in CL, specifies in what order the transitions are executed, depending on the environment, the state of the agent, and the preferences of the agent. The cycle theory takes the agent control beyond the one-size-fits-all approach used by most agent models, and allows us to specify agents with different preferences and profiles of behaviour (Sadri & Toni, 2005). In particular, whereas the majority of existing agent models rely upon an \u201cobserve-plan-act\u201d, by means of our cycle theory we can model behaviours such as \u201cobserve-revise goals-plan\u2013act\u201d or \u201cobserve-plan-sense action preconditions-act\u201d or \u201cobserve-plan-act-plan-act\u201d. We provide one example of cycle theory, that we refer to as normal, allowing all behaviours above depending on different circumstances (the environment in which the agent is situated and its preferences). Note also that, with respect to other agent models, the KGP model allows agents to revise their goals during their life-time, and observing the environment according to two modalities: active and passive observation.\nAn agent built with a KGP architecture dynamically determines its goals, plans (partially) how to achieve the goals, interleaves planning with action executions and with making observations in the environment and receiving any messages from other agents, adapts its goals and plans to any new information it receives, and any changes it observes, and generates appropriate reactions.\nA number of publications have already described aspects of (an initial version of) the KGP agents. A precursor of the overall model has been described by Kakas, Mancarella,\nSadri, Stathis, and Toni (2004b), its planning component has been presented by Mancarella, Sadri, Terreni, and Toni (2004), its cycle theory has been developed by Kakas, Mancarella, Sadri, Stathis, and Toni (2004a) and its implementation has been discussed by Stathis et al. (2004), by Yip, Forth, Stathis, and Kakas (2005), and by Bracciali, Endriss, Demetriou, Kakas, Lu, and Stathis (2006). In this paper, we provide the full formal specification of all the components of the KGP model, thus offering the complete technical account of KGP in one place. In providing this full formal specification, we have adjusted and further developed the model. In particular, the notion of state and its definition is novel, the reasoning capabilities have been simplified and some have been added, the physical capabilities have been extended (to include actuating) and formally defined, the transitions and the selection operators have been formally defined in full.\nThe rest of the paper is structured as follows. In Sections 2 and 3 we give an outline of the model and then review the background information necessary for the full description. In Sections 4, 5, 6 and 7, respectively, we describe the internal state of KGP agents, their reasoning and physical capabilities, and their transitions. In Section 8 we describe the selection operators which are then used in the cycle theory which is described in Section 9. Following the detailed description of KGP agents we illustrate the model by a series of examples in Section 10, and then compare the model with others in the literature in Section 11. Finally, we conclude the paper in Section 12."}, {"heading": "2. KGP Model: Outline", "text": "In this Section we give an overview of the KGP agent model and its components, and provide some informal examples of its functioning. This model relies upon\n\u2022 an internal (or mental) state, holding the agent Knowledge base (beliefs), Goals (desires) and Plans (intentions),\n\u2022 a set of reasoning capabilities,\n\u2022 a set of physical capabilities,\n\u2022 a set of transition rules, defining how the state of the agent changes, and defined in terms of the above capabilities,\n\u2022 a set of selection operators, to enable and provide appropriate inputs to the transitions,\n\u2022 a cycle theory, providing the control for deciding which transitions should be applied when.\nThe model is defined in a modular fashion, in that different activities are encapsulated within different capabilities and transitions, and the control is a separate module. The model also has a hierarchical structure, depicted in Figure 1."}, {"heading": "2.1 Internal State", "text": "This is a tuple \u3008KB0,F , C,\u03a3\u3009, where:\nOPERATORS\nCYCLE THEORY\nSELECTION TRANSITIONS\nT E\nA\nS T\nPHYSICAL CAPABILITIES\nCAPABILITIES REASONING\n\u2022 C is the Temporal Constraint Store, namely a set of constraint atoms in some given underlying constraint language. These constrain the time variables of the goals in F . For example, they may specify a time window over which the time of an action can be instantiated, at execution time.\n\u2022 \u03a3 is a set of equalities instantiating time variables with time constants. For example, when the time variables of actions are instantiated at action execution time, records of the instantiations are kept in \u03a3."}, {"heading": "2.2 Reasoning Capabilities", "text": "KGP supports the following reasoning capabilities:\n\u2022 Planning, which generates plans for mental goals given as input. These plans consist of temporally constrained sub-goals and actions designed for achieving the input goals.\n\u2022 Reactivity, which is used to provide new reactive top-level goals, as a reaction to perceived changes in the environment and the current plans held by the agent.\n\u2022 Goal Decision, which is used to revise the non-reactive top-level goals, adapting the agent\u2019s state to changes in the environment.\n\u2022 Identification of Preconditions and Identification of Effects for actions, which are used to determine appropriate sensing actions for checking whether actions may be safely executed (if their preconditions are known to hold) and whether recently executed actions have been successful (by checking that some of their known effects hold).\n\u2022 Temporal Reasoning, which allows the agent to reason about the evolving environment, and to make predictions about properties, including non-executable goals, holding in the environment, based on the (partial) information the agent acquires over its lifetime.\n\u2022 Constraint Solving, which allows the agent to reason about the satisfiability of the temporal constraints in C and \u03a3.\nIn the concrete realisation of the KGP model we provide in this paper, we have chosen to realise the above capabilities in various extensions of the logic programming paradigm. In particular, we use (conventional) logic programming for Identification of Preconditions and Effects, abductive logic programming with constraints (see Section 3.2) for Planning, Reactivity and Temporal Reasoning, and logic programming with priorities (see Section 3.3) for Goal Decision."}, {"heading": "2.3 Physical Capabilities", "text": "In addition to the reasoning capabilities, a KGP agent is equipped with \u201cphysical\u201d capabilities, linking the agent to its environment, consisting of\n\u2022 A Sensing capability, allowing the agent to observe that properties hold or do not hold, and that other agents have executed actions.\n\u2022 An Actuating capability, for executing (physical and communicative) actions."}, {"heading": "2.4 Transitions", "text": "The state \u3008KB0,F , C, \u03a3\u3009 of an agent evolves by applying transition rules, which employ the capabilities as follows:\n\u2022 Goal Introduction (GI), possibly changing the top-level goals in F , and using Goal Decision.\n\u2022 Plan Introduction (PI), possibly changing F and C and using Planning.\n\u2022 Reactivity (RE), possibly changing the reactive top-level goals in F and C, and using the Reactivity capability.\n\u2022 Sensing Introduction (SI), possibly introducing new sensing actions in F for checking the preconditions of actions already in F .\n\u2022 Passive Observation Introduction (POI), updating KB0 by recording unsolicited information coming from the environment, and using Sensing.\n\u2022 Active Observation Introduction (AOI), possibly updating \u03a3 and KB0, by recording the outcome of (actively sought) sensing actions, and using Sensing.\n\u2022 Action Execution (AE), executing all types of actions and as a consequence updating KB0 and \u03a3, and using Actuating.\n\u2022 State Revision (SR), possibly revising F , and using Temporal Reasoning and Constraint Solving."}, {"heading": "2.5 Cycle and Selection Operators", "text": "The behaviour of an agent is given by the application of transitions in sequences, repeatedly changing the state of the agent. These sequences are not determined by fixed cycles of behaviour, as in conventional agent architectures, but rather by reasoning with cycle theories. Cycle theories define preference policies over the order of application of transitions, which may depend on the environment and the internal state of an agent. They rely upon the use of selection operators for detecting which transitions are enabled and what their inputs should be, as follows:\n\u2022 action selection for inputs to AE; this selection operator uses the Temporal Reasoning and Constraint Solving capabilities;\n\u2022 goal selection for inputs to PI; this selection operator uses the Temporal Reasoning and Constraint Solving capabilities;\n\u2022 effect selection for inputs to AOI; this selection operator uses the Identification of Effect reasoning capability;\n\u2022 precondition selection for inputs to SI; this selection operator uses the Identification of Preconditions, Temporal Reasoning and Constraint Solving capabilities;\nThe provision of a declarative control for agents in the form of cycle theories is a highly novel feature of the model, which could, in principle, be imported into other agent systems. In the concrete realisation of the KGP model we provide in this paper, we have chosen to realise cycle theories in the same framework of logic programming with priorities and constraints (see Section 3.3) that we also use for Goal Decision.\nSome of the relationships between the capabilities, transitions and the selection operators are summarised in Tables 2.5 and 2 below. Table 2.5 indicates which capabilities (rows) are used by which transitions and selection operators. Table 2 indicates which selection operators are used to compute possible inputs for which transitions in the cycle theory.\nBefore we provide these components, though, we introduce below informally a scenario and some examples that will be used to illustrate the technical details of the KGP agent\nmodel throughout the paper. A full, formal presentation of these as well as additional examples will be given throughout the paper and in Section 10."}, {"heading": "2.6 Examples", "text": "We draw all our examples from a ubiquitous computing scenario that we call the San Vincenzo scenario, presented by de Bruijn and Stathis (2003) and summarised as follows. A businessman travels for work purposes to Italy and, in order to make his trip easier, carries a personal communicator, namely a device that is a hybrid between a mobile phone and a PDA. This device is the businessman\u2019s KGP agent. This agent can be considered as a personal service agent (Mamdani, Pitt, & Stathis, 1999) (or psa for short) because it provides proactive information management and flexible connectivity to smart services available in the global environment within which the businessman travels within."}, {"heading": "2.6.1 Setting 1", "text": "The businessman\u2019s psa requests from a \u2018San Vincenzo Station\u2019 agent, svs, the arrival time of the train tr01 from Rome. As svs does not have this information it answers with a refusal. Then later, svs receives information of the arrival time of the tr01 train from a \u2018Central Office\u2019 agent, co. When the psa requests the arrival time of tr01 again, svs will accept the request and provide the information.\nThis first example requires one to use the Reactivity capability to model rules of interaction and the RE transition (a) to achieve interaction amongst agents, and (b) to specify dynamic adjustments of the agent\u2019s behaviour to changes, allowing different reactions to the same request, depending on the current situation of the agent. Here, the interaction is a form of negotiation of resources amongst agents, where resources are items of information. Thus, the current situation of the agents amounts to what resources/information the agents currently own.\nThis example also requires the combination of transitions RE, POI, and AE to achieve the expected agents\u2019 behaviours, as follows:\n1. psa makes the initial request by applying AE\n2. svs becomes aware of this request by performing POI (and changing its KB0 accordingly)\n3. svs decides to reply with a refusal by performing RE (and adding the corresponding action to its plan in F)\n4. svs utters the refusal by performing AE\n5. svs becomes aware, by POI, of the arrival time (modifying its KB0 accordingly)\n6. psa makes the second request by applying AE again\n7. svs decides to reply with the requested information by performing RE (and adding the corresponding action to its plan in F) and communicates the information by performing AE.\nThis sequence of transitions is given by the so-called normal cycle theory that we will see in Section 9."}, {"heading": "2.6.2 Setting 2", "text": "In preparation of the businessman\u2019s next trip, his psa aims at getting a plane ticket from Madrid to Denver as well as obtaining a visa to the USA. One possible way to buy plane tickets is over the internet. Buying tickets this way is usually possible, but not to all destinations (depending on whether the airlines flying to the destinations sell tickets over the internet or not) and not without an internet connection. The psa does not currently have the connection, nor the information that Denver is indeed a destination for which tickets can be bought online. It plans to buy the ticket over the internet nonetheless, conditionally, but checks the conditions before executing the planned action. After successfully buying the ticket, psa focuses on the second goal, of obtaining a visa. This can be achieved by applying to the USA embassy in Madrid, but the application requires an address in the USA. This address can be obtained by arranging for a hotel in Denver.\nThis example illustrates the form of \u201cpartial\u201d planning adopted by the KGP model (where non-executable sub-goals as well as actions may be part of plans) and shows how the combination of transition PI with SI and AE allows the psa agent to deal with partial information, to generate conditional plans and plans with several \u201clayers\u201d, as follows:\n1. psa is initially equipped with the top-level goals to get a ticket to Denver and to obtain a visa (through an earlier application of GI)\n2. by PI for the first goal, psa adds a \u201cpartial\u201d plan to its F , of buying a ticket online subject to sub-goals that there is an internet connection available and that online tickets can be bought to Denver; these sub-goals are sensing goals\n3. by SI, sensing actions are added to F to evaluate the sensing sub-goals in the environment\n4. these sensing actions are executed by AE (and KB0 is modified accordingly)\n5. depending on the sensed values of the sensing sub-goals the buying action may or may not be executed by AE; let us assume in the remainder of the example that this action is executed\n6. SR is applied to eliminate all actions (since they have already been executed), subgoals and top-level goal of getting a ticket to Denver (since they have been achieved)\n7. by PI for the remaining top-level goal of obtaining a visa, psa adds a plan to fill in an application form (action) and acquiring a residence address in Denver (sub-goal)\n8. the action cannot be executed, as psa knows that the businessman is not resident in the USA; further PI introduces a plan for the sub-goal of booking a hotel (action) for the subgoal of acquiring a residence address in Denver\n9. AE executes the booking action\n10. AE executes the action of applying for a visa\n11. SR eliminates all actions (since they have already been executed), sub-goal and toplevel goal of getting a visa (since they have been achieved)."}, {"heading": "3. Background", "text": "In this section we give the necessary background for the reasoning capabilities and the cycle theory of KGP agents, namely:\n\u2022 Constraint Logic Programming, pervasive to the whole model, \u2022 Abductive Logic Programming, at the heart of the Planning, Reactivity and Temporal\nReasoning capabilities, and\n\u2022 Logic Programming with Priorities, at the heart of the Goal Decision capability and Cycle Theories."}, {"heading": "3.1 Constraint Logic Programming", "text": "Constraint Logic Programming (CLP) (Jaffar & Maher, 1994) extends logic programming with constraint predicates which are not processed as ordinary logic programming predicates, defined by rules, but are checked for satisfiability and simplified by means of a built-in, \u201cblack-box\u201d constraint solver. These predicates are typically used to constrain the values that variables in the conclusion of a rule can take (together with unification which is also treated via an equality constraint predicate). In the KGP model, constraints are used to determine the value of time variables, in goals and actions, under a suitable temporal constraint theory.\nThe CLP framework is defined over a structure < consisting of a domain D(<) and a set of constraint predicates which includes equality, together with an assignment of relations on D(<) for each such constraint predicate. In CLP, constraints are built as first-order formulae in the usual way from primitive constraints of the form c(t1, . . . , tn) where c is a constraint predicate symbol and t1, . . . , tn are terms constructed over the domain, D(<), of values. Then the rules of a constraint logic program, P , take the same form as rules in conventional logic programming given by\nH \u2190 L1, . . . , Ln with H an (ordinary) atom, L1, . . . , Ln literals, and n \u2265 0. Literals can be positive, namely ordinary atoms, or negative, namely of the form notB, where B is an ordinary atom, or constraint atoms over <. The negation symbol not indicates negation as failure (first introduced by Clark, 1978). All variables in H and Li are implicitly universally quantified, with scope the entire rule. H is called the head (or the conclusion) and L1, . . . , Ln is called the body (or the conditions) of a rule of the form above. If n = 0, the rule is called a fact.\nA valuation, \u03d1, of a set of variables is a mapping from these variables to the domain D(<) and the natural extension which maps terms to D(<). A valuation \u03d1, on the set of all variables appearing in a set of constraints C, is called an <-solution of C iff C\u03d1, obtained by applying \u03d1 to C, is satisfied, i.e. C\u03d1 evaluates to true under the given interpretation of the\nconstraint predicates and terms. This is denoted by \u03d1 |=< C. A set C is called <-solvable or <-satisfiable, denoted by |=< C, iff it has at least one <-solution, i.e. \u03d1 |=< C for some valuation \u03d1.\nOne way to give the meaning of a constraint logic program P is to consider the grounding of the program over its Herbrand base and all possible valuations, over D(<), of its constraint variables. In each such rule, if the ground constraints C in the body are evaluated to true then the rule is kept with the constraints C dropped, otherwise the whole rule is dropped. Let ground(P ) be the resulting ground program. The meaning of P is then given by the meaning |=LP of ground(P ), for which there are many different possible choices (Kakas, Kowalski, & Toni, 1998). The resulting overall semantics for the constraint logic program P will be referred to as |=LP (<). More precisely, given a constraint logic program P and a conjunction N \u2227 C (where N is a conjunction of non-constraint literals and C is a conjunction of constraint atoms), in the remainder of the paper we will write\nP |=LP (<) N \u2227 C to denote that there exists a ground substitution \u03d1 over the variables of N \u2227 C such that:\n\u2022 \u03d1 |=< C \u2022 ground(P ) |=LP N\u03d1."}, {"heading": "3.2 Abductive Logic Programming with Constraints", "text": "An abductive logic program with constraints is a tuple \u3008<, P,A, I\u3009 where: \u2022 < is a structure as in Section 3.1 \u2022 P is a constraint logic program, namely a set of rules of the form\nH \u2190 L1, . . . , Ln as in Section 3.1\n\u2022 A is a set of abducible predicates in the language of P . These are predicates not occurring in the head of any clause of P (without loss of generality, see (Kakas et al., 1998)). Atoms whose predicate is abducible are referred to as abducible atoms or simply as abducibles.\n\u2022 I is a set of integrity constraints, that is, a set of sentences in the language of P . All the integrity constraints in the KGP model have the implicative form\nL1, . . . , Ln \u21d2 A1 \u2228 . . . \u2228Am (n \u2265 0,m > 0) where Li are literals (as in the case of rules) 2, Aj are atoms (possibly the special atom false). The disjunction A1\u2228 . . .\u2228Am is referred to as the head of the constraint and the conjunction L1, . . . , Ln is referred to as the body. All variables in an integrity constraint are implicitly universally quantified from the outside, except for variables occurring only in the head, which are implicitly existentially quantified with scope the head itself.\n2. If n = 0, then L1, . . . , Ln represents the special atom true.\nGiven an abductive logic program with constraints \u3008<, P, A, I\u3009 and a formula (query) Q, which is an (implicitly existentially quantified) conjunction of literals in the language of P , the purpose of abduction is to find a (possibly minimal) set of (ground) abducible atoms \u0393 which, together with P , \u201centails\u201d (an appropriate ground instantiation of) Q, with respect to some notion of \u201centailment\u201d that the language of P is equipped with, and such that the extension of P by \u0393 \u201csatisfies\u201d I (see (Kakas et al., 1998) for possible notions of integrity constraint \u201csatisfaction\u201d). Here, the notion of \u201centailment\u201d is the combined semantics |=LP (<), as discussed in Section 3.1.\nFormally, given a query Q, a set \u2206 of (possibly non-ground) abducible atoms, and a set C of (possibly non-ground) constraints, the pair (\u2206, C) is an abductive answer (with constraints) for Q, with respect to an abductive logic program with constraints \u3008<, P, A, I\u3009, iff for all groundings \u03c3 for the variables in Q,\u2206, C such that \u03c3 |=< C, it holds that\n(i) P \u222a\u2206\u03c3 |=LP (<) Q\u03c3, and (ii) P\u222a\u2206\u03c3 |=LP (<) I, i.e. for each B \u21d2 H \u2208 I, if P\u222a\u2206\u03c3 |=LP (<) B then P\u222a\u2206\u03c3 |=LP (<) H.\nHere, \u2206\u03c3 plays the role of \u0393 in the earlier informal description of abductive answer. Note also that, by (ii), integrity constraints are not classical implications.\nNote also that, when representing knowledge as an abductive logic program, one needs to decide what should go into the logic program, what in the integrity constraints and what in the abducibles. Intuitively, integrity constraints are \u201cnormative\u201d in that they need to be enforced, by making sure that their head holds whenever their body does (by condition (ii) above), whereas logic programming rules enable, with the help of abducibles, the derivation of given goals (by condition (i) above). Finally, abducibles are chosen amongst the literals that cannot be derived by means of logic programming rules. In this paper, we will represent reactive constraints (that are condition-action rules forcing the reactive behaviour of agents) as integrity constraints, thus to some extent addressing this knowledge representation challenge posed by abductive logic programming by imposing a sort of \u201cstructure\u201d on the abductive logic programs we use.\nThe notion of abductive answer can be extended to take into account an initial set of (possibly non-ground) abducible atoms \u22060 and an initial set of (possibly non-ground) constraint atoms C0. In this extension, an abductive answer for Q, with respect to\n(\u3008<, P,A, I\u3009, \u22060, C0)\nis a pair (\u2206, C) such that\n(i) \u2206 \u2229\u22060 = {} (ii) C \u2229 C0 = {}, and (iii) (\u2206 \u222a \u22060, C \u222a C0) is an abductive answer for Q with respect to \u3008<, P,A, I\u3009 (in the\nearlier sense).\nIt is worth noticing that an abductive answer (\u2206, C) for the query true with respect to\n(\u3008<, P,A, I\u3009, \u22060, C0)\nshould be read as the fact that the abducibles in \u22060 \u222a \u2206, along with the constraints in C0 \u222a C, guarantee the overall consistency with respect to the integrity constraints given in I. This will be used for the specification of some capabilities of KGP agents.\nIn the remainder of the paper, for simplicity, we will omit < from abductive logic programs, which will be written simply as triples \u3008P, A, I\u3009. In addition, all abductive logic programs that will present in KGP are variants of a core event calculus (Kowalski & Sergot, 1986), that we will define in Section 5.1.1."}, {"heading": "3.3 Logic Programming with Priorities", "text": "For the purposes of this paper, a logic program with priorities over a constraint structure <, referred to as T , consists of four parts:\n(i) a low-level or basic part P , consisting of a logic program with constraints; each rule in P is assigned a name, which is a term; e.g. one such rule could be\nn(X, Y ) : p(X) \u2190 q(X,Y ), r(Y ) with name n(X,Y ) naming each ground instance of the rule;\n(ii) a high-level part H, specifying conditional, dynamic priorities amongst rules in P or H; e.g. one such priority could be\nh(X) : m(X) \u00c2 n(X) \u2190 c(X) to be read: if (some instance of) the condition c(X) holds, then (the corresponding instance of) the rule named by m(X) should be given higher priority than (the corresponding instance of) the rule named by n(X). The rule itself is named h(X);\n(iii) an auxiliary part A, which is a constraint logic program defining (auxiliary) predicates occurring in the conditions of rules in P, H and not in the conclusions of any rule in P or H;\n(iv) a notion of incompatibility which, for our purposes, can be assumed to be given as a set of rules defining the predicate incompatible/2, e.g.\nincompatible(p(X), p\u2032(X))\nto be read: any instance of the literal p(X) is incompatible with the corresponding instance of the literal p\u2032(X). We assume that incompatibility is symmetric and always includes that r \u00c2 s is incompatible with s \u00c2 r for any two rule names r, s. We refer to the set of all incompatibility rules as I.\nAny concrete LPP framework is equipped with a notion of entailment, which we denote by |=pr, that is defined on top of the underlying logic programming with constraints semantics |=LP (<). This is defined differently by different approaches to LPP but they all share the following pattern. Given a logic program with priorities T = \u3008P,H, A, I\u3009 and a conjunction \u03b1 of ground (non-auxiliary) atoms, T |=pr \u03b1 iff\n(i) there exists a subset P \u2032 of the basic part P such that P \u2032 \u222aA |=LP (<) \u03b1, and\n(ii) P \u2032 is \u201cpreferred\u201d wrt H\u222aA over any other subset P \u2032\u2032 of P that derives (under |=LP (<)) a conclusion that is incompatible, wrt I, with \u03b1.\nEach framework has its own way of specifying what is meant for one sub-theory P \u2032 to be \u201cpreferred\u201d over another sub-theory P \u2032\u2032. For example, in existing literature (Kakas et al., 1994; Prakken & Sartor, 1996; Kowalski & Toni, 1996; Kakas & Moraitis, 2003), |=pr is defined via argumentation. This is also the approach that we adopt, relying on the notion of an admissible argument as a sub-theory that is (i) consistent (does not have incompatible conclusions) and (ii) whose rules do not have lower priority, with respect to the high-level part H of our theory, than those of any other sub-theory that has incompatible conclusions with it. The precise definition of how sets of rules are to be compared again is a matter of choice in each specific framework of LPP.\nGiven such a concrete definition of admissible sub-theories, the preference entailment, T |=pr\u03b1, is then given by:\n(i) there exists a (maximal) admissible sub-theory T \u2032 of T such that T \u2032 |=LP (<) \u03b1, and\n(ii) for any \u03b1 that is incompatible with \u03b1 there does not exist an admissible sub-theory T \u2032\u2032 of T such that T \u2032\u2032 |=LP (<) \u03b1.\nWhen only the first condition of the above is satisfied we say that the theory T credulously prefers or possibly prefers \u03b1. When both conditions are satisfied we say that the theory sceptically prefers \u03b1."}, {"heading": "4. The State of KGP Agents", "text": "In this Section we define formally the concept of state for a KGP agent. We also introduce all the notation that we will use in the rest of the paper in order to refer to state components. Where necessary, we will also try to exemplify our discussion with simple examples."}, {"heading": "4.1 Preliminaries", "text": "In the KGP model we assume (possibly infinite) vocabularies of:\n\u2022 fluents, indicated with f, f \u2032, . . .,\n\u2022 action operators, indicated with a, a\u2032, . . .,\n\u2022 time variables, indicated with \u03c4, \u03c4 \u2032, . . .,\n\u2022 time constants, indicated with t, t\u2032, . . . , 1, 2, . . ., standing for natural numbers (we also often use the constant now to indicate the current time)\n\u2022 names of agents, indicated with c, c\u2032, . . . .\n\u2022 constants, other than the ones mentioned above, normally indicated with lower case letters, e.g. r, r1, . . .\n\u2022 a given constraint language, including constraint predicates <,\u2264, >,\u2264, =, 6=, with respect to some structure < (e.g. the natural numbers) and equipped with a notion of constraint satisfaction |=< (see Section 3.1).\nWe assume that the set of fluents is partitioned into two disjoint sets:\n\u2022 mental fluents, intuitively representing properties that the agent itself is able to plan for so that they can be satisfied, but can also be observed, and\n\u2022 sensing fluents, intuitively representing properties which are not under the control of the agent and can only be observed by sensing the external environment.\nFor example, problem fixed and have resource may represent mental fluents, namely the properties that a (given) problem has been fixed and a (given) resource should be obtained, whereas request accepted and connection on may represent sensing fluents, namely the properties that a request for some (given) resource is accepted and that some (given) connection is active. Note that it is important to distinguish between mental and sensing fluents as they are treated differently by the control of the agent: mental fluents need to be planned for, whereas sensing fluents can only be observed. This will be clarified later in the paper.\nWe also assume that the set of action operators is partitioned into three disjoint sets:\n\u2022 physical action operators, representing actions that the agent performs in order to achieve some specific effect, which typically causes some changes in the environment;\n\u2022 communication action operators, representing actions which involve communications with other agents;\n\u2022 sensing action operators, representing actions that the agent performs to establish whether some fluent (either a sensing fluent or an expected effect of some action) holds in the environment, or whether some agent has performed some action.\nFor example, sense(connection on, \u03c4) is an action literal representing the act of sensing whether or not a network connection is on at time \u03c4 , do(clear table, \u03c4) is an action literal representing the physical action of removing every item on a given table, and tell(c1, c2, request(r1), d, \u03c4) is an action literal representing a communication action which expresses that agent c1 is requesting from agent c2 the resource r1 within a dialogue with identifier d, at time \u03c43.\nEach fluent and action operator has an associated arity: we assume that this arity is greater than or equal to 1, in that one argument (the last one, by convention) is always the time point at which a given fluent holds or a given action takes place. This time point may be a time variable or a time constant. Given a fluent f of arity n > 0, we refer to f(s1, . . . , sn\u22121, x) and \u00acf(s1, . . . , sn\u22121, x), where each si is a constant and x is a time variable or a time constant as (timed) fluent literals 4. Given a fluent literal `, we denote by `\n3. The role of the dialogue identifier will become clearer in Section 10. Intuitively, this is used to \u201clink\u201d communication actions occurring within the same dialogue. 4. Note that \u00ac represents classical negation. Negation as failure occurs in the model only within the knowledge bases of agents, supporting the reasoning capabilities and the cycle theory. All other negations in the state are to be understood as classical negations.\nits complement, namely \u00acf(s1, . . . , sn\u22121, x) if ` is f(s1, . . . , sn\u22121, x), and f(s1, . . . , sn\u22121, x) if ` is \u00acf(s1, . . . , sn\u22121, x). Examples of fluent literals are have resource(pen, \u03c4), representing that a certain resource pen should be obtained at some time \u03c4 , as well as (the ground) \u00acon(box, table, 10), representing that at time 10 (a certain) box should not be on (a certain) table.\nNote that we assume that fluent literals are ground except for the time parameter. This will allow us to keep the notation simpler and to highlight the crucial role played by the time parameter. Given this simplification, we will often denote timed fluent literals simply by `[x].\nGiven an action operator a of arity n > 0, we refer to a(s1, . . . , sn\u22121, x), where each si is a constant and x is a time variable or time constant, as a (timed) action literal. Similarly to the case of fluent literals, for simplicity, we will assume that timed action literals are ground except possibly for the time. Hence, we will often denote timed action literals by a[x].\nWe will adopt a special syntax for sensing actions, that will always have the form (x is either a time variable or a time constant):\n\u2022 sense(f, x), where f is a fluent, or \u2022 sense(c : a, x), where c is the name of an agent and a is an action operator.\nIn the first case, the sensing action allows the agent to inspect the external environment in order to check whether or not the fluent f holds at the time x of sensing. In the second case, the sensing action allows the agent to determine whether, at time x, another agent c has performed some action a.\nWe will now define formally the concept of state \u3008KB0,F , C, \u03a3\u3009 of an agent."}, {"heading": "4.2 Forest: F", "text": "Each node in each tree in F is:\n\u2022 either a non-executable goal, namely a (non-ground) timed fluent literal, \u2022 or an executable goal, namely a (non-ground) timed action literal. An example of a tree in F is given in Figure 2, where p2 is some given problem that the agent (c1) needs to fix by getting two resources r1 and r2, and where the agent has already decided to get r1 from some other agent c2 and has already planned to ask c2 by the communication action tell(c1, c2, request(r1), d, \u03c44). For example, in the San Vincenzo scenario, p2 may be \u201ctransfer to airport needs to be arranged\u201d, r1 may be a taxi, and c2 a taxi company, needed for transportation to some train station, and finally r2 may be a train ticket.\nNote that the time variable \u03c4 in non-executable goals `[\u03c4 ] and actions a[\u03c4 ] in (any tree in) F is to be understood as a variable that is existentially quantified within the whole state of the agent. Whenever a goal or action is introduced within a state, its time variable is to be understood as a distinguished, fresh variable, also serving as its identifier.\nAs indicated in Section 2, roots of trees are referred to as top-level goals, executable goals are often called simply actions, non-executable goals may be top-level goals or subgoals. For example, in Figure 2, the node with identifier \u03c41 is a top-level goal, the nodes with identifiers \u03c42, \u03c43 are sub-goals and the node with identifier \u03c44 is an action.\nNotation 4.1 Given a forest F and a tree T \u2208 F :\n\u2022 for any node n of T , parent(n, T ), children(n, T ), ancestors(n, T ), siblings(n, T ), descendents(n, T ), will indicate the parent of node n in T , the children of n in T , etc. and leaf(n, T ) will have value true if n is a leaf in T , false otherwise.\n\u2022 for any node n of F , parent(n,F), children(n,F), ancestors(n,F), siblings(n,F), descendents(n,F), leaf(n,F) will indicate the parent(n, T ) for the tree T in F where n occurs, etc. (T is unique, due to the uniqueness of the time variable identifying nodes).\n\u2022 nodes(T ) will represent the set of nodes in T , and nodes(F) will represent the set nodes(F) = \u22c3T \u2208F nodes(T ).\nAgain, as indicated in Section 2, each top-level goal in each tree in F will be either reactive or non-reactive. We will see, in Section 7, that reactive top-level goals are introduced into the state by the RE transition whereas non-reactive top-level goals are introduced by the GI transition. For example, F of agent c1 may consist of the tree in Figure 2, with root a non-reactive goal, as well as a tree with root the reactive goal (action)\ntell(c1, c2, accept request(r3), d\u2032, \u03c45). This action may be the reply (planned by agent c1) to some request for resource r3 by agent c2 (for example, in the San Vincenzo scenario, r3 may be a meeting requested by some colleague).\nNotation 4.2 Given a forest F \u2022 Rootsr(F) (resp. Rootsnr(F)) will denote the set of all reactive (resp. non-reactive)\ntop-level goals in F \u2022 nodesr(F) (resp. nodesnr(F)) will denote the subset of nodes(F) consisting of nodes\nin all trees whose root is in Rootsr(F) (resp. Rootsnr(F)) \u2022 r(F) (resp. nr(F)) stands for the reactive (resp. non-reactive) part of F , namely the\nset of all trees in F whose root is in Rootsr(F) (resp. Rootsnr(F)). Trivially, r(F) and nr(F) are disjoint, and F= r(F) \u222a nr(F)."}, {"heading": "4.3 Temporal Constraint Store: C", "text": "This is a set of constraint atoms, referred to as temporal constraints, in the given underlying constraint language. Temporal constraints refer to time constants as well as to time variables associated with goals (currently or previously) in the state.\nFor example, given a forest with the tree in Figure 2, C may contain \u03c41 > 10, \u03c41 \u2264 20, indicating that the top-level goal (of fixing problem p2) needs to be achieved within the time interval (10, 20], \u03c42 < \u03c41, \u03c43 < \u03c41, indicating that resources r1 and r2 need to be acquired before the top-level goal can be deemed to be achieved, and \u03c44 < \u03c42, indicating that the agent needs to ask agent c2 first. Note that we do not need to impose that \u03c42 and \u03c43 are executed in some order, namely C may contain neither \u03c42 < \u03c43, nor \u03c43 < \u03c42.\n4.4 Agents\u2019 Dynamic Knowledge Base: KB0\nKB0 is a set of logic programming facts in the state of an agent, recording the actions which have been executed (by the agent or by others) and their time of execution, as well as the properties (i.e. fluents and their negation) which have been observed and the time of the observation. Formally, these facts are of the following forms:\n\u2022 executed(a, t) where a[t] is a ground action literal, meaning that action a has been executed by the agent at time t.\n\u2022 observed(`, t) where `[t] is a ground fluent literal, meaning that ` has been observed to hold at time t.\n\u2022 observed(c, a[t\u2032], t) where c is an agent\u2019s name, different from the name of the agent whose state we are defining, t and t\u2032 are time constants, and a[t\u2032] is a (ground) action literal. This means that the given agent has observed at time t that agent c has executed the action a at time t\u2032 5.\n5. We will see that, by construction, it will always be the case that t\u2032 \u2264 t. Note that the time of executed actions, t\u2032, and the time of their observation, t, will typically be different in any concrete implementation of the KGP model, as they depend, for example, on the time of execution of transitions within the operational trace of an agent.\nNote that all facts in KB0 are variable-free, as no time variables occur in them. Facts of the first kind record actions that have been executed by the agent itself. Facts of the second kind record observations made by the agent in the environment, excluding actions executed by other agents, which are represented instead as facts of the third kind.\nFor example, if the action labelled by \u03c44 in Figure 2 is executed (by the AE transition) at time 7 then executed(tell(c1, c2, request(r1), d), 7) will be added to KB0. Moreover, if, at time 9, c1 observes (e.g. by transition POI) that it has resource r2, then the observation observed(have resource(r2), 9) will be added to KB0. Finally, KB0 may contain\nobserved(c2, tell(c2, c1, request(r3), d\u2032, 1), 6)\nto represent that agent c1 has become aware, at time 6, that agent c2 has requested, at the earlier time 1, resource r3 from c1."}, {"heading": "4.5 Instantiation of Time Variables: \u03a3", "text": "When a time variable \u03c4 occurring in some non-executable goal `[\u03c4 ] or some action a[\u03c4 ] in F is instantiated to a time constant t (e.g. at action execution time), the actual instantiation \u03c4 = t is recorded in the \u03a3 component of the state of the agent. For example, if the action labelled by \u03c44 in Figure 2 is executed at time 7, then \u03c44 = 7 will be added to \u03a3.\nThe use of \u03a3 allows one to record the instantiation of time variables while at the same time keeping different goals with the same fluent distinguished. Clearly, for each time variable \u03c4 there exists at most one equality \u03c4 = t in \u03a3.\nNotation 4.3 Given a time variable \u03c4 , we denote by \u03a3(\u03c4) the time constant t, if any, such that \u03c4 = t \u2208 \u03a3.\nIt is worth pointing out that the valuation of any temporal constraint c \u2208 C will always take the equalities in \u03a3 into account. Namely, any ground valuation for the temporal variables in c must agree with \u03a3 on the temporal variables assigned to them in \u03a3. For example, given \u03a3 = {\u03c4 = 3} and C = {\u03c41 > \u03c4}, then \u03c41 = 10 is a suitable valuation, whereas \u03c41 = 1 is not."}, {"heading": "5. Reasoning Capabilities", "text": "In this section, we give detailed specifications for the various reasoning capabilities, specified within the framework of ordinary logic programming (for Temporal Reasoning and Identification of Preconditions and Effects), of Abductive Logic Programming with Constraints (Section 3.2, for Planning and Reactivity), of Logic Programming with Priorities with Constraints (Section 3.3, for Goal Decision), of constraint programming (Section 3.1, for Constraint Solving).\nThe reasoning capabilities are defined by means of a notion of \u201centailment\u201d with respect to an appropriate knowledge base (and a time point now, where appropriate), as follows:\n\u2022 |=TR and KBTR for Temporal Reasoning, where KBTR is a constraint logic program and a variant of the framework of the Event Calculus (EC) for reasoning about actions, events and changes (Kowalski & Sergot, 1986) 6;\n\u2022 |=nowplan and KBplan for Planning, where KBplan is an abductive logic program with constraints, extending KBTR;\n\u2022 |=nowreact and KBreact for Reactivity, where KBreact is an extension of KBplan, incorporating additional integrity constraints representing reactive rules;\n\u2022 |=pre and KBpre, where KBpre is a logic program contained in KBTR; \u2022 |=eff and KBeff , where KBeff is a logic program contained in KBTR; \u2022 |=nowGD and KBGD, where KBGD is a logic program with priorities and constraints. The constraint solving capability is defined in terms of an \u201centailment\u201d |=cs which is\nbasically |=< as defined in Section 3.1."}, {"heading": "5.1 Temporal Reasoning, Planning, Reactivity, Identification of Preconditions and Effects: EC-based Capabilities", "text": "These reasoning capabilities are all specified within the framework of the event calculus (EC) for reasoning about actions, events and changes (Kowalski & Sergot, 1986). Below, we first give the core EC and then show how to use it to define the various capabilities in this section."}, {"heading": "5.1.1 Preliminaries: Core Event Calculus", "text": "In a nutshell, the EC allows one to write meta-logic programs which \u201ctalk\u201d about objectlevel concepts of fluents, events (that we interpret as action operators) 7, and time points. The main meta-predicates of the formalism are:\n\u2022 holds at(F, T ) - a fluent F holds at a time T ; \u2022 clipped(T1, F, T2) - a fluent F is clipped (from holding to not holding) between times\nT1 and T2;\n\u2022 declipped(T1, F, T2) - a fluent F is declipped (from not holding to holding) between times T1 and T2;\n\u2022 initially(F ) - a fluent F holds at the initial time, say time 0; \u2022 happens(O, T ) - an operation O happens at a time T ; \u2022 initiates(O, T, F ) - a fluent F starts to hold after an operation O at time T ;\n6. A more sophisticated, abductive logic programming version of |=TR and KBTR is given by Bracciali and Kakas (2004). 7. In this section we use the original event calculus terminology of events instead of operators, as in the rest of the paper.\n\u2022 terminates(O, T, F ) - a fluent F ceases to hold after an operation O at time T .\nRoughly speaking, the last two predicates represent the cause-effects links between operations and fluents in the modelled world. We will also use a meta-predicate\n\u2022 precondition(O,F ) - the fluent F is one of the preconditions for the executability of the operation O.\nFluent literals in an agent\u2019s state are mapped onto the EC as follows. The EC-like representation of a fluent literal f [\u03c4 ] (resp. \u00acf [\u03c4 ]) in an agent\u2019s state is the atom holds at(f, \u03c4) (resp. holds at(\u00acf, \u03c4)). Moreover, when arguments other than the time variable need to be considered, the EC representation of a fluent literal f(x1, . . . , xn, \u03c4) (resp. \u00acf(x1, . . . , xn, \u03c4)) is holds at(f(x1, . . . , xn), \u03c4) (resp. holds at(\u00acf(x1, . . . , xn), \u03c4). 8\nSimilarly, action literals in the state of an agent can be represented in the EC in a straightforward way. Given an action literal a[\u03c4 ] its EC representation is happens(a, \u03c4). When arguments other than time are considered, as e.g. in a(x1, . . . , xn, \u03c4), the EC representation is given by happens(a(x1, . . . xn), \u03c4).\nIn the remainder of the paper, with an abuse of terminology, we will sometimes refer to f(x1, . . . , xn) and \u00acf(x1, . . . , xn) interchangeably as fluent literals or fluents (although strictly speaking they are fluent literals), and to a(x1, . . . xn) interchangeably as action literals or action operators (although strictly speaking they are action literals).\nThe EC allows one to represent a wide variety of phenomena, including operations with indirect effects, non-deterministic operations, and concurrent operations (Shanahan, 1997).\nThe core EC we use in this paper consists of two parts: domain-independent rules and domain-dependent rules. The basic domain-independent rules, directly borrowed from the original EC, are:\nholds at(F, T2) \u2190 happens(O, T1), initiates(O, T1, F ), T1 < T2, not clipped(T1, F, T2) holds at(\u00acF, T2) \u2190 happens(O, T1), terminates(O, T1, F ), T1 < T2, not declipped(T1, F, T2) holds at(F, T ) \u2190 initially(F ), 0 \u2264 T, not clipped(0, F, T ) holds at(\u00acF, T ) \u2190 initially(\u00acF ), 0 \u2264 T, not declipped(0, F, T ) clipped(T1, F, T2) \u2190 happens(O, T ), terminates(O, T, F ), T1 \u2264 T < T2 declipped(T1, F, T2) \u2190 happens(O, T ), initiates(O, T, F ), T1 \u2264 T < T2\nThe domain-dependent rules define initiates, terminates, and initially, e.g. in the case of setting 2.6.1 in Section 2.6 we may have\ninitiates(tell(C, svs, inform(Q, I), D), T, have info(svs,Q, I)) \u2190 holds at(trustworthy(C), T )\ninitially(\u00achave info(svs, arrival(tr01), I) 8. Note that we write holds at(\u00acf(x1, . . . , xn), \u03c4) instead of not holds at(f(x1, . . . , xn), \u03c4), as done e.g. by\nShanahan, 1997, because we want to reason at the object-level about properties being true or false in the environment. We use not within the meta-level axioms of the event calculus (see below) to implement persistence.\ninitially(trustworthy(co))\nNamely, an action by agent C of providing information I concerning a query Q to the agent svs (the \u201cSan Vincenzo station\u201d agent) initiates the agent svs having the information about Q, provided that C is trustworthy. Moreover, initially agent co (the \u201cCentral Office\u201d agent) is trustworthy, and agent svs has no information about the arrival time of tr01. The conditions for the rule defining initiates can be seen as preconditions for the effects of the operator tell to take place. Preconditions for the executability of operators are specified by means of a set of rules (facts) defining the predicate precondition, e.g.\nprecondition(tell(svs, C, inform(Q, I), D), have info(svs, Q, I))\nnamely the precondition for agent svs to inform any agent C of I about Q is that svs indeed has information I about Q.\nNotice that the presence in the language of fluents and their negation, e.g. f and \u00acf , poses the problem of \u201cinconsistencies\u201d, i.e. it may be the case that both holds at(f, t) and holds at(\u00acf, t) can be derived from the above axioms and a set of events (i.e. a given set of happens atoms). However, it can easily be shown that this is never the case, provided that the domain-dependent part does not contain two conflicting statements of the form initially(f) and initially(\u00acf) since inconsistencies cannot be caused except at the initial time point (see e.g. Miller & Shanahan, 2002, p. 459).\nIn the remainder of the paper we will assume that the domain-dependent part is always consistent for our agents.\nTo allow agents to draw conclusions from the contents of KB0, which represents the \u201cnarrative\u201d part of the agent\u2019s knowledge, we add to the domain-independent rules the following bridge rules:\nholds at(F, T2) \u2190 observed(F, T1), T1 \u2264 T2, not clipped(T1, F, T2) holds at(\u00acF, T2) \u2190 observed(\u00acF, T1), T1 \u2264 T2, not declipped(T1, F, T2) happens(O, T ) \u2190 executed(O, T ) happens(O, T ) \u2190 observed( , O[T ], )\nNotice that these bridge rules make explicit the translation from the state representation to the EC representation of fluents and actions we have mentioned earlier on in this section. Note also that we assume that a fluent holds from the time it is observed to hold. This choice is dictated by the rationale that observations can only be considered and reasoned upon from the moment the agent makes them. On the other hand, actions by other agents have effect from the time they have been executed 9.\nHaving introduced the ability to reason with narratives of events and observations, we need to face the problem of \u201cinconsistency\u201d due to conflicting observations, e.g. an agent may observe that both a fluent and its negation hold at the same time. As we have done\n9. If the time of the action is unknown at observation time, then the last rule above may be replaced by happens(O, T ) \u2190 observed( , O[ ], T ) namely the value of a fluent is changed according to observations from the moment the observations\nare made.\nabove for the set of initially atoms, we will assume that the external world is consistent too, i.e. it can never happen that both observed(f, t) and observed(\u00acf, t) belong to KB0, for any fluent f and time point t.\nHowever, we still need to cope with the frame consistency problem, which arises, e.g. given observations observed(f, t) and observed(\u00acf, t\u2032), with t 6= t\u2032. This issue is analogous to the case when two different events happen at the same time point and they initiate and terminate the same fluent. In the original EC suitable axioms for the predicates clipped and declipped are added, as given above, to avoid both a fluent and its negation holding at the same time after the happening of two such events at the same time. We adopt here a similar solution to cope with observations, namely by adding the following two axioms to the domain-independent part:\nclipped(T1, F, T2) \u2190 observed(\u00acF, T ), T1 \u2264 T < T2 declipped(T1, F, T2) \u2190 observed(F, T ), T1 \u2264 T < T2\nThis solution may be naive in some circumstances and more sophisticated solutions may be adopted, as e.g. the one proposed by Bracciali and Kakas (2004)."}, {"heading": "5.1.2 Temporal Reasoning", "text": "The temporal reasoning capability is invoked by other components of the KGP model (namely the Goal Decision capability, the State Revision transition and some of the selection operators, see Section 7) to prove or disprove that a given (possibly temporally constrained) fluent literal holds, with respect to a given theory KBTR. For the purposes of this paper KBTR is an EC theory composed of the domain-independent and domaindependent parts as given in Section 5.1.1, and of the \u201cnarrative\u201d part given by KB0. Then, given a state S, a fluent literal `[\u03c4 ] and a possibly empty set 10 of temporal constraints TC, the temporal reasoning capability |=TR is defined as\nS |=TR `[\u03c4 ] \u2227 TC iff KBTR |=LP (<) holds at(`, \u03c4) \u2227 TC.\nFor example, given the EC formulation in Section 5.1.1 for setting 2.6.1 in Section 2.6, if the state S = \u3008KB0,F , C, \u03a3\u3009 for agent svs contains\nKB0 = {observed(co, tell(co, svs, inform(arrival(tr01), 18), d, 15), 17)},\nthen S |=TR have info(svs, arrival(tr01), 18, \u03c4) \u2227 \u03c4 > 20."}, {"heading": "5.1.3 Planning", "text": "A number of abductive variants of the EC have been proposed in the literature to deal with planning problems, e.g. the one proposed by Shanahan, 1989. Here, we propose a novel variant, somewhat inspired by the E-language (Kakas & Miller, 1997), to allow situated agents to generate partial plans in a dynamic environment.\nWe will refer to KBplan = \u3008Pplan, Aplan, Iplan\u3009 as the abductive logic program where: 10. Here and in the remainder of the paper sets are seen as conjunctions, where appropriate.\n\u2022 Aplan = {assume holds, assume happens}, namely we consider two abducible predicates, corresponding to assuming that a fluent holds or that an action occurs, respectively, at a certain time point;\n\u2022 Pplan is obtained by adding to the core EC axioms and the \u201cnarrative\u201d given by KB0 the following rules happens(O, T ) \u2190 assume happens(O, T ) holds at(F, T ) \u2190 assume holds(F, T )\n\u2022 Iplan contains the following set of integrity constraints holds at(F, T ), holds at(\u00acF, T ) \u21d2 false assume happens(O, T ), precondition(O, P ) \u21d2 holds at(P, T ) assume happens(O, T ), not executed(O, T ), time now(T \u2032) \u21d2 T > T \u2032\nThese integrity constraints in Iplan prevent the generation of (partial) plans which are unfeasible. The first integrity constraint makes sure that no plan is generated which entails that a fluent and its negation hold at the same time. The second integrity constraint makes sure that, if a plan requires an action to occur at a certain time point, the further goal of enforcing the preconditions of that action to hold at that time point is taken into account in the same plan. This means that, if those preconditions are not already known to hold, the plan will need to accommodate actions to guarantee that they will hold at the time of execution of the action. Finally, the last integrity constraint forces all assumed unexecuted actions in a plan to be executable in the future, where the predicate time now( ) is meant to return the current time.\nIt is worth recalling that, in concrete situations, Pplan and Iplan will also contain domaindependent rules and constraints. Domain-dependent rules may be needed not only to define initiates, terminates, initially and precondition, but they may also contain additional rules/integrity constraints expressing ramifications, e.g.\nholds at(f, T ) \u21d2 holds at(f1, T ) \u2228 . . . \u2228 holds at(fn, T ) for some specific fluents in the domain. Moreover, integrity constraints may represent specific properties of actions and fluents in the domain. As an example, a domain-dependent constraint could express that two actions of some type cannot be executed at the same time, e.g.\nholds at(tell(c,X, accept request(R), D), T ), holds at(tell(c,X, refuse request(R), D), T ) \u21d2 false Intuitively, constructing a (partial) plan for a goal (that is a given leaf node in the current forest) amounts to identifying actions and further sub-goals allowing to achieve the goal, while assuming that all other nodes in the forest, both executable and non-executable, are feasible. Concretely, the abductive logic program KBplan supports partial planning as follows. Whenever a plan for a given goal requires the agent to execute an action, a[\u03c4 ] say, the corresponding atom assume happens(a, \u03c4) is assumed, which amounts to intending to execute the action (at some concrete time instantiating \u03c4). On the other hand, if a plan for a given goal requires to plan for a sub-goal, `[\u03c4 ] say, the corresponding atom assume holds(`, \u03c4) may be assumed, which amounts to setting the requirement that further planning will be needed for the sub-goal itself. Notice that if only total plans are taken into account, no atoms of the form assume holds( , ) will ever be generated.\nFormally, let KBnowplan be KBplan \u222a {time now(now)}, where now is a time constant (intuitively, the time when the planning capability is invoked). Then, the planning capability |=nowplan is specified as follows 11.\nLet S = \u3008KB0,F , C, \u03a3\u3009 be a state, and G = `[\u03c4 ] be a mental goal labeling a leaf node in a tree T of F . Let also\nCA = {assume happens(a, \u03c4 \u2032) | a[\u03c4 \u2032] \u2208 nodes(F)}, CG = {assume holds(`\u2032, \u03c4 \u2032) | `\u2032[\u03c4 \u2032] \u2208 nodes(F) \\ {`[\u03c4 ]}}\nand\n\u2022 \u22060 = CA \u222a CG \u2022 C0 = C \u222a \u03a3.\nThen, S, G |=nowplan (Xs, TC)\niff\nXs = {a[\u03c4 \u2032] | assume happens(a, \u03c4 \u2032) \u2208 \u2206} \u222a {`\u2032[\u03c4 \u2032] | assume holds(`\u2032, \u03c4 \u2032) \u2208 \u2206}\nfor some (\u2206, TC) which is an abductive answer for holds at(`, \u03c4), wrt (KBnowplan, \u22060, C0). If no such abductive answer exists, then S, G |=nowplan \u22a5, where \u22a5 is used here to indicate failure (i.e. that no such abductive answer exists).\nAs an example, consider setting 2.6.2 in Section 2.6. The domain-dependent part of KBplan for agent psa (looking after the businessman in our scenario) contains\ninitiates(buy ticket online(From, To), T, have ticket(From, To)) precondition(buy ticket online(From, To), available connection) precondition(buy ticket online(From, To), available destination(To))\nThe goal G is have ticket(madrid, denver, \u03c4). Assume F only consists of a single tree consisting solely of the root G, thus CA = CG = {}. Then, S, G |=nowplan (Xs, TC) where\nXs = {buy ticket online(madrid, denver, \u03c4 \u2032), available connection(\u03c4 \u2032\u2032), available destination(denver, \u03c4 \u2032\u2032\u2032)}\nand TC = {\u03c4 \u2032 < \u03c4, \u03c4 \u2032 = \u03c4 \u2032\u2032 = \u03c4 \u2032\u2032\u2032, \u03c4 \u2032 > now}."}, {"heading": "5.1.4 Reactivity", "text": "This capability supports the reasoning of reacting to stimuli from the external environment as well as to decisions taken while planning.\nAs knowledge base KBreact supporting reactivity we adopt an extension of the knowledge base KBplan as follows. KBreact = \u3008Preact, Areact, Ireact\u3009 where\n\u2022 Preact = Pplan 11. For simplicity we present the case of planning for single goals only.\n\u2022 Areact = Aplan \u2022 Ireact = Iplan \u222aRR\nwhere RR is a set of reactive constraints, of the form\nBody \u21d2 Reaction, TC\nwhere\n\u2022 Reaction is either assume holds(`, T ), `[T ] being a timed fluent literal, or assume happens(a, T ), a[T ] being a timed action literal, 12 and\n\u2022 Body is a non-empty conjunction of items of the form (where `[X] is a timed fluent literal and a[X] is a timed action literal, for any X):\n(i) observed(`, T \u2032),\n(ii) observed(c, a[T \u2032], T \u2032\u2032),\n(iii) executed(a, T \u2032), (iv) holds at(`, T \u2032),\n(v) assume holds(`, T \u2032),\n(vi) happens(a, T \u2032),\n(vii) assume happens(a, T \u2032),\n(viii) temporal constraints on (some of) T, T \u2032, T \u2032\u2032\nwhich contains at least one item from one of (i), (ii) or (iii).\n\u2022 TC are temporal constraints on (some of) T, T \u2032, T \u2032\u2032.\nAs for integrity constraints in abductive logic programming, all variables in Body are implicitly universally quantified over the whole reactive constraint, and all variables in Reaction, TC not occurring in Body are implicitly existentially quantified on the righthand side of the reactive constraint. 13\nNotice that Body must contain at least a trigger, i.e. a condition to be evaluated in KB0. Intuitively, a reactive constraint Body \u21d2 Reaction, TC is to be interpreted as follows: if (some instantiation of) all the observations in Body hold in KB0 and (some corresponding instantiation of) all the remaining conditions in Body hold, then (the appropriate instantiation of) Reaction, with associated (the appropriate instantiation of) the\n12. Here and below, with an abuse of notation, we use the notions of timed fluent and action literals liberally and allow them to be non-ground, even though we have defined timed fluent and action literals as ground except possibly for the time parameter. 13. Strictly speaking, syntactically reactive constraints are not integrity constraints (due to the presence of a conjunction, represented by \u201c,\u201d, rather than a disjunction in the head). However, any reactive constraint Body \u21d2 Reaction, TC can be transformed into an integrity constraint Body \u21d2 New with a new clause New \u2190 Reaction, TC in Preact. Thus, with an abuse of notation, we treat reactive constraints as integrity constraints.\ntemporal constraints TC, should be added to F and C, respectively. Notice that Reaction is an abducible so that no planning is performed by the reactivity capability.\nFormally, let KBnowreact be the theory KBreact \u222a {time now(now)}, where now is a time constant (intuitively, the time when the capability is invoked). Then, the reactivity capability |=nowreact is specified as follows. Let S = \u3008KB0,F , C,\u03a3\u3009 be a state. Let\nCA = {assume happens(a, \u03c4) | a[\u03c4 ] \u2208 nodesnr(F)},\nCG = {assume holds(`, \u03c4) | `[\u03c4 ] \u2208 nodesnr(F)}\nand\n\u2022 \u22060 = CA \u222a CG\n\u2022 C0 = C \u222a \u03a3.\nThen, S |=nowreact (Xs, TC)\niff Xs = {a[\u03c4 ] | assume happens(a, \u03c4) \u2208 \u2206} \u222a {`[\u03c4 ] | assume holds(`, \u03c4) \u2208 \u2206}\nfor some (\u2206, TC) which is an abductive answer for the query true wrt (KBnowreact, \u22060, C0). If no such abductive answer exists, then S |=nowreact \u22a5, where \u22a5 is used here to indicate failure (i.e. that no such abductive answer exists).\nAs an example, consider setting 2.6.1 in Section 2.6, and KBplan as given in Sections 5.1.1 and 5.1.3. Let RR of agent svs consist of:\nobserved(C, tell(C, svs, request(Q), D, T0), T ), holds at(have info(svs, Q, I), T ) \u21d2 assume happens(tell(svs, C, inform(Q, I), D), T \u2032), T \u2032 > T\nobserved(C, tell(C, svs, request(Q), D, T0), T ), holds at(no info(svs,Q), T ) \u21d2 assume happens(tell(svs, C, refuse(Q), D), T \u2032), T \u2032 > T\nThen, given now = 30 and S = \u3008KB0,F , C, \u03a3\u3009 with KB0 = {observed(co, tell(co, svs, inform(arrival(tr01), 18), d1, 15), 17), observed(psa, tell(psa, svs, request(arrival(tr01)), d2, 20), 22)} we obtain\nS |=nowreact ({tell(svs, psa, inform(arrival(tr01), 18), d2, \u03c4)}, \u03c4 > 30)."}, {"heading": "5.1.5 Identification Of Preconditions", "text": "This capability is used by KGP agents to determine the preconditions for the executability of actions which are planned for. These preconditions are defined in the domain-dependent part of the EC by means of a set of rules of the form precondition(O, F ), representing that the fluent F is a precondition for the executability of an action with action operator O (see 5.1.1). Let KBpre be the subset of KBTR containing the rules defining precondition( , ).\nThen the identification of preconditions capability |=pre is specified as follows. Given a state S = \u3008KB0,F , C, \u03a3\u3009 and a timed action literal a[\u03c4 ]\nS, a[\u03c4 ] |=pre Cs\niff\nCs = \u2227{`[\u03c4 ] | KBpre |=LP precondition(a, `)}14."}, {"heading": "5.1.6 Identification Of Effects", "text": "This capability is used by KGP agents to determine the effects of actions that have already been executed, in order to check whether these actions have been successful. Note that actions may have been unsuccessful because they could not be executed, or were executed but they did not have the expected effect. Both are possible in situations where the agent does not have full knowledge about the environment in which it is situated.\nThese effects are defined in the domain-dependent part of the EC by means of the set of rules defining the predicates initiates and terminates. Let KBeff be the theory consisting of the domain-dependent and domain-independent parts of the EC, as well as the narrative part KB0. Then, the identification of effects |=eff is specified as follows. Given a state S = \u3008KB0,F , C, \u03a3\u3009 and an action operator a[t],\nS, a[t] |=eff `\niff\n\u2022 ` = f and KBeff |=LP initiates(a, t, f) \u2022 ` = \u00acf and KBeff |=LP terminates(a, t, f)"}, {"heading": "5.2 Constraint Solving", "text": "The Constraint Solving capability can be simply defined in terms of the structure < and the |=< notion presented in Section 3.1. Namely, given a state S = \u3008KB0,F , C, \u03a3\u3009 and a set of constraints TC:\n\u2022 S |=cs TC iff |=< C \u2227 \u03a3 \u2227 TC; \u2022 there exists a total valuation \u03c3 such that S, \u03c3 |=cs TC iff there exists a total valuation\n\u03c3 such that \u03c3 |=< C \u2227 \u03a3 \u2227 TC."}, {"heading": "5.3 Goal Decision", "text": "The Goal Decision reasoning capability allows the agent to decide, at a given time point, the (non-reactive) top-level goals to be pursued, for which it will then go on to generate plans aiming at achieving them. The generated goals are the goals of current preferred interest but this interest may change over time. 14. We assume that \u2227{} = true.\nThe Goal Decision capability operates according to a theory, KBGD, in which the agent represents its goal preference policy. KBGD includes KBTR and thus the dynamic, observed knowledge, KB0, in the current state of the agent. KBGD is expressed in a variant of LPP described in Section 3.3, whereby the rules in the lower or basic part P of the LPP theory T have the form (T being a possibly empty sequence of variables): n(\u03c4, T ) : G[\u03c4, T ] \u2190 B[T ], C[T ] where\n\u2022 \u03c4 is a time variable, existentially quantified with scope the head of the rule and not a member of T ;\n\u2022 all variables except for \u03c4 are universally quantified with scope the rule; \u2022 the head G[\u03c4, T ] of the rule consists of a fluent literal conjoined with a (possibly\nempty) set of temporal constraints, represented as \u3008`[\u03c4 ], TC[\u03c4, T ]\u3009; \u2022 B(T ) is a non-empty conjunction of literals on a set of auxiliary predicates that can\ninclude atoms of the form holds at(`, T \u2032), where `[T \u2032] is a timed fluent literal, and the atom time now(T \u2032\u2032) for some variables T \u2032, T \u2032\u2032;\n\u2022 the conditions of the rule are constrained by the (possibly empty) temporal constraints C[T ].\nAny such rule again represents all of its ground instances under any total valuation of the variables in T that satisfies the constraints C[T ]. Each ground instance is named by the corresponding ground instance of n(\u03c4, T ). Intuitively, when the conditions of one such rule are satisfied at a time now that grounds the variable T \u2032\u2032 with the current time at which the capability is applied, then the goal in the head of the rule is sanctioned as one of the goals that the agent would possibly prefer to achieve at this time. The decision whether such a goal is indeed preferred would then depend on the high-level or strategy part H of KBGD, containing priority rules, as described in Section 3.3, between the rules in the lower-part or between other rules in H. These priority rules can also include temporal atoms of the form holds at(`, T \u2032) and the atom time now(T \u2032\u2032) in their conditions.\nTo accommodate this form of rules we only need to extend our notion of incompatibility I in T to be defined on conclusions \u3008`(\u03c4), TC[\u03c4, T ]\u3009. To simplify the notation, in the remainder we often write \u3008`(\u03c4), TC\u3009 instead of \u3008`(\u03c4), TC[\u03c4, T ]\u3009.\nThe incompatibility I can be defined in different ways. For example, a (relatively) weak notion of incompatibility is given as follows. Two pairs \u3008`1(\u03c41), TC1\u3009 and \u3008`2(\u03c42), TC2\u3009 are incompatible iff for every valuation \u03c3 such that TC1 and TC2 are both satisfied, the ground instances of `1(\u03c41)\u03c3 and `2(\u03c42)\u03c3 are incompatible. A stronger notion would require that it is sufficient for only one such valuation \u03c3 to exist that makes the corresponding ground literals incompatible.\nLet us denote by KBnowGD the theory KBGD \u222a {time now(now)}, where now is a time constant. Then, the goal decision capability, |=nowGD , is defined directly in terms of the preference entailment, |=pr, of LPP (see Section 3.3), as follows.\nGiven a state S = \u3008KB0,F , C,\u03a3\u3009, S |=nowGD Gs\nwhere\nGs = {G1, G2, . . . , Gn}, n \u2265 0, Gi = \u3008`i(\u03c4i), TCi\u3009 for all i = 1, . . . , n iff Gs is a maximal set such that\nKBnowGD |=pr \u3008`1(\u03c41), TC1\u3009 \u2227 . . . \u2227 \u3008`n(\u03c4n), TCn\u3009. This means that a new set of goals Gs is generated that is currently (sceptically) preferred under the goal preference policy represented in KBGD and the current information in KB0. Note that any two goals in Gs are necessarily compatible with each other. There are two special cases where there are no sceptically preferred goals at the time now. The first one concerns the case where there are no goals that are currently sanctioned by the (lower-part) of KBGD. When this is so |=nowGD returns an empty set of goals (n = 0). The second special case occurs when there are at least two goals which are each separately credulously preferred but these goals are incompatible which each other. Then S |=nowGD \u22a5, where \u22a5 is used to indicate failure in identifying new goals to be pursued.\nAs an example, consider the San Vincenzo scenario where the psa agent needs to decide whether to return home or to recharge its battery. The agent\u2019s goals are categorised and assigned priority according to their category and possibly other factors. The KBGD expressing this is given as follows:\n\u2022 The low-level part contains the rules:\nn(rh, \u03c41) : \u3008return home(\u03c41), {\u03c41 < T \u2032}\u3009 \u2190 holds at(finished work, T ), holds at(\u00acat home, T ), time now(T ), T \u2032 = T + 6 n(rb, \u03c42) : \u3008recharge battery(\u03c42), {\u03c42 < T \u2032}\u3009 \u2190 holds at(low battery, T ), time now(T ), T \u2032 = T + 2\n\u2022 The auxiliary part contains, in addition to KBTR and KB0, the following rules that specify the category of each goal and the relative urgency between these categories:\ntypeof(return home, required) typeof(recharge battery, operational) more urgent wrt type(operational, required)\n\u2022 The incompatibility part consists of\nincompatible(return home(T ), recharge battery(T ))\nNamely, the two goals are pairwise incompatible, i.e. the agent can only do one of these goals at a time.\n\u2022 The high-level part contains the following priority rule:\ngd pref(X, Y ) : n(X, ) \u227a n(Y, ) \u2190 typeof(X,XT ), typeof(Y, Y T ), more urgent wrt type(XT, Y T ).\nThen, for now = 1 and current state S = \u3008KB0,F , C, \u03a3\u3009 such that finished work and away from home both hold (by temporal reasoning) at time now, we have that\nS |=nowGD {\u3008return home(\u03c41), {\u03c41 < 7}\u3009}.\nSuppose instead that KB0 contains observed(low battery, 1). Then, using the weak notion of incompatibility, requiring that\nfor every \u03c3 such that \u03c3 |=cs {\u03c41 < 7, \u03c42 < 3} it holds that incompatible(return home(\u03c41)\u03c3, recharge battery(\u03c42)\u03c3)\nwe have:\nS |=nowGD {\u3008return home(\u03c41), {\u03c41 < 7}\u3009, \u3008recharge battery(\u03c42), {\u03c42 < 3}\u3009}.\nIndeed, for \u03c3 = {\u03c41 = 3, \u03c42 = 2}, incompatible(return home(3), recharge battery(2)) does not hold. However, using the stronger notion of incompatibility, requiring that\nthere exists \u03c3 such that \u03c3 |=cs {\u03c41 < 7, \u03c42 < 3} it holds that incompatible(return home(\u03c41)\u03c3, recharge battery(\u03c42)\u03c3)\nwe have:\nS |=nowGD {\u3008recharge battery(\u03c42), {\u03c42 < 3}\u3009}.\nSuppose now that KBGD contains a second operational goal \u3008replace part(\u03c43), {\u03c43 < 5}\u3009 that is also sanctioned by a rule in its lower part at time now = 1. Then under the stronger form of incompatibility the goal decision capability at now = 1 will return \u22a5 as both these operational goals are credulously preferred but none of them is sceptically preferred."}, {"heading": "6. Physical Capabilities", "text": "In addition to the reasoning capabilities we have defined so far, an agent is equipped with physical capabilities that allow it to experience the world in which it is situated; this world consists of other agents and/or objects that provide an environment for the agents in which to interact and communicate.\nWe identify two types of physical capabilities: sensing and actuating. In representing these capabilities we abstract away from the sensors and the actuators that an agent would typically rely upon to access and affect the environment. We will also assume that these sensors and actuators are part of the agent\u2019s body, which we classify as an implementation issue (Stathis et al., 2004).\nThe physical sensing capability models the way an agent interacts with its external environment in order to inspect it, e.g. to find out whether or not some fluent holds at a given time. On the other hand, the physical actuating capability models the way an agent interacts with its external environment in order to affect it, by physically executing its actions.\nWe represent the sensing physical capability of an agent as a function of the form:\nsensing(L, t) = L\u2032\nwhere:\n\u2022 L is a (possibly empty) set of \u2013 fluent literals f ,\n\u2013 terms of the form c : a (meaning that agent c has performed action a),\nall to be sensed at a concrete time t, and\n\u2022 L\u2032 is a (possibly empty) set of elements s\u2032 such that \u2013 s\u2032 is a term f : v, f being a fluent and v \u2208 {true, false}, meaning that fluent f\nhas been observed to have value v (namely to be true or to be false) at time t, or \u2013 s\u2032 is a term of the form c : a[t\u2032], c being an agent name and a being an action, meaning that agent c has performed action a at time t\u2032.\nNote that physical sensing requires the time-stamp t to specify the time at which it is applied within transitions. Note also that, given a non-empty set L, sensing(L, t) may be partial, e.g. for some fluent f \u2208 L, it can be that neither f : true \u2208 L\u2032, nor f : false \u2208 L\u2032.\nSimilarly, we represent the physical actuating capability as a function\nactuating(As, t) = As\u2032\nwhere:\n\u2022 As is a set of action literals {a1, \u00b7 \u00b7 \u00b7 , an}, n > 0, that the agent instructs the body to actuate at time t;\n\u2022 As\u2032 \u2286 As is the subset of actions that the body has actually managed to perform. The meaning of an action a belonging to As and not belonging to As\u2032 is that the physical actuators of the agent\u2019s body were not able to perform a in the current situation. It is worth pointing out that if an action a belongs to As\u2032 it does not necessarily mean that the effects of a have successfully been reached. Indeed, some of the preconditions of the executed action (i) may have been wrongly believed by the agent to be true at execution time (as other agents may have interfered with them) or (ii) the agent may have been unaware of these preconditions. For example, after having confirmed availability, the agent may have booked a hotel by sending an e-mail, but (i) some other agent has booked the last available room in the meanwhile, or (ii) the agent did not provide a credit card number to secure the booking. In other words, the beliefs of the agent (as held in KB0) may be incorrect and/or incomplete.\nIn Section 7 and Section 8 below, we will see that AOI (Active Observation Introduction) can be used to check effects of actions (identified by the fES effect selection operator, in turn using the |=eff reasoning capability) after actions have been executed. Moreover, SI (Sensing Introduction) can be used to check preconditions of actions (identified by the fPS precondition selection operator, in turn using the |=pre reasoning capability) just before they are executed, to make sure that the actions are indeed executable. Overall, the following cases may occur:\n\u2022 an action belongs to As\u2032 because it was executed and \u2013 its preconditions held at the time of execution and its effects hold in the envi-\nronment after execution;\n\u2013 its preconditions were wrongly believed to hold at the time of execution (because the agent has partial knowledge of the environment or its KBplan is incorrect) and as a consequence its effects do not hold after execution;\n\u2013 its preconditions were known not to hold at the time of execution (e.g. because the agent observed only after having planned that they did not hold, but had no time to -replan) and as a consequence its effects do not hold after execution;\n\u2022 an action belongs to As\\As\u2032 because it was not executed (the body could not execute it).\nThe actuating physical capability does not check preconditions/effects: this is left to other capabilities called within transitions before and/or after the transition invoking actuating, as we will show below. As before, the way the body will carry out the actions is an implementation issue (Stathis et al., 2004)."}, {"heading": "7. Transitions", "text": "The KGP model relies upon the state transitions GI, PI, RE, SI, POI, AOI, AE, SR, defined below using the following representation\n(T) \u3008KB0,F , C, \u03a3\u3009 X \u3008KB\u20320,F \u2032, C\u2032, \u03a3\u2032\u3009 now\nwhere T is the name of the transition, \u3008KB0,F , C, \u03a3\u3009 is the agent\u2019s state before the transition is applied, X is the input for the transition, now is the time of application of the transition, \u3008KB\u20320,F \u2032, C\u2032, \u03a3\u2032\u3009 is the revised state, resulting from the application of the transition T with input X at time now in state \u3008KB0,F , C, \u03a3\u3009. Please note that most transitions only modify some of the components of the state. Also, for some transitions (namely GI, RE, POI, SR) the input X is always empty and will be omitted. For the other transitions (namely PI, SI, AOI, AE) the input is always non-empty (see Section 9) and is selected by an appropriate selection operator (see Section 8).\nBelow we define each transition formally, by defining \u3008KB\u20320,F \u2032, C\u2032, \u03a3\u2032\u3009. Note that we assume that each transition takes care of possible renaming of time variables in the output of capabilities (if a capability is used by the transition), in order to guarantee that each goal/action in the forest is univocally identified by its time variable."}, {"heading": "7.1 Goal Introduction", "text": "This transition takes empty input. It calls the Goal Decision capability to determine the new (non-reactive) top-level goals of the agent. If this capability returns a set of goals, this means that the circumstances have now possibly changed the preferred top-level goals of the agent and the transition will reflect this by changing the forest in the new state to consist of one tree for each new (non-reactive) goal. On the other hand, if the Goal Decision capability does not return any (non-reactive) goals (namely it returns \u22a5) the state is left unchanged, as, although the goals in the current state are no longer sceptically preferred they may still be credulously preferred and, since there are no others to replace them, the agent will carry on with its current plans to achieve them.\n(GI) \u3008KB0,F , C, \u03a3\u3009 \u3008KB0,F \u2032, C\u2032, \u03a3\u3009 now\nwhere, given that S = \u3008KB0,F , C,\u03a3\u3009\n(i) If S |=nowGD \u22a5, then\n\u2013 F \u2032 = F \u2013 C\u2032 = C\n(ii) otherwise, if S |=nowGD Gs and Gs 6= \u22a5, then\n\u2013 F \u2032 is defined as follows: \u2217 nr(F \u2032) = {Tg[\u03c4 ] | \u3008g[\u03c4 ], \u3009 \u2208 Gs} where Tg[\u03c4 ] is a tree consisting solely of the\nroot g[\u03c4 ] \u2217 r(F \u2032) = {}\n\u2013 C\u2032 = {TC | \u3008 , TC\u3009 \u2208 Gs}\nThis transition drops (top-level) goals that have become \u201csemantically\u201d irrelevant (due to changed circumstances of the agent or changes in its environment), and replaces them with new relevant goals. We will see, in Section 7.8, that goals can also be dropped because\nof the book-keeping activities of the State Revision (SR) transition, but that transition can never add to the set of goals.\nNote that, as GI will replace the whole forest in the old state by a new forest, it is possible that the agent looses valuable information that it has in achieving its goals, when one of the new preferred goals of the agent is the same as (or equivalent to) a current goal. This effect though can be minimized by calling (in the cycle theory) the GI transition only at certain times, e.g. after the current goals have been achieved or timed-out. Alternatively, the earlier formalisation of the GI transition could be modified so that, in case (ii), for all goals in Gs that already occur (modulo their temporal variables and associated temporal constraints) as roots of (non-reactive) trees in F , these trees are kept in F \u2032. A simple way to characterise (some of) these goals is as follows. Let\nXs = {\u3008g[\u03c4 ], TC, \u03c4 = \u03c4 \u2032\u3009 | \u3008g[\u03c4 ], TC\u3009 \u2208 Gs, g[\u03c4 \u2032] \u2208 Rootsnr(F) and |=cs C iff |=cs (C \u222a TC \u222a {\u03c4 = \u03c4 \u2032})}\nGs\u2032 = {\u3008g[\u03c4 ], TC\u3009 | \u3008g[\u03c4 ], TC, \u03c4 = \u03c4 \u2032\u3009 \u2208 Xs}\nThe new constraints on goals in Gs\u2032 are equivalent to the old constraints in C. For example, Gs may contain\nG = \u3008have ticket(madrid, denver, \u03c42), {\u03c42 < 12}\u3009 with have ticket(madrid, denver, \u03c41) \u2208 Rootsnr(F) and C = {\u03c41 < 12}.\nThen, G definitely belongs to Gs\u2032. Let\nnewC = \u22c3\n\u3008 ,TC,\u03c4=\u03c4 \u2032\u3009\u2208Xs TC \u222a {\u03c4 = \u03c4 \u2032}.\nCase (ii) can be redefined as follows, using these definitions of Xs, Gs\u2032 and newC:\n(ii\u2032) otherwise, if S |=nowGD Gs and Gs 6= \u22a5, then, if it is not the case that |=cs C \u222a newC, then F \u2032 and C\u2032 are defined as in the earlier case (ii), otherwise (if |=cs C \u222a newC):\n\u2013 F \u2032 is defined as follows: \u2217 nr(F \u2032) = {Tg[\u03c4 ] | \u3008g[\u03c4 ], \u3009 \u2208 Gs \\Gs\u2032} \u222a F(Xs)\nwhere Tg[\u03c4 ] is a tree consisting solely of the root g[\u03c4 ] and F(Xs) is the set of all trees in F with roots goals of the form g[\u03c4 \u2032] such that \u3008g[\u03c4 ], , \u03c4 = \u03c4 \u2032\u3009 \u2208 Xs\n\u2217 r(F \u2032) = {} \u2013 C\u2032 = C \u222a {TC | \u3008 , TC\u3009 \u2208 Gs \\Gs\u2032} \u222a newC.\nNote that we keep all temporal constraints in the state, prior to the application of GI, but we force all variables of new goals that remain in the state after GI to be rewritten using the old identifiers of the goals."}, {"heading": "7.2 Reactivity", "text": "This transition takes empty input. It calls the Reactivity capability in order to determine the new top-level reactive goals in the state (if any), leaving the non-reactive part unchanged. If no new reactive goals exist, the reactive part of the new state will be empty.\n(RE) \u3008KB0,F , C, \u03a3\u3009 \u3008KB0,F \u2032, C\u2032, \u03a3\u3009 now\nwhere, given that S = \u3008KB0,F , C,\u03a3\u3009: (i) If S |=nowreact \u22a5, then\n\u2022 F \u2032 is defined as follows: \u2013 r(F \u2032) = {} \u2013 nr(F \u2032) = nr(F)\n\u2022 C\u2032 = C (ii) otherwise, if S |=nowreact (X s, TC), then\n\u2022 F \u2032 is defined as follows: \u2013 nr(F \u2032) = nr(F) \u2013 r(F \u2032) = {Tx[\u03c4 ] | x[\u03c4 ] \u2208 X s} where Tx[\u03c4 ] is a tree consisting solely of the root x[\u03c4 ] \u2022 C\u2032 = C \u222a TC\nNote that there is an asymmetry between case (ii) of GI and case (ii) of RE, as GI eliminates all reactive goals in this case, whereas RE leaves all non-reactive goals unchanged. Indeed, reactive goals may be due to the choice of specific non-reactive goals, so when the latter change the former need to be re-evaluated. Instead, non-reactive goals are not affected by newly acquired reactive goals (that are the outcome of enforcing reactive rules).\nNote also that in case (ii), similarly to GI, as RE replaces the whole (reactive) forest in the old state by a new (reactive) forest, it is possible that the agent loses valuable information that it has in achieving its reactive goals, when one of the new reactive goals is the same as (or equivalent to) a current goal. A variant of case (ii) for RE, mirroring the variant given earlier for GI and using |=cs as well, can be defined to avoid this problem."}, {"heading": "7.3 Plan Introduction", "text": "This transition takes as input a non-executable goal in the state (that has been selected by the goal selection operator, see Section 8) and produces a new state by calling the agent\u2019s Planning capability, if the selected goal is a mental goal, or by simply introducing a new sensing action, if the goal is a sensing goal.\n(PI) \u3008KB0,F , C,\u03a3\u3009 G \u3008KB0,F \u2032, C\u2032, \u03a3\u3009 now\nwhere G is the input goal (selected for planning in some tree T in F , and thus a leaf, see Section 8) and\nF \u2032 = (F \\ {T | G is a leaf in T }) \u222a New C\u2032 = C \u222a TC\nwhere New and TC are obtained as follows, S being \u3008KB0,F , C, \u03a3\u3009. (i) if G is a mental goal: let S,G |=nowplan P . Then,\n\u2013 either P = \u22a5 and New = {T } and TC = {}, \u2013 or P = (X s, TC) and New = {T \u2032} where T \u2032 is obtained from T by adding each element of X s as a child of G.\n(ii) if G = `[\u03c4 ] is a sensing goal, and a child of a goal G\u2032 in T : New = {T \u2032} where T \u2032 is T with (a node labelled by) sense(`, \u03c4 \u2032) as a new child of G\u2032 (here \u03c4 \u2032 is a new time variable) and\nTC = {\u03c4 \u2032 \u2264 \u03c4}. (iii) if G = `[\u03c4 ] is a sensing goal, and the root of T :\nNew = {T , T \u2032} where T \u2032 is a tree consisting solely of the root (labelled by) sense(`, \u03c4 \u2032) (here \u03c4 \u2032 is a new time variable) and TC = {\u03c4 \u2032 \u2264 \u03c4}."}, {"heading": "7.4 Sensing Introduction", "text": "This transition takes as input a set of fluent literals that are preconditions of some actions in the state and produces a new state by adding sensing actions as leaves in (appropriate) trees in its forest component. Note that, when SI is invoked, these input fluent literals are selected by the precondition selection operator, and are chosen amongst preconditions of actions that are not already known to be true (see Section 8).\n(SI) \u3008KB0,F , C, \u03a3\u3009 SPs\n\u3008KB0,F \u2032, C\u2032, \u03a3\u3009 now\nwith SPs a non-empty set of preconditions of actions (in the form of pairs \u201cprecondition, action\u201d) in some trees in F , where, given that:\n- New = {\u3008`[\u03c4 ], A, sense(`, \u03c4 \u2032)\u3009 | \u3008`[\u03c4 ], A\u3009 \u2208 SPs and \u03c4 \u2032 is a fresh variable} - addSibling(T , A, SA) denotes the tree obtained by adding all elements in SA as new\nsiblings of A to the tree T such that leaf(A, T ) then\nF \u2032 = F \\ {T | leaf(A, T ) and \u3008`[\u03c4 ], A\u3009 \u2208 SPs} \u222a {addSibling(T , A, SA) | leaf(A, T ) and\nSA = {sense(`, \u03c4 \u2032)|\u3008`[\u03c4 ], A, sense(`[\u03c4 \u2032])\u3009 \u2208 New}}\nC\u2032 = C \u222a {\u03c4 \u2032 < \u03c4 | \u3008`[\u03c4 ], , sense(`[\u03c4 \u2032])\u3009 \u2208 New}\nBasically, for each fluent literal selected by the precondition selection operator as a precondition of an action A, a new sensing action is added as a sibling of A, and the constraint expressing that this sensing action must be performed before A is added to the current set of temporal constraints."}, {"heading": "7.5 Passive Observation Introduction", "text": "This transition updates KB0 by adding new observed facts reflecting changes in the environment. These observations are not deliberately made by the agent, rather, they are \u201cforced\u201d upon the agent by the environment. These observations may be properties in the form of positive or negative fluents (for example that the battery is running out) or actions performed by other agents (for example messages addressed to the agent).\n(POI) \u3008KB0,F , C, \u03a3\u3009 \u3008KB\u20320,F , C, \u03a3\u3009 now\nwhere, if sensing(\u2205, now) = L, then KB\u20320 = KB0 \u222a\n{observed(f, now) | f : true \u2208 L} \u222a {observed(\u00acf, now) | f : false \u2208 L} \u222a {observed(c, a[t], now) | c : a[t] \u2208 L}."}, {"heading": "7.6 Active Observation Introduction", "text": "This transition updates KB0 by adding new facts deliberately observed by the agent, which seeks to establish whether or not some given fluents hold at a given time. These fluents are selected by the effect selection operator (see Section 8) and given as input to the transition. Whereas POI is not \u201cdecided\u201d by the agent (the agent is \u201cinterrupted\u201d and forced an observation by the environment), AOI is deliberate. Moreover, POI may observe fluents and actions, whereas AOI only considers fluents (that are effects of actions executed by the agent, as we will see in Section 8 and in Section 9).\n(AOI) \u3008KB0,F , C,\u03a3\u3009 SFs\n\u3008KB\u20320,F , C, \u03a3\u3009 now\nwhere SFs = {f1, . . . , fn}, n > 0, is a set of fluents selected for being actively sensed (by the effect selection operator), and, if sensing(SFs, now) = L, then\nKB\u20320 = KB0 \u222a {observed(f, now) | f : true \u2208 L} \u222a {observed(\u00acf, now) | f : false \u2208 L}."}, {"heading": "7.7 Action Execution", "text": "This transition updates KB0, recording the execution of actions by the agent. The actions to be executed are selected by the action selection operator (see Section 8) prior to the transition, and given as input to the transition.\n(AE) \u3008KB0,F , C,\u03a3\u3009 SAs\n\u3008KB\u20320,F , C, \u03a3\u2032\u3009 now\nwhere SAs is a non-empty set of actions selected for execution (by the action selection operator), and\n\u2022 let A be the subset of all non-sensing actions in SAs and S be the subset of all sensing actions in SAs;\n\u2022 let sensing(S\u2032, now) = L\u2032, where S\u2032 = {f | sense(f, \u03c4) \u2208 S} \u2022 let sensing(S\u2032\u2032, now) = L\u2032\u2032, where S\u2032\u2032 = {c : a | sense(c : a, \u03c4) \u2208 S} \u2022 let actuating(A\u2032, now) = A\u2032\u2032, where A\u2032 = {a | a[\u03c4 ] \u2208 A}. Then:\nKB\u20320 = KB0 \u222a {executed(a, now) | a \u2208 A\u2032\u2032} \u222a {observed(f, now) | f : true \u2208 L\u2032} \u222a {observed(\u00acf, now) | f : false \u2208 L\u2032} {observed(c, a[t], now) | c : a[t] \u2208 L\u2032\u2032 and\n\u2203\u03c3 such that \u03c3 |=cs C \u2227 \u03c4 = t where sense(c : a, \u03c4) \u2208 S}\nand\n\u03a3\u2032 = \u03a3 \u222a {\u03c4 = now | a[\u03c4 ] \u2208 SAs \u2227 a \u2208 A\u2032\u2032} \u222a {\u03c4 = now | sense(f, \u03c4) \u2208 SAs \u2227 (f : ) \u2208 L\u2032} \u222a {\u03c4 = t | c : a[t] \u2208 L\u2032\u2032 and \u2203\u03c3 such that \u03c3 |=cs C \u2227 \u03c4 = t where sense(c : a, \u03c4) \u2208 S}."}, {"heading": "7.8 State Revision", "text": "The SR transition revises a state by removing all timed-out goals and actions and all goals and actions that have become obsolete because one of their ancestors is already believed to have been achieved. We will make use of the following terminology.\nNotation 7.1 Given a state S, a timed fluent literal `[\u03c4 ], a timed fluent literal or action operator x[\u03c4 ], and a time-point now:\n\u2022 achieved(S, `[\u03c4 ], now) stands for there exists a total valuation \u03c3 such that S, \u03c3 |=cs \u03c4 \u2264 now and S |=TR `[\u03c4 ]\u03c3\n\u2022 timed out(S, x[\u03c4 ], now) stands for there exists no total valuation \u03c3 such that S, \u03c3 |=cs \u03c4 > now.\nThen, the specification of the transition is as follows.\n(SR) \u3008KB0,F , C, \u03a3\u3009 \u3008KB0,F \u2032, C, \u03a3\u3009 now\nwhere F \u2032 is the set of all trees in F pruned so that nodes(F \u2032) is the biggest subset of nodes(F) consisting of all goals/actions x[\u03c4 ] in some tree T in F such that (here S = \u3008KB0,F , C,\u03a3\u3009):\n(i) \u00actimed out(S, x[\u03c4 ], now), and (ii) if x is an action operator, it is not the case that executed(x, t) \u2208 KB0 and (\u03c4 = t) \u2208 \u03a3,\nand\n(iii) if x is a fluent literal, \u00acachieved(S, x[\u03c4 ], now), and (iv) for every y[\u03c4 \u2032] \u2208 siblings(x[\u03c4 ],F)\n\u2013 either y[\u03c4 \u2032] \u2208 siblings(x[\u03c4 ],F \u2032), \u2013 or y[\u03c4 \u2032] 6\u2208 siblings(x[\u03c4 ],F \u2032) and\n\u2217 if y is a fluent literal then achieved(S, y[\u03c4 \u2032], now), \u2217 if y is an action literal then executed(y, t) \u2208 KB0 and \u03c4 \u2032 = t \u2208 \u03a3,\nand\n(v) if x is a sensing action operator, x[\u03c4 ] = sense(`, \u03c4), then\n\u2013 either there exists a[\u03c4 \u2032] \u2208 siblings(x[\u03c4 ],F \u2032) such that ` is a precondition of a (i.e. S, a[\u03c4 \u2032] |=pre Cs and `[\u03c4 \u2032] \u2208 Cs) and \u03c4 < \u03c4 \u2032 \u2208 C, \u2013 or there exists `[\u03c4 \u2032] \u2208 siblings(x[\u03c4 ],F \u2032) such that ` is a sensing fluent and \u03c4 < \u03c4 \u2032 \u2208 C, and\n(vi) x[\u03c4 ] is a top-level goal or parent(x[\u03c4 ],F) = P and P \u2208 nodes(F \u2032). All conditions above specify what SR keeps in the trees in the forest in the state. Intuitively, these conditions may be understood in terms of what they prevent from remaining in such trees:\n\u2022 condition (i) removes timed-out goals and actions, \u2022 condition (ii) removes actions that have already been executed, \u2022 condition (iii) removes goals that are already achieved, \u2022 condition (iv) removes goals and actions whose siblings are already timed out and\nthus deleted, by condition (i),\n\u2022 condition (v) removes sensing actions for preconditions of actions that have been deleted and for sensing goals that have been deleted,\n\u2022 condition (vi) recursively removes actions and goals whose ancestors have been removed.\nThe following example illustrates how SR is used to provide adjustment of the agent\u2019s goals and plans in the light of newly acquired information."}, {"heading": "7.9 Setting 3", "text": "The agent psa has the goal to have a museum ticket for some (state-run) museum that the businessman wants to visit, and a plan to buy the ticket. But before executing the plan psa observes that it is the European Heritage day (ehd for short), via an appropriate \u201cmessage\u201d from another agent mus (representing the museum), stating that all state-run museums in Europe give out free tickets to anybody walking in on that day. Then, the psa\u2019s goal is already achieved and both goal and plan are deleted from its state.\nLet the agent\u2019s initial state be \u3008KB0,F , C, \u03a3\u3009 with:\n\u03a3 = { } = KB0 F = {T } C = {\u03c41 \u2264 10, \u03c42 = \u03c43, \u03c43 < \u03c41}\nwhere T consists of a top-level goal g1 = have(ticket, \u03c41), with two children,\ng2 = have money(\u03c42) and a1 = buy(ticket, \u03c43), 15\nand further assuming that KBTR contains\ninitiates(ehd, T, have(ticket)) initiates(buy(O), T, have(O)) precondition(buy(O), have money).\nThe remaining knowledge bases do not play any useful role for the purposes of this example, and can therefore be considered to be empty. The \u201cmessage\u201d from the museum agent mus is added to KB0 via POI, e.g. at time 6, in the following form:\nobserved(mus, ehd(5), 6)\ni.e. at time 6 it is observed that at time 5 mus has announced that all state-run museums in Europe are free on that day. Then, via SR, at time 8 say, g1, g2 and a1 are eliminated from F , as g1 is already achieved."}, {"heading": "8. Selection Operators", "text": "The KGP model relies upon selection operators:\n\u2022 fGS (goal selection, used to provide input to the PI transition);\n\u2022 fPS (precondition selection, used to provide input to the SI transition);\n\u2022 fES (effect selection, used to provide input to the AOI transition);\n\u2022 fAS (action selection, used to provide input to the AE transition). 15. g1 and a1 can be reactive or not, as this does not matter for this example.\nSelection operators are defined in terms of (some of the) capabilities (namely Temporal Reasoning, Identification of Preconditions and Effects and Constraint Solving).\nAt a high-level of description, the selection operators can all be seen as returning the set of all items from a given initial set that satisfy a certain number of conditions. For example, given a state \u3008KB0,F , C, \u03a3\u3009, the goal selection operator returns the set of all non-executable goals in trees in F that satisfy some conditions; the precondition selection operator returns the set of all pairs, each consisting of (i) a timed fluent literal which is a precondition of some action in some tree in F and (ii) that action, satisfying some conditions; the effect selection operator returns the set of all fluent literals which are effects of actions already executed (as recorded in KB0) that satisfy some conditions; the action selection operator returns the set of all actions in trees in F that satisfy some conditions.\nThe selection operators are formally defined below."}, {"heading": "8.1 Goal Selection", "text": "Informally, the set of conditions for the goal selection operator is as follows. Given a state S = \u3008KB0,F , C, \u03a3\u3009 and a time-point t, the set of goals selected by fGS is a singleton set consisting of a non-executable goal G in some tree in F such that at time t:\n1. G is not timed out,\n2. no ancestor of G is timed out,\n3. no child of any ancestor of G is timed out,\n4. neither G, nor any ancestor of G in any tree in F is already achieved. 5. G is a leaf\nIntuitively, condition 1 ensures that G is not already timed-out, conditions 2-3 impose that G belongs to a \u201cstill feasible\u201d plan for some top-level goal in F , and condition 4 makes sure that considering G is not wasteful.\nNote that, as already mentioned in Section 5.1.3, for simplicity we select a single goal. Formally, given a state S = \u3008KB0,F , C, \u03a3\u3009 and a time-point t, let G(S, t) be the set of all non-executable goals `[\u03c4 ] \u2208 nodes(F) such that:\n1. \u00actimed out(S, `[\u03c4 ], t) 2. \u00actimed out(S,G, t) for each G \u2208 ancestors(`[\u03c4 ],F), 3. \u00actimed out(S,X, t) for each X \u2208 nodes(F) such that X is the child of some P \u2208\nancestors(`[\u03c4 ],F) 4. \u00acachieved(S, G, t) for each G \u2208 {`[\u03c4 ]} \u222a ancestors(`[\u03c4 ],F) 5. leaf(G,F) Then, if G(S, t) 6= {}:\nfGS(S, t) = {G} for some G \u2208 G(S, t). Otherwise, fGS(S, t) = {}."}, {"heading": "8.2 Effect Selection", "text": "Informally, the set of conditions for the effect selection operator is as follows. Given a state S = \u3008KB0,F , C, \u03a3\u3009 and a time-point t, fES selects all fluents f such that f or \u00acf is one of the effects of some action a[\u03c4 ] that has \u201crecently\u201d been executed.\nNote that such f (or \u00acf) may not occur in F but could be some other (observable) effect of the executed action, which is not necessarily the same as the goal that the action contributes to achieving. For example, in order to check whether an internet connection is available, the agent may want to observe that it can access a skype network even though it is really interested in opening a browser (as it needs a browser in order to perform a booking online).\nFormally, given a state S = \u3008KB0,F , C, \u03a3\u3009 and a time-point now, the set of all (timed) fluents selected by fES is the set of all (timed) fluents f [\u03c4 ] such that there is an action operator a with\n1. executed(a, t\u2032) \u2208 KB0, t\u2032 = \u03c4 \u2208 \u03a3 and now \u2212 \u00b2 < t\u2032 < now, where \u00b2 is a sufficiently small number (that is left as a parameter here), and\n2. S, a[\u03c4 ] |=eff `, where ` = f or ` = \u00acf ."}, {"heading": "8.3 Action Selection", "text": "Informally, the set of conditions for the action selection operator is as follows. Given a state S = \u3008KB0,F , C,\u03a3\u3009 and a time-point t, the set of all actions selected by fAS is defined as follows. Let X (S, t) be the set of all actions A in trees in F such that:\n1. A can be executed,\n2. no ancestor of A is timed out,\n3. no child of any ancestor of A is timed out,\n4. no ancestor of A is already satisfied,\n5. no precondition of A is known to be false,\n6. A has not already been executed.\nThen fAS(S, t) \u2286 X (S, t) such that all actions in fAS(S, t) are executable concurrently at t.\nIntuitively, conditions 2-4 impose that A belongs to a \u201cstill feasible\u201d plan for some toplevel goals in F . Note that condition 1 in the definition of X (S, t) is logically redundant, as it is also re-imposed by definition of fAS(S, t). However, this condition serves as a first filter and is thus useful in practice.\nFormally, given a state S = \u3008KB0,F , C,\u03a3\u3009, and a time-point t, the set of all actions selected by fAS is defined as follows. Let X (S, t) be the set of all actions a[\u03c4 ] occurring as leaves of some trees in F such that:\n1. there exists a total valuation \u03c3 such that S, \u03c3 |=cs \u03c4 = t, and\n2. \u00actimed out(S,G, t) for each G \u2208 ancestors(a[\u03c4 ],F), and\n3. \u00actimed out(S,X, t) for each X \u2208 children(G,F) and G \u2208 ancestors(a[\u03c4 ],F), and\n4. \u00acachieved(S, G, t) for each G \u2208 ancestors(a[\u03c4 ],F), and\n5. let S, a[\u03c4 ] |=pre Cs and Cs = `1[\u03c4 ] \u2227 . . . \u2227 `n[\u03c4 ]; if n > 0, then for no i = 1, . . . , n there exists a total valuation \u03c3 such that S, \u03c3 |=cs \u03c4 = t and S |=TR `i[\u03c4 ]\u03c3, and\n6. there exists no t\u2032 such that \u03c4 = t\u2032 \u2208 \u03a3 and executed(a, t\u2032) \u2208 KB0.\nThe formalisation of condition 6 allows for other instances of action A to have been executed. Then, fAS(S, t) = {a1[\u03c41], . . . , am[\u03c4m]} \u2286 X (S, t) (where m \u2265 0), such that there exists a total valuation \u03c3 for the variables in C such that S, \u03c3 |=cs \u03c41 = t \u2227 . . . \u2227 \u03c4m = t.\nNote that the definition of the action selection operator can be extended to take into account a notion of urgency with respect to the temporal constraints. However, such an extension is beyond the scope of this work."}, {"heading": "8.4 Precondition Selection", "text": "Informally, the set of conditions for the precondition selection operator is as follows. Given a state S = \u3008KB0,F , C,\u03a3\u3009 and a time-point t, the set of preconditions (of actions in F) selected by fPS is the set of all pairs \u3008C, A\u3009 of (timed) preconditions C and actions A \u2208 nodes(F) such that:\n1. C is a precondition of A and\n2. C is not known to be true in S at t, and\n3. A is one of the actions that could be selected for execution if fAS would be called at the current time.\nThe reason why this selection operator returns pairs, rather then simply preconditions, is that the transition SI, which makes use of the outputs of this selection operator, needs to know the actions associated with the preconditions. This is because SI introduces sensing actions for each precondition returned and has to place these sensing actions as siblings of the associated actions in F , as seen in Section 7.4.\nFormally, given a state S = \u3008KB0,F , C, \u03a3\u3009 and a time-point t, the set of all preconditions of actions selected by fPS is the set of all pairs \u3008C,A\u3009 of (timed) preconditions C and actions A \u2208 nodes(F) such that:\n1. A = a[\u03c4 ], and S, a[\u03c4 ] |=pre Cs and C is a conjunct in Cs, and\n2. there exists no total valuation \u03c3 for the variables in C such that S, \u03c3 |=cs \u03c4 = t and S |=TR C\u03c3, and\n3. A \u2208 X (S, t), where X (S, t) is as defined in Section 8.3."}, {"heading": "9. Cycle Theory", "text": "The behaviour of KGP agents results from the application of transitions in sequences, repeatedly changing the state of the agent. These sequences are not fixed a priori, as in conventional agent architectures, but are determined dynamically by reasoning with declarative cycle theories, giving a form of flexible control. Cycle theories are given in the framework of Logic Programming with Priorities (LPP) as discussed in Section 3."}, {"heading": "9.1 Formalisation of Cycle Theories.", "text": "Here we use the following new notations:\n\u2022 T (S,X, S\u2032, t) to represent the application of transition T at time t in state S given input X and resulting in state S\u2032, and\n\u2022 \u2217T (S,X) to represent that transition T can potentially be chosen as the next transition in state S, with input X.\nRecall that, for some of the transitions, X may be the empty set {}, as indicated in Section 7. Formally, a cycle theory Tcycle consists of the following parts. \u2022 An initial part Tinitial, that determines the possible transitions that the agent could\nperform when it starts to operate. Concretely, Tinitial consists of rules of the form \u2217T (S0, X) \u2190 C(S0, X) which we refer to via the name R0|T (S0, X). These rules sanction that, if conditions C hold in the initial state S0 then the initial transition could be T , applied to state S0 and input X. For example, the rule R0|GI(S0, {}) : \u2217GI(S0, {}) \u2190 empty forest(S0) sanctions that the initial transition should be GI, if the forest in the initial state S0 is empty.\nNote that C(S0, X) may be empty, and, if non-empty, C(S0, X) may refer to the current time via a condition time now(t). For example, the rule\nR0|PI(S0, G) : \u2217PI(S0, G) \u2190 Gs = fGS(S0, t), Gs 6= {}, G \u2208 Gs, time now(t) sanctions that the initial transition should be PI, if the forest in the initial state S0 contains some goal that can be planned for at the current time (in that the goal selection operator picks that goal).\n\u2022 A basic part Tbasic that determines the possible transitions following given transitions, and consists of rules of the form\n\u2217T \u2032(S\u2032, X \u2032) \u2190 T (S, X, S\u2032, t), EC(S\u2032, X \u2032)\nwhich we refer to via the name RT |T \u2032(S\u2032, X \u2032). These rules sanction that, after transition T has been executed, starting at time t in the state S and resulting in state S\u2032, and the conditions EC evaluated in S\u2032 are satisfied, then transition T \u2032 could be the next transition to be applied in S\u2032, with input X \u2032.16 EC are enabling conditions as they determine when T \u2032 can be applied after T . They also determine input X \u2032 for T \u2032, via calls to selection operators. As for the initial part of Tcycle, EC may be empty and, if not, may refer to the current time. For example, the rule\nRAE|PI(S\u2032, G) : \u2217PI(S\u2032, G) \u2190 AE(S, As, S\u2032, t), Gs = fGS(S\u2032, t\u2032), Gs 6= {}, G \u2208 Gs, time now(t\u2032)\nsanctions that PI should follow AE if at the current time there is some goal in the current state that is selected by the goal selection function.\n\u2022 A behaviour part Tbehaviour that contains rules describing dynamic priorities amongst rules in Tbasic and Tinitial. Rules in Tbehaviour are of the form\nRT |T \u2032(S,X \u2032) \u00c2 RT |T \u2032\u2032(S, X \u2032\u2032) \u2190 BC(S, X \u2032, X \u2032\u2032) with T \u2032 6= T \u2032\u2032, which we will refer to via the name PTT \u2032\u00c2T \u2032\u2032 . Recall that RT |T \u2032(\u00b7) and RT |T \u2032\u2032(\u00b7) are (names of) rules in Tbasic\u222aTinitial. Note that, with an abuse of notation, T could be 0 in the case that one such rule is used to specify a priority over the first transition to take place, in other words, when the priority is over rules in Tinitial. These rules in Tbehaviour sanction that, after transition T , if the conditions BC hold, then we prefer the next transition to be T \u2032 over T \u2032\u2032. The conditions BC are behaviour conditions as they give the behavioural profile of the agent. For example, the rule\nPTGI\u00c2T \u2032 : RT |GI(S, {}) \u00c2 RT |T \u2032(S, X) \u2190 empty forest(S) sanctions that GI should be preferred to any other transition after any transition that results into a state with an empty forest. As for the other components of Tcycle, the conditions BC may refer to the current time.\n\u2022 An auxiliary part including definitions for any predicates occurring in the enabling and behaviour conditions.\n\u2022 An incompatibility part, in effect expressing that only one (instance of a) transition can be chosen at any one time.\nHence, Tcycle is an LPP-theory where: (i) P = Tinitial \u222a Tbasic, and (ii) H = Tbehaviour."}, {"heading": "9.2 Operational Trace", "text": "The cycle theory Tcycle of an agent is responsible for its behaviour, in that it induces an operational trace of the agent, namely a (typically infinite) sequence of transitions\nT1(S0, X1, S1, t1), . . . , Ti(Si\u22121, Xi, Si, ti), Ti+1(Si, Xi+1, Si+1, ti+1), . . .\nsuch that 16. Note that in order to determine that T \u2032 is a possible transition after T , with a rule of the earlier form,\none only needs to know that T has been applied and resulted into the state S\u2032. This is conveyed by the choice of name: RT |T \u2032(S\u2032, X \u2032). In other words, by using a Prolog notation, we could have represented the rule as \u2217T \u2032(S\u2032, X \u2032) \u2190 T ( , , S\u2032, ), EC(S\u2032, X \u2032). Thus, the rule is \u201cMarkovian\u201d.\n\u2022 S0 is the given initial state;\n\u2022 for each i \u2265 1, ti is given by the clock of the system (ti < ti+i);\n\u2022 (Tcycle \u2212 Tbasic) \u222a {time now(t1)} |=pr \u2217T1(S0, X1);\n\u2022 for each i \u2265 1 (Tcycle \u2212 Tinitial) \u222a {Ti(Si\u22121, Xi, Si, ti), time now(ti+1)} |=pr \u2217Ti+1(Si, Xi+1)\nnamely each (non-final) transition in a sequence is followed by the most preferred transition, as specified by Tcycle. If, at some stage, the most preferred transition determined by |=pr is not unique, we choose one arbitrarily."}, {"heading": "9.3 Normal Cycle Theory", "text": "The normal cycle theory is a concrete example of cycle theory, specifying a pattern of operation where the agent prefers to follow a sequence of transitions that allows it to achieve its goals in a way that matches an expected \u201cnormal\u201d behaviour. Other examples of possible cycle theories can be found in the literature (Kakas, Mancarella, Sadri, Stathis, & Toni, 2005; Sadri & Toni, 2006).\nBasically, the \u201cnormal\u201d agent first introduces goals (if it has none to start with) via GI, then reacts to them, via RE, and then repeats the process of planning for them, via PI, executing (part of) the chosen plans, via AE, revising its state, via SR, until all goals are dealt with (successfully or revised away). At this point the agent returns to introducing new goals via GI and repeating the above process. Whenever in this process the agent is interrupted via a passive observation, via POI, it chooses to introduce new goals via GI, to take into account any changes in the environment. Whenever it has actions which are \u201cunreliable\u201d, in the sense that their preconditions definitely need to be checked, the agent senses them (via SI) before executing the action. Whenever it has actions which are \u201cunreliable\u201d, in the sense that their effects definitely need to be checked, the agent actively introduces actions that aim at sensing these effects, via AOI, after having executed the original actions. If initially the agent is equipped with some goals, then it would plan for them straightaway by PI.\nThe full definition of the normal cycle theory is given in the appendix. This is used to provide the control in the examples of the next section. Here, note that, although the normal cycle theory is based on the classic observe-plan-act cycle of agent control, it generalises this in several ways giving more flexibility on the agent behaviour to adapt to a changing environment. For example, the goals of the agent need not be fixed but can be dynamically changed depending on newly acquired information. Let us illustrates this feature with a brief example here. Suppose that the current state of our agent contains the top-level nonreactive goal \u3008return home(\u03c41), {\u03c41 < 7}\u3009 and that a POI occurs which adds an observation observed(low battery, 2) at time 2. A subsequent GI transition generated by the normal cycle theory introduces a new goal \u3008recharge battery(\u03c42), {\u03c42 < 3}\u3009 which, depending on the details of KBGD, either replaces the previous goal or adds this as an additional goal. The normal cycle theory will next choose to do a PI transition for the new and more urgent goal of recharging its battery."}, {"heading": "10. Examples", "text": "In this section we revisit the examples introduced in Section 2.6 and used throughout the paper to illustrate the various components of the KGP model. Overall, the aim here is to illustrate the interplay of the transitions, and how this interplay provides the variety of behaviours afforded by the KGP model, including reaction to observations, generation and execution of conditional plans, and dynamic adjustment of goals and plans.\nUnless specified differently, we will assume that Tcycle will be the normal cycle theory presented in Section 9.3. We will provide any domain-dependent definition in the auxiliary part of Tcycle explicitly, where required."}, {"heading": "10.1 Setting 1 Formalised", "text": "We formalise here the initial state, knowledge bases and behaviour of svs for Setting 1 described in Section 2.6.1."}, {"heading": "10.1.1 Initial State", "text": "For simplicity, the observations, goals and the plan of svs can be assumed to be empty initially. More concretely let the (initial) state of svs be\nKB0 = { } F = { } C = { } \u03a3 = { }"}, {"heading": "10.1.2 Knowledge Bases", "text": "Following Section 5.1.4, we formulate the reactivity knowledge base for agent svs in terms of the utterances query ref, refuse, inform inspired by the FIPA specifications for communicative acts (FIPA, 2001a, 2001b). However, although we use the same names of communicative acts as in the FIPA specification, we do not adopt here their \u201cmentalistic\u201d semantic interpretation in terms of pre- and post-conditions. Thus, KBsvsreact is formulated as:\nobserved(C, tell(C, svs, query ref(Q), D, T0), T ), holds at(have info(Q, I), T ) \u21d2 assume happens(tell(svs, C, inform(Q, I), D), T \u2032), T \u2032 > T\nobserved(C, tell(C, svs, query ref(Q), D, T0), T ), holds at(no info(Q), T ) \u21d2 assume happens(tell(svs, C, refuse(Q), D), T \u2032), T \u2032 > T\nassume happens(tell(svs, C, inform(Q, I), D), T ), assume happens(tell(svs, C, refuse(Q), D), T \u2032) \u21d2 false\nassume happens(A, T ), not executable(A) \u21d2 false executable(tell(svs, C, S, D)) \u2190 C 6= svs\ninitially(no info(arrival(tr01))\nprecondition(tell(svs, C, inform(Q, I), D), have info(Q, I))\ninitiates(tell(C, svs, inform(Q, I), D), T, have info(Q, I))\nterminates(tell(C, svs, inform(Q, I), D), T, no info(Q))"}, {"heading": "10.1.3 Behaviour", "text": "To illustrate the behaviour of the psa we will assume that this agent requests from svs, at time 3, say, the arrival time of tr01. svs receives a request from psa at time 5 for the arrival time of tr01. Via POI at time 5 svs records in its KB0:\nobserved(psa, tell(psa, svs, query ref(arrival(tr01)), d, 3), 5)\nwhere d is the dialogue identifier. Then, via RE, at time 7, say, svs modifies its state by adding to F a tree T rooted at an action a1 to answer to psa. This action a1 is a refusal represented as:\na1 = tell(svs, psa, refuse(arrival(tr01)), d, \u03c4),\nand the temporal constraint \u03c4 > 7 is added to C. The refusal action is generated via the Reactivity capability because svs does not have information about the requested arrival time. svs executes the planned action a1 at time 10, say, via the AE transition, instantiating its execution time, adding the following record to KB0:\nexecuted(tell(svs, psa, refuse(arrival(tr01)), d), 10),\nand updating \u03a3 by adding \u03c4 = 10 to it. Suppose then that svs makes two observations as follows. At time 17 svs receives information of the arrival time (18) of the tr01 train from co. Via POI, svs records in its KB0 17:\nobserved(co, tell(co, svs, inform(arrival(tr01), 18), d\u2032, 15), 17).\nAssume further that at time 25 svs receives another request from psa about the arrival time of tr01 and, via POI, svs records in its KB0:\nobserved(psa, tell(psa, svs, query ref(arrival(tr01)), d\u2032\u2032, 20), 25)\nwith a new dialogue identifier d\u2032\u2032. This leads to a different answer from svs to the query of psa. svs adds an action to its state to answer psa with the arrival time. This is done again via RE, say at time 28. A new tree is added in F rooted at the (reactive) action\ntell(svs, psa, inform(arrival(tr01), 18), d\u2032\u2032, \u03c4 \u2032),\nand the new temporal constraint \u03c4 \u2032 > 28 is added to C. Via AE, svs executes the action, instantiating its execution time to 30, say, and adding the following record\n17. d\u2032 is the identifier of the dialogue within which this utterance has been performed, and would typically be different from the earlier d.\nexecuted(tell(svs, psa, inform(arrival(tr01), 18), d\u2032\u2032), 30)\nto KB0, and adding \u03c4 \u2032 = 30 to \u03a3. Eventually, SR will clear the planned (and executed) actions from the F component of the state of svs."}, {"heading": "10.2 Setting 2 Formalised", "text": "We formalise here the initial state, knowledge bases and behaviour of psa for Setting 2 described in Section 2.6.2."}, {"heading": "10.2.1 Initial State", "text": "Let us assume that initially the state of psa is as follows:\nKB0 = { } F = {T1, T2} C = {\u03c41 < 15, \u03c42 < 15} \u03a3 = { }\nwhere T1 and T2 consist of a goals (respectively): g1 = have ticket(madrid, denver, \u03c41) and g2 = have visa(usa, \u03c42)."}, {"heading": "10.2.2 Knowledge Bases", "text": "To plan for goal g1, the KB psa plan contains:\ninitiates(buy ticket online(From, To), T, have ticket(From, To)) precondition(buy ticket online(From, To), available connection) precondition(buy ticket online(From, To), available destination(To)).\nTo plan for goal g2, the KB psa plan contains:\ninitiates(apply visa(usa), T, have visa(usa)) precondition(apply visa(usa), have address(usa)) initiates(book hotel(L), T, have address(usa)) \u2190 holds(in(L, usa), T )."}, {"heading": "10.2.3 Behaviour", "text": "When PI is called on the above state, at time 2, say, it generates a partial plan for the goal, changing the state as follows. The goal g1 acquires three children in T1. These are:\ng11 = available connection(\u03c411), g12 = available destination(denver, \u03c412), a13 = buy ticket online(madrid, denver, \u03c413).\nAlso, consequently, the set of temporal constraints is updated to:\nC = {\u03c41 < 15, \u03c42 < 15, \u03c411 = \u03c413, \u03c412 = \u03c413, \u03c413 < \u03c41, \u03c41 > 2}.\nThe action a13 is generated as an action that initiates goal g1. Moreover, every plan that is generated must satisfy the integrity constraints in KBplan. In particular, any precondition of actions in the tree that do not already hold must be generated as sub-goals in the tree. This is why g11 and g12 are generated in the tree as above.\nNow via the transition SI, the following sensing actions are added to T1 as siblings of action a13 18:\na14 = sense(available connection, \u03c414) a15 = sense(available destination(denver), \u03c415)\nand the constraints\n\u03c414 = \u03c415, \u03c414 < \u03c413\nare added to C. Then, via AE, these two sensing actions are executed (before the original action a1), and KB0 is updated with the result of the sensing as follows. Suppose these two actions are executed at time 5. Consider the first action that senses the fluent available connection. If this fluent is confirmed by the physical sensing capability, i.e. if available connection : true is in X such that\nsensing({available connection, available destination}, 5) = X, then observed(available connection, 5) is added to KB0. On the other hand, if\navailable connection : false\nis in X as above, then observed(\u00acavailable connection, 5) is added to KB0. In both cases \u03c414 = 5 is added to \u03a3.\nIf neither of these cases occurs, i.e. if the sensing capability cannot confirm either of available connection or \u00acavailable connection, then no fact is added to KB0. Similarly for the other precondition, available destination. Let us assume that after this step of AE, KB0 becomes\nobserved(available connection, 5) observed(available destination(denver), 5)\nAE can then execute the original action a13. Note that the agent might decide to execute the action even if one or both preconditions are not known to be satisfied after the sensing. If g1 is achieved, SR will eliminate it and a13, a14, a15, g11, g12 from the state. In the resulting state, F = {T2}, and PI is called, say at time 6. This results in generating a partial plan for g2, and changing the state so that in T2 the root g2 has children\na21 = apply visa(usa, \u03c421) g22 = have address(usa, \u03c422)\nand \u03c421 < \u03c42, \u03c422 = \u03c421 are added to C. Then, further PI, say at time 7, introduces a23 = book hotel(denver, \u03c423)\n18. For this we assume that the auxiliary part of Tcycle contains the rule unreliable pre(As) \u2190 buy ticket online( , , ) \u2208 As\nas a child of g22 in T2, and adding \u03c423 < \u03c422 to C. Then, AE at time 8 executes a23, adding it to KB0, and further AE at time 9 executes a22, also updating KB0. Finally, SR eliminates all actions and goals in T2 and returns an empty F in the state."}, {"heading": "11. Related Work", "text": "Many proposals exist for models and architectures of individual agents based on computational logic foundations (see e.g. the survey by Fisher, Bordini, Hirsch, & Torroni, 2007). Some of these proposals are based on logic programming, for example IMPACT (Arisha, Ozcan, Ross, Subrahmanian, Eiter, & Kraus, 1999; Subrahmanian, Bonatti, Dix, Eiter, Kraus, Ozcan, & Ross, 2000), AAA (Balduccini & Gelfond, 2008; Baral & Gelfond, 2001), DALI (Costantini & Tocchio, 2004), MINERVA (Leite, Alferes, & Pereira, 2002), GOLOG (Levesque, Reiter, Lesperance, Lin, & Scherl, 1997), and IndiGolog (De Giacomo, Levesque, & Sardin\u0303a, 2001). Other proposals are based on modal logic or first-order logic approaches, for example the BDI model (Bratman et al., 1988; Rao & Georgeff, 1997) and its extensions to deal with normative reasoning (Broersen, Dastani, Hulstijn, Huang, & van der Torre, 2001), Agent0 (Shoham, 1993), AgentSpeak (Rao, 1996) and its variants, 3APL (Hindriks, de Boer, van der Hoek, & Meyer, 1999) and its variants (Dastani, Hobo, & Meyer, 2007).\nAt a high level of comparison there are similarities in the objectives of most existing computational logic models of agency and KGP, in that they all aim at specifying knowledgerich agents with certain desirable behaviours. There are also some similarities in the finer details of the KGP model and some of the above related work, as well as differences.\nA feature of the KGP which, to the best of our knowledge, is novel is the declarative and context-sensitive specification of an agent\u2019s cycle. To avoid a static cycle of control (Rao & Georgeff, 1991; Rao, 1996), KGP relies upon a cycle theory which determines, at run time, given the circumstances and the individual profile of the agent, what the next step should be. The cycle theory is sensitive to both solicited and unsolicited information that the agent receives from its environment, and helps the agent to adapt its behaviour to the changes it experiences. The approach closest to our work is that of 3APL (Hindriks et al., 1999) as extended by Dastani, de Boer, Dignum, and Meyer (2003), which provides meta-programming constructs for specifying the cycle of an agent such as goal selection, plan expansion, execution, as well as if-then-else and while-loop statements. Unlike the imperative constructs of 3APL, KGP uses a set of selection operators that can be extended to model different behaviours and types of agents. A flexible ordering of transitions is then obtained using preference reasoning about which transitions can be applied at a specific point in time. These preferences may change according to external events or changes in the knowledge of the agent.\nAnother central distinguishing feature of the KGP model, in comparison with existing models, including those based on logic programming, is its modular integration within a single framework of abductive logic programming, temporal reasoning, constraint logic programming, and preference reasoning based on logic programming with priorities, in order to support a diverse collection of capabilities. Each one of these is specified declaratively and equipped with its own provably correct computational counterpart (see Bracciali,\nDemetriou, Endriss, Kakas, Lu, Mancarella, Sadri, Stathis, Terreni, & Toni, 2004, for a detailed discussion).\nCompared with existing logic programming approaches KGP has two main similarities with MINERVA (Leite et al., 2002), an architecture that exploits computational logic and gives both declarative and operational semantics to its agents. Unlike KGP, a MINERVA agent consists of several specialised, possibly concurrent, sub-agents performing various tasks, and relies upon MDLP (Multidimensional Dynamic Logic Programming) (Leite et al., 2002). MDLP is the basic knowledge representation mechanism of an agent in MINERVA, which is based on an extension of answer-set programming and explicit rules for updating the agent\u2019s knowledge base. In KGP instead we integrate abductive logic programming and logic programming with priorities combined with temporal reasoning.\nClosely related to our work in KGP is the logic-based agent architecture for reasoning agents of Baral and Gelfond (2001). This architecture assumes that the state of an agent\u2019s environment is described by a set of fluents that evolve over time in terms of transitions labelled by actions. An agent is also assumed to be capable of correctly observing the state of the environment, performing actions, and remembering the history of what happened in it. The agent\u2019s knowledge base consists of an action description part specifying the internal agent transitions, which are domain specific and not generic as in KGP. The knowledge base also contains what the agent observes in the environment including its own actions, as in KGP\u2019s KB0. The temporal aspects of agent transitions are specified in the action language AL implemented in A-Prolog, a language of logic programs under the answerset programming semantics. The answer sets of domain specific programs specified in AL correspond to plans that in KGP are hypothetical narratives of the abductive event calculus. The control of the agent is based on a static observe-think-act cycle, an instance of the KGP cycle theories. A more recent and refined account of the overall approach has given rise to the AAA Architecture, see (Balduccini & Gelfond, 2008) for an overview.\nDALI (Costantini & Tocchio, 2004) is a logic programming language designed for executable specification of logical agents. Like KGP, DALI attempts to provide constructs to represent reactivity and proactivity in an agent using extended logic programs. A DALI agent contains reactive rules, events, and actions aimed at interacting with an external environment. Behaviour (in terms of reactivity or proactivity) of a DALI agent is triggered by different event types: external, internal, present, and past events. All the events and actions are time stamped so as to record when they occur. External events are like the observations in KGP, while past events are like past observations. However, KGP does not support internal events but has instead the idea of transitions that are called by the cycle theory to trigger reactive or proactive behaviour.\nIndiGolog (De Giacomo et al., 2001) is a high-level programming language for robots and intelligent agents that supports, like KGP, on-line planning, sensing and plan execution in dynamic and incompletely known environments. It is a member of the Golog family of languages (Levesque et al., 1997) that use a Situation Calculus theory of action to perform the reasoning required in executing the program. Instead in the KGP model we rely on abductive logic programming and logic programming with priorities combined with temporal reasoning. Instead of the Situation Calculus in KGP we use the Event Calculus for temporal reasoning, but our use of the Event Calculus is not a prerequisite of the model as in InterRaP (Mu\u0308ller, Fischer, & Pischel, 1998), but can be replaced with another temporal\nreasoning framework, if needed. Apart from the difference between the use of the Situation and Event Calculi, in IndiGolog goals cannot be decided dynamically, whereas in the KGP model they change dynamically according to the specifications in the Goal Decision capability.\nThere is an obvious similarity of the KGP model with the BDI model (Bratman et al., 1988) given by the correspondence between KGP\u2019s knowledge, goals and plan and BDI\u2019s beliefs, desires and intentions, respectively. Apart from the fact that the BDI model is based on modal logic, in KGP the knowledge (beliefs in BDI) is partitioned in modules, to support the various reasoning capabilities. KGP also tries to bridge the gap between specification and the practical implementation of an agent. This gap has been criticized in BDI by Rao (1996), when he developed the AgentSpeak(L) language. The computational model of AgentSpeak(L) has been formally studied by d\u2019Inverno and Luck (1998), while recent implementations of the AgentSpeak interpreter have been incorporated in the Jason platform (Bordini & Hu\u0308bner, 2005). Like the KGP implementation in PROSOCS (Bracciali et al., 2006), the Jason implementation too seeks to narrow the gap between specification and executable BDI agent programs. Jason also extends BDI with new features like belief revision (Alechina, Bordini, Hu\u0308bner, Jago, & Logan, 2006).\nA particular line of work in BDI is that of Padgham and Lambrix (2005), who investigate how the notion of capability can be integrated in the BDI Logic of Rao and Georgeff (1991), so that a BDI agent can reason about its own capabilities. A capability in this work is informally understood as the ability to act rationally towards achieving a particular goal, in the sense of having an abstract plan type that is believed to achieve the goal. Formally, the BDI logic of Rao and Georgeff is extended to incorporate a modality for capabilities that constrains agent goals and intentions to be compatible with what the agent believes are its capabilities. A set of compatibility axioms are then presented detailing the semantic conditions to capture the desired inter-relationships among an agent\u2019s beliefs, capabilities, goals, and intentions. The work also summarises how the extensions of the BDI model can be implemented by adapting the BDI interpreter to include capabilities, further arguing the benefits of the extension over the original BDI Interpreter of Rao and Georgeff (1992).\nIn KGP capabilities equate to the reasoning capabilities of an agent that allow the agent to plan actions from a given state, react to incoming observations, or decide upon which goals to adopt. However, in KGP, we do not use capabilities at the level of an agent\u2019s domain specific knowledge to guide the agent in determining whether or not it is rational to adopt a particular goal.\nThe issue of the separation between specification and implementation exists between the KGP model and Agent0 (Shoham, 1993), and its later refinement PLACA (Thomas, 1995). Two other differences between the KGP and Agent0 and PLACA are the explicit links that exist in the KGP model amongst the goals (in the structuring of the forest in the agent state) and the richer theories in the KGP that specify priorities amongst potential goals which are not restricted to temporal orderings. These explicit links are exploited when revising goals and state, via the Revision transition, in the light of new information or because of the passage of time.\nThe BOID architecture (Broersen et al., 2001) extends the well known BDI model (Rao & Georgeff, 1992) with obligations, thus giving rise to four main components in representing an agent: beliefs, obligations, intentions and desires. The focus of BOID is to find ways of\nresolving conflicts amongst these components. In order to do so they define agent types, including some well known types in agent theories such as realistic, selfish, social and simple minded agents. The agent types differ in that they give different priorities to the rules for each of the four components. For instance, the simple minded agent gives higher priority to intentions, compared to desires and obligations, whereas a social agent gives higher priority to obligations than desires. They use priorities with propositional logic formulae to specify the four components and the agent types.\nThe existing KGP model already resolves some of the conflicts that BOID tries to address. For example, if there is a conflict between a belief and a prior intention, which means that an intended action can no longer be executed due to the changes in the environment, the KGP agent will notice this and will give higher priority to the belief than the prior intention, allowing the agent in effect to retract the intended action and, time permitting, to replan for its goals. The KGP model also includes a notion of priority used in the Goal Decision capability and the cycle theory that controls the behaviour of the agent. The KGP model has also been extended to deal with normative concepts, the extended model is known as N-KGP (Sadri, Stathis, & Toni, 2006). What N-KGP has in common with BOID is that it seeks to extend KGP with the addition of obligations. The N-KGP model also extends the notion of priorities by incorporating them amongst different types of goals and actions. A detailed comparison of N-KGP with related work is presented by Sadri, Stathis, and Toni (2006).\nThere are features that are included in some other approaches that are absent in the KGP model. BDI and, more so, the IMPACT system (Arisha et al., 1999; Subrahmanian et al., 2000) allow agents to have in their knowledge bases representations of the knowledge of other agents. These systems allow the agents both some degree of introspection and the ability to reason about other agents\u2019 beliefs and reasoning. The KGP model to this date does not include any such features. IMPACT also allows the incorporation of legacy systems, possibly using diverse languages, and has a richer knowledge base language including deontic concepts and probabilities. Similarly, the 3APL, system is based on a combination of imperative and logic programming languages, and includes an optimisation component absent from the KGP. This component in 3APL includes rules that identify if in a given situation the agent is pursuing a suboptimal plan, and help the agent find a better way of achieving its goals. 3APL also includes additional functionalities such as learning (van Otterlo, Wiering, Dastani, & Meyer, 2003), which our model does not currently support. 2APL (Dastani et al., 2007) is an extension of 3APL with goals and goal-plan rules as well as external and internal events. 2APL has a customisable (via graphical interface) cycle which is fixed once customised."}, {"heading": "12. Conclusions", "text": "We have presented the computational logic foundations of the KGP model of agency. The model allows the specification of heterogeneous agents that can interact with each other, and can exhibit both proactive and reactive behaviour allowing them to function in dynamic environments by adjusting their goals and plans when changes happen in such environments. KGP incorporates a highly modular agent architecture that integrates a collection\nof reasoning and sensing capabilities, synthesised within transitions, orchestrated by cycle theories that take into account the dynamic context and agent preferences.\nThe formal specification of the KGP components within computational logic has the major advantage of facilitating both a formal analysis of the model and a direct verifiable implementation. This formal analysis has been started by Sadri and Toni (2006), where we give a formal analysis of KGP agents by exploring their effectiveness in terms of goal achievement, and reactive awareness, and the impact of their reasoning capabilities towards progress in goal achievement. An implementation of a precursor of this model, described by Kakas et al. (2004b), has already been developed within the PROSOCS platform of Stathis et al. (2004) upon provably correct computational counterparts defined for each component of the model as given by Kakas et al. (2004b). Concrete choices for these computational counterparts have been described by Bracciali et al. (2004). The resulting development framework allows the deployment and testing of the functionality of the earlier variant of KGP agents. Deployment of these agents relies upon the agent template designed by Stathis et al. (2002), which builds upon previous work with the head/body metaphor described by Steiner et al. (1991) and Haugeneder et al. (1994), and the mind/body architecture introduced by Bell (1995) and recently used by Huang, Eliens, and de Bra (2001). This development platform has been applied to a number of practical applications, and, in particular, to ambient intelligence by Stathis and Toni (2004). Also, Sadri (2005) has provided guidelines for specifying applications using KGP agents. Future work includes implementing and deploying the revised KGP model given in this paper: we envisage that this will pose limited conceptual challenges, as we will be able to capitalise on our experience in implementing and deploying the precursor of this model.\nSadri, Stathis, and Toni (2006) have explored how the precursor of the KGP agent model can be augmented with normative features allowing agents to reason about and choose between their social and personal goals, prohibitions and obligations. It would be interesting to continue this work for the finalised KGP model given in this paper.\nSadri and Toni (2005) have developed a number of different profiles of behaviour, defined in terms of specific cycle theories, and formally proved their advantages in given circumstances. It would be interesting to explore this dimension further, to characterise different agent personalities and provide guidance, through formal properties, as to the type of personality needed for applications.\nFuture work also includes extending the model to incorporate (i) other reasoning capabilities, including knowledge revision (e.g. by Inductive Logic Programming), and more sophisticated forms of temporal reasoning, including identifying explanations for unexpected observations, (ii) introspective reasoning and reasoning about the beliefs of other agents, (iii) further experimentation with the model via its implementation, and (iv) development of a concurrent implementation."}, {"heading": "Acknowledgments", "text": "This work was supported by the EU FET Global Computing Initiative, within the SOCS project (IST-2001-32530). We wish to thank all our colleagues in SOCS for useful discussions during the development of KGP. We are also grateful to Chitta Baral and the anonymous referees for helpful comments on an earlier version of this paper."}, {"heading": "Appendix A. Normal Cycle Theory", "text": "We give here the main parts of the normal Tcycle, but exclude others, for example the definitions for incompatible and the auxiliary part, including definitions for predicates such as empty forest, unreliable pre etc. For more details see (Kakas et al., 2005).\nTinitial: This consists of the following rules: R0|GI(S0, {}) : \u2217GI(S0, {}) \u2190 empty forest(S0) R0|AE(S0, As) : \u2217AE(S0, As) \u2190 empty non executable goals(S0), As = fAS(S0, t),\nAs 6= {}, time now(t) R0|PI(S0, G) : \u2217PI(S0, G) \u2190 Gs = fGS(S0, t), Gs 6= {}, G \u2208 Gs, time now(t)\nTbasic: This consists of the following rules: \u2022 The rules for deciding what might follow an AE transition are as follows:\nRAE|PI(S\u2032, G) : \u2217PI(S\u2032, G) \u2190 AE(S,As, S\u2032, t), Gs = fGS(S\u2032, t\u2032), Gs 6= {}, G \u2208 Gs, time now(t\u2032) RAE|AE(S\u2032, As\u2032) : \u2217AE(S\u2032, As\u2032) \u2190 AE(S,As, S\u2032, t), As\u2032 = fAS(S\u2032, t\u2032), As\u2032 6= {}, time now(t\u2032) RAE|AOI(S\u2032, Fs) : \u2217AOI(S\u2032, Fs) \u2190 AE(S,As, S\u2032, t), Fs = fES(S\u2032, t\u2032), Fs 6= {}, time now(t\u2032) RAE|SR(S\u2032) : \u2217SR(S\u2032, {}) \u2190 AE(S, As, S\u2032, t) RAE|GI(S\u2032, {}) : \u2217GI(S\u2032, {}) \u2190 AE(S, As, S\u2032, t)\nNamely, AE could be followed by another AE, or by a PI, or by an AOI, or by a SR, or by a GI, or by a POI.\n\u2022 The rules for deciding what might follow SR are as follows RSR|PI(S\u2032, G) : \u2217PI(S\u2032, G) \u2190 SR(S, {}, S\u2032, t), Gs = fGS(S\u2032, t\u2032), Gs 6= {}, G \u2208 Gs,\ntime now(t\u2032) RSR|GI(S\u2032, {}) : \u2217GI(S\u2032, {}) \u2190 SR(S, {}, S\u2032, t), Gs = fGS(S\u2032, t\u2032), Gs = {}, time now(t\u2032) RSR|AE(S\u2032, As) : \u2217AE(S\u2032, As) \u2190 SR(S, {}, S\u2032, t), As = fGS(S\u2032, t\u2032), As 6= {},\ntime now(t\u2032) Namely, SR can only be followed by PI or GI or AE, depending on whether or not there are goals to plan for in the state.\n\u2022 The rules for deciding what might follow PI are as follows RPI|AE(S\u2032, As) : \u2217AE(S\u2032, As) \u2190 PI(S, G, S\u2032, t), As = fAS(S\u2032, t\u2032), As 6= {},\ntime now(t\u2032) RPI|SI(S\u2032, Ps) : \u2217SI(S\u2032, Ps) \u2190 PI(S,G, S\u2032, t), Ps = fPS(S\u2032, t\u2032), Ps 6= {}, time now(t\u2032)\nThe second rule is here to allow the possibility of sensing the preconditions of an action before its execution.\n\u2022 The rules for deciding what might follow GI are as follows RGI|RE(S\u2032, {}) : \u2217RE(S\u2032, {}) \u2190 GI(S, {}, S\u2032, t) RGI|PI(S\u2032, G) : \u2217PI(S\u2032, G) \u2190 GI(S, {}, S\u2032, t), Gs = fGS(S\u2032, t\u2032), Gs 6= {}, G \u2208 Gs, time now(t\u2032) Namely, GI can only be followed by RE or PI, if there are goals to plan for.\n\u2022 The rules for deciding what might follow RE are as follows\nRRE|PI(S\u2032, G) : \u2217PI(S\u2032, G) \u2190 RE(S, {}, S\u2032, t), Gs = fGS(S\u2032, t\u2032), Gs 6= {}, G \u2208 Gs, time now(t\u2032) RRE|SI(S\u2032, Ps) : \u2217SI(S\u2032, Ps) \u2190 RE(S, {}, S\u2032, t), Ps = fPS(S\u2032, t\u2032), Ps 6= {}, time now(t\u2032)\n\u2022 The rules for deciding what might follow SI are as follows RSI|AE(S\u2032, As) : \u2217AE(S\u2032, As) \u2190 SI(S, Ps, S\u2032, t), As = fAS(S\u2032, t\u2032), As 6= {},\ntime now(t\u2032) RSI|SR(S\u2032, {}) : \u2217SR(S\u2032, {}) \u2190 SI(S, Ps, S\u2032, t)\n\u2022 The rules for deciding what might follow AOI are as follows RAOI|AE(S\u2032, As) : \u2217AE(S\u2032, As) \u2190 AOI(S, Fs, S\u2032, t), As = fAS(S\u2032, t\u2032), As 6= {},\ntime now(t\u2032) RAOI|SR(S\u2032, {}) : \u2217SR(S\u2032, {}) \u2190 AOI(S, Fs, S\u2032, t) RAOI|SI(S\u2032, Ps) : \u2217SI(S\u2032, Ps) \u2190 AOI(S, Fs, S\u2032, t), Ps = fPS(S\u2032, t\u2032), Ps 6= {},\ntime now(t\u2032) \u2022 The rules for deciding what might follow POI are as follows\nRPOI|GI(S\u2032, {}) : \u2217GI(S\u2032, {}) \u2190 POI(S, {}, S\u2032, t) Tbehaviour: This consists of the following rules: \u2022 GI should be given higher priority if there are no trees in the state:\nPTGI\u00c2T \u2032 : RT |GI(S, {}) \u00c2 RT |T \u2032(S,X) \u2190 empty forest(S) for all transitions T, T \u2032, T \u2032 6= GI, and with T possibly 0 (indicating that if there are no trees in the initial state of an agent, then GI should be its first transition). \u2022 GI is also given higher priority after a POI: PPOIGI\u00c2T : RPOI|GI(S\u2032, {}) \u00c2 RPOI|T (S, {}, S\u2032) for all transitions T 6= GI. \u2022 After GI, the transition RE should be given higher priority: PGIRE\u00c2T : RGI|RE(S, {}) \u00c2 RGI|T (S,X) for all transitions T 6= RE. \u2022 After RE, the transition PI should be given higher priority: PREPI\u00c2T : RRE|PI(S, G) \u00c2 RRE|T (S, X) for all transitions T 6= PI. \u2022 After PI, the transition AE should be given higher priority, unless there are actions in the actions selected for execution whose preconditions are \u201cunreliable\u201d and need checking, in which case SI will be given higher priority: PPIAE\u00c2T : RPI|AE(S,As) \u00c2 RPI|T (S, X) \u2190 not unreliable pre(As) for all transitions T 6= AE. PPISI\u00c2AE : RPI|SI(S, Ps) \u00c2 RPI|AE(S, As) \u2190 unreliable pre(As) \u2022 After SI, the transition AE should be given higher priority PSIAE\u00c2T : RSI|AE(S,As) \u00c2 RSI|T (S, X) for all transitions T 6= AE. \u2022 After AE, the transition AE should be given higher priority until there are no more actions to execute in the state, in which case either AOI or SR should be given higher priority, depending on whether there are actions which are \u201cunreliable\u201d, in the sense that their effects need checking, or not:\nPAEAE\u00c2T : RAE|AE(S, As) \u00c2 RAE|T (S,X) for all transitions T 6= AE. Note that, by definition of Tbasic, the transition AE is applicable only if there are still actions to be executed in the state. PAEAOI\u00c2T : RAE|AOI(S, Fs) \u00c2 RAE|T (S, X)) \u2190 BCAEAOI|T (S, Fs, t), time now(t) for all transitions T 6= AOI, where the behaviour condition BCAEAOI|T (S, Fs, t) is defined (in the auxiliary part) by: BCAEAOI|T (S, FS, t) \u2190 empty executable goals(S, t), unreliable effect(S, t) Similarly, we have: PAESR\u00c2T : RAE|SR(S, {}) \u00c2 RAE|T (S, X)) \u2190 BCAESR|T (S, t), time now(t) for all transitions T 6= SR where: BCAESR|T (S, t) \u2190 empty executable goals(S, t), not unreliable effect(S, t) Here, we assume that the auxiliary part of Tcycle specifies whether a given set of actions contains any \u201cunreliable\u201d action, in the sense expressed by unreliable effect, and defines the predicate empty executable goals. \u2022 After SR, the transition PI should have higher priority: PSRPI\u00c2T : RSR|PI(S,G) \u00c2 RSR|T (S, X)) for all transitions T 6= PI. Note that, by definition of Tbasic, the transition PI is applicable only if there are still goals to plan for in the state. If there are no actions and goals left in the state, then rule RGI|T would apply. \u2022 In the initial state PI should be given higher priority:\nP0PI\u00c2T : R0|PI(S, G) \u00c2 R0|T (S,X) for all transitions T 6= PI. Note that, by definition of Tinitial below, the transition PI is applicable initially only if there are goals to plan for in the initial state."}], "references": [], "referenceMentions": [], "year": 2008, "abstractText": "This paper presents the computational logic foundations of a model of agency called the KGP (Knowledge, Goals and Plan) model. This model allows the specification of heterogeneous agents that can interact with each other, and can exhibit both proactive and reactive behaviour allowing them to function in dynamic environments by adjusting their goals and plans when changes happen in such environments. KGP provides a highly modular agent architecture that integrates a collection of reasoning and physical capabilities, synthesised within transitions that update the agent\u2019s state in response to reasoning, sensing and acting. Transitions are orchestrated by cycle theories that specify the order in which transitions are executed while taking into account the dynamic context and agent preferences, as well as selection operators for providing inputs to transitions.", "creator": " TeX output 2008.11.05:0922"}}}