{"id": "1401.3461", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "A Bilinear Programming Approach for Multiagent Planning", "abstract": "multiagent planning and coordination problems encountered are common and known to be computationally hard. we show that together a wide finite range of two - agent problems can be formulated as bilinear programs. next we present a successive successive approximation algorithm that significantly outperforms the coverage set algorithm, which is the state - of - the - art method for this class of multiagent problems. again because the algorithm now is formulated for bilinear programs, it is more general and simpler to implement. the new algorithm can hopefully be terminated at any time and - so unlike the coverage set algorithm - it facilitates the derivation required of a useful online performance bound. it is also however much more efficient, on average reducing the computational computation time of sorting the optimal generic solution by about four orders of magnitude. finally, we introduce an automatic dimensionality reduction method that improves the effectiveness of the modified algorithm, sufficiently extending its applicability to new domains and so providing a new way to analyze a subclass table of bilinear programs.", "histories": [["v1", "Wed, 15 Jan 2014 05:21:26 GMT  (393kb)", "http://arxiv.org/abs/1401.3461v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["marek petrik", "shlomo zilberstein"], "accepted": false, "id": "1401.3461"}, "pdf": {"name": "1401.3461.pdf", "metadata": {"source": "CRF", "title": "A Bilinear Programming Approach for Multiagent Planning", "authors": ["Marek Petrik", "Shlomo Zilberstein"], "emails": ["petrik@cs.umass.edu", "shlomo@cs.umass.edu"], "sections": [{"heading": "1. Introduction", "text": "We present a new approach for solving a range of multiagent planning and coordination problems using bilinear programming. The problems we focus on represent various extensions of the Markov decision process (MDP) to multiagent settings. The success of MDP algorithms for planning and learning under uncertainty has motivated researchers to extend the model to cooperative multiagent problems. One possibility is to assume that all the agents share all the information about the underlying state. This results in a multiagent Markov decision process (Boutilier, 1999), which is essentially an MDP with a factored action set. A more complex alternative is to allow only partial sharing of information among agents. In these settings, several agents\u2013each having different partial information about the world\u2013must cooperate with each other in order to achieve some joint objective. Such problems are common in practice and can be modeled as decentralized partially observable MDPs (DEC-POMDPs) (Bernstein, Zilberstein, & Immerman, 2000). Some refinements of this model have been studied, for example by making certain independence assumptions (Becker, Zilberstein, & Lesser, 2003) or by adding explicit communication actions (Goldman & Zilberstein, 2008). DEC-POMDPs are closely related to extensive games (Rubinstein, 1997). In fact, any DEC-POMDP represents an exponentially larger extensive game with a common objective. Unfortunately, DEC-POMDPs with just two agents are intractable in general, unlike MDPs that can be solved in polynomial time.\nDespite recent progress in solving DEC-POMDPs, even state-of-the-art algorithms are generally limited to very small problems (Seuken & Zilberstein, 2008). This has motivated the development of algorithms that either solve a restricted class of problems (Becker,\nc\u00a92009 AI Access Foundation. All rights reserved.\nLesser, & Zilberstein, 2004; Kim, Nair, Varakantham, Tambe, & Yokoo, 2006) or provide only approximate solutions (Emery-Montemerlo, Gordon, Schneider, & Thrun, 2004; Nair, Roth, Yokoo, & Tambe, 2004; Seuken & Zilberstein, 2007). In this paper, we introduce an efficient algorithm for several restricted classes, most notably decentralized MDPs with transition and observation independence (Becker et al., 2003). For the sake of simplicity, we denote this model as DEC-MDP, although this is usually used to denote the model without the independence assumptions. The objective in these problems is to maximize the cumulative reward of a set of cooperative agents over some finite horizon. Each agent can be viewed as a single decision-maker operating on its own \u201clocal\u201d MDP. What complicates the problem is the fact that all these MDPs are linked through a common reward function that depends on their states.\nThe coverage set algorithm (CSA) was the first optimal algorithm to solve efficiently transition and observation independent DEC-MDPs (Becker, Zilberstein, Lesser, & Goldman, 2004). By exploiting the fact that the interaction between the agents is limited compared to their individual local problems, CSA can solve problems that cannot be solved by the more general exact DEC-POMDP algorithms. It also exhibits good anytime behavior. However, the anytime behavior is of limited applicability because solution quality is only known in hindsight, after the algorithm terminates.\nWe develop a new approach to solve DEC-MDPs\u2013as well as a range of other multiagent planning problems\u2013by representing them as bilinear programs. We also present an efficient new algorithm for solving these kinds of separable bilinear problems. When the algorithm is applied to DEC-MDPs, it improves efficiency by several orders of magnitude compared with previous state-of the art algorithms (Becker, 2006; Petrik & Zilberstein, 2007a). In addition, the algorithm provides useful runtime bounds on the approximation error, which makes it more useful as an anytime algorithm. Finally, the algorithm is formulated for general separable bilinear programs and therefore it can be easily applied to a range of other problems.\nThe rest of the paper is organized as follows. First, in Section 2, we describe the basic bilinear program formulation and how a range of multiagent planning problems can be expressed within this framework. In Section 3, we describe a new successive approximation algorithm for bilinear programs. The performance of the algorithm depends heavily on the number of interactions between the agents. To address that, we propose in Section 4 a method that automatically reduces the number of interactions and provides a bound on the degradation in solution quality. Furthermore, to be able to project the computational effort required to solve a given problem instance, we develop offline approximation bounds in Section 5. In Section 6, we examine the performance of the approach on a standard benchmark problem. We conclude with a summary of the results and a discussion of future work that could further improve the performance of this approach."}, {"heading": "2. Formulating Multiagent Planning Problems as Bilinear Programs", "text": "We begin with a formal description of bilinear programs and the different types of multiagent planning problems that can be formulated as such. In addition to multiagent planning problems, bilinear programs can be used to solve a variety of other problems such as robotic manipulation (Pang, Trinkle, & Lo, 1996), bilinear separation (Bennett & Mangasarian,\n1992), and even general linear complementarity problems (Mangasarian, 1995). We focus on multiagent planning problems where this formulation turns out to be particularly effective.\nDefinition 1. A separable bilinear program in the normal form is defined as follows:\nmaximize w,x,y,z f(w, x, y, z) = sT1w + r T 1 x+ x TCy + rT2 y + s T 2 z subject to A1x+B1w = b1 A2y +B2z = b2 w, x, y, z \u2265 0\n(1)\nThe size of the program is the total number of variables in w, x, y and z. The number of variables in y determines the dimensionality of the program1.\nUnless otherwise specified, all vectors are column vectors. We use boldface 0 and 1 to denote vectors of zeros and ones respectively of the appropriate dimensions. This program specifies two linear programs that are connected only through the nonlinear objective function term xTCy. The program contains two types of variables. The first type includes the variables x, y that appear in the bilinear term of the objective function. The second type includes the additional variables w, z that do not appear in the bilinear term. As we show later, this distinction is important because the complexity of the algorithm we propose depends mostly on the dimensionality of the problem, which is the number of variables y involved in the bilinear term.\nThe bilinear program in Eq. (1) is separable because the constraints on x and w are independent of the constraints on y and z. That is, the variables that participate in the bilinear term of the objective function are independently constrained. The theory of nonseparable bilinear programs is much more complicated and the corresponding algorithms are not as efficient (Horst & Tuy, 1996). Thus, we limit the discussion in this paper to separable bilinear programs and often omit the term \u201cseparable\u201d. As discussed later in more detail, a separable bilinear program may be seen as a concave minimization problem with multiple local minima. It can be shown that solving this problem is NP-complete, compared to polynomial time complexity of linear programs.\nIn addition to the formulation of the bilinear program shown in Eq. (1), we also use the following formulation, stated in terms of inequalities:\nmaximize x,y\nxTCy\nsubject to A1x \u2264 b1 x \u2265 0 A2y \u2264 b2 y \u2265 0\n(2)\nThe latter formulation can be easily transformed into the normal form using standard transformations of linear programs (Vanderbei, 2001). In particular, we can introduce slack\n1. It is possible to define the dimensionality in terms of x, or the minimum of dimensions of x and y. The issue is discussed in Appendix B.\nvariables w, z to obtain the following identical bilinear program in the normal form:\nmaximize w,x,y,z\nxTCy\nsubject to A1x\u2212 w = b1 A2y \u2212 z = b2 w, x, y, z \u2265 0\n(3)\nWe use the following matrix and block matrix notation in the paper. Matrices are denoted by square brackets, with columns separated by commas and rows separated by semicolons. Columns have precedence over rows. For example, the notation [A,B;C,D]\ncorresponds to the matrix ( A B C D ) .\nAs we show later, the presence of the variables w, z in the objective function may prevent a crucial function from being convex. Since this has an unfavorable impact on the properties of the bilinear program, we introduce a compact form of the problem.\nDefinition 2. We say that the bilinear program in Eq. (1) is in a compact form when s1 and s2 are zero vectors. It is in a semi-compact form if s2 is a zero vector.\nThe compactness requirement is not limiting because any bilinear program in the form shown in Eq. (1) can be expressed in a semi-compact form as follows:\nmaximize w,x,y,z,x\u0302,y\u0302 sT1w + r T 1 x+\n( xT x\u0302 )(C 0 0 1 )( y y\u0302 ) + rT2 y\nsubject to A1x+B1w = b1 A2y +B2z = b2\nx\u0302 = 1 y\u0302 = sT2 z\nw, x, y, z \u2265 0\n(4)\nClearly, feasible solutions of Eq. (1) and Eq. (4) have the same objective value when y\u0302 is set appropriately. Notice that the dimensionality of the bilinear term in the objective function increases by 1 for both x and y. Hence, this transformation increases the dimensionality of the program by 1.\nThe rest of this section describes several classes of multiagent planning problems that can be formulated as bilinear programs. Starting with observation and transition independent DEC-MDPs, we extend the formulation to allow a different objective function (maximizing average reward over an infinite horizon), to handle interdependent observations, and to find Nash equilibria in competitive settings."}, {"heading": "2.1 DEC-MDPs", "text": "As mentioned previously, any transition-independent and observation-independent DECMDP (Becker et al., 2004) may be formulated as a bilinear program. Intuitively, a DECMDP is transition independent when no agent can influence the other agents\u2019 transitions. A DEC-MDP is observation independent when no agent can observe the states of other agents. These assumptions are crucial since they ensure a lower complexity of the problem (Becker\net al., 2004). In the remainder of the paper, we use simply the term DEC-MDP to refer to transition and observation independent DEC-MDP.\nThe DEC-MDP model has proved useful in several multiagent planning domains. One example that we use is the Mars rover planning problem (Bresina, Golden, Smith, & Washington, 1999), first formulated as a DEC-MDP by Becker et al. (2003). This domain involves two autonomous rovers that visit several sites in a given order and may decide to perform certain scientific experiments at each site. The overall activity must be completed within a given time limit. The uncertainty about the duration of each experiment is modeled by a given discrete distribution. While the rovers operate independently and receive local rewards for each completed experiment, the global reward function also depends on some experiments completed by both rovers. The interaction between the rovers is thus limited to a relatively small number of such overlapping tasks. We return to this problem and describe it in more detail in Section 6.\nA DEC-MDP problem is composed of two MDPs with state-sets S1, S2 and action sets A1, A2. The functions r1 and r2 define local rewards for action-state pairs. The initial state distributions are \u03b11 and \u03b12. The MDPs are coupled through a global reward function defined by the matrix R. Each entry R(i, j) represents the joint reward for the state-action i by one agent and j by the other. Our definition of a DEC-MDP is based on the work of Becker et al. (2004), with some modifications that we discuss below.\nDefinition 3. A two-agent transition and observation independent DEC-MDP with extended reward structure is defined by a tuple \u3008S,F , \u03b1,A, P,R\u3009: \u2022 S = (S1,S2) is the factored set of world states \u2022 F = (F1 \u2286 S1,F2 \u2286 S2) is the factored set of terminal states. \u2022 \u03b1 = (\u03b11, \u03b12) where \u03b1i : Si 7\u2192 [0, 1] are the initial state distribution functions \u2022 A = (A1,A2) is the factored set of actions \u2022 P = (P1, P2), Pi : Si \u00d7 Ai \u00d7 Si 7\u2192 [0, 1] are the transition functions. Let a \u2208 Ai\nbe an action, then P ai : Si \u00d7 Si 7\u2192 [0, 1] is a stochastic transition matrix such that Pi(s, a, s\u2032) = P ai (s, s\n\u2032) is the probability of a transition from state s \u2208 Si to state s\u2032 \u2208 Si of agent i, assuming it takes action a. The transitions from the final states have 0 probability; that is Pi(s, a, s\u2032) = 0 if s \u2208 Fi, s\u2032 \u2208 Si, and a \u2208 Ai. \u2022 R = (r1, r2, R) where ri : Si \u00d7Ai 7\u2192 R are the local reward functions and R : (S1 \u00d7 A1) \u00d7 (S2 \u00d7 A2) 7\u2192 R is the global reward function. Local rewards ri are represented as vectors, and R is a matrix with (s1, a1) as rows and (s2, a2) as columns.\nDefinition 3 differs from the original definition of transition and observation independent DEC-MDP (Becker et al. 2004, Definition 1) in two ways. The modifications allow us to explicitly capture assumptions that are implicit in previous work. First, the individual MDPs in our model are formulated as stochastic shortest-path problems (Bertsekas & Tsitsiklis, 1996). That is, there is no explicit time horizon, but instead some states are terminal. The process stops upon reaching a terminal state. The objective is to maximize the cumulative reward received before reaching the terminal states.\nThe second modification of the original definition is that Definition 3 generalizes the reward structure of the DEC-MDP formulation, using the extended reward structure. The joint rewards in the original DEC-MDP are defined only for the joint states (s1 \u2208 S1, s2 \u2208 S2)\nvisited by both agents simultaneously. That is, if agent 1 visits states s11, s 1 2 and agent 2 visits states s21, s 2 2, then the reward can only be defined for joint states (s 1 1, s 2 1) and (s 1 2, s 2 2). However, our DEC-MDP formulation with extended reward structure also allows the reward to depend on (s11, s 2 2) and (s 1 2, s 2 1), even when they are not visited simultaneously. As a result, the global reward may depend on the history, not only on the current state. Note that this reward structure is more general than what is commonly used in DEC-POMDPs.\nWe prefer the more general definition because it has been already implicitly used in previous work. In particular, this extended reward structure arises from introducing the primitive and compound events in the work of Becker et al. (2004). This reward structure is necessary to capture the characteristics of the Mars rover benchmark. Interestingly, this extension does not complicate our proposed solution methods in any way. Note that the stochastic shortest path formulation (right side of Figure 1) inherently eliminates any loops because time always advances when an action is taken. Therefore, every state in that representation may be visited at most once. This property is commonly used when an MDP is formulated as a linear program (Puterman, 2005).\nThe solution of a DEC-MDP is a deterministic stationary policy \u03c0 = (\u03c01, \u03c02), where \u03c0i : Si 7\u2192 Ai is the standard MDP policy (Puterman, 2005) for agent i. In particular, \u03c0i(si) represents the action taken by agent i in state si. To define the bilinear program, we use variables x(s1, a1) to denote the probability that agent 1 visits state s1 and takes action a1 and y(s2, a2) to denote the same for agent 2. These are the standard dual variables in MDP formulation. Given a solution in terms of x for agent 1, the policy is calculated for s \u2208 S1 as follows, breaking ties arbitrarily.\n\u03c01(s) = arg max a\u2208A1 x(s, a)\nThe policy \u03c02 is similarly calculated from y. The correctness of the policy calculation follows from the existence of an optimal policy that is deterministic and depends only on the local states of that agent (Becker et al., 2004).\nThe objective in DEC-MDPs in terms of x and y is then to maximize:\u2211 s1\u2208S1 a1\u2208A1 r1(s1, a1)x(s1, a1) + \u2211 s1\u2208S1 a1\u2208A1 \u2211 s2\u2208S2 a2\u2208A2 R(s1, a1, s2, a2)x(s1, a1)y(s2, a2) + \u2211 s2\u2208S2 a2\u2208A2 r2(s2, a2)y(s2, a2).\nThe stochastic shortest path representation is more general because any finite-horizon MDP can be represented as such by keeping track of time as part of the state, as illustrated\nin Figure 1. This modification allows us to apply the model directly to the Mars rover benchmark problem. Actions in the Mars rover problem may have different durations, while all actions in finite-horizon MDPs take the same amount of time.\nA DEC-MDP problem with an extended reward structure can be formulated as a bilinear mathematical program as follows. Vector variables x and y represent the state-action probabilities for each agent, as used in the dual linear formulation of MDPs. Given the transition and observation independence, the feasible regions may be defined by linear equalities A1x = \u03b11 and x \u2265 0, and A2y = \u03b12 and y \u2265 0. The matrices A1 and A2 are the same as for the dual formulation of total expected reward MDPs (Puterman, 2005), representing the following equalities for agent i:\n\u2211 a\u2032\u2208Ai x(s\u2032, a\u2032)\u2212 \u2211 s\u2208Si \u2211 a\u2208Ai Pi(s, a, s\u2032)x(s, a) = \u03b1i(s\u2032),\nfor every s\u2032 \u2208 Si. As described above, variables x(s, a) represent the probabilities of visiting the state s and taking action a by the appropriate agent during plan execution. Note that for agent 2, the variables are y(s, a) rather than x(s, a). Intuitively, these equalities ensure that the probability of entering each non-terminal state, through either the initial step or from other states, is the same as the probability of leaving the state. The bilinear problem is then formulated as follows:\nmaximize x,y rT1 x+ x TRy + rT2 y subject to A1x = \u03b11 x \u2265 0 A2y = \u03b12 y \u2265 0\n(5)\nIn this formulation, we treat the initial state distributions \u03b1i as vectors, based on a fixed ordering of the states. The following simple example illustrates the formulation.\nExample 4. Consider a DEC-MDP with two agents, depicted in Figure 2. The transitions in this problem are deterministic, and thus all the branches represent actions ai, ordered for each state from left to right. In some states, only one action is available. The shared reward r1, denoted by a dotted line, is received when both agents visit the state. The local rewards are denoted by the numbers above next to the states. The terminal states are omitted. The\nagents start in states s11 and s 2 1 respectively. The bilinear formulation of this problem is:\nmaximize x(s14, a1) \u2217 r1 \u2217 y(s24, a1) subject to x(s11, a1) = 1 y(s 2 1, a1) + y(s 2 1, a2) = 1\nx(s12, a1) + x(s 1 2, a2)\u2212 x(s11, a1) = 0 y(s22, a1)\u2212 y(s21, a1) = 0\nx(s13, a1)\u2212 x(s12, a1) = 0 y(s23, a1)\u2212 y(s22, a1) = 0 x(s14, a1)\u2212 x(s12, a2) = 0 y(s24, a1)\u2212 y(s22, a1) = 0\ny(s25, a1)\u2212 y(s23, a1) = 0\nWhile the results in this paper focus on two-agent problems, our approach can be extended to DEC-MDPs with more than two agents in two ways. The first approach requires that each component of the global reward depends on at most two agents. The DEC-MDP then may be viewed as a graph with vertices representing agents and edges representing the immediate interactions or dependencies. To formulate the problem as a bilinear program, this graph must be bipartite. Interestingly, this class of problems has been previously formulated (Kim et al., 2006). Let G1 and G2 be the indices of the agents in the two partitions of the bipartite graph. Then the problem can be formulated as follows:\nmaximize x,y \u2211 i\u2208G1,j\u2208G2 rTi xi + x T i Rijyj + r T j yj subject to Aixi = \u03b11 xi \u2265 0 i \u2208 G1 Ajyj = \u03b12 yj \u2265 0 j \u2208 G2\n(6)\nHere, Rij denotes the global reward for interactions between agents i and j. This program is bilinear and separable because the constraints on the variables in G1 and G2 are independent.\nThe second approach to generalize the framework is to represent the DEC-MDP as a multilinear program. In that case, no restrictions on the reward structure are necessary. An algorithm to solve, say a trilinear program, could be almost identical to the algorithm we propose, except that the best response would be calculated using bilinear, not linear programs. However, the scalability of this approach to more than a few agents is doubtful."}, {"heading": "2.2 Average-Reward Infinite-Horizon DEC-MDPs", "text": "The previous formulation deals with finite-horizon DEC-MDPs. An average-reward problem may also be formulated as a bilinear program (Petrik & Zilberstein, 2007b). This is particularly useful for infinite-horizon DEC-MDPs. For example, consider the infinitehorizon version of the Multiple Access Broadcast Channel (MABC) (Rosberg, 1983; Ooi & Wornell, 1996). In this problem, which has been used widely in recent studies of decentralized decision making, two communication devices share a single channel, and they need to periodically transmit some data. However, the channel can transmit only a single message at a time. When both agents send messages at the same time, this leads to a collision, and the transmission fails. The memory of the devices is limited, thus they need to send the messages sooner rather than later. We adapt the model from the work of Rosberg (1983), which is particularly suitable because it assumes no sharing of local information among the devices.\nThe definition of average-reward two-agent transition and observation independent DECMDP is the same as Definition 3, with the exception of the terminal states, policy, and objective. There are no terminal states in average-reward DEC-MDPs, and the policy (\u03c01, \u03c02) may be stochastic. That is, \u03c0i(s, a) 7\u2192 [0, 1] is the probability of agent i taking an action a in state s. The objective is to find a stationary infinite-horizon policy \u03c0 that maximizes the average reward, or gain, defined as follows.\nDefinition 5. Let \u03c0 = (\u03c01, \u03c02) be a stochastic policy, and Xt and Yt be random variables that represent the probability distributions over the state-action pairs at time t of the two agents respectively according to \u03c0. The gain G of the policy \u03c0 is then defined for states s1 \u2208 S1 and s2 \u2208 S2 as:\nG(s1, s2) = lim N\u2192\u221e 1 N E(s1,\u03c01(s1)),(s2,\u03c02(s2)) [ N\u22121\u2211 t=0 r1(Xt) +R(Xt, Yt) + r2(Yt) ] ,\nwhere \u03c0i(si) is the distribution over the actions in state si. Note that the expectation is with respect to the initial states and action distributions (s1, \u03c01(s1)), (s2, \u03c02(s2)).\nThe actual gain of a policy depends on the agents\u2019 initial state distributions \u03b11, \u03b12 and may be expressed as \u03b1T1G\u03b12, with G represented as a matrix. Puterman (2005), for example, provides a more detailed discussion of the definition and meaning of policy gain.\nTo simplify the bilinear formulation of the average-reward DEC-MDP, we assume that r1 = 0 and r2 = 0. The bilinear program follows.\nmaximize p1,p2,q1,q2 \u03c4(p1, p2, q1, q2) = pT1Rp2 subject to p1, p2 \u2265 0 \u2200s\u2032 \u2208 S1\n\u2211 a\u2208A1 p1(s\u2032, a)\u2212 \u2211 s\u2208S1,a\u2208A1 p1(s, a)P a1 ( s, s\u2032 ) = 0\n\u2200s\u2032 \u2208 S1 \u2211 a\u2208A1 p1(s\u2032, a) + \u2211 a\u2208A1 q1(s\u2032, a)\u2212 \u2211 s\u2208S1,a\u2208A1 q1(s, a)P a1 ( s, s\u2032 ) = \u03b11(s\u2032)\n\u2200s\u2032 \u2208 S2 \u2211 a\u2208A2 p2(s\u2032, a)\u2212 \u2211 s\u2208S2,a\u2208A2 p2(s, a)P a2 ( s, s\u2032 ) = 0\n\u2200s\u2032 \u2208 S2 \u2211 a\u2208A2 p2(s\u2032, a) + \u2211 a\u2208A2 q2(s\u2032, a)\u2212 \u2211 s\u2208S2,a\u2208A2 q2(s, a)P a2 ( s, s\u2032 ) = \u03b12(s\u2032)\n(7)\nThe variables in the program come from the dual formulation of the average-reward MDP linear program (Puterman, 2005). The state sets of the MDPs is divided into recurrent and transient states. The recurrent states are expected to be visited infinitely many times, while the transient states are expected to be visited finitely many times. Variables p1 and p2 represent the limiting distributions of each MDP, which is non-zero for all recurrent states. The (possibly stochastic) policy \u03c0i of agent i is defined in the recurrent states by the probability of taking action a \u2208 Ai in state s \u2208 Si:\n\u03c0i(s, a) = pi(s, a)\u2211\na\u2032\u2208Ai pi(s, a \u2032) .\nThe variables pi are 0 in transient states. The policy in the transient states is calculated from variables qi as:\n\u03c0i(s, a) = qi(s, a)\u2211\na\u2032\u2208Ai qi(s, a \u2032) .\nThe correctness of the constraints follows from the dual formulation of optimal average reward (Puterman 2005, Equation 9.3.4). Petrik and Zilberstein (2007b) provide further details of this formulation."}, {"heading": "2.3 General DEC-POMDPs and Extensive Games", "text": "The general DEC-POMDP problem and extensive-form games with two agents, or players, can also be formulated as bilinear programs. However, the constraints may not be separable because actions of one agent influence the other agent. The approach in this case may be similar to linear complementarity problem formulation of extensive games (Koller, Megiddo, & von Stengel, 1994), and integer linear program formulation of DEC-POMDPs (Aras & Charpillet, 2007). The approach we develop is closely related to event-driven DECPOMDPs (Becker et al., 2004), but it is in general more efficient. Nevertheless, the size of the bilinear program is exponential in the size of the DEC-POMDP. This can be expected since solving DEC-POMDPs is NEXP-complete (Bernstein et al., 2000), while solving bilinear programs is NP-complete (Mangasarian, 1995). Because the general formulation in this case is somewhat cumbersome, we only illustrate it using the following simple example. Aras (2008) provides the details of a similar construction.\nExample 6. Consider the problem depicted in Figure 3, assuming that the agents are cooperative. The actions of the other agent are not observable, as denoted by the information sets. This approach can be generalized to any problem with any observable sets as long as the perfect recall condition is satisfied. Agents satisfy the perfect recall condition when they remember the set of actions taken in the prior moves (Osborne & Rubinstein, 1994). Rewards are only collected in the leaf-nodes in this case. The variables on the edges represent the probability of taking the action. Here, variables a denote the actions of one agent, and\nvariables b of the other. The total common reward received in the end is:\nr = a11b11a21r1 + a11b11a22r2 + a11b12a21r3 + a11b12a22r4 + a12b11a23r5 + a12b11a24r6 + a12b12a23r7 + a12b12a24r8.\nThe constraints in this problem are of the following form: a11 + a12 = 1.\nAny DEC-POMDP problem can be represented using the approach used above. It is also straightforward to extend the approach to problems with rewards in every node. However, the above formulation is clearly not bilinear. To apply our algorithm to this class of problems, we need to reformulate the problem in a bilinear form. This can be easily accomplished in a way similar to the construction of the dual linear program for an MDP. Namely, we introduce variables:\nc11 = a11 c12 = a12 c21 = a11a21 c22 = a11a22\nand so on for every set of variables on any path to a leaf node. Then, the objective may be reformulated as follows:\nr = c21b11r1 + c22b11r2 + c23b12r3 + c24b12r4 + c25b11r5 + c26b11r6 + c27b12r7 + c28b12r8.\nVariables bij are replaced in the same fashion. This objective function is clearly bilinear. The constraints may be reformulated as follows. The constraint a21 + a22 = 1 can be multiplied by a11 and then replaced by c21 + c22 = c11, and so on. That is, the variables in each level have to sum to the variable that is their least common parent in the level above for the same agent."}, {"heading": "2.4 General Two-Player Games", "text": "In addition to cooperative problems, some competitive problems with 2 players may be formulated as bilinear programs. It is known that the problem of finding an equilibrium for a bi-matrix game may be formulated as a linear complementarity problem (Cottle, Pang, & Stone, 1992). It has also been shown that a linear complementarity problem may be formulated as a bilinear problem (Mangasarian, 1995). However, a direct application of these two reductions results in a complex problem with a large dimensionality. Below, we demonstrate how a general game can be directly formulated as a bilinear program. There are many ways to formulate a game, thus we take a very general approach. We simply assume that each agent optimizes a linear program, as follows.\nmaximize x d1(x) = rT1 x+ x TC1y subject to A1x = b1 x \u2265 0\n(8)\nmaximize y d2(y) = rT2 y + x TC2y subject to A2y = b2 y \u2265 0\n(9)\nIn Eq. (8), the variable y is considered to be a constant and similarly in Eq. (9) the variable x is considered to be a constant. For normal form games, the constraint matrices A1 and A2 are simply rows of ones, and b1 = b2 = 1. For competitive DEC-MDPs, the constraint matrices A1 and A2 are the same as in Section 2.1. Extensive games may be formulated similarly to DEC-POMDPs, as described in Section 2.3.\nThe game specified by linear programs Eq. (8) and Eq. (9) may be formulated as a bilinear program as follows. First, define the reward vectors for each agent, given a policy of the other agent.\nq1(y) = r1 + C1y q2(x) = r2 + CT2 x.\nThese values are unrelated to those of Eq. (7). The complementary slackness values (Vanderbei, 2001) for the linear programs Eq. (8) and Eq. (9) are:\nk1(x, y, \u03bb1) = ( q1(y)T \u2212 \u03bbT1A1 ) x\nk2(x, y, \u03bb2) = ( q2(x)T \u2212 \u03bbT2A2 ) y,\nwhere \u03bb1 and \u03bb2 are the dual variables of the corresponding linear programs. For any primal feasible x and y, and dual feasible \u03bb1 and \u03bb2, we have that k1(x, y, \u03bb1) \u2265 0 and k2(x, y, \u03bb2) \u2265 0. The equality is attained if and only if x and y are optimal. This can be used to write the following optimization problem, in which we implicitly assume that x,y,\u03bb1,\u03bb2 are feasible in the appropriate primal and dual linear programs:\n0 \u2264 min x,y,\u03bb1,\u03bb2 k1(x, y, \u03bb1) + k2(x, y, \u03bb2)\n= min x,y,\u03bb1,\u03bb2 (q1(y)T \u2212 \u03bbT1A1)x+ (q2(x)T \u2212 \u03bbT2A2)y\n= min x,y,\u03bb1,\u03bb2 ((r1 + C1y)T \u2212 \u03bbT1A1)x+ ((r2 + CT2 x)T \u2212 \u03bbT2A2)y\n= min x,y,\u03bb1,\u03bb2 rT1 x+ r T 2 y + x T(C1 + C2)y \u2212 xTAT1 \u03bb1 \u2212 yTAT2 \u03bb2\n= min x,y,\u03bb1,\u03bb2 rT1 x+ r T 2 y + x T(C1 + C2)y \u2212 bT1 \u03bb1 \u2212 bT2 \u03bb2.\nTherefore, any feasible x and y that set the right hand side to 0 solve both linear programs in Eq. (8) and Eq. (9) optimally. Adding the primal and dual feasibility conditions to the above, we get the following bilinear program:\nminimize x,y,\u03bb1,\u03bb2 rT1 x+ r T 2 y + x T(C1 + C2)y \u2212 bT1 \u03bb1 \u2212 bT2 \u03bb2 subject to A1x = b1 A2y = b2\nr1 + C1y \u2212AT1 \u03bb1 \u2264 0\nr2 + CT2 x\u2212AT2 \u03bb2 \u2264 0 x \u2265 0 y \u2265 0\n(10)\nAlgorithm 1: IterativeBestResponse(B) x0, w0 \u2190 rand ;1 i\u2190 1 ;2 while yi\u22121 6= yi or xi\u22121 6= xi do3 (yi, zi)\u2190 arg maxy,z f(wi\u22121, xi\u22121, y, z) ;4 (xi, wi)\u2190 arg maxx,w f(w, x, yi, zi) ;5 i\u2190 i+ 16\nreturn f(wi, xi, yi, zi)7\nThe optimal solution of Eq. (10) is 0 and it corresponds to a Nash equilibrium. This is because both the primal variables x, y and dual variables \u03bb1, \u03bb2 are feasible and the complementary slackness condition is satisfied. The open question in this example are the interpretation of an approximate result and a formulation that would select the equilibrium. It is not clear yet whether it is possible to formulate the program so that the optimal solution will be a Nash equilibrium that maximizes a certain criterion. The approximate solutions of the program probably correspond to -Nash equilibria, but this remain an open question.\nThe algorithm in this case also relies on the number of shared rewards being small compared to the size of the problem. But even if this is not the case, it is often possible that the number of shared rewards may be automatically reduced as described in Section 4. In fact, it is easy to show that a zero-sum normal form game is automatically reduced to two uncoupled linear programs. This follows from the dimensionality reduction procedure in Section 4."}, {"heading": "3. Solving Bilinear Programs", "text": "One simple method often used for solving bilinear programs is the iterative procedure shown in Algorithm 1. The parameter B represents the bilinear program. While the algorithm often performs well in practice, it tends to converge to a suboptimal solution (Mangasarian, 1995). When applied to DEC-MDPs, this algorithm is essentially identical to JESP (Nair, Tambe, Yokoo, Pynadath, & Marsella, 2003)\u2013one of the early solution methods. In the following, we use f(w, x, y, z) to denote the objective value of Eq. (1).\nThe rest of this section presents a new anytime algorithm for solving bilinear programs. The goal of the algorithm to is to produce a good solution quickly and then improve the solution in the remaining time. Along with each approximate solution, the maximal approximation bound with respect to the optimal solution is provided. As we show below, our algorithm can benefit from results produced by suboptimal algorithms, such as Algorithm 1, to quickly determine tight approximation bounds."}, {"heading": "3.1 The Successive Approximation Algorithm", "text": "We begin with an overview of a successive approximation algorithm for bilinear problems that takes advantage of a low number of interactions between the agents. It is particularly suitable when the input problem is large in comparison to its dimensionality, as defined in Section 2. We address the issue of dimensionality reduction in Section 4.\nWe begin with a simple intuitive explanation of the algorithm, and then show how it can be formalized. The bilinear program can be seen as an optimization game played by two agents, in which the first agent sets the variables w, x and the second one sets the variables y, z. This is a general observation that applies to any bilinear program. In any practical application, the feasible sets for the two sets of variables may be too large to explore exhaustively. In fact, when this method is applied to DEC-MDPs, these sets are infinite and continuous. The basic idea of the algorithm is to first identify the set of best responses of one of the agents, say agent 1, to some policy of the other agent. This is simple because once the variables of agent 2 are fixed, the program becomes linear, which is relatively easy to solve. Once the set of best-response policies of agent 1 is identified, assuming it is of a reasonable size, it is possible to calculate the best response of agent 2.\nThis general approach is also used by the coverage set algorithm (Becker et al., 2004). One distinction is that the representation used in CSA applies only to DEC-MDPs, while our formulation applies to bilinear programs\u2013a more general representation. The main distinction between our algorithm and CSA is the way in which the variables y, z are chosen. In CSA, the values y, z are calculated in a way that simply guarantees termination in finite time. We, on the other hand, choose values y, z greedily so as to minimize the approximation bound on the optimal solution. This is possible because we establish bounds on the optimality of the solution throughout the calculation. As a result, our algorithm converges more rapidly and may be terminated at any time with a guaranteed performance bound. Unlike the earlier version of the algorithm (Petrik & Zilberstein, 2007a), the version described in this paper calculates the best response using only a subset of the values of y, z. As we show, it is possible to identify regions of y, z in which it is impossible to improve the current best solution and exclude these regions from consideration.\nWe now formalize the ideas described above. To simplify the notation, we define feasible sets as follows:\nX = {(x,w) A1x+B1w = b1} Y = {(y, z) A2y +B2z = b2}.\nWe use y \u2208 Y to denote that there exists z such that (y, z) \u2208 Y . In addition, we assume that the problem is in a semi-compact form. This is reasonable because any bilinear program may be converted to semi-compact form with an increase in dimensionality of one, as we have shown earlier.\nAssumption 7. The sets X and Y are bounded, that is, they are contained in a ball of a finite radius.\nWhile Assumption 7 is limiting, coordination problems under uncertainty typically have bounded feasible sets because the variables correspond to probabilities bounded to [0, 1].\nAssumption 8. The bilinear program is in a semi-compact form.\nThe main idea of the algorithm is to compute a set X\u0303 \u2286 X that contains only those elements that satisfy a necessary optimality condition. The set X\u0303 is formally defined as follows:\nX\u0303 \u2286 {\n(x\u2217, w\u2217) \u2203(y, z) \u2208 Y f(w\u2217, x\u2217, y, z) = max (x,w)\u2208X\nf(w, x, y, z) } .\nAs described above, this set may be seen as a set of best responses of one agent to the variable settings of the other. The best responses are easy to calculate since the bilinear program in Eq. (1) reduces to a linear program for fixed w, x or fixed y, z. In our algorithm, we assume that X\u0303 is potentially a proper subset of all necessary optimality points and focus on the approximation error of the optimal solution. Given the set X\u0303, the following simplified problem is solved.\nmaximize w,x,y,z f(w, x, y, z) subject to (x,w) \u2208 X\u0303 A2y +B2z = b2 y, z \u2265 0\n(11)\nUnlike the original continuous set X, the reduced set X\u0303 is discrete and small. Thus the elements of X\u0303 may be enumerated. For a fixed w and x, the bilinear program in Eq. (11) reduces to a linear program.\nTo help compute the approximation bound and to guide the selection of elements for X\u0303, we use the best-response function g(y), defined as follows:\ng(y) = max {w,x,z (x,w)\u2208X,(y,z)\u2208Y } f(w, x, y, z) = max {x,w (x,w)\u2208X} f(w, x, y, 0),\nwith the second equality for semi-compact programs only and feasible y \u2208 Y . Note that g(y) is also defined for y /\u2208 Y , in which case the choice of z is arbitrary since it does not influence the objective function. The best-response function is easy to calculate using a linear program. The crucial property of the function g that we use to calculate the approximation bound is its convexity. The following proposition holds because g(y) = max{x,w (x,w)\u2208X} f(w, x, y, 0) is a maximum of a finite set of linear functions.\nProposition 9. The function g(y) is convex when the program is in a semi-compact form.\nProposition 9 relies heavily on the separability of Eq. (1), which means that the constraints on the variables on one side of the bilinear term are independent of the variables on the other side. The separability ensures that w, x are valid solutions regardless of the values of y, z. The semi-compactness of the program is necessary to establish convexity, as shown in Example 23 in Appendix C. The example is constructed using the properties described in the appendix, which show that f(w, x, y, z) may be expressed as a sum of a convex and a concave function.\nWe are now ready to describe Algorithm 2, which computes the set X\u0303 for a bilinear problem B such that the approximation error is at most 0. The algorithm iteratively adds the best response (x,w) for a selected pivot point y into X\u0303. The pivot points are selected hierarchically. At an iteration j, the algorithm keeps a set of polyhedra S1 . . . Sj which represent the triangulation of the feasible space Y , which is possible based on Assumption 7. For each polyhedron Si = (y1 . . . yn+1), the algorithm keeps a bound i on the maximal difference between the optimal solution on the polyhedron and the best solution found so far. This error bound on a polyhedron Si is defined as:\ni = e(Si) = max {w,x,y|(x,w)\u2208X,y\u2208Si} f(w, x, y, 0)\u2212 max {w,x,y|(x,w)\u2208X\u0303,y\u2208Si} f(w, x, y, 0),\nAlgorithm 2: BestResponseApprox(B, 0) returns (w, x, y, z) // Create the initial polyhedron S1. S1 \u2190 (y1 . . . yn+1), Y \u2286 S1 ;1 // Add best-responses for vertices of S1 to X\u0303 X\u0303 \u2190 {arg max(x,w)\u2208X f(w, x, y1, 0), ..., arg max(x,w)\u2208X f(w, x, yn+1, 0)} ;2 // Calculate the error and pivot point \u03c6 of the initial polyhedron ( 1, \u03c61)\u2190 PolyhedronError(S1) ; // Section 3.2,Section 3.33 // Initialize the number of polyhedra to 1 j \u2190 1 ;4 // Continue until reaching a predefined precision 0 while maxi=1,...,j i \u2265 0 do5 // Find the polyhedron with the largest error i\u2190 arg maxk=1,...,j k ;6 // Select the pivot point of the polyhedron with the largest error y \u2190 \u03c6i ;7 // Add the best response to the pivot point y to the set X\u0303 X\u0303 \u2190 X \u222a {arg max(x,w)\u2208X f(w, x, y, 0)} ;8 // Calculate errors and pivot points of the refined polyhedra for k = 1, . . . , n+ 1 do9 j \u2190 j + 1 ;10 // Replace the k-th vertex by the pivot point y Sj \u2190 (y, y1 . . . yk\u22121, yk+1, . . . yn+1) ;11 ( j , \u03c6j)\u2190 PolyhedronError(Sj) ; // Section 3.2,Section 3.312 // Take the smaller of the errors on the original and the refined\npolyhedron. The error may not increase with the refinement, although the bound may\nj \u2190 min{ i, j} ;13 // Set the error of the refined polyhedron to 0, since the region is covered by the refinements i \u2190 0 ;14\n(w, x, y, z)\u2190 arg max{w,x,y,z (x,w)\u2208X\u0303,(y,z)\u2208Y } f(w, x, y, 0) ;15 return (w, x, y, z) ;16\nwhere X\u0303 represents the current, not final, set of best responses. Next, a point y0 is selected as described below and n + 1 new polyhedra are created by replacing one of the vertices by y0 to get: (y0, y2, . . .), (y1, y0, y3, . . .), . . . , (y1, . . . , yn, y0). This is depicted for a 2-dimensional set Y in Figure 4. The old polyhedron is discarded and the above procedure is then repeatedly applied to the polyhedron with the maximal approximation error.\nFor the sake of clarity, the pseudo-code of Algorithm 2 is simplified and does not address any efficiency issues. In practice, g(yi) could be cached, and the errors i could be stored in a prioritized heap or at least in a sorted array. In addition, a lower bound li and an upper bound ui is calculated and stored for each polyhedron Si = (y1 . . . yn+1). The function e(Si) calculates their maximal difference on the polyhedron Si and the point where it is attained. The error bound i on the polyhedron Si may not be tight, as we describe in Remark 13. As a result, when the polyhedron Si is refined to n polyhedra S\u20321 . . . S \u2032 n with online error\nbounds \u20321 . . . \u2032 n, it is possible that for some k: \u2032 k > i. Since S \u2032 1 . . . S \u2032 n \u2286 Si, the true error on S\u2032k is less than on Si and therefore \u2032 k may be set to i.\nConceptually, the algorithm is similar to CSA, but there are some important differences. The main difference is in the choice of the pivot point y0 and the bounds on g. CSA does not keep any upper bound and it evaluates g(y) on all the intersection points of planes defined by the current solutions in X\u0303. That guarantees that g(y) is eventually known precisely (Becker et al., 2004). A similar approach was also taken for POMDPs (Cheng, 1988). The upper bound on the number of intersection points in CSA is ( |X\u0303| dimY ) . The principal problem is that the bound is exponential in the dimension of Y , and experiments do not show a slower growth in typical problems. In contrast, we choose the pivot points to minimize the approximation error. This is more selective and tends to more rapidly reduce the error bound. In addition, the error at the pivot point may be used to determine the overall error bound. The following proposition states the soundness of the triangulation, proved in Appendix A. The correctness of the triangulation establishes that in each iteration the approximation error over Y is equivalent to the maximum of the approximation errors over the current polyhedra S1 . . . Sj .\nProposition 10. In the proposed triangulation, the sub-polyhedra do not overlap and they cover the whole feasible set Y , given that the pivot point is in the interior of S."}, {"heading": "3.2 Online Error Bound", "text": "The selection of the pivot point plays a key role in the performance of the algorithm, in both calculating the error bound and the speed of convergence to the optimal solution. In this section we show exactly how we use the triangulation in the algorithm to calculate an error bound. To compute the approximation bound, we define the approximate best-response function g\u0303(y) as:\ng\u0303(y) = max {x,w (x,w)\u2208X\u0303} f(w, x, y, 0).\nNotice that z is not considered in this expression, since we assume that the bilinear program is in the semi-compact form. The value of the best approximate solution during the execution of the algorithm is:\nmax {w,x,y,z (x,w)\u2208X\u0303,y\u2208Y } f(w, x, y, 0) = max y\u2208Y g\u0303(y).\nThis value can be calculated at runtime when each new element of X\u0303 is added. Then the maximal approximation error between the current solution and the optimal one may be calculated from the approximation error of the best-response function g(\u00b7), as stated by the following proposition.\nProposition 11. Consider a bilinear program in a semi-compact form. Then let w\u0303, x\u0303, y\u0303 be an optimal solution of Eq. (11) and let w\u2217, x\u2217, y\u2217 be an optimal solution of Eq. (1). The approximation error is then bounded by:\nf(w\u2217, x\u2217, y\u2217, 0)\u2212 f(w\u0303, x\u0303, y\u0303, 0) \u2264 max y\u2208Y (g(y)\u2212 g\u0303(y)) .\nProof.\nf(w\u2217, x\u2217, y\u2217, 0)\u2212 f(w\u0303, x\u0303, y\u0303, 0) = max y\u2208Y g(y)\u2212max y\u2208Y g\u0303(y) \u2264 max y\u2208Y g(y)\u2212 g\u0303(y)\nNow, the approximation error is maxy\u2208Y g(y)\u2212 g\u0303(y), which is bounded by the difference between an upper bound and a lower bound on g(y). Clearly, g\u0303(y) is a lower bound on g(y). Given points in which g\u0303(y) is the same as the best-response function g(y), we can use Jensen\u2019s inequality to obtain the upper bound. This is summarized by the following lemma. Lemma 12. Let yi \u2208 Y for i = 1, . . . , n+ 1 such that g\u0303(yi) = g(yi). Then g (\u2211n+1\ni=1 ciyi\n) \u2264 \u2211n+1 i=1 cig(yi) when \u2211n+1 i=1 ci = 1 and ci \u2265 0 for all i.\nThe actual implementation of the bound relies on the choice of the pivot points. Next we describe the maximal error calculation on a single polyhedron defined by S = (y1 . . . yn). Let matrix T have yi as columns, and let L = {x1 . . . xn+1} be the set of the best responses for its vertices. The matrix T is used to convert any y in absolute coordinates to a relative representation t that is a convex combination of the vertices. This is defined formally as follows:\ny = Tt =  . . .y1 y2 . . . . . .  t 1 = 1Tt 0 \u2264 t\nwhere the yi\u2019s are column vectors. We can represent a lower bound l(y) for g\u0303(y) and an upper bound u(y) for g(y) as:\nl(y) = max x\u2208L\nrTx+ xTCy\nu(y) = [g(y1), g(y2), . . .]Tt = [g(y1), g(y2), . . .]T ( T 1T )\u22121( y 1 ) ,\nThe upper bound correctness follows from Lemma 12. Notice that u(y) is a linear function, which enables us to use a linear program to determine the maximal-error point.\nAlgorithm 3: PolyhedronError(B, S) P \u2190 one of Eq. (12), or (13), or (14), or (20) ;1 t\u2190 the optimal solution of P ;2 \u2190 the optimal objective value of P ;3 // Coordinates t are relative to the vertices of S, convert them to absolute values in Y \u03c6\u2190 Tt ;4 return ( , \u03c6) ;5\nRemark 13. Notice that we use L instead of X\u0303 in calculating l(y). Using all of X\u0303 would lead to a tighter bound, as it is easy to show in three-dimensional examples. However, this also would substantially increase the computational complexity.\nNow, the error on a polyhedron S may be expressed as:\ne(S) \u2264 max y\u2208S u(y)\u2212 l(y) = max y\u2208S u(y)\u2212max x\u2208L rTx+ xTCy\n= max y\u2208S min x\u2208L\nu(y)\u2212 rTx\u2212 xTCy.\nWe also have y \u2208 S \u21d4 ( y = Tt \u2227 t \u2265 0 \u2227 1Tt = 1 ) .\nAs a result, the point with the maximal error bound may be determined using the following linear program in terms of variables t, :\nmaximize t, subject to \u2264 u(Tt)\u2212 rTx\u2212 xTCTt \u2200x \u2208 L\n1Tt = 1 t \u2265 0\n(12)\nHere x is not a variable. The formulation is correct because all feasible solutions are bounded below the maximal error and any maximal-error solution is feasible.\nProposition 14. The optimal solution of Eq. (12) is equivalent to maxy\u2208S |u(y)\u2212 l(y)|.\nWe thus select the next pivot point to greedily minimize the error. The maximal difference is actually achieved in points where some of the planes meet, as Becker et al. (2004) have suggested. However, checking these intersections is very similar to running the simplex algorithm. In general, the simplex algorithm is preferable to interior point methods for this program because of its small size (Vanderbei, 2001).\nAlgorithm 3 shows a general way to calculate the maximal error and the pivot point on the polyhedron S. This algorithm may use the basic formulation in Eq. (12), or the more advanced formulations in Eqs. (13), (14), and (20) defined in Section 3.3.\nIn the following section, we describe a more refined pivot point selection method that can in some cases dramatically improve the performance."}, {"heading": "3.3 Advanced Pivot Point Selection", "text": "As described above, the pivot points are chosen greedily to both determine the maximal error in each polyhedron and to minimize the approximation error. The basic approach described in Section 3.1 may be refined, because the goal is not to approximate the function g(y) with the least error, but to find the optimal solution. Intuitively, we can ignore those regions of Y that will not guarantee any improvement of the current solution, as illustrated in Figure 5. As we show below, the search for the maximal error point could be limited to this region as well.\nWe first define a set Yh \u2286 Y that we will search for the maximal error, given that the optimal solution f\u2217 \u2265 h.\nYh = {y g(y) \u2265 h, y \u2208 Y }.\nThe next proposition states that the maximal error needs to be calculated only in a superset of Yh.\nProposition 15. Let w\u0303, x\u0303, y\u0303, z\u0303 be the approximate optimal solution and w\u2217, x\u2217, y\u2217, z\u2217 be the optimal solution. Also let f(w\u2217, x\u2217, y\u2217, z\u2217) \u2265 h and assume some Y\u0303h \u2287 Yh. The approximation error is then bounded by:\nf(w\u2217, x\u2217, y\u2217, z\u2217)\u2212 f(w\u0303, x\u0303, y\u0303, z\u0303) \u2264 max y\u2208Y\u0303h g(y)\u2212 g\u0303(y).\nProof. First, f(w\u2217, x\u2217, y\u2217, z\u2217) = g(y\u2217) \u2265 h and thus y\u2217 \u2208 Yh. Then:\nf(w\u2217, x\u2217, y\u2217, z\u2217)\u2212 f(w\u0303, x\u0303, y\u0303, z\u0303) = max y\u2208Yh g(y)\u2212max y\u2208Y g\u0303(y)\n\u2264 max y\u2208Yh g(y)\u2212 g\u0303(y) \u2264 max y\u2208Y\u0303h g(y)\u2212 g\u0303(y)\nProposition 15 indicates that the point with the maximal error needs to be selected only from the set Yh. The question is how to easily identify Yh. Because the set is not convex in general, a tight approximation of this set needs to be found. In particular, we use methods\nthat approximate the intersection of a superset of Yh with the polyhedron that is being refined, using the following methods:\n1. Feasibility [Eq. (13)]: Require that pivot points are feasible in Y . 2. Linear bound [Eq. (14)]: Use the linear upper bound u(y) \u2265 h. 3. Cutting plane [Eq. (20)]: Use the linear inequalities that define Y Ch , where Y Ch = R|Y | \\ Yh is the complement of Yh.\nAny combination of these methods is also possible.\nFeasibility The first method is the simplest, but also the least constraining. The linear program to find the pivot point with the maximal error bound is as follows:\nmaximize ,t,y,z subject to \u2264 u(Tt)\u2212 rTx+ xTCTt \u2200x \u2208 L\n1Tt = 1 t \u2265 0 y = Tt\nA2y +B2z = b2 y, z \u2265 0\n(13)\nThis approach does not require that the bilinear program is in the semi-compact form.\nLinear Bound The second method, using the linear bound, is also very simple to implement and compute, and it is more selective than just requiring feasibility. Let:\nY\u0303h = {y u(y) \u2265 h} \u2287 {y g(y) \u2265 h} = Yh.\nThis set is convex and thus does not need to be approximated. The linear program used to find the pivot point with the maximal error bound is as follows:\nmaximize ,t subject to \u2264 u(Tt)\u2212 rTx+ xTCTt \u2200x \u2208 L\n1Tt = 1 t \u2265 0 u(Tt) \u2265 h\n(14)\nThe difference from Eq. (12) is the last constraint. This approach requires that the bilinear program is in the semi-compact form to ensure that u(y) is a bound on the total return.\nCutting Plane The third method, using the cutting plane elimination, is the most computationally intensive one, but also the most selective one. Using this approach requires additional assumptions on the other parts of the algorithm, which we discuss below. The method is based on the same principle as \u03b1-extensions in concave cuts (Horst & Tuy, 1996). We start with the set Y Ch because it is convex and may be expressed as:(\nmax w,x sT1w + r T 1 x+ y\nTCTx+ rT2 y ) \u2264 h (15)\nA1x+B1w = b1 (16) w, x \u2265 0 (17)\nTo use these inequalities in selecting the pivot point, we need to make them linear. But there are two obstacles: Eq. (15) contains a bilinear term and is a maximization. Both of these issues can be addressed by using the dual formulation of Eq. (15). The corresponding linear program and its dual for fixed y, ignoring constants h and rT2 y, are:\nmaximize w,x sT1w + r T 1 x+ y TCTx subject to A1x+B1w = b1 w, x \u2265 0\n(18) minimize \u03bb bT1 \u03bb\nsubject to AT1 \u03bb \u2265 r1 + Cy\nBT1 \u03bb \u2265 s1\n(19)\nUsing the dual formulation, Eq. (15) becomes:( min \u03bb bT1 \u03bb+ r T 2 y ) \u2264 h\nAT1 \u03bb \u2265 r1 + Cy BT1 \u03bb \u2265 s1\nNow, we use that for any function \u03c6 and any value \u03b8 the following holds:\nmin x \u03c6(x) \u2264 \u03b8 \u21d4 (\u2203x) \u03c6(x) \u2264 \u03b8.\nFinally, this leads to the following set of inequalities.\nrT2 y \u2264 h\u2212 bT1 \u03bb Cy \u2264 AT1 \u03bb\u2212 r1 s1 \u2264 BT1 \u03bb\nThe above inequalities define the convex set Y Ch . Because its complement Yh is not necessarily convex, we need to use its convex superset Y\u0303h on the given polyhedron. This is done by projecting Y Ch , or its subset, onto the edges of each polyhedron as depicted in Figure 6 and described in Algorithm 4. The algorithm returns a single constraint which cuts off part of the set Y Ch . Notice that only the combination of the first n points fk is\nAlgorithm 4: PolyhedronCut({y1, . . . , yn+1}, h) returns constraint \u03c3Ty \u2264 \u03c4 // Find vertices of the polyhedron {y1, . . . , yn+1} inside of Y Ch I \u2190 {yi yi \u2208 Y Ch } ;1 // Find vertices of the polyhedron outside of Y Ch O \u2190 {yi yi \u2208 Yh} ;2 // Find at least n points fk in which the edge of Yh intersects an edge of the polyhedron k \u2190 1 ;3 for i \u2208 O do4 for j \u2208 I do5 fk \u2190 yj + max\u03b2{\u03b2 \u03b2(yi \u2212 yj) \u2208 (Y Ch )} ;6 k \u2190 k + 1 ;7 if k \u2265 n then8 break ;9\nFind \u03c3 and \u03c4 , such that [f1, . . . , fn]\u03c3 = \u03c4 and 1T\u03c3 = 1 ;10 // Determine the correct orientation of the constraint to have all y in Yh\nfeasible\nif \u2203yj \u2208 O, and \u03c3Tyj > \u03c4 then11 // Reverse the constraint if it points the wrong way \u03c3 \u2190 \u2212\u03c3 ;12 \u03c4 \u2190 \u2212\u03c4 ;13\nreturn \u03c3Ty \u2264 \u03c414\nused. In general, there may be more than n points, and any subset of points fk of size n can be used to define a new cutting plane that constraints Yh. This did not lead to significant improvements in our experiments. The linear program to find the pivot point with the cutting plane option is as follows:\nmaximize ,t,y subject to \u2264 u(Tt)\u2212 rTx+ xTCTt \u2200x \u2208 L\n1Tt = 1 t \u2265 0 y = Tt\n\u03c3Ty \u2264 \u03c4\n(20)\nHere, \u03c3, and \u03c4 are obtained as a result of running Algorithm 4. Note that this approach requires that the bilinear program is in the semi-compact form to ensure that g(y) is convex. The following proposition states the correctness of this procedure.\nProposition 16. The resulting polyhedron produced by Algorithm 4 is a superset of the intersection of the polyhedron S with the complement of Yh.\nProof. The convexity of g(y) implies that Y Ch is also convex. Therefore, the intersection\nQ = {y \u03c3Ty \u2265 \u03c4} \u2229 S\nis also convex. It is also a convex hull of points fk \u2208 Y Ch . Therefore, from the convexity of Y Ch , we have that Q \u2286 Y Ch , and therefore S \u2212Q \u2287 Yh."}, {"heading": "4. Dimensionality Reduction", "text": "Our experiments show that the efficiency of the algorithm depends heavily on the dimensionality of the matrix C in Eq. (1). In this section, we show the principles behind automatically determining the necessary dimensionality of a given problem. Using the proposed procedure, it is possible to identify weak interactions and eliminate them. Finally, the procedure works for arbitrary bilinear programs and is a generalization of a method we have previously introduced (Petrik & Zilberstein, 2007a).\nThe dimensionality is inherently part of the model, not the problem itself. There may be equivalent models of a given problem with very different dimensionality. Thus, procedures for reducing the dimensionality are not necessary when the modeler can create a model with minimal dimensionality. However, this is nontrivial in many cases. In addition, some dimensions may have little impact on the overall performance. To determine which ones can be discarded, we need a measure of their contribution that can be computed efficiently. We define these notions more formally later in this section.\nWe assume that the feasible sets have bounded L2 norms, and assume a general formulation of the bilinear program, not necessarily in the semi-compact form. Given Assumption 7, this can be achieved by scaling the constraints when the feasible region is bounded.\nAssumption 17. For all x \u2208 X and y \u2208 Y , their norms satisfy \u2016x\u20162 \u2264 1 and \u2016y\u20162 \u2264 1.\nWe discuss the implications of and problems with this assumption after presenting Theorem 18. Intuitively, the dimensionality reduction removes those dimensions where g(y) is constant, or almost constant. Interestingly, these dimensions may be recovered based on the eigenvectors and eigenvalues of CTC. We use the eigenvectors of CTC instead of the eigenvectors of C, because our analysis is based on L2 norm of x and y and thus of C. The L2 norm \u2016C\u20162 is bounded by the largest eigenvalue of CTC. In addition, a symmetric matrix is required to ensure that the eigenvectors are perpendicular and span the whole space.\nGiven a problem represented using Eq. (1), let F be a matrix whose columns are all the eigenvectors of CTC with eigenvalues greater than some \u03bb\u0304. Let G be a matrix with all the remaining eigenvectors as columns. Notice that together, the columns of the matrices span the whole space and are real-valued, since CTC is a symmetric matrix. Assume without loss of generality that the eigenvectors are unitary. The compressed version of the bilinear program is then the following:\nmaximize w,x,y1,y2,z f\u0303(w, x, y1, y2, z) = rT1 x+ s T 2w + x\nTCFy1 + rT2 ( F G )(y1 y2 ) + sT2 z\nsubject to A1x+B1w = b\nA2 ( F G )(y1 y2 ) +B2z = b2 w, x, y1, y2, z \u2265 0\n(21)\nNotice that the program is missing the element xTCGy2, which would make its optimal solutions identical to the optimal solutions of Eq. (1). We describe a more practical approach to reducing the dimensionality in Appendix B. This approach is based on singular value decomposition and may be directly applied to any bilinear program. The following theorem quantifies the maximum error when using the compressed program.\nTheorem 18. Let f\u2217 and f\u0303\u2217 be optimal solutions of Eq. (1) and Eq. (21) respectively. Then:\n= |f\u2217 \u2212 f\u0303\u2217| \u2264 \u221a \u03bb\u0304.\nMoreover, this is the maximal linear dimensionality reduction possible with this error without considering the constraint structure. Proof. We first show that indeed the error is at most \u221a \u03bb\u0304 and that any linearly compressed problem with the given error has at least f dimensions. Using a mapping that preserves the feasibility of both programs, the error is bounded by:\n\u2264 \u2223\u2223\u2223\u2223f (w, x, (F G)(y1y2 ) , z ) \u2212 f\u0303 ( w, x, y1, ( y2 z ))\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223xTCGy2\u2223\u2223\u2223 . Denote the feasible region of y2 as Y2. From the orthogonality of [F,G], we have that \u2016y2\u20162 \u2264 1 as follows:\ny = ( F G )(y1 y2 ) ( FT\nGT\n) y = ( y1 y2 ) GTy = y2 \u2016GTy\u20162 = \u2016y2\u20162\nThen we have:\n\u2264 max y2\u2208Y2 max x\u2208X \u2223\u2223\u2223xTCGy2\u2223\u2223\u2223 \u2264 max y2\u2208Y2 \u2016CGy2\u20162\n\u2264 max y2\u2208Y2\n\u221a yT2 G\nTCTCGy2 \u2264 max y2\u2208Y2\n\u221a yT2 Ly2 \u2264 \u221a \u03bb\u0304\nThe result follows from Cauchy-Schwartz inequality, the fact that CTC is symmetric, and Assumption 17. The matrix L denotes a diagonal matrix of eigenvalues corresponding to eigenvectors of G.\nNow, let H be an arbitrary matrix that satisfies the preceding error inequality for G. Clearly, H \u2229 F = \u2205, otherwise \u2203y, \u2016y\u20162 = 1, such that \u2016CHy\u20162 > . Therefore, we have |H| \u2264 n\u2212 |F | \u2264 |G|, because |H|+ |F | = |Y |. Here | \u00b7 | denotes the number of columns of the matrix.\nAlternatively, the bound can be proved by replacing the equality A1x + B1w = b1 by \u2016x\u20162 = 1. The bound can then be obtained by Lagrange necessary optimality conditions. In these bounds we use L2-norm; an extension to a different norm is not straightforward. Note\nalso that this dimensionality reduction technique ignores the constraint structure. When the constraints have some special structure, it might be possible to obtain an even tighter bound. As described in the next section, the dimensionality reduction technique generalizes the reduction that Becker et al. (2004) used implicitly.\nThe result of Theorem 18 is based on an approximation of the feasible set Y by \u2016y\u20162 \u2264 1, as Assumption 17 states. This approximation may be quite loose in some problems, which may lead to a significant multiplicative overestimation of the bound in Theorem 18. For example, consider the feasible set depicted in Figure 7. The bound may be achieved in a point y\u0302, which is far from the feasible region. In specific problems, a tighter bound could be obtained by either appropriately scaling the constraints, or using a weighted L2 with a better precision. We partially address this issue by considering the structure of the constraints. To derive this, consider the following linear program and corresponding theorem:\nmaximize x\ncTx\nsubject to Ax = b x \u2265 0 (22)\nTheorem 19. The optimal solution of Eq. (22) is the same as when the objective function is modified to\ncT(I \u2212AT(AAT)\u22121A)x,\nwhere I is the identity matrix.\nProof. The objective function is:\nmax {x Ax=b, x\u22650}\ncTx =\n= max {x Ax=b, x\u22650}\ncT(I \u2212AT(AAT)\u22121A)x+ cTAT(AAT)\u22121Ax\n= cTAT(AAT)\u22121b+ max {x Ax=b, x\u22650} cT(I \u2212AT(AAT)\u22121A)x.\nThe first term may be ignored because it does not depend on the solution x.\nThe following corollary shows how the above theorem can be used to strengthen the dimensionality reduction bound. For example, in zero-sum games, this stronger dimensionality reduction splits the bilinear program into two linear programs.\nCorollary 20. Assume that there are no variables w and z in Eq. (1). Let:\nQi = (I \u2212ATi (AiATi )\u22121Ai)), i \u2208 {1, 2},\nwhere Ai are defined in Eq. (1). Let C\u0303 be:\nC\u0303 = Q1CQ2,\nwhere C is the bilinear-term matrix from Eq. (1). Then the bilinear programs will have identical optimal solutions with either C or C\u0303.\nProof. Using Theorem 19, we can modify the original objective function in Eq. (1) to:\nf(x, y) = rT1 x+ x T(I \u2212AT1 (A1AT1 )\u22121A1))C(I \u2212AT2 (A2AT2 )\u22121A2))y + rT2 y.\nFor the sake of simplicity we ignore the variables w and z, which do not influence the bilinear term. Because both (I \u2212 ATi (AiATi )\u22121Ai) for i = 1, 2 are orthogonal projection matrices, none of the eigenvalues in Theorem 18 will increase.\nThe dimensionality reduction presented in this section is related to the idea of compound events used in CSA. Allen, Petrik, and Zilberstein (2008a, 2008b) provide a detailed discussion of this issue."}, {"heading": "5. Offline Bound", "text": "In this section we develop an approximation bound that depends only on the number of points for which g(y) is evaluated and the structure of the problem. This kind of bound is useful in practice because it provides performance guarantees without actually solving the problem. In addition, the bound reveals which parameters of the problem influence the algorithm\u2019s performance. The bound is derived based on the maximal slope of g(y) and the maximal distance among the points.\nTheorem 21. To achieve an approximation error of at most , the number of points to be evaluated in a regular grid with k points in every dimension must satisfy:\nkn \u2265 ( \u2016C\u20162 \u221a n n ) ,\nwhere n is the number of dimensions of Y .\nThe theorem follows using basic algebraic manipulations from the following lemma.\nLemma 22. Assume that for each y1 \u2208 Y there exists y2 \u2208 Y such that \u2016y1\u2212 y2\u20162 \u2264 \u03b4 and g\u0303(y2) = g(y2). Then the maximal approximation error is:\n= max y\u2208Y g(y)\u2212 g\u0303(y) \u2264 \u2016C\u20162\u03b4.\nProof. Let y1 be a point where the maximal error is attained. This point is in Y , because this set is compact. Now, let y2 be the closest point to y1 in L2 norm. Let x1 and x2 be the best responses for y1 and y2 respectively. From the definition of solution optimality we can derive:\nrT1 x1 + r T 2 y2 + x T 1Cy2 \u2264 rT1 x2 + rT2 y2 + xT2Cy2\nrT1 (x1 \u2212 x2) \u2264 \u2212(x1 \u2212 x2)TCy2.\nThe error now can be expressed, using the fact that \u2016x1 \u2212 x2\u20162 \u2264 1, as:\n= rT1 x1 + r T 2 y1 + x T 1Cy1 \u2212 rT1 x2 \u2212 rT2 y1 \u2212 xT2Cy1\n= rT1 (x1 \u2212 x2) + (x1 \u2212 x2)TCy1 \u2264 \u2212(x1 \u2212 x2)TCy2 + (x1 \u2212 x2)TCy1 \u2264 (x1 \u2212 x2)TC(y1 \u2212 y2) \u2264 \u2016y1 \u2212 y2\u20162 (x1 \u2212 x2)T\n\u2016(x1 \u2212 x2)\u20162 C (y1 \u2212 y2) \u2016y1 \u2212 y2\u20162\n\u2264 \u2016y1 \u2212 y2\u20162 max {x \u2016x\u20162\u22641} max {y \u2016y\u20162\u22641} xTCy \u2264 \u03b4\u2016C\u20162\nThe above derivation follows from Assumption 17, and the bound reduces to the matrix norm using Cauchy-Schwartz inequality.\nNot surprisingly, the bound is independent of the local rewards and transition structure of the agents. Thus it in fact shows that the complexity of achieving a fixed approximation with a fixed interaction structure is linear in the problem size. However, the bounds are still exponential in the dimensionality of the space. Notice also that the bound is additive."}, {"heading": "6. Experimental Results", "text": "We now turn to an empirical analysis of the performance of the algorithm. For this purpose we use the Mars rover problem described earlier. We compared our algorithm with the original CSA and with a mixed integer linear program (MILP), derived for Eq. (1) as Petrik and Zilberstein (2007b) describe. Although Eq. (1) can also be modeled as a linear complementarity problem (LCP) (Murty, 1988; Cottle et al., 1992), we do not evaluate that option experimentally because LCPs are closely related to MILPs (Rosen, 1986). We expect these two formulations to exhibit similar performance. We also do not compare to any of the methods described by Horst and Tuy (1996) and Bennett and Mangasarian (1992) due to their very different nature and high complexity, and because some of these algorithms do not provide any optimality guarantees.\nIn our experiments, we applied the algorithm to randomly generated problem instances with the same parameters that Becker et al. (2003, 2004) used. Each problem instance includes 2 rovers and 6 sites. At each site, the rovers can decide to perform an experiment or to skip the site. Performing experiments takes some time, and all the experiments must be performed in 15 time units. The time required to perform an experiment is drawn from a discrete normal distribution with the mean uniformly chosen from 4.0-6.0. The variance\nAlgorithm 5: MPBP: Multiagent Planning with Bilinear Programming Formulate DEC-MDP M as a bilinear program B ; // [Section 2.1]1 B\u2032 \u2190 ReduceDimensionality(B) with \u2264 10\u22124 ; // [Section 4, Appendix B]2 Convert B\u2032 to a semi-compact form ; // [Definition 2]3 h\u2190 \u2212\u221e ;4 // Presolve step: run Algorithm 1 \u03b8 times with random initialization for i \u2208 {1 . . . \u03b8} do5 h\u2190 max{h, IterativeBestResponse(B\u2032)} ; // [Algorithm 1]6 BestResponseApprox(B\u2032, 0) ; // [Algorithm 2]7\nis 0.4 of the mean. The local reward for performing an experiment is selected uniformly from the interval [0.1,1.0] for each site and it is identical for both rovers. The global reward, received when both rovers perform an experiment on a shared site, is super-additive and is 1/2 of the local reward. The experiments were performed with sites {1, 2, 3, 4, 5} as shared sites. Typically, the performance of the algorithm degrades with the number of shared sites. Because the problem with fewer than 5 shared sites\u2013as used in the original CSA paper\u2013were too easy to solve, we only present results for problems with 5 shared sites. Note that CSA was used on this problem with an implicit dimensionality reduction due to the use of the compound events.\nIn these experiments, the naive dimensionality of Y in Eq. (5) is 6 \u2217 15 \u2217 2 = 180. This dimensionality can be reduced to be one per each shared site using the automatic dimensionality reduction procedure. Each dimension then represents the probability that an experiment on a shared site is performed regardless of the time. Therefore, the dimension represents the sum of the individual probabilities. Becker et al. (2004) achieved the same compression using compound events, where each compound event represents the fact that an experiment is performed on some site regardless of the specific time.\nThe complete algorithm\u2013Multiagent Planning with Bilinear Programming (MPBP)\u2013is summarized in Algorithm 5. The automatic dimensionality reduction reduces Y to 5 dimensions. Then, reformulating the problem to a semi-compact form increases the dimensionality to 6. We experimented with different configurations of the algorithm that differ in the way the refinements of the pivot point selection is performed. The different methods, described in Section 3.3, were used to create six configurations as shown in Figure 8. The configuration C1 corresponds to an earlier version of the algorithm (Petrik & Zilberstein, 2007a).\nWe executed the algorithm 20 times with each configuration on every problem, randomly generated according to the distribution described above. The results represent the average over the random instances. The maximum number of iterations of the algorithm was 200. Due to rounding errors, we considered any error less than 10\u22124 to be 0. The algorithm is implemented in MATLAB release 2007a. The linear solver we used is MOSEK version 5.0. The hardware configuration was Intel Core 2 Duo 1.6 GHz Low Voltage with 2GB RAM. The time to perform the dimensionality reduction is negligible and not included in the result.\nA direct comparison with CSA was not possible because CSA cannot solve problems with this dimensionality within a reasonable amount of time. However, in a very similar\nproblem setup with at most 4 shared sites, CSA solved only 76% of the problems, and the longest solution took approximately 4 hours (Becker et al., 2004). In contrast, MPBP solved all 200 problems with 4 shared sites optimally in less than 1 second on average, about 10000 times faster. In addition, MPBP returns solutions that are guaranteed to be close to optimal in the first few iterations. While CSA also returns solutions close to optimal very rapidly, it takes a very long time to confirm that.\nFigure 9 shows the average guaranteed ratio of the optimal solution, achieved as a function of the number of iterations, that is, points for which g(y) is evaluated. This figure, as all others, shows the result of the online error bound. This value is guaranteed and is not based on the optimal solution. This compares the performance of the various configurations of the algorithm, without using the presolve step. While the optimal solution was typically discovered in the first few iterations, it takes significantly longer to prove its optimality.\nThe average of absolute errors in both linear and log scale are shown in Figure 10. These results indicate that the methods proposed to eliminate the dominated region in searching for the pivot point can dramatically improve performance. While requiring that the new pivot points are feasible in Y improves the performance, it is much more significant with\na better approximation of Yh. As expected, the cutting plane elimination is most efficient, but also most complex.\nTo evaluate the tradeoffs in the implementation, we also show the average time per iteration and the average total time in Figure 11. These figures show that the time per iteration is significantly larger when the cutting plane elimination is used. Overall, the algorithm is faster when the simpler linear bound is used.\nThis trend is most likely problem specific. In problems with higher dimensionality, the more precise cutting plane algorithm may be more efficient. Implementation issues play a significant role in this problem too, and it is likely that the implementation of Algorithm 4 can be further improved.\nFigure 12 shows the influence of using the presolve method. The plots of C3 and C4 are identical to the plots of C5 and C6 respectively, indicating that the presolve method does not have any significant influence. This also indicates that a solution that is very close to optimal is obtained when the values of the initial points are calculated.\nWe also performed experiments with CPLEX\u2013a state-of-the-art MILP solver on the direct MILP formulation of the DEC-MDP. CPLEX was not able to solve any of the problems within 30 minutes, no matter how many of the sites were shared. The main reason for this is that it does not take any advantage of the limited interaction. Nevertheless, it is possible that some specialized MILP solvers may perform better."}, {"heading": "7. Conclusion and Further Work", "text": "We present an algorithm that significantly improves the state-of-the-art in solving two-agent coordination problems. The algorithm takes as input a bilinear program representing the problem, and solves the problem using a new successive approximation method. It provides a useful online performance bound that can be used to decide when the approximation is good enough. The algorithm can take advantage of the limited interaction among the agents, which is translated into a small dimensionality of the bilinear program. Moreover, using our approach, it is possible to reduce the dimensionality of such problems automatically, without extensive modeling effort. This makes it easy to apply our new method in practice. When applied to DEC-MDPs, the algorithm is much faster than the existing CSA method, on average reducing computation time by four orders of magnitude. We also show that a variety of other coordination problems can be treated within this framework.\nBesides multiagent coordination problems, bilinear programs have been previously used to solve problems in operations research and global optimization (Sherali & Shetty, 1980; White, 1992; Gabriel, Garca-Bertrand, Sahakij, & Conejo, 2005). Global optimization deals with finding the optimal solutions to problems with multi-extremal objective function. Solution techniques often share the same idea and are based on cutting plane methods. The main idea is to iteratively restrict the set of feasible solutions, while improving the incumbent\nsolution. Horst and Tuy (1996) provide an excellent overview of these techniques. These algorithms have different characteristics and cannot be directly compared to the algorithm we developed. Unlike these traditional algorithms, we focus on providing quickly good approximate solutions with error bounds. In addition, we exploit the small dimensionality of the best-response space Y to get tight approximation bounds.\nFuture work will address several interesting open questions with respect to the bilinear formulation as well as further improvement of the efficiency of the algorithm. With regard to the representation, it is yet to be determined whether the anytime behavior can be exploited when applied to games. That is, it is necessary to verify that an approximate solution to the bilinear program is also a meaningful approximation of the Nash equilibrium. It is also important to identify the classes of extensive games that can be efficiently formulated as bilinear programs.\nThe algorithm we present can be made more efficient in several ways. In particular, a significant speedup could be achieved by reducing the size of the individual linear programs. The programs are solved many times with the same constraints, but a different objective function. The objective function is always from a small-dimensional space. Therefore, the problems that are solved are all very similar. In the DEC-MDP domain, one option would be to use a procedure similar to action elimination. In addition, the performance could be significantly improved by starting with a tight initial triangulation. In our implementation, we simply use a single large polyhedron that covers the whole feasible region. A better approach would be to start with something that approximates the feasible region more tightly. A tighter approximation of the feasible region could also improve the precision of the dimensionality reduction procedure. Instead of the naive ellipsis used in Assumption 7, it is possible to use one that approximates the feasible region as tightly as possible. It is however very encouraging to see that even without these improvements, the algorithm is very effective compared with existing solution techniques."}, {"heading": "Acknowledgments", "text": "We thank Chris Amato, Raghav Aras, Alan Carlin, Hala Mostafa, and the anonymous reviewers for useful comments and suggestions. This work was supported in part by the Air Force Office of Scientific Research under Grants No. FA9550-05-1-0254 and FA955008-1-0181, and by the National Science Foundation under Grants No. IIS-0535061 and IIS-0812149."}, {"heading": "Appendix A. Proofs", "text": "Proof of Proposition 10 The proposition states that in the proposed triangulation, the sub-polyhedra do not overlap and they cover the whole feasible set Y , given that the pivot point is in the interior of S.\nProof. We prove the theorem by induction on the number of polyhedron splits that were performed. The base case is trivial: there is only a single polyhedron, which covers the whole feasible region.\nFor the inductive case, we show that for any polyhedron S the sub-polyhedra induced by the pivot point y\u0302 cover S and do not overlap. The notation we use is the following: T\ndenotes the original polyhedron and y\u0302 = Tc is the pivot point, where 1Tc = 1 and c \u2265 0. Note that T is a matrix and c, d, y\u0302 are vectors, and \u03b2 is a scalar.\nWe show that the sub-polyhedra cover the original polyhedron S as follows. Take any a = Td such that 1Td = 1 and d \u2265 0. We show that there exists a sub-polyhedron that contains a and has y\u0302 as a vertex. First, let\nT\u0302 = ( T 1T ) This matrix is square and invertible, since the polyhedron is non-empty. To get a representation of a that contains y\u0302, we show that there is a vector o such that for some i, o(i) = 0: (\na 1\n) = T\u0302 d = T\u0302 o+ ( \u03b2y\u0302 )\no \u2265 0,\nfor some \u03b2 > 0. This will ensure that a is in the sub-polyhedron with y\u0302 with vertex i replaced by y\u0302. The value o depends on \u03b2 as follows:\no = d\u2212 \u03b2T\u0302\u22121 ( y\u0302 1 ) .\nThis can be achieved by setting:\n\u03b2 = min i d(i) (T\u0302\u22121y\u0302)(i) .\nSince both d and c = T\u0302\u22121y\u0302 are non-negative. This leaves us with an equation for the sub-polyhedron containing the point a. Notice that the resulting polyhedron may be of a smaller dimension than n when o(j) = 0 for some i 6= j.\nTo show that the polyhedra do not overlap, assume there exists a point a that is common to the interior of at least two of the polyhedra. That is, assume that a is a convex combination of the vertices:\na = T3c1 + h1y\u0302 + \u03b21y1 a = T3c2 + h2y\u0302 + \u03b22y2,\nwhere T3 represents the set of points common to the two polyhedra, and y1 and y2 represent the disjoint points in the two polyhedra. The values h1, h2, \u03b21, and \u03b22 are all scalars, while c1 and c2 are vectors. Notice that the sub-polyhedra differ by at most one vertex. The coefficients satisfy:\nc1 \u2265 0 c2 \u2265 0 h1 \u2265 0 h2 \u2265 0 \u03b21 \u2265 0 \u03b22 \u2265 0\n1Tc1 + h1 + \u03b21 = 1 1Tc2 + h2 + \u03b22 = 1\nSince the interior of the polyhedron is non-empty, this convex combination is unique. First assume that h = h1 = h2. Then we can show the following:\na = T3c1 + hy\u0302 + \u03b21y1 = T3c2 + hy\u0302 + \u03b22y2 T3c1 + \u03b21y1 = T3c2 + \u03b22y2\n\u03b21y1 = \u03b22y2 \u03b21 = \u03b22 = 0\nThis holds since y1 and y2 are independent of T3 when the polyhedron is nonempty and y1 6= y2. The last equality follows from the fact that y1 and y2 are linearly independent. This is a contradiction, since \u03b21 = \u03b22 = 0 implies that the point a is not in the interior of two polyhedra, but at their intersection.\nFinally, assume WLOG that h1 > h2. Now let y\u0302 = T3c\u0302+ \u03b11y1 + \u03b12y2, for some scalars \u03b11 \u2265 0 and \u03b12 \u2265 0 that represent a convex combination. We get:\na = T3c1 + h1y\u0302 + \u03b21y1 = T3(c1 + h1c\u0302) + (h1\u03b11 + \u03b21)y1 + h1\u03b12y2 a = T3c2 + h2y\u0302 + \u03b22y2 = T3(c2 + h2c\u0302) + h2\u03b11y1 + (h2\u03b12 + \u03b22)y2.\nThe coefficients sum to one as shown below.\n1T(c1 + h1c\u0302) + (h1\u03b11 + \u03b21) + h1\u03b12 = 1Tc1 + \u03b21 + h1(1Tc\u0302+ \u03b11 + \u03b12) = 1Tc1 + \u03b21 + h1 = 1\n1T(c2 + h2c\u0302) + \u03b11 + (h2\u03b12 + \u03b22) = 1Tc2 + \u03b22 + h2(1Tc\u0302+ \u03b11 + \u03b12) = 1Tc2 + \u03b22 + h2 = 1\nNow, the convex combination is unique, and therefore the coefficients associated with each vertex for the two representations of a must be identical. In particular, equating the coefficients for y1 and y2 results in the following:\nh1\u03b11 + \u03b21 = h2\u03b11 h1\u03b12 = h2\u03b12 + \u03b22 \u03b21 = h2\u03b11 \u2212 h1\u03b11 \u03b22 = h1\u03b12 \u2212 h2\u03b12 \u03b21 = \u03b11(h2 \u2212 h1) > 0 \u03b22 = \u03b12(h1 \u2212 h2) < 0\nWe have that \u03b11 > 0 and \u03b12 > 0 from the fact that y\u0302 is in the interior of the polyhedron S. Then, having \u03b22 \u2264 0 is a contradiction with a being a convex combination of the vertices of S."}, {"heading": "Appendix B. Practical Dimensionality Reduction", "text": "In this section we describe an approach to dimensionality reduction that is easy to implement. Note that there are at least two possible approaches to take advantage of reduced dimensionality. First, it is possible to use the dimensionality information to limit the algorithm to work only in the significant dimensions of Y . Second, it is possible to modify the bilinear program to have a small dimensionality. While changing the algorithm may be more straightforward, it limits the use of the advanced pivot point selection methods described in Section 3.3. Here, we show how to implement the second option in a straightforward way using singular value decomposition.\nThe dimensionality reduction is applied to the following bilinear program:\nmaximize w,x,y,z rT1 x+ s T 1w + x TCy + rT2 y + s T 2 z subject to A1x+B1w = b1 A2y +B2z = b2 w, x, y, z \u2265 0\n(23)\nLet C = SV TT be a singular value decomposition. Let T = [T1, T2], such that the singular value of vectors ti in T2 is less than the required . Then, a bilinear program with reduced dimensionality may be defined as follows:\nmaximize w,x,y\u0304,y,z rT1 x+ s T 1w + x TSV T1y\u0304 + rT2 y + s T 2 z subject to T1y\u0304 = y\nA1x+B1w = b1 A2y +B2z = b2 w, x, y, z \u2265 0\n(24)\nNote that y\u0304 is not constrained to be non-negative. One problematic aspect of reducing the dimensionality is how to define the initial polyhedron that needs to encompass all feasible solutions. One option is to make it large enough to contain the set {y \u2016y\u20162 = 1}, but this may be too large. Often in practice, it may be more efficient to first triangulate a rough approximation of the feasible region, and then execute the algorithm on this triangulation."}, {"heading": "Appendix C. Sum of Convex and Concave Functions", "text": "In this section we show that the best-response function g(y) may not be convex when the program is not in a semi-compact form. The convexity of the best-response function is crucial in bounding the approximation error and in eliminating the dominated regions.\nWe show that when the program is not in a semi-compact form, the best-response function can we written as a sum of a convex function and a concave function. To show that consider the following bilinear program.\nmaximize w,x,y,z f = rT1 x+ s T 1w + x TCy + rT2 y + s T 2 z subject to A1x+B1w = b1 A2y +B2z = b2 w, x, y, z \u2265 0\n(25)\nThis problem may be reformulated as:\nf = max {y,z (y,z)\u2208Y } max {x,w (x,w)\u2208X} rT1 x+ s T 1w + x TCy + rT2 y + s T 2 z\n= max {y,z (y,z)\u2208Y }\ng\u2032(y) + sT2 z,\nwhere\ng\u2032(y) = max {x,w (x,w)\u2208X} rT1 x+ s T 1w + x TCy + rT2 y.\nNotice that function g\u2032(y) is convex, because it is a maximum of a set of linear functions. Since f = max{y (y,z)\u2208Y } g(y), the best-response function g(y) can be expressed as:\ng(y) = max {z (y,z)\u2208Y }\ng\u2032(y) + sT2 z = g \u2032(y) + max {z (y,z)\u2208Y } sT2 z\n= g\u2032(y) + t(y),\nwhere\nt(y) = max {z A2y+B2z=b2, y,z\u22650} sT2 z.\nFunction g\u2032(y) does not depend on z, and therefore could be taken out of the maximization. The function t(y) corresponds to a linear program, and its dual using the variable q is:\nminimize q (b2 \u2212A2y)Tq subject to BT2 q \u2265 s2 (26)\nTherefore:\nt(y) = min {q BT2 q\u2265s2} (b2 \u2212A2y)Tq,\nwhich is a concave function, because it is a minimum of a set of linear functions. The best-response function can now be written as:\ng(y) = g\u2032(y) + t(y),\nwhich is a sum of a convex function and a concave function, also known as a d.c. function (Horst & Tuy, 1996). Using this property, it is easy to construct a program such that g(y) will be convex on one part of Y and concave on another part of Y , as the following example shows. Note that in semi-compact bilinear programs t(y) = 0, which guarantees the convexity of g(y).\nExample 23. Consider the following bilinear program:\nmaximize x,y,z \u2212x+ xy \u2212 2z subject to \u22121 \u2264 x \u2264 1 y \u2212 z \u2264 2 z \u2265 0\n(27)\nA plot of the best response function for this program is shown in Figure 13."}], "references": [{"title": "Interaction structure and dimensionality in decentralized problem solving", "author": ["M. Allen", "M. Petrik", "S. Zilberstein"], "venue": "In Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Allen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Allen et al\\.", "year": 2008}, {"title": "Interaction structure and dimensionality in decentralized problem solving", "author": ["M. Allen", "M. Petrik", "S. Zilberstein"], "venue": "Tech. rep. 08-11,", "citeRegEx": "Allen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Allen et al\\.", "year": 2008}, {"title": "Mathematical programming methods for decentralized POMDPs", "author": ["R. Aras"], "venue": "Ph.D. thesis,", "citeRegEx": "Aras,? \\Q2008\\E", "shortCiteRegEx": "Aras", "year": 2008}, {"title": "A mixed integer linear programming method for the finitehorizon Dec-POMDP problem", "author": ["R. Aras", "F. Charpillet"], "venue": "In International Conference on Automated Planning and Scheduling (ICAPS),", "citeRegEx": "Aras and Charpillet,? \\Q2007\\E", "shortCiteRegEx": "Aras and Charpillet", "year": 2007}, {"title": "Exploiting Structure in Decentralized Markov Decision Processes", "author": ["R. Becker"], "venue": "Ph.D. thesis,", "citeRegEx": "Becker,? \\Q2006\\E", "shortCiteRegEx": "Becker", "year": 2006}, {"title": "Decentralized Markov decision processes with event-driven interactions", "author": ["R. Becker", "V. Lesser", "S. Zilberstein"], "venue": "In International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS),", "citeRegEx": "Becker et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2004}, {"title": "Transition-independent decentralized Markov decision processes", "author": ["R. Becker", "S. Zilberstein", "V. Lesser"], "venue": "In International Joint Conference on Autonomous Agents and Multi Agent Systems (AAMAS),", "citeRegEx": "Becker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2003}, {"title": "Solving transition independent decentralized Markov decision processes", "author": ["R. Becker", "S. Zilberstein", "V. Lesser", "C.V. Goldman"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Becker et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2004}, {"title": "Bilinear separation of two sets in n-space", "author": ["K.P. Bennett", "O.L. Mangasarian"], "venue": "Tech. rep.,", "citeRegEx": "Bennett and Mangasarian,? \\Q1992\\E", "shortCiteRegEx": "Bennett and Mangasarian", "year": 1992}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["D.S. Bernstein", "S. Zilberstein", "N. Immerman"], "venue": "In Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Bernstein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2000}, {"title": "Sequential optimality and coordination in multiagent systems", "author": ["C. Boutilier"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Boutilier,? \\Q1999\\E", "shortCiteRegEx": "Boutilier", "year": 1999}, {"title": "Increased fexibility and robustness of Mars rovers", "author": ["J.L. Bresina", "K. Golden", "D.E. Smith", "R. Washington"], "venue": "In International Symposium on AI, Robotics, and Automation in Space,", "citeRegEx": "Bresina et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Bresina et al\\.", "year": 1999}, {"title": "Algorithms for Partially Observable Markov Decision Processes", "author": ["H.T. Cheng"], "venue": "Ph.D. thesis,", "citeRegEx": "Cheng,? \\Q1988\\E", "shortCiteRegEx": "Cheng", "year": 1988}, {"title": "The Linear Complementarity Problem", "author": ["R.W. Cottle", "Pang", "J.-S", "R.E. Stone"], "venue": null, "citeRegEx": "Cottle et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Cottle et al\\.", "year": 1992}, {"title": "Approximate solutions for partially observable stochastic games with common payoffs", "author": ["R. Emery-Montemerlo", "G. Gordon", "J. Schneider", "S. Thrun"], "venue": "In International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Emery.Montemerlo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Emery.Montemerlo et al\\.", "year": 2004}, {"title": "A practical approach to approximate bilinear functions in mathematical programming problems by using Schurs decomposition and SOS type 2 variables", "author": ["S.A. Gabriel", "R. Garca-Bertrand", "P. Sahakij", "A.J. Conejo"], "venue": "Journal of the Operational Research Society,", "citeRegEx": "Gabriel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gabriel et al\\.", "year": 2005}, {"title": "Communication-based decomposition mechanisms for decentralized MDPs", "author": ["C.V. Goldman", "S. Zilberstein"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Goldman and Zilberstein,? \\Q2008\\E", "shortCiteRegEx": "Goldman and Zilberstein", "year": 2008}, {"title": "Global optimization: Deterministic approaches", "author": ["R. Horst", "H. Tuy"], "venue": null, "citeRegEx": "Horst and Tuy,? \\Q1996\\E", "shortCiteRegEx": "Horst and Tuy", "year": 1996}, {"title": "Exploiting locality of interaction in networked distributed POMDPs", "author": ["Y. Kim", "R. Nair", "P. Varakantham", "M. Tambe", "M. Yokoo"], "venue": "In AAAI Spring Symposium on Distributed Planning and Scheduling,", "citeRegEx": "Kim et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2006}, {"title": "Fast algorithms for finding randomized strategies in game trees", "author": ["D. Koller", "N. Megiddo", "B. von Stengel"], "venue": "In ACM Symposium on the Theory of Computing,", "citeRegEx": "Koller et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Koller et al\\.", "year": 1994}, {"title": "The linear complementarity problem as a separable bilinear program", "author": ["O.L. Mangasarian"], "venue": "Journal of Global Optimization,", "citeRegEx": "Mangasarian,? \\Q1995\\E", "shortCiteRegEx": "Mangasarian", "year": 1995}, {"title": "Linear complementarity, linear and nonlinear programming. Helderman-Verlag", "author": ["K.G. Murty"], "venue": null, "citeRegEx": "Murty,? \\Q1988\\E", "shortCiteRegEx": "Murty", "year": 1988}, {"title": "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings", "author": ["R. Nair", "M. Tambe", "M. Yokoo", "D. Pynadath", "S. Marsella"], "venue": "In International Joint Conference on Artificial Inteligence,", "citeRegEx": "Nair et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2003}, {"title": "Communication for improving policy computation in distributed pomdps", "author": ["R. Nair", "M. Roth", "M. Yokoo", "M. Tambe"], "venue": "In International Joint Conference on Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Nair et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2004}, {"title": "Decentralized control of a multiple access broadcast channel: performance bounds", "author": ["J.M. Ooi", "G.W. Wornell"], "venue": "In Proceeding of the IEEE Conference on Decision and Control,", "citeRegEx": "Ooi and Wornell,? \\Q1996\\E", "shortCiteRegEx": "Ooi and Wornell", "year": 1996}, {"title": "A course in game theory", "author": ["M.J. Osborne", "A. Rubinstein"], "venue": null, "citeRegEx": "Osborne and Rubinstein,? \\Q1994\\E", "shortCiteRegEx": "Osborne and Rubinstein", "year": 1994}, {"title": "A complementarity approach to a quasistatic rigid body motion problem", "author": ["Pang", "J.-S", "J.C. Trinkle", "G. Lo"], "venue": "Journal of Computational Optimization and Applications,", "citeRegEx": "Pang et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Pang et al\\.", "year": 1996}, {"title": "Anytime coordination using separable bilinear programs", "author": ["M. Petrik", "S. Zilberstein"], "venue": "In Conference on Artificial Intelligence,", "citeRegEx": "Petrik and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Petrik and Zilberstein", "year": 2007}, {"title": "Average reward decentralized Markov decision processes", "author": ["M. Petrik", "S. Zilberstein"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Petrik and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Petrik and Zilberstein", "year": 2007}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q2005\\E", "shortCiteRegEx": "Puterman", "year": 2005}, {"title": "Optimal decentralized control in a multiaccess channel with partial information", "author": ["Z. Rosberg"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Rosberg,? \\Q1983\\E", "shortCiteRegEx": "Rosberg", "year": 1983}, {"title": "Solution of general LCP by 0-1 mixed integer programming", "author": ["J.B. Rosen"], "venue": "Tech. rep. Computer Science Tech. Report 8623,", "citeRegEx": "Rosen,? \\Q1986\\E", "shortCiteRegEx": "Rosen", "year": 1986}, {"title": "Modeling bounded rationality", "author": ["A. Rubinstein"], "venue": null, "citeRegEx": "Rubinstein,? \\Q1997\\E", "shortCiteRegEx": "Rubinstein", "year": 1997}, {"title": "Memory bounded dynamic programming for DECPOMDPs", "author": ["S. Seuken", "S. Zilberstein"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "Seuken and Zilberstein,? \\Q2007\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2007}, {"title": "Formal models and algorithms for decentralized decision making under uncertainty", "author": ["S. Seuken", "S. Zilberstein"], "venue": "Autonomous Agents and Multiagent Systems,", "citeRegEx": "Seuken and Zilberstein,? \\Q2008\\E", "shortCiteRegEx": "Seuken and Zilberstein", "year": 2008}, {"title": "A finitely convergent algorithm for bilinear programming problems using polar cuts and disjunctive face cuts", "author": ["H.D. Sherali", "C.M. Shetty"], "venue": "Mathematical Programming,", "citeRegEx": "Sherali and Shetty,? \\Q1980\\E", "shortCiteRegEx": "Sherali and Shetty", "year": 1980}, {"title": "Linear Programming: Foundations and Extensions (2nd edition)", "author": ["R.J. Vanderbei"], "venue": null, "citeRegEx": "Vanderbei,? \\Q2001\\E", "shortCiteRegEx": "Vanderbei", "year": 2001}, {"title": "A linear programming approach to solving bilinear programmes", "author": ["D.J. White"], "venue": "Mathematical Programming,", "citeRegEx": "White,? \\Q1992\\E", "shortCiteRegEx": "White", "year": 1992}], "referenceMentions": [{"referenceID": 10, "context": "This results in a multiagent Markov decision process (Boutilier, 1999), which is essentially an MDP with a factored action set.", "startOffset": 53, "endOffset": 70}, {"referenceID": 32, "context": "DEC-POMDPs are closely related to extensive games (Rubinstein, 1997).", "startOffset": 50, "endOffset": 68}, {"referenceID": 6, "context": "In this paper, we introduce an efficient algorithm for several restricted classes, most notably decentralized MDPs with transition and observation independence (Becker et al., 2003).", "startOffset": 160, "endOffset": 181}, {"referenceID": 4, "context": "When the algorithm is applied to DEC-MDPs, it improves efficiency by several orders of magnitude compared with previous state-of the art algorithms (Becker, 2006; Petrik & Zilberstein, 2007a).", "startOffset": 148, "endOffset": 191}, {"referenceID": 20, "context": "1992), and even general linear complementarity problems (Mangasarian, 1995).", "startOffset": 56, "endOffset": 75}, {"referenceID": 36, "context": "The latter formulation can be easily transformed into the normal form using standard transformations of linear programs (Vanderbei, 2001).", "startOffset": 120, "endOffset": 137}, {"referenceID": 5, "context": "1 DEC-MDPs As mentioned previously, any transition-independent and observation-independent DECMDP (Becker et al., 2004) may be formulated as a bilinear program.", "startOffset": 98, "endOffset": 119}, {"referenceID": 4, "context": "One example that we use is the Mars rover planning problem (Bresina, Golden, Smith, & Washington, 1999), first formulated as a DEC-MDP by Becker et al. (2003). This domain involves two autonomous rovers that visit several sites in a given order and may decide to perform certain scientific experiments at each site.", "startOffset": 138, "endOffset": 159}, {"referenceID": 4, "context": "One example that we use is the Mars rover planning problem (Bresina, Golden, Smith, & Washington, 1999), first formulated as a DEC-MDP by Becker et al. (2003). This domain involves two autonomous rovers that visit several sites in a given order and may decide to perform certain scientific experiments at each site. The overall activity must be completed within a given time limit. The uncertainty about the duration of each experiment is modeled by a given discrete distribution. While the rovers operate independently and receive local rewards for each completed experiment, the global reward function also depends on some experiments completed by both rovers. The interaction between the rovers is thus limited to a relatively small number of such overlapping tasks. We return to this problem and describe it in more detail in Section 6. A DEC-MDP problem is composed of two MDPs with state-sets S1, S2 and action sets A1, A2. The functions r1 and r2 define local rewards for action-state pairs. The initial state distributions are \u03b11 and \u03b12. The MDPs are coupled through a global reward function defined by the matrix R. Each entry R(i, j) represents the joint reward for the state-action i by one agent and j by the other. Our definition of a DEC-MDP is based on the work of Becker et al. (2004), with some modifications that we discuss below.", "startOffset": 138, "endOffset": 1301}, {"referenceID": 29, "context": "This property is commonly used when an MDP is formulated as a linear program (Puterman, 2005).", "startOffset": 77, "endOffset": 93}, {"referenceID": 29, "context": "The solution of a DEC-MDP is a deterministic stationary policy \u03c0 = (\u03c01, \u03c02), where \u03c0i : Si 7\u2192 Ai is the standard MDP policy (Puterman, 2005) for agent i.", "startOffset": 124, "endOffset": 140}, {"referenceID": 4, "context": "In particular, this extended reward structure arises from introducing the primitive and compound events in the work of Becker et al. (2004). This reward structure is necessary to capture the characteristics of the Mars rover benchmark.", "startOffset": 119, "endOffset": 140}, {"referenceID": 5, "context": "The correctness of the policy calculation follows from the existence of an optimal policy that is deterministic and depends only on the local states of that agent (Becker et al., 2004).", "startOffset": 163, "endOffset": 184}, {"referenceID": 29, "context": "The matrices A1 and A2 are the same as for the dual formulation of total expected reward MDPs (Puterman, 2005), representing the following equalities for agent i: \u2211", "startOffset": 94, "endOffset": 110}, {"referenceID": 18, "context": "Interestingly, this class of problems has been previously formulated (Kim et al., 2006).", "startOffset": 69, "endOffset": 87}, {"referenceID": 30, "context": "For example, consider the infinitehorizon version of the Multiple Access Broadcast Channel (MABC) (Rosberg, 1983; Ooi & Wornell, 1996).", "startOffset": 98, "endOffset": 134}, {"referenceID": 30, "context": "For example, consider the infinitehorizon version of the Multiple Access Broadcast Channel (MABC) (Rosberg, 1983; Ooi & Wornell, 1996). In this problem, which has been used widely in recent studies of decentralized decision making, two communication devices share a single channel, and they need to periodically transmit some data. However, the channel can transmit only a single message at a time. When both agents send messages at the same time, this leads to a collision, and the transmission fails. The memory of the devices is limited, thus they need to send the messages sooner rather than later. We adapt the model from the work of Rosberg (1983), which is particularly suitable because it assumes no sharing of local information among the devices.", "startOffset": 99, "endOffset": 654}, {"referenceID": 29, "context": "Puterman (2005), for example, provides a more detailed discussion of the definition and meaning of policy gain.", "startOffset": 0, "endOffset": 16}, {"referenceID": 29, "context": "The variables in the program come from the dual formulation of the average-reward MDP linear program (Puterman, 2005).", "startOffset": 101, "endOffset": 117}, {"referenceID": 27, "context": "Petrik and Zilberstein (2007b) provide further details of this formulation.", "startOffset": 0, "endOffset": 31}, {"referenceID": 5, "context": "The approach we develop is closely related to event-driven DECPOMDPs (Becker et al., 2004), but it is in general more efficient.", "startOffset": 69, "endOffset": 90}, {"referenceID": 9, "context": "This can be expected since solving DEC-POMDPs is NEXP-complete (Bernstein et al., 2000), while solving bilinear programs is NP-complete (Mangasarian, 1995).", "startOffset": 63, "endOffset": 87}, {"referenceID": 20, "context": ", 2000), while solving bilinear programs is NP-complete (Mangasarian, 1995).", "startOffset": 56, "endOffset": 75}, {"referenceID": 2, "context": "The approach in this case may be similar to linear complementarity problem formulation of extensive games (Koller, Megiddo, & von Stengel, 1994), and integer linear program formulation of DEC-POMDPs (Aras & Charpillet, 2007). The approach we develop is closely related to event-driven DECPOMDPs (Becker et al., 2004), but it is in general more efficient. Nevertheless, the size of the bilinear program is exponential in the size of the DEC-POMDP. This can be expected since solving DEC-POMDPs is NEXP-complete (Bernstein et al., 2000), while solving bilinear programs is NP-complete (Mangasarian, 1995). Because the general formulation in this case is somewhat cumbersome, we only illustrate it using the following simple example. Aras (2008) provides the details of a similar construction.", "startOffset": 200, "endOffset": 743}, {"referenceID": 20, "context": "It has also been shown that a linear complementarity problem may be formulated as a bilinear problem (Mangasarian, 1995).", "startOffset": 101, "endOffset": 120}, {"referenceID": 36, "context": "The complementary slackness values (Vanderbei, 2001) for the linear programs Eq.", "startOffset": 35, "endOffset": 52}, {"referenceID": 20, "context": "While the algorithm often performs well in practice, it tends to converge to a suboptimal solution (Mangasarian, 1995).", "startOffset": 99, "endOffset": 118}, {"referenceID": 5, "context": "This general approach is also used by the coverage set algorithm (Becker et al., 2004).", "startOffset": 65, "endOffset": 86}, {"referenceID": 5, "context": "That guarantees that g(y) is eventually known precisely (Becker et al., 2004).", "startOffset": 56, "endOffset": 77}, {"referenceID": 12, "context": "A similar approach was also taken for POMDPs (Cheng, 1988).", "startOffset": 45, "endOffset": 58}, {"referenceID": 36, "context": "In general, the simplex algorithm is preferable to interior point methods for this program because of its small size (Vanderbei, 2001).", "startOffset": 117, "endOffset": 134}, {"referenceID": 4, "context": "The maximal difference is actually achieved in points where some of the planes meet, as Becker et al. (2004) have suggested.", "startOffset": 88, "endOffset": 109}, {"referenceID": 4, "context": "As described in the next section, the dimensionality reduction technique generalizes the reduction that Becker et al. (2004) used implicitly.", "startOffset": 104, "endOffset": 125}, {"referenceID": 21, "context": "(1) can also be modeled as a linear complementarity problem (LCP) (Murty, 1988; Cottle et al., 1992), we do not evaluate that option experimentally because LCPs are closely related to MILPs (Rosen, 1986).", "startOffset": 66, "endOffset": 100}, {"referenceID": 13, "context": "(1) can also be modeled as a linear complementarity problem (LCP) (Murty, 1988; Cottle et al., 1992), we do not evaluate that option experimentally because LCPs are closely related to MILPs (Rosen, 1986).", "startOffset": 66, "endOffset": 100}, {"referenceID": 31, "context": ", 1992), we do not evaluate that option experimentally because LCPs are closely related to MILPs (Rosen, 1986).", "startOffset": 97, "endOffset": 110}, {"referenceID": 18, "context": "(1) as Petrik and Zilberstein (2007b) describe.", "startOffset": 7, "endOffset": 38}, {"referenceID": 8, "context": "(1) can also be modeled as a linear complementarity problem (LCP) (Murty, 1988; Cottle et al., 1992), we do not evaluate that option experimentally because LCPs are closely related to MILPs (Rosen, 1986). We expect these two formulations to exhibit similar performance. We also do not compare to any of the methods described by Horst and Tuy (1996) and Bennett and Mangasarian (1992) due to their very different nature and high complexity, and because some of these algorithms do not provide any optimality guarantees.", "startOffset": 80, "endOffset": 349}, {"referenceID": 4, "context": "We also do not compare to any of the methods described by Horst and Tuy (1996) and Bennett and Mangasarian (1992) due to their very different nature and high complexity, and because some of these algorithms do not provide any optimality guarantees.", "startOffset": 83, "endOffset": 114}, {"referenceID": 4, "context": "Becker et al. (2004) achieved the same compression using compound events, where each compound event represents the fact that an experiment is performed on some site regardless of the specific time.", "startOffset": 0, "endOffset": 21}, {"referenceID": 5, "context": "problem setup with at most 4 shared sites, CSA solved only 76% of the problems, and the longest solution took approximately 4 hours (Becker et al., 2004).", "startOffset": 132, "endOffset": 153}, {"referenceID": 37, "context": "Besides multiagent coordination problems, bilinear programs have been previously used to solve problems in operations research and global optimization (Sherali & Shetty, 1980; White, 1992; Gabriel, Garca-Bertrand, Sahakij, & Conejo, 2005).", "startOffset": 151, "endOffset": 238}, {"referenceID": 17, "context": "Horst and Tuy (1996) provide an excellent overview of these techniques.", "startOffset": 0, "endOffset": 21}], "year": 2009, "abstractText": "Multiagent planning and coordination problems are common and known to be computationally hard. We show that a wide range of two-agent problems can be formulated as bilinear programs. We present a successive approximation algorithm that significantly outperforms the coverage set algorithm, which is the state-of-the-art method for this class of multiagent problems. Because the algorithm is formulated for bilinear programs, it is more general and simpler to implement. The new algorithm can be terminated at any time and\u2013unlike the coverage set algorithm\u2013it facilitates the derivation of a useful online performance bound. It is also much more efficient, on average reducing the computation time of the optimal solution by about four orders of magnitude. Finally, we introduce an automatic dimensionality reduction method that improves the effectiveness of the algorithm, extending its applicability to new domains and providing a new way to analyze a subclass of bilinear programs.", "creator": "LaTeX with hyperref package"}}}