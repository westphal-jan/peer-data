{"id": "1401.3488", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Content Modeling Using Latent Permutations", "abstract": "we present a novel bayesian topic model for learning discourse - level document table structure. our model leverages insights from discourse theory to constrain latent citation topic assignments in a a way format that reflects the underlying organization of document topics. we propose a shared global model in which both topic relation selection and ordering are biased to be similar across a collection of related documents. we show that this space of orderings can be effectively represented using a distribution criterion over permutations tasks called the generalized mallows model. we apply our method to three complementary discourse - level tasks : cross - document alignment, document segmentation, integration and information ordering. our experiments show suggest that incorporating upon our flexible permutation - based study model in these applications yields substantial improvements in performance over previously recently proposed methods.", "histories": [["v1", "Wed, 15 Jan 2014 05:38:17 GMT  (319kb)", "http://arxiv.org/abs/1401.3488v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.LG", "authors": ["harr chen", "s r k branavan", "regina barzilay", "david r karger"], "accepted": false, "id": "1401.3488"}, "pdf": {"name": "1401.3488.pdf", "metadata": {"source": "CRF", "title": "Content Modeling Using Latent Permutations", "authors": ["Harr Chen", "Regina Barzilay", "David R. Karger"], "emails": ["harr@csail.mit.edu", "branavan@csail.mit.edu", "regina@csail.mit.edu", "karger@csail.mit.edu"], "sections": [{"heading": "1. Introduction", "text": "A central problem of discourse analysis is modeling the content structure of a document. This structure encompasses the topics that are addressed and the order in which these topics appear across documents in a single domain. Modeling content structure is particularly germane for domains that exhibit recurrent patterns in content organization, such as news and encyclopedia articles. These models aim to induce, for example, that articles about cities typically contain information about History, Economy, and Transportation, and that descriptions of History usually precede those of Transportation.\nPrevious work (Barzilay & Lee, 2004; Elsner, Austerweil, & Charniak, 2007) has demonstrated that content models can be learned from raw unannotated text, and are useful in a variety of text processing tasks such as summarization and information ordering. However, the expressive power of these approaches is limited: by taking a Markovian view on content structure, they only model local constraints on topic organization. This shortcoming is substantial since many discourse constraints described in the literature are global in nature (Graesser, Gernsbacher, & Goldman, 2003; Schiffrin, Tannen, & Hamilton, 2001).\nIn this paper, we introduce a model of content structure that explicitly represents two important global constraints on topic selection.1 The first constraint posits that each document follows a progression of coherent, nonrecurring topics (Halliday & Hasan, 1976). Following the example above, this constraint captures the notion that a single topic, such\n1. Throughout this paper, we will use \u201ctopic\u201d to refer interchangeably to both the discourse unit and language model views of a topic.\nc\u00a92009 AI Access Foundation. All rights reserved.\nas History, is expressed in a contiguous block within the document, rather than spread over disconnected sections. The second constraint states that documents from the same domain tend to present similar topics in similar orders (Bartlett, 1932; Wray, 2002). This constraint guides toward selecting sequences with similar topic ordering, such as placing History before Transportation. While these constraints are not universal across all genres of human discourse, they are applicable to many important domains, ranging from newspaper text to product reviews.2\nWe present a latent topic model over related documents that encodes these discourse constraints by positing a single distribution over the entirety of a document\u2019s content ordering. Specifically, we represent content structure as a permutation over topics. This naturally enforces the first constraint since a permutation does not allow topic repetition. To learn the distribution over permutations, we employ the Generalized Mallows Model (GMM). This model concentrates probability mass on permutations close to a canonical permutation. Permutations drawn from this distribution are likely to be similar, conforming to the second constraint. A major benefit of the GMM is its compact parameterization using a set of real-valued dispersion values. These dispersion parameters allow the model to learn how strongly to bias each document\u2019s topic ordering toward the canonical permutation. Furthermore, the number of parameters grows linearly with the number of topics, thus sidestepping tractability problems typically associated with the large discrete space of permutations.\nWe position the GMM within a larger hierarchical Bayesian model that explains how a set of related documents is generated. For each document, the model posits that a topic ordering is drawn from the GMM, and that a set of topic frequencies is drawn from a multinomial distribution. Together, these draws specify the document\u2019s entire topic structure, in the form of topic assignments for each textual unit. As with traditional topic models, words are then drawn from language models indexed by topic. To estimate the model posterior, we perform Gibbs sampling over the topic structures and GMM dispersion parameters while analytically integrating out the remaining hidden variables.\nWe apply our model to three complex document-level tasks. First, in the alignment task, we aim to discover paragraphs across different documents that share the same topic. In our experiments, our permutation-based model outperforms the Hidden Topic Markov Model (Gruber, Rosen-Zvi, & Weiss, 2007) by a wide margin \u2014 the gap averaged 28% percentage points in F-score. Second, we consider the segmentation task, where the goal is to partition each document into a sequence of topically coherent segments. The model yields an average Pk measure of 0.231, a 7.9% percentage point improvement over a competitive Bayesian segmentation method that does not take global constraints into account (Eisenstein & Barzilay, 2008). Third, we apply our model to the ordering task, that is, sequencing a held out set of textual units into a coherent document. As with the previous two applications, the difference between our model and a state-of-the-art baseline is substantial: our model achieves an average Kendall\u2019s \u03c4 of 0.602, compared to a value of 0.267 for the HMM-based content model (Barzilay & Lee, 2004).\nThe success of the permutation-based model in these three complementary tasks demonstrates its flexibility and effectiveness, and attests to the versatility of the general document\n2. An example of a domain where the first constraint is violated is dialogue. Texts in such domains follow the stack structure, allowing topics to recur throughout a conversation (Grosz & Sidner, 1986).\nstructure induced by our model. We find that encoding global ordering constraints into topic models makes them more suitable for discourse-level analysis, in contrast to the local decision approaches taken by previous work. Furthermore, in most of our evaluation scenarios, our full model yields significantly better results than its simpler variants that either use a fixed ordering or are order-agnostic.\nThe remainder of this paper proceeds as follows. In Section 2, we describe how our approach relates to previous work in both topic modeling and statistical discourse processing. We provide a problem formulation in Section 3.1 followed by an overview of our content model in Section 3.2. At the heart of this model is the distribution over topic permutations, for which we provide background in Section 3.3, before employing it in a formal description of the model\u2019s probabilistic generative story in Section 3.4. Section 4 discusses the estimation of the model\u2019s posterior distribution given example documents using a collapsed Gibbs sampling procedure. Techniques for applying our model to the three tasks of alignment, segmentation, and ordering are explained in Section 5. We then evaluate our model\u2019s performance on each of these tasks in Section 6 before concluding by touching upon directions for future work in Section 7. Code, data sets, annotations, and the raw outputs of our experiments are available at http://groups.csail.mit.edu/rbg/code/mallows/."}, {"heading": "2. Related Work", "text": "We describe two areas of previous work related to our approach. From the algorithmic perspective our work falls into a broad class of topic models. While earlier work on topic modeling took the bag of words view of documents, many recent approaches have expanded topic models to capture some structural constraints. In Section 2.1, we describe these extensions and highlight their differences from our model. On the linguistic side, our work relates to research on modeling text structure in statistical discourse processing. We summarize this work in Section 2.2, drawing comparisons with the functionality supported by our model."}, {"heading": "2.1 Topic Models", "text": "Probabilistic topic models, originally developed in the context of language modeling, have today become popular for a range of NLP applications, such as text classification and document browsing. Topic models posit that a latent state variable controls the generation of each word. Their parameters are estimated using approximate inference techniques such as Gibbs sampling and variational methods. In traditional topic models such as Latent Dirichlet Allocation (LDA) (Blei, Ng, & Jordan, 2003; Griffiths & Steyvers, 2004), documents are treated as bags of words, where each word receives a separate topic assignment and words assigned to the same topic are drawn from a shared language model.\nWhile the bag of words representation is sufficient for some applications, in many cases this structure-unaware view is too limited. Previous research has considered extensions of LDA models in two orthogonal directions, covering both intrasentential and extrasentential constraints."}, {"heading": "2.1.1 Modeling Intrasentential Constraints", "text": "One promising direction for improving topic models is to augment them with constraints on topic assignments of adjoining words within sentences. For example, Griffiths, Steyvers, Blei, and Tenenbaum (2005) propose a model that jointly incorporates both syntactic and semantic information in a unified generative framework and constrains the syntactic classes of adjacent words. In their approach, the generation of each word is controlled by two hidden variables, one specifying a semantic topic and the other specifying a syntactic class. The syntactic class hidden variables are chained together as a Markov model, whereas semantic topic assignments are assumed to be independent for every word.\nAs another example of intrasentential constraints, Wallach (2006) proposes a way to incorporate word order information, in the form of bigrams, into an LDA-style model. In this approach, the generation of each word is conditioned on both the previous word and the topic of the current word, while the word topics themselves are generated from perdocument topic distributions as in LDA. This formulation models text structure at the level of word transitions, as opposed to the work of Griffiths et al. (2005) where structure is modeled at the level of hidden syntactic class transitions.\nOur focus is on modeling high-level document structure in terms of its semantic content. As such, our work is complementary to methods that impose structure on intrasentential units; it should be possible to combine our model with constraints on adjoining words."}, {"heading": "2.1.2 Modeling Extrasentential Constraints", "text": "Given the intuitive connection between the notion of topic in LDA and the notion of topic in discourse analysis, it is natural to assume that LDA-like models can be useful for discourselevel tasks such as segmentation and topic classification. This hypothesis motivated research on models where topic assignment is guided by structural considerations (Purver, Ko\u0308rding, Griffiths, & Tenenbaum, 2006; Gruber et al., 2007; Titov & McDonald, 2008), particularly relationships between the topics of adjacent textual units. Depending on the application, a textual unit may be a sentence, paragraph, or speaker utterance. A common property of these models is that they bias topic assignments to cohere within local segments of text.\nModels in this category vary in terms of the mechanisms used to encourage local topic coherence. For instance, the model of Purver et al. (2006) biases the topic distributions of adjacent utterances (textual units) to be similar. Their model generates each utterance from a mixture of topic language models. The parameters of this topic mixture distribution is assumed to follow a type of Markovian transition process \u2014 specifically, with high probability an utterance u will have the same topic distribution as the previous utterance u \u2212 1; otherwise, a new topic distribution is drawn for u. Thus, each textual unit\u2019s topic distribution only depends on the previous textual unit, controlled by a parameter indicating whether a new topic distribution is drawn.\nIn a similar vein, the Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) posits a generative process where each sentence (textual unit) is assigned a single topic, so that all of the sentence\u2019s words are drawn from a single language model. As with the model of Purver et al., topic transitions between adjacent textual units are modeled in a Markovian fashion \u2014 specifically, sentence i has the same topic as sentence i\u22121 with high probability, or receives a new topic assignment drawn from a shared topic multinomial distribution.\nIn both HTMM and our model, the assumption of a single topic per textual unit allows sections of text to be related across documents by topic. In contrast, Purver et al.\u2019s model is tailored for the task of segmentation, so each utterance is drawn from a mixture of topics. Thus, their model does not capture how utterances are topically aligned across related documents. More importantly, both HTMM and the model of Purver et al. are only able to make local decisions regarding topic transitions, and thus have difficulty respecting longrange discourse constraints such as topic contiguity. Our model instead takes a global view on topic assignments for all textual units by explicitly generating an entire document\u2019s topic ordering from one joint distribution. As we show later in this paper, this global view yields significant performance gains.\nThe recent Multi-Grain Latent Dirichlet Allocation (MGLDA) model (Titov & McDonald, 2008) has also studied topic assignments at the level of sub-document textual units. In MGLDA, a set of local topic distributions is induced for each sentence, dependent on a window of local context around the sentence. Individual words are then drawn either from these local topics or from document-level topics as in standard LDA. MGLDA represents local context using a sliding window, where each window frame comprises overlapping short spans of sentences. In this way, local topic distributions are shared between sentences in close proximity.\nMGLDA can represent more complex topical dependencies than the models of Purver et al. and Gruber et al., because the window can incorporate a much wider swath of local context than two adjacent textual units. However, MGLDA is unable to encode longer range constraints, such as contiguity and ordering similarity, because sentences not in close proximity are only loosely connected through a series of intervening window frames. In contrast, our work is specifically oriented toward these long-range constraints, necessitating a whole-document notion of topic assignment."}, {"heading": "2.2 Modeling Ordering Constraints in Statistical Discourse Analysis", "text": "The global constraints encoded by our model are closely related to research in discourse on information ordering with applications to text summarization and generation (Barzilay, Elhadad, & McKeown, 2002; Lapata, 2003; Karamanis, Poesio, Mellish, & Oberlander, 2004; Elsner et al., 2007). The emphasis of that body of work is on learning ordering constraints from data, with the goal of reordering new text from the same domain. These methods build on the assumption that recurring patterns in topic ordering can be discovered by analyzing patterns in word distribution. The key distinction between prior methods and our approach is that existing ordering models are largely driven by local constraints with limited ability to capture global structure. Below, we describe two main classes of probabilistic ordering models studied in discourse processing."}, {"heading": "2.2.1 Discriminative Models", "text": "Discriminative approaches aim directly to predict an ordering for a given set of sentences. Modeling the ordering of all sentences simultaneously leads to a complex structure prediction problem. In practice, however, a more computationally tractable two-step approach is taken: first, probabilistic models are used to estimate pairwise sentence ordering preferences; next, these local decisions are combined to produce a consistent global ordering (Lapata, 2003;\nAlthaus, Karamanis, & Koller, 2004). Training data for pairwise models is constructed by considering all pairs of sentences in a document, with supervision labels based on how they are actually ordered. Prior work has demonstrated that a wide range of features are useful in these classification decisions (Lapata, 2003; Karamanis et al., 2004; Ji & Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006). For instance, Lapata (2003) has demonstrated that lexical features, such as verb pairs from the input sentences, serve as a proxy for plausible sequences of actions, and thus are effective predictors of well-formed orderings. During the second stage, these local decisions are integrated into a global order that maximizes the number of consistent pairwise classifications. Since finding such an ordering is NP-hard (Cohen, Schapire, & Singer, 1999), various approximations are used in practice (Lapata, 2003; Althaus et al., 2004).\nWhile these two-step discriminative approaches can effectively leverage information about local transitions, they do not provide any means for representing global constraints. In more recent work, Barzilay and Lapata (2008) demonstrated that certain global properties can be captured in the discriminative framework using a reranking mechanism. In this set-up, the system learns to identify the best global ordering given a set of n possible candidate orderings. The accuracy of this ranking approach greatly depends on the quality of selected candidates. Identifying such candidates is a challenging task given the large search space of possible alternatives.\nThe approach presented in this work differs from existing discriminative models in two ways. First, our model represents a distribution over all possible global orderings. Thus, we can use sampling mechanisms that consider this whole space rather than being limited to a subset of candidates as with ranking models. The second difference arises out of the generative nature of our model. Rather than focusing on the ordering task, our order-aware model effectively captures a layer of hidden variables that explain the underlying structure of document content. Thus, it can be effectively applied to a wider variety of applications, including those where sentence ordering is already observed, by appropriately adjusting the observed and hidden components of the model."}, {"heading": "2.2.2 Generative Models", "text": "Our work is closer in technique to generative models that treat topics as hidden variables. One instance of such work is the Hidden Markov Model (HMM)-based content model (Barzilay & Lee, 2004). In their model, states correspond to topics and state transitions represent ordering preferences; each hidden state\u2019s emission distribution is then a language model over words. Thus, similar to our approach, these models implicitly represent patterns at the level of topical structure. The HMM is then used in the ranking framework to select an ordering with the highest probability.\nIn more recent work, Elsner et al. (2007) developed a search procedure based on simulated annealing that finds a high likelihood ordering. In contrast to ranking-based approaches, their search procedure can cover the entire ordering space. On the other hand, as we show in Section 5.3, we can define an ordering objective that can be maximized very efficiently over all possible orderings during prediction once the model parameters have been learned. Specifically, for a bag of p paragraphs, only O(pK) calculations of paragraph probabilities are necessary, where K is the number of topics.\nAnother distinction between our proposed model and prior work is in the way global ordering constraints are encoded. In a Markovian model, it is possible to induce some global constraints by introducing additional local constraints. For instance, topic contiguity can be enforced by selecting an appropriate model topology (e.g., by augmenting hidden states to record previously visited states). However, other global constraints, such as similarity in overall ordering across documents, are much more challenging to represent. By explicitly modeling the topic permutation distribution, we can easily capture this kind of global constraint, ultimately resulting in more accurate topic models and orderings. As we show later in this paper, our model substantially outperforms the approach of Barzilay and Lee on the information ordering task to which they applied the HMM-based content model."}, {"heading": "3. Model", "text": "In this section, we describe our problem formulation and proposed model."}, {"heading": "3.1 Problem Formulation", "text": "Our content modeling problem can be formalized as follows. We take as input a corpus {d1, . . . dD} of related documents, and a specification of a number of topics K.3 Each document d is comprised of an ordered sequence of Nd paragraphs (pd,1, . . . , pd,Nd). As output, we predict a single topic assignment zd,p \u2208 {1, . . . ,K} for each paragraph p.4 These z values should reflect the underlying content organization of each document \u2014 related content discussed within each document, and across separate documents, should receive the same z value.\nOur formulation shares some similarity with the standard LDA setup in that a common set of topics is assigned across a collection of documents. The difference is that in LDA each word\u2019s topic assignment is conditionally independent, following the bag of words view of documents, whereas our constraints on how topics are assigned let us connect word distributional patterns to document-level topic structure."}, {"heading": "3.2 Model Overview", "text": "We propose a generative Bayesian model that explains how a corpus of D documents can be produced from a set of hidden variables. At a high level, the model first selects how frequently each topic is expressed in the document, and how the topics are ordered. These topics then determine the selection of words for each paragraph. Notation used in this and subsequent sections is summarized in Figure 1.\nFor each document d with Nd paragraphs, we separately generate a bag of topics td and a topic ordering \u03c0d. The unordered bag of topics td, which contains Nd elements, expresses how many paragraphs of the document are assigned to each of the K topics. Equivalently, td can be viewed as a vector of occurrence counts for each topic, with zero counts for topics that do not appear at all. Variable td is constructed by taking Nd samples from a\n3. A nonparametric extension of this model would be to also learn K. 4. In well structured documents, paragraphs tend to be internally topically consistent (Halliday & Hasan,\n1976), so predicting one topic per paragraph is sufficient. However, we note that our approach can be applied with no modifications to other levels of textual granularity such as sentences.\ndistribution over topics \u03b8, a multinomial representing the probability of each topic being expressed. Sharing \u03b8 between documents captures the notion that certain topics are more likely across most documents in the corpus.\nThe topic ordering variable \u03c0d is a permutation over the numbers 1 through K that defines the order in which topics appear in the document. We draw \u03c0d from the Generalized Mallows Model, a distribution over permutations that we explain in Section 3.3. As we will see, this particular distribution biases the permutation selection to be close to a single centroid, reflecting the discourse constraint of preferring similar topic structures across documents.\nTogether, a document\u2019s bag of topics td and ordering \u03c0d determine the topic assignment zd,p for each of its paragraphs. For example, in a corpus with K = 4, a seven-paragraph document d with td = {1, 1, 1, 1, 2, 4, 4} and \u03c0d = (2, 4, 3, 1) would induce the topic sequence zd = (2, 4, 4, 1, 1, 1, 1). The induced topic sequence zd can never assign the same topic to two unconnected portions of a document, thus satisfying the constraint of topic contiguity.\nWe assume that each topic k is associated with a language model \u03b2k. The words of a paragraph assigned to topic k are then drawn from that topic\u2019s language model \u03b2k. This portion is similar to standard LDA in that each topic relates to its own language model. However, unlike LDA, our model enforces topic coherence for an entire paragraph rather than viewing a paragraph as a mixture of topics.\nBefore turning to a more formal discussion of the generative process, we first provide background on the permutation model for topic ordering."}, {"heading": "3.3 The Generalized Mallows Model over Permutations", "text": "A central challenge of the approach we have presented is modeling the distribution over possible topic orderings. For this purpose we use the Generalized Mallows Model (GMM) (Fligner & Verducci, 1986; Lebanon & Lafferty, 2002; Meila\u0306, Phadnis, Patterson, & Bilmes, 2007; Klementiev, Roth, & Small, 2008), which exhibits two appealing properties in the context of this task. First, the model concentrates probability mass on some canonical ordering and small perturbations (permutations) of that ordering. This characteristic matches our constraint that documents from the same domain exhibit structural similarity. Second, its parameter set scales linearly with the number of elements being ordered, making it sufficiently constrained and tractable for inference.\nWe first describe the standard Mallows Model over orderings (Mallows, 1957). The Mallows Model takes two parameters, a canonical ordering \u03c3 and a dispersion parameter \u03c1. It then sets the probability of any other ordering \u03c0 to be proportional to e\u2212\u03c1d(\u03c0,\u03c3), where d(\u03c0, \u03c3) represents some distance metric between orderings \u03c0 and \u03c3. Frequently, this metric is the Kendall \u03c4 distance, the minimum number of swaps of adjacent elements needed to transform ordering \u03c0 into the canonical ordering \u03c3. Thus, orderings which are close to the canonical ordering will have high probability, while those in which many elements have been moved will have less probability mass.\nThe Generalized Mallows Model, first introduced by Fligner and Verducci (1986), refines the standard Mallows Model by adding an additional set of dispersion parameters. These parameters break apart the distance d(\u03c0, \u03c3) between orderings into a set of independent components. Each component can then separately vary in its sensitivity to perturbation.\nTo tease apart the distance function into components, the GMM distribution considers the inversions required to transform the canonical ordering into an observed ordering. We first discuss how these inversions are parameterized in the GMM, then turn to the distribution\u2019s definition and characteristics."}, {"heading": "3.3.1 Inversion Representation of Permutations", "text": "Typically, permutations are represented directly as an ordered sequence of elements \u2014 for example, (3, 1, 2) represents permuting the initial order by placing the third element first, followed by the first element, and then the second. The GMM utilizes an alternative permutation representation defined by a vector (v1, . . . , vK\u22121) of inversion counts with respect to the identity permutation (1, . . . ,K). Term vj counts the number of times when a value greater than j appears before j in the permutation. Note that the jth inversion count vj can only take on integer values from 0 to K\u2212 j inclusive. Thus the inversion count vector has only K \u2212 1 elements, as vK is always zero. For instance, given the standard form permutation (3, 1, 5, 6, 2, 4), v2 = 3 because 3, 5, and 6 appear before 2, and v3 = 0 because no numbers appear before it; the entire inversion count vector would be (1, 3, 0, 2, 0). Likewise, our previous example permutation (2, 4, 3, 1) maps to inversion counts (3, 0, 1). The sum of all components of an entire inversion count vector is simply that ordering\u2019s Kendall \u03c4 distance from the canonical ordering.\nA significant appeal of the inversion representation is that every valid, distinct vector of inversion counts corresponds to a distinct permutation and vice versa. To see this, note that for each permutation we can straightforwardly compute its inversion counts. Conversely, given a sequence of inversion counts, we can construct the unique corresponding permutation. We insert items into the permutation, working backwards from item K. Assume that we have already placed items j + 1 through K in the proper order. To insert item j, we note that exactly vj of items j + 1 to K must precede it, meaning that it must be inserted after position vj in the current order (see the Compute-\u03c0 algorithm in Figure 1). Since there is only one place where j can be inserted that fulfills the inversion counts, induction shows that exactly one permutation can be constructed to satisfy the given inversion counts.\nIn our model, we take the canonical topic ordering to always be the identity ordering (1, . . . ,K). Because the topic numbers in our task are completely symmetric and not linked to any extrinsic meaning, fixing the global ordering to a specific arbitrary value does not sacrifice any representational power. In the general case of the GMM, the canonical ordering is a parameter of the distribution."}, {"heading": "3.3.2 Probability Mass Function", "text": "The GMM assigns probability mass to a particular order based on how that order is permuted from the canonical ordering. More precisely, it associates a distance with every permutation, where the canonical ordering has distance zero and permutations with many inversions with respect to this canonical ordering have larger distance. The distance assignment is based on K\u22121 real-valued dispersion parameters (\u03c11, . . . , \u03c1K\u22121). The distance of a permutation with inversion counts v is then defined to be \u2211 j \u03c1jvj . The GMM\u2019s probability\nmass function is exponential in this distance:\nGMM(v; \u03c1) = e\u2212\n\u2211 j \u03c1jvj\n\u03c8(\u03c1)\n= K\u22121\u220f j=1 e\u2212\u03c1jvj \u03c8j(\u03c1j) , (1)\nwhere \u03c8(\u03c1) = \u220f j \u03c8j(\u03c1j) is a normalization factor with value:\n\u03c8j(\u03c1j) = 1\u2212 e\u2212(K\u2212j+1)\u03c1j\n1\u2212 e\u2212\u03c1j . (2)\nSetting all \u03c1j equal to a single value \u03c1 recovers the standard Mallows Model with a Kendall \u03c4 distance function. The factorization of the GMM into independent probabilities per inversion count makes this distribution particularly easy to apply; we will use GMMj to refer to the jth multiplicand of the probability mass function, which is the marginal distribution over vj :\nGMMj(vj ; \u03c1j) = e\u2212\u03c1jvj\n\u03c8j(\u03c1j) . (3)\nDue to the exponential form of the distribution, requiring that \u03c1j > 0 constrains the GMM to assign highest probability mass to each vj being zero, i.e., the distributional mode is the canonical identity permutation. A higher value for \u03c1j assigns more probability mass to vj being close to zero, biasing j to have fewer inversions."}, {"heading": "3.3.3 Conjugate Prior", "text": "A major benefit of the GMM is its membership in the exponential family of distributions; this means that it is particularly amenable to a Bayesian representation, as it admits a natural independent conjugate prior for each parameter \u03c1j (Fligner & Verducci, 1990):\nGMM0(\u03c1j | vj,0, \u03bd0) \u221d e(\u2212\u03c1jvj,0\u2212log\u03c8j(\u03c1j))\u03bd0 . (4)\nThis prior distribution takes two parameters \u03bd0 and vj,0. Intuitively, the prior states that over \u03bd0 previous trials, the total number of inversions observed was \u03bd0vj,0. This distribution can be easily updated with the observed vj to derive a posterior distribution.\nBecause each vj has a different range, it is inconvenient to set the prior hyperparameters vj,0 directly. In our work, we instead assign a common prior value for each parameter \u03c1j , which we denote as \u03c10. Then we set each vj,0 such that the maximum likelihood estimate of \u03c1j is \u03c10. By differentiating the likelihood of the GMM with respect to \u03c1j , it is straightforward to verify that this works out to setting:\nvj,0 = 1 e\u03c10 \u2212 1 \u2212 K \u2212 j + 1 e(K\u2212j+1)\u03c10 \u2212 1 . (5)"}, {"heading": "3.4 Formal Generative Process", "text": "We now fully specify the details of our content model, whose plate diagram appears in Figure 1. We observe a corpus of D documents, where each document d is an ordered sequence ofNd paragraphs and each paragraph is represented as a bag of words. The number of topics K is assumed to be pre-specified. The model induces a set of hidden variables that probabilistically explain how the words of the corpus were produced. Our final desired output is the posterior distributions over the paragraphs\u2019 hidden topic assignment variables. In the following, variables subscripted with 0 are fixed prior hyperparameters.\n1. For each topic k, draw a language model \u03b2k \u223c Dirichlet(\u03b20). As with LDA, these are topic-specific word distributions.\n2. Draw a topic distribution \u03b8 \u223c Dirichlet(\u03b80), which expresses how likely each topic is to appear regardless of position.\n3. Draw the topic ordering distribution parameters \u03c1j \u223c GMM0(\u03c10, \u03bd0) for j = 1 to K \u2212 1. These parameters control how rapidly probability mass decays for having more inversions for each topic. A separate \u03c1j for every topic allows us to learn that some topics are more likely to be reordered than others.\n4. For each document d with Nd paragraphs:\n(a) Draw a bag of topics td by sampling Nd times from Multinomial(\u03b8).\n(b) Draw a topic ordering \u03c0d, by sampling a vector of inversion counts vd \u223c GMM(\u03c1), and then applying algorithm Compute-\u03c0 from Figure 1 to vd.\n(c) Compute the vector of topic assignments zd for document d\u2019s paragraphs by sorting td according to \u03c0d, as in algorithm Compute-z from Figure 1.5\n(d) For each paragraph p in document d: i. Sample each word w in p according to the language model of p: w \u223c\nMultinomial(\u03b2zd,p)."}, {"heading": "3.5 Properties of the Model", "text": "In this section we describe the rationale behind using the GMM to represent the ordering component of our content model.\n\u2022 Representational Power The GMM concentrates probability mass around one centroid permutation, reflecting our preferred bias toward document structures with similar topic orderings. Furthermore, the parameterization of the GMM using a vector of dispersion parameters \u03c1 allows for flexibility in how strongly the model biases toward a single ordering \u2014 at one extreme (\u03c1 =\u221e) only one ordering has nonzero probability, while at the other (\u03c1 = 0) all orderings are equally likely. Because \u03c1 is comprised\n5. Multiple permutations can contribute to the probability of a single document\u2019s topic assignments zd, if there are topics that do not appear in td. As a result, our current formulation is biased toward assignments with fewer topics per document. In practice, we do not find this to negatively impact model performance.\nof independent dispersion parameters (\u03c11, . . . , \u03c1K\u22121), the distribution can assign different penalties for displacing different topics. For example, we may learn that middle sections (in the case of Cities, sections such as Economy and Culture) are more likely to vary in position across documents than early sections (such as Introduction and History).\n\u2022 Computational Benefits The parameterization of the GMM using a vector of dispersion parameters \u03c1 is compact and tractable. Since the number of parameters grows linearly with the number of topics, the model can efficiently handle longer documents with greater diversity of content.\nAnother computational advantage of this model is its seamless integration into a larger Bayesian model. Due to its membership in the exponential family and the existence of its conjugate prior, inference does not become significantly more complex when the GMM is used in a hierarchical context. In our case, the entire document generative model also accounts for topic frequency and the words within each topic.\nOne final beneficial effect of the GMM is that it breaks the symmetry of topic assignments by fixing the distribution centroid. Specifically, topic assignments are not invariant to relabeling, because the probability of the underlying permutation would change. In contrast, many topic models assign the same probability to any relabeling of the topic assignments. Our model thus sidesteps the problem of topic identifiability, the issue where a model may have multiple maxima with the same likelihood due to the underlying symmetry of the hidden variables. Non-identifiable models such as standard LDA may cause sampling procedures to jump between maxima or produce draws that are difficult to aggregate across runs.\nFinally, we will show in Section 6 that the benefits of the GMM extend from the theoretical to the empirical: representing permutations using the GMM almost always leads to superior performance compared to alternative approaches."}, {"heading": "4. Inference", "text": "The variables that we aim to infer are the paragraph topic assignments z, which are determined by the bag of topics t and ordering \u03c0 for each document. Thus, our goal is to estimate the joint marginal distributions of t and \u03c0 given the document text while integrating out all remaining hidden parameters:\nP (t, \u03c0, | w). (6)\nWe accomplish this inference task through Gibbs sampling (Geman & Geman, 1984; Bishop, 2006). A Gibbs sampler builds a Markov chain over the hidden variable state space whose stationary distribution is the actual posterior of the joint distribution. Each new sample is drawn from the distribution of a single variable conditioned on previous samples of the other variables. We can \u201ccollapse\u201d the sampler by integrating over some of the hidden variables in the model, in effect reducing the state space of the Markov chain. Collapsed sampling has been previously demonstrated to be effective for LDA and its variants (Griffiths & Steyvers, 2004; Porteous, Newman, Ihler, Asuncion, Smyth, & Welling, 2008; Titov &\nMcDonald, 2008). It is typically preferred over the explicit Gibbs sampling of all the hidden variables because of the smaller search space and generally shorter mixing time.\nOur sampler analytically integrates out all but three sets of hidden variables: bags of topics t, orderings \u03c0, and permutation inversion parameters \u03c1. After a burn-in period, we treat the last samples of t and \u03c0 as a draw from the posterior. When samples of the marginalized variables \u03b8 and \u03b2 are necessary, they can be estimated based on the topic assignments as we show in Section 5.3. Figure 2 summarizes the Gibbs sampling steps of our inference procedure.\nDocument Probability As a preliminary step, consider how to calculate the probability of a single document\u2019s words wd given the document\u2019s paragraph topic assignments zd and the remaining documents and their topic assignments. Note that this probability is decomposable into a product of probabilities over individual paragraphs where paragraphs with different topics have conditionally independent word probabilities. Let w\u2212d and z\u2212d indicate the words and topic assignments to documents other than d, and W be the vocabulary size. The probability of the words in d is then:\nP (wd | z,w\u2212d, \u03b20) = K\u220f k=1 \u222b \u03b2k P (wd | zd,\u03b2k) P (\u03b2k | z,w\u2212d, \u03b20) d\u03b2k\n= K\u220f k=1 DCM({wd,i : zd,i = k} | {w\u2212d,i : z\u2212d,i = k}, \u03b20), (7)\nwhere DCM(\u00b7) refers to the Dirichlet compound multinomial distribution, the result of integrating over multinomial parameters with a Dirichlet prior (Bernardo & Smith, 2000). For a Dirichlet prior with parameters \u03b1 = (\u03b11, . . . , \u03b1W ), the DCM assigns the following probability to a series of observations x = {x1, . . . , xn}:\nDCM(x; \u03b1) = \u0393( \u2211\nj \u03b1j)\u220f j \u0393(\u03b1j) W\u220f i=1 \u0393(N(x, i) + \u03b1i) \u0393(|x|+ \u2211 j \u03b1j) , (8)\nwhere N(x, i) refers to the number of times word i appears in x. Here, \u0393(\u00b7) is the Gamma function, a generalization of the factorial for real numbers. Some algebra shows that the DCM\u2019s posterior probability density function conditioned on a series of observations y = {y1, . . . , yn} can be computed by updating each \u03b1i with counts of how often word i appears in y:\nDCM(x | y,\u03b1) = DCM(x;\u03b11 +N(y, 1), . . . , \u03b1W +N(y,W )). (9)\nEquations 7 and 9 will be used to compute the conditional distributions of the hidden variables. We now turn to how each individual random variable is resampled.\nBag of Topics First we consider how to resample td,i, the ith topic draw for document d conditioned on all other parameters being fixed (note this is not the topic of the ith paragraph, as we reorder topics using \u03c0d, which is generated separately):\nP (td,i = t | . . .) \u221d P (td,i = t | t\u2212(d,i), \u03b80) P (wd | td, \u03c0d,w\u2212d, z\u2212d, \u03b20) \u221d [ N(t\u2212(d,i), t) + \u03b80 |t\u2212(d,i)|+K\u03b80 ] P (wd | z,w\u2212d, \u03b20), (10)\nwhere td is updated to reflect td,i = t, and zd is deterministically computed in the last step using Compute-z from Figure 1 with inputs td and \u03c0d. The first step reflects an application of Bayes rule to factor out the term for wd; we then drop superfluous terms from the conditioning. In the second step, the former term arises out of the DCM, by updating the parameters \u03b80 with observations t\u2212(d,i) as in Equation 9 and dropping constants. The latter document probability term is computed using Equation 7. The new td,i is selected by sampling from this probability computed over all possible topic assignments.\nOrdering The parameterization of a permutation \u03c0d as a series of inversion values vd,j reveals a natural way to decompose the search space for Gibbs sampling. For each document d, we resample vd,j for j = 1 to K \u2212 1 independently and successively according to its conditional distribution:\nP (vd,j = v | . . .) \u221d P (vd,j = v | \u03c1j) P (wd | td, \u03c0d,w\u2212d, z\u2212d, \u03b20) = GMMj(v; \u03c1j) P (wd | z,w\u2212d, \u03b20), (11)\nwhere \u03c0d is updated to reflect vd,j = v, and zd is computed deterministically according to td and \u03c0d. The first term refers to Equation 3; the second is computed using Equation 7. This probability is computed for every possible value of v, which ranges from 0 to K \u2212 j, and term vd,j is sampled according to the resulting probabilities.\nGMM Parameters For each j = 1 to K \u2212 1, we resample \u03c1j from its posterior distribution: P (\u03c1j | . . .) = GMM0 ( \u03c1j ; \u2211 d vd,j + vj,0\u03bd0 N + \u03bd0 , N + \u03bd0 ) , (12)\nwhere GMM0 is evaluated according to Equation 4. The normalization constant of this distribution is unknown, meaning that we cannot directly compute and invert the cumulative distribution function to sample from this distribution. However, the distribution itself is univariate and unimodal, so we can expect that an MCMC technique such as slice sampling (Neal, 2003) should perform well. In practice, Matlab\u2019s built-in slice sampler provides a robust draw from this distribution.6\nComputational Issues During inference, directly computing document probabilities on the basis of Equation 7 results in many redundant calculations that slow the runtime of each iteration considerably. To improve the computational performance of our proposed inference procedure, we apply some memoization techniques during sampling. Within a single iteration, for each document, the Gibbs sampler requires computing the document\u2019s probability given its topic assignments (Equation 7) many times, but each computation frequently conditions on only slight variations of those topic assignments. A na\u0308\u0131ve approach would compute a probability for every paragraph each time a document probability is desired, performing redundant calculations when topic assignment sequences with shared subsequences are repeatedly considered.\nInstead, we use lazy evaluation to build a three-dimensional cache, indexed by tuple (i, j, k), as follows. Each time a document probability is requested, it is broken into independent subspans of paragraphs, where each subspan takes on one contiguous topic assignment. This is possible due to the way Equation 7 factorizes into independent per-topic\n6. In particular, we use the slicesample function from the Matlab Statistics Toolbox.\nmultiplicands. For a subspan starting at paragraph i, ending at paragraph j, and assigned topic k, the cache is consulted using key (i, j, k). For example, topic assignments zd = (2, 4, 4, 1, 1, 1, 1) would result in cache lookups at (1, 1, 2), (2, 3, 4), and (4, 7, 1). If a cached value is unavailable, the correct probability is computed using Equation 7 and the result is stored in the cache at location (i, j, k). Moreover, we also record values at every intermediate cache location (i, l, k) for l = i to j \u2212 1, because these values are computed as subproblems while evaluating Equation 7 for (i, j, k). The cache is reset before proceeding to the next document since the conditioning changes between documents. For each document, this caching guarantees that there are at most O(N2dK) paragraph probability calculations. In practice, because most individual Gibbs steps are small, this bound is very loose and the caching mechanism reduces computation time by several orders of magnitude.\nWe also maintain caches of word-topic and paragraph-topic assignment frequencies, allowing us to rapidly compute the counts used in equations 7 and 10. This form of caching is the same as what is used by Griffiths and Steyvers (2004)."}, {"heading": "5. Applications", "text": "In this section, we describe how our model can be applied to three challenging discourselevel tasks: aligning paragraphs of similar topical content between documents, segmenting each document into topically cohesive sections, and ordering new unseen paragraphs into a coherent document. In particular, we show that the posterior samples produced by our inference procedure from Section 4 can be used to derive a solution for each of these tasks."}, {"heading": "5.1 Alignment", "text": "For the alignment task we wish to find how the paragraphs of each document topically relate to paragraphs of other documents. Essentially, this is a cross-document clustering task \u2013 an alignment assigns each paragraph of a document into one of K topically related groupings. For instance, given a set of cell phone reviews, one group may represent text fragments that discuss Price, while another group consists of fragments about Reception.\nOur model can be readily employed for this task: we can view the topic assignment for each paragraph z as a cluster label. For example, for two documents d1 and d2 with topic assignments zd1 = (2, 4, 4, 1, 1, 1, 1) and zd2 = (4, 4, 3, 3, 2, 2, 2), paragraph 1 of d1 is grouped together with paragraphs 5 through 7 of d2, and paragraphs 2 and 3 of d1 with 1 and 2 of d2. The remaining paragraphs assigned to topics 1 and 3 form their own separate per-document clusters.\nPreviously developed methods for cross-document alignment have been primarily driven by similarity functions that quantify lexical overlap between textual units (Barzilay & Elhadad, 2003; Nelken & Shieber, 2006). These methods do not explicitly model document structure, but they specify some global constraints that guide the search for an optimal alignment. Pairs of textual units are considered in isolation for making alignment decisions. In contrast, our approach allows us to take advantage of global structure and shared language models across all related textual units without requiring manual specification of matching constraints."}, {"heading": "5.2 Segmentation", "text": "Segmentation is a well-studied discourse task where the goal is to divide a document into topically cohesive contiguous sections. Previous approaches have typically relied on lexical cohesion \u2014 that is, similarity in word choices within a document subspan \u2014 to guide the choice of segmentation boundaries (Hearst, 1994; van Mulbregt, Carp, Gillick, Lowe, & Yamron, 1998; Blei & Moreno, 2001; Utiyama & Isahara, 2001; Galley, McKeown, FoslerLussier, & Jing, 2003; Purver et al., 2006; Malioutov & Barzilay, 2006; Eisenstein & Barzilay, 2008). Our model relies on this same notion in determining the language models of topics, but connecting topics across documents and constraining how those topics appear allow it to better learn the words that are most indicative of topic cohesion.\nThe output samples from our model\u2019s inference procedure map straightforwardly to segmentations \u2014 contiguous spans of paragraphs that are assigned the same topic number are taken to be one segment. For example, a seven-paragraph document d with topic assignments zd = (2, 4, 4, 1, 1, 1, 1) would be segmented into three sections, comprised of paragraph 1, paragraphs 2 and 3, and paragraphs 4 through 7. Note that the segmentation ignores the specific values used for topic assignments, and only heeds the paragraph boundaries at which topic assignments change."}, {"heading": "5.3 Ordering", "text": "A third application of our model is to the problem of creating structured documents from collections of unordered text segments. This text ordering task is an important step in broader NLP tasks such as text summarization and generation. For this task, we assume we are provided with well structured documents from a single domain as training examples; once trained, the model is used to induce an ordering of previously unseen collections of paragraphs from the same domain.\nDuring training, our model learns a canonical ordering of topics for documents within the collection, via the language models associated with each topic. Because the GMM concentrates probability mass around the canonical (1, . . . ,K) topic ordering, we expect that highly probable words in the language models of lower -numbered topics tend to appear early in a document, whereas highly probable words in the language models of higher -numbered topics tend to appear late in a document. Thus, we structure new documents according to this intuition \u2014 paragraphs with words tied to low topic numbers should be placed earlier than paragraphs with words relating to high topic numbers.\nFormally, given an unseen document d comprised of an unordered set of paragraphs {p1, . . . , pn}, we order paragraphs according to the following procedure. First, we find the most probable topic assignment z\u0302i independently for each paragraph pi, according to parameters \u03b2 and \u03b8 learned during the training phase:\nz\u0302i = arg max k P (zi = k | pi,\u03b2,\u03b8)\n= arg max k P (pi | zi = k,\u03b2k)P (zi = k | \u03b8). (13)\nSecond, we sort the paragraphs by topic assignment z\u0302i in ascending order \u2014 since (1 . . .K) is the GMM\u2019s canonical ordering, this yields the most likely ordering conditioned on a single estimated topic assignment for each paragraph. Due to possible ties in topic assignments,\nthe resulting document may be a partial ordering; if a full ordering is required, ties are broken arbitrarily.\nA key advantage of this proposed approach is that it is closed-form and computationally efficient. Though the training phase requires running the inference procedure of Section 4, once the model parameters are learned, predicting an ordering for a new set of p paragraphs requires computing only pK probability scores. In contrast, previous approaches have only been able to rank a small subset of all possible document reorderings (Barzilay & Lapata, 2008), or performed a search procedure through the space of orderings to find an optimum (Elsner et al., 2007).7\nThe objective function of Equation 13 depends on posterior estimates of \u03b2 and \u03b8 given the training documents. Since our collapsed Gibbs sampler integrates out these two hidden variables, we need to back out the values of \u03b2 and \u03b8 from the known posterior samples of z. This can easily be done by computing a point estimate of each distribution based on the word-topic and topic-document assignment frequencies, respectively, as is done by Griffiths and Steyvers (2004). The probability mass \u03b2\u0302wk of word w in the language model of topic k is given by:\n\u03b2\u0302wk = N\u03b2(k,w) + \u03b20 N\u03b2(k) +W\u03b20 , (14)\nwhere N\u03b2(k,w) the total number of times word w was assigned to topic k, and N\u03b2(k) is the total number of words assigned to topic k, according to the posterior sample of z. We can derive a similar estimate for \u03b8\u0302k, the prior likelihood of topic k:\n\u03b8\u0302k = N\u03b8(k) + \u03b80 N\u03b8 +K\u03b80 , (15)\nwhere N\u03b8(k) is the total number of paragraphs assigned to topic k according to the sample of z, and N\u03b8 is the total number of paragraphs in the entire corpus."}, {"heading": "6. Experiments", "text": "In this section, we evaluate the performance of our model on the three tasks presented in Section 5: cross-document alignment, document segmentation, and information ordering. We first describe some preliminaries common to all three tasks, covering the data sets, reference comparison structures, model variants, and inference algorithm settings shared by each evaluation. We then provide a detailed examination of how our model performs on each individual task."}, {"heading": "6.1 General Evaluation Setup", "text": "Data Sets In our experiments we use five data sets, briefly described below (for additional statistics, see Table 1):\n7. The approach we describe is not the same as finding the most probable paragraph ordering according to data likelihood, which is how the optimal ordering is derived for the HMM-based content model. Our proposed ordering technique essentially approximates that objective by using a per-paragraph maximum a posteriori estimate of the topic assignments rather than the full posterior topic assignment distribution. This approximation makes for a much faster prediction algorithm that performs well empirically.\n\u2022 CitiesEn: Articles from the English Wikipedia about the world\u2019s 100 largest cities by population. Common topics include History, Culture, and Demographics. These articles are typically of substantial size and share similar content organization patterns.\n\u2022 CitiesEn500 : Articles from the English Wikipedia about the world\u2019s 500 largest cities by population. This collection is a superset of CitiesEn. Many of the lower-ranked cities are not well known to English Wikipedia editors \u2014 thus, compared to CitiesEn these articles are shorter on average and exhibit greater variability in content selection and ordering.\n\u2022 CitiesFr : Articles from the French Wikipedia about the same 100 cities as in CitiesEn.\n\u2022 Elements: Articles from the English Wikipedia about chemical elements in the periodic table,8 including topics such as Biological Role, Occurrence, and Isotopes.\n\u2022 Phones: Reviews extracted from PhoneArena.com, a popular cell phone review website. Topics in this corpus include Design, Camera, and Interface. These reviews are written by expert reviewers employed by the site, as opposed to lay users.9\nThis heterogeneous collection of data sets allows us to examine the behavior of the model under diverse test conditions. These sets vary in how the articles were generated, the language in which the articles were written, and the subjects they discuss. As a result, patterns in topic organization vary greatly across domains. For instance, within the Phones corpus, the articles are very formulaic, due to the centralized editorial control of the website, which establishes consistent standards followed by the expert reviewers. On the other hand, Wikipedia articles exhibit broader structural variability due to the collaborative nature of\n8. All 118 elements at http://en.wikipedia.org/wiki/Periodic table, including undiscovered element 117. 9. In the Phones set, 35 documents are very short \u201cexpress\u201d reviews without section headings; we include\nthem in the input to the model, but did not evaluate on them.\nWikipedia editing, which allows articles to evolve independently. While Wikipedia articles within the same category often exhibit similar section orderings, many have idiosyncratic inversions. For instance, in the CitiesEn corpus, both the Geography and History sections typically occur toward the beginning of a document, but History can appear either before or after Geography across different documents.\nEach corpus we consider has been manually divided into sections by their authors, including a short textual heading for each section. In Sections 6.2.1 and 6.3.1, we discuss how these author-created sections with headings are used to generate reference annotations for the alignment and segmentation tasks. Note that we only use the headings for evaluation; none of the heading information is provided to any of the methods under consideration. For the tasks of alignment and segmentation, evaluation is performed on the datasets presented in Table 1. For the ordering task, however, this data is used for training, and evaluation is performed using a separate held-out set of documents. The details of this held-out dataset are given in Section 6.4.1.\nModel Variants For each evaluation, besides comparing to baselines from the literature, we also consider two variants of our proposed model. In particular, we investigate the impact of the Mallows component of the model by alternately relaxing and tightening the way it constrains topic orderings:\n\u2022 Constrained : In this variant, we require all documents to follow the exact same canonical ordering of topics. That is, no topic permutation inversions are allowed, though documents may skip topics as before. This case can be viewed as a special case of the general model, where the Mallows inversion prior \u03c10 approaches infinity. From an implementation standpoint, we simply fix all inversion counts v to zero during inference.10\n\u2022 Uniform: This variant assumes a uniform distribution over all topic permutations, instead of biasing toward a small related set. Again, this is a special case of the full model, with inversion prior \u03c10 set to zero, and the strength of that prior \u03bd0 approaching infinity, thus forcing each item of \u03c1 to always be zero.\nNote that both of these variants still enforce the long-range constraint of topic contiguity, and vary from the full model only in how they capture topic ordering similarity.\nEvaluation Procedure and Parameter Settings For each evaluation of our model and its variants, we run the collapsed Gibbs sampler from five random seed states, and take the 10,000th iteration of each chain as a sample. Results presented are the average over these five samples.\nDirichlet prior hyperparameters for the bag of topics \u03b80 and language models \u03b20 are set to 0.1. For the GMM, we set the prior dispersion hyperparameter \u03c10 to 1, and the effective\n10. At first glance, the Constrained model variant appears to be equivalent to an HMM where each state i can transition to either i or i + 1. However, this is not the case \u2014 some topics may appear zero times in a document, resulting in multiple possible transitions from each state. Furthermore, the transition probabilities would be dependent on position within the document \u2014 for example, at earlier absolute positions within a document, transitions to high-index topics are unlikely, because that would require all subsequent paragraphs to have a high-index topic.\nsample size prior \u03bd0 to be 0.1 times the number of documents. These values are minimally tuned, and similar results are achieved for alternative settings of \u03b80 and \u03b20. Parameters \u03c10 and \u03bd0 control the strength of the bias toward structural regularity, trading off between the Constrained and Uniform model variants. The values we have chosen are a middle ground between those two extremes.\nOur model also takes a parameter K that controls the upper bound on the number of latent topics. Note that our algorithm can select fewer than K topics for each document, so K does not determine the number of segments in each document. In general, a higher K results in a finer-grained division of each document into different topics, which may result in more precise topics, but may also split topics that should be together. We report results in each evaluation using both K = 10 and 20."}, {"heading": "6.2 Alignment", "text": "We first evaluate the model on the task of cross-document alignment, where the goal is to group textual units from different documents into topically cohesive clusters. For instance, in the Cities-related domains, one such cluster may include Transportation-related paragraphs. Before turning to the results we first present details of the specific evaluation setup targeted to this task."}, {"heading": "6.2.1 Alignment Evaluation Setup", "text": "Reference Annotations To generate a sufficient amount of reference data for evaluating alignments we use section headings provided by the authors. We assume that two paragraphs are aligned if and only if their section headings are identical. These headings constitute noisy annotations in the Wikipedia datasets: the same topical content may be labeled with different section headings in different articles (e.g., for CitiesEn, \u201cPlaces of interest\u201d in one article and \u201cLandmarks\u201d in another), so we call this reference structure the noisy headings set.\nIt is not clear a priori what effect this noise in the section headings may have on evaluation accuracy. To empirically estimate this effect, we also use some manually annotated alignments in our experiments. Specifically, for the CitiesEn corpus, we manually annotated each article\u2019s paragraphs with a consistent set of section headings, providing us an additional reference structure to evaluate against. In this clean headings set, we found approximately 18 topics that were expressed in more than one document.\nMetrics To quantify our alignment output we compute a recall and precision score of a candidate alignment against a reference alignment. Recall measures, for each unique section heading in the reference, the maximum number of paragraphs with that heading that are assigned to one particular topic. The final score is computed by summing over each section heading and dividing by the total number of paragraphs. High recall indicates that paragraphs of the same section headings are generally being assigned to the same topic.\nConversely, precision measures, for each topic number, the maximum number of paragraphs with that topic assignment that share the same section heading. Precision is summed over each topic and normalized by the total number of paragraphs. High precision means that paragraphs assigned to a single topic usually correspond to the same section heading.\nRecall and precision trade off against each other \u2014 more finely grained topics will tend to improve precision at the cost of recall. At the extremes, perfect recall occurs when every paragraph is assigned the same topic, and perfect precision when each paragraph is its own topic.\nWe also present one summary F-score in our results, which is the harmonic mean of recall and precision.\nStatistical significance in this setup is measured with approximate randomization (Noreen, 1989), a nonparametric test that can be directly applied to nonlinearly computed metrics such as F-score. This test has been used in prior evaluations for information extraction and machine translation (Chinchor, 1995; Riezler & Maxwell, 2005).\nBaselines For this task, we compare against two baselines:\n\u2022 Hidden Topic Markov Model (HTMM) (Gruber et al., 2007): As explained in Section 2, this model represents topic change between adjacent textual units in a Markovian fashion. HTMM can only capture local constraints, so it would allow topics to recur non-contiguously throughout a document. We use the publicly available implementation,11 with priors set according to the recommendations made in the original work.\n\u2022 Clustering : We use a repeated bisection algorithm to find a clustering of the paragraphs that maximizes the sum of the pairwise cosine similarities of the items in each cluster.12 This clustering was implemented using the CLUTO toolkit.13 Note that this approach is completely structure-agnostic, treating documents as bags of paragraphs rather than sequences of paragraphs. These types of clustering techniques have been shown to deliver competitive performance for cross-document alignment tasks (Barzilay & Elhadad, 2003)."}, {"heading": "6.2.2 Alignment Results", "text": "Table 2 presents the results of the alignment evaluation. On all of the datasets, the best performance is achieved by our model or its variants, by a statistically significant and usually substantial margin.\nThe comparative performance of the baseline methods is consistent across domains \u2013 surprisingly, clustering performs better than the more complex HTMM model. This observation is consistent with previous work on cross-document alignment and multidocument summarization, which use clustering as their main component (Radev, Jing, & Budzikowska, 2000; Barzilay, McKeown, & Elhadad, 1999). Despite the fact that HTMM captures some dependencies between adjacent paragraphs, it is not sufficiently constrained. Manual examination of the actual topic assignments reveals that HTMM often assigns the same topic for disconnected paragraphs within a document, violating the topic contiguity constraint.\nIn all but one domain the full GMM-based approach yields the best performance compared to its variants. The one exception is in the Phone domain. There the Constrained\n11. http://code.google.com/p/openhtmm/ 12. This particular clustering technique substantially outperforms the agglomerative and graph partitioning-\nbased clustering approaches for our task. 13. http://glaros.dtc.umn.edu/gkhome/views/cluto/\nbaseline achieves the best result for both K by a small margin. These results are to be expected, given the fact that this domain exhibits a highly rigid topic structure across all documents. A model that permits permutations of topic ordering, such as the GMM, is too flexible for such highly formulaic domains.\nFinally, we observe that the evaluations based on manual and noisy annotations exhibit an almost entirely consistent ranking of the methods under consideration (see the clean and noisy headings results for CitiesEn in Table 2). This consistency indicates that the noisy headings are sufficient for gaining insight into the comparative performance of the different approaches."}, {"heading": "6.3 Segmentation", "text": "Next we consider the task of text segmentation. We test whether the model is able to identify the boundaries of topically coherent text segments."}, {"heading": "6.3.1 Segmentation Evaluation Setup", "text": "Reference Segmentations As described in Section 6.1, all of the datasets used in this evaluation have been manually divided into sections by their authors. These annotations are used to create reference segmentations for evaluating our model\u2019s output. Recall from Section 6.2.1 that we also built a clean reference structure for the CitiesEn set. That structure encodes a \u201cclean\u201d segmentation of each document because it adjusts the granularity of section headings to be consistent across documents. Thus, we also compare against the segmentation specified by the CitiesEn clean section headings.\nMetrics Segmentation quality is evaluated using the standard penalty metrics Pk and WindowDiff (Beeferman, Berger, & Lafferty, 1999; Pevzner & Hearst, 2002). Both pass a sliding window over the documents and compute the probability of the words at the end of the windows being improperly segmented with respect to each other. WindowDiff is stricter, and requires that the number of segmentation boundaries between the endpoints of the window be correct as well.14\nBaselines We first compare to BayesSeg (Eisenstein & Barzilay, 2008),15 a Bayesian segmentation approach that is the current state-of-the-art for this task. Interestingly, our model reduces to their approach when every document is considered completely in isolation, with no topic sharing between documents. Connecting topics across documents makes for a much more difficult inference problem than the one tackled by Eisenstein and Barzilay. At the same time, their algorithm cannot capture structural relatedness across documents.\nSince BayesSeg is designed to be operated with a specification of a number of segments, we provide this baseline with the benefit of knowing the correct number of segments for each document, which is not provided to our system. We run this baseline using the\n14. Statistical significance testing is not standardized and usually not reported for the segmentation task, so we omit these tests in our results. 15. We do not evaluate on the corpora used in their work, since our model relies on content similarity across documents in the corpus.\nauthors\u2019 publicly available implementation;16 its priors are set using a built-in mechanism that automatically re-estimates hyperparameters.\nWe also compare our method with the algorithm of Utiyama and Isahara (2001), which is commonly used as a point of reference in the evaluation of segmentation algorithms. This algorithm computes the optimal segmentation by estimating changes in the predicted language models of segments under different partitions. We used the publicly available implementation of the system,17 which does not require parameter tuning on a held-out development set. In contrast to BayesSeg, this algorithm has a mechanism for predicting the number of segments, but can also take a pre-specified number of segments. In our comparison, we consider both versions of the algorithm \u2013 U&I denotes the case when the correct number of segments is provided to the model and U&I denotes when the model estimates the optimal number of segments."}, {"heading": "6.3.2 Segmentation Results", "text": "Table 3 presents the segmentation experiment results. On every data set our model outperforms the BayesSeg and U&I baselines by a substantial margin regardless of K. This result provides strong evidence that learning connected topic models over related documents leads to improved segmentation performance.\nThe best performance is generally obtained by the full version of our model, with three exceptions. In two cases (CitiesEn with K = 10 using clean headings on the WindowDiff metric, and CitiesFr with K = 10 on the Pk metric), the variant that performs better than the full model only does so by a minute margin. Furthermore, in both of those instances, the corresponding evaluation with K = 20 using the full model leads to the best overall results for the respective domains.\nThe only case when a variant outperforms our full model by a notable margin is the Phones data set. This result is not unexpected given the formulaic nature of this dataset as discussed earlier."}, {"heading": "6.4 Ordering", "text": "The final task on which we evaluate our model is that of finding a coherent ordering of a set of textual units. Unlike the previous tasks, where prediction is based on hidden variable distributions, ordering is observed in a document. Moreover, the GMM model uses this information during the inference process. Therefore, we need to divide our data sets into training and test portions.\nIn the past, ordering algorithms have been applied to textual units of various granularities, most commonly sentences and paragraphs. Our ordering experiments operate at the level of a relatively larger unit \u2014 sections. We believe that this granularity is suitable to the nature of our model, because it captures patterns at the level of topic distributions rather than local discourse constraints. The ordering of sentences and paragraphs has been studied in the past (Karamanis et al., 2004; Barzilay & Lapata, 2008) and these two types of models can be effectively combined to induce a full ordering (Elsner et al., 2007).\n16. http://groups.csail.mit.edu/rbg/code/bayesseg/ 17. http://www2.nict.go.jp/x/x161/members/mutiyama/software.html#textseg"}, {"heading": "6.4.1 Ordering Evaluation Setup", "text": "Training and Test Data Sets We use the CitiesEn, CitiesFr and Phones data sets as training documents for parameter estimation as described in Section 5. We introduce additional sets of documents from the same domains as test sets. Table 4 provides statistics on the training and test set splits (note that out-of-vocabulary terms in the test sets are discarded).18\nEven though we perform ordering at the section level, these collections still pose a challenging ordering task: for example, the average number of sections in a CitiesEn test document is 11.2, comparable to the 11.5 sentences (the unit of reordering) per document of the National Transportation Safety Board corpus used in previous work (Barzilay & Lee, 2004; Elsner et al., 2007).\nMetrics We report the Kendall\u2019s \u03c4 rank correlation coefficient for our ordering experiments. This metric measures how much an ordering differs from the reference order \u2014 the underlying assumption is that most reasonable sentence orderings should be fairly similar to it. Specifically, for a permutation \u03c0 of the sections in an N -section document, \u03c4(\u03c0) is computed as\n\u03c4(\u03c0) = 1\u2212 2d(\u03c0, \u03c3)( N 2 ) , (16) where d(\u03c0, \u03c3) is, as before, the Kendall \u03c4 distance, the number of swaps of adjacent textual units necessary to rearrange \u03c0 into the reference order. The metric ranges from -1 (inverse orders) to 1 (identical orders). Note that a random ordering will yield a zero score in expectation. This measure has been widely used for evaluating information ordering (Lapata, 2003; Barzilay & Lee, 2004; Elsner et al., 2007) and has been shown to correlate with human assessments of text quality (Lapata, 2006).\nBaselines and Model Variants Our ordering method is compared against the original HMM-based content modeling approach of Barzilay and Lee (2004). This baseline delivers\n18. The Elements data set is limited to 118 articles, preventing us from splitting it into reasonably sized training and test sets. Therefore we do not consider it for our ordering experiments. For the Citiesrelated sets, the test documents are shorter because they were about cities of lesser population. On the other hand, for Phones the test set does not include short \u201cexpress\u201d reviews and thus exhibits higher average document length.\nstate-of-the art performance in a number of datasets and is similar in spirit to our model \u2014 it also aims to capture patterns at the level of topic distribution (see Section 2). Again, we use the publicly available implementation19 with parameters adjusted according to the values used in their previous work. This content modeling implementation provides an A* search procedure that we use to find the optimal permutation.\nWe do not include in our comparison local coherence models (Barzilay & Lapata, 2008; Elsner et al., 2007). These models are designed for sentence-level analysis \u2014 in particular, they use syntactic information and thus cannot be directly applied for section-level ordering. As we state above, these models are orthogonal to topic-based analysis; combining the two approaches is a promising direction for future work.\nNote that the Uniform model variant is not applicable to this task, since it does not make any claims to a preferred underlying topic ordering. In fact, from a document likelihood perspective, for any proposed paragraph order the reverse order would have the same probability under the Uniform model. Thus, the only model variant we consider here is Constrained."}, {"heading": "6.4.2 Ordering Results", "text": "Table 5 summarizes ordering results for the GMM- and HMM-based content models. Across all data sets, our model outperforms content modeling by a very large margin. For instance, on the CitiesEn dataset, the gap between the two models reaches 35%. This difference is expected. In previous work, content models were applied to short formulaic texts. In contrast, documents in our collection exhibit higher variability than the original collections. The HMM does not provide explicit constraints on generated global orderings. This may prevent it from effectively learning non-local patterns in topic organization.\nWe also observe that the Constrained variant outperforms our full model. While the difference between the two is small, it is fairly consistent across domains. Since it is not possible to predict idiosyncratic variations in the test documents\u2019 topic orderings, a more constrained model can better capture the prevalent ordering patterns that are consistent across the domain.\n19. http://people.csail.mit.edu/regina/code.html"}, {"heading": "6.5 Discussion", "text": "Our experiments with the three separate tasks reveal some common trends in the results. First, we observe that our single unified model of document structure can be readily and successfully applied to multiple discourse-level tasks, whereas previous work has proposed separate approaches for each task. This versatility speaks to the power of our topic-driven representation of document structure. Second, within each task our model outperforms state-of-the-art baselines by substantial margins across a wide variety of evaluation scenarios. These results strongly support our hypothesis that augmenting topic models with discourse-level constraints broadens their applicability to discourse-level analysis tasks.\nLooking at the performance of our model across different tasks, we make a few notes about the importance of the individual topic constraints. Topic contiguity is a consistently important constraint, allowing both of our model variants to outperform alternative baseline approaches. In most cases, introducing a bias toward similar topic ordering, without requiring identical orderings, provides further benefits when encoded in the model. Our more flexible models achieve superior performance in the segmentation and alignment tasks. In the case of ordering, however, this extra flexibility does not pay off, as the model distributes its probability mass away from strong ordering patterns likely to occur in unseen data.\nWe can also identify the properties of a dataset that most strongly affect the performance of our model. The Constrained model variant performs slightly better than our full model on rigidly formulaic domains, achieving highest performance on the Phones data set. When we know a priori that a domain is formulaic in structure, it is worthwhile to choose the model variant that suitably enforces formulaic topic orderings. Fortunately, this adaptation can be achieved in the proposed framework using the prior of the Generalized Mallows Model \u2014 recall that the Constrained variant is a special case of the full model.\nHowever, the performance of our model is invariant with respect to other data set characteristics. Across the two languages we considered, the model and baselines exhibit the same comparative performance for each task. Moreover, this consistency also holds between the general-interest cities articles and the highly technical chemical elements articles. Finally, between the smaller CitiesEn and larger CitiesEn500 data sets, we observe that our results are consistent."}, {"heading": "7. Conclusions and Future Work", "text": "In this paper, we have shown how an unsupervised topic-based approach can capture content structure. Our resulting model constrains topic assignments in a way that requires global modeling of entire topic sequences. We showed that the Generalized Mallows Model is a theoretically and empirically appealing way of capturing the ordering component of this topic sequence. Our results demonstrate the importance of augmenting statistical models of text analysis with structural constraints motivated by discourse theory. Furthermore, our success with the GMM suggests that it could be applied to the modeling of ordering constraints in other NLP applications.\nThere are multiple avenues of future extensions to this work. First, our empirical results demonstrated that for certain domains providing too much flexibility in the model may in fact be detrimental to predictive accuracy. In those cases, a more tightly constrained variant of our model yields superior performance. An interesting extension of our current\nmodel would be to allow additional flexibility in the prior of the GMM by drawing it from another level of hyperpriors. From a technical perspective, this form of hyperparameter re-estimation would involve defining an appropriate hyperprior for the Generalized Mallows Model and adapting its estimation into our present inference procedure.\nAdditionally, there may be cases when the assumption of one canonical topic ordering for an entire corpus is too limiting, e.g., if a data set consists of topically related articles from multiple sources, each with its own editorial standards. Our model can be extended to allow for multiple canonical orderings by positing an additional level of hierarchy in the probabilistic model, i.e., document structures can be generated from a mixture of several Generalized Mallows Models, each with its own distributional mode. In this case, the model would take on the additional burden of learning how topics are permuted between these multiple canonical orderings. Such a change to the model would greatly complicate inference as re-estimating a Generalized Mallows Model canonical ordering is in general NPhard. However, recent advances in statistics have produced efficient approximate algorithms with theoretically guaranteed correctness bounds (Ailon, Charikar, & Newman, 2008) and exact methods that are tractable for typical cases (Meila\u0306 et al., 2007).\nMore generally, the model presented in this paper assumes two specific global constraints on content structure. While domains that satisfy these constraints are plentiful, there are domains where our modeling assumptions do not hold. For example, in dialogue it is well known that topics recur throughout a conversation (Grosz & Sidner, 1986), thereby violating our first constraint. Nevertheless, texts in such domains still follow certain organizational conventions, e.g. the stack structure for dialogue. Our results suggest that explicitly incorporating domain-specific global structural constraints into a content model would likely improve the accuracy of structure induction.\nAnother direction of future work is to combine the global topic structure of our model with local coherence constraints. As previously noted, our model is agnostic toward the relationships between sentences within a single topic. In contrast, models of local coherence take advantage of a wealth of additional knowledge, such as syntax, to make decisions about information flow across adjoining sentences. Such a linguistically rich model would provide a powerful representation of all levels of textual structure, and could be used for an even greater variety of applications than we have considered here.\nBibliographic Note\nPortions of this work were previously presented in a conference publication (Chen, Branavan, Barzilay, & Karger, 2009). This article significantly extends our previous work, most notably by introducing a new algorithm for applying our model\u2019s output to the information ordering task (Section 5) and considering new data sets for our experiments that vary in genre, language, and size (Section 6)."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the funding support of the NSF CAREER grant IIS-0448168 and grant IIS-0712793, the NSF Graduate Fellowship, the Office of Naval Research, Quanta, Nokia, and the Microsoft Faculty Fellowship. We thank the many people who offered\nsuggestions and comments on this work, including Michael Collins, Aria Haghighi, Yoong Keok Lee, Marina Meila\u0306, Tahira Naseem, Christy Sauper, David Sontag, Benjamin Snyder, and Luke Zettlemoyer. We are especially grateful to Marina Meila\u0306 for introducing us to the Mallows model. This paper also greatly benefited from the thoughtful feedback of the anonymous reviewers. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations."}], "references": [{"title": "Aggregating inconsistent information: Ranking and clustering", "author": ["N. Ailon", "M. Charikar", "A. Newman"], "venue": "Journal of the ACM,", "citeRegEx": "Ailon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2008}, {"title": "Computing locally coherent discourses", "author": ["E. Althaus", "N. Karamanis", "A. Koller"], "venue": "In Proceedings of ACL", "citeRegEx": "Althaus et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Althaus et al\\.", "year": 2004}, {"title": "Remembering: a study in experimental and social psychology", "author": ["F.C. Bartlett"], "venue": "Cambridge University Press.", "citeRegEx": "Bartlett,? 1932", "shortCiteRegEx": "Bartlett", "year": 1932}, {"title": "Sentence alignment for monolingual comparable corpora", "author": ["R. Barzilay", "N. Elhadad"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Barzilay and Elhadad,? \\Q2003\\E", "shortCiteRegEx": "Barzilay and Elhadad", "year": 2003}, {"title": "Inferring strategies for sentence ordering in multidocument news summarization", "author": ["R. Barzilay", "N. Elhadad", "K. McKeown"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Barzilay et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2002}, {"title": "Modeling local coherence: An entity-based approach", "author": ["R. Barzilay", "M. Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Barzilay and Lapata,? \\Q2008\\E", "shortCiteRegEx": "Barzilay and Lapata", "year": 2008}, {"title": "Catching the drift: Probabilistic content models, with applications to generation and summarization", "author": ["R. Barzilay", "L. Lee"], "venue": "In Proceedings of NAACL/HLT", "citeRegEx": "Barzilay and Lee,? \\Q2004\\E", "shortCiteRegEx": "Barzilay and Lee", "year": 2004}, {"title": "Information fusion in the context of multi-document summarization", "author": ["R. Barzilay", "K. McKeown", "M. Elhadad"], "venue": "In Proceedings of ACL", "citeRegEx": "Barzilay et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 1999}, {"title": "Statistical models for text segmentation", "author": ["D. Beeferman", "A. Berger", "J.D. Lafferty"], "venue": "Machine Learning,", "citeRegEx": "Beeferman et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Beeferman et al\\.", "year": 1999}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "Springer.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Topic segmentation with an aspect hidden markov model", "author": ["D.M. Blei", "P.J. Moreno"], "venue": "In Proceedings of SIGIR", "citeRegEx": "Blei and Moreno,? \\Q2001\\E", "shortCiteRegEx": "Blei and Moreno", "year": 2001}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A bottom-up approach to sentence ordering for multi-document summarization", "author": ["D. Bollegala", "N. Okazaki", "M. Ishizuka"], "venue": "In Proceedings of ACL/COLING", "citeRegEx": "Bollegala et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bollegala et al\\.", "year": 2006}, {"title": "Global models of document structure using latent permutations", "author": ["H. Chen", "S. Branavan", "R. Barzilay", "D.R. Karger"], "venue": "In Proceedings of NAACL/HLT", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Statistical significance of MUC-6 results", "author": ["N. Chinchor"], "venue": "Proceedings of the 6th Conference on Message Understanding.", "citeRegEx": "Chinchor,? 1995", "shortCiteRegEx": "Chinchor", "year": 1995}, {"title": "Learning to order things", "author": ["W.W. Cohen", "R.E. Schapire", "Y. Singer"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cohen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 1999}, {"title": "Bayesian unsupervised topic segmentation", "author": ["J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Eisenstein and Barzilay,? \\Q2008\\E", "shortCiteRegEx": "Eisenstein and Barzilay", "year": 2008}, {"title": "A unified local and global model for discourse coherence", "author": ["M. Elsner", "J. Austerweil", "E. Charniak"], "venue": "In Proceedings of NAACL/HLT", "citeRegEx": "Elsner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Elsner et al\\.", "year": 2007}, {"title": "Distance based ranking models", "author": ["M. Fligner", "J. Verducci"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Fligner and Verducci,? \\Q1986\\E", "shortCiteRegEx": "Fligner and Verducci", "year": 1986}, {"title": "Posterior probabilities for a consensus", "author": ["M.A. Fligner", "J.S. Verducci"], "venue": "ordering. Psychometrika,", "citeRegEx": "Fligner and Verducci,? \\Q1990\\E", "shortCiteRegEx": "Fligner and Verducci", "year": 1990}, {"title": "Discourse segmentation of multi-party conversation", "author": ["M. Galley", "K.R. McKeown", "E. Fosler-Lussier", "H. Jing"], "venue": "In Proceedings of ACL", "citeRegEx": "Galley et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2003}, {"title": "Stochastic relaxation, gibbs distributions and the bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Geman and Geman,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman", "year": 1984}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Griffiths and Steyvers,? \\Q2004\\E", "shortCiteRegEx": "Griffiths and Steyvers", "year": 2004}, {"title": "Integrating topics and syntax", "author": ["T.L. Griffiths", "M. Steyvers", "D.M. Blei", "J.B. Tenenbaum"], "venue": "In Advances in NIPS", "citeRegEx": "Griffiths et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Griffiths et al\\.", "year": 2005}, {"title": "Attention, intentions, and the structure of discourse", "author": ["B.J. Grosz", "C.L. Sidner"], "venue": "Computational Linguistics,", "citeRegEx": "Grosz and Sidner,? \\Q1986\\E", "shortCiteRegEx": "Grosz and Sidner", "year": 1986}, {"title": "Hidden topic markov models", "author": ["A. Gruber", "M. Rosen-Zvi", "Y. Weiss"], "venue": "In Proceedings of AISTATS", "citeRegEx": "Gruber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "Multi-paragraph segmentation of expository text", "author": ["M. Hearst"], "venue": "Proceedings of ACL.", "citeRegEx": "Hearst,? 1994", "shortCiteRegEx": "Hearst", "year": 1994}, {"title": "Sentence ordering with manifold-based classification in multi-document summarization", "author": ["P.D. Ji", "S. Pulman"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Ji and Pulman,? \\Q2006\\E", "shortCiteRegEx": "Ji and Pulman", "year": 2006}, {"title": "Evaluating centeringbased metrics of coherence for text structuring using a reliably annotated corpus", "author": ["N. Karamanis", "M. Poesio", "C. Mellish", "J. Oberlander"], "venue": "In Proceedings of ACL", "citeRegEx": "Karamanis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Karamanis et al\\.", "year": 2004}, {"title": "Unsupervised rank aggregation with distancebased models", "author": ["A. Klementiev", "D. Roth", "K. Small"], "venue": "In Proceedings of the ICML,", "citeRegEx": "Klementiev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2008}, {"title": "Probabilistic text structuring: Experiments with sentence ordering", "author": ["M. Lapata"], "venue": "Proceedings of ACL.", "citeRegEx": "Lapata,? 2003", "shortCiteRegEx": "Lapata", "year": 2003}, {"title": "Automatic evaluation of information ordering: Kendall\u2019s tau", "author": ["M. Lapata"], "venue": "Computational Linguistics, 32 (4), 471\u2013484.", "citeRegEx": "Lapata,? 2006", "shortCiteRegEx": "Lapata", "year": 2006}, {"title": "Cranking: combining rankings using conditional probability models on permutations", "author": ["G. Lebanon", "J. Lafferty"], "venue": "In Proceedings of ICML", "citeRegEx": "Lebanon and Lafferty,? \\Q2002\\E", "shortCiteRegEx": "Lebanon and Lafferty", "year": 2002}, {"title": "Minimum cut model for spoken lecture segmentation", "author": ["I. Malioutov", "R. Barzilay"], "venue": "In Proceedings of ACL", "citeRegEx": "Malioutov and Barzilay,? \\Q2006\\E", "shortCiteRegEx": "Malioutov and Barzilay", "year": 2006}, {"title": "Non-null ranking models", "author": ["C.L. Mallows"], "venue": "Biometrika, 44, 114\u2013130.", "citeRegEx": "Mallows,? 1957", "shortCiteRegEx": "Mallows", "year": 1957}, {"title": "Consensus ranking under the exponential model", "author": ["M. Meil\u0103", "K. Phadnis", "A. Patterson", "J. Bilmes"], "venue": "In Proceedings of UAI", "citeRegEx": "Meil\u0103 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Meil\u0103 et al\\.", "year": 2007}, {"title": "Slice sampling", "author": ["R.M. Neal"], "venue": "Annals of Statistics, 31, 705\u2013767.", "citeRegEx": "Neal,? 2003", "shortCiteRegEx": "Neal", "year": 2003}, {"title": "Towards robust context-sensitive sentence alignment for monolingual corpora", "author": ["R. Nelken", "S.M. Shieber"], "venue": "In Proceedings of EACL", "citeRegEx": "Nelken and Shieber,? \\Q2006\\E", "shortCiteRegEx": "Nelken and Shieber", "year": 2006}, {"title": "Computer Intensive Methods for Testing Hypotheses", "author": ["E.W. Noreen"], "venue": "An Introduction. Wiley.", "citeRegEx": "Noreen,? 1989", "shortCiteRegEx": "Noreen", "year": 1989}, {"title": "A critique and improvement of an evaluation metric for text segmentation", "author": ["L. Pevzner", "M.A. Hearst"], "venue": "Computational Linguistics,", "citeRegEx": "Pevzner and Hearst,? \\Q2002\\E", "shortCiteRegEx": "Pevzner and Hearst", "year": 2002}, {"title": "Fast collapsed gibbs sampling for latent dirichlet allocation", "author": ["I. Porteous", "D. Newman", "A. Ihler", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "In Proceedings of SIGKDD", "citeRegEx": "Porteous et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Porteous et al\\.", "year": 2008}, {"title": "Unsupervised topic modelling for multi-party spoken discourse", "author": ["M. Purver", "K. K\u00f6rding", "T.L. Griffiths", "J.B. Tenenbaum"], "venue": "In Proceedings of ACL/COLING", "citeRegEx": "Purver et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Purver et al\\.", "year": 2006}, {"title": "Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation and user studies", "author": ["D. Radev", "H. Jing", "M. Budzikowska"], "venue": "In Proceedings of ANLP/NAACL Summarization Workshop", "citeRegEx": "Radev et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Radev et al\\.", "year": 2000}, {"title": "On some pitfalls in automatic evaluation and significance testing for MT. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization", "author": ["S. Riezler", "J.T. Maxwell"], "venue": null, "citeRegEx": "Riezler and Maxwell,? \\Q2005\\E", "shortCiteRegEx": "Riezler and Maxwell", "year": 2005}, {"title": "The Handbook of Discourse", "author": ["D. Schiffrin", "D. Tannen", "H.E. Hamilton"], "venue": null, "citeRegEx": "Schiffrin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Schiffrin et al\\.", "year": 2001}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["I. Titov", "R. McDonald"], "venue": "In Proceedings of WWW", "citeRegEx": "Titov and McDonald,? \\Q2008\\E", "shortCiteRegEx": "Titov and McDonald", "year": 2008}, {"title": "A statistical model for domain-independent text segmentation", "author": ["M. Utiyama", "H. Isahara"], "venue": "In Proceedings of ACL", "citeRegEx": "Utiyama and Isahara,? \\Q2001\\E", "shortCiteRegEx": "Utiyama and Isahara", "year": 2001}, {"title": "Text segmentation and topic tracking on broadcast news via a hidden markov model approach", "author": ["P. van Mulbregt", "I. Carp", "L. Gillick", "S. Lowe", "J. Yamron"], "venue": "Proceedings of ICSLP", "citeRegEx": "Mulbregt et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Mulbregt et al\\.", "year": 1998}, {"title": "Topic modeling: beyond bag of words", "author": ["H.M. Wallach"], "venue": "Proceedings of ICML.", "citeRegEx": "Wallach,? 2006", "shortCiteRegEx": "Wallach", "year": 2006}, {"title": "Formulaic Language and the Lexicon", "author": ["A. Wray"], "venue": "Cambridge University Press, Cambridge.", "citeRegEx": "Wray,? 2002", "shortCiteRegEx": "Wray", "year": 2002}], "referenceMentions": [{"referenceID": 2, "context": "The second constraint states that documents from the same domain tend to present similar topics in similar orders (Bartlett, 1932; Wray, 2002).", "startOffset": 114, "endOffset": 142}, {"referenceID": 49, "context": "The second constraint states that documents from the same domain tend to present similar topics in similar orders (Bartlett, 1932; Wray, 2002).", "startOffset": 114, "endOffset": 142}, {"referenceID": 47, "context": "As another example of intrasentential constraints, Wallach (2006) proposes a way to incorporate word order information, in the form of bigrams, into an LDA-style model.", "startOffset": 51, "endOffset": 66}, {"referenceID": 23, "context": "This formulation models text structure at the level of word transitions, as opposed to the work of Griffiths et al. (2005) where structure is modeled at the level of hidden syntactic class transitions.", "startOffset": 99, "endOffset": 123}, {"referenceID": 25, "context": "This hypothesis motivated research on models where topic assignment is guided by structural considerations (Purver, K\u00f6rding, Griffiths, & Tenenbaum, 2006; Gruber et al., 2007; Titov & McDonald, 2008), particularly relationships between the topics of adjacent textual units.", "startOffset": 107, "endOffset": 199}, {"referenceID": 25, "context": "In a similar vein, the Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) posits a generative process where each sentence (textual unit) is assigned a single topic, so that all of the sentence\u2019s words are drawn from a single language model.", "startOffset": 56, "endOffset": 77}, {"referenceID": 25, "context": "This hypothesis motivated research on models where topic assignment is guided by structural considerations (Purver, K\u00f6rding, Griffiths, & Tenenbaum, 2006; Gruber et al., 2007; Titov & McDonald, 2008), particularly relationships between the topics of adjacent textual units. Depending on the application, a textual unit may be a sentence, paragraph, or speaker utterance. A common property of these models is that they bias topic assignments to cohere within local segments of text. Models in this category vary in terms of the mechanisms used to encourage local topic coherence. For instance, the model of Purver et al. (2006) biases the topic distributions of adjacent utterances (textual units) to be similar.", "startOffset": 155, "endOffset": 627}, {"referenceID": 30, "context": "The global constraints encoded by our model are closely related to research in discourse on information ordering with applications to text summarization and generation (Barzilay, Elhadad, & McKeown, 2002; Lapata, 2003; Karamanis, Poesio, Mellish, & Oberlander, 2004; Elsner et al., 2007).", "startOffset": 168, "endOffset": 287}, {"referenceID": 17, "context": "The global constraints encoded by our model are closely related to research in discourse on information ordering with applications to text summarization and generation (Barzilay, Elhadad, & McKeown, 2002; Lapata, 2003; Karamanis, Poesio, Mellish, & Oberlander, 2004; Elsner et al., 2007).", "startOffset": 168, "endOffset": 287}, {"referenceID": 30, "context": "Prior work has demonstrated that a wide range of features are useful in these classification decisions (Lapata, 2003; Karamanis et al., 2004; Ji & Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006).", "startOffset": 103, "endOffset": 198}, {"referenceID": 28, "context": "Prior work has demonstrated that a wide range of features are useful in these classification decisions (Lapata, 2003; Karamanis et al., 2004; Ji & Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006).", "startOffset": 103, "endOffset": 198}, {"referenceID": 30, "context": "Since finding such an ordering is NP-hard (Cohen, Schapire, & Singer, 1999), various approximations are used in practice (Lapata, 2003; Althaus et al., 2004).", "startOffset": 121, "endOffset": 157}, {"referenceID": 1, "context": "Since finding such an ordering is NP-hard (Cohen, Schapire, & Singer, 1999), various approximations are used in practice (Lapata, 2003; Althaus et al., 2004).", "startOffset": 121, "endOffset": 157}, {"referenceID": 26, "context": "Prior work has demonstrated that a wide range of features are useful in these classification decisions (Lapata, 2003; Karamanis et al., 2004; Ji & Pulman, 2006; Bollegala, Okazaki, & Ishizuka, 2006). For instance, Lapata (2003) has demonstrated that lexical features, such as verb pairs from the input sentences, serve as a proxy for plausible sequences of actions, and thus are effective predictors of well-formed orderings.", "startOffset": 118, "endOffset": 228}, {"referenceID": 1, "context": "Since finding such an ordering is NP-hard (Cohen, Schapire, & Singer, 1999), various approximations are used in practice (Lapata, 2003; Althaus et al., 2004). While these two-step discriminative approaches can effectively leverage information about local transitions, they do not provide any means for representing global constraints. In more recent work, Barzilay and Lapata (2008) demonstrated that certain global properties can be captured in the discriminative framework using a reranking mechanism.", "startOffset": 136, "endOffset": 383}, {"referenceID": 17, "context": "In more recent work, Elsner et al. (2007) developed a search procedure based on simulated annealing that finds a high likelihood ordering.", "startOffset": 21, "endOffset": 42}, {"referenceID": 34, "context": "We first describe the standard Mallows Model over orderings (Mallows, 1957).", "startOffset": 60, "endOffset": 75}, {"referenceID": 18, "context": "The Generalized Mallows Model, first introduced by Fligner and Verducci (1986), refines the standard Mallows Model by adding an additional set of dispersion parameters.", "startOffset": 51, "endOffset": 79}, {"referenceID": 9, "context": "We accomplish this inference task through Gibbs sampling (Geman & Geman, 1984; Bishop, 2006).", "startOffset": 57, "endOffset": 92}, {"referenceID": 36, "context": "However, the distribution itself is univariate and unimodal, so we can expect that an MCMC technique such as slice sampling (Neal, 2003) should perform well.", "startOffset": 124, "endOffset": 136}, {"referenceID": 22, "context": "This form of caching is the same as what is used by Griffiths and Steyvers (2004).", "startOffset": 52, "endOffset": 82}, {"referenceID": 26, "context": "Previous approaches have typically relied on lexical cohesion \u2014 that is, similarity in word choices within a document subspan \u2014 to guide the choice of segmentation boundaries (Hearst, 1994; van Mulbregt, Carp, Gillick, Lowe, & Yamron, 1998; Blei & Moreno, 2001; Utiyama & Isahara, 2001; Galley, McKeown, FoslerLussier, & Jing, 2003; Purver et al., 2006; Malioutov & Barzilay, 2006; Eisenstein & Barzilay, 2008).", "startOffset": 175, "endOffset": 410}, {"referenceID": 41, "context": "Previous approaches have typically relied on lexical cohesion \u2014 that is, similarity in word choices within a document subspan \u2014 to guide the choice of segmentation boundaries (Hearst, 1994; van Mulbregt, Carp, Gillick, Lowe, & Yamron, 1998; Blei & Moreno, 2001; Utiyama & Isahara, 2001; Galley, McKeown, FoslerLussier, & Jing, 2003; Purver et al., 2006; Malioutov & Barzilay, 2006; Eisenstein & Barzilay, 2008).", "startOffset": 175, "endOffset": 410}, {"referenceID": 17, "context": "In contrast, previous approaches have only been able to rank a small subset of all possible document reorderings (Barzilay & Lapata, 2008), or performed a search procedure through the space of orderings to find an optimum (Elsner et al., 2007).", "startOffset": 222, "endOffset": 243}, {"referenceID": 17, "context": "In contrast, previous approaches have only been able to rank a small subset of all possible document reorderings (Barzilay & Lapata, 2008), or performed a search procedure through the space of orderings to find an optimum (Elsner et al., 2007).7 The objective function of Equation 13 depends on posterior estimates of \u03b2 and \u03b8 given the training documents. Since our collapsed Gibbs sampler integrates out these two hidden variables, we need to back out the values of \u03b2 and \u03b8 from the known posterior samples of z. This can easily be done by computing a point estimate of each distribution based on the word-topic and topic-document assignment frequencies, respectively, as is done by Griffiths and Steyvers (2004). The probability mass \u03b2\u0302w k of word w in the language model of topic k is given by: \u03b2\u0302 k = N\u03b2(k,w) + \u03b20 N\u03b2(k) +W\u03b20 , (14)", "startOffset": 223, "endOffset": 714}, {"referenceID": 38, "context": "Statistical significance in this setup is measured with approximate randomization (Noreen, 1989), a nonparametric test that can be directly applied to nonlinearly computed metrics such as F-score.", "startOffset": 82, "endOffset": 96}, {"referenceID": 14, "context": "This test has been used in prior evaluations for information extraction and machine translation (Chinchor, 1995; Riezler & Maxwell, 2005).", "startOffset": 96, "endOffset": 137}, {"referenceID": 25, "context": "\u2022 Hidden Topic Markov Model (HTMM) (Gruber et al., 2007): As explained in Section 2, this model represents topic change between adjacent textual units in a Markovian fashion.", "startOffset": 35, "endOffset": 56}, {"referenceID": 46, "context": "We also compare our method with the algorithm of Utiyama and Isahara (2001), which is commonly used as a point of reference in the evaluation of segmentation algorithms.", "startOffset": 49, "endOffset": 76}, {"referenceID": 28, "context": "The ordering of sentences and paragraphs has been studied in the past (Karamanis et al., 2004; Barzilay & Lapata, 2008) and these two types of models can be effectively combined to induce a full ordering (Elsner et al.", "startOffset": 70, "endOffset": 119}, {"referenceID": 17, "context": ", 2004; Barzilay & Lapata, 2008) and these two types of models can be effectively combined to induce a full ordering (Elsner et al., 2007).", "startOffset": 117, "endOffset": 138}, {"referenceID": 17, "context": "5 sentences (the unit of reordering) per document of the National Transportation Safety Board corpus used in previous work (Barzilay & Lee, 2004; Elsner et al., 2007).", "startOffset": 123, "endOffset": 166}, {"referenceID": 30, "context": "This measure has been widely used for evaluating information ordering (Lapata, 2003; Barzilay & Lee, 2004; Elsner et al., 2007) and has been shown to correlate with human assessments of text quality (Lapata, 2006).", "startOffset": 70, "endOffset": 127}, {"referenceID": 17, "context": "This measure has been widely used for evaluating information ordering (Lapata, 2003; Barzilay & Lee, 2004; Elsner et al., 2007) and has been shown to correlate with human assessments of text quality (Lapata, 2006).", "startOffset": 70, "endOffset": 127}, {"referenceID": 31, "context": ", 2007) and has been shown to correlate with human assessments of text quality (Lapata, 2006).", "startOffset": 79, "endOffset": 93}, {"referenceID": 6, "context": "Baselines and Model Variants Our ordering method is compared against the original HMM-based content modeling approach of Barzilay and Lee (2004). This baseline delivers", "startOffset": 121, "endOffset": 145}, {"referenceID": 17, "context": "We do not include in our comparison local coherence models (Barzilay & Lapata, 2008; Elsner et al., 2007).", "startOffset": 59, "endOffset": 105}, {"referenceID": 35, "context": "However, recent advances in statistics have produced efficient approximate algorithms with theoretically guaranteed correctness bounds (Ailon, Charikar, & Newman, 2008) and exact methods that are tractable for typical cases (Meil\u0103 et al., 2007).", "startOffset": 224, "endOffset": 244}], "year": 2009, "abstractText": "We present a novel Bayesian topic model for learning discourse-level document structure. Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model. We apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering. Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods.", "creator": "TeX"}}}