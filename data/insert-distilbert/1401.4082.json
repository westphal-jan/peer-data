{"id": "1401.4082", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2014", "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "abstract": "hence we marry with ideas from deep neural networks and approximate bayesian inference to derive a generalised class of deep, directed generative discrimination models, endowed with development a new algorithm for scalable inference and learning. our algorithm introduces a recognition basis model to represent approximate posterior distributions, and that acts collectively as a stochastic encoder of the data. there we develop stochastic weighted back - propagation - - rules algorithms for back - propagation achieved through stochastic dependent variables - - and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative matching and recognition fit model. we demonstrate on several real - world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for providing high - dimensional data quantum visualisation.", "histories": [["v1", "Thu, 16 Jan 2014 16:33:23 GMT  (4873kb,D)", "http://arxiv.org/abs/1401.4082v1", "Under review"], ["v2", "Fri, 9 May 2014 12:53:17 GMT  (33347kb,D)", "http://arxiv.org/abs/1401.4082v2", "Appears in Proceedings of the 31st International Conference on Machine Learning (ICML), Beijing, China, 2014"], ["v3", "Fri, 30 May 2014 10:00:36 GMT  (33346kb,D)", "http://arxiv.org/abs/1401.4082v3", "Appears In Proceedings of the 31st International Conference on Machine Learning (ICML), JMLR: W\\&amp;CP volume 32, 2014"]], "COMMENTS": "Under review", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG stat.CO stat.ME", "authors": ["danilo jimenez rezende", "shakir mohamed", "daan wierstra"], "accepted": true, "id": "1401.4082"}, "pdf": {"name": "1401.4082.pdf", "metadata": {"source": "META", "title": "Stochastic Back-propagation and Variational Inference in  Deep Latent Gaussian Models", "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "emails": ["daan}@deepmind.com"], "sections": [{"heading": "1. Introduction", "text": "There is an immense effort in machine learning and statistics to develop accurate and scalable probabilistic models of data. Such models are called upon whenever we are faced with tasks requiring probabilistic reasoning, such as prediction, missing data imputation and uncertainty estimation; or for those tasks requiring the ability to quickly generate samples from the model, allowing for confabulation and simulation-based analysis used in many scientific fields such as genetics, robotics and control.\nRecent efforts to develop generative models have focused on directed models, since samples are easily obtained by ancestral sampling from the generative process. Directed models such as belief networks and sim-\nArXiv preprint. Copyright 2014 by the author(s).\nilar latent variable models (Dayan et al., 1995; Frey, 1996; Saul et al., 1996; Bartholomew & Knott, 1999; Uria et al., 2013; Gregor et al., 2013) can be easily sampled from, but in most cases, efficient inference algorithms have remained elusive. These efforts, combined with the demand for accurate probabilistic inferences and fast simulation lead us to seek generative models that are i) deep, since hierarchical structure allows us to capture higher moments of the data, ii) are non-linear, allowing for complex structure in the data to be modelled, iii) allow for fast sampling of fantasy data from the inferred model, and iv) are tractable and scalable to large amounts of data.\nWe meet these desiderata by introducing a class of deep, directed generative models with Gaussian latent variables at each layer. To allow for efficient and tractable inference, we also introduce a corresponding recognition model, which can be interpreted as a stochastic encoder of the data, and represents an approximate posterior distribution. For the generative model, we derive the objective function for optimisation using variational principles; for the recognition model, we specify its structure and regularisation by exploiting recent advances in deep learning. Using this construction, we can train the entire model by a modified gradient back-propagation, which allows for optimisation of the parameters of both the generative and recognition models jointly.\nWe build upon the large body of prior work (contextualised in section 5) and make the following contributions: \u2022 We marry ideas from deep neural networks and\nprobabilistic latent variable modelling to derive a general class of deep, non-linear latent Gaussian models (section 2). \u2022 We present a new approach for scalable variational inference that allows for joint optimisation of both variational and model parameters by exploiting the properties of latent Gaussian distributions and gradient back-propagation (sections 3 and 4).\nar X\niv :1\n40 1.\n40 82\nv1 [\nst at\n.M L\n] 1\n6 Ja\nn 20\n14\n\u2022 We provide a comprehensive and systematic evaluation of the model demonstrating its applicability to problems in simulation, visualisation, prediction and missing data imputation (section 6)."}, {"heading": "2. Deep Latent Gaussian Models", "text": "Deep latent Gaussian models (DLGM) are a general class of deep directed graphical models that consist of Gaussian latent variables at each layer of a processing hierarchy. The model consists of L layers of latent variables. To generate a sample from the model, we begin at the top-most layer (L) by drawing from a Gaussian distribution. The activation hl at any lower layer is formed by a non-linear transformation of the layer above hl+1, perturbed by Gaussian noise. We then generate observations v by sampling from the likelihood using the activation h1. This process is described graphically in figure 1.\nThis generative process is described by the following equations:\n\u03bel = N (\u03be|0, I), l = 1, . . . , L (1) hL = GL\u03beL, (2)\nhl = Tl(hl+1) + Gl\u03bel, l = 1 . . . L\u2212 1 (3) v \u223c \u03c0(v|T0(h1)), (4)\nwhere \u03bel are Gaussian variables. The transformations Tl are represented by multi-layer perceptrons (MLPs) and Gl are matrices. At the visible layer, the data is generated from any appropriate distribution \u03c0(v|\u00b7) whose parameters are specified by a transformation of the first latent layer. This allows us to deal with data that may be of any type, such as binary, categorical, counts, real-values, or a heterogeneous combination of these data types. We typically make use of distributions in the exponential family, such as the Bernoulli distribution for binary data, but we are not restricted to this set of likelihood functions.\nThe joint probability distribution of this model can be expressed in two equivalent ways:\np(v,h) = p(v|h1)p(hL)p(\u03b8g) L\u22121\u220f l=1 pl(hl|hl+1), (5)\np(v, \u03be) = p(v|h1(\u03be1...L))p(\u03b8 g) L\u220f l=1 N (\u03bel). (6)\nA graphical model corresponding to these two views is shown in figure 1(a),(b). The conditional distribution p(hl|hl+1) is a distribution implicitly defined by equation (3) and are Gaussian distributions with mean \u00b5l = Tl(hl+1) and covariance Sl = GlG > l . Equation\n(6) makes explicit that this generative model works by applying a highly non-linear transformation to a spherical Gaussian distribution such that the transformed distribution tries to match the empirical distribution. We discuss this in more detail in appendix A. Throughout the paper we refer to the set of parameters in Tl and Gl by \u03b8\ng. We adopt a weak Gaussian prior over \u03b8g, p(\u03b8g) = N (\u03b8|0, \u03baI).\nThis specification for deep latent Gaussian models generalises a number of well known models. When we have only one layer of latent variables and use a linear mapping T (\u00b7), we recover factor analysis (Bartholomew & Knott, 1999) \u2013 more general mappings allow for a nonlinear factor analysis. When the mappings are of the form Tl(h) = Alf(h) + bl, for simple element-wise non-linearities f such as the probit function or the rectified linearity, we recover the non-linear Gaussian belief network (Frey & Hinton, 1999). We describe the relationship to other existing models in section 5. Given this specification, our key task is to develop a method for tractable inference. A number of approaches are known and currently used, in particular algorithms based on mean-field variational EM (Beal, 2003), the wake-sleep algorithm (Dayan, 2000), policygradient methods such as REINFORCE (Williams, 1992), or stochastic gradient methods (Hoffman et al., 2012). We develop an alternative to these inference algorithms that overcomes many of their limitations, and that is both scalable and efficient."}, {"heading": "3. Stochastic Back-propagation", "text": "The key tool we develop to allow for efficient inference is the set of rules to allow for gradient computations\nin models with random variables. We develop these identities and refer to them as the rules for stochastic back-propagation.\nGradient descent methods in latent variable models typically require computations of the form: \u2207\u03b8Eq\u03b8 [f(\u03be)], where \u03b8 is a set of parameters of a distribution q\u03b8(\u03be) and f is a loss function, which we assume to be integrable and smooth. Of special interest here are cases where the distribution q is a K-dimensional Gaussian N (\u03be|\u00b5,C) with parameters \u03b8 = {\u00b5,C}. In this setting, gradients can be computed using the Gaussian gradient identities:\n\u2207\u00b5iEN (\u00b5,C) [f(\u03be)] = EN (\u00b5,C) [\u2207\u03beif(\u03be)] (7)\n\u2207CijEN (\u00b5,C) [f(\u03be)] = 1\n2 EN (\u00b5,C)\n[ \u22072\u03bei,\u03bejf(\u03be) ] , (8)\nwhich are due to the theorems by Bonnet (1964) and Price (1958), respectively. This allows us to write a general rule for Gaussian gradient computation.\nGaussian back-propagation (GBP). Combining equations (7) and (8) and using the chain rule we can derive the full gradient\n\u2207\u03b8EN (\u00b5,C)[f(\u03be)]=EN (0,I) [ g> \u2202\u00b5\n\u2202\u03b8 + 12Tr\n( H \u2202C\n\u2202\u03b8\n)] , (9)\nwhere g and H are the gradient and the Hessian of the function f(\u03be), respectively. Equation (9) can be interpreted as a modified back-propagation rule for Gaussian distributions that takes into account the gradients through the mean and covariance and that reduces to the standard back-propagation rule when C is constant. Unfortunately this rule requires knowledge of the Hessian matrix of f(\u03be), which has an algorithmic complexity O(K3). For inference in DLGMs, we later introduce an unbiased though higher variance estimator that requires only quadratic complexity.\nWe can also derive general back-propagation rules for q-distributions other than the Gaussian. Such rules can be derived in two ways:"}, {"heading": "1. Using the product rule for integrals.", "text": "For many exponential family distributions, it is possible to use the product rule for integrals to express the gradient with respect to the parameters (mean or natural) as an expectation of gradients with respect to the random variables itself:\n\u2207\u03b8Ep(\u03be|\u03b8)[f(\u03be)]== \u2212Ep(\u03be|\u03b8)[\u2207\u03be[B(\u03be)f(\u03be)]] (10)\nWe can thus reduce the problem to searching for a suitable transformation function B(\u03be) that allows us\nto compute the required derivatives directly. This approach can be used to derive rules for many other distributions e.g., the Gaussian, inverse Gamma, logNormal and Wald distributions. We discuss this in more detail in appendix B."}, {"heading": "2. Using suitable co-ordinate transformations.", "text": "We can also derive stochastic back-propagation rules for any distribution that can be written as an smooth, invertible transformation of a canonical base distribution. For example, any Gaussian distribution N (\u00b5,C) can be obtained as transformation of a spherical Gaussian \u223c N (0, I), using the transformation y = \u00b5+R and C = RR>. This co-ordinate transformation allows the gradient of the expectation with respect to R to be written as:\n\u2207REN(\u00b5,C)[f(\u03be)] =\u2207REN(0,I)[f(\u00b5+R )] =EN(0,I) [ g> ] , (11)\nwhere g is the gradient of f evaluated at \u00b5 + R and provides a lower-cost alternative to Price\u2019s theorem (8). The estimator (11) will in general have higher variance than the estimators based on (8). A short analysis of the variance of these estimators is discussed section 5 and in appendix C.\nThis approach can be more general than the first approach, and such transformations are well known for many distributions, especially those with a selfsimilarity property or location-scale formulations, such as the Gaussian, Student\u2019s t-distribution, the class of stable distributions, and the class of generalised extreme value distributions. We provide further discussion in appendix B."}, {"heading": "4. Scalable Inference in DLGMs", "text": ""}, {"heading": "4.1. Free Energy Objective", "text": "To perform inference in DLGMs we integrate out the effect of the latent variables, requiring us to compute the integrated or marginal likelihood. In general, this will be an intractable integration and instead we optimise a lower bound on the marginal likelihood. We introduce an approximate posterior distribution q and applying Jensen\u2019s inequality, following the variational principle (Zemel, 1993; Beal, 2003) to obtain:\nlog p(V) = log \u222b p(V|\u03be,\u03b8g)p(\u03be,\u03b8g)d\u03be\n= log\n\u222b q(\u03be)\nq(\u03be) p(V|\u03be,\u03b8g)p(\u03be,\u03b8g)d\u03be (12)\n\u2265L(V)=\u2212DKL[q(\u03be)\u2016p(\u03be)]+Eq [log p(V|\u03be,\u03b8g)p(\u03b8g)] .\nWe use the variational free energy, defined as F(V) = \u2212L(V), as our objective function. This objective con-\nsists of two terms: the first is the KL-divergence between the variational distribution and the prior distribution (which acts a regulariser), and the second is a reconstruction error. We refer to the approximate posterior distribution q(\u03be) as a recognition model, and its design is important for the success of such variational methods.\nWe use a variational distribution q(\u03be) that factorises across the L layers, but not necessarily within a layer:\nq(\u03be|V,\u03b8r) = N\u220f n=1 L\u220f l=1 N ( \u03ben,l|\u00b5l(vn),Cl(vn) ) , (13)\nwhere \u00b5l(v) and Cl(v) are generic maps represented by MLPs corresponding to the mean vector and covariance matrix of the units in layer l respectively. We refer to the parameters of the recognition model q(\u03be|v) as \u03b8r.\nFor a Gaussian prior and a Gaussian recognition model, the KL term in the free energy (12) can be computed analytically and the free energy becomes:\nF(V) = \u2212 \u2211 n Eq [log p(vn|h(\u03ben))] + 12\u03ba\u2016\u03b8 g\u20162\n+ 1\n2 \u2211 n,l [ \u2016\u00b5n,l\u20162+Tr(Cn,l)\u2212log |Cn,l|\u22121 ] ,\n(14)\nwhere Tr(C) and |C| indicate the trace and the determinant of the covariance matrix C, respectively."}, {"heading": "4.2. Parameterising the Recognition Covariance", "text": "There are a number of approaches for parameterising the covariance matrix of the recognition model. Maintaining a full covariance matrix C in equation (14) would entail an algorithmic complexity O(LK3) where K is the number of latent variables per layer and L is the number of latent layers.\nThe simplest approach is to use a diagonal covariance matrix C = diag(d), where d is a K-dimensional vector. This approach is appealing since it allows for linear-time computation and sampling, but only allows for axis-aligned posterior distributions.\nWe can improve upon the diagonal approximation by considering a structured covariance parameterised by two independent vectors d and u. We parameterise the precision matrix C\u22121 (Magdon-Ismail & Purnell, 2010):\nC\u22121 = D + uuT , (15)\nwhere D = diag(d). This representation allows for arbitrary rotations of the Gaussian distribution along one principal direction, with relatively few additional parameters.\nBy application of the matrix inversion lemma, we obtain the covariance matrix in terms of d and u as:\nC = D\u22121 + \u03b7D\u22121uuTD\u22121, \u03b7 = 1 uTD\u22121u+1 , log |C| = log \u03b7 \u2212 log |D|. (16)\nThis allows both the trace Tr(C) and the log |C| needed in the computation of the Gaussian KL to be computed in O(KL) time.\nIn addition, we can factorise the covariance matrix as the product of two matrices C = RRT . R is a square matrix of the same size as C and can be computed directly in terms of the vectors d and u. One solution for R is:\nR = D\u2212 1 2 \u2212 [ 1\u2212\u221a\u03b7 uTD\u22121u ] D\u22121uuTD\u2212 1 2 . (17)\nDue to this structure, the product of R with an arbitrary vector can be computed in O(K) without having to compute R explicitly. This allows us to also sample efficiently from this low-rank Gaussian, since any Gaussian random variable \u03be with mean \u00b5 and covariance matrix C = RRT can be written as \u03be = \u00b5+R , where is a standard Gaussian variate."}, {"heading": "4.3. Gradients of the Free Energy", "text": "We wish to minimise the free energy F(V) (14) using stochastic gradient descent methods. For this, we need efficient estimators of the gradients of all terms in (14) with respect to both the parameters of the generative model \u03b8g and the recognition model \u03b8r.\nThe gradients with respect to the jth generative parameter \u03b8gj can computed using:\n\u2207\u03b8gjF(V) = \u2212Eq [ \u2207\u03b8gj log p(V|h) ] + 1 \u03ba \u03b8gj . (18)\nAn unbiased estimator of \u2207\u03b8gjF(V) is obtained by approximating (18) with a small number of samples from q (or even a single sample).\nTo obtain gradients with respect to the recognition parameters \u03b8r, we use the rules for Gaussian backpropagation developed in section 3. To address the complexity of the Hessian in the general rule (9), we use the co-ordinate transformation for the Gaussian to write the gradient with respect to the matrix R instead of the covariance C (recalling C = RR>) derived in\nequation (11), where derivatives are computed for the function f(\u03be) = log p(v|h(\u03be)).\nThe gradients of F(v) with respect to the variational mean \u00b5l(v) and the matrices Rl(v) are given by\n\u2207\u00b5lF(v) = \u2212Eq [ \u2207\u03bel log p(v|h(\u03be)) ] + \u00b5l (19)\n\u2207Rl,i,jF(v) = \u2212 1 2 Eq [ l,j\u2207\u03bel,i log p(v|h(\u03be)) ] + 1\n2 \u2207Rl,i,j [TrCn,l \u2212 log |Cn,l|] , (20)\nwhere the gradients \u2207Rl,i,j [TrCn,l \u2212 log |Cn,l|] are computed by back-propagation.\nUnbiased estimators of the gradients (19) and (20) are then obtained jointly by sampling from the recognition model \u03be \u223c q(\u03be|v) (bottom-up pass) and updating the values of the generative model layers using equation (3) (top-down pass).\nFinally the gradients \u2207\u03b8rjF(v), obtained from equations (19) and (20), are:\n\u2207\u03b8rF(v)=\u2207\u00b5F(v) > \u2202\u00b5\n\u2202\u03b8r +Tr\n( \u2207RF(v) \u2202R\n\u2202\u03b8r\n) . (21)\nWe can now use the gradients (18) and (21) to descend the free-energy surface with respect to both the generative and recognition parameters in a single optimisation step."}, {"heading": "4.4. Algorithm Summary and Complexity", "text": "Figure 1 (c) shows the flow of computation in the model. Our algorithm proceeds by performing a forward pass (black arrows): consisting of bottomup (recognition) phase and a top-down (generation phase), which updates the hidden activations of the recognition model and parameters of any Gaussian distributions; and a backward pass (red arrows) in which gradients are computed using the appropriate backpropagation rule for deterministic and stochastic layers.\nWe take a gradient descent step using:\n\u2206\u03b8g,r = \u2212\u0393g,r\u2207\u03b8g,rF(V), (22)\nwhere \u0393g,r are diagonal pre-conditioning matrices computed using the RMSprop heuristic1. The learning procedure is summarised in algorithm (1).\nThe computational complexity for producing S samples from the generative model is O(SLK\u03042) where K\u0304\n1Described by G. Hinton, RMSprop: Divide the gradient by a running average of its recent magnitude, in Coursera online course: Neural networks for machine learning, lecture 6e, 2012.\nAlgorithm 1 Learning in DLGMs\nwhile hasNotConverged() do V\u2190 getMiniBatch() \u03ben \u223c q(\u03ben|vn) (bottom-up pass) eq. (13) h\u2190 h(v\u03be) (top-down pass) eq. (3) updateGradients() eqs (19), (18), (20) \u03b8g,r \u2190 \u2206\u03b8g,r end while\nis the average number of neurons per layer, L is the number of layers (counting together deterministic and stochastic layers). The computational complexity per training sample during training is O(LK\u03042) , and is the same as that of an auto-encoder with a decoder and encoder of same size as the generative and recognition models, respectively."}, {"heading": "5. Related Work", "text": "Directed Graphical Models. There is a broad existing literature on directed graphical models with depth. Our model is directly related to non-linear Gaussian belief networks (NLGNBN) (Frey & Hinton, 1999), which also use latent Gaussian distributions throughout a multi-layer hierarchy, but whose means are simple non-linear transformations of higher layers. Sigmoid belief networks (SBN) (Saul et al., 1996) are closely related and are models with Bernoulli variables at every layer. Both these models are trained by a mean-field variational EM algorithm. More recently, Gregor et al. (2013) described Deep Auto-regressive Networks (DARN), which also form a directed graphical model that uses auto-regressive Bernoulli distributions at each layer. The Gaussian process latent variable model (GPLVM) (Lawrence, 2005) is the non-parametric analogue of our model, and employs Gaussian process priors over the non-linear functions between each layer. Some of the best results using directed models are provided by the neural autoregressive density estimator (NADE) (Larochelle & Murray, 2011; Uria et al., 2013), which uses function approximation to model conditional distributions within a directed acyclic graph. We will also see this performance on the benchmark tests we present, but major drawbacks exist, such as its inability to allow for deep representations and difficulties in extending to locally connected models (e.g., through the use of convolutional layers) that allow for high-dimensional scaling.\nRelation to denoising auto-encoders. Denoising auto-encoders (DAE) (Vincent et al., 2010) introduce a random corruption to the encoder network and at-\ntempt to minimize the expected reconstruction error under this corruption noise with additional regularisation terms. Bengio et al. (2013) show how these models can be used as generative models, but since the generative process underlying the denoising auto-encoder is unknown, simulation from the model requires a slow Markov chain sampling procedure. In the model we describe, the recognition distribution q(\u03be|v) can be interpreted as a stochastic encoder in the DAE setting. We can readily see the correspondence between the expression for the free energy (12) and the reconstruction error and regularization terms used in denoising autoencoders (c.f. equation (4) of Bengio et al. (2013)).\nIt is possible to now see denoising auto-encoders in the framework for variational learning in latent variable models. The key difference is that the form of encoding \u2018corruption\u2019 and regularisation terms used in our model have been derived from the variational principle to provide a strict bound on the data log-likelihood of a known directed generative model and that allows for easy generation of samples.\nAlternative Gradient Estimates. Other approaches for gradient estimation are typically used in the literature. The most general approaches are policy-gradient methods such as REINFORCE (Williams, 1992) that are simple to implement and applicable to both discrete and continuous models. This estimator can be written as:\n\u2207\u03b8Ep[f(\u03be)] = Ep[(f(\u03be)\u2212 b)\u2207\u03b8 log p(\u03be|\u03b8)], (23)\nwhere b is a baseline typically chosen to reduce the variance of the estimator; we use draws \u03be \u223c p(\u03be|\u03b8).\nUnfortunately the variance of (23) scales poorly with the number of random variables (Dayan et al., 1995). To see this limitation, consider\nfunctions of the form f(\u03be) = \u2211D i=1 f(\u03bei), where each individual term has a bounded variance, i.e., Var[f(\u03bei)] \u2264 \u03ba\u2200i for some \u03ba > 0, and consider independent or weakly correlated random variables. Given these assumptions we have the following bounds for the variances of (7) and REINFORCE (23):\nGBP: Var[\u2207\u03bef(\u03be)] \u2264 \u03ba REINFORCE: Var [ (\u03be\u2212\u00b5) \u03c32 (f(\u03be)\u2212 E[f(\u03be)]) ] \u2264 D\u03ba.\nThus, the REINFORCE estimator has the undesirable property that its variance scales linearly with the number of independent random variables in the target function, while the variance of GBP is bounded by a constant.\nAny correlation between the different terms in f(\u03be) would reduce the absolute value of the variance (5). The assumption of weakly correlated terms is relevant\nfor variational learning in larger generative models where variables tend to depend strongly only on other variables in their Markov blanket (i.e. only neighbouring nodes in the graphical model tend to display strong correlations). Moreover, due to independence assumptions and structure in the variational distribution, the resulting free energies are often summations over weakly correlated or independent terms, further supporting this view. We provide a second view on the issue in appendix C.\nAnother very general alternative to REINFORCE is the wake-sleep algorithm (Dayan et al., 1995). The wake-sleep algorithm fails to optimise a single consistent objective function and there is thus no guarantee that it leads to a decrease of the free energy (12). But it has been shown to work well in some small practical examples. In figure 2(c) we provide a comparison of the performance achieved by our model and the wakesleep algorithm.\nStochastic back-propagation in other contexts. The key theorems for the Gaussian stochastic backpropagation were first exploited by Opper & Archambeau (2009) for variational learning in Gaussian process regression, and subsequently by Graves (2011) for learning the parameters of large neural networks.\nOpper & Archambeau (2009) make use of an unconditional variational approximation, which typically requires several iterations for every visible pattern v in order to converge to a local minimum of the free energy. In contrast, we use a parametric recognition model (eq. (13)) which can produce a one-shot sample from the approximate posterior distribution (similar to the procedure in Dayan et al. (1995); Dayan (2000)). Using a conditional, parametric recognition model also provides a cheap probabilistic encoding of the dataset and provides a framework for treating generative models and classes of auto-encoders on the same principled ground (as described above). Recently, Kingma & Welling (2013) also make the connection between stochastic back-propagation, generative auto-encoders and variational inference that we describe here. This work was developed simultaneously with ours and provides an additional perspective on the use and derivation of stochastic back-propagation rules."}, {"heading": "6. Results", "text": "Generative models, such as the deep latent Gaussian model that we focus on, have a number of applications in simulation, prediction, data visualisation, missing data imputation and other forms of probabilistic reasoning. We describe the testing methodology we use\nand present results on a number of these tasks."}, {"heading": "6.1. Testing Methodology", "text": "We use training data of various types including binary and count-based data sets. In all cases, we train using mini-batches, which requires the introduction of scaling terms in the free energy objective function (14) in order to maintain the correct scale between the prior over the parameters and the remaining terms (Ahn et al., 2012; Welling & Teh, 2011). We make use of the objective:\nF(V) = \u2212\u03bb \u2211 n Eq [log p(vn|h(\u03ben))] + 12\u03ba\u2016\u03b8 g\u20162\n+ \u03bb\n2 \u2211 n,l [ \u2016\u00b5n,l\u20162 + Tr(Cn,l)\u2212 log |Cn,l| \u2212 1 ] , (24)\nwhere n is an index over observations in the mini-batch and \u03bb is equal to the ratio between the data-set and the mini-batch size. At each iteration, a random minibatch of size 200 observations is chosen.\nAll parameters of the model were initialized using samples from a Gaussian distribution with mean zero and variance 1\u00d7 106; the prior variance of the parameters was \u03ba = 1\u00d7 106. We compute the marginal likelihood on the test data by importance sampling using samples from the recognition model; we describe our estimator in appendix D.\nSince the recognition model is parameterised by a deep neural network, careful regularisation is needed to ensure that it provides useful inferences for unseen data. We regularise by introducing additional noise to the recognition model, specifically, bit-flip or drop-out noise at the input layer and small additional Gaussian noise to samples generated by the recognition model. We also use rectified non-linearities for any hidden layers."}, {"heading": "6.2. Analysing the Approximate Posterior", "text": "The specification of the recognition model affects how well we are able to capture the statistics of the data and the accuracy and efficiency of the learning.\nWe examine the quality of the approximate posterior distribution learnt by the recognition model in comparison to the true posterior distribution by training a deep latent Gaussian model on the MNIST data set of handwritten images. The images are of size 28\u00d728, and we use the binarised data set from Larochelle & Murray (2011).\nWe use sampling to evaluate the true posterior distribution for a number of MNIST digits under the diago-\nnal and the structured covariance parameterisation of the recognition model, described in section 4.2. We visualise the posterior distribution for a model with two Gaussian latent variables in figure 2. The true posterior distribution is shown by the grey regions and was computed by importance sampling with a large number of particles aligned in a grid between -5 and 5. In figure 2(a) we see that many posterior distributions are elliptical or spherical in shape and thus, it is reasonable to assume that they can be well described by a Gaussian approximation. Samples from the prior (shown in green) are spread widely over the space and very few samples fall in the region of significant posterior mass, explaining the inefficiency of estimation methods that rely on samples from the prior. Samples from the recognition model (shown in blue) are concentrated on the posterior mass, indicating that the recognition model has learnt the correct posterior statistics and should lead to efficient learning.\nIn figure 2(a) we see that samples from the recognition model are aligned to the axis and cannot capture any correlation in the posterior. Using the low-rank covariance model, we see in figure 2(b) that we are able to capture the posterior correlation. Not all posteriors are Gaussian in shape, but the recognition places mass in the best location possible to provide a reasonable approximation. This aspect emphasises the importance of posterior approximation, and one we continue to investigate. The performance in terms of test log-likelihood on the MNIST data is shown for four algorithms on the same model architecture. We compare factor analysis (FA), the wake-sleep algorithm, and our approach using a diagonal and low-rank covariance."}, {"heading": "6.3. Simulation and prediction", "text": "We evaluate the performance of a three layer latent Gaussian model on the MNIST data set. The model consists of two deterministic layers with 200 hidden units and a stochastic layer of 200 latent variables. We use mini-batches of 200 observations and trained the model using stochastic back-propagation. Samples from this model are shown in figure 3. We also compare the test log-likelihood to a large number of existing approaches in table 1. We used exactly the data set used in Uria et al. (2013) and quote the log-likelihoods in the lower part of the table from this work. These results show that we are able to compete with some of the best models currently available. The generated digits also match the true data well and visually appear as good as some of the best visualisations from these competing approaches.\nWe also analysed the performance of our model on\nthree high-dimensional real image data sets. The NORB object recognition data set consists of 24, 300 images that are 96\u00d796 pixels. We use a model consisting of 1 deterministic layer of 400 hidden units and one stochastic layer of 100 latent variables. Samples produced from this model are shown in figure 4(a). The CIFAR10 natural images data set consists of 50, 000 RGB images that are 32 \u00d7 32 pixels, which we split into 8\u00d7 8 patches. We use the same model as used for the MNIST experiment and show samples from the model in figure 4(b). The Frey faces data set consists of almost 2, 000 images of different facial expressions2 of size 28 \u00d7 20 pixels. In all cases we see that the model has learnt the different image statistics and produces recognisable samples. Results of this kind are extremely promising and suggest that performance can be improved greatly by scaling the model to more realistic scenarios by considering locally connected and convolutional architectures."}, {"heading": "6.4. Missing Data Imputation and Denoising", "text": "The generative models we describe are often used for problems in missing data imputation that form the core of applications in recommender system, bioinfor-\n2Images of Brendan Frey. Data from http://www.cs. nyu.edu/~roweis/data.html\nmatics and experimental design. We show the ability of the model to impute missing data using the MNIST data set in figure 5. We test the imputation ability under two different missingness types (Little & Rubin, 1987): Missing-at-random (MAR), where we consider 60% and 80% of the pixels to be missing randomly, and Not Missing-at-random (NMAR), where we consider a square region of the image to be missing. The model produces very good completions in both test cases. There is uncertainty in the identity of the image. This is expected and reflected in the errors in these completions as the resampling procedure is run, and further demonstrates the ability of the model to capture the diversity of the underlying data. We do not integrate over the missing values in our imputation procedure, but use a procedure that simulates a Markov chain that we show converges to the true marginal distribution. The procedure to sample from the missing pixels given the observed pixels is explained in appendix E."}, {"heading": "6.5. Data Visualisation", "text": "Latent variable models such as DLGMs are often used for visualisation of high-dimensional data sets. We project the MNIST data set to a 2-dimensional latent space and use this 2-D embedding as a visualisation of the data. A 2-dimensional embedding of the MNIST data set is shown in figure 6. The classes separate into different regions indicating that such a tool can be useful in gaining insight into the structure of highdimensional data sets."}, {"heading": "7. Discussion", "text": "Our algorithm generalises to a large class of models with continuous latent variables, which include Gaussian, non-negative or sparsity-promoting latent variables. For models with discrete latent variables (e.g., sigmoid belief networks), policy-gradient approaches that improve upon the REINFORCE approach remain the most general, but intelligent design is needed to control the gradient-variance in high dimensional settings.\nThese models are typically used with a large number\nof latent variables. In this setting, and under the appropriate conditions, the required expectations for inference could be well approximated by Gaussian integrals. Thus, we believe that our approach is applicable even in the high-dimensional discrete setting: we can apply the gradient estimators derived in section 4 as an approximation in high-dimensional discrete latent variable models, and potentially obtain new learning rules for these models.\nAn additional feature of our approach is that it can easily be combined with convolutional architectures and computation using GPUs to allow for scaling to the large-data settings we are now routinely faced with. More investigation is required to fully explore the impact of the structure of the recognition model and to allow more accurate covariance estimation. This is particularly important in the high-dimensional setting where we lack intuition regarding the characteristics of the posterior distribution. This and other extensions form the basis of much future work."}, {"heading": "8. Conclusion", "text": "We have developed a class of general-purpose and flexible generative models with Gaussian latent variables at each layer. Our approach introduces a recognition model, which can be seen as a stochastic encoding of the data, to allow for efficient and tractable inference. We derived a lower bound on the marginal likelihood for the generative model and specified the structure and regularisation of the recognition model by exploiting recent advances in deep learning. By developing modified rules for back-propagation through stochastic layers, we derived an efficient inference algorithm that allows for joint optimisation of all parameters, i.e. parameters of the generative and recognition models. We have demonstrated on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and can be a useful tool for high-dimensional data visualisation."}, {"heading": "A. Additional Model Details", "text": "In equation (6) we showed an alternative form of the joint log likelihood that makes explicit that the generative model works by applying a highly nonlinear transformation to a spherical Gaussian distribution N (\u03be) such that the transformed distribution best matches the empirical distribution. We provide more details on this view here for clarity.\nFrom the model description (3), (4), we can interpret the variables hl as deterministic functions of the noise variables \u03bel. This can be formally introduced as a coordinate transformation of the probability density in equation (5): we perform a change of coordinates hl \u2192 \u03bel. The density of the transformed variables \u03be can be expressed in terms of the density (5) times the determinant of the Jacobian of the transformation p(\u03be) = p(h(\u03be))| \u2202h\n\u2202\u03be |. Since the coordinate transforma-\ntion is linear we have | \u2202h \u2202\u03be | = |Gl| and the distribution of \u03be is:\np(\u03be)=p(hL)|GL| L\u22121\u220f l=1 |Gl|pl(hl|hl+1)= L\u220f l=1 |Gl||Sl|\u2212 1 2N (\u03bel)\n= L\u220f l=1 |Gl||GlGTl |\u2212 1 2N (\u03bel) = L\u220f l=1 N(\u03bel), (25)\nwhere N (\u03be) is a Gaussian with mean zero and covariance equal to the identity matrix. Using this view, we rewrite the joint probability as in (6).\nA simple recognition model that can be used, consists of a single deterministic layer and a stochastic Gaussian layer with the rank-one covariace structure and is constructed as:\nq(\u03be|v) = N ( \u03be|\u00b5; (diag(d) + uu>)\u22121 ) (26)\n\u00b5 = W\u00b5z + b\u00b5 (27)\nd = Wdz + bd; u = Wuz + bu (28)\nz = f(Wvv + bv) (29)\nwhere the function f is a rectified non-linearity (but other non-linearities such as tanh can be used.)"}, {"heading": "B. Deriving Stochastic Back-propagation Rules", "text": "In section 3 we described two ways in which to derive stochastic back-propagation rules. We show specific examples and provide some more discussion in this section.\nB.1. Using the Product Rule for Integrals\nWe can derive rules for stochastic back-propagation for many distributions by finding a appropriate nonlinear function that allows us to express the gradient with respect to the parameters of the distribution as a gradient with respect to the random variable directly. The approach we described in the main text was:\n\u2207\u03b8Ep[f(x)]= \u222b \u2207\u03b8p(x|\u03b8)f(x)dx= \u222b \u2207xp(x|\u03b8)B(x)f(x)dx\n= [B(x)f(x)p(x|\u03b8)]supp(x) \u2212 \u222b p(x|\u03b8)\u2207x[B(x)f(x)]\n= \u2212Ep(x|\u03b8)[\u2207x[B(x)f(x)]] (30)\nwhere we have introduced the non-linear function B(x) to allows the transformation of the gradients and have applied the product rule for integrals (rule for integration by parts) to rewrite the integral in two parts in the second line, and the supp(x) indicates that the term is evaluated at the boundaries of the support. To use this approach, we require that the density we are analysing be zero at the boundaries of the support to ensure that the first term in the second line is zero.\nAs an alternative, we can also write this differently and find an non-linear function of the form:\n\u2207\u03b8Ep[f(x)]== \u2212Ep(x|\u03b8)[B(x)\u2207xf(x)]. (31)\nConsider general exponential family distributions of the form:\np(x|\u03b8) = h(x) exp(\u03b7(\u03b8)>\u03c6(x)\u2212A(\u03b8)) (32)\nwhere h(x) is the base measure, \u03b8 is the set of mean parameters of the distribution, \u03b7 is the set of natural parameters, and A(\u03b8) is the log-partition function. We can express the non-linear function in (30) using these quantities as:\nB(x) = [\u2207\u03b8\u03b7(\u03b8)\u03c6(x)\u2212\u2207\u03b8A(\u03b8)]\n[\u2207x log[h(x)] + \u03b7(\u03b8)T\u2207x\u03c6(x)] . (33)\nThis can be derived for a number of distributions such as the Gaussian, inverse Gamma, Log-Normal, Wald (inverse Gaussian) and other distributions. We show some of these below:\nThe B(x) corresponding to the second formulation can also be derived and may be useful in certain situations, requiring the solution of a first order differential equation. This approach of searching for non-linear transformations leads us to the second approach for deriving stochastic back-propagation rules.\nFamily \u03b8 B(x)\nGaussian\n( \u00b5 \u03c32 ) ( \u22121 (x\u2212\u00b5\u2212\u03c3)(x\u2212\u00b5+\u03c3)\n2\u03c32(x\u2212\u00b5) ) Inv. Gamma ( \u03b1 \u03b2 ) ( x2(\u2212 ln x\u2212\u03a8(\u03b1)+ln \u03b2) \u2212x(\u03b1+1)+\u03b2\n( x 2\n\u2212x(\u03b1+1)+\u03b2 )(\u2212 1 x + \u03b1 \u03b2 ) ) Log-Normal ( \u00b5 \u03c32 ) ( \u22121 (ln x\u2212\u00b5\u2212\u03c3)(ln x\u2212\u00b5+\u03c3)\n2\u03c32(ln x\u2212\u00b5)\n)\nB.2. Using Alternative Co-ordinate Transformations\nThere are many distributions outside the exponential family that we would like to consider using. A simpler approach is to search for a co-ordinate transformation that allows us to separate the deterministic and stochastic parts of the distribution. We described the case of the Gaussian in section 3. Other distributions also have this property. As an example, consider the Levy distribution (which is a special case of the inverse Gamma considered above). Due to the self-similarity property of this distribution, if we draw X from a Levy distribution with known parameters X \u223c Levy(\u00b5, \u03bb), we can obtain any other Levy distribution by rescaling and shifting this base distribution: kX + b \u223c Levy(k\u00b5+ b, kc).\nMany other distributions hold this property, allowing stochastic back-propagation rules to be determined for distributions such as the Student\u2019s t-distribution, Logistic distribution, the class of stable distributions and the class of generalised extreme value distributions (GEV). Examples of co-ordinate transformations T (\u00b7) and the resulsting distributions are shown below for variates X drawn from the standard distribution listed in the first column.\nStd Distr. T (\u00b7) Gen. Distr. GEV (\u00b5, \u03c3, 0) mX+b GEV (m\u00b5+b,m\u03c3, 0) Exp(1) \u00b5+\u03b2ln(1+exp(\u2212X)) Logistic(\u00b5, \u03b2) Exp(1) \u03bbX 1 k Weibull(\u03bb, k)"}, {"heading": "C. Univariate variance analysis", "text": "In analysing the variance properties of many estimators, we showed the general scaling of likelihood ratio approaches in section 5. As an example to further emphasise the high-variance nature of these alternative approaches, we present a short analysis in the univariate case.\nConsider a random variable p(\u03be) = N (\u03be|\u00b5, \u03c32) and a simple quadratic function of the form\nf(\u03be) = c \u03be2\n2 . (34)\nFor this function we immediately obtain the following variances\nV ar[\u2207\u03bef(\u03be)] = c2\u03c32 (35) V ar[\u2207\u03be2f(\u03be)] = 0 (36)\nV ar[ (\u03be \u2212 \u00b5) \u03c3 \u2207\u03bef(\u03be)] = 2c2\u03c32 + \u00b52c2 (37)\nV ar[ (\u03be \u2212 \u00b5) \u03c32 (f(\u03be)\u2212 E[f(\u03be)])] = 2c2\u00b52 + 5 2 c2\u03c32 (38)\nEquations (35), (36) and (37) correspond to the variance of the estimators based on (7), (8), (11) respectively whereas equation (38) corresponds to the variance of the REINFORCE algorithm for the gradient with respect to \u00b5.\nFrom these relations we see that, for any parameter configuration, the variance of the REINFORCE estimator is strictly larger than the variance of the estimator based on (7). Additionally, the ratio between the variances of the former and later estimators is lowerbounded by 5/2. We can also see that the variance of the estimator based on (8) is zero for this specific function whereas the variance of the estimator based on (11) is not.\nThe high variance nature of the policy gradient approaches are undesirable and is magnified in multivarate settings, leading to the variance estimates that scale with the dimensionality of the latent variables that were discussed in section 5"}, {"heading": "D. Estimating the Marginal Likelihood", "text": "We compute the marginal likelihood by importance sampling by generating S samples from the recognition model and using the following estimator:\np(v) \u2248 1 S S\u2211 s=1 p(v|h(\u03be(s)))p(\u03be(s)) q(\u03bes) ; \u03be(s) \u223c q(\u03be|v)\n(39)"}, {"heading": "E. Missing Data Imputation", "text": "Image completion can be approximatively achieved by a simple iterative procedure which consists of (i) initializing the non-observed pixels with random values; (ii) sampling from the recognition distribution given the resulting image; (iii) reconstruct the image given the sample from the recognition model; (iv) iterate the procedure.\nWe denote the observed and missing entries in an observation as vm,vo, respectively. The imputation procedure can be written formally as a Markov chain with\ntransition kernel T q(vm \u2192 v\u2032m) given by\nT q(vm \u2192 v\u2032m) = \u222b p(v\u2032|\u03be)q(\u03be|v)d\u03be, (40)\nwhere v = (vm,vo) and v \u2032 = (v\u2032m,vo).\nProvided that the recognition model q(\u03be|v) constitutes a good approximation of the true posterior p(\u03be|v), (40) can be seen as an approximation of the Kernel\nT (vm \u2192 v\u2032m) = \u222b p(v\u2032|\u03be)p(\u03be|v)d\u03be. (41)\nThe kernel (41) has two important properties: (i) it has as eigen-distribution the model\u2019s marginal p(vm|vo); (ii) T (v \u2192 v\u2032) > 0\u2200v,v\u2032. The property (i) can be derived by applying the kernel (41) to the marginal p(vm|vo) and noting that it is an fixed point. Property (ii) is an immediate consequence of the smoothness of the model.\nWe apply the fundamental theorem for Markov chains (Neal, 1993, pp. 38) and conclude that given the above properties, a Markov chain generated by (41) is guaranteed to generate samples from the correct marginal p(vm|vo).\nIn practice, the stationary distribution of the completed pixels will not be exactly the model\u2019s marginal p(vm|vo), since we can only use the approximated kernel (40). We can nevertheless provide a bound on the L1 norm of the difference between the resulting stationary marginal and the target marginal p(vm|vo) by the following proposition.\nProposition E.1 (L1 bound on marginal error ). If the recognition model q(\u03be|v) is such that for all \u03be\n\u2203\u03b5 > 0 s.t. \u222b \u2223\u2223\u2223\u2223q(\u03be|v)p(v)p(\u03be) \u2212 p(v|\u03be) \u2223\u2223\u2223\u2223 dv \u2264 \u03b5 (42) then the marginal p(v) is a weak fixed point of the kernel (40) in the following sense:\n\u222b \u2223\u2223\u2223\u2223\u222b [T q(vm \u2192 v\u2032m)\u2212 T (v\u2192 v\u2032)] p(v)dv\u2223\u2223\u2223\u2223 dv\u2032 < \u03b5. (43)\nProof.\u222b \u2223\u2223\u2223\u2223\u222b [T q(vm \u2192 v\u2032m)\u2212 T (v\u2192 v\u2032)] p(v)dv\u2223\u2223\u2223\u2223 dv\u2032 =\n\u222b \u2223\u2223\u2223\u2223\u222b p(v\u2032|\u03be)p(v)[q(\u03be|v)\u2212 p(\u03be|v)]dvd\u03be\u2223\u2223\u2223\u2223 dv\u2032 = \u222b \u2223\u2223\u2223\u2223\u222b p(v\u2032|\u03be)p(v)[q(\u03be|v)\u2212 p(\u03be|v)]p(v)p(\u03be) p(\u03be)p(v)dvd\u03be \u2223\u2223\u2223\u2223 dv\u2032\n= \u222b \u2223\u2223\u2223\u2223\u222b p(v\u2032|\u03be)p(\u03be)[q(\u03be|v)p(v)p(\u03be) \u2212 p(v|\u03be)]dvd\u03be \u2223\u2223\u2223\u2223 dv\u2032\n\u2264 \u222b \u222b p(v\u2032|\u03be)p(\u03be) \u222b \u2223\u2223\u2223\u2223q(\u03be|v)p(v)p(\u03be) \u2212 p(v|\u03be) \u2223\u2223\u2223\u2223 dvd\u03bedv\u2032 \u2264 \u03b5.\nwhere we apply the condition (42) to obtain the last statement. That is, if the recognition model is sufficiently close to the true posterior to guarantee that (42) holds for some acceptable error \u03b5 than (43) guarantees that the fixed point of the Markov chain induced by (40) is not further and \u03b5 away from the true marginal with respect to the L1 norm.\nF. Variational Bayes for Deep Directed Models\nIn the main test we focussed on the variational problem of specifying an posterior on the latent variables only. It is natural to consider the variational Bayes problem in which we specify an approximate posterior for both the latent variables and model parameters.\nFollowing the same construction and considering an Gaussian approximate distribution on the model parameters \u03b8g, the free energy becomes:\nF(V) = \u2212 \u2211 n reconstruction error\ufe37 \ufe38\ufe38 \ufe37 Eq[log p(vn|h(\u03ben))]\n+ 1\n2 \u2211 n,l [ \u2016\u00b5n,l\u20162 + TrCn,l \u2212 log |Cn,l| \u2212 1 ] \ufe38 \ufe37\ufe37 \ufe38\nlatent regularization term\n+ 1\n2 \u2211 j [ m2j \u03ba + \u03c4j \u03ba + log \u03ba\u2212 log \u03c4j \u2212 1 ] \ufe38 \ufe37\ufe37 \ufe38\nparameter regularization term\n, (44)\nwhich now includes an additional term for the cost of using parameters and their regularisation. We must now compute the additional set of gradients with re-\nspect to the parameter\u2019s mean mj and variance \u03c4j are: \u2207mjF(v) = \u2212Eq [ \u2207\u03b8gj log p(v|h(\u03be)) ] +mj (45)\n\u2207\u03c4jF(v) = \u2212 12Eq [ \u03b8j \u2212mj \u03c4j \u2207\u03b8gj log p(v|h(\u03be)) ]\n+ 1 2\u03ba \u2212 1\n2\u03c4j\n(46)"}, {"heading": "G. Additional Related Work", "text": "There are a number of other aspects of related work, which due to space reasons were not included in the main text, but are worth noting.\nGeneral Classes of Latent Gaussian Models. The model class we describe here builds upon other widely-used models with latent Gaussian distributions and provides an inference mechanism for use within this diverse model class. Recognising the latent Gaussian structure shows the connection to models such as generalised linear regression, Gaussian process regression, factor analysis, probabilistic principal components analysis, stochastic volatility models, and other latent Gaussian graphical models (such as logGaussian Cox processes and models for covariance selection).\nAlternative Inference Approaches. The most common approach for inference in deep directed models has been variational EM. The alternating minimisation often suffers from slow convergence, expensive parameter learning steps, and typically requires the specification of additional local bounds to allows for efficient computation of the required expectations. The wake-sleep algorithm (Dayan, 2000) is a further alternative, but fails to optimise a single consistent objective function and has had limited success in the past. For latent Gaussian models, other alternative inference algorithms have been proposed: the Integrated Nested Laplace Approximation (INLA) has been popular, but it is limited to models with very few hyperparameters (Rue et al., 2009). Alternative variational algorithms such as expectation propagation (EP) (Minka, 2001) can also be used, but are numerically difficult to implement and have performance similar to approaches based on the variational lower bound."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient Fisher scoring", "author": ["S. Ahn", "A.K. Balan", "M. Welling"], "venue": "In ICML,", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Latent variable models and factor analysis, volume 7 of Kendall\u2019s library of statistics", "author": ["D.J. Bartholomew", "M. Knott"], "venue": null, "citeRegEx": "Bartholomew and Knott,? \\Q1999\\E", "shortCiteRegEx": "Bartholomew and Knott", "year": 1999}, {"title": "Variational Algorithms for approximate Bayesian inference", "author": ["M.J. Beal"], "venue": "PhD thesis, University of Cambridge,", "citeRegEx": "Beal,? \\Q2003\\E", "shortCiteRegEx": "Beal", "year": 2003}, {"title": "Generalized denoising auto-encoders as generative models", "author": ["Y. Bengio", "L. Yao", "G. Alain", "P. Vincent"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Transformations des signaux al\u00e9atoires a travers les syst\u00e8mes non lin\u00e9aires sans m\u00e9moire", "author": ["G. Bonnet"], "venue": "Annales des Te\u0301le\u0301communications,", "citeRegEx": "Bonnet,? \\Q1964\\E", "shortCiteRegEx": "Bonnet", "year": 1964}, {"title": "Helmholtz machines and wake-sleep learning. Handbook of Brain Theory and Neural Network", "author": ["P. Dayan"], "venue": null, "citeRegEx": "Dayan,? \\Q2000\\E", "shortCiteRegEx": "Dayan", "year": 2000}, {"title": "Variational inference for continuous sigmoidal Bayesian networks", "author": ["B.J. Frey"], "venue": "6th International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Frey,? \\Q1996\\E", "shortCiteRegEx": "Frey", "year": 1996}, {"title": "Variational learning in nonlinear Gaussian belief networks", "author": ["B.J. Frey", "G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Frey and Hinton,? \\Q1999\\E", "shortCiteRegEx": "Frey and Hinton", "year": 1999}, {"title": "Practical variational inference for neural networks", "author": ["A. Graves"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Graves,? \\Q2011\\E", "shortCiteRegEx": "Graves", "year": 2011}, {"title": "Deep autoregressive networks", "author": ["K. Gregor", "A. Mnih", "D. Wierstra"], "venue": "ArXiv preprint", "citeRegEx": "Gregor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2013}, {"title": "Stochastic variational inference", "author": ["M. Hoffman", "D.M. Blei", "C. Wang", "J. Paisley"], "venue": "arXiv preprint arXiv:1206.7051,", "citeRegEx": "Hoffman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2012}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma and Welling,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2013}, {"title": "The neural autoregressive distribution estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), JMLR W&CP 15:29\u201337,", "citeRegEx": "Larochelle and Murray,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "Probabilistic non-linear principal component analysis with Gaussian process latent variable models", "author": ["N. Lawrence"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lawrence,? \\Q2005\\E", "shortCiteRegEx": "Lawrence", "year": 2005}, {"title": "Approximating the covariance matrix of GMMs with low-rank perturbations", "author": ["M. Magdon-Ismail", "J.T. Purnell"], "venue": null, "citeRegEx": "Magdon.Ismail and Purnell,? \\Q2010\\E", "shortCiteRegEx": "Magdon.Ismail and Purnell", "year": 2010}, {"title": "A family of algorithms for approximate Bayesian inference", "author": ["T. Minka"], "venue": "PhD thesis,", "citeRegEx": "Minka,? \\Q2001\\E", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Probabilistic inference using Markov Chain Monte Carlo methods", "author": ["R.M. Neal"], "venue": "Technical Report CRG-TR-93-1, University of Toronto,", "citeRegEx": "Neal,? \\Q1993\\E", "shortCiteRegEx": "Neal", "year": 1993}, {"title": "The variational Gaussian approximation revisited", "author": ["M. Opper", "C. Archambeau"], "venue": "Neural computation,", "citeRegEx": "Opper and Archambeau,? \\Q2009\\E", "shortCiteRegEx": "Opper and Archambeau", "year": 2009}, {"title": "A useful theorem for nonlinear devices having Gaussian inputs", "author": ["R. Price"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Price,? \\Q1958\\E", "shortCiteRegEx": "Price", "year": 1958}, {"title": "Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations", "author": ["H. Rue", "S. Martino", "N. Chopin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Rue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rue et al\\.", "year": 2009}, {"title": "Mean field theory for sigmoid belief networks", "author": ["L.K. Saul", "T. Jaakkola", "M.I. Jordan"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Saul et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Saul et al\\.", "year": 1996}, {"title": "A deep and tractable density estimator", "author": ["B. Uria", "I. Murray", "H. Larochelle"], "venue": "ArXiv preprint", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Welling and Teh,? \\Q2011\\E", "shortCiteRegEx": "Welling and Teh", "year": 2011}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "A minimum description length framework for unsupervised learning", "author": ["R.S. Zemel"], "venue": null, "citeRegEx": "Zemel,? \\Q1993\\E", "shortCiteRegEx": "Zemel", "year": 1993}], "referenceMentions": [{"referenceID": 6, "context": "ilar latent variable models (Dayan et al., 1995; Frey, 1996; Saul et al., 1996; Bartholomew & Knott, 1999; Uria et al., 2013; Gregor et al., 2013) can be easily sampled from, but in most cases, efficient inference algorithms have remained elusive.", "startOffset": 28, "endOffset": 146}, {"referenceID": 20, "context": "ilar latent variable models (Dayan et al., 1995; Frey, 1996; Saul et al., 1996; Bartholomew & Knott, 1999; Uria et al., 2013; Gregor et al., 2013) can be easily sampled from, but in most cases, efficient inference algorithms have remained elusive.", "startOffset": 28, "endOffset": 146}, {"referenceID": 21, "context": "ilar latent variable models (Dayan et al., 1995; Frey, 1996; Saul et al., 1996; Bartholomew & Knott, 1999; Uria et al., 2013; Gregor et al., 2013) can be easily sampled from, but in most cases, efficient inference algorithms have remained elusive.", "startOffset": 28, "endOffset": 146}, {"referenceID": 9, "context": "ilar latent variable models (Dayan et al., 1995; Frey, 1996; Saul et al., 1996; Bartholomew & Knott, 1999; Uria et al., 2013; Gregor et al., 2013) can be easily sampled from, but in most cases, efficient inference algorithms have remained elusive.", "startOffset": 28, "endOffset": 146}, {"referenceID": 2, "context": "A number of approaches are known and currently used, in particular algorithms based on mean-field variational EM (Beal, 2003), the wake-sleep algorithm (Dayan, 2000), policygradient methods such as REINFORCE (Williams, 1992), or stochastic gradient methods (Hoffman et al.", "startOffset": 113, "endOffset": 125}, {"referenceID": 5, "context": "A number of approaches are known and currently used, in particular algorithms based on mean-field variational EM (Beal, 2003), the wake-sleep algorithm (Dayan, 2000), policygradient methods such as REINFORCE (Williams, 1992), or stochastic gradient methods (Hoffman et al.", "startOffset": 152, "endOffset": 165}, {"referenceID": 23, "context": "A number of approaches are known and currently used, in particular algorithms based on mean-field variational EM (Beal, 2003), the wake-sleep algorithm (Dayan, 2000), policygradient methods such as REINFORCE (Williams, 1992), or stochastic gradient methods (Hoffman et al.", "startOffset": 208, "endOffset": 224}, {"referenceID": 10, "context": "A number of approaches are known and currently used, in particular algorithms based on mean-field variational EM (Beal, 2003), the wake-sleep algorithm (Dayan, 2000), policygradient methods such as REINFORCE (Williams, 1992), or stochastic gradient methods (Hoffman et al., 2012).", "startOffset": 257, "endOffset": 279}, {"referenceID": 4, "context": "which are due to the theorems by Bonnet (1964) and Price (1958), respectively.", "startOffset": 33, "endOffset": 47}, {"referenceID": 4, "context": "which are due to the theorems by Bonnet (1964) and Price (1958), respectively.", "startOffset": 33, "endOffset": 64}, {"referenceID": 24, "context": "We introduce an approximate posterior distribution q and applying Jensen\u2019s inequality, following the variational principle (Zemel, 1993; Beal, 2003) to obtain:", "startOffset": 123, "endOffset": 148}, {"referenceID": 2, "context": "We introduce an approximate posterior distribution q and applying Jensen\u2019s inequality, following the variational principle (Zemel, 1993; Beal, 2003) to obtain:", "startOffset": 123, "endOffset": 148}, {"referenceID": 20, "context": "Sigmoid belief networks (SBN) (Saul et al., 1996) are closely related and are models with Bernoulli variables at every layer.", "startOffset": 30, "endOffset": 49}, {"referenceID": 13, "context": "The Gaussian process latent variable model (GPLVM) (Lawrence, 2005) is the non-parametric analogue of our model, and employs Gaussian process priors over the non-linear functions between each layer.", "startOffset": 51, "endOffset": 67}, {"referenceID": 21, "context": "Some of the best results using directed models are provided by the neural autoregressive density estimator (NADE) (Larochelle & Murray, 2011; Uria et al., 2013), which uses function approximation to model conditional distributions within a directed acyclic graph.", "startOffset": 114, "endOffset": 160}, {"referenceID": 6, "context": "Our model is directly related to non-linear Gaussian belief networks (NLGNBN) (Frey & Hinton, 1999), which also use latent Gaussian distributions throughout a multi-layer hierarchy, but whose means are simple non-linear transformations of higher layers. Sigmoid belief networks (SBN) (Saul et al., 1996) are closely related and are models with Bernoulli variables at every layer. Both these models are trained by a mean-field variational EM algorithm. More recently, Gregor et al. (2013) described Deep Auto-regressive Networks (DARN), which also form a directed graphical model that uses auto-regressive Bernoulli distributions at each layer.", "startOffset": 79, "endOffset": 488}, {"referenceID": 3, "context": "Bengio et al. (2013) show how these models can be used as generative models, but since the generative process underlying the denoising auto-encoder is unknown, simulation from the model requires a slow Markov chain sampling procedure.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Bengio et al. (2013) show how these models can be used as generative models, but since the generative process underlying the denoising auto-encoder is unknown, simulation from the model requires a slow Markov chain sampling procedure. In the model we describe, the recognition distribution q(\u03be|v) can be interpreted as a stochastic encoder in the DAE setting. We can readily see the correspondence between the expression for the free energy (12) and the reconstruction error and regularization terms used in denoising autoencoders (c.f. equation (4) of Bengio et al. (2013)).", "startOffset": 0, "endOffset": 574}, {"referenceID": 23, "context": "The most general approaches are policy-gradient methods such as REINFORCE (Williams, 1992) that are simple to implement and applicable to both discrete and continuous models.", "startOffset": 74, "endOffset": 90}, {"referenceID": 8, "context": "The key theorems for the Gaussian stochastic backpropagation were first exploited by Opper & Archambeau (2009) for variational learning in Gaussian process regression, and subsequently by Graves (2011) for learning the parameters of large neural networks.", "startOffset": 188, "endOffset": 202}, {"referenceID": 5, "context": "(13)) which can produce a one-shot sample from the approximate posterior distribution (similar to the procedure in Dayan et al. (1995); Dayan (2000)).", "startOffset": 115, "endOffset": 135}, {"referenceID": 5, "context": "(13)) which can produce a one-shot sample from the approximate posterior distribution (similar to the procedure in Dayan et al. (1995); Dayan (2000)).", "startOffset": 115, "endOffset": 149}, {"referenceID": 5, "context": "(13)) which can produce a one-shot sample from the approximate posterior distribution (similar to the procedure in Dayan et al. (1995); Dayan (2000)). Using a conditional, parametric recognition model also provides a cheap probabilistic encoding of the dataset and provides a framework for treating generative models and classes of auto-encoders on the same principled ground (as described above). Recently, Kingma & Welling (2013) also make the connection between stochastic back-propagation, generative auto-encoders and variational inference that we describe here.", "startOffset": 115, "endOffset": 432}, {"referenceID": 0, "context": "In all cases, we train using mini-batches, which requires the introduction of scaling terms in the free energy objective function (14) in order to maintain the correct scale between the prior over the parameters and the remaining terms (Ahn et al., 2012; Welling & Teh, 2011).", "startOffset": 236, "endOffset": 275}, {"referenceID": 21, "context": "We used exactly the data set used in Uria et al. (2013) and quote the log-likelihoods in the lower part of the table from this work.", "startOffset": 37, "endOffset": 56}, {"referenceID": 5, "context": "80 Wake-Sleep (Dayan, 2000) 91.", "startOffset": 14, "endOffset": 27}, {"referenceID": 21, "context": "Results below from Uria et al. (2013)", "startOffset": 19, "endOffset": 38}], "year": 2014, "abstractText": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic backpropagation \u2013 rules for back-propagation through stochastic variables \u2013 and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.", "creator": "LaTeX with hyperref package"}}}