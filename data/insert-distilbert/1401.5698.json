{"id": "1401.5698", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2014", "title": "Identification of Pleonastic It Using the Web", "abstract": "in a significant minority of cases, certain possessive pronouns, especially the pronoun it, can be used without referring to any specific entity. this phenomenon of pleonastic facial pronoun usage poses serious problems for all systems aiming best at even a slightly shallow scholarly understanding of natural language texts. in this paper, a novel approach is proposed to identify such uses of it : the extrapositional cases are identified electrically using a series of queries against scraping the web, and the cleft cases are identified indirectly using a simple subset set of ambiguous syntactic rules. the system is evaluated with four sets of conventional news online articles containing 679 extrapositional cases as well as 78 cleft constructs. the identification results are comparable to those obtained by human testing efforts.", "histories": [["v1", "Wed, 15 Jan 2014 05:11:43 GMT  (1152kb)", "http://arxiv.org/abs/1401.5698v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yifan li", "petr musilek", "marek reformat", "loren wyard-scott"], "accepted": false, "id": "1401.5698"}, "pdf": {"name": "1401.5698.pdf", "metadata": {"source": "CRF", "title": "Identification of Pleonastic It Using the Web", "authors": ["Yifan Li", "Petr Musilek", "Marek Reformat", "Loren Wyard-Scott"], "emails": ["yifan@ece.ualberta.ca", "musilek@ece.ualberta.ca", "reform@ece.ualberta.ca", "wyard@ece.ualberta.ca"], "sections": [{"heading": "1. Introduction", "text": "Anaphora resolution, which associates a word or phrase (the anaphor) with a previously mentioned entity (the antecedent), is an active field of Natural Language Processing (NLP) research. It has an important role in many applications where a non-trivial level of understanding of natural language texts is desired, most notably in information extraction and machine translation. To illustrate, an information extraction system trying to keep track of corporate activities may find itself dealing with news such as \u2018Microsoft today announced that it is adopting XML as the default file format for the next major version of its Microsoft Office software . . . \u2019 It would be impossible to provide any insight into what Microsoft\u2019s intention is without associating the pronominal anaphors it and its with their antecedent, Microsoft.\nAdding to the already complex problem of finding the correct antecedent, pronouns are not always used in the same fashion as shown in the earlier example. It is well-known that some pronouns, especially it, can occur without referring to a nominal antecedent, or any antecedent at all. Pronouns used without an antecedent, often referred to as being pleonastic or structural, pose a serious problem for anaphora resolution systems. Many anaphora resolution systems underestimate the issue and choose not to implement a specific module to handle pleonastic pronouns but instead have their input \u2018sanitized\u2019 manually to exclude such cases. However, the high frequency of pronoun usage in general and pleonastic cases in particular warrants that the phenomenon deserves more serious treatment. The pronoun it, which accounts for most of the pleonastic pronoun usages, is by far the most frequently used of all pronouns in the British National Corpus (BNC). In the Wall Street Journal Corpus (WSJ; Marcus, Marcinkiewicz, & Santorini, 1993), upon which this study\n\u00a92009 AI Access Foundation. All rights reserved.\nLi, Musilek, Reformat, & Wyard-Scott\nis based, it accounts for more than 30% of personal pronoun usage. The percentage of cases where it lacks a nominal antecedent is also significant: previous studies have reported figures between 16% and 50% (Gundel, Hedberg, & Zacharski, 2005) while our own analysis based upon the WSJ corpus results in a value around 25%, more than half of which are pleonastic cases.\nApplying criteria similar to those established by Gundel et al. (2005), the usage of it can be generally categorized as follows. The instances of it being analyzed are shown in italics; and the corresponding antecedents, extraposed clauses, and clefted constituents are marked by underlining.\n1. Referential with nominal antecedent\n[0006:002]1 The thrift holding company said it expects to obtain regulatory approval and complete the transaction by year-end.\nwhere it refers to the thrift holding company.\n2. Referential with clause antecedent\n[0041:029] He was on the board of an insurance company with financial problems, but he insists he made no secret of it.\nwhere it refers to the fact that the person was on the board of an insurance company.\n[0102:002-003] Everyone agrees that most of the nation\u2019s old bridges need to be repaired or replaced. But there\u2019s disagreement over how to do it.\nwhere it, together with do, refers to the action of repairing or replacing the bridge.\n3. No antecedent \u2013 Pleonastic\n(a) Extraposition [0034:020] But it doesn\u2019t take much to get burned. where the infinitive clause to get burned is extraposed and its original position filled with an expletive it. The equivalent non-extraposed sentence is \u2018But to get burned doesn\u2019t take much.\u2019 [0037:034] It \u2019s a shame their meeting never took place. The equivalent non-extraposed sentence is \u2018That their meeting never took place is a shame.\u2019\n(b) Cleft2\n[0044:026] And most disturbing, it is educators, not students, who are blamed for much of the wrongdoing.\nThe equivalent non-cleft version is \u2018And most disturbing, educators, not students, are blamed for much of the wrongdoing.\u2019\n1All example sentences are selected from the WSJ corpus, with locations encoded in the format [article:sentence].\n2Some claim that cleft pronouns should not be classified as expletive (Gundel, 1977; Hedberg, 2000). Nevertheless, this does not change the fact that the pronouns do not have nominal antecedents; hence clefts are included in this analysis.\nIdentification of Pleonastic It Using the Web\n[0591:021] It is partly for this reason that the exchange last week began trading in its own stock \u201cbasket\u201d product . . .\nThe equivalent non-cleft version is \u2018The exchange last week began trading in its own stock basket product partly for this reason.\u2019\n(c) Local Situation\n[0207:037] It was not an unpleasant evening . . .\nThis category consists of it instances related to weather, time, distance, and other information about the local situation. Since the texts reviewed in this study lack instances of other subtypes, only weather and time cases are discussed.\n4. Idiomatic\n[0010:010] The governor couldn\u2019t make it, so the lieutenant governor welcomed the special guests.\nThis paper focuses on pleonastic cases (the third category), where each subclass carries its unique syntactic and/or semantic signatures. The idiomatic category, while consisting of non-anaphoric cases as well, is less coherent and its identification is much more subjective in nature, making it a less attractive target.\nThis paper is organized as follows: Section 2 provides a brief survey of related work toward both classification of it and identification of pleonastic it ; Section 3 proposes a web-based approach for identification of pleonastic it ; Section 4 demonstrates the proposed method with a case study; Section 5 follows with evaluation; and finally, Section 6 discusses the findings and presents ideas for future work."}, {"heading": "2. Previous Work", "text": "As Evans (2001) pointed out, usage of it is covered in most serious surveys of English grammar, some of which (e.g. Sinclair, 1995) also provide classifications based on semantic categories. In a recent study, Gundel et al. (2005) classify third-person personal pronouns into the following comprehensive hierarchy:\n\u2022 Noun phrase (NP) antecedent \u2022 Inferrable \u2022 Non-NP antecedent\n\u2013 Fact \u2013 Proposition \u2013 Activity \u2013 Event \u2013 Situation \u2013 Reason\n\u2022 Pleonastic\n\u2013 Full extraposition \u2013 Full cleft \u2013 Truncated cleft \u2013 Truncated extraposition \u2013 Atmospheric \u2013 Other pleonastic\n\u2022 Idiom \u2022 Exophoric \u2022 Indeterminate\nLi, Musilek, Reformat, & Wyard-Scott\nWithout going into the details of each category, it is apparent from the length of the list that the phenomenon of pleonastic it, and more generally pronouns without explicit nominal antecedents, have been painstakingly studied by linguists. However, despite being identified as one of the open issues of anaphora resolution (Mitkov, 2001), work on automatic identification of pleonastic it is relatively scarce. To date, existing studies in the area fall into one of two categories: one wherein a rule-based approach is used, and the other using a machine-learning approach."}, {"heading": "2.1 Rule-based Approaches", "text": "Paice and Husk (1987) together with Lappin and Leass (1994) provide examples of rulebased systems that make use of predefined syntactic patterns and word lists. The Paice and Husk approach employs bracketing patterns such as it . . . to and it . . . who to meet the syntactic restrictions of extraposition and cleft. The matched portions of sentences are then evaluated by further rules represented by word lists. For example, the it . . . to rule prescribes that one of the \u2018task status\u2019 words, such as good or bad, must be present amid the construct. In order to reduce false positives, general restrictions are applied on sentence features such as construct length and intervening punctuation.\nLappin and Leass\u2019s (1994) approach employs a set of more detailed rules such as It is Modaladj that S and It is Cogv-ed that S, where Modaladj and Cogv are predefined lists of modal adjectives (e.g. good and useful) and cognitive verbs (e.g. think and believe), respectively. Compared to Paice and Husk\u2019s (1987) approach, this method is much more restrictive, especially in its rigidly-specified grammatical constraints. For example, it is not clear from the original Lappin and Leass paper whether the system would be able to recognize sentences such as [0146:014] \u2018It isn\u2019t clear, however, whether . . . \u2019 despite its claim that the system takes syntactic variants into consideration.\nLappin and Leass\u2019s (1994) approach is part of a larger system, and no evaluation is provided. The Paice and Husk (1987) approach, on the other hand, evaluates impressively. It has an accuracy of 93.9% in determining pleonastic constructs on the same data used for rule development, without using part-of-speech tagging or parsing.\nBoth rule-based systems rely on patterns to represent syntactic constraints and word lists to represent semantic constraints. This makes them relatively easy to implement and maintain. However, these features also make them less scalable \u2013 when challenged with large and unfamiliar corpora, their accuracies deteriorate. For example, Paice and Husk (1987) noticed nearly a 10% decrease in accuracy when rules developed using one subset of the corpus are applied to another subset without modifications. Boyd, Gegg-Harrison, and Byron (2005) also observed a significant performance penalty when the approach was applied to a different corpus. In other words, rule-based systems can only be as good as they are designed to be. Denber (1998) suggested using WordNet (Fellbaum, 1998) to extend the word lists, but it is doubtful how helpful this would be considering the enormous number of possible words that are not included in existing lists and the number of inapplicable words that will be identified by such an approach.\nIdentification of Pleonastic It Using the Web"}, {"heading": "2.2 Machine-learning Approaches", "text": "Recent years have seen a shift toward machine-learning approaches, which shed new light on the issue. Studies by Evans (2001, 2000) and Boyd et al. (2005) are examples of this class. Both systems employ memory-based learning on grammatical feature vectors; Boyd et al.\u2019s approach also includes a decision tree algorithm that produces less ideal results. In his attempt to place uses of it into seven categories, including pleonastic and nominal anaphoric among others, Evans uses 35 features to encode information such as position/proximity, lemmas, and part-of-speech, related to both the pronoun and other components of interest, such as words and noun phrases, in the sentence. Evans reported 73.38% precision and 69.25% recall for binary classification of pleonastic cases, and an overall binary classification accuracy of 71.48%. In a later study featuring MARS3, a fully automatic pronoun resolution system that employs the same approach, Mitkov, Evans, and Orasan (2002) reported a significantly higher binary classification accuracy of 85.54% when the approach is applied to technical manuals.\nBoyd et al.\u2019s (2005) approach targets pleonastic it alone. It uses 25 features, most of which concern lengths of specific syntactic structures; also included are part-of-speech information and lemmas of verbs. The study reports an overall precision of 82% and recall of 71%, and, more specifically, recalls on extrapositional and cleft constructs of 81% and 45%, respectively.\nIn addition, Clemente, Torisawa, and Satou (2004) used support vector machines with a feature-set similar to that proposed by Evans (2001) to analyze biological and medical texts, and reported an overall accuracy of 92.7% \u2013 higher than that of their own memory-based learning implementation. Ng and Cardie (2002) built a decision tree for binary anaphoricity classification on all types of noun phrases (including pronouns) using the C4.5 induction algorithm. Ng and Cardie reported overall accuracies of 86.1% and 84.0% on the MUC-6 and MUC-7 data sets. Categorical results, however, are not reported and it is not possible to determine the system\u2019s performance on pronouns. Using automatically induced rules, Mu\u0308ller (2006) reported an overall accuracy of 79.6% when detecting non-referential it in spoken dialogs. An inter-annotator agreement study conducted in the same paper indicates that it is difficult even for humans to classify instances of it in spoken dialogs. This finding is supported by our own experiences.\nMachine-learning approaches are able to partly circumvent the restrictions imposed by fixed word lists or rigid grammatical patterns through learning. However, their advantage also comes with a price \u2013 training is required in the initial development phase and for different corpora re-training is preferable since lemmas are part of the feature sets. Since the existing approaches fall within the area of supervised learning (i.e. training data need to be manually classified), the limited number of lemmas they gather from training may lead to degraded performance in unfamiliar circumstances. Moreover, the features used during learning are unable to reliably capture the subtleties of the original sentences, especially when considering non-technical documents. For example, the quantitative features frequently used in machine-learning approaches, such as position and distance, become less reliable when sentences contain a large number of adjuncts. Additionally, the meanings of lemmas are often domain-dependent and can vary with their local structural and lexical\n3Available online at http://clg.wlv.ac.uk/demos/MARS/\nLi, Musilek, Reformat, & Wyard-Scott\nenvironment \u2013 such nuances cannot be captured by the lemma features alone. In short, while machine-learning approaches generally deliver better performance classifying it than their rule-based counterparts do, they have their own inherent problems."}, {"heading": "3. A Web Based Approach", "text": "Both syntactic patterns and semantics of various clause constituents play important roles in determining if a third-person personal pronoun is pleonastic. The role of grammar is quite obvious since both extrapositions and clefts must follow the grammatical patterns by which they are defined. For example, the most commonly seen type of it-extraposition follows the pattern:\nit + copula + status + subordinate clause [0089:017] It is easy to see why the ancient art is on the ropes.\nIn contrast, the role semantics plays here is a little obscure until one sits down and starts to \u201cdream up exceptions\u201d (Paice & Husk, 1987) analogous to [0074:005] \u2018 . . . it has taken measures to continue shipments during the work stoppage.\u2019 vis-a\u0300-vis [0367:044] \u2018 . . . it didn\u2019t take a rocket scientist to change a road bike into a mountain bike . . . \u2019, where referential and pleonastic cases share the same syntactic structure. Despite its less overt role, failure to process semantic information can result in a severe degradation of performance. This observation is supported by the word-list-based systems\u2019 dramatic decay in accuracy when they are confronted with text other than that they obtained their word lists from.\nLike every other classification system, the proposed system strives to cover as many cases as possible and at the same time perform classification as accurately as possible. To achieve this, it attempts to make good use of both syntactic and semantic information embedded in sentences. A set of relaxed yet highly relevant syntactic patterns is first applied to the input text to filter out the syntactically inviable cases. Unlike the matching routines of some previous approaches, this process avoids detailed specification of syntactic patterns. Instead, it tries to include every piece of text containing a construct of possible interest. Different levels of semantic examinations are performed for each subtype of pleonastic constructs. For reasons discussed later in Section 3.2.2, semantic analysis is not performed on clefts. A WordNet-based analysis is used to identify weather/time cases because among the samples examined during the system\u2019s development stage, cases pertaining to this class are relatively uniform in their manner of expression. For the most complex and populous class, the extrapositions, candidates are subjected to a series of tests performed as queries against the web. Results of the queries provide direct evidence of how a specific configuration of clause constituents is generally used.\nThe reason that such a corpus-based approach is chosen versus applying manually constructed knowledge sources, such as a word list or WordNet, is fourfold:\n1. Manually constructed knowledge sources, regardless of how comprehensive they are, contain only a small portion of general world knowledge. In the particular settings of this study, general world knowledge is used for making judgements such as which words are allowed to serve as the matrix verb of an extraposition, and even more subtle, which specific sense of a word is permitted.\nIdentification of Pleonastic It Using the Web\n2. Manually compiled knowledge sources are subject to specific manners of organization that may not satisfy the system\u2019s needs. Taking WordNet as an example, it identifies a large number of various relationships among entities, but the information is mainly organized along the axes of synonyms, hypernyms (kind-of relationship), and holonyms (part-of relationship) etc., while it is the surroundings of a particular word that are of more interest to this study.\n3. Natural languages are evolving quickly. Taking English as an example, each year new words are incorporated into the language4 and the rules of grammar have not been immune to changes either. Using a large and frequently-updated corpus such as the web allows the system to automatically adapt to changes in language.\n4. Most importantly, corpora collect empirical evidence of language usage. When the sample size is large enough, as in the case of the web, statistics on how a specific construct is generally used in corpora can be employed as an indicator of its speaker\u2019s intention.\nThe proposed approach is also inspired by Hearst\u2019s (1992) work on mining semantic relationships using text patterns, and many other quests that followed in the same direction (Berland & Charniak, 1999; Poesio, Ishikawa, im Walde, & Vieira, 2002; Markert, Nissim, & Modjeska, 2003; Cimiano, Schmidt-Thieme, Pivk, & Staab, 2005). Unlike these investigations that focus on the semantic relationship among noun phrases, the pleonastic pronoun identification problem mandates more complex queries to be built according to the original sentences. However, the binary nature of the problem also makes it simpler to apply comparative analysis on results of multiple queries, which, in turn, leads to better immunity to noise.\nFigure 1 illustrates the general work flow of the proposed system. A sentence is first preprocessed to obtain a dependency tree with part-of-speech tags, which is then passed on to the syntactic filtering component to determine whether minimum grammatical requirements of the pleonastic constructs are met. It is also during the syntactic filtering process that clefts and weather/time expressions are identified using syntactic cues and the WordNet respectively. The candidate extrapositions are thereafter used to instantiate various queries on search engines; the results returned from the queries serve as parameters for the final decision-making mechanism."}, {"heading": "3.1 Preprocessing", "text": "The preprocessing component transforms the syntactic information embedded in natural language texts into machine-understandable structures. During the preprocessing stage, each word is assigned a part-of-speech tag, and the whole sentence is parsed using a dependency grammar (DG) parser. For simplicity\u2019s sake, the current system is designed to use the WSJ corpus, which is already tagged and parsed with context-free grammar (CFG). A head percolation table similar to that proposed by Collins (1999) is used to obtain the head component of each phrase. The rest of the phrase constituents are then rearranged under the\n4Metcalf and Barnhart (1999) have compiled a chronicle of many important additions to the vocabulary of American English.\nLi, Musilek, Reformat, & Wyard-Scott\nhead component to form the dependency tree using a procedure detailed by Xia and Palmer (2001). Figure 2 illustrates the syntactic structure of a sentence in the WSJ corpus. Both the original CFG parse tree and the derived dependency structure are shown side-by-side. Head entities are underlined in the CFG diagram and circled in the DG diagram.\nIdentification of Pleonastic It Using the Web\nAs shown in Figure 2, the function tags (e.g. SBJ, TMP, and CLR) and tracing information present in the context-free parse tree are not ported to the dependency tree. This is because real-world parsers usually do not produce such tags. Except this deliberate omission, both parse trees contain essentially the same information, only presented in different manners. In this study, dependency structure is preferred over the more popular phrase structure mainly because of its explicit marking of both the head components and the complementing/modifying relationships among various components. This feature is very helpful for instantiating the search-engine queries."}, {"heading": "3.2 Syntactic Filtering", "text": "The syntactic filtering process determines whether a clause meets the grammatical requirements of an extraposition or cleft construct by matching the clause against their respective syntactic patterns."}, {"heading": "3.2.1 Extrapositions", "text": "It-extrapositions occur when a clause is dislocated out of its ordinary position and replaced with it. An it-extraposition usually follows the pattern:\nmatrix clause\ufe37 \ufe38\ufe38 \ufe37 itsubject +  be +  noun phrase adjective phrase prepositional phrase  verb phrase \ufe38 \ufe37\ufe37 \ufe38 matrix verb phrase + extraposed clause (1)\nThis pattern summarizes the general characteristics of subject it-extrapositions, where the pronoun it assumes the subject position. When the matrix verb (the verb following it) is the main copula to be, which serves to equate or associate the subject and an ensuing logical predicate, it must be followed by either a noun phrase, an adjective phrase, or a prepositional phrase.5 There is no special requirement for the matrix verb phrase otherwise. Similarly, there is almost no restriction placed upon the extraposed clause except that a full clause should either be introduced without a complementizer (e.g. [0037:034] \u2018It \u2019s a shame their meeting never took place.\u2019) or led by that, whether, if, or one of the wh-adverbs (e.g. how, why, when, etc.). These constraints are developed by generalizing a small portion of the WSJ corpus and are largely in accordance with the patterns identified by Kaltenbo\u0308ck (2005). Compared to the patterns proposed by Paice and Husk (1987), which also cover cases such as it . . . to , it . . . that and it . . . whether , they allow for a broader range of candidates by considering sentences that are not explicitly marked (such as [0037:034]). The above configuration covers sentences such as:\n5Other copula verbs do not receive the same treatment. This arrangement is made to accommodate cases where verbs such as to seem and to appear are immediately followed by an extraposed clause.\nLi, Musilek, Reformat, & Wyard-Scott\n[0529:009] Since the cost of transporting gas is so important to producers\u2019 ability to sell it, it helps to have input and access to transportation companies.\n[0037:034] It \u2019s a shame their meeting never took place.\n[0360:036] It is insulting and demeaning to say that scientists \u201cneeded new crises to generate new grants and contracts . . .6\n[0336:019] It won\u2019t be clear for months whether the price increase will stick. Except in the case of the last sentence, the above constructs are generally overlooked by the previous rule-based approaches identified in Section 2.1. As the last sample sentence illustrates, the plus sign (+) in the pattern serves to indicate a forthcoming component rather than suggest two immediately adjacent components.\nSome common grammatical variants of the pattern are also recognized by the system, including questions (both direct and indirect), inverted sentences, and parenthetical expressions (Paice & Husk, 1987). This further expands the pattern\u2019s coverage to sentences such as:\n[0772:006] I remembered how hard it was for an outsider to become accepted . . .\n[0562:015] \u201cThe sooner our vans hit the road each morning, the easier it is for us to fulfill that obligation.\u201d\n[0239:009] Americans it seems have followed Malcolm Forbes\u2019s hot-air lead and taken to ballooning in a heady way.\nAside from being the subject of the matrix clause, extrapositional it can also appear in the object position. The system described here captures three flavors of object extraposition. The first type consists of instances of it followed by an object complement:\n[0044:014] Mrs. Yeargin was fired and prosecuted under an unusual South Carolina law that makes it a crime to breach test security.\nIn this case the system inserts a virtual copula to be between the object it and the object complement (a crime), making the construct applicable to the pattern of subject extraposition. For example, the underlined part of the prior example translates into \u2018it is a crime to breach test security\u2019.\nThe other two kinds of object extraposition are relatively rare:\n\u2022 Object of verb (without object complement)\n[0114:007] Speculation had it that the company was asking $100 million for an operation said to be losing about $20 million a year . . .\n\u2022 Object of preposition\n[1286:054] They should see to it that their kids don\u2019t play truant . . .\nThese cases cannot be analyzed within the framework of subject extraposition and thus must be approached with a different pattern:\nverb + [preposition] it object + full clause (2) 6Neither insulting nor demeaning is in Paice and Husk\u2019s (1987) list of \u2018task status words\u2019 and therefore\ncannot activate the it . . . to pattern.\nIdentification of Pleonastic It Using the Web\nThe current system requires that the full clauses start with a complementizer that. This restriction, however, is included only to simplify implementation. Although in object expositions it is more common to have clauses led by that, full clauses without a leading complementizer are also acceptable.\nAccording to Kaltenbo\u0308ck\u2019s (2005) analysis there are special cases in which noun phrases appear as an extraposed component, such as \u2018It\u2019s amazing the number of theologians that sided with Hitler.\u2019 He noted that these noun phrases are semantically close to subordinate interrogative clauses and can therefore be considered a marginal case of extraposition. However, no such cases were found in the corpus during the annotation process and they are consequently excluded from this study."}, {"heading": "3.2.2 Cleft", "text": "It-clefts are governed by a slightly more restricted grammatical pattern. Following Hedberg (1990), it-clefts can be expressed as follows:\nit subject + copula + clefted constituent + cleft clause (3)\nThe cleft clause must be finite (i.e. a full clause or a relative clause); and the clefted constituents are restricted to either noun phrases, clauses, or prepositional phrases.7 Examples of sentences meeting these constraints include:\n[0296:029] \u201cIt \u2019s the total relationship that is important.\u201d\n[0267:030] It was also in law school that Mr. O\u2019Kicki and his first wife had the first of seven daughters.\n[0121:048] \u201cIf the market goes down, I figure it \u2019s paper profits I\u2019m losing.\u201d In addition, another non-canonical and probably even marginal case is also identified as a cleft:\n[0296:037] I really do not understand how it is that Filipinos feel so passionately involved in this father figure that they want to dispose of and yet they need.\nText following the structure of this sample, where a wh-adverb immediately precedes it, is captured using the same syntactic pattern by appending a virtual prepositional phrase to the matrix copula (e.g. \u2018for this reason\u2019), as if the missing information has already been given.\nEach of the examples above represents a possible syntactic construct of it-clefts. While it is difficult to tell the second and the third cases apart from their respective extrapositional counterparts, it is even more difficult to differentiate the first case from an ordinary copula sentence with a restrictive relative clause (RRC). For example, the following sentence,\n[0062:012] \u201cIt \u2019s precisely the kind of product that\u2019s created the municipal landfill monster,\u201d the editors wrote.\nand its slightly modified version, [0062:012\u00b4] \u201cIt \u2019s this kind of product that\u2019s created the municipal landfill mon-\nster,\u201d the editors wrote. 7Adjective and adverb phrases are also possible but they are relatively less frequent and are excluded\nfrom this analysis.\nLi, Musilek, Reformat, & Wyard-Scott\nare similar in construction. However, the latter is considered a cleft construct while the first is an RRC construct. To make things worse, and as pointed out by many (e.g. Boyd et al., 2005, p.3, example 5), sometimes it is impossible to make such a distinction without resorting to the context of the sentence.\nFortunately, in the majority of cases the syntactic features, especially those of the clefted constituent, provide some useful cues. In an it-cleft construct, the cleft clause does not constitute a head-modifier relationship with the clefted constituent, but instead forms an existential and exhaustive presupposition8 (Davidse, 2000; Hedberg, 2000; Lambrecht, 2001). For example, \u2018I figure it\u2019s paper profits I\u2019m losing.\u2019 implies that in the context there is something (and only one thing) that the speaker is going to lose, and further associates \u2018paper profits\u2019 with it. This significant difference in semantics often leaves visible traces on the syntactic layer, some of which, such as the applicability of proper nouns as clefted constituents, are obvious. Others are less obvious. The system utilizes the following grammatical cues when deciding if a construct is an it-cleft9:\n\u2022 For the clefted constituent:\n\u2013 Proper nouns10 or pronouns, which cannot be further modified by an RRC;\n\u2013 Common nouns without determiner, which generally refer to kinds11;\n\u2013 Plurals, which violate number agreement;\n\u2013 Noun phrases that are grounded with demonstratives or possessives, or that are modified by RRCs, which unambiguously identify instances, making it unnecessary in most cases to employ an RRC;\n\u2013 Noun phrases grounded with the definite determiner the, and modified by an of -preposition whose object is also a noun phrase grounded with the or is in plural. These constructs are usually sufficient for introducing uniquely identifiable entities (through association), thus precluding the need for additional RRC modifiers. The words kind, sort, and their likes are considered exceptions of this rule;\n\u2013 Adverbial constructs that usually do not appear as complements. For example, phrases denoting location (here, there etc.) or a specific time (today, yesterday etc.), or a clause led by when; and\n\u2013 Full clauses, gerunds, and infinitives.\n\u2022 For the subordinate clause:\n\u2013 Some constructs appear awkward to be used as an RRC. For example, one would generally avoid using sentences such as \u2018it is a place that is dirty \u2019, as there are\n8This applies to canonical clefts, which do not include the class represented by [0267:030]. 9A construct is considered an it-cleft if any of the conditions are met.\n10There are exceptional cases where proper names are used with additional determiners and RRC modifiers, such as in \u2018the John who was on TV last night \u2019, c.f. Sloat\u2019s (1969) account.\n11The validity of this assertion is under debate (Krifka, 2003). Nevertheless, considering the particular syntactic setting in discussion, it is highly unlikely that bare noun phrases are used to denote specific instances.\nIdentification of Pleonastic It Using the Web\nbetter alternatives. In the current implementation two patterns are considered inappropriate for RRCs, especially in the syntactic settings described in Equation 3: A) the subordinate verb phrase consists of only a copula verb and an adjective; and B) the subordinate verb phrase consists of no element other than the verb itself.\n\u2022 Combined:\n\u2013 When the clefted constituent is a prepositional phrase and the subordinate clause is a full clause, such as in the case of [0267:030], the construct is classified as a cleft12.\nSome of these rules are based on heuristics and may have exceptions, making them less ideal guidelines. Moreover, as mentioned earlier, there are cleft cases that cannot be told apart from RRCs by any grammatical means. However, experiments show that these rules are relatively accurate and provide appropriate coverage, at least for the WSJ corpus."}, {"heading": "3.2.3 Additional Filters", "text": "Aside from the patterns described in earlier sections, a few additional filters are installed to eliminate some semantically unfit constructs and therefore reducing the number of trips to search engines. The filtering rules are as follows:\n\u2022 For a clause to be identified as a subordinate clause and subsequently processed for extraposition or cleft, the number of commas, dashes and colons between the clause and it should be either zero or more than one, a rule adopted from Paice and Husk\u2019s (1987) proposal. \u2022 Except the copula to be, sentences with matrix verbs appearing in their perfect tense are not considered for either extraposition or cleft. \u2022 When it is the subject of multiple verb phrases, the sentence is not considered for either extraposition or cleft. \u2022 Sentences having a noun phrase matrix logical predicate together with a subordinate relative clause are not considered for extraposition. \u2022 Sentences having both a matrix verb preceded by modal auxiliaries could or would and a subordinate clause led by if or a wh-adverb are not considered for extraposition. For example, [0013:017] \u2018 . . . it could complete the purchase by next summer if its bid is the one approved by . . . \u2019 is not considered for extraposition.\nExcept for the first, these rules are optional and can be deactivated in case they introduce false-negatives."}, {"heading": "3.3 Using the Web as a Corpus", "text": "The first question regarding using the web as a corpus is whether it can be regarded as a corpus at all. As Kilgarriff and Grefenstette (2003) pointed out, following the definition of\n12In case it is not a cleft, chances are that it is an extraposition. This assumption, therefore, does not affect the overall binary classification.\nLi, Musilek, Reformat, & Wyard-Scott\ncorpus-hood that \u2018a corpus is a collection of texts when considered as an object of language or literary study\u2019, the answer is yes. With the fundamental problem resolved, what remains is to find out whether the web can be an effective tool for NLP tasks.\nAs a corpus, the web is far from being well-balanced or error-free. However, it has one feature in which no other corpus can be even remotely comparable \u2013 its size. No one knows exactly how big it is, but each of the major search engines already indexes billions of pages. Indeed, the web is so large that sometimes a misspelled word can yield tens of thousands of results (try the word neglectible). This sends out a mixed signal about using the web as a corpus: on the good side, even relatively infrequent terms yield sizable results; on the bad side, the web introduces much more noise than manually-compiled corpora do. In Markert and Nissim\u2019s (2005) recent study evaluating different knowledge sources for anaphora resolution, the web-based method achieves far higher recall ratio than those that are BNC- and WordNet-based, while at the same time yielding slightly lower precision. Similar things can be said about the web\u2019s diverse and unbalanced composition, which means that it can be used as a universal knowledge source \u2013 only if one can manage not to get overwhelmed by non-domain-specific information.\nThat being said, it is still very hard to overstate the benefits that the web offers. As the largest collection of electronic texts in natural language, it not only hosts a good portion of general world knowledge, but also stores this information using the very syntax that defines our language. In addition, it is devoid of the systematic noise introduced into manually-constructed knowledge sources during the compilation process (e.g. failure to include less frequent items or inflexible ways of information organization). Overall, the web is a statistically reliable instrument for analyzing various semantic relationships stored in natural languages by means of examples.\nAs also suggested by Kilgarriff (2007) and many others, it is technically more difficult to exploit the web than to use a local corpus and it can often be dangerous to rely solely on statistics provided by commercial search engines. This is mainly due to the fact that commercial search engines are not designed for corpus research. Worse, some of their design goals even impede such uses. For example, search engines skew the order of results using a number of different factors in order to provide users with the \u2018best\u2019 results. Combined with this is the fact that they only return results up to certain thresholds, making it essentially impossible to get unbiased results. Other annoyances include unreliable result counts, lack of advanced search features13, and unwillingness to provide unrestricted access to their APIs. Before a new search engine specifically designed for corpus research is available, it seems we will have to work around some of those restrictions and live with the rest."}, {"heading": "3.4 Design of Search Engine Queries", "text": "As discussed in previous sections, it-extrapositions cannot be reliably identified using syntactic signatures alone or in combination with synthetic knowledge bases. To overcome the artificial limitations imposed by knowledge sources, the proposed system resorts to the web for the necessary semantic information.\n13For example, the wildcard (\u2217) feature on Google, which could be immensely useful for query construction, no longer restricts its results to single words since 2003; Yahoo\u2019s ability to support alternate words within quoted texts is limited, while MSN does not offer that feature at all.\nIdentification of Pleonastic It Using the Web\nThe system employs three sets of query patterns: the what-cleft, the comparative expletive test, and the missing-object construction. Each set provides a unique perspective of the sentence in question. The what-cleft pattern is designed to find out if the sentence under investigation has a valid what-cleft counterpart. Since it-extrapositions and what-clefts are syntactically compatible (as shown in Section 3.4.1) and valid readings can usually be obtained by transformations from one construct to the other, the validity of the what-cleft is indicative of whether or not the original sentence is extrapositional. The comparative expletive test patterns are more straightforward \u2013 they directly check whether the instance of it can be replaced by other entities that cannot be used expletively in the same context as that of an extrapositional it. If the alternate construct is invalid, the original sentence can be determined as expletive. The third set of patterns are supplemental. They are intended only for identifying the relatively rare phenomenon of missing-object construction, which may not be reliably handled by the previous pattern sets.\nDesigning the appropriate query patterns is the most important step in efforts to exploit large corpora as knowledge sources. For complex queries against the web, it is especially important to suppress unwanted uses of certain components, which could result from different word senses, different sentence configuration, or a speaker\u2019s imperfect command of the language. For example, the query \u201cit is a shame that\u201d could return both a valid extrapositional construct and an RRC such as \u2018It is a shame that is perpetuated in his life\u2019; and the query \u201cwhat is right is that\u201d could return both valid what-clefts and sentences such as \u2018Why we ought to do what is right is that . . . \u2019 This study employs three different approaches to curb unwanted results:\n\u2022 The first and most important measure is comparative analysis \u2013 pairs of similarlyconstructed queries are sent out to the search engine and the ratios of result counts are used for the decision. This method is effective for problems caused by both different sentence configuration and bad language usage, since generally neither contribute a fraction of results large enough to significantly affect the ratio. The method also provides a normalized view of the web because what is of interest to this study is not exactly how frequently a specific construct is used, but whether it is more likely to carry a specific semantic meaning when it is used.\n\u2022 The second measure is to use stubs in query patterns, as detailed in the following sections. Stubs help ensure that the outcomes of queries are syntactically and semantically similar to the original sentences and partly resolve the problems caused by word sense difference.\n\u2022 Finally, when it is infeasible to use comparative analysis, part of the query results are validated to obtain an estimated number of valid results.\n3.4.1 Query Pattern I: The What-cleft\nThe first query pattern,\nWhat + verb phrase + copula + stub (4)\nis a what-(pseudo-)cleft construct that encompasses matrix-level information found in an it-extraposition. The pattern is obtained using a three-step transformation as illustrated\nLi, Musilek, Reformat, & Wyard-Scott\nbelow:\nit + verb phrase + clause It is easy to see why the ancient art is on the ropes. [0089:017]\n1) \u21d3 clause + verb phrase To see why the ancient art is on the ropes is easy. 2) \u21d3 What + verb phrase + copula + clause What is easy is to see why the ancient art is on the ropes. 3) \u21d3 What + verb phrase + copula + stub What is easy is to\n(5)\nStep 1 transforms the original sentence (or clause) to the corresponding non-extraposition form by removing the pronoun it and restoring the information to the canonical subjectverb-complement order. In the above example, the clause to see . . . is considered the real subject and is moved back to its canonical position. The non-extraposition form is subsequently converted during step 2 to a what-cleft that highlights its verb phrase. Finally, in step 3, the subordinate clause is reduced into a stub to enhance the pattern\u2019s coverage. The choice of stub depends on the structure of the original subordinate clause: to is used when the original subordinate clause is an infinitive, a gerund, or a for . . . infinitive construct14. For the rest of the cases, the original complementizer, or that, in the case where there is no complementizer, is used as stub. The use of a stub in the pattern imposes a syntactic constraint, in addition to the ones prescribed by the pronoun what and the copula is, that demands a subordinate clause be present in query results. The choice of stubs also reflects, to a certain degree, the semantics of the original texts and therefore can be seen as a weak semantic constraint.\nBelow are a few other examples of the what-cleft transformation: [0059:014] It remains unclear whether the bond issue will be rolled over. \u21d2\nWhat remains unclear is whether\n[0037:034] It \u2019s a shame their meeting never took place. \u21d2 What is a shame is that\nThe what-cleft pattern only identifies whether the matrix verb phrase is capable of functioning as a constituent in an it-extraposition. Information in the subordinate clauses is discarded because this construct is used relatively infrequently and adding extra restrictions to the query will prohibit it from yielding results in many cases.\nSome it-extraposition constructs such as \u2018it appears that . . . \u2019 and \u2018it is said that . . . \u2019 do not have a valid non-extraposition counterpart, but the what-cleft versions often bear certain degrees of validity and queries instantiated from the pattern will often yield results (albeit not many) from reputable sources. It is also worth noting that although the input and output constructs of the transformation are syntactically compatible, they are not necessarily equivalent in terms of givenness (whether and how information in one sentence\n14According to Hamawand (2003), the for . . . infinitive construct carries distinct semantics; reducing it to the infinitive alone changes its function. However, with only a few exceptional cases, we find this reduction generally acceptable. i.e. The lost semantics does not affect the judgment of expletiveness.\nIdentification of Pleonastic It Using the Web\nhas been entailed by previous discourse). Kaltenbo\u0308ck (2005) noted that the percentage of extrapositional it constructs carrying new information varies greatly depending on the category of the text. In contrast, a what-cleft generally expresses new information in the subordinate clause. The presupposed contents in the two constructs are different, too. What-clefts, according to Gundel (1977), from which the it-clefts are derived, have the same existential and exhaustive presuppositions carried by their it-cleft counterparts. On the other hand, the it-extrapositions, which are semantically identical to their corresponding non-extrapositions, lack such presuppositions or, at most, imply them at a weaker strength (Geurts & van der Sandt, 2004). These discrepancies hint that a derived what-cleft is a \u2018stronger\u2019 expression than the original extraposition, which may have been why queries instantiated from the pattern tend to yield considerably less results.\nAnother potential problem with this pattern is its omission of the subordinate verb, which occasionally leads to false positives. For example, it does not differentiate between \u2018it helps to have input and access to transportation companies\u2019 and \u2018it helps expand our horizon\u2019. This deficiency is accommodated by additional query patterns."}, {"heading": "3.4.2 Query Pattern II: Comparative Expletiveness Test", "text": "The second group of patterns provides a simplified account of the original text in a few different flavors. After execution, the results from individual queries are compared to assess the expletiveness of the subject pronoun. This set of patterns takes the following general form:\npronoun + verb phrase + simplified extraposed clause (6)\nThe only difference among individual patterns lies in the choice of the matrix clause subject pronoun: it, which, who, this, and he. When the patterns are instantiated and submitted to a search engine, the number of hits obtained from the it version should by far outnumber that of the other versions combined if the original text is an it-extraposition; otherwise the number of hits should be at least comparable. This behavior reflects the expletive nature of the pronoun in an it-extraposition, which renders the sentence invalid when it is replaced with other pronouns that have no pleonastic use.\nA simplified extraposed clause can take a few different forms depending on its original structure:\n15The for . . . passive-infinitive is transformed into active voice (e.g. \u2018for products to be sold \u2019\u2192\u2018to sell products\u2019).\nLi, Musilek, Reformat, & Wyard-Scott\nSimilar to the case of Pattern I, the stub is used both as a syntactic constraint and a semantic cue. Depending on the type of search engine, the stub can be either the, which is the most widely used determiner, or a combination of various determiners, personal pronouns and possessive pronouns, all of which indicate a subsequent noun phrase. In the case that an infinitive construct involves a subordinate clause led by a wh-adverb or that, the complementizer is used as stub. This arrangement guarantees that the results returned from the query conform to the original text syntactically and semantically. A null value should be used for stubs in an object position if the original text lacks a nominal object. To illustrate the rules of transformation, consider the following sentence:\n[0044:010] \u201cMy teacher said it was OK for me to use the notes on the test,\u201d he said.\nThe relevant part of the sentence is:\nit + verb phrase + clause it was OK for me to use the notes on the test\nApplying the clause simplification rules, the first query is obtained:\nit + verb phrase + simplified clause it was OK to use the\nThe second query is generated by simply replacing the pronoun it with an alternative pronoun:\nalternative pronoun + verb phrase + simplified clause he was OK to use the\nGoogle reports 94,200 hits for the it query, while only one page is found using the alternative query. Since the pronoun it can be used in a much broader context, replacing it with he alone hardly makes a balanced comparison. Instead, the combination of which, who, this, and he is used, as illustrated in the following examples:\n[0044:010] \u201cMy teacher said it was OK for me to use the notes on the test,\u201d he said. \u21d2{\nit which/who/this/he\n} was ok to use the\n[0089:017] It is easy to see why the ancient art is on the ropes. \u21d2{ it which/who/this/he } is easy to see why\nA special set of patterns is used for object extrapositions16 to accommodate their unique syntactic construct:\nverb + [preposition] pronoun + that + stub (7)\nStubs are chosen according to the same rules for the main pattern set, however only one alternative pronoun \u2013 them \u2013 is used.\n16Instances containing object complements are treated under the framework of subject extraposition and are not included here.\nIdentification of Pleonastic It Using the Web\n[0114:007] Speculation had it that the company was asking $100 million for an operation said to be losing about $20 million a year . . . \u21d2\nhad { it them } that the"}, {"heading": "3.4.3 Query Pattern III: Missing-object Construction", "text": "One search engine annoyance is that they ignore punctuation marks. This means one can only search for text that matches a specific pattern string, but not sentences that end with a pattern string. The stubs used in Pattern II are generally helpful for excluding sentences that are semantically incompatible with the original from the search results. However, under circumstances where no stub is attached to the queries (where the query results should ideally consist of only sentences that end with the query string), the search engine may produce more results than needed. Sentences conforming to the pattern it + copula + missing-object construction, such as (referring to a book) \u2018it is easy to read\u2019, present one such situation. What is unique about the construction \u2013 and why special treatment is needed \u2013 is that a missing-object construction usually has an it-extraposition counterpart in which the object is present, for example \u2018it is easy to read the book \u2019. Since the missing-object constructions are virtually the same (only shorter) as their extrapositional counterparts, there is a good chance for them to be identified as extrapositions. The following are some additional examples of the missing-object construction:\n[0290:025] Where non-violent civil disobedience is the centerpiece, rather than a lawful demonstration that may only attract crime, it is difficult to justify.\n[0018:024-025] No price for the new shares has been set. Instead, the companies will leave it up to the marketplace to decide.\n[0111:005] He declined to elaborate, other than to say, \u201cIt just seemed the right thing to do at this minute.\nTwo sets of patterns are proposed17 to identify the likes of the foregoing examples. The first pattern, the compound adjective test, is inspired by Nanni\u2019s (1980) study considering the easy-type adjective followed by an infinitive (also commonly termed tough construction) as a single complex adjective. The pattern takes the form\nstub + adjectivebase-to -verb (8)\nwhere the stub, serving to limit the outcome of the query to noun phrases, takes a combination of determiners or a/an alone; the original adjective is also converted to its base form adjectivebase if it is in comparative or superlative form. Expanding on Nanni\u2019s original claims, the pattern can be used to evaluate all adjectives18 as well as constructs furnished with for . . . infinitive complements. The following example demonstrates the pattern\u2019s usage:\n17Preliminary experiments have confirmed the effectiveness of the patterns. However, due to sparseness of samples belonging to this class, they are not included in the reported evaluation.\n18This is based on the observation that compounds such as \u2018ready-to-fly\u2019 (referring to model aircrafts) exist, and that it is hard to obtain a complete enumeration of the easy-type adjectives.\nLi, Musilek, Reformat, & Wyard-Scott\n[0258:024] The machine uses a single processor, which makes it easier to program than competing machines using several processors. \u21d2 an easy-to-program\nThe second set consists of two patterns used for comparative analysis with the same general profile:\nthat + verbgerund + stub (9)\nwhere verbgerund is the gerund form of the original infinitive. The complementizer that is used for the sole purpose of ensuring that verbgerund appears as the subject of a subordinate clause in all sentences returned by the queries. In other words, phrases such as \u2018computer programming \u2019 and \u2018pattern matching \u2019 are excluded. For the first pattern, the stub is a combination of prepositions (currently in and from are chosen); for the second one, a combination of determiners or the alone is used. For example:\n[0258:024] The machine uses a single processor, which makes it easier to program than competing machines using several processors. \u21d2\nthat programming { in|from the } This set of patterns tests the transitivity of the verb in a semantic environment similar to that of the original sentence. If the verb is used transitively more often, the pattern with determiners should yield more results, and vice versa. As supported by all preceding sample sentences, a usually-transitive verb used without an object19 is a good indicator of missing-object construction and the sentence should be diagnosed as referential."}, {"heading": "3.4.4 Query Instantiation", "text": "Patterns must be instantiated with information found in original sentences before they are submitted to a search engine. Considering the general design principles of the system, it is not advisable to instantiate the patterns with original texts \u2013 doing so significantly reduces the queries\u2019 coverage. Instead, the object of the matrix verb phrase is truncated and the matrix verb expanded in order to obtain the desired level of coverage.\nThe truncation process provides different renditions based on the structure of the original object:\n\u2022 Adjective phrases: Only the head word is used. When the head word is modified by not or too, the modifier is also retained in order to better support the too . . . to construct and to maintain compatibility with the semantics of the original text.\n\u2022 Common noun phrases:\n\u2013 with a possessive ending/pronoun, or an of -preposition: The phrase is replaced by $PRPS$ plus the head word. $PRPS$ is either a list of possessive pronouns or one of those more widely used, depending on caliber of the search engine used. For example, \u2018his location\u2019 can be expanded to \u2018its | my | our | his | her | their | your location\u2019.\n19An omitted object of a preposition (e.g. \u2018It is difficult to account for.\u2019) has the same effect, but it is identifiable through syntactic means alone.\nIdentification of Pleonastic It Using the Web\n\u2013 with determiners: The phrase is replaced by a choice of $DTA$, $DTTS$, $DTTP$, or a combination of $DTA$ and $DTTS$, plus the head word. $DTA$ is a list of (or one of the) general determiners (i.e. a, an, any etc.). $DTTS$ refers to the combination of the definite article the and the singular demonstratives this and that. $DTTP$ is the plural counterpart of $DTTS$. The choice is based on the configuration of the original text so as to maintain semantic compatibility.\n\u2013 without determiner: Only the head word is used.\n\u2022 Proper nouns and pronouns: The phrase is replaced by $PRP$, which is a list of (or one of the) personal pronouns.\n\u2022 Prepositional phrases: The object of the preposition is truncated in a recursive operation.\n\u2022 Numeric values: The phrase \u2018a lot \u2019 is used instead.\nMatrix verbs are expanded to include both the simple past tense and the third person singular present form with the aid of WordNet and some generic patterns. Where applicable, particles such as out and up also remain attached to the verb.\nGenerally speaking, truncation and expansion are good ways of boosting the patterns\u2019 coverage. However, the current procedures of truncation are still crude, especially in their handling of complex phrases. For example, the phrase \u2018a reckless course of action\u2019 ([0198:011]) yields \u2018$PRPS$ course\u2019, which results in a total loss of the original semantics. Further enhancements of the truncation process may improve the performance but the improvement will likely be limited due to the endless possibilities of language usage and constraints imposed by search engines.\nAside from truncating and expanding the original texts, a stepped-down version of Pattern II, denoted Pattern II\u2032, is also provided to further enhance the system\u2019s coverage. The current scheme is to simply replace the extraposed clause with a new stub \u2013 to \u2013 if the original extraposed clause is an infinitive, a for . . . infinitive, or a gerund construct. For example,\n[0089:017] It is easy to see why the ancient art is on the ropes. \u21d2{ it which/who/this/he } is easy to\nIn other situations, no downgraded version is applied.\n3.5 Binary Classification of It-extraposition\nFive factors are taken into consideration when determining whether the sentence in question is an it-extraposition:\nEstimated popularity of the what-cleft construct (query Pattern I) denoted as\nW = nw \u00d7 vw\nLi, Musilek, Reformat, & Wyard-Scott\nwhere nw is the number of results reported by the search engine, and vw is the percentage of valid instances within the first batch of snippets (usually 10, depending on the search engine service) returned with the query. Validation is performed with a case-sensitive regular expression derived from the original query. Since the whatcleft pattern is capitalized at the beginning, the regular expression only looks for instances appearing at the beginning of a sentence. It is particularly important to validate the results of what-cleft queries because some search engines can produce results based on their own interpretation of the original query. For example, Google returns pages containing \u201cWhat\u2019s found is that\u201d for the query \u201cWhat found is that\u201d, which might be helpful for some but is counterproductive for the purpose of this study.\nResult of the comparative expletiveness test (query Pattern II) denoted as\nr = nX nit\nwhere nit is the number of results obtained from the original it version of the query, and nX is the total number of results produced by replacing it with other pronouns such as which and who. The smaller the ratio r is, the more likely that the sentence being investigated is an extraposition. Extrapositional sentences usually produce an r value of 0.1 or less. When both versions of the query yield insufficient results (max(nit, nX) < Nmin), r takes the value Rscarce = 1000. Since it-extrapositions are relatively rare, it is better to assume that a sentence is not extrapositional when there is insufficient data to judge otherwise. In the case where nX is sufficient but the it version of the query produces no result (nX >= Nmin AND nit = 0), r takes the value Rzero = 100. Values of Rzero and Rscarce are large numbers chosen arbitrarily, mainly for visualization purposes. In other words both Rzero and Rscarce hint that the sentence is probably not extrapositional, however neither indicates the degree of likelihood.\nResult of the stepped-down comparative expletiveness test denoted as r\u2032 = n \u2032 X\nn\u2032it , where n\u2032it and n \u2032 X are the number of results returned from the\nit version and the alternate version of the stepped-down queries (c.f. Section 3.4.4, Page 359). The stepped-down queries are \u2018simplified\u2019 versions of the queries used to calculate r. Due to this simplification, r\u2032 is usually more sensitive to extrapositions. However not all queries have stepped-down versions, in which case the original queries are reused, causing r\u2032 = r. Similar to the way r is defined, r\u2032 also takes the values Rscarce and Rzero in special situations.\nSynthesized expletiveness A new variable R is defined based on the values of r, nit, nX , and r\u2032:\nR = { r, if max(nit, nX) \u2265 Nmin, r\u2032, if max(nit, nX) < Nmin.\nIf the original queries yield enough results, R takes the value of r since the original queries better preserve sentence context and are generally more accurate. However,\nIdentification of Pleonastic It Using the Web\nwhen original queries fail, the system resorts to the back-up method of using the stepped-down queries and bases its judgement on their results instead. Overall, R can be seen as a synthesized indicator of how the subject pronoun is generally used in a similar syntactic and semantic setting to that of the original sentence.\nSyntactic structure of the sentence denoted as S, a binary variable indicating if the sentence under investigation belongs to a syntactic construct that is more prone to generating false-positives. On average the what-cleft queries yield fewer results and are less reliable since they cannot be used to provide comparative ratios. However, they are still useful as the last line of defence to curb the impacts of certain syntactic constructs that repeatedly cause the comparative expletive tests to produce false-positives. Currently only one construct is identified \u2013 the it verb infinitive construct, as in \u2018it helps to have input from everyone\u2019 and \u2018it expects to post the results tomorrow \u2019. Therefore,\nS = { TRUE, if sentence matches it verb infinitive, FALSE, otherwise.\nThe final binary classification of it-extraposition, E, is defined as follows:\nE = { ((R < Rexp) AND (W > Nmin)), if S = TRUE, (R < Rexp), if S = FALSE.\n(10)\nwhere Nmin and Rexp, set to 10 and 0.15 respectively in this study, are threshold constants chosen based upon empirical observations. In other words, the system recognizes an instance of it as extrapositional if it is unlikely (by comparing R to Rexp) that an alternative pronoun is used in its place under the same syntactic and semantic settings. For it verb infinitive constructs, it is also required that the sentence has a viable what-cleft variant (by comparing W to Nmin).\nIt is worth noting that today\u2019s major commercial search engines do not return the exact number of results for a query but rather their own estimates. The negative effect of this is somewhat mitigated by basing the final decision on ratios instead of absolute numbers."}, {"heading": "4. Case Study", "text": "To better illustrate the system work flow, two sample sentences are selected from the WSJ corpus to be taken through the whole process. The first sample, [0231:015], is classified as an it-extraposition; the other, [0331:033] (with the preceding sentence providing context), is a referential case with a nominal antecedent. Some particulars of the implementation are also discussed here.\n[0231:015] A fund manager at a life-insurance company said three factors make it difficult to read market direction.\n[0331:032-033] Her recent report classifies the stock as a \u201chold.\u201d But it appears to be the sort of hold one makes while heading for the door.\nLi, Musilek, Reformat, & Wyard-Scott"}, {"heading": "4.1 Syntactic Filtering", "text": "First, the syntactic structures of each sentence are identified and dependencies among the constituents are established, as shown in Figures 3 and 4.\nIdentification of Pleonastic It Using the Web\nIn sample sentence [0231:015], the expletive it appears as the object of the verb makes and is followed by the object complement difficult, therefore a virtual copula (tagged VBX) is created in the dependency tree in order to treat it under the same framework as subject it-extrapositions. For [0331:033], two different readings are produced \u2013 one by assuming appears to be the matrix verb (reading A, c.f. Figure 4), the other by taking be (reading B). This is accomplished by \u2018drilling\u2019 down the chain of verbs beginning with the parent verb of the it node. Once at the top of the chain, the system starts a recursive process to find verbs and infinitives that are directly attached to the current node and moves down to the newly found node. The process is interrupted if the current verb node is furnished with elements other than verbal or adverbial complements/modifiers.\nDuring the filtering process, various components of the sentences are identified, as listed in Table 2."}, {"heading": "4.2 Pattern Instantiation", "text": "Using the components identified in Table 2, five queries are generated for each reading, as listed in Tables 3-5. Patterns II\u2032-it and II\u2032-others refer to the stepped-down versions (c.f. Section 3.4.4, Page 359) of II-it and II-others respectively. The queries shown here are generated specifically for Google and take advantage of features only available in Google. To use an alternative search engine such as Yahoo, the component expansions and determiner lists have to be turned off, and separate queries need to be prepared for individual pronouns. In order to get accurate results, the queries must be enclosed in double quotes before they are sent to search engines.\nLi, Musilek, Reformat, & Wyard-Scott"}, {"heading": "4.3 Query Results and Classification", "text": "For every reading, the number of results for each of the five queries (nw for Pattern I; nit for II-it ; nX for II-others; n\u2032it for II \u2032-it ; and n\u2032X for II \u2032-others) is obtained from the search engine; the first 10 results for the what-cleft query are also validated to obtain the estimated percentage (vw) of valid constructs. W (= nw \u00d7 vw), r(= nX/nit), r\u2032(= n\u2032X/n\u2032it), and R (choosing between either r or r\u2032 depending on whether max(nit, nX) \u2265 10) are then calculated accordingly, as recorded in Table 6.\nWhat appears suspicious is that vw is set to 0 for reading [0331:033].A, which means no valid instances are found. A quick look at the returned snippets reveals that, indeed, none of the 10 snippets has the queried contents at the beginning of sentence. Also note that for reading [0331:033].B, both r and r\u2032, and consequently R have all been set to Rscarce = 1000 since no query produced enough results.\nIt can be decided from Table 2 that readings [0231:015] and [0331:033].B do not bear the it verb infinitive construct, hence S = FALSE; and for [0331:033].A S = TRUE. Applying Equation 10 in Section 3.5, for [0231:015] and [0331:033].B, the final classification\nIdentification of Pleonastic It Using the Web\nE is only based on whether R is sufficiently small (R < 0.15). For [0331:033].A, the system also needs to check whether the what-cleft query returned sufficient valid results (W > 10). The final classifications are listed in Table 7.\nSince neither readings of [0331:033] are classified as such, the sentence is not an it-extraposition construct."}, {"heading": "5. Evaluation", "text": "In order to provide a comprehensive picture of the system\u2019s performance, a twofold assessment is used. In the first evaluation, the system is exposed to the same sentence collection that assisted its development. Accordingly, results obtained from this evaluation reflect, to a certain degree, the system\u2019s optimal performance. The second evaluation aims at revealing the system\u2019s performance on unfamiliar texts by running the developed system on a random dataset drawn from the rest of the corpus. Two additional experiments are also conducted to provide an estimation of the system\u2019s performance over the whole corpus.\nThree performance measures are used throughout the section: precision, recall, and the balanced F-measure (van Rijsbergen, 1979). Precision is defined as the ratio of correctly classified instances in a specific category (or a collection of categories) to the number of instances identified by the system as belonging to the category (categories). In other words, precision is calculated as P = TPTP+FP , where TP and FP are the number of true positives and false positives respectively. Recall is defined as the ratio of correctly classified instances in a specific category (or a collection of categories) to the total number of instances in the category (categories), or R = TPTP+FN , where FN denotes the number of false negatives. Finally, the F-measure is the weighted harmonic mean of precision and recall used to indicate a system\u2019s overall performance. When precision and recall are weighted equally, as used in this study, the balanced F-measure is defined as F = 2PRP+R .\nFollowing Efron and Tibshirani\u2019s (1993) Bootstrap method, 95% confidence intervals are obtained using the 2.5th and 97.5th percentiles of the bootstrap replicates and are provided alongside the system performance figures to indicate their reliability. The number of replicates is arbitrarily set at B = 9999, which is much greater than the commonly suggested value of 1000 (e.g., see Davison & Hinkley, 1997; Efron & Tibshirani, 1993) because pleonastic instances are sparse. In the case that a precision or recall value is 100%, the bootstrap percentile method reports an interval of 100%-100%, which makes little sense. Therefore, in this situation the adjusted Wald interval (Agresti & Coull, 1998) is presented instead. When two systems are compared, an approximate randomization test (Noreen, 1989) similar to that used by Chinchor (1992) is performed to determine if the difference is of statistical significance. The significance level \u03b1 = 0.05 and number of shuffles R = 9999, both chosen arbitrarily, are used where significance tests are performed.\nLi, Musilek, Reformat, & Wyard-Scott"}, {"heading": "5.1 Development Dataset", "text": "For the purpose of this study, the first 1000 occurrences of it from the WSJ corpus have been manually annotated by the authors20. A part of the set has also been inspected in order to determine the values of the constants specified in Section 3.5, and to develop the surface structure processor. The annotation process is facilitated by a custom-designed utility that displays each sentence within its context represented by a nine-sentence window containing the six immediately preceding sentences, the original, and the two sentences that follow. Post-annotation review indicates that this presentation of corpus sentences worked well. Except for a few (less than 0.5%) cases, the authors found no need to resort to broader contexts to understand a sentence; and under no circumstances were valid antecedents located outside the context window while no antecedent was found within it.\nTable 8 summarizes the distribution of instances in the dataset according to the authors\u2019 consensus. The category labeled \u2018Other\u2019 consists mostly of instances that do not fit well into any other categories, e.g. when the identified nominal antecedent is in plural or the antecedent is inferred, as well as certain confusing instances. Out of the twenty-six instances, only two might be remotely recognized as one of the types that interests this study:\n[0101:007] And though the size of the loan guarantees approved yesterday is significant, recent experience with a similar program in Central America indicates that it could take several years before the new Polish government can fully use the aid effectively.\n[0296:048] It \u2019s just comic when they try to pretend they\u2019re still the master race. Neither instance can be identified as anaphoric. However, the first construct has neither a valid non-extraposition version nor a valid what-cleft version, making it difficult to justify as an extraposition, while the it in the second case is considered to refer to the atmosphere aroused by the action detailed in the when-clause.\nIn order to assess whether the pleonastic categories are well-defined and the ability of ordinary language users to identify pleonastic instances, two volunteers, both native English speakers, are invited to classify the it instances in the development dataset. To help them concentrate on the pleonastic categories, the volunteers are only required to assign each instance to one of the following categories: referential, extraposition, cleft, weather/time,\n20Annotations are published as an online appendix at http://www.ece.ualberta.ca/~musilek/pleo. zip.\nIdentification of Pleonastic It Using the Web\nand idiom. The referential category covers instances with both nominal antecedents and clause antecedents, as well as instances with inferrable antecedents. Table 9 outlines both annotators\u2019 performance in reference to the authors\u2019 consensus. The degree of agreement between the annotators, measured by the kappa coefficient (\u03ba; Cohen, 1960), is also given in the same table.\nThere are many factors contributing to the apparently low \u03ba values in Table 9, most notably the skewed distribution of the categories and inappropriate communication of the classification rules. As Di Eugenio and Glass (2004) and others pointed out, skewed distribution of the categories has a negative effect on the \u03ba value. Since the distribution of the it instances in the dataset is fairly unbalanced, the commonly-accepted guideline for interpreting \u03ba values (\u03ba > 0.67 and \u03ba > 0.8 as thresholds for tentative and definite conclusions respectively; Krippendorff, 1980) may not be directly applicable in this case. In addition, the classification rules are communicated to the annotators orally through examples and some of the not-so-common cases, such as the object it-extrapositions, might not have been well understood by both annotators. Another interesting note about the results is that there is a strong tendency for both annotators (albeit on different cases) to classify it-clefts as it-extrapositions. Rather than taking this as a sign that the cleft category is not well-defined, we believe it reflects the inherent difficulties in identifying instances pertaining to the category."}, {"heading": "5.2 Baselines", "text": "Two baselines are available for comparison \u2013 the WSJ annotation, which is done manually and provided with the corpus; and the results from a replication of Paice and Husk\u2019s (1987) algorithm (PHA). It should be cautioned that, given the subjectivity of the issues discussed in this paper and lack of consensus on certain topics in the field of linguistics, recall ratios of the presented baseline results and the forthcoming results of the proposed system should not be compared quantitatively. For example, the original Paice and Husk algorithm does not recognize certain types of object extrapositions and does not always distinguish between\nLi, Musilek, Reformat, & Wyard-Scott\nindividual types of pleonastic it ; and the WSJ corpus has neither special annotation for parenthetical it (c.f. Section 3.2.1, Page 348, [0239:009]) nor an established annotation policy for certain types of object extrapositions (Bies, Ferguson, Katz, & MacIntyre, 1995). No attempts have been made to correct these issues.\nTable 10 summarizes the performance of the baselines on the development dataset. As expected, Paice and Husk\u2019s (1987) algorithm does not perform very well since the WSJ articles are very different from, and tend to be more sophisticated than, the technical essays that the algorithm was designed for. Compared to the originally reported precision of 93% and recall of 96%, the replicated PHA yields only 54% and 75% respectively on the development dataset. The performance of the replica is largely in line with what Boyd et al. (2005) obtained from their implementation of the same algorithm on a different dataset.\nThe 31 (118 \u2212 87) extrapositional cases that are not annotated in WSJ can be broken down into the following categories followed by their respective number of instances:\nIdentification of Pleonastic It Using the Web\nBy stating that the \u2018Characteristic of it extraposition is that the final clause can replace it\u2019, Bies et al. (1995) define the class in the narrowest sense. Since interpretation of the definition is entirely a subjective matter, there is no way of determining the real coverage of the annotations. However, from the portions of the corpus that have been reviewed, the practice of annotation is not entirely consistent.\nTwo sentences are marked as extraposition in the corpus but the annotators\u2019 consensus indicates otherwise. Considering the \u2018golden standard\u2019 status of the WSJ corpus, they are also listed here:\n[0277:040] Moreover, as a member of the Mitsubishi group, which is headed by one of Japan\u2019s largest banks, it is sure to win a favorable loan.\n[0303:006] It is compromises such as this that convince Washington\u2019s liberals that if they simply stay the course, this administration will stray from its own course on this and other issues.\nThe first sentence is considered dubious and most likely referring to the company that is a member of the Mitsubishi group. The second one is considered a cleft and is actually also marked as cleft in the corpus. Since it is the only case in the corpus with both annotations, the extraposition marking was considered a mistake and was manually removed.\nThe Paice and Husk (1987) algorithm suffers from false-positive it . . . that and it . . . to construct detection, which may be fixed by incorporating part-of-speech and phrase structure information together with additional rules. However, such fixes will greatly complicate the original system."}, {"heading": "5.3 Results", "text": "On the development dataset, results produced by the proposed system are as follows:\nLi, Musilek, Reformat, & Wyard-Scott\nFurther statistical significance tests reveal more information regarding the system\u2019s performance in comparison to that of the two volunteers and the baselines:\nUsing the authors\u2019 annotation as reference, the system outperforms both human volunteers. While higher performance is usually desirable, in this particular case, it could indicate possible problems in the design of the experiment. Since the English language is not only used by its speakers but also shaped by the same group of people, it is impractical to have a system that \u2018speaks better English\u2019 than its human counterparts do. One plausible clue to the paradox is that an analytic approach is needed to gain insight into the issue of pronoun classification, but the casual English speakers do not see it from that perspective. As Green and Hecht (1992) and many others indicated, capable users of a language do not necessarily have the ability to formulate linguistic rules. However, these kinds of analytic skills is a prerequisite in order to explicitly classify a pronoun into one of the many categories. Thus, the true performance of casual speakers can only be measured by their ability to comprehend or produce the various pleonastic constructs. In addition, other factors, such as time constraints and imperfections in how the category definitions are conveyed, may also play a role in limiting the volunteers\u2019 performance. The authors\u2019 annotation, on the other hand, is much less influenced by such issues and is therefore considered expert opinion in this experiment. As shown in Section 5.2, the WSJ annotation of extrapositions and clefts, which is also considered expert opinion, is highly compatible with that of the authors. The differences between the two annotations can mostly be attributed to the narrower definition of extraposition adopted by the WSJ annotators. Therefore, the WSJ annotation\u2019s precision of 98.86% for extrapositions (when verified against the authors\u2019\nIdentification of Pleonastic It Using the Web\nannotation) is probably a more appropriate hint of the upper-limit for practically important system performance.\nIn the extraposition category, 279 individual cases passed the syntactic filters and were evaluated by search engine queries. Results of queries are obtained from Google through its web service, the Google SOAP21 Search API. All three (116 \u2212 113) cases of false-positives are caused by missing-object constructions and can be corrected using the patterns detailed in Section 3.4.3.\nThe five (118\u2212 113) false-negative cases are listed below: [0283:013] The newspaper said it is past time for the Soviet Union to create\nunemployment insurance and retraining programs like those of the West.\n[0209:040] \u201cIt \u2019s one thing to say you can sterilize, and another to then successfully pollinate the plant,\u201d he said.\n[0198:011] Sen. Kennedy said . . . but that it would be a \u201creckless course of action\u201d for President Bush to claim the authority without congressional approval.\n[0290:049] Worse, it remained to a well-meaning but naive president of the United States to administer the final infamy upon those who fought and died in Vietnam.\n[0085:047] \u201cIt \u2019s not easy to roll out something that comprehensive, and make it pay,\u201d Mr. Jacob says.\nSentence [0283:013] is misplaced as weather/time. Sentence [0209:040] is not properly handled by the syntactic processing subcomponent. Sentences [0198:011] and [0290:049] involve complex noun phrases (underlined) at the object position of the matrix verbs \u2013 it is very difficult to reduce them to something more generic, such as the head noun only or a pronoun, and still remain confident that the original semantics are maintained. The last case, sentence [0085:047], fails because the full queries (containing part of the subordinate clause) failed to yield enough results and the stepped-down versions are overwhelmed by noise.\nThe last four false-negatives are annotated correctly in the WSJ corpus. The system\u2019s recall ratio on the 87 verified WSJ extraposition annotations is therefore 95.40%, comparable to the overall recall."}, {"heading": "5.4 System Performance on Parser Output", "text": "Thus far, the system has been evaluated based on the assumption that the underlying sentences are tagged and parsed with (almost) perfect accuracy. Much effort has been made to reduce such dependency. For example, tracing information and function tags in the original phrase structures are deliberately discarded; and the system also tries to search for possible extraposed or cleft clauses that are marked as complements to the matrix object. However, deficiencies in tagging and parsing may still impact the system\u2019s performance. Occasionally, even the \u2018golden standard\u2019 manual markups appear problematic and happen to get in the way of the task.\n21The Simple Object Access Protocol is an XML-based message protocol for web services.\nLi, Musilek, Reformat, & Wyard-Scott\nIt is therefore necessary to evaluate the system on sentences that are automatically tagged and parsed in order to answer the question of how well it would perform in the real world. Two state-of-the-art parsers are employed for this study: the reranking parser by Charniak and Johnson (2005), and the Berkeley parser by Petrov, Barrett, Thibaux, and Klein (2006). The system\u2019s performance on their respective interpretations of the development dataset sentences are reported in Tables 14 and 15. Table 16 further compares the system\u2019s real-world performance to the various baselines.\nFurther significance tests reveal that:\n\u2022 using a parser has no statistically significant influence on the system\u2019s performance;\n\u2022 the system outperforms both volunteer annotators in identifying it-extrapositions;\n\u2022 regardless of the parser used, the difference between the system\u2019s performance and that of the WSJ annotation is not statistically significant; and\n\u2022 regardless of the parser used, the system outperforms the Paice and Husk (1987) algorithm."}, {"heading": "5.5 Correlation Analysis for Extrapositions", "text": "Figures 5 through 8 illustrate the correlation between the decision factors and the true expletiveness of the pronoun it in question. All 279 items that passed the initial syntactic filtering process are included in the dataset with the first 116 being extrapositional and the rest separated by a break on the X-axis. This arrangement is made in order to better visualize the contrast between the positive group and the negative group. In Figures 6 through 8, different grey levels are used to indicate the number of results returned by queries \u2013 the darker the shade, the more popular the construct in question is on the web. The constant Rexp = 0.15 is also indicated with a break on the Y-axis.\nLi, Musilek, Reformat, & Wyard-Scott\nAs illustrated, all factors identified in Section 3.5 are good indicators of expletiveness. W (Figure 5) is the weakest of the four factors due to the number of false positives produced by incorrect language usage. This is clear evidence that the web is noisier than ordinary corpora and that the results counts from the web may not be appropriate as the sole decision-making factor. In comparison, r (Figure 6) has almost perfect correlation with the expletiveness of instances. However, full versions of the queries usually return fewer results and in many cases yield too few results for expletive cases (unfilled items plotted on top of the graph indicate cases that do not have enough results, c.f. Section 3.5). The stepped-down versions of the queries (Figure 7), while being less accurate by themselves, serve well when used as \u2018back up\u2019, as illustrated by the R plot (Figure 8). Part of the false-positive outliers on the R plot are produced by full queries for expressions that are habitually associated with it, such as [0135:002] \u2018 . . . said it expects to post sales in the current fiscal year . . . \u2019. When used with a pronoun, these expressions usually describe information quoted from a person or organization already named earlier in the same sentence, making it a more natural choice of subject pronoun. Normally the problematic expressions take the form of verb infinitive-complement, i.e. S=TRUE. According to the decision process described in Section 3.5, W is also considered in this situation, which effectively eliminates such noise.\nIdentification of Pleonastic It Using the Web\nLi, Musilek, Reformat, & Wyard-Scott\nIdentification of Pleonastic It Using the Web\nLi, Musilek, Reformat, & Wyard-Scott"}, {"heading": "5.6 Generalization Study", "text": "In order to evaluate how well the system generalizes, 500 additional sample sentences are randomly selected from the rest of the WSJ corpus as the test dataset. The distribution of instances is comparable to that of the development dataset, as shown in Table 17.\nLi, Musilek, Reformat, & Wyard-Scott\nTable 19 summarizes the performance of the baselines on the test dataset. The two (54 \u2212 52) false-positive extrapositions from the WSJ annotation are listed below together with their respective context:\n[1450:054-055] Another solution cities might consider is giving special priority to police patrols of small-business areas. For cities losing business to suburban shopping centers, it may be a wise business investment to help keep those jobs and sales taxes within city limits.\n[1996:061-062] You think you can go out and turn things around. It \u2019s a tough thing when you can\u2019t.\nThe first case is considered referential, and the it in the second case is believed to refer to a hypothetical situation introduced by the when-clause."}, {"heading": "5.6.1 Performance Analysis", "text": "On the test dataset, the system is able to maintain its precision; it exhibits slight deterioration in recall but the overall performance is still within expectations. The findings are summarized in Table 20.\n149 instances were evaluated for extraposition using queries, covering 62 of the 63 extrapositions. The excluded case is introduced in the form of a direct question, whose particulars the syntactic processing subsystem is not prepared for. Of the other four false negatives, three involve noun phrases at the matrix object position. One of the two clefts that are not recognized arises out of imperfect processing in the corpus. In addition, the false positive in the weather/time category is caused by the verb \u2018hail \u2019, which was treated as a noun by the system.\nAll five (63\u2212 58) false-negative extraposition cases are annotated in the corpus and the WSJ annotation agrees with the six clefts identified by the proposed system. Thus the\nIdentification of Pleonastic It Using the Web\nsystem\u2019s recall ratio on the verified WSJ annotations is 90.38% for extraposition and 100% for cleft.\nLi, Musilek, Reformat, & Wyard-Scott\nResults of the significance tests, summarized in Table 21, reveal the following additional information about the system\u2019s performance on the test dataset:\n\u2022 the system\u2019s higher performance in recognizing it-extrapositions than both volunteers is statistically significant;\n\u2022 in the extraposition category, the difference between WSJ annotation\u2019s (higher) precision and that of the system is not statistically significant; and\n\u2022 the system outperforms the Paice and Husk (1987) algorithm, and the difference is statistically significant.\nTables 22 and 23 outline the system\u2019s performance on the test dataset when parsers are used. Again, both parsers cause slight deteriorations in system performance. However, such changes are not statistically significant. With either parser used, the system is able to perform as well as the WSJ annotations."}, {"heading": "5.6.2 Estimated System Performance on the Whole Corpus", "text": "The relative sparseness of clefts makes it hard to assess the real effectiveness of the proposed approach. To compensate for this, an approximate study is conducted. First, it instances in the whole corpus are processed automatically using the proposed approach. The identified\nIdentification of Pleonastic It Using the Web\ncleft instances are then merged with those that are already annotated in the corpus to form an evaluation dataset of 84 sentences, which is subsequently verified manually. 76 instances out of the 84 are considered to be valid cleft constructs by the authors. Respective performances of the proposed approach and the WSJ annotation are reported in Table 24; the differences are not statistically significant.\nThree of the false positives produced by the proposed approach are actually extrapositions22, which is expected (c.f. Footnote 12, Page 351). Thus, in a binary classification of pleonastic it, items in the cleft category will have higher contributions to the overall precision than they do for their own category. Until the whole corpus is annotated, it is impossible to obtain precise recall figures of either the WSJ annotations or the proposed approach. However, since the rest of the corpus (other than the synthetic dataset) does not contain any true positives for either system and contains the same number of false-negatives for both systems, the proposed system will maintain a higher recall ratio than that of the WSJ annotations on the whole corpus.\nA similar experiment is conducted for extrapositions using sentences that are already annotated in the corpus. All 656 annotated extrapositional it instances are manually verified and 637 (97.10%) of them turn out to be valid cases. The system produced queries for 623 instances and consequently recognized 575 of them, translating into 90.27% (95% C.I. 89.01- 93.56%) recall ratio on the verified annotations. Given the fact that on both the development dataset and the test dataset the proposed system yields slightly higher recall on the whole dataset than it does on the subsets identified by WSJ annotations, its performance for extrapositions on the whole WSJ corpus is likely to remain above 90% in recall.\nSimilar to the situation in the test based on random cases, a large portion of falsepositives are contributed by imperfect handling of both surface structures and noun phrases in the matrix object position, particularly in the form of it takes/took . . . to . . . From additional experiments, it seems that this particular construct can be addressed with a different pattern, what/whatever it takes to verb, which eliminates the noun phrase. Alternatively, the construct could possibly be assumed as extrapositional without issuing queries at all.\n22This kind of cleft can be separated from extrapositions using an additional pattern that attaches the prepositional phrase to the subordinate verb. However, the number of samples are too few to justify its inclusion in the study.\nLi, Musilek, Reformat, & Wyard-Scott"}, {"heading": "6. Discussion", "text": "In this paper a novel pleonastic-it identification system is proposed. Unlike its precursors, the system classifies extrapositions by submitting queries to the web and analyzing returned results. A set of rules are also proposed for classification of clefts, whose particular manner of composition makes it more difficult to apply the web-based approach. Components of the proposed system are simple and their effectiveness should be independent of the type of text being processed. As shown in the generalization tests, the system maintains its precision while recall degrades by only a small margin when confronted with unfamiliar texts. This is an indication that the general principles behind the system are not over-fitted to the text from which they were derived. Overall, when evaluated on WSJ news articles \u2013 which can be considered a \u2018difficult\u2019 type of nonfiction \u2013 the system is capable of producing results that are on par with or only slightly inferior to that of casually trained humans.\nThe system\u2019s success has important implications beyond the particular problem of pleonastic-it identification. First, it shows that the web can be used to answer linguistic questions that are based upon more than just simplistic semantic relationships. Second, the comparative study is an effective means to get highly accurate results from the web despite the fact that it is noisier than the manually compiled corpora. In addition, the success of the simple guidelines used in identifying clefts may serve as evidence that a speaker\u2019s intention can be heavily reflected by the surface structures of her utterance, in a bid to make it distinguishable from similarly constructed sentences.\nSome problems are left unaddressed in the current study, most notably the handling of complex noun phrases and prepositional phrases. Generally speaking, its approach to query instantiation is somewhat crude. To solve the noun-phrase issue, a finer-grained query downgrading is proposed, viz. first to supply the query with the original noun phrase, then the head noun, and finally the adjective that modifies the head noun, if there is one. The effectiveness of this approach is to be determined. As discussed in Section 5.6.2, a special rule can be used for the verb take. This, however, may open the door to exception-based processing, which contradicts the principle of the system to provide a unified approach to pleonastic pronoun identification. Overall, much more data and further experiments are needed before the query instantiation procedures can be finalized.\nAside from the two sets of patterns that are currently in use, other information can be used to assess the validity of a possible extraposition. For example, in extrapositions the matrix verbs are much more likely to remain in present tense than past tense, the noun phrases (if any) at the matrix object position are more likely to be indefinite, and the extraposed clauses are generally longer than the matrix verb phrases. A fuzzy-based decision system with multiple input variables could possibly provide significant performance gains.\nAlthough the system is able to yield reasonable performances on the output of either parser tested, both of them introduce additional errors to the final results. On the combined dataset of development and test items, both parsers cause statistically significant deteriorations in performance at a significance level of 0.1 (Charniak parser: p=0.008 for F-measure on extrapositions; p=0.071 for F-measure on clefts). It is possible that incorporating a pattern-based method will compensate for the problems caused by imperfect parsing and further improve recall ratios; however, more data is needed to confirm this.\nIdentification of Pleonastic It Using the Web\nAnother concern is that the syntactic processing component used in the system is limited. This limitation, caused by the designer\u2019s lack of exposure to a large variety of different constructs, is essentially different from the problem imposed by the limited number of patterns in some previous systems. Eventually, for the proposed system, this limitation can be eliminated. To illustrate, the current design is not able to correctly process sentences like what difference does it make which I buy ; however, it only takes minor effort to correct this by upgrading the subsystem so that it recognizes pre-posed objects. Each such upgrade, which may be performed manually or even automatically through some machine-learning approaches, solves one or more syntactic problems and moves the system closer to being able to recognize all grammatically valid constructs. In contrast, it will take considerably more effort to patch the rigidly defined rules or to upgrade the word lists before the rule-based systems can achieve comparable performances.\nDuring the writing of this article, Google deprecated their SOAP-based search API. This move makes it technically difficult to precisely replicate the results reported in this study since other search engines lack the ability to process alternate expressions (i.e. WordA OR WordB) embedded within a quoted query. To use a different search engine, the matrix verbs should not be expanded but should instead be converted to their respective third-person singular present form only. Stubs should also be in their simplest form only, as described in earlier sections. From preliminary experiments it also seems possible to replace the combination of which/who/this/he with they alone, plus some necessary changes to maintain number agreement among the constituents of the queries. These changes may have some negative effects on the final outcome of the system, but they are unlikely to be severe.\nLike most other NLP tasks, classifying the usage of it is inherently difficult, even for human annotators who already have some knowledge about the problem \u2013 it is one thing to speak the language, and another to then clearly explain the rationale behind a specific construct. Although it is widely accepted that an extrapositional it is expletive, the line between extrapositional cases and referential ones can sometimes be very thin. This is clearly manifested by the existence of truncated extrapositions (Gundel et al., 2005), which obviously have valid referential readings. Similar things can be said about the relationship among all three pleonastic categories as well as idioms. For example, Paice and Husk classify \u2018it remains to . . . \u2019 as an idiom while the same construct is classified as an extraposition in our evaluations. Aside from applying the syntactic guidelines proposed in this study, it is assumed during the annotation process that an extraposition should have either a valid non-extraposed reading or a valid what-cleft reading. It is also assumed that a cleft should generate a valid non-clefted reading by joining the clefted constituent directly to the cleft clause without any leading relative pronoun or adverb. In light of the subjective nature of the problem, our annotations are published on the web as an online appendix to better serve readers."}], "references": [{"title": "Approximate is better than \u201cexact\u201d for interval estimation of binomial proportions", "author": ["A. Agresti", "B.A. Coull"], "venue": "The American Statistician,", "citeRegEx": "Agresti and Coull,? \\Q1998\\E", "shortCiteRegEx": "Agresti and Coull", "year": 1998}, {"title": "Bracketing guidelines for Treebank II style", "author": ["A. Bies", "M. Ferguson", "K. Katz", "R. MacIntyre"], "venue": "Tech. rep. MS-CIS-95-06,", "citeRegEx": "Bies et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Bies et al\\.", "year": 1995}, {"title": "Identifying non-referential it : A machine learning approach incorporating linguistically motivated patterns", "author": ["A. Boyd", "W. Gegg-Harrison", "D. Byron"], "venue": "In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in Natural Language Processing,", "citeRegEx": "Boyd et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2005}, {"title": "Coarse-to-fine n-best parsing and maxent discriminative reranking", "author": ["E. Charniak", "M. Johnson"], "venue": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Charniak and Johnson,? \\Q2005\\E", "shortCiteRegEx": "Charniak and Johnson", "year": 2005}, {"title": "The statistical significance of the MUC-4 results", "author": ["N. Chinchor"], "venue": "Proceedings of the 4th conference on Message understanding (MUC4), pp. 30\u201350, San Mateo, CA. Morgan Kaufmann.", "citeRegEx": "Chinchor,? 1992", "shortCiteRegEx": "Chinchor", "year": 1992}, {"title": "Learning taxonomic relations from heterogeneous evidence", "author": ["P. Cimiano", "L. Schmidt-Thieme", "A. Pivk", "S. Staab"], "venue": "Frontiers in Artificial Intelligence and Applications,", "citeRegEx": "Cimiano et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cimiano et al\\.", "year": 2005}, {"title": "Improving the identification of nonanaphoric it using support vector machines", "author": ["J.C. Clemente", "K. Torisawa", "K. Satou"], "venue": "In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP04)", "citeRegEx": "Clemente et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Clemente et al\\.", "year": 2004}, {"title": "A coefficient of agreement for nominal scales", "author": ["J. Cohen"], "venue": "Educational and Psychological Measurement, 20 (1), 37\u201346.", "citeRegEx": "Cohen,? 1960", "shortCiteRegEx": "Cohen", "year": 1960}, {"title": "Head-Driven Statistical Models for Natural Language Parsing", "author": ["M. Collins"], "venue": "Ph.D. thesis, University of Pennsylvania.", "citeRegEx": "Collins,? 1999", "shortCiteRegEx": "Collins", "year": 1999}, {"title": "A constructional approach to clefts", "author": ["K. Davidse"], "venue": "Linguistics, 38 (6), 1101\u20131131.", "citeRegEx": "Davidse,? 2000", "shortCiteRegEx": "Davidse", "year": 2000}, {"title": "Bootstrap Methods and Their Application. Cambridge series on statistical and probabilistic mathematics", "author": ["A.C. Davison", "D.V. Hinkley"], "venue": null, "citeRegEx": "Davison and Hinkley,? \\Q1997\\E", "shortCiteRegEx": "Davison and Hinkley", "year": 1997}, {"title": "Automatic resolution of anaphora in English", "author": ["M. Denber"], "venue": "Tech. rep., Eastman Kodak Co.", "citeRegEx": "Denber,? 1998", "shortCiteRegEx": "Denber", "year": 1998}, {"title": "The kappa statistic: a second look", "author": ["B. Di Eugenio", "M. Glass"], "venue": "Computational Linguistics,", "citeRegEx": "Eugenio and Glass,? \\Q2004\\E", "shortCiteRegEx": "Eugenio and Glass", "year": 2004}, {"title": "An Introduction to the Bootstrap", "author": ["B. Efron", "R. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani,? \\Q1993\\E", "shortCiteRegEx": "Efron and Tibshirani", "year": 1993}, {"title": "A comparison of rule-based and machine learning methods for identifying non-nominal it", "author": ["R. Evans"], "venue": "Christodoulakis, D. (Ed.), Proceedings of the 2nd International Conference on Natural Language Processing (NLP00), Vol. 1835 of Lecture Notes in Computer Science, pp. 233\u2013241, Berlin. Springer.", "citeRegEx": "Evans,? 2000", "shortCiteRegEx": "Evans", "year": 2000}, {"title": "Applying machine learning toward an automatic classification of it", "author": ["R. Evans"], "venue": "Literary and Linguistic Computing, 16 (1), 45\u201357.", "citeRegEx": "Evans,? 2001", "shortCiteRegEx": "Evans", "year": 2001}, {"title": "WordNet: An Electronic Lexical Database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "Fellbaum,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum", "year": 1998}, {"title": "Implicit and explicit grammar: An empirical study", "author": ["P.S. Green", "K. Hecht"], "venue": "Applied Linguistics,", "citeRegEx": "Green and Hecht,? \\Q1992\\E", "shortCiteRegEx": "Green and Hecht", "year": 1992}, {"title": "Pronouns without NP antecedents: How do we know when a pronoun is referential", "author": ["J. Gundel", "N. Hedberg", "R. Zacharski"], "venue": "Cognitive and Computational Modelling,", "citeRegEx": "Gundel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gundel et al\\.", "year": 2005}, {"title": "Where do cleft sentences come from", "author": ["J.K. Gundel"], "venue": "Language, 53 (3), 543\u2013559.", "citeRegEx": "Gundel,? 1977", "shortCiteRegEx": "Gundel", "year": 1977}, {"title": "For-to complement clauses in English: A cognitive grammar analysis", "author": ["Z. Hamawand"], "venue": "Studia Linguistica, 57 (3), 171\u2013192.", "citeRegEx": "Hamawand,? 2003", "shortCiteRegEx": "Hamawand", "year": 2003}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["M.A. Hearst"], "venue": "Proceedings of the 14th international conference on Computational Linguistics, pp. 539\u2013545.", "citeRegEx": "Hearst,? 1992", "shortCiteRegEx": "Hearst", "year": 1992}, {"title": "The Discourse Function of Cleft Sentences in English", "author": ["N. Hedberg"], "venue": "Ph.D. thesis, University of Minnesota.", "citeRegEx": "Hedberg,? 1990", "shortCiteRegEx": "Hedberg", "year": 1990}, {"title": "The referential status of clefts", "author": ["N. Hedberg"], "venue": "Language, 76 (4), 891\u2013920.", "citeRegEx": "Hedberg,? 2000", "shortCiteRegEx": "Hedberg", "year": 2000}, {"title": "It-extraposition in English: A functional view", "author": ["G. Kaltenb\u00f6ck"], "venue": "International Journal of Corpus Linguistics, 10 (2), 119\u2013159.", "citeRegEx": "Kaltenb\u00f6ck,? 2005", "shortCiteRegEx": "Kaltenb\u00f6ck", "year": 2005}, {"title": "Googleology is bad science", "author": ["A. Kilgarriff"], "venue": "Computational Linguistics, 33 (1), 147\u2013151.", "citeRegEx": "Kilgarriff,? 2007", "shortCiteRegEx": "Kilgarriff", "year": 2007}, {"title": "Introduction to the special issue on the Web as corpus", "author": ["A. Kilgarriff", "G. Grefenstette"], "venue": "Computational Linguistics,", "citeRegEx": "Kilgarriff and Grefenstette,? \\Q2003\\E", "shortCiteRegEx": "Kilgarriff and Grefenstette", "year": 2003}, {"title": "Bare NPs: Kind-referring, indefinites, both, or neither", "author": ["M. Krifka"], "venue": "Proceedings of Semantics and Linguistic Theory (SALT) XIII, New York, USA. CLC Publications.", "citeRegEx": "Krifka,? 2003", "shortCiteRegEx": "Krifka", "year": 2003}, {"title": "Content Analysis: An Introduction to Methodology", "author": ["K. Krippendorff"], "venue": "Sage Publications, Inc., Beverly Hills, USA.", "citeRegEx": "Krippendorff,? 1980", "shortCiteRegEx": "Krippendorff", "year": 1980}, {"title": "A framework for the analysis of cleft constructions", "author": ["K. Lambrecht"], "venue": "Linguistics, 39 (3), 463\u2013516.", "citeRegEx": "Lambrecht,? 2001", "shortCiteRegEx": "Lambrecht", "year": 2001}, {"title": "An algorithm for pronominal anaphora resolution", "author": ["S. Lappin", "H.J. Leass"], "venue": "Computational Linguistics,", "citeRegEx": "Lappin and Leass,? \\Q1994\\E", "shortCiteRegEx": "Lappin and Leass", "year": 1994}, {"title": "Building a large annotated corpus of English: the Penn Treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational Linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Comparing knowledge sources for nominal anaphora resolution", "author": ["K. Markert", "M. Nissim"], "venue": "Computational Linguistics,", "citeRegEx": "Markert and Nissim,? \\Q2005\\E", "shortCiteRegEx": "Markert and Nissim", "year": 2005}, {"title": "Using the web for nominal anaphora resolution", "author": ["K. Markert", "M. Nissim", "N.N. Modjeska"], "venue": "Proceedings of the EACL Workshop on the Computational Treatment of Anaphora,", "citeRegEx": "Markert et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Markert et al\\.", "year": 2003}, {"title": "America in So Many Words: Words That Have Shaped America", "author": ["A. Metcalf", "D.K. Barnhart"], "venue": null, "citeRegEx": "Metcalf and Barnhart,? \\Q1999\\E", "shortCiteRegEx": "Metcalf and Barnhart", "year": 1999}, {"title": "Outstanding issues in anaphora resolution", "author": ["R. Mitkov"], "venue": "Gelbukh, A. (Ed.), Proceedings of the 2nd International Conference on Computational Linguistics and Intelligent Text Processing (CICLing01), Vol. 2004 of Lecture Notes in Computer Science, pp. 110\u2013125, Berlin. Springer.", "citeRegEx": "Mitkov,? 2001", "shortCiteRegEx": "Mitkov", "year": 2001}, {"title": "A new, fully automatic version of Mitkov\u2019s knowledge-poor pronoun resolution method", "author": ["R. Mitkov", "R. Evans", "C. Orasan"], "venue": "Proceedings of the 3rd International Conference on Computational Linguistics and Intelligent Text Processing (CICLing02),", "citeRegEx": "Mitkov et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Mitkov et al\\.", "year": 2002}, {"title": "Automatic detection of nonreferential it in spoken multi-party dialog", "author": ["C. M\u00fcller"], "venue": "Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL06), pp. 49\u201356.", "citeRegEx": "M\u00fcller,? 2006", "shortCiteRegEx": "M\u00fcller", "year": 2006}, {"title": "On the surface syntax of constructions with easy-type adjectives", "author": ["D.L. Nanni"], "venue": "Language, 56 (3), 568\u2013581.", "citeRegEx": "Nanni,? 1980", "shortCiteRegEx": "Nanni", "year": 1980}, {"title": "Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution", "author": ["V. Ng", "C. Cardie"], "venue": "In Proceedings of the 19th international conference on Computational linguistics (COLING02),", "citeRegEx": "Ng and Cardie,? \\Q2002\\E", "shortCiteRegEx": "Ng and Cardie", "year": 2002}, {"title": "Computer-Intensive Methods for Testing Hypotheses : An Introduction", "author": ["E.W. Noreen"], "venue": "Wiley-Interscience, New York, USA.", "citeRegEx": "Noreen,? 1989", "shortCiteRegEx": "Noreen", "year": 1989}, {"title": "Towards the automatic recognition of anaphoric features in english text: the impersonal pronoun \u201cit", "author": ["C.D. Paice", "G.D. Husk"], "venue": "Computer Speech & Language,", "citeRegEx": "Paice and Husk,? \\Q1987\\E", "shortCiteRegEx": "Paice and Husk", "year": 1987}, {"title": "Learning accurate, compact, and interpretable tree annotation", "author": ["S. Petrov", "L. Barrett", "R. Thibaux", "D. Klein"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL (ACL06),", "citeRegEx": "Petrov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2006}, {"title": "Acquiring lexical knowledge for anaphora resolution", "author": ["M. Poesio", "T. Ishikawa", "S.S. im Walde", "R. Vieira"], "venue": "In Proceedings of the Third International Conference on Language Resources and Evaluation,", "citeRegEx": "Poesio et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Poesio et al\\.", "year": 2002}, {"title": "Proper nouns in English", "author": ["C. Sloat"], "venue": "Language, 45 (1), 26\u201330.", "citeRegEx": "Sloat,? 1969", "shortCiteRegEx": "Sloat", "year": 1969}, {"title": "Information Retrieval (2nd edition)", "author": ["C.J. van Rijsbergen"], "venue": null, "citeRegEx": "Rijsbergen,? \\Q1979\\E", "shortCiteRegEx": "Rijsbergen", "year": 1979}, {"title": "Identification of Pleonastic", "author": ["F. Xia", "M. Palmer"], "venue": "In Proceedings of the first international conference on Human language technology research (HLT01),", "citeRegEx": "Xia and Palmer,? \\Q2001\\E", "shortCiteRegEx": "Xia and Palmer", "year": 2001}], "referenceMentions": [{"referenceID": 18, "context": "Applying criteria similar to those established by Gundel et al. (2005), the usage of it can be generally categorized as follows.", "startOffset": 50, "endOffset": 71}, {"referenceID": 19, "context": "Some claim that cleft pronouns should not be classified as expletive (Gundel, 1977; Hedberg, 2000).", "startOffset": 69, "endOffset": 98}, {"referenceID": 23, "context": "Some claim that cleft pronouns should not be classified as expletive (Gundel, 1977; Hedberg, 2000).", "startOffset": 69, "endOffset": 98}, {"referenceID": 14, "context": "As Evans (2001) pointed out, usage of it is covered in most serious surveys of English grammar, some of which (e.", "startOffset": 3, "endOffset": 16}, {"referenceID": 14, "context": "As Evans (2001) pointed out, usage of it is covered in most serious surveys of English grammar, some of which (e.g. Sinclair, 1995) also provide classifications based on semantic categories. In a recent study, Gundel et al. (2005) classify third-person personal pronouns into the following comprehensive hierarchy:", "startOffset": 3, "endOffset": 231}, {"referenceID": 35, "context": "However, despite being identified as one of the open issues of anaphora resolution (Mitkov, 2001), work on automatic identification of pleonastic it is relatively scarce.", "startOffset": 83, "endOffset": 97}, {"referenceID": 30, "context": "Paice and Husk (1987) together with Lappin and Leass (1994) provide examples of rulebased systems that make use of predefined syntactic patterns and word lists.", "startOffset": 36, "endOffset": 60}, {"referenceID": 16, "context": "Denber (1998) suggested using WordNet (Fellbaum, 1998) to extend the word lists, but it is doubtful how helpful this would be considering the enormous number of possible words that are not included in existing lists and the number of inapplicable words that will be identified by such an approach.", "startOffset": 38, "endOffset": 54}, {"referenceID": 39, "context": "For example, Paice and Husk (1987) noticed nearly a 10% decrease in accuracy when rules developed using one subset of the corpus are applied to another subset without modifications.", "startOffset": 13, "endOffset": 35}, {"referenceID": 39, "context": "For example, Paice and Husk (1987) noticed nearly a 10% decrease in accuracy when rules developed using one subset of the corpus are applied to another subset without modifications. Boyd, Gegg-Harrison, and Byron (2005) also observed a significant performance penalty when the approach was applied to a different corpus.", "startOffset": 13, "endOffset": 220}, {"referenceID": 11, "context": "Denber (1998) suggested using WordNet (Fellbaum, 1998) to extend the word lists, but it is doubtful how helpful this would be considering the enormous number of possible words that are not included in existing lists and the number of inapplicable words that will be identified by such an approach.", "startOffset": 0, "endOffset": 14}, {"referenceID": 2, "context": "Studies by Evans (2001, 2000) and Boyd et al. (2005) are examples of this class.", "startOffset": 34, "endOffset": 53}, {"referenceID": 2, "context": "Studies by Evans (2001, 2000) and Boyd et al. (2005) are examples of this class. Both systems employ memory-based learning on grammatical feature vectors; Boyd et al.\u2019s approach also includes a decision tree algorithm that produces less ideal results. In his attempt to place uses of it into seven categories, including pleonastic and nominal anaphoric among others, Evans uses 35 features to encode information such as position/proximity, lemmas, and part-of-speech, related to both the pronoun and other components of interest, such as words and noun phrases, in the sentence. Evans reported 73.38% precision and 69.25% recall for binary classification of pleonastic cases, and an overall binary classification accuracy of 71.48%. In a later study featuring MARS3, a fully automatic pronoun resolution system that employs the same approach, Mitkov, Evans, and Orasan (2002) reported a significantly higher binary classification accuracy of 85.", "startOffset": 34, "endOffset": 876}, {"referenceID": 2, "context": "Studies by Evans (2001, 2000) and Boyd et al. (2005) are examples of this class. Both systems employ memory-based learning on grammatical feature vectors; Boyd et al.\u2019s approach also includes a decision tree algorithm that produces less ideal results. In his attempt to place uses of it into seven categories, including pleonastic and nominal anaphoric among others, Evans uses 35 features to encode information such as position/proximity, lemmas, and part-of-speech, related to both the pronoun and other components of interest, such as words and noun phrases, in the sentence. Evans reported 73.38% precision and 69.25% recall for binary classification of pleonastic cases, and an overall binary classification accuracy of 71.48%. In a later study featuring MARS3, a fully automatic pronoun resolution system that employs the same approach, Mitkov, Evans, and Orasan (2002) reported a significantly higher binary classification accuracy of 85.54% when the approach is applied to technical manuals. Boyd et al.\u2019s (2005) approach targets pleonastic it alone.", "startOffset": 34, "endOffset": 1021}, {"referenceID": 2, "context": "Studies by Evans (2001, 2000) and Boyd et al. (2005) are examples of this class. Both systems employ memory-based learning on grammatical feature vectors; Boyd et al.\u2019s approach also includes a decision tree algorithm that produces less ideal results. In his attempt to place uses of it into seven categories, including pleonastic and nominal anaphoric among others, Evans uses 35 features to encode information such as position/proximity, lemmas, and part-of-speech, related to both the pronoun and other components of interest, such as words and noun phrases, in the sentence. Evans reported 73.38% precision and 69.25% recall for binary classification of pleonastic cases, and an overall binary classification accuracy of 71.48%. In a later study featuring MARS3, a fully automatic pronoun resolution system that employs the same approach, Mitkov, Evans, and Orasan (2002) reported a significantly higher binary classification accuracy of 85.54% when the approach is applied to technical manuals. Boyd et al.\u2019s (2005) approach targets pleonastic it alone. It uses 25 features, most of which concern lengths of specific syntactic structures; also included are part-of-speech information and lemmas of verbs. The study reports an overall precision of 82% and recall of 71%, and, more specifically, recalls on extrapositional and cleft constructs of 81% and 45%, respectively. In addition, Clemente, Torisawa, and Satou (2004) used support vector machines with a feature-set similar to that proposed by Evans (2001) to analyze biological and medical texts, and reported an overall accuracy of 92.", "startOffset": 34, "endOffset": 1427}, {"referenceID": 2, "context": "Studies by Evans (2001, 2000) and Boyd et al. (2005) are examples of this class. Both systems employ memory-based learning on grammatical feature vectors; Boyd et al.\u2019s approach also includes a decision tree algorithm that produces less ideal results. In his attempt to place uses of it into seven categories, including pleonastic and nominal anaphoric among others, Evans uses 35 features to encode information such as position/proximity, lemmas, and part-of-speech, related to both the pronoun and other components of interest, such as words and noun phrases, in the sentence. Evans reported 73.38% precision and 69.25% recall for binary classification of pleonastic cases, and an overall binary classification accuracy of 71.48%. In a later study featuring MARS3, a fully automatic pronoun resolution system that employs the same approach, Mitkov, Evans, and Orasan (2002) reported a significantly higher binary classification accuracy of 85.54% when the approach is applied to technical manuals. Boyd et al.\u2019s (2005) approach targets pleonastic it alone. It uses 25 features, most of which concern lengths of specific syntactic structures; also included are part-of-speech information and lemmas of verbs. The study reports an overall precision of 82% and recall of 71%, and, more specifically, recalls on extrapositional and cleft constructs of 81% and 45%, respectively. In addition, Clemente, Torisawa, and Satou (2004) used support vector machines with a feature-set similar to that proposed by Evans (2001) to analyze biological and medical texts, and reported an overall accuracy of 92.", "startOffset": 34, "endOffset": 1516}, {"referenceID": 2, "context": "Studies by Evans (2001, 2000) and Boyd et al. (2005) are examples of this class. Both systems employ memory-based learning on grammatical feature vectors; Boyd et al.\u2019s approach also includes a decision tree algorithm that produces less ideal results. In his attempt to place uses of it into seven categories, including pleonastic and nominal anaphoric among others, Evans uses 35 features to encode information such as position/proximity, lemmas, and part-of-speech, related to both the pronoun and other components of interest, such as words and noun phrases, in the sentence. Evans reported 73.38% precision and 69.25% recall for binary classification of pleonastic cases, and an overall binary classification accuracy of 71.48%. In a later study featuring MARS3, a fully automatic pronoun resolution system that employs the same approach, Mitkov, Evans, and Orasan (2002) reported a significantly higher binary classification accuracy of 85.54% when the approach is applied to technical manuals. Boyd et al.\u2019s (2005) approach targets pleonastic it alone. It uses 25 features, most of which concern lengths of specific syntactic structures; also included are part-of-speech information and lemmas of verbs. The study reports an overall precision of 82% and recall of 71%, and, more specifically, recalls on extrapositional and cleft constructs of 81% and 45%, respectively. In addition, Clemente, Torisawa, and Satou (2004) used support vector machines with a feature-set similar to that proposed by Evans (2001) to analyze biological and medical texts, and reported an overall accuracy of 92.7% \u2013 higher than that of their own memory-based learning implementation. Ng and Cardie (2002) built a decision tree for binary anaphoricity classification on all types of noun phrases (including pronouns) using the C4.", "startOffset": 34, "endOffset": 1690}, {"referenceID": 2, "context": "Studies by Evans (2001, 2000) and Boyd et al. (2005) are examples of this class. Both systems employ memory-based learning on grammatical feature vectors; Boyd et al.\u2019s approach also includes a decision tree algorithm that produces less ideal results. In his attempt to place uses of it into seven categories, including pleonastic and nominal anaphoric among others, Evans uses 35 features to encode information such as position/proximity, lemmas, and part-of-speech, related to both the pronoun and other components of interest, such as words and noun phrases, in the sentence. Evans reported 73.38% precision and 69.25% recall for binary classification of pleonastic cases, and an overall binary classification accuracy of 71.48%. In a later study featuring MARS3, a fully automatic pronoun resolution system that employs the same approach, Mitkov, Evans, and Orasan (2002) reported a significantly higher binary classification accuracy of 85.54% when the approach is applied to technical manuals. Boyd et al.\u2019s (2005) approach targets pleonastic it alone. It uses 25 features, most of which concern lengths of specific syntactic structures; also included are part-of-speech information and lemmas of verbs. The study reports an overall precision of 82% and recall of 71%, and, more specifically, recalls on extrapositional and cleft constructs of 81% and 45%, respectively. In addition, Clemente, Torisawa, and Satou (2004) used support vector machines with a feature-set similar to that proposed by Evans (2001) to analyze biological and medical texts, and reported an overall accuracy of 92.7% \u2013 higher than that of their own memory-based learning implementation. Ng and Cardie (2002) built a decision tree for binary anaphoricity classification on all types of noun phrases (including pronouns) using the C4.5 induction algorithm. Ng and Cardie reported overall accuracies of 86.1% and 84.0% on the MUC-6 and MUC-7 data sets. Categorical results, however, are not reported and it is not possible to determine the system\u2019s performance on pronouns. Using automatically induced rules, M\u00fcller (2006) reported an overall accuracy of 79.", "startOffset": 34, "endOffset": 2102}, {"referenceID": 21, "context": "The proposed approach is also inspired by Hearst\u2019s (1992) work on mining semantic relationships using text patterns, and many other quests that followed in the same direction (Berland & Charniak, 1999; Poesio, Ishikawa, im Walde, & Vieira, 2002; Markert, Nissim, & Modjeska, 2003; Cimiano, Schmidt-Thieme, Pivk, & Staab, 2005).", "startOffset": 42, "endOffset": 58}, {"referenceID": 8, "context": "A head percolation table similar to that proposed by Collins (1999) is used to obtain the head component of each phrase.", "startOffset": 53, "endOffset": 68}, {"referenceID": 46, "context": "head component to form the dependency tree using a procedure detailed by Xia and Palmer (2001). Figure 2 illustrates the syntactic structure of a sentence in the WSJ corpus.", "startOffset": 73, "endOffset": 95}, {"referenceID": 24, "context": "These constraints are developed by generalizing a small portion of the WSJ corpus and are largely in accordance with the patterns identified by Kaltenb\u00f6ck (2005). Compared to the patterns proposed by Paice and Husk (1987), which also cover cases such as it .", "startOffset": 144, "endOffset": 162}, {"referenceID": 24, "context": "These constraints are developed by generalizing a small portion of the WSJ corpus and are largely in accordance with the patterns identified by Kaltenb\u00f6ck (2005). Compared to the patterns proposed by Paice and Husk (1987), which also cover cases such as it .", "startOffset": 144, "endOffset": 222}, {"referenceID": 41, "context": "Neither insulting nor demeaning is in Paice and Husk\u2019s (1987) list of \u2018task status words\u2019 and therefore cannot activate the it .", "startOffset": 38, "endOffset": 62}, {"referenceID": 24, "context": "According to Kaltenb\u00f6ck\u2019s (2005) analysis there are special cases in which noun phrases appear as an extraposed component, such as \u2018It\u2019s amazing the number of theologians that sided with Hitler.", "startOffset": 13, "endOffset": 33}, {"referenceID": 22, "context": "Following Hedberg (1990), it-clefts can be expressed as follows:", "startOffset": 10, "endOffset": 25}, {"referenceID": 9, "context": "In an it-cleft construct, the cleft clause does not constitute a head-modifier relationship with the clefted constituent, but instead forms an existential and exhaustive presupposition8 (Davidse, 2000; Hedberg, 2000; Lambrecht, 2001).", "startOffset": 186, "endOffset": 233}, {"referenceID": 23, "context": "In an it-cleft construct, the cleft clause does not constitute a head-modifier relationship with the clefted constituent, but instead forms an existential and exhaustive presupposition8 (Davidse, 2000; Hedberg, 2000; Lambrecht, 2001).", "startOffset": 186, "endOffset": 233}, {"referenceID": 29, "context": "In an it-cleft construct, the cleft clause does not constitute a head-modifier relationship with the clefted constituent, but instead forms an existential and exhaustive presupposition8 (Davidse, 2000; Hedberg, 2000; Lambrecht, 2001).", "startOffset": 186, "endOffset": 233}, {"referenceID": 27, "context": "The validity of this assertion is under debate (Krifka, 2003).", "startOffset": 47, "endOffset": 61}, {"referenceID": 43, "context": "Sloat\u2019s (1969) account.", "startOffset": 0, "endOffset": 15}, {"referenceID": 41, "context": "\u2022 For a clause to be identified as a subordinate clause and subsequently processed for extraposition or cleft, the number of commas, dashes and colons between the clause and it should be either zero or more than one, a rule adopted from Paice and Husk\u2019s (1987) proposal.", "startOffset": 237, "endOffset": 261}, {"referenceID": 25, "context": "As Kilgarriff and Grefenstette (2003) pointed out, following the definition of", "startOffset": 3, "endOffset": 38}, {"referenceID": 31, "context": "In Markert and Nissim\u2019s (2005) recent study evaluating different knowledge sources for anaphora resolution, the web-based method achieves far higher recall ratio than those that are BNC- and WordNet-based, while at the same time yielding slightly lower precision.", "startOffset": 3, "endOffset": 31}, {"referenceID": 25, "context": "As also suggested by Kilgarriff (2007) and many others, it is technically more difficult to exploit the web than to use a local corpus and it can often be dangerous to rely solely on statistics provided by commercial search engines.", "startOffset": 21, "endOffset": 39}, {"referenceID": 20, "context": "According to Hamawand (2003), the for .", "startOffset": 13, "endOffset": 29}, {"referenceID": 23, "context": "Kaltenb\u00f6ck (2005) noted that the percentage of extrapositional it constructs carrying new information varies greatly depending on the category of the text.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "What-clefts, according to Gundel (1977), from which the it-clefts are derived, have the same existential and exhaustive presuppositions carried by their it-cleft counterparts.", "startOffset": 26, "endOffset": 40}, {"referenceID": 38, "context": "The first pattern, the compound adjective test, is inspired by Nanni\u2019s (1980) study considering the easy-type adjective followed by an infinitive (also commonly termed tough construction) as a single complex adjective.", "startOffset": 63, "endOffset": 78}, {"referenceID": 40, "context": "When two systems are compared, an approximate randomization test (Noreen, 1989) similar to that used by Chinchor (1992) is performed to determine if the difference is of statistical significance.", "startOffset": 65, "endOffset": 79}, {"referenceID": 12, "context": "Following Efron and Tibshirani\u2019s (1993) Bootstrap method, 95% confidence intervals are obtained using the 2.", "startOffset": 10, "endOffset": 40}, {"referenceID": 4, "context": "When two systems are compared, an approximate randomization test (Noreen, 1989) similar to that used by Chinchor (1992) is performed to determine if the difference is of statistical significance.", "startOffset": 104, "endOffset": 120}, {"referenceID": 7, "context": "The degree of agreement between the annotators, measured by the kappa coefficient (\u03ba; Cohen, 1960), is also given in the same table.", "startOffset": 82, "endOffset": 98}, {"referenceID": 28, "context": "Since the distribution of the it instances in the dataset is fairly unbalanced, the commonly-accepted guideline for interpreting \u03ba values (\u03ba > 0.67 and \u03ba > 0.8 as thresholds for tentative and definite conclusions respectively; Krippendorff, 1980) may not be directly applicable in this case.", "startOffset": 138, "endOffset": 246}, {"referenceID": 12, "context": "As Di Eugenio and Glass (2004) and others pointed out, skewed distribution of the categories has a negative effect on the \u03ba value.", "startOffset": 6, "endOffset": 31}, {"referenceID": 41, "context": "Two baselines are available for comparison \u2013 the WSJ annotation, which is done manually and provided with the corpus; and the results from a replication of Paice and Husk\u2019s (1987) algorithm (PHA).", "startOffset": 156, "endOffset": 180}, {"referenceID": 40, "context": "As expected, Paice and Husk\u2019s (1987) algorithm does not perform very well since the WSJ articles are very different from, and tend to be more sophisticated than, the technical essays that the algorithm was designed for.", "startOffset": 13, "endOffset": 37}, {"referenceID": 2, "context": "The performance of the replica is largely in line with what Boyd et al. (2005) obtained from their implementation of the same algorithm on a different dataset.", "startOffset": 60, "endOffset": 79}, {"referenceID": 1, "context": "By stating that the \u2018Characteristic of it extraposition is that the final clause can replace it\u2019, Bies et al. (1995) define the class in the narrowest sense.", "startOffset": 98, "endOffset": 117}, {"referenceID": 41, "context": "The Paice and Husk (1987) algorithm suffers from false-positive it .", "startOffset": 4, "endOffset": 26}, {"referenceID": 41, "context": "\u2022 Compared to Paice and Husk\u2019s (1987) algorithm, the system\u2019s higher precision is statistically significant.", "startOffset": 14, "endOffset": 38}, {"referenceID": 17, "context": "As Green and Hecht (1992) and many others indicated, capable users of a language do not necessarily have the ability to formulate linguistic rules.", "startOffset": 3, "endOffset": 26}, {"referenceID": 3, "context": "Two state-of-the-art parsers are employed for this study: the reranking parser by Charniak and Johnson (2005), and the Berkeley parser by Petrov, Barrett, Thibaux, and Klein (2006).", "startOffset": 82, "endOffset": 110}, {"referenceID": 3, "context": "Two state-of-the-art parsers are employed for this study: the reranking parser by Charniak and Johnson (2005), and the Berkeley parser by Petrov, Barrett, Thibaux, and Klein (2006). The system\u2019s performance on their respective interpretations of the development dataset sentences are reported in Tables 14 and 15.", "startOffset": 82, "endOffset": 181}, {"referenceID": 41, "context": "\u2022 regardless of the parser used, the system outperforms the Paice and Husk (1987) algorithm.", "startOffset": 60, "endOffset": 82}, {"referenceID": 41, "context": "\u2022 the system outperforms the Paice and Husk (1987) algorithm, and the difference is statistically significant.", "startOffset": 29, "endOffset": 51}, {"referenceID": 18, "context": "This is clearly manifested by the existence of truncated extrapositions (Gundel et al., 2005), which obviously have valid referential readings.", "startOffset": 72, "endOffset": 93}], "year": 2009, "abstractText": "In a significant minority of cases, certain pronouns, especially the pronoun it, can be used without referring to any specific entity. This phenomenon of pleonastic pronoun usage poses serious problems for systems aiming at even a shallow understanding of natural language texts. In this paper, a novel approach is proposed to identify such uses of it : the extrapositional cases are identified using a series of queries against the web, and the cleft cases are identified using a simple set of syntactic rules. The system is evaluated with four sets of news articles containing 679 extrapositional cases as well as 78 cleft constructs. The identification results are comparable to those obtained by human efforts.", "creator": "LaTeX with hyperref package"}}}