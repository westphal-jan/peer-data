{"id": "1401.6424", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2014", "title": "Toward Supervised Anomaly Detection", "abstract": "anomaly detection is being regarded as an unsupervised learning task as anomalies stem from adversarial or unlikely events with unknown distributions. however, the predictive performance of purely unsupervised anomaly detection often fails to vastly match the required detection rates in many tasks and there exists a need for labeled data to guide the model generation. our first contribution shows plainly that classical semi - supervised approaches, originating principally from a supervised classifier, are inappropriate and hardly detect new and unknown anomalies. we argue that semi - supervised anomaly detection needs to ground on the unsupervised regression learning paradigm and would devise just a novel algorithm that meets this operational requirement. although being intrinsically non - convex, we further show that the optimization problem has a convex equivalent under relatively mild assumptions. \u2026 additionally, we propose an active learning strategy to automatically filter all candidates for labeling. in an empirical study on network intrusion detection data, we observe that the proposed learning methodology requires recruiting much less labeled data than the state - of - the - art, while achieving higher detection accuracies.", "histories": [["v1", "Thu, 23 Jan 2014 02:46:53 GMT  (1578kb)", "http://arxiv.org/abs/1401.6424v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nico goernitz", "marius micha kloft", "konrad rieck", "ulf brefeld"], "accepted": false, "id": "1401.6424"}, "pdf": {"name": "1401.6424.pdf", "metadata": {"source": "CRF", "title": "Toward Supervised Anomaly Detection", "authors": ["Nico G\u00f6rnitz", "Marius Kloft", "Konrad Rieck", "Ulf Brefeld"], "emails": ["NICO.GOERNITZ@TU-BERLIN.DE", "KLOFT@TU-BERLIN.DE", "KONRAD.RIECK@UNI-GOETTINGEN.DE", "BREFELD@KMA.INFORMATIK.TU-DARMSTADT.DE"], "sections": [{"heading": "1. Introduction", "text": "Anomaly detection deals with identifying unlikely and rare events. The classical approach to anomaly detection is to compute a precise description of normal data. Every newly arriving instance is contrasted with the model of normality and an anomaly score is computed. The score describes the deviations of the new instance compared to the average data instance and, if the deviation exceeds a predefined threshold, the instance is considered an anomaly or an outlier and processed adequately (Markou & Singh, 2003a; Chandola, Banerjee, & Kumar, 2009; Markou & Singh, 2003b).\nIdentifying data that exhibits irregular and suspicious traits is crucial in many applications such as medical imaging and network security. In particular, the latter has become a vivid research area as computer systems are increasingly exposed to security threats, such as computer worms, network\nc\u00a92013 AI Access Foundation. All rights reserved.\nattacks, and malicious code (Andrews & Pregibon, 1978). Network intrusion detection deals with detecting previously unknown threats and attacks in network traffic. Conventional security techniques for intrusion detection are based on identifying known patterns of misuse, so called signatures (Roesch, 1999; Paxson, 1999) and thus\u2014although being effective against known attacks\u2014fail to protect from novel threats. This brings anomaly detection into the focus of security research (e.g., Eskin, Arnold, Prerau, Portnoy, & Stolfo, 2002; Kruegel, Vigna, & Robertson, 2005; Stolfo, Apap, Eskin, Heller, Hershkop, Honig, & Svore, 2005; Perdisci, Ariu, Fogla, Giacinto, & Lee, 2009). Thus, anomaly detection is the most beneficial in learning scenarios where many regular data instances are given, which allows the machine to approximate the underlying distribution well and leads to a concise model of normality. By contrast, outliers and anomalies are rare and can even originate from changing distributions (e.g., novel classes of network attacks). Especially in adversarial settings, such as network intrusion detection, differences in training and test distributions are eminent as novel threats and tactics are being continuously developed. As a consequence, anomaly detection is generally considered an unsupervised task and prominent learning methods, including one-class support vector machines (Sch\u00f6lkopf, Platt, Shawe-Taylor, Smola, & Williamson, 2001) and support vector data descriptions (SVDD, Tax & Duin, 2004), implement this spirit. However, the underlying assumptions on unsupervised methods are also their major drawback and in many application areas, unsupervised methods fail to achieve the required detection rates. Especially in adversarial application areas such as network intrusion detection, even a single undetected outlier may already suffice to capture the system. Therefore, the goal of this article is to incorporate a feedback-loop in terms of labeled data to make anomaly detection practical. By doing so, knowledge about historic threats and anomalies can be included in terms of labels and thus guide the model generation toward better generalizations.\nIn this article, we cast anomaly detection into the paradigm of semi-supervised learning (Chapelle, Sch\u00f6lkopf, & Zien, 2006). Usually, semi-supervised methods are deduced from existing supervised techniques, augmented by an appropriate bias to take the unlabeled data into account. For instance, a prominent bias assumes that the unlabeled data is structured in clusters so that closeness (with respect to some measure) is proportional to the probability of having the same class label (Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005; Chapelle, Chi, & Zien, 2006; Sindhwani, Niyogi, & Belkin, 2005). As a consequence, anomaly detection is often rephrased as a (multi-class) classification problem (Almgren & Jonsson, 2004; Stokes & Platt, 2008; Pelleg & Moore, 2004; Mao, Lee, Parikh, Chen, & Huang, 2009). Although assuming a cluster-structure of the data is\noften well justified in anomaly detection, recall that supervised learning techniques focus on discriminating concept classes while unsupervised techniques rather focus on data characterization. In this article, we show that differences in training and test distributions as well as the occurrence of previously unseen outlier classes render anomaly detection methods, derived from a supervised technique, inappropriate as they are likely to miss out novel and previously unseen classes of anomalies as depicted. By contrast, we argue that successful anomaly detection methods inherently need to ground on the unsupervised learning paradigm, see Figure 1. In sum, making anomaly detection practical requires the following key characteristics: (i) intrinsically following the unsupervised learning paradigm to cope with unknown events and (ii) additionally exploiting label information to obtain state-of-the-art results.\nIn Figure 2, we show the results of a controlled experiment that visualizes the different nature of the semi-supervised methods derived from supervised and unsupervised paradigms, respectively. On the left hand side, the achieved accuracies in the standard supervised classification scenario, where training and test distributions are identical, is shown. The performance of the unsupervised anomaly detection method is clearly outperformed by supervised and semi-supervised approaches. However, we observe from the right hand side of the figure how fragile the latter methods can be in an anomaly detection scenario. The experimental setup is identical to the former except that we discard two anomaly clusters in the training set (see Figure 3). Note that this change is not an arbitrary modification but an inherent characteristic of anomaly detection scenarios, where anomalies stem from novel and previously unseen distributions. Unsurprisingly, the (partially) supervised methods fail to detect the novel outliers and are clearly outperformed by the unsupervised approach, which robustly performs around unimpressive detection rates of about 40%. Finally, all methods are clearly outperformed by our novel semi-supervised anomaly detection (SSAD) which is devised from the unsupervised learning paradigm and allows for incorporating labeled data.\nThe main contribution of this article is to provide a mathematical sound methodology for semisupervised anomaly detection. Carefully conducted experiments and their discussions show the importance of distinguishing between the two semi-supervised settings as depicted in Figure 1. To meet this requirement, we propose a novel semi-supervised anomaly detection technique that is derived from the unsupervised learning paradigm, but allows for incorporating labeled data in the training process. Our approach is based on the support vector data description (SVDD) and contains the original formulation as a special case. Although the final optimization problem is not convex, we show that an equivalent convex formulation can be obtained under relatively mild assumptions. To guide the user in the labeling process, we additionally propose an active learning strategy to improve the actual model and to quickly detect novel anomaly clusters. We empirically evaluate our method on network intrusion detection tasks. Our contribution proves robust in scenarios where the performance of baseline approaches deteriorate due to obfuscation techniques. In addition, the active learning strategy is shown to be useful as a standalone method for threshold adaptation.\nThe remainder of this article is organized as follows. Section 2 reviews related work. The novel semi-supervised anomaly detection methods are presented in Section 3 and Section 4 introduces active learning strategies. Section 5 gives insights into the proposed learning paradigm and we report on results for real-world network intrusion scenarios in Section 6. Section 7 concludes."}, {"heading": "2. Related Work", "text": "Semi-supervised learning (Chapelle, Sch\u00f6lkopf et al., 2006) offers a mathematical sound framework for learning with partially labeled data. For instance, transductive approaches to semi-supervised learning assume a cluster structure in the data so that close points are likely to share the same label while points that are far away are likely to be labeled differently. The transductive support vector machine (TSVM, Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005) optimizes a max-margin hyperplane in feature space that implements the cluster assumption. In its most basic formulation, the TSVM is a non-convex integer programming problem on top of an SVM. This is computationally very expensive so that Chapelle, Chi et al. (2006) propose a more efficient smooth relaxation of the TSVM. Another related approach is low density separation (LDS) by Chapelle and Zien (2005), where the cluster structure is modeled by a graph of distances.\nA broad overview of anomaly detection can be found in the work of Chandola et al. (2009). Anomaly detection is being regarded as unsupervised learning task and therefore it is not surprising that there exist a large number of applications employing unsupervised anomaly detection methods. For instance, finding anomalies in network traffic (Eskin et al., 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006)\nFully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take a multi-view and co-training approach based on Blum and Mitchell (1998) to learn from labeled and unlabeled data.\nSupport vector learning has also been extended to many non-standard settings such as one-class learning (Sch\u00f6lkopf et al., 2001) and support vector data description (Tax & Duin, 2004). The idea of the latter is to learn a hypersphere that encloses the bulk of the provided data so that all instances that lie outside of the hypersphere are considered anomalous. By contrast, the one-class SVM learns a hyperplane in some feature space that divides the data points from the origin with maximum-margin. For translation-invariant kernel matrices, both approaches are equivalent.\nThere exist only a few semi-supervised methods that are based on unsupervised techniques. Blanchard, Lee, and Scott (2010) propose a method which has the appealing option of specifying an upper threshold on the false-positives rate. However, this method needs to include test instances at training time and is not applicable in online and streaming scenarios such as anomaly detection. The same holds true for an extension of the one-class SVM by Mu\u0303noz Mar\u00ed, Bovolo, G\u00f3mez-Chova, Bruzzone, and Camp-Valls (2010) that incorporates labeled examples in a graphLaplacian regularization term. Tax (2001) proposes a straight-forward extension of the SVDD to semi-supervised anomaly detection, where negatively labeled points are required to lie outside of the hypersphere\u2014otherwise a penalty is incurred. An advantage of this so-called SVDDneg approach is that no further assumptions on the underlying data-generating probability distribution such as manifold assumptions are imposed. Unfortunately, the primal SVDDneg problem is not a convex optimization problem, which makes it very difficult to accurately optimize. Moreover, dual optimization as proposed in the work of Tax (2001) cannot be considered an sound alternative due to possible duality gaps. However, in Appendix A we show that there is a convex reformulation of the SVDDneg for translation-invariant kernels, such as RBF-kernels. The new formulation does not suffer from duality gaps and can be easily solved by primal or dual descent methods. The same problem occurs in related semi-supervised one-class methods as proposed by Liu and Zheng (2006) and Wang, Neskovic, and Cooper (2005).\nAnother broad class of methods deals with learning from positive and unlabeled examples (LPUE). Intrinsically, one aims at solving a two-class problem but only data from one class (the positive class) is given together with unlabeled data points. LPUE can thus be applied to the problem setting at hand by identifying the outlier class with positively labeled data. Zhang and Lee (2005) show that this class of methods can be viewed as a special case of semi-supervised learning and emphasize that the SVDDneg (Tax, 2001) can be considered an instance of LPUE. Algorithmically, LPUE is often solved in an iterative manner by (i) identifying a reliable set of labeled examples using a classifier and (ii) re-training the classifier given the new training set (Liu, Dai,\nLi, Lee, & Yu, 2003; Zhang & Lee, 2005; Blum & Mitchell, 1998). Though some work addresses learning from non-i.i.d. data (e.g., Li & Liu, 2005), the underlying assumption usually implies that training and test sets are drawn from the same distribution.\nThe present article builds upon a previous paper of the same authors (G\u00f6rnitz, Kloft, & Brefeld, 2009). It extends the latter by a mathematical sound framework and intuitive philosophical insights. In addition, we present a more general problem formulation employing arbitrary convex loss functions and the computation of the dual representation thereof, and a new empirical analysis with comparisons to a larger variety of baseline approaches."}, {"heading": "3. Semi-supervised Anomaly Detection", "text": "In anomaly detection tasks, we are given n observations x1, . . . ,xn \u2208 X . The underlying assumption is that the bulk of the data stems from the same (unknown) distribution and we call this part of the data normal. Some few observations, however, originate from different distributions and are considered anomalies. These anomalies could for instance be caused by broken sensors or network attacks and cannot be sampled by definition. The goal in anomaly detection is to detect these anomalies by finding a concise description of the normal data, so that deviating observations become outliers. We thus aim at finding a scoring function f : X \u2192 R which defines the model of normality. Following the principle of empirical risk minimization, the optimization problem takes the following form,\nf\u2217 = argmin f\n\u2126(f) + \u03b7\nn n\u2211 i=1 l(f(xi)),\nwhere l : R \u2192 R is an appropriate loss function, \u2126 : Rd \u2192 R+ a regularizer on f , and \u03b7 is a trade-off parameter.\nOur approach is based on the SVDD, which computes a hypersphere with radius R and center c that encompasses the data. The hypersphere is our model of normality and the anomaly score for an instance x is computed by its distance to the center c,\nf(x) = ||\u03c6(x)\u2212 c||2 \u2212R2. (1)\nPoints lying outside of the ball (i.e., f(x) > 0) are considered anomalous, while points within (f(x) < 0) are treated as normal data. The corresponding optimization problem is known as support vector data description (SVDD, Tax, 2001) and has the following form\nmin R,c,\u03be\nR2 + \u03b7u n\u2211 i=1 \u03bei\ns.t. \u2200ni=1 : \u2016\u03c6(xi)\u2212 c\u20162 \u2264 R2 + \u03bei \u2200ni=1 : \u03bei \u2265 0 ,\nwhere the trade-off \u03b7u balances the minimization of the radius and the sum of erroneously placed points (that are, points lying outside of the normality radius). The parameter \u03b7u also serves as an estimate of the ratio between outliers and normal data in the n training examples. The resulting problem is convex and can be solved equivalently in dual space using the representation c = \u2211n i=1 \u03b1i\u03c6(xi). As a consequence, the input data can be expressed equivalently by a kernel function k(xi,xj) = \u03c6(xi)T\u03c6(xj) on X that corresponds to a feature map \u03c6 : X \u2192 F into a reproducing kernel Hilbert space F (see, e.g., M\u00fcller, Mika, R\u00e4tsch, Tsuda, & Sch\u00f6lkopf, 2001)."}, {"heading": "3.1 Semi-supervised Anomaly Detection", "text": "We now propose a novel approach to semi-supervised anomaly detection. The proposed method generalizes the vanilla SVDD and processes unlabeled and labeled examples. While existing extensions of the SVDD employ dual optimization techniques and inherently suffer from duality gaps due to their non-convexity, we propose a primal approach to semi-supervised anomaly detection. As discussed earlier, for translation-invariant kernels, the one-class SVM is contained in our framework as a special case. In addition to the n unlabeled examples x1, . . . ,xn \u2208 X , we are now given m labeled observations (x\u22171, y \u2217 1), . . . , (x \u2217 m, y \u2217 m) \u2208 X \u00d7 Y where Y denotes the set of class labels. For simplicity, we will focus on Y = {+1,\u22121} where y\u2217 = +1 encodes nominal data and y\u2217 = \u22121 anomalies.\nAs argued in the introduction, the goal is to derive a method that grounds on the unsupervised learning paradigm. We therefore stick to the hypersphere model of the SVDD and use the latter as blueprint for dealing with unlabeled data. The inclusion of labeled examples follows a simple pattern: If an example x\u2217 is labeled as nomial (y\u2217 = +1), we require that it lies within the hypersphere. By contrast, if an example is an anomaly or member of an outlier class (y\u2217 = \u22121), we want to have it placed outside of the ball. A straight-forward extension of the SVDD using both labeled and unlabeled examples is thus given by\nmin R,\u03b3,c,\u03be R2 \u2212 \u03ba\u03b3 + \u03b7u n\u2211 i=1 \u03bei + \u03b7l n+m\u2211 j=n+1 \u03be\u2217j\ns.t. \u2200ni=1 : \u2016\u03c6(xi)\u2212 c\u20162 \u2264 R2 + \u03bei \u2200n+mj=n+1 : y \u2217 j ( \u2016\u03c6(x\u2217j )\u2212 c\u20162 \u2212R2 ) \u2264 \u2212\u03b3 + \u03be\u2217j (2)\n\u2200ni=1 : \u03bei \u2265 0, \u2200n+mj=n+1 : \u03be \u2217 j \u2265 0 ,\nwhere \u03b3 is the margin of the labeled examples and \u03ba, \u03b7u, and \u03b7l are trade-off parameters. Unfortunately, the inclusion of negatively labeled data renders the above optimization problem non-convex and optimization in dual space is prohibitive. As a remedy, following the approach of Chapelle and Zien (2005), we translate Equation (2) into an unconstrained problem. We thereby resolve the slack terms from the above OP as follows\n\u03bei = ` ( R2 \u2212 ||\u03c6(xi)\u2212 c||2 ) (3)\n\u03be\u2217j = ` ( y\u2217j ( R2 \u2212 ||\u03c6(x\u2217j )\u2212 c||2 ) \u2212 \u03b3 ) .\nFor example, if we put `(t) = max{\u2212t, 0} (i.e., the common hinge loss), we recover (2). Furthermore, by an application of the representer theorem, we obtain the support-vector expansion\nc = n\u2211 i=1 \u03b1i\u03c6(xi) + n+m\u2211 j=n+1 \u03b1jy \u2217 j\u03c6(x \u2217 j ) (4)\n(see Appendix B for a detailed derivation). Combining (3) and (4), we can re-formulate optimization problem (2) solely in terms of kernels and without any constraints as follows:\nmin R,\u03b3,\u03b1\nR2 \u2212 \u03ba\u03b3 + \u03b7u n\u2211 i=1 ` ( R2 \u2212 k(xi,xi) + (2ei \u2212\u03b1)\u2032K\u03b1 ) + \u03b7l\nn+m\u2211 j=n+1 ` ( y\u2217j ( R2 \u2212 k(x\u2217j ,x\u2217j ) + (2e\u2217j \u2212\u03b1)\u2032K\u03b1 ) \u2212 \u03b3 ) . (5)\nHereby K = (kij)1\u2264i,j\u2264n denotes the kernel matrix given by kij = k(xi,xj) = \u3008\u03c6(xi), \u03c6(xj)\u3009 and e1, . . . , en+m is the standard base of Rn+m. By rephrasing the problem as an unconstrained optimization problem, its intrinsic complexity has not changed. Often, unconstrained optimization is easier to implement than constrained optimization. While non-smooth optimization is possible via e.g. non-convex bundle methods as described in the work of Do (2010), smooth optimization methods such as conjugate gradient or Newton\u2019s method are easier to apply. To obtain a smooth optimization technique, we choose Huber\u2019s robust loss (Huber, 1972). The Huber loss has two parameters controlling its quadratic approximation in terms of the center \u2206 and its witdh , see Figure 4. The optimization function becomes differentiable and off-the-shelf gradient-based optimization tools can be applied. The complete derivation of the gradients of optimization problem (5) using Huber\u2019s robust loss is shown in Appendix C."}, {"heading": "3.2 Convex Semi-supervised Anomaly Detection", "text": "The optimization problem of the previous section is easy to implement but, unfortunately, nonconvex. Therefore, optimizers may find a good local optimum, but several restarts are necessary to verify the quality of the solutions and in cases optimization might fail completely. We now show that, under rather mild assumptions, namely that the data is processed to have unit norm in feature space (as fulfilled by, e.g., RBF kernels), the above optimization problem can be converted into an equivalent convex one. Our derivation is very general as it postulates nothing but the convexity of the loss function. Our approach is based on a combination of Lagrangian duality and the notion of the Fenchel-Legendre conjugate function. Fenchel duality for machine learning has been pioneered by\nRifkin and Lippert (2007) under the assumption of full-rank kernels. Our approach is more general and allows us to use any kernel K. As a byproduct of our derivation, we show that the classical one-class SVM is a special case of a general class of density level set estimators that minimize a convex risk functional and give a general dual criterion for this class. To this aim, we introduce the Legendre-Fenchel conjugate for a given loss l(t) as\nlc(z) = sup t\n(zt\u2212 l(t))\nand use a slightly different formulation of the SSAD problem, that is, we eliminate the hinge loss slack variables \u03be\u2217,\u03be and reformulate the problem with explicit loss functions:\nmin \u03c1,\u03b3,w,t\n1 2 ||w||2 \u2212 \u03c1\u2212 \u03ba\u03b3 + \u03b7u n\u2211 i=1 l(ti) + \u03b7l n+m\u2211 j=n+1 l(tj)\ns.t. \u2200ni=1 : ti = (wT\u03c6(xi))\u2212 \u03c1 (P) \u2200n+mj=n+1 : tj = (y \u2217 jw\nT\u03c6(x\u2217j ))\u2212 y\u2217j \u03c1\u2212 \u03b3 and \u03b3 \u2265 0.\nNote that, in the above problem, auxiliary variables ti are introduced to deal with non-differentiable loss functions. Again, because of the convex nature of the stated optimization problem, we can solve it in the dual space. To this aim, we use the Lagrange Theorem to incorporate the constraints into the objective:\nL = 1\n2 ||w||2 \u2212 \u03c1\u2212 \u03ba\u03b3 + \u03b7u n\u2211 i=1 l(ti) + \u03b7l n+m\u2211 j=n+1 l(tj)\n\u2212 n\u2211 i=1 \u03b1i((w T\u03c6(xi))\u2212 \u03c1\u2212 ti) (6)\n\u2212 n+m\u2211 j=n+1 \u03b1j((y \u2217 jw T\u03c6(x\u2217j ))\u2212 y\u2217j \u03c1\u2212 \u03b3 \u2212 tj)\u2212 \u03b4\u03b3\nAn optimal solution can be found by solving the Lagrangian saddle point problem\nmax \u03b1,\u03b4 min \u03c1,\u03b3,w,t EQ6.\nIf we used a standard Lagrangian ansatz, we would now compute the derivate of the Lagrangian with respect to the primal variables. However, a general loss function l(\u00b7) is not necessarily differentiable. As a remedy, we only compute the derivatives wrt w, \u03c1 and \u03b3. Setting those to zero, yields the optimality conditions\n\u2200i : 0 \u2264 \u03b1i \u2264 \u03b7u \u2200j : 0 \u2264 \u03b1j \u2264 \u03b7l (7)\nw = n\u2211 i=1 \u03b1i\u03c6(xi) + n+m\u2211 j=n+1 \u03b1jy \u2217 j\u03c6(x \u2217 j ).\nInserting the above optimality conditions into the Lagrangian, the saddle point problem translates into\nmax \u03b1 \u2212 1 2 \u03b1TK\u03b1+ \u03b7u n\u2211 i=1 min t ( l(ti) + \u03b1i \u03b7u ti ) + \u03b7l n+m\u2211 j=n+1 min t\u2217 ( l(t\u2217j ) + \u03b1j \u03b7l t\u2217j ) .\nConverting the min into a max statement results in\nmax \u03b1 \u2212 1 2 \u03b1TK\u03b1\u2212 \u03b7u n\u2211 i=1 max t ( \u2212\u03b1i \u03b7u ti \u2212 l(ti) ) \u2212 \u03b7l n+m\u2211 j=n+1 max t\u2217 ( \u2212\u03b1j \u03b7l t\u2217j \u2212 l(t\u2217j ) ) .\nNow, making use of the Legendre-Fenchel conjugate lc(\u00b7) described above, we arrive at the following dual optimization problem\nmax \u03b1 \u2212 1 2 \u03b1TK\u03b1\u2212 \u03b7u n\u2211 i=1 lc(\u2212 \u03b1i \u03b7u )\u2212 \u03b7l n+m\u2211 j=n+1 lc(\u2212 \u03b1j \u03b7l ) (D)\ns.t. 1 = n\u2211 i=1 \u03b1i + n+m\u2211 j=n+1 \u03b1jy \u2217 j and \u03ba \u2264 n+m\u2211 j=n+1 \u03b1j .\nIn contrast to existing semi-supervised approaches to anomaly detection (Tax, 2001; Liu & Zheng, 2006; Wang et al., 2005), strong duality holds as shown by the following proposition.\nProposition 3.1 For the optimization problems (P) and (D) strong duality holds.\nProof This follows from the convexity of (P) and Slater\u2019s condition, which is trivially fulfilled for all \u03b3 by adjusting tj : \u2200\u03b3 > 0 \u2203tj \u2208 R : 0 = (y\u2217jwT\u03c6(x\u2217j ))\u2212 y\u2217j \u03c1\u2212 \u03b3 \u2212 tj .\nWe observe that the above optimization problems (P) and (D) contain the non-convex variant as a special case for translation-invariant kernels. Difficulties may arise in the presence of many equality and inequality constraints, which can increase computational requirements. However, this is not inherent; the left hand-side constraint can be removed by discarding the variable \u03c1 in the initial primal problem\u2014this leaves the regularization path of the optimization problem invariant\u2014and the right hand side inequality can be equivalently incorporated into the objective function by a Lagrangian argument (e.g., Proposition 12 in Kloft, Brefeld, Sonnenburg, & Zien, 2011). Note, that the convex model has only an intuitive interpretation for normalized kernels. In order to deal with a wider class of kernels, we need to resort to the more general non-convex formulation as presented in Section 3.1."}, {"heading": "4. Active Learning for Semi-supervised Anomaly Detection", "text": "In the previous section, we presented two optimization problems that incorporate labeled data into an unsupervised anomaly detection technique. However, we have not yet addressed the question of acquiring labeled examples. Many topical real-world applications involve millions of training instances (Sonnenburg, 2008) so that domain experts can only label a small fraction of the unlabeled data. Active learning deals with finding the instances that, once labeled and included in the training\nset, lead to the largest improvement of a re-trained model. In the following, we present an active learning strategy that is well-suited for anomaly detection. The core idea is to query low-confidence decisions to guide the user in the labeling process.\nOur approach works as follows. First, we initialize our method by training it on the unlabeled examples. The training set is then augmented by particular examples that have been selected by the active learning rule. The candidates are labeled by a domain expert and added to the training set. The model is retrained on the refined training set, which now consists of unlabeled and labeled examples. Subsequently labeling- and retraining-steps are repeated until the required performance is reached.\nThe active learning rule itself consists of two parts. We begin with a commonly used active learning strategy which simply queries borderline points. The idea of the method is to choose the point that is closest to the decision hypersphere (Almgren & Jonsson, 2004; Warmuth, Liao, R\u00e4tsch, Mathieson, Putta, & Lemmen, 2003) to be presented to the expert:\nx\u2032 = argmin x\u2208{x1,...,xn} \u2016f(x)\u2016 maxk \u2016f(xk)\u2016\n= argmin x\u2208{x1,...,xn} \u2225\u2225\u2225R2 \u2212 \u2016\u03c6(x)\u2212 c\u20162\u2225\u2225\u2225 . (8) For supervised support vector machines, this strategy is known as the margin strategy (Tong & Koller, 2000). Figure 5 (a) shows an illustratation for semi-supervised anomaly detection.\nWhen dealing with non-stationary outlier categories, it is beneficial to identify novel anomaly classes as soon as possible. We translate this requirement into an active learning strategy as follows. LetA = (aij)i,j=1,...,n+m be an adjacency matrix of the training instances, obtained by, for example, a k-nearest-neighbor approach, where aij = 1 if xi is among the k-nearest neighbors of xj and 0 otherwise. We introduce an extended labeling y\u03041 . . . , y\u0304n+m for all examples by defining y\u0304i = 0 for unlabeled instances and retaining the labels for labeled instances, i.e., y\u0304j = yj . Using these pseudo labels, Equation (9) returns the unlabeled instance according to\nx\u2032 = argmin xi\u2208{x1,...,xn}\n1\n2k n+m\u2211 j=1 (y\u0304j + 1) aij . (9)\nThe above strategy explores unknown clusters in feature space and thus labels orthogonal or complementary instances as illustrated in Figure 5 (b).\nNevertheless, using Equation (9) alone may result in querying points lying close to the center of the actual hypersphere. These points will hardly contribute to an improvement of the hypersphere. On the other hand, using the margin strategy alone does not allow for querying novel regions that lie far away from the margin. In other words, only a combination of both strategies (8) and (9) guarantees that points of interest are queried. Our final active learning strategy is therefore given by\nx\u2032 = argmin xi\u2208{x1,...,xn} = \u03b4 \u2016f(x)\u2016 c + 1\u2212 \u03b4 2k n+m\u2211 j=1 (y\u0304j + 1) aij (10)\nfor \u03b4 \u2208 [0, 1]. The combined strategy queries instances that are close to the boundary of the hypersphere and lie in potentially anomalous clusters with respect to the k-nearest neighbor graph, see Figure 5 (c) for an illustration. Depending on the actual value of \u03b4, the strategy jumps from cluster to cluster and thus helps to identify interesting regions in feature space. For the special case of no labeled points, our combined strategy reduces to the margin strategy.\nUsually, an active learning step is followed by an optimization step of the semi-supervised SVDD, to update the model with respect to recently labeled data. This procedure is of course timeconsuming and can be altered for practical settings, for instance by querying a couple of points before performing a model update. Irrespectively of the actual implementation, alternating between active learning and updating the model can be repeated until a desired predictive performance is obtained."}, {"heading": "5. Illustration of Proposed Learning Paradigm", "text": "In this section, we illustrate the weaknesses of existing learning paradigms in semi-supervised anomaly detection settings by means of a controlled experiment on synthetic data. The results, which have already been briefly sketched in the introduction (cf., Figure 2), are discussed in detail here.\nTo this end, we generate the nominal training and validation data from two isotropic Gaussian distributions in R2 and one anomaly cluster (shown in Figure 3 (left)). However, at testing time, two novel anomaly clusters appear in test data (shown in Figure 3 (right)). This reflects the characteristic that anomalies can stem from novel, previously unseen distributions. We compare our newly-developed method SSAD to the following baseline approaches: the unsupervised support\nvector domain description (SVDD, Tax & Duin, 2004), the corrected semi-supervised SVDDneg (Tax, 2001) described in Appendix A, a supervised support vector machine (SVM, Boser, Guyon, & Vapnik, 1992; Cortes & Vapnik, 1995), and the semi-supervised low-density separation (LDS, Chapelle & Zien, 2005), see Table 1. As common in anomaly detection setups, we measure the area under the ROC curve over the interval [0, 0.01] and report on AUCs averaged over 25 repetitions with distinct training, validation, and test sets. In every repetition, parameters \u03b7u, \u03b7l are adjusted on the respective validation set within the interval [10\u22122, 102]. In all experiments, we used \u03ba = 1. Error bars correspond to standard errors.\nThe results are shown in Figure 6, where the horizontal axis shows different ratios of labeled and randomly drawn unlabeled examples. Methods derived from the supervised learning paradigm such as SVM and LDS cannot cope with novel outlier clusters and perform poorly for all ratios of labeled and unlabeled examples; their performance remains below that of the unsupervised SVDD, which does not utilize labeled data at all and is thus unaffected by incorporating labels in the training process. By contrast, the two semi-supervised methods derived from the unsupervised learning paradigm clearly outperform all other baselines. However, the SVDDneg only benefits from anomalous labeled data and since these are sparse, needs a big fraction of labeled data to increase its performance. Our semi-supervised method SSAD exploits every single labeled example and needs only 15% of the labels to saturate around its optimum.\nFigure 7 visualizes typical contour lines of hypotheses computed by SSAD for three different scenarios. The figure shows a fully-supervised scenario where all instances are correctly labeled (left), a semi-supervised solution where 25% of the data is labeled and 75% remains unlabeled (center), and a completely unsupervised one using unlabeled data only (right). Unsurprisingly, the fully supervised solution discriminates perfectly between normal data and outliers while the unsupervised solution recognizes the latter as normal data by mistake. The intermediate semisupervised solution uses only little label information to also achieve a perfect separation of the involved classes.\nFigure 8 compares execution times of the different methods and shows the number of training examples versus training time. For simplicity, we discarded 50% of the labels in the training data at random. The results show that methods, such as SSAD, SVDDneg, and SVDD, that are derived from the unsupervised learning principle, perform similarly. The SVM performs best but uses only the labeled part of the training data and ignores the unlabeled examples. Low density separation (LDS) performs worst due to its transductive nature.\nBased on our observations, we draw the following conclusions. Anomaly detection scenarios render methods derived from the supervised learning paradigm inappropriate. Even unsupervised methods ignoring label information may perform better than their supervised peers. Intuitively, discarding label information is sub-optimal. Our experiment shows that semi-supervised methods from the unsupervised learning paradigm effectively incorporate label information and outperform all other competitors. We will confirm these findings in Section 6."}, {"heading": "6. Real-World Network Intrusion Detection", "text": "The goal of network intrusion detection is to identify attacks in incoming network traffic. Classical signature-based have proven insufficient for the identification of novel attacks, because signatures need to be manually crafted in advance. Therefore machine learning approaches have been gaining more and more attention by the intrusion detection research community.\nThe detection of unknown and novel attacks requires an adequate representation of network contents. In the remainder, we apply a technique for embedding network payloads in vector spaces derived from concepts of information retrieval (Salton, Wong, & Yang, 1975) and that has recently been applied in the application domain of network intrusion detection (Rieck & Laskov, 2007). A network payload x (the data contained in a network packet or connection) is mapped to a vector space using a set of strings S and an embedding function \u03c6. For each string s \u2208 S the function \u03c6s(x) returns 1 if s is contained in the payload x and 0 otherwise. By applying \u03c6s(x) for all elements of S we obtain the following map\n\u03c6 : X \u2192 R|S|, \u03c6 : x 7\u2192 (\u03c6s(x))s\u2208S , (11)\nwhere X is the domain of all network payloads. Defining a set S of relevant strings a priori is difficult as typical patterns of novel attacks are not available prior to their disclosure. As an alternative, we define the set S implicitly and associate S with all possible strings of length n. This resulting set of strings is often referred to as n-grams.\nAs a consequence of using n-grams, the network payloads are mapped to a vector space with 256n dimensions, which apparently contradicts efficient network intrusion detection. Fortunately, a payload of length T comprises at most (T \u2212 n) different n-grams and, consequently, the map \u03c6 is sparse, that is, the vast majority of dimensions is zero. This sparsity can be exploited to derive linear-time algorithms for extraction and comparison of embedded vectors. Instead of operating with full vectors, only non-zero dimensions are considered, where the extracted strings associated with each dimension can be maintained in efficient data structures (Rieck & Laskov, 2008).\nFor our experiments, we consider HTTP traffic recorded within 10 days at Fraunhofer Institute FIRST. The data set comprises 145,069 unmodified connections of average length of 489 bytes. The incoming byte stream of each connection is mapped to a vector space using 3-grams as detailed above. We refer to the FIRST data as the normal pool. The malicious pool contains 27 real attack classes generated using the Metasploit framework (Maynor, Mookhey, Cervini, & Beaver, 2007). It covers 15 buffer overflows, 8 code injections and 4 other attacks including HTTP tunnels and crosssite scripting. Every attack is recorded in 2\u20136 different variants using a virtual network environment and a decoy HTTP server, where the attack payload is adapted to match characteristics of the normal data pool. A detailed description of this data set is provided by Rieck (2009).\nTo study the robustness of our approach in a more realistic scenario, we also consider techniques to obfuscate malicious content by adapting attack payloads to mimic benign traffic in feature space (Fogla, Sharif, Perdisci, Kolesnikov, & Lee, 2006; Perdisci et al., 2009). As a consequence, the extracted features deviate less from normality and the classifier is likely to be fooled by the attack. For our purposes, it already suffices to study a simple cloaking technique by adding common HTTP headers to the payload while the malicious body of the attack remains unaltered. We apply this technique to the malicious pool and refer to the obfuscated set of attacks as cloaked pool."}, {"heading": "6.1 Detection Performance", "text": "In this section, we evaluate the statistical performance of SSAD in intrusion detection, in comparison to the baseline methods SVDD and SVDDneg. In addition, our combined active learning strategy is compared to random sampling.\nWe focus on two scenarios: normal vs. malicious and normal vs. cloaked data. For both settings, the respective byte streams are translated into a bag-of-3-grams representation. For each experiment, we randomly draw 966 training examples from the normal pool and 34 attacks, depending on the scenario, either from the malicious or the cloaked pool. Holdout and test sets are also drawn at random and consist of 795 normal connections and 27 attacks, each. We make sure that attacks of the same attack class occur either in the training, or in the test set but not in both. We report on 10 repetitions with distinct training, holdout, and test sets and measure the performance by the area under the ROC curve in the false-positive interval [0, 0.01] (AUC0.01)\nFigure 9(a) shows the results for normal vs. malicious data pools, where the x-axis depicts the percentage of randomly drawn labeled instances. Irrespectively of the amount of labeled data, the malicious traffic is detected by all methods equally well as the intrinsic nature of the attacks is well captured by the bag-of-3-grams representation (cf., Wang, Parekh, & Stolfo, 2006; Rieck & Laskov, 2006). There is no significant difference between the classifiers.\nFigure 9(b) shows the results for normal vs. cloaked data. First of all, the performance of the unsupervised SVDD drops to just 70%. We obtain a similar result for the SVDDneg; incorporating cloaked attack information into the training process of the SVDD leads to an increase of about 5% which is far from any practical value. Notice that the SVDDneg cannot make use of labeled data of the normal class. Thus, its moderate ascent in terms of the number of labeled examples is credited to the class ratio of 966/34 for the random labeling strategy. The bulk of additional information cannot be exploited and has to be left out. By contrast, our semi-supervised method SSAD includes all labeled data into the training process and clearly outperforms the two baselines. For only 5% labeled data, SSAD easily beats the best baseline and for randomly labeling 15% of the available data it separates normal and cloaked malicious traffic almost perfectly.\nNevertheless, labeling 15% of the data is not realistic for practical applications. We thus explore the benefit of active learning for inquiring label information of borderline and low-confidence points. Figure 9(c) shows the results for normal vs. cloaked data, where the labeled data for SVDDneg and SSAD is chosen according to the active learning strategy in Equation (10). The unsupervised SVDD does not make use of labeled information and is unaffected by this setup, remaining at an AUC0.01 of 70%. Compared to the results when using a random labeling strategy (Figure 9(b)), the performance of the SVDDneg increases significantly. The ascent of the SVDDneg is now steeper and its performance yields 85% for just 15% labeled data. However, SSAD also improves for active learning and dominates the baselines. Using active learning, we need to label only 3% of the data for attaining an almost perfect separation, compared to 25% for a random labeling strategy. We conclude that our active learning strategy effectively improves the performance and reduces the manual labeling effort significantly.\nIn Figure 10 the impact of our active learning strategy given by Equation (10) is shown. We compare the number of outliers detected by the combined strategy with the margin-based strategy in Equation (8) (see also, Almgren & Jonsson, 2004) and by randomly drawing instances from the unlabeled pool. As a sanity check, we also included the theoretical outcome for random sampling.\nThe results show that the combined strategy effectively detects malicious traffic much faster than the margin-based strategy."}, {"heading": "6.2 Threshold Adaptation", "text": "The previous experiments have demonstrated the advantages of active learning for network intrusion detection. So far, all results have been obtained using our method SSAD; however, the active learning techniques devised in Section 4 are also applicable for calibrating other learning-based methods. We herein focus on the vanilla SVDD with parameter value \u03bd = 1, which corresponds to classical centroid-based anomaly detection (e.g., Shawe-Taylor & Cristianini, 2004), such that\nresults directly transfer to anomaly detectors as Anagram (Wang et al., 2006), McPad (Perdisci et al., 2009) and TokDoc (Krueger, Gehl, Rieck, & Laskov, 2010).\nWe again draw a set of 3,750 network connections from the pool of normal data and split the resulting set into a training set of 2,500 connections and a test partition of 1,250 events. Both sets are mixed with cloaked attack instances. The SVDD is then trained on the normal training set delivering a threshold R. For application of the learned hypersphere to the test set, we evaluate different strategies for determining a radius R\u0302 using random sampling and active learning. In both cases, the selected connections are labeled and a threshold is obtained by computing the mean of all labeled instances:\nR\u0302 =  R : #pos = 0 \u2227 #neg = 0 maxi d(xi) : #pos > 0 \u2227 #neg = 0 minj d(xj) : #pos = 0 \u2227 #neg > 0\u2211 i d(xi)+ \u2211 j d(xj)\n#pos+#neg : #pos > 0 \u2227 #neg > 0\n(12)\nWhere xi are the positive labeled examples, xj the negative examples and d(x) = ||\u03c6(x) \u2212 c|| denotes the distance of the current sample x from the hyperspheres origin. Figure 11 shows the ROC curve of the SVDD and the computed thresholds for various levels of labeled data. Results have been averaged over 10 random draws of working sets. One can see that even for small amounts of labeled data the active learning strategy finds a reasonable radius while the random strategy and the vanilla SVDD completely fail with a false-positive rate of 0.5 and 1, respectively. This result demonstrates that active learning strategies enable calibrating anomaly detectors with a significantly\nreduced effort in comparison to random sampling and hence provide a valuable instrument when deploying learning methods in practice."}, {"heading": "7. Conclusion", "text": "In this article, we developed a framework for semi-supervised anomaly detection, which allows for the inclusion of prior and expert knowledge. We discussed the conceptual difference of semisupervised models which are derived from unsupervised or supervised techniques and proposed a generalization of the support vector data description to incorporate labeled data. The optimization problem of semi-supervised anomaly detection (SSAD) is an unconstrained, continuous problem, that allows for an efficient optimization by gradient-based methods and has a convex equivalent under mild assumptions on the kernel function.\nWe approached semi-supervised anomaly detection from an unsupervised learning paradigm. We proposed a novel active learning strategy that is specially tailored to anomaly detection. Our strategy guides the user to in the labeling process by querying instances that are not only close to the boundary of the hypersphere, but are also likely to contain instances of novel outlier categories.\nEmpirically, we applied semi-supervised anomaly detection to the application domain of network intrusion detection. We showed that rephrasing the unsupervised problem as a semi-supervised task is beneficial in practice: SSAD proves robust in scenarios where the performance of baseline approaches deteriorates due to obfuscation techniques. Moreover, we demonstrated the effectiveness of our active learning strategy on a couple of data sets and observed SSAD to significantly improve the prediction accuracy by effectively exploiting the limited amount of labeled available. We observed that only a handful labeled instances are necessary to boost the performance. This characteristic is especially appealing in tasks where labeling data is costly such as network security where the traffic has to be inspected for malicious patterns by an expert expert.\nThere are many possibilities to exploit and extend our learning approach as well as our active learning strategy. For example, replacing the `2-norm regularization by a sparsity-inducing `1-norm to incorporate automatic feature selection reduces the dimensionality of the solution. Learning sparse feature representations are of great interest for other computer security applications such as signature generation. A possible optimization strategy could be the linear programming (LP) approach by Campbell and Bennett (2001) for data domain description. However, other choices of regularizers are certainly possible including structured regularizers to incorporate hierarchies in the learning process or non-isotropic norms to encode additional domain knowledge. Incorporating multiple labels and rephrasing semi-supervised anomaly detection as a multi-task problem might also improve accuracy in complex application domains."}, {"heading": "Acknowledgments", "text": "The authors are very grateful to Klaus-Robert M\u00fcller for comments that helped improving the manuscript. This work was supported in part by the German Bundesministerium f\u00fcr Bildung und Forschung (BMBF) under the project PROSEC (FKZ 01BY1145), by the FP7-ICT Programme of the European Community, under the PASCAL2 Network of Excellence, and by the German National Science Foundation (DFG) under GA 1615/1-1, MU 987/6-1, MU 987/11-1 and RA 1894/1- 1. Furthermore, Marius Kloft acknowledges a PhD scholarship by the German Academic Exchange Service (DAAD) and a postdoctoral fellowship by the German Research Foundation (DFG) as well as funding by the Ministry of Education, Science, and Technology, through the National Research Foundation of Korea under Grant R31-10008. A part of the work was done while Marius Kloft was with Computer Science Division and Department of Statistics, University of California, Berkeley, CA 94720-1758, USA.\nAppendix A. Analysis of SVDDneg\nIn this appendix, we point out a limitation of previously published methods such as SVDDneg (Tax, 2001) as well as methods proposed by Hoi, Chan, Huang, Lyu, and King (2003), Liu and Zheng (2006), Wang et al. (2005), and Yuan and Casasent (2004). These methods suffer from potential duality gaps as they are optimized in dual space but\u2014depending on the training data\u2014run the risk of originating from a non-convex optimization problem. For instance, this is the case if only a single negatively labeled example is included in the training set. This issue is not addressed in the aforementioned papers.\nWe exemplarily illustrate the problem for the SVDDneg. The SVDDneg incorporates labeled examples of the outlier class into the otherwise unsupervised learning process. As before, the majority of the (unlabeled) data points shall lie inside the sphere while the labeled outliers are constrained to lie outside of the normality ball. This results in a two-class problem where the positive class consists of unlabeled data and the negative class is formed by the labeled outliers. After introducing class labels y \u2208 {+1,\u22121} (where unlabeled data points receive the class label yi = +1), the primal optimization problem is given by\nmin R,c,\u03be R2 + \u03b7 n\u2211 i=1 \u03bei\ns.t. \u2200ni=1 : yi\u2016\u03c6(xi)\u2212 c\u20162 \u2264 yiR2 + \u03bei and \u03bei \u2265 0, (13)\nand the corresponding optimization problem in dual space is given by\nmax \u03b1 n\u2211 i=1 \u03b1iyik(xi,xi)\u2212 n\u2211 i,j=1 \u03b1i\u03b1jyiyjk(xi,xj) (14)\ns.t. n\u2211 i=1 \u03b1iyi = 1 and 0 \u2264 \u03b1i \u2264 \u03b7 \u2200i = 1, . . . , n.\nThe existence of the duality gap is shown as follows: The second derivative of the primal constraints g(xi) = yi\u2016\u03c6(xi)\u2212 c\u20162 \u2212 yiR2 \u2212 \u03bei \u2264 0 given by \u22022g/\u2202c2 = 2yi is negative for outliers as their label equals yi = \u22121. This turns the whole optimization problem non-convex. As a consequence, the optimal solutions of the primal and dual problems may differ. Figure 12 shows an exemplary plot of the duality gap for artificially generated data where one nominal Gaussian is surrounded by a smaller \u2019anomalous\u2019 Gaussian. During the labeling process more and more data points receive their corresponding label and the more negative examples are present in the learning problem (horizontal axis) the larger is the duality gap and the larger is the difference of the two objective values (vertical axis). Note that the duality gap is not necessarily a monotonic function although the behavior is likely the case. Furthermore, the maximization of the dual problem yields a lower bound on the primal objective (blue line), whereas the latter is always greater or equal than the corresponding dual (red line).\nNevertheless, the following Theorem shows for the class of translation-invariant kernel functions, there exists an equivalent convex re-formulation in form of the one-class SVM (Sch\u00f6lkopf et al., 2001).\nTheorem A.1 The solution \u03b1\u2217 found by optimizing the dual of the non-convex SVDDneg as stated in Equations (14) is identical to the dual of the corresponding convex one-class SVM problem as stated in Equation (15) if the kernel is translation-invariant, i.e., k(xi,xi) = s \u2200i, s \u2208 R+.\nProof The dual of the one-class SVM is given by\nmax \u03b1 \u2212 1 2 n\u2211 i,j=1 \u03b1i\u03b1jyiyjk(xi,xj), s.t. n\u2211 i=1 \u03b1iyi = 1 and 0 \u2264 \u03b1i \u2264 \u03b7 \u2200i. (15)\nThe respective constraints are already equivalent. The dual SVDDneg objective with translationinvariant kernel reduces to\n\u03b1\u2217 = argmax \u03b1 n\u2211 i=1 \u03b1iyik(xi,xi)\u2212 n\u2211 i,j=1 \u03b1i\u03b1jyiyjk(xi,xj)\n= argmax \u03b1 ns n\u2211 i=1 \u03b1iyi \u2212 n\u2211 i,j=1 \u03b1i\u03b1jyiyjk(xi,xj)\n= argmax \u03b1\nns\u2212 n\u2211\ni,j=1\n\u03b1i\u03b1jyiyjk(xi,xj), (16)\nwhere we substituted the equality constraint of Eq. (15) in the last step. Finally, Equation (16) is precisely the one-class SVM dual objective, scaled by 12 and shifted by a constant ns. However, the optimal solution \u03b1\u2217 is not affected by this transformation which completes the proof."}, {"heading": "Appendix B. A Representer Theorem for SSAD", "text": "In this section, we show the applicability of the representer theorem for semi-supervised anomaly detection.\nTheorem B.1 (Representer Theorem in Sch\u00f6lkopf & Smola, 2002) LetH be a reproducing kernel Hilbert space with a kernel k : X \u00d7 X \u2192 R, a symmetric positive semi-definite function on the compact domain. For any function L : Rn \u2192 R, any nondecreasing function \u2126 : R\u2192 R. If\nJ\u2217 := min J(f)f\u2208H := min f \u2208 H{\u2126 ( ||f ||2H ) + L (f(x1), . . . , f(xn))}\nis well-defined, then there exist \u03b11, . . . , \u03b1n \u2208 R, such that\nf(\u00b7) = n\u2211 i=1 \u03b1ik(xi, \u00b7) (17)\nachieves J(f) = J\u2217. Furthermore, if \u2126 is increasing, then each minimizer of J(f) can be expressed in the form of Eq. (17).\nProposition B.2 The representer theorem can be applied to the non-expanded version of Equation (5).\nProof Recall the primal SSAD objective function which is given by\nJ(R, \u03b3, c) =R2 \u2212 \u03ba\u03b3 + \u03b7u n\u2211 i=1 ` ( R2 \u2212 ||\u03c6(xi)\u2212 c||2 ) + \u03b7l\nn+m\u2211 j=n+1 ` ( y\u2217j ( R2 \u2212 ||\u03c6(x\u2217j )\u2212 c||2 ) \u2212 \u03b3 ) .\nSubstituting T := R2 \u2212 ||c||2 leads to the new objective function\nJ(T, \u03b3, c) =||c||2 + T \u2212 \u03ba\u03b3 + \u03b7u n\u2211 i=1 ` ( T \u2212 ||\u03c6(xi)||2 + 2\u03c6(xi)\u2032c ) + \u03b7l\nn+m\u2211 j=n+1 ` ( y\u2217j ( T \u2212 ||\u03c6(x\u2217j )||2 + 2\u03c6(x\u2217j )\u2032c ) \u2212 \u03b3 ) .\nExpanding the center c in terms of labeled and unlabeled input examples is now covered by the representer theorem. After the optimization, T can be easily re-substituted to obtain the primal variables R, \u03b3, and c. This completes the proof.\nAppendix C. Computing the Gradients for Eq. (5)\nIn this section we compute the gradients of the SSAD formulation given by Eq. (5). This is a neccessary step to implement the gradient-based solver for SSAD. To this end, we consider the unconstrained optimization problem given by\nmin R,\u03b3,\u03b1\nR2 \u2212 \u03ba\u03b3 + \u03b7u n\u2211 i=1 `\u2206, ( R2 \u2212 k(xi,xi) + (2ei \u2212 \u03b1)\u2032K\u03b1 ) + \u03b7l\nn+m\u2211 j=n+1 `\u2206, ( y\u2217j ( R2 \u2212 k(x\u2217j ,x\u2217j ) + (2e\u2217j \u2212 \u03b1)\u2032K\u03b1 ) \u2212 \u03b3 ) ,\nwhere `\u2206, is the Huber loss given by\n`\u2206, (t) =  \u2206\u2212 t : t \u2264 \u2206\u2212\n(\u2206+ \u2212t)2 4 : \u2206\u2212 \u2264 t \u2264 \u2206 + 0 : otherwise.\nFor notational convenience, we focus on the Huber loss for `\u2206=0, (t) and move margin dependent terms into the argument t and compute the gradients in several steps, as follows: first, we build the gradient with respect to the primal variables R and c, which yields\n\u2202\u03bei \u2202R = 2R`\u2032 (R 2 \u2212 ||\u03c6(xi)\u2212 c||2) \u2202\u03bei \u2202c = 2(\u03c6(xi)\u2212 c)`\u2032 (R2 \u2212 ||\u03c6(xi)\u2212 c||2). (18)\nThe derivatives of their counterparts \u03be\u2217j for the labeled examples with respect to R, \u03b3, and c are given by\n\u2202\u03be\u2217j \u2202R = 2y\u2217jR` \u2032 ( y\u2217j ( R2 \u2212 ||\u03c6(x\u2217j )\u2212 c||2 ) \u2212 \u03b3 ) \u2202\u03be\u2217j \u2202\u03b3 = \u2212`\u2032 ( y\u2217j ( R2 \u2212 ||\u03c6(x\u2217j )\u2212 c||2 ) \u2212 \u03b3 ) \u2202\u03be\u2217j \u2202c = 2y\u2217j (\u03c6(x \u2217 j )\u2212 c)`\u2032 ( y\u2217j ( R2 \u2212 ||\u03c6(x\u2217j )\u2212 c||2 ) \u2212 \u03b3 ) .\nSubstituting the partial gradients, we resolve the gradient of Equation (5) with respect to the primal variables as follows:\n\u2202EQ5\n\u2202R = 2R+ \u03b7u n\u2211 i=1 \u2202\u03bei \u2202R + \u03b7l n+m\u2211 j=n+1 \u2202\u03be\u2217j \u2202R , (19)\n\u2202EQ5\n\u2202\u03b3 = \u2212\u03ba+ \u03b7l n+m\u2211 j=n+1 \u2202\u03be\u2217j \u2202\u03b3 , (20)\n\u2202EQ5\n\u2202c = \u03b7u n\u2211 i=1 \u2202\u03bei \u2202c + \u03b7l n+m\u2211 j=n+1 \u2202\u03be\u2217j \u2202c . (21)\nIn the following, we extend our approach to allow for the use of kernel functions. An application of the representer theorem shows that the center c can be expanded as\nc = n\u2211 i=1 \u03b1i\u03c6(xi) + n+m\u2211 j=n+1 \u03b1jy \u2217 j\u03c6(x \u2217 j ). (22)\nAccording to the chain rule, the gradient of Equation (5) with respect to the \u03b1i/j is given by\n\u2202EQ5 \u2202\u03b1i/j = \u2202EQ5 \u2202c \u2202c \u2202\u03b1i/j .\nUsing Equation (22), the partial derivatives \u2202c\u2202\u03b1i/j resolve to\n\u2202c\n\u2202\u03b1i = \u03c6(xi) and\n\u2202c\n\u2202\u03b1j = y\u2217j\u03c6(x \u2217 j ), (23)\nrespectively. Applying the chain-rule to Equations (19),(20),(21), and (23) gives the gradients of Equation (5) with respect to the \u03b1i/j ."}], "references": [{"title": "Using active learning in intrusion detection", "author": ["M. Almgren", "E. Jonsson"], "venue": "In Proc. of IEEE Computer Security Foundation Workshop,", "citeRegEx": "Almgren and Jonsson,? \\Q2004\\E", "shortCiteRegEx": "Almgren and Jonsson", "year": 2004}, {"title": "Finding the outliers that matter", "author": ["D.F. Andrews", "D. Pregibon"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Andrews and Pregibon,? \\Q1978\\E", "shortCiteRegEx": "Andrews and Pregibon", "year": 1978}, {"title": "Semi-Supervised Novelty Detection", "author": ["G. Blanchard", "G. Lee", "C. Scott"], "venue": "Journal of Machine Learning", "citeRegEx": "Blanchard et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blanchard et al\\.", "year": 2010}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In COLT\u2019 98: Proc. of the eleventh annual conference on Computational learning theory,", "citeRegEx": "Blum and Mitchell,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell", "year": 1998}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B. Boser", "I. Guyon", "V. Vapnik"], "venue": "Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory,", "citeRegEx": "Boser et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Boser et al\\.", "year": 1992}, {"title": "A linear programming approach to novelty detection", "author": ["C. Campbell", "K. Bennett"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Campbell and Bennett,? \\Q2001\\E", "shortCiteRegEx": "Campbell and Bennett", "year": 2001}, {"title": "Anomaly detection: A survey", "author": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "ACM Computing Surveys,", "citeRegEx": "Chandola et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chandola et al\\.", "year": 2009}, {"title": "A continuation method for semi-supervised SVMs", "author": ["O. Chapelle", "M. Chi", "A. Zien"], "venue": "In ICML,", "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Semi-supervised classification by low density separation", "author": ["O. Chapelle", "A. Zien"], "venue": "In Proc. of the International Workshop on AI and Statistics", "citeRegEx": "Chapelle and Zien,? \\Q2005\\E", "shortCiteRegEx": "Chapelle and Zien", "year": 2005}, {"title": "Semi-Supervised Learning (Adaptive Computation and Machine Learning)", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Support vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning,", "citeRegEx": "Cortes and Vapnik,? \\Q1995\\E", "shortCiteRegEx": "Cortes and Vapnik", "year": 1995}, {"title": "Regularized bundle methods for large-scale learning problems with an application to large margin training of hidden Markov models", "author": ["Do", "T.-M.-T"], "venue": "Ph.D. thesis,", "citeRegEx": "Do and T..M..T.,? \\Q2010\\E", "shortCiteRegEx": "Do and T..M..T.", "year": 2010}, {"title": "Applications of Data Mining in Computer Security, chap. A geometric framework for unsupervised anomaly detection: detecting intrusions in unlabeled data", "author": ["E. Eskin", "A. Arnold", "M. Prerau", "L. Portnoy", "S. Stolfo"], "venue": null, "citeRegEx": "Eskin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Eskin et al\\.", "year": 2002}, {"title": "Polymorphic blending attacks", "author": ["P. Fogla", "M. Sharif", "R. Perdisci", "O. Kolesnikov", "W. Lee"], "venue": "In Proc. of USENIX Security Symposium", "citeRegEx": "Fogla et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fogla et al\\.", "year": 2006}, {"title": "Using one-class and two-class svms for multiclass image annotation", "author": ["Goh", "K.-S", "E.Y. Chang", "B. Li"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Goh et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Goh et al\\.", "year": 2005}, {"title": "Active and semi-supervised data domain description", "author": ["N. G\u00f6rnitz", "M. Kloft", "U. Brefeld"], "venue": "In ECML/PKDD", "citeRegEx": "G\u00f6rnitz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "G\u00f6rnitz et al\\.", "year": 2009}, {"title": "One class support vector machines for detecting anomalous windows registry accesses", "author": ["K. Heller", "K. Svore", "A. Keromytis", "S. Stolfo"], "venue": "In Proc. of the workshop on Data Mining for Computer Security", "citeRegEx": "Heller et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Heller et al\\.", "year": 2003}, {"title": "Support vector machines for class representation and discrimination", "author": ["Hoi", "C.-H", "Chan", "K. Huang", "M. Lyu", "I. King"], "venue": "In Proc. of the International Joint Conference on Neural Networks", "citeRegEx": "Hoi et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hoi et al\\.", "year": 2003}, {"title": "Robust statistics: a review", "author": ["P. Huber"], "venue": "Ann. Statist.,", "citeRegEx": "Huber,? \\Q1972\\E", "shortCiteRegEx": "Huber", "year": 1972}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Joachims,? \\Q1999\\E", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "`p-norm multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kloft et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kloft et al\\.", "year": 2011}, {"title": "A multi-model approach to the detection of webbased attacks", "author": ["C. Kruegel", "G. Vigna", "W. Robertson"], "venue": "Computer Networks,", "citeRegEx": "Kruegel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kruegel et al\\.", "year": 2005}, {"title": "TokDoc: A self-healing web application firewall", "author": ["T. Krueger", "C. Gehl", "K. Rieck", "P. Laskov"], "venue": "In Proc. of 25th ACM Symposium on Applied Computing (SAC),", "citeRegEx": "Krueger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2010}, {"title": "A study on combining image representations for image classification and retrieval", "author": ["C. Lai", "D.M.J. Tax", "R.P.W. Duin", "E. Zbieta", "P. Ekalska", "P.P. Ik"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Lai et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2004}, {"title": "Learning from Positive and Unlabeled Examples with Different Data Distributions", "author": ["Li", "X.-l", "B. Liu"], "venue": "In ECML", "citeRegEx": "Li et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Li et al\\.", "year": 2005}, {"title": "Building Text Classifiers Using Positive and Unlabeled Examples", "author": ["B. Liu", "Y. Dai", "X. Li", "W.S. Lee", "P.S. Yu"], "venue": "In IEEE International Conference on Data Mining,", "citeRegEx": "Liu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2003}, {"title": "Minimum enclosing and maximum excluding machine for pattern description and discrimination", "author": ["Y. Liu", "Y.F. Zheng"], "venue": "In ICPR \u201906: Proc. of the 18th International Conference on Pattern Recognition,", "citeRegEx": "Liu and Zheng,? \\Q2006\\E", "shortCiteRegEx": "Liu and Zheng", "year": 2006}, {"title": "SemiSupervised One-Class Support Vector Machines for Classification of Remote Sensing Data", "author": ["J. M\u0169noz Mar\u00ed", "F. Bovolo", "L. G\u00f3mez-Chova", "L. Bruzzone", "G. Camp-Valls"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "Mar\u00ed et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mar\u00ed et al\\.", "year": 2010}, {"title": "One-class svms for document classification", "author": ["L.M. Manevitz", "M. Yousef"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Manevitz and Yousef,? \\Q2002\\E", "shortCiteRegEx": "Manevitz and Yousef", "year": 2002}, {"title": "Semi-supervised co-training and active learning based approach for multi-view intrusion detection", "author": ["Mao", "C.-H", "Lee", "H.-M", "D. Parikh", "T. Chen", "Huang", "S.-Y"], "venue": "In SAC \u201909: Proc. of the 2009 ACM symposium on Applied Computing,", "citeRegEx": "Mao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2009}, {"title": "Novelty detection: a review \u2013 part 1: statistical approaches", "author": ["M. Markou", "S. Singh"], "venue": "Signal Processing,", "citeRegEx": "Markou and Singh,? \\Q2003\\E", "shortCiteRegEx": "Markou and Singh", "year": 2003}, {"title": "Novelty detection: a review \u2013 part 2: neural network based approaches", "author": ["M. Markou", "S. Singh"], "venue": "Signal Processing,", "citeRegEx": "Markou and Singh,? \\Q2003\\E", "shortCiteRegEx": "Markou and Singh", "year": 2003}, {"title": "An introduction to kernelbased learning algorithms", "author": ["M\u00fcller", "K.-R", "S. Mika", "G. R\u00e4tsch", "K. Tsuda", "B. Sch\u00f6lkopf"], "venue": "IEEE Neural Networks,", "citeRegEx": "M\u00fcller et al\\.,? \\Q2001\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2001}, {"title": "One class classification methods based non-relevance feedback document retrieval", "author": ["T. Onoda", "H. Murata", "S. Yamada"], "venue": "In WI-IATW \u201906: Proc. of the 2006 IEEE/WIC/ACM international conference on Web Intelligence and Intelligent Agent Technology,", "citeRegEx": "Onoda et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Onoda et al\\.", "year": 2006}, {"title": "SVDD-based pattern denoising", "author": ["J. Park", "D. Kang", "J. Kim", "J.T. Kwok", "I.W. Tsang"], "venue": "Neural Computation,", "citeRegEx": "Park et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Park et al\\.", "year": 2007}, {"title": "Bro: A System for Detecting Network Intruders in Real-Time", "author": ["V. Paxson"], "venue": "Elsevier Computer Networks,", "citeRegEx": "Paxson,? \\Q1999\\E", "shortCiteRegEx": "Paxson", "year": 1999}, {"title": "Active learning for anomaly and rare-category detection", "author": ["D. Pelleg", "A. Moore"], "venue": "In Proc. Advances in Neural Information Processing Systems,", "citeRegEx": "Pelleg and Moore,? \\Q2004\\E", "shortCiteRegEx": "Pelleg and Moore", "year": 2004}, {"title": "McPAD: A multiple classifier system for accurate payload-based anomaly detection", "author": ["R. Perdisci", "D. Ariu", "P. Fogla", "G. Giacinto", "W. Lee"], "venue": "Computer Networks,", "citeRegEx": "Perdisci et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Perdisci et al\\.", "year": 2009}, {"title": "Machine Learning for Application-Layer Intrusion Detection", "author": ["K. Rieck"], "venue": "Ph.D. thesis, Berlin Institute of Technology (TU Berlin)", "citeRegEx": "Rieck,? \\Q2009\\E", "shortCiteRegEx": "Rieck", "year": 2009}, {"title": "Detecting unknown network attacks using language models. In Detection of Intrusions and Malware, and Vulnerability Assessment", "author": ["K. Rieck", "P. Laskov"], "venue": "Proc. of 3rd DIMVA Conference,", "citeRegEx": "Rieck and Laskov,? \\Q2006\\E", "shortCiteRegEx": "Rieck and Laskov", "year": 2006}, {"title": "Language models for detection of unknown attacks in network traffic", "author": ["K. Rieck", "P. Laskov"], "venue": "Journal in Computer Virology,", "citeRegEx": "Rieck and Laskov,? \\Q2007\\E", "shortCiteRegEx": "Rieck and Laskov", "year": 2007}, {"title": "Linear-time computation of similarity measures for sequential data", "author": ["K. Rieck", "P. Laskov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rieck and Laskov,? \\Q2008\\E", "shortCiteRegEx": "Rieck and Laskov", "year": 2008}, {"title": "Value regularization and fenchel duality", "author": ["R.M. Rifkin", "R.A. Lippert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rifkin and Lippert,? \\Q2007\\E", "shortCiteRegEx": "Rifkin and Lippert", "year": 2007}, {"title": "Snort: Lightweight intrusion detection for networks", "author": ["M. Roesch"], "venue": "In Proc. of USENIX Large Installation System Administration Conference LISA,", "citeRegEx": "Roesch,? \\Q1999\\E", "shortCiteRegEx": "Roesch", "year": 1999}, {"title": "A vector space model for automatic indexing", "author": ["G. Salton", "A. Wong", "C. Yang"], "venue": "Communications of the ACM,", "citeRegEx": "Salton et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1975}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "Kernel methods for pattern analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "Beyond the point cloud: from transductive to semi-supervised learning", "author": ["V. Sindhwani", "P. Niyogi", "M. Belkin"], "venue": "In ICML,", "citeRegEx": "Sindhwani et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sindhwani et al\\.", "year": 2005}, {"title": "Machine Learning for Genomic Sequence Analysis", "author": ["S. Sonnenburg"], "venue": "Ph.D. thesis, Fraunhofer Institute FIRST. supervised by K.-R. Mu\u0308ller and G. Ra\u0308tsch", "citeRegEx": "Sonnenburg,? \\Q2008\\E", "shortCiteRegEx": "Sonnenburg", "year": 2008}, {"title": "Aladin: Active learning of anomalies to detect intrusion", "author": ["J.W. Stokes", "J.C. Platt"], "venue": "Tech. rep., Microsoft Research", "citeRegEx": "Stokes and Platt,? \\Q2008\\E", "shortCiteRegEx": "Stokes and Platt", "year": 2008}, {"title": "A comparative evaluation of two algorithms for windows registry anomaly detection", "author": ["S.J. Stolfo", "F. Apap", "E. Eskin", "K. Heller", "S. Hershkop", "A. Honig", "K. Svore"], "venue": "In Journal of Computer Security,", "citeRegEx": "Stolfo et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Stolfo et al\\.", "year": 2005}, {"title": "One-class classification", "author": ["D.M.J. Tax"], "venue": "Ph.D. thesis,", "citeRegEx": "Tax,? \\Q2001\\E", "shortCiteRegEx": "Tax", "year": 2001}, {"title": "Support vector data description", "author": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "Machine Learning,", "citeRegEx": "Tax and Duin,? \\Q2004\\E", "shortCiteRegEx": "Tax and Duin", "year": 2004}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "In Proc. of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "Tong and Koller,? \\Q2000\\E", "shortCiteRegEx": "Tong and Koller", "year": 2000}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1998\\E", "shortCiteRegEx": "Vapnik", "year": 1998}, {"title": "Pattern classification via single spheres", "author": ["J. Wang", "P. Neskovic", "L.N. Cooper"], "venue": "In Computer Science: Discovery Science (DS),", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}, {"title": "Anagram: A content anomaly detector resistant to mimicry attack", "author": ["K. Wang", "J. Parekh", "S. Stolfo"], "venue": "In Recent Adances in Intrusion Detection (RAID),", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "Active learning with support vector machines in the drug discovery process", "author": ["M.K. Warmuth", "J. Liao", "G. R\u00e4tsch", "M. Mathieson", "S. Putta", "C. Lemmen"], "venue": "Journal of Chemical Information and Computer Sciences,", "citeRegEx": "Warmuth et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Warmuth et al\\.", "year": 2003}, {"title": "Pseudo relevance feedback with biased support vector machine", "author": ["C. Yuan", "D. Casasent"], "venue": "In Proc. of the International Joint Conference on Neural Networks", "citeRegEx": "Yuan and Casasent,? \\Q2004\\E", "shortCiteRegEx": "Yuan and Casasent", "year": 2004}, {"title": "A simple probabilistic approach to learning from positive and unlabeled examples", "author": ["D. Zhang", "W.S. Lee"], "venue": "In Proceedings of the 5th Annual UK Workshop on ", "citeRegEx": "Zhang and Lee,? \\Q2005\\E", "shortCiteRegEx": "Zhang and Lee", "year": 2005}], "referenceMentions": [{"referenceID": 43, "context": "Conventional security techniques for intrusion detection are based on identifying known patterns of misuse, so called signatures (Roesch, 1999; Paxson, 1999) and thus\u2014although being effective against known attacks\u2014fail to protect from novel threats.", "startOffset": 129, "endOffset": 157}, {"referenceID": 35, "context": "Conventional security techniques for intrusion detection are based on identifying known patterns of misuse, so called signatures (Roesch, 1999; Paxson, 1999) and thus\u2014although being effective against known attacks\u2014fail to protect from novel threats.", "startOffset": 129, "endOffset": 157}, {"referenceID": 55, "context": "For instance, a prominent bias assumes that the unlabeled data is structured in clusters so that closeness (with respect to some measure) is proportional to the probability of having the same class label (Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005; Chapelle, Chi, & Zien, 2006; Sindhwani, Niyogi, & Belkin, 2005).", "startOffset": 204, "endOffset": 321}, {"referenceID": 19, "context": "For instance, a prominent bias assumes that the unlabeled data is structured in clusters so that closeness (with respect to some measure) is proportional to the probability of having the same class label (Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005; Chapelle, Chi, & Zien, 2006; Sindhwani, Niyogi, & Belkin, 2005).", "startOffset": 204, "endOffset": 321}, {"referenceID": 19, "context": "The transductive support vector machine (TSVM, Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005) optimizes a max-margin hyperplane in feature space that implements the cluster assumption.", "startOffset": 40, "endOffset": 99}, {"referenceID": 18, "context": "The transductive support vector machine (TSVM, Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005) optimizes a max-margin hyperplane in feature space that implements the cluster assumption. In its most basic formulation, the TSVM is a non-convex integer programming problem on top of an SVM. This is computationally very expensive so that Chapelle, Chi et al. (2006) propose a more efficient smooth relaxation of the TSVM.", "startOffset": 61, "endOffset": 368}, {"referenceID": 8, "context": "Another related approach is low density separation (LDS) by Chapelle and Zien (2005), where the cluster structure is modeled by a graph of distances.", "startOffset": 60, "endOffset": 85}, {"referenceID": 12, "context": "For instance, finding anomalies in network traffic (Eskin et al., 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks.", "startOffset": 51, "endOffset": 71}, {"referenceID": 46, "context": "Support vector learning has also been extended to many non-standard settings such as one-class learning (Sch\u00f6lkopf et al., 2001) and support vector data description (Tax & Duin, 2004).", "startOffset": 104, "endOffset": 128}, {"referenceID": 52, "context": "Zhang and Lee (2005) show that this class of methods can be viewed as a special case of semi-supervised learning and emphasize that the SVDD (Tax, 2001) can be considered an instance of LPUE.", "startOffset": 141, "endOffset": 152}, {"referenceID": 4, "context": "A broad overview of anomaly detection can be found in the work of Chandola et al. (2009). Anomaly detection is being regarded as unsupervised learning task and therefore it is not surprising that there exist a large number of applications employing unsupervised anomaly detection methods.", "startOffset": 66, "endOffset": 89}, {"referenceID": 0, "context": ", 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks.", "startOffset": 430, "endOffset": 457}, {"referenceID": 0, "context": ", 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004).", "startOffset": 430, "endOffset": 564}, {"referenceID": 0, "context": ", 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take a multi-view and co-training approach based on Blum and Mitchell (1998) to learn from labeled and unlabeled data.", "startOffset": 430, "endOffset": 732}, {"referenceID": 0, "context": ", 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take a multi-view and co-training approach based on Blum and Mitchell (1998) to learn from labeled and unlabeled data.", "startOffset": 430, "endOffset": 809}, {"referenceID": 0, "context": ", 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take a multi-view and co-training approach based on Blum and Mitchell (1998) to learn from labeled and unlabeled data. Support vector learning has also been extended to many non-standard settings such as one-class learning (Sch\u00f6lkopf et al., 2001) and support vector data description (Tax & Duin, 2004). The idea of the latter is to learn a hypersphere that encloses the bulk of the provided data so that all instances that lie outside of the hypersphere are considered anomalous. By contrast, the one-class SVM learns a hyperplane in some feature space that divides the data points from the origin with maximum-margin. For translation-invariant kernel matrices, both approaches are equivalent. There exist only a few semi-supervised methods that are based on unsupervised techniques. Blanchard, Lee, and Scott (2010) propose a method which has the appealing option of specifying an upper threshold on the false-positives rate.", "startOffset": 430, "endOffset": 1550}, {"referenceID": 0, "context": ", 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take a multi-view and co-training approach based on Blum and Mitchell (1998) to learn from labeled and unlabeled data. Support vector learning has also been extended to many non-standard settings such as one-class learning (Sch\u00f6lkopf et al., 2001) and support vector data description (Tax & Duin, 2004). The idea of the latter is to learn a hypersphere that encloses the bulk of the provided data so that all instances that lie outside of the hypersphere are considered anomalous. By contrast, the one-class SVM learns a hyperplane in some feature space that divides the data points from the origin with maximum-margin. For translation-invariant kernel matrices, both approaches are equivalent. There exist only a few semi-supervised methods that are based on unsupervised techniques. Blanchard, Lee, and Scott (2010) propose a method which has the appealing option of specifying an upper threshold on the false-positives rate. However, this method needs to include test instances at training time and is not applicable in online and streaming scenarios such as anomaly detection. The same holds true for an extension of the one-class SVM by M\u0169noz Mar\u00ed, Bovolo, G\u00f3mez-Chova, Bruzzone, and Camp-Valls (2010) that incorporates labeled examples in a graphLaplacian regularization term.", "startOffset": 430, "endOffset": 1939}, {"referenceID": 0, "context": ", 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take a multi-view and co-training approach based on Blum and Mitchell (1998) to learn from labeled and unlabeled data. Support vector learning has also been extended to many non-standard settings such as one-class learning (Sch\u00f6lkopf et al., 2001) and support vector data description (Tax & Duin, 2004). The idea of the latter is to learn a hypersphere that encloses the bulk of the provided data so that all instances that lie outside of the hypersphere are considered anomalous. By contrast, the one-class SVM learns a hyperplane in some feature space that divides the data points from the origin with maximum-margin. For translation-invariant kernel matrices, both approaches are equivalent. There exist only a few semi-supervised methods that are based on unsupervised techniques. Blanchard, Lee, and Scott (2010) propose a method which has the appealing option of specifying an upper threshold on the false-positives rate. However, this method needs to include test instances at training time and is not applicable in online and streaming scenarios such as anomaly detection. The same holds true for an extension of the one-class SVM by M\u0169noz Mar\u00ed, Bovolo, G\u00f3mez-Chova, Bruzzone, and Camp-Valls (2010) that incorporates labeled examples in a graphLaplacian regularization term. Tax (2001) proposes a straight-forward extension of the SVDD to semi-supervised anomaly detection, where negatively labeled points are required to lie outside of the hypersphere\u2014otherwise a penalty is incurred.", "startOffset": 430, "endOffset": 2026}, {"referenceID": 0, "context": ", 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take a multi-view and co-training approach based on Blum and Mitchell (1998) to learn from labeled and unlabeled data. Support vector learning has also been extended to many non-standard settings such as one-class learning (Sch\u00f6lkopf et al., 2001) and support vector data description (Tax & Duin, 2004). The idea of the latter is to learn a hypersphere that encloses the bulk of the provided data so that all instances that lie outside of the hypersphere are considered anomalous. By contrast, the one-class SVM learns a hyperplane in some feature space that divides the data points from the origin with maximum-margin. For translation-invariant kernel matrices, both approaches are equivalent. There exist only a few semi-supervised methods that are based on unsupervised techniques. Blanchard, Lee, and Scott (2010) propose a method which has the appealing option of specifying an upper threshold on the false-positives rate. However, this method needs to include test instances at training time and is not applicable in online and streaming scenarios such as anomaly detection. The same holds true for an extension of the one-class SVM by M\u0169noz Mar\u00ed, Bovolo, G\u00f3mez-Chova, Bruzzone, and Camp-Valls (2010) that incorporates labeled examples in a graphLaplacian regularization term. Tax (2001) proposes a straight-forward extension of the SVDD to semi-supervised anomaly detection, where negatively labeled points are required to lie outside of the hypersphere\u2014otherwise a penalty is incurred. An advantage of this so-called SVDD approach is that no further assumptions on the underlying data-generating probability distribution such as manifold assumptions are imposed. Unfortunately, the primal SVDD problem is not a convex optimization problem, which makes it very difficult to accurately optimize. Moreover, dual optimization as proposed in the work of Tax (2001) cannot be considered an sound alternative due to possible duality gaps.", "startOffset": 430, "endOffset": 2600}, {"referenceID": 0, "context": ", 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take a multi-view and co-training approach based on Blum and Mitchell (1998) to learn from labeled and unlabeled data. Support vector learning has also been extended to many non-standard settings such as one-class learning (Sch\u00f6lkopf et al., 2001) and support vector data description (Tax & Duin, 2004). The idea of the latter is to learn a hypersphere that encloses the bulk of the provided data so that all instances that lie outside of the hypersphere are considered anomalous. By contrast, the one-class SVM learns a hyperplane in some feature space that divides the data points from the origin with maximum-margin. For translation-invariant kernel matrices, both approaches are equivalent. There exist only a few semi-supervised methods that are based on unsupervised techniques. Blanchard, Lee, and Scott (2010) propose a method which has the appealing option of specifying an upper threshold on the false-positives rate. However, this method needs to include test instances at training time and is not applicable in online and streaming scenarios such as anomaly detection. The same holds true for an extension of the one-class SVM by M\u0169noz Mar\u00ed, Bovolo, G\u00f3mez-Chova, Bruzzone, and Camp-Valls (2010) that incorporates labeled examples in a graphLaplacian regularization term. Tax (2001) proposes a straight-forward extension of the SVDD to semi-supervised anomaly detection, where negatively labeled points are required to lie outside of the hypersphere\u2014otherwise a penalty is incurred. An advantage of this so-called SVDD approach is that no further assumptions on the underlying data-generating probability distribution such as manifold assumptions are imposed. Unfortunately, the primal SVDD problem is not a convex optimization problem, which makes it very difficult to accurately optimize. Moreover, dual optimization as proposed in the work of Tax (2001) cannot be considered an sound alternative due to possible duality gaps. However, in Appendix A we show that there is a convex reformulation of the SVDD for translation-invariant kernels, such as RBF-kernels. The new formulation does not suffer from duality gaps and can be easily solved by primal or dual descent methods. The same problem occurs in related semi-supervised one-class methods as proposed by Liu and Zheng (2006) and Wang, Neskovic, and Cooper (2005).", "startOffset": 430, "endOffset": 3027}, {"referenceID": 0, "context": ", 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take a multi-view and co-training approach based on Blum and Mitchell (1998) to learn from labeled and unlabeled data. Support vector learning has also been extended to many non-standard settings such as one-class learning (Sch\u00f6lkopf et al., 2001) and support vector data description (Tax & Duin, 2004). The idea of the latter is to learn a hypersphere that encloses the bulk of the provided data so that all instances that lie outside of the hypersphere are considered anomalous. By contrast, the one-class SVM learns a hyperplane in some feature space that divides the data points from the origin with maximum-margin. For translation-invariant kernel matrices, both approaches are equivalent. There exist only a few semi-supervised methods that are based on unsupervised techniques. Blanchard, Lee, and Scott (2010) propose a method which has the appealing option of specifying an upper threshold on the false-positives rate. However, this method needs to include test instances at training time and is not applicable in online and streaming scenarios such as anomaly detection. The same holds true for an extension of the one-class SVM by M\u0169noz Mar\u00ed, Bovolo, G\u00f3mez-Chova, Bruzzone, and Camp-Valls (2010) that incorporates labeled examples in a graphLaplacian regularization term. Tax (2001) proposes a straight-forward extension of the SVDD to semi-supervised anomaly detection, where negatively labeled points are required to lie outside of the hypersphere\u2014otherwise a penalty is incurred. An advantage of this so-called SVDD approach is that no further assumptions on the underlying data-generating probability distribution such as manifold assumptions are imposed. Unfortunately, the primal SVDD problem is not a convex optimization problem, which makes it very difficult to accurately optimize. Moreover, dual optimization as proposed in the work of Tax (2001) cannot be considered an sound alternative due to possible duality gaps. However, in Appendix A we show that there is a convex reformulation of the SVDD for translation-invariant kernels, such as RBF-kernels. The new formulation does not suffer from duality gaps and can be easily solved by primal or dual descent methods. The same problem occurs in related semi-supervised one-class methods as proposed by Liu and Zheng (2006) and Wang, Neskovic, and Cooper (2005). Another broad class of methods deals with learning from positive and unlabeled examples (LPUE).", "startOffset": 430, "endOffset": 3065}, {"referenceID": 0, "context": ", 2002) or program behaviour (Heller, Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007) or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, & Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006) Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take a multi-view and co-training approach based on Blum and Mitchell (1998) to learn from labeled and unlabeled data. Support vector learning has also been extended to many non-standard settings such as one-class learning (Sch\u00f6lkopf et al., 2001) and support vector data description (Tax & Duin, 2004). The idea of the latter is to learn a hypersphere that encloses the bulk of the provided data so that all instances that lie outside of the hypersphere are considered anomalous. By contrast, the one-class SVM learns a hyperplane in some feature space that divides the data points from the origin with maximum-margin. For translation-invariant kernel matrices, both approaches are equivalent. There exist only a few semi-supervised methods that are based on unsupervised techniques. Blanchard, Lee, and Scott (2010) propose a method which has the appealing option of specifying an upper threshold on the false-positives rate. However, this method needs to include test instances at training time and is not applicable in online and streaming scenarios such as anomaly detection. The same holds true for an extension of the one-class SVM by M\u0169noz Mar\u00ed, Bovolo, G\u00f3mez-Chova, Bruzzone, and Camp-Valls (2010) that incorporates labeled examples in a graphLaplacian regularization term. Tax (2001) proposes a straight-forward extension of the SVDD to semi-supervised anomaly detection, where negatively labeled points are required to lie outside of the hypersphere\u2014otherwise a penalty is incurred. An advantage of this so-called SVDD approach is that no further assumptions on the underlying data-generating probability distribution such as manifold assumptions are imposed. Unfortunately, the primal SVDD problem is not a convex optimization problem, which makes it very difficult to accurately optimize. Moreover, dual optimization as proposed in the work of Tax (2001) cannot be considered an sound alternative due to possible duality gaps. However, in Appendix A we show that there is a convex reformulation of the SVDD for translation-invariant kernels, such as RBF-kernels. The new formulation does not suffer from duality gaps and can be easily solved by primal or dual descent methods. The same problem occurs in related semi-supervised one-class methods as proposed by Liu and Zheng (2006) and Wang, Neskovic, and Cooper (2005). Another broad class of methods deals with learning from positive and unlabeled examples (LPUE). Intrinsically, one aims at solving a two-class problem but only data from one class (the positive class) is given together with unlabeled data points. LPUE can thus be applied to the problem setting at hand by identifying the outlier class with positively labeled data. Zhang and Lee (2005) show that this class of methods can be viewed as a special case of semi-supervised learning and emphasize that the SVDD (Tax, 2001) can be considered an instance of LPUE.", "startOffset": 430, "endOffset": 3453}, {"referenceID": 8, "context": "As a remedy, following the approach of Chapelle and Zien (2005), we translate Equation (2) into an unconstrained problem.", "startOffset": 39, "endOffset": 64}, {"referenceID": 18, "context": "To obtain a smooth optimization technique, we choose Huber\u2019s robust loss (Huber, 1972).", "startOffset": 73, "endOffset": 86}, {"referenceID": 52, "context": "In contrast to existing semi-supervised approaches to anomaly detection (Tax, 2001; Liu & Zheng, 2006; Wang et al., 2005), strong duality holds as shown by the following proposition.", "startOffset": 72, "endOffset": 121}, {"referenceID": 56, "context": "In contrast to existing semi-supervised approaches to anomaly detection (Tax, 2001; Liu & Zheng, 2006; Wang et al., 2005), strong duality holds as shown by the following proposition.", "startOffset": 72, "endOffset": 121}, {"referenceID": 49, "context": "Many topical real-world applications involve millions of training instances (Sonnenburg, 2008) so that domain experts can only label a small fraction of the unlabeled data.", "startOffset": 76, "endOffset": 94}, {"referenceID": 52, "context": "vector domain description (SVDD, Tax & Duin, 2004), the corrected semi-supervised SVDDneg (Tax, 2001) described in Appendix A, a supervised support vector machine (SVM, Boser, Guyon, & Vapnik, 1992; Cortes & Vapnik, 1995), and the semi-supervised low-density separation (LDS, Chapelle & Zien, 2005), see Table 1.", "startOffset": 90, "endOffset": 101}, {"referenceID": 37, "context": "To study the robustness of our approach in a more realistic scenario, we also consider techniques to obfuscate malicious content by adapting attack payloads to mimic benign traffic in feature space (Fogla, Sharif, Perdisci, Kolesnikov, & Lee, 2006; Perdisci et al., 2009).", "startOffset": 198, "endOffset": 271}, {"referenceID": 37, "context": "Instead of operating with full vectors, only non-zero dimensions are considered, where the extracted strings associated with each dimension can be maintained in efficient data structures (Rieck & Laskov, 2008). For our experiments, we consider HTTP traffic recorded within 10 days at Fraunhofer Institute FIRST. The data set comprises 145,069 unmodified connections of average length of 489 bytes. The incoming byte stream of each connection is mapped to a vector space using 3-grams as detailed above. We refer to the FIRST data as the normal pool. The malicious pool contains 27 real attack classes generated using the Metasploit framework (Maynor, Mookhey, Cervini, & Beaver, 2007). It covers 15 buffer overflows, 8 code injections and 4 other attacks including HTTP tunnels and crosssite scripting. Every attack is recorded in 2\u20136 different variants using a virtual network environment and a decoy HTTP server, where the attack payload is adapted to match characteristics of the normal data pool. A detailed description of this data set is provided by Rieck (2009). To study the robustness of our approach in a more realistic scenario, we also consider techniques to obfuscate malicious content by adapting attack payloads to mimic benign traffic in feature space (Fogla, Sharif, Perdisci, Kolesnikov, & Lee, 2006; Perdisci et al.", "startOffset": 188, "endOffset": 1069}, {"referenceID": 57, "context": "results directly transfer to anomaly detectors as Anagram (Wang et al., 2006), McPad (Perdisci et al.", "startOffset": 58, "endOffset": 77}, {"referenceID": 37, "context": ", 2006), McPad (Perdisci et al., 2009) and TokDoc (Krueger, Gehl, Rieck, & Laskov, 2010).", "startOffset": 15, "endOffset": 38}, {"referenceID": 5, "context": "A possible optimization strategy could be the linear programming (LP) approach by Campbell and Bennett (2001) for data domain description.", "startOffset": 82, "endOffset": 110}, {"referenceID": 52, "context": "Analysis of SVDD In this appendix, we point out a limitation of previously published methods such as SVDD (Tax, 2001) as well as methods proposed by Hoi, Chan, Huang, Lyu, and King (2003), Liu and Zheng (2006), Wang et al.", "startOffset": 106, "endOffset": 117}, {"referenceID": 51, "context": "Analysis of SVDD In this appendix, we point out a limitation of previously published methods such as SVDD (Tax, 2001) as well as methods proposed by Hoi, Chan, Huang, Lyu, and King (2003), Liu and Zheng (2006), Wang et al.", "startOffset": 107, "endOffset": 188}, {"referenceID": 26, "context": "Analysis of SVDD In this appendix, we point out a limitation of previously published methods such as SVDD (Tax, 2001) as well as methods proposed by Hoi, Chan, Huang, Lyu, and King (2003), Liu and Zheng (2006), Wang et al.", "startOffset": 189, "endOffset": 210}, {"referenceID": 26, "context": "Analysis of SVDD In this appendix, we point out a limitation of previously published methods such as SVDD (Tax, 2001) as well as methods proposed by Hoi, Chan, Huang, Lyu, and King (2003), Liu and Zheng (2006), Wang et al. (2005), and Yuan and Casasent (2004).", "startOffset": 189, "endOffset": 230}, {"referenceID": 26, "context": "Analysis of SVDD In this appendix, we point out a limitation of previously published methods such as SVDD (Tax, 2001) as well as methods proposed by Hoi, Chan, Huang, Lyu, and King (2003), Liu and Zheng (2006), Wang et al. (2005), and Yuan and Casasent (2004). These methods suffer from potential duality gaps as they are optimized in dual space but\u2014depending on the training data\u2014run the risk of originating from a non-convex optimization problem.", "startOffset": 189, "endOffset": 260}, {"referenceID": 46, "context": "Nevertheless, the following Theorem shows for the class of translation-invariant kernel functions, there exists an equivalent convex re-formulation in form of the one-class SVM (Sch\u00f6lkopf et al., 2001).", "startOffset": 177, "endOffset": 201}], "year": 2013, "abstractText": "Anomaly detection is being regarded as an unsupervised learning task as anomalies stem from adversarial or unlikely events with unknown distributions. However, the predictive performance of purely unsupervised anomaly detection often fails to match the required detection rates in many tasks and there exists a need for labeled data to guide the model generation. Our first contribution shows that classical semi-supervised approaches, originating from a supervised classifier, are inappropriate and hardly detect new and unknown anomalies. We argue that semi-supervised anomaly detection needs to ground on the unsupervised learning paradigm and devise a novel algorithm that meets this requirement. Although being intrinsically non-convex, we further show that the optimization problem has a convex equivalent under relatively mild assumptions. Additionally, we propose an active learning strategy to automatically filter candidates for labeling. In an empirical study on network intrusion detection data, we observe that the proposed learning methodology requires much less labeled data than the state-of-the-art, while achieving higher detection accuracies.", "creator": "TeX"}}}