{"id": "1401.7727", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2014", "title": "Security Evaluation of Support Vector Machines in Adversarial Environments", "abstract": "support vector machines ( svms ) currently are among the most popular classification techniques adopted in security applications like malware detection, intrusion detection, and spam filtering. however, if svms problems are to be incorporated in real - world security systems, they must be able to cope with attack discovery patterns that can either mislead the learning algorithm ( poisoning ), evade detection ( evasion ), or gain information about their internal parameters ( privacy breaches ). the main contributions of this chapter are twofold. first, we thoroughly introduce a formal general framework for the empirical evaluation of the security of machine - learning systems. second, according to our framework, we demonstrate the feasibility of evasion, poisoning and privacy attacks against svms in real - world security problems. for each attack recognition technique, firstly we evaluate its impact and discuss whether ( and how ) it can be countered through an adversary - aware design of svms. gradually our experiments are easily reproducible thanks to any open - engine source code development that we have made available, together with all the employed datasets, on a public repository.", "histories": [["v1", "Thu, 30 Jan 2014 03:37:18 GMT  (559kb,D)", "http://arxiv.org/abs/1401.7727v1", "47 pages, 9 figures; chapter accepted into book 'Support Vector Machine Applications'"]], "COMMENTS": "47 pages, 9 figures; chapter accepted into book 'Support Vector Machine Applications'", "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["battista biggio", "igino corona", "blaine nelson", "benjamin i p rubinstein", "davide maiorca", "giorgio fumera", "giorgio giacinto", "and fabio roli"], "accepted": false, "id": "1401.7727"}, "pdf": {"name": "1401.7727.pdf", "metadata": {"source": "CRF", "title": "Security Evaluation of Support Vector Machines in Adversarial Environments", "authors": ["Battista Biggio", "Igino Corona", "Blaine Nelson", "Benjamin I. P. Rubinstein", "Davide Maiorca", "Giorgio Fumera", "Giorgio Giacinto", "Fabio Roli"], "emails": ["battista.biggio@diee.unica.it", "igino.corona@diee.unica.it", "davide.maiorca@diee.unica.it", "fumera@diee.unica.it", "giacinto@diee.unica.it", "roli@diee.unica.it", "blaine.nelson@gmail.com", "ben@bipr.net"], "sections": [{"heading": null, "text": "Battista Biggio, Igino Corona, Davide Maiorca, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d\u2019Armi 09123, Cagliari, Italy. e-mail: {battista.biggio,igino.corona,davide.maiorca}@diee.unica.it e-mail: {fumera,giacinto,roli}@diee.unica.it Blaine Nelson Institut fu\u0308r Informatik, Universita\u0308t Potsdam, August-Bebel-Stra\u00dfe 89, 14482 Potsdam, Germany. e-mail: blaine.nelson@gmail.com Benjamin I. P. Rubinstein IBM Research, Lvl 5 / 204 Lygon Street, Carlton, VIC 3053, Australia. e-mail: ben@bipr.net\n1\nar X\niv :1\n40 1.\n77 27\nv1 [\ncs .L\nG ]\n3 0\nJa n\n2 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli"}, {"heading": "1 Introduction", "text": "Machine-learning and pattern-recognition techniques are increasingly being adopted in security applications like spam filtering, network intrusion detection, and malware detection due to their ability to generalize, and to potentially detect novel attacks or variants of known ones. Support Vector Machines (SVMs) are among the most successful techniques that have been applied for this purpose [28, 54].\nHowever, learning algorithms like SVMs assume stationarity: that is, both the data used to train the classifier and the operational data it classifies are sampled from the same (though possibly unknown) distribution. Meanwhile, in adversarial settings such as the above mentioned ones, intelligent and adaptive adversaries may purposely manipulate data (violating stationarity) to exploit existing vulnerabilities of learning algorithms, and to impair the entire system. This raises several open issues, related to whether machine-learning techniques can be safely adopted in security-sensitive tasks, or if they must (and can) be re-designed for this purpose. In particular, the main open issues to be addressed include: 1. analyzing the vulnerabilities of learning algorithms; 2. evaluating their security by implementing the corresponding attacks; and 3. eventually, designing suitable countermeasures.\nThese issues are currently addressed in the emerging research area of adversarial machine learning, at the intersection between computer security and machine learning. This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].\nIn Section 2, we review the literature of adversarial machine learning, focusing mainly on the issue of security evaluation. We discuss both theoretical work and applications, including examples of how learning can be attacked in practical scenarios, either during its training phase (i.e., poisoning attacks that contaminate the learner\u2019s training data to mislead it) or during its deployment phase (i.e., evasion attacks that circumvent the learned classifier).\nIn Section 3, we summarize our recently defined framework for the empirical evaluation of classifiers\u2019 security [12]. It is based on a general model of an adversary that builds on previous models and guidelines proposed in the literature of adversarial machine learning. We expound on the assumptions of the adversary\u2019s goal, knowledge and capabilities that comprise this model, which also easily accommodate application-specific constraints. Having detailed the assumptions of his adversary, a security analyst can formalize the adversary\u2019s strategy as an optimization problem.\nSecurity Evaluation of SVMs in Adversarial Environments 3\nWe then demonstrate our framework by applying it to assess the security of SVMs. We discuss our recently devised evasion attacks against SVMs [8] in Section 4, and review and extend our recent work [14] on poisoning attacks against SVMs in Section 5. We show that the optimization problems corresponding to the above attack strategies can be solved through simple gradient-descent algorithms. The experimental results for these evasion and poisoning attacks show that the SVM is vulnerable to these threats for both linear and non-linear kernels in several realistic application domains including handwritten digit classification and malware detection for PDF files. We further explore the threat of privacy-breaching attacks aimed at the SVM\u2019s training data in Section 6 where we apply our framework to precisely describe the setting and threat model.\nOur analysis provides useful insights into the potential security threats from the usage of learning algorithms (and, particularly, of SVMs) in real-world applications, and sheds light on whether they can be safely adopted for security-sensitive tasks. The presented analysis allows a system designer to quantify the security risk entailed by an SVM-based detector so that he may weigh it against the benefits provided by the learning. It further suggests guidelines and countermeasures that may mitigate threats and thereby improve overall system security. These aspects are discussed for evasion and poisoning attacks in Sections 4 and 5. In Section 6 we focus on developing countermeasures for privacy attacks that are endowed with strong theoretical guarantees within the framework of differential privacy. We conclude with a summary and discussion in Section 7.\nIn order to support the reproducibility of our experiments, we published all the code and the data employed for the experimental evaluations described in this paper [24]. In particular, our code is released under open-source license, and carefully documented, with the aim of allowing other researchers to not only reproduce, but also customize, extend and improve our work."}, {"heading": "2 Background", "text": "In this section, we review the main concepts used throughout this chapter. We first introduce our notation and summarize the SVM learning problem. We then motivate the need for the proper assessment of the security of a learning algorithm so that it can be applied to security-sensitive tasks.\nLearning can be generally stated as a process by which data is used to form a hypothesis that performs better than an a priori hypothesis formed without the data. For our purposes, the hypotheses will be represented as functions of the form f : X \u2192 Y , which assign an input sample point x \u2208X to a class y \u2208 Y ; that is, given an observation from the input space X , a hypothesis f makes a prediction in the output space Y . For binary classification, the output space is binary and we use Y = {\u22121,+1}. In the classical supervised learning setting, we are given a paired training dataset {(xi,yi) | xi \u2208X ,yi \u2208 Y }ni=1, we assume each pair is drawn independently from an unknown joint distribution P(X,Y ), and we want to infer a\n4 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\nclassifier f able to generalize well on P(X,Y ); i.e., to accurately predict the label y of an unseen sample x drawn from that distribution."}, {"heading": "2.1 Support Vector Machines", "text": "In its simplest formulation, an SVM learns a linear classifier for a binary classification problem. Its decision function is thus f (x) = sign(w>x + b), where sign(a) =+1 (\u22121) if a\u2265 0 (a < 0), and w and b are learned parameters that specify the position of the decision hyperplane in feature space: the hyperplane\u2019s normal w gives its orientation and b is its displacement. The learning task is thus to find a hyperplane that well-separates the two classes. While many hyperplanes may suffice for this task, the SVM hyperplane both separates the training samples of the two classes and provides a maximum distance from itself to the nearest training point (this distance is called the classifier\u2019s margin), since maximum-margin learning generally reduces generalization error [65]. Although originally designed for linearly-separable classification tasks (hard-margin SVMs), SVMs were extended to non-linearly-separable classification problems by Vapnik [25] (soft-margin SVMs), which allow some samples to violate the margin. In particular, a soft-margin SVM is learned by solving the following convex quadratic program (QP):\nmin w,b,\u03be 1 2 w>w+C n \u2211 i=1 \u03bei s. t. \u2200 i = 1, . . . ,n yi(w>xi +b)\u2265 1\u2212\u03bei and \u03bei \u2265 0 ,\nwhere the margin is maximized by minimizing 12 w >w, and the variables \u03bei (referred to as slack variables) represent the extent to which the samples, xi, violate the margin. The parameter C tunes the trade-off between minimizing the sum of the slack violation errors and maximizing the margin.\nWhile the primal can be optimized directly, it is often solved via its (Lagrangian) dual problem written in terms of Lagrange multipliers, \u03b1i, which are constrained so that \u2211ni=1 \u03b1iyi = 0 and 0 \u2264 \u03b1i \u2264C for i = 1, . . . ,n. Solving the dual has a computational complexity that grows according to the size of the training data as opposed to the feature space\u2019s dimensionality. Further, in the dual formulation, both the data and the slack variables become implicitly represented\u2014the data is represented by a kernel matrix, K, of all inner products between pairs of data points (that is, Ki, j = x>i x j) and each slack variable is associated with a Lagrangian multiplier via the KKT conditions that arise from duality. Using the method of Lagrangian multipliers, the dual problem is derived, in matrix form, as\nmin \u03b1 1 2 \u03b1>Q\u03b1\u22121>n \u03b1 s. t. n\n\u2211 i=1 \u03b1iyi = 0 and \u2200 i = 1, . . . ,n 0\u2264 \u03b1i \u2264C ,\nSecurity Evaluation of SVMs in Adversarial Environments 5\nwhere Q = K \u25e6 yy> (the Hadamard product of K and yy>) and 1n is a vector of n ones.\nThrough the kernel matrix, SVMs can be extended to more complex feature spaces (where a linear classifier may perform better) via a kernel function\u2014an implicit inner product from the alternative feature space. That is, if some function \u03c6 : X \u2192\u03a6 maps training samples into a higher-dimensional feature space, then Ki j is computed via the space\u2019s corresponding kernel function, \u03ba(xi,x j) = \u03c6(xi)>\u03c6(x j). Thus, one need not explicitly know \u03c6 , only its corresponding kernel function.\nFurther, the dual problem and its KKT conditions elicit interesting properties of the SVM. First, the optimal primal hyperplane\u2019s normal vector, w, is a linear combination of the training samples;1 i.e., w=\u2211ni=1 \u03b1iyixi. Second, the dual solution is sparse, and only samples that lie on or within the hyperplane\u2019s margin have a nonzero \u03b1-value. Thus, if \u03b1i = 0, the corresponding sample xi is correctly classified, lies beyond the margin (i.e., yi(w>xi+b)> 1) and is called a non-support vector. If \u03b1i = C, the ith sample violates the margin (i.e., yi(w>xi +b)< 1) and is an error vector. Finally, if 0 < \u03b1i <C, the ith sample lies exactly on the margin (i.e., yi(w>xi +b) = 1) and is a support vector. As a consequence, the optimal displacement b can be determined by averaging yi\u2212w>xi over the support vectors."}, {"heading": "2.2 Machine Learning for Computer Security: Motivation, Trends, and Arms Races", "text": "In this section, we motivate the recent adoption of machine-learning techniques in computer security and discuss the novel issues this trend raises. In the last decade, security systems increased in complexity to counter the growing sophistication and variability of attacks; a result of a long-lasting and continuing arms race in securityrelated applications such as malware detection, intrusion detection and spam filtering. The main characteristics of this struggle and the typical approaches pursued in security to face it are discussed in Section 2.3.1. We now discuss some examples that better explain this trend and motivate the use of modern machine-learning techniques for security applications.\nIn the early years, the attack surface (i.e., the vulnerable points of a system) of most systems was relatively small and most attacks were simple. In this era, signature-based detection systems (e.g., rule-based systems based on string-matching techniques) were considered sufficient to provide an acceptable level of security. However, as the complexity and exposure of sensitive systems increased in the Internet Age, more targets emerged and the incentive for attacking them became increasingly attractive, thus providing a means and motivation for developing sophisticated and diverse attacks. Since signature-based detection systems can only detect attacks matching an existing signature, attackers used minor variations of\n1 This is an instance of the Representer Theorem which states that solutions to a large class of regularized ERM problems lie in the span of the training data [60].\n6 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\ntheir attacks to evade detection (e.g., string-matching techniques can be evaded by slightly changing the attack code). To cope with the increasing variability of attack samples and to detect never-before-seen attacks, machine-learning approaches have been increasingly incorporated into these detection systems to complement traditional signature-based detection. These two approaches can be combined to make accurate and agile detection: signature-based detection offers fast and lightweight filtering of most known attacks, while machine-learning approaches can process the remaining (unfiltered) samples and identify new (or less well-known) attacks.\nThe quest of image spam. A recent example of the above arms race is image spam (see, e.g., [10]). In 2006, to evade the textual-based spam filters, spammers began rendering their messages into images included as attachments, thus producing \u201cimage-based spam,\u201d or image spam for short. Due to the massive volume of image spam sent in 2006 and 2007, researchers and spam-filter designers proposed several different countermeasures. Initially, suspect images were analyzed by OCR tools to extract text for standard spam detection, and then signatures were generated to block the (known) spam images. However, spammers immediately reacted by randomly obfuscating images with adversarial noise, both to make OCR-based detection ineffective, and to evade signature-based detection. The research community responded with (fast) approaches mainly based on machine-learning techniques using visual features extracted from images, which could accurately discriminate between spam images and legitimate ones (e.g., photographs, plots, etc.). Although image spam volumes have since declined, the exact cause for this decrease is debatable\u2014these countermeasures may have played a role, but the image spam were also more costly to the spammer as they required more time to generate and more bandwidth to deliver, thus limiting the spammers\u2019 ability to send a high volume of messages. Nevertheless, had this arms race continued, spammers could have attempted to evade the countermeasures by mimicking the feature values exhibited by legitimate images, which would have, in fact, forced spammers to increase the number of colors and elements in their spam images thus further increasing the size of such files, and the cost of sending them.\nMisuse and anomaly detection in computer networks. Another example of the above arms race can be found in network intrusion detection, where misuse detection has been gradually augmented by anomaly detection. The former approach relies on detecting attacks on the basis of signatures extracted from (known) intrusive network traffic, while the latter is based upon a statistical model of the normal profile of the network traffic and detects anomalous traffic that deviates from the assumed model of normality. This model is often constructed using machine-learning techniques, such as one-class classifiers (e.g., one-class SVMs), or, more generally, using density estimators. The underlying assumption of anomaly-detection-based intrusion detection, though, is that all anomalous network traffic is, in fact, intrusive. Although intrusive traffic often does exhibit anomalous behavior, the opposite is not necessarily true: some non-intrusive network traffic may also behave anomalously. Thus, accurate anomaly detectors often suffer from high false-alarm rates.\nSecurity Evaluation of SVMs in Adversarial Environments 7"}, {"heading": "2.3 Adversarial Machine Learning", "text": "As witnessed by the above examples, the introduction of machine-learning techniques in security-sensitive tasks has many beneficial aspects, and it has been somewhat necessitated by the increased sophistication and variability of recent attacks and zero-day exploits. However, there is good reason to believe that machinelearning techniques themselves will be subject to carefully designed attacks in the near future, as a logical next step in the above-sketched arms race. Since machinelearning techniques were not originally designed to withstand manipulations made by intelligent and adaptive adversaries, it would be reckless to naively trust these learners in a secure system. Instead, one needs to carefully consider whether these techniques can introduce novel vulnerabilities that may degrade the overall system\u2019s security, or whether they can be safely adopted. In other words, we need to address the question raised by Barreno et al. [5]: can machine learning be secure?\nAt the center of this question is the effect an adversary can have on a learner by violating the stationarity assumption that the training data used to train the classifier comes from the same distribution as the test data that will be classified by the learned classifier. This is a conventional and natural assumption underlying much of machine learning and is the basis for performance-evaluation-based techniques like cross-validation and bootstrapping as well as for principles like empirical risk minimization (ERM). However, in security-sensitive settings, the adversary may purposely manipulate data to mislead learning. Accordingly, the data distribution is subject to change, thereby potentially violating non-stationarity, albeit, in a limited way subject to the adversary\u2019s assumed capabilities (as we discuss in Section 3.1.3). Further, as in most security tasks, predicting how the data distribution will change is difficult, if not impossible [12, 36]. Hence, adversarial learning problems are often addressed as a proactive arms race [12], in which the classifier designer tries to anticipate the next adversary\u2019s move, by simulating and hypothesizing proper attack scenarios, as discussed in the next section."}, {"heading": "2.3.1 Reactive and Proactive Arms Races", "text": "As mentioned in the previous sections, and highlighted by the examples in Section 2.2, security problems are often cast as a long-lasting reactive arms race between the classifier designer and the adversary, in which each player attempts to achieve his/her goal by reacting to the changing behavior of his/her opponent. For instance, the adversary typically crafts samples to evade detection (e.g., a spammer\u2019s goal is often to create spam emails that will not be detected), while the classifier designer seeks to develop a system that accurately detects most malicious samples while maintaining a very low false-alarm rate; i.e., by not falsely identifying legitimate examples. Under this setting, the arms race can be modeled as the following cycle [12]. First, the adversary analyzes the existing learning algorithm and manipulates her data to evade detection (or more generally, to make the learning algorithm ineffective). For instance, a spammer may gather some knowledge of the words used\n8 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\nby the targeted spam filter to block spam and then manipulate the textual content of her spam emails accordingly; e.g., words like \u201ccheap\u201d that are indicative of spam can be misspelled as \u201cche4p\u201d. Second, the classifier designer reacts by analyzing the novel attack samples and updating his classifier. This is typically done by retraining the classifier on the newly collected samples, and/or by adding features that can better detect the novel attacks. In the previous spam example, this amounts to retraining the filter on the newly collected spam and, thus, to adding novel words into the filter\u2019s dictionary (e.g., \u201cche4p\u201d may be now learned as a spammy word). This reactive arms race continues in perpetuity as illustrated in Figure 1.\nHowever, reactive approaches to this arms race do not anticipate the next generation of security vulnerabilities and thus, the system potentially remains vulnerable to new attacks. Instead, computer security guidelines traditionally advocate a proactive approach2\u2014the classifier designer should proactively anticipate the adversary\u2019s strategy by (i) identifying the most relevant threats, (ii) designing proper countermeasures into his classifier, and (iii) repeating this process for his new design before deploying the classifier. This can be accomplished by modeling the adversary (based on knowledge of the adversary\u2019s goals and capabilities) and using this model to simulate attacks, as is depicted in Figure 2 to contrast the reactive arms race. While such an approach does not account for unknown or changing aspects of the adversary, it can indeed lead to an improved level of security by delaying each step of the reactive arms race because it should reasonably force the adversary to exert greater effort (in terms of time, skills, and resources) to find new vulnerabilities. Accordingly, proactively designed classifiers should remain useful for a longer time, with less frequent supervision or human intervention and with less severe vulnerabilities.\nAlthough this approach has been implicitly followed in most of the previous work (see Section 2.3.2), it has only recently been formalized within a more general framework for the empirical evaluation of a classifier\u2019s security [12], which we summarize in Section 3. Finally, although security evaluation may suggest specific countermeasures, designing general-purpose secure classifiers remains an open problem.\n2 Although in certain abstract models we have shown how regret-minimizing online learning can be used to define reactive approaches that are competitive with proactive security [6].\nSecurity Evaluation of SVMs in Adversarial Environments 9"}, {"heading": "2.3.2 Previous Work on Security Evaluation", "text": "Previous work in adversarial learning can be categorized according to the two main steps of the proactive arms race described in the previous section. The first research direction focuses on identifying potential vulnerabilities of learning algorithms and assessing the impact of the corresponding attacks on the targeted classifier; e.g., [4, 5, 18, 36, 40, 41, 42, 46]. The second explores the development of proper countermeasures and learning algorithms robust to known attacks; e.g., [26, 41, 57].\nAlthough some prior work does address aspects of the empirical evaluation of classifier security, which is often implicitly defined as the performance degradation incurred under a (simulated) attack, to our knowledge a systematic treatment of this process under a unifying perspective was only first described in our recent work [12]. Previously, security evaluation is generally conducted within a specific application domain such as spam filtering and network intrusion detection (e.g., in [26, 31, 41, 47, 66]), in which a different application-dependent criteria is separately defined for each endeavor. Security evaluation is then implicitly undertaken by defining an attack and assessing its impact on the given classifier. For instance, in [31], the authors showed how camouflage network packets can mimic legitimate traffic to evade detection; and, similarly, in [26, 41, 47, 66], the content of spam emails was manipulated for evasion. Although such analyses provide indispensable insights into specific problems, their results are difficult to generalize to other domains and provide little guidance for evaluating classifier security in a different application. Thus, in a new application domain, security evaluation often must begin anew and it is difficult to directly compare with prior studies. This shortcoming highlights the need for a more general set of security guidelines and a more systematic definition of classifier security evaluation, that we began to address in [12].\nApart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53]. These models frame the secure learning problem and provide a foundation for a proper security evaluation scheme. In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12]. Below we summarize these foundations.\n10 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli"}, {"heading": "2.3.3 A Taxonomy of Potential Attacks against Machine Learning Algorithms", "text": "A taxonomy of potential attacks against pattern classifiers was proposed in [4, 5, 36] as a baseline to characterize attacks on learners. The taxonomy is based on three main features: the kind of influence of attacks on the classifier, the kind of security violation they cause, and the specificity of an attack. The attack\u2019s influence can be either causative, if it aims to undermine learning, or exploratory, if it targets the classification phase. Accordingly, a causative attack may manipulate both training and testing data, whereas an exploratory attack only affects testing data. Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66]. The security violation can be either an integrity violation, if it aims to gain unauthorized access to the system (i.e., to have malicious samples be misclassified as legitimate); an availability violation, if the goal is to generate a high number of errors (both false-negatives and false-positives) such that normal system operation is compromised (e.g., legitimate users are denied access to their resources); or a privacy violation, if it allows the adversary to obtain confidential information from the classifier (e.g., in biometric recognition, this may amount to recovering a protected biometric template of a system\u2019s client). Finally, the attack specificity refers to the samples that are affected by the attack. It ranges continuously from targeted attacks (e.g., if the goal of the attack is to have a specific spam email misclassified as legitimate) to indiscriminate attacks (e.g., if the goal is to have any spam email misclassified as legitimate).\nEach portion of the taxonomy specifies a different type of attack as laid out in Barreno et al. [4] and here we outline these with respect to a PDF malware detector. An example of a causative integrity attack is an attacker who wants to mislead the malware detector to falsely classify malicious PDFs as benign. The attacker could accomplish this goal by introducing benign PDFs with malicious features into the training set and the attack would be targeted if the features corresponded to a particular malware or otherwise an indiscriminate attack. Similarly, the attacker could cause a causative availability attack by injecting malware training examples that exhibited features common to benign messages; again, these would be targeted if the attacker wanted a particular set of benign PDFs to be misclassified. A causative privacy attack, however, would require both manipulation of the training and information obtained from the learned classifier. The attacker could inject malicious PDFs with features identifying a particular author and then subsequently test if other PDFs with those features were labeled as malicious; this observed behavior may leak private information about the authors of other PDFs in the training set.\nIn contrast to the causative attacks, exploratory attacks cannot manipulate the learner, but can still exploit the learning mechanism. An example of an exploratory integrity attack involves an attacker who crafts a malicious PDF for an existing malware detector. This attacker queries the detector with candidate PDFs to discover which attributes the detector uses to identify malware, thus, allowing her to re-design her PDF to avoid the detector. This example could be targeted to a single PDF exploit or indiscriminate if a set of possible exploits are considered. An exploratory privacy attack against the malware detector can be conducted in the\nSecurity Evaluation of SVMs in Adversarial Environments 11\nsame way as the causative privacy attack described above, but without first injecting PDFs into the training data. Simply by probing the malware detector with crafted PDFs, the attacker may divulge secrets from the detector. Finally, exploratory availability attacks are possible in some applications but are not currently considered to be of interest."}, {"heading": "3 A Framework for Security Evaluation", "text": "In Sections 2.3 and 2.3.1, we motivated the need for simulating a proactive arms race as a means for improving system security. We further argued that evaluating a classifier\u2019s security properties through simulations of different, potential attack scenarios is a crucial step in this arms race for identifying the most relevant vulnerabilities and for suggesting how to potentially counter them. Here, we summarize our recent work [12] that proposes a new framework for designing proactive secure classifiers by addressing the shortcomings of the reactive security cycle raised above. Namely, our approach allows one to empirically evaluate a classifier\u2019s security during its design phase by addressing the first three steps of the proactive arms race depicted in Figure 2: (i) identifying potential attack scenarios, (ii) devising the corresponding attacks, and (iii) systematically evaluating their impact. Although it may also suggest countermeasures to the hypothesized attacks, the final step of the proactive arms race remains unspecified as a unique design step that has to be addressed separately in an application-specific manner.\nUnder our proposed security evaluation process, the analyst must clearly scrutinize the classifier by considering different attack scenarios to investigate a set of distinct potential vulnerabilities. This amounts to performing a more systematic what-if analysis of classifier security [56]. This is an essential step in the design of security systems, as it not only allows the designer to identify the most important and relevant threats, but also it forces him/her to consciously decide whether the classifier can be reasonably deployed, after being made aware of the corresponding risks, or whether it is instead better to adopt additional countermeasure to mitigate the attack\u2019s impact before deploying the classifier.\nOur proposed framework builds on previous work and attempts to systematize and unify their views under a more coherent perspective. The framework defines how an analyst can conduct a security audit of a classifier, which we detail in the remainder of this section. First, in Section 3.1, we explain how an adversary model is constructed according to the adversary\u2019s anticipated goals, knowledge and capabilities. Based on this model, a simulation of the adversary can be conducted to find the corresponding optimal attack strategies and produce simulated attacks, as described in Section 3.1.4. These simulated attack samples are then used to evaluate the classifier by either adding them to the training or test data, in accordance with the adversary\u2019s capabilities from Section 3.1.3. We conclude this section by discussing how to exploit our framework in specific application domains in Section 3.2.\n12 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli"}, {"heading": "3.1 Modeling the Adversary", "text": "The proposed model of the adversary is based on specific assumptions about her goal, knowledge of the system, and capability to modify the underlying data distribution by manipulating individual samples. It allows the classifier designer to model the attacks identified in the attack taxonomy described as in Section 2.3.3 [4, 5, 36]. However, in our framework, one can also incorporate application-specific constraints into the definition of the adversary\u2019s capability. Therefore, it can be exploited to derive practical guidelines for developing optimal attack strategies and to guide the design of adversarially resilient classifiers."}, {"heading": "3.1.1 Adversary\u2019s Goal", "text": "According to the taxonomy presented first by Barreno et al. [5] and extended by Huang et al. [36], the adversary\u2019s goal should be defined based on the anticipated security violation, which might be an integrity, availability, or privacy violation (see Section 2.3.3), and also depending on the attack\u2019s specificity, which ranges from targeted to indiscriminate. Further, as suggested by Laskov and Kloft [42] and Kloft and Laskov [40], the adversary\u2019s goal should be defined in terms of an objective function that the adversary is willing to maximize. This allows for a formal characterization of the optimal attack strategy.\nFor instance, in an indiscriminate integrity attack, the adversary may aim to maximize the number of spam emails that evade detection, while minimally manipulating their content [26, 46, 53], whereas in an indiscriminate availability attack, the adversary may aim to maximize the number of classification errors, thereby causing a general denial-of-service due to an excess of false alarms [52, 14]."}, {"heading": "3.1.2 Adversary\u2019s Knowledge", "text": "The adversary\u2019s knowledge of the attacked system can be defined based on the main components involved in the design of a machine learning system, as described in [29] and depicted in Figure 3.\nAccording to the five design steps depicted in Figure 3, the adversary may have various degrees of knowledge (ranging from no information to complete information) pertaining to the following five components:\n(k.i) the training set (or part of it); (k.ii) the feature representation of each sample; i.e., how real objects (emails, net-\nwork packets, etc.) are mapped into the feature space; (k.iii) the learning algorithm and its decision function; e.g., that logistic regression is used to learn a linear classifier; (k.iv) the learned classifier\u2019s parameters; e.g., the actual learned weights of a linear\nclassifier;\nSecurity Evaluation of SVMs in Adversarial Environments 13\nAdversary model Knowledge\n1.\u202f Training samples 2.\u202f Features 3.\u202f Learning algorithm and kind of decision function 4.\u202f Decision function and its parameters (trained classifier) 5.\u202f Feedback from the classifier\n(k.v) feedback from the deployed classifier; e.g., the classification labels assigned to some of the samples by the targeted classifier.\nThese five elements represent different levels of knowledge about the system being attacked. A typical hypothesized scenario assumes that the adversary has perfect knowledge of the targeted classifier (k.iv). Although potentially too pessimistic, this worst-case setting allows one to compute a lower bound on the classifier performance when it is under attack [26, 41]. A more realistic setting is that the adversary knows the (untrained) learning algorithm (k.iii), and she may exploit feedback from the classifier on the labels assigned to some query samples (k.v), either to directly find optimal or nearly-optimal attack instances [46, 53], or to learn a surrogate classifier, which can then serve as a template to guide the attack against the actual classifier. We refer to this scenario as a limited knowledge setting in Section 4.\nNote that one may also make more restrictive assumptions on the adversary\u2019s knowledge, such as considering partial knowledge of the feature representation (k.ii), or a complete lack of knowledge of the learning algorithm (k.iii). Investigating classifier security against these uninformed adversaries may yield a higher level of security. However, such assumptions would be contingent on security through obscurity; that is, the provided security would rely upon secrets that must be kept unknown to the adversary even though such a high level of secrecy may not be practical. Reliance on unjustified secrets can potentially lead to catastrophic unforeseen vulnerabilities. Thus, this paradigm should be regarded as being complementary to security by design, which instead advocates that systems should be designed from the ground-up to be secure and, if secrets are assumed, they must be well-justified. Accordingly, security is often investigated by assuming that the adversary knows at least the learning algorithm and the underlying feature representation."}, {"heading": "3.1.3 Adversary\u2019s Capability", "text": "We now give some guidelines on how the attacker may be able to manipulate samples and the corresponding data distribution. As discussed in Section 2.3.3 [4, 5, 36], the adversary may control both training and test data (causative attacks), or only on test data (exploratory attacks). Further, training and test data may follow different\n14 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\ndistributions, since they can be manipulated according to different attack strategies by the adversary. Therefore, we should specify:\n(c.i) whether the adversary can manipulate training (TR) and/or testing (TS) data; i.e., the attack influence from the taxonomy in [4, 5, 36]);\n(c.ii) whether and to what extent the attack affects the class priors, for TR and TS; (c.iii) which and how many samples can be modified in each class, for TR and TS; (c.iv) which features of each attack sample can be modified and how can these\nfeatures\u2019 values be altered; e.g., correlated feature values can not be modified independently. Assuming a generative model p(X,Y ) = p(Y )p(X|Y ) (where we use ptr and pts for training and test distributions, respectively), assumption (c.ii) specifies how an attack can modify the priors ptr(Y ) and pts(Y ) while assumptions (c.iii) and (c.iv) specifies how it can alter the class-conditional distributions ptr(X|Y ) and pts(X|Y ).\nTo perform security evaluation according to the hypothesized attack scenario, it is thus clear that the collected data and generated attack samples should be resampled according to the above distributions to produce suitable training and test set pairs. This can be accomplished through existing resampling algorithms like cross-validation or bootstrapping, when the attack samples are independently sampled from an identical distribution (i.i.d.). Otherwise, one may consider different sampling schemes. For instance, in Biggio et al. [14] the attack samples had to be injected into the training data, and each attack sample depended on the current training data, which also included past attack samples. In this case, it was sufficient to add one attack sample at a time, until the desired number of samples was reached.3"}, {"heading": "3.1.4 Attack Strategy", "text": "Once specific assumptions on the adversary\u2019s goal, knowledge, and capability are made, one can compute the optimal attack strategy corresponding to the hypothesized attack scenario; i.e., the adversary model. This amounts to solving the optimization problem defined according to the adversary\u2019s goal, under proper constraints defined in accordance with the adversary\u2019s assumed knowledge and capabilities. The attack strategy can then be used to produce the desired attack samples, which then have to be merged consistently to the rest of the data to produce suitable training and test sets for the desired security evaluation, as explained in the previous section. Specific examples of how to derive optimal attacks against SVMs, and how to resample training and test data to properly include them are discussed in Sections 4 and 5.\n3 See [12] for more details on the definition of the data distribution and the resampling algorithm.\nSecurity Evaluation of SVMs in Adversarial Environments 15"}, {"heading": "3.2 How to use our Framework", "text": "We summarize here the steps that can be followed to correctly use our framework in specific application scenarios: 1. hypothesize an attack scenario by identifying a proper adversary\u2019s goal, and\naccording to the taxonomy in [4, 5, 36]; 2. define the adversary\u2019s knowledge according to (k.i-v), and capabilities accord-\ning to (c.i-iv); 3. formulate the corresponding optimization problem and devise the correspond-\ning attack strategy; 4. resample the collected (training and test) data accordingly; 5. evaluate classifier\u2019s security on the resampled data (including attack samples); 6. repeat the evaluation for different levels of adversary\u2019s knowledge and/or capa-\nbilities, if necessary; or hypothesize a different attack scenario. In the next sections we show how our framework can be applied to investigate three security threats to SVMs: evasion, poisoning, and privacy violations. We then discuss how our findings may be used to improve the security of such classifiers to the considered attacks. For instance, we show how careful kernel parameter selection, which trades off between security to attacks and classification accuracy, may complicate the adversary\u2019s task of subverting the learning process."}, {"heading": "4 Evasion Attacks against SVMs", "text": "In this section, we consider the problem of SVM evasion at test time; i.e., how to optimally manipulate samples at test time to avoid detection. The problem of evasion at test time has been considered in previous work, albeit either limited to simple decision functions such as linear classifiers [26, 46], or to cover any convexinducing classifiers [53] that partition the feature space into two sets, one of which is convex, but do not include most interesting families of non-linear classifiers such as neural nets or SVMs. In contrast to this prior work, the methods presented in our recent work [8] and in this section demonstrate that evasion of kernel-based classifiers at test time can be realized with a straightforward gradient-descent-based approach derived from Golland\u2019s technique of discriminative directions [33]. As a further simplification of the attacker\u2019s effort, we empirically show that, even if the adversary does not precisely know the classifier\u2019s decision function, she can learn a surrogate classifier on a surrogate dataset and reliably evade the targeted classifier.\nThis section is structured as follows. In Section 4.1, we define the model of the adversary, including her attack strategy, according to our evaluation framework described in Section 3.1. Then, in Section 4.2 we derive the attack strategy that will be employed to experimentally evaluate evasion attacks against SVMs. We report our experimental results in Section 4.3. Finally, we critically discuss and interpret our research findings in Section 4.4.\n16 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli"}, {"heading": "4.1 Modeling the Adversary", "text": "We show here how our framework can be applied to evaluate the security of SVMs against evasion attacks. We first introduce our notation, state our assumptions about attack scenario, and then derive the corresponding optimal attack strategy.\nNotation. We consider a classification algorithm f : X 7\u2192 Y that assigns samples represented in some feature space x \u2208X to a label in the set of predefined classes y \u2208 Y = {\u22121,+1}, where \u22121 (+1) represents the legitimate (malicious) class. The label fx = f (x) given by a classifier is typically obtained by thresholding a continuous discriminant function g : X 7\u2192 R. Without loss of generality, we assume that f (x) =\u22121 if g(x)< 0, and +1 otherwise. Further, note that we use fx to refer to a label assigned by the classifier for the point x (rather than the true label y of that point) and the shorthand fi for the label assigned to the ith training point, xi."}, {"heading": "4.1.1 Adversary\u2019s Goal", "text": "Malicious (positive) samples are manipulated to evade the classifier. The adversary may be satisfied when a sample x is found such that g(x) < \u2212\u03b5 where \u03b5 > 0 is a small constant. However, as mentioned in Section 3.1.1, these attacks may be easily defeated by simply adjusting the decision threshold to a slightly more conservative value (e.g., to attain a lower false negative rate at the expense of a higher false positive rate). For this reason, we assume a smarter adversary, whose goal is to have her attack sample misclassified as legitimate with the largest confidence. Analytically, this statement can be expressed as follows: find an attack sample x that minimizes the value of the classifier\u2019s discriminant function g(x). Indeed, this adversarial setting provides a worst-case bound for the targeted classifier."}, {"heading": "4.1.2 Adversary\u2019s Knowledge", "text": "We investigate two adversarial settings. In the first, the adversary has perfect knowledge (PK) of the targeted classifier; i.e., she knows the feature space (k.ii) and function g(x) (k.iii-iv). Thus, the labels from the targeted classifier (k.v) are not needed. In the second, the adversary is assumed to have limited knowledge (LK) of the classifier. We assume she knows the feature representation (k.ii) and the learning algorithm (k.iii), but that she does not know the learned classifier g(x) (k.iv). In both cases, we assume the attacker does not have knowledge of the training set (k.i).\nWithin the LK scenario, the adversary does not know the true discriminant function g(x) but may approximate it as g\u0302(x) by learning a surrogate classifier on a surrogate training set {(xi,yi)} nq i=1 of nq samples. This data may be collected by the adversary in several ways; e.g., she may sniff network traffic or collect legitimate and spam emails from an alternate source. Thus, for LK, there are two sub-cases related to assumption (k.v), which depend on whether the adversary can query the classifier. If so, the adversary can build the training set by submitting a set of nq\nSecurity Evaluation of SVMs in Adversarial Environments 17\nqueries xi to the targeted classifier to obtain their classification labels, yi = f (xi). This is indeed the adversary\u2019s true learning task, but it requires her to have access to classifier feedback; e.g., by having an email account protected by the targeted filter (for public email providers, the adversary can reasonably obtain such accounts). If not, the adversary may use the true class labels for the surrogate data, although this may not correctly approximate the targeted classifier (unless it is very accurate)."}, {"heading": "4.1.3 Adversary\u2019s Capability", "text": "In the evasion setting, the adversary can only manipulate testing data (c.i); i.e., she has no way to influence training data. We further assume here that the class priors can not be modified (c.ii), and that all the malicious testing samples are affected by the attack (c.iii). In other words, we are interested in simulating an exploratory, indiscriminate attack. The adversary\u2019s capability of manipulating the features of each sample (c.iv) should be defined based on application-specific constraints. However, at a more general level we can bound the attack point to lie within some maximum distance from the original attack sample, dmax, which then is a parameter of our evaluation. Similarly to previous work, the definition of a suitable distance measure d : X \u00d7X 7\u2192R is left to the specific application domain [26, 46, 53]. Note indeed that this distance should reflect the adversary\u2019s effort or cost in manipulating samples, by considering factors that can limit the overall attack impact; e.g., the increase in the file size of a malicious PDF, since larger files will lower the infection rate due to increased transmission times. For spam filtering, distance is often given as the number of modified words in each spam [26, 46, 52, 53], since it is assumed that highly modified spam messages are less effectively able to convey the spammer\u2019s message."}, {"heading": "4.1.4 Attack Strategy", "text": "Under the attacker\u2019s model described in Sections 4.1.1, 4.1.2 and 4.1.3, for any target malicious sample x0 (the adversary\u2019s true objective), an optimal attack strategy finds a sample x\u2217 to minimize g or its estimate g\u0302, subject to a bound on its modification distance from x0:\nx\u2217 = argmin x g\u0302(x) s.t. d(x,x0)\u2264 dmax .\nFor several classifiers, minimizing g(x) is equivalent to maximizing the estimated posterior p( fx = \u22121|x); e.g., for neural networks, since they directly output a posterior estimate, and for SVMs, since their posterior can be estimated as a sigmoidal function of the distance of x to the SVM hyperplane [55].\nGenerally, this is a non-linear optimization, which one may optimize with many well-known techniques (e.g., gradient descent, Newton\u2019s method, or BFGS) and below we use a gradient descent procedure. However, if g\u0302(x) is not convex, descent\n18 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\napproaches may not find a global optima. Instead, the descent path may lead to a flat region (local minimum) outside of the samples\u2019 support where p(x)\u2248 0 and the classification behavior of g is unspecified and may stymie evasion attempts (see the upper left plot in Figure 4).\nUnfortunately, our objective does not utilize the evidence we have about the distribution of data p(x), and thus gradient descent may meander into unsupported regions (p(x) \u2248 0) where g is relatively unspecified. This problem is further compounded since our estimate g\u0302 is based on a finite (and possibly small) training set making it a poor estimate of g in unsupported regions, which may lead to false evasion points in these regions. To overcome these limitations, we introduce an additional component into the formulation of our attack objective, which estimates p(x| fx = \u22121) using density-estimation techniques. This second component acts as a penalizer for x in low density regions and is weighted by a parameter \u03bb \u2265 0 yielding the following modified optimization problem:\nargmin x E(x) = g\u0302(x)\u2212 \u03bb n \u2211i| fi=\u22121\nk ( x\u2212xi\nh\n) (1)\ns.t. d(x,x0)\u2264 dmax ,\nSecurity Evaluation of SVMs in Adversarial Environments 19\nwhere h is a bandwidth parameter for a kernel density estimator (KDE), and n is the number of benign samples ( fx = \u22121) available to the adversary. This alternate objective trades off between minimizing g\u0302(x) (or p( fx = \u22121|x)) and maximizing the estimated density p(x| fx = \u22121). The extra component favors attack points to imitate features of known samples classified as legitimate, as in mimicry attacks [31]. In doing so, it reshapes the objective function and thereby biases the resulting densityaugmented gradient descent towards regions where the negative class is concentrated (see the bottom right plot in Figure 4).\nFinally, note that this behavior may lead our technique to disregard attack patterns within unsupported regions (p(x)\u2248 0) for which g(x)< 0, when they do exist (see, e.g., the upper right plot in Figure 4). This may limit classifier evasion especially when the constraint d(x,x0) \u2264 dmax is particularly strict. Therefore, the trade-off between the two components of the objective function should be carefully considered."}, {"heading": "4.2 Evasion Attack Algorithm", "text": "Algorithm 1 details a gradient-descent method for optimizing problem of Equation (1). It iteratively modifies the attack point x in the feature space as x\u2032\u2190 x\u2212t\u2207E, where \u2207E is a unit vector aligned with the gradient of our objective function, and t is the step size. We assume g to be differentiable almost everywhere (subgradients may be used at discontinuities). When g is non-differentiable or is not smooth enough for a gradient descent to work well, it is also possible to rely upon the mimicry / KDE term in the optimization of Equation (1).\nAlgorithm 1 Gradient-descent attack procedure Input: the initial attack point, x0; the step size, t; the trade-off parameter, \u03bb ; and \u03b5 > 0. Output: x\u2217, the final attack point.\n1: k\u2190 0. 2: repeat 3: k\u2190 k+1 4: Set \u2207E(xk\u22121) to a unit vector aligned with \u2207g(xk\u22121)\u2212\u03bb\u2207p(xk\u22121| fx =\u22121). 5: xk\u2190 xk\u22121\u2212 t\u2207E(xk\u22121) 6: if d(xk,x0)> dmax then 7: Project xk onto the boundary of the feasible region (enforcing application-specific constraints, if any). 8: end if 9: until E ( xk ) \u2212E ( xk\u22121 ) < \u03b5\n10: return: x\u2217 = xk\nIn the next sections, we show how to compute the components of \u2207E; namely, the gradient of the discriminant function g(x) of SVMs for different kernels, and the\n20 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\ngradient of the mimicking component (density estimation). We finally discuss how to project the gradient \u2207E onto the feasible region in discrete feature spaces."}, {"heading": "4.2.1 Gradient of Support Vector Machines", "text": "For SVMs, g(x) = \u2211i \u03b1iyik(x,xi) + b. The gradient is thus given by \u2207g(x) = \u2211i \u03b1iyi\u2207k(x,xi). Accordingly, the feasibility of the approach depends on the computability of this kernel gradient \u2207k(x,xi), which is computable for many numeric kernels. In the following, we report the kernel gradients for three main cases: (a) the linear kernel, (b) the RBF kernel, and (c) the polynomial kernel.\n(a) Linear kernel. In this case, the kernel is simply given by k(x,xi) = \u3008x,xi\u3009. Accordingly, \u2207k(x,xi) = xi (we remind the reader that the gradient has to be computed with respect to the current attack sample x), and \u2207g(x) = w = \u2211i \u03b1iyixi.\n(b) RBF kernel. For this kernel, k(x,xi) = exp{\u2212\u03b3\u2016x\u2212 xi\u20162}. The gradient is thus given by \u2207k(x,xi) =\u22122\u03b3 exp{\u2212\u03b3\u2016x\u2212xi\u20162}(x\u2212xi).\n(c) Polynomial kernel. In this final case, k(x,xi) = (\u3008x,xi\u3009+ c)p. The gradient is thus given by \u2207k(x,xi) = p(\u3008x,xi\u3009+ c)p\u22121xi."}, {"heading": "4.2.2 Gradients of Kernel Density Estimators", "text": "As with SVMs, the gradient of kernel density estimators depends on the gradient of its kernel. We considered generalized RBF kernels of the form\nk ( x\u2212xi\nh\n) = exp ( \u2212 d(x,xi)h ) ,\nwhere d(\u00b7, \u00b7) is any suitable distance function. We used here the same distance d(\u00b7, \u00b7) used in Equation (1), but they can be different, in general. For `2- and `1-norms (i.e., RBF and Laplacian kernels), the KDE (sub)gradients are respectively given by:\n\u2212 2 nh \u2211i| fi=\u22121\nexp ( \u2212\u2016x\u2212xi\u2016 2 2\nh\n) (x\u2212xi) ,\n\u2212 1 nh \u2211i| fi=\u22121\nexp ( \u2212\u2016x\u2212xi\u20161\nh\n) (x\u2212xi) .\nNote that the scaling factor here is proportional to O( 1nh ). Therefore, to influence gradient descent with a significant mimicking effect, the value of \u03bb in the objective function should be chosen such that the value of \u03bbnh is comparable to (or higher than) the range of the discriminant function g\u0302(x).\nSecurity Evaluation of SVMs in Adversarial Environments 21"}, {"heading": "4.2.3 Gradient Descent Attack in Discrete Spaces", "text": "In discrete spaces, gradient approaches may lead to a path through infeasible portions of the feature space. In such cases, we need to find feasible neighbors x that yield a steepest descent; i.e., maximally decreasing E(x). A simple approach to this problem is to probe E at every point in a small neighborhood of x: x\u2032 \u2190 argminz\u2208N (x) E(z). However, this approach requires a large number of queries. For classifiers with a differentiable decision function, we can instead use the neighbor whose difference from x best aligns with \u2207E(x); i.e., the update becomes\nx\u2032\u2190 arg max z\u2208N (x) (z\u2212x) \u2016z\u2212x\u2016\n> \u2207E(x) .\nThus, the solution to the above alignment is simply to modify a feature that satisfies argmaxi |\u2207E(x)i| for which the corresponding change leads to a feasible state. Note however that, sometimes, such a step may be relatively quite large, and may lead the attack out of a local minimum potentially increasing the objective function. Therefore, one should consider the best alignment that effectively reduces the objective function by disregarding features that lead to states where the objective function is higher."}, {"heading": "4.3 Experiments", "text": "In this section, we first report some experimental results on the MNIST handwritten digit classification task [32, 45], that visually demonstrate how the proposed algorithm modifies digits to mislead classification. This dataset is particularly useful because the visual nature of the handwritten digit data provides a semantic meaning for attacks. We then show the effectiveness of the proposed attack on a more realistic and practical scenario: the detection of malware in PDF files."}, {"heading": "4.3.1 Handwritten Digits", "text": "We first focus on a two-class sub-problem of discriminating between two distinct digits from the MNIST dataset [45]. Each digit example is represented as a grayscale image of 28\u00d7 28 pixels arranged in raster-scan-order to give feature vectors of d = 28\u00d7 28 = 784 values. We normalized each feature (pixel) x f \u2208 [0,1]d by dividing its value by 255, and we constrained the attack samples to this range. Accordingly, we optimized Equation (1) subject to 0\u2264 x f \u2264 1 for all f .\nFor our attacker, we assume the perfect knowledge (PK) attack scenario. We used the Manhattan distance (`1-norm) as the distance function, d, both for the kernel density estimator (i.e., a Laplacian kernel) and for the constraint d(x,x0)\u2264 dmax of Equation (1), which bounds the total difference between the gray level values of\n22 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\nthe original image x0 and the attack image x. We used an upper bound of dmax = 5000 255 to limit the total change in the gray-level values to 5000. At each iteration, we increased the `1-norm value of x\u2212x0 by 10255 , which is equivalent to increasing the difference in the gray level values by 10. This is effectively the gradient step size.\nFor the digit discrimination task, we applied an SVM with the linear kernel and C = 1. We randomly chose 100 training samples and applied the attacks to a correctly-classified positive sample.\nIn Figure 5 we illustrate gradient attacks in which a \u201c3\u201d is to be misclassified as a \u201c7\u201d. The left image shows the initial attack point, the middle image shows the first attack image misclassified as legitimate, and the right image shows the attack point after 500 iterations. When \u03bb = 0, the attack images exhibit only a weak resemblance to the target class \u201c7\u201d but are, nevertheless, reliably misclassified. This is the same effect we observed in the left plot of Figure 4: the classifier is evaded by making the attack sample dissimilar to the malicious class. Conversely, when \u03bb = 10 the attack images strongly resemble the target class because the mimicry term favors samples that are more similar to the target examples. This is the same effect illustrated in the rightmost plot of Figure 4.\nSecurity Evaluation of SVMs in Adversarial Environments 23"}, {"heading": "4.3.2 Malware Detection in PDF Files", "text": "We focus now on the problem of discriminating between legitimate and malicious PDF files, a popular medium for disseminating malware [67]. PDF files are excellent vectors for malicious-code, due to their flexible logical structure, which can described by a hierarchy of interconnected objects. As a result, an attack can be easily hidden in a PDF to circumvent file-type filtering. The PDF format further allows a wide variety of resources to be embedded in the document including JavaScript, Flash, and even binary programs. The type of the embedded object is specified by keywords, and its content is in a data stream. Several recent works proposed machine-learning techniques for detecting malicious PDFs use the file\u2019s logical structure to accurately identify the malware [49, 62, 63]. In this case study, we use the feature representation of Maiorca et al. [49] in which each feature corresponds to the tally of occurrences of a given keyword in the PDF file. Similar feature representations were also exploited in [62, 63].\nThe PDF application imposes natural constraints on attacks. Although it is difficult to remove an embedded object (and its corresponding keywords) without corrupting the PDF\u2019s file structure, it is rather easy to insert new objects (and, thus, keywords) through the addition of a new version to the PDF file [1]. In our feature representation, this is equivalent to allowing only feature increments,;i.e., requiring x0\u2264 x as an additional constraint in the optimization problem given by Equation (1). Further, the total difference in keyword counts between two samples is their Manhattan distance, which we again use for the kernel density estimator and the constraint in Equation (1). Accordingly, dmax is the maximum number of additional keywords that an attacker can add to the original x0.\nExperimental setup. For experiments, we used a PDF corpus with 500 malicious samples from the Contagio dataset4 and 500 benign samples collected from the web. We randomly split the data into five pairs of training and testing sets with 500 samples each to average the final results. The features (keywords) were extracted from each training set as described in [49]; on average, 100 keywords were found in each run. Further, we also bounded the maximum value of each feature (keyword count) to 100, as this value was found to be close to the 95th percentile for each feature. This limited the influence of outlying samples having very high feature values.\nWe simulated the perfect knowledge (PK) and the limited knowledge (LK) scenarios described in Section 4.1.2. In the LK case, we set the number of samples used to learn the surrogate classifier to nq = 100. The reason is to demonstrate that even with a dataset as small as the 20% of the original training set size, the adversary may be able to evade the targeted classifier with high reliability. Further, we assumed that the adversary uses feedback from the targeted classifier f ; i.e., the labels y\u0302i = fi = f (xi) for each surrogate sample xi. Similar results were also obtained using the true labels (without relabeling), since the targeted classifiers correctly classified almost all samples in the test set.\n4 http://contagiodump.blogspot.it\n24 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\nAs discussed in Section 4.2.2, the value of \u03bb is chosen according to the scale of the discriminant function g(x), the bandwidth parameter h of the kernel density estimator, and the number of samples n labeled as legitimate in the surrogate training set. For computational reasons, to estimate the value of the KDE at a given point x in the feature space, we only consider the 50 nearest (legitimate) training samples to x; therefore n \u2264 50 in our case. The bandwidth parameter was set to h = 10, as this value provided a proper rescaling of the Manhattan distances observed in our dataset for the KDE. We thus set \u03bb = 500 to be comparable with O(nh).\nFor each targeted classifier and training/testing pair, we learned five different surrogate classifiers by randomly selecting nq samples from the test set, and averaged their results. For SVMs, we sought a surrogate classifier that would correctly match the labels from the targeted classifier; thus, we used parameters C = 100, and \u03b3 = 0.1 (for the RBF kernel) to heavily penalize training errors.\nExperimental results. We report our results in Figure 6, in terms of the false negative (FN) rate attained by the targeted classifiers as a function of the maximum allowable number of modifications, dmax \u2208 [0,50]. We compute the FN rate corresponding to a fixed false positive (FP) rate of FP= 0.5%. For dmax = 0, the FN rate corresponds to a standard performance evaluation using unmodified PDFs. As expected, the FN rate increases with dmax as the PDF is increasingly modified, since the adversary has more flexibility in his attack. Accordingly, a more secure classifier will exhibit a more graceful increase of the FN rate. Results for \u03bb = 0. We first investigate the effect of the proposed attack in the PK case, without considering the mimicry component (Figure 6, top row), for varying parameters of the considered classifiers. The linear SVM (Figure 6, top-left plot) is almost always evaded with as few as 5 to 10 modifications, independent of the regularization parameter C. It is worth noting that attacking a linear classifier amounts to always incrementing the value of the same highest-weighted feature (corresponding to the /Linearized keyword in the majority of the cases) until it is bounded. This continues with the next highest-weighted non-bounded feature until termination. This occurs simply because the gradient of g(x) does not depend on x for a linear classifier (see Section 4.2.1). With the RBF kernel (Figure 6, top-right plot), SVMs exhibit a similar behavior with C = 1 and various values of its \u03b3 parameter,5 and the RBF SVM provides a higher degree of security compared to linear SVMs (cf. top-left plot and middle-left plot in Figure 6).\nIn the LK case, without mimicry (Figure 6, middle row), classifiers are evaded with a probability only slightly lower than that found in the PK case, even when only nq = 100 surrogate samples are used to learn the surrogate classifier. This aspect highlights the threat posed by a skilled adversary with incomplete knowledge: only a small set of samples may be required to successfully attack the target classifier using the proposed algorithm. Results for \u03bb = 500. When mimicry is used (Figure 6, bottom row), the success of the evasion of linear SVMs (with C = 1) decreases both in the PK (e.g., compare the\n5 We also conducted experiments using C = 0.1 and C = 100, but did not find significant differences compared to the presented results using C = 1.\nSecurity Evaluation of SVMs in Adversarial Environments 25\nblue curve in the top-left plot with the solid blue curve in the bottom-left plot) and LK case (e.g., compare the blue curve in the middle-left plot with the dashed blue curve in the bottom-left plot). The reason is that the computed direction tends to lead to a slower descent; i.e., a less direct path that often requires more modifications to evade the classifier. In the non-linear case (Figure 6, bottom-right plot), instead, mimicking exhibits some beneficial aspects for the attacker, although the constraint on feature addition may make it difficult to properly mimic legitimate samples. In particular, note how the targeted SVMs with RBF kernel (with C = 1 and \u03b3 = 1) in the PK case (e.g., compare the blue curve in the top-right plot with the solid blue\n26 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\ncurve in the bottom-right plot) is evaded with a significantly higher probability than in the case when \u03bb = 0. The reason is that a pure descent strategy on g(x) may find local minima (i.e., attack samples) that do not evade detection, while the mimicry component biases the descent towards regions of the feature space more densely populated by legitimate samples, where g(x) eventually attains lower values. In the LK case (e.g., compare the blue curve in the middle-right plot with the dashed blue curve in the bottom-right plot), however, mimicking does not exhibit significant improvements.\nAnalysis. Our attacks raise questions about the feasibility of detecting malicious PDFs solely based on logical structure. We found that /Linearized, /OpenAction, /Comment, /Root and /PageLayout were among the most commonly manipulated keywords. They indeed are found mainly in legitimate PDFs, but can be easily added to malicious PDFs by the versioning mechanism. The attacker can simply insert comments inside the malicious PDF file to augment its /Comment count. Similarly, she can embed legitimate OpenAction code to add /OpenAction keywords or she can add new pages to insert /PageLayout keywords.\nIn summary, our analysis shows that even detection systems that accurately classify non-malicious data can be significantly degraded with only a few malicious modifications. This aspect highlights the importance of developing detection systems that are accurate, but also designed to be robust against adversarial manipulation of attack instances."}, {"heading": "4.4 Discussion", "text": "In this section, we proposed a simple algorithm that allows for evasion of SVMs with differentiable kernels, and, more generally, of any classifier with a differentiable discriminant function. We investigated the attack effectiveness in the case of perfect knowledge of the attacked system. Further, we empirically showed that SVMs can still be evaded with high probability even if the adversary can only learn a classifier\u2019s copy on surrogate data (limited knowledge). We believe that the proposed attack formulation can easily be extended to classifiers with non-differentiable discriminant functions as well, such as decision trees and k-nearest neighbors.\nOur analysis also suggests some ideas for improving classifier security. In particular, when the classifier tightly encloses the legitimate samples, the adversary must increasingly mimic the legitimate class to evade (see Figure 4), and this may not always be possible; e.g., malicious network packets or PDF files still need to embed a valid exploit, and some features may be immutable. Accordingly, a guideline for designing secure classifiers is that learning should encourage a tight enclosure of the legitimate class; e.g., by using a regularizer that penalizes classifying \u201cblind spots\u201d\u2014regions with low p(x)\u2014as legitimate. Generative classifiers can be modified, by explicitly modeling the attack distribution, as in [11], and discriminative classifiers can be modified similarly by adding generated attack samples to the training set. However, these security improvements may incur higher FP rates.\nSecurity Evaluation of SVMs in Adversarial Environments 27\nIn the above applications, the feature representations were invertible; i.e., there is a direct mapping from the feature vectors x to a corresponding real-world sample (e.g., a spam email, or PDF file). However, some feature mappings can not be trivially inverted; e.g., n-gram analysis [31]. In these cases, one may modify the real-world object corresponding to the initial attack point at each step of the gradient descent to obtain a sample in the feature space that as close as possible to the sample that would be obtained at the next attack iteration. A similar technique has been already exploited in to address the pre-image problem of kernel methods [14].\nOther interesting extensions include (i) considering more effective strategies such as those proposed by [46, 53] to build a small but representative set of surrogate data to learn the surrogate classifier and (ii) improving the classifier estimate g\u0302(x); e.g.using an ensemble technique like bagging to average several classifiers [16]."}, {"heading": "5 Poisoning Attacks against SVMs", "text": "In the previous section, we devised a simple algorithm that allows for evasion of classifiers at test time and showed experimentally how it can be exploited to evade detection by SVMs and kernel-based classification techniques. Here we present another kind of attack, based on our work in [14]. Its goal is to force the attacked SVM to misclassify as many samples as possible at test time through poisoning of the training data, that is, by injecting well-crafted attack samples into the training set. Note that, in this case, the test data is assumed not to be manipulated by the attacker.\nPoisoning attacks are staged during classifier training, and they can thus target adaptive or online classifiers, as well as classifiers that are being re-trained on data collected during test time, especially if in an unsupervised or semi-supervised manner. Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58]. They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].\nIn this section, we follow the same structure of Section 4. In Section 5.1, we define the adversary model according to our framework; then, in Sections 5.1.4 and 5.2 we respectively derive the optimal poisoning attack and the corresponding algorithm; and, finally, in Sections 5.3 and 5.4 we report our experimental findings and discuss the results."}, {"heading": "5.1 Modeling the Adversary", "text": "Here, we apply our framework to evaluate security against poisoning attacks. As with the evasion attacks in Section 4.1, we model the attack scenario and derive the corresponding optimal attack strategy for poisoning.\n28 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\nNotation. In the following, we assume that an SVM has been trained on a dataset Dtr = {xi,yi}ni=1 with xi \u2208 Rd and yi \u2208 {\u22121,+1}. The matrix of kernel values between two sets of points is denoted with K, while Q = K \u25e6 yy> denotes its labelannotated version, and \u03b1 denotes the SVM\u2019s dual variables corresponding to each training point. Depending on the value of \u03b1i, the training points are referred to as margin support vectors (0 < \u03b1i < C, set S ), error support vectors (\u03b1i = C, set E ) or reserve vectors (\u03b1i = 0, set R). In the sequel, the lower-case letters s,e,r are used to index the corresponding parts of vectors or matrices; e.g., Qss denotes the sub-matrix of Q corresponding to the margin support vectors."}, {"heading": "5.1.1 Adversary\u2019s Goal", "text": "For a poisoning attack, the attacker\u2019s goal is to find a set of points whose addition to Dtr maximally decreases the SVM\u2019s classification accuracy. For simplicity, we start considering the addition of a single attack point (x\u2217,y\u2217). The choice of its label y\u2217 is arbitrary but fixed. We refer to the class of this chosen label as attacking class and the other as the attacked class."}, {"heading": "5.1.2 Adversary\u2019s Knowledge", "text": "According to Section 3.1.2, we assume that the adversary knows the training samples (k.i), the feature representation (k.ii), that an SVM learning algorithm is used (k.iii) and the learned SVM\u2019s parameters (k.iv), since they can be inferred by the adversary by solving the SVM learning problem on the known training set. Finally, we assume that no feedback is exploited by the adversary (k.v).\nThese assumptions amount to considering a worst-case analysis that allows us to compute the maximum error rate that the adversary can inflict through poisoning. This is indeed useful to check whether and under what circumstances poisoning may be a relevant threat for SVMs.\nAlthough having perfect knowledge of the training data is very difficult in practice for an adversary, collecting a surrogate dataset sampled from the same distribution may not be that complicated; for instance, in network intrusion detection an attacker may easily sniff network packets to build a surrogate learning model, which can then be poisoned under the perfect knowledge setting. The analysis of this limited knowledge poisoning scenario is however left to future work."}, {"heading": "5.1.3 Adversary\u2019s Capability", "text": "According to Section 3.1.3, we assume that the attacker can manipulate only training data (c.i), can manipulate the class prior and the class-conditional distribution of the attack point\u2019s class y\u2217 by essentially adding a number of attack points of that class into the training data, one at a time (c.ii-iii), and can alter the feature values of the\nSecurity Evaluation of SVMs in Adversarial Environments 29\nattack sample within some lower and upper bounds (c.iv). In particular, we will constrain the attack point to lie within a box, that is xlb \u2264 x\u2264 xub."}, {"heading": "5.1.4 Attack Strategy", "text": "Under the above assumptions, the optimal attack strategy amounts to solving the following optimization problem:\nx\u2217 = argmaxx P(x) = \u2211mk=1(1\u2212 yk fx(xk))+ = m\n\u2211 k=1 (\u2212gk)+ (2)\ns.t. xlb \u2264 x\u2264 xub , (3)\nwhere the hinge loss has to be maximized on a separate validation set Dval = {xk,yk}mk=1 to avoid considering a further regularization term in the objective function. The reason is that the attacker aims to maximize the SVM generalization error and not only its empirical estimate on the training data."}, {"heading": "5.2 Poisoning Attack Algorithm", "text": "In this section, we assume the role of the attacker and develop a method for optimizing x\u2217 according to Equation (2). Since the objective function is non-linear, we use a gradient-ascent algorithm, where the attack vector is initialized by cloning an arbitrary point from the attacked class and flipping its label. This initialized attack point (at iteration 0) is denoted by x0. In principle, x0 can be any point sufficiently deep within the attacking class\u2019s margin. However, if this point is too close to the boundary of the attacking class, the iteratively adjusted attack point may become a reserve point, which halts further progress.\nThe computation of the gradient of the validation error crucially depends on the assumption that the structure of the sets S , E and R does not change during the update. In general, it is difficult to determine the largest step t along the gradient direction \u2207P, which preserves this structure. Hence, the step t is fixed to a small constant value in our algorithm. After each update of the attack point xp, the optimal solution can be efficiently recomputed from the solution on Dtr, using the incremental SVM machinery [20]. The algorithm terminates when the change in the validation error is smaller than a predefined threshold."}, {"heading": "5.2.1 Gradient Computation", "text": "We now discuss how to compute the gradient \u2207P of our objective function. For notational convenience, we now refer to the attack point as xc instead of x.\n30 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\nAlgorithm 2 Poisoning attack against an SVM Input: Dtr, the training data; Dval, the validation data; y\u2217, the class label of the attack point; x0, the initial attack point; t, the step size. Output: x\u2217, the final attack point. 1: {\u03b1i,b}\u2190 learn an SVM on Dtr. 2: p\u2190 0. 3: repeat 4: Re-compute the SVM solution on Dtr\u222a{xp,y\u2217} using the incremental SVM [20]. This step\nrequires {\u03b1i,b}. If computational complexity is manageable, a full SVM can be learned at each step, instead.\n5: Compute \u2207P on Dval according to Equation (8). 6: Normalize \u2207P to have unit norm. 7: p\u2190 p+1 and xp\u2190 xp\u22121 + t\u2207P 8: if xlb > xp or xp > xub then 9: Project xp onto the boundary of the feasible region (enforce application-specific con-\nstraints, if any). 10: end if 11: until P(xp)\u2212P ( xp\u22121 ) < \u03b5 12: return: x\u2217 = xp\nFirst, we explicitly account for all terms in the margin conditions gk that are affected by the attack point xc:\ngk = \u2211 j Qk j\u03b1 j + ykb\u22121 (4)\n= \u2211 j 6=c Qk j\u03b1 j(xc)+Qkc(xc)\u03b1c(xc)+ ykb(xc)\u22121 .\nAs already mentioned, P(xc) is a non-convex objective function, and we thus exploit a gradient ascent technique to iteratively optimize it. We denote the initial location of the attack point as x0c . Our goal is to update the attack point as x p c = x (p\u22121) c + t\u2207P where p is the current iteration, \u2207P is a unit vector representing the attack direction (i.e., the normalized objective gradient), and t is the step size. To maximize our objective, the attack direction \u2207P is computed at each iteration.\nAlthough the hinge loss is not everywhere differentiable, this can be overcome by only considering point indices k with non-zero contributions to P; i.e., those for which \u2212gk > 0. Contributions of such points to \u2207P can be computed by differentiating Equation (4) with respect to xc using the product rule:\n\u2202gk \u2202xc = Qks \u2202\u03b1 \u2202xc + \u2202Qkc \u2202xc \u03b1c + yk \u2202b \u2202xc , (5)\nwhere, by denoting the lth feature of xc as xcl , we use the notation\n\u2202\u03b1 \u2202xc =\n \u2202\u03b11 \u2202xc1 \u00b7 \u00b7 \u00b7 \u2202\u03b11\u2202xcd ... . . .\n... \u2202\u03b1s \u2202xc1 \u00b7 \u00b7 \u00b7 \u2202\u03b1s\u2202xcd\n , simil. \u2202Qkc\u2202xc , \u2202b\u2202xc .\nSecurity Evaluation of SVMs in Adversarial Environments 31\nThe expressions for the gradient can be further refined using the fact that the gradient step must preserve the optimal SVM solution. This can expressed as an adiabatic update condition using the technique introduced in [20]. In particular, for the ith training point, the KKT conditions of the optimal SVM solution are:\ngi = \u2211 j\u2208Dtr Qi j\u03b1 j + yib\u22121  > 0; i \u2208R = 0; i \u2208S < 0; i \u2208 E\n(6)\nh = \u2211 j\u2208Dtr (y j\u03b1 j) = 0 . (7)\nThe form of these conditions implies that an infinitesimal change in the attack point xc causes a smooth change in the optimal solution of the SVM, under the restriction that the composition of the sets S , E and R remains intact. This equilibrium allows us to predict the response of the SVM solution to the variation of xc, as shown below.\nBy differentiation of the xc-dependent terms in Equations (6)\u2013(7) with respect to each feature xcl (1\u2264 l \u2264 d), we obtain, for any i \u2208S ,\n\u2202g \u2202xcl = Qss \u2202\u03b1 \u2202xcl + \u2202Qsc \u2202xcl \u03b1c + ys \u2202b \u2202xcl = 0 \u2202h \u2202xcl = y>s \u2202\u03b1 \u2202xcl = 0 .\nSolving these equations and computing an inverse matrix via the Sherman-MorrisonWoodbury formula [48] yields the following gradients:\n\u2202\u03b1 \u2202xc =\u2212 1 \u03b6 \u03b1c(\u03b6 Q\u22121ss \u2212\u03c5\u03c5>) \u00b7 \u2202Qsc \u2202xc \u2202b \u2202xc =\u2212 1 \u03b6 \u03b1c\u03c5> \u00b7 \u2202Qsc \u2202xc ,\nwhere \u03c5 = Q\u22121ss ys and \u03b6 = y>s Q\u22121ss ys. We thus obtain the following gradient of the objective used for optimizing our attack, which only depends on xc through gradients of the kernel matrix, \u2202Qkc\u2202xc :\n\u2207P = m\n\u2211 k=1\n{ Mk\n\u2202Qsc \u2202xc + \u2202Qkc \u2202xc\n} \u03b1c , (8)\nwhere Mk =\u2212 1\u03b6 (Qks(\u03b6 Q \u22121 ss \u2212\u03c5\u03c5T )+ yk\u03c5T ).\n32 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli"}, {"heading": "5.2.2 Kernelization", "text": "From Equation (8), we see that the gradient of the objective function at iteration p may depend on the attack point xpc = xp\u22121c + t\u2207P only through the gradients of the matrix Q. In particular, this depends on the chosen kernel. We report below the expressions of these gradients for three common kernels (see Section 4.2.1): \u2022 Linear kernel: \u2202Kic\u2202xc = \u2202 (xi\u00b7xc) \u2202xc = xi\n\u2022 Polynomial kernel: \u2202Kic\u2202xc = \u2202 (xi\u00b7xc+R)d \u2202xc = d(xi \u00b7xc +R) d\u22121xi \u2022 RBF kernel: \u2202Kic\u2202xc = \u2202e\u2212\u03b3||xi\u2212xc|| 2\n\u2202xc = 2\u03b3K(xi,xc)(xi\u2212xc) The dependence of these gradients on the current attack point, xc, can be avoided by using the previous attack point, provided that t is sufficiently small. This approximation enables a straightforward extension of our method to arbitrary differentiable kernels."}, {"heading": "5.3 Experiments", "text": "The experimental evaluation presented in the following sections demonstrates the behavior of our proposed method on an artificial two-dimensional dataset and evaluates its effectiveness on the classical MNIST handwritten digit recognition dataset [32, 45]."}, {"heading": "5.3.1 Two-dimensional Toy Example", "text": "Here we consider a two-dimensional example in which each class follows a Gaussian with mean and covariance matrices given by \u00b5\u2212 = [\u22121.5,0], \u00b5+ = [1.5,0], \u03a3\u2212 = \u03a3+ = 0.6I. The points from the negative distribution have label \u22121 (shown as red in subsequent figures) and otherwise +1 (shown as blue). The training and the validation sets, Dtr and Dval, consist of 25 and 500 points per class, respectively.\nIn the experiment presented below, the red class is the attacking class. That is, a random point of the blue class is selected and its label is flipped to serve as the starting point for our method. Our gradient ascent method is then used to refine this attack until its termination condition is satisfied. The attack\u2019s trajectory is traced as the black line in Figure 7 for both the linear kernel (upper two plots) and the RBF kernel (lower two plots). The background of each plot depicts an error surface: hinge loss computed on a validation set (leftmost plots) and the classification error (rightmost plots). For the linear kernel, the range of attack points is limited to the box x \u2208 [\u22124,4]2 shown as a dashed line. This implements the constraint of Equation (3).\nFor both kernels, these plots show that our gradient ascent algorithm finds a reasonably good local maximum of the non-convex error surface. For the linear kernel, it terminates at the corner of the bounded region, since the error surface is un-\nSecurity Evaluation of SVMs in Adversarial Environments 33\nbounded. For the RBF kernel, it also finds a good local maximum of the hinge loss which, incidentally, is the maximum classification error within this area of interest."}, {"heading": "5.3.2 Handwritten Digits", "text": "We now quantitatively validate the effectiveness of the proposed attack strategy on the MNIST handwritten digit classification task [32, 45], as with the evasion attacks in Section 4.3. In particular, we focus here on the following two-class sub-problems: 7 vs. 1; 9 vs. 8; 4 vs. 0. Each digit is normalized as described in Section 4.3.1. We consider again a linear SVM with C = 1. We randomly sample a training and a validation data of 100 and 500 samples, respectively, and retain the complete testing\ndata given by MNIST for Dts. Although it varies for each digit, the size of the testing data is about 2000 samples per class (digit).\nFigure 8 shows the effect of single attack points being optimized by our descent method. The leftmost plots of each row show the example of the attacked class used as starting points in our algorithm. The middle plots show the final attack point. The rightmost plots depict the increase in the validation and testing errors as the attack progresses. For this experiment we run the attack algorithm 5 times by re-initializing the gradient ascent procedure, and we retain the best result.\nVisualizing the attack points reveals that these attacks succeed by blurring the initial prototype to appear more like examples of the attacking class. In comparing the initial and final attack points, we see that the bottom segment of the 7 straightens to resemble a 1, the lower segment of the 9 is rounded to mimicking an 8, and ovular noise is added to the outer boundary of the 4 to make it similar to a 0. These blurred images are thus consistent with one\u2019s natural notion of visually confusing digits.\nThe rightmost plots further demonstrate a striking increase in error over the course of the attack. In general, the validation error overestimates the classification\nSecurity Evaluation of SVMs in Adversarial Environments 35\nerror due to a smaller sample size. Nonetheless, in the exemplary runs reported in this experiment, a single attack data point caused the classification error to rise from initial error rates of 2\u20135% to 15\u201320%. Since our initial attack points are obtained by flipping the label of a point in the attacked class, the errors in the first iteration of the rightmost plots of Figure 8 are caused by single random label flips. This confirms that our attack can achieve significantly higher error rates than random label flips, and underscores the vulnerability of the SVM to poisoning attacks.\nThe latter point is further illustrated in a multiple point, multiple run experiment presented in Figure 9. For this experiment, the attack was extended by repeatedly injecting attack points into the same class and averaging results over multiple runs on randomly chosen training and validation sets of the same size (100 and 500 samples, respectively). These results exhibit a steady rise in classification error as the percentage of attack points in the training set increases. The variance of the error is quite high, which can be attributed to the relatively small sizes of the training and validation sets. Also note that, in this experiment, to reach an error rate of 15\u201320%, the adversary needs to control at least 4\u20136% of the training data, unlike in the single point attacks of Figure 8. This is because Figure 8 displays the best single point attack from five restarts whereas here initial points are selected without restarts."}, {"heading": "5.4 Discussion", "text": "The poisoning attack presented in this section, summarized from our previous work in [14], is a first step toward the security analysis of SVM against training data attacks. Although our gradient ascent method is not optimal, it attains a surprisingly large impact on the SVM\u2019s classification accuracy.\nSeveral potential improvements to the presented method remain to be explored in future work. For instance, one may investigate the effectiveness of such an attack with surrogate data, that is, when the training data is not known to the adversary, who may however collect samples drawn from the same distribution to learn a classifier\u2019s copy (similarly to the limited knowledge case considered in the evasion attacks of Section 4). Another improvement may be to consider the simultaneous optimization of multi-point attacks, although we have already demonstrated that greedy, sequential single-point attacks may be rather successful.\nAn interesting analysis of the SVM\u2019s vulnerability to poisoning suggested from this work is to consider the attack\u2019s impact under loss functions other than the hinge loss. It would be especially interesting to analyze bounded loss functions, like the ramp loss, since such losses are designed to limit the impact of any single (attack) point on the outcome. On the other hand, while these losses may lead to improved security to poisoning, they also make the SVM\u2019s optimization problem non-convex, and, thus, more computationally demanding. This may be viewed as another tradeoff between computational complexity of the learning algorithm and security.\nAn important practical limitation of the proposed method is the assumption that the attacker controls the labels of injected points. Such assumptions may not hold\n36 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\nif the labels are assigned by trusted sources such as humans, e.g., anti-spam filters use their users\u2019 labeling of messages as ground truth. Thus, although an attacker can send arbitrary messages, he cannot guarantee that they will have the labels necessary for his attack. This imposes an additional requirement that the attack data must satisfy certain side constraints to fool the labeling oracle. Further work is needed to understand and incorporate these potential side constraints into attacks."}, {"heading": "6 Privacy Attacks against SVMs", "text": "We now consider a third scenario in which the attacker\u2019s goal is to affect a breach of the training data\u2019s confidentiality. We review our recent work [59] deriving mechanisms for releasing SVM classifiers trained on privacy-sensitive data while maintaining the data\u2019s privacy. Unlike previous sections, our focus here lies primary in the study of countermeasures, while we only briefly consider attacks in the context\nSecurity Evaluation of SVMs in Adversarial Environments 37\nof lower bounds. We adopt the formal framework of Dwork et al. [30], in which a randomized mechanism is said to preserve \u03b2 -differential privacy, if the likelihood of the mechanism\u2019s output changes by at most \u03b2 when a training datum is changed arbitrarily (or even removed). The power of this framework, which gained nearuniversal favor after its introduction, is that it quantifies privacy in a rigorous way and provides strong guarantees even against powerful adversaries with knowledge of almost all of the training data, knowledge of the mechanism (barring its source of randomness), arbitrary access to the classifier output by the mechanism, and the ability to manipulate almost all training data prior to learning.\nThis section is organized as follows. In Section 6.1 we outline our model of the adversary, which makes only weak assumptions. Section 6.2 provides background on differential privacy, presents a mechanism for training and releasing privacypreserving SVMs\u2014essentially a countermeasure to many privacy attacks\u2014and provides guarantees on differential privacy and also utility (e.g., controlling the classifier\u2019s accuracy). We then briefly touch on existing approaches for evaluation via lower bounds and discuss other work and open problems in Section 6.3."}, {"heading": "6.1 Modeling the Adversary", "text": "We first apply our framework to define the threat model for defending against privacy attacks within the broader context of differential privacy. We then focus on specific countermeasures in the form of modifications to SVM learning that provide differential privacy."}, {"heading": "6.1.1 Adversary\u2019s Goal", "text": "The ultimate goal of the attacker in this section is to determine features and/or the label of an individual training datum. The overall approach of the adversary towards this goal, is to inspect (arbitrary numbers of) test-time classifications made by a released classifier trained on the data, or by inspecting the classifier directly. The definition of differential privacy, and the particular mechanisms derived here, can be modified for related goals of determining properties of several training data; we focus on the above conventional case without loss of generality."}, {"heading": "6.1.2 Adversary\u2019s Knowledge", "text": "As alluded to above, we endow our adversary with significant knowledge of the learning system, so as to derive countermeasures that can withstand very strong attacks. Indeed the notion of differential privacy, as opposed to more syntactic notions of privacy such as k-anonymity [64], was inspired by decades-old work in cryptography that introduced mathematical formalism to an age-old problem, yielding\n38 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\nsignificant practical success. Specifically, we consider a scenario in which the adversary has complete knowledge of the raw input feature representation, the learning algorithm (the entire mechanism including the form of randomization it introduces, although not the source of randomness) and the form of its decision function (in this case, a thresholded SVM), the learned classifier\u2019s parameters (the kernel/feature mapping, primal weight vector, and bias term), and arbitrary/unlimited feedback from the deployed classifier (k.ii-v). We grant the attacker near complete knowledge of the training set (k.i): the attacker may have complete knowledge of all but one training datum, for which she has no knowledge of input feature values or its training label, and it is these attributes she wishes to reveal. For simplicity of exposition, but without loss of generality, we assume this to be the last datum in the training sample."}, {"heading": "6.1.3 Adversary\u2019s Capability", "text": "Like our assumptions on the attacker\u2019s knowledge, we impose weak limitations on the adversary\u2019s capability. We assume an adversary that can manipulate both training and test data (c.i), although the latter is subsumed by the attacker\u2019s complete knowledge of the decision function and learned parameters\u2014e.g., she may implement her own classifier and execute it arbitrarily, or she may submit or manipulate test points presented to a deployed classifier.\nOur attack model makes no assumptions about the origins of the training or test data. The data need not be sampled independently or even according to a distribution\u2014the definition of differential privacy provided below makes worst-case assumptions about the training data, and again the test data could be arbitrary. Thus the adversary may have arbitrary capability to modify class priors, training data features and labels (c.ii-iv) except that the adversary attacking the system may not directly modify the targeted training datum because she does not have knowledge of it. That said, however, differential privacy makes worst-case (no distributional) assumptions about the datum and thus one could consider even this data point as being adversarially manipulated by nature (i.e., nature does not collude with the attacker to share information about the target training datum, but that may collude to facilitate a privacy breach by selecting a \u201cconvenient\u201d target datum)."}, {"heading": "6.1.4 Attack Strategy", "text": "While no practical privacy attacks on SVMs have been explored in the past\u2014an open problem discussed in Section 6.3\u2014a general approach would be to approximate the inversion of the learning map on the released SVM parametrization (either primal weight vector, or dual variables) around the known portion of the training data. In practice this could be achieved by taking a similar approach as done in Section 5 whereby an initial guess of a missing training point is iterated on by taking gradient steps of the differential in the SVM parameter vector with respect to the\nSecurity Evaluation of SVMs in Adversarial Environments 39\nmissing training datum. An interpretation of this approach is one of simulation: to guess a missing training datum, given access to the remainder of the training set and the SVM solution on all the data, simulate the SVM on guesses for the missing datum, updating the guesses in directions that appropriately shift the intermediate solutions. As we discuss briefly in the sequel, theoretical lower bounds on achievable privacy relate to attacks in pathological cases."}, {"heading": "6.2 Countermeasures with Provable Guarantees", "text": "Given an adversary with such strong knowledge and capabilities as described above, it may seem difficult to provide effective countermeasures particularly considering the complication of abundant access to side information that is often used in publicized privacy attacks [51, 64]. However, the crux that makes privacy-preservation under these conditions possible lies in the fact that the learned quantity being released is an aggregate statistic of the sensitive data; intuitively the more data being aggregated, the less sensitive a statistic should be to changes or removal of any single datum. We now present results from our recent work that quantifies this effect [59], within the framework of differential privacy."}, {"heading": "6.2.1 Background on Differential Privacy", "text": "We begin by recalling the key definition due to Dwork et al. [30]. First, for any training set D = {(xi,yi)}ni=1 denote set D \u2032 to be a neighbor of D (or D \u2032 \u223c D) if D \u2032 = {(xi,yi)}n\u22121i=1 \u222a{(x\u2032n,y\u2032n)} where (xn,yn) 6= (x\u2032n,y\u2032n). In the present context, differential privacy is a desirable property of learning maps, which maps a training set {(xi,yi)}ni=1 to a continuous discriminant function of the form g : X \u2192 R\u2014here a learned SVM\u2014in some space of functions, H . We say that a randomized6 learning map L preserves \u03b2 -differential privacy if for all datasets D , all neighboring sets D \u2032 of D , and all possible functions g \u2208H , the following relation holds\nPr(L (D) = g) \u2264 exp(\u03b2 )Pr(L (D \u2032) = g) .\nIntuitively, if we initially fix a training set and neighboring training set, differential privacy simply says that the two resulting distributions induced on the learned functions are point-wise close\u2014and closer for smaller \u03b2 . For a patient deciding whether to submit her datum to a training set for a cancer detector, differential privacy means that the learned classifier will reveal little information about that datum. Even an adversary with access to the inner-workings of the learner, with access to\n6 That is, the learning map\u2019s output is not a deterministic function of the training data. The probability in the definition of differential privacy is due to this randomness. Our treatment here is only as complex as necessary, but to be completely general, the events in the definition should be on measurable sets G\u2282H rather than individual g \u2208H .\n40 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\nall other patients\u2019 data, and with the ability to guess-and-simulate the learning process repeatedly with various possible values of her datum, cannot reverse engineer her datum from the classifier released by the hospital because the adversary cannot distinguish the classifier distribution on one training set, from that on neighboring sets. Moreover, variations of this definition (which do not significantly affect the presented results) allow for neighboring databases to be defined as those missing a datum; or having several varying data, not just a single one.\nFor simplicity of exposition, we drop the explicit bias term b from our SVM learning process and instead assume that the data feature vectors are augmented with a unit constant, and that the resulting additional normal weight component corresponds to the bias. This is an equivalent SVM formulation that allows us to focus only on the normal\u2019s weight vector.\nA classic route to establish differential privacy is to define a randomized map L that returns the value of a deterministic, non-random L\u0302 plus a noise term. Typically, we use an exponential family in a term that matches an available Lipschitz condition satisfied by L\u0302 : in our case, for learning maps that return weight vectors in Rd , we aim to measure global sensitivity of L\u0302 via the L1 norm as\n\u2206(L\u0302 ) = max D ,D \u2032\u223cD \u2225\u2225L\u0302 (D)\u2212 L\u0302 (D \u2032)\u2225\u22251 . With respect to this sensitivity, we can easily prove that the randomized mechanism\nL (D) = L\u0302 (D)+Laplace(0,\u2206(L\u0302 )/\u03b2 ) ,\nis \u03b2 -differential private.7 The well-established proof technique [30] follows from the definition of the Laplace distribution involving the same norm as used in our measure of global sensitivity, and the triangle inequality: for any training set D , D \u2032 \u223cD , response g \u2208H , and privacy parameter \u03b2\nPr(L (D) = g) Pr(L (D \u2032) = g)\n= exp (\u2225\u2225L\u0302 (D \u2032)\u2212g\u2225\u22251 \u03b2/\u2206(L\u0302 ))\nexp (\u2225\u2225L\u0302 (D)\u2212g\u2225\u22251 \u03b2/\u2206(L\u0302 ))\n\u2264 exp (\u2225\u2225L\u0302 (D \u2032)\u2212 L\u0302 (D)\u2225\u22251 \u03b2/\u2206(L\u0302 )) \u2264 exp(\u03b2 ) .\nWe take the above route to develop a differentially-private SVM. As such, the onus is on calculating the SVM\u2019s global sensitivity, \u2206(L\u0302 )."}, {"heading": "6.2.2 Global Sensitivity of Linear SVM", "text": "Unlike much prior work applying the \u201cLaplace mechanism\u201d to achieving differential privacy, in which studied estimators are often decomposed as linear functions of\n7 Recall that the zero-mean multi-variate Laplace distribution with scale parameter s has density proportional to exp(\u2212\u2016x\u20161/s).\nSecurity Evaluation of SVMs in Adversarial Environments 41\nAlgorithm 3 Privacy-preserving SVM Input: D the training data; C > 0 soft-margin parameter; kernel k inducing a feature space with finite dimension F and \u03ba-bounded L2-norm; privacy parameter \u03b2 > 0. Output: learned weight vector w . 1: w\u0302\u2190 learn an SVM with parameter C and kernel k on data D . 2: \u00b5 \u2190 draw i.i.d. sample of F scalars from Laplace ( 0, 4C\u03ba \u221a F\n\u03b2\n) .\n3: return: w = w\u0302+\u00b5\ndata [15], measuring the sensitivity of the SVM appears to be non-trivial owing to the non-linear influence an individual training datum may have on the learned SVM. However, perturbations of the training data were studied by the learning-theory community in the context of algorithmic stability: there the goal is to establish bounds on classifier risk, from stability of the learning map, as opposed to leveraging combinatorial properties of the hypothesis class (e.g., the VC dimension, which is not always possible to control, and for the RBF kernel SVM is infinite) [61]. In recent work [59], we showed how these existing stability measurements for the SVM can be adapted to provide the following L1-global sensitivity bound.\nLemma 1. Consider SVM learning with a kernel corresponding to linear SVM in a feature space with finite-dimension F and L2-norm bounded8 by \u03ba , with hinge loss (as used throughout this chapter), and chosen parameter C > 0. Then the L1 global sensitivity of the resulting normal weight vector is upper-bounded by 4C\u03ba \u221a F.\nWe omit the proof, which is available in the original paper [59] and which follows closely the previous measurements for algorithmic stability. We note that the result extends to any convex Lipschitz loss."}, {"heading": "6.2.3 Differentially-Private SVMs", "text": "So far we have established that Algorithm 3, which learns an SVM and returns the resulting weight vector with added Laplace noise, preserves \u03b2 -differential privacy. More noise is added to the weight vector when either (i) a higher degree of privacy is desired (smaller \u03b2 ), (ii) the SVM fits closer to the data (higher C) or (iii) the data is more distinguishable (higher \u03ba or F\u2014the curse of dimensionality). Hidden in the above is the dependence on n: typically we take C to scale like 1/n to achieve consistency in which case we see that noise decreases with larger training data\u2014 akin to less individual influence\u2014as expected [59].\nProblematic in the above approach, is the destruction to utility due to preserving differential privacy. One approach to quantifying this effect, involves bounding the following notion of utility [59]. We say a privacy-preserving learning map L has (\u03b5,\u03b4 )-utility with respect to non-private map L\u0302 if for all training sets D ,\n8 That is \u2200x, k(x,x)\u2264 \u03ba2; e.g. for the RBF the norm is uniformly unity \u03ba = 1; more generally, we can make the standard assumption that the data lies within some \u03ba L2-ball.\n42 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli Pr (\u2225\u2225L (D)\u2212 L\u0302 (D)\u2225\u2225\u221e \u2264 \u03b5) \u2265 1\u2212\u03b4 .\nThe norm here is in the function space of continuous discriminators, learned by the learning maps, and is the point-wise L\u221e norm which corresponds to \u2016g\u2016\u221e = supx |g(x)|\u2014although for technical reasons we will restrict the supremum to be over a set M to be specified later. Intuitively, this indicates that the continuous predictions of the learned private classifier are close to those predictions of the learned non-private classifier, for all test points in M , with high probability (again, in the randomness due to the private mechanism). This definition draws parallels with PAC learnability, and in certain scenarios is strictly stronger than requiring that the private learner achieves good risk (i.e., PAC learns) [59]. Using the Chernoff tail inequality and known moment-generating functions, we establish the following bound on the utility of this private SVM [59].\nTheorem 1. The \u03b2 -differentially-private SVM of Algorithm 3 achieves (\u03b5,\u03b4 )-utility with respect to the non-private SVM run with the same C parameter and kernel, for 0 < \u03b4 < 1 and\n\u03b5 \u2265 8C\u03ba\u03a6 \u221a F (\nF + log 1 \u03b4\n) /\u03b2 ,\nwhere the set M supporting the supremum in the definition of utility is taken to be the pre-image of the feature mapping on the L\u221e ball of radius \u03a6 > 0.9\nAs expected, the more confidence \u03b4 or privacy \u03b2 required, the less accuracy is attainable. Similarly, when the training data is fitted more tightly via higher C, or when the data is less tightly packed for higher \u03ba,\u03a6 ,F , less accuracy is possible. Note that like the privacy result, this result can hold for quite general loss functions."}, {"heading": "6.3 Discussion", "text": "In this section, we have provided a summary of our recent results on strong countermeasures to privacy attacks on the SVM. We have shown how, through controlled addition of noise, SVM learning in finite-dimensional feature spaces can provide both privacy and utility guarantees. These results can be extended to certain translation-invariant kernels including the infinite-dimensional RBF [59]. This extension borrows a technique from large-scale learning where finding a dual solution of the SVM for large training data size n is infeasible. Instead, a primal SVM problem is solved using a random kernel that uniformly approximates the desired kernel. Since the approximating kernel induces a feature mapping of relatively small, finite dimensions, the primal solution becomes feasible. For privacy preservation, we use the same primal approach but with this new kernel. Fortunately, the distribution of\n9 Above we previously bounded the L2 norms of points in features space by \u03ba , the additional bound on the L\u221e norm here is for convenience and is standard practice in learning-theoretic results.\nSecurity Evaluation of SVMs in Adversarial Environments 43\nthe approximating kernel is independent of the training data, and thus we can reveal the approximating kernel without sacrificing privacy. Likewise the uniform approximation of the kernel composes with the utility result here to yield an analogous utility guarantee for translation-invariant kernels.\nWhile we demonstrated here a mechanism for private SVM learning with upper bounds on privacy and utility, we have previously also studied lower bounds that expose limits on the achievable utility of any learner that provides a given level of differential privacy. Further work is needed to sharpen these results. In a sense, these lower bounds are witnessed by pathological training sets and perturbation points and, as such, serve as attacks in pathological (unrealistic) cases. Developing practical attacks on the privacy of an SVM\u2019s training data remains unexplored.\nFinally, it is important to note that alternate approaches to differentially-private SVMs have been explored by others. Most notable is the work (parallel to our own) of Chaudhuri et al. [21]. Their approach to finite-dimensional feature mappings is, instead of adding noise to the primal solution, to add noise to the primal objective in the form of a dot product of the weight with a random vector. Initial experiments show their approach to be very promising empirically, although it does not allow for non-differentiable losses like the hinge loss."}, {"heading": "7 Concluding Remarks", "text": "In security applications like malware detection, intrusion detection, and spam filtering, SVMs may be attacked through patterns that can either evade detection (evasion), mislead the learning algorithm (poisoning), or gain information about their internal parameters or training data (privacy violation). In this chapter, we demonstrated that these attacks are feasible and constitute a relevant threat to the security of SVMs, and to machine learning systems, in general.\nEvasion. We proposed an evasion algorithm against SVMs with differentiable kernels, and, more generally, against classifiers with differentiable discriminant functions. We investigated the attack\u2019s effectiveness in perfect and limited knowledge settings. In both cases, our attack simulation showed that SVMs (both linear and RBF) can be evaded with high probability after a few modifications to the attack patterns. Our analysis also provides some general hints for tuning the classifier\u2019s parameters (e.g., the value of \u03b3 in SVMs with the RBF kernel) and for improving classifier security. For instance, if a classifier tightly encloses the legitimate samples, the adversary\u2019s samples must closely mimic legitimate samples to evade it, in which case, if such exact mimicry is still possible, it suggests an inherent flaw in the feature representation.\nPoisoning. We presented an algorithm that allows the adversary to find an attack pattern whose addition to the training set maximally decreases the SVM\u2019s classification accuracy. We found that the increase in error over the course of attack is especially striking. A single attack data point may cause the classification error to rise from the initial error rates of 2\u20135% to 15\u201320%. This confirms that our attack\n44 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli\ncan achieve significantly higher error rates than random label flips, and underscores the vulnerability of the SVM to poisoning attacks. As a future investigation, it may be of interest to analyze the effectiveness of poisoning attacks against non-convex SVMs with bounded loss functions, both empirically and theoretically, since such losses are designed to limit the impact of any single (attack) point on the resulting learned function. This has been also studied from a more theoretical perspective in [23], exploiting the framework of Robust Statistics [35, 50]. A similar effect is obtained by using bounded kernels (e.g., the RBF kernel) or bounded feature values.\nPrivacy. We developed an SVM learning algorithm that preserves differential privacy, a formal framework for quantifying the threat of a potential training set privacy violation incurred by releasing learned classifiers. Our mechanism involves adding Laplace-distributed noise to the SVM weight vector with a scale that depends on the algorithmic stability of the SVM and the desired level of privacy. In addition to presenting a formal guarantee that our mechanism preserves privacy, we also provided bounds on the utility of the new mechanism, which state that the privacy-preserving classifier makes predictions that are point-wise close to those of the non-private SVM, with high probability. Finally we discussed potential approaches for attacking SVMs\u2019 training data privacy, and known approaches to differentially-private SVMs with (possibly infinite-dimensional feature space) translation-invariant kernels, and lower bounds on the fundamental limits on utility for private approximations of the SVM.\nAcknowledgements This work has been partly supported by the project CRP-18293 funded by Regione Autonoma della Sardegna, L.R. 7/2007, Bando 2009, and by the project \u201cAdvanced and secure sharing of multimedia data over social networks in the future Internet\u201d (CUP F71J1100069 0002) funded by the same institution. Davide Maiorca gratefully acknowledges Regione Autonoma della Sardegna for the financial support of his PhD scholarship (P.O.R. Sardegna F.S.E. Operational Programme of the Autonomous Region of Sardinia, European Social Fund 2007-2013 - Axis IV Human Resources, Objective l.3, Line of Activity l.3.1.). Blaine Nelson thanks the Alexander von Humboldt Foundation for providing additional financial support. The opinions expressed in this chapter are solely those of the authors and do not necessarily reflect the opinions of any sponsor."}], "references": [{"title": "Proceedings of the 1st ACM Workshop on AISec (AISec)", "author": ["D. Balfanz", "Staddon", "J. (eds."], "venue": "ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Proceedings of the 2nd ACM Workshop on Security and Artificial Intelligence (AISec)", "author": ["D. Balfanz", "Staddon", "J. (eds."], "venue": "ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "The security of machine learning", "author": ["M. Barreno", "B. Nelson", "A. Joseph", "J. Tygar"], "venue": "Machine Learning 81, 121\u2013148", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Can machine learning be secure? In: ASIACCS \u201906: Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security, pp", "author": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "venue": "16\u201325. ACM, New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "A learning-based approach to reactive security", "author": ["A. Barth", "B.I.P. Rubinstein", "M. Sundararajan", "J.C. Mitchell", "D. Song", "P.L. Bartlett"], "venue": "IEEE Transactions on Dependable and Secure Computing 9(4), 482\u2013493", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Bagging classifiers for fighting poisoning attacks in adversarial environments", "author": ["B. Biggio", "I. Corona", "G. Fumera", "G. Giacinto", "F. Roli"], "venue": "the 10th International Workshop on Multiple Classifier Systems (MCS), Lecture Notes in Computer Science, vol. 6713, pp. 350\u2013359. Springer-Verlag", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Evasion attacks against machine learning at test time", "author": ["B. Biggio", "I. Corona", "D. Maiorca", "B. Nelson", "N. \u0160rndi\u0107", "P. Laskov", "G. Giacinto", "F. Roli"], "venue": "H. Blockeel et al. (ed.) European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), Part III, Lecture Notes in Artificial Intelligence, vol. 8190, pp. 387\u2013402. Springer-Verlag Berlin Heidelberg", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Poisoning attacks to compromise face templates", "author": ["B. Biggio", "L. Didaci", "G. Fumera", "F. Roli"], "venue": "the 6th IAPR International Conference on Biometrics (ICB)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "A survey and experimental evaluation of image spam filtering techniques", "author": ["B. Biggio", "G. Fumera", "I. Pillai", "F. Roli"], "venue": "Pattern Recognition Letters 32(10), 1436 \u2013 1446", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Design of robust classifiers for adversarial environments", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Int\u2019l Conf. on Systems, Man, and Cybernetics (SMC), pp. 977\u2013982", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Security evaluation of pattern classifiers under attack", "author": ["B. Biggio", "G. Fumera", "F. Roli"], "venue": "IEEE Transactions on Knowledge and Data Engineering 99(PrePrints), 1", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Poisoning adaptive biometric systems", "author": ["B. Biggio", "G. Fumera", "F. Roli", "L. Didaci"], "venue": "Structural, Syntactic, and Statistical Pattern Recognition, Lecture Notes in Computer Science, vol. 7626, pp. 417\u2013425", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Poisoning attacks against support vector machines", "author": ["B. Biggio", "B. Nelson", "P. Laskov"], "venue": "Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Practical privacy: the SuLQ framework", "author": ["A. Blum", "C. Dwork", "F. McSherry", "K. Nissim"], "venue": "Proceedings of the 24th Symposium on Principles of Database Systems, pp. 128\u2013138", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning 24(2), 123\u2013140", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "Static prediction games for adversarial learning problems", "author": ["M. Br\u00fcckner", "C. Kanzow", "T. Scheffer"], "venue": "Journal of Machine Learning Research 13, 2617\u20132654", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluation of classifiers: Practical considerations for security applications", "author": ["A.A. C\u00e1rdenas", "J.S. Baras"], "venue": "AAAI Workshop on Evaluation Methods for Machine Learning", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "The 5th ACM Workshop on Artificial Intelligence and Security (AISec)", "author": ["A.A. C\u00e1rdenas", "B. Nelson", "Rubinstein", "B.I. (eds."], "venue": "ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Incremental and decremental support vector machine learning", "author": ["G. Cauwenberghs", "T. Poggio"], "venue": "T.K. Leen, T.G. Dietterich, V. Tresp (eds.) NIPS, pp. 409\u2013415. MIT Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Differentially private empirical risk minimization", "author": ["K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate"], "venue": "Journal of Machine Learning Research 12(Mar), 1069\u20131109", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence (AISec)", "author": ["Y. Chen", "A.A. C\u00e1rdenas", "R. Greenstadt", "Rubinstein", "B. (eds."], "venue": "ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "On robust properties of convex risk minimization methods for pattern recognition", "author": ["A. Christmann", "I. Steinwart"], "venue": "Journal of Machine Learning Research 5, 1007\u20131034", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Adversarialib: a general-purpose library for the automatic evaluation of machine learning-based classifiers under adversarial attacks", "author": ["I. Corona", "B. Biggio", "D. Maiorca"], "venue": "URL http:// sourceforge.net/projects/adversarialib/", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning 20, 273\u2013297", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1995}, {"title": "Adversarial classification", "author": ["N. Dalvi", "P. Domingos", "Mausam", "S. Sanghai", "D. Verma"], "venue": "Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 99\u2013108", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Support vector machines for spam categorization", "author": ["H. Drucker", "D. Wu", "V.N. Vapnik"], "venue": "IEEE Transactions on Neural Networks 10(5), 1048\u20131054", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Pattern Classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "Wiley-Interscience", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "Proceedings of the 3rd Theory of Cryptography Conference (TCC 2006), pp. 265\u2013284", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Polymorphic blending attacks", "author": ["P. Fogla", "M. Sharif", "R. Perdisci", "O. Kolesnikov", "W. Lee"], "venue": "Proceedings of the 15th Conference on USENIX Security Symposium", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["A. Globerson", "S.T. Roweis"], "venue": "Proceedings of the 23rd International Conference on Machine Learning, pp. 353\u2013360", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Discriminative direction for kernel classifiers", "author": ["P. Golland"], "venue": "Neural Information Processing Systems (NIPS), pp. 745\u2013752", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Proceedings of the 3rd ACM Workshop on Artificial Intelligence and Security (AISec)", "author": ["Greenstadt", "R. (ed."], "venue": "ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust Statistics: The Approach Based on Influence Functions", "author": ["F.R. Hampel", "E.M. Ronchetti", "P.J. Rousseeuw", "W.A. Stahel"], "venue": "Probability and Mathematical Statistics. John Wiley and Sons, New York, NY, USA", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1986}, {"title": "Adversarial machine learning", "author": ["L. Huang", "A.D. Joseph", "B. Nelson", "B. Rubinstein", "J.D. Tygar"], "venue": "Proceedings of the 4th ACM Workshop on Artificial Intelligence and Security (AISec), pp. 43\u201357", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Online anomaly detection under adversarial impact", "author": ["M. Kloft", "P. Laskov"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 405\u2013412", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Security analysis of online centroid anomaly detection", "author": ["M. Kloft", "P. Laskov"], "venue": "Journal of Machine Learning Research 13, 3647\u20133690", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature weighting for improved classifier robustness", "author": ["A. Kolcz", "C.H. Teo"], "venue": "Proceedings of the 6th Conference on Email and Anti-Spam (CEAS)", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "A framework for quantitative security analysis of machine learning", "author": ["P. Laskov", "M. Kloft"], "venue": "Proceedings of the 2nd ACM Workshop on Security and Artificial Intelligence, pp. 1\u20134", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2009}, {"title": "Machine learning in adversarial environments", "author": ["P. Laskov", "R. Lippmann"], "venue": "Machine Learning 81, 115\u2013119", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Comparison of learning algorithms for handwritten digit recognition", "author": ["Y. LeCun", "L. Jackel", "L. Bottou", "A. Brunot", "C. Cortes", "J. Denker", "H. Drucker", "I. Guyon", "U. M\u00fcller", "E. S\u00e4ckinger", "P. Simard", "V. Vapnik"], "venue": "International Conference on Artificial Neural Networks, pp. 53\u201360", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1995}, {"title": "Adversarial learning", "author": ["D. Lowd", "C. Meek"], "venue": "Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 641\u2013647", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2005}, {"title": "Good word attacks on statistical spam filters", "author": ["D. Lowd", "C. Meek"], "venue": "Proceedings of the 2nd Conference on Email and Anti-Spam (CEAS)", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Handbook of matrices", "author": ["H. L\u00fctkepohl"], "venue": "John Wiley & Sons", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1996}, {"title": "A pattern recognition system for malicious PDF files detection", "author": ["D. Maiorca", "G. Giacinto", "I. Corona"], "venue": "MLDM, pp. 510\u2013524", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust Statistics: Theory and Methods", "author": ["R.A. Maronna", "R.D. Martin", "V.J. Yohai"], "venue": "Probability and Mathematical Statistics. John Wiley and Sons, New York, NY, USA", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust de-anonymization of large sparse datasets", "author": ["A. Narayanan", "V. Shmatikov"], "venue": "Proceedings of the 29th IEEE Symposium on Security and Privacy, pp. 111\u2013125", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2008}, {"title": "Exploiting machine learning to subvert your spam filter", "author": ["B. Nelson", "M. Barreno", "F.J. Chi", "A.D. Joseph", "B.I.P. Rubinstein", "U. Saini", "C. Sutton", "J.D. Tygar", "K. Xia"], "venue": "Proceedings of the 1st USENIX Workshop on Large-Scale Exploits and Emergent Threats, pp. 1\u20139", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2008}, {"title": "Query strategies for evading convex-inducing classifiers", "author": ["B. Nelson", "B.I. Rubinstein", "L. Huang", "A.D. Joseph", "S.J. Lee", "S. Rao", "J.D. Tygar"], "venue": "Journal of Machine Learning Research 13, 1293\u20131332", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Using an ensemble of one-class SVM classifiers to harden payload-based anomaly detection systems", "author": ["R. Perdisci", "G. Gu", "W. Lee"], "venue": "Proceedings of the International Conference on Data Mining (ICDM), pp. 488\u2013498", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods", "author": ["J. Platt"], "venue": "Advances in Large Margin Classifiers, pp. 61\u201374", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2000}, {"title": "What-if analysis", "author": ["S. Rizzi"], "venue": "Encyclopedia of Database Systems pp. 3525\u20133529", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "Robustness of multimodal biometric fusion methods against spoof attacks", "author": ["R.N. Rodrigues", "L.L. Ling", "V. Govindaraju"], "venue": "Journal of Visual Languages and Computing 20(3), 169\u2013179", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2009}, {"title": "Antidote: understanding and defending against poisoning of anomaly detectors", "author": ["B.I. Rubinstein", "B. Nelson", "L. Huang", "A.D. Joseph", "Lau", "S.h.", "S. Rao", "N. Taft", "J.D. Tygar"], "venue": "Proceedings of the 9th Conference on Internet Measurement Conference (IMC), pp. 1\u201314", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning in a large function space: Privacy-preserving mechanisms for SVM learning", "author": ["B.I.P. Rubinstein", "P.L. Bartlett", "L. Huang", "N. Taft"], "venue": "Journal of Privacy and Confidentiality 4(1), 65\u2013100", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2012}, {"title": "A generalized representer theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "Computational Learning Theory, Lecture Notes in Computer Science, vol. 2111, pp. 416\u2013426", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "MIT Press", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2001}, {"title": "Malicious PDF detection using metadata and structural features", "author": ["C. Smutz", "A. Stavrou"], "venue": "Proceedings of the 28th Annual Computer Security Applications Conference, pp. 239\u2013248", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2012}, {"title": "Detection of malicious PDF files based on hierarchical document structure", "author": ["N. \u0160rndi\u0107", "P. Laskov"], "venue": "Proceedings of the 20th Annual Network & Distributed System Security Symposium (NDSS)", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2013}, {"title": "k-anonymity: a model for protecting privacy", "author": ["L. Sweeney"], "venue": "International Journal on Uncertainty, Fuzziness and Knowledge-based Systems 10(5), 557\u2013570", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2002}, {"title": "The nature of statistical learning theory", "author": ["V.N. Vapnik"], "venue": "Springer-Verlag, Inc.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1995}, {"title": "On attacking statistical spam filters", "author": ["G.L. Wittel", "S.F. Wu"], "venue": "Proceedings of the 1st Conference on Email and Anti-Spam (CEAS)", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2004}, {"title": "2010 IBM x-force mid-year trend & risk report", "author": ["R. Young"], "venue": "Tech. rep., IBM", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "Support Vector Machines (SVMs) are among the most successful techniques that have been applied for this purpose [28, 54].", "startOffset": 112, "endOffset": 120}, {"referenceID": 48, "context": "Support Vector Machines (SVMs) are among the most successful techniques that have been applied for this purpose [28, 54].", "startOffset": 112, "endOffset": 120}, {"referenceID": 38, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 350, "endOffset": 354}, {"referenceID": 0, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 642, "endOffset": 660}, {"referenceID": 1, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 642, "endOffset": 660}, {"referenceID": 31, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 642, "endOffset": 660}, {"referenceID": 20, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 642, "endOffset": 660}, {"referenceID": 17, "context": "This field is receiving growing interest from the research community, as witnessed by an increasing number of recent events: the NIPS Workshop on \u201cMachine Learning in Adversarial Environments for Computer Security\u201d (2007) [43]; the subsequent Special Issue of the Machine Learning journal titled \u201cMachine Learning in Adversarial Environments\u201d (2010) [44]; the 2010 UCLA IPAM workshop on \u201cStatistical and Learning-Theoretic Challenges in Data Privacy\u201d; the ECML-PKDD Workshop on \u201cPrivacy and Security issues in Data Mining and Machine Learning\u201d (2010) [27]; five consecutive CCS Workshops on \u201cArtificial Intelligence and Security\u201d (2008-2012) [2, 3, 34, 22, 19], and the Dagstuhl Perspectives Workshop on \u201cMachine Learning for Computer Security\u201d (2012) [37].", "startOffset": 642, "endOffset": 660}, {"referenceID": 10, "context": "In Section 3, we summarize our recently defined framework for the empirical evaluation of classifiers\u2019 security [12].", "startOffset": 112, "endOffset": 116}, {"referenceID": 6, "context": "We discuss our recently devised evasion attacks against SVMs [8] in Section 4, and review and extend our recent work [14] on poisoning attacks against SVMs in Section 5.", "startOffset": 61, "endOffset": 64}, {"referenceID": 12, "context": "We discuss our recently devised evasion attacks against SVMs [8] in Section 4, and review and extend our recent work [14] on poisoning attacks against SVMs in Section 5.", "startOffset": 117, "endOffset": 121}, {"referenceID": 22, "context": "In order to support the reproducibility of our experiments, we published all the code and the data employed for the experimental evaluations described in this paper [24].", "startOffset": 165, "endOffset": 169}, {"referenceID": 59, "context": "While many hyperplanes may suffice for this task, the SVM hyperplane both separates the training samples of the two classes and provides a maximum distance from itself to the nearest training point (this distance is called the classifier\u2019s margin), since maximum-margin learning generally reduces generalization error [65].", "startOffset": 318, "endOffset": 322}, {"referenceID": 23, "context": "Although originally designed for linearly-separable classification tasks (hard-margin SVMs), SVMs were extended to non-linearly-separable classification problems by Vapnik [25] (soft-margin SVMs), which allow some samples to violate the margin.", "startOffset": 172, "endOffset": 176}, {"referenceID": 54, "context": "1 This is an instance of the Representer Theorem which states that solutions to a large class of regularized ERM problems lie in the span of the training data [60].", "startOffset": 159, "endOffset": 163}, {"referenceID": 8, "context": ", [10]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 3, "context": "[5]: can machine learning be secure? At the center of this question is the effect an adversary can have on a learner by violating the stationarity assumption that the training data used to train the classifier comes from the same distribution as the test data that will be classified by the learned classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Further, as in most security tasks, predicting how the data distribution will change is difficult, if not impossible [12, 36].", "startOffset": 117, "endOffset": 125}, {"referenceID": 33, "context": "Further, as in most security tasks, predicting how the data distribution will change is difficult, if not impossible [12, 36].", "startOffset": 117, "endOffset": 125}, {"referenceID": 10, "context": "Hence, adversarial learning problems are often addressed as a proactive arms race [12], in which the classifier designer tries to anticipate the next adversary\u2019s move, by simulating and hypothesizing proper attack scenarios, as discussed in the next section.", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "Under this setting, the arms race can be modeled as the following cycle [12].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "1 A conceptual representation of the reactive arms race [12].", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "2), it has only recently been formalized within a more general framework for the empirical evaluation of a classifier\u2019s security [12], which we summarize in Section 3.", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": "2 Although in certain abstract models we have shown how regret-minimizing online learning can be used to define reactive approaches that are competitive with proactive security [6].", "startOffset": 177, "endOffset": 180}, {"referenceID": 10, "context": "2 A conceptual representation of the proactive arms race [12].", "startOffset": 57, "endOffset": 61}, {"referenceID": 2, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 3, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 16, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 33, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 35, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 36, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 37, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 40, "context": ", [4, 5, 18, 36, 40, 41, 42, 46].", "startOffset": 2, "endOffset": 32}, {"referenceID": 24, "context": ", [26, 41, 57].", "startOffset": 2, "endOffset": 14}, {"referenceID": 36, "context": ", [26, 41, 57].", "startOffset": 2, "endOffset": 14}, {"referenceID": 51, "context": ", [26, 41, 57].", "startOffset": 2, "endOffset": 14}, {"referenceID": 10, "context": "Although some prior work does address aspects of the empirical evaluation of classifier security, which is often implicitly defined as the performance degradation incurred under a (simulated) attack, to our knowledge a systematic treatment of this process under a unifying perspective was only first described in our recent work [12].", "startOffset": 329, "endOffset": 333}, {"referenceID": 24, "context": ", in [26, 31, 41, 47, 66]), in which a different application-dependent criteria is separately defined for each endeavor.", "startOffset": 5, "endOffset": 25}, {"referenceID": 28, "context": ", in [26, 31, 41, 47, 66]), in which a different application-dependent criteria is separately defined for each endeavor.", "startOffset": 5, "endOffset": 25}, {"referenceID": 36, "context": ", in [26, 31, 41, 47, 66]), in which a different application-dependent criteria is separately defined for each endeavor.", "startOffset": 5, "endOffset": 25}, {"referenceID": 41, "context": ", in [26, 31, 41, 47, 66]), in which a different application-dependent criteria is separately defined for each endeavor.", "startOffset": 5, "endOffset": 25}, {"referenceID": 60, "context": ", in [26, 31, 41, 47, 66]), in which a different application-dependent criteria is separately defined for each endeavor.", "startOffset": 5, "endOffset": 25}, {"referenceID": 28, "context": "For instance, in [31], the authors showed how camouflage network packets can mimic legitimate traffic to evade detection; and, similarly, in [26, 41, 47, 66], the content of spam emails was manipulated for evasion.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "For instance, in [31], the authors showed how camouflage network packets can mimic legitimate traffic to evade detection; and, similarly, in [26, 41, 47, 66], the content of spam emails was manipulated for evasion.", "startOffset": 141, "endOffset": 157}, {"referenceID": 36, "context": "For instance, in [31], the authors showed how camouflage network packets can mimic legitimate traffic to evade detection; and, similarly, in [26, 41, 47, 66], the content of spam emails was manipulated for evasion.", "startOffset": 141, "endOffset": 157}, {"referenceID": 41, "context": "For instance, in [31], the authors showed how camouflage network packets can mimic legitimate traffic to evade detection; and, similarly, in [26, 41, 47, 66], the content of spam emails was manipulated for evasion.", "startOffset": 141, "endOffset": 157}, {"referenceID": 60, "context": "For instance, in [31], the authors showed how camouflage network packets can mimic legitimate traffic to evade detection; and, similarly, in [26, 41, 47, 66], the content of spam emails was manipulated for evasion.", "startOffset": 141, "endOffset": 157}, {"referenceID": 10, "context": "This shortcoming highlights the need for a more general set of security guidelines and a more systematic definition of classifier security evaluation, that we began to address in [12].", "startOffset": 179, "endOffset": 183}, {"referenceID": 2, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 15, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 24, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 33, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 35, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 37, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 40, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 47, "context": "Apart from application-specific work, several theoretical models of adversarial learning have been proposed [4, 17, 26, 36, 40, 42, 46, 53].", "startOffset": 108, "endOffset": 139}, {"referenceID": 2, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 3, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 33, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 34, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 35, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 37, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 55, "endOffset": 77}, {"referenceID": 10, "context": "In particular, we build upon elements of the models of [4, 5, 36, 38, 40, 42], which were used in defining our framework for security evaluation [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 2, "context": "A taxonomy of potential attacks against pattern classifiers was proposed in [4, 5, 36] as a baseline to characterize attacks on learners.", "startOffset": 76, "endOffset": 86}, {"referenceID": 3, "context": "A taxonomy of potential attacks against pattern classifiers was proposed in [4, 5, 36] as a baseline to characterize attacks on learners.", "startOffset": 76, "endOffset": 86}, {"referenceID": 33, "context": "A taxonomy of potential attacks against pattern classifiers was proposed in [4, 5, 36] as a baseline to characterize attacks on learners.", "startOffset": 76, "endOffset": 86}, {"referenceID": 12, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 46, "endOffset": 66}, {"referenceID": 34, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 46, "endOffset": 66}, {"referenceID": 35, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 46, "endOffset": 66}, {"referenceID": 46, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 46, "endOffset": 66}, {"referenceID": 52, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 46, "endOffset": 66}, {"referenceID": 24, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 110, "endOffset": 130}, {"referenceID": 28, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 110, "endOffset": 130}, {"referenceID": 36, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 110, "endOffset": 130}, {"referenceID": 41, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 110, "endOffset": 130}, {"referenceID": 60, "context": "Examples of causative attacks include work in [14, 38, 40, 52, 58], while exploratory attacks can be found in [26, 31, 41, 47, 66].", "startOffset": 110, "endOffset": 130}, {"referenceID": 2, "context": "[4] and here we outline these with respect to a PDF malware detector.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Here, we summarize our recent work [12] that proposes a new framework for designing proactive secure classifiers by addressing the shortcomings of the reactive security cycle raised above.", "startOffset": 35, "endOffset": 39}, {"referenceID": 50, "context": "This amounts to performing a more systematic what-if analysis of classifier security [56].", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "3 [4, 5, 36].", "startOffset": 2, "endOffset": 12}, {"referenceID": 3, "context": "3 [4, 5, 36].", "startOffset": 2, "endOffset": 12}, {"referenceID": 33, "context": "3 [4, 5, 36].", "startOffset": 2, "endOffset": 12}, {"referenceID": 3, "context": "[5] and extended by Huang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "[36], the adversary\u2019s goal should be defined based on the anticipated security violation, which might be an integrity, availability, or privacy violation (see Section 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "Further, as suggested by Laskov and Kloft [42] and Kloft and Laskov [40], the adversary\u2019s goal should be defined in terms of an objective function that the adversary is willing to maximize.", "startOffset": 42, "endOffset": 46}, {"referenceID": 35, "context": "Further, as suggested by Laskov and Kloft [42] and Kloft and Laskov [40], the adversary\u2019s goal should be defined in terms of an objective function that the adversary is willing to maximize.", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "For instance, in an indiscriminate integrity attack, the adversary may aim to maximize the number of spam emails that evade detection, while minimally manipulating their content [26, 46, 53], whereas in an indiscriminate availability attack, the adversary may aim to maximize the number of classification errors, thereby causing a general denial-of-service due to an excess of false alarms [52, 14].", "startOffset": 178, "endOffset": 190}, {"referenceID": 40, "context": "For instance, in an indiscriminate integrity attack, the adversary may aim to maximize the number of spam emails that evade detection, while minimally manipulating their content [26, 46, 53], whereas in an indiscriminate availability attack, the adversary may aim to maximize the number of classification errors, thereby causing a general denial-of-service due to an excess of false alarms [52, 14].", "startOffset": 178, "endOffset": 190}, {"referenceID": 47, "context": "For instance, in an indiscriminate integrity attack, the adversary may aim to maximize the number of spam emails that evade detection, while minimally manipulating their content [26, 46, 53], whereas in an indiscriminate availability attack, the adversary may aim to maximize the number of classification errors, thereby causing a general denial-of-service due to an excess of false alarms [52, 14].", "startOffset": 178, "endOffset": 190}, {"referenceID": 46, "context": "For instance, in an indiscriminate integrity attack, the adversary may aim to maximize the number of spam emails that evade detection, while minimally manipulating their content [26, 46, 53], whereas in an indiscriminate availability attack, the adversary may aim to maximize the number of classification errors, thereby causing a general denial-of-service due to an excess of false alarms [52, 14].", "startOffset": 390, "endOffset": 398}, {"referenceID": 12, "context": "For instance, in an indiscriminate integrity attack, the adversary may aim to maximize the number of spam emails that evade detection, while minimally manipulating their content [26, 46, 53], whereas in an indiscriminate availability attack, the adversary may aim to maximize the number of classification errors, thereby causing a general denial-of-service due to an excess of false alarms [52, 14].", "startOffset": 390, "endOffset": 398}, {"referenceID": 26, "context": "The adversary\u2019s knowledge of the attacked system can be defined based on the main components involved in the design of a machine learning system, as described in [29] and depicted in Figure 3.", "startOffset": 162, "endOffset": 166}, {"referenceID": 26, "context": "3 A representation of the design steps of a machine learning system [29] that may provide sources of knowledge for the adversary.", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "Although potentially too pessimistic, this worst-case setting allows one to compute a lower bound on the classifier performance when it is under attack [26, 41].", "startOffset": 152, "endOffset": 160}, {"referenceID": 36, "context": "Although potentially too pessimistic, this worst-case setting allows one to compute a lower bound on the classifier performance when it is under attack [26, 41].", "startOffset": 152, "endOffset": 160}, {"referenceID": 40, "context": "v), either to directly find optimal or nearly-optimal attack instances [46, 53], or to learn a surrogate classifier, which can then serve as a template to guide the attack against the actual classifier.", "startOffset": 71, "endOffset": 79}, {"referenceID": 47, "context": "v), either to directly find optimal or nearly-optimal attack instances [46, 53], or to learn a surrogate classifier, which can then serve as a template to guide the attack against the actual classifier.", "startOffset": 71, "endOffset": 79}, {"referenceID": 2, "context": "3 [4, 5, 36], the adversary may control both training and test data (causative attacks), or only on", "startOffset": 2, "endOffset": 12}, {"referenceID": 3, "context": "3 [4, 5, 36], the adversary may control both training and test data (causative attacks), or only on", "startOffset": 2, "endOffset": 12}, {"referenceID": 33, "context": "3 [4, 5, 36], the adversary may control both training and test data (causative attacks), or only on", "startOffset": 2, "endOffset": 12}, {"referenceID": 2, "context": ", the attack influence from the taxonomy in [4, 5, 36]); (c.", "startOffset": 44, "endOffset": 54}, {"referenceID": 3, "context": ", the attack influence from the taxonomy in [4, 5, 36]); (c.", "startOffset": 44, "endOffset": 54}, {"referenceID": 33, "context": ", the attack influence from the taxonomy in [4, 5, 36]); (c.", "startOffset": 44, "endOffset": 54}, {"referenceID": 12, "context": "[14] the attack samples had to be injected into the training data, and each attack sample depended on the current training data, which also included past attack samples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "3 See [12] for more details on the definition of the data distribution and the resampling algorithm.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "hypothesize an attack scenario by identifying a proper adversary\u2019s goal, and according to the taxonomy in [4, 5, 36]; 2.", "startOffset": 106, "endOffset": 116}, {"referenceID": 3, "context": "hypothesize an attack scenario by identifying a proper adversary\u2019s goal, and according to the taxonomy in [4, 5, 36]; 2.", "startOffset": 106, "endOffset": 116}, {"referenceID": 33, "context": "hypothesize an attack scenario by identifying a proper adversary\u2019s goal, and according to the taxonomy in [4, 5, 36]; 2.", "startOffset": 106, "endOffset": 116}, {"referenceID": 24, "context": "The problem of evasion at test time has been considered in previous work, albeit either limited to simple decision functions such as linear classifiers [26, 46], or to cover any convexinducing classifiers [53] that partition the feature space into two sets, one of which is convex, but do not include most interesting families of non-linear classifiers such as neural nets or SVMs.", "startOffset": 152, "endOffset": 160}, {"referenceID": 40, "context": "The problem of evasion at test time has been considered in previous work, albeit either limited to simple decision functions such as linear classifiers [26, 46], or to cover any convexinducing classifiers [53] that partition the feature space into two sets, one of which is convex, but do not include most interesting families of non-linear classifiers such as neural nets or SVMs.", "startOffset": 152, "endOffset": 160}, {"referenceID": 47, "context": "The problem of evasion at test time has been considered in previous work, albeit either limited to simple decision functions such as linear classifiers [26, 46], or to cover any convexinducing classifiers [53] that partition the feature space into two sets, one of which is convex, but do not include most interesting families of non-linear classifiers such as neural nets or SVMs.", "startOffset": 205, "endOffset": 209}, {"referenceID": 6, "context": "In contrast to this prior work, the methods presented in our recent work [8] and in this section demonstrate that evasion of kernel-based classifiers at test time can be realized with a straightforward gradient-descent-based approach derived from Golland\u2019s technique of discriminative directions [33].", "startOffset": 73, "endOffset": 76}, {"referenceID": 30, "context": "In contrast to this prior work, the methods presented in our recent work [8] and in this section demonstrate that evasion of kernel-based classifiers at test time can be realized with a straightforward gradient-descent-based approach derived from Golland\u2019s technique of discriminative directions [33].", "startOffset": 296, "endOffset": 300}, {"referenceID": 24, "context": "Similarly to previous work, the definition of a suitable distance measure d : X \u00d7X 7\u2192R is left to the specific application domain [26, 46, 53].", "startOffset": 130, "endOffset": 142}, {"referenceID": 40, "context": "Similarly to previous work, the definition of a suitable distance measure d : X \u00d7X 7\u2192R is left to the specific application domain [26, 46, 53].", "startOffset": 130, "endOffset": 142}, {"referenceID": 47, "context": "Similarly to previous work, the definition of a suitable distance measure d : X \u00d7X 7\u2192R is left to the specific application domain [26, 46, 53].", "startOffset": 130, "endOffset": 142}, {"referenceID": 24, "context": "For spam filtering, distance is often given as the number of modified words in each spam [26, 46, 52, 53], since it is assumed that highly modified spam messages are less effectively able to convey the spammer\u2019s message.", "startOffset": 89, "endOffset": 105}, {"referenceID": 40, "context": "For spam filtering, distance is often given as the number of modified words in each spam [26, 46, 52, 53], since it is assumed that highly modified spam messages are less effectively able to convey the spammer\u2019s message.", "startOffset": 89, "endOffset": 105}, {"referenceID": 46, "context": "For spam filtering, distance is often given as the number of modified words in each spam [26, 46, 52, 53], since it is assumed that highly modified spam messages are less effectively able to convey the spammer\u2019s message.", "startOffset": 89, "endOffset": 105}, {"referenceID": 47, "context": "For spam filtering, distance is often given as the number of modified words in each spam [26, 46, 52, 53], since it is assumed that highly modified spam messages are less effectively able to convey the spammer\u2019s message.", "startOffset": 89, "endOffset": 105}, {"referenceID": 49, "context": ", for neural networks, since they directly output a posterior estimate, and for SVMs, since their posterior can be estimated as a sigmoidal function of the distance of x to the SVM hyperplane [55].", "startOffset": 192, "endOffset": 196}, {"referenceID": 28, "context": "The extra component favors attack points to imitate features of known samples classified as legitimate, as in mimicry attacks [31].", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "In this section, we first report some experimental results on the MNIST handwritten digit classification task [32, 45], that visually demonstrate how the proposed algorithm modifies digits to mislead classification.", "startOffset": 110, "endOffset": 118}, {"referenceID": 39, "context": "In this section, we first report some experimental results on the MNIST handwritten digit classification task [32, 45], that visually demonstrate how the proposed algorithm modifies digits to mislead classification.", "startOffset": 110, "endOffset": 118}, {"referenceID": 39, "context": "We first focus on a two-class sub-problem of discriminating between two distinct digits from the MNIST dataset [45].", "startOffset": 111, "endOffset": 115}, {"referenceID": 61, "context": "We focus now on the problem of discriminating between legitimate and malicious PDF files, a popular medium for disseminating malware [67].", "startOffset": 133, "endOffset": 137}, {"referenceID": 43, "context": "Several recent works proposed machine-learning techniques for detecting malicious PDFs use the file\u2019s logical structure to accurately identify the malware [49, 62, 63].", "startOffset": 155, "endOffset": 167}, {"referenceID": 56, "context": "Several recent works proposed machine-learning techniques for detecting malicious PDFs use the file\u2019s logical structure to accurately identify the malware [49, 62, 63].", "startOffset": 155, "endOffset": 167}, {"referenceID": 57, "context": "Several recent works proposed machine-learning techniques for detecting malicious PDFs use the file\u2019s logical structure to accurately identify the malware [49, 62, 63].", "startOffset": 155, "endOffset": 167}, {"referenceID": 43, "context": "[49] in which each feature corresponds to the tally of occurrences of a given keyword in the PDF file.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "Similar feature representations were also exploited in [62, 63].", "startOffset": 55, "endOffset": 63}, {"referenceID": 57, "context": "Similar feature representations were also exploited in [62, 63].", "startOffset": 55, "endOffset": 63}, {"referenceID": 43, "context": "The features (keywords) were extracted from each training set as described in [49]; on average, 100 keywords were found in each run.", "startOffset": 78, "endOffset": 82}, {"referenceID": 44, "context": "We report our results in Figure 6, in terms of the false negative (FN) rate attained by the targeted classifiers as a function of the maximum allowable number of modifications, dmax \u2208 [0,50].", "startOffset": 184, "endOffset": 190}, {"referenceID": 9, "context": "Generative classifiers can be modified, by explicitly modeling the attack distribution, as in [11], and discriminative classifiers can be modified similarly by adding generated attack samples to the training set.", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": ", n-gram analysis [31].", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "A similar technique has been already exploited in to address the pre-image problem of kernel methods [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 40, "context": "Other interesting extensions include (i) considering more effective strategies such as those proposed by [46, 53] to build a small but representative set of surrogate data to learn the surrogate classifier and (ii) improving the classifier estimate \u011d(x); e.", "startOffset": 105, "endOffset": 113}, {"referenceID": 47, "context": "Other interesting extensions include (i) considering more effective strategies such as those proposed by [46, 53] to build a small but representative set of surrogate data to learn the surrogate classifier and (ii) improving the classifier estimate \u011d(x); e.", "startOffset": 105, "endOffset": 113}, {"referenceID": 14, "context": "using an ensemble technique like bagging to average several classifiers [16].", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "Here we present another kind of attack, based on our work in [14].", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 5, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 7, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 35, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 46, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 52, "context": "Examples of these attacks, besides our work [14], can be found in [13, 7, 9, 39, 40, 52, 58].", "startOffset": 66, "endOffset": 92}, {"referenceID": 5, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 112, "endOffset": 127}, {"referenceID": 35, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 112, "endOffset": 127}, {"referenceID": 52, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 112, "endOffset": 127}, {"referenceID": 5, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 144, "endOffset": 151}, {"referenceID": 46, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 144, "endOffset": 151}, {"referenceID": 7, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 203, "endOffset": 210}, {"referenceID": 11, "context": "They include specific application examples in different areas, such as intrusion detection in computer networks [7, 39, 40, 58], spam filtering [7, 52], and, most recently, even biometric authentication [9, 13].", "startOffset": 203, "endOffset": 210}, {"referenceID": 18, "context": "After each update of the attack point xp, the optimal solution can be efficiently recomputed from the solution on Dtr, using the incremental SVM machinery [20].", "startOffset": 155, "endOffset": 159}, {"referenceID": 18, "context": "3: repeat 4: Re-compute the SVM solution on Dtr\u222a{x,y} using the incremental SVM [20].", "startOffset": 80, "endOffset": 84}, {"referenceID": 18, "context": "This can expressed as an adiabatic update condition using the technique introduced in [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 42, "context": "Solving these equations and computing an inverse matrix via the Sherman-MorrisonWoodbury formula [48] yields the following gradients:", "startOffset": 97, "endOffset": 101}, {"referenceID": 29, "context": "The experimental evaluation presented in the following sections demonstrates the behavior of our proposed method on an artificial two-dimensional dataset and evaluates its effectiveness on the classical MNIST handwritten digit recognition dataset [32, 45].", "startOffset": 247, "endOffset": 255}, {"referenceID": 39, "context": "The experimental evaluation presented in the following sections demonstrates the behavior of our proposed method on an artificial two-dimensional dataset and evaluates its effectiveness on the classical MNIST handwritten digit recognition dataset [32, 45].", "startOffset": 247, "endOffset": 255}, {"referenceID": 29, "context": "We now quantitatively validate the effectiveness of the proposed attack strategy on the MNIST handwritten digit classification task [32, 45], as with the evasion attacks in Section 4.", "startOffset": 132, "endOffset": 140}, {"referenceID": 39, "context": "We now quantitatively validate the effectiveness of the proposed attack strategy on the MNIST handwritten digit classification task [32, 45], as with the evasion attacks in Section 4.", "startOffset": 132, "endOffset": 140}, {"referenceID": 12, "context": "The poisoning attack presented in this section, summarized from our previous work in [14], is a first step toward the security analysis of SVM against training data attacks.", "startOffset": 85, "endOffset": 89}, {"referenceID": 53, "context": "We review our recent work [59] deriving mechanisms for releasing SVM classifiers trained on privacy-sensitive data while maintaining the data\u2019s privacy.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "[30], in which a randomized mechanism is said to preserve \u03b2 -differential privacy, if the likelihood of the mechanism\u2019s output changes by at most \u03b2 when a training datum is changed arbitrarily (or even removed).", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "Indeed the notion of differential privacy, as opposed to more syntactic notions of privacy such as k-anonymity [64], was inspired by decades-old work in cryptography that introduced mathematical formalism to an age-old problem, yielding", "startOffset": 111, "endOffset": 115}, {"referenceID": 45, "context": "Given an adversary with such strong knowledge and capabilities as described above, it may seem difficult to provide effective countermeasures particularly considering the complication of abundant access to side information that is often used in publicized privacy attacks [51, 64].", "startOffset": 272, "endOffset": 280}, {"referenceID": 58, "context": "Given an adversary with such strong knowledge and capabilities as described above, it may seem difficult to provide effective countermeasures particularly considering the complication of abundant access to side information that is often used in publicized privacy attacks [51, 64].", "startOffset": 272, "endOffset": 280}, {"referenceID": 53, "context": "We now present results from our recent work that quantifies this effect [59], within the framework of differential privacy.", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "7 The well-established proof technique [30] follows from the definition of the Laplace distribution involving the same norm as used in our measure of global sensitivity, and the triangle inequality: for any training set D , D \u2032 \u223cD , response g \u2208H , and privacy parameter \u03b2", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "data [15], measuring the sensitivity of the SVM appears to be non-trivial owing to the non-linear influence an individual training datum may have on the learned SVM.", "startOffset": 5, "endOffset": 9}, {"referenceID": 55, "context": ", the VC dimension, which is not always possible to control, and for the RBF kernel SVM is infinite) [61].", "startOffset": 101, "endOffset": 105}, {"referenceID": 53, "context": "In recent work [59], we showed how these existing stability measurements for the SVM can be adapted to provide the following L1-global sensitivity bound.", "startOffset": 15, "endOffset": 19}, {"referenceID": 53, "context": "We omit the proof, which is available in the original paper [59] and which follows closely the previous measurements for algorithmic stability.", "startOffset": 60, "endOffset": 64}, {"referenceID": 53, "context": "Hidden in the above is the dependence on n: typically we take C to scale like 1/n to achieve consistency in which case we see that noise decreases with larger training data\u2014 akin to less individual influence\u2014as expected [59].", "startOffset": 220, "endOffset": 224}, {"referenceID": 53, "context": "One approach to quantifying this effect, involves bounding the following notion of utility [59].", "startOffset": 91, "endOffset": 95}, {"referenceID": 53, "context": ", PAC learns) [59].", "startOffset": 14, "endOffset": 18}, {"referenceID": 53, "context": "Using the Chernoff tail inequality and known moment-generating functions, we establish the following bound on the utility of this private SVM [59].", "startOffset": 142, "endOffset": 146}, {"referenceID": 53, "context": "These results can be extended to certain translation-invariant kernels including the infinite-dimensional RBF [59].", "startOffset": 110, "endOffset": 114}, {"referenceID": 19, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "This has been also studied from a more theoretical perspective in [23], exploiting the framework of Robust Statistics [35, 50].", "startOffset": 66, "endOffset": 70}, {"referenceID": 32, "context": "This has been also studied from a more theoretical perspective in [23], exploiting the framework of Robust Statistics [35, 50].", "startOffset": 118, "endOffset": 126}, {"referenceID": 44, "context": "This has been also studied from a more theoretical perspective in [23], exploiting the framework of Robust Statistics [35, 50].", "startOffset": 118, "endOffset": 126}], "year": 2014, "abstractText": "Support Vector Machines (SVMs) are among the most popular classification techniques adopted in security applications like malware detection, intrusion detection, and spam filtering. However, if SVMs are to be incorporated in real-world security systems, they must be able to cope with attack patterns that can either mislead the learning algorithm (poisoning), evade detection (evasion), or gain information about their internal parameters (privacy breaches). The main contributions of this chapter are twofold. First, we introduce a formal general framework for the empirical evaluation of the security of machine-learning systems. Second, according to our framework, we demonstrate the feasibility of evasion, poisoning and privacy attacks against SVMs in real-world security problems. For each attack technique, we evaluate its impact and discuss whether (and how) it can be countered through an adversary-aware design of SVMs. Our experiments are easily reproducible thanks to open-source code that we have made available, together with all the employed datasets, on a public repository. Battista Biggio, Igino Corona, Davide Maiorca, Giorgio Fumera, Giorgio Giacinto, and Fabio Roli Department of Electrical and Electronic Engineering, University of Cagliari, Piazza d\u2019Armi 09123, Cagliari, Italy. e-mail: {battista.biggio,igino.corona,davide.maiorca}@diee.unica.it e-mail: {fumera,giacinto,roli}@diee.unica.it Blaine Nelson Institut f\u00fcr Informatik, Universit\u00e4t Potsdam, August-Bebel-Stra\u00dfe 89, 14482 Potsdam, Germany. e-mail: blaine.nelson@gmail.com Benjamin I. P. Rubinstein IBM Research, Lvl 5 / 204 Lygon Street, Carlton, VIC 3053, Australia. e-mail: ben@bipr.net 1 ar X iv :1 40 1. 77 27 v1 [ cs .L G ] 3 0 Ja n 20 14 2 Biggio, Corona, Nelson, Rubinstein, Maiorca, Fumera, Giacinto, Roli", "creator": "LaTeX with hyperref package"}}}