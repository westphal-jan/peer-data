{"id": "1402.0563", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Feb-2014", "title": "Evaluating Indirect Strategies for Chinese-Spanish Statistical Machine Translation", "abstract": "although, chinese and spanish are two of the most spoken languages in the world, not much research has been primarily done in machine translation for this language pair. this paper focuses on investigating the state - of - the - art of chinese - to - spanish statistical search machine translation ( smt ), which established nowadays is one of the most popular approaches to machine translation. for this purpose, we report details of the available parallel corpus which are basic traveller expressions corpus ( btec ), holy tree bible and our united nations ( un ). additionally, we conduct experimental work with the largest of these three corpora to explore alternative smt strategies by means of using a pivot language. three alternatives options are considered for pivoting : cascading, pseudo - corpus and triangulation. as pivot language, we use { either english, arabic or french. results show that, for a phrase - based smt system, english is the best pivot producing language between chinese and spanish. we propose a system output combination using... the pivot strategies which is capable of outperforming the direct translation strategy.... the main objective of this work is motivating and involving the research community to work in this important pair of languages given their demographic impact.", "histories": [["v1", "Tue, 4 Feb 2014 01:34:56 GMT  (305kb)", "http://arxiv.org/abs/1402.0563v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marta r costa-juss\\`a", "carlos a henr\\'iquez", "rafael e banchs"], "accepted": false, "id": "1402.0563"}, "pdf": {"name": "1402.0563.pdf", "metadata": {"source": "CRF", "title": "Evaluating Indirect Strategies for Chinese\u2013Spanish Statistical Machine Translation", "authors": ["Marta R. Costa-juss\u00e0", "Rafael E. Banchs"], "emails": ["vismrc@i2r.a-star.edu.sg", "carlos.henriquez@upc.edu", "rembanchs@i2r.a-star.edu.sg"], "sections": [{"heading": "1. Introduction", "text": "Chinese and Spanish are very distant languages in many aspects. However, they come close together in the ranking of most spoken languages in the world (Ethnologue, 2012). In the Web 2.0 era, in which most of the content is produced by the users, the number of native speakers is an excellent indicator of the actual relevance of machine translation between two languages. Of course, other factors such as literacy, amount of text published and strength of commercial relationships are also to be taken into account, but these factors will actually support further our idea of the strategic importance of developing machine translation technologies between Chinese and Spanish. The huge increase in volume of online contents in Chinese during the last years, as well as the steady increase of commercial relationships between Spanish speaking Latin American countries and China are just two basic examples supporting this fact. Needless to say, these languages involve many economical interests\nc\u00a92012 AI Access Foundation. All rights reserved.\n(Zapatero, 2010). Nevertheless, these two languages seem to become far apart again when looking for bilingual resources.\nWe have been recently interested in gathering and collecting Chinese\u2013Spanish bilingual resources for research and machine translation application purposes. The amount of bilingual resources that are currently available for this specific language pair is surprisingly low. Similarly, the related amount of work we have found, within the computational linguistic community, can be reduced to a very small set of references (Banchs, Crego, Lambert, & Marin\u0303o, 2006; Banchs & Li, 2008; Bertoldi, Cattoni, Federico, & Barbaiani, 2008; Wang, Wu, Hu, Liu, Li, Ren, & Niu, 2008). Apart from the Btec1 corpus available through International Workshop on Spoken Language Translation (Iwslt) competition (Bertoldi et al., 2008) and Holy Bible datasets (Banchs & Li, 2008), we were not aware of any other Chinese\u2013 Spanish parallel corpus suitable for training phrase-based (Koehn, Och, & Marcu, 2003)2 statistical machine translation systems between these two languages, until a six-language parallel corpus (including both Chinese and Spanish) from United Nations was released for research purposes (Rafalovitch & Dale, 2009).\nUsing the recently released United Nations parallel corpus as a starting point, this work focuses on the problem of developing Chinese-to-Spanish phrase-based machine translation technologies with a limited set of bilingual resources. We explore and evaluate different alternatives for the problem in hand by means of pivot-language strategies through other languages available in the United Nations parallel corpus, such as Arabic, English and French 3. Existing strategies such as system cascading, pseudo-corpus generation and triangulation are implemented and compared against a baseline system built with a direct translation approach. As follows, we briefly describe these pivot approaches:\n\u2022 The cascaded approach generates Chinese-to-Spanish translations by concatenating a system that translates Chinese into a pivot language with a system that translates from the pivot language into Spanish.\n\u2022 The pseudo-corpus approach builds a synthetic Chinese\u2013Spanish corpus either by translating into Spanish the pivot side of a Chinese\u2013pivot corpus or by translating into Chinese the pivot side of a Pivot\u2013Spanish corpus.\n\u2022 The triangulation approach implements a Chinese-to-Spanish translation system by combining the translation table probabilities of a Chinese\u2013pivot system and a Pivot\u2013 Spanish system.\nAdditionally, we implement and evaluate a system combination of the three pivot strategies based on the minimum Bayes risk (Mbr) (Kumar & Byrne, 2004) technique. Such a combination strategy is capable of outperforming the direct system.\nBesides experimenting with different pivot languages to compare the mentioned approaches, we also wanted to determine which pivot alone gives the best results and why.\n1. Basic Traveller Expressions Corpus. 2. Note that phrase-based is commonly used to refer to statistical machine translation systems, in which\nthe term phrase refers to segments of one or more than one word and it does not have the usual meaning of multi-word syntactical consitutent, as it has in linguistics. 3. Although Russian is available in the Un corpus, we discard to use it because we do not have the proper preprocessing tools for it.\nHence, we present a short comparison of the amount of reordering and vocabulary sizes of pivot languages, following the study presented by Birch et al. (2008) where they identified these two properties as key elements for predicting machine translation quality. The results from such comparisons, together with the translation quality obtained in the different approaches, show that English was the best pivot language for Chinese-to-Spanish translation purposes in our experimental framework.\nThe paper is structured as follows. Section 2 motivates this work which is intended to bring some light into the investigation of Chinese-to-Spanish translation task. Section 3 presents some related work in the Chinese-to-Spanish translation task. Section 4 reports the details of the main parallel corpora available for this translation task. Next, section 5 describes the main strategies for performing Chinese-to-Spanish translation which are tested in this work: direct, cascade, pseudo-corpus and triangulation. Section 6 presents the evaluation framework which includes the corpus statistics, the system and evaluation details. Then, section 7 reports the experiments (including the system combination) and the results. Finally, section 8 concludes our work and proposes new research directions in the area."}, {"heading": "2. Motivation", "text": "Although some current web translation systems allow for performing translations between Chinese and Spanish, the quality of current Chinese-to-Spanish translations is still well below the quality achieved for other language pairs, such as English to Spanish. As far as we know, there is not much research in this translation task. The main reason may be the lack of parallel corpora. This study intends to make progress and involve other researchers in the area of Chinese\u2013Spanish statistical machine translation by:\n1. Listing the available parallel corpora for Chinese\u2013Spanish.\n2. Comparing different methodologies for performing statistical machine translation: cascaded (Wang et al., 2008), pseudo-corpus generation (Banchs et al., 2006; de Gispert & Marin\u0303o, 2006) and triangulation (Wu & Wang, 2007).\n3. Evaluating which is the best language (among Arabic, English and French) for generating the cascade, pseudo-corpus or triangulation Mt between Chinese\u2013Spanish.\n4. Performing an output system combination to explore new ways of improving Chineseto-Spanish translation."}, {"heading": "3. Related Work", "text": "One of the first works dealing with Chinese\u2013Spanish statistical machine translation was presented by Banchs et al. (2006). Authors experimented with two independent corpora Chinese\u2013English and English\u2013Spanish to translate from Chinese to Spanish. They built their translation systems using the so-called Ngram-based approach, which differs from the phrase-based system mainly in the translation and reordering model (Marin\u0303o, Banchs, Crego, de Gispert, Lambert, Fonollosa, & Costa-jussa\u0300, 2006).\nThe only research event recently performed for this language pair was the 2008 Iwslt evaluation campaign (Paul, 2008). This evaluation organized two Chinese-to-Spanish tracks. One of them focused on direct translation and the other one on pivot translation through English. The best translation results accordingly to the manual evaluation were obtained by far in the pivot task.\nThe best systems in both tracks were developed by Wang et al. (2008). Regarding the direct system, they used a standard phrase-based Smt system. What makes it different from the other participating systems is that they provide their own Chinese segmentation and the Ldc (Linguistic Data Consortium) bilingual dictionary. Regarding the pivot task, they compared two different approaches. The first one, referred to as triangulation, consisted of training two translation models on the Chinese\u2013English corpus and English\u2013Spanish corpus, and then building a new translation model for Chinese\u2013Spanish translation by combining the two previous models as proposed by Wu & Wang (2007); the second one obtained better results and it was based on a cascaded approach. The idea here is to translate from Chinese into English and then from English into Spanish, which means performing two translations.\nOther participants also proposed the cascaded methodology. This approximation can be done with the n-best translations (Khalilov, Costa-Jussa\u0300, Henr\u0301\u0131quez, Fonollosa, Herna\u0301ndez, Marin\u0303o, Banchs, Chen, Zhang, Aw, & Li, 2008).\nAnother proposal was to generate pseudo-corpus which means to translate either the English into Chinese or into Spanish, creating a parallel Chinese\u2013Spanish corpus. This pseudo-corpus is used to train the Chinese\u2013Spanish translation (Bertoldi et al., 2008).\nAs mentioned aboved, the comparison performed by Wang et al. (2008) showed that the cascaded approach performed better than the phrase-table combination for the Chinese\u2013 Spanish pivot task.\nFinally, our previous work (Costa-jussa\u0300, Henr\u0301\u0131quez, & Banchs, 2011b) compared two standard pivot approaches (pseudo-corpus and cascaded) using English and the direct system. Experiments in this work showed that the quality between the direct system and the pivot systems did not differ much. Additionally, the cascaded system presented slightly better results than the pseudo-corpus system. In our other previous work (Costa-jussa\u0300, Henr\u0301\u0131quez, & Banchs, 2011a), we compared again two pivot approaches (pseudo-corpus and cascaded) using Arabic, French and English as pivot languages and the direct system. We concluded that English was the best pivot language.\nIn the present work, we are extending the two previous studies by: (1) using more pivot strategies (including the triangulation strategy); (2) introducing a measure to pre-evaluate the quality of pivot approaches; (3) extending the pivot combination experiments; and (4) providing further evaluation.\nNote that we are working with the United Nations (Un) corpus rather than with the Btec corpus (the one used in the Iwslt). The former is freely available and larger than the latter."}, {"heading": "4. Chinese\u2013Spanish Parallel Corpora", "text": "There are very limited resources for the language pair Chinese\u2013Spanish in comparison to the number of native speakers in these languages. In practice, it is also common to translate Chinese into Spanish through English even when manual translations are conducted.\nAs parallel corpus at the sentence level, there is the Basic Travel Expressions Corpus (Btec) (Paul, Yamamoto, Sumita, & Nakamura, 2009), which is a collection of sentences that bilingual travel experts consider useful for people going to or coming from another country. This corpus contains around 160,000 parallel sentences but only around 20,000 sentences and 180,000 words are actively used for Mt purposes in the Iwslt evaluation campaign. The full corpus is not freely available, and the 20,000 version was only available for participation purposes in the 2008 Iwslt evaluation campaign.\nAnother parallel corpus is the Holy Bible, which has been proved to be a good resource for CLIR (Cross-language information retrieval) (Chew, Verzi, Bauer, & McClain, 2006). This corpus contains around 28,000 parallel sentences and around 800,000 tokens per language. The main advantages of using this corpus is that it is the world\u2019s most translated book; it covers a variety of literary styles including narrative, poetry, and correspondence; great care is taken over the translations; and, perhaps surprisingly, its vocabulary appears to have a high rate of coverage (as much as 85%) of modern-day language.\nFinally, there is the United Nations multilanguage corpus (Rafalovitch & Dale, 2009), which is freely available online for research purposes. Among others, it contains parallel texts at the sentence level in the following languages: Chinese, English, Spanish, French and Arabic. It consists of 2100 United Nations General Assembly resolutions with translation in the six official languages of the United Nations, with average of around 3 million tokens per language. This is the material that we are using in this work. Table 1 shows the statistics of the three different corpora with their corresponding languages.\nAdditionally, we can surf the web and find several publications which are available both in Chinese and Spanish e.g. Global Asia Magazine (2012), but this additional material consists mainly of comparable corpora rather than parallel corpora. This comparable material cannot directly be used in a statistical machine translation system. However, there are many nice algorithms which can extract parallel corpora from comparable corpora (Moore, 2002; Senrich, 2010; Abdul-Rauf, Fishel, Lambert, Noubours, & Sennrich, 2012)."}, {"heading": "5. Direct and Pivot Statistical Machine Translation Approaches", "text": "There are several strategies that we can follow when translating a pair of languages in statistical machine translation (Smt). In this section we present the details of the ones we are using in this work.\nIn general, a statistical machine translation system relies on the translation of a source language sentence s into a target language sentence t\u0302. Among all possible target language sentences t we choose the one with the highest probability, as show in equation (2):\nt\u0302 = arg max t\n[P (t|s)] (1)\n= arg max t\n[P (t)P (s|t)] (2)\nThis probability decomposition based on Bayes\u2019 theorem is known as the source-channel approach to statistical machine translation (Brown, Cocke, Della Pietra, Della Pietra, Jelinek, Lafferty, Mercer, & Roossin, 1990). It allows to model independently the target language model P (t) and the source translation model P (s|t). On the one hand, the translation model weights how likely words in the foreign language are translation of words in the source language; the language model, on the other hand, measures the fluency of hypothesis t\u0302. The search process is represented as the arg max operation.\nLater on, a variation was proposed by Och & Ney (2002) named log-linear model. It allows using more than two models or features and to weight them independently as can be seen in equation (3):\nt\u0302 = arg max t\n[ M\u2211\nm=1\n\u03bbmhm(s, t)\n] (3)\nThis equation should be interpreted as a maximum-entropy framework. We see that eq. (2) is a special case of eq. (3). In fact, it is the logarithm of (2) which would be similar to (3). Then, we have to identify h1(s, t) with log(p(t)) and h2(s, t) with log(p(s|t)), and taking M = 2 (two models) and \u03bb1 = \u03bb2 = 1. In the general case, \u03bb\u2019s are obtained by maximizing an objective function on a held-out set (development set).\nAmong the additional features that can be used with the log-linear model we have lexical models, word bonus, and the reordering model. The lexical models are particularly useful in cases where the translation model may be sparse. For example, for phrases which may have appeared few times the translation model probability may not be well estimated. Then, the lexical models provide a probability among words (Koehn et al., 2003). The word bonus is used to compensate the language model which benefits shorter outputs. The reordering model is used to provide reordering between phrases. If not, reordering would only be treated internally in each phrase. Finally, it should be mentioned that the name log-linear is clearly a misnomer as many of these features are not logarithms at all.\nAs regards the reordering model, the standard way of implementing it is with a distancebased model that gives a linear cost depending on the reordering distance. For instance, if consecutive target words t1, t2 come from translating source words s1 and s5, where the sub-scripts indicate the word position in their corresponding sentences, then a movement\nof d = 5 \u2212 1 = 4 words has taken place and its cost should be double than a movement of d = 2 words. A visual representation of these phrases can be seen in Figure 1\nBesides the traditional distance-based reordering mentioned before, state-of-the-art systems implement an additional lexicalized reordering model (Tillman, 2004). The lexicalized reordering model classifies phrases by the movement they make relative to the previous used phrase, i.e., for each phrase the model learns how likely it is followed by the previous phrase (monotone), swapped with it (swap) or not connected at all (discontinuous). For instance, considering again sub-scripts as word positions in their corresponding sentences, in Figure 1 the bilingual phrases (s1 \u2192 t1) and (s5 \u2192 t2) are not connected, (s7 \u2192 t6) is followed by (s6 \u2192 t5) and (s2 \u2192 t4) is swapped with (s3 s4 \u2192 t3)."}, {"heading": "5.1 Direct System", "text": "Our direct system uses the phrase-based translation approach (Koehn et al., 2003). The basic idea is to segment the given source sentence s into segments of one or more words, then each source segment is translated using a bilingual phrase obtained from the training corpus and finally compose the target sentence from these phrase translations. A bilingual phrase is a pair of m source words and n target words extracted from a parallel sentence that belongs to a bilingual corpus previously aligned by words. For extraction, we consider the words that are consecutive in both source and target sides and which are consistent with the word alignment. We consider a phrase is consistent with the word alignment if no word inside the phrase is aligned with one word outside the phrase.\nRegarding the segmentation of the sentence in K phrases, we assume that all possible segmentations (which are considered as a hidden variable M) have the same probability \u03b1(t):\nP (s|t) = \u2211 M P (s,M |t) (4)\n= \u2211 M P (M |t)P (s\u0304|t\u0304) (5)\n= \u03b1(t) \u2211 M P (s\u0304|t\u0304) (6)\nThen, we consider only monotone translations so the phrase s\u0304k is produced by t\u0304k (Zens, Och, & Ney, 2002).\nP (s\u0304|t\u0304) = K\u220f k=1 p(s\u0304k, t\u0304k) (7)\nFinally, phrase translation probabilities are estimated as relative frequencies over all bilingual phrases in the corpus.\np (s|t) = N (s, t) N (t)\n(8)\nwhere N (s, t) counts the number of times the phrase s is translated as t and N (t) the number of times the phrase in the target language appears in the training corpus."}, {"heading": "5.2 Pivot-Based Systems", "text": "The cascaded approach handles the source\u2013pivot and the pivot\u2013target system independently. They are both built and tuned to improve their local translation quality and then composed to translate from the source language to the target language in two steps: first, the translation output from source to pivot is computed and then it is used to obtain the target translation output.\nThe pseudo-corpus approach translates the pivot section of the source\u2013pivot parallel corpus to the target language using a pivot\u2013target system built previously. Then, a source\u2013target Smt system is built using the source side and the translated pivot side of the source\u2013pivot corpus. The pseudo-corpus system is tuned using an original source\u2013target development corpus, since we have it available.\nThe triangulation approach combines the source\u2013pivot (P (s|p) and P (p|s)) and pivot\u2013 target (P (p|t) and P (t|p)) relative frequencies following the strategy proposed by Cohn & Lapata (2007) in order to build a source\u2013target translation model. The translation probabilities are computed assuming the independence between the source and target phrases when given the pivot phrase.\nP (s|t) = \u2211 p P (s|p)P (p|t) (9)\nP (t|s) = \u2211 p P (t|p)P (p|s) (10)\nwhere s, t, and p represent phrases in the source, target and pivot language respectively.\nThe lexical weights are computed in a similar manner, following the strategy proposed by Cohn & Lapata (2007). This approach does not handle the lexicalized reordering and the other pivot strategies and therefore represents a limitation in its potential. Instead, a simple distance-based reordering is applied during decoding. This model gives a cost linear to the reordering distance. For instance, skipping over two words costs twice as much as skipping over one word.\nOnce the corresponding translation model have been obtained, the source\u2013target system is tuned using the same original source\u2013target development corpus mentioned in the previous approach."}, {"heading": "6. Evaluation Framework", "text": "The following section introduces the details of the evaluation framework. We report the statistics of the Un corpus, a description of how we built the systems and the evaluation details."}, {"heading": "6.1 Corpus Statistics", "text": "As far as we know, and as discussed in section 4, three parallel corpora are available for the Chinese\u2013Spanish language pair: Btec, Holy Bible and Un.4 The former was used in the 2008 Iwslt and complete experiments of pivot strategies are reported in works such as Bertoldi et al. (2008). The Holy Bible was used for the similar purposes by Henr\u0301\u0131quez, Banchs & Marin\u0303o (2010).\nIn this study we decide to use the Un corpus taking advantage of the fact that it is the largest corpus (among those three) and it contains the same sentences in six languages, therefore we can experiment with different pivot languages.\nWhen experimenting with different pivot languages, in order to make the systems as comparable as possible, we first did a sentence selection over the corpus so all systems were built exactly with the same training, tuning and testing sets. This selection process was as follows:\n1. All corpora were tokenized, using the standard tokenizer available in Moses (Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, & Herbst, 2007) for Spanish, English and French; ictclass (Zhang, Yu, Xiong, & Liu, 2003) for Chinese; and Mada+Tokan (Habash & Rambow, 2005) for Arabic.\n2. The Spanish, English and French corpora were lowercased.\n3. If a sentence had more than 100 words in any language, it was deleted from all corpora.\n4. If a sentence pair had a word ratio larger than three for any Chinese\u2013pivot or pivot\u2013 Spanish parallel corpora, it was deleted from all corpora.\n5. To extract the tuning and test sets we identified all sentences ocurring once in the corpora for all languages. The tuning and testing sets were drawn over these sentences to assure they do not appear in the training corpus. Additionally, from these sentences, we want to select those which differ most from the sentences in the training set and which have the lowest out-of-vocabulary rate. In order to do this, the perplexity over the English language model was computed on a sentence-by-sentence basis by using a leave-one-out strategy; then, we selected the two thousand sentences which had the highest perplexity and the lowest ratio of out-of-vocabulary words for constructing the tuning and testing sets. The highest perplexity criterion was used in order to avoid that tuning and test sentences were similar from the ones in the training set. The lowest out-of-vocabulary words criterion was used to minimize the number of outof-vocabulary words in the tuning and test translation. The two criteria were used\n4. During the review process of this paper, we have been aware of a new corpus KDE (K Desktop Environment), which is available from the recent OPUS project 5\nsequentially, first we selected the sentences with the highest perplexity, and among them, we selected those with the lowest ratio of out-of-vocabulary words.\nTable 2 shows the main statistics for all corpora used once divided for experimentation."}, {"heading": "6.2 System Implementation and Evaluation Details", "text": "Our systems were build using revision 4075 of Moses (Koehn et al., 2007). For all systems, we used the default Moses parameters which includes the grow-diagonal-final-and word alignment symmetrization, the lexicalized reordering (where possible), relative frequencies, lexical weights and phrase bonus for the translation model (with phrases up to length 10), a 5-gram language model using Kneser-Ney smoothing and a word penalty model. Therefore, 14 different features are combined in equation (3). The language model was built using Srilm (Stolcke, 2002) version 1.5.12. The optimization was done using Mert (Och, 2003). For word aligning we used Giza++ (Och & Ney, 2000) version 1.0.5.\nIn order to evaluate the translation quality, we used Bleu (Papineni, Roukos, Ward, & Zhu, 2001), Ter (Snover, Dorr, Schwartz, Micciulla, & Makhoul, 2006) and Meteor (Banerjee & Lavie, 2005) automatic evaluation metrics.\nAdditionally, significance tests were performed to study when a system was better than the other. These tests followed the \u201cpair bootstrap resampling\u201d method presented by Koehn (2004): Given two translation outputs coming from two different systems, we created two new virtual test sets by drawing sentences with replacement from the translation outputs. Once we obtained them, we computed their Bleus and observed which system performs better. This procedure was repeated 1, 000 times. At the end, if one of the systems outperformed the other 99% of the time, we concluded that it was indeed a better Bleu score with 99% statistical significance."}, {"heading": "7. Chinese\u2013Spanish Machine Translation Strategies", "text": "Given the different languages available in the Un corpora, we tested three different language pivots. Additionally, we compared the cascaded, pseudo-corpus and triangulation pivot strategies. Finally, we tried to combine the system outputs to improve the translation."}, {"heading": "7.1 Experimenting with Different Pivot Languages", "text": "We built and compared several translation approaches in order to study the impact of the different pivot languages when translating from Chinese into Spanish. Moreover, we evaluated how the quality of pivot approaches differs from a direct system. We built the pivot systems using five of the languages available in the Un parallel corpus: English, Spanish, Chinese, Arabic and French, and we built the direct system on a Chinese\u2013Spanish parallel corpus.\nIn particular, we experimented with the following Chinese-to-Spanish systems: the direct Chinese-to-Spanish system as a quality upper bound; three cascaded, three pseudo-corpus and three triangulation approaches, using English, Arabic and French as pivots. In order to build the pivot systems, we need the corresponding Chinese\u2013pivot and pivot\u2013Spanish systems.\nTable 3 shows the Bleu, Ter and Meteor scores achieved with the intermediate systems trained with the Un Corpus that were later used to built the different pivot approaches. Meteor score for the Chinese-to-Arabic system is not shown as we did not have the postprocessing tools required for the language.\nTable 4 shows the results for our Chinese-to-Spanish configurations with the Un corpus. We can see there that the best pivot system used the pseudo-corpus approach with English as the pivot language.\nIn Chinese-to-Spanish, the fact that the pseudo-corpus through English outperforms cascaded through English according to the Bleu score is not statistically significant, with a 99% confidence (Koehn, 2004). These results, however, are coherent with previous works using the same language pair (Bertoldi et al., 2008; Henr\u0301\u0131quez Q. et al., 2010) that also reported the pseudo-corpus strategy was better than the cascaded strategy. The cascaded and pseudo-corpus approaches through English are statistically significantly better than the triangulation approach, with a 99% confidence. To the best of our knowledge, reasons why one pivot approach is better than the other are not reported in the literature. Moreover, given that difference among approaches such as pseudo-corpus as cascaded approaches is\nnot significant, it is better to perform experiments for each particular task and language pair.\nIn all three approaches, according to the scores in table 4 English is the best pivot language, with a statistical signicance of 99%, which is coherent with the pivot\u2013Spanish results in table 3.\nAs follows, we use a procedure to predict the most suitable pivot language and justify why a language may be a better pivot than another. For example, the pivot vocabulary sizes play an important role. Birch et al. (2008) concluded in their study that the target vocabulary size has a negative impact in the translation quality as measured with the Bleu score and it can be seen that Arabic and French have both a larger vocabulary size than English.\nApart from the vocabulary size, the research mentioned above also measured the success of machine translation in terms of word reordering, i.e., differences in word order that occur in a parallel corpus, which are mainly driven by syntactic differences between the languages.\nIn order to measure reordering in translation they assumed that reordering only occurs between two adjacent blocks in the source side. This simplification allowed them to detect a extract all reordering in a deterministic way.\nA block As is defined by Birch et. al. (2008) as a segment of consecutive source words (source span) which is aligned to a set of target words. The target words also form a block At. With the definition of block set, they formally defined a reordering r as two blocks A and B that are adjacent in the source, the relative order of the blocks in the source is reversed in the target and the reordering is consistent. A reordering between blocks As and Bs is consistent if the block Cs, consisting of the union of blocks As and Bs, is also consistent. A block As is said to be consistent if the span defined by its corresponding target block At does not contain words that are aligned to source words outside of As. This definition of a consistent block is equivalent to the definition of a phrase in the phrase-based machine translation paradigm. Finally, the set of all reorderings r in a sentence is defined as R and it is unique for a given pair of sentences. Summarizing, this concept of reordering is equivalent to the swap movement described in the lexicalized reordering at the end of section 5.\nWith those concepts defined, they developed a metric called RQuantity, defined as a sentence level metric which is then averaged over a corpus:\nRQuantity =\n\u2211 r\u2208R |rAs |+ |rBs |\nI (11)\nwhere R is the set of reorderings for a sentence, I is the source sentence length, A and B are the two blocks involved in the reordering, |rAs | is the size or span of block A on the source side and |rBs | is the size or span of block B on the source side (Birch et al., 2008).\nThe objective of the RQuantity is to measure the amount of reordering we need when translating from a source language to a target language. The minimum RQuantity for a given sentence is 0 if the translation does not involve any word movement and its maximum is ( \u2211I\ni=2 i)/I when the words in the translation are inverted compared with their order in the source sentence.\nWe have computed the RQuantity for the different language pairs involved in our pivot approaches. It can be seen in table 5 that English appears as the best pivot because it has the lowest average RQuantity between the three, i.e. it is the pivot that needs the least amount of reordering in average to achieve the final translation. French and Arabic required less movements to translate into Spanish than English, but a lot of reordering is needed to obtain the first step from Chinese, hence penalizing the average. This result is coherent with the conclusion obtained by Birch et al. (2008), which says that the amount of reordering has also a negative impact in Bleu score.\nThese results support the intuitive idea that English works as a good intermediate step between Chinese and Spanish. Both French and Arabic have a vocabulary which is closer in size to Spanish and their reorderings are also more complex than English during the first step, making the source-to-pivot translation harder with these candidates. The gradual increase in difficulty (measured as target vocabulary size and reordering) presented in English seems to benefit the global result.\nNevertheless, it is also possible that most Un texts were authored in English, and then, translated into the other languages. This would also favour English as the best pivot language.\nIn order to observe the benefits of the pivot language against the direct translation, table 6 presents three examples where the Bleu scores of the pivot approach were better than those of the direct approach. Notice how some phrases that disappeared from the direct translation correctly appear on the pseudo-corpus approach."}, {"heading": "7.2 Pivot Combination", "text": "Using the 1-best translation output from the different pivot strategies, we built an n-best list and computed the final translation using minimum Bayes risk (Mbr) (Kumar & Byrne, 2004).\nWhen translating a sentence s, we obtain a translation t\u2032 which can then be evaluated against reference t to measure the system\u2019s performance. Mbr focuses on finding the best performance over all possible translations. To do so, it uses a loss function LF(t, t\u2032) that measures the loss of obtaining hypothesis t\u2032 instead of the real translation t. The Bayes Risk is defined as the expected value of the loss function over all possible hypotheses t\u2032 and translations t.\nE(LF) = \u2211 t,t\u2032 LF(t, t\u2032)p(t\u2032|s) (12)\nwhere p(t\u2032|s) is the translation probability of hypothesis t\u2032 given the source sentence s as obtained by the decoder, as an approximation of its real probability distribution.\nThe objective of finding the best performance over all possible translation is therefore to minimize the Bayes Risk. Given a loss function and a distribution, the decision rule that minimizes the Bayes Risk (Bickel & Doksum, 1977; Goel & Byrne, 2000) is given by:\nt\u0302 = arg min t\u2032 \u2211 t LF(t, t\u2032)p(t\u2032|s) (13)\nMbr has been used in literature both during decoding (Ehling, Zens, & Ney, 2007) and as a postprocess over a n-best list. For instance, this last approach was used by Khalilov et al. (2008) together with their cascaded approach in order to obtain the best Chinese\u2013Spanish translation. The current version of the Moses toolkit includes both implementations.\nThe Mbr algorithm implemented in Moses as postprocess uses 1\u2212Bleu(t, t\u2032) as the loss function. In our experiment, we consider all our hypothesis as equally likely and therefore p(t\u2032|s) was a positive constant and therefore could be discarded. At the end, Mbr chooses the hypotheses t\u0302 that fullfills:\nt\u0302 = arg min t\u2032 \u2211 t6=t\u2032 1\u2212Bleu(t, t\u2032)  (14) Different n-best lists were built to compare different Mbr outputs: a cascaded Mbr using all three pivot languages (hence n = 3, one hypothesis per pivot), a pseudo-corpus Mbr again using all three pivot languages (n = 3), a triangulation Mbr using all three languages (n = 3), a combination of cascaded, pseudo-corpus and triangulation outputs using two languages (n = 6, one hypothesis per pivot and strategy) and another using all of them (n = 9). It is important to mention that all n-best lists must have at least 3 hypothesis per sentence. Having only two hypothesis would not work as expected because the Loss Function would always choose the longest one, which can be explained by the definition of Bleu:\nBleu(t, t\u2032) = exp\n( N\u2211\nn=1\nlog pn(t, t\n\u2032)\nN\n) \u03b3(t, t\u2032) (15)\nwhere pn(t, t \u2032) is the precision of n-grams in the hypothesis t\u2032 with reference t; and \u03b3(t, t\u2032) is a brevity penalty disfavouring translation t\u2032 if it is shorter than the reference t. Then pn(t, t \u2032) = pn(t \u2032, t) and\n( \u2200t, t\u2032 : length(t) > length(t\u2032) ) (16) 1\u2212Bleu(t, t\u2032) \u2265 1\u2212Bleu(t\u2032, t) (17)\nTables 7 and 8 show the results of the different Chinese\u2013Spanish output systems (from table 4) combined with the Mbr technique. From these tables, it can be observed that combinations from all pivot strategies obtained better results in all metrics than the direct approach. Only in the case of Ar + Fr, the combination was not statistically significantly better than the direct system in terms of Bleu score (with a 99% confidence).\nThe Mbr cascaded and triangulation approach (1st row, 2nd and 4th column, respectively, in table 8) did not outperform the direct system.\nFinally, both A All and D+A All (which combine all languages and pivot system outputs from table 4 including or not the direct approach) are the best Chinese-to-Spanish systems.\nWe have not experimented on the reverse translation direction (from Spanish into Chinese) as we would be unable to assess subjective evaluations on the resulting translation outputs. However, in the reversed direction, our intuition is that the reordering difficulties will be then moved to the pivot\u2013target step of the cascade system.\nRegarding the fact of English being the best pivot language for the task under consideration, we can argue that English might constitute a better intermediate step between Spanish and Chinese, rather than French or Arabic, based on the assumption of Spanish being closer to French (both are romance languages derived from Latin) and Arabic (the Iberian peninsula was occupied by Arabic culture for more than 500 years, so Spanish has strong influence from Arabic) than Chinese to French and Arabic. In this sense, English seems to represent an optimal intermediate point between Chinese and Spanish, in which the translation complexity is divided in two phases. Most of the reordering burden is resolved in the Chinese-to-English phase and most of the morphology generation burden is resolved in the English-to-Spanish phase. Thinking on translation-space as a non-conservative field, we can say that English is just in the middle of the way between Chinese and Spanish, while passing through French or Arabic implies a larger path by some kind of detour in the proximities of Spanish. This is just a conjecture, of course, but it nicely explains what we are observing. Definitively, more research is needed to better understand what is happening."}, {"heading": "8. Conclusions", "text": "This work provided a brief survey in the state-of-the-art of Chinese\u2013Spanish Smt. First of all, this language pair is of great interest both economically and culturally if we take into account the high number of Chinese and Spanish speakers. Besides, statistical machine translation is the most popular approach in the field of Mt given that has shown great quality in all the international evaluation campaigns such as Nist (2009) and Wmt (2012). The main points covered in our study were:\n\u2022 There are mainly three Chinese\u2013Spanish parallel corpora (Btec, Holy Bible and Un) that are freely available for research purposes.\n\u2022 English is the best pivot language for conducting Chinese-to-Spanish translations compared to languages such as French or Arabic. The system built using English as pivot was significantly better than the ones built with French or Arabic, with a 99% confidence in both comparisons.\n\u2022 The preference for a pivot language appears to be correlated with other proposed translation-quality prediction metrics such as the differences in vocabulary sizes and the amount of reordering. According to the above conclusion, the best pivot language is English because it has the lowest increase in vocabulary size and the lowest increase in reordering complexity.\n\u2022 No significant difference is found among the best cascaded and pseudo-corpus pivot approaches, but the pseudo-corpus strategy is the best pivot strategy for Chineseto-Spanish. Additionally, pseudo-corpus and cascaded approaches are significantly better than the triangulation approach.\n\u2022 The output combination using Mbr is able to improve the direct system in 1 Bleu point in the best case. This improvement is significantly better with a 99% confidence and is coherent with improvements in all other evaluation metrics studied.\nAs future research we plan to work on the problem of automatically extracting parallel corpus from comparable corpora collected from the web. Additionally, we intend to develop a freely available Chinese\u2013Spanish translation system which would allow for collecting user feedback. Then, we will work on special techniques to incorporate this knowledge in the Smt system."}, {"heading": "Acknowledgments", "text": "The authors would like to specially thank the reviewers for their comments that helped a lot to improve this work. Additionally, the authors would like to thank the Universitat Polite\u0300cnica de Catalunya and the Institute for Infocomm Research for their support and permission to publish this research.\nThis work has been partially funded by the Seventh Framework Program of the European Commission through the International Outgoing Fellowship Marie Curie Action (IMTraP2011-29951); and by the Spanish Ministry of Economy and Competitiveness through the FPI Scholarship BES-2008-003851 for Ph.D. students under the AVIVAVOZ project (TEC200613694-C03-01); and the BUCEADOR project (TEC2009-14094-C04-01)."}], "references": [{"title": "Extrinsic evaluation of sentence alignment systems. In Proceedings of LREC workshop on Creating Cross-language Resources for Disconnected Languages and Styles, CREDISLAS, Istambul", "author": ["S. Abdul-Rauf", "M. Fishel", "P. Lambert", "S. Noubours", "R. Sennrich"], "venue": null, "citeRegEx": "Abdul.Rauf et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Abdul.Rauf et al\\.", "year": 2012}, {"title": "A Feasibility Study For Chinese-Spanish Statistical Machine Translation", "author": ["R.E. Banchs", "J.M. Crego", "P. Lambert", "J.B. Mari\u00f1o"], "venue": "In Proc. of the 5th Int. Symposium", "citeRegEx": "Banchs et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Banchs et al\\.", "year": 2006}, {"title": "Exploring Spanish Morphology effects on Chinese-Spanish SMT", "author": ["R.E. Banchs", "H. Li"], "venue": "MATMT", "citeRegEx": "Banchs and Li,? \\Q2008\\E", "shortCiteRegEx": "Banchs and Li", "year": 2008}, {"title": "METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization", "citeRegEx": "Banerjee and Lavie,? \\Q2005\\E", "shortCiteRegEx": "Banerjee and Lavie", "year": 2005}, {"title": "FBK @ IWSLT-2008", "author": ["N. Bertoldi", "R. Cattoni", "M. Federico", "M. Barbaiani"], "venue": "In Proc. of the International Workshop on Spoken Language Translation,", "citeRegEx": "Bertoldi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bertoldi et al\\.", "year": 2008}, {"title": "Mathematical: Basic Ideas and Selected topics", "author": ["P.J. Bickel", "K.A. Doksum"], "venue": "In HoldenDay Inc.,", "citeRegEx": "Bickel and Doksum,? \\Q1977\\E", "shortCiteRegEx": "Bickel and Doksum", "year": 1977}, {"title": "Predicting Success in Machine Translation", "author": ["A. Birch", "M. Osborne", "P. Koehn"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Birch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Birch et al\\.", "year": 2008}, {"title": "A Statistical Approach to Machine Translation", "author": ["P.F. Brown", "J. Cocke", "S.A. Della Pietra", "V.J. Della Pietra", "F. Jelinek", "J.D. Lafferty", "R.L. Mercer", "P.S. Roossin"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1990}, {"title": "Findings of the 2012 workshop on statistical machine translation", "author": ["C. Callison-Burch", "P. Koehn", "C. Monz", "M. Post", "R. Soricut", "L. Specia"], "venue": "In Proceedings of the Seventh Workshop on Statistical Machine Translation,", "citeRegEx": "Callison.Burch et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2012}, {"title": "Evaluation Of The Bible As A Resource For Cross-language Information Retrieval", "author": ["P.A. Chew", "S.J. Verzi", "T.L. Bauer", "J.T. McClain"], "venue": "In Proceedings of the Workshop on Multilingual Language Resources and Interoperability,", "citeRegEx": "Chew et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chew et al\\.", "year": 2006}, {"title": "Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora", "author": ["T. Cohn", "M. Lapata"], "venue": "In Proc. of the ACL", "citeRegEx": "Cohn and Lapata,? \\Q2007\\E", "shortCiteRegEx": "Cohn and Lapata", "year": 2007}, {"title": "Enhancing Scarce-resource Language Translation Through Pivot Combinations", "author": ["M. Costa-juss\u00e0", "C. Hen\u0155\u0131quez", "R.E. Banchs"], "venue": "In 5th International Joint Conference on Natural Language Processing,", "citeRegEx": "Costa.juss\u00e0 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Costa.juss\u00e0 et al\\.", "year": 2011}, {"title": "Evaluaci\u00f3n de estrategias para la traducci\u00f3n autom\u00e1tica estad\u0301\u0131stica de chino a castellano con el ingl\u00e9s como lengua pivote", "author": ["M. Costa-juss\u00e0", "C. Hen\u0155\u0131quez", "R.E. Banchs"], "venue": "In XXVII edicio\u0301n del Congreso Anual de la Sociedad Espan\u0303ola para el Procesamiento del Lenguaje Natural,", "citeRegEx": "Costa.juss\u00e0 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Costa.juss\u00e0 et al\\.", "year": 2011}, {"title": "Catalan-English Statistical Machine Translation without Parallel Corpus: Bridging through Spanish", "author": ["A. de Gispert", "J. Mari\u00f1o"], "venue": "In Proc. of LREC 5th Workshop on Strategies for developing Machine Translation for Minority Languages", "citeRegEx": "Gispert and Mari\u00f1o,? \\Q2006\\E", "shortCiteRegEx": "Gispert and Mari\u00f1o", "year": 2006}, {"title": "Minimum Bayes Risk Decoding for BLEU", "author": ["N. Ehling", "R. Zens", "H. Ney"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,", "citeRegEx": "Ehling et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ehling et al\\.", "year": 2007}, {"title": "Minimum Bayes-risk Automatic Speech Recognition", "author": ["V. Goel", "W. Byrne"], "venue": "Computer Speech and Language,", "citeRegEx": "Goel and Byrne,? \\Q2000\\E", "shortCiteRegEx": "Goel and Byrne", "year": 2000}, {"title": "Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop", "author": ["N. Habash", "O. Rambow"], "venue": "In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Habash and Rambow,? \\Q2005\\E", "shortCiteRegEx": "Habash and Rambow", "year": 2005}, {"title": "Learning Reordering Models for Statistical Machine Translation with a Pivot Language", "author": ["C.A. Hen\u0155\u0131quez Q", "R.E. Banchs", "J.B. Mari\u00f1o"], "venue": "Internal Report TALP-UPC", "citeRegEx": "Q. et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Q. et al\\.", "year": 2010}, {"title": "The TALP & I2R SMT Systems for IWSLT", "author": ["M. Khalilov", "M.R. Costa-Juss\u00e0", "C.A. Hen\u0155\u0131quez", "J.A.R. Fonollosa", "A. Hern\u00e1ndez", "J.B. Mari\u00f1o", "R.E. Banchs", "B. Chen", "M. Zhang", "A. Aw", "H. Li"], "venue": "In Proc. of the International Workshop on Spoken Language Translation,", "citeRegEx": "Khalilov et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Khalilov et al\\.", "year": 2008}, {"title": "Statistical Significance Tests For Machine Translation Evaluation", "author": ["P. Koehn"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Koehn,? \\Q2004\\E", "shortCiteRegEx": "Koehn", "year": 2004}, {"title": "Statistical Phrase-Based Translation", "author": ["P. Koehn", "F. Och", "D. Marcu"], "venue": "In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Minimum Bayes-Risk Decoding For Statistical Machine Translation", "author": ["S. Kumar", "W. Byrne"], "venue": "In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference (HLT/NAACL\u201904),", "citeRegEx": "Kumar and Byrne,? \\Q2004\\E", "shortCiteRegEx": "Kumar and Byrne", "year": 2004}, {"title": "N-gram Based Machine Translation", "author": ["J. Mari\u00f1o", "R.E. Banchs", "J.M. Crego", "A. de Gispert", "P. Lambert", "J.R. Fonollosa", "M.R. Costa-juss\u00e0"], "venue": "Computational Linguistics,", "citeRegEx": "Mari\u00f1o et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mari\u00f1o et al\\.", "year": 2006}, {"title": "Fast And Accurate Sentence Alignment Of Bilingual Corpora", "author": ["R. Moore"], "venue": "In Proc. of AMTA,", "citeRegEx": "Moore,? \\Q2002\\E", "shortCiteRegEx": "Moore", "year": 2002}, {"title": "Improved Statistical Alignment Models", "author": ["F.J. Och", "H. Ney"], "venue": "In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Och and Ney,? \\Q2000\\E", "shortCiteRegEx": "Och and Ney", "year": 2000}, {"title": "Minimum Error Rate Training In Statistical Machine Translation", "author": ["F. Och"], "venue": "In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Och,? \\Q2003\\E", "shortCiteRegEx": "Och", "year": 2003}, {"title": "Dicriminative training and maximum entropy models for statistical machine translation", "author": ["F. Och", "H. Ney"], "venue": "In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Och and Ney,? \\Q2002\\E", "shortCiteRegEx": "Och and Ney", "year": 2002}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "Zhu", "W.-J"], "venue": "IBM Research Report,", "citeRegEx": "Papineni et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2001}, {"title": "Overview of the iwslt 2008 evaluation campaign", "author": ["M. Paul"], "venue": "In Proc. of the International Workshop on Spoken Language Translation,", "citeRegEx": "Paul,? \\Q2008\\E", "shortCiteRegEx": "Paul", "year": 2008}, {"title": "On the Importance of Pivot Language Selection for Statistical Machine Translation", "author": ["M. Paul", "H. Yamamoto", "E. Sumita", "S. Nakamura"], "venue": "In HLT-NAACL (Short Papers),", "citeRegEx": "Paul et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2009}, {"title": "United Nations General Assembly Resolutions: A SixLanguage Parallel Corpus", "author": ["A. Rafalovitch", "R. Dale"], "venue": "In Proc. of the MT Summit XII,", "citeRegEx": "Rafalovitch and Dale,? \\Q2009\\E", "shortCiteRegEx": "Rafalovitch and Dale", "year": 2009}, {"title": "MT-based Sentence Alignment For OCR-generated Parallel Texts", "author": ["R. Senrich"], "venue": "In Proc. of AMTA, Denver", "citeRegEx": "Senrich,? \\Q2010\\E", "shortCiteRegEx": "Senrich", "year": 2010}, {"title": "A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of Association for Machine Translation in the Americas", "author": ["M. Snover", "B. Dorr", "R. Schwartz", "L. Micciulla", "J. Makhoul"], "venue": null, "citeRegEx": "Snover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "SRILM: an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "In Proc. of the Int. Conf. on Spoken Language Processing,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "A Block Orientation Model for Statistical Machine Translation", "author": ["C. Tillman"], "venue": "In HLT-NAACL", "citeRegEx": "Tillman,? \\Q2004\\E", "shortCiteRegEx": "Tillman", "year": 2004}, {"title": "The TCH Machine Translation System for IWSLT", "author": ["H. Wang", "H. Wu", "X. Hu", "Z. Liu", "J. Li", "D. Ren", "Z. Niu"], "venue": "In Proc. of the International Workshop on Spoken Language Translation,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Pivot Language Approach for Phrase-Based Statistical Machine Translation", "author": ["H. Wu", "H. Wang"], "venue": "In Proc. of the ACL,", "citeRegEx": "Wu and Wang,? \\Q2007\\E", "shortCiteRegEx": "Wu and Wang", "year": 2007}, {"title": "China is a top priority for the spanish economy; our companies are well aware of that", "author": ["J.R. Zapatero"], "venue": null, "citeRegEx": "Zapatero,? \\Q2010\\E", "shortCiteRegEx": "Zapatero", "year": 2010}, {"title": "Phrase-based statistical machine translation", "author": ["R. Zens", "F. Och", "H. Ney"], "venue": "In Verlag, S. (Ed.), Proc. German Conference on Artificial Intelligence (KI)", "citeRegEx": "Zens et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Zens et al\\.", "year": 2002}, {"title": "HHMM-based chinese lexical analyzer ICTCLAS", "author": ["H. Zhang", "H. Yu", "D. Xiong", "Q. Liu"], "venue": "In Proc. of the 2nd SIGHAN Workshop on Chinese language processing,", "citeRegEx": "Zhang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 37, "context": "(Zapatero, 2010).", "startOffset": 0, "endOffset": 16}, {"referenceID": 4, "context": "Apart from the Btec1 corpus available through International Workshop on Spoken Language Translation (Iwslt) competition (Bertoldi et al., 2008) and Holy Bible datasets (Banchs & Li, 2008), we were not aware of any other Chinese\u2013 Spanish parallel corpus suitable for training phrase-based (Koehn, Och, & Marcu, 2003)2 statistical machine translation systems between these two languages, until a six-language parallel corpus (including both Chinese and Spanish) from United Nations was released for research purposes (Rafalovitch & Dale, 2009).", "startOffset": 120, "endOffset": 143}, {"referenceID": 6, "context": "Hence, we present a short comparison of the amount of reordering and vocabulary sizes of pivot languages, following the study presented by Birch et al. (2008) where they identified these two properties as key elements for predicting machine translation quality.", "startOffset": 139, "endOffset": 159}, {"referenceID": 35, "context": "Comparing different methodologies for performing statistical machine translation: cascaded (Wang et al., 2008), pseudo-corpus generation (Banchs et al.", "startOffset": 91, "endOffset": 110}, {"referenceID": 1, "context": ", 2008), pseudo-corpus generation (Banchs et al., 2006; de Gispert & Mari\u00f1o, 2006) and triangulation (Wu & Wang, 2007).", "startOffset": 34, "endOffset": 82}, {"referenceID": 1, "context": "One of the first works dealing with Chinese\u2013Spanish statistical machine translation was presented by Banchs et al. (2006). Authors experimented with two independent corpora Chinese\u2013English and English\u2013Spanish to translate from Chinese to Spanish.", "startOffset": 101, "endOffset": 122}, {"referenceID": 28, "context": "The only research event recently performed for this language pair was the 2008 Iwslt evaluation campaign (Paul, 2008).", "startOffset": 105, "endOffset": 117}, {"referenceID": 4, "context": "This pseudo-corpus is used to train the Chinese\u2013Spanish translation (Bertoldi et al., 2008).", "startOffset": 68, "endOffset": 91}, {"referenceID": 27, "context": "The only research event recently performed for this language pair was the 2008 Iwslt evaluation campaign (Paul, 2008). This evaluation organized two Chinese-to-Spanish tracks. One of them focused on direct translation and the other one on pivot translation through English. The best translation results accordingly to the manual evaluation were obtained by far in the pivot task. The best systems in both tracks were developed by Wang et al. (2008). Regarding the direct system, they used a standard phrase-based Smt system.", "startOffset": 106, "endOffset": 449}, {"referenceID": 27, "context": "The only research event recently performed for this language pair was the 2008 Iwslt evaluation campaign (Paul, 2008). This evaluation organized two Chinese-to-Spanish tracks. One of them focused on direct translation and the other one on pivot translation through English. The best translation results accordingly to the manual evaluation were obtained by far in the pivot task. The best systems in both tracks were developed by Wang et al. (2008). Regarding the direct system, they used a standard phrase-based Smt system. What makes it different from the other participating systems is that they provide their own Chinese segmentation and the Ldc (Linguistic Data Consortium) bilingual dictionary. Regarding the pivot task, they compared two different approaches. The first one, referred to as triangulation, consisted of training two translation models on the Chinese\u2013English corpus and English\u2013Spanish corpus, and then building a new translation model for Chinese\u2013Spanish translation by combining the two previous models as proposed by Wu & Wang (2007); the second one obtained better results and it was based on a cascaded approach.", "startOffset": 106, "endOffset": 1058}, {"referenceID": 4, "context": "This pseudo-corpus is used to train the Chinese\u2013Spanish translation (Bertoldi et al., 2008). As mentioned aboved, the comparison performed by Wang et al. (2008) showed that the cascaded approach performed better than the phrase-table combination for the Chinese\u2013 Spanish pivot task.", "startOffset": 69, "endOffset": 161}, {"referenceID": 23, "context": "However, there are many nice algorithms which can extract parallel corpora from comparable corpora (Moore, 2002; Senrich, 2010; Abdul-Rauf, Fishel, Lambert, Noubours, & Sennrich, 2012).", "startOffset": 99, "endOffset": 184}, {"referenceID": 31, "context": "However, there are many nice algorithms which can extract parallel corpora from comparable corpora (Moore, 2002; Senrich, 2010; Abdul-Rauf, Fishel, Lambert, Noubours, & Sennrich, 2012).", "startOffset": 99, "endOffset": 184}, {"referenceID": 25, "context": "Later on, a variation was proposed by Och & Ney (2002) named log-linear model.", "startOffset": 38, "endOffset": 55}, {"referenceID": 20, "context": "Then, the lexical models provide a probability among words (Koehn et al., 2003).", "startOffset": 59, "endOffset": 79}, {"referenceID": 34, "context": "A visual representation of these phrases can be seen in Figure 1 Besides the traditional distance-based reordering mentioned before, state-of-the-art systems implement an additional lexicalized reordering model (Tillman, 2004).", "startOffset": 211, "endOffset": 226}, {"referenceID": 20, "context": "Our direct system uses the phrase-based translation approach (Koehn et al., 2003).", "startOffset": 61, "endOffset": 81}, {"referenceID": 4, "context": "4 The former was used in the 2008 Iwslt and complete experiments of pivot strategies are reported in works such as Bertoldi et al. (2008). The Holy Bible was used for the similar purposes by Hen\u0155\u0131quez, Banchs & Mari\u00f1o (2010).", "startOffset": 115, "endOffset": 138}, {"referenceID": 4, "context": "4 The former was used in the 2008 Iwslt and complete experiments of pivot strategies are reported in works such as Bertoldi et al. (2008). The Holy Bible was used for the similar purposes by Hen\u0155\u0131quez, Banchs & Mari\u00f1o (2010). In this study we decide to use the Un corpus taking advantage of the fact that it is the largest corpus (among those three) and it contains the same sentences in six languages, therefore we can experiment with different pivot languages.", "startOffset": 115, "endOffset": 225}, {"referenceID": 33, "context": "The language model was built using Srilm (Stolcke, 2002) version 1.", "startOffset": 41, "endOffset": 56}, {"referenceID": 25, "context": "The optimization was done using Mert (Och, 2003).", "startOffset": 37, "endOffset": 48}, {"referenceID": 19, "context": "Our systems were build using revision 4075 of Moses (Koehn et al., 2007). For all systems, we used the default Moses parameters which includes the grow-diagonal-final-and word alignment symmetrization, the lexicalized reordering (where possible), relative frequencies, lexical weights and phrase bonus for the translation model (with phrases up to length 10), a 5-gram language model using Kneser-Ney smoothing and a word penalty model. Therefore, 14 different features are combined in equation (3). The language model was built using Srilm (Stolcke, 2002) version 1.5.12. The optimization was done using Mert (Och, 2003). For word aligning we used Giza++ (Och & Ney, 2000) version 1.0.5. In order to evaluate the translation quality, we used Bleu (Papineni, Roukos, Ward, & Zhu, 2001), Ter (Snover, Dorr, Schwartz, Micciulla, & Makhoul, 2006) and Meteor (Banerjee & Lavie, 2005) automatic evaluation metrics. Additionally, significance tests were performed to study when a system was better than the other. These tests followed the \u201cpair bootstrap resampling\u201d method presented by Koehn (2004): Given two translation outputs coming from two different systems, we created two new virtual test sets by drawing sentences with replacement from the translation outputs.", "startOffset": 53, "endOffset": 1094}, {"referenceID": 19, "context": "In Chinese-to-Spanish, the fact that the pseudo-corpus through English outperforms cascaded through English according to the Bleu score is not statistically significant, with a 99% confidence (Koehn, 2004).", "startOffset": 192, "endOffset": 205}, {"referenceID": 4, "context": "These results, however, are coherent with previous works using the same language pair (Bertoldi et al., 2008; Hen\u0155\u0131quez Q. et al., 2010) that also reported the pseudo-corpus strategy was better than the cascaded strategy.", "startOffset": 86, "endOffset": 136}, {"referenceID": 6, "context": "Birch et al. (2008) concluded in their study that the target vocabulary size has a negative impact in the translation quality as measured with the Bleu score and it can be seen that Arabic and French have both a larger vocabulary size than English.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Birch et al. (2008) concluded in their study that the target vocabulary size has a negative impact in the translation quality as measured with the Bleu score and it can be seen that Arabic and French have both a larger vocabulary size than English. Apart from the vocabulary size, the research mentioned above also measured the success of machine translation in terms of word reordering, i.e., differences in word order that occur in a parallel corpus, which are mainly driven by syntactic differences between the languages. In order to measure reordering in translation they assumed that reordering only occurs between two adjacent blocks in the source side. This simplification allowed them to detect a extract all reordering in a deterministic way. A block As is defined by Birch et. al. (2008) as a segment of consecutive source words (source span) which is aligned to a set of target words.", "startOffset": 0, "endOffset": 798}, {"referenceID": 6, "context": "where R is the set of reorderings for a sentence, I is the source sentence length, A and B are the two blocks involved in the reordering, |rAs | is the size or span of block A on the source side and |rBs | is the size or span of block B on the source side (Birch et al., 2008).", "startOffset": 256, "endOffset": 276}, {"referenceID": 6, "context": "where R is the set of reorderings for a sentence, I is the source sentence length, A and B are the two blocks involved in the reordering, |rAs | is the size or span of block A on the source side and |rBs | is the size or span of block B on the source side (Birch et al., 2008). The objective of the RQuantity is to measure the amount of reordering we need when translating from a source language to a target language. The minimum RQuantity for a given sentence is 0 if the translation does not involve any word movement and its maximum is ( \u2211I i=2 i)/I when the words in the translation are inverted compared with their order in the source sentence. We have computed the RQuantity for the different language pairs involved in our pivot approaches. It can be seen in table 5 that English appears as the best pivot because it has the lowest average RQuantity between the three, i.e. it is the pivot that needs the least amount of reordering in average to achieve the final translation. French and Arabic required less movements to translate into Spanish than English, but a lot of reordering is needed to obtain the first step from Chinese, hence penalizing the average. This result is coherent with the conclusion obtained by Birch et al. (2008), which says that the amount of reordering has also a negative impact in Bleu score.", "startOffset": 257, "endOffset": 1245}, {"referenceID": 18, "context": "For instance, this last approach was used by Khalilov et al. (2008) together with their cascaded approach in order to obtain the best Chinese\u2013Spanish translation.", "startOffset": 45, "endOffset": 68}], "year": 2012, "abstractText": "Although, Chinese and Spanish are two of the most spoken languages in the world, not much research has been done in machine translation for this language pair. This paper focuses on investigating the state-of-the-art of Chinese-to-Spanish statistical machine translation (Smt), which nowadays is one of the most popular approaches to machine translation. For this purpose, we report details of the available parallel corpus which are Basic Traveller Expressions Corpus (Btec), Holy Bible and United Nations (Un). Additionally, we conduct experimental work with the largest of these three corpora to explore alternative Smt strategies by means of using a pivot language. Three alternatives are considered for pivoting: cascading, pseudo-corpus and triangulation. As pivot language, we use either English, Arabic or French. Results show that, for a phrase-based Smt system, English is the best pivot language between Chinese and Spanish. We propose a system output combination using the pivot strategies which is capable of outperforming the direct translation strategy. The main objective of this work is motivating and involving the research community to work in this important pair of languages given their demographic impact.", "creator": "TeX"}}}