{"id": "1402.1473", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2014", "title": "Near-Optimal Joint Object Matching via Convex Relaxation", "abstract": "joint matching over a collection of independent objects aims at aggregating information from a large collection of similar instances ( e. g. images, graphs, shapes ) to improve maps between pairs of them. given multiple matches computed between a few object pairs in isolation, the desirable goal is to recover an entire collection of maps that are ( 1 ) globally consistent, contiguous and ( 2 ) close to the provided maps - - - and under certain conditions provably the ground - truth maps. despite recent advances on this problem, seemingly the best - commonly known recovery guarantees are limited to leaving a small constant barrier - - - none probability of which the existing methods find theoretical strong support when more than $ 50 \\ % $ of input of correspondences are corrupted. moreover, prior approaches focus solely mostly on fully similar objects, such while it is still practically more commercially demanding to match instances that are only partially similar relative to each matching other.", "histories": [["v1", "Thu, 6 Feb 2014 20:16:35 GMT  (10892kb,D)", "http://arxiv.org/abs/1402.1473v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.IT math.IT math.OC stat.ML", "authors": ["yuxin chen", "leonidas j guibas", "qi-xing huang"], "accepted": true, "id": "1402.1473"}, "pdf": {"name": "1402.1473.pdf", "metadata": {"source": "CRF", "title": "Near-Optimal Joint Object Matching via Convex Relaxation", "authors": ["Yuxin Chen", "Leonidas J. Guibas", "Qi-Xing Huang"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we propose an algorithm to jointly match multiple objects that exhibit only partial similarities, given a few (possibly highly incomplete) pairwise matches that are densely corrupted. By encoding a consistent partial map collection into a 0-1 semidefinite matrix, we propose to recover the ground-truth maps via a parameter-free convex program called MatchLift, following a spectral method that pre-estimates the total number of distinct elements to be matched. Numerically, this program can be efficiently solved via alternating direction methods of multipliers (ADMM) along with a greedy rounding strategy. Theoretically, MatchLift exhibits near-optimal error-correction ability, i.e. in the asymptotic regime it is guaranteed to work even when a dominant fraction 1 \u2212 \u0398 ( log2 n\u221a\nn\n) of the input maps behave\nlike random outliers. Furthermore, MatchLift succeeds with minimal input complexity, namely, perfect matching can be achieved as soon as the provided maps form a connected map graph. We evaluate the proposed algorithm on various benchmark data sets including synthetic examples and real-world examples, all of which confirm the practical applicability and usefulness of MatchLift.\nIndex Terms: Joint graph matching, shape mapping, cycle consistency, dense error correction, partial similarity, convex relaxation, spectral methods, robust PCA, matrix completion, graph clustering, ADMM, MatchLift"}, {"heading": "1 Introduction", "text": "Finding consistent relations across multiple objects is a fundamental scientific problem spanning many fields. A partial list includes jigsaw puzzle solving [1, 2], structure from motion [3, 4], re-assembly of fragmented objects and documents [5, 6], and DNA/RNA shotgun assembly sequencing [7]. Compared with the rich literature in pairwise matching (e.g. of graphs, images or shapes), joint matching of multiple objects has not been well explored. A naive approach for joint object matching is to pick a base object and perform pairwise matching with each of the remaining objects. However, as pairwise matching algorithms typically generate noisy results, the performance of such approaches is often far from satisfactory in practice. This gives rise to the question as to how to aggregate and exploit information from all pairwise maps that one computes, in order to improve joint object matching in a consistent and efficient manner.\nIn this paper, we represent each object as a discrete set of points or elements, and investigate the problem of joint matching over n different sets, for which the input / observation is a collection of pairwise maps \u2217Y. Chen is with the Department of Electrical Engineering, Stanford University (email: yxchen@stanford.edu). \u2020L. J. Guibas is with the Department of Computer Science, Stanford University (email: guibas@stanford.edu). \u2021Q-X. Huang is with the Department of Computer Science, Stanford University (email: huangqx@stanford.edu).\nar X\niv :1\n40 2.\n14 73\nv1 [\ncs .L\nG ]\n6 F\neb 2\n01 4\ncomputed in isolation. A natural and popular criterion to preserve the global relational compatibility is called cycle-consistency, i.e., that composition of maps between two objects should be independent of the connecting path chosen. Such criterion has recently been invoked in many algorithms [3,8\u201311] to detect outliers among the pairwise input maps. These works have shown experimentally that one can use inconsistent cycles to prune outliers, provided that the corruption rate is sufficiently small.\nDespite the empirical advances of these works, little is known on the theoretical side, namely, under what conditions can the underlying ground-truth maps be reliably recovered. Recent work by [12] provided the first theoretical guarantee for robust and consistent joint matching. However, there are several fundamental issues left unaddressed that must be faced in order to accommodate practical challenges.\n1. Dense Input Errors: The state-of-the-art results (e.g. [12]) did not provide theoretical support when more than 50% of the input matches are corrupted. This gives rise to the question regarding their applicability in the presence of highly noisy sources, in which case the majority of the input maps can be corrupted. Observe that as the number n of objects to be matched increases, the amount of pairwise maps one can obtain significantly exceeds n. As a result, dense error correction is information theoretically possible as long as the global consistency across pairwise maps can be appropriately exploited. While one would expect an ideal algorithm to work even when most input maps are random outliers, the challenge remains as to whether there exist computationally feasible methods that can provably detect and separate dense outliers.\n2. Partial Similarity: To the best of our knowledge, all prior approaches dealt only with a restricted scenario where the ground-truth maps are given by full isomorphisms (i.e. one-to-one correspondences between any two sets). In reality, a collection of objects usually exhibit only partial similarity, as in the case of images of the same scene but from different camera positions. These practical scenarios require consistent matching of multiple objects that are only partially similar to each other.\n3. Incomplete Input Maps: Computing pairwise maps across all object pairs are often expensive, sometimes inadmissible, and in fact unnecessary. Depending on the characteristics of input sources, one might be able to infer unobserved maps from a small sample of noisy pairwise matches. While [12] considered incomplete inputs, the tradeoff between the undersampling factor and the error-correction ability remains unknown.\nAll in all, practical applications require matching partially similar objects from a small fraction of densely corrupted pairwise maps \u2014 a goal this paper aims to achieve."}, {"heading": "1.1 Contributions", "text": "This paper is concerned with joint object matching under dense input errors. Our main contributions in this regard are three-fold.\n1. Algorithms: Inspired by the recent evidence on the power of convex relaxation, we propose to solve the joint matching problem via a semidefinite program called MatchLift. The algorithm relaxes the binaryvalue constraints, and attempts to maximize the compatibility between the input and the recovered maps. The program is established upon a semidefinite conic constraint that relies on the total number m of distinct elements to be matched. To this end, we propose to pre-estimatem via a spectral method. Our methodology is essentially parameter free, and can be solved by scalable optimization algorithms.\n2. Theory: We derive performance guarantees for exact matching. Somewhat surprisingly, MatchLift admits perfect map recovery even in the presence of dense input corruptions. Our findings reveal the near-optimal error-correction ability of MatchLift, i.e. as n grows, the algorithm is guaranteed to work even when a dominant fraction \u2013 more precisely, a fraction 1 \u2212 \u2126 ( log2 n\u221a\nn\n) \u2013 of the inputs behave as\nrandom outliers. Besides, while the presence of partial similarity unavoidably incurs more severe types of input errors, MatchLift exhibits a strong recovery ability nearly order-wise equivalent to that in the full-similarity scenario, as long as the fraction of each object being disclosed is bounded away from zero. Finally, in many situations, MatchLift succeeds even with minimal input complexity, in the sense that it can reliably fill in all unobserved maps based on very few noisy partial inputs, as soon as the provided maps form a connected graph. This is information theoretically optimal.\n3. Practice: We have evaluated the performance of MatchLift on several benchmark datasets. These datasets include several synthetic examples as well as real examples from several popular benchmarks. Experimental results on synthetic examples corroborate our theoretical findings. On real datasets, the quality of the maps generated by MatchLift outperforms the state-of-the-art object matching and graph clustering algorithms."}, {"heading": "1.2 Prior Art", "text": "There has been numerous work studying the problem of object matching, either in terms of shape mapping, graph matching, or image mapping, which is impossible to enumerate. We list below a small sample of development on joint object matching, as well as its relation and distinction to the well-renowned graph clustering problem.\n\u2022 Object Matching. Early work on object matching focused primarily on matching pairs of objects in isolation (e.g. [13\u201315]). Due to the limited and biased information present in an isolated object pair, pairwise matching techniques can easily, sometimes unavoidably, generate false correspondences. Last few years have witnessed a flurry of activity in joint object matching, e.g. [9\u201312], which exploited the global cycle-consistency criterion to prune noisy maps. The fundamental understanding has recently been advanced by [12]. Nevertheless, none of the prior work have demonstrated provable recovery ability when the majority of input maps/correspondences are outliers, nor were they able to accommodate practical scenarios where different objects only exhibit partial similarity. Recent work [16] employed spectral methods for denoising in the full-similarity case. However, the errors considered therein are modeled as Gaussian-Wigner additive noise, which is not applicable in our setting. Another line of work [17, 18] proposed to recover global rigid transform between points via convex relaxation, where the point coordinates might only be partially observed. While this line of work is relevant, the problem considered therein is more specialized than the point-based joint matching studied in this paper; also, none of these paradigms are able to enable dense error correction.\n\u2022 Matrix Completion and Robust PCA. In a broader sense, our approach is inspired by the pioneering work in low-rank matrix completion [19,20] and robust principal component analysis [21\u201325], which reveal the power of convex relaxation in recovering low-dimensional structures among high-dimensional objects. In fact, the ground truth herein is equivalent to a block-constant low-rank matrix [26], as occurred in various graph-related problems. Nevertheless, their theoretical analyses fail to provide tight bounds in our setting, as the low-rank matrix relevant in our cases is highly sparse as well. That said, additional structural assumptions need to be incorporated in order to achieve optimal performance.\n\u2022 Graph Clustering. The joint matching problem can be treated as a structured graph clustering (GC) problem, where graph nodes represent points on objects and the edge set encodes all correspondences. In this regard, any GC algorithm [27\u201332] provides a heuristic to estimate graph matching. Nevertheless, there are several intrinsic structural properties herein that are not explored by any generic GC approaches. First, our input takes a block-matrix form, where each block is highly structured (i.e. doubly-substochastic), sparse, and inter-dependent. Second, the points belonging to the same object are mutually exclusive to each other. Third, the corruption rate for different entries can be highly nonsymmetric \u2013 when translated into GC languages, this means that in-cluster edges might suffer from an order-of-magnitude larger error rate than inter-cluster edges. As a result, the findings for generic GC methods do not deliver encouraging guarantees when applied to our setting. Detailed theoretical and empirical comparisons are provided in Sections 4 and 5, respectively."}, {"heading": "1.3 Organization", "text": "The rest of the paper is organized as follows. Section 2 formally presents the problem setup, including the input model and the expected output. Our two-step recovery procedure \u2013 a spectral method followed by a convex program called MatchLift \u2013 is described in Section 3. A scalable alternating direction method of multipliers (ADMM) together with a greedy rounding strategy is also introduced in Section 3. Section 4 presents the main theoretical performance guarantees for our method under a natural randomized model.\nAll proofs of the main theorems are deferred to the appendices. We introduce numerical experiments demonstrating the practicability of our method in Section 5, as well as empirical comparison with other best-known algorithms. Finally, Section 6 concludes the paper with a summary of our findings."}, {"heading": "2 Problem Formulation and Preliminaries", "text": "This section presents the problem setup for matching multiple partially similar objects, and introduces an algebraic form for representing a collection of pairwise maps."}, {"heading": "2.1 Terminology", "text": "Below we formally define several important notions that will be used throughout this paper.\n\u2022 Set. We represent objects to be matched as discrete sets. For example, these sets can represent the vertex sets in the graph matching problem, or encode feature points when matching images.\n\u2022 Partial Map. Given two discrete sets S and S \u2032, a subset \u03c6 \u2282 S \u00d7 S \u2032 is termed a partial map if each element of S (resp. S \u2032) is paired with at most one element of S \u2032 (resp. S) \u2014 in particular, not all elements need to be paired.\n\u2022 Map Graph. A graph G = (V, E) is called a map graph w.r.t. n sets S1, \u00b7 \u00b7 \u00b7 ,Sn if (i) V := {S1, \u00b7 \u00b7 \u00b7 ,Sn}, and (ii) (Si,Sj) \u2208 E implies that pairwise estimates on the partial maps \u03c6ij and \u03c6ji between Si and Sj are available."}, {"heading": "2.2 Input and Output", "text": "The input and expected output for the joint object matching problem are described as follows.\n\u2022 Input (Noisy Pairwise Maps). Given n sets S1, \u00b7 \u00b7 \u00b7 ,Sn with respective cardinality m1, \u00b7 \u00b7 \u00b7 ,mn and a (possibly sparse) map graph G, the input to the recovery algorithm consists of partial maps \u03c6inij ((i, j) \u2208 G) between Si and Sj estimated in isolation, using any off-the-shelf pairwise matching method. Note that the input maps \u03c6inij one obtain might not agree, partially or totally, with the ground truth.\n\u2022 Output (Consistent Global Matching). The main objective of this paper is to detect and prune incorrect pairwise input maps in an efficient and reliable manner. Specifically, we aim at proposing a tractable algorithm that returns a full collection of partial maps {\u03c6ij | 1 \u2264 i, j \u2264 n} that are (i) globally consistent, and (ii) close to the provided pairwise maps \u2013 and under some conditions provably the ground-truth maps.\nAs will be detailed later, the key idea of our approach is to explore global consistency across all pairwise maps. In fact, points across different objects must form several clusters, and the ground-truth maps only exhibit in-cluster edges. We will introduce a novel convex relaxation tailored to the structure of the input maps (Section 3) and investigate its theoretical performance (Section 4)."}, {"heading": "2.3 Joint Matching in Matrix Form", "text": "In the same spirit as most convex relaxation techniques (e.g., [12, 30]), we use matrices to encode maps between objects. Specifically, we encode a partial map \u03c6ij : Si 7\u2192 Sj as a binary matrix Xij \u2208 {0, 1}|Si|\u00d7|Sj | such that Xij(s, s\u2032) = 1 iff (s, s\u2032) \u2208 \u03c6ij . Valid partial map matrices Xij shall satisfy the following doubly sub-stochastic constraints:\n0 \u2264Xij1 \u2264 1, 0 \u2264X>ij1 \u2264 1. (1)\nWe then use an n \u00d7 n block matrix X \u2208 {0, 1}N\u00d7N to encode the entire collection of partial maps {\u03c6ij | 1 \u2264 i, j \u2264 n} over {S1, \u00b7 \u00b7 \u00b7 ,Sn}:\nX =  Im1 X12 \u00b7 \u00b7 \u00b7 X1n X21 Im2 \u00b7 \u00b7 \u00b7 X2n ... ... . . .\n... Xn1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Imn  , (2) where mi := |Si| and N := \u2211n i=1mi. Note that all diagonal blocks are identity matrices, as each object is isomorphic to itself. For notational simplicity, we will use X in throughout to denote the collection of pairwise input maps, i.e. each obtained pairwise estimate \u03c6inij is encoded as a binary map matrix X in ij \u2208 {0, 1}\nmi\u00d7mj obeying the constraint (1). Some other useful notation is summarized in Table 1."}, {"heading": "3 Methodology", "text": "This section presents a novel methodology, based on a theoretically rigorous and numerically efficient framework."}, {"heading": "3.1 MatchLift: A Novel Two-Step Algorithm", "text": "We start by discussing the consistency constraint on the underlying ground-truth maps. Assume that there exists a universe S = {1, \u00b7 \u00b7 \u00b7 ,m} of m elements such that i) each object Si is a (partial) image of S; ii) each element in S is contained in at least one object Si. Then the ground-truth correspondences shall connect points across objects that are associated with the same element.\nFormally speaking, let the binary matrix Y i \u2208 {0, 1}mi\u00d7m encode the underlying correspondences between each point and the universe, i.e. for any si \u2208 Si and s \u2208 S,\nY i(si, s) = 1, iff si corresponds to s.\nThis way one can express X = Y Y >\nwith Y = (Y >1 , \u00b7 \u00b7 \u00b7 ,Y >n )>, which makes clear that\nrank(X) = m.\nThis is equivalent to the graph partitioning setting with m cliques. Consequently, a natural candidate is to seek a low-rank and positive semidefinite (PSD) matrix to approximate the input. However, this strategy does not effectively explore the sparsity structure underlying the map collection.\nTo obtain a more powerful formulation, the proposed algorithm is based on the observation that even under dense input corruption, we are often able to obtain reliable estimates on m \u2013 the universe size, using spectral techniques. This motivates us to incorporate the information of m into the formulation so as to develop tighter relaxation. Specifically, we lift X with one more dimension and consider[\nm 1>\n1 X\n] = [ 1>\nY\n] [ 1 Y > ] 0, (3)\nwhich is strictly tighter than merely imposing X 0. Intuitively, the formulation (3) entitles us one extra degree of freedom to assist in outlier pruning, which turns out to be crucial in \u201cdebiasing\u201d the errors. Encouragingly, this tightened constraint leads to remarkably improved theoretical guarantees, as will be shown in Section 4. In the following, we formally present our two-step matching procedure.\n\u2022 Step I: Estimating m. We estimate m by tracking the spectrum of the input X in. According to common wisdom (e.g. [33]), a block-sparse matrix X in must first be trimmed in order to remove the undesired bias effect caused by over-represented rows / columns. One candidate trimming procedure is provided as follows.\n\u2013 Trimming Procedure. Set dmin to be the smallest vertex degree of G, and we say the a vertex is over-represented if its vertex degree in G exceeds 2dmin. Then for each overrepresented vertex i, randomly sample 2dmin edges incident to it and set to zero all blocks X inij associated with the remaining edges.\nWith this trimming procedure, we propose to pre-estimate m via Algorithm 1.\nAlgorithm 1 Estimating the size m of the universe S\n1) trim X in, and let X\u0303 in\nbe the output. 2) perform eigenvalue decomposition on X\u0303 in ; denote by \u03bbi the ith largest eigenvalue. 3) output: m\u0302 := arg maxM\u2264i<N |\u03bbi \u2212 \u03bbi+1|, where M = max{2,max1\u2264i\u2264nmi}.\nIn short, Algorithm 1 returns an estimate of m via spectral methods, which outputs the number of dominant principal components of X in.\n\u2022 Step II: Map Recovery. Now that we have obtained an estimate on m, we are in position to present our optimization heuristic that exploits the structural property (3). In order to guarantee that the recovery is close to the provided maps \u03c6inij , one alternative is to maximize correspondence agreement (i.e. the number of compatible non-zero entries) between the input and output. This results in an objective function: \u2211\n(i,j)\u2208G\n\u3008X inij ,Xij\u3009.\nAdditionally, since a non-negative map matrix X is inherently sparse, it is natural to add an `1 regularization term to encourage sparsity, which in our case reduces to\n\u30081 \u00b7 1>,X\u3009.\nSince searching over all 0-1 map matrices is intractable, we propose to relax the binary constraints. Putting these together leads to the following semidefinite program referred to as MatchLift :\n(MatchLift) maximize X\u2208RN\u00d7N \u2211 (i,j)\u2208G \u3008X inij ,Xij\u3009 \u2212 \u03bb\u30081 \u00b7 1>,X\u3009\nsubject to Xii = Imi , 1 \u2264 i \u2264 n, X \u2265 0,[ m 1>\n1 X\n] 0. (4)\nRemark 1. Here, \u03bb represents the regularization parameter that balances the compatibility to the input and the sparsity structure. As we will show, the recovery ability of MatchLift is not sensitive to the choice of \u03bb. By default, one can set\n\u03bb =\n\u221a |E|\n2n , (5)\nwhich results in a parameter-free formulation.\nRemark 2. Careful readers will note that the set of doubly stochastic constraints (1) can be further added into the program. Nevertheless, while enforcement of these constraints (1) results in a strictly tighter relaxation, it only leads to marginal improvement when (4) is present. As a result, we remove them for the sake of computational efficiency. We note, however, that in the scenario where m is difficulty to estimate, imposing (1) will \u201cbecome crucial in allowing a constant fraction (e.g. 50%) of error rate, although dense error correction might not be guaranteed.\nThis algorithm, all at once, attempts to disentangle the ground truth and outliers as well as predict unobserved maps via convex relaxation, inspired by recent success in sparse and low-rank matrix decomposition [21, 22]. Since the ground truth matrix is simultaneously low-rank and sparse; existing methodologies, which focus on dense low-rank matrices, typically yield loose, uninformative bounds in our setting.\nFinally, we note that our matching algorithm and main results are well suited for a broad class of scenarios where each pairwise input can be modeled as a (partial) permutation matrix. For instance, our setting subsumes phase correlation [34], angular synchronization [35], and multi-signal alignment [36] as special cases."}, {"heading": "3.2 Alternating Direction Methods of Multipliers (ADMM)", "text": "Most advanced off-the-shelf SDP solvers like SeDuMi or MOSEK are typically based on interior point methods, and such second-order methods are unable to handle problems with large dimensionality. For practical applicability, we propose a first-order optimization algorithm for approximately solving MatchLift, which is a variant of the ADMM method for semidefinite programs presented in [37]. Theoretically it is guaranteed to converge. Empirically, it is often the case that ADMM converges to modest accuracy within a reasonable amount of time, and produces desired results with the assistance of appropriate rounding procedures. This feature makes ADMM practically appealing in our case since the ground-truth matrix is known to be a 0-1 matrix, for which moderate entry-wise precision is sufficient to ensure good rounding accuracy. The details of the ADMM algorithm are deferred to Appendix A."}, {"heading": "3.3 Rounding Strategy", "text": "As MatchLift solves a relaxed program of the original convex problem, it may return fractional solutions. In this case, we propose a greedy rounding method to generate valid partial maps. Given the solution X\u0302 to MatchLift, the proposed strategy proceeds as in Algorithm 2. One can verify that this simple deterministic rounding strategy returns a matrix that encodes a consistent collection of partial maps. Note that vTi denotes the ith row of a matrix V .\nAlgorithm 2 Rounding Strategy\ninitialize compute the top r eigenvalues \u03a3 = diag(\u03c31, \u00b7 \u00b7 \u00b7 , \u03c3r) and eigenvectors U = (u1, \u00b7 \u00b7 \u00b7 ,ur) of X\u0302, where r is an estimate of the total number distinctive points to be recovered. Form V = U\u03a3 1 2 . repeat 1) Let O be a unitary matrix that obeys Ov1 = e1, and set V \u2190 V O>. 2) For each of the remaining rows vi belonging to each set Sj (i \u2208 Sj), perform\nvi \u2190 e1, if \u3008vi,v1\u3009 > 0.5 and i = arg max l\u2208Sj \u3008vl,v1\u3009 .\n3) All indices i obeying vi = e1 are declared to be matched with each other, and are then removed. Repeat 1) for the next row that has not been fixed.\nuntil all the rows of V have been fixed."}, {"heading": "4 Theoretical Guarantees: Exact Recovery", "text": "Our heuristic algorithm MatchLift recovers, under a natural randomized setting, the ground-truth maps even when only a vanishing portion of the input correspondences are correct. Furthermore, MatchLift succeeds with minimal input complexity, namely, the algorithm is guaranteed to work as soon as those input maps that coincide with the ground truth maps form a connected map graph."}, {"heading": "4.1 Randomized Model", "text": "In the following, we present a natural randomized model, under which the feature of MatchLift is easiest to interpret. Specifically, consider a universe [m] := {1, 2, \u00b7 \u00b7 \u00b7 ,m}. The randomized setting consider herein is generated through the following procedure.\n\u2022 For each set Si (1 \u2264 i \u2264 n), each point s \u2208 [m] is included in Si independently with probability pset.\n\u2022 Each X inij is observed / computed independently with probability pobs.\n\u2022 Each observed X inij coincides with the ground truth independently with probability ptrue = 1\u2212 pfalse.\n\u2022 Each observed but incorrect X inij is independently drawn from a set of partial map matrices satisfying\nEX inij = 1\nm 1 \u00b7 1>, if X inij is observed and corrupted. (6)\nRemark 3. The above mean condition (6) holds, for example, when the augmented block (i.e. that obtained by enhancing Si and Sj to have all m elements) is drawn from the entire set of permutation matrices or other symmetric groups uniformly at random. While we impose (6) primarily to simplify our presentation of the analysis, we remark that this assumption can be significantly relaxed without degrading the matching performance. Remark 4. We also note that the outliers do not need to be generated in an i.i.d. fashion. Our main results hold as long as they are jointly independent and satisfy the mean condition (6)."}, {"heading": "4.2 Main Theorem: Near-Optimal Matching", "text": "We are now in position to state our main results, which provide theoretical performance guarantees for our algorithms.\nTheorem 1 (Accurate Estimation of m). Consider the above randomized model. There exists an absolute constant c1 > 0 such that with probability exceeding 1\u2212 1m5n5 , the estimate on m returned by Algorithm 1 is exact as long as\nptrue \u2265 c1 log 2 (mn) \u221a npobspset . (7)\nProof. See Appendix B.\nTheorem 1 ensures that one can obtain perfect estimate on the universe size or, equivalently, the rank of the ground truth map matrix via spectral methods. With accurate information on m, MatchLift allows perfect matching from densely corrupted inputs, as revealed below.\nTheorem 2 (Exact and Robust Matching). Consider the randomized model described above. There exist universal constants c0, c1, c2 > 0 such that for any\nc1 ( pobs m + \u221a pobs log(mn)\nnp3set\n) \u2264 \u03bb \u2264 \u221a pobs log (mn)\npset , (8)\nif the non-corruption rate obeys\nptrue > c0 log 2 (mn) \u221a npobsp2set , (9)\nthen the solution to MatchLift is exact and unique with probability exceeding 1\u2212 (mn)\u22123.\nProof. See Appendix C. Note that the performance is not sensitive to \u03bb as it can be arbitrarily chosen between \u0398 (\u221a\npobs n\n) and\n\u0398( \u221a pobs). The implications of Theorem 2 are summarized as follows.\n1. Near-Optimal Recovery under Dense Errors. Under the randomized model, MatchLift succeeds in pruning all outliers and recovering the ground truth with high probability. Somewhat surprisingly, this is guaranteed to work even when the non-corrupted pairwise maps account for only a vanishing fraction of the inputs. As a result, MatchLift achieves near-optimal recovery performance in the sense that as the number n of objects grows, its outlier-tolerance rate can be arbitrarily close to 1. Equivalently speaking, in the asymptotic regime, almost all input maps \u2013 more precisely, a fraction\n1\u2212 \u2126 (\nlog2 n\u221a n\n) (10)\nof inputs \u2013 can be badly corrupted by random errors without degrading the matching accuracy. This in turn highlights the significance of joint object matching: no matter how noisy the input sources are, perfect matching can be obtained as long as sufficiently many instances are available.\nTo the best of our knowledge, none of the prior results can support perfect recovery with more than 50% corruptions, regardless of how large n can be. The only comparative performance is reported for the robust PCA setting, where semidefinite relaxation enables dense error correction [24,25]. However, their condition cannot be satisfied in our case. Experimentally, applying RPCA on joint matching is unable to tolerate dense errors (see Section 5).\n2. Exact Matching of Partially Similar Objects. The challenge for matching partially similar objects arises in that the overlapping ratio between each pair of objects is in the order of p2set while the size of each object is in the order of pset. As correct correspondences only come from overlapping regions, it is expected that with a fixed pfalse, the matching ability degrades when pset decreases, which coincides with the bound in (9). However, the order of fault-tolerance rate with n is independent of pset as long as pset is bounded away from 0.\n3. Minimal Input Complexity. Suppose that pset and pfalse are both constants bounded away from 0 and 1, and that m = nO(poly log(n)). Condition (9) asserts that: the algorithm is able to separate outliers and fill in all missing maps reliably with no errors, as soon as the input complexity (i.e. the number of pairwise maps provided) is about the order of npoly log(n). Recall that the connectivity threshold for an Erd\u0151s\u2013Renyi graph G(n, pobs) is pobs > lognn (see [38]). This implies that MatchLift allows exact recovery nearly as soon as the input complexity exceeds the information theoretic limits."}, {"heading": "4.3 Comparison with Prior Approaches", "text": "Our exact recovery condition significantly outperforms the best-known performance guarantees, including various SDP heuristics for matching problems, as well as general graph clustering approaches when applied to object matching, detailed below.\n\u2022 Semidefinite Programming: The SDP formulation proposed by Wang and Singer [17] admits exact recovery in the full-similarity setting when ptrue > c1 for some absolute constant c1 \u2248 50% in the asymptotic regime. One might also attempt recovery by minimizing a weighted sum of nuclear norm and `1 norm as suggested in matrix completion [19] and robust PCA [21,22]. In order to enable dense error correction, robust PCA requires the sparse components (which is X in \u2212 Xgt here with Xgt denoting the ground truth) to exhibit random signs [24, 25]. This cannot be satisfied in our setting since the sign pattern of X in\u2212Xgt is highly biased (i.e. all non-negative entries of X in\u2212Xgt lying in the support of Xgt have negative signs, while all non-negative entries of X in\u2212Xgt outside the support of Xgt have positive signs).\n\u2022 Graph Clustering: Various approaches for general graph clustering have been proposed with theoretical guarantees under different randomized settings [28,29,31]. These results typically operate under the assumption that in-cluster and inter-cluster correspondences are independently corrupted, which does not apply in our model. Due to the block structure input model, these two types of corruptions are highly correlated and usually experience order-of-magnitude difference in corruption rate (i.e. (1\u2212 ptrue) m\u22121m for in-cluster edges and (1\u2212 ptrue) 1 m for inter-cluster edges). To facilitate comparison,\nwe evaluate the most recent deterministic guarantees obtained by [31]. The key metric Dmax therein can be easily bounded by Dmax \u2265 1 \u2212 ptrue due to a significant degree of in-cluster edge errors. The recovery condition therein requires\nDmax < 1\nm+ 1 , \u21d2 ptrue >\nm\nm+ 1 ,\nwhich does not deliver encouraging guarantees compared with ptrue > \u0398 ( log2 n\u221a n ) achieved by MatchLift."}, {"heading": "5 Experimental Evaluation", "text": "In this section, we evaluate the performance of MatchLift and compare it against [29] and other graph matching methods. We consider both synthetic examples, which are used to verify the exact recovery conditions described above, as well as popular benchmark datasets for evaluating the practicability on realworld images."}, {"heading": "5.1 Synthetic Examples", "text": "We follow the randomized model described in Section 4 to generate synthetic examples. For simplicity, we only consider the full observation mode, which establishes input maps between all pairs of objects. In all examples, we fix the universe size such that it consists of m = 16 points. We then vary the remaining parameters, i.e., n, pset and pfalse, to assess the performance of an algorithm. We evaluate 31 \u00d7 36 sets of parameters for each scenario, where each parameter configuration is simulated by 10 Monte Carlo trials. The empirical success probability is reflected by the color of each cell. Blue denotes perfect recovery in all experiments, and red denotes failure for all trials.\nFigure 1(a) illustrates the phase transition for pset = 0.6, when the number of objects n and pfalse vary. We can see that MatchLift is exact even when the majority of the input correspondences are incorrect (e.g., 75% when n = 150). This is consistent with the theoretical result that the lower bound on ptrue for exact recovery is O(log2 n/ \u221a n).\nFigure 1(c) shows the phase transition for n = 100, when pset and pfalse vary. We can see that MatchLift tolerates more noise when pset is large. This is also consistent with the result that the error-correction ability improves with pset.\nIn comparison, Figure 1(b) and Figure 1(d) illustrate the phase transition diagrams achieved by the algorithm proposed in [29]. One can see that MatchLift is empirically superior, as [29] is unable to allow dense error correction in our case."}, {"heading": "5.2 Real-World Examples", "text": "We have applied our algorithm on six benchmark datasets, i.e., CMU-House, CMU-Hotel, two datasets (Graf and Bikes) from [39]1 and two new datasets (referred as Chair and Building, respectively) designed for evaluating joint partial object matching. As shown in Figures 2 and 3, the Building data set contains 16 images taken around a building [4], while the Chair data set contains 16 images of a chair model from different viewpoints. In the following, we first discuss the procedure for generating the input to our algorithm, i.e., the input sets and the initial maps. We then present the evaluation setup and analyze the results.\n\u2022 Feature points and initial maps. To make fair comparisons with previous techniques on CMUHouse and CMU-Hotel, we use the features points provided in [15] and apply the spectral matching\n1available online: robots.ox.ac.uk/ vgg/research/affine\nInput MatchLift RPCA LearnI LearnII House 68.2% 100% 92.2% 99.8% 96% Hotel 64.1% 100% 90.1% 99.8% 90%\nknowledge of data-driven effect, where large object collections possess stronger self-correction power than small object collections."}, {"heading": "6 Conclusions", "text": "This paper delivers some encouraging news: given a few noisy object matches computed in isolation, a collection of partially similar objects can be accurately matched via semidefinite relaxation \u2013 an approach which provably works under dense errors. The proposed algorithm is essentially parameter-free, and can be solved by ADMM achieving remarkable efficiency and accuracy, with the assistance of a greedy rounding strategy.\nThe proposed algorithm achieves near-optimal error-correction ability, as it is guaranteed to work even when a dominant fraction of inputs are corrupted. This in turn underscore the importance of joint object matching: however low the quality of input sources is, perfect matching is achievable as long as we obtain sufficiently many instances. Also, while partial matching may incur much more severe input errors than those occurring in full-similarity matching, in many situations, the recovery ability of our algorithm is nearly the same as that in the full-similarity case (up to some constant factor). In a broader sense, our findings suggest that a large class of combinatorial / integer programming problems might be solved perfectly by semidefinite relaxation."}, {"heading": "A Alternating Direction Method of Multipliers (ADMM)", "text": "This section presents the procedure for the ADMM algorithm. For notational simplicity, we represent the convex program as follows:\nminimize X \u3008W ,X\u3009 dual variable\nsubject to A(X) = b, yA X \u2265 0, Z \u2265 0 X 0, S 0\nwhere we denote X := [ m 1>\n1 X\n] . The matrices and operators are defined as follows\n(i) W encapsulate all block coefficient matrices W ij for all (i, j) \u2208 G; (ii) A(X) = b represents the constraint that Xii = Imi (1 \u2264 i \u2264 n) and the constraint X = [ m 1>\n1 X\n] ;\n(iii) The variables on the right hand, i.e., yA,Z and S, represent dual variables associated with respective constraints.\nThe Lagrangian associated with the convex program can be given as follows L = \u2329 W ,X \u232a + \u2329 yA,A(X)\u2212 b \u232a \u2212 \u3008Z,X\u3009 \u2212 \u2329 S,X \u232a = \u2329 W +A\u2217(yA)\u2212Z \u2212 S,X \u232a \u2212 \u3008b,yA\u3009 .\nwhere A\u2217 denotes the conjugate operator w.r.t. an operator A. The augmented Lagrangian for the convex program can now be written as\nL1/\u00b5 = \u3008b,yA\u3009+ \u2329 Z + S \u2212W \u2212A\u2217(yA),X \u232a + 1\n2\u00b5 \u2016Z + S \u2212W \u2212A\u2217(yA)\u20162F.\nHere, the linear terms above represent the negative standard Lagrangian, whereas the quadratic parts represent the augmenting terms. \u00b5 is the penalty parameter that balances the standard Lagrangian and the augmenting terms. The ADMM then proceeds by alternately optimizing each primal and dual variable with others fixed, which results in closed-form solution for each subproblem. Denote by superscript k the iteration number, then we can present the ADMM iterative update procedures as follows\ny (k+1) A = (AA\n\u2217) \u22121 { A ( \u2212W + S(k) + \u00b5X(k) + Z(k) ) \u2212 \u00b5b } ,\nZ(k+1) = ( W +A\u2217 ( y\n(k+1) A\n) \u2212 S(k) \u2212 \u00b5X(k) ) + ,\nS(k+1) = Ppsd ( W +A\u2217 ( y (k+1) A ) \u2212Z(k+1) \u2212 \u00b5X(k) ) , (11)\nX (k+1) = X k + 1\n\u00b5\n( Z(k+1) + S(k+1) \u2212W \u2212A\u2217 ( y\n(k+1) A\n)) (12)\n=\u2212 1 \u00b5 Pnsd\n( W +A\u2217 ( y\n(k+1) A\n) \u2212Z(k+1) \u2212 \u00b5X(k) ) . (13)\nHere, the operator Ppsd (resp. Pnsd) denotes the projection onto the positive (resp. negative) semidefinite cone, and (\u00b7)+ operator projects all entries of a vector / matrix to non-negative values. Within a reasonable amount of time, ADMM typically returns moderately acceptable results."}, {"heading": "B Proof of Theorem 1", "text": "The key step to the proof of Theorem 1 is to show that the set of outliers, even when they account for a dominant portion of the input matrix, behave only as a small perturbation to the spectrum of the noncorrupted components. Under the randomized model described in Section 4.1, it can be easily seen that\nthe trimming procedure is not invoked with high probability. Consequently, Theorem 1 can be established through the following lemma.\nLemma 1. Given any set of n permutation matrices P i \u2208 Rm\u00d7m (1 \u2264 i \u2264 n), generate a random matrix M via the following procedure.\n1. Generate a symmetric block matrix A = [Aij ]1\u2264i,j\u2264n such that\nAii = I, 1 \u2264 i \u2264 n\nand for all i < j,\nAij =  0, if \u00b5ij = 0, P iP > j , if \u03bdij = 1 and \u00b5ij = 1,\nU ij , else, (14)\nwhere \u03bdij \u223c Bernoulli (p) and \u00b5ij \u223c Bernoulli (\u03c4) are independent binary variables, and U ij \u2208 Rm\u00d7m are independent random permutation matrices obeying EU ij = 1m1m \u00b7 1 > m.\n2. M is a principal minor of A from rows / columns at indices from a set I \u2286 {1, 2, \u00b7 \u00b7 \u00b7 ,mn}, where each 1 \u2264 i \u2264 mn is contained in I independently with probability q.\nThen there exist absolute constants c1, c2 > 0 such that if p \u2265 c1 log 2(mn) q \u221a \u03c4n\n, one has{ \u03bbi (M) \u2265 ( 1\u2212 1log(mn) ) \u03c4pqn, if 1 \u2264 i \u2264 m\n\u03bbi (M) \u2264 c2 \u221a \u03c4n log (mn) < \u03c4pqnlog(mn) , if i > m\n(15)\nwith probability exceeding 1\u2212 1m5n5 . Here, \u03bbi(M) represents the ith largest eigenvalue of M .\nProof of Lemma 1. Without loss of generality, we assume that P i = Im for all 1 \u2264 i \u2264 n, since rearranging rows / columns of A does not change its eigenvalues. For convenience of presentation, we write A = Y +Z such that\nY ii = \u03c4 ( (1\u2212 p) m 1m \u00b7 1>m + pIm ) , 1 \u2264 i \u2264 n\nand for all 1 \u2264 i \u2264 j \u2264 n:\nY ij =  0, if \u00b5ij = 0, Im, if \u03bdij = 1 and \u00b5ij = 1, U ij , else.\n(16)\nThis means that\nZij = { Im \u2212 Y ii, if i = j, 0, else.\n(17)\nApparently, Z is a block diagonal matrix satisfying\n\u2016Z\u2016 \u2264 2, (18)\nwhich is only a mild perturbation of Y . This way we have reduced to the case where all blocks (including diagonal blocks) are i.i.d., which is slightly more convenient to analyze.\nDecompose Y into 2 components Y = Y mean + Y var such that\n\u22001 \u2264 i \u2264 j \u2264 n : Y meanij = \u03c4 (\n(1\u2212 p) m\n1m \u00b7 1>m + pIm ) , (19)\n\u22001 \u2264 i \u2264 n : Y varii = 0, (20)\nand\n\u22001 \u2264 i < j \u2264 n : Y varij =  \u2212\u03c4 ( (1\u2212p) m 1m \u00b7 1 > m + pIm ) , if \u00b5ij = 0, (1\u2212 \u03c4p) Im \u2212 (1\u2212p)m 1m \u00b7 1 > m, if \u03bdij = 1 and \u00b5ij = 1, U ij \u2212 \u03c4 ( (1\u2212p) m 1m \u00b7 1 > m + pIm ) , else.\n(21)\nIn other words, Y mean represents the mean component of Y , while Y var comprises all variations around the mean component. It is straightforward to check that\nY mean 0, rank (Y mean) \u2264 m+ 1.\nIf we denote by Y meanI the principal minor coming from the rows and columns of Y at indices from I, then from Weyl\u2019s inequality one can easily see that\n\u03bbi (M) \u2265 \u03bbi (Y meanI )\u2212 \u2016Y var\u2016 \u2212 \u2016Z\u2016 \u2265 \u03bbi (Y meanI )\u2212 \u2016Y var\u2016 \u2212 2, 1 \u2264 i \u2264 m (22)\nand \u03bbi (M) \u2264 \u03bbi (Y meanI ) + \u2016Y var\u2016+ \u2016Z\u2016 \u2264 \u03bbi (Y meanI ) + \u2016Y var\u2016+ 2, i > m. (23)\nIn light of this, it suffices to evaluate \u2016Y var\u2016 as well as the eigenvalues of Y meanI . We are now in position to quantify the eigenvalues of Y meanI . Without affecting its eigenvalue distribution, one can rearrange the rows / columns of Y meanI so that\nY meanI (permutation) = \u03c4p  1n1 \u00b7 1 > n1\n. . . 1nm \u00b7 1>nm\n+ \u03c4 (1\u2212 p) m 1N \u00b7 1>N . (24)\nHere, ni (1 \u2264 i \u2264 m) denotes the cardinality of a set Ii generated by independently sampling n elements each with probability q, and we set N := n1 + \u00b7 \u00b7 \u00b7+nm for simplicity. From Bernstein inequality, there exist universal constants c5, c6 > 0 such that if q > c5 log(mn) n , then\n|ni \u2212 nq| \u2264 c6 \u221a nq log (mn), 1 \u2264 i \u2264 m (25)\nholds with probability exceeding 1\u2212 (mn)\u221210. Since Y meanI is positive semidefinite, from (24) one can easily check that all non-zero eigenvalues of Y mean I are also eigenvalues of the following (m+ 1)\u00d7 (m+ 1) matrix\nY mean I : = \u03c4  \u221a p1>n1 \u221a p1>n2 . . .\n\u221a p1>nm\u221a\n1\u2212p m 1>N\n  \u221a p1n1 \u221a p1n2 . . .\n\u221a p1nm\n\u221a 1\u2212p m 1N \n= \u03c4  pn1\n\u221a p(1\u2212p)\nm n1\npn2\n\u221a p(1\u2212p)\nm n2\n. . . ...\npnm\n\u221a p(1\u2212p)\nm nm\u221a\np(1\u2212p) m n1\n\u221a p(1\u2212p)\nm n2 \u00b7 \u00b7 \u00b7\n\u221a p(1\u2212p)\nm nm\n1\u2212p m N\n (26)\n= \u03c4qn  p\n\u221a p(1\u2212p)\nm\n. . . ... p \u221a\np(1\u2212p) m\u221a\np(1\u2212p) m\n\u00b7 \u00b7 \u00b7 \u221a\np(1\u2212p) m\n1\u2212 p  \ufe38 \ufe37\ufe37 \ufe38\nY I,0\n+ \u03c4  p\u22061\n\u221a p(1\u2212p)\nm \u22061\n. . . ...\np\u2206m\n\u221a p(1\u2212p)\nm \u2206m\u221a\np(1\u2212p) m \u22061 \u00b7 \u00b7 \u00b7 \u221a p(1\u2212p) m \u2206m 1\u2212p m \u2206N  \ufe38 \ufe37\ufe37 \ufe38\nY I,\u2206\n, (27)\nwhere \u2206i = ni \u2212 nq 1 \u2264 i \u2264 m,\nand \u2206N = N \u2212 qnm,\nwhich satisfies |\u2206N | \u2264 mmax1\u2264i\u2264m |\u2206i|. By Schur complement condition for positive definite matrices [46], if [ C B\nB> D\n] 0, then C 0 and\nD \u2212B>C\u22121B 0. Applying this condition to Y I,0 suggests that Y I,0 0 can only hold when\n(1\u2212 p)\u2212 p (1\u2212 p) m 1 p 1>m \u00b7 1m > 0,\nwhich however cannot be satisfied since (1\u2212 p)\u2212 p(1\u2212p)m 1 p1 > m \u00b7 1m = 0. Thus, Y I,0 is rank deficient.\nIn fact, all non-zero eigenvalues of Y I,0 can be quantified as well. Specifically, for any vector\nzi := ei \u2212 1\nm [ 1m 0 ] , 1 \u2264 i \u2264 m\u2212 1,\none can compute Y I,0 \u00b7 zi = (\u03c4qpn) zi, 1 \u2264 i \u2264 m\u2212 1. (28)\nThat said, \u03c4qpn is an eigenvalue of Y I,0 with multiplicity m\u2212 1. On the other hand, we have Y I,0 \u00b7 [ 1m\u221a (1\u2212p)m p ] = \u03c4qn [ 1m\u221a (1\u2212p)m p ] , z>i \u00b7 [ 1m\u221a (1\u2212p)m\np\n] = 0,\n(29)\nindicating that \u03c4qn is another eigenvalue of Y I,0. Putting these together yields\n\u03bbi ( Y I,0 ) =  \u03c4qn, i = 1\n\u03c4pqn, 2 \u2264 i \u2264 m, 0, i > m.\n(30)\nFurthermore, the residual component Y I,\u2206 can be bounded as follows\n\u2225\u2225Y I,\u2206\u2225\u2225 \u2264 \u03c4 \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225  p\u22061 . . .\np\u2206m 1\u2212p m \u2206N\n \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225+ \u03c4 \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225  0\n\u221a p(1\u2212p)\nm \u22061\n. . . ... 0 \u221a\np(1\u2212p) m \u2206m\u221a p(1\u2212p)\nm \u22061 \u00b7 \u00b7 \u00b7\n\u221a p(1\u2212p)\nm \u2206m 0\n \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\nF \u2264 \u03c4 max { p max\n1\u2264i\u2264m |\u2206i| , 1\u2212 p m |\u2206N |\n} + \u03c4 \u221a\n2p (1\u2212 p) max 1\u2264i\u2264m |\u2206i|\n\u2264 2\u03c4 max 1\u2264i\u2264m\n|\u2206i| \u2264 2c6\u03c4 \u221a nq log (mn),\nwhere the last inequality follows from (25). This taken collectively with (27) and (30) yields that: when p > 2c6 log\n2(mn)\u221a nq or, equivalently, when 2c6\n\u221a nq log (mn) < 1\nlog1.5(mn) npq, one has \u03bbi (Y mean I ) \u2265 ( 1\u2212 1 log 3 2 (mn) ) \u03c4pqn, 1 \u2264 i \u2264 m,\n\u03bbi (Y mean I ) \u2264 2c6\u03c4\n\u221a nq log (mn) \u2264 1\nlog 3 2 (mn)\n\u03c4pqn, i > m. (31)\nFurthermore, observe that EY varij = 0, E \u2225\u2225\u2225 12\u221a\u03c4Y varij \u2225\u2225\u22252 \u2264 1, and 12\u221a\u03c4 \u2225\u2225Y varij \u2225\u2225 \u2264 1\u221a\u03c4 . When \u03c4 > 1n , Lemma\n3 yields that \u2016Y var\u2016 \u2264 2c0 \u221a \u03c4n log (mn) (32)\nwith probability at least 1\u2212 (mn)\u22125. Hence, \u2016Y var\u2016 = o (\u03c4pqn) if p > c10 log 2 n\nq \u221a \u03c4n for some constant c10 > 0. Finally, the claim follows by substituting (31) and (32) into (22) and (23)."}, {"heading": "C Proof of Theorem 2", "text": "To prove Theorem 2, we first analyze the Karush\u2013Kuhn\u2013Tucker (KKT) condition for exact recovery, which provides a sufficient and almost necessary condition for uniqueness and optimality. Valid dual certificates are then constructed to guarantee exact recovery.\nC.1 Preliminaries and Notations Without loss of generality, we can treat Xgt as a sub-matrix of an augmented square matrix Xgtsup such that\nXgtsup := 1 \u00b7 1> \u2297 In, (33)\nand\nXgt :=  \u03a01 \u03a02 . . .\n\u03a0n\nXgtsup  \u03a0>1 \u03a0>2\n. . . \u03a0>n  , (34) where the matrices \u03a0i \u2208 R|Si|\u00d7m are defined such that \u03a0i denotes the submatrix of Im coming from its rows at indices from Si. For instance, if Si = {2, 3}, then one has\n\u03a0i = [ 0 1 0 \u00b7 \u00b7 \u00b7 0 0 0 1 \u00b7 \u00b7 \u00b7 0 ] .\nWith this notation, \u03a0iM\u03a0>j represents a submatrix of M \u2208 Rm\u00d7m coming from the rows at indices from Si and columns at indices from Sj . Conversely, for any matrix M\u0303 \u2208 R|Si|\u00d7|Sj |, the matrix \u03a0>i M\u0303\u03a0j converts M\u0303 to an m\u00d7m matrix space via zero padding.\nWith this notation, we can represent X in as a submatrix of X insup, which is a corrupted version of X gt sup\nand obeys X inij := \u03a0i ( X insup ) ij \u03a0>j . (35)\nFor notational simplicity, we set\nW ij := { \u2212X inij + \u03bb1 \u00b7 1>, if (i, j) \u2208 G, \u03bb1 \u00b7 1>, else.\n(36)\nBefore continuing to the proof, it is convenient to introduce some notations that will be used throughout. Denote by \u2126gt and \u2126\u22a5gt the support of X\ngt and its complement support, respectively, and let P\u2126gt and P\u2126\u22a5gt represent the orthogonal projection onto the linear space of matrices supported on \u2126gt and its complement support \u2126\u22a5gt, respectively. Define Tgt to be the tangent space at X\ngt w.r.t. all symmetric matrices of rank at most m, i.e. the space of symmetric matrices of the form\nTgt :=   \u03a01 \u03a02 ...\n\u03a0n\nM + M> [ \u03a0>1 \u03a0>2 \u00b7 \u00b7 \u00b7 \u03a0>n ] : M \u2208 Rm\u00d7N  , (37)\nand denote by T\u22a5gt its orthogonal complement. We then denote by PTgt (resp. PT\u22a5gt ) the orthogonal projection onto Tgt (resp. T\u22a5gt). In passing, if we define\n\u03a3 := Diag\n{[ n\nn1 , \u00b7 \u00b7 \u00b7 , n nm\n]} , (38)\nthen the columns of\nU := 1\u221a n  \u03a01 \u03a02 ...\n\u03a0n \u03a3 12 (39) form the set of eigenvectors of Xgt, and for any symmetric matrix M ,\nPT\u22a5gt (M) = ( I \u2212UU> ) M ( I \u2212UU> ) . (40)\nFurthermore, we define a vector d to be\nd :=  \u03a01 \u03a02 ...\n\u03a0n \u03a31m. (41) Put another way, if any row index j of Xgt is associated with the element s \u2208 [m], then dj = nns . One can then easily verify that \u2329\nd \u00b7 d>,Xgt \u2212 1 m\n1 \u00b7 1> \u232a = \u2329 d \u00b7 d>,Xgt \u232a \u2212 1 m ( 1> \u00b7 d )2 = 0. (42)\nIn fact, when ni\u2019s are sufficiently close to each other, d \u00b7 d> is a good approximation of 1 \u00b7 1>, as claimed in the following lemma. Lemma 2. Consider a set of Bernoulli random variables \u03bdi \u223c Bernoulli (p) (1 \u2264 i \u2264 n), and set s :=\u2211n i=1 \u03bdi. Let ni (1 \u2264 i \u2264 m) be independent copies of s, and denote N = n1 + \u00b7 \u00b7 \u00b7+ nm. If p > c7 log 2(mn) n , then the matrix\nA := (np) 2  1 n1 1n1 1 n2 1n2 ...\n1 nm 1nm  [ 1n1 1>n1 1n2 1>n2 \u00b7 \u00b7 \u00b7 1nm 1>nm ] (43) satisfies \u2225\u2225\u2225\u2225 1mA\u2212 1m1N \u00b7 1>N\n\u2225\u2225\u2225\u2225 \u2264 c8\u221anp log(mn) (44) and \u2225\u2225A\u2212 1N \u00b7 1>N\u2225\u2225\u221e \u2264 c9 \u221a log(mn)\nnp (45)\nwith probability exceeding 1\u2212 1m5n5 , where c7, c8, c9 are some universal constants.\nProof. See Appendix D.1.\nSince p2d \u00b7 d> is equivalent to A defined in (43) up to row / column permutation, Lemma 2 reveals that\u2225\u2225\u2225\u2225p2md \u00b7 d> \u2212 1m1N \u00b7 1>N \u2225\u2225\u2225\u2225 \u2264 c8\u221anp log(mn)\nwith high probability. The following bound on the operator norm of a random block matrix is useful for deriving our main results.\nLemma 3. Let M = [M ij ]1\u2264i,j\u2264n be a symmetric block matrix, where M ij\u2019s are jointly independent mi\u00d7mj matrices satisfying\nEM ij = 0, E \u2016M ij\u20162 \u2264 1, and \u2016M ij\u2016 \u2264 \u221a n, (1 \u2264 i, j \u2264 n). (46)\nBesides, mi \u2264 m holds for all 1 \u2264 i \u2264 n. Then there exists an absolute constant c0 > 0 such that\n\u2016M\u2016 \u2264 c0 \u221a n log (mn)\nholds with probability exceeding 1\u2212 1m5n5 .\nProof. See Appendix D.2.\nAdditionally, the second smallest eigenvalue of the Laplacian matrix of a random Erd\u0151s\u2013R\u00e9nyi graph can be bounded below by the following lemma.\nLemma 4. Consider an Erd\u0151s\u2013R\u00e9nyi graph G \u223c G(n, p) and any positive integer m, and let L \u2208 Rn\u00d7n represent its (unnormalized) Laplacian matrix. There exist absolute constants c3, c4 > 0 such that if p > c3 log 2 (mn) /n, then the algebraic connectivity a (G) of G (i.e. the second smallest eigenvalue of L) satisfies\na (G) \u2265 np\u2212 c4 \u221a np log (mn) (47)\nwith probability exceeding 1\u2212 2(mn)5 .\nProof. See Appendix D.3.\nFinally, if we denote by ns (resp. ns,t) the number of sets Si (1 \u2264 i \u2264 n) containing the element s (resp. containing s and t simultaneously), then these quantities sharply concentrate around their mean values, as stated in the following lemma.\nLemma 5. There are some universal constants c8, c9 > 0 such that if p2set > log(mn) n , then\n|ns \u2212 npset| \u2264 \u221a c8npset log (mn), \u22001 \u2264 s \u2264 m,\u2223\u2223ns,t \u2212 np2set\u2223\u2223 \u2264\u221ac8np2set log (mn), \u22001 \u2264 s < t \u2264 m,\nhold with probability exceeding 1\u2212 1(mn)10 .\nProof. In passing, the claim follows immediately from the Bernstein inequality that\nP (\u2223\u2223\u2223\u2223\u2223 n\u2211 i=1 \u03bdi \u2212 np \u2223\u2223\u2223\u2223\u2223 > t ) \u2264 2 exp ( \u2212\n1 2 t 2\nnp(1\u2212 p) + 13 t ) where \u03bdi \u223c Bernoulli(p) are i.i.d. random variables. Interested readers are referred to [47] for a tutorial.\nC.2 Optimality and Uniqueness Condition Recall that ni := |Ii| denotes the number of sets Sj containing the element i. The convex relaxation is exact if one can construct valid dual certificates, as summarized in the following lemma.\nLemma 6. Suppose that there exist dual certificates \u03b1 > 0, Z = [Zij ]1\u2264i,j\u2264n \u2208 S N\u00d7N and Y = [Y ij ]1\u2264i,j\u2264n \u2208 SN\u00d7N obeying\nY \u2212 \u03b1dd> 0, (48) P\u2126gt (Z) = 0, P\u2126\u22a5gt (Z) \u2265 0, (49)\nY ij = W ij \u2212Zij , 1 \u2264 i < j \u2264 n, (50) Y \u2212 \u03b1dd> \u2208 T\u22a5gt . (51)\nThen Xgt is the unique solution to MatchLift if either of the following two conditions is satisfied: i) All entries of Zij (\u2200i 6= j) within the support \u2126\u22a5gt are strictly positive; ii) For all M satisfying PT\u22a5gt (M) 0,\u2329\nY \u2212 \u03b1dd>,PT\u22a5gt (M) \u232a > 0, (52)\nand, additionally, n\nni +\nn nj 6= n\n2\nninj , 1 \u2264 i, j \u2264 m. (53)\nProof. See Appendix D.4.\nThat said, to prove Theorem 2, it is sufficient (under the hypotheses of Theorem 2) to generate, with high probability, valid dual certificates Y , Z and \u03b1 > 0 obeying the optimality conditions of Lemma 6. This is the objective of the next subsection.\nC.3 Construction of Dual Certificates Decompose the input X in into two components X in = X false + Xtrue, where\nXtrue = P\u2126gt ( X in ) , and X false = P\u2126\u22a5gt ( X in ) . (54)\nThat said, Xtrue (resp. X false) consists of all correct (resp. incorrect) correspondences (i.e. non-zero entries) encoded in X in. This allows us to write\nW ij =\n{ \u2212X falseij + \u03bbEij \u2212X true ij + \u03bbE \u22a5 ij , if (i, j) \u2208 G,\n\u03bbEij + \u03bbE \u22a5 ij , else,\n(55)\nwhere E and E\u22a5 are defined to be\nE := P\u2126gt ( 1 \u00b7 1> ) , and E\u22a5 := 1 \u00b7 1> \u2212E. (56)\nWe propose constructing the dual certificate Y by producing three symmetric matrix components Y true,1, Y true,2, and Y L separately, as follows.\n1. Construction of Zm and Rm. For any \u03b2 \u2265 0, define \u03b1\u03b2 to be\n\u03b1\u03b2 := arg min \u03b1:\u03b21\u00b71>\u2212\u03b1d\u00b7d>\u22650 \u2225\u2225\u2225\u03b21 \u00b7 1> \u2212 \u03b1d \u00b7 d>\u2225\u2225\u2225 \u221e . (57)\nBy setting \u03b20 := \u03bb\u2212 (1\u2212ptrue)pobsm \u2212 \u221a c10pobs log(mn) np3set , we produce Zm and Rm as follows\nZm = P\u2126\u22a5gt\n(( \u03bb\u2212 (1\u2212 ptrue) pobs\nm\n) 1 \u00b7 1> \u2212 \u03b1\u03b20d \u00b7 d > )\n(58)\nand Rm = P\u2126gt (( \u03bb\u2212 (1\u2212 ptrue) pobs\nm\n) 1 \u00b7 1> \u2212 \u03b1\u03b20d \u00b7 d > )\n(59)\nfor some sufficiently large constant c10 > 0.\n2. Construction of Y true,1 and Y true,2. We set\nY true,1ij =\n{ \u2212Xtrueij +\n(1\u2212ptrue)pobs m Eij , if i < j,\u2211n\nj=1 ( Xtrueij \u2212 (1\u2212ptrue)pobs m Eij ) \u03a0j\u03a0 > i , if i = j,\nand\nY true,2ij = { Rmij , if i < j, \u2212 \u2211n j=1 R m ij\u03a0j\u03a0 > i , if i = j.\n1 2 1 3 2 3 4 2 3 1 0 0 0 0 0 0 0 0 0 2 0 0 0 \u22121 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 3 0 \u22121 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0\n1 2 1 3 2 3 4 2 3 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0.5 0 0 0.5 1 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0.5 0 0 0.5 0 2 0 0 0 0.5 0 \u22120.5 0 0 0 3 0 0.5 0 0 \u22120.5 0 0 0 0 4 0 0 0 0 0 0 0 0 0 2 0 0 0 0.5 0 0 0 0 \u22120.5 3 0 0.5 0 0 0 0 0 \u22120.5 0\n(a) Input Y 0 (b) ZL\nRemark 5. Below is a toy example to illustrate the proposed procedure for constructing ZL. Consider three sets S1 = {1, 2}, S2 = {1, 3}, S3 = {2, 3, 4}, and S4 = {1, 3}. Suppose that Y L,0 only contains two non-zero entries that incorrectly maps elements 1 to 3 in Y L,012 , as illustrated in Fig. 7(a). The resulting Z\nL is shown in Fig. 7(b). Clearly, Y L,0 + ZL obeys Y L,0 + ZL \u2208 T\u22a5gt .\nWith the above construction procedure, one can easily verify that: (1) Y true,1, Y true,2 and Y L are all contained in the space T\u22a5gt ; (2) P\u2126gt (Z) = 0; (3) If we set Mm := \u03b1\u03b20d \u00b7 d >, then for any i 6= j,\nY ij =Y true,1 ij + Y true,2 ij + Y L ij + M m ij\n=\u2212Xtrueij + (1\u2212 ptrue) pobs\nm Eij + R\nm ij \u2212X false ij + (1\u2212 ptrue) pobs m E\u22a5ij + Z L ij + M m ij\n=\u2212Xtrueij \u2212X false ij + \u03bb1 \u00b7 1> \u2212\n(( \u03bb\u2212 (1\u2212 ptrue) pobs\nm\n) 1 \u00b7 1> \u2212Rmij ) + ZLij + M m ij\n=W ij \u2212 ( Zmij \u2212Z L ij ) . (62)\nFurthermore, from Lemma 2 one can obtain\n\u2225\u2225\u2225d \u00b7 d> \u2212 1 \u00b7 1>\u2225\u2225\u2225 \u221e = O\n(\u221a log (mn)\nnpset\n) .\nThis taken collectively with (57) and the assumption (8) ensures that\n\u03b1\u03b20 = \u03bb\u2212 (1\u2212 ptrue) pobs\nm \u2212O\n(\u221a c10pobs log (mn)\nnp3set\n) > 0 (63)\nas long as p3set > c15 log(mn) n for some constant c15 > 0. Consequently, we will establish that Y and Z are valid dual certificates if they satisfy{\nall entries of Zmij \u2212Z L ij (\u2200i 6= j) within \u2126\u22a5gt are strictly positive; Y true,1 + Y true,2 + Y L 0. (64)\nSuch conditions will be established through the following lemmas.\nLemma 7. There are some universal constants c0, c1 > 0 such that\u2225\u2225\u2225Y L\u2225\u2225\u2225 \u2264 c0 \u221a npobs log (mn)\np2set\nand \u2225\u2225\u2225ZLij\u2225\u2225\u2225\u221e \u2264 \u221a c1pobs log (mn) np3set , 1 \u2264 i < j \u2264 n\nwith probability exceeding 1\u2212 1(mn)4 .\nProof. See Appendix D.5.\nLemma 8. There are some universal constants c5, c6, c7 > 0 such that if ptruepobspset > c7 log 2(mn) n and\n\u03bb <\n\u221a pobs log(mn)\npset , then with probability exceeding 1\u2212 1(mn)10 , one has\u2225\u2225Y true,2\u2225\u2225 \u2264 c5\u221anpobs\npset log (mn) ,\nand \u2329 vv>,Y true,1 \u232a \u2265 1\n2 npsetptruepobs \u2212 c6\n\u221a npsetpobs log (mn)\nfor all unit vector v satisfying vv> \u2208 T\u22a5gt .\nProof. See Appendix D.6.\nCombining Lemmas 7 and 8 yields that there exists an absolute constant c0 > 0 such that if\nptrue > c0 log2 (mn)\u221a npobsp4set ,\nthen Y = Y true,1 + Y true,2 + Y L 0.\nOn the other hand, observe that all entries of the non-negative matrix Zm lying in the index set \u2126\u22a5gt are bounded below in magnitude by \u221a\nc10pobs log(mn) np3set . For sufficiently large c10, one can conclude that all entries\nof Zmil \u2212Z L il outside \u2126gt are strictly positive.\nSo far we have justified that Y and Z satisfy (64), thereby certifying that the proposed algorithm correctly recovers the ground-truth matching."}, {"heading": "D Proofs of Auxiliary Lemmas", "text": "D.1 Proof of Lemma 2 Denote by A := 1N \u00b7 1TN . From Bernstein inequality, ni sharply concentrates around np such that if p > c6 log\n2(mn) n\n|ni \u2212 np| \u2264 c5 \u221a np log(mn), \u22001 \u2264 i \u2264 m (65)\nwith probability exceeding 1\u2212 (mn)\u221210, where c5, c6 > 0 are some absolute constants. The bound (65) also implies that\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225I \u2212  np n1 np n2\n. . . np nm\n \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2264 max1\u2264i\u2264m |ni \u2212 np| ni \u2264 c5 \u221a np log(mn) np\u2212 c5 \u221a np log(mn)\n\u2264 2c5\n\u221a log(mn)\nnp .\nSimilarly, one has |N \u2212 nmp| \u2264 c5 \u221a pmn log(mn)\nwith probability exceeding 1\u2212 (mn)\u221210, which implies that\u2225\u2225A\u2225\u2225 = N \u2264 nmp+ c5\u221apmn log(mn) < 2nmp. Rewrite A as\nA :=  np n1 Diag (1n1)\n. . . np nm Diag (1nm)\n \u00b7A \u00b7  np n1 Diag (1n1)\n. . . np nm Diag (1nm)\n .\nThis allows us to bound the deviation of A from A as follows \u2225\u2225A\u2212A\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225\u2225\u2225A\u2212  np n1 Diag (1n1)\n. . . np nm Diag (1nm)\nA \u2225\u2225\u2225\u2225\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225\u2225\u2225\u2225  np n1 Diag (1n1)\n. . . np nm Diag (1nm)\nA\u2212A \u2225\u2225\u2225\u2225\u2225\u2225\u2225\n\u2264  \u2225\u2225\u2225\u2225\u2225\u2225\u2225  np n1 Diag (1n1)\n. . . np nm Diag (1nm)\n \u2225\u2225\u2225\u2225\u2225\u2225\u2225+ 1 \u2225\u2225A\u2225\u2225 \u2225\u2225\u2225\u2225\u2225\u2225\u2225I \u2212  np n1 Diag (1n1)\n. . . np nm Diag (1nm)\n \u2225\u2225\u2225\u2225\u2225\u2225\u2225\n\u2264 ( 1 + c5 \u221a log(mn)\nnp + 1\n) 2nmp \u00b7 2c5 \u221a log(mn)\nnp \u2264 c6m \u221a np log(mn)\nfor some universal constant c6 > 0. On the other hand, it follows immediately from (65) that\n\u2225\u2225A\u2212 1 \u00b7 1>\u2225\u2225\u221e = max1\u2264i,j\u2264m \u2223\u2223\u2223\u2223\u2223 (np)2ninj \u2212 1 \u2223\u2223\u2223\u2223\u2223 = max1\u2264i,j\u2264m \u2223\u2223\u2223\u2223pn (pn\u2212 nj) + (pn\u2212 ni)njninj \u2223\u2223\u2223\u2223 \u2264 max\n1\u2264i,j\u2264m \u2223\u2223\u2223pn+ c5\u221anp log(mn)\u2223\u2223\u2223( pn\u2212 c5 \u221a np log(mn)\n)2 c5\u221anp log(mn) \u2264 c9 \u221a log(mn)\nnp\nfor some absolute constant c9 > 0.\nD.2 Proof of Lemma 3 The norm of M can be bounded via the moment method, which attempts to control tr(Mk) for some even integer k. See [48, Section 2.3.4] for a nice introduction.\nSpecifically, observe that Etr(Mk) can be expanded as follows\nEtr ( Mk ) = \u2211 1\u2264i1,\u00b7\u00b7\u00b7 ,ik\u2264n Etr (M i1i2M i2i3 \u00b7 \u00b7 \u00b7M iki1) ,\na trace sum over all k-cycles in the vertex set {1, \u00b7 \u00b7 \u00b7 , n}. Note that (i, i) are also treated as valid edges. For each term Etr(M i1i2M i2i3 \u00b7 \u00b7 \u00b7M iki1), if there exists an edge occurring exactly once, then the term vanishes due to the independence assumption. Thus, it suffices to examine the terms in which each edge is repeated at least twice. Consequently, there are at most k/2 relevant edges, which span at most k/2+1 distinct vertices. We also need to assign vertices to k/2 edges, which adds up to no more than (k/2)k different choices.\nBy following the same procedure and notation as adopted in [48, Page 119], we divide all non-vanishing k-cycles into (k/2)k classes based on the above labeling order; each class is associated with j (1 \u2264 j \u2264 k/2) edges e1, \u00b7 \u00b7 \u00b7 , ej with multiplicities a1, \u00b7 \u00b7 \u00b7 , aj , where (e1, \u00b7 \u00b7 \u00b7 , a1, \u00b7 \u00b7 \u00b7 , aj) determines the class of cycles and a1 + \u00b7 \u00b7 \u00b7+ aj = k. Since there are at most nj+1 distinct vertices, one can see that no more than nj+1 cycles falling within this particular class. For notational simplicity, set K = \u221a n, and hence \u2016M ij\u2016 \u2264 K. By assumption (46), one has\nEtr (M i1i2M i2i3 \u00b7 \u00b7 \u00b7M iki1) \u2264 mE ( \u2016M e1\u2016 a1 \u00b7 \u00b7 \u00b7 \u2225\u2225M ej\u2225\u2225aj)\n\u2264 mE \u2016M e1\u2016 2 \u00b7 \u00b7 \u00b7E \u2225\u2225M ej\u2225\u22252Ka1\u22122 \u00b7 \u00b7 \u00b7Kaj\u22122 \u2264 mKk\u22122j .\nThus, the total contribution of this class does not exceed\nmnj+1Kk\u22122j = mn k 2 +1.\nBy summing over all classes one obtains the crude bound\nEtr ( Mk ) \u2264 m ( k\n2\n)k n k 2 +1,\nwhich follows that\nE \u2016M\u2016k \u2264 Etr ( Mk ) \u2264 m ( k\n2\n)k n k 2 +1.\nIf we set k = log (mn), then from Markov\u2019s inequality we have\nP ( \u2016M\u2016 \u2265 k\n2 n\n1 2 + 1 k (mn) 5 k m 1 k ) \u2264 E \u2016M\u2016\nk( k 2n 1 2 + 1 k (mn) 5 k m 1 k\n)k \u2264 m ( k 2 )k n k 2 +1\nm ( k 2 )k n k 2 +1 (mn) 5 \u2264 1 (mn) 5 .\nSince n 1 log n = O (1), there exists a constant c0 > 0 such that\nP ( \u2016M\u2016 \u2265 c0n 1 2 log (mn) ) \u2264 1 m5n5 ,\nwhich completes the proof.\nD.3 Proof of Lemma 4 When G \u223c G(n, p), the adjacency matrixA consists of independent Bernoulli components (except for diagonal entries), each with mean p and variance p(1\u2212 p). Lemma 3 immediately implies that if p > 2 log(mn)n , then\n1\u221a p(1\u2212 p) \u2225\u2225A\u2212 p1n \u00b7 1>n \u2225\u2225 \u2264 c0\u221an log (mn) + 1 (66) with probability at least 1\u2212 (mn)\u22125. That said, there exists an absolute constant c1 > 0 such that\u2225\u2225A\u2212 p1n \u00b7 1>n \u2225\u2225 \u2264 c1\u221apn log (mn) (67) with probability exceeding 1\u2212 (mn)\u22125.\nOn the other hand, from Bernstein inequality, the degree of each vertex exceeds dmin := pn\u2212 c2 \u221a pn log (mn) (68)\nwith probability at least 1 \u2212 (mn)\u221210, where c2 is some constant. When p > 2 log(mn)n , G is connected, and hence the least eigenvalue of L is zero with the eigenvector 1n. This taken collectively with (67) and (68) suggests that when p > c 2 3 log\n2(mn) n , one has\na (G) \u2265 dmin \u2212 \u2225\u2225A\u2212 p1n \u00b7 1>n \u2225\u2225 \u2265 pn\u2212 c3\u221apn log (mn)\nwith high probability.\nD.4 Proof of Lemma 6 Suppose that Xgt + H is the solution to MatchLift for some perturbation H 6= 0. By Schur complement\ncondition for positive definiteness, the feasibility constraint [ m 1>\n1 Xgt + H\n] 0 is equivalent to\n{ Xgt + H 0, Xgt + H \u2212 1m1 \u00b7 1 > 0,\nwhich immediately yields PT\u22a5gt (H) = ( I \u2212UU> ) ( Xgt + H ) ( I \u2212UU> ) 0, (69) and \u2329 d \u00b7 d>,H \u232a = \u2329 d \u00b7 d>,Xgt \u2212 1\nm 1 \u00b7 1> + H\n\u232a \u2265 0. (70)\nThe above inequalities follow from the facts PT\u22a5gt ( Xgt ) = 0 and \u2329 d \u00b7 d>,Xgt \u2212 1m1 \u00b7 1 > \u232a\n= 0. From Assumption (51), one can derive\u2329\nY \u2212 \u03b1d \u00b7 d>,PT\u22a5gt (H) \u232a + \u2329 \u03b1d \u00b7 d>,H \u232a = \u2329 Y \u2212 \u03b1d \u00b7 d>,H \u232a + \u2329 \u03b1d \u00b7 d>,H \u232a = \u3008Y ,H\u3009 =\n\u2211 i 6=j \u3008Y ij ,Hij\u3009 . (71)\nThis allows us to bound \u2329 Y \u2212 \u03b1d \u00b7 d>,PT\u22a5gt (H) \u232a + \u2211 i6=j \u3008Zij ,Hij\u3009\n\u2264 \u2329 Y \u2212 \u03b1d \u00b7 d>,PT\u22a5gt (H) \u232a + \u2329 \u03b1d \u00b7 d>,H \u232a + \u2211 i 6=j \u3008Zij ,Hij\u3009 (72)\n= \u2211 i 6=j \u3008Y ij ,Hij\u3009+ \u2211 i 6=j \u3008Zij ,Hij\u3009 (73)\n= \u2211 i 6=j \u3008W ij ,Hij\u3009 , (74)\nwhere the first inequality follows from (70), and the last equality follows from Assumption (50). In order to preclude the possibility that Xgt + H is the solution to MatchLift, we need to show that\u2211 i 6=j \u3008W ij ,Hij\u3009 > 0. From (74) it suffices to establish that\u2329\nY \u2212 \u03b1d \u00b7 d>,PT\u22a5gt (H) \u232a + \u2211 i6=j \u3008Zij ,Hij\u3009 > 0 (75)\nfor any feasible H 6= 0. In fact, since Y \u2212 \u03b1d \u00b7 d> and PT\u22a5gt (H) are both positive semidefinite, one must have \u2329\nY \u2212 \u03b1d \u00b7 d>,PT\u22a5gt (H) \u232a \u2265 0. (76)\nOn the other hand, the constraints\nsupp (Z) \u2286 \u2126\u22a5gt, P\u2126\u22a5gt (Z) \u2265 0, and P\u2126\u22a5gt (H) \u2265 0\ntaken together imply that \u2211 i 6=j \u3008Zij ,Hij\u3009 \u2265 0. (77)\nPutting (76) and (77) together gives\u2329 Y \u2212 \u03b1d \u00b7 d>,PT\u22a5gt (H) \u232a + \u2211 i 6=j \u3008Zij ,Hij\u3009 \u2265 0.\nComparing this with (75), we only need to establish either \u2329 Y \u2212 \u03b1d \u00b7 d>,PT\u22a5gt (H) \u232a > 0 or \u2211 i6=j \u3008Zij ,Hij\u3009 > 0.\ni) Suppose first that all entries of Zij (\u2200i 6= j) in the support \u2126\u22a5gt are strictly positive. If the identity\u2211 i 6=j \u3008Zij ,Hij\u3009 = 0 holds, then the strict positivity assumption of Zij on \u2126\u22a5gt as well as the constraint P\u2126\u22a5gt (H) \u2265 0 immediately leads to P\u2126\u22a5gt (H) = 0.\nBesides, the feasibility constraint requires that P\u2126gt (Hij) \u2264 0. If P\u2126gt (Hij) 6= 0, then all non-zero entries of Hij are negative, and hence \u2329\nd \u00b7 d>,H \u232a = \u2329 d \u00b7 d>,P\u2126gt (H) \u232a < 0,\nwhich follows since all entries of d are strictly positive. This contradicts with (70). Consequently, we must either have H = 0 or \u2211 i 6=j \u3008Zij ,Hij\u3009 > 0. This together with (75) establishes the claim.\nii) Next, we prove the claim under Assumptions (52) and (53). In fact, Assumption (52) together with (69) asserts that \u2329 Y ,PT\u22a5gt (H) \u232a \u2264 0 can only occur if PT\u22a5gt (H) = 0. This necessarily leads to H = 0, as claimed by Lemma 9.\nLemma 9. Suppose that Xgt + H is feasible for MatchLift, and assume that\nn ni + n nj 6= n\n2\nninj , \u22001 \u2264 i, j \u2264 m. (78)\nIf PT\u22a5gt (H) = 0, then one has H = 0.\nProof. See Appendix D.7.\nIn summary, we can conclude that Xgt is the unique optimizer in both cases.\nD.5 Proof of Lemma 7 First, we would like to bound the operator norm of Y L. Since each randommatrixX inijI{Xinij is observed and corrupted} is independently drawn with mean (1\u2212ptrue)pobsm 1 \u00b7 1 >, it is straightforward to see that\nEY L,0 = E ( \u2212X false + (1\u2212 ptrue) pobs m E\u22a5 ) = 0.\nBy observing that ZL is constructed as a linear transform of Y L,0, one can also obtain\nEZL = 0, \u21d2 EY L = EZL + EY L,0 = 0. Thus, it suffices to examine the deviation of \u2225\u2225\u2225Y L\u2225\u2225\u2225 incurred by the uncertainty of X false.\nDenote by Ai,j \u2208 RN\u00d7N the component of ZL generated due to the (i, j)th block \u2212X falseij , which clearly satisfies\nZL = Ai,j \u2212 EAi,j .\nFor each non-zero entry of X falseij , if it encodes an incorrect correspondence between elements s and t, then it will affect no more than 6ns,t entries in Ai,j , where each of these entries are affected in magnitude by an amount at most 1ns,t . Recall that ns,t represents the number of sets Si (1 \u2264 i \u2264 n) containing s and t\nsimultaneously, which sharply concentrates within [ np2set \u00b1O (\u221a np2set log (mn) )] as asserted in Lemma 5. As a result, the sum of squares of these affected entries is bounded by\n6ns,t n2s,t = O\n( 1\nns,t\n) . (79)\nMoreover, since each row / column of X falseij can have at most one non-zero entry, we can rearrange A i,j with row / column permutation such that Ai,j becomes a block-diagonal matrix, where the components\naffected by different entries of X falseij are separated into distinct diagonal blocks. This together with (79) leads to \u2225\u2225Ai,j\u2225\u2225 \u2264 \u2225\u2225Ai,j\u2225\u2225\nF \u2264 max\ns 6=t\n\u221a 8\nns,t ,\nand hence \u2225\u2225\u2225EAi,j (Ai,j)>\u2225\u2225\u2225 \u2264 pobs(max s6=t \u221a 8\nns,t\n)2 \u2264 c16pobs\nnp2set\nfor some absolute constant c16 > 0, where the last inequality follows from Lemma 5. Observe that Ai,j \u2212 EAi,j (i 6= j) are independently generated with mean zero, whose operator norm\nis bounded above by 2 maxs 6=t \u221a\n8 ns,t . Applying the matrix Bernstein inequality [49, Theorem 1.4] suggests\nthat there exist universal constants c5, c6 > 0 such that for any t = O ( \u221a npoly log (mn)),\nP \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\n(i,j)\u2208G\nAi,j \u2212 EAi,j \u2225\u2225\u2225\u2225\u2225\u2225 > t  \u2264 n2 exp \u2212 12 t2\nn2 ( c16pobs np2set ) + 2 maxs 6=t \u221a 8 ns,t 3  . Put in another way, there exists a universal constant c6 > 0 such that\u2225\u2225\u2225ZL\u2225\u2225\u2225 =\n\u2225\u2225\u2225\u2225\u2225\u2225 \u2211 i 6=j Ai,j \u2212 EAi,j \u2225\u2225\u2225\u2225\u2225\u2225 < c6 \u221a npobs p2set log (mn) (80)\nholds with probability exceeding 1\u2212 1(mn)10 . This follows from Lemma 5. Additionally, observe that EY L,0ij = 0 and\u2225\u2225\u2225\u2225 1\u221apobsY L,0ij\n\u2225\u2225\u2225\u2225 \u2264 \u221an as long as pobs > 1n . Applying Lemma 3 suggests that\u2225\u2225\u2225Y L,0\u2225\u2225\u2225 < c0\u221anpobs log (mn) with probability at least 1\u2212 1(mn)5 . This combined with (80) yields\u2225\u2225\u2225Y L\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225Y L,0\u2225\u2225\u2225+ \u2225\u2225\u2225ZL\u2225\u2225\u2225 < c11 \u221a npobs log (mn)\np2set\nwith probability at least 1\u2212 3(mn)5 , where c11 is some universal constant. On the other hand, for each (s, t) entry of ZLil (i 6= l), it can only be affected by those observed blocks X falseij (or X false jl ) satisfying t \u2208 Sj (or s \u2208 Sj). Consequently, each entry of Z L il can be expressed as a sum of \u0398 (npsetpobs) zero-mean independent variables, each of them being bounded in magnitude by 1(mins 6=t ns,t) . From Hoeffding\u2019s inequality one can derive\nP (\u2225\u2225\u2225ZLil\u2225\u2225\u2225\u221e > t) \u2264 m2P \u2212 t2c7npsetpobs 1( min s 6=t ns,t )2  \u2264 m2P ( \u2212 t 2 c\u03037pobs 1 np3set )\nfor some constants c7, c\u03037 > 0, indicating that\u2225\u2225\u2225ZLil\u2225\u2225\u2225\u221e \u2264 \u221a c8pobs log (mn) np3set , \u2200i 6= l\nwith probability exceeding 1\u2212 1 (mn)10 .\nD.6 Proof of Lemma 8 By construction of Y true,1, one can see that all non-zero entries lie within the support \u2126gt. One important feature of Xgtij is that it can be converted, via row / column permutation, into a block diagonal matrix that consists of m all-one blocks, where the ith block is of size ni (1 \u2264 i \u2264 m). From Lemma 5, one has\nni \u2208 [ npset \u00b1 \u221a c8npset log (mn) ] , 1 \u2264 i \u2264 m\nwith high probability. Thus, Y true,1 can also be rearranged such that its non-zero entries form m disjoint diagonal blocks. We will quantify the eigenvalues of Y true,1 by bounding the spectrum of each of these matrix blocks.\nWe first decompose the matrix Y true,1 into two parts Y true,1 and Y\u0303 true,1 such that\n\u2200i 6= j, Y true,1ij =\n{ \u2212X inij , if X in ij is observed and not corrupted,\n0, else;\nand\n\u2200i 6= j, Y true,1ij =\n{ \u2212X inij + (1\u2212ptrue)pobs m , if X in ij is observed and corrupted,\n(1\u2212ptrue)pobs m , else.\nThat said, Y true,1 ij consists of all non-corrupted components, while Y\u0303 true,1\nconsists of all \u201cdebiased\u201d random outliers.\nBy Lemma 4, one can verify that for all unit vector v such that vv> \u2208 T\u22a5gt ,\u2329 vv>,Y true,1 \u232a \u2265 min\n1\u2264s\u2264m (nsptruepobs \u2212 c4\n\u221a nspobs log (mn))\n\u2265 1 2 npsetptruepobs \u2212 c5 \u221a npsetpobs log (mn) (81)\nfor some absolute constant c5 > 0, where the second inequality follows from the concentration result stated in Lemma 5.\nIn addition, each entry of Y\u0303 true,1 ij (i 6= j) lying in the support \u2126gt has mean zero and variance (1\u2212ptrue)pobs\nm\n( 1\u2212 (1\u2212ptrue)pobsm ) .\nLemma 3 then suggests that the norm of each non-zero block of Y\u0303 true,1\n(the ones with size ni) is bounded above by O (\u221a pobsni log (nm) ) . As a result,\u2225\u2225\u2225Y\u0303 true,1\u2225\u2225\u2225 \u2264 c15 max\n1\u2264s\u2264m\n\u221a pobsns log (nm) < c\u030315 \u221a npsetpobs log (nm) .\nThis taken collectively with (81) yields that\u2329 vv>,Y true,1 \u232a \u2265 1\n2 npsetptruepobs \u2212 (c5 + c\u030315)\n\u221a npsetpobs log (mn) . (82)\nOn the other hand, we know from the construction procedure and Lemma 2 that\n\u2016Rm\u2016\u221e \u2264\n\u221a c10pobs log (mn)\nnp3set + \u03bb \u2225\u2225\u2225d \u00b7 d> \u2212 1 \u00b7 1>\u2225\u2225\u2225 \u221e\n\u2264 c\u030310\n(\u221a pobs log (mn)\nnp3set +\n\u221a pobs log (mn)\npset\n\u221a log (mn)\nnpset\n)\n\u2264 2c\u030310 \u221a\npobs np3set log (mn)\nfor some constants c10, c\u030310 > 0. Since Rm \u2208 \u2126gt, we can also rearrange Rm into m diagonal blocks each of size ni (1 \u2264 i \u2264 m). Hence, a crude upper bound yields\u2225\u2225Y true,2\u2225\u2225 \u2264 \u2225\u2225Y true,2\u2225\u2225 1 \u2264 (\nmax 1\u2264i\u2264m ni\n)( 2c\u030310 \u221a pobs np3set log (mn) ) \u2264 c11npset \u221a npobs p3set log (mn)\n= c11n \u221a npobs pset log (mn)\nfor some universal constant c11 > 0.\nD.7 Proof of Lemma 9 Define an augmented matrix Hsup such that\nHsupij = \u03a0 > i Hij\u03a0j . (83)\nRecall that ni denotes the number of sets containing element i, and that\n\u03a3 :=  n n1 n n2\n. . . n nm  . The assumption that PT\u22a5gt (H) = 0 can be translated into(\nI \u2212 1 n\n(1n \u2297 Im) \u03a3 (1n \u2297 Im) ) Hsup ( I \u2212 1\nn (1n \u2297 Im) \u03a3 (1n \u2297 Im)\n) = 0.\nWe can easily compute that\nHsupii \u2212\u03a3H sup \u00b7i \u2212H sup i\u00b7 \u03a3 + \u03a3H sup \u00b7\u00b7 \u03a3 = 0, 1 \u2264 i \u2264 n,\nwhere  H sup \u00b7i := 1 n \u2211n j=1 H sup ji , H sup i\u00b7 := 1 n \u2211n j=1 H sup ij ,\nH sup\n\u00b7\u00b7 := 1 n2 \u2211n i=1 \u2211n j=1 H sup ij .\nThis combined with the identity Hii = 0 (and hence H sup ii = 0) yields\n\u03a3H sup \u00b7\u00b7 \u03a3 = \u03a3H sup \u00b7i + H sup i\u00b7 \u03a3, 1 \u2264 i \u2264 n.\nSumming over all i leads to\n\u03a3H sup\n\u00b7\u00b7 \u03a3 = \u03a3\n( 1\nn n\u2211 i=1 H sup \u00b7i\n) + ( 1\nn n\u2211 i=1 H sup i\u00b7\n) \u03a3 = \u03a3H sup\n\u00b7\u00b7 + H sup \u00b7\u00b7 \u03a3.\nExpanding it yields n2\nninj\n( H sup\n\u00b7\u00b7 ) i,j = ( n ni + n nj )( H sup \u00b7\u00b7 ) i,j , 1 \u2264 i, j \u2264 m.\nFrom our assumption that n 2\nninj 6= nni + n nj , we can derive\nH sup\n\u00b7\u00b7 = 0. (84)\nDue to the feasibility constraint, all diagonal entries of Hsupij are non-positive, and all off-diagonal entries of Hsupij are non-negative. These conditions together with (84) establish that H = 0."}], "references": [{"title": "A probabilistic image jigsaw puzzle solver", "author": ["T.S. Cho", "S. Avidan", "W.T. Freeman"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010, pp. 183\u2013190.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "A global approach to automatic solution of jigsaw puzzles", "author": ["D. Goldberg", "C. Malon", "M. Bern"], "venue": "Comput. Geom. Theory Appl., vol. 28, pp. 165\u2013174, June 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Disambiguating visual relations using loop constraints", "author": ["C. Zach", "M. Klopschitz", "M. Pollefeys"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010, pp. 1426\u20131433.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Discrete-continuous optimization for largescale structure from motion", "author": ["D. Crandall", "A. Owens", "N. Snavely", "D. Huttenlocher"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011, pp. 3001\u20133008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Reassembling fractured objects by geometric matching", "author": ["Q.-X. Huang", "S. Fl\u00f6ry", "N. Gelfand", "M. Hofer", "H. Pottmann"], "venue": "ACM Transactions on Graphics (TOG), vol. 25, no. 3. ACM, 2006, pp. 569\u2013578.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Globally consistent reconstruction of ripped-up documents", "author": ["L. Zhu", "Z. Zhou", "D. Hu"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 1, pp. 1\u201313, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Mitochondrial DNA as a genomic jigsaw puzzle", "author": ["W. Marande", "G. Burger"], "venue": "Science, vol. 318, no. 5849, pp. 415\u2013415, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Structure from motion for scenes with large duplicate structures", "author": ["R. Roberts", "S.N. Sinha", "R. Szeliski", "D. Steedly"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011, pp. 3137\u20133144.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "An optimization approach to improving collections of shape maps", "author": ["A. Nguyen", "M. Ben-Chen", "K. Welnicka", "Y. Ye", "L. Guibas"], "venue": "Computer Graphics Forum, vol. 30, no. 5. Wiley Online Library, 2011, pp. 1481\u20131491.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "An optimization approach for extracting and encoding consistent maps in a shape collection", "author": ["Q. Huang", "G. Zhang", "L. Gao", "S. Hu", "A. Butscher", "L. Guibas"], "venue": "ACM Transactions on Graphics, vol. 31, no. 6, p. 167, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploring collections of 3d models using fuzzy correspondences", "author": ["V. Kim", "W. Li", "N. Mitra", "S. DiVerdi", "T. Funkhouser"], "venue": "ACM SIGGRAPH, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Consistent shape maps via semidefinite programming", "author": ["Q. Huang", "L. Guibas"], "venue": "Computer Graphics Forum, vol. 32, no. 5, pp. 177\u2013186, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic subgraph matching based on convex relaxation", "author": ["C. Schellewald", "C. Schn\u00f6rr"], "venue": "Energy minimization methods in computer vision and pattern recognition. Springer, 2005, pp. 171\u2013186.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Balanced graph matching", "author": ["T. Cour", "P. Srinivasan", "J. Shi"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning graph matching", "author": ["T.S. Caetano", "J.J. McAuley", "L. Cheng", "Q.V. Le", "A.J. Smola"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 6, pp. 1048\u20131058, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Solving the multi-way matching problem by permutation synchronization.", "author": ["D. Pachauri", "R. Kondor", "V. Singh"], "venue": "Advanced in Neural Information Processing Systems (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Exact and stable recovery of rotations for robust synchronization", "author": ["L. Wang", "A. Singer"], "venue": "arxiv:1211.2441, 2013.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Global registration of multiple point clouds using semidefinite programming", "author": ["K. Chaudhury", "Y. Khoo", "A. Singer", "D. Cowburn"], "venue": "arXiv:1306.5226, 2013. 34", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Candes", "B. Recht"], "venue": "Foundations of Computational Mathematics, vol. 9, no. 6, pp. 717\u2013772, April 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Matrix completion from a few entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "IEEE Transactions on Information Theory, vol. 56, no. 6, pp. 2980\u20132998, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust principal component analysis?", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of ACM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "P. Parrilo", "A.S. Willsky"], "venue": "SIAM Journal on Optimization, vol. 21, no. 2, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust pca via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "Advances on Neural Information Processing Systems (NIPS), 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Dense error correction for low-rank matrices via principal component pursuit", "author": ["A. Ganesh", "J. Wright", "X. Li", "E.J. Candes", "Y. Ma"], "venue": "IEEE International Symposium on Information Theory Proceedings (ISIT), 2010, pp. 1513\u20131517.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Low-rank matrix recovery from errors and erasures", "author": ["Y. Chen", "A. Jalali", "S. Sanghavi", "C. Caramanis"], "venue": "IEEE Transactions on Information Theory, vol. 59, no. 7, pp. 4324\u20134337, July 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Jointly clustering rows and columns of binary matrices: Algorithms and trade-offs", "author": ["J. Xu", "R. Wu", "K. Zhu", "B. Hajek", "R. Srikant", "L. Ying"], "venue": "arxiv:1310.0512, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Correlation clustering", "author": ["N. Bansal", "A. Blum", "S. Chawla"], "venue": "Machine Learning, vol. 56, no. 1-3, pp. 89\u2013113, 2004.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Correlation clustering with noisy input", "author": ["C. Mathieu", "W. Schudy"], "venue": "ACM-SIAM SODA, 2010, pp. 712\u2013728.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Clustering partially observed graphs via convex optimization", "author": ["A. Jalali", "Y. Chen", "S. Sanghavi", "H. Xu"], "venue": "International Conf. on Machine Learning (ICML), 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Clustering sparse graphs", "author": ["Y. Chen", "S. Sanghavi", "H. Xu"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Clustering using max-norm constrained optimization", "author": ["A. Jalali", "N. Srebro"], "venue": "International Conference on Machine Learning (ICML), June 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Breaking the small cluster barrier of graph clustering", "author": ["N. Ailon", "Y. Chen", "X. Huan"], "venue": "International Conference on Machine Learning (2013), 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix completion from noisy entries", "author": ["R.H. Keshavan", "A. Montanari", "S. Oh"], "venue": "Journal of Machine Learning Research, vol. 99, pp. 2057\u20132078, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Phase-only matched filtering", "author": ["J.L. Horner", "P.D. Gianino"], "venue": "Applied optics, vol. 23, no. 6, pp. 812\u2013816, 1984.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1984}, {"title": "Angular synchronization by eigenvectors and semidefinite programming", "author": ["A. Singer"], "venue": "Applied and computational harmonic analysis, vol. 30, no. 1, pp. 20\u201336, 2011.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Multireference alignment using semidefinite programming", "author": ["A.S. Bandeira", "M. Charikar", "A. Singer", "A. Zhu"], "venue": "Conference on Innovations in Theoretical Computer Science, 2014, pp. 459\u2013470.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Alternating direction augmented lagrangian methods for semidefinite programming", "author": ["Z. Wen", "D. Goldfarb", "W. Yin"], "venue": "Mathematical Programming Computation, vol. 2, no. 3-4, pp. 203\u2013230, 2010.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Random graph dynamics", "author": ["R. Durrett"], "venue": "Cambridge university press,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "A performance evaluation of local descriptors", "author": ["K. Mikolajczyk", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 27, no. 10, pp. 1615\u20131630, 2005. 35", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "A spectral technique for correspondence problems using pairwise constraints", "author": ["M. Leordeanu", "M. Hebert"], "venue": "IEEE International Conference on Computer Vision (ICCV), vol. 2, 2005, pp. 1482\u20131489.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International journal of computer vision, vol. 60, no. 2, pp. 91\u2013110, 2004.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}, {"title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography", "author": ["M.A. Fischler", "R.C. Bolles"], "venue": "Commun. ACM, vol. 24, no. 6, pp. 381\u2013395, Jun. 1981.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1981}, {"title": "Non-rigid dense correspondence with applications for image enhancement", "author": ["Y. HaCohen", "E. Shechtman", "D. Goldman", "D. Lischinski"], "venue": "ACM Trans. Graph., vol. 30, no. 4, pp. 70:1\u201370:10, Jul. 2011.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2011}, {"title": "Dense correspondence finding for parametrization-free animation reconstruction from video.", "author": ["N. Ahmed", "C. Theobalt", "C. Rossl", "S. Thrun", "H. Seidel"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "Unsupervised learning for graph matching", "author": ["M. Leordeanu", "R. Sukthankar", "M. Hebert"], "venue": "International journal of computer vision, vol. 96, no. 1, pp. 28\u201345, 2012.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "The probabilistic method (3rd Edition)", "author": ["N. Alon", "J.H. Spencer"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2008}, {"title": "Topics in random matrix theory", "author": ["T. Tao"], "venue": "AMS Bookstore,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["J.A. Tropp"], "venue": "Foundations of Computational Mathematics, vol. 12, no. 4, pp. 389\u2013434, 2012. 36", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "A partial list includes jigsaw puzzle solving [1, 2], structure from motion [3, 4], re-assembly of fragmented objects and documents [5, 6], and DNA/RNA shotgun assembly sequencing [7].", "startOffset": 46, "endOffset": 52}, {"referenceID": 1, "context": "A partial list includes jigsaw puzzle solving [1, 2], structure from motion [3, 4], re-assembly of fragmented objects and documents [5, 6], and DNA/RNA shotgun assembly sequencing [7].", "startOffset": 46, "endOffset": 52}, {"referenceID": 2, "context": "A partial list includes jigsaw puzzle solving [1, 2], structure from motion [3, 4], re-assembly of fragmented objects and documents [5, 6], and DNA/RNA shotgun assembly sequencing [7].", "startOffset": 76, "endOffset": 82}, {"referenceID": 3, "context": "A partial list includes jigsaw puzzle solving [1, 2], structure from motion [3, 4], re-assembly of fragmented objects and documents [5, 6], and DNA/RNA shotgun assembly sequencing [7].", "startOffset": 76, "endOffset": 82}, {"referenceID": 4, "context": "A partial list includes jigsaw puzzle solving [1, 2], structure from motion [3, 4], re-assembly of fragmented objects and documents [5, 6], and DNA/RNA shotgun assembly sequencing [7].", "startOffset": 132, "endOffset": 138}, {"referenceID": 5, "context": "A partial list includes jigsaw puzzle solving [1, 2], structure from motion [3, 4], re-assembly of fragmented objects and documents [5, 6], and DNA/RNA shotgun assembly sequencing [7].", "startOffset": 132, "endOffset": 138}, {"referenceID": 6, "context": "A partial list includes jigsaw puzzle solving [1, 2], structure from motion [3, 4], re-assembly of fragmented objects and documents [5, 6], and DNA/RNA shotgun assembly sequencing [7].", "startOffset": 180, "endOffset": 183}, {"referenceID": 2, "context": "Such criterion has recently been invoked in many algorithms [3,8\u201311] to detect outliers among the pairwise input maps.", "startOffset": 60, "endOffset": 68}, {"referenceID": 7, "context": "Such criterion has recently been invoked in many algorithms [3,8\u201311] to detect outliers among the pairwise input maps.", "startOffset": 60, "endOffset": 68}, {"referenceID": 8, "context": "Such criterion has recently been invoked in many algorithms [3,8\u201311] to detect outliers among the pairwise input maps.", "startOffset": 60, "endOffset": 68}, {"referenceID": 9, "context": "Such criterion has recently been invoked in many algorithms [3,8\u201311] to detect outliers among the pairwise input maps.", "startOffset": 60, "endOffset": 68}, {"referenceID": 10, "context": "Such criterion has recently been invoked in many algorithms [3,8\u201311] to detect outliers among the pairwise input maps.", "startOffset": 60, "endOffset": 68}, {"referenceID": 11, "context": "Recent work by [12] provided the first theoretical guarantee for robust and consistent joint matching.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "[12]) did not provide theoretical support when more than 50% of the input matches are corrupted.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "While [12] considered incomplete inputs, the tradeoff between the undersampling factor and the error-correction ability remains unknown.", "startOffset": 6, "endOffset": 10}, {"referenceID": 12, "context": "[13\u201315]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 13, "context": "[13\u201315]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 14, "context": "[13\u201315]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "[9\u201312], which exploited the global cycle-consistency criterion to prune noisy maps.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "[9\u201312], which exploited the global cycle-consistency criterion to prune noisy maps.", "startOffset": 0, "endOffset": 6}, {"referenceID": 10, "context": "[9\u201312], which exploited the global cycle-consistency criterion to prune noisy maps.", "startOffset": 0, "endOffset": 6}, {"referenceID": 11, "context": "[9\u201312], which exploited the global cycle-consistency criterion to prune noisy maps.", "startOffset": 0, "endOffset": 6}, {"referenceID": 11, "context": "The fundamental understanding has recently been advanced by [12].", "startOffset": 60, "endOffset": 64}, {"referenceID": 15, "context": "Recent work [16] employed spectral methods for denoising in the full-similarity case.", "startOffset": 12, "endOffset": 16}, {"referenceID": 16, "context": "Another line of work [17, 18] proposed to recover global rigid transform between points via convex relaxation, where the point coordinates might only be partially observed.", "startOffset": 21, "endOffset": 29}, {"referenceID": 17, "context": "Another line of work [17, 18] proposed to recover global rigid transform between points via convex relaxation, where the point coordinates might only be partially observed.", "startOffset": 21, "endOffset": 29}, {"referenceID": 18, "context": "In a broader sense, our approach is inspired by the pioneering work in low-rank matrix completion [19,20] and robust principal component analysis [21\u201325], which reveal the power of convex relaxation in recovering low-dimensional structures among high-dimensional objects.", "startOffset": 98, "endOffset": 105}, {"referenceID": 19, "context": "In a broader sense, our approach is inspired by the pioneering work in low-rank matrix completion [19,20] and robust principal component analysis [21\u201325], which reveal the power of convex relaxation in recovering low-dimensional structures among high-dimensional objects.", "startOffset": 98, "endOffset": 105}, {"referenceID": 20, "context": "In a broader sense, our approach is inspired by the pioneering work in low-rank matrix completion [19,20] and robust principal component analysis [21\u201325], which reveal the power of convex relaxation in recovering low-dimensional structures among high-dimensional objects.", "startOffset": 146, "endOffset": 153}, {"referenceID": 21, "context": "In a broader sense, our approach is inspired by the pioneering work in low-rank matrix completion [19,20] and robust principal component analysis [21\u201325], which reveal the power of convex relaxation in recovering low-dimensional structures among high-dimensional objects.", "startOffset": 146, "endOffset": 153}, {"referenceID": 22, "context": "In a broader sense, our approach is inspired by the pioneering work in low-rank matrix completion [19,20] and robust principal component analysis [21\u201325], which reveal the power of convex relaxation in recovering low-dimensional structures among high-dimensional objects.", "startOffset": 146, "endOffset": 153}, {"referenceID": 23, "context": "In a broader sense, our approach is inspired by the pioneering work in low-rank matrix completion [19,20] and robust principal component analysis [21\u201325], which reveal the power of convex relaxation in recovering low-dimensional structures among high-dimensional objects.", "startOffset": 146, "endOffset": 153}, {"referenceID": 24, "context": "In a broader sense, our approach is inspired by the pioneering work in low-rank matrix completion [19,20] and robust principal component analysis [21\u201325], which reveal the power of convex relaxation in recovering low-dimensional structures among high-dimensional objects.", "startOffset": 146, "endOffset": 153}, {"referenceID": 25, "context": "In fact, the ground truth herein is equivalent to a block-constant low-rank matrix [26], as occurred in various graph-related problems.", "startOffset": 83, "endOffset": 87}, {"referenceID": 26, "context": "In this regard, any GC algorithm [27\u201332] provides a heuristic to estimate graph matching.", "startOffset": 33, "endOffset": 40}, {"referenceID": 27, "context": "In this regard, any GC algorithm [27\u201332] provides a heuristic to estimate graph matching.", "startOffset": 33, "endOffset": 40}, {"referenceID": 28, "context": "In this regard, any GC algorithm [27\u201332] provides a heuristic to estimate graph matching.", "startOffset": 33, "endOffset": 40}, {"referenceID": 29, "context": "In this regard, any GC algorithm [27\u201332] provides a heuristic to estimate graph matching.", "startOffset": 33, "endOffset": 40}, {"referenceID": 30, "context": "In this regard, any GC algorithm [27\u201332] provides a heuristic to estimate graph matching.", "startOffset": 33, "endOffset": 40}, {"referenceID": 31, "context": "In this regard, any GC algorithm [27\u201332] provides a heuristic to estimate graph matching.", "startOffset": 33, "endOffset": 40}, {"referenceID": 11, "context": ", [12, 30]), we use matrices to encode maps between objects.", "startOffset": 2, "endOffset": 10}, {"referenceID": 29, "context": ", [12, 30]), we use matrices to encode maps between objects.", "startOffset": 2, "endOffset": 10}, {"referenceID": 32, "context": "[33]), a block-sparse matrix X in must first be trimmed in order to remove the undesired bias effect caused by over-represented rows / columns.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "This algorithm, all at once, attempts to disentangle the ground truth and outliers as well as predict unobserved maps via convex relaxation, inspired by recent success in sparse and low-rank matrix decomposition [21, 22].", "startOffset": 212, "endOffset": 220}, {"referenceID": 21, "context": "This algorithm, all at once, attempts to disentangle the ground truth and outliers as well as predict unobserved maps via convex relaxation, inspired by recent success in sparse and low-rank matrix decomposition [21, 22].", "startOffset": 212, "endOffset": 220}, {"referenceID": 33, "context": "For instance, our setting subsumes phase correlation [34], angular synchronization [35], and multi-signal alignment [36] as special cases.", "startOffset": 53, "endOffset": 57}, {"referenceID": 34, "context": "For instance, our setting subsumes phase correlation [34], angular synchronization [35], and multi-signal alignment [36] as special cases.", "startOffset": 83, "endOffset": 87}, {"referenceID": 35, "context": "For instance, our setting subsumes phase correlation [34], angular synchronization [35], and multi-signal alignment [36] as special cases.", "startOffset": 116, "endOffset": 120}, {"referenceID": 36, "context": "For practical applicability, we propose a first-order optimization algorithm for approximately solving MatchLift, which is a variant of the ADMM method for semidefinite programs presented in [37].", "startOffset": 191, "endOffset": 195}, {"referenceID": 23, "context": "The only comparative performance is reported for the robust PCA setting, where semidefinite relaxation enables dense error correction [24,25].", "startOffset": 134, "endOffset": 141}, {"referenceID": 24, "context": "The only comparative performance is reported for the robust PCA setting, where semidefinite relaxation enables dense error correction [24,25].", "startOffset": 134, "endOffset": 141}, {"referenceID": 37, "context": "Recall that the connectivity threshold for an Erd\u0151s\u2013Renyi graph G(n, pobs) is pobs > logn n (see [38]).", "startOffset": 97, "endOffset": 101}, {"referenceID": 16, "context": "\u2022 Semidefinite Programming: The SDP formulation proposed by Wang and Singer [17] admits exact recovery in the full-similarity setting when ptrue > c1 for some absolute constant c1 \u2248 50% in the asymptotic regime.", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "One might also attempt recovery by minimizing a weighted sum of nuclear norm and `1 norm as suggested in matrix completion [19] and robust PCA [21,22].", "startOffset": 123, "endOffset": 127}, {"referenceID": 20, "context": "One might also attempt recovery by minimizing a weighted sum of nuclear norm and `1 norm as suggested in matrix completion [19] and robust PCA [21,22].", "startOffset": 143, "endOffset": 150}, {"referenceID": 21, "context": "One might also attempt recovery by minimizing a weighted sum of nuclear norm and `1 norm as suggested in matrix completion [19] and robust PCA [21,22].", "startOffset": 143, "endOffset": 150}, {"referenceID": 23, "context": "In order to enable dense error correction, robust PCA requires the sparse components (which is X in \u2212 X here with X denoting the ground truth) to exhibit random signs [24, 25].", "startOffset": 167, "endOffset": 175}, {"referenceID": 24, "context": "In order to enable dense error correction, robust PCA requires the sparse components (which is X in \u2212 X here with X denoting the ground truth) to exhibit random signs [24, 25].", "startOffset": 167, "endOffset": 175}, {"referenceID": 27, "context": "\u2022 Graph Clustering: Various approaches for general graph clustering have been proposed with theoretical guarantees under different randomized settings [28,29,31].", "startOffset": 151, "endOffset": 161}, {"referenceID": 28, "context": "\u2022 Graph Clustering: Various approaches for general graph clustering have been proposed with theoretical guarantees under different randomized settings [28,29,31].", "startOffset": 151, "endOffset": 161}, {"referenceID": 30, "context": "\u2022 Graph Clustering: Various approaches for general graph clustering have been proposed with theoretical guarantees under different randomized settings [28,29,31].", "startOffset": 151, "endOffset": 161}, {"referenceID": 30, "context": "To facilitate comparison, we evaluate the most recent deterministic guarantees obtained by [31].", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "In this section, we evaluate the performance of MatchLift and compare it against [29] and other graph matching methods.", "startOffset": 81, "endOffset": 85}, {"referenceID": 28, "context": "Figure 1: Phase Transition Diagrams of the proposed approach (MatchLift) and [29].", "startOffset": 77, "endOffset": 81}, {"referenceID": 28, "context": "We can see that MatchLift can recover the ground-truth maps even the majority of the input correspondences are wrong, while the exact recovery of [29] requires that the percentage of incorrect correspondences is less than 50%.", "startOffset": 146, "endOffset": 150}, {"referenceID": 28, "context": "In comparison, Figure 1(b) and Figure 1(d) illustrate the phase transition diagrams achieved by the algorithm proposed in [29].", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "One can see that MatchLift is empirically superior, as [29] is unable to allow dense error correction in our case.", "startOffset": 55, "endOffset": 59}, {"referenceID": 38, "context": ", CMU-House, CMU-Hotel, two datasets (Graf and Bikes) from [39]1 and two new datasets (referred as Chair and Building, respectively) designed for evaluating joint partial object matching.", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "As shown in Figures 2 and 3, the Building data set contains 16 images taken around a building [4], while the Chair data set contains 16 images of a chair model from different viewpoints.", "startOffset": 94, "endOffset": 97}, {"referenceID": 14, "context": "To make fair comparisons with previous techniques on CMUHouse and CMU-Hotel, we use the features points provided in [15] and apply the spectral matching 1available online: robots.", "startOffset": 116, "endOffset": 120}, {"referenceID": 44, "context": "We compare the proposed MatchLift algorithm with Robust PCA (RPCA) and two learning based graph matching methods: LearnI [45] and LearnII [15].", "startOffset": 121, "endOffset": 125}, {"referenceID": 14, "context": "We compare the proposed MatchLift algorithm with Robust PCA (RPCA) and two learning based graph matching methods: LearnI [45] and LearnII [15].", "startOffset": 138, "endOffset": 142}, {"referenceID": 39, "context": "algorithm described in [40] to establish initial maps between features points.", "startOffset": 23, "endOffset": 27}, {"referenceID": 40, "context": "We first detect dense SIFT feature points [41] on each image.", "startOffset": 42, "endOffset": 46}, {"referenceID": 41, "context": "We then apply RANSAC [42] to obtain correspondences between each pair of images.", "startOffset": 21, "endOffset": 25}, {"referenceID": 42, "context": "On Chair, Building, Graf and Bikes, we apply the metric described in [43], which evaluates the deviations of manual feature correspondences.", "startOffset": 69, "endOffset": 73}, {"referenceID": 43, "context": "As the feature points computed on each image do not necessarily align with the manual features, we apply [44] to interpolate feature level correspondences into pixel-wise correspondences for evaluation.", "startOffset": 105, "endOffset": 109}, {"referenceID": 28, "context": "In contrast, the method of [29] can only recover 92.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "Note that, MatchLift also outperforms state-of-the-art learning based graph matching algorithms [15,45].", "startOffset": 96, "endOffset": 103}, {"referenceID": 44, "context": "Note that, MatchLift also outperforms state-of-the-art learning based graph matching algorithms [15,45].", "startOffset": 96, "endOffset": 103}, {"referenceID": 28, "context": "Moreover, MatchLift significantly outperforms [29], as the fault-tolerance rate of [29] is limited by a small constant barrier.", "startOffset": 46, "endOffset": 50}, {"referenceID": 28, "context": "Moreover, MatchLift significantly outperforms [29], as the fault-tolerance rate of [29] is limited by a small constant barrier.", "startOffset": 83, "endOffset": 87}, {"referenceID": 45, "context": "Interested readers are referred to [47] for a tutorial.", "startOffset": 35, "endOffset": 39}], "year": 2014, "abstractText": "Joint matching over a collection of objects aims at aggregating information from a large collection of similar instances (e.g. images, graphs, shapes) to improve maps between pairs of them. Given multiple objects and matches computed between a few object pairs in isolation, the goal is to recover an entire collection of maps that are (1) globally consistent, and (2) close to the provided maps \u2014 and under certain conditions provably the ground-truth maps. Despite recent advances on this problem, the best-known recovery guarantees are limited to a small constant barrier \u2014 none of the existing methods find theoretical support when more than 50% of input correspondences are corrupted. Moreover, prior approaches focus mostly on fully similar objects, while it is practically more demanding to match instances that are only partially similar to each other (e.g., different views of a single physical object). In this paper, we propose an algorithm to jointly match multiple objects that exhibit only partial similarities, given a few (possibly highly incomplete) pairwise matches that are densely corrupted. By encoding a consistent partial map collection into a 0-1 semidefinite matrix, we propose to recover the ground-truth maps via a parameter-free convex program called MatchLift, following a spectral method that pre-estimates the total number of distinct elements to be matched. Numerically, this program can be efficiently solved via alternating direction methods of multipliers (ADMM) along with a greedy rounding strategy. Theoretically, MatchLift exhibits near-optimal error-correction ability, i.e. in the asymptotic regime it is guaranteed to work even when a dominant fraction 1 \u2212 \u0398 ( log n \u221a n ) of the input maps behave like random outliers. Furthermore, MatchLift succeeds with minimal input complexity, namely, perfect matching can be achieved as soon as the provided maps form a connected map graph. We evaluate the proposed algorithm on various benchmark data sets including synthetic examples and real-world examples, all of which confirm the practical applicability and usefulness of MatchLift.", "creator": "LaTeX with hyperref package"}}}