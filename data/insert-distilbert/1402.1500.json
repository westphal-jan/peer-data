{"id": "1402.1500", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2014", "title": "Co-clustering of Fuzzy Lagged Data", "abstract": "the next paper focuses on mining patterns that are characterized randomly by a fuzzy lagged relationship between the data objects forming them. such a regulatory mechanism is quite common in real life settings. it appears in a variety of fields : finance, gene expression, neuroscience, crowds and collective movements are but a limited list of examples. mining such patterns not only helps in understanding the relationship between different objects in the ecological domain, but assists in forecasting their future behavior. for most interesting variants of determining this problem, the finding an uneven optimal uniformly fuzzy lagged co - cluster is an np - complete problem. we thus present a functional polynomial - time monte - carlo approximation algorithm for mining fuzzy matching lagged co - clustered clusters. we prove that for any data related matrix, generally the algorithm mines a fuzzy lagged co - cluster with fixed probability, which encompasses the average optimal fuzzy : lagged correlated co - cluster by a dynamic maximum 2 ratio columns column overhead error and completely no rows overhead. moreover, the algorithm handles marginal noise, anti - correlations, missing values and zero overlapping patterns. the algorithm was extensively evaluated using both multiple artificial and real datasets. the results not surely only corroborate the ability of the algorithm overall to efficiently mine relevant and accurate fuzzy lagged co - clusters, but also illustrate the importance of including the fuzziness phenomenon in the lagged - pattern model.", "histories": [["v1", "Thu, 6 Feb 2014 21:02:16 GMT  (3947kb,D)", "https://arxiv.org/abs/1402.1500v1", "Under consideration for publication in Knowledge and Information Systems"], ["v2", "Thu, 15 May 2014 12:01:08 GMT  (4519kb,D)", "http://arxiv.org/abs/1402.1500v2", "Under consideration for publication in Knowledge and Information Systems. The final publication is available at Springer viathis http URL"]], "COMMENTS": "Under consideration for publication in Knowledge and Information Systems", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eran shaham", "david sarne", "boaz ben-moshe"], "accepted": false, "id": "1402.1500"}, "pdf": {"name": "1402.1500.pdf", "metadata": {"source": "CRF", "title": "Co-clustering of Fuzzy Lagged Data", "authors": ["Eran Shaham", "David Sarne", "Boaz Ben-Moshe"], "emails": ["erans@macs.biu.ac.il,", "sarned@macs.biu.ac.il,", "benmo@ariel.ac.il"], "sections": [{"heading": null, "text": "Keywords: fuzzy lagged data clustering; spatio-temporal patterns; time-lagged; biclustering; data mining"}, {"heading": "1. Introduction", "text": "A by-product of modern life is the ever growing trace of digital data; these might be pictures uploaded to the web, cellular trajectories collected by mobile providers, or the earth\u2019s climate monitored by buoys, balloons and satellites.\nReceived May 28, 2012 Revised Mar 25, 2014 Accepted May 03, 2014 The final publication is available at Springer via http://dx.doi.org/10.1007/s10115-014-0758-7\nar X\niv :1\n40 2.\n15 00\nv2 [\ncs .A\nThe feature common to such data is its temporal nature. Mining these data can facilitate uncovering the hidden regulatory mechanisms governing the data objects.\nEarly mining techniques used the key concept of clustering to look for patterns formed by a subset of the objects over all attributes, or vice versa [12, 36]. Following seminal work by Cheng and Church [17] in the area of gene expression using microarray technology, substantial focus has been placed in recent years on co-clustering [40, 52]. Co-clustering extends clustering by aiming to identify a subset of objects that exhibit similar behavior across a subset of attributes, or vice versa. Very few co-clustering studies have considered the problem of mining patterns that have a lagged correlation between a subset of the objects over a subset of the attributes [70, 79, 85]. For example, consider the problem of identifying a flock of pigeons from among a large collection of flight trajectories (that is, mining a coordinated movement of a subset of objects across a subset of time attributes) [56]. The flock\u2019s spatial coordinated flight, where each member follows the leader with some lag (delay), is a lagged pattern comprising the flock members\u2019 tempo-spatial locations (trajectories). The underlying assumption in these works is that the lagged correlation, if it exists, is fixed (i.e., with no noise whatsoever).\nIn real-life settings, however, lagged patterns are typically noisy. For example, consider the flock\u2019s coordinated flight described above. Overall the flock maintains a general lagged flight formation (each member follows the leader with some lag). Yet, a closer look will reveal that each member deviates from that lag to some extent (due to wind changes, threats, physical strength, etc). The flock\u2019s flight pattern can, however, be captured by a co-cluster comprising fuzzy lags. Fig. 1 presents such real-life flight trajectories, where each line represents a pigeon\u2019s trajectory (pigeons belonging to the same flock are denoted by the same color). The presence of interleaving trajectories presents a serious challenge to mining algorithms (e.g., density-based algorithms [20]), as well as to humans (see Subsection 4.2). We denote a lagged co-cluster which includes a fuzzy correlation between a subset of objects over a subset of lagged attributes as a fuzzy lagged co-cluster. The problem, as later proved, is NP-complete for most interesting cases.\nSimilar fuzzy lagged behavior can be observed during the mining of a group of people that coordinate their movements within a crowd (e.g., a group of terrorists trying to move from point A to point B). The group would maintain a general lagged formation where each member follows the leader with some lag. However,\ndue to obstacles, temporary loss of eye contact, and other difficulties, the group\u2019s members would probably be compelled to deviate from that fixed lag. Additional motivation for studying fuzzy lagged co-clusters comes from the field of medicine within the context of disease relationships and causality. Given a dataset where an object is a disease, an attribute is an age, and an entry of the matrix is the number of occurrences of a disease in an age (the number of occurrences can be obtained from medical articles, hospital records, etc), the causality of diseases would be captured by a lagged co-cluster. However, the lag is expected to be of a fuzzy nature due to change in medical treatment, difference in disease development, inaccuracy of the dataset, etc. Mining such patterns can assist not only in the early detection of diseases, but also in providing better preventive treatment.\nFig. 2 presents an example of a fuzzy lagged dataset and various clusters within it. Fig. 2a depicts an example of a matrix dataset (for simplicity, certain cells have been left blank). Fig. 2b represents the same matrix after row permutation. Three clusters emerge, as follows. Fig. 2c (middle part of matrix 2b): a co-cluster with neither lag nor fuzziness. The value of a cluster entry, Ai,j , may deviates from being expressed as the sum of the column profile, Ri (={3, 1, 2, 1} for row i={1, 4, 6, 11}, respectively), and the row profile, Cj (={2, 1, 2} for column j={2, 5, 8}, respectively), by a maximum allowed error \u03b5 \u2264 0.5. That\nis: |(Ri + Cj) \u2212 Ai,j | \u2264 \u03b5 = 0.5.1 Intuitively, the column profile indicates the regulation strength of the object, while the row profile indicates the regulatory intensity of the attribute. For example, the matrix entry of row r4 and column c8 is 3.2, which deviates from the expected value of: Ri+Cj=R4+C8=1+2=3, by an error of 0.2.\nFig. 2d (upper part of matrix 2b) exemplifies a lagged co-cluster, with no fuzziness. Here, the value of a cluster entry, Ai,j , may deviate from being expressed as Ri+Cj+Ti by a maximum error of 0.5. That is: |(Ri+Cj+Ti)\u2212Ai,j | \u2264 \u03b5 = 0.5. For example, the matrix entry of row r7 and column c3 is 8.4, which deviate from the expected value of: (T7=\u20131) Ri+Cj+Ti=R7+C3+T7=R7+C3\u22121=R7+C2=6+2=8, by an error of 0.4. Fig. 2e (lower part of matrix 2b) exemplifies a fuzzy lagged co-cluster. Here, the value of a cluster entry, Ai,j , not only may vertically deviate from being expressed as Ri + Cj+Ti by a maximum error of 0.5, but also may horizontally deviate from Cj+Ti by a maximum fuzziness, F , of two. That is: |(Ri + Cj+Ti+fi,j ) \u2212 Ai,j | \u2264 \u03b5 = 0.5, for some maxi,j{fi,j} \u2264 2. For example, the matrix entry of row r3 has a zero lag (T3=0) and zero fuzziness over the columns c3, c6 and c9, i.e., f3,j = {0, 0, 0} (to ease readability, Fig. 2 does not present the fuzziness values). Relative to r3, object r8 has a lag of T8=2 and fuzziness of f8,j = {0, 1, 0} (relative to r3 columns); object r10 has a lag of T10=1 and fuzziness of f10,j = {1, 1, 0}, and object r12 has a lag of T12=\u20131 and fuzziness of f12,j = {\u22121, 0, 2}. For example, the matrix entry of row r12 and column c8 is 7.3, which deviate from the expected value of: (T12=\u22121, f12,c8=2) Ri+Cj+Ti+fi,j=R12+C8+T12+f12,8=R12+C8\u22121+2=R12+C9=2+5=7, by an error of 0.3.\nThe main contribution of the paper is in introducing a polynomial time approximation algorithm for mining fuzzy lagged co-clusters, hereafter denoted as the FLC algorithm. To the best of our knowledge, this is the first attempt to develop such an algorithm. The input of the FLC algorithm is a real number matrix (where rows represent objects and columns represent attributes), a maximum error value and a maximum fuzziness degree. The algorithm uses a Monte-Carlo strategy to guarantee, with fixed probability, the mining of a fuzzy lagged co-cluster which encompasses the optimal fuzzy lagged co-cluster by a maximum 2 ratio columns overhead and completely no rows overhead. This guarantee holds for any monotonically increasing objective function defined over the cluster dimensions. Many of the inherent shortcomings common to nonfuzzy, non-lagged data [12, 75] are handled by the FLC algorithm, including noise (due to human or machine inaccuracies); missing values (e.g., equipment malfunction); anti-correlations (down-regulation, to adopt gene expression terminology) and overlapping patterns. The algorithm and its properties were extensively evaluated using both artificial and real-life datasets. The results not only corroborate the algorithm\u2019s ability to efficiently mine relevant and accurate fuzzy lagged co-clusters, but also illustrate the importance of including fuzziness in the lagged-pattern model. With this inclusion, a significant improvement is achieved in both coverage and F1 measures in comparison to using the regular (non-fuzzy) lagged co-clustering model. Moreover, the FLC algorithm presented\n1 Throughout the example we use the notations of Ri and Cj of the additive model which are an alternative representation to the notations of Gi and Hj of the multiplicative model. See more details in the formal model representation that follows, and in particular the definitions in Equations 1\u20132.\nclassification capabilities which were superior to the ones presented by both the non-fuzzy lagged model and those of human subjects.\nThe remainder of the paper is organized as follows. Section 2 formally introduces the model and shows that most interesting variants of the problem are NP-complete. In Section 3 we present the algorithm followed by a run-time analysis, proof of the probabilistic guarantee to efficiently mine relevant fuzzy lagged co-clusters and extensions to the algorithm. Section 4 presents the experiments that were conducted and their results. In Section 5 we review related work. We conclude with a discussion and suggested directions for future research in Section 6."}, {"heading": "2. Model", "text": "A lagged co-cluster of a real number matrix is a tuple (I, T, J), representing a submatrix determined by a subset of the columns J over a subset of the rows I with their corresponding lags T (|T |=|I|) [79] (see example in Fig. 2d). The fuzzy lagged co-clustering model augments the lagged co-cluster definition, enabling fuzziness in the lagged pattern.\nDefinition 1. A fuzzy lagged co-cluster of an m \u00d7 n real number matrix X is a tuple (I, T, J, F ), where J is a subset of the columns, I is a subset of the rows with their corresponding lags T , aligned to some fuzzy lagged mechanism by a maximal fuzziness degree of F (see example in Fig. 2e). The fuzziness reflects the ability of a column to deviate from its lagged location, by a maximum of F columns.\nA fuzzy lagged regulatory mechanism holds if for all j \u2208 J , each pair of rows i1, i2 \u2208 I, their corresponding lags Ti1 , Ti2 and fuzziness fi1,j , fi2,j , the proportion between the matrix entries is some constant, Ci1,i2 , dependent only on the rows i1, i2 and independent of the columns J : 2\nXi1,j+Ti1+fi1,j/Xi2,j+Ti2+fi2,j = Ci1,i2 .\nLet Gi indicate the regulation strength of object i; Ti indicate the influencing-lag of object i; Hj indicate the regulatory intensity of attribute j; and fi,j indicate the fuzzy alignment of object i to attribute j. Thus, the submatrix elements of a fuzzy lagged co-cluster should comply with the relation: Xi,j \u2248 GiHj+Ti+fi,j for all i \u2208 I and j \u2208 J . We use the non-fuzzy lagged co-clustering relative error criteria [70, 79] to express the deviation of Xi,j from the approximation of GiHj+Ti+fi,j . Thus, our aim is to mine large submatrices which follow a fuzzy lagged regulatory mechanism, with a relative error below a pre-defined threshold:\n1 \u03b7 \u2264 GiHj+Ti+fi,j Xi,j \u2264 \u03b7, \u2200 i \u2208 I, j \u2208 J. (1)\nTo ease analysis, we move from a multiplicative model to an additive model. We do so by applying a logarithm transformation, setting Ai,j = log(Xi,j), Ri\n2 Based on the standard co-clustering model definition, according to which \u2200j \u2208 J , Xi1,j/Xi2,j = Ci1,i2 [17, 53] and the lagged co-clustering model definition, according to which \u2200j \u2208 J , Xi1,j+Ti1 /Xi2,j+Ti2 = Ci1,i2 [70, 79].\n= log(Gi), Cj+Ti+fi,j = log(Hj+Ti+fi,j ) and \u03b5 = log(\u03b7). Therefore, our problem turns into finding Ri, Ti, Cj and fi,j , such that for all i, j: 3\n\u2212\u03b5 \u2264 Ri + Cj+Ti+fi,j \u2212Ai,j \u2264 \u03b5. (2)\nThe optimality of a submatrix depends on the objective function \u00b5(I, J) being used. Examples of such common functions are: area: \u00b5(I, J) = |I| \u00b7 |J |; perimeter: \u00b5(I, J) = |I| + |J |; and \u00b5(I, J) = |I|/\u03c8|J|, 0 < \u03c8 < 1 [53, 65], which favors the inclusion of one column over the exclusion of a relatively large amount of rows. Such preferment of columns over rows appears mostly in biologically-oriented datasets where m n [40]. Nevertheless, for many fuzzy lagged datasets, assumptions relating to the number of rows vs. the number of columns is usually futile, i.e., a temporal dataset will usually contain thousands of time readings or, in an on-line version, an infinite stream of columns. Consequently, we allow the use of any monotonically growing objective function \u00b5(I, J). Thus, our problem turns into mining an optimal size submatrix with a relative error below some given threshold.\nDefinition 2. The error of a submatrix A, defined by a subset J of the columns, a subset I of the rows and their corresponding lags T is:\n\u03b5 T,F (I, J) = min R,C max i\u2208I,j\u2208J |Ri + Cj+Ti+fi,j \u2212Ai,j |. (3)\nThe error reflects the maximum deviation of a fuzzy lagged co-cluster\u2019s entry, from being expressed as Ri + Cj+Ti+fi,j .\nAt this point, we have all that is required to formally define a fuzzy lagged cocluster. As mining small clusters, e.g., [2\u00d7 2], may not be of interest, we further extend the model to enable the user to specify the desired minimum dimensions: (1) minimum number of rows, expressed as a fraction of m, denoted \u03b2; and (2) minimum number of columns, expressed as a fraction of n, denoted \u03b3.\nDefinition 3. Let A be a matrix of size m\u00d7n, F \u2265 0 and 0 < \u03b2, \u03b3 \u2264 1 constants independent of the matrix dimensions. A fuzzy lagged co-cluster of a matrix A with an error w \u2265 0 is a tuple (I, T, J, F ) with J a subset of the columns, I a subset of the rows with their corresponding lags T , which satisfies the following:\n\u2013Size: The number of rows is 2 \u2264 \u03b2m \u2264 |I| and the number of columns is 2 \u2264 \u03b3n \u2264 |J |.\n\u2013Fuzziness: \u2212F \u2264 fi,j \u2264 F , for all i \u2208 I and j \u2208 J . \u2013Error: \u03b5\nT,F (I, J) \u2264 w. i.e., for all i \u2208 I and j \u2208 J there exists Ri, Ti and\nCj , such that |Ri + Cj+Ti+fi,j \u2212 Ai,j | \u2264 w. Ri, i \u2208 I will be called a column profile, Ti, i \u2208 I will be called a lagged column profile and Cj , j \u2208 J will be called a row profile.\nAs a consequence, lagging row i by Ti and shifting it by Ri, will place each column j \u2208 J , aligned with its fuzziness fi,j , within a maximal error of w of the row profile. The specific case of fi,j = 0 for all i \u2208 I and j \u2208 J , is equivalent to the non-fuzzy lagged co-cluster definition given in the previous chapter.\n3 For an anti fuzzy lagged correlations, i.e., Xi,j \u2248 Gi/Hj+Ti+fi,j , one should apply: \u2212\u03b5 \u2264 Ri \u2212 Cj+Ti+fi,j \u2212Ai,j \u2264 \u03b5."}, {"heading": "2.1. Hardness Results", "text": "The complexity of the fuzzy lagged co-clustering problem depends on the nature of the cluster being mined, which is reflected by the objective function \u00b5 being used. Former literature has shown that many such non-fuzzy and non-lagged instances are NP-complete [17, 49, 60, 70].\nObservation 1. Any hardness or inapproximability, resulting either from the non-fuzzy or the non-lagged problem, implies the same result for the fuzzy lagged problem.\nProof. The fuzzy lagged co-clustering problem extends the lagged co-clustering problem (fi,j=0, \u2200i \u2208 I, j \u2208 J), which in turn extends the co-clustering problem (Ti=0, \u2200i \u2208 I). Thus, any valid instance of the non-fuzzy or the non-lagged problems can be seen as an instance of the fuzzy lagged problem. By negation, a polynomial time algorithm for the fuzzy lagged co-clustering problem would allow the lagged co-clustering problem or the co-clustering problem to be solved optimally in polynomial time \u2013 contradiction.\nThe following observation demonstrates a polynomial reduction between a fuzzy lagged instance and a non-fuzzy, non-lagged instance.\nObservation 2. Let A be a fuzzy lagged matrix of size [m\u00d7n] and for all i \u2208 I, |{fi,j : \u2200j \u2208 J, fi,j 6= 0}| \u2264 log(mn). The matrix A can be presented as a non-fuzzy, non-lagged matrix A\u2032, of size [2mn(2F + 1) log(mn) \u00d7 (3n+ 2F )].\nProof. We duplicate each row i \u2208 A, to represent all possible lags and fuzziness for that row. Each row i (1 \u2264 i \u2264 m) can have 2n possible lags (\u2212n \u2264 lag \u2264 n) and (2F + 1)log(mn) possible fuzziness (a maximum of log(mn) columns, each with a possible fuzziness assignment of: \u2212F \u2264 fuzziness \u2264 F ), resulting in (3n + 2F ) columns. Null entries resulting from such alignments, i.e., lag and fuzziness, are marked as missing values. The resulting non-fuzzy, non-lagged matrix A\u2032, is therefore of size [m(2n)(2F + 1) log(mn) \u00d7 (3n + 2F )] = O((mn)c) for some c = O(log(F )). The result complies with a matrix of size [2mn \u00d7 3n] for the specific case of F=0 [70].\nCorollary 1. Let A be a fuzzy lagged matrix. The problem of finding the largest square fuzzy lagged co-cluster (I, T, J, F ) (|I|=|J |) in A is NP-complete.\nProof. Following Observation 1, the fuzzy lagged co-clustering problem is NPhard. Yet, verifying a submatrix of A to be a fuzzy lagged co-cluster can be done in polynomial time by examining whether each entry holds the inequality of \u2212\u03b5 \u2264 Ri + Cj+Ti+fi,j \u2212Ai,j \u2264 \u03b5. Therefore the problem is NP-complete.\nThe following NP-complete approximations are worth mentioning [70]: approximating the size of the largest combinatorial square co-cluster with an approximation factor of n1\u2212 ; approximating the size of the minimal sequential cluster-set for the co-clustering problem within a constant factor (Max-SNPHard); and, approximating the minimal set of combinatorial squares (co-cluster set) with an approximation factor of n1\u2212 ."}, {"heading": "3. The FLC Algorithm", "text": "We now present the FLC algorithm. This section also includes a proof for the algorithm\u2019s guarantee to mine with fixed probability, in a polynomial number of iterations, a fuzzy lagged co-cluster that encompasses an optimal fuzzy lagged co-cluster. In addition, we supply a run-time analysis and several extensions."}, {"heading": "3.1. The Algorithm", "text": "The input of the algorithm is: a matrix A of real numbers; a maximum allowed error value w; a maximum allowed fuzziness degree F ; a minimum fraction of the rows \u03b2; and, a minimum fraction of the columns \u03b3. The algorithm itself uses a projected clustering approach. This common technique for mining co-clusters [49, 65] uses iterative random projection (i.e., a Monte-Carlo strategy) to obtain the cluster\u2019s seed. It later grows the seed into a cluster. The output using this method is guaranteed, with fixed probability, to contain fuzzy lagged co-clusters that comply with the specified \u03b2, \u03b3, F , and encompass the optimal fuzzy lagged co-cluster. Each mined cluster precisely obtains the rows and lags of the optimal cluster, with a maximum 2 ratio of its columns (i.e., a maximum addition of J columns) and a maximum 2 ratio of its error.\nAlgorithm 1 presents the FLC algorithm. Generally, the algorithm can be divided into four stages, as follows. (1) Seeding (lines 3-4): a random selection of a row and a set of columns to serve as seeds. (2) Addition of rows (lines 8-12): we search for rows that reside within an error w of the row and column profiles (see Def. 3). Unfortunately, these profiles are unknown. It may happen that the seed lies within the edge of the cluster. In such cases, rows situated on the other edge of the cluster would be within an error of 2w. A naive exhaustive search is computationally not feasible, as there is an exponential number of combinations. To reduce this complexity, we use a sliding window technique. This technique enables a polynomial complexity. The window slides on the sorted set of events: (Ai,j+f \u2212 Ap,s), where i \u2208 m, j \u2208 n, |f | \u2264 F and s \u2208 S. In order to achieve an error of 2w, we set the width of the sliding window to 4w, which results in: \u03b5 T,F\n({i, p}, J) = (maxj\u2208J(Ai,j+f \u2212Ap,s)\u2212minj\u2208J(Ai,j+f \u2212Ap,s))/2 = (4w)/2 = 2w (see Remark 3, below). (3) Addition of columns (lines 14-18): this is similar to the previous stage, but accumulating only columns that comply with the accumulated rows. (4) Polynomial repetition of the above steps (line 1) providing a guarantee to mine an encompassed optimal fuzzy lagged co-cluster.\nThe FLC algorithm augments the (non-fuzzy) lagged co-clustering LC miner [70] to mine fuzzy lagged co-clusters. In addition, its improved design suggests a substantial improvement in run-time (in comparison to the LC miner) when mining non-fuzzy (i.e., F=0) clusters, from a run-time of O((mn)2\u2212log \u03b3) [70, Section 6], to O((mn)1\u2212log \u03b3 log2(mn)) (see Subsection 3.2).\nThe nature of the FLC algorithm suggests that it is sensitive to the error being set. This key parameter needs to be carefully set in order to mine meaningful clusters. Setting it too high might result in many artifact clusters, while setting it too low might preclude valid clusters. To choose an appropriate error value, one can adopt any of the methods suggested for the non-fuzzy lagged co-clustering model [70].\nInnately embedded within the algorithm are many desirable properties such as: (1) the ability to handle noise by allowing the fuzzy lagged co-cluster to de-\nAlgorithm 1: FLC algorithm Input: A, an m\u00d7 n matrix of real numbers; w, the maximum acceptable error; F\nthe maximum degree of fuzziness; \u03b2, the minimum fraction of rows; and \u03b3, the minimum fraction of columns.\nOutput: A collection of fuzzy lagged co-clusters (I, T, J, F ) whose error does not exceed 2w.\nInitialization: Setting N and |S| is thoroughly discussed in the following section. 1 loop N times\n2 // Initialization Phase 3 randomly choose a discriminating row p : 1 \u2264 p \u2264 m 4 randomly choose a discriminating set of columns S : S \u2286 n 5 I \u2190 {p} 6 J \u2190 S 7 // Row Addition Phase 8 foreach row i : 1 \u2264 i \u2264 m do 9 slide a 4w width window on\n{es,j,f | es,j,f = Ai,j+f \u2212Ap,s, \u2200j \u2208 n, \u2200s \u2208 S, \u2212F \u2264 f \u2264 F} 10 if (\u2200s \u2208 S \u2203t, t+s=j+f \u2227 \u2203es,j,f \u2208window) then 11 // found a common lag t for all s \u2208S 12 add (i, t) to (I, T )\n13 // Column Addition Phase 14 randomly choose a discriminating column s \u2208 S 15 foreach column j : 1 \u2264 j \u2264 n do 16 slide a 4w width window on {ei | ei = Ai,j+Ti+f \u2212Ai,s+Ti , \u2200i \u2208 I, \u2212F \u2264 f \u2264 F} 17 if (\u2200i\u2208I \u2203ei\u2208window) then 18 add j to J\n19 // Validation of Dimensions 20 if |I| < \u03b2m or |J | < \u03b3n then 21 discard (I, T, J, F )\n22 return a collection of valid (I, T, J, F )\nviate from the model (see Def. 3) by some pre-specified error. We accomplish this by using a window of width 4w as described above; (2) the ability to mine overlapping clusters by utilizing the Monte-Carlo strategy, which grows independent seeds into clusters on each repetitive run; (3) the ability to overcome missing values by calculating the coherence of a fuzzy lagged co-cluster on the non-missing values of the submatrix [53, 83]; and (4) anti-correlation (see Footnote 3). When both correlated and anti-correlated patterns may appear in the same fuzzy lagged co-cluster, one can exercise one of the following solutions: (i) duplicate each row of the input matrix to contain the anti-values of the row, i.e., for each row i \u2208 m, add to the input matrix a new row containing the values of: \u2212Ai,j , j \u2208 n; or (ii) the algorithm\u2019s row addition phase (lines 8-12) should be modified into a two-pass sliding window. The first pass (similar to the current line 9) is over events of the type: {es,j,f | es,j,f = Ap,s \u2212Ai,j+f , \u2200j \u2208 n, \u2200s \u2208 S, \u2212F \u2264 f \u2264 F}, while the second pass is over events of the type: {es,j,f | es,j,f = Ap,s +Ai,j+f , \u2200j \u2208 n, \u2200s \u2208 S, \u2212F \u2264 f \u2264 F}. The intuition behind the second pass is that an anti-correlated value is basically the value of (\u2212Ai,j). Therefore, the first sliding window pass, which includes\nevents of Ap,s\u2212Ai,j+f , should now be repeated over events of Ap,s\u2212 (\u2212Ai,j+f ), which equals to Ap,s +Ai,j+f ."}, {"heading": "3.2. Run-time", "text": "The row addition phase (lines 8-12) handles, for each of the m rows, a sliding window of O(n\u00b7|S|\u00b7F ) events. Therefore, its run-time is O(m\u00b7n|S|F \u00b7log(n|S|F )). In the same manner, the column addition phase (lines 14-18) handles, for each of the n columns, a sliding window of O(mF ) events. Therefore, its run-time is O(n\u00b7 mF \u00b7 log(mF )). Thus, the inner for-loops run-time is: O(mn log(mn) log(n)F ).\nThe total number of iterations is bounded by Theorem 2 to N = O(1/\u03b2\u03b3|S|). Thus, for the constants \u03b2 and \u03b3 independent of the matrix dimensions (see Def. 3), and the discriminating set |S| = O(log(mn)) (see Theorem 1), the FLC\u2019s total run-time is polynomial in the matrix size: O((mn)1\u2212log \u03b3 log2(mn)F ) which in many cases can be seen more permissibly as: O((mn)2\u2212log \u03b3)."}, {"heading": "3.3. Sub-optimality of FLC Algorithm", "text": "Next, we analyze the ability of the FLC algorithm to mine coherent and relevant fuzzy lagged co-clusters. In particular we prove that the algorithm guarantees to mine, with fixed probability, in a polynomial number of iterations, a fuzzy lagged co-cluster that encompasses an optimal fuzzy lagged co-cluster. The mined cluster will acquire the rows of the optimal cluster and their lags with a maximum 2 ratio of its columns. Consequently, the mined cluster will have a maximum 2 ratio of the optimal cluster error. We demonstrate this guarantee with experiments on both artificial and real-life datasets in Section 4.\nSince the FLC algorithm augments the non-fuzzy lagged co-clustering LC algorithm [70], its capabilities and theoretical analysis are deeply inspired by it. The structure of the proof consists of two major stages. The first stage is based on an important insight stating that a sufficient size for a discriminating set is logarithmic in the size of the set [49, 65]. Following this result, we show that by taking any small random subset of columns of size O(log(mn)), we can discriminate an optimal fuzzy lagged co-cluster with a probability of at least 0.5. The second stage utilizes the previous result to mine, in a polynomial number of iterations and with a probability of at least 0.5, clusters that encompass the optimal fuzzy lagged co-cluster.\nThe definition of a discriminating set for the fuzzy lagged model is given as follows.\nDefinition 4. Let (I, T, J, F ) be a fuzzy lagged co-cluster with an error w and p \u2208 I. S \u2286 J is a discriminating set for (I, T, J, F ) with respect to p if it satisfies: 1. \u03b5\nT,F ({i, p}, S) \u2264 w for all (i, t) \u2208 (I, T ).\n2. \u03b5 T,F ({i, p}, S) > w for all (i, t) /\u2208 (I, T ). The importance of using a discriminating set lies in its ability to discriminate, i.e., include fuzzy lagged rows which belong to the fuzzy lagged co-cluster and exclude those that do not. Therefore, as will be later shown, a discriminating set serving as a seed would grow in a deterministic way to a unique fuzzy lagged co-cluster, i.e., choosing a discriminating set more than once will yield the same\nfuzzy lagged co-cluster. Next, Theorem 1 states that for an optimal fuzzy lagged co-cluster (I\u2217, T \u2217, J\u2217, F ), there is an abundance of small sub-sets of columns, each of which is a discriminating set with a probability of at least 0.5.\nTheorem 1. Let (I\u2217, T \u2217, J\u2217, F ) be an optimal fuzzy lagged co-cluster of error w, with \u03b3 \u2264 (|J\u2217|/n) < \u03b3\u2032, and let p \u2208 I\u2217. Any randomly chosen columns subset S of J\u2217, of size |S| \u2265 log(4mn)/ log(1/3\u03b3\u2032(2F + 1)), is a discriminating set for (I\u2217, T \u2217, J\u2217, F ), with respect to p, with a probability of at least 0.5.\nProof. Let (I\u2217, T \u2217, J\u2217, F ) be a fuzzy lagged co-cluster with a column profile R\u2217i , i \u2208I\u2217, a lagged column profile T \u2217i , i \u2208I\u2217 and a row profile C\u2217j , j \u2208J\u2217. We show that for any S that satisfies the above, condition (1) of Def. 4 always holds and that the probability of condition (2) not to hold is less than 0.5. This allows the probabilistic guarantee by the repeated execution.\nCondition (1) is always satisfied, as {i, p} \u2286 I\u2217 and S \u2286 J\u2217. Therefore, \u03b5 T,F ({i, p}, S) \u2264 \u03b5 T\u2217,F (I \u2217, S) \u2264 \u03b5 T\u2217,F (I\n\u2217, J\u2217) \u2264 w, i.e., as being part of the optimal fuzzy lagged co-cluster, the error is not greater than w.\nMoving to condition (2), we first extract an upper bound for the probability of S to fail to be a discriminating set for (I\u2217, T \u2217, J\u2217, F ) with respect to p, for a particular row, its corresponding lag and fuzziness. Based on all possible combinations of rows, lags and fuzziness, we calculate the lower bound for the probability of S to discriminate, showing it to be greater than 0.5.\nThe subset S fails to be a discriminating set for (I\u2217, T \u2217, J\u2217, F ) with respect to p, only if there exists a fuzzy lagged row i with it\u2019s corresponding lag t, (i, t) /\u2208 (I\u2217, T \u2217), which fits the cluster, i.e., \u03b5\nT,F ({i, p}, S) \u2264 w. Next, we calculate\na bound for the probability of this to hold for a particular row i, lag t and fuzziness f . According to Def. 3, \u03b5\nT,F ({i, p}, S) \u2264 w means that there are Ri, Ti,\nfi,j , Rp, Tp(=0), fp,j (={0}) and Cj , j \u2208 S, such that: |Ai,j\u2212Ri\u2212Cj+Ti+fi,j | \u2264 w and |Ap,j \u2212Rp\u2212Cj+Tp+fp,j | \u2264 w \u2200j \u2208 S. Shifting and aligning row i \u2208 I (in the first inequality) by Ti and fi,j respectively, and subtracting the second inequality (of row p) we obtain, for all j \u2208 S and some R (= Ri \u2212Rp): |Ai,j \u2212Ap,j \u2212R| \u2264 2w. (4)\nNext, we show that due to the optimality of the fuzzy lagged co-cluster, there are no more than 3|J\u2217| columns that satisfy the above equation for each row i \u2208 I. If |Ai,j \u2212 Ap,j \u2212 R| \u2264 2w then: \u22122w \u2264 Ai,j \u2212 Ap,j \u2212 R \u2264 2w. After adding (Ap,j \u2212 C\u2217j \u2212 R\u2217p) to both sides we obtain: (Ap,j \u2212 C\u2217j \u2212 R\u2217p) \u2212 2w \u2264 Ai,j \u2212C\u2217j \u2212R\u2217p \u2212R \u2264 (Ap,j \u2212C\u2217j \u2212R\u2217p) + 2w. Since (I\u2217, T \u2217, J\u2217, F ) is an optimal fuzzy lagged co-cluster, then |Ap,j \u2212 C\u2217j \u2212R\u2217p| \u2264 w for all j \u2208 J\u2217. Therefore, we obtain:\n\u22123w \u2264 Ai,j \u2212 C\u2217j \u2212R\u2217p \u2212R \u2264 3w. (5) We now present Lemma 1, which enables calculating a bound for the number of columns that satisfies Equation 5, i.e., columns that if considered to be part of the discriminating set will result in adding rows that do not belong to the optimal fuzzy lagged co-cluster.\nLemma 1. Let J \u2286 J\u2217, and let (i, t) /\u2208 (I\u2217, T \u2217). If |Ai,j \u2212C\u2217j \u2212 r| \u2264 w for some r and all j \u2208 J , then J \u2282 J\u2217. Proof. By negation, suppose that (I, T, J, F ) is a fuzzy lagged co-cluster that augments the optimal fuzzy lagged co-cluster (I\u2217, T \u2217, J\u2217, F ) by using J \u2287 J\u2217,\nI = I\u2217 \u222a {i} and T = T \u2217 \u222a {t}. The new cluster is a fuzzy lagged co-cluster of error w satisfying \u00b5(I, J) > \u00b5(I\u2217, J\u2217), hence contradicting the optimality of (I\u2217, T \u2217, J\u2217, F ).\nThe result of Lemma 1 is that for a fuzzy lagged row (i, t) /\u2208 (I\u2217, T \u2217) there are at most |J\u2217| columns that lie in an interval of length 2w (derived from |Ai,j\u2212C\u2217j \u2212r| \u2264 w of Lemma 1). Therefore, the interval [\u22123w, 3w] of Equation 5, which can be seen as the three intervals [\u22123w,\u2212w], [\u2212w,w] and [w, 3w], contains at most 3|J\u2217| columns that satisfy Equation 4. Choosing all columns of S out of the above 3|J\u2217| columns would result in the inclusion of an undesirable fuzzy lagged row (i, t) /\u2208 (I\u2217, T \u2217). Therefore, choosing |S| columns from the 3|J\u2217| columns out of the n matrix columns has a probability which is bounded by: (3|J\u2217|/n)|S| \u2264 (3\u03b3\u2032)|S|.\nThe latter probability refers to a particular row i, lag t and fuzziness f . The number of combinations for some row i (1 \u2264 i \u2264m), some lag t (\u2212n \u2264 t \u2264 n) and some fuzziness f (\u2212F \u2264 f \u2264 F ) is: (m)(2n)(2F + 1)|S| (as each of the |F | columns can be assignment with any fuzziness within the range \u2212F \u2264 f \u2264 F ). Therefore, the probability of not discriminating is bounded (after substituting |S| \u2265 log(4mn)/ log(1/3\u03b3\u2032(2F+1))) by: 2mn(2F+1)|S|(3\u03b3\u2032)|S| = 2mn(3\u03b3\u2032(2F+ 1))|S| < 0.5.\nThis result of Theorem 1 is important since upon selecting p \u2208 I\u2217 and S \u2286 J\u2217, we can deduce I\u2217 and T \u2217.\nMoving to the second part of the proof, we show that when the FLC algorithm is run a polynomial number of iterations, it mines, with a probability of at least 0.5, a fuzzy lagged co-cluster encompassing the optimal fuzzy lagged co-cluster. We base this on Theorem 1, which shows the abundance of randomly selected discriminating sets of size O(log(mn)) with a discriminating probability of at least 0.5.\nTheorem 2. Let S be a discriminating set for an optimal fuzzy lagged co-cluster (I\u2217, T \u2217, J\u2217, F ) of error w. Provided N \u2265 2 ln 2/\u03b2\u03b3|S|, the FLC algorithm will mine a fuzzy lagged co-cluster (I, T, J, F ) of error 2w such that: I = I\u2217, T = T \u2217, J \u2287 J\u2217 and |J | \u2264 2|J\u2217|, with a probability of at least 0.5.\nProof. Since |I\u2217| \u2265 \u03b2m, the probability of choosing a row (see line 3) that satisfies p \u2208 I\u2217 is at least \u03b2. As |J\u2217| \u2265 \u03b3n, the probability of choosing a discriminating columns set (see line 4) which satisfies S \u2286 J\u2217 is at least \u03b3|S|. Following Theorem 1, any given S \u2286 J\u2217 is a discriminating set with a probability of at least 0.5 with respect to p. Therefore, the probability that all N iterations (see line 1) fail to find a discriminating row p and a discriminating columns set S is (1 \u2212 0.5\u03b2\u03b3|S|)N . Substituting N \u2265 2 ln 2/\u03b2\u03b3|S| we obtain a maximum probability of (1 \u2212 0.5\u03b2\u03b3|S|)2 ln 2/(\u03b2\u03b3|S|). Using the inequality (1 \u2212 1/x)x < 1/e, for x \u2265 1, with x = 2\n\u03b2\u03b3|S| we get a probability that does not\nexceed (1\u2212 \u03b2\u03b3 |S| 2 ) 2 \u03b2\u03b3|S| ln 2\n< 1/eln2 = 0.5. It follows that the algorithm\u2019s chances of mining a fuzzy lagged co-cluster upon a p \u2208 I\u2217 and S \u2286 J\u2217 is at least 0.5. When such a fuzzy lagged co-cluster is mined, we obtain from the discriminating property of S (see Def. 4) that I = I\u2217 and T = T \u2217.\nThe following lemmas prove that the mined fuzzy lagged co-cluster contains J\u2217 and at most |J\u2217| additional columns.\nLemma 2. |J | \u2264 2|J\u2217|, i.e., the size of the mined columns set J is a maximum 2 factor of the size of the optimal cluster columns set J\u2217.\nProof. A column j is added to J only if: maxi(Ai,j+Ti+f -Ai,s+Ti)-mini(Ai,j+Ti+f - Ai,s+Ti)\u22644w (see lines 16-17), which is equal to \u03b5T,F (I, J) \u2264 2w (see Remark 3, below, in the case of a matrix of two columns). Therefore, \u2200i \u2208 I and \u2200j \u2208 J there exists Ri, Ti, Cj and fi,j such that: \u22122w \u2264 Ri + Cj+Ti+fi,j \u2212 Ai,j \u2264 2w. Since I = I\u2217, T = T \u2217 and initially J = S \u2286 J\u2217, we obtain from the optimality of (I\u2217, T \u2217, J\u2217, F ), that for each of the intervals [\u22122w, 0] and [0, 2w], there are at most |J\u2217| columns j satisfying |R\u2217i +C\u2217j+T\u2217i +f\u2217i,j\u2212Ai,j | \u2264 w. Thus, J accumulates up to a maximum of 2|J\u2217| columns.\nLemma 3. J \u2287 J\u2217, i.e., the mined columns set J contains the optimal cluster columns set J\u2217.\nProof. For each j \u2208 J\u2217, we obtain from the optimality of (I\u2217, T \u2217, J\u2217, F ) that |Ai,j \u2212R\u2217i \u2212 C\u2217j+T\u2217i +f\u2217i,j | \u2264 w. Thus, j will be added to J , namely j \u2208 J .\nAs a consequence of Lemma 2, additional columns that are not in J\u2217 might be added. Yet the maximum number of added columns is |J\u2217| (see Lemma 3) and the mined cluster will have a maximal error of 2w.\nRemark 1. The bound of |S| \u2265 log(4mn)/ log(1/3\u03b3\u2032(2F + 1)) includes the parameter \u03b3\u2032 whose value is not given as part of the problem input. In Subsection 4.1, we show experimentally that a random subset of size 0.6 log2(4mn)\u2212 1 will suffice, freeing the user from the burden of specifying the \u03b3\u2032-related trade-off.\nRemark 2. Theorem 1 describes a discriminating set S with a minimum discriminating probability of 0.5. In Subsection 4.1, we illustrate the relation between various magnitudes of |S| and their discriminating probability.\nRemark 3. Theorem 1, Theorem 2 and Algorithm 1 all assume only the existence of the profiles R\u2217i , T \u2217 i and C \u2217 j . Although the actual values of the profile are not calculated in practice, an explicit calculation can be computed. In the case of a matrix A of size [2\u00d7k] (equivalent to [{i,p}\u00d7S], see Def. 4), one can use the following polynomial technique [53, Subsection 4.1]. First, permute the columns of the matrix A so that:\nA1,1 \u2212A2,1 \u2264 A1,2 \u2212A2,2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 A1,k \u2212A2,k.\nNext, set w = [(A1,k\u2212A2,k)\u2212(A1,1\u2212A2,1)]/2, h = [(A1,k\u2212A2,k)+(A1,1\u2212A2,1)]/2 and let ` be such that: A1,` \u2212 A2,` \u2264 h \u2264 A1,`+1 \u2212 A2,`+1. Then R =< 0,\u2212h> and C =<A1,1 +w,A1,2 +w, . . . , A1,`+w,A1,`+1\u2212w, . . . , A1,k\u2212w>. Therefore, we get \u03b5\nT,F (I, J)=[maxj\u2208J(A1,j \u2212 A2,j) \u2212 minj\u2208J(A1,j \u2212 A2,j)]/2, where |I|=2\nand |J |=k. For cases of a general matrix size, we refer the reader to Melkman et al. [53, Subsection 4.2], which is a discrete version of the Diliberto-Straus algorithm [18].\nRemark 4. Neither Theorem 1 nor Theorem 2 make any assumption whatsoever on the distribution of the data in the matrix nor on the distribution of the data in the fuzzy lagged co-cluster to be mined. The FLC algorithm is completely generic."}, {"heading": "3.4. Extensions", "text": "Next, we present several extensions to the FLC algorithm and the resulting algorithmic modifications supporting these extensions."}, {"heading": "3.4.1. Varying Fuzziness", "text": "A major characteristic of the FLC algorithm is the maximum allowed fuzziness F . This fuzziness is assumed to be common to all entries of the matrix. The algorithm can be extended to include a different maximum fuzziness for each row of the matrix, denoted Fi, i \u2208 m. To achieve this, the algorithm needs to be modified in the events which are later used by the sliding window (lines 9 and 16 of Algorithm 1). Essentially, the modification is in using the row\u2019s maximum allowed fuzziness Fi instead of the global fuzziness F . The modifications are as follows.\n\u25e6 Line 9, which accumulates rows, should be modified to: Slide a 4w width window on {es,j,f | es,j,f = Ai,j+f \u2212Ap,s, \u2200j \u2208 n, \u2200s \u2208 S, \u2212Fi \u2264 f \u2264 Fi}.\n\u25e6 Line 16, which accumulates columns, should be modified to: Slide a 4w width window on {ei | ei = Ai,j+Ti+f \u2212Ai,s+Ti , \u2200i \u2208 I, \u2212Fi \u2264 f \u2264 Fi}.\nSimilarly, the algorithm can also be extended to include a different maximum allowed fuzziness for each column of the matrix, denoted Fj , j \u2208 n."}, {"heading": "3.4.2. Reduction in Size of the Discriminating Set", "text": "The discriminating set S, as shown by Theorem 1, is a small subset of size O(log(mn)). Nevertheless, the number of iterations N required for achieving the probabilistic guarantee as shown by Theorem 2, is exponentially proportional to |S|. To improve the algorithm\u2019s run-time, we propose to take a subset of the discriminating columns set S, denoted S0, and assume it has zero fuzziness over all of the cluster\u2019s rows, i.e., let (I, T, J, F ) be a fuzzy lagged co-cluster with a discriminating set of columns S, with the assumption that fi,j=0, for all i \u2208 I, j \u2208 S0. The assumption reduces the combinatorial number of rows that needs to be filtered by the discriminating set, and thus reduces the set size needed for the task (line 4 of Algorithm 1). However, this comes with the cost of limiting the nature of the clusters mined, i.e., fuzzy lagged co-clusters which do not have a minimum of |S0| columns of zero fuzziness would not be mined. To achieve that, we modify the FLC algorithm in the following way. Line 4, in addition to randomly choosing S, also randomly chooses S0 \u2286 S. Next, we modify line 9 to set the fuzziness f such that if s \u2208 S0 then f=0, or otherwise \u2212F \u2264 f \u2264F . The results of an experiment to evaluate the effectiveness of this approach are reported in Subsection 4.1 (see Expt. II), revealing that even a moderated subset of S for which a zero fuzziness is assumed, e.g., |S0|=3, supplies a good balance between the gain in run-time and the constraint it implies on the model. Expt. IV suggests a technique which enables the use of an even lower discriminating set size."}, {"heading": "3.4.3. Finding the Maximal Columns Set", "text": "As part of the process of column addition (lines 14-18 of Algorithm 1), each added column has its fuzziness setting. Nevertheless, when considering those settings in the context of a cluster, it may well happen that they do not co-exist. Take for example the following simple scenario: column j1 has a fuzziness of fi,j1=1 and column j2 has a fuzziness of fi,j2=\u22121, \u2200i \u2208 I. In the case of zero lag (Ti=0) and j2=j1 + 1, the cluster\u2019s fuzziness setting would not be valid as although j1 < j2, the actual matrix columns for j1 and j2 would be j2 (= j1 + 1 = j1 + fi,j1) and j1 (= j2\u2212 1 = j2 + fi,j2), respectively. In the case where j1 < j2 are time points, we expect that their actual matrix entries will also maintain the same ordering relations (and not j2 < j1).\nOne solution to this problem can be a post-processing step. We denote each of the columns\u2019 fuzziness setting as a bridge, where the bridges are drawn on the discrete entries of the matrix A (see example in Fig. 3a). We therefore wish to find the maximum non-intersecting set of bridges. To do so, consider the following problem: let G=(V,E) be a bi-partite bridge graph with |V |=n vertices on each side, and each edge e\u2208E is a monotonic path between the upper and the lower side, i.e., a monotonous path of \u3008Ai1,j1 , Ai2,j2 , . . . , Aik,jk\u3009, where i1 < i2 < . . . < ik. The goal is to find the maximal set of non-intersecting edges in graph G. We do so by first showing in Lemma 4 that the intersection graph G\u0302 of the bridge graph G (see example in Fig. 3b and 3a, respectively) is a perfect graph. Next, we conclude in Corollary 2 that the graph G is also a perfect graph. As\nFig. 4a presents a 5-cycle intersection graph G\u0302. Fig. 4b illustrates the failure to draw the correlating 5-cycle bridge graph G.\nsuch, polynomial algorithms for finding a maximum clique can be applied, which results in finding the maximum set of non-intersecting columns\u2019 bridges.\nLemma 4. A bridge graph is a perfect graph.\nProof. Let G\u0302 be the intersection graph of the bridge graph G. We show by\nnegation that the intersection graph G\u0302 cannot have a cycle of a minimum length\nof 5, and thus G\u0302 is a perfect graph [66]. Fig. 4 depicts the following steps.\n1. By negation, let us assume that there is an intersection graph G\u0302 with a cycle of a minimum length 5 (see Fig. 4a).\n2. Let us denote the 5 vertices of G\u0302 as a, b, c, d and e.\n3. Because a and c do not intersect, assume (w.l.o.g.) that a is to the right of c (see Fig. 4b).\n4. Observe that because c and e do not intersect, e cannot be to the left of c as it should intersect a. Therefore e is to the right of c.\n5. b intersects c but does not intersect e. Therefore e is to the right of b.\n6. d does not intersect b: (a) if d is to the left of b it cannot intersect e \u2013 negation. (b) if d is to the right of b it must also be to the right of a but then it cannot intersect c \u2013 negation.\nCorollary 2. The algorithm\u2019s column addition phase, which results in a maximum set of non-intersecting columns, has a polynomial run-time.\nProof. Following Lemma 4, the intersection graph G\u0302 is a perfect graph. As the complement graph of a perfect graph is also a perfect graph [50], we can apply a maximum clique polynomial run-time procedure [28, 29] to the graph G in order to acquire a maximum columns set."}, {"heading": "4. Experiments", "text": "Next, we present an extensive evaluation of the FLC algorithm, using both artificial and real-life data."}, {"heading": "4.1. Experiments with Artificial Data", "text": "In comparison to real-life data, the use of artificial data enables maximum control over the algorithm\u2019s input and parameter settings, which in turn enables the verification and validation of the algorithm\u2019s output. Specifically, the contributions of the experimentation used for the FLC algorithm with artificial data are threefold. First, it establishes default values for the various parameters. Second, it enables the verification of theoretical bounds. Finally, it demonstrates a feasible actual run-time.4\nExpt. I: Probability of Artifacts\nAn interesting question in the context of the fuzzy lagged model is how frequently artifacts are mined. An artifact is a submatrix that was formed not as a result of some hidden regulatory mechanism, but as a mere aggregation of noise. Such artifacts are undesirable as they add irrelevant output.\nGiven a matrix with randomly generated values (from a uniform distribution), the probability of mining an artifact fuzzy lagged co-cluster (I, T, J, F ) depends on several parameters: (1) the matrix dimensions, [m\u00d7 n]; (2) the fuzzy lagged co-cluster dimensions, [|I| \u00d7 |J |]; (3) the error \u03b5, 0% \u2264 \u03b5 \u2264 100%; and (4) the fuzziness F . Intuitively, the larger the error \u03b5, fuzziness F , and matrix size m and n, and the smaller the requested cluster dimensions I and J , the greater the chance of mining artifact clusters with an increasing probability of smaller clusters. To examine the correlation between these parameters, we present the following upper bound probability analysis.\nAssume we know the column profile p. The probability of a column j \u2208 J of a fuzzy lagged row i \u2208 I to be within a surrounding of fuzziness F and error w, encircling p is: 1 \u2212 (1 \u2212 min(2\u03b5, 1))2F+1. Hence, the probability of all columns j \u2208 J of a fuzzy lagged row i \u2208 I to be within a surrounding of fuzziness F and error w, encircling p is: [1\u2212 (1\u2212min(2\u03b5, 1))2F+1]|J|. Thus, the probability of all rows I to form a fuzzy lagged co-cluster is: [1 \u2212 (1 \u2212min(2\u03b5, 1))2F+1]|I||J|. The probability of not having such a fuzzy lagged co-cluster is therefore: 1\u2212 [1\u2212 (1\u2212 min(2\u03b5, 1))2F+1]|I||J|. The representation of a fuzzy lagged matrix of size [m\u00d7n] as a non-lagged matrix, results in a matrix of size [2mn\u00d73n] (see Subsection 2.1). Thus, choosing a set size |I| out of (2mn) rows has ( 2mn |I| ) combinations. Similarly,\nchoosing a set size |J | out of (3n) columns has ( 3n |J| ) combinations. Therefore, the probability that none of the possible sub-matrices of this size in the matrix forms a fuzzy lagged co-cluster is: {1\u2212 [1\u2212 (1\u2212min(2\u03b5, 1))2F+1]|I||J|}( 2mn |I| )( 3n |J|). Hence, an upper bound for the probability of at least one artifact fuzzy lagged co-cluster\n4 While the number of iterations is proved to be polynomial, we want to ensure that the actual performance for large inputs is feasible.\nto exist is:\n1\u2212 {1\u2212 [1\u2212 (1\u2212min(2\u03b5, 1))2F+1]|I||J|}( 2mn |I| )( 3n |J|). (6)\nAs \u03b5, F , m and n increase, the above probability will increase. As I and J increase the above probability will decrease.\nTo facilitate understanding of the formula, we present Fig. 5 and Fig. 6 (generated by Wolfram|Alpha [82]). The main conclusion based on the figures is that fuzzy lagged co-clusters of small dimensions (i.e., clusters smaller than 0.5% of the matrix size) already have an insignificant probability of being caused by a random formation and presenting artifact patterns. Thus, fuzzy lagged co-clusters representing a regulatory mechanism, which are naturally large in dimensions, have an insignificant probability of being noise. Consequently, ordinary mining using practical dimensions has an insignificant probability of mining artifacts.\nExpt. II: Discriminating Set Size\nTheorem 1 provides us with the following bound for the discriminating set: |S| \u2265 log(4mn)/ log(1/3\u03b3\u2032(2F +1)), where \u03b3\u2032 specifies the ratio between the number of columns in an optimal fuzzy lagged co-cluster and the number of matrix columns. The fact that the bound depends on \u03b3\u2032 is undesirable, since this parameter is not part of the problem input and the user has no knowledge about it. To get a sense of the magnitude of feasible values for |S|, we conducted the following experiment. We first created random [m \u00d7 n] matrices, with sizes ranging from 102 to 105 and values uniformly distributed in the range of 100 to 1100. We set the dimensions of the cluster size to \u03b2, \u03b3 \u2208 {0.3, 0.5, 0.8}. Then, we generated a random fuzzy lagged co-cluster of error \u03b5=1%, fuzziness F=1, size [\u03b2m\u00d7\u03b3n] and put it at a random location in the matrix overriding the existing values. Then, a size - k subset of the fuzzy lagged co-cluster columns was chosen at random 100 times, to check whether it is a discriminating set according to Def. 4. This process was repeated for k=1,. . . until reaching a value for k which the subset successfully discriminated in all of the 100 trials. We repeated the above procedure for various sizes of S0 in order to examine the effectiveness of S0 in reducing the size of S (see Subsection 3.4.2).\nFig. 7 presents the results of extensive experimentation relating to the tradeoff between the size of the discriminating column set |S| as a function of log2(4mn) for various |S0|. The following important observations can be made from Fig. 7: (1) for each |S0| value used, we obtain the linear relationship derived from Theorem 1; (2) the decrease in size of the discriminating set S is proportional to the size of S0, i.e., the larger |S0| used, the lower |S| needed. Setting |S0|=3 seems to be the most effective in this case, as it offers a good balance between run-time reduction and the resultant limitation of the model (i.e., assuming |S0| non-fuzzy columns); and (3) we obtain an easy-to-use, \u03b3\u2032 free, formula for setting |S|. For example, using |S0|=3 we obtain: |S| = 0.6197 log2(4mn)\u22121.0063 \u2248 0.6 log2(4mn)\u2212 1.\nExpt. III: Discriminating Probability vs. Discriminating Set Size\nThe previous experiment considered a set of size |S| to be discriminating if it successfully discriminated in all N trials (N=100). In this experiment, we wish to explore the relationship between |S| and its discriminating probability (i.e., in\nhow many of the N trials did the set actually discriminate). We do so by recording different sizes of |S| and their ability to discriminate. The experiment was conducted using the same methodology as Expt. II, using |S0|=3 as suggested.\nFig. 8 presents the discriminating probability as a function of the discriminating set size |S|. The main finding from Fig. 8 is that even for small sizes of\n|S|, a substantial discriminating probability is achieved (e.g., 89% for |S|=7). Since |S| appears as an exponent in the estimated run-time, choosing a smaller |S| will have a notable effect on the reduction of run-time, without having any major negative effects on the results.\nExpt. IV: Discriminating Set Size vs. Number of Iterations Needed\nTheorem 2 shows that the probability of the FLC algorithm to mine a fuzzy lagged co-cluster is at least 0.5. We refer to that probability as a \u201chit rate\u201d. The hit rate depends on the discriminating probability p, of the discriminating set S, and the number of iterations N being used: (hit rate) = 1\u2212 (miss rate) = 1\u2212(1\u2212p\u03b2\u03b3|S|)N . Using Theorem 2, the number of iterations is N=2 ln 2/\u03b2\u03b3|S|, resulting in a hit rate of: 1\u2212 0.25p, i.e., (hit rate) = 1\u2212 (1\u2212 p\u03b2\u03b3|S|)N = 1\u2212 (1\u2212 p\u03b2\u03b3|S|) 2 ln 2 \u03b2\u03b3|S| = 1\u2212 (1\u2212 p\u03b2\u03b3|S|) p\u00b72 ln 2 p\u00b7\u03b2\u03b3|S| \u2265 1\u2212 1\ne2p ln 2 = 1\u2212 122p = 1\u2212 0.25 p.5\nExpt. III implies that using a discriminating set smaller than the one recommended by Expt. II will not only exponentially decrease the run-time, but will also ensure a reasonable discriminating probability. However, a decrease in the discriminating set size results in a decrease in the hit rate. Therefore, in order to improve the hit rate, an increase in the number of iterations is required. In practice, by reducing the discriminating set size, it is possible to reduce the run-time by more than the increase needed to ensure the desired hit-rate.\nWe next present an analysis aimed at finding the best setting to achieve a minimum run-time. As a base line, we use Expt. II discriminating sets, with\n5 Theorem 2 uses Theorem 1 discriminating sets of p=0.5 and thus results in a hit rate of 0.5.\na discriminating probability of \u2248100% and a size of 8.5 for matrices of size [100\u00d7 100]. Such sets will yield a hit rate of \u224875%.\nBased on the average curve, shown in Fig. 8, the discriminating probabilities of |S|={4, 5, 6, 7, 8, 9} are P|S|={8.2%, 40.8%, 74.3%, 89.4%, 94.1%, 95.6%}, respectively. In order to reach a hit rate of 75%, we need to compensate for the loss in the above discriminating probability by increasing the number of iterations. Note that when the number of iterations N is multiplied by C=1/p, the resulting hit rate becomes: 1 \u2212 (1 \u2212 p\u03b2\u03b3|S|)( 1 p \u00b7N) = 1 \u2212 (1 \u2212 p\u03b2\u03b3|S|) 2 ln 2 p\u03b2\u03b3|S| \u2265 1 \u2212 1 e2 ln 2\n= 1 \u2212 122 = 0.75. Therefore, by factoring the number of iterations N(|S|)=2 ln 2/(\u03b2\u03b3|S|) by the inverse of the discriminating probability, we obtain for |S|={4, 5, 6, 7, 8, 9}, a C|S|={1/8.2%, 1/40.8%, 1/74.3%, 1/89.4%, 1/94.1%, 1/95.6%}={12.24, 2.45, 1.35, 1.12, 1.06, 1.05}, respectively. Fig. 9 depicts the ratio between C(|S|)\u00d7N(|S|) and the base line N(8.5) for various |S| \u2208 [4\u2212 9]. The ratio is independent of \u03b2 and equal to C(|S|)\u00d7\u03b3(8.5\u2212|S|). The lower the ratio, the better the performance as less iterations are required. The main finding of the experiment is the ability to use discriminating sets with lower discriminating probability, compensated by a larger number of iterations to achieve the same hit rate levels while having lower run-time. An example from Fig. 9 is the preferable use of |S|=5 for \u03b3 \u2264 0.5 and |S|=6 for \u03b3 \u2265 0.6 over setting |S|=8.\nExpt. V: Run-time, Number of Iterations and Hit Rate\nTheorem 2 states that for any given discriminating set with a discriminating probability of 0.5 (see Theorem 1) and for N \u2265 2 ln 2/(\u03b2\u03b3|S|) trials, we are guaranteed to find a factor 2 optimal fuzzy lagged co-cluster with a probability of at least 0.5. We report on the actual performance of the algorithm in mining an optimal fuzzy lagged co-cluster in terms of those three parameters.\nThe experiment was conducted by creating a random matrix of size [100\u00d7100] and randomly placing random fuzzy lagged co-clusters of varying sizes (i.e., \u03b2, \u03b3 \u2208 {0.3, 0.5, 0.8}) overriding the original values. To obtain sets with a discriminating probability of 0.5, we set |S|=5 with a discriminating probability\nof 40.8% (see Expt. III). While repeating the execution of the algorithm 10,000 times for each cluster size \u03b2, \u03b3 \u2208 {0.3, 0.5, 0.8}, we counted:\n(1) hit rate: how many times out of the 10,000 repetitions the algorithm managed to mine the planted cluster; (2) iterations: how many iterations it took in practice to mine the optimal cluster; and (3) run-time: how long (in minutes) it took to mine the optimal cluster. The experiment was conducted using the platform: Intel core i7 @ 2.00GHz CPU with 6GB RAM, Windows 7 64 bit. The algorithm was programmed in Java 7.0. The results obtained are as follows.\n\u2013 Hit Rate: An actual average hit rate of 44.0%, higher than the expected hit rate of 43.2% for a discriminating set with a discriminating probability of 40.8%.6\n\u2013 Number of Iterations: Fig. 10 presents the actual number of iterations needed to mine the optimal fuzzy lagged co-cluster in relation to the theoretical boundary. On average, the actual number of iterations needed is 49% of the specified theoretical bound.\n\u2013 Run-time: The run-time boundary, as specified by Subsection 3.2 is: t = O((mn)2/\u03b3|S|). Fitting the actual run-time to an equation of type: t=c/(\u03b2x\u03b3y) (t in ms), where c = (mn)2, we obtain: x = 0.83, y = 4.49 and c = 107.2. As expected, the power of \u03b2 is close to 1 and the power of \u03b3 is close to 5 (we set |S|=5). These results are better than the theoretical bound due to \u03b2, \u03b3 \u2264 1.0.\nTo summarize, on our test set the FLC algorithm manages to mine fuzzy lagged co-clusters with a hit rate higher than the expected hit rate, less than 50% of the needed theoretical number of iterations, and does so within a feasible run-time.\n6 Following Expt. IV formula of: hit rate = 1\u22120.25p, a discriminating probability of p=40.8%, results in an expected hit rate of 43.2%.\nExpt. VI: The Effect of Error and Fuzziness\nThe main objective of the previous experiments was to demonstrate properties of the fuzzy lagged co-clustering model while focusing on the correctness of the theoretical bounds of Algorithm 1. In this experiment, we wish to examine the extent of changes in the mining results as a function of the error and fuzziness used. To do so, we planted random fuzzy lagged co-clusters of specific error and fuzziness and set the miner\u2019s parameters of error and fuzziness to various values.\nTo measure how well the miner performed in each setting, we used the complement of the RNIA score [59], defined as follows. Let C1 and C2 be fuzzy lagged co-clusters. RNIA(C1, C2) = (|U | \u2212 |I|)/|U |, where U and I are the matrix elements in the union and intersection of C1 and C2, respectively. Hence, 1\u2212RNIA(C1, C2)=|I|/|U |, achieves a score of 1 when C1 and C2 are equal, and a score of 0 when completely disjoint.\nFig. 11 depicts the mining performance (measured in terms of: 1\u2212RNIA) of four different combinations of error and fuzziness of the planted clusters (\u03b5\u2208 {0.01%, 0.1%}, F \u2208{2, 4}) as a function of the miner\u2019s configuration of error and fuzziness. Obviously the cluster in each configuration can be mined only if the fuzziness used by the miner is greater than or equal to the maximal fuzziness allowed when constructing the planted cluster. Therefore the miner\u2019s fuzziness configuration was set to F \u2265 2 in the case where the planted cluster was of maximum fuzziness of 2 and F \u2265 4 in the case of maximum fuzziness of 4. The\nfigure presents the average score over 100 trials for each of the four combinations of the planted clusters\u2019 parameters and for each configuration of the miner. The graphs demonstrate that, as expected, the best performances are achieved when the miner is set to the fuzziness of the planted cluster and to an error within the surrounding of the planted cluster. In addition, the higher the error or fuzziness to which the miner is set, the lower the performance achieved. This is due to increasing noise being added to the mined clusters. The charts in Fig. 11 are significant as they establish the importance of introducing fuzziness into the lagged-pattern model."}, {"heading": "4.2. Experiments with Flight of Pigeon Flocks", "text": "In a second set of experiments, we examined the capability of the FLC algorithm to mine clusters from real-life data. One key goal for these experiments was to demonstrate the extent of improvement achieved in terms of mining coherency when transitioning from the lagged model to the fuzzy lagged model. For that purpose, we used two real-life datasets containing GPS readings7 of the flight of pigeon flocks [56] (see a snapshot in Fig. 12): (1) homing flight data, consisting of four different datasets recording the flights of pigeons from point A to point B; and (2) free flight data, consisting of 11 different datasets recording the flights of pigeons around the home loft, i.e., flight from point A back to point A. Each dataset (four of homing flight and 11 of free flight) represents a different flock release, containing an average of nine individuals.\nGenerally speaking, a flock\u2019s flight formation is a lagged pattern where the lag is the distance between the fliers. Nevertheless, clustering the pigeons accord-\n7 Of the GPS readings, only the x and y coordinates were used. This is due to the error of the z-coordinate which is much larger than those of the horizontal directions [56].\ning to their flock membership is not a trivial task. The trajectories of the flock members depend on multiple parameters: flier (e.g., physical ability, navigation capabilities, leader-follower relationships, threats) and weather conditions (e.g., wind streams, temperature). Many of these parameters change dramatically over time and space. The data are inherently noisy due to human error and equipment inaccuracy (e.g., GPS errors/inaccuracies/distortion, loss of signal, device failure). A further complication in the dataset is that flight trajectories are spatially close and highly interleaved. This is caused by the fact that flocks were all released (at different times) from a similar location heading to the same destination. For example, the homing flight pigeons followed the Danube river for about 15km until reaching their loft. This lack of spatial differentiation imposes a great mining challenge, especially to density-based algorithms, which might mistakenly merge trajectories that belong to different flocks. Therefore, mining such datasets for fuzzy lagged co-clusters is highly complex.\nIn the following experiments, we consider a cluster to be accurate if all the participating pigeons belong to the same flock. We note that it is unlikely to mine a cluster containing all pigeons in the flock as it is fairly common for pigeons to deviate from their flock for a substantial period of the flight (e.g., during flight no.3, two birds broke away from the group soon after release). Thus, such deviating pigeons cannot be accurately clustered."}, {"heading": "4.2.1. Mixed Datasets: Homing Flight and Free Flight", "text": "The goal of the experiment is to test the error and fuzziness impact on the precision and recall of the mined clusters. To do so, we use a dataset compiled from mixed pairs of a homing flight and a free flight dataset (we compile 44 different pairs of datasets which are the result of four homing flights and 11 free flights dataset combinations, see example in Fig. 12b). We ran the FLC algorithm on each pair of datasets with various errors \u03b5\u2208[0.005%\u20130.5%] and fuzziness F\u2208[0\u201310] combinations, recording the F1 score8 of the mined clusters. In addition, we ran the DBSCAN algorithm [20] in order to compare its results to the FLC algorithm. The DBSCAN algorithm has two main parameters: (1) distance, denoted Eps, which represents the maximum neighborhood of a point; and (2) density, denoted MinPts, which represents the minimum number of points within the neighborhood of a point. The DBSCAN algorithm was run on each pair of datasets, with various combinations of distance Eps\u2208[0.001\u201310000] and density MinPts\u2208[2\u201310000], recording the F1 score of the mined clusters.\nFig. 13a and 13c depict the F1 score of the FLC algorithm as a function of the error (\u03b5) and fuzziness (F ), respectively. F=0 is in fact the case of mining lagged clusters with no fuzziness. As expected, any increase in \u03b5 or F results in an increase in the F1 score as more data points are reachable from the cluster\u2019s seed. An important finding obtained from the figures is that for relatively low errors, a significant increase in the F1 score is recorded when fuzziness is used. For example, for \u03b5=0.005%, we obtain for F={0, 1, 3, 5, 10} a score of F1={0.024, 0.164, 0.259, 0.295, 0.458}, which reflects an increase of a factor of \u00d7={1, 7, 10, 12, 19}, respectively. Figures 13b and 13d depict the equivalent performance of DBSCAN for the same settings (i.e., F1 score as a function of\n8 F1 score (also known as F-measure) is defined as: F1= 2 \u00b7 (precision \u00b7 recall)/(precision+ recall) [77]. In terms of Type-I and type-II errors: F1= (2 \u00b7 true positives)/(2 \u00b7 true positives+ false negatives+ false positives).\nthe distance (Eps) and density (MinPts), respectively) used for generating figures 13a and 13c. Comparison of the FLC and DBSCAN algorithms reveals the stability of setting the FLC parameters vs. the sensitivity of configuring the DBSCAN parameters (surveyed in [12, 32]). In addition, even when considering the best configuration for the DBSCAN algorithm, the FLC algorithm still outperforms the best F1 score achieved with DBSCAN. The comparison of Fig. 13a and Fig. 13b reveals the difference in the error (distance) behavior between the FLC and DBSCAN algorithms, respectively (note: the DBSCAN uses an Euclidean distance measure (L2 norm), while the FLC uses the Manhattan distance measure (L1 norm)). While the FLC algorithm maintains a high F1 score as the error increases, the DBSCAN algorithm results in mining futile clusters of F1=0.66 containing the entire dataset (the datasets used contain two classes with an equal number of members. Therefore, a cluster containing the entire dataset, will have a recall=1.0, precision=0.5 and thus F1=0.66). The comparison of Fig. 13c and Fig. 13d reveals that while the fuzziness parameter of the FLC algorithm steadily increases the achieved F1 score, the effect of the density parameter of the DBSCAN algorithm is not conclusive and depends on the adjacent error value.\nThe latter finding strengthens the importance and necessity of the fuzzy lagged model. The mining of non-fuzzy lagged co-clusters achieves a substantially\nlower F1 score in comparison to the mining of fuzzy lagged co-clusters. Indeed, an increase in the F1 measure can also be achieved by increasing the error \u03b5; however, this is dangerous as any increase in \u03b5 substantially increases the chance of mining artifacts. In addition, the increase in error may not always achieve a high F1 score. For example, datasets with large spatial distances between the data points would require an error so large that it might cover the entire dataset, which in turn results in futile clusters. On the other hand, the use of fuzziness as part of the model, enables mining accurate and coherent clusters without increasing the allowable error. In addition, as illustrated in Fig. 13, a score close to 1 for F1 can be obtained even when considering moderate fuzziness."}, {"heading": "4.2.2. Homing Flight Dataset", "text": "In order to examine the algorithm\u2019s ability to properly classify each pigeon to its flock, we merged all four homing flight datasets resulting in a matrix of size [37\u00d7 13892], comprising 13892 GPS readings of 37 pigeons.\nTo demonstrate the difficulty of clustering the dataset into flocks (i.e., specifying how many flocks are present) we conducted the following experiment. We asked 25 people, of differing sex, age, occupation and nationality, to specify how many flocks they could identify in the dataset. For that purpose, we enabled them to use Google Earth to view the pigeons\u2019 trajectories (see Fig. 14 for sample snapshots of the user view). The subjects could use all functionalities within Google Earth (e.g., view the trajectories from different angles, enlarge, and so on), and were given 5-minute to reach an answer. The average answer was 6.5, with a standard deviation of 3.2. Only 16% of the subjects gave the correct answer (i.e., 4 flocks). The results indicate that the dataset cannot be trivially mined.\nTo examine how many flocks the FLC algorithm would specify, we conducted the following experiment. We ran the FLC algorithm with an error set to \u03b5=0.005% (following Fig. 13c, this error setting would enable the examination of the fuzziness effect) and various values of fuzziness F\u2208{0, 1, 2, 3, 5, 10, 15, 20, 30, 40} for 20,000 trials. In addition, we wished to examine whether the use of partial knowledge (i.e., partial dataset), which necessarily reduces the overall mining run-time, would preserve the quality of the mining results. To do so, each\nof the above settings was run on various datasets comprising 4%, 7%, 11%, 14% and 100% of the dataset\u2019s columns (500, 1000, 1500, 2000 and 13892 of dataset\u2019s columns, respectively). Fig. 15 depicts the accuracy of the mined fuzzy lagged co-clusters. Fig. 15a presents the entropy of the mined clusters as a function of the fuzziness F and the number of columns. The entropy of a cluster C is\ncomputed as: H(C) = \u2212 \u2211k i=1 p(i|C) \u00b7 log(p(i|C)) for k class labels in cluster C.9 Ideally, a cluster C should contain objects of only one class and thus, have a zero entropy. For a set of clusters, we take the average entropy weighted by the number of objects per cluster. For readability, we normalize the entropy to the range of 0% to 100% by dividing by the maximum entropy, i.e., H(C)/ log(k) [6, 69]. Fig. 15b presents the percentage of inter-flock clusters, i.e., clusters which contain pigeons from different flocks, as a function of the fuzziness F and the number of columns. As the results demonstrate, due to the interleaving of the pigeons\u2019 trajectories, using a small number of columns does not supply enough data to discriminate between pigeons belonging to different flocks. The probability of an inter-flock cluster for small number of columns (i.e., less than 1000, which is 7% of the dataset columns) is high. On the other hand, when using a large enough number of columns (i.e., more than 10% of the dataset columns) the probability of mining an inter-flock cluster is insignificant, even with respect to a growing fuzziness. Although we do not claim the generality of this approach, this latter finding is important in the aspect of run-time, as also the use of a partial dataset yields promising results. Moreover, we can use a post-process procedure which merges clusters that share common objects (i.e., clusters that had at least one pigeon in common were merged). Thus, with a high probability, the merged clusters will represent the different flocks. Fig. 16 presents the number of flocks (as yielded by the post-process stage) as a function of the fuzziness F and the number of columns. We notice that for F=0 (i.e., using the lagged co-clustering model) we do not obtain the correct answer (i.e., four), regardless of the number of columns being used. On the other hand, when using the fuzzy lagged\n9 Due to the fact that classes are generally of the same size (membership-wise), no problem of imbalanced biasing arises.\nco-clustering model, we quickly converge to the correct result, as the value of F increases. Furthermore, we observe a quick convergence to the correct answer as the number of columns increases (e.g., for 13892 columns, we already obtain the correct result when using F=2). As observed from the figure, the use of fuzziness led to the correct finding (four flocks) regardless of the number of columns used. Both the number of columns and the value of F have a positive effect on the speed of convergence to the correct finding. In particular, with a large number of columns, only a very small level of fuzziness needs to be considered. In contrast, using the non-fuzzy model yielded on average only one group (one flock). As we next show, this is due to poor mining results as reflected by the coverage of F=0 in Fig. 17. Worth mentioning in this context is that human subjects gave an answer of (on average) 6.5 flocks.\nA by-product of the post-process merging stage is the actual flock coverage, i.e., how many of the flock members have been covered by the mined clusters. Fig. 17 depicts the average flock coverage (over the numbers of columns\u2208{1500, 2000, 13892}) for the homing flight dataset as a function of the fuzziness F . The results show a significant improvement in the accuracy and completeness of the mining process when using the fuzzy model (F\u22651) in comparison to the non-fuzzy model (F=0). Even the use of a fuzziness of a single column (i.e., F=1) has a notable impact of \u00d75 on the coverage. The use of high F , provides coverage of \u223c90% of the flocks\u2019 members.\nTo compare the performance of the FLC algorithm, we ran the DBSCAN algorithm [20] with various combinations of distance Eps\u2208[0.0001\u201310000] and density MinPts\u2208[2\u2013500000]. Fig. 18 depicts the number of flocks as obtained by the DBSCAN algorithm (as yielded by the post-process stage) as a function of the distance and density used. For the vast majority of the settings, the DBSCAN algorithm either does not find any clusters (upper right portion \u2013 red color) or\nthe clusters that it does find contain the entire dataset (lower left portion \u2013 blue color) and are therefore futile. Only for a negligible number of settings does the DBSCAN algorithm manage to report three flocks, which even then is not the correct answer of four. We note that the DBSCAN algorithm results as presented in Fig. 18 are based on a post-process merging stage, as the results without such a stage were considerably inferior. On the same range of settings as in Fig. 18, the obtained average number of flocks without the post-processing stage was 87 with a standard deviation of 463, where less than 0.5% of the settings yield the correct answer of 4.\nTo summarize, mining this dataset is not a trivial task. This is evidenced by the poor classification results of the human subjects, the DBSCAN algorithm and the non-fuzzy lagged method. Furthermore, when using a non-fuzzy mining model, the obtained results are characterized by a low F1 score and low coverage. On the other hand, the FLC algorithm performed well in mining fuzzy lagged co-clusters on various trajectories. It achieved a high F1 score and high coverage, doing so with only a small number of artifact (inter-flock) clusters. Although prior domain knowledge can be useful in configuring the miner\u2019s parameters, such knowledge is not mandatory. Subsection 3.3 provides default values for setting the parameters |S| and N (see Theorem 1 and 2, respectively). In order to choose an appropriate value of error, one can adopt any of the methods suggested for the non-fuzzy lagged co-clustering model [70], e.g., gradual increase, starting from a relatively small error. Setting the minimum cluster dimensions (i.e., \u03b2 and \u03b3) to relatively small values would suffice for mining accurate clusters (e.g., \u03b2=2 and \u03b3=10% as depicted by Fig. 15). When setting the fuzziness, the use of small values (e.g., F=1,2 as depicted by Fig. 16 and 17) already effects a considerable improvement in the clustering results over the non-fuzzy ones. In conclusion, the significant improvement in mining presented by the FLC algorithm, in comparison to the non-fuzzy algorithm, demonstrates the importance of including the fuzzy aspect in the model. The FLC algorithm can thus be used as a classifier in this domain."}, {"heading": "5. Related Work", "text": "With the vast amount of routinely collected data, the need for clustering as a mining tool emerges in many fields: biology, physics, economics and computer science are but a short list of domains with a wealth of research in this direction [36, 47]. A typical mining problem is the extraction of patterns from a dataset, where the rows represent objects, the columns represent attributes and the data entries are the measurements of the objects over the attributes [36, 40].\nSimple mining techniques look for a fully dimensional cluster: a subset of the objects over all attributes (or vice versa) [19, 40, 64, 71]. These techniques have several inherent vulnerabilities, e.g., difficulty in handling the common presence of irrelevant, noisy or missing attributes and inaccuracy due to the \u201ccurse of dimensionality\u201d [10, 13, 45, 54, 72]. All these may be counter-productive as they increase background noise [40, 52].\nCheng and Church [17], in their seminal work in the field of gene expression data, introduced a mining technique which focus on mining biclusters (also known as co-clusters or co-regulations): a subset of the objects over a subset of the attributes. Their approach was followed by many researchers (see surveys by [12, 40, 45, 52, 54, 75]), using various models (additive vs. multiplicity, axis alignment, rows over columns preferment, cluster scoring function, overlapping, etc.), and applying various algorithmic strategies: greedy [7, 17], divide-and-conquer [33], projected clustering [49, 65], exhaustive enumeration [74], spectral analysis [43, 73], CTWC [22], bayesian networks [9], etc.\nDue to the importance of datasets having a temporal nature (i.e., sequences of time series), specific efforts have been directed at utilizing the continuous nature of time as a natural order [8, 38, 39, 55], surveyed by [67, 81]. In particular, some have considered the delay (lag) between the object\u2019s behaviors and suggested different approaches for mining lagged co-clusters. These include\ndynamic-programming and hierarchical-merging (with pruning) [79, 85], polynomial time Monte-Carlo strategies to mine lagged co-clusters which encompass the optimal lagged co-cluster [70], and the reduction to some finite alphabet [27]. Despite the efficiency of these approaches in mining lagged co-clusters, they become ineffective when the lagged pattern is fuzzy. This is due to their underlying assumption of non-noisy (i.e., fixed) lags.\nExisting methods that are inherently designed for mining fuzzy lagged coclusters can be categorized into three types, each imposing a different design limitation. The first is a group of methods designed for mining pairs of sequences using variants of the edit distance measures, such as the Longest Common Sub Sequence (LCSS) measure [78] and others [15, 16, 84], surveyed in [35]. However, post-processing merging requires a combinatorial solution which is both time consuming and heavily dependent on the closeness of the merit function [40, 71]. Furthermore, pairs of objects might lack the transitivity characteristic, e.g., two stocks may appear to be correlated, while in fact the correlation is to the index which dominates them in volatile trading days [71]. Investments based on this in nonvolatile times may lead to poor results. Moreover, pairs might simply be merged as a mere aggregation of noise and not due to some hidden regulatory mechanism [13, 70]. The second type uses a space reduction approach to some finite alphabet [2, 23, 26, 37, 61, 62, 63], e.g., each trajectory coordinate is approximated to a grid cell. The main limitation of such methods is the reduction magnitude. On one hand, coarse abstraction using a small alphabet may lead to greater errors and finer clusters being missed. On the other hand, using a large alphabet will have a dramatic influence on the run-time as it is exponentially dependent on the alphabet size. Finally, there are methods that assume sequentiality of the cluster\u2019s columns, e.g., flock mining [11, 48].\nA popular technique for mining clusters of trajectories is the density-based approach. This approach uses the spatial closeness between data points to associate them into clusters [1, 31, 34, 61]. A well known representative of this technique is the DBSCAN algorithm [20] (followed by derivative algorithms of a non-temporal [3, 51, 57, 58, 76, 86] and temporal [14, 42, 48, 68, 76] nature, surveyed in [12, 32]). The disadvantage of the density-based approach is its sensitivity to noise (e.g., signal distortion), outliers (e.g., erroneous GPS measurements), missing values (in real-life, devices may be voluntarily disconnected by their owners, or be subject to machine failures or lost signal), related objects which are spatially distant (e.g., a roaming group where members are far apart from each other) and crossing trajectories of unrelated objects (e.g., trajectories of different groups interleave). These algorithms will find the data used in Subsection 4.2 challenging as trajectories of different groups interleave. This may cause clusters to contain inter-group trajectories, and thus, fail classification.\nWe note that all the works cited above were also unable to find substantial previous reference to the fuzzy lagged co-clustering problem and that state-ofthe art algorithms for this problem are either non-fuzzy [70, 79] or limited to data points which are spatially close [20].\nOn the application side, the last few years have witnessed an increasing interest in fuzzy lagged co-clustering. This is attributed to the dramatic increase in location-aware devices (e.g., cellular, GPS, RFID). Such devices leave behind spatio-temporal electronic trails. A dataset of such trajectories is of a fuzzy lagged nature. Commercial services such as Foursquare, Google Latitude, Microsoft GeoLife, and Facebook Places, use such data to maintain location-based social networks (LBSN), later used for personal marketing purposes. Another\nuse of such data is to extract patterns (see surveys in [4, 32, 44]) which may suggest Places Of Interest (POI) [24, 26, 41, 42, 58]. This is mostly used for the purposes of tourism [5, 24, 25, 26, 58], urban planning [24, 26, 30], crowd control [26, 30], traffic management [30, 58, 62, 63] and behavioral sciences [21, 46, 80]."}, {"heading": "6. Discussion, Conclusions and Future Work", "text": "The importance of clustering is unquestionable and has been thoroughly discussed and demonstrated in cited prior work. Similarly, the extensive co-clustering literature includes many examples of the benefit of mining co-clusters as opposed to traditional approaches. The fuzzy lagged co-cluster model generalizes the lagged co-cluster model, enabling the inclusion of an additional important dimension, a fuzzy aspect, in the regulatory paradigm. The results reported in the previous section not only corroborate the algorithm\u2019s ability to efficiently mine relevant and accurate fuzzy lagged co-clusters, but also illustrate the importance of including fuzziness in the lagged-pattern model. With the fuzziness dimension, a significant improvement is achieved in both coverage and F1 measures in comparison to using the regular lagged co-clustering model. One important strength of the new model relates to the chance of mining artifacts. In order to enlarge the dimensions of the mined clusters, traditional non-fuzzy methods tend to increase the error which in turn increases the risk of mining artifacts. The fuzzy model provides the user with the capability of keeping a low level of error, while improving the achieved performance, without introducing artifacts.\nAs proved in Subsection 2.1, the complexity of mining fuzzy lagged co-clusters is NP-complete for most interesting optimality measures. Thus the importance of the algorithm presented in this paper lies in promising the probability of mining an optimal fuzzy lagged co-cluster and a theoretical bound to the polynomial number of iterations it will take. In addition, the algorithm demonstrates a set of important capabilities such as handling noise, missing values, anti-correlations and overlapping patterns. Moreover, even if lagged clusters with no fuzziness at all need to be mined, the FLC algorithm has a better run-time in comparison to former algorithms inherently designed for such cases [79, 85] (including the Monte-Carlo based algorithms [70]). It is notable that due to the Monte-Carlo nature of the FLC algorithm, its iterations (and therefore, the mined clusters) are independent of each other. The algorithm can thus be implemented to take advantage of parallel computing or special hardware in a straightforward manner.\nThe experiments using an artificial environment (reported in the previous section) reveal actual performance which is far better, in terms of accuracy and efficiency, than the theoretical bounds. In addition, they supply default values for the various configurable parameters of the algorithm, releasing the user from this burden. When used on real-life datasets, the FLC algorithm was demonstrated to mine precise, coherent and relevant fuzzy lagged co-clusters in a practicable runtime and with almost no artifacts. This is in contrast to inferior results obtained by using a non-fuzzy model and despite the fact the datasets were large, highly noisy, contained many missing values, and were rich in overlapping clusters. In addition, the FLC algorithm presented classification capabilities which were superior to the ones presented by the non-fuzzy lagged model, those of human subjects and to the DBSCAN algorithm. This encouraging result is important in the sense of model validation and suggests great potential for mining fuzzy lagged co-clusters in many other fields of science, business, technology and medicine.\nAs in the non-fuzzy lagged model, the ability of the FLC algorithm to mine lagged co-clusters offers important functionalities such as forecasting. However, when mining fuzzy lagged co-clusters, one may have to choose between possibly intersecting columns (see Subsection 3.4.3). One can utilize the intersecting mechanism to place weights on the matrix columns so as to enable the mining of more \u201crecent\u201d clusters. It is reasonable to assume that the latest (up-to-date) columns will contribute more to the accuracy of the forecast than old and possibly irrelevant ones. We believe there is far more that can be developed in this aspect in terms of future research."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "The paper focuses on mining patterns that are characterized by a fuzzy lagged relationship between the data objects forming them. Such a regulatory mechanism is quite common in real-life settings. It appears in a variety of fields: finance, gene expression, neuroscience, crowds and collective movements, are but a limited list of examples. Mining such patterns not only helps in understanding the relationship between objects in the domain, but assists in forecasting their future behavior. For most interesting variants of this problem, finding an optimal fuzzy lagged co-cluster is an NP-complete problem. We present a polynomial-time Monte-Carlo approximation algorithm for mining fuzzy lagged co-clusters. We prove that for any data matrix, the algorithm mines a fuzzy lagged co-cluster with fixed probability, which encompasses the optimal fuzzy lagged co-cluster by a maximum 2 ratio columns overhead and completely no rows overhead. Moreover, the algorithm handles noise, anti-correlations, missing values and overlapping patterns. The algorithm was extensively evaluated using both artificial and real-life datasets. The results not only corroborate the ability of the algorithm to efficiently mine relevant and accurate fuzzy lagged co-clusters, but also illustrate the importance of including fuzziness in the lagged-pattern model.", "creator": "LaTeX with hyperref package"}}}