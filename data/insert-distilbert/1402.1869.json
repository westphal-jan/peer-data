{"id": "1402.1869", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2014", "title": "On the Number of Linear Regions of Deep Neural Networks", "abstract": "we study directly the complexity functional of functions computable by deep feedforward neural networks with piece - wise linear activations in terms of the number of regions deprived of linearity that they have. suppose deep networks are able to sequentially map simpler portions of each layer's input space to the same minimal output. in this first way, deep models compute functions modeled with a compositional structure that he is able to re - use pieces of computation exponentially often in terms of their depth. adding this note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece - wise linear activation functions.", "histories": [["v1", "Sat, 8 Feb 2014 17:16:27 GMT  (406kb,D)", "http://arxiv.org/abs/1402.1869v1", "12 pages"], ["v2", "Sat, 7 Jun 2014 19:56:14 GMT  (3672kb,D)", "http://arxiv.org/abs/1402.1869v2", null]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NE", "authors": ["guido f mont\u00fafar", "razvan pascanu", "kyunghyun cho", "yoshua bengio"], "accepted": true, "id": "1402.1869"}, "pdf": {"name": "1402.1869.pdf", "metadata": {"source": "CRF", "title": "On the Number of Linear Regions of Deep Neural Networks", "authors": ["Guido Mont\u00fafar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio", "PASCANU CHO BENGIO"], "emails": ["MONTUFAR@MIS.MPG.DE", "PASCANUR@IRO.UMONTREA.CA", "KYUNGHYUN.CHO@AALTO.FI", "YOSHUA.BENGIO@UMONTREA.CA"], "sections": [{"heading": "1. Introduction", "text": "Artificial neural networks with several hidden layers, called deep neural networks, have become popular due to their unprecedented success in a variety of machine learning tasks (see, e.g., Krizhevsky et al., 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012). In view of this empirical evidence, deep neural networks are becoming increasingly favoured over shallow networks, and are often implemented with more than five layers. At the time being, however, only a limited amount of publications have investigated deep networks from a theoretical perspective.\nRecently, Delalleau and Bengio (2011) showed that a shallow network (i.e., with a single hidden layer) requires exponentially many more sum-product hidden units than a deep sum-product network in order to compute certain families of polynomials. We are interested in extending this kind of analysis to more popular neural networks composed of layers of nonlinear computational units. A well-known result due to Hornik et al. (1989) states that a feedforward neural network with a single hidden layer is a universal approximator (of Borel measurable functions). Other works have investigated universal approximation of probability distributions by deep belief networks (Le Roux and Bengio, 2010; Montu\u0301far and Ay, 2011), as well as their approximation properties (Montu\u0301far, 2013; Krause et al., 2013).\nMore recently, Pascanu et al. (2013) reported a theoretical result on the complexity of functions computable by deep feedforward networks with piecewise linear units called rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), which have an activation function of the form g(x) = max{0,x}. They showed that in the asymptotic limit of many hidden layers, deep networks are\nc\u00a9 2014 G. Montu\u0301far, R. Pascanu, K. Cho & Y. Bengio.\nar X\niv :1\n40 2.\n18 69\nv1 [\nst at\n.M L\n] 8\n2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.02.0\n1.5\n1.0\n0.5\n0.0\nable to separate their input space into exponentially more regions of linearity than their shallow counterparts, using the same number of computational units.\nIn this paper, we follow the ideas of Pascanu et al. (2013) to analyze deep models, but we take a broader perspective. We show that deep models are able to sequentially map pieces of their inputs into the same output and in that way, the layer-wise composition of functions that they compute re-uses low-level computations exponentially often as the number of layers increases. Our analysis and results apply to a variety of deep networks, including deep rectifier networks. We estimate the number of regions of linearity of functions computable by deep neural networks for two important cases: with rectifier units and maxout units (Goodfellow et al., 2013). Our bound for the complexity of deep rectifier networks improves the previous bound obtained by Pascanu et al. (2013).\nThe number of linear regions of functions computed by a deep model is a measure of flexibility for the model. An example of this and of the advantage of depth is given in Fig. 1, which shows the learnt classification boundary of a single-layer model versus a two-layer one with the same number of hidden units. To minimize the effect of optimization, the models are trained with a variant of natural gradient and the plotted data is the best of several runs. The shallow model obtains an error of 123 misclassified examples, the deep model does only 24 errors.The two layer model is better able to capture a sinusoidal decision boundary by defining more linear segments.\nAs noted earlier, deep networks are able to identify an exponential number of input neighborhoods whose image through the model function has a non-empty intersection. This results in an exponential replication of features over their input and has the effect that even when these networks are defined with relatively few parameters, they are able to compute very complex functions, in terms of the number of regions of linearity that they have. The number of parameters is an upper bound for the dimension of the set of functions computable by a network, and a small number of parameters means that the class of computable functions has a low dimension. The set of functions computable by a deep feedforward piece-wise linear network, although low dimensional, achieves exponential complexity by re-using and composing features from layer to layer.\nThis paper is organized as follows. In Section 2 we discuss the complexity of functions computable by deep neural networks in terms of their number of regions of linearity. We show that deep\nmodels achieve high complexities by sequential compositions of features. In Section 3 we discuss the special case of deep networks with rectifier units and in Section 4 networks with maxout units."}, {"heading": "2. Complexity of Feedforward Networks and Their Compositional Properties", "text": "In this section we discuss the ability of deep feedforward networks to remap their input space in a complicated way by using only relatively few computational units. The key observation of our analysis is that each layer of a deep model is able to identify regions of its input that should be mapped to a common output. This leads to a compositional structure, where computations on higher layers are effectively duplicated in the identified regions in the input space. The capacity to replicate computations over the input space grows exponentially in the number of layers of the network.\nBefore expanding these ideas we introduce basic definitions that will be needed in the rest of the paper. At the end of this section we will give an intuitive perspective for reasoning about the replication capacity of deep models."}, {"heading": "2.1. Definitions", "text": "A feedforward neural network is a composition of computational units which define a function of the form\nu(x | \u03b8) = \u03c6 ( U>\u03c6L ( W>L \u00b7 \u00b7 \u00b7\u03c61 ( W>1 x ) \u00b7 \u00b7 \u00b7 )) ,\nwhere Wl are the weights and \u03c6l the nonlinear activation function of the l-th hidden layer, and W1 and U are the weights associated with the input and output, respectively. The bias term at each layer was dropped for the sake of clarity. Here, the output of a layer \u03c6l(x) = [g1(x), \u00b7 \u00b7 \u00b7 , gNl(x)]> has as coordinates the activations gi(x) of the units i \u2208 [Nl] in the layer. Given an input x from the preceding layer, each hidden unit computes a function\ng(v>x + b),\nwhere v and b are input and bias weights of the unit. We are mainly interested in piecewise linear activations, and will consider the following two important types.\n\u2022 Rectifier: g(x) = max {0,x}, x \u2208 Rn.\n\u2022 Maxout of rank k: gi(x) = (max{x(k\u22121)\u00b7i+1, . . . ,xk\u00b7i})i, where x \u2208 Rn and we use subscripts to pick entries of this vector. See more details in Definition 5.\nThe structure of a neural network refers to its arrangement of units. This is specified by n0, which is the number of dimensions in the input space; nl, which is the number of hidden units at layer l for each l from 1 to L, which is the number of layers.\nWe will measure the complexity of a piecewise linear function in terms of the number of regions of linearity that it has. A region of linearity of a function f : Rn \u2192 Rm is a maximal connected subset of Rn on which f is linear."}, {"heading": "2.2. Shallow Neural Networks", "text": "Assuming that the activation function g has a distinguished point at the origin of its domain, the combinatorics of the function computed by a layer of units can be described in terms of the hyperplanes Wi,:x + bi = 0.\nA region of a hyperplane arrangement is a connected component of the complement of the hyperplanes. The number of regions generated by a hyperplane arrangement can be given in terms of a characteristic function, which is a well known result by Zaslavsky (1975). We will only use the fact that an arrangement of n1 hyperplanes in Rn0 produces up to \u2211n0 j=0 ( n1 j ) regions. This number of regions is attained when the hyperplanes are in general position. This number is also the maximal number of regions of linearity of functions computable by a shallow rectifier network with n0 inputs and a single hidden layer of width n1 (see Pascanu et al., 2013, Proposition 5)."}, {"heading": "2.3. Deep Neural Networks", "text": "We build our analysis of deep models on the following concept of identified input regions.\nDefinition 1 A map g between continuous spaces identifies two input neighborhoods S and T if g(S) = g(T ). In this case we also say that S and T are identified by g.\nWithin this definition, for instance, the four quadrants of 2\u2013D Euclidean space are identified by the following coordinate-wise absolute value function g : R2 \u2192 R2;\ng(x1, x2) = [ |x1| |x2| ] . (1)\nBy identifying pieces of the input space of a neural network, each subsequent layer computation can be focused on a single output region, effectively acting on many inputs that have been identified by the previous layer. One can define the subsequent layers of the network in such a way that any of their computations concentrate on that single output region, thus replicating that complex computation on all identified inputs and generating an overall complicated-looking function.\nEach hidden layer j computes a function gj on the image of the preceding layer. We denote the image of input space through such a function gj by gj(S) and denote the set of regions of linearity\nof gj on its input domain gj\u22121(S) by R\u0304j . The regions of linearity of gj replicate on all identified subsets of the domain of the previous layer gj\u22122(S). See Fig. 2 for an illustration of this iterative replication.\nThe number of separate regions in the input space for each separate regionR in the output space can be expressed recursively as\nN jR = \u2211 R\u2032\u2208P\u0304 jR N j\u22121R\u2032 , N 1R = 1, for each region R \u2208 R\u03041, (2)\nwhere P\u0304 jR is the set of regions of gj\u22121(S) identified by gj such that their image contains R . For example, P\u0304 1R is the set of all input regions that are identified by the first layer such that their image through the function g1 (which represents this first layer) contains the region R.\nLet \u03b7j be the function that computes the activations of the hidden layer j starting from the input. Namely \u03b7j = gj \u25e6 gj\u22121 \u25e6 . . . \u25e6 g1. N jR \u2208 N is the number of regions in the input space that are identifiable with respect to the function \u03b7j such that the image of these identifiable regions through \u03b7j contains the region R.\nThis effectively builds a tree rooted at the region in the output and counts recursively the number of leaf nodes (see Fig. 2). In other words, Eq. (2) counts the number of times that each separate region in the output space is copied in the input space.\nBased on the recursion from Eq. (2), one can estimate the number of input space regions as follows.\nTheorem 2 (Maximal Number of Regions for Rectifier Networks) The maximal number of regions of linearity of the functions computed by a neural network with n0 input variables and L hidden layers of ni rectifiers for i \u2208 [L] is at least\nN = \u2211 R\u2208R\u0304L NLR ,\nwhere NLR is defined by Eq. (2) and the maximal cardinality of R\u0304L is given by \u2211n0 j=0 ( nL j ) .\nProof The equation for the maximal cardinality of R\u0304L is a consequence of Zaslavsky\u2019s theorem (see, e.g., Pascanu et al., 2013, Proposition 5). Each output region R is replicated within all the\nidentifiable regions of the input whose image through \u03b7L contains R. This value is given by NLR (see the definition in Eq.(2)). Therefore a lower bound on the maximal number of regions is given by summing up how many times each output region is replicated in the input space, which is just N = \u2211R\u2208R\u0304L NLR ."}, {"heading": "2.4. Identification of Inputs as Space Foldings", "text": "In this section, we discuss the intuition behind Theorem 2 in terms of space folding. A map g that identifies two subsets S and S \u2032 of a space can be considered as a folding operator that folds the space such that the subsets S and S \u2032 coincide. For instance, an coordinate-wise absolute function g : R2 \u2192 R2 in Eq. (1) folds the input space twice along both coordinates. In this case, this folding is equivalent to saying that the four quadrants of the 2\u2013D Euclidean space are identified by the map g. See Fig. 3 (a) for an illustration. The same map can be used again to fold the resulting image of the original input space.\nOne can easily see that each space folding corresponds to a single hidden layer of a deep neural network. Each hidden layer folds the space defined by the layer immediately below with respect to a specific map. A deep neural network effectively folds its input recursively, starting from the first layer.\nThe consequence of a recursive folding is that any partitioning of the final folded space will apply to all the collapsed subsets (identified by the map corresponding to the multiple levels of folding). Effectively, this means that any partitioning of the output space of a deep neural network is replicated over all the subsets of the input space which are identified by a map defined by a stack of hidden layers. See Fig. 3 (b) for an illustration of this replication property.\nThe space folding is not restricted to be done along the axes of the space, nor to preserve lengths. Rather, how the space is folded depends on the structure and nonlinear function used at each hidden layer. A set of complicated folding schemes may be applied to a space, which also means that the shapes of the identified subsets may differ from each other. See Fig. 4 for one more illustration."}, {"heading": "3. Deep Rectifier Networks", "text": "In this section we offer an analysis of deep neural networks with rectifier units, based on the general result from Sec. 2. We obtain a result that improves upon that by Pascanu et al. (2013), with a tighter\nbound on the maximal number of regions of linearity separated by functions computable by deep rectifier networks."}, {"heading": "3.1. Illustration", "text": "Consider a single hidden layer of n rectifiers with n0 input variables, where n \u2265 n0. We partition the set of rectifiers into n0 (non-overlapping) subsets of cardinality p = \u230a n n0 \u230b . Let us consider one of the n0 subsets. We construct p rectifiers such that\nf1(x) = max { 0,w>x } ,\nf2(x) = max { 0, 2w>x\u2212 1 } ,\nf3(x) = max { 0, 2w>x\u2212 2 } ,\n... fp(x) = max { 0, 2w>x\u2212 (p\u2212 1) } ,\nwhere w = [0, \u00b7 \u00b7 \u00b7 , 0, 1, 0, \u00b7 \u00b7 \u00b7 , 0] is a vector that chooses a single coordinate from the input space. We can linearly aggregate these p rectifiers into a single scalar value:\ng(x) = [ 1 \u22121 1 \u22121 1 \u00b7 \u00b7 \u00b7 ] [f1(x), f2(x), f3(x), f4(x), \u00b7 \u00b7 \u00b7 ]> . (3)\nSince each of these p rectifiers acts only on one common input dimension (marked with 1 in w), the constructed g effectively is a function that divides the input space (\u2212\u221e,\u221e) into p segments\n(\u2212\u221e,\u221e) = (\u2212\u221e, 0] \u222a (0, 1] \u222a (1, 2] \u222a \u00b7 \u00b7 \u00b7 \u222a (p\u2212 1,\u221e) .\nFor each segment, g computes a linear function whose image contains the interval (0, 1), as shown in Fig. 5.\nBy considering all n0 subsets of rectifiers in a given layer, we obtain an identification of pn0 hypercubes. The output of the hidden layer g = [g1, g2, \u00b7 \u00b7 \u00b7 , gp]> is symmetric about the hyperplane between any two hypercubes. Further computations by deeper layers on the image of the input space computed by g will apply to each of these pn0 hypercubes."}, {"heading": "3.2. Formal Result", "text": "We can generalize the procedure described above to the case where there are n0 input variables and L hidden layers of widths ni \u2265 n0 for all i \u2208 [L]. In this case, the maximal number of regions is lower-bounded by the following theorem.\nTheorem 3 The maximal number of regions of linearity of the functions computed by by a neural network with n0 input units and L hidden layers, with ni \u2265 n0 rectifiers at the i-th layer, is lower bounded by (\nL\u220f i=1 \u230a ni n0 \u230bn0) n0\u2211 j=0 ( nL j ) .\nProof The proof is done by counting the number of regions for a suitable choice of network parameters. We organize the units such that at each layer j the set of units is partitioned into n0 subsets of cardinality \u230a nj n0 \u230b . Each subset responds to a coordinate of an non-singular affine transformation of n0-dimensional space. By arranging the weights and biases of these units similarly to Sec. 3.1, we can fold each coordinate into itself p times, once at each inflexion point of the different units in the group. This partitions the space into a grid of identifiable regions.\nSpecifically, for the weights used in Sec. 3.1, the function g computes a partial sum of the responses (fk)k\u22651. It is sufficient to show that each of these partial sums, on the domain that they are defined, also takes value in the interval (0, 1). By inspecting the values of fk, we see that the intervals we need to explore are (0, 1], (1, 2], \u00b7 \u00b7 \u00b7 , (p\u2212 1,\u221e].\nFor some x \u2208 (0,\u221e) we denote g(x) = Sl(y), where l = min(p, bxc) and y = x\u2212 l. The form of g(x) becomes:\ng(x) = x+ l\u2211 i=1 ( 2(x\u2212 i)(\u22121)i ) = Sl(y) = l + y + 2 l\u2211 i=1 ( y(\u22121)i ) + 2 l\u2211 i=1 ( (l \u2212 i)(\u22121)i ) We solve this by induction, distinguishing between the case when l is odd or even. We first hypothesize that Sl(y) = y if l is even, and Sl(x) = 1 \u2212 y if l is odd. The base cases are trivially true by the formula of the partial sum. In the first induction step we consider l odd, implying l \u2212 1 is even. By induction we have\nSl(y) =(l \u2212 1) + 1 + y + 2 l\u22121\u2211 i=1 (l \u2212 1\u2212 i+ y + 1)(\u22121)i \u2212 2y\n=Sl\u22121(y) + l\u22121\u2211 i=1 2(\u22121)i + 1\u2212 2y = y + 1\u2212 2y = 1\u2212 y\nFor the second induction step we assume l is even, implying l \u2212 1 is odd, and\nSl(y) =(l \u2212 1) + 1 + y + 2 l\u22121\u2211 i=1 (l \u2212 1\u2212 i+ y + 1)(\u22121)i \u2212 2y\n=Sl\u22121(y) + l\u22121\u2211 i=1 2(\u22121)i + 1\u2212 2y = 1\u2212 y \u2212 2 + 1 + 2y = y\nWe can formulate an asymptotic expression for the behaviour of the provided bounds using big-O and \u2126 notation. Assuming that n0 = O(1) and ni = n for all i \u2265 1, for a single layer model with Ln hidden units, the number of regions behaves as O(Ln0nn0) (see Pascanu et al., 2013, Proposition 10). For a deep model, Theorem 3 implies the following asymptotic behaviour.\nCorollary 4 A rectifier neural network with n0 input units and L layers of width n > n0 can compute functions with \u2126 ((\nn n0\n)(L\u22121)n0 nn0 ) regions of linearity.\nWe see thus that the number of regions of linearity of deep models grows much faster than that of shallow models (exponentially in L and polynomially with n). Our bound significantly improves a previous bound obtained by (Pascanu et al., 2013, Proposition 10), which is \u2126 ((\nn n0\n)L\u22121 nn0 ) .\nIn particular, our result demonstrates that even for small values of L and n, deep rectifier models can produce substantially more linear regions than shallow ones."}, {"heading": "3.3. Stability to Perturbation", "text": "Our lower bounds on the complexity attainable by deep models are based on suitable choices of the network weights. This does not mean that the bounds only hold in singular cases.\nThe parametrization of the functions computed by a given network is continuous, that is, the map \u03c8 : RN \u2192 C(Rn0 ;RnL); \u03b8 = {W,b} 7\u2192 f\u03b8 is continuous. We considered the number of regions of linearity of the functions f\u03b8. By definition, each region of linearity contains an open neighborhood of the domain Rn0 . Given a function f\u03b8, there is an > 0 such that for each - perturbation of the parameter \u03b8, the resulting function f\u03b8+ has at least as many regions as f\u03b8, assuming there is only a finite number of regions. The regions of linearity of f\u03b8 are preserved under small perturbations of the parameters, because they have a finite volume. It may happen, however, that the perturbed function has additional regions of linearity, emerging at the intersection of regions from the unperturbed function.\nIf we define a probability density on the space of parameters (say uniform on a bounded domain), what is the probability of the event that the function represented by the network has a given number of regions of linearity? By the above discussion, the probability of getting a number of regions at least as large as the number resulting from any particular choice of parameters (for a uniform measure within a bounded domain) is nonzero, even though it may be very small. For future work it would be interesting to study the partitions of parameter space into regions where the resulting functions partition their input spaces in corresponding regions of linearity, and to investigate how many of these region in parameter space correspond to functions with a given number of regions of linearity."}, {"heading": "4. Deep Maxout Networks", "text": "A maxout network is a feedforward network with activations defined as follows.\nDefinition 5 Consider a linear map f : Rn \u2192 Rk\u00b7m; f(x) = Wx + b with n, k,m \u2208 N and W \u2208 Rk\u00b7m\u00d7n, b \u2208 Rk\u00b7m. If g : Rk\u00b7m \u2192 Rm is a function with coordinates gj(z) =\nmax{z(j\u22121)k+1, . . . , zjk}, then we call the composition g \u25e6 f a maxout layer of width m with n inputs and maxout units of rank k.\nThe maximum of two convex functions is also a convex function. Linear functions are convex, and so a maxout layer g \u25e6 f : Rn \u2192 Rm is also convex. The maximum of a collection of functions is called their upper envelope. We can view the graph of each linear function fi as a supporting hyperplane of a convex polytope in (n + 1)-dimensional space. In particular, if for each fi there is a neighbourhood of the input on which it is maximal, then the number of regions of the upper envelope gj = max{fi : i \u2208 [k]} is exactly k. The maximal number of regions of a maxout unit is equal to its rank. The linear regions of g = (gi)i\u2208[m] are the intersections of linear regions of the individual coordinates gi, i \u2208 [m]. In order to obtain the number of linear regions of g, we need to describe the structure of the regions of each gi and study their intersections.\nVoronoi diagrams are special cases of the partitions generated by maxout units (since they can be lifted to an upper envelope of linear functions). What is the number of regions of the intersection of mVoronoi diagrams with k seeds each? An upper bound is given by km, which would be the number of regions if all intersections \u2229jRj,i where different from each other. Computing intersections of Voronoi diagrams is not an easy task, in general. Therefore, we will consider simpler examples in the following. One simple example is the limiting case where a maxout unit of rank k divides its input by k \u2212 1 parallel hyperplanes. This corresponds to the Voronoi diagram of k seeds which lie on a line.\nIf m \u2264 n, we can consider the arrangement xi = 1, . . . , k \u2212 1 for each maxout unit i \u2208 [m]. In this case, the number of regions is km. Clearly, kn is a lower bound for the maximal number of regions attainable by a maxout layer of width m \u2265 n with n inputs.\nProposition 6 The maximal number of regions of a single layer maxout network with n inputs and m outputs of rank k is lower bounded by kmin{n,m} and upper bounded by km.\nRemark 7 There are some interesting cases where the superposition of Voronoi diagrams is well understood. Consider the network with m = n(n\u2212 1)/2 maxout units of rank 3 and let the regions of each unit be delimited by the hyperplanes xi \u2212 xj = 0, 1, where the units are labeled by pairs (i, j), 1 \u2264 i < j \u2264 n. The intersection of regions of all units is described by the superposition of all these hyperplanes. The resulting hyperplane arrangement is called the Shi arrangement Sn, and is known to have (n+ 1)n\u22121 regions. A similar arrangement is the Catalan arrangement, which has triplets of parallel hyperplanes, and n!Cn regions, where Cn = 1n+1 ( 2n n ) is the Catalan number. For details on these arrangements see (Stanley, 2004, Corollary 5.1 and Proposition 5.15).\nNow we will take a look at the deep maxout model. Note that a rank-2 maxout layer can be simulated by a rectifier layer with twice as many units. This implies that a rank-2 maxout network with n = n0 units in each layer can identify 2n0(k\u22121) regions from the input layer at layer k. Hence a constant width maxout model can produce at least 2n0(k\u22121)2n0 = 2n0k regions of linearity.\nFor the rank-k case, we identify inputs of a given layer in a similar way and obtain the following.\nTheorem 8 A maxout network with L layers of width n0 and rank k can compute functions with at least kL\u22121kn0 regions of linearity.\nProof Consider a network with n = n0 maxout units of rank k in each layer. We define the seeds of the maxout unit qj such that {Wi,:}i are unit vectors pointing in the positive and negative direction\nof bk/2c coordinate vectors. If k is larger than 2n0, then we forget about k \u2212 2n0 of them (just choose Wi,: = 0 for i > 2n0). In this case, qj is symmetric about the coordinate hyperplanes with normals ei with i \u2264 bk/2c and has one region of linearity for each such i, with gradient ei. For the remaining qj we consider similar functions, whereby we change the coordinate system by a slight rotation in some independent direction.\nThis implies that the output of each qj \u25e6 (f1, . . . , fk) is an interval [0,\u221e). The regions of linearity of each such composition divide the input space into r regions Rj,1, . . . , Rj,k. Since the change of coordinates used for each of them is a slight rotation in independent directions, we have that Ri := \u2229jRj,i is a cone of dimension n0 for all i \u2208 [k]. Furthermore, the gradients of qj \u25e6 fj for j \u2208 [n0] on each Ri are a basis of Rn0 . Hence the image of each Ri by the maxout layer contains an open cone of Rn0 which is identical for all i \u2208 [k]. This image can be shifted by bias terms such that the effective input of the next layer contains an open neighbourhood of the origin of Rn0 .\nThe above arguments show that a maxout layer of width n0 and rank k can identify at least k regions of its input. A network with L\u22121 layers with therefore identify kL\u22121 regions of the input.\nFrom Theorem 8 and Proposition 6 we can see that the number of regions in which deep maxout network divides the space can grow exponentially with the number of layers."}, {"heading": "5. Conclusions and Outlook", "text": "We studied the complexity of functions computable by deep feedforward neural networks in terms of their number of regions of linearity. We discussed the idea that each layer of a deep model is able to identify pieces of its input in such a way that the concatenation of layers identifies an exponential number of input regions, and that this leads to an exponential replication of the complexity of the functions computed in the higher layers of the model. The functions computed in this way by deep models are complicated, with rigidity caused by the replications that may help deep models generalize better than shallow models do.\nThis framework is applicable to any neural network that has a piecewise linear activation function. For example, if we consider a convolutional network with rectifier units, as the one used in (Krizhevsky et al., 2012), we can see that the convolution followed by max pooling at each layer identifies all patches of the input within a pooling region. This will let such a deep convolutional neural network recursively identify patches of the images of lower layers, resulting in exponentially many linear regions of the input space.\nThe parameter space of a given network is partitioned in the regions where the resulting functions have corresponding regions of linearity. This correspondence of the regions of linearity of the computed functions can be described in terms of their adjancency structure, or a poset of intersections of regions. Such combinatorial structures are in general hard to compute, even for simple hyperplane arrangements. One interesting question for future analysis is whether many regions of the parameter space of a given network correspond to functions which have a given number of regions of linearity."}], "references": [{"title": "Multi column deep neural network for traffic sign classification", "author": ["Dan Ciresan", "Ueli Meier", "Jonathan Masci", "Juergen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Ciresan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "Shallow vs. deep sum-product networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "Delalleau and Bengio.,? \\Q2011\\E", "shortCiteRegEx": "Delalleau and Bengio.", "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In AISTATS,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["Geoffrey Hinton", "Li Deng", "George E. Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara Sainath", "Brian Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["Kurt Hornik", "Maxwell Stinchcombe", "Halbert White"], "venue": "Neural Networks,", "citeRegEx": "Hornik et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Hornik et al\\.", "year": 1989}, {"title": "Approximation properties of DBNs with binary hidden units and real-valued visible units", "author": ["Oswin Krause", "Asja Fischer", "Tobias Glasmachers", "Christian Igel"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML-13),", "citeRegEx": "Krause et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krause et al\\.", "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Deep belief networks are compact universal approximators", "author": ["Nicolas Le Roux", "Yoshua Bengio"], "venue": "Neural Computation,", "citeRegEx": "Roux and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Roux and Bengio.", "year": 2010}, {"title": "Refinements of universal approximation results for deep belief networks and restricted Boltzmann machines", "author": ["Guido Mont\u00fafar", "Nihat Ay"], "venue": "Neural Computation,", "citeRegEx": "Mont\u00fafar and Ay.,? \\Q2011\\E", "shortCiteRegEx": "Mont\u00fafar and Ay.", "year": 2011}, {"title": "Universal approximation depth and errors of narrow belief networks with discrete units", "author": ["Guido F Mont\u00fafar"], "venue": "arXiv preprint arXiv:1303.7461,", "citeRegEx": "Mont\u00fafar.,? \\Q2013\\E", "shortCiteRegEx": "Mont\u00fafar.", "year": 2013}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "Proceedings of the Twenty-seventh International Conference on Machine Learning", "citeRegEx": "Nair and Hinton.,? \\Q2010\\E", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "On the number of inference regions of deep feed forward networks with piece-wise linear activations", "author": ["Razvan Pascanu", "Guido Mont\u00fafar", "Yoshua Bengio"], "venue": "[cs.LG],", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "An introduction to hyperplane arrangements", "author": ["Richard Stanley"], "venue": "In Lect. notes, IAS/Park City Math. Inst.,", "citeRegEx": "Stanley.,? \\Q2004\\E", "shortCiteRegEx": "Stanley.", "year": 2004}, {"title": "Facing Up to Arrangements: Face-Count Formulas for Partitions of Space by Hyperplanes", "author": ["Thomas Zaslavsky"], "venue": "Number no. 154 in Memoirs of the American Mathematical Society. American Mathematical Society,", "citeRegEx": "Zaslavsky.,? \\Q1975\\E", "shortCiteRegEx": "Zaslavsky.", "year": 1975}], "referenceMentions": [{"referenceID": 0, "context": "Introduction Artificial neural networks with several hidden layers, called deep neural networks, have become popular due to their unprecedented success in a variety of machine learning tasks (see, e.g., Krizhevsky et al., 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012).", "startOffset": 191, "endOffset": 295}, {"referenceID": 3, "context": "Introduction Artificial neural networks with several hidden layers, called deep neural networks, have become popular due to their unprecedented success in a variety of machine learning tasks (see, e.g., Krizhevsky et al., 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012).", "startOffset": 191, "endOffset": 295}, {"referenceID": 8, "context": "Other works have investigated universal approximation of probability distributions by deep belief networks (Le Roux and Bengio, 2010; Mont\u00fafar and Ay, 2011), as well as their approximation properties (Mont\u00fafar, 2013; Krause et al.", "startOffset": 107, "endOffset": 156}, {"referenceID": 9, "context": "Other works have investigated universal approximation of probability distributions by deep belief networks (Le Roux and Bengio, 2010; Mont\u00fafar and Ay, 2011), as well as their approximation properties (Mont\u00fafar, 2013; Krause et al., 2013).", "startOffset": 200, "endOffset": 237}, {"referenceID": 5, "context": "Other works have investigated universal approximation of probability distributions by deep belief networks (Le Roux and Bengio, 2010; Mont\u00fafar and Ay, 2011), as well as their approximation properties (Mont\u00fafar, 2013; Krause et al., 2013).", "startOffset": 200, "endOffset": 237}, {"referenceID": 2, "context": "(2013) reported a theoretical result on the complexity of functions computable by deep feedforward networks with piecewise linear units called rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), which have an activation function of the form g(x) = max{0,x}.", "startOffset": 159, "endOffset": 203}, {"referenceID": 10, "context": "(2013) reported a theoretical result on the complexity of functions computable by deep feedforward networks with piecewise linear units called rectifier units (Glorot et al., 2011; Nair and Hinton, 2010), which have an activation function of the form g(x) = max{0,x}.", "startOffset": 159, "endOffset": 203}, {"referenceID": 0, "context": ", 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012). In view of this empirical evidence, deep neural networks are becoming increasingly favoured over shallow networks, and are often implemented with more than five layers. At the time being, however, only a limited amount of publications have investigated deep networks from a theoretical perspective. Recently, Delalleau and Bengio (2011) showed that a shallow network (i.", "startOffset": 8, "endOffset": 414}, {"referenceID": 0, "context": ", 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012). In view of this empirical evidence, deep neural networks are becoming increasingly favoured over shallow networks, and are often implemented with more than five layers. At the time being, however, only a limited amount of publications have investigated deep networks from a theoretical perspective. Recently, Delalleau and Bengio (2011) showed that a shallow network (i.e., with a single hidden layer) requires exponentially many more sum-product hidden units than a deep sum-product network in order to compute certain families of polynomials. We are interested in extending this kind of analysis to more popular neural networks composed of layers of nonlinear computational units. A well-known result due to Hornik et al. (1989) states that a feedforward neural network with a single hidden layer is a universal approximator (of Borel measurable functions).", "startOffset": 8, "endOffset": 808}, {"referenceID": 0, "context": ", 2012; Ciresan et al., 2012; Goodfellow et al., 2013; Hinton et al., 2012). In view of this empirical evidence, deep neural networks are becoming increasingly favoured over shallow networks, and are often implemented with more than five layers. At the time being, however, only a limited amount of publications have investigated deep networks from a theoretical perspective. Recently, Delalleau and Bengio (2011) showed that a shallow network (i.e., with a single hidden layer) requires exponentially many more sum-product hidden units than a deep sum-product network in order to compute certain families of polynomials. We are interested in extending this kind of analysis to more popular neural networks composed of layers of nonlinear computational units. A well-known result due to Hornik et al. (1989) states that a feedforward neural network with a single hidden layer is a universal approximator (of Borel measurable functions). Other works have investigated universal approximation of probability distributions by deep belief networks (Le Roux and Bengio, 2010; Mont\u00fafar and Ay, 2011), as well as their approximation properties (Mont\u00fafar, 2013; Krause et al., 2013). More recently, Pascanu et al. (2013) reported a theoretical result on the complexity of functions computable by deep feedforward networks with piecewise linear units called rectifier units (Glorot et al.", "startOffset": 8, "endOffset": 1213}, {"referenceID": 11, "context": "In this paper, we follow the ideas of Pascanu et al. (2013) to analyze deep models, but we take a broader perspective.", "startOffset": 38, "endOffset": 60}, {"referenceID": 11, "context": "In this paper, we follow the ideas of Pascanu et al. (2013) to analyze deep models, but we take a broader perspective. We show that deep models are able to sequentially map pieces of their inputs into the same output and in that way, the layer-wise composition of functions that they compute re-uses low-level computations exponentially often as the number of layers increases. Our analysis and results apply to a variety of deep networks, including deep rectifier networks. We estimate the number of regions of linearity of functions computable by deep neural networks for two important cases: with rectifier units and maxout units (Goodfellow et al., 2013). Our bound for the complexity of deep rectifier networks improves the previous bound obtained by Pascanu et al. (2013). The number of linear regions of functions computed by a deep model is a measure of flexibility for the model.", "startOffset": 38, "endOffset": 778}, {"referenceID": 12, "context": "The number of regions generated by a hyperplane arrangement can be given in terms of a characteristic function, which is a well known result by Zaslavsky (1975). We will only use the fact that an arrangement of n1 hyperplanes in Rn0 produces up to \u2211n0 j=0 ( n1 j ) regions.", "startOffset": 144, "endOffset": 161}, {"referenceID": 11, "context": "We obtain a result that improves upon that by Pascanu et al. (2013), with a tighter", "startOffset": 46, "endOffset": 68}], "year": 2014, "abstractText": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the number of regions of linearity that they have. Deep networks are able to sequentially map portions of each layer\u2019s input space to the same output. In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth. This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.", "creator": "LaTeX with hyperref package"}}}