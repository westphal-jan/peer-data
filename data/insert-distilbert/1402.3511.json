{"id": "1402.3511", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2014", "title": "A Clockwork RNN", "abstract": "sequence prediction scheduling and classification are ubiquitous and challenging problems in machine bounded learning that can require identifying potentially complex dependencies between temporally distant inputs. recurrent neural networks ( rnns ) have the ability, in theory, to cope with these temporal dependencies by virtue of the computational short - term memory implemented by their recurrent ( feedback ) connections. however, in practice they are both difficult to train successfully when the long - term random memory is required. this paper introduces a simple, yet powerful modification to the standard rnn architecture, the intrinsic clockwork rnn ( cw - rnn ), in which the hidden layer is partitioned into separate control modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. rather than making the standard rnn models more complex, cw - rnn reduces the number of rnn parameters, improves the performance significantly in the tasks still tested, and speeds up the network evaluation. the network is demonstrated in preliminary experiments involving two tasks : audio signal generation and phonetic timit spoken word classification, where it outperforms both rnn and lstm networks.", "histories": [["v1", "Fri, 14 Feb 2014 16:05:12 GMT  (392kb,D)", "http://arxiv.org/abs/1402.3511v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["jan koutn\u00edk", "klaus greff", "faustino j gomez", "j\u00fcrgen schmidhuber"], "accepted": true, "id": "1402.3511"}, "pdf": {"name": "1402.3511.pdf", "metadata": {"source": "META", "title": "A Clockwork RNN", "authors": ["Jan Koutn\u0131\u0301k", "Klaus Greff", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "emails": ["HKOU@IDSIA.CH", "KLAUS@IDSIA.CH", "TINO@IDSIA.CH", "JUERGEN@IDSIA.CH"], "sections": [{"heading": "1. Introduction", "text": "Recurrent Neural Networks (RNNs; Robinson & Fallside, 1987; Werbos, 1988; Williams, 1989) are a class of connectionist models that possess internal state or short term memory due to recurrent feed-back connections, that make them suitable for dealing with sequential problems, such as speech classification, prediction and generation.\nStandard RNNs trained with stochastic gradient descent have difficulty learning long-term dependencies (i.e. spanning more that 10 time-steps) encoded in the input sequences\ndue to the vanishing gradient (Hochreiter, 1991; Hochreiter et al., 2001). The problem has been addressed for example by using a specialized neuron structure, or cell, in Long Short-Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) that maintains constant backward flow in the error signal; second-order optimization methods (Martens & Sutskever, 2011) preserve the gradient by estimating its curvature; or using informed random initialization (Sutskever et al., 2013) which allows for training the networks with momentum and stochastic gradient descent only.\nThis paper presents a novel modification to the simple RNN (SRN; Elman, 1988) architecture and, mutatis mutandis, an associated error back-propagation through time (Rumelhart et al., 1986; Werbos, 1988; Williams, 1989) training algorithm, that show superior performance in the generation and classification of sequences that contain long-term dependencies. Here, the long-term dependency problem is solved by having different parts (modules) of the RNN hidden layer running at different clock speeds, timing their computation with different, discrete clock periods, hence the name Clockwork Recurrent Neural Network (CW-RNN). CW-RNN train and evaluate faster since not all modules are executed at every time step, and have a smaller number of weights compared to SRNs, because slower modules are not connected to faster ones.\nCW-RNNs were tested on two supervised learning tasks: sequence generation where a target audio signal must be output by a network using no input; and spoken word classification using the TIMIT dataset. In these preliminary experiments, CW-RNN outperformed both SRN and LSTM with the same number of weights by a significant margin. The next section provides an overview of the related work, section 3 describes the CW-RNN architecture in detail and section 5 discusses the results of experiments in section 4 and future potential of Clockwork Recurrent Neural Networks.\nar X\niv :1\n40 2.\n35 11\nv1 [\ncs .N\nE ]\n1 4\nFe b\n20 14"}, {"heading": "2. Related Work", "text": "Contributions to the sequence modeling and recognition that are relevant to CW-RNN are introduced in this section. The primary focus is on RNN extensions that deal with the problem of bridging long time lags.\nOne model that is similar in spirit to our approach is the NARX RNN1 (Lin et al., 1996). But instead of simplifying the network, it introduces an additional sets of recurrent connections with time lags of 2,3..k time steps. These additional connections help to bridge long time lags, but introduce many additional parameters that make NARX RNN training more difficult and run k times slower.\nLong Short-Term Memory (LSTM; Hochreiter & Schmidhuber, 1997) uses a specialized architecture that allows information to be stored in a linear unit called a constant error carousel (CEC) indefinitely. The cell containing the CEC has a set of multiplicative units (gates) connected to other cells that regulate when new information enters the CEC (input gate), when the activation of the CEC is output to the rest of the network (output gate), and when the activation decays or is \u201dforgotten\u201d (forget gate). These networks have been very successful recently in speech and handwriting recognition (Graves et al., 2005; 2009; Sak et al., 2014).\nStacking LSTMs into several layers (Fernandez et al., 2007; Graves & Schmidhuber, 2009) aims for hierarchical sequence processing. Such a hierarchy, equipped with Connec-\n1NARX stands for Non-linear Auto-Regressive model with eXogeneous inputs\ntionist Temporal Classification (CTC; Graves et al., 2006), performs simultaneous segmentation and recognition of sequences. Its deep variant currently holds the state-of-theart result in phoneme recognition on the TIMIT database (Graves et al., 2013).\nTemporal Transition Hierarchy (TTH; Ring, 1993) incrementally adds high-order neurons in order to build a memory that is used to disambiguate an input at the current time step. This approach can, in principle, bridge time intervals of any length, but with proportionally growing network size. The model was recently improved by adding recurrent connections (Ring, 2011) that prevent it from bloating by reusing the high-level nodes through the recurrent connections.\nOne of the earliest attempts to enable RNNs to handle long-term dependencies is the Reduced Description Network (Mozer, 1992; 1994). It uses leaky neurons whose activation changes only a bit in response to its inputs. This technique was recently picked up by Echo State Networks (ESN; Jaeger, 2002).\nA similar technique has been used by Sutskever & Hinton (2010) to solve some serial recall tasks. These TemporalKernel RNNs add a connection from each neuron to itself that has a weight that decays exponentially in time. This is implemented in a way that can be computed efficiently, however, its performance is still inferior to LSTM.\nEvolino (Schmidhuber et al., 2005; 2007) feeds the input to an RNN (which can be e.g. LSTM to cope with long time lags) and then transforms the RNN outputs to the target sequences via a optimal linear mapping, that is computed\nanalytically by pseudo-inverse. The RNN is trained by an evolutionary algorithm, therefore it does not suffer from the vanishing gradient problem. Evolino outperformed LSTM on a set of synthetic problems and was used to perform complex robotic manipulation (Mayer et al., 2006).\nA modern theory of why RNNs fail to learn long-term dependencies is that simple gradient descent fails to optimize them correctly. One attempt to mitigate this problem is Hessian Free (HF) optimization (Martens & Sutskever, 2011), an adapted second-order training method that has been demonstrated to work well with RNNs. It allows RNNs to solve some long-term lag problems that were impossible with stochastic gradient descent. Their performance on rather synthetic, long-term memory benchmarks is approaching the performance of LSTM, though the number of optimization steps in HF-RNN is usually greater. Training networks by HF optimization is an orthogonal approach to the network architecture, so both LSTM and CW-RNN can still benefit from it.\nHF optimization allowed for training of Multiplicative RNN (MRNN; Sutskever et al., 2011) that port the concept of multiplicative gating units to SRNs. The gating units are represented by a factored 3-way tensor in order to reduce the number of parameters. Extensive training of an MRNN for a number of days on a graphics cards provided impressive results in text generation tasks.\nTraining RNNs with Kalman filters (Williams, 1992) has shown advantages in bridging long time lags as well, although this approach is computationally unfeasible for larger networks.\nThe methods mentioned above are strictly synchronous\u2013 elements of the network clock at the same speed. The Sequence Chunker, Neural History Compressor or Hierarchical Temporal Memory (Schmidhuber, 1991; 1992) consists of a hierarchy or stack of RNN that may run at different time scales, but, unlike the simpler CW-RNN, it requires unsupervised event predictors: a higher-level RNN receives an input only when the lower-level RNN below is unable to predict it. Hence the clock of the higher level may speed up or slow down, depending on the current predictability of the input stream. This contrasts the CW-RNN, in which the clocks always run at the same speed, some slower, some faster."}, {"heading": "3. A Clockwork Recurrent Neural Network", "text": "Clockwork Recurrent Neural Networks (CW-RNN) like SRNs, consist of input, hidden and output layers. There are forward connections from the input to hidden layer, and from the hidden to output layer, but, unlike the SRN,\nthe neurons in the hidden layer are partitioned into g modules of size k. Each of the modules is assigned a clock period Tn \u2208 {T1, . . . , Tg}. Each module is internally fullyinterconnected, but the recurrent connections from module j to module i exists only if the period Ti is smaller than period Tj . Sorting the modules by increasing period, the connections between modules propagate the hidden state right-to-left, from slower modules to faster modules, see Figure 1.\nThe standard RNN output, y(t)O , at a time step t is calculated using the following equations:\ny (t) H = fH(WH \u00b7 y (t\u22121) +WI \u00b7 x(t)), (1)\ny (t) O = fO(WO \u00b7 y (t) H ), (2)\nwhere WH , WI and WO are the hidden, input and output weight matrices, xt is the input vector at time step t, vectors y (t) H and y (t\u22121) H represent the hidden neuron activations at time steps t and t\u22121. Functions fH(.) and fO(.) are the nonlinear activation functions. For simplicity, neuron biases are omitted in the equations.\nThe main difference between CW-RNN and an RNN is that at each CW-RNN time step t, only the output of modules i that satisfy (t MOD Ti) = 0 are executed. The choice of the set of periods {T1, . . . , Tg} is arbitrary. In this paper, we use the exponential series of periods: module i has clock period of Ti = 2i\u22121.\nMatrices WH and WI are partitioned into g blocks-rows:\nWH = WH1... WHg  WI = WI1... WIg  (3) and WH is a block-upper triangular matrix, where each block-row, WHi , is partitioned into block-columns\n{01, . . . ,0i\u22121,WHi,i , . . . ,WHi,g}. At each forward pass time step, only the block-rows of WH and WI that correspond to the executed modules are used for evaluation in Equation (1):\nWHi = { WHi for (t MOD Ti) = 0 0 otherwise\n(4)\nand the corresponding parts of the output vector, yH , are updated. The other modules retain their output values from the previous time-step. Calculation of the hidden activation at time step t = 6 is illustrated in Figure 2.\nAs a result, the low-clock-rate modules process, retain and output the long-term information obtained from the input sequences (not being distracted by the high speed modules), whereas the high-speed modules focus on the local, highfrequency information (having the context provided by the low speed modules available).\nThe backward pass of the error propagation is similar to RNN as well. The only difference is that the error propagates only from modules that were executed at time step t. The error of non-activated modules gets copied back in time (similarly to copying the activations of nodes not activated at the time step t during the corresponding forward pass),\nwhere it is added to the back-propagated error.\nCW-RNN runs much faster than a simple RNN with the same number of hidden nodes since not all modules are evaluated at every time step. The lower bound for the CWRNN speedup compared to an RNN with the same number of neurons is g/4 in the case of this exponential clock setup, see Appendix for a detailed derivation."}, {"heading": "4. Experiments", "text": "CW-RNNs were compared to the simple RNN (SRN) and LSTM networks. All networks have one hidden layer with the tanh activation function, and the number of nodes in the hidden layer was chosen to obtain (approximately) the same number of parameters for all three methods (in the case of CW-RNN, the clock periods were included in the parameter count).\nInitial values for all the weights were drawn from a Gaussian distribution with zero mean and standard deviation of 0.1. Initial values of all internal state variables were set to 0. Each setup was run 100 times with different random initialization of parameters. All networks were trained using Stochastic Gradient Descent (SGD) with Nesterov-style momentum (Sutskever et al., 2013)."}, {"heading": "4.1. Sequence Generation", "text": "The goal of this task is to train a recurrent neural network, that receives no input, to generate a target sequence as accurately as possible. The weights of the network can be seen as a (lossy) encoding of the whole sequence, which could be used for compression.\nFive different target sequences were created by sampling a piece of music2 at 44.1Hz for 7ms. The resulting sequences of 320 data points each were scaled to the interval [\u22121, 1]. In the following experiments we compare performance on these five sequences.\nAll networks used the same architecture: no inputs, one hidden layer and a single linear output neuron. Each network type was run with 4 different sizes: 100, 250, 500, and 1000 parameters, see Table 1 for the summary of number of hidden nodes. The networks were trained over 2000 epochs to minimize the mean squared error. After that time the error no longer decreased noticeably. Momentum was set to 0.95 while the learning rate was optimized separately for every method, but kept the same for all network sizes.\nA learning rate of 3 \u00d7 10\u22124 was found to be optimal for 2taken from the beginning of the first track Many\u0301rista of album Musica Deposita by Cuprum\nRNN and CW-RNN while for LSTM 3\u00d7 10\u22125 gave better results. For LSTM it was also crucial to initialize the bias of the forget gates to a high value (5 in this case) to encourage the long-term memory. The hidden units of CW-RNN were divided into nine equally sized groups with exponential clock-timings {1, 2, 4, . . . , 256}.\nThe results for the experiments are shown in Figure 3. It is obvious that RNNs fail to generate the target sequence, and they do not seem to improve with network size. LSTM does much better, and shows an improvement as the networks get bigger. CW-RNNs give by far the best results, with the smallest one being roughly on par with the second-biggest LSTM network. Also, all but the smallest CW-RNN have significantly less variance than all the other methods. To get an intuitive understanding of what is happening, Figure 5 shows the output of the best network of each type on each one of the five audio samples. The average error of the best networks is summarized in Table 3 (row 1)."}, {"heading": "4.2. Spoken Word Classification", "text": "The second task is sequence classification instead of generation. Each sequence contains an audio signal of one spoken word from the TIMIT Speech Recognition Benchmark (Garofolo et al., 1993). The dataset contains 25 dif-\nferent words (classes) arranged in 5 clusters based on their suffix. Because of the suffix-similarity the network needs to learn long-term dependencies in order to disambiguate the words. The words are:\nCluster 1: making, walking, cooking, looking, working\nCluster 2: biblical, cyclical, technical, classical, critical\nCluster 3: tradition, addition, audition, recognition, competition\nCluster 4: musicians, discussions, regulations, accusations, conditions\nCluster 5: subway, leeway, freeway, highway, hallway\nFor every word there are 7 examples from different speakers, which were partitioned into 5 for training and 2 for testing, for a total of 175 sequences (125 train, 50 test). Each sequence element consists of 12-dimensional MFCC vector (Mermelstein, 1976) plus energy, sampled every 10ms over a 25ms window with a pre-emphasis coefficient of 0.97. Each of the 13 channels was then normalized to have zero mean and unit variance over the whole training set.\nAll network types used the same architecture: 13 inputs, a single hidden and a softmax output layer with 25 units. Five hidden layer sizes were chosen such that the total number of parameters for the whole network is roughly 0.5k, 1k, 2.5k, 5k, and 10k.\nAll networks used a learning rate of 3\u00d7 10\u22124, a momentum of 0.9, and were trained to minimize the Multinomial Cross Entropy Error. Every experiment was repeated 100 times with different random initializations.\nBecause the dataset is so small, Gaussian noise with a standard deviation of 0.6 was added to the inputs during training to guard against overfitting. Training was stopped once the error on the noise-free training set did not decrease for 5 epochs. To obtain good results with LSTM, it was again"}, {"heading": "4.1 NMSE 0.46\u00b10.08 0.04\u00b10.01 0.007\u00b10.004", "text": "important to initialize the forget gate bias to 5. For the CWRNN the neurons were divided evenly into 7 groups with exponentially increasing periods: {1, 2, 4, 8, 16, 32, 64}.\nFigure 4 shows the classification error of the different networks on the word classification task. Here again, RNNs perform the worst, followed by LSTMs, which give substantially better results, especially with more parameters. CW-RNNs beat both RNN and LSTM networks by a considerable margin of 8-20% on average irrespective of the number of parameters. The error of the largest networks is summarized in Table 3 (row 2)."}, {"heading": "5. Discussion", "text": "The experimental results show that the simple mechanism of running subsets of neurons at different speeds allows an RNN to efficiently learn the different dynamic time-scales inherent in complex signals.\nOther functions could be used to set the module periods: linear, Fibonacci, logarithmic series, or even fixed random periods. These were not considered in this paper because the intuitive setup of using an exponential series worked well in these preliminary experiments. Another option would be to learn the periods as well, which, to use error backpropagation would require a differentiable modulo function for triggering the clocks. Alternatively, one could train the clocks (together with the weights) using evolutionary algorithms which do not require a closed form for the gradient.\nNote that the lowest period in the network can be greater than 1. Such a network would not be able to change its output at every time step, which may be useful as a low-pass filter when the data contains noise.\nAlso, the modules do not have to be all of the same size. One could adjust them according to the expected information in the input sequences, by e.g. using frequency analysis of the data and setting up modules sizes and clocks proportional to the spectrum.\nGrouping hidden neurons into modules is a partway to having each weight have its own clock. Initial experiments, not included in this paper, have shown that such networks are hard to train and do not provide good results.\nCW-RNN showed superior performance on the speech data classification among all three models tested. Note that, unlike in the standard approach, in which the speech signal frequency coefficients are first translated to phonemes which are modeled with a standard approach like Hidden Markov Modes for complete words, CW-RNN attempts to model and recognize the complete words directly, where it benefits from the modules running at multiple speeds.\nFuture work will start by conducting a detailed analysis of the internal dynamics taking place in the CW-RNN to understand how the network is allocating resources for a given type of input sequence. Further testing on other classes of problems, such as reinforcement learning, and comparison to the larger set of connectionist models for sequential data processing are also planned.\nAppendix\nCW-RNN has fewer total parameters and even fewer operations per time step than a standard RNN with the same number of neurons. Assume CW-RNN consists of g modules of size k for a total of n = kg neurons. Because a neuron is only connected to other neurons with the same or larger period, the number of parameters NH for the recurrent matrix is:\nNH = g\u2211 i=1 k\u2211 j=1 k(g \u2212 i+ 1) = k2 g\u22121\u2211 i=0 (g \u2212 i) = n 2 2 + nk 2 .\nCompared to the n2 parameters in the recurrent matrix WH of RNN this results in roughly half as many parameters:\nNH n2\n= n2 2 + nk 2\nn2 =\nn2 + nk\n2n2 =\nn+ k\n2n =\ng + 1 2g \u2248 1 2 .\nEach module i is evaluated only every Ti-th time step, therefore the number of operations at a time step is:\nOH = k 2 g\u22121\u2211 i=0 g \u2212 i Ti .\nFor exponentially scaled periods, Ti = 2i, the upper bound for number of operations, OH , needed for WH per time step is:\nOh = k 2 g\u22121\u2211 i=0 g \u2212 i 2i = k2\n( g\ng\u22121\u2211 i=0 1\n2i\ufe38 \ufe37\ufe37 \ufe38 \u22642\n+ g\u22121\u2211 i=0 i\n2i\ufe38 \ufe37\ufe37 \ufe38 \u22642\n) \u2264\n\u2264 k2(2g \u2212 2) \u2264 2nk,\nbecause g \u2265 2 this is less than or equal to n2. Recurrent operations in CW-RNN are faster than in an RNN with the same number of neurons by a factor of at least g/2, which, for typical CW-RNN sizes ends up being between 2 and 5. Similarly, upper bound for the number of input weight evaluations, EI , is:\nOI = g\u22121\u2211 i=0 km Ti = km g\u22121\u2211 i=0 1 Ti \u2264 2km\nTherefore, the overall CW-RNN speed-up w.r.t RNN is:\nn2 + nm+ n\nOR +OI + 2n =\nk2g2 + kgm+ kg\nk2(2g \u2212 2) + 2km+ 2kg =\n= g(kg +m+ 1)\n2(k(g \u2212 1) +m+ g) =\ng\n2\n(kg +m+ 1)\nk(g \u2212 1) +m+ g\ufe38 \ufe37\ufe37 \ufe38 \u2265 12\n\u2265 g 4\nNote that this is a conservative lower bound."}, {"heading": "Acknowledgments", "text": "This research was supported by Swiss National Science Foundation grant #138219: \u201cTheory and Practice of Reinforcement Learning 2\u201d, and the EU FP7 project \u201cNanoBioTouch\u201d, grant #228844."}], "references": [{"title": "Finding structure in time. CRL Technical Report 8801", "author": ["J.L. Elman"], "venue": "Center for Research in Language,", "citeRegEx": "Elman,? \\Q1988\\E", "shortCiteRegEx": "Elman", "year": 1988}, {"title": "Sequence labelling in structured domains with hierarchical recurrent neural networks", "author": ["Fernandez", "Santiago", "Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Fernandez et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fernandez et al\\.", "year": 2007}, {"title": "DARPA TIMIT acoustic phonetic continuous speech corpus CD-ROM", "author": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren"], "venue": null, "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Rapid retraining on speech data with LSTM recurrent networks", "author": ["A. Graves", "N. Beringer", "J. Schmidhuber"], "venue": "Technical Report IDSIA-09-05,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural nets", "author": ["A. Graves", "S. Fernandez", "F.J. Gomez", "J. Schmidhuber"], "venue": "In ICML\u201906: Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "A novel connectionist system for improved unconstrained handwriting recognition", "author": ["A. Graves", "M. Liwicki", "S. Fernandez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Graves et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2009}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "rahman Mohamed", "Abdel", "Hinton", "Geoffrey E"], "venue": "In ICASSP,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Untersuchungen zu dynamischen neuronalen Netzen", "author": ["S. Hochreiter"], "venue": "Diploma thesis, Institut fu\u0308r Informatik, Lehrstuhl Prof. Brauer, Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "Hochreiter,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter", "year": 1991}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Short term memory in echo state networks. GMD-Report 152, GMD - German", "author": ["H. Jaeger"], "venue": "National Research Institute for Computer Science,", "citeRegEx": "Jaeger,? \\Q2002\\E", "shortCiteRegEx": "Jaeger", "year": 2002}, {"title": "Learning longterm dependencies in NARX recurrent neural networks", "author": ["T. Lin", "B.G. Horne", "P. Tino", "C.L. Giles"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Lin et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lin et al\\.", "year": 1996}, {"title": "Distance measures for speech recognition: Psychological and instrumental", "author": ["P. Mermelstein"], "venue": "Pattern Recognition and Artificial Intelligence,", "citeRegEx": "Mermelstein,? \\Q1976\\E", "shortCiteRegEx": "Mermelstein", "year": 1976}, {"title": "Induction of multiscale temporal structure", "author": ["M.C. Mozer"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mozer,? \\Q1992\\E", "shortCiteRegEx": "Mozer", "year": 1992}, {"title": "Neural network music composition by prediction: Exploring the benefits of psychoacoustic constraints and multi-scale processing", "author": ["M.C. Mozer"], "venue": "Connection Science,", "citeRegEx": "Mozer,? \\Q1994\\E", "shortCiteRegEx": "Mozer", "year": 1994}, {"title": "Learning sequential tasks by incrementally adding higher orders", "author": ["Ring", "Mark"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Ring and Mark.,? \\Q1993\\E", "shortCiteRegEx": "Ring and Mark.", "year": 1993}, {"title": "Recurrent transition hierarchies for continual learning: A general overview", "author": ["Ring", "Mark"], "venue": "In Lifelong Learning, volume WS-11-15 of AAAI Workshops. AAAI,", "citeRegEx": "Ring and Mark.,? \\Q2011\\E", "shortCiteRegEx": "Ring and Mark.", "year": 2011}, {"title": "The utility driven dynamic error propagation network", "author": ["A.J. Robinson", "F. Fallside"], "venue": "Technical Report CUED/FINFENG/TR.1,", "citeRegEx": "Robinson and Fallside,? \\Q1987\\E", "shortCiteRegEx": "Robinson and Fallside", "year": 1987}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Parallel Distributed Processing,", "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Long Short-Term Memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["Sak", "Ha\u015fim", "Senior", "Andrew", "Beaufays", "Fran\u00e7oise"], "venue": "Technical report,", "citeRegEx": "Sak et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sak et al\\.", "year": 2014}, {"title": "Neural sequence chunkers", "author": ["J. Schmidhuber"], "venue": "Technical Report FKI-148-91,", "citeRegEx": "Schmidhuber,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber", "year": 1991}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber", "year": 1992}, {"title": "Evolino: Hybrid neuroevolution / optimal linear search for sequence prediction", "author": ["J. Schmidhuber", "D. Wierstra", "F.J. Gomez"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Schmidhuber et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 2005}, {"title": "Training recurrent networks by Evolino", "author": ["J. Schmidhuber", "D. Wierstra", "M. Gagliolo", "F.J. Gomez"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Schmidhuber et al\\.", "year": 2007}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G. Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Temporal-kernel recurrent neural networks", "author": ["Sutskever", "Ilya", "Hinton", "Geoffrey"], "venue": "Neural Networks,", "citeRegEx": "Sutskever et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2010}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["P.J. Werbos"], "venue": "Neural Networks,", "citeRegEx": "Werbos,? \\Q1988\\E", "shortCiteRegEx": "Werbos", "year": 1988}, {"title": "Complexity of exact gradient computation algorithms for recurrent neural networks", "author": ["R.J. Williams"], "venue": "Technical Report NU-CCS-89-27,", "citeRegEx": "Williams,? \\Q1989\\E", "shortCiteRegEx": "Williams", "year": 1989}, {"title": "Training recurrent networks using the extended kalman filter", "author": ["Williams", "Ronald J"], "venue": "In Neural Networks,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}], "referenceMentions": [{"referenceID": 27, "context": "Recurrent Neural Networks (RNNs; Robinson & Fallside, 1987; Werbos, 1988; Williams, 1989) are a class of connectionist models that possess internal state or short term memory due to recurrent feed-back connections, that make them suitable for dealing with sequential problems, such as speech classification, prediction and generation.", "startOffset": 26, "endOffset": 89}, {"referenceID": 28, "context": "Recurrent Neural Networks (RNNs; Robinson & Fallside, 1987; Werbos, 1988; Williams, 1989) are a class of connectionist models that possess internal state or short term memory due to recurrent feed-back connections, that make them suitable for dealing with sequential problems, such as speech classification, prediction and generation.", "startOffset": 26, "endOffset": 89}, {"referenceID": 8, "context": "spanning more that 10 time-steps) encoded in the input sequences due to the vanishing gradient (Hochreiter, 1991; Hochreiter et al., 2001).", "startOffset": 95, "endOffset": 138}, {"referenceID": 10, "context": "spanning more that 10 time-steps) encoded in the input sequences due to the vanishing gradient (Hochreiter, 1991; Hochreiter et al., 2001).", "startOffset": 95, "endOffset": 138}, {"referenceID": 0, "context": "This paper presents a novel modification to the simple RNN (SRN; Elman, 1988) architecture and, mutatis mutandis, an associated error back-propagation through time (Rumelhart et al.", "startOffset": 59, "endOffset": 77}, {"referenceID": 19, "context": "This paper presents a novel modification to the simple RNN (SRN; Elman, 1988) architecture and, mutatis mutandis, an associated error back-propagation through time (Rumelhart et al., 1986; Werbos, 1988; Williams, 1989) training algorithm, that show superior performance in the generation and classification of sequences that contain long-term dependencies.", "startOffset": 164, "endOffset": 218}, {"referenceID": 27, "context": "This paper presents a novel modification to the simple RNN (SRN; Elman, 1988) architecture and, mutatis mutandis, an associated error back-propagation through time (Rumelhart et al., 1986; Werbos, 1988; Williams, 1989) training algorithm, that show superior performance in the generation and classification of sequences that contain long-term dependencies.", "startOffset": 164, "endOffset": 218}, {"referenceID": 28, "context": "This paper presents a novel modification to the simple RNN (SRN; Elman, 1988) architecture and, mutatis mutandis, an associated error back-propagation through time (Rumelhart et al., 1986; Werbos, 1988; Williams, 1989) training algorithm, that show superior performance in the generation and classification of sequences that contain long-term dependencies.", "startOffset": 164, "endOffset": 218}, {"referenceID": 12, "context": "One model that is similar in spirit to our approach is the NARX RNN1 (Lin et al., 1996).", "startOffset": 69, "endOffset": 87}, {"referenceID": 3, "context": "These networks have been very successful recently in speech and handwriting recognition (Graves et al., 2005; 2009; Sak et al., 2014).", "startOffset": 88, "endOffset": 133}, {"referenceID": 20, "context": "These networks have been very successful recently in speech and handwriting recognition (Graves et al., 2005; 2009; Sak et al., 2014).", "startOffset": 88, "endOffset": 133}, {"referenceID": 1, "context": "Stacking LSTMs into several layers (Fernandez et al., 2007; Graves & Schmidhuber, 2009) aims for hierarchical sequence processing.", "startOffset": 35, "endOffset": 87}, {"referenceID": 4, "context": "NARX stands for Non-linear Auto-Regressive model with eXogeneous inputs tionist Temporal Classification (CTC; Graves et al., 2006), performs simultaneous segmentation and recognition of sequences.", "startOffset": 104, "endOffset": 130}, {"referenceID": 7, "context": "Its deep variant currently holds the state-of-theart result in phoneme recognition on the TIMIT database (Graves et al., 2013).", "startOffset": 105, "endOffset": 126}, {"referenceID": 14, "context": "One of the earliest attempts to enable RNNs to handle long-term dependencies is the Reduced Description Network (Mozer, 1992; 1994).", "startOffset": 112, "endOffset": 131}, {"referenceID": 11, "context": "This technique was recently picked up by Echo State Networks (ESN; Jaeger, 2002).", "startOffset": 61, "endOffset": 80}, {"referenceID": 23, "context": "Evolino (Schmidhuber et al., 2005; 2007) feeds the input to an RNN (which can be e.", "startOffset": 8, "endOffset": 40}, {"referenceID": 25, "context": "HF optimization allowed for training of Multiplicative RNN (MRNN; Sutskever et al., 2011) that port the concept of multiplicative gating units to SRNs.", "startOffset": 59, "endOffset": 89}, {"referenceID": 21, "context": "The Sequence Chunker, Neural History Compressor or Hierarchical Temporal Memory (Schmidhuber, 1991; 1992) consists of a hierarchy or stack of RNN that may run at different time scales, but, unlike the simpler CW-RNN, it requires unsupervised event predictors: a higher-level RNN receives an input only when the lower-level RNN below is unable to predict it.", "startOffset": 80, "endOffset": 105}, {"referenceID": 2, "context": "Each sequence contains an audio signal of one spoken word from the TIMIT Speech Recognition Benchmark (Garofolo et al., 1993).", "startOffset": 102, "endOffset": 125}, {"referenceID": 13, "context": "Each sequence element consists of 12-dimensional MFCC vector (Mermelstein, 1976) plus energy, sampled every 10ms over a 25ms window with a pre-emphasis coefficient of 0.", "startOffset": 61, "endOffset": 80}], "year": 2014, "abstractText": "Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.", "creator": "LaTeX with hyperref package"}}}