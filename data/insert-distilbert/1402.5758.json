{"id": "1402.5758", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2014", "title": "Bandits with concave rewards and convex knapsacks", "abstract": "further in this paper, we also consider selecting a very general model for exploration - exploitation tradeoff which allows restricting arbitrary concave rewards and convex constraints on the decisions across time, in addition to the customary limitation on the time horizon. this model subsumes the classic multi - armed bandit ( mab ) model, and the bandits with knapsacks ( bwk ) model of abdullah badanidiyuru et al. [ 2013 ]. we also consider an extension of this model to allow linear contexts, similar to the linear contextual extension of the mab model. we demonstrate that a natural and simple extension of the ucb family of algorithms for mab provides a polynomial time algorithm that has near - optimal regret guarantees for this substantially more general model, and matches the bounds uniquely provided by badanidiyuru et al. [ 2013 ] for the special case of bwk, which is quite surprising. we also provide computationally more efficient algorithms by establishing interesting cross connections between this problem and other well studied problems / algorithms such as the blackwell approachability problem, online convex optimization, and the frank - wolfe technique for convex optimization. we shall give examples of several concrete applications, where this more general model of bandits allows for richer and / or more efficient formulations depending of the swarm problem.", "histories": [["v1", "Mon, 24 Feb 2014 09:27:18 GMT  (99kb)", "http://arxiv.org/abs/1402.5758v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shipra agrawal", "nikhil r devanur"], "accepted": false, "id": "1402.5758"}, "pdf": {"name": "1402.5758.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 2.\n57 58\nv1 [\ncs .L\nG ]\n2 4\nFe b\n20 14\nX\nBandits with concave rewards and convex knapsacks\nSHIPRA AGRAWAL, Microsoft Research NIKHIL R. DEVANUR, Microsoft Research\nIn this paper, we consider a very general model for exploration-exploitation tradeoff which allows arbitrary concave rewards and convex constraints on the decisions across time, in addition to the customary limitation on the time horizon. This model subsumes the classic multi-armed bandit (MAB) model, and the Bandits with Knapsacks (BwK) model of Badanidiyuru et al. [2013]. We also consider an extension of this model to allow linear contexts, similar to the linear contextual extension of the MAB model. We demonstrate that a natural and simple extension of the UCB family of algorithms for MAB provides a polynomial time algorithm that has near-optimal regret guarantees for this substantially more general model, and matches the bounds provided by Badanidiyuru et al. [2013] for the special case of BwK, which is quite surprising. We also provide computationally more efficient algorithms by establishing interesting connections between this problem and other well studied problems/algorithms such as the Blackwell approachability problem, online convex optimization, and the Frank-Wolfe technique for convex optimization.\nWe give examples of several concrete applications, where this more general model of bandits allows for\nricher and/or more efficient formulations of the problem."}, {"heading": "1. INTRODUCTION", "text": "Multi-armed bandit (henceforth, MAB) is a classic model for handling explorationexploitation tradeoff inherent in many sequential decision making problems. MAB algorithms have found a wide variety of applications in clinical trials, web search, internet advertising, multi-agent systems, queuing and scheduling etc. The classic MAB framework however only handles \u201clocal\u201d constraints and \u201clocal\u201d rewards: the constraint is only on the decision in each step and the total reward is necessarily a summation of the rewards in each step. (The only constraint allowed on decisions accross time is a bound on the number of trials.) For many real world problems there are multiple complex constraints on resources that are consumed during the entire decision process. Further, in some applications it may be desirable to evaluate the solution not simply by the sum of rewards obtained at individual time steps, but by a more complex utility function. We illustrate several such example scenarios in our Applications section (Section 3). This paper, in succession to the recent results by Badanidiyuru et al. [2013], extends the MAB framework to handle very general \u201cglobal\u201d constraints and rewards. Badanidiyuru et al. [2013] took the first step in this direction by successfully extending the MAB model to include linear knapsack constraints on the resources consumed over time. In their model, which they call Bandits with Knapsacks (BwK), decision at any time t results in a reward and a d-dimensional resource consumption vector, and there is a pre-specified budget representing the maximum amount of each resource that can be consumed in time t. Badanidiyuru et al. [2013] combine techniques from UCB family of algorithms for MAB, and techniques from online learning algorithms in a non-trivial manner to provide an algorithm with near-optimal regret guarantees for this problem. In this paper, we introduce a substantial generalization of the BwK setting, to include arbitrary concave rewards and arbitrary convex constraints. In our vector-valued bandit model, decision at any time t results in the observation of a d-dimensional vector vt. There is a prespecified convex set S and a prespecified concave obective function f , and the goal is that the average of the observed vectors in time T belongs to the specified convex set while maximizing the concave objective. This is essentially the most general convex optimization problem. We refer to this model as \u201cBandits with Convex knapsacks and concave Rewards\u201d (henceforth, BwCR). We also consider an ex-\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\ntension of BwCR to allow contexts, simiar to the linear contextual bandits extesion of MAB [Chu et al. 2011]. BwCR subsumes BwK as a special case when the convex set is simply given by the knapsack constraints, and the objective function is linear. We discuss applications in several domains such as sensor measurements, network routing, crowdsourcing, pay-per-click advertising, which substantially benefit from the more general BwCR framework \u2013 either by admitting richer models, or by more efficient formulation of existing models. Another important contribution of this paper is to demonstrate that a conceptually simple and natural extension of the UCB family of algorithms for MAB [Auer et al. 2002; Auer 2003] provides near-optimal regret bounds for this substantially more general BwCR setting, and even for the contextual version of BwCR. Even in the special case of BwK, this natural extension of UCB algorithm achieves regret bounds matching the problem-dependent lower (and upper) bounds provided by Badanidiyuru et al. [2013]. This is quite surprising and is in contrast to the discussion in Badanidiyuru et al. [2013], where the need for special techniques for this problem was emphasized, in order to achieve sublinear regret. However, this natural extension of the UCB algorithm for BwCR, even though polynomial-time implementable (as we show in this paper), may not be very computationally efficient. For example, our UCB algorithm for the special case of BwK requires solving an LP with m variables and d constraints at every time step. In general, we show that one would require solving a convex optimization problem by ellipsoid method at every time step, for which computing separating hyperplanes itself needs another application of the ellipsoid algorithm. Our final contribution is giving computationally more efficient algorithms by establishing (sometimes surprising) connections between the BwCR problem and other well studied problems/algorithms such as the Blackwell approachability problem [Blackwell 1956], online convex optimization [Zinkevich 2003], and the Frank-Wolfe (projection-free) algorithm for convex optimization [Frank and Wolfe 1956]. We provide two efficient algorithms, a \u201cprimal\u201d algorithm based on the Frank-Wolfe algorithm and a \u201cdual\u201d algorithm based on the reduction of Blackwell approachability to online convex optimization [Abernethy et al. 2011]. One may be faster than the other depending on the properties of the objective function f and convex set S. As an aside, the primal algorithm establishes a connection between Blackwell\u2019s algorithm for the approachability problem and the Frank-Wolf algorithm. The dual algorithm turns out to be almost identical to the primal-dual algorithm (PD-BwK) of Badanidiyuru et al. [2013] for the special case of BwK problem."}, {"heading": "2. PRELIMINARIES AND MAIN RESULTS", "text": ""}, {"heading": "2.1. Bandit with knapsacks (BwK)", "text": "The following problem was called Bandit with Knapsacks (BwK) by Badanidiyuru et al. [2013]. There is a fixed and known finite set of m arms (possible actions), available to the learner, henceforth called the algorithm. There are d resources and finite time-horizon T , where T is known to the algorithm. In each time step t, the algorithm plays an arm it of the m arms, receives reward rt \u2208 [0, 1], and consumes amount ct,j \u2208 [0, 1] of each resource j. The reward rt and consumption ct \u2208 Rd are revealed to the algorithm after choosing arm it. The rewards and costs in every round are generated i.i.d. from some unknown fixed underlying distribution. More precisely, there is some fixed but unknown \u00b5 \u2208 Rm,C \u2208 Rd\u00d7m such that E[rt|it] = \u00b5it , E[ct,j(t)|it] = Cj,it . In the beginning of every time step t, the algorithm needs to pick it, using only the history of plays and outcomes until time step t \u2212 1. There is a hard constraint of Bj\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\non the resource consumption of every j. The algorithm stops at the earliest time \u03c4 when one or more of the constraints is violated, i.e. if \u2211\u03c4 t=1 ct,j(t) > Bj for some j, or if the time horizon ends, i.e. \u03c4 > T . Its total reward is given by the sum of rewards in all rounds preceding \u03c4 , i.e. \u2211\u03c4\u22121\nt=1 rt. The goal of the algorithm is to maximize the expected total reward. The values of Bj are known to the algorithm, and without loss of generality we can assume Bj = B = minj Bj for all j. (Multiply each ct,j by B/Bj .)\nRegret and Benchmark. Regret is defined as the difference in the total reward obtained by the algorithm and OPT, where OPT denotes the total expected reward for the optimal dynamic policy.\nregret(T ) = OPT\u2212\u22111\u2264t<\u03c4 rt. (1)\nFor any \u00b5,C, let LP(\u00b5,C) denote the value of the following linear program.\nmaxp \u00b5 \u00b7 p s.t. Cp B\nT 1, p \u2208 \u2206m (2)\nwhere \u2206m denotes the m-dimensional simplex, i.e., \u2206m = {p : \u2211m\ni=1 pi = 1, pi \u2265 0, i = 1, . . . ,m}, and, , denote component-wise \u2264 and \u2265 respectively. It is easy to show that LP(\u00b5,C) \u2265 OPT\nT . (For example, see Devanur et al. [2011], or Lemma 3.1 of\nBadanidiyuru et al. [2013].) Hence T \u00b7 LP(\u00b5,C) is commonly used in place of OPT in the analysis of regret."}, {"heading": "2.2. Bandits with concave rewards and convex knapsacks (BwCR)", "text": "In this paper we consider a substantial generalization of BwK, to include arbitrary concave rewards and arbitrary convex constraints. This is essentially the most general convex optimization problem. We consider the problem with only convex constraints (BwC), and the problem with only concave rewards (BwR) as special cases. In the Bandits with concave rewards and convex knapsacks (BwCR) setting, on playing an arm it at time t, we observe a vector vt \u2208 [0, 1]d generated independent of the previous observations, from a fixed but unknown distribution such that E[vt|it] = V it , where V \u2208 [0, 1]d\u00d7m. We are given a convex set S, and a concave objective function f : [0, 1]d \u2192 [0, 1]. We further make the following assumption regarding Lipschitz continuity of f .\nASSUMPTION 1. Assume that function f is L-lipschitz with respect to norm || \u00b7 ||, i.e., f(x) \u2212 f(y) \u2264 L||x\u2212 y||. Since f is concave, this is equivalent to the condition that for all x in the domain of f , and all supergradients g \u2208 \u2202f(x), we have that ||g||\u2217 \u2264 L, where || \u00b7 ||\u2217 is the dual norm (refer to Lemma 2.6 in [Shalev-Shwartz 2012]). The goal is to make the average of the observed vectors 1\nT \u2211 t vt be contained in the\nset S, and at the same time maximize f( 1 T \u2211 t vt). Let OPTf denote the expected value of the optimal dynamic solution to this problem. Then, the following lemma provides a benchmark for defining regret. The proof follows simply from concavity of f , and is provided in Appendix A.\nLEMMA 2.1. There exists a distribution p\u2217 \u2208 \u2206m, such that V p\u2217 \u2208 S, and f(V p\u2217) \u2265 OPTf .\nWe minimize two kinds of regret: regret in objective and regret in constraints. The (average) regret in objective is defined as\navg-regret1(T ) := OPTf \u2212 f( 1T \u2211T t=1vt) \u2264 f(V p\u2217)\u2212 f( 1T \u2211T t=1vt). (3)\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nAnd, (average) regret in constraints is the distance of average observed vector from S,\navg-regret2(T ) := d( 1 T \u2211T t=1vt, S), (4)\nwhere the distance function d(x, S) is defined as ||x\u2212\u03c0S(x)||, \u03c0S(x) is the projection of x on S, and || \u00b7 || denotes an Lq norm. Below, we describe some special cases and extensions of this setting.\nHard constraints. In some applications, the constraints involved are hard constraints, that is, it is desirable that they are satisfied with high probability even if at a cost of higher regret in the objective. Therefore, we may want to tradeoff the regret in distance from S for possibly more regret in objective f . While this may not be always doable, under following conditions a simple modification of our algorithm can achieve this: the set S and function f are such that it is easy to define and use a shrunken set S\u01eb for any \u01eb \u2208 [0, 1], defined as a subset of S such that points within a distance of \u01eb from this set lie in S. And, S\u01eb contains at least one good point V p with objective function value within K\u01eb of the optimal value. More precisely,\nd(x, S\u01eb) \u2264 \u01eb \u21d2 x \u2208 S, and \u2203p \u2208 \u2206m : V p \u2208 S\u01eb, f(V p) \u2265 f(V p\u2217)\u2212K\u01eb, (5)\nfor some K \u2265 0. A special case is when S is a downward closed set, f is linear, and distance is L\u221e distance. In this case, we can define S\n\u01eb = {x(1 \u2212 \u01eb), \u2200x \u2208 S}, for which V p\u2217(1 \u2212 \u01eb) \u2208 S\u01eb and f(V p\u2217(1\u2212 \u01eb)) \u2265 (1\u2212 \u01eb)f(V p). In our algorithms, we will be able to simply substitute S\u01eb for S to achieve the desired tradeoff. This observation will be useful for BwK problem, which involves hard (downward closed) resource consumption constraints \u2013 the algorithm needs to abort when the resource constraints are violated.\nLinear contextual version of BwCR. We also consider an extension of our techniques to the linear contextual version of the BwCR problem, which can be derived from the linear contextual bandits problem [Auer et al. 2002; Chu et al. 2011]. In this setting, every arm i and component j is associated with a context vector bji, which is known to the algorithm. There is an unknown n-dimensional weight vector wj for every component j, such that V ji = bji \u00b7wj . Note that effectively, the d n-dimensional weight vectors are the unknown parameters to be learned in this problem, where n could be much smaller than the number of arms m. Algorithms for contextual bandits are expected to take advantage of this structure of the problem to produce low regret guarantees even when the number of arms is large. In a more general setting, the context vector for arm i could even change with time (but are provided to the algorithm before taking the decision at time t), however that can be handled with only notational changes to our solution, and for simplicity of illustration, we will restrict to static contexts in the main body of this paper.\nBwK, BwR, and BwC as special cases. Observe that BwCR subsumes the BwK problem, on defining objective function f(x) = x1, and S := {x : x\u22121 \u2264 BT 1}. We define Bandits with concave Rewards (BwR) as a special case of BwCR when there are no constraints, i.e., the set S = Rn. And, Bandits with Convex knapsacks (BwC) as the special case when the goal is only to satisfy the constraints, i.e. there is no objective function f . The average regret for BwR in time T is avg-regret1(T ), and for BwC it is avg-regret2(T )."}, {"heading": "2.3. Summary of Results", "text": "Our main result is that a natural extension of UCB algorithm (Algorithm 1) for BwCR achieves bounds of\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nO(L||1d|| \u221a m T ln(mTd \u03b4 )), and O(||1d|| \u221a m T ln(mTd \u03b4 )),\nwith probability 1 \u2212 \u03b4, on the average regret in the objective (avg-regret1(T )) and distance from constraint set (avg-regret2(T )), respectively. Here ||1d|| denotes the norm of d-dimensional vector of all 1\u2019s, with respect to the norm used in the Lipschitz condition of f , and, in defining the distance from set S, respectively. We extend our results to the linear contextual version of BwCR, and provide an algorithm with average regret bounds of\nO(Ln||1d|| \u221a 1 T ln(Td \u03b4 )), and O(n||1d|| \u221a 1 T ln(Td \u03b4 )),\nrespectively, when contexts are of dimension n. Note that these regret bounds do not depend on the number of arms m, which is crucial when number of arms is large, possibly infinite. Note that BwCR subsumes the MAB problem, and the contextual version of BwCR subsumes the linear contextual bandits problem, with d = 1, L = 1 and S = Rn. And, our regret bounds for these problems match the lower bounds provided in Bubeck and Cesa-Bianchi [2012] (Section 3.3) and Dani et al. [2008], respectively, within logarithmic factors. A more refined problem-dependent lower bound (and matching upper bound) for the special case of BwK was provided in [Badanidiyuru et al. 2013]. We show that our UCB algorithm when specialized to this case (Algorithm 2) achieves a regret bound of\nregret(T ) = O (\u221a log(mdT\n\u03b4 )(OPT \u221a m B + \u221a mOPT+m \u221a log(mTd \u03b4 )) ) ,\nwhich matches the bounds of [Badanidiyuru et al. 2013]. Thus, our UCB based algorithms provide near-optimal regret bounds. Precise statements of these results appear as Theorem 4.1 and Theorem 4.2. Section 5 and 6 are devoted to developing a general framework for converting the UCB algorithm to fast algorithms. We provide algorithms BwC and BwR for which the arm selection problem at time t is simply of the form:\nit = argmaxi=1,...,m \u03c9t,i.\nwhere \u03c9t,i for every i, can be computed using history until time t \u2212 1 in O(d) time. These fast algorithms can be viewed as approximate primal and dual implementations of the UCB algorithm, and come with a cost of increased regret, but we show that the regret increases by only constant factors. The derivation of these fast algorithms from UCB also provides interesting insights into connections between this problem, the Blackwell approachability problem, and the Frank-Wolfe projection technique for convex optimization, which may be of independent interest."}, {"heading": "2.4. Related Work", "text": "The BwCR problem, as defined in the previous section, is closely related to the stochastic multi-armed bandits (MAB) problem, to the generalized secretary problems under stochastic assumption, and to the Blackwell approachability problem. As we mentioned in the introduction, the major difference between the classic MAB model and settings like BwCR (or BwK) is that the latter allow for \u201cglobal\u201d constraints \u2013 constraints on decisions accross time. The only global constraint allowed in the classic MAB model is the time horizon T . Generalized secretary problems under i.i.d. distribution include online stochastic packing and covering problems (e.g., [Devanur et al. 2011], [Feldman et al. 2010]).\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nThese problems involve \u201cglobal\u201d packing or covering constraints on decisions over time, as we have in BwCR. However, a major difference between the secretary problems and a bandit setting like BwCR is that in secretary problems, before taking the decision at time t the algorithm knows howmuch the reward or consumption (or in general vt) will be for every possible decision. On the other hand, in the BwCR setting, vt is revealed after the algorithm chooses the arm to play at time t. One of the ideas in this paper is to estimate the observations at time t by UCB estimates computed using only history til time t \u2212 1, and before choosing the arm it. This effectively reduces the problem to secretary problem, with error in the UCB estimates to account for in regret bounds. Blackwell approachability problem considers a two player vector-valued game with a bi-affine payoff function, r(p, q) = pTMq. Further, it is assumed that for all q, there exists a p such that r(p, q) \u2208 S. The row player\u2019s goal is to direct the payoff vector to some convex set S. The Bandit with convex knapsacks (BwC) problem is closely related to the Blackwell approachability problem. The row player is the online algorithm and the column player is nature. However, in this case the nature always produces its outcome using a fixed (but unknown) mixed strategy (distribution) q\u2217. Also, this means a weaker assumption should suffice: there exists a p\u2217 for this particular q\u2217, such that r(p\u2217, q\u2217) \u2208 S (stated as the assumption \u2203p\u2217,V p\u2217 \u2208 S). The bigger difference algorithmically is that there is nothing to statistically estimate in the Blackwell approachability problem, the only unknown is the column player strategy which may change every time. On the other hand, esitmating the expected consumption is inherently the core part of any algorithm for BwC. Due to these differences, algorithms for none of these related problems directly solve the BwCR problem. Nonetheless, the similarities suffice to inspire many of the ideas for computationally efficient algorithms that we present in this paper. The work closest to our work is that of Badanidiyuru et al. [2013] on the BwK problem. We successfully generalize their setting to include arbitrary convex constraints and concave objectives, as well as linear contexts. Additionally, we demonstrate that a simple and natural extension of UCB algorithm suffices to obtain optimal regret for BwCR which subsumes BwK, and provide generalized techniques for deriving multiple efficient implementations of this algorithm \u2013 one of which reduces to an algorithm similar to the PD-BwK algorithm of Badanidiyuru et al. [2013] for the speical case of BwK."}, {"heading": "2.5. Fenchel duality", "text": "Fenchel duality will be used throughout the paper, below we provide some background on this useful mathematical concept. We define the Fenchel conjugate of f as\nf\u2217(\u03b8) := maxy\u2208[0,1]d{y \u00b7 \u03b8 + f(y)}\nSuppose that f is a concave function defined on [0, 1]d, and as in Assumption 1, at every point x, every supergradient gx of f has bounded dual norm ||gx||\u2217 \u2264 L. Then, the following dual relationship is known between f and f\u2217. A proof is provided in Appendix A for completeness.\nLEMMA 2.2. f(z) = min||\u03b8||\u2217\u2264L f \u2217(\u03b8)\u2212 \u03b8 \u00b7 z.\nA special case is when f(x) = \u2212d(x, S) for some convex set S. This function is 1- Lipschitz with respect to norm || \u00b7 || used in the definition of distance. In this case, f\u2217(\u03b8) = hS(\u03b8) := maxy\u2208S \u03b8 \u00b7 y, and Lemma 2.2 specializes to\nd(x, S) = max||\u03b8||\u2217\u22641 \u03b8 \u00b7 x\u2212 hS(\u03b8). The derivation of this equality also appears in Abernethy et al. [2011].\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014."}, {"heading": "2.6. Notations", "text": "We use bold alphabets or bold greek letters for vectors, and bold capital letters for matrices. Most matrices used in this paper will be d\u00d7m dimensional, and for a matrix A, Aji denotes its ji th element, Ai denotes its i th column vector, and Aj its j\nth row vector. For matrices which represent time dependent estimates, we use At for the matrix at time t, andAt,i,At,j and At,ji for its i\nth column, jth row, and ji component, respectively. For two vectors x,y, x \u00b7 y denotes the inner product."}, {"heading": "3. APPLICATIONS", "text": "Below, we demonstrate that BwCR setting and its extension to contextual bandits allows us to effectively handle much richer and complex models in applications like sensor networks, crowdsourcing, pay-per-click advertising etc., than those permitted by multi-armed bandits (MAB), or bandits with knapsacks (BwK) formulations. While some of these simply cannot be formulated in the MAB or BwK frameworks, others would require an exponential blowup of dimensions to convert the convex constraints to linear knapsack or covering constraints.\nSensor networks, network routing. Consider a sensor network with m sensors, each sensor i covering a subset Ai of N points, where N >> m, and N could even be exponential compared to m. Taking a reading from any sensor costs energy. Also, a sensor measurement may fail with probability qi. The aim is to take atmost T measurements such that each point has at least b successful readings. We are given that there exists a strategy for selecting the sensors, so that in expectation these covering constraints can be satisfied. A strategy corresponds to a distribution p \u2208 \u2206m such that you measure sensor i with probability pi. We are given that\n\u2203p\u2217 \u2208 \u2206m, T \u2211\ni:k\u2208Ai p\u2217i qi \u2265 b, \u2200k = 1, . . . , N.\nWe can model this as BwC by having vt \u2208 {0, 1}m (i.e., d = m), where on playing arm it, vt,it denotes whether the sensor it was successfully measured or not: vt,it = eit with probability qit , and 0 otherwise, and E[vt|it] = V it where V is am m\u00d7m diagonal matrix with V ii = qi. Define S as\nS = {x \u2208 [0, 1]m : \u2211i:k\u2208Ai xi \u2265 b T , k = 1, . . . , N}.\nNote that S is an m-dimensional convex set. Then, we wish to achieve 1 T \u2211T t=1 vt \u2208 S. And, from above there exists p\u2217 \u2208 \u2206m such that V p\u2217 \u2208 S. Our algorithms we will obtain O(||1m|| \u221a m T log(mT \u03b4 )) regret as per the results metioned above (d = m).\nNote that if we try to frame this problem in terms of linear covering constraints, we need to make vt to be N dimensional (i.e, d := N ), where N >> m. On playing arm i, vt = eAi with probability pi. Then, the constraints can be written as linear constraints\u2211\nt vt,j \u2265 b, j = 1, . . . , N. However, in that case, d = N will result in a ||1N || \u221a log(N)\nterm in the regret bound, which can be exponentially worse than ||1m|| \u221a log(m).\nSimilar applications include crowdsourcing a survey or data collection task, where workers are sensors each covering his/her (overlapping) neighborhood, and network monitoring, where monitors located at some nodes of the network are sensors, each covering a subset of the entire network. Another similar application is network routing, where routing requests are arriving online. There is a small number (d) of request types, and the hidden parameters to learn are expected usage for each type of request. But, there is a capacity constraint on each of the N >> d edges. Then, modeling it as BwK would get an ||1N || \u221a log(N)\nterm in the regret bound instead of ||1d|| \u221a log(d).\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nPay-per-click advertising. Pay-per-click advertising is one of the most touted applications for MAB, where explore-exploit tradeoff is observed in ad click-through rate (CTR) predictions. Our BwCR formulation with its contextual extension can considerably enrich the MAB formulations of this problem. Contexts are considered central to the effective use of bandit techniques in this problem \u2013 the CTR for an ad impression depends on the (query, ad) combination, and there are millions of these combinations, thus millions of arms. Contextual setting allows a compact representation of these arms as n-dimensional feature (context) vectors, and aims at learning the best weight vector that maps features to CTR. BwCR allows using the contextual setting along with multiple complex constraints on the decision process over time. In addition to simple budget constraints for every advertiser/campaign, we can efficiently represent budget constraints on family of overlapping subset of those, without blowing up the dimension d, as explained in some of our earlier applications. The ability to maximize a concave reward function is also very useful for such applications. Although in most models of pay-per-click advertising the reward is some simple linear function, the reality is more complex. A typical consideration is that advertisers (in dislay advertising) desire a certain mixture of different demographics such as equal number of men and women, or equal number of clicks from different cities. These are not hard constraints \u2013 the closer to the ideal mixture, the better it is. This is naturally modeled as a concave reward function of the vector of the number of clicks of each type the advertiser recieves. Further, we can now admit more nuanced risk-sensitive constraints. This includes convex risk functions on budget expenditure or on distance from the target click or revenue performance."}, {"heading": "4. UCB FAMILY OF ALGORITHMS", "text": "In this section, we present algorithms derived from the UCB family of algorithms [Auer et al. 2002] for the multi-armed bandit problems. We demonstrate that simple extensions of UCB algorithm provide near-optimal regret bounds for BwCR and all its extensions introduced earlier. In particular, our UCB algorithm will match the optimal regret bound provided by Badanidiyuru et al. [2013] for the special case of BwK. We start with some background on the UCB algorithm for classic multi-armed bandit problem. In the classic multi-armed bandit problem there are m arms and on playing an arm it at time t, a reward rt is generated i.i.d. with fixed but unknown mean \u00b5it . The objective is to choose arms in an online manner in order to minimize regret defined as \u2211T\nt=1(\u00b5i\u2217 \u2212 rt), where i\u2217 = argmaxi \u00b5i. UCB algorithm for multi-armed bandits was introduced in Auer et al. [2002]. The basic idea behind this family of algorithms is to use the observations from the past plays of each arm i at time t to construct estimates (UCBt,i) for the mean reward \u00b5i. These estimates are constructed to satisfy the following key properties.\n(1) The estimate UCBt,i for every arm is guaranteed to be larger than its mean reward with high probability, i.e., it is an Upper Confidence Bound on the mean reward.\nUCBt,i \u2265 \u00b5i, \u2200i, t (2) As an arm is played more and more, its estimate should approach the actual mean\nreward, so that with high probability, the total difference between estimated and actual reward for the played arms can be bounded as\n| \u2211T t=1(UCBt,it \u2212 rt)| \u2264 O\u0303( \u221a mT ).\nThis holds irrespective of how the arm it is chosen.\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nAt time t, the UCB algorithm simply plays the best arm according to the current estimates, i.e., the arm with the highest value of UCBt,i.\nit = argmaxi UCBt,i.\nThen, a corollary of the first property above, and the choice of arm made by algorithm, is that with high probability, UCBt,it \u2265 \u00b5i\u2217 . Using above observations, it is straightforward to bound the regret of this algorithm in time T .\nregret(T ) = \u2211T t=1(\u00b5i\u2217 \u2212 rt) \u2264 \u2211T t=1(UCBt,it \u2212 rt) \u2264 O\u0303( \u221a mT ).\nIn our UCB based algorithms, we use this same basic idea for algorithm design and regret analysis."}, {"heading": "4.1. Bandits with concave rewards and convex knapsacks (BwCR)", "text": "Since, our observation vector cannot be interpreted as cost or reward, we construct both lower and upper confidence bounds, and consider the range of estimates defined by these. More precisely, for every arm i and component j, we construct two estimates LCBt,ji(V ) and UCBt,ji(V ) at time t, using the past observations. The estimates for each component are constructed in a manner similar to the estimates used in the UCB algorithm for classic MAB, and satisfy the following generalization of the properties mentioned above.\n(1) The mean for every arm i and component j is guaranteed to lie in the range defined by its estimates LCBt,ji(V ) and UCBt,ji(V ) with high probability. That is,\nV \u2208 Ht, where, (6)\nHt := {V\u0303 : V\u0303ji \u2208 [LCBt,ji(V ),UCBt,ji(V )], j = 1, . . . , d, i = 1, . . . ,m}. (7) (2) Let arm i is played with probability pt,i at time t. Then, the total difference between\nestimated and actual observations for the played arms can be bounded as ||\u2211Tt=1(V\u0303 tpt \u2212 vt)|| \u2264 Q(T ), (8) for any {V\u0303 t}Tt=1 such that V\u0303 t \u2208 Ht. Here, Q(T ) is typically O\u0303(||1d|| \u221a mT ).\nA direct generalization of Property (2) from the MAB analysis mentioned before would have been a bound on ||\u2211Tt=1(V\u0303 t,it \u2212 vt)||. However, since we will choose a distribution pt over arms at time t and sample it from this distribution, the form of bound in (8) is more useful, and a straightforward extension. A specialized expression for Q(T ) in terms of problem specific parameters will be obtained in the specific case of BwK. As before, these are purely properties of the constructed estimates, and hold irrespective of how the choice of pt is made by an algorithm. At time t, our UCB algorithm plays the best arm (or, best distribution over arms) according to the best estimates in set Ht. ALGORITHM 1: UCB Algorithm for BwCR\nfor all t = 1, 2, . . . , T do\npt = arg max p\u2208\u2206m max U\u0303\u2208Ht f(U\u0303p)\ns.t. minV\u0303 \u2208Ht d(V\u0303 p, S) \u2264 0 (9)\nIf no feasible solution is found to the above problem, set pt arbitrarily. Play arm i with probability pt,i.\nend for\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nObserve that when f(\u00b7) is a monotone non-decreasing function as in the classic MAB problem (where f(x) = x), the inner maximizer in objective of (9) will be simply U\u0303 t = UCBt(V ), and therefore, for classic MAB problem this algorithm reduces to the UCB algorithm.\nLet U\u0303 t, V\u0303 t denote the inner maximizer and the inner minimizer in the problem (9). Then, a corollary of the first property above (refer to Equation (6)) is that with high probability, f(U\u0303 tpt) \u2265 f(V p\u2217), V\u0303 tpt \u2208 S. (10) This is because the conditions V \u2208 Ht and V p\u2217 \u2208 S imply that (p, V\u0303 , U\u0303) = (p\u2217,V ,V ) forms a feasible solution for problem (9) at time t. Using these observations, it is easy to bound the regret of this algorithm in time T . With high probability,\navg-regret1(T ) \u2264 f(V p\u2217)\u2212 f( 1T \u2211T t=1vt) \u2264 f(U\u0303 tpt)\u2212 f( 1T \u2211T t=1vt) \u2264 LT Q(T ),\navg-regret2(T ) = d( 1 T \u2211T t=1 vt, S) \u2264 d( 1T \u2211T t=1vt, 1 T \u2211T t=1V\u0303 tpt) \u2264 1T Q(T ),\n(11)\nwhere Q(T ) = O\u0303(||1d|| \u221a mT ). Below is a precise statement for the regret bound.\nTHEOREM 4.1. With probability 1\u2212 \u03b4, the regret of Algorithm 1 is bounded as avg-regret1(T ) = O(L||1d|| \u221a \u03b3m T ), avg-regret2(T ) = O(||1d|| \u221a \u03b3m T )\nwhere \u03b3 = O(log(mTd \u03b4 )), 1d is the d dimensional vector of all 1\u2019s.\nThe detailed proof with exact expressions for UCBt(V ),LCBt(V ) is in Appendix B.1.\n4.1.1. Extensions.\nLinear Contextual Bandits.. It is straightforward to extend Algorithm 1 to linear contextual bandits, using existing work on UCB family of algorithms for this problem. Using techniques in Abbasi-yadkori et al. [2012]; Auer [2003], instead of the hypercube Ht at time t, one can obtain an ellipsoid such that the weight vector wj is guaranteed to lie in this elliposid, for every component j. Then, simply substituting Ht with these ellipsoids in Algorithm 1 will provide an algorithm for the linear contextual version of BwCR with regret bounds\navg-regret1(T ) = O(Ln||1d|| \u221a \u03b3 T ), avg-regret2(T ) = O(n||1d|| \u221a \u03b3 T ),\nwith probability 1\u2212 \u03b4. Here \u03b3 = O(log(mTd \u03b4 )). Further details are in Appendix B.2.\nHard constraints. In this case, a shrunket set S\u01eb can be used instead of S in Algorithm 1 (refer to Section 2.2 for definition of S\u01eb), with \u01eb set to be an upper bound on avg-regret2(T ). For example, \u01eb can be set as ||1d|| \u221a \u03b3m T\nusing results in Theorem 4.1. Then, at the end of time horizon, with probability 1\u2212 \u03b4, the algorithm will satisfy,\nd( 1 T \u2211 t vt, S \u01eb) \u2264 \u01eb \u21d2 1 T \u2211 t vt \u2208 S, and avg-regret1(T ) = O(L||1d|| \u221a \u03b3m T +K\u01eb)."}, {"heading": "4.2. Bandit with knapsacks (BwK)", "text": "This is a special case of BwCR with vt = {rt; ct}, f(x) = x1, and S = {x : x\u22121 \u2264 BT 1}. Then, the problem (9) in Algorithm 1 reduces to the following LP.\nmaxp\u2208\u2206m UCBt(\u00b5) \u00b7 p s.t. LCBt(C)p BT 1,\n(12)\nwhere UCBt(\u00b5) \u2208 [0, 1]m denotes the UCB estimate constructed for \u00b5 and LCBt(C) \u2208 [0, 1]d\u00d7m denotes the LCB estimate for C. Above is same as LP(UCBt(\u00b5),LCBt(C)) (refer to Equation (2)).\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nSince this problem requires hard constraints on resource consumption, we would like to tradeoff the regret in constraint satisfaction for some more regret in reward. As discussed in Section 4.1.1, one way to achieve this is to use a shrunken constraint set. For any \u00b5,C, we define LP(\u00b5,C, \u01eb) by tightneing the constraints in LP(\u00b5,C) by a 1 \u2212 \u01eb factor, i.e. replacing B by (1 \u2212 \u01eb)B. Then, at time t, the algorithms simply solves LP(UCBt(\u00b5),LCBt(C), \u01eb) instead of LP(UCBt(\u00b5),LCBt(C)).\nALGORITHM 2: UCB algorithm for BwK\nfor all t = 1, 2, . . . , T do Exit if any resource consumption is more than B. Solve LP(UCBt(\u00b5),LCBt(C), \u01eb), and let pt denote the solution for this linear program. Play arm i with probability pt,i. end for\nTHEOREM 4.2. For the BwK problem, with probability 1\u2212\u03b4, the regret of Algorithm 2 with \u01eb = \u221a \u03b3m B + log(T )\u03b3m B , \u03b3 = log(mTd \u03b4 ), is bounded as\nregret(T ) = O (\u221a log(mdT\n\u03b4 )(OPT \u221a m B + \u221a mOPT+m \u221a log(mTd \u03b4 )) ) .\nPROOF. We use the same estimates for each component as in the previous section, to construct UCBt(\u00b5) and LCBt(C). We show that these UCB and LCB estimates satisfy the following more specialized versions of the properties given by Equation (6) and (8). With probability 1\u2212 (mTd)e\u2212\u2126(\u03b3),\n(1) UCBt(\u00b5) \u00b5,LCBt(C) C. (13)\n(2)\n\u2211T t=1(UCBt(\u00b5) \u00b7 pt \u2212 rt)| \u2264 O( \u221a \u03b3m ( \u2211 trt) + \u03b3m), |\u2211Tt=1(LCBt(C)pt \u2212 ct)| \u01ebB1. (14)\nProof of the second property is similar to Lemma 7.4 of [Badanidiyuru et al. 2013], and is provided in Appendix B.3 for completeness. Then, similar to (10), following is a corollary of the first property and the choice made by the algorithm at time step t. \u2211T\nt=1 UCBt(\u00b5) \u00b7 pt = LP(UCBt(\u00b5),LCBt(C), \u01eb) \u2265 LP(\u00b5,C, \u01eb) \u2265 (1\u2212 \u01eb)OPT, \u2211T\nt=1 LCBt(C)pt (1 \u2212 \u01eb)B1. (15)\nThen, using the second property above and (15), \u2211T\nt=1 ct \u2264 B1, and the algorithm will not terminate before time T . This means that the total reward for the algorithm will be given by ALGO = \u2211T\nt=1 rt. Also, using the second property,\nALGO = \u2211T t=1 rt \u2265 (1\u2212 \u01eb)OPT\u2212O( \u221a\n\u03b3m ALGO)\u2212O(\u03b3m) Therefore, either ALGO \u2265 OPT or\nALGO \u2265 (1\u2212 \u01eb)OPT\u2212O( \u221a\n\u03b3mOPT)\u2212O(\u03b3m). Now, assuming m\u03b3 \u2264 O(B), 1 \u01ebOPT = O(OPT \u221a m\u03b3 B ). Therefore,\nregret(T ) = OPT\u2212 ALGO \u2264 O ( OPT \u221a \u03b3m B + \u221a \u03b3mOPT+ \u03b3m ) .\n1This assumption was also made in [Badanidiyuru et al. 2013]\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nThen, substituting \u03b3 = \u0398(log(mTd \u03b4 )), we get the desired result."}, {"heading": "4.3. Implementability", "text": "Next, we investigate whether our UCB algorithm is efficiently implementable. For the special case of BwK problem, this reduces to Algorithm 2 which only requires solving an LP at every step. However, the poynomial-time implementability of Algorithm 1 is not so obvious. Below, we prove that the problem (9) required to be solved in every time step t is in fact a convex optimization problem, with separating hyperplanes computable in polynomial time. Thus, this problem can be solved by ellipsoid method, and every step of Algorithm 2 can be implemented in polynomial time.\nLEMMA 4.3. The functions \u03c8(p) := maxU\u0303\u2208Ht f(U\u0303p), and g(p) = minV\u0303 \u2208Ht d(V\u0303 p, S) are concave and convex functions respectively, and the subgradients for these functions at any given point can be computed in polynomial time using ellipsoid method for convex optimization.\nThe proof of above lemma is provided in Appendix B.1."}, {"heading": "5. COMPUTATIONALLY EFFICIENT ALGORITHMS FOR BWC AND BWR", "text": "In the UCB algorithm for BwCR, at every time step t, we need to solve the optimization problem (9). Even though this can be done in polynomial time (Lemma 4.3), this is an expensive step. It requires solving a convex optimization problem in p (possibly using ellipsoid method), for which computing the separating hyperplane at any point itself requires solving a convex optimization problem (again, possibly using ellipsoid method). For practical reasons, it is desirable to have a faster algorithm. In this section, we present alternate algorithms that are very efficient computationally at the expense of a slight increase in regret. The regret bounds remain the same in the O(\u00b7) notation and the increase is only in the constants. We present two such algorithms, a \u201cprimal\u201d algorithm based on the Frank-Wolfe algorithm [Frank and Wolfe 1956] and a \u201cdual\u201d algorithm based on the reduction of the Blackwell approachability problem to online convex optimization (OCO) in Abernethy et al. [2011]. In this section, for simplicity of illustration, we consider only the BwC and BwR problems, i.e., the problem with only constraint set S, and the problem with only the objective function f , respectively. In Section 6 we show that one could use any combination of these algorithms, or the UCB algorithm, for each of BwC and BwR to get an algorithm for BwCR. The basic idea is to replace the convex optimization problem with its \u201clinearization\u201d, which turns out to be a problem of optimizing a linear function over the unit simplex, and hence very easy to solve. For the BwC problem, the convex optimization problem (9) specializes to finding a pt such that V\u0303 pt \u2208 S for some V\u0303 \u2208 Ht. In our \u201clinearized\u201d\u2019 version, instead of this, we will only need to find a pt such that V\u0303 pt is in a halfspace containing the set S. A half space that contains S and is tangential to S is given by a vector \u03b8; such a halfspace is HS(\u03b8) := {x : \u03b8 \u00b7 x \u2264 hS(\u03b8)}, where hS(\u03b8) := maxs\u2208S \u03b8 \u00b7 s. Now given a \u03b8t in time step t, a point in HS(\u03b8t) can be found by simply minimizing \u03b8t \u00b7 x, which is a linear function. This is exactly what the algorithm does, at each time step t, it picks a vector \u03b8t and sets\n(pt, V\u0303 t) = arg min p\u2208\u2206m min V\u0303 \u2208Ht\n\u03b8t \u00b7 (V\u0303 p). (16)\nThe inner minimization is actually trivial and the optimal solution is at a vertex ofHt, independent of the value of p, i.e., V\u0303 t = Zt(\u03b8t), where\nZt(\u03b8)ji := { UCBt,ji(V ), \u03b8j \u2264 0, LCBt,ji(V ), \u03b8j > 0 , (17)\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nfor j = 1, . . . , d, i = 1, . . . ,m. With this observation, the outer minimization is also quite simple, since it optimizes a linear function over the unit simplex and the optimal solution occurs at one of the vertices. It is solved by setting pt = eit , where\nit = arg min i\u2208{1,...,m}\n\u03b8t \u00b7 V\u0303 t,i. (18)\nHence given \u03b8t, the procedure for picking the arm it is quite simple. A generalization of this idea is used for BwR: instead of optimizing f , we optimize a linear function that is tangential to f . A linear function that is tangential to f at a point y (and is an upper bound on f since f is concave) is\nlf (x;y) := f(y) +\u2207f(y) \u00b7 (x\u2212 y) \u2265 f(x) \u2200x,y. Then, instead of maximizing f(V\u0303 p) as in (9), we maximize lf (V\u0303 p;yt) over V\u0303 \u2208 Ht and p \u2208 \u2206m, for some yt. The latter is equivalent to minimizing x \u00b7 \u03b8t where \u03b8t = \u2212\u2207f(yt), therefore pt is still set as per (16) (which reduces to the simple rule in (18)).\nWe introduce some notation here, let xt := V\u0303 tpt, x \u2217 := V p\u2217, x\u03041:t := 1 T \u2211t s=1 xs and\nv\u03041:t := 1 T \u2211t s=1 vs.\nThe regret bound for the UCB algorithm followed rather straight-forwardly from the two properties (6) and (8), but the regret bounds for these algorithms will not be as easy. For one, we no longer have (10), instead we have the corresponding relations for lf and HS respectively:\nlf(xt;yt) \u2265 lf (x\u2217;yt) \u2265 f(x\u2217), xt \u2208 HS(\u03b8t). (19)\nSince we don\u2019t have that f(xt) \u2265 f(x\u2217) (or xt \u2208 S), the main task is to bound f(x\u2217) \u2212 f(x\u03041:T ) (and d(x\u03041:T , S)), and these will be extra terms in the regret bound. In particular, (11) is replaced by\navg-regret1(T ) \u2264 f(x\u2217)\u2212 f(v\u03041:T ) \u2264 f(x\u2217)\u2212 f(x\u03041:T ) + f(x\u03041:T )\u2212 f(v\u03041:T ) \u2264 f(x\u2217)\u2212 f(x\u03041:T ) + LT Q(T ).\navg-regret2(T ) = d(v\u03041:T , S) \u2264 d(v\u03041:T , x\u03041:T ) + d(x\u03041:T , S) \u2264 1T Q(T ) + d(x\u03041:T , S). (20)\nThe bounds on f(x\u2217) \u2212 f(x\u03041:T ) and d(x\u03041:T , S) will depend on the choice of \u03b8ts. Each of the two algorithms we present provides a specific method for choosing \u03b8ts to achieve desired regret bounds."}, {"heading": "5.1. The dual algorithm", "text": "This algorithm is inspired by the reduction of the Blackwell approachability problem to online convex optimization (OCO) in Abernethy et al. [2011]. It is also related to the fast algorithms to solve covering/packing LPs using multiplicative weight update [Devanur et al. 2011] and the algorithm of Badanidiyuru et al. [2013]. In fact, we give a reduction to OCO; any algorithm for OCO can then be used. In OCO, the algorithm has to pick a vector, say \u03b8t in each time step t. (The domain of \u03b8t is such that ||\u03b8t||\u2217 \u2264 L for our purpose here, where L is the Lipschitz constant of f , and L = 1 for distance function.) Once \u03b8t is picked the algorithm observes a convex loss function, gt, and the process repeats. The objective is to minimize regret defined as\nRc(T ) := \u2211Tt=1 gt(\u03b8t)\u2212min||\u03b8||\u2217\u2264L \u2211T t=1 gt(\u03b8).\nRecall from our discussion earlier, in each step t, the algorithm sets pt as per (16) for some \u03b8t. The choice of \u03b8t is via a reduction to OCO: we define a convex function gt\u22121\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nbased on the history upto time t\u2212 1 which is then fed as input to the OCO algorithm, whose output \u03b8t is used in picking pt. We first define gt for the BwR problem; the gt for BwC is obtained as a special case with f(x) = \u2212d(x, S). Define\ngt(\u03b8) := f \u2217(\u03b8)\u2212 \u03b8 \u00b7 xt,\nwhere f\u2217 is the Fenchel conjugate of f ,(see Section 2.5 for the definition), and xt = V\u0303 tpt.\nALGORITHM 3: Fenchel dual based algorithm for BwR\nInititalize \u03b81. for all t = 1, 2, . . . , T do\nSet (pt, V\u0303 t) = argminp\u2208\u2206m,V\u0303 \u2208Ht \u03b8t \u00b7 (V\u0303 p). Play arm i with probability pt,i. Choose \u03b8t+1 by doing an OCO update for the convex function gt(\u03b8) = f \u2217(\u03b8)\u2212 \u03b8 \u00b7 (V\u0303 tpt).\nend for\nThe following geometric intuition for the Fenchel conjugate is useful in the analysis: if yt = argmaxy{y \u00b7\u03b8t+ f(y)} then \u2212\u03b8t \u2208 \u2207f(yt) and f\u2217(\u03b8t) = yt \u00b7\u03b8t+ f(yt) = lf (0;yt), i.e., f\u2217(\u03b8t) is the y-intercept of lf (x;yt). We can therefore rewrite lf (x;yt) in terms of f\u2217 as follows\nlf (x;yt) = f \u2217(\u03b8t)\u2212 \u03b8t \u00b7 x.\nWith this and (19), we have\ngt(\u03b8t) = f \u2217(\u03b8t)\u2212 \u03b8t \u00b7 xt = lf (xt;yt) \u2265 f(x\u2217).\nThe above inequality states that the optimum of BwR is bounded above by what the algorithm gets for OCO. We next show that the optimum value for OCO is equal to what the algorithm of BwR gets, so there is a flip. This will produce bound on f(x\u2217)\u2212 f(x\u03041:T ) in terms of Rc(T ). Note that for a fixed \u03b8, gt\u2019s differ only in the linear term, so the average of gt\u2019s for all t is equal to f\u2217(\u03b8)\u2212 \u03b8 \u00b7 x\u03041:T . Then, minimizing this over all \u03b8 gives f(x\u03041:T ), by Lemma 2.2.\nmin||\u03b8||\u2217\u2264L 1 T \u2211 t gt(\u03b8) = min||\u03b8||\u2217\u2264L f\n\u2217(\u03b8)\u2212 \u03b8 \u00b7 x\u03041:T = f(x\u03041:T ). These two observations together give\nf(x\u2217)\u2212 f(x\u03041:T ) \u2264 1T \u2211 t gt(\u03b8t)\u2212min||\u03b8||\u2217\u2264L 1T \u2211 t gt(\u03b8) = 1 T Rc(T ).\nAlgorithm for BwC. The algorithm for BwC is obtained by letting f(x) = \u2212d(x, S). Note that for this function L = 1. Also, it can be shown that f\u2217(\u03b8) = hS(\u03b8), therefore gt(\u03b8) := hS(\u03b8)\u2212 \u03b8 \u00b7 xt. And, using the same calculations as in above, we will get\nd(x\u03041:T , S) \u2264 1T Rc(T ). This and (20) imply the following theorem.\nTHEOREM 5.1. With probability 1\u2212 \u03b4, the regret of Algorithm 3 is bounded as\navg-regret1(T ) = O(L||1d|| \u221a \u03b3m T + R c(T ) T ) for BwR, and\navg-regret2(T ) = O(||1d|| \u221a \u03b3m T + R c(T ) T ) for BwC,\nwhen used with f(x) = \u2212d(x, S). Here \u03b3 = log(mTd \u03b4 ), and Rc(T ) is the regret for the OCO method used.\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nIn case of Eucledian norm, online gradient descent (OGD) can be used to get Rc(T ) = O\u0303(GD \u221a T ), where G is an upper bound on Eucledian norm of subgradient of gt, and D is an upper bound on Eucledian norm of \u03b8 (refer to Zinkevich [2003], and Corollary 2.7 in Shalev-Shwartz [2012]). For our purpose, G \u2264 \u221a d and D \u2264 L. For other norms FoRel algorithm with appropriate regularization may provide better guarantees. For example, when || \u00b7 || is L\u221e norm (i.e., || \u00b7 ||\u2217 is L1), we can use FoRel algorithm with Entropic regularization (essentially a generalization of the Hedge algorithm [Freund and Schapire 1995]), to obtain an improved bound of O(L \u221a T log(d)) on Rc(T ) (refer to Corollary 2.14 in Shalev-Shwartz [2012]). Implementability. OCO algorithms like online gradient descent require gradient computation. In this case, we need to compute the gradient of the dual f\u2217 (that is why we call it the dual algorithm) which can be computed as argmaxy{\u03b8 \u00b7y+ f(y)}. for a given \u03b8."}, {"heading": "5.2. The primal algorithm", "text": "The algorithm presented in Section 5.1 required computing the gradient of the Fenchel dual f\u2217 which may be computationally expensive in some cases. Here we present a primal algorithm (for BwR) that requires computing the gradient of f in each step, based on the Frank-Wolfe algorithm [Frank and Wolfe 1956]. A caveat is that this requires a stronger assumption on f , that f is smooth in the following sense.\nASSUMPTION 2. We call concave functionf(\u00b7) to be \u03b2-smooth if\nf(z + \u03b1(y \u2212 z)) \u2265 f(z) + \u03b1\u2207f(z) \u00b7 (y \u2212 z)\u2212 \u03b2 2 \u03b12, (21)\nfor all y, z \u2208 [0, 1]d and \u03b1 \u2208 [0, 1]. If f is such that the gradient of f is Lipshitz continuous (with respect to any Lq norm) with a constant G, then \u03b2 \u2264 Gd. Note that the distance function (f(z) = \u2212d(z, S)) does not satisfy this assumption. Like the Fenchel dual based algorithm, in each step, this algorithm too picks a \u03b8t and sets pt according to (16). The difference is that \u03b8t is now simply \u2212\u2207f(x\u03041:t\u22121)! ALGORITHM 4: Frank-Wolfe based primal algorithm for BwR\nfor all t = 1, 2, . . . , T do (pt, V\u0303 t) = argmaxp\u2208\u2206m maxV\u0303 \u2208Ht(V\u0303 p) \u00b7 \u2207f(x\u03041:t\u22121),\nwhere xt = V\u0303 tpt. Play arm i with probability pt,i. end for\nTHEOREM 5.2. With probability 1 \u2212 \u03b4, the regret of Algorithm 4 for BwR problem with \u03b2-smooth function f (Assumption 2), is bounded as\navg-regret1(T ) = O(L||1d|| \u221a \u03b3m T + \u03b2 log(T ) T ).\nPROOF. Using (20), proving this regret bound essentially means bounding f(x\u2217) \u2212 f(x\u03041:T ). This quantity can be bounded by \u03b2 log(2T ) 2T using techniques similar to those used in the analysis of Frank-Wolfe algorithm for convex optimization [Frank and Wolfe 1956]. The complete proof is provided in Appendix C."}, {"heading": "5.3. Smooth approximation of Non-smooth f", "text": "Assumption 2 may be stronger than Assumption 1. For instance, for distance function (f(z) = \u2212d(z, S)) Assumption 1 is satisfied with L = 1, but not Assumption 2. In this section, we show how to use the technique of [Nesterov 2005] to convert a nonsmooth f that only satisfies Assumption 1 into one that satisfies Assumption 2. For\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nsimpicity, we assume || \u00b7 || to be Eucledian norm in this section. Interestingly, for the smooth approximation of distance function, this algorithm will have essentially the same structure as the (primal) algorithm for the Blackwell approachability problem, thus drawing a connection between two well known algorithms.\nTHEOREM 5.3. [Nesterov 2005] Define\nf\u0302\u03b7(z) := min||\u03b8||\u2264L{f\u2217(\u03b8) + \u03b72L\u03b8 \u00b7 \u03b8 \u2212 \u03b8 \u00b7 z}. (22) Then, f\u0302\u03b7 is concave, differentiable, and dL \u03b7 -smooth. Further, f\u0302\u03b7 \u2212 \u03b72L \u2264 f \u2264 f\u0302\u03b7.\nNow, if we run Algorithm 4 on f\u0302\u03b7, with \u03b7 = \u221a d T log(2T ), we get that f(x\u2217)\u2212f(x\u03041:T ) \u2264\n\u03b2 log(2T ) 2T + \u03b7 2L \u2264 L2\n\u221a d log(2T )\nT . The algorithm and regret bound for BwC can be obtained\nsimilarly by using this smooth approximation for distance function, i.e., for f(z) = \u2212d(z, S). We thus obtain the following theorem.\nTHEOREM 5.4. With probability 1 \u2212 \u03b4, the regret of Algorithm 4 when used with smooth approximation f\u0302\u03b7(z) of function f(z) (or, \u2212d\u0302\u03b7(z, S) of function \u2212d(z, S)), is bounded as\navg-regret1(T ) = O(L||1d|| \u221a \u03b3m T + L \u221a d log(T ) T ) for BwR, and\navg-regret2(T ) = O(||1d|| \u221a \u03b3m T + \u221a d log(T ) T ) for BwC.\nFor the distance function, this smooth approximation has some nice characteristics.\nLEMMA 5.5. For the distance function d(z, S), (22) provides smooth approximation d\u0302\u03b7(z, S) = max||\u03b8||\u22641 \u03b8 \u00b7 z \u2212 hS(\u03b8)\u2212 \u03b72\u03b8 \u00b7 \u03b8, and, the gradient of this function is given by\n\u2207d\u0302\u03b7(z) =    z\u2212\u03c0S(z) ||z\u2212\u03c0S(z)|| if ||z \u2212 \u03c0S(z)|| \u2265 \u03b7 z\u2212\u03c0S(z) \u03b7 if 0 < ||z \u2212 \u03c0S(z)|| < \u03b7\n0 if z \u2208 S ,\nwhere \u03c0S(z) denotes the projection of z on S.\nThe proof of the above lemma along with a proof of Theorem 5.3 is in Appendix C. Note that for Algorithm 4 only the direction of the gradient of f matters, and in this case the direction of gradient of f = \u2212d\u0302\u03b7 at z is \u2212(z \u2212 \u03c0S(z)) for all z /\u2208 S. For z \u2208 S, the gradient is 0, which means it does not really matter what p is picked. Therefore, Algorithm 4 reduces to the following.\nALGORITHM 5: Frank-Wolfe based primal algorithm for BwC\nfor all t = 1, 2, . . . , T do If x\u03041:t\u22121 \u2208 S, set pt arbitrarily. If x\u03041:t\u22121 /\u2208 S, find projection \u03c0S(x\u03041:t\u22121) of this point on S. And compute\n(pt, V\u0303 t) = argminp\u2208\u2206m minV\u0303 \u2208Ht(V\u0303 p) \u00b7 (x\u03041:t\u22121 \u2212 \u03c0S(x\u03041:t\u22121)), Play arm i with probability pt,i.\nend for\nAlgorithm 5 has the same structure as the Blackwell\u2019s algorithm for the approachability problem [Blackwell 1956], which asks to play anything at time t if x\u03041:t\u22121 is in S. Otherwise, find a point xt such that xt \u2212 x\u03041:t\u22121 makes a negative angle with (x\u03041:t\u22121 \u2212 \u03c0S(x\u03041:t\u22121)). We have xt = V\u0303 tpt. However, the proof of convergence of Blackwell\u2019s algorithm as given in [Blackwell 1956] seems to be different from the proof derived here, via the smooth approximation and Frank-Wolfe type analysis. This gives\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nan interesting connection between well known algorithms, Blackwell\u2019s algorithm for the approachability problem and Frank-Wolfe algorithm for convex optimization, via Nesterov\u2019s method of smooth approximations!!\nImplementability. The algorithm with smooth approximation needs to compute the\ngradient of f\u0302\u03b7 in each step and in general there is no easy method to compute this, except in some special cases like the distance function discussed above. Alternatively, one\ncould use the smooth approximation f\u0302\u03b7(z) = Eu\u2208B[f(z + \u03b4u)] given by [Flaxman et al. 2005], which has slightly worse smoothness coefficient but has easy-to-compute gradient by sampling."}, {"heading": "6. COMPUTATIONALLY EFFICIENT ALGORITHMS FOR BWCR", "text": "Any combination of the primal and dual approaches mentioned in the previous sections can be used to get an efficient algorithm for the BwCR problem.Using the observations in Equation (16) and (17), we obtain an algorithm with the following structure.\nALGORITHM 6: Efficient algorithm for BwCR\nInititalize \u03b81. for all t = 1, 2, . . . , T do\npt = argminp\u2208\u2206m (\u03b8t \u00b7Zt(\u03b8t))p\ns.t. (\u03c6t \u00b7Zt(\u03c6t))p \u2264 hS(\u03c6t). (23)\nPlay arm i with probability pt,i. Compute \u03b8t+1, \u03c6t+1. end for\nHere, Zt(\u00b7) is a vertex of Ht as defined in (17). Now, either primal or dual approach can be used to update \u03b8, irrespective of what approach is being used for updating \u03c6, and vice-versa. The choice between primal and dual approach will depend on properties of f and S, e.g., whether it is easy to compute the gradient of f or its dual f\u2217. It is easy to derive regret bounds for this efficient algorithm for BwCR using results in the previous section.\nTHEOREM 6.1. For Algorithm 6, avg-regret1(T ) is given by Theorem 5.1, Theorem 5.2, or Theorem 5.4, respectively, dependening on whether the dual, primal, or primal approach with smooth approximation is used for updating \u03b8. And, avg-regret2(T ) is given by Theorem 5.1 or Theorem 5.4, respectively, dependening on whether the dual or primal approach is used for updating \u03c6.\nOne can also substitute the constraint or objective in (23) by the corresponding expression in Equation (9) of the UCB algorithm, if efficiency is not as much of a concern as regret for either constraint or objective.\nImplementability. Every step t of Algorithm 6 requires solving a linear optmization problem over simplex with one additional linear constraint. This is a major improvement in efficiency over Algorithm 1, which required solving a difficult convex optimization problem over domain {pV\u0303 : V\u0303 \u2208 Ht,p \u2208 \u2206m}. Also, Algorithm 6 is particularly simple to implement when the given application allows not playing any arm at a given time step, i.e. relaxing the constraint \u2211\ni pi = 1 to \u2211 i pi \u2264 1. This is true in many applications, for example, an advertiser is allowed to not participate in a given auction. In particular, in BwK, the algorithm can abort at any time step, effectively chosing not to play any arm in the remaining time steps. In such an application, if further hS(\u03c6t) \u2265 0,\u03c6Tt Zt(\u03c6t)i > 0, \u2200i, (23) is a special case of fractional knapsack problem, and the greedy optimal solution in this case reduces to simply choosing the arm i that minimizes \u03b8Tt Zt(\u03b8t)i \u03c6Tt Zt(\u03c6t)i , and playing it with probabil-\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nity p, where p \u2208 [0, 1] is the highest value satisfying \u03c6Tt Zt(\u03c6t)ip \u2264 hS(\u03c6t). Even if \u03c6Tt Zt(\u03c6t)i \u2264 0 for some arms i, some simple tweaks to this greedy choice work. In the special case of BwK, it is not difficult to compute that\u2212\u03b8Tt Zt(\u03b8t)i = UCBt,i(\u00b5), \u03c6Tt Zt(\u03c6t)i = \u03c6 T t LCBt,i(C), and hS(\u03c6t) = B T\nfor all t, so that the above greedy rule simply becomes that of selecting arm\nit = argmaxi UCBt,i(\u00b5)\n\u03c6Tt LCBt,i(C) ,\nand playing it with largest probability p such that \u03c6Tt LCBt,i(C)p \u2264 BT . This is remarkably similar to the PD-BwK algorithm of Badanidiyuru et al. [2013], except that their algorithm plays this greedy choice with probability 1 and aborts when any constraint is violated."}, {"heading": "A. PRELIMINARIES", "text": "PROOF OF LEMMA 2.1. For a random instance of the problem, let p\u0303i denote the empirical probability of playing arm i in the optimal instance specific solution in hindsight, and vt denote the observation vector at time t. Then, it must be true that 1 T \u2211 t vt \u2208 S. Let p\u2217 = E[p\u0303]. Then,\nE[ 1\nT\n\u2211 tvt] = 1 T E[ \u2211 tE[vt|it]] = E[V p\u0303t] = V p\u2217.\nSo that, due to convexity of S, 1 T \u2211 t vt \u2208 S implies that V p\u2217 \u2208 S. And, by concavity of f ,\nOPTf \u2264 E[f( 1\nT\n\u2211\nt\nvt)] \u2264 f(E[ 1\nT\n\u2211\nt\nvt]) = f(V p \u2217).\nPROOF OF LEMMA 2.2.\nmin ||\u03b8||\u2217\u2264L f\u2217(\u03b8)\u2212 \u03b8 \u00b7 z = min ||\u03b8||\u2217\u2264L max y {y \u00b7 \u03b8 + f(y)\u2212 \u03b8 \u00b7 z}\n= max y min ||\u03b8||\u2217\u2264L\n{y \u00b7 \u03b8 + f(y)\u2212 \u03b8 \u00b7 z}.\nThe last equality uses minmax theorem. Now, by our assumption, for any z, there exists a vector ||gz ||\u2217 \u2264 L which is a supergradient of f at z, i.e., f(y)\u2212 f(z) \u2264 gz \u00b7 (y \u2212 z), \u2200y. Therefore, for all y,\nmin ||\u03b8||\u2264L\n{y \u00b7 \u03b8 + f(y)\u2212 \u03b8 \u00b7 z} \u2264 (\u2212gz) \u00b7 y + f(y)\u2212 (\u2212g) \u00b7 z \u2264 f(z),\nwith equality achieved for y = z."}, {"heading": "B. UCB FAMILY OF ALGORITHMS", "text": "We will use the following concentration theorem.\nLEMMA B.1. [Kleinberg et al. 2008; Babaioff et al. 2012; Badanidiyuru et al. 2013] Consider some distribution with values in [0, 1], and expectation \u03bd. Let \u03bd\u0302 be the average of N independent samples from this distribution. Then, with probability at least 1 \u2212 e\u2212\u2126(\u03b3), for all \u03b3 > 0, |\u03bd\u0302 \u2212 \u03bd| \u2264 rad(\u03bd\u0302, N) \u2264 3rad(\u03bd,N), (24) where rad(\u03bd,N) = \u221a \u03b3\u03bd N + \u03b3 N . More generally this result holds if X1, . . . , XN \u2208 [0, 1] are\nrandom variables, N\u03bd\u0302 = \u2211N t=1 Xt, and N\u03bd = \u2211N\nt=1 E[Xt|X1, . . . , Xt\u22121]. LEMMA B.2. [Badanidiyuru et al. 2013] For any two vectors a,n \u2208 Rm+ ,\nm\u2211\nj=1\nrad(aj , nj)nj \u2264 \u221a \u03b3m(a \u00b7 n) + \u03b3m.\nB.1. BwCR\nLEMMA B.3. Define empirical average V\u0302t,ji for each arm i and component j at time t as\nV\u0302t,ji =\n\u2211 s<t:is=i vt,j\nkt,i + 1 , (25)\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nwhere kt,i is the number of plays of arm i before time t. Then, V\u0302t,ji is close to the actual mean Vji: for every i, j, t, with probability 1\u2212 e\u2212\u2126(\u03b3),\n|V\u0302t,ji \u2212 Vt,ji| \u2264 2rad(V\u0302t,ji, kt,i + 1). PROOF. This proof follows from application of Lemma B.1. We apply Lemma B.1 to v1,j , . . . , vT,j , for each j, using E[vt,j |it] = Vt,jit , to get that with probability at least 1\u2212 e\u2212\u2126(\u03b3),\n|V\u0302t,ji \u2212 Vt,ji| \u2264 kt,i\nkt,i + 1 \u00b7 rad(V\u0302t,ji, kt,i) + Vji kt,i + 1\n\u2264 rad(V\u0302t,ji, kt,i + 1) + Vji\nkt,i + 1\n\u2264 2rad(V\u0302t,ji, kt,i + 1).\nPROOF OF THEOREM 4.1. We use the following estimates\nUCBt,ji(V ) = min{1, V\u0302t,ji + 2rad(V\u0302t,ji, kt,i + 1)}, LCBt,ji(V ) = max{0, V\u0302t,ji \u2212 2rad(V\u0302t,ji, kt,i + 1)},\n(26)\nfor i = 1, . . . ,m, j = 1, . . . , d, t = 1, . . . , T . Here rad(\u03bd,N) = \u221a\n\u03b3\u03bd N + \u03b3 N , kt,i is the number\nof plays of arm i before time t, and V\u0302t,ji is the empirical average as defined in Equation (25). These estimates are similar to those used in literature on UCB algorithm for classic MAB and to those used in [Badanidiyuru et al. 2013]. Then, using concentration Lemma B.1, we will prove that the properties in Equation (6) and (8) hold with probability 1 \u2212 (mTd)e\u2212\u2126(\u03b3), and with Q(T ) = O(||1d|| \u221a \u03b3mT ) where ||1d|| denotes the norm of d dimensional vector of all 1s. Theorem 4.1 will then follow from the calculations in Equation (11). Property (1) stated as Equation (6) is obtained as a corollary of Lemma B.3 by taking a union bound for all i, j, t. With probability 1\u2212 (mTd)e\u2212\u2126(\u03b3), UCBt,ji(V ) \u2265 Vji \u2265 LCBt,ji(V ), \u2200i, j, t.\nNext, we prove Property (2) stated in Equation (8). Given that arm i was played with probability pt,i at time t, for any {V\u0303 t}Tt=1 such that V\u0303 t \u2208 Ht for all t, we will show that with probability 1\u2212 (mTd)e\u2212\u2126(\u03b3),\n|| T\u2211\nt=1\n(V\u0303 tpt \u2212 vt)|| = O(||1d|| \u221a \u03b3mT ).\nWe use the observation that E[vt|it] = V it . Then, using concentration Lemma B.1, with probability 1\u2212 de\u2212\u2126(\u03b3)\n| T\u2211\nt=1\n(Vjit \u2212 vt,j)| \u2264 3rad( 1\nT\nT\u2211\nt=1\nVjit , T ) = O( \u221a \u03b3T ), (27)\nfor all j = 1, . . . , d. Therefore, it remains to bound \u2211\nt(V\u0303 tpt \u2212 V it). Again, since E[V\u0303 t,it |pt, V\u0303 t] = V\u0303 tpt, we can obtain, using Lemma B.1,\n| T\u2211\nt=1\n(V\u0303t,jit \u2212 V\u0303 t,jpt)| \u2264 3rad( 1\nT\nT\u2211\nt=1\nV\u0303 t,jpt, T ) = O( \u221a \u03b3T ), (28)\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nfor all j with probability 1 \u2212 de\u2212\u2126(\u03b3). Now, it remains to bound |\u2211Tt=1(V\u0303t,jit \u2212 Vjit )|. Using Lemma B.3, with probability 1\u2212 (mTd)e\u2212\u2126(\u03b3), for all i, j, t,\n|V\u0302t,ji \u2212 Vt,ji| \u2264 2rad(V\u0302t,ji, kt,i + 1). Applying this, along with the observation that for any V\u0303 \u2208 Ht, LCBt,ij(V ) \u2264 V\u0303t,ji \u2264 UCBt,ji(V ), we get\n| \u2211T t=1(V\u0303t,jit \u2212 Vjit)| \u2264 ( \u2211\nt\n4rad(V\u0302t,jit , kt,it + 1)\n)\n=\n \u2211\ni\nkT,i+1\u2211\nN=1\n4rad(V\u0302N,ji, N)\n \n\u2264 4 ( \u2211\ni\n(kT,i + 1)rad(1, kT,i + 1)\n)\n\u2264 O( \u221a \u03b3mT ). (29)\nwherewe used V\u0302 N,i to denote the empirical average for i th arm over its pastN\u22121 plays.\nIn the last inequality, we used Lemma B.2 along with the observation that \u2211m\ni=1 kT,i = T . Equation (27), (28), and (29) together give\n|| \u2211T t=1vt \u2212 V pt|| \u2264 O(||1d|| \u221a \u03b3mT ).\nPROOF OF LEMMA 4.3. We use Fenchel duality to derive an equivalent expression for f : f(x) = min||\u03b8||\u2217\u2264L f \u2217(\u03b8)\u2212 \u03b8 \u00b7 x (refer to Section 2.5). Then,\n\u03c8(p) = max U\u0303\u2208Ht min ||\u03b8||\u2217\u2264L f\u2217(\u03b8)\u2212 \u03b8 \u00b7 (U\u0303p) = min ||\u03b8||\u2217\u2264L f\u2217(\u03b8)\u2212 min U\u0303\u2208Ht \u03b8 \u00b7 (U\u0303p),\nby application of the minimax theorem. Now, due to the structure of set Ht, observe that for any given \u03b8, a vertex Zt(\u03b8) (as defined in Equation (17)) of Ht minimizes \u03b8 \u00b7 U\u0303 componentwise. Therefore, irrespective of what p is,\n\u03c8(p) = min ||\u03b8||\u2217\u2264L\nf\u2217(\u03b8)\u2212 \u03b8 \u00b7 (Zt(\u03b8)p),\nwhich is a concave function, and a subgradient of this function at a point p is \u2212\u03b8\u2032TZt(\u03b8\u2032), where \u03b8\u2032 is the minimizer of the above expression. The minimizer\n\u03b8\u2032 = arg min ||\u03b8||\u2217\u2264L ( max U\u0303\u2208Ht f\u2217(\u03b8)\u2212 \u03b8 \u00b7 (U\u0303p) )\nis computable (e.g., by ellipsoid method) because it minimizes a convex function in \u03b8, with subgradient \u2202f\u2217(\u03b8)\u2212Zt(\u03b8)p at point \u03b8. The same analysis can be applied for g(p), by using f(x) = \u2212d(x, S).\nB.2. Linear contextual Bandits\nIt is straightforward to extend Algorithm 1 to linear contextual bandits, using existing work on UCB family of algorithms for this problem. Recall that in the contextual setting a n-dimensional context vector bji is associated with every arm i and component j, and there is an unknown weight vector wj for every component j, such that\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nVji = bji \u00b7wj . Now, consider the following ellipsoid defined by inverse of Gram matrix at time t,\nEj(t) = {x : (x\u2212 w\u0302j(t))TAj(t)(x\u2212 w\u0302j(t)) \u2264 n},\nwhere\nAj(t) = In + t\u22121\u2211\ns=1\nbjisb T jis , and w\u0302j(t) = Aj(t) \u22121\nt\u22121\u2211\ns=1\nbjisvs,j ,\nfor j = 1, . . . , d. Results from existing literature on linear contextual bandits [Abbasi-yadkori et al. 2012; Chu et al. 2011; Auer 2003] provide that with high probability, the actual weight vector wj is guaranteed to lie in this elliposid, i.e.,\nwj \u2208 Ej(t).\nThis allows us to define new estimate set Ht as\nHt = {V\u0303 : V\u0303ji = bji \u00b7 w\u0303j , \u2200w\u0303j \u2208 Ej(t)}.\nThen, using results from the above-mentioned literature on linear contextual bandits, it is easy to show that the properties (1) and (2) in Equation (6) and (8) hold with high probability for this Ht with Q(T ) = ||1d||n \u221a T log(dT\n\u03b4 ). Therefore, simply substituing\nthis Ht in Algorithm 1 provides an algorithm for linear contextual version of BwCR , with regret bounds,\navg-regret1(T ) \u2264 O(L||1d||n \u221a 1 T log(dT \u03b4 )), and, avg-regret2(T ) \u2264 O(||1d||n \u221a 1 T log(dT \u03b4 )).\nB.3. BwK\nProperty (1) for BwK(stated in Equation (13)), is simply a special case of Property (1) for BwCR, which was proven in the previous subsection. The following two lemmas prove the Property (2) for BwK(stated as Equation (14)). The proofs are similar to the proof of Property (2) for BwCR illustrated in the previous section, except that a little more careful analysis is done to get the bounds in terms of problem dependent parameters B and OPT.\nLEMMA B.4. With probability 1\u2212 (mT )e\u2212\u2126(\u03b3),\n|| 1 T \u2211T t=1(rt \u2212UCBt(\u00b5) \u00b7 pt|| \u2264\n\u221a \u03b3m( \u2211\nt\nrt) + \u03b3m.\nPROOF. Similar to Equation (27) and Equation (28), we can apply the concentration bounds given by Lemma B.1 to get that with probability 1\u2212 (mT )e\u2212\u2126(\u03b3),\n| 1 T \u2211T t=1(rt \u2212 \u00b5it)| \u2264 3rad( 1 T \u2211T t=1\u00b5it , T )\n\u2264 3rad( 1 T \u2211T t=1UCBt,it(\u00b5), T ) (30)\n| 1 T \u2211T t=1(UCBt(\u00b5) \u00b7 pt \u2212UCBt,it(\u00b5))| \u2264 rad( 1 T \u2211T t=1UCBt,it(\u00b5), T ) (31)\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nAlso, using Lemma B.3,\n|\u2211Tt=1(\u00b5it \u2212UCBt,it(\u00b5))| \u2264 4 \u2211 trad(\u00b5\u0302t,it , kt,it + 1)\n\u2264 12\u2211trad(\u00b5it , kt,it + 1) = 12 \u2211\ni\nkT,i+1\u2211\nN=1\nrad(\u00b5i, N)\n\u2264 12 \u2211\ni\n(kT,i + 1)rad(\u00b5i, kT,i + 1)\n\u2264 12 \u221a\u221a\u221a\u221a\u03b3m ( \u2211\ni\n\u00b5i(kT,i + 1)\n) + 12\u03b3m\n(using Lemma B.2) \u2264 12 \u221a\u221a\u221a\u221a\u03b3m ( \u2211\nt\n\u00b5it\n) + 24\u03b3m\n\u2264 12 \u221a\u221a\u221a\u221a\u03b3m ( \u2211\nt\nUCBt,it(\u00b5)\n) + 24\u03b3m (32)\nLet A = \u2211T\nt=1 UCBt,it(\u00b5). Then, from (30) and (32), we have that for some constant \u03b1\nA\u2212 2 \u221a \u03b1\u03b3mA \u2264 T\u2211\nt=1\nrt +O(\u03b3m).\nwhich implies\n( \u221a A\u2212\u221a\u03b1\u03b3m)2 \u2264 T\u2211\nt=1\nrt +O(\u03b3m).\nTherefore, \u221a\u221a\u221a\u221a T\u2211\nt=1\nUCBt,it(\u00b5) \u2264\n\u221a\u221a\u221a\u221a T\u2211\nt=1\nrt +O( \u221a \u03b3m). (33)\nSubstituting (33) in (30), (31), (32), we get\n|\u2211Tt=1(rt \u2212 \u2211T t=1UCBt(\u00b5)pt)| \u2264 O( \u221a \u03b3m( \u2211T t=1 rt) + \u03b3m).\nLEMMA B.5. With probability 1\u2212 (mTd)e\u2212\u2126(\u03b3), for all j = 1, . . . , d,\n| \u2211T t=1(ct,j \u2212 LCBt,j(C)pt| \u2264 \u221a \u03b3mB + \u03b3m.\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nPROOF. Similar to Equation (27) and Equation (28), we can apply the concentration bounds given by Lemma B.1 to get that with probability 1\u2212 (mTd)e\u2212\u2126(\u03b3), for all j\n| 1 T \u2211T t=1(ct,j \u2212 Cjit)| \u2264 3rad( 1 T \u2211T t=1Cjit , T ) (34)\n| 1 T \u2211T t=1(LCBt,j(C)pt \u2212 LCBt,jit(C))| \u2264 rad( 1 T \u2211T t=1LCBt,jit(C), T )\n\u2264 rad( 1 T \u2211T t=1Cjit , T ) (35)\nAlso, using Lemma B.3,\n|\u2211Tt=1(Cjit \u2212 LCBt,it(C))| \u2264 4 \u2211 trad(C\u0302t,jit , kt,it + 1)\n\u2264 12\u2211trad(Cjit , kt,it + 1) = 12 \u2211\ni\nkT,i+1\u2211\nN=1\nrad(Cji, N)\n\u2264 12 \u2211\ni\n(kT,i + 1)rad(Cji, kT,i + 1)\n\u2264 12 \u221a\u221a\u221a\u221a\u03b3m ( \u2211\ni\nCji(kT,i + 1)\n) + 12\u03b3m\n\u2264 12 \u221a\u221a\u221a\u221a\u03b3m ( \u2211\nt\nCjit\n) + 24\u03b3m (36)\nLet A = \u2211\nt Cjit . Then, from (35) and (36), we have that for some constant \u03b1\nA \u2264 \u2211\nt\nLCBt(C)pt + 2 \u221a \u03b1\u03b3mA+O(\u03b3m) \u2264 B + 2 \u221a \u03b1\u03b3mA+O(\u03b3m),\nwhere we used that \u2211T\nt=1 LCBt(C)pt \u2264 B, which is a corollary of the choice of pt made by the algorithm. Then,\n( \u221a A\u2212\u221a\u03b1\u03b3m)2 \u2264 B +O(\u03b3m).\nThat is, \u221a\u2211\nt\nCjit \u2264 \u221a B + O( \u221a \u03b3m). (37)\nSubstituting (37) in (34), (35), (36), we get\n| \u2211T t=1(ct,j \u2212 \u2211T t=1LCBt,j(C)pt)| \u2264 O( \u221a \u03b3mB + \u03b3m)."}, {"heading": "C. FRANK-WOLFE", "text": "PROOF OF THEOREM 5.2. Let \u2206t := f(x \u2217) \u2212 f(x\u03041:t). We prove that \u2206t \u2264 \u03b2 log(2t)2t . (The base of the log is 2.) Once again, we use (19) for t + 1, with yt+1 = x\u03041:t, and rearrange terms as follows:\n\u2207f(x\u03041:t) \u00b7 (xt+1 \u2212 x\u03041:t) \u2265 f(x\u2217)\u2212 f(x\u03041:t). (38)\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nIn order to use (21), we rewrite x\u03041:t+1 = x\u03041:t + 1 t+1 (xt+1 \u2212 x\u03041:t). Using (21) first, followed by (38), gives us the following.\nf(x\u03041:t+1) \u2265 f(x\u03041:t) + 1\nt+ 1 \u2207f(x\u03041:t) \u00b7 (xt+1 \u2212 x\u03041:t)\u2212\n\u03b2\n2(t+ 1)2\n\u2265 f(x\u03041:t) + 1\nt+ 1 (f(x\u2217)\u2212 f(x\u03041:t)) \u2212\n\u03b2\n2(t+ 1)2\nWith this we can bound \u2206t+1 in terms of \u2206t.\n\u2206t+1 \u2264 \u2206t \u2212 1\n(t+ 1) \u2206t +\n\u03b2\n2(t+ 1)2 =\nt\n(t+ 1) \u2206t +\n\u03b2\n2(t+ 1)2 (39)\nRecall that we wish to show that \u2206t \u2264 \u03b2 log(2t)/2t. The rest of the proof is by induction on t. For the base case, we note that we can still use (39) with t = 0 and an arbitrary x0 which is used to set p1. This gives us that \u22061 \u2264 \u03b2/2. The inductive step for t + 1 follows from (39) and the inductive hypothesis for t if\nt (t+ 1) \u00b7 \u03b2 log(2t) 2t +\n\u03b2 2(t+ 1)2 \u2264 \u03b2 log(2(t+ 1)) 2(t+ 1)\n\u21d4 log(t) + 1 t+ 1 \u2264 log(t+ 1)\n\u21d4 1 t+ 1 \u2264 log(1 + 1 t ).\nThe last inequality follows from the fact that for any a > 0, log(1+ a) > a1+a , by setting a = 1/t. This completes the induction. Therefore, \u2206T = f(x \u2217) \u2212 f(x\u03041:T ) \u2264 \u03b2 log(2T )2T and combined with (20), we get the desired theorem statement.\nPROOF OF THEOREM 5.3 . We first show Lipshitz continuity of \u2207f\u0302\u03b7. Let x1 and x2 be any two points in the domain of f , then for \u2113 = 1, 2, \u2207f\u0302\u03b7(x\u2113) = \u2212\u03b8\u2113 where\n\u03b8\u2113 = arg min ||\u03b8||\u2264L {f\u2217(\u03b8) + \u03b7 2L ||\u03b8||2 \u2212 \u03b8 \u00b7 x\u2113}.\nWe use the following fact about convex functions: if y\u2217 minimizes a convex function \u03c8 over some domain and y is any other point in the domain then \u2207\u03c8(y\u2217) \u00b7 (y \u2212 y\u2217) \u2265 0. Using this fact for y\u2217 = \u03b81 and y = \u03b82, we get that\n( \u2207f\u2217(\u03b81) + \u03b7\nL \u03b81 \u2212 x1\n) \u00b7 (\u03b82 \u2212 \u03b81) \u2265 0. (40)\nUsing convexity of f\u2217 and strong convexity of || \u00b7 ||2, we get that f\u2217(\u03b82) \u2265 f\u2217(\u03b81) +\u2207f\u2217(\u03b81) \u00b7 (\u03b82 \u2212 \u03b81), (41)\n\u03b7\n2L ||\u03b82||2 \u2265\n\u03b7\n2L\n( ||\u03b81||2 + 2\u03b81 \u00b7 (\u03b82 \u2212 \u03b81) + ||\u03b82 \u2212 \u03b81||2 ) . (42)\nAdding (40\u201342) we get that\n\u2212x1 \u00b7 (\u03b82 \u2212 \u03b81) + f\u2217(\u03b82) + \u03b7\n2L ||\u03b82||2 \u2265 f\u2217(\u03b81) +\n\u03b7\n2L\n( ||\u03b81||2 + ||\u03b82 \u2212 \u03b81||2 ) .\nSimilarly, by switching x1 and x2, we get\n\u2212x2 \u00b7 (\u03b81 \u2212 \u03b82) + f\u2217(\u03b81) + \u03b7\n2L ||\u03b81||2 \u2265 f\u2217(\u03b82) +\n\u03b7\n2L\n( ||\u03b82||2 + ||\u03b82 \u2212 \u03b81||2 ) .\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nAdding these two, we get\n(x1 \u2212 x2) \u00b7 (\u03b81 \u2212 \u03b82) \u2265 \u03b7\nL ||\u03b82 \u2212 \u03b81||2.\nBy Caucy-Schwartz inequality, we have\n(x1 \u2212 x2) \u00b7 (\u03b81 \u2212 \u03b82) \u2264 ||x1 \u2212 x2|| \u00b7 ||\u03b81 \u2212 \u03b82|| \u2234 \u03b7\nL ||\u03b82 \u2212 \u03b81||2 \u2264 ||x1 \u2212 x2|| \u00b7 ||\u03b81 \u2212 \u03b82|| \u21d2 ||\u03b82 \u2212 \u03b81|| \u2264 L\n\u03b7 ||x1 \u2212 x2||.\nThis shows that the Lipschitz constant of \u2207f\u0302\u03b7 is L/\u03b7. Then, we can show that f\u0302\u03b7 is dL \u03b7 smooth as follows: f\u0302\u03b7(x+ \u03b1(y \u2212 x)) = f\u0302\u03b7(x)\u2212 \u222b \u03b1\nw:0\n\u2207f\u0302\u03b7(x+ w(y \u2212 x)) \u00b7 (y \u2212 x)dw\n= f\u0302\u03b7(x) + \u03b1\u2207f\u0302\u03b7(x)(y \u2212 x) + \u222b \u03b1\nw:0\n(\u2207f\u0302\u03b7(x+ w(y \u2212 x))\u2212\u2207f\u0302\u03b7(x)) \u00b7 (y \u2212 x)dw\nThen, using Lipschitz continuity of f\u0302\u03b7, \u2223\u2223\u2223\u2223 \u222b \u03b1\nw:0\n(\u2207f\u0302\u03b7(x+ w(y \u2212 x))\u2212\u2207f\u0302\u03b7(x)) \u00b7 (y \u2212 x)dw \u2223\u2223\u2223\u2223 \u2264 L\u03b12\n\u03b7 ||x\u2212 y|| \u00b7 ||x\u2212 y||\n\u222b \u03b1\n0\n(w)dw\n= L\u03b12\n2\u03b7 ||x\u2212 y||2\n\u2264 dL \u03b7 \u00b7 \u03b1 2 2\nIt remains to show that f\u0302\u03b7\u2212 \u03b7L2 \u2264 f \u2264 f\u0302\u03b7. This follows almost immediately from Lemma 2.2 and (22): the function inside the minimization for f\u0302\u03b7 is always larger than that of f , but not by more than \u03b7L2 .\nLEMMA C.1. \u2207f\u0302\u03b7(z) = \u2212\u03b8 iff \u2203 y such that (1) \u2212\u03b8 is a supergradient of f at y. We denote this by \u2212\u03b8 \u2208 \u2202f(y), and (2) \u2212\u03b8 = \u03b1(y \u2212 z) where \u03b1 = min{L/\u03b7, L/||y\u2212 z||}.\nPROOF. The gradient of f\u0302\u03b7 is equal to \u2212\u03b8 where \u03b8 is the argmin in (22), which is equivalent to\nmin ||\u03b8||\u2264L max y {f(y) + \u03b8 \u00b7 y + \u03b7 2L \u03b8 \u00b7 \u03b8 \u2212 \u03b8 \u00b7 z} = max y min ||\u03b8|\u2217\u2264L {f(y) + \u03b8 \u00b7 y + \u03b7 2L \u03b8 \u00b7 \u03b8 \u2212 \u03b8 \u00b7 z},\nby the min-max theorem. The two conditions in the hypothesis of the lemma are essentially the KKT conditions for the above. Given \u03b8, it must be that y optimizes the inner maximization in the first form, which happens when \u2212\u03b8 \u2208 \u2202f(y). On the other hand, given y, it must be that \u03b8 optimizes the inner minimization in the second form. Note that due to the spherical symmetry of the domain of \u03b8, the direction that minimizes is z \u2212 y. Therefore we may assume that \u03b8 = \u03b1(z \u2212 y) for some 0 \u2264 \u03b1 \u2264 L/||z \u2212 y||, since ||\u03b8|| \u2264 L. Given this, the inner minimization reduces to minimizing \u03b7\u03b12/(2L)\u2212 \u03b1, subject to the constraint on \u03b1 above, the solution to which is \u03b1 = min{L/\u03b7, L/||y\u2212 z||}.\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014.\nPROOF OF LEMMA 5.5. One can get a closed form expression for the subgradients of the distance function. Let \u03c0S(z) be the projection of z onto S for z /\u2208 S, and \u03bdS(z) be the set of unit normal vectors to S at z, for a z that is on the boundary of S. We extend the defintion of \u03bdS(z) to z /\u2208 S as\n\u03bdS(z) := z \u2212 \u03c0S(z)\n||z \u2212 \u03c0S(z)|| .\nThen, the set of subgradients of the distance function \u2202d(z, S) is as follows.\n\u2202d(z, S) = { \u03bdS(z) if z /\u2208 S {\u03b1\u03bdS(z), for all \u03b1 \u2208 [0, 1]} if z is on the boundary of S 0 if z \u2208 interior of S\nNote that d(\u00b7, S) is non-smooth near the boundary of S. We now show how d\u0302\u03b7(\u00b7, S) becomes smooth, and give the stated closed form expression for \u2207d\u0302\u03b7(\u00b7, S). We use Lemma C.1 for f(z) = \u2212d(z, S) to construct for each z, a y that satisfies the two conditions in the lemma, and gives \u2207f\u0302\u03b7(z) = \u2212\u2207d\u0302\u03b7(z, S) as claimed. Note that L = 1 in this case.\nCase 1: ||z \u2212 \u03c0S(z)|| \u2265 \u03b7. Pick y = \u03c0S(z). Note that \u03bdS(z) \u2208 \u03bdS(y) therefore \u2212\u03bdS(z) \u2208 \u2202f(y), and the first condition in Lemma C.1 is satisfied. Since ||z\u2212y|| \u2265 \u03b7, \u03b1 = 1/||z\u2212y|| and \u03b1(y \u2212 z) = \u2212\u03bdS(z), so the second condition in Lemma C.1 is satisfied. Case 2: 0 < ||z \u2212 \u03c0S(z)|| < \u03b7. Pick y = \u03c0S(z). As in Case 1, \u03bdS(z) \u2208 \u03bdS(y) therefore \u2212\u03bdS(z) ||z\u2212\u03c0S(z)||\u03b7 \u2208 \u2202f(y), and the first condition in Lemma C.1 is satisfied. Since ||z \u2212 y|| < \u03b7, \u03b1 = 1/\u03b7 and \u03b1(y \u2212 z) = \u03c0S(z)\u2212z\n\u03b7 so the second condition in Lemma C.1 is\nsatisfied.\nCase 3: z \u2208 S. Pick y = z. Note that 0 \u2208 \u2202f(y), and the conditions in Lemma C.1 are satisfied trivially.\nJournal Name, Vol. X, No. X, Article X, Publication date: February 2014."}], "references": [{"title": "Improved algorithms for linear", "author": ["Y. ABBASI-YADKORI", "D. P\u00c1L", "C. SZEPESV\u00c1RI"], "venue": null, "citeRegEx": "ABBASI.YADKORI et al\\.,? \\Q2012\\E", "shortCiteRegEx": "ABBASI.YADKORI et al\\.", "year": 2012}, {"title": "Finite-time analysis of the multiarmed", "author": ["Res", "P. 397\u2013422. AUER", "N. CESA-BIANCHI", "P. FISCHER"], "venue": null, "citeRegEx": "3 et al\\.,? \\Q2002\\E", "shortCiteRegEx": "3 et al\\.", "year": 2002}, {"title": "Dynamic pricing with", "author": ["M. BABAIOFF", "S. DUGHMI", "R. KLEINBERG", "A. SLIVKINS"], "venue": "bandit problem. Mach. Learn", "citeRegEx": "BABAIOFF et al\\.,? \\Q2012\\E", "shortCiteRegEx": "BABAIOFF et al\\.", "year": 2012}, {"title": "Bandits with knapsacks", "author": ["A. EC. BADANIDIYURU", "R. KLEINBERG", "A. SLIVKINS"], "venue": null, "citeRegEx": "BADANIDIYURU et al\\.,? \\Q2013\\E", "shortCiteRegEx": "BADANIDIYURU et al\\.", "year": 2013}, {"title": "An analog of the minimax theorem for vector payoffs", "author": ["D. FOCS. BLACKWELL"], "venue": "Pacific Journal of", "citeRegEx": "BLACKWELL,? 1956", "shortCiteRegEx": "BLACKWELL", "year": 1956}, {"title": "Near optimal online", "author": ["N.R. COLT. 355\u2013366. DEVANUR", "K. JAIN", "B. SIVAN", "C.A. WILKENS"], "venue": null, "citeRegEx": "DEVANUR et al\\.,? \\Q2011\\E", "shortCiteRegEx": "DEVANUR et al\\.", "year": 2011}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. STOC. NESTEROV"], "venue": "Mathematical Program-", "citeRegEx": "NESTEROV,? 2005", "shortCiteRegEx": "NESTEROV", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "Our final contribution is giving computationally more efficient algorithms by establishing (sometimes surprising) connections between the BwCR problem and other well studied problems/algorithms such as the Blackwell approachability problem [Blackwell 1956], online convex optimization [Zinkevich 2003], and the Frank-Wolfe (projection-free) algorithm for convex optimization [Frank and Wolfe 1956].", "startOffset": 240, "endOffset": 256}, {"referenceID": 4, "context": "Our final contribution is giving computationally more efficient algorithms by establishing (sometimes surprising) connections between the BwCR problem and other well studied problems/algorithms such as the Blackwell approachability problem [Blackwell 1956], online convex optimization [Zinkevich 2003], and the Frank-Wolfe (projection-free) algorithm for convex optimization [Frank and Wolfe 1956]. We provide two efficient algorithms, a \u201cprimal\u201d algorithm based on the Frank-Wolfe algorithm and a \u201cdual\u201d algorithm based on the reduction of Blackwell approachability to online convex optimization [Abernethy et al. 2011]. One may be faster than the other depending on the properties of the objective function f and convex set S. As an aside, the primal algorithm establishes a connection between Blackwell\u2019s algorithm for the approachability problem and the Frank-Wolf algorithm. The dual algorithm turns out to be almost identical to the primal-dual algorithm (PD-BwK) of Badanidiyuru et al. [2013] for the special case of BwK problem.", "startOffset": 206, "endOffset": 1000}, {"referenceID": 4, "context": "Blackwell approachability problem considers a two player vector-valued game with a bi-affine payoff function, r(p, q) = pMq. Further, it is assumed that for all q, there exists a p such that r(p, q) \u2208 S. The row player\u2019s goal is to direct the payoff vector to some convex set S. The Bandit with convex knapsacks (BwC) problem is closely related to the Blackwell approachability problem. The row player is the online algorithm and the column player is nature. However, in this case the nature always produces its outcome using a fixed (but unknown) mixed strategy (distribution) q. Also, this means a weaker assumption should suffice: there exists a p for this particular q, such that r(p, q) \u2208 S (stated as the assumption \u2203p\u2217,V p \u2208 S). The bigger difference algorithmically is that there is nothing to statistically estimate in the Blackwell approachability problem, the only unknown is the column player strategy which may change every time. On the other hand, esitmating the expected consumption is inherently the core part of any algorithm for BwC. Due to these differences, algorithms for none of these related problems directly solve the BwCR problem. Nonetheless, the similarities suffice to inspire many of the ideas for computationally efficient algorithms that we present in this paper. The work closest to our work is that of Badanidiyuru et al. [2013] on the BwK problem.", "startOffset": 0, "endOffset": 1363}, {"referenceID": 4, "context": "Blackwell approachability problem considers a two player vector-valued game with a bi-affine payoff function, r(p, q) = pMq. Further, it is assumed that for all q, there exists a p such that r(p, q) \u2208 S. The row player\u2019s goal is to direct the payoff vector to some convex set S. The Bandit with convex knapsacks (BwC) problem is closely related to the Blackwell approachability problem. The row player is the online algorithm and the column player is nature. However, in this case the nature always produces its outcome using a fixed (but unknown) mixed strategy (distribution) q. Also, this means a weaker assumption should suffice: there exists a p for this particular q, such that r(p, q) \u2208 S (stated as the assumption \u2203p\u2217,V p \u2208 S). The bigger difference algorithmically is that there is nothing to statistically estimate in the Blackwell approachability problem, the only unknown is the column player strategy which may change every time. On the other hand, esitmating the expected consumption is inherently the core part of any algorithm for BwC. Due to these differences, algorithms for none of these related problems directly solve the BwCR problem. Nonetheless, the similarities suffice to inspire many of the ideas for computationally efficient algorithms that we present in this paper. The work closest to our work is that of Badanidiyuru et al. [2013] on the BwK problem. We successfully generalize their setting to include arbitrary convex constraints and concave objectives, as well as linear contexts. Additionally, we demonstrate that a simple and natural extension of UCB algorithm suffices to obtain optimal regret for BwCR which subsumes BwK, and provide generalized techniques for deriving multiple efficient implementations of this algorithm \u2013 one of which reduces to an algorithm similar to the PD-BwK algorithm of Badanidiyuru et al. [2013] for the speical case of BwK.", "startOffset": 0, "endOffset": 1863}, {"referenceID": 4, "context": "We present two such algorithms, a \u201cprimal\u201d algorithm based on the Frank-Wolfe algorithm [Frank and Wolfe 1956] and a \u201cdual\u201d algorithm based on the reduction of the Blackwell approachability problem to online convex optimization (OCO) in Abernethy et al. [2011]. In this section, for simplicity of illustration, we consider only the BwC and BwR problems, i.", "startOffset": 164, "endOffset": 261}, {"referenceID": 4, "context": "The dual algorithm This algorithm is inspired by the reduction of the Blackwell approachability problem to online convex optimization (OCO) in Abernethy et al. [2011]. It is also related to the fast algorithms to solve covering/packing LPs using multiplicative weight update [Devanur et al.", "startOffset": 70, "endOffset": 167}, {"referenceID": 4, "context": "The dual algorithm This algorithm is inspired by the reduction of the Blackwell approachability problem to online convex optimization (OCO) in Abernethy et al. [2011]. It is also related to the fast algorithms to solve covering/packing LPs using multiplicative weight update [Devanur et al. 2011] and the algorithm of Badanidiyuru et al. [2013]. In fact, we give a reduction to OCO; any algorithm for OCO can then be used.", "startOffset": 70, "endOffset": 345}, {"referenceID": 6, "context": "In this section, we show how to use the technique of [Nesterov 2005] to convert a nonsmooth f that only satisfies Assumption 1 into one that satisfies Assumption 2.", "startOffset": 53, "endOffset": 68}, {"referenceID": 6, "context": "[Nesterov 2005] Define", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "Algorithm 5 has the same structure as the Blackwell\u2019s algorithm for the approachability problem [Blackwell 1956], which asks to play anything at time t if x\u03041:t\u22121 is in S.", "startOffset": 96, "endOffset": 112}, {"referenceID": 4, "context": "However, the proof of convergence of Blackwell\u2019s algorithm as given in [Blackwell 1956] seems to be different from the proof derived here, via the smooth approximation and Frank-Wolfe type analysis.", "startOffset": 71, "endOffset": 87}], "year": 2014, "abstractText": "In this paper, we consider a very general model for exploration-exploitation tradeoff which allows arbitrary concave rewards and convex constraints on the decisions across time, in addition to the customary limitation on the time horizon. This model subsumes the classic multi-armed bandit (MAB) model, and the Bandits with Knapsacks (BwK) model of Badanidiyuru et al. [2013]. We also consider an extension of this model to allow linear contexts, similar to the linear contextual extension of the MAB model. We demonstrate that a natural and simple extension of the UCB family of algorithms for MAB provides a polynomial time algorithm that has near-optimal regret guarantees for this substantially more general model, and matches the bounds provided by Badanidiyuru et al. [2013] for the special case of BwK, which is quite surprising. We also provide computationally more efficient algorithms by establishing interesting connections between this problem and other well studied problems/algorithms such as the Blackwell approachability problem, online convex optimization, and the Frank-Wolfe technique for convex optimization. We give examples of several concrete applications, where this more general model of bandits allows for richer and/or more efficient formulations of the problem.", "creator": "LaTeX with hyperref package"}}}