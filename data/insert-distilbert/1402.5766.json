{"id": "1402.5766", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2014", "title": "No more meta-parameter tuning in unsupervised sparse feature learning", "abstract": "we propose modeling a meta - parameter free, off - the - shelf, simple and fast theoretically unsupervised binary feature learning algorithm, which further exploits a new way of optimizing for sparsity. experiments on board stl - 10 show that efficiently the method presents state - of - the - art image performance and provides discriminative features that generalize well.", "histories": [["v1", "Mon, 24 Feb 2014 09:49:04 GMT  (263kb)", "http://arxiv.org/abs/1402.5766v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["adriana romero", "petia radeva", "carlo gatta"], "accepted": false, "id": "1402.5766"}, "pdf": {"name": "1402.5766.pdf", "metadata": {"source": "META", "title": "No more meta-parameter tuning in unsupervised sparse feature learning", "authors": ["Adriana Romero", "Petia Radeva"], "emails": ["ADRIANA.ROMERO@UB.EDU", "PETIA.IVANOVA@UB.EDU", "CGATTA@CVC.UAB.ES"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 2.\n57 66\nv1 [\ncs .L\nG ]\n2 4"}, {"heading": "1. Introduction", "text": "Significant effort has been devoted to handcraft appropriate feature representations of data in several fields. In tasks such as image classification and object recognition, unsupervised learned features have shown to compete well or even outperform manually designed ones (Ranzato et al., 2006; Yang et al., 2009; Coates et al., 2011). Unsupervised feature learning has also shown to be helpful in greedy layerwise pre-training of deep architectures (Hinton et al., 2006; Bengio et al., 2006; Larochelle et al., 2009; Erhan et al., 2010).\nIn (Bengio, 2009), the author claims that potentially interesting research involves pre-training algorithms, which \u201c[...] would be proficient at extracting good features but involving an easier optimization problem.\u201d In addition to that, one of the main criticisms to state-of-the-art methods is that they require a significant amount of metaparameters (Bengio et al., 2013). As stated in (Snoek et al., 2012), the tuning of these meta-parameters is a laborious task that requires expert knowledge, rules of thumb or extensive search and, whose setting can vary for dif-\nferent tasks. Therefore, there is great interest for metaparameter free methods (Ngiam et al., 2011) and automatic approaches to optimize the performance of learning algorithms (Snoek et al., 2012).\nNevertheless, little effort has been devoted to address this problem (see Table 1 for a comparison of meta-parameters required by unsupervised feature learning methods). To the best of our knowledge, work in this direction includes ICA (Hyva\u0308rinen & Oja, 2000; Hyva\u0308rinen et al., 2000) and sparse filtering (Ngiam et al., 2011). Although ICA provides good results at object recognition tasks (Le et al., 2011; Ngiam et al., 2011), the method scales poorly to large datasets and high input dimensionality.\nComputational complexity is also a major drawback of many state-of-the-art methods. ICA requires an expensive orthogonalization to be computed at each iteration. Sparse coding has an expensive inference, which requires a prohibitive iterative optimization. Significant amount of work has been done in order to overcome this limitation (Lee et al., 2006; Kavukcuoglu et al., 2010). Predictive Sparse Decomposition (PSD) (Kavukcuoglu et al., 2010) is a successful variant of sparse coding, which uses a predictor to approximate the sparse representation and solves the sparse coding computationally expensive encoding step.\nIn this paper, we aim to solve some of the above-mentioned problems. We propose a meta-parameter free, off-theshelf, simple and fast approach, which exploits a new way of optimizing for a sparsity, without explicitly modeling the data distribution. The method iteratively builds an ideally sparse target and optimizes the dictionary by minimizing the error between the system output and the ideally sparse target. Defining sparsity concepts in terms of expected output allows to exploit a new strategy in unsupervised training.\nIt is worth stressing that many optimization strategies can be used to minimize the above-mentioned error and that parameters of these optimization techniques must not be considered as belonging to our approach.\nExperiments on STL-10 dataset show that the method outperforms state-of-the-art methods in single layer image classification, providing discriminative features that generalize well.\nLinear feature extraction methods combined with sparse coding encodings are among best performers on object recognition datasets. The importance of properly combining training/encoding and encoding/pooling strategies has been argued in (Coates & Ng, 2011) and (Zeiler & Fergus, 2013) respectively. Since the goal of this paper is to propose a new method for unsupervised feature learning, dealing with all the possible combinations of encoding and pooling could mask the benefits of the method that we propose. However, for the sake of fair comparison with the state-of-the-art, we test the method with sparse coding and soft-threshold encodings combined with sum pooling, following the experimental pipeline of (Coates & Ng, 2011)."}, {"heading": "2. State-of-the-art", "text": "Commonly used algorithms for unsupervised feature learning include Restricted Boltzmann Machines (RBM) (Hinton et al., 2006), auto-encoders (Bengio et al., 2006), sparse coding (Raina et al., 2007) and hybrids such as PSD (Kavukcuoglu et al., 2010). Many other methods such as ICA (Hyva\u0308rinen & Oja, 2000; Hyva\u0308rinen et al., 2000), Reconstruction ICA (RICA) (Le et al., 2011), Sparse Filtering (Ngiam et al., 2011) and methods related to vector quantization such as Orthogonal Matching Pursuit (OMP-k) (Pati et al., 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) have also been used in the literature to extract unsupervised feature representations. These algorithms could be divided into two categories: explicitly modeling or not the input distribution. Sparse auto-encoders (Ranzato et al., 2006), sparse RBM (Hinton et al., 2006; Lee et al., 2008; Hinton,\n2010; Goh et al., 2012), sparse coding (Olshausen & Field, 1997), PSD (Kavukcuoglu et al., 2010), OMP-k (Pati et al., 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) and Reconstruction ICA (RICA) (Le et al., 2011) explicitly model the data distribution by minimizing the reconstruction error. Although learning a good approximation of the data distribution may be desirable, approaches such as sparse filtering (Ngiam et al., 2011) show that this seems not so important if the goal is to have a discriminative sparse system. Sparse filtering does not attempt to explicitly model the input distribution but focuses on the properties of the output distribution instead.\nSparsity is among the desirable properties of a good output representation (Field, 1994; Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011; Bengio et al., 2013). Sparse features consist of a large amount of outputs, which respond rarely and provide high responses when they do respond. Sparsity can be described in terms of population sparsity and lifetime sparsity (Willmore & Tolhurst, 2001). Both lifetime and population sparsity are important properties of the output distribution. On one hand, lifetime sparsity plays an important role in preventing bad solutions such as numerous dead outputs. There seems to be a consensus to overcome such degenerate solutions, which is to ensure similar statistics among outputs (Field, 1994; Willmore & Tolhurst, 2001; Ranzato et al., 2006; Ngiam et al., 2011). On the other hand, population sparsity helps providing a simple interpretation of the input data such as the ones found in early visual areas. To the best of our knowledge, the definition of population sparsity remains ambiguous.\nState-of-the-art methods optimize either for one or both sparsity forms in their objective function. The great majority seeks sparsity using the L1 penalty and does not optimize for an explicit level of sparsity in their outputs. Sparse auto-encoders optimize for a target activation allowing to deal with lifetime sparsity; nevertheless, the target activation requires tuning and does not explicitly control the level of population sparsity. OMP-k defines the level of population sparsity by setting k to the maximum expected\nnumber of non-zero elements per output code, whereas the methods in (Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011) do not explicitly define the proportion of outputs expected to be active at the same time."}, {"heading": "3. Method", "text": "In this section, we describe how the proposed method learns a sparse feature representation of the data in terms of population and lifetime sparsity. The method iteratively builds an ideally sparse target and optimizes the dictionary by minimizing the error between the system output and the ideally sparse target. Subsection 3.1 highlights the algorithm to enforce lifetime and population sparsity in the ideally sparse target. Subsection 3.2 provides implementation details on the system and optimization strategies used to minimize the error between the system output and the ideally sparse target."}, {"heading": "3.1. Enforcing Population and Lifetime Sparsity by defining an ideal target", "text": "We define population and lifetime sparsity as properties of an ideal sparse output. Given N training samples and an output of dimensionality Nh, we define the first property of the output as:\n1. Strong Lifetime Sparsity: The output vectors must be composed solely of active and inactive units (no intermediate values between two fixed scalars are allowed) and all outputs must activate for an equal number of inputs. Activation is exactly distributed among the Nh outputs.\nOur Strong Lifetime Sparsity definition is a more strict requirement than the high dispersal concept introduced in (Ngiam et al., 2011), since they only require that \u201cthe mean squared activations of each feature (output) [...] should be roughly the same for all features (outputs)\u201d. While high dispersal attempts to diversify the learned bases, it does not guarantee the output distribution, in the lifetime sense, to be composed of only a few activations. Furthermore, our definition ensures the absence of dead outputs.\nGiven our definition of Strong Lifetime Sparsity, the population sparsity must require that, for each training sample, only one output element is active:\n2. Strong Population Sparsity: For each training sample only one output must be active.\nThe rationale of our approach is to appropriately generate an ideal output target that fulfils properties (1) and (2), and then learn the parameters of the system by minimizing the\nL2 error between the output target and the output generated by the system during training. In this way, we seek a system optimized for both population and lifetime sparsity in an explicit way.\nThe key component of our approach is how to define the ideal output target based on the above-mentioned properties. However, to ensure that the optimization of the system parameters converges, we add a third property:\n3. Minimal Perturbation: The ideal output target should be defined as the best approximation of the system output by means of L2 error fulfilling properties (1) & (2).\nCreating the output target that ensures the above-mentioned properties is analogous to solving an assignment problem. The Hungarian method (Kuhn, 1955) is a combinatorial optimization algorithm, which solves the assignment problem. However, its computational costO((NNh)3/2) is prohibitive. Therefore, in the next section we propose a simple and fast O(NNh) algorithm to generate the ideal output target, which ensures sparsity properties (1) and (2) and provides an approximate solution for minimal perturbation property (3)."}, {"heading": "3.1.1. IDEAL TARGET GENERATION: THE ENFORCING POPULATION AND LIFETIME SPARSITY (EPLS) ALGORITHM", "text": "Let us assume that we have a system, which produces a row output vector h. We use the notation hj to refer to one element of h. We define an output matrix H composed of Nb output vectors of dimensionality Nh, such that Nb \u2264 N . Likewise, we define an ideal target output matrix T of the same size. Algorithm 1 details the EPLS algorithm to generate the ideal target T from H. For the sake of simplicity, every step of the algorithm where the subscript j appears must be applied \u2200j \u2208 {1, 2, . . . , Nh}.\nAlgorithm 1 EPLS Require: H, a, N Ensure: T, a\n1: T = 0 2: for n = 1 \u2192 Nb do 3: hj = Hn,j 4: k = argmaxj (hj \u2212 aj) 5: Tn,k = 1 6: ak = ak + Nh N\n+ \u01eb 7: end for 8: Remap T to active/inactive values of the corresponding func-\ntion.\nStarting with no activation in T (line 1), the algorithm proceeds as follows. A row vector h from H is processed at each iteration (line 3). The crucial step is performed in line\n4: the output k that has to be activated in the nth row of T is selected as the one that has the maximal activation value hj minus the inhibitor aj . The inhibitor aj can be seen as an accumulator that \u201ccounts\u201d the number of times an output j has been selected, increasing its inhibition progressively by Nh/N until reaching maximal inhibition. This prevents the selection of an output that has already been activated N/Nh times. The rationale behind the equation in line 4 is that, while selecting the maximal responses in the matrix H, we have to take care to distribute them evenly among all outputs (in order to ensure Strong Lifetime Sparsity). Using this strategy, it can be demonstrated that the resulting matrix T perfectly fulfills properties (1) and (2). In line 5, the algorithm activates the kth element of nth row of the target matrix T. By activating the \u201crelative\u201d maximum, we approximate property (3). Finally, the inhibitor a is updated in line 6."}, {"heading": "3.2. System and Optimization strategies", "text": "Let us assume that we have a system parameterized by \u0393 = {W,b}, with activation function f , which takes as input a data vector d and produces an output vector h = f(d,\u0393). We use the same notation as in Section 3 and define a data matrix D composed of N rows and Nd columns, where Nd is the input dimensionality.\nTo compare our training strategy to previous well known systems, we tested our algorithm using\nH = f (DW + b) , (1)\nwhere f is a logistic non-linearity."}, {"heading": "3.2.1. OPTIMIZATION STRATEGY", "text": "The system might be trained by means of an off-the-shelf mini-batch Stochastic Gradient Descent (SGD) method with adaptive learning rates such as variance-based SGD (vSGD) (Schaul et al., 2013). Algorithm 2 details the latter training process. The mini-batch size Nb can be set to any value, in all the experiments we have set Nb = Nh. Starting with \u0393 set to small random numbers as in (LeCun et al., 1998) (line 1), at each epoch we shuffle the samples of the training set (line 3), reset the EPLS inhibitor a to a flat activation (line 4) and process all mini-batches. For each mini-batch b, samples D(b) are selected (line 6). Then, the output H(b) is computed (line 7) and the EPLS is invoked to compute T(b) and update a (line 8). After that, the gradient of the error is computed (line 9) and the learning rate \u03b7 is estimated as in (Schaul et al., 2013) (line 10). The system parameters are then updated to minimize the L2 error E(b) = ||H(b)\u2212T(b)||22 (line 11). Finally, the bases W in \u0393 are limited to have unit norm to avoid degenerate solutions (line 13). This procedure is repeated until a stop condition is met; in our experiments, the training stops when the rel-\native decrement error between epochs is small (< 10\u22126).\nWhen updating the system parameters, we assume that T does not depend on \u0393, thus \u2202T\u2202\u0393 = 0; we carried out experiments that show that this approximation does not significantly influence the gradient descent convergence nor the quality of the minimization. Moreover, this assumption makes the algorithm faster, since we remove the need of computing the numerical partial derivatives of T.\nThe mini-batch vSGD allows to scale the algorithm easily, especially with respect to the number of samples N .\nAlgorithm 2 Standard EPLS training Require: D Ensure: \u0393\n1: \u0393 = small random values 2: repeat 3: Shuffle D randomly 4: a = flat activation 5: for b = 1 \u2192 \u230aN/Nb\u230b do 6: Select mini-batch samples D(b) 7: H(b) = f(D(b),\u0393) 8: (T(b),a) = EPLS(H(b),a, N) 9: G = \u2207\u0393||H(b) \u2212T(b)||22\n10: Estimate learning rate \u03b7 as in (Schaul et al., 2013) 11: \u0393 = \u0393\u2212 \u03b7G 12: end for 13: Limit the bases W in \u0393 to have unit norm 14: until stop condition verified"}, {"heading": "4. Experiments", "text": "The performance of training and encoding strategies in single layer networks has been extensively analyzed in the literature (Coates et al., 2011; Coates & Ng, 2011; Ngiam et al., 2011) on STL-101 dataset. STL-10 dataset consists of 96x96 pixels color images belonging to 10 different classes. The dataset is divided into a large unlabeled training set containing 100K images and smaller labeled training and test sets, containing 5000 and 8000 images, respectively. It has to be considered that in STL-10, the primary challenge is to make use of the unlabeled data (100K images), which is 100 times bigger than the labeled data used to train the classifier (1000 images per fold). In this case, the supervised training must strongly rely on the ability of the unsupervised method to learn discriminative features. Moreover, since the unlabeled dataset contains other types of animals (bears, rabbits, etc.) and vehicles (trains, buses, etc.) in addition to the ones in the labeled set, the unsupervised method should be able to generalize well.\nTo validate our method, we follow the experimental pipeline of (Coates et al., 2011). We first extract random patches and normalize them for local brightness and con-\n1http://www.stanford.edu/\u223cacoates/stl10/\ntrast. Note that EPLS does not require any whitening of the input data, since it decorrelates the data during the training by means of the imposed strong sparsity properties of the output target. Then, we apply the system to retrieve sparse features of patches covering the input image, pool them into 4 quadrants and finally train a L2 SVM for classification purposes. We tune the SVM parameter using 5- fold cross-validation. As in (Ngiam et al., 2011), we use a receptive field of 10x10 pixels and a stride of 1. The number of outputs is set to Nh = 1600 for fair comparison with the other state-of-the-art methods. We also provide the results of our method with sign split (Nh = 1600x2, using W and \u2212W for encoding as in (Coates & Ng, 2011)) and using the sparse coding (SC) encoder, which (Coates & Ng, 2011) found to be the best when small number of labeled data is available. For this encoder, we searched over the same set of parameter values as (Coates & Ng, 2011), i.e., \u03bb = {0.5, 0.75, 1.0, 1.25, 1.5}. The parameter \u03bb is tuned to consider the use of sparse coding as encoder after the training and, thus, does not belong to the method that we propose.\nTable 2 summarizes the results obtained on this dataset compared to other state-of-the-art methods. When pairing each training method with its associated natural encoding, EPLS outperforms all the other methods. When pairing the training methods with sparse coding, EPLS outperforms the state-of-the-art best performer in single layer networks as well, achieving 61.0% (0.58%) accuracy. More-\nover, the standard deviation of the folds is lower than the one provided by OMP-1 with sparse coding encoding. Results are even more impressive if we compare them to metaparameter free algorithms.\nFigure 1 shows a subset of 100 randomly selected bases learned by our method, 10x10 pixel receptive field and a system of Nh = 1600 outputs. As shown in the figure, the method learns not only common bases such as oriented edges/ridges in many directions and colors but also corner detectors, tri-banded colored filters, center surrounds and Laplacian of Gaussians among others. This suggests that enforcing lifetime sparsity helps the system to learn a set of complex, rich and diversified bases."}, {"heading": "5. Computational complexity", "text": "The EPLS algorithm requires the computation of T, which has O(NNh) cost, and therefore scales linearly on both N and Nh. Since we can use vSGD for optimization, the method scales linearly on N given a fixed number of epochs. Finally, applying the activation function, the cost of computing the derivative is linear with Nd, since we use a closed form for \u2202E\u2202\u0393 .\nThe memory complexity is related to the mini-batch size Nb. Consequently, the method can scale gracefully to very large datasets: theoretically, it requires to store in memory the mini-batch input data D(b) (NbNd elements), output H(b) (NbNh elements), target T(b) (NbNh elements) and the system parameters to optimize \u0393 (Nh (Nd + 1) elements); a total amount of Nh (Nd + 1)+Nb (Nd + 2Nh) elements."}, {"heading": "6. Discussion", "text": "Our results show that simultaneously enforcing both population and lifetime sparsity helps in learning discriminative dictionaries, which reflect in better performance, especially when compared to meta-parameter free methods (Ngiam et al., 2011; Le et al., 2011). Experiments suggest that our algorithm is able to extract features that generalize well on unseen data. When comparing the performance STL-10 dataset, our algorithm outperforms state-of-the-art best performers. Results suggest that our algorithm helps the classifier in generalizing with a few training examples (1% of the dataset), gaining 2% accuracy w.r.t. the stateof-the art best performer (OMP-1 paired with sparse coding) with a lower standard deviation across folds, suggesting more robustness to variations in the training folds.\nIt is important to highlight that OMP-1 can be seen as a special case of our algorithm, where the activation function is |DW| and lifetime sparsity is not taken into account in the optimization process (potentially leading to dead out-\nputs). Our algorithm has several advantages over OMP-1: (1) It can use any activation function; (2) by enforcing lifetime sparsity it does not suffer of the dead output problem, thus not requiring ad-hoc tricks to avoid it; (3) it does not require whitening, which can be a problem if the input dimensionality is large (Le et al., 2011).\nWith our proposal, we advance in the meta-parameter free line of ICA (Hyva\u0308rinen & Oja, 2000) and sparse filtering (Ngiam et al., 2011). It is clear that the advantage of sparse filtering over ICA comes from removing the orthogonality constraint, and imposing some sort of \u201ccompetition\u201d between outputs, which also permits overcomplete representations. Following this spirit, our algorithm imposes an even more strict form of competition to prevent dead outputs by means of Strong Lifetime Sparsity and confirms the trend of (Ngiam et al., 2011; Hyva\u0308rinen & Oja, 2000) that data reconstruction seems not so important if the goal is to have a discriminative sparse system.\nLast and most importantly, it is worth highlighting five interesting properties of the EPLS algorithm. First, the method is meta-parameter free, which highly simplifies the training process for practitioners, especially when used as a greedy pre-training method in deep architectures. Second, the method is fast and scales linearly with the number of training samples and the input/output dimensionalities. Third, EPLS is easy to implement. We implemented the EPLS in Algorithm 1 in less than 50 lines of C code. The mini-batch vSGD is a general purpose optimizer; our Matlab implementation of vSGD plus the EPLS mex source will be publicly available after publication. Fourth, the proposed learning strategy is not limited to perceptrons. Fifth, there is an interest in the literature in avoiding redundancy in the image representation by using the algorithms in a convolutional fashion (Kavukcuoglu et al., 2010). For this purpose, the EPLS can be slightly modified to apply the procedure to a whole image at once and consider the minibatch size to be the image divided into patches. This aspect is not considered in the paper and is left for future investigation."}, {"heading": "7. Conclusion", "text": "In this paper, we introduced the Enforcing Population and Lifetime Sparsity method. The algorithm provides a metaparameter free, off-the-shelf, simple and computationally efficient approach for unsupervised sparse feature learning. It seeks both lifetime and population sparsity in an explicit way in order to learn discriminative features, thus preventing dead outputs.\nResults show that the method significantly outperforms all state-of-the-art methods on STL-10 dataset with lower standard deviation across folds, suggesting more robust-\nness across training sets."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Bengio", "Yoshua"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio and Yoshua.,? \\Q2009\\E", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "In NIPS, pp", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron C", "Vincent", "Pascal"], "venue": "IEEE TPAMI,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "On the difference between orthogonal matching pursuit and orthogonal least squares", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Unpublished manuscript,", "citeRegEx": "Blumensath and Davies,? \\Q2007\\E", "shortCiteRegEx": "Blumensath and Davies", "year": 2007}, {"title": "An analysis of singlelayer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In AISTATS, pp", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["Coates", "Adam", "Ng", "Andrew"], "venue": "In ICML, pp", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["Erhan", "Dumitru", "Courville", "Aaron", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "In AISTATS,", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "What is the goal of sensory coding", "author": ["D.J. Field"], "venue": "Neural Computation,", "citeRegEx": "Field,? \\Q1994\\E", "shortCiteRegEx": "Field", "year": 1994}, {"title": "Unsupervised and supervised visual codes with restricted boltzmann machines", "author": ["Goh", "Hanlin", "Thome", "Nicolas", "Cord", "Matthieu", "Lim", "Joo-Hwee"], "venue": "In ECCV,", "citeRegEx": "Goh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goh et al\\.", "year": 2012}, {"title": "A Practical Guide to Training Restricted Boltzmann Machines", "author": ["G.E. Hinton"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Hinton,? \\Q2010\\E", "shortCiteRegEx": "Hinton", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "YeeWhye"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural Networks,", "citeRegEx": "Hyv\u00e4rinen and Oja,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Oja", "year": 2000}, {"title": "Independent component analysis", "author": ["A. Hyv\u00e4rinen", "J. Karhunen", "E. Oja"], "venue": "Wiley Interscience,", "citeRegEx": "Hyv\u00e4rinen et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen et al\\.", "year": 2000}, {"title": "Learning convolutional feature hierachies for visual recognition", "author": ["Kavukcuoglu", "Koray", "Sermanet", "Pierre", "Boureau", "Y-Lan", "Gregor", "Karol", "Mathieu", "Micha\u00ebl", "LeCun", "Yann"], "venue": "In NIPS,", "citeRegEx": "Kavukcuoglu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2010}, {"title": "The hungarian method for the assignment problem", "author": ["Kuhn", "Harold W"], "venue": "Naval Research Logistics Quarterly,", "citeRegEx": "Kuhn and W.,? \\Q1955\\E", "shortCiteRegEx": "Kuhn and W.", "year": 1955}, {"title": "Exploring strategies for training deep neural networks", "author": ["Larochelle", "Hugo", "Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Lamblin", "Pascal"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Larochelle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2009}, {"title": "ICA with reconstruction cost for efficient overcomplete feature learning", "author": ["Q.V. Le", "A. Karpenko", "J. Ngiam", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Efficient backprop. In Neural Networks: Tricks of the Trade, pp. 9\u201350", "author": ["LeCun", "Yann", "Bottou", "Leon", "Orr", "Genevieve", "M\u00fcller", "Klaus"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A.Y. Ng"], "venue": "In NIPS, pp", "citeRegEx": "Lee et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Lee et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by v1", "author": ["B. Olshausen", "D.J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1997}, {"title": "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition", "author": ["Y.C. Pati", "R. Rezaifar", "P.S. Krishnaprasad"], "venue": "In ACSSC,", "citeRegEx": "Pati et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pati et al\\.", "year": 1993}, {"title": "Self-taught learning: Transfer learning from unlabeled data", "author": ["Raina", "Rajat", "Battle", "Alexis", "Lee", "Honglak", "Packer", "Benjamin", "Ng", "Andrew Y"], "venue": "In ICML, pp", "citeRegEx": "Raina et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Raina et al\\.", "year": 2007}, {"title": "Efficient learning of sparse representations with an energybased model", "author": ["M.A. Ranzato", "C. Poultney", "S. Chopra", "Y. Lecun"], "venue": "In NIPS,", "citeRegEx": "Ranzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2006}, {"title": "No More Pesky Learning Rates", "author": ["Schaul", "Tom", "Zhang", "Sixin", "LeCun", "Yann"], "venue": "In ICML,", "citeRegEx": "Schaul et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2013}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "In NIPS, pp", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In IEEE CVPR,", "citeRegEx": "Yang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2009}, {"title": "Stochastic pooling for regularization of deep convolutional neural networks", "author": ["Zeiler", "Matthew D", "Fergus", "Rob"], "venue": "CoRR, abs/1301.3557,", "citeRegEx": "Zeiler et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": "In tasks such as image classification and object recognition, unsupervised learned features have shown to compete well or even outperform manually designed ones (Ranzato et al., 2006; Yang et al., 2009; Coates et al., 2011).", "startOffset": 161, "endOffset": 223}, {"referenceID": 26, "context": "In tasks such as image classification and object recognition, unsupervised learned features have shown to compete well or even outperform manually designed ones (Ranzato et al., 2006; Yang et al., 2009; Coates et al., 2011).", "startOffset": 161, "endOffset": 223}, {"referenceID": 4, "context": "In tasks such as image classification and object recognition, unsupervised learned features have shown to compete well or even outperform manually designed ones (Ranzato et al., 2006; Yang et al., 2009; Coates et al., 2011).", "startOffset": 161, "endOffset": 223}, {"referenceID": 10, "context": "Unsupervised feature learning has also shown to be helpful in greedy layerwise pre-training of deep architectures (Hinton et al., 2006; Bengio et al., 2006; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 114, "endOffset": 201}, {"referenceID": 1, "context": "Unsupervised feature learning has also shown to be helpful in greedy layerwise pre-training of deep architectures (Hinton et al., 2006; Bengio et al., 2006; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 114, "endOffset": 201}, {"referenceID": 15, "context": "Unsupervised feature learning has also shown to be helpful in greedy layerwise pre-training of deep architectures (Hinton et al., 2006; Bengio et al., 2006; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 114, "endOffset": 201}, {"referenceID": 6, "context": "Unsupervised feature learning has also shown to be helpful in greedy layerwise pre-training of deep architectures (Hinton et al., 2006; Bengio et al., 2006; Larochelle et al., 2009; Erhan et al., 2010).", "startOffset": 114, "endOffset": 201}, {"referenceID": 2, "context": "\u201d In addition to that, one of the main criticisms to state-of-the-art methods is that they require a significant amount of metaparameters (Bengio et al., 2013).", "startOffset": 138, "endOffset": 159}, {"referenceID": 25, "context": "As stated in (Snoek et al., 2012), the tuning of these meta-parameters is a laborious task that requires expert knowledge, rules of thumb or extensive search and, whose setting can vary for different tasks.", "startOffset": 13, "endOffset": 33}, {"referenceID": 25, "context": ", 2011) and automatic approaches to optimize the performance of learning algorithms (Snoek et al., 2012).", "startOffset": 84, "endOffset": 104}, {"referenceID": 12, "context": "To the best of our knowledge, work in this direction includes ICA (Hyv\u00e4rinen & Oja, 2000; Hyv\u00e4rinen et al., 2000) and sparse filtering (Ngiam et al.", "startOffset": 66, "endOffset": 113}, {"referenceID": 16, "context": "Although ICA provides good results at object recognition tasks (Le et al., 2011; Ngiam et al., 2011), the method scales poorly to large datasets and high input dimensionality.", "startOffset": 63, "endOffset": 100}, {"referenceID": 18, "context": "Significant amount of work has been done in order to overcome this limitation (Lee et al., 2006; Kavukcuoglu et al., 2010).", "startOffset": 78, "endOffset": 122}, {"referenceID": 13, "context": "Significant amount of work has been done in order to overcome this limitation (Lee et al., 2006; Kavukcuoglu et al., 2010).", "startOffset": 78, "endOffset": 122}, {"referenceID": 13, "context": "Predictive Sparse Decomposition (PSD) (Kavukcuoglu et al., 2010) is a successful variant of sparse coding, which uses a predictor to approximate the sparse representation and solves the sparse coding computationally expensive encoding step.", "startOffset": 38, "endOffset": 64}, {"referenceID": 10, "context": "Sparse RBM (Hinton et al., 2006; Lee et al., 2008) weight decay, sparseness constant, sparsity penalty, momentum Sparse auto-encoders (Ranzato et al.", "startOffset": 11, "endOffset": 50}, {"referenceID": 19, "context": "Sparse RBM (Hinton et al., 2006; Lee et al., 2008) weight decay, sparseness constant, sparsity penalty, momentum Sparse auto-encoders (Ranzato et al.", "startOffset": 11, "endOffset": 50}, {"referenceID": 23, "context": ", 2008) weight decay, sparseness constant, sparsity penalty, momentum Sparse auto-encoders (Ranzato et al., 2006) weight decay, sparseness constant, sparsity penalty Sparse Coding (Olshausen & Field, 1997) sparsity penalty RICA (Le et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 16, "context": ", 2006) weight decay, sparseness constant, sparsity penalty Sparse Coding (Olshausen & Field, 1997) sparsity penalty RICA (Le et al., 2011) reconstruction penalty PSD (Kavukcuoglu et al.", "startOffset": 122, "endOffset": 139}, {"referenceID": 13, "context": ", 2011) reconstruction penalty PSD (Kavukcuoglu et al., 2010) sparsity penalty, prediction penalty OMP-k (Pati et al.", "startOffset": 35, "endOffset": 61}, {"referenceID": 21, "context": ", 2010) sparsity penalty, prediction penalty OMP-k (Pati et al., 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) k (non-zero elements) ICA (Hyv\u00e4rinen & Oja, 2000; Hyv\u00e4rinen et al.", "startOffset": 51, "endOffset": 116}, {"referenceID": 12, "context": ", 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) k (non-zero elements) ICA (Hyv\u00e4rinen & Oja, 2000; Hyv\u00e4rinen et al., 2000) Sparse Filtering (Ngiam et al.", "startOffset": 80, "endOffset": 127}, {"referenceID": 10, "context": "Commonly used algorithms for unsupervised feature learning include Restricted Boltzmann Machines (RBM) (Hinton et al., 2006), auto-encoders (Bengio et al.", "startOffset": 103, "endOffset": 124}, {"referenceID": 1, "context": ", 2006), auto-encoders (Bengio et al., 2006), sparse coding (Raina et al.", "startOffset": 23, "endOffset": 44}, {"referenceID": 22, "context": ", 2006), sparse coding (Raina et al., 2007) and hybrids such as PSD (Kavukcuoglu et al.", "startOffset": 23, "endOffset": 43}, {"referenceID": 13, "context": ", 2007) and hybrids such as PSD (Kavukcuoglu et al., 2010).", "startOffset": 32, "endOffset": 58}, {"referenceID": 12, "context": "Many other methods such as ICA (Hyv\u00e4rinen & Oja, 2000; Hyv\u00e4rinen et al., 2000), Reconstruction ICA (RICA) (Le et al.", "startOffset": 31, "endOffset": 78}, {"referenceID": 16, "context": ", 2000), Reconstruction ICA (RICA) (Le et al., 2011), Sparse Filtering (Ngiam et al.", "startOffset": 35, "endOffset": 52}, {"referenceID": 21, "context": ", 2011) and methods related to vector quantization such as Orthogonal Matching Pursuit (OMP-k) (Pati et al., 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) have also been used in the literature to extract unsupervised feature representations.", "startOffset": 95, "endOffset": 160}, {"referenceID": 23, "context": "Sparse auto-encoders (Ranzato et al., 2006), sparse RBM (Hinton et al.", "startOffset": 21, "endOffset": 43}, {"referenceID": 10, "context": ", 2006), sparse RBM (Hinton et al., 2006; Lee et al., 2008; Hinton, 2010; Goh et al., 2012), sparse coding (Olshausen & Field, 1997), PSD (Kavukcuoglu et al.", "startOffset": 20, "endOffset": 91}, {"referenceID": 19, "context": ", 2006), sparse RBM (Hinton et al., 2006; Lee et al., 2008; Hinton, 2010; Goh et al., 2012), sparse coding (Olshausen & Field, 1997), PSD (Kavukcuoglu et al.", "startOffset": 20, "endOffset": 91}, {"referenceID": 9, "context": ", 2006), sparse RBM (Hinton et al., 2006; Lee et al., 2008; Hinton, 2010; Goh et al., 2012), sparse coding (Olshausen & Field, 1997), PSD (Kavukcuoglu et al.", "startOffset": 20, "endOffset": 91}, {"referenceID": 8, "context": ", 2006), sparse RBM (Hinton et al., 2006; Lee et al., 2008; Hinton, 2010; Goh et al., 2012), sparse coding (Olshausen & Field, 1997), PSD (Kavukcuoglu et al.", "startOffset": 20, "endOffset": 91}, {"referenceID": 13, "context": ", 2012), sparse coding (Olshausen & Field, 1997), PSD (Kavukcuoglu et al., 2010), OMP-k (Pati et al.", "startOffset": 54, "endOffset": 80}, {"referenceID": 21, "context": ", 2010), OMP-k (Pati et al., 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) and Reconstruction ICA (RICA) (Le et al.", "startOffset": 15, "endOffset": 80}, {"referenceID": 16, "context": ", 1993; Blumensath & Davies, 2007; Coates & Ng, 2011) and Reconstruction ICA (RICA) (Le et al., 2011) explicitly model the data distribution by minimizing the reconstruction error.", "startOffset": 84, "endOffset": 101}, {"referenceID": 7, "context": "Sparsity is among the desirable properties of a good output representation (Field, 1994; Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011; Bengio et al., 2013).", "startOffset": 75, "endOffset": 211}, {"referenceID": 23, "context": "Sparsity is among the desirable properties of a good output representation (Field, 1994; Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011; Bengio et al., 2013).", "startOffset": 75, "endOffset": 211}, {"referenceID": 19, "context": "Sparsity is among the desirable properties of a good output representation (Field, 1994; Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011; Bengio et al., 2013).", "startOffset": 75, "endOffset": 211}, {"referenceID": 16, "context": "Sparsity is among the desirable properties of a good output representation (Field, 1994; Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011; Bengio et al., 2013).", "startOffset": 75, "endOffset": 211}, {"referenceID": 2, "context": "Sparsity is among the desirable properties of a good output representation (Field, 1994; Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011; Bengio et al., 2013).", "startOffset": 75, "endOffset": 211}, {"referenceID": 7, "context": "There seems to be a consensus to overcome such degenerate solutions, which is to ensure similar statistics among outputs (Field, 1994; Willmore & Tolhurst, 2001; Ranzato et al., 2006; Ngiam et al., 2011).", "startOffset": 121, "endOffset": 203}, {"referenceID": 23, "context": "There seems to be a consensus to overcome such degenerate solutions, which is to ensure similar statistics among outputs (Field, 1994; Willmore & Tolhurst, 2001; Ranzato et al., 2006; Ngiam et al., 2011).", "startOffset": 121, "endOffset": 203}, {"referenceID": 23, "context": "number of non-zero elements per output code, whereas the methods in (Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011) do not explicitly define the proportion of outputs expected to be active at the same time.", "startOffset": 68, "endOffset": 170}, {"referenceID": 19, "context": "number of non-zero elements per output code, whereas the methods in (Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011) do not explicitly define the proportion of outputs expected to be active at the same time.", "startOffset": 68, "endOffset": 170}, {"referenceID": 16, "context": "number of non-zero elements per output code, whereas the methods in (Olshausen & Field, 1997; Ranzato et al., 2006; Lee et al., 2008; Le et al., 2011; Ngiam et al., 2011) do not explicitly define the proportion of outputs expected to be active at the same time.", "startOffset": 68, "endOffset": 170}, {"referenceID": 24, "context": "The system might be trained by means of an off-the-shelf mini-batch Stochastic Gradient Descent (SGD) method with adaptive learning rates such as variance-based SGD (vSGD) (Schaul et al., 2013).", "startOffset": 172, "endOffset": 193}, {"referenceID": 17, "context": "Starting with \u0393 set to small random numbers as in (LeCun et al., 1998) (line 1), at each epoch we shuffle the samples of the training set (line 3), reset the EPLS inhibitor a to a flat activation (line 4) and process all mini-batches.", "startOffset": 50, "endOffset": 70}, {"referenceID": 24, "context": "After that, the gradient of the error is computed (line 9) and the learning rate \u03b7 is estimated as in (Schaul et al., 2013) (line 10).", "startOffset": 102, "endOffset": 123}, {"referenceID": 24, "context": "9: G = \u2207\u0393||H \u2212T||2 10: Estimate learning rate \u03b7 as in (Schaul et al., 2013) 11: \u0393 = \u0393\u2212 \u03b7G 12: end for 13: Limit the bases W in \u0393 to have unit norm 14: until stop condition verified", "startOffset": 54, "endOffset": 75}, {"referenceID": 4, "context": "The performance of training and encoding strategies in single layer networks has been extensively analyzed in the literature (Coates et al., 2011; Coates & Ng, 2011; Ngiam et al., 2011) on STL-101 dataset.", "startOffset": 125, "endOffset": 185}, {"referenceID": 4, "context": "To validate our method, we follow the experimental pipeline of (Coates et al., 2011).", "startOffset": 63, "endOffset": 84}, {"referenceID": 16, "context": "Single-Layer with meta-parameters RICA (Le et al., 2011) (1600/Natural) 52.", "startOffset": 39, "endOffset": 56}, {"referenceID": 16, "context": "Our results show that simultaneously enforcing both population and lifetime sparsity helps in learning discriminative dictionaries, which reflect in better performance, especially when compared to meta-parameter free methods (Ngiam et al., 2011; Le et al., 2011).", "startOffset": 225, "endOffset": 262}, {"referenceID": 16, "context": "Our algorithm has several advantages over OMP-1: (1) It can use any activation function; (2) by enforcing lifetime sparsity it does not suffer of the dead output problem, thus not requiring ad-hoc tricks to avoid it; (3) it does not require whitening, which can be a problem if the input dimensionality is large (Le et al., 2011).", "startOffset": 312, "endOffset": 329}, {"referenceID": 13, "context": "Fifth, there is an interest in the literature in avoiding redundancy in the image representation by using the algorithms in a convolutional fashion (Kavukcuoglu et al., 2010).", "startOffset": 148, "endOffset": 174}], "year": 2014, "abstractText": "We propose a meta-parameter free, off-the-shelf, simple and fast unsupervised feature learning algorithm, which exploits a new way of optimizing for sparsity. Experiments on STL-10 show that the method presents state-of-the-art performance and provides discriminative features that generalize well.", "creator": "LaTeX with hyperref package"}}}