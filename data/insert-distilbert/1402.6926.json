{"id": "1402.6926", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2014", "title": "Sequential Complexity as a Descriptor for Musical Similarity", "abstract": "we propose string compressibility as a descriptor of temporal structure in audio, for if the purpose be of determining musical similarity. our descriptors are based on computing noisy track - wise byte compression rates of quantised audio features, using multiple temporal resolutions mapping and quantisation granularities.", "histories": [["v1", "Thu, 27 Feb 2014 14:51:48 GMT  (110kb,D)", "https://arxiv.org/abs/1402.6926v1", "27 pages, 11 figures, 4 tables"], ["v2", "Fri, 28 Feb 2014 15:14:37 GMT  (110kb,D)", "http://arxiv.org/abs/1402.6926v2", "27 pages, 11 figures, 4 tables"], ["v3", "Sun, 28 Sep 2014 23:33:44 GMT  (144kb,D)", "http://arxiv.org/abs/1402.6926v3", "13 pages, 9 figures, 8 tables. Accepted version"]], "COMMENTS": "27 pages, 11 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.IR cs.LG cs.SD", "authors": ["peter foster", "matthias mauch", "simon dixon"], "accepted": false, "id": "1402.6926"}, "pdf": {"name": "1402.6926.pdf", "metadata": {"source": "CRF", "title": "Sequential Complexity as a Descriptor for Musical Similarity", "authors": ["Peter Foster", "Simon Dixon"], "emails": ["pubs-permissions@ieee.org.", "peter.foster@eecs.qmul.ac.uk;", "matthias.mauch@eecs.qmul.ac.uk;", "mon.dixon@eecs.qmul.ac.uk)"], "sections": [{"heading": null, "text": "Index Terms\u2014Music content analysis, musical similarity measures, time series complexity\nI. INTRODUCTION\nWe are concerned with the task of quantifying musical similarity, which has received considerable interest in the field of audio-based music content analysis [1], [2]. Owing to the proliferation of music in digital formats and the expansion of web-based music databases, there is an impetus to develop novel search, navigation and recommendation systems. Music content analysis has found application in such information retrieval systems as an alternative to manual annotation processes, when the latter are infeasible, unavailable or amenable to be supplemented [3].\nWe may distinguish between music content analysis applications such as audio fingerprinting [4], version identification [5], genre classification [6] and mood identification [7]. Given a query track, audio fingerprinting typically should identify a unique track deemed similar with respect to a collection. In contrast, for genre and mood classification, the set of tracks deemed similar with respect to a collection is typically large. Thus, we may distinguish between music classification tasks according to the degree of specificity associated with the measure of musical similarity [1].\nCopyright c\u00a9 2014 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org.\nP.F. is funded by an Engineering and Physical Sciences Research Council Doctoral Training Account studentship. M.M. is funded by a Royal Academy of Engineering Research Fellowship.\nAll authors are with the School of Electronic Engineering and Computer Science, Queen Mary University of London, London, E1 4NS, UK. (Email: peter.foster@eecs.qmul.ac.uk; matthias.mauch@eecs.qmul.ac.uk; simon.dixon@eecs.qmul.ac.uk)\nIn this work, we consider two low-specificity tasks, namely similarity rating prediction and song year prediction. An important issue in our considered domain surrounds feature representation. In particular, we address the problem of representing temporal structure in audio features. We refer to summary statistics of audio features extracted from a song as descriptors. Descriptors may be characterised according to how temporal structure is accounted for [2]. We may distinguish between bag-of-features representations [8], which discard information on temporal structure, and sequential representations. As a sequential representation, we propose to estimate the complexity of audio feature time series, where we quantify complexity in terms of string compressibility. As a result, we obtain scalar-valued summary statistics which retain information on temporal structure.\nWe motivate our evaluations involving similarity rating prediction and song year prediction to test the hypothesis that our complexity descriptors capture temporal information in audio features and that such information is relevant for determining musical similarity. For similarity rating prediction, our ground truth is given by human similarity judgements and we assume that an objective musical similarity correlates with subjects\u2019 degree of perceived musical similarity, based on a five-point rating scale. For song year prediction, our ground truth is readily given by chart entry times of songs and we assume that musical similarity correlates with chart entry time proximity. Whereas song year prediction has received little attention in the literature, the song year is important in determining musical preference [9]. Thus, song year prediction might be applied in music recommendation [10]. Song year prediction might furthermore be incorporated in genre classification tasks, since musical genres are associated with particular years.\nSection II provides an overview of methods and descriptors for computing low-specificity similarity. In Section III, we describe our approach. In Section IV, we detail our experimental method and results; we provide separate accounts for similarity rating prediction and song year prediction in Sections IV-A and IV-B, respectively. Finally, in Section V we provide conclusions."}, {"heading": "II. BACKGROUND", "text": "For a detailed review of recent literature on methods for determining musical similarity, from the perspective of classification, we refer to the work of Fu et al. [2]. To determine musical similarity, one possible approach involves computing pairwise distances between tracks. The obtained distances may then be used for classification. A second approach consists in applying track-wise descriptors directly for classification.\nar X\niv :1\n40 2.\n69 26\nv3 [\ncs .I\nR ]\n2 8\nSe p\n20 14\nBased on the second approach, Tzanetakis and Cook [11] compute first and second-order moments on spectral features including MFCCs, to perform genre classification using the k-nearest neighbours (KNN) algorithm and Gaussian mixture models (GMMs) estimated on each target class. Li and Ogihara [12] propose to classify Daubechies wavelet histograms using GMMs and KNN for genre and mood classification. Using spectral features, West et al. [13] propose methods for learning similarity functions based on constructing decision trees for genre classification. Slaney et al. [14] propose feature transformations based on supervised learning and using onset and loudness features, for the purpose of album and artist classification.\nBased on the approach of determining distances between descriptors, Logan and Salomon [15] propose to estimate GMMs on individual tracks. Pairwise track distances are then computed using a combination of Kullback-Leibler divergence (KLD) and earth mover\u2019s distance, where the KLD is used to compare pairs of track centroids. The approach based on KLD assumes that each centroid follows a Gaussian distribution; thus the KLD may be computed in closed form as\nKLD = 1\n2\n( tr ( \u03a3\u221211 \u03a32 ) + (\u00b51 \u2212 \u00b52) T \u03a3\u221211 (\u00b51 \u2212 \u00b52)\n\u2212h\u2212 log |\u03a32| |\u03a31| ) (1) where \u03a31,\u03a32 and \u00b51,\u00b52 respectively denote the mean and covariance of two multivariate Gaussian distributions with dimensionality h. Aucouturier and Pachet [16] in contrast compute cross-likelihoods between GMMs using Monte Carlo approximations for the purpose of genre classification, whereas Berenzweig et al. [17] consider the asymptotic likelihood approximation of the KLD and centroid distances for the task of similarity rating prediction. Mandel and Ellis [18] instead represent tracks as single Gaussians and use (1) as a distance measure between track pairs. The obtained distances are then applied to artist identification, using support vector machines (SVMs) for classification. An alternative approach to computing the KLD is based on computing histograms of quantised features, as proposed by Vignoli and Pauws [19] for playlist recommendation; Levy and Sandler [20] compare approaches in the context of genre classification.\nThe previously described techniques are commonly referred to bag-of-features approaches, since they discard information on temporal structure. Yet, the relative convenience of bagof-features approaches stands in contrast to the importance of temporal structure in perception of musical timbre, as observed by McAdams et al. [21]. Aucouturier and Pachet [8] argue that the bag-of-features approach is insufficient to model polyphonic music for determining similarity. Sequential representations based on mid-level features are widely applied for the purpose of version identification [5]. For lowspecificity classification, one possible approach to mitigating the shortcoming of the bag-of-features approach involves the intermediate step of aggregating features locally, before summarising anew using obtained summary statistics. Tzanetakis and Cook [11] propose to estimate the local mean and variance of features contained in a 1s window. For the task of predicting\nmusical similarity, Seyerlehner et al. [22] apply a single, global summarisation step to overlapping windows, computing variance and percentiles. For the purpose of local aggregation, alternative pooling functions are considered by Mo\u0308rchen et al. [23], Hamel et al. [24], Wu\u0308lfing and Riedmiller [25].\nAn alternative approach is based on retaining the temporal order of features at each window position. Spectral analysis may be applied to the original features, resulting in a new feature sequence. Pampalk [26] proposes fluctuation patterns describing loudness modulation across frequency bands, whereas Lee et al. [27] propose statistics based on modulation spectral analysis. Mo\u0308rchen et al. [23] consider a variety of statistics based on spectral analysis and autocorrelation. Meng et al. [28], Coviello et al. [29] apply multivariate autoregressive modelling to windowed features, for the tasks of genre and tag classification.\nTo account for temporal structure, statistical modelling may be applied to quantised features. For genre classification, Li and Sleep [30] propose an SVM kernel in which pairwise distances are obtained by comparing dictionaries generated using the Lempel-Ziv compression algorithm [31]. Reed and Lee [32] apply latent semantic analysis to unigram and bigram counts for classification using SVMs, whereas Langlois and Marques [33] propose to estimate language models for computing sequence cross-likelihoods for genre and artist classification. Ren and Jang [34] propose an algorithm for computing histograms of feature codeword sequences for genre classification.\nRecent approaches attempt to model temporal structure using representations constructed at multiple time scales. Based on a bag-of-features approach, Foucard et al. [35] propose an ensemble of classifiers, where each classifier is trained on features at a given time scale. Features at successive resolutions are aggregated using averaging. Applied to tag and instrument classification, results indicate that a multiscale approach benefits performance. Dieleman and Schrauwen [36] apply feature learning based on spherical K-means clustering to tag classification. Evaluated aggregation techniques are based on varying the spectrogram window size, in addition to Gaussian and Laplacian pyramid smoothing techniques. Although not applied to classification, Mauch and Levy [37] propose a similar smoothing approach for characterising structural change at multiple time scales. Finally, convolutional neural networks have been proposed for modelling temporal structure: Dieleman et al. [38] propose deep learning architectures for genre, artist and key classification tasks. Hamel et al. [24] propose a deep learning architecture incorporating multiple feature aggregation functions for tag classification.\nThe approach proposed in this work resembles methods applying statistical models to quantised feature sequences [30], [32]\u2013[34]. In contrast, we propose to compute summary statistics directly from estimated sequential models. Since the obtained statistics may be compared using a metric, our approach has the potential to be combined with indexing and hashing schemes for computationally efficient retrieval [39]\u2013 [41], while retaining information on temporal structure. Our method of computing multiple representations using downsampling resembles the approach proposed by Dieleman and\nSchrauwen [36]. Note that our approach differs from Cilibrasi et al. [42], who propose pairwise sequence compressibility to quantify similarity. We did not pursue this approach for low-specificity tasks, based on results for the pairwise prediction approach reported in Section IV-A4. Note that we may take compression rates as estimates of sequential Shannon entropy rates, inviting further comparison or combination with related measures of sequential complexity [43]\u2013[45]. Such measures have to date not been evaluated quantitatively in music content analysis, inviting further investigation beyond the scope of this work."}, {"heading": "III. APPROACH", "text": "Assume that we are given the audio feature vector sequence V = (v1, . . . ,vT ). Similar to the descriptor proposed in [46], as a means of quantifying the sequential complexity of V, we compute the compression rate R\u03bb(V),\nR\u03bb(V) = C(V, \u03bb)\nT (2)\nwhere C(V, \u03bb) denotes the number of bits required to represent V, given a quantisation scheme with \u03bb levels and using a specified sequential compression scheme. To obtain a lengthinvariant measure of sequential complexity, we normalise with respect to the sequence length T .\nGiven the ith track in our collection, we compute compression rates for feature sequences extracted from musical audio. We refer to the set of compression rates as feature complexity descriptors (FCDs). For features based on constant frame rate, we compute FCDs using the original feature sequence, in addition to FCDs computed on downsampled versions of the original sequence; we consider downsampling factors 1, 2, 4, 8. We distinguish among temporal resolutions using the labels FCD1, FCD2, FCD4, FCD8, respectively. For features based on variable frame rate, we compute FCDs with no further downsampling applied.\nThus proposed, consider FCDs computed on a hypothetical scalar-valued feature sequence exhibiting a high amount of temporal structure, either due to periodicity or locally constant regions (Fig. 1 (a), (b)). For such sequences, we obtain low values for R\u03bb, since the quantised feature sequence may be encoded efficiently. Conversely, if we discard temporal structure by randomly shuffling the original feature sequence (Fig. 1 (c)), we obtain high values for R\u03bb, since the quantised feature sequence no longer admits an efficient encoding. In contrast to FCDs, feature moments such as mean and variance are invariant to any such re-ordering of features. We observe that feature moments have been widely applied for lowspecificity content analysis tasks. Considering that FCDs have similar dimensionality to feature moments and assuming that temporal order of features is informative for our considered tasks, we therefore expect that FCDs may be used to improve prediction accuracy with respect to using feature moments alone, for our considered tasks."}, {"heading": "A. Similarity rating prediction", "text": "For the task of similarity rating prediction, assume that we have a distance metric which we use to compare descriptor\nvectors computed on pairs of tracks. We hypothesise that the pairwise distance between descriptors correlates with the similarity rating associated with track pairs. To predict similarity ratings we take as our feature space pairwise distances between descriptor vectors and apply multinomial regression. We use ri,n to denote the nth descriptor vector computed for the ith track in our collection, with 1 \u2264 n \u2264 N and given a set of N available descriptor vectors. We compute separate descriptor vectors across audio features and across FCD resolutions, with each vector component in ri,n corresponding to a quantisation granularity \u03bb. We denote with d\u3008i,j\u3009 the distances between ri,n, rj,n obtained across all N descriptor vectors, using our assumed distance measure. Given the pair of tracks \u3008i, j\u3009 whose similarity rating we seek to predict, we estimate the probability of similarity score k \u2208 [1 ..K] as\nP ( S = k|d\u3008i,j\u3009 ) =\nexp ( \u03b2Tk d\u3008i,j\u3009 + \u03b3k ) \u2211K m=1 exp ( \u03b2Tm d\u3008i,j\u3009 + \u03b3m\n) (3) where \u03b2k, \u03b3k are the model parameters associated with outcome k, given a total of K similarity scores. We predict similarity ratings by determining the value of k which maximises P ( S = k|d\u3008i,j\u3009 ) . We describe our model estimation method in Section IV-A3."}, {"heading": "B. Song year prediction", "text": "For the task of song year prediction, we hypothesise that descriptor values correlate with the chart entry date of tracks. Following [10] we apply a linear regression model. Given the ith track in our collection, we predict the associated chart entry date yi using a linear combination of components in descriptor vectors ri,n,\ny\u0302i = N\u2211 n=1 \u03b8Tnri,n + \u03b1 (4)\nwhere \u03b8n denotes regression coefficients for the nth descriptor vector as specified for similarity rating prediction, and where \u03b1 denotes the model intercept. We describe our model estimation method for song year prediction in Section IV-B1. We motivate use of both multinomial and linear regression techniques as a straightforward means of evaluating the utility of FCDs for determining similarity based on a metric space. We perform our evaluation by considering predictive accuracy, in addition to interpreting estimated coefficients as feature utilities."}, {"heading": "IV. EVALUATION", "text": "For our evaluations, we use a collection of 15 473 entries from the American Billboard Hot 100 singles popularity chart1. Each entry in the dataset is represented by a track excerpt of approximately 30s of audio, and is annotated with a chart entry date. Chart entry dates span the years 1957\u20132010 (M = 1982.9y, SD = 15.4y).\nFor each track excerpt in the dataset, we extract a set of 25 audio features, using MIRToolbox [47] version 1.3.2 and using the framewise chromagram representation proposed by Ellis and Poliner [48]. With the exception of rhythmic features, which are computed using predicted onsets, features are based on a constant frame rate of 40Hz. Table I summarises the set of evaluated audio features.\nIn addition to FCDs, for each track excerpt we compute the mean and standard deviation, based on frame-level representation with no downsampling applied. We refer to the latter non-sequential descriptors as feature moment descriptors (FMDs). We compute FCDs as described in Section III, where for the case of the vector-valued features chroma, MFCCs and delta-MFCCs we apply principal component analysis (PCA)\n1http://www.billboard.com\nin track-wise fashion as a preliminary decorrelation step. We then quantise and compress each resulting component separately, before averaging obtained compression lengths across components. We apply PCA, since we seek to quantify temporal structure in feature vector sequences while disregarding any correlation among feature vector components. We quantise features by applying equal-frequency binning with \u03bb \u2208 {3, 4, 5} levels; we perform relatively coarse quantisation to ensure that each symbol occurs frequently, regardless of downsampling factor.\nWe choose equal-frequency binning to ensure that obtained strings have a consistent stationary distribution; the obtained compression rates therefore are a function of temporal structure alone. The value log \u03bb may be interpreted as the theoretical compression rate for a temporally uncorrelated sequence. We compress symbol sequences using the prediction by partial match (PPM) algorithm2, described in [55]. We consider PPM a general-purpose string compression algorithm which may be substituted with an alternative compressor; in initial experiments we obtained similar results using Lempel-Ziv compression [31]. Nevertheless, we note that PPM compresses efficiently compared to alternative compression schemes [55]. We set the PPM model order to 5 symbols, based on the observation that for uncorrelated sequences, distinct substrings of length 5 are unlikely to occur frequently.\nWith a view to characterising the feature space represented by FCDs, we perform a track-wise exploratory analysis of computed FCDs. For each track excerpt in our collection, we compute FCDs based on MFCC features alone. We obtain a scalar-valued score for each excerpt by averaging FCDs across quantisation levels \u03bb and across temporal resolutions. Next, across artists in our collection we compute the median of obtained FCD scores. To facilitate interpretation, we consider only artists with a minimum number of 20 chart entries; thus out of 5 455 artists in our collection we consider 129 artists. We then rank artists according to median FCD scores. Shown in Table II, we report the 20 lowest-ranking and highestranking artists. Additionally, across artists we report as medoid tracks those tracks whose FCD score minimises the error with respect to the median.\nComparing track groups, the lowest-ranking artists are predominantly vocalists with repertoire of jazz ballads and slowmoving pieces (e.g. Johnny Mathis, Barbara Streisand). In contrast, the artists with highest complexity values stand for music with strong percussive and aggressive components, from up-tempo surf-rock (Jan & Dean), through 1980s Power Rock (Van Halen) and Hip Hop (Eminem). Informal listening to medoid tracks supports this observation, with the exception of the medoid track by artist Etta James. We view this observation in support of our expectation that FCDs may be useful for lowspecificity similarity and subsequently demonstrate validity of our expectation for the similarity tasks considered in this work. Note however that we make no claim that FCDs capture any notion of musical complexity as proposed in [56]. While beyond the scope of this paper, track-wise analysis of FCDs merits further investigation.\n2http://www.cs.technion.ac.il/\u223cronbeg/vmm/index.html"}, {"heading": "A. Similarity rating prediction", "text": "We evaluate similarity rating prediction using annotations collected for a subset of the chart music dataset. Prior to our investigations, we obtained a total of 7 784 pairwise similarity ratings from 456 subjects participating in a webbased listening test3. Subjects were asked to quantify pairwise musical similarity between successive pairs of track excerpts using a five-point ordinal scale, with score \u20181\u2019 corresponding to \u2018not similar\u2019 and score \u20185\u2019 corresponding to \u2018very similar\u2019. We assume that subjects have an internal similarity scale which they use to perform ratings. Therefore, we omit any training step from the rating process. Note that while we prescribe that pairwise similarity ratings are made using a five-point scale, we do not assume that similarities are judged using an absolute scale across listeners. Given three track pairs for which we have respective ratings (4, 5), (5, 5), (1, 2), we view the ratings as quantifying relative agreement, compared to (4, 1), (5, 1), (1, 4).\nFor human similarity judgements, two issues prompt consideration: In addition to music being inherently subjective [57], human similarity judgements are context-dependent [58], [59]. We motivate our assumption of an internal similarity scale on the basis that Western popular music is widely disseminated and that listeners might form similarity judgements using a common factor. We verify our assumptions by quantifying similarity rating agreement.\nWhen presenting track pairs to listeners, we select the first song in each pair using uniform sampling. For the second song in each pair, we again apply uniform sampling, however we bias towards proximate chart entry times by restricting the permissible chart entry deviation to \u2264 1y with probability 0.9. We bias as a means of controlling for historical changes in audio production, which might affect similarity ratings [60]. We obtain a median of 6 ratings per subject, with each\n3http://webprojects.eecs.qmul.ac.uk/matthiasm/audioquality-pre/check.php\nrating corresponding to a unique track pair. Table III displays obtained score counts.\nAs shown in Table III, the majority of ratings are associated with scores less than \u20183\u2019, corresponding to relative dissimilarity on the five-point scale. We contend that for music content analysis based on an ensemble of systems as proposed in [61], the entire target set of predicted musical similarity might be used when forming recommendations. In contrast, for track recommendation relying on predicted similarity alone, when forming recommendations, it is typically of interest to consider tracks deemed similar to a query, while disregarding tracks deemed dissimilar [62]. Pertaining to the first use case, we perform evaluations using the five-point scale ratings, as defined previously. Pertaining to the second use case, we merge similarity ratings with scores \u20181\u2019 and \u20182\u2019, thus discarding any distinction between similarity ratings with low scores. We then perform our evaluations using the resulting four-point scale ratings.\nTo assess the consistency of similarity ratings, we collected an additional set of similarity ratings under controlled experimental conditions, involving 12 subjects aged 21y\u201342y. Subjects were assessed using the Ollen musical sophistication index (OMSI) [63]. We obtain a median OMSI score score of 241, with an associated median of 0.75 years of formal musical training. To avoid subject fatigue, we imposed no minimum number of ratings per subject, and collected ratings during two 30-minute sessions. We selected stimuli by sampling uniformly from the set of track pairs for which we have\nprior ratings. Across subjects, we obtain a median of 42 ratings (M = 45.4, SD = 29.3). We aggregate controlledcondition ratings across subjects and thus obtain a total of 509 controlled-condition similarity ratings, corresponding to 6.5% coverage of web-based similarity ratings. Table IV displays a confusion matrix of web-sourced versus controlled-condition similarity ratings.\nWe quantify the agreement between controlled-condition and web-sourced similarity ratings. We report results for both five-point and four-point rating scales; for each agreement statistic we report results for the four-point rating scale in brackets. We first quantify agreement using Kendall\u2019s correlation coefficient \u03c4b, as defined in (5). We obtain a correlation of 0.274 (0.250), with p < 0.001 based on a permutation test for the hypothesis of no correlation. We then compute a confidence interval for the obtained sample correlation by applying bootstrap sampling [64]. At the 95% level, we obtain correlations in the range [0.205, 0.337] ([0.173, 0.325]). Subsequently, we consider the correlation 0.337 (0.325) an upper bound on attainable accuracy using our proposed method of similarity rating prediction. As a second measure of rating agreement, we compute Spearman\u2019s correlation coefficient \u03c1s, where we obtain 0.329 (0.278) for ratings aggregated across subjects. Analogously by applying bootstrap sampling, at the 95% level we obtain correlations in the range [0.247, 0.404] ([0.193, 0.361]). We consider the correlation 0.404 (0.361) an upper bound on attainable accuracy based on \u03c1s. Finally, using Table IV and interpreting the controlled-condition rating process as a multinomial classification task, we obtain a balanced classification accuracy (BA) of 0.292 (0.345); the corresponding 95% confidence interval is [0.254, 0.336] ([0.304, 0.393]).\n1) Distance measures: We predict similarity ratings by applying multinomial regression to pairwise Euclidean distances between descriptor vectors, using the approach described in Section III-A. As an additional baseline distance measure, using (1) and assuming Gaussianity and diagonal covariance, we compute the KLD on pairs of FMDs. We logarithmically transform distances obtained using the KLD, which we observed improved prediction accuracy.\nAs a baseline distance accounting for temporal structure, we compute the cross-prediction error between audio feature sequences, with each feature sequence represented at the original frame level. Following [65], we apply state space embedding [66] separately to pairs of feature sequences. Given feature vectors (v1, . . . ,vT ) each with dimensionality h, state space embedding produces higher-dimensional feature\nvectors with dimensionality dh by stacking d consecutive vectors vt\u2212d, . . .vt\u22121 at each time step t. We perform crosspredictions by determining sequential successors of nearest neighbours in the embedded space, using the approach given in [67]. As a distance measure between predicted and observed feature sequences, we compute the normalised mean square error [65]. We consider parameter d \u2208 {8, 12, 16, 20} and report results for d = 12, which yields highest average correlation between computed pairwise distances and similarity annotations. We apply square-root transformation to pairwise distances, which we observed improved similarity rating prediction accuracy.\n2) Performance statistics: To quantify the accuracy of similarity rating prediction, as discussed in [68] we compute Kendall\u2019s \u03c4b and Spearman\u2019s \u03c1s, both which are ordinal measures of association between predicted and annotated similarity ratings. We define Kendall\u2019s \u03c4b as follows. Assume that we have sequences Q = (q1, . . . , qM ), O = (o1, . . . , oM ). The pair di,j = ((qi, oi), (qj , oj)) is termed concordant, if qi > qj and oi > oj , or if qi < qj and oi < oj . Analogously, di,j is termed discordant, if qi < qj and oi > oj , or if qi > qj and oi < oj . Kendall\u2019s \u03c4b is defined as\n\u03c4b = Mc \u2212Md\u221a\n(Mp \u2212Mq)(Mp \u2212Mo) (5)\nwhere Mc, Md respectively denote the number of concordant and discordant pairs and where Mp = 12M(M\u22121) denotes the total number of pairs. Terms Mq , Mo respectively denote the number of pairs with tied (qi, qj) and with tied (oi, oj). In the denominator, the normalisation is with respect to the geometric mean of adjusted pair counts (Mp \u2212 Mq), (Mp \u2212 Mo). Yielding values in the range [\u22121, 1], \u03c4b may be interpreted as an estimate of the difference in probability of sampling a concordant pair versus sampling a discordant pair in (Q,O), while accounting for ties.\nAs a second measure of prediction accuracy, we compute Spearman\u2019s \u03c1s, corresponding to the product-moment correlation coefficient between separately ranked Q, O. We assign unique ranks to tied values, before computing average ranks across tied values. Note that in contrast to \u03c4b, the value of \u03c1s is a function of assigned ranks. Thus, in the presence of ties \u03c4b may be viewed as a more appropriate means of comparing ordinal sequences [69]. Nevertheless, we compute \u03c1s, since its square yields a direct interpretation as proportion of explained variance between assigned ranks.\nAs a third performance measure, we view our prediction task as multinomial classification and compute BA. Note that in contrast to \u03c4b, \u03c1s, BA disregards the ordering of rating scores. Based on the notion of rating agreement given in Section IV-A, we thus consider BA a subsidiary measure of performance compared to \u03c4b, \u03c1s.\n3) Model estimation: We evaluate similarity rating prediction by applying hold-out validation to web-sourced annotations. We use 60% of annotations for training, with the remainder of annotations used for testing.\nWe apply multinomial regression separately to sets of distances between descriptor vectors, as specified in Table V. We standardise distances by subtracting the mean and dividing\nby the variance of the training data. Note that we compute FCD vectors separately across temporal resolutions and across audio features. Based on a set of 25 audio features, given a pair of tracks we thus obtain a total of 100 distances between compression-based descriptor vectors. Furthermore, note that when combining sets of descriptors we aggregate among obtained distances. Thus given a pair of tracks, when combining sets 1, 3, 4 as specified in Table V, we obtain 150 distances. As given in (3), we weight distances individually.\nIn our training step, we estimate multinomial regression parameters using elastic net regularisation (ENR) [70] based on coordinate descent4 [71]. We denote with \u03b2 = (\u03b2T1 , . . . ,\u03b2 T K)\nT , \u03b3 = (\u03b31, . . . , \u03b3K)T regression coefficients and model intercepts as given in (3). Using ENR, we solve\nmin \u03b2,\u03b3\n{ \u03b7 ( \u03bd\u2016\u03b2\u20161 + (1\u2212 \u03bd) 1\n2 \u2016\u03b2\u201622\n) \u2212 `(\u03b2,\u03b3) } (6)\nwhere `(\u03b2,\u03b3) denotes model log-likelihood. Furthermore, \u03b7 and \u03bd respectively are shrinkage and elastic net penalty parameters, with \u03b7 > 0 and 0 \u2264 \u03bd \u2264 1. Thus, \u03bd determines the relative contribution of regularisation due to L1 and L2 norms, whereas \u03b7 scales the regularisation penalty. For each performance statistic given in Section IV-A2 and for each rating scale as given in IV-A, we apply hold-out validation to training data and optimise \u03b7 by determining maximal prediction accuracy. We consider \u03bd a hyper-parameter which we assign constant value; we optimise Kendall\u2019s \u03c4b with respect to the five-point rating scale and using a model incorporating FCDs and FMDs, where we again apply hold-out validation to training data.\n4http://www.stanford.edu/\u223chastie/glmnet matlab/\n4) Results: We examine the correlation between descriptor distances and five-point scale similarity ratings across individual audio features. Fig. 2 depicts correlations \u03c4b for FCDs and FMDs, where we compare FMDs using both Euclidean distance and KLD. In addition to FMDs, as described in Section IV-A1 we consider as a baseline the cross-prediction error.\nWe observe that FCDs and FMDs both yield maximum correlation 0.19 (comparing FCD2 to FMDs, with both distances computed using Euclidean distance); similarly, FMDs compared using KLD yield maximum correlation 0.18. Across descriptors, with \u03b1 = 0.05 and applying Bonferroni correction, the majority of features yield significant correlations. In contrast, for cross-prediction, effect sizes are comparatively small. Comparing descriptors, for FCD2 we observe correlations exceeding 0.1 for 9 features, and for 12 features for the case of FMDs compared either using KLD or Euclidean distance. On average, FMDs yield greater correlation compared to FCD1 (0.095 versus 0.087). However, for specific features FCDs yield higher correlation than FMDs. Comparing FCDs amongst temporal resolutions, we observe a monotonically decreasing relationship between downsampling factor and average correlation.\nFig. 3 displays a comparison of similarity rating prediction accuracy, where for each descriptor set in Table VI we apply feature selection as described in Section IV-A3. We estimate models using \u03c4b, \u03c1s, BA as performance statistics. We consider both 5-point and 4-point rating scales. In particular, we consider the performance gain obtained by including FCDs in our models.\nAcross both rating scales, we observe that FCDs are outperformed by FMDs compared using KLD alone, or using Euclidean distance and KLD in combination. However, a combination of FCDs and FMDs outperforms evaluated combinations employing FMDs alone. By incorporating compression descriptors, compared to FMDs based on aggregated KLD and Euclidean distance, based on the five-point rating scale we obtain absolute performance gains of 0.033, 0.030, 0.013 with respect to \u03c1s, \u03c4b, BA. The respective relative performance gains are 10.4%, 11.3%, 4.7%. Based on the four-point rating scale, we obtain absolute performance gains of 0.059, 0.051, 0.021; the respective relative performance gains are 31.1%, 29.1%, 7.2%. For the model using \u03c1s and the four-point rating scale, Table VII displays confusion matrices of predicted versus annotated ratings. We test for differences between correlations by applying bootstrap sampling to predicted and observed similarity ratings, from which in turn we estimate standard errors of performance statistics. Based on a one-way analysis of variance with Tukey-Kramer post-hoc analysis and setting \u03b1 = 0.05, we reject the hypothesis of no difference between correlations across all considered pairs, for all considered performance statistics.\nFig. 4 displays regression coefficients across features and descriptor classes, where we consider the best-performing model evaluated in Fig. 3 based on \u03c1s and using the fivepoint rating scale. We sum regression coefficient magnitudes across each of the K binary classifiers given in (3), before normalising the obtained values to sum to one. Comparing\nFMDs and FCDs, we observe that both FCDs and FMDs are selected within individual features. FCDs appear to be selected across diverse temporal resolutions, with emphasis on higher temporal resolutions. We observe that multiple FCD resolutions are selected within the same feature."}, {"heading": "B. Song year prediction", "text": "For song year prediction, we compute FCDs and FMDs as performed for similarity rating prediction. We use chart entry dates as our annotation data and apply the linear regression model given in (4). Fig. 5 displays a histogram of chart entry dates.\n1) Model estimation: To evaluate our descriptors for song year prediction, we partition the dataset into random training and testing subsets, where we ensure that title or artist strings are not duplicated across subsets. We apply the aforementioned filtering procedure to control for potential cover version and album effects, in addition to any analogous effects at the level of artists [72]. The resulting training and testing datasets consist of 10 728 and 4 745 tracks respectively. We deem as outliers descriptor values in the training data exceeding 10 standard\ndeviations beyond the 99th percentile. We replace such outliers with imputed values, using the K-nearest neighbour algorithm.\nWe apply linear regression separately to sets of descriptor vectors, as specified in Table VI. We standardise descriptors by subtracting the mean and dividing by the variance of the training data. As performed for similarity rating prediction, we compute FCDs separately across temporal resolutions and across audio features. In contrast, we apply linear regression directly to descriptor vectors without the intermediate step of computing distances. Based on a set of 25 audio features, given a single track we obtain a total of 300 scalar-valued FCDs, for each of which we estimate a single regression coefficient. Note that since we represent FMDs using the mean and standard deviation, we estimate two regression coefficients for each univariate audio feature. For FMDs, it follows that we estimate 24 regression coefficients for MFCCs and chroma features.\nAs was performed for similarity rating prediction, we estimate linear regression parameters using ENR. We denote with \u03b8 = (\u03b8T1 , . . . ,\u03b8 T N )\nT , \u03b1 regression coefficients and the model intercept as given in (4). Using ENR, we solve\nmin \u03b8,\u03b1\n{ \u03b7 ( \u03bd\u2016\u03b8\u20161 + (1\u2212 \u03bd) 1\n2 \u2016\u03b8\u201622\n) + SSR(\u03b8, \u03b1) } (7)\nwhere SSR(\u03b8, \u03b1) denotes the sum of squared residuals. Both \u03b7, \u03bd behave as defined in (6). We apply cross-validation to training data and optimise \u03b7 by determining minimal prediction mean square error. We again consider \u03bd a hyper-parameter which we assign constant value; we optimise prediction mean square error based on a model incorporating FCDs and FMDs, and by applying cross-validation to training data. We threshold predictions to fall in the range [1957y .. 2010y].\nIn addition to the year prediction task based on individual tracks, we evaluate prediction performance when considering groups of tracks. We perform this experiment to establish whether FCDs consistently improve performance when combined with grouped FMDs, or if grouped FMDs amortise any potential performance gain due to FCDs. We select groups of tracks by applying a non-overlapping sliding window to chart entry dates. We then take as descriptor vector r\u2032w,n the average\nr\u2032w,n = 1 |Cw| \u2211 i\u2208Cw ri,n (8)\nwhere Cw denotes the set of tracks at window position w. We apply the windowing procedure separately to training and testing data sets. Note that by windowing tracks, at each window position we assume prior knowledge of differences among\nchart entry times in training and testing data, respectively. For a given window size, we average descriptor vectors in the training data and proceed as described in Section IV-B1. Given the obtained regression model and given averaged descriptor vectors at window position z in the testing data, we seek to predict the associated window centre y\u2032z .\n2) Performance statistics: We quantify prediction accuracy with respect to annotated chart entry dates, using the mean absolute error (MAE) and root mean square error (RMSE) statistics.\n3) Results: Fig. 6 displays the result of exploratory analysis for song year prediction, where for FMDs and FCDs we group descriptor values across time, by applying a non-overlapping 2-year sliding window to chart entry dates. We restrict analysis to obtained spectral spread features [47]. The resulting yearwise box plots suggest that the examined descriptors are non-stationary with respect to chart entry dates, exhibiting distinct trends. To examine the behaviour of descriptors at a finer time scale, we apply a non-overlapping 30-day sliding window to chart entry dates, where at each window position we compute the mean descriptor value. Examining the sample autocorrelation of the resulting time series for lags in the range [1 .. 15], we observe weaker correlations for FCDs compared to FMDs. Yet, both autocorrelations exhibit slowly decaying autocorrelations (Fig. 7), characteristic of a nonstationary time series [73]. Following the method of Box and Jenkins [74], we attempt to attain stationarity by applying firstorder differencing to the time series. However, we observe autocorrelation close to \u22120.5 at unit lag, suggesting that the time series have been overdifferenced [73]. We interpret these observations as evidence for a non-trivial, trend-exhibiting process governing observed descriptor values [75].\nTable VIII summarises the accuracy of song year prediction using MAE and RMSE statistics. Quantified using either MAE or RMSE, song year prediction based on FMDs outperforms prediction using FCDs alone. However, we observe that a combination of FMDs and FCDs yields the highest prediction accuracy. By incorporating FCDs we observe performance gains of 10.9%, 9.8% relative to FMDs, in terms of MAE and RMSE. As performed in Section IV-A4, we test for differences among prediction accuracies by applying bootstrap sampling to predicted and observed chart entry times, from which we estimate standard errors of MAE and RMSE statistics. Again using one-way analysis of variance with Tukey-Kramer posthoc analysis and setting \u03b1 = 0.05, we reject the hypothesis of no difference between prediction accuracies across all pairs, for both MAE and RMSE.\nFig. 8 displays regression coefficients obtained using unwindowed chart entry dates. We compute coefficient magnitudes and normalise to sum to one. Thus computed, we interpret coefficient magnitudes as predictive utilities across individual audio features. In addition, we consider the utility of FCDs across time scales, compared to FMDs. Summed across features, we observe that compared to FCD1, FMDs are weighted more strongly (0.591 versus 0.201). Further examining relative weightings, we observe a prevalence of weight assigned to FCD1 compared to higher downsampling factors. However, we observe that individual features may be weighted relatively strongly across multiple temporal scales. Note from Table V that for chroma features, MFCCs and derivatives, FMD weights are summed across 24 prediction coefficients, compared to 3 coefficients for FCDs.\nIn Fig. 9 we examine prediction accuracy in response to windowed descriptors, as described in Section IV-A3 and quantified using MAE. For increasing window size up to 60d, performance improves monotonically across all considered descriptor sets. Across considered window sizes, using combined FCDs and FMDs we observe a mean performance gain of 17.5%, relative to using FMDs alone."}, {"heading": "V. CONCLUSIONS", "text": "We have considered the problem of determining musical similarity, using feature sequences extracted from musical audio. In particular, we have considered musical similarity in the context of two low-specificity content retrieval tasks, namely similarity rating prediction and song year prediction. To this end, we have evaluated the utility of sequential complexity as a descriptor for quantifying musical similarity.\nFor both considered tasks, we observe that sequential complexity descriptors predict the outcome variable. Furthermore, in combination with feature moment descriptors, sequential complexity descriptors improve prediction accuracy with respect to the baseline. The results confirm that our proposed descriptors capture musically relevant information and that temporal structure is relevant in our chosen domain. Consequently, our results show that sequential complexity may\nbe used to improve the accuracy of low-specificity content retrieval based on bag-of features approaches.\nOur proposed descriptors are computed in an unsupervised manner and may be implemented efficiently, requiring O(n) time complexity for each track [76]. In addition, our proposed descriptors have similar dimensionality compared to feature moment descriptors. Since our descriptors may be computed off-line or incrementally and thereafter combined with indexing methods as proposed in [39]\u2013[41], we deem them potentially applicable in large-scale content retrieval systems.\nSimilar to results obtained in [24], [35], [36], [77], our results using sequential complexity descriptors suggest that an approach based on multiple temporal resolutions is advantageous for determining musical similarity. As an alternative to downsampled features, we initially employed beat-\nsynchronous representations, which yielded comparatively small gains in prediction accuracy, when combined with original frame-based features. This result suggests that for our chosen domain, temporal structure at short time scales is more advantageous, compared to temporal structure at the metrical level. One possible explanation for this behaviour is that an abundance of observations is beneficial when estimating compression rates. Alternatively, for our chosen tasks similarity judgements might predominantly be based on shortterm timbral characteristics, rather than long-term structures such as motifs and chord progressions. For future work, we aim to examine in closer detail the utility of representing features at multiple time scales, and to characterise the feature spaces relevant for similarity judgements.\nFor similarity rating prediction, note that by biasing towards tracks with proximate chart entry dates, we attempt to control for historical changes in audio production. For song year prediction, where we do not control in the described manner, audio production may confound the association between musical similarity and chart entry date. We acknowledge that in both cases, audio production may confound the association between similarity measures and respective outcome variables, as observed in [60]. For future work, we aim to measure the degree of confounding by introducing suitable audio degradations [78]. A further issue concerns the practical impact of predicted similarity in music information retrieval. We aim to evaluate our descriptors for search, navigation and recommendation tasks, using collections of various scales.\nFinally, the present work considers only a single sequential complexity measure, estimated using a single algorithm. It is conceivable that using multiple compression algorithms may reduce the error variance of estimated sequential complexity. Using alternative classification tasks, we aim to evaluate whether multiple compressors yield an improvement in prediction accuracy."}, {"heading": "VI. ACKNOWLEDGEMENTS", "text": "This work benefited from advice and comments from Andrew J. R. Simpson, Dan Stowell, Anssi Klapuri, Mark D. Plumbley, and Armand Leroi."}], "references": [{"title": "Content-based music information retrieval: Current directions and future challenges", "author": ["M. Casey", "R. Veltkamp", "M. Goto", "M. Leman", "C. Rhodes", "M. Slaney"], "venue": "Proc. IEEE, vol. 96, no. 4, pp. 668\u2013696, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "A survey of audio-based music classification and annotation", "author": ["Z. Fu", "G. Lu", "K. Ting", "D. Zhang"], "venue": "IEEE Trans. Multimedia, vol. 13, no. 2, pp. 303\u2013319, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Music recommendation and discovery in the long tail", "author": ["O. Celma"], "venue": "Ph.D. dissertation, Universitat Pompeu Fabra, Barcelona, Spain, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "A review of audio fingerprinting", "author": ["P. Cano", "E. Batlle", "T. Kalker", "J. Haitsma"], "venue": "Journal of VLSI Signal Processing Systems for Signal, Image and Video Technology, vol. 41, no. 3, pp. 271\u2013284, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Identification of versions of the same musical composition by processing audio descriptions", "author": ["J. Serr\u00e0"], "venue": "Ph.D. dissertation, Universitat Pompeu Fabra, Barcelona, Spain, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic genre classification of music content: a survey", "author": ["N. Scaringella", "G. Zoia", "D. Mlynek"], "venue": "IEEE Signal Processing Magazine, vol. 23, no. 2, pp. 133\u2013141, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Music emotion recognition: A state of the art review", "author": ["Y. Kim", "E. Schmidt", "R. Migneco", "B. Morton", "P. Richardson", "J. Scott", "J. Speck", "D. Turnbull"], "venue": "Proc. 11th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2010, pp. 255\u2013266.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "The bag-of-frames approach to audio pattern recognition: A sufficient model for urban soundscapes but not for polyphonic music", "author": ["J. Aucouturier", "B. Defreville", "F. Pachet"], "venue": "The Journal of the Acoustical Society of America, vol. 122, p. 881, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Music-evoked nostalgia: affect, memory, and personality", "author": ["F. Barrett", "K. Grimm", "R. Robins", "T. Wildschut", "C. Sedikides", "P. Janata"], "venue": "Emotion, vol. 10, no. 3, p. 390, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "The million song dataset", "author": ["T. Bertin-Mahieux", "D. Ellis", "B. Whitman", "P. Lamere"], "venue": "Proc. 12th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2011, pp. 591\u2013596.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Musical genre classification of audio signals", "author": ["G. Tzanetakis", "P. Cook"], "venue": "IEEE Trans. Speech and Audio Processing, vol. 10, no. 5, pp. 293\u2013302, 2002.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Toward intelligent music information retrieval", "author": ["T. Li", "M. Ogihara"], "venue": "IEEE Trans. Multimedia, vol. 8, no. 3, pp. 564\u2013574, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Incorporating machine-learning into music similarity estimation", "author": ["K. West", "S. Cox", "P. Lamere"], "venue": "Proc. 1st ACM workshop on Audio and Music Computing Multimedia, 2006, pp. 89\u201396.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning a metric for music similarity", "author": ["M. Slaney", "K. Weinberger", "W. White"], "venue": "Proc. 9th Intern. Conf. Music Information Retrieval (ISMIR), 2008, pp. 313\u2013318.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "A music similarity function based on signal analysis", "author": ["B. Logan", "A. Salomon"], "venue": "Proc. Intern. Conf. Multimedia and Expo. (ICME), 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Music similarity measures: What\u2019s the use?", "author": ["J. Aucouturier", "F. Pachet"], "venue": "in Proc. 3rd Intern. Conf. Music Information Retrieval (ISMIR),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "A large-scale evaluation of acoustic and subjective music-similarity measures", "author": ["A. Berenzweig", "B. Logan", "D. Ellis", "B. Whitman"], "venue": "Computer Music Journal, vol. 28, no. 2, pp. 63\u201376, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Song-level features and support vector machines for music classification", "author": ["M. Mandel", "D. Ellis"], "venue": "Proc. 6th Intern. Conf. Music Information Retrieval (ISMIR), 2005, pp. 594\u2013599.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "A music retrieval system based on user driven similarity and its evaluation", "author": ["F. Vignoli", "S. Pauws"], "venue": "Proc. 6th Intern. Conf. Music Information Retrieval (ISMIR), 2005, pp. 272\u2013279.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Lightweight measures for timbral similarity of musical audio", "author": ["M. Levy", "M. Sandler"], "venue": "Proc. 1st ACM workshop on Audio and music computing multimedia, 2006, pp. 27\u201336.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Perceptual scaling of synthesized musical timbres: Common dimensions, specificities, and latent subject classes", "author": ["S. McAdams", "S. Winsberg", "S. Donnadieu", "G. De Soete", "J. Krimphoff"], "venue": "Psychological Research, vol. 58, no. 3, pp. 177\u2013192, 1995.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Fusing block-level features for music similarity estimation", "author": ["K. Seyerlehner", "G. Widmer", "T. Pohle"], "venue": "Proc. 13th Int. Conf. on Digital Audio Effects (DAFx-10), 2010, pp. 225\u2013232.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Modeling timbre distance with temporal statistics from polyphonic music", "author": ["F. M\u00f6rchen", "A. Ultsch", "M. Thies", "I. Lohken"], "venue": "IEEE Trans. Audio, Speech and Language Processing, vol. 14, no. 1, pp. 81\u2013 90, 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Temporal pooling and multiscale learning for automatic annotation and ranking of music audio", "author": ["P. Hamel", "S. Lemieux", "Y. Bengio", "D. Eck"], "venue": "Proc. 12th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2011, pp. 729\u2013734.  12  IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL. 22, NO. 12, DECEMBER 2014", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised learning of local features for music classification", "author": ["J. W\u00fclfing", "M. Riedmiller"], "venue": "Proc. 13th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2012, pp. 139\u2013144.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Computational models of music similarity and their application in music information retrieval", "author": ["E. Pampalk"], "venue": "Ph.D. dissertation, Vienna University of Technology, Vienna, Austria, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic music genre classification based on modulation spectral analysis of spectral and cepstral features", "author": ["C. Lee", "J. Shih", "K. Yu", "H. Lin"], "venue": "IEEE Trans. Multimedia, vol. 11, no. 4, pp. 670\u2013682, 2009.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Temporal feature integration for music genre classification", "author": ["A. Meng", "P. Ahrendt", "J. Larsen", "L. Hansen"], "venue": "IEEE Trans. Audio, Speech and Language Processing, vol. 15, no. 5, pp. 1654\u20131664, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Multivariate autoregressive mixture models for music auto-tagging", "author": ["E. Coviello", "Y. Vaizman", "A. Chan", "G. Lanckriet"], "venue": "Proc. 13th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2012, pp. 547\u2013552.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Genre classification via an LZ78-based string kernel", "author": ["M. Li", "R. Sleep"], "venue": "Proc. 6th Intern. Conf. Music Information Retrieval (ISMIR), 2005, pp. 252\u2013259.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Compression of individual sequences via variable-rate coding", "author": ["J. Ziv", "A. Lempel"], "venue": "IEEE Trans. Information Theory, vol. 24, no. 5, pp. 530\u2013536, 1978.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1978}, {"title": "On the importance of modeling temporal information in music tag annotation", "author": ["J. Reed", "C. Lee"], "venue": "Proc. IEEE Intern. Conf. Acoustics, Speech and Signal Process. (ICASSP). IEEE, 2009, pp. 1873\u20131876.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "A music classification method based on timbral features", "author": ["T. Langlois", "G. Marques"], "venue": "Proc. 10th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2009, pp. 81\u201386.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Discovering time-constrained sequential patterns for music genre classification", "author": ["J. Ren", "J. Jang"], "venue": "IEEE Trans. Audio, Speech and Language Processing, vol. 20, no. 4, pp. 1134\u20131144, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-scale temporal fusion by boosting for music classification", "author": ["R. Foucard", "S. Essid", "M. Lagrange", "G. Richard"], "venue": "Proc. 12th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2011, pp. 663\u2013668.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiscale approaches to music audio feature learning", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "Proc. 14th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2013, pp. 3\u20138.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Structural change on multiple time scales as a correlate of musical complexity", "author": ["M. Mauch", "M. Levy"], "venue": "Proc. 12th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2011, pp. 489\u2013494.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Audio-based music classification with a pretrained convolutional network", "author": ["S. Dieleman", "P. Brakel", "B. Schrauwen"], "venue": "Proc. 12th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2011, pp. 669\u2013674.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Locality-sensitive hashing for finding nearest neighbors", "author": ["M. Slaney", "M. Casey"], "venue": "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 128\u2013 131, 2008.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Investigating music collections at different scales with AudioDB", "author": ["C. Rhodes", "T. Crawford", "M. Casey", "M. d\u2019Inverno"], "venue": "Journal of New Music Research, vol. 39, no. 4, pp. 337\u2013348, 2010.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning binary codes for efficient large-scale music similarity search", "author": ["J. Schl\u00fcter"], "venue": "Proc. 14th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2013, pp. 581\u2013586.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Algorithmic clustering of music based on string compression", "author": ["R. Cilibrasi", "P.M.B. Vit\u00e1nyi", "R. Wolf"], "venue": "Computer Music Journal, vol. 28, no. 4, pp. 49\u201367, 2004.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2004}, {"title": "Unified view of prediction and repetition structure in audio signals with application to interest point detection", "author": ["S. Dubnov"], "venue": "IEEE Trans. Audio, Speech and Language Process., vol. 16, no. 2, pp. 327\u2013337, 2008.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Information dynamics: Patterns of expectation and surprise in the perception of music", "author": ["S. Abdallah", "M. Plumbley"], "venue": "Connection Science, vol. 21, no. 2-3, pp. 89\u2013117, 2009.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Anatomy of a bit: Information in a time series observation", "author": ["R. James", "C. Ellison", "J. Crutchfield"], "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science, vol. 21, no. 3, p. 037109, 2011.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic characterization of music complexity: a multifaceted approach", "author": ["S. Streich"], "venue": "Ph.D. dissertation, Universitat Pompeu Fabra, Barcelona, Spain, 2006.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2006}, {"title": "A Matlab toolbox for musical feature extraction from audio", "author": ["O. Lartillot", "P. Toiviainen"], "venue": "Proc. Intern. Conf. Digital Audio Effects (DAFx), 2007, pp. 237\u2013244.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "Identifying \u2018cover songs\u2019 with chroma features and dynamic programming beat tracking", "author": ["D.P.W. Ellis", "G. Poliner"], "venue": "Proc. IEEE Intern. Conf. Acoustics, Speech and Signal Process. (ICASSP), 2007, pp. 1429\u20131432.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "Tonal consonance and critical bandwidth", "author": ["R. Plomp", "W. Levelt"], "venue": "Journal of the Acoustical Society of America, vol. 38, p. 548, 1965.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1965}, {"title": "Timbre models of musical sounds", "author": ["K. Jensen"], "venue": "Ph.D. dissertation, University of Copenhagen, Denmark, 1999.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1999}, {"title": "Auditory toolbox version 2", "author": ["M. Slaney"], "venue": "Interval Research Corporation, Tech. Rep., 1998.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1998}, {"title": "Computer modelling of sound for transformation and synthesis of musical signals", "author": ["P. Masri"], "venue": "Ph.D. dissertation, University of Bristol, United Kingdom, 1996.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1996}, {"title": "Tonal description of music audio signals", "author": ["E. G\u00f3mez"], "venue": "Ph.D. dissertation, Universitat Pompeu Fabra, Barcelona, Spain, 2006.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "Detecting harmonic change in musical audio", "author": ["C. Harte", "M. Sandler", "M. Gasser"], "venue": "Proc. 1st ACM workshop on Audio and music computing multimedia. ACM, 2006, pp. 21\u201326.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2006}, {"title": "On prediction using variable order Markov models", "author": ["R. Begleiter", "R. El-Yaniv", "G. Yona"], "venue": "Journal of Artificial Intelligence Research, vol. 22, pp. 385\u2013421, 2004.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2004}, {"title": "Cognitive complexity and the structure of musical patterns", "author": ["J. Pressing"], "venue": "Proc. 4th Conf. Australasian Cognitive Science Society, 1999.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1999}, {"title": "On the non-existence of music: Why music theory is a figment of the imagination", "author": ["G.A. Wiggins", "D. M\u00fcllensiefen", "M. Pearce"], "venue": "Musicae Scientiae, vol. 14, no. 1, pp. 231\u2013255, 2010.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Seven strictures on similarity", "author": ["N. Goodman"], "venue": "Problems and Projects. Indianapolis: Bobbs-Merrill, 1972.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1972}, {"title": "Features of similarity", "author": ["A. Tversky"], "venue": "Psychological review, vol. 84, no. 4, p. 327, 1977.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1977}, {"title": "Two systems for automatic music genre recognition: What are they really recognizing?", "author": ["B. Sturm"], "venue": "Proc. 2nd ACM Intern. workshop on Music information retrieval with user-centered and multimodal strategies,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2012}, {"title": "Unifying low-level and high-level music similarity measures", "author": ["D. Bogdanov", "J. Serr\u00e0", "N. Wack", "P. Herrera", "X. Serra"], "venue": "IEEE Trans. Multimedia, vol. 13, no. 4, pp. 687\u2013701, 2011.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "The music information retrieval evaluation exchange: Some observations and insights", "author": ["J. Downie", "A.F. Ehmann", "M. Bay", "M.C. Jones"], "venue": "Advances in music information retrieval. Springer, 2010, pp. 93\u2013115.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "A criterion-related validity test of selected indicators of musical sophistication using expert ratings", "author": ["J. Ollen"], "venue": "Ph.D. dissertation, Ohio State University, United States of America, 2006.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2006}, {"title": "The jackknife, the bootstrap and other resampling plans", "author": ["B. Efron"], "venue": "SIAM, 1982,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 1982}, {"title": "Predictability of music descriptor time series and its application to cover song detection", "author": ["J. Serr\u00e0", "H. Kantz", "X. Serra", "R. Andrzejak"], "venue": "IEEE Trans. Audio, Speech and Language Process., vol. 20, no. 2, pp. 514\u2013 525, 2012.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}, {"title": "Detecting strange attractors in turbulence", "author": ["F. Takens"], "venue": "Dynamical Systems and Turbulence, pp. 366\u2013381, 1981.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1981}, {"title": "Identification of cover songs using information theoretic measures of similarity", "author": ["P. Foster", "S. Dixon", "A. Klapuri"], "venue": "Proc. IEEE Intern. Conf. Acoustics, Speech and Signal Process. (ICASSP), 2013.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2013}, {"title": "Measuring the performance of ordinal classification", "author": ["J. Cardoso", "R. Sousa"], "venue": "Intern. Journal of Pattern Recognition and Artificial Intelligence, vol. 25, no. 08, pp. 1173\u20131195, 2011.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2011}, {"title": "The unimodal model for the classification of ordinal data", "author": ["J. Pinto da Costa", "H. Alonso", "J. Cardoso"], "venue": "Neural Networks, vol. 21, no. 1, pp. 78\u201391, 2008.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2008}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 67, no. 2, pp. 301\u2013320, 2005.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2005}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of statistical software, vol. 33, no. 1, p. 1, 2010.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2010}, {"title": "Effects of album and artist filters in audio similarity computed for very large music databases", "author": ["A. Flexer", "D. Schnitzer"], "venue": "Computer Music Journal, vol. 34, no. 3, pp. 20\u201328, 2010.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2010}, {"title": "Introduction to modern time series analysis", "author": ["G. Kirchgassner", "J. Wolters", "U. Hassler"], "venue": null, "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2012}, {"title": "Reinsel, Time series analysis: forecasting and control", "author": ["G. Box", "G. Jenkins"], "venue": null, "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2013}, {"title": "An introduction to long-memory time series models and fractional differencing", "author": ["C.W. Granger", "R. Joyeux"], "venue": "Journal of Time Series Analysis, vol. 1, no. 1, pp. 15\u201329, 1980.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 1980}, {"title": "PPM performance with BWT complexity: A new method for lossless data compression", "author": ["M. Effros"], "venue": "Proc. Data Compression Conf., 2000, pp. 203\u2013212.  FOSTER et al.: SEQUENTIAL COMPLEXITY AS A DESCRIPTOR FOR MUSICAL SIMILARITY  13", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2000}, {"title": "Building musically-relevant audio features through multiple timescale representations", "author": ["P. Hamel", "Y. Bengio", "D. Eck"], "venue": "Proc. 13th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2012, pp. 553\u2013558.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2012}, {"title": "The audio degradation toolbox and its application to robustness evaluation", "author": ["M. Mauch", "S. Ewert"], "venue": "Proc. 14th Intern. Society for Music Information Retrieval Conf. (ISMIR), 2013, pp. 83\u201388.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "We are concerned with the task of quantifying musical similarity, which has received considerable interest in the field of audio-based music content analysis [1], [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "We are concerned with the task of quantifying musical similarity, which has received considerable interest in the field of audio-based music content analysis [1], [2].", "startOffset": 163, "endOffset": 166}, {"referenceID": 2, "context": "Music content analysis has found application in such information retrieval systems as an alternative to manual annotation processes, when the latter are infeasible, unavailable or amenable to be supplemented [3].", "startOffset": 208, "endOffset": 211}, {"referenceID": 3, "context": "We may distinguish between music content analysis applications such as audio fingerprinting [4], version identification [5], genre classification [6] and mood identification [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": "We may distinguish between music content analysis applications such as audio fingerprinting [4], version identification [5], genre classification [6] and mood identification [7].", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "We may distinguish between music content analysis applications such as audio fingerprinting [4], version identification [5], genre classification [6] and mood identification [7].", "startOffset": 146, "endOffset": 149}, {"referenceID": 6, "context": "We may distinguish between music content analysis applications such as audio fingerprinting [4], version identification [5], genre classification [6] and mood identification [7].", "startOffset": 174, "endOffset": 177}, {"referenceID": 0, "context": "Thus, we may distinguish between music classification tasks according to the degree of specificity associated with the measure of musical similarity [1].", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "Descriptors may be characterised according to how temporal structure is accounted for [2].", "startOffset": 86, "endOffset": 89}, {"referenceID": 7, "context": "We may distinguish between bag-of-features representations [8], which discard information on temporal structure, and sequential representations.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "Whereas song year prediction has received little attention in the literature, the song year is important in determining musical preference [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 9, "context": "Thus, song year prediction might be applied in music recommendation [10].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Based on the second approach, Tzanetakis and Cook [11] compute first and second-order moments on spectral features including MFCCs, to perform genre classification using the k-nearest neighbours (KNN) algorithm and Gaussian mixture models (GMMs) estimated on each target class.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "Li and Ogihara [12] propose to classify Daubechies wavelet histograms using GMMs and KNN for genre and mood classification.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "[13] propose methods for learning similarity functions based on constructing decision trees for genre classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] propose feature transformations based on supervised learning and using onset and loudness features, for the purpose of album and artist classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Based on the approach of determining distances between descriptors, Logan and Salomon [15] propose to estimate GMMs on individual tracks.", "startOffset": 86, "endOffset": 90}, {"referenceID": 15, "context": "Aucouturier and Pachet [16] in contrast compute cross-likelihoods between GMMs using Monte Carlo approximations for the purpose of genre classification, whereas Berenzweig et al.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "[17] consider the asymptotic likelihood approximation of the KLD and centroid distances for the task of similarity rating prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Mandel and Ellis [18] instead represent tracks as single Gaussians and use (1) as a distance measure between track pairs.", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "An alternative approach to computing the KLD is based on computing histograms of quantised features, as proposed by Vignoli and Pauws [19] for playlist recommendation; Levy and Sandler [20] compare approaches in the context of genre classification.", "startOffset": 134, "endOffset": 138}, {"referenceID": 19, "context": "An alternative approach to computing the KLD is based on computing histograms of quantised features, as proposed by Vignoli and Pauws [19] for playlist recommendation; Levy and Sandler [20] compare approaches in the context of genre classification.", "startOffset": 185, "endOffset": 189}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Aucouturier and Pachet [8] argue that the bag-of-features approach is insufficient to model polyphonic music for determining similarity.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Sequential representations based on mid-level features are widely applied for the purpose of version identification [5].", "startOffset": 116, "endOffset": 119}, {"referenceID": 10, "context": "Tzanetakis and Cook [11] propose to estimate the local mean and variance of features contained in a 1s window.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "[22] apply a single, global summarisation step to overlapping windows, computing variance and percentiles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23], Hamel et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24], W\u00fclfing and Riedmiller [25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[24], W\u00fclfing and Riedmiller [25].", "startOffset": 29, "endOffset": 33}, {"referenceID": 25, "context": "Pampalk [26] proposes fluctuation patterns describing loudness modulation across frequency bands, whereas Lee et al.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "[27] propose statistics based on modulation spectral analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] consider a variety of statistics based on spectral analysis and autocorrelation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28], Coviello et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] apply multivariate autoregressive modelling to windowed features, for the tasks of genre and tag classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "For genre classification, Li and Sleep [30] propose an SVM kernel in which pairwise distances are obtained by comparing dictionaries generated using the Lempel-Ziv compression algorithm [31].", "startOffset": 39, "endOffset": 43}, {"referenceID": 30, "context": "For genre classification, Li and Sleep [30] propose an SVM kernel in which pairwise distances are obtained by comparing dictionaries generated using the Lempel-Ziv compression algorithm [31].", "startOffset": 186, "endOffset": 190}, {"referenceID": 31, "context": "Reed and Lee [32] apply latent semantic analysis to unigram and bigram counts for classification using SVMs, whereas Langlois and Marques [33] propose to estimate language models for computing sequence cross-likelihoods for genre and artist classification.", "startOffset": 13, "endOffset": 17}, {"referenceID": 32, "context": "Reed and Lee [32] apply latent semantic analysis to unigram and bigram counts for classification using SVMs, whereas Langlois and Marques [33] propose to estimate language models for computing sequence cross-likelihoods for genre and artist classification.", "startOffset": 138, "endOffset": 142}, {"referenceID": 33, "context": "Ren and Jang [34] propose an algorithm for computing histograms of feature codeword sequences for genre classification.", "startOffset": 13, "endOffset": 17}, {"referenceID": 34, "context": "[35] propose an ensemble of classifiers, where each classifier is trained on features at a given time scale.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Dieleman and Schrauwen [36] apply feature learning based on spherical K-means clustering to tag classification.", "startOffset": 23, "endOffset": 27}, {"referenceID": 36, "context": "Although not applied to classification, Mauch and Levy [37] propose a similar smoothing approach for characterising structural change at multiple time scales.", "startOffset": 55, "endOffset": 59}, {"referenceID": 37, "context": "[38] propose deep learning architectures for genre, artist and key classification tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] propose a deep learning architecture incorporating multiple feature aggregation functions for tag classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The approach proposed in this work resembles methods applying statistical models to quantised feature sequences [30], [32]\u2013[34].", "startOffset": 112, "endOffset": 116}, {"referenceID": 31, "context": "The approach proposed in this work resembles methods applying statistical models to quantised feature sequences [30], [32]\u2013[34].", "startOffset": 118, "endOffset": 122}, {"referenceID": 33, "context": "The approach proposed in this work resembles methods applying statistical models to quantised feature sequences [30], [32]\u2013[34].", "startOffset": 123, "endOffset": 127}, {"referenceID": 38, "context": "Since the obtained statistics may be compared using a metric, our approach has the potential to be combined with indexing and hashing schemes for computationally efficient retrieval [39]\u2013 [41], while retaining information on temporal structure.", "startOffset": 182, "endOffset": 186}, {"referenceID": 40, "context": "Since the obtained statistics may be compared using a metric, our approach has the potential to be combined with indexing and hashing schemes for computationally efficient retrieval [39]\u2013 [41], while retaining information on temporal structure.", "startOffset": 188, "endOffset": 192}, {"referenceID": 35, "context": "Schrauwen [36].", "startOffset": 10, "endOffset": 14}, {"referenceID": 41, "context": "[42], who propose pairwise sequence compressibility to quantify similarity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "Note that we may take compression rates as estimates of sequential Shannon entropy rates, inviting further comparison or combination with related measures of sequential complexity [43]\u2013[45].", "startOffset": 180, "endOffset": 184}, {"referenceID": 44, "context": "Note that we may take compression rates as estimates of sequential Shannon entropy rates, inviting further comparison or combination with related measures of sequential complexity [43]\u2013[45].", "startOffset": 185, "endOffset": 189}, {"referenceID": 45, "context": "Similar to the descriptor proposed in [46], as a means of quantifying the sequential complexity of V, we compute the compression rate R\u03bb(V),", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "Following [10] we apply a linear regression model.", "startOffset": 10, "endOffset": 14}, {"referenceID": 46, "context": "For each track excerpt in the dataset, we extract a set of 25 audio features, using MIRToolbox [47] version 1.", "startOffset": 95, "endOffset": 99}, {"referenceID": 47, "context": "2 and using the framewise chromagram representation proposed by Ellis and Poliner [48].", "startOffset": 82, "endOffset": 86}, {"referenceID": 47, "context": "Feature name Description Chroma (Ellis and Poliner) 12-component chromagram based on using phase-derivatives to identify tonal components in spectrum [48].", "startOffset": 150, "endOffset": 154}, {"referenceID": 48, "context": "roughness Average roughness [49] between peak pairs in magnitude spectrum.", "startOffset": 28, "endOffset": 32}, {"referenceID": 49, "context": "irregularity Squared amplitude difference between successive partials [50].", "startOffset": 70, "endOffset": 74}, {"referenceID": 50, "context": "mfcc 12-component MFCCs [51] (excluding energy coefficient).", "startOffset": 24, "endOffset": 28}, {"referenceID": 51, "context": "spectralflux Half-wave rectified L1 distance between magnitude spectrum at successive frames [52].", "startOffset": 93, "endOffset": 97}, {"referenceID": 52, "context": "keyclarity Peak correlation of chromagram with key profiles [53].", "startOffset": 60, "endOffset": 64}, {"referenceID": 53, "context": "hcdf Flux of 6-dimensional tonal centroid [54].", "startOffset": 42, "endOffset": 46}, {"referenceID": 54, "context": "We compress symbol sequences using the prediction by partial match (PPM) algorithm2, described in [55].", "startOffset": 98, "endOffset": 102}, {"referenceID": 30, "context": "We consider PPM a general-purpose string compression algorithm which may be substituted with an alternative compressor; in initial experiments we obtained similar results using Lempel-Ziv compression [31].", "startOffset": 200, "endOffset": 204}, {"referenceID": 54, "context": "Nevertheless, we note that PPM compresses efficiently compared to alternative compression schemes [55].", "startOffset": 98, "endOffset": 102}, {"referenceID": 55, "context": "Note however that we make no claim that FCDs capture any notion of musical complexity as proposed in [56].", "startOffset": 101, "endOffset": 105}, {"referenceID": 56, "context": "For human similarity judgements, two issues prompt consideration: In addition to music being inherently subjective [57], human similarity judgements are context-dependent [58], [59].", "startOffset": 115, "endOffset": 119}, {"referenceID": 57, "context": "For human similarity judgements, two issues prompt consideration: In addition to music being inherently subjective [57], human similarity judgements are context-dependent [58], [59].", "startOffset": 171, "endOffset": 175}, {"referenceID": 58, "context": "For human similarity judgements, two issues prompt consideration: In addition to music being inherently subjective [57], human similarity judgements are context-dependent [58], [59].", "startOffset": 177, "endOffset": 181}, {"referenceID": 59, "context": "We bias as a means of controlling for historical changes in audio production, which might affect similarity ratings [60].", "startOffset": 116, "endOffset": 120}, {"referenceID": 60, "context": "We contend that for music content analysis based on an ensemble of systems as proposed in [61], the entire target set of predicted musical similarity might be used when forming recommendations.", "startOffset": 90, "endOffset": 94}, {"referenceID": 61, "context": "In contrast, for track recommendation relying on predicted similarity alone, when forming recommendations, it is typically of interest to consider tracks deemed similar to a query, while disregarding tracks deemed dissimilar [62].", "startOffset": 225, "endOffset": 229}, {"referenceID": 62, "context": "Subjects were assessed using the Ollen musical sophistication index (OMSI) [63].", "startOffset": 75, "endOffset": 79}, {"referenceID": 63, "context": "We then compute a confidence interval for the obtained sample correlation by applying bootstrap sampling [64].", "startOffset": 105, "endOffset": 109}, {"referenceID": 64, "context": "Following [65], we apply state space embedding [66] separately to pairs of feature sequences.", "startOffset": 10, "endOffset": 14}, {"referenceID": 65, "context": "Following [65], we apply state space embedding [66] separately to pairs of feature sequences.", "startOffset": 47, "endOffset": 51}, {"referenceID": 66, "context": "We perform crosspredictions by determining sequential successors of nearest neighbours in the embedded space, using the approach given in [67].", "startOffset": 138, "endOffset": 142}, {"referenceID": 64, "context": "As a distance measure between predicted and observed feature sequences, we compute the normalised mean square error [65].", "startOffset": 116, "endOffset": 120}, {"referenceID": 67, "context": "2) Performance statistics: To quantify the accuracy of similarity rating prediction, as discussed in [68] we compute Kendall\u2019s \u03c4b and Spearman\u2019s \u03c1s, both which are ordinal measures of association between predicted and annotated similarity ratings.", "startOffset": 101, "endOffset": 105}, {"referenceID": 68, "context": "Thus, in the presence of ties \u03c4b may be viewed as a more appropriate means of comparing ordinal sequences [69].", "startOffset": 106, "endOffset": 110}, {"referenceID": 69, "context": "In our training step, we estimate multinomial regression parameters using elastic net regularisation (ENR) [70] based on coordinate descent4 [71].", "startOffset": 107, "endOffset": 111}, {"referenceID": 70, "context": "In our training step, we estimate multinomial regression parameters using elastic net regularisation (ENR) [70] based on coordinate descent4 [71].", "startOffset": 141, "endOffset": 145}, {"referenceID": 71, "context": "We apply the aforementioned filtering procedure to control for potential cover version and album effects, in addition to any analogous effects at the level of artists [72].", "startOffset": 167, "endOffset": 171}, {"referenceID": 46, "context": "We restrict analysis to obtained spectral spread features [47].", "startOffset": 58, "endOffset": 62}, {"referenceID": 72, "context": "7), characteristic of a nonstationary time series [73].", "startOffset": 50, "endOffset": 54}, {"referenceID": 73, "context": "Following the method of Box and Jenkins [74], we attempt to attain stationarity by applying firstorder differencing to the time series.", "startOffset": 40, "endOffset": 44}, {"referenceID": 72, "context": "5 at unit lag, suggesting that the time series have been overdifferenced [73].", "startOffset": 73, "endOffset": 77}, {"referenceID": 74, "context": "We interpret these observations as evidence for a non-trivial, trend-exhibiting process governing observed descriptor values [75].", "startOffset": 125, "endOffset": 129}, {"referenceID": 75, "context": "Our proposed descriptors are computed in an unsupervised manner and may be implemented efficiently, requiring O(n) time complexity for each track [76].", "startOffset": 146, "endOffset": 150}, {"referenceID": 38, "context": "Since our descriptors may be computed off-line or incrementally and thereafter combined with indexing methods as proposed in [39]\u2013[41], we deem them potentially applicable in large-scale content retrieval systems.", "startOffset": 125, "endOffset": 129}, {"referenceID": 40, "context": "Since our descriptors may be computed off-line or incrementally and thereafter combined with indexing methods as proposed in [39]\u2013[41], we deem them potentially applicable in large-scale content retrieval systems.", "startOffset": 130, "endOffset": 134}, {"referenceID": 23, "context": "Similar to results obtained in [24], [35], [36], [77], our results using sequential complexity descriptors suggest that an approach based on multiple temporal resolutions is advantageous for determining musical similarity.", "startOffset": 31, "endOffset": 35}, {"referenceID": 34, "context": "Similar to results obtained in [24], [35], [36], [77], our results using sequential complexity descriptors suggest that an approach based on multiple temporal resolutions is advantageous for determining musical similarity.", "startOffset": 37, "endOffset": 41}, {"referenceID": 35, "context": "Similar to results obtained in [24], [35], [36], [77], our results using sequential complexity descriptors suggest that an approach based on multiple temporal resolutions is advantageous for determining musical similarity.", "startOffset": 43, "endOffset": 47}, {"referenceID": 76, "context": "Similar to results obtained in [24], [35], [36], [77], our results using sequential complexity descriptors suggest that an approach based on multiple temporal resolutions is advantageous for determining musical similarity.", "startOffset": 49, "endOffset": 53}, {"referenceID": 59, "context": "We acknowledge that in both cases, audio production may confound the association between similarity measures and respective outcome variables, as observed in [60].", "startOffset": 158, "endOffset": 162}, {"referenceID": 77, "context": "For future work, we aim to measure the degree of confounding by introducing suitable audio degradations [78].", "startOffset": 104, "endOffset": 108}], "year": 2014, "abstractText": "We propose string compressibility as a descriptor of temporal structure in audio, for the purpose of determining musical similarity. Our descriptors are based on computing track-wise compression rates of quantised audio features, using multiple temporal resolutions and quantisation granularities. To verify that our descriptors capture musically relevant information, we incorporate our descriptors into similarity rating prediction and song year prediction tasks. We base our evaluation on a dataset of 15 500 track excerpts of Western popular music, for which we obtain 7 800 web-sourced pairwise similarity ratings. To assess the agreement among similarity ratings, we perform an evaluation under controlled conditions, obtaining a rank correlation of 0.33 between intersected sets of ratings. Combined with bag-offeatures descriptors, we obtain performance gains of 31.1% and 10.9% for similarity rating prediction and song year prediction. For both tasks, analysis of selected descriptors reveals that representing features at multiple time scales benefits prediction accuracy.", "creator": "LaTeX with hyperref package"}}}