{"id": "1403.0628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2014", "title": "Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations", "abstract": "we study algorithms defined for online linear optimization in hilbert spaces, some focusing widely on the case where even the player is effectively unconstrained. we develop a novel characterization of a large class of minimax algorithms, recovering, and even improving, several previous results as approximate immediate corollaries. moreover, using our tools, we develop explicitly an algorithm that provides a regret bound of $ \\ d mathcal { an o } \\ big ( u \\ sqrt { t \\ log ( u \\ sqrt { t } \\ log ^ \u00b7 2 t + 1 ) } \\ big ) $ where $ u $ is the $ \u2191 l _ to 2 $ norm decomposition of a comparator, and otherwise both $ t $ and $ u $ are unknown to the player. this bound is optimal up to $ \\ sqrt { \\ log \\ log \u2020 t } $ terms. when $ t $ is known, we derive yourself an algorithm with including an optimal regret outcome bound ( up subjected to constant bias factors ). for both the known and unknown $ t $ case, a normal approximation up to the conditional value of the game proves to be the key analysis tool.", "histories": [["v1", "Mon, 3 Mar 2014 23:06:24 GMT  (23kb)", "https://arxiv.org/abs/1403.0628v1", null], ["v2", "Wed, 21 May 2014 16:17:09 GMT  (23kb)", "http://arxiv.org/abs/1403.0628v2", "Proceedings of the 27th Annual Conference on Learning Theory (COLT 2014)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["h brendan mcmahan", "francesco orabona"], "accepted": false, "id": "1403.0628"}, "pdf": {"name": "1403.0628.pdf", "metadata": {"source": "CRF", "title": "Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations", "authors": ["H. Brendan McMahan", "Francesco Orabona"], "emails": ["mcmahan@google.com", "orabona@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 3.\n06 28\nv2 [\ncs .L\nG ]\nan algorithm that provides a regret bound of O ( U\n\u221a T log(U \u221a T log2 T + 1) ) ,\nwhere U is the L2 norm of an arbitrary comparator and both T and U are unknown to the player. This bound is optimal up to \u221a log log T terms. When T is known, we derive an algorithm with an optimal regret bound (up to constant factors). For both the known and unknown T case, a Normal approximation to the conditional value of the game proves to be the key analysis tool."}, {"heading": "1 Introduction", "text": "The online learning framework provides a scalable and flexible approach for modeling a wide range of prediction problems, including classification, regression, ranking, and portfolio management. Online algorithms work in rounds, where at each round a new instance is given and the algorithm makes a prediction. Then the environment reveals the label of the instance, and the learning algorithm updates its internal hypothesis. The aim of the learner is to minimize the cumulative loss it suffers due to its prediction error.\nResearch in this area has mainly focused on designing new prediction strategies and proving theoretical guarantees for them. However, recently, minimax analysis has been proposed as a general tool to design optimal prediction strategies [Rakhlin et al., 2012, 2013, McMahan and Abernethy, 2013]. The problem is cast as a sequential multi-stage zero-sum game between the player (the learner) and an adversary (the environment), providing the optimal strategies for both. In some cases the value of the game can be calculated exactly in an efficient way [Abernethy et al., 2008a], in others upper bounds on the value of the game (often based on the sequential Rademacher complexity) are used to construct efficient algorithms with theoretical guarantees [Rakhlin et al., 2012].\nWhile most of the work in this area has focused on the setting where the player is constrained to a bounded convex set [Abernethy et al., 2008a] (with the notable exception of McMahan and Abernethy [2013]), in this work we are interested in the general setting of unconstrained online learning with\n\u2217Both authors thank Jacob Abernethy for insightful feedback on this work.\nlinear losses in Hilbert spaces. In Section 4, extending the work of McMahan and Abernethy [2013], we provide novel and general sufficient conditions to be able to compute the exact minimax strategy for both the player and the adversary, as well as the value of the game. In particular, we show that under these conditions the optimal play of the adversary is always orthogonal or always parallel to the sum of his previous plays, while the optimal play of the player is always parallel. On the other hand, for some cases where the exact minimax strategy is hard to characterize, we introduce a new relaxation procedure based on a Normal approximation. In the particular application of interest, we show the relaxation is strong enough to yield an optimal regret bound, up to constant factors.\nIn Section 5, we use our new tools to recover and extend previous results on minimax strategies for linear online learning, including results for bounded domains. In fact, we show how to obtain a family of minimax strategies that smoothly interpolates between the minimax algorithm for a bounded feasible set and a minimax optimal algorithm in fact equivalent to unconstrained gradient descent. We emphasize that all the algorithms from this family are exactly minimax optimal,1 in a sense we will make precise in the next section. Moreover, if you are allowed to play outside of the comparator set, we show that some members of this family have a non-vacuous regret bound for the unconstrained setting, while remaining optimal for the constrained one.\nWhen studying unconstrained problems, a natural question is how small we can make the dependence of the regret bound on U , the L2 norm of an arbitrary comparator point, while still maintaining a \u221a T dependency on the time horizon. The best algorithm from the above family achieves Regret(U) \u2264 12 (U2 + 1) \u221a T . Streeter and McMahan [2012] and Orabona [2013] show it is possible to reduce the dependence on U to O(U logUT ). In order to improve on this, in Section 6 we apply our techniques to analyze a strategy, based on a Normal potential function, that gives a regret bound of O ( U \u221a T log(U \u221a T log2 T + 1) )\nwhere U is the L2 norm of a comparator, and both T and U are\nunknown. This bound is optimal up to \u221a log logT terms. Moreover, when T is known, we propose an algorithm based on a similar potential function that is optimal up to constant terms. This solves the open problem posed in those papers, matching the lower bound for this problem. Table 1 summarizes the regret bounds we prove, along with those for related algorithms.\nOur analysis tools for both known-T and unknown horizon algorithms rest heavily on the relationship between the reward (negative loss) achieved by the algorithm, potential functions that provide a benchmark for the amount of reward the algorithm should have, the regret of the algorithm with respect to a post-hoc comparator u, and the conditional value of the game. These are familiar concepts from the literature, but we summarize these relationships and provide some modest generalizations in Section 3."}, {"heading": "2 Notation and Problem Formulation", "text": "Let H be a Hilbert space with inner product \u3008\u00b7, \u00b7\u3009. The associated norm is denoted by \u2016 \u00b7 \u2016, i.e. \u2016x\u2016 = \u221a\n\u3008x, x\u3009. Given a closed and convex function f with domain S \u2286 H, we will denote its Fenchel conjugate by f\u2217 : H \u2192 R where f\u2217(u) = supv\u2208S ( \u3008v, u\u3009 \u2212 f(v) ) .\nWe consider a version of online linear optimization, a standard game for studying repeated decision making. On each of a sequence of rounds, a player chooses an action wt \u2208 H, an adversary chooses a linear cost function gt \u2208 G \u2286 H, and the player suffers loss \u3008wt, gt\u3009. For any sequence of plays\n1In this work, we use the term \u201cminimax\u201d to refer to the exact minimax solution to the zero sum game, as opposed to algorithms that only achieve the minimax optimal rate up to say constant factors.\nw1, . . . wT and g1, . . . , gT , we define the regret against a comparator u in the standard way:\nRegret(u) \u2261 T\u2211\nt=1\n\u3008gt, wt \u2212 u\u3009 .\nThis setting is general enough to cover the cases of online learning in, for example, Rd, in the vector space of matrices, and in a RKHS. We also define the reward of the algorithm, which is the earnings (or negative losses) of the player throughout the game:\nReward \u2261 T\u2211\nt=1\n\u3008\u2212gt, wt\u3009 .\nWe write \u03b8t \u2261 \u2212g1:t, where we use the compressed summation notation g1:t \u2261 \u2211T s=1 gs.\nThe Minimax View It will be useful to consider a full game-theoretic characterization of the above interaction when the number of rounds T is known to both players. This approach that has received significant recent interest [Abernethy et al., 2008a, 2007, Abernethy and Warmuth, 2010, Abernethy et al., 2008b, Streeter and McMahan, 2012].\nIn the constrained setting, where the comparator vector u \u2208 W , we have that the value of the game,\nthat is the regret when both the player and the adversary play optimally, is\nV \u2261 min w1\u2208H max g1\u2208G \u00b7 \u00b7 \u00b7 min wT\u2208H max gT\u2208G\n(\nsup u\u2208W\nT\u2211\nt=1\n\u3008wt \u2212 u, gt\u3009 )\n= min w1\u2208H max g1\u2208G \u00b7 \u00b7 \u00b7 min wT\u2208H max gT\u2208G\n( T\u2211\nt=1\n\u3008wt, gt\u3009+ sup u\u2208W\n\u3008u, \u03b8T \u3009 )\n= min w1\u2208H max g1\u2208G \u00b7 \u00b7 \u00b7 min wT\u2208H max gT\u2208G\n( T\u2211\nt=1\n\u3008wt, gt\u3009+B(\u03b8T ) ) ,\nwhere B(\u03b8) = sup\nw\u2208W \u3008w, \u03b8\u3009 . (1)\nFollowing McMahan and Abernethy [2013], we generalize the game in terms of a generic convex benchmark function B : H \u2192 R, instead of using the definition (1). This allows us to analyze the constrained and unconstrained setting in a unified way. Hence, the value of the game is the difference between the benchmark reward B(\u03b8T ) and the actual reward achieved by the player (under optimal play by both parties). Intuitively, viewing the units of loss/reward as dollars, V is the amount of starting capital we need (equivalently, the amount we need to borrow) to ensure we end the game with B(\u03b8T ) dollars. The motivation for defining the game in terms of an arbitrary B is made clear in the next section: It will allow us to derive Regret bounds in terms of the Fenchel conjugate of B.\nWe define inductively the conditional value of the game after g1, . . . , gt have been played by\nVt(\u03b8t) = min w\u2208H max g\u2208G\n(\u3008g, w\u3009+ Vt+1(\u03b8t \u2212 g)) with VT (\u03b8T ) = B(\u03b8T ) .\nThus, we can view the notation V for the value of the game as shorthand for V0(0). Under minimax play by both players, unrolling the previous equality, we have\n\u2211t s=1\u3008gs, ws\u3009+ Vt(\u2212g1:t) = V, or for\nt = T ,\nReward =\nT\u2211\nt=1\n\u3008\u2212gt, wt\u3009 = B(\u03b8T )\u2212 V . (2)\nWe also have that, given the conditional value of the game, a minimax-optimal strategy is\nwt+1 = argmin w max g\u2208G\n\u3008g, w\u3009+ Vt+1(\u03b8t \u2212 g) . (3)\nMcMahan and Abernethy [2013, Cor. 2] showed that in the unconstrained case, Vt is a smoothed version of B, where the smoothing comes from an expectation over future plays of the adversary. In this work, we show that in some cases (Theorem 4) we can find a closed form for Vt in terms of B, and in fact the solution to (3) will simply be the gradient of Vt, or equivalently, an FTRL algorithm with regularizer V \u2217t . On the other hand, to derive our main results, we face a case (Theorem 6) where Vt is generally not expressible in closed form, and the resulting algorithm does not look like FTRL. We solve the first problem by using a Normal approximation to the adversary\u2019s future moves, and we solve the second by showing (3) can still be solved in closed form with respect to this approximation to Vt."}, {"heading": "3 Potential Functions and the Duality of Reward and Regret", "text": "In the present section we will review some existing results in online learning theory as well as provide a number of mild generalizations for our purposes. Potential functions play a major role in the design\nand analysis of online learning algorithms [Cesa-Bianchi and Lugosi, 2006]. We will use q : H \u2192 R to describe the potential, and the key assumptions are that q should depend solely on the cumulative gradients g1:T and that q is convex in this argument.2 Since our aim is adaptive algorithms, we often look at a sequence of changing potential functions q1, . . . , qT , each of which takes as argument \u2212g1:t and is convex. These functions have appeared with different interpretations in many papers, with different emphasis. They can be viewed as 1) the conjugate of an (implicit) time-varying regularizer in a Mirror Descent or Follow-the-Regularized-Leader (FTRL) algorithm [Cesa-Bianchi and Lugosi, 2006, Shalev-Shwartz, 2007, Rakhlin, 2009], 2) as proxy for the conditional value of the game in a minimax setting [Rakhlin et al., 2012], or 3) a potential giving a bound on the amount of reward we want the algorithm to have obtained at the end of round t [Streeter and McMahan, 2012, McMahan and Abernethy, 2013].\nThese views are of course closely connected, but can lead to somewhat different analysis techniques. Following the last view, suppose we interpret qt(\u03b8t) as the desired reward at the end of round t, given the adversary has played \u03b8t = \u2212g1:t so far. Then, if we can bound our actual final reward in terms of qT (\u03b8T ), we also immediately get a regret bound stated in terms of the Fenchel conjugate q\u2217T . Generalizing Streeter and McMahan [2012, Thm. 1], we have the following result (all omitted proofs can be found in the Appendix).\nTheorem 1. Let \u03a8 : H \u2192 R be a convex function. An algorithm for the player guarantees\nReward \u2265 \u03a8(\u2212g1:T )\u2212 \u01eb\u0302 for any g1, . . . , gT (4)\nfor a constant \u01eb\u0302 \u2208 R if and only if it guarantees\nRegret(u) \u2264 \u03a8\u2217(u) + \u01eb\u0302 for all u \u2208 H . (5)\nFirst we consider the minimax setting, where we define the game in terms of a convex benchmark B. Then, (2) gives us an immediate lower bound on the reward of the minimax strategy for the player (against any adversary), and so applying Theorem 1 with \u03a8 = B gives\n\u2200u \u2208 H, Regret(u) \u2264 B\u2217(u) + V . (6)\nThe fundamental point, of which we will make much use, is this: even if one only cares about the traditional definition of regret, the study of the minimax game defined in terms of a general comparator benchmark B may be interesting, as the minimax algorithm for the player may then give novel bounds on regret. Note when B is defined as in (1), the theorem implies \u2200u \u2208 W , Regret(u) \u2264 V . More generally, even for non-minimax algorithms, Theorem 1 states that understanding the reward (equivalently, loss) of an algorithm as a function of the sum of gradients chosen by the adversary is both necessary and sufficient for understanding the regret of the algorithm.\nNow we consider the potential function view. The following general bound for any sequence of plays wt against gradients gt, for an arbitrary sequence of potential functions qt, has been used numerous times (see Orabona [2013, Lemma 1] and references therein). The claim is that\nRegret(u) \u2264 q\u2217T (u) + T\u2211\nt=1\n(qt(\u03b8t)\u2212 qt\u22121(\u03b8t\u22121) + \u3008wt, gt\u3009) , (7)\nwhere we take \u03b80 = ~0, and assume q0(~0) = 0. In fact, this statement is essentially equivalent to the argument of (4) and (5). For intuition, we can view qt(\u03b8t) as the amount of money we wish to have available at the end of round t. Suppose at the end of each round t, we borrow an additional sum \u01ebt\n2It is sometimes possible to generalize to potentials qt(g1, . . . , gt) that are functions of each gradient individually.\nas needed to ensure we actually have qt(\u03b8t) on hand. Then, based on this invariant, the amount of reward we actually have after playing on round t is qt\u22121(\u03b8t\u22121) + \u3008wt,\u2212gt\u3009, the money we had at the beginning of the round, plus the reward we get for playing wt. Thus, the additional amount we need to borrow at the end of round t in order to maintain the invariant is exactly\n\u01ebt(\u03b8t\u22121, gt) \u2261 qt(\u03b8t) \ufe38 \ufe37\ufe37 \ufe38\nReward desired\n\u2212 ( qt\u22121(\u03b8t\u22121) + \u3008wt,\u2212gt\u3009 )\n\ufe38 \ufe37\ufe37 \ufe38\nReward achieved\n, (8)\nrecalling \u03b8t = \u03b8t\u22121 \u2212 gt. Thus, if we can find bounds \u01eb\u0302t such that for all t, \u03b8t\u22121, and g \u2208 G,\n\u01eb\u0302t \u2265 \u01ebt(\u03b8t\u22121, gt) (9)\nwe can re-state (7) as exactly (5) with \u03a8 = qT and \u01eb\u0302 = \u01eb\u03021:T . Further, solving (8) for the per-round reward \u3008wt,\u2212gt\u3009, summing from t = 1 to T and canceling telescoping terms gives exactly (4). Not surprisingly, both Theorem 1 and (7) can be proved in terms of the Fenchel-Young inequality.\nWhen T is known, and the qt are chosen carefully, it is possible to obtain \u01eb\u0302t = 0. On the other hand, when T is unknown to the players, typically we will need bounds \u01eb\u0302t > 0. For example, in both Streeter and McMahan [2012, Thm. 6] and Orabona [2013], the key is showing the sum of these \u01eb\u0302t terms is always bounded by a constant. For completeness, we also state standard results where we interpret q\u2217t as a regularizer.\nThe conjugate regularizer and Bregman divergences The updates of many algorithms are based on a time-varying version of the FTRL strategy,\nwt+1 = \u2207qt(\u03b8t) = argmin w \u3008g1:t, w\u3009 + q\u2217t (w), (10)\nwhere we view q\u2217t as a time-varying regularizer (see Orabona et al. [2013] and references therein). Regret bounds can be easily obtained using (7) when the regularizers q\u2217t (w) are increasing with t, and they are strongly convex w.r.t. a norm \u2016 \u00b7 \u2016\u2217, using the fact that the potential functions qt will be strongly smooth. Then strong smoothness and particular choice of wt implies\nqt\u22121(\u03b8t) \u2264 qt\u22121(\u03b8t\u22121)\u2212 \u3008wt, gt\u3009+ 1\n2 \u2016gt\u20162, (11)\nwhich leads to the bound\n\u01ebt(\u03b8t, gt) = qt(\u03b8t)\u2212 qt\u22121(\u03b8t\u22121) + \u3008wt, gt\u3009 \u2264 qt(\u03b8t)\u2212 qt\u22121(\u03b8t) + 1\n2 \u2016gt\u20162 \u2264\n1 2 \u2016gt\u20162,\nwhere the last inequality follows from the fact that if f(x) \u2264 g(x), then f\u2217(y) \u2265 g\u2217(y) (immediate from the definition of the conjugate).\nWhen the regularizer q\u2217 is fixed, that is, qt = q for all t for some convex function q, we get the approach pioneered by Grove et al. [2001] and Kivinen and Warmuth [2001]:\n\u01ebt(\u03b8t, gt) = q(\u03b8t)\u2212 q(\u03b8t\u22121) + \u3008wt, gt\u3009 = q(\u03b8t)\u2212 ( q(\u03b8t\u22121) + \u3008\u2207q(\u03b8t\u22121), gt\u3009 ) = Dq(\u03b8t, \u03b8t\u22121),\nwhere Dq is the Bregman Divergence with respect to q, and we predict with wt = \u2207q(\u03b8t\u22121).\nAdmissible relaxations and potentials We extend the notion of relaxations of the conditional value of the game of Rakhlin et al. [2012] to the present setting. We say vt with corresponding strategy wt is a relaxation of Vt if\n\u2200\u03b8, vT (\u03b8) \u2265 B(\u03b8) and (12) \u2200t \u2208 {0, . . . , T \u2212 1}, g \u2208 G, \u03b8 \u2208 H, vt(\u03b8) + \u01eb\u0302t+1 \u2265 \u3008g, wt+1\u3009+ vt+1(\u03b8 \u2212 g), (13)\nfor constants \u01eb\u0302t \u2265 0. This definition matches Eq. (4) of Rakhlin et al. [2012] if we force all \u01eb\u0302t = 0, but if we allow some slack \u01eb\u0302t, (13) corresponds exactly to (8) and (9).\nNote that (13) is invariant to adding a constant to all vt. In particular, given an admissible vt, we can define qt(\u03b8) = vt(\u03b8)\u2212 v0(~0) so qt(~0) = 0 and q satisfies (9) with the same \u01eb\u0302t values for which vt satisfies (13). Or we could define q0(~0) = 0 and qt(\u03b8) = vt(\u03b8) for t \u2265 1, and take \u01eb\u03021 \u2190 \u01eb\u03021+ v0(~0) (or any other way of distributing the v0(~0) into the \u01eb\u0302). Generally, when T is known we will find working with admissible relaxations vt to be most useful, while for unknown horizons T , potential functions with q0(~0) = 0 will be more natural.\nFor our admissible relaxations, we have a result that closely mirrors Theorem 1:\nCorollary 2. Let v0, . . . , vT be an admissible relaxation for a benchmark B. Then, for any sequence g1, . . . , gT , for any wt chosen so (13) and (12) are satisfied, we have\nReward \u2265 B(\u03b8T )\u2212 v0(0)\u2212 \u01eb\u03021:T and Regret(u) \u2264 B\u2217(u) + v0(0) + \u01eb\u03021:T .\nProof. For the first statement, re-arranging and summing (13) shows Rewardt \u2265 vt(\u03b8t)\u2212 \u01eb\u03021:t\u2212 v0(0) and so final Reward \u2265 B(\u03b8)\u2212 v0(0)\u2212 \u01eb\u03021:T ; the second result then follows from Theorem 1.\nThe regret bound corresponds to (6); in particular, if we take vt to be the conditional value of the game, then (12) and (13) hold with equality with all \u01eb\u0302t = 0. Note if we define B as in (1), the regret guarantee becomes \u2200u \u2208 W , Regret(u) \u2264 v0(0) + \u01eb\u03021:T , analogous to [Rakhlin et al., 2012, Prop. 1] when \u01eb\u03021:T = 0.\nDeriving algorithms Consider an admissible relaxation vt. Given the form of the regret bounds we have proved, a natural strategy is to choose wt+1 so as to minimize \u01eb\u0302t+1, that is,\nwt+1 = argmin w max g\u2208G vt+1(\u03b8t \u2212 g)\u2212 vt(\u03b8t) + \u3008g, w\u3009 = argmin w max g\u2208G \u3008g, w\u3009+ vt+1(\u03b8t \u2212 g), (14)\nfollowing Rakhlin et al. [2012, Eq. (5)], Rakhlin et al. [2013], and Streeter and McMahan [2012, Eq. (8)]. We see that vt+1 is standing in for the conditional value of the game in (3). Since additive constants do not impact the argmin, we could also replace vt with a potential qt, say qt(\u03b8) = vt(\u03b8) \u2212 v0(0)."}, {"heading": "4 Minimax Analysis Approaches for Known-Horizon Games", "text": "In general, the problem of calculating the conditional value of a game Vt(\u03b8) is hard. And even for a known potential, deriving an optimal solution via (14) is also in general a hard problem. When the player is unconstrained, we can simplify the computation of Vt and the derivation of optimal strategies. For example, following ideas from McMahan and Abernethy [2013],\n\u01ebt(\u03b8t) = max p\u2208\u2206(G),Eg\u223cp[g]=0 E g\u223cp\n[qt+1(\u03b8t \u2212 g)]\u2212 qt(\u03b8t),\nwhere \u2206(G) is the set of probability distributions on G. McMahan and Abernethy [2013] shows that in some cases is possible to easily calculate this maximum, in particular when G = [\u2212G,G]d and qt decomposes on a per-coordinate spaces (that is, when the problem is essentially d independent, one-dimensional problems).\nIn this section we will state two quite general cases where we can obtain the exact value of the game, even though the problem does not decompose on a per coordinate basis. Note that in both cases the optimal strategy for wt+1 will be in the direction of \u03b8t.\nWe study the game when the horizon T is known, with a benchmark function of the form B(\u03b8) = f(\u2016\u03b8\u2016) for an increasing convex function f : [0,+\u221e] \u2192 R (which ensures B is convex). Note this form for B is particularly natural given our desire to prove results that hold for general Hilbert spaces. We will then be able to derive regret bounds using Theorem 1, and the following technical lemma:\nLemma 3. Let B(\u03b8) = f(\u2016\u03b8\u2016) for f : R \u2192 (\u2212\u221e,+\u221e] even. Then, B\u2217(u) = f\u2217(\u2016u\u2016). Recall that f is even if f(x) = f(\u2212x). Our key tool will be a careful study of the one-round version of this game. For this section, we let h :R \u2192 R be an even convex function that is increasing on [0,\u221e], G = {g : \u2016g\u2016 \u2264 G}, and d the dimension of H. We consider the one-round game\nH \u2261 min w max g\u2208G \u3008w, g\u3009+ h(\u2016\u03b8 \u2212 g\u2016) , (15)\nwhere \u03b8 \u2208 H is a fixed parameter. For results regarding this game, we let H(w, g) = \u3008w, g\u3009 + h(\u2016\u03b8 \u2212 g\u2016), w\u2217 = argminw maxg\u2208G H(w, g), and g\u2217 = argmaxg\u2208G H(w\u2217, g). Also, let \u03b8\u0302 = \u03b8\u2016\u03b8\u2016 if \u2016\u03b8\u2016 6= 0, and ~0 otherwise."}, {"heading": "4.1 The case of the orthogonal adversary", "text": "Let B(\u03b8) = f(\u2016\u03b8\u2016) for an increasing convex function f : [0,\u221e] \u2192 R, and define\nft(x) = f( \u221a x2 +G2(T \u2212 t)) .\nNote that ft(\u2016\u03b8\u2016) can be viewed as a smoothed version of B(\u03b8), since \u221a\n\u2016\u03b8\u20162 + C is a smoothed version of \u2016\u03b8\u2016 for a constant C > 0. Moreover, f0(\u2016\u03b8\u2016) = B(\u03b8).\nOur first key result is the following:\nTheorem 4. Let the adversary play from G = {g : \u2016g\u2016 \u2264 G} and assume all the ft satisfy\nmin w max g\u2208G\n\u3008w, g\u3009+ ft+1(\u2016\u03b8 \u2212 g\u2016) = ft+1 (\u221a \u2016\u03b8\u20162 +G2 ) . (16)\nThen the value of the game is f(G \u221a T ), the conditional value is Vt(\u03b8) = ft(\u2016\u03b8\u2016) = ft+1( \u221a\n\u2016\u03b8\u20162 +G2), and the optimal strategy can be found using (14) on Vt.\nFurther, a sufficient condition for (16) is that d > 1, f is twice differentiable, and f \u2032\u2032(x) \u2264 f \u2032(x)/x, for all x > 0. In this case we also have that the minimax optimal strategy is\nwt+1 = \u2207Vt(\u03b8t) = \u03b8t f \u2032(\n\u221a\n\u2016\u03b8t\u20162 +G2(T \u2212 t)) \u221a\n\u2016\u03b8t\u20162 +G2(T \u2212 t) . (17)\nIn this case, the minimax optimal strategy (20) is equivalent to the FTRL strategy in (10) with the time varying regularizer V \u2217t (w). The key lemma needed for the proof is the following: Lemma 5. Consider the game of (15). Then, if d > 1, h is twice differentiable, and h\u2032\u2032(x) \u2264 h \u2032(x) x for x > 0, we have:\nH = h (\u221a \u2016\u03b8\u20162 +G2 )\nand w\u2217 = \u03b8 \u221a \u2016\u03b8\u20162 +G2 h\u2032\n(\u221a \u2016\u03b8\u20162 +G2 ) .\nAny g\u2217 such that \u3008\u03b8, g\u2217\u3009 = 0 and \u2016g\u2217\u2016 = G is a minimax play for the adversary. We defer the proofs to the Appendix (of the proofs in the appendix, the proof of Lemmas 5 and 8 are perhaps the most important and instructive). Since the best response of the adversary is always to play a g\u2217 orthogonal to \u03b8, we call this the case of the orthogonal adversary."}, {"heading": "4.2 The case of the parallel adversary, and Normal approximations", "text": "We analyze a second case where (15) has closed-form solution, and hence derive a class of games where we can cleanly state the value of the game and the minimax optimal strategy. The results of McMahan and Abernethy [2013] can be viewed as a special case of the results in this section.\nFirst, we introduce some notation. We write \u03c4 \u2261 T \u2212 t when T and t are clear from context. We write r \u223c {\u22121, 1} to indicate r is a Rademacher random variable, and r\u03c4 \u223c {\u22121, 1}\u03c4 to indicate r\u03c4 is the sum of \u03c4 IID Rademacher random variables. Let \u03c3 = \u221a\n\u03c0/2. We write \u03c6 for a random variable with distribution N(0, \u03c32), and similarly define \u03c6\u03c4 \u223c N(0, (T \u2212 t)\u03c32). Then, define\nft(x) = E r\u03c4\u223c{\u22121,1}\u03c4 [f (|x+ r\u03c4G|)] and f\u0302t(x) = E \u03c6\u03c4\u223cN(0,\u03c4\u03c32) [f (|x+ \u03c6\u03c4G|)] , (18)\nand note B(\u03b8) = fT (\u2016\u03b8\u2016) = f\u0302T (\u2016\u03b8\u2016) since \u03c60 and r0 are always zero. These functions are exactly smoothed version of the function f used to define B. With these definitions, we can now state:\nTheorem 6. Let B(\u03b8) = f(\u2016\u03b8\u2016) for an increasing convex function f : [0,\u221e] \u2192 R, and let the adversary play from G = {g : \u2016g\u2016 \u2264 G}. Assume ft and f\u0302t as in (18) for all t. If all the ft satisfy\nmin w max g\u2208G \u3008w, g\u3009+ ft+1(\u2016\u03b8 \u2212 g\u2016) = E r\u223c{\u22121,1}\n[ ft+1 (\u2016\u03b8\u2016+ rG) ] , (19)\nthen Vt(\u03b8) = ft(\u2016\u03b8\u2016) is exactly the conditional value of the game, and (14) gives the minimax optimal strategy:\nwt+1 = \u03b8\u0302 ft+1 (\u2016\u03b8\u2016+G)\u2212 ft+1 (\u2016\u03b8\u2016 \u2212G)\n2G . (20)\nSimilarly, suppose the f\u0302t satisfy the equality (19) (with f\u0302t replacing ft). Then qt(\u03b8) = f\u0302t(\u2016\u03b8\u2016) is an admissible relaxation of Vt, satisfying (13) with \u01eb\u0302t = 0, using wt+1 based on (14). Further, a sufficient condition for (19) is that d = 1, or d > 1, the ft (or f\u0302t, respectively) are twice differentiable, and satisfy and f \u2032\u2032t (x) \u2265 f \u2032t(x)/x for all x > 0.\nContrary to the case of the orthogonal adversary, the strategy in (20) cannot easily be interpreted as an FTRL algorithm. The proof is based on two lemmas. The first provides the key tool in supporting the Normal relaxation:\nLemma 7. Let f : R \u2192 R be a convex function and \u03c32 = \u03c0/2. Then,\nE g\u223c{\u22121,1} [f(g)] \u2264 E \u03c6\u223cN(0,\u03c32) [f(\u03c6)] .\nProof. First observe that E[(\u03c6 \u2212 1)1{\u03c6 > 0}] = 0 and E[(\u03c6 + 1)1{\u03c6 < 0}] = 0 by our choice of \u03c3. We will use two lower bounds on the function f , which follow from convexity:\nf(x) \u2265 f(1) + f \u2032(1)(x\u2212 1) and f(x) \u2265 f(\u22121) + f \u2032(\u22121)(x+ 1) .\nWriting out the value of E[f(\u03c6)] explicitly we have\nE[f(\u03c6)] = E[f(\u03c6)1{\u03c6 < 0}] + E[f(\u03c6)1{\u03c6 > 0}] \u2265 E[(f(\u22121) + f \u2032(\u22121)(\u03c6+ 1))1{\u03c6 < 0}] + E[(f(1) + f \u2032(1)(\u03c6\u2212 1))1{\u03c6 > 0}]\n= f(\u22121) + f(1)\n2 + f \u2032(\u22121)E[(\u03c6+ 1)1{\u03c6 < 0}] + f \u2032(1)E[(\u03c6 \u2212 1)1{\u03c6 < 0}] .\nThe latter two terms vanish, giving the stated inequality.\nThe second lemma is used to prove the sufficient condition by solving the one-round game; again, the proof is deferred to the Appendix. Note that functions of the form h(x) = g(x2), with g convex always satisfies the conditions of the following Lemma.\nLemma 8. Consider the game of (15). Then, if d = 1, or if d > 1, h is twice differentiable, and h\u2032\u2032(x) > h\n\u2032(x) x for x > 0, then\nH = h (\u2016\u03b8\u2016+G) + h (\u2016\u03b8\u2016 \u2212G)\n2 and w\u2217 = \u03b8\u0302 h (\u2016\u03b8\u2016+G)\u2212 h (\u2016\u03b8\u2016 \u2212G) 2G .\nAny g\u2217 that satisfies |\u3008\u03b8, g\u2217\u3009| = G\u2016\u03b8\u2016 and \u2016g\u2217\u2016 =G is a minimax play for the adversary.\nThe adversary can always play g\u2217 = G \u03b8\u2016\u03b8\u2016 when \u03b8 6= 0, and so we describe this as the case of the parallel adversary. In fact, inductively this means that all the adversary\u2019s plays gt can be on the same line, providing intuition for the fact that this lemma also applies in the 1-dimensional case.\nTheorem 6 provides a recipe to produce suitable relaxations qt which may, in certain cases, exhibit nice closed form solutions. The interpretation here is that a \u201cGaussian adversary\u201d is stronger than one playing from the set [\u22121, 1] which leads to IID Rademacher behavior, and this allows us to generate such potential functions via Gaussian smoothing. In this view, note that our choice of \u03c32 gives E\u03c6[|\u03c6|] = 1."}, {"heading": "5 A Power Family of Minimax Algorithms", "text": "We analyze a family of algorithms based on potentials B(\u03b8) = f(\u2016\u03b8\u2016) where f(x) = Wp |x|p for parameters W > 0 and p \u2208 [1, 2], when the dimension is at least two. This is reminiscent of p-norm algorithms [Gentile, 2003], but the connection is superficial\u2014the norm we use to measure \u03b8 is always the norm of our Hilbert space. Our main result is:\nCorollary 9. Let d > 1 and W > 0, and let f and B be defined as above. Define ft(x) = Wp ( x2 + (T \u2212 t)G )p/2\n. Then, ft(\u2016\u03b8\u2016) is the conditional value of the game, and the optimal strategy is as in Theorem 4. If p \u2208 (1, 2], letting q \u2265 2 such that 1/p+ 1/q = 1, we have a bound\nRegret(u) \u2264 1 W q\u22121q \u2016u\u2016q + W p\n( G \u221a T )p \u2264 ( 1 p + 1 q \u2016u\u2016 q ) G \u221a T ,\nwhere the second inequality comes by takingW = (G \u221a T )1\u2212p. For all u, the bound\n( 1 p+ 1 q \u2016u\u2016q\n) G \u221a T\nis minimized by taking p = 2. For p = 1, we have\n\u2200u : \u2016u\u2016 \u2264 W, Regret(u) \u2264 WG \u221a T .\nProof. Let f(x) = Wp |x|p for p \u2208 [1, 2], Then, f \u2032\u2032(x) \u2264 f \u2032(x)/x, in fact basic calculations show f \u2032(x)/x f \u2032\u2032(x) = 1 p\u22121 \u2265 1 when p \u2264 2. Hence, we can apply Theorem 4, proving the claim on the ft. The regret bounds can then be derived from Corollary 2, which gives Regret(u) \u2264 f\u2217(u) + f(G \u221a T ), noting f\u2217(u) = Wq | uW |q when p > 1. The fact that p = 2 is an optimal choice in the first bound follows from the fact that ddp ( 1 p + 1 q \u2016u\u2016q ) \u2264 0 for p \u2208 (1, 2] with q = pp\u22121 .\nThe p = 1 case in fact exactly recaptures the result of Abernethy et al. [2008a] for linear functions, extending it also to spaces of dimension equal to two. The optimal update is wt+1 = \u25bdft(\u2016\u03b8t\u2016) =\nW\u03b8t/ \u221a\n\u2016\u03b8t\u20162 +G2(T \u2212 t). In addition to providing a regret bound for the comparator set W = {u : \u2016u\u2016 \u2264 W}, the algorithm will in fact only play points from this set.\nFor p = q = 2, writing W = \u03b7, we have\nRegret(u) \u2264 1 2\u03b7 \u2016u\u20162 + \u03b7 2 G2T,\nfor any u. In this case we see W = \u03b7 is behaving not like the radius of a comparator set, but rather as a learning rate. In fact, we have wt+1 = \u2207Vt(\u03b8t) = \u03b7\u03b8t = \u2212\u03b7g1:t, and so we see this minimax-optimal algorithm is in fact constant-step-size gradient descent. Taking \u03b7 = 1\nG \u221a T yields 12 (\u2016u\u20162 + 1)G \u221a T .\nThis result complements McMahan and Abernethy [2013, Thm. 7], which covers the d = 1 case, or d > 1 when the adversary plays from G = [\u22121, 1]d.\nComparing the p = 1 and p > 1 algorithms reveals an interesting fact. For simplicity, take G = 1. Then, the p = 1 algorithm with W = 1 is exactly the minimax optimal algorithm for minimizing regret against comparators in the L2 ball (for d > 1): the value of this game is \u221a T and we can do no better (even by playing outside of the comparator set). However, picking p > 1 gives us algorithms that will play outside of the comparator set. While they cannot do better than \u221a T , taking G = 1 and \u2016u\u2016 = 1 shows that all algorithms in this family in fact achieve Regret(u) \u2264 \u221a T when \u2016u\u2016 \u2264 1, matching the exact minimax optimal value. Further, the algorithms with p > 1 provide much stronger guarantees, since they also give non-vacuous guarantees for \u2016u\u2016 > 1, and tighter bounds when \u2016u\u2016 < 1. This suggests that the p = 2 algorithm will be the most useful algorithm in practice, something that indeed has been observed empirically (given the prevalence of gradient descent in real applications). This result also clearly demonstrates the value of studying minimax-optimal algorithms for different choices of the benchmark B, as this can produce algorithms that are no worse and in some cases significantly better than minimax algorithms defined in terms of regret minimization directly (i.e., via (1)).\nThe key difference in these algorithms is not how they play against a minimax optimal adversary for the regret game, but how they play against non-worst-case adversaries. In fact, a simple induction based on Lemma 5 shows that any minimax-optimal adversary will play so that \u221a\n\u2016\u03b8t\u20162 +G2(T \u2212 t) = G \u221a T . Against such an adversary, the p = 1 algorithm is identical to the p = 2 algorithm with learning rate \u03b7 = 1 G \u221a T\n. In fact, using the choice of W from Corollary 9, all of these algorithms play identically against a minimax adversary for the regret game."}, {"heading": "6 Tight Bounds for Unconstrained Learning", "text": "In this section we analyze algorithms based on benchmarks and potentials of the form exp(\u2016\u03b8\u20162/t), and show they lead to a minimal dependence on \u2016u\u2016 in the corresponding regret bounds for a given upper bound on regret against the origin (equal to the loss of the algorithm).\nFirst, we derive a lower bound for the known T game. Using Lemma 14 in the Appendix, we can show that the B(\u03b8) = exp(\u2016\u03b8\u20162/T ) benchmark approximately corresponds to a regularizer of the form \u2016u\u2016 \u221a T log( \u221a T\u2016u\u2016+ 1); there is actually some technical challenge here, as the conjugate B\u2217 cannot be computed in closed form\u2014the given regularizer is an upper bound. This kind of regularizer is particularly interesting because it is related to parameter-free sub-gradient descent algorithms [Orabona, 2013]; a similar potential function was used for a parameter-free algorithm by [Chaudhuri et al., 2009]. The lower bound for this game was proven in Streeter and McMahan [2012] for 1-dimensional spaces, and Orabona [2013] extended it to Hilbert spaces and improved the leading constant. We report it here for completeness.\nTheorem 10. Fix a non-trivial Hilbert space H and a specific online learning algorithm. If the algorithm guarantees a zero regret against the competitor with zero norm, then there exists a sequence\nof T cost vectors in H, such that the regret against any other competitor is \u2126(T ). On the other hand, if the algorithm guarantees a regret at most of \u01eb > 0 against the competitor with zero norm, then, for any 0 < \u03b7 < 1, there exists a T0 and a sequence of T \u2265 T0 unitary norm vectors gt \u2208 H, and a vector u \u2208 H such that\nRegret(u) \u2265 (1 \u2212 \u03b7)\u2016u\u2016 \u221a 1\nlog 2\n\u221a\nT log \u03b7\u2016u\u2016\n\u221a T\n3\u01eb \u2212 2 .\n6.1 Deriving a known-T algorithm with minimax rates via the Normal approximation\nConsider the game with fixed known T , an adversary that plays from G = {g \u2208 H | \u2016g\u2016 \u2264 G}, and\nB(\u03b8) = \u01eb exp (\u2016\u03b8\u20162 2aT ) ,\nfor constants a > 1 and \u01eb > 0. We will show that we are in the case of the parallel adversary, Section 4.2. Both computing the ft based on Rademacher expectations and evaluating the sufficient condition for those ft appear quite difficult, so we turn to the Normal approximation. We then have\nf\u0302t(x) = E \u03c6\u03c4\n[\n\u01eb exp\n( (x+ \u03c6\u03c4G) 2\n2at\n)]\n= \u01eb\n(\n1\u2212 \u03c0G 2(T \u2212 t) 2aT\n)\u2212 1 2\nexp\n( x2\n2aT \u2212 \u03c0G2(T \u2212 t)\n)\n,\nwhere we have computed the expectation in a closed form for the second equality. One can quickly verify that it satisfies the hypothesis of Theorem 6 for a > G2\u03c0/2, hence qt(\u03b8) = f\u0302t(\u2016\u03b8\u2016) will be an admissible relaxation. Thus, by Corollary 2, we immediately have\nRegret(u) \u2264 B\u2217(\u03b8T ) + \u01eb ( 1\u2212 \u03c0G 2\n2a\n)\u2212 1 2\n,\nand so by Lemma 14 in the Appendix, we can state the following Theorem, that matches the lower bound up to a constant multiplicative factor.\nTheorem 11. Let a > G2\u03c0/2, and G = {g : \u2016g\u2016 \u2264 G}. Denote by \u03b8\u0302 = \u03b8\u2016\u03b8\u2016 if \u2016\u03b8\u2016 6= 0, and ~0 otherwise. Fix the number of rounds T of the game, and consider the strategy\nwt+1 = \u01eb\u03b8\u0302t exp\n( (\u2016\u03b8t\u2016+G)2\n2aT\u2212\u03c0G2(T\u2212t\u22121)\n) \u2212 exp (\n(\u2016\u03b8t\u2016\u2212G)2 2aT\u2212\u03c0G2(T\u2212t\u22121)\n)\n2G \u221a 1\u2212 \u03c0G2(T\u2212t\u22121)2aT .\nThen, for any sequence of linear costs {gt}Tt=1, and any u \u2208 H, we have\nRegret(u) \u2264 \u2016u\u2016 \u221a \u221a \u221a \u221a2aT log (\u221a aT\u2016u\u2016 \u01eb + 1 ) + \u01eb (( 1\u2212 \u03c0G 2 2a )\u2212 1 2 \u2212 1 ) ."}, {"heading": "6.2 AdaptiveNormal: an adaptive algorithm for unknown T", "text": "Our techniques suggest the following recipe for developing adaptive algorithms: analyze the known T case, define a potential qt(\u03b8) \u2248 VT (\u03b8), and then analyze the incrementally-optimal algorithm for this\npotential (14) via Theorem 1. We follow this recipe in the current section. Again consider the game where an adversary that plays from G = {g \u2208 H | \u2016g\u2016 \u2264 G}. Define the function ft as\nft(x) = \u03b2t exp ( x\n2at\n)\n,\nwhere a > 3\u03c0G 2\n4 , and the \u03b2t is a decreasing sequence that will be specified in the following. From this, we define the potential qt(\u03b8) = ft(\u2016\u03b8\u20162). Suppose we play the incrementally-optimal algorithm of (14). Using Lemma 8 we can write the minimax value for the one-round game,\n\u01ebt(\u03b8t) = E r\u223c{\u22121,1}\n[ft+1((\u2016\u03b8t\u2016+ rG)2)]\u2212 qt(\u03b8t)\n\u2264 E \u03c6\u223cN(0,\u03c32) [ft+1((\u2016\u03b8t\u2016+ \u03c6G)2)]\u2212 qt(\u03b8t) . Lemma 7.\nUsing Lemma 17 in the Appendix and our hypothesis on a, we have that the RHS of this inequality is maximized for \u2016\u03b8t\u2016 = 0. Hence, using the inequality \u221a a+ b \u2264 \u221aa+ b\n2 \u221a a , \u2200a, b > 0, we get\n\u01ebt(\u03b8t) \u2264 \u03b2t+1\n\u221a\n1 + \u03c0G2 2a (t+ 1)\u2212 \u03c0G2 \u2212 \u03b2t \u2264 \u03b2t 2\n\u03c0G2 2a (t+ 1)\u2212 \u03c0G2 \u2264 \u03c0G2\u03b2t 4a t .\nThus, choosing \u03b2t = \u01eb/ log 2(t + 1), for example, is sufficient to prove that \u01eb1:T is bounded by \u01eb\u03c0G 2\na [Baxley, 1992]. Hence, again using Corollary 2 and Lemma 14 in the Appendix, we can state the following Theorem.\nTheorem 12. Let a > 3G2\u03c0/4, and G = {g : \u2016g\u2016 \u2264 G}. Denote by \u03b8\u0302 = \u03b8\u2016\u03b8\u2016 if \u2016\u03b8\u2016 6= 0, and ~0 otherwise. Consider the strategy\nwt+1 = \u01eb\u03b8\u0302t\n(\nexp ( (\u2016\u03b8t\u2016+G)2 2a(t+ 1) ) \u2212 exp ( (\u2016\u03b8t\u2016 \u2212G)2 2a(t+ 1) )) ( 2G log2(t+ 2) )\u22121 .\nThen, for any sequence of linear costs {gt}Tt=1, and any u \u2208 H, we have\nRegret(u) \u2264 \u2016u\u2016 \u221a \u221a \u221a \u221a2aT log (\u221a aT\u2016u\u2016 log2(T + 1)\n\u01eb + 1\n)\n+ \u01eb\n( \u03c0G2\na \u2212 1\n)\n."}, {"heading": "A Proofs", "text": ""}, {"heading": "A.1 Proof of Theorem 1", "text": "Proof. Suppose the algorithm provides the reward guarantee (4). First, note that for any comparator u, by definition we have Regret(u) = \u2212Reward\u2212\u3008g1:T , u\u3009 . (21) Then, applying the definitions of Reward, Regret, and the Fenchel conjugate, we have\nRegret(u) = \u03b8T \u00b7 u\u2212 Reward By (21) \u2264 \u03b8T \u00b7 u\u2212 qT (\u03b8T ) + \u01eb\u03021:T By assumption (4) \u2264 max\n\u03b8\n( \u03b8 \u00b7 u\u2212 qT (\u03b8) + \u01eb\u03021:T )\n= q\u2217T (u) + \u01eb\u03021:T .\nFor the other direction, assuming (5), we have for any comparator u,\nReward = \u03b8T \u00b7 u\u2212 Regret(u) By (21) = max\nv\n( \u03b8 \u00b7 v \u2212 Regret(v) )\n\u2265 max v\n( \u03b8 \u00b7 v \u2212 q\u2217T (v)\u2212 \u01eb\u03021:T ) By assumption (5)\n= qT (\u03b8) \u2212 \u01eb\u03021:T .\nAlternatively, one can prove this from the Fenchel-Young inequality."}, {"heading": "A.2 Proof of Lemma 3", "text": "Proof. We have B\u2217(u) = sup\u03b8 \u3008u, \u03b8\u3009 \u2212 f(\u2016\u03b8\u2016). If \u2016u\u2016 = 0, the stated equality is correct, in fact\nB\u2217(u) = sup \u03b8 \u2212f(\u2016\u03b8\u2016) = sup \u03b1\u22650 \u2212f(\u03b1) = sup \u03b1\u2208R \u2212f(\u03b1) = f\u2217(0) .\nHence we can assume \u2016u\u2016 6= 0, and by inspection we can take \u03b8 = \u03b1u/\u2016u\u2016, with \u03b1 \u2265 0, and so\nB\u2217(u) = sup \u03b1\u22650 \u03b1\u2016u\u2016 \u2212 f(\u03b1) = sup \u03b1\u2208R \u03b1\u2016u\u2016 \u2212 f(\u03b1) = f\u2217(\u2016u\u2016) ."}, {"heading": "A.3 Proof of Theorem 4", "text": "Proof. First we show that if f satisfies the condition on the derivatives, the same conditions is satisfied by ft, for all t. We have that all the ft have the form h(x) = f( \u221a x2 + a), where a \u2265 0. Hence we have to prove that xh \u2032\u2032(x)\nh\u2032(x) \u2264 1. We have that h\u2032(x) = xf \u2032(\n\u221a x2+a)\u221a\nx2+a , and h\u2032\u2032(x) =\nx2f \u2032\u2032( \u221a x2+a)+ a\u221a\nx2+a f \u2032(\n\u221a x2+a)\nx2+a , so\nxh\u2032\u2032(x)\nh\u2032(x) =\nx2f \u2032\u2032( \u221a x2 + a) \u221a x2 + a\nf \u2032( \u221a x2 + a)(x2 + a)\n+ a x2 + a \u2264 x\n2\nx2 + a +\na\nx2 + a = 1,\nwhere in the inequality we used the hypothesis on the derivatives of f .\nWe show Vt has the stated form by induction from T down to 0. The base case for t = T is immediate. For the induction step, we have\nVt(\u03b8) = min w max g\n\u3008w, g\u3009+ Vt+1(\u03b8 \u2212 g) Defn.\n= min w max g\n\u3008w, g\u3009+ ft+1 (\u2016\u03b8 \u2212 g\u2016) (IH)\n= ft+1\n(\u221a \u2016\u03b8\u20162 +G2 )\nAssumption (16)\n= f (\u221a \u2016\u03b8\u20162 +G2(T \u2212 t) ) .\nThe sufficient condition for (16) follow immediately from Lemma 5."}, {"heading": "A.4 Proof of Theorem 6", "text": "Proof. First, we need to show the functions ft and f\u0302t of (18) are even. Let r be a random variable draw from any symmetric distribution. Then, we have\nft(x) = E[f(|x+ r|)] = E[f(| \u2212 x\u2212 r|)] = E[f(| \u2212 x+ r|)] = ft(\u2212x),\nwhere we have used the fact that | \u00b7 | is even and the symmetry of r. We show ft(\u2016\u03b8\u2016) = Vt(\u03b8) inductively from t = T down to t = 0. The base case T = t follows from the definition of ft. Then, suppose the result holds for t+ 1. We have\nVt(\u03b8) = min w max g\u2208G\n\u3008w, g\u3009+ Vt+1(\u03b8 \u2212 g) Defn.\n= min w max g\u2208G\n\u3008w, g\u3009+ ft+1(\u2016\u03b8 \u2212 g\u2016) IH\n= E r\u223c{\u22121,1}\n[ ft+1(\u2016\u03b8\u2016+ rG) ] Lemma 8\n= E r\u223c{\u22121,1}\n[\nE r\u03c4\u22121\u223c{\u22121,1}\u03c4\u22121\n[ f(\u2016\u03b8\u2016+ rG + r\u03c4\u22121G)\n] ]\n= ft(\u2016\u03b8\u2016),\nwhere the last two lines follow from the definition of ft and ft+1. The case for f\u0302t is similar, using the hypothesis of the Theorem we have\nmin w max g\u2208G\n\u3008g, w\u3009+ f\u0302t+1(\u2016\u03b8 \u2212 g\u2016) = Er\u223c{\u22121,1}[f\u0302t+1(\u2016\u03b8\u2016+ rG)] \u2264 Er\u223cN(0,\u03c32)[f\u0302t+1(\u2016\u03b8\u2016+ \u03c6G)]\n= f\u0302t(\u2016\u03b8\u2016),\nwhere in the inequality we used Lemma 7, and in the second equality the definition of f\u0302t. Hence, qt(\u03b8) = f\u0302t(\u2016\u03b8\u2016) satisfy (13) with \u01eb\u0302t = 0. Finally, the sufficient conditions come immediately from Lemma 8.\nA.5 Analysis of the one-round game: Proofs of Lemmas 5 and 8\nIn the process of proving these lemmas, we also show the following general lower bound:\nLemma 13. Under the same definitions as in Lemma 5, if d > 1, we have\nH \u2265 h (\u221a \u2016\u03b8\u20162 +G2 ) .\nWe now proceed with the proofs. The d = 1 case for Lemma 8 was was proved in McMahan and Abernethy [2013].\nBefore proving the other results, we simplify a bit the formulation of the minimax problem. For the other results, the maximization wrt g of a convex function is always attained when \u2016g\u2016 = G. Moreover, in the case of \u2016\u03b8\u2016 = 0 the other results are true, in fact\nmin w max g\u2208G \u3008w, g\u3009+ h(\u2016\u03b8 \u2212 g\u2016) = min w max \u2016g\u2016=G \u3008w, g\u3009+ h(\u2016g\u2016) = min w G\u2016w\u2016 + h(G) = h(G) .\nHence, without loss of generality, in the following we can write w = \u03b1 \u03b8\u2016\u03b8\u2016 + w\u0302, where \u3008w\u0302, \u03b8\u3009 = 0. It is easy to see that in all the cases the optimal choice of g turns out to be g = \u03b2 \u03b8\u2016\u03b8\u2016 + \u03b3w\u0302, where \u03b3 \u2265 0. With these settings, the minimax problem is equivalent to\nmin w max g\u2208G \u3008w, g\u3009 + h(\u2016\u03b8 \u2212 g\u2016) = min \u03b1,w\u0302 max \u03b22+\u2016w\u0302\u20162\u03b32=G2\n\u03b1\u03b2 + \u03b3\u2016w\u0302\u20162 + h( \u221a \u2016\u03b8\u20162 \u2212 2\u03b2\u2016\u03b8\u2016+G2) .\nBy inspection, the player can always choose w\u0302 = 0 so \u03b3\u2016w\u0302\u20162 = 0. Hence we have a simplified and equivalent form of our optimization problem\nmin w max g\u2208G \u3008w, g\u3009+ h(\u2016\u03b8 \u2212 g\u2016) = min \u03b1 max \u03b22\u2264G2\n\u03b1\u03b2 + h( \u221a \u2016\u03b8\u20162 \u2212 2\u03b2\u2016\u03b8\u2016+G2) . (22)\nFor Lemma 13, it is enough to set \u03b2 = 0 in (22). For Lemma 5, we upper bound the minimum wrt to \u03b1 with the specific choice of \u03b1. In particular,\nwe set \u03b1 = \u2016\u03b8\u2016\u221a \u2016\u03b8\u20162+G2\nh\u2032 (\u221a \u2016\u03b8\u20162 +G2 ) in (22), and get\nmin w max g\u2208G \u3008w, g\u3009+ h(\u2016\u03b8 \u2212 g\u2016) \u2264 max \u03b22\u2264G2\n\u03b2\u2016\u03b8\u2016h\u2032( \u221a\n\u2016\u03b8\u20162 +G2) \u221a\n\u2016\u03b8\u20162 +G2 + h(\n\u221a\n\u2016\u03b8\u20162 \u2212 2\u03b2\u2016\u03b8\u2016+G2) .\nThe derivative of argument of the max wrt \u03b2 is\n\u2016\u03b8\u2016h\u2032 (\u221a \u2016\u03b8\u20162 +G2 )\n\u221a \u2016\u03b8\u20162 +G2 \u2212\n\u2016\u03b8\u2016h\u2032 (\u221a \u2016\u03b8\u20162 \u2212 2\u03b2\u2016\u03b8\u2016+G2 )\n\u221a \u2016\u03b8\u20162 \u2212 2\u03b2\u2016\u03b8\u2016+G2 . (23)\nWe have that if \u03b2 = 0 the first derivative is 0. Using the hypothesis on the first and second derivative of h, we have that the second term in (23) increases in \u03b2. Hence \u03b2 = 0 is the maximum. Comparing the obtained upper bound with the lower bound in Lemma 13, we get the stated equality.\nFor Lemma 8, the second derivative wrt \u03b2 of the argument of the minimax problem in (22) is\n\u2212\u2016\u03b8\u2016 \u2212\u2016\u03b8\u2016h\u2032\u2032(\n\u221a \u2016\u03b8\u2016+G2 \u2212 2\u03b2\u2016\u03b8\u2016) + h\u2032( \u221a \u2016\u03b8\u2016+G2 \u2212 2\u03b2\u2016\u03b8\u2016) \u2016\u03b8\u2016\u221a \u2016\u03b8\u2016+G2\u22122\u03b2\u2016\u03b8\u2016\n\u2016\u03b8\u2016+G2 \u2212 2\u03b2\u2016\u03b8\u2016\nthat is non negative, for our hypothesis on the derivatives of h. Hence, the argument of the minimax problem is convex wrt \u03b2, hence the maximum is achieved at the boundary of the domains, that is \u03b22 = G2. So, we have\nmin w max g\u2208G\n\u3008w, g\u3009 + h(\u2016\u03b8 \u2212 g\u2016) = max (\u2212G\u03b1+ h(\u2016\u03b8\u2016+G), G\u03b1 + h(|\u2016\u03b8\u2016 \u2212G|)) .\nThe argmin of this quantity wrt to \u03b1 is obtained when the the two terms in the max are equal, so we obtained the stated equality."}, {"heading": "A.6 Lemma 14", "text": "Lemma 14. Define f(\u03b8) = \u03b2 exp \u2016\u03b8\u2016 2\n2\u03b1 , for \u03b1, \u03b2 > 0. Then\nf\u2217(w) \u2264 \u2016w\u2016 \u221a 2\u03b1 log (\u221a \u03b1\u2016w\u2016 \u03b2 + 1 ) \u2212 \u03b2 .\nProof. From the definition of Fenchel dual, we have\nf\u2217(w) = max \u03b8 \u3008\u03b8, w\u3009 \u2212 f(\u03b8) \u2264 \u3008\u03b8\u2217, w\u3009 \u2212 \u03b2 .\nwhere \u03b8\u2217 = argmax\u03b8\u3008\u03b8, w\u3009 \u2212 f(\u03b8). We now use the fact that \u03b8\u2217 satisfies w = \u2207f(\u03b8\u2217), that is\nw = \u03b8\u2217 \u03b2\n\u03b1 exp (\u2016\u03b8\u2217\u20162 2\u03b1 ) ,\nin other words we have that \u03b8\u2217 and w are in the same direction. Hence we can set \u03b8\u2217 = qw, so that f\u2217(w) \u2264 q\u2016w\u20162 \u2212 \u03b2. We now need to look for q > 0, solving\nq\u03b2\n\u03b1 exp (\u2016w\u20162q2 2\u03b1 ) = 1 \u21d4 \u2016w\u2016 2q2 2\u03b1 + log \u03b2q \u03b1 = 0\n\u21d4 q = \u221a 2\u03b1\n\u2016w\u20162 log \u03b1 q\u03b2 =\n\u221a \u221a \u221a \u221a \u221a 2\u03b1\n\u2016w\u20162 log\n\n\n\u221a \u03b1\u2016w\u2016\n\u03b2 \u221a\n2 log \u03b1q\u03b2\n\n .\nUsing the elementary inequality log x \u2264 me x 1 m , \u2200m > 0, we have\nq2 = 2\u03b1 \u2016w\u20162 log \u03b1 q\u03b2 \u2264 2m\u03b1 e\u2016w\u20162 ( \u03b1 q\u03b2 ) 1 m\n\u21d2 q 2m+1m \u2264 2m\u03b1 m+1 m\ne\u2016w\u20162\u03b2 1m \u21d2 q \u2264\n( 2m e\u2016w\u20162 ) m 2m+1 \u03b1 m+1 2m+1 \u03b2\u2212 1 2m+1\n\u21d2 \u03b1 \u03b2q \u2265 ( e\u2016w\u20162 2m ) m 2m+1 \u03b11\u2212 m+1 2m+1\u03b2 1 2m+1 \u22121 = \u03b1 \u03b2q \u2265 (\u221a e 2m \u2016w\u2016\u221a\u03b1 \u03b2 ) 2m 2m+1 .\nHence we have\nq \u2264 \u221a \u221a \u221a \u221a \u221a 2\u03b1\n\u2016w\u20162 log\n\n\n\u221a \u03b1\u2016w\u2016\n\u03b2 \u221a\n4m 2m+1 log\n\u221a e 2m \u2016w\u2016\u221a\u03b1 \u03b2\n\n .\nWe set m such that \u221a\ne 2m \u2016w\u2016\u221a\u03b1 \u03b2 = \u221a e, that is 12 ( \u2016w\u2016\u221a\u03b1 \u03b2 )2 = m. Hence we have log \u221a e 2m \u2016w\u2016\u221a\u03b1 \u03b2 =\n1 2 and \u2016w\u2016\u221a\u03b1 \u03b2\u221a 2m = 1, and obtain\nf\u2217(w) \u2264 \u2016w\u2016 \u221a \u221a \u221a \u221a \u221a2\u03b1 log   \u221a (\u221a\n\u03b1\u2016w\u2016 \u03b2\n)2\n+ 1\n \u2212 \u03b2 \u2264 \u2016w\u2016 \u221a 2\u03b1 log (\u221a \u03b1\u2016w\u2016 \u03b2 + 1 ) \u2212 \u03b2 ."}, {"heading": "A.7 Lemma 17", "text": "Lemma 15. Let f(x) = b exp ( x2\na\n) \u2212 exp ( x2\nc\n)\n. If a \u2265 c > 0, b \u2265 0, and b c \u2264 a, then the function f(x) is decreasing for x \u2265 0.\nProof. The proof is immediate from the study of the first derivative.\nLemma 16. Let f(t) = a 3 2 t\n\u221a t+1\n(a (t+1)\u2212b) 3 2\n, with a \u2265 3/2b > 0. Then f(t) \u2264 1 for any t \u2265 0.\nProof. The sign of the first derivative of the function has the same sign of\n(2 a\u2212 3b)(t+ 1) + b,\nhence from the hypothesis on a and b the function is strictly increasing. Moreover the asymptote for t \u2192 \u221e is 1, hence we have the stated upper bound.\nLemma 17. Let ft(x) = \u03b2t exp ( x 2at ) , \u03b2t+1 \u2264 \u03b2t, \u2200t. If a \u2265 3\u03c0G 2 4 , then\nargmax x E \u03c6\u223cN(0,\u03c32)\n[ft+1(x+ \u03c6G)]\u2212 ft(x) = 0,\nwhere \u03c32 = \u03c02 .\nProof. We have\nE \u03c6\u223cN(0,\u03c32)\n[ft+1(x+ \u03c6G)] = \u03b2t+1\n\u221a\na (t+ 1) a (t+ 1)\u2212 \u03c32G2 exp (\nx2\n2 [a (t+ 1)\u2212 \u03c32G2]\n)\n,\nso we have to study the max of\n\u03b2t+1\n\u221a\na (t+ 1) a (t+ 1)\u2212 \u03c32G2 exp (\nx2\n2 [a (t+ 1)\u2212 \u03c32G2]\n) \u2212 \u03b2t exp ( x2\n2 a t\n)\n.\nThe function is even, so we have a maximum in zero iff the function is decreasing for x > 0. Observe that, from Lemma 16, for any t \u2265 0\n\u221a\na (t+ 1) a (t+ 1)\u2212 \u03c32G2 a t a (t+ 1)\u2212 \u03c32G2 \u2264 1 .\nHence, using Lemma 15, we obtain that the stated result."}], "references": [{"title": "Repeated games against budgeted adversaries", "author": ["J. Abernethy", "M.K. Warmuth"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Abernethy and Warmuth.,? \\Q2010\\E", "shortCiteRegEx": "Abernethy and Warmuth.", "year": 2010}, {"title": "Continuous experts and the Binning algorithm", "author": ["J. Abernethy", "J. Langford", "M.K. Warmuth"], "venue": "In Proceedings of the 19th Annual Conference on Learning Theory (COLT06),", "citeRegEx": "Abernethy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2007}, {"title": "Optimal strategies and minimax lower bounds for online convex games", "author": ["J. Abernethy", "P.L. Bartlett", "A. Rakhlin", "A. Tewari"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Optimal strategies from random walks", "author": ["J. Abernethy", "M.K. Warmuth", "J. Yellin"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Euler\u2019s constant, Taylor\u2019s formula, and slowly converging series", "author": ["J.V. Baxley"], "venue": "Mathematics Magazine,", "citeRegEx": "Baxley.,? \\Q1992\\E", "shortCiteRegEx": "Baxley.", "year": 1992}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "A parameter-free hedging algorithm", "author": ["K. Chaudhuri", "Y. Freund", "D. Hsu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Chaudhuri et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2009}, {"title": "The robustness of the p-norm algorithms", "author": ["C. Gentile"], "venue": "Machine Learning,", "citeRegEx": "Gentile.,? \\Q2003\\E", "shortCiteRegEx": "Gentile.", "year": 2003}, {"title": "General convergence results for linear discriminant updates", "author": ["A.J. Grove", "N. Littlestone", "D. Schuurmans"], "venue": "Machine Learning,", "citeRegEx": "Grove et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Grove et al\\.", "year": 2001}, {"title": "Relative loss bounds for multidimensional regression problems", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Kivinen and Warmuth.,? \\Q2001\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 2001}, {"title": "Minimax optimal algorithms for unconstrained linear optimization", "author": ["H.B. McMahan", "J. Abernethy"], "venue": "In NIPS,", "citeRegEx": "McMahan and Abernethy.,? \\Q2013\\E", "shortCiteRegEx": "McMahan and Abernethy.", "year": 2013}, {"title": "Dimension-free exponentiated gradient", "author": ["F. Orabona"], "venue": "In NIPS,", "citeRegEx": "Orabona.,? \\Q2013\\E", "shortCiteRegEx": "Orabona.", "year": 2013}, {"title": "A generalized online mirror descent with applications to classification and regression", "author": ["F. Orabona", "K. Crammer", "N. Cesa-Bianchi"], "venue": null, "citeRegEx": "Orabona et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2013}, {"title": "Lecture notes on online learning", "author": ["A. Rakhlin"], "venue": "Technical report,", "citeRegEx": "Rakhlin.,? \\Q2009\\E", "shortCiteRegEx": "Rakhlin.", "year": 2009}, {"title": "Localization and adaptation in online learning", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "In AISTATS,", "citeRegEx": "Rakhlin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2013}, {"title": "Relax and randomize: From value to algorithms", "author": ["S. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": "In NIPS,", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Online learning: Theory, algorithms, and applications", "author": ["S. Shalev-Shwartz"], "venue": "Technical report, The Hebrew University,", "citeRegEx": "Shalev.Shwartz.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2007}, {"title": "Online learning and online convex optimization", "author": ["S. Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2012}, {"title": "No-regret algorithms for unconstrained online convex optimization", "author": ["M. Streeter", "H.B. McMahan"], "venue": "In NIPS,", "citeRegEx": "Streeter and McMahan.,? \\Q2012\\E", "shortCiteRegEx": "Streeter and McMahan.", "year": 2012}, {"title": "Dual averaging method for regularized stochastic learning and online optimization", "author": ["L. Xiao"], "venue": "In NIPS,", "citeRegEx": "Xiao.,? \\Q2009\\E", "shortCiteRegEx": "Xiao.", "year": 2009}], "referenceMentions": [{"referenceID": 15, "context": ", 2008a], in others upper bounds on the value of the game (often based on the sequential Rademacher complexity) are used to construct efficient algorithms with theoretical guarantees [Rakhlin et al., 2012].", "startOffset": 183, "endOffset": 205}, {"referenceID": 1, "context": "In some cases the value of the game can be calculated exactly in an efficient way [Abernethy et al., 2008a], in others upper bounds on the value of the game (often based on the sequential Rademacher complexity) are used to construct efficient algorithms with theoretical guarantees [Rakhlin et al., 2012]. While most of the work in this area has focused on the setting where the player is constrained to a bounded convex set [Abernethy et al., 2008a] (with the notable exception of McMahan and Abernethy [2013]), in this work we are interested in the general setting of unconstrained online learning with", "startOffset": 83, "endOffset": 511}, {"referenceID": 10, "context": "In Section 4, extending the work of McMahan and Abernethy [2013], we provide novel and general sufficient conditions to be able to compute the exact minimax strategy for both the player and the adversary, as well as the value of the game.", "startOffset": 36, "endOffset": 65}, {"referenceID": 10, "context": "In Section 4, extending the work of McMahan and Abernethy [2013], we provide novel and general sufficient conditions to be able to compute the exact minimax strategy for both the player and the adversary, as well as the value of the game. In particular, we show that under these conditions the optimal play of the adversary is always orthogonal or always parallel to the sum of his previous plays, while the optimal play of the player is always parallel. On the other hand, for some cases where the exact minimax strategy is hard to characterize, we introduce a new relaxation procedure based on a Normal approximation. In the particular application of interest, we show the relaxation is strong enough to yield an optimal regret bound, up to constant factors. In Section 5, we use our new tools to recover and extend previous results on minimax strategies for linear online learning, including results for bounded domains. In fact, we show how to obtain a family of minimax strategies that smoothly interpolates between the minimax algorithm for a bounded feasible set and a minimax optimal algorithm in fact equivalent to unconstrained gradient descent. We emphasize that all the algorithms from this family are exactly minimax optimal,1 in a sense we will make precise in the next section. Moreover, if you are allowed to play outside of the comparator set, we show that some members of this family have a non-vacuous regret bound for the unconstrained setting, while remaining optimal for the constrained one. When studying unconstrained problems, a natural question is how small we can make the dependence of the regret bound on U , the L2 norm of an arbitrary comparator point, while still maintaining a \u221a T dependency on the time horizon. The best algorithm from the above family achieves Regret(U) \u2264 1 2 (U + 1) \u221a T . Streeter and McMahan [2012] and Orabona [2013] show it is possible to reduce the dependence on U to O(U logUT ).", "startOffset": 36, "endOffset": 1854}, {"referenceID": 10, "context": "In Section 4, extending the work of McMahan and Abernethy [2013], we provide novel and general sufficient conditions to be able to compute the exact minimax strategy for both the player and the adversary, as well as the value of the game. In particular, we show that under these conditions the optimal play of the adversary is always orthogonal or always parallel to the sum of his previous plays, while the optimal play of the player is always parallel. On the other hand, for some cases where the exact minimax strategy is hard to characterize, we introduce a new relaxation procedure based on a Normal approximation. In the particular application of interest, we show the relaxation is strong enough to yield an optimal regret bound, up to constant factors. In Section 5, we use our new tools to recover and extend previous results on minimax strategies for linear online learning, including results for bounded domains. In fact, we show how to obtain a family of minimax strategies that smoothly interpolates between the minimax algorithm for a bounded feasible set and a minimax optimal algorithm in fact equivalent to unconstrained gradient descent. We emphasize that all the algorithms from this family are exactly minimax optimal,1 in a sense we will make precise in the next section. Moreover, if you are allowed to play outside of the comparator set, we show that some members of this family have a non-vacuous regret bound for the unconstrained setting, while remaining optimal for the constrained one. When studying unconstrained problems, a natural question is how small we can make the dependence of the regret bound on U , the L2 norm of an arbitrary comparator point, while still maintaining a \u221a T dependency on the time horizon. The best algorithm from the above family achieves Regret(U) \u2264 1 2 (U + 1) \u221a T . Streeter and McMahan [2012] and Orabona [2013] show it is possible to reduce the dependence on U to O(U logUT ).", "startOffset": 36, "endOffset": 1873}, {"referenceID": 1, "context": "Regret bounds for known-T algorithms (A) Minimax Regret \u221a T for u \u2208 W , O(T ) otherwise Abernethy et al. [2008a] (B) OGD, fixed \u03b7 1 2 (1 + U) \u221a T E.", "startOffset": 88, "endOffset": 113}, {"referenceID": 1, "context": "Regret bounds for known-T algorithms (A) Minimax Regret \u221a T for u \u2208 W , O(T ) otherwise Abernethy et al. [2008a] (B) OGD, fixed \u03b7 1 2 (1 + U) \u221a T E.g., Shalev-Shwartz [2012] (C) pq-Algorithm (", "startOffset": 88, "endOffset": 174}, {"referenceID": 16, "context": "Theorem 11 Regret bounds for adaptive algorithms for unknown T (G) Adaptive FTRL/RDA (1 + 1 2 U) \u221a T Shalev-Shwartz [2007], Xiao [2009] (H) Dim.", "startOffset": 101, "endOffset": 123}, {"referenceID": 16, "context": "Theorem 11 Regret bounds for adaptive algorithms for unknown T (G) Adaptive FTRL/RDA (1 + 1 2 U) \u221a T Shalev-Shwartz [2007], Xiao [2009] (H) Dim.", "startOffset": 101, "endOffset": 136}, {"referenceID": 10, "context": "Following McMahan and Abernethy [2013], we generalize the game in terms of a generic convex benchmark function B : H \u2192 R, instead of using the definition (1).", "startOffset": 10, "endOffset": 39}, {"referenceID": 5, "context": "and analysis of online learning algorithms [Cesa-Bianchi and Lugosi, 2006].", "startOffset": 43, "endOffset": 74}, {"referenceID": 15, "context": "They can be viewed as 1) the conjugate of an (implicit) time-varying regularizer in a Mirror Descent or Follow-the-Regularized-Leader (FTRL) algorithm [Cesa-Bianchi and Lugosi, 2006, Shalev-Shwartz, 2007, Rakhlin, 2009], 2) as proxy for the conditional value of the game in a minimax setting [Rakhlin et al., 2012], or 3) a potential giving a bound on the amount of reward we want the algorithm to have obtained at the end of round t [Streeter and McMahan, 2012, McMahan and Abernethy, 2013].", "startOffset": 292, "endOffset": 314}, {"referenceID": 11, "context": "6] and Orabona [2013], the key is showing the sum of these \u01eb\u0302t terms is always bounded by a constant.", "startOffset": 7, "endOffset": 22}, {"referenceID": 11, "context": "where we view q\u2217 t as a time-varying regularizer (see Orabona et al. [2013] and references therein).", "startOffset": 54, "endOffset": 76}, {"referenceID": 8, "context": "When the regularizer q\u2217 is fixed, that is, qt = q for all t for some convex function q, we get the approach pioneered by Grove et al. [2001] and Kivinen and Warmuth [2001]: \u01ebt(\u03b8t, gt) = q(\u03b8t)\u2212 q(\u03b8t\u22121) + \u3008wt, gt\u3009 = q(\u03b8t)\u2212 ( q(\u03b8t\u22121) + \u3008\u2207q(\u03b8t\u22121), gt\u3009 ) = Dq(\u03b8t, \u03b8t\u22121), where Dq is the Bregman Divergence with respect to q, and we predict with wt = \u2207q(\u03b8t\u22121).", "startOffset": 121, "endOffset": 141}, {"referenceID": 8, "context": "When the regularizer q\u2217 is fixed, that is, qt = q for all t for some convex function q, we get the approach pioneered by Grove et al. [2001] and Kivinen and Warmuth [2001]: \u01ebt(\u03b8t, gt) = q(\u03b8t)\u2212 q(\u03b8t\u22121) + \u3008wt, gt\u3009 = q(\u03b8t)\u2212 ( q(\u03b8t\u22121) + \u3008\u2207q(\u03b8t\u22121), gt\u3009 ) = Dq(\u03b8t, \u03b8t\u22121), where Dq is the Bregman Divergence with respect to q, and we predict with wt = \u2207q(\u03b8t\u22121).", "startOffset": 121, "endOffset": 172}, {"referenceID": 13, "context": "Admissible relaxations and potentials We extend the notion of relaxations of the conditional value of the game of Rakhlin et al. [2012] to the present setting.", "startOffset": 114, "endOffset": 136}, {"referenceID": 13, "context": "(4) of Rakhlin et al. [2012] if we force all \u01eb\u0302t = 0, but if we allow some slack \u01eb\u0302t, (13) corresponds exactly to (8) and (9).", "startOffset": 7, "endOffset": 29}, {"referenceID": 13, "context": "following Rakhlin et al. [2012, Eq. (5)], Rakhlin et al. [2013], and Streeter and McMahan [2012, Eq.", "startOffset": 10, "endOffset": 64}, {"referenceID": 10, "context": "For example, following ideas from McMahan and Abernethy [2013],", "startOffset": 34, "endOffset": 63}, {"referenceID": 10, "context": "McMahan and Abernethy [2013] shows that in some cases is possible to easily calculate this maximum, in particular when G = [\u2212G,G]d and qt decomposes on a per-coordinate spaces (that is, when the problem is essentially d independent, one-dimensional problems).", "startOffset": 0, "endOffset": 29}, {"referenceID": 10, "context": "The results of McMahan and Abernethy [2013] can be viewed as a special case of the results in this section.", "startOffset": 15, "endOffset": 44}, {"referenceID": 7, "context": "This is reminiscent of p-norm algorithms [Gentile, 2003], but the connection is superficial\u2014the norm we use to measure \u03b8 is always the norm of our Hilbert space.", "startOffset": 41, "endOffset": 56}, {"referenceID": 1, "context": "The p = 1 case in fact exactly recaptures the result of Abernethy et al. [2008a] for linear functions, extending it also to spaces of dimension equal to two.", "startOffset": 56, "endOffset": 81}, {"referenceID": 11, "context": "This kind of regularizer is particularly interesting because it is related to parameter-free sub-gradient descent algorithms [Orabona, 2013]; a similar potential function was used for a parameter-free algorithm by [Chaudhuri et al.", "startOffset": 125, "endOffset": 140}, {"referenceID": 6, "context": "This kind of regularizer is particularly interesting because it is related to parameter-free sub-gradient descent algorithms [Orabona, 2013]; a similar potential function was used for a parameter-free algorithm by [Chaudhuri et al., 2009].", "startOffset": 214, "endOffset": 238}, {"referenceID": 6, "context": "This kind of regularizer is particularly interesting because it is related to parameter-free sub-gradient descent algorithms [Orabona, 2013]; a similar potential function was used for a parameter-free algorithm by [Chaudhuri et al., 2009]. The lower bound for this game was proven in Streeter and McMahan [2012] for 1-dimensional spaces, and Orabona [2013] extended it to Hilbert spaces and improved the leading constant.", "startOffset": 215, "endOffset": 312}, {"referenceID": 6, "context": "This kind of regularizer is particularly interesting because it is related to parameter-free sub-gradient descent algorithms [Orabona, 2013]; a similar potential function was used for a parameter-free algorithm by [Chaudhuri et al., 2009]. The lower bound for this game was proven in Streeter and McMahan [2012] for 1-dimensional spaces, and Orabona [2013] extended it to Hilbert spaces and improved the leading constant.", "startOffset": 215, "endOffset": 357}, {"referenceID": 4, "context": "Thus, choosing \u03b2t = \u01eb/ log (t + 1), for example, is sufficient to prove that \u01eb1:T is bounded by \u01eb 2 a [Baxley, 1992].", "startOffset": 102, "endOffset": 116}], "year": 2014, "abstractText": "We study algorithms for online linear optimization in Hilbert spaces, focusing on the case where the player is unconstrained. We develop a novel characterization of a large class of minimax algorithms, recovering, and even improving, several previous results as immediate corollaries. Moreover, using our tools, we develop an algorithm that provides a regret bound of O (", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}