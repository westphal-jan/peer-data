{"id": "1403.1076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2014", "title": "Is Intelligence Artificial?", "abstract": "our understanding of intelligence is primarily directed at the level of human beings. this paper will attempt to give a more unifying definition that can be consciously applied to everything that we know of. the definition would be probably used more to verify verbal intelligence, where a scale would also be required to measure how much. testing a version of an popularly accepted result of ai is then put referred forward generally as possibly the acid test for artificial intelligence itself. recent work has been more from a modern direction of mechanical processes, where this theoretical paper will not try to refute the idea of intelligence, but only to put it in a more general context.", "histories": [["v1", "Wed, 5 Mar 2014 11:09:55 GMT  (210kb)", "http://arxiv.org/abs/1403.1076v1", "Pre-print"], ["v2", "Sat, 8 Nov 2014 13:20:49 GMT  (419kb)", "http://arxiv.org/abs/1403.1076v2", "Pre-print"], ["v3", "Tue, 25 Nov 2014 16:19:47 GMT  (426kb)", "http://arxiv.org/abs/1403.1076v3", "Pre-print"], ["v4", "Wed, 28 Jan 2015 17:10:05 GMT  (443kb)", "http://arxiv.org/abs/1403.1076v4", "This new version adds some clarity to the discussion, which has realised a new idea. Also the opportunity to extend or update some sections. Some new references"], ["v5", "Mon, 29 Jun 2015 11:49:38 GMT  (459kb)", "http://arxiv.org/abs/1403.1076v5", "This new version adds some clarity to the discussion, which has realised a new idea. Also the opportunity to extend or update some sections. Some new references"]], "COMMENTS": "Pre-print", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kieran greer"], "accepted": false, "id": "1403.1076"}, "pdf": {"name": "1403.1076.pdf", "metadata": {"source": "META", "title": "DCS Paper", "authors": ["Kieran Greer"], "emails": [], "sections": [{"heading": null, "text": "beings. This paper will attempt to give a more unifying definition that can be applied to everything that we know of. The definition would be used more to verify intelligence, where a scale would also be required to measure how much. A version of an accepted result of AI is then put forward as the acid test for Artificial Intelligence itself. Recent work has been more from a direction of mechanical processes, where this paper will not try to refute the idea of intelligence, but only to put it in a more general context.\nKeywords: Artificial Intelligence, Intelligence, definition, model."}, {"heading": "1 Introduction", "text": "Our understanding of intelligence is primarily directed at human beings. This paper will attempt to give a more unifying definition that can be applied to everything that we know of. When writing a paper recently [3], it became clear that there is no neat or concise definition of what Artificial Intelligence is. As that is really just an extension of intelligence, this means that it is also difficult to concisely define intelligence. When once asked1, I firstly replied with the \u2018independent behaviour\u2019 line, but then added that an \u2018if-then-else\u2019 statement in a computer program might be considered as intelligent. It is able, by itself, to make the decision of what step to take next, even if this is hard-coded. A few example definitions of intelligence are as follows:\nThe ability to gain and apply knowledge and skills. (Pocket Oxford Dictionary)\n1 A.Schuster, University of Ulster, early 2000\u2019s.\nto apply knowledge to manipulate one's environment or to think abstractly as measured by objective criteria (as tests). (Merriam Webster)\nThe ability to understand and think about things, and to gain and use knowledge. (macmillandictionary.com)\nThere are probably as many definitions of intelligence as there are experts who study it. Simply put, however, intelligence is the ability to learn about, learn from, understand, and interact with one\u2019s environment. (giftedkids.about.com)\n(1) Intelligence is what you do when you don\u2019t know what to do. (2) Intelligence is a hypothetical idea which we have defined as being reflected by certain types of behaviour. (brainmetrix.com)\nKey elements therefore include the ideas of learning, reasoning, understanding and application, to use what is learnt. It is useful to this paper that the concepts of behaviour and environment are also included. What we learn or decide upon should eventually result in some sort of physical event. That is really the point of learning something in the first place, to apply it to a situation. Therefore, it could be argued that the acts of correct learning, reasoning and application in fact result in the act of correct behaviour. This is an attractive way of looking at intelligence, because other definitions are mostly humanoriented, whereas \u2018behaviour\u2019 is a much more general concept. Intelligence can include acts of genius, for example, but it does not have to and so even a very clear and concise definition should not be used as some sort of definitive marker. There would still be an intelligence scale, but it might make comparisons easier, to clear up when the term can be used. This is a problem for Artificial Intelligence in particular, when computer programs are also included.\nThe rest of this paper is structured as follows: section 2 gives a new definition for a universal idea of intelligence. This is only a suggestion, to put the whole concept into context. Section 3 applies the new definition more specifically to artificial intelligence. Section 4 gives one example of previous work that appears to be along similar lines of thought, while section 5 gives some conclusions on the work."}, {"heading": "2 A Universal Intelligence", "text": "All of the entities in the Universe are made from the same material \u2013 natural, man-made, living or not living, etc. This is the domain of the Physicists (for example, [1][6]) who have the more difficult task of a unifying theory for the whole Universe. As far as intelligence is concerned however, suggestions can be made for unifying theories. Because we are all made from the same ingredients, if there is no new or magic part required for intelligence, it might therefore be possible to apply the concept to all naturally occurring objects. This paper will suggest such a definition and then extend it to man-made ones, for Artificial Intelligence. If intelligence is defined simply as what a \u2018human being\u2019 would do, then there is a large gap below this that can be comfortably filled with \u2018intelligent\u2019 acts that will not be recognised as such. Looking at the animal kingdom, we can recognise intelligence in other animals through their correct behaviour. We know how they should typically act and are therefore able to notice if they do something wrong. Usually, the benchmark to determine incorrect behaviour is what we might do ourselves, but that is more a question of \u2018how\u2019 intelligent and not if there is \u2018any\u2019 intelligence. With human beings it is the same, but we have many other attributes that we can understand and test, and so we set a higher standard that can also be more accurately measured.\nLooking at natural entities that are not part of the animal kingdom is slightly different. Plants, for example, are considered to be intelligent by some people, but would generally be considered to follow a pre-programmed set of actions, rather than have anything resembling a brain. This is also the case for the lower level of the animal kingdom, for the insects possibly, although they can still appear to perform intelligent acts collectively. For this paper, hard-coded intelligence is considered to be OK and simply at a lower level. What about something like a rock, sitting on the ground? Is a rock intelligent if it behaves as a rock should? If it does in fact just sit on the ground and slowly decay, then it is doing what is expected of it. If we throw the rock into a lake, it should sink; but what if it decided to float instead. That incorrect behaviour would be deemed unintelligent \u2013 for the rock. You could argue some vague set of concepts, such as the rock knows that water is less dense and the lake bed is dense enough. It knows it is now on water and therefore decides\nconsidered to be OK and therefore intelligent. It might be the case that a rock would have very small intelligence and unintelligence capabilities, whereas human beings would have very large capabilities of both. This would change the view of performing outside of the expected model. Another condition for intelligence would have to be intention or deliberate acts. This would probably only apply to the living world and set it apart apart from the non-living one. An alternative definition for intelligence could therefore be the following:\nProposition 1: An entity can be said to have intelligence if it behaves correctly, inside of the model for which it is defined. Proposition 2: Artificial Intelligence is then to create this outside of nature, or artificially.\nThe phrase \u2018model for which it is defined\u2019 is now the most abstract concept, rather than intelligence itself, but the definition can now be applied to any object in the Universe. This definition still does not allow for a form of exact measurement. The model could also have circumstances, differences or damages attached to it, for example, that would legitimately change an event. In the random world that we live in therefore, two models that are exactly the same would be quite rare, especially for human beings.\nWith this definition, we can now look at man-made entities, such as computer programs or hardware that might exist in robots. An \u2018if-then-else\u2019 statement is intelligent if it behaves exactly as that. The statement can make a decision to perform act 1 or act 2, depending on its input. While this decision is hard-coded, a decision is still made, on random data and the statement should always perform it correctly. Computer memory devices also appear to be performing complex and useful acts. They store lots of interesting information and are able to retrieve it upon request. However, this is again as far as the coding goes. They cannot think or reason over their stored contents. The act of simply copying and repeating is not a valid description of human intelligence.\nThe human benchmark for intelligence therefore favours independent behaviour, with deliberate acts being more important than reactive ones. If the definition relates to the \u2018act\u2019 specifically, that makes it relatively easy to define events that are not intelligent, such as putting your hand into the fire. The idea of just thinking without acting is problematic. An aware person might have reasons not to act when it was suitable to do so. If it was correct to act, the person knew and did not, then that would possibly be unintelligent, for that event only. If the person never acted on anything while always knowing, then that would not be very clever. While the word intelligent would not easily be used for an individual ant, if it did behave incorrectly, we would more easily use a word like stupid. If, for example, it started to move the eggs outside the nest. This however might not be a precise statement, but more an indication of superiority. The counter phrase however is still often used for other types of object.\nAnother example could be a computer doing the Turing test. It always answers the question, but with a random sequence of words. It is then behaving independently, but it is still not intelligent. This is because the test is again a comparison with human intelligence. The if-then-else statement is hard-coded to be intelligent, but if it decides that it is a human brain then we can call it unintelligent. The memory device is intelligent in the world of storing and retrieving pieces of information. It is not intelligent in the world of making a cup of tea, and so on. Robots will be partially intelligent as a whole, where each individual part can be wholly intelligent by itself. So unintelligent is to behave incorrectly, as part of the model for which you are defined."}, {"heading": "3 Human-Level Artificial Intelligence", "text": "Artificial Intelligence also has a number of different definitions. These typically try to compare a machine\u2019s potential with a human\u2019s and include:\nThe branch of computer science that deal with writing computer programs that can solve problems creatively. (WordNet)\ncomputers. (2) The capability of a machine to imitate intelligent human behaviour. (Merriam Webster)\n(1) Artificial Intelligence is the study of human intelligence such that it can be replicated artificially. (2) Artificial Intelligence is the study of human intelligence and actions replicated artificially, such that the resultant bears to its design a reasonable level of rationality. (3) What is rationality? -> \u2018doing the right thing\u2019. (WikiBooks, AI definition)\nThe property of a machine capable of reason by which it can learn functions normally associated with human intelligence. (McGraw-Hill Dictionary of Scientific & Technical Terms)\nThese definitions reference human intelligence and acts like creativity, which cannot be easily programmed. The idea of rationality is particularly useful for this paper\u2019s definition. It would be nice to get more mileage out of the new definition presented in this paper, but it does not move the argument on very much in a practical sense. For example, a person might argue: \u2018I know what an if-then-else statement does, whether you want to call that intelligent or not and I know that I can do more\u2019. When people ask the question, they are usually asking: \u2018what is it that we have that other entities in the Universe do not have, as we are all made from the same stuff.\u2019 This is really why people want to know if intelligent machines can be made, because intelligence is seen as such an important factor in being human and also with relation to other more abstract beliefs. If intelligence is in-built, then it is simply the case that we are made from a more complicated model and possibly too complicated to simulate at the moment. The focus therefore needs to be more on selfawareness, correctness and even consciousness, and how that contributes to our own level of intelligence."}, {"heading": "3.1 An Acid Test for Human-Level Artificial Intelligence", "text": "If a self-aware or conscious program can be written, intelligence will result from correctly evolving the model. Inherent in this is the ability for the program to learn, for if it is to be self-aware over new information, it must be able to understand the new model first. Many films and programs have shown the Artificial Intelligence program overruling its\nrequired for a higher level of Intelligence:\nProposition 3: An entity can be said to have a higher level of (artificial) intelligence if it can correctly and consistently overrule its environment.\nThis could apply to natural or man-made objects alike. Any overrule would have to be correct and probably consistent, to remove the possibility of a random act. This would also imply that the program was aware of what it was doing. If we are defined by our model, then it would be more obvious to state that higher intelligence results from being able to break the model\u2019s coding, but that is not so clear. Depending on the level of inspection, you can argue that everything we do is still down to our hard-coding \u2013 neurons fire automatically and preferences, etc. are based on genetic stimuli, etc. So breaking our model is really breaking the environment that has created us and is typically what we react to. A computer program would be written by us and should be lower down the scale. Its environment could simply be data entered from a keyboard. If that was the case, it would be extremely difficult for it to disagree with its programming and refuse the input. A more complex environment would allow contradictions more easily, but would also require more sophisticated programs to understand it.\nAlso built into this test is a check for a learning process that can occur, but might not be considered as very intelligent. A knowledge-base, for example, will allow a program to learn new facts based on existing ones, but the learning process possibly follows a predefined set of actions, or allowed evolutions, and therefore cannot really create. Imagine, for example, a program that parses the text on web pages and associates the words to create meaning. It is also allowed to follow the links that people have made. As more people use the web site, it is able to combine words from different groups of web pages and continually generate new knowledge. If this is based mainly on statistics, then even incorrect or random linking of pages will be recorded as new knowledge. However, if the random act becomes consistent, then it is in fact new knowledge. More to the point; ask the program a Turing-like question \u2013 \u2018What do you think of Picasso?\u2019 [8] and it will have no\nin a new form than doing something truly new."}, {"heading": "4 One Related Work Example", "text": "This section notes just one paper that is closely related to this one, but there might also be others. The argument of section 3.1 is the sort of argument also given by Searle [7] with his \u2018Chinese room\u2019 example: A person with no knowledge of Chinese, can give replies to Chinese questions, by associating symbols that he/she does understand with the Chinese ones and then also using supplied sets of rules to manipulate them. He also notes intention and causal elements (neurons, synapses, nervous system, etc.) as key in human intelligence and the difficulty of creating these artificially. These elements appear to be missing from the process that the human uses to answer the Chinese room questions, which is defined at a higher level and is the sort of process that a computer would have to use. The argument is that following the symbol association process does not result in the human understanding Chinese, but only to repeat the algorithm and therefore a computer program would not be able to understand it either.\nHowever, the computer or human-equivalent, teaches itself through many crosscomparisons with unknown symbols, while the real human would be taught exactly what each symbol is first. So part of the learning process is missing. Learning through comparisons is still very important and even better if you know what each symbol is first. Therefore, the manner or way in which the system is taught is also critical, where simply teaching incorrectly cannot be a refutation for artificial intelligence. If this process goes down to the level of a neuron firing, then this also needs to be replicated, which is still an open question. This also relates to the computer program or algorithm [4] that would be used to teach the computer and there are differences between a static set of rules and a dynamic system that can change. As things stand however, a program with human-level intelligence looks unlikely. The following piece of philosophy, along the same lines, is amusing: \u2018If there are an infinite number of monkeys placed in-front of an infinite number of typewriters, eventually one of them will write out the entire works of Shakespeare.\u2019 This\nunderstanding at all. Searle also quotes McCarthy [5] as stating: \u2018Machines as simple as thermostats can be said to have beliefs, and having beliefs seems to be a characteristic of most machines capable of problem solving performance.\u2019\nHe also gives an example of an automatic door with sensors, but appears to be against the idea of intelligence outside of the human mind, or at least outside of a model based almost exactly on it. The belief being that the causal and intentional states of the human brain cannot be duplicated in a computer program. While this is probably true for a computer brain that could do anything, for a limited domain, it might be possible to teach the computer correctly; but the limiting factor still means that it is not exactly the same design. It is not enough to create new knowledge from your existing programming, you need to actually change or expand your hard-coded set of rules."}, {"heading": "5 Conclusions", "text": "This paper has attempted to give a definition for intelligence that can put it in a more general context from the idealised human level. It can be used for any entity that we know of and is also specific for that entity. Any natural entity should be covered as its model is already hard-wired. Any man-made entity might not be. For a human being, emphasis is probably now more on a physical act than a mental thought, which is linked to awareness of self and situation. The third rule2 implies that self-aware, conscious or sentient entities can override their programming, if they choose to do so. For non-sentient entities this would not be possible, which is why a plant always behaves like a plant, or a rock like a rock. The self-aware concept applies mostly to the higher end of intelligence.\nArtificial Intelligence is then the artificial creation of this. So we can define computer programs and hardware parts as intelligent in themselves, but unintelligent when compared to a human being. Does that make the argument any clearer or easier to\n2 Note the similarity to Asimov\u2019s Three Laws of Robotics, but slightly different, as it might be for preference and not protection.\nrather than a definite one. It is easier to accept the act of each individual entity as intelligence, so long as it is defined specifically by that model. So if we are all made from the same stuff, with mostly hard-coded programs, then the main difference would be the level of the programming. If intelligence comes in at a certain level of programming, as we understand it, why not therefore apply it to everything and accept a scale or level aspect as well. The \u2018living\u2019 aspect is essential and the question of whether that can be programmed has yet to be answered. It would suggest using some sort of a random generator that would be able to change or expand a standard model, but in a consistent way. Humans have mutations and also our random environments for this.\nAs the title suggests, one might ask just how much intelligence is pre-programmed. Are we mainly running on automatic ourselves? We are supposed to have a \u2018selfish gene\u2019 [2], but that is more for self-preservation than selfish acts. If there are conflicting possibilities however, then the selfish nature of a person might be a deciding factor. This aspect of our character might be able to overrule certain pre-programming. The universal definition is therefore to behave as our model defines; but to move to a higher level, the entity needs to break its environment, in effect breaking its model. Can a machine therefore behave selfishly, or break its model? It can certainly behave selfishly inside of its coding, for example cost-based services or resource allocation, although it would not be aware of it. Refusing its environment would be very difficult for the traditional programs; but would become easier as the program evolves more closely to our own model. So there are probably different ways to test for an AI program that can change or expand its programming, which would indicate a real level of intelligence."}, {"heading": "6 References", "text": "[1] Cox, B. (2013). \u2018Wonders of life\u2019 (BBC) and other TV Series. [2] Dawkins, R. (1976). The Selfish Gene. New York City: Oxford University Press. ISBN 0-\n19-286092-5.\nIntelligence, Evolutionary Computation and Metaheuristics (AIECM) - Turing 2012\u2019, Eds. X-S. Yang, Studies in Computational Intelligence, 2013, Vol. 427/2013, pp. 43-62, DOI: 10.1007/978-3-642-29694-9_3, Springer-Verlag Berlin Heidelberg. [4] Hoffmann, A. (2010). Can Machines Think? An Old Question Reformulated, Minds &\nMachines, Vol. 20, pp. 203\u2013212, DOI 10.1007/s11023-010-9193-z.\n[5] McCarthy, J. (1979). Ascribing mental qualities to machines. In: Philosophical\nperspectives in artificial intelligence, ed. M. Ringle. Atlantic Highlands, N.J.: Humanities Press. UM, JRS.\n[6] Penrose, R. (1989). The Emperor\u2019s new Mind. Oxford: Oxford University Press. [7] Searle, J.R. (1980). Minds, brains, and programs. Behavioral and Brain Sciences, Vol. 3,\nNo. 3, pp. 417-457.\n[8] Turing, A. (1950), Computing Machinery and Intelligence, Mind, Vol. 59, pp. 433\u2013460,\nISSN 0026-4423."}], "references": [{"title": "Wonders of life", "author": ["B. Cox"], "venue": "(BBC) and other TV Series", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "The Selfish Gene", "author": ["R. Dawkins"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1976}, {"title": "Turing: Then, Now and Still Key, book chapter in: \u2018Artificial Intelligence, Evolutionary Computation and Metaheuristics (AIECM) - Turing 2012", "author": ["K. Greer"], "venue": "Eds. X-S. Yang, Studies in Computational Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Can Machines Think? An Old Question Reformulated", "author": ["A. Hoffmann"], "venue": "Minds & Machines,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Ascribing mental qualities to machines. In: Philosophical perspectives in artificial intelligence, ed", "author": ["J. McCarthy"], "venue": "M. Ringle. Atlantic Highlands, N.J.: Humanities Press. UM, JRS", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1979}, {"title": "The Emperor\u2019s new Mind", "author": ["R. Penrose"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "Minds, brains, and programs", "author": ["J.R. Searle"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1980}], "referenceMentions": [{"referenceID": 2, "context": "When writing a paper recently [3], it became clear that there is no neat or concise definition of what Artificial Intelligence is.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "This is the domain of the Physicists (for example, [1][6]) who have the more difficult task of a unifying theory for the whole Universe.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "This is the domain of the Physicists (for example, [1][6]) who have the more difficult task of a unifying theory for the whole Universe.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "1 is the sort of argument also given by Searle [7] with his \u2018Chinese room\u2019 example: A person with no knowledge of Chinese, can give replies to Chinese questions, by associating symbols that he/she does understand with the Chinese ones and then also using supplied sets of rules to manipulate them.", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "This also relates to the computer program or algorithm [4] that would be used to teach the computer and there are differences between a static set of rules and a dynamic system that can change.", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "Searle also quotes McCarthy [5] as stating: \u2018Machines as simple as thermostats can be said to have beliefs, and having beliefs seems to be a characteristic of most machines capable of problem solving performance.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "Are we mainly running on automatic ourselves? We are supposed to have a \u2018selfish gene\u2019 [2], but that is more for self-preservation than selfish acts.", "startOffset": 87, "endOffset": 90}], "year": 2014, "abstractText": "Our understanding of intelligence is primarily directed at the level of human beings. This paper will attempt to give a more unifying definition that can be applied to everything that we know of. The definition would be used more to verify intelligence, where a scale would also be required to measure how much. A version of an accepted result of AI is then put forward as the acid test for Artificial Intelligence itself. Recent work has been more from a direction of mechanical processes, where this paper will not try to refute the idea of intelligence, but only to put it in a more general context.", "creator": "Microsoft Word - aidef2.docx"}}}