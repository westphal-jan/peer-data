{"id": "1403.1618", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2014", "title": "Design a Persian Automated Plagiarism Detector (AMZPPD)", "abstract": "till currently there are lots of plagiarism detection approaches. arguably but few of them implemented and adapted for persian languages. in this paper, our work on designing and implementation of a plagiarism detection system based practically on pre - processing and without nlp, technics will be described. and the results of testing on capturing a corpus will be presented.", "histories": [["v1", "Thu, 6 Mar 2014 22:57:29 GMT  (141kb)", "http://arxiv.org/abs/1403.1618v1", "3 pages, Published with International Journal of Engineering Trends and Technology (IJETT)Published with International Journal of Engineering Trends and Technology (IJETT)"]], "COMMENTS": "3 pages, Published with International Journal of Engineering Trends and Technology (IJETT)Published with International Journal of Engineering Trends and Technology (IJETT)", "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["maryam mahmoodi", "mohammad mahmoodi varnamkhasti"], "accepted": false, "id": "1403.1618"}, "pdf": {"name": "1403.1618.pdf", "metadata": {"source": "CRF", "title": "Design a Persian Automated Plagiarism Detector (AMZPPD)", "authors": ["Maryam Mahmoodi", "Mohammad Mahmoodi Varnamkhasti"], "emails": [], "sections": [{"heading": null, "text": "ISSN: 2231-5381 http://www.ijettjournal.org Page 465\nI. INTRODUCTION Todays, internet made sharing documents and information very easy. And consequently written-text plagiarism is now a real problem for most academic environments.\nMany methods and approaches have been developed for automatic detection of plagiarism mostly for English language. But for Persian language there are few works. So in this work the aim is to implement an accurate plagiarism detection system with the purpose of short paragraphs.\nII. ARTICHETURE OF METHODOLOGY In this section we described our system methodology that detects how similar are two suspicious texts. In [1], a multi\u2013stage new framework introduced that is adapted to Persian language in our work and used as the main Idea:"}, {"heading": "A. Pre-Processing Stage", "text": "In this stage some pre-processing techniques are used to extract single words from the structured text and removal of unnecessary things that make it difficult to recognize similarities. In section III, techniques and their techniques and the importance of each is described in Persian."}, {"heading": "B. Similarity comparison Stage", "text": "In this stage one of these comparison methodologies is used to compare two sequences of words were produced in the previous stage:\n2-gram similarity measures (using Jaccard or Clough & Stevenson used similarity metric)\n3-gram similarity measures(using Jaccard or Clough & Stevenson used similarity metric)\nLongest common subsequence\nC. Verdict stage Using numbers generated in pervious stages one of these verdicts is derived: Clean (Non-plagiarism): based on participants own knowledge as the original texts were not given. Heavy revision: rewriting of the original by paraphrasing and restructuring. Light revision: minor alteration of the original text by substituting words with synonyms and performing some grammatical changes.\nNear copy: copy-and-paste from the original text. [1] This may be obtained by single-criteria or multi-criteria\nanalysis.\nIII. PRE-PROSECCING"}, {"heading": "A. Normalizing", "text": "A Persian Normalizer removes extra spaces, organizes virtual spaces (for example correct \u201c\u0645\u0648\u0631 \u06cc\u0645\u201d to \u201c \u06cc\u0645 \u0645\u0648\u0631 \u201d), fix the problem of \u201c\u064a\u201d and \u201c\u06cc\u201d. This pre-processing technic is very important in Persian because most texts do not adhere to correct orthography. And it may lead to difficulty in detecting the similarity."}, {"heading": "B. Stop-word Removal", "text": "This technique removes common words (articles, prepositions, determiners,\u2026) such as \u201c\u06be\u06a9\u201d,\u201d\u06be\u0628\u201d,\u201d\u0632\u0627\u201d. These words sometimes have very little influence in meaning but sometimes play somewhat important role in the text.\nTwo kind of deep and shallow stop-word-removal tables tested to find the true impact of this technic."}, {"heading": "C. Sentence segmentation", "text": "Split text in the document into sentences and thereby allowing line-by-line processing in the subsequent tests [1]."}, {"heading": "D. Tokenization", "text": "Token (words, punctuation symbols, etc.) boundaries in sentences[1].\nISSN: 2231-5381 http://www.ijettjournal.org Page 466"}, {"heading": "E. Stemming", "text": "Stemming is the process of removing and replacing word suffixes to arrive at a common root form of the word. Sometimes people change the form of a word to hide their plagiarism.\nSuppose these two sentences: \u201c\u062f\u0646\u06be\u062f \u0645\u0627\u062c\u0646\u0627 \u0627\u0631 \u0644\u0627\u0646\u06af\u06cc\u0633 \u0634\u0632\u0627\u062f\u0631\u067e \u062f\u0646\u0646\u0627\u0648\u062a \u06cc\u0645 \u0627\u06be \u0647\u062f\u0646\u0632\u0627\u062f\u0631\u067e \u0646\u06cc\u0627\u201d And \u201c\u062f\u0646\u0631\u0627\u062f \u0627\u0631 \u0644\u0627\u0646\u06af\u06cc\u0633 \u0634\u0632\u0627\u062f\u0631\u067e \u0646\u0627\u0648\u062a \u0647\u062f\u0646\u0632\u0627\u062f\u0631\u067e \u0639\u0648\u0646 \u0646\u06cc\u0627\u201d In this case the similarity checker will make mistake if\nwords don\u2019t change into stems."}, {"heading": "F. Lemmatization", "text": "Transform words into their dictionary base forms in order to generalize the comparison analysis [1]. Sometimes lemmatization is mistaken for stemming. However, there is an essential difference. Stemming operates only with single words without any knowledge of the context, and therefore cannot distinguish among words having several different meanings [2]."}, {"heading": "G. Number Replacement", "text": "This one replaces any number with a dummy character. (\u201c#\u201d for example) The reason for doing this is that in some scientific reports, dishonest person can just change the numbers carefully to cheat the system."}, {"heading": "H. Synonymy Recognition", "text": "The motivation for using synonymy recognition comes from considering human behaviour, whereby people may seek to hide plagiarism by replacing words with appropriate synonyms [2].\nIn Persian Language synonymy recognition is very important because any word have many synonyms. And sometimes synonyms are the foreign equivalent of the words in Persian script. For example \u201c\u062f\u0631\u0648\u0628\u06cc\u06a9\u201d can be used instead of \u201c\u062f\u06cc\u0644\u06a9 \u06be\u062d\u0641\u0635\u201d In order to mislead the plagiarism detector."}, {"heading": "I. Part-of-Speech tagging", "text": "Assign grammatical tags to each word, such as \u201cnoun\u201d, \u201dverb\u201d, etc., for detecting cases where words have been replaced, but the style in terms of grammatical categories remains similar[1].\nIV. SIMILARITY COMPARISON With the end of the first stage, we have two sequences of words. One of the original text and one of the suspicious text. These sequences where cleaned by some of the techniques mentioned above and are ready to check by one of these similarity comparison methods:"}, {"heading": "A. N-grams + Jaccard similarity coefficient", "text": "If S(A) is the set of n-grams of original text and S(A) is the set of n-grams of suspicious text. The Jaccard similarity coefficient is defined as:"}, {"heading": "B. N-grams + Clough & Stevenson metric", "text": "In similar conditions with part A Clough & Stevenson metric defined as [1, 3]:"}, {"heading": "C. LCS", "text": "The longest sequence of words included in both original and suspicious documents may be used as the criteria of similarity.\nV. TEST COMBINATIONS In III and IV some candidates for pre-processing and similarity comparison were introduced. To identify the best possible combination, we tested most possible ones in table1 and table2 are the information of top10 combiantions.\nFor testing different combinations we first developed a Persian Corpus just like [3] that is available at:\nhttp://amzmohammad.com/AMZPPD/CPPD.tar\nTABLE I AVERAGE RESULTS OF SAMPLE COMBINATIONS\nC om\nbiantio n\nAverage similarity score Clean Heavy\nRevision Light Revision Copy\n1 0.0019 0.003 0.039 0.198 2 0.004 0.016 0.112 0.294 3 0.002 0.003 0.038 0.223 4 0.021 0.037 0.124 0.314 5 0.0043 0.007 0.075 0.248 6 0.0022 0.035 0.199 0.403 7 0.0044 0.017 0.077 0.302 8 0.0052 0.070 0.224 0.438 9 0.207 0.230 0.245 0.267 10 0.18 0.187 0.191 0.219\nISSN: 2231-5381 http://www.ijettjournal.org Page 467"}, {"heading": "9 Y D Y Y N Y Y Y N LCS", "text": ""}, {"heading": "10 Y D Y Y Y Y Y Y Y LCS", "text": "These ten combinations are selected because had meaningful difference in values obtained in the different groups.\nVI. CHOOSE THE BEST COMBINATION The criterion is coefficient of dispersion. That is defined as\nthe ratio of the variance to the mean in each group (verdict),\nVII. CONCLUSIONS In this work we had shown that the influence of NLP technics and pre-proceedings on Persian plagiarism detection\naccuracy is Significant. But because of orthography problems and maybe the ambiguities of language this influence is less than English.\nThe result of this work is now implemented using python, NLTK and HAZM library and is under testing and development in some academic environments.\nUnder the project name AMZPPD: http://amzmohammad.com/AMZPPD/\nREFERENCES [1] Chong, M., Specia, L. & Mitkov, R, Using Natural Language\nProcessing for Automatic Detection of Plagiarism, In: Proceedings of the 4th International Plagiarism Conference, Newcastle-upon-Tyne, UK, 2010.\n[2] Ceska, Zdenek and Fox, Chris ,The Influence of Text Pre-processing on Plagiarism Detection. In: Angelova, Galia andBontcheva, Kalina and Mitkov, Ruslan and Nicolov, Nicolas and Nikolov, Nikolai, (eds.) International Conference on Recent Advances in Natural Language Processing 2009. Association for Computational Linguistics, pp. 55-59, 2011. [3] Clough, P., & Stevenson, M, Developing a corpus of plagiarized short answers. Language Resources and Evaluation, LRE 2009. [4] Dreher, H. Automatic Conceptual Analysis for Plagiarism Detection. In Issues in Informing Science and Information Technology, 4, (pp. 1-14), 2007. [5] Z. Ceska. Plagiarism Detection Based on Singular Value Decomposition. Advances in Natural Language Processing, 5221: 108\u2013 119, August 2008. [6] Manning, C., & Schutze, H, Foundations of statistical natural language processing. Reading: MIT Press,1990. [7] Aiken, A,. MOSS: A System for Detecting Software Plagiarism. Stanford University,1994.\nA good combination has a little coefficient of dispersion in all verdicts. Because if coefficient of dispersion is a big number we cannot determine accurate intervals of verdicts and they may have overlap. As the resultant testing combinations on corpus, the four combinations 5,6,8,9 have better coefficient of dispersion and are our chosen combinations."}], "references": [{"title": "Using Natural Language Processing for Automatic Detection of Plagiarism", "author": ["M. Chong", "L. Specia", "R Mitkov"], "venue": "Proceedings of the 4th International Plagiarism Conference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Influence of Text Pre-processing on Plagiarism Detection", "author": ["Ceska", "Zdenek", "Fox", "Chris", "The"], "venue": "(eds.) International Conference on Recent Advances in Natural Language Processing", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Developing a corpus of plagiarized short answers", "author": ["P. Clough", "M Stevenson"], "venue": "Language Resources and Evaluation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Automatic Conceptual Analysis for Plagiarism Detection", "author": ["H. Dreher"], "venue": "In Issues in Informing Science and Information Technology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Plagiarism Detection Based on Singular Value Decomposition", "author": ["Z. Ceska"], "venue": "Advances in Natural Language Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Foundations of statistical natural language processing. Reading: MIT Press,1990", "author": ["C. Manning", "H Schutze"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "MOSS: A System for Detecting Software Plagiarism", "author": ["A Aiken"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}], "referenceMentions": [{"referenceID": 0, "context": "In [1], a multi\u2013stage new framework introduced that is adapted to Persian language in our work and used as the main Idea:", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "[1]", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Split text in the document into sentences and thereby allowing line-by-line processing in the subsequent tests [1].", "startOffset": 111, "endOffset": 114}, {"referenceID": 0, "context": ") boundaries in sentences[1].", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "Transform words into their dictionary base forms in order to generalize the comparison analysis [1].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "Stemming operates only with single words without any knowledge of the context, and therefore cannot distinguish among words having several different meanings [2].", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "The motivation for using synonymy recognition comes from considering human behaviour, whereby people may seek to hide plagiarism by replacing words with appropriate synonyms [2].", "startOffset": 174, "endOffset": 177}, {"referenceID": 0, "context": ", for detecting cases where words have been replaced, but the style in terms of grammatical categories remains similar[1].", "startOffset": 118, "endOffset": 121}, {"referenceID": 0, "context": "N-grams + Clough & Stevenson metric In similar conditions with part A Clough & Stevenson metric defined as [1, 3]:", "startOffset": 107, "endOffset": 113}, {"referenceID": 2, "context": "N-grams + Clough & Stevenson metric In similar conditions with part A Clough & Stevenson metric defined as [1, 3]:", "startOffset": 107, "endOffset": 113}, {"referenceID": 2, "context": "For testing different combinations we first developed a Persian Corpus just like [3] that is available at: http://amzmohammad.", "startOffset": 81, "endOffset": 84}], "year": 2014, "abstractText": "Currently there are lots of plagiarism detection approaches. But few of them implemented and adapted for Persian languages. In this paper, our work on designing and implementation of a plagiarism detection system based on preprocessing and NLP technics will be described. And the results of testing on a corpus will be presented. Keywords\u2014 External Plagiarism, Plagiarism, Copy detection, natural language processing, Artificial intelligence , Persian language.", "creator": null}}}