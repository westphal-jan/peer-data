{"id": "1404.5165", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2014", "title": "GP-Localize: Persistent Mobile Robot Localization Using Online Sparse Gaussian Process Observation Model", "abstract": "central to robot exploration and mapping is the task of persistent localization in environmental memory fields characterized by spatially highly correlated measurements. this paper presents a gaussian process localization ( gp - localize ) algorithm hypothesis that, in abrupt contrast to existing works, can exploit the spatially correlated field measurements taken during experiencing a robot'r s exploration ( instead of relying on prior automated training data ) for efficiently and scalably implemented learning the gp observation model online through our proposed emerging novel online sparse gp. thus as a result, spatial gp - localize is capable of achieving constant time and memory ( i. e., independent of the size bounds of the data ) per filtering step, computation which demonstrates the practical feasibility of using gps agents for persistent indigenous robot localization and autonomy. empirical evaluation via simulated experiments with real - world datasets and a real quantum robot experiment shows that gp - localize outperforms existing gp localization research algorithms.", "histories": [["v1", "Mon, 21 Apr 2014 10:28:00 GMT  (345kb,D)", "https://arxiv.org/abs/1404.5165v1", "28th AAAI Conference on Artificial Intelligence (AAAI 2014), Extended version with proofs, 10 pages"], ["v2", "Tue, 22 Apr 2014 08:03:33 GMT  (345kb,D)", "http://arxiv.org/abs/1404.5165v2", "28th AAAI Conference on Artificial Intelligence (AAAI 2014), Extended version with proofs, 10 pages"]], "COMMENTS": "28th AAAI Conference on Artificial Intelligence (AAAI 2014), Extended version with proofs, 10 pages", "reviews": [], "SUBJECTS": "cs.RO cs.LG stat.ML", "authors": ["nuo xu", "kian hsiang low", "jie chen", "keng kiat lim", "etkin baris ozgul"], "accepted": true, "id": "1404.5165"}, "pdf": {"name": "1404.5165.pdf", "metadata": {"source": "META", "title": "GP-Localize: Persistent Mobile Robot Localization using Online Sparse Gaussian Process Observation Model", "authors": ["Nuo Xu", "Kian Hsiang Low", "Jie Chen", "Keng Kiat Lim", "Bari\u015f \u00d6zg\u00fcl"], "emails": ["ebozgul}@comp.nus.edu.sg,", "chenjie@smart.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Recent research in robot exploration and mapping has focused on developing adaptive sampling and active sensing algorithms (Cao, Low, and Dolan 2013; Chen, Low, and Tan 2013; Chen et al. 2012; Hoang et al. 2014; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2007; 2012; Ouyang et al. 2014) to gather the most informative data/observations for modeling and predicting spatially varying environmental fields that are characterized by continuous-valued, spatially correlated measurements. Application domains (e.g., environmental sensing and monitoring) requiring such algorithms often contain multiple fields of interest: (a) Autonomous underwater and surface vehicles are tasked to sample ocean and freshwater phenomena including temperature, salinity, and oxygen concentration fields (Dolan et al. 2009; Podnar et al. 2010), (b) indoor environments are spanned by temperature, light, and carbon dioxide concentration fields that affect the occupants\u2019 comfort and satisfaction towards the environmental quality across different areas, and (c) WiFi access points/hotspots situated at neighboring locations produce different but overlapping wireless signal strength fields over the same environment. These algorithms operate with an assumption that the locations of every robot and its gathered observations are known and provided by its onboard sensors such as the widely-used GPS\nCopyright c\u00a9 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ndevice. However, GPS signals may be noisy (e.g., due to urban canyon effect between tall buildings) or unavailable (e.g., in underwater or indoor environments). So, it is desirable to alternatively consider exploiting the spatially correlated measurements taken by each robot for localizing itself within the environmental fields during its exploration; this will significantly extend the range of environments and application domains in which a robot can localize itself.\nTo achieve this, our robotics community will usually make use of a probabilistic state estimation framework known as the Bayes filter: It repeatedly updates the belief of a robot\u2019s location/state by assimilating the field measurements taken during the robot\u2019s exploration through its observation model. To preserve time efficiency, the Bayes filter imposes a Markov property on the observation model: Given the robot\u2019s current location, its current measurement is conditionally independent of the past measurements. Such a Markov property is severely violated by the spatial correlation structure of the environmental fields, thus strongly degrading the robot\u2019s localization performance. To resolve this issue, the works of Ko and Fox (2009a; 2009b) have integrated a rich class of Bayesian nonparametric models called the Gaussian process (GP) into the Bayes filter, which allows the spatial correlation structure between measurements to be formally characterized (i.e., by modeling each field as a GP) and the observation model to be represented by fully probabilistic predictive distributions (i.e., one per field/GP) with formal measures of the uncertainty of the predictions.\nUnfortunately, such expressive power of a GP comes at a high computational cost, which hinders its practical use in the Bayes filter for persistent robot localization: It incurs cubic time and quadratic memory in the size of the data/observations. Existing works (Brooks, Makarenko, and Upcroft 2008; Ferris, Ha\u0308hnel, and Fox 2006; Ferris, Fox, and Lawrence 2007; Ko and Fox 2009a; 2009b) have sidestepped this computational difficulty by assuming the availability of data/observations prior to exploration and localization for training the GP observation model offline; some (Brooks, Makarenko, and Upcroft 2008; Ferris, Ha\u0308hnel, and Fox 2006; Ko and Fox 2009a) have assumed these given prior measurements to be labeled with known locations while others (Ferris, Fox, and Lawrence 2007; Ko and Fox 2009b) have inferred their location labels. The Markov assumption on the observation model can then be ar X\niv :1\n40 4.\n51 65\nv2 [\ncs .R\nO ]\n2 2\nA pr\n2 01\n4\n\u201crelaxed\u201d to conditional independence between the robot\u2019s current measurement and past measurements (i.e., taken during its exploration) given its current location and the trained GPs using prior data/observations, thus improving the efficiency at each filtering step during its exploration to quadratic time in the size of the prior training data. Any measurement taken during the robot\u2019s actual exploration and localization is thus not used to train the GP observation model. Such a \u201crelaxed\u201d Markov assumption may hold in certain static environments. However, it becomes highly restrictive and is easily violated in general, practical environmental settings where, for example, (a) limited sampling budget (i.e., in terms of energy consumption, mission time, etc.) forbids the collection of prior training data or only permits extremely sparse prior data to be gathered relative to a large environment, thus resulting in an inaccurately trained GP observation model, (b) environmental changes invalidate the prior training data, and (c) the robot\u2019s actual exploration path is spatially distant from the prior observations, hence making the trained GP observation model uninformative to its localization. All these practical considerations motivate us to tackle a fundamental research question: Without prior training data, how can GPs be restructured to be used by a Bayes filter for persistent robot localization in environmental fields characterized by spatially correlated measurements?\nThis paper presents a Gaussian process localization (GPLocalize) algorithm that, in contrast to existing works mentioned above, can exploit the spatially correlated field measurements taken during a robot\u2019s exploration (instead of relying on prior training data) for efficiently and scalably learning the GP observation model online through our proposed novel online sparse GP (Section 3). As a result, GPLocalize is capable of achieving constant time and memory (i.e., independent of the size of the data/observations) per filtering step, which we believe is an important first step towards demonstrating the practical feasibility of employing GPs for persistent robot localization and autonomy. We empirically demonstrate through simulated experiments with three real-world datasets as well as a real robot experiment that GP-Localize outperforms existing GP localization algorithms (Section 4). 2 Background"}, {"heading": "2.1 Modeling Environmental Field with GP", "text": "The Gaussian process (GP) can be used to model an environmental field as follows1: The environmental field is defined to vary as a realization of a GP. Let X be a set of locations representing the domain of the environmental field such that each location x \u2208 X is associated with a realized (random) field measurement zx(Zx) if x is observed (unobserved). Let {Zx}x\u2208X denote a GP, that is, every finite subset of {Zx}x\u2208X has a multivariate Gaussian distribution (Rasmussen and Williams 2006). The GP is fully specified by its prior mean \u00b5x , E[Zx] and covariance \u03c3xx\u2032 , cov[Zx, Zx\u2032 ] for all x, x\u2032 \u2208 X , the latter of which characterizes the spatial correlation structure of the field\n1To simplify exposition, we only describe the GP for a single field; for multiple fields, we assume independence between them to ease computations.\nand can be defined using a covariance function. A common choice is the squared exponential covariance function \u03c3xx\u2032 , \u03c32s exp{\u22120.5(x\u2212x\u2032)>M\u22122(x\u2212x\u2032)+\u03c32n\u03b4xx\u2032}where \u03c32s and \u03c3 2 n are, respectively, the signal and noise variance controlling the intensity and the noise of the measurements, M is a diagonal matrix with length-scale components `1 and `2 controlling, respectively, the degree of spatial correlation or \u201csimilarity\u201d between measurements in the horizontal and vertical directions of the field, and \u03b4xx\u2032 is a Kronecker delta of value 1 if x = x\u2032, and 0 otherwise.\nA chief advantage of using the full GP to model the environmental field is its capability of performing probabilistic regression: Supposing a robot has visited and observed a set D of locations and taken a column vector zD of corresponding realized measurements, the full GP can exploit these observations to predict the measurement at any unobserved location x \u2208 X \\ D as well as provide its corresponding predictive uncertainty using a Gaussian predictive distribution p(zx|x,D, zD) = N (\u00b5x|D, \u03c3xx|D) with the following posterior mean and variance, respectively:\n\u00b5x|D , \u00b5x + \u03a3xD\u03a3 \u22121 DD (zD \u2212 \u00b5D) (1)\n\u03c3xx|D , \u03c3xx \u2212 \u03a3xD\u03a3\u22121DD\u03a3Dx (2) where \u00b5D is a column vector with mean components \u00b5x\u2032 for all x\u2032 \u2208 D, \u03a3xD is a row vector with covariance components \u03c3xx\u2032 for all x\u2032 \u2208 D, \u03a3Dx is the transpose of \u03a3xD, and \u03a3DD is a matrix with components \u03c3x\u2032x\u2032\u2032 for all x\u2032, x\u2032\u2032 \u2208 D."}, {"heading": "2.2 Sparse Gaussian Process Approximation", "text": "The key limitation hindering the practical use of the full GP in the Bayes filter for persistent robot localization is its poor scalability in the size |D| of the data/observations: Computing the Gaussian predictive distribution (i.e., (1) and (2)) requires inverting the covariance matrix \u03a3DD, which incurs O(|D|3) time and O(|D|2) memory. To improve its scalability, GP approximation methods (Chen et al. 2012; 2013; Quin\u0303onero-Candela and Rasmussen 2005) have been proposed, two of which will be described below.\nThe simple sparse subset of data (SoD) approximation method uses only a subset S of the set D of locations (i.e., S \u2282 D) observed and the realized measurements zS taken by the robot to produce a Gaussian predictive distribution of the measurement at any unobserved location x \u2208 X \\D with the following posterior mean and variance, which are similar to that of full GP (i.e., by replacing D in (1) and (2) with S):\n\u00b5x|S = \u00b5x + \u03a3xS\u03a3 \u22121 SS(zS \u2212 \u00b5S) (3)\n\u03c3xx|S = \u03c3xx \u2212 \u03a3xS\u03a3\u22121SS\u03a3Sx . (4) The covariance matrix \u03a3SS is inverted using O(|S|3) time and O(|S|2) memory, which are independent of |D|. The main criticism of SoD is that it does not exploit all the data for computing the Gaussian predictive distribution, thus yielding an unrealistic overestimate (4) of the predictive uncertainty (even with fairly redundant data and informative subset S) (Quin\u0303onero-Candela and Rasmussen 2005) and in turn an inaccurately trained observation model.\nThe sparse partially independent training conditional (PITC) approximation method is the most general form of a class of reduced-rank covariance matrix approximation\nmethods reported in (Quin\u0303onero-Candela and Rasmussen 2005) exploiting the notion of a support set S \u2282 X . Unlike SoD, PITC can utilize all data (i.e., D and zD) to derive a Gaussian predictive distribution of the measurement at any x \u2208 X \\ D with the following posterior mean and variance:\n\u00b5PITCx|D , \u00b5x + \u0393xD(\u0393DD + \u039b) \u22121(zD \u2212 \u00b5D) (5)\n\u03c3PITCxx|D , \u03c3xx \u2212 \u0393xD(\u0393DD + \u039b) \u22121\u0393Dx (6)\nwhere \u0393AA\u2032 = \u03a3AS\u03a3\u22121SS\u03a3SA\u2032 for all A,A\u2032 \u2282 X and \u039b is a block-diagonal matrix constructed from the N diagonal blocks of \u03a3DD|S , each of which is a matrix \u03a3DnDn|S for n = 1, \u00b7 \u00b7 \u00b7 , N where D = \u22c3N n=1Dn. Also, unlike SoD, the support set S does not have to be observed. The covariance matrix \u03a3DD in (1) and (2) is approximated by a reducedrank matrix \u0393DD summed with the resulting sparsified residual matrix \u039b in (5) and (6). So, computing either \u00b5PITCx|D (5) or \u03c3PITCxx|D (6), which requires inverting the approximated covariance matrix \u0393DD+\u039b, incursO(|D|(|S|2+(|D|/N)2)) time andO(|S|2 + (|D|/N)2) memory. The sparse fully independent training conditional (FITC) approximation method is a special case of PITC where \u039b is a diagonal matrix constructed from \u03c3x\u2032x\u2032|S for all x\u2032 \u2208 D (i.e., N = |D|). FITC is previously employed by Ko and Fox (2009a) to speed up the learning of observation model with prior training data. But, the time incurred by PITC or FITC grows with increasing size of data. So, it is computationally impractical to use them directly to repeatedly train the observation model at each filtering step for persistent localization."}, {"heading": "2.3 Bayes Filters", "text": "A Bayes filter is a probabilistic state estimation framework that repeatedly updates the belief of a robot\u2019s location/state by conditioning on its control actions performed and field measurements taken so far. Formally, let the robot\u2019s control action performed, its location visited and observed, and the corresponding realized field measurement taken at time/filtering step t be denoted by ut, xt, and zt2, respectively. To estimate the robot\u2019s location, a belief b(xt) , p(xt|u1:t, z1:t) is maintained over all its possible locations xt where u1:t , (u1, . . . , ut)> and z1:t , (z1, . . . , zt)> denote, respectively, column vectors of past control actions performed and realized field measurements taken by the robot up until time step t. To track such a belief, after the robot has performed an action ut and taken a realized measurement zt at each time step t, the Bayes filter updates the prior belief b(xt\u22121) of the robot\u2019s location to the posterior belief b(xt) = \u03b2p(zt|xt) \u222b p(xt|ut, xt\u22121)b(xt\u22121)dxt\u22121 where 1/\u03b2 is a normalizing constant, p(xt|ut, xt\u22121) is a motion model representing the probability of the robot moving from locations xt\u22121 to xt after performing action ut, and p(zt|xt) is an observation model describing the likelihood of taking realized measurement zt at location xt.\nTo preserve efficiency, the Bayes filter imposes a Markov property on the observation model: Given the robot\u2019s current location xt, its current measurement zt is conditionally\n2The field measurement zt is indexed by time step t instead of the corresponding location xt since xt is not known to the robot.\nindependent of past actions u1:t and measurements z1:t\u22121: p(zt|xt, u1:t, z1:t\u22121) = p(zt|xt) . (7)\nIn other words, the robot\u2019s past actions performed and measurements taken during its exploration and localization are not exploited for learning the observation model. This is conventionally assumed by existing works either representing the observation model using a parametric model with known parameters (Thrun, Burgard, and Fox 2005) or training it offline using prior training data. The disadvantages of the former are extensively discussed by Ko and Fox (2009a) while that of the latter are already detailed in Section 1.\nIn the case of multiple fields (say, M of them), let zmt denote the realized measurement taken from fieldm at location xt form = 1, . . . ,M . Then, the observation model becomes p(z1t , . . . , z M t |xt) = \u220fM m=1 p(z m t |xt) such that the equality follows from an assumption of independence of measurements between fields to ease computations."}, {"heading": "3 Online Sparse GP Observation Model", "text": "In contrast to existing works discussed in Section 1, our GPLocalize algorithm does not need to impose the restrictive Markov property (7) on the observation model, which can then be derived by marginalizing out the random locations visited and observed by the robot up until time step t\u2212 1: p(zt|xt, u1:t, z1:t\u22121)\n=\u03b7 \u222b b(x0) t\u220f i=1 p(xi|ui, xi\u22121)p(zt|xt, x1:t\u22121, z1:t\u22121)dx0:t\u22121 (8) where 1/\u03b7 = p(xt|u1:t, z1:t\u22121) is a normalizing constant, b(x0) = p(x0) is the belief of the robot\u2019s initial location at time step 0, x1:t\u22121 , {x1, . . . , xt\u22121} denotes a set of locations visited and observed by the robot up until time step t\u2212 1, and p(zt|xt, x1:t\u22121, z1:t\u22121) = N (\u00b5xt|x1:t\u22121 , \u03c3xtxt|x1:t\u22121) is a Gaussian predictive distribution provided by the GP (Section 2.1). The derivation of (8) is in Appendix A.\nTo make computations tractable but not constrain the type of motion model that can be specified, the observation model (8) is approximated using Monte Carlo integration:\np(zt|xt, u1:t, z1:t\u22121) \u2248 1\nC C\u2211 c=1 p(zt|xt, xc1:t\u22121, z1:t\u22121) (9)\nwhere xc1:t\u22121 denotes a c-th sample path simulated by first drawing the robot\u2019s initial location xc0 from b(x0) and then sampling xci from motion model p(xi|ui, xci\u22121) for i = 1, . . . , t \u2212 1 given its past actions u1:t\u22121 while ensuring p(xt|ut, xct\u22121) > 0, as observed in (8). For a practical implementation, instead of re-simulating the entire sample paths (hence, incurring linear time in t) at each time step, each c-th sample path is incrementally updated from xc1:t\u22122 (i.e., obtained in previous time step) to xc1:t\u22121 (i.e., needed in current time step) by including xct\u22121 sampled from motion model p(xt\u22121|ut, xct\u22122) without accounting for motion constraint p(xt|ut, xct\u22121) > 0. As a result, the time spent in incrementally updating the C sample paths at each time step is independent of t. To mitigate the effect of ignoring the constraint, we introduce a strategy in Remark 3 after Theorem 1 that exploits a structural property of our proposed online sparse GP. In practice, such an implementation yields considerable\ntime savings (i.e., time independent of t) and does not result in poor localization performance empirically (Section 4).\nThe scalability of our GP-Localize algorithm therefore depends on whether the Gaussian predictive probability p(zt|xt, xc1:t\u22121, z1:t\u22121) in (9) can be derived efficiently. Computing it with full GP, PITC, or FITC (Section 2) directly incurs, respectively,O(t3),O(t(|S|2 + (t/N)2)), and O(t|S|2) time. Since t is expected to be large for persistent localization, it is computationally impractical to use these offline full GP and sparse GP approximation methods to repeatedly train the observation model at each filtering step. Even when the online GP proposed by Csato\u0301 and Opper (2002) is used, it still incurs quadratic time in t per filtering step. In the following subsection, we will propose an online sparse GP that can achieve constant time (i.e., independent of t) at each filtering step t."}, {"heading": "3.1 Online Sparse GP Approximation", "text": "The key idea underlying our proposed online sparse GP is to summarize the newly gathered data/observations at regular time intervals/slices, assimilate the summary information of the new data with that of all the previously gathered data/observations, and then exploit the resulting assimilated summary information to compute the Gaussian predictive probability p(zt|xt, xc1:t\u22121, z1:t\u22121) in (9). The details of our proposed online sparse GP will be described next.\nLet each time slice n span time/filtering steps (n\u2212 1)\u03c4 + 1 to n\u03c4 for some user-defined slice size \u03c4 \u2208 Z+ and the number of time slices available thus far up until time step t be denoted by N (i.e., N\u03c4 < t). Definition 1 (Slice Summary) Given a support set S \u2282"}, {"heading": "X common to all C sample paths, the subset Dn ,", "text": "xc(n\u22121)\u03c4+1:n\u03c4 of the c-th sample path x c 1:t\u22121 simulated during the time slice n, and the column vector zDn = z(n\u22121)\u03c4+1:n\u03c4 of corresponding realized measurements taken by the robot, the slice summary of time slice n is defined as a tuple (\u00b5ns ,\u03a3 n s ) for n = 1, . . . , N where\n\u00b5ns , \u03a3SDn\u03a3 \u22121 DnDn|S(zDn \u2212 \u00b5Dn)\n\u03a3ns , \u03a3SDn\u03a3 \u22121 DnDn|S\u03a3DnS\nsuch that \u00b5Dn is defined in a similar manner as \u00b5D in (1) and \u03a3DnDn|S is a posterior covariance matrix with components \u03c3xx\u2032|S for all x, x\u2032 \u2208 Dn, each of which is defined in a similar way as (4). Remark. The support set S \u2282 X of locations does not have to be observed because the slice summary is independent of zS . So, the support set S can be selected prior to exploration and localization from X using an offline greedy active learning algorithm such as (Krause, Singh, and Guestrin 2008). Definition 2 (Assimilated Summary) Given (\u00b5ns ,\u03a3ns ), the assimilated summary (\u00b5na,\u03a3 n a) of time slices 1 to n is updated from the assimilated summary (\u00b5n\u22121a ,\u03a3 n\u22121 a ) of time slices 1 to n\u22121 using \u00b5na , \u00b5n\u22121a +\u00b5ns and \u03a3na , \u03a3n\u22121a +\u03a3ns for n = 1, . . . , N where \u00b50a , 0 and \u03a3 0 a , \u03a3SS .\nRemark 1. After constructing and assimilating (\u00b5ns ,\u03a3 n s ) with (\u00b5n\u22121a ,\u03a3 n\u22121 a ) to form (\u00b5na,\u03a3 n a), Dn = xc(n\u22121)\u03c4+1:n\u03c4 ,\nzDn = z(n\u22121)\u03c4+1:n\u03c4 , and (\u00b5 n s ,\u03a3 n s ) (Definition 1) are no longer needed and can be removed from memory. As a result, at time step t where N\u03c4 + 1 \u2264 t \u2264 (N + 1)\u03c4 , only (\u00b5Na ,\u03a3 N a ), x c N\u03c4+1:t\u22121, and zN\u03c4+1:t\u22121 have to be kept in memory, thus requiring only constant memory (i.e., independent of t). Remark 2. The slice summaries are constructed and assimilated at a regular time interval of \u03c4 , specifically, at time steps N\u03c4 + 1 for N \u2208 Z+. Theorem 1 Given S \u2282 X and (\u00b5Na ,\u03a3Na ), our online sparse GP computes a Gaussian predictive distribution p(zt|xt, \u00b5Na ,\u03a3Na ) = N (\u00b5\u0303xt , \u03c3\u0303xtxt) of the measurement at any location xt \u2208 X at time step t (i.e., N\u03c4 + 1 \u2264 t \u2264 (N + 1)\u03c4 ) with the following posterior mean and variance:\n\u00b5\u0303xt , \u00b5xt + \u03a3xtS ( \u03a3Na )\u22121 \u00b5Na (10)\n\u03c3\u0303xtxt , \u03c3xtxt \u2212 \u03a3xtS ( \u03a3\u22121SS \u2212 ( \u03a3Na )\u22121) \u03a3Sxt . (11)\nIf t = N\u03c4 + 1, \u00b5\u0303xt = \u00b5 PITC xt|xc1:t\u22121 and \u03c3\u0303xtxt = \u03c3 PITC xtxt|xc1:t\u22121 .\nIts proof is given in Appendix B. Remark 1. Theorem 1 implies that our proposed online sparse GP is in fact equivalent to an online learning formulation/variant of the offline PITC (Section 2.2). Supposing \u03c4 < |S|, theO(t|S|2) time incurred by offline PITC to compute p(zt|xt, xc1:t\u22121, z1:t\u22121) in (9) can then be reduced to O(\u03c4 |S|2) time (i.e., time independent of t) incurred by our online sparse GP at time steps t = N\u03c4+1 forN \u2208 Z+ when slice summaries are constructed and assimilated. Otherwise, our online sparse GP only incursO(|S|2) time per time step. Remark 2. The above equivalence result allows the structural property of our online sparse GP to be elucidated using that of offline PITC: The measurements ZD1 , . . . , ZDN , Zxt between different time slices are assumed to be conditionally independent given ZS . Such an assumption enables the data gathered during each time slice to be summarized independently of that in other time slices. Increasing slice size \u03c4 (i.e., less frequent assimilations of larger slice summaries) relaxes this conditional independence assumption (hence, potentially improving the fidelity of the resulting observation model), but incurs more time at time steps when slice summaries are constructed and assimilated (see Remark 1). Remark 3. Recall (see paragraph after (9)) that the motion constraint p(xt|ut, xct\u22121) > 0 is not accounted for when sampling xct\u22121 from motion model p(xt\u22121|ut, xct\u22122) at each time step t. To mitigate the effect of ignoring the constraint, at time steps t = N\u03c4+2 forN \u2208 Z+, we draw x\u2032N\u03c4 from the particle-based belief b(xN\u03c4 ) maintained in our experiments (Section 4) and use it (instead of xcN\u03c4 ) for sampling x c N\u03c4+1 from motion model p(xN\u03c4+1|ui, x\u2032N\u03c4 ). Doing this at a regular time interval of \u03c4 reduces the deviation of the simulated sample paths from the particle-based beliefs updated at each time step and consequently allows the sample paths to satisfy the motion constraint more often, especially when \u03c4 is small. Such a strategy may cause the sampled xcN\u03c4+1 not to be located close to xcN\u03c4 (hence, their corresponding realized measurements are less spatially correlated) since xcN\u03c4+1 is not sampled from motion model p(xN\u03c4+1|ui, xcN\u03c4 ). But,\nthis occurs at a lower frequency of 1/\u03c4 , as compared to not considering the motion constraint at every time step. Furthermore, this fits well with the structural property of our online sparse GP that assumes ZN\u03c4 and ZN\u03c4+1 (or, more generally, ZDN and Zxt ) to be conditionally independent. Remark 4. Since offline PITC generalizes offline FITC (Section 2.2), our online sparse GP generalizes the online learning variant of FITC (i.e., \u03c4 = 1) (Csato\u0301 and Opper 2002)3.\nWhen N\u03c4 + 1 < t \u2264 (N + 1)\u03c4 (i.e., before the next slice summary of time slice N + 1 is constructed and assimilated), the most recent observations (i.e., D\u2032 , xcN\u03c4+1:t\u22121 and zD\u2032 = zN\u03c4+1:t\u22121), which are often highly informative, are not used to update \u00b5\u0303xt (10) and \u03c3\u0303xtxt (11). This will hurt the localization performance, especially when \u03c4 is large and the robot is localizing in an unexplored area with little/no observations; the field within this area thus cannot be predicted well with the current assimilated summary. To resolve this, we exploit incremental update formulas of Gaussian posterior mean and variance (Appendix C) to update \u00b5\u0303xt and \u03c3\u0303xtxt with the most recent observations, thereby yielding a Gaussian predictive distribution p(zt|xt, \u00b5Na ,\u03a3Na ,D\u2032, zD\u2032) = N (\u00b5\u0303xt|D\u2032 , \u03c3\u0303xtxt|D\u2032) where\n\u00b5\u0303xt|D\u2032 , \u00b5\u0303xt + \u03a3\u0303xtD\u2032\u03a3\u0303 \u22121 D\u2032D\u2032 (zD\u2032 \u2212 \u00b5\u0303D\u2032) (12)\n\u03c3\u0303xtxt|D\u2032 , \u03c3\u0303xtxt \u2212 \u03a3\u0303xtD\u2032\u03a3\u0303 \u22121 D\u2032D\u2032\u03a3\u0303D\u2032xt (13)\nsuch that \u00b5\u0303D\u2032 is a column vector with mean components \u00b5\u0303x (i.e., defined similarly to (10)) for all x \u2208 D\u2032, \u03a3\u0303xtD\u2032 is a row vector with covariance components \u03c3\u0303xtx (i.e., defined similarly to (11)) for all x \u2208 D\u2032, \u03a3\u0303D\u2032xt is the transpose of \u03a3\u0303xtD\u2032 , and \u03a3\u0303D\u2032D\u2032 is a matrix with covariance components \u03c3\u0303xx\u2032 (i.e., defined similarly to (11)) for all x, x\u2032 \u2208 D\u2032. Theorem 2 Computing p(zt|xt, \u00b5Na ,\u03a3Na ,D\u2032, zD\u2032) (i.e., (12) and (13)) incurs O(\u03c4 |S|2) time at time steps t = N\u03c4 + 1 for N \u2208 Z+ and O(|S|2) time otherwise. It requires O(|S|2) memory at each time step. Its proof is given in Appendix D. Theorem 2 indicates that our online sparse GP incurs constant time and memory (i.e., independent of t) per time step."}, {"heading": "4 Experiments and Discussion", "text": "This section evaluates the localization performance, time efficiency, and scalability of our GP-Localize algorithm empirically through simulated experiments with three realworld datasets: (a) Wireless signal strength (WSS) (signalto-noise ratio) data (Chen and Guestrin 2007) produced by 6 WiFi access points (APs) and measured at over 200 locations throughout the fifth floor of Wean Hall in Carnegie Mellon University (Fig. 1, Section 4.1), (b) indoor environmental quality (IEQ) (i.e., temperature (\u25e6F) and light (Lux)) data (Bodik et al. 2004) measured by 54 sensors deployed in the Intel Berkeley Research lab (Fig. 3, Section 4.2), (c) urban traffic speeds (UTS) (km/h) data (Chen et al. 2012; 2013) measured at 775 road segments (including highways, arterials, slip roads, etc.) of an urban road network\n3Snelson (2007) pointed out that the sparse online GP of Csato\u0301 and Opper (2002) is an online learning variant of offline FITC.\nin Tampines area, Singapore during evening peak hours on April 20, 2011 with a mean speed of 47.6 km/h and a standard deviation of 20.5 km/h (Fig. 4a, Section 4.3), and (d) a real Pioneer 3-DX mobile robot (i.e., mounted with a weather board) experiment on a trajectory of about 280 m in the Singapore-MIT Alliance for Research and Technology Future Urban Mobility (SMART FM) IRG office/lab gathering 561 relative light (%) observations/data for GP localization (Fig. 5, Section 4.4). Different from the 2-dimensional spatial domains of the WSS, IEQ, and light fields, each road segment of the urban road network is specified by a 5-dimensional vector of features: length, number of lanes, speed limit, direction, and time. The UTS field is modeled using a relational GP (previously developed in (Chen et al. 2012)) whose correlation structure can exploit both the road segment features and road network topology information. The hyperparameters of each GP modeling a different field are learned using the data via maximum likelihood estimation (Rasmussen and Williams 2006). Our GP-Localize algorithm is implemented using an odometry motion model4, our online sparse GP (i.e., setting \u03c4 = 10 and |S| = 40) for representing the observation model (Section 3), and a particle filter4 of 400 particles for representing the belief of the robot\u2019s location. The number C of sample paths in (9) is set to 400 for all experiments. For the simulated experiments with the WSS and IEQ data, the control actions (i.e., odometry information) are generated using the realistic Pioneer mobile robot module in Player/Stage simulator (Gerkey, Vaughan, and Howard 2003) and the measurements taken along the generated trajectory of 421 (336) time steps from the WSS (IEQ) fields shown in Fig. 1 (Fig. 2) are the Gaussian predictive/posterior means (1) of each full GP modeling a separate field trained using the data. For the simulated experiment with the UTS data, the control actions of the mobile probe vehicle are assumed not to be known; its transition probability of moving from one road segment to another can be learned from vehicle route data using the hierarchical Bayesian nonparametric approach of Yu et al. (2012). The measurements taken along its generated trajectory of 370 time steps from the UTS field are shown in Fig. 4.\nThe localization performance/error (i.e., distance between the robot\u2019s estimated and true locations) and scalability of our GP-Localize algorithm is compared to that of two sparse GP localization algorithms: (a) The SoD-Truncate method uses |S| = 10 most recent observations (i.e., compared to |D\u2032| < \u03c4 = 10 most recent observations considered by our online sparse GP besides the assimilated summary) as training data at each filtering step while (b) the SoD-Even method uses |S| = 40 observations (i.e., compared to the support set of |S| = 40 possibly unobserved locations selected prior to localization and exploited by our online sparse GP) evenly distributed over the time of localization. The scalability of GP-Localize is further compared to that of GP localization algorithms employing full GP and offline PITC (Section 2).\n4Due to lack of space, an interested reader is referred to (Thrun, Burgard, and Fox 2005) for the technical details of the odometry motion model and particle filter."}, {"heading": "4.1 Wireless Signal Strength (WSS) Fields", "text": "Table 1 shows the localization errors (no units given in WSS data) of GP-localize, SoD-Truncate, and SoD-Even averaged over all 421 time steps and 5 simulation runs. It can be observed that GP-Localize outperforms the other two methods in every single field and in multiple fields (i.e., all 6): The observation model (i.e., represented by our online sparse GP) of GP-Localize can exploit the assimilated summary and the most recent observations to predict, respectively, the fields in explored and unexplored areas better. In contrast, SoD-Truncate performs poorly in explored areas since its predictive capability is limited by using only the most recent observations. The limited observations of SoDEven can only cover the entire area sparsely, thus producing an inaccurate observation model.\nIt can also be observed from Table 1 that GP-Localize achieves its largest (smallest) localization error in field 3 (4): Fig. 1a shows that the robot does not explore the area on the left with highly varying measurements well enough, thus yielding an assimilated summary that is less informative to localization in this area. Though it explores the area on the right densely, the field in this area is relatively constant, hence making localization difficult. As a result, localization error is high in field 3. On the other hand, Fig. 1b shows that the robot explores the area on the right with highly varying measurements densely, thus achieving low error in field 4.\nFig. 2a shows the localization error of GP-Localize at each time step in every single field and in multiple fields (i.e., all 6) averaged over 5 runs. It can be observed that although the error in multiple fields is not always smallest at each time step, it often tends to be close to (if not, lower than) the lowest error among all single fields and, more importantly, is not so high like those in single fields 1, 2, 3, 5, 6\nafter 200 time steps. In practice, since it is usually not known which single field yields a low or high error at each time step, a more robust GP-Localize algorithm (i.e., achieved by exploiting multiple fields) is preferred."}, {"heading": "4.2 Indoor Environmental Quality (IEQ) Fields", "text": "Table 2 shows the localization errors (m) of the tested methods averaged over all 336 time steps and 5 simulation runs. Similar to the observations for WSS fields (Section 4.1), GPLocalize outperforms the other two methods in every single field and in multiple fields, as explained in the previous section. It can also be observed that GP-Localize achieves a smaller error in the light field than in the temperature field because the measurements of the light field vary slightly more than that of the temperature field, as shown in Fig. 3. Fig. 2b shows the error of GP-Localize at each time step in every single field and in multiple fields averaged over 5 runs. It can again be observed that although the error in multiple fields is not always smallest at each time step, it is often close to (if not, lower than) the lowest error among all single fields and not as high as that in the single temperature field. Our GP-Localize algorithm exploiting multiple fields is therefore more robust in this experiment."}, {"heading": "4.3 Urban Traffic Speeds (UTS) Field", "text": "For the UTS field, the localization error is defined as the geodesic (i.e., shortest path) distance between the vehicle\u2019s\nestimated and true residing road segments with respect to the road network topology (Fig. 4). GP-Localize, SoD-Truncate, and SoD-Even achieve, respectively, localization errors of 2.8, 7.3, and 6.2 road segments averaged over all 370 time steps and 3 simulation runs."}, {"heading": "4.4 Real Pioneer 3-DX Mobile Robot Experiment", "text": "The adaptive Monte Carlo localization (AMCL) package in the Robot Operating System (ROS) is run on a Pioneer 3-DX mobile robot mounted with a SICK LMS200 laser rangefinder to determine its trajectory (Fig. 5a) and the 561 locations at which the relative light measurements are taken (Fig. 5b); these locations are assumed to be the ground truth. GP-Localize, SoD-Truncate, and SoD-Even achieve, respectively, localization errors of 2.1 m, 5.4 m, and 4.6 m averaged over all 561 time steps and 3 runs.\nFig. 4b shows the time incurred by GP-Localize, SoDTruncate, SoD-Even, full GP, and offline PITC at each time step using 100 particles averaged over 5 runs. It can be seen that, with more time steps, the time incurred by full GP, offline PITC, and SoD-Even increase while that of GPLocalize and SoD-Truncate remain constant. GP-Localize is clearly much more scalable (i.e., constant time) in t than full GP and offline PITC. Though it incurs slightly more time than SoD-Truncate and SoD-Even, it can localize significantly better (Sections 4.1 and 4.2)."}, {"heading": "5 Conclusion", "text": "This paper describes the GP-Localize algorithm for persistent robot localization whose observation model is represented by our novel online sparse GP, thus achieving constant time and memory (i.e., independent of the size of the data) per filtering step. We theoretically analyze the equivalence of our online sparse GP to the online learning variant of offline PITC. We empirically demonstrate that GPLocalize outperforms existing GP localization algorithms in terms of localization performance and scalability and achieves robustness by exploiting multiple fields. Besides using our online sparse GP for persistent robot localization, note that it can in fact be applied to a wide variety of applications and are especially desirable (i.e., due to runtime and memory being independent of the size of data) for tasks with data streaming in over time or real-time requirements. Some robotic tasks include adaptive sampling, information gathering, learning of robot arm control (Low, Leow, and Ang, Jr. 2002a; 2002b; 2005), visual tracking and head pose estimation for human-robot interaction. For non-robotic applications, they include traffic and weather prediction, online action recognition, online recommendation systems, online classification, among others.\nA limitation of GP-Localize, as observed in our experiments, is that it does not localize well in near-constant fields, which is expected. So, in our future work, we plan to generalize our algorithm to handle richer, high-dimensional sensing data like laser scans and camera images (Natarajan et al. 2012a; 2012b; Natarajan, Low, and Kankanhalli 2014). We also like to investigate the effect of varying the slice size \u03c4 on the localization error of GP-Localize empirically and remove the assumption of independence between fields by exploiting techniques like multi-output GPs and co-kriging for modeling their correlation. Lastly, as mentioned in Section 4, the hyperparameters of each GP are learned using the data by maximizing the log marginal likelihood. The sparse approximation method employed by offline PITC to improve the scalability of the full GP can be similarly applied to computing such a log marginal likelihood scalably, as explained in (Quin\u0303onero-Candela and Rasmussen 2005) (i.e., equation 30 in Section 9). Since our online sparse GP is the online variant of the offline PITC, the log marginal likelihood can be computed and maximized in an online manner as well. The exact details will be specified in the future extension of this work."}, {"heading": "Acknowledgments", "text": "This work was supported by Singapore-MIT Alliance for Research and Technology Subaward Agreement No. 41 R252-000-527-592."}, {"heading": "B Proof of Theorem 1", "text": "Since Dn = xc(n\u22121)\u03c4+1:n\u03c4 (Definition 1) and t = N\u03c4 + 1, D = \u22c3N n=1Dn = \u22c3N n=1 x c (n\u22121)\u03c4+1:n\u03c4 = x c 1:N\u03c4 = x c 1:t\u22121. Let us first simplify the \u0393xtD (\u0393DD + \u039b) \u22121 term on the right-hand side expressions of \u00b5PITCxt|D = \u00b5 PITC xt|xc1:t\u22121 (5) and \u03c3PITCxtxt|D = \u03c3 PITC xtxt|xc1:t\u22121 (6).\n(\u0393DD + \u039b) \u22121 = ( \u03a3DS\u03a3 \u22121 SS\u03a3SD + \u039b )\u22121 = \u039b\u22121 \u2212 \u039b\u22121\u03a3DS ( \u03a3SS + \u03a3SD\u039b \u22121\u03a3DS )\u22121 \u03a3SD\u039b \u22121\n= \u039b\u22121 \u2212 \u039b\u22121\u03a3DS ( \u03a3Na )\u22121 \u03a3SD\u039b \u22121 .\n(14) The second equality is due to the matrix inversion lemma. The last equality follows from\n\u03a3SS + \u03a3SD\u039b \u22121\u03a3DS\n= \u03a3SS + N\u2211 n=1 \u03a3SDn\u03a3 \u22121 DnDn|S\u03a3DnS\n= \u03a3SS + N\u2211 n=1 \u03a3ns = \u03a3 N a .\n(15)\nFrom (14),\n\u0393xtD (\u0393DD + \u039b) \u22121 = \u03a3xtS\u03a3 \u22121 SS\u03a3SD ( \u039b\u22121 \u2212 \u039b\u22121\u03a3DS ( \u03a3Na )\u22121 \u03a3SD\u039b \u22121 )\n= \u03a3xtS\u03a3 \u22121 SS ( \u03a3Na \u2212 \u03a3SD\u039b\u22121\u03a3DS ) ( \u03a3Na )\u22121 \u03a3SD\u039b \u22121\n= \u03a3xtS ( \u03a3Na )\u22121 \u03a3SD\u039b \u22121 .\n(16) The third equality is due to (15).\nFrom (5),\n\u00b5PITCxt|xc1:t\u22121 = \u00b5PITCxt|D\n= \u00b5xt + \u0393xtD (\u0393DD + \u039b) \u22121\n(zD \u2212 \u00b5D) = \u00b5xt + \u03a3xtS ( \u03a3Na )\u22121 \u03a3SD\u039b \u22121 (zD \u2212 \u00b5D)\n= \u00b5xt + \u03a3xtS ( \u03a3Na )\u22121\n\u00b5Na = \u00b5\u0303xt .\nThe third equality is due to (16). The fourth equality follows from \u03a3SD\u039b\u22121 (zD \u2212 \u00b5D) =\u2211N n=1 \u03a3SDn\u03a3 \u22121 DnDn|S (zDn \u2212 \u00b5Dn) = \u2211N n=1 \u00b5 n s = \u00b5 N a .\nFrom (6),\n\u03c3PITCxtxt|xc1:t\u22121 = \u03c3PITCxtxt|D\n= \u03c3xtxt \u2212 \u0393xtD (\u0393DD + \u039b) \u22121\n\u0393Dxt = \u03c3xtxt \u2212 \u03a3xtS ( \u03a3Na )\u22121 \u03a3SD\u039b \u22121\u03a3DS\u03a3 \u22121 SS\u03a3Sxt\n= \u03c3xtxt \u2212 \u03a3xtS ( \u03a3Na )\u22121 ( \u03a3Na \u2212 \u03a3SS ) \u03a3\u22121SS\u03a3Sxt\n= \u03c3xtxt \u2212 \u03a3xtS ( \u03a3\u22121SS \u2212 ( \u03a3Na )\u22121)\n\u03a3Sxt = \u03c3\u0303xtxt .\nThe third and fourth equalities follow from (16) and (15), respectively.\nC Incremental Update Formulas of Gaussian Posterior Mean and Variance\nUsing the matrix inversion lemma, the following incremental update formulas of the Gaussian posterior mean and variance can be obtained:\n\u00b5x|D\u222aD\u2032 , \u00b5x|D + \u03a3xD\u2032|D\u03a3 \u22121 D\u2032D\u2032|D ( zD\u2032 \u2212 \u00b5D\u2032|D ) \u03c3xx|D\u222aD\u2032 , \u03c3xx|D \u2212 \u03a3xD\u2032|D\u03a3\u22121D\u2032D\u2032|D\u03a3D\u2032x|D\nfor allD,D\u2032 \u2282 X such thatD\u2229D\u2032 = \u2205 and x \u2208 X\\(D\u222aD\u2032)."}, {"heading": "D Proof Sketch of Theorem 2", "text": "Firstly, (\u03a3Na )\n\u22121 in (10) and (11) has to be evaluated at time steps t = N\u03c4 + 1 for N \u2208 Z+. To avoid incurring O(|S|3) time to invert \u03a3Na , the matrix inversion lemma can be used to obtain (\u03a3Na )\n\u22121 from (\u03a3N\u22121a )\u22121 (i.e., previously derived at time step (N \u2212 1)\u03c4 + 1) in O(\u03c4 |S|2) time (i.e., assuming \u03c4 < |S|) and O(|S|2) memory, as observed in the following derivation:( \u03a3Na )\u22121\n= (\n\u03a3N\u22121a + \u03a3SDN\u03a3 \u22121 DNDN |S\u03a3DNS )\u22121 = ( \u03a3N\u22121a )\u22121 + ( \u03a3N\u22121a )\u22121 \u03a3SDN(\n\u03a3DNDN |S+\u03a3DNS ( \u03a3N\u22121a )\u22121 \u03a3SDN )\u22121 \u03a3DNS ( \u03a3N\u22121a )\u22121 .\nSince evaluating \u00b5Na in (10) also incursO(\u03c4 |S|2) time, \u03a3\u22121SS can be evaluated prior to exploration and localization while incurring O(|S|2) memory, and D\u2032 = \u2205 at time steps t = N\u03c4+1, computing \u00b5\u0303xt|D\u2032 = \u00b5\u0303xt (10) and \u03c3\u0303xtxt|D\u2032 = \u03c3\u0303xtxt (11) incurO(\u03c4 |S|2) time andO(|S|2) memory at time steps t = N\u03c4 + 1 for N \u2208 Z+.\nOn the other hand, when N\u03c4 + 1 < t \u2264 (N + 1)\u03c4 , \u03a3\u0303\u22121D\u2032D\u2032 in (12) and (13) has to be evaluated. Let D\u2032\u2212 , xcN\u03c4+1:t\u22122. Then, D\u2032 = D\u2032\u2212 \u222a xct\u22121. To avoid incurring O(|D\u2032|3) time to invert \u03a3\u0303D\u2032D\u2032 , the matrix inversion lemma can again be used to obtain \u03a3\u0303\u22121D\u2032D\u2032 from \u03a3\u0303 \u22121 D\u2032\u2212D\u2032\u2212 (i.e., previously derived at time step t\u22121) inO(|S|2) time andO(|S|2) memory (i.e., |D\u2032| < \u03c4 < |S|), as observed in the following derivation:\n\u03a3\u0303\u22121D\u2032D\u2032 = \u03a3\u0303\u22121(D\u2032\u2212\u222axct\u22121)(D\u2032\u2212\u222axct\u22121)\n= ( \u03a3\u0303D\u2032\u2212D\u2032\u2212 \u03a3\u0303D\u2032\u2212xct\u22121 \u03a3\u0303xct\u22121D\u2032\u2212 \u03c3\u0303xct\u22121xct\u22121 )\u22121 = ( \u03a3\u0303\u22121D\u2032\u2212D\u2032\u2212 + \u03a3\u0303\u22121D\u2032\u2212D\u2032\u2212 \u03a3\u0303D\u2032\u2212xct\u22121\u03a8 \u2212\u03a8 >\n\u2212\u03a8 \u03c3\u0303xct\u22121xct\u22121|D\u2032\u2212\n)\nwhere\n\u03c3\u0303xct\u22121xct\u22121|D\u2032\u2212 = \u03c3\u0303xct\u22121xct\u22121 \u2212 \u03a3\u0303xct\u22121D\u2032\u2212\u03a3\u0303 \u22121 D\u2032\u2212D\u2032\u2212 \u03a3\u0303D\u2032\u2212xct\u22121\nand \u03a8 = \u03c3\u0303xct\u22121xct\u22121|D\u2032\u2212\u03a3\u0303xct\u22121D\u2032\u2212\u03a3\u0303 \u22121 D\u2032\u2212D\u2032\u2212 . Note that computing \u03a3\u0303xct\u22121D\u2032\u2212 only incursO(|S| 2) time instead ofO(D\u2032|S|2) time because\n\u03a3\u0303xct\u22121D\u2032\u2212 = \u03a3xct\u22121D\u2032\u2212 \u2212 \u03a3xct\u22121S ( \u03a3\u22121SS \u2212 ( \u03a3Na )\u22121)\n\u03a3SD\u2032\u2212 = \u03a3xct\u22121D\u2032\u2212 \u2212 \u03a3xct\u22121S[(\n\u03a3\u22121SS\u2212 ( \u03a3Na )\u22121) \u03a3SxcN\u03c4+1:t\u22123 , ( \u03a3\u22121SS\u2212 ( \u03a3Na )\u22121) \u03a3Sxct\u22122 ]\nand (\u03a3\u22121SS \u2212 (\u03a3Na )\u22121)\u03a3SxcN\u03c4+1:t\u22123 is previously evaluated at time step t\u2212 1. Similarly, evaluating \u00b5\u0303D\u2032 in (12) only incursO(|S|2) time instead ofO(D\u2032|S|2) time because \u00b5\u0303D\u2032 = (\u00b5\u0303>D\u2032\u2212 , \u00b5\u0303xct\u22121) > and \u00b5\u0303D\u2032\u2212 is previously evaluated at time step t \u2212 1. Therefore, computing \u00b5\u0303xt|D\u2032 (12) and \u03c3\u0303xtxt|D\u2032 (13) incur only O(|S|2) time and O(|S|2) memory at time steps t where N\u03c4 + 1 < t \u2264 (N + 1)\u03c4 ."}, {"heading": "E Additional figures for Section 4.1", "text": ""}], "references": [{"title": "Gaussian process models for indoor and outdoor sensor-centric robot", "author": ["P. Bodik", "C. Guestrin", "W. Hong", "S. Madden", "M. Paskin", "R. Thibaux"], "venue": null, "citeRegEx": "Bodik et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bodik et al\\.", "year": 2004}, {"title": "Multi-robot informative path planning for active sensing of environmental phenomena: A tale of two algorithms", "author": ["N. Cao", "K.H. Low", "J.M. Dolan"], "venue": "Proc. AAMAS, 7\u201314. Chen, K., and Guestrin, C.", "citeRegEx": "Cao et al\\.,? 2013", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan", "A. Oran", "P. Jaillet", "J.M. Dolan", "G.S. Sukhatme"], "venue": "Proc. UAI, 163\u2013173.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Parallel Gaussian process regression with low-rank covariance matrix approximations", "author": ["J. Chen", "N. Cao", "K.H. Low", "R. Ouyang", "C.K.-Y. Tan", "P. Jaillet"], "venue": "Proc. UAI, 152\u2013161.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Gaussian process-based decentralized data fusion and active sensing for mobility-on-demand system", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan"], "venue": "Proc. RSS.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Sparse online Gaussian processes", "author": ["L. Csat\u00f3", "M. Opper"], "venue": "Neural Computation 14(2):641\u2013669.", "citeRegEx": "Csat\u00f3 and Opper,? 2002", "shortCiteRegEx": "Csat\u00f3 and Opper", "year": 2002}, {"title": "Cooperative aquatic sensing using the telesupervised adaptive ocean sensor fleet", "author": ["J.M. Dolan", "G. Podnar", "S. Stancliff", "K.H. Low", "A. Elfes", "J. Higinbotham", "J.C. Hosler", "T.A. Moisan", "J. Moisan"], "venue": "Proc. SPIE Conference on Remote Sensing of the Ocean, Sea Ice, and Large Water", "citeRegEx": "Dolan et al\\.,? 2009", "shortCiteRegEx": "Dolan et al\\.", "year": 2009}, {"title": "WiFi-SLAM using Gaussian process latent variable models", "author": ["B. Ferris", "D. Fox", "N. Lawrence"], "venue": "Proc. IJCAI, 2480\u20132485.", "citeRegEx": "Ferris et al\\.,? 2007", "shortCiteRegEx": "Ferris et al\\.", "year": 2007}, {"title": "Gaussian processes for signal strength-based location estimation", "author": ["B. Ferris", "D. H\u00e4hnel", "D. Fox"], "venue": "Proc. RSS.", "citeRegEx": "Ferris et al\\.,? 2006", "shortCiteRegEx": "Ferris et al\\.", "year": 2006}, {"title": "The Player/Stage project: Tools for multi-robot and distributed sensor systems", "author": ["B.P. Gerkey", "R.T. Vaughan", "A. Howard"], "venue": "Proc. ICAR, 317\u2013323.", "citeRegEx": "Gerkey et al\\.,? 2003", "shortCiteRegEx": "Gerkey et al\\.", "year": 2003}, {"title": "Nonmyopic -Bayes-optimal active learning of Gaussian processes", "author": ["T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli"], "venue": "Proc. ICML.", "citeRegEx": "Hoang et al\\.,? 2014", "shortCiteRegEx": "Hoang et al\\.", "year": 2014}, {"title": "GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation models", "author": ["J. Ko", "D. Fox"], "venue": "Autonomous Robots 27(1):75\u201390.", "citeRegEx": "Ko and Fox,? 2009a", "shortCiteRegEx": "Ko and Fox", "year": 2009}, {"title": "Learning GP-BayesFilters via Gaussian process latent variable models", "author": ["J. Ko", "D. Fox"], "venue": "Proc. RSS.", "citeRegEx": "Ko and Fox,? 2009b", "shortCiteRegEx": "Ko and Fox", "year": 2009}, {"title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "JMLR 9:235\u2013284.", "citeRegEx": "Krause et al\\.,? 2008", "shortCiteRegEx": "Krause et al\\.", "year": 2008}, {"title": "Adaptive sampling for multi-robot wide-area exploration", "author": ["K.H. Low", "G.J. Gordon", "J.M. Dolan", "P. Khosla"], "venue": "Proc. IEEE ICRA, 755\u2013760.", "citeRegEx": "Low et al\\.,? 2007", "shortCiteRegEx": "Low et al\\.", "year": 2007}, {"title": "Decentralized active robotic exploration and mapping for probabilistic field classification in environmental sensing", "author": ["K.H. Low", "J. Chen", "J.M. Dolan", "S. Chien", "D.R. Thompson"], "venue": "Proc. AAMAS, 105\u2013112.", "citeRegEx": "Low et al\\.,? 2012", "shortCiteRegEx": "Low et al\\.", "year": 2012}, {"title": "Adaptive multi-robot wide-area exploration and mapping", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 23\u201330.", "citeRegEx": "Low et al\\.,? 2008", "shortCiteRegEx": "Low et al\\.", "year": 2008}, {"title": "Informationtheoretic approach to efficient adaptive path planning for mobile robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. ICAPS, 233\u2013 240.", "citeRegEx": "Low et al\\.,? 2009", "shortCiteRegEx": "Low et al\\.", "year": 2009}, {"title": "Active Markov information-theoretic path planning for robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 753\u2013760.", "citeRegEx": "Low et al\\.,? 2011", "shortCiteRegEx": "Low et al\\.", "year": 2011}, {"title": "A hybrid mobile robot architecture with integrated planning and control", "author": ["K.H. Low", "W.K. Leow", "Ang, Jr., M.H."], "venue": "Proc. AAMAS, 219\u2013226.", "citeRegEx": "Low et al\\.,? 2002a", "shortCiteRegEx": "Low et al\\.", "year": 2002}, {"title": "Integrated planning and control of mobile robot with selforganizing neural network", "author": ["K.H. Low", "W.K. Leow", "Ang, Jr., M.H."], "venue": "Proc. IEEE ICRA, 3870\u2013 3875.", "citeRegEx": "Low et al\\.,? 2002b", "shortCiteRegEx": "Low et al\\.", "year": 2002}, {"title": "An ensemble of cooperative extended Kohonen maps for complex robot motion tasks", "author": ["K.H. Low", "W.K. Leow", "Ang, Jr., M.H."], "venue": "Neural Comput. 17(6):1411\u20131445.", "citeRegEx": "Low et al\\.,? 2005", "shortCiteRegEx": "Low et al\\.", "year": 2005}, {"title": "Decision-theoretic approach to maximizing observation of multiple targets in multi-camera surveillance", "author": ["P. Natarajan", "T.N. Hoang", "K.H. Low", "M. Kankanhalli"], "venue": "Proc. AAMAS, 155\u2013162.", "citeRegEx": "Natarajan et al\\.,? 2012a", "shortCiteRegEx": "Natarajan et al\\.", "year": 2012}, {"title": "Decision-theoretic coordination and control for active multi-camera surveillance in uncertain, partially observable environments", "author": ["P. Natarajan", "T.N. Hoang", "K.H. Low", "M. Kankanhalli"], "venue": "Proc. ICDSC.", "citeRegEx": "Natarajan et al\\.,? 2012b", "shortCiteRegEx": "Natarajan et al\\.", "year": 2012}, {"title": "Decision-theoretic approach to maximizing fairness in multi-target observation in multi-camera surveillance", "author": ["P. Natarajan", "K.H. Low", "M. Kankanhalli"], "venue": "Proc. AAMAS.", "citeRegEx": "Natarajan et al\\.,? 2014", "shortCiteRegEx": "Natarajan et al\\.", "year": 2014}, {"title": "Multirobot active sensing of non-stationary Gaussian processbased environmental phenomena", "author": ["R. Ouyang", "K.H. Low", "J. Chen", "P. Jaillet"], "venue": "Proc. AAMAS.", "citeRegEx": "Ouyang et al\\.,? 2014", "shortCiteRegEx": "Ouyang et al\\.", "year": 2014}, {"title": "Telesupervised remote surface water quality sensing", "author": ["G. Podnar", "J.M. Dolan", "K.H. Low", "A. Elfes"], "venue": "Proc. IEEE Aerospace Conference.", "citeRegEx": "Podnar et al\\.,? 2010", "shortCiteRegEx": "Podnar et al\\.", "year": 2010}, {"title": "A unifying view of sparse approximate Gaussian process regression", "author": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen"], "venue": "JMLR 6:1939\u20131959.", "citeRegEx": "Qui\u00f1onero.Candela and Rasmussen,? 2005", "shortCiteRegEx": "Qui\u00f1onero.Candela and Rasmussen", "year": 2005}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "Cambridge, MA: MIT Press.", "citeRegEx": "Rasmussen and Williams,? 2006", "shortCiteRegEx": "Rasmussen and Williams", "year": 2006}, {"title": "Flexible and efficient Gaussian process models for machine learning", "author": ["E.L. Snelson"], "venue": "Ph.D. Thesis, University College London, London, UK.", "citeRegEx": "Snelson,? 2007", "shortCiteRegEx": "Snelson", "year": 2007}, {"title": "Probabilistic Robotics", "author": ["S. Thrun", "W. Burgard", "D. Fox"], "venue": "Cambridge, MA: MIT Press.", "citeRegEx": "Thrun et al\\.,? 2005", "shortCiteRegEx": "Thrun et al\\.", "year": 2005}, {"title": "Hierarchical Bayesian nonparametric approach to modeling and learning the wisdom of crowds of urban traffic route planning agents", "author": ["J. Yu", "K.H. Low", "A. Oran", "P. Jaillet"], "venue": "Proc. IAT, 478\u2013485.", "citeRegEx": "Yu et al\\.,? 2012", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Recent research in robot exploration and mapping has focused on developing adaptive sampling and active sensing algorithms (Cao, Low, and Dolan 2013; Chen, Low, and Tan 2013; Chen et al. 2012; Hoang et al. 2014; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2007; 2012; Ouyang et al. 2014) to gather the most informative data/observations for modeling and predicting spatially varying environmental fields that are characterized by continuous-valued, spatially correlated measurements.", "startOffset": 123, "endOffset": 295}, {"referenceID": 10, "context": "Recent research in robot exploration and mapping has focused on developing adaptive sampling and active sensing algorithms (Cao, Low, and Dolan 2013; Chen, Low, and Tan 2013; Chen et al. 2012; Hoang et al. 2014; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2007; 2012; Ouyang et al. 2014) to gather the most informative data/observations for modeling and predicting spatially varying environmental fields that are characterized by continuous-valued, spatially correlated measurements.", "startOffset": 123, "endOffset": 295}, {"referenceID": 14, "context": "Recent research in robot exploration and mapping has focused on developing adaptive sampling and active sensing algorithms (Cao, Low, and Dolan 2013; Chen, Low, and Tan 2013; Chen et al. 2012; Hoang et al. 2014; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2007; 2012; Ouyang et al. 2014) to gather the most informative data/observations for modeling and predicting spatially varying environmental fields that are characterized by continuous-valued, spatially correlated measurements.", "startOffset": 123, "endOffset": 295}, {"referenceID": 25, "context": "Recent research in robot exploration and mapping has focused on developing adaptive sampling and active sensing algorithms (Cao, Low, and Dolan 2013; Chen, Low, and Tan 2013; Chen et al. 2012; Hoang et al. 2014; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2007; 2012; Ouyang et al. 2014) to gather the most informative data/observations for modeling and predicting spatially varying environmental fields that are characterized by continuous-valued, spatially correlated measurements.", "startOffset": 123, "endOffset": 295}, {"referenceID": 6, "context": ", environmental sensing and monitoring) requiring such algorithms often contain multiple fields of interest: (a) Autonomous underwater and surface vehicles are tasked to sample ocean and freshwater phenomena including temperature, salinity, and oxygen concentration fields (Dolan et al. 2009; Podnar et al. 2010), (b) indoor environments are spanned by temperature, light, and carbon dioxide concentration fields that affect the occupants\u2019 comfort and satisfaction towards the environmental quality across different areas, and (c) WiFi access points/hotspots situated at neighboring locations produce different but overlapping wireless signal strength fields over the same environment.", "startOffset": 273, "endOffset": 312}, {"referenceID": 26, "context": ", environmental sensing and monitoring) requiring such algorithms often contain multiple fields of interest: (a) Autonomous underwater and surface vehicles are tasked to sample ocean and freshwater phenomena including temperature, salinity, and oxygen concentration fields (Dolan et al. 2009; Podnar et al. 2010), (b) indoor environments are spanned by temperature, light, and carbon dioxide concentration fields that affect the occupants\u2019 comfort and satisfaction towards the environmental quality across different areas, and (c) WiFi access points/hotspots situated at neighboring locations produce different but overlapping wireless signal strength fields over the same environment.", "startOffset": 273, "endOffset": 312}, {"referenceID": 11, "context": "Existing works (Brooks, Makarenko, and Upcroft 2008; Ferris, H\u00e4hnel, and Fox 2006; Ferris, Fox, and Lawrence 2007; Ko and Fox 2009a; 2009b) have sidestepped this computational difficulty by assuming the availability of data/observations prior to exploration and localization for training the GP observation model offline; some (Brooks, Makarenko, and Upcroft 2008; Ferris, H\u00e4hnel, and Fox 2006; Ko and Fox 2009a) have assumed these given prior measurements to be labeled with known locations while others (Ferris, Fox, and Lawrence 2007; Ko and Fox 2009b) have inferred their location labels.", "startOffset": 15, "endOffset": 139}, {"referenceID": 11, "context": "Existing works (Brooks, Makarenko, and Upcroft 2008; Ferris, H\u00e4hnel, and Fox 2006; Ferris, Fox, and Lawrence 2007; Ko and Fox 2009a; 2009b) have sidestepped this computational difficulty by assuming the availability of data/observations prior to exploration and localization for training the GP observation model offline; some (Brooks, Makarenko, and Upcroft 2008; Ferris, H\u00e4hnel, and Fox 2006; Ko and Fox 2009a) have assumed these given prior measurements to be labeled with known locations while others (Ferris, Fox, and Lawrence 2007; Ko and Fox 2009b) have inferred their location labels.", "startOffset": 327, "endOffset": 412}, {"referenceID": 12, "context": "Existing works (Brooks, Makarenko, and Upcroft 2008; Ferris, H\u00e4hnel, and Fox 2006; Ferris, Fox, and Lawrence 2007; Ko and Fox 2009a; 2009b) have sidestepped this computational difficulty by assuming the availability of data/observations prior to exploration and localization for training the GP observation model offline; some (Brooks, Makarenko, and Upcroft 2008; Ferris, H\u00e4hnel, and Fox 2006; Ko and Fox 2009a) have assumed these given prior measurements to be labeled with known locations while others (Ferris, Fox, and Lawrence 2007; Ko and Fox 2009b) have inferred their location labels.", "startOffset": 505, "endOffset": 555}, {"referenceID": 28, "context": "Let {Zx}x\u2208X denote a GP, that is, every finite subset of {Zx}x\u2208X has a multivariate Gaussian distribution (Rasmussen and Williams 2006).", "startOffset": 106, "endOffset": 135}, {"referenceID": 2, "context": "To improve its scalability, GP approximation methods (Chen et al. 2012; 2013; Qui\u00f1onero-Candela and Rasmussen 2005) have been proposed, two of which will be described below.", "startOffset": 53, "endOffset": 115}, {"referenceID": 27, "context": "To improve its scalability, GP approximation methods (Chen et al. 2012; 2013; Qui\u00f1onero-Candela and Rasmussen 2005) have been proposed, two of which will be described below.", "startOffset": 53, "endOffset": 115}, {"referenceID": 27, "context": "The main criticism of SoD is that it does not exploit all the data for computing the Gaussian predictive distribution, thus yielding an unrealistic overestimate (4) of the predictive uncertainty (even with fairly redundant data and informative subset S) (Qui\u00f1onero-Candela and Rasmussen 2005) and in turn an inaccurately trained observation model.", "startOffset": 254, "endOffset": 292}, {"referenceID": 27, "context": "methods reported in (Qui\u00f1onero-Candela and Rasmussen 2005) exploiting the notion of a support set S \u2282 X .", "startOffset": 20, "endOffset": 58}, {"referenceID": 11, "context": "FITC is previously employed by Ko and Fox (2009a) to speed up the learning of observation model with prior training data.", "startOffset": 31, "endOffset": 50}, {"referenceID": 11, "context": "The disadvantages of the former are extensively discussed by Ko and Fox (2009a) while that of the latter are already detailed in Section 1.", "startOffset": 61, "endOffset": 80}, {"referenceID": 5, "context": "Even when the online GP proposed by Csat\u00f3 and Opper (2002) is used, it still incurs quadratic time in t per filtering step.", "startOffset": 36, "endOffset": 59}, {"referenceID": 5, "context": ", \u03c4 = 1) (Csat\u00f3 and Opper 2002)3.", "startOffset": 9, "endOffset": 31}, {"referenceID": 0, "context": ", temperature (\u25e6F) and light (Lux)) data (Bodik et al. 2004) measured by 54 sensors deployed in the Intel Berkeley Research lab (Fig.", "startOffset": 41, "endOffset": 60}, {"referenceID": 2, "context": "2), (c) urban traffic speeds (UTS) (km/h) data (Chen et al. 2012; 2013) measured at 775 road segments (including highways, arterials, slip roads, etc.", "startOffset": 47, "endOffset": 71}, {"referenceID": 2, "context": "The UTS field is modeled using a relational GP (previously developed in (Chen et al. 2012)) whose correlation structure can exploit both the road segment features and road network topology information.", "startOffset": 72, "endOffset": 90}, {"referenceID": 28, "context": "The hyperparameters of each GP modeling a different field are learned using the data via maximum likelihood estimation (Rasmussen and Williams 2006).", "startOffset": 119, "endOffset": 148}, {"referenceID": 2, "context": "Snelson (2007) pointed out that the sparse online GP of Csat\u00f3 and Opper (2002) is an online learning variant of offline FITC.", "startOffset": 56, "endOffset": 79}, {"referenceID": 2, "context": "The UTS field is modeled using a relational GP (previously developed in (Chen et al. 2012)) whose correlation structure can exploit both the road segment features and road network topology information. The hyperparameters of each GP modeling a different field are learned using the data via maximum likelihood estimation (Rasmussen and Williams 2006). Our GP-Localize algorithm is implemented using an odometry motion model4, our online sparse GP (i.e., setting \u03c4 = 10 and |S| = 40) for representing the observation model (Section 3), and a particle filter4 of 400 particles for representing the belief of the robot\u2019s location. The number C of sample paths in (9) is set to 400 for all experiments. For the simulated experiments with the WSS and IEQ data, the control actions (i.e., odometry information) are generated using the realistic Pioneer mobile robot module in Player/Stage simulator (Gerkey, Vaughan, and Howard 2003) and the measurements taken along the generated trajectory of 421 (336) time steps from the WSS (IEQ) fields shown in Fig. 1 (Fig. 2) are the Gaussian predictive/posterior means (1) of each full GP modeling a separate field trained using the data. For the simulated experiment with the UTS data, the control actions of the mobile probe vehicle are assumed not to be known; its transition probability of moving from one road segment to another can be learned from vehicle route data using the hierarchical Bayesian nonparametric approach of Yu et al. (2012). The measurements taken along its generated trajectory of 370 time steps from the UTS field are shown in Fig.", "startOffset": 73, "endOffset": 1484}, {"referenceID": 22, "context": "So, in our future work, we plan to generalize our algorithm to handle richer, high-dimensional sensing data like laser scans and camera images (Natarajan et al. 2012a; 2012b; Natarajan, Low, and Kankanhalli 2014).", "startOffset": 143, "endOffset": 212}, {"referenceID": 27, "context": "The sparse approximation method employed by offline PITC to improve the scalability of the full GP can be similarly applied to computing such a log marginal likelihood scalably, as explained in (Qui\u00f1onero-Candela and Rasmussen 2005) (i.", "startOffset": 194, "endOffset": 232}], "year": 2014, "abstractText": "Central to robot exploration and mapping is the task of persistent localization in environmental fields characterized by spatially correlated measurements. This paper presents a Gaussian process localization (GP-Localize) algorithm that, in contrast to existing works, can exploit the spatially correlated field measurements taken during a robot\u2019s exploration (instead of relying on prior training data) for efficiently and scalably learning the GP observation model online through our proposed novel online sparse GP. As a result, GP-Localize is capable of achieving constant time and memory (i.e., independent of the size of the data) per filtering step, which demonstrates the practical feasibility of using GPs for persistent robot localization and autonomy. Empirical evaluation via simulated experiments with real-world datasets and a real robot experiment shows that GP-Localize outperforms existing GP localization algorithms.", "creator": "TeX"}}}