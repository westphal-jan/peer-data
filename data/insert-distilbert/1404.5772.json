{"id": "1404.5772", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Apr-2014", "title": "Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks", "abstract": "click prediction is one of the fundamental problems in sponsored search. most of existing studies took advantage of machine learning behavioral approaches aiming to predict ad click for each event of ad view independently. however, as observed in the real - world sponsored search system, user's selective behaviors on ads yield high dependency on how the user directly behaved along with the past time, especially in terms of what queries she submitted, what ads either she clicked or ignored, and how long she spent it on the landing pages of clicked ads, etc. inspired by these observations, we introduce a novel framework based on recurrent neural networks ( rnn ). compared to traditional methods, this framework thus directly models place the dependency on user's sequential behaviors into the click prediction process through the recurrent structure in rnn. large parameter scale evaluations on the click - through logs from a commercial search engine demonstrate that our approach can significantly vastly improve the click prediction accuracy, compared to sequence - independent approaches.", "histories": [["v1", "Wed, 23 Apr 2014 10:14:41 GMT  (194kb,D)", "https://arxiv.org/abs/1404.5772v1", "Accepted by AAAI 2014"], ["v2", "Mon, 28 Apr 2014 05:56:18 GMT  (176kb,D)", "http://arxiv.org/abs/1404.5772v2", "Accepted by AAAI 2014"], ["v3", "Mon, 28 Jul 2014 13:59:03 GMT  (175kb,D)", "http://arxiv.org/abs/1404.5772v3", "Accepted by AAAI 2014"]], "COMMENTS": "Accepted by AAAI 2014", "reviews": [], "SUBJECTS": "cs.IR cs.LG cs.NE", "authors": ["yuyu zhang", "hanjun dai", "chang xu", "jun feng", "taifeng wang", "jiang bian", "bin wang 0004", "tie-yan liu"], "accepted": true, "id": "1404.5772"}, "pdf": {"name": "1404.5772.pdf", "metadata": {"source": "CRF", "title": "Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks", "authors": ["Yuyu Zhang", "Hanjun Dai", "Chang Xu", "Jun Feng", "Taifeng Wang", "Jiang Bian", "Bin Wang", "Tie-Yan Liu"], "emails": ["zhangyuyu@ict.ac.cn", "daihanjun@gmail.com", "changxu0731@gmail.com", "feng-j13@mails.tsinghua.edu.cn", "taifengw@microsoft.com", "jibian@microsoft.com", "wangbin@ict.ac.cn", "tyliu@microsoft.com"], "sections": [{"heading": "Introduction", "text": "Sponsored search has been a major business model for modern commercial Web search engines. Along with organic search results, it presents to users with sponsored search results, i.e., advertisements (ads) targeting to the search query. Sponsored search accounts for the overwhelming majority of income for three major search engines: Google, Yahoo and Bing. Even in the US search market alone, it generates over 20 billion dollars per year, the amount of which still keeps rising1. According to the common cost-per-click (CPC) model for sponsored search, advertisers are only charged once their advertisements are clicked by users. In this mechanism, to maximize the revenue for search engine and maintain a desirable user experience, it is crucial for search engines to estimate the click-through rate (CTR) of ads.\n\u2217This work was performed when the first three authors were visiting Microsoft Research Asia. Copyright c\u00a9 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n1Source: eMarketer, June 2013.\nRecently, click prediction has received much attention from both industry and academia (Fain and Pedersen 2006; Jansen and Mullen 2008). State-of-the-art sponsored search systems typically employ machine learning approaches to predict the click probability by using the feature extracted based on: 1) historical CTR for ad impressions 2 with respect to different elements, e.g., CTR of query, ad, user, and their combinations (Graepel et al. 2010; Richardson, Dominowska, and Ragno 2007); 2) semantic relevance between query and ad (Radlinski et al. 2008; Shaparenko, C\u0327etin, and Iyer 2009; Hillard et al. 2011).\nHowever, most of previous works take single ad impression as the input instance to predict the click probability, without considering dependency between different ad impressions. Some recent research work (Xu, Manavoglu, and Cantu-Paz 2010; Xiong et al. 2012) pointed out that they could achieve more accurate click prediction by modeling spatial relationship between ad slots in the same query session. Inspired by them, we conduct further data analysis to study other types of dependency between user\u2019s behaviors in sponsored search, through which we find that user\u2019s behaviors also yield explicit temporal dependency. For example, if a user clicks an ad, comes to the ad landing page, but closes it very quickly, the click probability of her next view of this ad will become fairly low; moreover, if a user has previously submitted a query on booking flight, he/she will be with higher probability to click the ads under flight booking. These findings motivate us to advance the state-ofthe-art of click prediction by modeling the important temporal dependency into the click prediction process. Although some kinds of dependency can be modeled as features, it\u2019s still hard to identify all of them explicitly. Thus, it is necessary to empower the model with the ability to extract and leverage various kinds of dependency automatically.\nIn real-world sponsored search system, the event of any ad impression, click, and corresponding context information (e.g. user query, ad text, click dwell time, etc.) is recorded with the time stamp in search logs. Thus, it is natural to employ time series analysis methods to model sequential dependency between user\u2019s behaviors. Previous studies on\n2We refer to a certain ad shown to a particular user in a specific search result page as an ad impression.\nar X\niv :1\n40 4.\n57 72\nv3 [\ncs .I\nR ]\n2 8\nJu l 2\n01 4\ntime series analysis (Kirchgassner, Wolters, and Hassler 2012; Box, Jenkins, and Reinsel 2013) usually focused on modeling trends or periodic patterns in data series. However, the sequential dependency between ad impressions is so complex and dynamic that time series analysis approaches is not capable enough to model it effectively. On the other hand, a few most recent studies leverage Recurrent Neural Networks (RNN) to model the temporal dependency in data. For example, RNN language model (Mikolov et al. 2010; Mikolov et al. 2011a; Mikolov et al. 2011b) successfully leverages long-span sequential information among the massive language corpus, which results in better performance than traditional neural networks language model (Bengio et al. 2006). Moreover, RNN based handwriting recognition (Graves et al. 2009), speech recognition (Kombrink et al. 2011), and machine translation (Auli et al. 2013) systems have also led to much improvement in the corresponding tasks. Compared to traditional feedforward neural networks, RNN has demonstrated its strong capability to exploit dependencies in the sequence due to its specific recurrent network structure.\nIn this work, we propose to leverage RNN to model sequential dependency into predicting ad click probability. We consider each user\u2019s ad browsing history as one sequence which yields the intrinsic internal dependency. In the training process of RNN model, features of each ad impression will be feedforwarded into the hidden layer, together with previously accumulated hidden state. In this way, the dependency among impressions will be embedded into the recurrent network structure. Our experiments on the large scale data from a commercial search engine reveal that, such RNN structure can give rise to a significant improvement on the click prediction accuracy compared with the state-of-the-art dependency-free models such as Neural Networks and Logistic Regression.\nThe main contributions of this paper are in three folds:\n\u2022 We investigate the sequential dependency among particular user\u2019s ad impressions, and identify several important sequential dependency relationships.\n\u2022 We use Recurrent Neural Networks to model user\u2019s click sequence, and successfully incorporate sequential dependency into enhancing the accuracy of click prediction.\n\u2022 We conduct large scale experiments to validate the RNN model\u2019s effectiveness for modeling sequential data in sponsored search.\nIn the following parts of this paper, we will first present our data analysis results to verify the potential dependency which might affects click prediction. Then, we propose our RNN model for the task of sequence-based click prediction. After that, we describe experimental settings followed by the experimental results and further performance study. At last, we summarize this paper and discuss some future work."}, {"heading": "Data Analysis on Sequential Dependency", "text": "To gain more understanding on why sequential information is important in click prediction, in this section, we will discuss the effects of sequential dependency from multiple per-\nspectives. We collect data for analysis from the logs of a commercial sponsored search system.\nOnce a user clicks an ad, she will enter into the corresponding ad landing page and stay for a certain period of time, which is referred to as the click dwell time. Generally, longer dwell time implies better user experience. For a particular user, all the ad impressions can be organized as an ordered sequence along with the time. To explore the sequential effect of click dwell time, we first pick up all the \u201cfirst clicks\u201d in each user\u2019s sequence, and track whether each ad will be clicked again in its consecutively next impression. The correlation between the previous click dwell time and the current click-through rate3 is shown in Figure 1. From this figure, it is clear to observe an obvious positive correlation, i.e., the longer a user stays on an ad\u2019s landing page, the more likely she will click this ad right at the next time.\nWhen the click dwell time is less than 20 seconds, we call such ad click as a \u201cquick back\u201d one. From the data analysis above, we can observe that \u201cquick back\u201d can give rise to rather lower click-through rate in the next impression, which indicates that users tend to avoid clicking the ad once they had an unsatisfied experience. However, it is not clear how users behave if they experienced a \u201cquick back\u201d click long time ago (e.g., half a month). To explore this, we collect all the \u201cquick back\u201d clicks and calculate the click-through rate after different time intervals, i.e., the time elapsed since the \u201cquick back\u201d click, respectively. Figure 2 shows the result, where the time interval is binning by half-days. As conjectured, along with increasing elapsed time, the overall clickthrough rate grows significantly and then stay steadily in a certain level, which implies that users tend to gradually forget the unsatisfied experience with the time passing.\nBesides studies on the sequential effect of dwell time, we further analyze such effects from the aspect of query. In sponsored search systems, ads are automatically selected by matching with the user submitted query. Queries also form a time ordered sequence, binding on that of ad impressions. After categorizing the queries into topics based on our proprietary query taxonomy, we calculate users\u2019 click-through rate when they submit each query topic for the first time, and compare with their future CTR on subsequent queries of the same query topic. Analysis results, as shown in Figure 3, il-\n3In this paper, we present relative click-through rate to preserve the proprietary information.\nlustrate that if a user has submitted a query belonging to a certain query topic, he/she will become more likely to click the ads under the same topic.\nAll the analysis results above indicate that user\u2019s previous behaviors in sponsored search may cause strong but quite dynamic impact her subsequent behaviors. As long as we can identify such sequential dependency between user behaviors, we can design features accordingly to enhance the click prediction. However, since a big challenge to enumerate such dependency in the data manually, it is necessary to let the model have the ability to learn such kind of dependency by itself. To this end, we propose to leverage a widely used framework, Recurrent Neural Network, as our model, as it naturally embeds dependencies in the sequence."}, {"heading": "The Proposed Framework", "text": ""}, {"heading": "Model", "text": "The architecture of a recurrent neural network is shown in Figure 4. It consists of an input layer i, an output unit, a hidden layer h, as well as inner weight matrices. Here we use t \u2208 N to represent the time stamp. For example, h(t) denotes the hidden state in time t. Specifically, the recurrent connections R between h(t \u2212 1) and h(t) can propagate sequential signals. The input layer consists of a vector i(t) that represents the features of current user behaviors, and the vector h(t\u2212 1) represents the values in the hidden layer computed from the previous step.\nThe activation values of the hidden and output layers are computed as\nh(t) = f(i(t)UT + h(t\u2212 1)RT ),\ny(t) = \u03c3(h(t)V T ),\nwhere f(z) = 1\u2212e \u22122z\n1+e\u22122z is the tanh function we use for nonlinear activation, and \u03c3(z) = 11+e\u2212z is the sigmoid function for predicting the click probability.\nThe hidden layer can be considered as an internal memory which records dynamic sequential states. The recurrent structure is able to capture a long-span history context of user behaviors. This makes RNN applicable to the tasks related to sequential prediction.\nIn our framework, i(t) represents the features correlated to user\u2019s current behavior and h(t) represents sequential information of user\u2019s previous behaviors. Thus, our prediction depends on not only the current input features, but also the sequential historical information."}, {"heading": "Feature Construction", "text": "In our study, we take ad impressions as instances for both model training and testing. Based on the rich impressioncentric information, we construct the input features that can carry crucial information to achieve accurate CTR prediction for a given impression. All these features can be grouped into several general categories: 1) Ad features consist of information about ad ID, ad display position, and ad text relevance with query. 2) User features include user ID and query (submitted by user) related semantic information. 3) Sequential features include time interval since the last impression, dwell time on the landing page of the last click event, and whether the current impression is the head of sequence, i.e., whether it is the first impression for this user.\nWith all these diverse types of features, each ad impression can be described in a large and complex feature space. In this paper, all of the click prediction models will follow this input feature setting, so that we can fairly compare their capabilities of predicting whether an impression will be clicked by user."}, {"heading": "Data Organization", "text": "To effectively harness the sequential information for modeling temporal dependencies, the training process requires large quantity of data. Luckily, the data in computational\nadvertising is big enough to make RNN tractable. To obtain the data for sequential prediction, we re-organize the input features along with the user dimension (i.e., reordered each user\u2019s historical behaviors according to the timeline). In particular, for the ads in the same search session, we rank them by their natural display orders in the mainline and then sidebar."}, {"heading": "Learning", "text": "Loss Function In our work, the loss function is defined as an averaged cross entropy, which aims at maximizing the likelihood of correct prediction,\nL = 1\nM M\u2211 i=1 (\u2212yilog(pi)\u2212 (1\u2212 yi)log(1\u2212 pi)),\nwhere M is the number of training samples. The ith sample is labeled with yi \u2208 {0, 1} and pi is the predicted click probability of the given ad impression.\nLearning Algorithm (BPTT) RNN can be trained in the same way as normal feedforward network using backpropagation algorithm. In this way, basically, the state of the hidden layer from previous time step is simply regarded as an additional input. With only one hidden layer, the network tries to optimize prediction of the next sample given the previous sample and previous hidden state. However, no effort is directly devoted towards longer context information, which may hurt the performance of RNN.\nA simple extension of the training algorithm is to unfold the network and backpropagate errors even further. This is called Back Propagation Through Time (BPTT) algorithm. BPTT was proposed in (Rumelhart, Hinton, and Williams 2002), and has been used in the practical application of RNN language model (Mikolov 2012).\nWe illustrate the overall training pipeline that applies BPTT to the RNN based click prediction models in Figure 4. Such unfolded RNN can be viewed as a deep neural network with T hidden layers where the recurrent weight matrices are shared and identical. In this approach, the hidden layer can actually exploit the information of the most recent inputs and put more importance to the latest input, which is essentially coherent with the the sequential dependency. In the following part, we use T to denote the number of unfolding steps in the BPTT algorithm.\nThe network is trained by Stochastic Gradient Descent (SGD). The gradient of the output layer is computed as\neo(t) = y(t)\u2212 l(t),\nwhere y(t) is the predicted click probability, and l(t) is the binary true label according to the ad is clicked or not. The weights V between the hidden layer h(t) and output unit y(t) are updated as\nV (t+ 1) = V (t)\u2212 \u03b1\u00d7 eo(t)\u00d7 h(t),\nwhere \u03b1 is the learning rate. Then, gradients of errors are propagated from the output layer to the hidden layer as\neh(t) = eo(t)V \u2217 (~1\u2212 h(t) \u2217 h(t)),\nwhere \u2217 represents the element-wise product, and~1 is a vector with all elements equal to one.\nErrors are also recursively propagated from the hidden layer h(t \u2212 \u03c4) to the hidden layer from previous step h(t\u2212 \u03c4 \u2212 1), that is\neh(t\u2212\u03c4\u22121) = eh(t\u2212\u03c4)R\u2217(~1\u2212h(t\u2212\u03c4\u22121)\u2217h(t\u2212\u03c4\u22121)),\nwhere \u03c4 \u2208 [0, T ). The weight matrix U and the recurrent weights R are then updated as\nU(t+ 1) = U(t)\u2212 \u03b1 [ T\u22121\u2211 z=0 eh(t\u2212 z)T i(t\u2212 z) ] ,\nR(t+ 1) = R(t)\u2212 \u03b1 [ T\u22121\u2211 z=0 eh(t\u2212 z)Th(t\u2212 z \u2212 1) ] .\nNote that, in our practical experiments, we add the bias terms and L2 penalty of weights to the model, and the gradients can still be computed easily based on a slight modification to the equations above."}, {"heading": "Inference", "text": "In contrast to traditional neural networks, RNN has a recurrent layer to store the previous hidden state. In the inference phase, we still need to store the hidden state of previous test sample, and feedforward it with the recurrent weights R.\nFigure 5 illustrates the testing process. The test data is also organized as ordered user behavior sequences. We feedforward current sample features, together with the hidden state of previous sample to get the current hidden state. Then we make the prediction and replace the stored hidden state with current values. Here we only record the hidden state of the last test sample, no matter how many unfolding steps there are in the BPTT training process."}, {"heading": "Experiments", "text": "This section first describes the settings of our experiments, and then reports the experimental results."}, {"heading": "Data Setting", "text": "To validate whether the RNN model we proposed can really help enhance the click prediction accuracy, we conduct a series of experiments based on the click-through logs of a commercial search engine. In particular, we collect half a\nmonth logs from November 9th to November 22nd in 2013 as our experimental dataset. And, we randomly sample a set of search engine users (fully anonymized) and collect their corresponding events from the original whole traffic. Finally, we collect over 7 million ad impressions in this period of time. After that, we use the first week\u2019s data to train click prediction models, and apply those models to the second week\u2019s data for testing. Detailed statistics of the dataset can be found in Table 1."}, {"heading": "Evaluation Metrics", "text": "In our work, there are multiple models to be applied to predict the click probability for ad impressions in the testing dataset. We use recorded user actions, i.e., click or non-click, in logs as the true labels. To measure the overall performance of each model, we follow the common practice in previous click prediction research in sponsored search and employ Area Under ROC Curve (AUC) and Relative Information Gain (RIG) as the evaluation metrics (Graepel et al. 2010; Xiong et al. 2012; Wang et al. 2013)."}, {"heading": "Compared Methods", "text": "In order to investigate the model effectiveness, we compare the performance of our RNN model with other classical click prediction models, including Logistic Regression (LR) and Neural Networks (NN), with identical feature set as described aforementioned. We set LR and NN as baseline models due to the following reasons: 1) Quite a few previous studies (Richardson, Dominowska, and Ragno 2007; McMahan et al. 2013; Wang et al. 2013) have demonstrated that they are state-of-the-art models for click prediction in sponsored search. 2) LR and NN models ignore the sequential dependency among the data, while our RNN based framework is able to model such information. Through the comparison with them, we will see whether RNN can successfully leverage dependencies in the data sequence to help improve the accuracy of click prediction."}, {"heading": "Experimental Results", "text": "Overall Performance For fair model comparison, we carefully select the parameters of each model with cross validation, and ensure every model achieve its best performance\nrespectively. To be more specific, parameters for grid search include: the coefficient of L2 penalty, the number of training epochs, the hidden layer size for RNN and NN models, and the number of unfolding steps for RNN. Finally, we get the best settings of parameters as follows: the coefficient of L2 penalty is 1e \u2212 6, the number of training epochs is 3, the hidden layer size is 13, and the number of unfolding steps should be 3 (more details will be provided later).\nTable 2 reports the overall AUC and RIG of all three methods on test dataset. It demonstrates that our proposed RNN model can significantly improve the accuracy of click prediction, compared with baseline approaches. In particular, in terms of RIG, there is about 17.3% relative improvement over LR, and about 10% relative improvement over NN. As for the metric of AUC, we can find there is about 1.7% relative gain over LR, and about 0.5% relative gain over NN. In real sponsored search system, such improvement in click prediction accuracy will lead to a significant revenue increment.\nThe overall performance above shows the effectiveness of our RNN model, which clearly transcends sequence independent models. Next, we will conduct detailed analysis on how the sequential information help to get more accurate click prediction.\nPerformance on Specific Ad Positions It is well-known that the click-through rate on different ad positions varies a lot, which is often referred to as the position bias. To further check the performance of models within specific positions, in our experiments, we separately analyze the performance of RNN model and two baseline algorithms on different ad positions: top first, mainline and sidebar.\nFigure 6 shows the evaluation results on different positions. In Figure 6(a), RNN outperforms NN and LR measured by AUC on all positions. In Figure 6(b), in terms of RIG, RNN achieves impressive relative gain over NN and LR, especially on mainline positions, where RNN beats NN by 3.12%. According to our statistics on daily traffic data, most of revenue comes from the mainline ad clicks, where RNN can achieve significantly better performance. While for sidebar positions, the ads shown there are easily to be ignored by users, so that the clicks or positive instances are very rare. This may drastically hurt the performance of LR model. Nevertheless, the RNN model still performs the best, which indicates that even in rare cases, sequential information can still help.\nEffect of Recurrent State for Inference We have shown the inference method of RNN in Figure 5. To further verify the importance of utilizing historical sequences in the inference phase, we remove the recurrent part of the RNN model after training, and feedforward the testing samples as a normal NN, which means that we just ignore the sequential dependencies in the testing phase. Finally, the AUC is 88.25% and RIG is 18.95%. Compared to Table 2, we can observe a severe drop of the performance of RNN model, which shows that the sequential information is indeed embedded in the recurrent structure and significantly contribute to the prediction accuracy.\nPerformance with Long vs. Short History In this part, we conduct experiments to check the model performance with different length of history. We first collect all available user sequences whose length is larger than a threshold T . In these sequences, the first T samples in each sequences are fed into model to serve as the \u201caccumulation period\u201d. Then, we continue feeding and testing samples for the rest part of each sequence, and calculate the AUC and RIG on all those rest parts. In such setting, the user sequences which are selected with a larger threshold T have longer history to feed as \u201caccumulation period\u201d. By doing so, we aim to verify whether our RNN model can maintain more robust sequential information in longer sequences.\nFigure 7 shows the results. Just as expected, it turns out that our RNN model performs the best in all settings. Moreover, when the \u201caccumulation period\u201d gets longer, our RNN model tends to achieve even more relative gain compared to baseline models. This result validates the capability of RNN to capture and accumulate sequential information, especially for long sequences, and help further improve the accuracy of click prediction.\nEffect of RNN Unfolding Step As described in the framework section, the unfolding structure plays an import role in RNN training process with BPTT algorithm. Since unfolding can directly incorporate several previous samples, and the depth of such explicit sequence modeling is determined by the steps of unfolding, it is necessary to delve into the effect of various RNN unfolding steps. This further analysis can help us better understand the property of the RNN model.\nAccording to our experimental results, the prediction ac-\ncuracy surges along with the increasing unfolding steps at the beginning. The best AUC and RIG can be achieved when unfolding 3 steps, after which the performance drops. By checking the error terms during the process of BPTT, we discover that the backpropagated error vanishes after 3 steps of unfolding, which explains why larger unfolding step is detrimental.\nWith these observations, intuitively, our RNN based framework can model sequential dependency in two ways: short-span dependency by explicitly learning from current input and several of its leading inputs through unfolding; long-span dependency by implicitly learning from all previous input, accumulated or embedded in the weights of the recurrent part. Meanwhile, in sponsored search, user\u2019s behavior is also affected by both the very recent events as explicit factor and the long-run history as implicit (background) factor. This reveals the intrinsic reason why RNN works so well for the sequential click prediction."}, {"heading": "Conclusion and Future Work", "text": "In this paper, we propose a novel framework for click prediction based on Recurrent Neural Networks. Different from traditional click prediction models, our method leverages the temporal dependency in user\u2019s behavior sequence through the recurrent structure. A series of experiments show that our method outperforms state-of-the-art click prediction models in various settings. In this future, we will continue this direction in several aspects: 1) The sequence is currently built on user level. We will study different kinds of sequence building methods, e.g. by (user, ad) pair, (user, query) pair, advertiser, or even merge all users on the level of whole system. 2) We are going to deduce the meaning of dependency learnt by RNN via deep understanding of RNN structure. This may help up better utilize the property of the recurrent part. 3) Recently, some research work (Hermans and Schrauwen 2013) has been done on Deep Recurrent Neural Networks (DRNN) and shows good results. We plan to study whether \u201cdeep\u201d structure can also help in click prediction, together with the \u201crecurrent\u201d structure."}], "references": [{"title": "Joint language and translation modeling with recurrent neural networks. EMNLP", "author": ["Auli"], "venue": null, "citeRegEx": "Auli,? \\Q2013\\E", "shortCiteRegEx": "Auli", "year": 2013}, {"title": "Neural probabilistic language models", "author": ["Bengio"], "venue": "In Innovations in Machine Learning", "citeRegEx": "Bengio,? \\Q2006\\E", "shortCiteRegEx": "Bengio", "year": 2006}, {"title": "Time series analysis: forecasting and control", "author": ["Jenkins Box", "G.E. Reinsel 2013] Box", "G.M. Jenkins", "G.C. Reinsel"], "venue": null, "citeRegEx": "Box et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Box et al\\.", "year": 2013}, {"title": "Sponsored search: A brief history. Bulletin of the American Society for Information Science and Technology 32(2):12\u201313", "author": ["Fain", "D.C. Pedersen 2006] Fain", "J.O. Pedersen"], "venue": null, "citeRegEx": "Fain et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fain et al\\.", "year": 2006}, {"title": "Web-scale bayesian clickthrough rate prediction for sponsored search advertising in microsoft\u2019s bing search engine", "author": ["Graepel"], "venue": "In Proceedings of the 27th International Conference on Machine Learning (ICML-10),", "citeRegEx": "Graepel,? \\Q2010\\E", "shortCiteRegEx": "Graepel", "year": 2010}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Graves"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 31(5):855\u2013868", "citeRegEx": "Graves,? \\Q2009\\E", "shortCiteRegEx": "Graves", "year": 2009}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Hermans", "M. Schrauwen 2013] Hermans", "B. Schrauwen"], "venue": null, "citeRegEx": "Hermans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2013}, {"title": "The sum of its parts: reducing sparsity in click estimation with query segments. Information Retrieval 14(3):315\u2013336", "author": ["Hillard"], "venue": null, "citeRegEx": "Hillard,? \\Q2011\\E", "shortCiteRegEx": "Hillard", "year": 2011}, {"title": "Sponsored search: An overview of the concept, history, and technology", "author": ["Jansen", "B.J. Mullen 2008] Jansen", "T. Mullen"], "venue": "International Journal of Electronic Business", "citeRegEx": "Jansen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jansen et al\\.", "year": 2008}, {"title": "Introduction to modern time series analysis", "author": ["Wolters Kirchgassner", "G. Hassler 2012] Kirchgassner", "J. Wolters", "U. Hassler"], "venue": null, "citeRegEx": "Kirchgassner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kirchgassner et al\\.", "year": 2012}, {"title": "Recurrent neural network based language modeling in meeting recognition", "author": ["Kombrink"], "venue": null, "citeRegEx": "Kombrink,? \\Q2011\\E", "shortCiteRegEx": "Kombrink", "year": 2011}, {"title": "Ad click prediction: a view from the trenches", "author": ["McMahan"], "venue": null, "citeRegEx": "McMahan,? \\Q2013\\E", "shortCiteRegEx": "McMahan", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Mikolov"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov,? \\Q2010\\E", "shortCiteRegEx": "Mikolov", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["Mikolov"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov,? \\Q2011\\E", "shortCiteRegEx": "Mikolov", "year": 2011}, {"title": "Rnnlm-recurrent neural network language modeling toolkit", "author": ["Mikolov"], "venue": "In Proc. IEEE workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "Mikolov,? \\Q2011\\E", "shortCiteRegEx": "Mikolov", "year": 2011}, {"title": "Optimizing relevance and revenue in ad search: a query substitution approach", "author": ["Radlinski"], "venue": "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development", "citeRegEx": "Radlinski,? \\Q2008\\E", "shortCiteRegEx": "Radlinski", "year": 2008}, {"title": "Predicting clicks: estimating the click-through rate for new ads", "author": ["Dominowska Richardson", "M. Ragno 2007] Richardson", "E. Dominowska", "R. Ragno"], "venue": "In Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "Richardson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2007}, {"title": "Learning representations by back-propagating errors. Cognitive modeling 1:213", "author": ["Hinton Rumelhart", "D.E. Williams 2002] Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 2002}, {"title": "Data-driven text features for sponsored search click prediction", "author": ["\u00c7etin Shaparenko", "B. Iyer 2009] Shaparenko", "\u00d6. \u00c7etin", "R. Iyer"], "venue": "In Proceedings of the Third International Workshop on Data Mining and Audience Intelligence for Advertising,", "citeRegEx": "Shaparenko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shaparenko et al\\.", "year": 2009}, {"title": "Exploring consumer psychology for click prediction in sponsored search", "author": ["Wang"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM", "citeRegEx": "Wang,? \\Q2013\\E", "shortCiteRegEx": "Wang", "year": 2013}, {"title": "Relational click prediction for sponsored search", "author": ["Xiong"], "venue": "In Proceedings of the fifth ACM international conference on Web search and data mining,", "citeRegEx": "Xiong,? \\Q2012\\E", "shortCiteRegEx": "Xiong", "year": 2012}, {"title": "Temporal click model for sponsored search", "author": ["Manavoglu Xu", "W. Cantu-Paz 2010] Xu", "E. Manavoglu", "E. Cantu-Paz"], "venue": "In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}], "referenceMentions": [], "year": 2014, "abstractText": "Click prediction is one of the fundamental problems in sponsored search. Most of existing studies took advantage of machine learning approaches to predict ad click for each event of ad view independently. However, as observed in the real-world sponsored search system, user\u2019s behaviors on ads yield high dependency on how the user behaved along with the past time, especially in terms of what queries she submitted, what ads she clicked or ignored, and how long she spent on the landing pages of clicked ads, etc. Inspired by these observations, we introduce a novel framework based on Recurrent Neural Networks (RNN). Compared to traditional methods, this framework directly models the dependency on user\u2019s sequential behaviors into the click prediction process through the recurrent structure in RNN. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that our approach can significantly improve the click prediction accuracy, compared to sequence-independent approaches.", "creator": "LaTeX with hyperref package"}}}