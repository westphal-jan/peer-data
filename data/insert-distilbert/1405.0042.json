{"id": "1405.0042", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2014", "title": "Learning with Incremental Iterative Regularization", "abstract": "we study the learning algorithm bounds corresponding to the incremental gradient descent defined by the empirical risk over an infinite sample dimensional hypotheses space. we consider implementing a statistical learning setting and... show that, provided with a universal step - size and a suitable early stopping rule, the learning algorithm thus obtained is universally consistent and derive finite sample bounds. our results truly provide a theoretical foundation for considering early stopping rule in online learning prevention algorithms and shed ample light on the effect mechanism of allowing for multiple passes over the data.", "histories": [["v1", "Wed, 30 Apr 2014 21:48:34 GMT  (58kb)", "http://arxiv.org/abs/1405.0042v1", "23 pages,1 figure"], ["v2", "Mon, 15 Jun 2015 13:12:12 GMT  (140kb,D)", "http://arxiv.org/abs/1405.0042v2", "30 pages"]], "COMMENTS": "23 pages,1 figure", "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC math.PR", "authors": ["lorenzo rosasco", "silvia villa"], "accepted": true, "id": "1405.0042"}, "pdf": {"name": "1405.0042.pdf", "metadata": {"source": "CRF", "title": "REGULARIZATION BY EARLY STOPPING FOR ONLINE LEARNING ALGORITHMS", "authors": ["LORENZO ROSASCO", "SILVIA VILLA"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n00 42\nv1 [\nst at\n.M L\n] 3\n0 A\npr 2\n01 4\nKey words. Online learning, incremental gradient descent, consistency\n1. Introduction. Early stopping is a widely used heuristic to achieve regularization in online algorithms for supervised learning, see e.g. [22]. However, its theoretical foundation is still poorly understood. While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning. Here, with online learning algorithms, we refer broadly to iterative procedures that have access, at each iteration, to the gradient of a loss function at a single input-output pair.\nWe consider the statistical learning theory framework and are interested in online learning algorithms for expected risk minimization. The situation typically analyzed in this context is the one where each iteration of the algorithm corresponds to a new input-output pair [20]. In this paper, we are interested in allowing the algorithm to do multiple passes over the data (epochs). Indeed, the effect of the number of epochs to the generalization performance of the algorithm is the subject of our study. The procedure we consider can be seen as the incremental gradient descent (IGD) defined by the empirical risk. However, unlike typical analyses of this method, see e.g. [25], we are not interested in convergence of the algorithm to the minimum of the empirical risk, but rather to the one of the expected risk. More precisely, we consider the least squares loss and a hypothesis space which is a (reproducing kernel) Hilbert space. In this setting the minimization of the expected risk is a convex, but not strongly convex, problem and the domain where we minimize is neither compact nor bounded. In particular, the gradients are not bounded, unlike in most studies in stochastic optimization [26]. The instance of IGD we consider is defined by an unpenalized empirical risk and the step size is fixed to a universal constant, so that the only free parameter in the algorithm is effectively the number of epochs. Indeed, we show that better generalization is achieved if multiple, although finitely many, passes over the data are considered. Our main results show that: 1) provided with a suitable stopping rule, the IGD algorithm is universally consistent, 2) finite sample bounds can be derived under a suitable smoothness assumption, and 3) model selection via hold-out cross validation, which is typically used in practice, can adaptively achieve the same generalization guarantees. A detailed discussion of previous works is deferred to Section 3.2. Here we note that, the results more closely related to the analysis we present are those in [2, 34, 38] where consistency and finite sample bounds are derived. In [2] a single pass over the data is shown to suffice, however the analysis is restricted to a finite dimensional setting, and the generalization performance is empirically shown to be still increasing after the first epoch. In [34] and [38] the step size and/or a penalization parameter need to be chosen in a distribution dependent way (or by cross validation) and multiple passes over\n\u2217 DIBRIS, Universita\u0300 di Genova, Via Dodecaneso, 35, 16146, Genova, Italy, (lrosasco@mit.edu) \u2020 LCSL, Istituto Italiano di Tecnologia and Massachusetts Institute of Technology, Bldg. 46-5155, 77 Mas-\nsachusetts Avenue, Cambridge, MA 02139, USA, (atacchett@mit.edu, Silvia.Villa@iit.it)\nthe data are not allowed. When compared to these methods, we note that the algorithm we consider naturally computes the whole regularization path. While it is possible to use a penalization parameter, or even the step size \u03b3, as a regularization parameter, with these choices computing the regularization path incurs in an extra computational cost. This cost is avoided when considering early stopping by exploiting a built-in warm-restart property.\nThe rest of the paper is organized as follows. In Section 2, we discuss the setting we consider and in Section 3 we introduce and discuss the IGD algorithm. The main results are presented in Section 4, while their proofs are discussed in Section 6. Section 5 discusses future work. Preliminary results and the rest of the proofs are given in the Appendices A, B, and C. Finally, Appendix D studies finite sample bounds in the hypotheses space norm.\nNotation We denote by R+ = [0,+\u221e[ and R++ = ]0,+\u221e[ . Given a normed space B and linear operators (Ai)1\u2264i\u2264m, Ai : B \u2192 B for every i, their composition Am \u25e6 \u00b7 \u00b7 \u00b7 \u25e6A1 will be denoted as\n\u220fm i=1 Ai. By convention, we set \u220fm i=m+1 Ai = I , where I is the identity\nof B. The operator norm will be denoted by \u2016 \u00b7 \u2016 and the Hilbert-Schmidt norm by \u2016 \u00b7 \u2016HS .\n2. Set-Up: Supervised Learning. We study the supervised learning problem. Consider an input space X , which is a measurable space, and an output space1 Y = R. Let Z = X \u00d7Y be endowed with a probability measure \u03c1, \u03c1X denote the marginal measure on X , and \u03c1(\u00b7|x) the conditional measure on Y given x \u2208 X . The conditional probability is well defined (see e.g. [32] Lemma A.3.16) and we denote by f\u03c1 = \u222b Y yd\u03c1(y|\u00b7) the conditional expectation, namely the regression function. We assume the distribution to be fixed, but known only through a set z = {(x1, y1), . . . , (xm, ym)} of m independent samples identically distributed according to \u03c1. The set z is called a training set and, for technical reasons, we assume m \u2265 2. With an abuse of notation, (\u2200m \u2208 N), the measure \u03c1m will be denoted by P.\nFurther, consider a separable reproducing kernel Hilbert space (RKHS) H [1], with inner product (norm) denoted by \u3008\u00b7, \u00b7\u3009H (\u2016\u00b7\u2016H), and a measurable reproducing kernelK : X\u00d7X \u2192 R. The function t 7\u2192 K(x, t) \u2208 H is denoted by Kx. Throughout the paper we make the following boundedness assumptions.\nASSUMPTION 1. Assume that there exist M > 0 and \u03ba > 0 such that supp{\u03c1(\u00b7|x)} \u2286 [\u2212M,M ], and \u221a\nK(x, x) < \u03ba, for almost all x \u2208 X . In the above setting, we are interested in solving the expected risk minimization problem,\ninf f\u2208H\nE(f), E(f) = \u222b (y \u2212 f(x))2d\u03c1(x, y) . (2.1)\nNote that the corresponding optimization problem is convex (but in general not strongly convex), the domain of optimization is neither compact nor bounded, and, especially, the gradient of the objective function is unbounded. Moreover, we do not assume the infimum to be achieved.\nWe are interested in approximate solutions f\u0302 which satisfy the following requirement\n(\u2200\u03c1), (\u2200\u01eb > 0), lim m\u2192\u221e P\n(\nE(f\u0302)\u2212 inf f\u2208H\nE(f) > \u01eb ) = 0. (2.2)\nWhen H is universal [32] this is exactly universal consistency, see for example [15]. In the following, with a slight abuse of terminology, we will still refer to the condition in equation (2.2) as to universal consistency, even when the kernel is not universal. Moreover, we are interested in strengthening the above requirement to replace convergence in probability with\n1Most results extend naturally to Y being a separable Hilbert space, e.g. RT .\nalmost sure convergence, as well as to prove convergence in probability uniformly with respect to \u03c1. As it is known, the latter result can only be derived under suitable assumptions on \u03c1 and it is equivalent to considering learning rates [16, 31].\nREMARK 2.1. When the set of minimizers of E is not empty, one could also consider consistency in the RKHS norm, that is\n(\u2200\u03c1), (\u2200\u01eb > 0), lim m\u2192\u221e P\n( \u2016f\u0302 \u2212 fH\u2016H > \u01eb ) = 0, (2.3)\nwhere fH is the minimum norm minimizer of E on H. The analysis of this case is analogous to the one required to prove (2.2), therefore we leave the results and the corresponding proofs in Appendix D.\nEXAMPLE 2.2 (The Linear Case). A particular case of the above setting is the one where X is a Euclidean space and K is taken to be the associated inner product. If H is the corresponding RKHS, we have that f \u2208 H if and only if (\u2200x \u2208 X ) f(x) = \u3008w, x\u3009, for some w \u2208 X and \u2016f\u2016H = \u2016w\u2016. Assumption 1 is satisfied if the marginal distribution \u03c1X is supported in a ball of radius \u03ba.\nREMARK 2.3 (Stochastic Optimization (SO)). In SO given a probability space (Z, \u03c1), a separable Hilbert space B, and a loss function L : Z \u00d7 B \u2192 [0,\u221e), we are interested in solving\ninf h\u2208B\n\u222b\nL(z, h)d\u03c1(z).\nIn this view, supervised learning corresponds to setting Z = X \u00d7 Y , B = H and L(z, f) = (y \u2212 f(x))2. Note however that due to the lack of strong convexity, unboundedness of X and unboundedness of the gradients, the classical assumptions required to apply stochastic gradient descent are not satisfied [26].\n3. Early Stopping for Online Learning Algorithms. We consider the estimator obtained applying the incremental gradient descent (IGD) algorithm [5, 4] to empirical risk minimization,\ninf f\u2208H E\u0302(f), E\u0302(f) = 1 m\nm \u2211\ni=1\n(yi \u2212 f(xi))2. (3.1)\nGiven f\u0302t \u2208 H an iteration of IGD generates f\u0302t+1 according to the recursion,\nf\u0302t+1 = \u03c8\u0302 m t , (3.2)\nwhere \u03c8\u0302mt is obtained at the end of one cycle, namely as the last step of the recursion\n\u03c8\u03020t = f\u0302t; \u03c8\u0302 i t = \u03c8\u0302 i\u22121 t \u2212 \u03b3t m (\u03c8\u0302i\u22121t (xi)\u2212 yi)Kxi , i = 1, . . . ,m (3.3)\nfor a suitable sequence of stepsizes {\u03b3t}t\u2208N, \u03b3t \u2208 R++. In this paper, we consider a sequential approach, where each point of the training set is selected exactly once within each cycle. Each iteration, called epoch, corresponds to one pass over data. In practice, other approaches, e.g. stochastic [25], can be considered and might lead to different behaviors, but we leave the analysis of these latter cases for future study.\nThe iteration in (3.2)-(3.3) can be readily implemented if we consider a linear kernel, see Example 2.2, or any kernel K(x, x\u2032) = \u3008\u03a6(x),\u03a6(x\u2032)\u3009\nRD defined by a finite dimensional\nfeature map \u03a6 : X \u2192 RD. In the latter case we can simply identify the function f\u0302t+1 : x 7\u2192\n\u3008w\u0302t+1,\u03a6(x)\u3009RD with w\u0302t+1 \u2208 RD. For a general kernel, it can be easily seen that, for f\u03020 = 0, the solution after t epochs can be written as f\u0302t(\u00b7) = \u2211m k=1(\u03b1t)kKxk , for suitable coefficients \u03b1t = ((\u03b1t)1, . . . , (\u03b1t)m) \u2208 Rm, given by the following recursion, \u03b1t+1 = c m t\nc0t = \u03b1t, (c i t)k =\n{ (ci\u22121t )k \u2212 \u03b3tm ( \u2211m j=1 K(xi, xj)(c i\u22121 t )j \u2212 yi ) , k = i\n(ci\u22121t )k, k 6= i The above algorithm is closely related, yet different, to the one discussed in [19], where an explicit regularization is considered (choose \u03bbk > 0 in the following Equation (3.4)). More precisely, consider the following online learning algorithm\nh\u0302k+1 = h\u0302k \u2212 \u03b3k m (h\u0302k(xik )\u2212 yik)Kxki + \u03bbkh\u0302k, k \u2208 N (3.4)\nwhere {\u03bbk}k\u2208N and {\u03b3k}k\u2208N are suitable sequences such that (\u2200k \u2208 N) \u03bbk \u2208 R+ and \u03b3k \u2208 R++, and ik \u2208 {1, . . . ,m}. We note that different kind of analyses can be carried out when considering the iteration (3.4). The incremental gradient descent algorithm (3.2)-(3.3) is an instance of the general iterative procedure (3.4). Indeed, it corresponds to the case where the gradient descent step is taken cyclically more than once with respect to the same point (ik = k mod m), we do not impose any explicit regularization setting \u03bbk = 0, and we keep \u03b3k constant on each pass over the training set. With this choice, for a given common initialization, we have f\u0302t = h\u0302mt, for all t. In the following we are specifically interested to study the effect of multiple passes over the data, when the goal is to minimize the expected, rather than the empirical, risk (see discussion in Section 3.2). Towards this end, we precisely consider the situation where the step size is chosen a priori in a distribution independent way and investigate the regularization effect of the number of epochs. Indeed, we show that early stopping, that is considering multiple \u2013 yet finitely many\u2013 epochs, is useful. Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.\n3.1. Early stopping, Learning and Computations. Recent interest in providing a better understanding of regularization by early stopping is largely motivated by its practical use. Indeed, early stopping has long been applied as a heuristic to achieve regularization, especially in the context of neural networks [22]. In particular, it is natural to consider early stopping when dealing with big data-sets, since in this context algorithms should ideally have computational requirements tailored to the generalization properties allowed by the data. This is exactly the main feature of early stopping regularization. Figure 1 gives an illustration of this fact in a numerical simulation. Here, we added an explicit regularization parameter \u03bb as in [19], see also (3.4), and plot the error of the estimator on a hold-out set, against the number of epochs for a few values of \u03bb. At least two interesting observations can be made. First, the smaller is \u03bb the fewer epochs are needed to achieve the best generalization. Among all choices, fixing \u03bb = 0 achieves comparable generalization with the minimal amount of computation (iterations). Second, the algorithm we consider naturally computes the whole regularization path. While it is possible to use \u03bb, or even the step size \u03b3, as a regularization parameter (see below), computing the regularization path incurs in an extra cost. This cost is avoided by the iterative procedure in (3.2),(3.3) by virtue of the natural warm-restart property which is built-in.\nBefore discussing our main results, we briefly compare IGD with other online learning schemes.\n3.2. Previous Work. Online learning algorithms have been analyzed within different settings. Since our perspective is perhaps somewhat different (we use IGD to solve expected, rather than empirical, risk minimization), we provide a brief overview of previous theoretical studies.\nMinimization of the expected risk.. The question of the consistency of online learning algorithms when H can be infinite dimensional has been considered in [34, 38] (see also references therein). Both papers deal with an infinite sequence of i.i.d. training points (m = +\u221e) and fix ik = k: no epochs are considered and the data points are seen only once (each iteration correspond to a point). In [34] it is shown that if the step size \u03b3k and the regularization parameter \u03bbk are chosen as suitable functions of the number of points, then the corresponding algorithm can be universally consistent with probability one. Moreover, under suitable smoothness assumption, see (4.2), learning rates are derived if both (\u03b3k)k\u2208N and (\u03bbk)k\u2208N are chosen depending on smoothness. In [38] it is shown that indeed similar results can be derived for \u03bbk = 0 for all k \u2208 N, but an horizon, that is the total number of points to be considered, needs to be known a priori to appropriately choose the step-size. Recently, a similar analysis, \u03bbk = 0 for all k \u2208 N, is developed in [2] in a finite dimensional setting. In this case it is shown that a suitable fixed step-size suffices to ensure convergence (as well as convergence rates) only with one pass over the data, however this is a by-product of considering a finite dimensional setting. Finally we note that another possibility is to consider the case where \u03bb is kept fixed (and different from zero). In this case, convergence of algorithm (3.4) to the solution of the regularized expected risk minimization problem,\nmin f\u2208H\nE(f) + \u03bb\u2016f\u20162H.\nis derived. These latter results could be coupled with an analysis of the (approximation) error minf\u2208H E(f) + \u03bb\u2016f\u20162H \u2212 infH E(f) to derive convergence for the expected risk.\nMinimization of the empirical risk.. In optimization theory, the IGD iteration is used to minimize objective functions which are the sum of m functions. In our case, by definition, we have\nE\u0302(f) = m \u2211\ni=1\nVi(f), Vi(f) = 1\nm (yi \u2212 f(xi))2.\ntherefore the IGD could be applied to minimize the empirical risk, i.e. problem (3.1). Results in this context focus on showing that E\u0302(f t) \u2212 inff\u2208H E\u0302(f) converges to zero when t\nincreases, see [5] and references therein. Related results consider the case where the empirical risk is replaced by a regularized empirical risk, see e.g. [21]. In this paper, we propose a novel perspective showing how IGD can be used to approximately solve the expected risk minimization, i.e. problem (2.1) rather than the empirical risk.\nSequential Prediction. Finally, we note that iterative schemes as in (3.4) have been recently extensively studied in online learning (a.k.a. sequential prediction problems), see e.g. [10]. In this context, the data are not stochastic, and the goal is to control the so called regret. If the data are indeed generated by a stochastic process, then there is a classic approach, sometimes called online-to-batch conversion [9], to convert regret bounds to expected risk bounds. However, this latter approach does not seem to be appropriate for studying the effect of multiple passes over the data.\n4. Main Results. We begin by stating the universal consistency of (cyclic) IGD with early stopping.\nTHEOREM 4.1. Let \u03b8 \u2208 [0, 1[ , and (\u2200t \u2208 N), \u03b3t = \u03ba\u22122(t + 1)\u2212\u03b8. Assume that (f\u0302t)t\u2208N is defined in (3.2)-(3.3). If we choose a stopping rule t\u2217 = t\u2217(m) so that\nlim m\u2192+\u221e t\u2217(m) = +\u221e and lim m\u2192+\u221e t\u2217(m)4(1\u2212\u03b8)/m = 0 (4.1)\nthen\nlim m\u2192\u221e E(f\u0302t\u2217) = inf f\u2208H E(f) almost surely.\nThe above result shows that consistency is achieved computing a suitable number t\u2217(m) of iterations of IGD given m points. The number of required iterations tends to infinity as the number of available training points increases. In particular, this excludes the choice t\u2217(m) = 1 for all m, namely considering only one pass over the data. Condition (4.1) can be interpreted as an early stopping rule, since it requires the number of epochs not to grow too fast. Below, we further discuss how a suitable stopping rule can be derived if the problem satisfies certain prior assumptions. We note that several choices of the step-size are allowed in Theorem 4.1. In particular it is possible to choose a constant step-size, see also the discussion below. We next consider finite sample bounds that can be derived if the regression function satisfies the following smoothness assumption,\n\u2225 \u2225L\u2212rf\u03c1 \u2225 \u2225 \u03c1 \u2264 R, for some r > 0. (4.2)\nwhere L : L2(X , \u03c1X ) \u2192 L2(X , \u03c1X ) is such that Lf(x) = \u222b X f(x)K(x, x \u2032)d\u03c1X and \u2016 \u00b7 \u2016\u03c1 denotes the norm in L2(X , \u03c1X ) = {f : X \u2192 R : \u222b\nd\u03c1X |f |2 < \u221e}. Assumption (4.2) is fairly standard (see [13], and [12, Section 4] for a discussion).\nTHEOREM 4.2. Let (\u2200t \u2208 N), \u03b3t = \u03ba\u22122(t + 1)\u2212\u03b8, for some \u03b8 \u2208 [0, 1[ . Assume that (4.2) holds, and fix \u03b4 \u2208 ]0, 1[. Then, with probability at least 1\u2212 \u03b4,\n(\u2200t \u2208 N) E(f\u0302t)\u2212 inf H\nE \u2264 C\u03b8 log2( 2 \u03b4 ) (t+ 1)4(1\u2212\u03b8) m + Crt \u22122r(1\u2212\u03b8) (4.3)\nwhere C\u03b8 = C2/2\u03ba6(1\u2212 \u03b8)4 for some C \u2208 R++ which does not depend on m, t, \u03b4, and Cr = R 2(2r/e)4r/\u03ba4(1\u2212 \u03b8)2. Moreover, if we choose the stopping rule\nt\u2217(m) = \u2308m 1(4+2r)(1\u2212\u03b8) \u2309 (4.4)\nthen, with probability at least 1\u2212 \u03b4,\nE(f\u0302t\u2217(m))\u2212 inf H\nE \u2264 D ( log 2\n\u03b4\n)2\nm\u2212 r r+2 , (4.5)\nwhere the constant D \u2208 R++ can be explicitly given. Equation (4.3) arises from a form of bias-variance (sample-approximation) decomposition of the error. Choosing the number of epochs that optimize the bound (4.3), we derive an a priori stopping rule (4.4) and a corresponding bound (4.5). Again, this result confirms that the number of epochs acts as a regularization parameter and the best choice following from Equation (4.3) suggests multiple passes over the data to be beneficial. Interestingly, the best distribution independent choice of the step-size is a constant with respect to t, that is \u03b8 = 0. Such a bound can be compared to known lower bounds as well as previous results for least squares algorithms that can be studied under the same prior. Lower bounds are known [7, 33] under assumption (4.2) and further assuming that the eigenvalues of L have a polynomial decay, that is\n(\u03c3i)i\u2208N \u223c i\u2212b, for some b \u2208 [1,\u221e[ . (4.6)\nThis latter property can be interpreted as a measure of the effective dimensionality of the hypotheses space [40, 7]. There are a few special regimes of interest; assuming r \u2265 1/2 in (4.2) implies that the infimum of the expected risk over H is achieved [11], and it is known that sharp bounds are harder to get if r < 1/2 (see discussion in [33]). For b = 1, the condition (\u03c3i)i\u2208N = O(i\u2212b) always holds. This is the situation we consider in this paper, and it is sometimes called the capacity independent setting. The lower bound, under assumptions (4.2), (4.6), are of order O(n\u2212 2rb 2rb+1 ), and O(n\u2212 2r 2r+1 ) in the capacity independent setting. The bound in (4.3) is not optimal as a consequence of a bad dependence of a suitably defined sample error estimate on the number of epochs. Deriving this bound is the main technical contribution of the paper and the proof is rather involved. While the dependence of the bound on the number of points is optimal, the dependence on the number of epochs can be improved. A comparison with the analysis of batch gradient descent suggests that the optimal dependence should be \u223c t(1\u2212\u03b8). The bound is proved generalizing classical results in inverse problems and is known to be essentially sharp [17]. We also note that several least squares estimator have been considered in this setting. Regularized least squares (a.k.a. kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29]. As we mentioned before, online learning algorithms are considered in [34, 38] where optimal bounds are derived in the capacity independent setting. The bound in the finite dimensional setting derived in [2] are also optimal. An important point here is that, while different least squares methods are based on the same class of priors and likely to have essentially the same statistical guarantees, their computational properties are different. In this view it is interesting to develop efficient least square estimators and early stopping provides a natural approach (see [42] for an interesting idea based on a divide and conquer approach).\nFinally, we present an adaptive early stopping rule. The stopping rule (4.4) depends on the smoothness parameter r which is typically unknown, and hold-out cross validation is often used in practice. Here we show that this procedure allows to adaptively achieve the same convergence rate as in (4.5). Our analysis follows the one in [8].\nGiven a training set z of cardinality m, we define\nz1 = {(x1, y1, . . . , (xn, yn)}, z2 = {(xn+1, yn+1), . . . , (xm, ym)}, (4.7)\nwhere n = \u230am/2\u230b (note that other splits are possible). Fix tmax \u2208 N, and use the training set z1 to compute the sequence (f\u0302t)t\u2208{1,...,tmax}. Then, set E\u03022 to be the empirical error corresponding to the training set z2, and\nt\u0302 = argmin{E\u03022(TM f\u0302t) : t \u2208 {1, . . . tmax}} . (4.8)\nwhere TM : H \u2192 H : TMf(x) = min{|f(x)|,M}f(x)/|f(x)| is the truncation operator. THEOREM 4.3. Let (\u2200t \u2208 N) \u03b3t = \u03ba\u22122(t + 1)\u2212\u03b8 for some \u03b8 \u2208 [0, 1[. If tmax > m1/(4(1\u2212\u03b8)), with probability greater than 1\u2212 2\u03b4,\nE(TM f\u0302t\u0302)\u2212 inf H\nE \u2264 2D ( log 2\n\u03b4\n)2\nm\u2212 r r+2 + 40M2 3(1\u2212 \u03b8) ( log m 2\u03b4 ) m\u22121 (4.9)\nwhere D is the constant appearing in equation (4.5). Noting that for every r > 0, (log(m/2\u03b4))m\u22121 goes to zero faster than m\u2212r/(r+2), we see that the estimator obtained with hold-out cross-validation achieves the same learning rate as the one corresponding to the optimal choice of the number of epochs. We end noting that, while an hold-out procedure requires splitting the data, it does not worsen the computational complexity of the algorithm, unlike other model selection criterions [29].\n5. Some Future Directions. The analysis presented in the paper is a first step towards exploring the properties of early stopping for online learning algorithms. We mention a few directions for future work.\n\u2022 Sharpen the bounds. In particular incorporating kernel dependent assumption such as (4.6), quantifying the effective dimension of the hypotheses space. This latter point would be particularly interesting to bridge the analysis in the finite and infinite dimensional setting. \u2022 Consider other iterative procedures. Particularly, stochastic variants of the IGD algorithms, where the indices in each cycle are randomly selected [5], and also accelerated version as proposed in [27]. Ideas related to this latter point have been considered in [3, 6, 8] for batch gradient techniques (the gradient of the empirical risk is considered in each iteration). In particular in [3] it is shown that a variant of gradient descent, sometimes called the \u03bd-method, can obtain the same generalization guarantees of non accelerated gradient descent learning, but using much fewer iterations. \u2022 Generalize the analysis. In particular, consider loss functions other than the least squares loss, and especially priors other than (4.2),(4.6). This latter question seems particularly interesting and possibly challenging.\n6. Proofs of the Main Results. In this section we the sketch the proofs of the main results. The error analysis is based on the following decomposition into sample error and approximation error\nE(f\u0302t)\u2212 inf H E \u2264 2\u03ba2\u2016f\u0302t \u2212 ft\u20162H + 2(E(ft)\u2212 inf H E) (6.1)\nwhere ft are the iterates of the IGD on the expected error defined in equation (B.9). Theorems 4.1 and 4.2. will follow from a probabilistic upper bound for the sample error and an upper bound for the approximation error.\nTHEOREM 6.1 (Sample error). Let f\u03020 = f0 = 0, (\u03b3t)t\u2208N be such that (\u2200t \u2208 N) \u03b3t \u2208]0,m\u03ba\u22122], and f\u0302t and ft be defined in (3.2) and (B.9), respectively. There exists C > 0\nsuch that, for all t \u2208 N and \u03b4 \u2208]0, 1[ (sufficiently small),\nP\n(\n\u2016f\u0302t \u2212 ft\u2016H \u2264 C\u221a m\n(\nlog 2\n\u03b4\n) t\u22121 \u2211\nk=1\n\u03b3k\nk\u22121 \u2211\ni=0\n\u03b3i\n)\n\u2265 1\u2212 \u03b4 .\nIn particular, if \u03b3i = \u03ba\u22122(i+ 1)\u2212\u03b8 with \u03b8 \u2208 [0, 1[ , with probability at least 1\u2212 \u03b4,\n\u2016f\u0302t \u2212 ft\u2016H \u2264 C\n2\u03ba4(1\u2212 \u03b8)2\u221am\n(\nlog 2\n\u03b4\n)\n(t+ 1)2(1\u2212\u03b8)\nThe proof of Theorem 6.1 is postponed to Appendix C. We next show that the approximation error E(ft) \u2212 infH E go to zero as t goes to infinity, and give a rate under assumption (4.2). For similar results with respect to the norm in H see Appendix D.\nTHEOREM 6.2 (Approximation error). Let f0 = 0, (\u03b3t)t\u2208N be such that (\u2200t \u2208 N) \u03b3t \u2208]0,m\u03ba\u22122[, \u2211 t\u2208N \u03b3t = +\u221e and (ft)t\u2208N be defined as in (B.9). Then\nE(ft)\u2212 inf H E \u2192 0. (6.2)\nMoreover, if f\u03c1 satisfies (4.2), then\nE(ft)\u2212 inf H\nE \u2264 R2(r/e)2r( t\u22121 \u2211\ni=0\n\u03b3i) \u22122r .\nIn particular, if \u03b3i = \u03ba\u22122(i+ 1)\u2212\u03b8 with \u03b8 \u2208 [0, 1[,\nE(ft)\u2212 inf H\nE \u2264 R 2(r/e)2r\n\u03ba2(1\u2212 \u03b8) t \u22122r(1\u2212\u03b8) .\nCombining the bounds for the sample and approximation errors, we get a proof of Theorem 4.2 and Theorem 4.1, solving a bias-variance trade-off. Proof. of Theorem 4.2. Recalling equation (6.1), we have\nE(f\u0302t)\u2212 inf H E \u2264 2\u03ba2\u2016f\u0302t \u2212 ft\u20162H + 2(E(ft)\u2212 inf H E) .\nApplying Theorem 6.1 and Theorem 6.2 we get that with probability at least 1\u2212 \u03b4\nE(f\u0302t)\u2212 inf H\nE \u2264 C 2\n2\u03ba6(1\u2212 \u03b8)2m\n(\nlog 2\n\u03b4\n)2\nt4(1\u2212\u03b8) + R(r/e)2r\n\u03ba2(1\u2212 \u03b8) t \u22122r(1\u2212\u03b8), (6.3)\nand (4.3) follows by suitably chosing the constant D. Next let t\u2217(m) = \u2308m\u03b1\u2309. Substituting this choice of t into the right hand side of (6.3), and minimizing over \u03b1, we get the linear equation\n4\u03b1(1\u2212 \u03b8)\u2212 1 = \u22122\u03b1r(1\u2212 \u03b8), (6.4)\nwhose solution is \u03b1 = 1/((1\u2212 \u03b8)(4 + 2r)). Proof. of Theorem 4.1. Combining Theorem 6.1 and Theorem 6.2 we immediately get convergence in probability of E(f\u0302t) \u2212 inff\u2208H E . Almost sure convergence follows applying the Borel-Cantelli lemma. Proof. of Theorem 4.3 We define\nt\u03c1 = argmin{E(TM f\u0302t) : t \u2208 {1, . . . , T }} . (6.5)\nThe latter definition is equivalent to\nt\u03c1 = argmint\u2208{1,...,tmax}\n\u2225 \u2225 \u2225S(TM f\u0302t)\u2212 f\u03c1 \u2225 \u2225 \u2225 2\n\u03c1 (6.6)\nwhere S is the operator defined in Appendix B. Since by assumption the support of \u03c1(y|x) is contained in [\u2212M,M ], it follows that f\u03c1(x) \u2208 [\u2212M,M ] almost surely. Therefore, from the definition of t\u03c1,\n\u2225 \u2225 \u2225S(TM f\u0302t\u03c1)\u2212 f\u03c1 \u2225 \u2225 \u2225\n\u03c1 \u2264\n\u2225 \u2225 \u2225S(TM f\u0302t\u2217(m))\u2212 f\u03c1 \u2225 \u2225 \u2225\n\u03c1 \u2264\n\u2225 \u2225 \u2225Sf\u0302t\u2217(m) \u2212 f\u03c1 \u2225 \u2225 \u2225\n\u03c1 . (6.7)\nIf we denote by Pf\u03c1 the projection onto the closure of H in L2(X , \u03c1X ), from the chain of inequalities in (6.7), noting that (\u2200g \u2208 H) \u2016Sg \u2212 Pf\u03c1\u20162\u03c1 = \u2016Sg \u2212 f\u03c1\u2016 2 \u03c1 \u2212 \u2016Pf\u03c1 \u2212 f\u03c1\u2016 2 \u03c1 we get\nE(TM f\u0302t\u03c1)\u2212 inf H\nE = \u2225 \u2225 \u2225S(TM f\u0302t\u03c1)\u2212 Pf\u03c1 \u2225 \u2225 \u2225 2\n\u03c1 \u2264\n\u2225 \u2225 \u2225Sf\u0302t\u2217(m) \u2212 Pf\u03c1 \u2225 \u2225 \u2225 2\n\u03c1 = E(f\u0302t\u2217(m))\u2212 inf H E . (6.8)\nFinally, we claim that with probability greater than 1\u2212 \u03b4, if tmax = m1/(4(1\u2212\u03b8)),\n\u2016S(TM f\u0302t\u0302)\u2212 Pf\u03c1\u20162\u03c1 \u2264 2\u2016S(TM f\u0302t\u03c1)\u2212 Pf\u03c1\u20162\u03c1 + 640M2(1\u2212 \u03b8)\nm log\n2m\n\u03b4 (6.9)\nFirst note that adding to both sides \u2016Pf\u03c1 \u2212 f\u03c1\u20162\u03c1 (6.8) is equivalent to\n\u2016S(TM f\u0302t\u0302)\u2212 f\u03c1\u20162\u03c1 \u2264 2\u2016S(TM f\u0302t\u03c1)\u2212 f\u03c1\u20162\u03c1 + 640M2(1\u2212 \u03b8)\nm log\n2m\n\u03b4 (6.10)\nThe statement then follows combining (6.8) and (6.9), and applying Theorem 4.2. To prove (6.10), let z2 = {(xn+1, yn+1, . . . , (xm, ym)}, and define the random variables for i = n+ 1, . . . ,m:\n\u03beti = (S(TM f\u0302t)(xi)\u2212 yi)2 \u2212 (f\u03c1(xi)\u2212 yi)2 .\nWe have for every t \u2208 {1, . . . , tmax}\n|\u03beti | \u2264 4M2 (6.11)\nE[(\u03beti )] =\n\u222b\nX\u00d7Y\n( (TM f\u0302t(x)\u2212 y)2 \u2212 (f\u03c1(x)\u2212 y)2 ) d\u03c1 = \u2016S(TM f\u0302t)\u2212 f\u03c1\u20162\u03c1 (6.12)\nE[(\u03beti ) 2] =\n\u222b\nX\u00d7Y\n(TM f\u0302t(x)\u2212 f\u03c1(x))2(TM f\u0302t(x) + f\u03c1(x)\u2212 2y)2 d\u03c1 (6.13)\n\u2264 16M2E[(\u03beti )] (6.14)\nApplying Proposition A.2 with Xi = \u03beti , \u00b5 = E[(\u03be t i )], B = 4M 2, \u03c32 = E[(\u03beti ) 2] \u2264\n16M2E[(\u03beti )], we obtain for all t \u2208 {1, . . . , tmax} with probability greater than 1\u2212 \u03b4\n1\nm\u2212 n\nm\u2212n \u2211\ni=1\n\u03beti \u2264 (1 + 16\u03b1M2)\u00b5+ \u01eb (6.15)\nand\n(1\u2212 16\u03b1M2)E[\u03beti ] \u2264 1\nm\u2212 n\nm\u2212n \u2211\ni=1\n\u03beti + \u01eb, (6.16)\nwith \u01eb = (3+16\u03b1M 2) 24(1\u2212\u03b8)(m\u2212n)\u03b1 log (m\u2212n) \u03b4 . Therefore, since tmax \u2265 t\u2217(m)\n\u2016Sf\u0302t\u0302 \u2212 f\u03c1\u20162\u03c1 = E[\u03bet\u0302i ] \u2264 1\n1\u2212 16\u03b1M2\n(\n1\nm\u2212 n\nm\u2212n \u2211\ni=1\n\u03bet\u0302i\n)\n+ \u01eb\n1\u2212 16\u03b1M2\n\u2264 1 1\u2212 16\u03b1M2\n(\n1\nm\u2212 n\nm\u2212n \u2211\ni=1\n\u03be t\u2217(m) i\n)\n+ \u01eb\n1\u2212 16\u03b1M2\n\u2264 1 + 16\u03b1M 2\n1\u2212 16\u03b1M2E[\u03be t\u2217(m) i ] +\n2\u01eb\n1\u2212 16\u03b1M2 .\nIf we choose \u03b1 = 1/(48M2) we get 16\u03b1M2 = 1/3 and\n\u2016Sf\u0302t\u0302 \u2212 f\u03c1\u20162\u03c1 \u2264 2(E(TM f\u0302t\u2217(m))\u2212 E(f\u03c1)) + 20M2 3(1\u2212 \u03b8)(m\u2212 n) log (m\u2212 n) \u03b4 . (6.17)\nRecalling that m\u2212 n \u2265 m/2 we obtain the statement. Acknowledgments. This material is based upon work supported by the FIRB project RBFR12M3AC \u201cLearning meets time: a new computational approach for learning in dynamic systems\u201d and the Center for Minds, Brains and Machines (CBMM), funded by NSF STC award CCF-1231216. S. V. is member of the Gruppo Nazionale per l\u2019Analisi Matematica, la Probabilita\u0300 e le loro Applicazioni (GNAMPA) of the Istituto Nazionale di Alta Matematica (INdAM). The authors are grateful to Francesco Orabona for fruitful discussions.\nREFERENCES\n[1] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc, 68(3):337\u2013404, 1950. [2] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n). In Advances in Neural Information Processing Systems (NIPS), 2013. [3] Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco. On regularization algorithms in learning theory. Journal of complexity, 23(1):52\u201372, 2007. [4] Dimitri P. Bertsekas. A new class of incremental gradient methods for least squares problems. SIAM J. Optim., 7(4):913\u2013926, 1997. [5] Dimitri P Bertsekas and John N Tsitsiklis. Gradient convergence in gradient methods with errors. SIAM Journal on Optimization, 10(3):627\u2013642, 2000. [6] G. Blanchard and N. Kra\u0308mer. Optimal learning rates for kernel conjugate gradient regression. In Advances in Neural Inf. Proc. Systems (NIPS), pages 226\u2013234, 2010. [7] A. Caponnetto and E. De Vito. Optimal rates for regularized least-squares algorithm. Found. Comput. Math., 2006. [8] A. Caponnetto and Yuan Yao. Adaptive rates for regularization operators in learning theory. Analysis and Applications, 08, 2010. [9] N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms.\nIEEE Transactions on Information Theory, 50(9):2050\u20132057, 2004. [10] Nicolo\u0300 Cesa-Bianchi and Ga\u0301bor Lugosi. Prediction, learning, and games. Cambridge University Press, 2006. [11] Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the American Mathematical Society, 39:1\u201349, 2002. [12] Felipe Cucker and Ding Xuan Zhou. Learning Theory: An Approximation Theory Viewpoint. Cambridge University Press, 2007. [13] E. De Vito, A. Caponnetto, and L. Rosasco. Model selection for regularized least-squares algorithm in learning theory. Found. Comput. Math., 5(1):59\u201385, 2005. [14] E. De Vito, L. Rosasco, A. Caponnetto, U. De Giovannini, and F. Odone. Learning from examples as an inverse problem. Journal of Machine Learning Research, 6:883\u2013904, 2005. [15] L. Devroye, L. Gyo\u0308rfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Number 31 in Applications of mathematics. Springer, New York, 1996. [16] Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference\non Winter simulation, pages 260\u2013265. ACM, 1986.\n[17] H. W. Engl, M. Hanke, and A. Neubauer. Regularization of inverse problems. Kluwer, 1996. [18] Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. Journal of Machine Learning Research-Proceedings Track, 23:9\u20131, 2012. [19] Jyrki Kivinen, Alexander J Smola, and Robert C Williamson. Online learning with kernels. Signal Processing, IEEE Transactions on, 52(8):2165\u20132176, 2004. [20] Harold J. Kushner and G. George Yin. Stochastic approximation algorithms and applications, volume 35 of Applications of Mathematics (New York). Springer-Verlag, New York, 1997. [21] Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential con-\nvergence rate for strongly-convex optimization with finite training sets. arXiv preprint arXiv:1202.6258, 2012.\n[22] Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efficient backprop. In G. Orr and Muller K., editors, Neural Networks: Tricks of the trade. Springer, 1998. [23] Laura Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto De Vito, and Alessandro Verri. Spectral algorithms for supervised learning. Neural Computation, 20(7):1873\u20131897, 2008. [24] Shahar Mendelson and Joseph Neeman. Regularization in kernel learning. The Annals of Statistics, 38(1):526\u2013 565, 2010. [25] Angelia Nedic and Dimitri P Bertsekas. Incremental subgradient methods for nondifferentiable optimization. SIAM Journal on Optimization, 12(1):109\u2013138, 2001. [26] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM J. Optim., 19(4):1574\u20131609, 2008. [27] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady AN SSSR, 269(3):543\u2013547, 1983. [28] Iosif Pinelis. Optimum bounds for the distributions of martingales in Banach spaces. Ann. Probab., 22(4):1679\u20131706, 1994. [29] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Early stopping for non-parametric regression: An optimal data-dependent stopping rule. In Communication, Control, and Computing (Allerton), 2011 49th Annual Allerton Conference on, pages 1318\u20131325. IEEE, 2011. [30] Michael Reed and Barry Simon. Methods of modern mathematical physics. I. Functional analysis. Academic Press, New York, 1972. [31] Ingo Steinwart. On the influence of the kernel on the consistency of support vector machines. The Journal of Machine Learning Research, 2:67\u201393, 2002. [32] Ingo Steinwart and Andreas Christmann. Support Vector Machines. Springer Publishing Company, Incorporated, 1st edition, 2008. [33] Ingo Steinwart, Don R. Hush, and Clint Scovel. Optimal rates for regularized least squares regression. In COLT, 2009. [34] Pierre Tarre\u0300s and Yuan Yao. Online learning as stochastic approximation of regularization paths. arXiv preprint arXiv:1103.5538, 2011. [35] Angus E. Taylor and David C. Lay. Introduction to functional analysis. 2nd ed. (Reprint of the orig. 1980, publ. by John Wiley &amp; Sons, Inc., New York etc.). Malabar, Florida: Robert E. Krieger Publishing Company. XI, 1986. [36] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389\u2013434, 2012. [37] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289\u2013315, 2007. [38] Yiming Ying and Massimiliano Pontil. Online gradient descent learning algorithms. Foundations of Computational Mathematics, 8(5):561\u2013596, 2008. [39] Vadim Yurinsky. Sums and Gaussian vectors, volume 1617 of Lecture Notes in Mathematics. Springer-Verlag, Berlin, 1995. [40] Tong Zhang. Learning bounds for kernel regression using effective data dimensionality. Neural Computation, 17:2077\u20132098, 2005. [41] Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. Annals of Statistics, pages 1538\u20131579, 2005. [42] Yuchen Zhang, John C. Duchi, and Martin J. Wainwright. Divide and conquer kernel ridge regression. In COLT, pages 592\u2013617, 2013.\nAppendix A. Useful Results. In the various proofs we repeatedly use the following wellknown equation. Let (Xr)r\u2208N be a sequence in H, defined recursively by\nXr+1 = ArXr +Br, (A.1)\nwith Ar : H \u2192 H a linear operator for all r \u2208 N, and {Br}r\u2208N \u2286 H . Then, for every s \u2208 N,\ns < r,\nXr =\n(\nr\u22121 \u220f\ni=s\nAi\n)\nXs + r\u22121 \u2211\nk=s\n(\nr\u22121 \u220f\ni=k+1\nAi\n)\nBk . (A.2)\nThe following concentration inequality due to [28] (see also [39] and [34, Proposition A.3]) and is the useful to obtain a probabilistic upper bound for the sample error.\nTHEOREM A.1 (Bernstein-Pinelis). Let \u03bei be a martingale difference sequence taking values in a Hilbert space. Suppose that \u2016\u03bei\u2016 \u2264 M . Then for every \u03b4 \u2208 ]0, 1[ the following holds\nP\n({\n\u2225 \u2225 \u2225 1\nm\nm \u2211\ni=1\n\u03bei\n\u2225 \u2225 \u2225 \u2264 8M 3 \u221a m log 2 \u03b4\n})\n\u2265 1\u2212 \u03b4 .\nWhile more refined concentration inequality could be considered [36], this would affect only the constants in the bound which are not the main focus of this paper. In the proof of Theorem 4.3, we will also use a variant of the previous Bernstein concentration inequality, which is taken from [8].\nPROPOSITION A.2. Let (\u03bei)1\u2264i\u2264m be real valued i.i.d. random variables with mean \u00b5, |\u03bei| \u2264 B and E[(\u03bei \u2212 \u00b5)2] \u2264 \u03c32, for all i \u2208 {1, . . . ,m}. Then for arbitrary \u03b1 \u2208 R++, \u03b4 \u2208]0, 1],\nP\n[\u2223\n\u2223 \u2223 \u2223 \u2223 1 m\nm \u2211\ni=1\n\u03bei \u2212 \u00b5 \u2223 \u2223 \u2223 \u2223\n\u2223 \u2265 \u03b1\u03c32 + 3 + 4\u03b1B 6m\u03b1 log 2\u03b4\n]\n\u2264 \u03b4 . (A.3)\nAppendix B. Recursive Expressions and Error Decomposition. We introduce some linear operators that will be useful in the following. Let S : H \u2192 L2(X , \u03c1X ) be the embedding operator. S is well defined and \u2016S\u2016 \u2264 \u03ba. If we set T = S\u2217S andL = SS\u2217, then T : H \u2192 H, L : L2(X , \u03c1X ) \u2192 L2(X , \u03c1X ) : Lf(x) = \u222b\nX f(x)K(x, x\u2032)d\u03c1X\nwith \u2016T \u2016 \u2264 \u03ba2 and \u2016L\u2016 \u2264 \u03ba2. Fix x \u2208 X . Then Sx : H \u2192 R : f 7\u2192 f(x) is well defined and \u2016Sx\u2016 \u2264 \u03ba. Its adjoint is S\u2217x : R \u2192 H : a 7\u2192 aKx. The operator S\u2217xSx is denoted by Tx. Clearly \u2016Tx\u2016 \u2264 \u03ba2. Finally, given the training set z, the operator\n\u2211m i=1 Txi/m is denoted by T\u0302 . Note that, using\nthese linear operators, we can write the empirical error and the risk as\nE\u0302(f) = 1 m\nm \u2211\ni=1\n(Sxif \u2212 yi)2, E(f) = \u2016Sf \u2212 f\u03c1\u20162\u03c1 + E(f\u03c1) , (B.1)\nfor alll f \u2208 H. We describe some useful recursive expressions for the IGD and the gradient descent (GD) iteration. These will be used in the error analysis and provide some useful comparison between these two methods.\nLEMMA B.1. Let f\u03020 \u2208 H and for every t \u2208 N, let f\u0302t be defined as in equations (3.2) and (3.3). Then\nf\u0302t+1 = m \u220f\ni=1\n( I \u2212 \u03b3t m Txi ) f\u0302t + \u03b3t m\nm \u2211\ni=1\nm \u220f\nk=i+1\n( I \u2212 \u03b3t m Txk ) S\u2217xiyi (B.2)\nProof. The update of \u03c8\u0302it in (3.3) is of the form (A.1), with Ar = I \u2212 \u03b3tmTxr+1 , and Br = \u03b3t mS\n\u2217 xr+1yr+1, and X0 = f\u0302t. Equation (B.2) follows by writing (A.2) for r = m. PROPOSITION B.2. Assume that m \u2265 2. The iteration of the incremental gradient\ndescent can be written as\nf\u0302t+1 =\n\nI \u2212 \u03b3t m\nm \u2211\nj=1\nTxj\n\n f\u0302t + \u03b3t\n(\n1\nm\nm \u2211\nj=1\nS\u2217xjyj\n)\n+ \u03b32t\n( A\u0302tf\u0302t \u2212 b\u0302t )\n(B.3)\nwith\nA\u0302t = 1\nm2\nm \u2211\nk=2\nm \u220f\ni=k+1\n( I \u2212 \u03b3t m Txi ) Txk\nk\u22121 \u2211\nj=1\nTxj , b\u0302t = 1\nm2\nm \u2211\nk=2\nm \u220f\ni=k+1\n( I \u2212 \u03b3t m Txi ) Txk\nk\u22121 \u2211\nj=1\nS\u2217xjyj\n(B.4)\nProof. In order to prove the equality in (B.3), we first derive a equation for the quantity hi = \u2211i j=1(Txj \u03c8\u0302 j\u22121 t \u2212 S\u2217xjyj). We show by induction that for all i = 1, . . . ,m\nhi = i \u2211\nj=1\nTxj f\u0302t \u2212 i \u2211\nj=1\nS\u2217xjyj + \u03b3t m R\u0302itf\u0302t + \u03b3t m r\u0302it (B.5)\nfor a linear operator R\u0302it : H \u2192 H and an element r\u0302it \u2208 H. Since\nh1 = Tx1\u03c8\u0302 0 t \u2212 S\u2217x1y1 = Tx1 f\u0302t \u2212 S \u2217 x1y1 ,\nclearly (B.5) holds for i = 1 with R\u03021t = 0 and r\u0302 1 t = 0. Now we suppose by inductive hypothesis that (B.5) holds for i, and we prove it for i+1. First note that \u03c8\u0302it = f\u0302t\u2212(\u03b3t/m)hi, and then by inductive hypothesis,\nhi+1 = hi + Txi+1\u03c8\u0302 i t \u2212 S\u2217xi+1yi+1\n= hi + Txi+1(f\u0302t \u2212 \u03b3t m hi)\u2212 S\u2217xi+1yi+1 = (I \u2212 \u03b3t m Txi+1)hi + Txi+1 f\u0302t \u2212 S\u2217xi+1yi+1\n= (I \u2212 \u03b3t m Txi+1)(\ni \u2211\nj=1\nTxj f\u0302t \u2212 i \u2211\nj=1\nS\u2217xjyj + \u03b3t m R\u0302itf\u0302t + \u03b3t m r\u0302it) + Txi+1 f\u0302t \u2212 S\u2217xi+1yi+1\n=\ni+1 \u2211\nj=1\nTxj f\u0302t \u2212 i+1 \u2211\nj=1\nS\u2217xjyj + \u03b3t m\n\n\n( I \u2212 \u03b3t m Txi+1 )\nR\u0302it \u2212 Txi+1 i \u2211\nj=1\nTxj\n\n f\u0302t (B.6)\n+ \u03b3t m\n(\n( 1\u2212 \u03b3t m Txi+1 ) r\u0302it + Txi+1\nj \u2211\ni=1\nS\u2217xjyj\n)\n.\nBy setting for i = 1, . . . ,m\u2212 1\nR\u0302i+1t = ( 1\u2212 \u03b3t m Txi+1 )\nR\u0302it\u2212Txi+1 i \u2211\nj=1\nTxj , r\u0302 i+1 t =\n( I \u2212 \u03b3t m Txi+1 ) r\u0302it+Txi+1\ni \u2211\nj=1\nS\u2217xjyj\nwe get (B.5) for i+ 1. From (B.5) we derive\nf\u0302t+1 = f\u0302t\u2212 \u03b3t m hm =\n\nI \u2212 \u03b3t m\nm \u2211\nj=1\nTxj \u2212 (\u03b3t m )2 R\u0302mt\n\n f\u0302t+ \u03b3t m\nm \u2211\nj=1\nS\u2217xjyj\u2212 (\u03b3t m )2 r\u0302it (B.7)\nEquation (B.3) then follows by defining A\u0302t = \u2212R\u0302mt /m2 and b\u0302t = r\u0302mt /m2 and recalling (3.3). Moreover, applying (A.2), we have\nA\u0302t = 1\nm2\nm \u2211\nk=2\nm \u220f\ni=k+1\n( I \u2212 \u03b3t m Txi ) Txk\nk\u22121 \u2211\nj=1\nTxj , b\u0302t = 1\nm2\nm \u2211\nk=2\nm \u220f\ni=k+1\n( I \u2212 \u03b3t m Txi ) Txk\nk\u22121 \u2211\nj=1\nS\u2217xjyj\nEquation (B.3) allows to compare the resulting update of one epoch of the incremental gradient descent with the one of a standard gradient descent on the empirical error with stepsize \u03b3t, which is given by\n\u03d5\u0302t+1 =\n\nI \u2212 \u03b3t m\nm \u2211\nj=1\nTxj\n\n \u03d5\u0302t + \u03b3t m\nm \u2211\nj=1\nS\u2217xjyj (B.8)\nfor an arbitrary \u03d5\u03020 \u2208 H. As can be seen directly comparing (B.3) and (B.8), the incremental gradient descent can be interpreted as a perturbed gradient descent step, with perturbation\ne\u0302t = \u03b3 2 t\n( A\u0302tf\u0302t \u2212 b\u0302t ) ,\nwhich is proportional to \u03b32t . To analyze consistency properties of IGD applied to the empirical risk, we need to introduce an auxiliary iteration, obtained by applying the incremental gradient descent algorithm to the expected loss, which clearly, for fixed m, can be written as\nf 7\u2192 m \u2211\ni=1\nE(f) m .\nReasoning as in Proposition B.2, simply replacing Txi by T and Sxi by S, we get\nft+1 = (I \u2212 \u03b3tT ) ft + \u03b3tS\u2217f\u03c1 + \u03b32t (Atft \u2212 bt) (B.9)\nwith\nAt = 1\nm2\nm \u2211\nk=2\n[\nm \u220f\ni=k+1\n( I \u2212 \u03b3t m T )\n]\nT\nk\u22121 \u2211\nj=1\nTft, bt = 1\nm2\nm \u2211\nk=2\n[\nm \u220f\ni=k+1\n( I \u2212 \u03b3t m T )\n]\nT\nk\u22121 \u2211\nj=1\nS\u2217f\u03c1\n(B.10)\nREMARK B.3. First note that, although not explicitly specified, the operator At and the element bt \u2208 H depend on m. Moreover, since in this case the gradient of each summand coincides with the gradient of the function E , one epoch of the incremental gradient method in (B.9) corresponds to m steps of a gradient descent with stepsize \u03b3t/m (where the stepsize is fixed across m iterations).\nAppendix C. Proof of Theorems 6.1 and 6.2. To prove Theorem 6.1 we need some auxiliary results. The proof will be given at the end of the section.\nLEMMA C.1. For all t \u2208 N,\nf\u0302t \u2212 ft = [ t\u22121 \u220f\nk=0\n(I \u2212 \u03b3kT + \u03b32kAk) ] (f\u03020 \u2212 f0) + t\u22121 \u2211\nk=0\n\u03b3k\n\n\nt\u22121 \u220f\nj=k+1\n(I \u2212 \u03b3jT + \u03b32jAj)\n\n\u03c7k\n(C.1) with\n\u03c7k = (T \u2212 T\u0302 )f\u0302k + \u03b3k(A\u0302k \u2212Ak)f\u0302k + ( 1\nm\nm \u2211\ni=1\nS\u0302\u2217xiyi \u2212 S \u2217f\u03c1) + \u03b3k(bk \u2212 b\u0302k), (C.2)\nwhere A\u0302k and b\u0302k are defined in (B.4), and Ak and bk in (B.10). Proof. Fix t \u2208 N. We have\nf\u0302t+1 = (I \u2212 \u03b3tT\u0302 + \u03b32t A\u0302t)f\u0302t + \u03b3t m \u2211\ni=1\nS\u0302\u2217xiyi \u2212 \u03b3 2 t b\u0302t .\nAdding and subtracting (\u2212\u03b3tT + \u03b32tAt)f\u0302t we get\nf\u0302t+1 = (I \u2212 \u03b3tT + \u03b32tAt)f\u0302t + \u03b3t(T \u2212 T\u0302 )f\u0302t + \u03b32t (A\u0302t \u2212At)f\u0302t + \u03b3t 1\nm\nm \u2211\ni=1\nS\u0302\u2217xiyi \u2212 \u03b3 2 t b\u0302t .\nTherefore\nf\u0302t+1 \u2212 ft+1 = (I \u2212 \u03b3tT + \u03b32tAt)(f\u0302t \u2212 ft) + \u03b3t(T \u2212 T\u0302 )f\u0302t + \u03b32t (A\u0302t \u2212At)f\u0302t\n+ \u03b3t\n( 1\nm\nm \u2211\ni=1\nS\u0302\u2217xiyi \u2212 S \u2217f\u03c1\n)\n+ \u03b32t (bt \u2212 b\u0302t)\nRelying on equation (A.1) we get (C.1). We next state other lemmas which are needed to bound the various terms appearing in the decomposition derived in Lemma C.1. First we bound the norm of the operator which is applied to the random variable \u03c7k.\nLEMMA C.2. Fix m \u2208 N, m \u2265 1, j \u2265 1, and let \u03b3j \u2208 ]0,m\u03ba\u22122]. Then\n\u2016I \u2212 \u03b3jT + \u03b32jAj\u2016 \u2264 1 . (C.3)\nProof. By definition of Aj\nI \u2212 \u03b3jT + \u03b32jAj = ( I \u2212 \u03b3j m T )m . (C.4)\nSince \u2016T \u2016 \u2264 \u03ba2 and by assumption \u03b3j/m \u2264 \u03ba\u22122, \u2225 \u2225I \u2212 \u03b3jmT \u2225 \u2225 \u2264 1 and the statement follows.\nThe longest part of the proof is the one required to get a bound on the norm of the random variable \u03c7k. We will proceed bounding each summand separately.\nLEMMA C.3. Assume (\u03b3k)k\u2208N to be a sequence in ]0,+\u221e[, and f\u03020 = 0. Then, for all t \u2208 N, \u2016f\u0302t\u2016K \u2264 \u03baM \u2211t\u22121 k=0 \u03b3k. The following lemma is a direct consequence of BernsteinPinelis inequality A.1, taking into account that \u2016T \u2016HS \u2264 \u03ba2 and \u2016Txi\u2016HS \u2264 \u03ba2 (see also [14])\nLEMMA C.4. For every \u03b4 \u2208 ]0, 1[\nP\n(\n\u2016 1 m\nm \u2211\ni=1\nTxi \u2212 T \u2016HS \u2264 16\u03ba2\n3 \u221a m\nlog 2\n\u03b4\n)\n\u2265 1\u2212 \u03b4 (C.5)\nand\nP\n(\n\u2016 1 m\nm \u2211\ni=1\nS\u2217xiyi \u2212 S \u2217f\u03c1\u2016H \u2264\n16\u03baM\n3 \u221a m\nlog 2\n\u03b4\n)\n\u2265 1\u2212 \u03b4 (C.6)\nLEMMA C.5. Let (\u03b3k)k\u2208N be a sequence in ]0,m\u03ba\u22122[. For any \u03b4 \u2208 ]0, 1[ and k \u2208 N,\nP\n(\n\u2016A\u0302k \u2212Ak\u2016HS \u2264 2\u03ba4\nm2 +\n64\u03ba4 9 \u221a m log 2 \u03b4\n)\n\u2265 1\u2212 \u03b4 (C.7)\nProof. We first show a useful decomposition. Recall that\nA\u0302k = 1\nm\nm \u2211\nj=2\n1\nm\nm \u220f\ni=j+1\n( I \u2212 \u03b3k m Txi ) Txj\nj\u22121 \u2211\nl=1\nTxj Ak = 1\nm\nm \u2211\nj=2\nm \u220f\ni=j+1\n( I \u2212 \u03b3k m T ) T 1 m\nj\u22121 \u2211\nl=1\nT .\nIf we set for j \u2208 {2, . . . ,m}\nB\u0302k,j =\n\n\nm \u220f\ni=j+1\n( I \u2212 \u03b3k m Txi )\n\n Txj , Bk,j =\n\n\nm \u220f\ni=j+1\n( I \u2212 \u03b3k m T )\n\nT\nwe have\nA\u0302k \u2212Ak = 1\nm\nm \u2211\nj=2\nj \u2212 1 m B\u0302k,j\n(\n1\nj \u2212 1\nj\u22121 \u2211\nl=1\nTxl\n)\n\u2212 1 m\nm \u2211\nj=2\nBk,j 1\nm\nj\u22121 \u2211\nl=1\nT (C.8)\n= 1\nm\n\n\nm \u2211\nj=2\nj \u2212 1 m B\u0302k,j\n(\n1\nj \u2212 1\nj\u22121 \u2211\nl=1\nTxl \u2212 T ) +QmT\n\n ,\nwith\nQm =\nm \u2211\nj=2\nj \u2212 1 m (B\u0302k,j \u2212Bk,j) . (C.9)\nWe next bound each term appearing in the decomposition in equation (C.8). Let (\u03b3k)k\u2208N be a sequence in ]0,m\u03ba\u22122[. By Lemma C.4, with probability greater than 1\u2212 \u03b4,\n\u2225 \u2225 \u2225 1\nj \u2212 1\nj\u22121 \u2211\nl=1\nTxl \u2212 T \u2225 \u2225 \u2225 HS \u2264 8\u03ba\n2\n3 \u221a j \u2212 1 log\n2 \u03b4 . (C.10)\nOn the other hand\n\u2016B\u0302k,j\u2016 \u2264 m \u220f\ni=j+1\n\u2225 \u2225 \u2225I \u2212 \u03b3k m Txi \u2225 \u2225 \u2225\u2016Txj\u2016 \u2264 \u03ba2. (C.11)\nNote that \u2211m j=2 j\u22121 m B\u0302k,j\n(\n1 j\u22121 \u2211j\u22121 l=1 Txl \u2212 T\n)\nis Hilbert-Schmidt, for Txl and T are Hilbert-\nSchmidt operators, with \u2016Txl\u2016 \u2264 \u03ba2 and \u2016T \u2016HS \u2264 \u03ba2, and the family of Hilbert-Schmidt\noperators is an ideal with respect to the composition in L(H) (see TheoremVI.22 and Exercise 28 in [30]). Therefore, combining (C.10) and (C.11), and by noting that\n\u2211m j=1\n\u221a j \u2212 1 \u2264\n2m3/2/3\n1\nm\n\u2225 \u2225 \u2225\nm \u2211\nj=2\nj \u2212 1 m B\u0302k,j ( 1 j \u2212 1\nj\u22121 \u2211\nl=1\nTxl \u2212 T )\u2225 \u2225 \u2225 HS \u2264 8\u03ba\n4\n3m2 log\n2\n\u03b4\nm \u2211\nj=2\n\u221a j \u2212 1 \u2264 16\u03ba 4\n9 1\u221a m log 2 \u03b4\n(C.12) holds with probability greater than 1\u2212 \u03b4, for any \u03b4 \u2208 ]0, 1].\nNext we write the quantity \u2211m j=2 j\u22121 m (B\u0302k,j \u2212 Bk,j) appearing in the second term in (C.8) as the sum of a martingale and a bounded term. For short, we set \u03b3 = \u03b3km and for all j \u2208 {2, . . . ,m} we denote\n\u03a0\u0302mj+1 = l \u2212 1 m\nm \u220f\ni=j+1\n(I \u2212 \u03b3Txi) , \u03a0mj+1 = j \u2212 1 m\nm\u22121 \u220f\ni=j+1\n(I \u2212 \u03b3T ) ,\nso that from the definition of Qm in (C.9),\nQm =\nm \u2211\nj=2\n(\u03a0\u0302mj+1Txj \u2212\u03a0mj+1T ),\nfor all m > 1. We can derive a recursive update for the quantity Qm as follows\nQm+1 =\nm+1 \u2211\nj=2\n(\u03a0\u0302m+1j+1 Txj \u2212\u03a0m+1j+1 T )\n= m\nm+ 1\n[ (Txm+1 \u2212 T ) + m \u2211\nj=2\n(\u03a0\u0302m+1j+1 Txj \u2212\u03a0m+1j+1 T ) ]\n= m\nm+ 1\n[ (Txm+1 \u2212 T ) + m \u2211\nj=2\n((I \u2212 \u03b3Txm+1)\u03a0\u0302mj+1Txj \u2212 (I \u2212 \u03b3T )\u03a0mj+1T ) ]\n= m\nm+ 1\n[ (Txm+1 \u2212 T ) + (I \u2212 \u03b3Txm+1) m \u2211\nj=2\n(\u03a0\u0302mj+1Txj \u2212\u03a0mj+1T )\n+ \u03b3(T \u2212 Txm+1) m \u2211\nj=2\n\u03a0mj+1T ]\n= m\nm+ 1 (I \u2212 \u03b3Txm+1)Qm +\nm\nm+ 1 (Txm+1 \u2212 T )\n( I \u2212 \u03b3 m \u2211\nj=2\n\u03a0mj+1T ) .\nApplying equation (A.1), we get\nQm =\nm \u220f\nl=3\nl \u2212 1 l ( I \u2212 \u03b3k m Txl ) 1 2 (Tx2 \u2212 T ) + \u03bek,l (C.13)\nwhere\n\u03bek,l =\nm \u2211\nl=3\nm \u220f\ni=l+1\ni\u2212 1 i ( I \u2212 \u03b3k m Txi ) l\u2212 1 l (Txl \u2212 T )\n\nI \u2212 \u03b3k m\nl\u22121 \u2211\nj=2\nj \u2212 1 l \u2212 1\nl\u22121 \u220f\ni=j+1\n( I \u2212 \u03b3k m T ) T\n\n .\nFor all l = 3, . . . ,m\nE[\u03bek,l] = 0,\nbeing Tx3 , . . . , Txm independent and E[(Txl \u2212 T )] = 0 for all l = 3, . . . ,m. Moreover the conditional expectation\nE[\u03bek,l | \u03bek,l+1, . . . , \u03bek,m] = 0,\nsince Txl is independent fromTxl+1, . . . , Txm . Therefore the sequence (\u03bek,l) for l = 3, . . . ,m is a martingale difference sequence for all k.\nThe operator \u03bek,l is Hilbert-Schmidt, since it is the composition of a Hilbert-Schmidt operator with a continuous one. Next, since the operator T is compact and self-adjoint and 0 \u2264 \u03b3k/m \u2264 1/\u2016T \u2016, from the spectral mapping theorem, for every l \u2208 {3, . . . ,m}\n\u2016I \u2212 \u03b3k m\nl\u22121 \u2211\nj=2\nj \u2212 1 l \u2212 1\nl\u22121 \u220f\ni=j+1\n( I \u2212 \u03b3k m T ) T \u2016 = sup x\u2208[0,1]\n\u2223 \u2223 \u22231\u2212 1 l \u2212 1\nl\u22121 \u2211\nj=2\n(j \u2212 1) (1\u2212 x)l\u2212j\u22121 x \u2223 \u2223 \u2223 .\nWe have\n0 \u2264 1 l \u2212 1\nl\u22121 \u2211\nj=2\n(j \u2212 1) (1\u2212 x)l\u2212j\u22121 x \u2264 l\u22121 \u2211\nj=2\n(1\u2212 x)l\u2212j\u22121 x\n=\nl\u22121 \u2211\nj=2\n(1\u2212 x)l\u2212j\u22121 \u2212 (1\u2212 x)l\u2212j = 1\u2212 (1\u2212 x)l\u22122 \u2264 1\nTherefore\n\u2016I \u2212 \u03b3k m\nl\u22121 \u2211\nj=2\nj \u2212 1 l \u2212 1\nl\u22121 \u220f\ni=j+1\n( I \u2212 \u03b3k m T ) T \u2016 \u2264 1 .\nUsing the last inequality, we derive\n\u2016\u03bek,l\u2016HS \u2264 l \u2212 1 l \u2225 \u2225 \u2225\nm \u220f\ni=l+1\ni\u2212 1 i ( I \u2212 \u03b3k m Txi )\u2225 \u2225 \u2225\u2016Txl \u2212 T \u2016HS\u2016I \u2212 \u03b3k m\nl\u22121 \u2211\nj=2\nj \u2212 1 l \u2212 1\nl\u22121 \u220f\ni=j+1\n( I \u2212 \u03b3k m T ) T \u2016\n\u2264 l \u2212 1 l \u2016Txl \u2212 T \u2016HS m \u220f\ni=l+1\ni\u2212 1 i\n\u2264 l \u2212 1 m (\u2016Txj\u2016HS + \u2016T \u2016HS) \u2264 2\u03ba2 .\nThen applying Theorem A.1 to \u03bek,l, with probability greater than 1\u2212 \u03b4\n\u2016 1 m\nm \u2211\nl=3\n\u03bek,l\u2016 \u2264 16\u03ba2\n3 \u221a m\nlog 2\n\u03b4 . (C.14)\nOn the other hand, \u2225\n\u2225 \u2225 \u2225 \u2225\nm \u220f\nl=3\nl \u2212 1 l ( I \u2212 \u03b3k m Txl ) Tx2 \u2212 T 2\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2264 2\u03ba2/m, (C.15)\ntherefore, combining (C.14) with (C.15), and recalling (C.13), with probability greater than 1\u2212 \u03b4\n1\nm\n\u2225 \u2225 \u2225QmT \u2225 \u2225 \u2225\nHS \u2264\n(\n2\u03ba2 m2 + 16\u03ba2 3 \u221a m log 2 \u03b4\n)\n\u03ba2 (C.16)\nThe statement then follows recalling the decomposition in (C.8), and summing (C.16) with (C.12).\nLEMMA C.6. Let (\u03b3k)k\u2208N be a sequence in ]0,m\u03ba\u22122[. For any \u03b4 \u2208 ]0, 1[ and k \u2208 N,\nP\n(\n\u2016b\u0302k \u2212 bk\u2016H \u2264 2\u03ba2M2\nm2 +\n64\u03ba2M2\n9 \u221a m\nlog 2\n\u03b4\n)\n\u2265 1\u2212 \u03b4 (C.17)\nProof. Using the same notation as in Lemma C.5, from the definition of b\u0302k and bk in equations (B.3) and (B.10) respectively, we have\nb\u0302k \u2212 bk = 1\nm2\nm \u2211\nj=1\nB\u0302k,j\nj\u22121 \u2211\nl=1\nS\u2217xlyl \u2212 1\nm2\nm \u2211\nj=1\nBk,j\nj\u22121 \u2211\nl=1\nS\u2217f\u03c1 . (C.18)\nStarting from this decomposition, the proof follows the same line as the one of Lemma C.5.\nWe are now ready to prove the probabilistic upper bound on the sample error. Proof. of Theorem 6.1 By Lemma C.1 we get\nf\u0302t \u2212 ft = t\u22121 \u2211\nk=0\n\u03b3k\n\n\nt\u22121 \u220f\nj=k+1\n(I \u2212 \u03b3jT + \u03b32jAj)\n\n\u03c7k (C.19)\nwith \u03c7k defined as in (C.2), for f\u03020 = f0 = 0. Hence, applying Lemma C.2,\n\u2016f\u0302t \u2212 ft\u2016H \u2264 t\u22121 \u2211\nk=1\n\u03b3k\u2016\u03c7k\u2016H. (C.20)\nRecalling that\n\u03c7k = (T \u2212 T\u0302 )f\u0302k + \u03b3k(A\u0302k \u2212Ak)f\u0302k + ( 1\nm\nm \u2211\ni=1\nS\u0302\u2217xiyi \u2212 S \u2217f\u03c1) + \u03b3k(bk \u2212 b\u0302k) .\nthen, by Lemmas C.3, C.4, C.6, and C.5, we get\n\u2016\u03c7k\u2016H \u2264 ( 16\u03ba2\n3 \u221a m\nlog 2\n\u03b4 + \u03b3k\n(2\u03ba4\nm2 +\n64\u03ba4 9 \u221a m log 2 \u03b4 )\n)\n\u03baM\nk\u22121 \u2211\ni=0\n\u03b3i\n+ 16\u03baM\n3 \u221a m\nlog 2\n\u03b4 + \u03b3k\n(2\u03ba2M2\nm2 +\n64\u03ba2M2\n9 \u221a m\nlog 2\n\u03b4\n)\nThe previous bound implies that, (if \u03b4 is sufficiently small) there exists a constant C > 0 such that\n\u2016\u03c7k\u2016H \u2264 C\u221a m\n(\nlog 2\n\u03b4\n) k\u22121 \u2211\ni=0\n\u03b3i . (C.21)\nTherefore, from (C.20) we get\n\u2016f\u0302t \u2212 ft\u2016H \u2264 C\u221a m\n(\nlog 2\n\u03b4\n) t\u22121 \u2211\nk=0\n\u03b3k\nk\u22121 \u2211\ni=0\n\u03b3i .\nNext, if (\u2200i \u2208 N), \u03b3i = \u03ba\u22122(i+ 1)\u2212\u03b8, then \u2211k\u22121 i=0 \u03b3i \u2264 1\u03ba2 (1 \u2212 \u03b8)k1\u2212\u03b8 and thus t\u22121 \u2211\nk=0\n\u03b3k\nk\u22121 \u2211\ni=0\n\u03b3i \u2264 1\n\u03ba4(1\u2212 \u03b8)\nt\u22121 \u2211\nk=0\n(k + 1)1\u22122\u03b8 .\nAn analogous computation shows that\n1\n\u03ba4(1\u2212 \u03b8)\nt\u22121 \u2211\nk=0\n(k + 1)1\u22122\u03b8 \u2264 1 2\u03ba4(1\u2212 \u03b8)2 (t+ 1) 2(1\u2212\u03b8),\nwhich gives the statement. Proof. of Theorem 6.2 Fix m \u2208 N, m \u2265 2. Let Pf\u03c1 be the projection onto the closure (in L2(X , \u03c1X )) of the range of S. By Remark B.3, we know that the t-th epoch of the incremental gradient descent iteration in (B.9) coincides with m steps of gradient descent with stepsize \u03b3t/m. More precisely, (\u2200t \u2208 N), ft = hmt, where h0 = 0 and\nhk+1 = (I \u2212 \u03b7kT )hk + \u03b7kS\u2217Pf\u03c1, \u03b7k = \u03b3t m , for k \u2208 [mt, (m+ 1)t\u2212 1] .\nFrom equation (A.1), it follows\nhk+1 =\nk \u2211\nj=0\nk \u220f\ni=j+1\n( I \u2212 \u03b7iT ) \u03b7jS \u2217Pf\u03c1 .\nHence\n\u2225 \u2225Shk+1 \u2212 Pf\u03c1 \u2225 \u2225\n\u03c1 =\n\u2225 \u2225(S\nk \u2211\nj=0\n\u03b7j\nk \u220f\ni=j+1\n( I \u2212 \u03b7iT ) S\u2217 \u2212 I)Pf\u03c1\u2016\u03c1 (C.22)\nBy the spectral theorem, see e.g. [17, equation (2.43)],\n\u2225 \u2225Shk+1 \u2212 Pf\u03c1 \u2225 \u2225\n\u03c1 =\n\u2225 \u2225(L\nk \u2211\nj=0\n\u03b7j\nk \u220f\ni=j+1\n( I \u2212 \u03b7iL ) \u2212 I)Pf\u03c1\u2016\u03c1 . (C.23)\nDefine the spectral function rt :]0, \u2016L\u2016[\u2192 R\nrt(\u03bb) = \u03bb k \u2211\nj=0\n\u03b7j\nk \u220f\ni=j+1\n( I \u2212 \u03b7i\u03bb ) \u2212 1 . (C.24)\nIt follows from the definition that Pf\u03c1 \u2208 R(L), for R(S) = R(L) by [35, Theorem 11.2(b)]. Thus, Pf\u03c1 \u2208 N(L)\u22a5. Then, by (C.23) and definition of rt, if (\u03c3n)n\u2208N are the strictly positive eigenvalues of L, and (vn)n\u2208N is the corresponding family of eigenvectors in L2(X , \u03c1X ), \u2225\n\u2225Shk+1 \u2212 Pf\u03c1 \u2225 \u2225 2 \u03c1 = \u2016rt(L)Pf\u03c1\u20162\u03c1 =\n+\u221e \u2211\nn=1\n(\n\u03c3n\nk \u2211\nj=0\n\u03b7j\nk \u220f\ni=j+1\n( 1\u2212 \u03b7i\u03c3n ) \u2212 1 )2 |\u3008Pf\u03c1, vn\u3009|2\n=\n+\u221e \u2211\nn=1\n( k \u220f\ni=0\n( 1\u2212 \u03b7i\u03c3n )\n)2\n|\u3008Pf\u03c1, vn\u3009|2 (C.25)\n(C.26)\nNote that (\u2200n \u2208 N), \u03b7i\u03c3n \u2208 ]0, 1[, and\n0 \u2264 k \u220f\ni=0\n( 1\u2212 \u03b7i\u03c3n ) \u2264 e\u2212\u03c3n \u2211k i=0 \u03b7i .\nSince \u03c3n > 0 and limk\u2192\u221e \u2211k i=0 \u03b7i = +\u221e, it follows that limk\u2192+\u221e e\u2212\u03c3n \u2211k\ni=0 \u03b7i = 0. On the other hand,\n0 \u2264 +\u221e \u2211\nn=1\n( k \u220f\ni=0\n( 1\u2212 \u03b7i\u03c3n )\n)2\n|\u3008Pf\u03c1, vn\u3009|2 \u2264 \u2016Pf\u03c1\u20162\u03c1. (C.27)\nHence, from equation (C.25)\nlim k\u2192+\u221e\n\u2016Shk+1 \u2212 Pf\u03c1 \u2225 \u2225 2\n\u03c1 = lim\nk\u2192+\u221e\n+\u221e \u2211\nn=1\n( k \u220f\ni=0\n( 1\u2212 \u03b7i\u03c3n )\n)2\n|\u3008Pf\u03c1, vn\u3009|2 = 0\nwhere the last equality follows by applying the dominated convergence theorem, which can be used thanks to (C.27). Equation (6.2) then follows recalling that ft = hmt.\nIf condition (4.2) holds, we can apply [37, Theorem 2.10] to (hk)k\u2208N. We have\nE(ft)\u2212 inf f\u2208H E = E(hmt\u2212 inf f\u2208H\nE) \u2264 (r/e)2r (\nt\u22121 \u2211\nj=0\n( (j+1)m\u22121 \u2211\ni=jm\n\u03b3j m\n))\u22122r\n= (r/e)2r(\nt\u22121 \u2211\nj=0\n\u03b3j) \u22122r .\n(C.28)\nAppendix D. Consistency with respect to the norm in H. In this section we collect two main theorems, which are the analogous of Theorems 4.1 and 4.2, respectively.\nTHEOREM D.1. Under the same assumptions as in Theorem 4.1, if E has minimizers on H, and fH is the one of minimal norm,\nlim m\u2192\u221e\n\u2016f\u0302t\u2217 \u2212 fH\u2016H = 0 almost surely.\nTHEOREM D.2. Under the same assumptions as in Theorem 4.2, suppose additionally that r \u2265 1/2. Then E has minimizers, so that fH exists. Moreover, for some constant C > 0,\n\u2016f\u0302t \u2212 fH\u2016H \u2264 C 2\u03ba4(1 \u2212 \u03b8)2 ( log 2 \u03b4 ) (t+ 1)2(1\u2212\u03b8) m + R((r \u2212 1/2)/e)(r\u22121/2) \u03ba2(1\u2212 \u03b8) t (\u2212r+1/2)(1\u2212\u03b8) . (D.1) If we choose the stopping rule t\u2217(m) = \u2308m 1 (3+2r)(1\u2212\u03b8) \u2309 then there exists a constant D such that, with probability at least 1\u2212 \u03b4,\n\u2016f\u0302t\u2217(m))\u2212 fH\u2016H \u2264 D log 2\n\u03b4 m\u2212\n2r\u22121 3+2r . (D.2)\nThe proof of the previous theorems rely on the following decomposition into sample ad approximation error\n\u2016f\u0302t \u2212 fH\u2016H \u2264 \u2016f\u0302t \u2212 ft\u2016H + \u2016ft \u2212 fH\u2016H. (D.3)\nTo estimate the sample error, we can rely on Theorem 6.1. An upper bound of the approximation error is given in the following theorem\nTHEOREM D.3 (Approximation error in H). Let f0 = 0, (\u03b3t)t\u2208N be such that (\u2200t \u2208 N) \u03b3t \u2208]0,m\u03ba\u22122[, \u2211\nt\u2208N \u03b3t = +\u221e and (ft)t\u2208N be defined as in (B.9). Assume that (4.2) is satisfied for some r \u2265 1/2. Then fH exists and\n\u2016ft \u2212 fH\u2016H \u2264 R((r \u2212 1/2)/e)(r\u22121/2)( t\u22121 \u2211\ni=0\n\u03b3i) \u2212r+1/2 .\nIn addition, if \u03b3t = \u03ba\u22122(t+ 1)\u2212\u03b8 for some \u03b8 \u2208 [0, 1[\n\u2016ft \u2212 fH\u2016H \u2264 R((r \u2212 1/2)/e)(r\u22121/2)\n\u03ba2(1\u2212 \u03b8) t (\u2212r+1/2)(1\u2212\u03b8) .\nThe theorem follows from [37, Theorem 2.10]. Proof. of Theorem D.1 Equation (D.1) immediately follows by applying Theorems 6.1 and D.3. Next let t\u2217(m) = \u2308m\u03b1\u2309. substituting this choice of t into the right hand side of (D.1), and minimizing over \u03b1, we get the linear equation\n2\u03b1(1\u2212 \u03b8)\u2212 1 = \u03b1(\u2212r + 1/2)(1\u2212 \u03b8), (D.4)\nwhose solution is \u03b1 = 1/((3 + 2r)(1 \u2212 \u03b8))."}], "references": [{"title": "Theory of reproducing kernels", "author": ["N. Aronszajn"], "venue": "Trans. Amer. Math. Soc,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1950}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate o(1/n)", "author": ["F. Bach", "E. Moulines"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "On regularization algorithms in learning theory", "author": ["Frank Bauer", "Sergei Pereverzev", "Lorenzo Rosasco"], "venue": "Journal of complexity,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "A new class of incremental gradient methods for least squares problems", "author": ["Dimitri P. Bertsekas"], "venue": "SIAM J. Optim.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Gradient convergence in gradient methods with errors", "author": ["Dimitri P Bertsekas", "John N Tsitsiklis"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Optimal learning rates for kernel conjugate gradient regression", "author": ["G. Blanchard", "N. Kr\u00e4mer"], "venue": "In Advances in Neural Inf. Proc. Systems (NIPS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Optimal rates for regularized least-squares algorithm", "author": ["A. Caponnetto", "E. De Vito"], "venue": "Found. Comput. Math.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Adaptive rates for regularization operators in learning theory", "author": ["A. Caponnetto", "Yuan Yao"], "venue": "Analysis and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "On the mathematical foundations of learning", "author": ["Felipe Cucker", "Steve Smale"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Learning Theory: An Approximation Theory Viewpoint", "author": ["Felipe Cucker", "Ding Xuan Zhou"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Model selection for regularized least-squares algorithm in learning theory", "author": ["E. De Vito", "A. Caponnetto", "L. Rosasco"], "venue": "Found. Comput. Math.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Learning from examples as an inverse problem", "author": ["E. De Vito", "L. Rosasco", "A. Caponnetto", "U. De Giovannini", "F. Odone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "A Probabilistic Theory of Pattern Recognition", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "Number 31 in Applications of mathematics", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Sample-based non-uniform random variate generation", "author": ["Luc Devroye"], "venue": "In Proceedings of the 18th conference on Winter simulation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Regularization of inverse problems", "author": ["H.W. Engl", "M. Hanke", "A. Neubauer"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}, {"title": "Random design analysis of ridge regression", "author": ["Daniel Hsu", "Sham M Kakade", "Tong Zhang"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Online learning with kernels", "author": ["Jyrki Kivinen", "Alexander J Smola", "Robert C Williamson"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Stochastic approximation algorithms and applications, volume 35 of Applications of Mathematics (New York)", "author": ["Harold J. Kushner", "G. George Yin"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets", "author": ["Nicolas Le Roux", "Mark Schmidt", "Francis Bach"], "venue": "arXiv preprint arXiv:1202.6258,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Spectral algorithms for supervised learning", "author": ["Laura Lo Gerfo", "Lorenzo Rosasco", "Francesca Odone", "Ernesto De Vito", "Alessandro Verri"], "venue": "Neural Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Regularization in kernel learning", "author": ["Shahar Mendelson", "Joseph Neeman"], "venue": "The Annals of Statistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Incremental subgradient methods for nondifferentiable optimization", "author": ["Angelia Nedic", "Dimitri P Bertsekas"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. Optim.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence o(1/k)", "author": ["Y. Nesterov"], "venue": "Doklady AN SSSR,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1983}, {"title": "Optimum bounds for the distributions of martingales in Banach spaces", "author": ["Iosif Pinelis"], "venue": "Ann. Probab.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1994}, {"title": "Early stopping for non-parametric regression: An optimal data-dependent stopping rule", "author": ["Garvesh Raskutti", "Martin J Wainwright", "Bin Yu"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Methods of modern mathematical physics. I. Functional analysis", "author": ["Michael Reed", "Barry Simon"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1972}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["Ingo Steinwart"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2002}, {"title": "Support Vector Machines", "author": ["Ingo Steinwart", "Andreas Christmann"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Optimal rates for regularized least squares regression", "author": ["Ingo Steinwart", "Don R. Hush", "Clint Scovel"], "venue": "In COLT,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Online learning as stochastic approximation of regularization paths", "author": ["Pierre Tarr\u00e8s", "Yuan Yao"], "venue": "arXiv preprint arXiv:1103.5538,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Introduction to functional analysis", "author": ["Angus E. Taylor", "David C. Lay"], "venue": "2nd ed. (Reprint of the orig", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1980}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["Joel A. Tropp"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "On early stopping in gradient descent learning", "author": ["Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto"], "venue": "Constructive Approximation,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Online gradient descent learning algorithms", "author": ["Yiming Ying", "Massimiliano Pontil"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "Sums and Gaussian vectors, volume 1617 of Lecture Notes in Mathematics", "author": ["Vadim Yurinsky"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1995}, {"title": "Learning bounds for kernel regression using effective data dimensionality", "author": ["Tong Zhang"], "venue": "Neural Computation,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2005}, {"title": "Boosting with early stopping: Convergence and consistency", "author": ["Tong Zhang", "Bin Yu"], "venue": "Annals of Statistics,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}], "referenceMentions": [{"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 36, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 2, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 5, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 7, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 28, "context": "While early stopping has been recently studied for classical gradient descent learning and related algorithms [41, 37, 3, 6, 8, 29], we are not aware of similar studies for online learning.", "startOffset": 110, "endOffset": 131}, {"referenceID": 19, "context": "The situation typically analyzed in this context is the one where each iteration of the algorithm corresponds to a new input-output pair [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "[25], we are not interested in convergence of the algorithm to the minimum of the empirical risk, but rather to the one of the expected risk.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "In particular, the gradients are not bounded, unlike in most studies in stochastic optimization [26].", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "Here we note that, the results more closely related to the analysis we present are those in [2, 34, 38] where consistency and finite sample bounds are derived.", "startOffset": 92, "endOffset": 103}, {"referenceID": 33, "context": "Here we note that, the results more closely related to the analysis we present are those in [2, 34, 38] where consistency and finite sample bounds are derived.", "startOffset": 92, "endOffset": 103}, {"referenceID": 37, "context": "Here we note that, the results more closely related to the analysis we present are those in [2, 34, 38] where consistency and finite sample bounds are derived.", "startOffset": 92, "endOffset": 103}, {"referenceID": 1, "context": "In [2] a single pass over the data is shown to suffice, however the analysis is restricted to a finite dimensional setting, and the generalization performance is empirically shown to be still increasing after the first epoch.", "startOffset": 3, "endOffset": 6}, {"referenceID": 33, "context": "In [34] and [38] the step size and/or a penalization parameter need to be chosen in a distribution dependent way (or by cross validation) and multiple passes over \u2217 DIBRIS, Universit\u00e0 di Genova, Via Dodecaneso, 35, 16146, Genova, Italy, (lrosasco@mit.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "In [34] and [38] the step size and/or a penalization parameter need to be chosen in a distribution dependent way (or by cross validation) and multiple passes over \u2217 DIBRIS, Universit\u00e0 di Genova, Via Dodecaneso, 35, 16146, Genova, Italy, (lrosasco@mit.", "startOffset": 12, "endOffset": 16}, {"referenceID": 31, "context": "[32] Lemma A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Further, consider a separable reproducing kernel Hilbert space (RKHS) H [1], with inner product (norm) denoted by \u3008\u00b7, \u00b7\u3009H (\u2016\u00b7\u2016H), and a measurable reproducing kernelK : X\u00d7X \u2192 R.", "startOffset": 72, "endOffset": 75}, {"referenceID": 31, "context": "2) When H is universal [32] this is exactly universal consistency, see for example [15].", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "2) When H is universal [32] this is exactly universal consistency, see for example [15].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "As it is known, the latter result can only be derived under suitable assumptions on \u03c1 and it is equivalent to considering learning rates [16, 31].", "startOffset": 137, "endOffset": 145}, {"referenceID": 30, "context": "As it is known, the latter result can only be derived under suitable assumptions on \u03c1 and it is equivalent to considering learning rates [16, 31].", "startOffset": 137, "endOffset": 145}, {"referenceID": 25, "context": "Note however that due to the lack of strong convexity, unboundedness of X and unboundedness of the gradients, the classical assumptions required to apply stochastic gradient descent are not satisfied [26].", "startOffset": 200, "endOffset": 204}, {"referenceID": 4, "context": "We consider the estimator obtained applying the incremental gradient descent (IGD) algorithm [5, 4] to empirical risk minimization, inf f\u2208H \u00ca(f), \u00ca(f) = 1 m m \u2211", "startOffset": 93, "endOffset": 99}, {"referenceID": 3, "context": "We consider the estimator obtained applying the incremental gradient descent (IGD) algorithm [5, 4] to empirical risk minimization, inf f\u2208H \u00ca(f), \u00ca(f) = 1 m m \u2211", "startOffset": 93, "endOffset": 99}, {"referenceID": 24, "context": "stochastic [25], can be considered and might lead to different behaviors, but we leave the analysis of these latter cases for future study.", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": ", k = i (c t )k, k 6= i The above algorithm is closely related, yet different, to the one discussed in [19], where an explicit regularization is considered (choose \u03bbk > 0 in the following Equation (3.", "startOffset": 103, "endOffset": 107}, {"referenceID": 40, "context": "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.", "startOffset": 89, "endOffset": 107}, {"referenceID": 36, "context": "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.", "startOffset": 89, "endOffset": 107}, {"referenceID": 2, "context": "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.", "startOffset": 89, "endOffset": 107}, {"referenceID": 5, "context": "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.", "startOffset": 89, "endOffset": 107}, {"referenceID": 28, "context": "Our results extend to incremental gradient the analysis of early stopping carried out in [41, 37, 3, 6, 29] for the classical (batch) gradient descent and shed light on the effect of considering multiple passes over the data in online learning algorithms.", "startOffset": 89, "endOffset": 107}, {"referenceID": 21, "context": "Indeed, early stopping has long been applied as a heuristic to achieve regularization, especially in the context of neural networks [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 18, "context": "Here, we added an explicit regularization parameter \u03bb as in [19], see also (3.", "startOffset": 60, "endOffset": 64}, {"referenceID": 33, "context": "The question of the consistency of online learning algorithms when H can be infinite dimensional has been considered in [34, 38] (see also references therein).", "startOffset": 120, "endOffset": 128}, {"referenceID": 37, "context": "The question of the consistency of online learning algorithms when H can be infinite dimensional has been considered in [34, 38] (see also references therein).", "startOffset": 120, "endOffset": 128}, {"referenceID": 33, "context": "In [34] it is shown that if the step size \u03b3k and the regularization parameter \u03bbk are chosen as suitable functions of the number of points, then the corresponding algorithm can be universally consistent with probability one.", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "In [38] it is shown that indeed similar results can be derived for \u03bbk = 0 for all k \u2208 N, but an horizon, that is the total number of points to be considered, needs to be known a priori to appropriately choose the step-size.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "Recently, a similar analysis, \u03bbk = 0 for all k \u2208 N, is developed in [2] in a finite dimensional setting.", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "increases, see [5] and references therein.", "startOffset": 15, "endOffset": 18}, {"referenceID": 20, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "If the data are indeed generated by a stochastic process, then there is a classic approach, sometimes called online-to-batch conversion [9], to convert regret bounds to expected risk bounds.", "startOffset": 136, "endOffset": 139}, {"referenceID": 12, "context": "2) is fairly standard (see [13], and [12, Section 4] for a discussion).", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Lower bounds are known [7, 33] under assumption (4.", "startOffset": 23, "endOffset": 30}, {"referenceID": 32, "context": "Lower bounds are known [7, 33] under assumption (4.", "startOffset": 23, "endOffset": 30}, {"referenceID": 39, "context": "6) This latter property can be interpreted as a measure of the effective dimensionality of the hypotheses space [40, 7].", "startOffset": 112, "endOffset": 119}, {"referenceID": 6, "context": "6) This latter property can be interpreted as a measure of the effective dimensionality of the hypotheses space [40, 7].", "startOffset": 112, "endOffset": 119}, {"referenceID": 10, "context": "2) implies that the infimum of the expected risk over H is achieved [11], and it is known that sharp bounds are harder to get if r < 1/2 (see discussion in [33]).", "startOffset": 68, "endOffset": 72}, {"referenceID": 32, "context": "2) implies that the infimum of the expected risk over H is achieved [11], and it is known that sharp bounds are harder to get if r < 1/2 (see discussion in [33]).", "startOffset": 156, "endOffset": 160}, {"referenceID": 16, "context": "The bound is proved generalizing classical results in inverse problems and is known to be essentially sharp [17].", "startOffset": 108, "endOffset": 112}, {"referenceID": 39, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 48, "endOffset": 67}, {"referenceID": 6, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 48, "endOffset": 67}, {"referenceID": 32, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 48, "endOffset": 67}, {"referenceID": 23, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 48, "endOffset": 67}, {"referenceID": 17, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 48, "endOffset": 67}, {"referenceID": 22, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 158, "endOffset": 162}, {"referenceID": 40, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 36, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 2, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 5, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 7, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 28, "context": "kernel ridge regression) has been considered in [40, 7, 33, 24, 18], and iterative methods, as well as a larger class of so called spectral filtering methods [23] have been analyzed in [41, 37, 3, 6, 8, 29].", "startOffset": 185, "endOffset": 206}, {"referenceID": 33, "context": "As we mentioned before, online learning algorithms are considered in [34, 38] where optimal bounds are derived in the capacity independent setting.", "startOffset": 69, "endOffset": 77}, {"referenceID": 37, "context": "As we mentioned before, online learning algorithms are considered in [34, 38] where optimal bounds are derived in the capacity independent setting.", "startOffset": 69, "endOffset": 77}, {"referenceID": 1, "context": "The bound in the finite dimensional setting derived in [2] are also optimal.", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "Our analysis follows the one in [8].", "startOffset": 32, "endOffset": 35}, {"referenceID": 28, "context": "We end noting that, while an hold-out procedure requires splitting the data, it does not worsen the computational complexity of the algorithm, unlike other model selection criterions [29].", "startOffset": 183, "endOffset": 187}, {"referenceID": 4, "context": "Particularly, stochastic variants of the IGD algorithms, where the indices in each cycle are randomly selected [5], and also accelerated version as proposed in [27].", "startOffset": 111, "endOffset": 114}, {"referenceID": 26, "context": "Particularly, stochastic variants of the IGD algorithms, where the indices in each cycle are randomly selected [5], and also accelerated version as proposed in [27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 2, "context": "Ideas related to this latter point have been considered in [3, 6, 8] for batch gradient techniques (the gradient of the empirical risk is considered in each iteration).", "startOffset": 59, "endOffset": 68}, {"referenceID": 5, "context": "Ideas related to this latter point have been considered in [3, 6, 8] for batch gradient techniques (the gradient of the empirical risk is considered in each iteration).", "startOffset": 59, "endOffset": 68}, {"referenceID": 7, "context": "Ideas related to this latter point have been considered in [3, 6, 8] for batch gradient techniques (the gradient of the empirical risk is considered in each iteration).", "startOffset": 59, "endOffset": 68}, {"referenceID": 2, "context": "In particular in [3] it is shown that a variant of gradient descent, sometimes called the \u03bd-method, can obtain the same generalization guarantees of non accelerated gradient descent learning, but using much fewer iterations.", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "REFERENCES [1] N.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Dimitri P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Dimitri P Bertsekas and John N Tsitsiklis.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] G.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Felipe Cucker and Steve Smale.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Felipe Cucker and Ding Xuan Zhou.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Luc Devroye.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Daniel Hsu, Sham M Kakade, and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Jyrki Kivinen, Alexander J Smola, and Robert C Williamson.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Harold J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Nicolas Le Roux, Mark Schmidt, and Francis Bach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Laura Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto De Vito, and Alessandro Verri.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Shahar Mendelson and Joseph Neeman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Angelia Nedic and Dimitri P Bertsekas.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] Iosif Pinelis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] Garvesh Raskutti, Martin J Wainwright, and Bin Yu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] Michael Reed and Barry Simon.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] Ingo Steinwart.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] Ingo Steinwart and Andreas Christmann.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] Ingo Steinwart, Don R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] Pierre Tarr\u00e8s and Yuan Yao.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] Angus E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] Joel A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] Yiming Ying and Massimiliano Pontil.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] Vadim Yurinsky.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40] Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] Tong Zhang and Bin Yu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "2) The following concentration inequality due to [28] (see also [39] and [34, Proposition A.", "startOffset": 49, "endOffset": 53}, {"referenceID": 38, "context": "2) The following concentration inequality due to [28] (see also [39] and [34, Proposition A.", "startOffset": 64, "endOffset": 68}, {"referenceID": 35, "context": "While more refined concentration inequality could be considered [36], this would affect only the constants in the bound which are not the main focus of this paper.", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "3, we will also use a variant of the previous Bernstein concentration inequality, which is taken from [8].", "startOffset": 102, "endOffset": 105}, {"referenceID": 13, "context": "1, taking into account that \u2016T \u2016HS \u2264 \u03ba and \u2016Txi\u2016HS \u2264 \u03ba (see also [14]) LEMMA C.", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "22 and Exercise 28 in [30]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": "T \u2016 = sup x\u2208[0,1] \u2223", "startOffset": 12, "endOffset": 17}], "year": 2017, "abstractText": "Abstract. We study the learning algorithm corresponding to the incremental gradient descent defined by the empirical risk over an infinite dimensional hypotheses space. We consider a statistical learning setting and show that, provided with a universal step-size and a suitable early stopping rule, the learning algorithm thus obtained is universally consistent and derive finite sample bounds. Our results provide a theoretical foundation for considering early stopping in online learning algorithms and shed light on the effect of allowing for multiple passes over the data.", "creator": "LaTeX with hyperref package"}}}