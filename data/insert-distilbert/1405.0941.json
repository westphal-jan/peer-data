{"id": "1405.0941", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2014", "title": "Towards a Benchmark of Natural Language Arguments", "abstract": ". the connections among natural language processing and argumentation theory are becoming stronger in the latest years, with it a much growing amount of works going in this direction, in different scenarios and applying heterogeneous neural techniques. so in this paper, we present two datasets we built to cope with the combination work of the textual entailment cognition framework and bipolar abstract filter argumentation. in our approach, such datasets are used to automatically identify neurons through a textual entailment system the relations among the arguments ( i. = e., attack, support ), and then the resulting bipolar argumentation graphs are analyzed to compute the accepted arguments.", "histories": [["v1", "Mon, 5 May 2014 16:03:04 GMT  (178kb,D)", "http://arxiv.org/abs/1405.0941v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["elena cabrio", "serena villata"], "accepted": false, "id": "1405.0941"}, "pdf": {"name": "1405.0941.pdf", "metadata": {"source": "META", "title": "Towards a Benchmark of Natural Language Arguments", "authors": ["Elena Cabrio", "Serena Villata"], "emails": [], "sections": [{"heading": "Introduction", "text": "Until recent years, the idea of \u201cargumentation\u201d as the process of creating arguments for and against competing claims was a subject of interest to philosophers and lawyers. In recent years, however, there has been a growth of interest in the subject from formal and technical perspectives in Artificial Intelligence, and a wide use of argumentation technologies in practical applications. However, such applications are always constrained by the fact that natural language arguments cannot be automatically processed by such argumentation technologies. Arguments are usually presented either as the abstract nodes of a directed graph where the edges represent the relations of attack and support (e.g., in abstract argumentation theory (Dung 1995) and in bipolar argumentation (Cayrol and Lagasquie-Schiex 2005), respectively).\nNatural language arguments are usually used in the argumentation literature to provide ad-hoc examples to help the reader in the understanding of the rationale behind the formal approach which is then introduced, but the need to find automatic ways to process natural language arguments is becoming more and more important. On the one side, when dealing with natural language processing techniques, the first step consists in finding the data on which the system is trained and evaluated. On the other side, in argumentation theory there is a growing need to define benchmarks for argumentation to test implemented systems and proposed theories. In this paper, we address the following research question: how to build a dataset of natural language arguments?\nThe definition of a dataset of natural language arguments is not a straightforward task: first, there is the need to iden-\ntify the kind of natural language arguments to be collected (e.g., online debates, newspaper articles, blogs and forums, etc.), and second, there is the need to annotate the data according to the addressed task from the natural language processing point of view (e.g., classification, textual entailment (Dagan et al. 2009), etc.).\nOur goal (Cabrio and Villata 2013) is to analyze natural language debates in order to understand, given a huge debate, what are the winning arguments (through acceptability semantics) and who proposed them. In order to achieve such goal, we have identified two different scenarios to extract our data: (i) online debate platforms like Debatepedia1 and ProCon2 present a set of topics to be discussed, and participants argue about the issue the platform proposes on a selected topic, highlighting whether their \u201carguments\u201d are in favor or against the central issue, or with respect to the other participants\u2019 arguments, and (ii) the screenplay of a movie titled \u201cTwelve Angry Men\u201d where the jurors of a trial discuss in order to decide whether a young boy is guilty or not, and before the end of each act they vote to verify whether they all agree about his guiltiness. These two scenarios lead to two different resources: the online debates resource collects the arguments in favor or against the main issue or the other arguments into small bipolar argumentation graphs, while the \u201cTwelve Angry Men\u201d resource collects again pro and con arguments but they compose three bipolar argumentation graphs whose complexity is higher than debates graphs. Note that the first resource consists of an integration of the dataset of natural language arguments we presented in (Cabrio and Villata 2013) with new data extracted from the ProCon debate platform.\nThese two resources represent a first step towards the construction of a benchmark of natural language arguments, to be exploited by existing argumentation systems as data-driven examples of argumentation frameworks. In our datasets, arguments are cast into pairs where the two arguments composing the pair are linked by a positive relation (a support relation in argumentation) or a negative relation (an attack relation in argumentation). From these pairs, the argumentation graphs are constructed.\nThe remainder of the paper is organized as follows:\n1http://idebate.org/debatabase 2http://www.procon.org/\nar X\niv :1\n40 5.\n09 41\nv1 [\ncs .A\nI] 5\nM ay\n2 01\n4\nthe next section presents the two datasets from Debatepedia/ProCon and Twelve Angry Men and how they have been extracted and annotated, then some conclusions are drawn."}, {"heading": "Natural Language Arguments: datasets", "text": "As introduced before, the rationale underlying the datasets of natural language arguments we created was to support the task of understanding, given a huge debate, what are the winning arguments, and who proposed them. In an application framework, we can divide such task into two consecutive subtasks, namely i) the recognition of the semantic relations between couples of arguments in a debate (i.e. if one statement is supporting or attacking another claim), ii) and given all the arguments that are part of a debate and the acceptability semantics, to reason over the graph of arguments with the aim of deciding which are the accepted ones.\nTo reflect this separation into two subtasks, each dataset that we will describe in detail in the following subsections is therefore composed of two layers. Given a set of arguments linked among them (e.g in a debate):\n1. we couple each argument with the argument to which it is related (i.e. that it attacks or supports). The first layer of the dataset is therefore composed of couples of arguments (each one labeled with a univocal ID), annotated with the semantic relations linking them (i.e. attack or support);\n2. starting from the pairs of arguments in the first layer of the dataset, we then build a bipolar entailment graph for each of the topics in the dataset. In the second layer of the dataset, we find therefore graphs of arguments, where the arguments are the nodes of the graph, and the relations among the arguments correspond to the edges of the graphs.\nTo create the data set of arguments pairs, we follow the criteria defined and used by the organizers of the Recognizing Textual Entailment challenge.3 To test the progress of TE systems in a comparable setting, the participants to RTE challenge are provided with data sets composed of T-H pairs involving various levels of entailment reasoning (e.g. lexical, syntactic), and TE systems are required to produce a correct judgment on the given pairs (i.e. to say if the meaning of one text snippet can be inferred from the other). Two kinds of judgments are allowed: two-way (yes or no entailment) or three-way judgment (entailment, contradiction, unknown). To perform the latter, in case there is no entailment between T and H systems must be able to distinguish whether the truth of H is contradicted by T, or remains unknown on the basis of the information contained in T. To correctly judge each single pair inside the RTE data sets, systems are expected to cope both with the different linguistic phenomena involved in TE, and with the complex ways in which they interact. The data available for the RTE challenges are not suitable for our goal, since the pairs are extracted from news and are not linked among each others (i.e. they do not report\n3Since its inception in 2004, the PASCAL RTE Challenges have promoted research in RTE http://www.nist.gov/ tac/2010/RTE/\nopinions on a certain topic). However, the task of recognizing semantic relations among pairs of textual fragments is very close to ours, and therefore we follow the guidelines provided by the organizers of RTE for the creation of their datasets. For instance, in (Cabrio and Villata 2013) we experiment with the application of a TE (Dagan et al. 2009) to automatically identify the arguments in the text and to specify which kind of relation links each couple of arguments."}, {"heading": "Debatepedia dataset", "text": "To build our first benchmark of natural language arguments, we selected Debatepedia and ProCon, two encyclopedias of pro and con arguments on critical issues. To fill in the first layer of the dataset, we manually selected a set of topics (Table 2 column Topics) of Debatepedia/ProCon debates, and for each topic we apply the following procedure:\n1. the main issue (i.e., the title of the debate in its affirmative form) is considered as the starting argument;\n2. each user opinion is extracted and considered as an argument;\n3. since attack and support are binary relations, the arguments are coupled with:\n(a) the starting argument, or (b) other arguments in the same discussion to which the\nmost recent argument refers (i.e., when a user opinion supports or attacks an argument previously expressed by another user, we couple the former with the latter), following the chronological order to maintain the dialogue structure;\n4. the resulting pairs of arguments are then tagged with the appropriate relation, i.e., attack or support4.\nUsing Debatepedia/ProCon as case study provides us with already annotated arguments (pro \u21d2 entailment5, and con \u21d2 contradiction), and casts our task as a yes/no entailment task. To show a step-by-step application of the procedure, let us consider the debated issue Can coca be classified as a narcotic?. At step 1, we transform its title into the affirmative form, and we consider it as the starting argument (a). Then, at step 2, we extract all the users opinions concerning this issue (both pro and con), e.g., (b), (c) and (d):\nExample 1. (a) Coca can be classified as a narcotic.\n(b) In 1992 the World Health Organization\u2019s Expert Committee on Drug Dependence (ECDD) undertook a \u201cprereview\u201d of coca leaf at its 28th meeting. The 28th ECDD report concluded that, \u201cthe coca leaf is appropriately scheduled as a narcotic under the Single Convention on Narcotic Drugs, 1961, since cocaine is readily extractable from the leaf.\u201d This ease of extraction makes coca\n4The data set is freely available at http://www-sop. inria.fr/NoDE/.\n5Here we consider only arguments implying another argument. Arguments \u201csupporting\u201d another argument, but not inferring it will be discussed in the next subsection.\nand cocaine inextricably linked. Therefore, because cocaine is defined as a narcotic, coca must also be defined in this way.\n(c) Coca in its natural state is not a narcotic. What is absurd about the 1961 convention is that it considers the coca leaf in its natural, unaltered state to be a narcotic. The paste or the concentrate that is extracted from the coca leaf, commonly known as cocaine, is indeed a narcotic, but the plant itself is not.\n(d) Coca is not cocaine. Coca is distinct from cocaine. Coca is a natural leaf with very mild effects when chewed. Cocaine is a highly processed and concentrated drug using derivatives from coca, and therefore should not be considered as a narcotic.\nAt step 3a we couple the arguments (b) and (d) with the starting issue since they are directly linked with it, and at step 3b we couple argument (c) with argument (b), and argument (d) with argument (c) since they follow one another in the discussion (i.e. user expressing argument (c) answers back to user expressing argument (b), so the arguments are concatenated - the same for arguments (d) and (c)). At step 4, the resulting pairs of arguments are then tagged with the appropriate relation: (b) supports (a), (d) attacks (a), (c) attacks (b) and (d) supports (c).\nWe have collected 260 T-H pairs (Table 2), 160 to train and 100 to test the TE system. The training set is composed by 85 entailment and 75 contradiction pairs, while the test set by 55 entailment and 45 contradiction pairs. The pairs considered for the test set concern completely new topics.\nBasing on the TE definition, an annotator with skills in linguistics has carried out a first phase of manual annotation of the Debatepedia data set. Then, to assess the validity of the annotation task and the reliability of the obtained data set, the same annotation task has been independently carried out also by a second annotator, so as to compute interannotator agreement. It has been calculated on a sample of 100 argument pairs (randomly extracted).\nThe statistical measure usually used in NLP to calculate the inter-rater agreement for categorical items is Cohen\u2019s kappa coefficient (Carletta 1996), that is generally thought to be a more robust measure than simple percent agreement calculation since \u03ba takes into account the agreement occurring by chance. More specifically, Cohen\u2019s kappa measures the agreement between two raters who each classifies N items into C mutually exclusive categories. The equation for \u03ba is:\n\u03ba = Pr(a)\u2212 Pr(e) 1\u2212 Pr(e)\n(1)\nwhere Pr(a) is the relative observed agreement among raters, and Pr(e) is the hypothetical probability of chance agreement, using the observed data to calculate the probabilities of each observer randomly saying each category. If the raters are in complete agreement then \u03ba = 1. If there is no agreement among the raters other than what would be expected by chance (as defined by Pr(e)), \u03ba = 0. For NLP tasks, the\ninter-annotator agreement is considered as significant when \u03ba >0.6. Applying the formula (1) to our data, the interannotator agreement results in \u03ba = 0.7. As a rule of thumb, this is a satisfactory agreement, therefore we consider these annotated data sets as the goldstandard. The goldstandard is the reference data set to which the performances of automated systems can be compared.\nTo build the bipolar argumentation graphs associated to the Debatepedia dataset, we have considered the pairs annotated in the first layer and we have built a bipolar entailment graph for each of the topic in the dataset (12 topics in the training set and 10 topics in the test set, listed in Table 2).\nFigure 1 shows the average dimension of a bipolar argumentation graph in the Debatepedia/ProCon dataset. Note that no cycle is present, as well as in all the other graphs of such dataset. All graphs are available online, together with the XML data set.\nDebatepedia extended dataset The dataset described in the previous section was created respecting the assumption that the TE relation and the support relation are equivalent, i.e. in all the previously collected pairs both TE and support relations (or contradiction and attack relations) hold.\nFor the second study described in (Cabrio and Villata 2013) we wanted to move a step further, to understand whether it is always the case that support is equivalent to TE\n(and contradiction to attack). We therefore apply again the extraction methodology described in the previous section to extend our data set. In total, our new data set contains 310 different arguments and 320 argument pairs (179 expressing the support relation among the involved arguments, and 141 expressing the attack relation, see Table 2). We consider the obtained data set as representative of human debates in a non-controlled setting (Debatepedia users position their arguments with respect to the others as PRO or CON, the data are not biased).\nAgain, an annotator with skills in linguistics has carried out a first phase of annotation of the extended Debatepedia data set. The goal of such annotation was to individually consider each pair of support and attack among arguments,\nand to additionally tag them as entailment, contradiction or null. The null judgment can be assigned in case an argument is supporting another argument without inferring it, or the argument is attacking another argument without contradicting it. As exemplified in Example 1, a correct entailment pair is (b)\u21d2 (a), while a contradiction is (d) ; (a). A null judgment is assigned to (d) - (c), since the former argument supports the latter without inferring it. Our data set is an extended version of (Cabrio and Villata 2012)\u2019s one allowing for a deeper investigation.\nAgain, to assess the validity of the annotation task, we have calculated the inter-annotator agreement. Another annotator with skills in linguistics has therefore independently annotated a sample of 100 pairs of the data set. We calculated the inter-annotator agreement considering the argument pairs tagged as support and attacks by both annotators, and we verify the agreement between the pairs tagged as entailment and as null (i.e. no entailment), and as contradiction and as null (i.e. no contradiction), respectively. Applying \u03ba to our data, the agreement for our task is \u03ba = 0.74. As a rule of thumb, this is a satisfactory agreement. Table 3 reports the results of the annotation on our Debatepedia data set, as resulting after a reconciliation phase carried out by the annotators6.\nOn the 320 pairs of the data set, 180 represent a support relation, while 140 are attacks. Considering only the supports, 111 argument pairs (i.e., 61.6%) are an actual entailment, while in 38.4% of the cases the first argument of the pair supports the second one without inferring it (e.g. (d) - (c) in Example 1). With respect to the attacks, 100 argument pairs (i.e., 71.4%) are both attack and contradiction, while only the 28.6% of the argument pairs does not contradict the arguments they are attacking, as in Example 2.\nExample 2. (e) Coca chewing is bad for human health. The decision to ban coca chewing fifty years ago was based on a 1950 report elaborated by the UN Commission of Inquiry on the Coca Leaf with a mandate from ECOSOC: \u201cWe believe that the daily, inveterate use of coca leaves by chewing is thoroughly noxious and therefore detrimental\u201d.\n(f) Chewing coca offers an energy boost. Coca provides an energy boost for working or for combating fatigue and cold.\nDifferently from the relation between support-entailment, the difference between attack and contradiction is more sub-\n6In this phase, the annotators discuss the results to find an agreement on the annotation to be released.\ntle, and it is not always straightforward to say whether an argument attacks another argument without contradicting it. In Example 2, we consider that (e) does not contradict (f) even if it attacks (f), since chewing coca can offer an energy boost, and still be bad for human health. This kind of attacks is less frequent than the attacks-contradictions (see Table 3).\nDebatepedia additional attacks dataset Starting from the comparative study addressed by (Cayrol and LagasquieSchiex 2011), in the third study of (Cabrio and Villata 2013) we have considered four additional attacks proposed in the literature: supported (if argument a supports argument b and b attacks argument c, then a attacks c) and secondary (if a supports b and c attacks a, then c attacks b) attacks (Cayrol and Lagasquie-Schiex 2010), mediated attacks (Boella et al. 2010) (if a supports b and c attacks b, then c attacks a), and extended attacks (Nouioua and Risch 2010; 2011) (if a supports b and a attacks c, then b attacks c).\nIn order to investigate the presence and the distribution of these attacks in NL debates, we extended again the data set extracted from Debatepedia to consider all these additional attacks, and we showed that all these models are verified in human debates, even if with a different frequency. More specifically, we took the original argumentation framework of each topic in our data set (Table 2), the following procedure is applied: the supported (secondary, mediated, and extended, respectively) attacks are added, and the argument pairs resulting from coupling the arguments linked by this relation are collected in the data set \u201csupported (secondary, mediated, and extended, respectively) attack\u201d. Collecting the argument pairs generated from the different types of complex attacks in separate data sets allows us to independently analyze each type, and to perform a more accurate evaluation.7 Figures 2a-d show the four AFs resulting from the addition of the complex attacks in the example Can coca be classified as a narcotic?. Note that the AF in Figure 2a, where the supported attack is introduced, is the same of Figure 2b where the mediated attack is introduced. Notice that, even if the additional attack which is introduced coincide, i.e., d attacks b, this is due indeed to different interactions among supports and attacks (as highlighted in the figure), i.e., in the case of supported attacks this is due to the support from d to c and the attack from c to b, while in the case of mediated attacks this is due to the support from b to a and the attack from d to a.\nA second annotation phase is then carried out on the data set, to verify if the generated argument pairs of the four data sets are actually attacks (i.e., if the models of complex attacks proposed in the literature are represented in real data). More specifically, an argument pair resulting from the application of a complex attack can be annotated as: attack (if it is a correct attack) or as unrelated (in case the meanings of the two arguments are not in conflict). For instance, the argument pair (g)-(h) (Example 3) resulting from the insertion of a supported attack, cannot be considered as an attack since the arguments are considering two different aspects of\n7Data sets freely available for research purposes at http://www-sop.inria.fr/NoDE/NoDE-xml.html# debatepedia\nthe issue. Example 3. (g) Chewing coca offers an energy boost. Coca provides an energy boost for working or for combating fatigue and cold.\n(h) Coca can be classified as a narcotic.\nIn the annotation, attacks are then annotated also as contradiction (if the first argument contradicts the other) or null (in case the first argument does not contradict the argument it is attacking, as in Example 2). Due to the complexity of the annotation, the same annotation task has been independently carried out also by a second annotator, so as to compute inter-annotator agreement. It has been calculated on a sample of 80 argument pairs (20 pairs randomly extracted from each of the \u201ccomplex attacks\u201d data set), and it has the goal to assess the validity of the annotation task (counting when the judges agree on the same annotation). We calculated the inter-annotator agreement for our annotation task in two steps. We (i) verify the agreement of the two judges on the argument pairs classification attacks/unrelated, and (ii) consider only the argument pairs tagged as attacks by both annotators, and we verify the agreement between the pairs tagged as contradiction and as null (i.e. no contradiction). Applying \u03ba to our data, the agreement for the first step is \u03ba = 0.77, while for the second step \u03ba = 0.71. As a rule of thumb, both agreements are satisfactory, although they reflect the higher complexity of the second annotation (contradiction/null).\nThe distribution of complex attacks in the Debatepedia data set, as resulting after a reconciliation phase carried out by the annotators, is shown in Table 4. As can be noticed, the mediated attack is the most frequent type of attack, generating 335 new argument pairs in the NL sample we considered (i.e. the conditions that allow the application of this kind of complex attacks appear more frequently in real debates). Together with secondary attacks, they appear in the AFs of all the debated topics. On the contrary, extended attacks are added in 11 out of 19 topics, and supported attacks in 17 out of 19 topics. Considering all the topics, on average only 6 pairs generated from the additional attacks were already present in the original data set, meaning that considering also these attacks is a way to hugely enrich our data set of NL debates."}, {"heading": "Twelve Angry Men", "text": "As a second scenario to extract natural language arguments we chose the scripts of \u201cTwelve Angry Men\u201d. The play con-\ncerns the deliberations of the jury of a homicide trial. As in most American criminal cases, the twelve men must unanimously decide on a verdict of \u201cguilty\u201d or \u201cnot guilty\u201d. At the beginning, they have a nearly unanimous decision of guilty, with a single dissenter of not guilty, who throughout the play sows a seed of reasonable doubt.\nThe play is divided into three acts: the end of each act corresponds to a fixed point in time (i.e. the halfway votes of the jury, before the official one), according to which we want to be able to extract a set of consistent arguments. For each act, we manually selected the arguments (excluding sentences which cannot be considered as self-contained arguments), and we coupled each argument with the argument it is supporting or attacking in the dialogue flow (as shown in Examples 4 to 7). More specifically, in discussions, one character\u2019s argument comes after the other (entailing or contradicting one of the arguments previously expressed by another character): therefore, we create our pairs in the graph connecting the former to the latter (more recent arguments are placed as T and the argument w.r.t. whom we want to detect the relation is placed as H). For instance, in Example 6, juror 1 claims argument (o), and he is attacked by juror 2, claiming argument (l). Juror 3 claims then argument (i) to support juror\u2019s 2 opinion. In the dataset we have therefore annotated the following couples: (o) is contradicted by (l); (l) is entailed by (i).\nIn Example 7, juror 1 claims argument (l) supported by juror 2 (argument (i)); juror 3 attacks juror\u2019s 2 opinion with argument (p). More specifically, (l) is entailed by (i); (i) is contradicted by (p). Example 4. (i) Maybe the old man didn\u2019t hear the boy yelling \u201cI\u2019m going to kill you\u201d. I mean with the el noise. (l) I don\u2019t think the old man could have heard the boy yelling. Example 5. (m) I never saw a guiltier man in my life. You sat right in court and heard the same thing I did. The man\u2019s a dangerous killer. (n) I don\u2019t know if he is guilty. Example 6. (i) Maybe the old man didn\u2019t hear the boy yelling \u201dI\u2019m going to kill you\u201d. I mean with the el noise.\n(l) I don\u2019t think the old man could have heard the boy yelling. (o) The old man said the boy yelled \u201dI\u2019m going to kill you\u201d out. That\u2019s enough for me.\nExample 7. (p) The old man cannot be a liar, he must have heard the boy yelling. (i) Maybe the old man didn\u2019t hear the boy yelling \u201dI\u2019m going to kill you\u201d. I mean with the el noise. (l) I don\u2019t think the old man could have heard the boy yelling.\nGiven the complexity of the play, and the fact that in human linguistic interactions a lot is left implicit, we simplified the arguments: i) adding the required context in T to make the pairs self-contained (in the TE framework entailment is detected based on the evidences provided in T); and ii) solving intra document coreferences, as in: Nobody has to prove that!, transformed into Nobody has to prove [that he is not guilty].\nWe collected 80 T-H pairs8, composed by 25 entailment pairs, 41 contradiction and 14 unknown pairs (contradiction and unknown pairs are then collapsed in the judgment non entailment for the two-way classification task).9 To calculate the inter annotator agreement, the same annotation task has been independently carried out on half of argument pairs (40 T-H pairs) also by a second annotator. Cohen\u2019s kappa (Carletta 1996) is 0.74. Again, this is a satisfactory agreement, confirming the reliability of the obtained resource.\nAlso in this scenario, we consider the pairs annotated in the first layer and we then build a bipolar entailment graph for each of the topic in the dataset (the three acts of the play). Again, the arguments are the nodes of the graph, and the relations among the arguments correspond to the edges of the graphs. The complexity of the graphs obtained for the Twelve Angry Men scenario is higher than the debates graphs (on average, 27 links per graph with respect to 9 links per graph in the Debatepedia dataset).\n8The dataset is available at http://www-sop.inria.fr/ NoDE/NoDE-xml.html#12AngryMen. It is built in standard RTE format.\n9The unknown pairs in the dataset are arguments attacking each others, without contradicting. Collapsing both judgments into one category for our experiments does not impact on our framework evaluation.\nFigure 3 shows the average dimension of a bipolar argumentation graph in the Twelve Angry Men dataset. Note that no cycle is present, as well as in all the other graphs of such dataset."}, {"heading": "Conclusions", "text": "In this paper, we describe two datasets of natural language arguments used in the context of debates. The only existing dataset composed of natural language arguments proposed and exploited in the argumentation community is Araucaria.10 Araucaria (Reed and Rowe 2004) is based on argumentation schemes (Walton, Reed, and Macagno 2008), and it is an online repository of arguments from heterogenous sources like newspapers (e.g., Wall Street Journal), parliamentary records (e.g., UK House of Parliament debates) and discussion fora (e.g., BBC talking point). Arguments are classified by argumentation schemes. Also in the context of argumentation schemes, (Cabrio, Tonelli, and Villata 2013) propose a new resource based on the Penn Discourse Treebank (PDTB), where a part of the corpus has been annotated with a selection of five argumentation schemes. This effort goes in the direction of trying to export a well known existing benchmark in the field of natural language processing (i.e., PDTB) into the argumentation field, through the identification and annotation of the argumentation schemes.\nThe benchmark of natural language arguments we presented in this paper has several potential uses. As all the data we presented is available on the Web in a machinereadable format, researchers interested in testing their own argumentation-based tool (both for arguments visualization and for reasoning) are allowed to download the data sets and verify on real data the performances of the tool. More-\n10http://araucaria.computing.dundee.ac.uk\nover, also from the theoretical point of view, the data set can be used by argumentation researchers to find real world example supporting the introduction of new theoretical frameworks. One of the aims of such benchmark is actually to move from artificial natural language examples of argumentation towards more realistic ones where other problems, maybe far from the ones addressed at the present stage in current argumentation research, emerge.\nIt is interesting to note that the abstract (bipolar) argumentation graphs resulting from our datasets result to be rather simple structures, where usually arguments are inserted in reinstatement chains, rather than complex structures with the presence of several odd and even cycles, as usually challenged in the argumentation literature. In this perspective, we plan to consider other sources of arguments, like costumer\u2019s opinions about a service or a product, to see whether more complex structures are identified, with the final goal to built a complete resource where also such complex patterns are present.\nA further point which deserves investigation concerns the use of abstract argumentation. Some of the examples we provided may suggest that in some cases adopting abstract argumentation might not be fully appropriate since such natural language arguments have (possibly complex) internal structures and may include sub-arguments (for example argument (d) of the \u201cCoca as narcotic\u201d example). We will investigate how to build a dataset of structured arguments, taking into account the discourse relations.\nFinally, in this paper, we have presented a benchmark of natural language arguments manually annotated by humans with skills in linguistics. Given the complexity of the annotation task, a manual annotation was the best choice ensuring an high quality of the data sets. However, in other tasks like discourse relations extraction, it is possible to adopt automated extraction techniques then further verified by human annotators to ensure an high resource\u2019s confidence.\nReferences Boella, G.; Gabbay, D. M.; van der Torre, L.; and Villata, S. 2010. Support in abstract argumentation. In Procs of COMMA, Frontiers in Artificial Intelligence and Applications 216, 111\u2013122. Cabrio, E., and Villata, S. 2012. Natural language arguments: A combined approach. In Procs of ECAI, Frontiers in Artificial Intelligence and Applications 242, 205\u2013210. Cabrio, E., and Villata, S. 2013. A natural language bipolar argumentation approach to support users in online debate interactions;. Argument & Computation 4(3):209\u2013230. Cabrio, E.; Tonelli, S.; and Villata, S. 2013. A natural language account for argumentation schemes. In Baldoni, M.; Baroglio, C.; Boella, G.; and Micalizio, R., eds., AI*IA, volume 8249 of Lecture Notes in Computer Science, 181\u2013192. Springer. Carletta, J. 1996. Assessing agreement on classification tasks: the kappa statistic. Comput. Linguist. 22(2):249\u2013254. Cayrol, C., and Lagasquie-Schiex, M.-C. 2005. On the acceptability of arguments in bipolar argumentation frameworks. In Procs of ECSQARU, LNCS 3571, 378\u2013389.\nCayrol, C., and Lagasquie-Schiex, M.-C. 2010. Coalitions of arguments: A tool for handling bipolar argumentation frameworks. Int. J. Intell. Syst. 25(1):83\u2013109. Cayrol, C., and Lagasquie-Schiex, M.-C. 2011. Bipolarity in argumentation graphs: Towards a better understanding. In Procs of SUM, LNCS 6929, 137\u2013148. Dagan, I.; Dolan, B.; Magnini, B.; and Roth, D. 2009. Recognizing textual entailment: Rational, evaluation and approaches. Natural Language Engineering (JNLE) 15(04):i\u2013 xvii. Dung, P. M. 1995. On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games. Artif. Intell. 77(2):321\u2013358. Nouioua, F., and Risch, V. 2010. Bipolar argumentation frameworks with specialized supports. In Procs of ICTAI, 215\u2013218. IEEE Computer Society. Nouioua, F., and Risch, V. 2011. Argumentation frameworks with necessities. In Procs of SUM, LNCS 6929, 163\u2013176. Reed, C., and Rowe, G. 2004. Araucaria: Software for argument analysis, diagramming and representation. International Journal on Artificial Intelligence Tools 13(4):961\u2013 980. Walton, D.; Reed, C.; and Macagno, F. 2008. Argumentation Schemes. Cambridge University Press."}], "references": [{"title": "Support in abstract argumentation", "author": ["G. Boella", "D.M. Gabbay", "L. van der Torre", "S. Villata"], "venue": "In Procs of COMMA, Frontiers in Artificial Intelligence and Applications", "citeRegEx": "Boella et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boella et al\\.", "year": 2010}, {"title": "Natural language arguments: A combined approach", "author": ["E. Cabrio", "S. Villata"], "venue": "Procs of ECAI, Frontiers in Artificial Intelligence and Applications 242, 205\u2013210.", "citeRegEx": "Cabrio and Villata,? 2012", "shortCiteRegEx": "Cabrio and Villata", "year": 2012}, {"title": "A natural language bipolar argumentation approach to support users in online debate interactions", "author": ["E. Cabrio", "S. Villata"], "venue": "Argument & Computation 4(3):209\u2013230.", "citeRegEx": "Cabrio and Villata,? 2013", "shortCiteRegEx": "Cabrio and Villata", "year": 2013}, {"title": "A natural language account for argumentation schemes", "author": ["E. Cabrio", "S. Tonelli", "S. Villata"], "venue": "Baldoni, M.; Baroglio, C.; Boella, G.; and Micalizio, R., eds., AI*IA, volume 8249 of Lecture Notes in Computer Science, 181\u2013192. Springer.", "citeRegEx": "Cabrio et al\\.,? 2013", "shortCiteRegEx": "Cabrio et al\\.", "year": 2013}, {"title": "Assessing agreement on classification tasks: the kappa statistic", "author": ["J. Carletta"], "venue": "Comput. Linguist. 22(2):249\u2013254.", "citeRegEx": "Carletta,? 1996", "shortCiteRegEx": "Carletta", "year": 1996}, {"title": "On the acceptability of arguments in bipolar argumentation frameworks", "author": ["C. Cayrol", "Lagasquie-Schiex", "M.-C."], "venue": "Procs of ECSQARU, LNCS 3571, 378\u2013389.", "citeRegEx": "Cayrol et al\\.,? 2005", "shortCiteRegEx": "Cayrol et al\\.", "year": 2005}, {"title": "Coalitions of arguments: A tool for handling bipolar argumentation frameworks", "author": ["C. Cayrol", "Lagasquie-Schiex", "M.-C."], "venue": "Int. J. Intell. Syst. 25(1):83\u2013109.", "citeRegEx": "Cayrol et al\\.,? 2010", "shortCiteRegEx": "Cayrol et al\\.", "year": 2010}, {"title": "Bipolarity in argumentation graphs: Towards a better understanding", "author": ["C. Cayrol", "Lagasquie-Schiex", "M.-C."], "venue": "Procs of SUM, LNCS 6929, 137\u2013148.", "citeRegEx": "Cayrol et al\\.,? 2011", "shortCiteRegEx": "Cayrol et al\\.", "year": 2011}, {"title": "Recognizing textual entailment: Rational, evaluation and approaches", "author": ["I. Dagan", "B. Dolan", "B. Magnini", "D. Roth"], "venue": "Natural Language Engineering (JNLE) 15(04):i\u2013 xvii.", "citeRegEx": "Dagan et al\\.,? 2009", "shortCiteRegEx": "Dagan et al\\.", "year": 2009}, {"title": "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games", "author": ["P.M. Dung"], "venue": "Artif. Intell. 77(2):321\u2013358.", "citeRegEx": "Dung,? 1995", "shortCiteRegEx": "Dung", "year": 1995}, {"title": "Bipolar argumentation frameworks with specialized supports", "author": ["F. Nouioua", "V. Risch"], "venue": "Procs of ICTAI, 215\u2013218. IEEE Computer Society.", "citeRegEx": "Nouioua and Risch,? 2010", "shortCiteRegEx": "Nouioua and Risch", "year": 2010}, {"title": "Argumentation frameworks with necessities", "author": ["F. Nouioua", "V. Risch"], "venue": "Procs of SUM, LNCS 6929, 163\u2013176.", "citeRegEx": "Nouioua and Risch,? 2011", "shortCiteRegEx": "Nouioua and Risch", "year": 2011}, {"title": "Araucaria: Software for argument analysis, diagramming and representation", "author": ["C. Reed", "G. Rowe"], "venue": "International Journal on Artificial Intelligence Tools 13(4):961\u2013 980.", "citeRegEx": "Reed and Rowe,? 2004", "shortCiteRegEx": "Reed and Rowe", "year": 2004}, {"title": "Argumentation Schemes", "author": ["D. Walton", "C. Reed", "F. Macagno"], "venue": "Cambridge University Press.", "citeRegEx": "Walton et al\\.,? 2008", "shortCiteRegEx": "Walton et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 9, "context": ", in abstract argumentation theory (Dung 1995) and in bipolar argumentation (Cayrol and Lagasquie-Schiex 2005), respectively).", "startOffset": 35, "endOffset": 46}, {"referenceID": 8, "context": ", classification, textual entailment (Dagan et al. 2009), etc.", "startOffset": 37, "endOffset": 56}, {"referenceID": 2, "context": "Our goal (Cabrio and Villata 2013) is to analyze natural language debates in order to understand, given a huge debate, what are the winning arguments (through acceptability semantics) and who proposed them.", "startOffset": 9, "endOffset": 34}, {"referenceID": 2, "context": "Note that the first resource consists of an integration of the dataset of natural language arguments we presented in (Cabrio and Villata 2013) with new data extracted from the ProCon debate platform.", "startOffset": 117, "endOffset": 142}, {"referenceID": 2, "context": "For instance, in (Cabrio and Villata 2013) we experiment with the application of a TE (Dagan et al.", "startOffset": 17, "endOffset": 42}, {"referenceID": 8, "context": "For instance, in (Cabrio and Villata 2013) we experiment with the application of a TE (Dagan et al. 2009) to automatically identify the arguments in the text and to specify which kind of relation links each couple of arguments.", "startOffset": 86, "endOffset": 105}, {"referenceID": 4, "context": "The statistical measure usually used in NLP to calculate the inter-rater agreement for categorical items is Cohen\u2019s kappa coefficient (Carletta 1996), that is generally thought to be a more robust measure than simple percent agreement calculation since \u03ba takes into account the agreement occurring by chance.", "startOffset": 134, "endOffset": 149}, {"referenceID": 2, "context": "For the second study described in (Cabrio and Villata 2013) we wanted to move a step further, to understand whether it is always the case that support is equivalent to TE", "startOffset": 34, "endOffset": 59}, {"referenceID": 1, "context": "Our data set is an extended version of (Cabrio and Villata 2012)\u2019s one allowing for a deeper investigation.", "startOffset": 39, "endOffset": 64}, {"referenceID": 2, "context": "Debatepedia additional attacks dataset Starting from the comparative study addressed by (Cayrol and LagasquieSchiex 2011), in the third study of (Cabrio and Villata 2013) we have considered four additional attacks proposed in the literature: supported (if argument a supports argument b and b attacks argument c, then a attacks c) and secondary (if a supports b and c attacks a, then c attacks b) attacks (Cayrol and Lagasquie-Schiex 2010), mediated attacks (Boella et al.", "startOffset": 145, "endOffset": 170}, {"referenceID": 0, "context": "Debatepedia additional attacks dataset Starting from the comparative study addressed by (Cayrol and LagasquieSchiex 2011), in the third study of (Cabrio and Villata 2013) we have considered four additional attacks proposed in the literature: supported (if argument a supports argument b and b attacks argument c, then a attacks c) and secondary (if a supports b and c attacks a, then c attacks b) attacks (Cayrol and Lagasquie-Schiex 2010), mediated attacks (Boella et al. 2010) (if a supports b and c attacks b, then c attacks a), and extended attacks (Nouioua and Risch 2010; 2011) (if a supports b and a attacks c, then b attacks c).", "startOffset": 458, "endOffset": 478}, {"referenceID": 10, "context": "2010) (if a supports b and c attacks b, then c attacks a), and extended attacks (Nouioua and Risch 2010; 2011) (if a supports b and a attacks c, then b attacks c).", "startOffset": 80, "endOffset": 110}, {"referenceID": 4, "context": "Cohen\u2019s kappa (Carletta 1996) is 0.", "startOffset": 14, "endOffset": 29}, {"referenceID": 12, "context": "10 Araucaria (Reed and Rowe 2004) is based on argumentation schemes (Walton, Reed, and Macagno 2008), and it is an online repository of arguments from heterogenous sources like newspapers (e.", "startOffset": 13, "endOffset": 33}], "year": 2014, "abstractText": "The connections among natural language processing and argumentation theory are becoming stronger in the latest years, with a growing amount of works going in this direction, in different scenarios and applying heterogeneous techniques. In this paper, we present two datasets we built to cope with the combination of the Textual Entailment framework and bipolar abstract argumentation. In our approach, such datasets are used to automatically identify through a Textual Entailment system the relations among the arguments (i.e., attack, support), and then the resulting bipolar argumentation graphs are analyzed to compute the accepted arguments.", "creator": "TeX"}}}