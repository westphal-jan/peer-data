{"id": "1405.2420", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2014", "title": "Optimal Learners for Multiclass Problems", "abstract": "the fundamental theorem of statistical learning states that so for binary classification problems, any empirical risk minimization ( erm ) learning rule has close to optimal acceptable sample complexity. in this paper we seek for a generic optimal learner for multiclass prediction. we start by proving a surprising result : effectively a generic global optimal multiclass learner class must be improper, namely, it constantly must have the ability inability to output hypotheses which don't accurately belong to the hypothesis distribution class, even though it occasionally knows again that all the labels supplied are generated by some hypothesis null from underneath the class. in particular, notably no erm learner is optimal. this brings back the fundmamental question of \" how to learn \"? we give a complete answer to this remarkable question by giving a new analysis feature of the one - inclusion multiclass learner paradox of rubinstein et her al ( 2006 ) showing that its resulting sample complexity is essentially optimal. then, we turn to study the popular hypothesis class of generalized linear classifiers. we derive optimal learners that, perhaps unlike the singular one - inclusion algorithm, are computationally efficient. furthermore, we better show that the relevant sample complexity of these learners is better than the sample complexity of the erm rule, thus settling in negative an open question due to collins ( 2005 ).", "histories": [["v1", "Sat, 10 May 2014 11:23:08 GMT  (58kb)", "http://arxiv.org/abs/1405.2420v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["amit daniely", "shai shalev-shwartz"], "accepted": false, "id": "1405.2420"}, "pdf": {"name": "1405.2420.pdf", "metadata": {"source": "CRF", "title": "Optimal Learners for Multiclass Problems", "authors": ["Amit Daniely"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n24 20\nv1 [\ncs .L\nG ]\n1 0\nM ay\n2 01"}, {"heading": "1. Introduction", "text": "Multiclass classification is the problem of learning a classifier h from a domain X to a label space Y, where |Y| > 2 and the error of a prediction is measured by the probability that h(x) is not the correct label. It is a basic problem in machine learning, surfacing a variety of domains, including object recognition, speech recognition, document categorization and many more. Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)). Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others. These developments made it possible to handle a variety of multiclass classification problems, including even problems that have a very complex label space, that is structured and exponentially large (e.g. speech recognition, OCR, and multiple object categorization).\nDespite being very basic and natural, and despite these developments and efforts, our theoretical understanding of multiclass classification is still far from being satisfactory, in particular relatively to our understanding of binary classification (i.e., when |Y| = 2). In this work, we focus on the sample complexity of (distribution free) learning of hypothesis classes H \u2286 YX . The two most fundamental questions are:\n1. What is learnable? More quantitatively, what is the sample complexity of a given class H?\n2. How to learn? In particular, is there a generic algorithm with optimal sample complexity?\nFor binary classification problems, these two questions are essentially solved (up to logfactors of the error and confidence parameters \u01eb and \u03b4): The fundamental result of Vapnik and Chervonenkis (1971) asserts that the VC dimension characterizes the sample complexity, and that any Empirical Risk Minimization (ERM) algorithm enjoys close-to-optimal sample complexity.\nIn a recent surprising result, Daniely et al. (2011) have shown that in multiclass classification there might be substantial gaps between the sample complexity of different ERMs. We start by showing an even stronger \u201cpeculiarity\u201d, discriminating binary from multiclass classification. Recall that an algorithm is called improper if it might return a hypothesis that does not belong to the learnt class. Traditionally, improper learning has been applied to enable efficient computations. It seems counter intuitive that computationally unbounded learner would benefit from returning a hypothesis outside of the learnt class. Surprisingly, we show that an optimal learning algorithm must be improper! Namely, we show that there are classes that are learnable only by an improper algorithm. Pointing out that we actually do not understand how to learn optimally, these results \u201creopen\u201d the above two basic questions for multiclass classification.\nIn this paper we essentially resolve these two questions. We give a new analysis of the multiclass one inclusion algorithm (Rubinstein et al. (2006) based on Haussler et al. (1988), see also Simon and Szo\u0308re\u0301nyi (2010)), showing that it is optimal up to a constant factor of 2 in a transductive setting. This improves on the original analysis, that yielded optimality only\nup to a factor of log(|Y|) (which, as explained, might be quite large in several situations). By showing reductions from transductive to inductive learning, we consequently obtain an optimal learner in the PAC model, up to a logarithmic factor of 1\u03b4 and 1 \u01eb . The analysis of the one inclusion algorithm results with a characterization of the sample complexity of a class H by a sequence of numbers \u00b5H(m). Concretely, it follows that the best possible guarantee on the error, after seeing m examples, is \u0398 (\n\u00b5H(m) m\n)\n.\nComparing to binary classification, we should still strive for a better characterization: We would like to have a characterization of the sample complexity by a single number (i.e. some notion of dimension) rather than a sequence. Our analysis of the one inclusion algorithm naturally leads to a new notion of dimension, of somewhat different character than previously studied notions. We show that this notion have certain advantages comparing to other previously studied notions, and formulate a concrete combinatorial conjecture that, if true, would lead to a crisper characterization of the sample complexity.\nDeparting general theory, we turn our focus to investigate hypothesis classes that are used in practice, in light of the above results and the result of Daniely et al. (2011). We consider classes of multiclass linear classifiers that are learnt by several popular learning paradigms, including multiclass SVM with kernels (Crammer and Singer, 2001), structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), and others. Arguably, the two most natural questions in this context are: (i) is the ERM rule still sub-optimal even for such classes? and (ii) If yes, are there efficient optimal learnears for these classes?\nRegarding the first question, we show that even though the sample complexity of these classes is upper bounded in terms of the dimension or the margin, there are sub-optimal ERMs whose sample complexity has additional multiplicative factor that depends on the number of labels. This settles in negative an open question due to Collins (2005). Regarding the second question above, as opposed to the one-inclusion algorithm, which is in general inefficient, for linear classes we derive computationally efficient learners (provided that the hypotheses can be evaluated efficiently), that enjoy optimal sample complexity.\nBasic definitions: Let X be an instance space and Y a label space. To account for margin-based classifiers as well, it would be convenient to allow classifiers to return the label \u2296 that will stand for \u201cdon\u2019t know\u201d. A classifier (or hypothesis) is a mapping h : X \u2192 (Y \u222a {\u2296}). A hypothesis class is a set of classifiers, H \u2282 (Y \u222a {\u2296})X . The error of a classifier with respect to a joint distribution over X \u00d7 Y is the probability that h(x) 6= y. Throughout this paper, we mainly consider learning in the realizable case, which means that there is h\u2217 \u2208 H which has zero error (extensions to agnostic learning are discussed in section A). Therefore, we can focus on the marginal distribution D over X and denote the error of a classifier h with respect to the realizing classifier h\u2217 as ErrD,h\u2217(h) := Prx\u223cD (h(x) 6= h\n\u2217(x)). A learning algorithm is a function A that receives a training set of m instances, S \u2208 Xm, together with their labels according to h\u2217. We denote the restriction of h\u2217 to the instances in S by h\u2217|S . The output of the algorithm A, denoted A(S, h\n\u2217|S) is a classifier. A learning algorithm is proper if it always outputs a hypothesis from H. A learning algorithm is an ERM learner for the class H if, for any sample, it returns a function in H that minimizes the empirical error relative to any other function in H. The (PAC) sample complexity of a learning algorithm A is the function mA,H defined as follows: For every \u01eb, \u03b4 > 0,\nmA,H(\u01eb, \u03b4) is the minimal integer such that for every m \u2265 mA,H(\u01eb, \u03b4), every distribution D on X , and every target hypothesis h\u2217 \u2208 H, PrS\u223cDm (ErrD,h\u2217(A(S, h\n\u2217|S)) > \u01eb) \u2264 \u03b4. Here and in subsequent definitions, we omit the subscript H when it is clear from context. If no integer satisfying the inequality above, define mA(\u01eb, \u03b4) = \u221e. H is learnable with A if for all \u01eb and \u03b4 the sample complexity is finite. The (PAC) sample complexity of a class H is mPAC,H(\u01eb, \u03b4) = infAmA,H(\u01eb, \u03b4), where the infimum is taken over all learning algorithms. The ERM sample complexity (a.k.a. the uniform convergence sample complexity) of H is the sample complexity that can be guaranteed for any ERM learner. It is defined by mERM,H(\u01eb, \u03b4) = supA\u2208ERM m a A,H(\u01eb, \u03b4) where the supremum is taken over all ERM learners for H. Clearly, we always have mPAC \u2264 mERM. We use [m] to denote the set {1, . . . ,m}. We treat vectors as column vectors. We denote by ei \u2208 R d the i\u2019th vector in the standard basis of Rd. We denote by Bd the closed unit ball in Rd. We denote by Md\u00d7k the space of real matrices with d rows and k columns. For a matrix X \u2208 Md\u00d7k and i \u2208 [k], we denote by X\ni \u2208 Rd the i\u2019th column of X. Given a subset A \u2286 X , we define H|A = {h|A : h \u2208 H}."}, {"heading": "2. No optimal learner can be proper", "text": "Our first result shows that, surprisingly, any learning algorithm with a close to optimal sample complexity must be improper.\nTheorem 1 For every 1 \u2264 d \u2264 \u221e there exists a hypothesis class Hd, with 2 d + 1 labels such that:\n\u2022 The PAC sample complexity of Hd is O ( log(1/\u03b4) \u01eb ) .\n\u2022 The PAC sample complexity of any proper learning algorithm for Hd is \u2126 ( d+log(1/\u03b4) \u01eb ) .\n\u2022 In particular, H\u221e is a learnable class that is not learnable by a proper algorithm.\nA detailed proof is given in the appendix, and here we sketch the main idea of the proof. Let X be some finite set and let Y = 2X \u222a {\u2217}. For every A \u2286 X define hA : X \u2192 Y\nby hA(x) =\n{\nA x \u2208 A \u2217 otherwise . Consider the hypothesis class HX ,Cantor = {hA | A \u2282 X} .\nThis class is due to Daniely et al. (2011) and we call it the first Cantor class due to the resemblance to the construction used for proving the famous theorem of Cantor from set theory (e.g., http://en.wikipedia.org/wiki/Cantor\u2019s_theorem). Daniely et al. (2011) employed this class to establish gaps between the sample complexity of different ERM learners. In particular, they have shown that there is an ERM learner with sample complexity \u2264 ln(1/\u03b4)\u01eb , while there are other ERMs whose sample complexity is \u2126 ( |X |+ln(1/\u03b4) \u01eb ) .\nTo show that no proper learner can be optimal, let Xd be a set consisting of d elements and define the following subclass of HXd,Cantor: Hd = { hA | |A| = \u230a d 2 \u230b}\n. Since Hd \u2282 HXd,Cantor, we can apply the \u201cgood\u201d ERM learner described in Daniely et al. (2011) with respect to the class HXd,Cantor and obtain an algorithm for Hd whose sample complexity is \u2264 ln(1/\u03b4)\u01eb . Note that this algorithm is improper \u2014 it might output a hypothesis from\nHXd,Cantor which is not in Hd. As we show, no proper algorithm is able to learn Hd using o (\nd \u01eb\n)\nexamples. To understand the main point in the proof, suppose that an adversary chooses hA \u2208 Hd uniformly at random, and let the algorithm learn it, where the distribution on Xd is uniform on the complement of A, denoted A\nc. Now, the error of every hypothesis hB \u2208 Hd is |B\\A| d . Therefore, to return a hypothesis with small error, the algorithm must recover a set that is almost disjoint from A, and therefore should recover A. However, if it sees only o(d) examples, all it knows is that some o(d) elements in X do not belong to A. It is not hard to be convinced that with this little information, the probability that the algorithm will succeed is negligible."}, {"heading": "3. An optimal learner for general classes", "text": "In this section we describe and analyze a generic optimal learning algorithm. We start with an algorithm for a transductive learning setting, in which the algorithm observes m \u2212 1 labeled examples and an additional unlabeled example, and it should output the label of the unlabeled example. Later, in Section 3.3 we show a generic reduction from the transductive setting to the usual inductive learning model (that is, the vanilla PAC model).\nFormally, in the transductive model, the algorithm observes a set of m unlabeled examples, S \u2208 Xm, and then one of them is picked uniformly at random, x \u223c U(S). The algorithm observes the labels of all the examples but the chosen one, and should predict the label of the chosen example. That is, the input of the algorithm, A, is the set S \u2208 Xm, and the restriction of some h\u2217 \u2208 H to S \\ x, denoted h\u2217|S\\x. The algorithm should output y \u2208 Y. The error rate of a transductive algorithm A is the function \u01ebA,H : N \u2192 [0, 1] defined as \u01ebA,H(m) = supS\u2208Xm,h\u2217\u2208H [ Prx\u223cU(S) ( A(S, h\u2217|S\\x) 6= h \u2217(x) )]\n. The error rate of a class H in the transductive model is defined as \u01ebH(m) = infA \u01ebA,H(m), where the infimum is over all transductive learning algorithms."}, {"heading": "3.1. The one-inclusion algorithm", "text": "We next describe the one-inclusion transductive learning algorithm of Rubinstein et al. (2006). Let S = {x1, . . . , xm} be an unlabelled sample. For every i \u2208 [m] and h \u2208 H|S, let ei,h \u2282 H|S be all the hypotheses in H|S whose restriction to S\\{xi} equals to h|S\\{xi}. That is, h\u2032 \u2208 ei,h iff for all j 6= i we have h \u2032(xj) = h(xj). Note that if h \u2032 \u2208 ei,h then ei,h\u2032 = ei,h.\nGiven (x1, y1), . . . , (xi\u22121, yi\u22121), (xi+1, yi+1), . . . , (xm, ym) let h \u2208 H|S be some hypothesis for which h(xj) = yj for all j 6= i. We know that the target hypothesis can be any hypothesis in ei,h. Therefore, we can think on the transductive algorithm as an algorithm that obtains some ei,h and should output one hypothesis from ei,h. Clearly, if |ei,h| = 1 we know that the target hypothesis is h. But, what should the algorithm do when |ei,h| > 1 ?\nThe idea of the one-inclusion algorithm is to think on the collection E = {ei,h}i\u2208[m],H\u2208H|S as a collection of hyperedges of a hypergraph G = (V,E). Recall that in a hypergraph, V is some set of vertices and each hyperedge e \u2208 E is some subset of V . In our case, the vertex set is V = H|S . This hypergraph is called the one-inclusion hypergraph. Note that if |e| = 2 for every e \u2208 E we obtain the usual definition of a graph. In such a case, an orientation of an undirected edge e = {v1, v2} is picking one of the vertices (e.g. v1) to be the \u201chead\u201d of the edge. Similarly, an orientation of a hyperedge is choosing one v \u2208 e to be the \u201chead\u201d of\nthe hyperedge. And, an orientation of the entire hypergraph is a function f : E \u2192 V such that for all e \u2208 E we have that f(e) \u2208 e.\nGetting back to our transductive learning task, it is easy to see that any (deterministic) transductive learning algorithm is equivalent to an orientation function f : E \u2192 V of the one-inclusion hypergraph. The error rate of such an algorithm, assuming the target function is h\u2217 \u2208 H|S , is\nPr i\u223cU([m])\n[f(ei,h\u2217) 6= h \u2217] =\n1\nm\nm \u2211\ni=1\n1[f(ei,h\u2217) 6= h \u2217] =\n|{e \u2208 E : h\u2217 \u2208 e \u2227 f(e) 6= h\u2217}|\nm . (1)\nThe quantity |{e \u2208 E : h\u2217 \u2208 e \u2227 f(e) 6= h\u2217}| is called the out-degree of the vertex h\u2217 and denoted d+(h\u2217). It follows that the error rate of an orientation f is maxh\u2217\u2208H|S d+(h\u2217)\nm . It follows that the best deterministic transductive algorithm should find an orientation of the hypergraph that minimizes the maximal out degree. This leads to the one-inclusion algorithm.\nAlgorithm 1 Multiclass one inclusion algorithm for H \u2282 YX\n1: Input: unlabeled examples S = (x1, . . . , xm), labels (y1, . . . , yi\u22121, yi+1, . . . , ym) 2: Define the one-inclusion graph G = (V,E) where V = H|S and E = {ej,h}j\u2208[m],h\u2208V 3: Find orientation f : E \u2192 V that minimizes the maximal out-degree of G 4: Let h \u2208 V be s.t. h(xj) = yj for all j 6= i, and let h\u0302 = f(ei,h) 5: Output: predict h\u0302(xi)"}, {"heading": "3.2. Analysis", "text": "The main result of this section is a new analysis of the one inclusion algorithm, showing its optimality in the transductive model, up to a constant factor of 1/2. In the next subsection we deal with the PAC model.\nTo state our results, we need a few definitions. Let G = (V,E) be a hypergraph. Throughout, we only consider hypergraphs for which E is an antichain (i.e., there are no e1, e2 \u2208 E such that e1 is strictly contained in e2). Given U \u2286 V , define the induced hypergraph, G[U ], as the hypergraph whose vertex set is U and whose edge set is all sets e \u2286 U such that e = U\u2229e\u2032 for some e\u2032 \u2208 E, |e| \u2265 2, and e is maximal w.r.t. these conditions.\nThe degree of a vertex v in a hypergraph G = (V,E) is the number of hyperedges, e \u2208 E, such that |e| \u2265 2 and v \u2208 e. The average degree of G is d(G) = 1|V | \u2211 v\u2208V d(v). The maximal average degree of G is md(G) = maxU\u2286V :|U |<\u221e d(G[U ]). For a hypothesis class H define\n\u00b5H(m) = max{md(G(H|S)) | S \u2208 X m} ,\nwhere G(H|S) is the one-inclusion hypergraph defined in Algorithm 1.\nTheorem 2 For every class H, 12 \u00b5H(m) m \u2264 \u01ebH(m) \u2264 \u00b5H(m) m .\nProof To prove the upper bound, recall that the one inclusion algorithm uses an orientation of the one-inclusion hypergraph that minimizes the maximal out-degree, and recall that in\n(1) we have shown that the error rate of an orientation function is upper bounded by the maximal out-degree over m. Therefore, the proof of the upper bound of the theorem follows directly from the following lemma:\nLemma 3 Let G = (V,E) be a hypergraph with maximal average degree d. Then, there exists an orientation of G with maximal out-degree of at most d.\nThe proof of the lemma is given in the appendix. While the above proof of the upper bound is close in spirit to the arguments used by Haussler et al. (1988) and Rubinstein et al. (2006), the proof of the lower bound relies on a new argument. As opposed to Rubinstein et al. (2006) who lower bounded \u01ebH(m) using the Natarajan dimension, we give a direct analysis.\nLet S \u2208 Xm be a set such that md(G(H|S)) = \u00b5H(m). For simplicity we assume that |S| = m (i.e., S does not contain multiple elements). Since md(G(H|S)) = \u00b5H(m), there is finite F \u2282 G with d(G(F|S)) = \u00b5H(m). Consider the following scenario. Suppose that h\u2217 \u2208 F|S is chosen uniformly at random, and in addition, a point x \u2208 S is also chosen uniformly at random. Now, suppose that a learner A is given the sample S with all points labelled by h\u2217 except x that is unlabelled. It is enough to show that the probability that A errs is \u2265 \u00b5H(m)2m .\nDenote by U the event that x correspond to an edge in G(F|S) coming out of h \u2217. Given U , the value of h\u2217(x), given what the algorithm sees, is distributed uniformly in the set {h(x) | h \u2208 F and h|S\\{x} = h \u2217|S\\{x}}. Since this set consists of at least two elements, given U , the algorithm errs with probability \u2265 12 .\nIt is therefore enough to prove that Pr(U) \u2265 \u00b5H(m)m . Indeed, given h \u2217, the probability that x corresponds to an edge coming out of h\u2217 is exactly the degree of h\u2217 over m. Therefore, the probability that x corresponds to an edge coming out of a randomly chosen h\u2217 is the average degree of G(F|S) over m, i.e., \u00b5H(m) m ."}, {"heading": "3.3. PAC optimality: from transductive to inductive learning", "text": "In the previous section we have analyzed the optimal error rate of learning in the transductive learning. We now turn to the inductive PAC model. By a simple reduction from inductive to transductive learning, we will show that a variant of the one-inclusion algorithm is essentially optimal in the PAC model.\nFirst, any transductive algorithm A can be naturally interpreted as an inductive algorithm, which we denote by Ai. Specifically, Ai returns, after seeing the sample S = {(xi, yi)} m\u22121 i=1 , the hypothesis h : X \u2192 Y such that h(x) is the label A would have predicted for x after seeing the labelled sample S. It holds that (see the appendix) the (worst case) expectation of the error of the hypothesis returned by Ai operating on m points sample, is the same, up to a factor of e to \u01ebA(m). Using this fact and a simple amplification argument, it is not hard to show that a variant of the one-inclusion algorithm is essentially optimal in the PAC model.\nNamely, we consider the algorithm I that splits the sample into 2 log(1/\u03b4) parts, run the one inclusion algorithm on log(1/\u03b4) different parts to obtain log(1/\u03b4) candidate hypotheses, and finally chooses the best one, by validation on the remaining points. As the following\ntheorem (whose proof is given in the appendix) shows, I is optimal up to a factor of O ( log (\n1 \u03b4\n) log ( 1 \u01eb )) in the PAC model, in the following sense:\nTheorem 4 For some c > 0, and every class H, mI,H(\u01eb, \u03b4) \u2264 mPAC,H (c\u01eb, \u03b4)\u00b7 1 c log(1/\u03b4) log(1/\u01eb)."}, {"heading": "4. Efficient optimal learning and gaps for linear classes", "text": "In this section we study the family of linear hypothesis classes. This family is widely used in practice and received a lot of attention in the literature\u2014see for example Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al. (2001); Taskar et al. (2003); Tsochantaridis et al. (2004). We show that, rather surprisingly, even for such simple classes, there can be gaps between the ERM sample complexity and the PAC sample complexity. This settles in negative an open question raised by Collins (2005). We also derive computationally efficient optimal learners for linear classes, based on the concept of compression schemes. This is in contrast to the one-inclusion algorithm from the previous section, which in general is inefficient. Due to the lack of space, most proofs are deferred to the appendix."}, {"heading": "4.1. Linear hypothesis classes", "text": "We first define the various hypothesis classes of multiclass linear classifiers that we study. All of these classes depend on a class-specific feature mapping, \u03a8 : X \u00d7 Y \u2192 Rd. We will provide several examples of feature mappings that are widely used in practice."}, {"heading": "4.1.1. Dimension based linear classifiers (denoted H\u03a8)", "text": "For w \u2208 Rd and x \u2208 X , define the multiclass predictor hw(x) = argmaxy\u2208Y\u3008w,\u03a8(x, y)\u3009. In case of a tie, hw(x) is assumed to be the \u201cdon\u2019t know label\u201d, \u2296. The corresponding hypothesis class is defined as H\u03a8 = {hw | w \u2208 R d}.\nExample 1 (multivector construction) If the labels are unstructured, a canonical choice of \u03a8 is the so called multivector construction. Here, Y = [k], X = Rd and \u03a8 : X \u00d7Y \u2192 Rdk is defined as follows: \u03a8(x, y) is the d \u00d7 k matrix whose y\u2019th column is x, while the rest are 0. In this case, every classifier corresponds to a matrix W , and the prediction on an instance x \u2208 Rd is the index of the column that maximizes the inner product with x."}, {"heading": "4.1.2. Large margin linear classifiers (denoted H\u03a8,R)", "text": "The second kind of hypothesis class induced by \u03a8 is margin based. Here, we assume that the range of \u03a8 is contained in the unit ball of Rd. Every vector w \u2208 Rd defines a function hw : X \u2192 (Y \u222a {\u2296}) by\n\u2200x \u2208 X , hw(x) =\n{\ny if\u3008w,\u03a8(x, y) \u2212\u03a8(x, y\u2032)\u3009 \u2265 1 for every y\u2032 6= y\n\u2296 if no such y exists\nThe class of linear classifiers of complexity R > 0 induced by \u03a8 is H\u03a8,R = { hw | \u2016w\u2016 2 \u2264 R } .\nExample 2 (multivector construction with margin) The margin based analogue to example 1 is defined similarly. This class is the class that is learnt by multiclass SVM.\n4.1.3. The classes Hd,t,q and Hd,t,q,R for structured output prediction\nNext we consider an embedding \u03a8 that is specialized and used in classification tasks where the number of possible labels is exponentially large, but the labels are structured (e.g. Taskar et al. (2003)). For example, in speech recognition, the label space might me the collection of all sequences of \u2264 20 English words.\nTo motivate the definition, consider the case that we are to recognize a t-letter word appearing in an image. Let q be the size of the alphabet. The set of possible labels is naturally associated with [q]t. A popular method to tackle this task (see for example Taskar et al. (2003)) is the following: The image is broken into t parts, each of which contains a single letter. Each letter is represented as a vector in Rd. Thus, each image is represented as a matrix in Md\u00d7t. To devise a linear hypothesis class to this problem, we should specify a mapping \u03a8 : Md\u00d7t \u00d7 [q]\nt \u2192 Rn for some n. Given X \u2208 Md\u00d7t and y \u2208 [q]t, \u03a8(X, y) will be a pair (\u03a81(X, y),\u03a82(X, y)). The mapping \u03a81 allows the classifiers to take into account the shape of the letters appearing in the different t parts the word was broken into. The mapping \u03a82 allows the classifiers to take into account the structure of the language (e.g. the fact that the letter \u201cu\u201d usually appears after the letter \u201cq\u201d). \u03a81(X, y) \u2208 Md\u00d7q is the matrix whose j\u2019th column is the sum of the columns X\ni with yi = j (in other words, the j\u2019th column is the sum of the letters in the image that are predicted to be j by y). \u03a82(X, y) \u2208 Mq,q will be the matrix with 1 in the (i, j) entry if the letter j appears after the letter i somewhere in the word y, and 0 in all other entries. Even though the number of labels is exponential in t, this class (in the realizable case) can be learnt in time polynomial in d, t and q (see Collins (2005)).\nWe will show gaps in the performance of different ERMs for the class H\u03a8. If fact, we will prove a slightly stronger result. We will consider the class H\u03a81 , that we will denote by Hd,t,q. It is easy to see that H\u03a81 can be realized by H\u03a8. Therefore, any lower bound for H\u03a81 automatically lower bounds also H\u03a8. As for upper bounds, as long as q = O(d), the upper bounds we show are the same for H\u03a8 and H\u03a81 . To summarize, the gaps we show for H\u03a81 automatically (as long as q = O(d)) hold for H\u03a8 as well.\nFinally, we define a margin-based analogue to Hd,t,q. The instance space is (B d)t, and we treat each X \u2208 (Bd)t as a matrix with t columns, each of which is a vector in Bd. The labels are [q]t. Define \u03a8 : (Bd)t \u00d7 [q]k \u2192 Md\u00d7q as follows: for X \u2208 (B\nd)t and y \u2208 [q]t, \u03a8(X, y) is the matrix whose j\u2019th column is 1q of the average of all columns X\ni such that yi = j. Note that the range of \u03a8 is contained in the unit ball. For R > 0, define Hd,t,q,R := H\u03a8,R."}, {"heading": "4.2. Results", "text": "We begin with linear predictors without margin. The first part of the following theorem asserts that for every \u03a8 : X \u00d7Y \u2192 Rd there is some algorithm that learns H\u03a8 with sample complexity O (\nd log(1/\u01eb)+log(1/\u03b4) \u01eb\n)\n. The second part of the theorem shows that in several\ncases (i.e., for some \u03a8\u2019s), this algorithm outperforms other ERMs, by a factor of log(|Y|).\nTheorem 5\n\u2022 For every \u03a8 : X \u00d7 Y \u2192 Rd, the PAC sample complexity of H\u03a8 is O ( d log(1/\u01eb)+log(1/\u03b4) \u01eb ) ,\nand is achievable by a new efficient1 compression scheme. \u2022 For every Y and d > 0, there is some \u03a8 : X \u00d7 Y \u2192 Rd for which the ERM sample\ncomplexity of H\u03a8 is \u2126 ( d log(|Y|)+log(1/\u03b4) \u01eb ) .\nTo put the result in the relevant context, it was known (e.g. Daniely et al. (2011)) that the sample complexity of every ERM for this class is O (\nd log(|Y|) log(1/\u01eb)+log(1/\u03b4) \u01eb\n)\n. In particular,\nthe second part of the theorem is tight, up to the logarithmic dependence over 1\u01eb . However, it was not known whether the factor of log(|Y|) for general ERM is necessary. The second part of the theorem shows that this factor is indeed necessary.\nAs to the tightness of the first part, for certain embeddings, including the multivector\nconstruction (example 1), a lower bound of \u2126 (\nd+log(1/\u03b4) \u01eb\n)\nis known for every algorithm.\nHence, the first part of the theorem is also tight up to the logarithmic dependence over 1\u01eb . Our second theorem for linear classes is analogous to theorem 5 for margin based classes. The first part shows that for every \u03a8 : X \u00d7 Y \u2192 Bd there is some algorithm that learns H\u03a8,R with sample complexity O ( R log(1/\u01eb)+log(1/\u03b4) \u01eb ) . The second part of the theorem shows that in several cases, the above algorithm outperforms other ERMs, by a factor of log(|Y|).\nTheorem 6\n\u2022 For every \u03a8 : X \u00d7 Y \u2192 Bd and R > 0, the PAC sample complexity of H\u03a8,R is\nO (\nR log(1/\u01eb)+log(1/\u03b4) \u01eb\n)\n. \u2022 For every Y and R > 0, there is some \u03a8 : X \u00d7 Y \u2192 Bd for2 which the ERM sample\ncomplexity of H\u03a8,R is \u2126 ( R log(|Y|)+log(1/\u03b4) \u01eb ) .\nThe first part of the theorem is not new. An algorithm that achieves this bound is the perceptron. It was known (e.g. Collins (2005)) that the sample complexity of every ERM for this class is O (\nR log(|Y|/\u01eb)+log(1/\u03b4) \u01eb\n)\n. In particular, the second part of the theorem is tight,\nup to the logarithmic dependence over 1\u01eb . However, it was not known whether the gap is real: In (Collins, 2005), it was left as an open question to show whether the perceptron\u2019s bound holds for every ERM. The second part of the theorem answers this open question in negative. Regarding lower bounds, as in the case of H\u03a8, for certain embeddings, including the multivector construction with margin (example 1), a lower bound of \u2126 (\nR+log(1/\u03b4) \u01eb\n)\nis\nknown and valid for every learning algorithm. In particular, the first part of the theorem is also tight up to the logarithmic dependence over 1\u01eb .\nAn additional result that we report on shows that, for every \u03a8 : X \u00d7 Y \u2192 Rd, the Natarajan dimension of H\u03a8 is at most d (the definition of the Natarajan dimension is recalled in the appendix). This strengthens the result of (Daniely et al., 2011) who showed that it is bounded by O(d log(d)). It is known (e.g. Daniely et al. (2012)) that for the multivector construction (example 1), in which the dimension of the range of \u03a8 is dk, the\n1. Assuming we have an appropriate separation oracle. 2. Here, d can be taken to be polynomial in R and log(|Y|).\nNatarajan dimension is lower bounded by (d \u2212 1)(k \u2212 1). Therefore, the theorem is tight up to a factor of 1 + o(1).\nTheorem 7 For every \u03a8 : X \u00d7 Y \u2192 Rd, Ndim(H\u03a8) \u2264 d.\nNext, we give analogs to theorems 5 and 6 for the structured output classes Hd,k and Hd,k,R. These theorems show that the phenomenon of gaps between different ERMs, as reported in (Daniely et al., 2011), happens also in hypothesis classes that are used in practice.\nTheorem 8 \u2022 For every d, t, q > 0, the PAC sample complexity of Hd,t,q is O ( dq log(1/\u01eb)+log(1/\u03b4) \u01eb ) . \u2022 For every d, t, q > 0 the ERM sample complexity of Hd,t,q is \u2126 ( dq log(t)+log(1/\u03b4) \u01eb ) .\nTheorem 9 \u2022 For every d, t, q,R > 0, the PAC sample complexity of Hd,t,q,R is O ( R log(1/\u01eb)+log(1/\u03b4) \u01eb )\n. \u2022 For every t, q,R > 0 and d \u2265 (t + 1)R, the ERM sample complexity of Hd,t,q,R is\n\u2126 (\nR log(t)+log(1/\u03b4) \u01eb\n)\n.\nThe first parts of theorems 8 and 9 are direct consequences of theorems 5 and 6. These results are also tight up to the logarithmic dependence over 1\u01eb . The second parts of the theorems do not follow from theorems 5 and 6. Regarding the tightness of the second part, the best known upper bounds for the ERM sample complexity of Hd,t,q and Hd,t,q,R are O ( dqt log( 1 \u01eb )+log(1/\u03b4)\n\u01eb\n) and O ( Rt log( 1 \u01eb )+log(1/\u03b4)\n\u01eb\n)\nrespectively. Closing the gap between these\nupper bounds and the lower bounds of theorems 8 and 9 is left as an open question."}, {"heading": "4.3. The compression-based optimal learners", "text": "Each of the theorems 5, 6, 8 and 9 are composed of two statements. The first claims that some algorithm have a certain sample complexity, while the second claims that there exists an ERM whose sample complexity is worse than the sample complexity of the algorithm from the first part. As explained in this subsection, the first parts of these theorems are established by devising (efficient) compression schemes. In the next subsection we will elaborate on the proof of the second parts (the lower bounds on specific ERMs). Unfortunately, due to lack of space, we must be very brief.\nWe now show that for linear classes, it is possible to derive optimal learners which are also computationally efficient. For the case of margin-based classes, this result is not new \u2014 an efficient algorithm based on the multiclass perceptron has been proposed in Collins (2002). For completeness, we briefly survey this approach in the appendix. For dimension based linear classes, we give a new efficient algorithm.\nThe algorithm relies on compression based generalization bounds (see Theorem 18 in the appendix). Based on this theorem, it is enough to show that for every \u03a8 : X \u00d7Y \u2192 Rd, H\u03a8 has a compression scheme of size d. We consider the following compression scheme. Given a realizable sample (x1, y1), . . . , (xm, ym), let Z \u2286 R\nd be the set of all vectors of the form \u03a8(xi, yi) \u2212 \u03a8(xi, y) for y 6= yi. Let w be the vector of minimal norm in the convex hull of Z, conv(Z). Note that by the convexity of conv(Z), w is unique and can be found efficiently using a convex optimization procedure. Represent w as a convex combination of\nd vectors from Z. This is possible since, by claim 1 below, 0 6\u2208 conv(Z). Therefore, w is in the boundary of the polytope conv(Z). Thus, w lies in a convex polytope whose dimension is \u2264 d\u2212 1, and is the convex hull of points from Z. Therefore, by Caratheodory\u2019s theorem (and using its efficient constructive proof), w is a convex combination of \u2264 d points from Z. Output the examples in the sample that correspond to the vectors in the above convex combination. If there are less than d such examples, arbitrarily output more examples.\nThe De-Compression procedure is as follows. Given (x1, y1), . . . , (xd, yd), let Z \u2032 \u2286 Rd be the set of all vectors of the form \u03a8(xi, yi)\u2212 \u03a8(xi, y) for y 6= yi. Then, output the minimal norm vector w \u2208 conv(Z \u2032).\nIn the appendix (Section D.5) we show that this is indeed a valid compression scheme, that is, if we start with a realizable sample (x1, y1), . . . , (xm, ym), compress it, and then de-compress it, we are left with a hypothesis that makes no errors on the original sample."}, {"heading": "4.4. Lower bounds for specific ERMs", "text": "Next, we explain how we prove the second parts of theorems 5, 6, 8 and 9. For theorems 5 and 6, the idea is to start with the first Cantor class (introduced in section 2) and by a geometric construction, realize it by a linear class. This realization enables us to extend the \u201cbad ERM\u201d for the first Cantor class, to a \u201cbad ERM\u201d for that linear class. The idea behind the lower bounds of theorems 8 and 9 is similar, but technically more involved. Instead of the first Cantor class, we introduce a new discrete class, the second Cantor class, which may be of independent interest. This class, which can be viewed as a dual to the first Cantor class, is defined as follows. Let Y\u0303 be some non-empty finite set. Let X = 2Y\u0303 and\nlet Y = Y\u0303 \u222a {\u2217}. For every y \u2208 Y\u0303 define a function hy : X \u2192 Y by hy(A) =\n{\ny y \u2208 A \u2217 otherwise .\nAlso, let h\u2217 : X \u2192 Y be the constant function \u2217. Finally, let HY ,Cantor = {hy | y \u2208 Y}. In section C we show that the graph dimension (see a definition in the appendix) of HY ,Cantor is \u0398(log(|Y|)). The analysis of the graph dimension of this class is more involved than the first Cantor class: by a probabilistic argument, we show that a random choice of \u2126 (log(|Y|)) points from X is shattered with positive probability. We show also (see section C) that the PAC sample complexity of HY ,Cantor is \u2264 log(1/\u03b4)\n\u01eb . Since the graph dimension characterizes the ERM sample complexity (see the appendix), this class provides another example of a hypothesis class with gaps between ERM and PAC learnability."}, {"heading": "5. A new dimension", "text": "Consider again the question of characterizing the sample complexity of learning a class H. Theorem 2 shows that the sample complexity of a class H is characterized by the sequence of densities \u00b5H(m). A better characterization would be a notion of dimension that assigns a single number, dim(H), that controls the growth of \u00b5H(m), and consequently, the sample complexity of learning H. To reach a plausible generalization, let us return for a moment to binary classification, and examine the relationships between the VC dimension and the sequence \u00b5H(m). It is not hard to see that\n\u2022 The VC dimension of H is the maximal number d such that \u00b5H(d) = d.\nMoreover, a beautiful result of Haussler et al. (1988) shows that\n\u2022 If |Y| = 2, then VCdim(H) \u2264 \u00b5H(m) \u2264 2VCdim(H) for every m \u2265 VCdim(H).\nThese definition and theorem naturally suggest a generalization to multiclass classification:\nDefinition 10 The dimension, dim(H), of the class H \u2282 YX is the maximal number d such that \u00b5H(d) = d.\nConjecture 11 There exists a constant C > 0 such that for every H and m \u2265 dim(H), dim(H) \u2264 \u00b5H(m) \u2264 C \u00b7 dim(H) . Consequently, by Theorem 2,\n\u01ebH(m) = \u0398\n(\ndim(H)\nm\n)\nand \u2126\n(\ndim(H) + log ( 1 \u03b4 )\n\u01eb\n)\n\u2264 mH(\u01eb, \u03b4) \u2264 O\n(\ndim(H) log ( 1 \u03b4 )\n\u01eb\n)\nFor concreteness, we give an equivalent definition of dim(H) and a formulation of conjecture 11 that are somewhat simpler, and do not involve the sequence \u00b5H(m)\nDefinition 12 Let H \u2282 YX . We say that A \u2282 X is shattered by H is there exists a finite F \u2282 H such that for every x \u2208 A and f \u2208 F there is g \u2208 F such that g(x) 6= f(x) and g|A\\{x} = f |A\\{x}. The dimension of H is the maximal cardinality of a shattered set.\nRecall that the degree (w.r.t. H \u2282 YX ) of f \u2208 H is the number of points x \u2208 X for which there exists g \u2208 H that disagree with f only on x. We denote the average degree of H by d(H).\nConjecture 13 There exists C > 0 such that for every finite H, d(H) \u2264 C \u00b7 dim(H).\nBy combination of theorems 2 and Rubinstein et al. (2006), a weaker version of conjecture 11 is true. Namely, that for some absolute constant C > 0\ndim(H) \u2264 \u00b5H(m) \u2264 C \u00b7 log(|Y|) \u00b7 dim(H) . (2)\nIn addition, it is not hard to see that the new dimension is bounded between the Natarajan and Graph dimensions, Ndim(H) \u2264 dim(H) \u2264 Gdim(H). For the purpose of characterizing the sample complexity, this inequality is appealing for two reasons. First, it is known (Daniely et al., 2011) that the graph dimension does not characterize the sample complexity, since it can be substantially larger than the sample complexity in several cases. Therefore, any notion of dimension that do characterize the sample complexity must be upper bounded by the graph dimension. As for the Natarajan dimension, it is known to lower bound the sample complexity. By Theorem 2 and equation (2), the new dimension also lower bounds the sample complexity. Therefore, the left inequality shows that the new dimension always provides a lower bound that is at least as good as the Natarajan dimension\u2019s lower bound."}, {"heading": "Appendix A. Agnostic learning and further directions", "text": "In this work we focused on learning in the realizable setting. For general hypothesis classes, it is left as an open question to find an optimal algorithm for the agnostic setting. However, for linear classes, our upper bounds are attained by compression schemes. Therefore, as indicated by Theorem 18, our results can be extended to the agnostic setting, yielding algorithms for H\u03a8 and H\u03a8,R whose sample complexity is O ( d log(d/\u01eb)+log(1/\u03b4) \u01eb2 ) and O (\nR log(R/\u01eb)+log(1/\u03b4) \u01eb2\n)\nrespectively. We note that these upper bounds are optimal, up to the\nfactors of log(d/\u01eb) and log(R/\u01eb). Our lower bounds clearly hold for agnostic learning (this is true for any lower bound on the realizable case). Yet, we would be excited to see better lower bounds for the agnostic setting. Specifically, are there classes H \u2282 YX of Natarajan dimension d with ERMs whose agnostic sample complexity is \u2126 (\nd log(|Y|) \u01eb2\n)\n?\nExcept extensions to the agnostic settings, the current work suggests several more directions for further research. First, it would be very interesting to go beyond multiclass classification, and to devise generic optimal algorithms for other families of learning problems. Second, as noted before, naive implementation of the one-inclusion algorithm is prohibitively inefficient. Yet, we still believe that the ideas behind the one-inclusion algorithm might lead to better efficient algorithms. In particular, it might be possible to derive efficient algorithms based on the principles behind the one-inclusion algorithm, and maybe even give an efficient implementation of the one-inclusion algorithm for concrete hypothesis classes."}, {"heading": "Appendix B. Background", "text": ""}, {"heading": "B.1. The Natarajan and Graph Dimensions", "text": "We recall two of the main generalizations of the VC dimension to multiclass hypothesis classes.\nDefinition 14 (Graph dimension) Let H \u2286 (Y \u222a {\u2296})X be a hypothesis class. We say that A \u2286 X is G-shattered if there exists h : A \u2192 Y such that for every B \u2286 A there is h\u2032 \u2208 H with h(A) \u2282 Y for which\n\u2200x \u2208 B, h\u2032(x) = h(x) while \u2200x \u2208 A \\B, h\u2032(x) 6= h(x) .\nThe graph dimension of H, denoted Gdim(H), is the maximal cardinality of a G-shattered set.\nAs the following theorem shows, the graph dimension essentially characterizes the ERM sample complexity.\nTheorem 15 (Daniely et al. (2011)) For every hypothesis class H with graph dimension d,\n\u2126\n(\nd+ log(1/\u03b4)\n\u01eb\n)\n\u2264 mERM(\u01eb, \u03b4) \u2264 O\n(\nd log(1/\u01eb) + log(1/\u03b4)\n\u01eb\n)\n.\nDefinition 16 (Natarajan dimension) Let H \u2286 (Y \u222a {\u2296})X be a hypothesis class. We say that A \u2286 X is N -shattered if there exist h1, h2 : A \u2192 Y such that \u2200x \u2208 A, h1(x) 6= h2(x) and for every B \u2286 A there is h \u2208 H for which\n\u2200x \u2208 B, h(x) = h1(x) while \u2200x \u2208 A \\B, h(x) = h2(x) .\nThe Natarajan dimension of H, denoted Ndim(H), is the maximal cardinality of an N - shattered set.\nTheorem 17 (essentially Natarajan (1989)) For every hypothesis class H \u2282 (Y \u222a {\u2296})X with Natarajan dimension d,\n\u2126\n(\nd+ log(1/\u03b4)\n\u01eb\n)\n\u2264 mPAC(\u01eb, \u03b4) \u2264 O\n(\nd log(|Y|) log(1/\u01eb) + log(1/\u03b4)\n\u01eb\n)\n.\nWe note that the upper bound in the last theorem follows from theorem 15 and the fact that (see Ben-David et al. (1995)) for every hypothesis class H,\nGdim(H) \u2264 5 log(|Y|)Ndim(H) . (3)\nWe also note that (Daniely et al., 2011) conjectured that the logarithmic factor of |Y| in Theorem 17 can be eliminated (maybe with the expense of poly-logarithmic factors of 1\u01eb , 1 \u03b4 and Ndim(H))."}, {"heading": "B.2. Compression Schemes", "text": "A compression scheme of size d for a class H is a pair of functions:\nCom : \u222a\u221em=d(X \u00d7 Y) m \u2192 (X \u00d7 Y)d and DeCom : (X \u00d7 Y)d \u2192 YX ,\nwith the property that for every realizable sample\nS = (x1, y1), . . . , (xm, ym)\nit holds that, if h = DeCom \u25e6Com(S) then\n\u22001 \u2264 i \u2264 m, yi = h(xi) .\nEach compression scheme yields a learning algorithm, namely, DeCom \u25e6Com. It is known that the sample complexity of this algorithm is upper bounded by the size of the compression scheme. Precisely, we have:\nTheorem 18 (Littlestone and Warmuth (1986)) Suppose that there exists a compression scheme of size d for a class H. Then:\n\u2022 The PAC sample complexity of H is upper bounded by O ( d log(1/\u01eb)+ 1 \u03b4\n\u01eb\n)\n\u2022 The agnostic PAC sample complexity of H is upper bounded by O ( d log(d/\u01eb)+ 1 \u03b4\n\u01eb2\n)"}, {"heading": "Appendix C. The Cantor classes", "text": "C.1. The first Cantor class\nLet X be some finite set and let Y = 2X \u222a {\u2217}. For every A \u2286 X define hA : X \u2192 Y by\nhA(x) =\n{\nA x \u2208 A \u2217 otherwise .\nFinally, let HX ,Cantor = {hA | A \u2282 X} .\nLemma 19 (Daniely et al. (2011))\n\u2022 The graph dimension of HX ,Cantor is |X |. Therefore, the ERM sample complexity of\nHX ,Cantor is \u2126 ( |X |+log(1/\u03b4) \u01eb ) .\n\u2022 The Natarajan dimension of HX ,Cantor is 1. Furthermore, the PAC sample complexity\nof HX ,Cantor is O ( log(1/\u03b4) \u01eb ) .\nProof For the first part, it is not hard to see that the function f\u2205 witnesses the G-shattering of X . The second part follows directly from Lemma 20, given below.\nLemma 20 (essentially Daniely et al. (2011)) Let H \u2282 YX be a hypothesis class with the following property: There is a label \u2217 \u2208 Y such that, for every f \u2208 H and x \u2208 X , either f(x) = \u2217 or f is the only function in H whose value at x is f(x). Then,\n\u2022 The PAC sample complexity of H is \u2264 log(1/\u03b4)\u01eb .\n\u2022 Ndim(H) \u2264 1.\nProof We first prove the second part. Assume on the way of contradiction that Ndim(H) > 1. Let {x1, x2} \u2286 X be an N -shattered set of cardinality 2 and let f1, f2 be two functions that witness the shattering. Since f1(x1) 6= f2(x1), at least one of f1(x1), f2(x1) is different from \u2217. W.l.o.g, assume that f1(x1) 6= \u2217. Now, by the definition of N -shattering, there is a function f \u2208 HY ,Cantor such that f(x1) = f1(x1) and f(x2) = f2(x2) 6= f1(x2). However, the only function in H satisfying f(x1) = f1(x1) is f1. A contradiction.\nWe proceed to the first part. Assume w.l.o.g. the the function f\u2217 \u2261 \u2217 is in H. Consider the following algorithm. Given a (realizable) sample\n(x1, y1), . . . , (xm, ym),\nif yi = \u2217 for every i then return the function f\u2217. Otherwise, return the hypothesis h \u2208 H, that is consistent with the sample. Note the the existence of a consistent hypothesis is guaranteed, as the sample is realizable. This consistent hypothesis is also unique: if yi 6= \u2217 then, by the assumption on H, there is at most one function f \u2208 H for which h(xi) = yi.\nThis algorithm is an ERM with the following property: For every learnt hypothesis and underlying distribution, the algorithm might return only one out of two functions \u2013 either f\u2217 or the learnt hypothesis. We claim that the sample complexity of such an ERM must be \u2264 log(1/\u03b4)\u01eb . Indeed such an algorithm returns a hypothesis with error \u2265 \u01eb only if:\n\u2022 Err(f\u2217) \u2265 \u01eb.\n\u2022 For every i \u2208 [m], yi = \u2217.\nHowever, if Err(f\u2217) \u2265 \u01eb, the probability that yi = \u2217 is \u2264 1 \u2212 \u01eb. Therefore, the probability of the the second condition is \u2264 (1\u2212 \u01eb)m \u2264 e\u2212m\u01eb, which is \u2264 \u03b4 if m \u2265 log(1/\u03b4)\u01eb .\nC.2. The second Cantor class\nLet Y\u0303 be some non-empty finite set. Let X = 2Y\u0303 and let Y = Y\u0303 \u222a {\u2217}. For every y \u2208 Y\u0303 define a function hy : X \u2192 Y by\nhy(A) =\n{\ny y \u2208 A \u2217 otherwise .\nAlso, let h\u2217 : X \u2192 Y be the constant function \u2217. Finally, let HY ,Cantor = {hy | y \u2208 Y}.\nLemma 21\n\u2022 The graph dimension of HY ,Cantor is \u0398(log (|Y|)). Therefore, the ERM sample com-\nplexity of HY ,Cantor is \u2126 ( log(|Y|)+log(1/\u03b4) \u01eb ) .\n\u2022 The Natarajan dimension of HY ,Cantor is 1. Furthermore, the PAC sample complexity\nof HY ,Cantor is O ( log(1/\u03b4) \u01eb ) .\nProof The second part of the lemma follows from Lemma 20. We proceed to the first part. First, by equation (3) and the second part, Gdim(HY ,Cantor) \u2264 5 log(|Y|). It remains to show that Gdim(HY ,Cantor) \u2265 \u2126 (log(|Y|)). To do so, we must show that there are r = \u2126(log(|Y|)) sets A = {A1, . . . , Ar} \u2286 X such that A is G-shattered by HY ,Cantor. To do so, we will use the probabilistic method (see e.g. Alon and Spencer (2000)). We will choose A1, . . . , Ar \u2286 Y\u0303 at random, such that each Ai is chosen uniformly at random from all subsets of Y\u0303 (i.e., each y \u2208 Y\u0303 is independently chosen to be in Ai with probability 1 2 ) and the different Ai\u2019s are independent. We will show that if r = \u230a log(|Y|\u22121)\n2 \u230b \u2212 2, then with positive probability A = {A1, . . . , Ar} is G-shattered and |A| = r (i.e., the Ai\u2019s are different).\nDenote d = |Y\u0303 |. Let \u03c8 : [r] \u2192 X be the (random) function \u03c8(i) = Ai and let \u03c6 : Y \u2192 {0, 1} be the function that maps each y \u2208 Y\u0303 to 1 and \u2217 to 0. Consider the (random) binary hypothesis class H = {\u03c6 \u25e6 hy \u25e6 \u03c8 | y \u2208 Y\u0303}. As we will show, for r = \u230a log(d)\n2 \u230b \u2212 2, E[|H|] > 2r \u2212 1. In particular, there exists some choice of A = {A1, . . . , Ar} for which |H| > 2r \u2212 1. Fix those sets for a moment. Since always |H| \u2264 2r, it must be the case that |H| = 2r, i.e., H = 2[r]. By the definition of H, it follows that for every B \u2286 [r], there is hy \u2208 HY ,Cantor such that for every i \u2208 B, hy(Ai) = \u2217, while for every i /\u2208 B, hy(Ai) 6= \u2217. It follows that |A| = r and A is G-shattered.\nIt remains to show that indeed, for r = \u230a log(d)2 \u230b \u2212 2, E[|H|] > 2 r \u2212 1. For every S \u2286 [r],\nLet \u03c7S be the indicator random variable that is 1 if and only if 1S \u2208 H. We have\nE[|H|] = E[ \u2211\nS\u2286[r]\n\u03c7S] = \u2211\nS\u2286[r]\nE[\u03c7S ] . (4)\nFix some S \u2286 [r]. For every y \u2208 Y\u0303 let \u03c7S,y be the indicator function that is 1 if and only if 1S = \u03c6 \u25e6 hy \u25e6 \u03c8. Note that \u2211\ny\u2208Y\u0303 \u03c7S,y > 0 if and only if \u03c7S = 1. Therefore,\nE[\u03c7S ] = Pr (\u03c7S = 1) = Pr ( \u2211 y\u2208Y\u0303 \u03c7S,y > 0 ) . Observe that\nE[ \u2211\ny\u2208Y\u0303\n\u03c7S,y] = \u2211\ny\u2208Y\u0303\nPr (y \u2208 Ai iff i \u2208 S) = d \u00b7 2 \u2212r .\nWe would like to use Chebyshev\u2019s inequality for the sum \u2211\ny\u2208Y\u0303 \u03c7S,y. For this to be effective,\nwe show next that for different y1, y2 \u2208 Y\u0303 , \u03c7S,y1 and \u03c7S,y2 are uncorrelated. Note that E[\u03c7S,y1\u03c7S,y2 ] is the probability that for every i \u2208 S, y1, y2 \u2208 Ai while for every i /\u2208 S, y1, y2 /\u2208 Ai. It follows that\nE[\u03c7S,y1\u03c7S,y2 ] = 2 \u22122r .\nTherefore, cov(\u03c7S,y1\u03c7S,y2) = E[\u03c7S,y1\u03c7S,y2 ] \u2212 E[\u03c7S,y1 ]E[\u03c7S,y2 ] = 2 \u22122r \u2212 2\u2212r2\u2212r = 0. We conclude that \u03c7S,y1 and \u03c7S,y2 are uncorrelated. Thus, by Chebyshev\u2019s inequality,\nPr (\u03c7S = 0) = Pr\n\n\n\u2211\ny\u2208Y\u0303\n\u03c7S,y = 0\n\n\n\u2264 Pr\n\n\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2211\ny\u2208Y\u0303\n\u03c7S,y \u2212 d \u00b7 2 \u2212r\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2265 d \u00b7 2\u2212r\u22121  \n\u2264 22r+2\nd2 var\n\n\n\u2211\ny\u2208Y\u0303\n\u03c7S,y\n\n\n= 22r+2\nd2\n\u2211\ny\u2208Y\u0303\nvar (\u03c7S,y)\n\u2264 22r+2\nd2\n\u2211\ny\u2208Y\u0303\nE[\u03c7S,y]\n= 22r+2\nd2 d2\u2212r =\n2r+2\nd .\nRemember that r = \u230a log(d)2 \u230b\u22122, so that d > 2 2r+2. Hence, E[\u03c7S ] = 1\u2212Pr(\u03c7S = 0) \u2265 1\u22122 \u2212r. Using equation (4), we conclude that\nE[|H|] > (1\u2212 2\u2212r)2r = 2r \u2212 1."}, {"heading": "Appendix D. Proofs", "text": "D.1. Some lemmas and additional notations\nLet X \u2032,Y \u2032 be another instance and label spaces. Let \u0393 : X \u2032 \u2192 X and \u039b : Y\u222a{\u2296} \u2192 Y \u2032\u222a{\u2296}. We denote\n\u039b \u25e6 H \u25e6 \u0393 = {\u039b \u25e6 h \u25e6 \u0393 | h \u2208 H} .\nIf \u0393 (respectively \u039b) is the identity function we simplify the above notation to \u039b \u25e6 H (respectively H \u25e6 \u0393). We say that a hypothesis class H\u2032 \u2286 (Y \u2032 \u222a {\u2296})X \u2032\nis realizable by H \u2286 (Y \u222a {\u2296})X if H\u2032 \u2286 \u039b \u25e6 H \u25e6 \u0393 for some functions \u0393 and \u039b. Note that in this case, the different notions of sample complexity with respect to H\u2032 are never larger than the corresponding notions with respect to H.\nLet H \u2282 (Y \u222a {\u2296})X be a hypothesis class. The disjoint union of m copies of H is the hypothesis class Hm whose instance space is Xm := X \u00d7 [m], whose label space is Y \u222a {\u2296}, and that is composed of all functions f : Xm \u2192 Y \u222a {\u2296} whose restriction to each copy of X is a function in H (namely, for every i \u2208 [m], the function x 7\u2192 f(x, i) belongs to H).\nLemma 22 Let H \u2286 YX be a hypothesis class. Let Hm be a disjoint union of m copies of H.\n1. If H is realized by H\u03a8 for some \u03a8 : X \u2032 \u00d7 Y \u2032 \u2192 Rd, then Hm is realized by H\u03a8m for\nsome \u03a8m : X \u2032 m \u00d7 Y \u2032 \u2192 Rdm. Here, X \u2032m is a disjoint union of m copies of X \u2032.\n2. If H is realized by H\u03a8,R for some \u03a8 : X \u2032 \u00d7Y \u2032 \u2192 Bd, then Hm is realized by H\u03a8m,mR\nfor some \u03a8m : X \u2032 m \u00d7 Y \u2032 \u2192 Bdm. Here, X \u2032m is a disjoint union of m copies of X \u2032.\n3. If H is realized by Hd,k, then Hm is realized by Hdm,k.\n4. If H is realized by Hd,k,R, then Hm is realized by Hdm,k,mR.\nProof We prove only part 1. The remaining three are very similar. Let \u0393 : X \u2192 X \u2032,\u039b : Y \u2032 \u2192 Y be two mappings for which\nH \u2286 \u039b \u25e6 H\u03a8 \u25e6 \u0393 .\nLet Xm = X \u00d7 [m] be a disjoint union of m copies of X . Let Ti : R d \u2192 Rdm be the linear mapping that maps ej to e(i\u22121)d+j . Define \u03a8m : Xm \u00d7 Y \u2192 R dm by \u03a8m((x, i), y) = Ti(\u03a8(x, y)). Define \u0393m : Xm \u2192 X \u2032 m by \u0393m(x, i) = (\u0393(x), i). It is not hard to check that\nHm \u2286 \u039b \u25e6 H\u03a8m \u25e6 \u0393m .\nLemma 23 Let H \u2286 (Y \u222a {\u2296)X be a hypothesis class and let Hm be a disjoint union of m copies of H. Then Gdim(Hm) = m \u00b7Gdim(H).\nProof A routine verification."}, {"heading": "D.2. Proof of Theorem 1", "text": "For simplicity, we prove the theorem for d even and d = \u221e. For finite d, fix some d-elements set Xd. Let Yd = 2 Xd \u222a {\u2217}. For A \u2286 Xd define hA : Xd \u2192 Yd by\nhA(x) =\n{\nA x \u2208 A \u2217 otherwise .\nFinally, let\nHd =\n{\nhA | |A| = d\n2\n}\n.\nWe next define a \u201climit\u201d of the classes Hd. Suppose that the sets {Xd}d is even integer are pairwise disjoint. Let X\u221e = \u222ad is evenXd and Y\u221e = ( \u222ad is even2 Xd )\n\u222a {\u2217}. For A \u2286 Xd, extend hA : Xd \u2192 Yd to a function hA : X\u221e \u2192 Y\u221e by defining it to be \u2217 outside of Xd. Finally, let\nH\u221e =\n{\nhA | for some d, A \u2286 Xd and |A| = d\n2\n}\n.\nWe will use the following version of Chernoff\u2019s bound:\nTheorem 24 Let X1, . . . ,Xn \u2208 {0, 1} be independent random variables, X = X1+. . .+Xn and \u00b5 = E[X]. Then Pr (X \u2265 2\u00b5) \u2264 exp (\n\u2212\u00b53 ) .\nWe are now ready to prove Theorem 1. The first part follows from Lemma 20. The last part is a direct consequence of the first and second part. We proceed to the second part. For d < \u221e, the task of properly learning Hd can be easily reduced to the task of properly learning H\u221e. Therefore, the sample complexity of learning H\u221e by a proper learning algorithm is lower bounded by the sample complexity of properly learning Hd. Therefore, it is enough to prove the second part for finite d.\nFix some x0 \u2208 X . Let \u01eb > 0. Let A \u2282 Xd \\ {x0} be a set with d 2 elements. Let DA be a distribution on Xd \u00d7 Yd that assigns a probability of 1 \u2212 16\u01eb to some point (x0, hA(x0)) \u2208 Xd \u00d7Yd and is uniform on the remaining points of the form {(x, hA(x)) | x 6\u2208 A}.\nWe claim that there is some A such that whenever A runs on DA with m \u2264 1 128 d \u01eb examples, it outputs with probability \u2265 12 a hypothesis with error \u2265 \u01eb. This shows that for every \u03b4 < 12 , mA(\u01eb, \u03b4) \u2265 1 128 d \u01eb . Also, since Hd contains two different function that agree on some point, by a standard argument, we have mA(\u01eb, \u03b4) = \u2126 ( log(1/\u03b4) \u01eb ) . Combining these two estimates, the proof is established. It remains to show the existence of such A. Suppose that A is chosen uniformly at random among all subsets of Xd \\ {x0} of size d 2 . Let X be the random variable counting the number of samples, out of 1128 d \u01eb i.i.d. examples drawn from DA, which are not (x0, hA(x0)). We have E[X] = 1 8d. Therefore, by Chernoff\u2019s bound 24, with probability > 1 \u2212 exp (\n\u2212 d24 ) > 12 , the algorithm will see less than d 4 examples whose instance is from\nX \\ {x0} \\A. Conditioning on this event, A is a uniformly chosen random set of size d 2 that is chosen uniformly from all subsets of a set X \u2032 \u2282 X with |X \u2032| \u2265 34d (X \u2032 is the set of all points that are not present in the sample), and the hypothesis returned by the algorithm is hB , where B \u2282 X is a subset of size d 2 that is independent from A. It is not hard to see that in this case E|B \\ A| \u2265 16d. Hence, there exists some A for which, with probability > 12 over the choice of the sample, |B \\ A| \u2265 1 6d. For such A we have, since hB errs on all elements in B \\ A and the probability of each such element is \u2265 16\u01ebd 2 = 32d \u01eb,\nErrDA(hB) \u2265 |B \\ A| 32\u01eb\nd \u2265\nd\n6\n32\u01eb\nd > \u01eb\nwith probability > 12 over the choice of the sample."}, {"heading": "D.3. Proof of Lemma 3", "text": "We first prove it to finite hypergraphs. We use induction on the number of vertices. By assumption, d(G) \u2264 d. Therefore, there is v0 \u2208 V with d(v0) \u2264 d. Let G\n\u2032 = (V \u2032, E\u2032) = G[V \\ {v0}]. By the induction hypothesis, there exists an orientation h\n\u2032 : E\u2032 \u2192 V \u2032 with maximal out-degree d. We define an orientation h : E \u2192 V by\nh(e) =\n{\nv e = {v0, v} h\u2032(e \\ {v0}) otherwise\nThe lemma extend to the case where Y is infinite by a standard application of the compactness theorem for propositional calculus."}, {"heading": "D.4. Proof of theorem 4", "text": "Let A be some learning algorithm, and denote by I the one inclusion algorithm. Suppose that we run A on mA,H ( \u01eb 2 , \u01eb 2 )\nexamples, obtain a hypothesis h and predict h(x) on some new example. The probability of error if \u2264 (\n1\u2212 \u01eb2 ) \u01eb 2 + \u01eb 2 \u2264 \u01eb. By theorem 2, it follows that\nmA,H\n( \u01eb 2 , \u01eb 2 ) \u2265 min\n{\nm | 1\n2e\n\u00b5H(m)\nm \u2264 \u01eb\n}\n=: m\u0304 .\nNow, if we run the one inclusion algorithm on m\u0304 examples then, again by theorem 2, the probability that the hypothesis it return will err a new example is \u2264 2e\u01eb. Therefore, the probability that the error of the returned hypothesis is \u2265 4e\u01eb is \u2264 12 . In follows that\nm\u0304 \u2265 mI,H\n(\n4e\u01eb, 1\n2\n)\n.\nCombining the two inequalities, we obtain that\nmI,H\n(\n4e\u01eb, 1\n2\n)\n\u2264 mA,H\n( \u01eb 2 , \u01eb 2 )\nSince this is true for every algorithm A, we have\nmI,H\n(\n4e\u01eb, 1\n2\n)\n\u2264 mPAC,H\n( \u01eb 2 , \u01eb 2 ) \u2264 mPAC,H\n(\n\u01eb 4 , 1 2\n)\n\u00b7 O (log(1/\u01eb))\nHere, the last inequality follows by a standard repetition argument. Equivalently,\nmI,H\n(\n\u01eb, 1\n2\n)\n\u2264 mPAC,H\n(\n\u01eb 16e , 1 2\n)\n\u00b7 O (log(1/\u01eb))\nAgain, using a repetition argument we conclude that\nmI,H(\u01eb, \u03b4) \u2264 mI,H\n(\n\u01eb 2 , 1 2\n)\n\u00b7O (log(1/\u03b4)) \u2264 mPAC,H\n(\n\u01eb 32e , 1 2\n)\n\u00b7O (log(1/\u03b4) log(1/\u01eb))\nD.5. Validity of the compression scheme given in Section 4.3\nIt is not hard to see that the hypothesis we output is the minimal-norm vector w \u2208 conv(Z) (where Z is the set defined in the compression step). It is left to show that w makes no errors on the original sample. Indeed, otherwise there exists z \u2208 Z for which \u3008w, z\u3009 \u2264 0. By claim 1, z 6= 0. For \u03b1 = \u2016w\u2016 2\n\u2016z\u20162+\u2016w\u20162 \u2208 (0, 1), let w\u2032 = (1 \u2212 \u03b1)w + \u03b1z. We have that\nw\u2032 \u2208 conv(Z). Moreover,\n\u2016w\u2032\u20162 = (1\u2212 \u03b1)2\u2016w\u20162 + \u03b12\u2016z\u20162 + 2\u03b1(1 \u2212 \u03b1)\u3008w, z\u3009 \u2264 (1\u2212 \u03b1)2\u2016w\u20162 + \u03b12\u2016z\u20162\n= \u2016z\u20164\u2016w\u20162 + \u2016w\u20164\u2016z\u20162\n(\u2016z\u20162 + \u2016w\u20162)2 =\n\u2016z\u20162\u2016w\u20162\n\u2016z\u20162 + \u2016w\u20162 < \u2016w\u20162 .\nThis contradicts the minimality of w. It only remains to prove the following claim, which was used in the analysis.\nClaim 1 Let (x1, y1), . . . , (xm, ym) be a realizable sample and let Z be the set of all vectors of the form \u03a8(xi, yi)\u2212\u03a8(xi, y) for y 6= yi. Then 0 6\u2208 conv(Z).\nProof Since the sample is realizable, there exists a vector w in Rd for which, \u2200z \u2208 Z, \u3008w, z\u3009 > 0. Clearly, this holds also for every z \u2208 conv(Z), hence 0 6\u2208 conv(Z).\nD.6. Proof of the second part of Theorem 5\nWithout loss of generality, we assume that Y consists of 2n + 1 elements for some natural number n (otherwise, use only 2n + 1 labels, where n is the largest number satisfying 2n + 1 \u2264 |Y|). Let X be a set consisting of n elements. By renaming the names of the labels, we can assume that Y = 2X \u222a {\u2217}. By Lemma 21, the ERM sample complexity of HX ,Cantor is \u2126 ( log(|Y|)+log(1/\u03b4) \u01eb ) . We will show that there exists a function \u03a8 : X \u00d7Y \u2192 R3, such that HX ,Cantor is realized by H\u03a8. It follows that the ERM sample complexity of H\u03a8 is also \u2126 (\nlog(|Y|)+log(1/\u03b4) \u01eb\n)\n. Therefore, the second part of Theorem 5 is proved for d = 3. The\nextension of the result to general d follows from Lemma 22. Definition of \u03a8: Denote k = 2|X | and let f : 2X \u2192 {0, 1, . . . , k \u2212 1} be some one-to-one mapping. For A \u2286 X define\n\u03c6(A) =\n(\ncos\n(\n2\u03c0f(A)\nk\n)\n, sin\n(\n2\u03c0f(A)\nk\n)\n, 0\n)\n.\nAlso, define \u03c6(\u2217) = (0, 0, 1) .\nNote that for different subsets A,B \u2286 X we have that\n\u3008\u03c6(A), \u03c6(B)\u3009 = cos\n(\n2\u03c0(f(A)\u2212 f(B))\nk\n)\n\u2264 cos\n(\n2\u03c0\nk\n)\n< 1\n2 +\n1 2 cos\n(\n2\u03c0\nk\n)\n< 1 (5)\nDefine \u03a8 : X \u00d7 Y \u2192 R3 by\n\u2200A \u2282 X , \u03a8(x,A) =\n{\n\u03c6(A) x \u2208 A\n0 x 6\u2208 A\n\u03a8(x, \u2217) =\n(\n1 2 + 1 2 cos\n(\n2\u03c0\nk\n))\n\u00b7 \u03c6(\u2217)\nClaim 2 HX ,Cantor is realized by H\u03a8.\nProof We will show that HX ,Cantor \u2286 H\u03a8. Let B \u2286 X . We must show that hB \u2208 H\u03a8. Let w \u2208 R3 be the vector\nw = \u03c6(B) + \u03c6(\u2217) .\nWe claim that for the function hw \u2208 H\u03a8, defined by w we have hw = hB . Indeed, let x \u2208 X we split into the cases x \u2208 B and x /\u2208 B.\nCase 1 (x \u2208 B): We must show that hw(x) = B. That is, for every y \u2208 Y \\ {B},\n\u3008w,\u03a8(x,B)\u3009 > \u3008w,\u03a8(x, y)\u3009 .\nNote that \u3008w,\u03a8(x,B)\u3009 = \u3008\u03c6(B) + \u03c6(\u2217), \u03c6(B)\u3009 = 1 .\nTherefore, for every y \u2208 Y \\ {B}, we must show that 1 > \u3008w,\u03a8(x, y)\u3009. We split into three cases. If y = A for some A \u2286 X and x \u2208 A then, using equation (5),\n\u3008w,\u03a8(x, y)\u3009 = \u3008\u03c6(B) + \u03c6(\u2217), \u03c6(A)\u3009 = \u3008\u03c6(B), \u03c6(A)\u3009 < 1 .\nIf y = A for some A \u2286 X and x 6\u2208 A then,\n\u3008w,\u03a8(x, y)\u3009 = \u3008\u03c6(B) + \u03c6(\u2217), 0\u3009 = 0 < 1 .\nIf y = \u2217 then,\n\u3008w,\u03a8(x, y)\u3009 =\n\u2329\n\u03c6(B) + \u03c6(\u2217),\n(\n1 2 + 1 2 cos\n(\n2\u03c0\nk\n))\n\u00b7 \u03c6(\u2217)\n\u232a\n= 1\n2 +\n1 2 cos\n(\n2\u03c0\nk\n)\n< 1 .\nCase 2 (x /\u2208 B): We must show that hw(x) = \u2217. That is, for every A \u2208 Y \\ {\u2217},\n\u3008w,\u03a8(x, \u2217)\u3009 > \u3008w,\u03a8(x,A)\u3009 .\nNote that\n\u3008w,\u03a8(x, \u2217)\u3009 =\n\u2329\n\u03c6(B) + \u03c6(\u2217),\n(\n1 2 + 1 2 cos\n(\n2\u03c0\nk\n))\n\u03c6(\u2217)\n\u232a\n= 1\n2 +\n1 2 cos\n(\n2\u03c0\nk\n)\n.\nTherefore, for every A \u2208 Y \\{B}, we must show that 12 + 1 2 cos\n(\n2\u03c0 k\n)\n> \u3008w,\u03a8(x,A)\u3009. Indeed, if x \u2208 A then A 6= B (since x /\u2208 B). Therefore, using equation (5),\n\u3008w,\u03a8(x,A)\u3009 = \u3008\u03c6(B) + \u03c6(\u2217), \u03c6(A)\u3009 = \u3008\u03c6(B), \u03c6(A)\u3009 < 1\n2 +\n1 2 cos\n(\n2\u03c0\nk\n)\n.\nIf x /\u2208 A then\n\u3008w,\u03a8(x,A)\u3009 = \u3008\u03c6(B) + \u03c6(\u2217), 0\u3009 = 0 < 1\n2 +\n1 2 cos\n(\n2\u03c0\nk\n)\n."}, {"heading": "D.7. Proof of Theorem 6", "text": "To prove the first part of Theorem 6, we will rely again on Theorem 18. We will show a compression scheme of size O(R), which is based on the multiclass perceptron. This compression scheme is not new. However, for completeness, we briefly survey it next. Recall that the multiclass perceptron is an online classification algorithm. At each step it receives an instance and tries to predict its label, based on the observed past. The two crucial properties of the preceptron that we will rely on are the following:\n\u2022 If the perceptron runs on a sequence of examples that is realizable by H\u03a8,R, then it makes at most O(R) mistakes.\n\u2022 The predictions made by the perceptron algorithm, are affected only by previous erroneous predictions.\nBased on these two properties, the compression scheme proceeds as follows: Given a realizable sample S = {(x1, y1), . . . , (xm, ym)}, it runs the preceptron algorithm \u2126(R) times on the sequence (x1, y1), . . . , (xm, ym) (without a reset between consecutive runs). By the first property, in at least one of these runs, the preceprton will make no mistakes on the sequence (x1, y1), . . . , (xm, ym) (otherwise, there would be \u2126(R) mistakes in total). The output of the compression step would be the erroneous examples previous to this sequence. By the first property, the number of such examples is O(R). The decompression will run the preceptron on these examples, and output the hypothesis h : X \u2192 Y, such that h(x) is the prediction of the perceptron on x, after operating on these examples. By the second property, h is correct on every xi.\nWe proceed to the second part. By Lemma 23 and Lemma 19, it is enough to show that a disjoint union of \u2126(R) copies of HX ,Cantor, with |X | = \u2126(log(|Y|)), can be realized by H\u03a8,R for an appropriate mapping \u03a8 : X \u00d7 Y \u2192 B\nd for some d > 0. By Lemma 22, it is enough to show that, for some universal constant C > 0, HX ,Cantor, with |X | = \u2126(log(|Y|)), can be realized by H\u03a8,C for an appropriate mapping \u03a8 : X \u00d7 Y \u2192 B\nd for some d > 0. Without loss of generality, we assume that |Y| \u2212 1 is a power of 2 (otherwise, use only k labels, where k is the largest integer such that k\u2212 1 is a poser of 2 and k \u2264 |Y|). Denote k = |Y| \u2212 1. Fix some finite set X of cardinality log(|Y| \u2212 1). By renaming the labels, we can assume that Y = 2X \u222a {\u2217}.\nLet {ey}y\u2208Y be a collection of unit vectors in R d with the property that for y1 6= y2,\n|\u3008ey1 , ey2\u3009| < 1\n100 . (6)\nRemark 25 Clearly, it is possible to find such a collection when d = k + 1 (simply take {ey}y\u2208Y to be an orthogonal basis of R\nk+1). However, equation (6) requires the collection to be just \u201calmost orthogonal\u201d. Such a collection can be found in Rd for d = O(log(k)) (see, e.g. Matousek (2002), chapter 13).\nDefine \u03a8 : X \u00d7 Y \u2192 Bd by\n\u2200A \u2282 X , \u03a8(x,A) =\n{\neA x \u2208 A\n0 x 6\u2208 A\n\u03a8(x, \u2217) = e\u2217\nThe following claim establishes the proof of Theorem 6.\nClaim 3 HX ,Cantor is realized by H\u03a8,8.\nProof We will show that HX ,Cantor \u2286 H\u03a8,8. Let B \u2286 X . We must show that hB \u2208 H\u03a8,8. Let w = W \u00b7(eB+ 1 2e\u2217) for W = 100 45 . We claim that the hypothesis in H\u03a8,8 that corresponds to w is hB . Indeed, let x \u2208 X . We split into the cases x \u2208 B and x /\u2208 B.\nCase 1 (x \u2208 B): We must show that for every y \u2208 Y \\ {B},\n\u3008w,\u03a8(x,B)\u3009 \u2265 1 + \u3008w,\u03a8(x, y)\u3009 .\nNote that\n\u3008w,\u03a8(x,B)\u3009 =\n\u2329\nW \u00b7\n(\neB + 1\n2 e\u2217\n)\n, eB\n\u232a\n= W\n(\n1 + 1\n2 \u3008e\u2217, eB\u3009\n)\n\u2265 W\n(\n1\u2212 1\n100\n)\n.\nNow, if y \u2208 Y \\ {B} then either y \u2286 X and x 6\u2208 y. In this case, \u3008w,\u03a8(x, y)\u3009 = \u3008w, 0\u3009 = 0. In the remaining cases,\n\u3008w,\u03a8(x, y)\u3009 =\n\u2329\nW \u00b7\n(\neB + 1\n2 e\u2217\n)\n, ey\n\u232a\n= W\n(\n\u3008ey , eB\u3009+ 1\n2 \u3008e\u2217, eB\u3009\n)\n\u2264 W 1\n50 .\nIt follows that\n\u3008w,\u03a8(x,B)\u3009 \u2212 \u3008w,\u03a8(x, y)\u3009 \u2265 24\n25 W \u2265 1 .\nCase 2 (x /\u2208 B): We must show that for every y \u2208 Y \\ {\u2217},\n\u3008w,\u03a8(x, \u2217)\u3009 \u2265 1 + \u3008w,\u03a8(x, y)\u3009 .\nNote that\n\u3008w,\u03a8(x, \u2217)\u3009 =\n\u2329\nW \u00b7\n(\neB + 1\n2 e\u2217\n)\n, e\u2217\n\u232a\n= W\n(\n\u3008eB , e\u2217\u3009+ 1\n2\n)\n\u2265 W\n(\n1 2 \u2212 1 100\n)\n.\nNow, suppose that A = y \u2208 Y \\ {\u2217}. If x /\u2208 A then,\n\u3008w,\u03a8(x, y)\u3009 =\n\u2329\nW \u00b7\n(\neB + 1\n2 e\u2217\n)\n, 0\n\u232a\n= 0 \u2264 1\n25 W .\nIf x \u2208 A then A 6= B. Therefore,\n\u3008w,\u03a8(x, y)\u3009 =\n\u2329\nW \u00b7\n(\neB + 1\n2 e\u2217\n)\n, eA\n\u232a\n= W\n(\n\u3008eB , eA\u3009+ 1\n2 \u3008e\u2217, eA\u3009\n)\n\u2264 1\n25 W .\nIt follows that\n\u3008w,\u03a8(x, \u2217)\u3009 \u2212 \u3008w,\u03a8(x, y)\u3009 \u2265 45\n100 W \u2265 1 ."}, {"heading": "D.8. Proof of Theorem 9", "text": "The first part of the theorem follows directly from the first part of Theorem 6. We proceed to the second part. First, we note that Hd,t,2,R can be realized by Hd,t,q,R. Therefore, it is enough to restrict to the case q = 2. To simplify notations, we denote Hd,t,2,R by Hd,t,R. Also, the label space of Hd,t,R will be {0, 1}\nt instead of [2]q. By Lemma 23 and Lemma 21, it is enough to show that a disjoint union of \u2126(R) copies of HY ,Cantor, with |Y| = \u2126(t), can be realized by Hd,t,R for d \u2265 (t+ 1)R. By Lemma 22, it is enough to show that, for some universal constant C > 0, HY ,Cantor, with |Y| = t+1, can be realized by Ht+1,t,C . Indeed:\nClaim 4 Let Y\u0303 = [t]. The class HY ,Cantor is realized by Ht+1,t,128.\nProof Recall that the instance space of HY ,Cantor is X = 2 [t]. Also, let e\u2217 := et+1 \u2208 B t+1. Consider the mapping \u0393 : X \u2192 (Bt+1)t defined as follows. For every A \u2208 X , \u0393(A) is the matrix whose i\u2019th column is 12ei + 1 4e\u2217 if i \u2208 A and 1 4e\u2217 otherwise. Let \u039b : {0, 1}\nt \u222a {\u2296} \u2192 [t] \u222a {\u2217} be any mapping that maps ei \u2208 {0, 1}\nt to i and 0 \u2208 {0, 1}t to \u2217. To establish the claim we will show that\nHY ,Cantor \u2286 \u039b \u25e6 Ht+1,t,128 \u25e6 \u0393 .\nWe must show that for every i \u2208 [t], hi \u2208 \u039b \u25e6 Ht+1,t,128 \u25e6 \u0393 and that h\u2217 \u2208 \u039b \u25e6 Ht+1,t,128 \u25e6 \u0393. We start with hi. Let W \u2208 M(t+1)\u00d72 be the matrix whose left column is 0 and whose right column is 8ei \u2212 8e\u2217. Let hW \u2208 Ht+1,t,128 be the hypothesis corresponding to W . We claim that hi = \u039b \u25e6 hW \u25e6 \u0393. Indeed, let A \u2208 X . We must show that \u039b(hW (\u0393(A))) = hi(A). By the definition of \u039b and hi, it is enough to show that hW (\u0393(A)) = ei if i \u2208 A, while hW (\u0393(A)) = 0 otherwise. Let \u03a8 : (B\nt+1)t \u00d7 {0, 1}t \u2192 Mt+1,2 be the mapping for which Ht+1,t,128 = H\u03a8,128. Since the left column of W is zero, we have that \u3008W,\u03a8(\u0393(A), 0)\u3009 = 0, and for 0 6= y \u2208 {0, 1}t,\n\u3008W,\u03a8(\u0393(A), y)\u3009 = 1\n2 \u00b7 |{j | yj = 1}|\n\u2211\nj|yj=1\n\u30084ei \u2212 4e\u2217, (\u0393(A)) j\u3009\n= 1\n|{j | yj = 1}|\n\u2211\nj|yj=1\n(2 \u00b7 1[i = j and i \u2208 A]\u2212 1)\n= 2 \u00b7 1[i \u2208 A and yi = 1]\n|{j | yj = 1}| \u2212 1 .\nIt follows that if i \u2208 A then \u3008W,\u03a8(\u0393(A), ei)\u3009 = 1 while \u3008W,\u03a8(\u0393(A), y)\u3009 \u2264 0 for every y 6= ei. Therefore, hW (\u0393(A)) = ei. If i /\u2208 A then \u3008W,\u03a8(\u0393(A), 0)\u3009 = 0 while \u3008W,\u03a8(\u0393(A), y)\u3009 \u2264 \u22121 for every y 6= 0. Therefore hW (\u0393(A)) = 0.\nThe fact that h\u2217 \u2208 \u039b\u25e6Ht+1,t,128\u25e6\u0393 follows from a similar argument, whereW \u2208 M(t+1)\u00d72 is the matrix whose left column is 0 and whose right column is \u22128e\u2217. It is not hard to see that if hW \u2208 Ht+1,t,128 is the hypothesis corresponding to W , we have h\u2217 = \u039b \u25e6 hW \u25e6 \u0393."}, {"heading": "D.9. Proof of Theorem 8", "text": "The first part of the theorem follows directly from the first part of Theorem 5. We proceed to the second part. First, by the following lemma, it is enough to restrict ourselves to the case q = 2. Given two hypothesis classes H \u2286 YX and H\u2032 \u2286 Y \u2032X \u2032 , we say that H\u2032 finitely realizes H if, for every finite U \u2282 X , H\u2032 realizes H|U . It is clear that in this case Gdim (H\u2032) \u2265 Gdim (H).\nLemma 26 For every d, t and q \u2265 2, a disjoint union of \u230a q2\u230b copies of Hd,t,2 is finitely realized by Hd+2,t,q\nProof For simplicity, assume that q is even and let r = q2 . Let X1, . . . ,Xr be finite subsets of Md,t. We should show that there is a mapping \u0393 : X1\u222a\u0307 . . . \u222a\u0307Xr \u2192 Md+2,t and a mapping\n\u039b : [q]t \u2192 [2]t such that\n(Hd,t,2)m |X1\u222a\u0307...\u222a\u0307Xr \u2282 (\u039b \u25e6 Hd+2,t,q \u25e6 \u0393) |X1\u222a\u0307...\u222a\u0307Xr (7)\nFor x \u2208 Xj we define\n\u0393(x) =\n(\nxT , cos\n(\nj 2\u03c0\nr\n)\n, sin\n(\nj 2\u03c0\nr\n))T\nAlso, let \u03bb : [q] \u2192 [2] be the function that maps odd numbers to 1 and even numbers to 2. Finally, define \u039b : [q]t \u2192 [2]t by \u039b(y1, . . . , yt) = (\u03bb(y1), . . . , \u03bb(yt)). We claim that (7) holds with these \u039b and \u0393.\nIndeed, letW1, . . . ,Wr \u2208 Md\u00d72. We should show that the function g \u2208 (Hd,t,2)m |X1\u222a\u0307...\u222a\u0307Xr defined by these function is of the form (\u039b \u25e6 h \u25e6 \u0393) |X1\u222a\u0307...\u222a\u0307Xr for some h \u2208 Hd+2,t,q. Fix M > 0 and let h be the hypothesis defined by the matrix W \u2208 Md+2,q defined as follows\nW =\n\n\nW 11 W 2 1 W 1 2 W 2 2 \u00b7 \u00b7 \u00b7 W 1 r W 2 r\nM cos ( 2\u03c0 r ) M cos ( 2\u03c0 r ) M cos ( 22\u03c0r ) M cos ( 22\u03c0r ) \u00b7 \u00b7 \u00b7 M cos ( r 2\u03c0r ) M cos ( r 2\u03c0r ) M sin (\n2\u03c0 r\n) M sin ( 2\u03c0 r ) M sin ( 22\u03c0r ) M sin ( 22\u03c0r ) \u00b7 \u00b7 \u00b7 M sin ( r 2\u03c0r ) M sin ( r 2\u03c0r )\n\n\nIt is not hard to check that for large enough M , g = (\u039b \u25e6 h \u25e6 \u0393) |X1\u222a\u0307...\u222a\u0307Xr\nNext we prove Theorem 8 for q = 2. To simplify notation, we let Hd,t := Hd,t,2. We make one further reduction, showing that it is enough to prove the theorem for the case d = 3. Indeed, by Lemma 23 and Lemma 21, it is enough to show that a disjoint union of \u2126(d) copies of HY ,Cantor, with |Y| = \u2126(k), can be realized by Hd,t. By Lemma 22, it is enough to show that, for some universal constant C > 0 (we will take C = 3), HY ,Cantor, with |Y| = t+ 1, can be realized by HC,t. Indeed:\nClaim 5 Let Y\u0303 = [t]. The class HY ,Cantor is realized by H3,t.\nProof [(sketch)] The proof is similar to the proof of the second part of Theorem 9. Recall that the instance space ofHY ,Cantor is X = 2 [t]. For i \u2208 [t] define \u03c6(i) = ( cos ( i2\u03c0 t ) , sin ( i2\u03c0 t ) , 0 )\n. Also, let\n\u03c6(\u2217) =\n(\n0, 0, 1\n2 +\n1 2 cos\n(\n2\u03c0\nt\n))\n.\nConsider the mapping \u0393 : X \u2192 (B3)t defined as follows. For every A \u2208 X , \u0393(A) is the matrix whose i\u2019th column is 12\u03c6(i) + 1 2\u03c6(\u2217) if i \u2208 A and 1 2\u03c6(\u2217) otherwise. Let \u039b : {0, 1}\nt \u222a {\u2296} \u2192 [k] \u222a {\u2217} be any mapping that maps ei \u2208 {0, 1}\nt to i and 0 \u2208 {0, 1}t to \u2217. To establish the claim we will show that\nHY ,Cantor \u2286 \u039b \u25e6 H3,t \u25e6 \u0393 .\nWe must show that for every i \u2208 [t], hi \u2208 \u039b \u25e6 H3,t \u25e6 \u0393 and that h\u2217 \u2208 \u039b \u25e6 H3,t \u25e6 \u0393. We start with hi. Let W \u2208 M3\u00d72 be the matrix whose left column is 0 and whose right column is \u03c6(i)\u2212 e3. It is not hard to see that if hW \u2208 H3,t is the hypothesis corresponding to W , we have hi = \u039b \u25e6 hW \u25e6 \u0393.\nFor h\u2217, let W \u2208 M3\u00d72 be the matrix whose left column is 0 and whose right column is \u2212e3. It is not hard to see that for hW \u2208 H3,t, we have h\u2217 = \u039b \u25e6 hW \u25e6 \u0393."}, {"heading": "D.10. Proof of Theorem 27", "text": "Theorem 27 For every \u03a8 : X \u00d7 Y \u2192 Rd, Ndim(H\u03a8) \u2264 d.\nProof Let C \u2286 X be an N -shattered set, and let f0, f1 : C \u2192 Y be two functions that witness the shattering. We must show that |C| \u2264 d. For every x \u2208 C let \u03c1(x) = \u03a8(x, f0(x)) \u2212 \u03a8(x, f1(x)). We claim that \u03c1(C) = {\u03c1(x) | x \u2208 C} consists of |C| elements (i.e. \u03c1 is one to one) and is shattered by the binary hypothesis class of homogeneous linear separators on Rd,\nH = {x 7\u2192 sign(\u3008w, x\u3009) | w \u2208 Rd} .\nSince VCdim(H) = d, it will follow that |C| = |\u03c1(C)| \u2264 d, as required. To establish our claim it is enough to show that |H|\u03c1(C)| = 2\n|C|. Indeed, given a subset B \u2286 C, by the definition of N -shattering, there exists hB \u2208 H\u03a8 for which\n\u2200x \u2208 B,hB(x) = f0(x) and \u2200x \u2208 C \\B,hB(x) = f1(x) .\nIt follows that there exists a vector wB \u2208 R d such that, for every x \u2208 B,\n\u3008w,\u03a8(x, f0(x))\u3009 > \u3008w,\u03a8(x, f1(x))\u3009 \u21d2 \u3008w, \u03c1(x)\u3009 > 0 .\nSimilarly, for every x \u2208 C \\B, \u3008w, \u03c1(x)\u3009 < 0 .\nIt follows that the hypothesis gB \u2208 H defined by w \u2208 R d label the points in \u03c1(B) by 1 and the points in \u03c1(C \\ B) by 0. It follows that if B1, B2 \u2286 C are two different sets then (hB1)|\u03c1(C) 6= (hB2)|\u03c1(C). Therefore |H|C | = 2 |C| as required.\nRemark 28 (Tightness of Theorem 27) Theorem 27 is tight for some functions \u03a8 : X \u00d7 Y \u2192 Rd. For example, consider the case that X = [d], Y = {\u00b11} and \u03a8(x, y) = y \u00b7 ex. It is not hard to see that H\u03a8 = Y\nX . Therefore, Ndim(H\u03a8) = VCdim(H\u03a8) = d. On the other hand, the theorem is not tight for every \u03a8. For example, if |X | < d, then for every \u03a8, Ndim(H\u03a8) \u2264 |X | < d."}], "references": [{"title": "The Probabilistic Method", "author": ["N. Alon", "J.H. Spencer"], "venue": "Wiley-Interscience, second edition,", "citeRegEx": "Alon and Spencer.,? \\Q2000\\E", "shortCiteRegEx": "Alon and Spencer.", "year": 2000}, {"title": "Characterizations of learnability for classes", "author": ["S. Ben-David", "N. Cesa-Bianchi", "D. Haussler", "P. Long"], "venue": "n}-valued functions. Journal of Computer and System Sciences,", "citeRegEx": "Ben.David et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 1995}, {"title": "Discriminative reranking for natural language parsing", "author": ["M. Collins"], "venue": "In Machine Learning,", "citeRegEx": "Collins.,? \\Q2000\\E", "shortCiteRegEx": "Collins.", "year": 2000}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods", "author": ["Michael Collins"], "venue": "In New developments in parsing technology,", "citeRegEx": "Collins.,? \\Q2005\\E", "shortCiteRegEx": "Collins.", "year": 2005}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Crammer and Singer.,? \\Q2001\\E", "shortCiteRegEx": "Crammer and Singer.", "year": 2001}, {"title": "Multiclass learnability and the erm principle", "author": ["A. Daniely", "S. Sabato", "S. Ben-David", "S. Shalev-Shwartz"], "venue": "In COLT,", "citeRegEx": "Daniely et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2011}, {"title": "multiclass learning approaches: A theoretical comparision with implications", "author": ["A. Daniely", "S. Sabato", "S. Shalev-Shwartz"], "venue": "In NIPS,", "citeRegEx": "Daniely et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2012}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Predicting {0, 1}-functions on randomly drawn points", "author": ["David Haussler", "Nick Littlestone", "Manfred K. Warmuth"], "venue": "In FOCS,", "citeRegEx": "Haussler et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1988}, {"title": "Phoneme alignment based on discriminative learning", "author": ["J. Keshet", "S. Shalev-Shwartz", "Y. Singer", "D. Chazan"], "venue": "In Interspeech,", "citeRegEx": "Keshet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Keshet et al\\.", "year": 2005}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Relating data compression and learnability", "author": ["N. Littlestone", "M. Warmuth"], "venue": "Unpublished manuscript,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1986\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1986}, {"title": "Lectures on discrete geometry, volume 212", "author": ["J. Matousek"], "venue": null, "citeRegEx": "Matousek.,? \\Q2002\\E", "shortCiteRegEx": "Matousek.", "year": 2002}, {"title": "On learning sets and functions", "author": ["B.K. Natarajan"], "venue": "Mach. Learn.,", "citeRegEx": "Natarajan.,? \\Q1989\\E", "shortCiteRegEx": "Natarajan.", "year": 1989}, {"title": "Shifting, one-inclusion mistake bounds and tight multiclass expected risk bounds", "author": ["Benjamin I Rubinstein", "Peter L Bartlett", "J Hyam Rubinstein"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rubinstein et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2006}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In NIPS,", "citeRegEx": "Taskar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2010}, {"title": "Let H \u2282 YX be a hypothesis class with the following property: There is a label \u2217 \u2208 Y such that, for every f \u2208 H and x \u2208 X , either f(x) = \u2217 or f is the only function in H whose value at x is f(x)", "author": ["Daniely"], "venue": "Multiclass Learning Lemma", "citeRegEx": "Daniely,? \\Q2011\\E", "shortCiteRegEx": "Daniely", "year": 2011}], "referenceMentions": [{"referenceID": 12, "context": "This brings back the fundmamental question of \u201chow to learn\u201d? We give a complete answer to this question by giving a new analysis of the one-inclusion multiclass learner of Rubinstein et al. (2006) showing that its sample complexity is essentially optimal.", "startOffset": 173, "endOffset": 198}, {"referenceID": 2, "context": "Furthermore, we show that the sample complexity of these learners is better than the sample complexity of the ERM rule, thus settling in negative an open question due to Collins (2005).", "startOffset": 170, "endOffset": 185}, {"referenceID": 14, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.", "startOffset": 94, "endOffset": 188}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.", "startOffset": 94, "endOffset": 188}, {"referenceID": 15, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.", "startOffset": 94, "endOffset": 188}, {"referenceID": 4, "context": "(Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)).", "startOffset": 0, "endOffset": 88}, {"referenceID": 10, "context": "(Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)).", "startOffset": 0, "endOffset": 88}, {"referenceID": 11, "context": "Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others.", "startOffset": 166, "endOffset": 260}, {"referenceID": 8, "context": ", 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others.", "startOffset": 39, "endOffset": 68}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)). Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others. These developments made it possible to handle a variety of multiclass classification problems, including even problems that have a very complex label space, that is structured and exponentially large (e.g. speech recognition, OCR, and multiple object categorization). Despite being very basic and natural, and despite these developments and efforts, our theoretical understanding of multiclass classification is still far from being satisfactory, in particular relatively to our understanding of binary classification (i.e., when |Y| = 2). In this work, we focus on the sample complexity of (distribution free) learning of hypothesis classes H \u2286 YX . The two most fundamental questions are: 1. What is learnable? More quantitatively, what is the sample complexity of a given class H? 2. How to learn? In particular, is there a generic algorithm with optimal sample complexity? For binary classification problems, these two questions are essentially solved (up to logfactors of the error and confidence parameters \u01eb and \u03b4): The fundamental result of Vapnik and Chervonenkis (1971) asserts that the VC dimension characterizes the sample complexity, and that any Empirical Risk Minimization (ERM) algorithm enjoys close-to-optimal sample complexity.", "startOffset": 112, "endOffset": 1714}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)). Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others. These developments made it possible to handle a variety of multiclass classification problems, including even problems that have a very complex label space, that is structured and exponentially large (e.g. speech recognition, OCR, and multiple object categorization). Despite being very basic and natural, and despite these developments and efforts, our theoretical understanding of multiclass classification is still far from being satisfactory, in particular relatively to our understanding of binary classification (i.e., when |Y| = 2). In this work, we focus on the sample complexity of (distribution free) learning of hypothesis classes H \u2286 YX . The two most fundamental questions are: 1. What is learnable? More quantitatively, what is the sample complexity of a given class H? 2. How to learn? In particular, is there a generic algorithm with optimal sample complexity? For binary classification problems, these two questions are essentially solved (up to logfactors of the error and confidence parameters \u01eb and \u03b4): The fundamental result of Vapnik and Chervonenkis (1971) asserts that the VC dimension characterizes the sample complexity, and that any Empirical Risk Minimization (ERM) algorithm enjoys close-to-optimal sample complexity. In a recent surprising result, Daniely et al. (2011) have shown that in multiclass classification there might be substantial gaps between the sample complexity of different ERMs.", "startOffset": 112, "endOffset": 1934}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)). Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others. These developments made it possible to handle a variety of multiclass classification problems, including even problems that have a very complex label space, that is structured and exponentially large (e.g. speech recognition, OCR, and multiple object categorization). Despite being very basic and natural, and despite these developments and efforts, our theoretical understanding of multiclass classification is still far from being satisfactory, in particular relatively to our understanding of binary classification (i.e., when |Y| = 2). In this work, we focus on the sample complexity of (distribution free) learning of hypothesis classes H \u2286 YX . The two most fundamental questions are: 1. What is learnable? More quantitatively, what is the sample complexity of a given class H? 2. How to learn? In particular, is there a generic algorithm with optimal sample complexity? For binary classification problems, these two questions are essentially solved (up to logfactors of the error and confidence parameters \u01eb and \u03b4): The fundamental result of Vapnik and Chervonenkis (1971) asserts that the VC dimension characterizes the sample complexity, and that any Empirical Risk Minimization (ERM) algorithm enjoys close-to-optimal sample complexity. In a recent surprising result, Daniely et al. (2011) have shown that in multiclass classification there might be substantial gaps between the sample complexity of different ERMs. We start by showing an even stronger \u201cpeculiarity\u201d, discriminating binary from multiclass classification. Recall that an algorithm is called improper if it might return a hypothesis that does not belong to the learnt class. Traditionally, improper learning has been applied to enable efficient computations. It seems counter intuitive that computationally unbounded learner would benefit from returning a hypothesis outside of the learnt class. Surprisingly, we show that an optimal learning algorithm must be improper! Namely, we show that there are classes that are learnable only by an improper algorithm. Pointing out that we actually do not understand how to learn optimally, these results \u201creopen\u201d the above two basic questions for multiclass classification. In this paper we essentially resolve these two questions. We give a new analysis of the multiclass one inclusion algorithm (Rubinstein et al. (2006) based on Haussler et al.", "startOffset": 112, "endOffset": 2974}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)). Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others. These developments made it possible to handle a variety of multiclass classification problems, including even problems that have a very complex label space, that is structured and exponentially large (e.g. speech recognition, OCR, and multiple object categorization). Despite being very basic and natural, and despite these developments and efforts, our theoretical understanding of multiclass classification is still far from being satisfactory, in particular relatively to our understanding of binary classification (i.e., when |Y| = 2). In this work, we focus on the sample complexity of (distribution free) learning of hypothesis classes H \u2286 YX . The two most fundamental questions are: 1. What is learnable? More quantitatively, what is the sample complexity of a given class H? 2. How to learn? In particular, is there a generic algorithm with optimal sample complexity? For binary classification problems, these two questions are essentially solved (up to logfactors of the error and confidence parameters \u01eb and \u03b4): The fundamental result of Vapnik and Chervonenkis (1971) asserts that the VC dimension characterizes the sample complexity, and that any Empirical Risk Minimization (ERM) algorithm enjoys close-to-optimal sample complexity. In a recent surprising result, Daniely et al. (2011) have shown that in multiclass classification there might be substantial gaps between the sample complexity of different ERMs. We start by showing an even stronger \u201cpeculiarity\u201d, discriminating binary from multiclass classification. Recall that an algorithm is called improper if it might return a hypothesis that does not belong to the learnt class. Traditionally, improper learning has been applied to enable efficient computations. It seems counter intuitive that computationally unbounded learner would benefit from returning a hypothesis outside of the learnt class. Surprisingly, we show that an optimal learning algorithm must be improper! Namely, we show that there are classes that are learnable only by an improper algorithm. Pointing out that we actually do not understand how to learn optimally, these results \u201creopen\u201d the above two basic questions for multiclass classification. In this paper we essentially resolve these two questions. We give a new analysis of the multiclass one inclusion algorithm (Rubinstein et al. (2006) based on Haussler et al. (1988), see also Simon and Sz\u00f6r\u00e9nyi (2010)), showing that it is optimal up to a constant factor of 2 in a transductive setting.", "startOffset": 112, "endOffset": 3006}, {"referenceID": 1, "context": "Over the years, multiclass classification has been subject to intense study, both theoretical (Natarajan, 1989; Ben-David et al., 1995; Rubinstein et al., 2006; Daniely et al., 2011, 2012) and practical (e.g. (Shalev-Shwartz et al., 2004; Collins, 2005; Keshet et al., 2005; Torralba et al., 2007)). Many methods have been developed to tackle this problem, starting from the the naive one-vs-all method, to more complex methods, such as structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), error correcting output codes (Dietterich and Bakiri, 1995) and others. These developments made it possible to handle a variety of multiclass classification problems, including even problems that have a very complex label space, that is structured and exponentially large (e.g. speech recognition, OCR, and multiple object categorization). Despite being very basic and natural, and despite these developments and efforts, our theoretical understanding of multiclass classification is still far from being satisfactory, in particular relatively to our understanding of binary classification (i.e., when |Y| = 2). In this work, we focus on the sample complexity of (distribution free) learning of hypothesis classes H \u2286 YX . The two most fundamental questions are: 1. What is learnable? More quantitatively, what is the sample complexity of a given class H? 2. How to learn? In particular, is there a generic algorithm with optimal sample complexity? For binary classification problems, these two questions are essentially solved (up to logfactors of the error and confidence parameters \u01eb and \u03b4): The fundamental result of Vapnik and Chervonenkis (1971) asserts that the VC dimension characterizes the sample complexity, and that any Empirical Risk Minimization (ERM) algorithm enjoys close-to-optimal sample complexity. In a recent surprising result, Daniely et al. (2011) have shown that in multiclass classification there might be substantial gaps between the sample complexity of different ERMs. We start by showing an even stronger \u201cpeculiarity\u201d, discriminating binary from multiclass classification. Recall that an algorithm is called improper if it might return a hypothesis that does not belong to the learnt class. Traditionally, improper learning has been applied to enable efficient computations. It seems counter intuitive that computationally unbounded learner would benefit from returning a hypothesis outside of the learnt class. Surprisingly, we show that an optimal learning algorithm must be improper! Namely, we show that there are classes that are learnable only by an improper algorithm. Pointing out that we actually do not understand how to learn optimally, these results \u201creopen\u201d the above two basic questions for multiclass classification. In this paper we essentially resolve these two questions. We give a new analysis of the multiclass one inclusion algorithm (Rubinstein et al. (2006) based on Haussler et al. (1988), see also Simon and Sz\u00f6r\u00e9nyi (2010)), showing that it is optimal up to a constant factor of 2 in a transductive setting.", "startOffset": 112, "endOffset": 3042}, {"referenceID": 5, "context": "We consider classes of multiclass linear classifiers that are learnt by several popular learning paradigms, including multiclass SVM with kernels (Crammer and Singer, 2001), structured output prediction (Collins, 2000, 2002; Lafferty et al.", "startOffset": 146, "endOffset": 172}, {"referenceID": 11, "context": "We consider classes of multiclass linear classifiers that are learnt by several popular learning paradigms, including multiclass SVM with kernels (Crammer and Singer, 2001), structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), and others.", "startOffset": 203, "endOffset": 297}, {"referenceID": 2, "context": "Departing general theory, we turn our focus to investigate hypothesis classes that are used in practice, in light of the above results and the result of Daniely et al. (2011). We consider classes of multiclass linear classifiers that are learnt by several popular learning paradigms, including multiclass SVM with kernels (Crammer and Singer, 2001), structured output prediction (Collins, 2000, 2002; Lafferty et al.", "startOffset": 153, "endOffset": 175}, {"referenceID": 2, "context": "We consider classes of multiclass linear classifiers that are learnt by several popular learning paradigms, including multiclass SVM with kernels (Crammer and Singer, 2001), structured output prediction (Collins, 2000, 2002; Lafferty et al., 2001; Taskar et al., 2003; Tsochantaridis et al., 2004), and others. Arguably, the two most natural questions in this context are: (i) is the ERM rule still sub-optimal even for such classes? and (ii) If yes, are there efficient optimal learnears for these classes? Regarding the first question, we show that even though the sample complexity of these classes is upper bounded in terms of the dimension or the margin, there are sub-optimal ERMs whose sample complexity has additional multiplicative factor that depends on the number of labels. This settles in negative an open question due to Collins (2005). Regarding the second question above, as opposed to the one-inclusion algorithm, which is in general inefficient, for linear classes we derive computationally efficient learners (provided that the hypotheses can be evaluated efficiently), that enjoy optimal sample complexity.", "startOffset": 204, "endOffset": 850}, {"referenceID": 6, "context": "This class is due to Daniely et al. (2011) and we call it the first Cantor class due to the resemblance to the construction used for proving the famous theorem of Cantor from set theory (e.", "startOffset": 21, "endOffset": 43}, {"referenceID": 6, "context": "This class is due to Daniely et al. (2011) and we call it the first Cantor class due to the resemblance to the construction used for proving the famous theorem of Cantor from set theory (e.g., http://en.wikipedia.org/wiki/Cantor\u2019s_theorem). Daniely et al. (2011) employed this class to establish gaps between the sample complexity of different ERM learners.", "startOffset": 21, "endOffset": 263}, {"referenceID": 6, "context": "Since Hd \u2282 HXd,Cantor, we can apply the \u201cgood\u201d ERM learner described in Daniely et al. (2011) with respect to the class HXd,Cantor and obtain an algorithm for Hd whose sample complexity is \u2264 ln(1/\u03b4) \u01eb .", "startOffset": 72, "endOffset": 94}, {"referenceID": 15, "context": "The one-inclusion algorithm We next describe the one-inclusion transductive learning algorithm of Rubinstein et al. (2006). Let S = {x1, .", "startOffset": 98, "endOffset": 123}, {"referenceID": 9, "context": "While the above proof of the upper bound is close in spirit to the arguments used by Haussler et al. (1988) and Rubinstein et al.", "startOffset": 85, "endOffset": 108}, {"referenceID": 9, "context": "While the above proof of the upper bound is close in spirit to the arguments used by Haussler et al. (1988) and Rubinstein et al. (2006), the proof of the lower bound relies on a new argument.", "startOffset": 85, "endOffset": 137}, {"referenceID": 9, "context": "While the above proof of the upper bound is close in spirit to the arguments used by Haussler et al. (1988) and Rubinstein et al. (2006), the proof of the lower bound relies on a new argument. As opposed to Rubinstein et al. (2006) who lower bounded \u01ebH(m) using the Natarajan dimension, we give a direct analysis.", "startOffset": 85, "endOffset": 232}, {"referenceID": 2, "context": "This family is widely used in practice and received a lot of attention in the literature\u2014see for example Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al.", "startOffset": 105, "endOffset": 131}, {"referenceID": 2, "context": "This family is widely used in practice and received a lot of attention in the literature\u2014see for example Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al. (2001); Taskar et al.", "startOffset": 132, "endOffset": 177}, {"referenceID": 2, "context": "This family is widely used in practice and received a lot of attention in the literature\u2014see for example Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al. (2001); Taskar et al. (2003); Tsochantaridis et al.", "startOffset": 132, "endOffset": 199}, {"referenceID": 2, "context": "This family is widely used in practice and received a lot of attention in the literature\u2014see for example Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al. (2001); Taskar et al. (2003); Tsochantaridis et al. (2004). We show that, rather surprisingly, even for such simple classes, there can be gaps between the ERM sample complexity and the PAC sample complexity.", "startOffset": 132, "endOffset": 229}, {"referenceID": 2, "context": "This family is widely used in practice and received a lot of attention in the literature\u2014see for example Crammer and Singer (2001); Collins (2000, 2002); Lafferty et al. (2001); Taskar et al. (2003); Tsochantaridis et al. (2004). We show that, rather surprisingly, even for such simple classes, there can be gaps between the ERM sample complexity and the PAC sample complexity. This settles in negative an open question raised by Collins (2005). We also derive computationally efficient optimal learners for linear classes, based on the concept of compression schemes.", "startOffset": 132, "endOffset": 445}, {"referenceID": 13, "context": "Taskar et al. (2003)).", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "Taskar et al. (2003)). For example, in speech recognition, the label space might me the collection of all sequences of \u2264 20 English words. To motivate the definition, consider the case that we are to recognize a t-letter word appearing in an image. Let q be the size of the alphabet. The set of possible labels is naturally associated with [q]t. A popular method to tackle this task (see for example Taskar et al. (2003)) is the following: The image is broken into t parts, each of which contains a single letter.", "startOffset": 0, "endOffset": 421}, {"referenceID": 2, "context": "Even though the number of labels is exponential in t, this class (in the realizable case) can be learnt in time polynomial in d, t and q (see Collins (2005)).", "startOffset": 142, "endOffset": 157}, {"referenceID": 6, "context": "Daniely et al. (2011)) that the sample complexity of every ERM for this class is O (", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Collins (2005)) that the sample complexity of every ERM for this class is O (", "startOffset": 0, "endOffset": 15}, {"referenceID": 4, "context": "However, it was not known whether the gap is real: In (Collins, 2005), it was left as an open question to show whether the perceptron\u2019s bound holds for every ERM.", "startOffset": 54, "endOffset": 69}, {"referenceID": 6, "context": "This strengthens the result of (Daniely et al., 2011) who showed that it is bounded by O(d log(d)).", "startOffset": 31, "endOffset": 53}, {"referenceID": 6, "context": "This strengthens the result of (Daniely et al., 2011) who showed that it is bounded by O(d log(d)). It is known (e.g. Daniely et al. (2012)) that for the multivector construction (example 1), in which the dimension of the range of \u03a8 is dk, the 1.", "startOffset": 32, "endOffset": 140}, {"referenceID": 6, "context": "These theorems show that the phenomenon of gaps between different ERMs, as reported in (Daniely et al., 2011), happens also in hypothesis classes that are used in practice.", "startOffset": 87, "endOffset": 109}, {"referenceID": 2, "context": "For the case of margin-based classes, this result is not new \u2014 an efficient algorithm based on the multiclass perceptron has been proposed in Collins (2002). For completeness, we briefly survey this approach in the appendix.", "startOffset": 142, "endOffset": 157}, {"referenceID": 9, "context": "Moreover, a beautiful result of Haussler et al. (1988) shows that \u2022 If |Y| = 2, then VCdim(H) \u2264 \u03bcH(m) \u2264 2VCdim(H) for every m \u2265 VCdim(H).", "startOffset": 32, "endOffset": 55}, {"referenceID": 6, "context": "First, it is known (Daniely et al., 2011) that the graph dimension does not characterize the sample complexity, since it can be substantially larger than the sample complexity in several cases.", "startOffset": 19, "endOffset": 41}, {"referenceID": 12, "context": "By combination of theorems 2 and Rubinstein et al. (2006), a weaker version of conjecture 11 is true.", "startOffset": 33, "endOffset": 58}, {"referenceID": 6, "context": "Theorem 15 (Daniely et al. (2011)) For every hypothesis class H with graph dimension d, \u03a9 ( d+ log(1/\u03b4) \u01eb )", "startOffset": 12, "endOffset": 34}, {"referenceID": 14, "context": "Definition 16 (Natarajan dimension) Let H \u2286 (Y \u222a {\u2296}) be a hypothesis class. We say that A \u2286 X is N -shattered if there exist h1, h2 : A \u2192 Y such that \u2200x \u2208 A, h1(x) 6= h2(x) and for every B \u2286 A there is h \u2208 H for which \u2200x \u2208 B, h(x) = h1(x) while \u2200x \u2208 A \\B, h(x) = h2(x) . The Natarajan dimension of H, denoted Ndim(H), is the maximal cardinality of an N shattered set. Theorem 17 (essentially Natarajan (1989)) For every hypothesis class H \u2282 (Y \u222a {\u2296}) with Natarajan dimension d, \u03a9 ( d+ log(1/\u03b4) \u01eb )", "startOffset": 15, "endOffset": 410}, {"referenceID": 6, "context": "(3) We also note that (Daniely et al., 2011) conjectured that the logarithmic factor of |Y| in Theorem 17 can be eliminated (maybe with the expense of poly-logarithmic factors of 1\u01eb , 1 \u03b4 and Ndim(H)).", "startOffset": 22, "endOffset": 44}, {"referenceID": 1, "context": "We note that the upper bound in the last theorem follows from theorem 15 and the fact that (see Ben-David et al. (1995)) for every hypothesis class H, Gdim(H) \u2264 5 log(|Y|)Ndim(H) .", "startOffset": 96, "endOffset": 120}, {"referenceID": 12, "context": "Precisely, we have: Theorem 18 (Littlestone and Warmuth (1986)) Suppose that there exists a compression scheme of size d for a class H.", "startOffset": 32, "endOffset": 63}, {"referenceID": 6, "context": "Lemma 19 (Daniely et al. (2011)) \u2022 The graph dimension of HX ,Cantor is |X |.", "startOffset": 10, "endOffset": 32}, {"referenceID": 6, "context": "Lemma 20 (essentially Daniely et al. (2011)) Let H \u2282 YX be a hypothesis class with the following property: There is a label \u2217 \u2208 Y such that, for every f \u2208 H and x \u2208 X , either f(x) = \u2217 or f is the only function in H whose value at x is f(x).", "startOffset": 22, "endOffset": 44}, {"referenceID": 0, "context": "Alon and Spencer (2000)).", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "Matousek (2002), chapter 13).", "startOffset": 0, "endOffset": 16}], "year": 2014, "abstractText": "The fundamental theorem of statistical learning states that for binary classification problems, any Empirical Risk Minimization (ERM) learning rule has close to optimal sample complexity. In this paper we seek for a generic optimal learner for multiclass prediction. We start by proving a surprising result: a generic optimal multiclass learner must be improper, namely, it must have the ability to output hypotheses which do not belong to the hypothesis class, even though it knows that all the labels are generated by some hypothesis from the class. In particular, no ERM learner is optimal. This brings back the fundmamental question of \u201chow to learn\u201d? We give a complete answer to this question by giving a new analysis of the one-inclusion multiclass learner of Rubinstein et al. (2006) showing that its sample complexity is essentially optimal. Then, we turn to study the popular hypothesis class of generalized linear classifiers. We derive optimal learners that, unlike the one-inclusion algorithm, are computationally efficient. Furthermore, we show that the sample complexity of these learners is better than the sample complexity of the ERM rule, thus settling in negative an open question due to Collins (2005).", "creator": "LaTeX with hyperref package"}}}