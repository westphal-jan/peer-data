{"id": "1405.2590", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2014", "title": "Efficient Computation of the Well-Founded Semantics over Big Data", "abstract": "data originating from the web, sensor readings and social media result in increasingly huge datasets. the so called big case data discipline comes with new scientific and technological challenges while creating lucrative new opportunities, hence the increasing interest in academia and intelligence industry. traditionally, logic programming processing has focused on complex knowledge structures / programs, so the question usually arises whether and how it can work in the specific face of big data. in this paper, we examine how the well - founded semantics can process huge amounts of data through mass parallelization. think more specifically, we propose analyze and evaluate like a parallel approach using the mapreduce comparison framework. already our experimental results indicate confidently that our approach is easily scalable and that well - created founded semantics can be applied to billions of facts. to the best of our knowledge, this subject is the first work that addresses large scale nonmonotonic reasoning without the restriction of stratification for predicates of randomly arbitrary data arity. to appear in theory and practice of logic interaction programming ( tplp ).", "histories": [["v1", "Sun, 11 May 2014 21:57:50 GMT  (77kb,D)", "http://arxiv.org/abs/1405.2590v1", "16 pages, 4 figures, ICLP 2014, 30th International Conference on Logic Programming July 19-22, Vienna, Austria"]], "COMMENTS": "16 pages, 4 figures, ICLP 2014, 30th International Conference on Logic Programming July 19-22, Vienna, Austria", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ilias tachmazidis", "grigoris antoniou", "wolfgang faber"], "accepted": false, "id": "1405.2590"}, "pdf": {"name": "1405.2590.pdf", "metadata": {"source": "CRF", "title": "Efficient Computation of the Well-Founded Semantics over Big Data", "authors": ["Ilias Tachmazidis", "Grigoris Antoniou", "Wolfgang Faber"], "emails": ["ilias.tachmazidis@hud.ac.uk)", "g.antoniou@hud.ac.uk)", "w.faber@hud.ac.uk)"], "sections": [{"heading": null, "text": "KEYWORDS: Well-Founded Semantics, Big Data, MapReduce Framework"}, {"heading": "1 Introduction", "text": "Huge amounts of data are being generated at an increasing pace by sensor networks, government authorities and social media. Such data is heterogeneous, and often needs to be combined with other information, including database and web data, in order to become more useful. This big data challenge is at the core of many contemporary scientific, technological and business developments.\nThe question arises whether the reasoning community, as found in the areas of knowledge representation, rule systems, logic programming and semantic web, can connect to the big data wave. On the one hand, there is a clear application scope, e.g. deriving higher-level knowledge, assisting decision support and data cleaning. But on the other hand, there are significant challenges arising from the area\u2019s traditional focus on rich knowledge structures instead of large amounts of data, and its reliance on in-memory methods. The best approach for enabling reasoning with big data is parallelization, as established e.g. by the LarKC project (Fensel et al. (2008)).\nAs discussed in (Fensel et al. (2008)), reasoning on the large scale can be achieved through parallelization by distributing the computation among nodes. There are mainly two proposed approaches in the literature, namely rule partitioning and data partitioning (Soma and Prasanna (2008)).\nIn the case of rule partitioning, the computation of each rule is assigned to a node in the cluster.\nar X\niv :1\n40 5.\n25 90\nv1 [\ncs .A\nI] 1\n1 M\nThus, the workload for each rule (and node) depends on the structure and the size of the given rule set, which could possibly prevent balanced work distribution and high scalability. On the other hand, for the case of data partitioning, data is divided in chunks with each chunk assigned to a node, allowing more balanced distribution of the computation among nodes.\nParallel reasoning, based on data partitioning, has been studied extensively. In particular, MARVIN (Oren et al. (2009)), follows the divide-conquer-swap strategy in which triples are being swapped between nodes in the cluster in order to achieve balanced workload. MARVIN implements the SpeedDate method, presented in (Kotoulas et al. (2010)), where authors pointed out and addressed the scalability challenge posed by the highly uneven distribution of Semantic Web data.\nWebPIE (Urbani et al. (2012)) implements forward reasoning under RDFS and OWL ter Horst semantics over the MapReduce framework (Dean and Ghemawat (2004)) scaling up to 100 billion triples. In (Goodman et al. (2011)) authors present RDFS inference scaling up to 512 processors with the ability to process, entirely in-memory, 20 billion triples.\nFuzzyPD (Liu et al. (2011, 2012)) is a MapReduce based prototype system allowing fuzzy reasoning in OWL pD\u2217 with scalability of up to 128 process units and over 1 billion triples. Description logic in the form of E L+ have been studied in (Mutharaju et al. (2010)). The authors parallelize an existing algorithm for E L+ classification by converting it into MapReduce algorithms, while experimental evaluation was deferred to future work.\n(Tachmazidis et al. (2012)) deals with defeasible logic for unary predicates scaling up to billions of facts, while authors extend their approach in (Tachmazidis et al. (2012)) for predicates of arbitrary arity, under the assumption of stratification, scaling up to millions of facts. Finally, the computation of stratified semantics of logic programming that can be applied to billions of facts is reported in (Tachmazidis and Antoniou (2013)).\nIn this paper, we propose a parallel approach for the well-founded semantics computation using the MapReduce framework. Specifically, we adapt and incorporate the computation of joins and anti-joins, initially described in (Tachmazidis and Antoniou (2013)). The crucial difference is that in this paper recursion through negation is allowed, meaning that the well-founded model can contain undefined atoms. A challenge in this respect is that materializing the Herbrand base is impractical in the context of big data. To overcome this scalability barrier we require programs to be safe and apply a reasoning procedure that allows closure calculation based on the consecutive computation of true and unknown literals, requiring significantly less information. Experimental results highlight the advantages of the applied optimizations, while showing that our approach can scale up to 1 billion facts even on a modest computational setup.\nThe rest of the paper is organized as follows. Section 2 introduces briefly the MapReduce framework, the well-founded semantics and the alternating fixpoint procedure. Join and anti-join operations for the well-founded semantics are described in Section 3. Section 4 provides a parallel implementation over the MapReduce framework, while experimental results are presented in Section 5. We conclude and discuss future directions in Section 6."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 MapReduce Framework", "text": "MapReduce is a framework for parallel processing over huge datasets (Dean and Ghemawat (2004)). Processing is carried out in a map and a reduce phase. For each phase, a set of user-\ndefined map and reduce functions are run in parallel. The former performs a user-defined operation over an arbitrary part of the input and partitions the data, while the latter performs a user-defined operation on each partition.\nMapReduce is designed to operate over key/value pairs. Specifically, each Map function receives a key/value pair and emits a set of key/value pairs. Subsequently, all key/value pairs produced during the map phase are grouped by their key and passed to reduce phase. During the reduce phase, a Reduce function is called for each unique key, processing the corresponding set of values.\nLet us illustrate the wordcount example. In this example, we take as input a large number of documents and calculate the frequency of each word. The pseudo-code for the Map and Reduce functions is provided in Appendix A.\nConsider the following documents as input:\nDoc1: \u201cHello world.\u201d Doc2: \u201cHello MapReduce.\u201d\nDuring map phase, each map operation gets as input a line of a document. Map function extracts words from each line and emits pairs of the form <w, \u201c1\u201d> meaning that word w occurred once (\u201c1\u201d), namely the following pairs:\n<Hello, 1> <world, 1> <Hello, 1> <MapReduce, 1>\nMapReduce framework will perform grouping/sorting resulting in the following intermediate pairs:\n<Hello, <1,1>> <world, 1> <MapReduce, 1>\nDuring the reduce phase, the Reduce function sums up all occurrence values for each word emitting a pair containing the word and the frequency of the word. Thus, the reducer with key:\nHello will emit <Hello, 2> world will emit <world, 1> MapReduce will emit <MapReduce, 1>"}, {"heading": "2.2 Well-Founded Semantics", "text": "In this section we provide the definition of the well-founded semantics (WFS) as it was defined in (Gelder et al. (1991)).\nDefinition 2.1 (Gelder et al. (1991)) A general logic program is a finite set of general rules, which may have both positive and negative subgoals. A general rule is written with its head, or conclusion on the left, and its subgoal (body), if any to the right of the symbol \u201c\u2190\u201d, which may be read \u201cif\u201d. For example,\np(X)\u2190 a(X), not b(X).\nis a rule in which p(X) is the head, a(X) is a positive subgoal, and b(X) is a negative subgoal. This rule may be read as \u201cp(X) if a(X) and not b(X)\u201d. A Horn rule is one with no negative subgoals, and a Horn logic program is one with only Horn rules.\nWe use the following conventions. A logical variable starts with a capital letter while a constant or a predicate starts with a lowercase letter. Note that functions are not allowed. A predicate of arbitrary arity will be referred as a literal. Constants, variables and literals are terms. A ground term is a term with no variables. The Herbrand universe is the set of constants in a given program. The Herbrand base is the set of ground terms that are produced by the substitution of variables with constants in the Herbrand universe. In this paper, we will refer to Horn rules also as definite rules, likewise Horn programs will also be referred to as definite programs.\nDefinition 2.2 (Gelder et al. (1991)) Given a program P, a partial interpretation I is a consistent set of literals whose atoms are in the Herbrand base of P. A total interpretation is a partial interpretation that contains every atom of the Hebrand base or its negation. We say a ground (variable-free) literal is true in I when it is in I and say it is false in I when its complement is in I. Similarly, we say a conjunction of ground literals is true in I if all of the literals are true in I, and is false in I if any of its literals is false in I.\nDefinition 2.3 (Gelder et al. (1991)) Let a program P, its associated Herbrand base H and a partial interpretation I be given. We say A \u2286 H is an unfounded set (of P) with respect to I if each atom p \u2208 A satisfies the following condition: For each instantiated rule R of P whose head is p, (at least) one of the following holds:\n1. Some (positive or negative) subgoal of the body is false in I. 2. Some positive subgoal of the body occurs in A.\nA literal that makes (1) or (2) above true is called a witness of unusability for rule R (with respect to I).\nTheorem 2.1 (Gelder et al. (1991)) The data complexity of the well-founded semantics for function-free programs is polynomial time.\nIn this paper, we require each rule to be safe, that is, each variable in a rule must occur (also) in a positive subgoal. Safe programs consist of safe rules only. This safety criterion is an adaptation of range restriction (Nicolas (1982)), which guarantees the important concept of domain independence, originally studied in deductive databases (see for example (Abiteboul et al. (1995))). Apart from this semantic property, the safety condition implicitly also enforces a certain locality of computation, which is important for our proposed method, as we shall discuss in Section 4.2."}, {"heading": "2.3 Alternating Fixpoint Procedure", "text": "In this section, we provide the definition of the alternating fixpoint procedure as it was defined in (Brass et al. (2001)).\nDefinition 2.4 (Brass et al. (2001)) For a set S of literals we define the following sets:\npos(S) := {A \u2208 S | A is a positive literal }, neg(S) := {A | not A \u2208 S}.\nDefinition 2.5 (Brass et al. (2001)) (Extended Immediate Consequence Operator) Let P be a normal logic program. Let I and J be sets of ground atoms. The set TP,J(I) of immediate consequences of I w.r.t. P and J is defined as follows:\nTP,J(I) := {A | there is A\u2190B \u2208 ground(P) with pos(B) \u2286 I and neg(B) \u2229 J = /0}.\nIf P is definite, the set J is not needed and we obtain the standard immediate consequence operator TP by TP(I) = TP, /0(I).\nFor an operator T we define T \u2191 0 := /0 and T \u2191 i := T (T \u2191 i\u22121), for i > 0. lfp(T ) denotes the least fixpoint of T , i.e. the smallest set S such that T (S) = S.\nDefinition 2.6 (Brass et al. (2001)) (Alternating Fixpoint Procedure) Let P be a normal logic program. Let P+ denote the subprogram consisting of the definite rules of P. Then the sequence (Ki,Ui)i\u22650 with set Ki of true (known) facts and Ui of possible (unknown) facts is defined by:\nK0 := lfp(TP+) U0 := lfp(TP,K0 ) i > 0 : Ki := lfp(TP,Ui\u22121 ) Ui := lfp(TP,Ki )\nThe computation terminates when the sequence becomes stationary, i.e., when a fixpoint is reached in the sense that (Ki,Ui) = (Ki+1,Ui+1). This computation schema is called the Alternating Fixpoint Procedure (AFP).\nWe rely on the definition of the well-founded partial model W\u2217p of P as given in (Gelder et al. (1991)).\nTheorem 2.2 (Brass et al. (2001)) (Correctness of AFP) Let the sequence (Ki,Ui)i\u22650 be defined as above. Then there is a j \u2265 0 such that (K j,U j)= (K j+1,U j+1). The well-founded model W\u2217p of P can be directly derived from the fixpoint (K j,U j), i.e.,\nW\u2217p = {L | L is a positive ground literal and L \u2208 K j or L is a negative ground literal not A and A \u2208 BASE(P) \u2212 U j},\nwhere BASE(P) is the Herbrand base of program P.\nLemma 2.1 (Brass et al. (2001)) (Monotonicity) Let the sequence (Ki,Ui)i\u22650 be defined as above. Then the following holds for i\u2265 0 : Ki \u2286 Ki+1, Ui \u2287 Ui+1, Ki \u2286 Ui.\n3 Computing TP,J(I)\nConsider the following program:\np(X,Y)\u2190 a(X,Z), b(Z,Y), not c(X,Z), not d(Z,Y).\nHere p(X,Y) is our final goal, a(X,Z) and b(Z,Y) are positive subgoals, while c(X,Z) and d(Z,Y) are negative subgoals. In order to compute our final goal p(X,Y) we need to ensure that {a(X,Z), b(Z,Y)} \u2286 I and {c(X,Z), d(Z,Y)} \u2229 J = /0 (see Definition 2.5), namely both a(X,Z) and b(Z,Y) are in I while none of c(X,Z) and d(Z,Y) is found in J.\nAs positive subgoals depend on I we can group them into a positive goal. A positive goal consists of a new predicate (say ab) that contains as arguments the union of two sets: (a) all the arguments of the final goal (X,Y) and (b) all the common arguments between positive and negative subgoals (X,Z,Y), namely we need to compute ab(X,Z,Y). The final goal (p(X,Y)) consists of all values of the positive goal (ab(X,Z,Y)) that do not match any of the negative subgoals (c(X,Z) and d(Z,Y)) on their common arguments (X,Z and Z,Y respectively)."}, {"heading": "3.1 Positive goal calculation", "text": "Consider the following program:\np(X,Y)\u2190 a(X,Z), b(Z,Y), not c(X,Z), not d(Z,Y).\nwhere\nI = {a(1,2), a(1,3), b(2,4), b(3,5)} J = {c(1,2), d(2,3)}\nA single join (Cluet and Moerkotte (1994)), calculating the positive goal ab(X,Z,Y), can be performed as described below. The pseudo-code for the Map and Reduce functions is provided in Appendix A. Note that we use only literals from I.\nThe Map function will emit pairs of the form <Z,(a,X)> for predicate a and <Z,(b,Y)> for predicate b, namely the following pairs:\n<2, (a,1)> <3, (a,1)> <2, (b,4)> <3, (b,5)>\nMapReduce framework will perform grouping/sorting resulting in the following intermediate pairs:\n<2, <(a,1), (b,4)>> <3, <(a,1), (b,5)>>\nDuring the reduce phase we match predicates a and b on their common argument (which is the key) and use the values to emit positive goals. Thus, the reducer with key:\n2 will emit ab(1,2,4) 3 will emit ab(1,3,5)\nNote that we need to filter out possibly occurring duplicates as soon as possible because they will produce unnecessary duplicates as well, affecting the overall performance. The pseudo-code and a brief description of duplicate elimination are provided in Appendix A.\nFor rules with more than one join between positive subgoals we need to apply multi-joins (multi-way join).\nConsider the following program:\nq(X,Y)\u2190 a(X,Z), b(Z,W), c(W,Y), not d(X,W).\nWe can compute the positive goal (abc(X,W,Y)) by applying our approach for single join twice. First, we need to join a(X,Z) and b(Z,W) on Z, producing a temporary literal (say ab(X,W)), and\nthen join ab(X,W) and c(W,Y) on W producing the positive goal (abc(X,W,Y)). Once abc(X,W,Y) is calculated, we proceed with calculating the final goal q(X,Y) by retaining all the values of abc(X,W,Y) that do not match d(X,W) on their common arguments (X,W).\nFor details on single and multi-way join, readers are referred to literature. More specifically, multi-way join has been described and optimized in (Afrati and Ullman (2010)). In order to achieve an efficient implementation, optimizations in (Afrati and Ullman (2010)) should be taken into consideration."}, {"heading": "3.2 Final goal calculation", "text": "Consider the program mentioned at the beginning of Section 3.1. By calculating the positive goal ab(X,Z,Y) we obtain the following knowledge:\nab(1,2,4) ab(1,3,5)\nIn order to calculate the final goal (p(X,Y)) we need to perform an anti-join (Cluet and Moerkotte (1994)) between ab(X,Z,Y) and each negative subgoal (c(X,Z) and d(Z,Y)). Note that to perform an anti-join we use only the previously calculated positive goal (ab(X,Z,Y)) and literals from J.\nWe start by performing an anti-join between ab(X,Z,Y) and c(X,Z) on their common arguments (X,Z), creating a new literal (say abc(X,Z,Y)), which contains all the results from ab(X,Z,Y) that are not found in c(X,Z), as described below. The pseudo-code for the Map and Reduce functions is provided in Appendix A.\nThe Map function will emit pairs of the form <(X,Z),(ab,Y)> for predicate ab and <(X,Z),c> for predicate c (while predicate d will be taken into consideration during the next anti-join), namely the following pairs:\n<(1,2), (ab,4)> <(1,3), (ab,5)> <(1,2), c>\nMapReduce framework will perform grouping/sorting resulting in the following intermediate pairs:\n<(1,2), <(ab,4), (c)>> <(1,3), (ab,5)>\nDuring the reduce phase we output values of the predicate ab only if it is not matched by predicate c on their common arguments (which are contained in the key) and emit abc(X,Z,Y). Thus, the reducer with key:\n(1,2) will have no output (1,3) will emit abc(1,3,5)\nIn order to calculate the final goal (p(X,Y)), we need to perform an additional anti-join between abc(X,Z,Y) and d(Z,Y) on their common arguments (Z,Y). Here, abc(1,3,5) and d(2,3) do not match on their common arguments (Z,Y) as (3,5) 6= (2,3). Thus, our calculated final goal is p(1,5)."}, {"heading": "4 Computing the Well-Founded Semantics", "text": "In this section we describe an optimized implementation for the calculation of the well-founded semantics. A naive implementation is considered as one following Definition 2.6 while ignoring the monotonicity properties of the well-founded semantics (see Lemma 2.1)."}, {"heading": "4.1 Optimized implementation", "text": "A naive implementation would introduce unnecessary overhead to the overall computation since it comes with the overhead of reasoning over and storage of overlapping sets of knowledge. A more refined version of both WFS fixpoint and least fixpoint of TP,J(I) is defined in Algorithm 1 and Algorithm 2 respectively.\nAlgorithm 1 Optimized WFS fixpoint opt WFS fixpoint(P): . input: program P\n1: K0 = opt lfp(P+, /0, /0); . output: set of literals Ki\u22121, Ui\u22121 2: i = 0; 3: repeat 4: Ui = Ki \u222a opt lfp(P, Ki, Ki); 5: i++; . next \u201cinference step\u201d 6: Ki = Ki\u22121 \u222a opt lfp(P, Ki\u22121, Ui\u22121); 7: until Ki\u22121.size() == Ki.size() 8: return Ki\u22121, Ui\u22121;\nAlgorithm 2 Optimized least fixpoint of TP,J(I) opt lfp(P, I, J): . precondition: I \u2286 lfp(TP,J( /0))\n1: S = /0; . input: program P, set of literals I and J 2: new = /0; . output: set of literals S (lfp(TP,J(I) - I) 3: repeat 4: S = S \u222a new; 5: new = T(P, (I \u222a S), J); 6: new = new - (I \u222a S); 7: until new == /0 8: return S;\nOur first optimization is the changed calculation of the least fixpoint of TP,J(I) (opt lfp), which is depicted in Algorithm 2. Instead of calculating the least fixpoint starting from I = /0, for a given program P and a set of literals J, we allow the calculation to start from a given I, provided that I \u2286 lfp(TP,J( /0)), and return only the newly inferred literals (S) that led us to the least fixpoint. Thus, the actual set of literals that the least fixpoint of TP,J(I) consists of is I \u222a S. In order to reassure correctness we need to take into consideration both I and S while calculating the least fixpoint, namely new literals are inferred by calculating TP,J(I \u222a S). However, we use a temporary set of inferred literals (new) in order to eliminate duplicates (new = new \u2212 (I \u222a S)) prior to adding newly inferred literals to the set S (S = S \u222a new). Note that the set of literals I remains unchanged when the optimized least fixpoint is calculated.\nThe optimized version of the least fixpoint is used, in Algorithm 1, for the computation of each set of literals K and U. K0 is a special case where we start from I = /0 and J = /0, and thus, unable to fully utilize the advantages of the optimized least fixpoint.\nThe proposed optimizations are mainly based on the monotonicity of the well-founded semantics as given in Lemma 2.1. Note that in this section, the indices of the sets K and U found in Lemma 2.1 are adjusted to the indices used in Algorithm 1 in order to facilitate our discussion.\nSince Ki \u2286 Ui, for i\u2265 0 (see Lemma 2.1), the computation of Ui can start from Ki, namely I = Ki. Thus, instead of recomputing all literals of Ki while calculating Ui, we can use them to speed up the process. Note that the actual least fixpoint of Ui is the union of sets Ki and opt lfp(P, Ki, Ki), as the optimized least fixpoint computes only new literals (which are not included in given I).\nSince Ki\u22121 \u2286 Ki, for i \u2265 1 (see Lemma 2.1), the computation of Ki can start from Ki\u22121, namely I = Ki\u22121. Once opt lfp(P, Ki\u22121, Ui\u22121) is computed, we append it to our previously stored knowledge Ki\u22121, resulting in Ki. In addition, a WFS fixpoint is reached when Ki\u22121 = Ki, namely when Ki\u22121 and Ki have the same number of literals.\nProof If Ki\u22121 = Ki, for i\u2265 1, then Ui\u22121 = Ki\u22121 \u222a opt lfp(P, Ki\u22121, Ki\u22121) = Ki \u222a opt lfp(P, Ki, Ki) = Ui Thus, fixpoint is reached as (Ki\u22121,Ui\u22121) =(Ki,Ui).\nAccording to Theorem 2.2, having reached WFS fixpoint at step i, we can determine which literals are true, undefined and false as follows: (a) true literals, denoted by Ki, (b) undefined literals, denoted by Ui \u2212 Ki and (c) false literals, denoted by BASE(P) \u2212 Ui.\nAlthough for Ki calculation only new literals are inferred during each \u201cinference step\u201d, for Ui we have to recalculate a subset of literals that can be found in Ui\u22121, as literals in Ui\u22121 \u2212 Ki\u22121 are discarded prior to the computation of Ui. However, the computational overhead coming from the calculation of opt lfp(P, Ki, Ki) reduces over time since the set of literals in Ui \u2212 Ki becomes smaller after each \u201cinference step\u201d due to Ki\u22121 \u2286 Ki and Ui\u22121 \u2287 Ui, for i\u2265 1, (see Lemma 2.1).\nWe may further optimize our approach by minimizing the amount of stored literals. A naive implementation would require the storage of up to four overlapping sets of literals (Ki\u22121, Ui\u22121, Ki, Ui). However, as Ki \u2286 Ui, while calculating Ui, we need to store in our knowledge base only the sets Ki and opt lfp(P, Ki, Ki), since Ui = Ki \u222a opt lfp(P, Ki, Ki).\nAs Ki\u22121 \u2286 Ki, for the calculation of Ki, we need to store in our knowledge base only three sets of literals, namely: (a) Ki\u22121, (b) Ui\u22121 \u2212 Ki\u22121 = opt lfp(P, Ki\u22121, Ki\u22121) and (c) currently calculating least fixpoint opt lfp(P, Ki\u22121, Ui\u22121). All newly inferred literals in opt lfp(P, Ki\u22121, Ui\u22121), are added to Ki (replacing our prior knowledge about Ki\u22121), while literals in Ui\u22121 - Ki\u22121 = opt lfp(P, Ki\u22121, Ki\u22121) are deleted, if fixpoint is not reached, as they cannot be used for the computation of Ui.\nA WFS fixpoint is reached when Ki\u22121 = Ki, namely when no new literals are derived during the calculation of Ki, which practically is the calculation of opt lfp(P, Ki\u22121, Ui\u22121). Since (Ki\u22121,Ui\u22121) = (Ki,Ui), we return the sets of literals Ki\u22121 and Ui\u22121, representing our fixpoint knowledge base.\nIn practice, the maximum amount of stored data occurs while calculating Ki, for i\u2265 1, where we need to store three sets of literals, namely: (a) Ki\u22121, (b) Ui\u22121 \u2212 Ki\u22121 and (c) opt lfp(P, Ki\u22121, Ui\u22121), requiring significantly less storage space compared to the naive implementation."}, {"heading": "4.2 Computational Impact of Safety", "text": "In this paper, we follow the alternating fixpoint procedure, over safe WFS programs, in order to avoid full materialization of or reasoning over the Herbrand base for any predicate. Storing or performing reasoning over the entire Herbrand base may easily become prohibiting even for small datasets, and thus, not applicable to big data.\nApart from the semantic motivation of the safety requirement outlined in Section 2.2, it also\nhas considerable impact on the computational method followed in this paper. Recall that safety requires that each variable in a rule must occur (also) in a positive subgoal. If this safety condition is not met, an anti-join is no longer a single lookup between the positive goal and a negative subgoal, but a comparison between a subset of the Herbrand base and a given set of literals J. An efficient implementation for such computation is yet to be defined and problematic, as illustrated next.\nConsider the following program:\np(X,Y)\u2190 a(X,Y), not b(Y,Z). q(X,Y)\u2190 c(X,U), not d(W,U), not e(U,Y).\nFor the first rule, each (X,Y) in a(X,Y) is included in the final goal (p(X,Y)) only if for a given Y, there is a Z the in Herbrand universe such that b(Y,Z) does not belong to J. For the second rule, for each (X,Y) that is included in the final goal (q(X,Y)) there should be a literal c(X,U) that does not match neither d(W,U) on U, for any W in Herbrand universe, nor e(U,Y) on U, for any Y in Herbrand universe. Thus, we need to perform reasoning over a subset of the Herbrand base for b(Y,Z), d(W,U) and e(U,Y) in order to find the nonmatching literals."}, {"heading": "5 Experimental results", "text": "Methodology. In order to evaluate our approach, we surveyed available benchmarks in the literature. In (Liang et al. (2009)), the authors evaluate the performance of several rule engines on data that fit in main memory. However, our approach is targeted on data that exceed the capacity of the main memory. Thus, we follow the proposed methodology in (Liang et al. (2009)) while adjusting several parameters. In (Liang et al. (2009)) loading and inference time are separated, focusing on inference time. However, for our approach such a separation is difficult as loading and inference time may overlap.\nWe evaluate our approach considering default negation by applying the win-not-win test and merge large (anti-)join tests with datalog recursion and default negation, creating a new test called transitive closure with negation. Other metrics in (Liang et al. (2009)), such as indexing, are not supported by the MapReduce framework, while all optimizations and cost-based analysis were performed manually.\nPlatform. We have implemented our experiments using the Hadoop MapReduce framework1, version 1.2.1. We have performed experiments on a cluster of the University of Huddersfield. The cluster consists of 8 nodes (one node was allocated as \u201cmaster\u201d node), using a Gigabit Ethernet interconnect. Each node was equipped with 4 cores running at 2.5GHz, 8GB RAM and 250GB of storage space.\nEvaluation tests. The win-not-win test (Liang et al. (2009)) consists of a single rule, where move is the base relation:\nwin(X)\u2190 move(X,Y), not win(Y).\nWe test the following data distributions:\n\u2022 the base facts form a cycle: {move(1,2), ..., move(i, i+1), ..., move(n-1,n), move(n,1)}. \u2022 the data is tree-structured: {move(i, 2*i), move(i, 2*i+1) | 1 \u2264 i \u2264 n}.\n1 http://hadoop.apache.org/mapreduce/\nWe used four cyclic datasets and four tree-structured datasets with 125M, 250M, 500M and 1000M facts.\nThe transitive closure with negation test consists of the following rule set, where b is the base relation:\ntc(X,Y)\u2190 par(X,Y). par(X,Y)\u2190 b(X,Y), not q(X,Y). tc(X,Y)\u2190 par(X,Z), tc(Z,Y). par(X,Y)\u2190 b(X,Y), b(Y,Z), not q(Y,Z). q(X,Y)\u2190 b(Z,X), b(X,Y), not q(Z,X).\nWe test the following data distribution:\n\u2022 the base facts are chain-structured: {b(i, i+k) | 1 \u2264 i \u2264 n, k < n}. Intuitively, the i values are distributed over dn/ke levels, allowing dn/ke \u2212 1 joins in the formed chain.\nThe transitive closure with negation test allows for comparing the performance of the naive and the optimized WFS fixpoint calculation when the computation of lfp(TP,J(I)) starts from I = /0 and I 6= /0 respectively. For Ui and Ki+1, for i \u2265 0, the optimized implementation speeds up the process by using, as input, the previously computed transitive closure of Ki, while the naive implementation comes with the overhead of recomputing previously inferred literals. Intuitively, this test allows the subsequent computation of transitive closure that becomes larger after each \u201cinference step\u201d.\nWe used four chain-structured datasets for increasing number of joins in the initially formed chain (dn/ke \u2212 1) with n = 125M, and k = 41.7M, 25M, 13.9M and 7.36M, and four chainstructured datasets for a constant number of joins in the initially formed chain (dn/ke \u2212 1) with n = 62.5M, 125M, 250M and 500M, and k = 12.5M, 25M, 50M and 100M respectively.\nResults. We can identify four main factors that affect the performance of our approach: (a) number of facts, affecting the input size, (b) number of rules, affecting the output size, (c) data distribution, affecting the number of required MapReduce jobs, and (d) rule set structure, affecting the number of required MapReduce jobs.\nFigure 1 presents the runtimes of our system for the win-not-win test over cyclic datasets with input sizes up to 1 billion facts. In this case, our system scales linearly with respect to both dataset size and number of nodes. This is attributed to the fact that the runtime per MapReduce job scales linearly for increasing data sizes, while the number of jobs remains constant.\nFigure 2 shows the runtimes of our system for the win-not-win test over tree-structured datasets with input sizes up to 1 billion facts. Our approach scales linearly for increasing data sizes and number of nodes.\nFigure 3 depicts the scaling properties of our system for the transitive closure with negation test over chain-structured datasets, when run on 7 nodes. Practically, transitive closure depends on the number of joins in the initially formed chain, which are equal to dn/ke \u2212 1, namely 2, 4, 8 and 16, and thus, appropriate for scalability evaluation. The length of the chain affects both the size of the transitive closure and the number of \u201cinference steps\u201d, leading to polynomial complexity. Note that our results are in line with Theorem 2.1. Finally, the speedup of the optimized over the naive implementation is higher for longer chains, since the naive implementation has to recompute larger transitive closures.\nFigure 4 illustrates the scalability properties of our system for the transitive closure with negation test over chain-structured datasets for constant number of joins in the initially formed chain, when run on 7 nodes. Our approach scales linearly, both for naive and optimized implementation as the number of jobs remains constant, while the runtime per job scales linearly for increasing number of facts."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we studied the feasibility of computing the well-founded semantics, while allowing recursion through negation, over large amounts of data. In particular, we proposed a parallel approach based on the MapReduce framework, ran experiments for various rule sets and data sizes, and showed the performance speedup coming from the optimized implementation when compared to a naive implementation. Our experimental results indicate that this method can be applied to billions of facts.\nIn future work, we plan to study more complex knowledge representation methods including Answer-Set programming (Gelfond (2008)), and RDF/S ontology evolution (Konstantinidis et al. (2008)) and repair (Roussakis et al. (2011)). We believe that these complex forms of reasoning do not fall under the category of \u201cembarrassingly parallel\u201d problems for which MapReduce is designed, and thus, a more complex computational model is required. Parallelization techniques such as OpenMP2 and Message Passing Interface (MPI) may provide higher degree of flexibility than the MapReduce framework, giving the opportunity to overcome arising limitations. In fact, in Answer-Set programming, the system claspar (Gebser et al. (2011)) uses MPI, but it needs a preliminary grounding step, as it accepts only ground or propositional programs. (Perri et al. (2013)) uses POSIX threads on shared memory for parallelized grounding. Combining these two approaches and making them more data-driven would be an interesting challenge.\n2 http://openmp.org/wp/"}, {"heading": "Appendix A MapReduce algorithms", "text": "In the appendix, we include the algorithms that are used in the running examples of this paper. More specifically, Algorithm 3 refers to the wordcount example in Section 2.1.\nAlgorithm 3 Wordcount example map(Long key, String value): . key: position in document\n1: for all word w \u2208 value do . value: document line 2: emit(w, \u201c1\u201d); 3: end for\nreduce(String key, Iterator values): . key: a word 4: int count = 0; . values: list of counts 5: for all value \u2208 values do 6: count += parseInt(value); 7: end for 8: emit(key, count);\nIn Section 3.1 we described the calculation of the positive goal by applying a single join following Algorithm 4.\nAlgorithm 4 Single join map(Long key, String value): . key: position in document (irrelevant)\n1: if value.predicate == \u201ca\u201d then . value: document line (literal in I) 2: emit(value.Z,{value.predicate,value.X}); 3: else if value.predicate == \u201cb\u201d then 4: emit(value.Z,{value.predicate,value.Y}); 5: end if\nreduce(String key, Iterator values): . key: matching argument 6: List a List = /0, b List = /0; . values: literals in I for matching 7: for all value \u2208 values do 8: if value.predicate == \u201ca\u201d then 9: a List.add(value.X);\n10: else if value.predicate == \u201cb\u201d then 11: b List.add(value.Y); 12: end if 13: end for 14: for all a \u2208 a List do 15: for all b \u2208 b List do 16: emit(\u201cab(a.X,key.Z,b.Y)\u201d,\u201c\u201d); 17: end for 18: end for\nAlthough we mentioned, in Section 3.1, that duplicate elimination should take place as soon as\npossible in order to minimize overhead, the description of the algorithm was deferred to this appendix. Duplicate elimination can be performed as described in Algorithm 5. Practically, the Map function emits every inferred literal as the key, with an empty value. The MapReduce framework performs grouping/sorting resulting in one group (of duplicates) for each unique literal. Each group of duplicates consists of the unique literal as the key and a set of empty values (with values being eventually ignored). The actual duplicate elimination takes place during the reduce phase since for each group of duplicates, we emit the (unique) inferred literal once, using the key, while ignoring the values.\nAlgorithm 5 Duplicate elimination map(Long key, String value): . key: position in document (irrelevant)\n1: emit(value, \u201c\u201d); . value: document line (inferred literal)\nreduce(String key, Iterator values): . key: inferred literal 2: emit(key, \u201c\u201d); . values: empty values (not used)\nFinally, the calculation of the final goal as described in Section 3.2 follows Algorithm 6.\nAlgorithm 6 Anti-join map(Long key, String value): . key: position in document (irrelevant)\n1: if value.predicate == \u201cab\u201d then . value: document line (literal) 2: emit({value.X,value.Z},{value.predicate,value.Y}); 3: else if value.predicate == \u201cc\u201d then 4: emit({value.X,value.Z},value.predicate); 5: end if\nreduce(String key, Iterator values): . key: matching argument 6: List ab List = /0; . values: literals for matching 7: for all value \u2208 values do 8: if value.predicate == \u201cab\u201d then 9: ab List.add(value.Y); 10: else if value.predicate == \u201cc\u201d then 11: return ; . matched by predicate c 12: end if 13: end for 14: for all ab \u2208 ab List do 15: emit(\u201cabc(key.X,key.Z,ab.Y)\u201d,\u201c\u201d); 16: end for"}], "references": [{"title": "Foundations of Databases", "author": ["S. ABITEBOUL", "R. HULL", "V. VIANU"], "venue": "Addison-Wesley. AFRATI, F. N. AND ULLMAN, J. D. 2010. Optimizing joins in a map-reduce environment. In", "citeRegEx": "ABITEBOUL et al\\.,? 1995", "shortCiteRegEx": "ABITEBOUL et al\\.", "year": 1995}, {"title": "Transformation-based bottom-up computation of the well-founded model", "author": ["S. BRASS", "J. DIX", "B. FREITAG", "U. ZUKOWSKI"], "venue": "Theory and Practice of Logic Programming 1, 5, 497\u2013538.", "citeRegEx": "BRASS et al\\.,? 2001", "shortCiteRegEx": "BRASS et al\\.", "year": 2001}, {"title": "Classification and optimization of nested queries in object bases", "author": ["S. CLUET", "G. MOERKOTTE"], "venue": "Tech. rep.", "citeRegEx": "CLUET and MOERKOTTE,? 1994", "shortCiteRegEx": "CLUET and MOERKOTTE", "year": 1994}, {"title": "MapReduce: simplified data processing on large clusters", "author": ["J. DEAN", "S. GHEMAWAT"], "venue": "Proceedings of the 6th conference on Symposium on Opearting Systems Design & Implementation - Volume 6. USENIX Association, Berkeley, CA, USA, 10\u201310.", "citeRegEx": "DEAN and GHEMAWAT,? 2004", "shortCiteRegEx": "DEAN and GHEMAWAT", "year": 2004}, {"title": "Towards LarKC: A Platform for Web-Scale Reasoning", "author": ["D. FENSEL", "F. VAN HARMELEN", "B. ANDERSSON", "P. BRENNAN", "H. CUNNINGHAM", "E.D. VALLE", "F. FISCHER", "Z. HUANG", "A. KIRYAKOV", "T.K. IL LEE", "L. SCHOOLER", "V. TRESP", "S. WESNER", "M. WITBROCK", "N. ZHONG"], "venue": "ICSC. 524\u2013529.", "citeRegEx": "FENSEL et al\\.,? 2008", "shortCiteRegEx": "FENSEL et al\\.", "year": 2008}, {"title": "Clusterbased asp solving with claspar", "author": ["M. GEBSER", "R. KAMINSKI", "B. KAUFMANN", "T. SCHAUB", "B. SCHNOR"], "venue": "Logic Programming and Nonmonotonic Reasoning - 11th International Conference, LPNMR 2011, Vancouver, Canada, May 16-19, 2011. Proceedings, J. P. Delgrande and W. Faber, Eds. Lecture Notes in Computer Science, vol. 6645. 364\u2013369.", "citeRegEx": "GEBSER et al\\.,? 2011", "shortCiteRegEx": "GEBSER et al\\.", "year": 2011}, {"title": "The well-founded semantics for general logic programs", "author": ["A.V. GELDER", "K.A. ROSS", "J.S. SCHLIPF"], "venue": "J. ACM 38, 3, 620\u2013650.", "citeRegEx": "GELDER et al\\.,? 1991", "shortCiteRegEx": "GELDER et al\\.", "year": 1991}, {"title": "Chapter 7 answer sets", "author": ["M. GELFOND"], "venue": "Handbook of Knowledge Representation, V. L. F. van Harmelen and B. Porter, Eds. Foundations of Artificial Intelligence, vol. 3. Elsevier, 285\u2013316.", "citeRegEx": "GELFOND,? 2008", "shortCiteRegEx": "GELFOND", "year": 2008}, {"title": "High-performance computing applied to semantic databases", "author": ["E.L. GOODMAN", "E. JIMENEZ", "D. MIZELL", "S. AL-SAFFAR", "B. ADOLF", "D.J. HAGLIN"], "venue": "ESWC (2), G. Antoniou, M. Grobelnik, E. P. B. Simperl, B. Parsia, D. Plexousakis, P. D. Leenheer, and J. Z. Pan, Eds. Lecture Notes in Computer Science, vol. 6644. Springer, 31\u201345.", "citeRegEx": "GOODMAN et al\\.,? 2011", "shortCiteRegEx": "GOODMAN et al\\.", "year": 2011}, {"title": "A Formal Approach for RDF/S Ontology Evolution", "author": ["G. KONSTANTINIDIS", "G. FLOURIS", "G. ANTONIOU", "V. CHRISTOPHIDES"], "venue": "ECAI, M. Ghallab, C. D. Spyropoulos, N. Fakotakis, and N. M. Avouris, Eds. Frontiers in Artificial Intelligence and Applications, vol. 178. IOS Press, 70\u201374.", "citeRegEx": "KONSTANTINIDIS et al\\.,? 2008", "shortCiteRegEx": "KONSTANTINIDIS et al\\.", "year": 2008}, {"title": "Mind the data skew: distributed inferencing by speeddating in elastic regions", "author": ["S. KOTOULAS", "E. OREN", "F. VAN HARMELEN"], "venue": "WWW, M. Rappa, P. Jones, J. Freire, and S. Chakrabarti, Eds. ACM, 531\u2013540.", "citeRegEx": "KOTOULAS et al\\.,? 2010", "shortCiteRegEx": "KOTOULAS et al\\.", "year": 2010}, {"title": "Openrulebench: an analysis of the performance of rule engines", "author": ["S. LIANG", "P. FODOR", "H. WAN", "M. KIFER"], "venue": "Proceedings of the 18th international conference on World wide web. WWW \u201909. ACM, New York, NY, USA, 601\u2013610.", "citeRegEx": "LIANG et al\\.,? 2009", "shortCiteRegEx": "LIANG et al\\.", "year": 2009}, {"title": "Large scale fuzzy pD* reasoning using mapreduce", "author": ["LIU C.", "QI G.", "WANG H.", "YU", "Y."], "venue": "Proceedings of the 10th international conference on The semantic web - Volume Part I. ISWC\u201911. Springer-Verlag, Berlin, Heidelberg, 405\u2013420.", "citeRegEx": "C. et al\\.,? 2011", "shortCiteRegEx": "C. et al\\.", "year": 2011}, {"title": "Reasoning with Large Scale Ontologies in fuzzy pD* Using MapReduce", "author": ["LIU C.", "QI G.", "WANG H.", "YU", "Y."], "venue": "IEEE Comp. Int. Mag. 7, 2, 54\u201366.", "citeRegEx": "C. et al\\.,? 2012", "shortCiteRegEx": "C. et al\\.", "year": 2012}, {"title": "A MapReduce Algorithm for EL+", "author": ["R. MUTHARAJU", "F. MAIER", "P. HITZLER"], "venue": "Description Logics.", "citeRegEx": "MUTHARAJU et al\\.,? 2010", "shortCiteRegEx": "MUTHARAJU et al\\.", "year": 2010}, {"title": "Logic for improving integrity checking in relational data bases", "author": ["NICOLAS", "J.-M."], "venue": "Acta Informatica 18, 227\u2013253.", "citeRegEx": "NICOLAS and J..M.,? 1982", "shortCiteRegEx": "NICOLAS and J..M.", "year": 1982}, {"title": "Marvin: Distributed reasoning over large-scale Semantic Web data", "author": ["E. OREN", "S. KOTOULAS", "G. ANADIOTIS", "R. SIEBES", "A. TEN TEIJE", "F. VAN HARMELEN"], "venue": "J. Web Sem. 7, 4, 305\u2013316.", "citeRegEx": "OREN et al\\.,? 2009", "shortCiteRegEx": "OREN et al\\.", "year": 2009}, {"title": "Parallel instantiation of asp programs: techniques and experiments", "author": ["S. PERRI", "F. RICCA", "M. SIRIANNI"], "venue": "Theory and Practice of Logic Programming 13, 2, 253\u2013278.", "citeRegEx": "PERRI et al\\.,? 2013", "shortCiteRegEx": "PERRI et al\\.", "year": 2013}, {"title": "Declarative Repairing Policies for Curated KBs", "author": ["Y. ROUSSAKIS", "G. FLOURIS", "V. CHRISTOPHIDES"], "venue": "HDMS.", "citeRegEx": "ROUSSAKIS et al\\.,? 2011", "shortCiteRegEx": "ROUSSAKIS et al\\.", "year": 2011}, {"title": "Parallel Inferencing for OWL Knowledge Bases", "author": ["R. SOMA", "V.K. PRASANNA"], "venue": "ICPP. IEEE Computer Society, 75\u201382.", "citeRegEx": "SOMA and PRASANNA,? 2008", "shortCiteRegEx": "SOMA and PRASANNA", "year": 2008}, {"title": "Computing the Stratified Semantics of Logic Programs over Big Data through Mass Parallelization", "author": ["I. TACHMAZIDIS", "G. ANTONIOU"], "venue": "RuleML, L. Morgenstern, P. S. Stefaneas, F. L\u00e9vy, A. Wyner, and A. Paschke, Eds. Lecture Notes in Computer Science, vol. 8035. Springer, 188\u2013202.", "citeRegEx": "TACHMAZIDIS and ANTONIOU,? 2013", "shortCiteRegEx": "TACHMAZIDIS and ANTONIOU", "year": 2013}, {"title": "Towards Parallel Nonmonotonic Reasoning with Billions of Facts", "author": ["I. TACHMAZIDIS", "G. ANTONIOU", "G. FLOURIS", "S. KOTOULAS"], "venue": "KR, G. Brewka, T. Eiter, and S. A. McIlraith, Eds. AAAI Press.", "citeRegEx": "TACHMAZIDIS et al\\.,? 2012", "shortCiteRegEx": "TACHMAZIDIS et al\\.", "year": 2012}, {"title": "Large-scale Parallel Stratified Defeasible Reasoning", "author": ["I. TACHMAZIDIS", "G. ANTONIOU", "G. FLOURIS", "S. KOTOULAS", "L. MCCLUSKEY"], "venue": "ECAI, L. D. Raedt, C. Bessi\u00e8re, D. Dubois, P. Doherty, P. Frasconi, F. Heintz, and P. J. F. Lucas, Eds. Frontiers in Artificial Intelligence and Applications, vol. 242. IOS Press, 738\u2013743.", "citeRegEx": "TACHMAZIDIS et al\\.,? 2012", "shortCiteRegEx": "TACHMAZIDIS et al\\.", "year": 2012}, {"title": "Webpie: A Web-scale parallel inference engine using MapReduce", "author": ["J. URBANI", "S. KOTOULAS", "J. MASSEN", "F. VAN HARMELEN", "BAL", "H."], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web 10, 0.", "citeRegEx": "URBANI et al\\.,? 2012", "shortCiteRegEx": "URBANI et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "by the LarKC project (Fensel et al. (2008)).", "startOffset": 22, "endOffset": 43}, {"referenceID": 4, "context": "by the LarKC project (Fensel et al. (2008)). As discussed in (Fensel et al. (2008)), reasoning on the large scale can be achieved through parallelization by distributing the computation among nodes.", "startOffset": 22, "endOffset": 83}, {"referenceID": 4, "context": "by the LarKC project (Fensel et al. (2008)). As discussed in (Fensel et al. (2008)), reasoning on the large scale can be achieved through parallelization by distributing the computation among nodes. There are mainly two proposed approaches in the literature, namely rule partitioning and data partitioning (Soma and Prasanna (2008)).", "startOffset": 22, "endOffset": 332}, {"referenceID": 12, "context": "In particular, MARVIN (Oren et al. (2009)), follows the divide-conquer-swap strategy in which triples are being swapped between nodes in the cluster in order to achieve balanced workload.", "startOffset": 23, "endOffset": 42}, {"referenceID": 8, "context": "MARVIN implements the SpeedDate method, presented in (Kotoulas et al. (2010)), where authors pointed out and addressed the scalability challenge posed by the highly uneven distribution of Semantic Web data.", "startOffset": 54, "endOffset": 77}, {"referenceID": 8, "context": "MARVIN implements the SpeedDate method, presented in (Kotoulas et al. (2010)), where authors pointed out and addressed the scalability challenge posed by the highly uneven distribution of Semantic Web data. WebPIE (Urbani et al. (2012)) implements forward reasoning under RDFS and OWL ter Horst semantics over the MapReduce framework (Dean and Ghemawat (2004)) scaling up to 100 billion triples.", "startOffset": 54, "endOffset": 236}, {"referenceID": 3, "context": "(2012)) implements forward reasoning under RDFS and OWL ter Horst semantics over the MapReduce framework (Dean and Ghemawat (2004)) scaling up to 100 billion triples.", "startOffset": 106, "endOffset": 131}, {"referenceID": 3, "context": "(2012)) implements forward reasoning under RDFS and OWL ter Horst semantics over the MapReduce framework (Dean and Ghemawat (2004)) scaling up to 100 billion triples. In (Goodman et al. (2011)) authors present RDFS inference scaling up to 512 processors with the ability to process, entirely in-memory, 20 billion triples.", "startOffset": 106, "endOffset": 193}, {"referenceID": 3, "context": "(2012)) implements forward reasoning under RDFS and OWL ter Horst semantics over the MapReduce framework (Dean and Ghemawat (2004)) scaling up to 100 billion triples. In (Goodman et al. (2011)) authors present RDFS inference scaling up to 512 processors with the ability to process, entirely in-memory, 20 billion triples. FuzzyPD (Liu et al. (2011, 2012)) is a MapReduce based prototype system allowing fuzzy reasoning in OWL pD\u2217 with scalability of up to 128 process units and over 1 billion triples. Description logic in the form of E L+ have been studied in (Mutharaju et al. (2010)).", "startOffset": 106, "endOffset": 587}, {"referenceID": 3, "context": "(2012)) implements forward reasoning under RDFS and OWL ter Horst semantics over the MapReduce framework (Dean and Ghemawat (2004)) scaling up to 100 billion triples. In (Goodman et al. (2011)) authors present RDFS inference scaling up to 512 processors with the ability to process, entirely in-memory, 20 billion triples. FuzzyPD (Liu et al. (2011, 2012)) is a MapReduce based prototype system allowing fuzzy reasoning in OWL pD\u2217 with scalability of up to 128 process units and over 1 billion triples. Description logic in the form of E L+ have been studied in (Mutharaju et al. (2010)). The authors parallelize an existing algorithm for E L+ classification by converting it into MapReduce algorithms, while experimental evaluation was deferred to future work. (Tachmazidis et al. (2012)) deals with defeasible logic for unary predicates scaling up to billions of facts, while authors extend their approach in (Tachmazidis et al.", "startOffset": 106, "endOffset": 789}, {"referenceID": 3, "context": "(2012)) implements forward reasoning under RDFS and OWL ter Horst semantics over the MapReduce framework (Dean and Ghemawat (2004)) scaling up to 100 billion triples. In (Goodman et al. (2011)) authors present RDFS inference scaling up to 512 processors with the ability to process, entirely in-memory, 20 billion triples. FuzzyPD (Liu et al. (2011, 2012)) is a MapReduce based prototype system allowing fuzzy reasoning in OWL pD\u2217 with scalability of up to 128 process units and over 1 billion triples. Description logic in the form of E L+ have been studied in (Mutharaju et al. (2010)). The authors parallelize an existing algorithm for E L+ classification by converting it into MapReduce algorithms, while experimental evaluation was deferred to future work. (Tachmazidis et al. (2012)) deals with defeasible logic for unary predicates scaling up to billions of facts, while authors extend their approach in (Tachmazidis et al. (2012)) for predicates of arbitrary arity, under the assumption of stratification, scaling up to millions of facts.", "startOffset": 106, "endOffset": 938}, {"referenceID": 3, "context": "(2012)) implements forward reasoning under RDFS and OWL ter Horst semantics over the MapReduce framework (Dean and Ghemawat (2004)) scaling up to 100 billion triples. In (Goodman et al. (2011)) authors present RDFS inference scaling up to 512 processors with the ability to process, entirely in-memory, 20 billion triples. FuzzyPD (Liu et al. (2011, 2012)) is a MapReduce based prototype system allowing fuzzy reasoning in OWL pD\u2217 with scalability of up to 128 process units and over 1 billion triples. Description logic in the form of E L+ have been studied in (Mutharaju et al. (2010)). The authors parallelize an existing algorithm for E L+ classification by converting it into MapReduce algorithms, while experimental evaluation was deferred to future work. (Tachmazidis et al. (2012)) deals with defeasible logic for unary predicates scaling up to billions of facts, while authors extend their approach in (Tachmazidis et al. (2012)) for predicates of arbitrary arity, under the assumption of stratification, scaling up to millions of facts. Finally, the computation of stratified semantics of logic programming that can be applied to billions of facts is reported in (Tachmazidis and Antoniou (2013)).", "startOffset": 106, "endOffset": 1206}, {"referenceID": 3, "context": "(2012)) implements forward reasoning under RDFS and OWL ter Horst semantics over the MapReduce framework (Dean and Ghemawat (2004)) scaling up to 100 billion triples. In (Goodman et al. (2011)) authors present RDFS inference scaling up to 512 processors with the ability to process, entirely in-memory, 20 billion triples. FuzzyPD (Liu et al. (2011, 2012)) is a MapReduce based prototype system allowing fuzzy reasoning in OWL pD\u2217 with scalability of up to 128 process units and over 1 billion triples. Description logic in the form of E L+ have been studied in (Mutharaju et al. (2010)). The authors parallelize an existing algorithm for E L+ classification by converting it into MapReduce algorithms, while experimental evaluation was deferred to future work. (Tachmazidis et al. (2012)) deals with defeasible logic for unary predicates scaling up to billions of facts, while authors extend their approach in (Tachmazidis et al. (2012)) for predicates of arbitrary arity, under the assumption of stratification, scaling up to millions of facts. Finally, the computation of stratified semantics of logic programming that can be applied to billions of facts is reported in (Tachmazidis and Antoniou (2013)). In this paper, we propose a parallel approach for the well-founded semantics computation using the MapReduce framework. Specifically, we adapt and incorporate the computation of joins and anti-joins, initially described in (Tachmazidis and Antoniou (2013)).", "startOffset": 106, "endOffset": 1464}, {"referenceID": 3, "context": "MapReduce is a framework for parallel processing over huge datasets (Dean and Ghemawat (2004)).", "startOffset": 69, "endOffset": 94}, {"referenceID": 6, "context": "In this section we provide the definition of the well-founded semantics (WFS) as it was defined in (Gelder et al. (1991)).", "startOffset": 100, "endOffset": 121}, {"referenceID": 6, "context": "1 (Gelder et al. (1991)) A general logic program is a finite set of general rules, which may have both positive and negative subgoals.", "startOffset": 3, "endOffset": 24}, {"referenceID": 6, "context": "2 (Gelder et al. (1991)) Given a program P, a partial interpretation I is a consistent set of literals whose atoms are in the Herbrand base of P.", "startOffset": 3, "endOffset": 24}, {"referenceID": 6, "context": "3 (Gelder et al. (1991)) Let a program P, its associated Herbrand base H and a partial interpretation I be given.", "startOffset": 3, "endOffset": 24}, {"referenceID": 6, "context": "1 (Gelder et al. (1991)) The data complexity of the well-founded semantics for function-free programs is polynomial time.", "startOffset": 3, "endOffset": 24}, {"referenceID": 0, "context": "This safety criterion is an adaptation of range restriction (Nicolas (1982)), which guarantees the important concept of domain independence, originally studied in deductive databases (see for example (Abiteboul et al. (1995))).", "startOffset": 201, "endOffset": 225}, {"referenceID": 1, "context": "In this section, we provide the definition of the alternating fixpoint procedure as it was defined in (Brass et al. (2001)).", "startOffset": 103, "endOffset": 123}, {"referenceID": 1, "context": "4 (Brass et al. (2001)) For a set S of literals we define the following sets:", "startOffset": 3, "endOffset": 23}, {"referenceID": 1, "context": "5 (Brass et al. (2001)) (Extended Immediate Consequence Operator) Let P be a normal logic program.", "startOffset": 3, "endOffset": 23}, {"referenceID": 1, "context": "6 (Brass et al. (2001)) (Alternating Fixpoint Procedure) Let P be a normal logic program.", "startOffset": 3, "endOffset": 23}, {"referenceID": 6, "context": "We rely on the definition of the well-founded partial model Wp of P as given in (Gelder et al. (1991)).", "startOffset": 81, "endOffset": 102}, {"referenceID": 1, "context": "2 (Brass et al. (2001)) (Correctness of AFP) Let the sequence (Ki,Ui)i\u22650 be defined as above.", "startOffset": 3, "endOffset": 23}, {"referenceID": 1, "context": "1 (Brass et al. (2001)) (Monotonicity) Let the sequence (Ki,Ui)i\u22650 be defined as above.", "startOffset": 3, "endOffset": 23}, {"referenceID": 2, "context": "A single join (Cluet and Moerkotte (1994)), calculating the positive goal ab(X,Z,Y), can be performed as described below.", "startOffset": 15, "endOffset": 42}, {"referenceID": 2, "context": "In order to calculate the final goal (p(X,Y)) we need to perform an anti-join (Cluet and Moerkotte (1994)) between ab(X,Z,Y) and each negative subgoal (c(X,Z) and d(Z,Y)).", "startOffset": 79, "endOffset": 106}, {"referenceID": 11, "context": "In (Liang et al. (2009)), the authors evaluate the performance of several rule engines on data that fit in main memory.", "startOffset": 4, "endOffset": 24}, {"referenceID": 11, "context": "In (Liang et al. (2009)), the authors evaluate the performance of several rule engines on data that fit in main memory. However, our approach is targeted on data that exceed the capacity of the main memory. Thus, we follow the proposed methodology in (Liang et al. (2009)) while adjusting several parameters.", "startOffset": 4, "endOffset": 272}, {"referenceID": 11, "context": "In (Liang et al. (2009)), the authors evaluate the performance of several rule engines on data that fit in main memory. However, our approach is targeted on data that exceed the capacity of the main memory. Thus, we follow the proposed methodology in (Liang et al. (2009)) while adjusting several parameters. In (Liang et al. (2009)) loading and inference time are separated, focusing on inference time.", "startOffset": 4, "endOffset": 333}, {"referenceID": 11, "context": "In (Liang et al. (2009)), the authors evaluate the performance of several rule engines on data that fit in main memory. However, our approach is targeted on data that exceed the capacity of the main memory. Thus, we follow the proposed methodology in (Liang et al. (2009)) while adjusting several parameters. In (Liang et al. (2009)) loading and inference time are separated, focusing on inference time. However, for our approach such a separation is difficult as loading and inference time may overlap. We evaluate our approach considering default negation by applying the win-not-win test and merge large (anti-)join tests with datalog recursion and default negation, creating a new test called transitive closure with negation. Other metrics in (Liang et al. (2009)), such as indexing, are not supported by the MapReduce framework, while all optimizations and cost-based analysis were performed manually.", "startOffset": 4, "endOffset": 769}, {"referenceID": 11, "context": "In (Liang et al. (2009)), the authors evaluate the performance of several rule engines on data that fit in main memory. However, our approach is targeted on data that exceed the capacity of the main memory. Thus, we follow the proposed methodology in (Liang et al. (2009)) while adjusting several parameters. In (Liang et al. (2009)) loading and inference time are separated, focusing on inference time. However, for our approach such a separation is difficult as loading and inference time may overlap. We evaluate our approach considering default negation by applying the win-not-win test and merge large (anti-)join tests with datalog recursion and default negation, creating a new test called transitive closure with negation. Other metrics in (Liang et al. (2009)), such as indexing, are not supported by the MapReduce framework, while all optimizations and cost-based analysis were performed manually. Platform. We have implemented our experiments using the Hadoop MapReduce framework1, version 1.2.1. We have performed experiments on a cluster of the University of Huddersfield. The cluster consists of 8 nodes (one node was allocated as \u201cmaster\u201d node), using a Gigabit Ethernet interconnect. Each node was equipped with 4 cores running at 2.5GHz, 8GB RAM and 250GB of storage space. Evaluation tests. The win-not-win test (Liang et al. (2009)) consists of a single rule, where move is the base relation:", "startOffset": 4, "endOffset": 1351}, {"referenceID": 6, "context": "In future work, we plan to study more complex knowledge representation methods including Answer-Set programming (Gelfond (2008)), and RDF/S ontology evolution (Konstantinidis et al.", "startOffset": 113, "endOffset": 128}, {"referenceID": 6, "context": "In future work, we plan to study more complex knowledge representation methods including Answer-Set programming (Gelfond (2008)), and RDF/S ontology evolution (Konstantinidis et al. (2008)) and repair (Roussakis et al.", "startOffset": 113, "endOffset": 189}, {"referenceID": 6, "context": "In future work, we plan to study more complex knowledge representation methods including Answer-Set programming (Gelfond (2008)), and RDF/S ontology evolution (Konstantinidis et al. (2008)) and repair (Roussakis et al. (2011)).", "startOffset": 113, "endOffset": 226}, {"referenceID": 5, "context": "In fact, in Answer-Set programming, the system claspar (Gebser et al. (2011)) uses MPI, but it needs a preliminary grounding step, as it accepts only ground or propositional programs.", "startOffset": 56, "endOffset": 77}, {"referenceID": 5, "context": "In fact, in Answer-Set programming, the system claspar (Gebser et al. (2011)) uses MPI, but it needs a preliminary grounding step, as it accepts only ground or propositional programs. (Perri et al. (2013)) uses POSIX threads on shared memory for parallelized grounding.", "startOffset": 56, "endOffset": 205}], "year": 2014, "abstractText": "Data originating from the Web, sensor readings and social media result in increasingly huge datasets. The so called Big Data comes with new scientific and technological challenges while creating new opportunities, hence the increasing interest in academia and industry. Traditionally, logic programming has focused on complex knowledge structures/programs, so the question arises whether and how it can work in the face of Big Data. In this paper, we examine how the well-founded semantics can process huge amounts of data through mass parallelization. More specifically, we propose and evaluate a parallel approach using the MapReduce framework. Our experimental results indicate that our approach is scalable and that wellfounded semantics can be applied to billions of facts. To the best of our knowledge, this is the first work that addresses large scale nonmonotonic reasoning without the restriction of stratification for predicates of arbitrary arity.", "creator": "LaTeX with hyperref package"}}}