{"id": "1405.2875", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2014", "title": "Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms for Repeated Principal-Agent Problems", "abstract": "crowdsourcing markets have emerged as a popular consumer platform for matching available workers with tasks to complete. the payment for a particular task is typically set by the task's requester, and may occasionally be adjusted periodically based on affecting the quality of the completed work, for example, incurred through the use of \" bonus \" payments. in this paper, we study the job requester'potential s strategic problem of dynamically adjusting quality - contingent payments for tasks. we additionally consider a multi - round implementation version of the well - known principal - agent model, whereby in each round a replacement worker makes a major strategic choice of the effort level which is not directly observable by the requester. in particular, our formulation significantly well generalizes the budget - free online task pricing problems studied in prior organization work.", "histories": [["v1", "Mon, 12 May 2014 18:52:28 GMT  (164kb)", "https://arxiv.org/abs/1405.2875v1", "This is the full version of a paper in the ACM Conference on Economics and Computation (ACM-EC), 2014"], ["v2", "Wed, 2 Sep 2015 04:21:07 GMT  (166kb)", "http://arxiv.org/abs/1405.2875v2", "This is the full version of a paper in the ACM Conference on Economics and Computation (ACM-EC), 2014"]], "COMMENTS": "This is the full version of a paper in the ACM Conference on Economics and Computation (ACM-EC), 2014", "reviews": [], "SUBJECTS": "cs.DS cs.GT cs.LG", "authors": ["chien-ju ho", "aleksandrs slivkins", "jennifer wortman vaughan"], "accepted": false, "id": "1405.2875"}, "pdf": {"name": "1405.2875.pdf", "metadata": {"source": "CRF", "title": "Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms for Repeated Principal-Agent Problems\u2217", "authors": ["Chien-Ju Ho", "Aleksandrs Slivkins", "Jennifer Wortman Vaughan"], "emails": ["cjho@ucla.edu.", "slivkins@microsoft.com.", "jenn@microsoft.com."], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n28 75\nv2 [\ncs .D\nS] 2\nS ep\nWe treat this problem as a multi-armed bandit problem, with each \u201carm\u201d representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature).\nOur results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing.\nACM Categories and subject descriptors: F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems; F.1.2 [Computation by Abstract Devices]: Modes of Computation\u2014Online computation; J.4 [Social and Behavioral Sciences]: Economics\nKeywords: crowdsourcing; principal-agent; dynamic pricing; multi-armed bandits; regret.\n\u2217Preliminary version of this paper has been published in the ACM Conference on Economics and Computation (ACM-EC), 2014. The conference version omits many of the proofs, including all of Section 8, the details of the simulation results, and a detailed discussion of related work (Section 9). Moreover, the present version contains revised and expanded Conclusions section, and an updated discussion of the follow-up work.\nCompared to the initial technical report (arXiv:1405.2875v1, May 2014), this version includes updated citations and discussion of the follow-up work.\nMuch of this research was completed while Ho was an intern at Microsoft Research. This research was partially supported by the NSF under grant IIS-1054911. Any opinions, findings, conclusions, or recommendations are those of the authors alone.\n\u2020UCLA, Los Angeles, CA, USA. Email: cjho@ucla.edu. \u2021Microsoft Research, New York, NY, USA. Email: slivkins@microsoft.com. \u00a7Microsoft Research, New York, NY, USA. Email: jenn@microsoft.com."}, {"heading": "1 Introduction", "text": "Crowdsourcing harnesses human intelligence and common sense to complete tasks that are difficult to accomplish using computers alone. Crowdsourcing markets, such as Amazon Mechanical Turk and Microsoft\u2019s Universal Human Relevance System, are platforms designed to match available human workers with tasks to complete. Using these platforms, requesters may post tasks that they would like completed, along with the amount of money they are willing to pay. Workers then choose whether or not to accept the available tasks and complete the work.\nOf course not all human workers are equal, nor is all human-produced work. Some tasks, such as proofreading English text, are easier for some workers than others, requiring less effort to produce high quality results. Additionally, some workers are more dedicated than others, willing to spend extra time to make sure a task is completed properly. To encourage high quality results, requesters may set quality-contingent \u201cbonus\u201d payments on top of the base payment for each task, rewarding workers for producing valuable output. This can be viewed as offering workers a \u201ccontract\u201d that specifies how much they will be paid based on the quality of their output.1\nWe examine the requester\u2019s problem of dynamically setting quality-contingent payments for tasks. We consider a setting in which time evolves in rounds. In each round, the requester posts a new contract, a performance-contingent payment rule which specifies different levels of payment for different levels of output. A random, unidentifiable worker then arrives in the market and strategically decides whether to accept the requester\u2019s task and how much effort to exert; the choice of effort level is not directly observable by the requester. After the worker completes the task (or chooses not to complete it), the requester observes the worker\u2019s output, pays the worker according to the offered contract, and adjusts the contract for the next round. The properties of a random worker (formally: the distribution over the workers\u2019 types) are not known to the requester, but may be learned over time. The goal of the requester is to maximize his expected utility, the value he receives from completed work minus the payments made. We call it the dynamic contract design problem.\nFor concreteness, consider a special case in which a worker can strategically choose to perform a task with low effort or with high effort, and the task may be completed either at low quality or at high quality. The low effort incurs no cost and results in low quality, which in turn brings no value to the requester. The high effort leads to high quality with some positive probability (which may vary from one worker to another, and is unknown to the requester). The requester only observes the quality of completed tasks, and therefore cannot infer the effort level. This example captures the two main tenets of our model: that the properties of a random worker are unknown to the requester and that workers\u2019 strategic decisions are unobservable.\nWe treat the dynamic contract design problem as a multi-armed bandit (MAB) problem, with each arm representing a potential contract. Since the action space is large (potentially infinite) and has a well-defined real-valued structure, it is natural to consider an algorithm that uses discretization. Our algorithm, AgnosticZooming, divides the action space into regions, and chooses among these regions, effectively treating each region as a single \u201cmeta-arm.\u201d The discretization is defined adaptively, so that the more promising areas of the action space are eventually discretized more finely than the less promising areas. While the general idea of adaptive discretization has\n1For some tasks, such as labeling websites as relevant to a particular search query or not, verifying the quality of work may be as difficult as completing the task. These tasks can be assigned in batches, with each batch containing one or more instances in which the correct answer is already known. Quality-contingent payments can then be based on the known instances.\nappeared in prior work on MAB [Kleinberg et al., 2008, Bubeck et al., 2011a, Slivkins, 2014, 2011], our approach to adaptive discretization is new and problem-specific. The main difficulty, compared to this prior work, is that an algorithm is not given any information that links the observable numerical structure of contracts and the expected utilities thereof.\nTo analyze performance, we propose a concept called \u201cwidth dimension\u201d which measures how \u201cnice\u201d a particular problem instance is. We show that AgnosticZooming achieves regret sublinear in the time horizon for problem instances with small width dimension. In particular, if the width dimension is d, it achieves regret O(log T \u00b7T (d+1)/(d+2)) after T rounds. For problem instances with large width dimension, AgnosticZooming matches the performance of the naive algorithm which uniformly discretizes the space and runs a standard bandit algorithm. We illustrate our general results via some corollaries and special cases, including the high-low example described above. We support the theoretical results with simulations.\nFurther, we consider a special case of our setting where each worker only chooses whether to accept or reject a given task. This special case corresponds to a dynamic pricing problem previously studied in the literature. Our results significantly improve over the prior work on this problem.\nOur contributions can be summarized as follows. We define a broad, practically important setting in crowdsourcing markets; identify novel problem-specific structure, for both the algorithm and the regret bounds; distill ideas from prior work to work with these structures; argue that our approach is productive by deriving corollaries and comparing to prior work; and identify and analyze specific examples where our theory applies. The main conceptual contributions are the model itself and the adaptive discretization approach mentioned above. Finally, this paper prompts further research on dynamic contract design along several directions that we outline in the conclusion.\nRelated work. Our work builds on three areas of research. First, our model can be viewed as a multi-round version of the classical principal-agent model from contract theory [Laffont and Martimort, 2002]. A single round of our model corresponds to the basic principal-agent setting, with adverse selection (unknown worker\u2019s type) andmoral hazard (unobservable worker\u2019s decisions). Unlike much of the prior work in contract theory, the prior over worker types is not known to the principal, but may be learned over time. Accordingly, our techniques are very different from those employed in contract theory.\nSecond, our methods build on those developed in the rich literature on MAB with continuous outcome spaces. The closest line of work is that on Lipschitz MAB [Kleinberg et al., 2008], in which the algorithm is given a distance function on the arms, and the expected rewards of the arms are assumed to satisfy Lipschitz-continuity (or a relaxation thereof) with respect to this distance function, [Agrawal, 1995, Kleinberg, 2004, Auer et al., 2007, Kleinberg et al., 2008, Bubeck et al., 2011a, Slivkins, 2014]. Most related to our techniques is the idea of adaptive discretization [Kleinberg et al., 2008, Bubeck et al., 2011a, Slivkins, 2014], and in particular, the zooming algorithm [Kleinberg et al., 2008, Slivkins, 2014]. However, the zooming algorithm cannot be applied directly in our setting because the required numerical similarity information is not immediately available. This problem also arises in web search and advertising, where it is natural to assume that an algorithm can only observe a tree-shaped taxonomy on arms [Kocsis and Szepesvari, 2006, Munos and Coquelin, 2007, Pandey et al., 2007] which can be used to explicitly reconstruct relevant parts of the underlying metric space [Slivkins, 2011, Bull, 2013]. We take a different approach, using a notion of \u201cvirtual width\u201d to estimate similarity information. Explicit comparisons between our results and prior MAB work are made throughout the paper.\nFinally, our work follows several other theoretical papers on pricing in crowdsourcing markets\n[Kleinberg and Leighton, 2003b, Badanidiyuru et al., 2012, Singer and Mittal, 2013, Singla and Krause, 2013, Badanidiyuru et al., 2013]. In particular, Badanidiyuru et al. [2012] and Singla and Krause [2013] study a version of our setting with simple, single-price contracts (independent of the output), where the focus is on dealing with a global budget constraint.\nA more thorough literature review (including a discussion of some related empirical work) can be found in Section 9."}, {"heading": "2 Our setting: the dynamic contract design problem", "text": "In this section, we formally define the problem that we set out to solve. We start by describing a static model, which captures what happens in a single round of interaction between a requester and a worker. As described above, this is a version of the standard principal-agent model [Laffont and Martimort, 2002]. We then define our dynamic model, an extension of the static model to multiple rounds, with a new worker arriving each round. We then detail the objective of our pricing algorithm and the simplifying assumptions that we make throughout the paper. Finally, we compare our setting to the classic multi-armed bandit problem.\nStatic model. We begin with a description of what occurs during each interaction between the requester and a single worker. The requester first posts a task which may be completed by the worker, and a contract specifying how the worker will be paid if she completes the task. If the task is completed, the requester pays the worker as specified in the contract, and the requester derives value from the completed task; for normalization, we assume that the value derived is in [0, 1]. The requester\u2019s utility from a given task is this value minus the payment to the worker.\nWhen the worker observes the contract and decides whether or not to complete the task, she also chooses a level of effort to exert, which in turn determines her cost (in terms of time, energy, or missed opportunities) and a distribution over the quality of her work. To model quality, we assume that there is a (small) finite set of possible outcomes that result from the worker completing the task (or choosing not to complete it), and that the realized outcome determines the value that the requester derives from the task. The realized outcome is observed by the requester, and the contract that the requester offers is a mapping from outcomes to payments for the worker.\nWe emphasize two crucial (and related) features of the principal-agent model: that the mapping from effort level to outcomes can be randomized, and that the effort level is not directly observed by the requester. This is in line with a standard observation in crowdsourcing that even honest, high-effort workers occasionally make errors.\nThe worker\u2019s utility from a given task is the payment from the requester minus the cost corresponding to her chosen effort level. Given the contract she is offered, the worker chooses her effort level strategically so as to maximize her expected utility. Crucially, the chosen effort level is not directly observable by the requester.\nThe worker\u2019s choice not to perform a task is modeled as a separate effort level of zero cost (called the null effort level) and a separate outcome of zero value and zero payment (called the null outcome) such that the null effort level deterministically leads to the null outcome, and it is the only effort level that can lead to this outcome.\nThe mapping from outcomes to the requester\u2019s value is called the requester\u2019s value function. The mapping from effort levels to costs is called the cost function, and the mapping from effort levels to distributions over outcomes is called the production function. For the purposes of this paper, a worker is completely specified by these two functions; we say that the cost function and the\nproduction function comprise the worker\u2019s type. Unlike some traditional versions of the principalagent problem, in our setting a worker\u2019s type is not observable by the requester, nor is any prior given.\nDynamic model. The dynamic model we consider in this paper is a natural extension of the static model to multiple rounds and multiple workers. We are still concerned with just a single requester. In each round, a new worker arrives. We assume a stochastic environment in which the worker\u2019s type in each round is an i.i.d. sample from some fixed and unknown distribution over types, called the supply distribution. The requester posts a new task and a contract for this task. All tasks are of the same type, in the sense that the set of possible effort levels and the set of possible outcomes are the same for all tasks. The worker strategically chooses her effort level so as to maximize her expected utility from this task. Based on the chosen effort level and the worker\u2019s production function, an outcome is realized. The requester observes this outcome (but not the worker\u2019s effort level) and pays the worker the amount specified by the contract. The type of the arriving worker is never revealed to the requester. The requester can adjust the contract from one round to another, and his total utility is the sum of his utility over all rounds. For simplicity, we assume that the number of rounds is known in advance, though this assumption can be relaxed using standard tricks.\nThe dynamic contract design problem. Throughout this paper, we take the point of view of the requester interacting with workers in the dynamic model. The algorithms we examine dynamically choose contracts to offer on each round with the goal of maximizing the requester\u2019s expected utility. A problem instance consists of several quantities, some of which are known to the algorithm, and some of which are not. The known quantities are the number of outcomes, the requester\u2019s value function, and the time horizon T (i.e., the number of rounds). The latent quantities are the number of effort levels, the set of worker types, and the supply distribution. The algorithm adjusts the contract from round to round and observes the realized outcomes but receives no other feedback.\nWe focus on contracts that are bounded (offer payments in [0, 1]), and monotone (assign equal or higher payments for outcomes with higher value for the requester). Let X be the set of all bounded, monotone contracts. We compare a given algorithm against a given subset of \u201ccandidate contracts\u201d Xcand \u2282 X. Letting OPT(Xcand) be the optimal utility over all contracts inXcand, the goal is to minimize the algorithm\u2019s regret R(T |Xcand), defined as T \u00d7 OPT(Xcand) minus the algorithm\u2019s expected utility.\nThe subset Xcand may be finite or infinite, possibly Xcand = X. The most natural example of a finite Xcand is the set of all bounded, monotone contracts with payments that are integer multiples of some \u03c8 > 0; we call it the uniform mesh with granularity \u03c8, and denote it Xcand(\u03c8).\nNotation. Let v(\u00b7) be the value function of the requester, with v(\u03c0) denoting the value of outcome \u03c0. Let O be the set of all outcomes and let m be the number of non-null outcomes. We will index the outcomes as O = {0, 1, 2 , . . . ,m} in the order of increasing value (ties broken arbitrarily), with a convention that 0 is the null outcome.\nLet ci(\u00b7) and fi(\u00b7) be the cost function and production function for type i. Then the cost of choosing effort level e is ci(e), and the probability of obtaining outcome \u03c0 having chosen effort e is fi(\u03c0|e). Let Fi(\u03c0|e) = \u2211 \u03c0\u2032\u2265\u03c0 fi(\u03c0\n\u2032|e). Recall that a contract x is a function from outcomes to (non-negative) payments. If contract x is offered to a worker sampled i.i.d. from the supply distribution, V (x) is the expected value to the requester, P (x) \u2265 0 is the expected payment, and U(x) = V (x) \u2212 P (x) is the expected utility\nof the requester. Let OPT(Xcand) = supx\u2208Xcand U(x). Assumption: First-order stochastic dominance (FOSD). Given two effort levels e and e\u2032, we say that e has FOSD over e\u2032 for type i if Fi(\u03c0|e) \u2265 Fi(\u03c0|e\u2032) for all outcomes \u03c0, with a strict inequality for at least one outcome.2 We say that type i satisfies the FOSD assumption if for any two distinct effort levels, one effort level has FOSD over the other for type i. We assume that all types satisfy this assumption.\nAssumption: Consistent tie-breaking. If multiple effort levels maximize the expected utility of a given worker for a contract x, we assume the tie is broken consistently in the sense that this worker chooses the same effort level for any contract that leads to this particular tie. This assumption is minor; it can be avoided (with minor technical complications) by adding random perturbations to the contracts. This assumption is implicit throughout the paper."}, {"heading": "2.1 Discussion", "text": "Number of outcomes. Our results assume a small number of outcomes. This regime is important in practice, as the quality of submitted work is typically difficult to evaluate in a very fine granularity. Even with m = 2 non-null outcomes, our setting has not been studied before. The special case m = 1 is equivalent to the dynamic pricing problem from Kleinberg and Leighton [2003a]; we obtain improved results for it, too.\nThe benchmark. Our benchmark OPT(\u00b7) only considers contracts that are bounded and monotone. In practice, restricting to such contracts may be appealing to all human parties involved. However, this restriction is not without loss of generality: there are problem instances in which monotone contracts are not optimal; see Appendix A for an example. Further, it is not clear whether bounded monotone contracts are optimal among monotone contracts.\nOur benchmark OPT(Xcand) is relative to a given set Xcand, which is typically a finite discretization of the contract space. There are two reasons for this. First, crowdsourcing platforms may require the payments to be multiples of some minimum unit (e.g., one cent), in which case it is natural to restrict our attention to contracts satisfying the same constraint. Second, achieving guarantees relative to OPT(X) for the full generality of our problem appears beyond the reach of our techniques. As in many other machine learning scenarios, it is useful to consider a restricted \u201cbenchmark set\u201d \u2013 set of alternatives to compare to.3 In such settings, it is considered important to handle arbitrary benchmark sets, which is what we do.\nOne known approach to obtain guarantees relative to OPT(X) is to start with some finiteXcand \u2282 X, design an algorithm with guarantees relative to OPT(Xcand), and then, as a separate result, bound the discretization error OPT(X)\u2212 OPT(Xcand). Then the choice of Xcand drives the tradeoff between the discretization error and regret R(T |Xcand), and one can choose Xcand to optimize this tradeoff. However, while one can upper-bound the discretization error in some (very) simple special cases (see Section 5), it is unclear whether this can be extended to the full generality of dynamic contract design.\nAlternative worker models. One of the crucial tenets in our model is that the workers maximize their expected utility. This \u201crationality assumption\u201d is very standard in Economics, and is\n2This mimics the standard notion of FOSD between two distributions over a linearly ordered set. 3A particularly relevant analogy is contextual bandits with policy sets, e.g., Dudik et al. [2011].\noften used to make the problem amenable to rigorous analysis. However, there is a considerable literature suggesting that in practice workers may deviate from this \u201crational\u201d behavior. Thus, it is worth pointing out that our results do not rely heavily on the rationality assumption. The FOSD assumption (which is also fairly standard) can be circumvented, too. In fact, all our assumptions regarding worker behavior serve only to enable us to prove Lemma 3.1, and more specifically to guarantee that the collective worker behavior satisfies the following natural property (which is used in the proof of Lemma 3.1): if the requester increases the \u201cincrement payment\u201d (as described in the next section) for a particular outcome, the probability of obtaining an outcome at least that good also increases.\nMinimum wage. For ethical or legal reasons one may want to enforce some form of minimum wage. This can be expressed within our model as a minimal payment \u03b8 for a completed task, i.e., for any non-null outcome. Our algorithm can be easily modified to accommodate this constraint. Essentially, it suffices to restrict the action space to contracts that pay at least \u03b8 for a completed task. Formally, the \u201cincrement space\u201d defined in Section 3 should be [\u03b8, 1] \u00d7 [0, 1]m\u22121 rather than [0, 1]m, and the \u201cquadrants\u201d of each \u201ccell\u201d are defined by splitting the cell in half in each dimension. All our results easily carry over to this version (restricting Xcand to contracts that pay at least \u03b8 for a completed task). We omit further discussion of this issue for the sake of simplicity.\nComparison to multi-armed bandits (MAB). Dynamic contract design can be modeled as special case of the MAB problem with some additional, problem-specific structure. The basic MAB problem is defined as follows. An algorithm repeatedly chooses actions from a fixed action space and collects rewards for the chosen actions; the available actions are traditionally called arms. More specifically, time is partitioned into rounds, so that in each round the algorithm selects an arm and receives a reward for the chosen arm. No other information, such as the reward the algorithm would have received for choosing an alternative arm, is revealed. In an MAB problem with stochastic rewards, the reward of each arm in a given round is an i.i.d. sample from some distribution which depends on the arm but not on the round. A standard measure of algorithm\u2019s performance is regret with respect to the best fixed arm, defined as the difference in expected total reward between a benchmark (usually the best fixed arm) and the algorithm.\nThus, dynamic contract design can be naturally modeled as an MAB problem with stochastic rewards, in which arms correspond to monotone contracts. The prior work on MAB with large / infinite action spaces often assumes known upper bounds on similarity between arms. More precisely, this prior work would assume that an algorithm is given a metric D on contracts such that expected rewards are Lipschitz-continuous with respect to D, i.e., we have upper bounds |U(x) \u2212 U(y)| \u2264 D(x, y) for any two contracts x, y.4 However, in our setting such upper bounds are absent. On the other hand, our problem has some supplementary structure compared to the standard MAB setting. In particular, the algorithm\u2019s reward decomposes into value and payment, both of which are determined by the outcome, which in turn is probabilistically determined by the worker\u2019s strategic choice of the effort level. Effectively, this supplementary structure provides some \u201csoft\u201d information on similarity between contracts, in the sense that numerically similar contracts are usually (but not always) similar to one another.\n4Such upper bound is informative if and only if D(x, y) < 1.\n3 Our algorithm: AgnosticZooming\nIn this section, we specify our algorithm. We call it AgnosticZooming because it \u201czooms in\u201d on more promising areas of the action space, and does so without knowing a precise measure of the similarity between contracts. This zooming can be viewed as a dynamic form of discretization. Before stating the algorithm itself, we discuss the discretization of the action space in more detail, laying the groundwork for our approach."}, {"heading": "3.1 Discretization of the action space", "text": "In each round, the AgnosticZooming algorithm partitions the action space into several regions and chooses among these regions, effectively treating each region as a \u201cmeta-arm.\u201d In this section, we discuss which subsets of the action space are used as regions, and introduce some useful notions and properties of such subsets.\nIncrement space and cells. To describe our approach to discretization, it is useful to think of contracts in terms of increment payments. Specifically, we represent each monotone contract x : O \u2192 [0,\u221e) as a vector x \u2208 [0,\u221e)m, where m is the number of non-null outcomes and x\u03c0 = x(\u03c0) \u2212 x(\u03c0 \u2212 1) \u2265 0 for each non-null outcome \u03c0. (Recall that by convention 0 is the null outcome and x(0) = 0.) We call this vector the increment representation of contract x, and denote it incr(x). Note that if x is bounded, then incr(x) \u2208 [0, 1]m. Conversely, call a contract weakly bounded if it is monotone and its increment representation lies in [0, 1]m. Such a contract is not necessarily bounded.\nWe discretize the space of all weakly bounded contracts, viewed as a multi-dimensional unit cube. More precisely, we define the increment space as [0, 1]m with a convention that every vector represents the corresponding weakly bounded contract. Each region in the discretization is a closed, axis-aligned m-dimensional cube in the increment space; henceforth, such cubes are called cells. A cell is called relevant if it contains at least one candidate contract. A relevant cell is called atomic if it contains exactly one candidate contract, and composite otherwise.\nIn each composite cell C, the algorithm will only use two contracts: themaximal corner, denoted x+(C), in which all increment payments are maximal, and the minimal corner, denoted x\u2212(C), in which all increment payments are minimal. These two contracts are called the anchors of C. In each atomic cell C, the algorithm will only use one contract: the unique candidate contract, also called the anchor of C.\nVirtual width. To take advantage of the problem structure, it is essential to estimate how similar the contracts within a given composite cell C are. Ideally, we would like to know the maximal difference in expected utility:\nwidth(C) = supx,y\u2208C |U(x)\u2212 U(y)| .\nWe estimate the width using a proxy, called virtual width, which is expressed in terms of the anchors:\nVirtWidth(C) = ( V (x+(C))\u2212 P (x\u2212(C)) ) \u2212 ( V (x\u2212(C))\u2212 P (x+(C)) ) . (1)\nThis definition is one crucial place where the problem structure is used. (Note that it is not the difference in utility at the anchors.) It is useful due to the following lemma (proved in Section 3.3).\nLemma 3.1. If all types satisfy the FOSD assumption and consistent tie-breaking holds, then width(C) \u2264 VirtWidth(C) for each composite cell C.\nRecall that the proof of this lemma is the only place in the paper where we use our assumptions on worker behavior. All further developments hold for any model of worker behavior which satisfies Lemma 3.1."}, {"heading": "3.2 Description of the algorithm", "text": "With these ideas in place, we are now ready to describe our algorithm. The high-level outline of AgnosticZooming is very simple. The algorithm maintains a set of active cells which cover the increment space at all times. Initially, there is only a single active cell comprising the entire increment space. In each round t, the algorithm chooses one active cell Ct using an upper confidence index and posts contract xt sampled uniformly at random among the anchors of this cell. After observing the feedback, the algorithm may choose to zoom in on Ct, removing Ct from the set of active cells and activating all relevant quadrants thereof, where the quadrants of cell C are defined as the 2m sub-cells of half the size for which one of the corners is the center of C. In the remainder of this section, we specify how the cell Ct is chosen (the selection rule), and how the algorithm decides whether to zoom in on Ct (the zooming rule).\nLet us first introduce some notation. Consider cell C that is active in some round t. Let U(C) be the expected utility from a single round in which C is chosen by the algorithm, i.e., the average expected utility of the anchor(s) of C. Let nt(C) be the number of times this cell has been chosen before round t. Consider all rounds in which C is chosen by the algorithm before round t. Let Ut(C) be the average utility over these rounds. For a composite cell C, let V + t (C) and P + t (C) be the average value and average payment over all rounds when anchor x+(C) is chosen. Similarly, let V \u2212t (C) and P \u2212 t (C) be the average value and average payment over all rounds when anchor x\n\u2212(C) is chosen. Accordingly, we can estimate the virtual width of composite cell C at time t as\nWt(C) = ( V +t (C)\u2212 P\u2212t (C) ) \u2212 ( V \u2212t (C)\u2212 P+t (C) ) . (2)\nTo bound the deviations, we define the confidence radius as\nradt(C) = \u221a crad log(T )/nt(C), (3)\nfor some absolute constant crad; in our analysis, crad \u2265 16 suffices. We will show that with high probability all sample averages defined above will stay within radt(C) of the respective expectations. If this high probability event holds, the width estimate Wt(C) will always be within 4 radt(C) of VirtWidth(C).\nSelection rule. Now we are ready to complete the algorithm. The selection rule is as follows. In each round t, the algorithm chooses an active cell C with maximal index It(\u00b7). It(C) is an upper confidence bound on the expected utility of any candidate contract in C, defined as\nIt(C) =\n{ Ut(C) + radt(C) if C is an atomic cell,\nUt(C) +Wt(C) + 5 radt(C) otherwise. (4)\nZooming rule. We zoom in on a composite cell Ct if\nWt+1(Ct) > 5 radt+1(Ct),\nALGORITHM 1: AgnosticZooming\nInputs: subset Xcand \u2282 X of candidate contracts. Data structure: Collection A of cells. Initially, A = { [0, 1]m }. For each round t = 1 to T\nLet Ct = argmaxC\u2208A It(C), where It(\u00b7) is defined as in Equation (4). Sample contract xt u.a.r. among the anchors of Ct. \\\\ Anchors are defined in Section 3.1. Post contract xt and observe feedback. If |C \u2229Xcand| > 1 and 5 radt+1(Ct) < Wt+1(Ct) then\nA \u2190 A\u222a {all relevant quadrants of Ct} \\ {Ct}. \\\\ C is relevant if |C \u2229Xcand| \u2265 1.\ni.e., the uncertainty due to random sampling, expressed by the confidence radius, becomes sufficiently small compared to the uncertainty due to discretization, expressed by the virtual width. We never zoom in on atomic cells. The pseudocode is summarized in Algorithm 1.\nInteger payments. In practice it may be necessary to only allow contracts in which all payments are integer multiples of some amount \u03c8, e.g., whole cents. (In this case we can assume that candidate contracts have this property, too.) Then we can redefine the two anchors of each composite cell: the maximal (resp., minimal) anchor is the nearest allowed contract to the maximal (resp., minimal) corner. Width can be redefined as a sup over all allowed contracts in a given cell. With these modifications, the analysis goes through without significant changes. We omit further discussion of this issue."}, {"heading": "3.3 Proof of Lemma 3.1 (virtual width)", "text": "For two vectors x,x\u2032 \u2208 \u211cm, write x\u2032 x if x\u2032 pointwise dominates x, i.e., if x\u2032j \u2265 xj for all j. For two monotone contracts x, x\u2032, write x\u2032 x if incr(x\u2032) incr(x).\nClaim 3.2. Consider a worker whose type satisfies the FOSD assumption and two weakly bounded contracts x, x\u2032 such that x\u2032 x. Let e (resp., e\u2032) be the effort levels exerted by this worker when he is offered contract x (resp., x\u2032). Then e does not have FOSD over e\u2032.\nProof. For the sake of contradiction, assume that e has FOSD over e\u2032. Note that e 6= e\u2032. Let i be the worker\u2019s type. Recall that Fi(\u03c0|e) denotes the probability of generating an outcome \u03c0\u2032 \u2265 \u03c0 given the effort level e. Define F = ( Fi(1|e) , . . . , Fi(m|e) ), and define F\u2032 similarly for e\u2032. Let x and x\u2032 be the increment representations for x and x\u2032. Given contract x, the worker\u2019s expected utility for effort level e is Ui(x|e) = x \u00b7 F\u2212 ci(e). Since e is the optimal effort level given this contract, we have Ui(x|e) \u2265 Ui(x|e\u2032), and therefore\nx \u00b7 F\u2212 x \u00b7 F\u2032 \u2265 ci(e)\u2212 ci(e\u2032).\nSimilarly, since e\u2032 is the optimal effort level given contract x\u2032, we have\nx\u2032 \u00b7 F\u2032 \u2212 x\u2032 \u00b7 F \u2265 ci(e\u2032)\u2212 ci(e).\nCombining the above two inequalities, we obtain\n(x\u2212 x\u2032) \u00b7 (F \u2212 F\u2032) \u2265 0. (5)\nNote that if Equation (5) holds with equality then Ui(x|e) = Ui(x|e\u2032) and Ui(x\u2032|e) = Ui(x\u2032|e\u2032), so the worker breaks the tie between e and e\u2032 in a different way for two different contracts. This contradicts the consistent tie-breaking assumption. However, Equation (5) cannot hold with a strict equality, either, because x\u2032 x and (since e has FOSD over e\u2032) we have F F\u2032 and F\u03c0 > F\u2032\u03c0 for some outcome \u03c0 > 0. Therefore we obtain a contradiction, completing the proof.\nThe proof of Claim 3.2 is the only place in the paper where we directly use the consistent tie-breaking assumption. (But the rest of the paper relies on this claim.)\nClaim 3.3. Assume all types satisfy the FOSD assumption. Consider weakly bounded contracts x, x\u2032 such that x\u2032 x. Then V (x\u2032) \u2265 V (x) and P (x\u2032) \u2265 P (x).\nProof. Consider some worker, let i be his type. Let e and e\u2032 be the chosen effort levels for contracts x and x\u2032, respectively. By the FOSD assumption, either e = e\u2032, or e\u2032 has FOSD over e, or e has FOSD over e\u2032. Claim 3.2 rules out the latter possibility.\nDefine vectors F and F\u2032 as in the proof of Claim 3.2. Note that F\u2032 F. Then P = x \u00b7 F and P \u2032 = x\u2032 \u00b7 F\u2032 is the expected payment for contracts x and x\u2032, respectively. Further, letting v denote the increment representation of the requester\u2019s value for each outcome, V = v \u00b7 F and V \u2032 = v \u00b7 F\u2032 is the expected requester\u2019s value for contracts x and x\u2032, respectively. Since x\u2032 x and F\u2032 F, it follows that P \u2032 \u2265 P and V \u2032 \u2265 V . Since this holds for each worker, this also holds in expectation over workers.\nTo finish the proof of Lemma 3.1, fix a contract x \u2208 C and observe that V (x+) \u2265 V (x) \u2265 V (x\u2212) and P (x+) \u2265 P (x) \u2265 P (x\u2212), where x+ = x+(C) and x\u2212 = x\u2212(C) are the two anchors."}, {"heading": "4 Regret bounds and discussion", "text": "We present the main regret bound for AgnosticZooming. Formulating this result requires some new, problem-specific structure. Stated in terms of this structure, the result is somewhat difficult to access. To explain its significance, we state several corollaries, and compare our results to prior work.\nThe main result. We start with the main regret bound. Like the algorithm itself, this regret bound is parameterized by the set Xcand of candidate contracts; our goal is to bound the algorithm\u2019s regret with respect to candidate contracts.\nRecall that OPT(Xcand) = supx\u2208Xcand U(x) is the optimal expected utility over candidate contracts. The algorithm\u2019s regret with respect to candidate contracts is R(T |Xcand) = T OPT(Xcand)\u2212 U , where T is the time horizon and U is the expected cumulative utility of the algorithm.\nDefine the badness \u2206(x) of a contract x \u2208 X as the difference in expected utility between an optimal candidate contract and x: \u2206(x) = OPT(Xcand)\u2212 U(x). Let X\u01eb = {x \u2208 Xcand : \u2206(x) \u2264 \u01eb}.\nWe will only be interested in cells that can potentially be used by AgnosticZooming. Formally, we recursively define a collection of feasible cells as follows: (i) the cell [0, 1]m is feasible, (ii) for each feasible cell C, all relevant quadrants of C are feasible. Note that the definition of a feasible cell implicitly depends on the set Xcand of candidate contracts.\nLet F\u01eb denote the collection of all feasible, composite cells C such that VirtWidth(C) \u2265 \u01eb. For Y \u2282 Xcand, let F\u01eb(Y ) be the collection of all cells C \u2208 F\u01eb that overlap with Y , and let N\u01eb(Y ) =\n|F\u01eb(Y )|; sometimes we will write N\u01eb(Y |Xcand) in place of N\u01eb(Y ) to emphasize the dependence on Xcand.\nUsing the structure defined above, the main theorem is stated as follows. We prove this theorem in Section 6.\nTheorem 4.1. Consider the dynamic contract design problem with all types satisfying the FOSD assumption and a constant number of outcomes. Consider AgnosticZooming, parameterized by some set Xcand of candidate contracts. Assume T \u2265 max(2m+1, 18). There is an absolute constant \u03b20 > 0 such that for any \u03b4 > 0,\nR(T |Xcand) \u2264 \u03b4T +O(log T ) \u2211\n\u01eb=2\u2212j\u2265\u03b4: j\u2208N\nN\u01eb \u03b20(X\u01eb|Xcand) \u01eb . (6)\nRemark 1. As discussed in Section 2.1, we target the practically important case of a small number of outcomes. The impact of larger m is an exponential dependence on m in the O() notation, and, more importantly, increased number of candidate policies (typically exponential in m for a given granularity).\nRemark 2. Our regret bounds do not depend on the number of worker types, in line with prior work on dynamic pricing. Essentially, this is because bandit approaches tend to depend only on expected reward of a given \u201carm\u201d (and perhaps also on the variance), not the finer properties of the distribution.\nEquation (6) has a shape similar to several other regret bounds in the literature, as discussed below. To make this more apparent, we observe that regret bounds in \u201cbandits in metric spaces\u201d are often stated in terms of covering numbers. (For a fixed collection F of subsets of a given ground set X, the covering number of a subset Y \u2282 X relative to F is the smallest number of subsets in F that is sufficient to cover Y .) The numbers N\u01eb(Y |Xcand) are, essentially, about covering Y with feasible cells with virtual width close to \u01eb. We make this point more precise as follows. Let an \u01eb-minimal cell be a cell in F\u01eb which does not contain any other cell in F\u01eb. Let Nmin\u01eb (Y ) be the covering number of Y relative to the collection of \u01eb-minimal cells, i.e., the smallest number of \u01eb-minimal cells sufficient to cover Y . Then\nN\u01eb(Y ) \u2264 \u2308log 1\u03c8\u2309 N min \u01eb (Y ) for any Y \u2282 Xcand and \u01eb \u2265 0, (7)\nwhere \u03c8 is the smallest size of a feasible cell.5 Thus, Equation (6) can be easily restated using the covering numbers Nmin\u01eb (\u00b7) instead of N\u01eb(\u00b7). Corollary: Polynomial regret. Literature on regret-minimization often states \u201cpolynomial\u201d regret bounds of the form R(T ) = O\u0303(T \u03b3), \u03b3 < 1. While covering-number regret bounds are more precise and versatile, the exponent \u03b3 in a polynomial regret bound expresses algorithms\u2019 performance in a particularly succinct and lucid way.\nFor \u201cbandits in metric spaces\u201d the exponent \u03b3 is typically determined by an appropriately defined notion of \u201cdimension\u201d, such as the covering dimension,6 which succinctly captures the difficulty of the problem instance. Interestingly, the dependence of \u03b3 on the dimension d is typically\n5To prove Equation (7), observe that for each cell C \u2208 F\u01eb(Y ) there exists an \u01eb-minimal cell C \u2032 \u2282 C, and for each\n\u01eb-minimal cell C\u2032 there exist at most \u2308log 1 \u03c8 \u2309 cells C \u2208 F\u01eb(Y ) such that C \u2032 \u2282 C. 6Given covering numbers N\u01eb(\u00b7), the covering dimension of Y is the smallest d \u2265 0 such that N\u01eb(Y ) = O(\u01eb\n\u2212d) for all \u01eb > 0.\nof the same shape; \u03b3 = (d + 1)/(d + 2), for several different notions of \u201cdimension\u201d. In line with this tradition, we define the width dimension:\nWidthDim\u03b1 = inf { d \u2265 0 : N\u01eb \u03b20(X\u01eb|Xcand) \u2264 \u03b1 \u01eb\u2212d for all \u01eb > 0 } , \u03b1 > 0. (8)\nNote that the width dimension depends on Xcand and the problem instance, and is parameterized by a constant \u03b1 > 0. By optimizing the choice of \u03b4 in Equation (6), we obtain the following corollary.\nCorollary 4.2. Consider the the setting of Theorem 4.1. For any \u03b1 > 0, let d = WidthDim\u03b1. Then\nR(T |Xcand) \u2264 O(\u03b1 log T ) T (1+d)/(2+d). (9)\nThe width dimension is similar to the \u201czooming dimension\u201d in Kleinberg et al. [2008] and \u201cnear-optimality dimension\u201d in Bubeck et al. [2011a] in the work on \u201cbandits in metric spaces\u201d."}, {"heading": "4.1 Comparison to prior work", "text": "Non-adaptive discretization. One approach from prior work that is directly applicable to the dynamic contract design problem is non-adaptive discretization. This is an algorithm, call it NonAdaptive, which runs an off-the-shelf MAB algorithm, treating a set of candidate contracts Xcand as arms.\n7 For concreteness, and following the prior work [Kleinberg and Leighton, 2003a, Kleinberg, 2004, Kleinberg et al., 2008], we use a well-known algorithm UCB1 [Auer et al., 2002] as an off-the-shelf MAB algorithm.\nTo compare AgnosticZooming with NonAdaptive, it is useful to derive several \u201cworst-case\u201d corollaries of Theorem 4.1, replacing N\u01eb(X\u01eb) with various (loose) upper bounds. 8\nCorollary 4.3. In the setting of Theorem 4.1, the regret of AgnosticZooming can be upper-bounded as follows:\n(a) R(T |Xcand) \u2264 \u03b4T + \u2211\n\u01eb=2\u2212j\u2265\u03b4: j\u2208N O\u0303(|X\u01eb| /\u01eb), for each \u03b4 \u2208 (0, 1). (b) R(T |Xcand) \u2264 O\u0303( \u221a T |Xcand|).\nHere the O\u0303() notation hides the logarithmic dependence on T and \u03b4.\nThe best known regret bounds for NonAdaptive coincide with those in Corollary 4.3 up to poly-logarithmic factors. However, the regret bounds in Theorem 4.1 may be significantly better than the ones in Corollary 4.3. We further discuss this in the next section, in the context of a specific example.\nBandits in metric spaces. Consider a variant of dynamic contract design in which an algorithm is given a priori information on similarity between contracts: a function D : Xcand \u00d7Xcand \u2192 [0, 1] such that |U(x) \u2212 U(y)| \u2264 D(x, y) for any two candidate contracts x, y. If an algorithm is given this function D (call such algorithm D-aware), the machinery from \u201cbandits in metric spaces\u201d Kleinberg et al. [2008], Bubeck et al. [2011a] can be used to perform adaptive discretization and obtain a significant advantage over NonAdaptive. We argue that we obtain similar results with AgnosticZooming without knowing the D.\n7To simplify the proofs of the lower bounds, we assume that the candidate contracts are randomly permuted when given to the MAB algorithm.\n8We use the facts that X\u01eb \u2282 Xcand, N\u01eb(Y ) \u2264 N0(Y ), and N min 0 (Y ) \u2264 |Y | for all subsets Y \u2282 X.\nIn practice, the similarity information D would be coarse, probably aggregated according to some predefined hierarchy. To formalize this idea, the hierarchy can be represented as a collection F of subsets of Xcand, so that D(x, y) is a function of the smallest subset in F containing both x and y. The hierarchy F should be natural given the structure of the contract space. One such natural hierarchy is the collection of all feasible cells, which corresponds to splitting the cells in half in each dimension. Formally, D(x, y) = f(Cx,y) for some f with f(Cx,y) \u2265 width(Cx,y), where Cx,y is the smallest feasible cell containing both x and y.\nGiven this shape of D, let us state the regret bounds for D-aware algorithms in Kleinberg et al. [2008] and Bubeck et al. [2011a]. To simplify the notation, we assume that the action space is restricted to Xcand. The regret bounds have a similar \u201cshape\u201d as that in Theorem 4.1:\nR(T |Xcand) \u2264 \u03b4T +O(log T ) \u2211\n\u01eb=2\u2212j\u2265\u03b4: j\u2208N\nN\u2217\u2126(\u01eb)(X\u01eb)\n\u01eb , (10)\nwhere the numbers N\u2217\u01eb (\u00b7) have a similar high-level meaning as N\u01eb(\u00b7), and nearly coincide with Nmin\u01eb (\u00b7) when D(x, y) = VirtWidth(Cx,y). One can use Equation (10) to derive a polynomial regret bound like Equation (9).\nFor a more precise comparison, we focus on the results in Kleinberg et al. [2008] . (The regret bounds in Bubeck et al. [2011a] are very similar in spirit, but are stated in terms of a slightly different structure.) The \u201ccovering-type\u201d regret bound in Kleinberg et al. [2008] focuses on balls of radius at most \u01eb according to distance D, so that N\u2217\u01eb (Y ) is the smallest number of such balls that is sufficient to cover Y . In the special case D(x, y) = VirtWidth(Cx,y) balls of radius \u2264 \u01eb are precisely feasible cells of virtual width \u2264 \u01eb. This is very similar (albeit not technically the same) as the \u01eb-minimal cells in the definition of Nmin\u01eb (\u00b7).\nFurther, the covering numbers N\u2217\u01eb (Y ) determine the \u201czooming dimension\u201d:\nZoomDim\u03b1 = inf { d \u2265 0 : N\u2217\u01eb/8(X\u01eb) \u2264 \u03b1 \u01eb\u2212d for all \u01eb > 0 } , \u03b1 > 0. (11)\nThis definition coincides with the covering dimension in the worst case, and can be much smaller for \u201cnice\u201d problem instances in which X\u01eb is a significantly small subset of Xcand. With this definition, one obtains a polynomial regret bound which is version of Equation (9) with d = ZoomDim\u03b1.\nWe conclude that AgnosticZooming essentially matches the regret bounds for D-aware algorithms, despite the fact that D-aware algorithms have access to much more information."}, {"heading": "5 A special case: the \u201chigh-low example\u201d", "text": "We apply the machinery in Section 4 on a special case, and we show that AgnosticZooming significantly outperforms NonAdaptive.\nThe most basic special case is when there is just one non-null outcome. Essentially, each worker makes a strategic choice whether to accept or reject a given task (where \u201creject\u201d corresponds to the null effort level), and this choice is fully observable. This setting has been studied before [Kleinberg and Leighton, 2003a, Badanidiyuru et al., 2012, Singla and Krause, 2013, Badanidiyuru et al., 2013]; we will call it dynamic task pricing. Here the contract is completely specified by the price p for the non-null outcome. The supply distribution is summarized by the function S(p) = Pr[accept|p], so that the corresponding expected utility is U(p) = S(p)(v\u2212 p), where v is the value for the non-null\noutcome. This special case is already quite rich, because S(\u00b7) can be an arbitrary non-decreasing function. By using adaptive discretization, we achieve significant improvement over prior work; see Section 8 for further discussion.\nWe consider a somewhat richer setting in which workers\u2019 strategic decisions are not observable; this is a salient feature of our setting, called moral hazard in the contract theory literature. There are two non-null outcomes (low and high), and two non-null effort levels (low and high). Low outcome brings zero value to the requester, while high outcome brings value v > 0. Low effort level inflicts zero cost on a worker and leads to low outcome with probability 1. We assume that workers break ties between effort levels in a consistent way: high better than low better than null. (Hence, as low effort incurs zero cost, the only possible outcomes are low and high.) We will call this the high-low example; it is perhaps the simplest example that features moral hazard.\nIn this example, the worker\u2019s type consists of a pair (ch, \u03b8h), where ch \u2265 0 is the cost for high effort and \u03b8h \u2208 [0, 1] is the probability of high outcome given high effort. Note that dynamic task pricing is equivalent to the special case \u03b8h = 1.\nThe following claim states a crucial property of the high-low example.\nClaim 5.1. Consider the high-low example with a fixed supply distribution. Then the probability of obtaining high outcome given contract x Pr[high outcome | contract x] depends only on p = x(high) \u2212 x(low); denote this probability by S(p). Moreover, S(p) is non-decreasing in p. Therefore:\n\u2022 expected utility is U(x) = S(p)(v \u2212 p)\u2212 x(low). \u2022 discretization error OPT(X)\u2212 OPT(Xcand(\u03c8)) is at most 3\u03c8, for any \u03c8 > 0. Recall that Xcand(\u03c8), the uniform mesh with granularity \u03c8 > 0, consists of all bounded, monotone contracts with payments in \u03c8N. For our purposes, the supply distribution is summarized via the function S(\u00b7). Denote U\u0303(p) = S(p)(v \u2212 p). Note that U(x) is maximized by setting x(low) = 0, in which case U(x) = U\u0303(p). Thus, if an algorithm knows that it is given a high-low example, it can set x(low) = 0, thereby reducing the dimensionality of the search space. Then the problem essentially reduces to dynamic task pricing with the same S(\u00b7).\nHowever, in general an algorithm does not know whether it is presented with the high-low example (because the effort levels are not observable). So in what follows we will consider algorithms that do not restrict themselves to x(low) = 0.\n\u201cNice\u201d supply distribution. We focus on a supply distribution D that is \u201cnice\u201d, in the sense that S(\u00b7) satisfies the following two properties:\n\u2022 S(p) is Lipschitz-continuous: |S(p)\u2212 S(p\u2032)| \u2264 L|p\u2212 p\u2032| for some constant L. \u2022 U\u0303(p) is strongly concave, in the sense that U\u0303 \u2032\u2032(\u00b7) exists and satisfies U\u0303 \u2032\u2032(\u00b7) \u2264 C < 0.\nHere L and C are absolute constants. We call such D strongly Lipschitz-concave. The above properties are fairly natural. For example, they are satisfied if \u03b8h is the same for all worker types and the marginal distribution of ch is piecewise uniform such that the density is between 1\u03bb and \u03bb, for some absolute constant \u03bb \u2265 1.\nWe show that for any choice Xcand \u2282 X, AgnosticZooming has a small width dimension in this setting, and therefore small regret.\nLemma 5.2. Consider the high-low example with a strongly Lipschitz-concave supply distribution. Then the width dimension is at most 12 , for any given Xcand \u2282 X. Therefore, AgnosticZooming with this Xcand has regret R(T |Xcand) = O(log T )T 3/5.\nWe contrast this with the performance of NonAdaptive, parameterized with the natural choice Xcand = Xcand(\u03c8). We focus on R(T |X): regret w.r.t. the best contract in X. We show that AgnosticZooming achieves R(T |X) = O\u0303(T 3/5) for a wide range of Xcand, whereas NonAdaptive cannot do better than R(T |X) = O(T 3/4) for any Xcand = Xcand(\u03c8), \u03c8 > 0.\nLemma 5.3. Consider the setting of Lemma 5.2. Then: (a) AgnosticZooming with Xcand \u2283 Xcand(T\u22122/5) has regret R(T |X) = O(T 3/5 log T ). (b) NonAdaptive with Xcand = Xcand(\u03c8) cannot achieve regret R(T |X) < o(T 3/4) over all problem instances, for any \u03c8 > 0. 9"}, {"heading": "5.1 Proofs", "text": "Proof of Claim 5.1. Consider a contract x with x(low) = b and x(high) = b + p, and a worker of type (ch, \u03b8h). If the worker exerts high effort, she pays cost ch and receives expected payment \u03b8h(p+ b) + (1\u2212 \u03b8h)b, for a total expected payoff p\u03b8h + b\u2212 ch. Her expected payoff for exerting low effort is b. Therefore she will choose to exert high effort if and only if p\u03b8h + b \u2212 ch \u2265 b, i.e., if ch/\u03b8h \u2264 p, and choose to exert low effort otherwise. Therefore\nPr[high outcome | contract x] = E (ch,\u03b8h)\n[ \u03b8h 1{ch/\u03b8h\u2264p} ] .\nThis is a function of p, call it S(p). Moreover, this is a non-decreasing function simply because the expression inside the expectation is non-decreasing in p.\nIt trivially follows that U(x) = S(p)(v \u2212 p)\u2212 x(low). We can upper-bound the discretization error using a standard approach from the work on dynamic pricing Kleinberg and Leighton [2003b]. Fix discretization granularity \u03c8 > 0. For any \u01eb > 0, there exists a contract x\u2217 \u2208 X such that OPT(X)\u2212U(x\u2217) < \u01eb. Round x\u2217(high) and x\u2217(low) up and down, respectively, to the nearest integer multiple of \u03c8; let x \u2208 Xcand(\u03c8) be the resulting contract. Denoting p = x(high)\u2212x(low) and p\u2217 = x\u2217(high)\u2212x\u2217(low), we see that p\u2217 \u2264 p \u2264 p\u2217+2\u03c8. It follows that U(x) \u2265 U(x\u2217)\u2212 3\u03c8 \u2265 OPT(X)\u2212 \u01eb\u2212 3\u03c8. Since this holds for any \u01eb > 0, we conclude that OPT(X) \u2212 OPT(Xcand(\u03c8)) \u2264 3\u03c8.\nProof of Lemma 5.2. To calculate the width dimension, we need to count the number of feasible cells in the increment space which (i) has virtual width larger than or equal to O(\u01eb) and (ii) overlaps with X\u01eb, the set of contracts with badness smaller than \u01eb.\nWe first characterize X\u01eb. We use xp,b to denote the contract with x(high) = p+b and x(low) = b. The benefit of this representation is that, p and b would then be the two axis in the increment space. Let xp\u2217,0 be an optimal contract. Since U(xp,b) is strongly concave in p, we know that for any b, there exists constants C1 and C2 such that for any p \u2208 [0, 1], C1(p\u2217\u2212p)2 \u2264 U(xp\u2217,b)\u2212U(xp,b) \u2264 C2(p\u2217\u2212p)2. Also we know that U(xp\u2217,b) = U(xp\u2217,0)\u2212 b. Therefore.\nX\u01eb = {xp,b : (p\u2212 p\u2217)2 + b \u2264 O(\u01eb)} 9This lower bound holds even if UCB1 in NonAdaptive is replaced with any other MAB algorithm.\nWe can also write it as\nX\u01eb = {xp,b : p\u2217 \u2212 \u03b8h( \u221a \u01eb) \u2264 p \u2264 p\u2217 + \u03b8h( \u221a \u01eb) and b \u2264 O(\u01eb)}\nIntuitively, X\u01eb contains contracts {xp,b} with p not O( \u221a \u01eb) away from p\u2217 and b not O(\u01eb) away from b\u2217 = 0. Next we characterize the virtual width of a cell. We use Cp,b,d to denote the cell with size d and with anchors {xp,b, x(p+d),(b+d)}. We can derive the expected payment and value on the two anchors as:\n\u2022 P+(Cp,b,d) = (p+ d)S(p + d) + b+ d \u2022 V +(Cp,b,d) = vS(p + d) \u2022 P\u2212(Cp,b,d) = pS(p) + b \u2022 V \u2212(Cp,b,d) = vS(p)\nBy definition, we can get that (we use dF to represent S(p+ d)\u2212 S(p) for simplification) VirtWidth(Cp,b,d) = (v + p)dF + dS(p) + d dF + d.\nNow we can count the number of feasible cells with virtual width larger than \u03b8h(\u01eb) which overlaps with X\u01eb. Note that since the total number of feasible cells Cp,b,d with large d is small, we can treat the number of cells with large d as a constant. Also, for any relevant cell Cp,b,d, we have p \u2248 p\u2217. Therefore, we only care about feasible cells Cp,b,d with small d and when p is close to p\n\u2217. Since S(p) is Lipschitz, we have dF = O(d). Therefore, for any relevant cell Cp,d,\nVirtWidth(Cp,b,d) = O(d)\nGiven the above two arguments, we know that the number of cells with virtual width larger than \u01eb which also overlaps with X\u01eb is O(\u01eb/\u01eb)\u00d7O( \u221a \u01eb/\u01eb) = O(\u01eb\u22121/2). Therefore the width dimension is 1/2.\nProof Sketch of Lemma 5.3(b). Consider a version of NonAdaptive that runs an off-the-shelf MAB algorithm ALG on candidate contracts Xcand = Xcand(\u03c8). For ALG, the \u201carms\u201d are the candidate contracts; recall that the arms are randomly permuted before they are given to ALG.\nFix \u03c8 > 0. It is easy to construct a problem instance with discretization error Error , OPT(X)\u2212OPT(Xcand(\u03c8)) \u2265 \u2126(\u03c8). Note that Xcand contains N = \u2126(\u03c8\u22122) suboptimal contracts that are suboptimal w.r.t. OPT(Xcand). (For example, all contracts x with x(low) > 0 are suboptimal.)\nFix any problem instance I of MAB with N suboptimal arms. Using standard lower-bound arguments for MAB, one can show that if one runs ALG on a problem instance obtained by randomly permuting the arms in I, then the expected regret in T rounds is at least \u2126( \u221a NT ).\nTherefore, R(T |Xcand) \u2265 \u2126( \u221a NT ). It follows that\nR(T |X) \u2265 \u2126( \u221a NT ) + Error \u00b7 T \u2265 \u2126( \u221a T/\u03c8 + \u03c8T ) \u2265 \u2126(T 3/4)."}, {"heading": "6 Proof of the main regret bound (Theorem 4.1)", "text": "We now prove the main result from Section 4. Our high-level approach is to define a clean execution of an algorithm as an execution in which some high-probability events are satisfied, and derive bounds on regret conditional on the clean execution. The analysis of a clean execution does not involve any \u201cprobabilistic\u201d arguments. This approach tends to simplify regret analysis.\nWe start by listing some simple invariants enforced by AgnosticZooming:\nInvariant 6.1. In each round t of each execution of AgnosticZooming: (a) All active cells are relevant, (b) Each candidate contract is contained in some active cell, (c) Wt(C) \u2264 5 radt(C) for each active composite cell C. Note that the zooming rule is essential to ensure Invariant 6.1(c)."}, {"heading": "6.1 Analysis of the randomness", "text": "Definition 6.2 (Clean Execution). An execution of AgnosticZooming is called clean if for each round t and each active cell C it holds that\n|U(C)\u2212 Ut(C)| \u2264 radt(C), (12) |VirtWidth(C)\u2212Wt(C)| \u2264 4 radt(C) (if C is composite). (13)\nLemma 6.3. Assume crad \u2265 16 and T \u2265 max(1 + 2m, 18). Then: (a) Pr [ Equation (12) holds \u2200 rounds t, active cells C ] \u2265 1\u2212 2T\u22122. (b) Pr [ Equation (13) holds \u2200 rounds t, active composite cells C ] \u2265 1\u2212 16T\u22122. Consequently, an execution of AgnosticZooming is clean with probability at least 1\u2212 1/T . Lemma 6.3 follows from the standard concentration inequality known as \u201cChernoff Bounds\u201d. However, one needs to be careful about conditioning and other details.\nProof of Lemma 6.3(a). Consider an execution of AgnosticZooming. Let N be the total number of activated cells. Since at most 2m cells can be activated in any one round, N \u2264 1 + 2mT \u2264 T 2. Let Cj be the min(j,N)-th cell activated by the algorithm. (If multiple \u201cquadrants\u201d are activated in the same round, order them according to some fixed ordering on the quadrants.)\nFix some feasible cell C and j \u2264 T 2. We claim that\nPr [ |U(C)\u2212 Ut(C)| \u2264 radt(C) for all rounds t | Cj = C ] \u2265 1\u2212 2T\u22124. (14)\nLet n(C) = n1+T (C) be the total number of times cell C is chosen by the algorithm. For each s \u2208 N: 1 \u2264 s \u2264 n(C) let Us be the requester\u2019s utility in the round when C is chosen for the s-th time. Further, let DC be the distribution of U1, conditional on the event n(S) \u2265 1. (That is, the per-round reward from choosing cell C.) Let U \u20321 , . . . , U \u2032 T be a family of mutually independent random variables, each with distribution DC . Then for each n \u2264 T , conditional on the event {Cj = C} \u2227 {n(C) = n}, the tuple (U1 , . . . , Un) has the same joint distribution as the tuple (U \u20321 , . . . , U \u2032 n). Consequently, applying Chernoff Bounds to the latter tuple, it follows that\nPr [ \u2223\u2223U(C)\u2212 1n \u2211n s=1 Us \u2223\u2223 \u2264 \u221a 1 n crad log(T ) \u2223\u2223\u2223 {Cj = C} \u2227 {n(C) = n} ]\n\u2265 1\u2212 2T\u22122crad \u2265 1\u2212 2T\u22125.\nTaking the Union Bound over all n \u2264 T , and plugging in radt(Cj), nt(Cj), and Ut(Cj), we obtain Equation (14).\nNow, let us keep j fixed in Equation (14), and integrate over C. More precisely, let us multiply both sides of Equation (14) by Pr[Cj = C] and sum over all feasible cells C. We obtain, for all j \u2264 T 2:\nPr [ |U(Cj)\u2212 Ut(Cj)| \u2264 radt(Cj) for all rounds t ] \u2265 1\u2212 2T\u22124. (15)\n(Note that to obtain Equation (15), we do not need to take the Union Bound over all feasible cells C.) To conclude, we take the Union Bound over all j \u2264 1 + T 2.\nProof Sketch of Lemma 6.3(b). We show that\nPr [ \u2223\u2223V +(C)\u2212 V +t (C) \u2223\u2223 \u2264 radt(C) \u2200 rounds t, active composite cells C ] \u2265 1\u2212 4 T 2 , (16)\nand similarly for V \u2212(), P+() and P\u2212(). Each of these four statements is proved similarly, using the technique from Lemma 6.3(a). In what follows, we sketch the proof for one of the four cases, namely for Equation (16).\nFor a given composite cell C, we are only interested in rounds in which anchor x+(C) is selected by the algorithm. Letting n+t (C) be the number of times this anchor is chosen up to time t, let us define the corresponding notion of \u201cconfidence radius\u201d:\nrad+t (C) = 1\n2\n\u221a crad log T\nn+t (C) .\nWith the technique from the proof of Lemma 6.3(a), we can establish the following highprobability event:\n\u2223\u2223V +(C)\u2212 V +t (C) \u2223\u2223 \u2264 rad+t (C). (17)\nMore precisely, we can prove that\nPr [ Equation (17) holds \u2200 rounds t, active composite cells C ] \u2265 1\u2212 2T\u22122.\nFurther, we need to prove that w.h.p. the anchor x+(C) is played sufficiently often. Noting that E[n+t (C)] = 1 2 nt(C), we establish an auxiliary high-probability event: 10\nn+t (C) \u2265 12 nt(C)\u2212 14 radt(C). (18)\nMore precisely, we can use Chernoff Bounds to show that, if crad \u2265 16,\nPr [ Equation (18) holds \u2200 rounds t, active composite cells C ] \u2265 1\u2212 2T\u22122. (19)\nNow, letting n0 = (crad log T ) 1/3, observe that\nnt(C) \u2265 n0 \u21d2 n+t (C) \u2265 14 nt(C) \u21d2 rad + t (C) \u2264 radt(C),\nnt(C) < n0 \u21d2 radt(C) \u2265 1 \u21d2 \u2223\u2223V +(C)\u2212 V +t (C) \u2223\u2223 \u2264 radt(C).\nTherefore, once Equations (17) and (18) hold, we have \u2223\u2223V +(C)\u2212 V +t (C) \u2223\u2223 \u2264 radt(C). This completes the proof of Equation (16).\n10The constant 1 4 in Equation (18) is there to enable a consistent choice of n0 in the remainder of the proof."}, {"heading": "6.2 Analysis of a clean execution", "text": "The rest of the analysis focuses on a clean execution. Recall that Ct is the cell chosen by the algorithm in round t.\nClaim 6.4. In any clean execution, I(Ct) \u2265 OPT(Xcand) for each round t.\nProof. Fix round t, and let x\u2217 be any candidate contract. By Invariant 6.1(b), there exists an active cell, call it C\u2217t , which contains x\n\u2217. We claim that It(C \u2217 t ) \u2265 U(x\u2217). We consider two cases, depending on whether C\u2217t is atomic.\nIf C\u2217t is atomic then the anchor is unique, so U(C \u2217 t ) = U(x \u2217), and It(C \u2217 t ) \u2265 U(x\u2217) by the clean execution. If C\u2217t is composite then\nIt(C \u2217 t ) \u2265 U(C\u2217t ) + VirtWidth(C\u2217t ) by clean execution\n\u2265 U(C\u2217t ) + width(C\u2217t ) by Lemma 3.1 \u2265 U(x\u2217) by definition of width, since x\u2217 \u2208 C\u2217t .\nWe have proved that It(C \u2217 t ) \u2265 U(x\u2217). Now, by the selection rule we have It(Ct) \u2265 It(C\u2217t ) \u2265 U(x\u2217). Since this holds for any candidate contract x\u2217, the claim follows.\nClaim 6.5. In any clean execution, for each round t, the index It(Ct) is upper-bounded as follows: (a) if Ct is atomic then I(Ct) \u2264 U(Ct) + 2 radt(Ct). (b) if Ct is composite then I(Ct) \u2264 U(x) +O(radt(Ct)) for each contract x \u2208 Ct.\nProof. Fix round t. Part (a) follows because It(Ct) = Ut(Ct) + radt(Ct) by definition of the index, and Ut(Ct) \u2264 U(Ct) + radt(Ct) by clean execution.\nFor part (b), fix a contract x \u2208 Ct. Then:\nUt(Ct) \u2264 U(Ct) + radt(Ct) by clean execution \u2264 U(x) + width(Ct) + radt(Ct) by definition of width \u2264 U(x) + VirtWidth(Ct) + radt(Ct) by Lemma 3.1 \u2264 U(x) +Wt(Ct) + 5 radt(Ct) by clean execution. (20)\nIt(Ct) = Ut(Ct) +Wt(Ct) + 5 radt(Ct) by definition of index\n\u2264 U(x) + 2Wt(Ct) + 10 radt(Ct) by Equation (20) \u2264 U(x) + 20 radt(Ct) by Invariant 6.1(c).\nFor each relevant cell C, define badness \u2206(C) as follows. If C is composite, \u2206(C) = supx\u2208C \u2206(x) is the maximal badness among all contracts in C. If C is atomic and x \u2208 C is the unique candidate contract in C, then \u2206(C) = \u2206(x).\nClaim 6.6. In any clean execution, \u2206(C) \u2264 O(radt(C)) for each round t and each active cell C.\nProof. By Claims 6.4 and 6.5, \u2206(Ct) \u2264 O(radt(Ct)) for each round t. Fix round t and let C be an active cell in this round. If C has never be selected before round t, the claim is trivially true. Else, let s be the most recent round before t when C is selected by the algorithm. Then \u2206(C) \u2264 O(rads(C)). The claim follows since rads(C) = radt(C).\nClaim 6.7. In a clean execution, each cell C is selected \u2264 O(log T/(\u2206(C))2) times.\nProof. By Claim 6.6, \u2206(C) \u2264 O(radT (C)). The claim follows from the definition of radT in Equation (3).\nLet n(x) and n(C) be the number of times contract x and cell C, respectively, are chosen by the algorithm. Then regret of the algorithm is\nR(T |Xcand) = \u2211 x\u2208X n(x) \u2206(x) \u2264 \u2211 cells C n(C)\u2206(C). (21)\nThe next result (Lemma 6.8) upper-bounds the right-hand side of Equation (21) for a clean execution. By Lemma 6.3, this suffices to complete the proof of Theorem 4.1\nLemma 6.8. Consider a clean execution of AgnosticZooming. For any \u03b4 \u2208 (0, 1), \u2211\ncells C n(C)\u2206(C) \u2264 \u03b4T +O(log T ) \u2211 \u01eb=2\u2212j\u2265\u03b4: j\u2208N |F\u01eb(X2\u01eb)| \u01eb .\nThe proof of Lemma 6.8 relies on some simple properties of \u2206(\u00b7), stated below. Claim 6.9. Consider two relevant cells C \u2282 Cp. Then:\n(a) \u2206(C) \u2264 \u2206(Cp). (b) If \u2206(C) \u2264 \u01eb for some \u01eb > 0, then C overlaps with X\u01eb.\nProof. To prove part (a), one needs to consider two cases, depending on whether cell Cp is composite. If it is, the claim follows trivially. If Cp is atomic, then C is atomic, too, and so \u2206(C) = \u2206(Cp) = \u2206(x), where x is the unique candidate contract in Cp.\nFor part (b), there exists a candidate contract x \u2208 C. It is easy to see that \u2206(x) \u2264 \u2206(C) (again, consider two cases, depending on whether C is composite.) So, x \u2208 X\u01eb.\nProof of Lemma 6.8. Let \u03a3 denote the sum in question. Let A\u2217 be the collection of all cells ever activated by the algorithm. Among such cells, consider those with badness on the order of \u01eb:\nG\u01eb := { C \u2208 A\u2217 : \u2206(C) \u2208 [\u01eb, 2\u01eb) } . By Claim 6.7, the algorithm chooses each cell C \u2208 G\u01eb at most O(log T/\u01eb2) times, so n(C)\u2206(C) \u2264 O(log T/\u01eb).\nFix some \u03b4 \u2208 (0, 1) and observe that all cells C with \u2206(C) \u2264 \u03b4 contribute at most \u03b4T to \u03a3. Therefore it suffices to focus on G\u01eb, \u01eb \u2265 \u03b4/2. It follows that\n\u03a3 \u2264 \u03b4T +O(log T )\u2211\u01eb=2\u2212i\u2265\u03b4/2 |G\u01eb| \u01eb . (22)\nWe bound |G\u01eb| as follows. Consider a cell C \u2208 G\u01eb. The cell is called a leaf if it is never zoomed in on (i.e., removed from the active set) by the algorithm. If C is activated in the round when cell Cp is zoomed in on, Cp is called the parent of C. We consider two cases, depending on whether or not C is a leaf.\n(i) Assume cell C is not a leaf. Since \u2206(C) < 2\u01eb, C overlaps with X2\u01eb by Claim 6.9(b). Note that C is zoomed in on in some round, say in round t\u2212 1. Then\n5 radt(C) \u2264 Wt(C) by the zooming rule \u2264 VirtWidth(C) + 4 radt(C) by clean execution,\nso radt(C) \u2264 VirtWidth(C). Therefore, using Claim 6.6, we have \u01eb \u2264 \u2206(C) \u2264 O(radt(C)) \u2264 O(VirtWidth(C)).\nIt follows that C \u2208 F\u2126(\u01eb)(X2\u01eb).\n(ii) Assume cell C is a leaf. Let Cp be the parent of C. Since C \u2282 Cp, we have \u2206(C) \u2264 \u2206(Cp) by Claim 6.9(a). Therefore, invoking case (i), we have\n\u01eb \u2264 \u2206(C) \u2264 \u2206(Cp) \u2264 O(VirtWidth(Cp)).\nSince \u2206(C) < 2\u01eb, C overlaps with X2\u01eb by Claim 6.9(b), and therefore so does Cp. It follows that Cp \u2208 F\u2126(\u01eb)(X2\u01eb).\nCombing these two cases, it follows that |G\u01eb| \u2264 (2m + 1) \u2223\u2223F\u2126(\u01eb)(X2\u01eb) \u2223\u2223. Plugging this into (22) and making an appropriate substitution \u01eb \u2192 \u0398(\u01eb) to simplify the resulting expression, we obtain the regret bound in Theorem 4.1"}, {"heading": "7 Simulations", "text": "We evaluate the performance of AgnosticZooming through simulations. AgnosticZooming is compared with two versions of NonAdaptive that use, respectively, two standard bandit algorithms: UCB1 [Auer et al., 2002] and Thompson Sampling [Thompson, 1933] (with Gaussian priors). For both UCB1 and AgnosticZooming, we replace the logarithmic confidence terms with small constants. (We find such changes beneficial in practice, for both algorithms; this observation is consistent with prior work [Radlinski et al., 2008, Slivkins et al., 2013].) All three algorithms are run with Xcand = Xcand(\u03c8), where \u03c8 > 0 is the granularity of the discretization.\nSetup. We consider a version of the high-low example, as described in Section 5. We set the requester\u2019s values to V (high) = 1 and V (low) = .3. The probability of obtaining high outcome given high effort is set to \u03b8h = .8. Thus, the worker\u2019s type is characterized by the cost ch for high effort. We consider three supply distributions:\n\u2022 Uniform Worker Market : ch is uniformly distributed on [0, 1].\n\u2022 Homogeneous Worker Market : ch is the same for every worker.\n\u2022 Two-Type Market : ch is uniformly distributed over two values, c\u2032h and c\u2032\u2032h .\nThese first two markets represent the extreme cases when workers are extremely homogeneous or extremely diverse, and the third market is one way to represent the middle ground. For each market, we run each algorithm 100 times. For Homogeneous Worker Market, ch is drawn uniformly at random from [0, 1] for each run. For Two-Type Market, c\u2032h and c \u2032\u2032 h are drawn independently and uniformly from [0, 1] on each run.\nOverview of the results. Across all simulations, AgnosticZooming performs comparably to or better than NonAdaptive. Its performance does not appear to suffer from large \u201chidden constants\u201d that appear in the analysis. We find that AgnosticZooming converges faster than NonAdaptive when \u03c8 is near-optimal or smaller; this is consistent with the intuition that AgnosticZooming focuses on exploring the more promising regions. When \u03c8 is large, AgnosticZooming converges slower than NonAdaptive, but eventually achieves the same performance. Further, we find that AgnosticZooming with small \u03c8 performs well compared to NonAdaptive with larger \u03c8: not much worse initially, and much better eventually.\nOur simulations suggest that if time horizon T is known in advance and one can tune \u03c8 to T , then NonAdaptive can achieve similar performance as AgnosticZooming. However, in real\napplications approximately optimal \u03c8 may be difficult to compute, and the T may not be known in advance.\nDetailed results. Recall that in both UCB1 and AgnosticZooming, the logarithmic confidence terms are replaced with small constants. For UCB1, the confidence term is 1, so that if a given arm a has been played na times, its index is simply the average reward plus 1/ \u221a na. For AgnosticZooming, we set radt(\u00b7) = 1 in the selection rule, and radt(\u00b7) = .6 in the zooming rule. For both algorithms, we tried several values and picked those that performed well across all three markets; we found that the performance of both algorithms is not very sensitive to the particular choice of these constants, as long as they are on the order of 1.\nFor each algorithm, we compute the time-averaged cumulative utility after T rounds given granularity \u03c8, denote it U\u0302(T, \u03c8), for various values of T and \u03c8.\nFirst, we fix the time horizon T to 5K rounds, and study how U\u0302(T, \u03c8) changes with \u03c8 (see Figure 1). We observe that AgnosticZooming either matches or outperforms both versions of NonAdaptive, across all markets and all values of \u03c8. AgnosticZooming has a huge advantage when \u03c8 is small.\nSecond, we study how the three algorithms perform over time. Specifically, we plot U\u0302(T, \u03c8) vs. T , for three values of \u03c8, namely 0.02, 0.8, and 0.2. Since setting to 0.08 is close to optimal in our examples, these values of \u03c8 represent, resp., too small, adequate, and too large. The results are shown in Figure 2. We find that AgnosticZooming converges faster than NonAdaptive when \u03c8 is adequate or small; this is consistent with the intuition that AgnosticZooming focuses on exploring the more promising regions. When \u03c8 is large, AgnosticZooming converges slower than NonAdaptive, but eventually achieves the same performance.\nOur simulations suggest that if time horizon T is known in advance and one can optimize the \u03c8 given this T , then NonAdaptive can achieve similar performance as AgnosticZooming. However, in real applications approximately optimal \u03c8 may be difficult to calculate; further, the T may be unknown in advance.\nThird, we argue that AgnosticZooming performs well with a small \u03c8: we compare its performance against that for NonAdaptive with different values of \u03c8. For each algorithm and each choice of \u03c8, we plot U\u0302(T, \u03c8) vs. T , see Figure 3. 11 We find that for small T , AgnosticZooming with small \u03c8 converges nearly as fast as NonAdaptive with larger \u03c8. When T is large, AgnosticZooming with small \u03c8 converges to a better payoff than NonAdaptive with larger \u03c8.\nFourth, we confirm the intuition that OPT(Xcand(\u03c8)) decreases with the granularity \u03c8. To this end, we run AgnosticZooming for 50K rounds, and take the average utility over the last 5K rounds, see Figure 4.\nThe standard errors in all plots are in the order of 0.001 or less. (Note that each point is not only the average of 100 runs but also the average of all previous rounds.)"}, {"heading": "8 Application to dynamic task pricing", "text": "We discuss dynamic task pricing, which can be seen as the special case of dynamic contract design in which there is exactly one non-null outcome. We identify an important family of problem instances for which AgnosticZooming out-performs NonAdaptive.\nSome background. The dynamic task pricing problem, in its most basic version, is defined as follows. There is one principal (buyer) who sequentially interacts with multiple agents (sellers). In each round t, an agent arrives, with one item for sale. The principal offers price pt for this item, and the agent agrees to sell if and only if pt \u2265 ct, where ct \u2208 [0, 1] is the agent\u2019s private cost for this item. The principal derives value v for each item bought; his utility is the value from bought items minus the payment. The time horizon T (the number of rounds) is known. Each private cost ct is an independent sample from some fixed distribution, called the supply distribution. We are interested in the prior-independent version, where the supply distribution is not known to the principal. The algorithm\u2019s goal is to choose the offered prices pt so as to maximize the expected utility of the principal.\nDynamic task pricing can be seen as the special case of dynamic contract design in which there is exactly one non-null outcome (which corresponds to a sale). Indeed, in this special case there is exactly one non-null effort level e without loss of generality (because any non-null effort levels deterministically lead to the non-null outcome).\n11We only show the results for Uniform Worker Market; the results for Homogeneous Worker Market are very similar. We only show the version of NonAdaptive with UCB1, because in our experiments it performs better that Thompson Sampling. (We conjecture that this is because we replaced the logarithmic confidence term in UCB1 with 1.)\nOne crucial simplification compared to the full generality of dynamic contract design is that the discretization error can now be easily bounded from above: 12\nOPT(X) \u2212 OPT(Xcand(\u03c8)) \u2264 \u03c8 for each \u03c8 > 0.\nWorst-case regret bounds are implicit in prior work on dynamic inventory-pricing [Kleinberg and Leighton, 2003a].13 Let NonAdaptive(\u03c8) denote algorithm NonAdaptive with Xcand = Xcand(\u03c8). Then, by the analysis in Kleinberg and Leighton [2003a], NonAdaptive(\u03c8) achieves regret R(T ) = O\u0303(\u03c8T + \u03c8\u22122). This is optimized to R(T ) = O\u0303(T 2/3) if and only if \u03c8 = O\u0303(T\u22121/3). Moreover, there is a matching lower bound: R(T ) = \u2126(T 2/3) for any algorithm.\nFurther, it is a folklore result that NonAdaptive(\u03c8) achieves regret R(T ) = O\u0303(T 2/3) if and only if \u03c8 = \u0398\u0303(T\u22121/3). (We sketch a lower-bounding example in the proof of Lemma 8.4, to make the paper more self-contained.)\nPreliminaries. Each contract is summarized by a single number: the offered price p for the non-null outcome. Let F (p) be the probability of a worker accepting a task at price p, and let U(p) = F (p) (v \u2212 p) be the corresponding expected utility of the algorithm.\nNote that all contracts are trivially monotone and any optimal contract is bounded without loss of generality. It follows that OPT(X) = supp\u22650 U(p), the optimal expected utility over all possible prices.\nA cell C is just a price interval C = [p, p\u2032] \u2282 [0, 1], and its virtual width is\nVirtWidth(C) = ( v F (p\u2032)\u2212 pF (p) ) \u2212 ( v F (p)\u2212 p\u2032 F (p\u2032) ) .\nOur results: the general case. We will be using AgnosticZooming with Xcand = X. First, let us prove that this is a reasonable choice in the worst case: namely, that we achieve the optimal O\u0303(T 2/3) regret.\nLemma 8.1. Consider the dynamic task pricing problem. AgnosticZooming with Xcand = X achieves regret O(T 2/3 log T ).\nProof Sketch. Fix \u01eb > 0. The key observation is that if VirtWidth(C) \u2265 \u01eb then either p\u2032 \u2212 p \u2265 \u01eb4 , or F (p\u2032) \u2212 F (p) \u2265 \u01eb4 . Call C a red cell if the former happens, and blue cell otherwise. Therefore in any collection of mutually disjoint cells of virtual width \u2265 \u01eb there can be at most O(1\u01eb ) red cells and at most O(1\u01eb ) blue cells, hence at most O( 1 \u01eb ) cells total. It follows that there can be at most O(1\u01eb ) active cells of virtual width \u2265 \u01eb. So, in the notation of Theorem 4.1 we have N\u01eb(\u00b7) \u2264 O(1\u01eb ). It follows that the width dimension is at most 1, which in turn implies the desired regret bound.\nOur results: \u201cnice\u201d problem instances. We focus on problem instances with piecewise-uniform costs and bounded density. Formally, we say that an instance of dynamic task pricing has kpiecewise-uniform costs if the interval [0,1] is partitioned into k \u2208 N sub-intervals such that the\n12Recall that Xcand(\u03c8) denotes the set of all prices in [0, 1] that are integer multiples of a given \u03c8 > 0; call this set the additive \u03c8-mesh.\n13The algorithmic result for dynamic task pricing is an easy modification of the analysis in Kleinberg and Leighton [2003a] for dynamic inventory-pricing. The lower bound in in Kleinberg and Leighton [2003a] can also be \u201ctranslated\u201d from dynamic inventory-pricing to dynamic task pricing without introducing any new ideas. We omit the details from this version.\nsupply distribution is uniform on each sub-interval. A problem instance has \u03bb-bounded density, \u03bb \u2265 1 if the supply distribution has a probability density function almost everywhere, and the density is between 1\u03bb and \u03bb. Using the full power of Theorem 4.1, we obtain the following regret bound.\nTheorem 8.2. Consider the dynamic task pricing problem with k-piecewise-uniform costs and \u03bbbounded density, for some absolute constants k \u2208 N and \u03bb > 1. AgnosticZooming with Xcand = X achieves regret R(T ) = O\u0303(T 3/5).\nProof Sketch. Since the supply distribution has density at most \u03bb, it follows that F (\u00b7) is a Lipschitzcontinuous function with Lipschitz constant \u03bb. It follows that each cell of virtual width at least \u01eb has diameter at least \u2126(\u01eb/\u03bb), for any \u01eb > 0. (Note that each \u201ccell\u201d is now simply a sub-interval [p, q] \u2282 [0, 1], so its diameter is simply q \u2212 p.)\nSecond, we claim that X\u01eb is contained in a union of k intervals of diameter O( \u221a \u01eb\u03bb). To see this, consider the partition of [0, 1] into k subintervals such that the supply distribution has a uniform density on each subinterval. Let [pj, qj ] be the j-th subinterval. Let p \u2217 j be the local optimum of U(\u00b7) on this subinterval, and let Xj,\u01eb = {x \u2208 [pj , qj] : U(p\u2217j) \u2212 U(x) \u2264 \u01eb}. Then X\u01eb \u2282 \u222ajXj,\u01eb. We can show that Xj,\u01eb \u2282 [p\u2217j \u2212 \u03b4, p\u2217j + \u03b4] for some \u03b4 = O( \u221a \u01eb\u03bb).\nRecall that N\u01eb\u03b20(X\u01eb) is the number of feasible cells of virtual width at least \u01eb\u03b20 which overlap with X\u01eb. It follows that N\u01eb\u03b20(X\u01eb) is at most k times the maximal number of feasible cells of diameter at least \u2126(\u01eb/\u03bb) that overlap with an interval of diameter O( \u221a \u01eb\u03bb). Therefore: N\u01eb\u03b20(X\u01eb) = O(k\u03bb3/2\u01eb\u22121/2 log 1\u01eb ). Moreover, we have a less sophisticated upper bound on N\u01eb\u03b20(X\u01eb): it is at most the number of feasible cell of diameter at least \u2126(\u01eb/\u03bb). So N\u01eb\u03b20(X\u01eb) = O(\u03bb/\u01eb)(log 1 \u01eb ). The theorem follows by plugging both upper bounds on N\u01eb\u03b20(X\u01eb) into Equation (6).\nComparison with NonAdaptive. Consider NonAdaptive(\u03c80), where \u03c80 = \u0398\u0303(T \u22121/3) is the granularity required for the optimal worst-case performance. Call a problem instance nice if it has 2-piecewise-uniform costs and \u03bb-bounded density, for some sufficiently large absolute constant \u03bb; say \u03bb = 4 for concreteness. We claim that AgnosticZooming outperforms NonAdaptive(\u03c80) on the \u201cnice\u201d problem instances.\nLemma 8.3. NonAdaptive(\u03c80) achieves regret R(T ) = \u2126(T 2/3) in the worst case over all \u201cnice\u201d problem instances.\nProof Sketch. Recall that for k = 2 the supply distribution has density \u03bb1 on interval [0, p0], and density \u03bb2 on interval [p0, 1], for some numbers \u03bb1, \u03bb2, p0. We pick p0 so that it is sufficiently far from any point in Xcand(\u03c80). Note that the function U(\u00b7) is a parabola on each of the two intervals. We adjust the densities so that U(\u00b7) achieves its maximum at p0, and the maximum of either of the two parabolas is sufficiently far from p0. Then the discretization error of Xcand(\u03c80) is at least \u2126(\u03c80), which implies regret \u2126(\u03c80T ).\nLower bound for NonAdaptive. We provide a specific lower-bounding example for the worstcase performance of NonAdaptive(\u03c8), for an arbitrary \u03c8 > 0. Let F be the family of all problem instances with k-piecewise-uniform costs and \u03bb-bounded density, for all k \u2208 N and \u03bb = 4.\nLemma 8.4. Let R\u03c8(T ) be the maximal regret of NonAdaptive(\u03c8) over all problem instances in F . Then R\u03c8(T ) = \u2126(\u03c8T + \u221a T/\u03c8) \u2265 \u2126(T 2/3).\nProof Sketch. For piecewise-uniform costs, we have F (0) = 0 and F (p) = 1. Assume that the principal derives value v = 1 from each item. Then the expected utility from price p is U(p) = F (p)(1 \u2212 p).\nFix \u03c8 > 0. Use the following problem instance. Let P\u03b4 = [25 , 35 ] \u2229 {4j\u03c8 + \u03b4 : j \u2208 N}. Set U(p) = 14 for each p \u2208 P0. Further, pick some p\u2217 \u2208 P\u03c8/2 and set U(p\u2217) = 14 + \u2126(\u03c8). This defines F (p) for p \u2208 P \u222a {0, 1, p\u2217}. For the rest of the prices, define F (\u00b7) via linear interpolation. This completes the description of the problem instance.\nWe show that X\u03c8 consists of N = \u2126( 1 \u03c8 ) candidate contracts. Therefore, using standard lower-\nbounding arguments for MAB, we obtain R(T |Xcand) \u2265 \u2126( \u221a TN) = \u2126( \u221a T/\u03c8). Further, we show that the discretization error is at least \u2126(\u03c8), implying that R(T ) \u2265 R(T |Xcand) + \u2126(\u03c8T )."}, {"heading": "9 Related work", "text": "This paper is related to three different areas: contract theory, market design for crowdsourcing, and online decision problems. Below we outline connections to each of these areas.\nContract theory. Our model can be viewed as an extension of the classic principal-agent model from contract theory [Laffont and Martimort, 2002]. In the most basic version of the classic model, a single principal interacts with a single agent whose type (specified by a cost function and production function, as described in Section 2) is generally assumed to be known. The principal specifies a contract mapping outcomes to payments that the principal commits to make to the agent. The agent then chooses an action (i.e., effort level) that stochastically results in an outcome in order to maximize his expected utility given the contract. The principal observes the outcome, but cannot directly observe the agent\u2019s effort level, creating a moral hazard problem. The goal of the principal is to design a contract to maximize her own expected utility, which is the difference between the utility she receives from the outcome and the payment she makes. This maximization can be written as a constrained optimization problem, and it can be shown that linear contracts are optimal.\nThe adverse selection variation of the principal-agent problem relaxes the assumption that the agent\u2019s type is known. Most existing literature on the principal-agent problem with adverse selection focuses on applying the revelation principle [Laffont and Martimort, 2002]. In this setting, the principal offers a menu of contracts, and the contract chosen by the agent reveals the agent\u2019s type. The problem of selecting a menu of contracts that maximizes the principal\u2019s expected utility can again be formulated as a constrained optimization.\nOur work differs from the classic setting in that we consider a principal interacting with multiple agents, and the principal may adjust her contract over time in an online manner. Several other authors have considered extensions of the classic model to multiple agents. Levy and Vukina [2002] show that with multiple agents it is optimal to set individual linear contracts for each agent rather than a single uniform contract for all agents, but offer a variety of descriptive explanations for why it is more common to see uniform contracts in practice. Babaioff et al. [2006] consider a setting in which one principal interacts with multiple agents, but observes only a single outcome which is a function of all agents\u2019 effort levels. Misra et al. [2012] consider a variant in which the algorithm must decide both how to set a uniform contract for many agents and how to select a subset of agents to hire.\nAlternative online versions of the problem have been considered in the literature as well. In dynamic principal agent problem [Sannikov, 2008, Williams, 2009, Sannikov, 2012], a single principal interacts with a single agent repeatedly over a period of time. The agent can choose to exert different effort at different time, and the outcome at time t is a function of all the efforts exerts by the agent before t. The principal cannot observe the agent\u2019s efforts but can observe the outcome. The goal of the principal is to design an optimal contract over time to maximize his payoff. Our work is different from this line of work since we consider the setting with multiple agents with different, unknown types. Our algorithm needs to learn the distribution of agent types and design an optimal contract accordingly.\nConitzer and Garera [2006] studies the online principal agent problem with a similar setting to ours. However, they focus on empirically comparing different online algorithms, including bandit approaches with uniform discretization, gradient ascent, and Bayesian update approaches to the problem. Our goal is to provide an algorithm with nice theoretical guarantees.\nBohren and Kravitz [2013] studies the setting when the outcome is unverifiable. To address this issue, they propose to assign a bundle of tasks to each worker. To verify the outcome, each task in the bundle is chosen as a verifiable task with some non-trivial probability. A verifiable task can either be a gold standard task with known answer or a task which is assigned to multiple workers for verification. The payment for a task bundle is then conditional only on the outcome of verified tasks. In our setting, we assume the task outcome is verifiable. We can relax this assumption by adopting similar approaches.\nIncentives in crowdsourcing systems. Researchers have recently begun to examine the design of incentive mechanisms to encourage high-quality work in crowdsourcing systems. Jain et al. [2012] explore ways in which to award virtual points to users in online question-and-answer forums to improve the quality of answers. Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets. While we also consider crowdsourcing markets, our work differs in that it focuses on how to design contracts, perhaps the most natural incentive scheme, to incentivize workers to exert effort.\nThe problem closest to ours which has been studied in the context of crowdsourcing systems is the online task pricing problem in which a requester has an unlimited supply of tasks to be completed and a budget B to spend on them [Badanidiyuru et al., 2012, Singer and Mittal, 2013]. Workers with private costs arrive online, and the requester sets a single price for each arriving worker. The goal is to learn the optimal single fixed price over time. Our work can be viewed as a generalization of the task pricing problem, which is a special case of our setting with the number of non-null outcomes m fixed at 1.\nThere has also been empirical work examining how workers\u2019 behavior varies based on the financial incentives offered in crowdsourcing markets. Mason and Watts [2009] study how workers react to changes of performance-independent financial incentives. In their study, increasing financial incentives increases the number of tasks workers complete, but not the quality of their output. Yin et al. [2013] provide a potential explanation for this phenomenon using the concept of \u201canchoring effect\u201d: a worker\u2019s cost for completing a task is influenced by the first price the worker sees for this task. Horton and Chilton [2010] run experiments to estimate workers\u2019 reservation wage for completing tasks. They show that many workers respond rationally to offered contracts, whereas\nsome of the workers appeared to have some \u201ctarget payment\u201d in mind. Some recent research studies the effects of performance-based payments (PBPs). Harris [2011] runs MTurk experiments on resume screening, where workers can get a bonus if they perform well. He concludes that the quality of work is better with PBPs than with uniform payments. Yin et al. [2013] show that varying the magnitude of the bonus does not have much effect in certain settings. Ho et al. [2015] perform a more comprehensive set of experiments aimed at determining whether, when, and why PBPs increase the quality of submitted work. Their results suggest that PBPs can increase quality on tasks for which increased time or effort leads to higher quality work. Their results also suggest that workers may interpret a contract as performance-based even if it is not stated as such (since requesters always have the option to reject work). Based on this evidence, they propose a new model of worker behavior that extends the principal-agent model to explicitly reflect workers\u2019 subjective beliefs about their likelihood of being paid.\nOverall, previous empirical work demonstrates that workers in crowdsourcing markets do respond to the change of financial incentives, but that their behavior does not always follow the traditional rational-worker model \u2014 similar to people in any real-world market. In our work, we start our analysis with the rational-worker assumption ubiquitous in economic theory, but demonstrate that our results can still hold without these assumptions as long as the collective worker behavior satisfies some natural properties (namely, as long as Lemma 3.1 holds). We note that our results hold under the generalized worker model proposed by Ho et al. [2015], which is consistent with their experimental evidence as discussed above.\nSequential decision problems. In sequential decision problems, an algorithm makes sequential decisions over time.Two directions that are relevant to this paper are multi-armed bandits (MAB) and dynamic pricing.\nMAB have been studied since 1933 [Thompson, 1933] in Operations Research, Economics, and several branches of Computer Science including machine learning, theoretical computer science, AI, and algorithmic economics. A survey of prior work on MAB is beyond the scope of this paper; the reader is encouraged to refer to Cesa-Bianchi and Lugosi [2006] or Bubeck and Cesa-Bianchi [2012] for background on prior-independent MAB, and to Gittins et al. [2011] for background on Bayesian MAB. Below we briefly discuss the lines of work on MAB that are directly relevant to our paper.\nOur setting can be modeled as prior-independent MAB with stochastic rewards: the reward of a given arm i is an i.i.d. sample of some time-invariant distribution, and neither this distribution nor a Bayesian prior on it are known to the algorithm. The basic formulation (with a small number of arms) is well-understood [Lai and Robbins, 1985, Auer et al., 2002, Bubeck and Cesa-Bianchi, 2012]. To handle problems with a large or infinite number of arms, one typically needs side information on similarity between arms. A typical way to model this side information, called Lipschitz MAB Kleinberg et al. [2008], is that an algorithm is given a distance function on the arms, and the expected rewards are assumed to satisfy Lipschitz-continuity (or a relaxation thereof) with respect this distance function, e.g. [Agrawal, 1995, Kleinberg, 2004, Auer et al., 2007, Kleinberg et al., 2008, Bubeck et al., 2011a, Slivkins, 2014]. Most related to this paper is the idea of adaptive discretization which is often used in this setting [Kleinberg et al., 2008, Bubeck et al., 2011a, Slivkins, 2014], and particularly the zooming algorithm [Kleinberg et al., 2008, Slivkins, 2014]. In particular, the general template of our algorithm is similar to the one in the zooming algorithm (but our \u201cselection rule\u201d and \u201czooming rule\u201d are very different, reflecting the lack of a priori known similarity information).\nIn some settings (including ours), the numerical similarity information required for Lipschitz\nMAB is not immediately available. For example, in applications to web search and advertising it is natural to assume that an algorithm can only observe a tree-shaped taxonomy on arms [Kocsis and Szepesvari, 2006, Munos and Coquelin, 2007, Pandey et al., 2007, Slivkins, 2011, Bull, 2013]. In particular, Slivkins [2011] and Bull [2013] explicitly reconstruct (the relevant parts of) the metric space defined by the taxonomy. In a different direction, Bubeck et al. [2011b] study a version of Lipschitz MAB where the Lipschitz constant is not known, and essentially recover the performance of NonAdaptive for this setting.\nDynamic pricing (a.k.a. online posted-price auctions) refers to settings in which a principal interacts with agents that arrive over time and offers each agent a price for a transaction, such as selling or buying an item. The version in which the principal sells items has been extensively studied in Operations Research, typically in a Bayesian setting; see den Boer [2015] for a through literature review. The study of prior-independent, non-parameterized formulations has been initiated in Blum et al. [2003] and Kleinberg and Leighton [2003a] and continued by several others [Besbes and Zeevi, 2009, Babaioff et al., 2015, Besbes and Zeevi, 2012, Wang et al., 2014, Badanidiyuru et al., 2013, 2014]. Further, Badanidiyuru et al. [2012] and Singla and Krause [2013] studied the version in which the principal buys items, or equivalently commissions tasks; we call this version dynamic task pricing. Modulo budget constraints, this is essentially the special case of our setting where in each round a worker is offered the chance to perform a task at a specified price, and can either accept or reject this offer. In particular, the worker\u2019s strategic choice is directly observable. More general settings have been studied in [Badanidiyuru et al., 2013, 2014, Agrawal and Devanur, 2014, Agrawal et al., 2015].14 However, all this work (after the initial papers [Blum et al., 2003, Kleinberg and Leighton, 2003a]) has focused on models with constraints on the principal\u2019s supply or budgets, and does not imply any improved results when specialized to unconstrained settings."}, {"heading": "10 Conclusions", "text": "Motivated by applications to crowdsourcing markets, we define the dynamic contract design problem, a multi-round version of the principal-agent model with unobservable strategic decisions. We treat this problem as a multi-armed bandit problem, design an algorithm for this problem, and derive regret bounds which compare favorably to prior work. Our main conceptual contribution, aside from identifying the model, is the adaptive discretization approach that does not rely on Lipschitz-continuity assumptions. We provably improve on the uniform discretization approach from prior work, both in the general case and in some illustrative special cases. These theoretical results are supported by simulations.\nWe believe that the dynamic contract design problem deserves further study, in several directions that we outline below.\n1. It is not clear whether our provable results can be improved, perhaps using substantially different algorithms and relative to different problem-specific structures. In particular, one needs to establish lower bounds in order to argue about optimality; no lower bounds for dynamic contract design are currently known.\n2. Our adaptive discretization approach may be fine-tuned to improve its performance in practice. In particular, the definition of the \u201cindex\u201d It(C) of a given feasible cell C may be re-defined in\n14Badanidiyuru et al. [2014] and Agrawal and Devanur [2014] is concurrent and independent work with respect to the conference publication of this paper, and Agrawal et al. [2015] is subsequent work.\nseveral different ways. First, it can use the information from C in a more sophisticated way, similar to the more sophisticated indices for the basic K-armed bandit problem; for example, see Garivier and Cappe\u0301 [2011]. Second, the index can incorporate information from other cells. Third, it can be defined in a \u201csmoother\u201d, probabilistic way, e.g., as in Thompson Sampling [Thompson, 1933].\n3. Deeper insights into the structure of the (static) principal-agent problem are needed, primarily in order to optimize the choice of Xcand, the set of candidate contracts. The most natural target here is the uniform mesh Xcand(\u01eb). To optimize the granularity \u01eb, one needs to upper-bound the discretization error OPT(Xcand)\u2212 OPT(Xcand(\u01eb)) in terms of some function f(\u01eb) such that f(\u01eb) \u2192 0 as \u01eb \u2192 0. The first-order open question is to resolve whether this can be done in the general case, or provide a specific example when it cannot. A related open question concerns the effect of increasing the granularity: upper-bound the difference OPT(Xcand(\u01eb))\u2212 OPT(Xcand(\u01eb\u2032)), \u01eb > \u01eb\u2032 > 0, in terms of some function of \u01eb and \u01eb\u2032. Further, it is not known whether the optimal mesh of contracts is in fact a uniform mesh.\nAlso of interest is the effect of restricting our attention to monotone contracts. While we prove that monotone contracts may not be optimal (Appendix A), the significance of this phenomenon is unclear. One would like to characterize the scenarios when restricting to monotone contracts is alright (in the sense that the best monotone contract is as good, or not much worse, than the best contract), and the scenarios when this restriction results in a significant loss. For the latter scenarios, different algorithms may be needed.\n4. A much more extensive analysis of special cases is in order. Our general results are difficult to access (which appears to be an inherent property of the general problem), so the most immediate direction for special cases is deriving lucid corollaries from the current regret bounds. In particular, it is desirable to optimize the choice of candidate contracts. Apart from \u201cmassaging\u201d the current results, one can also design improved algorithms and derive specialized lower bounds. Particularly appealing special cases concern supply distributions that are mixtures of a small number of types, and supply distributions that belong to a (simple) parameterized family with unknown parameter.\nGoing beyond our current model, a natural direction is to incorporate a budget constraint, extending the corresponding results on dynamic task pricing. The main difficulty for such settings is that a distribution over two contracts may perform much better than any fixed contract; see Badanidiyuru et al. [2013] for discussion. Effectively, an algorithm needs to optimize over the distributions. As a first step, one can use non-adaptive discretization in conjunction with the general algorithms for bandits with budget constraints (sometimes called \u201cbandits with knapsacks\u201d [Badanidiyuru et al., 2013, Agrawal and Devanur, 2014]). However, it is not clear how to choose an optimal mesh of contracts (as we discussed throughout the paper), and this mesh is not likely to be uniform (because it is not uniform for the special case of dynamic task pricing with a budget [Badanidiyuru et al., 2013]). The eventual target in this research direction is to marry adaptive discretization and the techniques from prior work on \u201cbandits with knapsacks.\u201d"}, {"heading": "A Monotone contracts may not be optimal", "text": "In this section we provide an example of a problem instance for which all monotone contracts are suboptimal (at least when restricting attention to only those contracts with non-negative payoffs). In this example, there are three non-null outcomes (i.e., m = 3), and two non-null effort levels, \u201clow\u201d effort and \u201chigh\u201d effort, which we denote e\u2113 and eh respectively. There is only a single worker type. Since there is only one type, we drop the subscript when describing the cost function c. We let c(e\u2113) = 0, and let c(eh) be any positive value less than 0.5(v(2)\u2212 v(1)). If a worker chooses low effort, the outcome is equally likely to be 1 or 3. If the worker chooses high effort, it is equally likely to be 2 or 3. It is easy to verify that this type satisfies the FOSD assumption. Finally, for simplicity, we assume that all workers break ties between high effort and any other effort level in favor of high effort, and that all workers break ties between low effort and the null effort level in favor of low effort.\nLet\u2019s consider the optimal contract. Since there is just a single worker type and all workers of this type break ties in the same way, we can consider separately the best contract that would make all workers choose the null effort level, the best contract that would make all workers choose low effort, and the best contract that would make all workers choose high effort, and compare the requester\u2019s expected value for each.\nSince c(e\u2113) = 0 and workers break ties between low effort and null effort in favor of low effort, there is no contract that would cause workers to choose null effort; workers always prefer low effort to null effort.\nIt is easy to see that the best contract (in terms of requester expected value) that would make workers choose low effort would set x(1) = x(3) = 0 and x(2) sufficiently low that workers would not be enticed to choose high effort; setting x(2) = 0 is sufficient. In this case, the expected value of the requester would be 0.5(v(1) + v(3)).\nNow let\u2019s consider contracts that cause workers to choose high effort. If a worker chooses high effort, the expected value to the requester is\n0.5(v(2) \u2212 x(2) + v(3) \u2212 x(3)). (23)\nWorkers will choose high effort if and only if\n0.5(x(1) + x(3)) \u2264 0.5(x(2) + x(3)) \u2212 c(eh)\nor 0.5x(1) \u2264 0.5x(2) \u2212 c(eh). (24)\nSo to find the contract that maximizes the requester\u2019s expected value when workers choose high effort, we want to maximize Equation 23 subject to the constraint in Equation 24. Since x(3) doesn\u2019t appear in Equation 24, we can set it to 0 to maximize Equation 23. Since x(1) does not appear in Equation 23, we can set x(1) = 0 to make Equation 24 as easy as possible to satisfy. We can then see that the optimal occurs when x(2) = 2c(eh).\nPlugging this contact x into Equation 23, the expected utility in this case is 0.5(v(2) + v(3))\u2212 c(eh). Since we assumed that c(eh) < 0.5(v(2) \u2212 v(1))), this is strictly preferable to the constant 0 contract, and is in fact the unique optimal contract. Since x(2) > x(3), the unique optimal contract is not monotonic."}], "references": [{"title": "The continuum-armed bandit problem", "author": ["Rajeev Agrawal"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Agrawal.,? \\Q1926\\E", "shortCiteRegEx": "Agrawal.", "year": 1926}, {"title": "Bandits with concave rewards and convex knapsacks", "author": ["Shipra Agrawal", "Nikhil R. Devanur"], "venue": null, "citeRegEx": "Agrawal and Devanur.,? \\Q1995\\E", "shortCiteRegEx": "Agrawal and Devanur.", "year": 1995}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2015}, {"title": "Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi. Prediction, learning, and games", "author": ["Conitzer", "Nikesh Garera"], "venue": "Intl. Conf. on Algorithmic Learning Theory (ALT),", "citeRegEx": "Conitzer and Garera.,? \\Q2013\\E", "shortCiteRegEx": "Conitzer and Garera.", "year": 2013}, {"title": "Efficient optimal leanring for contextual bandits", "author": ["Miroslav Dudik", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "In 27th Conf. on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Dudik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dudik et al\\.", "year": 2011}, {"title": "The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond", "author": ["Aur\u00e9lien Garivier", "Olivier Capp\u00e9"], "venue": "In 24th Conf. on Learning Theory (COLT),", "citeRegEx": "Garivier and Capp\u00e9.,? \\Q2011\\E", "shortCiteRegEx": "Garivier and Capp\u00e9.", "year": 2011}, {"title": "A game-theoretic analysis of rank-order mechanisms for user-generated content", "author": ["Arpita Ghosh", "Patrick Hummel"], "venue": "In EC,", "citeRegEx": "Ghosh and Hummel.,? \\Q2011\\E", "shortCiteRegEx": "Ghosh and Hummel.", "year": 2011}, {"title": "Learning and incentives in user-generated content: Multi-armed bandits with endogenous arms", "author": ["Arpita Ghosh", "Patrick Hummel"], "venue": "In ICTS,", "citeRegEx": "Ghosh and Hummel.,? \\Q2013\\E", "shortCiteRegEx": "Ghosh and Hummel.", "year": 2013}, {"title": "Incentivizing high-quality user-generated content", "author": ["Arpita Ghosh", "Preston McAfee"], "venue": "In WWW,", "citeRegEx": "Ghosh and McAfee.,? \\Q2011\\E", "shortCiteRegEx": "Ghosh and McAfee.", "year": 2011}, {"title": "Multi-Armed Bandit Allocation Indices", "author": ["John Gittins", "Kevin Glazebrook", "Richard Weber"], "venue": null, "citeRegEx": "Gittins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gittins et al\\.", "year": 2011}, {"title": "You\u2019re hired! an examination of crowdsourcing incentive models in human resource tasks", "author": ["Christopher G. Harris"], "venue": "In CSDM,", "citeRegEx": "Harris.,? \\Q2011\\E", "shortCiteRegEx": "Harris.", "year": 2011}, {"title": "Towards social norm design for crowdsourcing markets", "author": ["Chien-Ju Ho", "Yu Zhang", "Jennifer Wortman Vaughan", "Mihaela van der Schaar"], "venue": "HCOMP,", "citeRegEx": "Ho et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2012}, {"title": "Incentivizing high quality crowdwork", "author": ["Chien-Ju Ho", "Aleksandrs Slivkins", "Siddharth Suri", "Jennifer Wortman Vaughan"], "venue": "In 24th Intl. World Wide Web Conf. (WWW),", "citeRegEx": "Ho et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2015}, {"title": "The labor economics of paid crowdsourcing", "author": ["John J. Horton", "Lydia B. Chilton"], "venue": "In EC,", "citeRegEx": "Horton and Chilton.,? \\Q2010\\E", "shortCiteRegEx": "Horton and Chilton.", "year": 2010}, {"title": "Designing incentives for online question-and-answer forums", "author": ["Shaili Jain", "Yiling Chen", "David Parkes"], "venue": "Games and Economic Behavior,", "citeRegEx": "Jain et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2012}, {"title": "Nearly tight bounds for the continuum-armed bandit problem", "author": ["Robert Kleinberg"], "venue": "In 18th Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Kleinberg.,? \\Q2004\\E", "shortCiteRegEx": "Kleinberg.", "year": 2004}, {"title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions", "author": ["Robert Kleinberg", "Tom Leighton"], "venue": "IEEE Symp. on Foundations of Computer Science (FOCS),", "citeRegEx": "Kleinberg and Leighton.,? \\Q2003\\E", "shortCiteRegEx": "Kleinberg and Leighton.", "year": 2003}, {"title": "Multi-armed bandits in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "venue": "In 40th ACM Symp. on Theory of Computing (STOC),", "citeRegEx": "Kleinberg et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2008}, {"title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions", "author": ["Robert D. Kleinberg", "Frank T. Leighton"], "venue": "In IEEE Symp. on Foundations of Computer Science (FOCS),", "citeRegEx": "Kleinberg and Leighton.,? \\Q2003\\E", "shortCiteRegEx": "Kleinberg and Leighton.", "year": 2003}, {"title": "Bandit Based Monte-Carlo Planning", "author": ["Levente Kocsis", "Csaba Szepesvari"], "venue": "European Conf. on Machine Learning (ECML),", "citeRegEx": "Kocsis and Szepesvari.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesvari.", "year": 2006}, {"title": "The Theory of Incentives: The Principal-Agent Model", "author": ["Jean-Jacques Laffont", "David Martimort"], "venue": null, "citeRegEx": "Laffont and Martimort.,? \\Q2002\\E", "shortCiteRegEx": "Laffont and Martimort.", "year": 2002}, {"title": "Asymptotically efficient Adaptive Allocation Rules", "author": ["Tze Leung Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "Optimal linear contracts with heterogeneous agents", "author": ["Armando Levy", "Tomislav Vukina"], "venue": "In European Review of Agricultural Economics,", "citeRegEx": "Levy and Vukina.,? \\Q2002\\E", "shortCiteRegEx": "Levy and Vukina.", "year": 2002}, {"title": "Financial incentives and the \u201cperformance of crowds", "author": ["Winter Mason", "Duncan Watts"], "venue": "HCOMP,", "citeRegEx": "Mason and Watts.,? \\Q2009\\E", "shortCiteRegEx": "Mason and Watts.", "year": 2009}, {"title": "Reputation-based incentive protocols in crowdsourcing applications", "author": ["Yu Zhang", "Mihaela van der Schaar"], "venue": null, "citeRegEx": "Zhang and Schaar.,? \\Q2013\\E", "shortCiteRegEx": "Zhang and Schaar.", "year": 2013}], "referenceMentions": [{"referenceID": 20, "context": "First, our model can be viewed as a multi-round version of the classical principal-agent model from contract theory [Laffont and Martimort, 2002].", "startOffset": 116, "endOffset": 145}, {"referenceID": 17, "context": "The closest line of work is that on Lipschitz MAB [Kleinberg et al., 2008], in which the algorithm is given a distance function on the arms, and the expected rewards of the arms are assumed to satisfy Lipschitz-continuity (or a relaxation thereof) with respect to this distance function, [Agrawal, 1995, Kleinberg, 2004, Auer et al.", "startOffset": 50, "endOffset": 74}, {"referenceID": 15, "context": "[Kleinberg and Leighton, 2003b, Badanidiyuru et al., 2012, Singer and Mittal, 2013, Singla and Krause, 2013, Badanidiyuru et al., 2013]. In particular, Badanidiyuru et al. [2012] and Singla and Krause [2013] study a version of our setting with simple, single-price contracts (independent of the output), where the focus is on dealing with a global budget constraint.", "startOffset": 1, "endOffset": 179}, {"referenceID": 15, "context": "[Kleinberg and Leighton, 2003b, Badanidiyuru et al., 2012, Singer and Mittal, 2013, Singla and Krause, 2013, Badanidiyuru et al., 2013]. In particular, Badanidiyuru et al. [2012] and Singla and Krause [2013] study a version of our setting with simple, single-price contracts (independent of the output), where the focus is on dealing with a global budget constraint.", "startOffset": 1, "endOffset": 208}, {"referenceID": 20, "context": "As described above, this is a version of the standard principal-agent model [Laffont and Martimort, 2002].", "startOffset": 76, "endOffset": 105}, {"referenceID": 15, "context": "The special case m = 1 is equivalent to the dynamic pricing problem from Kleinberg and Leighton [2003a]; we obtain improved results for it, too.", "startOffset": 73, "endOffset": 104}, {"referenceID": 4, "context": ", Dudik et al. [2011].", "startOffset": 2, "endOffset": 22}, {"referenceID": 15, "context": "The width dimension is similar to the \u201czooming dimension\u201d in Kleinberg et al. [2008] and \u201cnear-optimality dimension\u201d in Bubeck et al.", "startOffset": 61, "endOffset": 85}, {"referenceID": 15, "context": "The width dimension is similar to the \u201czooming dimension\u201d in Kleinberg et al. [2008] and \u201cnear-optimality dimension\u201d in Bubeck et al. [2011a] in the work on \u201cbandits in metric spaces\u201d.", "startOffset": 61, "endOffset": 142}, {"referenceID": 15, "context": "If an algorithm is given this function D (call such algorithm D-aware), the machinery from \u201cbandits in metric spaces\u201d Kleinberg et al. [2008], Bubeck et al.", "startOffset": 118, "endOffset": 142}, {"referenceID": 15, "context": "If an algorithm is given this function D (call such algorithm D-aware), the machinery from \u201cbandits in metric spaces\u201d Kleinberg et al. [2008], Bubeck et al. [2011a] can be used to perform adaptive discretization and obtain a significant advantage over NonAdaptive.", "startOffset": 118, "endOffset": 165}, {"referenceID": 15, "context": "Given this shape of D, let us state the regret bounds for D-aware algorithms in Kleinberg et al. [2008] and Bubeck et al.", "startOffset": 80, "endOffset": 104}, {"referenceID": 15, "context": "Given this shape of D, let us state the regret bounds for D-aware algorithms in Kleinberg et al. [2008] and Bubeck et al. [2011a]. To simplify the notation, we assume that the action space is restricted to Xcand.", "startOffset": 80, "endOffset": 130}, {"referenceID": 15, "context": "For a more precise comparison, we focus on the results in Kleinberg et al. [2008] .", "startOffset": 58, "endOffset": 82}, {"referenceID": 15, "context": "For a more precise comparison, we focus on the results in Kleinberg et al. [2008] . (The regret bounds in Bubeck et al. [2011a] are very similar in spirit, but are stated in terms of a slightly different structure.", "startOffset": 58, "endOffset": 128}, {"referenceID": 15, "context": "For a more precise comparison, we focus on the results in Kleinberg et al. [2008] . (The regret bounds in Bubeck et al. [2011a] are very similar in spirit, but are stated in terms of a slightly different structure.) The \u201ccovering-type\u201d regret bound in Kleinberg et al. [2008] focuses on balls of radius at most \u01eb according to distance D, so that N\u2217 \u01eb (Y ) is the smallest number of such balls that is sufficient to cover Y .", "startOffset": 58, "endOffset": 276}, {"referenceID": 15, "context": "We can upper-bound the discretization error using a standard approach from the work on dynamic pricing Kleinberg and Leighton [2003b]. Fix discretization granularity \u03c8 > 0.", "startOffset": 103, "endOffset": 134}, {"referenceID": 15, "context": "Worst-case regret bounds are implicit in prior work on dynamic inventory-pricing [Kleinberg and Leighton, 2003a].13 Let NonAdaptive(\u03c8) denote algorithm NonAdaptive with Xcand = Xcand(\u03c8). Then, by the analysis in Kleinberg and Leighton [2003a], NonAdaptive(\u03c8) achieves regret R(T ) = \u00d5(\u03c8T + \u03c8\u22122).", "startOffset": 82, "endOffset": 243}, {"referenceID": 15, "context": "The algorithmic result for dynamic task pricing is an easy modification of the analysis in Kleinberg and Leighton [2003a] for dynamic inventory-pricing.", "startOffset": 91, "endOffset": 122}, {"referenceID": 15, "context": "The algorithmic result for dynamic task pricing is an easy modification of the analysis in Kleinberg and Leighton [2003a] for dynamic inventory-pricing. The lower bound in in Kleinberg and Leighton [2003a] can also be \u201ctranslated\u201d from dynamic inventory-pricing to dynamic task pricing without introducing any new ideas.", "startOffset": 91, "endOffset": 206}, {"referenceID": 20, "context": "Our model can be viewed as an extension of the classic principal-agent model from contract theory [Laffont and Martimort, 2002].", "startOffset": 98, "endOffset": 127}, {"referenceID": 20, "context": "Most existing literature on the principal-agent problem with adverse selection focuses on applying the revelation principle [Laffont and Martimort, 2002].", "startOffset": 124, "endOffset": 153}, {"referenceID": 20, "context": "Our model can be viewed as an extension of the classic principal-agent model from contract theory [Laffont and Martimort, 2002]. In the most basic version of the classic model, a single principal interacts with a single agent whose type (specified by a cost function and production function, as described in Section 2) is generally assumed to be known. The principal specifies a contract mapping outcomes to payments that the principal commits to make to the agent. The agent then chooses an action (i.e., effort level) that stochastically results in an outcome in order to maximize his expected utility given the contract. The principal observes the outcome, but cannot directly observe the agent\u2019s effort level, creating a moral hazard problem. The goal of the principal is to design a contract to maximize her own expected utility, which is the difference between the utility she receives from the outcome and the payment she makes. This maximization can be written as a constrained optimization problem, and it can be shown that linear contracts are optimal. The adverse selection variation of the principal-agent problem relaxes the assumption that the agent\u2019s type is known. Most existing literature on the principal-agent problem with adverse selection focuses on applying the revelation principle [Laffont and Martimort, 2002]. In this setting, the principal offers a menu of contracts, and the contract chosen by the agent reveals the agent\u2019s type. The problem of selecting a menu of contracts that maximizes the principal\u2019s expected utility can again be formulated as a constrained optimization. Our work differs from the classic setting in that we consider a principal interacting with multiple agents, and the principal may adjust her contract over time in an online manner. Several other authors have considered extensions of the classic model to multiple agents. Levy and Vukina [2002] show that with multiple agents it is optimal to set individual linear contracts for each agent rather than a single uniform contract for all agents, but offer a variety of descriptive explanations for why it is more common to see uniform contracts in practice.", "startOffset": 99, "endOffset": 1900}, {"referenceID": 20, "context": "Our model can be viewed as an extension of the classic principal-agent model from contract theory [Laffont and Martimort, 2002]. In the most basic version of the classic model, a single principal interacts with a single agent whose type (specified by a cost function and production function, as described in Section 2) is generally assumed to be known. The principal specifies a contract mapping outcomes to payments that the principal commits to make to the agent. The agent then chooses an action (i.e., effort level) that stochastically results in an outcome in order to maximize his expected utility given the contract. The principal observes the outcome, but cannot directly observe the agent\u2019s effort level, creating a moral hazard problem. The goal of the principal is to design a contract to maximize her own expected utility, which is the difference between the utility she receives from the outcome and the payment she makes. This maximization can be written as a constrained optimization problem, and it can be shown that linear contracts are optimal. The adverse selection variation of the principal-agent problem relaxes the assumption that the agent\u2019s type is known. Most existing literature on the principal-agent problem with adverse selection focuses on applying the revelation principle [Laffont and Martimort, 2002]. In this setting, the principal offers a menu of contracts, and the contract chosen by the agent reveals the agent\u2019s type. The problem of selecting a menu of contracts that maximizes the principal\u2019s expected utility can again be formulated as a constrained optimization. Our work differs from the classic setting in that we consider a principal interacting with multiple agents, and the principal may adjust her contract over time in an online manner. Several other authors have considered extensions of the classic model to multiple agents. Levy and Vukina [2002] show that with multiple agents it is optimal to set individual linear contracts for each agent rather than a single uniform contract for all agents, but offer a variety of descriptive explanations for why it is more common to see uniform contracts in practice. Babaioff et al. [2006] consider a setting in which one principal interacts with multiple agents, but observes only a single outcome which is a function of all agents\u2019 effort levels.", "startOffset": 99, "endOffset": 2184}, {"referenceID": 20, "context": "Our model can be viewed as an extension of the classic principal-agent model from contract theory [Laffont and Martimort, 2002]. In the most basic version of the classic model, a single principal interacts with a single agent whose type (specified by a cost function and production function, as described in Section 2) is generally assumed to be known. The principal specifies a contract mapping outcomes to payments that the principal commits to make to the agent. The agent then chooses an action (i.e., effort level) that stochastically results in an outcome in order to maximize his expected utility given the contract. The principal observes the outcome, but cannot directly observe the agent\u2019s effort level, creating a moral hazard problem. The goal of the principal is to design a contract to maximize her own expected utility, which is the difference between the utility she receives from the outcome and the payment she makes. This maximization can be written as a constrained optimization problem, and it can be shown that linear contracts are optimal. The adverse selection variation of the principal-agent problem relaxes the assumption that the agent\u2019s type is known. Most existing literature on the principal-agent problem with adverse selection focuses on applying the revelation principle [Laffont and Martimort, 2002]. In this setting, the principal offers a menu of contracts, and the contract chosen by the agent reveals the agent\u2019s type. The problem of selecting a menu of contracts that maximizes the principal\u2019s expected utility can again be formulated as a constrained optimization. Our work differs from the classic setting in that we consider a principal interacting with multiple agents, and the principal may adjust her contract over time in an online manner. Several other authors have considered extensions of the classic model to multiple agents. Levy and Vukina [2002] show that with multiple agents it is optimal to set individual linear contracts for each agent rather than a single uniform contract for all agents, but offer a variety of descriptive explanations for why it is more common to see uniform contracts in practice. Babaioff et al. [2006] consider a setting in which one principal interacts with multiple agents, but observes only a single outcome which is a function of all agents\u2019 effort levels. Misra et al. [2012] consider a variant in which the algorithm must decide both how to set a uniform contract for many agents and how to select a subset of agents to hire.", "startOffset": 99, "endOffset": 2363}, {"referenceID": 3, "context": "Conitzer and Garera [2006] studies the online principal agent problem with a similar setting to ours.", "startOffset": 0, "endOffset": 27}, {"referenceID": 3, "context": "Conitzer and Garera [2006] studies the online principal agent problem with a similar setting to ours. However, they focus on empirically comparing different online algorithms, including bandit approaches with uniform discretization, gradient ascent, and Bayesian update approaches to the problem. Our goal is to provide an algorithm with nice theoretical guarantees. Bohren and Kravitz [2013] studies the setting when the outcome is unverifiable.", "startOffset": 0, "endOffset": 393}, {"referenceID": 8, "context": "Jain et al. [2012] explore ways in which to award virtual points to users in online question-and-answer forums to improve the quality of answers.", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.", "startOffset": 0, "endOffset": 58}, {"referenceID": 6, "context": "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets.", "startOffset": 0, "endOffset": 257}, {"referenceID": 6, "context": "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets.", "startOffset": 0, "endOffset": 293}, {"referenceID": 6, "context": "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets. While we also consider crowdsourcing markets, our work differs in that it focuses on how to design contracts, perhaps the most natural incentive scheme, to incentivize workers to exert effort. The problem closest to ours which has been studied in the context of crowdsourcing systems is the online task pricing problem in which a requester has an unlimited supply of tasks to be completed and a budget B to spend on them [Badanidiyuru et al., 2012, Singer and Mittal, 2013]. Workers with private costs arrive online, and the requester sets a single price for each arriving worker. The goal is to learn the optimal single fixed price over time. Our work can be viewed as a generalization of the task pricing problem, which is a special case of our setting with the number of non-null outcomes m fixed at 1. There has also been empirical work examining how workers\u2019 behavior varies based on the financial incentives offered in crowdsourcing markets. Mason and Watts [2009] study how workers react to changes of performance-independent financial incentives.", "startOffset": 0, "endOffset": 1402}, {"referenceID": 6, "context": "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets. While we also consider crowdsourcing markets, our work differs in that it focuses on how to design contracts, perhaps the most natural incentive scheme, to incentivize workers to exert effort. The problem closest to ours which has been studied in the context of crowdsourcing systems is the online task pricing problem in which a requester has an unlimited supply of tasks to be completed and a budget B to spend on them [Badanidiyuru et al., 2012, Singer and Mittal, 2013]. Workers with private costs arrive online, and the requester sets a single price for each arriving worker. The goal is to learn the optimal single fixed price over time. Our work can be viewed as a generalization of the task pricing problem, which is a special case of our setting with the number of non-null outcomes m fixed at 1. There has also been empirical work examining how workers\u2019 behavior varies based on the financial incentives offered in crowdsourcing markets. Mason and Watts [2009] study how workers react to changes of performance-independent financial incentives. In their study, increasing financial incentives increases the number of tasks workers complete, but not the quality of their output. Yin et al. [2013] provide a potential explanation for this phenomenon using the concept of \u201canchoring effect\u201d: a worker\u2019s cost for completing a task is influenced by the first price the worker sees for this task.", "startOffset": 0, "endOffset": 1637}, {"referenceID": 6, "context": "Ghosh and Hummel [2011, 2013] and Ghosh and McAfee [2011] study how to distribute user generated content (e.g., Youtube videos) to users to encourage the production of high-quality internet content by people who are motivated by attention. Ho et al. [2012] and Zhang and van der Schaar [2012] consider the design of two-sided reputation systems to encourage good behavior from both workers and requesters in crowdsourcing markets. While we also consider crowdsourcing markets, our work differs in that it focuses on how to design contracts, perhaps the most natural incentive scheme, to incentivize workers to exert effort. The problem closest to ours which has been studied in the context of crowdsourcing systems is the online task pricing problem in which a requester has an unlimited supply of tasks to be completed and a budget B to spend on them [Badanidiyuru et al., 2012, Singer and Mittal, 2013]. Workers with private costs arrive online, and the requester sets a single price for each arriving worker. The goal is to learn the optimal single fixed price over time. Our work can be viewed as a generalization of the task pricing problem, which is a special case of our setting with the number of non-null outcomes m fixed at 1. There has also been empirical work examining how workers\u2019 behavior varies based on the financial incentives offered in crowdsourcing markets. Mason and Watts [2009] study how workers react to changes of performance-independent financial incentives. In their study, increasing financial incentives increases the number of tasks workers complete, but not the quality of their output. Yin et al. [2013] provide a potential explanation for this phenomenon using the concept of \u201canchoring effect\u201d: a worker\u2019s cost for completing a task is influenced by the first price the worker sees for this task. Horton and Chilton [2010] run experiments to estimate workers\u2019 reservation wage for completing tasks.", "startOffset": 0, "endOffset": 1858}, {"referenceID": 10, "context": "Harris [2011] runs MTurk experiments on resume screening, where workers can get a bonus if they perform well.", "startOffset": 0, "endOffset": 14}, {"referenceID": 10, "context": "Harris [2011] runs MTurk experiments on resume screening, where workers can get a bonus if they perform well. He concludes that the quality of work is better with PBPs than with uniform payments. Yin et al. [2013] show that varying the magnitude of the bonus does not have much effect in certain settings.", "startOffset": 0, "endOffset": 214}, {"referenceID": 10, "context": "Harris [2011] runs MTurk experiments on resume screening, where workers can get a bonus if they perform well. He concludes that the quality of work is better with PBPs than with uniform payments. Yin et al. [2013] show that varying the magnitude of the bonus does not have much effect in certain settings. Ho et al. [2015] perform a more comprehensive set of experiments aimed at determining whether, when, and why PBPs increase the quality of submitted work.", "startOffset": 0, "endOffset": 323}, {"referenceID": 10, "context": "Harris [2011] runs MTurk experiments on resume screening, where workers can get a bonus if they perform well. He concludes that the quality of work is better with PBPs than with uniform payments. Yin et al. [2013] show that varying the magnitude of the bonus does not have much effect in certain settings. Ho et al. [2015] perform a more comprehensive set of experiments aimed at determining whether, when, and why PBPs increase the quality of submitted work. Their results suggest that PBPs can increase quality on tasks for which increased time or effort leads to higher quality work. Their results also suggest that workers may interpret a contract as performance-based even if it is not stated as such (since requesters always have the option to reject work). Based on this evidence, they propose a new model of worker behavior that extends the principal-agent model to explicitly reflect workers\u2019 subjective beliefs about their likelihood of being paid. Overall, previous empirical work demonstrates that workers in crowdsourcing markets do respond to the change of financial incentives, but that their behavior does not always follow the traditional rational-worker model \u2014 similar to people in any real-world market. In our work, we start our analysis with the rational-worker assumption ubiquitous in economic theory, but demonstrate that our results can still hold without these assumptions as long as the collective worker behavior satisfies some natural properties (namely, as long as Lemma 3.1 holds). We note that our results hold under the generalized worker model proposed by Ho et al. [2015], which is consistent with their experimental evidence as discussed above.", "startOffset": 0, "endOffset": 1608}, {"referenceID": 7, "context": "A survey of prior work on MAB is beyond the scope of this paper; the reader is encouraged to refer to Cesa-Bianchi and Lugosi [2006] or Bubeck and Cesa-Bianchi [2012] for background on prior-independent MAB, and to Gittins et al. [2011] for background on Bayesian MAB.", "startOffset": 215, "endOffset": 237}, {"referenceID": 1, "context": "The basic formulation (with a small number of arms) is well-understood [Lai and Robbins, 1985, Auer et al., 2002, Bubeck and Cesa-Bianchi, 2012]. To handle problems with a large or infinite number of arms, one typically needs side information on similarity between arms. A typical way to model this side information, called Lipschitz MAB Kleinberg et al. [2008], is that an algorithm is given a distance function on the arms, and the expected rewards are assumed to satisfy Lipschitz-continuity (or a relaxation thereof) with respect this distance function, e.", "startOffset": 95, "endOffset": 362}, {"referenceID": 19, "context": "For example, in applications to web search and advertising it is natural to assume that an algorithm can only observe a tree-shaped taxonomy on arms [Kocsis and Szepesvari, 2006, Munos and Coquelin, 2007, Pandey et al., 2007, Slivkins, 2011, Bull, 2013]. In particular, Slivkins [2011] and Bull [2013] explicitly reconstruct (the relevant parts of) the metric space defined by the taxonomy.", "startOffset": 150, "endOffset": 286}, {"referenceID": 19, "context": "For example, in applications to web search and advertising it is natural to assume that an algorithm can only observe a tree-shaped taxonomy on arms [Kocsis and Szepesvari, 2006, Munos and Coquelin, 2007, Pandey et al., 2007, Slivkins, 2011, Bull, 2013]. In particular, Slivkins [2011] and Bull [2013] explicitly reconstruct (the relevant parts of) the metric space defined by the taxonomy.", "startOffset": 150, "endOffset": 302}, {"referenceID": 19, "context": "For example, in applications to web search and advertising it is natural to assume that an algorithm can only observe a tree-shaped taxonomy on arms [Kocsis and Szepesvari, 2006, Munos and Coquelin, 2007, Pandey et al., 2007, Slivkins, 2011, Bull, 2013]. In particular, Slivkins [2011] and Bull [2013] explicitly reconstruct (the relevant parts of) the metric space defined by the taxonomy. In a different direction, Bubeck et al. [2011b] study a version of Lipschitz MAB where the Lipschitz constant is not known, and essentially recover the performance of NonAdaptive for this setting.", "startOffset": 150, "endOffset": 439}, {"referenceID": 13, "context": "[2003] and Kleinberg and Leighton [2003a] and continued by several others [Besbes and Zeevi, 2009, Babaioff et al.", "startOffset": 11, "endOffset": 42}, {"referenceID": 13, "context": "[2003] and Kleinberg and Leighton [2003a] and continued by several others [Besbes and Zeevi, 2009, Babaioff et al., 2015, Besbes and Zeevi, 2012, Wang et al., 2014, Badanidiyuru et al., 2013, 2014]. Further, Badanidiyuru et al. [2012] and Singla and Krause [2013] studied the version in which the principal buys items, or equivalently commissions tasks; we call this version dynamic task pricing.", "startOffset": 11, "endOffset": 235}, {"referenceID": 13, "context": "[2003] and Kleinberg and Leighton [2003a] and continued by several others [Besbes and Zeevi, 2009, Babaioff et al., 2015, Besbes and Zeevi, 2012, Wang et al., 2014, Badanidiyuru et al., 2013, 2014]. Further, Badanidiyuru et al. [2012] and Singla and Krause [2013] studied the version in which the principal buys items, or equivalently commissions tasks; we call this version dynamic task pricing.", "startOffset": 11, "endOffset": 264}, {"referenceID": 0, "context": "[2014] and Agrawal and Devanur [2014] is concurrent and independent work with respect to the conference publication of this paper, and Agrawal et al.", "startOffset": 11, "endOffset": 38}, {"referenceID": 0, "context": "[2014] and Agrawal and Devanur [2014] is concurrent and independent work with respect to the conference publication of this paper, and Agrawal et al. [2015] is subsequent work.", "startOffset": 11, "endOffset": 157}, {"referenceID": 5, "context": "First, it can use the information from C in a more sophisticated way, similar to the more sophisticated indices for the basic K-armed bandit problem; for example, see Garivier and Capp\u00e9 [2011]. Second, the index can incorporate information from other cells.", "startOffset": 167, "endOffset": 193}], "year": 2014, "abstractText": "Crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. The payment for a particular task is typically set by the task\u2019s requester, and may be adjusted based on the quality of the completed work, for example, through the use of \u201cbonus\u201d payments. In this paper, we study the requester\u2019s problem of dynamically adjusting quality-contingent payments for tasks. We consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. In particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work. We treat this problem as a multi-armed bandit problem, with each \u201carm\u201d representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature). Our results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing. ACM Categories and subject descriptors: F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems; F.1.2 [Computation by Abstract Devices]: Modes of Computation\u2014Online computation; J.4 [Social and Behavioral Sciences]: Economics", "creator": "gnuplot 4.2 patchlevel 6 "}}}