{"id": "1405.3162", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2014", "title": "Circulant Binary Embedding", "abstract": "binary embedding of high - dimensional data cannot requires long codes to dramatically preserve the higher discriminative numerical power of the larger input space. traditional topological binary coding methods nearly often directly suffer from with very high computation and storage costs in such a scenario. to physically address this problem, we propose 2d circulant binary embedding ( cbe ) which generates smaller binary codes by projecting the data with a circulant matrix. the circulant structure enables the possible use of fast fourier transformation to speed clean up the computation. compared positively to methods that use unstructured matrices, the proposed method always improves the sampling time complexity from $ \\ mathcal { o } ( d ^ ^ 2 ) $ to $ \\ mathcal { o } ( d \\ log { d } ) $, and the space complexity from $ \\ mathcal { o } ( d ^ 2 ) $ to $ \\ mathcal { d o } ( d ) $ where $ d $ is the input dimensionality. we also propose conducting a novel time - frequency alternating optimization to learn data - dependent circulant projections, which alternatively minimizes the theoretical objective in original and fourier domains. we show by extensive experiments that the proposed approach approach gives much better performance than the state - of - the - art approaches for fixed fixed time, and often provides much faster computation processes with no performance degradation for fixed number of vector bits.", "histories": [["v1", "Tue, 13 May 2014 14:17:11 GMT  (322kb)", "http://arxiv.org/abs/1405.3162v1", "ICML 2014"]], "COMMENTS": "ICML 2014", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["felix x yu", "sanjiv kumar", "yunchao gong", "shih-fu chang"], "accepted": true, "id": "1405.3162"}, "pdf": {"name": "1405.3162.pdf", "metadata": {"source": "META", "title": "Circulant Binary Embedding", "authors": ["Felix X. Yu", "Yunchao Gong", "Shih-Fu Chang"], "emails": ["YUXINNAN@EE.COLUMBIA.EDU", "SANJIVK@GOOGLE.COM", "YUNCHAO@CS.UNC.EDU", "SFCHANG@EE.COLUMBIA.EDU"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n31 62\nv1 [\nst at\n.M L\n] 1\n3 M"}, {"heading": "1. Introduction", "text": "Embedding input data in binary spaces is becoming popular for efficient retrieval and learning on massive data sets (Li et al., 2011; Gong et al., 2013a; Raginsky & Lazebnik, 2009; Gong et al., 2012; Liu et al., 2011). Moreover, in a large number of application domains such as com-\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nputer vision, biology and finance, data is typically highdimensional. When representing such high dimensional data by binary codes, it has been shown that long codes are required in order to achieve good performance. In fact, the required number of bits is O(d), where d is the input dimensionality (Li et al., 2011; Gong et al., 2013a; Sa\u0301nchez & Perronnin, 2011). The goal of binary embedding is to well approximate the input distance as Hamming distance so that efficient learning and retrieval can happen directly in the binary space. It is important to note that another related area called hashing is a special case with slightly different goal: creating hash tables such that points that are similar fall in the same (or nearby) bucket with high probability. In fact, even in hashing, if high accuracy is desired, one typically needs to use hundreds of hash tables involving tens of thousands of bits.\nMost of the existing linear binary coding approaches generate the binary code by applying a projection matrix, followed by a binarization step. Formally, given a data point, x \u2208 Rd, the k-bit binary code, h(x) \u2208 {+1,\u22121}k is generated simply as\nh(x) = sign(Rx), (1)\nwhere R \u2208 Rk\u00d7d, and sign(\u00b7) is a binary map which returns element-wise sign1. Several techniques have been proposed to generate the projection matrix randomly without taking into account the input data (Charikar, 2002; Raginsky & Lazebnik, 2009). These methods are very popular due to their simplicity but often fail to give the best performance due to their inability to adapt the codes with respect to the input data. Thus, a number of data-dependent techniques have been proposed with different optimization criteria such as reconstruction error (Kulis & Darrell, 2009), data dissimilarity (Norouzi & Fleet, 2012; Weiss et al., 2008), rank-\n1A few methods transform the linear projection via a nonlinear map before taking the sign (Weiss et al., 2008; Raginsky & Lazebnik, 2009).\ning loss (Norouzi et al., 2012), quantization error after PCA (Gong et al., 2013b), and pairwise misclassification (Wang et al., 2010). These methods are shown to be effective for learning compact codes for relatively lowdimensional data. However, the O(d2) computational and space costs prohibit them from being applied to learning long codes for high-dimensional data. For instance, to generate O(d)-bit binary codes for data with d \u223c1M, a huge projection matrix will be required needing TBs of memory, which is not practical2. In order to overcome these computational challenges, Gong et al. (2013a) proposed a bilinear projection based coding method for high-dimensional data. It reshapes the input vector x into a matrix Z, and applies a bilinear projection to get the binary code:\nh(x) = sign(RT1 ZR2). (2)\nWhen the shapes of Z,R1,R2 are chosen appropriately, the method has time and space complexity of O(d1.5) and O(d), respectively. Bilinear codes make it feasible to work with datasets with very high dimensionality and have shown good results in a variety of tasks.\nIn this work, we propose a novel Circulant Binary Embedding (CBE) technique which is even faster than the bilinear coding. It is achieved by imposing a circulant structure on the projection matrix R in (1). This special structure allows us to use Fast Fourier Transformation (FFT) based techniques, which have been extensively used in signal processing. The proposed method further reduces the time complexity to O(d log d), enabling efficient binary embedding for very high-dimensional data3. Table 1 compares the time and space complexity for different methods. This work makes the following contributions:\n\u2022 We propose the circulant binary embedding method, which has space complexity O(d) and time complexity O(d log d) (Section 2, 3). \u2022 We propose to learn the data-dependent circulant projection matrix by a novel and efficient time-frequency alternating optimization, which alternatively optimizes the objective in the original and frequency domains (Section 4). \u2022 Extensive experiments show that, compared to the state-of-the-art, the proposed method improves the result dramatically for a fixed time cost, and provides much faster computation with no performance degradation for a fixed number of bits (Section 5).\n2In principle, one can generate the random entries of the matrix on-the-fly (with fixed seeds) without needing to store the matrix. But this will increase the computational time even further.\n3One could in principal use other structured matrices like Hadamard matrix along with a sparse random Gaussian matrix to achieve fast projection as was done in fast Johnson-Lindenstrauss transform(Ailon & Chazelle, 2006; Dasgupta et al., 2011), but it is still slower than circulant projection and needs more space."}, {"heading": "2. Circulant Binary Embedding (CBE)", "text": "A circulant matrix R \u2208 Rd\u00d7d is a matrix defined by a vector r = (r0, r2, \u00b7 \u00b7 \u00b7 , rd\u22121)T (Gray, 2006)4.\nR = circ(r) :=\n\n      \nr0 rd\u22121 . . . r2 r1 r1 r0 rd\u22121 r2 ... r1 r0 . . . ...\nrd\u22122 . . . . . . rd\u22121 rd\u22121 rd\u22122 . . . r1 r0\n\n       . (3)\nLet D be a diagonal matrix with each diagonal entry being a Bernoulli variable (\u00b11 with probability 1/2). For x \u2208 Rd, its d-bit Circulant Binary Embedding (CBE) with r \u2208 Rd is defined as:\nh(x) = sign(RDx), (4)\nwhere R = circ(r). The k-bit (k < d) CBE is defined as the first k elements of h(x). The need for such a D is discussed in Section 3. Note that applying D to x is equivalent to applying random sign flipping to each dimension of x. Since sign flipping can be carried out as a preprocessing step for each input x, here onwards for simplicity we will drop explicit mention of D. Hence the binary code is given as h(x) = sign(Rx).\nThe main advantage of circulant binary embedding is its ability to use Fast Fourier Transformation (FFT) to speed up the computation.\nProposition 1. For d-dimensional data, CBE has space complexity O(d), and time complexity O(d log d). Since a circulant matrix is defined by a single column/row, clearly the storage needed is O(d). Given a data point x, the d-bit CBE can be efficiently computed as follows. Denote \u229b as operator of circulant convolution. Based on the definition of circulant matrix,\nRx = r\u229b x. (5)\nThe above can be computed based on Discrete Fourier Transformation (DFT), for which fast algorithm (FFT) is available. The DFT of a vector t \u2208 Cd is a d-dimensional vector with each element defined as\n4The circulant matrix is sometimes equivalently defined by \u201ccirculating\u201d the rows instead of the columns.\nF(t)l = d\u22121 \u2211\nm=0\ntn \u00b7 e\u2212i2\u03c0lm/d, l = 0, \u00b7 \u00b7 \u00b7 , d\u2212 1. (6)\nThe above can be expressed equivalently in a matrix form as F(t) = Fdt, (7) where Fd is the d-dimensional DFT matrix. Let FHd be the conjugate transpose of Fd. It is easy to show that F \u22121\nd = (1/d)FHd . Similarly, for any t \u2208 Cd, the Inverse Discrete Fourier Transformation (IDFT) is defined as\nF\u22121(t) = (1/d)FHd t. (8)\nSince the convolution of two signals in their original domain is equivalent to the hadamard product in their frequency domain (Oppenheim et al., 1999),\nF(Rx) = F(r) \u25e6 F(x). (9) Therefore,\nh(x) = sign ( F\u22121(F(r) \u25e6 F(x)) ) . (10)\nFor k-bit CBE, k < d, we only need to pick the first k bits of h(x). As DFT and IDFT can be efficiently computed in O(d log d) with FFT (Oppenheim et al., 1999), generating CBE has time complexity O(d log d)."}, {"heading": "3. Randomized Circulant Binary Embedding", "text": "A simple way to obtain CBE is by generating the elements of r in (3) independently from the standard normal distribution N (0, 1). We call this method randomized CBE (CBE-rand). A desirable property of any embedding method is its ability to approximate input distances in the embedded space. Suppose Hk(x1,x2) is the normalized Hamming distance between k-bit codes of a pair of points x1,x2 \u2208 Rd:\nHk(x1,x2)= 1\nk\nk\u22121 \u2211\ni=0\n\u2223 \u2223sign(Ri\u00b7x1)\u2212sign(Ri\u00b7x2) \u2223 \u2223/2, (11)\nand Ri\u00b7 is the i-th row of R, R = circ(r). If r is sampled from N (0, 1), from (Charikar, 2002),\nPr ( sign(rTx1) 6= sign(rTx2) ) = \u03b8/\u03c0, (12)\nwhere \u03b8 is the angle between x1 and x2. Since all the vectors that are circulant variants of r also follow the same distribution, it is easy to see that\nE(Hk(x1,x2)) = \u03b8/\u03c0. (13)\nFor the sake of discussion, if k projections, i.e., first k rows of R, were generated independently, it is easy to show that the variance of Hk(x1,x2) will be\nVar(Hk(x1,x2)) = \u03b8(\u03c0 \u2212 \u03b8)/k\u03c02. (14)\nThus, with more bits (larger k), the normalized hamming distance will be close to the expected value, with lower variance. In other words, the normalized hamming distance approximately preserves the angle5. Unfortunately in CBE, the projections are the rows of R = circ(r), which are not independent. This makes it hard to derive the variance analytically. To better understand CBE-rand, we run simulations to compare the analytical variance of normalized hamming distance of independent projections (14), and the sample variance of normalized hamming distance of circulant projections in Figure 1. For each \u03b8 and k, we randomly generate x1,x2 \u2208 Rd such that their angle is \u03b86. We then generate k-dimensional code with CBE-rand, and compute the hamming distance. The variance is estimated by applying CBE-rand 1,000 times. We repeat the whole process 1,000 times, and compute the averaged variance. Surprisingly, the curves of \u201cIndependent\u201d and \u201cCirculant\u201d variances are almost indistinguishable. This means that bits generated by CBE-rand are generally as good as the independent bits for angle preservation. An intuitive explanation is that for the circulant matrix, though all the rows are dependent, circulant shifting one or multiple elements will in fact result in very different projections in most cases. We will later show in experiments on real-world data that CBE-rand and Locality Sensitive Hashing (LSH)7 has almost identical performance (yet CBE-rand is significantly faster) (Section 5).\n5In this paper, we consider the case that the data points are \u21132 normalized. Therefore the cosine distance, i.e., 1 - cos(\u03b8), is equivalent to the l2 distance.\n6This can be achieved by extending the 2D points (1, 0), (cos \u03b8, sin \u03b8) to d-dimension, and performing a random orthonormal rotation, which can be formed by the Gram-Schmidt process on random vectors.\n7Here, by LSH we imply the binary embedding using R such that all the rows of R are sampled iid. With slight abuse of notation, we still call it \u201chashing\u201d following (Charikar, 2002).\nNote that the distortion in input distances after circulant binary embedding comes from two sources: circulant projection, and binarization. For the circulant projection step, recent works have shown that the Johnson-Lindenstrausstype lemma holds with a slightly worse bound on the number of projections needed to preserve the input distances with high probability (Hinrichs & Vyb\u0131\u0301ral, 2011; Zhang & Cheng, 2013; Vyb\u0131\u0301ral, 2011; Krahmer & Ward, 2011). These works also show that before applying the circulant projection, an additional step of randomly flipping the signs of input dimensions is necessary8. To show why such a step is required, let us consider the special case when x is an all-one vector, 1. The circulant projection with R = circ(r) will result in a vector with all elements to be rT1. When r is independently drawn from N (0, 1), this will be close to 0, and the norm cannot be preserved. Unfortunately the Johnson-Lindenstrauss-type results do not generalize to the distortion caused by the binarization step.\nOne problem with the randomized CBE method is that it does not utilize the underlying data distribution while generating the matrix R. In the next section, we propose to learn R in a data-dependent fashion, to minimize the distortions due to circulant projection and binarization."}, {"heading": "4. Learning Circulant Binary Embedding", "text": "We propose data-dependent CBE (CBE-opt), by optimizing the projection matrix with a novel time-frequency alternating optimization. We consider the following objective function in learning the d-bit CBE. The extension of learning k < d bits will be shown in Section 4.2.\nargmin B,r\n||B\u2212XRT ||2F + \u03bb||RRT \u2212 I||2F (15)\ns.t. R = circ(r),\nwhere X \u2208 Rn\u00d7d, is the data matrix containing n training points: X = [x0, \u00b7 \u00b7 \u00b7 ,xn\u22121]T , and B \u2208 {\u22121, 1}n\u00d7d is the corresponding binary code matrix.9\nIn the above optimization, the first term minimizes distortion due to binarization. The second term tries to make the projections (rows of R, and hence the corresponding bits) as uncorrelated as possible. In other words, this helps to reduce the redundancy in the learned code. If R were to be an orthogonal matrix, the second term will vanish and the optimization would find the best rotation such that the distortion due to binarization is minimized. However, when R is a circulant matrix, R, in general, will not be orthogonal. Similar objective has been used in previous works including (Gong et al., 2013b;a) and (Wang et al., 2010).\n8For each dimension, whether the sign needs to be flipped is predetermined by a (p = 0.5) Bernoulli variable.\n9If the data is \u21132 normalized, we can set B \u2208 {\u22121/ \u221a d, 1/ \u221a d}n\u00d7d to make B and XRT more comparable. This does not empirically influence the performance."}, {"heading": "4.1. The Time-Frequency Alternating Optimization", "text": "The above is a combinatorial optimization problem, for which an optimal solution is hard to find. In this section we propose a novel approach to efficiently find a local solution. The idea is to alternatively optimize the objective by fixing r, and B, respectively. For a fixed r, optimizing B can be easily performed in the input domain (\u201ctime\u201d as opposed to \u201cfrequency\u201d). For a fixed B, the circulant structure of R makes it difficult to optimize the objective in the input domain. Hence we propose a novel method, by optimizing r in the frequency domain based on DFT. This leads to a very efficient procedure.\nFor a fixed r. The objective is independent on each element of B. Denote Bij as the element of the i-th row and j-th column of B. It is easy to show that B can be updated as:\nBij =\n{\n1 if Rj\u00b7xi \u2265 0 \u22121 if Rj\u00b7xi < 0 , (16)\ni = 0, \u00b7 \u00b7 \u00b7 , n\u2212 1. j = 0, \u00b7 \u00b7 \u00b7 , d\u2212 1. For a fixed B. Define r\u0303 as the DFT of the circulant vector r\u0303 := F(r). Instead of solving r directly, we propose to solve r\u0303, from which r can be recovered by IDFT.\nKey to our derivation is the fact that DFT projects the signal to a set of orthogonal basis. Therefore the \u21132 norm can be preserved. Formally, according to Parseval\u2019s theorem , for any t \u2208 Cd (Oppenheim et al., 1999),\n||t||22 = (1/d)||F(t)||22. Denote diag(\u00b7) as the diagonal matrix formed by a vector. Denote \u211c(\u00b7) and \u2111(\u00b7) as the real and imaginary parts, respectively. We use Bi\u00b7 to denote the i-th row of B. With complex arithmetic, the first term in (15) can be expressed in the frequency domain as:\n||B\u2212XRT ||2F = 1\nd\nn\u22121 \u2211\ni=0\n||F(BTi\u00b7 \u2212Rxi)||22 (17)\n= 1\nd\nn\u22121 \u2211\ni=0\n||F(BTi\u00b7 )\u2212r\u0303\u25e6F(xi)||22= 1\nd\nn\u22121 \u2211\ni=0\n||F(BTi\u00b7)\u2212diag(F(xi))r\u0303||22\n= 1\nd\nn\u22121 \u2211\ni=0\n( F(BTi\u00b7)\u2212diag(F(xi))r\u0303 ) T ( F(BTi\u00b7)\u2212diag(F(xi))r\u0303 )\n= 1\nd\n[ \u211c(r\u0303)TM\u211c(r\u0303)+\u2111(r\u0303)TM\u2111(r\u0303)+\u211c(r\u0303)Th+\u2111(r\u0303)Tg ] +||B||2F ,\nwhere,\nM=diag (\nn\u22121 \u2211\ni=0\n\u211c(F(xi))\u25e6\u211c(F(xi))+\u2111(F(xi))\u25e6\u2111(F(xi)) )\nh = \u22122 n\u22121 \u2211\ni=0\n\u211c(F(xi))\u25e6\u211c(F(BTi\u00b7 ))+\u2111(F(xi)) \u25e6 \u2111(F(BTi\u00b7))\ng = 2\nn\u22121 \u2211\ni=0\n\u2111(F(xi)) \u25e6 \u211c(F(BTi\u00b7))\u2212 \u211c(F(xi)) \u25e6 \u2111(F(BTi\u00b7 )).\nFor the second term in (15), we note that the circulant matrix can be diagonalized by DFT matrix Fd and its conjugate transpose FHd . Formally, for R = circ(r), r \u2208 Rd, R = (1/d)FHd diag(F(r))Fd. (18) Let Tr(\u00b7) be the trace of a matrix. Therefore, ||RRT \u2212 I||2F = || 1\nd FHd (diag(r\u0303) Hdiag(r\u0303)\u2212 I)Fd||2F\n=Tr\n[\n1 d FHd (diag(r\u0303) Hdiag(r\u0303)\u2212I)H(diag(r\u0303)Hdiag(r\u0303)\u2212I)Fd ]\n=Tr [ (diag(r\u0303)Hdiag(r\u0303)\u2212 I)H(diag(r\u0303)Hdiag(r\u0303)\u2212 I) ]\n=||r\u0303H \u25e6 r\u0303\u2212 1||2 2 = ||\u211c(r\u0303)2 + \u2111(r\u0303)2 \u2212 1||2 2 . (19)\nFurthermore, as r is real-valued, additional constraints on r\u0303 are needed. For any u \u2208 C, denote u as the complex conjugate of u. We have the following result (Oppenheim et al., 1999): For any real-valued vector t \u2208 Cd, F(t)0 is real-valued, and\nF(t)d\u2212i = F(t)i, i = 1, \u00b7 \u00b7 \u00b7 , \u230ad/2\u230b.\nFrom (17) \u2212 (19), the problem of optimizing r\u0303 becomes argmin\nr\u0303\n\u211c(r\u0303)TM\u211c(r\u0303) + \u2111(r\u0303)TM\u2111(r\u0303) + \u211c(r\u0303)Th\n+ \u2111(r\u0303)Tg + \u03bbd||\u211c(r\u0303)2 + \u2111(r\u0303)2 \u2212 1||22 (20) s.t. \u2111(r\u03030) = 0\n\u211c(r\u0303i) = \u211c(r\u0303d\u2212i), i = 1, \u00b7 \u00b7 \u00b7 , \u230ad/2\u230b \u2111(r\u0303i) = \u2212\u2111(r\u0303d\u2212i), i = 1, \u00b7 \u00b7 \u00b7 , \u230ad/2\u230b.\nThe above is non-convex. Fortunately, the objective function can be decomposed, such that we can solve two variables at a time. Denote the diagonal vector of the diagonal matrix M as m. The above optimization can then be decomposed to the following sets of optimizations.\nargmin r\u03030\nm0r\u0303 2 0 + h0r\u03030+ \u03bbd ( r\u030320 \u2212 1 )2 , s.t. r\u03030 = r\u03030. (21)\nargmin r\u0303i\n(mi +md\u2212i)(\u211c(r\u0303i)2 + \u2111(r\u0303i)2) (22)\n+ 2\u03bbd ( \u211c(r\u0303i)2 + \u2111(r\u0303i)2 \u2212 1 )2 + (hi + hd\u2212i)\u211c(r\u0303i) + (gi \u2212 gd\u2212i)\u2111(r\u0303i), i = 1, \u00b7 \u00b7 \u00b7 , \u230ad/2\u230b.\nIn (21), we need to minimize a 4th order polynomial with one variable, with the closed form solution readily available. In (22), we need to minimize a 4th order polynomial with two variables. Though the closed form solution is hard (requiring solution of a cubic bivariate system), we can find local minima by gradient descent, which can be considered as having constant running time for such small-scale problems. The overall objective is guaranteed to be nonincreasing in each step. In practice, we can get a good solution with just 5-10 iterations. In summary, the proposed time-frequency alternating optimization procedure has running time O(nd log d)."}, {"heading": "4.2. Learning k < d Bits", "text": "In the case of learning k < d bits, we need to solve the following optimization problem:\nargmin B,r\n||BPk\u2212XPTkRT ||2F +\u03bb||RPkPTk RT\u2212I||2F\ns.t. R = circ(r), (23)\nin which Pk =\n[\nIk O O Od\u2212k\n]\n, Ik is a k\u00d7 k identity matrix, and Od\u2212k is a (d\u2212 k)\u00d7 (d\u2212 k) all-zero matrix. In fact, the right multiplication of Pk can be understood as a \u201ctemporal cut-off\u201d, which is equivalent to a frequency domain convolution. This makes the optimization difficult, as the objective in frequency domain can no longer be decomposed. To address this issues, we propose a simple solution in which Bij = 0, i = 0, \u00b7 \u00b7 \u00b7 , n \u2212 1, j = k, \u00b7 \u00b7 \u00b7 , d \u2212 1 in (15). Thus, the optimization procedure remains the same, and the cost is also O(nd log d). We will show in experiments that this heuristic provides good performance in practice."}, {"heading": "5. Experiments", "text": "To compare the performance of the proposed circulant binary embedding technique, we conducted experiments on three real-world high-dimensional datasets used by the current state-of-the-art method for generating long binary codes (Gong et al., 2013a). The Flickr-25600 dataset contains 100K images sampled from a noisy Internet image collection. Each image is represented by a 25, 600 dimensional vector. The ImageNet-51200 contains 100k images sampled from 100 random classes from ImageNet (Deng et al., 2009), each represented by a 51, 200 dimensional vector. The third dataset (ImageNet-25600) is another random subset of ImageNet containing 100K images in 25, 600 dimensional space. All the vectors are normalized to be of unit norm.\nWe compared the performance of the randomized (CBErand) and learned (CBE-opt) versions of our circulant embeddings with the current state-of-the-art for highdimensional data, i.e., bilinear embeddings. We use both the randomized (bilinear-rand) and learned (bilinear-opt) versions. Bilinear embeddings have been shown to perform similar or better than another promising technique called Product Quantization (Jegou et al., 2011). Finally, we also compare against the binary codes produced by the baseline LSH method (Charikar, 2002), which is still applicable to 25,600 and 51,200 dimensional feature but with much longer running time and much more space. We also show an experiment with relatively low-dimensional data in 2048 dimensional space using Flickr data to compare against techniques that perform well for low-dimensional data but do not scale to high-dimensional scenario. Example tech-\nniques include ITQ (Gong et al., 2013b), SH (Weiss et al., 2008), SKLSH (Raginsky & Lazebnik, 2009), and AQBC (Gong et al., 2012).\nFollowing (Gong et al., 2013a; Norouzi & Fleet, 2012; Gordo & Perronnin, 2011), we use 10,000 randomly sampled instances for training. We then randomly sample 500 instances, different from the training set as queries. The performance (recall@1-100) is evaluated by averaging the recalls of the query instances. The ground-truth of each query instance is defined as its 10 nearest neighbors based on \u21132 distance. For each dataset, we conduct two sets of experiments: fixed-time where code generation time is fixed and fixed-bits where the number of bits is fixed across all techniques. We also show an experiment where the binary codes are used for classification.\nThe proposed CBE method is found robust to the choice of \u03bb in (15). For example, in the retrieval experiments, the performance difference for \u03bb = 0.1, 1, 10, is within 0.5%. Therefore, in all the experiments, we simply fix \u03bb = 1. For the bilinear method, in order to get fast computation, the feature vector is reshaped to a near-square matrix, and the dimension of the two bilinear projection matrices are also chosen to be close to square. Parameters for other techniques are tuned to give the best results on these datasets.\nComputational Time. When generating k-bit code for d-dimensional data, the full projection, bilinear projection, and circulant projection methods have time complexity O(kd), O( \u221a kd), and O(d log d), respectively. We compare the computational time in Table 2 on a fixed hardware. Based on our implementation, the computational time of the above three methods can be roughly characterized as d2 : d \u221a d : 5d log d. Note that faster implementation of FFT algorithms will lead to better computational time for CBE by further reducing the constant factor. Due to the small storage requirement O(d), and the wide availability of highly optimized FFT libraries, CBE is also suitable for implementation on GPU. Our preliminary tests based on GPU shows up to 20 times speedup compared to CPU. In this paper, for fair comparison, we use same CPU based implementation for all the methods.\nRetrieval. The recall for different methods is compared on\nthe three datasets in Figure 2, 3, and 4 respectively. The top row in each figure shows the performance for different methods when the code generation time for all the methods is kept the same as that of CBE. For a fixed time, the proposed CBE yields much better recall than other methods. Even CBE-rand outperforms LSH and Bilinear code by a large margin. The second row compares the performance for different techniques with codes of same length. In this case, the performance of CBE-rand is almost identical to LSH even though it is hundreds of time faster. This is consistent with our analysis in Section 3. Moreover, CBEopt/CBE-rand outperform the Bilinear-opt/Bilinear-rand in addition to being 2-3 times faster.\nClassification. Besides retrieval, we also test the binary codes for classification. The advantage is to save on storage allowing even large scale datasets to fit in memory (Li et al., 2011; Sa\u0301nchez & Perronnin, 2011). We follow the asymmetric setting of (Sa\u0301nchez & Perronnin, 2011) by training linear SVM on binary code sign(Rx), and testing on the original Rx. This empirically has been shown to give better accuracy than the symmetric procedure. We use ImageNet-25600, with randomly sampled 100 images per category for training, 50 for validation and 50 for testing. The code dimension is set as 25,600. As shown in Table 3, CBE, which has much faster computation, does not show any performance degradation compared to LSH or bilinear codes in classification task as well.\nLow-Dimensional Experiment. There exist several techniques that do not scale to high-dimensional case. To compare our method with those, we conducted experiments\nwith fixed number of bits on a relatively low-dimensional dataset (Flickr-2048), constructed by randomly sampling 2,048 dimensions of Flickr-25600. As shown in Figure 5, though CBE is not designed for such scenario, the CBEopt performs better or equivalent to other techniques except\nITQ which scales very poorly with d (O(d3)). Moreover, as the number of bits increases, the gap between ITQ and CBE becomes much smaller suggesting that the performance of ITQ is not expected to be better than CBE even if one could run ITQ on high-dimensional data."}, {"heading": "6. Semi-supervised Extension", "text": "In some applications, one can have access to a few labeled pairs of similar and dissimilar data points. Here we show how the CBE formulation can be extended to incorporate such information in learning. This is achieved by adding an additional objective term J(R).\nargmin B,r\n||B\u2212XRT ||2F +\u03bb||RRT \u2212 I||2F + \u00b5J(R) (24)\ns.t. R = circ(r),\nJ(R)= \u2211\ni,j\u2208M\n||Rxi\u2212Rxj ||22\u2212 \u2211\ni,j\u2208D\n||Rxi\u2212Rxj ||22. (25)\nHere M and D are the set of \u201csimilar\u201d and \u201cdissimilar\u201d instances, respectively. The intuition is to maximize the distances between the dissimilar pairs, and minimize the distances between the similar pairs. Such a term is commonly used in semi-supervised binary coding methods (Wang et al., 2010). We again use the time-frequency alternating optimization procedure of Section 4. For a fixed r, the optimization procedure to update B is the same. For a fixed B, optimizing r is done in frequency domain by expanding J(R) as below, with similar techniques used in Section 4.\n||Rxi\u2212Rxj||22 = (1/d)||diag(F(xi)\u2212F(xj))r\u0303||22.\nTherefore,\nJ(R) = (1/d)(\u211c(r\u0303)TA\u211c(r\u0303) + \u2111(r\u0303)TA\u2111(r\u0303)), (26)\nwhere, A = A1 +A2 \u2212A3 \u2212A4, and\nA1= \u2211\n(i,j)\u2208M\n\u211c(diag(F(xi)\u2212F(xj)))T\u211c(diag(F(xi)\u2212F(xj))),\nA2= \u2211\n(i,j)\u2208M\n\u2111(diag(F(xi)\u2212F(xj)))T\u2111(diag(F(xi)\u2212F(xj))),\nA3= \u2211\n(i,j)\u2208D\n\u211c(diag(F(xi)\u2212F(xj)))T\u211c(diag(F(xi)\u2212F(xj))),\nA4= \u2211\n(i,j)\u2208D\n\u2111(diag(F(xi)\u2212F(xj)))T\u2111(diag(F(xi)\u2212F(xj))).\nHence, the optimization can be carried out as in Section 4, whereM in (17) is simply replaced byM+\u00b5A. Our experiments show that the semi-supervised extension improves over the non-semi-supervised version by 2% in terms of averaged AUC on the ImageNet-25600 dataset."}, {"heading": "7. Conclusion", "text": "We have proposed circulant binary embedding for generating long codes for very high-dimensional data. A novel time-frequency alternating optimization was also introduced to learn the model parameters from the training data. The proposed method has time complexity O(d log d) and space complexity O(d), while showing no performance degradation on real-world data compared to more expensive approaches (O(d2) or O(d1.5)). On the contrary, for the fixed time, it showed significant accuracy gains. The full potential of the method can be unleashed when applied to ultra-high dimensional data (say d \u223c100M), for which no other methods are applicable."}], "references": [{"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["Ailon", "Nir", "Chazelle", "Bernard"], "venue": "In ACM Symposium on Theory of Computing,", "citeRegEx": "Ailon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2006}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Charikar", "Moses S"], "venue": "In ACM Symposium on Theory of Computing,", "citeRegEx": "Charikar and S.,? \\Q2002\\E", "shortCiteRegEx": "Charikar and S.", "year": 2002}, {"title": "Fast locality-sensitive hashing", "author": ["Dasgupta", "Anirban", "Kumar", "Ravi", "Sarl\u00f3s", "Tam\u00e1s"], "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Dasgupta et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2011}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng", "Jia", "Dong", "Wei", "Socher", "Richard", "Li", "Li-Jia", "Kai", "Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Angular quantization-based binary codes for fast similarity search", "author": ["Gong", "Yunchao", "Kumar", "Sanjiv", "Verma", "Vishal", "Lazebnik", "Svetlana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Learning binary codes for highdimensional data using bilinear projections", "author": ["Gong", "Yunchao", "Kumar", "Sanjiv", "Rowley", "Henry A", "Lazebnik", "Svetlana"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Gong", "Yunchao", "Lazebnik", "Svetlana", "Gordo", "Albert", "Perronnin", "Florent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Asymmetric distances for binary embeddings", "author": ["Gordo", "Albert", "Perronnin", "Florent"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Gordo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gordo et al\\.", "year": 2011}, {"title": "Toeplitz and circulant matrices: A review", "author": ["Gray", "Robert M"], "venue": "Now Pub,", "citeRegEx": "Gray and M.,? \\Q2006\\E", "shortCiteRegEx": "Gray and M.", "year": 2006}, {"title": "Johnson-Lindenstrauss lemma for circulant matrices", "author": ["Hinrichs", "Aicke", "Vyb\u0131\u0301ral", "Jan"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Hinrichs et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hinrichs et al\\.", "year": 2011}, {"title": "Product quantization for nearest neighbor search", "author": ["Jegou", "Herve", "Douze", "Matthijs", "Schmid", "Cordelia"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Jegou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jegou et al\\.", "year": 2011}, {"title": "New and improved Johnson-Lindenstrauss embeddings via the restricted isometry property", "author": ["Krahmer", "Felix", "Ward", "Rachel"], "venue": "SIAM Journal on Mathematical Analysis,", "citeRegEx": "Krahmer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Krahmer et al\\.", "year": 2011}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["Kulis", "Brian", "Darrell", "Trevor"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kulis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2009}, {"title": "Hashing algorithms for largescale learning", "author": ["Li", "Ping", "Shrivastava", "Anshumali", "Moore", "Joshua", "Konig", "Arnd Christian"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Hashing with graphs", "author": ["Liu", "Wei", "Wang", "Jun", "Kumar", "Sanjiv", "Chang", "Shih-Fu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Minimal loss hashing for compact binary codes", "author": ["Norouzi", "Mohammad", "Fleet", "David"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Norouzi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "Hamming distance metric learning", "author": ["Norouzi", "Mohammad", "Fleet", "David", "Salakhutdinov", "Ruslan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Norouzi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "Localitysensitive binary codes from shift-invariant kernels", "author": ["Raginsky", "Maxim", "Lazebnik", "Svetlana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Raginsky et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Raginsky et al\\.", "year": 2009}, {"title": "High-dimensional signature compression for large-scale image classification", "author": ["S\u00e1nchez", "Jorge", "Perronnin", "Florent"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "S\u00e1nchez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "S\u00e1nchez et al\\.", "year": 2011}, {"title": "A variant of the Johnson\u2013Lindenstrauss lemma for circulant matrices", "author": ["Vyb\u0131\u0301ral", "Jan"], "venue": "Journal of Functional Analysis,", "citeRegEx": "Vyb\u0131\u0301ral and Jan.,? \\Q2011\\E", "shortCiteRegEx": "Vyb\u0131\u0301ral and Jan.", "year": 2011}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["Wang", "Jun", "Kumar", "Sanjiv", "Chang", "Shih-Fu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Spectral hashing", "author": ["Weiss", "Yair", "Torralba", "Antonio", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Weiss et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2008}, {"title": "New bounds for circulant Johnson-Lindenstrauss embeddings", "author": ["Zhang", "Hui", "Cheng", "Lizhi"], "venue": "arXiv preprint arXiv:1308.6339,", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 13, "context": "Introduction Embedding input data in binary spaces is becoming popular for efficient retrieval and learning on massive data sets (Li et al., 2011; Gong et al., 2013a; Raginsky & Lazebnik, 2009; Gong et al., 2012; Liu et al., 2011).", "startOffset": 129, "endOffset": 230}, {"referenceID": 4, "context": "Introduction Embedding input data in binary spaces is becoming popular for efficient retrieval and learning on massive data sets (Li et al., 2011; Gong et al., 2013a; Raginsky & Lazebnik, 2009; Gong et al., 2012; Liu et al., 2011).", "startOffset": 129, "endOffset": 230}, {"referenceID": 14, "context": "Introduction Embedding input data in binary spaces is becoming popular for efficient retrieval and learning on massive data sets (Li et al., 2011; Gong et al., 2013a; Raginsky & Lazebnik, 2009; Gong et al., 2012; Liu et al., 2011).", "startOffset": 129, "endOffset": 230}, {"referenceID": 13, "context": "In fact, the required number of bits is O(d), where d is the input dimensionality (Li et al., 2011; Gong et al., 2013a; S\u00e1nchez & Perronnin, 2011).", "startOffset": 82, "endOffset": 146}, {"referenceID": 21, "context": "Thus, a number of data-dependent techniques have been proposed with different optimization criteria such as reconstruction error (Kulis & Darrell, 2009), data dissimilarity (Norouzi & Fleet, 2012; Weiss et al., 2008), rankA few methods transform the linear projection via a nonlinear map before taking the sign (Weiss et al.", "startOffset": 173, "endOffset": 216}, {"referenceID": 21, "context": ", 2008), rankA few methods transform the linear projection via a nonlinear map before taking the sign (Weiss et al., 2008; Raginsky & Lazebnik, 2009).", "startOffset": 102, "endOffset": 149}, {"referenceID": 15, "context": "ing loss (Norouzi et al., 2012), quantization error after PCA (Gong et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 20, "context": ", 2013b), and pairwise misclassification (Wang et al., 2010).", "startOffset": 41, "endOffset": 60}, {"referenceID": 4, "context": ", 2012), quantization error after PCA (Gong et al., 2013b), and pairwise misclassification (Wang et al., 2010). These methods are shown to be effective for learning compact codes for relatively lowdimensional data. However, the O(d2) computational and space costs prohibit them from being applied to learning long codes for high-dimensional data. For instance, to generate O(d)-bit binary codes for data with d \u223c1M, a huge projection matrix will be required needing TBs of memory, which is not practical2. In order to overcome these computational challenges, Gong et al. (2013a) proposed a bilinear projection based coding method for high-dimensional data.", "startOffset": 39, "endOffset": 579}, {"referenceID": 2, "context": "One could in principal use other structured matrices like Hadamard matrix along with a sparse random Gaussian matrix to achieve fast projection as was done in fast Johnson-Lindenstrauss transform(Ailon & Chazelle, 2006; Dasgupta et al., 2011), but it is still slower than circulant projection and needs more space.", "startOffset": 195, "endOffset": 242}, {"referenceID": 20, "context": ", 2013b;a) and (Wang et al., 2010).", "startOffset": 15, "endOffset": 34}, {"referenceID": 3, "context": "The ImageNet-51200 contains 100k images sampled from 100 random classes from ImageNet (Deng et al., 2009), each represented by a 51, 200 dimensional vector.", "startOffset": 86, "endOffset": 105}, {"referenceID": 10, "context": "Bilinear embeddings have been shown to perform similar or better than another promising technique called Product Quantization (Jegou et al., 2011).", "startOffset": 126, "endOffset": 146}, {"referenceID": 21, "context": ", 2013b), SH (Weiss et al., 2008), SKLSH (Raginsky & Lazebnik, 2009), and AQBC (Gong et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 4, "context": ", 2008), SKLSH (Raginsky & Lazebnik, 2009), and AQBC (Gong et al., 2012).", "startOffset": 53, "endOffset": 72}, {"referenceID": 13, "context": "The advantage is to save on storage allowing even large scale datasets to fit in memory (Li et al., 2011; S\u00e1nchez & Perronnin, 2011).", "startOffset": 88, "endOffset": 132}, {"referenceID": 20, "context": "Such a term is commonly used in semi-supervised binary coding methods (Wang et al., 2010).", "startOffset": 70, "endOffset": 89}], "year": 2014, "abstractText": "Binary embedding of high-dimensional data requires long codes to preserve the discriminative power of the input space. Traditional binary coding methods often suffer from very high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure enables the use of Fast Fourier Transformation to speed up the computation. Compared to methods that use unstructured matrices, the proposed method improves the time complexity from O(d2) to O(d log d), and the space complexity from O(d2) to O(d) where d is the input dimensionality. We also propose a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. We show by extensive experiments that the proposed approach gives much better performance than the state-of-the-art approaches for fixed time, and provides much faster computation with no performance degradation for fixed number of bits.", "creator": "LaTeX with hyperref package"}}}