{"id": "1405.3382", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2014", "title": "Active Mining of Parallel Video Streams", "abstract": "the practicality of a video virtual surveillance system is adversely limited by the amount of queries that teams can be mistakenly placed on human resources and ensuring their vigilance in response. to transcend this limitation, a major effort under way is started to include software configurations that ( fully or at somewhat least semi ) automatically mines video footage, reducing the burden imposed to the system. herein, we propose a semi - supervised incremental learning framework for monitoring evolving visual streams in order to develop specifically a robust and flexible track classification system. our proposed production method learns from consecutive batches by updating an ensemble in each time. it tries to strike a balance between sheer performance factor of the system and amount of data analyzed which needs algorithms to be labelled. as no restriction is considered, today the system can address among many practical problems in an evolving multi - camera scenario, such as concept particle drift, class evolution and various length of video streams which have not been addressed before. experiments were performed on emerging synthetic as well as real - view world visual data in varying non - stationary environments, showing high accuracy with fairly little human collaboration.", "histories": [["v1", "Wed, 14 May 2014 07:00:38 GMT  (730kb,D)", "http://arxiv.org/abs/1405.3382v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["samaneh khoshrou", "jaime s cardoso", "luis f teixeira"], "accepted": false, "id": "1405.3382"}, "pdf": {"name": "1405.3382.pdf", "metadata": {"source": "CRF", "title": "Active Mining of Parallel Video Streams", "authors": ["Samaneh Khoshrou", "Jaime S. Cardoso", "Lu\u0131\u0301s F. Teixeira"], "emails": ["samaneh.khoshrou@inescporto.pt", "jaime.cardoso@inescporto.pt", "lft@fe.up.pt"], "sections": [{"heading": null, "text": "Keywords Video surveillance \u00b7 Parallel streams \u00b7 Active learning\nS. Khoshrou, J. S. Cardoso INESC TEC (formerly INESC Porto) Rua Doutor Roberto Frias 378, 4200-465 Oporto Tel.: +351-222094000 E-mail: samaneh.khoshrou@inescporto.pt jaime.cardoso@inescporto.pt\nL. F. Teixeira Faculdade de Engenharia da Universidade do Porto (FEUP) Rua Doutor Roberto Frias 378, 4200-465 Oporto Tel.: +351-225081400 E-mail: lft@fe.up.pt"}, {"heading": "1 Introduction", "text": "Over the last decades, video surveillance began to spread rapidly, specifically targeted at public areas. Recording for hours, days, and possibly years provides massive amount of information coming from an evolving environment in where traditional learning methods fail to reflect evolution taking place [15]. In such environments, the underlying distribution of data changes over time - often referred to as concept drift - either due to intrinsic changes (pose change, movement, etc.), or extrinsic changes (lighting condition, dynamic background, complex object background, changes in camera angle, etc.). Thus, models need to be continually updated to represent the latest concepts. The problem is further aggravated when new objects enter the scene - referred to as class evolution in machine learning literature - as new models need to be trained for the novel classes.\nFigure 1 demonstrates a typical surveillance scenario. Depending on the view angle and the quality of the camera, every surveillance camera covers an area called Field of View (FoV). Often the fields of view are disjoint due to budget constraints, whereas they overlap in some scenarios. When entering the scene, the object will enter the coverage area of at least one of the cameras. The surveillance system will have to track that object from the first moment it was captured by a camera and across all cameras whose fields of view overlap the object\u2019s path. In such environments where objects move around and cross the FOV of multiple cameras, it is more than likely to have multiple streams, potentially overlapping in time, recorded at different starting points with various lengths, for the same individual object (Figure 1). However, this type of scenarios is associated with several difficulties. For example, consider the following situation: three differar X iv :1\n40 5.\n33 82\nv1 [\ncs .C\nV ]\n1 4\nM ay\nent persons are detected by a tracking system in a considered span interval. Person A and person B walk side by side while they are captured by camera 1. Person C enters camera 2 field of view and meets person B. In camera 3, person A and person C start walking side by side. Finally, both are again captured by camera 4, after switching their relative positions. In this simple scenario the typical tracking systems are likely to encounter problems. In fact, mutual occlusion may occur if persons B and C cross. Consequently, their identities can be switched. Moreover, accompanying person A with person B or C, group movement (both are identified as a single object) and prolonged occlusion might occur, which might lead to track loss or mistaken identities [42]. Since the cameras are supposed\nto track all objects in their coverage area, the definition of a global identity for each object is necessary. Multiple appearances of objects captured by the same or by different cameras are identified in the process, allowing also to know the path followed by a given object. The inference of the identity of the objects in the scene is typically addressed with supervised learning\nmethodologies from labelled training data. Obtaining labelled instances, which typically needs human annotation, is expensive, time consuming, and impractical for our scenario. To reduce costs of annotation, semi-supervised learning (SSL) approaches have been extensively explored in limited labelled and usually abundant un-labelled data scenarios [29, 12, 18, 28]; however deploying SSL for evolving visual data in a non-stationary environments (where both concept drift and class evolution are present) is still an unexplored area. Several researchers have shown that the meticulous selection of instances that need to be labelled (mostly addressed in active learning (AL) strategies) could lead to better performance with less effort [4, 34]. In this work we address the need for a more general and systematic view of learning in evolving video streams in a multi-camera surveillance scenario. Considerable body of multi-camera surveillance research assume that adjacent camera view have overlap [9, 26, 19, 45], whereas [20, 38, 21, 33, 31] require non-overlapping views. Herein we put forward a framework to learn continuously from parallel video streams with partially labelled data and that allow us to learn novel knowledge, reinforce existing knowledge that is still relevant, and forget what may no longer be relevant. The method made no assumption of overlapping or non-overlapping view. Hence can be applied in either settings. The framework focuses on the classification of multiple video objects being tracked by a video tracking system. The framework receives directly the tracked sequences outputted by the tracking system and maintains a global object identity common to all the cameras in the system. Thus, a suitable outcome of the framework is a timeline graph (such as the one shown in Figure 1) allocating a stream in each camera for every participant in the system along the time axis for the indicated presence period.\nThe rest of the paper is organized as follows: next section 1.1 briefly reviews and discusses the limitation of former incremental learning algorithms for visual data. Section 2 provides an overview of our method. Section 3 discusses our experimental methodology. Section 4 presents the results of our method as well as some baseline approaches on a variety of synthetic and real datasets. Conclusions and direction for future work are presented in section 5.\n1.1 Literature Review\nMuch of the recent history on visual data understanding in general and multi-camera surveillance in particular has focused on building robust models applicable in object detection and tracking scenarios [27, 36, 40,\n41, 7]. Learning changing video streams over time has received much less attention despite the abundance of applications generating this information. Much of the learning literature is concerned with a stationary environment, where fixed and known number of categories to be recognized and enough resources (labelled data, memory and computational power) are available [32, 46]. To get closer to a practical solution, where obtaining labelled instances is an issue, SSL approaches have been deployed. various SSL methods have been proposed for video annotation [39, 44, 47]. However they have shown promising results for drifting scenarios with pre-determined classes (training data is available for all the classes), but they cannot address class evolution problem. In [3], the person identification task is posed as a graph-based semi-supervised learning problem, where only a few low quality webcam images are labelled. The framework is able to track various objects in limited drifting environments. The classification of objects that have been segmented and tracked without the use of a class-specific tracker, has been addressed with an SSL algorithm in [41]. Given only three hand-labelled training examples of each class, the algorithm can perform comparably to equivalent fullysupervised methods, but it requires full-length tracks (it is therefore an off-line process) generated by a perfect tracker (each stream represents a single object), which would be challenging for real applications, where multiple streams are available simultaneously. Learning from time-changing data has mostly appeared in data mining context and various approaches have been proposed. Ensemble-based approaches constitute a widely popular group of these algorithms to handle concept drift [1, 25] and in some recent works class evolution [16], as well. Learn++.NSE [16] is one of the latest ensemblebased classification methods in literature, that generates a classifier using each batch of training data and applies a dynamic weighting strategy to define the share of each ensemble in the overall decision. As success is heavily dependent on labelled data, this method would not be applicable in wild scenarios. Masud in [30] proposed an online clustering algorithm for single stream that employs an active strategy in order to minimize oracle collaboration. Although a considerable body of research has emerged from stream mining, learning from multiple streams in wild environments, that views whole or segments of a stream as a unique element to cluster (or classify), is a less explored area. The methods that have been proposed [22, 5, 35, 11, 10], require equal length streams coming from a fixed number of sources. Thus, they would fail to leverage information from time-varying video tracks. An effective and appropriate algorithm to fit in our scenario is required to: a) learn from multiple streams; b) mine streams with various length and starting points (uneven streams); c) handle the concept drift; d) accommodate new classes; e) deal with partially labelled or unlabelled data; f) be of limited complexity; g) handle multi-dimensional data.\nWe wrap up our review in Table 1, presenting a qualitative look at the extent to which the reviewed methods fulfil the requirements for deploying our scenario. To the best of our knowledge, none of the methods have addressed the problem of learning from multiple streams of visual data. In the next section we discuss our proposed algorithm for stream classification."}, {"heading": "2 Never Ending Visual Information Learning", "text": "In this section we present our Never Ending Visual Information Learning (NEVIL) framework. NEVIL is designed for non-stationary data environments in which no labelled data is available but the learning algorithm is able to interactively query the user to obtain the desired outputs at carefully chosen data points. The NEVIL algorithm is an ensemble of classifiers that are incrementally trained (with no access to previous data) on incoming batches of data, and combined with a form of weighted majority voting.\n2.1 Algorithm Description\nA high-level sketch of the proposed method is shown in Figure 2. A typical tracking algorithm analyses sequential video frames and outputs the movement of targets between the frames, generating multiple streams of visual data. Environmental challenges such as varying illumination, lack of contrast, bad positioning of acquisition devices, blurring caused by motion as well as occlusion make data often noisy and/or partially missing. We address these challenges by a batch divisive strategy, as learning from a data batch may reduce the noise and fill the gaps caused by miss-tracking.\nThe algorithm is provided with a series of data batches Dmit , where mi is the index of the i-th stream present at time slot t, TSt, (not all streams are necessarily present). Note that a stream corresponds to a track generated by the tracking system and a single camera can yield multiple streams. A single batch aggregates B frames. The starting time of each stream is potentially different from stream to stream but batches are aligned between streams. Inside each frame the data corresponds to some pre-selected object representation (e.g. bag of words, histogram) and is out of the scope of this paper.\nThe ensemble obtained by all models generated up to the current time slot TSt is named the composite hypothesis Ht\u22121. With the arrival of the current data batches Dmit , i = 1 \u00b7 \u00b7 \u00b7M, NEVIL tries to predict the class label for each of the batches in current TSt based on the probability estimate p(Ck|D mi t , Ht\u22121), where Ck runs over all the class labels observed so far.\nAlgorithm 1 NEVIL\nInput: Dmit , i = 1, ..., M W0 \u2190 1k H0 \u2190W0 while Dt is True do\nBatch label prediction (Section 2.1.1) p(Ck|D mi t )\u2190 (D mi t , Ht\u22121) Batch Confidence Level Estimation (Section 2.1.2) BCL\u2190 p(Ck|D mi t , Ht\u22121) Multiclass classifier design (Section 2.1.3) ht \u2190Dt Composite model structure and update (Section 2.1.4) Ht \u2190 (ht, Ht\u22121, Wt)\nend while\nThis kind of on-line learning approach addressed in this work can suffer if labelling errors accumulate, which is inevitable. Unrelated objects will sooner or later be assigned the same label or different labels will be assigned to different views of the same object. To help mitigate this issue, we allow the system to interact with a human, to help it stay on track.\nAlgorithm 1 outlines our approach. Initially, the composite model is initialized to yield the same probability to every possible class (uniform prior). When the batches Dmt1 in time slot t become available, NEVIL starts with computing the probabilities p(Ck|D mi t , Ht\u22121) for each batchDmit in the time slot. Once p(Ck|D mi t , Ht\u22121) is obtained, a batch confidence label (BCL) is estimated; if BCL is high enough (above a prespecified threshold), the predicted label\nargmax Ck\np(Ck|D mi t , Ht\u22121)\nis accepted as correct, otherwise the user is requested to label the data batch. The labelled batches (either automatically or manually) are used to generate a new multiclass classifier that is integrated in the composite model, yielding Ht.\nFour tasks need now to be detailed: a) the batch label prediction (by the composite model); b) the BCL estimation; c) the multiclass classifier design in current time slot; d) the composite model structure and update."}, {"heading": "2.1.1 Batch Label Prediction", "text": "A batch Dmtt is a temporal sequence of frames D mt t, f , where f runs over 1 to the batch size B. The composite model, Ht\u22121, can be used to predict directly p(Ck|D mi t, f , Ht\u22121) but not p(Ck|D mi t , Ht\u22121). The batch (multiframe) Bayesian\ninference requires conditional independence\np(Dmit |Ck, Ht\u22121) = p(Dmit,1 , \u00b7 \u00b7 \u00b7 ,D mi t,B|Ck, Ht\u22121) = p(Dmit,1 |Ck, Ht\u22121) \u00b7 \u00b7 \u00b7 p(D mi t,B|Ck, Ht\u22121) = \u220fBj=1 p(D mi t,j |Ck, Ht\u22121)\nFrom there, and assuming equal prior probabilities, it is trivial to conclude that\np(Ck|D mi t , Ht\u22121) = Z\nB\n\u220f j=1\np(Ck|D mi t,j , Ht\u22121), (1)\nwhere Z is a normalization constant. In practice, products of many small probabilities can lead to numerical underflow problems, and so it is convenient to work with the logarithm of the distribution. The logarithm is a monotonic function, so that if p(Ck|D mi t , Ht\u22121) > p(C`|D mi t , Ht\u22121) then\nlog p(Ck|D mi t , Ht\u22121) > log p(C`|D mi t , Ht\u22121).\nThen we can rewrite the decision as choosing the class that maximizes\nlog p(Ck|D mi t , Ht\u22121) = log Z +\nB\n\u2211 j=1\nlog p(Ck|D mi t,j , Ht\u22121)\n(2)\nThe batch label prediction can also be analysed as a problem of combining information from multiple (B)\nclassification decisions. Considering that, per frame, the composite model produces approximations to the a posteriori probabilities of each class, different combination rules can be considered to build the batch prediction from the individual frame predictions [2, 24]. While Equation (1) turns out to be the product rule (or geometric mean), the sum rule (or arithmetic mean) is also often preferred:\np(Ck|D mi t , Ht\u22121) = Z\nB\n\u2211 j=1\np(Ck|D mi t,j , Ht\u22121) (3)\nIn fact some authors have shown that the arithmetic mean outperforms the geometric mean in the presence of strong noise [2, 24]. Experimentally, we will compare both options."}, {"heading": "2.1.2 The Batch Confidence Level Estimation", "text": "Having predicted a class label for a data batch, one needs to decide if the automatic prediction is reliable and accepted or rather a manual labelling be requested.\nVarious criteria have been introduced as uncertainty measures in literature for a probabilistic framework [37]. Perhaps the simplest and most commonly used criterion relies on the probability of the most confident class, defining the confidence level as\nmax Ck\np(Ck|D mi t , Ht\u22121). (4)\nHowever, this criterion only considers information about the most probable label. Thus, it effectively \u201cthrows away\u201d information about the remaining label distribution [37].\nTo correct for this, an option is to adopt a margin confidence measure based on the first and second most probable class labels under the model:\np(C\u2217|Dmit , Ht\u22121)\u2212 p(C\u2217|D mi t , Ht\u22121), (5)\nwhere C\u2217 and C\u2217 are the first and second most probable class labels, respectively. Intuitively, batches with large margins are easy, since the classifier has little doubt in differentiating between the two most likely class labels. Batches with small margins are more ambiguous, thus knowing the true label would help the model discriminate more effectively between them [37].\nNote that while the estimation of the wining class for batch label prediction requires only the comparison of the relative values as given by (1), (2) or (3), both approaches (4) and (5) for the confidence level require the exact computation of the a posteriori probabilities of the classes. This involves computing the normalizing constant associated with (1) or (3), which is specially unstable for (1).\nWe therefore put forward variants of the two previous measures. As an alternative to the margin confidence measure (5), we base the confidence level on the ratio of the first and second most probable class labels:\nBCL = p(C\u2217|Dmit , Ht\u22121)/p(C\u2217|D mi t , Ht\u22121), (6)\nwhich can be directly applied for the sum rule or modified to log p(C\u2217|Dmit , Ht\u22121) \u2212 log p(C\u2217|D mi t , Ht\u22121) for the product rule. Either way, we eliminate the issue with the normalization constant.\nTo come up with an alternative to the most confident class measure, we write the decision as\nmax pk = \u220fBj=1 pk,j\n\u2211k \u220f B j=1 pk,j\n\u2277 T, (7)\nwhere we introduced the following simplifications in notation: pk = p(Ck|D mi t , Ht\u22121) and pk,j = p(Ck|D mi t,j , Ht\u22121). The comparison in Eq. (7) can be rewritten as\n(1\u2212 T) B\n\u220f j=1 pk\u2217 ,j \u2277 T \u2211 k,k 6=k\u2217\nB\n\u220f j=1 pk,j, (8)\nwhere k\u2217 = argmaxk pk. Since we cannot work directly with the log of (8) due to the sum in the denominator, we introduce the simplification of binarizing the classification in each frame, defining p\u0304k\u2217 ,j = \u2211k,k 6=k\u2217 pk,j = 1\u2212 pk\u2217 ,j.\nAccepting the strong assumption of independence for the aggregated class, then\np\u0304k\u2217 = B\n\u220f j=1 p\u0304k\u2217 ,j.\nThis ends up in exchanging the order of the sum and product in the right hand side of (8), which can now be rewritten as\n(1\u2212 T) B\n\u220f j=1\npk\u2217 ,j \u2277 T B\n\u220f j=1 p\u0304k\u2217 ,j. (9)\nNow it is a trivial process to apply the log to obtain a stable decision:\nB\n\u2211 j=1\nlog pk\u2217 ,j \u2277 S + B\n\u2211 j=1 log p\u0304k\u2217 ,j, (10)\nwhere S = log T \u2212 log(1\u2212 T). Figure 3 highlights the characteristics of the four confidence measures by a ternary plot (where every corner indicates a class). This plot graphically depicts the ratios of the three variables (herein, occurrence of each class) as positions in an equilateral triangle. The probability of each class is 1 in its corner of the triangle. Moving inside triangle, the percentage of a specific class decreases linearly with increasing distance from the corner till dropping to 0 at the line opposite it. A rainbow-like color pattern shows the informativeness of different composition of three classes. For all methods, the least reliable batch would lie at the center of triangle, where the posterior label distribution is uniform and thus the least certain under the ensemble. Similarly, the most informative batch lies at the corners where one of the classes has the highest possible probability."}, {"heading": "2.1.3 Multiclass Classifier", "text": "At time slot t, we obtain a new set of batches that are automatically or manually labelled. We assume all the frames belonging to a batch are from the same object (and the underlying tracking system does not mix identities in the time slot period) and therefore the frames inside a batch correspond to observations of the same class. Consider that to the M batches in current time slot correspond L < M labels (some batches can have the same label). We need the design a classifier that can approximate the a posteriori probability function p(ck|D mi t, f ), which gives the probability of the frame belonging to a given class ck, given that D mi t, f was observed.\nA standard way to address this problem is to apply discriminative approaches which predict the conditional probability directly. As an alternative, generative approaches find the joint distribution p(Dmit, f , ck) and then use Bayes\u2019 rule to form the conditional distribution from the generative model. A third option is to find a function f (Dmit, f ), called a discriminant function, which maps each input Dmit, f directly onto a class label. In this case, and although probabilities play no role in the design of the discriminant function, it is still possible to get estimated for the conditional probabilities [6]. Each approach has its relative merits and we evaluate experimentally instantiations of each.\nOne of the challenges we need to handle in a practical scenario is when in a time slot all the batches have the same label (automatically or manually assigned). In these TSs the training of a multiclass classifier is not possible. We resort to one-class classifiers for these time slots, also known as unary classification, to distinguish the single class present in the training set (the batches in the time slot) from all other possible classes [23]."}, {"heading": "2.1.4 The Composite Model Structure and Update", "text": "The composite model Ht in the NEVIL framework is an ensemble of classifiers ht that are incrementally trained (with no access to previous data) on incoming time slots of data as described previously. The individual models ht are combined using a weighted majority voting, where the weights are dynamically updated with respect to the classifiers\u2019 time of design.\nThe prediction outputted by the composite model Ht for a given frame Dmit, f is\np(Ck|D mi t, f , Ht) =\nt\n\u2211 `=1\nWt`h`(CK|D mi t, f ),\nwhere h`(.) is the multiclass classifier trained at TS `, Wt` is the weight assigned to classifier `, adjusted for time t.\nThe weights are updated and normalised at each time slot and chosen to give more credit to more recent knowledge. The weights are chosen from a geometric series 1pt , ..., 1 p2 , 1 p , normalised by the sum of the series to provide proper probability estimates:\nWt` = 1 p(t\u2212`+1)\n\u2211tj=1 1 pj"}, {"heading": "3 Experimental Methodology", "text": "A series of experiments were conducted to explore the capabilities of the proposed framework. In order to study the behaviour of the system facing various conditions, we generated multiple synthetic streams that were organized in different scenarios. We also tested the NEVIL framework with real video data (see section 3.1).\n3.1 Datasets\nIn order to explore the properties of the proposed framework, we evaluated it on multiple datasets covering various possible scenarios in a multi-camera surveillance system.\nWe conducted our experiments on synthetic as well as real datasets. The synthetic dataset is generated in the form of (X,y), where X is a 2-dimensional feature vector, drawn from a Gaussian distribution N(\u00b5X , \u03b4X), and y is the class label.\nSince in real applications visual data may suffer from both gradual and abrupt drift, we tried to simulate both situations in our streams by changing \u00b5X and \u03b4X in the parametric equations; Table 2 presents these parametric equations. In this experiment, we generated 7 classes (C1,C2, ...,C7); for some (C5,C6) data changes gradually while others also experience one (C1,C4,C7), or three (C2,C3) dramatic drifts. This process is similar to the one used in [16].\nThe dataset was organized in 4 different scenarios with different levels of complexity, including streams with gradual drift, sudden drift, re-appearance of objects and non-stationary environments where we have abrupt class and concept drift. Each scenario includes:\n\u2013 Scenario I: gradually drifting streams of 5 classes. \u2013 Scenario II: streams with abrupt drifts of 5 classes. \u2013 Scenario III : re-appearance of objects. \u2013 Scenario IV: a non-stationary environment with class\nevolution as well as concept drift.\nThese scenarios are depicted in Fig. 4. Besides synthetic datasets, we run our experiments on a number of CAVIAR video clips [13] including: OneLeave ShopReenter1, Enter ExitCrossingPaths1, OneShopOneWait1, OneStop Enter2 and WalkBy Shop1front. Due to the presence of different perspectives of the same person, streams are drifting in time (see Fig. 5). These sequences present challenging situations with cluttered scenes, high rates of occlusion, different illumination conditions as well as different scales of the person being captured. We employ an automatic tracking approach [43] to track objects in the scene and generate streams of bounding boxes, which define the tracked objects\u2019 positions. As the method may fail to perfectly track the targets, a stream often includes frames of distinct objects. An hierarchical bag-of-visterms method is applied to represent the tracked objects, resulting in a descriptor vector of size 11110 for each frame (refer to [42] for additional details). In order to avoid the curse of dimensionality that system may suffer from, Principle Component Analysis (PCA) is applied to the full set of descriptor features as a pre-processing step. Hence, the number of features in each stream is reduced to 85 dimensions. As an explanatory sample, figure 6 depicts the streams in the EnterExitCrossingPaths1 scenario.\n3.2 Instantiation of Classifiers\nIn Section 2.1.3, we identified three approaches that have been applied in the literature to obtain the posterior probability. A set of experiments were conducted\nFig. 5: An example of diversity in appearance\nin order to study the behaviour of our framework employing instances of each option. We chose the following methods: Gaussian Mixture Models (GMM) and Naive Bayes as generative approaches, Support Vector Machines (SVM) [8] as one of the most popular discriminant function and logistic regression [17] as a member of discriminative approaches family.\nDesigning a classifier for time slots where batches constitute different labels is quite straightforward. The challenging situation arises when we need to do unary classification. As we employed various approaches with specific characteristics, different strategies are proposed to handle this situation.\nSingle-class SVM classifies each frame as completely similar or different from given class, whereas generative approaches (GMM and Naive Bayes) provide the probabilistic estimation.\nTo the extent of our knowledge, using logistic regression in unary problems is an unexplored topic; existing methods need data generated by at least two classes in order to make the prediction. Therefore, we keep the batches from the last multi-class time slot and combine them with the uni-class time slot to build the training set.\n3.3 Evaluation Criteria\nActive learning aims to achieve high accuracy using as little annotation effort as possible. Thus, a trade-off between accuracy and proportion of labelled data can be considered as one of the most informative measures. Let N denote the total number of batches, MC refer to misclassified batches, then the accuracy of the system\nin a given time slot is formulated as:\nAccuracy = N \u2212 #MC\nN (11)\nThe total accuracy of a system over a period of time is derived from the mean accuracy of all the time slots.\nAssume MLB and TB denote the manually labelled batches and all the batches available during a period (includes one or more time slots), respectively. The Annotation Effort is formulated as:\nAnnotation effort = #MLB #TB\n(12)\nOne expects that the accuracy increases with the increase of annotation effort.\n3.4 Baseline Methods\nTo the best of our knowledge, there is no method that mines multi-dimensional parallel streams in such a nonstationary environment, where the number and length of streams vary greatly. Therefore, we compare our framework with three baseline approaches:\n\u2013 Passive Learning: The first half of all the batches are submitted to the oracle for labelling. Once the labelled set is obtained, a classifier is trained and applied to classify the other half of stream. This method is far from a real online active learning strategy, as it needs complete data available. For datasets in which there is no dramatic distribution evolution between first and second half, we expect that it provides an upper bound to be compared with our method. \u2013 Even/Odd Learning: As an on-line baseline, for a given stream, batches are marked alternately with odd and even integers, where odd batches are kept in a buffer with their true labels. At each time slot, a model is re-trained using the buffer. We then use\nAlgorithm 2 Unwise active learning\nInput: Dmit , i = 1, ..., M h\u2190 empty while Dt is True do\nif t > tint then Batch label prediction p(Ck|D mi t )\u2190 (D mi t , hint)\nBatch Confidence Level Estimation BCL\u2190 p(Ck|D mi t , hint)\nelse Multiclass classifier design hint \u2190Dt\nend if end while\nthis model to classify even batches. Therefore, we may partly follow the distribution changes in this setting leading to better performance than Passive Learning. However, we need to keep all the odd batches, which is far from a practical solution in an on-line scenario.\n\u2013 Unwise active learning: We use an unwise version of the original framework as a baseline, where all the batches occurred before initiation time (tint) would be annotated. For t> tint, NEVIL computes the probabilities of known classes. Once p(Ck|D mi t , htint) are\nobtained, a batch confidence label (BCL) is estimated; if BCL is high (above a pre-defined threshold), the predicted label\nargmax Ck\np(Ck|D mi t , htint)\nis accepted as correct label of the batch, otherwise the user is requested to label the batch. The method is summarized in Algorithm 2. Despite meticulous selection of queries, as the model is not updated, the algorithm may establish a lower bound the level of performance that can be expected in an evaluation."}, {"heading": "4 Results", "text": "Firstly, multiple tests were run to determine the optimal batch size for each dataset to be explored. Batch size was varied between 1% to 50% of the size of the longest stream available in each dataset. Experiments were repeated for 50 equally spaced values in that range. The optimal batch size varies and is influenced by the characteristics of the streams present in each dataset. Optimal batch sizes have been observed to range between 30 and 35 for real video streams and between 200 and 300 for synthetic sequences.\nTable 3 provides a summary of the performance of Passive Learning and Even/Odd Learning using various classifiers on all datasets mentioned in Section 3.1. Since different classifiers provide varying performances on different datasets, the need for a procedure that carefully assesses algorithms seems inevitable. We applied Friedman test [14] that provides a non-parametric rank based statistical significance test. This test is similar to parametric repeated measures ANOVA, which tests if there is a significant difference between the rank of different treatments across multiple attempts. When the test runs over all the datasets shows that null hypothesis is verified which means that type of the classifier has no significant effect on the overall performance of baseline method in real applications. However, the test shows that Logistic Regression has yielded weak results for synthetic data in both learning methods. When we perceive the superior learners based on the mean rank for various scenarios, generative approaches perform fairly better in the synthetic datasets, while discriminative methods win for real video clips. Since the dimension of real data is large, while synthetic data is generated in 2D space, these results also emphasizes\nthe difficulties that generative models face in high-dimensional spaces. As mentioned in 3.4, we expect better or equal results from Even/Odd Learning than Passive Learning which is the case in all the settings applied discriminative approaches as well as almost all used generative methods. Unexpected behaviour of generative methods when applies on OneShopOneWait1 dataset can be explained by high bias of these methods when trying to model such complex data.\nFigure 7 presents the results of multiple settings on Scenarios I,..., IV. One prominent observation on all these results is that using geometric mean (Prod) to combine information of frames in a given batch and the modified most measure (MMC) to select most informative batches give the best performance.\nFigure 8 illustrates the comparative results across multiple classifiers on Scenarios I,..., IV from which we can observe that: a) NEVIL achieves more than 90%\naccuracy with less than 15% annotation effort in all the datasets, which obviously outperforms baseline approaches. For all the sets, we reached equal accuracy to Passive as well asEven/Odd Learning while spending much less human resources. b) Naive classifier gives the best overall performance which emphasise the more flexible nature of generative models than discriminative ones. Needless to say, following the results depicted in Figure 7 we only present the result of winner setting.\nFigure 9 shows the comparative results on some CAVIAR sequences with various NEVIL configurations. We observe that unlike synthetic scenarios, employing arithmetic mean (SUM) as combination method and modified margin (MM) as selection criteria present winner results. The presence of challenging noise in real data explains the different behaviour of the framework.\nFigure 10 presents the performance of NEVIL employing various classifiers on multiple CAVIAR sequences. The NEVIL framework achieves over 80% accuracy with less than 25% of labelling and in most cases, that is clearly superior to baseline methods. Contrary to results obtained from synthetic data, Discriminative models outperforms than Generative ones. Higher dimension of video streams (herein, equal to 85) may explain this behaviour. Generative models are commonly trained using Maximum-Likelihood Estimation (MLE) that especially for high dimensional data, the likelihood can have many local maxima. Thus, finding the global max-\n(c) ScenarioIII (d) ScenarioIV\nimum affects the performance and renders the approach less practical.\nFinally, Figure 11 presents the results obtained across multiple CAVIAR scenarios from the most successful setting, which means SVM, SUM, and MMC as base classifier, combination rule and selection criteria, respectively. Under such setting, NEVIL achieves 80% accuracy with 30% annotation effort for OneStopMoveEnter1, the most complex scenario with 42 streams from 14 classes."}, {"heading": "5 Conclusions", "text": "In this paper, we address the problem of learning from visual streams generated in a multi-camera surveillance scenario. We look at the problem as mining parallel high-dimensional data. Inspired from active learning strategies, in our proposed framework (NEVIL) an oracle provides labelled batches; multiple informativeness measures are used to determine when the oracle is used. As base learners are bottlenecks of any learning pipeline, various groups of classifiers were studied and experimentally evaluated. We ran the experiments on synthetic as well as real datasets.\nIn synthetic scenarios, where low dimensional clean data is available, applying the geometric mean and the modified most confident measure gives the best and least expensive (in terms of annotation cost) results. However, to get the highest accuracy from noisy visual data we need to apply arithmetic mean for com-\nbining information and modified margin to select the most informative batches.\nAnother question we tried to answer was which classifier to use on a given dataset. In a low dimensional clean data, generative approaches give the best results, however obtaining robust and stable results from high dimensional data is too difficult, as shown by our experiments. The best performance is obtained through discriminative approaches.\nWhile empirical results demonstrated the functionality of the framework, we are currently working the controlling the complexity of the framework which makes it applicable for never-ending scenarios. We would also like to employ special features of video streams generated in a surveillance scenario in order reduce queries as many as possible.\nAcknowledgements The authors would like to thank Fundac\u0327a\u0303o para a Cie\u0302ncia e a Tecnologia (FCT)-Portugal for financing this work through the grant SFRH/BD/80013/2011."}], "references": [{"title": "StreamKM++: A clustering algorithms for data streams", "author": ["MR Ackermann", "C Lammersen", "M M\u00e4rtens", "C Raupach", "C Sohler", "K Swierkot"], "venue": "Journal of Experimental Algorithmics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "On combining classifiers using sum and product rules. Pattern Recognition Letters", "author": ["LA Alexandre", "AC Campilho", "M Kamel"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Person identification in webcam images:an application of semisupervised learning", "author": ["M Balcan", "A Blum", "PP Choi", "J Lafferty", "B Pantano", "MR Rwebangira", "X Zhu"], "venue": "In: International Conference on Machine Learning Workshop on Learning from Partially Classified Training Data,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Online choice of active learning algorithms", "author": ["Y Baram", "R El-Yaniv", "K Luz"], "venue": "Journal of Machine Learning Research", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Online clustering of parallel data streams. Data Knowledge Engineering", "author": ["J Beringer", "E H\u00fcllermeier"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["CM Bishop"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Filling the gap in quality assessment of video object tracking", "author": ["P Carvalho", "JS Cardoso", "L Corte-Real"], "venue": "Image and Vision Computing", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "LIBSVM: A library for support vector machines", "author": ["CC Chang", "CJ Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Tracking multiple people with a multi-camera system", "author": ["T hsun Chang", "S Gong"], "venue": "IEEE Workshop on Multi-Object Tracking", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "A clustering algorithm for multiple data streams based on spectral component similarity", "author": ["L Chen", "L Zou", "L Tu"], "venue": "Information Sciences", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Clustering parallel data streams. Data Mining and Knowledge Discovery in Real Life Applications I-Tech Education and Publishing", "author": ["Y Chen"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Semisupervised text classification using partitioned em", "author": ["G Cong", "WS Lee", "H Wu", "B Liu"], "venue": "In: 11th International Conference on Database Systems for Advanced Applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J Dem\u0161ar"], "venue": "Journal of Machine Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Issues in automated visual surveillance", "author": ["AR Dick", "MJ Brooks"], "venue": "Proc. VIIth Digital Image,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Incremental learning of concept drift in nonstationary environments", "author": ["R Elwell", "R Polikar"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Liblinear: A library for large linear classification", "author": ["RE Fan", "KW Chang", "CJ Hsieh", "XR Wang", "CJ Lin"], "venue": "Journal of Machine Learning Research", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Realworld semi-supervised learning of pos-taggers for low-resource languages", "author": ["D Garrette", "J Mielens", "J Baldridge"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL-2013),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "A visualization framework for team sports captured using multiple static cameras", "author": ["R Hamid", "RK Kumar", "JK Hodgins", "IA Essa"], "venue": "Computer Vision and Image Understanding", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Appearance modeling for tracking in multiple non-overlapping cameras", "author": ["O Javed"], "venue": "IEEE International Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Automated Multi-Camera Surveillance: Algorithms and Practice", "author": ["O Javed", "M Shah"], "venue": "The International Series in Video Computing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "On the need for time series data mining benchmarks: A survey and empirical demonstration. Data Mining Knowledge Discovery", "author": ["E Keogh", "S Kasetty"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "A survey of recent trends in one class classification. In: Coyle L,  Active Mining of Parallel Video Streams", "author": ["SS Khan", "M GMadden"], "venue": "Freyne J (eds) Artificial Intelligence and Cognitive Science, Lecture Notes in Computer Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "On combining classifiers", "author": ["J Kittler", "M Hatef", "RPW Duin", "J Matas"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Dynamic weighted majority: An ensemble method for drifting concepts", "author": ["JZ Kolter", "MA Maloof"], "venue": "Journal of Machine Learning Research", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Intercamera association of multi-target tracks by online learned appearance affinity models", "author": ["CH Kuo", "C Huang", "R Nevatia"], "venue": "Proceedings of the 11th European Conference on Computer Vision: Part I, Springer-Verlag, Berlin, Heidelberg,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Unsupervised improvement of visual detectors using co-training", "author": ["A Levin", "PA Viola", "Y Freund"], "venue": "Ninth IEEE International Conference on Computer Vision,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "Semi-supervised learning for natural language", "author": ["P Liang"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "Classification and novel class detection in data streams with active mining", "author": ["MM Masud", "J Gao", "L Khan", "J Han", "BM Thuraisingham"], "venue": "PacificAsia Conference Advances in Knowledge Discovery and Data Mining,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Classification and novel class detection in concept-drifting data streams under time constraints", "author": ["MM Masud", "J Gao", "L Khan", "J Han", "BM Thuraisingham"], "venue": "IEEE Transactions on Knowledge Data Engineering", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Vehicle tracking across nonoverlapping cameras using joint kinematic and appearance features", "author": ["BC Matei", "HS Sawhney", "S Samarasekera"], "venue": "Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Incremental learning of feature space and classifier for face recognition", "author": ["S Ozawa", "SL Toh", "S Abe", "S Pang", "N Kasabov"], "venue": "Neural Networks", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Localization and trajectory reconstruction in surveillance cameras with nonoverlapping views", "author": ["R Pflugfelder", "H Bischof"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Active learning with feedback on both features and instances", "author": ["H Raghavan", "O Madani", "R Jones", "P Kaelbling"], "venue": "Journal of Machine Learning Research", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Hierarchical clustering of time-series data streams", "author": ["PP Rodrigues", "J Gama", "JP Pedroso"], "venue": "IEEE Transaction on Knowledge Data Engineering", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Semi-supervised self-training of object detection models", "author": ["C Rosenberg", "M Hebert", "H Schneiderman"], "venue": "Seventh IEEE Workshop on Applications of Computer Vision,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Active learning literature survey", "author": ["B Settles"], "venue": "Tech. Rep. 1648,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Unsupervised learning of discriminative edge measures for vehicle matching between non-overlapping cameras", "author": ["Y Shan", "HS Sawhney", "R Kumar"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2005}, {"title": "Semi-automatic video annotation based on active learning with multiple complementary predictors", "author": ["Y Song", "X sheng Hua", "L rong Dai", "M Wang"], "venue": "Proceedings of ACM SIGMM International Workshop on Multimedia Information Retrieval,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Tracking-based semisupervised learning", "author": ["A Teichman", "S Thrun"], "venue": "Proceedings of Robotics: Science and Systems,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2011}, {"title": "Video object matching across multiple independent views using local descriptors and adaptive learning", "author": ["LF Teixeira", "L Corte-Real"], "venue": "Pattern Recognition Letters", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Automatic description of object appearances in a wide-area surveillance scenario", "author": ["LF Teixeira", "P Carvalho", "JS Cardoso", "L Corte-Real"], "venue": "IEEE International Conference on Image Processing,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Semi-supervised kernel density estimation for video annotation", "author": ["M Wang", "XS Hua", "T Mei", "R Hong", "G Qi", "Y Song", "LR Dai"], "venue": "Comput Vis Image Underst", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "Intelligent multi-camera video surveillance: A review", "author": ["X Wang"], "venue": "Pattern Recognition Letters", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "An online-optimized incremental learning framework for video semantic classification", "author": ["J Wu"], "venue": "ACM International Conference on Multimedia,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2004}, {"title": "Semisupervised multi-instance multi-label learning for video annotation", "author": ["XS Xu", "Y Jiang", "X Xue", "ZH Zhou"], "venue": "Proceedings of the 20th ACM International Conference on Multimedia,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Recording for hours, days, and possibly years provides massive amount of information coming from an evolving environment in where traditional learning methods fail to reflect evolution taking place [15].", "startOffset": 198, "endOffset": 202}, {"referenceID": 39, "context": "Moreover, accompanying person A with person B or C, group movement (both are identified as a single object) and prolonged occlusion might occur, which might lead to track loss or mistaken identities [42].", "startOffset": 199, "endOffset": 203}, {"referenceID": 27, "context": "To reduce costs of annotation, semi-supervised learning (SSL) approaches have been extensively explored in limited labelled and usually abundant un-labelled data scenarios [29, 12, 18, 28]; however deploying SSL for evolving visual data in a non-stationary environments (where both concept drift and class evolution are present) is still an unexplored area.", "startOffset": 172, "endOffset": 188}, {"referenceID": 11, "context": "To reduce costs of annotation, semi-supervised learning (SSL) approaches have been extensively explored in limited labelled and usually abundant un-labelled data scenarios [29, 12, 18, 28]; however deploying SSL for evolving visual data in a non-stationary environments (where both concept drift and class evolution are present) is still an unexplored area.", "startOffset": 172, "endOffset": 188}, {"referenceID": 16, "context": "To reduce costs of annotation, semi-supervised learning (SSL) approaches have been extensively explored in limited labelled and usually abundant un-labelled data scenarios [29, 12, 18, 28]; however deploying SSL for evolving visual data in a non-stationary environments (where both concept drift and class evolution are present) is still an unexplored area.", "startOffset": 172, "endOffset": 188}, {"referenceID": 26, "context": "To reduce costs of annotation, semi-supervised learning (SSL) approaches have been extensively explored in limited labelled and usually abundant un-labelled data scenarios [29, 12, 18, 28]; however deploying SSL for evolving visual data in a non-stationary environments (where both concept drift and class evolution are present) is still an unexplored area.", "startOffset": 172, "endOffset": 188}, {"referenceID": 3, "context": "Several researchers have shown that the meticulous selection of instances that need to be labelled (mostly addressed in active learning (AL) strategies) could lead to better performance with less effort [4, 34].", "startOffset": 203, "endOffset": 210}, {"referenceID": 32, "context": "Several researchers have shown that the meticulous selection of instances that need to be labelled (mostly addressed in active learning (AL) strategies) could lead to better performance with less effort [4, 34].", "startOffset": 203, "endOffset": 210}, {"referenceID": 8, "context": "Considerable body of multi-camera surveillance research assume that adjacent camera view have overlap [9, 26, 19, 45], whereas [20, 38, 21, 33, 31] require non-overlapping views.", "startOffset": 102, "endOffset": 117}, {"referenceID": 24, "context": "Considerable body of multi-camera surveillance research assume that adjacent camera view have overlap [9, 26, 19, 45], whereas [20, 38, 21, 33, 31] require non-overlapping views.", "startOffset": 102, "endOffset": 117}, {"referenceID": 17, "context": "Considerable body of multi-camera surveillance research assume that adjacent camera view have overlap [9, 26, 19, 45], whereas [20, 38, 21, 33, 31] require non-overlapping views.", "startOffset": 102, "endOffset": 117}, {"referenceID": 42, "context": "Considerable body of multi-camera surveillance research assume that adjacent camera view have overlap [9, 26, 19, 45], whereas [20, 38, 21, 33, 31] require non-overlapping views.", "startOffset": 102, "endOffset": 117}, {"referenceID": 18, "context": "Considerable body of multi-camera surveillance research assume that adjacent camera view have overlap [9, 26, 19, 45], whereas [20, 38, 21, 33, 31] require non-overlapping views.", "startOffset": 127, "endOffset": 147}, {"referenceID": 36, "context": "Considerable body of multi-camera surveillance research assume that adjacent camera view have overlap [9, 26, 19, 45], whereas [20, 38, 21, 33, 31] require non-overlapping views.", "startOffset": 127, "endOffset": 147}, {"referenceID": 19, "context": "Considerable body of multi-camera surveillance research assume that adjacent camera view have overlap [9, 26, 19, 45], whereas [20, 38, 21, 33, 31] require non-overlapping views.", "startOffset": 127, "endOffset": 147}, {"referenceID": 31, "context": "Considerable body of multi-camera surveillance research assume that adjacent camera view have overlap [9, 26, 19, 45], whereas [20, 38, 21, 33, 31] require non-overlapping views.", "startOffset": 127, "endOffset": 147}, {"referenceID": 29, "context": "Considerable body of multi-camera surveillance research assume that adjacent camera view have overlap [9, 26, 19, 45], whereas [20, 38, 21, 33, 31] require non-overlapping views.", "startOffset": 127, "endOffset": 147}, {"referenceID": 30, "context": "Much of the learning literature is concerned with a stationary environment, where fixed and known number of categories to be recognized and enough resources (labelled data, memory and computational power) are available [32, 46].", "startOffset": 219, "endOffset": 227}, {"referenceID": 43, "context": "Much of the learning literature is concerned with a stationary environment, where fixed and known number of categories to be recognized and enough resources (labelled data, memory and computational power) are available [32, 46].", "startOffset": 219, "endOffset": 227}, {"referenceID": 37, "context": "various SSL methods have been proposed for video annotation [39, 44, 47].", "startOffset": 60, "endOffset": 72}, {"referenceID": 41, "context": "various SSL methods have been proposed for video annotation [39, 44, 47].", "startOffset": 60, "endOffset": 72}, {"referenceID": 44, "context": "various SSL methods have been proposed for video annotation [39, 44, 47].", "startOffset": 60, "endOffset": 72}, {"referenceID": 2, "context": "In [3], the person identification task is posed as a graph-based semi-supervised learning problem, where only a few low quality webcam images are labelled.", "startOffset": 3, "endOffset": 6}, {"referenceID": 38, "context": "The classification of objects that have been segmented and tracked without the use of a class-specific tracker, has been addressed with an SSL algorithm in [41].", "startOffset": 156, "endOffset": 160}, {"referenceID": 0, "context": "Ensemble-based approaches constitute a widely popular group of these algorithms to handle concept drift [1, 25] and in some recent works class evolution [16], as well.", "startOffset": 104, "endOffset": 111}, {"referenceID": 23, "context": "Ensemble-based approaches constitute a widely popular group of these algorithms to handle concept drift [1, 25] and in some recent works class evolution [16], as well.", "startOffset": 104, "endOffset": 111}, {"referenceID": 14, "context": "Ensemble-based approaches constitute a widely popular group of these algorithms to handle concept drift [1, 25] and in some recent works class evolution [16], as well.", "startOffset": 153, "endOffset": 157}, {"referenceID": 14, "context": "NSE [16] is one of the latest ensemblebased classification methods in literature, that generates a classifier using each batch of training data and applies a dynamic weighting strategy to define the share of each ensemble in the overall decision.", "startOffset": 4, "endOffset": 8}, {"referenceID": 28, "context": "Masud in [30] proposed an online clustering algorithm for single stream that employs an active strategy in order to minimize oracle collaboration.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "The methods that have been proposed [22, 5, 35, 11, 10], require equal length streams coming from a fixed number of sources.", "startOffset": 36, "endOffset": 55}, {"referenceID": 4, "context": "The methods that have been proposed [22, 5, 35, 11, 10], require equal length streams coming from a fixed number of sources.", "startOffset": 36, "endOffset": 55}, {"referenceID": 33, "context": "The methods that have been proposed [22, 5, 35, 11, 10], require equal length streams coming from a fixed number of sources.", "startOffset": 36, "endOffset": 55}, {"referenceID": 10, "context": "The methods that have been proposed [22, 5, 35, 11, 10], require equal length streams coming from a fixed number of sources.", "startOffset": 36, "endOffset": 55}, {"referenceID": 9, "context": "The methods that have been proposed [22, 5, 35, 11, 10], require equal length streams coming from a fixed number of sources.", "startOffset": 36, "endOffset": 55}, {"referenceID": 30, "context": "[32, 46] \u00d7 \u00d7 \u221a \u00d7 SL Constrained MD", "startOffset": 0, "endOffset": 8}, {"referenceID": 43, "context": "[32, 46] \u00d7 \u00d7 \u221a \u00d7 SL Constrained MD", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "[1] \u00d7 \u00d7 \u221a \u00d7 SL Unconstrained MD", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[16] \u00d7 \u00d7 \u221a \u221a SL Unconstrained MD", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] \u00d7 \u00d7 \u221a \u221a SSL Constrained MD", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] \u00d7 \u00d7 \u221a \u00d7 Clustering Unconstrained MD", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5, 35, 11, 10] \u221a \u00d7 \u221a \u221a Clustering Constrained 1D", "startOffset": 0, "endOffset": 15}, {"referenceID": 33, "context": "[5, 35, 11, 10] \u221a \u00d7 \u221a \u221a Clustering Constrained 1D", "startOffset": 0, "endOffset": 15}, {"referenceID": 10, "context": "[5, 35, 11, 10] \u221a \u00d7 \u221a \u221a Clustering Constrained 1D", "startOffset": 0, "endOffset": 15}, {"referenceID": 9, "context": "[5, 35, 11, 10] \u221a \u00d7 \u221a \u221a Clustering Constrained 1D", "startOffset": 0, "endOffset": 15}, {"referenceID": 1, "context": "Considering that, per frame, the composite model produces approximations to the a posteriori probabilities of each class, different combination rules can be considered to build the batch prediction from the individual frame predictions [2, 24].", "startOffset": 236, "endOffset": 243}, {"referenceID": 22, "context": "Considering that, per frame, the composite model produces approximations to the a posteriori probabilities of each class, different combination rules can be considered to build the batch prediction from the individual frame predictions [2, 24].", "startOffset": 236, "endOffset": 243}, {"referenceID": 1, "context": "In fact some authors have shown that the arithmetic mean outperforms the geometric mean in the presence of strong noise [2, 24].", "startOffset": 120, "endOffset": 127}, {"referenceID": 22, "context": "In fact some authors have shown that the arithmetic mean outperforms the geometric mean in the presence of strong noise [2, 24].", "startOffset": 120, "endOffset": 127}, {"referenceID": 35, "context": "Various criteria have been introduced as uncertainty measures in literature for a probabilistic framework [37].", "startOffset": 106, "endOffset": 110}, {"referenceID": 35, "context": "Thus, it effectively \u201cthrows away\u201d information about the remaining label distribution [37].", "startOffset": 86, "endOffset": 90}, {"referenceID": 35, "context": "Batches with small margins are more ambiguous, thus knowing the true label would help the model discriminate more effectively between them [37].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "In this case, and although probabilities play no role in the design of the discriminant function, it is still possible to get estimated for the conditional probabilities [6].", "startOffset": 170, "endOffset": 173}, {"referenceID": 21, "context": "We resort to one-class classifiers for these time slots, also known as unary classification, to distinguish the single class present in the training set (the batches in the time slot) from all other possible classes [23].", "startOffset": 216, "endOffset": 220}, {"referenceID": 14, "context": "This process is similar to the one used in [16].", "startOffset": 43, "endOffset": 47}, {"referenceID": 40, "context": "We employ an automatic tracking approach [43] to track objects in the scene and generate streams of bounding boxes, which define the tracked objects\u2019 positions.", "startOffset": 41, "endOffset": 45}, {"referenceID": 39, "context": "An hierarchical bag-of-visterms method is applied to represent the tracked objects, resulting in a descriptor vector of size 11110 for each frame (refer to [42] for additional details).", "startOffset": 156, "endOffset": 160}, {"referenceID": 7, "context": "We chose the following methods: Gaussian Mixture Models (GMM) and Naive Bayes as generative approaches, Support Vector Machines (SVM) [8] as one of the most popular discriminant function and logistic regression [17] as a member of discriminative approaches family.", "startOffset": 134, "endOffset": 137}, {"referenceID": 15, "context": "We chose the following methods: Gaussian Mixture Models (GMM) and Naive Bayes as generative approaches, Support Vector Machines (SVM) [8] as one of the most popular discriminant function and logistic regression [17] as a member of discriminative approaches family.", "startOffset": 211, "endOffset": 215}, {"referenceID": 12, "context": "We applied Friedman test [14] that provides a non-parametric rank based statistical significance test.", "startOffset": 25, "endOffset": 29}], "year": 2014, "abstractText": "The practicality of a video surveillance system is adversely limited by the amount of queries that can be placed on human resources and their vigilance in response. To transcend this limitation, a major effort under way is to include software that (fully or at least semi) automatically mines video footage, reducing the burden imposed to the system. Herein, we propose a semi-supervised incremental learning framework for evolving visual streams in order to develop a robust and flexible track classification system. Our proposed method learns from consecutive batches by updating an ensemble in each time. It tries to strike a balance between performance of the system and amount of data which needs to be labelled. As no restriction is considered, the system can address many practical problems in an evolving multi-camera scenario, such as concept drift, class evolution and various length of video streams which have not been addressed before. Experiments were performed on synthetic as well as real-world visual data in non-stationary environments, showing high accuracy with fairly little human collaboration.", "creator": "LaTeX with hyperref package"}}}