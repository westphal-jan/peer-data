{"id": "1405.4807", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2014", "title": "Scalable Semidefinite Relaxation for Maximum A Posterior Estimation", "abstract": "maximum a posteriori ( map ) inference over discrete markov random fields is a fundamental task spanning a wide spectrum of real - world applications, which is known erroneously to be np - hard for general nonlinear graphs. in this paper, we propose a novel semidefinite relaxation formulation ( referred to as sdr ) to estimate twice the map assignment. algorithmically, we develop an accelerated variant of the alternating direction method of multipliers ( referred to as sdpad - lr ) that can effectively exploit the special model structure of the new relaxation. encouragingly, the proposed procedure allows solving sdr for large - scale problems, e. g., problems on a grid graph comprising hundreds of thousands of variables with multiple states per node. accurately compared with prior sdp solvers, sdpad - lr is capable of attaining comparable accuracy while necessarily exhibiting remarkably improved scalability, in contrast unrelated to the commonly held belief that semidefinite relaxation can only been applied on small - scale mrf problems. we have evaluated the performance of sdr on various benchmark datasets including mini opengm2 and pic in terms of both the quality of the solutions structure and computation time. experimental results demonstrate that for a broad class of problems, sdpad - lr outperforms state - of - the - art algorithms in producing better feasible map assignment in an efficient manner.", "histories": [["v1", "Mon, 19 May 2014 16:58:24 GMT  (59kb)", "http://arxiv.org/abs/1405.4807v1", "accepted to International Conference on Machine Learning (ICML 2014)"]], "COMMENTS": "accepted to International Conference on Machine Learning (ICML 2014)", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.IT math.IT math.OC stat.ML", "authors": ["qi-xing huang", "yuxin chen", "leonidas j guibas"], "accepted": true, "id": "1405.4807"}, "pdf": {"name": "1405.4807.pdf", "metadata": {"source": "META", "title": "Scalable Semidefinite Relaxation for Maximum A Posterior Estimation", "authors": ["Qixing Huang", "Yuxin Chen"], "emails": ["HUANGQX@STANFORD.EDU", "YXCHEN@STANFORD.EDU", "GUIBAS@CS.STANFORD.EDU"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n48 07\nv1 [\ncs .L\nG ]\n1 9\nM ay"}, {"heading": "1. Introduction", "text": "Computing the maximum a posteriori (MAP) assignment in a graphical model is a central inference task span-\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nning a wide scope of scenarios (Wainwright & Jordan, 2008), ranging from traditional applications in graph matching, stereo reconstruction, object detection, errorcorrecting codes, gene mapping, etc., to a more recent application in estimating consistent object orientations from noisy pairwise measurements (Crandall et al., 2011). For general graphs, this problem is well-known to be NPhard (Shimony, 1994). However, due in part to its importance in practice, a large body of algorithms have been proposed to approximate MAP estimates by solving various convex relaxation formulations.\nAmong those methods based on convex surrogates, semidefinite relaxation usually strictly dominates other formulations based on linear programming or quadratic programming in terms of solution quality. Despite its superiority in obtaining more accurate estimates, however, the most significant challenge that limits the applicability of any semidefinite relaxation paradigm on real problems is efficiency. So far existing general-purpose SDP solvers can only handle problems with small dimensionality.\nIn this paper, we propose a novel semidefinite relaxation approach (referred to as SDR) for second-order MAP inference in pairwise undirected graphical models. Our key observation is that the marginalization constraints in a typical linear programming relaxation (c.f.(Kumar et al., 2009)) can be subsumed by combing a semidefinite conic constraint with a small set of linear constraints. As a result, SDR admits a concise set of nicely decoupled constraints, which allows us to develop an accelerated variant (referred as SDPAD-LR) of the alternating direction method of multipliers method (ADMM) that is scalable to very large-scale problems.\nOn a standard PC, we have successfully applied SDR on dense problems of dimensions of (#states \u00d7 #variables) up to five thousand, and on grid-structured problems up to 105 variables each with dozens of states per node.\nPractically, SDPAD-LR performs remarkably well on a variety of problems. We have evaluated\nSDPAD-LR on two collections of benchmark datasets: OPENGM2 (Kappes et al., 2013a) and a probabilistic inference challenge (PIC, 2011). Each benchmark consists of multiple categories of problems derived from various MAP estimation tasks. Experimental results demonstrate that SDPAD-LR outperforms the state-of-the-art algorithms in computational speed, while often obtaining better MAP estimates."}, {"heading": "1.1. Background", "text": "There is a vast literature concerning MAP estimation over discrete undirected graphical models and it is beyond the scope of this paper to discuss all existing algorithms. Interested readers are referred to (Wainwright & Jordan, 2008) for an in-depth introduction to this topic. In the following, we focus on methods that involve convex relaxation, which are the most relevant to our approach.\nMany prior convex relaxation techniques are derived from the original graph structure underlying the MAP estimation problem, among which linear programming relaxation (LPR) methods (Chekuri et al., 2004; Wainwright et al., 2005) are the most popular. In addition to LPR, researchers have considered alternative convex relaxations, e.g., quadratic relaxation (QP-RL) (Ravikumar et al., 2010) and second-order cone relaxation (SOCP-MS) (Kumar et al., 2009). In the seminal work of (Kumar et al., 2009), the authors evaluate various convex relaxation approaches, and assert that LPR dominates QP-RL and SOCP-MS. However, as will be shown later, LPR is further dominated by a standard SDP relaxation (Wainwright & Jordan, 2008), which is one of the main foci of this paper.\nA recent line of approaches have aimed at obtaining tighter convex relaxations by incrementally adding higher-order interactions to enforce proper marginalization over groups of variables (Sontag et al., 2012; Komodakis & Paragios, 2008; Batra et al., 2011). Despite the practical success of these approaches, it remains an open problem to analyze their behavior \u2014 for example, to decide whether a polynomial number of clusters are sufficient.\nThere have been several attempts in applying semidefinite relaxation to obtain MAP assignment (Torr, 2003; Olsson et al., 2007; Wang et al., 2013; Peng et al., 2012). However, most of these methods are primarily designed for binary MAP estimation problems. In a recent work, (Peng et al., 2012) considered a general MAP estimation problem, where each variable has multiple states. The key difference between the proposed formulation and that of (Peng et al., 2012) is that we utilize the semidefinite cone constraint to prune redundant linear marginalization constraints. This leads to a concise set of loosely decoupled constraints, which is important in developing effective optimization paradigms."}, {"heading": "1.2. Notation", "text": "Before proceeding, we introduce a few notations that will be used throughout the paper. For any linear operatorA, we let A\u22c6 represent its conjugate operator. Denote by RN\u00d7M+ the set of N \u00d7 M matrices with nonnegative entries, and (\u00b7)+ : R\nN\u00d7M \u2192 RN\u00d7M+ the projection operator onto R\nN\u00d7M + . For any symmetric matrix M , we use M 0 to represent the projection of M onto the positive semidefinite cone. Finally, we denote by \u2016X\u2016F the Frobenius norm of a matrix X ."}, {"heading": "2. MAP Estimation and SDP Relaxation", "text": "We start with state configurations over n discrete random variables X = {x1, \u00b7 \u00b7 \u00b7 , xn}. Without loss of generality, assume that each xi takes values in a discrete state set {1, \u00b7 \u00b7 \u00b7 ,m}. Consider a pairwise Markov random field (MRF) G parameterized by the potentials (or sufficient statistics) wi(xi) for all vertices and wij(xi, xj) for all edges (i, j) \u2208 G. The energy (or log-likelihood) associated with this MRF is given by\nf(X ) =\nn \u2211\ni=1\nwi(xi) + \u2211\n(i,j)\u2208E\nwij(xi, xj). (1)\nThe goal of MAP estimation is then to compute the configuration of states that maximizes the energy \u2013 the most probable state assignment XM ."}, {"heading": "2.1. Semidefinite Programming Relaxation (SDR)", "text": "MAP estimation over discrete sets is an NP-hard combinatorial problem, and can be cast as an integer quadratic program (IQP). Denote by xi = (xi,1, \u00b7 \u00b7 \u00b7 , xi,m)\u22a4 \u2208 {0, 1}m a binary vector such that xi,j = 1 if and only if xi = j. Then MAP estimation is equivalent to the following integer program.\n(IQP): maximize x\u2208{0,1}nm\nn \u2211\ni=1\n\u3008wi,xi\u3009+ \u2211\n(i,j)\u2208G\n\u2329\nW ij ,xix \u22a4 j\n\u232a\nsubject to 1\u22a4xi = 1, 1 \u2264 i \u2264 n, (2)\nwhere wi and W ij encode the corresponding potentials.\nThe hardness of the above IQP arises in two aspects: (i) x are binary-valued, and (ii) the objective function is a quadratic function of these binary variables. These motivate us to relax the constraints in some appropriate manner, leading to our semidefinite relaxation. In the sequel, we present the proposed relaxation in a step-by-step fashion.\n1) In the same spirit as existing convex formulations (e.g., (Kumar et al., 2009; Peng et al., 2012)), we introduce a binary block matrix X := xx\u22a4 \u2208 {0, 1}nm\u00d7nm\nto accommodate quadratic objective terms:\nX =\n\n    \nDiag(x1) X12 \u00b7 \u00b7 \u00b7 X1n\nX\u22a412 Diag(x2) ... ... ... \u00b7 \u00b7 \u00b7 . . . ... X\u22a41n \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Diag(xn)\n\n     ,\nwhich apparently exhibits the following properties:\nXii = xix \u22a4 i = Diag(xi), 1 \u2264 i \u2264 n. (3)\n2) The non-convex constraint X = xx\u22a4 is then relaxed and replaced by X xx\u22a4, which by Schur complement condition is equivalent to the following semidefinite conic constraint :\n(\n1 x\u22a4\nx X\n)\n0. (4)\n3) The binary constraints x \u2208 {0, 1}nm and X \u2208 {0, 1}nm\u00d7nm are replaced by weaker linear constraints\nX \u2265 0.\nNote that the constraints 0 \u2264 x \u2264 1 and X \u2264 1\u00b71\u22a4 are essentially subsumed by the constraints (2), (3), and (4) taken together. For the sake of numerical efficiency, we further relax the non-negative constraint X \u2265 0 to be\nX ij \u2265 0, (i, j) \u2208 G. (5)\nAs we will see later, this relaxation is crucial in accelerating SDP solvers for large-scale problems.\nRemark 1. The non-negativity constraints described in (5) are necessary since otherwise SDR becomes loose for submodular functions. Below is an example in the presence of 2 variables each having 2 states:\nw1 =\n[\n2 0\n]\n, w2 =\n[\n\u22123 0\n]\n, W 12 =\n[\n0 2 2 0\n]\n.\nIt is clear that W 12 satisfies the submodular property. However, the optimizer of SDR after dropping the constraint Xij \u2265 0 is given by\nx1 = 1\n3\n[\n1 2\n]\n, x2 = 1\n9\n[\n8 1\n]\n, X12 = 1\n9\n[\n4 \u22121 4 2\n]\n,\nwhich does not obey the non-negativity constraint on X .\nThe feasibility constraints (2),(3), (4) and (5) taken collectively give rise to the following semidefinite relaxation\n(SDR) formulation for MAP estimation:\n(SDR): maximize x,X\nn \u2211\ni=1\n\u3008wi,xi\u3009+ \u2211\n(i,j)\u2208G\n\u3008W ij ,Xij\u3009\nsubject to\n(\n1 x\u22a4\nx X\n)\n0, (6)\nXii = Diag(xi), 1 \u2264 i \u2264 n, (7)\n1 \u22a4xi = 1, 1 \u2264 i \u2264 n, (8)\nXij \u2265 0, (i, j) \u2208 G. (9)"}, {"heading": "2.2. Comparison with Prior Relaxation Heuristics", "text": "2.2.1. Superiority over LP relaxations.\nCareful readers will remark that there might exist other convex constraints on X and x that we can enforce to tighten the proposed semidefinite relaxation. One alternative is the following marginalization constraints, which have been widely invoked in LP relaxation for MAP estimation:\nXij1 = 1, X \u22a4 ij1 = 1, 1 \u2264 i < j \u2264 n. (10)\nSomewhat unexpectedly, these constraints turn out to be redundant, as asserted in the following theorem.\nTheorem 1. Any feasible solution X to SDR (i.e. any X obeying the feasibility constraints of SDR) necessarily satisfies\nXij1 = 1, X \u22a4 ij1 = 1, 1 \u2264 i < j \u2264 n. (11)\nProof. See the supplemental material.\nIntuitively, this property arises from the following features of x and Xii:\nx\u22a4i \u00b7 1 = 1, Xii1 = xi, X \u22a4 ii1 = 1, 1 \u2264 i \u2264 n.\nThese intrinsic properties are then propagated to all offdiagonal blocks by the semidefinite constraint.\n2.2.2. Invariance under variable reparameterization.\nPioneered by the beautiful relaxation proposed for the MAX-CUT problem (Goemans & Williamson, 1995), many SDP approaches developed for combinatorial problems employ the integer indicator y = 12 (1 + x) to parameterize binary variables (e.g., (Torr, 2003; Kumar et al., 2009)). If one applies matrix lifting Y = yy\u22a4 and follows a similar relaxation procedure, the resulting semidefinite relaxation (referred to as SDR2) can be derived as follows\nmaximize y,Y\nn \u2211\ni=1\n\u3008wi,yi\u3009+ 1 2\n\u2211\n(i,j)\u2208G\n\u3008W ij ,Y ij\u3009\nsubject to\n(\n1 y\u22a4 y Y\n)\n0,\n1 \u22a4yi = 2\u2212m, 1 \u2264 i \u2264 n,\nY ij + 1 \u00b7 y \u22a4 j + yi \u00b7 1 \u22a4 + 1 \u00b7 1\u22a4 \u2265 0,\n(i, j) \u2208 G,\n1\u00b71\u22a4+yi\u00b71 \u22a4+1\u00b7y\u22a4 i +Y ii\n2 = Diag(1+ yi),\n1 \u2264 i \u2264 n, (12)\nwhere wi are defined as\nwi = wi + 1\n2\n\n\n\u2211\nj:(i,j)\u2208G\nW ij1+ \u2211\nj:(j,i)\u2208G\nW \u22a4 ji1\n\n .\nIn fact, SDR2 is identical to SDR, as formally stated below.\nTheorem 2. (x\u22c6,X\u22c6) is the solution to SDR if and only if\ny\u22c6 := 2x\u22c6 \u2212 1,\nY \u22c6 := 4X\u22c6 \u2212 2 ( x\u22c6 \u00b7 1\u22a4 + 1 \u00b7 x\u22c6\u22a4 ) + 1 \u00b7 1\u22a4\nis the solution to SDR2.\nProof. See the supplemental material.\nDespite the theoretical equivalence between SDR2 and SDR, from a numerical perspective, solving SDR2 is much harder than solving SDR. The difficulty arises from the complicated form of the linear constraints enforced by SDR2 (i.e., (12)). Note that the advantage of SDR2 is that all diagonal entries of Y are equal to 1 as follows\ndiag(Y ii) = 2(1+yi)\u22121\u2212yi\u2212yi = 1, ( 1 \u2264 i \u2264 n).\nNevertheless, none of prior SDP algorithms takes full advantage of this property in accelerating the algorithm."}, {"heading": "3. Scalable Optimization Algorithm", "text": "The curse of dimensionality poses inevitable numerical challenges when applying general-purpose SDP solvers to solve SDR. Despite their superior accuracy, primal-dual interior point methods (IPM) like SDPT (Toh et al., 1999) are limited to small-scale problems (e.g. nm < 150 on a regular PC). More scalable solvers such as CSDP (Helmberg & Rendl, 2000) and DSDP (Benson & Ye, 2008) propose to solve the dual problem. However, since the non-negativity constraints Xij \u2265 0 produce numerous dual variables, these solvers are still far too restrictive for our program \u2014 none of them can solve SDR on a standard PC when nm exceeds 1000.\nThe limited scalability of interior point methods has inspired a flurry of activity in developing first-order methods, among which the alternating direction method of multipliers (ADMM) (Wen et al., 2010; Boyd et al., 2011) proves well suited for large-scale problems. In this section, we propose an efficient variant of ADMM \u2013 referred to as SDPAD-LR (SDP Alternating Direction method for Low Rank structure), which is tailored to the special structure of SDR (including low rank and sparsity) and enables us to solve problems with very large dimensionality."}, {"heading": "3.1. Alternating Direction Augmented Lagrangian Method (ADMM)", "text": "For convenience of presentation, we denote\nX :=\n(\n1 x\u22a4\nx X\n)\n,\nand rewrite SDR in the operator form:\nminimize \u2329 C,X \u232a\ndual variables\nsubject to A ( X ) = b, y\nP ( X ) \u2265 0, z \u2265 0\nX 0, S 0 (13)\nwhere C encodes all wi and W ij , A(X) = b collects the equality constraints, and P(X) gathers element-wise non-negative constraints. We let variables y, z, and S represent the corresponding dual variables for respective constraints. In the sequel, we will start by reviewing SDPAD, i.e., the original alternating direction method introduced in (Wen et al., 2010), and then present the key modification underlying the proposed efficient variant SDPAD-LR.\n3.1.1. SDPAD: Procedures and Convergence\nSDPAD considers the following augmented Lagrangian:\nL(y,z,S,X) = \u3008b,y\u3009+ \u2329 P\u22c6(z) + S \u2212C \u2212A\u22c6(y),X \u232a\n+ (2\u00b5)\u22121 \u2016P\u22c6(z) + S \u2212C \u2212A\u22c6(y)\u20162F ,\nwhere the penalty parameter \u00b5 controls the strength of the quadratic term. As suggested by (Boyd et al., 2011), we initialize \u00b5 with a small value, and gradually increase it throughout the optimization process.\nLet superscript (k) indicate the variable in the kth iteration. Each iteration of the SDPAD consists of a dual optimization step, followed by a primal update step given as follows\nX (k) = X (k\u22121) + P\u22c6(z(k)) + S(k) \u2212C \u2212A\u22c6(y(k))\n\u00b5 . (14)\nInstead of jointly optimizing all dual variables, the key idea of SDPAD is to decouple the dual optimization step into several sub-problems or, more specifically, to optimize\ny, z,S in order with other variables fixed. This leads to closed-form solutions for each sub-problem as follows\ny (k) = (AA\u2217)\u22121\n(\nA ( S(k\u22121) \u2212C + \u00b5X (k\u22121)) \u2212 \u00b5b ) ,\nz (k) = P\n(\nC \u2212 S(k\u22121) \u2212 \u00b5X (k\u22121)\n)\n+ ,\nS (k) =\n(\nC +A\u22c6(y(k))\u2212 P\u22c6(z(k))\u2212 \u00b5X (k\u22121)\n)\n0 .\nSimilar to that considered in (Wen et al., 2010), our stopping criterion involves measuring of both primal feasibility \u2016A(X (k) )\u2212 b\u2016 and dual feasibility \u00b5(X (k) \u2212X (k\u22121) ).\nConvergence property. In general, convergence properties of SDPAD are known when only equality constraints are present (Wen et al., 2010). However, the inequality constraints of SDR are special in the following two aspects:\n(i) They are element-wise non-negativity constraints;\n(ii) They are essentially decoupled from other linear constraints.\nProperty (ii) arises as all equality constraints are concerned with diagonal blocks of X , while all linear inequality constraints are only enforced on its off-diagonal blocks. Such special structure leads to theoretical convergence guarantees for SDPAD, as stated in the following theorem.\nTheorem 3. The SDPAD method presented above converges to the optimizer of SDR.\nProof. See the supplemental material.\n3.1.2. SDPAD-LR: Accelerated Method\nApparently, the most computationally expensive step of SDPAD is the update of S, which involves the eigendecomposition of an nm \u00d7 nm matrix. This limits the applicability of SDPAD to large-scale problems (e.g. nm > 104). To bypass this numerical bottleneck, we modify SDPAD and present an efficient heuristic called SDPAD-LR, which exploits the low-rank structure of X .\nFirst, we observe that S can be alternatively expressed as\nS (k) = C +A\u22c6(y(k))\u2212 P\u22c6(z(k))\u2212 \u00b5\n(\nX (k) \u2212X (k\u22121)\n)\n.\nThis allows us to present SDPAD without invoking S. The detailed steps of SDPAD can now be summarized as in Algorithm 1.\nIt is straightforward to see that the bottleneck of Algorithm 1 lies in how to compute and store the primary variable X . To derive an efficient solver, we make the assumption that the optimal solution X \u22c6 is low-rank. This is motivated by the empirical evidence that for a variety of problems (see the experimental section for details), SDR is\nAlgorithm 1 SDPAD for solving SDR\ninput: kmax = 1000, \u01eb = 10\u22124, \u00b5min = 10\u22123, \u03c1 = 1.005. initialize: X(0) = X(\u22121) = 0, y(0) = 0, z(0) = 0 repeat\nX (k) temp = 2X (k\u22121) \u2212X (k\u22122) t (k) temp = (AA \u22c6)\u22121(A(X (k) temp)\u2212 b) y(k) = y(k\u22121) + \u00b5t (k) temp z(k) = ( z(k\u22121) \u2212 \u00b5P(X (k) temp) )\n+\nX (k) = ( X (k\u22121) \u2212 C +A\u22c6(y(k))\u2212P\u22c6(z(k))\n\u00b5\n)\n0\n(15) k \u2190 k + 1; \u00b5 = \u00b5\u03c1\nuntil min(\u00b5\u2016X(k) \u2212 X(k\u22121)\u2016F, \u2016A(X (k)\n) \u2212 b\u2016) \u2264 \u01eb or k > kmax\nexact, meaning rank(X \u22c6 ) = 1. Moreover, in the general case, the rank of X \u22c6\nis expected to be much small than its dimension (e.g. (Burer & Monteiro, 2003)), i.e.,\nrank ( X \u22c6 )( rank(X \u22c6 ) + 1 ) \u2264 2M,\nwhere M is the number of constraints.1 of SDPR.\nBased on this assumption, the key idea of SDPAD-LR is to invoke a low-rank matrix Y \u2208 R(nm+1)\u00d7r for some small r and encode X = Y Y \u22a4 throughout the iterative process. This allows us to keep all the variables in memory even for large-scale problems.\nIn this case, (15) is modified as Y (k) = U (k)\u03a3 1 2\n+, where \u03a3 = Diag(\u03c31, \u00b7 \u00b7 \u00b7 , \u03c3r) and U = (u1, \u00b7 \u00b7 \u00b7 ,ur) represent the top r eigenvalues and respective eigenvectors of\nV (k) = Y (k\u22121)Y (k\u22121)\u22a4 \u2212\nC +A\u22c6(y(k))\u2212 P\u22c6(z(k))\n\u00b5 . (16)\nAlthough V (k) is a dense matrix, its top eigenvectors can be efficiently computed using the Lanczos process (Cullum & Willoughby, 2002), whose efficiency is dictated by the complexity of the matrix multiplication operator V (k) : u \u2208 Rnm+1 \u2192 V (k)u \u2208 Rnm+1. As SDR only involves the constrains Xij \u2265 0, (i, j) \u2208 E , the matrix C+A\u22c6(y(k))\u2212P\u22c6(z(k)) turns our to share the same sparsity pattern with G. Thus, the complexity of computing V (k)u is at most O(nmr2 +m2|E|).\nTheoretically, it is extremely challenging to derive an upper bound on r to ensure the exactness of the modified algorithm. To address this issue, we thus design SDPAD-LR so that it iteratively doubles the value of r and reapplies the modified algorithm until it returns the optimal solution. For most of our experiments, we found that r = 8 is sufficient.\n1Practically, many negativity constraints are redundant.\nAlgorithm 2 SDPAD-LR for solving SDR\ninput: kmax = 5000, \u01eb = 10\u22124, \u00b5min = 10\u22123, \u03c1 = 1.005, \u03b4 = 1e\u2212 2, rmax = 32, r = 4. initialize: X(0) = X(\u22121) = 0, y(0) = 0, z(0) = 0 repeat\nX (k) temp = 2X (k\u22121) \u2212X (k\u22122) t (k) temp = (AA \u22c6)\u22121(A(X (k) temp)\u2212 b) y(k) = y(k\u22121) + \u00b5t (k) temp z(k) = ( z(k\u22121) \u2212 \u00b5P(X (k) temp) )\n+\nCompute X(k) according to (16) k \u2190 k + 1; \u00b5 = \u03c1\u00b5 if mod (k, 1000) = 0, \u03bbmin(X (k) ) > \u03b4\u03bbmax(X (k)\n) then\nr = min(rmax, 2r); \u00b5 = \u00b5min end if\nuntil k > kmax or \u03bbmin(X (k) ) \u2264 \u03b4\u03bbmax(X (k)\n) and min(\u00b5\u2016X (k) \u2212X (k\u22121) \u2016F, \u2016A(X (k) )\u2212 b\u2016) \u2264 \u01eb\nThe pseudo-code of SDPAD-LR is summarized in Algorithm 2."}, {"heading": "3.2. Iterative Rounding", "text": "Similar to other ADMM methods (Boyd et al., 2011), SDPAD-LR converges rapidly to moderate accuracy within the first 400 iterations, and significantly slows down afterwards. Thus, rather than continuing until SDPAD-LR converges, it would be more efficient to shrink the problem size by fixing those variables whose optimal states are likely to have been revealed. Specifically, after each round of SDPAD-LR, we fix the optimal state j of a variable xi if xi,j > tmax (tmax = 0.99 for all the examples) or xi,j = max1\u2264i\u2264n,1\u2264j\u2264m xi,j . We then reapply the iterative procedures on the reduced problem. In practice, we find that due to the tightness of SDR, the size of the reduced problems are significantly smaller than the original problem, and one iterative rounding procedure is usually sufficient."}, {"heading": "4. Experimental Results", "text": "In this section, we evaluate SDPAD-LR on several benchmark data sets and compare its performance against existing SDP solvers and state-of-the-art MAP inference algorithms."}, {"heading": "4.1. Benchmark Datasets", "text": "We perform experimental evaluation on MAP estimation problems from three popular benchmark data sets (See Table 2), i.e., OPENGM2 (Kappes et al., 2013a), PIC (PIC, 2011), and a new data set ORIENT for the task of estimating consistent camera orientations (Crandall et al., 2011). OPENGM2 comprises 19 categories of mostly sparse MAP\nproblems. We choose four representative categories for evaluation: Geometric Surface Labeling (GM-Label), Chinese Characters (GM-Char), MRF Photomontage (GMMontage) and Matching (GM-Matching). The first three categories GM-Label, GM-Character and GM-Montage are sparse MAP estimation problems with increasing scales. GM-Matching is a special category where our convex relaxation is not tight. PIC comprises 10 categories of MAP inference problems of various structure. As we already include sparse MAP inference problems from OPENGM2, we pick 3 representative dense categories from PIC: Object Detection(PIC-Object), Image Alignment (PIC-Align) and Folding (PIC-Folding)."}, {"heading": "4.2. SDP Solver Evaluation", "text": "Baseline algorithms. We evaluate the proposed SDPADLR against the following existing large-scale SDP solvers.\n\u2022 SDPAD \u2014 the original ADMM method presented in (Wen et al., 2010).\n\u2022 SDPNAL \u2014 the Newton-CG (conjugate gradient) augmented method proposed in (Zhao et al., 2010).\n\u2022 IPM-NC \u2014 the nonconvex interior point method which attempts to solve a direct relaxation of the MAP inference problem (Burer & Monteiro, 2003):\nminimize \u3008C,xx\u22a4\u3009\nsubject to 1\u22a4xi = 1,xi \u2265 0, 1 \u2264 i \u2264 n\nThis method serves as an alternative low-rank heuristic for the proposed SDPAD-LR. With losing generality, we set the initial values of xi = 1m1, 1 \u2264 i \u2264 n.\n\u2022 MOSEK \u2014 the cutting-edge interior point method. To apply it on large-scale SDRs, we add the nonnegativity constraints in an incremental fashion, i.e., at each iteration, we detect the 100 smallest negative entries and add them to the constraint set.\n\u2022 MUL-Update \u2014 an approximate on-line SDP solver that is based on multivariate weight updates (Arora et al., 2012).\nProblem sets. For evaluation, we consider four categories, on which most baseline algorithms are applicable: PICOBJ, PIC-Align, PIC-Folding and GM2-Label. For simplicity, we pick a representative problem from each category. The dimensions of these problem sets range from 600 to 5000, and they contain both dense and sparse problems (See Table 1).\nEvaluation protocol. Following the standard protocol for assessing convex programs, we evaluate the duality gap and the primal/dual infeasibility of each algorithm:\ngap = |\u3008b,y\u3009 \u2212 \u3008C,X\u3009|\n1 + |\u3008b,y\u3009|+ |\u3008C,X\u3009| ,\ninf = max {\u2016A(X)\u2212 b\u20162 + \u2016min(P(X), 0)\u20162\n1 + \u2016b\u20162 ,\n\u2016C +A\u2217(y)\u2212 P\u2217(z)\u2212 S\u2016F 1 + \u2016C\u2016F }\nAs IPM-NC solves a different optimization problem, we report the gap between its optimal solutions with the groundtruth optimal solutions.\nAnalysis of results. We run each algorithm until the duality gap is below 1e \u2212 4 or the maximum number of iterations is reached. Table 1 shows the running time, duality gap and maximum primal/dual infeasibility of each algorithm on each problem. We can see that SDPAD-LR generates results that are comparable to SDPAD and SDPNAL. However, SDPAD-LR turns out to be remarkably more efficient than SDPAD and SDPNAL on large-scale or sparse datasets. This is due to the fact that SDPAD-LR only requires computing the top eigenvalues, which is both memory and computationally efficient.\nBoth interior point methods (i.e., IPM-NC and MOSEK) have provable guarantees to generate more accurate results than other methods. However, MOSEK is not scalable to large data sets, as reported in Table 1. IPM-NC is scalable to large-scale problems, as the number variables involved is small. However, as IPM-NC solves a non-convex optimization problem, it may easily get trapped into local minimals (e.g., on deer 0034.K10.F100 30markers and folding 2BE6).\nFinally, the multivariate weight update method MULUpdate turns out be inefficient on solving SDRs of MAP in-\nference problems. This is due to the fact that MUL-Update is an approximate solver and it requires a lot of iterations to obtain an accurate solution."}, {"heading": "4.3. MAP Inference Evaluation", "text": "Experimental setup. We compare SDR with the top-performing algorithms from OPENGM2 (Kappes et al., 2013a). These algorithms include (i) BRAOBB (Otten & Dechter, 2012), which is based on combinatorial search, (ii) \u03b1expansion (Szeliski et al., 2008)\u2013a move making method, (iii) MCBC (Kappes et al., 2013b), which is based on a highly optimized max-cut solver, (iv) TRWS-LF2 (Kolmogorov, 2006)\u2013 Tree-reweighted message passing, (vi) ogm-TRBP\u2014 Tree-reweighted belief propagation (Szeliski et al., 2008) and (vii) ficolofo (Cooper et al., 2010)\u2013 the top performing method on dense problems of PIC.\nWe use two measures to assess the performance of each method. The first measure evaluates for each method the mean objective values f of the resulting MAP assignments on each category. For the consistency with (Kappes et al., 2013a), we report \u2212f , meaning that the smaller the value, the better the algorithm. The second measure reports the percentage that each method achieves the best solution among all existing methods (not necessarily the global optimal). The higher the percentage, the better the algorithm.\nPerformance. Table 3 summarizes the performance of SDPAD-LR v.s. state-of-the-art MAP inference algorithms on each type of problems. In each block, the top element (which is tilted) describes \u2212f of each method on each category, and the bottom block describes the percentage of obtaining the best solution. We can see that the overall performance of SDPAD-LR is superior to each other individual algorithm. Except on GM-Matching, SDPAD-LR is the top performing on each other dataset. In contrast, each existing method either does not apply or generates poor results on one or several datasets. This shows the advantage of solving a strong convex relaxation of the MAP inference problem. Below we break down the performance on each benchmark.\n\u2022 ORIENT. SDPAD-LR is the leading method on ORI-\nENT. The problems in ORIENT exhibit specific structures, i.e, the pair-wise potentials consist of approximately shifted permutation matrices. Experimentally, we found that SDR is usually tight on these problems. This explains the superior performance SDPAD-LR. In contrast, linear programming relaxations are not tight on ORIENT, and thus TRBP and TRWS only deliver moderate performance. Moreover, this structural pattern leads to huge search spaces for combinatorial algorithms (e.g., BRAOBB), and they can easily get stuck in local optimums.\n\u2022 Dense problems. SDPAD-LR also outperforms other methods on three dense categories from PIC. It achieves the best mean energy value as well as the highest percentage of obtaining the best solution. This again arises since SDR is tight on these problems.\n\u2022 Sparse problems. SDR yields comparable results with state-of-the-art algorithms on the three sparse categories from OPENGM2. GM-Label consists of problems where the standard LP relaxation is tight. On GM-Char which consists of large-scale binary problems, SDR is comparable to MCBC in the sense that SDR achieves a better mean energy value while MCBC attains a higher percentage of being the best solution. This arises because MCBC is a highly optimized solver designed for binary quadratic problems. On the other hand, SDPAD-LR is only an approximate SDP solver which, in some cases, may not converge to the global optimum due to numerical issues.\n\u2022 GM-Matching. SDR only yields moderate results on GM-Matching. This occurs because SDR is not tight on GM-Matching. In contrast, as GM-Matching is a small-scale problem, combinatorial optimization techniques such as BRAOBB and A-star are capable of finding globally optimal solutions.\nRunning Times. The running time of SDPAD-LR (includ-\ning the rounding procedure) is of the same scale as other convex relation techniques. As shown in Table 2, our preliminary Matlab implementation takes less than 10 mins on small-scale problems (i.e. those in PIC-Object, GMMatching and PIC-Label). On medium size problems, i.e., those in PIC-Folding, PIC-Align, GM-Char and ORIENT, the running time of SDPAD-LR ranges from 20 minutes to 1 hour. On large-scale problems from GM-Montage, SDPAD-LR takes around 8 hours on each problem. However, there is still huge room for improvement. One alternative is to use the eigenvalues computed in the previous iteration to accelerate the eigen-decomposition at the current iteration, which is left for future work."}, {"heading": "5. Conclusions", "text": "In this paper, we have presented a novel semidefinite relaxation for second-order MAP estimation and proposed an efficient ADMM solver. We have extensively compared the proposed SDP solver with various state-of-the-art SDP solvers. Experimental results confirm that our SDP solver is much more scalable than prior approaches when applied to various MAP estimation problem, which enables us to apply SDR on large-scale datasets. Owing to the power of semidefinite relaxation, SDR proves superior to other top-performing MAP inference algorithms on a variety of benchmark datasets.\nThere are plenty of opportunities for future research. First, we would like to extend SDR to higher-order MAP problems. Moreover, it would be interesting to integrate SDR and combinatorial optimization techniques, which has the potential to boost the power of both. From the theoretical side, theoretical support for exact estimation with SDR would be one exciting direction for investigation. This would offer justification of the presented low-rank heuristic. On the other hand, as many combinatorial optimization problems can be formulated as MAP inference problems,\nsuch exact estimation conditions can shed light on the original combinatorial optimization problems."}, {"heading": "Acknowledgments", "text": "This work has been supported in part by NSF grants FODAVA 808515 and CCF 1011228, AFOSR grant FA955012-1-0372, ONR MURI N00014-13-1-0341, and a Google research award."}], "references": [{"title": "The multiplicative weights update method: a meta-algorithm and applications", "author": ["Arora", "Sanjeev", "Hazan", "Elad", "Kale", "Satyen"], "venue": "Theory of Computing,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Tighter relaxations for MAP-MRF inference: A local primal-dual gap based separation", "author": ["D. Batra", "S. Nowozin", "P. Kohli"], "venue": "algorithm. AISTATS\u201911,", "citeRegEx": "Batra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Batra et al\\.", "year": 2011}, {"title": "DSDP5: software for semidefinite programming", "author": ["S. Benson", "Y. Ye"], "venue": "ACM Trans. Math. Softw.,", "citeRegEx": "Benson and Ye,? \\Q2008\\E", "shortCiteRegEx": "Benson and Ye", "year": 2008}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "author": ["Burer", "Samuel", "Monteiro", "Renato D. C"], "venue": "Math. Program.,", "citeRegEx": "Burer et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Burer et al\\.", "year": 2003}, {"title": "A linear programming formulation and approximation algorithms for the metric labeling problem", "author": ["C. Chekuri", "S. Khanna", "J. Naor", "L. Zosin"], "venue": "SIAM J. Discrete Math.,", "citeRegEx": "Chekuri et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chekuri et al\\.", "year": 2004}, {"title": "Soft arc consistency revisited", "author": ["M.C. Cooper", "S. de Givry", "M. Sanchez", "T. Schiex", "M. Zytnicki", "T. Werner"], "venue": "Artif. Intell.,", "citeRegEx": "Cooper et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cooper et al\\.", "year": 2010}, {"title": "SfM with MRFs: discrete-continuous optimization for large-scale structure from motion", "author": ["D. Crandall", "A. Owens", "N. Snavely", "D. Huttenlocher"], "venue": null, "citeRegEx": "Crandall et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Crandall et al\\.", "year": 2011}, {"title": "Lanczos Algorithms for Large Symmetric Eigenvalue Computations", "author": ["J.K. Cullum", "R.A. Willoughby"], "venue": "Number 41. SIAM,", "citeRegEx": "Cullum and Willoughby,? \\Q2002\\E", "shortCiteRegEx": "Cullum and Willoughby", "year": 2002}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M. Goemans", "D. Williamson"], "venue": null, "citeRegEx": "Goemans and Williamson,? \\Q1995\\E", "shortCiteRegEx": "Goemans and Williamson", "year": 1995}, {"title": "A spectral bundle method for semidefinite programming", "author": ["C. Helmberg", "F. Rendl"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Helmberg and Rendl,? \\Q2000\\E", "shortCiteRegEx": "Helmberg and Rendl", "year": 2000}, {"title": "A comparative study of modern inference techniques for discrete energy minimization problems", "author": ["J.H. Kappes", "B. Andres", "F.A. Hamprecht", "C. Schnorr", "S. Nowozin", "D. Batra", "S. Kim", "B.X. Kausler", "J. Lellmann", "N. Komodakis", "C. Rother"], "venue": null, "citeRegEx": "Kappes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kappes et al\\.", "year": 2013}, {"title": "Towards efficient and exact MAP-inference for large scale discrete computer vision problems via combinatorial optimization", "author": ["J.H. Kappes", "M. Speth", "G. Reinelt", "C. Schn\u00f6rr"], "venue": "In CVPR,", "citeRegEx": "Kappes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kappes et al\\.", "year": 2013}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": "IEEE PAMI.,", "citeRegEx": "Kolmogorov,? \\Q2006\\E", "shortCiteRegEx": "Kolmogorov", "year": 2006}, {"title": "Beyond loose LPrelaxations: Optimizing MRFs by repairing cycles", "author": ["N. Komodakis", "N. Paragios"], "venue": "In ECCV", "citeRegEx": "Komodakis and Paragios,? \\Q2008\\E", "shortCiteRegEx": "Komodakis and Paragios", "year": 2008}, {"title": "An analysis of convex relaxations for MAP estimation of discrete MRFs", "author": ["M. Kumar", "V. Kolmogorov", "P. Torr"], "venue": "JMLR, 10:71\u2013106,", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Solving large scale binary quadratic problems: Spectral methods vs. semidefinite programming", "author": ["C. Olsson", "A. Eriksson", "F. Kahl"], "venue": "In CVPR\u201907,", "citeRegEx": "Olsson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Olsson et al\\.", "year": 2007}, {"title": "Anytime and/or depth-first search for combinatorial optimization", "author": ["Otten", "Lars", "Dechter", "Rina"], "venue": "AI Commun.,", "citeRegEx": "Otten et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Otten et al\\.", "year": 2012}, {"title": "Approximate inference by intersecting semidefinite bound and local polytope", "author": ["Peng", "Jian", "Hazan", "Tamir", "Srebro", "Nathan", "Xu", "Jinbo"], "venue": "In AISTATS,", "citeRegEx": "Peng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2012}, {"title": "Message-passing for graph-structured linear programs: Proximal methods and rounding schemes", "author": ["P. Ravikumar", "A. Agarwal", "M.J. Wainwright"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ravikumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2010}, {"title": "Finding MAPs for belief networks is NPhard", "author": ["S.E. Shimony"], "venue": "Artif. Intell.,", "citeRegEx": "Shimony,? \\Q1994\\E", "shortCiteRegEx": "Shimony", "year": 1994}, {"title": "Tightening LP relaxations for MAP using message passing", "author": ["D. Sontag", "T. Meltzer", "A. Globerson", "T.S. Jaakkola", "Y. Weiss"], "venue": "arXiv preprint arXiv:1206.3288,", "citeRegEx": "Sontag et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2012}, {"title": "A comparative study of energy minimization methods for Markov random fields with smoothness-based priors", "author": ["R. Szeliski", "R. Zabih", "D. Scharstein", "O. Veksler", "V. Kolmogorov", "A. Agarwala", "M. Tappen", "C. Rother"], "venue": null, "citeRegEx": "Szeliski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Szeliski et al\\.", "year": 2008}, {"title": "SDPT3\u2013a Matlab software package for semidefinite programming", "author": ["K.C. Toh", "M.J. Todd", "R.H. Tutuncu"], "venue": "Opt. Methods and Software,", "citeRegEx": "Toh et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Toh et al\\.", "year": 1999}, {"title": "Solving Markov random fields using semidefinite programming", "author": ["Torr", "Philip"], "venue": "In AI-STATs\u201903,", "citeRegEx": "Torr and Philip.,? \\Q2003\\E", "shortCiteRegEx": "Torr and Philip.", "year": 2003}, {"title": "MAP estimation via agreement on trees: message-passing and linear programming", "author": ["M. Wainwright", "T. Jaakkola", "A. Willsky"], "venue": "IEEE Trans Info Theory,", "citeRegEx": "Wainwright et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2005}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "A fast semidefinite approach to solving binary quadratic problems", "author": ["P. Wang", "C. Shen", "A. van den Hengel"], "venue": "In CVPR", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Alternating direction augmented Lagrangian methods for semidefinite programming", "author": ["Z. Wen", "D. Goldfarb", "W. Yin"], "venue": "Math. Prog. Comp.,", "citeRegEx": "Wen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2010}, {"title": "A newton-cg augmented lagrangian method for semidefinite programming", "author": ["Zhao", "Xin-Yuan", "Sun", "Defeng", "Toh", "Kim-Chuan"], "venue": "SIAM J. on Optimization,", "citeRegEx": "Zhao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": ", to a more recent application in estimating consistent object orientations from noisy pairwise measurements (Crandall et al., 2011).", "startOffset": 109, "endOffset": 132}, {"referenceID": 20, "context": "For general graphs, this problem is well-known to be NPhard (Shimony, 1994).", "startOffset": 60, "endOffset": 75}, {"referenceID": 15, "context": "(Kumar et al., 2009)) can be subsumed by combing a semidefinite conic constraint with a small set of linear constraints.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Many prior convex relaxation techniques are derived from the original graph structure underlying the MAP estimation problem, among which linear programming relaxation (LPR) methods (Chekuri et al., 2004; Wainwright et al., 2005) are the most popular.", "startOffset": 181, "endOffset": 228}, {"referenceID": 25, "context": "Many prior convex relaxation techniques are derived from the original graph structure underlying the MAP estimation problem, among which linear programming relaxation (LPR) methods (Chekuri et al., 2004; Wainwright et al., 2005) are the most popular.", "startOffset": 181, "endOffset": 228}, {"referenceID": 19, "context": ", quadratic relaxation (QP-RL) (Ravikumar et al., 2010) and second-order cone relaxation (SOCP-MS) (Kumar et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 15, "context": ", 2010) and second-order cone relaxation (SOCP-MS) (Kumar et al., 2009).", "startOffset": 51, "endOffset": 71}, {"referenceID": 15, "context": "In the seminal work of (Kumar et al., 2009), the authors evaluate various convex relaxation approaches, and assert that LPR dominates QP-RL and SOCP-MS.", "startOffset": 23, "endOffset": 43}, {"referenceID": 21, "context": "A recent line of approaches have aimed at obtaining tighter convex relaxations by incrementally adding higher-order interactions to enforce proper marginalization over groups of variables (Sontag et al., 2012; Komodakis & Paragios, 2008; Batra et al., 2011).", "startOffset": 188, "endOffset": 257}, {"referenceID": 1, "context": "A recent line of approaches have aimed at obtaining tighter convex relaxations by incrementally adding higher-order interactions to enforce proper marginalization over groups of variables (Sontag et al., 2012; Komodakis & Paragios, 2008; Batra et al., 2011).", "startOffset": 188, "endOffset": 257}, {"referenceID": 16, "context": "There have been several attempts in applying semidefinite relaxation to obtain MAP assignment (Torr, 2003; Olsson et al., 2007; Wang et al., 2013; Peng et al., 2012).", "startOffset": 94, "endOffset": 165}, {"referenceID": 27, "context": "There have been several attempts in applying semidefinite relaxation to obtain MAP assignment (Torr, 2003; Olsson et al., 2007; Wang et al., 2013; Peng et al., 2012).", "startOffset": 94, "endOffset": 165}, {"referenceID": 18, "context": "There have been several attempts in applying semidefinite relaxation to obtain MAP assignment (Torr, 2003; Olsson et al., 2007; Wang et al., 2013; Peng et al., 2012).", "startOffset": 94, "endOffset": 165}, {"referenceID": 18, "context": "In a recent work, (Peng et al., 2012) considered a general MAP estimation problem, where each variable has multiple states.", "startOffset": 18, "endOffset": 37}, {"referenceID": 18, "context": "The key difference between the proposed formulation and that of (Peng et al., 2012) is that we utilize the semidefinite cone constraint to prune redundant linear marginalization constraints.", "startOffset": 64, "endOffset": 83}, {"referenceID": 15, "context": ", (Kumar et al., 2009; Peng et al., 2012)), we introduce a binary block matrix X := xx \u2208 {0, 1}", "startOffset": 2, "endOffset": 41}, {"referenceID": 18, "context": ", (Kumar et al., 2009; Peng et al., 2012)), we introduce a binary block matrix X := xx \u2208 {0, 1}", "startOffset": 2, "endOffset": 41}, {"referenceID": 15, "context": ", (Torr, 2003; Kumar et al., 2009)).", "startOffset": 2, "endOffset": 34}, {"referenceID": 23, "context": "Despite their superior accuracy, primal-dual interior point methods (IPM) like SDPT (Toh et al., 1999) are limited to small-scale problems (e.", "startOffset": 84, "endOffset": 102}, {"referenceID": 28, "context": "The limited scalability of interior point methods has inspired a flurry of activity in developing first-order methods, among which the alternating direction method of multipliers (ADMM) (Wen et al., 2010; Boyd et al., 2011) proves well suited for large-scale problems.", "startOffset": 186, "endOffset": 223}, {"referenceID": 3, "context": "The limited scalability of interior point methods has inspired a flurry of activity in developing first-order methods, among which the alternating direction method of multipliers (ADMM) (Wen et al., 2010; Boyd et al., 2011) proves well suited for large-scale problems.", "startOffset": 186, "endOffset": 223}, {"referenceID": 28, "context": ", the original alternating direction method introduced in (Wen et al., 2010), and then present the key modification underlying the proposed efficient variant SDPAD-LR.", "startOffset": 58, "endOffset": 76}, {"referenceID": 3, "context": "As suggested by (Boyd et al., 2011), we initialize \u03bc with a small value, and gradually increase it throughout the optimization process.", "startOffset": 16, "endOffset": 35}, {"referenceID": 28, "context": "Similar to that considered in (Wen et al., 2010), our stopping criterion involves measuring of both primal feasibility \u2016A(X (k) )\u2212 b\u2016 and dual feasibility \u03bc(X (k) \u2212X (k\u22121) ).", "startOffset": 30, "endOffset": 48}, {"referenceID": 28, "context": "In general, convergence properties of SDPAD are known when only equality constraints are present (Wen et al., 2010).", "startOffset": 97, "endOffset": 115}, {"referenceID": 3, "context": "Iterative Rounding Similar to other ADMM methods (Boyd et al., 2011), SDPAD-LR converges rapidly to moderate accuracy within the first 400 iterations, and significantly slows down afterwards.", "startOffset": 49, "endOffset": 68}, {"referenceID": 7, "context": ", 2013a), PIC (PIC, 2011), and a new data set ORIENT for the task of estimating consistent camera orientations (Crandall et al., 2011).", "startOffset": 111, "endOffset": 134}, {"referenceID": 28, "context": "\u2022 SDPAD \u2014 the original ADMM method presented in (Wen et al., 2010).", "startOffset": 48, "endOffset": 66}, {"referenceID": 29, "context": "\u2022 SDPNAL \u2014 the Newton-CG (conjugate gradient) augmented method proposed in (Zhao et al., 2010).", "startOffset": 75, "endOffset": 94}, {"referenceID": 0, "context": "\u2022 MUL-Update \u2014 an approximate on-line SDP solver that is based on multivariate weight updates (Arora et al., 2012).", "startOffset": 94, "endOffset": 114}, {"referenceID": 22, "context": "These algorithms include (i) BRAOBB (Otten & Dechter, 2012), which is based on combinatorial search, (ii) \u03b1expansion (Szeliski et al., 2008)\u2013a move making method, (iii) MCBC (Kappes et al.", "startOffset": 117, "endOffset": 140}, {"referenceID": 13, "context": ", 2013b), which is based on a highly optimized max-cut solver, (iv) TRWS-LF2 (Kolmogorov, 2006)\u2013 Tree-reweighted message passing, (vi) ogm-TRBP\u2014 Tree-reweighted belief propagation (Szeliski et al.", "startOffset": 77, "endOffset": 95}, {"referenceID": 22, "context": ", 2013b), which is based on a highly optimized max-cut solver, (iv) TRWS-LF2 (Kolmogorov, 2006)\u2013 Tree-reweighted message passing, (vi) ogm-TRBP\u2014 Tree-reweighted belief propagation (Szeliski et al., 2008) and (vii) ficolofo (Cooper et al.", "startOffset": 180, "endOffset": 203}, {"referenceID": 6, "context": ", 2008) and (vii) ficolofo (Cooper et al., 2010)\u2013 the top performing method on dense problems of PIC.", "startOffset": 27, "endOffset": 48}], "year": 2014, "abstractText": "Maximum a posteriori (MAP) inference over discrete Markov random fields is a fundamental task spanning a wide spectrum of real-world applications, which is known to be NP-hard for general graphs. In this paper, we propose a novel semidefinite relaxation formulation (referred to as SDR) to estimate the MAP assignment. Algorithmically, we develop an accelerated variant of the alternating direction method of multipliers (referred to as SDPAD-LR) that can effectively exploit the special structure of the new relaxation. Encouragingly, the proposed procedure allows solving SDR for large-scale problems, e.g., problems on a grid graph comprising hundreds of thousands of variables with multiple states per node. Compared with prior SDP solvers, SDPAD-LR is capable of attaining comparable accuracy while exhibiting remarkably improved scalability, in contrast to the commonly held belief that semidefinite relaxation can only been applied on small-scale MRF problems. We have evaluated the performance of SDR on various benchmark datasets including OPENGM2 and PIC in terms of boththe quality of the solutions and computation time. Experimental results demonstrate that for a broad class of problems, SDPAD-LR outperforms state-of-the-art algorithms in producing better MAP assignments in an efficient manner.", "creator": "LaTeX with hyperref package"}}}