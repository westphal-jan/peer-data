{"id": "1405.6076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2014", "title": "Online Linear Optimization via Smoothing", "abstract": "we present a new optimization - theoretic approach to analyzing follow - the - leader style algorithms, enabling particularly in constructing the setting where perturbations are used as a tool for regularization. we show that adding a strongly convex penalty \u03b4 function starts to enforce the decision rule and adding those stochastic perturbations to data correspond to global deterministic and stochastic smoothing operations, respectively. we establish an equivalence between \" follow the regularized leader \" and \" follow the perturbed leader \" up to the smoothness properties. this intuition leads to a new generic analysis framework that recovers and improves the previous known regret bounds of the derived class of algorithms commonly known as - follow... the perturbed leader.", "histories": [["v1", "Fri, 23 May 2014 14:33:48 GMT  (42kb)", "http://arxiv.org/abs/1405.6076v1", "COLT 2014"]], "COMMENTS": "COLT 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jacob abernethy", "chansoo lee", "abhinav sinha", "ambuj tewari"], "accepted": false, "id": "1405.6076"}, "pdf": {"name": "1405.6076.pdf", "metadata": {"source": "CRF", "title": "Online Linear Optimization via Smoothing", "authors": ["Jacob Abernethy", "Abhinav Sinha", "Ambuj Tewari", "ABERNETHY LEE", "SINHA TEWARI"], "emails": ["JABERNET@UMICH.EDU", "CHANSOOL@UMICH.EDU", "ABSI@UMICH.EDU", "TEWARIA@UMICH.EDU"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 5.\n60 76\nv1 [\ncs .L\nG ]\n2 3"}, {"heading": "1. Introduction", "text": "In this paper, we study online learning (other names include adversarial learning or no-regret learning) in which the learner iteratively plays actions based on the data received up to the previous iteration. The data sequence is chosen by an adversary and the learner\u2019s goal is to minimize the worst-case regret. The key to developing optimal algorithms is regularization, interpreted as hedging against an adversarial future input and avoiding overfitting to the observed data. In this paper, we focus on regularization techniques for online linear optimization problems where the learner\u2019s action is evaluated on a linear reward function.\nFollow the Regularized Leader (FTRL) is an algorithm that uses explicit regularization via penalty function, which directly changes the optimization objective. At every iteration, FTRL selects an action by optimizing argmaxw f(w,\u0398) \u2212 R(w) where f is the true objective, \u0398 is the observed data, and R is a strongly convex penalty function such as the well-known \u21132-regularizer \u2016 \u00b7 \u20162. The regret analysis of FTRL reduces to the analysis of the second-order behavior of the penalty function (Shalev-Shwartz, 2012), which is well-studied due to the powerful convex analysis tools. In fact, regularization via penalty methods for online learning in general are very well understood. Srebro et al. (2011) proved that Mirror Descent, a regularization via penalty method, achieves a nearly optimal regret guarantee for a general class of online learning problems, and McMahan (2011) showed that FTRL is equivalent to Mirror Descent under some assumptions.\nFollow the Perturbed Leader (FTPL), on the other hand, uses implicit regularization via perturbations. At every iteration, FTPL selects an action by optimizing argmaxw f(w,\u0398 + u) where\n\u0398 is the observed data and u is some random noise vector, often referred to as a \u201cperturbation\u201d of the input. Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014). Convex analysis techniques, which led to our current thorough understanding of FTRL, have not been applied to FTPL, partly because the decision rule of FTPL does not explicitly contain a convex function.\nIn this paper, we present a new analysis framework that makes it possible to analyze FTPL in the same way that FTRL has been analyzed, particularly with regards to second-order properties of convex functions. We show that both FTPL and FTRL naturally arise as smoothing operations of a non-smooth potential function and the regret analysis boils down to controlling the smoothing parameters as defined in Section 3. This new unified analysis framework not only recovers the known optimal regret bounds, but also gives a new type of generic regret bounds.\nPrior to our work, Rakhlin et al. (2012) showed that both FTPL and FTRL naturally arise as admissible relaxations of the minimax value of the game between the learner and adversary. In short, adding a random perturbation and adding a regularization penalty function are both optimal ways to simulate the worst-case future input sequence. We establish a stronger connection between FTRL and FTPL; both algorithms are derived from smoothing operations and they are equivalent up to the smoothing parameters. This equivalence is in fact a very strong result, considering the fact that Harsanyi (1973) showed that there is no general bijection between FTPL and FTRL.\nThis paper also aligns itself with the previous work that studied the connection between explicit regularization via penalty and implicit regularization via perturbations. Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton et al., 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the Fisher information. These results are derived from Taylor approximations, but our FTPL-FTRL connection is derived from the convex conjugate duality.\nAn interesting feature of our analysis framework is that we can directly apply existing techniques from the optimization literature, and conversely, our new findings in online linear optimization may apply to optimization theory. In Section 4.3, a straightforward application of the results on Gaussian smoothing by Nesterov (2011) and Duchi et al. (2012) gives a generic regret bound for an arbitrary online linear optimization problem. In Section 4.1 and 4.2, we improve this bound for the special cases that correspond to canonical online linear optimization problems, and these results may be of interest to the optimization community."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Convex Analysis", "text": "Let f be a differentiable, closed, and proper convex function whose domain is domf \u2286 RN . We say that f is L-Lipschitz with respect to a norm \u2016 \u00b7 \u2016 when f satisfies |f(x)\u2212 f(y)| \u2264 L\u2016x\u2212 y\u2016 for all x, y \u2208 dom(f).\nThe Bregman divergence, denoted Df (y, x), is the gap between f(y) and the linear approximation of f(y) around x. Formally, Df (y, x) = f(y) \u2212 f(x) \u2212 \u3008\u2207f(x), y \u2212 x\u3009. We say that f is \u03b2-strongly convex with respect to a norm \u2016 \u00b7 \u2016 if we have Df (y, x) \u2265 \u03b22 \u2016y \u2212 x\u20162 for all x, y \u2208 domf . Similarly, f is said to be \u03b2-strongly smooth with respect to a norm \u2016 \u00b7 \u2016 if we have\nDf (y, x) \u2264 \u03b22 \u2016y\u2212x\u20162 for all x, y \u2208 domf . The Bregman divergence measures how fast the gradient changes, or equivalently, how large the second derivative is. In fact, we can bound the Bregman divergence by analyzing the local behavior of Hessian, as the following adaptation of Abernethy et al. (2013, Lemma 4.6) shows.\nLemma 1 Let f be a twice-differentiable convex function with domf \u2286 RN . Let x \u2208 domf , such that vT\u22072f(x+\u03b1v)v \u2208 [a, b] (a \u2264 b) for all \u03b1 \u2208 [0, 1]. Then, a\u2016v\u20162/2 \u2264 Df (x+v, x) \u2264 b\u2016v\u20162/2.\nThe Fenchel conjugate of f is f\u22c6(\u03b8) = supw\u2208dom(f){\u3008w, \u03b8\u3009 \u2212 f(w)}, and it is a dual mapping that satisfies f = (f\u22c6)\u22c6 and \u2207f\u22c6 \u2208 dom(f). By the strong convexity-strong smoothness duality, f is \u03b2-strongly smooth with respect to a norm \u2016 \u00b7 \u2016 if and only if f\u22c6 is 1\n\u03b2 -strongly smooth with respect\nto the dual norm \u2016 \u00b7 \u2016\u22c6. For more details and proofs, readers are referred to an excellent survey by Shalev-Shwartz (2012)."}, {"heading": "2.2. Online Linear Optimization", "text": "Let X and Y be convex and closed subsets of RN . The online linear optimization is defined to be the following iterative process:\nOn round t = 1, . . . , T , \u2022 the learner plays wt \u2208 X . \u2022 the adversary reveals \u03b8t \u2208 Y . \u2022 the learner receives a reward1 \u3008wt, \u03b8t\u3009.\nWe say X is the decision set and Y is the reward set. Let \u0398t = \u2211t\ns=1 \u03b8s be the cumulative reward. The learner\u2019s goal is to minimize the (external) regret, defined as:\nRegret = max w\u2208X \u3008w,\u0398T \u3009 \ufe38 \ufe37\ufe37 \ufe38\nbaseline potential\n\u2212 T\u2211\nt=1\n\u3008wt, \u03b8t\u3009. (1)\nThe baseline potential function \u03a6(\u0398) := maxw\u2208X \u3008w,\u0398\u3009 is the comparator term against which we define the regret, and it coincides with the support function of X . For a bounded compact set X , the support function of X is sublinear2 and Lipschitz continuous with respect to any norm \u2016 \u00b7 \u2016 with the Lipschitz constant supx\u2208X \u2016x\u2016. For more details and proofs, readers are referred to Rockafellar (1997, Section 13) or Molchanov (2005, Appendix F)."}, {"heading": "3. Online Linear Optimization Algorithms via Smoothing", "text": ""}, {"heading": "3.1. Gradient-Based Prediction Algorithm", "text": "Follow-the-Leader style algorithms solve an optimization objective every round and play an action of the form wt = argmaxw\u2208X f(w,\u0398t\u22121) given a fixed \u0398t\u22121. For example, Follow the Regularized Leader maximizes f(w,\u0398) = \u3008w,\u0398\u3009 \u2212 R(w) where R is a strongly convex regularizer, and Follow the Perturbed Leader maximizes f = \u3008w,\u0398 + u\u3009 where u is a random noise. A surprising\n1. Our somewhat less conventional choice of maximizing the reward instead of minimizing the loss was made so that we directly analyze the convex function max(\u00b7) without cumbersome sign changes. 2. A function f is sublinear if it is positive homogeneous (i.e., f(ax) = af(x) for all a > 0) and subadditive (i.e., f(x) + f(y) \u2265 f(x+ y)).\nfact about these algorithms is that there are many scenarios in which the action wt is exactly the gradient of some scalar potential function \u03a6t evaluated at \u0398t\u22121. This perspective gives rise to what we call the Gradient-based Prediction Algorithm (GBPA), presented below. Note that Cesa-Bianchi and Lugosi (2006, Theorem 11.6) presented a similar algorithm, but our formulation eliminates all dual mappings.\nAlgorithm 1: Gradient-Based Prediction Algorithm (GBPA)\nInput: X ,Y \u2286 RN Initialize \u03980 = 0 for t = 1 to T do\nThe learner chooses differentiable \u03a6t : RN \u2192 R whose gradient satisfies Image(\u2207\u03a6t) \u2286 X The learner plays wt = \u2207\u03a6t(\u0398t\u22121) The adversary reveals \u03b8t \u2208 Y and the learner gets a reward of \u3008wt, \u03b8t\u3009 Update \u0398t = \u0398t\u22121 + \u03b8t\nend\nLemma 2 (GBPA Regret) Let \u03a6 be the baseline potential function for an online linear optimization problem. The regret of the GBPA can be written as:\nRegret = \u03a6(\u0398T )\u2212 \u03a6T (\u0398T ) \ufe38 \ufe37\ufe37 \ufe38\nunderestimation penalty\n+ T\u2211\nt=1\n(\n(\u03a6t(\u0398t\u22121)\u2212 \u03a6t\u22121(\u0398t\u22121)) \ufe38 \ufe37\ufe37 \ufe38\noverestimation penalty\n+D\u03a6t(\u0398t,\u0398t\u22121) \ufe38 \ufe37\ufe37 \ufe38\ndivergence penalty\n)\n, (2)\nwhere \u03a60 \u2261 \u03a6.\nProof See Appendix A.1.\nIn the existing FTPL analysis, the counterpart of the divergence penalty is \u3008wt+1\u2212wt, \u03b8t\u3009, which is controlled by analyzing the probability that the noise would cause the two random variables wt+1 and wt to differ. In our framework, wt is the gradient of a function \u03a6t of \u0398, which means that if \u03a6t is twice-differentiable, we can take the derivative of wt with respect to \u0398. This derivative is the Hessian matrix of \u03a6t, which essentially controls \u3008wt \u2212 wt\u22121\u3009 with the help of Lemma 1. Since we focus on the curvature property of functions as opposed to random vectors, our FTPL analysis involves less probabilistic analysis than Devroye et al. (2013) or van Erven et al. (2014) does.\nWe point out a couple of important facts about Lemma 2: (a) If \u03a61 \u2261 \u00b7 \u00b7 \u00b7 \u2261 \u03a6T , then the overestimation penalty sums up to \u03a61(0)\u2212\u03a6(0) = \u03a6T (0)\u2212\u03a6(0). (b) If \u03a6t is \u03b2-strongly smooth with respect to \u2016 \u00b7 \u2016, the divergence penalty at t is at most \u03b22 \u2016\u03b8t\u20162."}, {"heading": "3.2. Smoothability of the Baseline Potential", "text": "Equation 2 shows that the regret of the GBPA can be broken into two parts. One source of regret is the Bregman divergence of \u03a6t; since \u03b8t is not known until playing wt, the GBPA always ascends along the gradient that is one step behind. The adversary can exploit this and play \u03b8t to induce a large gap between \u03a6t(xt) and the linear approximation of \u03a6t(\u0398t) around \u0398t\u22121. Of course, the learner can reduce this gap by choosing a smooth \u03a6t whose gradient changes slowly. The learner,\nhowever, cannot achieve low regret by choosing an arbitrarily smooth \u03a6t, because the other source of regret is the difference between \u03a6t and \u03a6. In short, the GBPA achieves low regret if the potential function \u03a6t gives a favorable tradeoff between the two sources of regret. This tradeoff is captured by the following definition of smoothability.\nDefinition 3 (Beck and Teboulle, 2012, Definition 2.1) Let \u03a6 be a closed proper convex function. A collection of functions {\u03a6\u0302\u03b7 : \u03b7 \u2208 R} is said to be an \u03b7-smoothing of a smoothable function \u03a6 with smoothing parameters (\u03b1, \u03b2, \u2016 \u00b7 \u2016), if for every \u03b7 > 0\n(i) There exists \u03b11 (underestimation bound) and \u03b12 (overestimation bound) such that\nsup \u0398\u2208dom(\u03a6) \u03a6(\u0398)\u2212 \u03a6\u0302\u03b7(\u0398) \u2264 \u03b11\u03b7 and sup \u0398\u2208dom(\u03a6) \u03a6\u0302\u03b7(\u0398)\u2212 \u03a6(\u0398) \u2264 \u03b12\u03b7\nwith \u03b11 + \u03b12 = \u03b1. (ii) \u03a6\u0302\u03b7 is \u03b2 \u03b7\n-strongly smooth with respect to \u2016 \u00b7 \u2016. We say \u03b1 is the deviation parameter, and \u03b2 is the smoothness parameter.\nA straightforward application of Lemma 2 gives the following statement:\nCorollary 4 Let \u03a6 be the baseline potential for an online linear optimization problem. Suppose {\u03a6\u0302\u03b7} is an \u03b7-smoothing of \u03a6 with parameters (\u03b1, \u03b2, \u2016 \u00b7 \u2016). Then, the GBPA run with \u03a61 \u2261 \u00b7 \u00b7 \u00b7 \u2261 \u03a6T \u2261 \u03a6\u0302\u03b7 has regret at most\nRegret \u2264 \u03b1\u03b7 + \u03b2 2\u03b7\nT\u2211\nt=1\n\u2016\u03b8t\u20162\nIn online linear optimization, we often consider the settings where the marginal reward vectors \u03b81, . . . , \u03b8t are constrained by a norm, i.e., \u2016\u03b8t\u2016 \u2264 r for all t. In such settings, the regret grows in O( \u221a r\u03b1\u03b2T ) for the optimal choice of \u03b1. The product \u03b1\u03b2, therefore, is at the core of the GBPA regret analysis."}, {"heading": "3.3. Algorithms", "text": "Follow the Leader (FTL) Consider the GBPA run with a fixed potential function \u03a6t \u2261 \u03a6 for t = 1, . . . , T , i.e., the learner chooses the baseline potential function every iteration. At iteration t, this algorithm plays \u2207\u03a6t(\u0398t\u22121) = argmaxw\u3008w,\u0398t\u22121\u3009, which is equivalent to FTL (Cesa-Bianchi and Lugosi, 2006, Section 3.2). FTL suffers zero regret from the over- or underestimation penalty, but the divergence penalty grows linearly in T in the worst case, resulting in an \u2126(T ) regret.\nFollow the Regularized Leader (FTRL) Consider the GBPA run with a regularized potential:\n\u2200t,\u03a6t(\u0398) = R\u22c6(\u0398) = max w\u2208X {\u3008w,\u0398\u3009 \u2212 R(w)} (3)\nwhere R : X \u2192 R is a \u03b2-strongly convex function. At time t, this algorithm plays \u2207\u03a6t(\u0398t\u22121) = argmaxw{\u3008w,\u0398t\u22121\u3009\u2212R(w)}, which is equivalent to FTRL. By the strong convexity-strong smoothness duality, \u03a6t is 1\u03b2 -strongly smooth with respect to the dual norm \u2016 \u00b7 \u2016\u22c6. In Section 5, we give an alternative interpretation of FTRL as a deterministic smoothing technique called inf-conv smoothing.\nFollow the Perturbed Leader (FTPL) Consider the GBPA run with a stochastically smoothed potential:\n\u2200t,\u03a6t(\u0398) = \u03a6\u0303(\u0398; \u03b7,D) def= Eu\u223cD[\u03a6(\u0398 + \u03b7u)] = Eu\u223cD [\nmax w\u2208X\n{\u3008w,\u0398 + \u03b7u\u3009} ]\n(4)\nwhere D is a smoothing distribution with the support RN and \u03b7 > 0 is a scaling parameter. This technique of stochastic smoothing has been well-studied in the optimization literature for gradientfree optimization algorithms (Glasserman, 1991; Yousean et al., 2010) and accelerated gradient methods for non-smooth optimizations (Duchi et al., 2012). If the max expression inside the expectation has a unique maximizer with probability one, we can swap the expectation and gradient (Bertsekas, 1973, Proposition 2.2) to obtain\n\u2207\u03a6t(\u0398t\u22121) = Eu\u223cD [\nargmax w\u2208X\n{\u3008w,\u0398t\u22121 + \u03b7u\u3009} ] . (5)\nEach argmax expression is equivalent to the decision rule of FTPL (Hannan, 1957; Kalai and Vempala, 2005); the GBPA on a stochastically smoothed potential can thus be seen as playing the expected action of FTPL. Since the learner gets a linear reward in online linear optimization, the regret of the GBPA on a stochastically smoothed potential is equal to the expected regret of FTPL.\nFTPL-FTRL Duality Our potential-based formulation of FTRL and FTPL reveals that a strongly convex regularizer defines a smooth potential function via duality, while adding perturbations is a direct smoothing operation on the baseline potential function. By the strong convexity-strong smoothness duality, if the stochastically smoothed potential function is (1/\u03b2)-strongly smooth with respect to \u2016 \u00b7 \u2016\u22c6, then its Fenchel conjugate implicitly defines a regularizer that is \u03b2-strongly convex with respect to \u2016 \u00b7 \u2016.\nThis connection via duality is a bijection in the special case where the decision set is onedimensional. Previously it had been observed3 that the Hedge Algorithm (Freund and Schapire, 1997), which can be cast as FTRL with an entropic regularization R(w) = \u2211i wi logwi, is equivalent to FTPL with Gumbel-distributed noise. Hofbauer and Sandholm (2002, Section 2) gave a generalization of this fact to a much larger class of perturbations, although they focused on repeated game playing where the learner\u2019s decision set X is the probability simplex. The inverse mapping from FTPL to FTRL, however, does not appear to have been previously published.\nTheorem 5 Consider the one-dimensional online linear optimization problem with X = Y = [0, 1]. Let R : X \u2192 R be a strongly convex regularizer. Its Fenchel conjugate R\u22c6 defines a valid CDF of a continuous distribution D such that Equation 3 and Equation 4 are equal. Conversely, let FD be a CDF of a continuous distribution D with a finite expectation. If we define R to be such that R(w)\u2212R(0) = \u2212 \u222b w\n0 F \u22121 D (1\u2212 z)dz, then Equation 3 and Equation 4 are equal.\nProof In Appendix B.1.\n3. Adam Kalai first described this result in personal communication and Warmuth (2009) expanded it into a short note available online. However, the result appears to be folklore in the area of probabilistic choice models, and it is mentioned briefly in Hofbauer and Sandholm (2002)."}, {"heading": "4. Online Linear Optimization via Gaussian Smoothing", "text": "Gaussian smoothing is a standard technique for smoothing a function. In computer vision applications, for example, image pixels are viewed as a function of the (x, y)-coordinates, and Gaussian smoothing is used to blur noises in the image. We first present basic results on Gaussian smoothing from the optimization literature.\nDefinition 6 (Gaussian smoothing) Let \u03a6 : RN \u2192 R be a function. Then, we define its Gaussian smoothing, with a scaling parameter \u03b7 > 0 and a covariance matrix \u03a3, as\n\u03a6\u0303(\u0398; \u03b7,N (0,\u03a3)) = Eu\u223cN (0,\u03a3)\u03a6(\u0398 + \u03b7u) = (2\u03c0)\u2212 N 2 det(\u03a3)\u2212 1 2\n\u222b\nRN\n\u03a6(\u0398 + \u03b7u)e\u2212 1 2 uT\u03a3\u22121u du\nIn this section, when the smoothing parameters are clear from the context, we use a shorthand notation \u03a6\u0303. An extremely useful property of Gaussian smoothing is that \u03a6\u0303 is always twice-differentiable, even when \u03a6 is not. The trick is to introduce a new variable \u0398\u0303 = \u0398 + \u03b7u. After substitutions, the variable \u0398 only appears in the exponent, which can be safely differentiated.\nLemma 7 (Nesterov 2011, Lemma 2, and Bhatnagar 2007, Section 3) Let \u03a6 : RN \u2192 R be a function. For any positive \u03b7, \u03a6\u0303(\u00b7 ; \u03b7,N (0,\u03a3)) is twice-differentiable and\n\u2207\u03a6\u0303(\u0398; \u03b7,N (0,\u03a3)) = 1 \u03b7 Eu[\u03a6(\u0398 + \u03b7u)\u03a3 \u22121u] (6)\n\u22072\u03a6\u0303(\u0398; \u03b7,N (0,\u03a3)) = 1 \u03b72 Eu\n[ \u03a6(\u0398 + \u03b7u) ( (\u03a3\u22121u)(\u03a3\u22121u)T \u2212 \u03a3\u22121 )]\n(7)\nIf \u03a6(\u0398+ \u03b7u) is differentiable almost everywhere, then we can directly differentiate Equation 6 by swapping the expectation and gradient (Bertsekas, 1973, Proposition 2.2) and obtain an alternative expression for Hessian:\n\u22072\u03a6\u0303(\u0398; \u03b7,N (0,\u03a3)) = 1 \u03b7 Eu[\u2207\u03a6 ( \u0398+ \u03b7u)(\u03a3\u22121u)T ]. (8)"}, {"heading": "4.1. Experts Setting (\u21131-\u2113\u221e case)", "text": "The experts setting is where X = \u2206N def= {w \u2208 RN : \u2211iwi = 1, wi \u2265 0 \u2200i}, and Y = {\u03b8 \u2208 R N : \u2016\u03b8\u2016\u221e \u2264 1}. The baseline potential function is \u03a6(\u0398) = maxw\u2208X \u3008w,\u0398\u3009 = \u0398i\u2217(\u0398), where we define i\u2217(z) := min{i : i \u2208 argmaxj zj}. Our regret bound in Theorem 8 is data-dependent, and it is stronger than the previously known O( \u221a T logN) regret bounds of the algorithms that use similar perturbations. In the game theoretic analysis of Gaussian perturbations by Rakhlin et al. (2012), the algorithm uses the scaling parameter \u03b7t = \u221a T \u2212 t, which requires the knowledge of T and does not adapt to data. Devroye et al. (2013) proposed the Prediction by Random Walk (PRW) algorithm, which flips a fair coin every round and decides whether to add 1 to each coordinate. Due to the discrete nature of the algorithm, the analysis must assume the worst case where \u2016\u03b8t\u2016\u22c6 = 1 for all t.\nTheorem 8 Let \u03a6 be the baseline potential for the experts setting. The GBPA run with the Gaussian smoothing of \u03a6, i.e., \u03a6t(\u00b7) = \u03a6\u0303(\u00b7; \u03b7t,N (0, I)) for all t has regret at most\nRegret \u2264 \u221a 2 logN ( \u03b7T + \u2211T\nt=1 1 \u03b7t \u2016\u03b8t\u20162\u221e\n)\n. (9)\nIf the algorithm selects \u03b7t = \u221a \u2211T t=1 \u2016\u03b8t\u20162\u221e for all t (with the help of hindsight), we have\nRegret \u2264 2 \u221a 2 \u2211T\nt=1 \u2016\u03b8t\u20162\u221e logN.\nIf the algorithm selects \u03b7t adaptively according to \u03b7t = \u221a 2(1 + \u2211t\u22121 s=1 \u2016\u03b8s\u20162\u221e), we have\nRegret \u2264 4 \u221a (1 + \u2211T\nt=1 \u2016\u03b8t\u20162\u221e) logN.\nProof In order to apply Lemma 2, we need to upper bound (i) the overestimation and underestimation penalty, and (ii) the Bregman divergence. To bound (i), first note that due to convexity of \u03a6, the smoothed potential \u03a6\u0303 is also convex and upper bounds the baseline potential. Hence, the underestimation penalty is at most 0, and when \u03b7t is fixed for all t, it is straightforward to bound the overestimation penalty:\n\u03a6T (0)\u2212 \u03a6(0) \u2264 Eu\u223cN (0,I)[\u03a6(\u03b7Tu)] \u2264 \u03b7T \u221a 2 logN. (10)\nThe first inequality is the triangle inequality. The second inequality is a well-known result and we included the proof in Appendix C.1 for completeness. For the adaptive \u03b7t, we apply Lemma 10, which we prove at the end of this section, to get the same bound.\nIt now remains to bound the Bregman divergence. This is achieved in Lemma 9 where we upper bound \u2211\ni,j |\u22072ij\u03a6|, which is an upper bound on max\u03b8:\u2016\u03b8\u2016\u221e=1 \u03b8T (\u22072\u03a6)\u03b8. The final step is to apply Lemma 1.\nThe proof of Theorem 8 shows that for the experts setting, the Gaussian smoothing is an \u03b7smoothing with parameters (O( \u221a logN), O( \u221a logN), \u2016 \u00b7 \u2016). This is in contrast to the Hedge Algorithm (Freund and Schapire, 1997), which is an \u03b7-smoothing with parameters (logN, 1, \u2016 \u00b7 \u2016) (See Section 5 for details). Interestingly, the two algorithms obtain the same optimal regret (up to constant factors) although they have different smoothing parameters.\nLemma 9 Let \u03a6 be the baseline potential for the experts setting. Let the Hessian matrix of the Gaussian-smoothed baseline potential be denoted H , i.e., H = \u22072\u03a6\u0303(\u0398; \u03b7,N (0, I)). Then,\n\u2211\ni,j\n|Hij| \u2264 2 \u221a 2 logN\n\u03b7 .\nProof With probability one, \u03a6(\u0398 + \u03b7u) is differentiable and from Lemma 7, we can write\nH = 1\n\u03b7 E[\u2207\u03a6(\u0398+ \u03b7u)uT ] = E[ei\u2217(\u03b7u+\u0398)uT ],\nwhere ei \u2208 RN is the i-th standard basis vector.\nFirst, we note that all off-diagonals of H are negative and all diagonal entries in H are positive. This is because the Hessian matrix is the covariance matrix between the probability that i-th coordinate is the maximum and the extra random Gaussian noise added to the j-th coordinate; for any positive number \u03b1, uj = \u03b1 and uj = \u2212\u03b1 have the same probability, but the indicator for i = i\u2217 has a higher probability to be 1 when ui is positive (hence Hii > 0) and uj is negative for i 6= j (hence Hij < 0 for i 6= j).\nSecond, the entries of H sum up to 0, as\n\u2211\ni,j\nHij = 1\n\u03b7 E\n[ \u2211\nj uj \u2211 i 1{i = i\u2217(\u0398 + u)} ] = 1\n\u03b7 E\n[ \u2211\nj uj\n]\n= 0.\nCombining the two observations, we have\n\u2211\ni,j\n|Hij| = \u2211\ni,j:Hij>0\nHij + \u2211\ni,j:Hij<0\n\u2212Hij = 2 \u2211\ni,j:Hij>0\nHij = 2Tr(H)\n. Finally, the trace is bounded as follows:\nTr(H) = 1\n\u03b7 E\n[\u2211\ni\nui1{i = i\u2217(\u0398 + u)} ] \u2264 1 \u03b7 E [ (max k uk) \u2211\ni\n1{i = i\u2217(\u0398 + u)} ]\n= 1\n\u03b7 E[max k uk] \u2264\n1\n\u03b7\n\u221a\n2 logN,\nwhere the final inequality is shown in Appendix C.1. Multiplying both sides by 2 completes the proof.\nTime-Varying Scaling Parameters When the scaling parameter \u03b7t changes every iteration, the overestimation penalty becomes a sum of T terms. The following lemma shows that using the sublinearity of the baseline potential, we can collapse them into one.\nLemma 10 Let \u03a6 : RN \u2192 R be a sublinear function, and D be a continuous distribution with the support RN . Let \u03a6t(\u0398) = \u03a6\u0303(\u0398; \u03b7t,D) for t = 0, . . . , T and choose \u03b7t to be a non-decreasing sequence of non-negative numbers (\u03b70 = 0 so that \u03a60 = \u03a6). Then, the overestimation penalty in Equation 2 has the following upper bound:\nT\u2211\nt=1\n\u03a6t(\u0398t\u22121)\u2212 \u03a6t\u22121(\u0398t\u22121) \u2264 \u03b7TEu\u223cD[\u03a6(u)].\nProof See Appendix C.2"}, {"heading": "4.2. Online Linear Optimization over Euclidean Balls (\u21132-\u21132 case)", "text": "The Euclidean balls setting is where X = Y = {x \u2208 RN : \u2016x\u20162 \u2264 1}. The baseline potential function is \u03a6(\u0398) = maxw\u2208X \u3008w,\u0398\u3009 = \u2016\u0398\u20162. We show that the GBPA with Gaussian smoothing achieves a minimax optimal regret (Abernethy et al., 2008) up to a constant factor.\nTheorem 11 Let \u03a6 be the baseline potential for the Euclidean balls setting. The GBPA run with \u03a6t(\u00b7) = \u03a6\u0303(\u00b7; \u03b7,N (0, I)) for all t has regret at most\nRegret \u2264 \u03b7T \u221a N + 1\n2 \u221a N \u2211T t=1 1 \u03b7t \u2016\u03b8t\u201622. (11)\nIf the algorithm selects \u03b7t = \u221a \u2211T s=1 \u2016\u03b8s\u201622/(2N) for all t (with the help of hindsight), we have\nRegret \u2264 \u221a 2 \u2211T\nt=1 \u2016\u03b8t\u201622.\nIf the algorithm selects \u03b7t adaptively according to \u03b7t = \u221a (1 + \u2211t\u22121 s=1 \u2016\u03b8s\u201622))/N , we have\nRegret \u2264 2 \u221a 1 + \u2211T\nt=1 \u2016\u03b8t\u201622\nProof The proof is mostly similar to that of Theorem 8. In order to apply Lemma 2, we need to upper bound (i) the overestimation and underestimation penalty, and (ii) the Bregman divergence.\nThe Gaussian smoothing always overestimates a convex function, so it suffices to bound the overestimation penalty. Furthermore, it suffices to consider the fixed \u03b7t case due to Lemma 1. The overestimation penalty can be upper-bounded as follows:\n\u03a6T (0)\u2212 \u03a6(0) = Eu\u223cN (0,I)\u2016\u0398+ \u03b7Tu\u20162 \u2212 \u2016\u0398\u20162 \u2264 \u03b7TEu\u223cN (0,I)\u2016u\u20162 \u2264 \u03b7T \u221a Eu\u223cN (0,I)\u2016u\u201622 = \u03b7T \u221a N\nThe first inequality is from the triangle inequality, and the second inequality is from the concavity of the square root.\nFor the divergence penalty, note that the upper bound on maxv:\u2016\u03b8\u20162=1 \u03b8 T (\u22072\u03a6\u0303)\u03b8 is exactly the maximum eigenvalue of the Hessian, which we bound in Lemma 12. The final step is to apply Lemma 1.\nLemma 12 Let \u03a6 be the baseline potential for the Euclidean balls setting. Then, for all \u0398 \u2208 RN and \u03b7 > 0, the Hessian matrix of the Gaussian smoothed potential satisfies\n\u22072\u03a6\u0303(\u0398; \u03b7,N (0, I)) 1 \u03b7 \u221a N I.\nProof The Hessian of the Euclidean norm \u22072\u03a6(\u0398) = \u2016\u0398\u2016\u221212 I\u2212\u2016\u0398\u2016\u221232 \u0398\u0398T diverges near \u0398 = 0. Expectedly, the maximum curvature is at origin even after Gaussian smoothing (See Appendix C.3). So, it suffices to prove\n\u22072\u03a6(0) = Eu\u223cN (0,I)[\u2016u\u20162(uuT \u2212 I)] \u221a 1 N I,\nwhere the Hessian expression is from Equation 8. By symmetry, all off-diagonal elements of the Hessian are 0. Let Y = \u2016u\u20162, which is Chisquared with N degrees of freedom. So,\nTr(E[\u2016u\u20162(uuT \u2212 I)]) = E[Tr(\u2016u\u20162(uuT \u2212 I))] = E[\u2016u\u201632 \u2212N\u2016u\u20162] = E[Y 3 2 ]\u2212NE[Y 12 ]\nUsing the Chi-Squared moment formula (Harvey, 1965, p. 20), the above becomes:\n2 3 2\u0393(32 + N 2 )\n\u0393(N2 ) \u2212 N2\n1 2\u0393(12 + N 2 )\n\u0393(N2 ) =\n\u221a 2\u0393(12 + N 2 )\n\u0393(N2 ) . (12)\nFrom the log-convexity of the Gamma function,\nlog \u0393 ( 1 2 + N 2 ) \u2264 12 ( log \u0393 ( N 2 ) + log \u0393 ( N 2 + 1 )) = log \u0393 ( N 2\n) \u221a\nN 2 .\nExponentiating both sides, we obtain\n\u0393 ( 1 2 + N 2 ) \u2264 \u0393 ( N 2\n) \u221a\nN 2 ,\nwhich we apply to Equation 12 and get Tr(\u22072\u03a6(0)) \u2264 \u221a N . To complete the proof, note that by symmetry, each entry must have the same expected value, and hence it is bounded by \u221a 1/N ."}, {"heading": "4.3. General Bound", "text": "In this section, we will use a generic property of Gaussian smoothing to derive a regret bound that holds for any arbitrary online linear optimization problem.\nLemma 13 (Duchi et al., 2012, Lemma E.2) Let \u03a6 be a real-valued convex function on a closed domain which is a subset of RN . Suppose \u03a6 is L-Lipschitz with respect to \u2016 \u00b7 \u20162, and let \u03a6\u0302\u03b7 be the Gaussian smoothing of \u03a6 with the scaling parameter \u03b7 and identity covariance. Then, {\u03a6\u0302\u03b7} is an \u03b7-smoothing of \u03a6 with parameters (L \u221a N,L, \u2016 \u00b7 \u20162).\nConsider an instance of online linear optimization with decision set X and reward set Y . The baseline potential function \u03a6 is \u2016X\u20162-Lipschitz with respect to \u2016 \u00b7\u20162, where \u2016X\u20162 = supx\u2208X \u2016x\u20162. From Lemma 13 and Corollary 4, it follows that\nRegret \u2264 \u03b7 \u221a N\u2016X\u20162 + \u2016X\u20162 2 T\u2211\nt=1\n\u2016\u03b8t\u201622,\nwhich is O(N 1 4 \u2016X\u20162\u2016Y\u20162 \u221a T ) after tuning \u03b7. This regret bound, however, often gives a suboptimal dependence on the dimension N . For example, it gives O(N 3 4T 1 2 ) regret bound for the experts setting where \u2016X\u20162 = 1 and \u2016Y\u20162 = \u221a N , and O(N 1 4T 1\n2 ) regret bound for the Euclidean balls setting where \u2016X\u20162 = \u2016Y\u20162 = 1."}, {"heading": "4.4. Online Convex Optimization", "text": "In online convex optimization, the learner receives a sequence of convex functions ft whose domain is X and its subgradients are in the set Y (Zinkevich, 2003). After the learner plays wt \u2208 X , the reward function ft is revealed. The learner gains ft(wt) and observes \u2207ft(wt), a subgradient of ft at wt.\nA simple linearization argument shows that our regret bounds for online linear optimization generalize to online convex optimization. Let w\u2217 be the optimal fixed point in hindsight. The\ntrue regret is upper bounded by the linearized regret, as ft(w\u2217) \u2212 ft(wt) \u2264 \u3008w\u2217 \u2212 wt,\u2207ft(wt)\u3009 for any subgradient \u2207ft(\u00b7), and our analysis bounds the linearized regret. Unlike in the online linear optimization settings, however, the regret bound is valid only for the GBPA with smoothed potentials, which plays the expected action of FTPL."}, {"heading": "5. Online Linear Optimization via Inf-conv Smoothing", "text": "Beck and Teboulle (2012) proposed inf-conv smoothing, which is an infimal convolution with a strongly smooth function. In this section, we will show that FTRL is equivalent to the GBPA run with the inf-conv smoothing of the baseline potential function.\nLet (X , \u2016 \u00b7 \u2016) be a normed vector space, and (X \u22c6, \u2016 \u00b7 \u2016\u22c6) be its dual. Let \u03a6 : X \u22c6 \u2192 R be a closed proper convex function, and let S be a \u03b2-strongly smooth function on X \u22c6 with respect to \u2016 \u00b7 \u2016\u22c6. Then, the inf-conv smoothing of \u03a6 with S is defined as:\n\u03a6ic(\u0398; \u03b7,S) def= inf \u0398\u2217\u2208X \u22c6\n{ \u03a6(\u0398\u2217) + \u03b7S ( \u0398\u2212\u0398\u2217\n\u03b7\n)}\n= max w\u2208X\n{ \u3008w,\u0398\u3009 \u2212 \u03a6\u22c6(w)\u2212 \u03b7S\u22c6(w) } . (13)\nThe first expression with infimum is precisely the infimal convolution of \u03a6(\u00b7) and \u03b7S( \u00b7 \u03b7 ), and the second expression with supremum is an equivalent dual formulation. The inf-conv smoothing \u03a6ic(\u0398; \u03b7,S) is finite, and it is an \u03b7-smoothing of \u03a6 (Definition 3) with smoothing parameters\n(\nmax \u0398\u2208X \u22c6 max w\u2208\u2202\u03a6(\u0398)\nS\u22c6(w), \u03b2, \u2016 \u00b7 \u2016 ) . (14)\nwhere \u2202\u03a6(\u0398) is a set of subgradients of \u03a6 at \u0398.\nConnection to FTRL Consider an online linear optimization problem with decision set X \u2286 RN . Then, the dual space X \u22c6 is simply RN . Let R be a \u03b2-strongly convex function on X with respect to a norm \u2016\u00b7\u2016. By the strong convexity-strong smoothness duality, R\u22c6 is 1\n\u03b2 -strongly smooth. Consider\nthe inf-conv smoothing of the baseline potential function \u03a6 with R\u22c6, denoted \u03a6ic(\u0398; \u03b7,R\u22c6). We will that the GBPA run with \u03a6ic(\u0398; \u03b7,R\u22c6) is equivalent to FTRL with R as the regularizer.\nFirst, note that the baseline potential is the convex conjugate of the null regularizer, i.e., \u03a6\u22c6(w) = 0 for all w \u2208 X . The dual formulation of inf-conv smoothing (Equation 13) thus becomes\n\u03a6ic(\u0398; \u03b7,S) = max w\u2208X\n{ \u3008w,\u0398\u3009 \u2212 \u03b7R(w) } ,\nwhich is identical to Equation 3 except that the above expression has an extra parameter \u03b7 that controls the degree of smoothing. To simplify the deviation parameter in Equation 14, note that the subgradients of \u03a6 always lie in X because of duality. Hence, the two supremum expressions collapse to one supremum: maxw\u2208X S\u22c6(w). Plugging the smoothing parameters into Corollary 4 gives the well-known FTRL regret bound as in Theorem 2.11 or 2.21 of Shalev-Shwartz (2012):\nRegret \u2264 \u03b7max w\u2208X S\u22c6(w) + \u03b2 2\u03b7\nT\u2211\nt=1\n\u2016\u03b8t\u20162."}, {"heading": "Acknowledgments", "text": "CL and AT gratefully acknowledge the support of NSF under grant IIS-1319810. We thank the anonymous reviewers for their helpful suggestions. We would also like to thank Andre Wibisono for several very useful discussions and his help improving the manuscript. Finally we thank Elad Hazan for early support in developing the ideas herein."}, {"heading": "Appendix A. Gradient-Based Prediction Algorithm", "text": "A.1. Proof of Lemma 2\nProof We note that since \u03a60(0) = 0,\n\u03a6T (\u0398T ) =\nT\u2211\nt=1\n\u03a6t(\u0398t)\u2212 \u03a6t\u22121(\u0398t\u22121)\n= T\u2211\nt=1\n(( \u03a6t(\u0398t)\u2212 \u03a6t(\u0398t\u22121) ) + ( \u03a6t(\u0398t\u22121)\u2212 \u03a6t\u22121(\u0398t\u22121) ))\nThe first difference can be rewritten as:\n\u03a6t(\u0398t)\u2212 \u03a6t(\u0398t\u22121) = \u3008\u2207\u03a6t(\u0398t\u22121),\u0398t)\u3009+D\u03a6t(\u0398t,\u0398t\u22121)\nBy combining the above two,\nRegret = \u03a6(\u0398T )\u2212 T\u2211\nt=1\n\u3008\u2207\u03a6t(\u0398t\u22121),\u0398t\u3009\n= \u03a6(\u0398T )\u2212 \u03a6T (\u0398T ) + T\u2211\nt=1\nD\u03a6t(\u0398t,\u0398t\u22121) + \u03a6t(\u0398t\u22121)\u2212 \u03a6t\u22121(\u0398t\u22121)\nwhich completes the proof."}, {"heading": "Appendix B. FTPL-FTRL Duality", "text": "B.1. Proof of Theorem 5\nProof Consider a one-dimensional online linear optimization prediction problem where the player chooses an action wt from X = [0, 1] and the adversary chooses a reward \u03b8t from Y = [0, 1]. This can be interpreted as a two-expert setting; the player\u2019s action wt \u2208 X is the probability of following the first expert and \u03b8t is the net excess reward of the first expert over the second. The baseline potential for this setting is \u03a6(\u0398) = maxw\u2208[0,1]w\u0398.\nLet us consider an instance of FTPL with a continuous distribution D whose cumulative density function (cdf) is FD . Let \u03a6\u0303 be the smoothed potential function (Equation 4) with distribution D. Its derivative is\n\u03a6\u0303\u2032(\u0398) = E[argmax w\u2208K w(\u0398 + u)] = P[u > \u2212\u0398] (15)\nbecause the maximizer is unique with probability 1. Notice, crucially, that the derivative \u03a6\u0303\u2032(\u0398) is exactly the expected solution of our FTPL instance. Moreover, by differentiating it again, we see that the second derivative of \u03a6\u0303 at \u0398 is exactly the pdf of D evaluated at (\u2212\u0398).\nWe can now precisely define the mapping from FTPL to FTRL. Our goal is to find a convex regularization function R such that P(u > \u2212\u0398) = argmaxw\u2208X (w\u0398 \u2212 R(w)). Since this is a\none-dimensional convex optimization problem, we can differentiate for the solution. The characterization of R is:\nR(w)\u2212R(0) = \u2212 \u222b w\n0 F\u22121D (1\u2212 z)dz. (16)\nNote that the cdf FD(\u00b7) is indeed invertible since it is a strictly increasing function. The inverse mapping is just as straightforward. Given a regularization function R well-defined over [0, 1], we can always construct its Fenchel conjugate R\u22c6(\u0398) = supw\u2208X \u3008w,\u0398\u3009 \u2212 R(w). The derivative of R\u22c6 is an increasing convex function, whose infimum is 0 at \u0398 = \u2212\u221e and supremum is 1 at \u0398 = +\u221e. Hence, R\u22c6 defines a cdf, and an easy calculation shows that this perturbation distribution exactly reproduces FTRL corresponding to R."}, {"heading": "Appendix C. Gaussian smoothing", "text": "C.1. Proof of Equation 10\nLet X1, . . . ,XN be independent standard Gaussian random variables, and let Z = maxi=1,...,N Xi. For any real number a, we have\nexp(aE[Z]) \u2264 E exp(aZ) = E max i=1,...,N\nexp(aXi) \u2264 N\u2211\ni=1\nE[exp(aXi)] = N exp(a 2/2).\nThe first inequality is from the convexity of the exponential function, and the last equality is by the definition of the moment generating function of Gaussian random variables. Taking the natural logarithm of both sides and dividing by a gives\nE[Z] \u2264 logN a + a 2 .\nIn particular, by choosing a = \u221a 2 logN , we have E[Z] \u2264 \u221a2 logN.\nC.2. Proof of Lemma 10\nProof By the subadditivity (triangle inequality) of \u03a6,\n\u03a6\u0303(\u0398; \u03b7,N (0, I)) \u2212 \u03a6\u0303(\u0398; \u03b7\u2032,N (0, I)) = Eu\u223cN (0,I)[\u03a6(\u0398 + \u03b7u)\u2212\u03a6(\u0398 + \u03b7\u2032u)] (17) \u2264 Eu\u223cN (0,I)[\u03a6((\u03b7 \u2212 \u03b7\u2032)u)] (18)\nand the statement follows from the positive homogeneity of \u03a6.\nC.3. Proof that the origin is the worst case (Lemma 12)\nProof Let \u03a6(\u0398) = \u2016\u0398\u20162 and \u03b7 be a positive number. By continuity of eigenvectors, it suffices to show that the maximum eigenvalue of the Hessian matrix of the Gaussian smoothed potential \u03a6\u0303(\u0398; \u03b7,N (0, I)) is decreasing in \u2016\u0398\u2016 for \u2016\u0398\u2016 > 0.\nBy Lemma 7, the gradient can be written as follows:\n\u2207\u03a6\u0303(\u0398; \u03b7,N (0, I)) = 1 \u03b7 Eu\u223cN (0,I)[u\u2016\u0398+ \u03b7u\u2016] (19)\nLet ui be the i-th coordinate of the vector u. Since the standard normal distribution is spherically symmetric, we can rotate the random variable u such that its first coordinate u1 is along the direction of \u0398. After rotation, the gradient can be written as\n1 \u03b7 Eu\u223cN (0,I)\n\nu \u221a \u221a \u221a \u221a(\u2016\u0398\u2016+ \u03b7u1)2 + N\u2211\nk=2\n\u03b72u2k\n\n\nwhich is clearly independent of the coordinates of \u0398. The pdf of standard Gaussian distribution has the same value at (u1, u2, . . . , un) and its sign-flipped pair (u1,\u2212u2, . . . ,\u2212un). Hence, in expectation, the two vectors cancel out every coordinate but the first, which is along the direction of \u0398. Therefore, there exists a function \u03b1 such that Eu\u223cN (0,I)[u\u2016\u0398+ \u03b7u\u2016] = \u03b1(\u2016\u0398\u2016)\u0398.\nNow, we will show that \u03b1 is decreasing in \u2016\u0398\u2016. Due to symmetry, it suffices to consider \u0398 = te1 for t \u2208 R+, without loss of generality. For any t > 0,\n\u03b1(t) = E[u1\n\u221a\n(t+ \u03b7u1)2 + u2rest)]/t\n= Eurest [Eu1 [u1 \u221a\n(t+ \u03b7u1)2 + b2|urest = b]]/t = Eurest [Ea=\u03b7|u1|[a (\u221a (t+ a)2 + b2 \u2212 \u221a (t\u2212 a)2 +B ) |urest = b]]/t\nLet g(t) = (\u221a (t+ a)2 +B \u2212 \u221a (t\u2212 a)2 +B ) /t. Take the first derivative with respect to t,\nand we have:\ng\u2032(t) = 1\nt2\n( \u221a\n(t\u2212 a)2 + b2 \u2212 t(t\u2212 a)\u221a (t+ a)2 + b2 \u2212 \u221a (t+ a)2 + b2 + t(t\u2212 a) \u221a (t+ a)2 + b2\n)\n= 1\nt2\n(\na2 + b2 \u2212 at \u221a (t\u2212 a)2 + b2 \u2212 a 2 + b2 + at \u221a (t+ a)2 + b2\n)\n( (a2 + b2)\u2212 at )2( (t+ a)2 + b2 ) \u2212 ( (a2 + b2) + at )2( (t\u2212 a)2 + b2 ) = \u22124ab2t3 < 0\nbecause t, \u03b7, u\u2032, B are all positive. So, g(t) < 0, which proves that \u03b1 is decreasing in \u0398. The final setp is to write the gradient as \u2207(\u03a6\u0303; \u03b7,N (0, I))(\u0398) = \u03b1(\u2016\u0398\u2016)\u0398 and differentiate it:\n\u22072f\u03b7(\u0398) = \u03b1\u2032(\u2016\u0398\u2016) \u2016\u0398\u2016 \u0398\u0398 T + \u03b1(\u2016\u0398\u2016)I\nThe Hessian has two distinct eigenvalues \u03b1(\u2016\u0398\u2016) and \u03b1(\u2016\u0398\u2016) + \u03b1\u2032(\u2016\u0398\u2016)\u2016\u0398\u2016, which correspond to the eigenspace orthogonal to \u0398 and parallel to \u0398, respectively. Since \u03b1\u2032 is negative, \u03b1 is always the maximum eigenvalue and it decreases in \u2016\u0398\u2016."}], "references": [{"title": "Optimal stragies and minimax lower bounds for online convex games", "author": ["Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Efficient market making via convex optimization, and a connection to online learning", "author": ["Jacob Abernethy", "Yiling Chen", "Jennifer Wortman Vaughan"], "venue": "ACM Transactions on Economics and Computation,", "citeRegEx": "Abernethy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2013}, {"title": "Smoothing and first order methods: A unified framework", "author": ["Amir Beck", "Marc Teboulle"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Beck and Teboulle.,? \\Q2012\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2012}, {"title": "Stochastic optimization problems with nondifferentiable cost functionals", "author": ["Dimitri P. Bertsekas"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Bertsekas.,? \\Q1973\\E", "shortCiteRegEx": "Bertsekas.", "year": 1973}, {"title": "Adaptive newton-based multivariate smoothed functional algorithms for simulation optimization", "author": ["Shalabh Bhatnagar"], "venue": "ACM Transactions on Modeling and Computer Simulation,", "citeRegEx": "Bhatnagar.,? \\Q2007\\E", "shortCiteRegEx": "Bhatnagar.", "year": 2007}, {"title": "Training with noise is equivalent to tikhonov regularization", "author": ["Chris M. Bishop"], "venue": "Neural Computation,", "citeRegEx": "Bishop.,? \\Q1995\\E", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Prediction by random-walk perturbation", "author": ["Luc Devroye", "G\u00e1bor Lugosi", "Gergely Neu"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Devroye et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Randomized smoothing for stochastic optimization", "author": ["John Duchi", "Peter L. Bartlett", "Martin J. Wainwright"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Duchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2012}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "Gradient Estimation Via Perturbation Analysis. Kluwer international series in engineering and computer science: Discrete event dynamic systems", "author": ["Paul Glasserman"], "venue": null, "citeRegEx": "Glasserman.,? \\Q1991\\E", "shortCiteRegEx": "Glasserman.", "year": 1991}, {"title": "Approximation to bayes risk in repeated play", "author": ["James Hannan"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "Hannan.,? \\Q1957\\E", "shortCiteRegEx": "Hannan.", "year": 1957}, {"title": "Oddness of the number of equilibrium points: a new proof", "author": ["John C. Harsanyi"], "venue": "International Journal of Game Theory,", "citeRegEx": "Harsanyi.,? \\Q1973\\E", "shortCiteRegEx": "Harsanyi.", "year": 1973}, {"title": "Fractional moments of a quadratic form in noncentral normal random variables", "author": ["James R. Harvey"], "venue": null, "citeRegEx": "Harvey.,? \\Q1965\\E", "shortCiteRegEx": "Harvey.", "year": 1965}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "ArXiv preprint,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "On the global convergence of stochastic fictitious play", "author": ["Josef Hofbauer", "William H. Sandholm"], "venue": null, "citeRegEx": "Hofbauer and Sandholm.,? \\Q2002\\E", "shortCiteRegEx": "Hofbauer and Sandholm.", "year": 2002}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam T. Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization", "author": ["H. Brendan McMahan"], "venue": "In AISTATS,", "citeRegEx": "McMahan.,? \\Q2011\\E", "shortCiteRegEx": "McMahan.", "year": 2011}, {"title": "Theory of random sets. Probability and its applications", "author": ["Ilya S. Molchanov"], "venue": null, "citeRegEx": "Molchanov.,? \\Q2005\\E", "shortCiteRegEx": "Molchanov.", "year": 2005}, {"title": "Random gradient-free minimization of convex functions", "author": ["Yurii Nesterov"], "venue": "ECORE Discussion Paper,", "citeRegEx": "Nesterov.,? \\Q2011\\E", "shortCiteRegEx": "Nesterov.", "year": 2011}, {"title": "Relax and randomize : From value to algorithms", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Convex Analysis. Convex Analysis", "author": ["R.T. Rockafellar"], "venue": null, "citeRegEx": "Rockafellar.,? \\Q1997\\E", "shortCiteRegEx": "Rockafellar.", "year": 1997}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2012}, {"title": "On the universality of online mirror descent", "author": ["Nati Srebro", "Karthik Sridharan", "Ambuj Tewari"], "venue": "In Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "Srebro et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2011}, {"title": "Follow the leader with dropout perturbations", "author": ["Tim van Erven", "Wojciech Kotlowski", "Manfred K. Warmuth"], "venue": "In Proceedings of Conference on Learning Theory (COLT),", "citeRegEx": "Erven et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Erven et al\\.", "year": 2014}, {"title": "Dropout training as adaptive regularization", "author": ["Stefan Wager", "Sida Wang", "Percy Liang"], "venue": "In Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "Wager et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2013}, {"title": "Convex nondifferentiable stochastic optimization: A local randomized smoothing technique", "author": ["Farzad Yousean", "Angelia Nedi\u0107", "Uday V. Shanbhag"], "venue": "In Proceedings of American Control Conference (ACC),", "citeRegEx": "Yousean et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yousean et al\\.", "year": 2010}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 23, "context": "The regret analysis of FTRL reduces to the analysis of the second-order behavior of the penalty function (Shalev-Shwartz, 2012), which is well-studied due to the powerful convex analysis tools.", "startOffset": 105, "endOffset": 127}, {"referenceID": 22, "context": "The regret analysis of FTRL reduces to the analysis of the second-order behavior of the penalty function (Shalev-Shwartz, 2012), which is well-studied due to the powerful convex analysis tools. In fact, regularization via penalty methods for online learning in general are very well understood. Srebro et al. (2011) proved that Mirror Descent, a regularization via penalty method, achieves a nearly optimal regret guarantee for a general class of online learning problems, and McMahan (2011) showed that FTRL is equivalent to Mirror Descent under some assumptions.", "startOffset": 106, "endOffset": 316}, {"referenceID": 18, "context": "(2011) proved that Mirror Descent, a regularization via penalty method, achieves a nearly optimal regret guarantee for a general class of online learning problems, and McMahan (2011) showed that FTRL is equivalent to Mirror Descent under some assumptions.", "startOffset": 168, "endOffset": 183}, {"referenceID": 17, "context": "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014).", "startOffset": 145, "endOffset": 216}, {"referenceID": 7, "context": "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014).", "startOffset": 145, "endOffset": 216}, {"referenceID": 15, "context": "(2013) showed that for online learning, dropout training (Hinton et al., 2012) is similar to AdagGrad (Duchi et al.", "startOffset": 57, "endOffset": 78}, {"referenceID": 8, "context": ", 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the Fisher information.", "startOffset": 31, "endOffset": 51}, {"referenceID": 6, "context": "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014). Convex analysis techniques, which led to our current thorough understanding of FTRL, have not been applied to FTPL, partly because the decision rule of FTPL does not explicitly contain a convex function. In this paper, we present a new analysis framework that makes it possible to analyze FTPL in the same way that FTRL has been analyzed, particularly with regards to second-order properties of convex functions. We show that both FTPL and FTRL naturally arise as smoothing operations of a non-smooth potential function and the regret analysis boils down to controlling the smoothing parameters as defined in Section 3. This new unified analysis framework not only recovers the known optimal regret bounds, but also gives a new type of generic regret bounds. Prior to our work, Rakhlin et al. (2012) showed that both FTPL and FTRL naturally arise as admissible relaxations of the minimax value of the game between the learner and adversary.", "startOffset": 171, "endOffset": 1018}, {"referenceID": 6, "context": "Unfortunately, the analysis of FTPL lacks a generic framework and relies substantially on clever algebra tricks and heavy probabilistic analysis (Kalai and Vempala, 2005; Devroye et al., 2013; van Erven et al., 2014). Convex analysis techniques, which led to our current thorough understanding of FTRL, have not been applied to FTPL, partly because the decision rule of FTPL does not explicitly contain a convex function. In this paper, we present a new analysis framework that makes it possible to analyze FTPL in the same way that FTRL has been analyzed, particularly with regards to second-order properties of convex functions. We show that both FTPL and FTRL naturally arise as smoothing operations of a non-smooth potential function and the regret analysis boils down to controlling the smoothing parameters as defined in Section 3. This new unified analysis framework not only recovers the known optimal regret bounds, but also gives a new type of generic regret bounds. Prior to our work, Rakhlin et al. (2012) showed that both FTPL and FTRL naturally arise as admissible relaxations of the minimax value of the game between the learner and adversary. In short, adding a random perturbation and adding a regularization penalty function are both optimal ways to simulate the worst-case future input sequence. We establish a stronger connection between FTRL and FTPL; both algorithms are derived from smoothing operations and they are equivalent up to the smoothing parameters. This equivalence is in fact a very strong result, considering the fact that Harsanyi (1973) showed that there is no general bijection between FTPL and FTRL.", "startOffset": 171, "endOffset": 1575}, {"referenceID": 5, "context": "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al.", "startOffset": 0, "endOffset": 14}, {"referenceID": 5, "context": "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton et al.", "startOffset": 0, "endOffset": 165}, {"referenceID": 5, "context": "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton et al., 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the Fisher information. These results are derived from Taylor approximations, but our FTPL-FTRL connection is derived from the convex conjugate duality. An interesting feature of our analysis framework is that we can directly apply existing techniques from the optimization literature, and conversely, our new findings in online linear optimization may apply to optimization theory. In Section 4.3, a straightforward application of the results on Gaussian smoothing by Nesterov (2011) and Duchi et al.", "startOffset": 0, "endOffset": 805}, {"referenceID": 5, "context": "Bishop (1995) showed that adding Gaussian noise to features of the training examples is equivalent to Tikhonov regularization, and more recently Wager et al. (2013) showed that for online learning, dropout training (Hinton et al., 2012) is similar to AdagGrad (Duchi et al., 2010) in that both methods scale features by the Fisher information. These results are derived from Taylor approximations, but our FTPL-FTRL connection is derived from the convex conjugate duality. An interesting feature of our analysis framework is that we can directly apply existing techniques from the optimization literature, and conversely, our new findings in online linear optimization may apply to optimization theory. In Section 4.3, a straightforward application of the results on Gaussian smoothing by Nesterov (2011) and Duchi et al. (2012) gives a generic regret bound for an arbitrary online linear optimization problem.", "startOffset": 0, "endOffset": 829}, {"referenceID": 0, "context": "In fact, we can bound the Bregman divergence by analyzing the local behavior of Hessian, as the following adaptation of Abernethy et al. (2013, Lemma 4.6) shows. Lemma 1 Let f be a twice-differentiable convex function with domf \u2286 R . Let x \u2208 domf , such that vT\u22072f(x+\u03b1v)v \u2208 [a, b] (a \u2264 b) for all \u03b1 \u2208 [0, 1]. Then, a\u2016v\u20162/2 \u2264 Df (x+v, x) \u2264 b\u2016v\u20162/2. The Fenchel conjugate of f is f(\u03b8) = supw\u2208dom(f){\u3008w, \u03b8\u3009 \u2212 f(w)}, and it is a dual mapping that satisfies f = (f) and \u2207f\u22c6 \u2208 dom(f). By the strong convexity-strong smoothness duality, f is \u03b2-strongly smooth with respect to a norm \u2016 \u00b7 \u2016 if and only if f is 1 \u03b2 -strongly smooth with respect to the dual norm \u2016 \u00b7 \u2016\u22c6. For more details and proofs, readers are referred to an excellent survey by Shalev-Shwartz (2012).", "startOffset": 120, "endOffset": 759}, {"referenceID": 7, "context": "Since we focus on the curvature property of functions as opposed to random vectors, our FTPL analysis involves less probabilistic analysis than Devroye et al. (2013) or van Erven et al.", "startOffset": 144, "endOffset": 166}, {"referenceID": 7, "context": "Since we focus on the curvature property of functions as opposed to random vectors, our FTPL analysis involves less probabilistic analysis than Devroye et al. (2013) or van Erven et al. (2014) does.", "startOffset": 144, "endOffset": 193}, {"referenceID": 11, "context": "This technique of stochastic smoothing has been well-studied in the optimization literature for gradientfree optimization algorithms (Glasserman, 1991; Yousean et al., 2010) and accelerated gradient methods for non-smooth optimizations (Duchi et al.", "startOffset": 133, "endOffset": 173}, {"referenceID": 27, "context": "This technique of stochastic smoothing has been well-studied in the optimization literature for gradientfree optimization algorithms (Glasserman, 1991; Yousean et al., 2010) and accelerated gradient methods for non-smooth optimizations (Duchi et al.", "startOffset": 133, "endOffset": 173}, {"referenceID": 9, "context": ", 2010) and accelerated gradient methods for non-smooth optimizations (Duchi et al., 2012).", "startOffset": 70, "endOffset": 90}, {"referenceID": 12, "context": "Each argmax expression is equivalent to the decision rule of FTPL (Hannan, 1957; Kalai and Vempala, 2005); the GBPA on a stochastically smoothed potential can thus be seen as playing the expected action of FTPL.", "startOffset": 66, "endOffset": 105}, {"referenceID": 17, "context": "Each argmax expression is equivalent to the decision rule of FTPL (Hannan, 1957; Kalai and Vempala, 2005); the GBPA on a stochastically smoothed potential can thus be seen as playing the expected action of FTPL.", "startOffset": 66, "endOffset": 105}, {"referenceID": 10, "context": "Previously it had been observed3 that the Hedge Algorithm (Freund and Schapire, 1997), which can be cast as FTRL with an entropic regularization R(w) = \u2211i wi logwi, is equivalent to FTPL with Gumbel-distributed noise.", "startOffset": 58, "endOffset": 85}, {"referenceID": 16, "context": "However, the result appears to be folklore in the area of probabilistic choice models, and it is mentioned briefly in Hofbauer and Sandholm (2002).", "startOffset": 118, "endOffset": 147}, {"referenceID": 20, "context": "In the game theoretic analysis of Gaussian perturbations by Rakhlin et al. (2012), the algorithm uses the scaling parameter \u03b7t = \u221a T \u2212 t, which requires the knowledge of T and does not adapt to data.", "startOffset": 60, "endOffset": 82}, {"referenceID": 7, "context": "Devroye et al. (2013) proposed the Prediction by Random Walk (PRW) algorithm, which flips a fair coin every round and decides whether to add 1 to each coordinate.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "This is in contrast to the Hedge Algorithm (Freund and Schapire, 1997), which is an \u03b7-smoothing with parameters (logN, 1, \u2016 \u00b7 \u2016) (See Section 5 for details).", "startOffset": 43, "endOffset": 70}, {"referenceID": 0, "context": "We show that the GBPA with Gaussian smoothing achieves a minimax optimal regret (Abernethy et al., 2008) up to a constant factor.", "startOffset": 80, "endOffset": 104}, {"referenceID": 28, "context": "Online Convex Optimization In online convex optimization, the learner receives a sequence of convex functions ft whose domain is X and its subgradients are in the set Y (Zinkevich, 2003).", "startOffset": 169, "endOffset": 186}, {"referenceID": 2, "context": "Online Linear Optimization via Inf-conv Smoothing Beck and Teboulle (2012) proposed inf-conv smoothing, which is an infimal convolution with a strongly smooth function.", "startOffset": 50, "endOffset": 75}, {"referenceID": 23, "context": "21 of Shalev-Shwartz (2012):", "startOffset": 6, "endOffset": 28}], "year": 2014, "abstractText": "We present a new optimization-theoretic approach to analyzing Follow-the-Leader style algorithms, particularly in the setting where perturbations are used as a tool for regularization. We show that adding a strongly convex penalty function to the decision rule and adding stochastic perturbations to data correspond to deterministic and stochastic smoothing operations, respectively. We establish an equivalence between \u201cFollow the Regularized Leader\u201d and \u201cFollow the Perturbed Leader\u201d up to the smoothness properties. This intuition leads to a new generic analysis framework that recovers and improves the previous known regret bounds of the class of algorithms commonly known as Follow the Perturbed Leader.", "creator": "LaTeX with hyperref package"}}}