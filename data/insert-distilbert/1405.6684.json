{"id": "1405.6684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2014", "title": "Visualizing Random Forest with Self-Organising Map", "abstract": "random forest ( rf ) is ideally a powerful ensemble method for classification and regression tasks. it consists of decision trees set. traditionally although, a 3d single tree is well interpretable for human, the resulting ensemble of trees is hardly a black - box model. originally the popular technique to look inside the rf model is to visualize a rf proximity matrix obtained on data samples with multidimensional scaling ( mds ) computational method. herein, we present a novel method method based on self - organising maps ( som ) for independently revealing intrinsic relationships in data that lay inside the rf used tree for classification tasks. alternately we propose an algorithm attempting to learn the som with the proximity matrix obtained from the rf. the easier visualization of spatial rf proximity matrix with mds technique and som is compared. what is or more, the classical som learned with choosing the rf proximity matrix has better classification accuracy in comparison to som learned with euclidean distance. presented approach enables better understanding of the rf and furthermore additionally improves accuracy of the som.", "histories": [["v1", "Mon, 26 May 2014 19:00:15 GMT  (386kb,D)", "http://arxiv.org/abs/1405.6684v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["piotr p{\\l}o\\'nski", "krzysztof zaremba"], "accepted": false, "id": "1405.6684"}, "pdf": {"name": "1405.6684.pdf", "metadata": {"source": "CRF", "title": "Visualizing Random Forest with Self-Organising Map", "authors": ["Piotr P lo\u0144ski", "Krzysztof Zaremba"], "emails": ["pplonski@ire.pw.edu.pl", "zaremba@ire.pw.edu.pl"], "sections": [{"heading": null, "text": "Keywords: Random Forest, Self-Organising Maps, visualization, classification, proximity matrix"}, {"heading": "1 Introduction", "text": "Nowadays, there is a need for efficient data mining techniques. The human readability of the model is an important factor of a good data mining algorithm. Among various data mining methods very popular are decision trees [20], [3]. Although, they have an easy interpretable model, a single tree does not always obtain the highest accuracy. To overcome this problem, various ensemble methods were proposed. Among them, the popular is Random Forest (RF) proposed by Leo Breiman [2]. The RF builds a set of trees using bagging and random subspace methods. The final output is a mode of responses from all individual trees. The RF can be used for classification and regression tasks. Despite the high accuracy of the RF, the human readability of the model is lost. There exist some methods to look inside RF black-box, like: examining variable importance [4], parallel cooridinate plots by variable [2] or visualizing the RF proximity distance\n0 The final publication is available at http://link.springer.com/chapter/10.1007/ 978-3-319-07176-3_6\nar X\niv :1\n40 5.\n66 84\nv1 [\ncs .L\nG ]\n2 6\nM ay\n2 01\n4\nmatrix with Multidimensional Scaling [6]. Herein, we propose a novel method for visualizing the RF proximity matrix based on Self-Organising Maps (SOM) [8]. The SOM is an artificial neural network model that maps high-dimensional input data space onto usually two-dimensional lattice of neurons in an unsupervised way. Although, the SOM is an originally unsupervised algorithm there exist supervised extensions [9], [14], [13], [7]. The SOM has been proved as an efficient data mining tool in many real life applications [11], [12], [18], [19]. In this paper, we focus on using the labeled SOM model for mapping the RF used in classification tasks. The RF proximity matrix will be used for the SOM learning. It was shown that using more sophisticated distance metric than Euclidean can improve the accuracy of the SOM [15]. The RF proximity matrix was used earlier for improving clustering accuracy. Horvath et al. [16] presented method for building clusters from the RF learned with unlabeled data and successfully used it for tumor detection [17]. Moosmann et al. [10] used the RF for efficient segmentation of images, where leaves were assigned to distinct image regions rather than to specific class. Gray et al. [5] used the RF proximity matrix and the MDS for classification of medical images of different types of dementia. The paper is organized as follow: firstly, we describe the SOM and the RF algorithms; secondly, proposed approach for learning the SOM with the RF proximity matrix is presented; then, the MDS vs the SOM visualization and the accuracy of the SOM learned with Euclidean metric and the RF proximity matrix are compared."}, {"heading": "2 Methods", "text": "Let\u2019s denote data set as D = {(xi, ci)}, where xi is an attribute vector, x \u2208 RM , M is attribute vector length and ci is a discrete class number of i -th sample, i = [1, 2, ..., N ] and c = [1, 2, ..., C]."}, {"heading": "2.1 Self Organising Maps", "text": "In this paper, we used the SOM as a two-dimensional grid of neurons. Each neuron is represented by a weights vector Wpq, where (p, q) are indices of the neuron in the grid. It is important to notice, that neuron\u2019s weights vector has the same length as sample\u2019s attribute vector in the data set. The neuron\u2019s weights directly corresponds to attributes in the data set. In the training phase, for each sample we search for a neuron which is the closest to the i -th sample. In the original SOM algorithm the distance is computed with squared Euclidean distance by following equation:\nDisttrain(Di,Wpq) = (xi \u2212Wpq)T (xi \u2212Wpq). (1)\nThe neuron (p, q) with the smallest distance to i -th sample is so-called the Best Matching Unit (BMU), and we note its indicies as (r, v). Once the BMU is found, the weights update step is executed. The weights of each neuron are updated with formula:\nWpq(t+ 1) = Wpq(t) + \u03b7h (i) pq (xi \u2212Wpq(t)), (2)\nwhere t is an iteration number and \u03b7 is a learning coefficient and h (i) pq is a neighbourhood function. We assume that one iteration is a presentation of one training sample, whereas a presentation of all training samples is one learning epoch. The learning coefficient \u03b7 is decreased between consecutive epochs to improve network\u2019s ability to remember patterns. It is described by:\n\u03b7 = \u03b70exp(\u2212e\u03bb\u03b7), (3)\nwhere \u03b70 is the initial step size, e is the current epoch number and \u03bb\u03b7 is responsible for regulating the speed of the decrease. The neighbourhood function controls changing of weights with respect to the distance to the BMU in the grid. It is noted as:\nh(i)pq = exp(\u2212\u03b1((r \u2212 p)2 + (v \u2212 q)2)), (4)\nwhere \u03b1 describes the neighbourhood function width. This parameter is increasing during learning \u03b1 = \u03b10exp(\u2212(estop \u2212 e)\u03bb\u03b1) - it assures that neighbourhood becomes narrower during the training. The network is trained till chosen number of learning procedure epochs estop is exceeded.\nIn the described algorithm the class label information is not used. The simplest approach of using the SOM as a classifier is to label the neurons after the unsupervised training. For each neuron we remember the overal sum of neighbourhood values h (i) pq from each class over all samples. The label of major class is assigned to the neuron. In the testing phase, the input sample\u2019s class is designated based on the class of the found BMU."}, {"heading": "2.2 Random Forest", "text": "In the RF algorithm a set of single trees is built. The process of constructing one tree can be described in the following steps:\n1. Draw a bootstrap data set D\u2032 by choosing n times with replacement from all N training samples. 2. Determine a decision at node using only m attributes, where m is smaller than M . The split is selected based on maximal Information Gain. 3. Move the data through the node with respect to decision from the step 2. 4. Repeat steps 2, 3 till full tree is grown.\nAt the end of tree constructing, the class label is assigned to each leaf based on a class of samples in it. In the testing phase, a new sample is pushed down through all trees. From each tree a class label is remembered based on class of the reached leaf. The final response is the mode of votes from all trees. The proximity matrix Prox, with size NxN , can be easily obtained by putting all samples down the all trees. If two samples i and j are in the same terminal node in the tree, their proximity is increased by one Prox(i, j) = Prox(i, j) + 1. After the presentation of all samples the proximities are divided by the number of trees in the RF. The greater proximity value is, the more similar samples are. The dissimilarity measure can be formulated as Dis(i, j) = 1\u2212 Prox(i, j)."}, {"heading": "2.3 Self Organising Maps learned with Random Forest (RF-SOM)", "text": "In the proposed approach we assume that the RF is already learned. The learning of the network in one epoch can be summarized in the following steps:\n1. Build a data set H as a union of all network\u2019s weights W and attribute vector xj of j-th sample, H = W \u222a xj . The matrix W size is LxM , where the L is a total number of neurons in the network. In this matrix each row contains weights from one neuron, the mapping from neurons\u2019s 2D grid to matrix W is assumed. 2. For set H compute dissimilarity matrix DisH using the RF. The DisH size is (L+ 1)x(L+ 1). 3. Find the smallest distance to the neurons in dissimilarity matrix DisH , in distances corresponding to j-th sample:\nv = arg min h DisH(j, h), h 6= j, (5)\nwhere v is an index of BMU in the matrix W , which can be mapped into r, v indices in the network 2D grid.\n4. Update the network weigths with formula 2. 5. Repeat steps 1-4 for all samples in the training set.\nAfter the end of the SOM learning, it is labeled as described in Section 2.1. In the testing phase, for input sample the BMU search is performed by taking the steps 1-3. The output class label is the same as the BMU class. We will denote the proposed method as RF-SOM. The computational complexity of using the Euclidean distance in the SOM is O(N \u2217L), because we need to compute for each sample, from N samples, a distance between sample and L neurons. Whereas, the using of the RF proximity matrix in the proposed RF-SOM has complexity O(N \u2217 L \u2217 T \u2217 log2(treesize))1, where treesize is a number of nodes in the tree. In the RF-SOM for each sample we propagate L + 1 input vectors through T trees in the RF, and passage through a tree has complexity O(log2(treesize)). The complexity of distance computation using the RF proximity matrix is worse than using Euclidean distance. Although, it is beneficial when compared to the memory complexity of the MDS used for RF proximity visualization, which uses O(N2) memory. The RF-SOM requires only O(L2). This discards the MDS as a method of the RF proximity matrix visualization for large data sets."}, {"heading": "3 Results", "text": "We used 6 real data sets to examine properties of the RF-SOM. There were used data sets: \u2019Glass\u2019, \u2019Wine\u2019, \u2019Iris\u2019, \u2019Sonar\u2019, \u2019Ionosphere\u2019, \u2019Pima\u2019 from the \u2019UCI Machine Learning Repository\u2019 [1]. In the Table 1 are presented data sets properties. In all experiments we used following parameters values for each SOM type:\n1 We omit cost of constructing the RF in the complexity assessment.\nestop = 200, \u03b70 = 0.1, \u03bb\u03b7 = 0.0345, \u03b10 = 0.1, \u03bb\u03b1 = 0.008. We will denote a network learned with Eculidean distance as SOM. The network sizes for the SOM and the RF-SOM for each data set are equal, they are presented in the Table 1. The network sizes were chosen arbitrarily because selecting optimal network size is not in the scope of this paper. In all cases the SOM and the RF-SOM starts learning from the same initial weights values. The RF was constructed with 100 trees and m = \u221a M for all data sets.\nTo present visulization properties of the proposed method we used \u2019Pima\u2019 data set. In the Fig.1 there are presented: the SOM (Fig.1a) and the MDS (Fig.1c) both learned with Euclidean distance; and RF-SOM (Fig.1b) and RFMDS (Fig.1d) constructed using the RF proximity matrix. The SOM networks were presented as a 2D grid of neurons, where for each neuron, its weigths are presented by a polar area diagram (sometimes called coxcomb plot). In the MDS and the RF-MDS plots information about points distribution in reduced 2D space is available. However, information about how point\u2019s position is affected by combination of attributes values is missing. What is more, there is hard to find crisp border to distinguish two classes on the MDS neither on the RF-MDS. In contrary to MDS technique, the SOM and RF-SOM plots do not provide information about explicit point distribution but rather a mapping of attributes combination onto 2D grid of neurons. After network labeling the class labels are assigned to neurons, therefore distinguishing specific combination of attributes in each class is possible. As expected, although, the SOM and the RF-SOM started learning from the same initial weights values they have different final distribution of attributes combinations across the network.\nTo compare the accuracy of the SOM learned with Euclidean distance and RF-SOM learned with RF proximity matrix we use information about samples class label. We measure the accuracy of classification. The results of comparison are presented in the Table 2. All of the results are mean over 10-fold cross validation. In the Table 2 we also include the accuracy of the alone RF as the reference. The RF-SOM on 4 data sets (\u2019Glass\u2019, \u2019Sonar\u2019, \u2019Ionosphere\u2019, \u2019Pima\u2019) obtained better results than the SOM. The greatest improvement over the SOM was achived on \u2019Sonar\u2019 set, it was 12.57%. The same performance of the SOM\nand the RF-SOM was on \u2019Wine\u2019 and \u2019Iris\u2019, when on the latter the RF-SOM has higher standard deviation value. It is worth noting, that the improvement of the RF-SOM over the SOM depends on the accuracy of the RF. On data sets (like \u2019Sonar\u2019 or \u2019Glass\u2019) where the accuracy difference between the RF and the SOM is high, the RF-SOM noted large improvement over the SOM. Whereas, on data sets (\u2019Wine\u2019 and \u2019Iris\u2019) with small accuracy difference between the RF and the SOM, the RF-SOM obtained the same mean accuracy as the SOM.\nWe measure classfication accuracy for different number of trees in the RF to examine the influence of a number of trees in the RF to performance of the RF-SOM. The accuracy for the RF and the RF-SOM for a number of trees in the RF, T = {10, 20, 50, 100, 200, 500}, is presented in Fig.2. It can be observed that for all data sets except \u2019Sonar\u2019 and \u2019Glass\u2019 the accuracy of the RF does not depend on T . The good results of classification are obtained even for 10 trees in the RF. For \u2019Sonar\u2019 and \u2019Glass\u2019 sets the accuracy of the RF increases with increasing a number of trees. The very similar behaviour can be observed for the RF-SOM. The accuracy for \u2019Wine\u2019, \u2019Iris\u2019, \u2019Ionosphere\u2019 and \u2019Pima\u2019 slightly varies with T growing. However, for \u2019Sonar\u2019 and \u2019Glass\u2019 sets the RF-SOM obtains better results if more trees are used in the RF. The observed behaviour can be explained by more complex data relationships in \u2019Sonar\u2019 and \u2019Glass\u2019 sets which are better modeled with greater number of trees in the RF."}, {"heading": "4 Conclusions", "text": "The novel method for visualizing the RF proximity matrix by the SOM was proposed. The RF-SOM method uses the RF to compute distances between input sample and neurons. The proposed method of visualization provide better understanding of relationship between data in the RF structure than the MDS. The RF-SOM contrary to the MDS provides a mapping of data onto 2D neurons grid. In case of new coming samples there is no need to recompute the whole RF proximity matrix like in the MDS method. Additionally, the proposed method has lower memory complexity than the MDS, which for large data sets is not applicable. What is more, the experimental results show that the RF-SOM learned with the RF dissimilarity gained better or the same accuracy than the SOM learned with Euclidean distance. As pointed in [16] the RF dissimilarity has attractive features: it can handle mixed variable types well, is invariant to monotonic transformations of the input variables and is robust to outliers. The attractiveness of RF dissimilarity and obtained results with RF-SOM encourage to focus our future work on using the RF proximity matrix in other clustering algorithms."}, {"heading": "5 Acknowledgements", "text": "PP has been supported by the European Union in the framework of European Social Fund through the Warsaw University of Technology Development Programme."}], "references": [{"title": "UCI machine learning repository", "author": ["A. Asuncion", "D.J. Newman"], "venue": "University of California, Irvine, School of Information and Computer Sciences", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Random Forests", "author": ["L. Breiman"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Multi-Test Decision Trees for Gene Expression Data Analysis, Lecture", "author": ["M. Czajkowski", "M. Grze\u015b", "M. Kretowski"], "venue": "Notes in Computer Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Random Forest-Based Manifold Learning for Classification of Imaging Data in Dementia", "author": ["K.R. Gray", "P. Aljabar", "R.A. Heckemann", "A. Hammers", "D. Rueckert"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Classification and Regression by randomForest", "author": ["A. Liaw", "M. Wiener"], "venue": "R News,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Fuzzy Supervised Self-Organizing Map for Semisupervised Vector Quantization", "author": ["M. K\u00e4stner", "T. Villmann"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The Self-Organizing Map", "author": ["T. Kohonen"], "venue": "Proceedings of the IEEE, vol.78, pp 14641480", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning Associations by Self-Organization: The LASSO model, Neurocomputing, vol.6, pp", "author": ["S. Midenet", "A. Grumbach"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Randomized Clustering Forests for Image Classification", "author": ["F. Moosmann", "E. Nowak", "F. Jurie"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Employing Self-Organizing Map for Fraud Detection", "author": ["D. Olszewski", "J. Kacprzyk", "S. Zadrozny"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "An Experimental Study on Asymmetric Self-Organizing Map", "author": ["D. Olszewski"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Fuzzy Clustering Neural Network for Classification of ECG Beats", "author": ["S. Osowski", "T.H. Linh"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Self-Organising Maps for Classification with MetropolisHastings Algorithm for Supervision", "author": ["P. P lo\u0144ski", "K. Zaremba"], "venue": "Lecture Notes in Computer Science", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Improving Performance of Self-Organising Maps with Distance Metric Learning Method", "author": ["P. P lo\u0144ski", "K. Zaremba"], "venue": "Lecture Notes in Computer Science, vol.7267, pp 169-177", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised Learning with Random Forest Predictors", "author": ["T. Shi", "S. Horvath"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Tumor classification by tissue microarray profiling: random forest clustering applied to renal cell carcinoma", "author": ["T. Shi", "D. Seligson", "A.S. Belldegrun", "A. Palotie", "S. Horvath"], "venue": "Modern Pathology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Self Organizing Maps for Visualization of Categories", "author": ["J. Szyma\u0144ski", "W. Duch"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Self\u2013Organizing Map Representation for Clustering Wikipedia Search Results", "author": ["J. Szyma\u0144ski"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "Among various data mining methods very popular are decision trees [20], [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "Among them, the popular is Random Forest (RF) proposed by Leo Breiman [2].", "startOffset": 70, "endOffset": 73}, {"referenceID": 1, "context": "There exist some methods to look inside RF black-box, like: examining variable importance [4], parallel cooridinate plots by variable [2] or visualizing the RF proximity distance 0 The final publication is available at http://link.", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "Zaremba matrix with Multidimensional Scaling [6].", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "Herein, we propose a novel method for visualizing the RF proximity matrix based on Self-Organising Maps (SOM) [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "Although, the SOM is an originally unsupervised algorithm there exist supervised extensions [9], [14], [13], [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 12, "context": "Although, the SOM is an originally unsupervised algorithm there exist supervised extensions [9], [14], [13], [7].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "Although, the SOM is an originally unsupervised algorithm there exist supervised extensions [9], [14], [13], [7].", "startOffset": 103, "endOffset": 107}, {"referenceID": 5, "context": "Although, the SOM is an originally unsupervised algorithm there exist supervised extensions [9], [14], [13], [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "The SOM has been proved as an efficient data mining tool in many real life applications [11], [12], [18], [19].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "The SOM has been proved as an efficient data mining tool in many real life applications [11], [12], [18], [19].", "startOffset": 94, "endOffset": 98}, {"referenceID": 16, "context": "The SOM has been proved as an efficient data mining tool in many real life applications [11], [12], [18], [19].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "The SOM has been proved as an efficient data mining tool in many real life applications [11], [12], [18], [19].", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "It was shown that using more sophisticated distance metric than Euclidean can improve the accuracy of the SOM [15].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "[16] presented method for building clusters from the RF learned with unlabeled data and successfully used it for tumor detection [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] presented method for building clusters from the RF learned with unlabeled data and successfully used it for tumor detection [17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 8, "context": "[10] used the RF for efficient segmentation of images, where leaves were assigned to distinct image regions rather than to specific class.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[5] used the RF proximity matrix and the MDS for classification of medical images of different types of dementia.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "There were used data sets: \u2019Glass\u2019, \u2019Wine\u2019, \u2019Iris\u2019, \u2019Sonar\u2019, \u2019Ionosphere\u2019, \u2019Pima\u2019 from the \u2019UCI Machine Learning Repository\u2019 [1].", "startOffset": 125, "endOffset": 128}], "year": 2014, "abstractText": "Abstract. Random Forest (RF) is a powerful ensemble method for classification and regression tasks. It consists of decision trees set. Although, a single tree is well interpretable for human, the ensemble of trees is a black-box model. The popular technique to look inside the RF model is to visualize a RF proximity matrix obtained on data samples with Multidimensional Scaling (MDS) method. Herein, we present a novel method based on Self-Organising Maps (SOM) for revealing intrinsic relationships in data that lay inside the RF used for classification tasks. We propose an algorithm to learn the SOM with the proximity matrix obtained from the RF. The visualization of RF proximity matrix with MDS and SOM is compared. What is more, the SOM learned with the RF proximity matrix has better classification accuracy in comparison to SOM learned with Euclidean distance. Presented approach enables better understanding of the RF and additionally improves accuracy of the SOM.", "creator": "TeX"}}}