{"id": "1406.0189", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2014", "title": "Convex Total Least Squares", "abstract": "we study the total positive least squares ( tls ) problem that generalizes least squares regression by allowing measurement errors in both dependent and independent variables. tls is widely used in applied fields including computer procedural vision, system identification and econometrics. the special case when all dependent and other independent variables have the same level of uncorrelated gaussian noise, known as ordinary tls, can be solved informally by singular value decomposition ( svd ). however, svd regression cannot solve many important and practical data tls problems with realistic noise structure, such as having varying measurement noise, known structure on the errors, or large outliers requiring robust error - norms. to solve such problems, we develop convex relaxation approaches for a general class of structured tls ( stls ). we show both theoretically and experimentally, that while approaching the plain nuclear norm relaxation incurs large approximation errors for stls, the re - weighted nuclear norm approach is very effective, and achieves better accuracy and on challenging stls problems than popular non - convex solvers. we describe a fast solution based on augmented lagrangian formulation, and apply our approach to an important class of biological problems that use population average measurements to infer cell - type and physiological - state specific expression levels that are very hard to ultimately measure directly.", "histories": [["v1", "Sun, 1 Jun 2014 18:13:08 GMT  (406kb,D)", "http://arxiv.org/abs/1406.0189v1", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG q-bio.GN q-bio.QM stat.AP", "authors": ["dmitry malioutov", "nikolai slavov"], "accepted": true, "id": "1406.0189"}, "pdf": {"name": "1406.0189.pdf", "metadata": {"source": "META", "title": "Convex Total Least Squares", "authors": ["Dmitry Malioutov", "Nikolai Slavov"], "emails": ["dmalioutov@us.ibm.com", "nslavov@alum.mit.edu"], "sections": [{"heading": null, "text": "We study the total least squares (TLS) problem that generalizes least squares regression by allowing measurement errors in both dependent and independent variables. TLS is widely used in applied fields including computer vision, system identification and econometrics. The special case when all dependent and independent variables have the same level of uncorrelated Gaussian noise, known as ordinary TLS, can be solved by singular value decomposition (SVD). However, SVD cannot solve many important practical TLS problems with realistic noise structure, such as having varying measurement noise, known structure on the errors, or large outliers requiring robust error-norms. To solve such problems, we develop convex relaxation approaches for a general class of structured TLS (STLS). We show both theoretically and experimentally, that while the plain nuclear norm relaxation incurs large approximation errors for STLS, the re-weighted nuclear norm approach is very effective, and achieves better accuracy on challenging STLS problems than popular non-convex solvers. We describe a fast solution based on augmented Lagrangian formulation, and apply our approach to an important class of biological problems that use population average measurements to infer cell-type and physiological-state specific expression levels that are very hard to measure directly.\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s)."}, {"heading": "1. Introduction", "text": "Total least squares is a powerful generalization of ordinary least squares (LS) which allows errors in the measured explanatory variables (Golub & Van Loan, 1980). It has become an indispensable tool in a variety of disciplines including chemometrics, system identification, astronomy, computer vision, and econometrics (Markovsky & Van Huffel, 2007). Consider a least squares problem y \u2248 X\u03b2, where we would like to find coefficients \u03b2 to best predict the target vector y based on measured variables X. The usual assumption is thatX is known exactly, and that the errors come from i.i.d. additive Gaussian noise n: y = X\u03b2 + n. The LS problem has a simple closed-form solution by minimizing \u2016y\u2212X\u03b2\u201622 with respect to \u03b2. In many applications not only y but also X is known only approximately, X = X0 + Ex, where X0 are the uncorrupted values, and Ex are the unknown errors in observed variables. The total least squares (TLS) formulation, or errors in variables regression, tries to jointly minimize errors in y and in X (`2-norm of n and Frobenius norm of Ex):\nmin n,Ex,\u03b2\n\u2016n\u201622 + \u2016Ex\u20162F where y = (X \u2212Ex)\u03b2 + n (1)\nWhile the optimization problem in this form is not convex, it can in fact be reformulated as finding the closest rank-deficient matrix to a given matrix, and solved in closed form via the singular value decomposition (SVD) (Golub & Van Loan, 1980).\nMany error-in-variables problems of practical interest have additional information: for example, a subset of the entries in X may be known exactly, we may know different entries with varying accuracy, and in general X may exhibit a certain structure, e.g. block-diagonal, Toeplitz, or Hankel in system identification literature (Markovsky et al., 2005). Furthermore, it is often important to use an error-norm robust to outliers, e.g. Huber loss or `1-loss. Unfortunately, with rare exceptions1, none of these problems allow an efficient solution, and the state of the art approach is to solve them\n1A closed form solution exists when subsets of columns are\nar X\niv :1\n40 6.\n01 89\nv1 [\nst at\n.M L\n] 1\nJ un\n2 01\nby local optimization methods (Markovsky & Usevich, 2014; Zhu et al., 2011; Srebro & Jaakkola, 2003).The only available guarantee is typically the ability to reach a stationary point of the non-convex objective.\nIn this paper we propose a principled formulation for STLS based on convex relaxations of matrix rank. Our approach uses the re-weighted nuclear norm relaxation (Fazel et al., 2001) and is highly flexible: it can handle very general linear structure on errors, including arbitrary weights (changing noise for different entries), patterns of observed and unobserved errors, Toeplitz and Hankel structures, and even norms other than the Frobenius norm. The nuclear norm relaxation has been successfully used for a range of machine learning problems involving rank constraints, including low-rank matrix completion, low-order system approximation, and robust PCA (Cai et al., 2010; Chandrasekaran et al., 2011). The STLS problem is conceptually different in that we do not seek low-rank solutions, but on the contrary nearly full-rank solutions. We show both theoretically and experimentally that while the plain nuclear norm formulation incurs large approximation errors, these can be dramatically improved by using the re-weighted nuclear norm. We suggest fast first-order methods based on Augmented Lagrangian multipliers (Bertsekas, 1982) to compute the STLS solution. As part of ALM we derive new updates for the re-weighted nuclear-norm based on solving the Sylvester\u2019s equation, which can also be used for many other machine learning tasks relying on matrixrank, including matrix completion and robust PCA.\nAs a case study of our approach to STLS we consider an important application in biology, quantification of cellular heterogeneity (Slavov & Botstein, 2011). We develop a new representation for the problem as a large structured linear system, and extend it to handle noise by a structured TLS problem with block-diagonal error structure. Experiments demonstrate the effectiveness of STLS in recovering physiological-state specific expression levels from aggregate measurements."}, {"heading": "1.1. Total Least Squares", "text": "We first review the solution of ordinary TLS problems. We simplify the notation from (1): combining our noisy data X and y into one matrix, A\u0304 , [X \u2212y], and the errors into E , [Ex \u2212 n] we have\nmin \u2016E\u20162F where (A\u0304\u2212 E) [ \u03b2 1 ] = 0. (2)\nThe matrix A\u0304 is in general full-rank, and a solution can be obtained by finding a rank-deficient matrix closest\nfully known; a Fourier transform based approach can handle block-circulant errors Ex (Beck & Ben-Tal, 2005).\nto A\u0304 in terms of the Frobenius norm. This finds smallest errors Ex and n such that y+n is in the range space of X \u2212 Ex. The closest rank-deficient matrix is simply obtained by computing the SVD, A\u0304 = USV T and setting the smallest singular value to be zero.\nStructured TLS problems (Markovsky & Van Huffel, 2007) allow more realistic errors Ex: with subsets of measurements that may be known exactly; weights reflecting different measurement noise for each entry; requiring linear structure of errors Ex such as Toeplitz that is crucial in deconvolution problems in signal processing. Unfortunately, the SVD does not apply to any of these more general versions of TLS (Srebro & Jaakkola, 2003; Markovsky & Van Huffel, 2007). Existing solutions to structured TLS problems formulate a non-convex optimization problem and attempt to solve it by local optimization (Markovsky & Usevich, 2014) that suffers from local optima and lack of guarantees on accuracy. We follow a different route and use a convex relaxation for the STLS problem."}, {"heading": "2. STLS via a nuclear norm relaxation", "text": "The STLS problem in a general form can be described as follows (Markovsky & Van Huffel, 2007). Using the notation in Section 1.1, suppose our observed matrix A\u0304 is M \u00d7 N with full column rank. We aim to find a nearby rank-deficient matrix A, rank(A) \u2264 N \u2212 1, where the errors E have a certain linear structure:\nmin \u2016W E\u20162F , where rank(A) \u2264 N \u2212 1 A = A\u0304\u2212 E , and L(E) = b (3)\nThe key components here are the linear equalities that E has to satisfy, L(E) = b. This notation represents a set of linear constraints tr(LTi E) = bi, for i = 1, .., J . In our application to cell heterogeneity quantification these constraints correspond to knowing certain entries of A exactly, i.e. Eij = 0 for some subset of entries, while other entries vary freely. One may require other linear structure such as Toeplitz or Hankel. We also allow an element-wise weighting W E, with Wi,j \u2265 0 on the errors, as some observations may be measured with higher accuracy than others. Finally, while we focus on the Frobenius norm of the error, any other convex error metric, for example, mean absolute error, or robust Huber loss, could be used instead. The main difficulty in the formulation is posed by the nonconvex rank constraint. The STLS problem is a special case of the structured low-rank approximation problem, where rank is exactly N \u2212 1 (Markovsky & Usevich, 2014). Next, we propose a tractable formulation for STLS based on convex relaxations of matrix rank.\nWe start by formulating the nuclear-norm relaxation for TLS and then improve upon it by using the re-\nweighted nuclear norm. The nuclear norm \u2016A\u2016\u2217 is a popular relaxation used to convexify rank constraints (Cai et al., 2010), and it is defined as the sum of the singular values of the matrix A, i.e. \u2016A\u2016\u2217 = \u2211 i \u03c3i(A). It can be viewed as the `1-norm of the singular value spectrum2 favoring few non-zero singular values, i.e., matrices with low-rank. Our initial nuclear norm relaxation for the STLS problem is:\nmin \u2016A\u2016\u2217 + \u03b1\u2016W E\u20162F such that A = A\u0304\u2212 E , and L(E) = b (4)\nThe parameter \u03b1 balances error residuals vs. the nuclear norm (proxy for rank). We chose the largest \u03b1, i.e. smallest nuclear norm penalty, that still produces rank(A) \u2264 N \u2212 1. This can be achieved by a simple binary search over \u03b1. In contrast to matrix completion and robust PCA, the STLS problem aims to find almost fully dense solutions with rank N \u2212 1, so it requires different analysis tools. We present theoretical analysis specifically for the STLS problem in Section 4. Next, we describe the re-weighted nuclear norm, which, as we show in Section 4, is better suited for the STLS problem than the plain nuclear norm."}, {"heading": "2.1. Reweighted nuclear norm and the log-determinant heuristic for rank", "text": "A very effective improvement of the nuclear norm comes from re-weighting it (Fazel et al., 2001; Mohan & Fazel, 2010) based on the log-determinant heuristic for rank. To motivate it, we first describe a closely related approach in the vector case (where instead of searching for low-rank matrices one would like to find sparse vectors). Suppose that we seek a sparse solution to a general convex optimization problem. A popular approach penalizes the `1-norm of the solution x \u2016x\u20161 = \u2211 i |xi| to encourage sparse solutions. A dramatic improvement in finding sparse signals can be obtained simply by using the weighted `1-norm, i.e. \u2211 i wi|xi| with suitable positive weights wi (Candes et al., 2008) instead of a plain `1-norm. Ideally the weights would be based on the unknown signal, to provide a closer approximation to sparsity (`0-norm) by penalizing large elements less than small ones. A practical solution first solves a problem involving the unweighted `1-norm, and uses the solution x\u0302 to define the weights wi =\n1 \u03b4+|x\u0302i| , with \u03b4 a small positive\nconstant. This iterative approach can be seen as an iterative local linearization of the concave log-penalty for sparsity, \u2211 i log(\u03b4+ |xi|) (Fazel et al., 2001; Candes et al., 2008). In both empirical and emerging theoretical studies(Needell, 2009; Khajehnejad et al., 2009)\n2For diagonal matrices A the nuclear norm is exactly equivalent to the `1-norm of the diagonal elements.\nre-weighting the `1-norm has been shown to provide a tighter relaxation of sparsity.\nIn a similar way, the re-weighted nuclear norm tries to penalize large singular values less than small ones by introducing positive weights. There is an analogous direct connection to the iterative linearization for the concave log-det relaxation of rank (Mohan & Fazel, 2010). Recall that the problem of minimizing the nuclear norm subject to convex set constraints C,\nmin \u2016A\u2016\u2217 such that A \u2208 C, (5)\nhas a semi-definite programming (SDP) representation (Fazel et al., 2001). Introducing auxiliary symmetric p.s.d. matrix variables Y,Z 0, we rewrite it as:\nmin A,Y,Z tr(Y ) + tr(Z) s.t. [ Y A AT Z ] 0, A \u2208 C (6)\nInstead of using the convex nuclear norm relaxation, it has been suggested to use the concave log-det approximation to rank:\nmin A,Y,Z log det(Y + \u03b4I) + log det(Z + \u03b4I)\ns.t. [ Y A AT Z ] 0, A \u2208 C (7)\nHere I is the identity matrix and \u03b4 is a small positive constant. The log-det relaxation provides a closer approximation to rank than the nuclear norm, but it is more challenging to optimize. By iteratively linearizing this objective one obtains a sequence of weighted nuclear-norm problems (Mohan & Fazel, 2010):\nmin A,Y,Z\ntr((Y k + \u03b4I)\u22121Y ) + tr((Zk + \u03b4I)\u22121Z)\ns.t. [ Y A AT Z ] 0, A \u2208 C (8)\nwhere Y k, Zk are obtained from the previous iteration, and Y 0, Z0 are initialized as I. Let W k1 = (Y k+\u03b4I)\u22121/2 and W k2 = (Z\nk+\u03b4I)\u22121/2 then the problem is equivalent to a weighted nuclear norm optimization in each iteration k:\nmin A,Y,Z\n\u2016W k1 AW k2 \u2016\u2217 s.t. A \u2208 C (9)\nThe re-weighted nuclear norm approach iteratively solves convex weighted nuclear norm problems in (9):\nRe-weighted nuclear norm algorithm: Initialize: k = 0, W 01 = W 0 2 = I.\n(1) Solve the weighted NN problem in (9) to get Ak+1.\n(2) Compute the SVD: W k1 A k+1W k2 = U\u03a3V T , and set Y k+1 = (W k1 ) \u22121U\u03a3UT (W k1 ) \u22121 and\nZk+1 = (W k2 ) \u22121V \u03a3V T (W k2 ) \u22121.\n(3) Set W k1 = (Y k+\u03b4I)\u22121/2 and W k2 = (Z k+\u03b4I)\u22121/2.\nThere are various ways to solve the plain and weighted nuclear norm STLS formulations, including interiorpoint methods (Toh et al., 1999) and iterative thresholding (Cai et al., 2010). In the next section we focus on augmented Lagrangian methods (ALM) (Bertsekas, 1982) which allow fast convergence without using computationally expensive second-order information."}, {"heading": "3. Fast computation via ALM", "text": "While the weighted nuclear norm problem in (9) can be solved via an interior point method, it is computationally expensive even for modest size data because of the need to compute Hessians. We develop an effective first-order approach for STLS based on the augmented Lagrangian multiplier (ALM) method (Bertsekas, 1982; Lin et al., 2010). Consider a general equality constrained optimization problem:\nmin x f(x) such that h(x) = 0. (10)\nALM first defines an augmented Lagrangian function:\nL(x,\u03bb, \u00b5) = f(x) + \u03bbTh(x) + \u00b5\n2 \u2016h(x)\u201622 (11)\nThe augmented Lagrangian method alternates optimization over x with updates of \u03bb for an increasing sequence of \u00b5k. The motivation is that either if \u03bb is near the optimal dual solution for (10), or, if \u00b5 is large enough, then the solution to (11) approaches the global minimum of (10). When f and h are both continuously differentiable, if \u00b5k is an increasing sequence, the solution converges Q-linearly to the optimal one (Bertsekas, 1982). The work of (Lin et al., 2010) extended the analysis to allow objective functions involving nuclear-norm terms. The ALM method iterates the following steps:\nAugmented Lagrangian Multiplier method\n(1) xk+1 = arg minx L(x,\u03bbk, \u00b5k)\n(2) \u03bbk+1 = \u03bbk + \u00b5kh(xk+1)\n(3) Update \u00b5k \u2192 \u00b5k+1 (we use \u00b5k = ak with a > 1).\nNext, we derive an ALM algorithm for nuclear-norm STLS and extend it to use reweighted nuclear norms based on a solution of the Sylvester\u2019s equations."}, {"heading": "3.1. ALM for nuclear-norm STLS", "text": "We would like to solve the problem:\nmin \u2016A\u2016\u2217 + \u03b1\u2016E\u20162F , such that (12) A\u0304 = A+ E, and L(E) = b\nTo view it as (10) we have f(x) = \u2016A\u2016\u2217 + \u03b1\u2016E\u20162F and h(x) = {A\u0304\u2212A\u2212E,L(E)\u2212b}. Using \u039b as our matrix Lagrangian multiplier, the augmented Lagrangian is:\nmin E:L(E)=b\n\u2016A\u2016\u2217+\u03b1\u2016E\u20162F +tr(\u039b T (A\u0304\u2212A\u2212E))+\n\u00b5 2 \u2016A\u0304\u2212A\u2212E\u20162F .\n(13)\nInstead of a full optimization over x = (E,A), we use coordinate descent which alternates optimizing over each matrix variable holding the other fixed. We do not wait for the coordinate descent to converge at each ALM step, but rather update \u039b and \u00b5 after a single iteration, following the inexact ALM algorithm in (Lin et al., 2010)3. Finally, instead of relaxing the constraint L(E) = b, we keep the constrained form, and follow each step by a projection (Bertsekas, 1982).\nThe minimum of (13) over A is obtained by the singular value thresholding operation (Cai et al., 2010):\nAk+1 = S\u00b5\u22121 ( A\u0304\u2212 Ek + \u00b5\u22121k \u039bk ) (14)\nwhere S\u03b3(Z) soft-thresholds the singular values of Z = USV T , i.e. S\u0303 = max(S \u2212 \u03b3, 0) to obtain Z\u0302 = US\u0303V T .\nThe minimum of (13) over E is obtained by setting the gradient with respect to E to zero, followed by a projection4 onto the affine space defined by L(E) = b:\nE\u0303k+1 = 1\n2\u03b1+ \u00b5k\n( \u039bk + \u00b5k(A\u0304\u2212A) ) and Ek+1 = \u03a0E:L(E)=bE\u0303k+1 (15)"}, {"heading": "3.2. ALM for re-weighted nuclear-norm STLS", "text": "To use the log-determinant heuristic, i.e., the reweighted nuclear norm approach, we need to solve the weighted nuclear norm subproblems:\nmin \u2016W1AW2\u2016\u2217 + \u03b1\u2016E\u20162F where (16) A\u0304 = A+ E , and L(E) = b\nThere is no known analytic thresholding solution for the weighted nuclear norm, so instead we follow (Liu et al., 2010) to create a new variable D = W1AW2 and add this definition as an additional linear constraint:\nmin \u2016D\u2016\u2217 + \u03b1\u2016E\u20162F where (17) A\u0304 = A+ E, D = W1AW2 , and L(E) = b\nNow we have two Lagrangian multipliers \u039b1 and \u039b2\n3This is closely related to the popular alternating direction of multipliers methods (Boyd et al., 2011).\n4For many constraints of interest this projection is highly efficient: when the constraint fixes some entries Eij = 0, projection simply re-sets these entries to zero. Projection onto Toeplitz structure simply takes an average along each diagonal, e.t.c\nAlgorithm 1 ALM for weighted NN-STLS\nInput: A\u0304, W1, W2, \u03b1 repeat \u2022 Update D via soft-thresholding: Dk+1 = S\u00b5\u22121k ( W1AW2 \u2212 1/\u00b5k\u039bk2 ) .\n\u2022 Update E as in (15). \u2022 Solve Sylvester system for A in (19). \u2022 Update \u039bk+11 = \u039bk1 + \u00b5k(A\u0304\u2212A\u2212 E),\n\u039bk+12 = \u039b k 2 + \u00b5k(D \u2212W1AW2) and \u00b5k \u2192 \u00b5k+1.\nuntil convergence\nand the augmented Lagrangian is\nmin E:L(E)=b\n\u2016D\u2016\u2217 + \u03b1\u2016E\u20162F + tr(\u039bT1 (A\u0304\u2212A\u2212 E)) +\ntr(\u039bT2 ( D \u2212W1AW2)) + \u00b5 2 \u2016A\u0304\u2212A\u2212 E\u20162F + \u00b5 2 \u2016D \u2212W1AW2\u20162F (18)\nWe again follow an ALM strategy, optimizing over D,E,A separately followed by updates of \u039b1,\u039b2 and \u00b5. Note that (Deng et al., 2012) considered a strategy for minimizing re-weighted nuclear norms for matrix completion, but instead of using exact minimization over A, they took a step in the gradient direction. We derive the exact update, which turns out to be very efficient via a Sylvester equation formulation. The updates over D and over E look similar to the unreweighted case. Taking a derivative with respect to A we obtain a linear system of equations in an unusual form: \u2212\u039b1 \u2212Wy\u039b2WZ \u2212 \u00b5(A\u0304 \u2212 A \u2212 E) \u2212 \u00b5W1(D \u2212 W1AW2)W2 = 0. Rewriting it, we obtain:\nA+W 21AW 2 2 =\n1\n\u00b5k (\u039b1 +W1\u039b2W2)+(A\u0304\u2212E)+W1DW2\n(19) we can see that it is in the form of Sylvester equation arising in discrete Lyapunov systems (Kailath, 1980):\nA+B1AB2 = C (20)\nwhere A is the unknown, and B1, B2, C are coefficient matrices. An efficient solution is described in (Bartels & Stewart, 1972). These ALM steps for reweighted nuclear norm STLS are summarized in Algorithm 1.\nTo obtain the full algorithm for STLS, we combine the above algorithm with steps of re-weighting the nuclear norm and the binary search over \u03b1 as described in Section 2.1. We use it for experiments in Section 5. A faster algorithm that avoids the need for a binary search will be presented in a future publication."}, {"heading": "4. Accuracy analysis for STLS", "text": "In context of matrix completion and robust PCA, the nuclear norm relaxation has strong theoretical accuracy guarantees (Recht et al., 2010; Chandrasekaran\net al., 2011). We now study accuracy guarantees for the STLS problem via the nuclear norm and the reweighted nuclear norm approaches. The analysis is conducted in the plain TLS setting, where the optimal solution is available via the SVD, and it gives valuable insight into the accuracy of our approach for the much harder STLS problem. In particular, we quantify the dramatic benefit of using reweighting. In this section we study a simplification of our STLS algorithm, where we set the regularization parameter \u03b1 once and do not update it through the iterations. The full adaptive approach from Section 2.1 is analyzed in the addendum to this paper where we show that it can in fact recover the exact SVD solution for plain TLS.\nWe first consider the problem min \u2016A\u2212 A\u0304\u20162F such that rank(A) \u2264 N \u2212 1. For the exact solution via the SVD, the minimum approximation error is simply the square of the last singular value ErrSV D = \u2016A\u0302SV D \u2212 A\u0304\u20162F = \u03c32N . The nuclear-norm approximation will have a higher error. We solve min \u2016A\u2212 A\u0304\u20162F + \u03b1\u2016A\u2016\u2217 for the smallest choice of \u03b1 that makes A rank-deficient. A closed form solution for A is the soft-thresholding operation with \u03b1 = \u03c3N . It subtracts \u03b1 from all the singular values, making the error Errnn = N\u03c3 2 N . While it is bounded, this is a substantial increase from the SVD solution. Using the log-det heuristic, we obtain much tighter accuracy guarantees even when we fix \u03b1, and do not update it during re-weighting. Let ai =\n\u03c3i \u03c3N\n, the ratio of the i-th and the smallest singular values. In the appendix using \u2018log-thresholding\u2019 we derive that\nErrrw-nn \u2248 \u03c32N\n( 1 + 1\n2 \u2211 i<N (ai \u2212 \u221a a2i \u2212 1) 2) ) (21)\n\u2264 \u03c32N (1 + 1\n2 \u2211 i<N 1 a2i ).\nFor larger singular values the approximation is much more accurate than for the smallest ones. In contrast, for the plain nuclear norm approach, the errors are equally bad for the largest and smallest singular values. Considering that natural signals (and singular value spectra) often exhibit fast decay (exponential or power-law decay), we can quantify the improvement. Suppose that the singular values have exponential decay, \u03c3i = \u03c3Na\nN\u2212i, with a > 1, or power-law decay \u03c3i = \u03c3N (N \u2212 i + 1)p. The approximation errors are Errexp = \u03c3 2 N ( 1 + 12 \u2211 i<N (a i \u2212 \u221a a2i \u2212 1)2 ) and Errp = \u03c3 2 N ( 1 + 12 \u2211 i<N (i p \u2212 \u221a i2p \u2212 1)2 ) respectively. For exponential decay, if N = 100, and a = 1.1, the approximation error is 1.84 \u03c32N for our approach, and N\u03c32N = 100 \u03c3 2 N , for the nuclear norm relaxation. This is a dramatic improvement in approximation, that strongly supports using the log-det\nheuristic over the nuclear norm for approximating matrix rank!"}, {"heading": "5. Experimental Results", "text": "Our first experiment considers plain TLS, where we know the optimal solution via the SVD. We evaluate the accuracy of the nuclear norm (NN) and two flavors of the reweighted nuclear norm algorithm: the full adaptive one described in Section 2.1, which we will refer to as (RW-NN), and the simplified approach with fixed \u03b1 as described in Section 4 (log-det).\nWe simulate random i.i.d. Gaussian matrices A of size N \u00d7N , use a maximum of 3 re-weightings in RW-NN, and update the ALM parameter \u00b5k as \u00b5k = 1.05\nk. We plot the relative error of NN-TLS with respect to exact TLS via SVD, i.e. the norm of error (w.o. squaring) for NN-TLS divided by the norm of error for TLS. We compare it to the relative error for full RW-NN TLS (again with respect to exact TLS) in Figure 1 (a). The results are averaged over 100 independent trials. The NN solution, as we expect, is a factor of \u221a N worse than TLS in Frobenius norm. The full RW-NN always recovers the optimal TLS solution, i.e. the relative error is exactly 1, as we establish in the addendum. The simplified non-adaptive log-det STLS in Figure 1 (b) is almost as good as the adaptive: the average error is only about 1% higher than exact TLS, dramatically better than \u221a N for plain NN. These empirical results agree with our theoretical analysis in Section 4.\nNext, we compare NN and RW-NN for a structured TLS problem with a pattern of entries in E fixed at 0 (entries are fixed independently with probability 0.5). This is a practically important case where the entries of E fixed at zero represent exact measurements while allowing other entries to have noisy measurements. The solution of plain TLS via SVD ignores the constraints and is infeasible for this structured problem. We still compute the relative error with respect to exact TLS to quantify the increase in error needed to obey the imposed structure. Again, in Figure 2 (a) we can see that the RW-NN solution provides much bet-\nter accuracy than NN, and not far worse than 1, the infeasible lower-bound given by plain TLS.\nNext we consider Toeplitz structured errors, which means that the matrix E is constant on the diagonals:\n e1 e2 e3 ... e0 e1 e2 ... e\u22121 e0 e1 ... ... ... ... ...  (22)\nToeplitz structure arises in time-series modeling, analysis of linear systems, and system identification, as the convolution operation can be represented as a multiplication by a Toeplitz matrix (Kailath, 1980). We simulate the Toeplitz entries at the start of each diagonal as i.i.d. Gaussian. The Toeplitz structure is quite restrictive, with only M + N \u2212 1 degrees of freedom instead of O(MN). However, we reach a similar conclusion as we had before: RW-NN solution provides much better accuracy than NN, much closer to the infeasible lower-bound via plain TLS. We show the results for STLS with Toeplitz structure in Figure 2(b).\nFinally, we compare our re-weighted nuclear norm approach to the latest widely used non-convex solver for STLS, SLRA (Markovsky & Usevich, 2014). The success of non-convex solvers depends heavily on a good initialization. We consider a problem with a blockdiagonal structure where some entries are corrupted by large outliers. The weights on these entries are set to be very small, so an ideal STLS solver should find the solution while minimizing the influence of the outliers. Figure 3 shows that for moderate levels of outliers, both RW-NN and the non-convex SLRA approach find very accurate solutions. However, for larger levels of noise, while RW-NN continues to have good performance, the accuracy of SLRA plummets, presumably due to the difficulty of finding a good initialization. For this setting we know the exact solution without outliers, and we measure accuracy by correlation (i.e. cosine of subspace angles) of the recovered and the exact STLS nullspaces, averaged over 100 trials."}, {"heading": "5.1. Quantification of cellular heterogeneity", "text": "We now demonstrate the utility of STLS for solving a broad and important class of problems arising in biology, namely inferring heterogeneity in biological systems. Most biological systems (such as human cancers, tissues, the human microbiome and other microbial communities) are mixtures of cells in different physiological states or even different cell types. While the primary biomedical interest is in characterizing the different cell types and physiological states, experimental approaches can typically measure only the population average across physiological states. Our aim is to combine these readily available population-average measurements and use STLS to infer the distribution of cells across distinct physiological states.\nWe consider a cell culture containing cells in K distinct physiological states, such as phases of cell growth or division cycles (Slavov et al., 2011; 2012). As the growth rate of the culture changes, the fraction of cells in each physiological state changes. This fractional change is of primary interest but it is often too expensive or even technologically impossible to measure directly. Since the cells cannot be easily separated we consider the general case when we know M indicator genes (such as cyclins) that are either present or absent in K distinct physiological states, S \u2208 RM\u00d7K . Existing methods for high-throughput measurements of mRNA levels, such as DNA microarrays and RNA-seq, can quantify relative changes of mRNA levels across different conditions but cannot accurately quantify the ratios between different mRNAs, i.e., depending on chemical composition and the physical properties, each RNA has its own normalization scaler accounting for biases such as GC (guanine-cytosine) content. To avoid such biases we explicitly scale the measured relative expression levels X \u2208 RM\u00d7N by an unknown positive diagonal matrix Z = diag(z). The goal is to find U \u2208 RK\u00d7N , the fraction of cells across the K physiological states for each of N different conditions, such as different steady-state growth rates. Mathematically the problem is:\nX = ZSU, (23)\nwhere we aim to recover the decomposition up-to scal-\ning knowing X and S only. We now study conditions for identifiability without noise, and extend it to a structured TLS problem in presence of noise.\nLinear Algebraic solution We define \u03bb = [ 1z1 , ..., 1 zM\n], and \u039b = diag(\u03bb). Thus \u039b = Z\u22121. We now have to find \u039b and U :\n\u039bX = SU, (24)\nand both unknowns enter the equations linearly. We transpose both sides and move everything to one side to get: UTST \u2212XT\u039b = 0. Now let us stack columns of UT , i.e. rows of U into a vector, u = vec(UT ). Then vec(UTST ) = (S \u2297 I)u, where \u2297 stands for the Kronecker product. Similarly defining a blockdiagonal matrix blkdiag(XT ), with columns of XT (i.e. rows of X) in diagonal blocks. This way XT\u039b = blkdiag(XT )\u03bb. Combining this together we have:\n[ (S \u2297 I), \u2212 blkdiag(XT ) ] [u \u03bb ] = 0 (25)\nAny vector in the nullspace of A ,[ (S \u2297 I) \u2212 blkdiag(XT ) ] is a solution to this problem. If we have a single vector in the nullspace of A, then we have a unique solution up-to scaling.\nNoisy case: structured Total Least Squares approach When the observation matrix X is corrupted by noise, it is no longer low-rank. The structured matrix A in (25) will only have a trivial null-space. Furthermore, the simple approach of setting the smallest singular value to zero will not work because it ignores the structure of the compound matrix A. The errors in X correspond to errors in the block-diagonal portions of the right part of A. Other entries are known exactly. This is precisely the realm of structured total least squares (STLS) that we explored in Section 2.\nWe will now experimentally apply our reweighted nuclear norm approach for STLS for the cell heterogeneity quantification problem to demonstrate the inference of the fractions of cells in different physiological states. We use experimentally measured levels of 14\ngenes, five expressed in HOC phase, six expressed in LOC phase, and three in both phases, across 6 exponentially growing yeast cultures at different growth rates. The resulting A matrix in (25) is 84\u00d726. Our algorithm infers the fraction of cells in HOC and in LOC phase, up to a scalar factor, in close agreement with expectations from physical measurements in synchronized cultures (Slavov et al., 2011; Slavov & Botstein, 2011). Thus we can extend the observed trend to asynchronous cultures where this fraction is very hard to measure experimentally. Such analysis can empower research on cancer heterogeneity that is a major obstacle to effective cancer therapies. This modest size experiment provides a proof of concept and we are pursuing applications to more complex biological systems."}, {"heading": "6. Appendix: Error analysis for re-weighted STLS", "text": "To gain insight into the re-weighted nuclear norm we consider the diagonal case first, where A = diag(x). The diagonal matrix case penalizing rank of A is equivalent to the vector problem penalizing sparsity of x, so we use the vector notation for simplicity. As both the Frobenius and nuclear norms are unitarily invariant5, the analysis directly extends to the non-diagonal case.\nThe log heuristic for sparsity solves the following problem: min 12\u2016x\u2212y\u2016 2 2+\u03b1 \u2211 i log(\u03b4+|xi|), for a very small \u03b4 > 0. This is a separable problem with a closed form solution for each coordinate6 (contrast this with the\n5 Taking the SVD A = USV T we have \u2016USV \u20162F = \u2016S\u2016 2 F and \u2016USV \u2016\u2217 = \u2016S\u2016\u2217 since U , V are unitary. 6 For \u03b4 small enough, the global minimum is always at 0, but if y > 2 \u221a \u03b1 there is also a local minimum with a large domain of attraction between 0 and y. Iterative linearization methods with small enough step size starting at y will converge to this local minimum.\nsoft-thresholding operation):\nxi =  1 2 ( (yi \u2212 \u03b4) + \u221a (yi \u2212 \u03b4)2 \u2212 4(\u03b1\u2212 yi\u03b4) ) , yi > 2 \u221a \u03b1 1 2 ( (yi + \u03b4)\u2212 \u221a (yi + \u03b4)2 \u2212 4(\u03b1+ yi\u03b4) ) , yi < \u22122 \u221a \u03b1\n0, otherwise\n(26)\nAssuming that \u03b4 is negligible, then we have:\nxi \u2248  1 2 (yi + \u221a y2i \u2212 4\u03b1), if yi > 2 \u221a \u03b1 1 2 (yi \u2212 \u221a y2i \u2212 4\u03b1), if yi < \u22122 \u221a \u03b1\n0, otherwise,\n(27)\nand we chose \u03b1 to annihilate the smallest entry in x, i.e. \u03b1 = 14 mini y 2 i . Sorting the entries in |y| in increasing order, with y0 = ymin, and defining ai = |yi| |y0| , we have ai \u2265 1 and the error in approximating the i-th entry, for i > 0 is\nErri = |xi\u2212yi|2 = y20 2\n( ai \u2212 \u221a a2i \u2212 1 )2 \u2264 y 2 0\n2a2i . (28)\nAlso, by our choice of \u03b1, we have Err0 = y 2 0 for i = 0. The approximation error quickly decreases for larger entries. In contrast, for `1 soft-thresholding, the errors of approximating large entries are as bad as the ones for small entries. This analysis extends directly to the log-det heuristic for relaxing matrix rank."}, {"heading": "7. Conclusions", "text": "We considered a convex relaxation for a very rich class of structured TLS problems, and provided theoretical guarantees. We also developed an efficient firstorder augmented Lagrangian multipliers algorithm for reweighted nuclear norm STLS, which can be applied beyond TLS to matrix completion and robust PCA problems. We applied STLS to quantifying cellular heterogeneity from population average measurements. In future work we will study STLS with sparse and group sparse solutions, and explore connections to robust LS (El Ghaoui & Lebret, 1997)."}], "references": [{"title": "Solution of the matrix equation AX+", "author": ["R.H. Bartels", "G.W. Stewart"], "venue": "XB = C. Communications of the ACM,", "citeRegEx": "Bartels and Stewart,? \\Q1972\\E", "shortCiteRegEx": "Bartels and Stewart", "year": 1972}, {"title": "A global solution for the structured total least squares problem with block circulant matrices", "author": ["A. Beck", "A. Ben-Tal"], "venue": "SIAM Journal on Matrix Analysis and Applic.,", "citeRegEx": "Beck and Ben.Tal,? \\Q2005\\E", "shortCiteRegEx": "Beck and Ben.Tal", "year": 2005}, {"title": "Constrained Optim. and Lagrange Multiplier Methods", "author": ["D.P. Bertsekas"], "venue": null, "citeRegEx": "Bertsekas,? \\Q1982\\E", "shortCiteRegEx": "Bertsekas", "year": 1982}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Boyd et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2011}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J. Cai", "E.J. Candes", "Z. Shen"], "venue": "SIAM Journal on Optim.,", "citeRegEx": "Cai et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Cai et al\\.", "year": 1956}, {"title": "Enhancing sparsity by reweighted l1 minimization", "author": ["E.J. Candes", "M.B. Wakin", "S.P. Boyd"], "venue": "J. of Fourier Analysis and Applic.,", "citeRegEx": "Candes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2008}, {"title": "Rank-sparsity incoherence for matrix decomposition", "author": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"], "venue": "SIAM Journal on Optim.,", "citeRegEx": "Chandrasekaran et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chandrasekaran et al\\.", "year": 2011}, {"title": "Low-rank structure learning via log-sum heuristic recovery", "author": ["Y. Deng", "Q. Dai", "R. Liu", "Z. Zhang", "S. Hu"], "venue": "arXiv preprint arXiv:1012.1919,", "citeRegEx": "Deng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2012}, {"title": "Robust solutions to leastsquares problems with uncertain data", "author": ["L. El Ghaoui", "H. Lebret"], "venue": "SIAM J. on Matrix Analysis and Applic.,", "citeRegEx": "Ghaoui and Lebret,? \\Q1997\\E", "shortCiteRegEx": "Ghaoui and Lebret", "year": 1997}, {"title": "A rank minimization heuristic with application to minimum order system approximation", "author": ["M. Fazel", "H. Hindi", "S.P. Boyd"], "venue": "In IEEE American Control Conference,", "citeRegEx": "Fazel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fazel et al\\.", "year": 2001}, {"title": "An analysis of the total least squares problem", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Golub and Loan,? \\Q1980\\E", "shortCiteRegEx": "Golub and Loan", "year": 1980}, {"title": "Weighted `1 minimization for sparse recovery with prior information", "author": ["A. Khajehnejad", "W. Xu", "S. Avestimehr", "B. Hassibi"], "venue": "In IEEE Int. Symposium on Inf. Theory,", "citeRegEx": "Khajehnejad et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Khajehnejad et al\\.", "year": 2009}, {"title": "The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Z. Lin", "M. Chen", "Y. Ma"], "venue": "arXiv preprint arXiv:1009.5055,", "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Robust recovery of subspace structures by low-rank representation", "author": ["G. Liu", "Z. Lin", "S. Yan", "J. Sun", "Y. Yu", "Y. Ma"], "venue": "arXiv preprint arXiv:1010.2955,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Software for weighted structured low-rank approximation", "author": ["I. Markovsky", "K. Usevich"], "venue": "J. Comput. Appl. Math.,", "citeRegEx": "Markovsky and Usevich,? \\Q2014\\E", "shortCiteRegEx": "Markovsky and Usevich", "year": 2014}, {"title": "Overview of total least-squares methods", "author": ["I. Markovsky", "S. Van Huffel"], "venue": "Signal processing,", "citeRegEx": "Markovsky and Huffel,? \\Q2007\\E", "shortCiteRegEx": "Markovsky and Huffel", "year": 2007}, {"title": "Application of structured total least squares for system identification and model reduction", "author": ["I. Markovsky", "J.C. Willems", "S. Van Huffel", "B. De Moor", "R. Pintelon"], "venue": "Automatic Control, IEEE Trans. on,", "citeRegEx": "Markovsky et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Markovsky et al\\.", "year": 2005}, {"title": "Reweighted nuclear norm minimization with application to system identification", "author": ["K. Mohan", "M. Fazel"], "venue": "In American Control Conference,", "citeRegEx": "Mohan and Fazel,? \\Q2010\\E", "shortCiteRegEx": "Mohan and Fazel", "year": 2010}, {"title": "Noisy signal recovery via iterative reweighted l1-minimization", "author": ["D. Needell"], "venue": "In Forty-Third Asilomar Conference on Signals, Systems and Computers,", "citeRegEx": "Needell,? \\Q2009\\E", "shortCiteRegEx": "Needell", "year": 2009}, {"title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization", "author": ["B. Recht", "M. Fazel", "P.A. Parrilo"], "venue": "SIAM Review,", "citeRegEx": "Recht et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2010}, {"title": "Coupling among growth rate response, metabolic cycle, and cell division cycle in yeast", "author": ["N. Slavov", "D. Botstein"], "venue": "Molecular bio. of the cell,", "citeRegEx": "Slavov and Botstein,? \\Q2011\\E", "shortCiteRegEx": "Slavov and Botstein", "year": 2011}, {"title": "Metabolic cycling without cell division cycling in respiring yeast", "author": ["N. Slavov", "J. Macinskas", "A. Caudy", "D. Botstein"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Slavov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Slavov et al\\.", "year": 2011}, {"title": "A conserved cell growth cycle can account for the environmental stress responses of divergent eukaryotes", "author": ["Slavov", "Nikolai", "Airoldi", "Edoardo M", "van Oudenaarden", "Alexander", "Botstein", "David"], "venue": "Molecular Biology of the Cell,", "citeRegEx": "Slavov et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Slavov et al\\.", "year": 1986}, {"title": "Weighted low-rank approximations", "author": ["N. Srebro", "T. Jaakkola"], "venue": "In Int. Conf. Machine Learning (ICML),", "citeRegEx": "Srebro and Jaakkola,? \\Q2003\\E", "shortCiteRegEx": "Srebro and Jaakkola", "year": 2003}, {"title": "SDPT3 \u2013 a Matlab software package for semidefinite programming, version 1.3", "author": ["K.C. Toh", "M.J. Todd", "R.H. T\u00fct\u00fcnc\u00fc"], "venue": "Optim. Method. Softw.,", "citeRegEx": "Toh et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Toh et al\\.", "year": 1999}, {"title": "Weighted and structured sparse total least-squares for perturbed compressive sampling", "author": ["H. Zhu", "G.B. Giannakis", "G. Leus"], "venue": "In IEEE Int. Conf. Acoustics, Speech and Signal Proc.,", "citeRegEx": "Zhu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 16, "context": "block-diagonal, Toeplitz, or Hankel in system identification literature (Markovsky et al., 2005).", "startOffset": 72, "endOffset": 96}, {"referenceID": 25, "context": "by local optimization methods (Markovsky & Usevich, 2014; Zhu et al., 2011; Srebro & Jaakkola, 2003).", "startOffset": 30, "endOffset": 100}, {"referenceID": 9, "context": "Our approach uses the re-weighted nuclear norm relaxation (Fazel et al., 2001) and is highly flexible: it can handle very general linear structure on errors, including arbitrary weights (changing noise for different entries), patterns of observed and unobserved errors, Toeplitz and Hankel structures, and even norms other than the Frobenius norm.", "startOffset": 58, "endOffset": 78}, {"referenceID": 6, "context": "The nuclear norm relaxation has been successfully used for a range of machine learning problems involving rank constraints, including low-rank matrix completion, low-order system approximation, and robust PCA (Cai et al., 2010; Chandrasekaran et al., 2011).", "startOffset": 209, "endOffset": 256}, {"referenceID": 2, "context": "We suggest fast first-order methods based on Augmented Lagrangian multipliers (Bertsekas, 1982) to compute the STLS solution.", "startOffset": 78, "endOffset": 95}, {"referenceID": 9, "context": "A very effective improvement of the nuclear norm comes from re-weighting it (Fazel et al., 2001; Mohan & Fazel, 2010) based on the log-determinant heuristic for rank.", "startOffset": 76, "endOffset": 117}, {"referenceID": 5, "context": "\u2211 i wi|xi| with suitable positive weights wi (Candes et al., 2008) instead of a plain `1-norm.", "startOffset": 45, "endOffset": 66}, {"referenceID": 9, "context": "This iterative approach can be seen as an iterative local linearization of the concave log-penalty for sparsity, \u2211 i log(\u03b4+ |xi|) (Fazel et al., 2001; Candes et al., 2008).", "startOffset": 130, "endOffset": 171}, {"referenceID": 5, "context": "This iterative approach can be seen as an iterative local linearization of the concave log-penalty for sparsity, \u2211 i log(\u03b4+ |xi|) (Fazel et al., 2001; Candes et al., 2008).", "startOffset": 130, "endOffset": 171}, {"referenceID": 18, "context": "In both empirical and emerging theoretical studies(Needell, 2009; Khajehnejad et al., 2009)", "startOffset": 50, "endOffset": 91}, {"referenceID": 11, "context": "In both empirical and emerging theoretical studies(Needell, 2009; Khajehnejad et al., 2009)", "startOffset": 50, "endOffset": 91}, {"referenceID": 9, "context": "has a semi-definite programming (SDP) representation (Fazel et al., 2001).", "startOffset": 53, "endOffset": 73}, {"referenceID": 24, "context": "There are various ways to solve the plain and weighted nuclear norm STLS formulations, including interiorpoint methods (Toh et al., 1999) and iterative thresholding (Cai et al.", "startOffset": 119, "endOffset": 137}, {"referenceID": 2, "context": "In the next section we focus on augmented Lagrangian methods (ALM) (Bertsekas, 1982) which allow fast convergence without using computationally expensive second-order information.", "startOffset": 67, "endOffset": 84}, {"referenceID": 2, "context": "We develop an effective first-order approach for STLS based on the augmented Lagrangian multiplier (ALM) method (Bertsekas, 1982; Lin et al., 2010).", "startOffset": 112, "endOffset": 147}, {"referenceID": 12, "context": "We develop an effective first-order approach for STLS based on the augmented Lagrangian multiplier (ALM) method (Bertsekas, 1982; Lin et al., 2010).", "startOffset": 112, "endOffset": 147}, {"referenceID": 2, "context": "When f and h are both continuously differentiable, if \u03bck is an increasing sequence, the solution converges Q-linearly to the optimal one (Bertsekas, 1982).", "startOffset": 137, "endOffset": 154}, {"referenceID": 12, "context": "The work of (Lin et al., 2010) extended the analysis to allow objective functions involving nuclear-norm terms.", "startOffset": 12, "endOffset": 30}, {"referenceID": 12, "context": "We do not wait for the coordinate descent to converge at each ALM step, but rather update \u039b and \u03bc after a single iteration, following the inexact ALM algorithm in (Lin et al., 2010).", "startOffset": 163, "endOffset": 181}, {"referenceID": 2, "context": "Finally, instead of relaxing the constraint L(E) = b, we keep the constrained form, and follow each step by a projection (Bertsekas, 1982).", "startOffset": 121, "endOffset": 138}, {"referenceID": 13, "context": "There is no known analytic thresholding solution for the weighted nuclear norm, so instead we follow (Liu et al., 2010) to create a new variable D = W1AW2 and add this definition as an additional linear constraint:", "startOffset": 101, "endOffset": 119}, {"referenceID": 3, "context": "3This is closely related to the popular alternating direction of multipliers methods (Boyd et al., 2011).", "startOffset": 85, "endOffset": 104}, {"referenceID": 7, "context": "Note that (Deng et al., 2012) considered a strategy for minimizing re-weighted nuclear norms for matrix completion, but instead of using exact minimization over A, they took a step in the gradient direction.", "startOffset": 10, "endOffset": 29}, {"referenceID": 19, "context": "In context of matrix completion and robust PCA, the nuclear norm relaxation has strong theoretical accuracy guarantees (Recht et al., 2010; Chandrasekaran et al., 2011).", "startOffset": 119, "endOffset": 168}, {"referenceID": 6, "context": "In context of matrix completion and robust PCA, the nuclear norm relaxation has strong theoretical accuracy guarantees (Recht et al., 2010; Chandrasekaran et al., 2011).", "startOffset": 119, "endOffset": 168}, {"referenceID": 21, "context": "We consider a cell culture containing cells in K distinct physiological states, such as phases of cell growth or division cycles (Slavov et al., 2011; 2012).", "startOffset": 129, "endOffset": 156}, {"referenceID": 21, "context": "Our algorithm infers the fraction of cells in HOC and in LOC phase, up to a scalar factor, in close agreement with expectations from physical measurements in synchronized cultures (Slavov et al., 2011; Slavov & Botstein, 2011).", "startOffset": 180, "endOffset": 226}], "year": 2014, "abstractText": "We study the total least squares (TLS) problem that generalizes least squares regression by allowing measurement errors in both dependent and independent variables. TLS is widely used in applied fields including computer vision, system identification and econometrics. The special case when all dependent and independent variables have the same level of uncorrelated Gaussian noise, known as ordinary TLS, can be solved by singular value decomposition (SVD). However, SVD cannot solve many important practical TLS problems with realistic noise structure, such as having varying measurement noise, known structure on the errors, or large outliers requiring robust error-norms. To solve such problems, we develop convex relaxation approaches for a general class of structured TLS (STLS). We show both theoretically and experimentally, that while the plain nuclear norm relaxation incurs large approximation errors for STLS, the re-weighted nuclear norm approach is very effective, and achieves better accuracy on challenging STLS problems than popular non-convex solvers. We describe a fast solution based on augmented Lagrangian formulation, and apply our approach to an important class of biological problems that use population average measurements to infer cell-type and physiological-state specific expression levels that are very hard to measure directly. Proceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).", "creator": "LaTeX with hyperref package"}}}