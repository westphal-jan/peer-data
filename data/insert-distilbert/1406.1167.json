{"id": "1406.1167", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2014", "title": "Learning to Diversify via Weighted Kernels for Classifier Ensemble", "abstract": "typical classifier ensemble generally should combine diverse component quality classifiers. but however, it is difficult to give about a specific definitive connection between diversity measure and ensemble accuracy. considering given a sheer list of available available component classifiers, how to adaptively and diversely ensemble classifiers becomes a relevant big challenge in the literature. in this paper, we argue that diversity, not performing direct diversity on samples but adaptive diversity with ensembles data, is highly correlated corresponding to ensemble accuracy, and we propose a novel technology for classifier ensemble, learning to diversify, program which learns to adaptive adaptively combine classifiers by considering together both accuracy and diversity. research specifically, our approach, learning to diversify scales via weighted kernels ( l2dwk ), performs classifier item combination by optimizing a direct but simple criterion : maximizing ensemble accuracy and adaptive diversity simultaneously respectively by minimizing approximately a convex loss function. given a measure formulation, the diversity is calculated with weighted kernels ( : i. e., the diversity is measured on the component classifiers'outputs which are kernelled and weighted ), and assume the kernel weights are automatically learned. we minimize scale this probability loss function by estimating the kernel weights in conjunction with the classifier weights, and propose a self - training algorithm for conducting this computational convex optimization procedure all iteratively. extensive experiments on a variety of 32 uci classification benchmark datasets show that implementing the proposed approach consistently outperforms state - of - the - art ensembles such as bagging, gradient adaboost, random forests, gasen, regularized selective weighted ensemble, and ensemble invariant pruning via semi - definite programming.", "histories": [["v1", "Wed, 4 Jun 2014 09:16:42 GMT  (80kb)", "http://arxiv.org/abs/1406.1167v1", "Submitted to IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)"]], "COMMENTS": "Submitted to IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI)", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["xu-cheng yin", "chun yang", "hong-wei hao"], "accepted": false, "id": "1406.1167"}, "pdf": {"name": "1406.1167.pdf", "metadata": {"source": "CRF", "title": "Learning to Diversify via Weighted Kernels for Classifier Ensemble", "authors": ["Xu-Cheng Yin", "Hong-Wei Hao"], "emails": ["xuchengyin@ustb.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n11 67\nv1 [\ncs .L\nG ]\n4 J\nun 2\n01 4\n1\nLearning to Diversify via Weighted Kernels for Classifier Ensemble\nXu-Cheng Yin, Member, IEEE, Chun Yang, and Hong-Wei Hao\nAbstract\u2014Classifier ensemble generally should combine diverse component classifiers. However, it is difficult to give a definitive connection between diversity measure and ensemble accuracy. Given a list of available component classifiers, how to adaptively and diversely ensemble classifiers becomes a big challenge in the literature. In this paper, we argue that diversity, not direct diversity on samples but adaptive diversity with data, is highly correlated to ensemble accuracy, and we propose a novel technology for classifier ensemble, learning to diversify, which learns to adaptively combine classifiers by considering both accuracy and diversity. Specifically, our approach, Learning TO Diversify via Weighted Kernels (L2DWK), performs classifier combination by optimizing a direct but simple criterion: maximizing ensemble accuracy and adaptive diversity simultaneously by minimizing a convex loss function. Given a measure formulation, the diversity is calculated with weighted kernels (i.e., the diversity is measured on the component classifiers\u2019 outputs which are kernelled and weighted), and the kernel weights are automatically learned. We minimize this loss function by estimating the kernel weights in conjunction with the classifier weights, and propose a self-training algorithm for conducting this convex optimization procedure iteratively. Extensive experiments on a variety of 32 UCI classification benchmark datasets show that the proposed approach consistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning via Semi-Definite Programming.\nIndex Terms\u2014Classifier ensemble, classifier combination, learning to diversify, diversity-based ensembles, kernel methods.\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "THERE are many famous ensembles (e.g., Bag-ging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications. Some previous researches show that the performance of a classifier ensemble relies on not only the accuracy but also the diversity of base classifiers [6], [10], [11], [12]. Generally speaking, ensemble of diverse classifiers can allow us to get higher accuracy, which is often not achievable by a single model. Consequently, how to diversely combine classifiers plays an important role and becomes a main topic in ensemble classifier. Given a diversity measure formulation, most conventional diversity-based ensemble methods combine classifiers by calculating and evaluating this fixed measure directly, e.g. [13], [14], [15]. Several diversity measures are extensively acknowledged in classifier ensemble, e.g., Disagreement, Q-Statistics, Double Fault, and Kappa [16], [17], [18]. Obviously, the diversity\n\u2022 X.-C. Yin is with the Department of Computer Science and Technology, School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083, China. E-mail: xuchengyin@ustb.edu.cn \u2022 C. Yang is with the Department of Computer Science and Technology, School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing 100083, China. \u2022 H.-W. Hao is with the Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China.\nis useful, and an ensemble should combine diverse component classifiers for improving accuracy; in an extreme case, ensembling all same classifiers has no any improvement. However, how to adaptively use diversity and suitably optimize combination is still unclear and challenging in the literature. Up to now, many research evidences showed that accuracy did not monotonically increase with diversity using various diversity measures, and seemingly verified that there was no clear correlation between diversity and accuracy in ensembles [19], [18], [20]. Some researchers even argued that in practice it was not possible to define and use a diversity measure that is clearly linked to the ensemble accuracy [20]. The point is \u201chow to use the useful diversity\u201d. Currently, this seems like a \u201cfast knot\u201d in ensemble learning. From a different view to conventional diversitybased ensembles, firstly, we argue that the diversity (called as adaptive diversity with data) should be strongly correlated to the accuracy, not directly measured on the training data but adaptively on the latent data distribution. Currently, given data samples, all existing diversity measures are calculated on the validation set without considering data noise and measure space, although there are a few related descriptions. For example, Ruta and Gabrys observed that certain diversity measures (e.g., Q statistics) performed well on artificial data, but was inadequate for realistic datasets [19]. Secondly, we also believe that diversity-based ensembles, i.e., ensembles with both accuracy and (adaptive) diversity, can improve performance of the final classification system.\n2 Consequently, we introduce a new term in classifier ensemble, learning to diversify 1, which should learn to adaptively select and combine a subset of classifiers by considering both accuracy and diversity, given a list of available component classifiers. There are two main essential tasks of this learning to diversify strategy: (1) First and obviously, same as diversitybased ensembles, the finally selected classifiers in an ensemble should be more diverse than the general combination strategy, e.g., averaging; (2) second but more importantly, according to different data under various situations, learning to diversify should adaptively evaluate the diversity with the given formulation, i.e., adaptive diversity with data. Specifically, within a linear classifier ensemble framework, we propose a unified learningto-diversity approach, Learning TO Diversify via Weighted Kernels (L2DWK). For the first task, we propose to learn classifier weights by optimizing a direct but simple criterion: maximizing the accuracy and the diversity simultaneously. Moreover, we formulate this procedure in a convex minimization problem. For the second task, different from the conventional methods where the diversity is directly calculated on the component classifiers\u2019 outputs from the validation samples, we seek adaptive diversity with data in the view of measure space (with kerneling) and data noise (with weighting). The diversity in our approach is adaptively evaluated with a set of weights (called as kernel weights) on the samples\u2019 outputs within a kernel, i.e., the classifiers\u2019 outputs are converted into another space with weighted kernels, and the kernel weights are learned automatically according to the contribution of validation samples. Furthermore, we combine these two tasks into a unified framework with a self-training algorithm. In this algorithm, first, given the kernel and weights (kernel weights), the classifier weights for combination are learned by minimizing a convex loss function, which maximizes accuracy and diversity of the ensemble simultaneously. Then, the kernel weights are automatically updated with a dynamically damped learning trick. This two-step optimization is then repeated until convergence or the maximum number of iterations is exceeded. We argue that this diversity measure via weighted kernels is adaptive and robust for noise data in different situations, and thereby classifier ensemble based on learning to diversify can produce a subset of important and diverse classifiers. To some extent, we hopefully arrive the above point: \u201cto use the useful diversify\u201d with an adaptively learning strategy. Our approach is extensively evaluated and ver-\n1. In information retrieval, given a list of documents or web pages, learning to diversify is to learn to rank and present a diverse set of results by considering both relevance and diversity [21], [22]. Similarly, here given a list of available component classifiers, learning to diversify in classifier ensemble is to learn to select and combine a diverse set of classifiers by considering both accuracy and diversity.\nified on a large set of 32 typical UCI classification datasets, and consistently outperforms state-ofthe-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen [7], Regularized Selective Ensemble [13], and Ensemble Pruning via Semi-Definite Programming [23]. The rest of the paper is organized as follows. Related work is presented in Section 2. Section 3 describes some notations used in this paper. Our proposed method, Learning TO Diversify via Weighted Kernels (L2DWK), is presented in Section 4 in detail. Section 5 shows extensive experimental results on 32 UCI benchmark datasets. Finally, some conclusions are drawn in Section 6."}, {"heading": "2 RELATED WORK", "text": "Classifier ensemble can be mainly divided into two categories. The first one aims at learning multiple classifiers at the feature level, where multiple classifiers are trained and combined in the learning process, e.g., Boosting [2] and Bagging [1]. The second tries to combine classifiers at the output level, where the results of multiple available classifiers are combined to solve the targeted problem, e.g., multiple classifier systems (classifier combination). For example, in most learning application systems, there are a lot of decision experts derived from various homogeneous or heterogeneous resources. Given a list of these learners, one applicable solution is to merge all these decisions, e.g., the merging framework in the Waston DeepQA [24]. In this paper, we focus on the second one. Namely, given multiple classifiers (available or sequently learned, homogeneous or heterogeneous), classifier ensemble is learned by combining intelligently these component classifiers. Our Learning TO Diversify via Weighted Kernels is a proper technique to combine various available classifiers in such general learning systems. From the view of ensemble pruning, Zhou divided related methods into three categories: ordering-based pruning, clustering-based pruning, and optimizationbased pruning approaches [12]. More specifically, optimization-based pruning methods formulate the ensemble pruning problem as an optimization problem that aims to find the subset of available component classifiers which maximizes or minimizes an objective related to the generalization ability of the final ensemble. Our L2DWK ensemble falls into the optimization-based pruning ensemble methods. Main techniques of optimization-based pruning include heuristic optimization [7], mathematical programming (e.g., ensemble pruning via semi-definite programming [23], and selective ensemble under regularization framework with quadratic programming [13]), and probabilistic pruning methods [25]."}, {"heading": "2.1 Diversity-Based Classifier Ensembles", "text": "As described above, diversity should be a necessary condition for high generalization ability of classi-\nX.-C. YIN et al.: LEARNING TO DIVERSIFY VIA WEIGHTED KERNELS FOR CLASSIFIER ENSEMBLE 3\nfier ensemble. However, there exists many challenges in diversity-based classifier ensemble, e.g., diversity measurement, effectiveness analysis, and ensemble optimization. In the literature, most researchers try to solve these issues in two ways. First, researchers want to give proper formulations for measuring ensemble diversity. Actually, several diversity measures have been proposed, where Disagreement [16], Double Fault [26], Kappa [27], and Difficult [6] are common ones. Specifically, Kuncheva and Whitaker compared 10 popular diversity measures (4 pair-wise and 6 non-pair-wise ones) on a variety of benchmark datasets [18]. They found that in most cases, non-pair-wise methods gained a slightly higher accuracy than pair-wise ones. However, non-pair-wise measures have a much higher complexity. In most optimization-based ensemble methods, pair-wise diversity measures are chosen. Second, based on diversity measures, many researchers want to find proper strategies for combining classifiers. Numerous diversity-based ensembles have also been proposed, most of which are related to ensemble pruning. Margineantu et al. proposed a Kappa pruning approach, which aimed at maximizing the pair-wise diversity among the selected component classifiers [28]. Martinez et al. sorted component classifiers by diversity and selected the top 20% \u2212 40% classifiers for combining [15]. Li et al. tried to present a theoretical study on the effect of diversity on the generalization performance of voting in the PAClearning framework for classifier ensemble. Following this analysis, they also proposed the diversity regularized ensemble pruning method [29]. Trawinski et al. used Genetic Algorithm (GA) to search the best classifier subset by a linear combination of accuracy and diversity [30]. Yin et al. proposed a heuristic learning approach with diversity and sparsity to learn classifiers\u2019 weights and combine multiple classifiers [31], [32]. The major problem with the above methods is that when it comes to optimizing some criteria of the selected subset, they all resort to greedy search and may get stuck in a local optima. To get the global optima, several ensemble pruning methods with mathematical programming optimization have been proposed. Zhang et al. [23] proposed a Semi-Definite Programming (SDP) approach, which formulates the ensemble pruning problem as a quadratic integer programming problem and solve it by a semi-definite programming solution technique. In their method, the size of the selected subset after pruning should be known in advance. Li et al. [13] proposed Regularized Selective Ensemble (RSE) approach, which formulates the ensemble pruning problem as a quadratic programming problem, and learns classifier weights that have the optimal accuracydiversity trade-off. By introducing slack variables, the solution of RSE involves computing the weights of validation samples, and it turns out to be effective. However, some researchers argue that it is difficult to give a definitive connection between diversity measure and ensemble accuracy. For example, Ruta and Gabrys [19] studied the relationship between majority voting errors and diversity measures operating on binary classifier outputs, and found that some measures were consistently correlated with majority vote error, but there was no clear correlation between diversity and accuracy. Kuncheva and Whitaker pointed out that it was difficult to give a definitive connection between the diversity measures and the improvement of the accuracy, and the use of diversity measures for enhancing the design of classifier ensembles was still an open question [18]. Saitta analyzed and experimented a numerous diversity measures, and concluded that all these measures have only a very loose relation with the expected accuracy of the classifier [20]. Tang et al. also showed from experiments that the relationship between diversity and margins of an ensemble was not so obvious, e.g., the minimum margin of an ensemble was not monotonically increasing with respect to diversity [33]. Currently, most existing diversity measures are evaluated on the validation set without considering data noise and measure space with given samples. In this paper, by introducing kernel methods, we expand existing diversity measures with weighted kernels, and propose a new diversity-based ensemble method, Learning TO Diversify via Weighted Kernels, which learns to adaptively combine classifiers by considering both accuracy and diversity."}, {"heading": "2.2 Kernel Methods", "text": "Kernel methods [34], [35], [36] approach the problem by mapping the data into a high dimensional feature space, where each coordinate corresponds to one feature of the data items. In that space, a variety of methods can be used to find relations in the data. Since the mapping can be quite general (e.g., not necessarily linear), the relations found in this way are accordingly very general. According to different situations and applications, many kernel functions are proposed, e.g., Linear, Gaussian, Polynomial, Bayesian [37], Wave [38] and Wavelet kernels [39], where Linear, Gaussian and Polynomial kernels are three common ones. Their definitions are as follows. The Linear Kernel is the simplest kernel function. It is given by the inner product \u3008x, y\u3009 of variable pairs plus an optional constant c \u2208 R, kl(x, y) = x T y + c (1) The Gaussian Kernel is an example of the radial basis function kernel (\u03c3 \u2208 R), kg(x, y) = exp(\u2212 (x\u2212 y)2\n2\u03c32 ) (2)\n4 The Polynomial Kernel is a non-stationary kernel,\nkp(x, y) = (x T y + c)d (3)\nwhere c \u2208 R and d \u2208 N. In this paper, we use these three common kernel functions to expand the diversity measures. In recent years, kernel methods are widely used in feature selection. For example, Guyon et al. proposed SVM-RFE (Recursive Feature Elimination) to eliminate features from the full set sequentially, and used it to select genes for cancer classification [40]. Lin and Zhang proposed COSSO (COmponent Selection and Smoothing Operator) to select features in smoothing spline regression by breaking up the regularization term into components on individual dimensions [41]. For the above kernel-based feature selection methods, all possible feature combinations must be explored in order to find the optimal solution. This is usually intractable. Hence, most feature selection methods use heuristics which make it unclear how good the solution is. One alternative solution for this problem is the weighted kernel method, which parameterizes the kernel with a weight on each feature [42]. An example of the weighted kernel with two d-dimension variables x and y is\nk(x, y) = exp(\u2212 d \u2211\ni=1\n\u03b2i\u2016x T i \u2212 yi\u2016 2)\nwhere \u03b2 are the weights and the Gaussian kernel is performed in Gaussian processes [42]. Similarly, some researchers also used SVMs for feature selection [43], [44]. More specifically, Keerthi et al. proposed an efficient algorithm that alternates between learning an SVM and optimizing the feature weights [45]. Varma and Babu proposed a projected gradient method with l1 regularization for multiple kernel learning (with weighted kernels) and reported encouraged results in a variety of benchmark vision and UCI databases [46]. Inspired by feature selection with weighted kernels, considering data noise and measure space for different data under various situations, we introduce learning to diversify via weighted kernels to evaluate the diversity measure, and then select and ensemble available classifiers with both accuracy and diversity."}, {"heading": "3 NOTATIONS", "text": "Let the training dataset be Tr = {x1, x2, ..., xN}, where yi is the output of sample xi, and all the outputs are in C classes {\u03c91, \u03c92, ..., \u03c9C}. The base classifiers H = {h1, h2, ..., hL} of ensemble are trained on the training set, and an output of a base classifier hj on sample xi is hj(xi). Given each base classifier hj , together with its weight wj , we define the vector of classifier weights as w = [w1,w2, ...,wL], where \u2211L\nj=1 wj = 1, wj \u2265 0. In this paper, we focus on the\nlinear combination of classifiers. By taking a weighted vote among the base classifiers and choosing the class label receives the largest weighted vote, the ensemble H classifies sample xi as H(xi). Instead of the original output, the oracle output O of the ensemble is often used for ensemble optimization, which is a N \u00d7 L matrix, and its element is\nOij =\n{\n1 hj(xi) = yi\n\u22121 hj(xi) 6= yi (4)\nThe main notations used in this paper are summarized as follows:\n1) N : number of total samples.\n2) L: number of total base classifiers.\n3) Oij : the oracle output of the base classifier hj on sample xi.\n4) Oj : the oracle output of the base classifier hj on the whole sample set, i.e., Oj = [O1j , O2j , .., ONj ] T .\n5) mi: the margin of sample xi, and\nmi = \u2211\nhj(xi)=yi wj \u2212 \u2211 hj(xi) 6=yi wj = \u2211L j=1 wjOij . (5)\n6) P: the average \u201caccuracy\u201d of classifiers on the training set. P = [P1, P2, ..., PL]\nT , where Pj is the accuracy of the jth classifier. Here, in order to simplify the formula derivation, the accuracy is defined as\nP = 1\nN 1 T N\u00d71O, (6)\ninstead of the traditional one, i.e.,\nP = 1\n2N 1 T N\u00d71(O+ 1N\u00d7L).\n7) div(w): the diversity of ensemble with classifier weights w. If we use the pairwise diversity, div() can be calculated by averaging with\ndiv(w) = wTDw\nD = fD(O T O,1TN\u00d71O)\n(7)\nwhere D is the diversity matrix of base classifiers, and D has a functional relationship fD() with O T O and 1TO. For example, the diversity matrixes with two common pairwise diversity measures (Disagreement [16] and Double Fault [26]) are shown below. The diversity matrix with Disagreement (dis) is\nDdis = 1\n2N (N1L\u00d7L \u2212O\nT O). (8)\nThe diversity matrix with Double Fault (df) is calculated with\nDdf = 1\n4N (1N\u00d7L \u2212O)\nT (1N\u00d7L \u2212O)\n= 1 4N [N1L\u00d7L \u2212 1L\u00d7NO\u2212 (1L\u00d7NO) T +OTO].\n(9)\nX.-C. YIN et al.: LEARNING TO DIVERSIFY VIA WEIGHTED KERNELS FOR CLASSIFIER ENSEMBLE 5"}, {"heading": "4 LEARNING TO DIVERSIFY", "text": "In diversity-based ensembles, the diversity is measured on the classifiers\u2019 outputs. We argue that by introducing kernels, i.e., classifiers\u2019 outputs are kernelled, the diversity measure via kerneling can be adaptive and more useful in a variety of situations for different data. Moreover, we are also sure that by introducing weights, i.e., classifier\u2019s outputs are weighted, the diversity measure via weighting can be more representative for various data with noise and redundancy in diversity-based ensembles. Consequently, we expand the ensemble diversity via weighted kernels, and propose a novel ensemble method, learning to diversity via weighted kernels. We hope that this new method can learn to adaptively select and combine a subset of classifiers by considering both accuracy and (adaptive) diversity."}, {"heading": "4.1 Learning to Diversify via Weighted Kernels", "text": "We first summarize our Learning TO Diversify via Weighted Kernels (L2DWK) approach by giving a direct but simple optimization criterion. Given a response for the truth of the classifiers\u2019 outputs Y = {y1, y2, ..., xN}, classifier weights w, a kernel k for a classifier\u2019output, and the kernel weights \u03b1, we optimize the following criterion,\nminw,\u03b1loss(Y, k\u03b1,w)\u2212 \u03bbdiv(Y, k\u03b1,w), s.t. w 0, 1Tw = 1. (10)\nwhere loss() is the loss function of the classification error, div() is the diversity of the ensemble, and \u03bb is the diversity regularization control parameter. Obviously, this criterion maximizes the accuracy and diversity of the ensemble simultaneously. Thus, learning to diversify is integrated into the original combination problem formulation through the accuracy and diversity loss function with weighted kernels. Selecting component classifiers and measuring the diversity are achieved automatically by optimizing this criterion. We then present analysis and formulations of L2DWK in detail. For classifiers hi and hj , the products of 1TN\u00d71 and O, and O T and O are kernelled as\nk(1TN\u00d71O)j = k(1N\u00d71,Oj);\nk(OTO)i,j = k(Oi,Oj).\nNext, these kernels are weighted, and we have\nk\u03b1(1 T N\u00d71O)j =\n\u2211\nk \u03b1kk(1,Okj),\nk\u03b1(O T O)i,j =\n\u2211 k \u03b1kk(Oki,Okj), (11)\nwhere k = 1, 2, ..., N for data samples. Now if we use the average accuracy P (Equation (6)) to compute the classification (error) loss, we have\nloss(Y,w) = \u22121TN\u00d71Ow.\nWith the weighted kernel k\u03b1, there is\nloss(Y, k\u03b1,w) = \u2212k\u03b1(1 T N\u00d71O)w. (12)\nUsing the pairwise diversity for div() (Equation (7)), we have\ndiv(Y,w) = wT fD(O T O,1TO)w\nEmbedding the weighted kernel k\u03b1, we will get\ndiv(Y, k\u03b1,w) = w T fD(k\u03b1(O T O), k\u03b1(1 T O))w (13)\nConsequently, by combining the above two formulations (Equation (12) and (13)), the criterion (in Equation (10)) changes to\nminw,\u03b1loss(Y,k\u03b1,w)\u2212 \u03bbdiv(Y, k\u03b1,w) = minw,\u03b1 \u2212 k\u03b1(1TO)w \u2212 \u03bbwT fD(k\u03b1(O T O), k\u03b1(1TO))w\n(14)\nIf we use the Disagreement diversity (Equation (8)), the element of Dk\u03b1 will equal\nDdis,k\u03b1 = fDdis(k\u03b1(O T O), k\u03b1(1 T O)) =\n1 2 (1L\u00d7L \u2212 k\u03b1(O T O))\n(15)\nReplacing Equation (15) in Equation (14), the optimization function is converted to\nminw,\u03b1 \u2212 k\u03b1(1 T O)w \u2212 \u03bbwT 1\n2 (1L\u00d7L \u2212 k\u03b1(O\nT O))w\n(16) Similarly, with the Double Fault diversity (Equation (9)), the element of Dk\u03b1 is equal to\nDdf,k\u03b1 = fDdf(k\u03b1(O T O), k\u03b1(1TO)) =\n1 4N [1L\u00d7L \u2212 k\u03b1(1L\u00d7NO)\u2212 k\u03b1(1L\u00d7NO) T + k\u03b1(OTO)]\n(17)\nReplacing Equation (17) in Equation (14), the optimization function changes to\nminw,\u03b1 \u2212 k\u03b1(1TO)w\u2212 \u03bbwT 1 4N [1L\u00d7L \u2212 k\u03b1(1L\u00d7NO) \u2212 k\u03b1(1L\u00d7NO) T + k\u03b1(OTO)]w\n(18)\nAs a result, given a general kernel k (linear, Gaussian or polynomial) and the kernel weights \u03b1, the object optimization (Equation (10)) in our L2DWK approach can be conveniently converted to a convex quadratic programming problem (Equation (16) or (18)). We finally present some remarks about L2DWK. If we use the average accuracy to calculate loss() and the pairwise diversity to calculate div() but without weighted kernels, then our L2DWK model (Equation (10)) changes to\nwopt = argminw \u2212 \u03bbw T Dw \u2212Pw\ns.t. wopt 0, 1 T wopt = 1.\n(19)\nClassifier weights of the ensemble can be optimized by solving Equation (19). We call such convex optimization as Quadratic Programming problem with Diversity (QPD) for classifier ensemble, the similar idea of which can be seen in [13].\n6"}, {"heading": "4.2 Self-Training Algorithm", "text": "Generally, it is difficult to find the solution for the optimization in Equation (14) without known both w (classifier weights) and (\u03b1/k) (kernel weights). However, with known k and \u03b1, the optimization is simplified to a convex quadratic programming problem for learning classifiers\u2019 weights. Consequently, we propose a self-training algorithm (shown in Algorithm I) for L2DWK. There are three important steps in this learning algorithm: initialization, classifier weight calculation, and kernel weight updating, which are sequentially presented in the following subsections in detail. At last, we also empirically analyze the convergence performance of this self-training algorithm."}, {"heading": "4.2.1 Initialization", "text": "In the self-training algorithm, firstly given the training set Tr and base classifiers H for optimization, assign the pairwise diversity D. Then set the max epoch T as a stop constraint, and \u03b1i = 1/N for each sample xi, where N is the number of samples in Tr. In our method, we can use general kernels (k), e.g., linear, polynomial, and Gaussian kernels."}, {"heading": "4.2.2 Classifier Weight Calculation", "text": "In each iteration, given a kernel with known weights \u03b1, we first use Equation (14) and Equation (11) to calculate the base classifier weights vector w. As described above, this optimization can be solved as a typical convex quadratic programming problem. Then samples of training set Tr are classified with w, and the ensemble classification error rate \u01ebt is calculated."}, {"heading": "4.2.3 Kernel Weight Updating", "text": "Updating the kernel weights \u03b1t is a key process in L2DWK. In the self-training algorithm, we assume the kernel weights \u03b1t+1 have a relationship with \u03b1t, and use a dynamically damped trick, i.e., the damped factor \u03b2t \u2208 [0, 1] and \u03b2t \u2264 \u03b2t+1. We set \u03b2t at the t th iteration as\n\u03b2t = 1\nt . (20)\nThen, we use the following equation to update kernel weights for the (t+ 1)th iteration,\n\u03b1t+1 = \u03b2t\u03b1 t\u2217 + (1 \u2212 \u03b2t)\u03b1 t, (21)\nwhere \u03b1t and \u03b1t\u2217 are the original and new kernel weights at the tth iteration respectively. Here, we want the new weight vector \u03b1t\u2217 for kernel weight updating to increase the weights of easily wrong-classified samples. Correspondingly, we design two methods to calculate \u03b1t\u2217 for L2DWK. One method, called L2DWK-Hinge, gets the idea from the hinge loss [47]. The hinge loss is a loss function used to train classifiers in conventional machine\nlearning techniques. It is usually used for \u201cmaximummargin\u201d classification, e.g., Support Vector Machines. L2DWK-Hinge computes \u03b1t\u2217 as\n\u03b1t\u2217i =\n{\n1\nN\u01ebt mi \u2264 0\n0 otherwise , (22)\nwhere mi is the margin of sample xi (Equation (5)), N\u01ebt is the number of samples which are wrongly classified by the ensemble at iteration t. The other method, called L2DWK-Exp, gets the idea from the adaptive re-weighting step in Boosting [2]. In Boosting, a distribution of weights over training samples is adaptively maintained, and base classifiers are created sequentially with each classifier concentrating on instances that are not well learnt by previous ones. Similarly, L2DWK-Exp updates \u03b1t\u2217 by\n\u03b8 = 1 2 ln1\u2212 \u01ebt\u01ebt , \u03b1t+1,\u2217i = \u03b1t\u2217i exp(\u2212\u03b8mi)\nZt+1 ,\n(23)\nwhere mi is the margin of sample xi (Equation (5)), Zt+1 is a normalization factor so that \u03b1\nt+1,\u2217 is still a valid distribution. In addition, for both above methods, as \u2211N\ni=1 \u03b1 1 i =\n1, the sum of vector \u03b1t+1 is \u2211N\ni=1 \u03b1 t+1 = \u2211N i=1(\u03b2t\u03b1 t\u2217 i + (1 \u2212 \u03b2t)\u03b1 t i)\n= \u03b2t \u2211N\ni=1(\u03b1 t\u2217 i ) + (1 \u2212 \u03b2t) \u2211N i=1(\u03b1 t i)\n= \u03b2t + (1\u2212 \u03b2t) \u2211N i=1(\u03b1 t i) = \u03b2t + (1\u2212 \u03b2t)\u03b2t\u22121 + . . . \u220ft p=1(1\u2212 \u03b2p) \u2211N i=1(\u03b1 1 i ) = \u03b2t + (1\u2212 \u03b2t)\u03b2t\u22121 + . . . \u220ft\np=1(1\u2212 \u03b2p)\n= 1\n(24)\nObviously, there are \u2211N i=1 \u03b1 t i = 1 (\u2200t) and \u03b1 t i \u2208 [0, 1] (\u2200 t, i)."}, {"heading": "5 EXPERIMENTS", "text": "In this section, we first extensively compare our L2DWK methods with several state-of-the-art methods on a variety of 32 UCI classification datasets. Then, for parameter selection in L2DWK, we perform some experiments with different values of the diversity control parameter \u03bb and different kernels. Next, we present experiments on ensembles with different regularization components (e.g, accuracy, or/and diversity with or without weighted kernels). Finally, we empirically analyze the relation between accuracy and diversity in ensembles."}, {"heading": "5.1 Experimental Setting and Datasets", "text": "Our L2DWK method is performed with three common kernels, i.e., linear (L2DWK-Linear), Gaussian (L2DWK-Gauss) and Polynomial kernels (L2DWKPoly). We use base classifiers generated from two baseline ensembles, Bagging and Random Forests. Please note that without specification, in the following\nX.-C. YIN et al.: LEARNING TO DIVERSIFY VIA WEIGHTED KERNELS FOR CLASSIFIER ENSEMBLE 7\nAlgorithm I: Self-training algorithm for Learning TO Diversify via Weighted Kernels (L2DWK) Input:\nTr: the training set. |Tr| = N H = {h1, h2, ..., hL}: the base classifier set, |H | = L. D: pairwise diversity. k: a kernel function.\nOutput: w: the classifier weights. Parameters: T : the max iteration. \u03b1t: a 1\u00d7N vector (the kernel weights), and \u03b1ti is the kernel weight of sample xi at the t\nth iteration. \u03b1t\u2217: a 1\u00d7N vector (the new kernel weights for updating) at the tth iteration. \u01ebt: the ensemble error rate at the t\nth iteration. \u03b2t: a parameter that \u03b2t \u2208 [0, 1], and \u03b2t \u2264 \u03b2t+1.\nProcedure: 1: Set \u03b11i = 1/N . 2: For t = 1, 2, ..., T 3: Learn classifier weights w (Equation (14)) with weighted kernels (k and \u03b1t). 4: Calculate the ensemble classification error \u01ebt by w and Tr. 5: Compute new kernel weights \u03b1t\u2217 with \u01ebt (Equation (22) or (23)). 6: Update kernel weights \u03b1t+1 with \u03b1t and \u03b1t\u2217 (Equation (21)). 7: End\nexperiments the Classification and Regression Tree (CART) classifier is used as the base learner in all ensembles (see discussions in Section ??), where CART in Bagging and AdaBoost is derived from the standard Matlab CART function and CART in Random Forests is from a Random Forests toolbox 2. The Disagreement (Equation (8)) is chosen to measure the ensemble diversity. The L2DWK-Hinge (Equation (22)) 3 is used to compute and update kernel weights in L2DWK methods. Moreover, 10-fold cross validation is conducted on each dataset, and the validation set is bootstrapped from the training set. A plenty number of base classifiers (here, 301) 4 are trained in Bagging or Random Forests for all experimental ensembles. To verify the effect of the proposed method compared to a number of other ensembles, 32 typical datasets from UCI machine learning repository [48] are used in our experiments. These datasets are rather challenging with a low accuracy for general classification techniques (e.g., CART). More information for the datasets is presented in Table 1.\n5.2 Experiments with State-of-the-Art Ensembles (Coming soon.)"}, {"heading": "6 CONCLUSIONS", "text": "Classifier ensemble is widely considered as an effective method to improve accuracy of base classifiers, which has a variety of applications in pattern\n2. https://code.google.com/p/randomforest-matlab/. 3. We perform experiments of L2DWK methods with both L2DWK-Hinge (Equation (22)) and L2DWK-Exp (Equation (23)). As the experimental results are very similar, we only present results with L2DWK-Hinge in this paper. 4. The number of base classifiers for Bagging is always default 101, while the number for Random Forests is sometimes default 501. In our experiments, we select 301, a middle value, for both Bagging and Random Forests.\nrecognition, information retrieval and data mining. Generally speaking, ensemble of diverse classifiers should allow us to get higher accuracy. However, there is not a definitive connection between diversity measure and ensemble accuracy. In this paper, we introduce learning to diversify in classifier ensemble, and construct a fairly clear relation between diversity and accuracy. Specifically, within a linear classifier ensemble framework, we propose Learning TO Diversify via Weighted Kernels (L2DWK) which learns classifier weights by maximizing the accuracy and\n8 the diversity simultaneously in a convex minimization problem, where the diversity is measured with a set of weights on the samples\u2019outputs within a kernel. Moreover, we propose a self-training algorithm to adaptively learn classifier weights and kernel weights."}, {"heading": "ACKNOWLEDGMENTS", "text": ""}], "references": [{"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 24, pp. 122\u2013140, 1996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "Experiments with a new Boosting algorithm", "author": ["Y. Freund", "R. Schapire"], "venue": "Proceedings of International Conference on Machine Learning (ICML\u201996), 1996, pp. 148\u2013156.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "A decision-theoretic generalization of on-line learning and an application to Boosting", "author": ["\u2014\u2014"], "venue": "Journal of Computer and System Sciences, vol. 55, no. 1, pp. 119\u2013139, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Stacked generalization", "author": ["D. Wolpert"], "venue": "Neural Networks, vol. 5, no. 2, pp. 241\u2013260, 1992.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1992}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 45, pp. 5\u201332, 2001.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural network ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Trans. Pattern Analysis Machine Intelligence, vol. 12, no. 10, pp. 993\u20131001, 1990.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Ensembling neural networks: Many could be better than all", "author": ["Z.H. Zhou", "J. Wu", "andW. Tang"], "venue": "Artificial Intelligence, vol. 137, pp. 239\u2013263, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient optimization of performance measures by classifier adaptation", "author": ["N. Li", "I.W. Tsang", "Z.-H. Zhou"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 35, no. 6, pp. 1370\u20131382, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Linear dependency modeling for classifier fusion and feature combination", "author": ["A.J. Ma", "P.C. Yuen", "J.-H. Lai"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 35, no. 5, pp. 1135\u20131148, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Generating accurate and diverse members of a neural network ensemble", "author": ["D.W. Opitz", "J.W. Shavlik"], "venue": "Advances in Neural Information Processing Systems (NIPS\u201996). MIT Press, 1996, pp. 535\u2013541.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Statistical mechanics of ensemble learning", "author": ["A. Krogh", "P. Sollich"], "venue": "Physical Review E, vol. 55, no. 1, pp. 811\u2013825, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Ensemble Methods: Foundations and Algorithms", "author": ["Z.-H. Zhou"], "venue": "Boca Raton, FL: Chamman & Hall/CRC,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Selective ensemble under regularization framework", "author": ["N. Li", "Z.H. Zhou"], "venue": "Proceedings of International Workshop on Multiple Classifier Systems, 2009, pp. 293\u2013303.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Pruning in ordered Bagging ensembles", "author": ["G. Mart\u0131\u0301nez-Mu\u00f1oz", "A. Su\u00e1rez"], "venue": "Proceedings of International Conference on Machine learning (ICML\u201906), 2006, pp. 609\u2013616.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "An analysis of ensemble pruning techniques based on ordered aggregation", "author": ["G. Martinez-Munoz", "D. Hernandez-Lobato", "A. Suarez"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 2, pp. 245\u2013259, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "The sources of increased accuracy for two proposed Boosting algorithms", "author": ["D.B. Skalak"], "venue": "Proceeding of American Association for Artificial Intelligence (AAAI\u201996), 1996.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1996}, {"title": "Relationship between combination methods and measures of diversity in combining classifiers", "author": ["C.A. Shipp", "L. Kuncheva"], "venue": "Information Fusion, vol. 3, pp. 135\u2013148, 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Measures of diversity in classifier ensembles", "author": ["L.I. Kuncheva", "C.J. Whitaker"], "venue": "Machine Learning, vol. 51, no. 2, pp. 181\u2013 207, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Analysis of the correlation between majority voting error and the diversity measures in multiple classifier systems", "author": ["D. Ruta", "B. Gabrys"], "venue": "Proceedings of International Symposium on Soft Computing and Intelligent Systems for Industry, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Hypothesis diveristy in ensemble classification", "author": ["L. Saitta"], "venue": "Proceedings of International Symposium ISMIS, 2006, pp. 662\u2013670.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning diverse rankings with multi-armed bandits", "author": ["F. Radlinski", "R. Kleinberg", "T. Joachims"], "venue": "Proceedings of International Conference on Machine Learning (ICML\u201908), 2008, pp. 784\u2013791.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Redundancy, diversity and interdependent document relevance", "author": ["F. Radlinski", "P.N. Bennett", "B. Carterette", "T. Joachims"], "venue": "SIGIR Forum, vol. 43, no. 2, pp. 46\u201352, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Ensemble pruning via semi-definite programming", "author": ["Y. Zhang", "S. Burer", "W.N. Street", "K. Bennett", "E. Parradohern"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 1315\u20131338, 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "A framework for merging and ranking of answers in DeepQA", "author": ["D. Gondek", "A. Lally", "A. Kalyanpur", "J. Murdock", "P. Duboue", "L. Zhang", "Y. Pan", "Z. Qiu", "C. Welty"], "venue": "IBM Journal of Research and Development, vol. 56, no. 3-4, pp. 14:1\u201314:12, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Predictive ensemble pruning by expectation propagation", "author": ["H. Chen", "P. Tino", "X. Yao"], "venue": "IEEE Trans. Knowledge and Data Engineering, vol. 21, no. 7, pp. 999\u20131013, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Design of effective neural network ensembles for image classification processes", "author": ["G. Giacinto", "F. Roli"], "venue": "Image Vision and Computing, vol. 19, no. 9-10, pp. 699\u2013707, 2000.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2000}, {"title": "The Kappa statistic in reliability studies: Use, interpretation, and sample size requirements", "author": ["J. Sim", "C.C. Wright"], "venue": "Physical Therapy, vol. 85, no. 3, pp. 257\u2013268, 2005.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Pruning adaptive Boosting", "author": ["D.D. Margineantu", "T.G. Dietterich"], "venue": "Proceedings of International Conference on Machine Learning (ICML\u201997), 1997, pp. 211\u2013218.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "Diversity regularized ensemble pruning", "author": ["N. Li", "Y. Yu", "Z.-H. Zhou"], "venue": "ECML/PKDD, 2012, pp. 330\u2013345.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "On the combination of accuracy and diversity measures for genetic selection of Bagging fuzzy rule-based multiclassification systems", "author": ["K. Trawinski", "A. Quirin", "O. Cordon"], "venue": "Proceedings of International Conference on Intelligent Systems Design and Applications, 2009, pp. 121\u2013127.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Classifier ensemble using a heuristic learning with sparsity and diversity", "author": ["X.-C. Yin", "K. Huang", "H.-W. Hao", "K. Iqbal", "Z.-B. Wang"], "venue": "Proceedings of International Conference on Neural Information Processing (ICONIP\u201912), 2012, pp. 100\u2013107.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "A novel classifier ensemble method with sparsity and diversity", "author": ["\u2014\u2014"], "venue": "Neurocomputing, 2013, accepted.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "An analysis of diversity measures", "author": ["E.K. Tang", "P.N. Suganthan", "X. Yao"], "venue": "Machine Learning, vol. 65, no. 1, pp. 247\u2013 271, 2006.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to kernel-based learning algorithms", "author": ["K.-R. Muller", "S. Mika", "G. Ratsch", "K. Tsuda", "B. Scholkopf"], "venue": "IEEE Trans. Neural Networks, vol. 12, no. 2, pp. 181\u2013201, 2001.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}, {"title": "Kernel methods: A survey of current techniques", "author": ["C. Campbell"], "venue": "Neurocomputing, vol. 48, no. 1-4, pp. 63\u201384, 2002.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "Kernel methods in machine learning", "author": ["T. Hofmann", "B. Scholkopf", "A.J. Smola"], "venue": "The Ananls of Statistics, vol. 36, no. 3, pp. 1171\u20131220, 2008.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian generalized kernel mixed models", "author": ["Z. Zhang", "G. Dai", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 111\u2013139, 2011.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "The wave kernel signature: A quantum mechanical approach to shape analysis", "author": ["M. Aubry", "U. Schlickewei", "D. Cremers"], "venue": "ICCV Workshops, 2011, pp. 1626\u20131633.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Wavelet kernel support vector machines forecasting techniques: Case study on water-level predictions during typhoons", "author": ["C.-C. Wei"], "venue": "Expert Systems and Applications, vol. 39, no. 5, pp. 5189\u20135199, 2012.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine Learning, vol. 46, no. 1-3, pp. 389\u2013422, 2002.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2002}, {"title": "Component selection and smoothing in multivariate nonparametric regression", "author": ["Y. Lin", "H.H. Zhang"], "venue": "The Annals of Statistics, vol. 34, no. 5, pp. 2272\u20132297, 2006.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "Gaussian Processes in Machine Learning", "author": ["C.E. Rasmussen", "C.K. Williams"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}, {"title": "Adaptive scaling for feature selection in SVMs.", "author": ["Y. Grandvalet", "S. Canu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "Feature selection for nonlinear kernel support vector machines.", "author": ["O.L. Mangasarian", "G. Kou"], "venue": "in ICDM Workshops,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "An efficient method for gradient-based adaptation of hyperparameters in SVM models.", "author": ["S.S. Keerthi", "V. Sindhwani", "O. Chapelle"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2006}, {"title": "More generality in efficient multiple kernel learning", "author": ["M. Varma", "B.R. Babu"], "venue": "Proceedings of International Conference on Machine Learning (ICML\u201909), 2009, pp. 1065\u20131072.  X.-C. YIN et al.: LEARNING TO DIVERSIFY VIA WEIGHTED KERNELS FOR CLASSIFIER ENSEMBLE  9", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Are loss functions all the same?", "author": ["L. Rosasco", "D.V. Caponnetto", "M. Piana", "A. Verri"], "venue": "Neural Computation,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 10, "endOffset": 13}, {"referenceID": 1, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 43, "endOffset": 46}, {"referenceID": 4, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 96, "endOffset": 99}, {"referenceID": 6, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": ", Bagging [1], Boosting [2], [3], Stacking [4], Random Forests [5] and neural network ensembles [6], [7]) and also many recent ensemble methods [8], [9], many of which have been widely applied in numerous realworld applications.", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "Some previous researches show that the performance of a classifier ensemble relies on not only the accuracy but also the diversity of base classifiers [6], [10], [11], [12].", "startOffset": 151, "endOffset": 154}, {"referenceID": 9, "context": "Some previous researches show that the performance of a classifier ensemble relies on not only the accuracy but also the diversity of base classifiers [6], [10], [11], [12].", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Some previous researches show that the performance of a classifier ensemble relies on not only the accuracy but also the diversity of base classifiers [6], [10], [11], [12].", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "Some previous researches show that the performance of a classifier ensemble relies on not only the accuracy but also the diversity of base classifiers [6], [10], [11], [12].", "startOffset": 168, "endOffset": 172}, {"referenceID": 12, "context": "[13], [14], [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13], [14], [15].", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "[13], [14], [15].", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": ", Disagreement, Q-Statistics, Double Fault, and Kappa [16], [17], [18].", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": ", Disagreement, Q-Statistics, Double Fault, and Kappa [16], [17], [18].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": ", Disagreement, Q-Statistics, Double Fault, and Kappa [16], [17], [18].", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "Up to now, many research evidences showed that accuracy did not monotonically increase with diversity using various diversity measures, and seemingly verified that there was no clear correlation between diversity and accuracy in ensembles [19], [18], [20].", "startOffset": 239, "endOffset": 243}, {"referenceID": 17, "context": "Up to now, many research evidences showed that accuracy did not monotonically increase with diversity using various diversity measures, and seemingly verified that there was no clear correlation between diversity and accuracy in ensembles [19], [18], [20].", "startOffset": 245, "endOffset": 249}, {"referenceID": 19, "context": "Up to now, many research evidences showed that accuracy did not monotonically increase with diversity using various diversity measures, and seemingly verified that there was no clear correlation between diversity and accuracy in ensembles [19], [18], [20].", "startOffset": 251, "endOffset": 255}, {"referenceID": 19, "context": "Some researchers even argued that in practice it was not possible to define and use a diversity measure that is clearly linked to the ensemble accuracy [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 18, "context": ", Q statistics) performed well on artificial data, but was inadequate for realistic datasets [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 20, "context": "In information retrieval, given a list of documents or web pages, learning to diversify is to learn to rank and present a diverse set of results by considering both relevance and diversity [21], [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 21, "context": "In information retrieval, given a list of documents or web pages, learning to diversify is to learn to rank and present a diverse set of results by considering both relevance and diversity [21], [22].", "startOffset": 195, "endOffset": 199}, {"referenceID": 6, "context": "ified on a large set of 32 typical UCI classification datasets, and consistently outperforms state-ofthe-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen [7], Regularized Selective Ensemble [13], and Ensemble Pruning via Semi-Definite Programming [23].", "startOffset": 168, "endOffset": 171}, {"referenceID": 12, "context": "ified on a large set of 32 typical UCI classification datasets, and consistently outperforms state-ofthe-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen [7], Regularized Selective Ensemble [13], and Ensemble Pruning via Semi-Definite Programming [23].", "startOffset": 204, "endOffset": 208}, {"referenceID": 22, "context": "ified on a large set of 32 typical UCI classification datasets, and consistently outperforms state-ofthe-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen [7], Regularized Selective Ensemble [13], and Ensemble Pruning via Semi-Definite Programming [23].", "startOffset": 261, "endOffset": 265}, {"referenceID": 1, "context": ", Boosting [2] and Bagging [1].", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": ", Boosting [2] and Bagging [1].", "startOffset": 27, "endOffset": 30}, {"referenceID": 23, "context": ", the merging framework in the Waston DeepQA [24].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "From the view of ensemble pruning, Zhou divided related methods into three categories: ordering-based pruning, clustering-based pruning, and optimizationbased pruning approaches [12].", "startOffset": 178, "endOffset": 182}, {"referenceID": 6, "context": "Main techniques of optimization-based pruning include heuristic optimization [7], mathematical programming (e.", "startOffset": 77, "endOffset": 80}, {"referenceID": 22, "context": ", ensemble pruning via semi-definite programming [23], and selective ensemble under regularization framework with quadratic programming [13]), and probabilistic pruning methods [25].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": ", ensemble pruning via semi-definite programming [23], and selective ensemble under regularization framework with quadratic programming [13]), and probabilistic pruning methods [25].", "startOffset": 136, "endOffset": 140}, {"referenceID": 24, "context": ", ensemble pruning via semi-definite programming [23], and selective ensemble under regularization framework with quadratic programming [13]), and probabilistic pruning methods [25].", "startOffset": 177, "endOffset": 181}, {"referenceID": 15, "context": "Actually, several diversity measures have been proposed, where Disagreement [16], Double Fault [26], Kappa [27], and Difficult [6] are common ones.", "startOffset": 76, "endOffset": 80}, {"referenceID": 25, "context": "Actually, several diversity measures have been proposed, where Disagreement [16], Double Fault [26], Kappa [27], and Difficult [6] are common ones.", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "Actually, several diversity measures have been proposed, where Disagreement [16], Double Fault [26], Kappa [27], and Difficult [6] are common ones.", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "Actually, several diversity measures have been proposed, where Disagreement [16], Double Fault [26], Kappa [27], and Difficult [6] are common ones.", "startOffset": 127, "endOffset": 130}, {"referenceID": 17, "context": "Specifically, Kuncheva and Whitaker compared 10 popular diversity measures (4 pair-wise and 6 non-pair-wise ones) on a variety of benchmark datasets [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 27, "context": "proposed a Kappa pruning approach, which aimed at maximizing the pair-wise diversity among the selected component classifiers [28].", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "sorted component classifiers by diversity and selected the top 20% \u2212 40% classifiers for combining [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 28, "context": "Following this analysis, they also proposed the diversity regularized ensemble pruning method [29].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "used Genetic Algorithm (GA) to search the best classifier subset by a linear combination of accuracy and diversity [30].", "startOffset": 115, "endOffset": 119}, {"referenceID": 30, "context": "proposed a heuristic learning approach with diversity and sparsity to learn classifiers\u2019 weights and combine multiple classifiers [31], [32].", "startOffset": 130, "endOffset": 134}, {"referenceID": 31, "context": "proposed a heuristic learning approach with diversity and sparsity to learn classifiers\u2019 weights and combine multiple classifiers [31], [32].", "startOffset": 136, "endOffset": 140}, {"referenceID": 22, "context": "[23] proposed a Semi-Definite Programming (SDP) approach, which formulates the ensemble pruning problem as a quadratic integer programming problem and solve it by a semi-definite programming solution technique.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] proposed Regularized Selective Ensemble (RSE) approach, which formulates the ensemble pruning problem as a quadratic programming problem, and learns classifier weights that have the optimal accuracydiversity trade-off.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "For example, Ruta and Gabrys [19] studied the relationship between majority voting errors and diversity measures operating on binary classifier outputs, and found that some measures were consistently correlated with majority vote error, but there was no clear correlation between diversity and accuracy.", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "Kuncheva and Whitaker pointed out that it was difficult to give a definitive connection between the diversity measures and the improvement of the accuracy, and the use of diversity measures for enhancing the design of classifier ensembles was still an open question [18].", "startOffset": 266, "endOffset": 270}, {"referenceID": 19, "context": "Saitta analyzed and experimented a numerous diversity measures, and concluded that all these measures have only a very loose relation with the expected accuracy of the classifier [20].", "startOffset": 179, "endOffset": 183}, {"referenceID": 32, "context": ", the minimum margin of an ensemble was not monotonically increasing with respect to diversity [33].", "startOffset": 95, "endOffset": 99}, {"referenceID": 33, "context": "Kernel methods [34], [35], [36] approach the problem by mapping the data into a high dimensional feature space, where each coordinate corresponds to one feature of the data items.", "startOffset": 15, "endOffset": 19}, {"referenceID": 34, "context": "Kernel methods [34], [35], [36] approach the problem by mapping the data into a high dimensional feature space, where each coordinate corresponds to one feature of the data items.", "startOffset": 21, "endOffset": 25}, {"referenceID": 35, "context": "Kernel methods [34], [35], [36] approach the problem by mapping the data into a high dimensional feature space, where each coordinate corresponds to one feature of the data items.", "startOffset": 27, "endOffset": 31}, {"referenceID": 36, "context": ", Linear, Gaussian, Polynomial, Bayesian [37], Wave [38] and Wavelet kernels [39], where Linear, Gaussian and Polynomial kernels are three common ones.", "startOffset": 41, "endOffset": 45}, {"referenceID": 37, "context": ", Linear, Gaussian, Polynomial, Bayesian [37], Wave [38] and Wavelet kernels [39], where Linear, Gaussian and Polynomial kernels are three common ones.", "startOffset": 52, "endOffset": 56}, {"referenceID": 38, "context": ", Linear, Gaussian, Polynomial, Bayesian [37], Wave [38] and Wavelet kernels [39], where Linear, Gaussian and Polynomial kernels are three common ones.", "startOffset": 77, "endOffset": 81}, {"referenceID": 39, "context": "proposed SVM-RFE (Recursive Feature Elimination) to eliminate features from the full set sequentially, and used it to select genes for cancer classification [40].", "startOffset": 157, "endOffset": 161}, {"referenceID": 40, "context": "Lin and Zhang proposed COSSO (COmponent Selection and Smoothing Operator) to select features in smoothing spline regression by breaking up the regularization term into components on individual dimensions [41].", "startOffset": 204, "endOffset": 208}, {"referenceID": 41, "context": "One alternative solution for this problem is the weighted kernel method, which parameterizes the kernel with a weight on each feature [42].", "startOffset": 134, "endOffset": 138}, {"referenceID": 41, "context": "where \u03b2 are the weights and the Gaussian kernel is performed in Gaussian processes [42].", "startOffset": 83, "endOffset": 87}, {"referenceID": 42, "context": "Similarly, some researchers also used SVMs for feature selection [43], [44].", "startOffset": 65, "endOffset": 69}, {"referenceID": 43, "context": "Similarly, some researchers also used SVMs for feature selection [43], [44].", "startOffset": 71, "endOffset": 75}, {"referenceID": 44, "context": "proposed an efficient algorithm that alternates between learning an SVM and optimizing the feature weights [45].", "startOffset": 107, "endOffset": 111}, {"referenceID": 45, "context": "Varma and Babu proposed a projected gradient method with l1 regularization for multiple kernel learning (with weighted kernels) and reported encouraged results in a variety of benchmark vision and UCI databases [46].", "startOffset": 211, "endOffset": 215}, {"referenceID": 15, "context": "For example, the diversity matrixes with two common pairwise diversity measures (Disagreement [16] and Double Fault [26]) are shown below.", "startOffset": 94, "endOffset": 98}, {"referenceID": 25, "context": "For example, the diversity matrixes with two common pairwise diversity measures (Disagreement [16] and Double Fault [26]) are shown below.", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "We call such convex optimization as Quadratic Programming problem with Diversity (QPD) for classifier ensemble, the similar idea of which can be seen in [13].", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": ", the damped factor \u03b2t \u2208 [0, 1] and \u03b2t \u2264 \u03b2t+1.", "startOffset": 25, "endOffset": 31}, {"referenceID": 46, "context": "One method, called L2DWK-Hinge, gets the idea from the hinge loss [47].", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "The other method, called L2DWK-Exp, gets the idea from the adaptive re-weighting step in Boosting [2].", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "Obviously, there are \u2211N i=1 \u03b1 t i = 1 (\u2200t) and \u03b1 t i \u2208 [0, 1] (\u2200 t, i).", "startOffset": 55, "endOffset": 61}, {"referenceID": 0, "context": "\u03b2t: a parameter that \u03b2t \u2208 [0, 1], and \u03b2t \u2264 \u03b2t+1.", "startOffset": 26, "endOffset": 32}], "year": 2014, "abstractText": "Classifier ensemble generally should combine diverse component classifiers. However, it is difficult to give a definitive connection between diversity measure and ensemble accuracy. Given a list of available component classifiers, how to adaptively and diversely ensemble classifiers becomes a big challenge in the literature. In this paper, we argue that diversity, not direct diversity on samples but adaptive diversity with data, is highly correlated to ensemble accuracy, and we propose a novel technology for classifier ensemble, learning to diversify, which learns to adaptively combine classifiers by considering both accuracy and diversity. Specifically, our approach, Learning TO Diversify via Weighted Kernels (L2DWK), performs classifier combination by optimizing a direct but simple criterion: maximizing ensemble accuracy and adaptive diversity simultaneously by minimizing a convex loss function. Given a measure formulation, the diversity is calculated with weighted kernels (i.e., the diversity is measured on the component classifiers\u2019 outputs which are kernelled and weighted), and the kernel weights are automatically learned. We minimize this loss function by estimating the kernel weights in conjunction with the classifier weights, and propose a self-training algorithm for conducting this convex optimization procedure iteratively. Extensive experiments on a variety of 32 UCI classification benchmark datasets show that the proposed approach consistently outperforms state-of-the-art ensembles such as Bagging, AdaBoost, Random Forests, Gasen, Regularized Selective Ensemble, and Ensemble Pruning via Semi-Definite Programming.", "creator": "LaTeX with hyperref package"}}}