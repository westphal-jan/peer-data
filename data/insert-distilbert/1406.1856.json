{"id": "1406.1856", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2014", "title": "A Drifting-Games Analysis for Online Learning and Applications to Boosting", "abstract": "locally we provide a general mechanism to design online learning algorithms based on a minimax analysis within a flexible drifting - games framework. different online learning settings ( hedge, multi - armed bandit problems and online convex block optimization ) are studied by converting investing into various variant kinds of drifting games. the theoretical original minimax analysis for drifting games is then used and generalized by applying a finite series of three relaxations, starting from choosing up a convex surrogate of the 0 - 1 loss stability function. with different choices of surrogates, we not only recover previous existing algorithms, but also propose new algorithms that are totally parameter - free and enjoy other useful properties. here moreover, our drifting - games linear framework naturally allows behind us to study high probability bounds without resorting to any concentration related results, and also a generalized notion of regret that measures how good the algorithm is compared to all but totally the top small fraction of candidates. finally, we translate our very new hedge algorithm into a new naive adaptive boosting algorithm that is computationally faster as shown explicitly in experiments, since it ignores a large number of examples appearing on each round.", "histories": [["v1", "Sat, 7 Jun 2014 03:11:05 GMT  (46kb)", "https://arxiv.org/abs/1406.1856v1", null], ["v2", "Thu, 30 Oct 2014 17:40:59 GMT  (47kb)", "http://arxiv.org/abs/1406.1856v2", "In NIPS2014"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["haipeng luo", "robert e schapire"], "accepted": true, "id": "1406.1856"}, "pdf": {"name": "1406.1856.pdf", "metadata": {"source": "CRF", "title": "A Drifting-Games Analysis for Online Learning and Applications to Boosting", "authors": ["Haipeng Luo", "Robert E. Schapire"], "emails": ["haipengl@cs.princeton.edu", "schapire@cs.princeton.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n18 56\nv2 [\ncs .L\nG ]\n3 0"}, {"heading": "1 Introduction", "text": "In this paper, we study online learning problems within a drifting-games framework, with the aim of developing a general methodology for designing learning algorithms based on a minimax analysis.\nTo solve an online learning problem, it is natural to consider game-theoretically optimal algorithms which find the best solution even in worst-case scenarios. This is possible for some special cases ([7, 1, 3, 21]) but difficult in general. On the other hand, many other efficient algorithms with optimal regret rate (but not exactly minimax optimal) have been proposed for different learning settings (such as the exponential weights algorithm [14, 15], and follow the perturbed leader [18]). However, it is not always clear how to come up with these algorithms. Recent work by Rakhlin et al. [26] built a bridge between these two classes of methods by showing that many existing algorithms can indeed be derived from a minimax analysis followed by a series of relaxations.\nIn this paper, we provide a parallel way to design learning algorithms by first converting online learning problems into variants of drifting games, and then applying a minimax analysis and relaxations. Drifting games [28] (reviewed in Section 2) generalize Freund\u2019s \u201cmajority-vote game\u201d [13] and subsume some well-studied boosting and online learning settings. A nearly minimax optimal algorithm is proposed in [28]. It turns out the connections between drifting games and online learning go far beyond what has been discussed previously. To show that, we consider variants of drifting games that capture different popular online learning problems. We then generalize the minimax analysis in [28] based on one key idea: relax a 0-1 loss function by a convex surrogate. Although\n\u2217R. Schapire is currently at Microsoft Research in New York City.\nthis idea has been applied widely elsewhere in machine learning, we use it here in a new way to obtain a very general methodology for designing and analyzing online learning algorithms. Using this general idea, we not only recover existing algorithms, but also design new ones with special useful properties. A somewhat surprising result is that our new algorithms are totally parameterfree, which is usually not the case for algorithms derived from a minimax analysis. Moreover, a generalized notion of regret (\u01eb-regret, defined in Section 3) that measures how good the algorithm is compared to all but the top \u01eb fraction of candidates arises naturally in our drifting-games framework. Below we summarize our results for a range of learning settings.\nHedge Settings: (Section 3) The Hedge problem [14] investigates how to cleverly bet across a set of actions. We show an algorithmic equivalence between this problem and a simple drifting game (DGv1). We then show how to relax the original minimax analysis step by step to reach a general recipe for designing Hedge algorithms (Algorithm 3). Three examples of appropriate convex surrogates of the 0-1 loss function are then discussed, leading to the well-known exponential weights algorithm and two other new ones, one of which (NormalHedge.DT in Section 3.3) bears some similarities with the NormalHedge algorithm [10] and enjoys a similar \u01eb-regret bound simultaneously for all \u01eb and horizons. However, our regret bounds do not depend on the number of actions, and thus can be applied even when there are infinitely many actions. Our analysis is also arguably simpler and more intuitive than the one in [10] and easy to be generalized to more general settings. Moreover, our algorithm is more computationally efficient since it does not require a numerical searching step as in NormalHedge. Finally, we also derive high probability bounds for the randomized Hedge setting as a simple side product of our framework without using any concentration results.\nMulti-armed Bandit Problems: (Section 4) The multi-armed bandit problem [6] is a classic example for learning with incomplete information where the learner can only obtain feedback for the actions taken. To capture this problem, we study a quite different drifting game (DGv2) where randomness and variance constraints are taken into account. Again the minimax analysis is generalized and the EXP3 algorithm [6] is recovered. Our results could be seen as a preliminary step to answer the open question [2] on exact minimax optimal algorithms for the multi-armed bandit problem.\nOnline Convex Optimization: (Section 4) Based the theory of convex optimization, online convex optimization [31] has been the foundation of modern online learning theory. The corresponding drifting game formulation is a continuous space variant (DGv3). Fortunately, it turns out that all results from the Hedge setting are ready to be used here, recovering the continuous EXP algorithm [12, 17, 24] and also generalizing our new algorithms to this general setting. Besides the usual regret bounds, we also generalize the \u01eb-regret, which, as far as we know, is the first time it has been explicitly studied. Again, we emphasize that our new algorithms are adaptive in \u01eb and the horizon.\nBoosting: (Section 4) Realizing that every Hedge algorithm can be converted into a boosting algorithm ([29]), we propose a new boosting algorithm (NH-Boost.DT) by converting NormalHedge.DT. The adaptivity of NormalHedge.DT is then translated into training error and margin distribution bounds that previous analysis in [29] using nonadaptive algorithms does not show. Moreover, our new boosting algorithm ignores a great many examples on each round, which is an appealing property useful to speeding up the weak learning algorithm. This is confirmed by our experiments.\nRelated work: Our analysis makes use of potential functions. Similar concepts have widely appeared in the literature [8, 5], but unlike our work, they are not related to any minimax analysis and might be hard to interpret. The existence of parameter free Hedge algorithms for unknown number of actions was shown in [11], but no concrete algorithms were given there. Boosting algorithms that ignore some examples on each round were studied in [16], where a heuristic was used to ignore examples with small weights and no theoretical guarantee is provided."}, {"heading": "2 Reviewing Drifting Games", "text": "We consider a simplified version of drifting games similar to the one described in [29, chap. 13] (also called chip games). This game proceeds through T rounds, and is played between a player and an adversary who controls N chips on the real line. The positions of these chips at the end of round t are denoted by st \u2208 RN , with each coordinate st,i corresponding to the position of chip i. Initially, all chips are at position 0 so that s0 = 0. On every round t = 1, . . . , T : the player first chooses a distribution pt over the chips, then the adversary decides the movements of the chips zt so that the\nnew positions are updated as st = st\u22121 + zt. Here, each zt,i has to be picked from a prespecified set B \u2282 R, and more importantly, satisfy the constraint pt \u00b7 zt \u2265 \u03b2 \u2265 0 for some fixed constant \u03b2. At the end of the game, each chip is associated with a nonnegative loss defined by L(sT,i) for some nonincreasing function L mapping from the final position of the chip to R+. The goal of the player is to minimize the chips\u2019 average loss 1N \u2211N i=1 L(sT,i) after T rounds. So intuitively, the player aims to \u201cpush\u201d the chips to the right by assigning appropriate weights on them so that the adversary has to move them to the right by \u03b2 in a weighted average sense on each round. This game captures many learning problems. For instance, binary classification via boosting can be translated into a drifting game by treating each training example as a chip (see [28] for details).\nWe regard a player\u2019s strategy D as a function mapping from the history of the adversary\u2019s decisions to a distribution that the player is going to play with, that is, pt = D(z1:t\u22121) where z1:t\u22121 stands for z1, . . . , zt\u22121. The player\u2019s worst case loss using this algorithm is then denoted by LT (D). The minimax optimal loss of the game is computed by the following expression: minD LT (D) = minp1\u2208\u2206N maxz1\u2208Zp1 \u00b7 \u00b7 \u00b7minpT\u2208\u2206N maxzT\u2208ZpT 1 N \u2211N i=1 L( \u2211T t=1 zt,i), where \u2206N is the N dimensional simplex and Zp = BN \u2229 {z : p \u00b7 z \u2265 \u03b2} is assumed to be compact. A strategy D\u2217 that realizes the minimum in minD LT (D) is called a minimax optimal strategy. A nearly optimal strategy and its analysis is originally given in [28], and a derivation by directly tackling the above minimax expression can be found in [29, chap. 13]. Specifically, a sequence of potential functions of a chip\u2019s position is defined recursively as follows:\n\u03a6T (s) = L(s), \u03a6t\u22121(s) = min w\u2208R+ max z\u2208B\n(\u03a6t(s+ z) + w(z \u2212 \u03b2)). (1)\nLet wt,i be the weight that realizes the minimum in the definition of \u03a6t\u22121(st\u22121,i), that is, wt,i \u2208 argminw maxz(\u03a6t(st\u22121,i + z) + w(z \u2212 \u03b2)). Then the player\u2019s strategy is to set pt,i \u221d wt,i. The key property of this strategy is that it assures that the sum of the potentials over all the chips never increases, connecting the player\u2019s final loss with the potential at time 0 as follows:\n1\nN\nN \u2211\ni=1\nL(sT,i) \u2264 1\nN\nN \u2211\ni=1\n\u03a6T (sT,i) \u2264 1\nN\nN \u2211\ni=1\n\u03a6T\u22121(sT\u22121,i) \u2264 \u00b7 \u00b7 \u00b7 \u2264 1\nN\nN \u2211\ni=1\n\u03a60(s0,i) = \u03a60(0).\n(2) It has been shown in [28] that this upper bound on the loss is optimal in a very strong sense.\nMoreover, in some cases the potential functions have nice closed forms and thus the algorithm can be efficiently implemented. For example, in the boosting setting, B is simply {\u22121,+1}, and one can verify\u03a6t(s) = 1+\u03b2 2 \u03a6t+1(s+1)+ 1\u2212\u03b2 2 \u03a6t+1(s\u22121) and wt,i = 12 (\u03a6t(st\u22121,i \u2212 1)\u2212 \u03a6t(st\u22121,i + 1)). With the loss function L(s) being 1{s \u2264 0}, these can be further simplified and eventually give exactly the boost-by-majority algorithm [13]."}, {"heading": "3 Online Learning as a Drifting Game", "text": "The connection between drifting games and some specific settings of online learning has been noticed before ([28, 23]). We aim to find deeper connections or even an equivalence between variants of drifting games and more general settings of online learning, and provide insights on designing learning algorithms through a minimax analysis. We start with a simple yet classic Hedge setting."}, {"heading": "3.1 Algorithmic Equivalence", "text": "In the Hedge setting [14], a player tries to earn as much as possible (or lose as little as possible) by cleverly spreading a fixed amount of money to bet on a set of actions on each day. Formally, the game proceeds for T rounds, and on each round t = 1, . . . , T : the player chooses a distribution pt over N actions, then the adversary decides the actions\u2019 losses \u2113t (i.e. action i incurs loss \u2113t,i \u2208 [0, 1]) which are revealed to the player. The player suffers a weighted average loss pt \u00b7 \u2113t at the end of this round. The goal of the player is to minimize his \u201cregret\u201d, which is usually defined as the difference between his total loss and the loss of the best action. Here, we consider an even more general notion of regret studied in [20, 19, 10, 11], which we call \u01eb-regret. Suppose the actions are ordered according to their total losses after T rounds (i.e.\n\u2211T t=1 \u2113t,i) from smallest to largest, and let i\u01eb be the index\nInput: A Hedge Algorithm H for t = 1 to T do\nQuery H: pt = H(\u21131:t\u22121). Set: DR(z1:t\u22121) = pt. Receive movements zt from the adversary. Set: \u2113t,i = zt,i \u2212minj zt,j, \u2200i.\nAlgorithm 1: Conversion of a Hedge Algorithm H to a DGv1 Algorithm DR\nInput: A DGv1 Algorithm DR for t = 1 to T do\nQuery DR: pt = DR(z1:t\u22121). Set: H(\u21131:t\u22121) = pt. Receive losses \u2113t from the adversary. Set: zt,i = \u2113t,i \u2212 pt \u00b7 \u2113t, \u2200i.\nAlgorithm 2: Conversion of a DGv1 Algorithm DR to a Hedge Algorithm H\nof the action that is the \u2308N\u01eb\u2309-th element in the sorted list (0 < \u01eb \u2264 1). Now, \u01eb-regret is defined as R\u01ebT (p1:T , \u21131:T ) = \u2211T t=1 pt \u00b7 \u2113t \u2212 \u2211T t=1 \u2113t,i\u01eb . In other words, \u01eb-regret measures the difference between the player\u2019s loss and the loss of the \u2308N\u01eb\u2309-th best action (recovering the usual regret with \u01eb \u2264 1/N ), and sublinear \u01eb-regret implies that the player\u2019s loss is almost as good as all but the top \u01eb fraction of actions. Similarly, R\u01ebT (H) denotes the worst case \u01eb-regret for a specific algorithm H. For convenience, when \u01eb \u2264 0 or \u01eb > 1, we define \u01eb-regret to be \u221e or \u2212\u221e respectively. Next we discuss how Hedge is highly related to drifting games. Consider a variant of drifting games where B = [\u22121, 1], \u03b2 = 0 and L(s) = 1{s \u2264 \u2212R} for some constant R. Additionally, we impose an extra restriction on the adversary: |zt,i \u2212 zt,j | \u2264 1 for all i and j. In other words, the difference between any two chips\u2019 movements is at most 1. We denote this specific variant of drifting games by DGv1 (summarized in Appendix A) and a corresponding algorithm by DR to emphasize the dependence on R. The reductions in Algorithm 1 and 2 and Theorem 1 show that DGv1 and the Hedge problem are algorithmically equivalent (note that both conversions are valid). The proof is straightforward and deferred to Appendix B. By Theorem 1, it is clear that the minimax optimal algorithm for one setting is also minimax optimal for the other under these conversions.\nTheorem 1. DGv1 and the Hedge problem are algorithmically equivalent in the following sense: (1) Algorithm 1 produces a DGv1 algorithm DR satisfying LT (DR) \u2264 i/N where i \u2208 {0, . . . , N} is such that R(i+1)/NT (H) < R \u2264 R i/N T (H).\n(2) Algorithm 2 produces a Hedge algorithm H with R\u01ebT (H) < R for any R such that LT (DR) < \u01eb."}, {"heading": "3.2 Relaxations", "text": "From now on we only focus on the direction of converting a drifting game algorithm into a Hedge algorithm. In order to derive a minimax Hedge algorithm, Theorem 1 tells us it suffices to derive minimax DGv1 algorithms. Exact minimax analysis is usually difficult, and appropriate relaxations seem to be necessary. To make use of the existing analysis for standard drifting games, the first obvious relaxation is to drop the additional restriction in DGv1, that is, |zt,i \u2212 zt,j| \u2264 1 for all i and j. Doing this will lead to the exact setting discussed in [23] where a near optimal strategy is proposed using the recipe in Eq. (1). It turns out that this relaxation is reasonable and does not give too much more power to the adversary. To see this, first recall that results from [23], written in our\nnotation, state that minDR LT (DR) \u2264 12T \u2211\nT\u2212R 2 j=0 ( T+1 j ) , which, by Hoeffding\u2019s inequality, is upper\nbounded by 2 exp ( \u2212 (R+1) 2\n2(T+1)\n)\n. Second, statement (2) in Theorem 1 clearly remains valid if the input\nof Algorithm 2 is a drifting game algorithm for this relaxed version of DGv1. Therefore, by setting \u01eb > 2 exp ( \u2212 (R+1) 2\n2(T+1)\n) and solving for R, we have R\u01ebT (H) \u2264 O ( \u221a T ln(1\u01eb ) ) , which is the known\noptimal regret rate for the Hedge problem, showing that we lose little due to this relaxation.\nHowever, the algorithm proposed in [23] is not computationally efficient since the potential functions \u03a6t(s) do not have closed forms. To get around this, we would want the minimax expression in Eq. (1) to be easily solved, just like the case when B = {\u22121, 1}. It turns out that convexity would allow us to treat B = [\u22121, 1] almost as B = {\u22121, 1}. Specifically, if each \u03a6t(s) is a convex function of s, then due to the fact that the maximum of a convex function is always realized at the boundary of a compact region, we have\nmin w\u2208R+ max z\u2208[\u22121,1] (\u03a6t(s+ z) + wz) = min w\u2208R+ max z\u2208{\u22121,1}\n(\u03a6t(s+ z) + wz) = \u03a6t(s\u2212 1) + \u03a6t(s+ 1)\n2 ,\n(3)\nInput: A convex, nonincreasing, nonnegative function \u03a6T (s). for t = T down to 1 do\nFind a convex function \u03a6t\u22121(s) s.t. \u2200s, \u03a6t(s\u2212 1) + \u03a6t(s+ 1) \u2264 2\u03a6t\u22121(s). Set: s0 = 0. for t = 1 to T do\nSet: H(\u21131:t\u22121) = pt s.t. pt,i \u221d \u03a6t(st\u22121,i \u2212 1)\u2212 \u03a6t(st\u22121,i + 1). Receive losses \u2113t and set st,i = st\u22121,i + \u2113t,i \u2212 pt \u00b7 \u2113t, \u2200i.\nAlgorithm 3: A General Hedge Algorithm H\nwith w = (\u03a6t(s \u2212 1) \u2212 \u03a6t(s + 1))/2 realizing the minimum. Since the 0-1 loss function L(s) is not convex, this motivates us to find a convex surrogate of L(s). Fortunately, relaxing the equality constraints in Eq. (1) does not affect the key property of Eq. (2) as we will show in the proof of Theorem 2. \u201cCompiling out\u201d the input of Algorithm 2, we thus have our general recipe (Algorithm 3) for designing Hedge algorithms with the following regret guarantee.\nTheorem 2. For Algorithm 3, if R and \u01eb are such that \u03a60(0) < \u01eb and \u03a6T (s) \u2265 1{s \u2264 \u2212R} for all s \u2208 R, then R\u01ebT (H) < R.\nProof. It suffices to show that Eq. (2) holds so that the theorem follows by a direct application of statement (2) of Theorem 1. Let wt,i = (\u03a6t(st\u22121,i \u2212 1) \u2212 \u03a6t(st\u22121,i + 1))/2. Then \u2211\ni\u03a6t(st,i) \u2264 \u2211 i (\u03a6t(st\u22121,i + zt,i) + wt,izt,i) since pt,i \u221d wt,i andpt\u00b7zt \u2265 0. On the other hand, by Eq. (3), we have \u03a6t(st\u22121,i + zt,i) +wt,izt,i \u2264 minw\u2208R+ maxz\u2208[\u22121,1] (\u03a6t(st\u22121,i + z) + wz) = 1 2 (\u03a6t(st\u22121,i \u2212 1) + \u03a6t(st\u22121,i + 1)), which is at most \u03a6t\u22121(st\u22121,i) by Algorithm 3. This shows\u2211\ni\u03a6t(st,i) \u2264 \u2211 i\u03a6t\u22121(st\u22121,i) and Eq. (2) follows.\nTheorem 2 tells us that if solving \u03a60(0) < \u01eb for R gives R > R for some value R, then the regret of Algorithm 3 is less than any value that is greater than R, meaning the regret is at most R."}, {"heading": "3.3 Designing Potentials and Algorithms", "text": "Now we are ready to recover existing algorithms and develop new ones by choosing an appropriate potential \u03a6T (s) as Algorithm 3 suggests. We will discuss three different algorithms below, and summarize these examples in Table 1 (see Appendix C).\nExponential Weights (EXP) Algorithm. Exponential loss is an obvious choice for \u03a6T (s) as it has been widely used as the convex surrogate of the 0-1 loss function in the literature. It turns out that this will lead to the well-known exponential weights algorithm [14, 15]. Specifically, we pick \u03a6T (s) to be exp (\u2212\u03b7(s+R)) which exactly upper bounds 1{s \u2264 \u2212R}. To compute \u03a6t(s) for t \u2264 T , we simply let \u03a6t(s \u2212 1) + \u03a6t(s + 1) \u2264 2\u03a6t\u22121(s) hold with equality. Indeed, direct computations show that all \u03a6t(s) share a similar form: \u03a6t(s) = ( e\u03b7+e\u2212\u03b7\n2\n)T\u2212t \u00b7 exp (\u2212\u03b7(s+R)) .\nTherefore, according to Algorithm 3, the player\u2019s strategy is to set\npt,i \u221d \u03a6t(st\u22121,i \u2212 1)\u2212 \u03a6t(st\u22121,i + 1) \u221d exp (\u2212\u03b7st\u22121,i) , which is exactly the same as EXP (note that R becomes irrelevant after normalization). To derive re-\ngret bounds, it suffices to require \u03a60(0) < \u01eb, which is equivalent to R > 1\u03b7\n(\nln(1\u01eb ) + T ln e\u03b7+e\u2212\u03b7 2\n)\n.\nBy Theorem 2 and Hoeffding\u2019s lemma (see [9, Lemma A.1]), we thus know R\u01ebT (H) \u2264 1\u03b7 ln ( 1 \u01eb ) +\nT\u03b7 2 =\n\u221a\n2T ln ( 1 \u01eb )\nwhere the last step is by optimally tuning \u03b7 to be \u221a\n2(ln 1\u01eb )/T . Note that this algorithm is not adaptive in the sense that it requires knowledge of T and \u01eb to set the parameter \u03b7.\nWe have thus recovered the well-known EXP algorithm and given a new analysis using the driftinggames framework. More importantly, as in [26], this derivation may shed light on why this algorithm works and where it comes from, namely, a minimax analysis followed by a series of relaxations, starting from a reasonable surrogate of the 0-1 loss function.\n2-norm Algorithm. We next move on to another simple convex surrogate: \u03a6T (s) = a[s]2\u2212 \u2265 1{s \u2264 \u22121/\u221aa}, where a is some positive constant and [s]\u2212 = min{0, s} represents a truncating operation. The following lemma shows that \u03a6t(s) can also be simply described.\nLemma 1. If a > 0, then \u03a6t(s) = a ( [s]2\u2212 + T \u2212 t ) satisfies \u03a6t(s\u2212 1) + \u03a6t(s+ 1) \u2264 2\u03a6t\u22121(s).\nThus, Algorithm 3 can again be applied. The resulting algorithm is extremely concise:\npt,i \u221d \u03a6t(st\u22121,i \u2212 1)\u2212 \u03a6t(st\u22121,i + 1) \u221d [st\u22121,i \u2212 1]2\u2212 \u2212 [st\u22121,i + 1]2\u2212. We call this the \u201c2-norm\u201d algorithm since it resembles the p-norm algorithm in the literature when p = 2 (see [9]). The difference is that the p-norm algorithm sets the weights proportional to the derivative of potentials, instead of the difference of them as we are doing here. A somewhat surprising property of this algorithm is that it is totally adaptive and parameter-free (since a disappears under normalization), a property that we usually do not expect to obtain from a minimax analysis. Direct application of Theorem 2 (\u03a60(0) = aT < \u01eb \u21d4 1/ \u221a a > \u221a\nT/\u01eb) shows that its regret achieves the optimal dependence on the horizon T .\nCorollary 1. Algorithm 3 with potential \u03a6t(s) defined in Lemma 1 produces a Hedge algorithm H such that R\u01ebT (H) \u2264 \u221a T/\u01eb simultaneously for all T and \u01eb.\nNormalHedge.DT. The regret for the 2-norm algorithm does not have the optimal dependence on \u01eb. An obvious follow-up question would be whether it is possible to derive an adaptive algorithm that achieves the optimal rate O( \u221a\nT ln(1/\u01eb)) simultaneously for all T and \u01eb using our framework. An even deeper question is: instead of choosing convex surrogates in a seemingly arbitrary way, is there a more natural way to find the right choice of \u03a6T (s)?\nTo answer these questions, we recall that the reason why the 2-norm algorithm can get rid of the dependence on \u01eb is that \u01eb appears merely in the multiplicative constant a that does not play a role after normalization. This motivates us to let \u03a6T (s) in the form of \u01ebF (s) for some F (s). On the other hand, from Theorem 2, we also want \u01ebF (s) to upper bound the 0-1 loss function 1{s \u2264 \u2212 \u221a\ndT ln(1/\u01eb)} for some constant d. Taken together, this is telling us that the right choice of F (s) should be of the form\u0398 ( exp(s2/T ) )\n1. Of course we still need to refine it to satisfy the monotonicity and other properties. We define \u03a6T (s) formally and more generally as:\n\u03a6T (s) = a ( exp ( [s]2 \u2212\ndT\n) \u2212 1 ) \u2265 1 { s \u2264 \u2212 \u221a dT ln (\n1 a + 1\n)\n}\n,\nwhere a and d are some positive constants. This time it is more involved to figure out what other \u03a6t(s) should be. The following lemma addresses this issue (proof deferred to Appendix C).\nLemma 2. If bt = 1\u2212 12 \u2211T \u03c4=t+1 ( exp ( 4 d\u03c4 ) \u2212 1 ) , a > 0, d \u2265 3 and \u03a6t(s) = a ( exp ( [s]2 \u2212 dt ) \u2212 bt ) (define \u03a60(s) \u2261 a(1 \u2212 b0)), then we have \u03a6t(s \u2212 1) + \u03a6t(s + 1) \u2264 2\u03a6t\u22121(s) for all s \u2208 R and t = 2, . . . , T . Moreover, Eq. (2) still holds.\nNote that even if \u03a61(s\u2212 1) + \u03a61(s+ 1) \u2264 2\u03a60(s) is not valid in general, Lemma 2 states that Eq. (2) still holds. Thus Algorithm 3 can indeed still be applied, leading to our new algorithm:\npt,i \u221d \u03a6t(st\u22121,i \u2212 1)\u2212 \u03a6t(st\u22121,i + 1) \u221d exp ( [st\u22121,i\u22121]2\u2212 dt ) \u2212 exp ( [st\u22121,i+1] 2 \u2212 dt ) .\nHere, d seems to be an extra parameter, but in fact, simply setting d = 3 is good enough:\nCorollary 2. Algorithm 3 with potential \u03a6t(s) defined in Lemma 2 and d = 3 produces a Hedge algorithm H such that the following holds simultaneously for all T and \u01eb:\nR\u01ebT (H) \u2264 \u221a 3T ln ( 1 2\u01eb ( e4/3 \u2212 1 ) (lnT + 1) + 1 ) = O ( \u221a T ln (1/\u01eb) + T ln lnT ) .\nWe have thus proposed a parameter-free adaptive algorithm with optimal regret rate (ignoring the ln lnT term) using our drifting-games framework. In fact, our algorithm bears a striking similarity to NormalHedge [10], the first algorithm that has this kind of adaptivity. We thus name our algorithm NormalHedge.DT2. We include NormalHedge in Table 1 for comparison. One can see that the main differences are: 1) On each round NormalHedge performs a numerical search to find out the right parameter used in the exponents; 2) NormalHedge uses the derivative of potentials as weights.\n1Similar potential was also proposed in recent work [22, 25] for a different setting. 2\u201cDT\u201d stands for discrete time.\nCompared to NormalHedge, the regret bound for NormalHedge.DT has no explicit dependence on N , but has a slightly worse dependence on T (indeed ln lnT is almost negligible). We emphasize other advantages of our algorithm over NormalHedge: 1) NormalHedge.DT is more computationally efficient especially when N is very large, since it does not need a numerical search for each round; 2) our analysis is arguably simpler and more intuitive than the one in [10]; 3) as we will discuss in Section 4, NormalHedge.DT can be easily extended to deal with the more general online convex optimization problem where the number of actions is infinitely large, while it is not clear how to do that for NormalHedge by generalizing the analysis in [10]. Indeed, the extra dependence on the number of actionsN for the regret of NormalHedge makes this generalization even seem impossible. Finally, we will later see that NormalHedge.DT outperforms NormalHedge in experiments. Despite the differences, it is worth noting that both algorithms assign zero weight to some actions on each round, an appealing property when N is huge. We will discuss more on this in Section 4."}, {"heading": "3.4 High Probability Bounds", "text": "We now consider a common variant of Hedge: on each round, instead of choosing a distribution pt, the player has to randomly pick a single action it, while the adversary decides the losses \u2113t at the same time (without seeing it). For now we only focus on the player\u2019s regret to the best action: RT (i1:T , \u21131:T ) = \u2211T t=1 \u2113t,it \u2212mini \u2211T t=1 \u2113t,i. Notice that the regret is now a random variable, and we are interested in a bound that holds with high probability. Using Azuma\u2019s inequality, standard analysis (see for instance [9, Lemma 4.1]) shows that the player can simply draw it according to pt = H(\u21131:t\u22121), the output of a standard Hedge algorithm, and suffers regret at most RT (H) + \u221a\nT ln(1/\u03b4) with probability 1 \u2212 \u03b4. Below we recover similar results as a simple side product of our drifting-games analysis without resorting to concentration results, such as Azuma\u2019s inequality.\nFor this, we only need to modify Algorithm 3 by setting zt,i = \u2113t,i \u2212 \u2113t,it . The restriction pt \u00b7 zt \u2265 0 is then relaxed to hold in expectation. Moreover, it is clear that Eq. (2) also still holds in expectation. On the other hand, by definition and the union bound, one can show that \u2211\ni E[L(sT,i)] = \u2211 i Pr [sT,i \u2264 \u2212R] \u2265 Pr [RT (i1:T , \u21131:T ) \u2265 R]. So setting \u03a60(0) = \u03b4 shows that the regret is smaller than R with probability 1\u2212 \u03b4. Therefore, for example, if EXP is used, then the regret would be at most \u221a\n2T ln(N/\u03b4) with probability 1\u2212\u03b4, giving basically the same bound as the standard analysis. One draw back is that EXP would need \u03b4 as a parameter. However, this can again be addressed by NormalHedge.DT for the exact same reason that NormalHedge.DT is independent of \u01eb. We have thus derived high probability bounds without using any concentration inequalities."}, {"heading": "4 Generalizations and Applications", "text": "Multi-armed Bandit (MAB) Problem: The only difference between Hedge (randomized version) and the non-stochastic MAB problem [6] is that on each round, after picking it, the player only sees the loss for this single action \u2113t,it instead of the whole vector \u2113t. The goal is still to compete with the best action. A common technique used in the bandit setting is to build an unbiased estimator \u2113\u0302t for the losses, which in this case could be \u2113\u0302t,i = 1{i = it}\u00b7\u2113t,it/pt,it . Then algorithms such as EXP can be used by replacing \u2113t with \u2113\u0302t, leading to the EXP3 algorithm [6] with regret O( \u221a TN lnN).\nOne might expect that Algorithm 3 would also work well by replacing \u2113t with \u2113\u0302t. However, doing so breaks an important property of the movements zt,i: boundedness. Indeed, Eq. (3) no longer makes sense if z could be infinitely large, even if in expectation it is still in [\u22121, 1] (note that zt,i is now a random variable). It turns out that we can address this issue by imposing a variance constraint on zt,i. Formally, we consider a variant of drifting games where on each round, the adversary picks a random movement zt,i for each chip such that: zt,i \u2265 \u22121,Et[zt,i] \u2264 1,Et[z2t,i] \u2264 1/pt,i and Et[pt \u00b7 zt] \u2265 0. We call this variant DGv2 and summarize it in Appendix A. The standard minimax analysis and the derivation of potential functions need to be modified in a certain way for DGv2, as stated in Theorem 4 (Appendix D). Using the analysis for DGv2, we propose a general recipe for designing MAB algorithms in a similar way as for Hedge and also recover EXP3 (see Algorithm 4 and Theorem 5 in Appendix D). Unfortunately so far we do not know other appropriate potentials due to some technical difficulties. We conjecture, however, that there is a potential function that could recover the poly-INF algorithm [4, 5] or give its variants that achieve the optimal regret O( \u221a TN).\nOnline Convex Optimization: We next consider a general online convex optimization setting [31]. Let S \u2282 Rd be a compact convex set, and F be a set of convex functions with range [0, 1] on S. On each round t, the learner chooses a point xt \u2208 S, and the adversary chooses a loss function ft \u2208 F (knowing xt). The learner then suffers loss ft(xt). The regret after T rounds is RT (x1:T , f1:T ) = \u2211T\nt=1 ft(xt) \u2212 minx\u2208S \u2211T\nt=1 ft(x). There are two general approaches to OCO: one builds on convex optimization theory [30], and the other generalizes EXP to a continuous space [12, 24]. We will see how the drifting-games framework can recover the latter method and also leads to new ones.\nTo do so, we introduce a continuous variant of drifting games (DGv3, see Appendix A). There are now infinitely many chips, one for each point in S. On round t, the player needs to choose a distribution over the chips, that is, a probability density function pt(x) on S. Then the adversary decides the movements for each chip, that is, a function zt(x) with range [\u22121, 1] on S (not necessarily convex or continuous), subject to a constraint Ex\u223cpt [zt(x)] \u2265 0. At the end, each point x is associated with a loss L(x) = 1{\u2211t zt(x) \u2264 \u2212R}, and the player aims to minimize the total loss \u222b x\u2208S L(x)dx.\nOCO can be converted into DGv3 by setting zt(x) = ft(x)\u2212ft(xt) and predictingxt = Ex\u223cpt [x] \u2208 S. The constraint Ex\u223cpt [zt(x)] \u2265 0 holds by the convexity of ft. Moreover, it turns out that the minimax analysis and potentials for DGv1 can readily be used here, and the notion of \u01eb-regret, now generalized to the OCO setting, measures the difference of the player\u2019s loss and the loss of a best fixed point in a subset of S that excludes the top \u01eb fraction of points. With different potentials, we obtain versions of each of the three algorithms of Section 3 generalized to this setting, with the same \u01eb-regret bounds as before. Again, two of these methods are adaptive and parameter-free. To derive bounds for the usual regret, at first glance it seems that we have to set \u01eb to be close to zero, leading to a meaningless bound. Nevertheless, this is addressed by Theorem 6 using similar techniques in [17], giving the usual O( \u221a dT lnT ) regret bound. All details can be found in Appendix E.\nApplications to Boosting: There is a deep and well-known connection between Hedge and boosting [14, 29]. In principle, every Hedge algorithm can be converted into a boosting algorithm; for instance, this is how AdaBoost was derived from EXP. In the same way, NormalHedge.DT can be converted into a new boosting algorithm that we call NH-Boost.DT. See Appendix F for details and further background on boosting. The main idea is to treat each training example as an \u201caction\u201d, and to rely on the Hedge algorithm to compute distributions over these examples which are used to train the weak hypotheses. Typically, it is assumed that each of these has \u201cedge\u201d \u03b3, meaning its accuracy on the training distribution is at least 1/2 + \u03b3. The final hypothesis is a simple majority vote of the weak hypotheses. To understand the prediction accuracy of a boosting algorithm, we often study the training error rate and also the distribution of margins, a well-established measure of confidence (see Appendix F for formal definitions). Thanks to the adaptivity of NormalHedge.DT, we can derive bounds on both the training error and the distribution of margins after any number of rounds:\nTheorem 3. After T rounds, the training error of NH-Boost.DT is of order O\u0303(exp(\u2212 13T\u03b32)), and the fraction of training examples with margin at most \u03b8(\u2264 2\u03b3) is of order O\u0303(exp(\u2212 13T (\u03b8\u2212 2\u03b3)2)).\nThus, the training error decreases at roughly the same rate as AdaBoost. In addition, this theorem implies that the fraction of examples with margin smaller than 2\u03b3 eventually goes to zero as T gets large, which means NH-Boost.DT converges to the optimal margin 2\u03b3; this is known not to be true for AdaBoost (see [29]). Also, like AdaBoost, NH-Boost.DT is an adaptive boosting algorithm that does not require \u03b3 or T as a parameter. However, unlike AdaBoost, NH-Boost.DT has the striking property that it completely ignores many examples on each round (by assigning zero weight), which is very helpful for the weak learning algorithm in terms of computational efficiency. To test this, we conducted experiments to compare the efficiency of AdaBoost, \u201cNH-Boost\u201d (an analogous boosting algorithm derived from NormalHedge) and NH-Boost.DT. All details are in Appendix G. Here we only briefly summarize the results. While the three algorithms have similar performance in terms of training and test error, NH-Boost.DT is always the fastest one in terms of running time for the same number of rounds. Moreover, the average faction of examples with zero weight is significantly higher for NH-Boost.DT than for NH-Boost (see Table 3). On one hand, this explains why NHBoost.DT is faster (besides the reason that it does not require a numerical step). On the other hand, this also implies that NH-Boost.DT tends to achieve larger margins, since zero weight is assigned to examples with large margin. This is also confirmed by our experiments.\nAcknowledgements. Support for this research was provided by NSF Grant #1016029. The authors thank Yoav Freund for helpful discussions and the anonymous reviewers for their comments."}, {"heading": "A Summary of Drifting Game Variants", "text": "We study three different variants of drifting games throughout the paper, which corresponds to the Hedge setting, the multi-armed bandit problem and online convex optimization respectively. The protocols of these variants are summarized below.\nDGv1\nGiven: a loss function L(s) = 1{s \u2264 \u2212R}. For t = 1, . . . , T :\n1. The player chooses a distribution pt over N chips.\n2. The adversary decides the movement of each chip zt,i \u2208 [\u22121, 1] subject to pt \u00b7 zt \u2265 0 and |zt,i \u2212 zt,j | \u2264 1 for all i and j.\nThe player suffers loss \u2211N i=1 L( \u2211T t=1 zt,i).\nDGv2\nGiven: a loss function L(s) = 1{s \u2264 \u2212R}. For t = 1, . . . , T :\n1. The player chooses a distribution pt over N chips.\n2. The adversary randomly decides the movement of each chip zt,i \u2265 \u22121 subject to Et[zt,i] \u2264 1,Et[z2t,i] \u2264 1/pt,i and Et[pt \u00b7 zt] \u2265 0.\nThe player suffers loss \u2211N i=1 L( \u2211T t=1 zt,i).\nDGv3\nGiven: a compact convex set S, a loss function L(s) = 1{s \u2264 \u2212R}. For t = 1, . . . , T :\n1. The player chooses a density function pt(x) on S.\n2. The adversary decides a function zt(x) : S \u2192 [\u22121, 1] subject to Ex\u223cpt [zt(x)] \u2265 0.\nThe player suffers loss \u222b x\u2208S L( \u2211T t=1 zt(x))dx."}, {"heading": "B Proof of Theorem 1", "text": "Proof. We first show that both conversions are valid. In Algorithm 1, it is clear that \u2113t,i \u2265 0. Also, \u2113t,i \u2264 1 is guaranteed due to the extra restriction of DGv1. For Algorithm 2, zt,i lies in B = [\u22121, 1] since \u2113t,i \u2208 [0, 1], and direct computation showspt\u00b7zt = 0 \u2265 \u03b2(= 0) and |zt,i\u2212zt,j| = |\u2113t,i\u2212\u2113t,j| \u2264 1 for all i and j.\n(1) For any choices of zt, we have N \u2211\ni=1\nL(sT,i) =\nN \u2211\ni=1\nL\n(\nN \u2211\nt=1\nzt,i\n)\n\u2264 N \u2211\ni=1\nL\n(\nN \u2211\nt=1\n(zt,i \u2212 pt \u00b7 zt) ) ,\nwhere the inequality holds since pt \u00b7 zt is required to be nonnegative and L is a nonincreasing function. By Algorithm 1, zt,i \u2212 pt \u00b7 zt is equal to \u2113t,i \u2212 pt \u00b7 \u2113t, leading to\nN \u2211\ni=1\nL(sT,i) \u2264 N \u2211\ni=1\nL\n(\nN \u2211\nt=1\n(\u2113t,i \u2212 pt \u00b7 \u2113t) ) = N \u2211\ni=1\n1\n{\nR \u2264 N \u2211\nt=1\n(pt \u00b7 \u2113t \u2212 \u2113t,i) } .\nSince R(i+1)/NT (H) < R \u2264 R i/N T (H), we must have \u2211N t=1 (pt \u00b7 \u2113t \u2212 \u2113t,j) < R except for the best i actions, which means \u2211N\ni=1 L(sT,i) \u2264 i. This holds for any choices of zt, so LT (DR) \u2264 i/N . (2) By Algorithm 2 and the condition LT (DR) < \u01eb , we have\n1\nN\nN \u2211\ni=1\n1\n{\nR \u2264 N \u2211\nt=1\n(pt \u00b7 \u2113t \u2212 \u2113t,i) } = 1\nN\nN \u2211\ni=1\nL(sT,i) \u2264 LT (DR) < \u01eb,\nwhich means there are at most \u2308N\u01eb\u2309 \u2212 1 actions satisfying R \u2264 \u2211N t=1 (pt \u00b7 \u2113t \u2212 \u2113t,i), and thus \u2211N\nt=1 (pt \u00b7 \u2113t \u2212 \u2113t,i\u01eb) < R. Since this holds for any choices of \u2113t, we have R\u01ebT (H) < R."}, {"heading": "C Summary of Hedge Algorithms and Proofs of Lemma 1, Lemma 2 and", "text": "Corollary 2\nProof of Lemma 1. It suffices to show [s \u2212 1]2\u2212 + [s + 1]2\u2212 \u2264 2[s]2\u2212 + 2. When s \u2265 0, LHS = [s\u2212 1]2\u2212 \u2264 1 < 2 = RHS. When s < 0, LHS \u2264 (s\u2212 1)2 + (s+ 1)2 = 2s2 + 2 = RHS.\nProof of Lemma 2. Let F (s) = exp ( [s\u22121]2 \u2212\ndt\n) +exp ( [s+1]2 \u2212\ndt\n) \u22122 exp ( [s]2 \u2212\nd(t\u22121)\n)\n. It suffices to show\nF (s) \u2264 2(bt \u2212 bt\u22121) = exp ( 4\ndt\n)\n\u2212 1,\nwhich is clearly true for the following 3 cases:\nF (s) =\n\n  \n  \n0 if s > 1; exp (\n(s\u22121)2 dt\n)\n\u2212 1 < exp ( 1 dt ) \u2212 1 if 0 < s \u2264 1; exp (\n(s\u22121)2 dt\n) + 1\u2212 2 exp ( s2\nd(t\u22121)\n)\n< exp ( 4 dt ) \u2212 1 if \u22121 < s \u2264 0.\nFor the last case s \u2264 \u22121, if we can show that F (s) is increasing in this region, then the lemma follows. Below, we show this by proving F \u2032(s) is nonnegative when s \u2264 \u22121.\nLet h(s, c) = \u2202 exp(s2/c)\n\u2202s = 2s c exp\n(\ns2\nc\n)\n. F \u2032(s) can now be written as\nF \u2032(s) = h(s\u2212 1, c) + h(s+ 1, c)\u2212 2h(s, c) + 2(h(s, c)\u2212 h(s, c\u2032)), where c = dt and c\u2032 = d(t \u2212 1). Next we apply (one-dimensional) Taylor expansion to h(s\u2212 1, c) and h(s+ 1, c) around s, and h(s, c\u2032) around c, leading to\nF \u2032(s) = \u221e \u2211\nk=1\n(\u22121)k k! \u2202kh(s, c) \u2202sk +\n\u221e \u2211\nk=1\n1\nk!\n\u2202kh(s, c)\n\u2202sk \u2212 2\n\u221e \u2211\nk=1\n(c\u2032 \u2212 c)k k! \u2202kh(s, c) \u2202ck\n= 2\n\u221e \u2211\nk=1\n(\n1\n(2k)!\n\u22022kh(s, c) \u2202s2k \u2212 (\u2212d) k k! \u2202kh(s, c) \u2202ck\n)\n.\nDirect computation (see Lemma 3 below) shows that \u2202 kh(s,c) \u2202ck and \u2202 2kh(s,c) \u2202s2k\nshare exact same forms only with different constants:\n\u2202kh(s, c)\n\u2202ck = exp\n(\ns2\nc\n) k \u2211\nj=0\n(\u22121)k\u03b1k,j \u00b7 s2j+1\nck+j+1 ,\n\u22022kh(s, c)\n\u2202s2k = exp\n(\ns2\nc\n) k \u2211\nj=0\n\u03b2k,j \u00b7 s2j+1\nck+j+1 ,\n(4)\nwhere \u03b1k,j and \u03b2k,j are recursively defined as:\n\u03b1k+1,j = \u03b1k,j\u22121 + (k + j + 1)\u03b1k,j , \u03b2k+1,j = 4\u03b2k,j\u22121 + (8j + 6)\u03b2k,j + (2j + 3)(2j + 2)\u03b2k,j+1, (5)\nwith initial values \u03b10,0 = \u03b20,0 = 2 (when j 6\u2208 {0, . . . , k}, \u03b1k,j and \u03b2k,j are all defined to be 0). Therefore, F \u2032(s) can be further simplified as\nF \u2032(s) = 2 exp\n(\ns2\nc\n) \u221e \u2211\nk=1\nk \u2211\nj=0\ns2j+1\nck+j+1\n(\n\u03b2k,j (2k)! \u2212 d k\u03b1k,j k!\n)\n.\nSince s is negative, it suffices to show that \u03b2k,j(2k)! \u2264 dk\u03b1k,j\nk! holds for all k and j, which turns out to be true as long as d \u2265 3, as shown by induction in the technical lemma 4 below. To sum up, \u03a6t(s\u2212 1) + \u03a6t(s+ 1) \u2264 2\u03a6t\u22121(s) for all s \u2208 R and t = 2, . . . , T . Finally, we need to show that Eq. (2) still holds. The inequality we just proved above implies \u2211\ni\u03a6t(st,i) \u2264 \u2211\ni \u03a6t\u22121(st\u22121,i) for t = 2, . . . , T , as shown in Theorem 2. Thus the only thing we need to show here is the case when t = 1. Note that \u03a61(s \u2212 1) + \u03a61(s + 1) \u2264 2\u03a60(s) does not hold for all s apparently. However, in order to prove \u2211\ni \u03a61(s1,i) \u2264 \u2211\ni \u03a60(s0,i), we in fact only need a much weaker statement: \u03a61(\u22121) + \u03a61(1) \u2264 2\u03a60(0) since s0,i \u2261 0. This is equivalent to exp (1/d)\u2212 1 \u2264 exp (4/d)\u2212 1, which is true trivially.\nLemma 3. Let h(s, c) = 2sc exp ( s2 c ) . The partial derivatives of h(s, c) satisfy Eq. (4) and (5).\nProof. The base case holds trivially. Assume Eq. (4) holds for a fixed k. Then we have\n\u2202k+1h(s, c)\n\u2202ck+1 = exp\n(\ns2\nc\n) k \u2211\nj=0\n(\u22121)k\u03b1k,j \u00b7 ( \u2212s 2 c2 s2j+1 ck+j+1 \u2212 (k + j + 1) s 2j+1 ck+j+2 )\n= exp\n(\ns2\nc\n) k \u2211\nj=0\n(\u22121)k+1\u03b1k,j \u00b7 ( s2(j+1)+1\nc(k+1)+(j+1)+1 + (k + j + 1)\ns2j+1\nc(k+1)+j+1\n)\n= exp\n(\ns2\nc\n) k+1 \u2211\nj=0\n(\u22121)k+1 (\u03b1k,j\u22121 + (k + j + 1)\u03b1k,j) \u00b7 s2j+1\nc(k+1)+j+1\n= exp\n(\ns2\nc\n) k+1 \u2211\nj=0\n(\u22121)k+1\u03b1k+1,j \u00b7 s2j+1\nc(k+1)+j+1 ,\nand\n\u22022(k+1)h(s, c)\n\u2202s2(k+1) = \u2202\n\nexp\n(\ns2\nc\n) k \u2211\nj=0\n\u03b2k,j \u00b7 ( 2s2j+2\nck+j+2 + (2j + 1)\ns2j\nck+j+1\n)\n\n\n/\n\u2202s\n= exp\n(\ns2\nc\n) k \u2211\nj=0\n\u03b2k,j \u00b7 ( 4s2j+3\nck+j+3 + (8j + 6)\ns2j+1\nck+j+2 + (2j + 1)2j\ns2j\u22121\nck+j+1\n)\n= exp\n(\ns2\nc\n) k+1 \u2211\nj=0\n(4\u03b2k,j\u22121 + (8j + 6)\u03b2k,j + (2j + 3)(2j + 2)\u03b2k,j+1) \u00b7 s2j+1\nck+j+2\n= exp\n(\ns2\nc\n) k+1 \u2211\nj=0\n\u03b2k+1,j \u00b7 s2j+1\nck+j+2 ,\nconcluding the proof.\nLemma 4. Let \u03b1k,j and \u03b2k,j be defined as in Eq. (5). Then \u03b2k,j (2k)! \u2264 dk\u03b1k,j k! holds for all k \u2265 0 and j \u2208 {0, . . . , k} when d \u2265 3.\nProof. We prove the lemma by induction on k. The base case k = 0 is trivial. Assume \u03b2k,j(2k)! \u2264 dk\u03b1k,j\nk! holds for a fixed k and all j \u2208 {0, . . . , k}, then we have \u2200j,\n\u03b2k+1,j (2k + 2)! = 4\u03b2k,j\u22121 + (8j + 6)\u03b2k,j + (2j + 3)(2j + 2)\u03b2k,j+1 (2k + 2)!\n\u2264 d k (4\u03b1k,j\u22121 + (8j + 6)\u03b1k,j + (2j + 3)(2j + 2)\u03b1k,j+1)\n(2k + 2)(2k + 1)k! .\nWe need to show that the above expression is at most dk+1\u03b1k+1,j/(k + 1)!, which, after arrangements, is equivalent to 2\u03b1k,j\u22121 + (4j + 3)\u03b1k,j + (2j + 3)(j + 1)\u03b1k,j+1 \u2264 d(2k + 1)\u03b1k+1,j . We will prove this by another induction on k. Then the lemma follows.\nThe base case (k = 0) is simplified to 6 \u2264 2d, which is true by our assumption d \u2265 3. Assume the inequality holds for a fixed k, then by the definition of \u03b1k,j , one has\n2\u03b1k+1,j\u22121 + (4j + 3)\u03b1k+1,j + (2j + 3)(j + 1)\u03b1k+1,j+1 = (2\u03b1k,j\u22122 + (4j + 3)\u03b1k,j\u22121 + (2j + 3)(j + 1)\u03b1k,j)+\n(2(k + j)\u03b1k,j\u22121 + (4j + 3)(k + j + 1)\u03b1k,j + (2j + 3)(j + 1)(k + j + 2)\u03b1k,j+1)\n= (2\u03b1k,j\u22122 + (4j \u2212 1)\u03b1k,j\u22121 + (2j + 1)j\u03b1k,j)+ (k + j + 2) (2\u03b1k,j\u22121 + (4j + 3)\u03b1k,j + (2j + 3)(j + 1)\u03b1k,j+1) \u2264 d(2k + 1)(\u03b1k+1,j\u22121 + (k + j + 2)\u03b1k+1,j) (by induction) = d(2k + 1)\u03b1k+2,j \u2264 d(2k + 3)\u03b1k+2,j ,\ncompleting the induction.\nProof of Corollary 2. Recall that \u03a6T (s) \u2265 1 { s \u2264 \u2212 \u221a dT ln ( 1 a + 1 ) } . So by setting \u03a60(0) = a(1\u2212 b0) < \u01eb and applying Theorem 2, we arrive at\nR\u01ebT (H) \u2264 \u221a dT ln (\n1\u2212 b0 \u01eb + 1\n)\n.\nIt suffices to upper bound 1 \u2212 b0, which, by definition, is 12 \u2211T t=1 ( exp ( 4 dt ) \u2212 1 ) . Since ex \u2212 1 \u2264 ec\u22121\nc x for any x \u2208 [0, c], we have\nT \u2211\nt=1\n(\nexp\n(\n4\ndt\n) \u2212 1 ) \u2264 (e4/d \u2212 1) T \u2211\nt=1\n1 t \u2264 (e4/d \u2212 1)(lnT + 1).\nPlugging d = 3 gives the corollary."}, {"heading": "D A General MAB Algorithm and Regret Bounds", "text": "Input: A convex, nonincreasing, nonnegative function \u03a6T (s) \u2208 C2, with nonincreasing second derivative. for t = T down to 1 do Find a convex function \u03a6t\u22121(s) s.t. the conditions of Theorem 4 hold. Set: s0 = 0. for t = 1 to T do\nSet: pt,i \u221d \u03a6t(st\u22121,i \u2212 1)\u2212 \u03a6t(st\u22121,i + 1). Draw it \u223c pt and receive loss \u2113t,it . Set: zt,i = 1{i = it} \u00b7 \u2113t,it/pt,it \u2212 \u2113t,it , \u2200i. Set: st = st\u22121 + zt.\nAlgorithm 4: A General MAB Algorithm\nTheorem 4. Suppose \u03a6t(s) is convex, twice continuously differentiable (i.e. \u03a6t(s) \u2208 C2), have nonincreasing second derivative, and satisfies:\n(\n1 2 +N\u03b1t\n) \u03a6t(s\u2212 1) + ( 1 2 \u2212N\u03b1t ) \u03a6t(s+ 1) \u2264 \u03a6t\u22121(s), \u2200s \u2208 R (6)\nwhere \u03b1t = 12 maxs \u03a6\u2032\u2032t (s\u22121)\n\u03a6t(s\u22121)\u2212\u03a6t(s+1) . If the player\u2019s strategy is such that pt,i \u221d \u03a6t(st\u22121,i \u2212 1)\u2212 \u03a6t(st\u22121,i + 1), then Eq. (2) holds in expectation.\nProof of Theorem 4. As discussed before, the main difficulty here is the unboundedness of zt,i. However, the expectation of zt,i is still in [\u22121, 1] as in DGv1. To exploit this fact, we apply Taylor\u2019s theorem to \u03a6t(st\u22121,i + zt,i) to the second order term:\n\u03a6t(st,i) = \u03a6t(st\u22121,i + zt,i)\n= \u03a6t(st\u22121,i) + \u03a6 \u2032 t(st\u22121,i)zt,i + 1 2\u03a6 \u2032\u2032 t (\u03bet,i)z 2 t,i \u2264 \u03a6t(st\u22121,i) + \u03a6\u2032t(st\u22121,i)zt,i + 12\u03a6 \u2032\u2032 t (st\u22121,i \u2212 1)z2t,i,\nwhere \u03bet,i is between st\u22121,i+zt,i and st\u22121,i, and the inequality holds because\u03a6\u2032\u2032t (s) is nonincreasing and zt,i \u2265 \u22121 by assumption. Now taking expectation on both sides with respect to the randomness of zt,i, using the convexity of \u03a6t(s), and plugging the assumption Et[z2t,i] \u2264 1/pt,i give:\nEt[\u03a6t(st,i)] \u2264 \u03a6t(st\u22121,i) + \u03a6\u2032t(st\u22121,i)Et[zt,i] + 12\u03a6 \u2032\u2032 t (st\u22121,i \u2212 1)Et[z2t,i]\n\u2264 \u03a6t (st\u22121,i + Et[zt,i]) + 12\u03a6 \u2032\u2032 t (st\u22121,i \u2212 1)/pt,i.\nLet wt,i = 12 (\u03a6t(st\u22121,i \u2212 1)\u2212 \u03a6t(st\u22121,i + 1)). Further plugging pt,i \u221d wt,i and summing over all i, we arrive at\nN \u2211\ni=1\nEt[\u03a6t(st,i)] \u2264 N \u2211\ni=1\n(\n\u03a6t (st\u22121,i + Et[zt,i]) + \u03a6\u2032\u2032t (st\u22121,i \u2212 1)\n2wt,i \u00b7\nN \u2211\ni=1\nwt,i\n)\n\u2264 N \u2211\ni=1\n(\n\u03a6t (st\u22121,i + Et[zt,i]) + 2\u03b1t\nN \u2211\ni=1\nwt,i\n)\n(by the defintion of \u03b1t)\n=\nN \u2211\ni=1\n(\u03a6t (st\u22121,i + Et[zt,i]) + 2N\u03b1twt,i) .\nSince Et[pt \u00b7 zt] \u2265 0 implies \u2211N i=1 wt,iEt[zt,i] \u2265 0, we thus have N \u2211\ni=1\nEt[\u03a6t(st,i)] \u2264 N \u2211\ni=1\n(\u03a6t (st\u22121,i + Et[zt,i]) + wt,iEt[zt,i] + 2N\u03b1twt,i)\n\u2264 N \u2211\ni=1\n(\nmax z\u2208[\u22121,+1] (\u03a6t (st\u22121,i + z) + wt,iz) + 2N\u03b1twt,i\n)\n= N \u2211\ni=1\n(\nmax z\u2208{\u22121,+1} (\u03a6t (st\u22121,i + z) + wt,iz) + 2N\u03b1twt,i\n)\n(by the convexity of \u03a6t(s))\n=\nN \u2211\ni=1\n((\n1 2 +N\u03b1t\n) \u03a6t(st\u22121,i \u2212 1) + ( 1 2 \u2212N\u03b1t ) \u03a6t(st\u22121,i + 1) )\n\u2264 N \u2211\ni=1\n\u03a6t\u22121(st\u22121,i). (by assumption)\nThe theorem follows by taking expectation on both sides with respect to the past (i.e. the randomness of z1, . . . , zt\u22121).\nTheorem 5. For Algorithm 4, if R and \u01eb are such that \u03a60(0) < \u01eb and \u03a6T (s) \u2265 1{s \u2264 \u2212R} for all s \u2208 R, then E[\u2211Tt=1 \u2113t,it \u2212 \u2211T t=1 \u2113t,i\u01eb ] < R for any non-oblivious adversary. Moreover, using \u03a6T (s) = exp(\u2212\u03b7(s + R)) (and let Eq. (6) hold with equality) gives exactly the EXP3 algorithm with regret O( \u221a TN ln(1/\u01eb)).\nProof of Theorem 5. We first show that Algorithm 4 converts the multi-armed bandit problem to a valid instance of DGv2. It suffices to prove that zt,i = 1{i = it} \u00b7 \u2113t,it/pt,it \u2212 \u2113t,it satisfies all conditions defined in DGv2, as shown below (zt,i \u2265 \u22121 is trivial):\nEt[zt,i] = \u2113t,i \u2212 pt \u00b7 \u2113t \u2264 1,\nEt[z 2 t,i] = pt,i\n(\n\u2113t,i pt,i\n\u2212 \u2113t,i )2 + \u2211\nj 6=i pt,j\u2113\n2 t,j \u2264 pt,i\n(\n1\npt,i \u2212 1\n)2\n+ \u2211\nj 6=i pt,j =\n1 pt,i \u2212 1 \u2264 1 pt,i ,\nEt[pt \u00b7 zt] = Et\n \u2113t,it \u2212 N \u2211\nj=1\npt,j\u2113t,it\n\n = 0.\nTherefore, we can apply Theorem 4 directly, arriving at:\n1\nN\nN \u2211\ni=1\nE[\u03a6T (sT,i)] \u2264 \u00b7 \u00b7 \u00b7 \u2264 1\nN\nN \u2211\ni=1\nE[\u03a60(s0,i)] = \u03a60(0) \u2264 \u01eb.\nOn the other hand, by applying Jensen\u2019 inequality, we have\nE[\u03a6T (sT,i)] \u2265 \u03a6T (E[sT,i]) \u2265 1{E[sT,i] \u2264 \u2212R}.\nNote that E[sT,i] is equal to E [ \u2211T t=1 (\u2113t,i \u2212 \u2113t,it) ] . We thus know\n1\nN\nN \u2211\ni=1\n1\n{\nE\n[\nT \u2211\nt=1\n(\u2113t,i \u2212 \u2113t,it) ] \u2264 \u2212R } < \u01eb,\nwhich implies E [\n\u2211T t=1 \u2113t,it \u2212 \u2211T t=1 \u2113t,i\u01eb\n]\n< R for any non-oblivious adversary for the exact same\nargument used in the proof of Theorem 2.\nFinally, we show how to recover EXP3 using Algorithm 4 with input \u03a6T (s) = exp(\u2212\u03b7(s+R)). To compute \u03a6t(s) for t < T , we simply use Eq. (6) with equality. One can verify using induction that\n\u03a6t(s) = exp (\u2212\u03b7(s+R)) ( e\u03b7 + e\u2212\u03b7 +Ne\u03b7\u03b72\n2\n)T\u2212t ,\n\u03b1t = 1\n2 max s \u03b72\u03a6t(s\u2212 1) \u03a6t(s\u2212 1)\u2212 \u03a6t(s+ 1) = e\u03b7\u03b72 2(e\u03b7 \u2212 e\u2212\u03b7) ,\n\u03a6\u2032\u2032\u2032t (s) = \u2212\u03b73\u03a6t(s) \u2264 0.\nThe player\u2019s strategy is thus pt,i \u221d exp(\u2212\u03b7 \u2211t\u22121\n\u03c4=1 \u2113\u0302\u03c4,i) (recall \u2113\u0302t,i = 1{i = it} \u00b7 \u2113t,it/pt,it is the estimated loss), which is exactly the same as EXP3 (in fact a simplified version of the original EXP3, see for example [30]). Moreover, the regret can be computed by setting \u03a60(0) = \u01eb, leading to\nR = 1\n\u03b7 ln\n(\n1\n\u01eb\n)\n+ T\n\u03b7 ln\n( e\u03b7 + e\u2212\u03b7\n2 +\n1 2 Ne\u03b7\u03b72\n)\n\u2264 1 \u03b7 ln\n(\n1\n\u01eb\n)\n+ T\n\u03b7 ln\n(\ne\u03b7 2/2 +\n1 2 Ne\u03b7\u03b72\n)\n(by Hoeffding\u2019s Lemma)\n\u2264 1 \u03b7 ln\n(\n1\n\u01eb\n)\n+ T\n\u03b7\n(\n\u03b72\n2 +\n1 2 Ne\u03b7\u2212 \u03b72 2 \u03b72 )\n(ln(1 + x) \u2264 x)\nIf \u03b7 \u2264 1 so that e\u03b7\u2212\u03b72/2 \u2264 \u221ae, then we have R \u2264 1\u03b7 ln(1\u01eb ) + T\u03b7 ( 1 2 + N \u221a e 2 )\n, which is \u221a\n2T (1 +N \u221a e) ln(1/\u01eb) after optimally choosing \u03b7 (\u03b7 \u2264 1 will be satisfied when T is large\nenough)."}, {"heading": "E A General OCO Algorithm and Regret Bounds", "text": "Input: A convex, nonincreasing, nonnegative function \u03a6T (s) for t = T down to 1 do\nFind a convex function \u03a6t\u22121(s) s.t. \u2200s, \u03a6t(s\u2212 1) + \u03a6t(s+ 1) \u2264 2\u03a6t\u22121(s). Set: s0(x) \u2261 0. for t = 1 to T do\nPredict xt = Ex\u223cpt [x] where pt is such that pt(x) \u221d \u03a6t(st\u22121(x) \u2212 1)\u2212 \u03a6t(st\u22121(x) + 1). Receive loss function ft from the adversary. Set: zt(x) = ft(x) \u2212 ft(xt). Set: st(x) = st\u22121(x) + zt(x).\nAlgorithm 5: A General OCO Algorithm\nDefinition of \u01eb-regret in the OCO setting: Let S\u01eb \u2282 S be such that the ratio of its volume and the one of S is \u01eb and also\n\u2211T t=1 ft(x \u2032) \u2264 \u2211Tt=1 ft(x) for all x\u2032 \u2208 S\u01eb and x \u2208 S\\S\u01eb (it is clear that such set exists). Then \u01eb-regret is defined as R\u01ebT (x1:T , f1:T ) = \u2211T t=1 ft(xt)\u2212 infx\u2208S\\S\u01eb \u2211T t=1 ft(x). Theorem 6. For Algorithm 5, if R is such that \u03a6T (s) \u2265 1{s \u2264 \u2212R} and \u03a60(0) < \u01eb, then we have R\u01ebT (x1:T , f1:T ) < R and RT (x1:T , f1:T ) < R+ T \u01eb 1/d. Specifically, if R = O( \u221a\nT ln(1/\u01eb)), then setting \u01eb = T\u2212d gives RT (x1:T , f1:T ) = O( \u221a dT lnT ).\nProof of Theorem 6. Let wt(x) = 12 (\u03a6t(st\u22121(x)\u2212 1)\u2212 \u03a6t(st\u22121(x) + 1)). Similarly to the Hedge setting, the \u201csum\u201d of potentials never increases: \u222b\nx\u2208S \u03a6t(st(x))dx \u2264\n\u222b\nx\u2208S (\u03a6t(st\u22121(x) + zt(x)) + wt(x)zt(x)) dx \u2264\n\u222b\nx\u2208S \u03a6t\u22121(st\u22121(x))dx.\nHere, the first inequality is due to Ex\u223cpt [zt(x)] \u2265 0, and the second inequality holds for the exact same reason as in the case for Hedge. Therefore, we have\n\u222b\nx\u2208S 1{sT (x) \u2264 \u2212R}dx \u2264\n\u222b\nx\u2208S \u03a6T (sT (x))dx \u2264 \u00b7 \u00b7 \u00b7 \u2264\n\u222b\nx\u2208S \u03a60(0)dx < \u01ebV,\nwhere V is the volume of S. Recall the construction of S\u01eb. There must exist a point x\u2032 \u2208 S\u01eb such that sT (x\u2032) > \u2212R, otherwise \u222b x 1{sT (x) \u2264 \u2212R}dx would be at least \u01ebV . Unfolding sT (x\u2032), we arrive at \u2211\nt ft(xt) \u2212 \u2211 t ft(x \u2032) < R. Using the fact \u2211 t ft(x \u2032) \u2264 infx\u2208S\\S\u01eb \u2211\nt ft(x) gives the bound for \u01eb-regret.\nNext consider a shrunk version of S: S\u2032\u01eb = {(1 \u2212 \u01eb 1 d )x\u2217 + \u01eb 1 dx : x \u2208 S} where x\u2217 \u2208 argminx \u2211 t ft(x). Then \u222b\nx\u2208S 1{sT (x) \u2264 \u2212R}dx is at least \u222b\nx\u2208S\u2032\u01eb 1{sT (x) \u2264 \u2212R}dx = \u01eb\n\u222b\nx\u2208S 1\n{\nsT\n( (1\u2212 \u01eb 1d )x\u2217 + \u01eb 1dx ) \u2264 \u2212R } dx,\nwhich, by the convexity and the boundedness of ft(x), is at least\n\u01eb\n\u222b\nx\u2208S 1\n{\nT \u2211\nt=1\n(\n(1\u2212 \u01eb 1d )ft(x\u2217) + \u01eb 1 d ft(x)\u2212 ft(xt)\n) \u2264 \u2212R } dx\n\u2265 \u01eb \u222b\nx\u2208S 1\n{\nT \u2211\nt=1\n(ft(x \u2217)\u2212 ft(xt)) \u2264 \u2212R\u2212 T \u01eb 1 d\n}\ndx\n= \u01ebV \u00b7 1 { T \u2211\nt=1\n(ft(x \u2217)\u2212 ft(xt)) \u2264 \u2212R\u2212 T \u01eb 1 d\n}\n.\nFollowing the previous discussion, the expression in the last line above is strictly less than \u01ebV \u00b7, which means that the value of the indicator function has to be 0, namely, RT (x1:T , f1:T ) < R + T \u01eb1/d."}, {"heading": "F NH-Boost.DT, NH-Boost and Proof of Theorem 3", "text": "Input : Training examples (xi, yi) \u2208 Rd \u00d7 {\u22121,+1}, i = 1, . . . , N. Input : A weak learning algorithm. Input : Number of rounds T . Output: A Hypothesis H(x) : Rd \u2192 {\u22121,+1}. Set: s0 = 0. for t = 1 to T do\nSet: pt,i \u221d exp ( [st\u22121,i \u2212 1]2\u2212/3t ) \u2212 exp ( [st\u22121,i + 1]2\u2212/3t ) , \u2200i. Invoke the weak learning algorithm to get ht with edge \u03b3t = 12 \u2211\ni pt,iyiht(xi). Set: st,i = st\u22121,i + 12yiht(xi)\u2212 \u03b3t, \u2200i.\nSet: H(x) = sign( \u2211T\nt=1 ht(x)).\nAlgorithm 6: NH-Boost.DT\nInput : Training examples (xi, yi) \u2208 Rd \u00d7 {\u22121,+1}, i = 1, . . . , N. Input : A weak learning algorithm. Input : Number of rounds T . Output: A Hypothesis H(x) : Rd \u2192 {\u22121,+1}. Set: s0 = 0. for t = 1 to T do\nif t = 1 then Set: p1 to be a uniform distribution. else Find: c such that\n\u2211N i=1 exp ( [st\u22121,i]2\u2212/c )\n= Ne. Set: pt,i \u221d \u2212[st\u22121,i]\u2212 exp ( [st\u22121,i]2\u2212/c )\n, \u2200i. Invoke the weak learning algorithm to get ht with edge \u03b3t = 12 \u2211\ni pt,iyiht(xi). Set: st,i = st\u22121,i + 12yiht(xi)\u2212 \u03b3t, \u2200i.\nSet: H(x) = sign( \u2211T\nt=1 ht(x)).\nAlgorithm 7: NH-Boost\nIn the boosting setting for binary classification, we are given a set of training examples (xi, yi)i=1,...,N where xi \u2208 Rd is an example and yi \u2208 {\u22121,+1} is its label. A boosting algorithm proceeds for T rounds. On each round, a distribution pt over the examples is computed and fed into a weak learning algorithm which returns a \u201cweak\u201d hypothesis ht : Rd \u2192 {\u22121,+1} with a guaranteed small edge, that is, \u03b3t = 12 \u2211\ni pt,iyiht(xi) \u2265 \u03b3 > 0. At the end, a linear combination of all ht is computed as the final \u201cstrong\u201d hypothesis which is expected to have low training error and potentially low generalization error.\nThe conversion of a Hedge algorithm into a boosting algorithm is to treat each example as an \u201caction\u201d and set \u2113t,i = 1{ht(xi) = yi} so that the booster tends to increase weights for those \u201chard\u201d\nexamples. The final hypothesis is a simple majority vote of all ht, that is, H(x) = sign( \u2211\nt ht(x)) where sign(x) is the sign function that outputs 1 if x is positive, and \u22121 otherwise. The margin of example xi is defined as 1T \u2211T t=1 yiht(xi), that is, the difference between the fractions of correct hypotheses and incorrect hypotheses on this example. The boosting algorithms derived from NormalHedge.DT and NormalHedge in this way are given in Algorithm 6 and 7.\nProof the Theorem 3. Let (x\u0303i, y\u0303i)i=1,...,N be a permutation of the training examples such that their margins are sorted from smallest to largest: \u2211\nt y\u03031ht(x\u03031) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u2211\nt y\u0303Nht(x\u0303N ), which also implies \u2211\nt 1{ht(x\u03031) = y\u03031} \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u2211 t 1{ht(x\u0303N ) = y\u0303N}. Recall that NormalHedge.DT is essentially playing a Hedge game using NormalHedge.DT with loss \u2113t,i = 1{ht(xi) = yi}. Therefore, the \u01eb-regret bound for the Hedge setting together with the assumption on the weak learning algorithm implies: \u2200j \u2208 {1, . . . , N},\n1 2 + \u03b3 \u2264 1 T\nT \u2211\nt=1\nN \u2211\ni=1\npt,i1{ht(xi) = yi} \u2264 1\nT\nT \u2211\nt=1\n1{ht(x\u0303j) = y\u0303j}+ R\nj/N T T , (7)\nwhere Rj/NT = O\u0303( \u221a 3T ln(N/j)) is the j/N -regret bound for NormalHedge.DT. So if j is such that \u03b3 > Rj/NT /T , we have 1 T \u2211T t=1 1{ht(x\u0303j) = y\u0303j} > 12 , which is saying that example (x\u0303j , y\u0303j) will eventually be classified correctly by H(x) due to the fact that H(x) is taking a majority vote of all ht. This is in fact true for all examples (x\u0303i, y\u0303i) such that i \u2265 j and thus the training error rate will be at most (j \u2212 1)/N , which is of order O\u0303(exp(\u2212 13T\u03b32)). For the margin bound, by plugging 1{ht(x\u0303j) = y\u0303j} = (y\u0303jht(x\u0303j) + 1)/2, we rewrite Eq. (7) as:\n2\n(\n\u03b3 \u2212 R j/N T\nT\n)\n\u2264 1 T\nT \u2211\nt=1\ny\u0303jht(x\u0303j).\nTherefore, if j is such that \u03b8 < 2(\u03b3 \u2212Rj/NT /T ), then the fraction of examples with margin at most \u03b8 is again at most (j \u2212 1)/N , which is of order O\u0303(exp(\u2212 13T (\u03b8 \u2212 2\u03b3)2))."}, {"heading": "G Experiments in a Boosting Setting", "text": "We conducted experiments to compare the performance of three boosting algorithms for binary classification: AdaBoost [14], NH-Boost (Algorithm 7) and NH-Boost.DT (Algorithm 6), using a set of benchmark data available from the UCI repository3 and LIBSVM datasets4. Some datasets are preprocessed according to [27]. The number of features, training examples and test examples can be found in Table 2.\nAll features are binary. The weak learning algorithm is a simple (exhaustive) decision stump (see for instance [29]). On each round, the weak learning algorithm enumerates all features, and for each feature computes the weighted error of the corresponding stump on the weighted training examples. Therefore, if the number of examples with zero weight is relatively large, then the weak learning algorithm would be faster in computing the weighted error and thus faster in finding the best feature.\nAll boosting algorithms are run for two hundred rounds. The results are summarized in Table 3, with bold entries being the best ones among the three (AB, NB and NBDT stand for AdaBoost, NH-Boost and NH-Boost.DT respectively). As we can see, in terms of training error and test error, all three algorithms have similar performance. However, our NH-Boost.DT algorithm is always the fastest one. The average fraction of examples with zero weights for NH-Boost.DT is significantly higher than the one for NH-Boost (note that AdaBoost does not assign zero weight at all). We plot the change of this fraction over rounds in Figure 1 (using three datasets). As both algorithms proceed, they tend to ignore more and more examples on each round, but NH-Boost.DT consistently ignores more examples than NH-Boost.\nSince st,i is positively correlated to the margin of example i ( 1t \u2211t\n\u03c4=1 yih\u03c4 (xi)) and large st,i leads to zero weight, the above phenomenon in fact implies that the examples\u2019 margins should be larger for\n3http://archive.ics.uci.edu/ml/ 4http://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/\nNH-Boost.DT than for NH-Boost. This is confirmed by Figure 2, where we plot the final cumulative margins on three datasets (i.e. each point represents the fraction of examples with at most some fixed margin). One can see that the lines for NH-Boost.DT are below the ones for NH-Boost (and even AdaBoost) for most time, meaning that NH-Boost.DT achieves larger margins in general. This could explain NH-Boost.DT\u2019s better test error on some datasets."}], "references": [{"title": "Optimal strategies and minimax lower bounds for online convex games", "author": ["Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Minimax games with bandits", "author": ["Jacob Abernethy", "Manfred K. Warmuth"], "venue": "In Proceedings of the 22st Annual Conference on Learning Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Repeated games against budgeted adversaries", "author": ["Jacob Abernethy", "Manfred K. Warmuth"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Regret bounds and minimax policies under partial monitoring", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Regret in online combinatorial optimization", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "How to use expert advice", "author": ["Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "David Haussler", "David P. Helmbold", "Robert E. Schapire", "Manfred K. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Potential-based algorithms in on-line prediction and game theory", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "A parameter-free hedging algorithm", "author": ["Kamalika Chaudhuri", "Yoav Freund", "Daniel Hsu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Prediction with advice of unknown number of experts", "author": ["Alexey Chernov", "Vladimir Vovk"], "venue": "arXiv preprint arXiv:1006.0475,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Universal portfolios", "author": ["Thomas M. Cover"], "venue": "Mathematical Finance,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1991}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Yoav Freund"], "venue": "Information and Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Adaptive game playing using multiplicative weights", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Games and Economic Behavior,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Additive logistic regression: A statistical view of boosting", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Anytime algorithms for multi-armed bandit problems", "author": ["Robert Kleinberg"], "venue": "In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Online decision problems with large strategy sets", "author": ["Robert David Kleinberg"], "venue": "PhD thesis,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Towards Minimax Online Learning with Unknown Time Horizon", "author": ["Haipeng Luo", "Robert E. Schapire"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Unconstrained online linear learning in hilbert spaces: Minimax algorithms and normal approximations", "author": ["H Brendan McMahan", "Francesco Orabona"], "venue": "In Proceedings of the 27th Annual Conference on Learning Theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Learning with continuous experts using drifting games", "author": ["Indraneel Mukherjee", "Robert E. Schapire"], "venue": "Theoretical Computer Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Random walk approach to regret minimization", "author": ["Hariharan Narayanan", "Alexander Rakhlin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Simultaneous model selection and optimization through parameter-free stochastic learning", "author": ["Francesco Orabona"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Relax and localize: From value to algorithms", "author": ["Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "How boosting the margin can also boost classifier complexity", "author": ["Lev Reyzin", "Robert E. Schapire"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Drifting games", "author": ["Robert E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "Boosting: Foundations and Algorithms", "author": ["Robert E. Schapire", "Yoav Freund"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}], "referenceMentions": [{"referenceID": 6, "context": "This is possible for some special cases ([7, 1, 3, 21]) but difficult in general.", "startOffset": 41, "endOffset": 54}, {"referenceID": 0, "context": "This is possible for some special cases ([7, 1, 3, 21]) but difficult in general.", "startOffset": 41, "endOffset": 54}, {"referenceID": 2, "context": "This is possible for some special cases ([7, 1, 3, 21]) but difficult in general.", "startOffset": 41, "endOffset": 54}, {"referenceID": 20, "context": "This is possible for some special cases ([7, 1, 3, 21]) but difficult in general.", "startOffset": 41, "endOffset": 54}, {"referenceID": 13, "context": "On the other hand, many other efficient algorithms with optimal regret rate (but not exactly minimax optimal) have been proposed for different learning settings (such as the exponential weights algorithm [14, 15], and follow the perturbed leader [18]).", "startOffset": 204, "endOffset": 212}, {"referenceID": 14, "context": "On the other hand, many other efficient algorithms with optimal regret rate (but not exactly minimax optimal) have been proposed for different learning settings (such as the exponential weights algorithm [14, 15], and follow the perturbed leader [18]).", "startOffset": 204, "endOffset": 212}, {"referenceID": 17, "context": "On the other hand, many other efficient algorithms with optimal regret rate (but not exactly minimax optimal) have been proposed for different learning settings (such as the exponential weights algorithm [14, 15], and follow the perturbed leader [18]).", "startOffset": 246, "endOffset": 250}, {"referenceID": 25, "context": "[26] built a bridge between these two classes of methods by showing that many existing algorithms can indeed be derived from a minimax analysis followed by a series of relaxations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Drifting games [28] (reviewed in Section 2) generalize Freund\u2019s \u201cmajority-vote game\u201d [13] and subsume some well-studied boosting and online learning settings.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "Drifting games [28] (reviewed in Section 2) generalize Freund\u2019s \u201cmajority-vote game\u201d [13] and subsume some well-studied boosting and online learning settings.", "startOffset": 85, "endOffset": 89}, {"referenceID": 27, "context": "A nearly minimax optimal algorithm is proposed in [28].", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "We then generalize the minimax analysis in [28] based on one key idea: relax a 0-1 loss function by a convex surrogate.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "Hedge Settings: (Section 3) The Hedge problem [14] investigates how to cleverly bet across a set of actions.", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "3) bears some similarities with the NormalHedge algorithm [10] and enjoys a similar \u01eb-regret bound simultaneously for all \u01eb and horizons.", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "Our analysis is also arguably simpler and more intuitive than the one in [10] and easy to be generalized to more general settings.", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "Multi-armed Bandit Problems: (Section 4) The multi-armed bandit problem [6] is a classic example for learning with incomplete information where the learner can only obtain feedback for the actions taken.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "Again the minimax analysis is generalized and the EXP3 algorithm [6] is recovered.", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "Our results could be seen as a preliminary step to answer the open question [2] on exact minimax optimal algorithms for the multi-armed bandit problem.", "startOffset": 76, "endOffset": 79}, {"referenceID": 11, "context": "Fortunately, it turns out that all results from the Hedge setting are ready to be used here, recovering the continuous EXP algorithm [12, 17, 24] and also generalizing our new algorithms to this general setting.", "startOffset": 133, "endOffset": 145}, {"referenceID": 16, "context": "Fortunately, it turns out that all results from the Hedge setting are ready to be used here, recovering the continuous EXP algorithm [12, 17, 24] and also generalizing our new algorithms to this general setting.", "startOffset": 133, "endOffset": 145}, {"referenceID": 23, "context": "Fortunately, it turns out that all results from the Hedge setting are ready to be used here, recovering the continuous EXP algorithm [12, 17, 24] and also generalizing our new algorithms to this general setting.", "startOffset": 133, "endOffset": 145}, {"referenceID": 28, "context": "Boosting: (Section 4) Realizing that every Hedge algorithm can be converted into a boosting algorithm ([29]), we propose a new boosting algorithm (NH-Boost.", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "DT is then translated into training error and margin distribution bounds that previous analysis in [29] using nonadaptive algorithms does not show.", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "Similar concepts have widely appeared in the literature [8, 5], but unlike our work, they are not related to any minimax analysis and might be hard to interpret.", "startOffset": 56, "endOffset": 62}, {"referenceID": 4, "context": "Similar concepts have widely appeared in the literature [8, 5], but unlike our work, they are not related to any minimax analysis and might be hard to interpret.", "startOffset": 56, "endOffset": 62}, {"referenceID": 10, "context": "The existence of parameter free Hedge algorithms for unknown number of actions was shown in [11], but no concrete algorithms were given there.", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "Boosting algorithms that ignore some examples on each round were studied in [16], where a heuristic was used to ignore examples with small weights and no theoretical guarantee is provided.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "For instance, binary classification via boosting can be translated into a drifting game by treating each training example as a chip (see [28] for details).", "startOffset": 137, "endOffset": 141}, {"referenceID": 27, "context": "A nearly optimal strategy and its analysis is originally given in [28], and a derivation by directly tackling the above minimax expression can be found in [29, chap.", "startOffset": 66, "endOffset": 70}, {"referenceID": 27, "context": "(2) It has been shown in [28] that this upper bound on the loss is optimal in a very strong sense.", "startOffset": 25, "endOffset": 29}, {"referenceID": 12, "context": "With the loss function L(s) being 1{s \u2264 0}, these can be further simplified and eventually give exactly the boost-by-majority algorithm [13].", "startOffset": 136, "endOffset": 140}, {"referenceID": 27, "context": "3 Online Learning as a Drifting Game The connection between drifting games and some specific settings of online learning has been noticed before ([28, 23]).", "startOffset": 146, "endOffset": 154}, {"referenceID": 22, "context": "3 Online Learning as a Drifting Game The connection between drifting games and some specific settings of online learning has been noticed before ([28, 23]).", "startOffset": 146, "endOffset": 154}, {"referenceID": 13, "context": "1 Algorithmic Equivalence In the Hedge setting [14], a player tries to earn as much as possible (or lose as little as possible) by cleverly spreading a fixed amount of money to bet on a set of actions on each day.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "action i incurs loss lt,i \u2208 [0, 1]) which are revealed to the player.", "startOffset": 28, "endOffset": 34}, {"referenceID": 19, "context": "Here, we consider an even more general notion of regret studied in [20, 19, 10, 11], which we call \u01eb-regret.", "startOffset": 67, "endOffset": 83}, {"referenceID": 18, "context": "Here, we consider an even more general notion of regret studied in [20, 19, 10, 11], which we call \u01eb-regret.", "startOffset": 67, "endOffset": 83}, {"referenceID": 9, "context": "Here, we consider an even more general notion of regret studied in [20, 19, 10, 11], which we call \u01eb-regret.", "startOffset": 67, "endOffset": 83}, {"referenceID": 10, "context": "Here, we consider an even more general notion of regret studied in [20, 19, 10, 11], which we call \u01eb-regret.", "startOffset": 67, "endOffset": 83}, {"referenceID": 22, "context": "Doing this will lead to the exact setting discussed in [23] where a near optimal strategy is proposed using the recipe in Eq.", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "To see this, first recall that results from [23], written in our notation, state that minDR LT (DR) \u2264 1 2T \u2211 T\u2212R 2 j=0 ( T+1 j ) , which, by Hoeffding\u2019s inequality, is upper bounded by 2 exp ( \u2212 (R+1) 2 2(T+1) )", "startOffset": 44, "endOffset": 48}, {"referenceID": 22, "context": "However, the algorithm proposed in [23] is not computationally efficient since the potential functions \u03a6t(s) do not have closed forms.", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "It turns out that this will lead to the well-known exponential weights algorithm [14, 15].", "startOffset": 81, "endOffset": 89}, {"referenceID": 14, "context": "It turns out that this will lead to the well-known exponential weights algorithm [14, 15].", "startOffset": 81, "endOffset": 89}, {"referenceID": 25, "context": "More importantly, as in [26], this derivation may shed light on why this algorithm works and where it comes from, namely, a minimax analysis followed by a series of relaxations, starting from a reasonable surrogate of the 0-1 loss function.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "We call this the \u201c2-norm\u201d algorithm since it resembles the p-norm algorithm in the literature when p = 2 (see [9]).", "startOffset": 110, "endOffset": 113}, {"referenceID": 9, "context": "In fact, our algorithm bears a striking similarity to NormalHedge [10], the first algorithm that has this kind of adaptivity.", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "Similar potential was also proposed in recent work [22, 25] for a different setting.", "startOffset": 51, "endOffset": 59}, {"referenceID": 24, "context": "Similar potential was also proposed in recent work [22, 25] for a different setting.", "startOffset": 51, "endOffset": 59}, {"referenceID": 9, "context": "DT is more computationally efficient especially when N is very large, since it does not need a numerical search for each round; 2) our analysis is arguably simpler and more intuitive than the one in [10]; 3) as we will discuss in Section 4, NormalHedge.", "startOffset": 199, "endOffset": 203}, {"referenceID": 9, "context": "DT can be easily extended to deal with the more general online convex optimization problem where the number of actions is infinitely large, while it is not clear how to do that for NormalHedge by generalizing the analysis in [10].", "startOffset": 225, "endOffset": 229}, {"referenceID": 5, "context": "4 Generalizations and Applications Multi-armed Bandit (MAB) Problem: The only difference between Hedge (randomized version) and the non-stochastic MAB problem [6] is that on each round, after picking it, the player only sees the loss for this single action lt,it instead of the whole vector lt.", "startOffset": 159, "endOffset": 162}, {"referenceID": 5, "context": "Then algorithms such as EXP can be used by replacing lt with l\u0302t, leading to the EXP3 algorithm [6] with regret O( \u221a TN lnN).", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "We conjecture, however, that there is a potential function that could recover the poly-INF algorithm [4, 5] or give its variants that achieve the optimal regret O( \u221a TN).", "startOffset": 101, "endOffset": 107}, {"referenceID": 4, "context": "We conjecture, however, that there is a potential function that could recover the poly-INF algorithm [4, 5] or give its variants that achieve the optimal regret O( \u221a TN).", "startOffset": 101, "endOffset": 107}, {"referenceID": 0, "context": "Let S \u2282 R be a compact convex set, and F be a set of convex functions with range [0, 1] on S.", "startOffset": 81, "endOffset": 87}, {"referenceID": 29, "context": "There are two general approaches to OCO: one builds on convex optimization theory [30], and the other generalizes EXP to a continuous space [12, 24].", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": "There are two general approaches to OCO: one builds on convex optimization theory [30], and the other generalizes EXP to a continuous space [12, 24].", "startOffset": 140, "endOffset": 148}, {"referenceID": 23, "context": "There are two general approaches to OCO: one builds on convex optimization theory [30], and the other generalizes EXP to a continuous space [12, 24].", "startOffset": 140, "endOffset": 148}, {"referenceID": 16, "context": "Nevertheless, this is addressed by Theorem 6 using similar techniques in [17], giving the usual O( \u221a dT lnT ) regret bound.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Applications to Boosting: There is a deep and well-known connection between Hedge and boosting [14, 29].", "startOffset": 95, "endOffset": 103}, {"referenceID": 28, "context": "Applications to Boosting: There is a deep and well-known connection between Hedge and boosting [14, 29].", "startOffset": 95, "endOffset": 103}, {"referenceID": 28, "context": "DT converges to the optimal margin 2\u03b3; this is known not to be true for AdaBoost (see [29]).", "startOffset": 86, "endOffset": 90}, {"referenceID": 0, "context": "References [1] Jacob Abernethy, Peter L.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Jacob Abernethy and Manfred K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Jacob Abernethy and Manfred K.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Jean-Yves Audibert and S\u00e9bastien Bubeck.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Jean-Yves Audibert, S\u00e9bastien Bubeck, and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Peter Auer, Nicol\u00f2 Cesa-Bianchi, Yoav Freund, and Robert E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Nicol\u00f2 Cesa-Bianchi, Yoav Freund, David Haussler, David P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Nicol\u00f2 Cesa-Bianchi and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Kamalika Chaudhuri, Yoav Freund, and Daniel Hsu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Alexey Chernov and Vladimir Vovk.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Thomas M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Yoav Freund.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Yoav Freund and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Yoav Freund and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Jerome Friedman, Trevor Hastie, and Robert Tibshirani.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Elad Hazan, Amit Agarwal, and Satyen Kale.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Adam Kalai and Santosh Vempala.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Robert Kleinberg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Robert David Kleinberg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Haipeng Luo and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] H Brendan McMahan and Francesco Orabona.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Indraneel Mukherjee and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Hariharan Narayanan and Alexander Rakhlin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Francesco Orabona.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Lev Reyzin and Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] Robert E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] Shai Shalev-Shwartz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "For Algorithm 2, zt,i lies in B = [\u22121, 1] since lt,i \u2208 [0, 1], and direct computation showspt\u00b7zt = 0 \u2265 \u03b2(= 0) and |zt,i\u2212zt,j| = |lt,i\u2212lt,j| \u2264 1 for all i and j.", "startOffset": 55, "endOffset": 61}, {"referenceID": 29, "context": "The player\u2019s strategy is thus pt,i \u221d exp(\u2212\u03b7 \u2211t\u22121 \u03c4=1 l\u0302\u03c4,i) (recall l\u0302t,i = 1{i = it} \u00b7 lt,it/pt,it is the estimated loss), which is exactly the same as EXP3 (in fact a simplified version of the original EXP3, see for example [30]).", "startOffset": 226, "endOffset": 230}, {"referenceID": 13, "context": "G Experiments in a Boosting Setting We conducted experiments to compare the performance of three boosting algorithms for binary classification: AdaBoost [14], NH-Boost (Algorithm 7) and NH-Boost.", "startOffset": 153, "endOffset": 157}, {"referenceID": 26, "context": "Some datasets are preprocessed according to [27].", "startOffset": 44, "endOffset": 48}, {"referenceID": 28, "context": "The weak learning algorithm is a simple (exhaustive) decision stump (see for instance [29]).", "startOffset": 86, "endOffset": 90}], "year": 2014, "abstractText": "We provide a general mechanism to design online learning algorithms based on a minimax analysis within a drifting-games framework. Different online learning settings (Hedge, multi-armed bandit problems and online convex optimization) are studied by converting into various kinds of drifting games. The original minimax analysis for drifting games is then used and generalized by applying a series of relaxations, starting from choosing a convex surrogate of the 0-1 loss function. With different choices of surrogates, we not only recover existing algorithms, but also propose new algorithms that are totally parameter-free and enjoy other useful properties. Moreover, our drifting-games framework naturally allows us to study high probability bounds without resorting to any concentration results, and also a generalized notion of regret that measures how good the algorithm is compared to all but the top small fraction of candidates. Finally, we translate our new Hedge algorithm into a new adaptive boosting algorithm that is computationally faster as shown in experiments, since it ignores a large number of examples on each round.", "creator": "LaTeX with hyperref package"}}}