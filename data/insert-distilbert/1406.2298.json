{"id": "1406.2298", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2014", "title": "Explaining Violation Traces with Finite State Natural Language Generation Models", "abstract": "an essential element of any verification technique is that of identifying and communicating to the user, concerning system behaviour which leads directly to a deviation from producing the expected behaviour. such behaviours are typically made available as long traces of system actions behavior which seemingly would benefit from a natural language explanation of the trace and especially in the organizational context of business logic level specifications. in this paper firstly we present a natural language generation model which later can be used to explain such traces. a commercially key idea is specifically that the explanation language is a cnl that is, formally speaking, regular language defining susceptible transformations models that can be expressed with finite state machinery. at the same time itself it admits various forms of abstraction and simplification which contribute notably to the naturalness of explanations that are communicated to the user.", "histories": [["v1", "Mon, 9 Jun 2014 19:51:10 GMT  (22kb)", "http://arxiv.org/abs/1406.2298v1", null]], "reviews": [], "SUBJECTS": "cs.SE cs.CL", "authors": ["gordon j pace", "michael rosner"], "accepted": false, "id": "1406.2298"}, "pdf": {"name": "1406.2298.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Michael Rosner"], "emails": ["gordon.pace@um.edu.mt", "mike.rosner@um.edu.mt"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n22 98\nv1 [\ncs .S\nE ]\n9 J\nun 2\ning and communicating to the user, system behaviour which leads to a deviation from the expected behaviour. Such behaviours are typically made available as long traces of system actions which would benefit from a natural language explanation of the trace and especially in the context of business logic level specifications. In this paper we present a natural language generation model which can be used to explain such traces. A key idea is that the explanation language is a CNL that is, formally speaking, regular language susceptible transformations that can be expressed with finite state machinery. At the same time it admits various forms of abstraction and simplification which contribute to the naturalness of explanations that are communicated to the user."}, {"heading": "1 Introduction", "text": "The growth in size and complexity of computer systems has been accompanied by an increase in importance given to the application of verification techniques, attempting to avoid or at least mitigate problems arising due to errors in the system design and implementation. Given a specification of how the system should behave (or, dually, of what the system should not do), techniques ranging from testing to runtime verification and model checking attempt to answer the question of whether or not the system is correct. One common issue with all these techniques, is that a negative answer is useless unless accompanied by a trace showing how the system may perform leading to a violation of the expected behaviour.\nConsider, for example, the specification of a system which allows user to log in, as shown in Figure 1, which states that \u201cafter three consecutive failed user authentications, users should not be allowed to attempt another login\u201d. A testing or runtime verification tool may deduce that the system may perform a long sequence of events which lead to a violation. Although techniques have been developed to shorten such counter-examples [ZH02], such traces may be rather long, and using them to understand the circumstances in which the system failed to work as expected may not always be straightforward.\nIn the case of implementation-level properties and traces, tools such as debuggers and simulators may enable the processing of long traces by developers to understand the nature of the bug, but in the case of higher-level specifications, giving businesslogic level properties, such traces may need to be processed by management personnel. For example, a fraud expert may be developing fraud rules to try to match against the\nbehaviour of known black-listed users, and may want to understand why a trace showing the behaviour of such a user does to trigger a rule he may have just set up. In such cases, a natural language explanation of such a trace would help the expert to understand better what is going wrong and why.\nIn this paper, we present the use of finite state natural language generation (NLG) models to explain violation traces. We assume that the basis of the controlled natural language used to describe such behaviour is given by the person writing the specification, by articulating how the actions can be described using a natural language, and how they can be abstracted into more understandable explanations.We present a stepwise refinement of the process, explaining how a more natural feel to the generated controlled natural language text can be given using finite state techniques.\nAlthough the work we present is still exploratory, we believe that the approach can be generalised to work on more complex systems, and it can give insight into how far out the limits of finite state NLG techniques can be pushed."}, {"heading": "2 The Roles of NLG and CNL", "text": "In this section we illustrate a solution to the problem of generating reasonably natural explanations from sequences of the above type in a computationally efficient way. The two critical ingredients are (i) NLG, which, in a general sense, provides a set of techniques for generating text flexibly given an abstract non-linguistic representation of\nsemantic content, and (ii) CNLs which, in a nutshell, are natural languages with a designer element \u2014 natural in the sense that they can be understood by native speakers of the \u201cparent\u201d language and designed to to be simpler than that language from some computational perspective such as translation to logic or, as in this paper, NLG. An excellent survey and classification scheme for CNLs appears in Kuhn [Kuh14]\nThe final output of NLG is clearly natural language of some kind. The nature of the process that produces that output is somewhat less clear, in that there are still many approaches though most research in the area is consistent with the assumption that that it includes at least the stages of content planning (decidingwhat to say), content packaging (packaging the content into sentence-sizedmessages), and surface realisation (constructing individual sentences). These three stages are linked together in a pipeline, according to the architecture proposed by Reiter and Dale [RD00].\nThe complexity of NLG arises from fact that the input content severely underdetermines the surface realisation and that there are few guiding principles available to narrow the realisation choices. Consequently, the process is even more nondeterministic than the inverse process of of natural language understandingwhere at least it is possible to appeal to common sense when attempting to choose amongst competing interpretations.With NLG, some dimensions of complexitymust be sacrificed for the computation to be feasible.\nIn this paper, the sacrifice comes down to a choice concerning two sets of languages: (i) that which expresses the content, which we will call the C language, and (ii) a sequence of languages in which explanations are realised that we will callE1,E2, etc.C is a form of semantic representation language, whilst E1, E2, . . . En are CNLs in Kuhn\u2019s sense.\nIn both cases, we assume that bothC andEi languages are regular languages in the formal sense. This has a number of advantages: the computational properties of such languages are well understood, and we know that algorithms for parsing and generation of are of relatively low complexity. Additionally, we can express linguistic processes as relations over such languages that can be computed by finite-state transducers. Elementary transductions can be composed together to carry out complex linguistic processing tasks. Using techniques originally advocated for morphological analysis by Beesley and Kattunen [BK03] we can envisage a complex NLG process as a series of finite state transductions combined together under relational composition, thus opening up the possibility of describing the synthesis of an explanation to efficient, finite-state machinery.\nOf course, the restriction to regular languages imposes certain limitations upon what content can possibly be expressed in C, and may also impact the naturalness of the violation description expressed in E. However, these are empirical issues that will not be tackled in this paper\nWe are not the first to have used simplified languages in the attempt to reduce the complexity of natural language processing. In the domain of NLG, Wilcock [Wil01] proposed \u201cthe use of XML-based tools to implement existing well-known approaches to NLG\u201d. Power [Pow12] uses finite-state representations for expressing descriptions of OWL-LITE sentences. It is of course in the area of computational morphology where finite state methods are best known.\nThe main contribution of the paper is to substantiate and present the hypothesis that according to the choice of C and E, it is possible to realise a family of efficient NLG systems that are based on steadfastly finite-state technology."}, {"heading": "3 Languages", "text": "In what follows we first present the C language and then a sequence of E languages, progressively adding features to attain a more natural explanation of the trace. As we shall investigate in more detail in Section 4, at each stage we use further information to obtain more natural generated text."}, {"heading": "3.1 The C Language", "text": "We assume that the basic specification of the C language is given by the automaton shown in Figure 2. The following trace is a sentence\nlgrxlblgwwxlgrwxlgxlblblbl\nNote that although the automaton itself is not necessary for the explanations that ensue, it could in principle be used to check which trace prefix leads to an error state to allow for an explanation when such a state is reached.\nNext we turn to the series of E languages. Since these are all CNLs we will refer to them as CNL0, CNL1, CNL2 and CNL3 respectively. All four languages are similar insofar as they all talk about the same underlying, domain-specific world of states and actions, and they all finite state. At the same time they are somewhat different linguistically."}, {"heading": "3.2 CNL0", "text": "Sentences of the CNL0 language are very simple declarative sentences of the kind that we typically associate with simple predicate-argument structures. In the example shown in Figure 3 here, each sentence has a subject, a verb, and possibly a direct object.\nIn this paper, the mapping between theC language and CLN0 is given extensionally by means of a lexicon that connects the individual transition names with a sentence with a simple and fixed syntactic structure. The lexicon itself is expressed as a finite state transducer, as described in Section 4.1. For more complex systems such an approach might not be practical, and a solution could then be to derive the sentence associated with each transition from more fundamental properties of the underlying machine.\nCNL0 provides for a somewhat na\u00efve explanation of traces using the explanations\nprovided by the domain expert directly."}, {"heading": "3.3 CNL1", "text": "Next we turn to CNL1 which offers some improvements. The main feature of CNL1 is that it is a sequence of paragraphs, where each paragraph is simply a sequence of CNL0 sentences, as shown in Figure 4\nThere are two consequences to this slightly richer structure. One is that it provides the skeleton upon which to hang the numbered steps. This is a presentation issue that arguably increases the naturalness and improves comprehension. The other is that it gives a structural identity to each paragraph that could be exploited in order to attribute certain semantic properties to the associated sequence of actions. For example, we have the notion of correctnesswhich has the potential to figure in explanations. Nevertheless, this property is not actually exploited in CNL1."}, {"heading": "3.4 CNL2", "text": "The main novelty in CNL2, (see Figure 5) in contrast to CNL1, is the use of aggregation to reduce each multi-sentence paragraph to a single, more complex sentence. This is\na technique which is used for removing redundancy (see Dalianis and Hovy [DH93]), yielding texts that are more fluid, more acceptable and generally less prone to being misunderstood by human readers than CNL1-style descriptions.\nThe linguistic renderings resulting from aggregation in CNL2 include:\n1. Punctuation other than full stops\n2. Temporal connectives (\u201cafter\u201d, \u201cthen\u201d, \u201cfinally\u201d)\n3. The use of contrastive conjunctions like \u201cbut\"\n4. Collective terms (\u201ctwice\")"}, {"heading": "3.5 CNL3", "text": "Finally CNL3 (see Figure 6) is considerably more complex, because it not only contains further aggregation but also summarisation.\nIn this example, there are only two sentences. The first sentence not only aggregates the first six sentences, but it also omits some of the information (for example, the the user read from a file, that the user logged out etc.). It also includes the use of certain phrases whose correct interpretation, as mentioned earlier, requires consideration of the context of occurrence as well as use of adverbs (\u201cshe unsuccessfully attempted\") and the use of more complex tenses (\u201cshould not have been allowed\")."}, {"heading": "4 Finite State Generation", "text": "In this section we will look into using finite state CNLs for NLG. This is based on finite state techniques as embodied in xfst (Beesley and Karttunen [BK03]) that has already been used extensively in several other areas of language processing such as computational morphology and light parsing. xfst provides a language for the description of complex transducers together with a compiler and a user interface for running and testing transducers.Our aim is to better understand the tradeoffs involvedbetween producing reasonably natural explanations from traces and the use of the efficient computational machinery described here."}, {"heading": "4.1 Na\u00efve Generation: CNL0", "text": "Just as in Figure 2, our starting point is a regular input language C defined as follows\ndefine SIGMA b|l|g|x|r|w; define C SIGMA*;\nSIGMA is the alphabet of the original FSA and the entire generation mechanism accepts inputs that are arbitrary strings over this alphabet. Strings containing illegal characters yield the empty string and hence, no output.\nCNL0 can be obtained more or less directly via a dictionary which links symbols in\nSIGMA to simple declarative sentences, as follows1:\n1 Some of the syntactically more obscure aspects of this definition have been omitted for the sake\nof clarity.\ndefine SP \" \"; define USR {the SP user}; define DICT b-> [{user} SP {gives} SP {bad} SP {password}],\nl-> [{user} SP {requests} SP {login}], g-> [{user} SP {gives} SP {good} SP {password}], x-> [{user} SP {logs} SP {out}], r-> [{user} SP {reads} SP {from} SP {a} SP {file}], w-> [{user} SP {writes} SP {to} SP {a} SP {file}];\ndefine CNL0 C .o. DICT;\nThe first line defines the space character, and the second the symbol USR. The third defines the dictionary DICT which is implemented as finite state transducer that maps from the individual action symbols to primitive sentences, all of which have the same basic structure. The input sequence is represented as a string\ndefine input {lgrxlblgwwxlgrwxlgxlblblbl};\nTo get the output we compose CNL0with input using the expression ([input .o.\nCNL0]), extract the lower side of the relationwith the l operator ([input .o. CNL0].l). The problem with the generated output is that there are no separators between the sentences. The solution is to compose the input with a transducer sentencesep that inserts a separator.\ninput .o. sentencesep .o. CNL0\nThis turns the input into the following string:\nl.g.r.x.l.b.l.g.w.w.x.l.g.r.w.x.l.g.x.l.b.l.b.l.b.l.\nSuch a string can be made to yield exactly the sentences of CNL0 by arranging for the mapping of the fullstops to insert a space. This is just another transducer that is composed into the pipeline. The result of this process is exactly the text shown in Figure 3."}, {"heading": "4.2 Adding Structural Information: CNL1", "text": "At a simplest level, we can specify how the explanation may be split into an enumerated sequence of paragraphs, aiding the comprehension of the trace explanation. Consider being given the following list of subtrace specifications using regular explanations:\nCorrect login session: lg(r + w)\u2217x. Sequence of incorrect login requests: (lb)\u2217.\nIn CNL1, the main feature is that we will use this information to group text. We will\nassume that the following paragraph definitions are supplied:\ndefine correct l g [r | w]* x; define incorrect [l b]; define group1 correct @-> ... %|, incorrect @-> ... %|;\nThe group1 definition includes a piece of xfst notation that causes a vertical bar to be inserted just after whatever matched the left hand side of the rule, yielding\nlgrx|lb|lgwwx|lgrwx|lgx|lb|lb|lb|l\nAs shown earlier, we can when applied to the input, where the vertical bar is used to\ndelimit paragraphs.\nl.g.r.x.|l.b.|l.g.w.w.x.|l.g.r.w.x.|l.g.x.|l.b.|l.b.|l.b.|l.\nFig. 7. CNL1 representation just prior to lexicalisation\nComposing this with an augmented version of CNL0 that also handles the paragraph breaks yields exactly the paragraph structure of the CNL1 rendering shown in Figure 4. An inherent limitation of this approach is that it is impossible to produce a finite-state transducer that will output a numbering scheme for arbitrary numbers of paragraphs. Our solution is to postprocess the output, and generate, for instance HTML or LATEX output which will handle the enumeration as required.."}, {"heading": "4.3 Adding Aggregation: CNL2", "text": "We can now move on to CNL2. This involves several intermediate stages which are diagrammed below:"}, {"heading": "A: l.g.r.x.|l.b.|l.g.w.w.x.|l.g.r.w.x.|l.g.x.|l.b.|l.b.|l.b.|l.", "text": ""}, {"heading": "B: l,g,r,x.|l,b.|l,g,w,w,x.|l,g,r,w,x.|l,g,x.|l,b.|l,b.|l,b.|l.|", "text": ""}, {"heading": "C: aggregation1", "text": ""}, {"heading": "D: aggregation2", "text": "A is as shown in Figure 7. We must now prepare for aggregation by first replacing all but the paragraph-final fullstops with commas. Because the transducer that achieves this uses the paragraph marker to identify the final fullstop, we must first insert that final paragraph marker as as shown in B. The next two phases of aggregation are best explained with the following example: we wish to transform \u201cthe user requested to login. the user gave a good password. the user logged out.\u201d to the more natural \u201cthe user requested login, gave a good password, and logged out\u201d. The first phase removes the subject (i.e. the phrase \u201c the user\u201d) of all sentences but reinstates the same subject at the beginning of the paragraph. The second inserts an \u201cand\" just before the final verb phrase of each aggregated sentence. In this way we are able to achieve paragraph 2 of the CNL2 example as shown in Figure Similar, surface-oriented techniques can be used to obtain the other paragraphs in Figure 5. Specifically, we have composed rules for inserting the words \u201cafter\", \u201cthen\", \u201ctwice\", \u201canother\", \u201cfinally\" and \u201cand\". However, space limitations prevent us from describing these in full."}, {"heading": "4.4 Adding Abstraction: CNL3", "text": "We note that certain sequences of actions can be combined into a simpler explanation, abstracting away (possibly) irrelevant detail, thus aiding comprehension. For instance, consider the following rules, consisting of (i) a regular expression matching a collection of subtraces which may be explained more concisely; and (ii) a natural language explanation which may replace the detailed text one would obtain from the whole subtrace:\nConsecutive correct login sessions: (lg(r + w)\u2217x)n explained as \u201cThe user successfully logged in n times\u201d. Consecutive correct failed login attempts: (lb)n explained as \u201cThe user unsuccessfully attempted to log in n times\u201d. Correct login sessions interspersed with occasional incorrect one: ((lg(r+w)\u2217x)\u2217 lb(lg(r+w)\u2217x)+)\u2217 explained as\u201cThe user successfully logged in a numberof times, with one off bad logins in between\u201d. Correct login sessions interspersed with occasional incorrect one or two: ((lg(r + w)\u2217x) \u2217 (lb+ lblb)(lg(r+ w)\u2217x))\u2217 explained as \u201cThe user logged in a number of times, interspersed by sequences of one or two bad logins\u201d.\nNote that xfst allows regular expressions that are parametrised for the number of\ntimes a repeated expression matches. For example, the statement\ndefine success3 [l g [r | w]* x]^3;\nachieves the first definition above and associates it with themulticharacter symbolsuccess3. This can be added to the dictionary DICT and associated with the string in much the same way as the strings associated with transitions, as shown above.\nWewill assume that these ruleswill be applied using amaximal length strategy\u2014we prefer a longer match, and in case of a tie, the first rule specified is applied. xfst allows the user to choose between longest and shortest match strategies. Using appropriatexfst rules would result in the description given in Figure 6."}, {"heading": "4.5 Adding Contextuality: CNL4", "text": "To further enrich the generation explanations, we can extend the approach used in the previous section for CNL3, to allow for actions to be described using different terms in different contexts. For example, a logout action when logged in may be described as \u2018the user logged out\u2019, while a logout occuringwhile the user is already logged out would better be described as \u2018the user attempted to log out\u2019. We can use techniques similar to the ones presented in the previous section, using regular expressions to specify contexts in which an action will be described in a particular manner.\nConsider the specification below, in which each action and natural language description pair is accompanied by two regular expressions which have to match with the part of the trace immediately preceding and following the action for that description to be used2:\n2 We use the notation a to signify any single symbol except for a.\nAction Pre Post CNL rendering\nx l x\u2217 \u2013 user logs out\notherwise user attempts to log out\nl \u2013 b the user attempts to log in\notherwise the user logs in\nThis technique can be further extended and refined to deal with repetition of actions as shown below with repeated logins:\nAction Pre Post CNL rendering\nl l b l \u2217\nb user attempts to log in again\n\u2013 b user attempts to log in\nl b l \u2217\n\u2013 user logs in again\notherwise user logs in\nIt is interesting to see how far this approach can be pushed and generalised to allow for the generation of more natural sounding text from the input traces."}, {"heading": "5 Discussion and Conclusions", "text": "In this paperwe have presented preliminary results illustrating howfinite state approaches can be used generate controlled natural language explanations of traces. Although there is still much to be done, the results are promising and it is planned that we use such an approach to allow for the specification of natural language explanations to be used in the runtime verification tool Larva [CPS09].\nTwo problems underlying our task are: (i) the discovery of subsequences that are interesting for the domain in question and (ii) how to turn an interesting subsequence into a natural-sounding explanation. In this paperwe have provided somewhat ad hoc solutions to both these problems. While one can use profiling techniques to discover interesting, or frequently occurring subsequences, clearly there needs to be a strong human input in identifying which of these sequences should be used to abstract and explain traces more effectively. On the other hand, we see that many of the ad hoc solutions adopted to make explanations more natural-sounding may be generalised to work on a wide-range of situations. We envisage that the person building the specification may add hints as to how to improve the explanation, such as the tables shown in Section 4.4 to improve abstraction and the ones given in Section 4.5 to add contextuality.\nGiven that, essentially, we are using regular grammars to specify our natural language generator, the generalisation process to reduce human inputwhile generatingmore natural-sounding text is bound to hit a limit. It is of interest to us, however, to investigate how far these approaches can be taken without resorting to more sophisticated techniques usually applied to language generation."}], "references": [{"title": "Finite State Morphology. Number v. 1 in Studies in computational linguistics", "author": ["K.R. Beesley", "L. Karttunen"], "venue": "CSLI Publications,", "citeRegEx": "Beesley and Karttunen.,? \\Q2003\\E", "shortCiteRegEx": "Beesley and Karttunen.", "year": 2003}, {"title": "Larva \u2014 safer monitoring of real-time java programs (tool paper)", "author": ["Christian Colombo", "Gordon J. Pace", "Gerardo Schneider"], "venue": "In Seventh IEEE International Conference on Software Engineering and Formal Methods (SEFM),", "citeRegEx": "Colombo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Colombo et al\\.", "year": 2009}, {"title": "Aggregation in natural language generation", "author": ["Hercules Dalianis", "Eduard H. Hovy"], "venue": "EWNLG, volume 1036 of Lecture Notes in Computer Science,", "citeRegEx": "Dalianis and Hovy.,? \\Q1993\\E", "shortCiteRegEx": "Dalianis and Hovy.", "year": 1993}, {"title": "A survey and classification of controlled natural languages", "author": ["Tobias Kuhn"], "venue": "Computational Linguistics,", "citeRegEx": "Kuhn.,? \\Q2014\\E", "shortCiteRegEx": "Kuhn.", "year": 2014}, {"title": "Owl simplified english: A finite-state language for ontology editing", "author": ["Richard Power"], "venue": "CNL, volume 7427 of Lecture Notes in Computer Science,", "citeRegEx": "Power.,? \\Q2012\\E", "shortCiteRegEx": "Power.", "year": 2012}, {"title": "Building Natural Language Generation Systems", "author": ["Ehud Reiter", "Robert Dale"], "venue": null, "citeRegEx": "Reiter and Dale.,? \\Q2000\\E", "shortCiteRegEx": "Reiter and Dale.", "year": 2000}, {"title": "Pipelines, templates and transformations: Xml for natural language generation", "author": ["G. Wilcock"], "venue": "Proceedings of the First NLP and XML Workshop, NLPXML 2001EWNLG, Lecture Notes in Computer Science,", "citeRegEx": "Wilcock.,? \\Q2001\\E", "shortCiteRegEx": "Wilcock.", "year": 2001}, {"title": "Simplifying and isolating failure-inducing input", "author": ["Andreas Zeller", "Ralf Hildebrandt"], "venue": "IEEE Trans. Softw. Eng.,", "citeRegEx": "Zeller and Hildebrandt.,? \\Q2002\\E", "shortCiteRegEx": "Zeller and Hildebrandt.", "year": 2002}], "referenceMentions": [], "year": 2017, "abstractText": "An essential element of any verification technique is that of identifying and communicating to the user, system behaviour which leads to a deviation from the expected behaviour. Such behaviours are typically made available as long traces of system actions which would benefit from a natural language explanation of the trace and especially in the context of business logic level specifications. In this paper we present a natural language generation model which can be used to explain such traces. A key idea is that the explanation language is a CNL that is, formally speaking, regular language susceptible transformations that can be expressed with finite state machinery. At the same time it admits various forms of abstraction and simplification which contribute to the naturalness of explanations that are communicated to the user.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}