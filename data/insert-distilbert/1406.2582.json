{"id": "1406.2582", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2014", "title": "Probabilistic ODE Solvers with Runge-Kutta Means", "abstract": "runge - kutta methods are the classic family of solvers for ordinary differential equations ( odes ), and the basis for explaining the state - of - the - second art. like most numerical methods, they return point estimates. we construct a family of probabilistic numerical methods requiring that only instead return a gauss - metric markov process defining a probability signal distribution over the ode solution. in in contrast to prior work, we construct this family such roughly that posterior means match the outputs of the runge - kutta family exactly, thus inheriting their proven good properties., remaining degrees \" of freedom not identified by the match to runge - kutta are chosen carefully such that the posterior probability measure fits beside the observed discrete structure of the ode. suddenly our results shed light on the structure characteristics of runge - hilbert kutta solvers from a new approximate direction, provide a richer, probabilistic output, have low computational cost, and recently raise new research questions.", "histories": [["v1", "Tue, 10 Jun 2014 15:13:24 GMT  (628kb,D)", "https://arxiv.org/abs/1406.2582v1", "18 pages (9 page conference paper, plus supplements)"], ["v2", "Fri, 24 Oct 2014 11:45:49 GMT  (866kb,D)", "http://arxiv.org/abs/1406.2582v2", "18 pages (9 page conference paper, plus supplements); appears in Advances in Neural Information Processing Systems (NIPS), 2014"]], "COMMENTS": "18 pages (9 page conference paper, plus supplements)", "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NA math.NA", "authors": ["michael schober", "david k duvenaud", "philipp hennig"], "accepted": true, "id": "1406.2582"}, "pdf": {"name": "1406.2582.pdf", "metadata": {"source": "CRF", "title": "Probabilistic ODE Solvers with Runge-Kutta Means", "authors": ["Michael Schober", "David Duvenaud", "Philipp Hennig"], "emails": ["mschober@tue.mpg.de", "dkd23@cam.ac.uk", "phennig@tue.mpg.de"], "sections": [{"heading": "1 Introduction", "text": "Differential equations are a basic feature of dynamical systems. Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8]. Here we address the latter, classic numerical problem. Runge-Kutta (RK) methods [9, 10] are standard tools for this purpose. Over more than a century, these algorithms have matured into a very well-understood, efficient framework [11].\nAs recently pointed out by Hennig and Hauberg [6], since Runge-Kutta methods are linear extrapolation methods, their structure can be emulated by Gaussian process (GP) regression algorithms. Such an algorithm was envisioned by Skilling in 1991 [5], and the idea has recently attracted both theoretical [8] and practical [6, 7] interest. By returning a posterior probability measure over the solution of the ODE problem, instead of a point estimate, Gaussian process solvers extend the functionality of RK solvers in ways that are particularly interesting for machine learning. Solution candidates can be drawn from the posterior and marginalized [7]. This can allow probabilistic solvers to stop earlier, and to deal (approximately) with probabilistically uncertain inputs and problem definitions [6]. However, current GP ODE solvers do not share the good theoretical convergence properties of Runge-Kutta methods. Specifically, they do not have high polynomial order, explained below.\nWe construct GP ODE solvers whose posterior mean functions exactly match those of the RK families of first, second and third order. This yields a probabilistic numerical method which combines the strengths of Runge-Kutta methods with the additional functionality of GP ODE solvers. It also provides a new interpretation of the classic algorithms, raising new conceptual questions.\nWhile our algorithm could be seen as a \u201cBayesian\u201d version of the Runge-Kutta framework, a philosophically less loaded interpretation is that, where Runge-Kutta methods fit a single curve (a point estimate) to an IVP, our algorithm fits a probability distribution over such potential solutions, such that the mean of this distribution matches the Runge-Kutta estimate exactly. We find a family of models in the space of Gaussian process linear extrapolation methods with this property, and select a member of this family (fix the remaining degrees of freedom) through statistical estimation.\nar X\niv :1\n40 6.\n25 82\nv2 [\nst at\n.M L\n] 2\n4 O\nct 2"}, {"heading": "2 Background", "text": "An ODE Initial Value Problem (IVP) is to find a function x(t) \u2236 R \u2192 RN such that the ordinary differential equation x\u0307 = f(x, t) (where x\u0307 = \u2202x/\u2202t) holds for all t \u2208 T = [t0, tH], and x(t0) = x0. We assume that a unique solution exists. To keep notation simple, we will treat x as scalar-valued; the multivariate extension is straightforward (it involves N separate GP models, explained in supp.).\nRunge-Kutta methods1 [9, 10] are carefully designed linear extrapolation methods operating on small contiguous subintervals [tn, tn + h] \u2282 T of length h. Assume for the moment that n = 0. Within [t0, t0 +h], an RK method of stage s collects evaluations yi = f(x\u0302i, t0 +hci) at s recursively defined input locations, i = 1, . . . , s, where x\u0302i is constructed linearly from the previously-evaluated yj<i as\nx\u0302i = x0 + h i\u22121\n\u2211 j=1 wijyj , (1)\nthen returns a single prediction for the solution of the IVP at t0 + h, as x\u0302(t0 + h) = x0 + h\u2211si=1 biyi (modern variants can also construct non-probabilistic error estimates, e.g. by combining the same observations into two different RK predictions [12]). In compact form,\nyi = f \u239b \u239d x0 + h i\u22121 \u2211 j=1 wijyj , t0 + hci \u239e \u23a0 , i = 1, . . . , s, x\u0302(t0 + h) = x0 + h s \u2211 i=1 biyi. (2)\nx\u0302(t0 +h) is then taken as the initial value for t1 = t0 +h and the process is repeated until tn +h \u2265 tH . A Runge-Kutta method is thus identified by a lower-triangular matrix W = {wij}, and vectors c = [c1, . . . , cs], b = [b1, . . . , bs], often presented compactly in a Butcher tableau [13]:\nc1 0 c2 w21 0 c3 w31 w32 0 \u22ee \u22ee \u22ee \u22f1 \u22f1 cs ws1 ws2 \u22ef ws,s\u22121 0\nb1 b2 \u22ef bs\u22121 bs As Hennig and Hauberg [6] recently pointed out, the linear structure of the extrapolation steps in Runge-Kutta methods means that their algorithmic structure, the Butcher tableau, can be constructed naturally from a Gaussian process regression method over x(t), where the yi are treated as \u201cobservations\u201d of x\u0307(t0 + hci) and the x\u0302i are subsequent posterior estimates (more below). However, proper RK methods have structure that is not generally reproduced by an arbitrary Gaussian process prior on x: Their distinguishing property is that the approximation x\u0302 and the Taylor series of the true solution coincide at t0 + h up to the p-th term\u2014their numerical error is bounded by \u2223\u2223x(t0+h)\u2212 x\u0302(t0+h)\u2223\u2223 \u2264Khp+1 for some constant K (higher orders are better, because h is assumed to be small). The method is then said to be of order p [11]. A method is consistent, if it is of order p = s. This is only possible for p < 5 [14, 15]. There are no methods of order p > s. High order is a strong desideratum for ODE solvers, not currently offered by Gaussian process extrapolators.\nTable 1 lists all consistent methods of order p \u2264 3 where s = p. For s = 1, only Euler\u2019s method (linear extrapolation) is consistent. For s = 2, there exists a family of methods of order p = 2, parametrized\n1In this work, we only address so-called explicit RK methods (shortened to \u201cRunge-Kutta methods\u201d for simplicity). These are the base case of the extensive theory of RK methods. Many generalizations can be found in [11]. Extending the probabilistic framework discussed here to the wider Runge-Kutta class is not trivial.\nby a single parameter \u03b1 \u2208 (0,1], where \u03b1 = 1/2 and \u03b1 = 1 mark the midpoint rule and Heun\u2019s method, respectively. For s = 3, third order methods are parameterized by two variables u, v \u2208 (0,1]. Gaussian processes (GPs) are well-known in the NIPS community, so we omit an introduction. We will use the standard notation \u00b5 \u2236 R \u2192 R for the mean function, and k \u2236 R \u00d7 R \u2192 R for the covariance function; kUV for Gram matrices of kernel values k(ui, vj), and analogous for the mean function: \u00b5T = [\u00b5(t1), . . . , \u00b5(tN)]. A GP prior p(x) = GP(x;\u00b5, k) and observations (T,Y ) = {(t1, y1), . . . , (ts, ys)} having likelihood N (Y ;xT ,\u039b) give rise to a posterior GPs(x;\u00b5s, ks) with\n\u00b5st = \u00b5t + ktT (kTT +\u039b)\u22121(Y \u2212 \u00b5T ) and ksuv = kuv \u2212 kuT (kTT +\u039b)\u22121kTv. (3)\nGPs are closed under linear maps. In particular, the joint distribution over x and its derivative is\np [(x x\u0307 )] = GP [(x x\u0307 ) ;( \u00b5 \u00b5\u2202 ) ,( k k\n\u2202\nk\u2202 k\u2202 \u2202 )] (4)\nwith \u00b5\u2202 = \u2202\u00b5(t) \u2202t , k\u2202 = \u2202k(t, t \u2032) \u2202t\u2032 , k\u2202 = \u2202k(t, t \u2032) \u2202t , k\u2202 \u2202 = \u2202 2k(t, t\u2032) \u2202t\u2202t\u2032 . (5)\nA recursive algorithm analogous to RK methods can be constructed [5, 6] by setting the prior mean to the constant \u00b5(t) = x0, then recursively estimating x\u0302i in some form from the current posterior over x. The choice in [6] is to set x\u0302i = \u00b5i(t0 + hci). \u201cObservations\u201d yi = f(x\u0302i, t0 + hci) are then incorporated with likelihood p(yi \u2223x) = N (yi; x\u0307(t0 + hci),\u039b). This recursively gives estimates\nx\u0302(t0 + hci) = x0 + i\u22121\n\u2211 j=1\ni\u22121 \u2211 `=1 k\u2202(t0 + hci, t0 + hc`)( K\u2202 \u2202 +\u039b)\u22121`j yj = x0 + h\u2211 j wijyj , (6)\nwith K\u2202 \u2202ij = k\u2202 \u2202(t0 + hci, t0 + hcj). The final prediction is the posterior mean at this point:\nx\u0302(t0 + h) = x0 + s\n\u2211 i=1\ns\n\u2211 j=1\nk\u2202(t0 + h, t0 + hcj)( K\u2202 \u2202 +\u039b)\u22121ji yi = x0 + h s\n\u2211 i biyi. (7)"}, {"heading": "3 Results", "text": "The described GP ODE estimate shares the algorithmic structure of RK methods (i.e. they both use weighted sums of the constructed estimates to extrapolate). However, in RK methods, weights and evaluation positions are found by careful analysis of the Taylor series of f , such that low-order terms cancel. In GP ODE solvers they arise, perhaps more naturally but also with less structure, by the choice of the ci and the kernel. In previous work [6, 7], both were chosen ad hoc, with no guarantee of convergence order. In fact, as is shown in the supplements, the choices in these two works\u2014square-exponential kernel with finite length-scale, evaluations at the predictive mean\u2014do not even give the first order convergence of Euler\u2019s method. Below we present three specific regression models based on integrated Wiener covariance functions and specific evaluation points. Each model is the improper limit of a Gauss-Markov process, such that the posterior distribution after s evaluations is a proper Gaussian process, and the posterior mean function at t0 + h coincides exactly with the Runge-Kutta estimate. We will call these methods, which give a probabilistic interpretation to RK methods and extend them to return probability distributions, Gauss-Markov-Runge-Kutta (GMRK) methods, because they are based on Gauss-Markov priors and yield Runge-Kutta predictions."}, {"heading": "3.1 Design choices and desiderata for a probabilistic ODE solver", "text": "Although we are not the first to attempt constructing an ODE solver that returns a probability distribution, open questions still remain about what, exactly, the properties of such a probabilistic numerical method should be. Chkrebtii et al. [8] previously made the case that Gaussian measures are uniquely suited because solution spaces of ODEs are Banach spaces, and provided results on consistency. Above, we added the desideratum for the posterior mean to have high order, i.e. to reproduce the Runge-Kutta estimate. Below, three additional issues become apparent:\nMotivation of evaluation points Both Skilling [5] and Hennig and Hauberg [6] propose to put the \u201cnodes\u201d x\u0302(t0 + hci) at the current posterior mean of the belief. We will find that this can be made\nconsistent with the order requirement for the RK methods of first and second order. However, our third-order methods will be forced to use a node x\u0302(t0 + hci) that, albeit lying along a function w(t) in the reproducing kernel Hilbert space associated with the posterior GP covariance function, is not the mean function itself. It will remain open whether the algorithm can be amended to remove this blemish. However, as the nodes do not enter the GP regression formulation, their choice does not directly affect the probabilistic interpretation.\nExtension beyond the first extrapolation interval Importantly, the Runge-Kutta argument for convergence order only holds strictly for the first extrapolation interval [t0, t0 + h]. From the second interval onward, the RK step solves an estimated IVP, and begins to accumulate a global estimation error not bounded by the convergence order (an effect termed \u201cLady Windermere\u2019s fan\u201d by Wanner [16]). Should a probabilistic solver aim to faithfully reproduce this imperfect chain of RK solvers, or rather try to capture the accumulating global error? We investigate both options below.\nCalibration of uncertainty A question easily posed but hard to answer is what it means for the probability distribution returned by a probabilistic method to be well calibrated. For our Gaussian case, requiring RK order in the posterior mean determines all but one degree of freedom of an answer. The remaining parameter is the output scale of the kernel, the \u201cerror bar\u201d of the estimate. We offer a relatively simple statistical argument below that fits this parameter based on observed values of f .\nWe can now proceed to the main results. In the following, we consider extrapolation algorithms based on Gaussian process priors with vanishing prior mean function, noise-free observation model (\u039b = 0 in Eq. (3)). All covariance functions in question are integrals over the kernel k0(t\u0303, t\u0303\u2032) = \u03c32 min(t\u0303 \u2212 \u03c4, t\u0303\u2032 \u2212 \u03c4) (parameterized by scale \u03c32 > 0 and off-set \u03c4 \u2208 R; valid on the domain t\u0303, t\u0303\u2032 > \u03c4 ), the covariance of the Wiener process [17]. Such integrated Wiener processes are Gauss-Markov processes, of increasing order, so inference in these methods can be performed by filtering, at linear cost [18]. We will use the shorthands t = t\u0303 \u2212 \u03c4 and t\u2032 = t\u0303\u2032 \u2212 \u03c4 for inputs shifted by \u03c4 ."}, {"heading": "3.2 Gauss-Markov methods matching Euler\u2019s method", "text": "Theorem 1. The once-integrated Wiener process prior p(x) = GP(x; 0, k1) with\nk1(t, t\u2032) =\u222c t\u0303,t\u0303\u2032\n\u03c4 k0(u, v)dudv = \u03c32 (min 3(t, t\u2032) 3 + \u2223t \u2212 t\u2032\u2223min 2(t, t\u2032) 2 ) (8)\nchoosing evaluation nodes at the posterior mean gives rise to Euler\u2019s method.\nProof. We show that the corresponding Butcher tableau from Table 1 holds. After \u201cobserving\u201d the initial value, the second observation y1, constructed by evaluating f at the posterior mean at t0, is\ny1 = f (\u00b5\u2223x0(t0), t0) = f ( k(t0, t0) k(t0, t0) x0, t0) = f(x0, t0), (9)\ndirectly from the definitions. The posterior mean after incorporating y1 is\n\u00b5\u2223x0,y1(t0 + h) = [k(t0 + h, t0) k\u2202(t0 + h, t0)] [ k(t0, t0) k\u2202(t0, t0) k\u2202(t0, t0) k\u2202 \u2202(t0, t0) ] \u22121 (x0 y1 ) = x0 + hy1.\n(10) An explicit linear algebraic derivation is available in the supplements."}, {"heading": "3.3 Gauss-Markov methods matching all Runge-Kutta methods of second order", "text": "Extending to second order is not as straightforward as integrating the Wiener process a second time. The theorem below shows that this only works after moving the onset \u2212\u03c4 of the process towards infinity. Fortunately, this limit still leads to a proper posterior probability distribution. Theorem 2. Consider the twice-integrated Wiener process prior p(x) = GP(x; 0, k2) with\nk2(t, t\u2032) =\u222c t\u0303,t\u0303\u2032\n\u03c4 k1(u, v)dudv = \u03c32 (min 5(t, t\u2032) 20 + \u2223t \u2212 t \u2032\u2223 12 ((t + t\u2032)min3(t, t\u2032) \u2212 min 4(t, t\u2032) 2 )) .\n(11) Choosing evaluation nodes at the posterior mean gives rise to the RK family of second order methods in the limit of \u03c4 \u2192\u221e.\n(The twice-integrated Wiener process is a proper Gauss-Markov process for all finite values of \u03c4 and t\u0303, t\u0303\u2032 > 0. In the limit of \u03c4 \u2192\u221e, it turns into an improper prior of infinite local variance.)\nProof. The proof is analogous to the previous one. We need to show all equations given by the Butcher tableau and choice of parameters hold for any choice of \u03b1. The constraint for y1 holds trivially as in Eq. (9). Because y2 = f(x0 + h\u03b1y1, t0 + h\u03b1), we need to show \u00b5\u2223x0,y1(t0 + h\u03b1) = x0 + h\u03b1y1. Therefore, let \u03b1 \u2208 (0,1] arbitrary but fixed:\n\u00b5\u2223x0,y1(t0 + h\u03b1) = [k(t0 + h, t0) k\u2202(t0 + h, t0)] [ k(t0, t0) k\u2202(t0, t0) k\u2202 (t0, t0) k\u2202 \u2202(t0, t0) ] \u22121 (x0 y1 )\n= [ t 3 0(10(h\u03b1) 2 +15h\u03b1t0+6t 2 0)\n120\nt20(6(h\u03b1) 2 +8h\u03b1t0+3t 2 0) 24 ] [ t50/20 t40/8 t40/8 t30/3]\n\u22121\n(x0 y1 )\n= [1 \u2212 10(h\u03b1) 2\n3t20 h\u03b1 + 2(h\u03b1)\n2 t0 ] (x0 y1 )\n\u00d0\u00d0\u00d0\u2192 \u03c4\u2192\u221e x0 + h\u03b1y1 (12)\nAs t0 = t\u03030 \u2212 \u03c4 , the mismatched terms vanish for \u03c4 \u2192\u221e. Finally, extending the vector and matrix with one more entry, a lengthy computation shows that lim\u03c4\u2192\u221e \u00b5\u2223x0,y1,y2(t0 + h) = x0 + h(1 \u2212 1/2\u03b1)y1 + h/2\u03b1y2 also holds, analogous to Eq. (10). Omitted details can be found in the supplements. They also include the final-step posterior covariance. Its finite values mean that this posterior indeed defines a proper GP."}, {"heading": "3.4 A Gauss-Markov method matching Runge-Kutta methods of third order", "text": "Moving from second to third order, additionally to the limit towards an improper prior, also requires a departure from the policy of placing extrapolation nodes at the posterior mean. Theorem 3. Consider the thrice-integrated Wiener process prior p(x) = GP(x; 0, k3) with\nk3(t, t\u2032) =\u222c t\u0303,t\u0303\u2032\n\u03c4 k2(u, v)dudv\n= \u03c32 (min 7(t, t\u2032) 252 + \u2223t \u2212 t \u2032\u2223min4(t, t\u2032) 720 (5 max2(t, t\u2032) + 2tt\u2032 + 3 min2(t, t\u2032))) .\n(13)\nEvaluating twice at the posterior mean and a third time at a specific element of the posterior covariance functions\u2019 RKHS gives rise to the entire family of RK methods of third order, in the limit of \u03c4 \u2192\u221e.\nProof. The proof progresses entirely analogously as in Theorems 1 and 2, with one exception for the term where the mean does not match the RK weights exactly. This is the case for y3 = x0 + h[(v \u2212 v(v\u2212u)/u(2\u22123u))y1 + v(v\u2212u)/u(2\u22123u)y2] (see Table 1). The weights of Y which give the posterior mean at this point are given by kK\u22121 (cf. Eq. (3), which, in the limit, has value (see supp.):\nlim \u03c4\u2192\u221e\n[k(t0 + hv, t0) k\u2202(t0 + hv, t0) k\u2202(t0 + hv, t0 + hu)]K\u22121\n= [1 h(v \u2212 v 2\n2u ) h v\n2\n2u ]\n= [1 h (v \u2212 v(v\u2212u) u(2\u22123u) \u2212v(3v\u22122) 2(3u\u22122) ) h ( v(v\u2212u) u(2\u22123u) +v(3v\u22122) 2(3u\u22122) )]\n= [1 h (v \u2212 v(v\u2212u) u(2\u22123u) ) h ( v(v\u2212u) u(2\u22123u) )] + [0 \u2212h v(3v\u22122) 2(3u\u22122) h v(3v\u22122) 2(3u\u22122) ] (14)\nThis means that the final RK evaluation node does not lie at the posterior mean of the regressor. However, it can be produced by adding a correction term w(v) = \u00b5(v) + \u03b5(v)(y2 \u2212 y1) where\n\u03b5(v) = v 2 3v \u2212 2 3u \u2212 2\n(15)\nis a second-order polynomial in v. Since k is of third or higher order in v (depending on the value of u), w can be written as an element of the thrice integrated Wiener process\u2019 RKHS [19, \u00a76.1]. Importantly, the final extrapolation weights b under the limit of the Wiener process prior again match the RK weights exactly, regardless of how y3 is constructed.\nWe note in passing that Eq. (15) vanishes for v = 2/3. For this choice, the RK observation y2 is generated exactly at the posterior mean of the Gaussian process. Intriguingly, this is also the value for \u03b1 for which the posterior variance at t0 + h is minimized."}, {"heading": "3.5 Choosing the output scale", "text": "The above theorems have shown that the first three families of Runge-Kutta methods can be constructed from repeatedly integrated Wiener process priors, giving a strong argument for the use of such priors in probabilistic numerical methods. However, requiring this match to a specific Runge-Kutta family in itself does not yet uniquely identify a particular kernel to be used: The posterior mean of a Gaussian process arising from noise-free observations is independent of the output scale (in our notation: \u03c32) of the covariance function (this can also be seen by inspecting Eq. (3)). Thus, the parameter \u03c32 can be chosen independent of the other parts of the algorithm, without breaking the match to Runge-Kutta. Several algorithms using the observed values of f to choose \u03c32 without major cost overhead have been proposed in the regression community before [e.g. 20, 21]. For this particular model an even more basic rule is possible: A simple derivation shows that, in all three families of methods defined above, the posterior belief over \u2202sx/\u2202ts is a Wiener process, and the posterior mean function over the s-th derivative after all s steps is a constant function. The Gaussian model implies that the expected distance of this function from the (zero) prior mean should be the marginal standard deviation \u221a \u03c32. We choose \u03c32 such that this property is met, by setting \u03c32 = [\u2202s\u00b5s(t)/\u2202ts]2.\nFigure 1 shows conceptual sketches highlighting the structure of GMRK methods. Interestingly, in both the second- and third-order families, our proposed priors are improper, so the solver can not actually return a probability distribution until after the observation of all s gradients in the RK step.\nSome observations We close the main results by highlighting some non-obvious aspects. First, it is intriguing that higher convergence order results from repeated integration of Wiener processes. This repeated integration simultaneously adds to and weakens certain prior assumptions in the implicit (improper) Wiener prior: s-times integrated Wiener processes have marginal variance ks(t, t) \u221d t2s+1. Since many ODEs (e.g. linear ones) have solution paths of values O(exp(t)), it is tempting to wonder whether there exists a limit process of \u201cinfinitely-often integrated\u201d Wiener processes giving natural coverage to this domain (the results on a linear ODE in Figure 1 show how the polynomial posteriors cannot cover the exponentially diverging true solution). In this context,\nit is also noteworthy that s-times integrated Wiener priors incorporate the lower-order results for s\u2032 < s, so \u201chighly-integrated\u201d Wiener kernels can be used to match finite-order Runge-Kutta methods. Simultaneously, though, sample paths from an s-times integrated Wiener process are almost surely s-times differentiable. So it seems likely that achieving good performance with a Gauss-MarkovRunge-Kutta solver requires trading off the good marginal variance coverage of high-order Markov models (i.e. repeatedly integrated Wiener processes) against modelling non-smooth solution paths with lower degrees of integration. We leave this very interesting question for future work."}, {"heading": "4 Experiments", "text": "Since Runge-Kutta methods have been extensively studied for over a century [11], it is not necessary to evaluate their estimation performance again. Instead, we focus on an open conceptual question for the further development of probabilistic Runge-Kutta methods: If we accept high convergence order as a prerequisite to choose a probabilistic model, how should probabilistic ODE solvers continue after the first s steps? Purely from an inference perspective, it seems unnatural to introduce new evaluations of x (as opposed to x\u0307) at t0 + nh for n = 1,2, . . . . Also, with the exception of the Euler case, the posterior covariance after s evaluations is of such a form that its renewed use in the next interval will not give Runge-Kutta estimates. Three options suggest themselves:\nNa\u00efve Chaining One could simply re-start the algorithm several times as if the previous step had created a novel IVP. This amounts to the classic RK setup. However, it does not produce a joint \u201cglobal\u201d posterior probability distribution (Figure 2, left column).\nSmoothing An ad-hoc remedy is to run the algorithm in the \u201cNa\u00efve chaining\u201d mode above, producing N \u00d7 s gradient observations and N function evaluations, but then compute a joint posterior distribution by using the first s gradient observations and 1 function evaluation as described in Section 3, then using the remaining s(N \u2212 1) gradients and (N \u2212 1) function values as in standard GP inference. The appeal of this approach is that it produces a GP posterior whose mean goes through the RK points (Figure 2, center column). But from a probabilistic standpoint it seems contrived. In particular, it produces a very confident posterior covariance, which does not capture global error.\nContinuing after s evaluations Perhaps most natural from the probabilistic viewpoint is to break with the RK framework after the first RK step, and simply continue to collect gradient observations\u2014 either at RK locations, or anywhere else. The strength of this choice is that it produces a continuously growing marginal variance (Figure 2, right). One may perceive the departure from the established RK paradigm as problematic. However, we note again that the core theoretical argument for RK methods is only strictly valid in the first step, the argument for iterative continuation is a lot weaker.\nFigure 2 shows exemplary results for these three approaches on the (stiff) linear IVP x\u0307(t) = \u22121/2x(t), x(0) = 1. Na\u00efve chaining does not lead to a globally consistent probability distribution. Smoothing does give this global distribution, but the \u201cobservations\u201d of function values create unnatural nodes of certainty in the posterior. The probabilistically most appealing mode of continuing inference directly offers a naturally increasing estimate of global error. At least for this simple test case, it also happens to work better in practice (note good match to ground truth in the plots). We have found similar results for other test cases, notably also for non-stiff linear differential equations. But of course, probabilistic continuation breaks with at least the traditional mode of operation for Runge-Kutta methods, so a closer theoretical evaluation is necessary, which we are planning for a follow-up publication.\nComparison to Square-Exponential kernel Since all theoretical guarantees are given in forms of upper bounds for the RK methods, the application of different GP models might still be favorable in practice. We compared the continuation method from Fig. 2 (right column) to the ad-hoc choice of a square-exponential (SE) kernel model, which was used by Hennig and Hauberg [6] (Fig. 3). For this test case, the GMRK method surpasses the SE-kernel algorithm both in accuracy and calibration: its mean is closer to the true solution than the SE method, and its error bar covers the true solution, while the SE method is over-confident. This advantage in calibration is likely due to the more natural choice of the output scale \u03c32 in the GMRK framework."}, {"heading": "5 Conclusions", "text": "We derived an interpretation of Runge-Kutta methods in terms of the limit of Gaussian process regression with integrated Wiener covariance functions, and a structured but nontrivial extrapolation model. The result is a class of probabilistic numerical methods returning Gaussian process posterior distributions whose means can match Runge-Kutta estimates exactly.\nThis class of methods has practical value, particularly to machine learning, where previous work has shown that the probability distribution returned by GP ODE solvers adds important functionality over those of point estimators. But these results also raise pressing open questions about probabilistic ODE solvers. This includes the question of how the GP interpretation of RK methods can be extended beyond the 3rd order, and how ODE solvers should proceed after the first stage of evaluations."}, {"heading": "Acknowledgments", "text": "The authors are grateful to Simo S\u00e4rkk\u00e4 for a helpful discussion."}, {"heading": "A Multivariate extension", "text": "The GMRK model can be extended to the multivariate case analogously to Runge-Kutta methods. All equations in the RK framework also work with vector-valued function values, and all derivations presented in the paper and in this supplement carry over to the non-scalar case without modification: Consider dimension j \u2208 {1, . . . ,N}. The projected outputs are the same as if j were an independent one-dimensional problem, which can be modeled with a separate Gaussian process. For a joint notation, vectorize the N dimensions with a Kronecker product: if k(t, t\u2032) is an one-dimensional covariance function, the function\nk\u0304(t, t\u2032) =Dijk(ti, t\u2032j), (16)\nwhere D is an N \u00d7N positive semi-definite matrix, defines a covariance function over N dimensions, if t and t\u2032 are N dimensional. Choosing D = I results in a N -dimensional GP model where output dimensions are independent of each other as required."}, {"heading": "B Covariance functions of integrated Wiener processes", "text": "It was observed that integrated Wiener processes generate RK methods of various order with higher number of integrations leading to RK methods of higher order.\nHere we present the derivation of the covariance functions of the applied Wiener process kernels as well as their derivatives as needed.\nB.1 The once integrated Wiener process\nThe Wiener process covariance function is given by\nkWP (t, t\u2032) = \u03c32 min(t, t\u2032) (17)\nIt is only defined for t, t\u2032 > 0. Integration with respect to both arguments leads to the once integrated Wiener process which is once differentiable. Its covariance function is\nk1(t, t\u2032) = \u222b t\n0 du\u222b\nt\u2032\n0 dv \u03c32 min(u, v)\n= \u03c32 \u222b t\n0 du\u222b\nt\u2032\n0 dv min(u, v)\nt>t\u2032= \u03c32 (\u222b t\nt\u2032 du\u222b\nt\u2032\n0 dv v + 2\u222b\nt\u2032\n0 du\u222b\nu\n0 dv v)\n= \u03c32 (\u222b t\nt\u2032 du\n1 2 t\u20322 + 2\u222b\nt\u2032\n0 du\n1 2 u2)\n= \u03c32 (1 2 (t \u2212 t\u2032)t\u20322 + 1 3 (t\u20323))\n= \u03c32 (min 3(t, t\u2032) 3 + \u2223t \u2212 t\u2032\u2223min 2(t, t\u2032) 2 ) (18)\nwhere t, t\u2032 were replaced with min(t, t\u2032) and max(t, t\u2032) at the last step.\n2http://probabilistic-numerics.org/ODEs.html\nThe necessary derivatives of this kernel are\nk\u2202(t, t\u2032) = \u03c32 \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 t < t\u2032 \u2236 t 2 2 t > t\u2032 \u2236 (tt\u2032 \u2212 t \u20322 2 )\n(19)\nk\u2202 \u2202(t, t\u2032) = \u03c32 min(t, t\u2032) = kWP (t, t\u2032). (20)\nB.2 The twice integrated Wiener process\nIterating this process leads to the twice integrated Wiener process. Its covariance function is\nk2(t, t\u2032) = \u03c32 (\u222b t\n0 du\u222b\nt\u2032\n0 dv min3(u, v) 3 + \u2223u \u2212 v\u2223min 2(u, v) 2 )\nt>t\u2032= \u03c32 ((\u222b t\nt\u2032 du\u222b\nt\u2032\n0 dv (u \u2212 v)v\n2\n2 + v\n3\n3 ) + (\u222b\nt\u2032\n0 du\u222b\nu\n0 dv (u \u2212 v)v\n2\n2 + v\n3\n3 )\n+ (\u222b v\n0 du\u222b\nt\u2032\n0 dv (v \u2212 u)u\n2\n2 + u\n3\n3 ))\n\u22ee\n= \u03c32 (min 5(t, t\u2032) 20 + \u2223t \u2212 t \u2032\u2223 12 ((t + t\u2032)min3(t, t\u2032) \u2212 min 4(t, t\u2032) 2 )) (21)\nDerivatives of this kernel are\nk\u2202(t, t\u2032) = \u03c32 \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 t > t\u2032 \u2236 ( t \u20322 24 (t\u20322 \u2212 4tt\u2032 + 6t2)) t \u2264 t\u2032 \u2236 (\u2212 t 4 24 + t \u2032t3 6 ) (22)\nk\u2202 \u2202(t, t\u2032) = \u03c32 (min 3(t, t\u2032) 3 + \u2223t \u2212 t\u2032\u2223min 2(t, t\u2032) 2 ) = k1(t, t\u2032). (23)\nB.3 The thrice integrated Wiener process\nSimilarly, the thrice integrated Wiener process is obtained by\nk3(t, t\u2032) = \u03c32 (\u222b t\n0 du\u222b\nt\u2032\n0 dv min5(u, v) 20 + \u2223u \u2212 v\u2223 12 ((u + v)min3(u, v) \u2212 min 4(u, v) 2 ))\n\u22ee\n= \u03c32 (min 7(t, t\u2032) 252 + \u2223t \u2212 t \u2032\u2223min4(t, t\u2032) 720 (5 max2(t, t\u2032) + 2tt\u2032 + 3 min2(t, t\u2032))) (24)\nOmitted steps are similar as in the derivation of (18) and (21).\nIts derivatives are given by\nk\u2202(t, t\u2032) = \u03c32 \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 t > t\u2032 \u2236 ( t \u20323 720 (20t3 \u2212 15t2t\u2032 + 6tt\u20322 \u2212 t\u20323)) t \u2264 t\u2032 \u2236 ( t 4 720 (15t\u20322 \u2212 6tt\u2032 + t2))\n(25)\nk\u2202 \u2202(t, t\u2032) = \u03c32 (min 5(t, t\u2032) 20 + \u2223t \u2212 t \u2032\u2223 12 ((t + t\u2032)min3(t, t\u2032) \u2212 min 4(t, t\u2032) 2 )) = k2(t, t\u2032). (26)"}, {"heading": "C Posterior predictive GP distributions", "text": "In order to build GMRK methods it is necessary to compute closed forms of the resulting posterior mean and covariance functions after s evaluations. Forms are given below. In cases where a derivation is omitted, results were obtained with MATLAB\u2019s Symbolic Math Toolbox. Code is available online.\nC.1 Posterior predictive mean and covariance functions of the once integrated WP\nBelow are the formulas of the posterior mean and covariance of the once integrated WP after each step.\n\u00b5\u2223x0(t) = k(t, t0) k(t0, t0) x0\n= t30/3 + \u2223t \u2212 t0\u2223t20/2\nt30/3 x0\n= (1 + \u2223t \u2212 t0\u2223 3\n2t0 )x0 (27)\n\u00b5\u2223x0,y1(t) = [k(t, t0) k\u2202(t, t0)] [ k(t0, t0) k\u2202(t0, t0) k\u2202 (t0, t0) k\u2202 \u2202(t0, t0) ] \u22121\n\u00b4\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b8\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b6 =\u2236K\n(x0 y1 )\n= \u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9 t \u2265 t0 \u2236 1\u2223K\u2223 [t 2t0/2 \u2212 t3/6 t2/2] [ t0 \u2212 t20/2 \u2212t20/2 t30/3 ]( x0 y1 ) t < t0 \u2236 1\u2223K\u2223 [tt 2 0/2 \u2212 t30/6 tt0 \u2212 t20/2] [ t0 \u2212t20/2 \u2212t20/2 t30/3 ]( x0 y1 )\n\u22ee\n= \u23a7\u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 t \u2265 t0 \u2236 x0 + (t \u2212 t0)y1 t < t0 \u2236 3t 2t0\u22122t 3 t30 x0 \u2212 t 2t0\u2212t 3 t20 y1\n(28)\nEqs. (27) and (28) also complete the proof of Theorem 1 by observing that y1 = f(x0, t0) = f(\u00b5\u2223x0(t0), t0) and x1 = x0 + hy1 = \u00b5\u2223x0,y1(t0 + h), which match Euler\u2019s method.\nWithout loss of generality, we can assume that t\u2032 \u2264 t. With this convention the posterior covariances functions are\nk\u2223x0(t, t \u2032) = k1(t, t\u2032) \u2212 k 1(t, t0)k1(t0, t\u2032) k1(t0, t0)\n=(min 3(t, t\u2032) 3 + \u2223t \u2212 t\u2032\u2223min 2(t, t\u2032) 2 )\n\u2212 1 24t30\n(min2(t, t0)min2(t0, t\u2032)(t + t0 + 2\u2223t \u2212 t0\u2223)(t\u2032 + t0 + 2\u2223t\u2032 \u2212 t0\u2223)) (29)\nk\u2223x0,y1(t, t \u2032) = k1(t, t\u2032) \u2212 [k(t, t0) k\u2202(t, t0)]K\u22121 [ k(t0, t\u2032) k\u2202 (t0, t\u2032) ]\n\u22ee\n= \u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9 t, t\u2032 > t0 \u2236 (t0\u2212t \u2032 ) 2 (3t\u2212t\u2032\u22122t0) 6 t > t0 \u2265 t\u2032 \u2236 0 t, t\u2032 \u2264 t0 \u2236 t \u20322 (t0\u2212t) 2 (3tt0\u2212t \u2032t0\u22122tt\u2032) 6t30\n(30)\nC.2 Predictive mean and covariance of the twice integrated WP\nBelow are the formulas for the posterior mean for the twice integrated WP and the generic 2-stage RK method. Throughout, we write \u00b5(t) = \u00b5(t0 + s) with appropriate s \u2208 R, which will simplify formulas significantly. Furthermore, we omit stating the generating formulas and intermediate steps as the former are analogous to the ones in Sec. C.1 and the latter were performed with MATLAB\u2019s\nSymbolic Math Toolbox.\n\u00b5\u2223x0(t0 + s) = [1 + 5s 2t0 + 5s\n2\n3t20 + I(\u2212\u221e,s) (\ns5 6t50 )]x0 (31)\n\u00b5\u2223x0,y1(t0 + s) = [1 \u2212 10s\n2\n3t20 + I(\u2212\u221e,s) (\n5s4\nt40 + 8s\n5\n3t50 )]x0\n+ [s + 2s 2\nt0 \u2212 I(\u2212\u221e,s) (\n2s4\nt30 + s\n5\nt40 )] y0\n(32)\nlim t0\u2192\u221e\n\u00b5\u2223x0,y1,y2(t0 + s) = x0 + (s \u2212 s2\n2h\u03b1 ) y1 +\ns2\n2h\u03b1 y2 (33)\nAs was the case for the posterior mean, we will write the posterior covariance function as k(t, t\u2032) = k(t0 + s, t0 + s\u2032) while also assuming w.l.o.g. that s\u2032 \u2264 s. The posterior covariance functions are then given by:\nk\u2223x0(t0 + s, t0 + s \u2032) =ss\n\u2032\n48 t30 + (ss\u20322 + s2s\u2032) t20 24 + s 2s\u20322 9 t0\n+( \u2223s\u2223 5 240 + s 2s\u20323 24 + s 3s\u20322 24 \u2212 ss \u20324 48 \u2212 s 4s\u2032 48 + \u2223s \u2032\u22235 240 \u2212 \u2223s \u2032 \u2212 s\u22235 240 ) t00\n\u2212 (ss\u20325 + s5s\u2032 \u2212 s\u2223s\u2032\u22235 \u2212 s\u2032\u2223s\u22235) t \u22121 0 96 \u2212 (s2s\u20325 + s5s\u20322 \u2212 s2\u2223s\u2032\u22235 \u2212 s\u20322\u2223s\u22235) t \u22122 0 144 \u2212 (s5 \u2212 \u2223s\u22235) (s\u20325 \u2212 \u2223s\u2032\u22235) t \u22125 0\n2880 (34)\n\u00d0\u00d0\u00d0\u2192 t0\u2192\u221e \u221e\nk\u2223x0,y1(t0 + s, t0 + s \u2032) = \u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9 s, s\u2032 > 0 \u2236 s \u20325 240 + s 2s\u20323 24 + s 3s\u20322 24 \u2212 \u2223s \u2032 \u2212s\u22235 240 + s 5 240 \u2212 ss \u20324 48 \u2212 s 4s\u2032 48 + s 2s\u20322 36 t0 s > 0 \u2265 s\u2032 \u2236 s 2s\u20322(s\u2032+t0)3 36t02 s, s\u2032 \u2264 0 \u2236 s 2s\u20323 24 \u2212 s \u20325 240 + s 3s\u20322 24 \u2212 \u2223s \u2032 \u2212s\u22235 240 \u2212 s 5 240 + ss \u20324 48 + s 4s\u2032 48 + s 2s\u20322 36 t0 + s 2s\u20322(s2+s\u20322) 12t0 + s 2s\u20322(s3+s\u20323) 36t20 \u2212 s 4s\u20324(s\u2032+s) 24t40 \u2212 s 4s\u20324 12t03 \u2212 s 5s\u20325 45t05\n(35) \u00d0\u00d0\u00d0\u2192 t0\u2192\u221e \u221e\nFor the final posterior covariance, it is also necessary to distinguish between the cases whether s, s\u2032 \u2265 h\u03b1, s \u2265 h\u03b1 > s\u2032 and s, s\u2032 \u2264 h\u03b1.\nk\u2223x0,y1,y2(t0 + s, t0 + s \u2032) = \u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9 s, s\u2032 > (h\u03b1) \u2236 [(8s\u20325 \u2212 40ss\u20324 + 80(s2s\u20323 + (h\u03b1)2(s2s\u2032 + ss\u20322)) \u221220(h\u03b1)3(s2 + s\u20322) \u2212 160(h\u03b1)s2s\u20322) t0 \u221215(h\u03b1)6 + 60(h\u03b1)5(s + s\u2032) \u2212 90(h\u03b1)4(s2 + s\u20322) +24(h\u03b1)s\u20325 + 360(h\u03b1)3(ss\u20322 + s2s\u2032) \u2212120(h\u03b1)ss\u20324 \u2212 540(h\u03b1)2s2s\u20322 + 240(h\u03b1)s2s\u20323 \u2212240(h\u03b1)4ss\u2032] [960t0 + 2880h\u03b1]\u22121 s > (h\u03b1) \u2265 s\u2032 > 0 \u2236 [(20(s2s\u20324 \u2212 (h\u03b1)4s\u20322) + 80(h\u03b1)3ss\u20322 + 8(h\u03b1)s\u20325 \u221240((h\u03b1)2s2s\u20322 + (h\u03b1)ss\u20324)) t0 + 24(h\u03b1)2s\u20325 +15(h\u03b1)3s\u20324 \u2212 60(h\u03b1)4s\u20323 \u2212 180(h\u03b1)2ss\u20324 +240(h\u03b1)3ss\u20323 \u2212 120(h\u03b1)2s2s\u20323 + 90(h\u03b1)s2s\u20324] [960t0h\u03b1 + 2880(h\u03b1)2] \u22121 s > (h\u03b1) > 0 \u2265 s\u2032 \u2236 [(h\u03b1)s\u20322(s\u2032 + t0)3 (4(h\u03b1)s \u2212 2s2 \u2212 (h\u03b1)2)] [48t20(t0 + 3(h\u03b1))] \u22121 (h\u03b1) \u2265 s, s\u2032 > 0 \u2236 [(80(h\u03b1)2s2s\u20322((h\u03b1) \u2212 s) \u2212 40(h\u03b1)2ss\u20324 +8(h\u03b1)2s\u20325 + 20(h\u03b1)s2s\u20322(s2 + s\u20322)) t0 \u221215s4s\u20324 + 24(h\u03b1)3s\u20325 \u2212 120(h\u03b1)3ss\u20324 +240(h\u03b1)2s2s\u20323((h\u03b1) \u2212 s) +60(h\u03b1)s3s\u20323(s + s\u2032)] [960t0(h\u03b1)2 + 2880(h\u03b1)3] \u22121 (h\u03b1) \u2265 s > 0 \u2265 s\u2032 \u2236 [s2s\u20322(s\u2032 + t0)3(s \u2212 2(h\u03b1))2] [48t20(h\u03b1)(t0 + 3(h\u03b1))] \u22121 s, s\u2032 \u2264 0 \u2236 \u2212 [s2(s\u2032 + t0)3 (8s3s\u20322 \u2212 9s3s\u2032t0 + 3s3t20 +15s2s\u2032t0(s\u2032 \u2212 t0) \u2212 10s\u20322t30)] [360t50] \u22121 \u2212 [s2s\u20322(s + t0)3(s\u2032 + t0)3] [36t40(t0 + 3(h\u03b1))] \u22121\n(36)\nlim t0\u2192\u221e\nk\u2223x0,y1,y2(t0 + s, t0 + s \u2032) = \u23a7\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9 s, s\u2032 > (h\u03b1) \u2236 [s\u20323/12 \u2212 (h\u03b1)s\u20322/6 + (h\u03b1)2s\u2032/12 \u2212 (h\u03b1)3/48] s2 [(h\u03b1)2s\u20322/12 \u2212 s\u20324/24] s + s\u20325/120 \u2212 (h\u03b1)3s\u20322/48 s > (h\u03b1) \u2265 s\u2032 > 0 \u2236 [s\u20322 (20(h\u03b1)3s \u2212 10(h\u03b1)s((h\u03b1)s + s\u20322) +2(h\u03b1)s\u20323 + 5(s2s\u20322 \u2212 (h\u03b1)4))] [240(h\u03b1)]\u22121 s > (h\u03b1) > 0 \u2265 s\u2032 \u2236 \u22121/48 [(h\u03b1)s\u20322((h\u03b1)2 \u2212 4(h\u03b1)s + 2s2] (h\u03b1) \u2265 s, s\u2032 > 0 \u2236 [s\u20322 (20(h\u03b1)s2((h\u03b1) \u2212 s) \u2212 10(h\u03b1)ss\u20322 +2(h\u03b1)s\u20323 + 5s2(s2 + s\u20322))] [240(h\u03b1)]\u22121 (h\u03b1) \u2265 s > 0 \u2265 s\u2032 \u2236 [s2s\u20322(s \u2212 2(h\u03b1))2] [48(h\u03b1)]\u22121\ns, s\u2032 \u2264 0 \u2236 \u22121/120 [s2(s3 \u2212 5s2s\u2032 + 10ss\u20322 \u2212 10(h\u03b1)s\u20322] (37)\nEq. (37) concludes the proof of Theorem 2 as it shows that the covariance function after the RK step is finite for all values of s, s\u2032.\nC.3 Posterior predictive mean and covariance of the thrice integrated WP\nHere we list the equations of posterior mean and covariance for the thrice integrated WP and the generic 3-stage RK method. The same structure as in Sec. C.2 was applied.\nlim t0\u2192\u221e \u00b5\u2223x0(t0 + s) = x0 (38)\nlim t0\u2192\u221e \u00b5\u2223x0,y1(t0 + s) = x0 + sy1 (39)\nlim t0\u2192\u221e\n\u00b5\u2223x0,y1,y2(t0 + s) = x0 + (s \u2212 s2\n2hu ) y1 +\ns2\n2hu y2 (40)\nlim t0\u2192\u221e \u00b5\u2223x0,y1,y2,y3(t0 + s) = x0 + \u239b \u239d s \u2212\nh( s 2u 2 + s 2v 2 ) \u2212 s 3 3\nh2uv\n\u239e \u23a0 y1 (41)\n+ (s 2(2s \u2212 3hv)\n6h2u(u \u2212 v) ) y2 + (\u2212 s2(2s \u2212 3hu) 6h2v(u \u2212 v) ) y3 (42)\nAs in the case of the twice integrated Wiener process, the covariance function is infinite for limt0\u2192\u221e. Therefore, we only list the final posterior covariance function.\nFor s, s\u2032 > hv > hu > 0: lim t0\u2192\u221e k\u2223x0,y1,y2,y3(t0 + s, t0 + s \u2032) =\n{[\u221221h5u(s2 + s\u20322) + 14h4(s3 + s\u20323)] v5 + [\u221221h5u2(s2 + s\u20322) + 14h4u(s3 + s\u20323) +126h4u(s2s\u2032 + ss\u20322) \u2212 84h3(s3s\u2032 + ss\u20323)] v4 + [\u221221h5u3(s2 + s\u20322) + 14h4u2(s3 + s\u20323) +126h4u2(s2s\u2032 + ss\u20322) \u2212 84h3u(s3s\u2032 + ss\u20323) \u2212 630h3us2s\u20322 + 210h2(s3s\u20322 + s2s\u20323)] v3\n+ [\u221221h5u4(s2 + s\u20322) + 14h4u3(s3 + s\u20323) + 126h4u3(s2s\u2032 + ss\u20322) \u2212 84h3u2(s3s\u2032 + ss\u20323) \u2212252h3u2s2s\u20322 + 378h2u(s3s\u20322 + s2s\u20323) \u2212 392hs3s\u20323] v2 + [14h4u4(s3 + s\u20323)\n\u2212 84h3u3(s3s\u2032 + ss\u20323) + 126h2u2(s3s\u20322 \u2212 s2s\u20322 + s2s\u20323) \u2212 224hus3s\u20323 + 210s3s\u20324 \u2212 126s2s\u20325\n+42ss\u20326 \u2212 6s\u20327] v + 42h2u3(s3s\u20322 + s2s\u20323) \u221256hu2s3s\u20323} /(30240v) (43)\nFor s > hv \u2265 s\u2032 > hu > 0: lim t0\u2192\u221e k\u2223x0,y1,y2,y3(t0 + s, t0 + s \u2032) =\n{ (21h7us\u20322 \u2212 14h6s\u20323) v6 + (\u2212126suh6s\u20322 + 84sh5s\u20323) v5\n+ (315uh5s2s\u20322 \u2212 210h4s2s\u20323)v4\n+ (\u2212378h5s2s\u20322u2 \u2212 168h4s3s\u20322u + 252h4s2s\u20323u + 112h3s3s\u20323)v3\n+ (\u221221h7s2u5 \u2212 21h7s\u20322u5 + 126h6s2s\u2032u4 + 126h6ss\u20322u4 \u2212 126h5s2s\u20322u3 + 252h4s3s\u20322u2\n+ 252h4s2s\u20323u2 \u2212 168h3s3s\u20323u \u2212 315h3s2s\u20324u + 126h2s2s\u20325 \u2212 42h2ss\u20326 + 6h2s\u20327)v2\n+ (14h6s3u5 \u2212 84h5s3s\u2032u4 \u2212 126h5s2s\u20322u4 \u2212 84h5ss\u20323u4 + 84h4s3s\u20322u3\n+ 84h4s2s\u20323u3 \u2212 168h3s3s\u20323u2 + 210h2s3s\u20324u + 42h2ss\u20326u \u2212 6h2s\u20327u \u2212 84hs3s\u20325)v +42h4s3s\u20322u4+42h4s2s\u20323u4\u221256h3s3s\u20323u3\u221221hs2s\u20326u+14s3s\u20326}/(\u221230240h2v2+30240uh2v)\n(44) For s > hv > hu \u2265 s\u2032 > 0:\nlim t0\u2192\u221e\nk\u2223x0,y1,y2,y3(t0 + s, t0 + s \u2032) =\ns\u20322\n30240h2uv [ \u2212 21h7(u5v2 + u4v3 + u3v4 + u2v5) \u2212 21hs2s\u20324(u + v) \u2212 126h4s2u3v(hu \u2212 s\u2032)\n+ 126h6s(u4v2 + u3v3 + u2v4) + 14h6s\u2032(u5v + u4v2 + u3v3 + u2v4 + uv5) + 14s3s\u20324\n+ 63h5u3v2s2 \u2212 315h5u2v3s2 \u2212 84h5ss\u2032(u4v + u3v2 + u2v3 + uv4) \u2212 84h4u3vs3\n+ 42h4u4s2(s + s\u2032) \u2212 42h4u2v2s2s\u2032 + 42h2uvss\u20324 + 168h4u2v2s3 + 210h4uv3s2s\u2032\n\u2212 56h3u2s3s\u2032(u \u2212 v) \u2212 112h3uv2s3s\u2032 \u2212 6h2uvs\u20325] (45)\nFor s > hv > hu > 0 \u2265 s\u2032:\nlim t0\u2192\u221e\nk\u2223x0,y1,y2,y3(t0 + s, t0 + s \u2032) =\nhs\u20322\n4320v [(\u22123h4u4v2 \u2212 3h4u3v3 \u2212 3h4u2v4 \u2212 3h4uv5 + 18h3su3v2 + 18h3su2v3 + 18h3suv4\n+ 2s\u2032h3u4v + 2s\u2032h3u3v2 + 2s\u2032h3u2v3 + 2s\u2032h3uv4 + 2s\u2032h3v5 \u2212 18h2s2u3v + 9h2s2u2v2\n\u2212 45h2s2uv3 \u2212 12s\u2032h2su3v \u2212 12s\u2032h2su2v2 \u2212 12s\u2032h2suv3 \u2212 12s\u2032h2sv4 + 6hs3u3\n\u2212 12hs3u2v + 24hs3uv2 + 6s\u2032hs2u3 + 18s\u2032hs2u2v \u2212 6s\u2032hs2uv2 + 30s\u2032hs2v3 \u2212 8s\u2032s3u2\n+ 8s\u2032s3uv \u2212 16s\u2032s3v2)] (46)\nFor hv \u2265 s, s\u2032 > hu > 0:\nlim t0\u2192\u221e\nk\u2223x0,y1,y2,y3(t0 + s, t0 + s \u2032) =\n{(378h5s2s\u20322u2 \u2212 252h4s3s\u20322u \u2212 252h4s2s\u20323u + 168h3s3s\u20323)v3\n+ (21h7s2u5 + 21h7s\u20322u5 \u2212 126h6s2s\u2032u4 \u2212 126h6ss\u20322u4 + 126h5s2s\u20322u3\n\u2212 252h4s3s\u20322u2 \u2212 252h4s2s\u20323u2 + 315h3s4s\u20322u + 168h3s3s\u20323u + 315h3s2s\u20324u\u2032\n\u2212 210h2s4s\u20323 \u2212 126h2s2s\u20325 + 42h2ss\u20326 \u2212 6h2s\u20327)v2\n+ (\u221214h6s3u5 \u2212 14h6s\u20323u5 + 84h5s3s\u2032u4 + 126h5s2s\u20322u4 + 84h5ss\u20323u4\n\u2212 84h4s3s\u20322u3 \u2212 84h4s2s\u20323u3 + 168h3s3s\u20323u2 \u2212 126h2s5s\u20322u \u2212 126h2s5s\u20322u \u2212 210h2s3s\u20324u \u2212 42h2ss\u20326u + 6h2s\u20327u + 84hs5s\u20323 + 84hs3s\u20326)v\n\u2212 42h4s3s\u20322u4 \u2212 42h4s2s\u20323u4 + 56h3s3s\u20323u3 + 21hs6s\u20322u + 21hs2s\u20326u \u2212 14s6s\u20323 \u2212 14s3s\u20326}/(30240h2v2 \u2212 30240uh2v) (47)\nFor hv \u2265 s > hu \u2265 s\u2032 > 0:\nlim t0\u2192\u221e\nk\u2223x0,y1,y2,y3(t0 + s, t0 + s \u2032) =\ns\u20322\n30240h2uv(u \u2212 v) [ \u2212 21h7u6v2 \u2212 21hu2s6 \u2212 21hs2s\u20324(u2 \u2212 v2) + 126h2u2vs5\n+ 126h4u4vs(h2uv \u2212 hus \u2212 s2) + 14s\u2032(h6u6v + s6u) + 14s3s\u20324(u \u2212 v) + 189h5u4v2s2\n\u2212 378h5u3v3s2 \u2212 84h4u4vss\u2032(hu \u2212 s) \u2212 84huvs5s\u2032 + 42h4u5s2(s + s\u2032) + 42h2ss\u20324(u2v \u2212 uv2) + 252h4u2v2s2(us + vs + vs\u2032) \u2212 168h3uv2s2s\u2032(hu2 + us + vs) \u2212 315h3u2v2s4 \u2212 56h3u4s3s\u2032 + 112h3u3vs3s\u2032 + 210h4uv4s4s\u2032 \u2212 6h2s\u20325(u2v \u2212 uv2)]\n(48)\nFor hv \u2265 s > hu > 0 \u2265 s\u2032:\nlim t0\u2192\u221e\nk\u2223x0,y1,y2,y3(t0 + s, t0 + s \u2032) =\ns\u20322\n4320h2v(u \u2212 v) [ \u2212 3h7u5v2 + 18h6su4v2 + 2s\u2032h6u5v \u2212 18h5s2u4v + 27h5s2u3v2\n\u2212 54h5s2u2v3 \u2212 12s\u2032h5su4v + 6h4s3u4 \u2212 18h4s3u3v + 36h4s3u2v2 + 36h4s3uv3\n+ 6s\u2032h4s2u4 + 12s\u2032h4s2u3v \u2212 24s\u2032h4s2u2v2 + 36s\u2032h4s2uv3 \u2212 45h3s4uv2 \u2212 8s\u2032h3s3u3\n+ 16s\u2032h3s3u2v \u2212 24s\u2032h3s3uv2 \u2212 24s\u2032h3s3v3 + 18h2s5uv + 30s\u2032h2s4s2 \u2212 3hs6u \u2212 12s\u2032h5v + 2s\u2032s6] (49)\nFor hv > hu \u2265 s, s\u2032 > 0:\nlim t0\u2192\u221e\nk\u2223x0,y1,y2,y3(t0 + s, t0 + s \u2032) =\n\u2212 s \u20322\n30240h2uv [126h5s2u4v \u2212 378h5s2u3v2 \u2212 42h4s3u4 + 84h4s3u3v + 252h4s3u2v2\n\u2212 42h4s2s\u2032u4 + 84h4s2s\u2032u3v + 252h4s2s\u2032u2v2 + 56h3s3s\u2032u3 \u2212 336h3s3s\u2032u2v \u2212 168h3s3s\u2032uv2 \u2212 126h2s5uv + 210h2s4s\u2032uv \u2212 42h2ss\u20324uv + 6h2s\u20325uv\n+ 21hs6v + 21hs2s\u20324u + 21hs2s\u20324v \u2212 14s6s\u2032 \u2212 14s3s\u20324] (50) For hv > hu \u2265 s > 0 \u2265 s\u2032:\nlim t0\u2192\u221e\nk\u2223x0,y1,y2,y3(t0 + s, t0 + s \u2032) =\n\u2212 s 2s\u20322\n4320h2uv [18h5u4v \u2212 54h5u3v2 \u2212 6h4su4\n+ 12h4su3v + 36h4su2v2 \u2212 6s\u2032h4u4 + 12s\u2032h4u3v + 36s\u2032h4u2v2 + 8s\u2032h3su3\n\u2212 48s\u2032h3su2v \u2212 24s\u2032h3suv2 \u2212 18h2s3uv + 30s\u2032h2s2uuv + 3hs4v \u2212 2s\u2032s4] (51) For hv > hu > 0 \u2265 s, s\u2032:\nlim t0\u2192\u221e\nk\u2223x0,y1,y2,y3(t0 + s, t0 + s \u2032) =\nhs2s\u20322u2 (hsu \u2212 4ss\u2032 + 3hs\u2032u) 2160v\n\u2212 s 2\n5040 [21h3s\u20322u3 \u2212 63vh3s\u20322u2 + 14h2ss\u20322u2 + 42vh2ss\u20322\n+ 14h2s\u20323u2 + 42vh2s\u20323u \u2212 56hss\u20323u \u2212 28vhss\u20323\n\u2212 s5 + 7s4s\u2032 \u2212 21s3s\u20322 + 35s2 + s\u20323] (52)"}, {"heading": "D Square-exponential kernel cannot yield Euler\u2019s method", "text": "We show that the square-exponetial (SE, aka. RBF, Gaussian) kernel cannot yield Euler\u2019s method for finite length-scales.\nThe SE kernel and its derivatives are k(t, t\u2032) = \u03b82 exp(\u2212\u2223\u2223t\u2212t\u2032\u2223\u22232/2\u03bb2) (53)\nk\u2202(t, t\u2032) = (t \u2212 t \u2032)\n\u03bb2 k(t, t\u2032) (54)\nk\u2202 \u2202(t, t\u2032) = \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 1 \u03bb2 \u2212 ((t \u2212 t \u2032) \u03bb2 ) 2\u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 k(t, t\u2032) (55)\nTo show that this choice does not yield Euler\u2019s method, we proceed as in the case for the GMRK methods. The predictive mean after observing x0 and y1 is given by\n\u00b5\u2223x0,y1(t0 + s) = [k(t0 + s, t0) k\u2202(t0 + s, t0)] [ k(t0, t0) k\u2202(t0, t0) k\u2202 (t0, t0) k\u2202 \u2202(t0, t0) ] \u22121\n\u00b4\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b8\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b9\u00b6 =\u2236K\n(x0 y1 )\n= [\u03b82 exp(\u2212\u2223\u2223s\u2223\u22232/2\u03bb2) s \u03b8 2 exp(\u2212\u2223\u2223s\u2223\u22232/2\u03bb2) \u03bb2 ] [\u03b8 2 0 0 \u03b8 2/\u03bb2] \u22121 (x0 y1 )\n= [exp(\u2212\u2223\u2223s\u2223\u22232/2\u03bb2) s exp(\u2212\u2223\u2223s\u2223\u22232/2\u03bb2)] (x0 y1 )\n= exp(\u2212\u2223\u2223s\u2223\u22232/2\u03bb2)x0 + s exp(\u2212\u2223\u2223s\u2223\u22232/2\u03bb2)y1 (56)\nevaluated at h yields\n= exp(\u2212\u2223\u2223h\u2223\u22232/2\u03bb2)x0 + h exp(\u2212\u2223\u2223h\u2223\u22232/2\u03bb2)y1 (57)\nAn interesting observation, left out in the main paper to avoid confusion, is that Eq. (57) does indeed produce the weights for Euler\u2019s method for the limit lim\u03bb\u2192\u221e. In fact, it can even be used to derive second and third order Runge-Kutta means, too. Future work will provide more insight into this property. However, this limit in the length-scale yields a Gaussian process posterior that has little use as a probabilistic numerical method, because its posterior covariance vanishes everywhere. This is in contrast to the integrated Wiener processes discussed in the paper, which yield proper finite, interpretable posterior variances, even after in the limit in \u03c4 . Finally, SE kernel-GPs are not Markov. Inference in these models has cost cubic in the number of observations, reducing their utility as numerical methods."}], "references": [{"title": "Solving noisy linear operator equations by Gaussian processes: Application to ordinary and partial differential equations", "author": ["T. Graepel"], "venue": "In: International Conference on Machine Learning (ICML)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Accelerating Bayesian inference over nonlinear differential equations with Gaussian processes.", "author": ["B. Calderhead", "M. Girolami", "N. Lawrence"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "ODE parameter inference using adaptive gradient matching with Gaussian processes", "author": ["F. Dondelinger"], "venue": "Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Gaussian Processes for Bayesian Estimation in Ordinary Differential Equations", "author": ["Y. Wang", "D. Barber"], "venue": "In: International Conference on Machine Learning (ICML)", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Bayesian solution of ordinary differential equations", "author": ["J. Skilling"], "venue": "In: Maximum Entropy and Bayesian Methods, Seattle ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "Probabilistic Solutions to Differential Equations and their Application to Riemannian Statistics", "author": ["P. Hennig", "S. Hauberg"], "venue": "In: Proc. of the 17th int. Conf. on Artificial Intelligence and Statistics (AISTATS). Vol. 33. JMLR, W&CP", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic shortest path tractography in DTI using Gaussian Process ODE solvers", "author": ["M. Schober"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Bayesian Uncertainty Quantification for Differential Equations", "author": ["O. Chkrebtii"], "venue": "arXiv prePrint", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "\u00dcber die numerische Aufl\u00f6sung von Differentialgleichungen", "author": ["C. Runge"], "venue": "In: Mathematische Annalen 46 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1895}, {"title": "Beitrag zur n\u00e4herungsweisen Integration totaler Differentialgleichungen", "author": ["W. Kutta"], "venue": "In: Zeitschrift f\u00fcr Mathematik und Physik 46 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1901}, {"title": "Solving Ordinary Differential Equations I \u2013 Nonstiff Problems", "author": ["E. Hairer", "S. N\u00f8rsett", "G. Wanner"], "venue": "Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "A family of embedded Runge-Kutta formulae", "author": ["J.R. Dormand", "P.J. Prince"], "venue": "In: Journal of computational and applied mathematics 6.1 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1980}, {"title": "Coefficients for the study of Runge-Kutta integration processes", "author": ["J. Butcher"], "venue": "In: Journal of the Australian Mathematical Society 3.02 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1963}, {"title": "Probl\u00e8mes diff\u00e9rentiels de conditions initiales (m\u00e9thodes num\u00e9riques)", "author": ["F. Ceschino", "J. Kuntzmann"], "venue": "Dunod Paris", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1963}, {"title": "Solutions of Differential Equations by Evaluations of Functions", "author": ["E.B. Shanks"], "venue": "In: Mathematics of Computation 20.93 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1966}, {"title": "Numerical solution of ordinary differential equations", "author": ["E. Hairer", "C. Lubich"], "venue": "In: The Princeton Companion to Applied Mathematics, ed. by N. Higham. PUP", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Extrapolation", "author": ["N. Wiener"], "venue": "interpolation, and smoothing of stationary time series with engineering applications\u201d. In: Bull. Amer. Math. Soc. 56 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1950}, {"title": "Bayesian filtering and smoothing", "author": ["S. S\u00e4rkk\u00e4"], "venue": "Cambridge University Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Gaussian Processes for Machine Learning", "author": ["C. Rasmussen", "C. Williams"], "venue": "MIT", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "An approach to time series smoothing and forecasting using the EM algorithm", "author": ["R. Shumway", "D. Stoffer"], "venue": "In: Journal of time series analysis 3.4 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1982}], "referenceMentions": [{"referenceID": 0, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 172, "endOffset": 184}, {"referenceID": 1, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 172, "endOffset": 184}, {"referenceID": 2, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 172, "endOffset": 184}, {"referenceID": 3, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 172, "endOffset": 184}, {"referenceID": 4, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 275, "endOffset": 287}, {"referenceID": 5, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 275, "endOffset": 287}, {"referenceID": 6, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 275, "endOffset": 287}, {"referenceID": 7, "context": "Hence, researchers in machine learning have repeatedly been interested in both the problem of inferring an ODE description from observed trajectories of a dynamical system [1, 2, 3, 4], and its dual, inferring a solution (a trajectory) for an ODE initial value problem (IVP) [5, 6, 7, 8].", "startOffset": 275, "endOffset": 287}, {"referenceID": 8, "context": "Runge-Kutta (RK) methods [9, 10] are standard tools for this purpose.", "startOffset": 25, "endOffset": 32}, {"referenceID": 9, "context": "Runge-Kutta (RK) methods [9, 10] are standard tools for this purpose.", "startOffset": 25, "endOffset": 32}, {"referenceID": 10, "context": "Over more than a century, these algorithms have matured into a very well-understood, efficient framework [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 5, "context": "As recently pointed out by Hennig and Hauberg [6], since Runge-Kutta methods are linear extrapolation methods, their structure can be emulated by Gaussian process (GP) regression algorithms.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "Such an algorithm was envisioned by Skilling in 1991 [5], and the idea has recently attracted both theoretical [8] and practical [6, 7] interest.", "startOffset": 53, "endOffset": 56}, {"referenceID": 7, "context": "Such an algorithm was envisioned by Skilling in 1991 [5], and the idea has recently attracted both theoretical [8] and practical [6, 7] interest.", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "Such an algorithm was envisioned by Skilling in 1991 [5], and the idea has recently attracted both theoretical [8] and practical [6, 7] interest.", "startOffset": 129, "endOffset": 135}, {"referenceID": 6, "context": "Such an algorithm was envisioned by Skilling in 1991 [5], and the idea has recently attracted both theoretical [8] and practical [6, 7] interest.", "startOffset": 129, "endOffset": 135}, {"referenceID": 6, "context": "Solution candidates can be drawn from the posterior and marginalized [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "This can allow probabilistic solvers to stop earlier, and to deal (approximately) with probabilistically uncertain inputs and problem definitions [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 10, "context": "Table 1: All consistent Runge-Kutta methods of order p \u2264 3 and number of stages s = p (see [11]).", "startOffset": 91, "endOffset": 95}, {"referenceID": 8, "context": "Runge-Kutta methods1 [9, 10] are carefully designed linear extrapolation methods operating on small contiguous subintervals [tn, tn + h] \u2282 T of length h.", "startOffset": 21, "endOffset": 28}, {"referenceID": 9, "context": "Runge-Kutta methods1 [9, 10] are carefully designed linear extrapolation methods operating on small contiguous subintervals [tn, tn + h] \u2282 T of length h.", "startOffset": 21, "endOffset": 28}, {"referenceID": 11, "context": "by combining the same observations into two different RK predictions [12]).", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": ", bs], often presented compactly in a Butcher tableau [13]: c1 0 c2 w21 0 c3 w31 w32 0 \u22ee \u22ee \u22ee \u22f1 \u22f1 cs ws1 ws2 \u22ef ws,s\u22121 0 b1 b2 \u22ef bs\u22121 bs As Hennig and Hauberg [6] recently pointed out, the linear structure of the extrapolation steps in Runge-Kutta methods means that their algorithmic structure, the Butcher tableau, can be constructed naturally from a Gaussian process regression method over x(t), where the yi are treated as \u201cobservations\u201d of \u1e8b(t0 + hci) and the x\u0302i are subsequent posterior estimates (more below).", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": ", bs], often presented compactly in a Butcher tableau [13]: c1 0 c2 w21 0 c3 w31 w32 0 \u22ee \u22ee \u22ee \u22f1 \u22f1 cs ws1 ws2 \u22ef ws,s\u22121 0 b1 b2 \u22ef bs\u22121 bs As Hennig and Hauberg [6] recently pointed out, the linear structure of the extrapolation steps in Runge-Kutta methods means that their algorithmic structure, the Butcher tableau, can be constructed naturally from a Gaussian process regression method over x(t), where the yi are treated as \u201cobservations\u201d of \u1e8b(t0 + hci) and the x\u0302i are subsequent posterior estimates (more below).", "startOffset": 157, "endOffset": 160}, {"referenceID": 10, "context": "The method is then said to be of order p [11].", "startOffset": 41, "endOffset": 45}, {"referenceID": 13, "context": "This is only possible for p < 5 [14, 15].", "startOffset": 32, "endOffset": 40}, {"referenceID": 14, "context": "This is only possible for p < 5 [14, 15].", "startOffset": 32, "endOffset": 40}, {"referenceID": 10, "context": "Many generalizations can be found in [11].", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "A recursive algorithm analogous to RK methods can be constructed [5, 6] by setting the prior mean to the constant \u03bc(t) = x0, then recursively estimating x\u0302i in some form from the current posterior over x.", "startOffset": 65, "endOffset": 71}, {"referenceID": 5, "context": "A recursive algorithm analogous to RK methods can be constructed [5, 6] by setting the prior mean to the constant \u03bc(t) = x0, then recursively estimating x\u0302i in some form from the current posterior over x.", "startOffset": 65, "endOffset": 71}, {"referenceID": 5, "context": "The choice in [6] is to set x\u0302i = \u03bc(t0 + hci).", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "In previous work [6, 7], both were chosen ad hoc, with no guarantee of convergence order.", "startOffset": 17, "endOffset": 23}, {"referenceID": 6, "context": "In previous work [6, 7], both were chosen ad hoc, with no guarantee of convergence order.", "startOffset": 17, "endOffset": 23}, {"referenceID": 7, "context": "[8] previously made the case that Gaussian measures are uniquely suited because solution spaces of ODEs are Banach spaces, and provided results on consistency.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Motivation of evaluation points Both Skilling [5] and Hennig and Hauberg [6] propose to put the \u201cnodes\u201d x\u0302(t0 + hci) at the current posterior mean of the belief.", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "Motivation of evaluation points Both Skilling [5] and Hennig and Hauberg [6] propose to put the \u201cnodes\u201d x\u0302(t0 + hci) at the current posterior mean of the belief.", "startOffset": 73, "endOffset": 76}, {"referenceID": 15, "context": "From the second interval onward, the RK step solves an estimated IVP, and begins to accumulate a global estimation error not bounded by the convergence order (an effect termed \u201cLady Windermere\u2019s fan\u201d by Wanner [16]).", "startOffset": 210, "endOffset": 214}, {"referenceID": 16, "context": "All covariance functions in question are integrals over the kernel k(t\u0303, t\u0303) = \u03c3 min(t\u0303 \u2212 \u03c4, t\u0303 \u2212 \u03c4) (parameterized by scale \u03c3 > 0 and off-set \u03c4 \u2208 R; valid on the domain t\u0303, t\u0303 > \u03c4 ), the covariance of the Wiener process [17].", "startOffset": 221, "endOffset": 225}, {"referenceID": 17, "context": "Such integrated Wiener processes are Gauss-Markov processes, of increasing order, so inference in these methods can be performed by filtering, at linear cost [18].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "Since Runge-Kutta methods have been extensively studied for over a century [11], it is not necessary to evaluate their estimation performance again.", "startOffset": 75, "endOffset": 79}, {"referenceID": 5, "context": "Figure 3: Comparison of a 2nd order GMRK method and the method from [6].", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "2 (right column) to the ad-hoc choice of a square-exponential (SE) kernel model, which was used by Hennig and Hauberg [6] (Fig.", "startOffset": 118, "endOffset": 121}], "year": 2014, "abstractText": "Runge-Kutta methods are the classic family of solvers for ordinary differential equations (ODEs), and the basis for the state of the art. Like most numerical methods, they return point estimates. We construct a family of probabilistic numerical methods that instead return a Gauss-Markov process defining a probability distribution over the ODE solution. In contrast to prior work, we construct this family such that posterior means match the outputs of the Runge-Kutta family exactly, thus inheriting their proven good properties. Remaining degrees of freedom not identified by the match to Runge-Kutta are chosen such that the posterior probability measure fits the observed structure of the ODE. Our results shed light on the structure of Runge-Kutta solvers from a new direction, provide a richer, probabilistic output, have low computational cost, and raise new research questions.", "creator": "LaTeX with hyperref package"}}}