{"id": "1406.2751", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2014", "title": "Reweighted Wake-Sleep", "abstract": "independent training deep directed graphical models with many relatively hidden variables and performing inference remains a major challenge. helmholtz machines and deep belief networks are such models, and the improved wake - sleep algorithm has been proposed to train them. the wake - sleep algorithm currently relies considerably on training not just following the directed generative model but also a conditional generative model ( the inference research network ) that runs backward from visible sequences to latent, estimating the posterior distribution of latent outcomes given visible. we propose a novel interpretation of the generalized wake - sleep algorithm format which suggests that better estimators of the gradient can be quickly obtained by sampling latent variables multiple times from the inference network. this view is based directly on importance sampling detection as primarily an estimator of the expected likelihood, with the approximate inference network as a proposal distribution. basically this interpretation is confirmed experimentally, showing that possible better likelihood can be achieved with this lightly reweighted wake - sleep procedure, which also provides a natural way to estimate the likelihood rule itself. based on this interpretation, we propose that a sigmoid belief gathering network is not sufficiently and powerful for the layers of the inference network, in order to recover a good approximation estimator of the posterior distribution of latent variables. our experiments show that using a distinctly more powerful layer model, such as nade, yields substantially better predictions generative models.", "histories": [["v1", "Wed, 11 Jun 2014 00:44:31 GMT  (593kb,D)", "http://arxiv.org/abs/1406.2751v1", null], ["v2", "Fri, 5 Dec 2014 23:30:10 GMT  (593kb,D)", "http://arxiv.org/abs/1406.2751v2", null], ["v3", "Sat, 20 Dec 2014 04:25:43 GMT  (643kb,D)", "http://arxiv.org/abs/1406.2751v3", null], ["v4", "Thu, 16 Apr 2015 17:22:58 GMT  (643kb,D)", "http://arxiv.org/abs/1406.2751v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["j\\\"org bornschein", "yoshua bengio"], "accepted": true, "id": "1406.2751"}, "pdf": {"name": "1406.2751.pdf", "metadata": {"source": "CRF", "title": "Reweighted Wake-Sleep", "authors": ["J\u00f6rg Bornschein", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Training directed graphical models \u2013 especially models with multiple layers of hidden variables \u2013 remains a major challenge. This is unfortunate because, as has been argued previously (Hinton et al., 2006; Bengio, 2009), a deeper generative model has the potential to capture high-level abstractions and thus generalize better. The exact log-likelihood gradient is intractable, be it for Helmholtz machines (Dayan et al., 1995), sigmoid belief networks (SBNs), or deep belief networks (DBNs) (Hinton et al., 2006), which are directed, or deep Boltzmann machines (DBMs), which are undirected. Even obtaining an unbiased estimator of the gradient of the DBN or Helmholtz machine likelihood is not something that has been achieved in the past. Here we show that it is possible to get an unbiased estimator of the likelihood (which unfortunately makes it a slightly biased estimator of the log-likelihood), using an importance sampling approach. Past proposals to train Helmholtz machines and DBNs rely on maximizing a variational bound as proxy for the log-likelihood (Hinton et al., 1995; Kingma and Welling, 2014b; Rezende et al., 2014). The first of these is the wake-sleep algorithm (Hinton et al., 1995), which relies on combining a \u201crecognition\u201d network (which we call an approximate inference network, here, or simply inference network) with a \u201cgenerative\u201d network. In the wake-sleep algorithm, they basically provide targets for each other. We review these previous approaches and introduce a novel approach that we generalizes the wake-sleep algorithm. Whereas the original justification of the wake-sleep algorithm has been questioned (because we are optimizing a KL-divergence in the wrong direction), a contribution of this paper is to shed a different light on the wake-sleep algorithm, viewing it as a special case of the proposed reweighted wake-sleep\nar X\niv :1\n40 6.\n27 51\nv1 [\ncs .L\nG ]\n1 1\nJu n\n20 14\n(RWS) algorithm, i.e., as reweighted wake-sleep with a single sample. This makes it clear that wake-sleep corresponds to optimizing a somewhat biased estimator of the likelihood gradient, while using more samples (i.e., RWS) makes the estimator less biased (and asymptotically unbiased as more samples are considered). We empirically show that effect, with clearly better results obtained with K = 5 samples than with K = 1 (wake-sleep), and 5 or 10 being sufficient to achieve good results. Unlike in the case of DBMs, which rely on a Markov chain to get samples and estimate the gradient by a mean over those samples, here the samples are iid, avoiding the very serious problem of mixing between modes that can plague MCMC methods (Bengio et al., 2013) when training undirected graphical models.\nAnother contribution of this paper regards the architecture of the deep generative model and of the approximate inference network. We view the inference network as estimating the posterior distribution of latent variables given the observed input. With this view, it is plausible that the classical architecture of the inference network (an SBN, details below) is inappropriate and we test this hypothesis empirically. In the classical sigmoidal belief network (SBN) (e.g., in the DBN and Helmholtz machine), the conditional distribution of each layer of the inference network, given the previous layer, is a factorized Bernoulli (where the probability for each bit is computed as in a logistic regression with the previous layer bits as inputs). We find that more powerful parametrizations of each layer as a conditional probability model yields better results."}, {"heading": "2 Reweighted Wake-Sleep", "text": ""}, {"heading": "2.1 The Wake-Sleep Algorithm", "text": "The wake-sleep algorithm was proposed as a way to train Helmholtz machines, which are deep directed graphical models p(x,h) over visible variables x and latent variables h, where the latent variables are organized in layers hk, with the k-th layer taking as input the random vector generated by the previous layer in the generating sequence, hk+1. In the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995), the top layer, hL, has a factorized unconditional distribution, so that ancestral sampling can proceed from hL down to h1 and then the generated sample x is generated by the bottom layer, given h1. In the deep belief network (DBN) (Hinton et al., 2006), the top layer is instead generated by a restricted Boltzmann machine (RBM), i.e., by a Markov chain, while simple ancestral sampling is used for the others. Each intermediate layer is specified by a conditional distribution parametrized as a stochastic sigmoidal layer (see Section 3 for details).\nThe wake-sleep algorithm is a training procedure for such generative models, which involves training an auxiliary network, called the inference network, that takes a visible vector x as input and stochastically outputs samples hk for all layers k = 1 to L. The inference network outputs samples are from a distribution that should estimate the conditional probability of the latent variables of the generative model (at all layers) given the input. Note that in these kinds of models (and this is generally the case with latent variables), exact inference, i.e., sampling from p(h|x) is intractable. The wake-sleep algorithm proceeds in two phases. In the wake phase, an observation x (from the real world) is sampled from the data generating distribution and propagated stochastically up the inference network (one layer at a time), thus sampling latent values h from q(h|x). Together with x, the sampled h forms a target for training p, i.e., one performs a step of gradient ascent update with respect to maximum likelihood over the generative model p(x,h), with the data x and the inferred h. This is useful because whereas computing the gradient of the marginal likelihood p(x) = \u2211 h p(x,h) is intractable, we assume here that computing the gradient of the complete loglikelihood log p(x,h) is easy. In addition, these updates decouple all the layers (because both the input and the target of each layer are considered observed). In the sleep phase, a \u201cdream\u201d sample is obtained from the generative network by ancestral sampling from p(x,h) and is used as a target for the maximum likelihood training of the inference network, i.e., q is trained to estimate p(h|x). The justification for the wake-sleep algorithm that was originally proposed is based on the following variational bound,\nlog p(x) \u2265 \u2211 h q(h|x) log p(x,h) q(h|x)\nthat is true for any inference network q, but the bound becomes tight as q(h|x) approaches p(h|x). Maximizing this bound with respect to p corresponds to the wake phase update. The update with respect to q should minimize KL(q(h|x)||p(h|x)) (with q as the reference) but instead the sleep phase update minimizes the reversed KL divergence, KL(p(h|x)||q(h|x)) (with p as the reference)."}, {"heading": "2.2 An Importance Sampling View yields Reweighted Wake-Sleep", "text": "If we think of q(h|x) as estimating p(h|x) and train it accordingly (which is basically what the sleep phase of wake-sleep does), then we can reformulate the likelihood as an importance-weighted average:\np(x) = \u2211 h p(x,h) = \u2211 h q (h |x) p(x,h) q (h |x)\n= E h\u223cq(h |x)\n[ p(x,h)\nq (h |x) ] ' 1 K\nK\u2211 k=1\nh(k)\u223cq(h |x)\np(x,h(k)) q ( h(k) |x ) (1) Eq. (1) is a consistent and unbiased estimator for the marginal likelihood p(x). This already gives us something interesting if we want to estimate the likelihood associated with a test sample x. But depending on the q (h |x), it might suffer from very high variance. The optimal q that results in a minimum variance estimator is q\u2217(h |x) = p (h |x). We can even show that this is a zero-variance estimator, i.e., the best possible one:\nE h\u223cq\u2217(h |x) [ p (h |x) p(x) q\u2217(h |x) ] = p(x) E h\u223cq\u2217(h |x) [1] = p(x) (2)\nIn other words: If q(h|x) = q\u2217(h|x) = p (h |x), then an arbitrary single sample h \u223c q(h|x) will lead to an exact p(x) estimate. Of course, q(h|x) = p (h |x) is a unrealistic assumption. But q is only used as proposal distribution, any mismatch between q and p will increase variance, it will not introduce bias.\nIn practice however, we typically want to get an estimator of the log-likelihood, because we are interested in the likelihood of a whole set of test examples. In this case, in expectation over samples, we get a lower bound on the log-likelihood, by Jensen\u2019s inequality: the expected value over samples of the log of the importance weighted likelihood estimator is less or equal than the log of the expected value, i.e., less or equal to the true log-likelihood. This is good because it gives us a conservative estimator of the log-likelihood, in average, i.e., it tends to underestimate the ground truth. From the point of view of the log-likelihood gradient, it means that our estimator is the gradient of a lower bound on the log-likelihood. This is also true of variational methods such as described below (Section 2.4). However, unlike with these methods, here the bound can be made arbitrarily tighter by simply using more samples, because the inner average over samples (before applying the log) converges to its expectation."}, {"heading": "2.3 Training by Reweighted Wake-Sleep", "text": "We now consider the models p and q parameterized respectively with parameters \u03b8 and \u03c6.\nUpdating p\u03b8 for given q\u03c6: For an observed x we would like to estimate the gradient of the marginal log-likelihood log p\u03b8(x): (see the supplementary material for a detailed derivation)\n\u2202\n\u2202\u03b8 Lp(\u03b8,x \u223c D) =\n1\np(x) E h\u223cq(h |x)\n[ p(x,h)\nq (h |x) \u2202 \u2202\u03b8 log p(x,h) ] ' 1\u2211\nk \u03c9k K\u2211 k=1 \u03c9k \u2202 \u2202\u03b8 log p(x,h(k)) (3)\nwith \u03c9k = p(x,h(k)) q ( h(k) |x ) and h(k) \u223c q (h |x) Equation (3) is a biased but consistent (asymptotically unbiased) estimator of the gradient.\nAlgorithm 1 Reweighted Wake-Sleep training procedure and likelihood estimator. K is the number of approximate inference samples and controls the trade-off between computation and accuracy of the estimators (both for the gradient and for the likelihood). We typically use a large value (K=500) for test set likelihood estimator but a small value (K=5) for estimating gradients. Both the wakephase and sleep-phase update rules for q are optionally included (either one or both can be used, and best results were obtained using both). The original wake-sleep algorithm has K=1 and only uses the sleep-phase update of q. To estimate the log-likelihood at test time, only the computations up to L\u0302L are required.\nfor number of training iterations do \u2022 Sample example(s) x from the data generating distribution for k = 1 to K do \u2022 Sample latent variables h(k) from q(h|x) layerwise (first layer above x, second layer, etc. up to top hidden layer). \u2022 Compute q(h(k)|x) and p(x,h(k))\nend for \u2022 Compute unnormalized weights \u03c9k = p(x,h (k)) q(h(k) |x) \u2022 Normalize the weights: \u03c9\u0303k = \u03c9k\u2211\nk\u2032 \u03c9k\u2032\n\u2022 Compute unbiased likelihood estimator p\u0302(x) = averagek \u03c9k, or asymptotically unbiased estimator of log-likelihood L\u0302L(x) = log averagek \u03c9k \u2022 Wake-phase update of p. Compute asymptotically unbiased estimator of log-likelihood gradient w.r.t. p, \u2211 k \u03c9\u0303k \u2202 log p(x,h(k)) \u2202\u03b8 , and perform an update of p\u2019s parameters using it \u2022 Optionally, wake-phase update of q. Use gradient averagek \u03c9\u0303k \u2202 log q(h(k)|x)\n\u2202\u03c6\n\u2022 Optionally, sleep-phase update of q. Sample (x\u2032,h\u2032) from p and use gradient \u2202 log q(h \u2032|x\u2032)\n\u2202\u03c6\nend for\nUpdating q\u03c6 for given p\u03b8: In order to minimize the variance of the estimator 1 we would like q (h |x) to track p (h |x). To perform maximum likelihood learning on the q-distribution (with loss Lq) we have at least two reasonable options: (1) maximize Lq under the empirical distribution of the data: x \u223c D, h \u223c p (h |x), or (2) maximize Lq under the generative model (x,h) \u223c p\u03b8(x,h). We will refer to the former as wake-q-learning and to the latter as sleep-q-learning. In the case of a DBN (where the top layer is generated by an RBM), there is an intermediate solution, which has been proposed in (Hinton et al., 2006) and called contrastive wake-sleep. In contrastive wake-sleep we sample x from the data, propagate it stochastically into top layer and use that h as starting point for a short Markov chain in the RBM, then sample the other layers in the generative network p to generate the rest of (x,h). The objective is to put the inference network\u2019s capacity where it matters most, i.e., near the input configurations that are seen in the training set.\nGradients for wake-q-learning can be derived to be:\n\u2202\n\u2202\u03c6 Lq(\u03c6,x \u223c D) ' 1\u2211 k \u03c9k K\u2211 k=1 \u03c9k \u2202 \u2202\u03c6 log q\u03c6(h (k)|x) (4)\nwith the same importance weights \u03c9k as in (3). Note that this is equivalent to optimizing q so as to minimize KL(p(\u00b7|x) \u2016 q(\u00b7|x)). The details of this derivation can be found in the supplement. For sleep-q-learning we derive the gradients under the model distribution p(x,h):\n\u2202\n\u2202\u03c6 Lq(\u03c6, (x,h) \u223c p(x,h)) '\n\u2202\n\u2202\u03c6 log q\u03c6(h|x) with x,h \u223c p(x,h) (5)"}, {"heading": "2.4 Relation to Wake-Sleep and Variational Bayes", "text": "There has been a resurgence of interest in algorithms related to the wake-sleep algorithm for directed graphical models such as the sigmoidal belief networks (SBN) and the Helmholtz machine (which is a generative SBN that is paired with an approximate inference SBN).\nIn Neural variational inference and learning (NVIL, Mnih and Gregor (2014)) the authors propose to maximize the variational lower bound on the log-likelihood to get a joint objective for both p\u03b8\nand q\u03c6. It was known that this approach results in a gradient estimate of very high variance for the recognition network q (Dayan and Hinton, 1996). In the NVIL paper the authors therefore use variance reductions techniques such as baselines to obtain a practical algorithm that enhances significantly over the original wake-sleep algorithm.\nRecent examples for continuous latent variables include the auto-encoding variational Bayes (Kingma and Welling, 2014a) and stochastic backpropagation papers (Rezende et al., 2014). In both cases one maximizes a variational lower bound on the log-likelihood that is rewritten as two terms: one that is just log-likelihood reconstruction error through a stochastic encoder (approximate inference) - decoder (generative model) pair, and one that regularizes the output of the approximate inference stochastic encoder so that its marginal distribution matches the generative prior on the latent variables (and the latter is also trained, to match the marginal of the encoder output).\nBesides the fact that these variational auto-encoders are only for continuous latent variables, another difference with the reweighted wake-sleep algorithm proposed here is that in the former, a single sample from the approximate inference distribution is sufficient to get an unbiased estimator of the gradient of a proxy (the variational bound). Instead, with the reweighted wake-sleep, a single sample would correspond to regular wake-sleep, which gives a biased estimator of the likelihood gradient. On the other hand, as the number of samples increases, reweighted wake-sleep provides a less biased (asymptotically unbiased) estimator of the log-likelihood and of it\u2019s gradient."}, {"heading": "3 Component Layers", "text": "Although the framework can be readily applied to continuous variables, we here restrict ourselves to distributions over binary visible and binary latent variables. We build our models by combining probabilistic components, each one associated with one of the layers of the generative network or of the inference network. The generative model can therefore be written as p\u03b8(x,h) = p0(x|h1) p1(h1|h2) \u00b7 \u00b7 \u00b7 pL(hL), while the inference network has the form q\u03c6(h |x) = q1(h1 |x) \u00b7 \u00b7 \u00b7 qL(hL |hL\u22121). For a distribution P to be a suitable component we must have a way to efficiently compute P (x(k)|y(k)) given some samples (x(k) , y(k)), and we must have a method to efficiently draw iid samples x(k) from P (x = x(k)|y) given y. In the following we will describe experiments containing three kinds of layers:\nSignoid Belief Network (SBN): A SBN layer (Saul et al., 1996) is a directed graphical model with independent variables xi given the parents y.\nP SBN(xi = 1 |y) = \u03c3(W i,: y + bi). (6)\nAlthough a SBN is a very simple generative model given y, doing inference for y given x is in general intractable.\nDeep AutoRegressive Network (DARN): An autoregressive sigmoid belief network layer (Frey, 1998; Bengio and Bengio, 2000; Gregor et al., 2014) is similar to an SBN layer but with the output units xi not being independent of each other, given the layer\u2019s input y. Instead their dependency is captured by a fully connected directed acyclic graph where the xi can be predicted like in a logistic regression in terms of its predecessors x<i and of the input of the layer, y:\nPDARN(xi = 1 |x<i,y) = \u03c3(W i,: y + Si,<ix<i + bi). (7)\nWe use x<i = (x1, x2, \u00b7 \u00b7 \u00b7 , xi\u22121) to refer to the vector containing the first i-1 observed variables. The matrix S is a lower triangular matrix that contains the autoregressive weights between the observed variables. With Si,<j we refer to the first j-1 elements of the i-th row of this matrix.\nConditional NADE: The Neural Autoregressive Distribution Estimator (NADE, Larochelle and Murray (2011)) is a model that uses an internal, accumulating hidden layer to predict an observed variable xi given the vector containing all previously observed variables xj . DARN is thus a special case of NADE without (deterministic) hidden layer to mediate the conditional dependency between xi and its predecessors. Instead of a logistic regression, that dependency is mediated by an MLP (Bengio and Bengio, 2000).\nP (xi = 1 |x<i) = \u03c3(V i,:\u03c3(W :,<i x<i + a) + bi)). (8)\n100 101 102 training samples\n\u2212130\n\u2212120\n\u2212110\n\u2212100\n\u221290\n\u221280\nFi na\nlL L\nes tim\nat e\n(t es\nts et\n)\nNA DE 200 SBN 10-200-200 SBN 200\nA B\nbias (epoch 50) bias (last epoch) std dev. (epoch50)\n100 101 102 training samples\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nbi as\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nst d-\nde v.\nstd dev. (last epoch)\nFigure 1: A Final log-likelihood estimate wrt number of samples used during training. B L2-norm of the bias and standard deviation of the low-sample estimated p\u03b8 gradient relative to a high-sample (K=5,000) based estimate.\nNVIL wake-sleep wake-sleep reweighted WS reweighted WS P-model size (NLL bound) (NLL bound) (NLL est.) Q-model: SBN Q-model: NADE SBN 200 113.1 120.7 116.3 105.7 96.7 SBN 200-200 99.8 109.4 106.9 98.3 92.3 SBN 200-200-200 96.7 104.4 101.3 94.7 91.8 SBN 10-200-200 97.8 91.9 88.9 DARN 200 91.8 DARN 200-200 95.2 NADE 200 86.8 NADE 200-200 87.6\nTable 1: MNIST for different architectures and network depths. In the third column we cite the numbers reported by Mnih and Gregor (2014). Columns three and four report the variational NLL bounds; columns 5 to 8 report the NLL estimates (See Section 2.2).\nwhere W and V are the encoding and decoding matrices for the NADE hidden layer. For our purposes we need to condition this model on another layer of random variables y:\nPNADE(xi = 1 |x<i,y) = \u03c3(V i,:\u03c3(W :,<i x<i + Ua y + a) + U i,:b y + bi)). (9) Such a conditional NADE has been used previously for modeling musical sequences (BoulangerLewandowski et al., 2012).\nFor each layer distribution we can construct an unconditioned distribution by removing the random variable y (i.e., setting y=0). For a SBN layer we obtain a factorized Bernoulli distribution, for the DARN layer we obtain a fully-visible sigmoid belief network (FVSBN, (Frey, 1998)) and for the conditional NADE layer we obtain a regular, unconditioned NADE. We use such unconditioned distributions as top layer for the generative network p(x,h)."}, {"heading": "4 Experiments", "text": "Here we present series of experiments on the MNIST and the CalTech-Silhouettes datasets. The supplement describes additional experiments on various well known but smaller scale datasets. With these experiments we want to (1) quantitatively analyze the influence of the number of samples K; (2) demonstrate that using a more powerful layer-model for the inference network q can significantly enhance the results even when the generative model is a simple SBN; and (3) that we can reach (close-to) state-of-the-art performance, especially when using powerful layer models such as a conditional NADE. Our implementation is available at https://github.com/jbornschein/ reweighted-ws/."}, {"heading": "4.1 MNIST", "text": "We use the MNIST dataset that was binarized according to Murray and Salakhutdinov (2009) and downloaded in it\u2019s binarized form from (Larochelle, 2011). We use the last 1000 datapoints as\nvalidation set and do early-stopping with a lookahead of 10. For training we use stochastic gradient decent with momentum (\u03b2=0.95) and set mini-batch size to 25. The experiments in this paragraph were run with learning rates of {0.0003, 0.001, and 0.003}. From these three we always report the experiment with the highest validation log-likelihood. In the majority of our experiments a learning rate of 0.001 gave the best results, even across different layer models (SBN, DARN and NADE). If not noted otherwise we use K=5 samples during training and K=500 samples to estimate the final log-likelihood on the test set1.\nTo disentangle the influence of the different q-learning methods we setup p and q networks consisting of SBN layers with three hidden layers of 10, 200 and 200 units (SBN/SBN 10-200-200). After convergence, the model trained using sleep-q-learning reached a final estimated log-likelihood of \u221293.4, the model trained with wake-q-learning reached \u221292.8 and the model trained with both reached \u221291.9. As a control we trained a model that does not update q\u03c6 at all. This model reached \u2212171.4. We confirmed that combining wake-q-learning and sleep-q-learning generally gives the best results by repeating this experiment with various other architectures. For the remainder of this paper we therefore train all models with combined wake-q and sleep-q-learning.\nNext we investigate the influence of the number of samples used during training. The results are visualized in Fig. 4 A. Although the results depend on the layer-distributions used and on the depth and width of the architectures, we generally observe that the final estimated log-likelihood reaches\n1Although we refer to them as log-likelihood estimates, we actually report the lower-bounds of the loglikelihood that can be arbitrarily tightened by increasing the number of test-samples (see Section 2.2).\nplateau when using between 5 and 25 training samples. But we can go one step further: we can quantify the bias and the variance of the gradient estimator (3) using bootstrapping. While training a SBN/SBN 10-200-200 model with K=100 training samples, we use K=5,000 samples to get a high quality estimate of the gradient for a small but fixed set of 25 datapoints (the size of one mini-batch). By repeatedly resampling smaller sets of {1, 2, 5, \u00b7 \u00b7 \u00b7 , 500} samples and computing the gradient based on these, we get a measure for the bias and the variance of the small sample estimates relative the hight quality estimate. These results are visualized in Fig. 4 B. In Fig. 2 A we finally investigate the quality of the log-likelihood estimator (based on equation 1) when applied to the MNIST test set.\nTable 1 summarizes how different architectures compare to each other and how reweighted wakesleep compares to related methods for training directed models. In Table 2 (left) we compare our best trained models to the state-of-the-art results published on MNIST: our model with the largest log-likelihood reaches 85.32 and is a shallow model composed of (conditional) NADEs with with 250 hidden units. This model was trained using 50 epochs with a learning rate of 0.003 and K=5 samples and another 50 epochs with a learning rate of 0.001 and K=25 training samples."}, {"heading": "4.2 CalTech 101 Silhouettes", "text": "We applied reweighted wake-sleep to the 28\u00d728 pixel CalTech 101 Silhouettes dataset. This dataset consists of 4100 examples in the training set, 2264 examples in the validation set and 2307 examples in the test set. We trained various architectures on this dataset using the same hyperparameter as for the MNIST experiments. Table 2 (right) summarizes our results. Note that our best SBN/SBN model is a relatively deep network with 4 hidden layers (300-100-50-10) and reaches a estimated LL of -116.9 on the test set. Our best network, a shallow NADE/NADE-150 network reaches -104.3 and improves over the previous state of the art (\u2212107.8, an RBM with 4000 hidden units by Cho et al. (2013))."}, {"heading": "5 Conclusions", "text": "We have introduced a novel training procedure for deep generative models, which can have either discrete or continuous latent variables, reweighted wake-sleep. It generalises and improves over the wake-sleep algorithm providing a lower bias and lower variance estimator of the log-likelihood and it\u2019s gradient, at the price of more samples from the inference network. We were able to train models to reach or improve upon the state of the art on several discrete data distributions (Caltech Silhouettes in the main paper, various smaller datasets from the UCI repository in the supplemental material). For the MNIST dataset we did not reach the current state of the art (85.23\u00b10.43 for RWS vs. \u2248 84.55 for a DBN). Yet, we could demonstrate that training directed models with reweighted wake-sleep results in competitive models that produce high quality samples and that are, in terms of test set log-likelihood, on par with the most powerful models and training methods in the literature. It is furthermore noteworthy to mention, that we were able to train fairly deep networks with up to 4 layers without layerwise pretraining, without carefully tuned learning schedules and other tricks to enhance learning. We found that even if the generator network uses SBN layers, better results can be obtained with an inference network that has more powerful layers, such as DARN or NADE. However, while our best models with autoregressive layers in the generative network were always\nproviding significantly better results than the models using SBN layers only, these models where always shallow with only one hidden layer. At this point it is unclear if this is due to optimization problems."}, {"heading": "Acknowledgments", "text": "We would like to thank Laurent Dinh, Vincent Dumoulin and Li Yao for helpful discussions and the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) for their powerful software. We furthermore acknowledge CIFAR and Canada Research Chairs for funding and Compute Canada, and Calcul Que\u0301bec for providing computational resources."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Now Publishers", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Modeling high-dimensional discrete data with multi-layer neural networks", "author": ["Y. Bengio", "S. Bengio"], "venue": "In NIPS\u201999,", "citeRegEx": "Bengio and Bengio,? \\Q2000\\E", "shortCiteRegEx": "Bengio and Bengio", "year": 2000}, {"title": "Better mixing via deep representations", "author": ["Y. Bengio", "G. Mesnil", "Y. Dauphin", "S. Rifai"], "venue": "In Proceedings of the 30th International Conference on Machine Learning (ICML\u201913)", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "In ICML\u20192012", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Enhanced gradient for training restricted boltzmann machines", "author": ["K. Cho", "T. Raiko", "A. Ilin"], "venue": "Neural computation,", "citeRegEx": "Cho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2013}, {"title": "The Helmholtz machine", "author": ["P. Dayan", "G.E. Hinton", "R.M. Neal", "R.S. Zemel"], "venue": "Neural computation,", "citeRegEx": "Dayan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1995}, {"title": "Graphical models for machine learning and digital communication", "author": ["B.J. Frey"], "venue": null, "citeRegEx": "Frey,? \\Q1998\\E", "shortCiteRegEx": "Frey", "year": 1998}, {"title": "Deep autoregressive networks", "author": ["K. Gregor", "A. Mnih", "D. Wierstra"], "venue": null, "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "The wake-sleep algorithm for unsupervised neural networks", "author": ["G.E. Hinton", "P. Dayan", "B.J. Frey", "R.M. Neal"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR)", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Efficient gradient-based inference through transformations between bayes nets and neural nets", "author": ["D.P. Kingma", "M. Welling"], "venue": "Technical report,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Binarized mnist dataset. http://www.cs.toronto.edu/ \u0303larocheh/ public/datasets/binarized_mnist/binarized_mnist_[train|valid|test].amat", "author": ["H. Larochelle"], "venue": null, "citeRegEx": "Larochelle,? \\Q2011\\E", "shortCiteRegEx": "Larochelle", "year": 2011}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u20192011),", "citeRegEx": "Larochelle and Murray,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray", "year": 2011}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "A deep and tractable density estimator", "author": ["B.U.I. Murray", "H. Larochelle"], "venue": null, "citeRegEx": "Murray and Larochelle,? \\Q2014\\E", "shortCiteRegEx": "Murray and Larochelle", "year": 2014}, {"title": "Evaluating probabilities under high-dimensional latent variable models", "author": ["I. Murray", "R. Salakhutdinov"], "venue": "In NIPS\u201908,", "citeRegEx": "Murray and Salakhutdinov,? \\Q2009\\E", "shortCiteRegEx": "Murray and Salakhutdinov", "year": 2009}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "Technical report,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Salakhutdinov and Murray,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Murray", "year": 2008}, {"title": "Mean field theory for sigmoid belief networks", "author": ["L.K. Saul", "T. Jaakkola", "M.I. Jordan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Saul et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Saul et al\\.", "year": 1996}], "referenceMentions": [{"referenceID": 11, "context": "This is unfortunate because, as has been argued previously (Hinton et al., 2006; Bengio, 2009), a deeper generative model has the potential to capture high-level abstractions and thus generalize better.", "startOffset": 59, "endOffset": 94}, {"referenceID": 1, "context": "This is unfortunate because, as has been argued previously (Hinton et al., 2006; Bengio, 2009), a deeper generative model has the potential to capture high-level abstractions and thus generalize better.", "startOffset": 59, "endOffset": 94}, {"referenceID": 7, "context": "The exact log-likelihood gradient is intractable, be it for Helmholtz machines (Dayan et al., 1995), sigmoid belief networks (SBNs), or deep belief networks (DBNs) (Hinton et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 11, "context": ", 1995), sigmoid belief networks (SBNs), or deep belief networks (DBNs) (Hinton et al., 2006), which are directed, or deep Boltzmann machines (DBMs), which are undirected.", "startOffset": 72, "endOffset": 93}, {"referenceID": 10, "context": "Past proposals to train Helmholtz machines and DBNs rely on maximizing a variational bound as proxy for the log-likelihood (Hinton et al., 1995; Kingma and Welling, 2014b; Rezende et al., 2014).", "startOffset": 123, "endOffset": 193}, {"referenceID": 19, "context": "Past proposals to train Helmholtz machines and DBNs rely on maximizing a variational bound as proxy for the log-likelihood (Hinton et al., 1995; Kingma and Welling, 2014b; Rezende et al., 2014).", "startOffset": 123, "endOffset": 193}, {"referenceID": 10, "context": "The first of these is the wake-sleep algorithm (Hinton et al., 1995), which relies on combining a \u201crecognition\u201d network (which we call an approximate inference network, here, or simply inference network) with a \u201cgenerative\u201d network.", "startOffset": 47, "endOffset": 68}, {"referenceID": 3, "context": "Unlike in the case of DBMs, which rely on a Markov chain to get samples and estimate the gradient by a mean over those samples, here the samples are iid, avoiding the very serious problem of mixing between modes that can plague MCMC methods (Bengio et al., 2013) when training undirected graphical models.", "startOffset": 241, "endOffset": 262}, {"referenceID": 10, "context": "In the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995), the top layer, hL, has a factorized unconditional distribution, so that ancestral sampling can proceed from hL down to h1 and then the generated sample x is generated by the bottom layer, given h1.", "startOffset": 25, "endOffset": 66}, {"referenceID": 7, "context": "In the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995), the top layer, hL, has a factorized unconditional distribution, so that ancestral sampling can proceed from hL down to h1 and then the generated sample x is generated by the bottom layer, given h1.", "startOffset": 25, "endOffset": 66}, {"referenceID": 11, "context": "In the deep belief network (DBN) (Hinton et al., 2006), the top layer is instead generated by a restricted Boltzmann machine (RBM), i.", "startOffset": 33, "endOffset": 54}, {"referenceID": 11, "context": "In the case of a DBN (where the top layer is generated by an RBM), there is an intermediate solution, which has been proposed in (Hinton et al., 2006) and called contrastive wake-sleep.", "startOffset": 129, "endOffset": 150}, {"referenceID": 16, "context": "In Neural variational inference and learning (NVIL, Mnih and Gregor (2014)) the authors propose to maximize the variational lower bound on the log-likelihood to get a joint objective for both p\u03b8", "startOffset": 52, "endOffset": 75}, {"referenceID": 19, "context": "Recent examples for continuous latent variables include the auto-encoding variational Bayes (Kingma and Welling, 2014a) and stochastic backpropagation papers (Rezende et al., 2014).", "startOffset": 158, "endOffset": 180}, {"referenceID": 21, "context": "In the following we will describe experiments containing three kinds of layers: Signoid Belief Network (SBN): A SBN layer (Saul et al., 1996) is a directed graphical model with independent variables xi given the parents y.", "startOffset": 122, "endOffset": 141}, {"referenceID": 8, "context": "Deep AutoRegressive Network (DARN): An autoregressive sigmoid belief network layer (Frey, 1998; Bengio and Bengio, 2000; Gregor et al., 2014) is similar to an SBN layer but with the output units xi not being independent of each other, given the layer\u2019s input y.", "startOffset": 83, "endOffset": 141}, {"referenceID": 2, "context": "Deep AutoRegressive Network (DARN): An autoregressive sigmoid belief network layer (Frey, 1998; Bengio and Bengio, 2000; Gregor et al., 2014) is similar to an SBN layer but with the output units xi not being independent of each other, given the layer\u2019s input y.", "startOffset": 83, "endOffset": 141}, {"referenceID": 9, "context": "Deep AutoRegressive Network (DARN): An autoregressive sigmoid belief network layer (Frey, 1998; Bengio and Bengio, 2000; Gregor et al., 2014) is similar to an SBN layer but with the output units xi not being independent of each other, given the layer\u2019s input y.", "startOffset": 83, "endOffset": 141}, {"referenceID": 2, "context": "Instead of a logistic regression, that dependency is mediated by an MLP (Bengio and Bengio, 2000).", "startOffset": 72, "endOffset": 97}, {"referenceID": 12, "context": "Conditional NADE: The Neural Autoregressive Distribution Estimator (NADE, Larochelle and Murray (2011)) is a model that uses an internal, accumulating hidden layer to predict an observed variable xi given the vector containing all previously observed variables xj .", "startOffset": 74, "endOffset": 103}, {"referenceID": 16, "context": "In the third column we cite the numbers reported by Mnih and Gregor (2014). Columns three and four report the variational NLL bounds; columns 5 to 8 report the NLL estimates (See Section 2.", "startOffset": 52, "endOffset": 75}, {"referenceID": 8, "context": "For a SBN layer we obtain a factorized Bernoulli distribution, for the DARN layer we obtain a fully-visible sigmoid belief network (FVSBN, (Frey, 1998)) and for the conditional NADE layer we obtain a regular, unconditioned NADE.", "startOffset": 139, "endOffset": 151}, {"referenceID": 14, "context": "1 MNIST We use the MNIST dataset that was binarized according to Murray and Salakhutdinov (2009) and downloaded in it\u2019s binarized form from (Larochelle, 2011).", "startOffset": 140, "endOffset": 158}, {"referenceID": 17, "context": "1 MNIST We use the MNIST dataset that was binarized according to Murray and Salakhutdinov (2009) and downloaded in it\u2019s binarized form from (Larochelle, 2011).", "startOffset": 65, "endOffset": 97}, {"referenceID": 15, "context": "Table 2: Results for various reweighted wake-sleep trained directed models (top rows) in relation to previously published methods (bottom): [1] (Larochelle and Murray, 2011), [2] (Murray and Larochelle, 2014), [3] (Salakhutdinov and Murray, 2008), [4] (Murray and Salakhutdinov, 2009), [5] (Cho et al.", "startOffset": 144, "endOffset": 173}, {"referenceID": 17, "context": "Table 2: Results for various reweighted wake-sleep trained directed models (top rows) in relation to previously published methods (bottom): [1] (Larochelle and Murray, 2011), [2] (Murray and Larochelle, 2014), [3] (Salakhutdinov and Murray, 2008), [4] (Murray and Salakhutdinov, 2009), [5] (Cho et al.", "startOffset": 179, "endOffset": 208}, {"referenceID": 20, "context": "Table 2: Results for various reweighted wake-sleep trained directed models (top rows) in relation to previously published methods (bottom): [1] (Larochelle and Murray, 2011), [2] (Murray and Larochelle, 2014), [3] (Salakhutdinov and Murray, 2008), [4] (Murray and Salakhutdinov, 2009), [5] (Cho et al.", "startOffset": 214, "endOffset": 246}, {"referenceID": 18, "context": "Table 2: Results for various reweighted wake-sleep trained directed models (top rows) in relation to previously published methods (bottom): [1] (Larochelle and Murray, 2011), [2] (Murray and Larochelle, 2014), [3] (Salakhutdinov and Murray, 2008), [4] (Murray and Salakhutdinov, 2009), [5] (Cho et al.", "startOffset": 252, "endOffset": 284}, {"referenceID": 6, "context": "Table 2: Results for various reweighted wake-sleep trained directed models (top rows) in relation to previously published methods (bottom): [1] (Larochelle and Murray, 2011), [2] (Murray and Larochelle, 2014), [3] (Salakhutdinov and Murray, 2008), [4] (Murray and Salakhutdinov, 2009), [5] (Cho et al., 2013).", "startOffset": 290, "endOffset": 308}, {"referenceID": 6, "context": "8, an RBM with 4000 hidden units by Cho et al. (2013)).", "startOffset": 36, "endOffset": 54}, {"referenceID": 4, "context": "We would like to thank Laurent Dinh, Vincent Dumoulin and Li Yao for helpful discussions and the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) for their powerful software.", "startOffset": 118, "endOffset": 163}, {"referenceID": 0, "context": "We would like to thank Laurent Dinh, Vincent Dumoulin and Li Yao for helpful discussions and the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012) for their powerful software.", "startOffset": 118, "endOffset": 163}], "year": 2014, "abstractText": "Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is confirmed experimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure, which also provides a natural way to estimate the likelihood itself. Based on this interpretation, we propose that a sigmoid belief network is not sufficiently powerful for the layers of the inference network, in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models.", "creator": "LaTeX with hyperref package"}}}