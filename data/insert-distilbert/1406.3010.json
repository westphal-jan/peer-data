{"id": "1406.3010", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2014", "title": "\"Mental Rotation\" by Optimizing Transforming Distance", "abstract": "the human visual system is able to recognize objects despite transformations that can drastically alter their appearance. to handle this end, much major effort has been devoted to the invariance properties of recognition systems. invariance can be engineered ( e. g. convolutional nets ), or learned from data explicitly ( e. g. temporal coherence ) or implicitly ( e. g. by data augmentation ). one idea that has not, to anymore date, been explored is the integration of latent variables which permit a search over a learned space of transformations. motivated by increasing evidence that people mentally simulate transformations in space orientation while comparing examples, so - thus called \" fuzzy mental rotation \", we propose a transforming distance. here, a trained relational texture model actively transforms pairs of examples so that they are maximally behave similar in some geometric feature space yet respect the learned transformational constraints. we apply our method to nearest - neighbour problems on the toronto face database and norb.", "histories": [["v1", "Wed, 11 Jun 2014 19:38:05 GMT  (217kb,D)", "https://arxiv.org/abs/1406.3010v1", null], ["v2", "Fri, 5 Dec 2014 17:55:09 GMT  (210kb,D)", "http://arxiv.org/abs/1406.3010v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["weiguang ding", "graham w taylor"], "accepted": false, "id": "1406.3010"}, "pdf": {"name": "1406.3010.pdf", "metadata": {"source": "CRF", "title": "\u201cMental Rotation\u201d by Optimizing Transforming Distance", "authors": ["Weiguang Ding", "Graham W. Taylor"], "emails": ["wding@uoguelph.ca", "gwtaylor@uoguelph.ca"], "sections": [{"heading": "1 Introduction", "text": "It has long been conjectured that humans, when tasked with recognizing the identity of an object or judging the similarity between objects, mentally simulate transformations of internal representations of those objects. Shepard and Metzler [32] were the first to formalize this phenomenon, and assess it experimentally. They presented people with two line drawings, each of which portrayed a threedimensional object in space. They showed that the reaction time for participants to decide whether the objects were the same (except for a rotation in space) or were different, was linearly related to the angle between the spatial orientation of the two objects. This finding indicated that a type of \u201cmental rotation\u201d was performed. If instead, a rotationally invariant representation of each object was formed, then the time required for a decision would presumably be independent of angle.\nAlthough much progress on object recognition by machines has been inspired by human biology [28], these models have rarely accounted for the explicit transformation of internal representations analogous to human mental rotation. Instead, much focus has been placed on developing recognizers that are invariant to spatial transformations. One example where invariance is engineered into the model is convolutional networks [18], which gain invariance to small translations in the input because they pool features over local regions. An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33]. Additionally, canonical correlation analysis (CCA) [14] and its non-linear variants have been used to model relationships between example pairs, including images of the same object under different view angles [21] and images under different illumination conditions [10]. Another related stream includes various deformable part models [7] and diffeomorphism models [34], where knowledge about specific spatial relationships or classes of transformations are encoded.\nSome non-parametric methods bear a resemblance to mental rotation, for example, nearest-neighbor techniques in which examples are explicitly compared. However, for such methods to work well in\nar X\niv :1\n40 6.\n30 10\nv2 [\ncs .L\nG ]\n5 D\nec 2\ndomains with large intra-class variance, one either needs to maintain a database of essentially all different appearances of objects, or learn an invariant similarity measure [9, 11].\nA class of relational unsupervised learning techniques [23] use multiplicative interactions between inputs to represent correlation patterns across multiple images. One application of these methods has been to learn to represent transformations between image pairs [24]. Such a model can be used to assess the similarity between images and used for nearest-neighbor classification [36]. Another related line of research focuses on disentangling different attributes [6, 30], for example face identity and expression. These models have multiple groups of hidden units corresponding to one group of visible units, where each group learns \u2018absolute\u2019 representation of one attribute. In contrast, relational models [23, 24] learn to encode \u2018relative\u2019 transformations between multiple groups of visible units, where each group represents one transformed instance of an image.\nIn this paper, we propose a neural architecture, inspired by mental rotation, that explicitly transforms representations of images while evaluating their similarity. The two key components of our approach are i) a relational model trained on pairs of images which learns about the range of valid transformations; and ii) an optimization that attempts to transform an image such that its representation matches that of a target while respecting the constraints learned by the relational model."}, {"heading": "2 Background", "text": "Our work attempts to link two areas of study which have to-date remained disparate: unsupervised learning of image transformations and learning a similarity measure such that objects that are perceptually similar will have a high measurable similarity.\nThe latter field has focused on methods which are computationally tractable, often learning a Mahalanobis distance [5] or other functional mapping [9, 11] in which distances can be computed for tasks such as nearest neighbour classification or document retrieval. Such methods typically exploit distance information that is intrinsically available: binary or real-valued similarity or dissimilarity labels, user input, or class-based information. Rarely has the set of known transformations of an object been incorporated in learning. An exception is Hadsell et al. [11], though they do not attempt to model transformations. However, transformations are a cue for learning an invariant measure and often can be acquired cheaply through unlabeled data, for example, video.\nCharacterizing a transformation is central to our task. The field of representation learning [2] is concerned with learning features which untangle unknown underlying factors of variation in data. These representations enable higher-level reasoning without domain-specific engineering. While the majority of these techniques are concerned with learning representations of individual objects (e.g. images) a subclass of these methods aim to learn relations from pairs of objects [24, 25, 16]. These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].\nBoth probabilistic [24, 25]) and non-probabilistic [22] variants of relational feature learning methods exist. The former is based on extending the restricted Boltzmann machine to three-way rather than pairwise interactions. The latter extends auto-encoders in a similar way. Although the auto-encoderbased approach is easier to train using gradient descent, we adopt the RBM formalism in our work due to the simplicity with which its free energy can be computed to score input pairs under the model. Recent work also suggests a means of scoring inputs under an autoencoder [15]."}, {"heading": "3 Methods", "text": "A relational model captures the transformation between meaningful pairs of images (x,y), such as photos taken from different views of the same object, and images with different expressions of the same face. It encodes the transformation information of x to y as a hidden representation h, which can also be used to transform the \u201csource\u201d x towards the \u201ctarget\u201d y."}, {"heading": "3.1 Factored gated restricted Boltzmann machine", "text": "The factored gated RBM (fgRBM) [25] relates x, y and h by the following energy function:\n\u2212E(y,h;x) = N\u2211 n=1 ( I\u2211 i=1 vinxi)( J\u2211 j=1 wjnyj)( M\u2211 m=1 umnhm) + 1 2 J\u2211 j=1 (yj \u2212 bj)2 + M\u2211 l=m cmhm , (1)\nwhere I , J and M are the dimensionality of x, y and h respectively; vin, wjn, and uln are a set of filters that project x, y and h onto N factors. bj and cm are the bias coefficients for y and h respectively. Equation 1 defines a Gaussian-Bernoulli [13] version of fgRBM capable of modeling real-valued data, which has a slightly different energy function from that of the original binary fgRBM in [25]. Equation 1 assumes that x and y are real-valued and h is binary.\nWe express the probability of y and h, conditioned on x, in terms of the defined energy function as:\np(y,h;x) = 1\nZ(x) exp(\u2212E(y,h;x)), (2)\nwhere the partition function Z(x) = \u2211\ny,h exp(\u2212E(y,h;x)) ensures normalization. Marginalizing over h, we obtain the probability distribution of y:\np(y;x) = \u2211\nh\u2208{0,1}M p(y,h;x) =\n1\nZ(x) exp(\u2212F (y;x)), (3)\nwhere F (y;x) is the free energy of y, which is defined as \u2212log \u2211\nh\u2208{0,1}M exp(\u2212E(y,h;x)), or\nF (y;x) = \u22121 2 \u2211 j (yj \u2212 bj)2 \u2212 \u2211 l log(1 + exp( \u2211 f umn \u2211 i vinxi \u2211 j wjnyj + cm)). (4)"}, {"heading": "3.1.1 Factored gated RBM training", "text": "To train the fgRBM, we would like to maximize the average log probability of y given x for a set of training pairs {(x\u03b1,y\u03b1)}:\nL(\u03b8) = \u2211 \u03b1 logp(y\u03b1;x\u03b1) (5)\nwhere \u03b8 represents all the involved parameters.\nThe partial derivative of the likelihood L(\u03b8) with respect to any parameter \u03b8 is\n\u2212\u2202L \u2202\u03b8 = \u2211 \u03b1 \u3008\u2202E(y \u03b1,h;x\u03b1) \u2202\u03b8 \u3009h \u2212 \u3008 \u2202E(y,h;x\u03b1) \u2202\u03b8 \u3009y,h = \u2211 \u03b1 \u2202F (y\u03b1;x\u03b1) \u2202\u03b8 \u2212 \u3008\u2202F (y;x \u03b1) \u2202\u03b8 \u3009y (6)\nwhere \u3008\u3009z denotes the expectation with respect to z. The first term is the derivative of the free energy which is tractable to compute. The second term involves integrating over all possible y and therefore is intractable in non-trivial settings. In this paper, we used the contrastive divergence learning algorithm [12]."}, {"heading": "3.2 Transforming distance", "text": "We can use a trained fgRBM to transform an image x with a given h which encodes a transformation. The transforming function can be derived from the conditional p(y|h;x) by taking the conditional expectation of y|x. We use t(x,h) to represent the transformed x with respect to h. t(x,h) has the same dimensionality as y and we express the jth element of t as\ntj(x,h) = \u2211 n wjn \u2211 i vinxi \u2211 m umnhm + bj . (7)\nWith t(x,h), we can define the transforming distance D between a pair of images (x\u03b1,x\u03b2), given their transformations represented by h\u03b1 and h\u03b2 respectively:\nDs(x \u03b1,x\u03b2 ,h\u03b1) = d ( f (t (x\u03b1,h\u03b1)) , f ( x\u03b2 )) , (8)\nDd(x \u03b1,x\u03b2 ,h\u03b1,h\u03b2) = d ( f (t (x\u03b1,h\u03b1)) , f ( t ( x\u03b2 ,h\u03b2 ))) , (9)\nwhere Ds represents the transforming distance with a single-sided transformation and Dd uses a dual-sided transformation. Intuitively, the difference between these approaches is whether x\u03b1 is transformed to match x\u03b2 or if x\u03b1 and x\u03b2 are both transformed to match each other.\nd(p, q) can be any pairwise distance and f(q) can be any feature representation of q. In this paper, for distance metrics, we simply consider the Euclidean distance; for feature representations, we consider raw pixels, principal component analysis (PCA) and the contractive autoencoder (CAE) [31]. Diagrams for both single and dual-sided transformed distances are shown in Fig. 2.\nBy doing \u201cmental rotation\u201d during image comparison, images undergo learned transformations to minimize their distance. If we only minimize the transforming distance D over h, the transformed image t(x\u03b1,h\u03b1) might be very similar to image x\u03b2 while not respecting the constraints learned by the fgRBM, i.e. not being a \u201cvalid\u201d transformation of x\u03b1. To effectively tie the \u201cidentity\u201d of t(x\u03b1,h\u03b1) to x\u03b1 throughout the transformation, we regularize this optimization by its free energy defined in Equation 4. Accordingly, we have the following cost function:\nL(x\u03b1,x\u03b2 ,h) = D(x\u03b1,x\u03b2 ,h) + \u03bbR(x\u03b1,x\u03b2 ,h) , (10)\nwhere h represents both h\u03b1 and h\u03b2 for brevity. Regularization term R represents the free energy term(s): for a single-sided transformation, R(x\u03b1,h) = F (t(x\u03b1,h);x\u03b1); for a dual-sided transformation R(x\u03b1,h) = F (t(x\u03b1,h);x\u03b1) + F (t(x\u03b2 ,h);x\u03b2). \u03bb is an empirically chosen weight constant to balance D and R (we set \u03bb = 1 in all of our experiments).\nThe optimized fgRBM hidden vector h\u2217 is defined as:\nh\u2217(x\u03b1,x\u03b2) = argmin h L(x\u03b1,x\u03b2 ,h) . (11)\nInstead of optimizing the binary hidden units directly, we optimize the real-valued logits (presigmoid inputs). The pairwise image distance of (x\u03b1,x\u03b2) after mental rotation is defined as:\nD\u2217(x\u03b1,x\u03b2) = D(x\u03b1,x\u03b2 ,h\u2217(x\u03b1,x\u03b2)) . (12)\nWe use gradient descent with momentum to compute Equation 11. To allow parallel processing over pairs on a GPU, we do not set a stopping criterion for individual pairs. We stop the optimization after 30 iterations. We note that more sophisticated gradient-based optimization methods can be used, though they may not be as amenable to GPU-based parallelization."}, {"heading": "4 Experiments", "text": "We considered two datasets: the Toronto Face Database (TFD) [35] and the NORB dataset [19]. A qualitative visual assessment was performed to demonstrate the validity of optimizing the transforming distance. To quantitatively evaluate, we replaced the L2 distance in K nearest neighbor (KNN) algorithm with the proposed transforming distance. We denote this variant of KNN as transforming KNN. Specifically, a fgRBM is trained at the training stage; during test time, this fgRBM defines a transforming distance that is used to match test images with examples in the KNN database."}, {"heading": "4.1 Data", "text": "The TFD contains 102,236 face images of size 48 \u00d7 48. 3,874 of them are labeled with identities of 963 people. Among these, 626 identities (3,537 images) have at least 2 images for each identity. We refer to this subset as the identity-labeled images. We first learned the valid expression transformation within the same face identity using the fgRBM (see Fig. 1b). We expect the transforming distance to be expression invariant and evaluate this by performing identity recognition.\nThe TFD dataset was divided into four sets:\n\u2022 the feature training set, which contains all the identity-unlabeled images and identities with a single image (separated so that features are not biased towards either training or test set);\n\u2022 the KNN training set (database), used for both training the fgRBM and as a KNN database; \u2022 the KNN validation set, used for hyper-parameter cross-validation; \u2022 and the KNN test set.\nTo the best of our knowledge, we are unaware of any reported work on identity recognition using the TFD dataset. Therefore, we explore two methods to divide the identity-labeled images into training, validation and test sets, refered as TFDs1 and TFDs2:\n\u2022 TFDs1: For each identity, randomly take 1 image as test. For the remaining data, randomly take 1 as validation from any identity with at least 2 images. Each random division results in a training set of size 2,540, a validation set of size 371 and a test set of size 626. The training set contains 21,948 image pairs, including self pairs (i.e. the identity transformation).\n\u2022 TFDs2: For each identity with at least 4 images, randomly take 1 image as test and 1 as validation. Each division results in a training set of size 2,937, a validation set of size 300 and a test set of size 300. The training set contains 23,607 image pairs, including self pairs.\nThere are several differences between TFDs1 and TFDs2. In TFDs1, the test set is larger. However, 326 identities have only one example. This challenges the model in two ways: i) identities cannot be learned by the fgRBM; ii) each identity appears only once in the KNN database. In the TFDs2 training set, images always appear in pairs, which favors both fgRBM training and KNN. However, since only identities with at least 4 instances contribute to the validation and test sets, the KNN database is \u201cdiluted\u201d by 891 \u201cextra\u201d images which share identity with neither validation nor test.\nThe NORB dataset contains 96\u00d7 96 stereo image pairs of 50 toys from 5 categories. Images were taken under different lighting conditions, elevations and azimuths [19]. We used the \u201csmall NORB\u201d variant, where the images have a clean background. The training and test sets each contain 24,300 images. For NORB, we try to model the transformation between images of the same object with different azimuth (see Fig. 1c). We expect the transforming distance to be rotation invariant.\nWe arbitrarily assigned all images with instance number 7 to the validation set, which includes 4,860 images, and all images with instance label 4, 6, 8, 9 to the training set, which contains 19,440 images. The training set was used for feature learning, fgRBM training and as the KNN database. We used the default test set split."}, {"heading": "4.2 Model training", "text": "Before training the fgRBM and performing transforming distance, TFD images were preprocessed by local contrast normalization (LCN) [28] with kernel size 9. For the NORB dataset, we only used\nthe first image from each stereo pair. The images were first down-sampled to 32 \u00d7 32 and then preprocessed by LCN with kernel size 3. Examples are shown in Fig. 1 (b) and (c).\nFor TFD, the fgRBM is trained on image pairs with the same identity. The fgRBM encodes valid expression transformation information. For NORB, within each image pair, 2 images share the same class, instance, lighting condition and elevation. The fgRBM encodes the valid azimuth transformation information. Based on the allowed azimuth difference between images, we trained 2 types of models: 1) AnyTrans, where the images within a pair have an arbitrary azimuth difference; and 2) SmallTrans, where the absolute value of azimuth difference is less than or equal to 40\u25e6."}, {"heading": "4.2.1 Hyperparameter selection", "text": "For the feature representation, we cross-validated the choice of PCA and CAE with 64, 128 and 256 hidden units on regular KNN. The best one of these 6 combinations was chosen. For the fgRBM, we cross-validated hidden dimensionality of 64, 128 and 256 by performing a single-sided transforming KNN with pixel distance. For NORB, the K value for KNN was cross-validated in the range of 1 to 30. While for TFD, K was set to 1 because of the nature of the task. The number of factors in the fgRBM is fixed as twice the dimensionality of the hidden units. Due to space constraints, details of the remaining settings are provided in supplemental material."}, {"heading": "4.3 Qualitative evaluation", "text": "To assess the transforming distance optimization process, we observed the transformed image t(x\u03b1,h\u2217) at each optimization step. Fig. 3 shows the optimization process of both same-identity and different-identity image pairs. We see that during the optimization process, the transformed images are all valid transformations of the source images: the faces images preserves their identities, even when an image is transformed to match a face image with different identity; the car and human figure are merely rotated, even when the human figure image is transformed to match the car image. This is an interesting generalization result, since the fgRBM was only trained on same-identity pairs."}, {"heading": "4.4 Quantitative evaluation", "text": "For quantitative evaluation, the transforming distance was combined with KNN as the transforming KNN. It is compared with 2 baseline methods: the regular KNN and a proposed augmented KNN."}, {"heading": "4.4.1 Augmented KNN", "text": "By performing \u201cspace searching\u201d when calculating transforming distances, each example in KNN database covers a volume, a manifold composed of all its valid transformations, in the data space. It is interesting to see if this \u201cexpanding effect\u201d can be achieved by augmenting the KNN database\nusing the learned transformation and then performing regular KNN. This comparison is meaningful especially when the data space is not well covered by the KNN database (e.g. in the TFD dataset).\nTherefore, we proposed the \u201caugmented KNN\u201d and test it on TFD. Additional training images were sampled from a trained fgRBM. We used each image in the KNN database as the \u201csource\u201d image, and performed Gibbs sampling alternating between hidden and target units. We retained one sample every 100 iterations. By repeating this 9 times for every image in the KNN database, we acquired a database 10 times as large as the original database. We are unaware of any prior work which has used a relational model for dataset augmentation. Provided the data to train the relational model is available, this seems like an attractive option to the usual approach of hand-coding perturbations [1, 3]."}, {"heading": "4.4.2 KNN performance", "text": "TFD results are shown in Table 1. For both TFDs1 and TFDs2, we analyzed both single and dual-sided transforming KNN. Results are compared against augmented KNN and regular KNN. Transforming KNN brings 6 \u2013 10 % performance increase for TFDs1 and about 25 % performance increase for TFDs2. Augmented KNN also brings some amount of accuracy increase, but not as high as transforming KNN. This indicates that the augmented database does not cover as much as volume as the transforming distance, at least when the database is expanded tenfold.\nThe performance differences between TFDs1 and TFDs2 correspond to our analysis in Section 4.1. Accuracies in every column is higher in TFDs2 than in TFDs1. This is probably due to the fact that in TFDs2, every test image has at least two corresponding images in the KNN database, while in TFDs1, about half of the test images only have one. The relative performance increase is higher in TFDs2 than in TFDs1. This is probably because that in TFDs2, expression transformation was learned for every identity, while in TFDs1, this was done only on about half of the identities.\nNORB results are given in Table 2. With a cross-validated K value, the SmallTrans KNN has a 4% performance increase over regular KNN. We are not surprised by the marginal improvement of the transforming KNN. This is because the NORB training set contains images of the same object taken from different azimuths (every 20\u25e6), which already provides some degree of rotation invariance. Therefore SmallTrans and AnyTrans can only do slightly better, if they provide a finer rotation invariance, say rotations under 20\u25e6 difference, for some cases. However, the rotation invariance afforded by the training set disappears when there are less examples in the database, which is further illustrated in Fig. 5 and discussed in Section 4.4.3.\nFig. 4 shows KNN accuracy with respect to different K values. For TFD, we can see that both regular KNN and transforming KNN degrade as K increases. This is mainly due to the lack of examples in its KNN database: if K is large, number of noise examples will be larger than true examples even the true examples might have higher similarities, which is also aggravated by the identity-imbalanced nature of TFD. In comparison, augmented KNN is the most robust to larger settings of K, due to the richness of same-identity examples in the KNN database.\nFor NORB, regular KNN accuracy decreases with increasing K. This is because without transforming distance, a test image should only be close to examples with the same class label and, with equal importance, the orientation in the KNN database. Therefore, test images cannot utilize many exam-\nples to provide effective distances. On the contrary, transforming KNN can utilize training examples regardless of their orientation, which results in an increasing robustness with increasing K."}, {"heading": "4.4.3 KNN with a reduced database", "text": "As previously mentioned, under transforming KNN, each example in the database can span a manifold and covers a \u201cvolume\u201d. We examine this assumption by performing KNN on reduced databases and showing the relationship between accuracy and missing data rate (randomly discarded) in Fig. 5. For both TFD and NORB, transforming KNN works reasonably well with missing data. Their performance are still comparable to the regular KNN with complete database, when 70% (TFDs1), 80% (TFDs2) and even 99% (NORB, AnyTrans) data are missing.\nFor TFD, missing data results in missing identities. Therefore, no matter how good the relational model is, a performance drop is inevitable. In comparison, NORB contains only 5 classes. This makes it possible to maintain performance under a very high missing data rate. In Fig. 5 (c), AnyTrans KNN maintains its accuracy even when 99% data are missing (199 examples left in database); SmallTrans KNN is still better than regular KNN when 90% data are missing.\nSmallTrans KNN performs better than AnyTrans KNN when the missing data rate is low, but degrades as the missing data rate increases. We suspect that the fgRBM has difficulty encoding complicated transformations (e.g. arbitrary 3D changes of objects). This conjecture (on the complexity of transformation) is supported by the fact that the best cross-validated fgRBM dimensionality is 128 for SmallTrans and 256 for AnyTrans.\nAlthough the AnyTrans fgRBM might have learned less realistic transformations, it seems to be able to bridge large transformation gaps when the database is sparse. It maintains its performance even when 99% data are missing, and still provides a 60% accuracy when 99.9% data are missing. This hints at the potential of transforming KNN on \u201cweakly\u201d labeled datasets, where only a small portion of data have class labels and the majority of the data has weaker \u201crelational labels\u201d indicating image pairs. Relational labels can be acquired from video. Finally, using a reduced KNN database could potentially speed up transforming KNN by 100 times at test time. This alleviates the additional computational burden brought by test-time optimization."}, {"heading": "5 Conclusion", "text": "The key novelty in our work is the idea of augmenting learned similarity functions with latent variables capturing factors of variation. Although we were inspired by the evidence of a mental faculty for the simulation of spatial transformations, we believe that this is just one example out of a richer class of dynamic similarity models achievable within this framework. That is to say, the proposed framework is generic, since it is composed of interchangeable components.\nNevertheless, performing inference or optimization over latent variables while comparing examples is much more computationally demanding than the typical \u201ctest-time\u201d application of learned similarity models. This can be partially alleviated by a reduced need for large databases. Unfortunately our method precludes the use of approximate nearest neighbour techniques, which are typically used on large-scale problems of the type we considered. We have, however, achieved modest gains by parallelizing on GPUs. We believe that the computational cost is the main concern in considering similarity models with latent variables and we intend to address this issue in future work.\nIn each of the two datasets we considered, only a single transformation class was learned. Relational models like the fgRBM and relational autoencoder can, in theory, capture a rich set of transformations. Therefore another avenue of future work is demonstrating the efficacy of transforming similarity across a wider range of transformations."}], "references": [{"title": "Deep learners benefit more from out-of-distribution", "author": ["Y. Bengio", "F. Bastien", "A. Bergeron", "N. Boulanger-Lewandowski", "T. Breuel", "Y. Chherawala", "M. Cisse", "M. C\u00f4t\u00e9", "D. Erhan", "J. Eustache"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D. Cire\u015fan", "U. Meier", "J. Masci", "L. Gambardella", "J. Schmidhuber"], "venue": "In IJCAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Phone recognition with the mean-covariance restricted boltzmann machine", "author": ["G. Dahl", "M. Ranzato", "A. Mohamed", "G. E Hinton"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Information-theoretic metric learning", "author": ["J. Davis", "B. Kulis", "P. Jain", "S. Sra", "I. Dhillon"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Disentangling factors of variation via generative entangling", "author": ["Guillaume Desjardins", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1210.5474,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Object detection with discriminatively trained part-based models", "author": ["Pedro F Felzenszwalb", "Ross B Girshick", "David McAllester", "Deva Ramanan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Learning invariance from transformation sequences", "author": ["P. F\u00f6ldi\u00e1k"], "venue": "Neural Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Spatio-chromatic adaptation via higher-order canonical correlation analysis of natural images", "author": ["Michael U Gutmann", "Valero Laparra", "Aapo Hyv\u00e4rinen", "Jes\u00fas Malo"], "venue": "PloS one,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Comput.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Relations between two sets of variates", "author": ["Harold Hotelling"], "venue": "Biometrika, pages 321\u2013377,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1936}, {"title": "On autoencoder scoring", "author": ["H. Kamyshanska", "R. Memisevic"], "venue": "In ICML, pages 720\u2013728,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Emergence of complex cell properties by learning to generalize in natural scenes", "author": ["Y. Karklin", "M. Lewicki"], "venue": "Nature, 457(7225):83\u201386,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "The role of spatio-temporal synchrony in the encoding of motion", "author": ["K. Konda", "R. Memisevic", "V. Michalski"], "venue": "In Proc. of International Conference on Learning Representations,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "Fu J. Huang", "L. Bottou"], "venue": "In CVPR,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Learning invariant representations and applications to face verification", "author": ["Q. Liao", "J. Leibo", "T. Poggio"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Appearance models based on kernel canonical correlation analysis", "author": ["Thomas Melzer", "Michael Reiter", "Horst Bischof"], "venue": "Pattern recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1961}, {"title": "Gradient-based learning of higher-order image features", "author": ["R. Memisevic"], "venue": "In ICCV. IEEE,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Learning to relate images", "author": ["R. Memisevic"], "venue": "IEEE TPAMI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Unsupervised learning of image transformations", "author": ["R. Memisevic", "G. Hinton"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Learning to represent spatial transformations with factored higher-order boltzmann machines", "author": ["R. Memisevic", "G. Hinton"], "venue": "Neural Comput.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Deep learning from temporal coherence in video", "author": ["H. Mobahi", "R. Collobert", "J. Weston"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Transformation Pursuit for Image Classification", "author": ["M. Paulin", "J. Revaud", "Z. Harchaoui", "F. Perronnin", "C. Schmid"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Why is real-world visual object recognition hard", "author": ["N. Pinto", "D. Cox", "J. DiCarlo"], "venue": "PLoS compbiol,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Modeling pixel means and covariances using factorized third-order boltzmann machines", "author": ["M. Ranzato", "G. Hinton"], "venue": "In CVPR,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Mental rotation of three-dimensional objects", "author": ["R. Shepard", "J. Metzler"], "venue": "Science, 171(3972):701,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1971}, {"title": "Learning invariant representations with local transformations", "author": ["K. Sohn", "H. Lee"], "venue": "In ICML,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Novel morphometric based classification via diffeomorphic based shape representation using manifold learning", "author": ["Rachel Sparks", "Anant Madabhushi"], "venue": "In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "The toronto face database", "author": ["J. Susskind", "A. Anderson", "G. Hinton"], "venue": "U of Toronto, Tech. Rep,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Modeling the joint density of two images under a variety of transformations", "author": ["J. Susskind", "R. Memisevic", "G. Hinton", "M. Pollefeys"], "venue": "In CVPR,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Convolutional learning of spatio-temporal features", "author": ["G. Taylor", "R. Fergus", "Y. LeCun", "C. Bregler"], "venue": "In ECCV,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}], "referenceMentions": [{"referenceID": 30, "context": "Shepard and Metzler [32] were the first to formalize this phenomenon, and assess it experimentally.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "Although much progress on object recognition by machines has been inspired by human biology [28], these models have rarely accounted for the explicit transformation of internal representations analogous to human mental rotation.", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "One example where invariance is engineered into the model is convolutional networks [18], which gain invariance to small translations in the input because they pool features over local regions.", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 121, "endOffset": 131}, {"referenceID": 2, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 121, "endOffset": 131}, {"referenceID": 26, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 121, "endOffset": 131}, {"referenceID": 7, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 155, "endOffset": 166}, {"referenceID": 25, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 155, "endOffset": 166}, {"referenceID": 19, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 155, "endOffset": 166}, {"referenceID": 31, "context": "An alternative is to learn invariance, for example by augmenting the training set with perturbations of the training set [1, 3, 27], through temporal cues [8, 26, 20] or incorporating linear transformation operators into feature learning algorithms [33].", "startOffset": 249, "endOffset": 253}, {"referenceID": 13, "context": "Additionally, canonical correlation analysis (CCA) [14] and its non-linear variants have been used to model relationships between example pairs, including images of the same object under different view angles [21] and images under different illumination conditions [10].", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "Additionally, canonical correlation analysis (CCA) [14] and its non-linear variants have been used to model relationships between example pairs, including images of the same object under different view angles [21] and images under different illumination conditions [10].", "startOffset": 209, "endOffset": 213}, {"referenceID": 9, "context": "Additionally, canonical correlation analysis (CCA) [14] and its non-linear variants have been used to model relationships between example pairs, including images of the same object under different view angles [21] and images under different illumination conditions [10].", "startOffset": 265, "endOffset": 269}, {"referenceID": 6, "context": "Another related stream includes various deformable part models [7] and diffeomorphism models [34], where knowledge about specific spatial relationships or classes of transformations are encoded.", "startOffset": 63, "endOffset": 66}, {"referenceID": 32, "context": "Another related stream includes various deformable part models [7] and diffeomorphism models [34], where knowledge about specific spatial relationships or classes of transformations are encoded.", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "domains with large intra-class variance, one either needs to maintain a database of essentially all different appearances of objects, or learn an invariant similarity measure [9, 11].", "startOffset": 175, "endOffset": 182}, {"referenceID": 10, "context": "domains with large intra-class variance, one either needs to maintain a database of essentially all different appearances of objects, or learn an invariant similarity measure [9, 11].", "startOffset": 175, "endOffset": 182}, {"referenceID": 22, "context": "A class of relational unsupervised learning techniques [23] use multiplicative interactions between inputs to represent correlation patterns across multiple images.", "startOffset": 55, "endOffset": 59}, {"referenceID": 23, "context": "One application of these methods has been to learn to represent transformations between image pairs [24].", "startOffset": 100, "endOffset": 104}, {"referenceID": 34, "context": "Such a model can be used to assess the similarity between images and used for nearest-neighbor classification [36].", "startOffset": 110, "endOffset": 114}, {"referenceID": 5, "context": "Another related line of research focuses on disentangling different attributes [6, 30], for example face identity and expression.", "startOffset": 79, "endOffset": 86}, {"referenceID": 22, "context": "In contrast, relational models [23, 24] learn to encode \u2018relative\u2019 transformations between multiple groups of visible units, where each group represents one transformed instance of an image.", "startOffset": 31, "endOffset": 39}, {"referenceID": 23, "context": "In contrast, relational models [23, 24] learn to encode \u2018relative\u2019 transformations between multiple groups of visible units, where each group represents one transformed instance of an image.", "startOffset": 31, "endOffset": 39}, {"referenceID": 4, "context": "The latter field has focused on methods which are computationally tractable, often learning a Mahalanobis distance [5] or other functional mapping [9, 11] in which distances can be computed for tasks such as nearest neighbour classification or document retrieval.", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "The latter field has focused on methods which are computationally tractable, often learning a Mahalanobis distance [5] or other functional mapping [9, 11] in which distances can be computed for tasks such as nearest neighbour classification or document retrieval.", "startOffset": 147, "endOffset": 154}, {"referenceID": 10, "context": "The latter field has focused on methods which are computationally tractable, often learning a Mahalanobis distance [5] or other functional mapping [9, 11] in which distances can be computed for tasks such as nearest neighbour classification or document retrieval.", "startOffset": 147, "endOffset": 154}, {"referenceID": 10, "context": "[11], though they do not attempt to model transformations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The field of representation learning [2] is concerned with learning features which untangle unknown underlying factors of variation in data.", "startOffset": 37, "endOffset": 40}, {"referenceID": 23, "context": "images) a subclass of these methods aim to learn relations from pairs of objects [24, 25, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 24, "context": "images) a subclass of these methods aim to learn relations from pairs of objects [24, 25, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 15, "context": "images) a subclass of these methods aim to learn relations from pairs of objects [24, 25, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 28, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 82, "endOffset": 89}, {"referenceID": 3, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 82, "endOffset": 89}, {"referenceID": 23, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 122, "endOffset": 134}, {"referenceID": 34, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 122, "endOffset": 134}, {"referenceID": 21, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 122, "endOffset": 134}, {"referenceID": 35, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 190, "endOffset": 198}, {"referenceID": 16, "context": "These techniques have been applied to feature covariances in image and audio data [29, 4], learning image transformations [24, 36, 22], and spatio-temporal features for activity recognition [37, 17].", "startOffset": 190, "endOffset": 198}, {"referenceID": 23, "context": "Both probabilistic [24, 25]) and non-probabilistic [22] variants of relational feature learning methods exist.", "startOffset": 19, "endOffset": 27}, {"referenceID": 24, "context": "Both probabilistic [24, 25]) and non-probabilistic [22] variants of relational feature learning methods exist.", "startOffset": 19, "endOffset": 27}, {"referenceID": 21, "context": "Both probabilistic [24, 25]) and non-probabilistic [22] variants of relational feature learning methods exist.", "startOffset": 51, "endOffset": 55}, {"referenceID": 14, "context": "Recent work also suggests a means of scoring inputs under an autoencoder [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "The factored gated RBM (fgRBM) [25] relates x, y and h by the following energy function:", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "Equation 1 defines a Gaussian-Bernoulli [13] version of fgRBM capable of modeling real-valued data, which has a slightly different energy function from that of the original binary fgRBM in [25].", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "Equation 1 defines a Gaussian-Bernoulli [13] version of fgRBM capable of modeling real-valued data, which has a slightly different energy function from that of the original binary fgRBM in [25].", "startOffset": 189, "endOffset": 193}, {"referenceID": 11, "context": "In this paper, we used the contrastive divergence learning algorithm [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 29, "context": "In this paper, for distance metrics, we simply consider the Euclidean distance; for feature representations, we consider raw pixels, principal component analysis (PCA) and the contractive autoencoder (CAE) [31].", "startOffset": 206, "endOffset": 210}, {"referenceID": 33, "context": "We considered two datasets: the Toronto Face Database (TFD) [35] and the NORB dataset [19].", "startOffset": 60, "endOffset": 64}, {"referenceID": 18, "context": "We considered two datasets: the Toronto Face Database (TFD) [35] and the NORB dataset [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 18, "context": "Images were taken under different lighting conditions, elevations and azimuths [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 27, "context": "Before training the fgRBM and performing transforming distance, TFD images were preprocessed by local contrast normalization (LCN) [28] with kernel size 9.", "startOffset": 131, "endOffset": 135}, {"referenceID": 0, "context": "Provided the data to train the relational model is available, this seems like an attractive option to the usual approach of hand-coding perturbations [1, 3].", "startOffset": 150, "endOffset": 156}, {"referenceID": 2, "context": "Provided the data to train the relational model is available, this seems like an attractive option to the usual approach of hand-coding perturbations [1, 3].", "startOffset": 150, "endOffset": 156}], "year": 2014, "abstractText": "The human visual system is able to recognize objects despite transformations that can drastically alter their appearance. To this end, much effort has been devoted to the invariance properties of recognition systems. Invariance can be engineered (e.g. convolutional nets), or learned from data explicitly (e.g. temporal coherence) or implicitly (e.g. by data augmentation). One idea that has not, to date, been explored is the integration of latent variables which permit a search over a learned space of transformations. Motivated by evidence that people mentally simulate transformations in space while comparing examples, so-called \u201cmental rotation\u201d, we propose a transforming distance. Here, a trained relational model actively transforms pairs of examples so that they are maximally similar in some feature space yet respect the learned transformational constraints. We apply our method to nearest-neighbour problems on the Toronto Face Database and NORB.", "creator": "LaTeX with hyperref package"}}}