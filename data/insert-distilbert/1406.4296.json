{"id": "1406.4296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2014", "title": "Self-Learning Camera: Autonomous Adaptation of Object Detectors to Unlabeled Video Streams", "abstract": "learning object detectors requires massive amounts of labeled training samples from the specific data source of interest. this is impractical when dealing with many different sources ( e. g., in camera networks ), or constantly frequently changing ones such as mobile cameras ( e. g., in robotics or driving assistant brake systems ). in solving this paper, we address the problem of self - evaluating learning detectors in an autonomous manner, i. e. ( i ) detectors continuously updating themselves to efficiently adapt to streaming data sources ( contrary to transductive algorithms ), ( ii ) without any labeled data gathering strongly related to the target data stream ( contrary to self - paced learning ), and ( iii ) without manual intervention to set aside and update hyper - parameters. to that end, we propose an unsupervised, on - line, and self - tuning learning algorithm to optimize accurately a multi - task learning convex objective. our method uses confident but laconic oracles ( high - precision but low - recall off - the - shelf generic detectors ), and exploits the structure of the problem graph to jointly learn on - line an ensemble of instance - description level trackers, from which we derive an adapted category - level object detector. our approach is validated on real - state world publicly available video object datasets.", "histories": [["v1", "Tue, 17 Jun 2014 09:51:18 GMT  (1261kb,D)", "https://arxiv.org/abs/1406.4296v1", null], ["v2", "Wed, 18 Jun 2014 12:33:22 GMT  (1261kb,D)", "http://arxiv.org/abs/1406.4296v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["adrien gaidon", "gloria zen", "jose a rodriguez-serrano"], "accepted": false, "id": "1406.4296"}, "pdf": {"name": "1406.4296.pdf", "metadata": {"source": "CRF", "title": "Self-Learning Camera: Autonomous Adaptation of Object Detectors to Unlabeled Video Streams", "authors": ["Adrien Gaidon", "Gloria Zen", "Jose A. Rodriguez-Serrano"], "emails": ["adrien.gaidon@xrce.xerox.com"], "sections": [{"heading": "1 Introduction", "text": "The theoretical and practical success of many machine learning algorithms often rests on two fundamental assumptions: (i) access to many independent and identically distributed (i.i.d. ) samples, (ii) stationarity of the underlying data distribution (and in particular, that it does not change from the observed samples to the unobserved ones). This explains why data collection and labeling are crucial tasks for most supervised learning algorithms. In practice, these tasks are particularly challenging in videos, due in part to the large volume and variability typically observed in video collections.\nIn this paper, we focus on the problem of on-line unsupervised learning of classifiers, where the data source is a stream of unlabeled and non-i.i.d. samples drawn from a non-stationary distribution. In particular, we propose an unsupervised adaptation method to learn models (video object detectors) that automatically and continuously adapt over time, which we refer to as self-learning and autonomous adaptation.\nInstead of relying on labeled data strongly related to the target distribution, we assume only the availability of confident but laconic oracles: black-box classifiers pre-trained on standard generalpurpose datasets in a high-precision but low-recall regime, so that they can automatically label a few\n\u2217adrien.gaidon@xrce.xerox.com\nar X\niv :1\n40 6.\n42 96\nv2 [\ncs .C\nV ]\n1 8\nof the easiest target samples. We propose to learn a classifier from the underlying latent commonalities between these samples using an on-line multi-task learning formulation allowing to automatically label and incorporate more difficult target samples (in particular negative ones) along a data stream. Consequently, our approach differs from most unsupervised domain adaptation methods, as we do not rely only on easy target samples (in contrast to works inspired by the self-paced learning of Kumar et al. [1]), and we are not limited to the stationary and transductive learning settings (see Section 2 for a more detailed discussion). Figure 1 presents an overview of our approach.\nSelf-adapting a classifier might suffer from negative transfer, either because of over-fitting to only a few positive samples, or because of drifting due to the inclusion of false positives. Another challenge is that there is in general no way to tune hyper-parameters due to the lack of labeled target validation data. In this work, we address these issues by leveraging the spatio-temporal structure of video data. First, in Section 3, we propose to use multi-task learning in conjunction with tracking as a way to automatically elicit a useful set of target positives and negatives, especially \u201chard negatives\u201d, which are crucial for high detection performance [2, 3]. Our multi-task learning objective yields an adapted shared latent model of the category, which bridges the gap between instance-level models used for tracking and category-level models used for detection. Second, in Section 4, we detail how to learn this model. We propose a novel efficient on-line algorithm applicable to non-stationary streaming scenarios. Our method rests on the combination of recent stochastic optimization techniques with a novel hyper-parameter self-tuning method exploiting a \u201cno-teleportation and no-cloning\u201d assumption. Experiments, in Section 5, on two challenging real-world datasets illustrate that we are able to learn and adapt detectors from scratch on different scenes with no labeled data."}, {"heading": "2 Related work", "text": "Unsupervised domain adaptation approaches use annotated data from a fixed dataset (the source set) and only unlabeled data from the target dataset. They often require one or more passes over a large pool of annotated samples in order to adapt the model to each new target dataset, which makes them impractical for continuous adaptation to streaming data. For instance, Taskar et al. [4] leverage \u201cunseen\u201d features from instances classified with high confidence, learning which features are useful for classification, and Gong et al. [5] \u201creshape\u201d datasets to minimize their distribution mismatch. These methods assume that the target domain is stationary, with many unlabeled target samples readily available at each update (transductive setting [6]). Consequently, these approaches\nare not suited to non-stationary streaming scenarios, such as when using on-board cameras (e.g., for autonomous driving or robotics).\nMost related to our work are methods exploiting the spatio-temporal structure of videos in order to collect target data samples. Prest et al. [7] learn object detectors by relying on joint motion segmentation of a set of videos containing the object of interest moving differently from the background. Their goal is to learn generic detectors that adapt well from video to image data. Tang et al. [8] are inspired by the self-paced learning approach of Kumar et al. [1], i.e. \u201clearn easy things first\u201d, which is designed for fine-tuning on target domains related to a labeled source one. The transductive approach of Tang et al. [8] iteratively re-weights labeled source samples by using the tracks of the closest target samples. Sharma et al. [9] propose a similarly inspired multiple instance learning algorithm, also relying on self-paced learning and off-line iterative re-training of a generic detector. These approaches can only be applied in stationary transductive settings, and do not allow for efficient model adaptation along a particular video stream.\nSeveral tracking-by-detection methods are also related to our work, in particular the trackinglearning-detection approach of Kalal et al. [10] and the multi-task learning approach of Zhang et al. [11]. However, they are designed to work on short sequences, and their goal is to learn only instance-specific models, whereas our aim is to learn and adapt category-level ones. Note that we also differ from the family of multi-class transfer learning methods aiming to share features across categories, such as the \u201clearning to borrow\u201d approach of Lim et al. [12]. These approaches are different in intent from our continuous category-level adaptation to a new data stream. Furthermore, they are not straightforwardly applicable for object detection with one or few categories of interest (e.g., cars and pedestrians)."}, {"heading": "3 Multi-task learning of an ensemble of trackers", "text": ""}, {"heading": "3.1 Tracking-by-detection", "text": "In this work, the term \u201cobject detector\u201d denotes a binary classifier parametrized by a (learned) vector w \u2208 Rd. This classifier computes the probability that a window \u2014 an image sub-region x represented by a feature vector \u03c6(x) \u2208 Rd \u2014 contains an object of the category of interest by:\nP (x) = ( 1 + e\u2212(w T\u03c6(x)+b) )\u22121 . (1)\nWhen a new frame is available, the first step of our method consists in calling a confident but laconic oracle returning a few (or no) automatically labeled samples (image regions with object category labels in our case). We assume that the automatic labels of these seed target samples are mostly correct, which means that they correspond only to the \u201ceasiest\u201d target samples (high-precision but low-recall regime). We track each of these seeds in subsequent frames using a simple but efficient tracking-by-detection algorithm inspired by the P-N learning algorithm of Kalal et al. [10]. The tracker associated to each seed i maintains its own personal detector w(t)i . This detector is run on a new frame t + 1 to return a pool of candidate locations. The new location of object i is then simply the candidate region that yields the closest match with the location in the previous frame (after simple motion interpolation based on sparse optical flow). If there is no match, then the object i is considered lost, and it is not tracked further. If a new location is found, then the corresponding region is considered as positive and the other detections as negatives in order to update the model w\n(t) i of object i. This relies on both a spatio-temporal smoothness assumption (based on the observed motion of each object instance), and the uniqueness of each object being tracked. In other words, we assume there is neither teleportation, nor cloning of the individual instances.\nNote that these negatives are in fact false positives, i.e. \u201cdifficult\u201d target samples, also called hard negatives [2]. Contrary to the easier samples used in self-paced learning (e.g., in Tang et al. [8]), they are key to the successful update of the model. This is, indeed, well established in the object detection community (cf. Felzenszwalb et al. [3] for instance). Furthermore, we conducted preliminary experiments where directly learning a video-specific detector on seed samples as positives and random negatives consistently resulted in negative transfer (i.e. worse performance than the generic detector oracle alone). We also observed this negative transfer when replacing the automatic seed\npositives by the ground truth bounding boxes of the moving objects only. Note that our tracking approach does not rely on background subtraction or motion segmentation, and is therefore applicable to stationary objects and moving cameras.\nThis negative transfer illustrates the difficulty of bridging the gap between instance-level models and category-level ones, and the importance of difficult target samples. Note that there is also a risk of negative transfer when using difficult samples (as they might be wrongly labeled). In the following, we describe how we address these challenges via our novel multi-task tracking formulation to exploit the commonalities between seeds."}, {"heading": "3.2 The Ensemble of Instance Trackers (EIT) model", "text": "Let Nt+1 be the number of object instances currently being tracked, each one associated with its own detector w(t)i . Updating each detector amounts to minimizing the regularized empirical risk:\nw (t+1) i = arg minwi ni\u2211 k=1 `(x (i) k , y (i) k ,wi) + \u03bb\u2126(wi), i = 1, \u00b7 \u00b7 \u00b7 , Nt+1 (2)\nwhere x(i)k is one of the ni training samples of object i in frame t + 1 \u2014 either a random window (to get negatives during initialization), a seed from the generic detector, or a candidate detection obtained by running the current version of the model w(t)i \u2014 and y (i) k is a binary label. The labels are obtained using the aforementioned \u201cno teleportation and no cloning\u201d assumption, with the addition that negative samples overlapping any other object currently tracked are not used (a region that is not Ann\u2019s car does not mean it is not a car). Here, `(x, y,w) is the loss incurred by classifying x as y using parameters w, and \u2126(w) is a regularizer. In our experiments, we use the logistic loss:\n`(x, y,w) = log ( 1 + exp ( \u2212y ( wT\u03c6(x) + b ))) , (3)\nas this gives calibrated probabilities with Eq. (1), and enjoys useful theoretical properties for on-line optimization [13]. The set of Nt+1 trackers, with joint detector parameters W(t) = {w(t)1 , . . . ,w (t) Nt+1 } is denoted as Ensemble of Instance Trackers (EIT). With this notation, we can express the Nt+1 equations in Eq. (2) as a joint minimization over the ensemble parameters:\nW(t+1) = arg min W L(X(t+1),y(t+1),W) + \u03bb\u2126t(W), (4)\nClearly, if L(X(t+1),y(t+1),W) = \u2211 i \u2211 k `(x (i) k , y (i) k ,wi) and \u2126t(W) = \u2211 i \u2126(wi), we recover exactly Eq. (2) where each classifier is learned independently. In order to jointly learn all the classifiers, we impose the following multi-task regularization term instead:\n\u2126t(W) = 1\n2Nt+1 Nt+1\u2211 i=1 \u2016wi \u2212 w\u0304(t)\u201622. (5)\nwhere \u2016wi\u20162 denotes the l2 norm of wi, and w\u0304(t) is the (running) mean of all previous instance models, which comprises all past values of the models of currently tracked or now lost seed-specific detectors. Note that this formulation is closely related to the mean-regularized multi-task learning formulation of Evgeniou and Pontil [14], with the difference that it is designed for on-line learning in streaming scenarios. This regularization promotes solutions where classifiers (past and present) are similar to each other (in the Euclidean sense). Therefore, this regularization prevents each detector from over-fitting to the appearance of the individual object appearances, and allows them to generalize across tasks (object instances) by modeling the latent commonalities (the appearance of the category) with the mean of the trackers.\nImportantly, this provides a theoretical justification to using the running average w\u0304(t) as a single category-level detector. Consequently, once the detectors wi are updated in frame t + 1, a new scene-adapted detector is readily available as:\nw\u0304(t+1) = 1\nN\u0304t +Nt+1 N\u0304tw\u0304(t) + Nt+1\u2211 i=1 w (t+1) i  , (6)\nwhere N\u0304t = \u2211t \u03c4=1N\u03c4 . In our object detection case, this approach can be interpreted as learning a category-level model from the average of instance-specialized models. As we use linear classifiers, this multi-task learning could be seen as an improvement of \u201clate fusion\u201d of exemplar-based models, such as the Exemplar-SVM of Malisiewicz et al. [15]. A major difference is that our models are learned jointly and adapt continuously to both the data stream and other exemplars.\nAnother benefit of this regularization is that it learns a model that is more robust to erroneous seeds (initial false detections of the generic detector), as they are likely to significantly differ from the mean, and, therefore, the corresponding trackers will be quickly under-fitting and lose the object. In contrast, the correct seeds will be tracked for longer, as they share common appearance factors, thus contributing more to the category model. Note, however, that the l2-norm used in Eq. (5) is not robust to outliers. Therefore, the handling of incorrect seeds could be improved by using more robust alternatives \u2014 e.g., sparsity-inducing or trace-norm regularizers, replacing the mean by the median, or outlier detection methods \u2014 at the cost of a more computationally demanding solution, which is also more complex to update on-line than our running average. Our approach is compatible with these improved regularizers, but we focus on the simple regularization of Eq. (5) in order to directly show the potential of our self-tuning on-line multi-task learning algorithm described next."}, {"heading": "4 Continuous self-tuning on-line adaptation", "text": ""}, {"heading": "4.1 Streaming asynchronous optimization", "text": "We solve the optimization problem in Eq. (4) using Averaged Stochastic Gradient Descent (ASGD), also called Polyak-Rupert averaging [16]. This on-line first order method achieves optimal convergence rate with only one pass over the data and a constant learning rate with a logistic regression model [13]. The update rule for each model wi is:\nw (t+1,k) i = w (t+1,k\u22121) i \u2212 \u03b7\n( \u2202`\n\u2202w (x\n(i) k , y (i) k ,w (t+1,k\u22121) i ) +\n\u03bb\nNt+1\n( wi \u2212 w\u0304(t) )) , (7)\nwhere \u03b7 is the learning rate, (x(i)k , y (i) k ) a training sample, k = 1, \u00b7 \u00b7 \u00b7 , ni, and w (t+1,0) i = w (t) i . The actual model used is then the average of all previous updates, which is simply obtained by maintaining a running mean of the updates to compute:\nw (t+1) i =\n1\nni ni\u2211 k=1 w (t+1,k) i (8)\nEquations (4),(5), (6) and (7) show that the learning process is a joint one: the update of the detector wi includes a contribution of all the other detectors (both current and past ones). Another advantage is that our approach is well suited to a streaming and asynchronous scenario, as the update of each specific model wi only relies on the current frame and the current running average of all models. In practice, we use a mini-batch variant of ASGD with potentially multiple passes over the data available at time t+1. This allows to share window preprocessing operations at the frame level, and reduces the variance of the gradients. We also maintain the running average of the parameters not only across gradient descent steps, but also across frames (not depicted in Eq. (8) for simplicity)."}, {"heading": "4.2 Self-tuning of hyper-parameters", "text": "A key issue in our unsupervised on-line learning setting is how to select the values of the different hyper-parameters involved, amongst which are the learning rate \u03b7 in Eq. (7), the regularization parameter \u03bb in Eq. (4), and the number of iterations per mini-batch. This is particularly important, as we have to continuously update appearance models in a streaming fashion, where data samples are likely to be non-i.i.d. , and the video stream to be non-stationary (e.g., in long-term surveillance scenarios). The learning rate, in particular, is a crucial parameter of ASGD, and should be set on a per-update basis in this non-stationary case.\nCross-validation, typically used to tune hyper-parameters, is not applicable in our setting, as we have only one positive example at a time. Furthermore, optimizing the window classification performance is not guaranteed to result in optimal detection performance due to the additional post-processing steps applied in most detection methods [17].\nInstead, we use a strategy consisting in greedily searching for the least-over-fitting parameters \u2014 e.g., smallest \u03b7 and number of iterations, largest \u03bb \u2014 that optimize the rank of the correct detection in the current frame. This only involves running the detector after each tentative update in order to assess whether it yields a top ranked detection strongly overlapping with the tracker\u2019s prediction, and is efficient in practice, as it re-uses the already extracted frame-level computations (in particular the feature extraction). See Figure 2 for a high-level pseudo-code description of our method.\n(a) Self-learning camera algorithm\n1: Input: generic detector, video stream 2: Output: adapted detector w\u0304(t+1) 3: Initialization: w\u0304(0) = 0,W = \u2205 4: while video stream is not finished do 5: Get seeds not inW from the generic detector on current frame t+ 1 6: for each new seed do 7: Learn initial detector warm-started from w\u0304(t) with random negatives 8: Add to pool of instancesW 9: end for\n10: for each instance i inW do 11: Run the detector w(t)i 12: if object i is lost then 13: Remove i fromW 14: else 15: Get {(x(i)k , y (i) k ), k = 1 : ni} from the detections (cf. Sec. 3) 16: Compute w(t+1)i (cf. Alg. 2b) 17: end if 18: end for 19: Compute w\u0304(t+1) (Eq. 6) 20: end while\n(b) Autonomous self-tuning adaptation\n1: Input: w(t)i , {(x (i) k , y (i) k ), k = 1 : ni}\nwith x(i)p the current location estimate of i 2: Output: w(t+1)i 3: for nsteps \u2208 {1, 10, 100} do 4: for \u03bb \u2208 {1.0, 0.9, 0.5, 0.1} do 5: for \u03b7 \u2208 {10\u22125, 10\u22124, \u00b7 \u00b7 \u00b7 , 10\u22121} do 6: Compute tentative w(t+1)i (Eq. 8) 7: Run detector w(t+1)i 8: Compute rank of detection matching x(i)p (if any) 9: if rank is 1 then\n10: return w(t+1)i 11: end if 12: end for 13: end for 14: end for\n# In case no update yielded rank 1 15: return w(t+1)i with best ranked prediction\nFigure 2: Pseudo-code overview of our approach (book-keeping operations like maintaining counters and running averages are not described for simplicity, refer to the main text for more details)"}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Datasets", "text": "In order to validate our approach, we use publicly available benchmarks: five video sequences from CAVIAR1, and a long HD video sequence from VIRAT2. Objects of interest in CAVIAR are persons, while VIRAT objects of interest are cars. Sample images from the two scenarios are shown in Figure 3. Some dataset details are reported in Table 1.\nOn CAVIAR, we use the same sequences as Wang et al. [18] \u2014 Ols1, Ols2, Osow1, Olsr2, and Ose \u2014 and also upscale the image size by a factor of 2.0. We run independent experiments on each of the five sequences. Each sequence is used both for unsupervised learning and evaluation (i.e. the same setting as in [18]). In our case, this corresponds to a scenario where the object detector is continuously adapted along a finite video stream, to later be applied to the entirety of this video. Note that we compare to Wang et al. [18], a state-of-the-art unsupervised video adaptation method also relying only on a pre-trained generic detector, and assuming the most confident detections are true positives. In contrast to our approach however, Wang et al. [18] propose a \u201clazy learning\u201d method consisting in re-ranking the low scoring detections using the similarity to high scoring ones. While this re-ranking process improves the detection performance, the method does not produce an adapted detector, and can, therefore, only be applied in a batch setting, as on the CAVIAR dataset.\n1http://homepages.inf.ed.ac.uk/rbf/CAVIAR/ 2http://www.viratdata.org\nFor VIRAT, we use sequence 0401, because it is fully annotated with both static and moving objects, and corresponds to a typical parking scene. The size (375K objects) and duration of VIRAT-0401 (58K frames) allows splitting the sequence in two parts (of equal length). Our self-learning algorithm is run on the first part of the video to self-learn and autonomously adapt the specific detector, whose generalization performance (on different scenes from the same target domain) is then evaluated on the second part of the video stream."}, {"heading": "5.2 Implementation details", "text": "Generic detector oracle\nLike in [18], we use the state-of-the-art Deformable Part Model (DPM) [3] as our black box generic object detector. This generic detector is pre-trained on Pascal VOC 2007, and is publicly available online3. The performance obtained by DPM is reported in Table 2. In order to use DPM as a confident but laconic oracle, we use only its top 5% detections.\nTracking-by-detection object detector\nOur formulation is applicable to any detector based on a linear classifier, and is, therefore, usable with most state-of-the-art object detection and window representation methods [3, 17, 19]. In our experiments, we use Fisher Vectors (FV) [20], as this representation is particularly suited to our problem for the following reasons. First, FV are among state-of-the-art representations for object detection [17]. Second, they proved to be efficient for both category-level image classification [21] and instance-level retrieval problems [22]. This highlights their potential for both category-level and instance-level appearance modeling. Third, FV are high-dimensional representations, which allows our mean-regularized multi-task learning to, in principle, be able to model complex distributions using a single linear representation. To the best of our knowledge, our method is the first application of FV to tracking.\nIn our experiments, we followed the implementation of Cinbis et al. [17], which yields competitive detection results on Pascal VOC, with the difference that we use a more efficient but approximate sliding window approach similar to the one described in Oneata et al. [23]."}, {"heading": "5.3 Results", "text": "Table 2 contains our quantitative experimental results. Performance is measured using Average Precision (AP), which corresponds to the area under the precision-recall curve. It is the standard metric used to measure object detection performance [24]. Our results using the adapted detector obtained by our multi-task Ensemble of Instance Trackers are reported in the \u201cEIT\u201d row.\nWe compare our approach to three other object detectors applicable in our unsupervised setting: (i) the pre-trained DPM detector used alone over the whole video; (ii) our implementation of the \u201cDetection by Detections\u201d (DbD) unsupervised object detection approach of Wang et al. [18]4; (iii) our EIT algorithm without multi-task regularization, i.e. where the trackers are learned independently, which is referred to as \u201cI-EIT\u201d. Note that we do not compare to additional unsupervised video adaptation methods (e.g., [8, 9]), as our contribution is on autonomous on-line self-adaptation, i.e. we do not assume access to a large fixed labeled dataset related to the target videos, or that all target video frames are available at once (in contrast to [8, 9]).\nFirst, we can observe that our approach improves over the generic detector by +4% on average over the scenarios. This confirms that unsupervised learning from scratch of our simple and efficient detector for a specific video stream can outperform a state of the art, carefully tuned, complex, and generic object detector trained on large amounts of unrelated data. Note that we improve in all scenarios but one (Ols1), which corresponds to the smallest video, which is of roughly ten seconds (cf. Table 1). This means that this video is not enough to learn a detector that is better than the generic detector trained on much more data (thousands of images in the case of Pascal VOC).\nSecond, jointly learning all trackers (EIT) improves by +1.7% on average over learning them independently. For some scenarios, the gain is substantial (+4.4% on Ols2), while for one scenario it slightly degrades performance (\u22120.8% on Ose2). On the one hand, this illustrates that our multitask tracking formulation can help learning a better detector when the tracked seed instances share appearance traits useful to recognize the category of objects of interest. On the other hand, this calls for a more robust multi-task learning algorithm than our simple mean-regularized formulation in order to handle more robustly outliers and the large intra-class variations typically present in broad object categories like cars.\nThird, our approach yields a small improvement over the state-of-the-art DbD method (+0.6% on average over the CAVIAR scenarios). This suggests that our approach is accurate enough to discard the generic detector after seeing a large enough part of the video stream (to get enough seeds), whereas DbD must constantly run the generic detector, as it is a \u201clazy learning\u201d approach based on k-nearest neighbors (it does not learn a stand-alone detector). Note also that DbD could be applied as a post-processing step on the results of our detector in order to improve performance, but this leads to additional computational costs.\n3http://www.cs.berkeley.edu/ rbg/latent/voc-release5.tgz 4\u201cDbD\u201d relies on re-ranking the DPM detections according to their similarities with the FV descriptors of the highest scoring ones (the ones we use as seeds); note that this is a batch off-line approach, therefore, not directly applicable to scenarios like VIRAT"}], "references": [{"title": "Self-paced learning for latent variable models", "author": ["P. Kumar", "B. Packer", "D. Koller"], "venue": "NIPS, 2010. 2, 3", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR, 2005. 2, 3", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "Dand Ramanan"], "venue": "PAMI, 2010. 2, 3, 7, 8", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning on the test data: Leveraging unseen features", "author": ["B. Taskar", "M.-F.Wong", "D. Koller"], "venue": "ICML, 2003. 2", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Reshaping visual datasets for domain adaptation", "author": ["B. Gong", "K. Grauman", "F. Sha"], "venue": "NIPS, 2013. 2", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "ICML, 1999. 2", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning object class detectors from weakly annotated video", "author": ["A. Prest", "C. Leistner", "J. Civera", "C. Schmid", "V. Ferrari"], "venue": "CVPR, 2012. 3", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Shifting weights: Adapting object detectors from image to video", "author": ["K. Tang", "V. Ramanathan", "L. Fei-Fei", "D. Koller"], "venue": "NIPS, 2012. 3, 8", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised incremental learning for improved object detection in a video", "author": ["P. Sharma", "C. Huang", "R. Nevatia"], "venue": "CVPR, 2012. 3, 8", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Tracking-learning-detection", "author": ["Z. Kalal", "K. Mikolajczyk", "J. Matas"], "venue": "PAMI, 2012. 3", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust visual tracking via structured multi-task sparse learning", "author": ["T. Zhang", "B. Ghanem", "S. Liu", "N. Ahuja"], "venue": "IJCV, 2013. 3", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning by borrowing examples for multiclass object detection", "author": ["J.J. Lim", "R. Salakhutdinov", "A. Torralba"], "venue": "NIPS, 2011. 3", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Non-strongly-convex smooth stochastic approximation with convergence rate O ( 1 / n )", "author": ["F. Bach", "E. Moulines"], "venue": "NIPS, 2013. 4, 5", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularized multi\u2013task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "SIGKDD, 2004. 4", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Ensemble of Exemplar-SVMs for object detection and beyond", "author": ["T. Malisiewicz", "A. Gupta", "A. Efros"], "venue": "ICCV, 2011. 5", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B. Polyak", "A. Juditsky"], "venue": "SIAM Journal on Control and Optimization, 1992. 5", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Segmentation driven object detection with Fisher vectors", "author": ["R. Cinbis", "J. Verbeek", "C. Schmid"], "venue": "ICCV, 2013. 6, 7, 8", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Detection by detections: Non-parametric detector adaptation for a video", "author": ["X. Wang", "G. Hua", "T. Han"], "venue": "CVPR, 2012. 6, 7, 8", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR, 2014. 7", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["F. Perronnin", "C. Dance"], "venue": "CVPR, 2007. 7", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Image Classification with the Fisher Vector: Theory and Practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": "IJCV, 2013. 7", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Aggregating local image descriptors into compact codes", "author": ["H. J\u00e9gou", "F. Perronnin", "M. Douze", "J. S\u00e1nchez", "P. P\u00e9rez", "C. Schmid"], "venue": "PAMI, 2012. 7", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient Action Localization with Approximately Normalized Fisher Vectors", "author": ["D. Oneata", "J. Verbeek", "C. Schmid"], "venue": "CVPR, 2014. 8", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, 2010. 8 9", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "[1]), and we are not limited to the stationary and transductive learning settings (see Section 2 for a more detailed discussion).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "First, in Section 3, we propose to use multi-task learning in conjunction with tracking as a way to automatically elicit a useful set of target positives and negatives, especially \u201chard negatives\u201d, which are crucial for high detection performance [2, 3].", "startOffset": 247, "endOffset": 253}, {"referenceID": 2, "context": "First, in Section 3, we propose to use multi-task learning in conjunction with tracking as a way to automatically elicit a useful set of target positives and negatives, especially \u201chard negatives\u201d, which are crucial for high detection performance [2, 3].", "startOffset": 247, "endOffset": 253}, {"referenceID": 3, "context": "[4] leverage \u201cunseen\u201d features from instances classified with high confidence, learning which features are useful for classification, and Gong et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] \u201creshape\u201d datasets to minimize their distribution mismatch.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "These methods assume that the target domain is stationary, with many unlabeled target samples readily available at each update (transductive setting [6]).", "startOffset": 149, "endOffset": 152}, {"referenceID": 6, "context": "[7] learn object detectors by relying on joint motion segmentation of a set of videos containing the object of interest moving differently from the background.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] are inspired by the self-paced learning approach of Kumar et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] iteratively re-weights labeled source samples by using the tracks of the closest target samples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] propose a similarly inspired multiple instance learning algorithm, also relying on self-paced learning and off-line iterative re-training of a generic detector.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] and the multi-task learning approach of Zhang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "\u201cdifficult\u201d target samples, also called hard negatives [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "[8]), they are key to the successful update of the model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] for instance).", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "(1), and enjoys useful theoretical properties for on-line optimization [13].", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "Note that this formulation is closely related to the mean-regularized multi-task learning formulation of Evgeniou and Pontil [14], with the difference that it is designed for on-line learning in streaming scenarios.", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "(4) using Averaged Stochastic Gradient Descent (ASGD), also called Polyak-Rupert averaging [16].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "This on-line first order method achieves optimal convergence rate with only one pass over the data and a constant learning rate with a logistic regression model [13].", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "Furthermore, optimizing the window classification performance is not guaranteed to result in optimal detection performance due to the additional post-processing steps applied in most detection methods [17].", "startOffset": 201, "endOffset": 205}, {"referenceID": 17, "context": "[18] \u2014 Ols1, Ols2, Osow1, Olsr2, and Ose \u2014 and also upscale the image size by a factor of 2.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "the same setting as in [18]).", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "[18], a state-of-the-art unsupervised video adaptation method also relying only on a pre-trained generic detector, and assuming the most confident detections are true positives.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] propose a \u201clazy learning\u201d method consisting in re-ranking the low scoring detections using the similarity to high scoring ones.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Like in [18], we use the state-of-the-art Deformable Part Model (DPM) [3] as our black box generic object detector.", "startOffset": 8, "endOffset": 12}, {"referenceID": 2, "context": "Like in [18], we use the state-of-the-art Deformable Part Model (DPM) [3] as our black box generic object detector.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "Our formulation is applicable to any detector based on a linear classifier, and is, therefore, usable with most state-of-the-art object detection and window representation methods [3, 17, 19].", "startOffset": 180, "endOffset": 191}, {"referenceID": 16, "context": "Our formulation is applicable to any detector based on a linear classifier, and is, therefore, usable with most state-of-the-art object detection and window representation methods [3, 17, 19].", "startOffset": 180, "endOffset": 191}, {"referenceID": 18, "context": "Our formulation is applicable to any detector based on a linear classifier, and is, therefore, usable with most state-of-the-art object detection and window representation methods [3, 17, 19].", "startOffset": 180, "endOffset": 191}, {"referenceID": 19, "context": "In our experiments, we use Fisher Vectors (FV) [20], as this representation is particularly suited to our problem for the following reasons.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "First, FV are among state-of-the-art representations for object detection [17].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "Second, they proved to be efficient for both category-level image classification [21] and instance-level retrieval problems [22].", "startOffset": 81, "endOffset": 85}, {"referenceID": 21, "context": "Second, they proved to be efficient for both category-level image classification [21] and instance-level retrieval problems [22].", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "[17], which yields competitive detection results on Pascal VOC, with the difference that we use a more efficient but approximate sliding window approach similar to the one described in Oneata et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Ols1 Ols2 Olsr2 Osow1 Ose2 VIRAT-0401 DPM [3] 30.", "startOffset": 42, "endOffset": 45}, {"referenceID": 17, "context": "0 DbD [18] 32.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "\u201cDPM\u201d is the pre-trained generic detector (DPM [3], using all detections, not just seeds).", "startOffset": 47, "endOffset": 50}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "It is the standard metric used to measure object detection performance [24].", "startOffset": 71, "endOffset": 75}, {"referenceID": 17, "context": "[18]4; (iii) our EIT algorithm without multi-task regularization, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": ", [8, 9]), as our contribution is on autonomous on-line self-adaptation, i.", "startOffset": 2, "endOffset": 8}, {"referenceID": 8, "context": ", [8, 9]), as our contribution is on autonomous on-line self-adaptation, i.", "startOffset": 2, "endOffset": 8}, {"referenceID": 7, "context": "we do not assume access to a large fixed labeled dataset related to the target videos, or that all target video frames are available at once (in contrast to [8, 9]).", "startOffset": 157, "endOffset": 163}, {"referenceID": 8, "context": "we do not assume access to a large fixed labeled dataset related to the target videos, or that all target video frames are available at once (in contrast to [8, 9]).", "startOffset": 157, "endOffset": 163}], "year": 2014, "abstractText": "Learning object detectors requires massive amounts of labeled training samples from the specific data source of interest. This is impractical when dealing with many different sources (e.g., in camera networks), or constantly changing ones such as mobile cameras (e.g., in robotics or driving assistant systems). In this paper, we address the problem of self-learning detectors in an autonomous manner, i.e. (i) detectors continuously updating themselves to efficiently adapt to streaming data sources (contrary to transductive algorithms), (ii) without any labeled data strongly related to the target data stream (contrary to self-paced learning), and (iii) without manual intervention to set and update hyper-parameters. To that end, we propose an unsupervised, on-line, and self-tuning learning algorithm to optimize a multi-task learning convex objective. Our method uses confident but laconic oracles (high-precision but low-recall off-the-shelf generic detectors), and exploits the structure of the problem to jointly learn on-line an ensemble of instance-level trackers, from which we derive an adapted category-level object detector. Our approach is validated on real-world publicly available video object datasets.", "creator": "LaTeX with hyperref package"}}}