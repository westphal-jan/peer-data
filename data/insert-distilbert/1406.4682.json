{"id": "1406.4682", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2014", "title": "Exact Decoding on Latent Variable Conditional Models is NP-Hard", "abstract": "latent variable conditional models, including the latent conditional random fields as a desirable special case, are popular models accessible for many natural language processing and vision - processing software tasks. the computational complexity of the exact decoding / inference in latent conditional random fields is unclear. in this current paper, we try to clarify the computational complexity of the exact candidate decoding. we analyze the complexity and demonstrate fully that it is an np - hard problem even on a sequential labeling setting. furthermore, we propose the latent - dynamic inference ( ldi - naive ) method and recommends its bounded version ( ldi - strongly bounded ), which are able to perform exact - inference or almost - exact - inference by using top - $ s n $ search and dynamic programming.", "histories": [["v1", "Wed, 18 Jun 2014 11:17:58 GMT  (204kb)", "http://arxiv.org/abs/1406.4682v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CC cs.LG", "authors": ["xu sun"], "accepted": false, "id": "1406.4682"}, "pdf": {"name": "1406.4682.pdf", "metadata": {"source": "CRF", "title": "Exact Decoding on Latent Variable Conditional Models is NP-Hard", "authors": ["Xu Sun"], "emails": ["xusun@pku.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 6.\n46 82\nv1 [\ncs .A\nI] 1\n8 Ju\nn 20\n14 1\nIndex Terms\u2014Latent Variable Conditional Models, Computational Complexity Analysis, Exact Decoding.\nI. INTRODUCTION\nReal-world problems may contain hidden structures that are difficult to be captured by conventional structured classification models without latent variables. For example, in the syntactic parsing task for natural language, the hidden structures can be refined grammars that are unobservable in the supervised training data [1]. In the gesture recognition task of the computational vision area, there are also hidden structures which are crucial for successful gesture recognition [2]. There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9]\nIn such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9]. As a representative structured classification model with latent variables, the latent conditional random fields (LCRFs) have become widely-used for performing a variety of tasks with hidden structures, e.g., vision recognition [4], and syntactic parsing [1], [9]. For example, [4] demonstrated that LCRF models can learn latent structures of vision recognition problems efficiently, and outperform several widely-used conventional models, such as support vector machines (SVMs), conditional random fields (CRFs) and hidden Markov models (HMMs). [1] and [9] reported on a syntactic parsing task that LCRF models can learn more accurate grammars than models that use conventional techniques without latent variables.\nExact inference in the latent conditional models is a remaining problem. In conventional models, such as conditional random fields (CRFs), the optimal labeling can be efficiently obtained by dynamic programming algorithms (for example, the Viterbi algorithm). Nevertheless, for latent conditional random fields, the inference is not straightforward, because of the inclusion of latent variables. The computational complexity\nX. Sun is with Department of Computer Science, School of EECS, Peking University. E-mail: xusun@pku.edu.cn\nFig. 1. Comparison between CRF models and LCRF models on the training stage. x represents the observation sequence, y represents labels and h represents the latent variables assigned to the labels. Note that only the white circles are observed variables. Also, only the links with the current observations are depicted, but for both models, long range dependencies are possible.\nof inference in LCRFs, with a disjoint association among latent variables and a linear chain structure, is still unclear. In this paper, we will show that such kind of inference is actually NPhard. This is a critical limitation on the real-world applications of LCRFs. Most of the previous applications of LCRFs tried to make simplified approximations on the inference [13], [4], [1], but the inference accuracy can be limited.\nAlthough we will show that the inference in LCRFs is NP-hard, in real-world applications, we have an interesting observation on LCRF models: they normally have a highly concentrated probability distribution. The major probability is distributed on top-n ranked latent-labelings. In this paper, we try to make systematic analysis on this probability concentration phenomenon. We will show that probability concentration is reasonable and expected in latent conditional random fields. Based on this analysis, we will propose a new inference algorithm: latent dynamic inference (LDI), by systematically combining efficient top-n search with dynamic programming. The LDI is an exact inference method, producing the most probable label sequence. In addition, for speeding up the inference, we will also propose a bounded version of the LDI algorithm."}, {"heading": "II. LATENT CONDITIONAL RANDOM FIELDS", "text": "Given the training data, the task is to learn a mapping between a sequence of observations x = x1, x2, . . . , xm and a sequence of labels y = y1, y2, . . . , ym. Each yj is a class label for the j\u2019th token of a word sequence, and is a member of a set Y of possible class labels. For each sequence, the model also assumes a sequence of latent variables h = h1, h2, . . . , hm, which is unobservable in training examples. See Figure 1 for the comparsion between CRFs and LCRFs.\n2 h1\nh2\nh3\nh4\nh5\nh6\nx1 x2 x3 x4 x5\ny1\ny2\n{\n{\nwhere w represents the parameter vector of the model. To make the training efficient, a restriction is made for the model: for each label, the latent variables associated with it have no intersection with the latent variables from other labels [4], [1]. This simplification is also a popular practice in other latent conditional models, including hidden-state conditional random fields (HCRF) [14]. Each h is a member in a set H(y) of possible latent variables for the class label y, and H(yj) \u2229 H(yk) = \u2205 if yj 6= yk. H is defined as the set of all possible latent variables; i.e., the union of all H(y) sets: H = \u222ay\u2208YH(y). In other words, h can have any value from H, but P (y|h) is zero except for only one of y in Y . The disjoint restriction indicates a discrete simplification of P (y |h,x,w):\nP (y |h,x,w) = 1 if h \u2208 H(y1)\u00d7 . . .\u00d7H(ym)\nP (y |h,x,w) = 0 if h /\u2208 H(y1)\u00d7 . . .\u00d7H(ym)\nwhere m is the length of the labeling1: m = |y |. The formula h \u2208 H(y1)\u00d7 . . .\u00d7H(ym) indicates that the latent-labeling h is a latent-labeling of the labeling y , which can be more formally defined as follows:\nh \u2208 H(y1)\u00d7 . . .\u00d7H(ym) \u21d0\u21d2 hj\u2208H(yj) for j = 1, . . . ,m.\nSince sequences that have any hj /\u2208 H(yj) will by definition have P (y |hj ,x,w) = 0, the model can be simplified as:\nP (y|x,w) , \u2211\nh\u2208H(y1)\u00d7...\u00d7H(ym)\nP (h|x,w). (1)\nIn Figure 2, an example is shown to illustrate the way to compute P (y |x,w) for a given labeling by summing over probabilities of all its latent-labelings. The item P (h|x,w) is defined by the usual conditional random field formulation:\nP (h|x,w) = exp\n{ w\u22a4f (h,x) }\n\u2211\nh\u2032\u2208H\u00d7...\u00d7H\nexp { w\u22a4f (h\u2032 ,x) } , (2)\nin which f (h,x) is a global feature vector. LCRF models can be seen as a natural extension of CRF models, and CRF\n1A labeling is a sequence of predicted classes: y = y1, y2, . . . , ym.\n1 2\n43\nFig. 3. An example of an undirected graph that contains one maximal clique. This example will be used for complexity analysis.\nmodels can be seen as a special case of LCRFs that employ only one latent variable for each label (i.e., |H(y)| = 1 for each y in Y). The global feature vector can be calculated by summing over all its local feature vectors:\nf (h,x) = \u2211\ni=1,\u00b7\u00b7\u00b7 ,m\nf i(h,x) + \u2211\ni=1,\u00b7\u00b7\u00b7 ,m\u22121\nf i,i+1(h,x), (3)\nin which f i(h,x) represents a local feature vector that depends only on hi and x. The f i,i+1(h,x) represents the local feature vector that depends only on hi, hi+1, and x.\nGiven a training set consisting of n labeled sequences, (xi, y i), for i = 1 . . . n, parameter estimation is performed by optimizing the objective function,\nL(w) =\nn \u2211\ni=1\nlogP (y i|xi,w)\u2212R(w).\nThe first term of this equation represents a conditional marginal log-likelihood of a training data. The second term is a regularizer that is used for reducing overfitting in parameter estimation. L2 regularization is often used, and it is defined as: R(w) = ||w|| 2\n2\u03c32 , where \u03c3 is the hyper-parameter that controls the degree of regularization."}, {"heading": "III. COMPLEXITY OF EXACT INFERENCE", "text": "In this section, we will make a formal analysis of the computational complexity of inference in LCRFs. The computational complexity analysis will be based on a reduction from a well-known NP-hard problem to the inference problem on LCRFs. We assume that the reader has basic background in the complexity theory, including the notions of NP and NP-hardness [15], [16], [17], [18]. Normally, a minimum requirement for an algorithm to be considered as efficient is that its running time is polynomial: O(nc), where c is a constant real value and n is the size of the input. It is believed that the NP-hard problems cannot be solved in polynomial time, although there has been no proof of a super-polynomial lower bound.\nThe well-known Maximum-Clique problem is an NP-hard problem. Figure 3 gives an example of the maximum clique problem. The graph consists of 4 connected nodes, and the number of maximum-clique nodes is 3, because the maximum clique is {2, 3, 4}.\nWe will prove that the exact inference in LCRFs is an NP-hard problem, and hence, finding the labeling with the maximum probability on LCRFs is likely to be intractable. Inspired by the consensus string problem on hidden Markov models [19], we establish the hardness analysis of the problem by a reduction from the maximum clique problem.\n3 Definition 1. Maximum clique problem Instance: An undirected graph G = {V ,D} with the indexed nodes V = {1, 2, . . . , |V|} and the edge set D defined among V . Question: What is the size of the maximum clique of G?\nSince the x and w are fixed for the inference task, we can simplify the denotation and define node scores \u03d5i(h) = exp{w\u22a4f i(h,x)} and edge scores \u03d5i,i+1(h) = exp{w\u22a4f i,i+1(h,x)}. Then, based on Eq. 3, we can reformulate Eq. 2 as follows:\nP (h|x,w)\n=\n\u220f\ni=1,\u00b7\u00b7\u00b7 ,m\n\u03d5i(h) \u00b7 \u220f\ni=1,\u00b7\u00b7\u00b7 ,m\u22121\n\u03d5i,i+1(h)\n\u2211\nh\u2032\u2208H\u00d7...\u00d7H\n{\n\u220f\ni=1,\u00b7\u00b7\u00b7 ,m\n\u03d5i(h \u2032) \u00b7\n\u220f\ni=1,\u00b7\u00b7\u00b7 ,m\u22121\n\u03d5i,i+1(h \u2032)\n} .\nThis reformulation indicates that the probability of a latent labeling h can be computed by the path-score of h, and divided by the summation of all path-scores. The path-score of h is defined by the multiplication over all the node scores and edges scores of h.\nDefinition 2. Inference in LCRFs Instance: A latent variable lattice (e.g., see Figure 2) M = {x,Y,H,\u03a6n,\u03a6e}: For input sequence x, |x| = m; Y and H are defined as before; \u03a6n is a m \u00d7 |H| matrix, in which an element \u03a6n(i, j) represents a real-value node score \u03d5n for the latent variable j \u2208 H on the position i (corresponding to xi); \u03a6e is a m\u00d7 |H| \u00d7 |H| three-dimensional array, in which an element \u03a6e(i, j, k) represents a real-value edge score \u03d5e for the edge (transition) between the latent variables j \u2208 H and k \u2208 H, which are on the positions i and i+ 1, respectively. Question: With the P (y |x,w) being defined in Eq. 1, what is the optimal labeling y\u2217 in M such that y\u2217 = argmaxy P (y |x,w)?\nBased on those definitions, we have the following theorem:\nTheorem 1. The computational complexity of the exact inference in LCRFs, y\u2217 = argmaxy P (y |x,w), is NP-hard.\nThis means the exact inference on LCRFs is an NPhard problem. The proof is extended from related work on complexity analysis [20], [21]. In [20], the complexity analysis is based on Bayesian networks. In [21], the analysis is based on grammar models. We extend the complexity analysis to LCRFs."}, {"heading": "A. Proof", "text": "Here, we prove the Theorem 1. For an undirected graph G = {V ,D}, we present the construction of a corresponding latent conditional model MG from G, so that the size of the maximum clique of G is proportional to the probability of the optimal labeling of MG. We set the length of the input sequence: m = |V|. We set Y = {E,N}. We set H = {E1, E2, . . . , E|V|, N1, N2, . . . , N |V|}, and the disjoint association between Y and H is as follows: H(E) = {E1, E2, . . . , E|V|}, and H(N) = {N1, N2, . . . , N |V|}. The\n2/18\n4/18\n4/18\n8/18 * 1/2\n1\n1\n1\n1/2\n1/2 1/2\n1/2\n1/2 1/2\n1/2\n1\n1\n8/18 * 1/2\nE1\nN1\nE2\nN2\nE3\nN3\nE4\nN4\nFig. 4. The latent conditional model MG constructed from the graph G in Fig 3.\n2/18\n4/18\n4/18\n8/18 * 1/2\n8/18 * 1/2\nE1\nN1\nE2\nN2\nE3\nN3\nE4\nN4\nE\nN\n{\n{\nFig. 5. The same latent conditional model MG transformed from Fig 4. This figure is to show that the latent conditional model constructed in Fig 4 is valid.\nsettings on node-scores and edge-scores on the lattice are as follows: We group paths with non-zero probability in the lattice into |V| layers, such that each layer contains two horizontal rows of the lattice, and there is a layer corresponding to a node in V . In what follows, we say a path is valid if it has a non-zero probability, and a node is valid if there is at least one valid path that passes the node. The properties of a layer are as follows:\nThere are a total of |V| layers L = {L1, L2, . . . , L|V|}. A layer Li corresponds to a node i \u2208 V . Any paths that go through nodes from different layers are not valid. The layer Li contains only Ei and N i. All paths (latent-labelings) in the layer should pass the Ei and avoid the N i on the i\u2019th position: \u03a6n(i, Ei) = 1 and \u03a6n(i, N i) = 0. For any position k other than i in the layer Li, if the node k in G is connected to node i in G, (k, i) \u2208 D, then both the Ei and N i are valid on the position k: \u03a6n(k,Ei) = 1 and \u03a6n(k,N i) = 1. Otherwise, only the N i is valid. For the edge scores \u03a6e, all the edges involving an invalid node (with node score of 0) will become an invalid edge (with edge score of 0). For any of the remaining valid edges (j, k), its edge score is 1/2 if node j starts two valid edges and 1 if it starts only one valid edge. Finally, we adjust the node scores of the beginning nodes. If\n4 both of the beginning nodes are valid in a layer, both of the nodes will have the probability of 1/2. The node scores of the beginning nodes of the layer Li are multiplied a factor \u03b4(i) that is related to the degree of the node i in the graph G: \u03b4i =\n2deg(i)\u2211 v\u2208V 2deg(v) .\nGiven any valid path h in the constructed model, it can be proved that the total number of valid paths are \u03b1, with \u03b1 = \u2211\nv\u2208V 2 deg(v). Also, all the valid paths have the same path-\nscore, 1 \u03b1 . The summation of all the path scores is exactly 1. An example for constructing a latent conditional model based on the previous simple graph (see Figure 3) is shown in Figure 4. In the figure, we only show the valid edges for simplicity. The path-score of any valid path in Figure 4 is 1/18. The model structure in Figure 4 is a valid type of structure of LCRFs (see Figure 5). The reduction is always possible in polynomial time.\nLemma 2. If a labeling y in MG has a probability of c/\u03b1 (c is a integer with c \u2265 1), then the graph G must have a maximum clique with the size of at least c.\nSince each valid latent-labeling has the identical probability of 1/\u03b1, P (y |x,w) = c/\u03b1 means that y must have c different latent-labelings. A clear property of MG is that one layer can only produce, at most, one latent-labeling for a specified y . Therefore, each of the c latent-labelings of y must come from c different layers. Suppose that the c different layers are Lx1 , Lx2, . . . , Lxc , and X = {x1, . . . , xc} is the set of indexes of the c different layers. If y contains a latent-labeling from a layer Li, then Ei must be chosen on the i\u2019th position. Therefore, y must have the label E on at least c different positions x1, . . . , xc. It indicates that each of the c latentlabelings of y must have Ei on at least c different positions x1, . . . , xc, and for each case, the node i in G is connected to all the c\u2212 1 nodes indexed by the set X \u2212 {i}. Thus, the nodes x1, . . . , xc in G are connected to each other, and they form a clique of the size c in G.\nLemma 3. If G has a clique of the size c, there must be a labeling in MG with the probability of at least c/\u03b1.\nSuppose that the c nodes of the clique in G are indexed by a set X = {x1, . . . , xc}; then, in each layer Li with i \u2208 X , there must be a valid hi that passes the Ei on all of the positions of X . The N i is always valid for all positions, except the position i. Especially, N i is valid for any position k /\u2208 X . On the other hand, Ei is valid at each position k\u2032 \u2208 X since X forms a clique in G. Hence, for Li, the hi described as follows must be valid: hi passes Ei for all positions in X and passes N i for all positions not in X (i.e., V \u2212X ). The c latent-labelings from different layers are consistent and belong to an identical labeling, y . Since each latent-labeling has the probability of 1/\u03b1, the y has the probability of at least c/\u03b1.\nCombining the two lemmas, we can see that the graph G has a maximum clique of the size c if and only if the model MG has a maximum-probability labeling with the probability c/\u03b1. Since the reduction is always possible in polynomial time, we have Theorem 1."}, {"heading": "IV. A PRACTICAL SOLUTION BASED ON PROBABILITY CONCENTRATION", "text": "We have shown that exact inference in latent conditional models is an NP-hard problem. Nevertheless, we try to solve this difficult problem based on an interesting observation. In real world applications, we have an observation on LCRFs: They normally have a highly concentrated probability distribution. That is, most of the probability mass is distributed on top-n ranked latent labelings."}, {"heading": "A. Probability Concentration from Optimization", "text": "To formally analyze the reason of the probability concentration on LCRFs, we first analyze the optimization process on LCRFs. Since the optimization process on LCRFs is based on the gradient information of its objective function, it is critical to analyze the trends of the gradient formulation of the LCRF objective function. In training LCRFs, people perform gradient ascent for maximizing the objective function. The log-likelihood portion of the objective function is as follows:\nL = log\n{\n\u2211\nh\u2208y\u2217 exp [w \u22a4f (h,x)]\n\u2211\n\u2200h\u2032 exp [w \u22a4f (h\u2032 ,x)]\n}\n= log\n{\n\u2211\nh\u2208y\u2217\nexp [w\u22a4f (h,x)]\n}\n\u2212 log\n{\n\u2211\n\u2200h\u2032\nexp [w\u22a4f (h\u2032 ,x)]\n}\n.\nHence, its gradient is as follows:\n\u2207wL = \u2211\nh\u2208y\u2217\n{ P \u2217(h)f (h,x) } \u2212 \u2211\n\u2200h\u2032\n{ P (h\u2032)f (h\u2032 , x) }\n= \u2211\nh\u2208y\u2217\n{ [P \u2217(h)\u2212 P (h)]f (h,x) } \u2212 \u2211\nh\u2032 /\u2208y\u2217\n{ P (h\u2032)f (h\u2032 , x) } , (4)\nwhere P \u2217(h) is the probability of h with regard to only y\u2217 . In other words,\nP \u2217(h) = exp[w\u22a4f (h,x)] \u2211\nh\u2208y\u2217 exp[w \u22a4f (h,x)]\n.\nFrom Equation 4, we can see that a latent labeling h \u2208 y\u2217 with higher probability can \u201cdominate\u201d the gradient with higher degree. Since the LCRF model is trained with gradient ascent, a latent labeling h \u2208 y\u2217 with higher probability will in turn be updated with more gains in the next gradient ascent step (because it \u201cdominated\u201d the current gradient with more degrees). Hence, we expect latent labelings will have a \u201crich get richer\u201d trend during the training of a LCRF model. This \u201crich get richer\u201d trend is also meaningful in real-world data and tasks, because in this way the latent structure can be discovered with higher confidence.\nOn the other hand, note that the probability is expected to be concentrated highly, but not completely. This is because real-world tasks are usually ambiguous. Another reason is from regularization terms of the objective function, which controls overfitting, including the potential overfitting of latent structures."}, {"heading": "B. Latent-Dynamic Inference", "text": "Based on the highly (but not completely) concentrated probabilities in LCRFs, we propose a novel inference method, which is efficient and exact in most of the real-world applications.\n5 1: Definitions: 2: \u201cn\u201d represents the current search step (# of latent-labelings\nbeing searched). 3: \u201cProbGap\u201d is a real value recording the difference between\nP (y \u2032) and Premain. 4: \u201cS\u201d indicates a set of \u201calready searched labelings\u201d. 5: \u201cFindLatentLabeling(n)\u201d uses A\u2217 search to find the n\u2019th\nranked latent-labeling. 6: \u201cFindParentLabeling(h)\u201d finds the corresponding label-\ning from the latent-labeling: FindParentLabeling(h) = y \u21d0\u21d2 hj \u2208 H(yj) for j = 1 . . .m.\n7: P (h) , P (h|x,w). 8: P (y) , P (y |x,w). 9:\n10: Initialization: 11: n = 0; ProbGap = \u22121; P (y \u2032) = 0; S0 = \u2205. 12: 13: Procedure LDI(): 14: while ProbGap < 0 do 15: n = n+ 1; 16: hn = FindLatentLabeling(n); 17: yn = FindParentLabeling(hn); 18: if yn /\u2208 Sn\u22121 then 19: Sn = Sn\u22121 \u222a {yn}; 20: P (yn) = ComputeProbability(yn); 21: if P (yn) > P (y \u2032) then 22: y \u2032 = yn; 23: Premain = 1\u2212 \u2211\nyk\u2208Sn P (yk);\n24: ProbGap = P (y \u2032)\u2212 Premain; 25: else 26: Sn = Sn\u22121; 27: return y \u2032; 28:\nFig. 6. The algorithm of the LDI inference for LCRFs."}, {"heading": "C. Latent-Dynamic Inference (LDI-Naive)", "text": "In the inference stage, given a test sequence x, we want to find the most probable label sequence, y\u2217:\ny\u2217 = argmax y P (y|x,w). (5)\nFor latent conditional models like LCRFs, the y\u2217 cannot directly be produced by the Viterbi algorithm, because of the incorporation of latent variables.\nIn this section, we describe an exact inference algorithm, the latent-dynamic inference (LDI), for producing the optimal label sequence y\u2217 on LCRFs (see Figure 6). In short, the algorithm generates the best latent-labelings in the order of their probabilities. Then, the algorithm maps each of these latent-labelings to its associated labelings and uses a dynamic programming method (the forward-backward algorithm) to compute the probabilities of the corresponding labelings. The algorithm continues to generate the next best latent-labeling and the associated labeling until there is not enough probability mass left to beat the best labeling.\nIn detail, an A\u2217 search algorithm [22], [23] with a Viterbi heuristic function is adopted to produce top-n latent-labelings,\nh1,h2, . . .hn. In addition, a forward-backward-style algorithm is used to compute the probabilities of their corresponding labelings, y1, y2, . . . yn. The algorithm then tries to determine the optimal labeling based on the top-n statistics, without enumerating the remaining low-probability labelings, in which the number is exponentially large.\nThe optimal labeling y\u2217 will be y \u2032 when the following \u201cexact-condition\u201d is achieved:\nP (y \u2032|x,w)\u2212 ( 1\u2212 \u2211\nyk\u2208Sn\nP (yk|x,w) ) \u2265 0, (6)\nwhere y \u2032 is the most probable label sequence in the current stage. It is straightforward to prove that y\u2217 = y \u2032, and further search is unnecessary. This is because the remaining probability mass, 1 \u2212 \u2211\nyk\u2208Sn P (yk|x,w), cannot beat the\ncurrent optimal labeling in this case.\nTheorem 4. In the procedure defined in Figure 6, the labeling y \u2032 (satisfying the exact condition Eq. 6) is guaranteed to be the exact optimal labeling:\ny \u2032 = argmax y P (y |x,w).\nThe proof of Theorem 4 is simple. Given the exact condition, suppose there is a label sequence y \u2032\u2032 with a larger probability, P (y \u2032\u2032|x,w) > P (y \u2032|x,w), then it follows that y \u2032\u2032 /\u2208 Sn. Since P (y \u2032\u2032|x,w) > P (y \u2032|x,w) and y \u2032\u2032 /\u2208 Sn, it follows that P (y \u2032\u2032|x,w) + \u2211\nyk\u2208Sn P (yk|x,w) >\nP (y \u2032|x,w) + \u2211\nyk\u2208Sn P (yk|x,w). According to the exact\ncondition, it follows that P (y \u2032|x,w) + \u2211\nyk\u2208Sn P (yk|x,w) \u2265\n(\n1 \u2212 \u2211\nyk\u2208Sn P (yk|x,w)\n)\n+ \u2211\nyk\u2208Sn P (yk|x,w). The right\nside of the inequality is 1. Therefore, we have P (y \u2032\u2032|x,w) + \u2211\nyk\u2208Sn P (yk|x,w) > 1, which is not possible. Hence, the\nassumed y \u2032\u2032 is impossible."}, {"heading": "D. Admissible and Tight Heuristics for Efficient Search", "text": "We have presented the framework of the LDI inference. Here, we describe the details on implementing its crucial component: designing the heuristic function for the A\u2217 heuristic search. Our heuristic search aims at finding the top-n most probable latent-labelings. Recall that the probability of a latent-labelings is defined as\nP (h|x,w) = exp\n{ w\u22a4f (h,x) }\n\u2211\nh\u2032\u2208H\u00d7...\u00d7H\nexp { w\u22a4f (h\u2032 ,x) } .\nTo find out top-n most probable latent-labelings, an easier way for achieving the same target is to find out top-n \u201chighestscore\u201d latent-labelings with the score defined as\n\u03d5(h|x,w) = w\u22a4f (h,x).\nIn A\u2217 search, the cost function is normally defined as:\nf(i) = g(i) + heu(i),\nwhere i is the current node. f(i) is the cost function. g(i) is the cost from the start node to the current node. heu(i) is the estimated cost from current node to the target node. If the heuristic function is non-admissible, the A\u2217 algorithm may\n6 1: Procedure LDI-Bounded(n\u2032): 2: while ProbGap < 0 do 3: n = n+ 1; 4: if n > n\u2032 then 5: return y \u2032; 6: hn = FindLatentLabeling(n); 7: ... 8: return y \u2032; 9:\nFig. 7. The algorithm of the LDI-Bounded inference for LCRFs. Since a majority of the steps are similar to the Figure 6, we do not repeat the description. The new input variable n\u2032 (n\u2032 \u2265 1) represents the threshold value for bounding the search steps.\noverlook the optimal solution [23]. In our case, a heuristic is admissible if it never underestimates the scores from the current position to the target. Note that, admissible heuristics do not guarantee the efficiency of the search. In our LDI case, we not only want admissible heuristics, but also try to make the search efficient enough. For this concern, we need a monotone (or, consistent) heuristic. A monotone heuristic is an admissible heuristic with additional properties. Informally, we can think a monotone heuristic is an admissible and tight heuristic. A formal definition of monotone heuristics is as follows for the highest-score path search problem:\nheu(j) \u2265 c(j, k) + heu(k), and\nheu(G) = 0,\nwhere j is every possible current node, and k is every possible successor of j generated by any possible action a(j, k) with the cost c(j, k). G is the goal node. Here we present a monotone heuristic function for the LDI task. The LDI algorithm first scans in a backward direction (right to left) to compute the monotone heuristic function for each latent variables in the lattice. After that, the A\u2217 search was performed in a forward direction (left to right) based on the computed heuristics. For a latent variable hj on the position i, its monotone heuristic function is designed as follows: heu(i, j) = maxh\u2032\u2208L(i,j) \u03d5(h \u2032\n|x,w), where L(i, j) represents a set of all possible partial latent-labelings starting from the latent variable hj \u2208 H on position i and ending at the goal position m. In implementation, the heuristics are computed efficiently by using a Viterbi-style algorithm:\n(1) Initialization :\nfor j = 1, . . . , |H|, heu(m, j) = 0.\n(2) Recursion (for i = m\u2212 1, . . . , 1) :\nfor j = 1, . . . , |H|,\nheu(i, j) = max k=1,...,|H| [heu(i+ 1, k) + \u03a6n(i+ 1, k) + \u03a6e(i, j, k)].\nThe \u03a6n(i+ 1, k) is the score of the latent variable hk on the position i+1; the \u03a6e(i, j, k) is the score of the edge between the latent variable hj on the position i and the following latent variable hk on the next position."}, {"heading": "E. A Bounded Version (LDI-Bounded)", "text": "By simply setting a threshold value on the search step, n, we can derive a bounded version of the LDI; i.e., LDIBounded (see Figure 7). This method is a straightforward way for approximating the LDI. We have also tried other methods for approximation. Intuitively, one alternative method is to design an approximated \u201cexact condition\u201d, by using a factor, \u03b1, to estimate the distribution of the remaining probability:\nP (y \u2032|x,w)\u2212 \u03b1 ( 1\u2212 \u2211\nyk\u2208Sn\nP (yk|x,w) ) \u2265 0.\nFor example, if at most 50% of the unknown probability, 1 \u2212 \u2211\nyk\u2208Sn P (yk|x,w), can be distributed on a single la-\nbeling, we can set \u03b1 = 0.5 to make a loose condition to stop the inference. At first glance, this seems to be quite intuitive. However, when we compared this alternative method with the LDI-Bounded method, we found that the performance and speed of the former method was worse than for the latter."}, {"heading": "F. Existing Inference Methods on Latent Conditional Models", "text": "In [13], the optimal labeling is approximated by using a modified Viterbi inference (MVI) method. In the MVI inference, there are two steps. First, the MVI searches for the optimal latent-labeling using the Viterbi algorithm:\nh\u2217 = argmax h P (h|x,w).\nThen, a labeling y is derived by directly locating the corresponding labeling of the latent-labeling h\u2217:\ny = FindParentLabeling(h\u2217),\nwhich means that hj \u2208 H(yj) for j = 1 . . .m. The MVI inference can be seen as a simple adaptation of the traditional Viterbi inference in the case of latent conditional models.\nIn [4], y\u2217 is estimated by a point-wise marginal inference (PMI) method. To estimate the label yj of token xj , the marginal probabilities P (hj |x,w) are computed for all possible latent variables hj \u2208 H. Then the marginal probabilities are summed up (according to the association between latent variables and labels) for computing P (yj |x,w) for all possible labels yj \u2208 Y . In this way, the optimal labeling is approximated by choosing the labels with the maximum marginal probabilities at each position j independently:\ny = argmax yj\u2208Y P (yj |x,w) for j = 1, . . . ,m,\nwhere\nP (yj |x,w) =\n\u2211\nh\u2208H(yj) P (h|x,w)\n\u2211 h\u2208H P (h|x,w) .\nThe LDI-Naive and the LDI-Bounded perform exact inference or almost-exact inference, while the MVI and the PMI perform a rough estimation on y\u2217. We will compare the different methods via experiments in Section ??.\n7"}, {"heading": "G. Comparison with MAP Algorithms", "text": "The MAP problem refers to finding the Maximum a Posteriori hypothesis, which aims at finding the most likely configuration of a set of variables in a Bayesian network, given some partial evidence about the complement of that set. Several researchers have proposed algorithms for solving the MAP problem [24], [20], [25], [26].\nIn [20], an efficient approximate local search algorithm is proposed for approximating MAP: hill climbing and taboo search. Compared to the approximate local search algorithm, the LDI algorithm can perform exact inference under a reasonable number of search steps (with a tractable cost). In the case that exactitude is required, this characteristic of the LDI algorithm is important. In [26], a dynamic weighting A\u2217 (DWA\u2217) search algorithm is proposed for solving MAP in Bayesian networks. Like the local search algorithms, the DWA\u2217 search is an approximate method and it does not guarantee an exact solution. In [24], an effective method is proposed to compute a relatively tight upper-bound on the probability of a MAP solution. The upper bound is then used to develop a branch-and-bound search algorithm for solving MAP exactly. Whether or not the branch-and-bound search can be used for solving LCRFs is unclear, because of the structural difference. In addition, the quality of tightness of the computed bound is crucial for the tractability of the branch-and-bound search. The quality of tightness is unclear concerning LCRFs."}, {"heading": "V. CONCLUSIONS", "text": "We made a formal analysis of the inference in latent conditional models, and showed that it is an NP-hard problem, even when latent conditional models have a disjoint assumption and linear-chain structures. More importantly, based on an observation of probability concentration, we proposed the latent-dynamic inference method (LDI-Naive) and its bounded version (LDI-Bounded), which are able to perform exact and fast inference in latent conditional models, even though the original problem is NP-hard."}], "references": [{"title": "Discriminative log-linear grammars with latent variables", "author": ["S. Petrov", "D. Klein"], "venue": "Proceedings of NIPS\u201908. MIT Press, 2008, pp. 1153\u2013 1160.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Hidden conditional random fields for gesture recognition.", "author": ["S.B. Wang", "A. Quattoni", "L. Morency", "D. Demirdjian", "T. Darrell"], "venue": "Proceedings of CVPR\u201906,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Conditional random fields for object recognition.", "author": ["A. Quattoni", "M. Collins", "T. Darrell"], "venue": "Proceedings of NIPS\u201904,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Latent-dynamic discriminative models for continuous gesture recognition", "author": ["L. Morency", "A. Quattoni", "T. Darrell"], "venue": "Proceedings of CVPR\u201907, 2007, pp. 1\u20138.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Modeling latentdynamic in shallow parsing: A latent conditional model with improved inference", "author": ["X. Sun", "L.-P. Morency", "D. Okanohara", "J. Tsujii"], "venue": "Proceedings of COLING\u201908, Manchester, UK, 2008, pp. 841\u2013848.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust approach to abbreviating terms: A discriminative latent variable model with global information", "author": ["X. Sun", "N. Okazaki", "J. Tsujii"], "venue": "Proceedings of the ACL\u201909, Suntec, Singapore, August 2009, pp. 905\u2013913.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Hidden conditional random field with distribution constraints for phone classification", "author": ["D. Yu", "L. Deng", "A. Acero"], "venue": "Proceedings of InterSpeech\u201909, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Max-margin hidden conditional random fields for human action recognition", "author": ["Y. Wang", "G. Mori"], "venue": "Proceedings of CVPR\u201909, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Products of random latent variable grammars", "author": ["S. Petrov"], "venue": "Proceedings of NAACL\u201910, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "A discriminative latent variable chinese segmenter with hybrid word/character information", "author": ["X. Sun", "Y. Zhang", "T. Matsuzaki", "Y. Tsuruoka", "J. Tsujii"], "venue": "Proceedings of NAACL-HLT\u201909, 2009, pp. 56\u201364.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Latent variable perceptron algorithm for structured classification", "author": ["X. Sun", "T. Matsuzaki", "D. Okanohara", "J. Tsujii"], "venue": "Proceedings of IJCAI\u201909, 2009, pp. 1236\u20131242.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Sequential labeling with latent variables: An exact inference algorithm and its efficient approximation", "author": ["X. Sun", "J. Tsujii"], "venue": "Proceedings of EACL\u201909, Athens, Greece, March 2009, pp. 772\u2013780.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic CFG with latent annotations", "author": ["T. Matsuzaki", "Y. Miyao", "J. Tsujii"], "venue": "Proceedings of ACL\u201905, June 2005, pp. 75\u201382.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Hidden conditional random fields", "author": ["A. Quattoni", "S. Wang", "L. Morency", "M. Collins", "T. Darrell"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 10, pp. 1848\u20131852, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1848}, {"title": "Introduction to Automata Theory, Languages, and Computation", "author": ["J.E. Hopcroft", "J.D. Ullman"], "venue": "Addison Wesley,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1979}, {"title": "The consensus string problem and the complexity of comparing hidden markov models.", "author": ["R.B. Lyngs\u00f8", "C.N.S. Pedersen"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Complexity results and approximation strategies for MAP explanations.", "author": ["J. Park", "A. Darwiche"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Computational complexity of probabilistic disambiguation", "author": ["K. Sima\u2019an"], "venue": "Grammars, vol. 5, no. 2, pp. 125\u2013151, 2002.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "A formal basis for the heuristic determination of minimum cost path", "author": ["P. Hart", "N. Nilsson", "B. Raphael"], "venue": "IEEE Trans. On System Science and Cybernetics, vol. SSC-4(2), pp. 100\u2013107, 1968.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1968}, {"title": "Solving MAP exactly using systematic search.", "author": ["J.D. Park", "A. Darwiche"], "venue": "Proceedings of UAI\u201903. Morgan Kaufmann,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Finding the m most probable configurations using loopy belief propagation", "author": ["C. Yanover", "Y. Weiss"], "venue": "Proceedings of NIPS\u201903. MIT Press, 2003.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Dynamic weighting a* searchbased MAP algorithm for bayesian networks.", "author": ["X. Sun", "M.J. Druzdzel", "C. Yuan"], "venue": "Proceedings of IJ-", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "For example, in the syntactic parsing task for natural language, the hidden structures can be refined grammars that are unobservable in the supervised training data [1].", "startOffset": 165, "endOffset": 168}, {"referenceID": 1, "context": "In the gesture recognition task of the computational vision area, there are also hidden structures which are crucial for successful gesture recognition [2].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 108, "endOffset": 111}, {"referenceID": 7, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 113, "endOffset": 116}, {"referenceID": 8, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 203, "endOffset": 206}, {"referenceID": 3, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 208, "endOffset": 211}, {"referenceID": 0, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 213, "endOffset": 216}, {"referenceID": 4, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 218, "endOffset": 221}, {"referenceID": 5, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 223, "endOffset": 226}, {"referenceID": 9, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 228, "endOffset": 232}, {"referenceID": 10, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 234, "endOffset": 238}, {"referenceID": 11, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 240, "endOffset": 244}, {"referenceID": 8, "context": "There are also plenty of hidden structure examples in other tasks among different areas [3], [4], [5], [6], [7], [8], [9] In such cases, models that exploit latent variables are advantageous in learning [3], [4], [1], [5], [6], [10], [11], [12], [9].", "startOffset": 246, "endOffset": 249}, {"referenceID": 3, "context": ", vision recognition [4], and syntactic parsing [1], [9].", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": ", vision recognition [4], and syntactic parsing [1], [9].", "startOffset": 48, "endOffset": 51}, {"referenceID": 8, "context": ", vision recognition [4], and syntactic parsing [1], [9].", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "For example, [4] demonstrated that LCRF models can learn latent structures of vision recognition problems efficiently, and outperform several widely-used conventional models, such as support vector machines (SVMs), conditional random fields (CRFs) and hidden Markov models (HMMs).", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "[1] and [9] reported on a syntactic parsing task that LCRF models can learn more accurate grammars than models that use conventional techniques without latent variables.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[1] and [9] reported on a syntactic parsing task that LCRF models can learn more accurate grammars than models that use conventional techniques without latent variables.", "startOffset": 8, "endOffset": 11}, {"referenceID": 12, "context": "Most of the previous applications of LCRFs tried to make simplified approximations on the inference [13], [4], [1], but the inference accuracy can be limited.", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "Most of the previous applications of LCRFs tried to make simplified approximations on the inference [13], [4], [1], but the inference accuracy can be limited.", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "Most of the previous applications of LCRFs tried to make simplified approximations on the inference [13], [4], [1], but the inference accuracy can be limited.", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "The LCRF model is defined as follows [4]:", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "To make the training efficient, a restriction is made for the model: for each label, the latent variables associated with it have no intersection with the latent variables from other labels [4], [1].", "startOffset": 190, "endOffset": 193}, {"referenceID": 0, "context": "To make the training efficient, a restriction is made for the model: for each label, the latent variables associated with it have no intersection with the latent variables from other labels [4], [1].", "startOffset": 195, "endOffset": 198}, {"referenceID": 13, "context": "This simplification is also a popular practice in other latent conditional models, including hidden-state conditional random fields (HCRF) [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 14, "context": "We assume that the reader has basic background in the complexity theory, including the notions of NP and NP-hardness [15], [16], [17], [18].", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "Inspired by the consensus string problem on hidden Markov models [19], we establish the hardness analysis of the problem by a reduction from the maximum clique problem.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "The proof is extended from related work on complexity analysis [20], [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 17, "context": "The proof is extended from related work on complexity analysis [20], [21].", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "In [20], the complexity analysis is based on Bayesian networks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [21], the analysis is based on grammar models.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In detail, an A search algorithm [22], [23] with a Viterbi heuristic function is adopted to produce top-n latent-labelings, h1,h2, .", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "Existing Inference Methods on Latent Conditional Models In [13], the optimal labeling is approximated by using a modified Viterbi inference (MVI) method.", "startOffset": 59, "endOffset": 63}, {"referenceID": 3, "context": "In [4], y is estimated by a point-wise marginal inference (PMI) method.", "startOffset": 3, "endOffset": 6}, {"referenceID": 19, "context": "Several researchers have proposed algorithms for solving the MAP problem [24], [20], [25], [26].", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "Several researchers have proposed algorithms for solving the MAP problem [24], [20], [25], [26].", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "Several researchers have proposed algorithms for solving the MAP problem [24], [20], [25], [26].", "startOffset": 85, "endOffset": 89}, {"referenceID": 21, "context": "Several researchers have proposed algorithms for solving the MAP problem [24], [20], [25], [26].", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "In [20], an efficient approximate local search algorithm is proposed for approximating MAP: hill climbing and taboo search.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In [26], a dynamic weighting A (DWA) search algorithm is proposed for solving MAP in Bayesian networks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "In [24], an effective method is proposed to compute a relatively tight upper-bound on the probability of a MAP solution.", "startOffset": 3, "endOffset": 7}], "year": 2014, "abstractText": "Latent variable conditional models, including the latent conditional random fields as a special case, are popular models for many natural language processing and vision processing tasks. The computational complexity of the exact decoding/inference in latent conditional random fields is unclear. In this paper, we try to clarify the computational complexity of the exact decoding. We analyze the complexity and demonstrate that it is an NP-hard problem even on a sequential labeling setting. Furthermore, we propose the latent-dynamic inference (LDI-Naive) method and its bounded version (LDI-Bounded), which are able to perform exact-inference or almost-exactinference by using top-n search and dynamic programming.", "creator": "LaTeX with hyperref package"}}}