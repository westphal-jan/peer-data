{"id": "1406.5291", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2014", "title": "Generalized Dantzig Selector: Application to the k-support norm", "abstract": "we propose a generalized dantzig selector ( gds ) for linear models, areas in which any norm encoding the parameter structure can be leveraged for estimation. we investigate both computational and statistical aspects of the gds. based on conjugate proximal operator, a larger flexible inexact admm framework is designed for solving gds, and non - asymptotic high - probability bounds are established on achieving the estimation error, sets which rely on gaussian width of unit norm ball and suitable set - encompassing estimation error. further, we consider a non - trivial example of the gds using $ k $ - support norm. also we derive an efficient method to compute the proximal operator for $ k $ - estimated support norm since existing methods are inapplicable in this setting. for statistical analysis, we provide upper bounds for discovering the gaussian widths needed in the gds analysis, yielding the first statistical recovery guarantee for estimation with the $ k $ - support norm. the underlying experimental learning results confirm our theoretical analysis.", "histories": [["v1", "Fri, 20 Jun 2014 07:11:44 GMT  (61kb,D)", "https://arxiv.org/abs/1406.5291v1", null], ["v2", "Tue, 8 Jul 2014 14:53:00 GMT  (61kb,D)", "http://arxiv.org/abs/1406.5291v2", "Added acknowledgements section"], ["v3", "Mon, 2 Feb 2015 17:54:14 GMT  (78kb,D)", "http://arxiv.org/abs/1406.5291v3", "Updates to bound"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["soumyadeep chatterjee", "sheng chen", "arindam banerjee"], "accepted": true, "id": "1406.5291"}, "pdf": {"name": "1406.5291.pdf", "metadata": {"source": "CRF", "title": "Generalized Dantzig Selector: Application to the k-support norm", "authors": ["Soumyadeep Chatterjee", "Sheng Chen"], "emails": ["chatter@cs.umn.edu", "shengc@cs.umn.edu", "banerjee@cs.umn.edu"], "sections": [{"heading": "1 Introduction", "text": "The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation. While DS does not consider a regularized maximum likelihood approach, [2] has established clear similarities between the estimates from DS and Lasso. While norm regularized regression approaches have been generalized to more general norms, such as decomposable norms [11], the literature on DS has primarily focused on the sparse L1 norm case, with a few notable exceptions which have considered extensions to sparse group-structured norms [8].\nIn this paper, we consider linear models of the form y = X\u03b8\u2217+w, where y \u2208 Rn is a set of observations, X \u2208 Rn\u00d7p is a design matrix, and w \u2208 Rn is i.i.d. noise. For any given norm R(\u00b7), the parameter \u03b8\u2217 is assumed to structured in terms of having a low value of R(\u03b8\u2217). For this setting, we propose the following Generalized Dantzig Selector (GDS) for parameter estimation:\n\u03b8\u0302 = argmin \u03b8\u2208Rp\nR(\u03b8)\ns.t.R\u2217 ( XT (y \u2212X\u03b8) ) \u2264 \u03bbp ,\n(1)\nwhere R\u2217(\u00b7) is the dual norm of R(\u00b7), and \u03bbp is a suitable constant. If R(\u00b7) is the L1 norm, (1) reduces to standard DS [3]. A key novel aspect of GDS is that the constraint is in terms of the dual norm R\u2217(\u00b7) of the\nar X\niv :1\n40 6.\n52 91\nv3 [\nst at\n.M L\n] 2\noriginal structure inducing norm R(\u00b7). It is instructive to contrast GDS with the recently proposed atomic norm based estimation framework [4] which, unlike GDS, considers constraints based on the L2 norm of the error \u2016y \u2212X\u03b8\u20162, and focuses only on atomic norms.\nIn this paper, we consider both computational and statistical aspects of the GDS. For the L1-norm Dantzig selector, [3] proposed a primal-dual interior point method since the optimization is a linear program. DASSO and its generalization proposed in [6, 5] focused on homotopy methods, which provide a piecewise linear solution path through a sequential simplex-like algorithm. However, none of the algorithms above can be immediately extended to our general formulation. In recent work, the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1 Dantzig selection problem [9, 18], and the linearized version in [18] proved to be efficient. Motivated by such results for DS, we propose a general inexact ADMM [17] framework for GDS where the primal update steps, interestingly, turn out respectively to be proximal updates involving R(\u03b8) and its convex conjugate, the indicator of R\u2217(x) \u2264 \u03bb. As a result, by Moreau decomposition, it suffices to develop efficient proximal update for eitherR(\u03b8) or its conjugate. On the statistical side, we establish non-asymptotic high-probability bounds on the estimation error \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162. Interestingly, the bound depends on the Gaussian width of the unit norm ball ofR(\u00b7) as well as the Gaussian width of suitable set where the estimation error belongs [4, 13].\nAs a non-trivial example of the GDS framework, we consider estimation using the recently proposed k-support norm [1, 10]. We show that proximal operators for k-support norm can be efficiently computed in O(p log p+log k log(p\u2212k)), and hence the estimation can be done efficiently. Note that existing work [1, 10] on k-support norm has focused on the proximal operator for the square of the k-support norm, which is not directly applicable in our setting. On the statistical side, we provide upper bounds for the Gaussian widths of the unit norm ball and the error set as needed in the GDS framework, yielding the first statistical recovery guarantee for estimation with the k-support norm.\nThe rest of the paper is organized as follows: We establish general optimization and statistical recovery results for GDS for any norm in Section 2. In Section 3, we present efficient algorithms and estimation error bounds for the k-support norm. We present experimental results in Section 4 and conclude in Section 5. All technical analyses and proofs are in the supplement."}, {"heading": "2 General Optimization and Statistical Recovery Guarantees", "text": "The problem in (1) is a convex program, and a suitable choice of \u03bbp ensures that the feasible set is not empty. We start the section with an inexact ADMM framework for solving problems of the form (1), and then present bounds on the estimation error establishing statistical consistency of GDS."}, {"heading": "2.1 General Optimization Framework using Inexact ADMM", "text": "In optimization, we temporarily drop the subscript p of \u03bbp for convenience. We let A = XTX, u = XTy, and define the set C\u03bb = {v : R\u2217(v) \u2264 \u03bb}. The optimization problem is equivalent to\nmin \u03b8,v\u2208Rp\nR(\u03b8) s.t. u\u2212A\u03b8 = v, v \u2208 C\u03bb . (2)\nDue to the nonsmoothness of bothR andR\u2217, solving (2) can be quite challenging and a generally applicable algorithm is Alternating Direction Method of Multipliers (ADMM). The augmented Lagrangian function for (2) is given as\nLR(\u03b8,v, z) = R(\u03b8) + \u3008z,A\u03b8 + v \u2212 u\u3009+ \u03c1\n2 ||A\u03b8 + v \u2212 u||22 . (3)\nin which z is the Lagrange multiplier and \u03c1 controls the penalty introduced by the quadratic term. The iterative updates of the variables (\u03b8,v, z) in standard ADMM are given by\n\u03b8k+1 \u2190 argmin \u03b8 LR(\u03b8,vk, zk) , (4) vk+1 \u2190 argmin v\u2208C\u03bb LR(\u03b8k+1,v, zk) , (5) zk+1 \u2190 zk + \u03c1(A\u03b8k+1 + vk+1 \u2212 u) . (6)\nNote that update (4) amounts to a regularized least squares problem of \u03b8, which can be computationally expensive. Thus we use an inexact update for \u03b8 instead, which can alleviate the computational cost and lead to a quite simple algorithm. Inspired by [18], we consider a simpler subproblem for the \u03b8-update which minimizes\nL\u0303kR(\u03b8,vk, zk) = R(\u03b8) + \u3008zk,A\u03b8 + vk \u2212 u\u3009+ \u03c1\n2\n(\u2225\u2225A\u03b8k + vk \u2212 u\u2225\u22252 2 +\n2 \u2329 \u03b8 \u2212 \u03b8k,AT (A\u03b8k + vk \u2212 u) \u232a + \u00b5\n2\n\u2225\u2225\u03b8 \u2212 \u03b8k\u2225\u22252 2 ) ,\n(7)\nwhere \u00b5 is a user-defined parameter. L\u0303kR(\u03b8,vk, zk) can be viewed as an approximation of LR(\u03b8,vk, zk) with the quadratic term linearized at \u03b8k. Then the update (4) is replaced by\n\u03b8k+1 \u2190 argmin \u03b8 L\u0303kR(\u03b8,vk, zk)\n= argmin \u03b8 { 2R(\u03b8) \u03c1\u00b5 + 1 2 \u2225\u2225\u2225\u03b8 \u2212 (\u03b8k \u2212 2 \u00b5 AT (A\u03b8k + vk \u2212 u + z k \u03c1 ) )\u2225\u2225\u22252 2 } .\n(8)\nSimilarly the update of v in (5) can be recast as\nvk+1 \u2190 argmin v\u2208C\u03bb LR(\u03b8k+1,v, zk) = argmin v\u2208C\u03bb\n1\n2 \u2225\u2225v \u2212 (u\u2212A\u03b8k+1 \u2212 zk \u03c1 ) \u2225\u22252 2 . (9)\nIn fact, the updates of both \u03b8 and v turn out to compute certain proximal operators. In general, the proximal operator proxh(\u00b7) of a closed proper convex function h : Rp \u2192 R \u222a {+\u221e} is defined as\nproxh(x) = argmin w\u2208Rp {1 2 \u2016w \u2212 x\u201622 + h(w) } .\nHence it is easy to see that (8) and (9) correspond to prox 2R \u03c1\u00b5 (\u00b7) and proxIC\u03bb (\u00b7), respectively, where IC\u03bb(\u00b7) is the indicator function of set C\u03bb given by\nIC\u03bb(x) = { 0 if x \u2208 C\u03bb +\u221e if otherwise .\nIn Algorithm 1, we provide our general ADMM for the GDS. For the ADMM to work, we need two subroutines that can efficiently compute the proximal operators for the functions in Line 3 and 4 respectively. The simplicity of the proposed approach stems from the fact that we in fact need only one subroutine, for any one of the functions, since the functions are conjugates of each other.\nAlgorithm 1 ADMM for Generalized Dantzig Selector\nInput: A = XTX, u = XTy, \u03c1, \u00b5 Output: Optimal \u03b8\u0302 of (1)\n1: Initialize (\u03b8,v, z) 2: while not converged do 3: \u03b8k+1 \u2190 prox 2R\n\u03c1\u00b5\n( \u03b8k \u2212 2\u00b5AT (A\u03b8k + vk \u2212 u + z k \u03c1 ) )\n4: vk+1 \u2190 proxIC\u03bb ( u\u2212A\u03b8k+1 \u2212 zk\u03c1 ) 5: zk+1 \u2190 zk + \u03c1(A\u03b8k+1 + vk+1 \u2212 u) 6: end while\nProposition 1 Given \u03b2 > 0 and a norm R(\u00b7), the two functions, f(x) = \u03b2R(x) and g(x) = IC\u03b2 (x) are convex conjugate to each other, thus giving the following identity,\nx = proxf (x) + proxg(x) . (10)\nProof: The Proposition 1 simply follows the definition of convex conjugate and dual norm, and (10) is just Moreau decomposition provided in [12].\nThe decomposition enables conversion of the two types of proximal operator to each other at negligible cost (i.e., vector subtraction). Thus we have the flexibility in Algorithm 1 to focus on the proximal operator that is efficiently computable, and the other can be simply obtained through (10). Remark on convergence: Note that Algorithm 1 is a special case of inexact Bregman ADMM proposed in [17], which matches the case of linearizing quadratic penalty term by using B\u03d5\u2032\u03b8(\u03b8,\u03b8k) = 1 2\u2016\u03b8 \u2212 \u03b8k\u201622 as Bregman divergence. In order to converge, the algorithm requires \u00b52 to be larger than the spectral radius of ATA, and the convergence rate is O(1/T ) according to Theorem 2 in [17]."}, {"heading": "2.2 Statistical Recovery for Generalized Dantzig Selector", "text": "Our goal is to provide error bounds on \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 between the population parameter \u03b8\u2217 and the minimizer \u03b8\u0302 of (1). Let the error vector be defined as \u2206\u0302 = \u03b8\u0302 \u2212 \u03b8\u2217. For any set \u2126 \u2286 Rp, we would measure the size of this set using its Gaussian width [14, 4], which is defined as \u03c9(\u2126) = Eg [supz\u2208\u2126\u3008g, z\u3009] , where g is a vector of i.i.d. standard Gaussian entries. We also consider the error cone TR(\u03b8\u2217), generated by the set of possible error vectors \u2206 and containing the error vector \u2206\u0302, defined as\nTR(\u03b8\u2217) := cone {\u2206 \u2208 Rp : R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} . (11)\nNote that this set contains a restricted set of directions and does not in general span the entire space of Rp. Further, let \u2126R := {u : R(u) \u2264 1}. With these definitions, we obtain our main result.\nTheorem 1 Suppose the design matrix X consists of i.i.d. Gaussian entries with zero mean variance 1, and we solve the optimization problem (1) with\n\u03bbp \u2265 cE [ R\u2217(XTw) ] . (12)\nThen, with probability at least (1\u2212 \u03b71 exp(\u2212\u03b72n)), we have\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 4c\u03a8R\u03c9(\u2126R)\n\u03baL \u221a n\n, (13)\nwhere \u03c9(TR(\u03b8\u2217) \u2229 Sp\u22121) is the Gaussian width of the intersection of TR(\u03b8\u2217) and the unit spherical shell Sp\u22121, \u03c9(\u2126R) is the Gaussian width of the unit norm ball, \u03baL > 0 is the gain given by\n\u03baL = 1\nn\n( `n \u2212 \u03c9(TR(\u03b8\u2217) \u2229 Sp\u22121) )2 , (14)\n\u03a8R = sup\u2206\u2208TR R(\u2206)/\u2016\u2206\u20162 is a norm compatibility factor, `n is the expected length of a length n i.i.d. standard Gaussian vector with n\u221a\nn+1 < `n <\n\u221a n, and c > 1, \u03b71, \u03b72 > 0 are constants.\nRemark: The choice of \u03bbp is also intimately connected to the notion of Gaussian width. Note that for X i.i.d. Gaussian entries, and w i.i.d. standard Gaussian vector, XTw = \u2016w\u20162 ( XT w\u2016w\u20162 ) = \u2016w\u20162z where z is an i.i.d. standard Gaussian vector. Therefore,\n\u03bbp \u2265 cE [ R\u2217(XTw) ] = cEw[\u2016w\u20162] \u00b7EX [ R\u2217(XT w\u2016w\u20162 ) ] (15)\n= cEw[\u2016w\u20162]Ez [\nsup u: R(u)\u22641\n\u3008u, z\u3009 ]\n(16)\n= c`n\u03c9 (\u2126R) , (17)\nwhich is a scaled Gaussian width of the unit ball of the normR(\u00b7). Example: L1-norm Dantzig Selector When R(\u00b7) is chosen to be L1 norm, the dual norm is the L\u221e norm, and (1) is reduced to the standard DS, given by\n\u03b8\u0302 = argmin \u03b8\u2208Rp\n\u2016\u03b8\u20161 s.t. \u2016XT (y \u2212X\u03b8)\u2016\u221e \u2264 \u03bb . (18)\nWe know that prox\u03b2\u2016\u00b7\u20161(\u00b7) is given by the elementwise soft-thresholding operation[ prox\u03b2\u2016\u00b7\u20161(x) ] i\n= sign(xi) \u00b7max(0, |xi| \u2212 \u03b2) . (19) Based on Proposition 1, the ADMM updates in Algorithm 1 can be instantiated as\n\u03b8k+1 \u2190 prox 2\u2016\u00b7\u20161 \u03c1\u00b5\n( \u03b8k \u2212 2\n\u00b5 AT (A\u03b8k + vk \u2212 u + z\nk \u03c1 ) ) ,\nvk+1 \u2190 (u\u2212A\u03b8k+1 \u2212 z k\n\u03c1 )\u2212 prox\u03bb\u2016\u00b7\u20161\n( u\u2212A\u03b8k+1 \u2212 z k\n\u03c1\n) ,\nzk+1 \u2190 zk + \u03c1(A\u03b8k+1 + vk+1 \u2212 u) , where the update of v leverages the decomposition (10). Similar updates were used in [18] for L1-norm Dantzig selector.\nFor statistical recovery, we assume that \u03b8\u2217 is s-sparse, i.e., contains s non-zero entries, and that \u2016\u03b8\u2217\u20162 = 1, so that \u2016\u03b8\u2217\u20161 \u2264 s. It was shown in [4] that the Gaussian width of the set (TL1(\u03b8\u2217) \u2229 Sp\u22121) is upper bounded as \u03c9(TL1(\u03b8\u2217)\u2229 Sp\u22121)2 \u2264 2s log (p s ) + 54s. Also note that E [ R\u2217(XTw) ] = E[\u2016w\u20162]E[\u2016g\u2016\u221e] \u2264\u221a\nn \u221a\nlog p, where g is a vector of i.i.d. standard Gaussian entries [3]. Further, [11] has shown that \u03a8R =\u221a s. Therefore, if we solve (18) with \u03bbp = c \u221a n log p, then\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 4c \u221a s log p\n\u03baL \u221a n = O\n(\u221a s log p\nn\n) (20)\nwith high probability, which agrees with known results for DS [2, 3].\n3 Dantzig Selection with k-support norm We first introduce some notations. Given any \u03b8 \u2208 Rp, let |\u03b8| denote its absolute-valued counterpart and \u03b8\u2193 denote the permutation of \u03b8 with its elements arranged in decreasing order. In previous work [1, 10], the k-support norm is defined as\n\u2016\u03b8\u2016spk = min  \u2211 I\u2208G(k) \u2016vI\u20162 : supp(vI) \u2286 I, \u2211 I\u2208G(k) vI = \u03b8  , (21) where G(k) denotes the set of subsets of {1, . . . , p} of cardinality at most k. The unit ball of this norm is the set Ck = conv {\u03b8 \u2208 Rp : \u2016\u03b8\u20160 \u2264 k, \u2016\u03b8\u20162 \u2264 1} . The dual norm of the k-support norm is given by\n\u2016\u03b8\u2016sp\u2217k = max { \u2016\u03b8G\u20162 : G \u2208 G(k) } = ( k\u2211 i=1 |\u03b8|\u21932i ) 1 2 . (22)\nThe k-support norm was proposed in order to overcome some of the empirical shortcomings of the elastic net [20] and the (group)-sparse regularizers. It was shown in [1] to behave similarly as the elastic net in the sense that the unit norm ball of the k-support norm is within a constant factor of \u221a 2 of the unit elastic net ball. Although multiple papers have reported good empirical performance of the k-support norm on selecting highly correlated features, wherein L1 regularization fails, there exists no statistical analysis of the k-support norm. Besides, current computational methods consider square of k-support norm in their formulation, which might fail to work out in certain cases.\nIn the rest of this section, we focus on GDS withR(\u03b8) = \u2016\u03b8\u2016spk given as\n\u03b8\u0302 = argmin \u03b8\u2208Rp\n\u2016\u03b8\u2016spk s.t. \u2016XT (y \u2212X\u03b8)\u2016 sp\u2217 k \u2264 \u03bbp . (23)\nFor the indicator function IC\u03bb(\u00b7) of the dual norm, we present a fast algorithm for computing its proximal operator by exploiting the structure of its solution, which can be directly plugged in Algorithm 1 to solve (23). Further, we prove statistical recovery bounds for k-support norm Dantzig selection, which hold even for a high-dimensional scenario, where n < p."}, {"heading": "3.1 Computation of Proximal Operator", "text": "In order to solve (23), either prox\u03bb\u2016\u00b7\u2016spk (\u00b7) or proxIC\u03bb (\u00b7) for \u2016 \u00b7 \u2016 sp\u2217\nk should be efficiently computable. Existing methods [1, 10] are inapplicable to our scenario since they compute the proximal operator for squared k-support norm, from which proxIC\u03bb (\u00b7) cannot be directly obtained. In Theorem 2, we show that proxIC\u03bb (\u00b7) can be efficiently computed, and thus Algorithm 1 is applicable.\nTheorem 2 Given \u03bb > 0 and x \u2208 Rp, if \u2016x\u2016sp\u2217k \u2264 \u03bb, then w\u2217 = proxIC\u03bb (x) = x. If \u2016x\u2016 sp\u2217 k > \u03bb, define Asr = \u2211r i=s+1 |x| \u2193 i , Bs = \u2211s i=1(|x| \u2193 i )\n2, in which 0 \u2264 s < k and k \u2264 r \u2264 p, and construct the nonlinear equation of \u03b2,\n(k \u2212 s)A2sr [\n1 + \u03b2\nr \u2212 s+ (k \u2212 s)\u03b2\n]2 \u2212 \u03bb2(1 + \u03b2)2 +Bs = 0 . (24)\nLet \u03b2sr be given by\n\u03b2sr = { nonnegative root of (24) if s > 0 and the root exists 0 otherwise . (25)\nThen the proximal operator w\u2217 = proxIC\u03bb (x) is given by\n|w\u2217|\u2193i =  1 1+\u03b2s\u2217r\u2217 |x|\u2193i if 1 \u2264 i \u2264 s\u2217\u221a \u03bb2\u2212Bs\u2217 k\u2212s\u2217 if s \u2217 < i \u2264 r\u2217 and \u03b2s\u2217r\u2217 = 0 As\u2217r\u2217 r\u2217\u2212s\u2217+(k\u2212s\u2217)\u03b2s\u2217r\u2217 if s\u2217 < i \u2264 r\u2217 and \u03b2s\u2217r\u2217 > 0\n|x|\u2193i if r\u2217 < i \u2264 p\n, (26)\nwhere the indices s\u2217 and r\u2217 with computed |w\u2217|\u2193 make the following two inequalities hold,\n|w\u2217|\u2193s\u2217 > |w\u2217|\u2193k , (27)\n|x|\u2193r\u2217+1 \u2264 |w\u2217|\u2193k < |x| \u2193 r\u2217 . (28)\nThere might be multiple pairs of (s, r) satisfying the inequalities (27)-(28), and we choose the pair with the smallest \u2016|x|\u2193 \u2212 |w|\u2193\u20162. Finally, w\u2217 is obtained by sign-changing and reordering |w\u2217|\u2193 to conform to x.\nRemark: The nonlinear equation (24) is quartic, for which we can use general formula to get all the roots [15]. In addition, if it exists, the nonnegative root is unique, as we show in the proof.\nTheorem 2 indicates that computing proxIC\u03bb (\u00b7) requires sorting of entries in |x| and a two-dimensional linear search of s\u2217 and r\u2217. Hence the total time complexity is O(p log p + k(p \u2212 k)). However, a more careful observation can particularly reduce the search complexity from O(k(p\u2212 k)) to O(log k log(p\u2212 k)), which is motivated by Theorem 3.\nTheorem 3 In search of (s\u2217, r\u2217) defined in Theorem 2, there can be only one r\u0303 for a given candidate s\u0303 of s\u2217, such that the inequality (28) is satisfied. Moreover if such r\u0303 exists, then for any r < r\u0303, the associated |w\u0303|\u2193k violates the first part of (28), and for r > r\u0303, |w\u0303| \u2193 k violates the second part of (28). On the other hand, based on the r\u0303, we have following assertion of s\u2217,\ns\u2217  > s\u0303 if r\u0303 does not exist \u2265 s\u0303 if r\u0303 exists and corresponding |w\u0303|\u2193k satisfies (27) < s\u0303 if r\u0303 exists but corresponding |w\u0303|\u2193k violates (27) . (29)\nBased on Theorem 3, the accelerated search procedure of (s\u2217, r\u2217) is to execute a two-dimensional binary search, and Algorithm 2 gives the details. Therefore the total time complexity becomes O(p log p + log k log(p\u2212 k)). Compared with previous proximal operators for squared k-support norm, this complexity is better than that in [1], and roughly the same as the most recent one in [10].\n3.2 Statistical Recovery Guarantees for k-support norm\nThe analysis of the generalized Dantzig Selector for k-support norm consists of addressing two key challenges. First, note that Theorem 1 requires an appropriate choice of \u03bbp. Second, one needs to compute the Gaussian width of the subset of the error set TR(\u03b8\u2217) \u2229 Sp\u22121. For the k-support norm, we can get upper bounds to both of these quantities. We start by defining some notations. Let G\u2217 \u2286 G(k) be the set of groups intersecting with the support of \u03b8\u2217, and let S be the union of groups in G\u2217, such that s = |S|. Then, we have the following bounds which are used for choosing \u03bbp, and bounding the Gaussian width.\nAlgorithm 2 Algorithm for computing proxIC\u03bb (\u00b7) of \u2016 \u00b7 \u2016 sp\u2217 k Input: x, k, \u03bb Output: w\u2217 = proxIC\u03bb (x)\n1: if \u2016x\u2016sp\u2217k \u2264 \u03bb then 2: w\u2217 := x 3: else 4: l := 0, u := k \u2212 1, and sort |x| to get |x|\u2193 5: while l \u2264 u do 6: s\u0303 := b(l + u)/2c, and binary search for r\u0303 that satisfies (28) and compute w\u0303 based on (26) 7: if r\u0303 does not exist then 8: l := s\u0303+ 1 9: else if r\u0303 exists and (27) is satisfied then\n10: w\u2217 := w\u0303, l := s\u0303+ 1 11: else if r\u0303 exists but (27) is not satisfied then 12: u := s\u0303\u2212 1 13: end if 14: end while 15: end if\nTheorem 4 For the k-support norm Generalized Dantzig Selection problem (23), we obtain For the ksupport norm Generalized Dantzig Selection problem (23), we obtain\nE [ R\u2217(XTw) ] \u2264 \u221an (\u221a 2k log (pe k ) + \u221a k ) (30)\n\u03c9(\u2126R) \u2264 (\u221a 2k log (pe k ) + \u221a k ) (31)\n\u03c9(TA(\u03b8\u2217) \u2229 Sp\u22121)2 \u2264 (\u221a 2k log ( p\u2212 k \u2212 \u2308 s k \u2309 + 2 ) + \u221a k )2 \u00b7 \u2308 s k \u2309 + s . (32)\nWe prove these two bounds using the analysis technique for group lasso with overlaps developed in [13]. Thereafter, choosing \u03bbp = \u221a n (\u221a 2k log (pe k ) + \u221a k )\n, and under the assumptions of Theorem 1, we obtain the following result on the error bound for the minimizer of (23).\nCorollary 1 Suppose that all conditions of Theorem 1 hold, and we solve (23) with \u03bbp chosen as above. Then, with high probability, we obtain\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 4c\u03a8R\n(\u221a 2k log (pe k ) + \u221a k )\n\u03baL \u221a n\n(33)\nRemark The error bound provides a natural interpretation for the two special cases of the k-support norm, viz. k = 1 and k = p. First, for k = 1 the k-support norm is exactly the same as the L1 norm, and the error\nbound obtained will be O (\u221a\ns log p n\n) , the same as known results of DS, and shown in Section 2.2. Second,\nfor k = p, the k-support norm is equal to the L2 norm, and the error cone (11) is then simply a half space (there is no structural constraint). Therefore, \u03a8R = O(1), and the error bound scales as O (\u221a p n ) ."}, {"heading": "4 Experimental Results", "text": "On optimization side, our ADMM framework is concentrated on its generality, and its efficiency has been shown in [18] for the special case of L1 norm. Hence we focus on the efficiency of different proximal operators related to k-support norm. On statistical side, we concentrate on the behavior and performance of GDS with k-support norm. All experiments are implemented in MATLAB."}, {"heading": "4.1 Efficiency of Proximal Operator", "text": "We tested four proximal operators related to k-support norm, which are our normal proxIC\u03bb (\u00b7) and its accelerated version, prox 1\n2\u03b2 (\u2016\u00b7\u2016spk )2 (\u00b7) in [1], and prox\u03bb 2 \u2016\u00b7\u20162\u0398 (\u00b7) in [10]. The dimension p of vector in experiment varied from 1000 to 10000, and the ratio p/k = {200, 100, 50, 20}. As illustrated in Figure 1, in general, the speedup of accelerated proxIC\u03bb (\u00b7) is considerable when compared with the normal proxIC\u03bb (\u00b7) and prox 1\n2\u03b2 (\u2016\u00b7\u2016spk )2 (\u00b7). Empirically it is also slightly better than the prox\u03bb 2 \u2016\u00b7\u20162\u0398 (\u00b7)."}, {"heading": "4.2 Statistical Recovery on Synthetic Data", "text": "Data generation We fixed p = 600, and \u03b8\u2217 = (10, . . . , 10\ufe38 \ufe37\ufe37 \ufe38 10 , 10, . . . , 10\ufe38 \ufe37\ufe37 \ufe38 10 , 10, . . . , 10\ufe38 \ufe37\ufe37 \ufe38 10 , 0, 0, . . . , 0\ufe38 \ufe37\ufe37 \ufe38 570 ) throughout the experiment, in which nonzero entries were divided equally into three groups. The design matrix X were generated from a normal distribution such that the entries in the same group have the same mean sampled from N (0, 1). X was normalized afterwards. The response vector y was given by y = X\u03b8\u2217 + 0.01 \u00d7 N (0, 1). The number of samples n is specified later.\nROC curves with different k We fixed n = 400 to obtain the ROC plot for k = {1, 10, 50} as shown in Figure 2(a). \u03bbp ranged from 10\u22122 to 103.\nL2 error vs. n We investigated how theL2 error \u2016\u03b8\u0302\u2212\u03b8\u2217\u20162 of Dantzig selector changes as the number of samples increases, where k = {1, 10, 50} and n = {30, 60, 90, . . . , 300}. The plot is shown in Figure 2(b).\nL2 error vs. k We also looked at the L2 error with different k. We again fixed n = 400 and varied k from 1 to 39. For each k, we repeated the experiment 100 times, and obtained the mean and standard deviation plot in Figure 2(c)."}, {"heading": "5 Conclusions", "text": "In this paper, we introduced the GDS, which generalizes the standard L1-norm Dantzig Selector to estimation with any norm, such that structural information encoded in the norm can be efficiently exploited. A flexible framework based on inexact ADMM is proposed for solving the GDS, which only requires one of conjugate proximal operators to be efficiently solved. Further, we provide a unified statistical analysis framework for the GDS, which utilizes Gaussian widths of certain restricted sets for proving consistency. In the non-trivial example of k-support norm, we showed that the proximal operators used in the inexact ADMM can be computed more efficiently compared to previously proposed variants. Our statistical analysis for the k-support norm provides the first result of consistency of this structured norm. Last, experimental results provided sound support to the theoretical development in the paper."}, {"heading": "Acknowledgements", "text": "The research was supported by NSF grant IIS-1029711. The work was also supported by NSF grants IIS0916750, IIS-0953274 and CNS-1314560 and by NASA grant NNX12AQ39A. A. B. acknowledges support from IBM and Yahoo."}, {"heading": "A Proof of Theorem 1", "text": "Statement of Theorem: Suppose the design matrix X consists of i.i.d. Gaussian entries with zero mean variance 1, and we solve the optimization problem (1) with\n\u03bbp \u2265 cE [ R\u2217(XTw) ] . (34)\nThen, with probability at least (1\u2212 \u03b71 exp(\u2212\u03b72n)), we have\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 4c\u03a8R\u03c9(\u2126R)\n\u03baL \u221a n\n, (35)\nwhere \u03c9(TR(\u03b8\u2217) \u2229 Sp\u22121) is the Gaussian width of the intersection of TR(\u03b8\u2217) and the unit spherical shell Sp\u22121, \u03c9(\u2126R) is the Gaussian width of the unit norm ball, \u03baL > 0 is the gain given by\n\u03baL = 1\nn\n( `n \u2212 \u03c9(TR(\u03b8\u2217) \u2229 Sp\u22121) )2 , (36)\n\u03a8R = sup\u2206\u2208TR R(\u2206)/\u2016\u2206\u20162 is a norm compatibility factor, `n is the expected length of a length n i.i.d. standard Gaussian vector with n\u221a\nn+1 < `n <\n\u221a n, and c > 1, \u03b71, \u03b72 > 0 are constants.\nProof: We use the following lemma for the proof.\nLemma 1 Suppose we solve the minimization problem (1) with \u03bbp \u2265 R\u2217 ( XTw ) . Then the error vector \u2206\u0302 belongs to the set TR(\u03b8\u2217) := cone {\u2206 \u2208 Rp : R(\u03b8\u2217 + \u2206) \u2264 R(\u03b8\u2217)} , (37)\nand the error \u2206\u0302 = \u03b8\u0302 \u2212 \u03b8\u2217 satisfies the following bound R\u2217 ( XTX\u2206\u0302 ) \u2264 2\u03bbp (38)\nProof: By our choice of \u03bbp, both \u03b8\u2217 and \u03b8\u0302 lie in the feasible set of (1) , and by optimality of \u03b8\u0302, R ( \u03b8\u2217 + \u2206\u0302 ) = R(\u03b8\u0302) \u2264 R(\u03b8\u2217) . (39)\nAlso, by triangle inequality R\u2217 ( XTX\u2206\u0302 ) = R\u2217 ( XTX(\u03b8\u0302 \u2212 \u03b8\u2217) ) (40)\n\u2264 R\u2217 ( XT (y \u2212X\u03b8\u2217) ) +R\u2217 ( XT (y \u2212X\u03b8\u0302) ) \u2264 2\u03bbp . (41)\nNow, note that X and w are independent and we can rewrite\nEX,w [ R\u2217(XTw) ] = Ew [ EX [ R\u2217(XTw)|w ]] = Ew [ \u2016w\u20162EX [ R\u2217 ( XT w\n\u2016w\u20162\n) |w ]] . (42)\nSince w/\u2016w\u20162 is an isotropic unit vector uniformly distributed over the surface of the unit sphere,( XT w\u2016w\u20162 ) = g is an i.i.d. N (0, 1) Gaussian vector. Therefore\nEX,w [ R\u2217(XTw) ] = Ew[\u2016w\u20162]Eg[R\u2217(g)] . (43)\nAlso, note that R\u2217(\u00b7) is Lipschitz continuous with Lipschitz constant of 1 w.r.t. the normR\u2217, and hence by Gaussian concentration of Lipschitz functions [7],\nP (R\u2217(g) \u2265 Eg[R\u2217(g)] + \u03c4) \u2264 exp [ \u2212\u03c4 2\n2\n] , (44)\nand similarly \u2016w\u20162 \u2264 `n + \u03c4 with probability at least 1 \u2212 exp(\u2212\u03c42/2), where n\u221an+1 \u2264 `n \u2264 \u221a n is the expected length of w. Therefore, for some c > 1 choosing \u03bbp \u2265 cE [ R\u2217(XTw) ] = c `nEg[R\u2217(g)] implies that\nP ( \u03bbp \u2265 R\u2217(XTw) ) \u2265 ( 1\u2212 exp [ \u2212 c1E 2 g[R\u2217(g)]\n2\n])( 1\u2212 exp [ \u2212c2` 2 n\n2\n]) = 1\u2212\u03b7\u20321 exp(\u2212\u03b7\u20322n) , (45)\nfor some constant c1, c2, \u03b7\u20321, \u03b7 \u2032 2 > 0. Further, note that Eg[R\u2217(g)] = \u03c9(\u2126R), the Gaussian width of the unit ball of normR. Also, from Lemma 1, we have\nR\u2217 ( XTX\u2206\u0302 ) \u2264 2\u03bbp (46)\nNow, note that \u2016X\u2206\u0302\u201622 = \u3008\u2206\u0302,XTX\u2206\u0302\u3009 \u2264 |\u3008\u2206\u0302,XTX\u2206\u0302\u3009| \u2264 R(\u2206\u0302)R\u2217 ( XTX\u2206\u0302 ) \u2264 2\u03bbpR(\u2206\u0302) , (47) where we have used Ho\u0308lder\u2019s inequality, and the boundR\u2217 ( XTX\u2206\u0302 ) \u2264 2\u03bbp from above.\nNext, we use Gordon\u2019s theorem, which states that for X with i.i.d. Gaussian (0, 1) entries,\nE [ min\nz\u2208TR(\u03b8\u2217)\u2229Sp\u22121 \u2016Xz\u20162\n] \u2265 `n \u2212 \u03c9 ( TR(\u03b8\u2217) \u2229 Sp\u22121 ) , (48)\nwhere `n is the expected length of an i.i.d. Gaussian random vector of length n, and \u03c9 ( TR(\u03b8\u2217) \u2229 Sp\u22121 ) is the Gaussian width of the set \u2126 = ( TR(\u03b8\u2217) \u2229 Sp\u22121 ) . Now, since the function X \u2192 minz\u2208\u2126 \u2016Xz\u20162 is Lipschitz continuous with constant 1 over the set \u2126, we can use Gaussian concentration of Lipschitz functions [7] to obtain\n\u2016X\u2206\u20162 \u2265 1\n2\n( `n \u2212 \u03c9(TR(\u03b8\u2217) \u2229 Sp\u22121) ) \u2016\u2206\u20162 (49)\n\u21d2 1\u221a n \u2016X\u2206\u20162 \u2265\n( `n \u2212 \u03c9(TR(\u03b8\u2217) \u2229 Sp\u22121) ) 2 \u221a n \u2016\u2206\u20162 (50)\n\u21d2 1 n \u2016X\u2206\u201622 \u2265 \u03baL 2 \u2016\u2206\u201622 , (51)\nwith probability greater than 1\u2212 exp ( \u221218 ( `n \u2212 \u03c9(TR(\u03b8\u2217) \u2229 Sp\u22121) )2) = 1\u2212 \u03b7\u2032\u20321 exp(\u2212\u03b7\u2032\u20322n), where \u03baL =(\n`n \u2212 \u03c9(TR(\u03b8\u2217) \u2229 Sp\u22121) )2 /n > 0 is the gain, and \u03b7\u2032\u20321 , \u03b7 \u2032\u2032 2 > 0 are constants .\nCombining (51) and (47), and using the choice of \u03bbp, we obtain\n\u2016\u03b8\u0302n \u2212 \u03b8\u2217\u20162 = \u2016\u2206\u0302\u20162 \u2264 4cE\n[ R\u2217(XTw) ] \u03baLn R(\u2206) \u2016\u2206\u20162 \u2264 4c\u03a8R\u03c9(\u2126R) \u03baL \u221a n\n(52)\nwith probability greater than (1 \u2212 \u03b7\u20321 exp(\u2212\u03b7\u20322n))(1 \u2212 \u03b7\u2032\u20321 exp(\u2212\u03b7\u2032\u20322n)) = 1 \u2212 \u03b71 exp(\u2212\u03b72n), for constants \u03b71, \u03b72 where\n\u03a8R = sup \u2206\u2208TR R(\u2206) \u2016\u2206\u20162 . (53)\nThe statement of the theorem follows."}, {"heading": "B Proof of Theorem 2", "text": "Statement of Theorem: Given \u03bb > 0 and x \u2208 Rp, if \u2016x\u2016sp\u2217k \u2264 \u03bb, then w\u2217 = proxIC\u03bb (x) = x. If \u2016x\u2016sp\u2217k > \u03bb, define Asr = \u2211r i=s+1 |x| \u2193 i , Bs = \u2211s i=1(|x| \u2193 i )\n2, in which 0 \u2264 s < k and k \u2264 r \u2264 p, and construct the nonlinear equation of \u03b2,\n(k \u2212 s)A2sr [\n1 + \u03b2\nr \u2212 s+ (k \u2212 s)\u03b2\n]2 \u2212 \u03bb2(1 + \u03b2)2 +Bs = 0 . (54)\nLet \u03b2sr be given by\n\u03b2sr = { nonnegative root of (54) if s > 0 and the root exists 0 otherwise . (55)\nThen the proximal operator w\u2217 = proxIC\u03bb (x) is given by\n|w\u2217|\u2193i =  1 1+\u03b2s\u2217r\u2217 |x|\u2193i if 1 \u2264 i \u2264 s\u2217\u221a \u03bb2\u2212Bs\u2217 k\u2212s\u2217 if s \u2217 < i \u2264 r\u2217 and \u03b2s\u2217r\u2217 = 0 As\u2217r\u2217 r\u2217\u2212s\u2217+(k\u2212s\u2217)\u03b2s\u2217r\u2217 if s\u2217 < i \u2264 r\u2217 and \u03b2s\u2217r\u2217 > 0\n|x|\u2193i if r\u2217 < i \u2264 p\n, (56)\nwhere the indices s\u2217 and r\u2217 with computed |w\u2217|\u2193 make the following two inequalities hold,\n|w\u2217|\u2193s\u2217 > |w\u2217|\u2193k , (57)\n|x|\u2193r\u2217+1 \u2264 |w\u2217|\u2193k < |x| \u2193 r\u2217 . (58)\nThere might be multiple pairs of (s, r) satisfying the inequalities (57)-(58), and we choose the pair with the smallest \u2016|x|\u2193 \u2212 |w|\u2193\u20162. Finally, w\u2217 is obtained by sign-changing and reordering |w\u2217|\u2193 to conform to x. Proof: Let w\u2217 = proxIC\u03bb (x) = argminw\u2208C\u03bb 1 2\u2016x \u2212 w\u201622. For simplicity, we drop the constant 12 in later discussion. Given a vector x, we use the notation xi:j to denote its subvector (xi,xi+1, . . . ,xj). We consider the following two cases.\nCase 1: if \u2016x\u2016sp\u2217k \u2264 \u03bb, it is trivial that w\u2217 = x, which is also the global minimizer of \u2016x\u2212w\u201622 without the constraint x \u2208 C\u03bb.\nCase 2: if \u2016x\u2016sp\u2217k > \u03bb, first we start by noting that given x and w, the following inequality holds\n\u2016x\u2212w\u201622 = \u2016x\u201622 \u2212 2\u3008x,w\u3009+ \u2016w\u201622 \u2265 \u2016x\u201622 \u2212 2\u3008|x|\u2193, |w|\u2193\u3009+ \u2016w\u201622 ,\nwhich implies that w\u2217 should achieve this lower bound by conforming with the signs and orders of elements in x. Without loss of generality, we are simply focused on the case where x = |x|\u2193.\nFor w\u2217 to be the optimal, w\u2217k:p should be chosen such that w \u2217 k:r = (w \u2217 k,w \u2217 k, . . . ,w \u2217 k) and w \u2217 r+1:p =\nx\u2217r+1:p, where r satisfies\nxr > w \u2217 k \u2265 xr+1 ,\notherwise either the decreasing order of w\u2217 will be violated or the \u2016xk:p \u2212wk:p\u20162 is not minimized. As for w\u22171:k\u22121, we similarly assume w \u2217 s+1:k\u22121 = (w \u2217 k,w \u2217 k, . . . ,w \u2217 k) for some 0 \u2264 s \u2264 k \u2212 1, then w\u22171:s should be chosen to minimize \u2016x1:s \u2212w1:s\u20162 such that\n\u2016w1:s\u201622 = \u2016w\u22171:k\u201622 \u2212 \u2016w\u2217s+1:k\u201622 \u2264 \u03bb2 \u2212 (k \u2212 s)(w\u2217k)2.\nBy Cauchy-Schwarz Inequality, we have\n\u2016x1:s \u2212w1:s\u201622 \u2265 \u2016x1:s\u201622 \u2212 2\u2016x1:s\u20162\u2016w1:s\u20162 + \u2016w1:s\u201622 ,\nwhere the equality holds when w\u22171:s follows the form of w \u2217 1:s = 1 1+\u03b2sr x1:s, and \u03b2sr \u2265 0 satisfies the constraint Bs\n(1+\u03b2sr)2 = \u03bb2 \u2212 (k \u2212 s)(wk)2.\nSo far we have figured out the structure of w\u2217 = (w\u22171:s,w \u2217 s+1:r,w \u2217 r+1:p), in which the three subvectors, compared with x, are shrunk by a common factor 1 + \u03b2sr, constant w\u2217k, or unchanged. Next we need to determine the value of \u03b2sr and w\u2217k. By optimality, \u2016x \u2212w\u201622 = \u2016x1:r \u2212w1:r\u201622 must be minimized at w\u2217, so we have the following problem,\nmin \u03b2,wk\n\u2016x1:r \u2212w1:r\u201622 = \u2016x1:s \u2212w1:s\u201622 + \u2016xs+1:r \u2212ws+1:r\u201622\n= ( \u03b2\n1 + \u03b2 )2Bs + r\u2211 i=s+1 (xi \u2212wk)2 (59)\ns.t. (\u2016w\u2016sp\u2217k )2 = Bs\n(1 + \u03b2)2 + (k \u2212 s)(wk)2 = \u03bb2 (60)\nReplacing wk in (59) with wk =\n\u221a \u03bb2\u2212 Bs\n(1+\u03b2)2\nk\u2212s obtained from (60), we express \u2016x1:r \u2212w1:r\u201622 as a function of \u03b2,\n\u03a6sr(\u03b2) = ( \u03b2\n1 + \u03b2 )2Bs + r\u2211 i=s+1 ( xi \u2212\n\u221a \u03bb2 \u2212 Bs\n(1+\u03b2)2 k \u2212 s )2 (61)\nSet derivative of \u03a6sr(\u03b2) to be zero, we have\nd\nd\u03b2 \u03a6sr(\u03b2) =\nd\nd\u03b2\n[ ( \u03b2\n1 + \u03b2 )2Bs + r\u2211 i=s+1 ( xi \u2212\n\u221a \u03bb2 \u2212 Bs\n(1+\u03b2)2 k \u2212 s )2] (62)\n= 2\u03b2\n(1 + \u03b2)3 Bs \u2212\n2AsrBs (1 + \u03b2)3(k \u2212 s) \u221a \u03bb2\u2212 Bs (1+\u03b2)2\nk\u2212s\n+ 2(r \u2212 s)Bs\n(k \u2212 s)(1 + \u03b2)3 (63)\n= 2Bs (k \u2212 s)(1 + \u03b2)3 [ (k \u2212 s)\u03b2 \u2212 Asr\u221a\n\u03bb2\u2212 Bs (1+\u03b2)2\nk\u2212s\n+ (r \u2212 s) ] = 0 (64)\nIf s > 0, then Bs > 0 and (64) is equivalent to (54). And we can see that the quantity inside the bracket of (64) is monotonically increasing when \u03b2 \u2265 max(0, \u221a Bs\u2212\u03bb \u03bb ), thus ensuring the nonnegative root \u03b2sr is unique if it exists. If the nonnegative root exists, the expression for w\u2217s+1:r can be obtained from (64), whose entries are all equal to w\u2217k.\nIf s > 0 and a nonnegative root of (64) is nonexistent, the derivative is always positive when \u03b2 \u2265 0, which means that \u03a6sr(\u03b2) is increasing. Hence the minimizer of \u03a6sr(\u03b2) is \u03b2sr = 0. If s = 0, we actually do not care about the value of \u03b2sr because the problem defined by (59) and (60) is independent of \u03b2, and we set it to be 0 for simplicity. According to (60), both cases of \u03b2sr = 0 lead to the same expression for w\u2217s+1:r in (56).\nAs we do not know beforehand which s and r to choose, we need to search for s\u2217 and r\u2217 that give the smallest \u2016|x|\u2193 \u2212 |w|\u2193\u20162, and also need to check whether the w\u2217 obtained by (56) is in decreasing order, which are the conditions (57) and (58) presented in Theorem 2."}, {"heading": "C Proof of Theorem 3", "text": "To prove Theorem 3, we first need the following corollary from Theorem 2.\nCorollary 2 When \u03b2 \u2265 max(0, \u221a Bs\u2212\u03bb \u03bb ), \u03a6sr(\u03b2) defined in (61) is decreasing when \u03b2 < \u03b2sr, and increasing when \u03b2 > \u03b2sr. Equivalently, \u03a6sr(\u03b2) = \u2016x1:r \u2212w1:r\u201622, when treated as function of wk, is decreasing when wk < w\u2217k and increasing when wk > w \u2217 k.\nProof: The first part simply follows the monotonicity of dd\u03b2\u03a6sr(\u03b2) mentioned in the proof of Theorem 2, which implies that dd\u03b2\u03a6sr(\u03b2) is negative when \u03b2 < \u03b2sr, and positive when \u03b2 > \u03b2sr . The constraint (60) implies that wk increases as \u03b2 increases. So \u2016x1:r\u2212w1:r\u201622, as a function of wk, has the same monotonicity w.r.t. wk.\nStatement of Theorem: In search of (s\u2217, r\u2217) defined in Theorem 2, there can be only one r\u0303 for a given candidate s\u0303 of s\u2217, such that the inequality (58) is satisfied. Moreover if such r\u0303 exists, then for any r < r\u0303, the associated |w\u0303|\u2193k violates the first part of (58), and for r > r\u0303, |w\u0303| \u2193 k violates the second part of (58). On the other hand, based on the r\u0303, we have following assertion of s\u2217,\ns\u2217  > s\u0303 if r\u0303 does not exist \u2265 s\u0303 if r\u0303 exists and corresponding |w\u0303|\u2193k satisfies (57) < s\u0303 if r\u0303 exists but corresponding |w\u0303|\u2193k violates (57) . (65)\nProof: We again focus on the case of x = |x|\u2193. First we show by contradiction that for a given s\u0303, the r\u0303 that satisfies (58) can be at most one.\nSuppose there are two indices, say r1 and r2, which satisfy that condition with the same s\u0303. Without loss of generality, let r1 < r2, we know that their corresponding w(1) and w(2) should minimize \u2016x1:r1\u2212w1:r1\u201622 and \u2016x1:r2 \u2212w1:r2\u201622, respectively. As r1 < r2, then w (1) k \u2265 xr2 > w (2) k according to (58). Construct\nw\u2032 = ( x1\n1 + \u03b2\u2032 , . . . , xs\u0303 1 + \u03b2\u2032\ufe38 \ufe37\ufe37 \ufe38\ns\u0303\n,xr2 , . . . ,xr2\ufe38 \ufe37\ufe37 \ufe38 r2\u2212s\u0303 ,xr2+1, . . . ,xp)\nwhere \u03b2\u2032 is chosen to satisfy the constraint (60) with w\u2032k = xr2 , and \u2016x1:r2 \u2212w (2) 1:r2 \u201622 can be decomposed\nas\n\u2016x1:r2 \u2212w (2) 1:r2 \u201622 = \u2016x1:r1 \u2212w (2) 1:r1 \u201622 + \u2016xr1+1:r2 \u2212w (2) r1+1:r2 \u201622 > \u2016x1:r1 \u2212w\u20321:r1\u201622 + \u2016xr1+1:r2 \u2212w\u2032r1+1:r2\u201622 = \u2016x1:r2 \u2212w\u20321:r2\u201622\nwhich contradicts that w(2)1:r2 minimizes \u2016x1:r2 \u2212w1:r2\u201622. Note that \u2016x1:r1 \u2212w (2) 1:r1 \u201622 > \u2016x1:r1 \u2212w\u20321:r1\u201622 simply follows Corollary 2 as w(1)k \u2265 xr2 = w\u2032k > w (2) k , and \u2016xr1+1:r2 \u2212 w (2) r1+1:r2\n\u201622 > \u2016xr1+1:r2 \u2212 w\u2032r1+1:r2\u201622 is due to the fact that xr1+1 \u2265 . . . \u2265 xr2 = w\u2032k > w (2) k .\nNext we show by contradiction that if r\u0303 exists for given s\u0303, then any r < r\u0303 violates the first part of (58), and any r > r\u0303 violates the second part.\nLet w\u0303 denote the minimizer of \u2016x1:r\u0303 \u2212w1:r\u0303\u201622. Suppose r < r\u0303 and the first part of (58) is not violated, then its second part must be violated due to the uniqueness of r\u0303. Then we can construct new\nw\u2032 = ( x1\n1 + \u03b2\u2032 , . . . , xs\u0303 1 + \u03b2\u2032\ufe38 \ufe37\ufe37 \ufe38\ns\u0303\n,xr\u0303, . . . ,xr\u0303\ufe38 \ufe37\ufe37 \ufe38 r\u0303\u2212s\u0303 ,xr\u0303+1, . . . ,xp) ,\nwhere \u03b2\u2032 is again chosen to satisfy the constraint (60) with w\u2032k = xr\u0303. This by the same argument for proving the uniqueness of r\u0303 make the following inequality hold,\n\u2016x1:r\u0303 \u2212 w\u03031:r\u0303\u201622 = \u2016x1:r \u2212 w\u03031:r\u201622 + \u2016xr+1:r\u0303 \u2212 w\u0303r+1:r\u0303\u201622 > \u2016x1:r \u2212w\u20321:r\u201622 + \u2016xr+1:r\u0303 \u2212w\u2032r+1:r\u0303\u201622 = \u2016x1:r\u0303 \u2212w\u20321:r\u0303\u201622 .\nThis contradicts that w\u0303 is the minimizer of \u2016x1:r\u0303\u2212w1:r\u0303\u201622. Similar argument applies to the case when r > r\u0303. Let \u03b2\u2032\u2032 satisfy (60) together with w\u2032\u2032k = xr+1, and we construct\nw\u2032\u2032 = ( x1\n1 + \u03b2\u2032\u2032 , . . . , xs 1 + \u03b2\u2032\u2032\ufe38 \ufe37\ufe37 \ufe38\ns\u0303\n,xr+1, . . . ,xr+1\ufe38 \ufe37\ufe37 \ufe38 r\u2212s\u0303 ,xr+1, . . . ,xp) ,\nwhich gives smaller \u2016x1:r \u2212 w1:r\u201622 than any w with wk < xr+1. Therefore it is impossible for r > r\u0303 to violate the first inequality.\nFinally we show the assertion (65) for s\u2217. We note that given s\u0303 , finding solution to the proximal operator can be viewed as minimization of (59) under the constraint \u2016w1:k\u20162 \u2264 \u03bb and wk = wk\u22121 = . . . = ws\u0303+1. So for s < s\u0303, the minimization problem is equivalent to the one for s\u0303 under additional constraint ws\u0303+1 = ws\u0303 = . . . = ws+1. If the r\u0303 does not exist, for s < s\u0303, r\u0303 is nonexistent either, thus s\u2217 > s\u0303. If the r\u0303 exists and (57) is satisfied, then s\u2217 \u2265 s\u0303 because s < s\u0303 considers a more restricted problem and is unable to obtain a smaller \u2016x\u2212w\u20162.\nFor the situation in which r\u0303 exists for s\u0303 but the associated w\u0303k violates (57), we show by contradiction that for any s > s\u0303, (57) is also violated.\nAssume that w\u2032 (different from the previously used) satisfies both (57) and (58) for s\u2032 = s\u0303 + 1 and the corresponding r\u2032. It is not difficult to see that w\u2032k < w\u0303k and r\n\u2032 \u2265 r\u0303, otherwise \u2016w\u20321:k\u20162 > \u03bb. By the violation we have shown for r, the minimizer of (59) for (s\u2032, r\u0303), denoted by w\u2032\u2032, satisfies w\u2032\u2032k \u2264 w\u2032k (Note that w\u2032 is the minimizer of (59) for (s\u2032, r\u2032) and r\u2032 \u2265 r\u0303). Combined with w\u2032k < w\u0303k, this indicates by Corollary 2 that \u03a6s\u2032r\u0303(\u00b7) is increasing on the interval [w\u2032\u2032k, w\u0303k]. Then we consider two sequential modifications on w\u0303,\n1. Replacing the w\u03031:s\u2032 in w\u0303 with \u2016w\u03031:s\u2032\u20162 \u2016x1:s\u2032\u20162 x1:s\u2032 ,\n2. Decreasing w\u0303s\u2032+1:r\u0303 by certain amount and amplifying the new w\u03031:s\u2032 by some factor, such that (60) still holds for s\u2032 and w\u0303s\u2032+1 = w\u0303s\u2032 .\nNote that the two modifications both decrease \u2016x1:r\u0303 \u2212 w\u03031:r\u0303\u20162. Decrease in Modification 1 is the result of Cauchy Schwarz Inequality, and decrease in Modification 2 is due to the monotonicity of \u03a6s\u2032r\u0303(\u00b7) we mentioned afront. The modified w\u0303 satisfies w\u0303s\u0303+1 = w\u0303s\u0303+2 = . . . = w\u0303k, thus contradicting that the old w\u0303 is the minimizer of (59) for (s\u0303, r\u0303). Hence, by induction, we conclude that for any s\u2032 > s\u0303, its solution also violates (57).\nAssembling the conclusions above, we have (65) for s\u2217."}, {"heading": "D Proof of Theorem 4", "text": "Statement of Theorem: For the k-support norm Generalized Dantzig Selection problem (23), we obtain\nE [ R\u2217(XTw) ] \u2264 \u221an (\u221a 2k log (pe k ) + \u221a k ) (66)\n\u03c9(\u2126R) \u2264 (\u221a 2k log (pe k ) + \u221a k ) (67)\n\u03c9(TA(\u03b8\u2217) \u2229 Sp\u22121)2 \u2264 (\u221a 2k log ( p\u2212 k \u2212 \u2308 s k \u2309 + 2 ) + \u221a k )2 \u00b7 \u2308 s k \u2309 + s . (68)\nProof: We first illustrate that the k-support norm is an atomic norm, and then prove Theorem 4.\nD.1 k-Support norm as an Atomic Norm\nHere we show that k-support norm satisfies the definition of atomic norms [4]. Consider Gj to be the set of all subsets of {1, 2, . . . , p} of size j, so that\nG(k) = {Gj}kj=1 . (69)\nFor every j, consider the set\nAj = {w : \u2016(wGj )\u20162 = 1, Gj \u2208 Gj , wi = 1\u221a j , \u2200i \u2208 Gj , wi = 0, \u2200i /\u2208 Gj} , (70)\ncorresponding to Gj , and the union of such sets\nA = {Aj}j\u2208{1,...,k} . (71)\nNote that since every non-zero element in a vector in Aj is 1\u221aj , such an element cannot be represented as a convex combination of elements of the set Al, l < j, whose non-zero elements are 1\u221al . Therefore none of the elements w in the set A lies in the convex hull of the other elements A \\ {w}. Further, note that\nconv(A) = Ck , (72)\nand the k-support norm defines the gauge function of the A. Thus the k-support norm is an atomic norm.\nD.2 The Error set and its Gaussian width\nNote that the cardinality of the set G(k) is\nM =\n( p\nk\n) + ( p\nk \u2212 1\n) + ( p\nk \u2212 2\n) + \u00b7 \u00b7 \u00b7+ ( p\n1\n) (73)\nThe error set is given by\nTA(\u03b8\u2217) = cone{\u2206 \u2208 Rp : \u2016\u2206 + \u03b8\u2217\u2016spk \u2264 \u2016\u03b8\u2217\u2016 sp k } . (74)\nNote that this set is a cone, and we can define the normal cone of this set as\nNA(\u03b8\u2217) = {u : \u3008u,\u2206\u3009 \u2264 0, \u2200\u2206 \u2208 TA(\u03b8\u2217)} (75) (76)\nThe following proposition, shown in [13], shows that the normal cone can be written in terms of the dual norm of the k-support norm.\nProposition 2 The normal cone to the tangent cone defined in (74) can written as\nNA(\u03b8\u2217) = {u : \u2203t > 0 s.t. \u3008u,\u03b8\u2217\u3009 = t\u2016\u03b8\u2217\u2016spk , \u2016u\u2016 sp\u2217 k \u2264 t} . (77)\nWe provide a simple proof of this statement for our case for ease of understanding. Proof: We re-write the definition of the normal cone in terms of the estimated parameter \u03b8\u0302 as\nNA(\u03b8\u2217) = {u \u2208 Rp : \u3008u,\u03b8 \u2212 \u03b8\u2217\u3009 \u2264 0,\u2200\u03b8 \u2212 \u03b8\u2217 \u2208 TA(\u03b8\u2217)} . (78)\nNote that this means that u \u2208 NA(\u03b8\u2217) if and only if\n\u3008u,\u03b8 \u2212 \u03b8\u2217\u3009 \u2264 0, \u2200\u2016\u03b8\u2016spk \u2264 \u2016\u03b8\u2217\u2016 sp k (79) \u21d2\u3008u,\u03b8\u3009 \u2264 \u3008u,\u03b8\u2217\u3009 \u2200\u2016\u03b8\u2016spk \u2264 \u2016\u03b8\u2217\u2016 sp k . (80)\nNow, we claim that \u3008u,\u03b8\u2217\u3009 \u2265 0 for all such u. This can be shown as follows. Assume the contrary, i.e. there exists a u\u0302 \u2208 NA(\u03b8\u2217) such that \u3008u\u0302,\u03b8\u2217\u3009 < 0. Now, noting that (\u2212\u03b8\u2217) \u2208 TA(\u03b8\u2217), we have\n\u3008u\u0302,\u2212\u03b8\u2217\u3009 = \u2212\u3008u\u0302,\u03b8\u2217\u3009 > 0 , (81)\nso that u\u0302 /\u2208 NA(\u03b8\u2217), which is a contradiction, and the claim follows. Therefore, we can write \u3008u,\u03b8\u2217\u3009 = t\u2016\u03b8\u2217\u2016spk (82) for some t \u2265 0. Then, u \u2208 NA(\u03b8\u2217) if and only if\n\u2203t \u2265 0 , \u3008u,\u03b8\u2217\u3009 = t\u2016\u03b8\u2217\u2016spk , \u3008u,\u03b8\u3009 \u2264 t\u2016\u03b8\u2217\u2016 sp k \u2200\u2016\u03b8\u2016 sp k \u2264 \u2016\u03b8\u2217\u2016 sp k . (83)\nSince \u3008u,\u03b8\u3009 \u2264 t\u2016\u03b8\u2217\u2016spk , \u2200\u2016\u03b8\u2016 sp k \u2264 \u2016\u03b8\u2217\u2016 sp k \u21d2 \u2016u\u2016 sp\u2217\nk \u2264 t , (84) the statement follows.\nThe k-support norm can be thought of as a group sparse norm with overlaps, such as been dealt with in [13]. Therefore, we can utilize some of the analysis techniques developed in [13], specialized to the\nstructure of the k-support norm. We begin by stating a theorem which enables us to bound the Gaussian width of the error set. Henceforth, we write NA = NA(\u03b8\u2217) and TA = TA(\u03b8\u2217) where the dependence on \u03b8\u2217 is understood.\nFirst, we define sets that involve the support set of \u03b8\u2217. Let us define the set G\u2217 \u2286 G(k) to be the set of all groups in G(k) which overlap with the support of \u03b8\u2217, i.e.\nG\u2217 = {G \u2208 G(k) : G \u2229 supp(\u03b8\u2217) 6= \u2205} . (85) Let S be the union of all groups in G\u2217, i.e. S = \u22c3G\u2208G\u2217 G, and the size of S be |S| = s. We are going to use three lemmas in order to prove the above bound. The first lemma, proved in [4], upper bounds the Gaussian width by an expected distance to the normal cone as follows.\nLemma 2 ([4] Proposition 3.6) Let C be any nonempty convex in Rp, and g \u223c N (0, Ip) be a random gaussian vector. Then \u03c9(C \u2229 Sp\u22121) \u2264 Eg[dist(g,C\u2217)] , (86) where C\u2217 is the polar cone of C.\nNote that NA is the polar cone of TA by definition. Therefore, using Jensen\u2019s inequality, we obtain \u03c9(TA \u2229 Sp\u22121)2 \u2264 E2g[dist(g,NA)] \u2264 Eg[dist(g,NA)2] \u2264 Eg[\u2016g \u2212 z(g)\u201622] , (87)\nwhere z(g) \u2208 NA is a (random) vector constructed to lie always in the normal cone. The construction proceeds as follows. Constructing z(g): Note that \u03b8\u2217Sc = 0. Let us choose a vector v \u2208 NA such that \u2016v\u2016sp\u2217k = 1 and vSc = 0 . (88) We can choose an appropriately scaled v so that\n\u3008v,\u03b8\u2217\u3009 = \u2016\u03b8\u2217\u2016spk , (89) and let us write without loss of generality v = [vS vSc ].\nNext, let g \u223c N (0, Ip), and write g = [gS gSc ]. We define the quantity\nt(g) = max { \u2016gG\u20162 : G \u2208 G(k), G \u2286 Sc } = max  (\u2211 i\u2208G g2i ) 1 2 : G \u2208 G(k), G \u2286 Sc  , (90)\nand let z = z(g) = [zS zSc ] such that\nzS = t(g)vS , zSc = gSc . (91)\nNote that \u3008z,\u03b8\u2217\u3009 = t(g)\u3008vS ,\u03b8\u2217S\u3009 = t(g)\u2016\u03b8\u2217\u2016spk , (92)\nand \u2016z\u2016sp\u2217k = max { \u2016zG\u20162 : G \u2208 G(k) } (93)\n= max { max{\u2016zG\u20162 : G \u2208 G(k), G \u2286 S} , max{\u2016zG\u20162 : G \u2208 G(k), G \u2286 Sc} }\n(94)\n(a) = max { t(g)\u2016v\u2016sp\u2217k , t(g) } (95)\n= t(g) (96)\nwhere (a) follows from the definition of t(g) and the fact that\nmax{\u2016zG\u20162 : G \u2208 G(k), G \u2286 S} = t(g) max{\u2016vG\u20162 : G \u2208 G(k), G \u2286 S} = t(g)\u2016v\u2016sp \u2217 k , (97)\nand since \u2016v\u2016sp\u2217k = 1. Therefore, z(g) \u2208 NA(\u03b8\u2217) by definition in (77) . In order to upper bound the expectation of t(g), we use the following comparison inequality from [13].\nLemma 3 ([13] Lemma 3.2) Let q1, q2, . . . , qL be L, \u03c7-squared random variables with d degrees of freedom. Then\nE [ max\n1\u2264i\u2264L qi\n] \u2264 (\u221a 2 logL+ \u221a d )2 . (98)\nLast, we prove an upper bound on the expected value of t(g), as shown in the following lemma.\nLemma 4 Consider G\u2217 \u2286 G(k) to be the set of groups intersecting with the support of \u03b8\u2217, and let S be the union of groups in G\u2217, such that s = |S|. Then,\nEg[t(g) 2] \u2264 (\u221a 2k log ( p\u2212 k \u2212 \u2308 s k \u2309 + 2 ) + \u221a k )2 . (99)\nProof: Note that\nEg[t(g) 2] = Eg [( max { \u2016gG\u20162 : G \u2208 G(k), G \u2286 Sc })2] (100)\n\u2264 Eg [ max { \u2016gG\u201622 : G \u2208 G(k), G \u2286 Sc }] (101)\nEach term \u2016gG\u201622 is a \u03c7-squared variable with at most k degrees of freedom. Since the set S has size s, the set G\u2217 has to contain at least sk = \u2308 s k \u2309 groups of size k. Therefore,\ns = |S| \u2265 k + (sk \u2212 1) , (102)\nand therefore the size of its complement is upper bounded by\n|Sc| \u2264 p\u2212 k \u2212 sk + 1 . (103)\nTherefore the following inequality provides an upper bound on the number of groups involved in computing the maximum in (101)\u2223\u2223\u2223{G \u2208 G(k), G \u2286 Sc}\u2223\u2223\u2223 \u2264 (p\u2212 k \u2212 sk + 1\nk\n) + ( p\u2212 k \u2212 sk + 1\nk \u2212 1\n) + \u00b7 \u00b7 \u00b7+ ( p\u2212 k \u2212 sk + 1\n1\n) (104)\n\u2264 (p\u2212 k \u2212 sk + 2)k (105)\nwhere we have used the following inequality( n\nh\n) \u2264 n h\nh! , \u2200n \u2265 h \u2265 0 , (106)\nwhich also provides k\u2211\nh=1\n( n\nh\n) \u2264 (n+ 1)k . (107)\nTherefore, we can upper bound (101) using Lemma 3 as\nEg[t(g) 2] \u2264 Eg [ max { \u2016gG\u201622 : G \u2208 G(k), G \u2286 Sc }] (108)\n\u2264 (\u221a 2 log ( (p\u2212 k \u2212 \u2308 s k \u2309 + 2)k ) + \u221a k )2 (109)\nand the statement follows.\nNow we are ready to prove the upper bound on the Gaussian width. First, note that\n\u03c9(TA(\u03b8\u2217) \u2229 Sp\u22121)2 \u2264 Eg[dist(g,NA(\u03b8\u2217))2] (110) (a) \u2264 Eg[\u2016g \u2212 z(g)\u201622] (111) = Ew[\u2016zS \u2212 gS\u201622] (112)\n(b) = E[\u2016zS\u201622] + E[\u2016gS\u201622] (113) (c) = E[t(g) 2] \u00b7 \u2016vS\u201622 + |S| (114)\n(d) \u2264\n(\u221a 2k log ( (p\u2212 k \u2212 \u2308 s k \u2309 + 2) ) + \u221a k )2 \u00b7 \u2308 s k \u2309 + s , (115)\nwhere (a) follows from the definition of distance to a set, (b) follows from the independence of gS and gSc , (c) follows from the fact that the expected length of an |S| length random i.i.d. Gaussian vector is \u221a |S|, and\n(d) follows since |S| = ksk , and that \u2016vS\u20162 \u2264 \u221a\u2308 s k \u2309 \u2016vS\u2016sp \u2217 k = \u221a\u2308 s k \u2309 . Thus inequality (68) follows.\nNext, we prove inequality (66). Let us denote t = XT (\nw \u2016w\u20162\n) , and note that t \u223c N (0, Ip). Also note\nthat E [ R\u2217(XTw) ] = E[\u2016w\u20162\u2016]E[R\u2217(t)], and\n\u2016t\u2016sp\u2217k = max{\u2016tG\u20162 : G \u2208 G(k)} . (116)\nTherefore, we can use Lemma 3 in order to bound the expectation E[\u2016t\u2016sp\u2217k ] as\nE[\u2016t\u2016sp\u2217k ] = E[max{\u2016tG\u20162 : G \u2208 G(k)}] (117) = E[max{\u2016tG\u20162 : G \u2208 G(k), |G| = k} (118)\n\u2264 (\u221a 2 log ( p\nk\n) + \u221a k ) (119)\n\u2264 (\u221a 2k log (pe k ) + \u221a k ) , (120)\nwhere we have used the following inequality obtained using Stirling\u2019s approximation( p\nk ) \u2264 (pe k )k . (121)\nTherefore, inequality (66) follows, and by our choice of \u03bbp, with high probability, \u03b8\u2217 lies in the feasible set. Last, note that\n\u03c9(\u2126R) = E[\u2016t\u2016sp \u2217 k ] \u2264 (\u221a 2k log (pe k ) + \u221a k ) , (122)\nas proved above."}], "references": [{"title": "Sparse prediction with the k-support norm", "author": ["Andreas Argyriou", "Rina Foygel", "Nathan Srebro"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Simultaneous analysis of lasso and dantzig selector", "author": ["Peter J Bickel", "Ya\u2019acov Ritov", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "The Dantzig selector: Statistical estimation when p is much larger than n", "author": ["Emmanuel Candes", "Terence Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "The convex geometry of linear inverse problems", "author": ["Venkat Chandrasekaran", "Benjamin Recht", "Pablo A Parrilo", "Alan S Willsky"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A generalized dantzig selector with shrinkage", "author": ["Gareth M. James", "Peter Radchenko"], "venue": "tuning. Biometrika,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Dasso: connections between the dantzig selector and lasso", "author": ["Gareth M. James", "Peter Radchenko", "Jinchi Lv"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Probability in Banach Spaces: isoperimetry and processes, volume 23", "author": ["Michel Ledoux", "Michel Talagrand"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "The group dantzig selector", "author": ["Han Liu", "Jian Zhang", "Xiaoye Jiang", "Jun Liu"], "venue": "JMLR Proceedings,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "An alternating direction method for finding dantzig selectors", "author": ["Zhaosong Lu", "Ting Kei Pong", "Yong Zhang"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "New Perspectives on k-Support and Cluster Norms", "author": ["A.M. McDonald", "M. Pontil", "D. Stamos"], "venue": "ArXiv e-prints,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers", "author": ["Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu"], "venue": "Statistical Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Universal measurement bounds for structured sparse signal recovery", "author": ["Nikhil S Rao", "Ben Recht", "Robert D Nowak"], "venue": "In AISTATS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "On sparse reconstruction from fourier and gaussian measurements", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Galois Theory, Third Edition", "author": ["I. Stewart"], "venue": "Chapman Hall/CRC Mathematics Series. Taylor & Francis,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Robert Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Bregman Alternating Direction Method of Multipliers", "author": ["H. Wang", "A. Banerjee"], "venue": "ArXiv e-prints,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "The linearized alternating direction method of multipliers for dantzig selector", "author": ["Xiangfeng Wang", "Xiaoming Yuan"], "venue": "SIAM J. Scientific Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "On model selection consistency of lasso", "author": ["Peng Zhao", "Bin Yu"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.", "startOffset": 41, "endOffset": 47}, {"referenceID": 2, "context": "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.", "startOffset": 41, "endOffset": 47}, {"referenceID": 14, "context": "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.", "startOffset": 123, "endOffset": 131}, {"referenceID": 17, "context": "1 Introduction The Dantzig Selector (DS) [2, 3] provides an alternative to regularized regression approaches such as Lasso [16, 19] for sparse estimation.", "startOffset": 123, "endOffset": 131}, {"referenceID": 1, "context": "While DS does not consider a regularized maximum likelihood approach, [2] has established clear similarities between the estimates from DS and Lasso.", "startOffset": 70, "endOffset": 73}, {"referenceID": 10, "context": "While norm regularized regression approaches have been generalized to more general norms, such as decomposable norms [11], the literature on DS has primarily focused on the sparse L1 norm case, with a few notable exceptions which have considered extensions to sparse group-structured norms [8].", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "While norm regularized regression approaches have been generalized to more general norms, such as decomposable norms [11], the literature on DS has primarily focused on the sparse L1 norm case, with a few notable exceptions which have considered extensions to sparse group-structured norms [8].", "startOffset": 290, "endOffset": 293}, {"referenceID": 2, "context": "If R(\u00b7) is the L1 norm, (1) reduces to standard DS [3].", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "It is instructive to contrast GDS with the recently proposed atomic norm based estimation framework [4] which, unlike GDS, considers constraints based on the L2 norm of the error \u2016y \u2212X\u03b8\u20162, and focuses only on atomic norms.", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "For the L1-norm Dantzig selector, [3] proposed a primal-dual interior point method since the optimization is a linear program.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "DASSO and its generalization proposed in [6, 5] focused on homotopy methods, which provide a piecewise linear solution path through a sequential simplex-like algorithm.", "startOffset": 41, "endOffset": 47}, {"referenceID": 4, "context": "DASSO and its generalization proposed in [6, 5] focused on homotopy methods, which provide a piecewise linear solution path through a sequential simplex-like algorithm.", "startOffset": 41, "endOffset": 47}, {"referenceID": 8, "context": "In recent work, the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1 Dantzig selection problem [9, 18], and the linearized version in [18] proved to be efficient.", "startOffset": 124, "endOffset": 131}, {"referenceID": 16, "context": "In recent work, the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1 Dantzig selection problem [9, 18], and the linearized version in [18] proved to be efficient.", "startOffset": 124, "endOffset": 131}, {"referenceID": 16, "context": "In recent work, the Alternating Direction Method of Multipliers (ADMM) has been applied to the L1 Dantzig selection problem [9, 18], and the linearized version in [18] proved to be efficient.", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "Motivated by such results for DS, we propose a general inexact ADMM [17] framework for GDS where the primal update steps, interestingly, turn out respectively to be proximal updates involving R(\u03b8) and its convex conjugate, the indicator of R\u2217(x) \u2264 \u03bb.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "Interestingly, the bound depends on the Gaussian width of the unit norm ball ofR(\u00b7) as well as the Gaussian width of suitable set where the estimation error belongs [4, 13].", "startOffset": 165, "endOffset": 172}, {"referenceID": 11, "context": "Interestingly, the bound depends on the Gaussian width of the unit norm ball ofR(\u00b7) as well as the Gaussian width of suitable set where the estimation error belongs [4, 13].", "startOffset": 165, "endOffset": 172}, {"referenceID": 0, "context": "As a non-trivial example of the GDS framework, we consider estimation using the recently proposed k-support norm [1, 10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 9, "context": "As a non-trivial example of the GDS framework, we consider estimation using the recently proposed k-support norm [1, 10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 0, "context": "Note that existing work [1, 10] on k-support norm has focused on the proximal operator for the square of the k-support norm, which is not directly applicable in our setting.", "startOffset": 24, "endOffset": 31}, {"referenceID": 9, "context": "Note that existing work [1, 10] on k-support norm has focused on the proximal operator for the square of the k-support norm, which is not directly applicable in our setting.", "startOffset": 24, "endOffset": 31}, {"referenceID": 16, "context": "Inspired by [18], we consider a simpler subproblem for the \u03b8-update which minimizes L\u0303R(\u03b8,v, z) = R(\u03b8) + \u3008z,A\u03b8 + v \u2212 u\u3009+ \u03c1 2 (\u2225\u2225A\u03b8k + v \u2212 u\u2225\u22252 2 +", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Remark on convergence: Note that Algorithm 1 is a special case of inexact Bregman ADMM proposed in [17], which matches the case of linearizing quadratic penalty term by using B\u03c6\u03b8(\u03b8,\u03b8k) = 1 2\u2016\u03b8 \u2212 \u03b8k\u20162 as Bregman divergence.", "startOffset": 99, "endOffset": 103}, {"referenceID": 15, "context": "In order to converge, the algorithm requires \u03bc2 to be larger than the spectral radius of ATA, and the convergence rate is O(1/T ) according to Theorem 2 in [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "For any set \u03a9 \u2286 Rp, we would measure the size of this set using its Gaussian width [14, 4], which is defined as \u03c9(\u03a9) = Eg [supz\u2208\u03a9\u3008g, z\u3009] , where g is a vector of i.", "startOffset": 83, "endOffset": 90}, {"referenceID": 3, "context": "For any set \u03a9 \u2286 Rp, we would measure the size of this set using its Gaussian width [14, 4], which is defined as \u03c9(\u03a9) = Eg [supz\u2208\u03a9\u3008g, z\u3009] , where g is a vector of i.", "startOffset": 83, "endOffset": 90}, {"referenceID": 16, "context": "Similar updates were used in [18] for L1-norm Dantzig selector.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": "It was shown in [4] that the Gaussian width of the set (TL1(\u03b8) \u2229 Sp\u22121) is upper bounded as \u03c9(TL1(\u03b8)\u2229 Sp\u22121)2 \u2264 2s log (p s ) + 54s.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "standard Gaussian entries [3].", "startOffset": 26, "endOffset": 29}, {"referenceID": 10, "context": "Further, [11] has shown that \u03a8R = \u221a s.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "with high probability, which agrees with known results for DS [2, 3].", "startOffset": 62, "endOffset": 68}, {"referenceID": 2, "context": "with high probability, which agrees with known results for DS [2, 3].", "startOffset": 62, "endOffset": 68}, {"referenceID": 0, "context": "In previous work [1, 10], the k-support norm is defined as", "startOffset": 17, "endOffset": 24}, {"referenceID": 9, "context": "In previous work [1, 10], the k-support norm is defined as", "startOffset": 17, "endOffset": 24}, {"referenceID": 0, "context": "It was shown in [1] to behave similarly as the elastic net in the sense that the unit norm ball of the k-support norm is within a constant factor of \u221a 2 of the unit elastic net ball.", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "Existing methods [1, 10] are inapplicable to our scenario since they compute the proximal operator for squared k-support norm, from which IC\u03bb (\u00b7) cannot be directly obtained.", "startOffset": 17, "endOffset": 24}, {"referenceID": 9, "context": "Existing methods [1, 10] are inapplicable to our scenario since they compute the proximal operator for squared k-support norm, from which IC\u03bb (\u00b7) cannot be directly obtained.", "startOffset": 17, "endOffset": 24}, {"referenceID": 13, "context": "Remark: The nonlinear equation (24) is quartic, for which we can use general formula to get all the roots [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "Compared with previous proximal operators for squared k-support norm, this complexity is better than that in [1], and roughly the same as the most recent one in [10].", "startOffset": 109, "endOffset": 112}, {"referenceID": 9, "context": "Compared with previous proximal operators for squared k-support norm, this complexity is better than that in [1], and roughly the same as the most recent one in [10].", "startOffset": 161, "endOffset": 165}, {"referenceID": 11, "context": "We prove these two bounds using the analysis technique for group lasso with overlaps developed in [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "4 Experimental Results On optimization side, our ADMM framework is concentrated on its generality, and its efficiency has been shown in [18] for the special case of L1 norm.", "startOffset": 136, "endOffset": 140}, {"referenceID": 0, "context": "1 Efficiency of Proximal Operator We tested four proximal operators related to k-support norm, which are our normal IC\u03bb (\u00b7) and its accelerated version, prox 1 2\u03b2 (\u2016\u00b7\u2016 k )2 (\u00b7) in [1], and prox\u03bb 2 \u2016\u00b7\u2016\u0398 (\u00b7) in [10].", "startOffset": 180, "endOffset": 183}, {"referenceID": 9, "context": "1 Efficiency of Proximal Operator We tested four proximal operators related to k-support norm, which are our normal IC\u03bb (\u00b7) and its accelerated version, prox 1 2\u03b2 (\u2016\u00b7\u2016 k )2 (\u00b7) in [1], and prox\u03bb 2 \u2016\u00b7\u2016\u0398 (\u00b7) in [10].", "startOffset": 209, "endOffset": 213}], "year": 2015, "abstractText": "We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS, and non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian width of unit norm ball and suitable set encompassing estimation error. Further, we consider a non-trivial example of the GDS using k-support norm. We derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the k-support norm. The experimental results confirm our theoretical analysis.", "creator": "LaTeX with hyperref package"}}}