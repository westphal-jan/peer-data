{"id": "1406.7443", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2014", "title": "Efficient Learning in Large-Scale Combinatorial Semi-Bandits", "abstract": "in this paper, we consider efficient fuzzy learning in large - scale combinatorial semi - bandits with linear optimization generalization, variance and as a solution, propose a novel learning algorithm called randomized combinatorial maximization ( rcm ). rcm is motivated by thompson sampling, and is computationally critically efficient even as long as the offline version of the combinatorial problem can be solved efficiently. we establish that any rcm is provably statistically technically efficient in the coherent gaussian case, except by developing a bayes regret bound that variance is independent of the problem scale ( number of hidden items ) and sublinear in time. we also evaluate rcm on a variety of real - analysis world problems running with thousands each of items. our presented experimental results effectively demonstrate that rcm learns two orders of magnitude faster than the best baseline.", "histories": [["v1", "Sat, 28 Jun 2014 21:50:56 GMT  (51kb,D)", "https://arxiv.org/abs/1406.7443v1", null], ["v2", "Tue, 14 Apr 2015 06:38:56 GMT  (163kb,D)", "http://arxiv.org/abs/1406.7443v2", null], ["v3", "Mon, 8 Jun 2015 06:35:54 GMT  (130kb,D)", "http://arxiv.org/abs/1406.7443v3", null], ["v4", "Tue, 31 Jan 2017 05:32:13 GMT  (152kb,D)", "http://arxiv.org/abs/1406.7443v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["zheng wen", "branislav kveton", "azin ashkan"], "accepted": true, "id": "1406.7443"}, "pdf": {"name": "1406.7443.pdf", "metadata": {"source": "META", "title": "Efficient Learning in Large-Scale Combinatorial Semi-Bandits", "authors": ["Zheng Wen", "Branislav Kveton", "Azin Ashkan"], "emails": ["ZHENGWEN@YAHOO-INC.COM", "KVETON@ADOBE.COM", "AZIN.ASHKAN@TECHNICOLOR.COM"], "sections": [{"heading": "1. Introduction", "text": "Combinatorial optimization is a mature field (Papadimitriou & Steiglitz, 1998), which has countless practical applications. One of the most studied problems in combinatorial optimization is maximization of a modular function\nsubject to combinatorial constraints. Many important problems, such as minimum spanning tree (MST), shortest path, and maximum-weight bipartite matching, can be viewed as instances of this problem.\nIn practice, the optimized modular function is often unknown and needs to be learned while repeatedly solving the problem. This class of learning problems was recently formulated as a combinatorial bandit/semi-bandit, depending on the feedback model (Audibert et al., 2014). Since then, many combinatorial bandit/semi-bandit algorithms have been proposed: for the stochastic setting (Gai et al., 2012; Chen et al., 2013; Russo & Van Roy, 2014; Kveton et al., 2015b); for the adversarial setting (Cesa-Bianchi & Lugosi, 2012; Audibert et al., 2014; Neu & Barto\u0301k, 2013); and for subclasses of combinatorial problems, matroid and polymatroid bandits (Kveton et al., 2014a;b), submodular maximization (Wen et al., 2013; Gabillon et al., 2013), and cascading bandits (Kveton et al., 2015a). Many regret bounds have been established for the combinatorial semibandit algorithms. To achieve an O( \u221a n) dependence on time n, all of the regret bounds are \u2126( \u221a L), where L is the number of items. The dependence on L is intrinsic because the algorithms estimate the weight of each item separately, and matching lower bounds have been established (Section 3.2).\nHowever, in many real-world problems, the number of items L is intractably large. For instance, online advertising in a mainstream commercial website can be viewed as a bipartite matching problem with millions of users and products; routing in the Internet can be formulated as a shortest path problem with billions of edges. Thus, learning algorithms with \u2126( \u221a L) regret are impractical in such problems. On the other hand, in many problems, items have features and their weights are similar when the features are similar. In movie recommendation, for instance, the expected ratings of movies that are close in the latent space are also similar. In this work, we show how to leverage this struc-\nar X\niv :1\n40 6.\n74 43\nv4 [\ncs .L\nG ]\n3 1\nJa n\nture to learn to make good decisions more efficiently. More specifically, we assume a linear generalization across the items: conditioned on the features of an item, the expected weight of that item can be estimated using a linear model. Our goal is to develop more efficient learning algorithms for combinatorial semi-bandits with linear generalization.\nIt is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al. (2008); AbbasiYadkori et al. (2011)) , to combinatorial semi-bandits with linear generalization. In this paper, we propose two learning algorithms, Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB), based on Thompson sampling and LinUCB. Both CombLinTS and CombLinUCB are computationally efficient, as long as the offline version of the combinatorial problem can be solved efficiently. The first major contribution of the paper is that we establish a Bayes regret bound on CombLinTS and a regret bound on CombLinUCB, under reasonable assumptions. Both bounds are L-independent, and sublinear in time. The second major contribution of the paper is that we evaluate CombLinTS on a variety of problems with thousands of items, and two of these problems are based on real-world datasets. We only evaluate CombLinTS since recent literature (Chapelle & Li, 2011) suggests that Thompson sampling algorithms usually outperform UCB-like algorithms in practice. Our experimental results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines. It is worth mentioning that our derived L-independent regret bounds also hold in cases with L = \u221e. Moreover, as we will discuss in Section 7, our proposed algorithms and their analyses can be easily extended to the contextual combinatorial semibandits.\nFinally, we briefly review some relevant papers. Gabillon et al. (2014) and Yue & Guestrin (2011) focus on submodular maximization with linear generalization. Our paper differs from these two papers in the following two aspects: (1) our paper allows general combinatorial constraints while they do not; (2) our paper focuses on maximization of modular functions while they focus on submodular maximization."}, {"heading": "2. Combinatorial Optimization", "text": "We focus on a class of combinatorial optimization problems that aim to find a maximum-weight set from a given family of sets. Specifically, one such combinatorial optimization problem can be represented as a triple (E,A,w), where (1) E = {1, . . . , L} is a set of L items, called the ground set, (2) A \u2286 {A \u2286 E : |A| \u2264 K} is a family of\nsubsets of E with up to K items, where K \u2264 L, and (3) w : E \u2192 R is a weight function that assigns each item e in the ground set E a real number. The total weight of all items in a set A \u2286 E is defined as:\nf(A,w) = \u2211 e\u2208Aw(e), (1)\nwhich is a linear functional of w and a modular function in A. A set Aopt is a maximum-weight set in A if:\nAopt \u2208 arg max A\u2208A f(A,w) = arg max A\u2208A\n\u2211 e\u2208Aw(e). (2)\nMany classical combinatorial optimization problems, such as finding an MST, bipartite matching, the shortest path problem and the traveling salesman problem (TSP), have form (2). Though some of these problems can be solved efficiently (e.g. bipartite matching), others (e.g. TSP) are known to be NP-hard. However, for many such NPhard problems, there exist computationally efficient approximation algorithms and/or randomized algorithms that achieve near-optimal solutions with high probability. Similarly to Chen et al. (2013), in this paper, we allow the agent to use any approximation / randomized algorithm ORACLE to solve (2), and denote its solution as A\u2217 = ORACLE(E,A,w). To distinguish from a learning algorithm, we refer to a combinatorial optimization algorithm as an oracle in this paper."}, {"heading": "3. Combinatorial Semi-Bandits with Linear Generalization", "text": "Many real-world problems are combinatorial in nature. In recommender systems, for instance, the user is typically recommendedK items out of L. The value of an item, such as the expected rating of a movie, is never known perfectly and has to be refined while repeatedly recommending to the pool of the users. Recommender problems are known to be highly structured. In particular, it is well known that the user-item matrix is typically low-rank (Koren et al., 2009) and that the value of an item can be written as a linear combination of its position in the latent space. In this work, we propose a learning algorithm for combinatorial optimization that leverages this structure. In particular, we assume that the weight of each item is a linear function of its features and then we learn the parameters of this model, jointly for all items."}, {"heading": "3.1. Combinatorial Semi-Bandits", "text": "We formalize our learning problem as a combinatorial semi-bandit. A combinatorial semi-bandit is a triple (E,A, P ), where E and A are defined in Section 2 and P is a probability distribution over the weights w \u2208 RL of the items in the ground set E. We assume that the weights w are drawn i.i.d. from P . The mean weight is denoted by\nw\u0304 = E[w]. Each item e is associated with an arm and we assume that multiple arms can be pulled. A subset of arms A \u2286 E can be pulled if and only if A \u2208 A. The return of pulling arms A is f(A,w) (Equation (1)), the sum of the weights of all items in A. After the arms A are pulled, we observe the individual return of each arm, {w(e) : e \u2208 A}. This feedback model is known as semi-bandit (Audibert et al., 2014).\nWe assume that the combinatorial structure (E,A) is known and the distribution P is unknown. We would like to stress that we do not make any structural assumptions on P . The optimal solution to our problem is a maximumweight set in expectation:\nAopt \u2208 arg max A\u2208A Ew[f(A,w)] = arg max A\u2208A \u2211 e\u2208A w\u0304(e). (3)\nThis objective is equivalent to the one in Equation (2).\nOur learning problem is episodic. In each episode t, the learning agent adaptively chooses At \u2208 A based on its observations of the weights up to episode t, gains f(At,wt), and observes the weights of all chosen items in episode t, {(e,wt(e)) : e \u2208 At}. The learning agent interacts with the combinatorial semi-bandit for n times and its goal is to maximize the expected cumulative return in n-episodes E[ \u2211n t=1 f(A\nt,wt)], where the expectation is over (1) the random weights wt\u2019s, (2) possible randomization in the learning algorithm, and (3) w\u0304 if it is randomly generated. Notice that the choice ofAt impacts both the return and observations in episode t. So we need to trade off exploration and exploitation, similarly to other bandit problems."}, {"heading": "3.2. Linear Generalization", "text": "As we have discussed in Section 1, many provably efficient algorithms have been developed for various combinatorial semi-bandits of form (3) (Chen et al., 2013; Gai et al., 2012; Kveton et al., 2014a; Russo & Van Roy, 2014). However, since there areL parameters to learn and these algorithms do not consider generalization across items, the derived upper bounds on the expected cumulative regret and/or the Bayes cumulative regret of these algorithms are at least O( \u221a L). Furthermore, Audibert et al. (2014) has derived an \u2126( \u221a LKn) lower bound on adversarial combinatorial semi-bandits, while Kveton et al. (2014a) has derived an asymptotic \u2126(L log(n)/\u2206) gap-dependent lower bound on stochastic combinatorial semi-bandits, where \u2206 is the \u201cgap\u201d.\nHowever, in many modern combinatorial semi-bandit problems, L tends to be enormous. Thus, an O( \u221a L) regret is unacceptably large in these problems. On the other hand, in many practical problems, there exists a generalization model based on which the weight of one item can be (approximately) inferred based on the weights of\nother items. By exploiting such generalization models, an o( \u221a L) or even an L-independent cumulative regret might be achieved.\nIn this paper, we assume that there is a (possibly imperfect) linear generalization model across the items. Specifically, we assume that the agent knows a generalization matrix \u03a6 \u2208 RL\u00d7d s.t. w\u0304 either lies in or is \u201cclose\u201d to the subspace span [\u03a6]. We use \u03c6e to denote the transpose of the e-th row of \u03a6, and refer to it as the feature vector of item e. Without loss of generality, we assume that rank [\u03a6] = d.\nSimilar to Wen & Van Roy (2013), we distinguish between the coherent learning cases, in which w\u0304 \u2208 span [\u03a6], and the agnostic learning cases, in which w\u0304 /\u2208 span [\u03a6]. Like existing literature on linear bandits (Dani et al., 2008; Abbasi-Yadkori et al., 2011), the analysis in this paper focuses on coherent learning cases. However, we would like to emphasize that both of our proposed algorithms, CombLinTS and CombLinUCB, are also applicable to the agnostic learning cases. As is demonstrated in Section 6, CombLinTS performs well in the agnostic learning cases.\nFinally, we define \u03b8\u2217 = arg min \u03b8 \u2016w\u0304 \u2212 \u03a6\u03b8\u20162. Since rank [\u03a6] = d, \u03b8\u2217 is uniquely defined. Moreover, in coherent learning cases, we have w\u0304 = \u03a6\u03b8\u2217."}, {"heading": "3.3. Performance Metrics", "text": "Let A\u2217 = ORACLE(E,A, w\u0304). In this paper, we measure the performance loss of a learning algorithm with respect to A\u2217. Recall that the learning algorithm chooses At in episode t, we define Rt = f(A\u2217,wt) \u2212 f(At,wt) as the realized regret in episode t. If the expected weight w\u0304 is fixed but unknown, we define the expected cumulative regret of the learning algorithm in n episodes as\nR(n) = \u2211n t=1 E [Rt|w\u0304] , (4)\nwhere the expectation is over random weights and possible randomization in the learning algorithm. If necessary, we denote R(n) as R(n; w\u0304) to emphasize the dependence on w\u0304. On the other hand, if w\u0304 is randomly generated or the agent has a prior belief in w\u0304, then from Russo & Van Roy (2013), the Bayes cumulative regret of the learning algorithm in n episodes is defined as\nRBayes(n) = Ew\u0304[R(n; w\u0304)] = \u2211n t=1 E [Rt] , (5)\nwhere the expectation is also over w\u0304. That is, RBayes(n) is a weighted average of R(n; w\u0304) under the prior on w\u0304."}, {"heading": "4. Learning Algorithms", "text": "In this section, we propose two learning algorithms for combinatorial semi-bandits: Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear\nAlgorithm 2 Combinatorial Linear Thompson Sampling Input: Combinatorial structure (E,A), generalization matrix \u03a6 \u2208 RL\u00d7d, algorithm parameters \u03bb, \u03c3 > 0, oracle ORACLE\nInitialize \u03a31 \u2190 \u03bb2I \u2208 Rd\u00d7d and \u03b8\u03041 = 0 \u2208 Rd for all t = 1, 2, . . . , n do\nSample \u03b8t \u223c N ( \u03b8\u0304t,\u03a3t ) Compute At \u2190 ORACLE(E,A,\u03a6\u03b8t) Choose set At, and observe wt(e), \u2200e \u2208 At Compute \u03b8\u0304t+1 and \u03a3t+1 based on Algorithm 1\nend for\nUCB (CombLinUCB), which are respectively motivated by Thompson sampling and LinUCB. Both algorithms maintain a mean vector \u03b8\u0304t and a covariance matrix \u03a3t, and use Kalman filtering to update \u03b8\u0304t and \u03a3t. They differ in how to choose At (i.e. how to explore) in each episode t: CombLinTS chooses At based on a randomly sampled coefficient vector \u03b8t, while CombLinUCB choosesAt based on the optimism in the face of uncertainty (OFU) principle."}, {"heading": "4.1. Combinatorial Linear Thompson Sampling", "text": "The psuedocode of CombLinTS is given in Algorithm 2, where (E,A) is the combinatorial structure, \u03a6 is the generalization matrix, ORACLE is a combinatorial optimization algorithm, and \u03bb and \u03c3 are two algorithm parameters controlling the learning rate. Specifically, \u03bb is an inverseregularization parameter and smaller \u03bb makes the covariance matrix \u03a3t closer to 0. Thus, a too small \u03bb will lead to insufficient exploration and significantly reduce the performance of CombLinTS. On the other hand, \u03c3 controls the decrease rate of the covariance matrix \u03a3t. In particular, a large \u03c3 will lead to slow learning, while a too small \u03c3 will make the algorithm quickly converge to some sub-optimal coefficient vector.\nIn each episode t, Algorithm 2 consists of three steps. First, it randomly samples a coefficient vector \u03b8t from a Gaussian distribution. Second, it computes At based on \u03b8t and the pre-specified oracle. Finally, it updates the mean vector \u03b8\u0304t+1 and the covariance matrix \u03a3t+1 based on Kalman filtering (Algorithm 1).\nIt is worth pointing our that if (1) w\u0304 = \u03a6\u03b8\u2217, (2) the prior on \u03b8\u2217 is N(0, \u03bb2I), and (3) \u2200(t, e), the noise \u03b7t(e) = wt(e) \u2212 w\u0304(e) is independently sampled from N(0, \u03c32), then in each episode t, the CombLinTS algorithm samples \u03b8t from the posterior distribution of \u03b8\u2217. We henceforth refer to a case satisfying condition (1)-(3) as a coherent Gaussian case. Obviously, the CombLinTS algorithm can be applied to more general cases, even to cases with no prior and/or agnostic learning cases.\nAlgorithm 3 Combinatorial Linear UCB Input: Combinatorial structure (E,A), generalization matrix \u03a6 \u2208 RL\u00d7d, algorithm parameters \u03bb, \u03c3, c > 0, oracle ORACLE\nInitialize \u03a31 \u2190 \u03bb2I \u2208 Rd\u00d7d and \u03b8\u03041 = 0 \u2208 Rd for all t = 1, 2, . . . , n do\nDefine the UCB weight vector w\u0302t as\nw\u0302t(e) = \u2329 \u03c6e, \u03b8\u0304t \u232a + c \u221a \u03c6Te \u03a3t\u03c6e \u2200e \u2208 E\nCompute At \u2190 ORACLE(E,A, w\u0302t) Choose set At, and observe wt(e), \u2200e \u2208 At Compute \u03b8\u0304t+1 and \u03a3t+1 based on Algorithm 1\nend for"}, {"heading": "4.2. Combinatorial Linear UCB", "text": "The pseudocode of CombLinUCB is given in Algorithm 3, where E, A, \u03a6 and ORACLE are defined the same as in Algorithm 2, and \u03bb, \u03c3, and c are three algorithm parameters. Similarly, \u03bb is an inverse-regularization parameter, \u03c3 controls the decrease rate of the covariance matrix, and c controls the degree of optimism (exploration). Specifically, if c is too small, the algorithm might converge to some suboptimal coefficient vector due to insufficient exploration; on the other hand, too large c will lead to excessive exploration and slow learning.\nIn each episode t, Algorithm 3 also consists of three steps. First, for each e \u2208 E, it computes an upper confidence bound (UCB) w\u0302t(e). Second, it computes At based on w\u0302t and the pre-specified oracle. Finally, it updates \u03b8\u0304t+1 and \u03a3t+1 based on Kalman filtering (Algorithm 1)."}, {"heading": "5. Regret Bounds", "text": "In this section, we present a Bayes regret bound on CombLinTS, and a regret bound on CombLinUCB. We will also briefly discuss how these bounds are derived, as well as their tightness. The detailed proofs are left to the appendices. Without loss of generality, throughout this section, we assume that \u2016\u03c6e\u20162 \u2264 1, \u2200e \u2208 E.\n5.1. Bayes Regret Bound on CombLinTS\nWe have the following upper bound on RBayes(n) when CombLinTS is applied to a coherent Gaussian case with the right parameter.\nTheorem 1. If (1) w\u0304 = \u03a6\u03b8\u2217, (2) the prior on \u03b8\u2217 is N(0, \u03bb2I), (3) the noises are i.i.d. sampled from N(0, \u03c32), and (4) \u03bb \u2265 \u03c3, then under CombLinTS algorithm with pa-\nAlgorithm 1 Compute \u03b8\u0304t+1 and \u03a3t+1 based on Kalman Filtering Input: \u03b8\u0304t, \u03a3t, \u03c3, and feature-observation pairs {(\u03c6e,wt(e)) : e \u2208 At} Initialize \u03b8\u0304t+1 \u2190 \u03b8\u0304t and \u03a3t+1 \u2190 \u03a3t for k = 1, . . . , |At| do\nUpdate \u03b8\u0304t+1 and \u03a3t+1 as follows, where atk is the kth element in A t\n\u03b8\u0304t+1 \u2190 [ I \u2212 \u03a3t+1\u03c6atk\u03c6 T atk\n\u03c6T atk \u03a3t+1\u03c6atk + \u03c3 2\n] \u03b8\u0304t+1 + [ \u03a3t+1\u03c6atk\n\u03c6T atk \u03a3t+1\u03c6atk + \u03c3 2\n] wt ( atk )\n\u03a3t+1 \u2190\u03a3t+1 \u2212 \u03a3t+1\u03c6atk\u03c6 T atk \u03a3t+1\n\u03c6T atk \u03a3t+1\u03c6atk + \u03c3 2 , (6)\nend for Output: \u03b8\u0304t+1 and \u03a3t+1\nrameter (\u03a6, \u03bb, \u03c3), we have RBayes(n) \u2264 O\u0303 ( K\u03bb \u221a dnmin {ln(L), d} ) . (7)\nNotice that condition (1)-(3) ensure it is a coherent Gaussian case, and condition (4) almost always holds1. The O\u0303 notation hides the logarithm factors. We also note that Equation (7) is a minimum of two bounds. The first bound is L-dependent, but it is only O( \u221a ln(L)); on the other\nhand, the second bound is L-independent, but is O\u0303(d) instead of O\u0303( \u221a d).\nWe now outline the proof of Theorem 1, which is motivated by Russo & Van Roy (2013) and Dani et al. (2008). LetHt denote the \u201chistory\u201d (i.e. all the available information) by the start of episode t. Note that from the Bayesian perspective, conditioning on Ht, \u03b8\u2217 and \u03b8t are i.i.d. drawn from N(\u03b8\u0304t,\u03a3t) (Russo & Van Roy, 2013). This is because that conditioning on Ht, the posterior belief in \u03b8\u2217 is N(\u03b8\u0304t,\u03a3t) and based on Algorithm 2, \u03b8t is independently sampled from N(\u03b8\u0304t,\u03a3t). Since ORACLE is a fixed combinatorial optimization algorithm (even though it can be independently randomized), and E,A,\u03a6 are all fixed, then conditioning on Ht, A\u2217 and At are also i.i.d., furthermore, A\u2217 is conditionally independent of \u03b8t, and At is conditionally independent of \u03b8\u2217.\nTo simplify the exposition, \u2200\u03b8 \u2208 Rd and \u2200A \u2286 E, we define\ng(A, \u03b8) = \u2211 e\u2208A \u3008\u03c6e, \u03b8\u3009 ,\nwhere \u3008\u00b7, \u00b7\u3009 is an alternative notation for inner product. Thus we have E[Rt|Ht] = E[g(A\u2217, \u03b8\u2217)\u2212 g(At, \u03b8\u2217)|Ht]. We also define a UCB function Ut : 2E \u2192 R as\nUt(A) = \u2211 e\u2208A [\u2329 \u03c6e, \u03b8\u0304t \u232a + c \u221a \u03c6Te \u03a3t\u03c6e ] ,\n1Condition (4) is not essential, please refer to Theorem 3 in Appendix A for a Bayes regret bound without condition (4).\nwhere c > 0 is a constant to be specified. Notice that conditioning on Ht, Ut is a deterministic function and A\u2217, At are i.i.d., then E[Ut(At)\u2212 Ut(A\u2217)|Ht] = 0 and\nE[Rt|Ht] =E[g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)|Ht] +E [ Ut(A t)\u2212 g(At, \u03b8\u2217) \u2223\u2223Ht] . (8)\nTheorem 1 follows by respectively bounding the two terms on the righthand side of Equation (8). Two key observations are (1) if c = O\u0303 (\u221a min {ln(L), d} ) , then\nE[g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)|Ht] = O(1),\nand (2) E [ Ut(A t)\u2212 g(At, \u03b8\u2217) \u2223\u2223Ht] = cE[\u2211e\u2208At\u221a\u03c6Te \u03a3t\u03c6e\u2223\u2223\u2223Ht] ,\nand we have a worst-case bound (see Lemma 4 in Appendix A) on \u2211n t=1 \u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e. Please refer to Appendix A for the detailed proof for Theorem 1.\nFinally, we briefly discuss the tightness of our bound. Without loss of generality, we assume that \u03bb = 1. For the special case when \u03a6 = I (i.e. no generalization), Russo & Van Roy (2014) provides an O( \u221a LK log(L/K)n) upper bound on RBayes(n) when Thompson sampling is applied, and Audibert et al. (2014) provides an \u2126( \u221a LKn) lower bound2. Since L = d when \u03a6 = I , the above results indicate that for general \u03a6, the best upper bound one can hope is O( \u221a Kdn). Hence, our bound is at most\nO\u0303( \u221a K min{ln(L), d}) larger. It is well-known that the O( \u221a d) factor is due to linear generalization (Dani et al., 2008; Abbasi-Yadkori et al., 2011), and as is discussed in the appendix (see Remark 1), the extra O( \u221a K) factor is\n2Audibert et al. (2014) focuses on the adversarial setting but the lower bound is stochastic. So it is a reasonable lower bound to compare with.\nalso due to linear generalization. They might be intrinsic, but we leave the final word and tightness analysis to future work.\n5.2. Regret Bound on CombLinUCB\nUnder the assumptions that (1) the support of P is a subset of [0, 1]L, (2) the stochastic item weights {w(e)}e\u2208E are statistically independent under P , and (3) the oracle ORACLE exactly solves the offline optimization problem3, we have the following upper bound on R(n) when CombLinUCB is applied to coherent learning cases: Theorem 2. For any \u03bb, \u03c3 > 0, any \u03b4 \u2208 (0, 1), and any c satisfying\nc \u2265 1 \u03c3\n\u221a d ln ( 1 + nK\u03bb2\nd\u03c32\n) + 2 ln ( 1\n\u03b4 ) + \u2016\u03b8\u2217\u20162 \u03bb , (9)\nif w\u0304 = \u03a6\u03b8\u2217 and the above three assumptions hold, then under CombLinUCB algorithm with parameter (\u03a6, \u03bb, \u03c3, c), we have\nR(n) \u2264 2cK\u03bb\n\u221a dn ln ( 1 + nK\u03bb 2\nd\u03c32 ) ln ( 1 + \u03bb 2\n\u03c32 ) + nK\u03b4. Generally speaking, the proof for Theorem 2 proceeds as follows. We first construct a confidence set G of \u03b8\u2217 based on the \u201cself normalized bound\u201d developed in Abbasi-Yadkori et al. (2011). Then we decompose the regret over the high-probability \u201cgood\u201d event G and the low-probability \u201cbad\u201d event G\u0304, where G\u0304 is the complement of G. Finally, we bound the term associated with the event G based on the same worst-case bound on \u2211n t=1 \u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e used in the analysis for CombLinTS (see Lemma 4 in Appendix A), and bound the term associated with the event G\u0304 based on a naive bound. Please refer to Appendix B for the detailed proof of Theorem 2.\nNotice that if we choose \u03bb = \u03c3 = 1, \u03b4 = 1/(nK), and c as the lower bound specified in Inequality (9), then the regret bound derived in Theorem 2 is also O\u0303(Kd \u221a n). Compared with the lower bound derived in Audibert et al. (2014), this bound is at most O\u0303( \u221a Kd) larger. Similarly, the extra O( \u221a K) and O( \u221a d) factors are also due to linear generalization.\nFinally, we would like to clarify that the assumption that the support of P is bounded is not essential. By slightly modifying the analysis, we can achieve a similar highprobability bound on the realized cumulative regret as long as P is sub-Gaussian. We also want to point out that the Lindependent bounds derived in both Theorem 1 and 2 will still hold even if L =\u221e.\n3 If ORACLE is an approximation algorithm, a variant of Theorem 2 can be proved (see Appendix D)."}, {"heading": "6. Experiments", "text": "In this section, we evaluate CombLinTS on three problems. The first problem is synthetic, but the last two problems are constructed based on real-world datasets. As we have discussed in Section 1, we only evaluate CombLinTS since in practice Thompson sampling algorithms usually outperform the UCB-like algorithms. Our experiment results in the synthetic problem demonstrate that CombLinTS is both scalable and robust to the choice of algorithm parameters. They also suggest the Bayes regret bound derived in Theorem 1 is likely to be tight. On the other hand, our experiment results in the last two problems show the value of linear generalization in real-world settings: with domainspecific but imperfect linear generalization (i.e. agnostic learning), CombLinTS can significantly outperform stateof-the-art learning algorithms that do not exploit linear generalization, which serve as baselines in these two problems.\nIn all three problems, the oracle ORACLE exactly solves the offline combinatorial optimization problem. Moreover, in the two real-world problems, we demonstrate the experiment results using a new performance metric, the expected per-step return in n episodes, which is defined as\n1 n Ew1,...,wn [\n\u2211n t=1 f(A t,wt)|w\u0304] . (10)\nObviously, it is the expected cumulative return in n episodes divided by n. We demonstrate experiment results using expected cumulative return rather than R(n) since it is more illustrative."}, {"heading": "6.1. Longest Path", "text": "We first evaluate CombLinTS on a synthetic problem. Specifically, we experiment with a stochastic longest path problem on an (m+1)\u00d7(m+1) square grid4. The items in the ground set E are the edges in the grid, L = 2m(m+ 1) in total. The feasible set A are all paths in the grid from the upper left corner to the bottom right corner that follow the directions of the edges. The length of these paths is K = 2m. In this problem, we focus on coherent Gaussian cases and randomly sample the linear generalization matrix \u03a6 \u2208 RL\u00d7d to weaken the dependence on a particular choice of \u03a6.\nOur experiments are parameterized by a sextuple (m, d, \u03bbtrue, \u03c3true, \u03bb, \u03c3), where m, d, \u03bb, and \u03c3 are defined before and \u03bbtrue and \u03c3true are respectively the true standard deviations of \u03b8\u2217 and the observation noises. In each round of simulation, we first construct a problem instance as follows: (1) generate \u03a6 by sampling each component of \u03a6 i.i.d. from N(0, 1); (2) sample \u03b8\u2217 inde-\n4That is, each side has m edges and m + 1 nodes. Notice that the longest path problem and the shortest path problem are mathematically equivalent.\npendently from N(0, \u03bb2trueI) and set w\u0304 = \u03a6\u03b8 \u2217; and (3) \u2200(t, e), the observation noise \u03b7t(e) = wt(e)\u2212w\u0304(e) is i.i.d. sampled from N(0, \u03c32true). Then we apply CombLinTS with parameter (\u03bb, \u03c3) to the constructed instance for n episodes. Notice that in general (\u03bb, \u03c3) 6= (\u03bbtrue, \u03c3true). We average the experiment results over 200 simulations to estimate the Bayes cumulative regret RBayes(n).\nWe start with a \u201cdefault case\u201d with m = 30, d = 200, \u03bbtrue = \u03bb = 10 and \u03c3true = \u03c3 = 1. Notice in this case L = 1860 and |A| \u2248 1.18 \u00d7 1017. We choose n = 150 since in the default case, the Bayes per-episode regret of CombLinTS vanishes far before period 150. In the default case RBayes(150) \u2248 1.56 \u00d7 104. In the experiments, we vary only one and only one parameter while keeping all the other parameters fixed to their \u201cdefault values\u201d specified above to demonstrate the scalability and robustness of CombLinTS.\nFirst, we study how the Bayes cumulative regret of CombLinTS scales with the size of the problem by varying m = 10, 20, . . . , 100, and show the result in Figure 1(a). The experiment results show that RBayes(150) roughly increases linearly withm, which indicates that CombLinTS is scalable with respect to the problem size m. We also experiment with m = 250, in this case we have L \u2248 125k, |A| \u2248 1.17\u00d710149, andRBayes(150) \u2248 6.56\u00d7104, which is only 4.2 times of RBayes(150) in the default case. It is worth mentioning that this result also suggests that the Bayes regret bound derived in Theorem 1 is (almost) tight in this problem5. To see it, notice that K = 2m and\n5Recall that Theorem 1 requires maxe\u2208E \u2016\u03c6e\u20162 \u2264 1. It can\nL = O(m2), and hence the Bayes regret bound derived in Theorem 1 is O\u0303(m).\nSecond, we study how the Bayes cumulative regret of CombLinTS scales with d, the dimension of the feature vectors, by varying d = 10, 60, 110, . . . , 510, and demonstrate the result in Figure 1(b). The experiment results indicate that RBayes(150) also roughly increases linearly with d, and hence CombLinTS is also scalable with the feature dimension d. This result also suggests that the O\u0303( \u221a d) bound in Theorem 1 is (almost) tight5.\nFinally, we study the robustness of CombLinTS with respect to the algorithm parameters \u03c3 and \u03bb. In Figure 1(c), we vary \u03c3 = 10\u22127, 10\u22126, . . . , 104 and in Figure 1(d), we vary \u03bb = 10\u22122, 10\u22121, . . . , 105. We would like to emphasize again that we only vary the algorithm parameters and fix \u03c3true = 1 and \u03bbtrue = 10. The experiment results show that CombLinTS is robust to the choice of algorithm parameters and performs well for a wide range of \u03c3 and \u03bb. However, too small or too large \u03c3, or too small \u03bb, can significantly reduce the performance of CombLinTS, as we have discussed in Section 4.1."}, {"heading": "6.2. Online Advertising", "text": "In the second experiment, we evaluate CombLinTS on an advertising problem. Our objective is to identify 100 people that are most likely to accept an advertisement offer, subject to the targeting constraint that exactly half of them are females. Specifically, the ground set E includes 33k representative people from Adult dataset (Asuncion & Newman, 2007), which was collected in the 1994 US census. A feasible solution A is any subset of E with |A| = 100 and satisfying the targeting constraint mentioned above. We assume that person e accepts an advertisement offer with probability\nw\u0304(e) = { 0.15 income is at least 50k 0.05 otherwise,\nand people accept offers independently of each other. The features in the generalization matrix \u03a6 are the age, which is binned into 7 groups; gender; whether the person works more than 40 hours per week; and the length of education in years. All these features can be constructed based on the Adult dataset.\nCombLinTS is compared to three baselines. The first baseline is the optimal solution Aopt. The second baseline is\nbe easily extended to cases with maxe\u2208E \u2016\u03c6e\u20162 \u2264M by scaling the Bayes regret bound by M . However, in this problem \u03c6e is not bounded since it is sampled from a Gaussian distribution. We believe that Theorem 1 can be extended to this case by exploiting the properties of Gaussian distribution. Roughly speaking, in this problem, with high probability, \u2016\u03c6e\u20162 = O( \u221a d).\nCombUCB1 (Kveton et al., 2015b). This algorithm estimates the probability that person e accepts the offer w\u0304(e) independently of the other probabilities. The third baseline is CombLinTS without linear generalization, which we simply refer to as CombTS. As in CombUCB1, this algorithm estimates the probability that person e accepts the offer w\u0304(e) independently of the other probabilities. The posterior of w\u0304(e) is modeled as a beta distribution.\nOur experiment results are reported in Figure 2. We observe two major trends. First, CombLinTS learns extremely quickly. In particular, its per-step return at episode 100 is 70% of the optimum, and its per-step return at episode 1k is 80% of the optimum. These results are remarkable since the linear generalization is imperfect in this problem. Second, both CombUCB1 and CombTS perform poorly due to insufficient observations with respect to the model complexity. Specifically, in 1k episodes, the people in E are observed 100k times, which implies that each person is observed only 3 times on average. This is not enough to discriminate the people who are likely to accept the advertisement offer from those that are not."}, {"heading": "6.3. Artist Recommendation", "text": "In the last experiment, we evaluate CombLinTS on a problem of recommending K = 10 music artists that are most likely to be chosen by an average user of a music recommendation website. Specifically, the ground set E include artists from the Last.fm music recommendation dataset (Cantador et al., 2011). The dataset contains tagging and music artist listening information from a set of 2k users from Last.fm online music system6. The tagging part includes the tag assignments of all artists provided by the users. For each user, the artists to whom she listened and the number of listening events are also available in the dataset.\n6http://www.lastfm.com\nWe choose E as the set of artists that were listened by at least two users and had at least one tag assignment among the top 20 most popular tags, and |E| \u2248 6k. For each artist e, we construct its feature vector \u03c6e \u2208 [0, 1]20 by setting its jth component as the fraction of users who assigned tag j to this artist. We assume that each artist e is chosen by an average user with probability w\u0304(e) = 1|Ue| \u2211 u\u2208Ue wu(e), where Ue is the set of users that listened to artist e, and w\u0304u(e) is the probability that user u likes artist e. We estimate w\u0304u(e) based on a Na\u0131\u0308ve Bayes classifier with respect to the number of person/artist listening events.\nLike Section 6.2, we also compare CombLinTS to three baselines: the optimal solution Aopt, the CombUCB1 algorithm and the CombTS algorithm. Our experiment results are reported in Figure 3. Similarly as Figure 2, the expected per-step return of CombLinTS approaches that of Aopt much faster than CombUCB1 and CombTS. Moreover, both CombUCB1 and CombTS perform poorly due to the insufficient observations with respect to the model complexity: In 1k episodes, each artist is observed less than 2 times on average, which is not enough to discriminate most popular artists from less popular artists."}, {"heading": "7. Conclusion", "text": "We have proposed two learning algorithms, CombLinTS and CombLinUCB, for stochastic combinatorial semibandits with linear generalization. The main contribution of this work is two-fold: First, we have established Lindependent regret bounds for these two algorithms under reasonable assumptions, where L is the number of items. Second, we have also evaluated CombLinTS on a variety of problems. The experiment results in the first problem show that CombLinTS is scalable and robust, and the experiment results in the other two problems demonstrate the value of exploiting linear generalization in real-world settings.\nIt is worth mentioning that our results can be easily ex-\ntended to the contextual combinatorial semi-bandits with linear generalization. In a contextual combinatorial semibandit, the probability distribution P (and hence the expected weight w\u0304) also depends on a context x, which either follows an exogenous stochastic process or is adaptively chosen by an adversary. Assume that each state-item pair (x, e) is associated with a feature vector \u03c6x,e, then similar to Agrawal & Goyal (2013), both CombLinTS and CombLinUCB, as well as their analyses, can be generalized to the contextual combinatorial semi-bandits.\nWe leave open several questions of interest. One interesting open question is how to derive regret bounds for CombLinTS and CombLinUCB in the agnostic learning cases. Another interesting open question is how to extend the results to combinatorial semi-bandits with nonlinear generalization. We believe that our results can be extended to combinatorial semi-bandits with generalized linear generalization7, but leave it to future work."}, {"heading": "A. Proof for Theorem 1", "text": "To prove Theorem 1, we first prove the following theorem:\nTheorem 3. If (1) w\u0304 = \u03a6\u03b8\u2217, (2) the prior on \u03b8\u2217 is N(0, \u03bb2I), and (3) the noises are i.i.d. sampled from N(0, \u03c32), then under CombLinTS algorithm with parameter (\u03a6, \u03bb, \u03c3), then we have\nRBayes(n) \u2264 1 +K\u03bbmin {\u221a ln ( \u03bbLn\u221a\n2\u03c0\n) , \u221a d ln ( 2dKn\u03bb\u221a\n2\u03c0\n)}\u221a 2dn ln ( 1 + nK\u03bb 2\nd ) ln ( 1 + \u03bb 2\n\u03c32 ) . (11) Notice that Theorem 1 follows immediately from Theorem 3. Specifically, if \u03bb \u2265 \u03c3, then we have\nBBayes(n) \u2264 1 +K\u03bbmin {\u221a ln ( \u03bbLn\u221a\n2\u03c0\n) , \u221a d ln ( 2dKn\u03bb\u221a\n2\u03c0\n)}\u221a 2dn log2 ( 1 + nK\u03bb2\nd ) = O\u0303 ( K\u03bb \u221a dnmin {ln(L), d} ) . (12)\nWe now outline the proof of Theorem 3, which is based on (Russo & Van Roy, 2013; Dani et al., 2008). LetHt denote the \u201chistory\u201d (i.e. all the available information) by the start of episode t. Note that from the Bayesian perspective, conditioning on Ht, \u03b8\u2217 and \u03b8t are i.i.d. drawn from N(\u03b8\u0304t,\u03a3t) (see (Russo & Van Roy, 2013)). This is because that conditioning on Ht, the posterior belief in \u03b8\u2217 is N(\u03b8\u0304t,\u03a3t) and based on Algorithm 2, \u03b8t is independently sampled from N(\u03b8\u0304t,\u03a3t). Since ORACLE is a fixed combinatorial optimization algorithm (even though it can be independently randomized), and E,A,\u03a6 are all fixed, then conditioning onHt, A\u2217 and At are also i.i.d., furthermore, A\u2217 is conditionally independent of \u03b8t, and At is conditionally independent of \u03b8\u2217.\nTo simplify the exposition, \u2200\u03b8 \u2208 Rd and \u2200A \u2286 E, we define\ng(A, \u03b8) = \u2211 e\u2208A \u3008\u03c6e, \u03b8\u3009 , (13)\nthen we have E[f(A\u2217,wt)|Ht, \u03b8\u2217, \u03b8t, A\u2217, At] = g(A\u2217, \u03b8\u2217) and E[f(At,wt)|Ht, \u03b8\u2217, \u03b8t, A\u2217, At] = g(At, \u03b8\u2217), hence we have E[Rt|Ht] = E[g(A\u2217, \u03b8\u2217)\u2212 g(At, \u03b8\u2217)|Ht]. We also define the upper confidence bound (UCB) function Ut : 2E \u2192 R as\nUt(A) = \u2211 e\u2208A [\u2329 \u03c6e, \u03b8\u0304t \u232a + c \u221a \u03c6Te \u03a3t\u03c6e ] , (14)\nwhere c > 0 is a constant to be specified. Notice that conditioning on Ht, Ut is a deterministic function and A\u2217, At are i.i.d., then E[Ut(At)\u2212 Ut(A\u2217)|Ht] = 0 and\nE[Rt|Ht] = E[g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)|Ht] + E [ Ut(A t)\u2212 g(At, \u03b8\u2217) \u2223\u2223Ht] . (15)\nOne key observation is that\nE [ Ut(A t)\u2212 g(At, \u03b8\u2217) \u2223\u2223Ht] (a)= \u2211 e\u2208E E [ 1 { e \u2208 At } [\u2329 \u03c6e, \u03b8\u0304t \u2212 \u03b8\u2217 \u232a + c \u221a \u03c6Te \u03a3t\u03c6e ]\u2223\u2223\u2223\u2223Ht] (b) = \u2211 e\u2208E E [ 1 { e \u2208 At }\u2223\u2223Ht]E[\u2329\u03c6e, \u03b8\u0304t \u2212 \u03b8\u2217\u232a\u2223\u2223Ht]+ cE[\u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e \u2223\u2223\u2223\u2223\u2223Ht ]\n(c) = cE [\u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e \u2223\u2223\u2223\u2223\u2223Ht ] , (16)\nwhere (b) follows from the fact that At and \u03b8\u2217 are conditionally independent, and (c) follows from E[\u03b8\u2217|Ht] = \u03b8\u0304t. Hence BBayes(n) = \u2211n t=1 E[g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)] + c \u2211n t=1 E [\u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e ] . We can show that (1)\nEfficient Learning in Large-Scale Combinatorial Semi-Bandits\u2211n t=1 E[g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)] \u2264 1 if we choose\nc \u2265 min {\u221a ln ( \u03bbLn\u221a\n2\u03c0\n) , \u221a d ln ( 2dKn\u03bb\u221a\n2\u03c0\n)} , (17)\nand (2) \u2211n t=1 E [\u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e ] \u2264 K\u03bb \u221a 2dn ln ( 1 + nK\u03bb 2 d ) / ln ( 1 + \u03bb 2 \u03c32 ) . Thus, the bound in Theorem 3 holds. Please refer to the remainder of this section for the full proof. A.1. Bound on \u2211n t=1 E[g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)]\nWe first prove that if we choose\nc \u2265 min {\u221a ln ( \u03bbLn\u221a\n2\u03c0\n) , \u221a d ln ( 2dKn\u03bb\u221a\n2\u03c0\n)} , (18)\nthen \u2211n t=1 E[g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)] \u2264 1. To prove this result, we use the following inequality for truncated Gaussian distribution.\nLemma 1. If X \u223c N(\u00b5, s2), then we have\nE[X1{X \u2265 0}] = \u00b5 [ 1\u2212 \u03a6G ( \u2212\u00b5 s )] + s\u221a 2\u03c0 exp ( \u2212 \u00b5 2 2s2 ) ,\nwhere \u03a6G is the cumulative distribution function (CDF) of the standard Gaussian distribution N(0, 1). Furthermore, if \u00b5 \u2264 0, we have E[X1{X \u2265 0}] \u2264 s\u221a 2\u03c0 exp ( \u2212 \u00b5 2 2s2 ) .\nBased on Lemma 1, we can prove the following lemmas: Lemma 2. If c \u2265 \u221a ln ( \u03bbLn\u221a\n2\u03c0\n) , then we have \u2211n t=1 E[g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)] \u2264 1.\nProof. We have the following naive bound:\ng(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217) = \u2211 e\u2208A\u2217 [\u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e ] \u2264 \u2211 e\u2208A\u2217 [\u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e ] 1 {\u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e \u2265 0\n} \u2264 \u2211 e\u2208E [\u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e ] 1 {\u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e \u2265 0 } .\nNotice that conditioning on Ht, \u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e is a Gaussian random variable with mean \u2212c \u221a \u03c6Te \u03a3t\u03c6e and variance \u03c6Te \u03a3t\u03c6e. Thus, from Lemma 1, we have\nE\u03b8\u2217,A\u2217 [g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)|Ht] (a) \u2264 \u2211 e\u2208E E\u03b8\u2217 [[\u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e ] 1 {\u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e \u2265 0 }\u2223\u2223\u2223\u2223Ht] (b)\n\u2264 \u2211 e\u2208E\n\u221a \u03c6Te \u03a3t\u03c6e\n2\u03c0 exp\n( \u2212c 2\n2 ) (c)\n\u2264 exp ( \u2212c 2\n2 )\u2211 e\u2208E \u03bb\u2016\u03c6e\u2016\u221a 2\u03c0 \u2264 exp ( \u2212c 2 2 ) \u03bbL\u221a 2\u03c0 , (19)\nwhere the last two inequalities follow from the fact that \u03c6Te \u03a3t\u03c6e \u2264 \u03c6Te \u03a31\u03c6e \u2264 \u03bb2\u2016\u03c6e\u20162 \u2264 \u03bb2, since \u2016\u03c6e\u2016 \u2264 1 by assumption8. Thus we have\nE [ n\u2211 t=1 [g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)] ] \u2264 exp ( \u2212c 2 2 ) n\u03bbL\u221a 2\u03c0 . (20)\nIf we choose c \u2265 \u221a 2 ln ( \u03bbLn\u221a\n2\u03c0\n) , then we have E[ \u2211n t=1 [g(A \u2217, \u03b8\u2217)\u2212 Ut(A\u2217)]] \u2264 1.\nLemma 3. If c \u2265 \u221a d ln ( 2dKn\u03bb\u221a\n2\u03c0\n) , then we also have \u2211n t=1 E[g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)] \u2264 1.\nProof. We use v1, . . . , vd to denote a fixed set of d orthonormal eigenvectors of \u03a3t, and \u039b21, . . . ,\u039b 2 d to denote the associated eigenvalues. Notice that for i 6= j, we have vTi \u03a3tvj = \u039b2i vTi vj = 0. \u2200i = 1, . . . , d, we define vi+d = \u2212vi and \u039bi+d = \u039bi, which allows us to define the following conic decomposition:\n\u03c6e = 2d\u2211 i=1 \u03b1eivi, \u2200e \u2208 E,\nsubject to the constraints that \u03b1ei \u2265 0, \u2200(e, i). Notice that \u03b1ei\u2019s are uniquely determined. Furthermore, for i and j s.t. |i\u2212 j| = d, by definition of conic decomposition, we have \u03b1ei\u03b1ej = 0. In other words, \u03b1e is a d-sparse vector.\nSince we assume that \u2016\u03c6e\u2016 \u2264 1, we have that \u22112d i=1 \u03b1 2 ei \u2264 1, \u2200e \u2208 E. Thus, for any e, we have that \u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a\n=\u22112d i=1 \u03b1ei \u2329 vi, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a and\n\u03c6Te \u03a3t\u03c6e = ( 2d\u2211 i=1 \u03b1eiv T i ) \u03a3t  2d\u2211 j=1 \u03b1eivj  =\n2d\u2211 i=1 2d\u2211 j=1 \u03b1ei\u03b1ejv T i \u03a3tvj . (21)\nNotice that for i 6= j, if |i \u2212 j| 6= d, then vTi \u03a3tvj = 0; on the other hand, if |i \u2212 j| = d, \u03b1ei\u03b1ej = 0. Thus, if i 6= j, we have \u03b1ei\u03b1ejv T i \u03a3tvj = 0. Consequently,\n\u03c6Te \u03a3t\u03c6e = 2d\u2211 i=1 \u03b12eiv T i \u03a3tvi = 2d\u2211 i=1 \u03b12ei\u039b 2 i .\nThus we have\n\u221a \u03c6Te \u03a3t\u03c6e = \u221a\u221a\u221a\u221a 2d\u2211 i=1 \u03b12ei\u039b 2 i \u2265 1\u221a d 2d\u2211 i=1 \u03b1ei\u039bi, (22)\nwhere the inequality follows from Cauchy-Schwartz inequality, specifically, define si = 1 if \u03b1ei\u039bi 6= 0, and si = 0 if \u03b1ei\u039bi = 0, then we have\n2d\u2211 i=1 \u03b1ei\u039bi = 2d\u2211 i=1 \u03b1ei\u039bisi \u2264 \u221a\u221a\u221a\u221a 2d\u2211 i=1 s2i \u221a\u221a\u221a\u221a 2d\u2211 i=1 \u03b12ei\u039b 2 i \u2264 \u221a d \u221a\u221a\u221a\u221a 2d\u2211 i=1 \u03b12ei\u039b 2 i ,\n8Notice that in the derivation of Inequality (19), we implicitly assume that \u03c6Te \u03a3t\u03c6e > 0, \u2200e \u2208 E. It is worth pointing out that the case with \u03c6Te \u03a3t\u03c6e = 0 is a trivial case and this inequality still holds in this case.\nwhere the last inequality follows from the fact that \u03b1e is d-sparse. Thus, for any e, we have that\n\u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e \u2264 2d\u2211 i=1 \u03b1ei \u2329 vi, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c\u221a d 2d\u2211 i=1 \u03b1ei\u039bi. (23)\nConsequently, we have\n\u2211 e\u2208A\u2217 [\u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e ] \u2264 2d\u2211 i=1 (\u2329 vi, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c\u039bi\u221a d )(\u2211 e\u2208A\u2217 \u03b1ei ) . (24)\nDefine Xi = \u2329 vi, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c\u039bi\u221a\nd , notice that conditioning onHt, we have Xi|Ht \u223c N\n( \u2212 c\u039bi\u221a\nd ,\u039b2i\n) . Hence we have\n\u2211 e\u2208A\u2217 \u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e (a) \u2264 2d\u2211 i=1 Xi [\u2211 e\u2208A\u2217 \u03b1ei ] (b)\n\u2264 2d\u2211 i=1 Xi1{Xi \u2265 0} [\u2211 e\u2208A\u2217 \u03b1ei ] ,\nwhere the inequality (b) follows from the fact that Xi \u2264 Xi1{Xi \u2265 0} and [\u2211 e\u2208A\u2217 \u03b1ei ] \u2265 0. On the other hand, notice that |A\u2217| \u2264 K\n\u2211 e\u2208A\u2217 \u03b1ei \u2264 \u221a |A\u2217| \u221a\u2211 e\u2208A\u2217 \u03b12ei \u2264 \u221a |A\u2217| \u221a\u221a\u221a\u221a\u2211 e\u2208A\u2217 d\u2211 j=1 \u03b12ej \u2264 \u221a |A\u2217| \u221a\u2211 e\u2208A\u2217 1 = |A\u2217| \u2264 K.\nSince Xi1{Xi \u2265 0} \u2265 0, we have\n\u2211 e\u2208A\u2217 \u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e \u2264 K 2d\u2211 i=1 Xi1{Xi \u2265 0} ,\nnotice that the RHS does not include A\u2217. Hence we have E\u03b8\u2217 [g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)|Ht] = E\u03b8\u2217 [\u2211 e\u2208A\u2217 \u2329 \u03c6e, \u03b8 \u2217 \u2212 \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e \u2223\u2223\u2223\u2223\u2223Ht ]\n\u2264 K 2d\u2211 i=1 E\u03b8\u2217 [Xi1{Xi \u2265 0}|Ht]\n\u2264 K 2d\u2211 i=1 \u039bi\u221a 2\u03c0 exp ( \u2212 c 2 2d ) \u2264 2dK\u03bb\u221a 2\u03c0 exp ( \u2212 c 2 2d ) ,\nwhere the last inequality follows from the fact that \u039bi \u2264 \u03bb. Hence we have n\u2211 t=1 E[g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)] \u2264 2dKn\u03bb\u221a 2\u03c0 exp ( \u2212 c 2 2d ) ,\nif we choose c \u2265 \u221a 2d ln (\n2dKn\u03bb\u221a 2\u03c0\n) , then we have \u2211n t=1 E[f(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)] \u2264 1.\nCombining the results from Lemma 2 and 3, we have proved that if\nc \u2265 min {\u221a ln ( \u03bbLn\u221a\n2\u03c0\n) , \u221a d ln ( 2dKn\u03bb\u221a\n2\u03c0\n)} ,\nthen \u2211n t=1 E[g(A\u2217, \u03b8\u2217)\u2212 Ut(A\u2217)] \u2264 1.\nA.2. Bound on \u2211n t=1 E [\u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e ] In this subsection, we derive a bound on \u2211n t=1 E [\u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e ] . Our analysis is motivated by the analysis in (Dani\net al., 2008). Specifically, we provide a worst-case bound on \u2211n t=1 \u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e, for any realization of random variable wt\u2019s, \u03b8t\u2019s, At\u2019s, A\u2217, and \u03b8\u2217. Lemma 4. \u2211n t=1 \u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e \u2264 K\u03bb \u221a dn log ( 1+nK\u03bb 2 d\u03c32 ) log ( 1+\u03bb 2\n\u03c32\n) .\nProof. To simplify the exposition, we define\nzt,k = \u221a \u03c6T atk \u03a3t\u03c6atk . (25)\nFirst, notice that \u03a3\u22121t is the Gramian matrix and satisfies\n\u03a3\u22121t+1 = \u03a3 \u22121 t +\n1\n\u03c32 |At|\u2211 k=1 \u03c6atk\u03c6 T atk . (26)\nHence for any t, k, we have that\ndet [ \u03a3\u22121t+1 ] \u2265 det [ \u03a3\u22121t + 1\n\u03c32 \u03c6atk\u03c6 T atk\n] = det [ \u03a3 \u2212 12 t ( I + 1\n\u03c32 \u03a3\n1 2 t \u03c6atk\u03c6 T atk \u03a3 1 2 t ) \u03a3 \u2212 12 t ] = det [ \u03a3\u22121t ] det [ I + 1\n\u03c32 \u03a3\n1 2 t \u03c6atk\u03c6 T atk \u03a3 1 2 t\n] = det [ \u03a3\u22121t ]( 1 + 1\n\u03c32 \u03c6Tatk \u03a3t\u03c6atk ) = det [ \u03a3\u22121t ]( 1 + z2t,k \u03c32 ) . (27)\nHence we have that\n( det [ \u03a3\u22121t+1 ])|At| \u2265 (det [\u03a3\u22121t ])|At| |A t|\u220f\nk=1\n( 1 +\nz2t,k \u03c32\n) . (28)\nRemark 1. This is where the extraO( \u221a K) factor arises. Notice that this extra factor is purely due to linear generalization. Specifically, if \u03a6 = I , then \u03a3t\u2019s and \u03a3\u22121t \u2019s will be diagonal, and we have\ndet [ \u03a3\u22121t+1 ] = det [ \u03a3\u22121t ] |At|\u220f k=1\n( 1 +\nz2t,k \u03c32\n) . (29)\nNotice that Equation 28 further implies that\n( det [ \u03a3\u22121t+1 ])K \u2265 (det [\u03a3\u22121t ])K |A t|\u220f\nk=1\n( 1 +\nz2t,k \u03c32\n) , (30)\nsince det [ \u03a3\u22121t+1 ] \u2265 det [ \u03a3\u22121t ] and |At| \u2264 K. Recall that det [ \u03a3\u221211 ] = ( 1 \u03bb2 )d , we have that\n( det [ \u03a3\u22121n+1 ])K \u2265 (det [\u03a3\u221211 ])K n\u220f t=1 |At|\u220f k=1\n( 1 +\nz2t,k \u03c32\n) = 1\n\u03bb2dK n\u220f t=1 |At|\u220f k=1\n( 1 +\nz2t,k \u03c32\n) . (31)\nOn the other hand, we have\ntrace [ \u03a3\u22121n+1 ] = trace  1 \u03bb2 I + 1 \u03c32 n\u2211 t=1 |At|\u2211 k=1 \u03c6atk\u03c6 T atk  = d \u03bb2 + 1 \u03c32 n\u2211 t=1 |At|\u2211 k=1 \u2016\u03c6atk\u2016 2 \u2264 d \u03bb2 + nK \u03c32 , (32)\nwhere the last inequality follows from the assumption that \u2016\u03c6e\u2016 \u2264 1, \u2200e \u2208 E and |At| \u2264 K. From the trace-determinant inequality, we have\n1 d trace\n[ \u03a3\u22121n+1 ] \u2265 ( det [ \u03a3\u22121n+1 ]) 1 d ,\nwhich implies that\n( 1\n\u03bb2 + nK d\u03c32\n)dK \u2265 ( 1\nd trace\n[ \u03a3\u22121n+1 ])dK \u2265 ( det [ \u03a3\u22121n+1 ])K \u2265 1 \u03bb2dK n\u220f t=1 |At|\u220f k=1 ( 1 + z2t,k \u03c32 ) .\nTaking the logarithm, we have\ndK log ( 1 + nK\u03bb2\nd\u03c32\n) \u2265 n\u2211 t=1 |At|\u2211 k=1 log ( 1 + z2t,k \u03c32 ) . (33)\nNotice that z2t,k = \u03c6 T atk \u03a3t\u03c6atk , hence we have that 0 \u2264 z 2 t,k \u2264 \u03c6Tatk\u03a31\u03c6atk \u2264 \u03bb 2\u2016\u03c6atk\u2016 2 \u2264 \u03bb2. We have the following technical lemma:\nLemma 5. For any real number x \u2208 [0, \u03bb2], we have x \u2264 \u03bb 2 log ( 1+\u03bb 2\n\u03c32 ) log (1 + x\u03c32 ). Proof. Define h(x) = \u03bb 2\nlog ( 1+\u03bb 2\n\u03c32 ) log (1 + x\u03c32 )\u2212x, thus we only need to prove h(x) \u2265 0 for x \u2208 [0, \u03bb2]. Notice that h(x) is a strictly concave function for x \u2265 0, and h(0) = 0, h(\u03bb2) = 0. From Jensen\u2019s inequality, for any x \u2208 (0, \u03bb2), we have h(x) > 0.\nHence we have that\nn\u2211 t=1 |At|\u2211 k=1 z2t,k \u2264 \u03bb2 log ( 1 + \u03bb 2 \u03c32 ) n\u2211 t=1 |At|\u2211 k=1 log\n( 1 +\nz2t,k \u03c32\n) \u2264 dK\u03bb2 log ( 1 + nK\u03bb 2 d\u03c32 ) log ( 1 + \u03bb 2\n\u03c32 ) (34) Finally, we have that\nn\u2211 t=1 |At|\u2211 k=1 zt,k \u2264 \u221a nK \u221a\u221a\u221a\u221a n\u2211 t=1 |At|\u2211 k=1 z2t,k \u2264 K\u03bb \u221a dn log ( 1 + nK\u03bb 2 d\u03c32 ) log ( 1 + \u03bb 2 \u03c32 ) . (35)\nRecall that the above bound holds for any realization of random variables, thus, we have\nE [ n\u2211 t=1 [ Ut(A t)\u2212 g(At, \u03b8\u2217) ]] = cE  n\u2211 t=1 |At|\u2211 k=1 zt,k  \u2264 cK\u03bb\u221adn log (1 + nK\u03bb2d ) log ( 1 + \u03bb 2 \u03c32\n) . With\nc = min {\u221a ln ( \u03bbLn\u221a\n2\u03c0\n) , \u221a d ln ( 2dKn\u03bb\u221a\n2\u03c0\n)} , (36)\nand combining the results in the previous subsection, we have proved Theorem 3."}, {"heading": "B. Proof for Theorem 2", "text": "We start by writing an alternative formula for \u03a3t and \u03b8\u0304t. Notice that based on Algorithm 1, we have:\n\u03a3\u22121t = 1\n\u03bb2 I +\n1\n\u03c32 t\u22121\u2211 \u03c4=1 |A\u03c4 |\u2211 k=1 \u03c6a\u03c4k\u03c6 T a\u03c4k\n\u03a3\u22121t \u03b8\u0304t = 1\n\u03c32 t\u22121\u2211 \u03c4=1 |A\u03c4 |\u2211 k=1 \u03c6a\u03c4kw\u03c4 (a \u03c4 k) (37)\nInterested readers might refer to Appendix C for the derivation of Equation (37). The proof proceeds as follows. We first construct a confidence set of \u03b8\u2217 based on the \u201cself normalized bound\u201d developed in (Abbasi-Yadkori et al., 2011). Then we derive a regret bound based on Lemma 4 derived above.\nB.1. Confidence Set\nOur construction of confidence set is motivated by the analysis in (Agrawal & Goyal, 2013). We start by defining some useful notation. Specifically, for any t = 1, 2, . . . , n, any k = 1, 2, . . . , |At|, we define\n\u03b7t,k = wt ( atk ) \u2212 w\u0304 ( atk ) .\nOne key observation is that \u03b7t,k\u2019s form a Martingale difference sequence (MDS)9 since w(e)\u2019s are statistically independent under P . Moreover, since wt (atk) is bounded in interval [0, 1], \u03b7t,k\u2019s are sub-Gaussian with constant R = 1. We further define\nVt = \u03c32\n\u03bb2 I + t\u22121\u2211 \u03c4=1 |A\u03c4 |\u2211 k=1 \u03c6a\u03c4k\u03c6 T a\u03c4k\n\u03bet = t\u22121\u2211 \u03c4=1 |A\u03c4 |\u2211 k=1 \u03c6a\u03c4k\u03b7\u03c4,k\nAs we will see later, we define Vt and \u03bet to use the \u201cself normalized bound\u201d developed in (Abbasi-Yadkori et al., 2011) (see Theorem 1 of (Abbasi-Yadkori et al., 2011)). Notice that based on the above definition, we have \u03a3\u22121t = 1 \u03c32Vt, and\n\u03b8\u0304t \u2212 \u03b8\u2217 = \u03a3t ( 1\n\u03c32 \u03bet \u2212\n1 \u03bb2 \u03b8\u2217 ) .\nTo see why the second equality holds, notice that\n\u03a3\u22121t \u03b8\u0304t = 1\n\u03c32 t\u22121\u2211 \u03c4=1 |A\u03c4 |\u2211 k=1 \u03c6a\u03c4k ( \u03c6Ta\u03c4k\u03b8 \u2217 + \u03b7\u03c4,k ) = ( \u03a3\u22121t \u2212 1\n\u03bb2 I\n) \u03b8\u2217 + 1\n\u03c32 \u03bet.\nHence, for any e \u2208 E, we have\n\u2223\u2223\u2329\u03c6e, \u03b8\u0304t \u2212 \u03b8\u2217\u232a\u2223\u2223 = \u2223\u2223\u2223\u2223\u03c6Te \u03a3t( 1\u03c32 \u03bet \u2212 1\u03bb2 \u03b8\u2217 )\u2223\u2223\u2223\u2223\n\u2264 \u2016\u03c6e\u2016\u03a3t \u2225\u2225\u2225\u2225 1\u03c32 \u03bet \u2212 1\u03bb2 \u03b8\u2217 \u2225\u2225\u2225\u2225\n\u03a3t\n\u2264 \u2016\u03c6e\u2016\u03a3t\n[ 1\n\u03c32 \u2016\u03bet\u2016\u03a3t +\n1\n\u03bb2 \u2016\u03b8\u2217\u2016\u03a3t\n] ,\n9Note that the notion of \u201ctime\u201d is indexed by a pair (t, k), and follows the lexicographical order.\nwhere the first inequality follows from the Cauchy-Schwarz inequality, and the second inequality follows from the triangular inequality. Notice that\n\u2016\u03b8\u2217\u2016\u03a3t \u2264 \u2016\u03b8 \u2217\u2016\u03a31 = \u03bb \u2016\u03b8 \u2217\u20162 , hence we have \u2223\u2223\u2329\u03c6e, \u03b8\u0304t \u2212 \u03b8\u2217\u232a\u2223\u2223 \u2264 \u2016\u03c6e\u2016\u03a3t [ 1\u03c32 \u2016\u03bet\u2016\u03a3t + 1\u03bb \u2016\u03b8\u2217\u20162 ] .\nMoreover, we have\n1\n\u03c32 \u2016\u03bet\u2016\u03a3t =\n1\n\u03c32 \u2016\u03bet\u2016\u03c32V \u22121t =\n1 \u03c3 \u2016\u03bet\u2016V \u22121t .\nSo we have \u2223\u2223\u2329\u03c6e, \u03b8\u0304t \u2212 \u03b8\u2217\u232a\u2223\u2223 \u2264 \u2016\u03c6e\u2016\u03a3t [ 1\u03c3 \u2016\u03bet\u2016V \u22121t + 1\u03bb \u2016\u03b8\u2217\u20162 ] . (38)\nThe above inequality always holds. We now provide a high probability bound on \u2016\u03bet\u2016V \u22121t , based on the \u201cself normalized bound\u201d proposed in (Abbasi-Yadkori et al., 2011). From Theorem 1 of (Abbasi-Yadkori et al., 2011), we know for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4,\n\u2016\u03bet\u2016V \u22121t \u2264\n\u221a 2 log ( det(Vt)1/2 det(V1)\u22121/2\n\u03b4\n) \u2200t = 1, 2, . . . .\nObviously, det (V1) = [ \u03c32\n\u03bb2\n]d , on the other hand, we have\n[det(Vt)] 1/d \u2264 trace(Vt) d = \u03c32 \u03bb2 + 1 d t\u22121\u2211 \u03c4=1 |A\u03c4 |\u2211 k=1 \u2016\u03c6a\u03c4k\u2016 2 \u2264 \u03c3 2 \u03bb2 + (t\u2212 1)K d ,\nwhere the last inequality follows from the assumption that \u2016\u03c6e\u2016 \u2264 1. Hence, for t \u2264 n, we have\n[det(Vt)] 1/d \u2264\u03c3\n2\n\u03bb2 + nK d .\nThus, with probability at least 1\u2212 \u03b4, we have\n\u2016\u03bet\u2016V \u22121t \u2264\n\u221a d log ( 1 + nK\u03bb2\nd\u03c32\n) + 2 log ( 1\n\u03b4\n) \u2200t = 1, 2, . . . , n.\nThus, we have the following lemma:\nLemma 6. For any \u03bb, \u03c3 > 0 and any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4, we have \u2223\u2223\u2329\u03c6e, \u03b8\u0304t \u2212 \u03b8\u2217\u232a\u2223\u2223 \u2264 \u2016\u03c6e\u2016\u03a3t [ 1\n\u03c3\n\u221a d log ( 1 + nK\u03bb2\nd\u03c32\n) + 2 log ( 1\n\u03b4 ) + \u2016\u03b8\u2217\u20162 \u03bb ] , (39)\nfor all t = 1, 2, . . . , n, and for all e \u2208 E. Notice that \u2016\u03c6e\u2016\u03a3t = \u221a \u03c6Te \u03a3t\u03c6e, thus, the above lemma immediately implies the following lemma:\nLemma 7. For any \u03bb, \u03c3 > 0, any \u03b4 \u2208 (0, 1), and any\nc \u2265 1 \u03c3\n\u221a d log ( 1 + nK\u03bb2\nd\u03c32\n) + 2 log ( 1\n\u03b4 ) + \u2016\u03b8\u2217\u20162 \u03bb ,\nwith probability at least 1\u2212 \u03b4, we have\u2329 \u03c6e, \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e \u2264 \u3008\u03c6e, \u03b8\u2217\u3009 \u2264 \u2329 \u03c6e, \u03b8\u0304t \u232a + c \u221a \u03c6Te \u03a3t\u03c6e,\nfor all e \u2208 E and t = 1, 2, . . . n.\nNotice that\n\u3008\u03c6e, \u03b8\u2217\u3009 \u2264 \u2329 \u03c6e, \u03b8\u0304t \u232a + c \u221a \u03c6Te \u03a3t\u03c6e\nis exactly w\u0304(e) \u2264 w\u0302t(e).\nB.2. Regret Analysis\nWe define event G as\nG = {\u2329 \u03c6e, \u03b8\u0304t \u232a \u2212 c \u221a \u03c6Te \u03a3t\u03c6e \u2264 \u3008\u03c6e, \u03b8\u2217\u3009 \u2264 \u2329 \u03c6e, \u03b8\u0304t \u232a + c \u221a \u03c6Te \u03a3t\u03c6e \u2200e \u2208 E, \u2200t = 1, . . . , n } , (40)\nand use G\u0304 to denote the complement of event G. Recall that Lemma 7 states that if\nc \u2265 1 \u03c3\n\u221a d log ( 1 + nK\u03bb2\nd\u03c32\n) + 2 log ( 1\n\u03b4\n) + 1\n\u03bb \u2016\u03b8\u2217\u20162 , (41)\nthen P(G) \u2265 1\u2212 \u03b4. Moreover, by definition, under event G, we have w\u0304(e) \u2264 w\u0302t(e), for all t = 1, . . . , n and any e \u2208 E.\nNotice that\nR(n) = n\u2211 t=1 E [\u2211 e\u2208A\u2217 wt(e)\u2212 \u2211 e\u2208At wt(e) ]\n= n\u2211 t=1 E [\u2211 e\u2208A\u2217 w\u0304(e)\u2212 \u2211 e\u2208At w\u0304(e) ]\n=P (G) n\u2211 t=1 E [\u2211 e\u2208A\u2217 w\u0304(e)\u2212 \u2211 e\u2208At w\u0304(e) \u2223\u2223\u2223\u2223\u2223G ] + P ( G\u0304 ) n\u2211 t=1 E [\u2211 e\u2208A\u2217 w\u0304(e)\u2212 \u2211 e\u2208At w\u0304(e) \u2223\u2223\u2223\u2223\u2223G\u0304 ]\n\u2264 n\u2211 t=1 E [\u2211 e\u2208A\u2217 w\u0304(e)\u2212 \u2211 e\u2208At w\u0304(e) \u2223\u2223\u2223\u2223\u2223G ] + P ( G\u0304 ) nK,\nwhere the last inequality follows from the naive bound on the realized regret. If c satisfies inequality (41), we have P ( G\u0304 ) \u2264 \u03b4, hence we have\nR(n) \u2264 n\u2211 t=1 E [\u2211 e\u2208A\u2217 w\u0304(e)\u2212 \u2211 e\u2208At w\u0304(e) \u2223\u2223\u2223\u2223\u2223G ] + nK\u03b4.\nFinally, we bound \u2211n t=1 E [\u2211 e\u2208A\u2217 w\u0304(e)\u2212 \u2211 e\u2208At w\u0304(e) \u2223\u2223G] using a worst-case bound conditioning onG (worst-case over all the possible random realizations), notice that conditioning on G, we have\u2211\ne\u2208A\u2217 w\u0304(e) \u2264 \u2211 e\u2208A\u2217 w\u0302t(e) \u2264 \u2211 e\u2208At w\u0302t(e),\nwhere the first inequality follows from the definition of eventG, and the second inequality follows from thatAt is the exact solution of the combinatorial optimization problem (E,A, w\u0302t). Thus we have\u2211\ne\u2208A\u2217 w\u0304(e)\u2212 \u2211 e\u2208At w\u0304(e) \u2264 \u2211 e\u2208At w\u0302t(e)\u2212 \u2211 e\u2208At w\u0304(e)\n= \u2211 e\u2208At [\u2329 \u03c6e, \u03b8\u0304t \u2212 \u03b8\u2217 \u232a + c \u221a \u03c6Te \u03a3t\u03c6e ] \u2264 2c\n\u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e,\nwhere the last inequality follows from the definition of G. Recall that from Lemma 4, we have\nn\u2211 t=1 \u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e \u2264 K\u03bb\n\u221a dn log ( 1 + nK\u03bb 2\nd\u03c32 ) log ( 1 + \u03bb 2\n\u03c32 ) . Thus we have\nn\u2211 t=1 E [\u2211 e\u2208A\u2217 w\u0304(e)\u2212 \u2211 e\u2208At w\u0304(e) \u2223\u2223\u2223\u2223\u2223G ] \u2264 2cE [ n\u2211 t=1 \u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e ] \u2264 2cK\u03bb \u221a dn log ( 1 + nK\u03bb 2 d\u03c32 ) log ( 1 + \u03bb 2 \u03c32\n) , which implies\nR(n) \u2264 2cK\u03bb\n\u221a dn log ( 1 + nK\u03bb 2\nd\u03c32 ) log ( 1 + \u03bb 2\n\u03c32 ) + nK\u03b4."}, {"heading": "C. Technical Lemma", "text": "In this section, we derive Equation (37). We first prove the following technical lemma:\nLemma 8. For any \u03c6, \u03b8\u0304 \u2208 Rd, any positive definite \u03a3 \u2208 Rd\u00d7d, any \u03c3 > 0, and any w \u2208 R, if we define\n\u03a3new = \u03a3\u2212 \u03a3\u03c6\u03c6T\u03a3\n\u03c6T\u03a3\u03c6+ \u03c32\n\u03b8\u0304new =\n[ I \u2212 \u03a3\u03c6\u03c6 T\n\u03c6T\u03a3\u03c6+ \u03c32\n] \u03b8\u0304 + [ \u03a3\u03c6\n\u03c6T\u03a3\u03c6+ \u03c32\n] w,\nthen we have\n\u03a3\u22121new = \u03a3 \u22121 +\n1\n\u03c32 \u03c6\u03c6T (42)\n\u03a3\u22121new\u03b8\u0304new = \u03a3 \u22121\u03b8\u0304 +\n1\n\u03c32 \u03c6w. (43)\nProof. Notice that Equation (42) follows directly from the Woodbury matrix identity (matrix inversion lemma). We now prove Equation (43). Notice that we have\n\u03b8\u0304new =\n[ I \u2212 \u03a3\u03c6\u03c6 T\n\u03c6T\u03a3\u03c6+ \u03c32\n] \u03b8\u0304 + [ \u03a3\u03c6\n\u03c6T\u03a3\u03c6+ \u03c32\n] w\n= [ \u03a3\u2212 \u03a3\u03c6\u03c6 T\u03a3\n\u03c6T\u03a3\u03c6+ \u03c32\n] \u03a3\u22121\u03b8\u0304 + [ \u03a3\u03c6\n\u03c6T\u03a3\u03c6+ \u03c32\n] w\n= \u03a3new\u03a3 \u22121\u03b8\u0304 +\n[ \u03a3\u03c6\n\u03c6T\u03a3\u03c6+ \u03c32\n] w,\nthat is,\n\u03a3\u22121new\u03b8\u0304new = \u03a3 \u22121\u03b8\u0304 +\n[ \u03a3\u22121new\u03a3\u03c6\n\u03c6T\u03a3\u03c6+ \u03c32\n] w. (44)\nNotice that\n\u03a3\u22121new\u03a3\u03c6 =\n[ \u03a3\u22121 + 1\n\u03c32 \u03c6\u03c6T\n] \u03a3\u03c6 = \u03c6+ \u03c6T\u03a3\u03c6\n\u03c32 \u03c6 =\n\u03c32 + \u03c6T\u03a3\u03c6\n\u03c32 \u03c6. (45)\nPlug Equation (45) into Equation (44), we have Equation (43).\nBased on Lemma 8, by mathematical induction, we have\n\u03a3\u22121t = \u03a3 \u22121 1 +\n1\n\u03c32 t\u22121\u2211 \u03c4=1 |A\u03c4 |\u2211 k=1 \u03c6a\u03c4k\u03c6 T a\u03c4k\n\u03a3\u22121t \u03b8\u0304t = \u03a3 \u22121 1 \u03b8\u03041\n1\n\u03c32 t\u22121\u2211 \u03c4=1 |A\u03c4 |\u2211 k=1 \u03c6a\u03c4kw\u03c4 (a \u03c4 k) ,\nfurther noting that \u03a31 = \u03bb2I and \u03b8\u03041 = 0, we can derive Equation (37)."}, {"heading": "D. A Variant of Theorem 2 for Approximation Algorithms", "text": "By suitably redefining the realized regret, we can prove a variant of Theorem 2 in which ORACLE can be an approximation algorithm. Specifically, for a (possibly approximation) algorithm ORACLE, let A\u2217(w) be the solution of ORACLE to the optimization problem (E,A,w), we say \u03b3 \u2208 [0, 1) is a sub-optimality gap of ORACLE if\nf(A\u2217(w),w) \u2265 (1\u2212 \u03b3) max A\u2208A f(A,w), \u2200w. (46)\nThen we define the (scaled) realized regret R\u03b3t as\nR\u03b3t = f ( Aopt,wt ) \u2212 f (A t,wt)\n1\u2212 \u03b3 , (47)\nwhere Aopt is the exact solution to the optimization problem (E,A, w\u0304). The (scaled) cumulative regret R\u03b3(n) is defined as\nR\u03b3(n) = n\u2211 t=1 E [R\u03b3t |w\u0304] .\nUnder the assumptions that (1) the support of P is a subset of [0, 1]L (i.e. wt(e) \u2208 [0, 1] \u2200t and \u2200e \u2208 E), (2) the item weight w(e)\u2019s are statistically independent under P , and (3) the oracle ORACLE has sub-optimality gap \u03b3 \u2208 [0, 1), we have the following variant of Theorem 2 when CombLinUCB is applied to coherent learning cases:\nTheorem 4. For any \u03bb, \u03c3 > 0, any \u03b4 \u2208 (0, 1), and any c satisfying\nc \u2265 1 \u03c3\n\u221a d ln ( 1 + nK\u03bb2\nd\u03c32\n) + 2 ln ( 1\n\u03b4 ) + \u2016\u03b8\u2217\u20162 \u03bb , (48)\nif w\u0304 = \u03a6\u03b8\u2217 and the above two assumptions hold, then under CombLinUCB algorithm with parameter (\u03a6, \u03bb, \u03c3, c), we have\nR\u03b3(n) \u2264 2cK\u03bb 1\u2212 \u03b3\n\u221a dn ln ( 1 + nK\u03bb 2\nd\u03c32 ) ln ( 1 + \u03bb 2\n\u03c32 ) + nK\u03b4. Proof. Notice that Lemma 7 in Section B.1 still holds. With G defined in Equation (40), we have\nR\u03b3(n) = n\u2211 t=1 E [ \u2211 e\u2208Aopt wt(e)\u2212 1 1\u2212 \u03b3 \u2211 e\u2208At wt(e) ]\n= n\u2211 t=1 E [ \u2211 e\u2208Aopt w\u0304(e)\u2212 1 1\u2212 \u03b3 \u2211 e\u2208At w\u0304(e) ]\n=P (G) n\u2211 t=1 E [ \u2211 e\u2208Aopt w\u0304(e)\u2212 1 1\u2212 \u03b3 \u2211 e\u2208At w\u0304(e) \u2223\u2223\u2223\u2223\u2223G ] + P ( G\u0304 ) n\u2211 t=1 E [ \u2211 e\u2208Aopt w\u0304(e)\u2212 1 1\u2212 \u03b3 \u2211 e\u2208At w\u0304(e) \u2223\u2223\u2223\u2223\u2223G\u0304 ]\n\u2264 n\u2211 t=1 E [ \u2211 e\u2208Aopt w\u0304(e)\u2212 1 1\u2212 \u03b3 \u2211 e\u2208At w\u0304(e) \u2223\u2223\u2223\u2223\u2223G ] + P ( G\u0304 ) nK,\nwhere the last inequality follows from the naive bound on R\u03b3t . If c satisfies inequality (41), we have P ( G\u0304 ) \u2264 \u03b4, hence we have\nR\u03b3(n) \u2264 n\u2211 t=1 E [ \u2211 e\u2208Aopt w\u0304(e)\u2212 1 1\u2212 \u03b3 \u2211 e\u2208At w\u0304(e) \u2223\u2223\u2223\u2223\u2223G ] + nK\u03b4.\nFinally, we bound \u2211n t=1 E [\u2211 e\u2208Aopt w\u0304(e)\u2212 1 1\u2212\u03b3 \u2211 e\u2208At w\u0304(e) \u2223\u2223\u2223G] using a worst-case bound conditioning on G (worstcase over all the possible random realizations), notice that conditioning on G, we have\u2211\ne\u2208Aopt w\u0304(e) \u2264 \u2211 e\u2208Aopt w\u0302t(e) \u2264 max A\u2208A \u2211 e\u2208A w\u0302t(e) \u2264 1 1\u2212 \u03b3 \u2211 e\u2208At w\u0302t(e),\nwhere\n\u2022 The first inequality follows from the definition of event G. Specifically, under event G, w\u0304(e) \u2264 w\u0302t(e) for all t = 1, . . . , n and all e \u2208 E.\n\u2022 The second inequality follows from Aopt \u2208 A.\n\u2022 The last inequality follows from At \u2190 ORACLE(E,A, w\u0302t) and ORACLE has sub-optimality gap \u03b3 (see Equation (46)).\nThus we have\n\u2211 e\u2208Aopt w\u0304(e)\u2212 1 1\u2212 \u03b3 \u2211 e\u2208At w\u0304(e) \u2264 1 1\u2212 \u03b3 [\u2211 e\u2208At w\u0302t(e)\u2212 \u2211 e\u2208At w\u0304(e) ]\n= 1 1\u2212 \u03b3 \u2211 e\u2208At [\u2329 \u03c6e, \u03b8\u0304t \u2212 \u03b8\u2217 \u232a + c \u221a \u03c6Te \u03a3t\u03c6e ] \u2264 2c\n1\u2212 \u03b3 \u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e,\nwhere the last inequality follows from the definition of G. Recall that from Lemma 4, we also have\nn\u2211 t=1 \u2211 e\u2208At \u221a \u03c6Te \u03a3t\u03c6e \u2264 K\u03bb\n\u221a dn log ( 1 + nK\u03bb 2\nd\u03c32 ) log ( 1 + \u03bb 2\n\u03c32 ) . Putting the above inequalities together, we have proved the theorem."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Abbasi-Yadkori", "Yasin", "P\u00e1l", "D\u00e1vid", "Szepesv\u00e1ri", "Csaba"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["Agrawal", "Shipra", "Goyal", "Navin"], "venue": "In COLT 2012 - The 25th Annual Conference on Learning Theory, June", "citeRegEx": "Agrawal et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2012}, {"title": "Thompson sampling for contextual bandits with linear payoffs", "author": ["Agrawal", "Shipra", "Goyal", "Navin"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Agrawal et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2013}, {"title": "Regret in online combinatorial optimization", "author": ["Audibert", "Jean-Yves", "Bubeck", "Sebastien", "Lugosi", "Gabor"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2014}, {"title": "Using confidence bounds for exploitationexploration trade-offs", "author": ["Auer", "Peter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Auer and Peter.,? \\Q2002\\E", "shortCiteRegEx": "Auer and Peter.", "year": 2002}, {"title": "Second workshop on information heterogeneity and fusion in recommender systems (hetrec", "author": ["Cantador", "Iv\u00e1n", "Brusilovsky", "Peter", "Kuflik", "Tsvi"], "venue": "In Proceedings That is,", "citeRegEx": "Cantador et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cantador et al\\.", "year": 2011}, {"title": "Combinatorial bandits", "author": ["Cesa-Bianchi", "Nicol\u00f2", "Lugosi", "G\u00e1bor"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2012}, {"title": "An empirical evaluation of Thompson sampling", "author": ["Chapelle", "Olivier", "Li", "Lihong"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Combinatorial multi-armed bandit: General framework and applications", "author": ["Chen", "Wei", "Wang", "Yajun", "Yuan", "Yang"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["Dani", "Varsha", "Hayes", "Thomas", "Kakade", "Sham"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory, pp", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Adaptive submodular maximization in bandit setting", "author": ["Gabillon", "Victor", "Kveton", "Branislav", "Wen", "Zheng", "Eriksson", "Brian", "S. Muthukrishnan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gabillon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2013}, {"title": "Large-scale optimistic adaptive submodularity", "author": ["Gabillon", "Victor", "Kveton", "Branislav", "Wen", "Zheng", "Eriksson", "Brian", "S. Muthukrishnan"], "venue": "In Proceedings of the 28th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Gabillon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2014}, {"title": "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations", "author": ["Gai", "Yi", "Krishnamachari", "Bhaskar", "Jain", "Rahul"], "venue": "IEEE/ACM Transactions on Networking,", "citeRegEx": "Gai et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gai et al\\.", "year": 2012}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Koren", "Yehuda", "Bell", "Robert", "Volinsky", "Chris"], "venue": "IEEE Computer,", "citeRegEx": "Koren et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Matroid bandits: Fast combinatorial optimization with learning", "author": ["Kveton", "Branislav", "Wen", "Zheng", "Ashkan", "Azin", "Eydgahi", "Hoda", "Eriksson", "Brian"], "venue": "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Kveton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2014}, {"title": "Learning to act greedily: Polymatroid semi-bandits", "author": ["Kveton", "Branislav", "Wen", "Zheng", "Ashkan", "Azin", "Eydgahi", "Hoda", "Valko", "Michal"], "venue": "CoRR, abs/1405.7752,", "citeRegEx": "Kveton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kveton et al\\.", "year": 2014}, {"title": "Combinatorial Optimization", "author": ["Papadimitriou", "Christos", "Steiglitz", "Kenneth"], "venue": "Dover Publications, Mineola, NY,", "citeRegEx": "Papadimitriou et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Papadimitriou et al\\.", "year": 1998}, {"title": "Learning to optimize via posterior sampling", "author": ["Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "CoRR, abs/1301.2609,", "citeRegEx": "Russo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2013}, {"title": "An informationtheoretic analysis of thompson", "author": ["Russo", "Daniel", "Van Roy", "Benjamin"], "venue": "sampling. CoRR,", "citeRegEx": "Russo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russo et al\\.", "year": 2014}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "Efficient exploration and value function generalization in deterministic systems", "author": ["Wen", "Zheng", "Van Roy", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Wen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2013}, {"title": "Sequential Bayesian search", "author": ["Wen", "Zheng", "Kveton", "Branislav", "Eriksson", "Brian", "Bhamidipati", "Sandilya"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Wen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2013}, {"title": "Linear submodular bandits and their application to diversified retrieval", "author": ["Yue", "Yisong", "Guestrin", "Carlos"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Yue et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "This class of learning problems was recently formulated as a combinatorial bandit/semi-bandit, depending on the feedback model (Audibert et al., 2014).", "startOffset": 127, "endOffset": 150}, {"referenceID": 12, "context": "Since then, many combinatorial bandit/semi-bandit algorithms have been proposed: for the stochastic setting (Gai et al., 2012; Chen et al., 2013; Russo & Van Roy, 2014; Kveton et al., 2015b); for the adversarial setting (Cesa-Bianchi & Lugosi, 2012; Audibert et al.", "startOffset": 108, "endOffset": 190}, {"referenceID": 8, "context": "Since then, many combinatorial bandit/semi-bandit algorithms have been proposed: for the stochastic setting (Gai et al., 2012; Chen et al., 2013; Russo & Van Roy, 2014; Kveton et al., 2015b); for the adversarial setting (Cesa-Bianchi & Lugosi, 2012; Audibert et al.", "startOffset": 108, "endOffset": 190}, {"referenceID": 3, "context": ", 2015b); for the adversarial setting (Cesa-Bianchi & Lugosi, 2012; Audibert et al., 2014; Neu & Bart\u00f3k, 2013); and for subclasses of combinatorial problems, matroid and polymatroid bandits (Kveton et al.", "startOffset": 38, "endOffset": 110}, {"referenceID": 20, "context": ", 2014a;b), submodular maximization (Wen et al., 2013; Gabillon et al., 2013), and cascading bandits (Kveton et al.", "startOffset": 36, "endOffset": 77}, {"referenceID": 10, "context": ", 2014a;b), submodular maximization (Wen et al., 2013; Gabillon et al., 2013), and cascading bandits (Kveton et al.", "startOffset": 36, "endOffset": 77}, {"referenceID": 19, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al.", "startOffset": 89, "endOffset": 151}, {"referenceID": 16, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al.", "startOffset": 71, "endOffset": 192}, {"referenceID": 9, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al. (2008); AbbasiYadkori et al.", "startOffset": 193, "endOffset": 212}, {"referenceID": 9, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al. (2008); AbbasiYadkori et al. (2011)) , to combinatorial semi-bandits with linear generalization.", "startOffset": 193, "endOffset": 241}, {"referenceID": 9, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al. (2008); AbbasiYadkori et al. (2011)) , to combinatorial semi-bandits with linear generalization. In this paper, we propose two learning algorithms, Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB), based on Thompson sampling and LinUCB. Both CombLinTS and CombLinUCB are computationally efficient, as long as the offline version of the combinatorial problem can be solved efficiently. The first major contribution of the paper is that we establish a Bayes regret bound on CombLinTS and a regret bound on CombLinUCB, under reasonable assumptions. Both bounds are L-independent, and sublinear in time. The second major contribution of the paper is that we evaluate CombLinTS on a variety of problems with thousands of items, and two of these problems are based on real-world datasets. We only evaluate CombLinTS since recent literature (Chapelle & Li, 2011) suggests that Thompson sampling algorithms usually outperform UCB-like algorithms in practice. Our experimental results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines. It is worth mentioning that our derived L-independent regret bounds also hold in cases with L = \u221e. Moreover, as we will discuss in Section 7, our proposed algorithms and their analyses can be easily extended to the contextual combinatorial semibandits. Finally, we briefly review some relevant papers. Gabillon et al. (2014) and Yue & Guestrin (2011) focus on submodular maximization with linear generalization.", "startOffset": 193, "endOffset": 1693}, {"referenceID": 9, "context": "It is relatively easy to extend many linear bandit algorithms, such as Thompson sampling (Thompson, 1933; Agrawal & Goyal, 2012; Russo & Van Roy, 2013) and Linear UCB (LinUCB, see Auer (2002); Dani et al. (2008); AbbasiYadkori et al. (2011)) , to combinatorial semi-bandits with linear generalization. In this paper, we propose two learning algorithms, Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB), based on Thompson sampling and LinUCB. Both CombLinTS and CombLinUCB are computationally efficient, as long as the offline version of the combinatorial problem can be solved efficiently. The first major contribution of the paper is that we establish a Bayes regret bound on CombLinTS and a regret bound on CombLinUCB, under reasonable assumptions. Both bounds are L-independent, and sublinear in time. The second major contribution of the paper is that we evaluate CombLinTS on a variety of problems with thousands of items, and two of these problems are based on real-world datasets. We only evaluate CombLinTS since recent literature (Chapelle & Li, 2011) suggests that Thompson sampling algorithms usually outperform UCB-like algorithms in practice. Our experimental results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines. It is worth mentioning that our derived L-independent regret bounds also hold in cases with L = \u221e. Moreover, as we will discuss in Section 7, our proposed algorithms and their analyses can be easily extended to the contextual combinatorial semibandits. Finally, we briefly review some relevant papers. Gabillon et al. (2014) and Yue & Guestrin (2011) focus on submodular maximization with linear generalization.", "startOffset": 193, "endOffset": 1719}, {"referenceID": 8, "context": "Similarly to Chen et al. (2013), in this paper, we allow the agent to use any approximation / randomized algorithm ORACLE to solve (2), and denote its solution as A\u2217 = ORACLE(E,A,w).", "startOffset": 13, "endOffset": 32}, {"referenceID": 13, "context": "In particular, it is well known that the user-item matrix is typically low-rank (Koren et al., 2009) and that the value of an item can be written as a linear combination of its position in the latent space.", "startOffset": 80, "endOffset": 100}, {"referenceID": 3, "context": "This feedback model is known as semi-bandit (Audibert et al., 2014).", "startOffset": 44, "endOffset": 67}, {"referenceID": 8, "context": "Linear Generalization As we have discussed in Section 1, many provably efficient algorithms have been developed for various combinatorial semi-bandits of form (3) (Chen et al., 2013; Gai et al., 2012; Kveton et al., 2014a; Russo & Van Roy, 2014).", "startOffset": 163, "endOffset": 245}, {"referenceID": 12, "context": "Linear Generalization As we have discussed in Section 1, many provably efficient algorithms have been developed for various combinatorial semi-bandits of form (3) (Chen et al., 2013; Gai et al., 2012; Kveton et al., 2014a; Russo & Van Roy, 2014).", "startOffset": 163, "endOffset": 245}, {"referenceID": 9, "context": "Like existing literature on linear bandits (Dani et al., 2008; Abbasi-Yadkori et al., 2011), the analysis in this paper focuses on coherent learning cases.", "startOffset": 43, "endOffset": 91}, {"referenceID": 0, "context": "Like existing literature on linear bandits (Dani et al., 2008; Abbasi-Yadkori et al., 2011), the analysis in this paper focuses on coherent learning cases.", "startOffset": 43, "endOffset": 91}, {"referenceID": 2, "context": "Furthermore, Audibert et al. (2014) has derived an \u03a9( \u221a LKn) lower bound on adversarial combinatorial semi-bandits, while Kveton et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 2, "context": "Furthermore, Audibert et al. (2014) has derived an \u03a9( \u221a LKn) lower bound on adversarial combinatorial semi-bandits, while Kveton et al. (2014a) has derived an asymptotic \u03a9(L log(n)/\u2206) gap-dependent lower bound on stochastic combinatorial semi-bandits, where \u2206 is the \u201cgap\u201d.", "startOffset": 13, "endOffset": 144}, {"referenceID": 2, "context": "Furthermore, Audibert et al. (2014) has derived an \u03a9( \u221a LKn) lower bound on adversarial combinatorial semi-bandits, while Kveton et al. (2014a) has derived an asymptotic \u03a9(L log(n)/\u2206) gap-dependent lower bound on stochastic combinatorial semi-bandits, where \u2206 is the \u201cgap\u201d. However, in many modern combinatorial semi-bandit problems, L tends to be enormous. Thus, an O( \u221a L) regret is unacceptably large in these problems. On the other hand, in many practical problems, there exists a generalization model based on which the weight of one item can be (approximately) inferred based on the weights of other items. By exploiting such generalization models, an o( \u221a L) or even an L-independent cumulative regret might be achieved. In this paper, we assume that there is a (possibly imperfect) linear generalization model across the items. Specifically, we assume that the agent knows a generalization matrix \u03a6 \u2208 RL\u00d7d s.t. w\u0304 either lies in or is \u201cclose\u201d to the subspace span [\u03a6]. We use \u03c6e to denote the transpose of the e-th row of \u03a6, and refer to it as the feature vector of item e. Without loss of generality, we assume that rank [\u03a6] = d. Similar to Wen & Van Roy (2013), we distinguish between the coherent learning cases, in which w\u0304 \u2208 span [\u03a6], and the agnostic learning cases, in which w\u0304 / \u2208 span [\u03a6].", "startOffset": 13, "endOffset": 1171}, {"referenceID": 9, "context": "We now outline the proof of Theorem 1, which is motivated by Russo & Van Roy (2013) and Dani et al. (2008). LetHt denote the \u201chistory\u201d (i.", "startOffset": 88, "endOffset": 107}, {"referenceID": 9, "context": "It is well-known that the O( \u221a d) factor is due to linear generalization (Dani et al., 2008; Abbasi-Yadkori et al., 2011), and as is discussed in the appendix (see Remark 1), the extra O( \u221a K) factor is Audibert et al.", "startOffset": 73, "endOffset": 121}, {"referenceID": 0, "context": "It is well-known that the O( \u221a d) factor is due to linear generalization (Dani et al., 2008; Abbasi-Yadkori et al., 2011), and as is discussed in the appendix (see Remark 1), the extra O( \u221a K) factor is Audibert et al.", "startOffset": 73, "endOffset": 121}, {"referenceID": 2, "context": "no generalization), Russo & Van Roy (2014) provides an O( \u221a LK log(L/K)n) upper bound on RBayes(n) when Thompson sampling is applied, and Audibert et al. (2014) provides an \u03a9( \u221a LKn) lower bound2.", "startOffset": 138, "endOffset": 161}, {"referenceID": 0, "context": ", 2008; Abbasi-Yadkori et al., 2011), and as is discussed in the appendix (see Remark 1), the extra O( \u221a K) factor is Audibert et al. (2014) focuses on the adversarial setting but the lower bound is stochastic.", "startOffset": 8, "endOffset": 141}, {"referenceID": 0, "context": "We first construct a confidence set G of \u03b8\u2217 based on the \u201cself normalized bound\u201d developed in Abbasi-Yadkori et al. (2011). Then we decompose the regret over the high-probability \u201cgood\u201d event G and the low-probability \u201cbad\u201d event \u1e20, where \u1e20 is the complement of G.", "startOffset": 94, "endOffset": 123}, {"referenceID": 0, "context": "We first construct a confidence set G of \u03b8\u2217 based on the \u201cself normalized bound\u201d developed in Abbasi-Yadkori et al. (2011). Then we decompose the regret over the high-probability \u201cgood\u201d event G and the low-probability \u201cbad\u201d event \u1e20, where \u1e20 is the complement of G. Finally, we bound the term associated with the event G based on the same worst-case bound on \u2211n t=1 \u2211 e\u2208At \u221a \u03c6e \u03a3t\u03c6e used in the analysis for CombLinTS (see Lemma 4 in Appendix A), and bound the term associated with the event \u1e20 based on a naive bound. Please refer to Appendix B for the detailed proof of Theorem 2. Notice that if we choose \u03bb = \u03c3 = 1, \u03b4 = 1/(nK), and c as the lower bound specified in Inequality (9), then the regret bound derived in Theorem 2 is also \u00d5(Kd \u221a n). Compared with the lower bound derived in Audibert et al. (2014), this bound is at most \u00d5( \u221a Kd) larger.", "startOffset": 94, "endOffset": 809}, {"referenceID": 5, "context": "fm music recommendation dataset (Cantador et al., 2011).", "startOffset": 32, "endOffset": 55}, {"referenceID": 9, "context": "We now outline the proof of Theorem 3, which is based on (Russo & Van Roy, 2013; Dani et al., 2008).", "startOffset": 57, "endOffset": 99}, {"referenceID": 9, "context": "Our analysis is motivated by the analysis in (Dani et al., 2008).", "startOffset": 45, "endOffset": 64}, {"referenceID": 0, "context": "We first construct a confidence set of \u03b8\u2217 based on the \u201cself normalized bound\u201d developed in (Abbasi-Yadkori et al., 2011).", "startOffset": 92, "endOffset": 121}, {"referenceID": 0, "context": "As we will see later, we define Vt and \u03bet to use the \u201cself normalized bound\u201d developed in (Abbasi-Yadkori et al., 2011) (see Theorem 1 of (Abbasi-Yadkori et al.", "startOffset": 90, "endOffset": 119}, {"referenceID": 0, "context": ", 2011) (see Theorem 1 of (Abbasi-Yadkori et al., 2011)).", "startOffset": 26, "endOffset": 55}, {"referenceID": 0, "context": "We now provide a high probability bound on \u2016\u03bet\u2016V \u22121 t , based on the \u201cself normalized bound\u201d proposed in (Abbasi-Yadkori et al., 2011).", "startOffset": 105, "endOffset": 134}, {"referenceID": 0, "context": "From Theorem 1 of (Abbasi-Yadkori et al., 2011), we know for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4,", "startOffset": 18, "endOffset": 47}], "year": 2017, "abstractText": "A stochastic combinatorial semi-bandit is an online learning problem where at each step a learning agent chooses a subset of ground items subject to combinatorial constraints, and then observes stochastic weights of these items and receives their sum as a payoff. In this paper, we consider efficient learning in large-scale combinatorial semi-bandits with linear generalization, and as a solution, propose two learning algorithms called Combinatorial Linear Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both algorithms are computationally efficient as long as the offline version of the combinatorial problem can be solved efficiently. We establish that CombLinTS and CombLinUCB are also provably statistically efficient under reasonable assumptions, by developing regret bounds that are independent of the problem scale (number of items) and sublinear in time. We also evaluate CombLinTS on a variety of problems with thousands of items. Our experiment results demonstrate that CombLinTS is scalable, robust to the choice of algorithm parameters, and significantly outperforms the best of our baselines.", "creator": "LaTeX with hyperref package"}}}