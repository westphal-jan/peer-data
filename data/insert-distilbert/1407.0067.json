{"id": "1407.0067", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2014", "title": "Rates of Convergence for Nearest Neighbor Classification", "abstract": "nearest neighbor methods are a popular class description of nonparametric estimators with several desirable properties, such as adaptivity to scaling different distance scales in different regions of space. prior development work on convergence rates for nearest kin neighbor classification has not stopped fully realised reflected these subtle properties. mainly we analyze the behavior of these estimators in noisy metric learning spaces and provide finite - sample, distribution - dependent rates of convergence under minimal assumptions. as a by - product, we are able to establish the universal empirical consistency of nearest neighbor in a broader range of data spaces than was previously known. we illustrate our upper and lower bounds then by introducing smoothness classes that are customized for nearest neighbor problem classification.", "histories": [["v1", "Mon, 30 Jun 2014 22:00:57 GMT  (36kb,D)", "https://arxiv.org/abs/1407.0067v1", null], ["v2", "Wed, 2 Jul 2014 00:44:29 GMT  (37kb,D)", "http://arxiv.org/abs/1407.0067v2", null]], "reviews": [], "SUBJECTS": "cs.LG math.ST stat.ML stat.TH", "authors": ["kamalika chaudhuri", "sanjoy dasgupta"], "accepted": true, "id": "1407.0067"}, "pdf": {"name": "1407.0067.pdf", "metadata": {"source": "CRF", "title": "Rates of Convergence for Nearest Neighbor Classification", "authors": ["Kamalika Chaudhuri", "Sanjoy Dasgupta"], "emails": ["kamalika@cs.ucsd.edu", "dasgupta@cs.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we deal with binary prediction in metric spaces. A classification problem is defined by a metric space (X , \u03c1) from which instances are drawn, a space of possible labels Y = {0, 1}, and a distribution P overX \u00d7Y . The goal is to find a function h : X \u2192 Y that minimizes the probability of error on pairs (X,Y ) drawn from P; this error rate is the risk R(h) = P(h(X) 6= Y ). The best such function is easy to specify: if we let \u00b5 denote the marginal distribution of X and \u03b7 the conditional probability \u03b7(x) = P(Y = 1|X = x), then the predictor 1(\u03b7(x) \u2265 1/2) achieves the minimum possible risk, R\u2217 = EX [min(\u03b7(X), 1 \u2212 \u03b7(X))]. The trouble is that P is unknown and thus a prediction rule must instead be based only on a finite sample of points (X1, Y1), . . . , (Xn, Yn) drawn independently at random from P.\nNearest neighbor (NN) classifiers are among the simplest prediction rules. The 1-NN classifier assigns each point x \u2208 X the label Yi of the closest point in X1, . . . , Xn (breaking ties arbitrarily, say). For a positive integer k, the k-NN classifier assigns x the majority label of the k closest points in X1, . . . , Xn. In the latter case, it is common to let k grow with n, in which case the sequence (kn : n \u2265 1) defines a kn-NN classifier. The asymptotic consistency of nearest neighbor classification has been studied in detail, starting with the work of Fix and Hodges [7]. The risk Rn is a random variable that depends on the training sample (X1, Y1), . . . , (Xn, Yn); the usual order of business is to first determine the limiting behavior of the expected value ERn and to then study stronger modes of convergence of Rn. Cover and Hart [2] studied the asymptotics of ERn in general metric spaces, under the assumption that every x in the support of \u00b5 is either a continuity point of \u03b7 or has \u00b5({x}) > 0. For the 1-NN classifier, they found that ERn \u2192 EX [2\u03b7(X)(1 \u2212 \u03b7(X))] \u2264 2R\u2217(1 \u2212 R\u2217); for kn-NN with kn \u2191 \u221e and kn/n \u2193 0, they found\nar X\niv :1\n40 7.\n00 67\nv2 [\ncs .L\nG ]\n2 J\nul 2\n01 4\nERn \u2192 R\u2217. For points in Euclidean space, a series of results starting with Stone [15] established consistency without any distributional assumptions. For kn-NN in particular, Rn \u2192 R\u2217 almost surely [5]. These consistency results place nearest neighbor methods in a favored category of nonparametric estimators. But for a fuller understanding it is important to also have rates of convergence. For instance, part of the beauty of nearest neighbor is that it appears to adapt automatically to different distance scales in different regions of space. It would be helpful to have bounds that encapsulate this property.\nRates of convergence are also important in extending nearest neighbor classification to settings such as active learning, semisupervised learning, and domain adaptation, in which the training data is not a fullylabeled data set obtained by i.i.d. sampling from the future test distribution. For instance, in active learning, the starting point is a set of unlabeled points X1, . . . , Xn, and the learner requests the labels of just a few of these, chosen adaptively to be as informative as possible about \u03b7. There are many natural schemes for deciding which points to label: for instance, one could repeatedly pick the point furthest away from the labeled points so far, or one could pick the point whose k nearest labeled neighbors have the largest disagreement among their labels. The asymptotics of such selective sampling schemes has been considered in earlier work [4], but ultimately the choice of scheme must depend upon finite-sample behavior. The starting point for understanding this behavior is to first obtain a characterization in the non-active setting."}, {"heading": "1.1 Previous work on rates of convergence", "text": "The earliest rates of convergence for nearest neighbor were distribution-free. Cover [3] studied the 1-NN classifier in the case X = R, under the assumption of class-conditional densities with uniformly-bounded third derivatives. He showed that ERn converges at a rate of O(1/n2). Wagner [18] and later Fritz [8] also looked at 1-NN, but in higher dimension X = Rd. The latter obtained an asymptotic rate of convergence for Rn under the milder assumption of non-atomic \u00b5 and lower semi-continuous class-conditional densities.\nDistribution-free results are of some value, but fail to precisely characterize which properties of a distribution most influence the performance of nearest neighbor classification. More recent work has investigated several different approaches to obtaining distribution-dependent bounds. Kulkarni and Posner [12] obtained finitesample rates of convergence for 1-NN and kn-NN in terms of the smoothness of \u03b7. They assumed that for some constants K and \u03b1, and for all x1, x2 \u2208 X , |\u03b7(x1)\u2212 \u03b7(x2)| \u2264 K\u03c1(x1, x2)2\u03b1. They then gave bounds in terms of the Holder parameter \u03b1. Gyorfi [9] looked at the case X = Rd, under the weaker assumption that for some function K : Rd \u2192 R and some \u03b1, and for all z \u2208 Rd and all r > 0,\u2223\u2223\u2223\u2223\u2223\u03b7(z)\u2212 1\u00b5(B(z, r)) \u222b B(z,r) \u03b7(x)\u00b5(dx)\n\u2223\u2223\u2223\u2223\u2223 \u2264 K(z)r\u03b1. This \u03b1 is similar in spirit to the earlier Holder parameter, but does not require \u03b7 to be continuous. Gyorfi obtained asymptotic rates in terms of \u03b1. Another generalization of standard smoothness conditions was proposed recently [17] in a \u201cprobabilistic Lipschitz\u201d assumption, and in this setting rates were obtained for NN classification in bounded spaces X \u2282 Rd. The convergence rates obtained so far have been unsatisfactory in several regards. The finite-sample rates require continuity and thus, for instance, do not apply to discrete distributions. The use of a single Holder parameter is convenient but does not capture cases where different regions of the data space have different distance-scales: a common situation in which NN methods might be expected to shine. Most importantly, what is crucial for nearest neighbor is not how |\u03b7(x) \u2212 \u03b7(x\u2032)| scales with \u03c1(x, x\u2032)\u2014which is what any Lipschitz or Holder constant captures\u2014but rather how it scales with \u00b5(B(x, \u03c1(x, x\u2032))). In other words, a suitable smoothness parameter for NN is one that measures the change in \u03b7(x) with respect to probability mass rather than distance. We will try to make this point clearer in the next section."}, {"heading": "1.2 Some illustrative examples", "text": "We now look at a few examples to get a sense of what properties of a distribution most critically affect the convergence rate of nearest neighbor. In each case, we study the k-NN classifier.\nAs a first example, consider a finite instance space X . For large enough n, the k nearest neighbors of a query x will all be x itself, leading immediately to an error bound. However, this kind of reasoning yields an asymptotic rate of convergence. To get a finite-sample rate, we proceed more generally and observe that for any number of points n, the k nearest neighbors of x will lie within a ball B = B(x, r) whose probability mass under \u00b5 is roughly k/n. The quality of the prediction can be assessed by how much \u03b7 varies within this ball. To be slightly more precise, let \u03b7(B) = (1/\u00b5(B)) \u222b B \u03b7(x)\u00b5(dx) denote the average \u03b7 value within the ball. For the k-NN prediction at x to be good, we require that if \u03b7(x) is significantly more than 1/2 then so is \u03b7(B); and likewise if \u03b7(x) is significantly less than 1/2.\nAs a second example, consider a distribution over X = R in which the two classes (Y = 0 and Y = 1) have class-conditional densities \u00b51 and \u00b52, respectively. Assume that these two distributions are supported on disjoint intervals, as shown on the left side of Figure 1. Now let\u2019s determine the probability that the k-NN classifier makes a mistake on a specific query x. Clearly, this will happen only if x is near the boundary between the two classes. To be precise, consider an interval around x of probability mass k/n, that is, an interval B = [x\u2212 r, x+ r] with \u00b5(B) = k/n. Then the k nearest neighbors will lie roughly in this interval, and there will likely be an error only if the interval contains a substantial portion of the wrong class. Whether or not \u03b7 is smooth, or the \u00b5i are smooth, is irrelevant.\nIt should already be clear that the central objects in analyzing k-NN are balls of probability mass \u2248 k/n, specifically those near the decision boundary. Now let\u2019s see a variant of the previous example (Figure 1, right) in which it is no longer the case that \u03b7 \u2208 {0, 1}. Although one of the class-conditional densities in the figure is highly non-smooth, this erratic behavior occurs far from the decision boundary and thus does not affect nearest neighbor performance. And in the vicinity of the boundary, what matters is not how much \u03b7 varies within intervals of any given radius r, but rather within intervals of probability mass k/n.\nThese examples hopefully clarify that rates of convergence based only on Holder-continuity of \u03b7\u2014or similar notions\u2014are inadequate for properly characterizing the statistical behavior of nearest neighbor classifiers."}, {"heading": "1.3 Results of this paper", "text": "Let us return to our earlier setting of pairs (X,Y ), where X takes values in a metric space (X , \u03c1) and has distribution \u00b5, while Y \u2208 {0, 1} has conditional probability function \u03b7(x) = Pr(Y = 1|X = x). We obtain rates of convergence for k-NN by attempting to make precise the intuitions discussed above. This leads to a somewhat different style of analysis than has been used in earlier work.\nFor any positive integers k \u2264 n, we define a notion of effective boundary for k-NN under sample size n. For the moment, denote this set by An,k \u2282 X .\n\u2022 We show that with high probability over the training data, the misclassification rate of the k-NN classifier (with respect to the Bayes-optimal classifer) is bounded above by \u00b5(An,k) plus a small additional term that can be made arbitrarily small.\n\u2022 We identify a general condition under which, as n and k grow, An,k approaches the actual decision boundary {x | \u03b7(x) = 1/2}. This yields universal consistency in a broader range of metric spaces than just Rd.\n\u2022 We give a lower bound on the error probability using a different notion of effective boundary. \u2022 We introduce a Holder-like smoothness condition that is tailored to nearest neighbor. We compare\nour upper and lower bounds under this kind of smoothness. \u2022 We obtain risk bounds under the margin condition of Tsybakov that match the best known results\nfor nonparametric classification. \u2022 We look at additional specific cases of interest: when \u03b7 is bounded away from 1/2, and the even\nmore extreme scenario where \u03b7 \u2208 {0, 1} (zero Bayes risk)."}, {"heading": "2 Definitions and results", "text": "Let (X , \u03c1) denote a separable metric space, and \u00b5 a Borel regular probability measure on this space (that is, open sets are measurable, and every set is contained in a Borel set of the same measure) from which instances X are drawn. The label of an instance X = x is Y \u2208 {0, 1} and is distributed according to the conditional probability function \u03b7 : X \u2192 [0, 1] as follows: Pr(Y = 1|X = x) = \u03b7(x).\nGiven a training set S = ((X1, Y1), . . . , (Xn, Yn)) and a query point x \u2208 X , we use the notation X(i)(x) to denote the i-th nearest neighbor of x in the training set, and Y (i)(x) to denote its label. Distances are calculated with respect to the given metric \u03c1, and ties are broken by preferring points earlier in the sequence. The k-NN classifier is defined by\ngn,k(x) = { 1 if Y (1)(x) + \u00b7 \u00b7 \u00b7+ Y (k)(x) \u2265 k/2 0 otherwise\nWe analyze the performance of gn,k by comparing it with g(x) = 1(\u03b7(x) \u2265 1/2), the omniscent Bayesoptimal classifier. Specifically, we obtain bounds on PrX(gn,k(X) 6= g(X)) that hold with high probability over the choice of training data S."}, {"heading": "2.1 Definitions", "text": "We begin with some definitions and notation.\nThe radius and probability-radius of a ball. For any x \u2208 X , let\nBo(x, r) = {x\u2032 \u2208 X | \u03c1(x, x\u2032) < r} and B(x, r) = {x\u2032 \u2208 X | \u03c1(x, x\u2032) \u2264 r}\ndenote the open and closed balls, respectively, of radius r centered at x. We will mostly be dealing with balls that contain a prescribed probability mass. To this end, for any x \u2208 X and any 0 \u2264 p \u2264 1, define\nrp(x) = inf{r | \u00b5(B(x, r)) \u2265 p}.\nThus \u00b5(B(x, rp(x))) \u2265 p (Lemma 22), and rp(x) is the smallest radius for which this holds.\nThe support of \u00b5. The support of distribution \u00b5 is defined as\nsupp(\u00b5) = {x \u2208 X | \u00b5(B(x, r)) > 0 for all r > 0}.\nIt was shown by [2] that in separable metric spaces, \u00b5(supp(\u00b5)) = 1. For the interested reader, we reproduce their brief proof in the appendix (Lemma 23).\nThe conditional probability function for a set. The conditional probability function \u03b7 is defined for points x \u2208 X , and can be extended to measurable sets A \u2282 X with \u00b5(A) > 0 as follows:\n\u03b7(A) = 1\n\u00b5(A) \u222b A \u03b7 d\u00b5. (1)\nThis is the probability that Y = 1 for a point X chosen at random from the distribution \u00b5 restricted to set A. We exclusively consider sets A of the form B(x, r), in which case \u03b7 is defined whenever x \u2208 supp(\u00b5).\nThe effective interiors of the two classes, and the effective boundary. When asked to make a prediction at point x, the k-NN classifier finds the k nearest neighbors, which can be expected to lie in B(x, rp(x)) for p \u2248 k/n. It then takes an average over these k labels, which has a standard deviation of \u2206 \u2248 1/ \u221a k. With this in mind, there is a natural definition for the effective interior of the Y = 1 region: the points x with \u03b7(x) > 1/2 on which the k-NN classifier is likely to be correct:\nX+p,\u2206 = {x \u2208 supp(\u00b5) | \u03b7(x) > 1 2 , \u03b7(B(x, r)) \u2265 1 2 + \u2206 for all r \u2264 rp(x)}.\nThe corresponding definition for the Y = 0 region is\nX\u2212p,\u2206 = {x \u2208 supp(\u00b5) | \u03b7(x) < 1 2 , \u03b7(B(x, r)) \u2264 1 2 \u2212\u2206 for all r \u2264 rp(x)}.\nThe remainder of X is the effective boundary,\n\u2202p,\u2206 = X \\ (X+p,\u2206 \u222a X \u2212 p,\u2206).\nObserve that \u2202p\u2032,\u2206\u2032 \u2282 \u2202p,\u2206 whenever p\u2032 \u2264 p and \u2206\u2032 \u2264 \u2206. Under mild conditions, as p and \u2206 tend to zero, the effective boundary tends to the actual decision boundary {x | \u03b7(x) = 1/2} (Lemma 12), which we shall denote \u2202o."}, {"heading": "2.2 A general bound on the misclassification error", "text": "We begin with a general upper bound on the misclassification rate of the k-NN classifier. We will later specialize it to various situations of interest. All proofs appear in the appendix.\nTheorem 1. Pick any 0 < \u03b4 < 1 and positive integers k < n. Let gn,k denote the k-NN classifier based on n training points, and g(x) the Bayes-optimal classifier. With probability at least 1 \u2212 \u03b4 over the choice of training data,\nPrX(gn,k(X) 6= g(X)) \u2264 \u03b4 + \u00b5 ( \u2202p,\u2206 ) ,\nwhere\np = k n \u00b7 1 1\u2212 \u221a (4/k) ln(2/\u03b4) , and \u2206 = min\n( 1\n2 ,\n\u221a 1\nk ln\n2\n\u03b4\n) .\nConvergence results for nearest neighbor have traditionally studied the excess riskRn,k\u2212R\u2217, whereRn,k = Pr(Y 6= gn,k(X)). If we define the pointwise quantities\nRn,k(x) = Pr(Y 6= gn,k(x)|X = x) R\u2217(x) = min(\u03b7(x), 1\u2212 \u03b7(x)),\nfor all x \u2208 X , we see that Rn,k(x)\u2212R\u2217(x) = |1\u2212 2\u03b7(x)|1(gn,k(x) 6= g(x)). (2) Taking expectation over X , we then have Rn,k \u2212 R\u2217 \u2264 PrX(gn,k(X) 6= g(X)), and so Theorem 1 is also an upper bound on the excess risk.\nTo obtain an asymptotic result, we can take a sequence of integers (kn) and reals (\u03b4n) for which the corresponding pn,\u2206n \u2193 0. As we will see, this implies that \u2202pn,\u2206n converges to the decision boundary, \u2202o."}, {"heading": "2.3 Universal consistency", "text": "A series of results, starting with [15], has shown that kn-NN is strongly consistent (Rn = Rn,kn \u2192 R\u2217 almost surely) when X is a finite-dimensional Euclidean space and \u00b5 is a Borel measure. A consequence of Theorem 1 is that this phenomenon holds quite a bit more generally. In fact, strong consistency holds in any metric measure space (X , \u03c1, \u00b5) for which the Lebesgue differentiation theorem is true: that is, spaces in which, for any bounded measurable f ,\nlim r\u21930\n1\n\u00b5(B(x, r)) \u222b B(x,r) f d\u00b5 = f(x) (3)\nfor almost all (\u00b5-a.e.) x \u2208 X . For more details on this differentiation property, see [6, 2.9.8] and [10, 1.13]. It holds, for instance:\n\u2022 When (X , \u03c1) is a finite-dimensional normed space [10, 1.15(a)]. \u2022 When (X , \u03c1, \u00b5) is doubling [10, 1.8], that is, when there exists a constant C(\u00b5) such that \u00b5(B(x, 2r)) \u2264 C(\u00b5)\u00b5(B(x, r)) for every ball B(x, r).\n\u2022 When \u00b5 is an atomic measure on X . Theorem 2. Suppose metric measure space (X , \u03c1, \u00b5) satisfies differentiation condition (3). Pick a sequence of positive integers (kn), and for each n, let Rn = Rn,kn be the risk of the kn-NN classifier gn,kn .\n1. If kn \u2192\u221e and kn/n\u2192 0, then for all > 0, lim n\u2192\u221e Prn(Rn \u2212R\u2217 > ) = 0.\nHere Prn denotes probability over the training set (X1, Y1), . . . , (Xn, Yn).\n2. If in addition kn/(log n)\u2192\u221e, then Rn \u2192 R\u2217 almost surely."}, {"heading": "2.4 A lower bound", "text": "Next, we give a counterpart to Theorem 1 that lower-bounds the expected probability of error of gn,k. For any positive integers k < n, define the high-error set En,k = E+n,k \u222a E \u2212 n,k, where\nE+n,k = { x \u2208 supp(\u00b5) | \u03b7(x) > 1\n2 , \u03b7(B(x, r)) \u2264 1 2 + 1\u221a k\nfor all rk/n(x) \u2264 r \u2264 r(k+\u221ak+1)/n(x) }\nE\u2212n,k = { x \u2208 supp(\u00b5) | \u03b7(x) < 1\n2 , \u03b7(B(x, r)) \u2265 1 2 \u2212 1\u221a k for all rk/n(x) \u2264 r \u2264 r(k+\u221ak+1)/n(x)\n} .\n(Recall the definition (1) of \u03b7(A) for sets A.) We will see that for smooth \u03b7 this region is comparable to the effective decision boundary \u2202k/n,1/\u221ak. Meanwhile, here is a lower bound that applies to any (X , \u03c1, \u00b5). Theorem 3. For any positive integers k < n, let gn,k denote the k-NN classifier based on n training points. There is an absolute constant co such that the expected misclassification rate satisfies EnPrX(gn,k(X) 6= g(X)) \u2265 co \u00b5(En,k), where En is expectation over the choice of training set."}, {"heading": "2.5 Smooth measures", "text": "For the purposes of nearest neighbor, it makes sense to define a notion of smoothness with respect to the marginal distribution on instances. We use a variant of Holder-continuity: for \u03b1,L > 0, we say the conditional probability function \u03b7 is (\u03b1,L)-smooth in metric measure space (X , \u03c1, \u00b5) if for all x, x\u2032 \u2208 X , |\u03b7(x)\u2212 \u03b7(x\u2032)| \u2264 L\u00b5(Bo(x, \u03c1(x, x\u2032)))\u03b1. This is stated to resemble standard smoothness conditions, but what we will really need is the weaker assertion that for all x \u2208 supp(\u00b5) and all r > 0,\n|\u03b7(B(x, r))\u2212 \u03b7(x)| \u2264 L\u00b5(Bo(x, r))\u03b1.\nIn such circumstances, the earlier upper and lower bounds on generalization error take on a more easily interpretable form. Recall that the key term in the upper bound (Theorem 1) is \u00b5(\u2202p,\u2206), for p \u2248 k/n and \u2206 \u2248 1/ \u221a k. Lemma 4. If \u03b7 is (\u03b1,L)-smooth in (X , \u03c1, \u00b5), then for any p,\u2206 \u2265 0,\n\u2202p,\u2206 \u2229 supp(\u00b5) \u2282 { x \u2208 X \u2223\u2223\u2223\u2223 \u2223\u2223\u03b7(x)\u2212 12 \u2223\u2223 \u2264 \u2206 + Lp\u03b1 } .\nThis yields a bound on PrX(gn,k(X) 6= g(X)) that is roughly of the form \u00b5({x | |\u03b7(x) \u2212 1/2| \u2264 k\u22121/2 + L(k/n)\u03b1). The optimal setting of k is then \u223c n2\u03b1/(2\u03b1+1). The key term in the lower bound of Theorem 3 is \u00b5(En,k). Under the smoothness condition, it becomes directly comparable to the upper bound. Lemma 5. If \u03b7 is (\u03b1,L)-smooth in (X , \u03c1, \u00b5), then for any k, n,\nEn,k \u2283 { x \u2208 supp(\u00b5) \u2223\u2223\u2223\u2223 \u03b7(x) 6= 12 , |\u03b7(x)\u2212 12 | \u2264 1\u221ak \u2212 L ( k + \u221a k + 1 n )\u03b1} .\nIt is common to analyze nonparametric classifiers under the assumption that X = Rd and that \u03b7 is \u03b1H - Holder continuous for some \u03b1 > 0, that is,\n|\u03b7(x)\u2212 \u03b7(x\u2032)| \u2264 L\u2016x\u2212 x\u2032\u2016\u03b1H\nfor some constant L. These bounds typically also require \u00b5 to have a density that is uniformly bounded (above and/or below). We now relate these assumptions to our notion of smoothness. Lemma 6. Suppose that X \u2282 Rd, and \u03b7 is \u03b1H -Holder continuous, and \u00b5 has a density with respect to Lebesgue measure that is\u2265 \u00b5min on X . Then there is a constant L such that for any x \u2208 supp(\u00b5) and r > 0 with B(x, r) \u2282 X ,\n|\u03b7(x)\u2212 \u03b7(B(x, r))| \u2264 L\u00b5(Bo(x, r))\u03b1H/d.\n(To remove the requirement that B(x, r) \u2282 X , we would need the boundary of X to be well-behaved, for instance by requiring that X contains a constant fraction of every ball centered in it. This is a familiar assumption in results on nonparametric classification, including the seminal work of [1] that we discuss in the next section.)\nOur smoothness condition for nearest neighbor problems can thus be seen as a generalization of the usual Holder conditions. It applies in broader range of settings, for example for discrete \u00b5."}, {"heading": "2.6 Margin bounds", "text": "An achievement of statistical theory in the past two decades has been margin bounds, which give fast rates of convergence for many classifiers when the underlying data distribution P (given by \u00b5 and \u03b7) satisfies a large margin condition stipulating, roughly, that \u03b7 moves gracefully away from 1/2 near the decision boundary.\nFollowing [13, 16, 1], for any \u03b2 \u2265 0, we say P satisfies the \u03b2-margin condition if there exists a constant C > 0 such that\n\u00b5 ({ x \u2223\u2223\u2223 \u2223\u2223\u03b7(x)\u2212 1\n2 \u2223\u2223 \u2264 t}) \u2264 Ct\u03b2 . Larger \u03b2 implies a larger margin. We now obtain bounds for the misclassification rate and the excess risk of k-NN under smoothness and margin conditions. Theorem 7. Suppose \u03b7 is (\u03b1,L)-smooth in (X , \u03c1, \u00b5) and satisfies the \u03b2-margin condition (with constant C), for some \u03b1, \u03b2, L,C \u2265 0. In each of the two following statements, ko and Co are constants depending on \u03b1, \u03b2, L,C.\n(a) For any 0 < \u03b4 < 1, set k = kon2\u03b1/(2\u03b1+1)(log(1/\u03b4))1/(2\u03b1+1). With probability at least 1\u2212 \u03b4 over the choice of training data,\nPrX(gn,k(X) 6= g(X)) \u2264 \u03b4 + Co ( log(1/\u03b4)\nn\n)\u03b1\u03b2/(2\u03b1+1) .\n(b) Set k = kon2\u03b1/(2\u03b1+1). Then EnRn,k \u2212R\u2217 \u2264 Con\u2212\u03b1(\u03b2+1)/(2\u03b1+1).\nIt is instructive to compare these bounds with the best known rates for nonparametric classification under the margin assumption. The work of [1] (Theorems 3.3 and 3.5) shows that when (X , \u03c1) = (Rd, \u2016 \u00b7 \u2016), and \u03b7 is \u03b1H -Holder continuous, and \u00b5 lies in the range [\u00b5min, \u00b5max] for some \u00b5max > \u00b5min > 0, and the \u03b2-margin condition holds (along with some other assumptions), an excess risk of n\u2212\u03b1H(\u03b2+1)/(2\u03b1H+d) is achievable and is also the best possible. This is exactly the rate achieved by nearest neighbor classification, once we translate between the different notions of smoothness as per Lemma 6.\nAnother interesting scenario is when \u03b7 is bounded away from 1/2, that is, there exists some \u2206\u2217 for which\n\u00b5 ({ x \u2223\u2223\u2223 \u2223\u2223\u03b7(x)\u2212 1\n2 \u2223\u2223 \u2264 \u2206\u2217}) = 0 It follows from Lemma 4 that if \u03b7 is (\u03b1,L)-smooth in (X , \u03c1, \u00b5), then \u00b5(\u2202p,\u2206) = 0 whenever \u2206+Lp\u03b1 \u2264 \u2206\u2217. Invoking Theorem 1 with\nk = n\n2\n( \u2206\u2217\n2L\n)1/\u03b1 , \u03b4 = 2e\u2212k(\u2206 \u2217)2/4,\nyields an exponentially fast rate of convergence: Pr(gn(X) 6= g(X)) \u2264 2e\u2212Con, where Co = (\u2206\u2217)2+1/\u03b1/(8(2L)1/\u03b1).\nA final case of interest is when \u03b7 \u2208 {0, 1}, so that the Bayes risk R\u2217 is zero. We treat this in Section 2.14 in the appendix."}, {"heading": "Appendix: Analysis", "text": ""}, {"heading": "2.7 A tie-breaking mechanism", "text": "In some situations, such as discrete instance spaces, there is a non-zero probability that two or more of the training points will be equidistant from the query point. In practice, we break ties by a simple rule such as preferring points that appear earlier in the sequence. To accurately reflect this in the analysis, we adopt the following mechanism: for each training point X , we also draw a value Z independently and uniformly at random from [0, 1]. When breaking ties, points with lower Z value are preferred. We use the notation X \u2032 = (X,Z) \u2208 X \u00d7 [0, 1] to refer to the augmented training instances, drawn from the product measure \u00b5\u00d7 \u03bd, where \u03bd is Lebesgue measure on [0, 1]. Given a query point x \u2208 X and training points X \u20321, . . . , X \u2032n \u2208 X \u00d7 [0, 1], let X \u2032(1)(x), . . . , X \u2032 (n)(x) denote a reordering of these points by increasing distance from x, where each X \u2032(i) is of the form (X(i), Z(i)). With probability 1, this ordering is unambiguous. Also, let Y(1)(x), . . . , Y(n)(x) be the corresponding labels.\nWe will need to consider balls in the augmented space. For xo \u2208 X , ro \u2265 0, and zo \u2208 [0, 1], define B\u2032(xo, ro, zo) = {(x, z) \u2208 X \u00d7 [0, 1] | either \u03c1(xo, x) < ro or (\u03c1(xo, x) = ro and z < zo)}\n= ( Bo(xo, ro)\u00d7 [0, 1] )\u22c3( (B(xo, ro) \\Bo(xo, ro))\u00d7 [0, zo) ) .\nGiven a set of training points (X \u2032i, Yi) and an augmented ball B \u2032 = B\u2032(xo, ro, zo), let Y\u0302 (B\u2032) denote the mean of the Yi for points X \u2032i \u2208 B\u2032; if there is no X \u2032i \u2208 B\u2032, then this is undefined. Let \u03b7(B\u2032) denote the mean probability that Y = 1 for points (x, z) \u2208 B\u2032; formally, it is given by\n\u03b7(B\u2032) = 1\n(\u00b5\u00d7 \u03bd)(B\u2032) \u222b B\u2032 \u03b7 d(\u00b5\u00d7 \u03bd)\nwhenever (\u00b5\u00d7 \u03bd)(B\u2032) > 0. Here \u03b7(x, z) is defined to be \u03b7(x). The ball B\u2032 = B(xo, ro, zo) in the augmented space can be thought of as lying between the open ball Bo = Bo(xo, ro) and the closed ball B = B(xo, ro) in the original space; and indeed \u03b7(B\u2032) is a convex combination of \u03b7(B) and \u03b7(Bo) (Lemma 24)."}, {"heading": "2.8 Proof of Theorem 1", "text": "Theorem 1 rests on the following basic observation. Lemma 8. Let gn,k denote the k-NN classifier based on training data (X \u20321, Y1), . . . , (X \u2032n, Yn). Pick any xo \u2208 X and any 0 \u2264 p \u2264 1, 0 \u2264 \u2206 \u2264 1/2. Let B\u2032 = B\u2032(xo, \u03c1(xo, X(k+1)(xo)), Z(k+1)). Then\n1(gn,k(xo) 6= g(xo)) \u2264 1(xo \u2208 \u2202p,\u2206) + 1(\u03c1(xo, X(k+1)(xo)) > rp(xo)) +\n1(|Y\u0302 (B\u2032)\u2212 \u03b7(B\u2032)| \u2265 \u2206).\nProof. Suppose xo 6\u2208 \u2202p,\u2206. Then, without loss of generality, xo lies in X+p,\u2206, whereupon \u03b7(B(xo, r)) \u2265 1/2 + \u2206 for all r \u2264 rp(xo). Next, suppose r = \u03c1(xo, X(k+1)(xo)) \u2264 rp(xo). Then \u03b7(B(xo, r)) and \u03b7(Bo(xo, r)) are both \u2265 1/2 + \u2206 (Lemma 25). By Lemma 24, \u03b7(B\u2032) is a convex combination of these and is thus also \u2265 1/2 + \u2206.\nThe prediction gn,k(xo) is based on the average of the Yi values of the k points closest to xo, in other words, Y\u0302 (B\u2032). If this average differs from \u03b7(B\u2032) by less than \u2206, then it is > 1/2, whereupon the prediction is correct.\nWhen we take expectations in the inequality of Lemma 8, we see that there are three probabilities to be bounded. The second of these, the probability that \u03c1(xo, X(k+1)(xo)) > rp(xo), can easily be controlled when p is sufficiently large.\nLemma 9. Fix any xo \u2208 X and 0 \u2264 p, \u03b3 \u2264 1. Pick any positive integer k \u2264 (1\u2212 \u03b3)np. Let X1, . . . , Xn be chosen uniformly at random from \u00b5. Then\nPrn(\u03c1(xo, X(k+1)(xo)) > rp(xo)) \u2264 e\u2212np\u03b3 2/2 \u2264 e\u2212k\u03b3 2/2.\nProof. The probability that any givenXi falls inB(xo, rp(xo)) is at least p (Lemma 22). The probability that \u2264 k \u2264 (1\u2212 \u03b3)np of them land in this ball is, by the multiplicative Chernoff bound, at most e\u2212np\u03b32/2.\nTo bound the probability that Y\u0302 (B\u2032) differs substantially from \u03b7(B\u2032), a slightly more careful argument is needed.\nLemma 10. Fix any xo \u2208 X and any 0 \u2264 \u2206 \u2264 1/2. Draw (X1, Z1, Y1), . . . , (Xn, Zn, Yn) independently at random and let B\u2032 = B\u2032(xo, \u03c1(xo, X(k+1)(xo)), Z(k+1)) \u2282 X \u00d7 [0, 1]. Then\nPrn(|Y\u0302 (B\u2032)\u2212 \u03b7(B\u2032)| \u2265 \u2206) \u2264 2e\u22122k\u2206 2 .\nMoreover, if \u03b7(B\u2032) \u2208 {0, 1} then Y\u0302 (B\u2032) = \u03b7(B\u2032) with probability one.\nProof. We will pick the points X \u2032i = (Xi, Zi) and their labels Yi in the following manner:\n1. First pick a point (X1, Z1) \u2208 X \u00d7 [0, 1] according to the marginal distribution of the (k + 1)st nearest neighbor of xo.\n2. Pick k points uniformly at random from the distribution \u00b5 \u00d7 \u03bd restricted to B\u2032 = B\u2032(xo, \u03c1(xo, X1), Z1).\n3. Pick n\u2212k\u22121 points uniformly at random from the distribution \u00b5\u00d7\u03bd restricted to (X \u00d7 [0, 1])\\B\u2032.\n4. Randomly permute the n points obtained in this way.\n5. For each (Xi, Zi) in the permuted order, pick a label Yi from the conditional distribution \u03b7(Xi).\nThe k nearest neighbors of xo are the points picked in step 2. Their Y values are independent and identically distributed with expectation \u03b7(B\u2032). The main bound in the lemma now follows from a direct application of Hoeffding\u2019s inequality.\nThe final statement of the lemma is trivial and is needed to cover situations in which \u2206 = 1/2.\nWe now complete the proof of Theorem 1. Adopt the settings of p and \u2206 from the theorem statement, and define the central bad event to be\nBAD(Xo, X \u2032 1, . . . , X \u2032 n, Y1, . . . , Yn) = 1(\u03c1(Xo, X(k+1)(Xo)) > rp(Xo)) + 1(|Y\u0302 (B\u2032)\u2212 \u03b7(B\u2032)| \u2265 \u2206),\nwhere B\u2032 is a shorthand for B\u2032(Xo, \u03c1(Xo, X(k+1)(Xo)), Z(k+1)), as before. Fix any xo \u2208 X . If \u2206 < 1/2, then by Lemmas 9 and 10,\nEnBAD(xo, X \u20321, . . . , X \u2032n, Y1, . . . , Yn) \u2264 exp(\u2212k\u03b32/2) + 2 exp(\u22122k\u22062) \u2264 \u03b42, where \u03b3 = 1\u2212(k/np) = \u221a\n(4/k) ln(2/\u03b4) and En is expectation over the choice of training data. If \u2206 = 1/2 then \u03b7(B\u2032) \u2208 {0, 1} and we have\nEnBAD(xo, X \u20321, . . . , X \u2032n, Y1, . . . , Yn) \u2264 exp(\u2212k\u03b32/2) \u2264 \u03b42. Taking expectation over Xo, EXoEnBAD(Xo, X \u20321, . . . , X \u2032n, Y1, . . . , Yn) \u2264 \u03b42, from which, by switching expectations and applying Markov\u2019s inequality, we have\nPrn(EXo BAD(Xo, X \u20321, . . . , X \u2032n, Y1, . . . , Yn) \u2265 \u03b4) \u2264 \u03b4. The theorem then follows by writing the result of Lemma 8 as\nPrXo(gn,k(Xo) 6= g(Xo)) \u2264 \u00b5(\u2202p,\u2206) + EXo BAD(Xo, X \u20321, . . . , X \u2032n, Y1, . . . , Yn)."}, {"heading": "2.9 Proof of Theorem 2", "text": "Recall that we define Rn = PrX(gn,kn(X) 6= Y ). From equation (2), we have: Rn \u2212R\u2217 \u2264 PrX(\u03b7(X) 6= 1/2 and gn,kn(X) 6= g(X)). Defining \u2202o = {x \u2208 X | \u03b7(x) = 1/2} to be the decision boundary, we then have the following corollary of Theorem 1. Corollary 11. Let (\u03b4n) be any sequence of positive reals, and (kn) any sequence of positive integers. For each n, define (pn) and (\u2206n) as in Theorem 1. Then\nPrn ( Rn \u2212R\u2217 > \u03b4n + \u00b5 ( \u2202pn,\u2206n \\ \u2202o )) \u2264 \u03b4n,\nwhere Prn is probability over the choice of training data.\nFor the rest of the proof, assume that (X , \u03c1, \u00b5) satisfies Lebesgue\u2019s differentiation theorem: that is, for any bounded measurable f : X \u2192 R,\nlim r\u21930\n1\n\u00b5(B(x, r)) \u222b B(x,r) f d\u00b5 = f(x)\nfor almost all (\u00b5-a.e.) x \u2208 X . We\u2019ll see that, as a result, \u00b5(\u2202pn,\u2206n \\ \u2202o)\u2192 0. Lemma 12. There exists Xo \u2282 X with \u00b5(Xo) = 0, such that any x \u2208 X \\ Xo with \u03b7(x) 6= 1/2 lies in X+p,\u2206 \u222a X \u2212 p,\u2206 for some p,\u2206 > 0.\nProof. As a result of the differentiation condition,\nlim r\u21930 \u03b7(B(x, r)) = lim r\u21930\n1\n\u00b5(B(x, r)) \u222b B(x,r) \u03b7 d\u00b5 = \u03b7(x) (4)\nfor almost all (\u00b5-a.e.) x \u2208 X . Let Xo denote the set of x\u2019s for which (4) fails to hold or that are outside supp(\u00b5). Then, \u00b5(Xo) = 0. Now pick any x 6\u2208 Xo such that \u03b7(x) 6= 1/2. Without loss of generality, \u03b7(x) > 1/2. Set \u2206 = (\u03b7(x) \u2212 1/2)/2 > 0. By (4), there is some ro > 0 such that \u03b7(B(x, r)) \u2265 1/2 + \u2206 whenever 0 \u2264 r \u2264 ro. Define p = \u00b5(B(x, ro)) > 0. Then rp(x) \u2264 ro and x \u2208 X+p,\u2206.\nLemma 13. If pn,\u2206n \u2193 0, then lim n\u2192\u221e \u00b5 ( \u2202pn,\u2206n \\ \u2202o ) = 0.\nProof. LetAn = \u2202pn,\u2206n \\\u2202o. ThenA1 \u2283 A2 \u2283 A3 \u2283 \u00b7 \u00b7 \u00b7 . We\u2019ve seen earlier that for any x \u2208 X \\(Xo\u222a\u2202o) (where Xo is defined in Lemma 12), there exist p,\u2206 > 0 such that x 6\u2208 \u2202p,\u2206. Therefore,\u22c2\nn\u22651\nAn \u2282 Xo,\nwhereupon, by continuity from above, \u00b5(An)\u2192 0.\nConvergence in probability follows immediately.\nLemma 14. If kn \u2192\u221e and kn/n\u2192 0, then for any > 0,\nlim n\u2192\u221e\nPrn(Rn \u2212R\u2217 > ) = 0.\nProof. First define \u03b4n = exp(\u2212k1/2n ), and define the corresponding pn,\u2206n as in Theorem 1. It is easily checked that the three sequences \u03b4n, pn,\u2206n all go to zero.\nPick any > 0. By Lemma 13, we can choose a positive integer N so that \u03b4n \u2264 /2 and \u00b5(\u2202pn,\u2206n \\ \u2202o) \u2264 /2 whenever n \u2265 N . Then by Corollary 11, for n \u2265 N ,\nPrn(Rn \u2212R\u2217 > ) \u2264 \u03b4n.\nNow take n\u2192\u221e.\nWe finish with almost sure convergence.\nLemma 15. Suppose that in addition to the conditions of Lemma 14, we have kn/(log n) \u2192 \u221e. Then Rn \u2192 R\u2217 almost surely.\nProof. Choose \u03b4n = 1/n2, and for each n set pn,\u2206n as in Theorem 1. It can be checked that the resulting sequences (pn) and (\u2206n) both go to zero.\nPick any > 0. Choose N so that \u2211 n\u2265N \u03b4n \u2264 . Letting \u03c9 denote a realization of an infinite training sequence (X1, Y1), (X2, Y2), . . ., we have from Corollary 11 that\nPr { \u03c9 \u2223\u2223 \u2203n \u2265 N : Rn(\u03c9)\u2212R\u2217 > \u03b4n + \u00b5(\u2202pn,\u2206n \\ \u2202o)} \u2264 \u2211\nn\u2265N\n\u03b4n \u2264 .\nTherefore, with probability at least 1\u2212 over \u03c9, we have Rn(\u03c9)\u2212R\u2217 \u2264 \u03b4n + \u00b5 ( \u2202pn,\u2206n \\ \u2202o ) for all n \u2265 N , whereupon, by Lemma 13, Rn(\u03c9) \u2192 R\u2217. The result follows since this is true for any > 0."}, {"heading": "2.10 Proof of Theorem 3", "text": "For positive integer n and 0 \u2264 p \u2264 1, let bin(n, p) denote the (binomial) distribution of the sum of n independent Bernoulli(p) random variables. We will use bin(n, p;\u2265 k) to denote the probability that this sum is \u2265 k; and likewise bin(n, p;\u2264 k). It is well-known that the binomial distribution can be approximated by a normal distribution, suitably scaled. Slud [14] has finite-sample results of this form that will be useful to us.\nLemma 16. Pick any 0 < p \u2264 1/2 and any nonnegative integer `.\n(a) [14, p. 404, item (v)] If ` \u2264 np, then bin(n, p;\u2265 `) \u2265 1\u2212 \u03a6((`\u2212 np)/\u221anp). (b) [14, Thm 2.1] If np \u2264 ` \u2264 n(1\u2212 p), then bin(n, p;\u2265 `) \u2265 1\u2212 \u03a6((`\u2212 np)/ \u221a np(1\u2212 p)).\nHere \u03a6(a) = (2\u03c0)\u22121/2 \u222b a \u2212\u221e exp(\u2212t 2/2)dt is the cumulative distribution function of the standard normal.\nNow we begin the proof of Theorem 3. Fix any integers k < n, and any xo \u2208 En,k. Without loss of generality, \u03b7(xo) < 1/2.\nPick X1, . . . , Xn and Z1, . . . , Zn (recall the discussion on tie-breaking in Section 2.7) in the following manner:\n1. First pick a point (X1, Z1) \u2208 X \u00d7 [0, 1] according to the marginal distribution of the (k + 1)st nearest neighbor of xo.\n2. Pick k points uniformly at random from the distribution \u00b5 \u00d7 \u03bd restricted to B\u2032 = B\u2032(xo, \u03c1(xo, X1), Z1); recall the earlier definition of the augmented space X \u00d7 [0, 1] and augmented balls within this space.\n3. Pick n\u2212k\u22121 points uniformly at random from the distribution \u00b5\u00d7\u03bd restricted to (X \u00d7 [0, 1])\\B\u2032.\n4. Randomly permute the n points obtained in this way.\nThe (k+1)st nearest neighbor of xo, denotedX(k+1)(xo), is the point chosen in the first step. With constant probability, it lies within a ball of probability mass (k + \u221a k + 1)/n centered at xo, but not within a ball of probability mass k/n. Call this event G1:\nG1 : rk/n(xo) \u2264 \u03c1(xo, X(k+1)(xo)) \u2264 r(k+\u221ak+1)/n(xo)\nLemma 17. There is an absolute constant c1 > 0 such that Pr(G1) \u2265 c1.\nProof. The expected number of points Xi that fall in B(xo, r(k+\u221ak+1)/n(xo)) is \u2265 k+ \u221a k+ 1; the probability that the actual number is \u2264 k is at most bin(n, (k+ \u221a k+ 1)/n;\u2264 k). Likewise, the expected number of points that fall in Bo(xo, rk/n(xo)) is \u2264 k, and the probability that the actual number is \u2265 k + 1 is at most bin(n, k/n;\u2265 k + 1). If neither of these bad events occurs, then G1 holds. Therefore,\nPr(G1) \u2265 1\u2212 bin ( n, k + \u221a k + 1\nn ; \u2264 k\n) \u2212 bin ( n, k\nn ; \u2265 k + 1\n) .\nThe last term is easy to bound: it is\u2264 1/2 since k is the median of bin(n, k/n) [11]. To bound the first term, we use Lemma 16(a):\nbin ( n, k + \u221a k + 1\nn ; \u2264 k\n) = 1\u2212 bin ( n, k + \u221a k + 1\nn ; \u2265 k + 1 ) \u2264 \u03a6 ( (k + 1)\u2212 (k + \u221a k + 1)\u221a\nk + \u221a k + 1\n) \u2264 \u03a6(\u22121/ \u221a 3),\nwhich is 1/2\u2212 c1 for some constant c1 > 0. Thus Pr(G1) \u2265 c1.\nNext, we lower-bound the probability that (conditional on event G1), the k nearest neighbors of xo have an average Y value with the wrong sign. Recalling that \u03b7(xo) < 1/2, define the event\nG2 : Y\u0302 (B \u2032) > 1/2\nwhere as before, B\u2032 denotes the ball B\u2032(xo, X(k+1)(xo), Zk+1) in the augmented space. Lemma 18. There is an absolute constant c2 > 0 such that Pr(G2|G1) \u2265 c2.\nProof. EventG1 depends only on step 1 of the sampling process. Assuming this event occurs, step 2 consists in drawing k points from the distribution \u00b5\u00d7\u03bd restricted to B\u2032. Since xo \u2208 En,k, we have (by an application of Lemmas 24 and 25) that \u03b7(B\u2032) \u2265 1/2 \u2212 1/ \u221a k. Now, Y\u0302 (B\u2032) follows a bin(k, \u03b7(B\u2032)) distribution, and hence, by Lemma 16(b),\nPr ( Y\u0302 (B\u2032) > k\n2\n) = Pr ( Y\u0302 (B\u2032) \u2265 \u2308 k + 1\n2\n\u2309) \u2265 Pr ( Z \u2265 2\n\u221a k + 2\u221a k ) where Z is a standard normal. The last tail probability is at least some constant c2.\nIn summary, for xo \u2208 En,k, Prn(gn,k(xo) 6= g(xo)) \u2265 Pr(G1 \u2227G2) \u2265 c1c2. Taking expectation over xo, we then get EnPrX(gn,k(x) 6= g(x)) \u2265 c1c2\u00b5(En,k), as claimed."}, {"heading": "2.11 Proofs of Lemmas 4 and 5", "text": "It is immediate that if \u03b7 is (\u03b1,L)-smooth in (X , \u03c1, \u00b5), then for all x \u2208 supp(\u00b5) and all r > 0, |\u03b7(B(x, r))\u2212 \u03b7(x)| \u2264 L\u00b5(Bo(x, r))\u03b1 (5)\nPick any x \u2208 supp(\u00b5) and any p \u2265 0. For r \u2264 rp(x), we have \u00b5(Bo(x, r)) \u2264 p and thus, by (5), |\u03b7(B(x, r))\u2212 \u03b7(x)| \u2264 Lp\u03b1.\nAs a result, if \u03b7(x) > 1/2 + \u2206 +Lp\u03b1 then \u03b7(B(x, r)) > 1/2 + \u2206 whenever r \u2264 rp(x). Therefore, such an x lies in the effective interior X+p,\u2206. A similar result applies to x with \u03b7(x) < 1/2 \u2212\u2206 \u2212 Lp\u03b1. Therefore, the boundary region \u2202p,\u2206 can only contain points x for which |\u03b7(x) \u2212 1/2| \u2264 \u2206 + Lp\u03b1, as claimed by Lemma 4.\nA similar argument yields Lemma 5. Any point x \u2208 supp(\u00b5) with 1\n2 < \u03b7(x) \u2264 1 2 + 1\u221a k \u2212 L\n( k + \u221a k + 1\nn )\u03b1 has \u03b7(B(x, r)) \u2264 1/2 + 1/ \u221a k for all r \u2264 r(k+\u221ak+1)/n(x), and therefore lies in E + n,k. Likewise for E \u2212 n,k."}, {"heading": "2.12 Proof of Lemma 6", "text": "Suppose that \u03b7 satisfies the \u03b1-Holder condition so that for some constant C > 0,\n|\u03b7(x)\u2212 \u03b7(x\u2032)| \u2264 C\u2016x\u2212 x\u2032\u2016\u03b1H\nwhenever x, x\u2032 \u2208 X . For any x \u2208 supp(\u00b5) and r > 0, we then have |\u03b7(x)\u2212 \u03b7(B(x, r))| \u2264 Cr\u03b1H .\nIf \u00b5 has a density that is lower-bounded by \u00b5min, and B(x, r) \u2282 X , we also have\n\u00b5(Bo(x, r)) \u2265 \u00b5minvdrd, where vd is the volume of the unit ball in Rd. The lemma follows by combining these two inequalities."}, {"heading": "2.13 Proof of Theorem 7", "text": "Assume that \u03b7 is (\u03b1,L)-smooth in (X , \u03c1, \u00b5), that is, |\u03b7(B(x, r))\u2212 \u03b7(x)| \u2264 L\u00b5(Bo(x, r))\u03b1 (6)\nfor all x \u2208 supp(\u00b5) and all r > 0, and also that it satisfies the \u03b2-margin condition (with constant C), under which, for any t \u2265 0,\n\u00b5 ({ x \u2223\u2223\u2223 \u2223\u2223\u03b7(x)\u2212 1\n2 \u2223\u2223 \u2264 t}) \u2264 Ct\u03b2 . (7) Proof of Theorem 7(a)\nSet p,\u2206 as specified in Theorem 1. It follows from that theorem and from Lemma 4 that under (6) and (7), for any \u03b4 > 0, with probability at least 1\u2212 \u03b4 over the choice of training data,\nPrX(gn,k(X) 6= g(X)) \u2264 \u03b4 + \u00b5(\u2202p,\u2206) \u2264 \u03b4 + C(\u2206 + Lp\u03b1)\u03b2 . Expanding p,\u2206 in terms of k and n, this becomes\nPrX(gn,k(X) 6= g(X)) \u2264 \u03b4 + C\n(( ln(2/\u03b4)\nk\n)1/2 + L ( 2k\nn\n)\u03b1)\u03b2 ,\nprovided k \u2265 16 ln(2/\u03b4). The result follows by setting k \u221d n2\u03b1/(2\u03b1+1)(log(1/\u03b4))1/(2\u03b1+1).\nProof of Theorem 7(b)\nTheorem 7(b) is an immediate consequence of Lemma 20 below. We begin, however, with an intermediate result about the pointwise expected risk.\nFix any n and any k < n, and set p = 2k/n. Define\n\u2206(x) = |\u03b7(x)\u2212 1/2| \u2206o = Lp \u03b1\nRecall that the Bayes classifier g(x) has risk R\u2217(x) = min(\u03b7(x), 1 \u2212 \u03b7(x)) at x. The pointwise risk of the k-NN classifier gn,k is denoted Rn,k(x). Lemma 19. Pick any x \u2208 supp(\u00b5) with \u2206(x) > \u2206o. Under (6),\nEnRn,k(x)\u2212R\u2217(x) \u2264 exp(\u2212k/8) + 4\u2206(x) exp(\u22122k(\u2206(x)\u2212\u2206o)2).\nProof. Assume without loss of generality that \u03b7(x) > 1/2. By (6), for any 0 \u2264 r \u2264 rp(x), we have\n\u03b7(B(x, r)) \u2265 \u03b7(x)\u2212 Lp\u03b1 = \u03b7(x)\u2212\u2206o = 1\n2 + (\u2206(x)\u2212\u2206o),\nwhereby x \u2208 X+p,\u2206(x)\u2212\u2206o (and thus x 6\u2208 \u2202p,\u2206(x)\u2212\u2206o ).\nNext, recalling (2), and then applying Lemma 8,\nRn,k(x)\u2212R\u2217(x) = 2\u2206(x)1(gn,k(x) 6= g(x)) \u2264 2\u2206(x) ( 1(\u03c1(x,X(k+1)(x)) > rp(x)) + 1(|Y\u0302 (B\u2032)\u2212 \u03b7(B\u2032)| \u2265 \u2206(x)\u2212\u2206o) ) ,\nwhere B\u2032 is as defined in that lemma statement. We can now take expectation over the training data and invoke Lemmas 9 and 10 to conclude\nEnRn,k(x)\u2212R\u2217(x) \u2264 2\u2206(x) ( Prn(\u03c1(x,X(k+1)(x)) > rp(x)) + Prn(|Y\u0302 (B\u2032)\u2212 \u03b7(B\u2032)| \u2265 \u2206(x)\u2212\u2206o) )\n\u2264 2\u2206(x) ( exp ( \u2212k\n2\n( 1\u2212 k\nnp\n)2) + 2 exp ( \u22122k(\u2206(x)\u2212\u2206o)2 )) ,\nfrom which the lemma follows by substituting p = 2k/n and observing \u2206(x) \u2264 1/2.\nLemma 20. Under (6) and (7),\nEnRn,k \u2212R\u2217 \u2264 exp(\u2212k/8) + 6C max ( 2L ( 2k\nn\n)\u03b1 , \u221a 8(\u03b2 + 2)\nk\n)\u03b2+1 .\nProof. Recall the definitions of p(= 2k/n) and \u2206o,\u2206(x) above. Further, for each integer i \u2265 1, define \u2206i = \u2206o \u00b7 2i. Fix any io \u2265 1. Lemma 19 bounds the expected pointwise risk for any x with \u2206(x) > \u2206o. We will apply it to points with \u2206(x) > \u2206io . For all remaining x, we have EnRn,k(x)\u2212R\u2217(x) \u2264 2\u2206io . Taking expectation over X ,\nEnRn \u2212R\u2217 \u2264 EX [ 2\u2206io1(\u2206(X) \u2264 \u2206io) + exp(\u2212k/8) + 4\u2206(X) exp(\u22122k(\u2206(X)\u2212\u2206o)2)1(\u2206(X) > \u2206io) ] \u2264 2\u2206ioPrX(\u2206(X) \u2264 \u2206io) + exp(\u2212k/8) + 4EX [ \u2206(X) exp(\u22122k(\u2206(X)\u2212\u2206o)2)1(\u2206(X) > \u2206io) ] .\nBy the margin condition (7), we have PrX(\u2206(X) \u2264 t) \u2264 Ct\u03b2 . Thus only the last expectation remains to be bounded. We do so by considering each interval \u2206i < \u2206(X) \u2264 \u2206i+1 separately:\nEX [ \u2206(X) exp(\u22122k(\u2206(X)\u2212\u2206o)2)1(\u2206i < \u2206(X) \u2264 \u2206i+1) ] \u2264 \u2206i+1 exp(\u22122k(\u2206i \u2212\u2206o)2)PrX(\u2206(X) \u2264 \u2206i+1) \u2264 C\u2206\u03b2+1i+1 exp(\u22122k(\u2206i \u2212\u2206o) 2). (8)\nIf we set\nio = max\n( 1, \u2308 log2 \u221a 2(\u03b2 + 2)\nk\u22062o\n\u2309) ,\nthen for i \u2265 io, the terms (8) are upper-bounded by a geometric series with ratio 1/2. This is because the ratio of two successive terms can be bounded as\nC\u2206\u03b2+1i+1 exp(\u22122k(\u2206i \u2212\u2206o)2) C\u2206\u03b2+1i exp(\u22122k(\u2206i\u22121 \u2212\u2206o)2) = 2\u03b2+1 exp(\u22122k((2i\u2206o \u2212\u2206o)2 \u2212 (2i\u22121\u2206o \u2212\u2206o)2))\n= 2\u03b2+1 exp(\u22122k\u22062o((2i \u2212 1)2 \u2212 (2i\u22121 \u2212 1)2)) \u2264 2\u03b2+1 exp(\u221222i\u22121k\u22062o) \u2264 2\u03b2+1 exp(\u2212(\u03b2 + 2)) \u2264 1/2.\nTherefore\nEX [ \u2206(X) exp(\u22122k(\u2206(X)\u2212\u2206o)2)1(\u2206(X) > \u2206io) ] = \u2211 i\u2265io EX [ \u2206(X) exp(\u22122k(\u2206(X)\u2212\u2206o)2)1(\u2206i < \u2206(X) \u2264 \u2206i+1)\n] \u2264 \u2211 i\u2265io C\u2206\u03b2+1i+1 exp(\u22122k(\u2206i \u2212\u2206o) 2) \u2264 C\u2206\u03b2+1io exp(\u22122k(\u2206io\u22121 \u2212\u2206o) 2) \u2264 C\u2206\u03b2+1io .\nPutting these together, we have EnRn,k \u2212 R\u2217 \u2264 6C\u2206\u03b2+1io + e \u2212k/8. We finish by substituting \u2206io = 2io\u2206o."}, {"heading": "2.14 Zero Bayes Risk", "text": "An interesting case is when there is no inherent uncertainty in the conditional probability distribution p(y|x). Formally, for all x in the sample space X , except those in a subset X0 of measure zero, \u03b7(x) is either 0 or 1. In this case, the omniscient Bayes classifier will incur risk R\u2217 = 0; however, a classifier based on a finite sample that is unaware of the true \u03b7 will incur some non-zero classification error.\nAn interesting quantity to consider in this case is the effective interiors of the classes as a whole:\nX+p = {x \u2208 supp(\u00b5) | \u03b7(x) = 1, \u03b7(B(x, r)) = 1 for all r \u2264 rp(x)}. X\u2212p = {x \u2208 supp(\u00b5) | \u03b7(x) = 0, \u03b7(B(x, r)) = 0 for all r \u2264 rp(x)}.\nThus, X+p = X+p,1/2, and X \u2212 p = X\u2212p,1/2. The rest of X is the effective boundary between the two classes:\n\u2202p = X \\ (X+p \u222a X\u2212p ).\nIncorporating these two quantities into Theorem 1 yields a bound of the following form.\nLemma 21. Let \u03b4 be any positive real and let k < n be positive integers. With probability \u2265 1\u2212 \u03b4 over the choice of the training data, the error of the k-nearest neighbor classifier gn,k is bounded as:\nPr X\n(gn,k(X) 6= g(X)) \u2264 \u03b4 + \u00b5(\u2202p),\nwhere\np = k\nn +\n2 log(2/\u03b4)\nn\n( 1 + \u221a 1 +\nk\nlog(2/\u03b4)\n)\nProof. The proof is the same as that of Theorem 1, except that the probability of the central bad event is different. We will therefore bound the probabilities of these events.\nObserve that under the conditions of the lemma, for any p,\n\u2202p, 12 = \u2202p\nMoreover, for any x0 /\u2208 \u2202p, \u03b7(B\u2032(x0, rp(x0))) is either 0 or 1; this implies that for all x \u2208 B\u2032(x0, rp(x0)) except those in a measure zero subset, \u03b7(x) is either 0 or 1. Therefore, the probability Pr(Y\u0302 (B\u2032) 6= \u03b7(B\u2032)) is zero.\nThe rest of the lemma follows from plugging this fact in to the proof of Theorem 1 and some simple algebra.\nIn particular, observe that since p increases with increasing k, and the dependence on \u2206 is removed, the best bounds are achieved at k = 1 for:\np = 1\nn +\n2(1 + \u221a 2) log(2/\u03b4)\nn\nThis corroborates the admissibility results of [2], which essentially state that there is no k > 1 such that the k-nearest neighbor algorithm has equal or better error than the 1-nearest neighbor algorithm against all distributions."}, {"heading": "2.15 Additional technical lemmas", "text": "Lemma 22. For any x \u2208 X and 0 \u2264 p \u2264 1, we have \u00b5(B(x, rp(x))) \u2265 p.\nProof. Let r\u2217 = rp(x) = inf{r | \u00b5(B(x, r)) \u2265 p}. For any n \u2265 1, let Bn = B(x, r\u2217 + 1/n). Thus B1 \u2283 B2 \u2283 B3 \u2283 \u00b7 \u00b7 \u00b7 , with \u00b5(Bn) \u2265 p. Since B(x, r\u2217) = \u22c2 nBn, it follows by continuity from above of probability measures that \u00b5(Bn)\u2192 \u00b5(B(x, r\u2217)), so this latter quantity is \u2265 p.\nLemma 23 (Cover-Hart). \u00b5(supp(\u00b5)) = 1.\nProof. Let Xo denote a countable dense subset of X . Now, pick any point x 6\u2208 supp(\u00b5); then there is some r > 0 such that \u00b5(B(x, r)) = 0. It is therefore possible to choose a ball Bx centered in Xo, with rational radius, such that x \u2208 Bx and \u00b5(Bx) = 0. Since there are only countably many balls of this sort,\n\u00b5(X \\ supp(\u00b5)) \u2264 \u00b5 ( \u22c3 x 6\u2208supp(\u00b5) Bx ) = 0.\nLemma 24. Pick any xo \u2208 supp(\u00b5), ro > 0, and any Borel set I \u2282 [0, 1]. Define Bo = Bo(xo, ro) and B = B(xo, ro) to be open and closed balls centered at xo, and let A \u2282 X \u00d7 [0, 1] be given by A = (Bo \u00d7 [0, 1]) \u22c3 ((B \\Bo)\u00d7 I). Then\n\u03b7(A) = \u00b5(B)\u03bd(I)\n\u00b5(B)\u03bd(I) + \u00b5(Bo)(1\u2212 \u03bd(I)) \u03b7(B) + \u00b5(Bo)(1\u2212 \u03bd(I)) \u00b5(B)\u03bd(I) + \u00b5(Bo)(1\u2212 \u03bd(I)) \u03b7(Bo).\nProof. Since xo lies in the support of \u00b5, we have (\u00b5\u00d7 \u03bd)(A) \u2265 \u00b5(Bo) > 0; hence \u03b7(A) is well-defined.\n\u03b7(A) = 1\n(\u00b5\u00d7 \u03bd)(A) \u222b A \u03b7 d(\u00b5\u00d7 \u03bd)\n= 1\n\u00b5(Bo) + \u00b5(B \\Bo)\u03bd(I) (\u222b Bo \u03b7 d\u00b5+ \u222b B\\Bo \u03bd(I)\u03b7 d\u00b5 )\n= 1\n\u00b5(Bo) + (\u00b5(B)\u2212 \u00b5(Bo))\u03bd(I) (\u222b Bo \u03b7 d\u00b5+ \u03bd(I) (\u222b B \u03b7 d\u00b5\u2212 \u222b Bo \u03b7 d\u00b5 )) = \u00b5(Bo)\u03b7(Bo) + \u03bd(I)(\u00b5(B)\u03b7(B)\u2212 \u00b5(Bo)\u03b7(Bo))\n\u00b5(Bo)(1\u2212 \u03bd(I)) + \u00b5(B)\u03bd(I) ,\nas claimed.\nLemma 25. Suppose that for some xo \u2208 supp(\u00b5) and ro > 0 and q > 0, it is the case that \u03b7(B(xo, r)) \u2265 q whenever r \u2264 ro. Then \u03b7(Bo(xo, ro)) \u2265 q as well.\nProof. Let Bo = Bo(xo, ro). Since Bo = \u22c3 r<ro B(xo, r), it follows from the continuity from below of probability measures that\nlim r\u2191ro\n\u00b5(B(xo, r)) = \u00b5(B o)\nand by dominated convergence that\nlim r\u2191ro \u222b B(xo,r) \u03b7 d\u00b5 = lim r\u2191ro \u222b Bo 1(x \u2208 B(xo, r))\u03b7(x)\u00b5(dx) = \u222b Bo \u03b7 d\u00b5.\nFor any r \u2264 ro, we have \u03b7(B(xo, r)) \u2265 q, which can be rewritten as\u222b B(xo,r) \u03b7 d\u00b5\u2212 q \u00b5(B(xo, r)) \u2265 0.\nTaking the limit r \u2191 ro, we then get the desired statement."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Nearest neighbor methods are a popular class of nonparametric estimators with several<lb>desirable properties, such as adaptivity to different distance scales in different regions of<lb>space. Prior work on convergence rates for nearest neighbor classification has not fully<lb>reflected these subtle properties. We analyze the behavior of these estimators in metric<lb>spaces and provide finite-sample, distribution-dependent rates of convergence under min-<lb>imal assumptions. As a by-product, we are able to establish the universal consistency of<lb>nearest neighbor in a broader range of data spaces than was previously known. We illus-<lb>trate our upper and lower bounds by introducing smoothness classes that are customized<lb>for nearest neighbor classification.", "creator": "LaTeX with hyperref package"}}}