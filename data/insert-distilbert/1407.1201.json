{"id": "1407.1201", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2014", "title": "Improving Performance of Self-Organising Maps with Distance Metric Learning Method", "abstract": "self - activation organising maps ( som ) operators are artificial neural networks used in pattern recognition tasks. nearly their major advantage over other architectures is human readability of a model. however, they often gain poorer accuracy. mostly used metric in som is the euclidean distance, which is not even the traditional best approach to some problems. in this paper, we study an impact of describing the metric change measurement on the som's performance in classification automation problems. in order to change the defining metric of the underlying som we applied a distance coding metric learning method, so - called'large margin nearest neighbour '. it computes the mahalanobis integration matrix, which assures small distance between nearest neighbour points from the same class separately and separation of points belonging to different classes by large average margin. results are presented on several real data sets, containing for example recognition of usually written digits, spoken spelling letters or faces.", "histories": [["v1", "Fri, 4 Jul 2014 12:14:48 GMT  (148kb)", "http://arxiv.org/abs/1407.1201v1", "9 pages, 2 figures"]], "COMMENTS": "9 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["piotr p{\\l}o\\'nski", "krzysztof zaremba"], "accepted": false, "id": "1407.1201"}, "pdf": {"name": "1407.1201.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Krzysztof Zaremba"], "emails": ["pplonski@ire.pw.edu.pl", "zaremba@ire.pw.edu.pl"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 7.\n12 01\nv1 [\ncs .L\nG ]\n4 J\nul 2\nKeywords: Self-Organising Maps, Distance Metric Learning, LMNN, Mahalanobis distance, Classification"}, {"heading": "1 Introduction", "text": "Some real-world problems do not have an exact algorithmic solution. Currently, there is a vast number of Artificial Intelligence(AI) methods which can be used to solve them. One of the branches of AI are Artificial Neural Networks. They are mathematical models inspired by biology. In 1982 T.Kohonen presented architecture called Self-Organising Maps (SOM) [10], which provides a method of feature mapping from multi-dimensional space to usually a two-dimensional grid of neurons in an unspervised way. This way of data analysis was proved as an efficient tool in many applications, both in academic and industrial solutions [11]. For example in character recognition tasks, image recognition tasks, face recognition [2], analysis of words[12], grouping of documents [9], visualisation [5], and even bioinformatics (for example phylogenetic tree reconstruction [4]).\nThere exists a huge number of methods for improving SOM\u2019s performance. Some of them concentrated on finding an optimal size of a network[1], faster\n0 The final publication is available at http://link.springer.com/chapter/10.1007/ 978-3-642-29347-4_20\nlearning [14] or applying different neighbourhood functions [8]. In this paper we investigate two additional improvements. The first one [6], [2] uses Mahalanobis metric instead of the Euclidean one. The second improvement [13] shows how to use SOM in a supervised manner.\nIn our approach, contrary to [6], [2], [3], instead of computing the Mahalanobis matrix as an inverse of covariance matrix, it is learned in a way assuring the smallest distance between points from the same class and large margin separation of points from different classes. Several algorithms exist for distance metric learning (DML) [17], [7]. In this paper we use so-called Large Margin Nearest Neighbour (LMNN) method [16]. It introduces the distance metric learning problem as a convex optimization, which assures that the global minimum can be efficiently computed. First, we shortly describe SOM model used in supervised manner and LMNN method. Then we show how we combine these two approaches into our SOM+DML model. Finally we present results on real data sets."}, {"heading": "2 Methods", "text": "Let\u2019s denote data set as D = {(xi,yi)}, where xi is an attribute vector of i-th sample, and yi is a class vector, where yij = 0 for j 6= classi and yij = 1 for j = classi, where classi is class number for i-th sample."}, {"heading": "2.1 SOM model", "text": "In this paper, we used the SOM architecture in a supervised manner so-called \u2019Learning Associations by Self-Organisation\u2019 (LASSO), first described in [13]. The main difference between this SOM architecture and the original Kohonen\u2019s SOM architecture [10] is that during the learning phase the LASSO method takes into consideration class vector, additionally to attributes. Herein, we used two-dimensional grid of neurons. Each neuron is represented by a weight vector Wpq, consisting of a vector corresponding to attributes Apq, and to a class Cpq (Wpq = [Apq;Cpq ]), where (p, q) are indexes of the neuron in the grid. In a learning phase all samples are shown to the network in one epoch. For each sample we search for a neuron which is closest to the i-th sample. The distance is computed by:\nDisttrain(Di,Wpq) = (xi \u2212Apq) T (xi \u2212Apq) + (yi \u2212 Cpq) T (yi \u2212 Cpq). (1)\nThe neuron (p, q) with the smallest distance to i-th sample is called the Best Matching Unit (BMU), we note its indexes as (Bi, Bj). Once the BMU is found, the weight update step is executed. The weights of each neuron are updated with following formulas:\nApq = Apq + \u03b7(Apq \u2212 xi), (2)\nCpq = Cpq + \u03b7(Cpq \u2212 yi), (3)\nwhere \u03b7 is a learning coefficient, consisting of a learning step size parameter \u00b5 and a neighbourhood function \u03c4 , so \u03b7 = \u00b5\u03c4 . Learning speed parameter is decreased between consecutive epochs, so that network\u2019s ability to remember patterns is improved. It is described by \u00b5 = \u00b50exp(\u2212t\u03bb), where \u00b50 is a starting value of the learning speed, t is the current epoch and \u03bb is responsible for regulating the speed of the decrease. Neighbourhood function controls changing the weights with respect to the distance to the BMU. It is noted as \u03c4 = exp(\u2212\u03b1S(Bi, Bj , p, q)), where \u03b1 describes the neighbourhood function width and S(Bi, Bj , p, q) is the distance in the grid between the neuron and the BMU, computed by the following formula:\nS(Bi, Bj , p, q) = (Bi \u2212 p) 2 + (Bj \u2212 q) 2. (4)\nWe assumed a cost function as a sum of distances between samples and corresponding BMUs:\nF = \u2211\nl\nDisttrain(Dl,WBi,Bj ). (5)\nWe train network till the cost function stops decreasing or a selected number of learning procedure iterations is exceed.\nThe exploitation phase is performed after the learning phase. New samples, which do not take part in the training, are shown to the network in order to designate their class. It should be noted that only the part with attributes is presented to the network. The BMU is found by computing a distance between an attribute input vector and an attribute part of the weights using the following formula:\nDisttest(Di,Wpq) = (xi \u2212Apq) T (xi \u2212Apq). (6)\nFor the tested sample, the designated class corresponds to position of maximum value in the part which code class information Cpq in BMU weights."}, {"heading": "2.2 LMNN method", "text": "In many cases the mostly used metric is an Euclidean one. It often gives poor accuracy, because it takes all dimensions with equal contribution and assumes no correlations between the dimensions. Mahalanobis distance seems a better metric choice, because it is scale-invariant and takes into account input dimensions correlations. It is defined by:\nDistM (xi,xj) = (xi \u2212 xj) TM(xi \u2212 xj), (7)\nwhere M is usually an inverse of a covariance matrix. In case where M is an identity matrix, the distance (7) is equal to the Euclidean distance.\nIn this paper, we learned Mahalanobis matrix using the method described in [16]. Matrix coefficients are computed in a way assuring large margin separation of points from different classes and a small distance between the points of the same class. Before starting the matrix learning for each point, k nearest neighbours with the same class are found. They are called target neighbours, denoted by \u03b8ij = {0, 1}, where \u03b8i,j = 1 means that xj is the target neighbour of xi, and\n\u03b8i,j = 0 means otherwise. With no prior knowledge, the Euclidean distance can be used to point the target neighbours. Target neighbours are unchanged during the whole learning. Let\u2019s add a variable to indicate when the two samples have the same class, denoted as yil = {0, 1}, where yil = 1 when i-th and l -th samples are from the same class, zero when they are from different classes.\nFinding an optimal matrix M can be expressed as a semidefinite programming (SDP) optimization problem with the following cost function (8) and constrains (9), (10), (11):\nminimize \u2211\nij\n\u03b8i,jDistM (xi,xj) + c \u2211\ni,j,l\n\u03b8i,j(1\u2212 yil)\u03beijl, (8)\nDistM (xi,xl)\u2212DistM (xi,xj) \u2265 1\u2212 \u03beijl, (9)\n\u03beijl \u2265 0, (10)\nM 0. (11)\nThe first term in the minimization function penalizes a large distance between the samples and their target neighbours. The second term penalizes a small distance between the samples from different classes - it is expressed as slack variables \u03beijl . The c parameter balances the influence between these two terms. In this paper it is set to 0.5, which gives equal strength to each term. The constraint given in (11) requires the M matrix to be positive semidefinite - all its eigenvalues should be nonnegative. Semidefinite programming is a convex optimization problem, so a global minimum can be efficiently computed. SPD can be solved using general purpose solvers, however in our approach we used Matalb implementation code1 described in [16], which is finely tuned to efficiently solve this kind of problems. Here most of slack variables are not used because samples are well separated."}, {"heading": "2.3 SOM+DML model", "text": "We are interested in a such linear transformation of sample attributes that will assure that the Euclidean distance computed on the transformed attributes will be equal to the Mahalanobis distance computed on the original attributes. Mahalanobis matrix M can be written as M = LTL, where L is the searched transformation. Lets denote ui as the transformed attributes of i-th sample and uj as the transformed attributes of j -th sample:\nui = Lxi, (12)\nuj = Lxj . (13)\nThe distance between the transformed attributes in Euclidean distance should be equal to Mahalanobis distance between the original attributes:\nDistM (xi,xj) = DistE(ui,uj). (14)\n1 Matalb impementation of LMNN algorithm available from http://www.cse.wustl. edu/~kilian/code/code.html\nWe will now search for L. Now for matrix M we will find eigenvectors \u03a6 and a matrix with eigenvalues on diagonal \u039b:\nM\u03a6 = \u03a6\u039b. (15)\nMatrix \u039b can be expressed as:\n\u03a6TM\u03a6 = \u039b. (16)\nSince matrix M found by the LMNN algorithm is positive semidefine, diagonal elements in matrix \u039b are nonnegative. Thus, we can write:\n\u039b\u2212 1 2\u039b\u039b\u2212 1 2 = I. (17)\nSubstituting (16) into (17), we obtain:\n\u039b\u2212 1 2\u03a6TM\u03a6\u039b\u2212 1 2 = I. (18)\nNow we can note L as: L = \u039b\u2212 1 2\u03a6T . (19)\nUsing (19) we can write (18) as:\nLMLT = I. (20)\nWe see that M = LTL and hence we can write:\n(xi \u2212 xj) TM(xi \u2212 xj) = (Lxi \u2212 Lxj) T (Lxi \u2212 Lxj) (21)\nFrom (21) we see that (14) is true. This transformation of input attributes is also known as whitening transform. It is worth mentioning that, in a transformation phase, only attributes x are transformed, the class part of input vector y is unchanged. Therefore, even though the data were pre-processed using the L transformation, we still can use the original SOM algorithm."}, {"heading": "3 Results", "text": "Performance of the SOM+DML method was compared to the SOM model on six real data sets. As an accuracy measure we take the percentage of incorrect classifications. It is worth mentioning that getting the highest number of correct classifications is not the goal of this paper. Data sets are described in Table 1. Sets \u2019Wine\u2019, \u2019Ionosphere\u2019, \u2019Iris\u2019, \u2019Isolet\u2019, \u2019Digits\u2019 are sets from the \u2019UCI Machine Learing Repository\u2019 2, set \u2019Faces\u2019 are from the \u2019The ORL Database of Faces\u20193.\nNow we briefly introduce the origin of the sets. Data sets \u2019Wine\u2019, \u2019Ionosphere\u2019, \u2019Iris\u2019 are classic benchmark sets, often used in testing newly developed classification algorithms. \u2019Isolet\u2019 data set represents a spoken letter recognition task.\n2 http://archive.ics.uci.edu/ml/ 3 http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\nIts samples correspond to 26 letters of the alphabet. The original number of attributes (617) was projected by PCA to 100 principal components, which covers 93% of its variance. \u2019Digits\u2019 data set represents a handwritten digit recognition problem. The samples are from 10 classes obtained from 43 people. Original images were 32x32 bitmaps downsampled to 64 attributes by creators. The face recognition task is presented by data set \u2019Faces\u2019. It contains images of faces obtained from 40 people (40 classes). For each person 10 images were taken in different times, varying lightening, changing facial expressions and details. The original data - 92x112 pixels images in 256 gray levels was projected by PCA to 50 leading components (83% of variance), the so-called egienfaces method[15]. Each data set, if not originally divided to train/test subsets, was randomly divided by us - 70% of data to training subset and 30% to testing subset.\nFor each data set, we arbitrarily chose the network size (selecting optimal network size is not in the scope of this paper). The network size for the SOM and the SOM+DML models was equal. For all data sets, the following values of the learning parameters were used: \u00b50 = 0.01, \u03bb = 0.005, \u03b1 = 0.1. For each data set a number of k target neighbours in the LMNN algorithm was selected using cross validation with ten times repetition. Fig.1 presents results of a selecting the target neighbours parameter (k) for all sets. Resulting k values are shown in Table 1. SOM weights were initialized with random numbers drawn from a normal distribution with mean 0 and standard deviation 0.5. Several runs were performed for each data set (see Table 1), so that local minimums were avoided. The final result is a mean over all runs. The SOM and the SOM+DML models were trained with identical number of iterations.\nThe comparison of results obtained by the SOM and the SOM+DML method is presented in Table 2. With one exception the SOM+DML method achieves lower error rates in both training and testing subsets. On the \u2019Iris\u2019 data set the LMNN algorithm seems to cause the overfitting effect. It is clearly visible in Fig.1 during search for the k parameter. The greatest improvement was achieved on\nthe \u2019Faces\u2019 set (20.87% over the SOM model). The comparison of example face pictures classified as belonging to the same class by the SOM method and the SOM+DMLmethod is presented in the Fig.2. For the SOM+DMLmethod, when using the \u2019Faces\u2019 set we observed a significant difference between the training error and the testing error. It is a small set, therefore the metric was well matched to the training set, giving small error. On \u2019Wine\u2019 and \u2019Isolet\u2019 data sets the improvement was 1.75% and 1.89% respectively. On the \u2019Digits\u2019 set there was a 2.68% improvement on the testing subset, which corresponds to roughly 50 digits. For the \u2019Ionosphere\u2019 data set the improvement was 6.67%. It is worth mentioning that for this set the largest k value was used."}, {"heading": "4 Conclusions", "text": "A method of improving performance of the Self-Organising Maps in classification tasks was described. Linear transformation of data was performed be-\nfore SOM training phase. Matrix for the transformation has been obtained from the LMNN algorithm, which computes Mahalanobis matrix while assuring large margin separation between the points of different classes. We called our method SOM+DML. Testing of the method was demonstrated on several data sets, focused on recognition of: faces, handwritten digits and spoken letters. Test results confirm that the distance metric learning method improves the performance of the SOM network. Finding the optimal matrix for linear transformation plays a crucial role in obtaining improved results. Matlab implementation of the SOM+DML model is available from http://home.elka.pw. edu.pl/~pplonski/som_dml."}], "references": [{"title": "Dynamic Self Organizing Maps With Controlled Growth for Knowledge Discovery", "author": ["D. Alahakoon", "S.K. Halgamuge", "B. Sirinivasan"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Face Recognition under Varying Illumination Using Mahalanobis Self-organizing Map", "author": ["S. Aly", "N. Tsuruta", "R. Taniguchi"], "venue": "Artificial Life and Robotics, vol.13, no. 1, pp 298-301", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Improving the K-NN Classification with the Euclidean Distance Through Linear Data Transformations", "author": ["L. Bobrowski", "M. Topczewska"], "venue": "In Industrial Conference on Data", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Phylogenetic reconstruction using an unsupervised growing neural network that adopts the topology of a phylogenetic tree", "author": ["J. Dopazo", "J.M. Carazo"], "venue": "Journal of Molecular Evolution,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "On Global Self-Organizing Maps", "author": ["W. Duch", "A. Naud"], "venue": "ESANN", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Comparison of Supervised SelfOrganizing Maps Using Euclidian or Mahalanobis Distance in Classification Context", "author": ["F. Fessant", "P. Aknin", "L. Oukhellou", "S. Midenet"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "The impact of network topology on selforganizing maps", "author": ["F. Jiang", "H. Berry", "M. Schoenauer"], "venue": "In GEC Summit(2009),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Create Self-organizing Maps of Documents in a Distributed System", "author": ["M.A. K lopotek", "T. Pachecki"], "venue": "Intelligent Information Systems, Siedlce, pp 315-320,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Self-organized formation of topologically correct feature maps", "author": ["T. Kohonen"], "venue": "Biological Cybernetics, vol.43 pp 59-69", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1982}, {"title": "Contextually Self-Organized Maps of Chinese Words", "author": ["T. Kohonen", "H. Xing"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Learning Associations by Self-Organization: The LASSO model", "author": ["S. Midenet", "A. Grumbach"], "venue": "Neurocomputing vol.6(3) pp 343-361", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "parSOM: A Parallel Implementation of the Self-Organizing Map Exploiting Cache Effects: Making the SOM Fit for Interactive High-Performance Data Analysis", "author": ["A. Rauber", "P. Tomsich", "D. Merkl"], "venue": "International Joint Conference on Neural Networks(2000),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Eigenfaces for recognition", "author": ["M. Turk", "A. Pentland"], "venue": "Journal of Cognitive Neuroscience, vol.3, no.(1), pp 71\u201386,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1991}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Distance Metric Learning with Application to Clustering with Side-Information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}], "referenceMentions": [{"referenceID": 9, "context": "Kohonen presented architecture called Self-Organising Maps (SOM) [10], which provides a method of feature mapping from multi-dimensional space to usually a two-dimensional grid of neurons in an unspervised way.", "startOffset": 65, "endOffset": 69}, {"referenceID": 1, "context": "For example in character recognition tasks, image recognition tasks, face recognition [2], analysis of words[12], grouping of documents [9], visualisation [5], and even bioinformatics (for example phylogenetic tree reconstruction [4]).", "startOffset": 86, "endOffset": 89}, {"referenceID": 10, "context": "For example in character recognition tasks, image recognition tasks, face recognition [2], analysis of words[12], grouping of documents [9], visualisation [5], and even bioinformatics (for example phylogenetic tree reconstruction [4]).", "startOffset": 108, "endOffset": 112}, {"referenceID": 8, "context": "For example in character recognition tasks, image recognition tasks, face recognition [2], analysis of words[12], grouping of documents [9], visualisation [5], and even bioinformatics (for example phylogenetic tree reconstruction [4]).", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "For example in character recognition tasks, image recognition tasks, face recognition [2], analysis of words[12], grouping of documents [9], visualisation [5], and even bioinformatics (for example phylogenetic tree reconstruction [4]).", "startOffset": 155, "endOffset": 158}, {"referenceID": 3, "context": "For example in character recognition tasks, image recognition tasks, face recognition [2], analysis of words[12], grouping of documents [9], visualisation [5], and even bioinformatics (for example phylogenetic tree reconstruction [4]).", "startOffset": 230, "endOffset": 233}, {"referenceID": 0, "context": "Some of them concentrated on finding an optimal size of a network[1], faster", "startOffset": 65, "endOffset": 68}, {"referenceID": 12, "context": "learning [14] or applying different neighbourhood functions [8].", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "learning [14] or applying different neighbourhood functions [8].", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "The first one [6], [2] uses Mahalanobis metric instead of the Euclidean one.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "The first one [6], [2] uses Mahalanobis metric instead of the Euclidean one.", "startOffset": 19, "endOffset": 22}, {"referenceID": 11, "context": "The second improvement [13] shows how to use SOM in a supervised manner.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "In our approach, contrary to [6], [2], [3], instead of computing the Mahalanobis matrix as an inverse of covariance matrix, it is learned in a way assuring the smallest distance between points from the same class and large margin separation of points from different classes.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "In our approach, contrary to [6], [2], [3], instead of computing the Mahalanobis matrix as an inverse of covariance matrix, it is learned in a way assuring the smallest distance between points from the same class and large margin separation of points from different classes.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "In our approach, contrary to [6], [2], [3], instead of computing the Mahalanobis matrix as an inverse of covariance matrix, it is learned in a way assuring the smallest distance between points from the same class and large margin separation of points from different classes.", "startOffset": 39, "endOffset": 42}, {"referenceID": 15, "context": "Several algorithms exist for distance metric learning (DML) [17], [7].", "startOffset": 60, "endOffset": 64}, {"referenceID": 6, "context": "Several algorithms exist for distance metric learning (DML) [17], [7].", "startOffset": 66, "endOffset": 69}, {"referenceID": 14, "context": "In this paper we use so-called Large Margin Nearest Neighbour (LMNN) method [16].", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "In this paper, we used the SOM architecture in a supervised manner so-called \u2019Learning Associations by Self-Organisation\u2019 (LASSO), first described in [13].", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "The main difference between this SOM architecture and the original Kohonen\u2019s SOM architecture [10] is that during the learning phase the LASSO method takes into consideration class vector, additionally to attributes.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "In this paper, we learned Mahalanobis matrix using the method described in [16].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "SPD can be solved using general purpose solvers, however in our approach we used Matalb implementation code described in [16], which is finely tuned to efficiently solve this kind of problems.", "startOffset": 121, "endOffset": 125}, {"referenceID": 13, "context": "The original data - 92x112 pixels images in 256 gray levels was projected by PCA to 50 leading components (83% of variance), the so-called egienfaces method[15].", "startOffset": 156, "endOffset": 160}], "year": 2014, "abstractText": "Self-Organising Maps (SOM) are Artificial Neural Networks used in Pattern Recognition tasks. Their major advantage over other architectures is human readability of a model. However, they often gain poorer accuracy. Mostly used metric in SOM is the Euclidean distance, which is not the best approach to some problems. In this paper, we study an impact of the metric change on the SOM\u2019s performance in classification problems. In order to change the metric of the SOM we applied a distance metric learning method, so-called \u2019Large Margin Nearest Neighbour\u2019. It computes the Mahalanobis matrix, which assures small distance between nearest neighbour points from the same class and separation of points belonging to different classes by large margin. Results are presented on several real data sets, containing for example recognition of written digits, spoken letters or faces.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}