{"id": "1407.2538", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jul-2014", "title": "Learning Deep Structured Models", "abstract": "in an recent years the performance of deep learning algorithms has thus been demonstrated in a variety of application domains. the goal : of this paper is to simultaneously enrich deep learning to be able to predict a set of generic random variables while taking into account that their dependencies. towards increasing this goal, we propose an efficient algorithm that is able to learn structured models with non - linear functions. we publicly demonstrate the effectiveness of our algorithm in the tasks of predicting words as well as codes from 2d noisy images, and again show that by jointly learning multilayer perceptrons and pairwise features, significant gains back in performance can be obtained.", "histories": [["v1", "Wed, 9 Jul 2014 15:54:27 GMT  (380kb,D)", "https://arxiv.org/abs/1407.2538v1", "10 pages including reference"], ["v2", "Fri, 19 Dec 2014 21:50:10 GMT  (776kb,D)", "http://arxiv.org/abs/1407.2538v2", "11 pages including reference"], ["v3", "Mon, 27 Apr 2015 21:11:32 GMT  (4321kb,D)", "http://arxiv.org/abs/1407.2538v3", "11 pages including reference"]], "COMMENTS": "10 pages including reference", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["liang-chieh chen", "alexander g schwing", "alan l yuille", "raquel urtasun"], "accepted": true, "id": "1407.2538"}, "pdf": {"name": "1407.2538.pdf", "metadata": {"source": "CRF", "title": "LEARNING DEEP STRUCTURED MODELS", "authors": ["Liang-Chieh Chen", "Alexander G. Schwing", "Alan L. Yuille", "Raquel Urtasun"], "emails": ["lcchen@cs.ucla.edu", "aschwing@cs.toronto.edu", "yuille@stat.ucla.edu", "urtasun@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep learning algorithms attempt to model high-level abstractions of the data using architectures composed of multiple non-linear transformations. A multiplicity of variants have been proposed (Hinton et al., 1984; LeCun et al., 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014). Recently, state-of-the-art results have been achieved in many computer vision tasks, outperforming competitive methods by a large margin (Krizhevsky et al., 2013; Girshick et al., 2014).\nDeep neural networks can, however, be even more powerful when combined with graphical models in order to capture the statistical dependencies between the variables of interest. For example, Deng et al. (2014) exploit mutual exclusion, overlapping and subsumption properties of class labels in order to better predict in large scale classification tasks. In pose estimation, more accurate predictions can be obtained when encoding the spatial relationships between joint locations (Tompson et al., 2014).\nIt is, however, an open problem how to develop scalable deep learning algorithms that can learn higher-order knowledge taking into account the output variable\u2019s dependencies. Existing approaches often rely on a two-step process (Nowozin et al., 2011; Xu et al., 2014) where a non-linear classifier that employs deep features is trained first, and its output is used to generate potentials for the structured predictor. This piece-wise training is, however, suboptimal as the deep features are learned\n\u2217The first two authors contributed equally to this work.\nar X\niv :1\n40 7.\n25 38\nv3 [\ncs .L\nG ]\n2 7\nA pr\nwhile ignoring the dependencies between the variables of interest, e.g., independently learned segmentation and detection features (Hariharan et al., 2014) might be focusing on predicting the same examples correctly. But when learned jointly they can improve their predictive power by exploiting complementary information to fix additional mistakes.\nIn this paper we extend deep learning algorithms to learn complex representations taking into account the dependencies between the output random variables. Towards this goal, we propose a learning algorithm that is able to learn structured models with arbitrary graphs jointly with deep features that form Markov random field (MRF) potentials. Our approach is efficient as it blends learning and inference resulting in a single loop algorithm which makes use of GPU acceleration. We demonstrate the effectiveness of our method in the tasks of predicting words from noisy images, and multi-class classification of Flickr photographs. We show that joint learning of deep features and MRF parameters results in big performance gains."}, {"heading": "2 LEARNING DEEP STRUCTURED MODELS", "text": "In this section we investigate how to learn \u2018deep features\u2019 that take into account the dependencies between the output variables. Let y \u2208 Y be the set of random variables y = (y1, . . . , yN ) that we are interested in predicting. In this work we assume the space of valid configurations to be a product space, i.e., Y = \u220fN i=1 Yi, and the domain of each individual variable yi to be discrete, i.e., Yi = {1, . . . , |Yi|}. Given input data x \u2208 X and parameters w \u2208 RA of the function F (x, y;w) : X \u00d7 Y \u00d7 RA \u2192 R, inference amounts to finding the highest scoring configuration y\u2217 = arg maxy F (x, y;w). Note that if F is a deep network and there are no connections between the output variables to be predicted, inference corresponds to a forward pass to evaluate the function, followed by independently finding the largest response for each variable. This can be interpreted as inference in a graphical model with only unary potentials. However, for arbitrary graphical models it is NP-hard to find the maximizing configuration y\u2217 since the inference program generally requires a search over a space of size \u220fN i=1 |Yi|. Note also that log-linear models are a special case of this program, with F (x, y;w) = w>\u03c6(x, y) and \u03c6(x, y) denoting a feature vector computed using the input-output pair (x, y).\nIn this work, we consider the general setting where F (x, y;w) is an arbitrary scalar-valued function of w and (x, y). In our experiments F is a function composition of non-linear base mappings like convolutions, rectifications, pooling etc. We let the probability of an arbitrary configuration y\u0302 be given by the annealed soft-max p(x,y)(y\u0302|w, ) = 1Z (x,w) exp(F (x, y\u0302;w))\n1/ . Hereby Z (x,w) refers to the partition function, normalizing the distribution p(x,y) to lie within the probability simplex \u2206 via Z(x,w) = \u2211 y\u0302\u2208Y exp(F (x, y\u0302;w))\n1/ . The annealing/temperature parameter \u2265 0 is used to adjust the uniformity of the distribution. We consider general graphical models where the computation of Z (x,w) is #P-hard."}, {"heading": "2.1 LEARNING VIA GRADIENT DESCENT", "text": "During learning, given a training set D of input-output pairs (x, y) \u2208 D, we are interested in finding the parameters w of the model. We do so by maximizing the data likelihood, i.e., minimizing the negative log-likelihood \u2212 ln \u220f (x,y)\u2208D p(x,y)(y|w, ) which yields\nmin w \u2211 (x,y)\u2208D ( lnZ (x,w)\u2212 F (x, y;w)) . (1)\nNote that this is equivalent to maximizing the cross-entropy between a target distribution p(x,y),tg(y\u0302) = \u03b4(y\u0302 = y) placing all its mass on the groundtruth label, and the model distribution p(x,y)(y\u0302|w, ). Hence Eq. (1) is equivalently obtained by maxw \u2211 (x,y),y\u0302\u2208Y p(x,y),tg(y\u0302) ln p(x,y)(y\u0302|w, ). It is easily possible to incorporate more general target distributions into Eq. (1). Note also that = 0 recovers the structured hinge loss objective.\nMinimizing Eq. (1) w.r.t. w requires computation of the gradient \u2202\u2202w \u2211\n(x,y)\u2208D \u2212 ln p(x,y)(y|w, ), which is given by a transformed difference between the distributions of the model p(x,y)(y\u0302|w, ) and\nthe target p(x,y),tg(y\u0302):\u2211 (x,y)\u2208D \u2211 y\u0302\u2208Y \u2202 \u2202w F (x, y\u0302;w) ( p(x,y)(y\u0302|w, )\u2212 p(x,y),tg(y\u0302) ) . (2)\nA gradient descent algorithm for minimizing Eq. (1) will iterate between the following steps: (i) For a given w evaluate the function F , (ii) compute the model distribution p(x,y)(y\u0302|w, ), (iii) propagate the difference between the model and target distribution using a backward pass (resembling the chain rule for composite functions) and (iv) update the parameters w. This is summarized in Fig. 1."}, {"heading": "2.2 APPROXIMATE LEARNING", "text": "Note that for general graphical models the exact computation of p(x,y)(y\u0302|w, ) is not possible. As a consequence it is intractable to compute the exact gradient of the cost-function given in Eq. (2) and one has to resort to approximate solutions.\nInspired by approximations used for log-linear models, we make use of the following identity (Wainwright & Jordan, 2008; Koller & Friedman, 2009):\nlnZ (x,w) = max p(x,y)(y\u0302)\u2208\u2206 E[F (x, y\u0302;w)] + H(p(x,y)), (3)\nwhere E denotes an expectation over p(x,y)(y\u0302) and H refers to the entropy.\nFor most applications, F (x, y;w) decomposes into a sum of functions, each depending on a local subset of variables yr, i.e., F (x, y;w) = \u2211 r\u2208R fr(x, yr;w).Hereby r is a restriction of the variable tuple y = (y1, . . . , yN ) to the subset r \u2286 {1, . . . , N}, i.e., yr = (yi)i\u2208r. All subsets r required to compute the model function F are summarized in the setR. Plugging this decomposition into Eq. (3), we equivalently get the log-partition function lnZ (x,w) via maxp(x,y)(y\u0302)\u2208\u2206 \u2211 r,y\u0302r\np(x,y),r(y\u0302r)fr(x, y\u0302r;w) + H(p(x,y)), where we use marginals p(x,y),r(y\u0302r) = \u2211 y\\yr p(x,y)(y).\nDespite the assumed locality of the scoring function, the learning task remains computationally challenging since the entropyH(p(x,y)) can only be computed exactly for a very small set of models, e.g., models for which the joint distribution p(x,y)(y) is equivalently described by low tree-width models. In addition, the marginalization constraints are exponential in size.\nTo deal with both issues a common solution in log-linear models is to approximate the true marginals p(x,y),r with local beliefs b(x,y),r that are not required to fulfill marginalization constraints globally, but only locally (Wainwright & Jordan, 2008). That means marginals b(x,y),r are not required to arise from a common joint distribution p(x,y). In addition, we approximate the entropy via the fractional entropy (Wiegerinck & Heskes, 2003), i.e., H(p(x,y)) \u2248 \u2211 r crH(b(x,y),r). Counting numbers cr are employed to weight the marginal entropies. Putting all this together, we obtain the following approximation for lnZ (x,w):\nmax b(x,y)\u2208C(x,y) \u2211 r,y\u0302r b(x,y),r(y\u0302r)fr(x, y\u0302r;w) + \u2211 r crH(b(x,y),r). (4)\nHereby beliefs are constrained to the local polytope\nC(x,y) = { \u2200r b(x,y),r \u2208 \u2206 \u2200r, y\u0302r, p \u2208 P (r)\n\u2211 y\u0302p\\y\u0302r b(x,y),p(y\u0302p) = b(x,y),r(y\u0302r),\nwith P (r) the set of parents of region r, i.e., P (r) \u2286 {p \u2208 R : r \u2282 p}, which subsumes those regions for which we want the marginalization constraint to hold. Conversely, we define the set of children as C(r) = {c \u2208 R : r \u2208 P (c)}. We can thus rewrite the learning problem by plugging the approximations derived in Eq. (4) into Eq. (1). This gives rise to the new approximated learning program\nmin w \u2211 (x,y)\u2208D  max b(x,y)\u2208C(x,y) \u2211 r,y\u0302r b(x,y),r(y\u0302r)fr(x, y\u0302r;w) + \u2211 r crH(b(x,y),r) \u2212 F (x, y;w)  . (5)\nTo iteratively update the parameters for the non-smooth approximated cost function given in Eq. (5) we require a sub-gradient w.r.t. w, which in turn requires to solve the maximization w.r.t. the beliefs b exactly. This is a non-trivial task in itself as inference in general graphical models is NP-hard. Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed. Importantly, note that combining the procedure outlined in Fig. 1 with iterative message passing to approximate p(x,y)(y\u0302|w, ) results in a double-loop algorithm which would be slow for many graphical models of interest."}, {"heading": "2.3 EFFICIENT APPROXIMATE LEARNING BY BLENDING LEARNING AND INFERENCE", "text": "In this section we propose a more efficient algorithm that is based on the principle of blending learning (i.e., parameter updates) and inference. Thus we are interested in only performing a single message passing iteration before updating the parameters w. Note that simply reducing the number of iterations is generally not an option as the obtained beliefs b(x,y),r are by no means accurate. However, assuming all counting numbers cr to be positive, we can derive an algorithm that is able to interleave minimization w.r.t. w and maximization of the beliefs b. Such a procedure is more efficient as we are able to update the parameters w much more frequently.\nTo interleave both programs we convert maximization of the beliefs into a minimization by employing the dual program as detailed in the following claim. This is possible since the maximization problem is concave in b(x,y) if \u2200r, cr \u2265 0.\nClaim 1 Assume cr \u2265 0 \u2200r and let F (w) = \u2211\n(x,y)\u2208D F (x, y;w) denote the sum of empirical function observations. Let \u03bb(x,y),r\u2192p(y\u0302r) be the Lagrange multipliers for each marginalization constraint \u2211 y\u0302p\\y\u0302r b(x,y),p(y\u0302p) = b(x,y),r(y\u0302r) within the polytope C(x,y). Then the approximated general structured prediction task shown in Eq. (5) is equivalent to\nmin w,\u03bb \u2211 (x,y),r cr ln \u2211 y\u0302r exp f\u0302r(x, y\u0302r;w, \u03bb) cr \u2212 F (w), (6)\nwhere we employed the re-parameterization score f\u0302r(x, y\u0302r;w, \u03bb) = fr(x, y\u0302r;w) +\u2211 c\u2208C(r) \u03bb(x,y),c\u2192r(y\u0302c)\u2212 \u2211 p\u2208P (r) \u03bb(x,y),r\u2192p(y\u0302r).\nProof: To obtain the dual of the maximization w.r.t. b(x,y) we utilize its Lagrangian L(x,y) =\u2211 r,y\u0302r b(x,y),r(y\u0302r)f\u0302r(x, y\u0302r;w, \u03bb)+ \u2211 r crH(b(x,y),r).Maximization of the Lagrangian w.r.t. the primal variables b is possible by employing the relationship stated in Eq. (3) locally \u2200r. We then obtain the dual function being the first term in Eq. (6). For strict convexity, i.e., cr > 0, we reconstruct the beliefs to be proportional to the exponentiated, loss-augmented re-parameterization score b(x,y),r \u221d exp f\u0302r(x,y\u0302r;w,\u03bb) cr . For cr = 0 the beliefs correspond to a uniform distribution over the set of maximizers of the loss-augmented re-parameterization score f\u0302r(x, y\u0302r;w, \u03bb).\nIt is important to note that by applying duality we managed to convert the min-max task in Eq. (5) into a single minimization as shown in Eq. (6). Performing block coordinate descent updates to minimize Eq. (6), we are therefore able to interleave both, updating the weights (i.e., learning) and the messages (i.e., inference). This results in a more efficient algorithm, as inference does not have to be run until convergence. Even a single update of the messages suffices. We note that this is possible only if cr \u2265 0 \u2200r. Strictly speaking, we require concavity only within the set of feasible beliefs C(x,y). However, for simplicity we neglect this extension in the following. Fig. 2 summarizes our efficient deep structured prediction algorithm which iterates between the following steps. Given parameters w we perform a standard forward pass to compute fr(x, y\u0302r;w) for all regions. We then iterate through all regions r and use block-coordinate descent to find the globally optimal value of Eq. (6) w.r.t. \u03bb(x,y),r\u2192p(y\u0302r) \u2200(x, y), y\u0302r, p \u2208 P (r). This can be done in closed form and therefore is computed very efficiently. We refer the reader to Schwing (2013) for a derivation in the log-linear setting. We then compute the gradient using a standard backward pass before we update the parameters by performing a step of size \u03b7 along the negative gradient."}, {"heading": "2.4 IMPLEMENTATION DETAILS", "text": "We implemented the general algorithm presented in Fig. 2 in C++ as a library for Linux, Windows and OS X platforms. It supports usage of the GPU for the forward and backward pass using both, standard linear algebra packages and manually tuned GPU-kernels. In addition to standard gradient descent, we allow specification of both mini-batches, moments and different regularizers like 2-norm and \u221e-norm. Between iterations the step-size can be reduced based on either the negative log-likelihood or validation set performance. Contrasting available deep learning packages, our function F is specified using a general computation tree. Hence we support an arbitrarily nested function structure composed of data, parameters and function prototypes (convolution, affine function aka fully connected, dropout, local response normalization, pooling, rectified linear, sigmoid and softmax units). The aforementioned library is accompanied by a program performing learning, inference and gradient checks. To accommodate for large datasets it reads data from HDF5 storage while a second thread simultaneously performs the computation. Google protocol buffers are employed to effectively specify the function F without the need to modify any source code. We will release this library upon publication. We believe that it will be useful for many researchers."}, {"heading": "3 EXPERIMENTAL EVALUATION", "text": "We demonstrate the performance of our model on two tasks: word recognition and image classification. We investigate four strategies to learn the model parameters. \u2018Unary only\u2019 denotes training only unary classifiers while ignoring the structure of the graphical model, i.e., pairwise weights are equal to 0. \u2018JointTrain\u2019 initializes all weights at random and trains them jointly. \u2018PwTrain\u2019 uses piecewise training by first training the unary potentials and then keeping them fixed when learning the pairwise potentials. \u2018PreTrainJoint\u2019 pre-trains the unaries but jointly optimizes pairwise weights as well as unary weights in a second step."}, {"heading": "3.1 WORD RECOGNITION: WORD50", "text": "Our first task consists of word recognition from noisy images. Towards this goal, we created a challenging dataset by randomly selecting 50 words, each consisting of five characters. We then generated writing variations of each word as follows: we took the lower case characters from the Chars74K dataset (de Campos et al., 2009), and inserted them in random background image patches (similar to Larochelle et al. (2007)) by alpha matting, i.e., characters have transparency. To increase the difficulty, we perturbed each character image of size 28\u00d728 by scaling, rotation and translation. As shown in Fig. 3 the task is very challenging, some characters are fairly difficult to recognize even for humans. We denote the resulting dataset \u2018Word50.\u2019 The training, validation and test sets have 10, 000, 2, 000 and 2, 000 variations of words respectively.\nWe experimented with graphical models composed of unary and pairwise regions defined over five random variables, one per character. We encode unary potentials fr(x, yi;wu) using multi-layer perceptrons (MLPs) with rectified linear units (ReLU). Unless otherwise stated, we define all pairwise\ninteractions via\nfr(x, yi, yj ;wp) = \u2211 mn Wmn \u00b7 \u03b4(yi = m, yj = n), (7)\nwhere r = {i, j}, wp = {W}, Wmn is the element of matrix W , and \u03b4 refers to the indicator function.\nFor all experiments, we share all unary weights across the nodes of the graphical model as well as all pairwise weights for all edges. Note that due to the use of ReLU units, the negative log-likelihood is non-smooth, non-linear and non-convex w.r.t. w. Because of the non-smoothness of F , we utilize momentum based sub-gradient descent methods to estimate the weights. In particular, we use a mini-batch size of 100, a step size of 0.01 and a momentum of 0.95. If the unary potential is pretrained, the initial step size is reduced to 0.001. All the unary classifiers are trained with 100, 000 iterations over mini-batches. For all experiments, the validation set is only used to decrease the step size, i.e., if the accuracy on the validation set decreases, we reduce the step size by 0.5. We use = 1, set cr = 1 for all regions r, and perform 10 message passing iterations to compute the marginal beliefs b(x,y),r at step 2 in Fig. 2 when dealing with loopy models.\nWe experiment with two graphical models, Markov models of first (i.e., there are links only between yi and yi+1) and second order (i.e., there are links between yi and yi+1, yi+2) as well as two types of unary potentials with varying degree of structure. We report two metrics, the average character and word accuracy, which correspond to Hamming loss and zero-one loss respectively. Table 1 depicts the results for the different models, learning strategies and number of hidden units. We observe the following trends.\nJoint training helps: Joint training with pre-trained unary classifiers (PreTrainJoint) outperforms all the other approaches in almost all cases. The piecewise training method (PwTrain), unable to adapt the non-linearities while learning pairwise weights, often leads to performance worse than joint training.\nStructure helps: Adding structure to the model is key to capture complex dependencies. As shown in Table 1, more structured models (i.e., second order Markov model) consistently improves performance.\nDeep helps: We tested our models using one layer and two-layer perceptrons with both shortrange and long-range connections in the MRF. For the two-layer MLP, the number of hidden units in the first layer is fixed to H1 = 512, and we varied the number of hidden units H2 in the second layer. As shown in Table 1 we observe that the deeper and the more structured the model, the better the performance we achieve. As expected, performance also grows with the number of hidden units.\nEfficiency: Using GPUs, it takes on average 0.064s per iteration for the 1st order Markov model and 0.104s for the 2nd order Markov model. The time employed for training the one layer vs. the multi-layer models is approximately the same. Note that our approach is very efficient, as this is the time per iteration to train 831,166 weights.\nLearned parameters: As shown in the left column of Fig. 4, the learned unary weights resemble character strokes. The middle two panels show the learned pairwise weights for distance-1 edges (i.e., edges with only neighboring connections) and distance-2 edges (i.e., edges connecting every other variable). For example, it shows that \u2018q\u2019 is likely to be followed by \u2018u,\u2019 and \u2018e\u2019 is likely to be distance-2 away from \u2018q\u2019 in this dataset. On the right-most panel, we also show the negative log-likelihood as a function of the number of joint training iterations. PreTrainJoint can achieve the lowest cost value, while PwTrain has the highest value.\nNon-linear pairwise functions: To further demonstrate the generality of our approach, we replaced the linear pairwise function in Eq. (7) by a one-layer MLP, while keeping the other settings identical. For this experiment we utilize a 1st order Markov model. As shown in Fig. 5, our model attains best performance when using a non-linear pairwise function. We found 16 to 64 hidden units for the non-linear pairwise function to be sufficient for modeling the bi-gram combinations in this dataset. In this case the largest model has 974,846 weights and training takes on average 0.068s per iteration."}, {"heading": "3.2 IMAGE CLASSIFICATION: FLICKR", "text": "We next evaluate the importance of blending learning and inference. Towards this goal, we make use of the Flickr dataset, which consists of 10, 000 training and 10, 000 test images from Flickr. The task is to predict which of 38 possible tags should be assigned to each image. Fig. 6 shows some examples. The graphical model has 38 binary random variables, each denoting the presence/absence of a particular tag. We define the non-linear unaries fr(x, yi;wu) using the 8-layer deep-net architecture from Krizhevsky et al. (2013) followed by a 76-dimensional top layer. Hence the function is composed out of two subsequent stacks of convolution, rectified linear (ReLU), pooling and local response normalization units. Those are followed by three convolution\u2013ReLU function pairs. Afterwards pooling is applied before two fully-connected\u2013ReLU\u2013dropout combinations are employed to yield the input into a fully connected layer which finally computes the unary potentials. We employ pairwise potentials similar to Eq. (7) which now fully model the correlations between any pair of\noutput variables. This amounts to a total of 57, 182, 408 parameters arising from the convolutional units, fully connected units and corresponding biases as well as the pairwise weights.\nWe use a momentum based sub-gradient method for training with a mini-batch size of 300, a step size of 0.0001, a momentum of 0.95 and set = 1 and cr = 1 \u2200r. We initialized the deep-net parameters using a model pre-trained on ImageNet (Deng et al., 2009). Our error metric is the classification error (i.e., Hamming loss).\nJoint training helps: The mean error for unary only potentials (\u2018Unary only\u2019), piecewise training (\u2018PwTrain\u2019) and joint pretraining (\u2018PreTrainJoint\u2019) is 9.36%, 7.70% and 7.25% respectively. Similar to the Word50 dataset we observe that joint training is beneficial. We provide examples for perfect (two left-most images), roughly accurate and failing predictions (right image) in Fig. 6.\nLearned pairwise weights: In Fig. 7(a) we illustrate the learned correlations for a subset of the 38 classes. We observe that the class \u2018people\u2019 correlates highly with \u2018female,\u2019 \u2018male,\u2019 and \u2018portrait.\u2019 The \u2018indoor\u2019 tag does not co-occur with \u2018sky,\u2019 \u2018structures,\u2019 \u2018plant life\u2019 and \u2018tree.\u2019 \u2018Sea\u2019 appears typically with \u2018water,\u2019 \u2018clouds,\u2019 \u2018lake\u2019 and \u2018sky.\u2019\nEfficiency of Blending: To illustrate that blending is indeed beneficial we compare the negative log-likelihood and the training error as a function of run-time in Fig. 7(b). The standard approach is limited to 20 iterations of message passing to avoid time-consuming, repeated computation of a stopping criterion involving both the approximated log-partition function and its dual. As show in Fig. 7(b) blending learning and inference speeds up parameter estimation significantly. For larger graphical models, we expect the differences to be even more significant."}, {"heading": "4 DISCUSSION & CONCLUSION", "text": "Joint training of neural networks and graphical models: Neural Networks have been incorporated as unary potentials in graphical models. One of the earliest works by Bridle (1990) jointly optimizes a system consisting of multilayer perceptrons and hidden Markov models for speech recognition. For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition. Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities. However, they assume that exact inference can be performed either via a forward-backward pass within the graphical model or dynamic programming. In contrast, in this paper we present learning algorithms for general graphical models, where inference is hard. Moreover, all the previous works (except Do & Artieres (2010)) do not consider max-margin loss during training which is incorporated into our framework by choosing = 0. More recently, Li & Zemel (2014) use a hinge loss to learn the unary term defined as a neural net, but keep the pairwise potentials fixed (i.e., no joint training). Domke (2013) considers non-linear structured prediction and decomposes the learning problem into a subset of logistic regressors, which require the parameter updates to be run till convergence before updating the messages. Tompson et al. (2014) also jointly train convolutional neural networks and a graphical model for pose estimation. However, the MRF inference procedure is approximated by their Spatial-Model which ignores the partition function.\nBlending learning and inference: In this paper we defined learning to be a min-max task. The blending strategy, which was previously employed for learning log-linear models by (Meshi et al., 2010; Hazan & Urtasun, 2010), amounts to converting the maximization task into a minimization problem using its dual. Subsequently we make use of block-coordinate descent strategies to obtain a more efficient algorithm. Importantly any order of block-updates is possible. It remains an open problem to find the optimal tradeoff.\nWe have proposed an efficient algorithm to learn deep models enriched to capture the dependencies between the output variables. Our experiments on word prediction from noisy images and multiclass image classification showed that the deeper and the more structured the model, the better the performance we achieve. Furthermore, joint learning of all weights outperforms all other strategies. In the future we plan to learn deeper models in applications such as holistic semantic scene understanding. We will also extend our approach to deal with hidden variables."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank NVIDIA Corporation for the donation of GPUs used in this research. This work was partially funded by ONR-N00014-14-1-0232."}], "references": [{"title": "Greedy Layer-Wise Training of Deep Networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "In Proc. NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["L. Bottou", "Y. Bengio", "Y. LeCun"], "venue": "In Proc. CVPR,", "citeRegEx": "Bottou et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 1997}, {"title": "Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters", "author": ["J.S. Bridle"], "venue": "In Proc. NIPS,", "citeRegEx": "Bridle,? \\Q1990\\E", "shortCiteRegEx": "Bridle", "year": 1990}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Character recognition in natural images", "author": ["T.E. de Campos", "B.R. Babu", "M. Varma"], "venue": "In Proc. VISAPP,", "citeRegEx": "Campos et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Campos et al\\.", "year": 2009}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": "In Proc. CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Large-Scale Object Classification using Label Relation Graphs", "author": ["J. Deng", "N. Ding", "Y. Jia", "A. Frome", "K. Murphy", "S. Bengio", "Y. Li", "H. Neven", "H. Adam"], "venue": "In Proc. ECCV,", "citeRegEx": "Deng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2014}, {"title": "Neural conditional random fields", "author": ["Do", "T.-M.-T", "T. Artieres"], "venue": "In Proc. AISTATS,", "citeRegEx": "Do et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Do et al\\.", "year": 2010}, {"title": "Structured Learning via Logistic Regression", "author": ["J. Domke"], "venue": "In Proc. NIPS,", "citeRegEx": "Domke,? \\Q2013\\E", "shortCiteRegEx": "Domke", "year": 2013}, {"title": "Understanding Deep Architectures using a Recursive Convolutional Network", "author": ["D. Eigen", "J. Rolfe", "R. Fergus", "Y. LeCun"], "venue": "In Proc. ICLR,", "citeRegEx": "Eigen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Eigen et al\\.", "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In Proc. CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "In Proc. ECCV,", "citeRegEx": "Hariharan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hariharan et al\\.", "year": 2014}, {"title": "A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction", "author": ["T. Hazan", "R. Urtasun"], "venue": "In Proc. NIPS,", "citeRegEx": "Hazan and Urtasun,? \\Q2010\\E", "shortCiteRegEx": "Hazan and Urtasun", "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Boltzmann Machines: Constraint Satisfaction Networks that Learn", "author": ["G.E. Hinton", "T.J. Sejnowski", "D.H. Ackley"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Hinton et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1984}, {"title": "Caffe: An Open Source Convolutional Architecture for Fast Feature Embedding", "author": ["Y. Jia"], "venue": "http://caffe. berkeleyvision.org/,", "citeRegEx": "Jia,? \\Q2013\\E", "shortCiteRegEx": "Jia", "year": 2013}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Proc. NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2013}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In Proc. ICML,", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In Proc. ICML,", "citeRegEx": "Lee et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "High Order Regularization for Semi-Supervised Learning of Structured Output Problems", "author": ["Y. Li", "R. Zemel"], "venue": "In Proc. ICML,", "citeRegEx": "Li and Zemel,? \\Q2014\\E", "shortCiteRegEx": "Li and Zemel", "year": 2014}, {"title": "A conditional neural fields model for protein", "author": ["J. Ma", "J. Peng", "S. Wang", "J. Xu"], "venue": "threading. Bioinformatics,", "citeRegEx": "Ma et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2012}, {"title": "Convergent Message Passing Algorithms: a unifying view", "author": ["T. Meltzer", "A. Globerson", "Y. Weiss"], "venue": "In Proc. UAI,", "citeRegEx": "Meltzer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Meltzer et al\\.", "year": 2009}, {"title": "Learning Efficiently with Approximate inference via Dual Losses", "author": ["O. Meshi", "D. Sontag", "T. Jaakkola", "A. Globerson"], "venue": "In Proc. ICML,", "citeRegEx": "Meshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Meshi et al\\.", "year": 2010}, {"title": "Conditional random fields for integrating local discriminative classifiers", "author": ["J. Morris", "E. Fosler-Lussier"], "venue": "IEEE Trans. Audio, Speech, and Language Processing,", "citeRegEx": "Morris and Fosler.Lussier,? \\Q2008\\E", "shortCiteRegEx": "Morris and Fosler.Lussier", "year": 2008}, {"title": "Decision tree fields", "author": ["S. Nowozin", "C. Rother", "S. Bagon", "T. Sharp", "B. Yao", "P. Kohli"], "venue": "In Proc. ICCV,", "citeRegEx": "Nowozin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2011}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Backpropagation training for multilayer conditional random field based phone recognition", "author": ["R. Prabhavalkar", "E. Fosler-Lussier"], "venue": "In Proc. ICASSP,", "citeRegEx": "Prabhavalkar and Fosler.Lussier,? \\Q2010\\E", "shortCiteRegEx": "Prabhavalkar and Fosler.Lussier", "year": 2010}, {"title": "An Efficient Learning Procedure for Deep Boltzmann Machines", "author": ["R.R. Salakhutdinov", "G.E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2012}, {"title": "Inference and Learning Algorithms with Applications to 3D Indoor Scene Understanding", "author": ["A.G. Schwing"], "venue": "PhD thesis, ETH Zurich,", "citeRegEx": "Schwing,? \\Q2013\\E", "shortCiteRegEx": "Schwing", "year": 2013}, {"title": "Convolutional-Recursive Deep Learning for 3D Object Classification", "author": ["R. Socher", "B. Huval", "B. Bhat", "C.D. Manning", "A.Y. Ng"], "venue": "In Proc. NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation", "author": ["J. Tompson", "A. Jain", "Y. LeCun", "C. Bregler"], "venue": "In Proc. NIPS,", "citeRegEx": "Tompson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tompson et al\\.", "year": 2014}, {"title": "Graphical Models, Exponential Families and Variational Inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "A new class of upper bounds on the log partition function", "author": ["M.J. Wainwright", "T. Jaakkola", "A.S. Willsky"], "venue": null, "citeRegEx": "Wainwright et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2005}, {"title": "MAP Estimation, Linear Programming and Belief Propagation with Convex Free Energies", "author": ["Y. Weiss", "C. Yanover", "T. Meltzer"], "venue": "In Proc. UAI,", "citeRegEx": "Weiss et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2007}, {"title": "Fractional belief propagation", "author": ["W. Wiegerinck", "T. Heskes"], "venue": "In Proc. NIPS,", "citeRegEx": "Wiegerinck and Heskes,? \\Q2003\\E", "shortCiteRegEx": "Wiegerinck and Heskes", "year": 2003}, {"title": "Tell me what you see and I will show you where it is", "author": ["J. Xu", "A.G. Schwing", "R. Urtasun"], "venue": "In Proc. CVPR,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Trans. Information Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In Proc. ECCV,", "citeRegEx": "Zeiler and Fergus,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus", "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "A multiplicity of variants have been proposed (Hinton et al., 1984; LeCun et al., 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al.", "startOffset": 46, "endOffset": 191}, {"referenceID": 19, "context": "A multiplicity of variants have been proposed (Hinton et al., 1984; LeCun et al., 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al.", "startOffset": 46, "endOffset": 191}, {"referenceID": 0, "context": "A multiplicity of variants have been proposed (Hinton et al., 1984; LeCun et al., 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al.", "startOffset": 46, "endOffset": 191}, {"referenceID": 20, "context": ", 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014).", "startOffset": 218, "endOffset": 313}, {"referenceID": 31, "context": ", 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014).", "startOffset": 218, "endOffset": 313}, {"referenceID": 15, "context": ", 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014).", "startOffset": 218, "endOffset": 313}, {"referenceID": 17, "context": ", 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014).", "startOffset": 218, "endOffset": 313}, {"referenceID": 9, "context": ", 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014).", "startOffset": 218, "endOffset": 313}, {"referenceID": 17, "context": "Recently, state-of-the-art results have been achieved in many computer vision tasks, outperforming competitive methods by a large margin (Krizhevsky et al., 2013; Girshick et al., 2014).", "startOffset": 137, "endOffset": 185}, {"referenceID": 10, "context": "Recently, state-of-the-art results have been achieved in many computer vision tasks, outperforming competitive methods by a large margin (Krizhevsky et al., 2013; Girshick et al., 2014).", "startOffset": 137, "endOffset": 185}, {"referenceID": 32, "context": "In pose estimation, more accurate predictions can be obtained when encoding the spatial relationships between joint locations (Tompson et al., 2014).", "startOffset": 126, "endOffset": 148}, {"referenceID": 26, "context": "Existing approaches often rely on a two-step process (Nowozin et al., 2011; Xu et al., 2014) where a non-linear classifier that employs deep features is trained first, and its output is used to generate potentials for the structured predictor.", "startOffset": 53, "endOffset": 92}, {"referenceID": 37, "context": "Existing approaches often rely on a two-step process (Nowozin et al., 2011; Xu et al., 2014) where a non-linear classifier that employs deep features is trained first, and its output is used to generate potentials for the structured predictor.", "startOffset": 53, "endOffset": 92}, {"referenceID": 0, "context": ", 1998; Hinton & Salakhutdinov, 2006; Bengio et al., 2007; Salakhutdinov & Hinton, 2012; Zeiler & Fergus, 2014) and shown to be extremely successful in a wide variety of applications including object detection, speech recognition as well as natural language processing (Lee et al., 2009; Socher et al., 2012; Jia, 2013; Krizhevsky et al., 2013; Eigen et al., 2014). Recently, state-of-the-art results have been achieved in many computer vision tasks, outperforming competitive methods by a large margin (Krizhevsky et al., 2013; Girshick et al., 2014). Deep neural networks can, however, be even more powerful when combined with graphical models in order to capture the statistical dependencies between the variables of interest. For example, Deng et al. (2014) exploit mutual exclusion, overlapping and subsumption properties of class labels in order to better predict in large scale classification tasks.", "startOffset": 38, "endOffset": 762}, {"referenceID": 11, "context": ", independently learned segmentation and detection features (Hariharan et al., 2014) might be focusing on predicting the same examples correctly.", "startOffset": 60, "endOffset": 84}, {"referenceID": 27, "context": "Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed.", "startOffset": 37, "endOffset": 139}, {"referenceID": 38, "context": "Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed.", "startOffset": 37, "endOffset": 139}, {"referenceID": 34, "context": "Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed.", "startOffset": 37, "endOffset": 139}, {"referenceID": 35, "context": "Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed.", "startOffset": 37, "endOffset": 139}, {"referenceID": 23, "context": "Iterative message passing algorithms (Pearl, 1988; Yedidia et al., 2005; Wainwright et al., 2005; Weiss et al., 2007; Meltzer et al., 2009) are typically employed.", "startOffset": 37, "endOffset": 139}, {"referenceID": 30, "context": "We refer the reader to Schwing (2013) for a derivation in the log-linear setting.", "startOffset": 23, "endOffset": 38}, {"referenceID": 4, "context": "We then generated writing variations of each word as follows: we took the lower case characters from the Chars74K dataset (de Campos et al., 2009), and inserted them in random background image patches (similar to Larochelle et al. (2007)) by alpha matting, i.", "startOffset": 126, "endOffset": 238}, {"referenceID": 17, "context": "We define the non-linear unaries fr(x, yi;wu) using the 8-layer deep-net architecture from Krizhevsky et al. (2013) followed by a 76-dimensional top layer.", "startOffset": 91, "endOffset": 116}, {"referenceID": 5, "context": "We initialized the deep-net parameters using a model pre-trained on ImageNet (Deng et al., 2009).", "startOffset": 77, "endOffset": 96}, {"referenceID": 3, "context": "Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities.", "startOffset": 14, "endOffset": 163}, {"referenceID": 22, "context": "Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities.", "startOffset": 14, "endOffset": 163}, {"referenceID": 24, "context": "The blending strategy, which was previously employed for learning log-linear models by (Meshi et al., 2010; Hazan & Urtasun, 2010), amounts to converting the maximization task into a minimization problem using its dual.", "startOffset": 87, "endOffset": 130}, {"referenceID": 1, "context": "One of the earliest works by Bridle (1990) jointly optimizes a system consisting of multilayer perceptrons and hidden Markov models for speech recognition.", "startOffset": 29, "endOffset": 43}, {"referenceID": 1, "context": "For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition.", "startOffset": 33, "endOffset": 54}, {"referenceID": 1, "context": "For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition. Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities. However, they assume that exact inference can be performed either via a forward-backward pass within the graphical model or dynamic programming. In contrast, in this paper we present learning algorithms for general graphical models, where inference is hard. Moreover, all the previous works (except Do & Artieres (2010)) do not consider max-margin loss during training which is incorporated into our framework by choosing = 0.", "startOffset": 33, "endOffset": 738}, {"referenceID": 1, "context": "For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition. Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities. However, they assume that exact inference can be performed either via a forward-backward pass within the graphical model or dynamic programming. In contrast, in this paper we present learning algorithms for general graphical models, where inference is hard. Moreover, all the previous works (except Do & Artieres (2010)) do not consider max-margin loss during training which is incorporated into our framework by choosing = 0. More recently, Li & Zemel (2014) use a hinge loss to learn the unary term defined as a neural net, but keep the pairwise potentials fixed (i.", "startOffset": 33, "endOffset": 878}, {"referenceID": 1, "context": "For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition. Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities. However, they assume that exact inference can be performed either via a forward-backward pass within the graphical model or dynamic programming. In contrast, in this paper we present learning algorithms for general graphical models, where inference is hard. Moreover, all the previous works (except Do & Artieres (2010)) do not consider max-margin loss during training which is incorporated into our framework by choosing = 0. More recently, Li & Zemel (2014) use a hinge loss to learn the unary term defined as a neural net, but keep the pairwise potentials fixed (i.e., no joint training). Domke (2013) considers non-linear structured prediction and decomposes the learning problem into a subset of logistic regressors, which require the parameter updates to be run till convergence before updating the messages.", "startOffset": 33, "endOffset": 1023}, {"referenceID": 1, "context": "For document processing systems, Bottou et al. (1997) propose Graph Transformer Networks to jointly optimize sub-tasks, such as word segmentation and character recognition. Several works (Collobert et al., 2011; Peng et al., 2009; Ma et al., 2012; Do & Artieres, 2010; Prabhavalkar & Fosler-Lussier, 2010; Morris & Fosler-Lussier, 2008) have extended the linear unary potential in MRFs to incorporate non-linearities. However, they assume that exact inference can be performed either via a forward-backward pass within the graphical model or dynamic programming. In contrast, in this paper we present learning algorithms for general graphical models, where inference is hard. Moreover, all the previous works (except Do & Artieres (2010)) do not consider max-margin loss during training which is incorporated into our framework by choosing = 0. More recently, Li & Zemel (2014) use a hinge loss to learn the unary term defined as a neural net, but keep the pairwise potentials fixed (i.e., no joint training). Domke (2013) considers non-linear structured prediction and decomposes the learning problem into a subset of logistic regressors, which require the parameter updates to be run till convergence before updating the messages. Tompson et al. (2014) also jointly train convolutional neural networks and a graphical model for pose estimation.", "startOffset": 33, "endOffset": 1255}], "year": 2015, "abstractText": "Many problems in real-world applications involve predicting several random variables which are statistically related. Markov random fields (MRFs) are a great mathematical tool to encode such relationships. The goal of this paper is to combine MRFs with deep learning algorithms to estimate complex representations while taking into account the dependencies between the output random variables. Towards this goal, we propose a training algorithm that is able to learn structured models jointly with deep features that form the MRF potentials. Our approach is efficient as it blends learning and inference and makes use of GPU acceleration. We demonstrate the effectiveness of our algorithm in the tasks of predicting words from noisy images, as well as multi-class classification of Flickr photographs. We show that joint learning of the deep features and the MRF parameters results in significant performance gains.", "creator": "LaTeX with hyperref package"}}}