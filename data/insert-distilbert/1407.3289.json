{"id": "1407.3289", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jul-2014", "title": "Altitude Training: Strong Bounds for Single-Layer Dropout", "abstract": "dropout training, originally designed for deep - neural networks, always has been fairly successful on high - dimensional single - layer natural language tasks. perhaps this paper proposes a theoretical critical explanation for this phenomenon : we show that, under a generative poisson topic model with long documents, dropout training improves atop the exponent in the generalization bound for empirical risk minimization. dropout achieves this gain much like a marathon runner who practices at altitude : once a classifier learns to perform reasonably well on training examples that have been artificially corrupted by dropout, it will do very well on the uncorrupted consensus test set. we also show that, under similar conditions, dropout preserves down the bayes decision boundary and should therefore induce nearly minimal bias in already high dimensions.", "histories": [["v1", "Fri, 11 Jul 2014 20:32:34 GMT  (99kb,D)", "https://arxiv.org/abs/1407.3289v1", null], ["v2", "Fri, 31 Oct 2014 18:30:18 GMT  (101kb,D)", "http://arxiv.org/abs/1407.3289v2", "Advances in Neural Information Processing Systems (NIPS), 2014"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.ST stat.TH", "authors": ["stefan wager", "william fithian", "sida i wang", "percy liang"], "accepted": true, "id": "1407.3289"}, "pdf": {"name": "1407.3289.pdf", "metadata": {"source": "CRF", "title": "Altitude Training: Strong Bounds for Single-Layer Dropout", "authors": ["Stefan Wager", "William Fithian", "Sida Wang"], "emails": ["wfithian}@stanford.edu,", "pliang}@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Dropout training [1] is an increasingly popular method for regularizing learning algorithms. Dropout is most commonly used for regularizing deep neural networks [2, 3, 4, 5], but it has also been found to improve the performance of logistic regression and other single-layer models for natural language tasks such as document classification and named entity recognition [6, 7, 8]. For single-layer linear models, learning with dropout is equivalent to using \u201cblankout noise\u201d [9].\nThe goal of this paper is to gain a better theoretical understanding of why dropout regularization works well for natural language tasks. We focus on the task of document classification using linear classifiers where data comes from a generative Poisson topic model. In this setting, dropout effectively deletes random words from a document during training; this corruption makes the training examples harder. A classifier that is able to fit the training data will therefore receive an accuracy boost at test time on the much easier uncorrupted examples. An apt analogy is altitude training, where athletes practice in more difficult situations than they compete in. Importantly, our analysis does not rely on dropout merely creating more pseudo-examples for training, but rather on dropout creating more challenging training examples. Somewhat paradoxically, we show that removing information from training examples can induce a classifier that performs better at test time.\nMain Result Consider training the zero-one loss empirical risk minimizer (ERM) using dropout, where each word is independently removed with probability \u03b4 \u2208 (0, 1). For a class of Poisson generative topic models, we show that dropout gives rise to what we call the altitude training phenomenon: dropout improves the excess risk of the ERM by multiplying the exponent in its decay rate by 1/(1 \u2212 \u03b4). This improvement comes at the cost of an additive term of O(1/ \u221a \u03bb), where \u03bb is the average number of words per document. More formally, let h\u2217 and h\u03020 be the expected and\nS. Wager and W. Fithian are supported by a B.C. and E.J. Eaves Stanford Graduate Fellowship and NSF VIGRE grant DMS\u20130502385 respectively.\nar X\niv :1\n40 7.\n32 89\nv2 [\nst at\n.M L\n] 3\n1 O\nct 2\nempirical risk minimizers, respectively; let h\u2217\u03b4 and h\u0302\u03b4 be the corresponding quantities for dropout training. Let Err(h) denote the error rate (on test examples) of h. In Section 4, we show that:\nErr ( h\u0302\u03b4 ) \u2212 Err (h\u2217\u03b4)\ufe38 \ufe37\ufe37 \ufe38\ndropout excess risk\n= O\u0303P (Err(h\u03020)\u2212 Err (h\u2217))\ufe38 \ufe37\ufe37 \ufe38 ERM excess risk 1 1\u2212\u03b4 + 1\u221a \u03bb  , (1) where O\u0303P is a variant of big-O in probability notation that suppresses logarithmic factors. If \u03bb is large (we are classifying long documents rather than short snippets of text), dropout considerably accelerates the decay rate of excess risk. The bound (1) holds for fixed choices of \u03b4. The constants in the bound worsen as \u03b4 approaches 1, and so we cannot get zero excess risk by sending \u03b4 to 1.\nOur result is modular in that it converts upper bounds on the ERM excess risk to upper bounds on the dropout excess risk. For example, recall from classic VC theory that the ERM excess risk is O\u0303P ( \u221a d/n), where d is the number of features (vocabulary size) and n is the number of training examples. With dropout \u03b4 = 0.5, our result (1) directly implies that the dropout excess risk is O\u0303P (d/n+ 1/ \u221a \u03bb).\nThe intuition behind the proof of (1) is as follows: when \u03b4 = 0.5, we essentially train on half documents and test on whole documents. By conditional independence properties of the generative topic model, the classification score is roughly Gaussian under a Berry-Esseen bound, and the error rate is governed by the tails of the Gaussian. Compared to half documents, the coefficient of variation of the classification score on whole documents (at test time) is scaled down by \u221a 1\u2212 \u03b4 compared to half documents (at training time), resulting in an exponential reduction in error. The additive penalty of 1/ \u221a \u03bb stems from the Berry-Esseen approximation.\nNote that the bound (1) only controls the dropout excess risk. Even if dropout reduces the excess risk, it may introduce a bias Err(h\u2217\u03b4)\u2212Err(h\u2217), and thus (1) is useful only when this bias is small. In Section 5, we will show that the optimal Bayes decision boundary is not affected by dropout under the Poisson topic model. Bias is thus negligible when the Bayes boundary is close to linear.\nIt is instructive to compare our generalization bound to that of Ng and Jordan [10], who showed that the naive Bayes classifier exploits a strong generative assumption\u2014conditional independence of the features given the label\u2014to achieve an excess risk of OP ( \u221a (log d)/n). However, if the generative assumption is incorrect, then naive Bayes can have a large bias. Dropout enables us to cut excess risk without incurring as much bias. In fact, naive Bayes is closely related to logistic regression trained using an extreme form of dropout with \u03b4 \u2192 1. Training logistic regression with dropout rates from the range \u03b4 \u2208 (0, 1) thus gives a family of classifiers between unregularized logistic regression and naive Bayes, allowing us to tune the bias-variance tradeoff.\nOther perspectives on dropout In the general setting, dropout only improves generalization by a multiplicative factor. McAllester [11] used the PAC-Bayes framework to prove a generalization bound for dropout that decays as 1 \u2212 \u03b4. Moreover, provided that \u03b4 is not too close to 1, dropout behaves similarly to an adaptive L2 regularizer with parameter \u03b4/(1\u2212\u03b4) [6, 12], and at least in linear regression such L2 regularization improves generalization error by a constant factor. In contrast, by leveraging the conditional independence assumptions of the topic model, we are able to improve the exponent in the rate of convergence of the empirical risk minimizer.\nIt is also possible to analyze dropout as an adaptive regularizer [6, 9, 13]: in comparison with L2 regularization, dropout favors the use of rare features and encourages confident predictions. If we believe that good document classification should produce confident predictions by understanding rare words with Poisson-like occurrence patterns, then the work on dropout as adaptive regularization and our generalization-based analysis are two complementary explanations for the success of dropout in natural language tasks."}, {"heading": "2 Dropout Training for Topic Models", "text": "In this section, we introduce binomial dropout, a form of dropout suitable for topic models, and the Poisson topic model, on which all our analyses will be based.\nBinomial Dropout Suppose that we have a binary classification problem1 with count features x(i) \u2208 {0, 1, 2, . . .}d and labels y(i) \u2208 {0, 1}. For example, x(i)j is the number of times the j-th word in our dictionary appears in the i-th document, and y(i) is the label of the document. Our goal is to train a weight vector w\u0302 that classifies new examples with features x via a linear decision rule y\u0302 = I{w\u0302 \u00b7 x > 0}. We start with the usual empirical risk minimizer:\nw\u03020 def = argminw\u2208Rd { n\u2211 i=1 ` ( w; x(i), y(i) )} (2)\nfor some loss function ` (we will analyze the zero-one loss but use logistic loss in experiments, e.g., [10, 14, 15]). Binomial dropout trains on perturbed features x\u0303(i) instead of the original features x(i):\nw\u0302\u03b4 def = argminw { n\u2211 i=1 E [ ` ( w; x\u0303(i), y(i) )]} , where x\u0303(i)j = Binom ( x (i) j ; 1\u2212 \u03b4 ) . (3)\nIn other words, during training, we randomly thin the j-th feature xj with binomial noise. If xj counts the number of times the j-th word appears in the document, then replacing xj with x\u0303j is equivalent to independently deleting each occurrence of word j with probability \u03b4. Because we are only interested in the decision boundary, we do not scale down the weight vector obtained by dropout by a factor 1\u2212 \u03b4 as is often done [1]. Binomial dropout differs slightly from the usual definition of (blankout) dropout, which alters the feature vector x by setting random coordinates to 0 [6, 9, 11, 12]. The reason we chose to study binomial rather than blankout dropout is that Poisson random variables remain Poisson even after binomial thinning; this fact lets us streamline our analysis. For rare words that appear once in the document, the two types of dropout are equivalent.\nA Generative Poisson Topic Model Throughout our analysis, we assume that the data is drawn from a Poisson topic model depicted in Figure 1a and defined as follows. Each document i is assigned a label y(i) according to some Bernoulli distribution. Then, given the label y(i), the document gets a topic \u03c4 (i) \u2208 \u0398 from a distribution \u03c1y(i) . Given the topic \u03c4 (i), for every word j in the vocabulary, we generate its frequency x(i)j according to x (i) j\n\u2223\u2223 \u03c4 (i) \u223c Poisson(\u03bb(\u03c4(i))j ), where \u03bb(\u03c4)j \u2208 [0,\u221e) is the expected number of times word j appears under topic \u03c4 . Note that \u2016\u03bb(\u03c4)\u20161 is the average length of a document with topic \u03c4 . Define \u03bb def= min\u03c4\u2208\u0398 \u2016\u03bb(\u03c4)\u20161 to be the shortest average document length across topics. If \u0398 contains only two topics\u2014one for each class\u2014we get the naive Bayes model. If \u0398 is the (K \u2212 1)-dimensional simplex where \u03bb(\u03c4) is a \u03c4 -mixture over K basis vectors, we get the K-topic latent Dirichlet allocation [16].2\nNote that although our generalization result relies on a generative model, the actual learning algorithm is agnostic to it. Our analysis shows that dropout can take advantage of a generative structure while remaining a discriminative procedure. If we believed that a certain topic model held exactly and we knew the number of topics, we could try to fit the full generative model by EM. This, however, could make us vulnerable to model misspecification. In contrast, dropout benefits from generative assumptions while remaining more robust to misspecification."}, {"heading": "3 Altitude Training: Linking the Dropout and Data-Generating Measures", "text": "Our goal is to understand the behavior of a classifier h\u0302\u03b4 trained using dropout. During dropout, the error of any classifier h is characterized by two measures. In the end, we are interested in the usual generalization error (expected risk) of hwhere x is drawn from the underlying data-generating measure:\nErr (h) def = P [y 6= h(x)] . (4)\n1Dropout training is known to work well in practice for multi-class problems [8]. For simplicity, however, we will restrict our theoretical analysis to a two-class setup.\n2 In topic modeling, the vertices of the simplex \u0398 are \u201ctopics\u201d and \u03c4 is a mixture of topics, whereas we call \u03c4 itself a topic.\nHowever, since dropout training works on the corrupted data x\u0303 (see (3)), in the limit of infinite data, the dropout estimator will converge to the minimizer of the generalization error with respect to the dropout measure over x\u0303:\nErr\u03b4 (h) def = P [y 6= h(x\u0303)] . (5)\nThe main difficulty in analyzing the generalization of dropout is that classical theory tells us that the generalization error with respect to the dropout measure will decrease as n \u2192 \u221e, but we are interested in the original measure. Thus, we need to bound Err in terms of Err\u03b4 . In this section, we show that the error on the original measure is actually much smaller than the error on the dropout measure; we call this the altitude training phenomenon.\nUnder our generative model, the count features xj are conditionally independent given the topic \u03c4 . We thus focus on a single fixed topic \u03c4 and establish the following theorem, which provides a per-topic analogue of (1). Section 4 will then use this theorem to obtain our main result. Theorem 1. Let h be a binary linear classifier with weights w, and suppose that our features are drawn from the Poisson generative model given topic \u03c4 . Let c\u03c4 be the more likely label given \u03c4 :\nc\u03c4 def = arg max c\u2208{0,1} P [ y(i) = c \u2223\u2223 \u03c4 (i) = \u03c4] . (6) Let \u03b5\u0303\u03c4 be the sub-optimal prediction rate in the dropout measure\n\u03b5\u0303\u03c4 def = P [ I { w \u00b7 x\u0303(i) > 0 } 6= c\u03c4 \u2223\u2223 \u03c4 (i) = \u03c4] , (7) where x\u0303(i) is an example thinned by binomial dropout (3), and P is taken over the data-generating process. Let \u03b5\u03c4 be the sub-optimal prediction rate in the original measure\n\u03b5\u03c4 def = P [ I { w \u00b7 x(i) > 0 } 6= c\u03c4 \u2223\u2223 \u03c4 (i) = \u03c4] . (8) Then:\n\u03b5\u03c4 = O\u0303 ( \u03b5\u0303 1 1\u2212\u03b4 \u03c4 + \u221a \u03a8\u03c4 ) , (9)\nwhere \u03a8\u03c4 = maxj { w2j } / \u2211d j=1 \u03bb (\u03c4) j w 2 j , and the constants in the bound depend only on \u03b4.\nTheorem 1 only provides us with a useful bound when the term \u03a8\u03c4 is small. Whenever the largest w2j is not much larger than the average w 2 j , then \u221a \u03a8\u03c4 scales as O(1/ \u221a \u03bb), where \u03bb is the average document length. Thus, the bound (9) is most useful for long documents.\nA Heuristic Proof of Theorem 1. The proof of Theorem 1 is provided in the technical appendix. Here, we provide a heuristic argument for intuition. Given a fixed topic \u03c4 , suppose that it is optimal to predict c\u03c4 = 1, so our test error is \u03b5\u03c4 = P [ w \u00b7 x \u2264 0 \u2223\u2223 \u03c4] . For long enough documents, by the central limit theorem, the score s def= w \u00b7 x will be roughly Gaussian s \u223c N ( \u00b5\u03c4 , \u03c3 2 \u03c4 ) , where\n\u00b5\u03c4 = \u2211d j=1 \u03bb (\u03c4) j wj and \u03c3 2 \u03c4 = \u2211d j=1 \u03bb (\u03c4) j w 2 j . This implies that \u03b5\u03c4 \u2248 \u03a6 (\u2212\u00b5\u03c4/\u03c3\u03c4 ) , where \u03a6 is the cumulative distribution function of the Gaussian. Now, let s\u0303 def= w \u00b7 x\u0303 be the score on a dropout sample. Clearly, E [s\u0303] = (1 \u2212 \u03b4)\u00b5\u03c4 and Var [s\u0303] = (1 \u2212 \u03b4)\u03c32\u03c4 , because the variance of a Poisson random variable scales with its mean. Thus,\n\u03b5\u0303\u03c4 \u2248 \u03a6 ( \u2212 \u221a\n1\u2212 \u03b4 \u00b5\u03c4 \u03c3\u03c4\n) \u2248 \u03a6 ( \u2212\u00b5\u03c4 \u03c3\u03c4 )(1\u2212\u03b4) \u2248 \u03b5(1\u2212\u03b4)\u03c4 . (10)\nFigure 1b illustrates the relationship between the two Gaussians. This explains the first term on the right-hand side of (9). The extra error term \u221a \u03a8\u03c4 arises from a Berry-Esseen bound that approximates Poisson mixtures by Gaussian random variables."}, {"heading": "4 A Generalization Bound for Dropout", "text": "By setting up a bridge between the dropout measure and the original data-generating measure, Theorem 1 provides a foundation for our analysis. It remains to translate this result into a statement about the generalization error of dropout. For this, we need to make a few assumptions.\nOur first assumption is fundamental: if the classification signal is concentrated among just a few features, then we cannot expect dropout training to do well. The second and third assumptions, which are more technical, guarantee that a classifier can only do well overall if it does well on every topic; this lets us apply Theorem 1. A more general analysis that relaxes Assumptions 2 and 3 may be an interesting avenue for future work.\nAssumption 1: well-balanced weights First, we need to assume that all the signal is not concentrated in a few features. To make this intuition formal, we say a linear classifier with weights w is well-balanced if the following holds for each topic \u03c4 :\nmaxj { w2j } \u2211d\nj=1 \u03bb (\u03c4) j\u2211d\nj=1 \u03bb (\u03c4) j w 2 j\n\u2264 \u03ba for some 0 < \u03ba <\u221e. (11)\nFor example, suppose each word was either useful (|wj | = 1) or not (wj = 0); then \u03ba is the inverse expected fraction of words in a document that are useful. In Theorem 2 we restrict the ERM to well-balanced classifiers and assume that the expected risk minimizer h\u2217 over all linear rules is also well-balanced.\nAssumption 2: discrete topics Second, we assume that there are a finite number T of topics, and that the available topics are not too rare or ambiguous: the minimal probability of observing any topic \u03c4 is bounded below by P [\u03c4 ] \u2265 pmin > 0, (12) and that each topic-conditional probability is bounded away from 12 (random guessing):\u2223\u2223\u2223\u2223P [y(i) = c \u2223\u2223 \u03c4 (i) = \u03c4]\u2212 12\n\u2223\u2223\u2223\u2223 \u2265 \u03b1 > 0 (13) for all topics \u03c4 \u2208 {1, ..., T}. This assumption substantially simplifies our arguments, allowing us to apply Theorem 1 to each topic separately without technical overhead.\nAssumption 3: distinct topics Finally, as an extension of Assumption 2, we require that the topics be \u201cwell separated.\u201d First, define Errmin = P[y(i) 6= c\u03c4(i) ], where c\u03c4 is the most likely label given topic \u03c4 (6); this is the error rate of the optimal decision rule that sees topic \u03c4 . We assume that the best linear rule h\u2217\u03b4 satisfying (11) is almost as good as always guessing the best label c\u03c4 under the dropout measure:\nErr\u03b4 (h \u2217 \u03b4) = Errmin +O ( 1\u221a \u03bb ) , (14)\nwhere, as usual, \u03bb is a lower bound on the average document length. If the dimension d is larger than the number of topics T , this assumption is fairly weak: the condition (14) holds whenever the matrix \u03a0 of topic centers has full rank, and the minimum singular value of \u03a0 is not too small (see Proposition 6 in the Appendix for details). This assumption is satisfied if the different topics can be separated from each other with a large margin.\nUnder Assumptions 1\u20133 we can turn Theorem 1 into a statement about generalization error. Theorem 2. Suppose that our features x are drawn from the Poisson generative model (Figure 1a), and Assumptions 1\u20133 hold. Define the excess risks of the dropout classifier h\u0302\u03b4 on the dropout and data-generating measures, respectively:\n\u03b7\u0303 def = Err\u03b4 ( h\u0302\u03b4 ) \u2212 Err\u03b4 (h\u2217\u03b4) and \u03b7 def = Err ( h\u0302\u03b4 ) \u2212 Err (h\u2217\u03b4) . (15)\nThen, the altitude training phenomenon applies: \u03b7 = O\u0303 ( \u03b7\u0303 1 1\u2212\u03b4 +\n1\u221a \u03bb\n) . (16)\nThe above bound scales linearly in p\u22121min and \u03b1 \u22121; the full dependence on \u03b4 is shown in the appendix.\nIn a sense, Theorem 2 is a meta-generalization bound that allows us to transform generalization bounds with respect to the dropout measure (\u03b7\u0303) into ones on the data-generating measure (\u03b7) in a modular way. As a simple example, standard VC theory provides an \u03b7\u0303 = O\u0303P ( \u221a d/n) bound which, together with Theorem 2, yields:\nCorollary 3. Under the same conditions as Theorem 2, the dropout classifier h\u0302\u03b4 achieves the following excess risk:\nErr ( h\u0302\u03b4 ) \u2212 Err (h\u2217\u03b4) = O\u0303P (\u221a d n ) 1 1\u2212\u03b4 + 1\u221a \u03bb  . (17) More generally, we can often check that upper bounds for Err(h\u0302) \u2212 Err(h\u2217) also work as upper bounds for Err\u03b4(h\u0302\u03b4)\u2212 Err\u03b4(h\u2217\u03b4); this gives us the heuristic result from (1)."}, {"heading": "5 The Bias of Dropout", "text": "In the previous section, we showed that under the Poisson topic model in Figure 1a, dropout can achieve a substantial cut in excess risk Err(h\u0302\u03b4)\u2212Err(h\u2217\u03b4). But to complete our picture of dropout\u2019s performance, we must address the bias of dropout: Err(h\u2217\u03b4)\u2212 Err(h\u2217). Dropout can be viewed as importing \u201chints\u201d from a generative assumption about the data. Each observed (x, y) pair (each labeled document) gives us information not only about the conditional class probability at x, but also about the conditional class probabilities at numerous other hypothetical values x\u0303 representing shorter documents of the same class that did not occur. Intuitively, if these x\u0303 are actually good representatives of that class, the bias of dropout should be mild.\nFor our key result in this section, we will take the Poisson generative model from Figure 1a, but further assume that document length is independent of the topic. Under this assumption, we will show that dropout preserves the Bayes decision boundary in the following sense: Proposition 4. Let (x, y) be distributed according to the Poisson topic model of Figure 1a. Assume that document length is independent of topic: \u2016\u03bb(\u03c4)\u20161 = \u03bb for all topics \u03c4 . Let x\u0303 be a binomial dropout sample of x with some dropout probability \u03b4 \u2208 (0, 1). Then, for every feature vector v \u2208 Rd, we have:\nP [ y = 1 \u2223\u2223 x\u0303 = v] = P [y = 1 \u2223\u2223x = v] . (18) If we had an infinite amount of data (x\u0303, y) corrupted under dropout, we would predict according to I{P [ y = 1\n\u2223\u2223 x\u0303 = v] > 12}. The significance of Proposition 4 is that this decision rule is identical to the true Bayes decision boundary (without dropout). Therefore, the empirical risk minimizer of a sufficiently rich hypothesis class trained with dropout would incur very small bias.\nHowever, Proposition 4 does not guarantee that dropout incurs no bias when we fit a linear classifier. In general, the best linear approximation for classifying shorter documents is not necessarily the best for classifying longer documents. As n \u2192 \u221e, a linear classifier trained on (x, y) pairs will eventually outperform one trained on (x\u0303, y) pairs.\nDropout for Logistic Regression To gain some more intuition about how dropout affects linear classifiers, we consider logistic regression. A similar phenomenon should also hold for the ERM, but discussing this solution is more difficult since the ERM solution does not have have a simple characterization. The relationship between the 0-1 loss and convex surrogates has been studied by, e.g., [14, 15]. The score criterion for logistic regression is 0 = \u2211n i=1 ( y(i) \u2212 p\u0302i ) x(i), where p\u0302i = (1 + e \u2212w\u0302\u00b7x(i))\u22121 are the fitted probabilities. Note that easily-classified examples (where p\u0302i is close to y(i)) play almost no role in driving the fit. Dropout turns easy examples into hard examples, giving more examples a chance to participate in learning a good classification rule.\nFigure 2a illustrates dropout\u2019s tendency to spread influence more democratically for a simple classification problem with d = 2. The red class is a 99:1 mixture over two topics, one of which is much less common, but harder to classify, than the other. There is only one topic for the blue class. For long documents (open circles in the top right), the infrequent, hard-to-classify red cluster dominates the fit while the frequent, easy-to-classify red cluster is essentially ignored. For dropout documents with \u03b4 = 0.75 (small dots, lower left), both red clusters are relatively hard to classify, so the infrequent one plays a less disproportionate role in driving the fit. As a result, the fit based on dropout is more stable but misses the finer structure near the decision boundary. Note that the solid gray curve, the Bayes boundary, is unaffected by dropout, per Proposition 4. But, because it is nonlinear, we obtain a different linear approximation under dropout."}, {"heading": "6 Experiments and Discussion", "text": "Synthetic Experiment Consider the following instance of the Poisson topic model: We choose the document label uniformly at random: P [ y(i) = 1 ] = 12 . Given label 0, we choose topic \u03c4 (i) = 0 deterministically; given label 1, we choose a real-valued topic \u03c4 (i) \u223c Exp(3). The per-topic Poisson intensities \u03bb(\u03c4) are defined as follows:\n\u03b8(\u03c4) =  (1, . . . , 1 \u2223\u2223 0, . . . , 0 \u2223\u2223 0, . . . , 0) if \u03c4 = 0, (0, . . . , 0\ufe38 \ufe37\ufe37 \ufe38\n7\n\u2223\u2223 \u03c4, . . . , \u03c4\ufe38 \ufe37\ufe37 \ufe38 7 \u2223\u2223 0, . . . , 0\ufe38 \ufe37\ufe37 \ufe38 486 ) otherwise, \u03bb(\u03c4)j = 1000 \u00b7 e\u03b8 (\u03c4) j\u2211500 j\u2032=1 e \u03b8 (\u03c4) j\u2032 . (19)\nThe first block of 7 independent words are indicative of label 0, the second block of 7 correlated words are indicative of label 1, and the remaining 486 words are indicative of neither.\nWe train a model on training sets of various size n, and evaluate the resulting classifiers\u2019 error rates on a large test set. For dropout, we recalibrate the intercept on the training set. Figure 2b shows the results. There is a clear bias-variance tradeoff, with logistic regression (\u03b4 = 0) and naive Bayes (\u03b4 = 1) on the two ends of the spectrum.3 For moderate values of n, dropout improves performance, with \u03b4 = 0.95 (resulting in roughly 50-word documents) appearing nearly optimal for this example.\nSentiment Classification We also examined the performance of dropout as a function of training set size on a document classification task. Figure 3a shows results on the Polarity 2.0 task [17], where the goal is to classify positive versus negative movie reviews on IMDB. We divided the dataset into a training set of size 1,200 and a test set of size 800, and trained a bag-of-words logistic regression model with 50,922 features. This example exhibits the same behavior as our simulation. Using a larger \u03b4 results in a classifier that converges faster at first, but then plateaus. We also ran experiments on a larger IMDB dataset [18] with training and test sets of size 25,000 each and approximately 300,000 features. As Figure 3b shows, the results are similar, although the training set is not large enough for the learning curves to cross. When using the full training set, all but three pairwise comparisons in Figure 3 are statistically significant (p < 0.05 for McNemar\u2019s test).\nDropout and Generative Modeling Naive Bayes and empirical risk minimization represent two divergent approaches to the classification problem. ERM is guaranteed to find the best model as n\u2192 \u221e but can have suboptimal generalization error when n is not large relative to d. Conversely, naive Bayes has very low generalization error, but suffers from asymptotic bias. In this paper, we showed that dropout behaves as a link between ERM and naive Bayes, and can sometimes achieve a more favorable bias-variance tradeoff. By training on randomly generated sub-documents rather than on whole documents, dropout implicitly codifies a generative assumption about the data, namely that excerpts from a long document should have the same label as the original document (Proposition 4).\nLogistic regression with dropout appears to have an intriguing connection to the naive Bayes SVM (NBSVM) [19], which is a way of using naive Bayes generative assumptions to strengthen an SVM. In a recent survey of bag-of-words classifiers for document classification, NBSVM and dropout often obtain state-of-the-art accuracies, e.g., [7]. This suggests that a good way to learn linear models for document classification is to use discriminative models that borrow strength from an approximate generative assumption to cut their generalization error. Our analysis presents an interesting contrast to other work that directly combine generative and discriminative modeling by optimizing a hybrid likelihood [20, 21, 22, 23, 24, 25]. Our approach is more guarded in that we only let the generative assumption speak through pseudo-examples.\nConclusion We have presented a theoretical analysis that explains how dropout training can be very helpful under a Poisson topic model assumption. Specifically, by making training examples artificially difficult, dropout improves the exponent in the generalization bound for ERM. We believe that this work is just the first step in understanding the benefits of training with artificially corrupted features, and we hope the tools we have developed can be extended to analyze other training schemes under weaker data-generating assumptions.\n3When the logistic regression fit is degenerate, we use L2-regularized logistic regression with weight 10\u22127."}, {"heading": "A Technical Results", "text": "We now give detailed proofs of the theorems in the paper.\nA.1 Altitude Training Phenomeon\nWe begin with a proof of our main generalization bound result, namely Theorem 1. The proof is built on top of the following Berry-Esseen type result. Lemma 5. Let Z1, ..., Zd be independent Poisson random variables with means \u03bbj \u2208 R+, and let\nS = d\u2211 j=1 wjZj , \u00b5 = E [S] , and \u03c32 = Var [S]\nfor some fixed set of weights {wj}dj=1. Then, writing FS for the distribution function of S and \u03a6 for the standard Gaussian distribution,\nsup x\u2208R \u2223\u2223\u2223\u2223FS(x)\u2212 \u03a6(x\u2212 \u00b5\u03c3 )\u2223\u2223\u2223\u2223 \u2264 CBE \u221a\u221a\u221a\u221a maxj{w2j}\u2211d j=1 \u03bbjw 2 j , (20)\nwhere CBE \u2264 4. Proof. Our first step is to write S as a sum of bounded i.i.d. random variables. Let N = \u2211d j=1 Zj . Conditional on N , the Zj are distributed as a multinomial with parameters \u03c0j = \u03bbj/\u03bb where \u03bb =\u2211d j=1 \u03bbj . Thus,\nL ( S \u2223\u2223N) d=L( N\u2211\nk=1\nWk \u2223\u2223N) ,\nwhere Wk \u2208 {w1, ..., wd} is a single multinomial draw from the available weights with probability parameters P [Wk = wj ] = \u03c0j . This implies that,\nS d = N\u2211 k=1 Wk,\nwhere N itself is a Poisson random variable with mean \u03bb.\nWe also know that a Poisson random variable can be written as a limiting mixture of many rare Bernoulli trials: B(m) \u21d2 N, with B(m) = Binom ( m, \u03bb\nm\n) .\nThe upshot is that\nS(m) \u21d2 S, with S(m) = m\u2211 k=1 WkIk, (21)\nwhere the Wk are as before, and the Ik are independent Bernoulli draws with parameter \u03bb/m. Because S(m) converges to S in distribution, it suffices to show that (20) holds for large enough m. The moments of S(m) are correct in finite samples: E [ S(m) ] = \u00b5 and Var [ S(m) ] = \u03c32 for all m.\nThe key ingredient in establishing (20) is the Berry-Esseen inequality [26], which in our case implies that\nsup x\u2208R \u2223\u2223\u2223\u2223FS(m)(x)\u2212 \u03a6(x\u2212 \u00b5\u03c3 )\u2223\u2223\u2223\u2223 \u2264 \u03c1m2s3m\u221am,\nwhere\ns2m = Var [WkIk] , \u03c1m = E [ |WkIk \u2212 E [WkIk]|3 ] ,\nWe can show that\ns2m = E [ (WkIk) 2 ] \u2212 E [WkIk]2 = \u03bb m E [ W 2k ] \u2212 ( \u03bb m E [Wk] )2 , and\n\u03c1m \u2264 8 ( E [ |WkIk|3 ] + E [|WkIk|]3 ) = 8\n( \u03bb\nm E [ |Wk|3 ] + ( \u03bb m E [|Wk|] )3) .\nTaking m to\u221e, this implies that\nsup x\u2208R \u2223\u2223\u2223\u2223FS(x)\u2212 \u03a6(x\u2212 \u00b5\u03c3 )\u2223\u2223\u2223\u2223 \u2264 4E\n[ |W |3 ] E [W 2]3/2 1\u221a \u03bb .\nThus, to establish (20), it only remains to bound E [ |W |3 ] /E [ W 2 ]3/2 . Notice that Pj def =\n\u03c0jw 2 j/E\n[ W 2 ] defines a probability distribution on {1, . . . , d}, and\nE [ |W |3 ] E [W 2] = EP [|W |] \u2264 max j {|wj |}.\nThus,\nE [ |W |3 ] E [W 2]3/2 \u2264 \u221a\u221a\u221a\u221a maxj{w2j}\u2211d j=1 \u03c0jw 2 j .\nWe are now ready to prove our main result.\nProof of Theorem 1. The classifier h is a linear classifier of the form\nh (x) = I {S > 0} where S def= d\u2211 j=1 wjxj ,\nwhere by assumption xj \u223c Poisson ( \u03bb (\u03c4) j ) . Our model was fit by dropout, so during training we\nonly get to work with x\u0303 instead of x, where\nx\u0303j \u223c Binom (xj , 1\u2212 \u03b4) , and so unconditionally x\u0303j \u223c Poisson ( (1\u2212 \u03b4) \u03bb(\u03c4)j ) .\nWithout loss of generality, suppose that c\u03c4 = 1, so that we can write the error rate \u03b5\u03c4 during dropout as\n\u03b5\u03c4 = P [ S\u0303 < 0 \u2223\u2223 \u03c4] , where S\u0303 = d\u2211 j=1 wj x\u0303j . (22)\nIn order to prove our result, we need to translate the information about S\u0303 into information about S.\nThe key to the proof is to show that the sums S and S\u0303 have nearly Gaussian distributions. Let\n\u00b5 = d\u2211 j=1 \u03bb (\u03c4) j wj and \u03c3 2 = d\u2211 j=1 \u03bb (\u03c4) j w 2 j\nbe the mean and variance of S. After dropout, E [ S\u0303 ] = (1\u2212 \u03b4)\u00b5 and Var [ S\u0303 ] = (1\u2212 \u03b4)\u03c32.\nWriting FS and FS\u0303 for the distributions of S and S\u0303, we see from Lemma 5 that\nsup x\u2208R \u2223\u2223\u2223\u2223FS(x)\u2212 \u03a6(x\u2212 \u00b5\u03c3 )\u2223\u2223\u2223\u2223 \u2264 CBE\u221a\u03a8\u03c4 and\nsup x\u2208R \u2223\u2223\u2223\u2223FS\u0303(x)\u2212 \u03a6(x\u2212 (1\u2212 \u03b4)\u00b5\u221a1\u2212 \u03b4 \u03c3 )\u2223\u2223\u2223\u2223 \u2264 CBE\u221a1\u2212 \u03b4 \u221a\u03a8\u03c4 ,\nwhere \u03a8\u03c4 is as defined in (9). Recall that our objective is to bound \u03b5\u03c4 = FS(0) in terms of \u03b5\u0303\u03c4 = FS\u0303(0). The above result implies that\n\u03b5\u03c4 \u2264 \u03a6 ( \u2212\u00b5 \u03c3 ) + CBE \u221a \u03a8\u03c4 , and\n\u03a6 ( \u2212 \u221a\n1\u2212 \u03b4 \u00b5 \u03c3\n) \u2264 \u03b5\u0303\u03c4 +\nCBE\u221a 1\u2212 \u03b4\n\u221a \u03a8\u03c4 .\nNow, writing t = \u221a 1\u2212 \u03b4 \u00b5/\u03c3, we can use the Gaussian tail inequalities\n\u03c4 \u03c42 + 1 < \u221a 2\u03c0 e \u03c42 2 \u03a6 (\u2212\u03c4) < 1 \u03c4 for all \u03c4 > 0 (23)\nto check that for all t \u2265 1,\n\u03a6 ( \u2212 t\u221a\n1\u2212 \u03b4\n) \u2264 1\u221a\n2\u03c0\n\u221a 1\u2212 \u03b4 t e\u2212 t2 2(1\u2212\u03b4)\n=\n\u221a 1\u2212 \u03b4 t \u03b4 1\u2212\u03b4\n\u221a 2\u03c0 \u2212 \u03b41\u2212\u03b4\n( 1\u221a 2\u03c0 1 t e\u2212 t2 2 ) 1 1\u2212\u03b4\n\u2264 2 1 1\u2212\u03b4\n\u221a 1\u2212 \u03b4 t \u03b4 1\u2212\u03b4\n\u221a 2\u03c0 \u2212 \u03b41\u2212\u03b4\n( 1\u221a 2\u03c0 t t2 + 1 e\u2212 t2 2 ) 1 1\u2212\u03b4\n\u2264 2 1\n1\u2212\u03b4 \u221a\n1\u2212 \u03b4 \u221a\n2\u03c0 \u2212 \u03b41\u2212\u03b4\nt \u03b4 1\u2212\u03b4 \u03a6 (\u2212t) 1 1\u2212\u03b4\nand so noting that in t\u03a6(\u2212t) is monotone decreasing in our range of interest and that t \u2264\u221a \u22122 log \u03a6(\u2212t), we conclude that for all \u03b5\u0303\u03c4 + CBE/ \u221a 1\u2212 \u03b4 \u221a \u03a8\u03c4 \u2264 \u03a6(\u22121),\n\u03b5\u03c4 \u2264 2\n1 1\u2212\u03b4 \u221a\n1\u2212 \u03b4 \u221a\n4\u03c0 \u2212 \u03b41\u2212\u03b4\n(\u221a \u2212 log ( \u03b5\u0303+\nCBE\u221a 1\u2212 \u03b4\n\u221a \u03a8\u03c4\n)) \u03b41\u2212\u03b4\n\u00b7 ( \u03b5\u0303+\nCBE\u221a 1\u2212 \u03b4\n\u221a \u03a8\u03c4\n) 1 1\u2212\u03b4\n+ CBE \u221a \u03a8\u03c4 . (24)\nWe can also write the above expression in more condensed form: P [ I{w\u0302 \u00b7 x(i)} 6= c\u03c4 \u2223\u2223 \u03c4 (i) = \u03c4] (25) = O  \u03b5\u0303\u03c4 + \u221a\u221a\u221a\u221a max{w2j}\u2211d j=1 \u03bb (\u03c4) j w 2 j (1\u2212\u03b4)  1 1\u2212\u03b4 \u00b7max { 1, \u221a \u2212 log (\u03b5\u0303\u03c4 ) \u03b4 1\u2212\u03b4\n} . The desired conclusion (9) is equivalent to the above expression, except it uses notation that hides the log factors.\nProof of Theorem 2. We can write the dropout error rate as\nErr\u03b4 ( h\u0302\u03b4 ) = Errmin +\u2206,\nwhere Errmin is the minimal possible error from assumption (14) and \u2206 is the the excess error\n\u2206 = T\u2211 \u03c4=1 P [\u03c4 ] \u03b5\u0303\u03c4 \u00b7 \u2223\u2223\u2223P [y(i) = 1 \u2223\u2223 \u03c4 (i) = \u03c4]\u2212 P [y(i) = 0 \u2223\u2223 \u03c4 (i) = \u03c4]\u2223\u2223\u2223 .\nHere, P [\u03c4 ] is the probability of observing a document with topic \u03c4 and \u03b5\u0303\u03c4 is as in Theorem 1. The equality follows by noting that, for each topic \u03c4 , the excess error rate is given by the rate at which we make sub-optimal guesses, i.e., \u03b5\u0303\u03c4 , times the excess probability that we make a classification error given that we made a sub-optimal guess, i.e.,\n\u2223\u2223P [y(i) = 1 \u2223\u2223 \u03c4 (i) = \u03c4]\u2212 P [y(i) = 0 \u2223\u2223 \u03c4 (i) = \u03c4]\u2223\u2223. Now, thanks to (14), we know that\nErr\u03b4 (h \u2217 \u03b4) = Errmin +O ( 1\u221a \u03bb ) ,\nand so the generalization error \u03b7\u0303 under the dropout measure satisfies \u2206 = \u03b7\u0303 +O (\n1\u221a \u03bb\n) .\nUsing (12), we see that \u03b5\u0303\u03c4 \u2264 \u2206 / (2\u03b1pmin)\nfor each \u03c4 , and so\n\u03b5\u0303\u03c4 = O ( \u03b7\u0303 +\n1\u221a \u03bb ) uniformly in \u03c4 . Thus, given the bound (11), we conclude using (25) that\n\u03b5\u03c4 = O (( \u03b7\u0303 + \u03bb\u2212 1\u2212\u03b4 2 ) 1 1\u2212\u03b4 max { 1, \u221a \u2212 log (\u03b7\u0303) \u03b4 1\u2212\u03b4 }) for each topic \u03c4 , and so\n\u03b7 = Err ( h\u0302\u03b4 ) \u2212 Err (h\u2217\u03b4) (26)\n= O (( \u03b7\u0303 + \u03bb\u2212 1\u2212\u03b4 2 ) 1 1\u2212\u03b4 max { 1, \u221a \u2212 log (\u03b7\u0303) \u03b4 1\u2212\u03b4 }) ,\nwhich directly implies (16). Note \u03b7 will in general be larger than the \u03b5\u03c4 , because guessing the optimal label c\u03c4 is not guaranteed to lead to a correct classification decision (unless each topic is pure, i.e., only represents one class). Here, substracting the optimal error Err (h\u2217\u03b4) allows us to compensate for this effect.\nProof of Corollary 3. Here, we prove the more precise bound\nErr ( h\u0302\u03b4 ) \u2212 Err (h\u2217\u03b4) = OP \u221a( d n + 1 \u03bb(1\u2212\u03b4) ) max { 1, log (n d )}1+\u03b4 11\u2212\u03b4 . (27) To do this, we only need to show that\nErr\u03b4 ( h\u0302\u03b4 ) \u2212 Err\u03b4 (h\u2217\u03b4) = OP\n(\u221a d\nn max\n{ 1, log (n d )}) , (28)\ni.e., that dropout generalizes at the usual rate with respect to the dropout measure. Then, by applying (26) from the proof of Theorem 2, we immediately conclude that h\u0302\u03b4 converges at the rate given in (17) under the data-generating measure.\nLet E\u0302rr\u03b4(h) be the average training loss for a classifier h. The empirical loss is unbiased, i.e., E [ E\u0302rr\u03b4(h) ] = Err\u03b4(h).\nGiven this unbiasedness condition, standard methods for establishing rates as in (28) [27] only require that the loss due to any single training example (x(i), y(i)) is bounded, and that the training examples are independent; these conditions are needed for an application of Hoeffding\u2019s inequality. Both of these conditions hold here.\nA.2 Distinct Topics Assumption\nProposition 6. Let the generative model from Section 2 hold, and define \u03c0(\u03c4) = \u03bb(\u03c4)/ \u2225\u2225\u2225\u03bb(\u03c4)\u2225\u2225\u2225\n1\nfor the topic-wise word probability vectors and\n\u03a0 = (\u03c0(1), . . . , \u03c0(T )) \u2208 Rd\u00d7T\nfor the induced matrix. Suppose that \u03a0 has rank T , and that the minimum singular value of \u03a0 (in absolute value) is bounded below by\n|\u03c3min (\u03a0)| \u2265\n\u221a T\n(1\u2212 \u03b4)\u03bb\n( 1 + \u221a log+ \u03bb\n2\u03c0\n) , (29)\nwhere log+ is the positive part of log. Then (14) holds.\nProof. Our proof has two parts. We begin by showing that, given (29), there is a vector w with \u2016w\u20162 \u2264 1 such that\nI { w \u00b7 \u03c0(\u03c4) > 0 } = c\u03c4 , and \u2223\u2223\u2223w \u00b7 \u03c0(\u03c4)\u2223\u2223\u2223 \u2265 \u2212 1\u221a (1\u2212 \u03b4)\u03bb \u03a6\u22121 ( 1\u221a \u03bb ) (30)\nfor all topics \u03c4 ; in other words, the topic centers can be separated with a large margin. After that, we show that (30) implies (14).\nWe can re-write the condition (30) as\nmin { \u2016w\u20162 : c\u03c4w \u00b7 \u03c0 (\u03c4) \u2265 1 for all \u03c4 } \u2264 ( \u2212 1\u221a\n(1\u2212 \u03b4)\u03bb \u03a6\u22121 ( 1\u221a \u03bb ))\u22121 ,\nor equivalently that\nmin { \u2016w\u20162 : S\u03a0 >w \u2265 1 } \u2264 ( \u2212 1\u221a\n(1\u2212 \u03b4)\u03bb \u03a6\u22121 ( 1\u221a \u03bb ))\u22121 where S = diag(c\u03c4 ) is a diagonal matrix of class signs. Now, assuming that rank(\u03a0) \u2265 T , we can verify that\nmin { \u2016w\u20162 : S\u03a0 >w \u2265 1 } = min {\u221a z> (\u03a0>S2\u03a0) \u22121 z : z \u2265 1 } \u2264 \u221a 1> (\u03a0>\u03a0) \u22121 1\n\u2264 |\u03c3min (\u03a0)|\u22121 \u221a T\n\u2264 ( 1\u221a\n(1\u2212 \u03b4)\u03bb\n( 1 + \u221a log+ \u03bb\n2\u03c0\n))\u22121 ,\nwhere the last line followed by hypothesis. Now, by (23)\n\u03a6 ( \u2212 ( 1 + \u221a log+ \u03bb\n2\u03c0\n)) \u2264 1\u221a\n2\u03c0 exp\n( \u22121\n2 log\n\u03bb\n2\u03c0\n) =\n1\u221a \u03bb .\nBecause \u03a6\u22121 is monotone increasing, this implies that( 1 + \u221a log+ \u03bb\n2\u03c0\n)\u22121 \u2264 ( \u2212\u03a6\u22121 ( 1\u221a \u03bb ))\u22121 ,\nand so (30) holds.\nNow, taking (30) as given, it suffices to check that the sub-optimal prediction rate is O ( 1/ \u221a \u03bb )\nuniformly for each \u03c4 . Focusing now on a single topic \u03c4 , suppose without loss of generality that c\u03c4 = 1. We thus need to show that P [w \u00b7 x\u0303 \u2264 0] = O (\n1\u221a \u03bb\n) ,\nwhere x\u0303 is a feature vector thinned by dropout. By Lemma 5 together with (11), we know that\nP [w \u00b7 x\u0303 \u2264 0] \u2264 \u03a6 ( \u2212 E [w \u00b7 x\u0303]\u221a\nVar [w \u00b7 x\u0303]\n) +O ( 1\u221a \u03bb ) .\nBy hypothesis,\nE [w \u00b7 x\u0303] \u2265 \u2212 \u221a (1\u2212 \u03b4)\u03bb(\u03c4)\u03a6\u22121 (\n1\u221a \u03bb\n) ,\nand we can check that Var [w \u00b7 x\u0303] = (1\u2212 \u03b4) d\u2211 j=1 w2j\u03bb (\u03c4) j \u2264 (1\u2212 \u03b4)\u03bb (\u03c4)\nbecause \u2016w\u20162 \u2264 1. Thus,\n\u03a6 ( \u2212 E [w \u00b7 x\u0303]\u221a\nVar [w \u00b7 x\u0303]\n) \u2264 \u03a6 ( \u03a6\u22121 ( 1\u221a \u03bb )) = 1\u221a \u03bb ,\nand (14) holds.\nA.3 Dropout Preserves the Bayes Decision Boundary\nProof of Proposition 4. Another way to view our topic model is as follows. For each topic \u03c4 , define a distribution over words \u03c0(\u03c4) \u2208 \u2206d\u22121: \u03c0(\u03c4) def= \u03bb(\u03c4)/\u2016\u03bb(\u03c4)\u20161. The generative model is equivalent to first drawing the length of the document and then drawing the words from a multinomial:\nLi \u223c Poisson ( \u2016\u03bb(\u03c4)\u20161 ) , and x(i) \u2223\u2223 \u03c4 (i), Li \u223c Multinom(\u03c0(\u03c4(i)), Li) . (31) Now, write the multinomial probability mass function (31) as\nPm [x; \u03c0, L] = L!\nx1! \u00b7 \u00b7 \u00b7xp! \u03c0x11 \u00b7 \u00b7 \u00b7\u03c0 xd d\nFor each label c, define \u03a0c to be the distribution over the probability vectors induced by the distribution over topics. Note that we could have an infinite number of topics. By Bayes rule,\nP [ x = v \u2223\u2223 y = c] = P L = d\u2211\nj=1\nvj  \u00b7 \u222b Pm v; \u03c0, d\u2211\nj=1\nvj  d\u03a0c(\u03c0), and P [ y = c \u2223\u2223x = v] = P [c] \u222b Pm [ v; \u03c0, \u2211d j=1 vj ] d\u03a0c(\u03c0)\u2211\nc\u2032 P [c\u2032] \u222b Pm [ v; \u03c0, \u2211d j=1 vj ] d\u03a0c\u2032(\u03c0) .\nThe key part is that the distribution of L doesn\u2019t depend on c, so that when we condition on x = v, it cancels. As for the joint distribution of (x\u0303, y), note that, given \u03c0 and L\u0303 = \u2211d j=1 x\u0303j , x\u0303 is\nconditionally Multinom(\u03c0, L\u0303). So then\nP [ x\u0303 = v \u2223\u2223 y = c] = P L\u0303 = d\u2211\nj=1\nvj  \u00b7 \u222b Pm v; \u03c0, d\u2211\nj=1\nvj  d\u03a0c(\u03c0), and P [ y = c \u2223\u2223 x\u0303 = v] = P [c] \u222b Pm [ v; \u03c0, \u2211d j=1 vj ] d\u03a0c(\u03c0)\u2211\nc\u2032 P [c\u2032] \u222b Pm [ v; \u03c0, \u2211d j=1 vj ] d\u03a0c\u2032(\u03c0) .\nIn both cases, L and L\u0303 don\u2019t depend on the topic, and when we condition on x and x\u0303, we get the same distribution over y."}], "references": [{"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Adaptive dropout for training deep neural networks", "author": ["Jimmy Ba", "Brendan Frey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Dropout training as adaptive regularization", "author": ["Stefan Wager", "Sida Wang", "Percy Liang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Fast dropout training", "author": ["Sida I Wang", "Christopher D Manning"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Feature noising for log-linear structured prediction", "author": ["Sida I Wang", "Mengqiu Wang", "Stefan Wager", "Percy Liang", "Christopher D Manning"], "venue": "In Empirical Methods in Natural Language Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Learning with marginalized corrupted features", "author": ["Laurens van der Maaten", "Minmin Chen", "Stephen Tyree", "Kilian Q Weinberger"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes", "author": ["Andrew Ng", "Michael Jordan"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "A PAC-Bayesian tutorial with a dropout", "author": ["David McAllester"], "venue": "bound. arXiv:1307.2118,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "The dropout learning algorithm", "author": ["Pierre Baldi", "Peter Sadowski"], "venue": "Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Nightmare at test time: robust learning by feature deletion", "author": ["Amir Globerson", "Sam Roweis"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Statistical behavior and consistency of classification methods based on convex risk minimization", "author": ["Tong Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Latent Dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the Association for Computational Linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Learning word vectors for sentiment analysis", "author": ["Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the Association for Computational Linguistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D Manning"], "venue": "In Proceedings of the Association for Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Classification with hybrid generative/discriminative models", "author": ["R. Raina", "Y. Shen", "A. Ng", "A. McCallum"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "The trade-off between generative and discriminative classifiers", "author": ["G. Bouchard", "B. Triggs"], "venue": "In International Conference on Computational Statistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Principled hybrids of generative and discriminative models", "author": ["J.A. Lasserre", "C.M. Bishop", "T.P. Minka"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Bias-variance tradeoff in hybrid generative-discriminative models", "author": ["Guillaume Bouchard"], "venue": "In International Conference on Machine Learning and Applications. IEEE,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Multi-conditional learning: Generative/discriminative training for clustering and classification", "author": ["A. McCallum", "C. Pal", "G. Druck", "X. Wang"], "venue": "In Association for the Advancement of Artificial Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["Percy Liang", "Michael I Jordan"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "An introduction to probability theory and its applications, volume 2", "author": ["Willliam Feller"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1971}], "referenceMentions": [{"referenceID": 0, "context": "Dropout training [1] is an increasingly popular method for regularizing learning algorithms.", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "Dropout is most commonly used for regularizing deep neural networks [2, 3, 4, 5], but it has also been found to improve the performance of logistic regression and other single-layer models for natural language tasks such as document classification and named entity recognition [6, 7, 8].", "startOffset": 68, "endOffset": 80}, {"referenceID": 2, "context": "Dropout is most commonly used for regularizing deep neural networks [2, 3, 4, 5], but it has also been found to improve the performance of logistic regression and other single-layer models for natural language tasks such as document classification and named entity recognition [6, 7, 8].", "startOffset": 68, "endOffset": 80}, {"referenceID": 3, "context": "Dropout is most commonly used for regularizing deep neural networks [2, 3, 4, 5], but it has also been found to improve the performance of logistic regression and other single-layer models for natural language tasks such as document classification and named entity recognition [6, 7, 8].", "startOffset": 68, "endOffset": 80}, {"referenceID": 4, "context": "Dropout is most commonly used for regularizing deep neural networks [2, 3, 4, 5], but it has also been found to improve the performance of logistic regression and other single-layer models for natural language tasks such as document classification and named entity recognition [6, 7, 8].", "startOffset": 277, "endOffset": 286}, {"referenceID": 5, "context": "Dropout is most commonly used for regularizing deep neural networks [2, 3, 4, 5], but it has also been found to improve the performance of logistic regression and other single-layer models for natural language tasks such as document classification and named entity recognition [6, 7, 8].", "startOffset": 277, "endOffset": 286}, {"referenceID": 6, "context": "Dropout is most commonly used for regularizing deep neural networks [2, 3, 4, 5], but it has also been found to improve the performance of logistic regression and other single-layer models for natural language tasks such as document classification and named entity recognition [6, 7, 8].", "startOffset": 277, "endOffset": 286}, {"referenceID": 7, "context": "For single-layer linear models, learning with dropout is equivalent to using \u201cblankout noise\u201d [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "It is instructive to compare our generalization bound to that of Ng and Jordan [10], who showed that the naive Bayes classifier exploits a strong generative assumption\u2014conditional independence of the features given the label\u2014to achieve an excess risk of OP ( \u221a (log d)/n).", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "McAllester [11] used the PAC-Bayes framework to prove a generalization bound for dropout that decays as 1 \u2212 \u03b4.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "Moreover, provided that \u03b4 is not too close to 1, dropout behaves similarly to an adaptive L2 regularizer with parameter \u03b4/(1\u2212\u03b4) [6, 12], and at least in linear regression such L2 regularization improves generalization error by a constant factor.", "startOffset": 128, "endOffset": 135}, {"referenceID": 10, "context": "Moreover, provided that \u03b4 is not too close to 1, dropout behaves similarly to an adaptive L2 regularizer with parameter \u03b4/(1\u2212\u03b4) [6, 12], and at least in linear regression such L2 regularization improves generalization error by a constant factor.", "startOffset": 128, "endOffset": 135}, {"referenceID": 4, "context": "It is also possible to analyze dropout as an adaptive regularizer [6, 9, 13]: in comparison with L2 regularization, dropout favors the use of rare features and encourages confident predictions.", "startOffset": 66, "endOffset": 76}, {"referenceID": 7, "context": "It is also possible to analyze dropout as an adaptive regularizer [6, 9, 13]: in comparison with L2 regularization, dropout favors the use of rare features and encourages confident predictions.", "startOffset": 66, "endOffset": 76}, {"referenceID": 11, "context": "It is also possible to analyze dropout as an adaptive regularizer [6, 9, 13]: in comparison with L2 regularization, dropout favors the use of rare features and encourages confident predictions.", "startOffset": 66, "endOffset": 76}, {"referenceID": 8, "context": ", [10, 14, 15]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 12, "context": ", [10, 14, 15]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 13, "context": ", [10, 14, 15]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 0, "context": "Because we are only interested in the decision boundary, we do not scale down the weight vector obtained by dropout by a factor 1\u2212 \u03b4 as is often done [1].", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "Binomial dropout differs slightly from the usual definition of (blankout) dropout, which alters the feature vector x by setting random coordinates to 0 [6, 9, 11, 12].", "startOffset": 152, "endOffset": 166}, {"referenceID": 7, "context": "Binomial dropout differs slightly from the usual definition of (blankout) dropout, which alters the feature vector x by setting random coordinates to 0 [6, 9, 11, 12].", "startOffset": 152, "endOffset": 166}, {"referenceID": 9, "context": "Binomial dropout differs slightly from the usual definition of (blankout) dropout, which alters the feature vector x by setting random coordinates to 0 [6, 9, 11, 12].", "startOffset": 152, "endOffset": 166}, {"referenceID": 10, "context": "Binomial dropout differs slightly from the usual definition of (blankout) dropout, which alters the feature vector x by setting random coordinates to 0 [6, 9, 11, 12].", "startOffset": 152, "endOffset": 166}, {"referenceID": 14, "context": "If \u0398 is the (K \u2212 1)-dimensional simplex where \u03bb is a \u03c4 -mixture over K basis vectors, we get the K-topic latent Dirichlet allocation [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "(4) Dropout training is known to work well in practice for multi-class problems [8].", "startOffset": 80, "endOffset": 83}, {"referenceID": 12, "context": ", [14, 15].", "startOffset": 2, "endOffset": 10}, {"referenceID": 13, "context": ", [14, 15].", "startOffset": 2, "endOffset": 10}, {"referenceID": 15, "context": "0 dataset [17].", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "(b) IMDB dataset [18].", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "0 task [17], where the goal is to classify positive versus negative movie reviews on IMDB.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "We also ran experiments on a larger IMDB dataset [18] with training and test sets of size 25,000 each and approximately 300,000 features.", "startOffset": 49, "endOffset": 53}, {"referenceID": 17, "context": "Logistic regression with dropout appears to have an intriguing connection to the naive Bayes SVM (NBSVM) [19], which is a way of using naive Bayes generative assumptions to strengthen an SVM.", "startOffset": 105, "endOffset": 109}, {"referenceID": 5, "context": ", [7].", "startOffset": 2, "endOffset": 5}, {"referenceID": 18, "context": "Our analysis presents an interesting contrast to other work that directly combine generative and discriminative modeling by optimizing a hybrid likelihood [20, 21, 22, 23, 24, 25].", "startOffset": 155, "endOffset": 179}, {"referenceID": 19, "context": "Our analysis presents an interesting contrast to other work that directly combine generative and discriminative modeling by optimizing a hybrid likelihood [20, 21, 22, 23, 24, 25].", "startOffset": 155, "endOffset": 179}, {"referenceID": 20, "context": "Our analysis presents an interesting contrast to other work that directly combine generative and discriminative modeling by optimizing a hybrid likelihood [20, 21, 22, 23, 24, 25].", "startOffset": 155, "endOffset": 179}, {"referenceID": 21, "context": "Our analysis presents an interesting contrast to other work that directly combine generative and discriminative modeling by optimizing a hybrid likelihood [20, 21, 22, 23, 24, 25].", "startOffset": 155, "endOffset": 179}, {"referenceID": 22, "context": "Our analysis presents an interesting contrast to other work that directly combine generative and discriminative modeling by optimizing a hybrid likelihood [20, 21, 22, 23, 24, 25].", "startOffset": 155, "endOffset": 179}, {"referenceID": 23, "context": "Our analysis presents an interesting contrast to other work that directly combine generative and discriminative modeling by optimizing a hybrid likelihood [20, 21, 22, 23, 24, 25].", "startOffset": 155, "endOffset": 179}], "year": 2014, "abstractText": "Dropout training, originally designed for deep neural networks, has been successful on high-dimensional single-layer natural language tasks. This paper proposes a theoretical explanation for this phenomenon: we show that, under a generative Poisson topic model with long documents, dropout training improves the exponent in the generalization bound for empirical risk minimization. Dropout achieves this gain much like a marathon runner who practices at altitude: once a classifier learns to perform reasonably well on training examples that have been artificially corrupted by dropout, it will do very well on the uncorrupted test set. We also show that, under similar conditions, dropout preserves the Bayes decision boundary and should therefore induce minimal bias in high dimensions.", "creator": "LaTeX with hyperref package"}}}