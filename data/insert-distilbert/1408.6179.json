{"id": "1408.6179", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2014", "title": "Evaluating Neural Word Representations in Tensor-Based Compositional Settings", "abstract": "we provide a comparative study between neural word representations and traditional vector spaces based on minimal co - occurrence cluster counts, in a number of compositional tasks. usually we use three different semantic spaces and finally implement each seven natural tensor - based compositional models, which we then test ( together with simpler additive and multiplicative approaches ) in computing tasks involving verb disambiguation and sentence similarity. to check confirm their scalability, though we additionally evaluate the spaces using simple compositional methods relies on larger - case scale tasks with less constrained language : paraphrase detection methodology and dialogue act upon tagging. in the more constrained tasks, co - occurrence vectors are competitive, although greater choice of an compositional method is important ; on developing the larger - scale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks.", "histories": [["v1", "Tue, 26 Aug 2014 16:28:21 GMT  (30kb)", "http://arxiv.org/abs/1408.6179v1", "To be published in EMNLP 2014"]], "COMMENTS": "To be published in EMNLP 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dmitrijs milajevs", "dimitri kartsaklis", "mehrnoosh sadrzadeh", "matthew purver"], "accepted": true, "id": "1408.6179"}, "pdf": {"name": "1408.6179.pdf", "metadata": {"source": "CRF", "title": "Evaluating Neural Word Representations in Tensor-Based Compositional Settings", "authors": ["Dmitrijs Milajevs", "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver"], "emails": ["d.milajevs@qmul.ac.uk", "m.sadrzadeh@qmul.ac.uk", "m.purver@qmul.ac.uk", "dimitri.kartsaklis@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 8.\n61 79\nv1 [\ncs .C\nL ]\n2 6\nA ug\n2 01"}, {"heading": "1 Introduction", "text": "Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks. While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive and multiplicative composition, compared against composition via a neural autoencoder.\nThe purpose of this paper is to provide a more complete picture regarding the potential of neu-\nral word embeddings in compositional tasks, and meaningfully compare them with the traditional distributional approach based on co-occurrence counts. We are especially interested in investigating the performance of neural word vectors in compositional models involving general mathematical composition operators, rather than in the more task- or domain-specific deep-learning compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others).\nIn particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.g. (Stolcke et al., 2000)).\nIn all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in detail below. The general picture we get from the results is that in almost all cases the neural vectors are more effective than the traditional approaches.\nWe proceed as follows: Section 2 provides a concise introduction to distributional word representations in natural language processing. Section\n3 takes a closer look to the subject of compositionality in vector space models of meaning and describes the range of compositional operators examined here. In Section 4 we provide details about the vector spaces used in the experiments. Our experimental work is described in detail in Section 5, and the results are discussed in Section 6. Finally, Section 7 provides conclusions."}, {"heading": "2 Meaning representation", "text": "There are several approaches to the representation of word, phrase and sentence meaning. As natural languages are highly creative and it is very rare to see the same sentence twice, any practical approach dealing with large text segments must be compositional, constructing the meaning of phrases and sentences from their constituent parts. The ideal method would therefore express not only the similarity in meaning between those constituent parts, but also between the results of their composition, and do this in ways which fit with linguistic structure and generalisations thereof.\nFormal semantics Formal approaches to the semantics of natural language have long built upon the classical idea of compositionality \u2013 that the meaning of a sentence is a function of the meanings of its parts (Frege, 1892). In compositional type-logical approaches, predicateargument structures representing phrases and sentences are built from their constituent parts by \u03b2reduction within the lambda calculus framework (Montague, 1970): for example, given a representation of John as john \u2032 and sleeps as \u03bbx.sleep \u2032(x), the meaning of the sentence \u201cJohn sleeps\u201d can be constructed as \u03bbx.sleep \u2032(x)(john \u2032) = sleep \u2032(john \u2032). Given a suitable pairing between words and semantic representations of them, this method can produce structured sentential representations with broad coverage and good generalisability (see e.g. (Bos, 2008)). The above logical approach is extremely powerful because it can capture complex aspects of meaning such as quantifiers and their interaction (see e.g. (Copestake et al., 2005)), and enables inference using well studied and developed logical methods (see e.g. (Bos and Gabsdil, 2000)).\nDistributional hypothesis However, such formal approaches are less able to express similarity in meaning. We would like to capture the intuition that while John and Mary are distinct,\nthey are rather similar to each other (both of them are humans) and dissimilar to words such as dog, pavement or idea. The same applies at the phrase and sentence level: \u201cdogs chase cats\u201d is similar in meaning to \u201chounds pursue kittens\u201d, but less so to \u201ccats chase dogs\u201d (despite the lexical overlap).\nDistributional methods provide a way to address this problem. By representing words and phrases as vectors or tensors in a (usually highly dimensional) vector space, one can express similarity in meaning via a suitable distance metric within that space (usually cosine distance); furthermore, composition can be modelled via suitable linearalgebraic operations.\nCo-occurrence-based word representations One way to produce such vectorial representations is to directly exploit Harris (1954)\u2019s intuition that semantically similar words tend to appear in similar contexts. We can construct a vector space in which the dimensions correspond to contexts, usually taken to be words as well. The word vector components can then be calculated from the frequency with which a word has co-occurred with the corresponding contexts in a window of words, with a predefined length.\nTable 1 shows 5 3-dimensional vectors for the words Mary, John, girl, boy and idea. The words philosophy, book and school signify vector space dimensions. As the vector for John is closer to Mary than it is to idea in the vector space\u2014a direct consequence of the fact that John\u2019s contexts are similar to Mary\u2019s and dissimilar to idea\u2019s\u2014we can infer that John is semantically more similar to Mary than to idea.\nMany variants of this approach exist: performance on word similarity tasks has been shown to be improved by replacing raw counts with weighted values (e.g. mutual information)\u2014see (Turney et al., 2010) and below for discussion, and (Kiela and Clark, 2014) for a detailed comparison.\nNeural word embeddings Deep learning techniques exploit the distributional hypothesis differently. Instead of relying on observed cooccurrence frequencies, a neural language model is trained to maximise some objective function related to e.g. the probability of observing the surrounding words in some context (Mikolov et al., 2013b):\n1\nT\nT\u2211\nt=1\n\u2211\n\u2212c\u2264j\u2264c,j 6=0\nlog p(wt+j |wt) (1)\nOptimizing the above function, for example, produces vectors which maximise the conditional probability of observing words in a context around the target word wt, where c is the size of the training window, and w1w2, \u00b7 \u00b7 \u00b7wT a sequence of words forming a training instance. Therefore, the resulting vectors will capture the distributional intuition and can express degrees of lexical similarity.\nThis method has an obvious advantage compared to co-occurrence method: since now the context is predicted, the model in principle can be much more robust in data sparsity problems, which is always an important issue for cooccurrence word spaces. Additionally, neural vectors have also proven successful in other tasks (Mikolov et al., 2013c), since they seem to encode not only attributional similarity (the degree to which similar words are close to each other), but also relational similarity (Turney, 2006). For example, it is possible to extract the singular:plural relation (apple:apples, car:cars) using vector subtraction:\n\u2212\u2212\u2212\u2192 apple \u2212 \u2212\u2212\u2212\u2212\u2192 apples \u2248 \u2212\u2192car \u2212\u2212\u2212\u2192cars\nPerhaps even more importantly, semantic relationships are preserved in a very intuitive way:\n\u2212\u2212\u2192 king \u2212\u2212\u2212\u2192man \u2248 \u2212\u2212\u2212\u2192queen \u2212\u2212\u2212\u2212\u2212\u2192woman\nallowing the formation of analogy queries similar to \u2212\u2212\u2192 king\u2212\u2212\u2212\u2192man+\u2212\u2212\u2212\u2212\u2192woman = ?, obtaining \u2212\u2212\u2212\u2192queen as the result.1\nBoth neural and co-occurrence-based approaches have advantages over classical formal approaches in their ability to capture lexical semantics and degrees of similarity; their success at\n1Levy et al. (2014) improved Mikolov et al. (2013c)\u2019s method of retrieving relational similarities by changing the underlying objective function.\nextending this to the sentence level and to more complex semantic phenomena, though, depends on their applicability within compositional models, which is the subject of the next section."}, {"heading": "3 Compositional models", "text": "Compositional distributional models represent meaning of a sequence of words by a vector, obtained by combining meaning vectors of the words within the sequence using some vector composition operation. In a general classification of these models, one can distinguish between three broad cases: simplistic models which combine word vectors irrespective of their order or relation to one another, models which exploit linear word order, and models which use grammatical structure.\nThe first approach combines word vectors by vector addition or point-wise multiplication (Mitchell and Lapata, 2008)\u2014as this is independent of word order, it cannot capture the difference between the two sentences \u201cdogs chase cats\u201d and \u201ccats chase dogs\u201d. The second approach has generally been implemented using some form of deep learning, and captures word order, but not by necessarily caring about the grammatical structure of the sentence. Here, one works by recursively building and combining vectors for subsequences of words within the sentence using e.g. autoencoders (Socher et al., 2012) or convolutional filters (Kalchbrenner et al., 2014). We do not consider this approach in this paper. This is because, as mentioned in the introduction, their vectors and composition operators are task-specific. These are trained directly to achieve specific objectives in certain pre-determined tasks. We are interested in vector and composition operators that work for any compositional task, and which can be combined with results in linguistics and formal semantics to provide generalisable models that can canonically extend to complex semantic phenomena. The third (i.e. the grammatical) approach promises a way to achieve this, and has been instantiated in various ways in the work of Baroni and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al. (2012).\nGeneral framework Formally, we can specify the vector representation of a word sequence w1w2 \u00b7 \u00b7 \u00b7wn as the vector \u2212\u2192s = \u2212\u2192w1 \u22c6 \u2212\u2192w2 \u22c6 \u00b7 \u00b7 \u00b7 \u22c6\n\u2212\u2192wn, where \u22c6 is a vector operator, such as addition +, point-wise multiplication \u2299, tensor product \u2297, or matrix multiplication \u00d7.\nIn the simplest compositional models (the first approach described above), \u22c6 is + or \u2299, e.g. see (Mitchell and Lapata, 2008). Grammar-based compositional models (the third approach) are based on a generalisation of the notion of vectors, known as tensors. Whereas a vector \u2212\u2192v is an element of an atomic vector space V , a tensor z is an element of a tensor space V \u2297W \u2297 \u00b7 \u00b7 \u00b7 \u2297 Z . The number of tensored spaces is referred to by the order of the space. Using a general duality theorem from multi-linear algebra (Bourbaki, 1989), it follows that tensors are in one-one correspondence with multi-linear maps, that is we have:\nz \u2208 V \u2297W\u2297\u00b7 \u00b7 \u00b7\u2297Z \u223c= fz : V \u2192 W \u2192 \u00b7 \u00b7 \u00b7 \u2192 Z\nIn such a tensor-based formalism, meanings of nouns are vectors and meanings of predicates such as adjectives and verbs are tensors. Meaning of a string of words is obtained by applying the compositions of multi-linear map duals of the tensors to the vectors. For the sake of demonstration, take the case of an intransitive sentence \u201cSbj Verb\u201d; the meaning of the subject is a vector \u2212\u2192 Sbj \u2208 V and the meaning of the intransitive verb is a tensor Verb \u2208 V \u2297 W . Meaning of the sentence is obtained by applying f\nV erb to\n\u2212\u2192 Sbj, as follows:\n\u2212\u2212\u2212\u2212\u2212\u2192 Sbj Verb = f V erb ( \u2212\u2192 Sbj)\nBy tensor-map duality, the above becomes equivalent to the following, where composition has now become the familiar notion of matrix multiplication, that is \u22c6 is \u00d7:\nVerb \u00d7 \u2212\u2192 Sbj\nIn general and for words with tensors of order higher than two, \u22c6 becomes a generalisation of \u00d7, referred to by tensor contraction, see e.g. Kartsaklis and Sadrzadeh (2013). Since the creation and manipulation of tensors of order higher than 2 is difficult, one can work with simplified versions of tensors, faithful to their underlying mathematical basis; these have found intuitive interpretations, e.g. see Grefenstette and Sadrzadeh (2011a), Kartsaklis and Sadrzadeh (2014). In such cases, \u22c6 becomes a combination of a range of operations such as \u00d7, \u2297, \u2299, and +.\nSpecific models In the current paper we will experiment with a variety of models. In Table 2, we present these models in terms of their composition operators and a reference to the main paper in\nwhich each model was introduced. For the simple compositional models the sentence is a string of any number of words; for the grammar-based models, we consider simple transitive sentences \u201cSbj Verb Obj\u201d and introduce the following abbreviations for the concrete method used to build a tensor for the verb:\n1. Verb is a verb matrix computed using the formula \u2211 i \u2212\u2212\u2192 Sbji\u2297 \u2212\u2212\u2192 Obji, where \u2212\u2212\u2192 Sbji and \u2212\u2212\u2192 Obji are\nthe subjects and objects of the verb across the corpus. These models are referred to by relational (Grefenstette and Sadrzadeh, 2011a); they are generalisations of predicate semantics of transitive verbs, from pairs of individuals to pairs of vectors. The models reduce the order 3 tensor of a transitive verb to an order 2 tensor (i.e. a matrix).\n2. V\u0303erb is a verb matrix computed using the formula \u2212\u2212\u2192 Verb \u2297 \u2212\u2212\u2192 Verb, where \u2212\u2212\u2192 Verb is the distri-\nbutional vector of the verb. These models are referred to by Kronecker, which is the term sometimes used to denote the outer product of tensors (Grefenstette and Sadrzadeh, 2011b). This models also reduces the order 3 tensor of a transitive verb to an order 2 tensor.\n3. The models of the last five lines of the table use the so-called Frobenius operators from categorical compositional distributional semantics (Kartsaklis et al., 2012) to expand the relational matrices of verbs from order 2 to order 3. The expansion is obtained by either copying the dimension of the subject into the space provided by the third tensor, hence referred to by Copy-Sbj, or copying the dimension of the object in that space, hence referred to by Copy-Obj; furthermore, we can take addition, multiplication, or outer product of these, which are referred to by FrobeniusAdd, Frobenius-Mult, and Frobenius-Outer (Kartsaklis and Sadrzadeh, 2014)."}, {"heading": "4 Semantic word spaces", "text": "Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies). We instantiate two co-occurrence-based vectors spaces with different underlying corpora and weighting schemes.\nGS11 Our first word space is based on a typical configuration that has been used in the past extensively for compositional distributional models (see below for details), so it will serve as a useful baseline for the current work. In this vector space, the co-occurrence counts are extracted from the British National Corpus (BNC) (Leech et al., 1994). As basis words, we use the most frequent nouns, verbs, adjectives and adverbs (POS tags SUBST, VERB, ADJ and ADV in the BNC XML distribution2). The vector space is lemmatized, that is, it contains only \u201ccanonical\u201d forms of words.\nIn order to weight the raw co-occurrence counts, we use positive point-wise mutual information (PPMI). The component value for a target word t and a context word c is given by:\nPPMI(t, c) = max ( 0, log p(c|t)\np(c)\n)\nwhere p(c|t) is the probability of word c given t in a symmetric window of length 5 and p(c) is the probability of c overall.\nVector spaces based on point-wise mutual information (or variants thereof) have been successfully applied in various distributional and compositional tasks; see e.g. Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al. (2014) for details. PPMI has been shown to achieve state-of-the-art results (Levy et al., 2014) and is suggested by the review of Kiela and Clark (2014). Our use here of the BNC as a corpus and the window length of 5 is based on previous use and better performance of these parameters in a number of compositional experiments (Grefenstette and Sadrzadeh, 2011a; Grefenstette\n2http://www.natcorp.ox.ac.uk/\nand Sadrzadeh, 2011b; Mitchell and Lapata, 2008; Kartsaklis et al., 2012).\nKS14 In this variation, we train a vector space from the ukWaC corpus3 (Ferraresi et al., 2008), originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content). The vector space is again lemmatized. As context we consider a 5-word window from either side of the target word, while as our weighting scheme we use local mutual information (i.e. point-wise mutual information multiplied by raw counts). In a further step, the vector space was normalized and projected onto a 300-dimensional space using singular value decomposition (SVD).\nIn general, dimensionality reduction produces more compact word representations that are robust against potential noise in the corpus (Landauer and Dumais, 1997; Schu\u0308tze, 1997). SVD has been shown to perform well on a variety of tasks similar to ours (Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2014).\nNeural word embeddings (NWE) For our neural setting, we used the skip-gram model of Mikolov et al. (2013b) trained with negative sampling. The specific implementation that was tested in our experiments was a 300-dimensional vector space learned from the Google News corpus and provided by the word2vec4 toolkit. Furthermore, the gensim library (R\u030cehu\u030ar\u030cek and Sojka, 2010) was used for accessing the vectors. On the contrary with the previously described co-\n3http://wacky.sslmit.unibo.it/ 4https://code.google.com/p/word2vec/\noccurrence vector spaces, this version is not lemmatized.\nThe negative sampling method improves the objective function of Equation 1 by introducing negative examples to the training algorithm. Assume that the probability of a specific (c, t) pair of words (where t is a target word and c another word in the same context with t), coming from the training data, is denoted as p(D = 1|c, t). The objective function is then expressed as follows:\n\u220f\n(c,t)\u2208D\np(D = 1|c, t) (2)\nThat is, the goal is to set the model parameters in a way that maximizes the probability of all observations coming from the training data. Assume now that D\u2032 is a set of randomly selected incorrect (c\u2032, t\u2032) pairs that do not occur in D, then Equation 2 above can be recasted in the following way:\n\u220f\n(c,t)\u2208D\np(D = 1|c, t) \u220f\n(c\u2032 ,t\u2032)\u2208D\u2032\np(D = 0|c\u2032, t\u2032)\n(3) In other words, the model tries to distinguish a target word t from random draws that come from a noise distribution. In the implementation we used for our experiments, c is always selected from a 5-word window around t. More details about the negative sampling approach can be found in (Mikolov et al., 2013b); the note of Goldberg and Levy (2014) also provides an intuitive explanation of the underlying setting."}, {"heading": "5 Experiments", "text": "Our experiments explore the use of the vector spaces above, together with the compositional operators described in Section 3, in a range of tasks all of which require semantic composition: verb sense disambiguation; sentence similarity; paraphrasing; and dialogue act tagging."}, {"heading": "5.1 Disambiguation", "text": "We use the transitive verb disambiguation dataset described in Grefenstette and Sadrzadeh (2011a)5. This dataset consists of ambiguous transitive verbs together with their arguments, landmark verbs that identify one of the verb senses, and human judgements that specify how similar is the disambiguated sense of the verb in the given context to\n5This and the sentence similarity dataset are available at http://www.cs.ox.ac.uk/activities/ compdistmeaning/\none of the landmarks. This is similar to the intransitive dataset described in (Mitchell and Lapata, 2008). Consider the sentence \u201csystem meets specification\u201d; here, meets is the ambiguous transitive verb, and system and specification are its arguments in this context. Possible landmarks for meet are satisfy and visit; for this sentence, the human judgements show that the disambiguated meaning of the verb is more similar to the landmark satisfy and less similar to visit.\nThe task is to estimate the similarity of the sense of a verb in a context with a given landmark. To get our similarity measures, we compose the verb with its arguments using one of our compositional models; we do the same for the landmark and then compute the cosine similarity of the two vectors. We evaluate the performance by averaging the human judgements for the same verb, argument and landmark entries, and calculating the Spearman\u2019s correlation between the average values and the cosine scores. As a baseline, we compare this with the correlation produced by using only the verb vector, without composing it with its arguments.\nTable 3 shows the results of the experiment. NWE copy-object composition yields the best correlation with the human judgements, and top performance across all vector spaces and models with a Spearman \u03c1 of 0.456. For the KS14 space, the best result comes from Frobenius outer (0.350),\nwhile the best operator for the GS11 space is point-wise multiplication (0.348).\nFor simple point-wise composition, only multiplicative GS11 and additive NWE improve over their corresponding verb-only baselines (but both perform worse than the KS14 baseline). With tensor-based composition in co-occurrence based spaces, copy subject yields lower results than the corresponding baselines. Other composition methods, except Kronecker for KS14, improve over the verb-only baselines. Finally we should note that, despite the small training corpus, the GS11 vector space performs comparatively well: for instance, Kronecker model improves the previously reported score of 0.28 (Grefenstette and Sadrzadeh, 2011b)."}, {"heading": "5.2 Sentence similarity", "text": "In this experiment we use the transitive sentence similarity dataset described in Kartsaklis and Sadrzadeh (2014). The dataset consists of transitive sentence pairs and a human similarity judgement6. The task is to estimate a similarity measure between two sentences. As in the disambiguation task, we first compose word vectors to obtain sentence vectors, then compute cosine similarity of them. We average the human judgements for identical sentence pairs to compute a correlation with cosine scores.\nTable 4 shows the results. Again, the best performing vector space is KS14, but this time with addition: the Spearman \u03c1 correlation score with averaged human judgements is 0.732. Addition was the means for the other vector spaces to achieve top performance as well: GS11 and NWE got 0.682 and 0.689 respectively.\nNone of the models in tensor-based composition outperformed addition. KS14 performs worse with tensor-based methods here than in the other vector spaces. However, GS11 and NWE, except copy subject for both of them and Frobenius multiplication for NWE, improved over their verb-only baselines."}, {"heading": "5.3 Paraphrasing", "text": "In this experiment we evaluate our vector spaces on a mainstream paraphrase detection task.\n6The textual content of this dataset is the same as that of (Kartsaklis and Sadrzadeh, 2013), the difference is that the dataset of (Kartsaklis and Sadrzadeh, 2014) has updated human judgements whereas the previous dataset used the original annotations of the intransitive dataset of (Mitchell and Lapata, 2010).\nSpecifically, we get classification results on the Microsoft Research Paraphrase Corpus paraphrase corpus (Dolan et al., 2005) working in the following way: we construct vectors for the sentences of each pair; if the cosine similarity between the two sentence vectors exceeds a certain threshold, the pair is classified as a paraphrase, otherwise as not a paraphrase. For this experiment and that of Section 5.4 below, we investigate only the addition and point-wise multiplication compositional models, since at their current stage of development tensor-based models can only efficiently handle sentences of fixed structure. Nevertheless, the simple point-wise compositional models still allow for a direct comparison of the vector spaces, which is the main goal of this paper.\nFor each vector space and model, a number of different thresholds were tested on the first 2000 pairs of the training set, which we used as a development set; in each case, the best-performed threshold was selected for a single run of our \u201cclassifier\u201d on the test set (1726 pairs). Additionally, we evaluate the NWE model with a lemmatized version of the corpus, so that the experimental setup is maximally similar for all vector spaces. The results are shown in the first part of Table 5.\nAdditive NWE gives the highest performance, with both lemmatized and un-lemmatized versions outperforming the GS11 and KS14 spaces. In the un-lemmatized case, the accuracy of our simple \u201cclassifier\u201d (0.73) is close to state-of-the-art range. The state-of-the art result (0.77 accuracy\nand 0.84 F-score7) by the time of this writing has been obtained using 8 machine translation metrics and three constituent classifiers (Madnani et al., 2012).\nThe multiplicative model gives lower results than the additive model across all vector spaces. The KS14 vector space shows the steadiest performance, with a drop in accuracy of only 0.04 and no drop in F-score, while for the GS11 and NWE spaces both accuracy and F-score experienced drops by more than 0.20."}, {"heading": "5.4 Dialogue act tagging", "text": "As our last experiment, we evaluate the word spaces on a dialogue act tagging task (Stolcke et al., 2000) over the Switchboard corpus (Godfrey et al., 1992). Switchboard is a collection of approximately 2500 dialogs over a telephone line by 500 speakers from the U.S. on predefined topics.8\nThe experiment pipeline follows (Milajevs and Purver, 2014). The input utterances are preprocessed so that the parts of interrupted utterances are concatenated (Webb et al., 2005). Disfluency markers and commas are removed from the utterance raw texts. For GS11 and KS14 the utterance tokens are POS-tagged and lemmatized; for NWE, we test the vectors in both a lemmatized and an un-lemmatized version of the corpus.9 We split the training and testing utterances as suggested by Stolcke et al. (2000). Utterance vectors are then obtained as in the previous experiments; they are reduced to 50 dimensions using SVD and a knearest-neighbour classifier is trained on these reduced utterance vectors (the 5 closest neighbours by Euclidean distance are retrieved to make a clas-\n7F-scores use the standard definition F = 2(precision \u2217 recall)/(precision + recall).\n8The dataset and a Python interface to it are available at http://compprag.christopherpotts.net/ swda.html\n9We use WordNetLemmatizer of the NLTK library (Bird, 2006).\nsification decision). The results are shown in the second part of Table 5.\nUn-lemmatized NWE addition gave the best accuracy (0.63) and F-score (0.60) (averaged over tag classes), i.e. similar results to (Milajevs and Purver, 2014)\u2014although note that the dimensionality of our NWE vectors is 10 times lower than theirs. Multiplicative NWE outperformed the corresponding model in (Milajevs and Purver, 2014). In general, addition consistently outperforms multiplication for all the models. Lemmatization dramatically lowers tagging accuracy: the lemmatized GS11, KS14 and NWE models perform much worse than un-lemmatized NWE, suggesting that morphological features are important for this task."}, {"heading": "6 Discussion", "text": "Previous comparisons of co-occurrence-based and neural word vector representations vary widely in their conclusions. While Baroni et al. (2014) conclude that \u201ccontext-predicting models obtain a thorough and resounding victory against their count-based counterparts\u201d, this seems to contradict, at least at the first consideration, the more conservative conclusion of Levy et al. (2014) that \u201canalogy recovery is not restricted to neural word embeddings [. . . ] a similar amount of relational similarities can be recovered from traditional distributional word representations\u201d and the findings of Blacoe and Lapata (2012) that \u201cshallow approaches are as good as more computationally intensive alternatives\u201d on phrase similarity and paraphrase detection tasks.\nIt seems clear that neural word embeddings have an advantage when used in tasks for which they have been trained; our main questions here are whether they outperform co-occurrence based alternatives across the board; and which approach lends itself better to composition using general mathematical operators. To partially an-\nswer this question, we can compare model behaviour against the baselines in isolation.\nFor the disambiguation and sentence similarity tasks the baseline is the similarity between verbs only, ignoring the context\u2014see above. For the paraphrase task, we take the global vector-based similarity reported in (Mihalcea et al., 2006): 0.65 accuracy and 0.75 F-score. For the dialogue act tagging task the baseline is the accuracy of the bag-of-unigrams model in (Milajevs and Purver, 2014): 0.60.\nSections 5.1 and 5.2 show that although the best choice of vector representation might vary, for small-scale tasks all methods give fairly competitive results. The choice of compositional operator seems to be more important and more taskspecific: while a tensor-based operation (Frobenius copy-object) performs best for verb disambiguation, the best result for sentence similarity is achieved by a simple additive model, with all other compositional methods behaving worse than the verb-only baseline in the KS14 case. GS11 and NWE, on the other hand, outperform their baselines with a number of compositional methods, although both of them achieve lower performance than KS14 overall.\nBased on only small-scale experiment results, one could conclude that there is little significant difference between the two ways of obtaining vectors. GS11 and NWE show similar behaviour in comparison to their baselines, while it is possible to tune a co-occurrence based vector space (KS14) and obtain the best result. Large scale tasks reveal another pattern: the GS11 vector space, which behaves stably on the small scale, drags behind the KS14 and NWE spaces in the paraphrase detection task. In addition, NWE consistently yields best results. Finally, only the NWE space was able to provide adequate results on the dialogue act tagging task. Table 6 summarizes model performance with regard to baselines."}, {"heading": "7 Conclusion", "text": "In this work we compared the performance of two co-occurrence-based semantic spaces with vectors learned by a neural network in compositional settings. We carried out two small-scale tasks (word sense disambiguation and sentence similarity) and two large-scale tasks (paraphrase detection and dialogue act tagging).\nOn small-scale tasks, where the sentence structures are predefined and relatively constrained, NWE gives better or similar results to count-based vectors. Tensor-based composition does not always outperform simple compositional operators, but for most of the cases gives results within the same range.\nOn large-scale tasks, neural vectors are more successful than the co-occurrence based alternatives. However, this study does not reveal whether this is because of their neural nature, or just because they are trained on a larger amount of data.\nThe question of whether neural vectors outperform co-occurrence vectors therefore requires further detailed comparison to be entirely resolved; our experiments suggest that this is indeed the case in large-scale tasks, but the difference in size and nature of the original corpora may be a confounding factor. In any case, it is clear that the neural vectors of word2vec package perform steadily off-the-shelf across a large variety of tasks. The size of the vector space (3 million words) and the available code-base that simplifies the access to the vectors, makes this set a good and safe choice for experiments in the future. Of course, even better performances can be achieved by training neural language models specifically for a given task (see e.g. Kalchbrenner et al. (2014)).\nThe choice of compositional operator (tensorbased or a simple point-wise operation) depends strongly on the task and dataset: tensor-based composition performed best with the verb disambiguation task, where the verb senses depend strongly on the arguments of the verb. However, it seems to depend less on the nature of the vectors itself: in the disambiguation task, tensor-based\ncomposition proved best for both co-occurrencebased and neural vectors; in the sentence similarity task, where point-wise operators proved best, this was again true across vector spaces."}, {"heading": "Acknowledgements", "text": "We would like to thank the three anonymous reviewers for their fruitful comments. Support by EPSRC grant EP/F042728/1 is gratefully acknowledged by Milajevs, Kartsaklis and Sadrzadeh. Purver is partly supported by ConCreTe: the project ConCreTe acknowledges the financial support of the Future and Emerging Technologies (FET) programme within the Seventh Framework Programme for Research of the European Commission, under FET grant number 611733."}], "references": [{"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Marco Baroni", "Roberto Zamparelli."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski."], "venue": "context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "NLTK: the natural language toolkit", "author": ["Steven Bird."], "venue": "Proceedings of the COLING/ACL on Interactive presentation sessions, pages 69\u201372. Association for Computational Linguistics.", "citeRegEx": "Bird.,? 2006", "shortCiteRegEx": "Bird.", "year": 2006}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["William Blacoe", "Mirella Lapata."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "First-order inference and the interpretation of questions and answers", "author": ["Johan Bos", "Malte Gabsdil."], "venue": "Proceedings of Gotelog, pages 43\u201350.", "citeRegEx": "Bos and Gabsdil.,? 2000", "shortCiteRegEx": "Bos and Gabsdil.", "year": 2000}, {"title": "Wide-coverage semantic analysis with boxer", "author": ["Johan Bos."], "venue": "Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics, pages 277\u2013286. College Publi-", "citeRegEx": "Bos.,? 2008", "shortCiteRegEx": "Bos.", "year": 2008}, {"title": "Commutative Algebra: Chapters 1-7", "author": ["N. Bourbaki."], "venue": "Srpinger Verlag, Berlin/New York.", "citeRegEx": "Bourbaki.,? 1989", "shortCiteRegEx": "Bourbaki.", "year": 1989}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark."], "venue": "CoRR, abs/1003.4394.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Minimal recursion semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A Sag."], "venue": "Research on Language and Computation, 3(2-3):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Microsoft research paraphrase corpus", "author": ["Bill Dolan", "Chris Brockett", "Chris Quirk."], "venue": "Retrieved May, 29:2013.", "citeRegEx": "Dolan et al\\.,? 2005", "shortCiteRegEx": "Dolan et al\\.", "year": 2005}, {"title": "Introducing and evaluating ukWaC, a very large web-derived corpus of English", "author": ["Adriano Ferraresi", "Eros Zanchetta", "Marco Baroni", "Silvia Bernardini."], "venue": "Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google, pages 47\u201354.", "citeRegEx": "Ferraresi et al\\.,? 2008", "shortCiteRegEx": "Ferraresi et al\\.", "year": 2008}, {"title": "On sense and reference", "author": ["Gottlob Frege."], "venue": "Ludlow (1997), pages 563\u2013584.", "citeRegEx": "Frege.,? 1892", "shortCiteRegEx": "Frege.", "year": 1892}, {"title": "Switchboard: Telephone speech corpus for research and development", "author": ["John J Godfrey", "Edward C Holliman", "Jane McDaniel."], "venue": "Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE International Conference on, volume 1,", "citeRegEx": "Godfrey et al\\.,? 1992", "shortCiteRegEx": "Godfrey et al\\.", "year": 1992}, {"title": "word2vec Explained: deriving Mikolov et al.\u2019s negativesampling word-embedding method", "author": ["Yoav Goldberg", "Omer Levy"], "venue": "arXiv preprint arXiv:1402.3722", "citeRegEx": "Goldberg and Levy.,? \\Q2014\\E", "shortCiteRegEx": "Goldberg and Levy.", "year": 2014}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1394\u20131404.", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011a", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Experimenting with transitive verbs in a DisCoCat", "author": ["Edward Grefenstette", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 62\u201366, Edinburgh, UK, July. As-", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011b", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Distributional structure", "author": ["Z.S. Harris."], "venue": "Word.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Recurrent convolutional neural networks for discourse compositionality", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 119\u2013126, Sofia, Bulgaria, August. Asso-", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Prior disambiguation of word tensors for constructing sentence vectors", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNL), pages 1590\u20131601, Seat-", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2013", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2013}, {"title": "A study of entanglement in a categorical framework of natural language", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh."], "venue": "Proceedings of the 11th Workshop on Quantum Physics and Logic (QPL), Kyoto, Japan, June.", "citeRegEx": "Kartsaklis and Sadrzadeh.,? 2014", "shortCiteRegEx": "Kartsaklis and Sadrzadeh.", "year": 2014}, {"title": "A unified sentence space for categorical distributional-compositional semantics: Theory and experiments", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Stephen Pulman."], "venue": "Proceedings of COLING 2012: Posters, pages 549\u2013558, Mumbai, India,", "citeRegEx": "Kartsaklis et al\\.,? 2012", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "A systematic study of semantic vector space model parameters", "author": ["Douwe Kiela", "Stephen Clark."], "venue": "Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 21\u201330, Gothenburg, Sweden, April.", "citeRegEx": "Kiela and Clark.,? 2014", "shortCiteRegEx": "Kiela and Clark.", "year": 2014}, {"title": "A Solution to Plato\u2019s Problem: The Latent Semantic Analysis Theory of Acquision, Induction, and Representation of Knowledge", "author": ["T. Landauer", "S. Dumais."], "venue": "Psychological Review.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Claws4: the tagging of the british national corpus", "author": ["Geoffrey Leech", "Roger Garside", "Michael Bryant."], "venue": "Proceedings of the 15th conference on Computational linguistics-Volume 1, pages 622\u2013 628. Association for Computational Linguistics.", "citeRegEx": "Leech et al\\.,? 1994", "shortCiteRegEx": "Leech et al\\.", "year": 1994}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Omer Levy", "Yoav Goldberg", "Israel Ramat-Gan."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Baltimore, Maryland, USA, June.", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Re-examining machine translation metrics for paraphrase identification", "author": ["Nitin Madnani", "Joel Tetreault", "Martin Chodorow."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of", "citeRegEx": "Madnani et al\\.,? 2012", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Corpus-based and knowledge-based measures of text semantic similarity", "author": ["Rada Mihalcea", "Courtney Corley", "Carlo Strapparava."], "venue": "AAAI, volume 6, pages 775\u2013780.", "citeRegEx": "Mihalcea et al\\.,? 2006", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of NAACLHLT, pages 746\u2013751.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Investigating the contribution of distributional semantic information for dialogue act classification", "author": ["Dmitrijs Milajevs", "Matthew Purver."], "venue": "Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC),", "citeRegEx": "Milajevs and Purver.,? 2014", "shortCiteRegEx": "Milajevs and Purver.", "year": 2014}, {"title": "Vector-based models of semantic composition", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Proceedings of ACL-08: HLT, pages 236\u2013244. Association for Computational Linguistics.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata."], "venue": "Cognitive Science, 34(8):1388\u20131439.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Universal grammar", "author": ["Richard Montague."], "venue": "Theoria, 36(3):373\u2013398.", "citeRegEx": "Montague.,? 1970", "shortCiteRegEx": "Montague.", "year": 1970}, {"title": "Improving distributional semantic vectors through context selection and normalisation", "author": ["Tamara Polajnar", "Stephen Clark."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 230\u2013", "citeRegEx": "Polajnar and Clark.,? 2014", "shortCiteRegEx": "Polajnar and Clark.", "year": 2014}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u2013 50, Valletta, Malta, May. ELRA. http://is.", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "Ambiguity resolution in natural language learning", "author": ["Hinrich Sch\u00fctze."], "venue": "csli. Stanford, CA, 4:12\u201336.", "citeRegEx": "Sch\u00fctze.,? 1997", "shortCiteRegEx": "Sch\u00fctze.", "year": 1997}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Dialogue act modeling for automatic tagging and recognition", "author": ["Andreas Stolcke", "Klaus Ries", "Noah Coccaro", "Elizabeth Shriberg", "Rebecca Bates", "Daniel Jurafsky", "Paul Taylor", "Carol Van Ess-Dykema", "Rachel Martin", "Marie Meteer"], "venue": null, "citeRegEx": "Stolcke et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Stolcke et al\\.", "year": 2000}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D Turney", "Patrick Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}, {"title": "Similarity of semantic relations", "author": ["Peter D Turney."], "venue": "Computational Linguistics, 32(3):379\u2013416.", "citeRegEx": "Turney.,? 2006", "shortCiteRegEx": "Turney.", "year": 2006}, {"title": "Dialogue act classification based on intra-utterance features", "author": ["Nick Webb", "Mark Hepple", "Yorick Wilks."], "venue": "Proceedings of the AAAI Workshop on Spoken Language Understanding. Citeseer.", "citeRegEx": "Webb et al\\.,? 2005", "shortCiteRegEx": "Webb et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks.", "startOffset": 23, "endOffset": 95}, {"referenceID": 9, "context": "Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks.", "startOffset": 23, "endOffset": 95}, {"referenceID": 30, "context": "Neural word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013a) have received much attention in the distributional semantics community, and have shown state-of-the-art performance in many natural language processing tasks.", "startOffset": 23, "endOffset": 95}, {"referenceID": 27, "context": "While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive", "startOffset": 107, "endOffset": 147}, {"referenceID": 1, "context": "While they have been compared with co-occurrence based models in simple similarity tasks at the word level (Levy et al., 2014; Baroni et al., 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive", "startOffset": 107, "endOffset": 147}, {"referenceID": 4, "context": ", 2014), we are aware of only one work that attempts a comparison of the two approaches in compositional settings (Blacoe and Lapata, 2012), and this is limited to additive", "startOffset": 114, "endOffset": 139}, {"referenceID": 39, "context": "We are especially interested in investigating the performance of neural word vectors in compositional models involving general mathematical composition operators, rather than in the more task- or domain-specific deep-learning compositional settings they have generally been used with so far (for example, by Socher et al. (2012), Kalchbrenner and Blunsom (2013) and many others).", "startOffset": 308, "endOffset": 329}, {"referenceID": 19, "context": "(2012), Kalchbrenner and Blunsom (2013) and many others).", "startOffset": 8, "endOffset": 40}, {"referenceID": 34, "context": "We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks.", "startOffset": 113, "endOffset": 140}, {"referenceID": 41, "context": "(Stolcke et al., 2000)).", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks.", "startOffset": 194, "endOffset": 215}, {"referenceID": 8, "context": "In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures.", "startOffset": 194, "endOffset": 479}, {"referenceID": 8, "context": "In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures.", "startOffset": 194, "endOffset": 558}, {"referenceID": 8, "context": "In particular, this is the first large-scale study to date that applies neural word representations in tensor-based compositional distributional models of meaning similar to those formalized by Coecke et al. (2010). We test a range of implementations based on this framework, together with additive and multiplicative approaches (Mitchell and Lapata, 2008), in a variety of different tasks. Specifically, we use the verb disambiguation task of Grefenstette and Sadrzadeh (2011a) and the transitive sentence similarity task of Kartsaklis and Sadrzadeh (2014) as small-scale focused experiments on pre-defined sentence structures. Additionally, we evaluate our vector spaces on paraphrase detection (using the Microsoft Research Paraphrase Corpus of Dolan et al. (2005)) and dialogue act tagging using the Switchboard Corpus (see e.", "startOffset": 194, "endOffset": 768}, {"referenceID": 30, "context": "In all of the above tasks, we compare the neural word embeddings of Mikolov et al. (2013a) with two vector spaces both based on co-occurrence counts and produced by standard distributional techniques, as described in detail below.", "startOffset": 68, "endOffset": 91}, {"referenceID": 13, "context": "Formal semantics Formal approaches to the semantics of natural language have long built upon the classical idea of compositionality \u2013 that the meaning of a sentence is a function of the meanings of its parts (Frege, 1892).", "startOffset": 208, "endOffset": 221}, {"referenceID": 36, "context": "In compositional type-logical approaches, predicateargument structures representing phrases and sentences are built from their constituent parts by \u03b2reduction within the lambda calculus framework (Montague, 1970): for example, given a representation of John as john \u2032 and sleeps as \u03bbx.", "startOffset": 196, "endOffset": 212}, {"referenceID": 6, "context": "(Bos, 2008)).", "startOffset": 0, "endOffset": 11}, {"referenceID": 18, "context": "Co-occurrence-based word representations One way to produce such vectorial representations is to directly exploit Harris (1954)\u2019s intuition that semantically similar words tend to appear in similar contexts.", "startOffset": 114, "endOffset": 128}, {"referenceID": 24, "context": ", 2010) and below for discussion, and (Kiela and Clark, 2014) for a detailed comparison.", "startOffset": 38, "endOffset": 61}, {"referenceID": 26, "context": "Table 1: Word co-occurrence frequencies extracted from the BNC (Leech et al., 1994).", "startOffset": 63, "endOffset": 83}, {"referenceID": 31, "context": "the probability of observing the surrounding words in some context (Mikolov et al., 2013b):", "startOffset": 67, "endOffset": 90}, {"referenceID": 32, "context": "Additionally, neural vectors have also proven successful in other tasks (Mikolov et al., 2013c), since they seem to encode not only attributional similarity (the degree to which similar words are close to each other), but also relational similarity (Turney, 2006).", "startOffset": 72, "endOffset": 95}, {"referenceID": 43, "context": ", 2013c), since they seem to encode not only attributional similarity (the degree to which similar words are close to each other), but also relational similarity (Turney, 2006).", "startOffset": 162, "endOffset": 176}, {"referenceID": 34, "context": "The first approach combines word vectors by vector addition or point-wise multiplication (Mitchell and Lapata, 2008)\u2014as this is independent of word order, it cannot capture the difference between the two sentences \u201cdogs chase cats\u201d and \u201ccats chase dogs\u201d.", "startOffset": 89, "endOffset": 116}, {"referenceID": 40, "context": "autoencoders (Socher et al., 2012) or convolutional filters (Kalchbrenner et al.", "startOffset": 13, "endOffset": 34}, {"referenceID": 20, "context": ", 2012) or convolutional filters (Kalchbrenner et al., 2014).", "startOffset": 33, "endOffset": 60}, {"referenceID": 16, "context": "and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al.", "startOffset": 22, "endOffset": 57}, {"referenceID": 16, "context": "and Zamparelli (2010),Grefenstette and Sadrzadeh (2011a), and Kartsaklis et al. (2012).", "startOffset": 22, "endOffset": 87}, {"referenceID": 34, "context": "see (Mitchell and Lapata, 2008).", "startOffset": 4, "endOffset": 31}, {"referenceID": 7, "context": "Using a general duality theorem from multi-linear algebra (Bourbaki, 1989), it follows that tensors are in one-one correspondence with multi-linear maps, that is we have:", "startOffset": 58, "endOffset": 74}, {"referenceID": 21, "context": "Kartsaklis and Sadrzadeh (2013). Since the creation and manipulation of tensors of order higher than 2 is difficult, one can work with simplified versions of tensors, faithful to their underlying mathematical basis; these have found intuitive interpretations,", "startOffset": 0, "endOffset": 32}, {"referenceID": 16, "context": "see Grefenstette and Sadrzadeh (2011a), Kartsaklis and Sadrzadeh (2014).", "startOffset": 4, "endOffset": 39}, {"referenceID": 16, "context": "see Grefenstette and Sadrzadeh (2011a), Kartsaklis and Sadrzadeh (2014). In such cases, \u22c6 becomes a combination of a range of operations such", "startOffset": 4, "endOffset": 72}, {"referenceID": 16, "context": "These models are referred to by relational (Grefenstette and Sadrzadeh, 2011a); they are generalisations of predicate semantics of transitive verbs, from pairs of individuals to pairs of vectors.", "startOffset": 43, "endOffset": 78}, {"referenceID": 17, "context": "These models are referred to by Kronecker, which is the term sometimes used to denote the outer product of tensors (Grefenstette and Sadrzadeh, 2011b).", "startOffset": 115, "endOffset": 150}, {"referenceID": 23, "context": "The models of the last five lines of the table use the so-called Frobenius operators from categorical compositional distributional semantics (Kartsaklis et al., 2012) to expand the relational matrices of verbs from order 2 to order 3.", "startOffset": 141, "endOffset": 166}, {"referenceID": 22, "context": "Add, Frobenius-Mult, and Frobenius-Outer (Kartsaklis and Sadrzadeh, 2014).", "startOffset": 41, "endOffset": 73}, {"referenceID": 24, "context": "Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies).", "startOffset": 121, "endOffset": 170}, {"referenceID": 37, "context": "Co-occurrence-based vector space instantiations have received a lot of attention from the scientific community (refer to (Kiela and Clark, 2014; Polajnar and Clark, 2014) for recent studies).", "startOffset": 121, "endOffset": 170}, {"referenceID": 34, "context": "Addition w1w2 \u00b7 \u00b7 \u00b7wn \u2212\u2192 w1 +\u2212\u2192 w2 + \u00b7 \u00b7 \u00b7+\u2212\u2192 wn Mitchell and Lapata (2008) Multiplication w1w2 \u00b7 \u00b7 \u00b7wn \u2212\u2192 w1 \u2299\u2212\u2192 w2 \u2299 \u00b7 \u00b7 \u00b7 \u2299 \u2212\u2192 wn Mitchell and Lapata (2008)", "startOffset": 49, "endOffset": 76}, {"referenceID": 34, "context": "Addition w1w2 \u00b7 \u00b7 \u00b7wn \u2212\u2192 w1 +\u2212\u2192 w2 + \u00b7 \u00b7 \u00b7+\u2212\u2192 wn Mitchell and Lapata (2008) Multiplication w1w2 \u00b7 \u00b7 \u00b7wn \u2212\u2192 w1 \u2299\u2212\u2192 w2 \u2299 \u00b7 \u00b7 \u00b7 \u2299 \u2212\u2192 wn Mitchell and Lapata (2008)", "startOffset": 49, "endOffset": 160}, {"referenceID": 16, "context": "Relational Sbj Verb Obj Verb \u2299 ( \u2212\u2192 Sbj \u2297 \u2212\u2192 Obj) Grefenstette and Sadrzadeh (2011a)", "startOffset": 50, "endOffset": 85}, {"referenceID": 16, "context": "Kronecker Sbj Verb Obj \u1e7cerb \u2299 ( \u2212\u2192 Sbj \u2297 \u2212\u2192 Obj) Grefenstette and Sadrzadeh (2011b)", "startOffset": 49, "endOffset": 84}, {"referenceID": 23, "context": "Copy object Sbj Verb Obj \u2212\u2192 Sbj \u2299 (Verb \u00d7 \u2212\u2192 Obj) Kartsaklis et al. (2012)", "startOffset": 50, "endOffset": 75}, {"referenceID": 23, "context": "Copy subject Sbj Verb Obj \u2212\u2192 Obj \u2299 (Verb T \u00d7 \u2212\u2192 Sbj) Kartsaklis et al. (2012)", "startOffset": 53, "endOffset": 78}, {"referenceID": 21, "context": "Sbj Verb Obj ( \u2212\u2192 Sbj \u2299 (Verb \u00d7 \u2212\u2192 Obj)) + ( \u2212\u2192 Obj \u2299 (Verb T \u00d7 \u2212\u2192 Sbj)) Kartsaklis and Sadrzadeh (2014)", "startOffset": 73, "endOffset": 105}, {"referenceID": 21, "context": "Sbj Verb Obj ( \u2212\u2192 Sbj \u2299 (Verb \u00d7 \u2212\u2192 Obj))\u2299 ( \u2212\u2192 Obj \u2299 (Verb T \u00d7 \u2212\u2192 Sbj)) Kartsaklis and Sadrzadeh (2014)", "startOffset": 72, "endOffset": 104}, {"referenceID": 21, "context": "outer Sbj Verb Obj ( \u2212\u2192 Sbj \u2299 (Verb \u00d7 \u2212\u2192 Obj))\u2297 ( \u2212\u2192 Obj \u2299 (Verb T \u00d7 \u2212\u2192 Sbj)) Kartsaklis and Sadrzadeh (2014)", "startOffset": 78, "endOffset": 110}, {"referenceID": 26, "context": "In this vector space, the co-occurrence counts are extracted from the British National Corpus (BNC) (Leech et al., 1994).", "startOffset": 100, "endOffset": 120}, {"referenceID": 27, "context": "PPMI has been shown to achieve state-of-the-art results (Levy et al., 2014)", "startOffset": 56, "endOffset": 75}, {"referenceID": 16, "context": "Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al.", "startOffset": 0, "endOffset": 35}, {"referenceID": 16, "context": "Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al.", "startOffset": 0, "endOffset": 63}, {"referenceID": 16, "context": "Grefenstette and Sadrzadeh (2011a), Mitchell and Lapata (2008), Levy et al. (2014) for details.", "startOffset": 0, "endOffset": 83}, {"referenceID": 24, "context": "and is suggested by the review of Kiela and Clark (2014). Our use here of the BNC as a corpus and the window length of 5 is based on previ-", "startOffset": 34, "endOffset": 57}, {"referenceID": 12, "context": "KS14 In this variation, we train a vector space from the ukWaC corpus3 (Ferraresi et al., 2008), originally using as a basis the 2,000 content words with the highest frequency (but excluding a list of stop words as well as the 50 most frequent content words since they exhibit low information content).", "startOffset": 71, "endOffset": 95}, {"referenceID": 0, "context": "SVD has been shown to perform well on a variety of tasks similar to ours (Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2014).", "startOffset": 73, "endOffset": 134}, {"referenceID": 22, "context": "SVD has been shown to perform well on a variety of tasks similar to ours (Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2014).", "startOffset": 73, "endOffset": 134}, {"referenceID": 30, "context": "Neural word embeddings (NWE) For our neural setting, we used the skip-gram model of Mikolov et al. (2013b) trained with negative sampling.", "startOffset": 84, "endOffset": 107}, {"referenceID": 38, "context": "Furthermore, the gensim library (\u0158eh\u016f\u0159ek and Sojka, 2010) was used for accessing the vectors.", "startOffset": 32, "endOffset": 57}, {"referenceID": 31, "context": "More details about the negative sampling approach can be found in (Mikolov et al., 2013b); the note of Goldberg and Levy (2014) also provides an intuitive explanation of the underlying setting.", "startOffset": 66, "endOffset": 89}, {"referenceID": 15, "context": ", 2013b); the note of Goldberg and Levy (2014) also provides an intuitive explanation of the underlying setting.", "startOffset": 22, "endOffset": 47}, {"referenceID": 16, "context": "We use the transitive verb disambiguation dataset described in Grefenstette and Sadrzadeh (2011a)5.", "startOffset": 63, "endOffset": 98}, {"referenceID": 34, "context": "This is similar to the intransitive dataset described in (Mitchell and Lapata, 2008).", "startOffset": 57, "endOffset": 84}, {"referenceID": 17, "context": "28 (Grefenstette and Sadrzadeh, 2011b).", "startOffset": 3, "endOffset": 38}, {"referenceID": 21, "context": "In this experiment we use the transitive sentence similarity dataset described in Kartsaklis and Sadrzadeh (2014). The dataset consists of transitive sentence pairs and a human similarity judgement6.", "startOffset": 82, "endOffset": 114}, {"referenceID": 21, "context": "The textual content of this dataset is the same as that of (Kartsaklis and Sadrzadeh, 2013), the difference is that the dataset of (Kartsaklis and Sadrzadeh, 2014) has updated human judgements whereas the previous dataset used the original annotations of the intransitive dataset of (Mitchell and Lapata, 2010).", "startOffset": 59, "endOffset": 91}, {"referenceID": 22, "context": "The textual content of this dataset is the same as that of (Kartsaklis and Sadrzadeh, 2013), the difference is that the dataset of (Kartsaklis and Sadrzadeh, 2014) has updated human judgements whereas the previous dataset used the original annotations of the intransitive dataset of (Mitchell and Lapata, 2010).", "startOffset": 131, "endOffset": 163}, {"referenceID": 35, "context": "The textual content of this dataset is the same as that of (Kartsaklis and Sadrzadeh, 2013), the difference is that the dataset of (Kartsaklis and Sadrzadeh, 2014) has updated human judgements whereas the previous dataset used the original annotations of the intransitive dataset of (Mitchell and Lapata, 2010).", "startOffset": 283, "endOffset": 310}, {"referenceID": 11, "context": "Specifically, we get classification results on the Microsoft Research Paraphrase Corpus paraphrase corpus (Dolan et al., 2005) working in the following way: we construct vectors for the sentences of each pair; if the cosine similarity between the two sentence vectors exceeds a certain threshold, the pair is classified as a paraphrase, otherwise as not a paraphrase.", "startOffset": 106, "endOffset": 126}, {"referenceID": 28, "context": "84 F-score7) by the time of this writing has been obtained using 8 machine translation metrics and three constituent classifiers (Madnani et al., 2012).", "startOffset": 129, "endOffset": 151}, {"referenceID": 41, "context": "As our last experiment, we evaluate the word spaces on a dialogue act tagging task (Stolcke et al., 2000) over the Switchboard corpus (Godfrey et al.", "startOffset": 83, "endOffset": 105}, {"referenceID": 14, "context": ", 2000) over the Switchboard corpus (Godfrey et al., 1992).", "startOffset": 36, "endOffset": 58}, {"referenceID": 33, "context": "The experiment pipeline follows (Milajevs and Purver, 2014).", "startOffset": 32, "endOffset": 59}, {"referenceID": 44, "context": "The input utterances are preprocessed so that the parts of interrupted utterances are concatenated (Webb et al., 2005).", "startOffset": 99, "endOffset": 118}, {"referenceID": 41, "context": "9 We split the training and testing utterances as suggested by Stolcke et al. (2000). Utterance vectors are then obtained as in the previous experiments; they are", "startOffset": 63, "endOffset": 85}, {"referenceID": 3, "context": "html We use WordNetLemmatizer of the NLTK library (Bird, 2006).", "startOffset": 50, "endOffset": 62}, {"referenceID": 33, "context": "similar results to (Milajevs and Purver, 2014)\u2014although note that the dimensionality of our NWE vectors is 10 times lower than theirs.", "startOffset": 19, "endOffset": 46}, {"referenceID": 33, "context": "Multiplicative NWE outperformed the corresponding model in (Milajevs and Purver, 2014).", "startOffset": 59, "endOffset": 86}, {"referenceID": 1, "context": "While Baroni et al. (2014) conclude that \u201ccontext-predicting models obtain a thorough and resounding victory against their count-based counterparts\u201d, this seems to contradict, at least at the first consideration, the more conservative conclusion of Levy et al.", "startOffset": 6, "endOffset": 27}, {"referenceID": 1, "context": "While Baroni et al. (2014) conclude that \u201ccontext-predicting models obtain a thorough and resounding victory against their count-based counterparts\u201d, this seems to contradict, at least at the first consideration, the more conservative conclusion of Levy et al. (2014) that", "startOffset": 6, "endOffset": 268}, {"referenceID": 4, "context": "tributional word representations\u201d and the findings of Blacoe and Lapata (2012) that \u201cshallow approaches are as good as more computationally intensive alternatives\u201d on phrase similarity and paraphrase detection tasks.", "startOffset": 54, "endOffset": 79}, {"referenceID": 29, "context": "For the paraphrase task, we take the global vector-based similarity reported in (Mihalcea et al., 2006): 0.", "startOffset": 80, "endOffset": 103}, {"referenceID": 33, "context": "For the dialogue act tagging task the baseline is the accuracy of the bag-of-unigrams model in (Milajevs and Purver, 2014): 0.", "startOffset": 95, "endOffset": 122}, {"referenceID": 20, "context": "Kalchbrenner et al. (2014)).", "startOffset": 0, "endOffset": 27}], "year": 2014, "abstractText": "We provide a comparative study between neural word representations and traditional vector spaces based on cooccurrence counts, in a number of compositional tasks. We use three different semantic spaces and implement seven tensor-based compositional models, which we then test (together with simpler additive and multiplicative approaches) in tasks involving verb disambiguation and sentence similarity. To check their scalability, we additionally evaluate the spaces using simple compositional methods on larger-scale tasks with less constrained language: paraphrase detection and dialogue act tagging. In the more constrained tasks, co-occurrence vectors are competitive, although choice of compositional method is important; on the largerscale tasks, they are outperformed by neural word embeddings, which show robust, stable performance across the tasks.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}