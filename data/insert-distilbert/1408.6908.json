{"id": "1408.6908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2014", "title": "AI Evaluation: past, present and future", "abstract": "artificial intelligence develops mathematical techniques and systems whose performance measurements must be evaluated on a regular basis in order to certify competence and foster progress in the discipline. we will describe and critically assess on the different ways ai systems are evaluated. we first focus on the traditional task - oriented evaluation approach. we see that black - box ( behavioural evaluation ) is becoming more and more common, as multiple ai systems are becoming more complex and unpredictable. we identify its three kinds of evaluation : human discrimination, problem benchmarks verification and peer confrontation. we describe the limitations of the many evaluation settings and competitions in merging these three categories and propose several ideas for a more systematic and robust evaluation. we then focus on a less customary ( and challenging ) ability - oriented evaluation approach, where a system is characterised by its ( cognitive ) abilities, rather than classed by the tasks it is designed to solve. we discuss several possibilities : the adaptation of cognitive tests used for humans and animals, supporting the development of tests derived from algorithmic information theory or more general approaches under the perspective of universal psychometrics.", "histories": [["v1", "Fri, 29 Aug 2014 02:44:28 GMT  (560kb,D)", "http://arxiv.org/abs/1408.6908v1", "29 pages"], ["v2", "Fri, 10 Oct 2014 02:16:32 GMT  (568kb,D)", "http://arxiv.org/abs/1408.6908v2", "34 pages"], ["v3", "Sun, 21 Aug 2016 22:44:31 GMT  (572kb,D)", "http://arxiv.org/abs/1408.6908v3", "34 pages. This paper is largely superseded by the following paper: \"Evaluation in artificial intelligence: from task-oriented to ability-oriented measurement\" Journal of Artificial Intelligence Review (2016). doi:10.1007/s10462-016-9505-7, \\url{this http URL}. Please check and refer to the journal paper"]], "COMMENTS": "29 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jose hernandez-orallo"], "accepted": false, "id": "1408.6908"}, "pdf": {"name": "1408.6908.pdf", "metadata": {"source": "CRF", "title": "AI Evaluation: past, present and future\u2217", "authors": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "emails": ["jorallo@dsic.upv.es"], "sections": [{"heading": null, "text": "Keywords: AI evaluation, AI competitions, benchmark evaluation, sampling, narrow vs general AI, measurement, universal psychometrics, Turing Test."}, {"heading": "1 Introduction", "text": "The evaluation of any discipline must necessarily be linked to the purpose of the discipline. What is the purpose of artificial intelligence (AI)? McCarthy\u2019s pristine definition of AI sets this unambiguously: \u201c[AI is] the science and engineering of making intelligent machines\u201d [95]. As a consequence, AI evaluation should focus on evaluating the intelligence of the artefacts it builds. However, as we will further discuss below, \u2018intelligence tests\u2019 (of whatever kind) are not the everyday evaluation approach for AI. The explanation for this is that most AI research is better identified by Minsky\u2019s more pragmatic definition: \u201c[AI is] the science of making machines capable of performing tasks that would require intelligence if done by [humans]\u201d [100, p.v]. As a result, AI evaluation focusses on checking whether machines do these tasks correctly.\nThis has led to an important anomaly of AI. AI artefacts solve these tasks without featuring intelligence. Paradoxically, this is one of the reasons of AI success. Systems are designed for a particular functionality and perform their task more predictably than humans, from driving cars to supply chain planning. Frequently, some tasks are not considered AI problems anymore, once they are solved without full-fledged intelligence. This phenomenon is known as the \u201cAI effect\u201d [96]. It would be unfair, however, to deny that some current AI systems, especially those that incorporate some learning potential, exhibit some intelligent behaviour.\nAnyway, it is not the purpose of this paper to dig further into the time-worn debate between narrow AI vs. general AI. Both approaches are valid and genuine parts of AI research. It is useful to have specific AI systems, as well as systems that have abilities so that they can solve new problems they have never faced before. The intention of stressing this duality is that this should necessarily pervade the evaluation procedures in AI. Specific AI systems should require a task-oriented evaluation, while general AI systems should require an ability-oriented evaluation.\n\u2217This paper corresponds to a lecture given for the Summer School of the Spanish Association for Artificial Intelligence, in A Corun\u0303a, Spain, September 2014.\nar X\niv :1\n40 8.\n69 08\nv1 [\ncs .A\nI] 2\n9 A\nug 2\n01 4\nThis paper pays attention to the way evaluation is done in AI. As any science and engineering discipline, measuring is crucial for AI. Disciplines progress when they have objective evaluation tools to measure the elements and objects of study, assess the prototypes and artefacts that are being built and examine the discipline as a whole. As we will discuss in subsequent sections, despite the significant progress in the past couple of decades (with the generalisation of several AI benchmarks and competitions) there is still a huge margin of improvement in the way AI systems are evaluated. Also, it is probably a crucial moment to overhaul the way AI evaluation is performed, after the recent progress in areas of AI that are detaching from the narrow AI approach, such as developmental robotics [4], deep learning [3], inductive programming [53, 48, 47], artificial general intelligence [45], universal artificial intelligence [71], etc.\nBy overhauling AI evaluation, we aim at filling a gap, because, to our knowledge, there is no comprehensive analysis about how evaluation is performed in AI and how it can be improved and adapted to the challenges of the future. Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how. Nonetheless, we will refer to many of these work along the text.\nSome of the old analysis still hold today. For instance, in [19] we find criteria for evaluating research problems, methods, implementations, experiments\u2019 design, and evaluation of the experiments. In the criteria for experiments\u2019 design, we see several of the topics we will address in the paper: \u201c1. How many examples can be demonstrated?\u201d (are they sufficient and qualitative different and illustrative?), \u201c2. Should the program\u2019s performance be compared to a standard?\u201d, \u201c3. What are the criteria for good performance?\u201d, \u201c4. Does the program purport to be general (domain-independent)?\u201d (do the domains being tested constitute a represenative class?), and \u201c5. Is a series of related programs being evaluated?\u201d. Other statements in [19] are not so up-to-date, and show that there has been an improvement in AI evaluation. For instance, we found the recommendation \u201cthat editors, program committees, and reviewers should begin to insist on evaluation.\u201d. Today this recommendation has been generalised (e.g., [21] report that more than 60% of ICAIL papers in 1987 did not have any evaluation in front of 20% in 2011). Hence, a lack of evaluation is no longer the problem. However, there is still a great deal of disaggregation, many ad-hoc procedures, bad habits and loopholes about what is being measured and how is being measured. In this paper, the focus will be focussed on these issues.\nWe will start with a state of the art of the task-oriented evaluation approach in AI, by far more common in AI research. The notion of performance is relatively easy to determine as it is directly linked to the set or class of problems we are interested in for the evaluation. Nonetheless, we will identify several problems, most of them derived from the confusion of a task definition with its evaluation. An appropriate sampling procedure from the class of problems defining the task is not always easy. We will give some hints to derive better evaluation protocols. With this perspective we will argue that white-box evaluation (by algorithm inspection) is becoming less predominant in AI, and will focus the rest of our attention to black-box evaluation (by behaviour). We will distinguish three types of behavioural evaluation: by human discrimination (performing a comparison against or by humans), problem benchmarks (a repository or generator of problems) and by peer confrontation (1-vs-1 or multi-agent \u2018matches\u2019). We will survey some of the competitions and repositories in these three categories and highlight some problems in how these evaluation settings are held and used.\nIn a second part of the paper, we will pay attention to the more elusive and challenging problem of ability-based evaluation. The three types of evaluation seen for task-oriented evaluation are not directly applicable, as we now do not want to evaluate systems about what they do but about what they are able to (learn to) do. In other words, we are looking for signs or indications that show that the system has a certain ability. One idea that has been around since the inception of AI is to use human (or animal) intelligence tests, such as the IQ-tests used in psychometrics. Each particular test tries to identify a series of exercises that are representative (necessary and sufficient) for a given ability. We will briefly discuss their use and possible adaptation for the evaluation of AI systems. A quite different approach is based on algorithmic information theory (AIT), where problem classes and their difficulty are derived from computational principles. In this way, we are sure about what we are actually evaluating. Also, exercise generators can be derived from first principles.\nWhile task-oriented evaluation is opposed to ability-oriented evaluation in this paper, we can have a more gradual view in terms of task classes that go from specific to general. Also, we will analyse a more unified view that integrates the different evaluation paradigms and procedures that we find in many disciplines,\ndepending on the subject that is being measured. This view, known as \u2018universal psychometrics\u2019, is based on the notion of \u2018universal test\u2019. This unified approach makes it possible that the schemas that were identified for the task-based evaluation can be generalised to the ability-based evaluation problem.\nThe rest of the paper goes along the organisation described above, with two parts: section 2, focussing on task-oriented evaluation and section 3, focussing on ability-oriented evaluation. This is followed by the conclusions, which feature some guidelines about how competitions and problem generators can be improved, integrated or overhauled for a more robust and efficient AI system evaluation."}, {"heading": "2 Task-oriented evaluation", "text": "AI is a successful discipline. The range of applications has been greatly enlargened over the years. We have successful applications in computer vision, speech recognition, music analysis, machine translation, text summarisation, information retrieval, robotic navigation and interaction, automated vehicles, game playing, prediction, estimation, planning, automated deduction, expert systems, etc. (see, e.g., [108]). Most of these application problems are specific. This implies that the goals are clear and that researchers can focus on the problem. This does not mean that we are not allowed to more general principles and techniques to solve many of these problems, but that the task is sufficiently specific so that systems are specialised for these systems. For instance, robotic navigation of a Mars rover can share some of the techniques with a driverless car on Earth, but the final application is extremely specialised in both cases.\nThis specialisation leads to an application-specific (task-oriented) evaluation. In fact, going from an abstract problem to a specific task is encouraged: \u201crefine the topic to a task\u201d, provided it is \u201crepresentative\u201d [19]. Given a precise definition of the task, we only need to define a notion of performance from it. Clearly, we measure performance, and not intelligence. In fact, many of the best systems usually solve problems in a way that is different to the way humans solve the same problem. Also, systems usually include a great amount of built-in programming and knowledge for the task. It is not unfair to say that we evaluate the researchers that have designed the system rather than the system itself. For instance, we can say that it was the research team after Deep Blue [15] (with the help of a powerful computer) who actually defeated Kasparov.\nDisregarding who is to praise for each new successful application, AI systems that address specialised problems with a clear performance should be easy to evaluate. The reality is not that straightforward, mostly because there are many different (and usually ad-hoc) evaluation approaches. Let us have a perusal over them."}, {"heading": "2.1 Types of performance measurement in AI", "text": "An application, as described above, can be characterised by a set of problems, tasks or exercises M . In order to evaluate each exercise \u00b5 \u2208 M we can get a measurement R(\u03c0, \u00b5) of the performance of system \u03c0. Measurements can be imperfect. Also, the system, the problem or the measurement may be nondeterministic. As a result, it is usual to work with the expected value of the performance of \u03c0 as E[R(\u03c0, \u00b5)].\nThe definition of M and R does not specify how we want to aggregate the results when M has more than one problem. The most common approaches are:\n\u2022 Worst-case performance1: \u03a6min(\u03c0,M) = min\n\u00b5\u2208M E[R(\u03c0, \u00b5)]\n\u2022 Best-case performance: \u03a6max(\u03c0,M) = min\n\u00b5\u2208M E[R(\u03c0, \u00b5)]\n\u2022 Average-case performance: \u03a6(\u03c0,M, p) = \u2211 \u00b5\u2208M p(\u00b5) \u00b7 E[R(\u03c0, \u00b5)] (1)\nwhere p is a probability distribution on M .\n1Note that this formula does not have the size of the instance as a parameter, and hence it is not comparable to the usual view of worst-case analysis of algorithms.\nIt is assumed that the magnitudes of R for different \u03c0 \u2208 M are commensurate. For instance, if R can range between 0 and 1 for problem \u00b51 but ranges between 0 and 10,000 for problem \u00b52, the latter will have a much higher weight and will dominate the aggregation. This is not necessarily wrong, e.g., if they are measured with the same unit (e.g., euros). In general, however, R is a construct that needs to be normalised. The choice of a performance metric that is sufficiently normalised such that the results are commensurate is not always easy, but possible to some extent (see, e.g., [140]).\nAt this point, it is pertinent to make a comment about the well-known no-free-lunch (NFL) theorems [145, 143, 144], as these theorems are usually misunderstood. These theorems do not mean that given all possible problems, no method can work better than any other on average. The argument to support this interpretation is that, considering all problems, if method \u03c0A is better than method \u03c0B for one problem then \u03c0B will be worse than \u03c0A for another problem. Some people have even interpreted that research in AI (including search and optimisation problems in computer science) is futile. However, what the NFL theorem really shows is that \u2014under some conditions\u2014 no method is better on average for all problems. The conditions state that M must be infinite and include all possible problems. Also, the problems can be shuffled without affecting the probability, e.g., the uniform distribution would be a special case. Nonetheless, these conditions are not plausible if the problems are taken from the real world. It is unrealistic to assume that the problems we face are taken from a series of random bits, or that a problem, and its opposite problem (whatever it is) are equally probable. Many other distributions are much more plausible. A universal distribution [120, 89], e.g., which is consistent with the idea that problems are generated by physical laws, processes, living creatures, etc., states that random (incompressible) problems are less likely. So, for many distributions p, the conditions of the NFL do not hold and we have that there can be methods \u03c0A and \u03c0B such that: \u03a6(\u03c0A,M, p) > \u03a6(\u03c0B ,M, p). In fact, there can be optimal methods.\nAfter this clarification, it is relevant to determine how R is going to be obtained. It is a possibility, at least for relatively simple solutions, to have the code or algorithm of the system \u03c0 that can be well understood so that its computational properties clearly determined. We use the term \u2018white-box\u2019 evaluation when R is inferred through program inspection or algorithm analysis. White-box evaluation is powerful because we can obtain R theoretically for a given agent \u03c0 and a problem class M (provided both are defined theoretically). One common type of problems that are evaluated with a white-box approach are those where the solution to the problem has to be correct or optimal (i.e., perfect). In this case, the performance metric R is defined in terms of time and/or space resources. This is the case of classical computational complexity theory. Worst-case analysis is more common than average-case analysis, although the latter has also become popular recently [82, 88, 46]. Nonetheless, many AI problems are so challenging nowadays that perfect solutions are no longer considered as a constraint. Instead, approximate solvers are designed to optimise a performance metric that is defined in terms of the level of error of the solution and the time and/or space resources. In this case, the use of an average-case analysis is more common, although worst-case analysis can also be studied under some paradigms (e.g., Probably Approximately Correct learning [129]). The theoretical analysis of \u2018white-box\u2019 evaluation has also been applied to games. For instance, in board games, algorithms can be derived and analysed whether they are optimal, such as noughts and crosses (tic-tac-toe) and English draughts (checkers), the latter solved by Jonathan Schaeffer [110]. Finally, in game theory, the expected payoff plays the role of R and optimal strategies can be determined for some simple games, as well as equilibria and other properties. In games, some results can be obtained independently of the opponent, but others are only true if we also know the algorithm that the other players are using (so it becomes a double \u2018white-box\u2019 approach to evaluation).\nAs AI systems become more sophisticated, white-box assessment becomes more difficult, if not impossible, because the unpredictability of complex systems. Many AI systems incorporate many different techniques and have stochastic behaviours. This is also in agreement with a view of AI as an experimental science [14, 117]. As a result, a black-box approach, is taken. This means that R is obtained exclusively from the behaviour of the system in an empirical way. In this case, average-case evaluation is usual2.\nThere are many kinds of white-box assessment in AI, but we can group them into three main categories:\n\u2022 Human discrimination: The assessment is made by and/or against humans through observation, 2Although it is not uncommon, as we will see, that the set of problems from M are chosen by the research team that is evaluating its own method, so the probability to choose from M can be biased in such a way that it is actually a best-case evaluation.\nscrutiny and/or interview. Although it can be based on a questionnaire or a procedure, the assessment is usually informal and subjective. This type of evaluation is common in psychology, ethology and comparative psychology. In AI, this kind of evaluation is not very usual, except for the Turing Test and variants, as we will discuss later on.\n\u2022 Problem benchmarks: The assessment is performed against a collection or repository of problems (M). This approach is very frequent in AI, where we have problem libraries, repositories, corpora, etc. It is also usual in psychology and comparative psychology, although in these areas the tests are not publicly available to the systems that are to be evaluated. This has been proposed or suggested in AI occasionally (e.g., the \u201csecret generalized methodology\u201d [140]). For instance, M can be generated in real time using problem generator, which actually defines M and p.\n\u2022 Peer confrontation: The assessment for (multi-agent) games is performed through a series of (1-vs-1 or n-vs-n) matches. The result is relative to the other participants. Given this relative value, in order to allow for a numerical comparison, sophisticated performance metrics can be derived (e.g., the Elo system in chess [35]).\nThe combination of some of the above is also common for evaluation. In what follows, we analyse each of the three categories in more detail."}, {"heading": "2.2 Evaluation by human discrimination", "text": "In this first category we include the evaluation approaches that are performed by a comparison with or by humans. The Turing Test [128, 103] is a case in which there is both comparison against humans and evaluation by human judges. While the \u2018imitation game\u2019 was introduced by Turing as a philosophical instrument in his response to nine objections against machine intelligence, the game has been (mis-)understood as an actual test ever since, with the standard interpretation of one human, one machine pretending to be a human, and a human interrogator through a teletype acting as a judge. The latter must tell which one is the machine and the human.\nNot only has the game been taken as an actual test, but it has had several implementations, such as the Loebner Prize3, held every year since 1991. Despite the criticisms of how this prize is conducted and its interpretation through the years, there have been more implementations. In 2014, Kevin Warwick at the University of Warwick organised a similar competition that took place at the Royal Society in London. Even if the results were not significantly different to previous results of the Loebner Prize (or even what Weizenbaum\u2019s ELIZA was able to do fifty years ago [138]), the over-reaction and publicity of this outcome were preposterous. The reputation of the Turing Test if (further) stained with statement such as this: \u201cIf a computer is mistaken for a human more than 30% of the time during a series of five minute keyboard conversations it passes the test. No computer has ever achieved this, until now. Eugene managed to convince 33% of the human judges (30 judges took part [...]) that it was human.\u201d [135]. And Kevin Warwick goes on: \u201cWe are therefore proud to declare that Alan Turing\u2019s Test was passed for the first time. [...] This milestone will go down in history as one of the most exciting\u201d.\nIs the imitation game a valid test? Even assuming that the times and thresholds are stricter than the previous incarnations, the Turing Test has many problems as an intelligence test. First, it is a test of humanity, relative to human characteristics (i.e., anthropocentric). It is neither gradual nor factorial and needs human intervention (it cannot be automated). If done properly, it may take too much time. As a result, the Turing Test is neither a sufficient nor a necessary condition for intelligence. Despite the criticism, the Turing test still has many advocates [104]. It is also an inspiration for countless philosophical debates and has led to connections with other concepts in AI or computation [61, 62].\nIn any case, Turing is not to be blamed by a failure of the Turing Test as a useful test to evaluate AI systems. Turing did not conceive the test as a practical test to measure intelligence up to and beyond human intelligence. He is not to blame for a philosophical construct that has had a great impact in the philosophy and understanding of machine intelligence, but a negative impact on its measurement.\nDoes this mean that we should discard the idea of evaluating by human judges or by comparing with humans? Not at all. Recently, there have been variants of the Turing Test (Total Turing Tests [112], Visual\n3http://www.loebner.net/Prizef/loebner-prize.html.\nTuring Tests including sensory information, robotic interfaces, virtual worlds, etc. [101, 67]) that may be useful for chatterbot evaluation, personal assistants and videogames. It is within the area of videogames where the notion of \u2018believability\u2019 has appeared, as the property of a bot of looking \u2018believable\u2019 as a human [90, 68]. This term is interesting, as it clearly detaches these tests from the evaluationg of intelligence. In videogames, there are applications where we want bots that can fool opponents into thinking that they are just another human player. Other highly subjective properties may also be of interest: enjoyability, fun, etc.\nFinally, there is a kind of test that is related to the Turing Test, the so-called CAPTCHA, Completely Automated Public Turing test to tell Computers and Humans Apart [131, 132]. It is said to be a \u2018reverse Turing Test\u2019 because the goal is to tell computers and humans apart in order to ensure that an action or access is only performed by a human (e.g., making a post, registering in a service, etc.). CAPTCHAs are quick and practical, omnipresent nowadays. However, they are designed according to the tasks that are solved by the current state of AI technology. At present, for instance, a common CAPTCHA is a series of distorted letters, which are usually easy to recognise by humans but current OCR systems struggle. Logically, when character recognition systems improve, CAPTCHAs need to be updated to more distorted words or to other tasks that are beyond AI technology.\nTable 1 includes a selection of evaluation settings under the human-discrimination category. As it is not possible to go into the details of all of them because of brevity, let us choose one that is most representative and with a strong future projection: the BotPrize competition, which has been held since 2008. This contest awards the bot that is deemed as more believable (playing like a human) by the other (human) players. The competition uses a first-person shooter videogame, DeathMatch game type, as used in Unreal Tournament 2004. It is important to clarify that the bots do not process the image but receive a description of it through textual messages in a specific language through the GameBots2004 interface (Pogamut). For the competition, chatting is disabled (as it is not a chatbot competition). There is a \u201cjudging gun\u201d and the human judges also play, trying to play normally (a prize for the judges exists for those that are considered more \u201chuman\u201d by other judges).\nSome questions have been raised about how well the competition evaluates the believability of the participants. For instance, believability is said to be better assessed from a third-person perspective (judging recorded video of other players without playing) than with a first-person perspective [127]. The reason is that human judges can concentrate on judging and not on not being killed or aiming at high scores. Actually, this third-person perspective is included in the 2014 competition using a crowdsourcing platform [91] so that the 2014 edition incorporates the two judging systems: the First-Person Assessment (FPA), using the BotPrize in-game judging system, and the Third-Person Assessment (TPA), using a crowdsourcing platform. Another issue that could be considered in the future is a richer (and more challenging) representation of the environment, closer to the way humans perceive the images of the game (such as the graphical processing required for the Arcade Learning Environment [7]).\nFinally, as a summary of the limitations and potentials of the human-discrimination category, we first acknowledge that some variants are being useful. However, the format differs significantly from a standard Turing test. For instance, the human-discrimination approach to evaluation can be just solved by a more\n4http://www.loebner.net/Prizef/loebner-prize.html 5http://www.reading.ac.uk/news-and-events/releases/PR583836.aspx 6http://botprize.org/ 7http://www.robochatchallenge.com/ 8http://www.captcha.net/\ntraditional interview format with a procedure or storyline (as in psychology interviews), or by an evaluation through observation (using a committee of dedicated judges). This casts doubts about whether evaluation by imitation as understood in the standard interpretation of the Turing Test is being practical for AI evaluation. It is the concept that is useful, and deserves being adapted to several applications."}, {"heading": "2.3 Evaluation through problem benchmarks", "text": "In this very common approach to evaluation, M is defined as a set of problems. This fits equation 1 perfectly. Necessarily, the quality of the evaluations depends on M and how exhaustively this set is explored. There are other issues that could compromise the quality of the measurement. For instance, when M is a public problem repository and is not very large, we have that the systems can specialise for M . Also, the solutions may also be available beforehand, or can be inferred by humans, so the systems can embed part of the solutions. In fact, a system can succeed in a benchmark with a small size of M by using a technique known as the \u201cbig switch\u201d, i.e., the system recognises which problem is facing and uses the hardwired solution for that specific exercise. Things can become worse if the selection of examples from M is made by the researchers themselves (e.g., the usual procedure in machine learning of selecting 10 or 20 datasets from the UCI repository [5], as we will discuss below). In general, the size of M and a bona fide attitude to research somewhat limit these concerns. Nonetheless, it is generally acknowledged that most systems actually embed what the researchers have learnt from M . In a way, these benchmarks actually evaluate the researchers, not their systems.\nBecause of the \u2018evaluation overfitting\u2019 [140], \u2018method overfitting\u2019 problem [37] or other \u201cclever methods of overfitting\u201d [83], it is much better if M is very large or infinite, or at least the problems are not disclosed until evaluation time. Problem generators are an alternative. However, it is not always easy to generate a large M of realistic problems. Generators can be based on the use of some prototypes with parameter variations or distortions. These prototypes can be \u201cbased on reality\u201d, so that the generator \u201ctakes as input a real domain, analyses it automatically and generates deformations [...] that follow certain high-level characteristics\u201d [33]. More powerful and diverse generators can be defined by the use of problem representation languages. A general and elegant approach is to determine a probabilistic or stochastic generator (e.g., a grammar) of problems, which directly defines the probability p for the average-case performance equation 1. Nonetheless, it is not easy to make a generator that can rule out unusable or Frankenstein problems.\nWhen the set of problems is large or generated, we clearly cannot evaluate AI systems efficiently with the whole set M . So we need to do some sampling of M . It is at this point when we need to distinguish the benchmark or problem definition from an effective evaluation. Assume we have a limited number of exercises n that we can administer. The goal will be to reduce the variance of the measurement given n. One naive approach is to sort M by decreasing p and evaluate the system with the first n exercises. This maximises the accumulated mass for p for a given n. The problem about this procedure is that it is highly predictable. Systems will surely specialise on the first n exercises. Also, this approach is not very meaningful when R is non-deterministic and/or not completely reliable. Repeated testing may be necessary, which raises the question of whether to explore a higher n or to perform more repetitions.\nRandom sampling using p seems to be a more reasonable alternative. As said above, if R is nondeterministic and/or subject to measurement error, then random sampling can be with replacement. If M and p define the benchmark, is probability-proportional sampling on p the best way to evaluate systems? The answer is no, in general. There are better ways of approximating equation 1. The idea is to sample in such a way that the diversity of the selection is increased. This \u2018diversity-driven sampling\u201d is related to some kinds of sampling known as importance sampling [121], stratified sampling [18] and other forced Monte Carlo procedures. The key issue is that we use a different probability distribution for sampling. Although there are many ways of obtaining a \u2018diverse\u2019 sample, we just highlight two main approaches that can be useful for AI evaluation:\n\u2022 Information-driven sampling: Assume that we have a similarity function sim(\u00b51, \u00b52), which indicates how similar (or correlated) exercises \u00b51 and \u00b52 in M are. In this case, we need to sample on M such that the accumulated mass on p is high and that diversity is also high. The rationale is that if \u00b51 and \u00b52 are very similar, using one of them can \u2018fill the gap\u2019 of the other, and we can assume as if both \u00b51 and \u00b52 had been explored, actually accumulating p(\u00b51) + p(\u00b52). One possible way of doing this is by\ncluster sampling. Information-driven sampling suffers from the need of defining the similarity function sim. An alternative is to derive m features that describe the exercises, so creating an m-dimensional space where distances and other topological information can be used to support the notion of diversity (and performing clustering). An example of this procedure is shown in Figure 1 (left).\n\u2022 Difficulty-driven sampling. A set M can contain very easy and very challenging problems, which can lack interest for a specific subject or system. The idea is to choose a range of difficulties for which the evaluation results may be informative (or to give higher probability to exercises inside this range), as in Figure 1 (right). This procedure is done to a greater or lesser degree in many evaluations and benchmarks in AI. In fact, more challenging problems are usually added over the years, as the systems are able to solve the easy problems (that soon become \u2018toy problems\u2019). One of the crucial points of difficulty-driven sampling is the definition of a difficulty function d : M \u2192 R+. Ideally, we would like that for every \u03c0,\u03a6(\u03c0, \u00b51, p) < \u03a6(\u03c0, \u00b52, p) iff d(\u00b51) < d(\u00b52). In practice, this condition is too strong, and more flexible characterisations are expected, such as that for every \u03c0, and two difficulties a and b such that a \u2264 b we have that \u03a6(\u03c0,Ma, p) \u2265 \u03a6(\u03c0,Mb, p) (where Ma denotes all the exercises in M of difficulty a). This could still too strong and we may use a relaxed version such that for every \u03c0, there is a t such that for all a and b \u2265 a + t: \u03a6(\u03c0,Ma, p) \u2265 \u03a6(\u03c0,Mb, p). In experimental sciences, we have a population-based view of difficulty such that d(\u00b5) is monotonically decreasing on E\u03c0\u2208\u2126[\u03a6(\u03c0, \u00b5, p)], where \u2126 is a population of subjects, agents or systems that are evaluated for the same problem class. In fact, Item Response Theory [36] in psychometrics follows this approach. Finally, we can derive the diffuclty of a problem as a function of the complexity of the problem itself. The complexity metric can be specific to the application (such as the complexity for mazes in [147] or gridworld domains in [123]) or it can be a more general approach (e.g., Kolmogorov complexity). Note that some of the definitions of difficulty above would not be possible for a set M and distribution p it the conditions of the NFL theorem held.\nBoth the information-driven sampling and the difficulty-driven sampling can be made adaptive. The first\nis represented by what is known by adaptive cluster sampling [114] and it is common in population surveys and many experimental sciences. However, when evaluating performance, it is difficulty-driven sampling that has been used more systematically in the past, especially in psychometrics. In psychometrics, difficulty is inferred from a population of subjects (in the case of AI, this could be a set of solvers or algorithms). Instead of difficulty, items are analysed by proficiency, represented by \u03b8, a corresponding concept to difficulty from the point of view of the solver (higher problem difficulty requires higher agent proficiency).\nItem response theory (IRT) [36] estimates mathematical models to infer the associated probability and informativeness estimations for each item. One very common model for problems where R is discrete is the three-parameter logistic model, where the item response function (or curve) corresponds to the probability that an agent with proficiency \u03b8 gives a correct response to an item. This model is characterised as follows:\np(\u03b8) , c+ 1\u2212 c\n1 + e\u2212a(\u03b8\u2212b)\nwhere a is the discrimination (the maximum slope of the curve), b is the difficulty or item location (the value of \u03b8 leading to a probability half-way between c and 1, i.e., (1 + c)/2), and c is the chance or asymptotic minimum (the value that is obtained by random guess, as in multiple choice items). The zero-ability expected result is given when \u03b8 = 0, which is exactly z = c + 1\u2212c\n1+eab . Figure 2 (left) shows an example of a logistic\nitem response curve. For continuous R, if they are bounded, the logistic model above may be appropriate. On other occasions, especially if Ris unbounded, a linear model may be preferred [99, 38]:\nX(\u03b8) , z + \u03bb\u03b8 + \u03f5\nwhere z is the intercept (zero-ability expected result), \u03bb is the loading or slope, and \u03f5 is the measurement error. Again, the slope \u03bb is positively related to most measures of discriminating power [39]. Figure 2 (right) shows an example of a linear item response curve.\nWorking with item response models is very useful for the design of tests, because if we have a collection of items, we can choose the most suited one for the subject (or population) we want to evaluate. According to the results that the subject has obtained on previous items, we may choose more difficult items if the subject has succeeded on the easy ones, we may look for those items that are most discriminating (i.e., most\ninformative) in the area we have doubts, etc. Note that discrimination is not a global issue: a curve may have a very high slope at a given point, so it is highly discriminating in this area, but the curve will almost be flat when we are far from this point. Conversely, if we have a low slope, then the item covers a wide range of difficulties but the result of the item will not be so informative as for a higher slope.\nFigure 3 shows an example of an adaptive test using IRT. The sequence of exercise difficulties is shown on the left. The plot on the right shows that averaging the results (especially here, as the outcome of R is discrete, either 0 or 1) makes the estimation of \u03a6 more difficult with a non-adaptive test.\nTable 2 includes a selection of evaluation settings in the problem benchmarks category. We see the variety of repositories, challenges and competitions. As it is impossible to survey all of them in detail, we will focus on one of them, perhaps the most widespread repository in computer science, the UCI machine learning repository [5]. Most of the discussion below is applicable to other repositories and, to some extent, to competitions and challenges in machine learning. The UCI repository includes many supervised (classification and regression) and some unsupervised datasets. The repository is publicly available and is regularly used in machine learning research. The procedure, which is referred as \u201cThe UCI test\u201d [93] or the \u201cde facto approach\u201d [33][77] follows the general form of equation 1 where M is the repository, p is the choice of datasets and R is one particular performance metric (accuracy, AUC, Brier score, F-measure, MSE, etc. [40, 60]). With the chosen datasets, several algorithms (where one or more are usually introduced by the\n9http://www.cs.miami.edu/~tptp/CASC/ 10http://termination-portal.org/wiki/Termination_Competition_2014 11http://www.rl-competition.org/ 12http://www.sygus.org/ 13http://www.aerialroboticscompetition.org/ 14http://archive.darpa.mil/grandchallenge04/index.htm 15http://archive.darpa.mil/grandchallenge/ 16http://www.darpa.mil/cybergrandchallenge/ 17http://www.theroboticschallenge.org/ 18http://ipc.icaps-conference.org/ 19http://archive.ics.uci.edu/ml/ 20http://sci2s.ugr.es/keel/datasets.php 21http://prtools.org/ 22http://www.sigkdd.org/kddcup/index.php 23https://www.kaggle.com/ 24http://www.statmt.org/europarl/ 25http://www.statmt.org/setimes/ 26http://matrix.statmt.org/matrix/info 27https://www.ldc.upenn.edu/new-corpora 28http://www.arcadelearningenvironment.org/ 29http://gpbenchmarks.org/ 30http://www.movingai.com/benchmarks/\nauthors of the research work) can be evaluated by their performance of their datasets. The aggregation over several datasets according equation 1, however, is not very common in machine learning, as there is the common belief that averaging the results for several datasets is wrong, as results are not commensurate (see, e.g., [23]). We already discussed this issue in section 2.1 and saw that there are ways to normalise the performance metric or use some utility or cost measure instead (e.g., what are the costs, in euros, of false positives and false negatives for each dataset) such that they can be aggregated.\n\u201cThe UCI test\u201d cam be seen as a bona-fide mix of the problem benchmark approach and the peer confrontation approach. Even if there is a repository (M), only a few problems are chosen, and can be cherry-picked (p is changing and arbitrary). Also, as the researchers\u2019 algorithm has to be compared with other algorithms, a few competitors are chosen, which can also be cherry-picked, without much effort on fine-tuning their best parameters. Finally, as the results are analysed by statistical tests, cross-validation or other repetition approaches are used to reduce the variance of R(\u03c0, \u00b5, p) so that we have fewer \u201cties\u201d. The UCI repository is not to blame for this procedure, which frequently leads to claims about new methods being better than the rest. Many of these claims are, apart from uninteresting, dubious, even for papers in good venues.\nAs a result, there have been suggestions of a better use of the UCI repository. These suggestions imply an improvement of the procedure but also of the repository itself. For instance, UCI+, \u201ca mindful UCI\u201d [93], proposes the characterisation of the problems in the UCI repository by a set of complexity measures from [69] (the number of classes, however, is not part of the description). This characterisation can be used to make samples that are more diverse and representative. Also, they discuss the notion of a problem being \u2018challenging\u2019, trying to infer a notion of \u2018difficulty\u2019. In the end, an artificial dataset generator is proposed to complement the original UCI dataset. It is a distortion-based generator (similar to Soares\u2019s UCI++ [118]). Finally, [93] suggest ideas about sharing and arranging the results of previous evaluations so that each new algorithm can be compared immediately with many other algorithms using the same experimental setting. An automated submission procedure, such as Kaggle, if performed for a wide range of problems at a time, could be a way of controlling some of the methodological problems of how the UCI repository is used.\nAlthough some of these improvements are in the line of better sampling approaches (more representative and more effecitve), there are still many issues about the way these repositories are constructed and used. The complexity measures could be used to derive how representative a problem is with respect to the whole distribution in order to make a more adequate sampling procedure (e.g., a clustering sampling). Also, a pattern-based generator instead of a distortion-based generator could give more control of what is generated\nand its difficulty. This could be done with a stochastic generative grammar for different kinds of patterns, as is usually done with artificial datasets, using Gaussians or geometrical constructs. . Finally, if results are aggregated according to equation 1, the experimental setting and the use of repetitions should be overhauled. For instance, by using 20 different problems with 10 repetitions using cross-validation (a very common setting in machine learning experiments) we have less information than by using 200 different problems with 1 repetition. Choosing teh least informative procedure only makes sense because of the way results are fitted into the statistical tests and also because repetitions usually involve less effort than preparing a large number of datasets.\nOverall, even if the UCI repository and machine learning are very particular, many of the benchmarks in Table 2 suffer from the same problems about how representative the problems are (if M is small) or how representative the sample is (if M is large).\nOther problems are the estimation of task difficulty and whether M is able to discriminate between a set of AI systems. Also, none of the benchmarks in AI is adaptive."}, {"heading": "2.4 Evaluation by peer confrontation", "text": "In the evaluation by peer confrontation, we evaluate a system by confronting it to another system. This usually means that a match is played between peers. This is usual for games (including game theory) and part of multi-agent research. The results of each match (possibly repeated with the same peer) may serve as an estimation of which of the two systems is best (and how much). Nonetheless, the main problem about this approach is that the results are relative to the opponents. This is natural in games, as people are said to be good or bad at chess, for instance, depending of whom they are compared with.\nDespite this relative character of the evaluation, we can still see the average performance according to equation 1. In order to do this, we must first identify the set of opponents \u2126. Then, the set of problems M is enriched (or even substituted) by the parametrisation of each single game (e.g., chess) with different competitors from \u2126. In 1-vs-1 matches we have that |M | = |\u2126| \u2212 1 (if we do not consdier a match between a system and itself). In other multi-agent situations where many agents play at the same time, |M | can grow combinatorially on |\u2126.\nNonetheless, for AI research, our main concern is about robustness and standardisation of results. For instance, how can we compare results between two different competitions if opponents are different? If these competitions are performed year after year, how can we compare progress? If there are common players, we can use rankings, such as the Elo ranking, to see whether there are progress. In fact, it would be very informative for AI competitions based on peer confrontation to keep all participants from previous editions in subsequent editions. However, this comes with a drawback, as systems could specialise to the kind of opponents that are expected in a competition (if a high percentage of competitors are inherited from previous editions, specialisation to those old (and bad) systems could be common). It is insightful to think how many of these problems are addressed in sport competitions. For instance, some tournaments adapt their matches according to previous information (by round, by ranking, etc.). In fact, a league may be redundant (for the same reasons why the information-driven or difficulty-driven sampling are introduced) and other tournament arrangements are more effective with almost the same robustness and much fewer matches.\nAs an alternative, games and multi-agent environments could be evaluated against standardised opponents. However, how can we choose a set of standardised opponents? If the opponents are known, the systems can be specialised to the opponents. For instance, in an English draughts (checkers) competition, we could have players being specialised to play against Chinook, the proven optimal player [110]. Again, this ends up again in the design of an opponent generator. This of course does not mean a random player (which is usually very bad), but players that can play well. One option is to use old systems where some parameters are changed. Alternatively, a more far-reading approach is to define an agent language and generate players (programs) with that language. As it is expected that this generation will not achieve very good players (otherwise we would be facing a very simple problem), a possible solution is to give more information and resources to these standardised opponents to make them more competitive (e.g., in some applications these opponents could have more sophisticated sensor mechanisms or some extra information about the match that regular players do not have).\nBe the set \u2126 composed of old opponents or generated opponents, we need to assess whether \u2126 is sufficiently challenging and it is able to discriminate the participants. For instance, some competitions in AI finally award\na champion, but there is the feeling that the result is mostly arbitrary and caused by luck, as happens with many sport competitions31. How can we assess whether the set \u2126 has sufficiently difficulty and discriminative power? This is of course a difficult problem, which has recently be analysed in [54]. For instance, Figure 4 shows the distribution of results of an agent competing in a multi-agent system according to the complexity of the agent complexity itself.\nTable 3 shows a sample of evaluation settings based on peer confrontation. Once again, because of obvious space constraints, we will just choose one representative and interesting case from the table. We will discuss the General Game Competition, which has been run yearly since 2005. According to the webpage32, \u201cgeneral game players are systems able to accept descriptions of arbitrary games at runtime and able to use such descriptions to play those games effectively without human intervention. In other words, they do not know the rules until the games start\u201d. Games are described in the language GDL (Game Description Language). The description of the game is given to the players. Different kinds of games are allowed, such as noughts and crosses (tic tac toe), chess, in static or dynamic worlds, with complete or partial information, with varying number of players, with simultaneous or alternating plays, etc. There are several rounds, qualifications, etc. For the competition, games are chosen \u2014non-randomly, i.e., manually by the organisers\u2014 from the pool of games already described in GDL and new games that can be introduced for the competition. As a result, game specialisation is difficult.\nDespite being one of the most interesting AI competitions, there is still some margin for improvement. For instance, a more sophisticated analysis of how difficult and representative each problem is would be useful. For instance, several properties about the adequacy of an environment or game for peer-confrontation evaluation can be identified and analysed depending on the population of opponents that is being considered\n31Statistical tests are not used to determine when a contestant can be said to be significantly better than another. 32http://games.stanford.edu/ 33http://www.robocup.org/ 34http://games.stanford.edu/ 35http://www.icga.org/ 36http://www.icga.org/ 37http://www.computerpokercompetition.org/ 38http://tradingagents.eecs.umich.edu/ 39http://theaigames.com/competitions/warlight-ai-challenge/rules\n[75]. Also, rankings (e.g., using the Elo system) could be calculated, and former participants could be kept for the following competitions, so there are more participants (and more overlap between competitions). A more radical change would be to learn without the description of the game, as a reinforcement learning problem (where the system learns the rules from many matches). An adaptation between the general game playing and RL-glue, which is used in the reinforcement learning competition, that could make this possible has been done in [8].\nSumming up our observations on peer confrontation problems, we see that the dependency on the set |\u2126| makes this kind of evaluation more problematic. Nonetheless, as AI research is becoming more socially oriented, with significantly more presence of multi-agent systems and game theory, an effort has to be done to make this kind of evaluation more systematic, instead of the plethora of arrangement that we see in sports, for instance.\nAs a summary of this whole section about task-oriented evaluation, we have identified many issues in many evaluation setting in AI. Nonetheless, the three types of evaluation settings have their niches of application and some of the ideas above can be used to make the evaluation more controlled, automated and robust."}, {"heading": "3 Towards ability-oriented evaluation", "text": "Many areas AI is successful nowadays took a long time to flourish in applications (e.g., driverless cars, machine translators, game bots, etc.). Most of them correspond to specific tasks and require task-oriented evaluation. Other tasks that are still not properly solved by AI technology are already evaluated in this way and will be successful one day. However, if instead of AI applications we think about AI systems, we see that there are some kinds of AI systems for which task-oriented evaluation is not appropriate. For instance, cognitive robots, artificial pets, assistants, avatars, smartbots, smart houses, etc., are not designed to cover one particular application but are expected to be customised by the user for a variety of tasks. In order to cover this wide range of (previously unseen) tasks, these systems must have some abilities such as reasoning skills, inductive learning abilities, verbal abilities, motion abilities, etc. Hence, this entails that apart from task-oriented evaluation methods we may also need ability-oriented evaluation techniques.\nThings are more conspicuous when we look at the evaluation of the progress of AI as a discipline. If we look at AI with Minsky\u2019s 1968 definition seen in the introduction, i.e., by achievemant of tasks that would require intelligence, AI has progressed very significantly. For instance, one way of evaluating AI progress is by looking at a task and check in which category an AI system is currently: optimal if no other system can perform better, strong super-human if it performs better than all humans, super-human if it performs better than most humans, par-human if it performs similarly to most humans, and sub-human if it performs worse than most humans [106]. Note that this approach does not imply that the task is necessarily evaluated with a human-discriminative approach. Having these categories in mind, we have seen how AI has scaled up for many tasks, even before AI had a name. For instance, calculation became super-human in the nineteenth century, cryptography in the 1940s, simple games such as noughts and crosses became optimal in 1960s, more complex games (draughts, bridges) a couple of decades later, printed (non-distorted) character recognition in the 1970s, statistical inference in the 1990s, chess in the 1990s, speech recognition in the 2000s, and TV Quizzes, driving a car, technical translation, texas hold \u2019em poker in the 2010s. According to this evolution, the progress of AI has been impressive.\nHowever, let us first realise that no system can do (or can learn to do) all of these things together. The big-switch approach may be useful for a few of them (e.g., a robot with an advanced computer vision system that detects whether it is facing a chess board or a bridge table and then switch to the appropriate program to play the game that it has just recognised). Second, if we look at AI with McCarthy\u2019s definition seen in the introduction, i.e., by making intelligent machines, things are less enthusiastic. Not only has the progress been more limited, but also there is a huge controversy for quantifying this progress (in fact, some argue that machines are more intelligent today than fifty years ago while others say that there has been no progress at all other than computational power). Hence, worse than having a poor progress or no progress at all, we regard with contempt that we do not have effective evaluation mechanisms to evaluate this progress. It seems that none of the evaluation settings seen in the previous section is able to evaluate whether the AI systems of today are more intelligent than the AI systems of yore. Also, for developmental robotics and other areas of AI where systems are supposed to their performance with time, we want to know if a 6-month-old robot has progressed over its initial state, in the same way that we see how abilities increase and crystallise with humans, from toddlers to adults. Ability-oriented evaluation, and not task-oriented evaluation, seems to have a better chance of answering this question.\nIt would be unfair to forget to acknowledge that some attempts seen in the previous section have made an effor to make AI evaluation more general. The general game competition seen in the previous section is one example of how some things are changing in evaluation. Researchers and users are becoming tired of a big-switch approach. They conceive for systems that are able to cover more and more general task classes. Nonetheless, it is still a limited generalisation, which is too based on a very specific range of tasks. Many good players at the general game competition will be helpless at any game of the Arcade Learning Environments, and vice versa. Actually, only some reinforcement learning (and perhaps genetic programming) systems can at least participate in (adaptations to) of both games \u2014excelling in them would not be possible though without an important degree of specialisation.\nIn the rest of this section we will introduce what an ability is and how they can be evaluated in AI. The title of this section (starting with \u2018Towards\u2019) suggests that what follows is more interdisciplinary and contains proposals that are not well consolidated yet, or that may even be going in the wrong direction. Nonetheless, let us be more lenient and have in mind that ability-based evaluation is much more challenging than task-specific evaluation."}, {"heading": "3.1 What is an ability?", "text": "We must first clarify that we are talking about cognitive abilities, in the same way that in the previous section we referred to cognitive tasks. Some AI applications require physical abilities, most especially in robotics, but AI deals with how the sensors and actuators are controlled, not about their strength, consumption, etc. After this clarification, we can define a cognitive ability as a property of individuals that allows them to perform well in a range of information-processing tasks. At first sight this definition may just look like a change of perspective (from problems to systems). However, what we see now is that the ability is required, and performance is worse without featuring the ability. In other words, the ability is necessary but it does not have to be sufficient (e.g., spatial abilities are necessary but not sufficient for driving a car). Also, the ability is assumed to be general, to cover a range of tasks.\nThe major issue about abilities is that they are \u2018properties\u2019, and as such they have to be conceptualised and identified. While tasks can be used as measuring instruments, abilities are constructs. Many different cognitive abilities or factors have been identified and have been arranged in different ways [111]. For instance, one well-known comprehensive theory of human cognitive abilities is the Cattell-Horn-Carroll theory [78]. Figure 5 shows a graphical representation of these abilities. The top level represents the g-factor or general intelligence, the middle level identifies a set of broad abilities and the bottom level may include many narrow abilities.\nInterestingly, this is not surprising from an AI standpoint. The broad abilities seem to correspond to subfields in AI. For instance, looking at any AI textbook (e.g. [108]), we can enumerate areas such as problem solving, use of knowledge, reasoning, learning, perception, natural language processing, etc., that would roughly correspond to some of the cognitive abilities in Figure 5.\nCan we evaluate broad abilities as we did for specific tasks? Application-specific (task-oriented) approaches will not do. But is ability-oriented evaluation ready for this? The answer, as we will see below, is\nthat this type of evaluation is still in a very incipient stage in AI. There are several reasons for this. First, general (ability-oriented) evaluation is more challenging. Second, we no longer have a clear definition of the task(s). In fact, defining the ability depends on a conceptualisation, and from there we need to find a set of representative exercises that require the ability. And third, there have not been too many general AI systems to date, so task-oriented evaluation has seemed sufficient for the evaluation of AI systems so far. However, things are changing as new kinds of AI systems (e.g., developmental robotics) are becoming more general.\nBefore starting with some approaches in the direction of ability-oriented evaluation, it can be argued that some existing evaluation settings in AI are already ability-oriented. For example, even if the planning competition features a set of tasks, it goes around the ability of planning, which is more general than any particular task. However, the systems are not able to determine when planning is required for a range of problems. In other words, the ability is not a resource of the system, but the very goal of the system. In the end, researchers incorporate planning modules in several application-specific systems."}, {"heading": "3.2 The anthropocentric approach: psychometrics", "text": "Psychometrics was developed by Binet, Spearman and many others at the end of the XIXth century and first half of the XXth century. An early concept that arose was the distinction between knowledge and abilities. For instance, an \u201cidiot savant\u201d could have a lot of knowledge or could have developed a sophisticated skill during the years for some specific domain, but could be obtuse for other problems. On the contrary a very able person with no previous knowledge could perform well in a range of tasks, provided they are culture-fair. This distinction took several decades to consolidate. In a way, this bears resemblance with the narrow vs. general dilemma in AI.\nPsychometrics is concerned about measuring cognitive abilities, personality traits and other psychological properties [122]. Factors differ from abilities, in principle, in that they are obtained through testing and further analysed through systematic approaches based on factor analysis. Some factors have been equated and named after existing abilities while others are \u2018discovered\u2019 and receive new technical names. Several indices can be derived from a battery of tests by aggregating abilities and factors. One joint index that is usually determined from some of these tests is known as IQ (Intelligence Quotient). Although IQ was originally normalised by the subject\u2019s age (hence its name), its value for adults today is normalised relative to an adult population, then normalised (mean \u00b5=100, standard deviation \u03c3=15).\nIQ tests incorporate items of variable difficulty. Item difficulty is relative to a population, i.e., determined by the percentage of subjects that are able to solve the item, using functional models as in Item Response Theory [92, 36], as seen in the previous section. Note that this difficulty assessment is relative to the population and not linked to the nature of the item itself.\nIQ tests are easy to administer, fast and accurate, and they are used by companies and governments, essential in education and pedagogy. IQ tests are generally culture-fair through the use of abstract exercises (except for the verbal comprehension abilities).\nAs they work reasonably well for humans, their use for evaluating machines has been suggested many\ntimes, even since the early days of AI. More recently, their use has been vindicated by Bringsjord and Schmimanski [13, 12], with the so-called \u2018Psychometric AI\u2019 (PAI), as \u201cthe field devoted to building informationprocessing entities capable of at least solid performance on all established, validated tests of intelligence and mental ability, a class of tests that includes not just the rather restrictive IQ tests, but also tests of artistic and literary creativity, mechanical ability, and so on\u201d. It is important to clarify that PAI is a redefinition or new roadmap for AI \u2014not an evaluation methodology\u2014 and does not further develop or adapt IQ tests for AI systems. In fact, it does not claim that IQ tests are the best way to evaluate AI systems.\nNot surprisingly, this claim that IQ tests are the best way to evaluate AI system has recently come from human intelligence research. Detterman, editor of the Intelligence Journal, wrote an editorial [24] where he suggested that Watson (the then recent winner of the Jeopardy! TV quiz [41]) should be evaluated with IQ tests. The challenge is very explicit: \u201cI, the editorial board of Intelligence, and members of the International Society for Intelligence Research will develop a unique battery of intelligence tests that would be administered to that computer and would result in an actual IQ score\u201d [24]. Detterman established two levels for the challenge, a first level, where the type of IQ tests can be seen beforehand by the AI system programmer, and a second level, where the types of tests would have not seen beforehand. Only computers passing the second level \u201ccould be said to be truly intelligent\u201d [24]. These two levels are related to the big-switch approach and the problem overfitting issue, which we have already mentioned in previous sections for AI evaluation settings. It is apposite at this point to recall that IQ tests and many other standardised psychological tests are never made public, because otherwise people could practise on them and game the evaluation. Note that the non-disclosure of the tests until evaluation time is something that we only find in very few evaluation settings in the previous section (especially the problem benchmarks).\nDetterman was unaware that almost a decade before, in 2003, Sanghi and Dowe [109] implemented a small program (less than 1000 lines of code) which could score relatively well on many IQ tests, as shown in Table 4. The program used a big-switch approach and was programmed to some specific kinds of IQ tests the programmers had seen beforehand, but the authors still made made the point unequivocally: this program is not intelligent and can pass IQ tests.\nWhile it must be conceded that the results only reach the first level of Detterman\u2019s challenge \u2014so there is a test administration issue (i.e., an evaluation flaw)\u2014, there are some weaknesses about human IQ tests that would also arise if a system passed the second level as well. In particular, \u201cthe editorial board of Intelligence, and members of the International Society for Intelligence Research\u201d could be tempted to devise or choose those IQ tests that are more \u2018machine-unfriendly\u2019. If AI systems eventually passed some of them, the battery could be refined again and again, in a similar way as how CAPTCHAs are updated when they become obsolete. In other words, this selection (or battery) of IQ tests would need to be changed and made more elaborate year after year as AI technology advances. Also, the limitations of this approach if AI systems ever become more intelligent than humans are notorious.\nThe main problem about IQ tests is that they are anthropocentric, i.e., they have been devised for humans and take many things for granted. On top of that, they are specialised to the average human. For instance, tests are significantly different when evaluating small children, people with disabilities, etc. Also, the relation between items and abilities have been studied during the past century exclusively using humans, so it is not clear that a set of items would measure the same ability for a human or for a machine. For a more complete discussion about why IQ tests are not ready for AI evaluation, the reader is referred to a response [30] to Detterman\u2019s editorial. Having said all this and despite the limitations of IQ tests for AI evaluation, their use is becoming more popular in the past decade and systems whose results are like those of Table 4 are becoming common. (for a survey, see [63]).\nAs just said, one of the problems of IQ tests is that they are specialised for humans. In fact, standardise adult IQ tests do not work with people with disabilities or children of different ages. In a similar way, we do not expect animals to behave well on a standard human IQ test, starting from the fact that they will not be able to read the text. This leads us to the consideration of how cognitive abilities are evaluated in animals. Comparative psychology and comparative cognition [115, 116] are the main disciplines that perform this evaluation. For a time, much research about cognitive abilities in animals was performed on apes. The term \u2018chimpocentric\u2019 was introduced as a criticism about tests that had gone from being anthropocentric to being chimpocentric. Nonetheless, in the past decades, any species may be a subject of study for comparative psychology: mammals (apes, cetaceans, dogs and mice), birds and some cephalopods. The evaluation focus on \u201cbasic processes\u201d, such as perception, attention, memory, associative learning and the discrimination of concepts, and recently on more sophisticated instrumental or social abilities [116].\nOne of the most distinctive features of animal evaluation is the use of rewards, as instructions cannot be used. This setting is very similar to the way reinforcement learning works. Animal evaluation has also brought attention to the relevance of the interface. Clearly, the same test may require very different interfaces for a dolphin and a bonobo.\nHuman evaluation and animal evaluation have become more integrated in the past years, and testing procedures half way between psychometrics and comparative cognition are becoming more usual. For instance, several kinds of skills are evaluated in human children and apes in [65]. In recent years, many abilities that were considered exclusively human have been found to some extent in many animals.\nDoes the enlargement from humans to the whole animal kingdom suggest that these tests for animals can be used for machines? While the lowest ranges of the studied abilities and the use of rewards can facilitate its application to AI system significantly, we still have many issues about whether they can be applied to machines at least directly. First, the selection of tasks and abilities is not systematic. Second, many of the tasks that are applied to animals would be too easy for machines (e.g., memory). And third, others would be difficult (e.g., orientation, recognition, interaction). Nonetheless, there is an increasing need for the evaluation of animats [142] and the evaluation procedures for animals are the first candidates to try."}, {"heading": "3.3 Evaluation using AIT", "text": "A radically different approach to AI evaluation started in the late 1990s. If intelligence was viewed as a \u201ckind of information processing\u201d [17] then it seemed reasonable to look at information theory for an \u201cessential nature or formal basis of intelligence and the proper theoretical framework for it\u201d [17]. This was finally done with algorithmic information theory (AIT), and the related notions of Solomonoff universal probability [120], Kolmogorov complexity [89] and Wallace\u2019s Minimum Message Length (MML) [133, 134].\nThere are several good properties about algorithmic information theory for evaluation. First, several definitions of information and complexity can be defined exclusively in computational terms, actually relative to a Universal Turing Machine (UTM), a fundamental and universal model of effective computation. For instance, the Kolmogorov complexity of an object (expressed as a binary string) relative to a UTM is defined as the shortest program (for that machine) that describes/outputs the object. Even if these definitions depend on the UTM that is used, the invariance theorem states that their values will only differ up to a constant for two different UTM (because one can emulate the other) [89]. Algorithmic probability allows the definition of a universal distribution for each UTM, which is just defined as the probability of objects as outputs of a UTM fed by a fair coin. While in general, this means that compressible strings are more likely than uncompressible ones, it can be shown that every computable probability distribution can be approximated by a universal distribution. There are reasons to think that many phenomena and, as a result,\nmany of the problems that we face everyday, follow a universal distribution. This is directly linked to the traditional discussion about the probability p to choose for equation 1. In a way, Solomonoff, the father of algorithmic probability [120] gave a theoretical backing to Occam\u2019s razor. Also, we have the relevant fact, which is very significant as well for evaluation as well, that all universal distributions are immune to the no-free-lunch theorems. And finally, Kolmogorov complexity and algorithmic probability are two sides of the same coin, which led to a formal connection (if not identification) of compression and inductive inference. It has been acknowledged that Solomonoff \u201csolved the problem of induction\u201d [119, 27]. Of course, not everything in AIT is straightforward. For instance, some of these concepts lead to incomputable functions, although approximations exist, such as Kolmogorov complexity, which can be approximated by Levin\u2019s Kt [87].\nThe application of AIT to (artificial) intelligence evaluation started with a variant of the Turing test that featured compression problems [28, 29] to make the test more sufficient. While one of the goals of this work was to criticise Searle\u2019s Chinese room [113] (an argument that has faded with time), this is one of the first intelligence test proposals using AIT. At roughly the same time, the C-test was introduced by [64, 49], where tasks are formally derived and their difficulty can be explicitly quantified. Everything is derived from formal concepts in AIT. Figure 6 shows examples of sequences that appear in this test. They clearly resemble some exercises found in IQ tests. The major differences are that sequences are obtained by a generator (a UTM with some post-conditions about the generated sequence) and the fact that each sequence is accompanied by a theoretical assessment of difficulty (the size of the shortest program that generates the problem instance). Note the implications for evaluation of such a test, as exercises are derived from first principles (instead of being contrived by psychometricians) and the difficulty of these exercises is intrinsic, and not based on how difficult humans find them. Finally, these sequences were used to define a test and the results of the exercises were aggregated in a way that highly resembles our recurrent equation 1, where M is formally defined and the probability covers a range of difficulties, as in Figure 1 (right).\nSome preliminary experimental results showed that human performance correlated with the absolute difficulty (k) of each exercise and also with IQ test results for the same subjects. This suggests that this approach could be used for IQ-test re-engineering. Some extensions of the C-test were suggested (\u201crewards and penalties could be used instead\u201d [50] and several variants for other abilities [51]. Nonetheless, this line of research was sharply restrained in 2003 by the evidence that very simple programs could pass IQ tests [109], as we have seen in section 3.2 (see Table 4).\nNonetheless, the above ideas were extended from static tests to dynamic tests, with agents being evaluated in an environment with actions, observations and rewards. Dobrev [25, 26] introduced a formal definition of intelligence as performance in a range of worlds. Worlds were defined as interactive environments with a measure of success (R in our notation). The set of possible worlds M was described by Turing machines, and M was chosen as a finite set, bounded by their Kolmogorov complexity. In other words, only compressible environments (up to a limit) were used in the definition. From there, intelligence was measured as an average, using a uniform distribution p over M , so it follows again the general average-case evaluation (equation 1). Dobrev also suggests how agents should achieve high intelligence (using Levin search). Similar ideas are introduced by [86], but with a better formalisation. Both approaches can be seen as interactive extensions of the C-tests, from sequences to environments. While the idea of evaluating intelligence as performance in a (wide) range of worlds is appealing and the most general instance of equation 1, the above proposals present several problems. First, M or the probability distribution (or both) are not computable, so approximations need to be used. Also, most environments are not really discriminative, and all agents will score the same, will just \u2018die\u2019 or be stuck after a few steps (Dobrev discusses this issue briefly, with the notion of a \u201cworld without fatal errors\u201d). Also, the number of steps in each world needs to be regulated. Finally, time (or speed) is not considered for the environment or for the agent. For more detail about these issues and some possible solutions, the reader is referred to [66] [55, secs. 3.3 and 4].\nThe information-theoretic approach is not isolated from some of the approaches seen so far in section 2, some hybridisations and integrated approaches have been proposed [58, 32, 62, 61, 72] (apart from the compresson-enriched Turing Tests ([28, 29] already mentioned above)."}, {"heading": "3.4 Universal psychometrics", "text": "Figure 7 shows the fragmentation of the approaches seen in previous sections. As we see, this fragmentation is originated by what kind of measurement we are interested in but most especially by the kind of subject that is being measured. In [55], the notion of \u2018universal test\u2019 is introduced, as a test that is applicable to \u201cany biological or artificial system that exists at this time or in the future\u201d: human, non-human animal, machine, hybrid or collective. The stakes were set high, as the tests should work without knowledge about the subject, derived from computational principles, non-biased (species, culture, language, ...), no human intervention, producing a meaning score, practical and anytime (the more time we have for the test the higher the reliability of the score). Note that in order to apply the same test to several subjects we are allowed to customise the interface, provided the features and difficulty of the items are remained unaltered. Also, we need to think about the speed of the subject, and adapt to it accordingly. Also, the abilities of the subject can be quite varied, so the ranges of difficult need to adapt to the agent. That suggests that universal tests must necessarily be adaptive.\nA first framework for universal, anytime intelligence tests is introduced in [55], where a class of environment is carefully chosen to be discriminative. The test starts with very simple environments and adapts to the subject\u2019s performance and speed. This was developed upon some of the ideas about using AIT for intelligence evaluation, as seen in section 3.3. Some experiments were performed [74, 73, 76], using the environment class defined in [52]. Complexity was estimated using a variant of Levin\u2019s Kt. The same test compared Q-learning [136] with humans. Two different interfaces were designed on purpose. The experimental settings featured many limitations (simplifications, non-adaptiveness, absence of noise, low-complexity patterns, no incrementality, no social behaviour, etc.) and, probably because of this, the results did not show the actual difference between Q-learning and humans. Despite the limited results, the experiment had quite a repercussion [81, 9, 57, 146]. Nonetheless, the tests were a first effort towards a universal test and highlighted some of the challenges.\nAbout the lack of richness of interaction and social behaviours, it is clear that an environment that is randomly generated will have an extremely low probability of showing some social behaviour, which is a distinctive trait of human intelligence. This has suggested other ways of generating the environments and how to incorporate other agents into them (e.g., the Darwin-Wallace distribution [58], but it is still a puzzle how to adapt these ideas to the measurement of social intelligence and multi-agent systems [32, 72, 54, 75].\nThe fragmentation of Figure 7 and the need of solving many of the above issues has suggested the introduction of a new perspective, dubbed \u2018universal psychometrics\u2019 [59]. This comprehensive view is born with many hurdles ahead. Evaluation is always harder the less we know about the subject. The less we take for granted about the subjects the more difficult it is to construct a test for them. For instance, human intelligence evaluation (psychometrics) works because it is highly specialised for humans. Similarly, animal testing works (relatively well) because tests are designed in a very specific way to each species. And some of the AI evaluation settings we have already seen work because they are specialised for some kind of AI systems that are designed for some specific applications. In the case of AI, who would try to tackle a more general problem (evaluating any system) instead of the actual problem (evaluating machines)?\nThe answer to this question is that the actual problem for AI is the universal problem. Notions such as \u2018animat\u2019 [142], machine-enhanced humans [20], human-enhanced machines [130], other kind of hybrids and, most especially, collective [105] of any of the former, suggest that the distinction between animals, humans and machines is not only inappropriate, but no longer useful to advance in the evaluation of cognitive abilities. The notion of \u2018machine kingdom\u2019, as illustrated in Figure 8, is not very surprising to the current scientific paradigm but clarifies which class of subjects is most comprehensive.\nUniversal psychometrics attempts to integrate and standardise a series of concepts. A subject is seen as a physically computable (resource-bounded) interactive system. Cognitive tasks are seen as physically computable interactive systems with a score function. Interfaces are defined between subjects and tasks (observations-outputs, actions-inputs). Cognitive abilities are seen as properties over set of cognitive tasks (or task classes). As a result, the separation between task-specific and ability-specific becomes a progressive thing, depending on the generality of the class. Distributions are defined over task classes and performance as average case performance on a task class (again, a generalised version of equation 1). Difficulty functions are computationally defined from each task. Overall, some of these elements found in psychometrics, comparative cognition and AI evaluation are overhauled here with the theory of computation and AIT. As a result, cognitive abilities are no longer what the cognitive tests measure, as in human psychometrics (adapting the (in)famous statement that intelligence is \u201cwhat intelligence test measures\u201d [11]), but they are properties that emanate from (general) classes of tasks, perfectly defined in computational terms. As a consequence, the relation between abilities can be explored experimentally, but also theoretically, and measures are absolute and not relativised wrt. a population (except for social abilities). This could imply some revitalisation of the white-box approach, especially for those AI systems that can be formally described in a theoretical way (e.g., some results in [71] and [66] take a white-box evaluation approach).\nThis view of a cognitive ability is consistent with its association with a \u201cclass of cognitive tasks\u201d [16] that must be \u2018representative\u2019 for the ability. From the association between abilities and classes of tasks, \u201cwe see that by merging two cognitive task classes we get a more general cognitive task class, and a more general ability. Typically, this is studied in a hierarchical way, starting with the so-called elementary cognitive tasks [16, page 11] (closely related to the notion of primary mental abilities of [126])\u201d [59]. This redraws our dilemma between task-oriented and ability-oriented into a gradual hierarchy from specific tasks to general\nabilities, with general intelligence at the very top (including all possible cognitive tasks, i.e., all interactive Turing machines with a score function). The questions about how to sample from a task class for an effective evaluation can be generalised from our discussion in section 2.\nThis sets a dual view of cognitive tasks on one hand and cognitive systems on the other hand, where both spaces (the ability space and the machine kingdom) can be explored. Both cognitive tasks and cognitive systems are defined as interactive systems. One singularity of cognitive systems, however, is that they can evolve with time, and their abilities can change. In other words, it seems that some abilities need to be constructed on top of other abilities, and this seem to be independent of the subject to some extent, in the same way that it seems difficult to be able to multiply without being able to add. A theoretical analysis of ability interdependency, how they can develop and the notion of potential intelligence, are still in a very incipient stage [70, 56].\nThere can be objections and disagreements about how many of the above concepts are understood and defined. There can also be objections about how universal a test can be [31]. But a more integrated view of cognitive abilities for humans, animals, robots, agents, animats, hybrids, swarms, etc., is not only possible but useful. Bear in mind that universal psychometrics does not exclude the use of non-universal tests, as tests that are non-universal can be more efficient (tests can be universal or not, depending on the application), but aims at having a more integrated and well-founded view of how intelligent systems and beings are evaluated."}, {"heading": "4 Conclusions", "text": "We started this paper looking at the way AI evaluation is commonly performed, through task-oriented evaluation, mostly with a black-box approach. We identified several problems and limitations, and we noticed that there is still a huge margin of improvement in the way AI systems are evaluated. The key issues are M and p, and distinguishing the definition of the problem class from an effective sampling procedure (testing procedure). Then we switched to ability-oriented evaluation, a much more immature approach, but that may have a more relevant role in the future. The notion and evaluation of ability is more elusive than the notion of task. We have argued that this requires the integration of several perspectives that are currently scattered efforts in AI, psychometrics, AIT and comparative cognition. The different areas, philosophies, tools, foundations, terminologies and the different kinds of subjects to be evaluated can be unified with an integrated perspective known as universal psychometrics. Here, the exploration of the machine kingdom is dual to the exploration of the set of possible cognitive abilities/tasks. In both spaces we aim at becoming more general, which is where evaluation is more challenging (see Figure 9). This resembles the duality in the theory of computation (e.g., problem classes and automata classes). The more formal approach advocated by universal psychometrics can make the white-box evaluation approach recover more relevance in AI.\nFrom the problems and limitations found in AI evaluation and the tools and ideas that have appeared along the paper, we enumerate a number of generic guidelines. These can be considered when an AI evaluation\nsetting is under consideration.\n\u2022 The definition of \u2126, the set of possible systems that can be evaluated (or that can be confronted with in peer confrontation evaluation), must be clarified from the beginning. Information about their proficiency and expected characteristics may be very useful. If humans are considered, the way in which will be admitted and how they will be instructed will be defined. The more general \u2126 is the less we can assume about the evaluation process. If the systems are varied, different interfaces must be considered.\n\u2022 The definition of M , the set of possible tasks, and its associated distribution p configure what we are measuring. This can be built from a set of problems or using a generator. This pair \u27e8M,p\u27e9 has to be representative of a task (in task-oriented evaluation) or an ability (in ability-oriented evaluation). If it is a peer confrontation evaluation, M will be enlarged with as many combinations of game/environment with agents in \u2126 are possible. The distribution p will be udpated accordingly.\n\u2022 The definition of R and its aggregation \u03a6 must ensure that the values R(\u00b5) for all \u00b5 \u2208 M are going to be commensurate and that the aggregation is bounded (calculating or estimating the best possible and the worst possible values would show the limits). An analysis about expected measurement error is useful at this point. How robust R depending of the length or time left for each episode will indicate whether repetitions are needed to reduce the measurement error given by R(\u00b5).\n\u2022 As much as possible, the similarity between tasks or a set of features describing them should be identified. An intrinsic difficulty function (even if approximate) is always very useful. Showing the distribution of difficulty for M can be highly informative. If difficulty is available, item response curves could be plotted (or prepared).\n\u2022 The sampling method must be as much efficient as possible, by using, e.g., a clustering sampling or a range of difficulties if we have a non-adaptive problem-benchmarks evaluation. For the peerconfrontation evaluation, the arrangement of matches can be designed beforehand if the evaluation is not adaptive. Similarly, the procedure for an adaptive evaluation must also be carefully designed to ensure measurement robustness. Simulations can be useful to estimate this.\n\u2022 Information about how the evaluation is performed (including R, \u03a6 and some illustrative problems) can be disclosed to the systems that are being evaluated (or to their designers). However, \u2126, M and p should not be disclosed. If possible, the problems should not be disclosed after the evaluation either, as keeping them secret allows the comparison with the same problems for different subjects or at different times (e.g., we can evaluate progress of a system or a discipline during a period).\n\u2022 After the evaluation, results must be analysed beyond the mere calculation of the aggregated results. Item response functions and agent response functions [59] can be constructed empirically from the results and compared with the theoretical functions or any other information about \u2126 and M . Discrepancies or anomalies may suggest that the evaluation setting has to be revised.\nIt is of course an open question to what extent the above recommendations will be followed on a regular basis for AI evaluation. It can be argued whether AI evaluation has been a priority for AI in the past, but it seems that it has not been recognised as an imperative problem or a mainstream area of research. If this is the case, this paper can help change this perspective. Anyhow, the question of AI evaluation remains and there is space for significant improvement, even for the most specific sets \u2126 and M (bottom-left part of Figure 9). At the other end, measuring intelligence and doing it universally is a key ingredient for understanding what intelligence is (and, of course, to devise intelligent artefacts). AI evaluation is no longer limited to task-specific evaluation of AI systems or to evaluating progress in AI. A more scientific theory of AI evaluation is being required for many applications (CAPTCHAs, social networks, agent certification, etc.) and it will be more and more common in a future with a plethora of bots, robots, artificial agents, avatars, control systems, \u2018animats\u2019, hybrids, collectives, etc. It is also crucial for the technological singularity once (and if) achieved [34], especially because some of the prophecies and forecasts disregard that the first thing to consider about the singularity is to have metrics to detect whether and where AI progresses towards it.\nSumming up, AI requires an accurate, effective, non-anthropocentric, meaningful and computational way of evaluating its progress, by evaluating its artefacts. This paper can serve as a comprehensive source of the state of the art of the AI evaluation, its challenges and the avenues for future work."}, {"heading": "Acknowledgements", "text": "This work was supported by the EU (FEDER) and the Spanish MINECO projects CONSOLIDER-INGENIO CSD2007-00022, TIN 2010-21062-C02-02, and TIN 2013-45732-C4-1-P, by Generalitat Valenciana projects Prometeo/2008/051 and PROMETEO/2011/052, and the REFRAME project granted by the European Coordinated Research on Long-term Challenges in Information and Communication Sciences & Technologies ERA-Net (CHISTERA), and funded by the respective national research councils and ministries (Ministerio de Econom\u0131\u0301a y Competitividad, with code PCIN-2013-037, in Spain).\nI thank the organisers of the Summer School of the Spanish Association for Artificial Intelligence, in A Corun\u0303a, Spain, held in September 2014, for giving me the opportunity to give a lecture on \u2018AI Evaluation\u2019. This paper evolved in parallel with that lecture.\nFigure 5 is courtesy of Fernando Mart\u0301\u0131nez-Plumed."}], "references": [{"title": "Keel data-mining software tool: Data set repository, integration of algorithms and experimental analysis framework", "author": ["J. Alcal\u00e1", "A. Fern\u00e1ndez", "J. Luengo", "J. Derrac", "S. Gar\u0107\u0131a", "L. S\u00e1nchez", "F. Herrera"], "venue": "Journal of Multiple-Valued Logic and Soft Computing, 17:255\u2013287,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Syntax-guided synthesis", "author": ["R. Alur", "R. Bodik", "G. Juniwal", "M.M.K. Martin", "M. Raghothaman", "S.A. Seshia", "R. Singh", "A. Solar-Lezama", "E. Torlak", "A. Udupa"], "venue": "Formal Methods in Computer-Aided Design (FMCAD), 2013, pages 1\u201317. IEEE,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep machine learning-a new frontier in artificial intelligence research [research frontier", "author": ["I. Arel", "D.C. Rose", "T.P. Karnowski"], "venue": "Computational Intelligence Magazine, IEEE, 5(4):13\u201318,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Cognitive developmental robotics: a survey", "author": ["M. Asada", "K. Hosoda", "Y. Kuniyoshi", "H. Ishiguro", "T. Inui", "Y. Yoshikawa", "M. Ogino", "C. Yoshida"], "venue": "Autonomous Mental Development, IEEE Transactions on, 1(1):12\u201334,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The process of research investigations in artificial intelligence-a unified view", "author": ["D. Baldwin", "S.B. Yadav"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on, 25(5):852\u2013861,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279, 06", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Integration of general game playing with rl-glue", "author": ["J.L. Benacloch-Ayuso"], "venue": "Technical report, DSIC, Universitat Polit\u00e8cnica de Val\u00e8ncia,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Ultimate IQ: one test to rule them all", "author": ["C. Biever"], "venue": "New Scientist, 211(2829,10 September 2011):42\u201345,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical implementation of a graphics turing test", "author": ["M. Borg", "S.S. Johansen", "D.L. Thomsen", "M. Kraus"], "venue": "Advances in Visual Computing, pages 305\u2013313. Springer,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Intelligence as the tests test it", "author": ["E.G. Boring"], "venue": "New Republic, pages 35\u201337,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1923}, {"title": "Psychometric artificial intelligence", "author": ["S. Bringsjord"], "venue": "Journal of Experimental & Theoretical Artificial Intelligence, 23(3):271\u2013277,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "What is artificial intelligence? Psychometric AI as an answer", "author": ["S. Bringsjord", "B. Schimanski"], "venue": "International Joint Conference on Artificial Intelligence, pages 887\u2013893,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Artificial intelligence as an experimental science", "author": ["B.G. Buchanan"], "venue": "Springer,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1988}, {"title": "Deep Blue", "author": ["M. Campbell", "A.J. Hoane", "F. Hsu"], "venue": "Artificial Intelligence, 134(1-2):57 \u2013 83,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Human cognitive abilities: A survey of factor-analytic studies", "author": ["J.B. Carroll"], "venue": "Cambridge University Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "What kind of information processing is intelligence? In The foundation of artificial intelligence\u2014a sourcebook, pages 14\u201346", "author": ["B Chandrasekaran"], "venue": "Cambridge University Press,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Sampling techniques", "author": ["W.G. Cochran"], "venue": "John Wiley & Sons,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "How evaluation guides ai research: The message still counts more than the medium", "author": ["P.R. Cohen", "A.E. Howe"], "venue": "AI Magazine, 9(4):35,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "Testing and cognitive enhancement", "author": ["Y. Cohen"], "venue": "Technical report, National Institute for Testing and Evaluation, Jerusalem, Israel,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "The significance of evaluation in ai and law: a case study re-examining icail proceedings", "author": ["J.G. Conrad", "J. Zeleznikow"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Law, pages 186\u2013191. ACM,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluating research in cooperative distributed problem solving", "author": ["K.S. Decker", "E.H. Durfee", "V.R. Lesser"], "venue": "Distributed Artificial Intelligence, 2:487\u2013519,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1989}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "The Journal of Machine Learning Research, 7:1\u201330,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "A challenge to Watson", "author": ["D.K. Detterman"], "venue": "Intelligence, 39(2-3):77 \u2013 78,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "AI - What is this? A definition of artificial intelligence", "author": ["D. Dobrev"], "venue": "PC Magazine Bulgaria (in Bulgarian, English version at http://www.dobrev.com/AI),", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "Formal definition of artificial intelligence", "author": ["D. Dobrev"], "venue": "International Journal of Information Theories and Applications, 12(3):277\u2013285,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Introduction to Ray Solomonoff 85th memorial conference", "author": ["D. L Dowe"], "venue": "D. L. Dowe, editor, Algorithmic Probability and Friends. Bayesian Prediction and Artificial Intelligence, volume 7070 of Lecture Notes in Computer Science, pages 1\u201336. Springer Berlin Heidelberg,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "A computational extension to the Turing Test", "author": ["D.L. Dowe", "A.R. Hajek"], "venue": "Proceedings of the 4th Conference of the Australasian Cognitive Science Society, University of Newcastle, NSW, Australia,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "A non-behavioural, computational extension to the Turing Test", "author": ["D.L. Dowe", "A.R. Hajek"], "venue": "Intl. Conf. on Computational Intelligence & multimedia applications (ICCIMA\u201998), Gippsland, Australia, pages 101\u2013106,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "IQ tests are not for machines, yet", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "Intelligence, 40(2):77\u201381,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "How universal can an intelligence test be", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "Adaptive Behavior,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Compression and intelligence: social environments and communication", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo", "P.K. Das"], "venue": "J. Schmidhuber, K.R. Th\u00f3risson, and M. Looks, editors, Artificial General Intelligence, volume 6830, pages 204\u2013211. LNAI series, Springer,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Warning: statistical benchmarking is addictive", "author": ["C. Drummond", "N. Japkowicz"], "venue": "kicking the habit in machine learning. Journal of Experimental & Theoretical Artificial Intelligence, 22(1):67\u201380,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Singularity hypotheses: A scientific and philosophical assessment", "author": ["A.H. Eden", "J.H. Moor", "J.H. Soraker", "E. Steinhart"], "venue": "Springer,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "The rating of chessplayers, past and present, volume 3", "author": ["A.E. Elo"], "venue": "Batsford London,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1978}, {"title": "Item response theory for psychologists", "author": ["S.E. Embretson", "S.P. Reise"], "venue": "L. Erlbaum,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2000}, {"title": "On method overfitting", "author": ["E. Falkenauer"], "venue": "Journal of Heuristics, 4(3):281\u2013287,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1998}, {"title": "Difficulty, discrimination, and information indices in the linear factor analysis model for continuous item responses", "author": ["P.J. Ferrando"], "venue": "Applied Psychological Measurement, 33(1):9\u201324,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Assessing the discriminating power of item and test scores in the linear factor-analysis model", "author": ["P.J. Ferrando"], "venue": "Psicol\u00f3gica, 33:111\u2013139,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "An experimental comparison of performance measures for classification", "author": ["C. Ferri", "J. Hern\u00e1ndez-Orallo", "R Modroiu"], "venue": "Pattern Recognition Letters, 30(1):27\u201338,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Building Watson: An overview of the DeepQA project", "author": ["D. Ferrucci", "E. Brown", "J. Chu-Carroll", "J. Fan", "D. Gondek", "A.A. Kalyanpur", "A. Lally", "J.W. Murdock", "E. Nyberg", "J. Prager"], "venue": "AI Magazine,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "Evaluation of expert systems: Issues and case studies", "author": ["J. Gaschnig", "P. Klahr", "H. Pople", "E. Shortliffe", "A. Terry"], "venue": "Building expert systems, 1:241\u2013278,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1983}, {"title": "Verification & validation", "author": ["J. R Geissman", "R.D. Schultz"], "venue": "AI Expert, 3(2):26\u201333,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1988}, {"title": "General game playing: Overview of the AAAI competition", "author": ["M. Genesereth", "N. Love", "B. Pell"], "venue": "AI Magazine, 26(2):62,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2005}, {"title": "Special issue on worst-case versus average-case complexity \u2013 editors\u2019 foreword", "author": ["O. Goldreich", "S. Vadhan"], "venue": "computational complexity, 16(4):325\u2013330,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2007}, {"title": "Inductive programming meets the real world", "author": ["S. Gulwani", "J. Hern\u00e1ndez-Orallo", "E. Kitzelmann", "S.H. Muggleton", "U. Schmid", "B. Zorn"], "venue": "Submitted,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Approaches and applications of inductive programming (dagstuhl seminar 13502)", "author": ["S. Gulwani", "E. Kitzelmann", "U. Schmid"], "venue": "Dagstuhl Reports, 3(12),", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Beyond the Turing Test", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "J. Logic, Language & Information, 9(4):447\u2013466,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2000}, {"title": "On the computational measurement of intelligence factors", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "A. Meystel, editor, Performance metrics for intelligent systems workshop, pages 1\u20138. National Institute of Standards and Technology, Gaithersburg, MD, U.S.A.,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2000}, {"title": "Thesis: Computational measures of information gain and reinforcement in inference processes", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "AI Communications, 13(1):49\u201350,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2000}, {"title": "A (hopefully) non-biased universal environment class for measuring intelligence of biological and artificial systems", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "M. Hutter et al., editor, Artificial General Intelligence, 3rd Intl Conf, pages 182\u2013183. Atlantis Press, Extended report at http://users.dsic.upv.es/proy/anynt/unbiased.pdf,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep knowledge: Inductive programming as an answer", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "Approaches and Applications of Inductive Programming (Dagstuhl Seminar 13502), Gulwani, S. and Kitzelmann, E. and Schmid, U. (eds.),", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "On environment difficulty and discriminating power", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "Autonomous Agents and Multi-Agent Systems, pages 1\u201353,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Measuring universal intelligence: Towards an anytime intelligence test", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Artificial Intelligence, 174(18):1508 \u2013 1539,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "On potential cognitive abilities in the machine kingdom", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Minds and Machines, 23:179\u2013210,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "Mammals, machines and mind games", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Who\u2019s the smartest?  The Conversation, http: // theconversation. edu. au/ articles/ mammals-machines-and-mind-games-whos-the-smartest-1125 , April", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2011}, {"title": "On more realistic environment distributions for defining, evaluating and developing intelligence", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Insa-Cabrera"], "venue": "J. Schmidhuber, K.R. Th\u00f3risson, and M. Looks, editors, Artificial General Intelligence, volume 6830, pages 82\u201391. LNAI, Springer,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2011}, {"title": "Universal psychometrics: Measuring cognitive abilities in the machine kingdom", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "M.V. Hern\u00e1ndez-Lloreda"], "venue": "Cognitive Systems Research, 27:5074,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "A unified view of performance metrics: Translating threshold choice into expected classification loss", "author": ["J. Hern\u00e1ndez-Orallo", "P. Flach", "C. Ferri"], "venue": "The Journal of Machine Learning Research, 13(1):2813\u20132869,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2012}, {"title": "Turing Tests with Turing machines", "author": ["J. Hern\u00e1ndez-Orallo", "J. Insa", "D.L. Dowe", "B. Hibbard"], "venue": "Andrei Voronkov, editor, Turing-100, volume 10, pages 140\u2013156. EPiC Series,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2012}, {"title": "Turing machines and recursive turing tests", "author": ["J. Hern\u00e1ndez-Orallo", "J. Insa-Cabrera", "D.L. Dowe", "B. Hibbard"], "venue": "V. Muller and A. Ayesh, editors, AISB/IACAP 2012 Symposium \u201cRevisiting Turing and his Test\u201d, pages 28\u201333. The Society for the Study of Artificial Intelligence and the Simulation of Behaviour,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2012}, {"title": "Computer models solving human intelligence test problems: progress and implications", "author": ["J. Hern\u00e1ndez-Orallo", "F. Mart\u0301\u0131nez-Plumed", "U. Schmid", "M. Siebers", "D.L. Dowe"], "venue": null, "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2014}, {"title": "A formal definition of intelligence based on an intensional variant of Kolmogorov complexity", "author": ["J. Hern\u00e1ndez-Orallo", "N. Minaya-Collado"], "venue": "Proc. Intl Symposium of Engineering of Intelligent Systems (EIS\u201998), pages 146\u2013163. ICSC Press,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1998}, {"title": "Humans have evolved specialized skills of social cognition: The cultural intelligence hypothesis", "author": ["E. Herrmann", "J. Call", "M.V. Hern\u00e1ndez-Lloreda", "B. Hare", "M. Tomasello"], "venue": "Science, Vol 317(5843):1360\u20131366,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2007}, {"title": "Bias and no free lunch in formal measures of intelligence", "author": ["B. Hibbard"], "venue": "Journal of Artificial General Intelligence, 1(1):54\u201361,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2009}, {"title": "A new design for a turing test for bots", "author": ["P. Hingston"], "venue": "Computational Intelligence and Games (CIG), 2010 IEEE Symposium on, pages 345\u2013350. IEEE,", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2010}, {"title": "Believable Bots: Can Computers Play Like People", "author": ["P. Hingston"], "venue": null, "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2012}, {"title": "Complexity measures of supervised classification problems", "author": ["T.K. Ho", "M. Basu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(3):289\u2013300,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2002}, {"title": "The fastest and shortest algorithm for all well-defined problems", "author": ["M. Hutter"], "venue": "International Journal of Foundations of Computer Science, 13:431\u2013443,", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2002}, {"title": "Universal algorithmic intelligence: A mathematical top\u2192down approach", "author": ["M. Hutter"], "venue": "B. Goertzel and C. Pennachin, editors, Artificial General Intelligence, Cognitive Technologies, pages 227\u2013290. Springer, Berlin,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2007}, {"title": "On measuring social intelligence: Experiments on competition and cooperation", "author": ["J. Insa-Cabrera", "J.L. Benacloch-Ayuso", "J. Hern\u00e1ndez-Orallo"], "venue": "J. Bach, B. Goertzel, and M. Ikl\u00e9, editors, AGI, volume 7716 of Lecture Notes in Computer Science, pages 126\u2013135. Springer,", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2012}, {"title": "Comparing humans and AI agents", "author": ["J. Insa-Cabrera", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Hern\u00e1ndez-Orallo"], "venue": "J. Schmidhuber, K.R. Th\u00f3risson, and M. Looks, editors, Artificial General Intelligence, volume 6830, pages 122\u2013132. LNAI, Springer,", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2011}, {"title": "Evaluating a reinforcement learning algorithm with a general intelligence test", "author": ["J. Insa-Cabrera", "D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "J.A. Moreno J.A. Lozano, J.A. Gamez, editor, Current Topics in Artificial Intelligence. CAEPIA 2011. LNAI Series 7023, Springer,", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2011}, {"title": "Definition and properties to assess multi-agent environments as social intelligence tests", "author": ["J. Insa-Cabrera", "J. Hern\u00e1ndez-Orallo"], "venue": "arXiv preprint, http://arxiv.org/abs/1408.6350,", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2014}, {"title": "The anynt project intelligence test : Lambda - one", "author": ["J. Insa-Cabrera", "J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "S. Espa Na", "M.V. Hern\u00e1ndez-Lloreda"], "venue": "V. Muller and A. Ayesh, editors, AISB/IACAP 2012 Symposium \u201cRevisiting Turing and his Test\u201d, pages 20\u201327. The Society for the Study of Artificial Intelligence and the Simulation of Behaviour,", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluating Learning Algorithms", "author": ["N. Japkowicz", "M. Shah"], "venue": "Cambridge University Press,", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2011}, {"title": "Cattell\u2013Horn\u2013Carroll abilities and cognitive tests: What we\u2019ve learned from 20 years of research", "author": ["T.Z. Keith", "M. R Reynolds"], "venue": "Psychology in the Schools, 47(7):635\u2013650,", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2010}, {"title": "Competitive benchmarking: Lessons learned from the trading agent competition", "author": ["W. Ketter", "A. Symeonidis"], "venue": "AI Magazine, 33(2):103,", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2012}, {"title": "Robocup: The robot world cup initiative", "author": ["H. Kitano", "M. Asada", "Y. Kuniyoshi", "I. Noda", "E. Osawa"], "venue": "Proceedings of the first international conference on Autonomous agents, pages 340\u2013347. ACM,", "citeRegEx": "80", "shortCiteRegEx": null, "year": 1997}, {"title": "Who are you calling bird-brained? An attempt is being made to devise a universal intelligence test", "author": ["K. Kleiner"], "venue": "The Economist, 398(8723, 5 March 2011):82,", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2011}, {"title": "Sorting and searching, volume 3 of The Art of Computer Programming", "author": ["D.E. Knuth"], "venue": "Addison-Wesley,", "citeRegEx": "82", "shortCiteRegEx": null, "year": 1973}, {"title": "Clever methods of overfitting", "author": ["J. Langford"], "venue": "Machine Learning (Theory), http: // hunch. net ,", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2005}, {"title": "Research papers in machine learning", "author": ["P. Langley"], "venue": "Machine Learning, 2(3):195\u2013198,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 1987}, {"title": "The changing science of machine learning", "author": ["P. Langley"], "venue": "Machine Learning, 82(3):275\u2013279,", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2011}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["S. Legg", "M. Hutter"], "venue": "Minds and Machines, 17(4):391\u2013444,", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2007}, {"title": "Universal sequential search problems", "author": ["L.A. Levin"], "venue": "Problems of Information Transmission, 9(3):265\u2013266,", "citeRegEx": "87", "shortCiteRegEx": null, "year": 1973}, {"title": "Average case complete problems", "author": ["L.A. Levin"], "venue": "SIAM J. on Computing, 15:285\u2013286,", "citeRegEx": "88", "shortCiteRegEx": null, "year": 1986}, {"title": "An introduction to Kolmogorov complexity and its applications (3rd ed.)", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": null, "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2008}, {"title": "Turing\u2019s test and believable ai in games", "author": ["D. Livingstone"], "venue": "Computers in Entertainment (CIE), 4(1):6,", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2006}, {"title": "Artificial intelligence approaches for the generation and assessment of believable human-like behaviour in virtual characters", "author": ["J.M. Llargues-Asensio", "J. Peralta", "R. Arrabales", "M. Gonzalez-Bedia", "P. Cortez", "A.L. Lopez-Pe\u00f1a"], "venue": "Expert Systems with Applications,", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2014}, {"title": "Applications of item response theory to practical testing problems", "author": ["F.M. Lord"], "venue": "Mahwah, NJ: Erlbaum,", "citeRegEx": "92", "shortCiteRegEx": null, "year": 1980}, {"title": "Towards uci+: A mindful repository design", "author": ["N. Maci\u00e0", "E. Bernad\u00f3-Mansilla"], "venue": "Information Sciences, 261:237\u2013 262,", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2014}, {"title": "The termination competition", "author": ["C. March\u00e9", "H. Zantema"], "venue": "Term Rewriting and Applications, pages 303\u2013313. Springer,", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2007}, {"title": "What is artificial intelligence", "author": ["J. McCarthy"], "venue": "Technical report, Stanford University, http://www-formal. stanford.edu/jmc/whatisai.html,", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2007}, {"title": "Machines who think", "author": ["P. McCorduck"], "venue": "A K Peters/CRC Press,", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2004}, {"title": "Genetic programming needs better benchmarks", "author": ["J. McDermott", "D.R. White", "S. Luke", "L. Manzoni", "M. Castelli", "L. Vanneschi", "W. Ja\u015bkowski", "K. Krawiec", "R. Harper", "K. De Jong", "U.-M. O\u2019Reilly"], "venue": "In Proceedings of the fourteenth international conference on Genetic and evolutionary computation conference,", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2012}, {"title": "Graphics turing test", "author": ["M. McGuigan"], "venue": "arXiv preprint cs/0603132,", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2006}, {"title": "Generalized linear item response theory", "author": ["G.J. Mellenbergh"], "venue": "Psychological Bulletin, 115(2):300,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 1994}, {"title": "Semantic Information Processing", "author": ["M.L. Minsky", "editor"], "venue": null, "citeRegEx": "100", "shortCiteRegEx": "100", "year": 1968}, {"title": "Adapting the turing test for embodied neurocognitive evaluation of biologicallyinspired cognitive agents", "author": ["S.T. Mueller", "B.S. Minnery"], "venue": "Proc. 2008 AAAI Fall Symposium on Biologically Inspired Cognitive Architectures,", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2008}, {"title": "Computer science as empirical inquiry: Symbols and search", "author": ["A. Newell", "H.A. Simon"], "venue": "Communications of the ACM, 19(3):113\u2013126,", "citeRegEx": "102", "shortCiteRegEx": null, "year": 1976}, {"title": "The Turing Test", "author": ["G. Oppy", "D.L. Dowe"], "venue": "Edward N. Zalta, editor, Stanford Encyclopedia of Philosophy, pages Stanford University, http://plato.stanford.edu/entries/turing\u2013test/,", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2011}, {"title": "Anthropomorphism and ai: Turing\u2019s much misunderstood imitation game", "author": ["D. Proudfoot"], "venue": "Artificial Intelligence, 175(5):950\u2013957,", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2011}, {"title": "Human computation: a survey and taxonomy of a growing field", "author": ["A.J. Quinn", "B.B. Bederson"], "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 1403\u20131412. ACM,", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2011}, {"title": "Artificial intelligence \u2013 man or machine", "author": ["S. Rajani"], "venue": "International Journal of Information Technology, 4(1):173\u2013 176,", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2011}, {"title": "Evaluating expert system tools: A framework and methodology\u2013workshops", "author": ["J. Rothenberg", "J. Paul", "I. Kameny", "J.R. Kipps", "M. Swenson"], "venue": "Technical report, DTIC Document,", "citeRegEx": "107", "shortCiteRegEx": null, "year": 1987}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S. Russell", "P. Norvig"], "venue": "Prentice Hall,", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2009}, {"title": "A computer program capable of passing IQ tests", "author": ["P. Sanghi", "D.L. Dowe"], "venue": "4th Intl. Conf. on Cognitive Science (ICCS\u201903), Sydney, pages 570\u2013575,", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2003}, {"title": "Checkers is solved", "author": ["J. Schaeffer", "N. Burch", "Y. Bjornsson", "A. Kishimoto", "M. Muller", "R. Lake", "P. Lu", "S. Sutphen"], "venue": "Science, 317(5844):1518,", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2007}, {"title": "Primary mental abilities", "author": ["K Warner Schaie"], "venue": "Corsini Encyclopedia of Psychology,", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2010}, {"title": "The truly total turing test", "author": ["P. Schweizer"], "venue": "Minds and Machines, 8(2):263\u2013272,", "citeRegEx": "112", "shortCiteRegEx": null, "year": 1998}, {"title": "Minds, brains, and programs", "author": ["J.R. Searle"], "venue": "The Behavioral and Brain Sciences, 3:417\u2013457,", "citeRegEx": "113", "shortCiteRegEx": null, "year": 1980}, {"title": "Adaptive cluster sampling", "author": ["G.A.F. Seber", "M.M. Salehi"], "venue": "Adaptive Sampling Designs, pages 11\u201326. Springer,", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2013}, {"title": "Cognition, evolution, and behavior", "author": ["S.J. Shettleworth"], "venue": "Oxford University Press,", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2010}, {"title": "Fundamentals of Comparative Cognition", "author": ["S.J. Shettleworth", "P. Bloom", "L. Nadel"], "venue": "Oxford University Press,", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2013}, {"title": "Artificial intelligence: an empirical science", "author": ["H.A. Simon"], "venue": "Artificial Intelligence, 77(1):95\u2013127,", "citeRegEx": "117", "shortCiteRegEx": null, "year": 1995}, {"title": "UCI++: Improved support for algorithm selection using datasetoids", "author": ["C. Soares"], "venue": "Advances in Knowledge Discovery and Data Mining, pages 499\u2013506. Springer,", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2009}, {"title": "Does algorithmic probability solve the problem of induction", "author": ["R. Solomonoff"], "venue": "Information, Statistics and Induction in Science, pages 7\u20138,", "citeRegEx": "119", "shortCiteRegEx": null, "year": 1996}, {"title": "A formal theory of inductive inference", "author": ["R.J. Solomonoff"], "venue": "Part I. Information and control, 7(1):1\u201322,", "citeRegEx": "120", "shortCiteRegEx": null, "year": 1964}, {"title": "Importance sampling: Applications in communications and detection", "author": ["R. Srinivasan"], "venue": "Springer,", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2002}, {"title": "Handbook of intelligence", "author": ["R.J. Sternberg (ed"], "venue": null, "citeRegEx": "122", "shortCiteRegEx": "122", "year": 2000}, {"title": "Benchmarks for grid-based pathfinding", "author": ["N. Sturtevant"], "venue": "Transactions on Computational Intelligence and AI in Games, 4(2):144 \u2013 148,", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2012}, {"title": "The TPTP Problem Library and Associated Infrastructure: The FOF and CNF Parts, v3.5.0", "author": ["G. Sutcliffe"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "124", "shortCiteRegEx": "124", "year": 2009}, {"title": "The State of CASC", "author": ["G. Sutcliffe", "C. Suttner"], "venue": "AI Communications, 19(1):35\u201348,", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2006}, {"title": "Primary mental abilities", "author": ["L.L. Thurstone"], "venue": "Psychometric monographs,", "citeRegEx": "126", "shortCiteRegEx": null, "year": 1938}, {"title": "Assessing believability", "author": ["J. Togelius", "G.N. Yannakakis", "S. Karakovskiy", "N. Shaker"], "venue": "Believable Bots, pages 215\u2013230. Springer,", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2012}, {"title": "Computing machinery and intelligence", "author": ["A.M. Turing"], "venue": "Mind, 59:433\u2013460,", "citeRegEx": "128", "shortCiteRegEx": null, "year": 1950}, {"title": "A theory of the learnable", "author": ["L.G. Valiant"], "venue": "Communications of the ACM, 27(11):1134\u20131142,", "citeRegEx": "129", "shortCiteRegEx": null, "year": 1984}, {"title": "Human computation", "author": ["L. von Ahn"], "venue": "In Design Automation Conference,", "citeRegEx": "130", "shortCiteRegEx": "130", "year": 2009}, {"title": "Telling humans and computers apart automatically", "author": ["L. von Ahn", "M. Blum", "J. Langford"], "venue": "Communications of the ACM,", "citeRegEx": "131", "shortCiteRegEx": "131", "year": 2004}, {"title": "RECAPTCHA: Human-based character recognition via web security", "author": ["L. von Ahn", "B. Maurer", "C. McMillen", "D. Abraham", "M. Blum"], "venue": "measures. Science,", "citeRegEx": "132", "shortCiteRegEx": "132", "year": 2008}, {"title": "An information measure for classification", "author": ["C.S. Wallace", "D.M. Boulton"], "venue": "Computer Journal, 11(2):185\u2013194,", "citeRegEx": "133", "shortCiteRegEx": null, "year": 1968}, {"title": "Minimum message length and Kolmogorov complexity", "author": ["C.S. Wallace", "D.L. Dowe"], "venue": "Computer Journal, 42(4):270\u2013283,", "citeRegEx": "134", "shortCiteRegEx": null, "year": 1999}, {"title": "Turing test success marks milestone in computing history", "author": ["K. Warwick"], "venue": "University or Reading Press Release, 8 June", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2014}, {"title": "Q-learning", "author": ["C.J.C.H. Watkins", "P. Dayan"], "venue": "Mach. learning, 8(3):279\u2013292,", "citeRegEx": "136", "shortCiteRegEx": null, "year": 1992}, {"title": "Better data from better measurements using computerized adaptive testing", "author": ["D.J. Weiss"], "venue": "Journal of Methods and Measurement in the Social Sciences, 2(1):1\u201327,", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2011}, {"title": "ELIZA \u2013 a computer program for the study of natural language communication between man and machine", "author": ["J. Weizenbaum"], "venue": "Communications of the ACM, 9(1):3645,", "citeRegEx": "138", "shortCiteRegEx": null, "year": 1966}, {"title": "Better gp benchmarks: Community survey results and proposals", "author": ["D.R. White", "J. McDermott", "M. Castelli", "L. Manzoni", "B.W. Goldman", "G. Kronberger", "W. Ja\u015bkowski", "U.-M. O\u2019Reilly", "S. Luke"], "venue": "Genetic Programming and Evolvable Machines,", "citeRegEx": "139", "shortCiteRegEx": "139", "year": 2013}, {"title": "Protecting against evaluation overfitting in empirical reinforcement learning", "author": ["S. Whiteson", "B. Tanner", "M.E. Taylor", "P. Stone"], "venue": "Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), 2011 IEEE Symposium on, pages 120\u2013127. IEEE,", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2011}, {"title": "The Reinforcement Learning Competitions", "author": ["S. Whiteson", "B. Tanner", "A. White"], "venue": "The AI magazine, 31(2):81\u2013 94,", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2010}, {"title": "Information dynamics of evolved agents", "author": ["P.L. Williams", "R.D. Beer"], "venue": "From Animals to Animats 11, pages 38\u201349. Springer,", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2010}, {"title": "The lack of a priori distinctions between learning algorithms", "author": ["D.H. Wolpert"], "venue": "Neural Computation, 8(7):1341\u2013 1390,", "citeRegEx": "143", "shortCiteRegEx": null, "year": 1996}, {"title": "What the no free lunch theorems really mean; how to improve search algorithms", "author": ["D.H. Wolpert"], "venue": "Technical report, Santa fe Institute Working Paper,", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2012}, {"title": "No free lunch theorems for search", "author": ["D.H. Wolpert", "W.G. Macready"], "venue": "Technical report, Technical Report SFI-TR-95-02-010 (Santa Fe Institute),", "citeRegEx": "145", "shortCiteRegEx": null, "year": 1995}, {"title": "Toward a standard metric of machine intelligence", "author": ["R. Yonck"], "venue": "World Future Review, 4(2):61\u201370,", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning mazes with aliasing states: An LCS algorithm with associative perception", "author": ["Z. Zatuchna", "A. Bagnall"], "venue": "Adaptive Behavior, 17(1):28\u201357,", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 92, "context": "What is the purpose of artificial intelligence (AI)? McCarthy\u2019s pristine definition of AI sets this unambiguously: \u201c[AI is] the science and engineering of making intelligent machines\u201d [95].", "startOffset": 184, "endOffset": 188}, {"referenceID": 93, "context": "This phenomenon is known as the \u201cAI effect\u201d [96].", "startOffset": 44, "endOffset": 48}, {"referenceID": 3, "context": "Also, it is probably a crucial moment to overhaul the way AI evaluation is performed, after the recent progress in areas of AI that are detaching from the narrow AI approach, such as developmental robotics [4], deep learning [3], inductive programming [53, 48, 47], artificial general intelligence [45], universal artificial intelligence [71], etc.", "startOffset": 206, "endOffset": 209}, {"referenceID": 2, "context": "Also, it is probably a crucial moment to overhaul the way AI evaluation is performed, after the recent progress in areas of AI that are detaching from the narrow AI approach, such as developmental robotics [4], deep learning [3], inductive programming [53, 48, 47], artificial general intelligence [45], universal artificial intelligence [71], etc.", "startOffset": 225, "endOffset": 228}, {"referenceID": 50, "context": "Also, it is probably a crucial moment to overhaul the way AI evaluation is performed, after the recent progress in areas of AI that are detaching from the narrow AI approach, such as developmental robotics [4], deep learning [3], inductive programming [53, 48, 47], artificial general intelligence [45], universal artificial intelligence [71], etc.", "startOffset": 252, "endOffset": 264}, {"referenceID": 45, "context": "Also, it is probably a crucial moment to overhaul the way AI evaluation is performed, after the recent progress in areas of AI that are detaching from the narrow AI approach, such as developmental robotics [4], deep learning [3], inductive programming [53, 48, 47], artificial general intelligence [45], universal artificial intelligence [71], etc.", "startOffset": 252, "endOffset": 264}, {"referenceID": 44, "context": "Also, it is probably a crucial moment to overhaul the way AI evaluation is performed, after the recent progress in areas of AI that are detaching from the narrow AI approach, such as developmental robotics [4], deep learning [3], inductive programming [53, 48, 47], artificial general intelligence [45], universal artificial intelligence [71], etc.", "startOffset": 252, "endOffset": 264}, {"referenceID": 68, "context": "Also, it is probably a crucial moment to overhaul the way AI evaluation is performed, after the recent progress in areas of AI that are detaching from the narrow AI approach, such as developmental robotics [4], deep learning [3], inductive programming [53, 48, 47], artificial general intelligence [45], universal artificial intelligence [71], etc.", "startOffset": 338, "endOffset": 342}, {"referenceID": 99, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 40, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 104, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 41, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 20, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 81, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 82, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 12, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 114, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 4, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 35, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 80, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 137, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 31, "context": "Some previous works discussing AI evaluation [102, 42, 107, 43, 22, 84, 85, 14, 117, 6, 37, 83, 140, 33] are relatively old, restrictive to a specific area of AI and/or focussed on the experimental methodology rather than what is being measured and how.", "startOffset": 45, "endOffset": 104}, {"referenceID": 17, "context": "For instance, in [19] we find criteria for evaluating research problems, methods, implementations, experiments\u2019 design, and evaluation of the experiments.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "Other statements in [19] are not so up-to-date, and show that there has been an improvement in AI evaluation.", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": ", [21] report that more than 60% of ICAIL papers in 1987 did not have any evaluation in front of 20% in 2011).", "startOffset": 2, "endOffset": 6}, {"referenceID": 105, "context": ", [108]).", "startOffset": 2, "endOffset": 7}, {"referenceID": 17, "context": "In fact, going from an abstract problem to a specific task is encouraged: \u201crefine the topic to a task\u201d, provided it is \u201crepresentative\u201d [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 13, "context": "For instance, we can say that it was the research team after Deep Blue [15] (with the help of a powerful computer) who actually defeated Kasparov.", "startOffset": 71, "endOffset": 75}, {"referenceID": 137, "context": ", [140]).", "startOffset": 2, "endOffset": 7}, {"referenceID": 142, "context": "At this point, it is pertinent to make a comment about the well-known no-free-lunch (NFL) theorems [145, 143, 144], as these theorems are usually misunderstood.", "startOffset": 99, "endOffset": 114}, {"referenceID": 140, "context": "At this point, it is pertinent to make a comment about the well-known no-free-lunch (NFL) theorems [145, 143, 144], as these theorems are usually misunderstood.", "startOffset": 99, "endOffset": 114}, {"referenceID": 141, "context": "At this point, it is pertinent to make a comment about the well-known no-free-lunch (NFL) theorems [145, 143, 144], as these theorems are usually misunderstood.", "startOffset": 99, "endOffset": 114}, {"referenceID": 117, "context": "A universal distribution [120, 89], e.", "startOffset": 25, "endOffset": 34}, {"referenceID": 86, "context": "A universal distribution [120, 89], e.", "startOffset": 25, "endOffset": 34}, {"referenceID": 79, "context": "Worst-case analysis is more common than average-case analysis, although the latter has also become popular recently [82, 88, 46].", "startOffset": 116, "endOffset": 128}, {"referenceID": 85, "context": "Worst-case analysis is more common than average-case analysis, although the latter has also become popular recently [82, 88, 46].", "startOffset": 116, "endOffset": 128}, {"referenceID": 43, "context": "Worst-case analysis is more common than average-case analysis, although the latter has also become popular recently [82, 88, 46].", "startOffset": 116, "endOffset": 128}, {"referenceID": 126, "context": ", Probably Approximately Correct learning [129]).", "startOffset": 42, "endOffset": 47}, {"referenceID": 107, "context": "For instance, in board games, algorithms can be derived and analysed whether they are optimal, such as noughts and crosses (tic-tac-toe) and English draughts (checkers), the latter solved by Jonathan Schaeffer [110].", "startOffset": 210, "endOffset": 215}, {"referenceID": 12, "context": "This is also in agreement with a view of AI as an experimental science [14, 117].", "startOffset": 71, "endOffset": 80}, {"referenceID": 114, "context": "This is also in agreement with a view of AI as an experimental science [14, 117].", "startOffset": 71, "endOffset": 80}, {"referenceID": 137, "context": ", the \u201csecret generalized methodology\u201d [140]).", "startOffset": 39, "endOffset": 44}, {"referenceID": 33, "context": ", the Elo system in chess [35]).", "startOffset": 26, "endOffset": 30}, {"referenceID": 125, "context": "The Turing Test [128, 103] is a case in which there is both comparison against humans and evaluation by human judges.", "startOffset": 16, "endOffset": 26}, {"referenceID": 100, "context": "The Turing Test [128, 103] is a case in which there is both comparison against humans and evaluation by human judges.", "startOffset": 16, "endOffset": 26}, {"referenceID": 135, "context": "Even if the results were not significantly different to previous results of the Loebner Prize (or even what Weizenbaum\u2019s ELIZA was able to do fifty years ago [138]), the over-reaction and publicity of this outcome were preposterous.", "startOffset": 158, "endOffset": 163}, {"referenceID": 132, "context": "\u201d [135].", "startOffset": 2, "endOffset": 7}, {"referenceID": 101, "context": "Despite the criticism, the Turing test still has many advocates [104].", "startOffset": 64, "endOffset": 69}, {"referenceID": 58, "context": "It is also an inspiration for countless philosophical debates and has led to connections with other concepts in AI or computation [61, 62].", "startOffset": 130, "endOffset": 138}, {"referenceID": 59, "context": "It is also an inspiration for countless philosophical debates and has led to connections with other concepts in AI or computation [61, 62].", "startOffset": 130, "endOffset": 138}, {"referenceID": 109, "context": "Recently, there have been variants of the Turing Test (Total Turing Tests [112], Visual", "startOffset": 74, "endOffset": 79}, {"referenceID": 87, "context": "[90, 68] Robo Chat Challenge Chattering bots competition.", "startOffset": 0, "endOffset": 8}, {"referenceID": 65, "context": "[90, 68] Robo Chat Challenge Chattering bots competition.", "startOffset": 0, "endOffset": 8}, {"referenceID": 128, "context": "[131, 132] Graphics Turing Test Tell between a computer-generated virtual world and a real camera.", "startOffset": 0, "endOffset": 10}, {"referenceID": 129, "context": "[131, 132] Graphics Turing Test Tell between a computer-generated virtual world and a real camera.", "startOffset": 0, "endOffset": 10}, {"referenceID": 95, "context": "[98, 10]", "startOffset": 0, "endOffset": 8}, {"referenceID": 8, "context": "[98, 10]", "startOffset": 0, "endOffset": 8}, {"referenceID": 98, "context": "[101, 67]) that may be useful for chatterbot evaluation, personal assistants and videogames.", "startOffset": 0, "endOffset": 9}, {"referenceID": 64, "context": "[101, 67]) that may be useful for chatterbot evaluation, personal assistants and videogames.", "startOffset": 0, "endOffset": 9}, {"referenceID": 87, "context": "It is within the area of videogames where the notion of \u2018believability\u2019 has appeared, as the property of a bot of looking \u2018believable\u2019 as a human [90, 68].", "startOffset": 146, "endOffset": 154}, {"referenceID": 65, "context": "It is within the area of videogames where the notion of \u2018believability\u2019 has appeared, as the property of a bot of looking \u2018believable\u2019 as a human [90, 68].", "startOffset": 146, "endOffset": 154}, {"referenceID": 128, "context": "Finally, there is a kind of test that is related to the Turing Test, the so-called CAPTCHA, Completely Automated Public Turing test to tell Computers and Humans Apart [131, 132].", "startOffset": 167, "endOffset": 177}, {"referenceID": 129, "context": "Finally, there is a kind of test that is related to the Turing Test, the so-called CAPTCHA, Completely Automated Public Turing test to tell Computers and Humans Apart [131, 132].", "startOffset": 167, "endOffset": 177}, {"referenceID": 124, "context": "For instance, believability is said to be better assessed from a third-person perspective (judging recorded video of other players without playing) than with a first-person perspective [127].", "startOffset": 185, "endOffset": 190}, {"referenceID": 88, "context": "Actually, this third-person perspective is included in the 2014 competition using a crowdsourcing platform [91] so that the 2014 edition incorporates the two judging systems: the First-Person Assessment (FPA), using the BotPrize in-game judging system, and the Third-Person Assessment (TPA), using a crowdsourcing platform.", "startOffset": 107, "endOffset": 111}, {"referenceID": 5, "context": "Another issue that could be considered in the future is a richer (and more challenging) representation of the environment, closer to the way humans perceive the images of the game (such as the graphical processing required for the Arcade Learning Environment [7]).", "startOffset": 259, "endOffset": 262}, {"referenceID": 137, "context": "Because of the \u2018evaluation overfitting\u2019 [140], \u2018method overfitting\u2019 problem [37] or other \u201cclever methods of overfitting\u201d [83], it is much better if M is very large or infinite, or at least the problems are not disclosed until evaluation time.", "startOffset": 40, "endOffset": 45}, {"referenceID": 35, "context": "Because of the \u2018evaluation overfitting\u2019 [140], \u2018method overfitting\u2019 problem [37] or other \u201cclever methods of overfitting\u201d [83], it is much better if M is very large or infinite, or at least the problems are not disclosed until evaluation time.", "startOffset": 76, "endOffset": 80}, {"referenceID": 80, "context": "Because of the \u2018evaluation overfitting\u2019 [140], \u2018method overfitting\u2019 problem [37] or other \u201cclever methods of overfitting\u201d [83], it is much better if M is very large or infinite, or at least the problems are not disclosed until evaluation time.", "startOffset": 122, "endOffset": 126}, {"referenceID": 31, "context": "] that follow certain high-level characteristics\u201d [33].", "startOffset": 50, "endOffset": 54}, {"referenceID": 118, "context": "This \u2018diversity-driven sampling\u201d is related to some kinds of sampling known as importance sampling [121], stratified sampling [18] and other forced Monte Carlo procedures.", "startOffset": 99, "endOffset": 104}, {"referenceID": 16, "context": "This \u2018diversity-driven sampling\u201d is related to some kinds of sampling known as importance sampling [121], stratified sampling [18] and other forced Monte Carlo procedures.", "startOffset": 126, "endOffset": 130}, {"referenceID": 34, "context": "In fact, Item Response Theory [36] in psychometrics follows this approach.", "startOffset": 30, "endOffset": 34}, {"referenceID": 144, "context": "The complexity metric can be specific to the application (such as the complexity for mazes in [147] or gridworld domains in [123]) or it can be a more general approach (e.", "startOffset": 94, "endOffset": 99}, {"referenceID": 120, "context": "The complexity metric can be specific to the application (such as the complexity for mazes in [147] or gridworld domains in [123]) or it can be a more general approach (e.", "startOffset": 124, "endOffset": 129}, {"referenceID": 111, "context": "is represented by what is known by adaptive cluster sampling [114] and it is common in population surveys and many experimental sciences.", "startOffset": 61, "endOffset": 66}, {"referenceID": 34, "context": "Item response theory (IRT) [36] estimates mathematical models to infer the associated probability and informativeness estimations for each item.", "startOffset": 27, "endOffset": 31}, {"referenceID": 96, "context": "On other occasions, especially if Ris unbounded, a linear model may be preferred [99, 38]:", "startOffset": 81, "endOffset": 89}, {"referenceID": 36, "context": "On other occasions, especially if Ris unbounded, a linear model may be preferred [99, 38]:", "startOffset": 81, "endOffset": 89}, {"referenceID": 37, "context": "Again, the slope \u03bb is positively related to most measures of discriminating power [39].", "startOffset": 82, "endOffset": 86}, {"referenceID": 90, "context": "The procedure, which is referred as \u201cThe UCI test\u201d [93] or the \u201cde facto approach\u201d [33][77] follows the general form of equation 1 where M is the repository, p is the choice of datasets and R is one particular performance metric (accuracy, AUC, Brier score, F-measure, MSE, etc.", "startOffset": 51, "endOffset": 55}, {"referenceID": 31, "context": "The procedure, which is referred as \u201cThe UCI test\u201d [93] or the \u201cde facto approach\u201d [33][77] follows the general form of equation 1 where M is the repository, p is the choice of datasets and R is one particular performance metric (accuracy, AUC, Brier score, F-measure, MSE, etc.", "startOffset": 83, "endOffset": 87}, {"referenceID": 74, "context": "The procedure, which is referred as \u201cThe UCI test\u201d [93] or the \u201cde facto approach\u201d [33][77] follows the general form of equation 1 where M is the repository, p is the choice of datasets and R is one particular performance metric (accuracy, AUC, Brier score, F-measure, MSE, etc.", "startOffset": 87, "endOffset": 91}, {"referenceID": 38, "context": "[40, 60]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 57, "context": "[40, 60]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 122, "context": "Evaluation Setting Description CADE ATP System Competition Theorem proving [125] using the TPTP library [124].", "startOffset": 75, "endOffset": 80}, {"referenceID": 121, "context": "Evaluation Setting Description CADE ATP System Competition Theorem proving [125] using the TPTP library [124].", "startOffset": 104, "endOffset": 109}, {"referenceID": 91, "context": "Termination Competition Termination of term rewriting and programs [94].", "startOffset": 67, "endOffset": 71}, {"referenceID": 138, "context": "The reinforcement learning competition Reinforcement learning [141].", "startOffset": 62, "endOffset": 67}, {"referenceID": 1, "context": "Syntax-guided synthesis competition Program synthesis [2].", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "UCI and KEEL Machine learning dataset repositories [5] [1].", "startOffset": 55, "endOffset": 58}, {"referenceID": 5, "context": "The Arcade Learning Environment Atari 2600 videogames (reinforcement learning) [7].", "startOffset": 79, "endOffset": 82}, {"referenceID": 94, "context": "GP benchmarks Genetic programming [97, 139].", "startOffset": 34, "endOffset": 43}, {"referenceID": 136, "context": "GP benchmarks Genetic programming [97, 139].", "startOffset": 34, "endOffset": 43}, {"referenceID": 120, "context": "Pathfinding benchmarks Gridworld domains (mazes) [123].", "startOffset": 49, "endOffset": 54}, {"referenceID": 21, "context": ", [23]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 90, "context": "For instance, UCI+, \u201ca mindful UCI\u201d [93], proposes the characterisation of the problems in the UCI repository by a set of complexity measures from [69] (the number of classes, however, is not part of the description).", "startOffset": 36, "endOffset": 40}, {"referenceID": 66, "context": "For instance, UCI+, \u201ca mindful UCI\u201d [93], proposes the characterisation of the problems in the UCI repository by a set of complexity measures from [69] (the number of classes, however, is not part of the description).", "startOffset": 147, "endOffset": 151}, {"referenceID": 115, "context": "It is a distortion-based generator (similar to Soares\u2019s UCI++ [118]).", "startOffset": 62, "endOffset": 67}, {"referenceID": 90, "context": "Finally, [93] suggest ideas about sharing and arranging the results of previous evaluations so that each new algorithm can be compared immediately with many other algorithms using the same experimental setting.", "startOffset": 9, "endOffset": 13}, {"referenceID": 107, "context": "For instance, in an English draughts (checkers) competition, we could have players being specialised to play against Chinook, the proven optimal player [110].", "startOffset": 152, "endOffset": 157}, {"referenceID": 51, "context": "Figure 4: We show the distributions of reward (roughly corresponding to R in this paper) for different configurations for the multi-agent system SCMAS introduced in [54].", "startOffset": 165, "endOffset": 169}, {"referenceID": 51, "context": "How can we assess whether the set \u03a9 has sufficiently difficulty and discriminative power? This is of course a difficult problem, which has recently be analysed in [54].", "startOffset": 163, "endOffset": 167}, {"referenceID": 77, "context": "Evaluation Setting Description Robocup Robotics (robot football/soccer) [80].", "startOffset": 72, "endOffset": 76}, {"referenceID": 42, "context": "General game playing AAAI competition General game playing using GDL [44].", "startOffset": 69, "endOffset": 73}, {"referenceID": 76, "context": "Trading Agents Competition Training agents [79].", "startOffset": 43, "endOffset": 47}, {"referenceID": 72, "context": "[75].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "An adaptation between the general game playing and RL-glue, which is used in the reinforcement learning competition, that could make this possible has been done in [8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 103, "context": "For instance, one way of evaluating AI progress is by looking at a task and check in which category an AI system is currently: optimal if no other system can perform better, strong super-human if it performs better than all humans, super-human if it performs better than most humans, par-human if it performs similarly to most humans, and sub-human if it performs worse than most humans [106].", "startOffset": 387, "endOffset": 392}, {"referenceID": 108, "context": "Many different cognitive abilities or factors have been identified and have been arranged in different ways [111].", "startOffset": 108, "endOffset": 113}, {"referenceID": 75, "context": "For instance, one well-known comprehensive theory of human cognitive abilities is the Cattell-Horn-Carroll theory [78].", "startOffset": 114, "endOffset": 118}, {"referenceID": 105, "context": "[108]), we can enumerate areas such as problem solving, use of knowledge, reasoning, learning, perception, natural language processing, etc.", "startOffset": 0, "endOffset": 5}, {"referenceID": 119, "context": "Psychometrics is concerned about measuring cognitive abilities, personality traits and other psychological properties [122].", "startOffset": 118, "endOffset": 123}, {"referenceID": 89, "context": ", determined by the percentage of subjects that are able to solve the item, using functional models as in Item Response Theory [92, 36], as seen in the previous section.", "startOffset": 127, "endOffset": 135}, {"referenceID": 34, "context": ", determined by the percentage of subjects that are able to solve the item, using functional models as in Item Response Theory [92, 36], as seen in the previous section.", "startOffset": 127, "endOffset": 135}, {"referenceID": 106, "context": "Table 4: Results by a rudimentary program for passing IQ tests (from [109]).", "startOffset": 69, "endOffset": 74}, {"referenceID": 11, "context": "More recently, their use has been vindicated by Bringsjord and Schmimanski [13, 12], with the so-called \u2018Psychometric AI\u2019 (PAI), as \u201cthe field devoted to building informationprocessing entities capable of at least solid performance on all established, validated tests of intelligence and mental ability, a class of tests that includes not just the rather restrictive IQ tests, but also tests of artistic and literary creativity, mechanical ability, and so on\u201d.", "startOffset": 75, "endOffset": 83}, {"referenceID": 10, "context": "More recently, their use has been vindicated by Bringsjord and Schmimanski [13, 12], with the so-called \u2018Psychometric AI\u2019 (PAI), as \u201cthe field devoted to building informationprocessing entities capable of at least solid performance on all established, validated tests of intelligence and mental ability, a class of tests that includes not just the rather restrictive IQ tests, but also tests of artistic and literary creativity, mechanical ability, and so on\u201d.", "startOffset": 75, "endOffset": 83}, {"referenceID": 22, "context": "Detterman, editor of the Intelligence Journal, wrote an editorial [24] where he suggested that Watson (the then recent winner of the Jeopardy! TV quiz [41]) should be evaluated with IQ tests.", "startOffset": 66, "endOffset": 70}, {"referenceID": 39, "context": "Detterman, editor of the Intelligence Journal, wrote an editorial [24] where he suggested that Watson (the then recent winner of the Jeopardy! TV quiz [41]) should be evaluated with IQ tests.", "startOffset": 151, "endOffset": 155}, {"referenceID": 22, "context": "The challenge is very explicit: \u201cI, the editorial board of Intelligence, and members of the International Society for Intelligence Research will develop a unique battery of intelligence tests that would be administered to that computer and would result in an actual IQ score\u201d [24].", "startOffset": 276, "endOffset": 280}, {"referenceID": 22, "context": "Only computers passing the second level \u201ccould be said to be truly intelligent\u201d [24].", "startOffset": 80, "endOffset": 84}, {"referenceID": 106, "context": "Detterman was unaware that almost a decade before, in 2003, Sanghi and Dowe [109] implemented a small program (less than 1000 lines of code) which could score relatively well on many IQ tests, as shown in Table 4.", "startOffset": 76, "endOffset": 81}, {"referenceID": 28, "context": "For a more complete discussion about why IQ tests are not ready for AI evaluation, the reader is referred to a response [30] to Detterman\u2019s editorial.", "startOffset": 120, "endOffset": 124}, {"referenceID": 60, "context": "(for a survey, see [63]).", "startOffset": 19, "endOffset": 23}, {"referenceID": 112, "context": "Comparative psychology and comparative cognition [115, 116] are the main disciplines that perform this evaluation.", "startOffset": 49, "endOffset": 59}, {"referenceID": 113, "context": "Comparative psychology and comparative cognition [115, 116] are the main disciplines that perform this evaluation.", "startOffset": 49, "endOffset": 59}, {"referenceID": 113, "context": "The evaluation focus on \u201cbasic processes\u201d, such as perception, attention, memory, associative learning and the discrimination of concepts, and recently on more sophisticated instrumental or social abilities [116].", "startOffset": 207, "endOffset": 212}, {"referenceID": 62, "context": "For instance, several kinds of skills are evaluated in human children and apes in [65].", "startOffset": 82, "endOffset": 86}, {"referenceID": 139, "context": "Nonetheless, there is an increasing need for the evaluation of animats [142] and the evaluation procedures for animals are the first candidates to try.", "startOffset": 71, "endOffset": 76}, {"referenceID": 15, "context": "If intelligence was viewed as a \u201ckind of information processing\u201d [17] then it seemed reasonable to look at information theory for an \u201cessential nature or formal basis of intelligence and the proper theoretical framework for it\u201d [17].", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "If intelligence was viewed as a \u201ckind of information processing\u201d [17] then it seemed reasonable to look at information theory for an \u201cessential nature or formal basis of intelligence and the proper theoretical framework for it\u201d [17].", "startOffset": 228, "endOffset": 232}, {"referenceID": 117, "context": "This was finally done with algorithmic information theory (AIT), and the related notions of Solomonoff universal probability [120], Kolmogorov complexity [89] and Wallace\u2019s Minimum Message Length (MML) [133, 134].", "startOffset": 125, "endOffset": 130}, {"referenceID": 86, "context": "This was finally done with algorithmic information theory (AIT), and the related notions of Solomonoff universal probability [120], Kolmogorov complexity [89] and Wallace\u2019s Minimum Message Length (MML) [133, 134].", "startOffset": 154, "endOffset": 158}, {"referenceID": 130, "context": "This was finally done with algorithmic information theory (AIT), and the related notions of Solomonoff universal probability [120], Kolmogorov complexity [89] and Wallace\u2019s Minimum Message Length (MML) [133, 134].", "startOffset": 202, "endOffset": 212}, {"referenceID": 131, "context": "This was finally done with algorithmic information theory (AIT), and the related notions of Solomonoff universal probability [120], Kolmogorov complexity [89] and Wallace\u2019s Minimum Message Length (MML) [133, 134].", "startOffset": 202, "endOffset": 212}, {"referenceID": 86, "context": "Even if these definitions depend on the UTM that is used, the invariance theorem states that their values will only differ up to a constant for two different UTM (because one can emulate the other) [89].", "startOffset": 198, "endOffset": 202}, {"referenceID": 46, "context": "Figure 6: Several series of different complexity 9, 12, and 14 used in the C-test [49].", "startOffset": 82, "endOffset": 86}, {"referenceID": 117, "context": "In a way, Solomonoff, the father of algorithmic probability [120] gave a theoretical backing to Occam\u2019s razor.", "startOffset": 60, "endOffset": 65}, {"referenceID": 116, "context": "It has been acknowledged that Solomonoff \u201csolved the problem of induction\u201d [119, 27].", "startOffset": 75, "endOffset": 84}, {"referenceID": 25, "context": "It has been acknowledged that Solomonoff \u201csolved the problem of induction\u201d [119, 27].", "startOffset": 75, "endOffset": 84}, {"referenceID": 84, "context": "For instance, some of these concepts lead to incomputable functions, although approximations exist, such as Kolmogorov complexity, which can be approximated by Levin\u2019s Kt [87].", "startOffset": 171, "endOffset": 175}, {"referenceID": 26, "context": "The application of AIT to (artificial) intelligence evaluation started with a variant of the Turing test that featured compression problems [28, 29] to make the test more sufficient.", "startOffset": 140, "endOffset": 148}, {"referenceID": 27, "context": "The application of AIT to (artificial) intelligence evaluation started with a variant of the Turing test that featured compression problems [28, 29] to make the test more sufficient.", "startOffset": 140, "endOffset": 148}, {"referenceID": 110, "context": "While one of the goals of this work was to criticise Searle\u2019s Chinese room [113] (an argument that has faded with time), this is one of the first intelligence test proposals using AIT.", "startOffset": 75, "endOffset": 80}, {"referenceID": 61, "context": "At roughly the same time, the C-test was introduced by [64, 49], where tasks are formally derived and their difficulty can be explicitly quantified.", "startOffset": 55, "endOffset": 63}, {"referenceID": 46, "context": "At roughly the same time, the C-test was introduced by [64, 49], where tasks are formally derived and their difficulty can be explicitly quantified.", "startOffset": 55, "endOffset": 63}, {"referenceID": 47, "context": "Some extensions of the C-test were suggested (\u201crewards and penalties could be used instead\u201d [50] and several variants for other abilities [51].", "startOffset": 92, "endOffset": 96}, {"referenceID": 48, "context": "Some extensions of the C-test were suggested (\u201crewards and penalties could be used instead\u201d [50] and several variants for other abilities [51].", "startOffset": 138, "endOffset": 142}, {"referenceID": 106, "context": "Nonetheless, this line of research was sharply restrained in 2003 by the evidence that very simple programs could pass IQ tests [109], as we have seen in section 3.", "startOffset": 128, "endOffset": 133}, {"referenceID": 23, "context": "Dobrev [25, 26] introduced a formal definition of intelligence as performance in a range of worlds.", "startOffset": 7, "endOffset": 15}, {"referenceID": 24, "context": "Dobrev [25, 26] introduced a formal definition of intelligence as performance in a range of worlds.", "startOffset": 7, "endOffset": 15}, {"referenceID": 83, "context": "Similar ideas are introduced by [86], but with a better formalisation.", "startOffset": 32, "endOffset": 36}, {"referenceID": 63, "context": "For more detail about these issues and some possible solutions, the reader is referred to [66] [55, secs.", "startOffset": 90, "endOffset": 94}, {"referenceID": 55, "context": "The information-theoretic approach is not isolated from some of the approaches seen so far in section 2, some hybridisations and integrated approaches have been proposed [58, 32, 62, 61, 72] (apart from the compresson-enriched Turing Tests ([28, 29] already mentioned above).", "startOffset": 170, "endOffset": 190}, {"referenceID": 30, "context": "The information-theoretic approach is not isolated from some of the approaches seen so far in section 2, some hybridisations and integrated approaches have been proposed [58, 32, 62, 61, 72] (apart from the compresson-enriched Turing Tests ([28, 29] already mentioned above).", "startOffset": 170, "endOffset": 190}, {"referenceID": 59, "context": "The information-theoretic approach is not isolated from some of the approaches seen so far in section 2, some hybridisations and integrated approaches have been proposed [58, 32, 62, 61, 72] (apart from the compresson-enriched Turing Tests ([28, 29] already mentioned above).", "startOffset": 170, "endOffset": 190}, {"referenceID": 58, "context": "The information-theoretic approach is not isolated from some of the approaches seen so far in section 2, some hybridisations and integrated approaches have been proposed [58, 32, 62, 61, 72] (apart from the compresson-enriched Turing Tests ([28, 29] already mentioned above).", "startOffset": 170, "endOffset": 190}, {"referenceID": 69, "context": "The information-theoretic approach is not isolated from some of the approaches seen so far in section 2, some hybridisations and integrated approaches have been proposed [58, 32, 62, 61, 72] (apart from the compresson-enriched Turing Tests ([28, 29] already mentioned above).", "startOffset": 170, "endOffset": 190}, {"referenceID": 26, "context": "The information-theoretic approach is not isolated from some of the approaches seen so far in section 2, some hybridisations and integrated approaches have been proposed [58, 32, 62, 61, 72] (apart from the compresson-enriched Turing Tests ([28, 29] already mentioned above).", "startOffset": 241, "endOffset": 249}, {"referenceID": 27, "context": "The information-theoretic approach is not isolated from some of the approaches seen so far in section 2, some hybridisations and integrated approaches have been proposed [58, 32, 62, 61, 72] (apart from the compresson-enriched Turing Tests ([28, 29] already mentioned above).", "startOffset": 241, "endOffset": 249}, {"referenceID": 52, "context": "In [55], the notion of \u2018universal test\u2019 is introduced, as a test that is applicable to \u201cany biological or artificial system that exists at this time or in the future\u201d: human, non-human animal, machine, hybrid or collective.", "startOffset": 3, "endOffset": 7}, {"referenceID": 52, "context": "A first framework for universal, anytime intelligence tests is introduced in [55], where a class of environment is carefully chosen to be discriminative.", "startOffset": 77, "endOffset": 81}, {"referenceID": 71, "context": "Some experiments were performed [74, 73, 76], using the environment class defined in [52].", "startOffset": 32, "endOffset": 44}, {"referenceID": 70, "context": "Some experiments were performed [74, 73, 76], using the environment class defined in [52].", "startOffset": 32, "endOffset": 44}, {"referenceID": 73, "context": "Some experiments were performed [74, 73, 76], using the environment class defined in [52].", "startOffset": 32, "endOffset": 44}, {"referenceID": 49, "context": "Some experiments were performed [74, 73, 76], using the environment class defined in [52].", "startOffset": 85, "endOffset": 89}, {"referenceID": 133, "context": "The same test compared Q-learning [136] with humans.", "startOffset": 34, "endOffset": 39}, {"referenceID": 78, "context": "Despite the limited results, the experiment had quite a repercussion [81, 9, 57, 146].", "startOffset": 69, "endOffset": 85}, {"referenceID": 7, "context": "Despite the limited results, the experiment had quite a repercussion [81, 9, 57, 146].", "startOffset": 69, "endOffset": 85}, {"referenceID": 54, "context": "Despite the limited results, the experiment had quite a repercussion [81, 9, 57, 146].", "startOffset": 69, "endOffset": 85}, {"referenceID": 143, "context": "Despite the limited results, the experiment had quite a repercussion [81, 9, 57, 146].", "startOffset": 69, "endOffset": 85}, {"referenceID": 55, "context": ", the Darwin-Wallace distribution [58], but it is still a puzzle how to adapt these ideas to the measurement of social intelligence and multi-agent systems [32, 72, 54, 75].", "startOffset": 34, "endOffset": 38}, {"referenceID": 30, "context": ", the Darwin-Wallace distribution [58], but it is still a puzzle how to adapt these ideas to the measurement of social intelligence and multi-agent systems [32, 72, 54, 75].", "startOffset": 156, "endOffset": 172}, {"referenceID": 69, "context": ", the Darwin-Wallace distribution [58], but it is still a puzzle how to adapt these ideas to the measurement of social intelligence and multi-agent systems [32, 72, 54, 75].", "startOffset": 156, "endOffset": 172}, {"referenceID": 51, "context": ", the Darwin-Wallace distribution [58], but it is still a puzzle how to adapt these ideas to the measurement of social intelligence and multi-agent systems [32, 72, 54, 75].", "startOffset": 156, "endOffset": 172}, {"referenceID": 72, "context": ", the Darwin-Wallace distribution [58], but it is still a puzzle how to adapt these ideas to the measurement of social intelligence and multi-agent systems [32, 72, 54, 75].", "startOffset": 156, "endOffset": 172}, {"referenceID": 56, "context": "The fragmentation of Figure 7 and the need of solving many of the above issues has suggested the introduction of a new perspective, dubbed \u2018universal psychometrics\u2019 [59].", "startOffset": 165, "endOffset": 169}, {"referenceID": 139, "context": "Notions such as \u2018animat\u2019 [142], machine-enhanced humans [20], human-enhanced machines [130], other kind of hybrids and, most especially, collective [105] of any of the former, suggest that the distinction between animals, humans and machines is not only inappropriate, but no longer useful to advance in the evaluation of cognitive abilities.", "startOffset": 25, "endOffset": 30}, {"referenceID": 18, "context": "Notions such as \u2018animat\u2019 [142], machine-enhanced humans [20], human-enhanced machines [130], other kind of hybrids and, most especially, collective [105] of any of the former, suggest that the distinction between animals, humans and machines is not only inappropriate, but no longer useful to advance in the evaluation of cognitive abilities.", "startOffset": 56, "endOffset": 60}, {"referenceID": 127, "context": "Notions such as \u2018animat\u2019 [142], machine-enhanced humans [20], human-enhanced machines [130], other kind of hybrids and, most especially, collective [105] of any of the former, suggest that the distinction between animals, humans and machines is not only inappropriate, but no longer useful to advance in the evaluation of cognitive abilities.", "startOffset": 86, "endOffset": 91}, {"referenceID": 102, "context": "Notions such as \u2018animat\u2019 [142], machine-enhanced humans [20], human-enhanced machines [130], other kind of hybrids and, most especially, collective [105] of any of the former, suggest that the distinction between animals, humans and machines is not only inappropriate, but no longer useful to advance in the evaluation of cognitive abilities.", "startOffset": 148, "endOffset": 153}, {"referenceID": 9, "context": "As a result, cognitive abilities are no longer what the cognitive tests measure, as in human psychometrics (adapting the (in)famous statement that intelligence is \u201cwhat intelligence test measures\u201d [11]), but they are properties that emanate from (general) classes of tasks, perfectly defined in computational terms.", "startOffset": 197, "endOffset": 201}, {"referenceID": 68, "context": ", some results in [71] and [66] take a white-box evaluation approach).", "startOffset": 18, "endOffset": 22}, {"referenceID": 63, "context": ", some results in [71] and [66] take a white-box evaluation approach).", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "This view of a cognitive ability is consistent with its association with a \u201cclass of cognitive tasks\u201d [16] that must be \u2018representative\u2019 for the ability.", "startOffset": 102, "endOffset": 106}, {"referenceID": 123, "context": "Typically, this is studied in a hierarchical way, starting with the so-called elementary cognitive tasks [16, page 11] (closely related to the notion of primary mental abilities of [126])\u201d [59].", "startOffset": 181, "endOffset": 186}, {"referenceID": 56, "context": "Typically, this is studied in a hierarchical way, starting with the so-called elementary cognitive tasks [16, page 11] (closely related to the notion of primary mental abilities of [126])\u201d [59].", "startOffset": 189, "endOffset": 193}, {"referenceID": 67, "context": "A theoretical analysis of ability interdependency, how they can develop and the notion of potential intelligence, are still in a very incipient stage [70, 56].", "startOffset": 150, "endOffset": 158}, {"referenceID": 53, "context": "A theoretical analysis of ability interdependency, how they can develop and the notion of potential intelligence, are still in a very incipient stage [70, 56].", "startOffset": 150, "endOffset": 158}, {"referenceID": 29, "context": "There can also be objections about how universal a test can be [31].", "startOffset": 63, "endOffset": 67}, {"referenceID": 56, "context": "Item response functions and agent response functions [59] can be constructed empirically from the results and compared with the theoretical functions or any other information about \u03a9 and M .", "startOffset": 53, "endOffset": 57}, {"referenceID": 32, "context": "It is also crucial for the technological singularity once (and if) achieved [34], especially because some of the prophecies and forecasts disregard that the first thing to consider about the singularity is to have metrics to detect whether and where AI progresses towards it.", "startOffset": 76, "endOffset": 80}], "year": 2017, "abstractText": "Artificial intelligence develops techniques and systems whose performance must be evaluated on a regular basis in order to certify and foster progress in the discipline. We will describe and critically assess the different ways AI systems are evaluated. We first focus on the traditional task-oriented evaluation approach. We see that black-box (behavioural evaluation) is becoming more and more common, as AI systems are becoming more complex and unpredictable. We identify three kinds of evaluation: Human discrimination, problem benchmarks and peer confrontation. We describe the limitations of the many evaluation settings and competitions in these three categories and propose several ideas for a more systematic and robust evaluation. We then focus on a less customary (and challenging) ability-oriented evaluation approach, where a system is characterised by its (cognitive) abilities, rather than by the tasks it is designed to solve. We discuss several possibilities: the adaptation of cognitive tests used for humans and animals, the development of tests derived from algorithmic information theory or more general approaches under the perspective of universal psychometrics.", "creator": "LaTeX with hyperref package"}}}