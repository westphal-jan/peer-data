{"id": "1408.6988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2014", "title": "An Information Retrieval Approach to Short Text Conversation", "abstract": "human computer simple conversation is always regarded as one of the most difficult problems in artificial intelligence. furthermore in this paper, we address one of its key sub - problems, referred to as live short text conversation, commonly in which given a message from said human, half the computer returns a randomly reasonable response to the message. we leverage the vast amount of short sms conversation data available on social media to study the issue. importantly we propose formalizing short text conversation as a search problem at the first step, and employing state - of - the - art information retrieval ( ir ) matching techniques deployed to carry out the task. we investigate the significance as well as the limitation of the ir approach. our experiments demonstrate that the only retrieval - based model can make the knowledge system behave rather \" intelligently \", when combined with a huge repository of conversation data from legitimate social media.", "histories": [["v1", "Fri, 29 Aug 2014 12:04:15 GMT  (524kb,D)", "http://arxiv.org/abs/1408.6988v1", "21 pages, 4 figures"]], "COMMENTS": "21 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["zongcheng ji", "zhengdong lu", "hang li"], "accepted": false, "id": "1408.6988"}, "pdf": {"name": "1408.6988.pdf", "metadata": {"source": "CRF", "title": "An Information Retrieval Approach to Short Text Conversation", "authors": ["Zongcheng Jia", "Zhengdong Lub", "Hang Lib"], "emails": [], "sections": [{"heading": null, "text": "Human computer conversation is regarded as one of the most difficult problems in artificial intelligence. In this paper, we address one of its key sub-problems, referred to as short text conversation, in which given a message from human, the computer returns a reasonable response to the message. We leverage the vast amount of short conversation data available on social media to study the issue. We propose formalizing short text conversation as a search problem at the first step, and employing state-of-the-art information retrieval (IR) techniques to carry out the task. We investigate the significance as well as the limitation of the IR approach. Our experiments demonstrate that the retrieval-based model can make the system behave rather \u201cintelligently\u201d, when combined with a huge repository of conversation data from social media.\nKeywords: Short Text Conversation, Information Retrieval, Learning to Rank, Learning to Match"}, {"heading": "1. Introduction", "text": "Human computer conversation is one of the most challenging AI problems, which involves language understanding, reasoning, and use of common sense knowledge. Despite a significant amount of effort on the research in the past decades, the progress on the problem is unfortunately quite limited. One of the major reasons for that is lack of large volumes of real conversation data (Chen et al., 2011; Nouri et al., 2011).\nIn this paper, we consider a much simplified version of the problem: one round of conversation formed by two short texts, with the former being a message from human and the latter being a response to the message from the computer. We refer to it as short text conversation (STC). Thanks to the extremely large amount of short text conversation data available on social media such as Twitter1 and Weibo2, we anticipate that significant progress could be made in the research on the problem with the use of the big data, much like what has happened in machine translation, community question answering, etc.\nModeling a short text conversation is much simpler than modeling a complete dialogue, which often requires several rounds of interactions (e.g., a dialogue system as in Litman et al. (2000)). However, it can shed important light on understanding of the complicated mechanism of natural language dialogues and can significantly enhance the research toward the ultimate goal of passing the Turing test. The research on the problem will instantly help applications such as chatbot at a web site, automatic short-message reply on mobile phone, and voice assistant like Siri3. With the emergence of social media, as well as the spread of mobile devices, conversation via short texts has become an important way of communication for people in our time.\nIThis paper is an extended version of Wang et al. (2013) with the following new content: (1) empirical verification of effectiveness of several new matching models including translation model and deep matching model, and (2) proposal of topic-word model. \u2217Corresponding author. Email addresses: jizongcheng@gmail.com (Zongcheng Ji), lu.zhengdong@huawei.com (Zhengdong Lu), hangli.hl@huawei.com (Hang Li) 1http://twitter.com/ 2http://weibo.com/ 3http://en.wikipedia.org/wiki/Siri\nar X\niv :1\n40 8.\n69 88\nv1 [\ncs .I\nR ]\n2 9\nA ug\n2 01\n4\nOne simple approach to STC, and perhaps the first approach which one would want to try, is to take it as an information retrieval (IR) problem, maintain a large repository of short text conversation data, and develop a conversation system mainly based on IR technologies. Given a message, the system retrieves related responses from the repository and returns the most reasonable response. That is to say, we would not generate a new response, but select the most suitable response (originally made to other messages) as reply to the current message. We refer to the former approach as generation-based STC and the latter approach as retrieval-based STC. With advanced IR technologies and a dataset with previously unthinkable volume, we would expect that the conversation system can behave almost like a human in each round of conversation.\nRetrieval-based STC is similar to some IR tasks such as community question answering (CQA). In the former task, each instance consists of a message-response pair (or a post-comment pair in social media), while in the latter task, each instance consists of a question-answer pair. The differences are also evident, however. The messages tend to be longer than the responses in STC, while the answers tend to be longer than the questions in CQA. More importantly, the relations between texts are different in the two problems. The answers in CQA must be solutions to the questions, which mostly involves knowledge, while the responses in STC need only be explanations, opinions, or criticisms on the messages, which is more about appropriateness or human-likeness.\nIn this paper, we try to answer the question of to what extent the IR approach can effectively manipulate STC. We first propose a framework, which employs a learning to rank method for training of the ranking model and matching models as features of the ranking model. The matching models include (1) basic matching models based on cosine similarities, (2) translation model, (3) latent space model (linear model), (4) deep matching model (non-linear), and (5) topic-word model. The latent space model and deep matching model are techniques recently developed for search and question answering, and the topic-word model is devised for STC in this paper. We then conduct large scale experiments with a dataset from Weibo, which we have created and released for research on STC. Our experiments show that the retrieval-based approach can achieve fairly good performance with the precision at position one being 0.64. Experiments also show that all the matching models can significantly improve the performance. We also conduct case study to show the significance and limitation of the IR approach.\nThe contributions of this paper, which is an extension of our previous paper (Wang et al., 2013), include (1) proposal of the IR approach to STC, (2) empirical verification of effectiveness and restriction of the approach, (3) proposal of topic-word model for STC, and (4) development of public dataset for the research.\nThe rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 shows an example of conversation on Weibo. Section 4 gives the definition of retrieval-based STC and a three-stage retrieval based framework to perform STC. Section 5 and section 6 give details on the dataset and the matching features used in the framework. Section 7 and 8 describe the experimental results and case studies, respectively. Finally, the work is concluded and future research directions are identified in Section 9."}, {"heading": "2. Related Work", "text": ""}, {"heading": "2.1. Short Text Conversation", "text": "Early work on modeling dialogues is either rule-based (Weizenbaum, 1966) or learning-based (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007). These approaches require no data (e.g., rule based) or little data (e.g., reinforcement learning based) for training, but much manual effort in building the model, which is usually very costly. Furthermore, the coverage of the systems is also not satisfactory.\nAn alternative approach is to build a dialogue system with a knowledge base consisting of large number of question-answer pairs. For example, the system in Leuski et al. (2006) and Leuski and Traum (2011) selects the most suitable response to the current message from the question-answer pairs using a statistical language model in cross-lingual information retrieval. The major bottleneck of this approach is creation of the knowledge base (i.e., question-answer pairs). Instead of building knowledge base by hand, Chen et al. (2011) and Nouri et al. (2011) propose augmenting a knowledge base with question-answer pairs derived from texts using a question generation tool. The results show that the augmented system can answer questions about new topics, with certain performance drop on questions about existing topics. The number of question-answer pairs obtained in this way is still small (a few thousands). Furthermore, the statistical language model can only match questions and answers at the word level, not at the semantic level, which may hinder the performance of the system.\nAll the above systems work at small scale in the sense that they can only respond to a small variety of messages. Recently, with the fast development of social media, such as community question answering and microblog services, a very large amount of conversation data becomes available. Ritter et al. (2011) investigate the feasibility of conducting short text conversation by using statistical machine translation (SMT) techniques, as well as millions of naturally occurring conversation data in Twitter. The results show that phrase-based SMT (Koehn et al., 2007) works better than vector space model (VSM) (Salton et al., 1975) in IR in terms of BLEU score (Papineni et al., 2002). In the approach, a response is generated from a model, not retrieved from a repository, and thus it cannot be guaranteed to be a legitimate natural language text.\nIn this paper, we conduct short text conversation (STC) by leveraging the state-of-the-art IR technologies and the vast amount of conversation data on social media. A related but slightly different problem has been studied in Jafarpour et al. (2010), as an initial step for building a chatbot, referred to as learning to chat (L2C). L2C attempts to perform human computer conversation by utilizing machine learning and large scale dialogue data (each instance consists of several rounds of conversation). Their method first filters and then ranks responses to find the best candidate, on the basis of all information in the context. It is, thus, not directly comparable with our method in this paper."}, {"heading": "2.2. Search", "text": "Learning to rank and semantic matching are considered state-of-the-art techniques for search (Liu, 2009; Li, 2011a,b; Li and Xu, 2014). Given a query, documents containing the query terms are first retrieved from the index. Matching between the query and each of the documents is then carried out using different models such as traditional IR model of BM25 (Robertson et al., 1995), translation model (Berger and Lafferty, 1999), and latent space model (Wu et al., 2013). The matching scores of each document are taken as features of the document. Next, the documents are assigned scores by the ranking model on the basis of their features. Finally, the documents are sorted by their ranking scores. The ranking model is trained in advance using learning to rank, and the matching models are trained in advance using semantic matching techniques (or in general learning to match). In this paper, we employ the IR techniques for short text conversation."}, {"heading": "3. Conversation on Social Media", "text": "Weibo is a microblog service in China, similar to Twitter, on which a user can publish a short message (referred to as post in the remainder of the paper) visible to the public or a group of users following her/him. Just like Twitter, Weibo also has the length limit of 140 Chinese characters on each post. Users can attach a short message to a published post, with the same length limit, referred to as comment in this paper. Figure 1 shows an example of post and associated comments (in Chinese).\nWe argue that the post-comment pairs on Weibo provide a rather valuable resource for studying short text conversation between users. The comments to a post can be of flexible forms and diverse topics, as illustrated in the example\nin Table 1. With a post being a report about the user\u2019s current status (travelling to Hawaii), the comments can be a question about the user\u2019s future status, a request to the user, a greeting to the user, and so on, but are apparently all appropriate.\nIn many cases, the post-comment pair is self-contained, which means that one does not need any background knowledge and context information to understand the conversation (Examples of that include the comments of users B, D, G and H). In some cases, one may need extra knowledge and information to understand the conversation. For example, the comment of user E will be fairly elusive if taken out of the context that A\u2019s Hawaii trip is for an international conference and he is going to give a talk there. We argue that the number of self-contained postcomment pairs is vast, and therefore the collected post-comment pairs can serve as a rich resource for exploring rather sophisticated patterns and structures of natural language conversations."}, {"heading": "4. Retrieval-based Short Text Conversation", "text": ""}, {"heading": "4.1. Problem Definition", "text": "Short text conversation (STC) is defined as one round of conversation via two short texts, with the former being a message from human and the latter being a response to the message given by the computer. As the first step, we formalize STC as an information retrieval (IR) problem, i.e., conduct retrieval-based STC. Given a message (query), the system retrieves related responses from the large repository of conversation data and returns the most reasonable response. With advanced IR technologies and a dataset with previously unthinkable volume, we would expect the conversation system can behave almost like a human in each round of conversation.\nFormally, for a given query q, we select from the repository of post-comment pairs (p, r) the response r with the highest ranking score.\nr\u2217 = arg max (p,r) score(q, (p, r)) (1)\nwhere the score is an ensemble of individual matching features.\nscore(q, (p, r)) = \u2211 i\u2208\u2126 \u03c9i\u03a6i(q, (p, r)) (2)\nwhere \u03a6i(q, (p, r) is the score of the i-th matching feature and \u03c9i is the corresponding feature weight."}, {"heading": "4.2. System Architecture", "text": "The system performs retrieval-based short text conversation in three-stages, as illustrated in Figure 2:\n\u2022 Stage I (retrieval), the system employs three fast basic linear matching models (see Section 6.1) to retrieve a number of candidate post-comment pairs for the given query q, forming a reduced candidate set C(reduced)q .\n\u2022 Stage II (matching), the system utilizes more matching models (see Section 6.2 \u223c 6.5) to further evaluate all the comments in C(reduced)q , returning a matching feature set {\u03a6i(q, (p, r), i \u2208 \u2126} for each candidate post-comment pair. The matching models are learned offline with techniques referred to as learning to match (cf., Section 6 for details).\n\u2022 Stage III (ranking), the system uses a linear ranking function defined in Equation (2) with the matching models as features to further evaluate all the comments (responses) in C(reduced)q , and assigns a ranking score to each candidate comment. Then, the system ranks the candidate comments based on their scores and selects the comment with the highest score to respond. The linear ranking function is learned offline with learning to rank techniques."}, {"heading": "4.3. Learning of Ranking Model", "text": "We employ linear RankingSVM (Herbrich et al., 1999), a state-of-the-art method of learning to rank, to train the ranking model. We use as training data labeled post-comment pairs, as explained in Section 5.2. From the labeled data, we derive pairwise preference data (q, (p, r)+, (p, r)\u2212) such that score(q, (p, r)+) > score(q, (p, r)\u2212). Specifically, (p, r)+ are selected from the labeled positive instances with respect to q, while (p, r)\u2212 are selected from the labeled negative instances. We have confirmed that the use of labeled negative instances, instead of randomly selected instances, can yield slightly better results.4"}, {"heading": "5. Short Text Conversation Dataset", "text": "This section introduces the dataset used for retrieval-based STC.5 There are (1) original post-comment pairs used as the retrieval repository, and (2) labeled post-comment pairs used for training and testing different retrieval models. We give the detail of creation in the following subsections."}, {"heading": "5.1. Original Post-Comment Pairs", "text": "The original post-comment pairs are sampled from Weibo posts published by users in a loosely connected community and the comments they received (may not be from this community). The creation process of the dataset, as illustrated in Figure 3, consists of three consecutive steps: (1) crawling the community of users, (2) crawling their posts and the associated comments, and (3) cleaning the data, with more details described below.\n4This is because the negative instances are collected from the top ranked candidates with several simple retrieval models, and thus they are more indicative of the difference between positive and negative instances.\n5We have got permission from Weibo to release the dataset for research purpose and it is available at http://data.noahlab.com.hk/conversation/."}, {"heading": "5.1.1. Sampling Strategy", "text": "We take the following sampling strategy for collecting the post-comment pairs to make the topic relatively focused. We first locate 3,200 users from a loosely connected community of Natural Language Processing (NLP) and Machine Learning (ML) in China. The community is mainly composed of professors, researchers, and students. This is done through crawling followees6 of ten manually selected seed users who are NLP researchers active on Weibo (with no less than 2 posts per day on average) and popular enough (with no less than 100 followers).\nWe crawl the posts and the associated comments (not necessarily from the crawled community) for two months (from April 5th, 2013 to June 5th, 2013). The topics are relatively limited due to our choice of the users, with the most saliently ones being:\n\u2022 Research: discussion on research ideas, papers, books, tutorials, conferences, and researchers in NLP and ML, etc;\n\u2022 General Arts and Science: mathematics, physics, biology, music, painting, etc;\n\u2022 IT Technology: mobile phones, IT companies, jobs opportunities, etc;\n\u2022 Life: traveling (both touring or conference trips), food, photography, etc."}, {"heading": "5.1.2. Processing, Filtering, and Data Cleaning", "text": "On the crawled posts and comments, we first perform a four-step filtering on the posts and comments:\n\u2022 We first remove a post-comment pair if the length of the post is less than 10 Chinese characters or the length of the comment is less than 5 Chinese characters. The reason for that is two-fold: (1) if the text is too short, it can barely contain information that can be reliably captured, e.g. the following example\nPost \u4e09\u4e2a\u4e86\uff0c\u8fd8\u5dee\u4e24\u4e2a\u3002 Three down, two to go.\nand (2) some of the posts or comments are too general to be interesting for other cases, e.g. the comment in the example below\nPost \u9910\u5385\u4e0d\u9519\uff0c\u5f3a\u70c8\u63a8\u8350\u3002\u9664\u4e86\u6392\u961f\u7b49\u5f85\uff0c\u8fd9\u91cc\u4ec0\u4e48\u90fd\u597d\u3002 Nice restaurant. I\u2019d strongly recommend it. Everything here is good except the long waiting line.\nComment \u54c7\uff01 Wow.\n6When user A follows user B, A is called B\u2019s follower, and B is called A\u2019s followee.\nThen, for the remained posts and comments, we remove punctuation marks and emotions, and use ICTCLAS (Zhang et al., 2003) for Chinese word segmentation.\nFinally, we get 38,016 Weibo posts and their corresponding 618,104 comments, forming 618,104 original postcomment pairs, which are used as the retrieval repository in all the experiments. The statistics of the dataset are summarized in Table 2."}, {"heading": "5.2. Labeled Post-Comment Pairs", "text": "This subsection introduces the creation process of the labeled post-comment pairs, which are used for training and testing different retrieval models for STC.\nWe employ a pooling strategy widely used in information retrieval for getting the instances to label (Voorhees, 2002). More specifically, for a given query, we use each of three basic retrieval models to select top 10 comments (see Section 6.1 for the description of the basic matching models), and merge them to form a much reduced candidate set with size \u2264 30. Then we assign the comments in the reduced candidate set into \u201csuitable\u201d and \u201cunsuitable\u201d categories. Basically we consider a comment suitable for a given query if we cannot tell whether it is an original comment. More specifically the suitability of a comment is judged based on the following three criteria7:\n\u2022 Semantic Relevance: This requires the content of the comment to be semantically relevant to the post. As shown in the example right below, the query is about soccer, and so is comment 1, and hence it is semantically relevant, whereas comment 2 is about food and hence semantically irrelevant.\nQuery \u82f1\u683c\u5170\u7981\u533a\u91cc\u8001\u662f\u516b\u4e2a\u4eba. . . . . . There are always 8 English players in their own penalty area. Unbelievable!\nComment 1 \u54c8\u54c8\u54c8\u4ecd\u7136\u662f0\uff1a0\u3002\u8fd8\u6ca1\u770b\u5230\u8fdb\u7403\u3002 Haha, it is still 0:0, no goal so far. Comment 2 \u82f1\u683c\u5170\u7684\u98df\u7269\u771f\u53ef\u6015! The food in England is horrible.\nAnother important aspect of semantic relevance is entity association. This requires that the entities in the comment to be strongly associated with those in the query. In other words, if the query is about entity A, while\n7Note that although our criteria in general favor short and general comments like \u201c\u8bf4\u5f97\u5f88\u597d\uff01(Well said!)\u201d or \u201c\u592a\u597d\u4e86\uff01(Nice!)\u201d, most of the general comments have already been filtered out due to their lengths (see Section 5.1.2).\nthe comment is about entity B, they are very likely to be mismatched. As shown in the following example, where the query is about Paris, while comment 2 talks about London:\nQuery \u5df4\u9ece\u7684\u5929\u7a7a\uff01\u6696\u6696\u7684\u592a\u9633+\u9635\u9635\u7684\u5fae\u98ce\uff0c\u771f\u8ba9\u4eba\u4e0d\u820d\u5f97\u79bb\u5f00\u3002 The sky in Paris! Warm sun + bursts of breeze. Really hate to leave.\nComment 1 \u597d\u597d\u4eab\u53d7\u4f60\u5728\u5df4\u9ece\u7684\u65f6\u95f4\u5427\u3002 Enjoy your time in Paris.\nComment 2 \u6211\u5e0c\u671b\u6211\u73b0\u5728\u5728\u4f26\u6566\u3002 Man, I wish I am in London right now.\nThis is however not absolute, since a comment containing a different entity could still be sound, as demonstrated by the following two comments to the query above:\nComment 1 \u597d\u597d\u4eab\u53d7\u4f60\u5728\u6cd5\u56fd\u7684\u65f6\u95f4\u5427\u3002 Enjoy your time in France.\nComment 2 \u4f26\u6566\u7684\u79cb\u5929\u4e5f\u5f88\u6f02\u4eae\u3002 The fall of London is nice too.\n\u2022 Logic Consistency: This requires the content of the comment to be logically consistent with the query. For example, in the table right below, the query states that the Huawei mobile phone \u201cHonor\u201d is already in the market of mainland China. Comment 1 talks about a personal preference over the same phone model (hence logically consistent), whereas comment 2 asks a question to which the answer is already clear from the query (hence logically inconsistent).\nQuery \u534e\u4e3a\u8363\u8000\u624b\u673a\u5728\u4e2d\u56fd\u5927\u9646\u5927\u5356\uff01 HUAWEI\u2019s mobile phone, Honor, sells well in mainland China.\nComment 1 \u534e\u4e3a\u8363\u8000\u624b\u673a\u662f\u6211\u6700\u559c\u6b22\u7684\u4e00\u6b3e\u624b\u673a\u3002 HUAWEI Honor is my favorite phone\nComment 2 \u534e\u4e3a\u8363\u8000\u624b\u673a\u4ec0\u4e48\u65f6\u5019\u4f1a\u5728\u4e2d\u56fd\u5927\u9646\u4e0a\u5e02\uff1f When will HUAWEI Honor get to the market in mainland China?\n\u2022 Speech Act Agreement: Another important factor in determining the suitability of a comment is speech act. For example, when a question is posed in the Weibo post, a certain act (e.g., answering or forwarding it) is expected. In the example below, the query asks a special question about location. Comment 1 forwards and comments 2 answers the question, whereas comment 3 is an imperative sentence and therefore does not correspond well in speech act.\nQuery \u6709\u4eba\u77e5\u9053\u540e\u5e74\u7684KDD\u4f1a\u5728\u54ea\u91cc\u4e3e\u884c\u5462\uff1f Any one knows where KDD will be held the year after next?\nComment 1 \u540c\u95ee\u3002\u5e0c\u671b\u5728\u6b27\u6d32 co-ask. Hopefully Europe\nComment 2 \u542c\u8bf4\u5728\u7ebd\u7ea6 New York, as I heard Comment 3 \u8bf7\u5e2e\u5fd9\u6269\u6563\u8fd9\u6761\u5173\u4e8eKDD\u7684\u4fe1\u606f\u3002 Please help me distribute the information of KDD.\nFinally, we manually label 422 queries and their associated comments, about 30 comments for each post. Note that (1) the labeling is only on a small subset of the 38,016 posts, and (2) for each selected (query) post, the labeled comments are not originally given to it. The statistics of this part of dataset are also summarized in Table 2."}, {"heading": "6. Matching Features", "text": "In this section, we introduce matching features (calculated from matching models) used in our retrieval-based STC. Three basic linear matching models are first introduced as the baselines. Then, a translation-based language model (TransLM) is introduced for alleviating the lexical gap problem. Next, a deep matching model (DeepMatch) is described, which matches query and response (comment) with a deep architecture. After that, a topic-word model (TopicWord) is explained for addressing the topic alignment of query and response (comment). Finally, other simple matching features used in our retrieval-based STC are described."}, {"heading": "6.1. Basic Linear Matching Models", "text": "We use the following three basic linear matching models for fast retrieval in Stage I. Moreover, these matching models are also used in Stage II to generate three matching features for each post-comment pair."}, {"heading": "6.1.1. Query-Response Similarity", "text": "Here we use a simple vector space model for measuring the similarity between a query q and a candidate response r\nsimQ2R(q, r) = qTr \u2016q\u2016\u2016r\u2016 (3)\nwhere q and r are respectively the TF-IDF vectors of q and r. Although it is not necessarily true that a good response has many common words as the query, but this measurement is often helpful in finding relevant responses. For example, when the query and the candidate response both have \u201cNational Palace Museum in Taipei\u201d, it is a strong signal that they are about similar topics. Unlike other semantic matching features, this simple similarity requires no learning and works on infrequent words. Our empirical results show that it can often capture the query-response relation which semantic matching features cannot."}, {"heading": "6.1.2. Query-Post Similarity", "text": "The basic idea here is to find posts (messages) similar to the query q and use their comments (responses) as the candidates. Again we use the vector space model for measuring the query-post similarity\nsimQ2P(q,p) = qTp \u2016q\u2016\u2016p\u2016 (4)\nwhere q and p are respectively the TF-IDF vectors of q and p. The assumption here is that if a post p is similar to the query q, its associated comments (responses) might be appropriate for q. It however often fails, especially when a response to p addresses parts of p not contained by q, which fortunately can be alleviated when combined with other measures."}, {"heading": "6.1.3. Query-Response Matching in Latent Space", "text": "This particular matching function relies on mapping of posts and responses in the original vector spaces to a lowdimensional latent space, learned from data. The matching score between a query q and a candidate response r can be measured as the inner product between their images in the latent space.\nLatentMatch(q, r) = qTLqLTr r (5)\nThis is to capture the semantic matching between a post and a response, which may not be well captured by a word-to-word matching. We find the mapping functions Lq and Lr through a large number of query-response pairs and a large margin variant of the method in Wu et al. (2013).\narg min Lq,Lr \u2211 i max(1 \u2212 \u2211 i qTi LqL T r ri, 0)\ns.t. \u2016Ln,q\u20161 \u2264 \u00b51, n = 1, 2, ...,Nq \u2016Lm,r\u20161 \u2264 \u00b51,m = 1, 2, ...,Nr \u2016Ln,q\u20162 = \u00b52, n = 1, 2, ...,Nq \u2016Lm,r\u20162 = \u00b52,m = 1, 2, ...,Nr\n(6)\nwhere i indices the original post-comment pairs. Our experiments (see Table 14) indicate that this simple linear model can learn meaningful semantic matching patterns, due to its effective use of massive data. For example, the image of the word \u201cItaly\u201d in the post in the latent space matches well the words \u201cSicily\u201d, \u201cMediterranean sea\u201d and \u201ctravel\u201d. Once the mapping Lq and Lr are learned, the semantic matching score qTLqLTr r will be utilized as a feature for modeling the overall suitability of r as a response to q."}, {"heading": "6.2. Translation-based Language Model", "text": ""}, {"heading": "6.2.1. Motivation", "text": "With the three basic matching models and some simple features (see Section 6.5), the model for retrieval-based STC can achieve fairly good performance (see Section 7.2). However, there are also some cases in which the model fails. One of the serious problems is the lexical gap, which is the major challenge for most information retrieval tasks, between the query and the candidate post-comment pairs. Table 3 shows a real example of the lexical gap problem. Two candidate responses are suitable to the query, while their ranking is very low in the top 30 candidates. The main reason is that there is no word overlap between the candidate responses and the query, although there is one common word \u201c\u665a\u5b89(Good Night)\u201d between the original posts and the query."}, {"heading": "6.2.2. Model Description", "text": "To alleviate the lexical gap problem, we employ the state-of-the-art translation-based language model (TransLM) (Xue et al., 2008) with a small modification for retrieval-based STC. Given a query q and a candidate post-comment pair (p, r), the ranking function based on TransLM is written as\nPTransLM(q|(p, r)) = \u220f w\u2208q PTransLM(w|(p, r)) (7)\nPTransLM(w|(p, r)) = (1 \u2212 \u03b1)Pmx(w|(p, r)) + \u03b1Pml(w|C) (8)\nPmx(w|(p, r)) =(1 \u2212 \u03b2) (1 \u2212 \u03b3)Pml(w|p) + \u03b3\u2211\nt\u2208p T (w|t)Pml(t|p)  + \u03b2\n(1 \u2212 \u03b3)Pml(w|r) + \u03b3\u2211 t\u2208r T (w|t)Pml(t|r) \n(9)\nwhere Pml(w|p), Pml(w|r), and Pml(w|C) are the unigram language models (LM), which are estimated with maximum likelihood, for the post part p, the response part r and the whole collection C, respectively. T (w|t) is the probability of translating a word t in p or r into a word w in q. \u2211 t\u2208p T (w|t)Pml(t|p) and \u2211 t\u2208r T (w|t)Pml(t|r) are the translation models (Trans) for the post part and response part, respectively. To bridge the lexical gap between queries and candidate post-comment pairs, the two translation models allow a post and its response translate any one of their words t to a different but semantically related query word w with a non-zero probability. \u03b1 is the Jelinek-Mercer smoothing factor (Zhai and Lafferty, 2001). \u03b2 is the interpolation parameter between the post model and the response model. \u03b3 is the parameter to balance between the unigram language model and the translation model for alleviating the lexical gap problem and the self-translation problem (Xue et al., 2008).\nThe main difference between our TransLM for retrieval-based STC and that for question retrieval in Community Question Answering (CQA) (Xue et al., 2008) is that we add an additional part \u2211 t\u2208r T (w|t)Pml(t|r) in the response part while Xue et al. (2008) does not do that in the answer part. The reasons are two-fold: (1) we focus on finding suitable responses given a query while Xue et al. (2008) focus on finding similar questions given a query question, and (2) the responses tend to be shorter than the posts in STC while the answers tent to be longer than the questions in CQA."}, {"heading": "6.2.3. Learning Translation Probabilities", "text": "The performance of the translation-based language model relies on the quality of the word-to-word translation probabilities. We follow the method of Xue et al. (2008) to learn the word translation probabilities. In our experiments, original post-comment pairs are used for training, and the GIZA++8 (Och and Ney, 2003) toolkit is used to learn the translation model. There are two different settings of source and target for constructing the parallel corpus: (1) set the posts as the source and the responses as the target, i.e., collection (p, r)1, ..., (p, r)n, and (2) set the responses as the source and the posts as the target, i.e., collection (r, p)1, ..., (r, p)n. Following Xue et al. (2008), a pooling strategy is adopted to combine the above two collections of pairs to get a pooled collection (p, r)1, ..., (p, r)n, (r, p)1, ..., (r, p)n. Moreover, we also filter low-frequency words, whose frequency is less than 10."}, {"heading": "6.3. Deep Matching Model", "text": ""}, {"heading": "6.3.1. Motivation", "text": "The matching models above are linear models. These models, although proven to be effective, are insufficient for capturing the rich structures in matching complicated objects like texts. We propose employing a new deep neural network model, referred to it as deep matching model (DeepMatch) in the paper, to model the complicated matching relations between query and candidate response in retrieval-based STC.9 This new architecture is mainly based on the following two intuitions:\n\u2022 Localness: There are salient local structures in the semantic space of parallel texts to be matched, which can be roughly captured by the co-occurrence pattern of words. This localness however should not prevent two \u201cdistant\u201d components from correlating with each other on a higher level, hence call for the hierarchical characteristic of the model;\n\u2022 Hierarchy: The decision making for matching has different levels of abstraction. The local decisions, capturing the interactions between semantically related words, will be combined later layer-by-layer to form the final and global decision on matching."}, {"heading": "6.3.2. Model Description", "text": "The deep matching model (DeepMatch) consists of two parts: (1) many bilinear local matching models, and (2) a deep neural network to further combine the local matching models to generate the final matching score. Each local matching model (indexed k) is in charge of a small subset of words in both short texts (x and y) and a pair of projection matrices (L(k)x , L (k) y ). The score from the kth matching model is given as follows\na(k)(x, y) = f (k) ( (x(k))>L(k)x (L (k) y ) >y(k) + b(k) ) , k = 1, \u00b7 \u00b7 \u00b7 ,K (10)\n8https://code.google.com/p/giza-pp/ 9The detail can be found in Lu and Li (2013).\nwhere f (k)(\u00b7) is a function with sigmoid shape for mapping the matching score to (0, 1). Those local matching decisions are then used as input to feed into a multi-layer perceptron to calculate the final matching score. The overall architecture of DeepMatch is given in Figure 4."}, {"heading": "6.3.3. Model Training", "text": "The training of DeepMatch is divided into two phases: (1) bilingual topic modeling for finding potentially matched subsets (topics) of word-pairs and building the model architecture, and (2) training of the parameters of the local topic models and deep neural network. In this paper, we train the deep matching model with a ranking-based objective. More specially, we employ a large margin objective defined on preference pairs in ranking. Suppose that we are given the following triples (x, y+, y\u2212) from the oracle, with x (\u2208 X) matched with y+ better than with y\u2212 (both \u2208 Y). The ranking-based objective is defined as follows\nL(W,Dtrn) = \u2211\n(xi,y+i ,y \u2212 i )\u2208Dtrn\neW(xi, y+i , y \u2212 i ) + R(W), (11)\nwhere R(W) is the regularization term and eW(xi, y+i , y\u2212i ) is the error for triple (xi, y+i , y\u2212i ), given by the following large margin form:\nei = eW(xi, y+i , y \u2212 i ) = max(0,m + s(xi, y \u2212 i ) \u2212 s(xi, y+i )),\nwith parameter 0 < m controlling the margin. In the experiments we use m = 2, but we find that the results are rather insensitive to the values of m varying in a fairly large range."}, {"heading": "6.4. Topic-Word Model", "text": ""}, {"heading": "6.4.1. Motivation", "text": "The matching models above are mainly for representing semantic relevance between query and response. They do not capture the matching relations between the main topics of the query and the response. Table 4 shows a real example of the problem. The top 2 candidate responses are unsuitable to the query, while the first suitable response is ranked at 9. The main reason is that the word \u201c\u83dc\u9e1f(rookie)\u201d has higher term frequencies in the query and the unsuitable responses, and thus dominates their matching scores. However, the main topics of the query is not \u201c\u83dc\u9e1f(rookie)\u201d but \u201c\u4ee3\u7801\u63a7\u5236\u5de5\u5177(code control tool) SVN GIT\u201d, and thus the top 2 candidate responses are not suitable. One possible solution is to identify the topic words of the query and the candidate responses and give higher weights to the matching of the topic words. This may alleviate the topic mismatch problem and improve the performance of STC. In Table 4, the topic words of the query and the candidate responses are shown in bold. After identifying the topic words, the first two responses clearly become unsuitable to the query, while the third response having the same topic becomes suitable.\nOne key question is how to differentiate topic words from the other words in a short text. We propose a method for the task in the following subsections."}, {"heading": "6.4.2. Learning Topic Words", "text": "A short text, such as a post or a comment in Weibo, usually centers around a specific theme, which is usually captured by a number of words in the text. We refer to the words as topic words. Examples are show in Table 4. In this paper, we employ a probabilistic approach based on logistic regression to compute the probability of a word being the topic word of a short text. In the logistic regression model, we specifically compute P(topic|w) as follows, where w denotes a word and topic denotes a binary variable representing whether or not w is a topic word.\nlog (\nP(topic|w) 1 \u2212 P(topic|w)\n) = ~\u03c9 \u00b7 ~x + c (12)\nP(topic|w) = e ~\u03c9\u00b7~x+c\n1 + e~\u03c9\u00b7~x+c (13)\nwhere ~x is a short text, such as a post or a comment in Weibo, represented as a vector of features, ~\u03c9 is a vector of weights associated with the features, and c is a constant.\nIn the logistic regression model we make use of the features, as listed in Table 5. The first two features (TF, IDF) are from the traditional term weighting schemes in information retrieval. The third feature (SF) is based on the observation that words appearing in multiple sentences in the short text are more likely to be topic words. The next two features (First, Last) represent the positions of word in the short text. Our observation is that topic words are most likely to appear in the first or the last sentence. The next three features (NE, NE First, NE Last) are named entity related features. The last feature (POS) is the part of speech of a word, which is based on the fact that topic words are usually denoted by nouns and verbs.\nTo create the training data, we randomly select 200 short texts (including posts and comments) to label their topic words. We make labeling judgments on all words in a short text (except some stop words), as our model is applied to words. Each word is assigned to one of the two classes, \u201cpositive\u201d class and \u201cnegative\u201d class, depending on whether or not it is a topic word. The judgments are made based on the principle that the topic words of a short text should be indicative to the main theme of the short text. Finally, we obtain 2,008 words for the 200 short texts. Table 6 summarizes the statistics of the dataset and Table 7 lists the topic word distributions over word positions, parts of speech and named entities in the dataset. As shown in Table 7, topic words are typically nouns in the first sentence of a short text.\nWe use ICTCLAS (Zhang et al., 2003) to obtain the part of speech and named entity information and LIBLINEAR (Fan et al., 2008) to build the logistic regression model and predict the probability of a word being a topic word. The accuracy of our model is 81.57%."}, {"heading": "6.4.3. Model Description", "text": "With the trained model, we can assign a probability value to each word in the short text, which indicates the probability of the word being a topic word. The question is how to use this information in retrieval-based STC. In this work, we simply take the probability values as term weights and use them to calculate the similarities between a query q and a candidate response r or its original post p with a vector space model, which are similar to the query-response similarity and query-post similarity introduced in Section 6.1.\nsimQ2R TopicWord(q, r) = qTr \u2016q\u2016\u2016r\u2016 (14)\nsimQ2P TopicWord(q,p) = qTp \u2016q\u2016\u2016p\u2016 (15)\nwhere q, p and r are respectively the vectors of q, p and r with topic word probability P(topic|w) as weight."}, {"heading": "6.5. Other Simple Matching Features", "text": "We also use some other simple matching features as follows:\n\u2022 Longest Common String (LCS): this feature measures the length of the longest common string between the query q and the candidate response r, which is useful for capturing the quotes common in microblog comments and is also fairly robust to errors in Chinese word segmentation.\nLCS Q2R(q, r) = |LCS (q, r)| (16)\n\u2022 Co-occurrence features: these features represent the size, the rate, the sum, and the average of IDF values of the co-occurring words between the query q and the candidate response r or its original post p.\ncooccur size(x, y) = |cooccur(x, y)| (17)\ncooccur rate(x, y) = |cooccur(x, y)|\n|y| (18)\ncooccur sumIDF(x, y) = \u2211\nw\u2208cooccur(x,y) IDF(w) (19)\ncooccur averageIDF(x, y) = \u2211\nw\u2208cooccur(x,y) IDF(w) |cooccur(x, y)| (20)\nwhere x stands for the query q and y stands for the candidate response r or its original post p, cooccur(x, y) is the set of common words between x and y."}, {"heading": "7. Experiments", "text": "We conduct experiments and report results on retrieval-based STC with the dataset we have created."}, {"heading": "7.1. Experimental Setup", "text": ""}, {"heading": "7.1.1. Evaluation Metrics", "text": "We evaluate the performance of different retrieval models for STC based on the following two metrics: Mean Average Precision (MAP) and Precision@1 (P@1). MAP rewards methods that return suitable responses on the top and also rewards methods that return correct ranking of responses. P@1 reports the fraction of suitable responses among the top 1 responses retrieved. All the results reported below are based on 5-fold cross-validation on the 422 queries. We also perform a significance test using a paired t-test with a significant level of 0.05.\nWe measure the performance of the logistic regression classifier for learning topic words in terms of Accuracy based on 5-fold cross-validation on the 2008 labeled words."}, {"heading": "7.1.2. Parameter Settings", "text": "There are five parameters to be set in our experiments. We tune the best parameters with 5-fold cross-validation. Finally, we set \u03b1 = 0.8 as the Jelinek-Mercer smoothing factor in Equation (8), \u03b2 = 0.9 to interpolate the post model and the response model in Equation (9), \u03b3 = 0.5 to interpolate the unigram language model and the translation model in Equation (9). We set c = 0 in Equation (12). When training the weighs of features in Equation (2) with linear RankingSVM (Herbrich et al., 1999), we use a fixed penalty parameter (i.e., 50), as the performance is fairly insensitive to the choice of the parameter."}, {"heading": "7.2. Results of Basic Linear Matching Models", "text": "We first evaluate the performance of the three basic linear matching models combined with the simple matching features in the learning to rank framework with Equation (2).\nThe results are shown in Table 8. In the table, Q2R stands for the features based on the query-response similarity (Section 6.1.1) and query-response related simple matching features (Section 6.5). Q2P stands for the features based on the query-post similarity (Section 6.1.2) and query-post related simple matching features (Section 6.5). LatentMatch stands for the latent matching feature introduced in Section 6.1.3. As shown in the table, combining the three basic linear matching models with all the simple matching features achieves fairly good performance (row 4) and we name this model as Baseline, which will be used in the following subsections. In particular, we find that the LatentMatch feature helps slightly improve the overall performance on P@1."}, {"heading": "7.3. Results of Combining all the Features", "text": "Then, we further incorporate TransLM, DeepMatch and TopicWord as matching features into the learning to rank framework with Equation (2).\nTable 9 shows the comparison of different combinations of the matching features for retrieval-based STC. Baseline stands for the model, which combines the three basic linear matching models with all the simple matching features, based on the learning to rank framework (see row 4 in Table 8). From the table, we can see that the three new matching features can significantly improve the retrieval performance. When combining all the three new features with Baseline, the model achieves the best performance, which outperforms Baseline by 3.3 percent and 6.3 percent in terms of MAP and P@1, respectively (row 12 vs. row 5).\nIn order to clearly see the contributions of the three new features, we make comparisons between the model with and without each of the features. Table 10, 11 and 12 show the contributions of TransLM, DeepMatch and TopicWord, respectively. Take Table 10 as example, X and X+TransLM stands for the models without and with TransLM, respectively. X includes Baseline, Baseline+DeepMatch, Baseline+TopicWord, and Baseline+DeepMatch +TopicWord. %impr means the percentage of improvements of X+TransLM over X in terms of MAP and P@1. Table 11 and 12 are similar to Table 10. From the three tables, we have the following findings:\n\u2022 The feature TransLM can bring at least 1.3 and 3.1 percent improvements in terms of MAP and P@1, respectively.\n\u2022 The feature DeepMatch can bring at least 1.3 percent improvements in terms of P@1, although it brings no much improvement in terms of MAP.\n\u2022 The feature TopicWord can bring at least 1.1 and 1.2 percent improvements in terms of MAP and P@1, respectively.\nFrom the above analysis, we find that all the three features make significant contributions to the enhancement of performance. In addition, TransLM contributes the most, TopicWord the second largest, and DeepMatch the least."}, {"heading": "X 0.621 0.628 0.635 0.641", "text": ""}, {"heading": "X 0.574 0.587 0.586 0.606", "text": ""}, {"heading": "8. Case Study", "text": "To get a better understanding of the effectiveness of the matching features, we conduct case study of the features. We illustrate the results through several examples. Section 8.1 shows the effectiveness of the basic linear matching features. Sections 8.2, 8.3, and 8.4 show the effectiveness of using translation-based language model, deep matching model and topic-word model. Section 8.5 gives several examples which are not addressed well with our current model and we will leave them to future work."}, {"heading": "8.1. The Effectiveness of Basic Linear Matching", "text": "The basic linear matching features are mostly vector-space based, which are fairly good at capturing semantic relevance, as illustrated in Table 13. The suitable responses are retrieved mainly because they have common words with the queries. The experiments also show that we may find interesting and suitable responses that have no common words with the query, as show in the example in Table 14."}, {"heading": "8.2. The Effectiveness of Translation-based Language Model", "text": "The experimental results show that TransLM has superior performance when used as a feature in the learning to rank framework. Candidate post-comment pairs that do not share many common words with the query tend to be ranked low by the other matching models. However, the translation-based model is able to fill the lexical gap and find\nlexically dissimilar but semantically similar post-comment pairs, and rank them high. Table 15 gives some retrieved post-comment pairs for a given query. We can see that the model with TransLM as one of the features can rank the suitable responses higher than that without TransLM. This is due to the word translation probabilities: T (\u5915\u9633|\u65e5 \u843d)=0.018, T (\u5915\u9633|\u9ec4\u660f)=0.012, and T (\u897f\u897f\u91cc\u5c9b|\u6b27\u6d32)=0.001."}, {"heading": "8.3. The Effectiveness of Deep Matching Model", "text": "Table 16 shows some retrieved responses for a given query. From the table, we can see that although the two suitable responses share almost no common words with the query, the model with DeepMatch as one of the features can match them well and rank them higher than that without the feature."}, {"heading": "8.4. The Effectiveness of Topic-Word Model", "text": "Table 17 gives some retrieved responses for a given query. From the table, we can clearly see that the unsuitable responses, which do not share topic words with the query, are ranked lower when using TopicWord as one of the features; while the suitable response, which share topic words with the query, is ranked higher when using TopicWord as one of the features. More specifically, the word \u201c\u83dc\u9e1f(rookie)\u201d in the query is not a topic word, thus has a low term weight (i.e., the probability of being a topic word). Although the word \u201c\u83dc\u9e1f(rookie)\u201d in the unsuitable responses is\na topic word with a high term weight, the cosine similarities between the query and the two unsuitable responses are still not high after using the term weighting of topic words. Moreover, the suitable response is ranked higher mainly because it has common topic words \u201c\u63a7\u5236(control),\u5de5\u5177(tool)\u201d, with the query, which are assigned higher weights."}, {"heading": "8.5. Some Failed Issues", "text": "In this subsection, we show several issues which cannot be addressed well with our current model and we will leave them to future work."}, {"heading": "8.5.1. Entity Association", "text": "Entity association is only partially addressed with features like query-response cosine similarity, with entity names treated as words, which is apparently not enough for preventing the following type of mistakes (see Table 18) when the candidate response and the query match well on other parts. Actually, for query 1, we only need to modify the word \u201c\u674e\u6559\u6388(Prof. Li)\u201d in the unsuitable response 1 to the word \u201c\u738b\u6559\u6388(Prof. Wang)\u201d, to get expected response 1, which is suitable to the query 1. For query 2, we only need to modify the word \u201c\u753b(drawing)\u201d in the unsuitable response 2 to the word \u201c\u74f7\u5668(china)\u201d, to get expected response 2, which is suitable to the query 2."}, {"heading": "8.5.2. Logic Consistency", "text": "Our current model does not directly maintain the logic consistency between the candidate response and the query, since logic consistency requires a deeper analysis of the texts, and therefore hard to implement. Table 19 shows two examples which are semantically relevant, and correct with respect to speech act, but logically inappropriate."}, {"heading": "9. Conclusions", "text": "In this paper we have proposed a retrieval-based model for short text conversation (STC), to leverage massive data collected from social media. Our experiments show that the retrieval-based model performs reasonably well, when combined with a set of carefully designed matching features and a huge repository of conversation data.\nThis work opens to several interesting directions for future work with regard to STC. When performing retrievalbased STC, we need to consider matching between query and response in terms of semantic relevance. In addition, we may also need to consider matching between query and response in terms of speech act, sentiment, entity association, logic consistency and discourse structure. How to model these factors and how to enhance the accuracy based on the factors in STC are open and challenging issues."}], "references": [{"title": "Information retrieval as statistical translation", "author": ["A. Berger", "J. Lafferty"], "venue": "Proceedings of the 22Nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. SIGIR \u201999. ACM,", "citeRegEx": "Berger and Lafferty,? \\Q1999\\E", "shortCiteRegEx": "Berger and Lafferty", "year": 1999}, {"title": "Evaluating conversational characters created through question generation", "author": ["G. Chen", "E. Tosch", "R. Artstein", "A. Leuski", "D.R. Traum"], "venue": "FLAIRS Conference", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan", "R.-E.", "Chang", "K.-W.", "Hsieh", "C.-J.", "Wang", "X.-R.", "Lin", "C.-J.", "Jun."], "venue": "Journal of Machine Learning Research 9, 1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Herbrich et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 1999}, {"title": "Filter, rank, and transfer the knowledge: Learning to chat", "author": ["S. Jafarpour", "C.J. Burges", "A. Ritter"], "venue": "Advances in Ranking,", "citeRegEx": "Jafarpour et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jafarpour et al\\.", "year": 2010}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. ACL \u201907", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Building effective question answering characters", "author": ["A. Leuski", "R. Patel", "D. Traum", "B. Kennedy"], "venue": "Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue. SigDIAL \u201906. Association for Computational Linguistics,", "citeRegEx": "Leuski et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Leuski et al\\.", "year": 2006}, {"title": "Npceditor: Creating virtual human dialogue using information retrieval techniques", "author": ["A. Leuski", "D. Traum"], "venue": "AI Magazine", "citeRegEx": "Leuski and Traum,? \\Q2011\\E", "shortCiteRegEx": "Leuski and Traum", "year": 2011}, {"title": "A short introduction to learning to rank", "author": ["H. Li"], "venue": "IEICE Transactions on Information and Systems", "citeRegEx": "Li,? \\Q2011\\E", "shortCiteRegEx": "Li", "year": 2011}, {"title": "Learning to rank for information retrieval. Foundations and Trends in Information Retrieval", "author": ["PA Stroudsburg"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Stroudsburg,? \\Q2009\\E", "shortCiteRegEx": "Stroudsburg", "year": 2009}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney", "Mar"], "venue": "Computational Linguistics", "citeRegEx": "Och et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Och et al\\.", "year": 2011}, {"title": "Data-driven response generation in social media", "author": ["A. 311\u2013318. Ritter", "C. Cherry", "W.B. Dolan"], "venue": "Proceedings of the Conference on Empirical", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A vector space model for automatic indexing", "author": ["G. 109\u2013109. Salton", "A. Wong", "C.S. Yang", "Nov"], "venue": "Commun. ACM", "citeRegEx": "Salton et al\\.,? \\Q1975\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1975}, {"title": "The philosophy of information retrieval evaluation. In: Evaluation of cross-language information retrieval", "author": ["E.M. Voorhees"], "venue": "The Knowledge Engineering Review", "citeRegEx": "Voorhees,? \\Q2002\\E", "shortCiteRegEx": "Voorhees", "year": 2002}, {"title": "Eliza - a computer program for the study of natural language communication between man and machine", "author": ["J. Weizenbaum", "Jan"], "venue": "Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "citeRegEx": "Weizenbaum and Jan.,? \\Q1966\\E", "shortCiteRegEx": "Weizenbaum and Jan.", "year": 1966}, {"title": "Partially observable markov decision processes for spoken dialog systems", "author": ["J.D. Williams", "S. Young"], "venue": "Computer Speech & Language", "citeRegEx": "Williams and Young,? \\Q2007\\E", "shortCiteRegEx": "Williams and Young", "year": 2007}, {"title": "Learning bilinear model for matching queries and documents", "author": ["W. 393\u2013422. Wu", "Z. Lu", "H. Li", "Jan"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Wu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2013}, {"title": "Retrieval models for question and answer archives", "author": ["X. 2519\u20132548. Xue", "J. Jeon", "W.B. Croft"], "venue": "Proceedings of the 31st Annual International ACM", "citeRegEx": "Xue et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xue et al\\.", "year": 2008}, {"title": "A study of smoothing methods for language models applied to ad hoc information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "SIGIR Conference on Research and Development in Information Retrieval. SIGIR \u201908. ACM,", "citeRegEx": "Zhai and Lafferty,? \\Q2001\\E", "shortCiteRegEx": "Zhai and Lafferty", "year": 2001}, {"title": "Hhmm-based chinese lexical analyzer ictclas", "author": ["Zhang", "H.-P", "Yu", "H.-K", "Xiong", "D.-Y", "Q. Liu"], "venue": "Proceedings of the Second SIGHAN", "citeRegEx": "Zhang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "One of the major reasons for that is lack of large volumes of real conversation data (Chen et al., 2011; Nouri et al., 2011).", "startOffset": 85, "endOffset": 124}, {"referenceID": 1, "context": "One of the major reasons for that is lack of large volumes of real conversation data (Chen et al., 2011; Nouri et al., 2011). In this paper, we consider a much simplified version of the problem: one round of conversation formed by two short texts, with the former being a message from human and the latter being a response to the message from the computer. We refer to it as short text conversation (STC). Thanks to the extremely large amount of short text conversation data available on social media such as Twitter1 and Weibo2, we anticipate that significant progress could be made in the research on the problem with the use of the big data, much like what has happened in machine translation, community question answering, etc. Modeling a short text conversation is much simpler than modeling a complete dialogue, which often requires several rounds of interactions (e.g., a dialogue system as in Litman et al. (2000)).", "startOffset": 86, "endOffset": 922}, {"referenceID": 15, "context": "Short Text Conversation Early work on modeling dialogues is either rule-based (Weizenbaum, 1966) or learning-based (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007).", "startOffset": 115, "endOffset": 187}, {"referenceID": 5, "context": "For example, the system in Leuski et al. (2006) and Leuski and Traum (2011) selects the most suitable response to the current message from the question-answer pairs using a statistical language model in cross-lingual information retrieval.", "startOffset": 27, "endOffset": 48}, {"referenceID": 5, "context": "For example, the system in Leuski et al. (2006) and Leuski and Traum (2011) selects the most suitable response to the current message from the question-answer pairs using a statistical language model in cross-lingual information retrieval.", "startOffset": 27, "endOffset": 76}, {"referenceID": 1, "context": "Instead of building knowledge base by hand, Chen et al. (2011) and Nouri et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 1, "context": "Instead of building knowledge base by hand, Chen et al. (2011) and Nouri et al. (2011) propose augmenting a knowledge base with question-answer pairs derived from texts using a question generation tool.", "startOffset": 44, "endOffset": 87}, {"referenceID": 5, "context": "The results show that phrase-based SMT (Koehn et al., 2007) works better than vector space model (VSM) (Salton et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 12, "context": ", 2007) works better than vector space model (VSM) (Salton et al., 1975) in IR in terms of BLEU score (Papineni et al.", "startOffset": 51, "endOffset": 72}, {"referenceID": 9, "context": "Ritter et al. (2011) investigate the feasibility of conducting short text conversation by using statistical machine translation (SMT) techniques, as well as millions of naturally occurring conversation data in Twitter.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "A related but slightly different problem has been studied in Jafarpour et al. (2010), as an initial step for building a chatbot, referred to as learning to chat (L2C).", "startOffset": 61, "endOffset": 85}, {"referenceID": 0, "context": ", 1995), translation model (Berger and Lafferty, 1999), and latent space model (Wu et al.", "startOffset": 27, "endOffset": 54}, {"referenceID": 16, "context": ", 1995), translation model (Berger and Lafferty, 1999), and latent space model (Wu et al., 2013).", "startOffset": 79, "endOffset": 96}, {"referenceID": 3, "context": "Learning of Ranking Model We employ linear RankingSVM (Herbrich et al., 1999), a state-of-the-art method of learning to rank, to train the ranking model.", "startOffset": 54, "endOffset": 77}, {"referenceID": 19, "context": "Then, for the remained posts and comments, we remove punctuation marks and emotions, and use ICTCLAS (Zhang et al., 2003) for Chinese word segmentation.", "startOffset": 101, "endOffset": 121}, {"referenceID": 13, "context": "We employ a pooling strategy widely used in information retrieval for getting the instances to label (Voorhees, 2002).", "startOffset": 101, "endOffset": 117}, {"referenceID": 16, "context": "We find the mapping functions Lq and Lr through a large number of query-response pairs and a large margin variant of the method in Wu et al. (2013).", "startOffset": 131, "endOffset": 148}, {"referenceID": 17, "context": "Model Description To alleviate the lexical gap problem, we employ the state-of-the-art translation-based language model (TransLM) (Xue et al., 2008) with a small modification for retrieval-based STC.", "startOffset": 130, "endOffset": 148}, {"referenceID": 18, "context": "\u03b1 is the Jelinek-Mercer smoothing factor (Zhai and Lafferty, 2001).", "startOffset": 41, "endOffset": 66}, {"referenceID": 17, "context": "\u03b3 is the parameter to balance between the unigram language model and the translation model for alleviating the lexical gap problem and the self-translation problem (Xue et al., 2008).", "startOffset": 164, "endOffset": 182}, {"referenceID": 17, "context": "The main difference between our TransLM for retrieval-based STC and that for question retrieval in Community Question Answering (CQA) (Xue et al., 2008) is that we add an additional part \u2211 t\u2208r T (w|t)Pml(t|r) in the response part while Xue et al.", "startOffset": 134, "endOffset": 152}, {"referenceID": 17, "context": "\u03b3 is the parameter to balance between the unigram language model and the translation model for alleviating the lexical gap problem and the self-translation problem (Xue et al., 2008). The main difference between our TransLM for retrieval-based STC and that for question retrieval in Community Question Answering (CQA) (Xue et al., 2008) is that we add an additional part \u2211 t\u2208r T (w|t)Pml(t|r) in the response part while Xue et al. (2008) does not do that in the answer part.", "startOffset": 165, "endOffset": 438}, {"referenceID": 17, "context": "\u03b3 is the parameter to balance between the unigram language model and the translation model for alleviating the lexical gap problem and the self-translation problem (Xue et al., 2008). The main difference between our TransLM for retrieval-based STC and that for question retrieval in Community Question Answering (CQA) (Xue et al., 2008) is that we add an additional part \u2211 t\u2208r T (w|t)Pml(t|r) in the response part while Xue et al. (2008) does not do that in the answer part. The reasons are two-fold: (1) we focus on finding suitable responses given a query while Xue et al. (2008) focus on finding similar questions given a query question, and (2) the responses tend to be shorter than the posts in STC while the answers tent to be longer than the questions in CQA.", "startOffset": 165, "endOffset": 582}, {"referenceID": 17, "context": "We follow the method of Xue et al. (2008) to learn the word translation probabilities.", "startOffset": 24, "endOffset": 42}, {"referenceID": 17, "context": "We follow the method of Xue et al. (2008) to learn the word translation probabilities. In our experiments, original post-comment pairs are used for training, and the GIZA++8 (Och and Ney, 2003) toolkit is used to learn the translation model. There are two different settings of source and target for constructing the parallel corpus: (1) set the posts as the source and the responses as the target, i.e., collection (p, r)1, ..., (p, r)n, and (2) set the responses as the source and the posts as the target, i.e., collection (r, p)1, ..., (r, p)n. Following Xue et al. (2008), a pooling strategy is adopted to combine the above two collections of pairs to get a pooled collection (p, r)1, .", "startOffset": 24, "endOffset": 576}, {"referenceID": 8, "context": "com/p/giza-pp/ 9The detail can be found in Lu and Li (2013). 11", "startOffset": 50, "endOffset": 60}, {"referenceID": 19, "context": "We use ICTCLAS (Zhang et al., 2003) to obtain the part of speech and named entity information and LIBLINEAR (Fan et al.", "startOffset": 15, "endOffset": 35}, {"referenceID": 2, "context": ", 2003) to obtain the part of speech and named entity information and LIBLINEAR (Fan et al., 2008) to build the logistic regression model and predict the probability of a word being a topic word.", "startOffset": 80, "endOffset": 98}, {"referenceID": 3, "context": "When training the weighs of features in Equation (2) with linear RankingSVM (Herbrich et al., 1999), we use a fixed penalty parameter (i.", "startOffset": 76, "endOffset": 99}], "year": 2014, "abstractText": "Human computer conversation is regarded as one of the most difficult problems in artificial intelligence. In this paper, we address one of its key sub-problems, referred to as short text conversation, in which given a message from human, the computer returns a reasonable response to the message. We leverage the vast amount of short conversation data available on social media to study the issue. We propose formalizing short text conversation as a search problem at the first step, and employing state-of-the-art information retrieval (IR) techniques to carry out the task. We investigate the significance as well as the limitation of the IR approach. Our experiments demonstrate that the retrieval-based model can make the system behave rather \u201cintelligently\u201d, when combined with a huge repository of conversation data from social media.", "creator": "LaTeX with hyperref package"}}}