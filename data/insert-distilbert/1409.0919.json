{"id": "1409.0919", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2014", "title": "Solving the Problem of the K Parameter in the KNN Classifier Using an Ensemble Learning Approach", "abstract": "this concluding paper presents a new explicit solution for choosing the k parameter in the k - nearest neighbor ( knn ) algorithm, for the alternative solution depending heavily on the idea of ensemble learning, in which variation a weak labeled knn classifier is used each time with a parameter different k, alternatively starting from one to the square root of the size of the training set. the results of the weak classifiers are combined using the weighted sum rule. the proposed solution was tested and compared to other solutions using : a group composed of parallel experiments in real life problems. the experimental results obtained show that the proposed classifier outperforms the traditional knn classifier that uses a different number of neighbors, is competitive with other classifiers, and is a promising classifier with strong potential recognition for a wide range of applications.", "histories": [["v1", "Tue, 2 Sep 2014 23:28:22 GMT  (806kb)", "http://arxiv.org/abs/1409.0919v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ahmad basheer hassanat", "mohammad ali abbadi", "ghada awad altarawneh", "ahmad ali alhasanat"], "accepted": false, "id": "1409.0919"}, "pdf": {"name": "1409.0919.pdf", "metadata": {"source": "META", "title": "Solving the Problem of the K Parameter in the KNN Classifier Using an Ensemble Learning Approach", "authors": ["Ahmad Basheer Hassanat", "Mohammad Ali Abbadi", "Ghada Awad Altarawneh", "Ahmad Ali Alhasanat"], "emails": [], "sections": [{"heading": null, "text": "Keywords- KNN; supervised learning; machine learning; ensemble learning; nearest neighbor;\nI. INTRODUCTION\nThe nearest neighbor approach was first introduced by [1] and later studied by [2]. This approach is one of the simplest and oldest methods used for pattern classification. It often yields efficient performance and, in certain cases, its accuracy is greater than state-of the-art classifiers [3] [4].\nThe KNN classifier categorizes an unlabelled test example using the label of the majority of examples among its k-nearest (most similar) neighbors in the training set. The similarity depends on a specific distance metric, therefore, the performance of the classifier depends significantly on the distance metric used [5].\nThe KNN classifier is one of the most popular neighborhood classifiers in pattern recognition [6] and [7], because the technique is very simple, and highly efficient in the field of pattern recognition, machine learning, text categorization, data mining, object recognition, etc. [8] and [9]. However, it has limitations, such as memory requirement and time complexity, because it is fully dependent on every example in the training set.\nThere are two major problems inherited from the design of the KNN [10] and [7]:\n1. There is no output trained model to be used; the algorithm has to use all the training examples on each test, therefore its time complexity is linear O(n).\n2. Its classification performance depends on choosing the optimal number of neighbors (k), which is different from one data sample to another.\nMany studies have attempted to solve the first problem, dependent on reducing the size of the training set [11], [12], [4], [13] and [14]. Hart proposed a simple local search method called the \u201cCondensed Nearest Neighbor\u201d (CNN) which attempts to minimize the number of stored examples and stores only a subset of the training set to be used for classification later. Their idea is based on removing the similar redundant examples [11].\nGate presented the \u201cReduced Nearest Neighbor\u201d (RNN) method, which is basically based on the CNN. The aim of the method is to further shrink the CNN stored subset by removing all examples from the subset that do not affect the accuracy of the classifier, i.e. removing them causes no significant error overall [12].\nOther studies in the same vein include [15], [16], [17], [18] and [19]. Other works used some hashing techniques to increase classification speed; this includes the work of [20] and [21].\nOn the other hand, to the best of the authors' knowledge, there has been little work in the literature focuses on the second problem; therefore, the purpose of this study is to solve the second problem of the KNN classifier, by removing the need for using a specific k with the classifier.\nII. RELATED WORK\nUsually, the K parameter in the KNN classifier is chosen empirically. Depending on each problem, different numbers of nearest neighbors are tried, and the parameter with the best performance (accuracy) is chosen to define the classifier.\nChoosing the optimal K is almost impossible for a variety of problems [22], as the performance of a KNN classifier varies significantly when K is changed as well as the change of distance metric used. However, it is shown in the literature that when the examples are not uniformly distributed, determining the value of K in advance becomes difficult [23].\nGuo et al. converted the training set to another smaller domain called the \u201cKNN Model\u201d. Their model groups each number of similar examples from the data set, based on their\n33 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nsimilarity to each other. The output model consists of tuples containing the class of the group, the similarity of the most distance point inside the group (local region) to the central data point, in addition to the number of the points of that group (region). There is no need to choose the best k, because the number of points in each group can be seen as an optimal k, i.e. different parameters are used in each group. This work is tested using six data sets, obtaining good results. Their work reduces the size of the training data, and removes the need for choosing the k parameter [10]. However, there is still a need to define other thresholds such as \u201cerror tolerant degree\u201d and the minimum number of points allowed in each group.\nSong et al. presented two approaches \u2013 (local informativeKNN (LI-KNN) and global informative-KNN (GI-KNN)) \u2013 to solve the problem of the k parameter in the KNN classifier. Their goal was to improve the performance of the KNN. They used a new concept, which they called \u201cInformativeness\u201d. This was used as a query-based distance metric. Their experiments (based on 10 data sets from the benchmark corpus [24]) showed that their methods were less sensitive to the change of parameters than the conventional KNN classifier [22].\nHamamoto et al. used a bootstrap method for nearest neighbor classifier. Their experimental results showed that the nearest neighbor classifier based on the bootstrap samples outperforms the conventional KNN classifiers, mainly when the tested examples are in high dimensions [3].\nYang and Liu argue that the performance of the KNN classifier is relatively stable when choosing a large number of neighbors. They used large values for the k parameter such as (30, 45 and 60), and the best results of the classifier were included in their results tables [25] and [26].\nEnas and Choi show that the best choice of the k parameter was found to be dependent on several factors, namely, the dimension of the sample space, the size of the space, the covariance structure, as well as the sample proportions [27].\nThe \u201cinverted indexes of neighbors classifier\u201d (IINC) [28], [29] and [30] is one of the best attempts found in the literature to solve the problem. The aim of their work was not intentionally to solve the problem of the k parameters; rather it was designed to increase the accuracy of the classifier. The main idea of the IINC is to use all the neighbors in the training set, rewarding the nearest neighbors, and penalizing the furthest one.\nTheir algorithm works as follows: the similarity distance of the test point is calculated with all the points in the training set. The distances are sorted in ascending order, keeping track of their classes. The summation of the inverted indexes is then calculated for each class using Eq(1). The probability of each class is then calculated using Eq(2). Obviously, the class with the highest probability is then predicted.\nRemark 1: the previous approach is based on the hypothesis that the influence of the nearest neighbors is larger than those of the furthest distance from the query point [2], [28], [29] and [30].\nThe summation of the inverted indexes for class c is:\nS =\n(1)\nwhere Lc is the number of points of class c, i is the order of the point in the training set after sorting the distances.\nThe probability of a test point x belongs to a class c can be estimated as:\nP x|c = (2)\nwhere =\nand N is the number of examples in the training set.\nJirina and Jirina argue that the experimental results based on 24 data sets taken from the benchmark corpus [24], showed that (in most tasks) the IINC outperformed some other well known classifiers such as the traditional KNN, support vector machines, decision trees, artificial neural networks, and naive Bayes classifiers. Therefore there can be an alternative to standard classification methods [28], [29] and [30].\nIII. THE PROPOSED WORK\nThere are three problems associated with the reported IINC:\n1. It requires all the points in the training data to be used to calculate all the inverted indices; this prevents any attempt to reduce the size of the training set and enforces time consuming.\n2. There is bias against the class of the smallest number of points; even if some of those points are around the query point, still the points far away from the query point somehow contribute to increase the probability of the class of the largest number of points. Even if each single contribution of each point get smaller as the points go further, when adding together with large number of points (examples) the contribution become significant.\n3. Distances need to be sorted in ascending order to calculate the inverted indices; this take as long a time, at least O(nlogn) if quick sort is used; this is worse than the traditional KNN algorithm, which takes a linear time.\nWe propose to use ensemble learning using the same nearest neighbor rule. Basically, the traditional KNN classifier is used each time with a different K. Starting from k=1 to k = the square root of the training set, each classifier votes for a specific class. Then our multi classifiers system uses majority rule to identify the class, i.e. the class with the highest number of votes (by 1-NN, 3-NN, 5-NN\u2026 \u221an-NN) is chosen.\nWe choose to have a maximum number of classifiers to be not greater than the square root of the training data set size, because the often used rule of thumb is that k equals the square root of the number of points in the training data set [28], [29] and [30]. Another reason is that more classifiers increases computation time. This complies with what the pilot study\n34 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nsuggests, since using this threshold was based on benefit cost, the highest accuracy with the lowest computation time.\nThe proposed multi classifiers system uses the odd numbers for the k parameter for three reasons: 1) to increase the speed of the algorithm by avoiding the even classifiers; 2) to avoid the chance of two different classes having the same number of votes; and 3) the pilot experiments having the even ks show no significant change of the results.\nRecalling remark (1), the proposed classifier gives higher weights to the decision of classifiers with the nearest neighbors. The weighted sum rule is used to combine the KNN classifiers. Empirically, we found the best weighting function is using the inverted logarithmic function as in Eq(3). Figure 1 illustrates the function used.\n= (3)\nWhen a test example is compared with all examples in the training set, using a distance function, an array (A) is created to contain the nearest \u221an classes, and the weighted sum (WS) rule is defined for each class using:\n= \u2211 \u2211 , # = $0, &'\u210e)* +), - \u221a/- , = + 2 (4)\nwhere for each class, we have the outer sum representing the KNN classifier for each odd k, and the inner sum calculates the weights for each classifier.\nBy applying Eq(4), the highest the votes for a class the highest its WS, and the nearest an example (belonging to a class) to the test example the highest its WS will be. Therefore, the predicted class is the one with the maximum weighted sum:\n$23++ = argmax (5) To illustrate the proposed classifier, assume that we have 25 points in 2 dimensional feature space belonging to 2 different classes, in addition to one test point (the green triangle) as shown in the upper section of Figure 2.\nAs shown in Figure 2, the ensemble system uses the 1-NN, 3-NN and 5-NN classifiers using the weighted sum rule to find the class of the unknown point the (green triangle), which in this example is predicted to be class 1 (red square).\n---------------------------------------------------------------------------- Algorithm 1: The proposed ensemble KNN classifier ---------------------------------------------------------------------------- Input: training data set TD, test example TE\nOutput: class\u2019s index\n1. Array Distances[n=Size(TD] 2. index=0 3. For each example as E in TD { 4. Distances[index]=distanc(E,TE)//any distance // function 5. index=index+1 6. } 7. Array minClasses[\u221an] 8. minClasses = classes (min \u221an Distances) //ordered by\n// distance 9. Array SW[number of classes in TD]// weight sum for\n// each class 10. Initililze SW// fill with zeros 11. for k=1 to \u221an , k=k+2 12. for i=1 to k , i=i+1 13. SW[minClasses[i]]=classes[minClasses[i]]+1/Log(1+i,2) 14. return argmax(classes) ---------------------------------------------------------------------------- Based on the time complexity analysis of algorithm 1, we can state the following theorem.\nTheorem 1: Time complexity of the proposed ensemble KNN classifier can be approximated to linear function O(n).\nProof: Obviously, lines 1 and 2 take O(1), lines 3,4 and 5 take O(n), n is the size of the training data. Line 7 consumes O(1).\nLine 8 consumes O(nlog\u221an) if we iterate the distance array n times, and insert each element into a binary search tree\n35 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nbounded with size \u221an, and remove the maximum number when the size of the tree exceeds \u221an.\nSince \u221an<<n, O(log\u221an) can be approximate to a constant k, therefore line 8 consumes O(n k).\nLine 9 consumes O(1). Line 10 consumes O(m), where m is the number of classes in the training set, which normally is a constant. Thus it can be approximated to O(1).\nLine 11 consumes O(\u221an/2) because it works only on the odd numbers, The nested loop in line 12 and the line inside (13) consume O(\u221an/2*\u221an/2) = O(n). And the last line consumes O(1).\nThis makes the total time complexity:\n2O(1)+3O(n)+O(1)+O(n k)+ 2O(1)+O(\u221an/2)+2O(n)+O(1)\u2248O(n k) (6)\nWe can write O(k) \u2248 O(1), therefore:\nO(n k) \u2248 O(n) \u25a1\nThe time complexity of the proposed classifier O(n log \u221an) \u2248 O(n) is better than that of the IINC, which is O(n log n), because we use only the first \u221an nearest distances . However, if we worked the na\u00efve version of finding the minimum k distances each time from n elements, it would then cost O(kn), since k=\u221an, time complexity becomes O(n\u221an). This function grows even faster than O(nlogn).\nIV. RESULTS AND DISCUSSION\nThe proposed classifier is applied and compared to other methods that are found in the literature to solve the problem of the k parameter in the KNN classifier. For the experiments, we chose 28 different data sets to represent real life classification problems, taken from the UCI Machine Learning Repository [24]. Table 1 depicts the data sets used.\nTABLE I. DESCRIPTION OF THE DATA SETS USED.\nName #E #F #C data type Min Max\nHeart 270 25 2 pos integer 0 564\nBalance 625 4 3 pos integer 1 5\nCancer 683 9 2 pos integer 0 9\nGerman 1000 24 2 pos integer 0 184\nLiver 345 6 2 pos integer 0 297\nVehicle 846 18 4 pos integer 0 1018\nVote 399 10 2 pos integer 0 2\nBCW 699 10 2 pos integer 1 13454352\nHaberman 306 3 2 pos integer 0 83\nLetter recognition 20000 16 26 pos integer 0 15\nWholesale 440 7 2 pos integer 1 112151\nAustralian 690 42 2 pos real 0 100001\nGlass 214 9 6 pos real 0 75.41\nSonar 208 60 2 pos real 0 1\nWine 178 13 3 pos real 0.13 1680\nEEG 14980 14 2 pos real 86.67 715897\nParkinson 1040 27 2 pos real 0 1490\nIris 150 4 3 pos real 0.1 7.9\nDiabetes 768 8 2 real & integer 0 846\nMonkey1 556 17 2 binary 0 1\nIonosphere 351 34 2 real -1 1\nPhoneme 5404 5 2 real -1.82 4.38\nSegmen 2310 19 7 real -50 1386.33\nVowel 528 10 11 real -5.21 5.07\nWave21 5000 21 3 real -4.2 9.06\nWave40 5000 40 3 real -3.97 8.82\nBanknote 1372 4 2 real -13.77 17.93\nQSAR 1055 41 2 real -5.256 147\n#E: Number of examples. #F: Number of features. #C: Number of classes.\nEach data set is divided into two data sets\u2013 one for training and the other for testing. 30% of the data set is used for testing, and the rest of the data is for training. Ten types of classifiers have been designed to compare their performances with the proposed classifier; these are 1-NN, 3-NN, 5-NN, 7-NN, 9-NN, \u221an \u2013NN, 30-NN, 45-NN, 60-NN, and the IINC. These include the traditional KNN classifier using small, medium and large number of neighbors, in addition to the IINC classifier, which arguably bests state-of-the-art classifiers [28], [29] and [30].\nEach classifier is used to classify the test samples using Manhattan distance. The 30% of data which were used as a test sample are chosen randomly and each experiment on each data set is repeated 10 times to obtain random examples for testing and training. Table 2 shows the results of the experiments. The accuracy of each classifier on each normalized data set is the average of 10 runs.\nAs can be seen from the results, there is no optimal k, as there is no specific number of neighbors that are suitable for all data sets to be used with the nearest neighbor rule. Each data set favors a specific number (k) of neighbors. This note justifies the proposed method, which attempts to use the power of each classifier, and employs it to enhance the overall performance of the proposed method.\nAccording to the experiments, the using k = \u221an did not yield excellent results compared to other methods, so using k = \u221an as a rule of thumb is not a good choice for the KNN classifier. In addition to the use of a large number of neighbors such as k= 30, 45 and 60, does not help in increasing the accuracy of the KNN classifier as argued by [25] and [26]. They argued that the performance of the KNN becomes more stable when using large k. Perhaps that is because their reported results were based on text categorization data sets, while none of the above-mentioned data sets is related to the text categorization problem. Therefore, we cannot generalize their note to other data sets and classification problems.\n36 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nOn the other hand, the performance of both the proposed method and the IINC is better than the other classifiers in general. Both methods do not ask for a specific k. The good performance of the IINC is justified by the use of all the neighbors, and the good performance of the proposed method is justified by the use of ensemble learning, which makes use of weak classifiers to generate a stronger one.\nIt can be noted from the results that the proposed method outperformed all classifiers in 8 data sets, and even when it is\nbehind other classifiers the difference is not more than 0.02 from the best performance. The performance of the IINC is slightly better than the proposed method, as it outperformed all classifiers in 9 data sets. However, both methods have almost the same performance in general.\nIt is well established in the literature [31] and according to the \u2018no free lunch\u2019 theorem [32], there is no optimal classifier that works perfectly for every class of problems, as the performance of the classifier depends mainly on the problem and the data used.\n37 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nOur method has yet another feature, which is the linear time complexity, compared to logarithmic linear time of the IINC, which needs to sort the distances to start calculating the inverted indexes. Moreover, the need for all examples in the training set prevents the IINC from speeding up using some methods such as the CNN and RNN. On the other hand, the proposed method can benefit from such methods, because it uses only the square root of the nearest neighbors.\nV. CONCLUSION AND FUTURE WORK\nThis work proposes a new classifier based on the KNN classifier, which solves the problem of choosing the number of neighbors that participate in the final decision using the majority rule of the nearest neighbor approach. The proposed method makes use of the ensemble learning approach, where the traditional KNN is used with a different number of neighbors each time.\nThe experimental results using a variety of data sets of real life problems have demonstrated the superiority of the proposed method over the tradition KNN using variety of k neighbors. In addition, the proposed method was found competitive to other classifiers such as the IINC classifier. Moreover, we have shown that the speed of the proposed method (linear time) was found to be better than that of the IINC which is logarithmic linear time.\nThere is room for enhancing the complexity time of the proposed method using KD-trees [33] or other hashing techniques [20] and [21]. Such efforts are best left to be done in the future.\nACKNOWLEDGMENT\nAll the data sets used in this paper were taken from the UCI Irvine Machine Learning Repository [24], therefore the authors would like to thank and acknowledge the people behind this great corpus. Also the authors would like to thank the anonymous reviewers of this paper.\nREFERENCES [1] E. Fix and J. Hodges, \"Discriminatory Analysis: Nonparametric\nDiscrimination: Consistency Properties,\" 4, 1951.\n[2] T. M. Cover and P. E. Hart, \"Nearest Neighbor Pattern Classification,\" IEEE Trans. Inform. Theory, vol. IT-13, pp. 21- 27, 1967.\n[3] Y. Hamamoto, S. Uchimura, and S. Tomita, \"A Bootstrap Technique for Nearest Neighbor Classifier Design,\" IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, vol. 19, no. 1, pp. 73-79, 1997.\n[4] E. Alpaydin, \"Voting Over Multiple Condensed Nearest Neoghbors,\" Artificial Intelligence Review, vol. 11, pp. 115-132, 1997.\n[5] K. Q. Weinberger and L. K. Saul, \"Distance Metric Learning for Large Margin Nearest Neighbor Classification,\" Journal of Machine Learning Research, vol. 10, pp. 207-244, 2009.\n[6] A. Kataria and M. D. Singh, \"A Review of Data Classification Using K-Nearest Neighbour Algorithm,\" International Journal of Emerging Technology and Advanced Engineering, vol. 3, no. 6, pp. 354-360, 2013.\n[7] N. Bhatia and A. Vandana, \"Survey of Nearest Neighbor\nTechniques,\" (IJCSIS) International Journal of Computer Science and Information Security, vol. 8, no. 2, pp. 302-305, 2010.\n[8] A. B. A. Hassant, \"Visual Speech Recognition,\" in Speech Technologies, I. Ipsic, Ed. Rijeka: InTech - Open Access Publisher, 2011, vol. 2, ch. 14.\n[9] A. B. A. Hassanat, \"Visual Passwords Using Automatic Lip Reading,\" International Journal of Sciences: Basic and Applied Research (IJSBAR), vol. 13, no. 1, pp. 218-231, 2014.\n[10] G. Guo, H. Wang, D. Bell, Y. Bi, and K. Greer, \"KNN ModelBased Approach in Classification,\" Lecture Notes in Computer Science, vol. 2888, pp. 986-996, 2003.\n[11] P. Hart, \"The Condensed Nearest Neighbour Rule,\" IEEE Transactions on Information Theory, vol. 14, pp. 515-516, 1968.\n[12] G. Gates, \"The Reduced Nearest Neighbour Rule,\" IEEE Transactions on Information Theory, vol. 18, pp. 431-433, 1972.\n[13] M. Kubat and M. Jr, \"Voting Nearest-Neighbour Subclassifiers,\" in Proceedings of the 17th International Conference on Machine Learning, ICML-2000, Stanford, CA, 2000, pp. 503-510.\n[14] D. R. Wilson and T. R. Martinez, \"Reduction Techniques for Exemplar-Based Learning Algorithms,\" Machine learning, vol. 38, no. 3, pp. 257-286, 2000.\n[15] Y. Zeng, Y. Yang, and L. Zhou, \"Pseudo Nearest Neighbor Rule for Pattern Recognition,\" Expert Systems with Applications, vol. 36, pp. 3587-3595, 2009.\n[16] H. Parvin, H. Alizadeh, and B. Minaei, \"A Modification on KNearest Neighbor Classifier,\" Global Journal of Computer Science and Technology, vol. 10, no. 14, pp. 37-41, 2010.\n[17] Z. Yong, \"An Improved kNN Text Classification Algorithm based on Clustering,\" Journal of Computers, vol. 4, no. 3, 2009.\n[18] Q.-B. Gao and Z.-Z. Wang, \"Center-based nearest neighbor classifier,\" Pattern Recognition, vol. 40, pp. 346-349, 2007.\n[19] Z. Yong, L. Youwen, and X. Shixiong, \"An Improved KNN Text Classification Algorithm Based on Clustering,\" JOURNAL OF COMPUTERS, vol. 4, no. 3, pp. 230-237, 2009.\n[20] P. Indyk and R. Motwani, \"Approximate nearest neighbor: towards removing the curse of dimensionality,\" in Proc. 30th Annu. ACM Symp. Comput. Geometry, 1998, p. 604\u2013613.\n[21] A. Andoni and P. Indyk, \"Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions,\" COMMUNICATIONS OF THE ACM, vol. 51, no. 1, pp. 117-122, 2008.\n[22] Y. Song, J. Huang, D. Zhou, H. Zha, and C. L. Giles, \"Iknn: Informative k-nearest neighbor pattern classification,\" in Proceedings of the 11th European conference on Principles and Practice of Knowledge Discovery in Databases, Berlin, 2007, pp. 248-264.\n[23] M. Latourrette, \"Toward an explanatory similarity measure for nearest-neighbor classification,\" in Proceedings of the 11th European Conference on Machine Learning, London, 2000, pp. 238-245.\n[24] K. Bache and M. Lichman. (2013) UCI Machine Learning Repository. [Online]. http://archive.ics.uci.edu/ml\n[25] Y. Yang, \"An evaluation of statistical approaches to text categorization,\" Information Retrieval, vol. 1, pp. 69-90, 1999.\n[26] Y. Yang and X. Liu, \"A re-examination of text categorization methods,\" in Proceedings of SIGIR-99, 22nd ACM International Conference on Research and Development in Information Retrieval, Berkeley, 1999, pp. 42-49.\n[27] G. G. Enas and S. C. Choi, \"Choice of the smoothing parameter and efficiency of k-nearest neighbor classification,\" Computers &\n38 http://sites.google.com/site/ijcsis/ ISSN 1947-5500\nMathematics with Applications, vol. 12, no. 2, pp. 235-244, 1986.\n[28] M. Jirina and M. J. Jirina, \"Classifier Based on Inverted Indexes of Neighbors,\" Institute of Computer Science, Technical Report No. V-1034, 2008.\n[29] M. Jirina and M. J. Jirina, \"Using Singularity Exponent in Distance Based Classifier,\" in Proceedings of the 10th International Conference on Intelligent Systems Design and Applications (ISDA2010), Cairo, 2010, pp. 220-224.\n[30] M. Jirina and M. J. Jirina, \"Classifiers Based on Inverted Distances,\" in New Fundamental Technologies in Data Mining, K. Funatsu, Ed. InTech, 2011, vol. 1, ch. 19, pp. 369-387.\n[31] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classification, 2nd ed. Wiley, 2001.\n[32] D. H. Wolpert and W. G. Macready, \"No free lunch theorems for optimization,\" IEEE Trans. Evol. Comput., vol. 1, p. 67\u201382, 1997.\n[33] J. L. Bentley, \"K-d trees for semidynamic point sets,\"In Proceedings of Sixth Annual Symposium on Computational Geometry, 1990, pp. 187-197.\nAUTHORS PROFILE\nAhmad B. A Hassanat was born and grew up in Jordan, received his Ph.D. in Computer Science from the University of Buckingham at Buckingham, UK in 2010, and B.S. and M.S. degrees in Computer Science from Mutah University/Jordan and Al al-Bayt University/Jordan in 1995 and 2004,\nrespectively. He has been a faculty member of Information Technology department at Mutah University since 2010. His main interests include computer vision, Machine learning and pattern recognition.\nMohammad Ali Abbadi received his Ph.D and M.S. in computer science from George Washington University, USA, in 2000, and 1996 respectivly, and B.S. in computer science in 1990 from Mutah University. He has been a faculty member of Information Technology department at Mutah University since 2000. His research interests include Data Compression, Multimedia Databases & Digital Libraries, Audio/Image/Video Processing, Operating Systems, Fault Tolerance and Watermarking.\nAhmad Ali Alhasanat received his M.S. degree in Management Information Systems from Al-Balqa' Applied University/Jordan in 2014, and B.S. degree in Computer Science from Mutah University/Jordan in 2003. He has been a computer lab supervisor in College of Business Administration & Economics at Al-Hussein Bin Talal University since 2004. His main interests include management information systems and artficial intelligence.\nGhada Awad Altarawneh received her Ph.D. in Accounting from the University of Buckingham at Buckingham, UK in 2011, and B.S. and M.S. degrees in Accounting from Mutah University/Jordan and Al al-Bayt University/Jordan in 2002 and 2005, respectively. She has been a faculty member of Accounting department at Mutah University since 2011. Her main interests include Accounting information systems, and using artificial intelligence in accounting systems.\n39 http://sites.google.com/site/ijcsis/ ISSN 1947-5500"}], "references": [{"title": "Discriminatory Analysis: Nonparametric Discrimination: Consistency Properties", "author": ["E. Fix", "J. Hodges"], "venue": "4, 1951.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1951}, {"title": "Nearest Neighbor Pattern Classification", "author": ["T.M. Cover", "P.E. Hart"], "venue": "IEEE Trans. Inform. Theory, vol. IT-13, pp. 21- 27, 1967.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1967}, {"title": "A Bootstrap Technique for Nearest Neighbor Classifier Design", "author": ["Y. Hamamoto", "S. Uchimura", "S. Tomita"], "venue": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, vol. 19, no. 1, pp. 73-79, 1997.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Voting Over Multiple Condensed Nearest Neoghbors", "author": ["E. Alpaydin"], "venue": "Artificial Intelligence Review, vol. 11, pp. 115-132, 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 207-244, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "A Review of Data Classification Using K-Nearest Neighbour Algorithm", "author": ["A. Kataria", "M.D. Singh"], "venue": "International Journal of Emerging Technology and Advanced Engineering, vol. 3, no. 6, pp. 354-360, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Survey of Nearest Neighbor  Techniques", "author": ["N. Bhatia", "A. Vandana"], "venue": "(IJCSIS) International Journal of Computer Science and Information Security, vol. 8, no. 2, pp. 302-305, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual Speech Recognition", "author": ["A.B.A. Hassant"], "venue": "Speech Technologies, I. Ipsic, Ed. Rijeka: InTech - Open Access Publisher, 2011, vol. 2, ch. 14.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Visual Passwords Using Automatic Lip Reading", "author": ["A.B.A. Hassanat"], "venue": "International Journal of Sciences: Basic and Applied Research (IJSBAR), vol. 13, no. 1, pp. 218-231, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "KNN Model- Based Approach in Classification", "author": ["G. Guo", "H. Wang", "D. Bell", "Y. Bi", "K. Greer"], "venue": "Lecture Notes in Computer Science, vol. 2888, pp. 986-996, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "The Condensed Nearest Neighbour Rule", "author": ["P. Hart"], "venue": "IEEE Transactions on Information Theory, vol. 14, pp. 515-516, 1968.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1968}, {"title": "The Reduced Nearest Neighbour Rule", "author": ["G. Gates"], "venue": "IEEE Transactions on Information Theory, vol. 18, pp. 431-433, 1972.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1972}, {"title": "Voting Nearest-Neighbour Subclassifiers", "author": ["M. Kubat", "M. Jr"], "venue": "Proceedings of the 17th International Conference on Machine Learning, ICML-2000, Stanford, CA, 2000, pp. 503-510.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Reduction Techniques for Exemplar-Based Learning Algorithms", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Machine learning, vol. 38, no. 3, pp. 257-286, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Pseudo Nearest Neighbor Rule for Pattern Recognition", "author": ["Y. Zeng", "Y. Yang", "L. Zhou"], "venue": "Expert Systems with  Applications, vol. 36, pp. 3587-3595, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "A Modification on K- Nearest Neighbor Classifier", "author": ["H. Parvin", "H. Alizadeh", "B. Minaei"], "venue": "Global Journal of Computer Science and Technology, vol. 10, no. 14, pp. 37-41, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "An Improved kNN Text Classification Algorithm based on Clustering", "author": ["Z. Yong"], "venue": "Journal of Computers, vol. 4, no. 3, 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Center-based nearest neighbor classifier", "author": ["Q.-B. Gao", "Z.-Z. Wang"], "venue": "Pattern Recognition, vol. 40, pp. 346-349, 2007.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "An Improved KNN Text Classification Algorithm Based on Clustering", "author": ["Z. Yong", "L. Youwen", "X. Shixiong"], "venue": "JOURNAL OF COMPUTERS, vol. 4, no. 3, pp. 230-237, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Approximate nearest neighbor: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "Proc. 30th Annu. ACM Symp. Comput. Geometry, 1998, p. 604\u2013613.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "COMMUNICATIONS OF THE ACM, vol. 51, no. 1, pp. 117-122, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Iknn: Informative k-nearest neighbor pattern classification", "author": ["Y. Song", "J. Huang", "D. Zhou", "H. Zha", "C.L. Giles"], "venue": "Proceedings of the 11th European conference on Principles and Practice of Knowledge Discovery in  Databases, Berlin, 2007, pp. 248-264.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Toward an explanatory similarity measure for nearest-neighbor classification", "author": ["M. Latourrette"], "venue": "Proceedings of the 11th European Conference on Machine  Learning, London, 2000, pp. 238-245.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "An evaluation of statistical approaches to text categorization", "author": ["Y. Yang"], "venue": "Information Retrieval, vol. 1, pp. 69-90, 1999.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "A re-examination of text categorization methods", "author": ["Y. Yang", "X. Liu"], "venue": "Proceedings of SIGIR-99, 22nd ACM International Conference on Research and Development in Information Retrieval, Berkeley, 1999, pp. 42-49.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Choice of the smoothing parameter and efficiency of k-nearest neighbor classification", "author": ["G.G. Enas", "S.C. Choi"], "venue": "Computers & 38  http://sites.google.com/site/ijcsis/ ISSN 1947-5500  (IJCSIS) International Journal of Computer Science and Information Security, Vol. 12, No. 8, August 2014 Mathematics with Applications, vol. 12, no. 2, pp. 235-244, 1986.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1947}, {"title": "Classifier Based on Inverted Indexes of Neighbors", "author": ["M. Jirina", "M.J. Jirina"], "venue": "Institute of Computer Science, Technical Report No. V-1034, 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Using Singularity Exponent in Distance Based Classifier", "author": ["M. Jirina", "M.J. Jirina"], "venue": "Proceedings of the 10th International Conference on Intelligent Systems Design and Applications (ISDA2010), Cairo, 2010, pp. 220-224.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Classifiers Based on Inverted Distances", "author": ["M. Jirina", "M.J. Jirina"], "venue": "New Fundamental Technologies in Data  Mining, K. Funatsu, Ed. InTech, 2011, vol. 1, ch. 19, pp. 369-387.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "No free lunch theorems for optimization", "author": ["D.H. Wolpert", "W.G. Macready"], "venue": "IEEE Trans. Evol. Comput., vol. 1, p. 67\u201382, 1997.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION The nearest neighbor approach was first introduced by [1] and later studied by [2].", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "INTRODUCTION The nearest neighbor approach was first introduced by [1] and later studied by [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "It often yields efficient performance and, in certain cases, its accuracy is greater than state-of the-art classifiers [3] [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "It often yields efficient performance and, in certain cases, its accuracy is greater than state-of the-art classifiers [3] [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 4, "context": "The similarity depends on a specific distance metric, therefore, the performance of the classifier depends significantly on the distance metric used [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 5, "context": "The KNN classifier is one of the most popular neighborhood classifiers in pattern recognition [6] and [7], because the technique is very simple, and highly efficient in the field of pattern recognition, machine learning, text categorization, data mining, object recognition, etc.", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "The KNN classifier is one of the most popular neighborhood classifiers in pattern recognition [6] and [7], because the technique is very simple, and highly efficient in the field of pattern recognition, machine learning, text categorization, data mining, object recognition, etc.", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "[8] and [9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8] and [9].", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "There are two major problems inherited from the design of the KNN [10] and [7]: 1.", "startOffset": 66, "endOffset": 70}, {"referenceID": 6, "context": "There are two major problems inherited from the design of the KNN [10] and [7]: 1.", "startOffset": 75, "endOffset": 78}, {"referenceID": 10, "context": "Many studies have attempted to solve the first problem, dependent on reducing the size of the training set [11], [12], [4], [13] and [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "Many studies have attempted to solve the first problem, dependent on reducing the size of the training set [11], [12], [4], [13] and [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 3, "context": "Many studies have attempted to solve the first problem, dependent on reducing the size of the training set [11], [12], [4], [13] and [14].", "startOffset": 119, "endOffset": 122}, {"referenceID": 12, "context": "Many studies have attempted to solve the first problem, dependent on reducing the size of the training set [11], [12], [4], [13] and [14].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "Many studies have attempted to solve the first problem, dependent on reducing the size of the training set [11], [12], [4], [13] and [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "Their idea is based on removing the similar redundant examples [11].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "removing them causes no significant error overall [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "Other studies in the same vein include [15], [16], [17], [18] and [19].", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "Other studies in the same vein include [15], [16], [17], [18] and [19].", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "Other studies in the same vein include [15], [16], [17], [18] and [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "Other studies in the same vein include [15], [16], [17], [18] and [19].", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "Other studies in the same vein include [15], [16], [17], [18] and [19].", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "Other works used some hashing techniques to increase classification speed; this includes the work of [20] and [21].", "startOffset": 101, "endOffset": 105}, {"referenceID": 20, "context": "Other works used some hashing techniques to increase classification speed; this includes the work of [20] and [21].", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "Choosing the optimal K is almost impossible for a variety of problems [22], as the performance of a KNN classifier varies significantly when K is changed as well as the change of distance metric used.", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "However, it is shown in the literature that when the examples are not uniformly distributed, determining the value of K in advance becomes difficult [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 9, "context": "Their work reduces the size of the training data, and removes the need for choosing the k parameter [10].", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Their experiments (based on 10 data sets from the benchmark corpus [24]) showed that their methods were less sensitive to the change of parameters than the conventional KNN classifier [22].", "startOffset": 184, "endOffset": 188}, {"referenceID": 2, "context": "Their experimental results showed that the nearest neighbor classifier based on the bootstrap samples outperforms the conventional KNN classifiers, mainly when the tested examples are in high dimensions [3].", "startOffset": 203, "endOffset": 206}, {"referenceID": 23, "context": "They used large values for the k parameter such as (30, 45 and 60), and the best results of the classifier were included in their results tables [25] and [26].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": "They used large values for the k parameter such as (30, 45 and 60), and the best results of the classifier were included in their results tables [25] and [26].", "startOffset": 154, "endOffset": 158}, {"referenceID": 25, "context": "Enas and Choi show that the best choice of the k parameter was found to be dependent on several factors, namely, the dimension of the sample space, the size of the space, the covariance structure, as well as the sample proportions [27].", "startOffset": 231, "endOffset": 235}, {"referenceID": 26, "context": "The \u201cinverted indexes of neighbors classifier\u201d (IINC) [28], [29] and [30] is one of the best attempts found in the literature to solve the problem.", "startOffset": 54, "endOffset": 58}, {"referenceID": 27, "context": "The \u201cinverted indexes of neighbors classifier\u201d (IINC) [28], [29] and [30] is one of the best attempts found in the literature to solve the problem.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "The \u201cinverted indexes of neighbors classifier\u201d (IINC) [28], [29] and [30] is one of the best attempts found in the literature to solve the problem.", "startOffset": 69, "endOffset": 73}, {"referenceID": 1, "context": "Remark 1: the previous approach is based on the hypothesis that the influence of the nearest neighbors is larger than those of the furthest distance from the query point [2], [28], [29] and [30].", "startOffset": 170, "endOffset": 173}, {"referenceID": 26, "context": "Remark 1: the previous approach is based on the hypothesis that the influence of the nearest neighbors is larger than those of the furthest distance from the query point [2], [28], [29] and [30].", "startOffset": 175, "endOffset": 179}, {"referenceID": 27, "context": "Remark 1: the previous approach is based on the hypothesis that the influence of the nearest neighbors is larger than those of the furthest distance from the query point [2], [28], [29] and [30].", "startOffset": 181, "endOffset": 185}, {"referenceID": 28, "context": "Remark 1: the previous approach is based on the hypothesis that the influence of the nearest neighbors is larger than those of the furthest distance from the query point [2], [28], [29] and [30].", "startOffset": 190, "endOffset": 194}, {"referenceID": 26, "context": "Therefore there can be an alternative to standard classification methods [28], [29] and [30].", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "Therefore there can be an alternative to standard classification methods [28], [29] and [30].", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "Therefore there can be an alternative to standard classification methods [28], [29] and [30].", "startOffset": 88, "endOffset": 92}, {"referenceID": 26, "context": "We choose to have a maximum number of classifiers to be not greater than the square root of the training data set size, because the often used rule of thumb is that k equals the square root of the number of points in the training data set [28], [29] and [30].", "startOffset": 239, "endOffset": 243}, {"referenceID": 27, "context": "We choose to have a maximum number of classifiers to be not greater than the square root of the training data set size, because the often used rule of thumb is that k equals the square root of the number of points in the training data set [28], [29] and [30].", "startOffset": 245, "endOffset": 249}, {"referenceID": 28, "context": "We choose to have a maximum number of classifiers to be not greater than the square root of the training data set size, because the often used rule of thumb is that k equals the square root of the number of points in the training data set [28], [29] and [30].", "startOffset": 254, "endOffset": 258}, {"referenceID": 26, "context": "These include the traditional KNN classifier using small, medium and large number of neighbors, in addition to the IINC classifier, which arguably bests state-of-the-art classifiers [28], [29] and [30].", "startOffset": 182, "endOffset": 186}, {"referenceID": 27, "context": "These include the traditional KNN classifier using small, medium and large number of neighbors, in addition to the IINC classifier, which arguably bests state-of-the-art classifiers [28], [29] and [30].", "startOffset": 188, "endOffset": 192}, {"referenceID": 28, "context": "These include the traditional KNN classifier using small, medium and large number of neighbors, in addition to the IINC classifier, which arguably bests state-of-the-art classifiers [28], [29] and [30].", "startOffset": 197, "endOffset": 201}, {"referenceID": 23, "context": "In addition to the use of a large number of neighbors such as k= 30, 45 and 60, does not help in increasing the accuracy of the KNN classifier as argued by [25] and [26].", "startOffset": 156, "endOffset": 160}, {"referenceID": 24, "context": "In addition to the use of a large number of neighbors such as k= 30, 45 and 60, does not help in increasing the accuracy of the KNN classifier as argued by [25] and [26].", "startOffset": 165, "endOffset": 169}, {"referenceID": 29, "context": "It is well established in the literature [31] and according to the \u2018no free lunch\u2019 theorem [32], there is no optimal classifier that works perfectly for every class of problems, as the performance of the classifier depends mainly on the problem and the data used.", "startOffset": 91, "endOffset": 95}, {"referenceID": 19, "context": "There is room for enhancing the complexity time of the proposed method using KD-trees [33] or other hashing techniques [20] and [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 20, "context": "There is room for enhancing the complexity time of the proposed method using KD-trees [33] or other hashing techniques [20] and [21].", "startOffset": 128, "endOffset": 132}], "year": 2014, "abstractText": "This paper presents a new solution for choosing the K parameter in the k-nearest neighbor (KNN) algorithm, the solution depending on the idea of ensemble learning, in which a weak KNN classifier is used each time with a different K, starting from one to the square root of the size of the training set. The results of the weak classifiers are combined using the weighted sum rule. The proposed solution was tested and compared to other solutions using a group of experiments in real life problems. The experimental results show that the proposed classifier outperforms the traditional KNN classifier that uses a different number of neighbors, is competitive with other classifiers, and is a promising classifier with strong potential for a wide range of applications. KeywordsKNN; supervised learning; machine learning; ensemble learning; nearest neighbor;", "creator": "PDFCreator Version 1.7.3"}}}