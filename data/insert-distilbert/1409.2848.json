{"id": "1409.2848", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2014", "title": "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate", "abstract": "we describe and analyze a simple algorithm for local principal component analysis, vr - pca, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal normal solution. in contrast, existing algorithms suffer either from slow convergence, moderate or computationally intensive linear iterations whose runtime scales with far the reduced data size. the algorithm builds on a recent fast variance - reduced stochastic gradient technique, which was previously analyzed for strongly dynamic convex optimization, whereas here we thoroughly apply it to the entirely non - convex pca problem, using a very different analysis.", "histories": [["v1", "Tue, 9 Sep 2014 19:31:52 GMT  (28kb,D)", "http://arxiv.org/abs/1409.2848v1", null], ["v2", "Sun, 25 Jan 2015 08:28:03 GMT  (32kb,D)", "http://arxiv.org/abs/1409.2848v2", "Some improvements and additions to previous version"], ["v3", "Sun, 19 Apr 2015 14:19:08 GMT  (36kb,D)", "http://arxiv.org/abs/1409.2848v3", "Some improvements and additions to previous version; fixed bug in proof of lemma 1"], ["v4", "Sun, 26 Apr 2015 12:20:12 GMT  (36kb,D)", "http://arxiv.org/abs/1409.2848v4", null], ["v5", "Fri, 31 Jul 2015 04:41:42 GMT  (36kb,D)", "http://arxiv.org/abs/1409.2848v5", "Fixed a minor bug in the proof of lemma 1 (which does not affect the result)"]], "reviews": [], "SUBJECTS": "cs.LG cs.NA math.OC stat.ML", "authors": ["ohad shamir"], "accepted": true, "id": "1409.2848"}, "pdf": {"name": "1409.2848.pdf", "metadata": {"source": "CRF", "title": "A Stochastic PCA Algorithm with an Exponential Convergence Rate", "authors": ["Ohad Shamir"], "emails": ["ohad.shamir@weizmann.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Principal Component Analysis (PCA) is one of the most common tools for unsupervised data analysis and preprocessing. Given a dataset of n instances x1, . . . ,xn in Rd, we are interested in finding a a d\u00d7k matrix U with orthonormal columns, which minimizes\n\u2212 1 n n\u2211 i=1 \u2016U>x\u20162. (1)\nU can be interpreted as defining the k-dimensional subspace, on which the projection of the data has largest possible variance. Finding this subspace has numerous uses, from dimensionality reduction and data compression to data visualization, and the problem is extremely well-studied.\nIn this paper, we will focus on the simplest possible form of this problem, where k = 1, and we are interested in finding a single direction along which the variance of the data is maximized (however, as we discuss later, the algorithm to be presented can be readily extended to solve Eq. (1) for k > 1). This direction is specified by a unit vector v1, which optimizes\nmin v:\u2016v\u2016=1 \u2212 1 n n\u2211 i=1 \u3008v1,x\u30092 = \u2212 v>1\n( 1\nn n\u2211 i=1 xix > i\n) v1. (2)\nIn other words, we seek to find the largest eigenvector v1 of the covariance matrix 1n \u2211n i=1 xix > i .\nWhen the data size n and the dimension d are modest, this problem can be solved exactly by computing the d \u00d7 d covariance matrix, and performing an eigendecomposition. However, the required runtime is O(nd2 + d3), which is prohibitive in large-scale applications. Moreover, even storing a d \u00d7 d matrix in memory can be impossible when d is very large. One possible approach is using iterative methods such as power iteration or the Lanczos method [4]. If the covariance matrix has an eigengap \u03bb between its first and\nar X\niv :1\n40 9.\n28 48\nv1 [\ncs .L\nG ]\n9 S\nep 2\n01 4\nsecond eigenvalues, then these algorithms can be shown to produce a unit vector which is -far from v1 (or \u2212v1) afterO ( log(1/ ) \u03bbp ) iterations (where p = 1 for power iterations, and p = 1/2 for the Lanczos method). However, each iteration requires multiplying one or more vectors by the covariance matrix, which requires O(nd) time (by passing through the entire data). Thus, the total runtime is O ( dn log(1/ )\u03bbp ) . When \u03bb is small, this runtime is equivalent to many passes over the data, which can be prohibitive for large datasets. An alternative to these deterministic algorithms are stochastic and incremental algorithms (e.g. [8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method. In contrast to the algorithms above, these algorithms perform much cheaper iterations by choosing some xi (uniformly at random or otherwise), and updating wt using only xit . In general, the runtime of each iteration is only O(d). On the flip side, due to their stochastic and incremental nature, the convergence rate (when known) is quite slow, with the number of required iterations scaling at least as 1/ . Thus, the runtime of these methods is at least on the order of O ( d1 ) : Useful for getting a low to medium-accuracy solution, but prohibitive when a high-accuracy solution is required. In this paper, we propose a new stochastic PCA algorithm, denoted as VR-PCA 1, which under suitable assumptions, has provable runtime of\nO ( d ( n+ 1\n\u03bb2\n) log ( 1 )) ,\nwhere \u03bb is the eigengap parameter. This algorithm combines the advantages of the previously discussed approaches, while avoiding their main pitfalls: On one hand, the runtime depends only logarithmically on the accuracy , so it is suitable to get high-accuracy solutions; While on the other hand, the runtime scales as the sum of the data size n and a factor involving the eigengap parameter \u03bb, rather than their product. This means that the algorithm is still applicable when \u03bb is relatively small. In fact, as long as \u03bb \u2265 \u2126(1/ \u221a n), this runtime bound is better than those mentioned earlier, and equals dn up to logarithmic factors: Proportional to the time required to perform a single scan of the data.\nVR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction, which has been previously studied (see [6] as well as [9, 7]). However, the setting in which we apply this technique is quite different from previous works, which crucially relied on the strong convexity of the optimization problem (at least locally), and often assume an unconstrained domain. In contrast, our algorithm attempts to minimize the function in Eq. (2), which is nowhere convex, let alone strongly convex (in fact, it is concave everywhere), and over a non-convex domain. As a result, the analysis in previous papers is inapplicable, and we require a new and different analysis to understand the performance of the algorithm."}, {"heading": "2 Algorithm and Analysis", "text": "The pseudo-code of our algorithm appears as Algorithm 1 below. We refer to a single execution of the inner loop as an iteration, and each execution of the outer loop as an epoch. Thus, the algorithm consists of several epochs, each of which consists of running m iterations. We note that the runtime of each iteration isO(dn), and the runtime of each epoch, besides the iterations, is dominated by computing u\u0303 in O(dn) time.\n1VR stands for \u201cvariance-reduced\u201d.\nAlgorithm 1 VR-PCA Parameters: Step size \u03b7, epoch length m Input: Data set {xi}ni=1, Initial unit vector w\u03030 for s = 1, 2, . . . do u\u0303 = 1n \u2211n i=1 xix > i w\u0303s\u22121\nw0 = w\u0303s\u22121 for t = 1, 2, . . . ,m do\nPick it \u2208 {1, . . . , n} uniformly at random w\u2032t = wt\u22121 + \u03b7 ( xitx > it (wt\u22121 \u2212 w\u0303s\u22121) + u\u0303 ) wt = 1 \u2016w\u2032t\u2016\nw\u2032t end for w\u0303s = wm\nend for\nTo understand the structure of the algorithm, it is helpful to consider first the well-known Oja\u2019s algorithm for stochastic PCA optimization [11], on which our algorithm is based. In our setting, this rule is reduced to repeatedly sampling xit uniformly at random, and performing the update\nw\u2032t = wt\u22121 + \u03b7txitx > itwt\u22121 , wt =\n1\n\u2016w\u2032t\u2016 wt.\nLetting A = 1n \u2211n i=1 xix > i , this can be equivalently rewritten as\nw\u2032t = (I + \u03b7tA)wt\u22121 + \u03b7t ( xitx > it \u2212A ) wt\u22121 , wt = 1\n\u2016w\u2032t\u2016 wt. (3)\nThus, at each iteration, the algorithm performs a power iteration (using a shifted and scaled version of the matrix A), adds a stochastic zero-mean term \u03b7t ( xitx > it \u2212A ) wt\u22121, and projects back to the unit sphere. Recently, [3] gave a rigorous finite-time analysis of this algorithm, showing that if \u03b7t = O(1/t), then under suitable conditions, we get a convergence rate of O(1/T ).\nThe reason for the relatively slow convergence rate of this algorithm is the constant variance of the stochastic term added in each step. Inspired by recent variance-reduced stochastic methods for convex optimization [6], we change the algorithm in a way which encourages the variance of the stochastic term to decay over time. Specifically, we can rewrite the update of our VR-PCA algorithm as\nw\u2032t = (I + \u03b7A)wt\u22121 + \u03b7 ( xitx > it \u2212A ) (wt\u22121 \u2212 u\u0303) , wt = 1\n\u2016w\u2032t\u2016 wt. (4)\nComparing Eq. (4) to Eq. (3), we see that our algorithm also performs a type of power iteration, followed by adding a stochastic term zero-mean term. However, our algorithm picks a fixed step size \u03b7, which is more aggressive that a decaying step size \u03b7t. Moreover, the variance of the stochastic term is no longer constant, but rather controlled by \u2016wt\u22121 \u2212 u\u0303\u2016. As we get closer to the optimal solution, we expect that both u\u0303 and wt\u22121 will be closer and closer to each other, leading to decaying variance, and a much faster convergence rate, compared to Oja\u2019s algorithm.\nTo generalize the algorithm to the case where more than one variance direction is sought (i.e. solve Eq. (1) for k > 1), all that is needed is to replace the vectors wt, w\u0303, u\u0303 etc. by d\u00d7 k matrices Wt, W\u0303 , U\u0303 , and replace the normalization step 1\u2016w\u2032t\u2016w \u2032 t by an orthogonalization step. This is completely analogous to how\niterative algorithms such as power iterations and Oja\u2019s algorithm are generalized to the k > 1 case, and the same intuition discussed above still holds.\nA formal analysis of the algorithm appears as Thm. 1. We note that the parameter settings are designed to get the final bound, and may differ from the optimal choice in practice. This issue is further discussed in Sec. 3.\nTheorem 1. Let v1 be an eigenvector of A = 1n \u2211n i=1 xix > i corresponding to the largest singular value. Suppose that\n\u2022 A has singular values s1, . . . , sd, which satisfy\ns1 \u2265 1 + \u03bb > 1 \u2265 s2 \u2265 s3 \u2265 . . . \u2265 sd \u2265 0\nfor some \u03bb > 0.\n\u2022 maxi \u2016xi\u2016 and \u03bb are upper-bounded by a constant.\n\u2022 \u3008w\u03030,v1\u3009 \u2265 1\u221a2 .\nLet \u03b4, \u2208 (0, 1) be fixed. If we run the algorithm with any epoch length parameter m and step size \u03b7, such that\n\u03b7 \u2264 c1\u03b42\u03bb , m \u2265 c2 log(2/\u03b4)\n\u03b7\u03bb , m\u03b72 +\n\u221a m\u03b72 log(2/\u03b4) \u2264 c3, (5)\n(where c1, c2, c3 designates certain positive numerical constants), and for T = \u2308 log(1/ ) log(2/\u03b4) \u2309 epochs, then with probability at least 1\u2212 2 log(1/ )\u03b4, it holds that\n\u3008w\u0303T ,v1\u30092 \u2265 1\u2212 .\nThe proof of the theorem is provided in Sec. 4. It is easy to verify that for any fixed \u03b4, Eq. (5) holds for any sufficiently large m on the order of 1\u03b7\u03bb , as long as \u03b7 is chosen to be sufficiently smaller than \u03bb. Therefore, by running the algorithm form = \u0398 ( 1 \u03bb2 ) iterations per epoch, and T = \u0398(log(1/ )) epochs, we get accuracy with high probability2 1\u22122 log(1/ )\u03b4. Since each iteration requiresO(d) time to implement, and each epoch requires an additional O(dn) time to compute u\u0303, we get a total runtime of\nO ( d ( n+ 1\n\u03bb2\n) log ( 1 )) , (6)\nestablishing an exponential convergence rate. If \u03bb \u2265 \u2126(1/ \u221a n), then the runtime is O(dn log(1/ )) \u2013 up to log-factors, proportional to the time required just to scan the data once. The assumptions of the theorem require some discussion. First, the assumption that maxi \u2016xi\u2016 is bounded by some constant is without loss of generality, since we can always rescale the data to make it hold, and the scaling factor is absorbed into the eigengap parameter \u03bb. Moreover, based on the experiments in Sec. 3, we suspect it can be relaxed to suitable moment conditions (e.g. the algorithm appears to work\n2Strictly speaking, this statement is non-trivial only in the regime of where log ( 1 ) 1\n\u03b4 , but if \u03b4 is a reasonably small ( 1),\nthen this is the practically relevant regime. Moreover, as long as the success probability is positive, we can get an algorithm which succeeds with exponentially high probability by an amplification argument: Simply run several independent instantiations of the algorithm, and pick the solution w for which w> ( 1 n \u2211n i=1 xix > i ) is largest.\nwell for data from a high-dimensional Gaussian distribution, whose norm scales with \u221a d). Second, the theorem assumes that we initialize the algorithm with w\u03030 for which \u3008w\u03030,v1\u3009 \u2265 1\u221a2 . This is not trivial, since if we have no prior knowledge on v1, and we choose w\u03030 uniformly at random from the unit sphere, then it is well-known that |\u3008w\u03030,v1\u3009| \u2264 O(1/ \u221a d) with high probability. Thus, the theorem should be interpreted as analyzing the algorithm\u2019s convergence after an initial \u201cburn-in\u201d period, which results in some w\u03030 with a certain constant distance from v1. This period requires a separate analysis, which we leave to future work. However, since we only need to get to a constant distance from v1, the runtime of that period is independent of the desired accuracy . Alternatively, one can use some different stochastic algorithm with finite-time analysis (e.g. [3]) to get to this constant accuracy, from which point our algorithm and analysis takes over. In any case, we note that some assumption on \u3008w\u03030,v1\u3009 being bounded away from 0 must hold: If we initialize the algorithm with w\u03030 such that \u3008w\u03030,v1\u3009 = 0, then the algorithm may fail to converge (a similar property holds for power iterations, and follows from the non-convex nature of the optimization problem).\nFinally, we note that in the context of strongly convex optimization problems, the variance-reduced technique we use leads to algorithms with runtime\nO ( d ( n+ 1\n\u03bb\n) log ( 1 )) ,\nwhere \u03bb is the strong convexity parameter of the problem [6]. Comparing this with our algorithm\u2019s runtime, and drawing a parallel between strong convexity and the eigengap in PCA problems, it is tempting to conjecture that the 1/\u03bb2 in our runtime analysis can be improved to 1/\u03bb. However, we don\u2019t know if this is true, or whether the 1/\u03bb2 factor is necessary in our setting."}, {"heading": "3 Experiments", "text": "We now turn to present some preliminary experimental results, which demonstrates the performance of the VR-PCA algorithm.\nFirst, we performed experiments on several synthetic datasets, where 50,000 examples are drawn i.i.d. from a Gaussian distribution in R1000, with zero mean and covariance matrix I + \u03bbeie>i . The spectrum of this matrix equals (1 + \u03bb, 1, 1, . . . , 1), corresponding to an eigengap of roughly \u03bb (in practice, due to finite sample effects, the eigengap of the data covariance matrix is slightly different). Each dataset corresponds to a different value of \u03bb. We note that these datasets do not satisfy the boundedness assumption in our analysis (here the norm of each instance scales as \u221a d), but nonetheless the algorithm appears to work well in practice. We used a fixed choice of parameters, where m = n and \u03b7 = 0.05/ \u221a n. This choice of m ensures that at each epoch, the runtime is about equally divided between the stochastic updates and the computation of u\u0303. The choice of \u03b7 is motivated by our theoretical analysis, which requires \u03b7 on the order of 1/ \u221a n in the regime where m should be on the order of n. For comparison, we also implemented Oja\u2019s algorithm, using several different step sizes. All algorithms were initialized from the same same random vector, chosen uniformly at random from the unit ball. Again, compared to our analysis, this makes things harder for our algorithm, since we require it to perform well also in the \u2018burn-in\u2019 phase. The results are displayed in figure 1, and we see that for all values of \u03bb considered, VR-PCA converges much faster than all versions of Oja\u2019s algorithm, on which it is based, even though we did not tune its parameters. Moreover, since the y-axis is in logarithmic scale, we see that the convergence rate is indeed exponential in general.\nNext, we performed a similar experiment using the well-known MNIST dataset, consisting of 70, 000 binary images of handwritten digits, each represented by a 784-dimensional vector. We pre-processed the data by centering it and dividing each coordinate by its standard deviation times the squared root of the\nVR-PCA, to perform n iterations plus computing u\u0303), and the y-axis equals log10\n(\n1\u2212 w>Aw\nv>1 Av1\n)\n, where w is\nthe vector obtained so far.\ndimension. The results appear in figure 2. As before, the VR-PCA algorithm converges at an exponential rate, and much faster than its competitors. However, on this dataset, the initial convergence is relatively slow. The reason for this is that initially we are still very far from the optimum, and the variance-reduction technique is yet to kick in. To mitigate this, we consider a simple hybrid method, which initializes the VRPCA algorithm with the result of running n iterations of Oja\u2019s algorithm. The decaying step size of Oja\u2019s algorithm is more suitable for this \u2018burn-in\u2019 phase, and the resulting hybrid algorithm performs uniformly better than each algorithm alone."}, {"heading": "4 Proof of Thm. 1", "text": "We use c to designate positive numerical constants, whose value can vary at different places (even in the same line or expression).\nLet\nA = 1\nn n\u2211 i=1 xix > i = d\u2211 i=1 siviv > i ,\nbe an eigendecomposition of A, where v1, . . . ,vd are orthonormal vectors, and recall that we assume\ns1 \u2265 1 + \u03bb > 1 \u2265 s2 \u2265 s3 \u2265 . . . \u2265 sd \u2265 0.\nfor some \u03bb > 0.\ndata size (assuming 2n accesses per epoch for VR-PCA), and the y-axis equals log10\n(\n1\u2212 w>Aw\nv>1 Av1\n)\n, where\nw is the vector obtained so far.\nPart I: Establishing a Stochastic Recurrence Relation\nWe begin by focusing on a single epoch of the algorithm, and a single iteration t, and analyze how 1 \u2212 \u3008wt,v1\u30092 evolves during that iteration. The key result we need is the following lemma:\nLemma 1. Suppose that \u3008wt,v1\u3009 \u2265 12 , and that \u3008w\u0303s\u22121,v1\u3009 \u2265 0. If \u03b7 \u2264 min{c, 1 1+\u03bb , c\u03bb}, then\nE [( 1\u2212 \u3008wt+1,v1\u30092 )\u2223\u2223wt] \u2264 (1\u2212 \u03b7\u03bb\n16\n)( 1\u2212 \u3008wt,v1\u30092 ) + c\u03b72 ( 1\u2212 \u3008w\u0303s\u22121,v1\u30092 ) .\nProof. Since we focus on a particular epoch s, let us drop the subscript from w\u0303s\u22121, and denote it simply at w\u0303. Rewriting the update equations from the algorithm, we have that\nwt+1 = w\u2032t+1 \u2016w\u2032t+1\u2016 , where w\u2032t+1 = (I + \u03b7A)wt + \u03b7(xx > \u2212A)(wt \u2212 w\u0303),\nwhere x is the random instance chosen at iteration t. It is easy to verify that\n\u3008w\u2032t+1,vi\u3009 = ai + zi, (7)\nwhere ai = (1 + \u03b7si)\u3008wt,vi\u3009 , zi = \u03b7v>i (xx> \u2212A)(wt \u2212 w\u0303).\nMoreover, since v1, . . . ,vd form an orthonormal basis in Rd, we have\n\u2016w\u2032t+1\u20162 = d\u2211 i=1 \u3008vi,w\u2032t+1\u30092 = d\u2211 i=1 (ai + zi) 2. (8)\nLet E denote expectation with respect to x, conditioned on wt. Combining Eq. (7) and Eq. (8), we have\nE [ \u3008wt+1,v1\u30092 ] = E [ \u3008 w\u2032t+1 \u2016w\u2032t+1\u2016 ,v1\u30092 ] = E [\u3008w\u2032t+1,v1\u30092 \u2016w\u2032t+1\u20162 ] = E [ (a1 + z1) 2\u2211d i=1(ai + zi) 2 ] . (9)\nBy definition, z1, . . . , zd are zero-mean random variables (as a function of x, conditioned on wt) whereas a1, . . . , ad are fixed. Therefore, we can write the above as\nEz1Ez2,...,zd\n[ (a1 + z1) 2\n(a1 + z1)2 + \u2211d i=2(ai + zi) 2\n] .\nBy Jensen\u2019s inequality and the fact that z2, . . . , zd are zero-mean, this is at least\nEz1\n[ (a1 + z1) 2\n(a1 + z1)2 + Ez2,...,zd \u2211d i=2(ai + zi) 2\n]\n= Ez1\n[ (a1 + z1) 2\n(a1 + z1)2 + Ez2,...,zd \u2211d i=2(a 2 i + 2azi + z 2 i )\n]\n= Ez1\n[ (a1 + z1) 2\n(a1 + z1)2 + \u2211d i=2 a 2 i + Ez2,...,zd \u2211d i=2 z 2 i\n] (10)\nBy definition of zi and the fact that v1, . . . ,vd are orthonormal (hence \u2211 i viv > i is the identity matrix), we have\nd\u2211 i=2 z2i \u2264 d\u2211 i=1 z2i = \u03b7 2(wt \u2212 w\u0303)>(xx> \u2212A) ( d\u2211 i=1 viv > i ) (xx> \u2212A)(wt \u2212 w\u0303)\n= \u03b72(wt \u2212 w\u0303)>(xx> \u2212A)(xx> \u2212A)(wt \u2212 w\u0303) = \u03b72\u2016(xx> \u2212A)(wt \u2212 w\u0303)\u20162.\nSince the spectral norm of xx> \u2212A is assumed to be bounded by a constant, this is at most c\u03b72\u2016wt \u2212 w\u0303\u20162 for some constant c. Plugging this back to Eq. (10), we get the lower bound\nEz1\n[ (a1 + z1) 2\n(a1 + z1) + \u2211d i=2 a 2 i + c\u03b7 2\u2016wt \u2212 w\u0303\u20162\n] . (11)\nWe now use a Taylor expansion to lower bound the expression in a more convenient form. To simplify the notation, let\nq = d\u2211 i=2 a2i + c\u03b7 2\u2016wt \u2212 w\u0303\u20162,\nso we can write Eq. (11) as\nEz1 [ (a1 + z1) 2\n(a1 + z1)2 + q\n] , (12)\nwhere q is a non-negative quantity. By a Taylor expansion around z1 = 0 (using a Lagrange remainder term), and using the fact that z1 is zero-mean, we have\nEz1 [ (a1 + z1) 2\n(a1 + z1)2 + q\n] \u2265 Ez1 [ a21 a21 + q \u2212 ( 2a31 (a21 + q) 2 \u2212 2a1 a21 + q ) z1 \u2212 ( max z1 (3(a1 + z1) 2 \u2212 q)q ((a1 + z1)2 + q)3 ) z21 ] \u2265 a 2 1\na21 + q \u2212 ( max z1\n(a1 + z1) 2\n((a1 + z1)2 + q)3\n) 3qE[z21 ]. (13)\nTo continue, we note that\n\u2022 By definition of a1 and the assumption \u3008wt,v1\u3009 \u2265 12 , we have |ai| \u2265 1 2 .\n\u2022 By definition of zi and the assumption that the spectral norm of xx>\u2212A is bounded by some constant, we have that |zi| \u2264 c\u03b7\u2016wt \u2212 w\u0303\u2016 \u2264 c\u03b7. Moreover, since we assume \u3008wt,v1\u3009 \u2265 12 , it follows that\n|zi| \u2264 c\u03b7 = 2c\u03b7 1\n2 \u2264 2c\u03b7\u3008wt,v1\u3009 \u2264 2c\u03b7|ai| \u2264\n1 2 |ai|\nif \u03b7 is small enough.\nCombining these two observations, we have\nmax z1\n(a1 + z1) 2\n((a1 + z1)2 + q)3 \u2264\n( 3 2a1 )2((\n1 2a1 )2 + q )3 \u2264 94a21(1 4a 2 1 + 1 4q ) ( 1 4a 2 1 )2 = 9/4(1/4)3 a21a21 + q . Plugging this back into Eq. (13), and using the fact that |z1| \u2264 c\u03b7\u2016wt \u2212 w\u0303\u2016, we get that\nEz1 [ (a1 + z1) 2\n(a1 + z1)2 + q\n] \u2265 a 2 1\na21 + q\n( 1\u2212 cqE[z21 ] ) \u2265 a 2 1\na21 + q\n( 1\u2212 cq\u03b72\u2016wt \u2212 w\u0303\u20162 ) . (14)\nConsidering the definition of q, and the fact that v1, . . . ,vd is an orthonormal basis for Rd, we have\nq \u2264 c d\u2211 i=2 \u3008vi,wt\u30092 + c\u03b72\u2016wt \u2212 w\u0303\u20162 \u2264 c+ c\u03b72,\nwhich is at most a constant assuming \u03b7 is small enough. This means that 1\u2212 cq\u03b72\u2016wt\u2212 w\u0303\u20162 from Eq. (14) can be lower bounded by 1\u2212 c\u03b72\u2016wt \u2212 w\u0303\u20162, so we get\nEz1 [ (a1 + z1) 2\n(a1 + z1)2 + q\n] \u2265 a 2 1\na21 + q\n( 1\u2212 c\u03b72\u2016wt \u2212 w\u0303\u20162 ) . (15)\nWe now continue by analyzing the a21/(a 2 1 + q) term. Recalling the definition of a1, q, and the fact that\nv1, . . . ,vd is an orthonormal basis, we have\na21 a21 + q = (1 + \u03b7s1) 2\u3008wt,v1\u30092 (1 + \u03b7s1)2\u3008wt,v1\u30092 + \u2211d i=2(1 + \u03b7si) 2\u3008vi,wt\u30092 + c\u03b72\u2016wt \u2212 w\u0303\u20162\n\u2265 \u3008wt,v1\u3009 2 \u3008wt,v1\u30092 + ( 1+\u03b7s2 1+\u03b7s1 )2\u2211d i=2\u3008vi,wt\u30092 + c\u03b72\u2016wt \u2212 w\u0303\u20162 = \u3008wt,v1\u30092\n\u3008wt,v1\u30092 + ( 1+\u03b7s2 1+\u03b7s1 )2 (1\u2212 \u3008wt,v1\u30092) + c\u03b72\u2016wt \u2212 w\u0303\u20162\n= \u3008wt,v1\u30092 1\u2212 ( 1\u2212 ( 1+\u03b7s2 1+\u03b7s1 )2) (1\u2212 \u3008wt,v1\u30092) + c\u03b72\u2016wt \u2212 w\u0303\u20162\n\u2265 \u3008wt,v1\u30092 ( 1 + ( 1\u2212 ( 1 + \u03b7s2 1 + \u03b7s1 )2)( 1\u2212 \u3008wt,v1\u30092 ) \u2212 c\u03b72\u2016wt \u2212 w\u0303\u20162 ) ,\nwhere in the last step we used the elementary inequality 11\u2212x \u2265 1 + x for all x \u2264 1 (and this is indeed justified since \u3008wt,v1\u3009 \u2264 1 and 1+\u03b7s21+\u03b7s1 \u2264 1). We can simplify this expression by noting that since s1 \u2265 1 + \u03bb > 1 \u2265 s2, and the assumption in the lemma statement that \u03b7(1 + \u03bb) \u2264 1, we have\n1 + \u03b7s2 1 + \u03b7s1 \u2264 1 + \u03b7 1 + \u03b7(1 + \u03bb) = 1\u2212 \u03b7\u03bb 1 + \u03b7(1 + \u03bb) \u2264 1\u2212 \u03b7\u03bb 2 ,\nso we actually have\na21 a21 + q\n\u2265 \u3008wt,v1\u30092 ( 1 + \u03b7\u03bb\n2\n( 1\u2212 \u3008wt,v1\u30092 ) \u2212 c\u03b72\u2016wt \u2212 w\u0303\u20162 ) .\nPlugging this back into Eq. (15), we get that Ez1 [ (a1 + z1) 2\n(a1 + z1)2 + q\n] \u2265 \u3008wt,v1\u30092 ( 1 + \u03b7\u03bb\n2\n( 1\u2212 \u3008wt,v1\u30092 ) \u2212 c\u03b72\u2016wt \u2212 w\u0303\u20162 )( 1\u2212 c\u03b72\u2016wt \u2212 w\u0303\u20162 ) \u2265 \u3008wt,v1\u30092 ( 1 + \u03b7\u03bb\n2\n( 1\u2212 \u3008wt,v1\u30092 ) \u2212 c\u03b72\u2016wt \u2212 w\u0303\u20162 ) ,\nwhere we used the elementary inequality (1 + x)(1 \u2212 y) \u2265 1 + x \u2212 2y if y \u2265 0 and x \u2264 1. Plugging this lower bound back into Eq. (12), and recalling that this constitutes a lower bound on E[\u3008wt+1,v1\u30092], we get\nE[\u3008wt+1,v1\u30092] \u2265 \u3008wt,v1\u30092 ( 1 + \u03b7\u03bb\n2\n( 1\u2212 \u3008wt,v1\u30092 ) \u2212 c\u03b72\u2016wt \u2212 w\u0303\u20162 ) . (16)\nWe now get rid of the \u2016wt \u2212 w\u0303\u20162 term, by noting that\n\u2016wt \u2212 w\u0303\u2016 \u2264 \u2016wt \u2212 v1\u2016+ \u2016w\u0303 \u2212 v1\u2016 = 2\u2212 2\u3008wt,v1\u3009+ 2\u2212 2\u3008wt,v1\u3009.\nSince we assume that \u3008wt,v1\u3009, \u3008w\u0303,v1\u3009 are both positive, and they are also at most 1, this is at most\n2\u2212 2\u3008wt,v1\u30092 + 2\u2212 2\u3008wt,v1\u30092 = 2 ( 1\u2212 \u3008wt,v1\u30092 ) + 2 ( 1\u2212 \u3008w\u0303,v1\u30092 ) .\nPlugging this back into Eq. (16), we get that E[\u3008wt+1,v1\u30092] \u2265 \u3008wt,v1\u30092 ( 1 + ( \u03b7\u03bb\n2 \u2212 c\u03b72\n)( 1\u2212 \u3008wt,v1\u30092 ) \u2212 c\u03b72 ( 1\u2212 \u3008w\u0303,v1\u30092 )) ,\nand since we can assume \u03b7 < \u03bb8c by picking \u03b7 small enough, this can be simplified to\nE[\u3008wt+1,v1\u30092] \u2265 \u3008wt,v1\u30092 ( 1 + \u03b7\u03bb\n4\n( 1\u2212 \u3008wt,v1\u30092 ) \u2212 c\u03b72 ( 1\u2212 \u3008w\u0303,v1\u30092 )) .\nThe final stage of the proof consists of converting the bound above to a bound on E[1\u2212 \u3008wt+1,v1\u30092] in terms of 1\u2212\u3008wt+1,v1\u30092. To simplify the notation, let b = ( 1\u2212 \u3008wt,v1\u30092 ) and b\u0303 = c\u03b72 ( 1\u2212 \u3008w\u0303,v1\u30092 ) , so the bound above implies\nE[1\u2212 \u3008wt+1,v1\u30092] \u2264 1\u2212 (1\u2212 b) ( 1 + \u03b7\u03bb\n4 b\u2212 b\u0303 ) = 1\u2212 (1\u2212 b)\u2212 \u03b7\u03bb\n4 b(1\u2212 b) + (1\u2212 b)b\u0303\n= b\u2212 \u03b7\u03bb 4 b(1\u2212 b)\u2212 bb\u0303+ b\u0303\n= b ( 1\u2212 \u03b7\u03bb\n4 (1\u2212 b)\u2212 b\u0303\n) + b\u0303.\nPlugging back the definitions of b\u0303, b, we get that E[1\u2212\u3008wt+1,v1\u30092] \u2264 ( 1\u2212 \u3008wt,v1\u30092 )( 1\u2212 \u03b7\u03bb\n4 \u3008wt,v1\u30092 \u2212 c\u03b72\n( 1\u2212 \u3008w\u0303,v1\u30092 )) + c\u03b72 ( 1\u2212 \u3008w\u0303,v1\u30092 ) .\nSince we assume \u3008wt,v1\u3009 \u2265 12 , we can upper bound this by( 1\u2212 \u3008wt,v1\u30092 )( 1\u2212 \u03b7\u03bb\n16\n) + c\u03b72 ( 1\u2212 \u3008w\u0303,v1\u30092 ) as required.\nPart II: Solving the Recurrence Relation for a Single Epoch\nAs before, since we focus on a single epoch, we drop the subscript from w\u0303s\u22121 and denote it simply as w\u0303. Suppose that \u03b7 = \u03b1\u03bb, where \u03b1 is a small parameter to be chosen later. Also, let\nbt = 1\u2212 \u3008wt,v1\u30092 and b\u0303 = 1\u2212 \u3008w\u0303,v1\u30092.\nThen Lemma 1 tells us that if \u03b1 is sufficiently small, bt \u2264 34 , and \u3008w\u0303,v1\u3009 \u2265 0, then\nE [bt+1|wt] \u2264 ( 1\u2212 \u03b1 16 \u03bb2 ) bt + c\u03b1 2\u03bb2b\u0303. (17)\nLemma 2. Let B be the event that bt \u2264 34 for all t = 0, 1, 2, . . . ,m. If \u03b1 \u2264 c, and \u3008w\u0303,v1\u3009 \u2265 0, then\nE[bm|B,w0] \u2264 (( 1\u2212 \u03b1 16 \u03bb2 )m + c\u03b1 ) b\u0303.\nProof. Recall that bt is a deterministic function of the random variable wt, which depends in turn on wt\u22121 and the random instance chosen at round m. We assume that w0 (and hence b\u0303) are fixed, and consider how bt evolves as a function of t. Using Eq. (17), we have\nE[bt+1|wt, B] = E [ bt+1|wt, bt+1 \u2264 3\n4\n] \u2264 E[bt+1|wt] \u2264 ( 1\u2212 \u03b1 16 \u03bb2 ) bt + c\u03b1 2\u03bb2b\u0303.\nTaking expectation over wt (conditioned on B), we get that E[bt+1|B] \u2264 E [( 1\u2212 \u03b1 16 \u03bb2 ) bt + c\u03b1 2\u03bb2b\u0303 \u2223\u2223\u2223B]\n= ( 1\u2212 \u03b1 16 \u03bb2 ) E [bt|B] + c\u03b12\u03bb2b\u0303.\nUnwinding the recursion, and using that b0 = b\u0303, we therefore get that\nE[bm|B] \u2264 ( 1\u2212 \u03b1 16 \u03bb2 )m b\u0303+ c\u03b12\u03bb2b\u0303 m\u22121\u2211 i=0 ( 1\u2212 \u03b1 16 \u03bb2 )i\n\u2264 ( 1\u2212 \u03b1 16 \u03bb2 )m b\u0303+ c\u03b12\u03bb2b\u0303 \u221e\u2211 i=0 ( 1\u2212 \u03b1 16 \u03bb2 )i\n= ( 1\u2212 \u03b1 16 \u03bb2 )m b\u0303+ c\u03b12\u03bb2b\u0303 1 (\u03b1/16)\u03bb2\n= (( 1\u2212 \u03b1 16 \u03bb2 )m + c\u03b1 ) b\u0303.\nas required.\nWe now turn to prove that the event B assumed in Lemma 2 indeed holds with high probability:\nLemma 3. Suppose that \u03b1 \u2264 c, and \u3008w\u0303,v1\u3009 \u2265 0. Then for any \u03b2 \u2208 (0, 1) and m, if\nb\u0303+ cm\u03b12\u03bb2 + c \u221a m\u03b12\u03bb2 log(1/\u03b2) \u2264 3\n4 , (18)\nthen it holds with probability at least 1\u2212 \u03b2 that\nbt \u2264 b\u0303+ cm\u03b12\u03bb2 + c \u221a m\u03b12\u03bb2 log(1/\u03b2) \u2264 3\n4\nfor all t = 0, 1, 2, . . . ,m, as well as \u3008wm,v1\u3009 \u2265 0.\nProof. To prove the lemma, we analyze the stochastic process b\u0303 = b0, b1, b2, . . . , bm, and use a concentration of measure argument. First, we collect the following facts:\n\u2022 b\u0303 = b0 \u2264 34 : This directly follows from the assumption stated in the lemma.\n\u2022 The conditional expectation of bt+1 is close to bt, as long as bt \u2264 34 : Supposing that bt \u2264 3 4 for some\nt, and \u03b1 is sufficiently small. Then by Eq. (17), E [bt+1|wt] \u2264 ( 1\u2212 \u03b1 16 \u03bb2 ) bt + c\u03b1 2\u03bb2b\u0303 = bt \u2212 \u03b1\u03bb2 ( 1 16 bt \u2212 c\u03b1b\u0303 ) \u2264 bt + c\u03b12\u03bb2b\u0303.\n\u2022 |bt+1 \u2212 bt| is bounded by c\u03b1\u03bb: We have |bt+1 \u2212 bt| \u2264 \u2223\u2223\u3008wt+1,v1\u30092 \u2212 \u3008wt,v1\u30092\u2223\u2223 = |\u3008wt+1,v1\u3009+ \u3008wt,v1\u3009| \u2217 |\u3008wt+1,v1\u3009 \u2212 \u3008wt,v1\u3009|\n\u2264 2 |\u3008wt+1,v1\u3009 \u2212 \u3008wt,v1\u3009| \u2264 2\u2016wt+1 \u2212wt\u2016.\nRecalling the definition of wt+1 in our algorithm, and the fact that the instances x and the matrix A are assumed to have bounded norm, it is easy to verify that \u2016wt+1 \u2212 wt\u2016 \u2264 c\u03b7 = c\u03b1\u03bb for some appropriate constant c, assuming \u03b1 is sufficiently small.\nUsing the maximal version of the Hoeffding-Azuma inequality [5], it follows that with probability at least 1\u2212 \u03b2, it holds simultaneously for all t = 1, . . . ,m (and for t = 0 by assumption) that\nbt \u2264 b\u0303+mc\u03b12\u03bb2b\u0303+ c \u221a m\u03b12\u03bb2 log(1/\u03b2)\nfor some constant c, as long as the expression above is less than 34 . If the expression is indeed less than 3 4 , then we get that bt \u2264 34 for all t. Upper bounding b\u0303 by 1 and \u03bb by a constant, and slightly simplifying, we get the statement in the lemma.\nIt remains to prove that if bt \u2264 34 for all t, then \u3008wm,v1\u3009 \u2265 0. Suppose on the contrary that \u3008wm,v1\u3009 < 0. Since |\u3008wt+1,v1\u3009 \u2212 \u3008wt,v1\u3009| \u2264 \u2016wt+1 \u2212wt\u2016 \u2264 c\u03b1\u03bb as we\u2019ve seen earlier, and \u3008w0,v1\u3009 \u2265 0, it means there must have been some wt such that \u3008wt,v1\u3009 \u2264 c\u03b1\u03bb. But this means that bt = (1 \u2212 \u3008wt,v1\u30092) \u2265 1\u2212 c2\u03b12\u03bb2 > 34 (as long as \u03b1 is sufficiently small, since we assume \u03bb is bounded), invalidating the assumption that bt \u2264 34 for all t. Therefore, \u3008wm,v1\u3009 \u2265 0 as required.\nCombining Lemma 2 and Lemma 3, and using Markov\u2019s inequality, we get the following corollary:\nLemma 4. Let confidence parameters \u03b2, \u03b3 \u2208 (0, 1) be fixed. Suppose that \u3008w\u0303,v1\u3009 \u2265 0, and that m,\u03b1 are chosen such that\nb\u0303+ cm\u03b12\u03bb2 + c \u221a m\u03b12\u03bb2 log(1/\u03b2) \u2264 3\n4 .\nThen with probability at least 1\u2212 (\u03b2 + \u03b3), it holds that \u3008wm,v1\u3009 \u2265 0, and\nbm \u2264 1\n\u03b3\n(( 1\u2212 \u03b1 16 \u03bb2 )m + c\u03b1 ) b\u0303.\nPart III: Analyzing the Entire Algorithm\u2019s Run\nGiven the analysis in Lemma 4 for a single epoch, we are now ready to prove our theorem. Let\nb\u0303s = 1\u2212 \u3008w\u0303s,v1\u30092.\nBy assumption, at the beginning of the first epoch, we have b\u03030 = 1 \u2212 \u3008w\u03030,v1\u30092 \u2264 1 \u2212 12 = 1 2 . Therefore, by Lemma 4, for any \u03b2, \u03b3 \u2208 ( 0, 12 ) , if we pick\n\u03b1 \u2264 c\u03b32 and m \u2265 c log(1/\u03b3) \u03b1\u03bb2 such that 1 2 + cm\u03b12\u03bb2 + c\n\u221a m\u03b12\u03bb2 log(1/\u03b2) \u2264 3\n4 (19)\n(where the constant c in \u03b1 \u2264 c\u03b32 is sufficiently small, and the constant c in m \u2265 c log(1/\u03b3) \u03b1\u03bb2\nis sufficiently large), then we get with probability at least 1\u2212 (\u03b2 + \u03b3) that\nb\u03031 \u2264 1\n\u03b3 (1\u2212 \u03b1\u03bb2 16 ) c log(1/\u03b3) \u03b1\u03bb2 + 1 2 \u03b32  b\u03030 = 1 \u03b3 ( exp ( \u22123 log ( 1 \u03b3 )) + 1 2 \u03b32 ) b\u03030\n= 1\n\u03b3\n( \u03b33 + 1 2 \u03b32 ) b\u03030 \u2264 \u03b3b\u03030,\nas well as \u3008w\u03031,v1\u3009 \u2265 0. Since b\u03031 is only smaller than b\u03030, the conditions of Lemma 4 are fulfilled for b\u0303 = b\u03031, so again with probability at least 1\u2212 (\u03b2 + \u03b3), by the same calculation, we have\nb\u03032 \u2264 \u03b3b\u03031 \u2264 \u03b32b\u03030.\nRepeatedly applying Lemma 4 and using a union bound, we get that after T epochs, with probability at least 1\u2212 T (\u03b2 + \u03b3),\n1\u2212 \u3008w\u0303T ,v1\u30092 = b\u0303T \u2264 \u03b3T b\u03030 < \u03b3T . Therefore, for any desired accuracy parameter , we simply need to use T = \u2308 log(1/ ) log(1/\u03b3) \u2309 epochs, and get\n1\u2212 \u3008w\u0303T ,v1\u30092 \u2264 with probability at least 1\u2212 T (\u03b2 + \u03b3) = 1\u2212 \u2308 log(1/ ) log(1/\u03b3) \u2309 (\u03b2 + \u03b3).\nTo get the theorem statement, using a confidence parameter \u03b4, we pick \u03b2 = \u03b3 = \u03b42 , which ensures that the accuracy bound above holds with probability at least\n1\u2212 \u2308 log(1/ )\nlog(2/\u03b4)\n\u2309 \u03b4 \u2265 1\u2212 log(1/ )\nlog(2/\u03b4) \u03b4 \u2265 1\u2212 2 log\n( 1 ) \u03b4.\nSubstituting this choice of \u03b2, \u03b3 into Eq. (19), and recalling that the step size \u03b7 equals \u03b1\u03bb, the theorem follows."}], "references": [{"title": "Stochastic optimization for PCA and PLS", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "In 2012 50th Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Stochastic optimization of pca with capped msg", "author": ["R. Arora", "A. Cotter", "N. Srebro"], "venue": "In NIPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "The fast convergence of incremental pca", "author": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Matrix computations (4", "author": ["G. Golub", "C. van Loan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American statistical association,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1963}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Semi-stochastic gradient descent methods", "author": ["J. Konecn\u00fd", "P. Richt\u00e1rik"], "venue": "CoRR, abs/1312.1666,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "The method of stochastic approximation for the determination of the least eigenvalue of a symmetrical matrix", "author": ["T.P. Krasulina"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1969}, {"title": "Mixed optimization for smooth functions", "author": ["M. Mahdavi", "L. Zhang", "R. Jin"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["E. Oja"], "venue": "Journal of mathematical biology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1982}, {"title": "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix", "author": ["E. Oja", "J. Karhunen"], "venue": "Journal of mathematical analysis and applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1985}], "referenceMentions": [{"referenceID": 3, "context": "One possible approach is using iterative methods such as power iteration or the Lanczos method [4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.", "startOffset": 0, "endOffset": 11}, {"referenceID": 10, "context": "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.", "startOffset": 31, "endOffset": 41}, {"referenceID": 1, "context": "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.", "startOffset": 31, "endOffset": 41}, {"referenceID": 5, "context": "VR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction, which has been previously studied (see [6] as well as [9, 7]).", "startOffset": 132, "endOffset": 135}, {"referenceID": 8, "context": "VR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction, which has been previously studied (see [6] as well as [9, 7]).", "startOffset": 147, "endOffset": 153}, {"referenceID": 6, "context": "VR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction, which has been previously studied (see [6] as well as [9, 7]).", "startOffset": 147, "endOffset": 153}, {"referenceID": 9, "context": "To understand the structure of the algorithm, it is helpful to consider first the well-known Oja\u2019s algorithm for stochastic PCA optimization [11], on which our algorithm is based.", "startOffset": 141, "endOffset": 145}, {"referenceID": 2, "context": "Recently, [3] gave a rigorous finite-time analysis of this algorithm, showing that if \u03b7t = O(1/t), then under suitable conditions, we get a convergence rate of O(1/T ).", "startOffset": 10, "endOffset": 13}, {"referenceID": 5, "context": "Inspired by recent variance-reduced stochastic methods for convex optimization [6], we change the algorithm in a way which encourages the variance of the stochastic term to decay over time.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "[3]) to get to this constant accuracy, from which point our algorithm and analysis takes over.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "where \u03bb is the strong convexity parameter of the problem [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "Using the maximal version of the Hoeffding-Azuma inequality [5], it follows that with probability at least 1\u2212 \u03b2, it holds simultaneously for all t = 1, .", "startOffset": 60, "endOffset": 63}], "year": 2017, "abstractText": "We describe and analyze a simple algorithm for principal component analysis, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly convex optimization, whereas here we apply it to the non-convex PCA problem, using a very different analysis.", "creator": "LaTeX with hyperref package"}}}