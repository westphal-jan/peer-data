{"id": "1409.2905", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2014", "title": "Non-Convex Boosting Overcomes Random Label Noise", "abstract": "the sensitivity of adaboost to random label noise accuracy is a well - studied problem. logitboost, brownboost and robustboost are boosting algorithms claimed to generally be less sensitive to beta noise than adaboost. we present the results of experiments evaluating these algorithms significantly on both synthetic and real datasets. combined we compare the raw performance on and each of different datasets when repeatedly the labels are corrupted by different levels of associated independent label noise. in presence of random label noise, we found that brownboost and robustboost perform significantly better ratios than adaboost and logitboost, while the difference between each pair of algorithms is quite insignificant. we provide all an explanation for the difference based on the margin distributions of how the algorithms.", "histories": [["v1", "Tue, 9 Sep 2014 21:36:47 GMT  (251kb,D)", "http://arxiv.org/abs/1409.2905v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sunsern cheamanunkul", "evan ettinger", "yoav freund"], "accepted": false, "id": "1409.2905"}, "pdf": {"name": "1409.2905.pdf", "metadata": {"source": "CRF", "title": "Non-Convex Boosting Overcomes Random Label Noise", "authors": ["Sunsern Cheamanunkul", "Evan Ettinger"], "emails": ["scheaman@eng.ucsd.edu", "evanettinger@gmail.com", "yfreund@eng.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "Adaboost [10] is a very popular classification learning algorithm. It is a simple and effective algorithm. While generally successful, the sensitivity of Adaboost to random label noise is well documented [6, 9, 2]. The random label noise setup is one where we take a dataset for which our learning algorithm generates an accurate classifier and we flip each label in the training set with some small fixed probability. Note that the classifier that was a good classifier in the noiseless setup is still a good classifier. The problem is that in the noisy setup the noisy examples mislead the learning algorithm and cause it to diverge significantly from the good classifier.\nLogitBoost [7] is believed to be less sensitive to random noise than Adaboost, but it still falls pray to high levels of random labels noise.\nIn fact, Servedio and Long [8] proved that, in general, any boosting algorithm that uses a convex potential function can be misled by random label noise. Freund [4] suggested a boosting algorithm, called Brownboost, that uses a non-convex potential function and claims to overcome random label\nar X\niv :1\n40 9.\n29 05\nnoise. The main contribution of this paper is experimental evidence that support this claim. The other contribution is a heuristic for automatically tuning the parameters that Brownboost needs as input."}, {"heading": "2 Boosting, margins and convexity", "text": "All non-recursive boosting algorithms generate a classification rule which is a thresholded linear combination of so-called \u201cbase\u201d classification rules. More precisely, let (x, y), with y \u2208 {\u22121,+1} denote a labeled example. Let hi : X \u2192 {\u22121,+1} denote the base rules. then the output of the boosting algorithm is a rule of the form\nF (x) = sign (\u2211 i \u03b1ihi(x) )\nAs it turns out, the sum which is the operand of the sign function is important for understanding the operation of boosting algorithms as well as the generalization error of the generated classifier. It is convenient to replace the sum with a dot product:\u2211\ni\n\u03b1ihi(x) = ~\u03b1 \u00b7 ~h(x)\nwith ~\u03b1 and ~h defined in the natural way.\nTo characterize the relationship of the value of the sum and the label y, Schapire et. al. [11] defines the \u201cmargin\u201d of an example as:\nm(x, y) = y~\u03b1 \u00b7 ~h(x)\nThus m(x, y) > 0 if and only if the classification rule is correct on the example (x, y). The natural goal is therefore to find base rules {hi} and weights {\u03b1i} such that the number of training examples with negative margin i.e. the number of misclassified examples is minimized.\nFrom a computational point of view, the easy case occurs when the training data is separable. In other words, when when there exists a setting of \u03b1 such that m(x, y) > 0 for all of the training examples. In that case finding an appropriate setting for \u03b1 is easy and can be done using the perceptron algorithm.\nOn the other hand, when the training set is not linearly separable, the problem of finding the error minimizing plane is NP-hard. We therefore have to resort to approximations. The approximation used in Adaboost and Logitboost is to use a convex function that upper bounds the step function that corresponds to the number of misclassifications. Specifically, Adaboost corresponds to minimizing the potential function:\n\u03c6(x, y) = e\u2212m(x,y)\nand Logitboost corresponds to minimizing the potential function \u03c6(x, y) = ln ( 1 + e\u2212m(x,y) )\nAs both of these potential functions are convex, minimizing them can be done efficiently. See figure 1 for a depiction of the 0/1 error function and of the potential functions corresponding to Adaboost and LogitBoost. Using a convex upper bound makes the problem tractable, but obviously there can be a significant gap between the bound and the step function which can lead us to a suboptimal solution.\nMoreover, as was shown in [8], algorithms that minimize convex potential functions can always be fooled by the addition of random label noise. This naturally leads us to considering non-convex potential functions that upper bound the 0/1 error function. However, before we get to that. We devote a section to the question: \u201cis minimizing the training error the right goal for a learning algorithm?\u201d"}, {"heading": "2.1 Margins and generalization", "text": "Our ultimate goal when learning classifiers to reduce the test error - the number of mistakes the classifier makes on the test set. As we only have access to the training data we cannot minimize the generalization error directly. The natural goal of the algorithm is to minimize the training error. However, Schapire et. al. [11], showed that there is a better performance measure the performance of the boosted classifier on the training set. That is to maximize the number of training examples whose normalized margin is larger than some \u03b8 > 0. Where the positive margin of the example (x, y) is defined to be\nm\u0302(x, y) = y ~\u03b1 \u00b7 ~h(x) \u2016\u03b1\u20161\nThe intuition, presented and justified in [11], is that large positive margins correspond to confident predictions. Specifically, Theorem 2 in [11] states that, with probability 1 \u2212 \u03b4 over the random choice of the training set, the following inequality holds for all \u03b8 > 0\nPD (m\u0302(x, y) \u2264 0) \u2264 PS (m\u0302(x, y) \u2264 \u03b8) +O ( 1\u221a n ( d log2(n/d) \u03b82 + log(1/\u03b4) )1/2) . (1)\nwhere PD is the probability with respect to the true distribution, PS is the probability with respect to the training set S whose size is n and d is the VC dimension of the base classifiers.\nNote that the bound consists of two terms, the first corresponds to the fraction of the training set whose margin is at most \u03b8 and the second which isO(1/\u03b8). The first term increases with \u03b8 while the second term decreases with \u03b8. As the bound holds uniformly for all values of \u03b8 we are free to choose the values of \u03b8 that would minimize the bound. Intuitively, the first term corresponds to the examples on which we \u201cgive up\u201d. Note that giving up on an example increases the RHS of Equation 1 by 1/m regardless of amount by which the margin of the example is smaller than \u03b8.\nThe goal of learning now becomes to minimize a step function that is thresholded at \u03b8 (see Figure 1). This does not make the problem any easier than minimizing the training error. The suggestion is, however, that minimizing this potential function will yield classifiers with smaller test error."}, {"heading": "2.2 Random label noise", "text": "Consider the effect of random label noise on boosting. The weight assigned to example (x, y) by Adaboost is exponential in the margin w(x, y) = e\u2212m(x,y). Suppose c(x) is the best rule for the noise-free data. Suppose we now add independent label noise to the dataset. The margin of c(x) on examples whose label has been flipped will be negative, resulting in a large weight being assigned to the noisy examples, resulting in base classifiers that fit the noisy examples."}, {"heading": "3 Three non-convex boosting algorithms", "text": "Freund [3, 4, 5] suggested several boosting algorithms that use non-convex potential functions. We briefly review three of these algorithms: Boost-by-majority, BrownBoost and RobustBoost."}, {"heading": "3.1 Boosting-by-majority", "text": "Boost by majority (BBM) combines the base rules using equal weights for each one of the rules. Two additional assumptions are made: that the error of each of the base rules with respect to the corresponding distribution is smaller than a fixed number: 1/2 \u2212 \u03b3, and that the number of boosting iterations is known in advance. Combining these three restrictions allowed Freund to cast the learning problem in the form of a mathematical game and find the optimal solution for that game. The result is a potential function which depends both on the margin and on the number of steps remaining until the end of the game. Specifically, let T be the total number of iterations and let t be the current iteration. Suppose the example we currently consider is (x, y) and that i is the number of existing base rules that predict correctly on the (x, y) (i corresponds to the margin) then the optimal boosting algorithm uses a potential function \u03a6ti that is defined by the following recursion.\n\u03a6T+1i = {\n1 if i < 0 0 otherwise\n(2)\n\u03a6ti = ( 1\n2 + \u03b3)\u03a6t+1i+1 + (\n1 2 \u2212 \u03b3)\u03a6t+1i\u22121 (3)"}, {"heading": "3.2 BrownBoost and RobustBoost", "text": "While BBM is a theoretically optimal boosting algorithm, it is not applicable in practice, because it requires knowing the number of steps in advance and giving each base rule the same weight. Contrast this with Adaboost and LogitBoost which adapt their step size to the error of the last base classifier.\nBrownboost [4] overcomes this deficiency by taking the limit of the BBM game where the number of steps goes to infinity. Taking this limit is not trivial but the end result is rather simple. In this limit both the margin s and the time t are continuous:\n\u03a6(s, t) = 12\n( 1\u2212 erf ( s+ 2 \u221a \u03b2(1\u2212 t)\u221a\n2(1\u2212 t)\n)) (4)\nA slightly different limit yields the Robust-Boost potential:\n\u03a6(s, t) = 12\n( 1\u2212 erf ( s\u2212 \u00b5(t) \u03c3(t) )) (5)\nWhere\n\u03c3(t) = \u221a c1e\u22122t \u2212 1\n\u00b5(t) = c2e \u2212t + 2\u03c1\nand c1, c2 are real valued constants."}, {"heading": "3.3 Solving the potential function", "text": "RobustBoost and BrownBoost require an additional step in the boosting algorithm. The potential functions here change as a function of time, and time is a continuous variable (it is not proportional to the number of iterations).\nWe therefor need to solve at each iteration, a set of two non-linear equations in two unknowns: the base rule weight \u03b1 and the time advance \u2206t. We use a standard numerical solver to do that."}, {"heading": "3.4 Setting the parameters", "text": "Unlike Adaboost and Logitboost, The non-convex boosting algorithms require choosing two parameters. These come in different forms, but they are all equivalent to choosing the error goal and the margin goal \u03b8. The error goal corresponds to a guess of the fraction of the examples on which we need to \u201cgive up\u201d. While the margin goal defines the minimal margin for the examples on which we are not giving up. In the next section we propose an adaptive algorithm for choosing .\nThe time associated with the n-th iteration of of the BrownBoost and RobustBoost algorithms is defined as tn = \u2211n i=1(\u2206t)i. The initial time equals zero t0 = 0 and ti\u22121 \u2265 ti increases at each iteration. The termination time is defined to be t = 1. If the algorithm reaches that time it stops. In some cases the setting for is too low and the setting for \u03b8 is too high. As a result algorithm is is not able to reach t = 1 even after a large number of iterations. The final time reached by the algorithm is a good indication of whether the parameters were set ambitiously, causing the algorithm to never reach t = 1 or not ambitiously enough, causing t = 1 to be reached after a small number of iterations.\nWe can use this as a method for tuning and \u03b8, but it is a very slow process as each trial requires running the boosting algorithm until it terminates or gets \u201cstuck\u201d."}, {"heading": "4 Adaptive- Heuristic", "text": "In BrownBoost (BB) and RobustBoost (RB), we need to specify the target error rate . The choice of can greatly influence the performance of the trained classifier. When is set too low or too high, the algorithms often produce a classifier that performs poorly even on the training data. Figure 2 shows the margin distributions of BB and RB using different on a dataset with 30% label noise after 200 iterations. Note that when \u2264 0.30, the classifier cannot separate examples around zero-margin.\nWhen the true noise rate is known, a good rule of thumb is to set a little higher than the noise rate. Table 1 summarizes the final time tf and the final training error rate Ef of BB and RB on a synthetic dataset with the true noise rate \u03b7 = {0.1, 0.2, 0.3} using two different settings slightly above and below \u03b7. However, when the true noise rate in not known, the tuning of is usually done by cross validation. The process can be very inefficient and usually involves a grid search over a small interval.\nIn this paper we propose a heuristic for tuning automatically for BB and RB. The idea is based on the following observation. When is too small, the time tk advances too slowly that the boosting procedure seems \u201cstuck\u201d. This situation can often be remedied by slightly increasing the value of without having to restart the boosting process. The heuristic can be described as follows. Initially, we set = 0 and start the boosting procedure. When the boosting algorithm does not to advance for a few iterations or the numerical solver fails to solve the non-linear equations, we slightly increase\nand resume the boosting process. In our experiments, we denote BrownBoost and Robust with adaptive- heuristic and RobustBoost with adaptive- , BBA and RBA respectively."}, {"heading": "5 Experiments", "text": "In this section we compare the performance of BrownBoost with adaptive- (BBA), RobustBoost with adaptive- (RBA) to AdaBoost (ADB) and LogLossBoost (LLB) 1 on 3 datasets with and without random label noise. Specifically we will look closely at the margin distributions for each of the boosting algorithms. Additionally, we will study the impact of using positive target margin \u03b8 on BBA and RBA.\nWe implemented all of the boosting algorithms in MATLAB and utilized the Optimization Toolbox for numerically solving BB and RB equations. For RBA, we use \u03c3f = 0.001 for all experiments."}, {"heading": "5.1 Datasets", "text": "We conducted experiments on 3 different datasets: LS, Face and Satimage. Each dataset can be described as follows. First, LS dataset is a synthetic dataset whose construction is suggested by Long and Servidio in [8]. The dataset has input x \u2208 R21 with binary features xi \u2208 \u22121,+1 and label y \u2208 \u22121,+1. Each instance is generated as follows. First, the label y is chosen to be \u22121 or +1 with equal probability. Given y and the margin width parameter \u03b4, the features xi are chosen according to the following mixture distribution:\n\u2022 Large margin: With probability 1/4, we choose xi = y for all 1 \u2264 i \u2264 21\n\u2022 Pullers: With probability 1/4, we choose xi = y for 1 \u2264 i \u2264 10 + \u03b4 and xi = \u2212y for 11 + \u03b4 \u2264 i \u2264 21\n\u2022 Penalizers: With probability 1/2, we choose 5 + b\u03b4/2c random coordinates from the first 11 and 5 + d\u03b4/2e from the last 10 to be equal to the label y. The remaining 10 coordinates are equal to \u2212y.\nThe data from this distribution can be classified perfectly by a simple linear classifier f(x) = sgn( \u2211 i xi). Note that \u03b4 essentially controls the separation margin of the examples. Larger \u03b4 yields a larger margin.\nFace dataset is a collection of face and non-face images consisting of 10000 face images and 20000 non-faces images. For each image, a feature vector of R176 is calculated based on histogram of colors and gradients. When label noise is added to Face dataset, we only added noise to the negative examples. We use 70% of the examples for training and 20% for testing.\nSatimage is a dataset from the UCI repository [1]. There are 6435 examples and 36 attributes. The original label of 1-3 is grouped as +1 and the rest is group as -1. Similar to Face dataset, we use 70% of the examples for training and 20% for testing.\n1LogLossBoost is our implementation of LogitBoost with decision stumps."}, {"heading": "5.2 Results", "text": "We first compared the performance of ADB, LLB, BBA and RBA with different label noise level \u03b7 \u2208 0.0, 0.1, 0.2, 0.3 on LS using 2 settings of the margin width parameter \u03b4 \u2208 1, 3. We used the training set size N = 1600 and ran each boosting algorithm for 200 iterations. For BBA and RBA, the margin parameter \u03b8 = 0 and \u03c3f = 0.001.\nWhen there was no label noise, all boosting algorithms managed to learn the correct linear classifier. However, with presence of label noise in both settings of \u03b4, BBA and RBA successfully converged to the correct classifier while ADA and LLB did not as indicated by the higher test error rates with respect to the true labels. Table 2 summarizes the average test error rates with respect to the true labels and the noisy labels over 10 runs and the standard deviation is reported in parentheses.\nWe also examined the progression of the margin distributions for each of the boosting algorithms. Figure 3 shows the margin distributions progression when \u03b7 = 0.3. In LS with \u03b4 = 1, ADB and LLB stopped progressing after 50 iterations due to the large weights put on the noisy largemargin examples pushing the classifier away from the correct hypothesis. On the contrary, after 100 iterations, RBA and BBA significantly decreased the weights of the noisy large-margin examples as these examples are being \u201cgiven up\u201d. As a result, the boosting process continued on and eventually converged to the correct classifier. Interestingly, in LS with \u03b4 = 3, we found that all boosting algorithms managed to attain the training error rate of 0 in all noise settings. However, the test error rates of ADB and LLB remained relatively high compared to those of BBA and RBA.\nWe further explored the benefits of the margin parameter \u03b8. We found that using a positive \u03b8 can improve generalization error. Figure 4 shows the test errors of BBA and RBA using different \u03b8 on LS with 20% noise. Both algorithms have lower generalization error when using positive \u03b8. We found that RBA is less sensitive to the setting of \u03b8 than BBA. Figure 5 summarizes the test error rates as a function of training set size for LS dataset with \u03b4 = 1 and \u03b4 = 3. For BBA and RBA, \u03b8 is tuned by cross-validation.\n\ud835\udeff = 1 \ud835\udeff = 3\nWe also compared the performance of ADB, LLB, BBA and RBA on Face and Satimage. We ran each boosting algorithm for 800 iterations on both datasets with 2 different noise levels \u03b7 = 0.0, 0.2. We also repeated the experiment after holding out of 75% of the training examples. The area under the average ROC curves is summarized in Table 3.\nFor both Face and Satimage, using paired t-test, we found that area under ROC of RBA and BBA is significantly larger than that of LLB with p < 0.001 in all settings. We also found that the difference between ADB and RBA is insignificant in noise-free cases and the difference between RBA and BBA is insignificant in all cases. For Satimage dataset,"}, {"heading": "6 Conclusion", "text": "Our experiments show that Brownboost and Robustboost are significantly more resistant to label noise than Adaboost and LogitBoost. We show how this is related to the progressions of the margin distribution over time. We show that the setting of the target error rate is of critical importance for the final performance and provide a practical heuristics for setting it. Our experiments also show that, for noisy small training sets, maximizing the margin on the examples on which we don\u2019t \u201cgive up\u201d is significantly better than minimizing the training error."}], "references": [{"title": "An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization", "author": ["T.G. Dietterich"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Y. Freund"], "venue": "Inf. Comput.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "An Adaptive Version of the Boost by Majority Algorithm", "author": ["Y. Freund"], "venue": "Mach. Learn.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "A more robust boosting algorithm", "author": ["Y. Freund"], "venue": "[stat.ML],", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Experiments with a New Boosting Algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In Proceedings of the 13th International conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Additive Logistic Regression: a Statistical View of Boosting", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Random classification noise defeats all convex potential boosters", "author": ["P.M. Long", "R. a. Servedio"], "venue": "Proceedings of the 25th international conference on Machine learning - ICML", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "An Empirical Evaluation of Bagging and Boosting", "author": ["R. Maclin", "D. Opitz"], "venue": "Proceedings of the Fourteenth National Conference on Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Boosting: Foundations and Algorithms", "author": ["R.E. Schapire", "Y. Freund"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "author": ["R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S. Lee"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}], "referenceMentions": [{"referenceID": 8, "context": "1 Introduction Adaboost [10] is a very popular classification learning algorithm.", "startOffset": 24, "endOffset": 28}, {"referenceID": 4, "context": "While generally successful, the sensitivity of Adaboost to random label noise is well documented [6, 9, 2].", "startOffset": 97, "endOffset": 106}, {"referenceID": 7, "context": "While generally successful, the sensitivity of Adaboost to random label noise is well documented [6, 9, 2].", "startOffset": 97, "endOffset": 106}, {"referenceID": 0, "context": "While generally successful, the sensitivity of Adaboost to random label noise is well documented [6, 9, 2].", "startOffset": 97, "endOffset": 106}, {"referenceID": 5, "context": "LogitBoost [7] is believed to be less sensitive to random noise than Adaboost, but it still falls pray to high levels of random labels noise.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "In fact, Servedio and Long [8] proved that, in general, any boosting algorithm that uses a convex potential function can be misled by random label noise.", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "Freund [4] suggested a boosting algorithm, called Brownboost, that uses a non-convex potential function and claims to overcome random label", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "[11] defines the \u201cmargin\u201d of an example as: m(x, y) = y~ \u03b1 \u00b7 ~h(x) Thus m(x, y) > 0 if and only if the classification rule is correct on the example (x, y).", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Moreover, as was shown in [8], algorithms that minimize convex potential functions can always be fooled by the addition of random label noise.", "startOffset": 26, "endOffset": 29}, {"referenceID": 9, "context": "[11], showed that there is a better performance measure the performance of the boosted classifier on the training set.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "m\u0302(x, y) = y ~ \u03b1 \u00b7 ~h(x) \u2016\u03b1\u20161 The intuition, presented and justified in [11], is that large positive margins correspond to confident predictions.", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "Specifically, Theorem 2 in [11] states that, with probability 1 \u2212 \u03b4 over the random choice of the training set, the following inequality holds for all \u03b8 > 0", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "3 Three non-convex boosting algorithms Freund [3, 4, 5] suggested several boosting algorithms that use non-convex potential functions.", "startOffset": 46, "endOffset": 55}, {"referenceID": 2, "context": "3 Three non-convex boosting algorithms Freund [3, 4, 5] suggested several boosting algorithms that use non-convex potential functions.", "startOffset": 46, "endOffset": 55}, {"referenceID": 3, "context": "3 Three non-convex boosting algorithms Freund [3, 4, 5] suggested several boosting algorithms that use non-convex potential functions.", "startOffset": 46, "endOffset": 55}, {"referenceID": 2, "context": "Brownboost [4] overcomes this deficiency by taking the limit of the BBM game where the number of steps goes to infinity.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "First, LS dataset is a synthetic dataset whose construction is suggested by Long and Servidio in [8].", "startOffset": 97, "endOffset": 100}], "year": 2014, "abstractText": "The sensitivity of Adaboost to random label noise is a well-studied problem. LogitBoost, BrownBoost and RobustBoost are boosting algorithms claimed to be less sensitive to noise than AdaBoost. We present the results of experiments evaluating these algorithms on both synthetic and real datasets. We compare the performance on each of datasets when the labels are corrupted by different levels of independent label noise. In presence of random label noise, we found that BrownBoost and RobustBoost perform significantly better than AdaBoost and LogitBoost, while the difference between each pair of algorithms is insignificant. We provide an explanation for the difference based on the margin distributions of the algorithms.", "creator": "LaTeX with hyperref package"}}}