{"id": "1409.2993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Sep-2014", "title": "\"Look Ma, No Hands!\" A Parameter-Free Topic Model", "abstract": "it has always been a burden matter to the users of statistical topic models to predetermine each the right number of topics, at which is a key distinguishing parameter of most topic models. mostly conventionally, automatic selection of considering this parameter is done through either statistical model selection ( e. g., cross - validation, aic, or evolutionary bic ) or bayesian continuum nonparametric models ( e. - g., hierarchical formal dirichlet process ). indeed these research methods either literally rely on repeated runs of the inference algorithm to search through a large range distributions of parameter values which does not follow suit like the mining of big procedure data, or replace this parameter with alternative parameters that are less intuitive and and still seem hard to define be determined. in this paper, thus we began explore to \" eliminate \" this parameter from a new perspective. we first present a nonparametric treatment of the plsa model named nonparametric probabilistic _ latent semantic analysis ( nplsa ). the inference procedure of nplsa allows for the exploration and comparison of different numbers of topics within a single execution, yet remains as simple as that of plsa. this is achieved by substituting the parameter of the number of topics with an alternative parameter that is therefore the minimal goodness of consistent fit of a document. we show that the new parameter can be further quickly eliminated by two parameter - free treatments : either by monitoring the diversity among the discovered topics or by requiring a weak supervision from users in the form of an exemplar topic. the chosen parameter - free topic model consistently finds the appropriate number of topics when the diversity among the discovered topics is maximized, or when the granularity of the discovered topics matches the exemplar taxonomy topic. experiments on sampling both synthetic synthetic and real data prove that the parameter - free topic table model extracts topics with a comparable quality for comparing to classical topic content models with \" manual transmission \". the quality determination of the topics outperforms those extracted through classical continuous bayesian nonparametric models.", "histories": [["v1", "Wed, 10 Sep 2014 08:41:35 GMT  (473kb,D)", "http://arxiv.org/abs/1409.2993v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.IR", "authors": ["jian tang", "ming zhang", "qiaozhu mei"], "accepted": false, "id": "1409.2993"}, "pdf": {"name": "1409.2993.pdf", "metadata": {"source": "CRF", "title": "\"Look Ma, No Hands!\" A Parameter-Free Topic Model", "authors": ["Jian Tang", "Ming Zhang", "Qiaozhu Mei"], "emails": ["tangjian@net.pku.edu.cn", "mzhang@net.pku.edu.cn", "qmei@umich.edu"], "sections": [{"heading": null, "text": "In this paper, we explore to \u201celiminate\u201d this parameter from a new perspective. We first present a nonparametric treatment of the PLSA model named nonparametric probabilistic latent semantic analysis (nPLSA). The inference procedure of nPLSA allows for the exploration and comparison of different numbers of topics within a single execution, yet remains as simple as that of PLSA. This is achieved by substituting the parameter of the number of topics with an alternative parameter that is the minimal goodness of fit of a document. We show that the new parameter can be further eliminated by two parameter-free treatments: either by monitoring the diversity among the discovered topics or by a weak supervision from users in the form of an exemplar topic. The parameter-free topic model finds the appropriate number of topics when the diversity among the discovered topics is maximized, or when the granularity of the discovered topics matches the exemplar topic. Experiments on both synthetic and real data prove that the parameterfree topic model extracts topics with a comparable quality comparing to classical topic models with \u201cmanual transmission.\u201d The quality of the topics outperforms those extracted through classical Bayesian nonparametric models.\n\u2217This study is done when the first author is visiting the University of Michigan.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$10.00.\nCategories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Text Mining\nGeneral Terms Algorithm, Experimentation\nKeywords Nonparametric models, diversity, parameter-free"}, {"heading": "1. INTRODUCTION", "text": "How many topics are there in science? 100? 1,000? How many topics are there in Wikipedia? Twitter? The Web?\nStatistical topic models (e.g., [14, 4]) are widely adopted to analyze text collections in various domains, such as the Web, scientific literature, social media, and digital humanities. Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].\nThe success of topic models largely ascribes to its ease of use and interpretability of results in exploratory data analysis. However, it has always been a burden for the users of topic models to predefine the number of topics, which is the key parameter of most topic models. The situation becomes worse when the data is large and when the domain is open, as it is even hard to determine an appropriate range of the number of topics with limited prior knowledge. As a result, the user has to execute the same algorithm many times in order to enumerate different numbers of topics. The optimal number of topics is selected either by manually assessing the quality of topics extracted or through cross validation. Such a \u201cmanual transmission\u201d process is apparently undesirable when the size of data and the number of topics are large.\nEfforts in literature have attempted to solve this problem through Bayesian nonparametric models. Such models are built on an infinite-dimensional parameter space, thus have the capability to model infinite mixtures of topics and adapt the number of topics to the complexity of the data. These nonparametric models typically substitute one parameter (i.e., number of topics) with another (e.g., the concentration parameter in hierarchical Dirichlet process), which is even less intuitive and still hard to tune. These methods also usually rely on a more sophisticated inference algorithm. The quality of the topics extracted by the nonparametric models is often compromised comparing to those discovered through\nar X\niv :1\n40 9.\n29 93\nv1 [\ncs .L\nG ]\n1 0\nSe p\n20 14\nclassical, parametric topic models. In practice, parametric models such as the probabilistic latent semantic analysis (PLSA) [14] and the latent Dirichlet allocation (LDA) [4] are still widely preferred.\nIt is our desire to find a user-friendly model that is simple, efficient, and free of parameters. Ideally, such a model should employ an inference procedure as simple as those of the classical models, and thus could be easily implemented and scaled up to handle large scale datasets. The algorithm should either automatically find the appropriate number of topics, or allow a user to interactively guide it towards the desirable number of topics, instead of shooting in the dark.\nIn this paper, we seek for such a model by exploring a new direction. We start with the intuition of how human labels books into categories. Without prior knowledge about the number of categories, a human cataloger would scan the books in order and start with a minimal number of labels. A book will be tagged with a label that it fits in, or a new label will be created for that book and passed along, if no existing label describes the book well. With this strategy, the cataloger does not have to go over all the books again and again. Can a topic model work as smartly as the human cataloger? If the existing topics cannot describe a document, it implies that another topic may exist that is significantly different from the existing ones, thus a new topic should be generated. Embedding such a process into classical topic models, we are able to grow the number of topics without the overhead of separate runs of the inference process.\nTo do this, the model has to read from the cataloger\u2019s mind for a measure of the \u201cgoodness of fit.\u201d Inspired by the likelihood ratio test [25], we can measure how well a document is described by the current topics using the likelihood ratio of it being generated by the existing topics versus being generated by all plus one more topic. In this way, the topic model can find the right number of topics whenever a minimal threshold of the goodness of fit is provided.\nSuch a treatment actually substitutes the old burden of finding the number of topics with a new burden of finding this minimal threshold of the goodness of fit. We would further eliminate this burden to the users by finding this threshold automatically. One intuition of ours is that a cataloger wants more than the goodness of fit, who also expects the categories be clearly distinguishable. With this intuition, we propose an adaptive process so that a topic model can explore decreasing thresholds and stop when the diversity among topics is maximized. We find that the diversity of topics is a good indicator of the optimal number of topics. With such an adaptive process, we present a topic model that finds the appropriate number of topics without arbitrarily defined parameters.\nAnother intuition is that a user should be allowed to guide the model towards the number of topics that best fits his personal need. While telling how many topics I anticipate is hard, naming what kind of topics I expect is much easier. With a minimal effort, the user can provide an exemplar topic as simple as a keyword. The topic model would then extract topics at a similar granularity to this example. For instance, when a user queries for \u201cmachine learning,\u201d the model returns topics including \u201cmachine learning,\u201d \u201cinformation retrieval,\u201d and \u201cdata mining,\u201d without going deeper, for example, into \u201cfrequent pattern mining.\u201d\nTo summarize, we present the following contributions:\n1. We propose a nonparametric version of PLSA, named\nnPLSA, which dynamically grows the number of topics in a single inference process. Such a treatment substitutes the parameter of the number of topics with a threshold of the minimal goodness of fit, which allows us to further eliminate the parameters.\n2. We propose a parameter-free treatment of nPLSA based on the diversity among the extracted topics, which is shown to be a good indicator of the optimal number of topics. The parameter-free nPLSA finds the appropriate number of topics when the extracted topics are the most distinctive from each other.\n3. We propose an alternative parameter-free treatment of nPLSA by receiving weak supervision from users. The model extracts topics at the same granularity of a user-provided exemplar topic (e.g., a keyword). Both models require no arbitrary parameter from the user."}, {"heading": "2. RELATED WORK", "text": "To the best of our knowledge, the two parameter-free treatments of nPLSA are the first topic models that are truly free of any parameters to be predetermined by the users. Traditional clustering algorithms such as k-means [11] and spectral clustering [33], and statistical topic models, such as PLSA and LDA, all require users to specify the number of clusters or topics. One way to choose this parameter is to run the same algorithm on the same data many times with different parameter values, and then choose the best model based on statistical model selection techniques such as the perplexity on held-out data, the Akaike information criterion (AIC) [1], or the Bayesian information criterion (BIC) [27]. These criteria generally balance the goodness of fit on the training or test data and the complexity of the model. All these methods have to execute the inference algorithm many times to search for the parameter, which is costly.\nAnother direction leads to Bayesian nonparametric models [29, 3, 8, 10], which have been attracting increasing attention in the machine learning community. These models are able to model infinite mixtures and adapt the number of clusters to the complexity of the data. For clustering problems, the Dirichlet process mixture [12] model is widely used as the nonparametric prior of the mixture components. In [29], Teh et al. proposed a Bayesian nonparametric topic model called hierarchical Dirichlet process (HDP). In HDP, each document is modeled with an individual Dirichlet process, and the mixture components are shared across documents by using the same base measure, which itself is distributed according to a Dirichlet process. In [3], a hierarchical latent Dirichlet allocation (hLDA) model was proposed based on the Dirichlet process to infer topic hierarchies from the data. Although these models are flexible and theoretically sound, they all require more sophisticated inference procedures that are difficult to implement or to scale up. Bayesian nonparametric models do not mean \u201cparameter free.\u201d In fact, these nonparametric models generally replace one parameter (i.e., number of topics) with another (e.g., the concentration parameter in HDP), which is even less intuitive and still has to be predefined.\nSome recent effort has revisited classical clustering algorithms from the Bayesian nonparametric viewpoint. In [16], Kulis and Jordan designed the DP-means algorithm, which is a scalable clustering algorithm built on top of k-means and\nis able to infer the number of clusters from the data. DPmeans shares a similar iterative process with k-means except that it allows new clusters to be generated during the learning process. The central idea of DP-means is that when the minimum distance between an instance and existing cluster centers is above some threshold, the algorithm will generate a new cluster centered in this instance. The nonparametric PLSA (nPLSA) we will present shares a similar intuition. However, the DP-means algorithm still requires users to specify the threshold of the minimal distance, which is even harder than defining the number of clusters. The two parameter-free treatments of nPLSA eliminate arbitrary parameters like such, which can also be applied to the context of data clustering."}, {"heading": "3. PLSA", "text": "The classical parametric topic model probabilistic latent semantic analysis (PLSA) assumes that each document is a mixture proportion of topics, with each topic corresponding to a multinomial distribution over the word vocabulary. The log-likelihood L(D) of the training data D is calculated by:\nL(D) = \u2211 d \u2211 w n(d,w) log \u2211 z p(z|d)p(w|z). (1)\nwhere n(d,w) is the frequency of word w in document d, p(z|d) is the probability of topic z in document d, and p(w|z) is the probability of word w being generated by topic z. Following the maximum-likelihood principle, the parameters of the model, i.e. p(z|d) and p(w|z), are estimated by maximizing the log-likelihood objective function (1). Due to the non-convexity of the log-likelihood defined by (1), it is difficult to obtain the global optimum value. To this end, the EM algorithm is generally applied for the problem. The EM algorithm alternates between two steps: E-step and M-step. In the E-step, it calculates the posterior distribution of the latent variable z conditioned on the observation and current model parameters. In the M-step, it updates the model parameters based on the posterior probability calculated in the E-step. We summarize the updating equations as below:\nE-step: computing the posterior of the latent variable p(z|d,w) as follows:\np(z|d,w) = p(z|d)p(w|z)\u2211 z\u2032 p(z \u2032|d)p(w|z\u2032) (2)\nM-step: updating the model parameters p(z|d) and p(w|z) based on the E-step:\np(z|d) \u221d \u2211 w n(d,w)p(z|d,w) (3) p(w|z) \u221d \u2211 d n(d,w)p(z|d,w) (4)\nTo apply PLSA for topic modeling, users are required to specify the number of topics, K. However, it is generally very hard for users to estimate the number of topics in their datasets a priori, and this brings a considerable burden to the users. In the next section, we introduce a series of methods to eliminate the parameter K."}, {"heading": "4. ELIMINATING THE PARAMETER K", "text": "To eliminate the parameter K, or to automatically choose the optimum K, one natural way is to execute the PLSA\nalgorithm with different values of K and then choose the value that maximizes the likelihood of a held-out dataset or a statistical model selection criterion, such as AIC and BIC. These methods inevitably require many runs of the algorithm and hence are computationally expensive. A better way is to explore different numbers of topics within a single execution of the algorithm. Motivated by this, in Section 4.1 we introduce a nonparametric variation of PLSA named nonparametric PLSA (nPLSA), which allows the number of topics to dynamically increase and hence can compare different values of K within a single run of the algorithm. Although the nPLSA model still relies on a user parameter different from K, it can be eliminated either by making use of the diversity among the topics (Section 4.2) or weak supervision from users by providing an exemplar topic, which is as simple as simple as a query (Section 4.3)."}, {"heading": "4.1 Nonparametric PLSA", "text": "The nonparametric PLSA (nPLSA) model is motivated by the intuition that if a document is not well fitted by the current topics, then it implies at least one different topic exists in the document, which is not covered by existing topics. One natural reaction is to allow the document to self-promote into a new topic. In this way, this document should fit well into this new set of topics (the union of the original topics and the new topic emitted by the document). This procedure can be naturally applied to a collection of documents in a sequential manner.\nOne important issue is how to measure the goodness of fit of a document by the existing topics. Let \u0398 be a set of topics and \u03b8d be the language model of document d. If a new topic was promoted by the document d, an expanded set of topics would emerge as \u0398\u2032 = \u0398 \u222a \u03b8d. Naturally, the new topic would emerge as the language model of the document d. This decision can be made by comparing how well the document d is fitted by the two models \u0398 and \u0398\u2032. If \u0398\u2032 gives a much better explanation of d than \u0398, then the new topic should be generated; otherwise the current set of topics should be carried on. Formally, a likelihood ratio test [25] can be used to compare the goodness of fit of two models, with the requirement that one is nested within the other (in our case, \u0398 is nested within \u0398\u2032). Inspired by this, we define a distance metric between a document d and a set of topics \u0398 as the log-likelihood ratio of the two models \u0398,\u0398\u2032:\n\u2206(d,\u0398) = log p(wd|\u0398\u2032 = \u0398 \u222a \u03b8d)\u2212 log p(wd|\u0398)1, (5)\nwhere wd refers to all the words in d. In reality, since the maximum likelihood estimation of document d will yield to its own language model \u03b8d, we have log p(wd|\u0398\u2032) = log p(wd|\u03b8d). Eqn. (5) becomes:\n\u2206(d,\u0398) = log p(wd|\u03b8d)\u2212 log p(wd|\u0398). (6)\nLet be a minimal threshold of the distance above (note this threshold parameter will be eliminated in the parameterfree treatments in the next two subsections). A new topic will be generated if \u2206(d,\u0398) is above this threshold, and otherwise \u0398 will be carried on. This process can be easily integrated into the EM algorithm of the original PLSA model, resulting in a nonparametric algorithm summarized into Algorithm 1. More specifically, when making the inference of each document d in the E-step, we first calculate 1For clarity purpose, we use the notation log p(wd|\u0398) as the maximum log-likelihood of document wd fitted by \u0398.\nthe goodness of the document d by existing topics \u0398, i.e. \u2206(d,\u0398). If \u2206(d,\u0398) > , the algorithm will generate a new topic initialized as \u03b8d, and all the words in the document d will be initially labeled with the new topic. Otherwise, the algorithm will not generate a new topic and instead infer the latent variables z using the current model. Note that when a document is visited in one of the EM iterations, the number of topics may have changed since it was visited in the previous iteration. That says, if the number of topics equals the number of topics previously fitted into this document, then the inference of this document can be done based on Eqn. (2); otherwise, we will need to fit this document with the new set of topics so that the new topics have a chance to be adapted to the document. This can be achieved by a \u201cfold-in\u201d process [14], which updates the topic distribution of the words in the document, i.e. p(z|d,w), and topic proportions of the document, i.e. p(z|d), by maximizing the likelihood of the document. This process continues until no new topic is generated and the log-likelihood converges.\nAlgorithm 1: The nPLSA model\nInput: Training data D, : the threshold of the minimal distance \u2206(d,\u0398). ; Output: Number of topics K, the word distributions of topics {p(w|z)}z=1,...,K ; initialization: K = 1, randomly initialize this topic; set the number of topics fitted to each document Td = 1, for each d = 1, . . . |D|; while not convergence do\nE-step: for each document d in {1, . . . , |D|}\n1. Calculate the log-likelihood ratio \u2206(d,\u0398)\n2. if \u2206(d,\u0398) > then\n(a) set K = K + 1, \u0398 = \u0398 \u222a \u03b8d; (b) p(z = K|w, d) = 1 and p(z = j|w, d) = 0 for\nj < K;\nelse if Td=K then make inference based on Eqn. (2);\nelse conduct a \u201cfold-in\u201d process ;\nTd = K;\nM-step: update model parameters \u0398 according to Eqn. (3) and (4).\nend"}, {"heading": "A Theoretical Interpretation", "text": "The procedure of Algorithm 1 is similar to that of original PLSA model except that it allows new topics to be generated in the E-step. Although remaining simple, the algorithm does have a principled theoretical interpretation. Specifically, one can prove that the whole procedure is actually monotonically maximizing the following objective function\u2211\nd \u2211 w n(d,w) log K\u2211 z=1 p(z|d)p(w|z)\u2212 K, (7)\nwhich is the log-likelihood of the training data penalized by the number of topics. The coefficient of the penalization is\nessentially the threshold in Algorithm 1. In other words, the introduction of one extra topic results in the loss of one minimal threshold of likelihood ratio, . Clearly, a smaller results in a larger number of topics. Such an objective function is in fact closely related to the Akaike information criterion (AIC) [1] in statistical model selection and also related to the objective in DP-means [16].\nWe now prove that Algorithm 1 is maximizing the objective function (7) in both the E-step and the M-step. In E-step, there are three different cases: (1) \u2206(d,\u0398) > , based on which one can obtain log p(wd|\u0398\u2032) \u2212 (K + 1) > log p(wd|\u0398)\u2212 K, with the likelihood of all other documents unchanged, thus improving the objective in Eqn. (7); (2) Td = K, which boils down to the same inference procedure of the E-step in PLSA; and (3) Td < K, which introduces a \u201cfold-in\u201d process that maximizes the likelihood of the current document without touching other documents. The M-step is the same as classical PLSA procedure, which maximizes the log-likelihood of data without changing the penalty term, thus also improving the objective. The objective (7) is upper-bounded as long as there is finite number of topics in the data. By the monotone convergence theorem, the algorithm will finally converge to a local maximum of the objective function.\nIn summary, the nPLSA model is a nonparametric topic model with an inference procedure much easier than those of the Bayesian nonparametric methods. The nPLSA model replaces the parameter K with a different parameter . Although can be as hard to predetermine as the K, the benefit of doing this is that now nPLSA can automatically compare different numbers of topics within a single execution of the algorithm. This makes it possible to further eliminating such an arbitrary parameter, or to find the optimal parameter value automatically. In the following, we provide two ways to get rid of the parameter by utilizing the diversity among the topics (Section 4.2) or requesting weak supervision from users, which is an exemplar topic as simple as a query keyword (Section 4.3)."}, {"heading": "4.2 Parameter-free nPLSA through Topic Diversity", "text": "The nPLSA model makes it possible to gradually grow the number of topics in a single inference procedure. Intuitively, it is desirable to discover the most significant topics first, and then dive deeper into the finer topics. With nPLSA, one can start with a large to generate topics that are largely divergent from existing topics, and then gradually decay to distinguish topics that are closer together. As long as is monotonically decreasing, one is still maximizing the objective (7) with the smallest explored. The question is when to stop generating new topics, or stop exploring a smaller .\nOur treatment makes use of a measure of diversity among the topics as a criterion to stop generating new topics. Diversity is widely studied as a criteria in ranking [20], query suggestion [19] and document summarization [5]. Some existing work has used diversity as an evaluation criteria for topic quality [21], but not for selecting the optimum number of topics (compare across different numbers of topics). We formally define the diversity among a set of topics as follows:\nDiversity(\u0398) = 2\n(K \u2212 1)K K\u22121\u2211 i=1 K\u2211 j=i+1 dist(\u03b8i, \u03b8j), (8)\nAlgorithm 2: The Parameter-free nPLSA model\nInput: Training data D ; Output: Number of topics K, the word distributions of topics {p(w|z)}z=1,...,K ; initialization: K = 1, randomly initialize this topic ; while not convergence do\nCalculate Diversity(\u0398); E-step: if Topic diversity does not achieve the optima then\n1. calculate the log-likelihood ratio \u2206(d,\u0398) for each document d, and find d\u2217 = argmaxd\u2206(d,\u0398)\n2. set K = K + 1, \u0398 = \u0398 \u222a \u03b8d\u2217 ;\n3. p(z = K|w, d\u2217) = 1 and p(z = j|w, d\u2217) = 0 for j < K;\n4. for d 6= d\u2217, conduct a \u201cfold-in\u201d process ;\nelse for each d, make inference based on Eqn. (2); M-step: update model parameters \u0398 according to Eqn. (3) and (4).\nend\nwhere K is the number of topics, \u03b8i is the word distribution of the ith topic, and dist(\u00b7, \u00b7) is a distance function between the two models. In practice, one can instantiate the distance function with the L2 distance or the Jensen\u2212Shannon divergence [7] (the symmetric version of the Kullback\u2212Leibler divergence [7]). In this work, we simply adopt the L2 distance because it is more sensitive to the top-ranked words.\nIntuitively, if the number of topics is small, the learned topics tend to be close to the background language model and thus do not distinguish well between each other. When the number of topics grows, the granularity of topics becomes finer and the topics become more distinguishable, thus increasing the diversity. However, when the number of topics becomes too large, we start to obtain many small topics which may be too close to each other, which decreases the topic diversity. Therefore, diversity seems to be a good measure to capture the right granularity of topics.\nTo verify this, we conducted topic modeling analysis on a synthetic dataset, in which the correct number of topics is known (see Figure 2(a) in Section 5). The result confirms our intuition and proves that topic diversity can be a good indicator of the right number of topics.\nTherefore, we utilize the diversity among topics as a criteria for stopping the generation of new topics in nPLSA, which yields a new process shown in Algorithm 2. Specifically, we adopt a farthest-first heuristic. In each iteration, we first find the document d\u2217 that has the largest distance \u2206(d,\u0398) to existing topics \u0398. Then a new topic is generated from document d\u2217, i.e. \u0398 = \u0398 \u222a \u03b8d\u2217 . This heuristic is related to the process for sampling seedling points in the KMeans++ algorithm [2]. This means exactly one new topic is generated in each EM iteration. We do foresee relaxations of this heuristic so that multiple new topics could be generated in a single iteration, which will further improve the efficiency of the algorithm. For other documents, a \u201cfold-in\u201d process is conducted to fit them with the new configuration of topics. During the process, the diversity among the cur-\nrent set of topics is monitored. When the diversity of the topics achieves an optima, the algorithm stops generating new topics and the original inference procedure of PLSA will be conducted until convergence. Such a process does not rely on any predefined parameter, neither K or .\nThe parameter-free nPLSA model through topic diversity provides an automatic way of finding the number of topics that best fits the data. In practice, a user may have a personal preference about the granularity of topics. For example, he or she may want to explore topics at a higher level like \u201cdata mining\u201d or \u201cmachine learning,\u201d while another user may explore topics with a finer granularity like \u201cfrequent pattern mining\u201d or \u201csemi-supervised learning.\u201d A friendly topic model may adapt the number of topics to this personal need. Below we present another parameter-free treatment to nPLSA, which utilizes weak supervision from a user and finds the number of topics that best fits the need of the user."}, {"heading": "4.3 Parameter-free nPLSA through Weak Supervision", "text": "The alternative parameter-free treatment of nPLSA allows a user to steer the topic modeling process by providing limited supervision. In practice, we found that while it is hard for a user to define the number of topics in a given dataset, it is usually much easier for her to describe an exemplar topic of her expectation (e.g., \u201cI want to find topics like data mining.\u201d). To minimize the burden on a user, we thus allow her to provide an exemplar topic in the form of a keyword query, to guide the algorithm towards the desirable granularity of topics. This query-by-example design makes a much more reasonable assumption about the users, because it is an everyday practice to construct search queries.\nLet us define the distance of a set of topics \u0398 to the query q as the distance between q and the topic closest to it:\nd(\u03b8q,\u0398) = min \u03b8i\u2208\u0398 L2(\u03b8q, \u03b8i), (9)\nwhere \u03b8q is the query language model. While a query can be as short as a keyword, it is the common practice in information retrieval to estimate a robust \u03b8q though model-based pseudo feedback [34]. We first retrieve all the documents containing q, which are called feedback documents F . Each feedback document is assumed to be a mixture of the query model \u03b8q and a collection background model \u03b8C . \u03b8q is estimated by maximizing the likelihood of the feedback documents F under this mixture assumption.\nIn the process of Algorithm 2, the granularity of topics decreases as they are split into subtopics. Initially the algorithm may identify a topic with a coarse granularity that is closest to \u03b8q. As the algorithm runs, this topic is split into subtopics, and one of its subtopics may become closer to \u03b8q. During this process, d(\u03b8q,\u0398) will decrease. When the granularity of the topics matches the granularity of \u03b8q, the distance d(\u03b8q,\u0398) will achieve the minimum. Now when the topic closest to q is further split into smaller topics, the centers of the subtopics are likely to move away from the center of \u03b8q (see our empirical results in Figure 6). The algorithm thus stops generating new topics when d(\u03b8q,\u0398) achieves a minimum, and at that point it discovers topics with the same granularity as q. The whole process remains the same as Algorithm 2 except that the criterion for stopping generating new topics becomes minimizing d(\u03b8q,\u0398) instead of maximizing diversity.\nTo summarize, the two parameter-free treatments of nPLSA\nautomatically find the number of topics that are either the most distinctive from each other or the best fits to the user\u2019s need, without enumerative runs of the inference algorithm. Next we provide more discussion on some practical issues of the above three models."}, {"heading": "4.4 Discussion", "text": "How to determine when topic diversity is maximized? Since the diversity among the topics may not be a convex function with respect to the number of topics, in practice we can wait several iterations until the topic diversity begins to decrease smoothly. An alternative solution is to run the parameter-free nPLSA with enough iterations to see how the topic diversity changes, from which we can figure out the optimum number of topics. Then one can simply execute a classical topic model using this optimum number of topics. It is similar to determine when d(\u03b8q,\u0398) achieves the minimum. Sensitivity to the order of documents. As the nPLSA model introduced in Section 4.1 processes the documents in sequential order, one concern is whether the order of the documents affects the result. We will empirically show that the result is not sensitive to the order of the documents (See Figure 3(d) in Section 5). The parameter-free treatment of nPLSA with either topic diversity or weak supervision does not depend on the order of the documents. In each iteration, each of the two scans the entire dataset to get the farthest document and generate a new topic from it; for the rest of the documents, a \u201cfold-in\u201d process is conducted to fit them with the current set of topics. Scalability. The parameter-free nPLSA introduced in Section 4.2 and 4.3 are batch algorithms. To suit the mining of streaming and big data, extending them into online inference algorithms appears to be more reasonable than making them parallel. We leave the online inference procedure of parameter-free nPLSA for future work."}, {"heading": "5. EXPERIMENTS", "text": "We evaluate the nPLSA model and its parameter-free versions against classical topic models PLSA and LDA, and also the Bayesian nonparametric topic model HDP2. We evaluate all the models on both synthetic and real-world datasets."}, {"heading": "5.1 Evaluation Metrics", "text": "We first introduce the evaluation metrics. On synthetic data, we know the ground truth of the topics. We denote the ground truth topics as \u0398\u2217 = {\u03b8\u2217k}k=1,...K\u2217 , and the learned topics as \u0398 = {\u03b8j}j=1,...K , where K\u2217,K are the number of topics in ground truth and the number of topics learned by the algorithm(s). The quality of the learned topics can be measured by comparing them to the ground-truth topics.\nTopic Quality Error. The topic quality error measures the error that the learned topics introduce to the ground truth topics. The quality error of each individual learned topic is defined as the minimum distance between the topic and the ground truth topics. The overall topic quality error\n2We adopt the implementation of HDP from http://www. cs.cmu.edu/~chongw/resource.html. The concentration parameters for both levels are set as 1 by default.\nis averaged over all the learned topics:\nTQE = 1\nK K\u2211 j=1 min k=1,...K\u2217 L2(\u03b8j , \u03b8 \u2217 k). (10)\nOn real data, the ground truth is unknown. We introduce two metrics that do not depend on the ground truth of the topics. One of them measures the semantic coherence of the topics discovered [24, 23, 22]. In [24], the point-wise mutual information is used. Finally, we present the metric of perplexity, which is a commonly used metric in literature.\nTopic Semantic Coherence. The topic semantic coherence is used to measure the semantic coherence of the learned topics. For each topic, the semantic coherence is defined as the average point-wise mutual information score of every pair of the top-ranked words in a topic \u03b8 \u2208 \u0398:\nPMI(\u0398) = 1\nK \u2211 \u03b8\u2208\u0398\n2 N(N \u2212 1) \u2211\n1\u2264i<j\u2264N log\np(wi,\u03b8, wj,\u03b8)\np(wi,\u03b8)p(wj,\u03b8) , (11)\nwhere wi,\u03b8, wj,\u03b8 are the words ranked at the i th and jth positions in topic \u03b8. p(wi,\u03b8, wj,\u03b8) is the probability that the pair of words co-occur in the same document while p(wi,\u03b8) is probability of a word wi,\u03b8 appearing in a single document. In our experiments, we focus on the top 20 words per topic (N = 20). A large dataset is required to calculate the pointwise mutual information of the word pairs.\nPerplexity. The perplexity is used to measure the predictive performance of the topics learned on a held-out dataset. Specifically, for each document wj in the held-out dataset, we split the document into two parts wj = (wj1,wj2) and compute the predictive likelihood of the second part wj2 (20% of the words) conditioned on the first 80% of the words and the training data, the same way used in [30]. It can be calculated by:\nperplexity = exp { \u2212 \u2211 j log p(wj2|wj1, Dtrain)\u2211 j |wj2| }\n(12)"}, {"heading": "5.2 Results on Synthetic Dataset", "text": "We first compare the behaviors of models on a synthetic dataset so that we know the ground truth of topics. We generate a collection of 1,000 documents with 200 words each using the generative process of LDA with 20 topics, a vocabulary size of 1,000, and the parameters of Dirichlet priors for topic mixtures and word distributions being 0.1 and 0.01. To make the underlying topics differentiable, we only keep topics whose minimum distance to existing topics is larger than 0.5.\nFigure 1 presents the results evaluated by different metrics on the synthetic datasets. All the results are averaged over 10 runs with random data generation. Figure 1(a) shows the topic quality errors of different models w.r.t. different numbers of topics. For the nPLSA model, we vary from 10 to 400, and plot the results w.r.t the number of topics the model generates. For HDP, we use the default settings and plot the performance similarly (each run corresponds to one data point on the figure ). The performances of PLSA, LDA and nPLSA are comparable and all significantly outperform HDP. The optimum performances of PLSA, LDA, and nPLSA are achieved around the ground truth number of topics.\nThe most interesting observation presents in Figure 1(b), which plots the diversity of the topics learned by different models. The diversity of the topics discovered by the nPLSA model is also comparable with that of PLSA and LDA, much higher than that of HDP. Reconfirming our intuition in Section 4.2, the topic diversity increases when the number of topics increases from a small number of topics, peaks when the number of topics arrives at the ground truth, and drops when even more topics are generated. Note that diversity measure does not rely on the ground truth topics, suggesting that it is a good criterion for topic modeling in real data.\nResults of the parameter-free nPLSA\nNext, we investigate the performance of the parameterfree nPLSA model (Algorithm 2). Figure 2(a) presents the diversity of the topics discovered by the parameter-free nPLSA model in a single run along the iterations or the number of topics (exactly one topic is generated per iteration). The topic diversity monotonically increases to the optima when the number of topics hits 21, and then monotonically drops. This means the parameter-free nPLSA model will yield 21 topics, which is very close to the ground truth, 20. Figure 2(b) shows the distributions of \u2206(d,\u0398) of all the documents w.r.t. the number of topics. Overall, the average of \u2206(d,\u0398) is decreasing as more topics are generated. This means that the documents are fitted better with more topics. When the number of topics reaches the ground truth, the distribution of \u2206(d,\u0398) becomes stable. This is because the nPLSA model has already discovered all the underlying topics, and introducing more topics will hardly fit the documents better.\nTo summarize, on synthetic data the nPLSA model can obtain comparable performance as PLSA and LDA and significantly outperforms HDP. The diversity of the topics will achieve the optima around the right number of topics, making it a good criteria for selecting the right number of topics. With a single run, the paramter-free nPLSA model can find this optima and yield the appropriate number of topics."}, {"heading": "5.3 Results on Real-World Datasets", "text": "Besides the promising results on synthetic data, we step forward to evaluate all the models on real-world datasets. The first dataset we use is the computer science bibliography DBLP from [28] 3. It includes the records of 1,572,277 papers, from which we kept all the papers with complete abstracts, which yields 529,386 abstracts. We randomly select 5,000 abstracts from the collection and use them as the training data. We removed all the stop words and words that appeared in less than 20 training documents. In this way, 2,899 unique words are obtained. We conducted a 10- fold cross validation on the training dataset. For calculating the semantic coherence of the topics, we utilized the whole abstract dataset.\nFigure 3 presents the behaviors of different models on the DBLP dataset. Figure 3(a) plots the perplexity of held-out data against the number of topics. The perplexity of PLSA, LDA and nPLSA all becomes saturated when the number of topics is sufficiently large. The nPLSA performs comparably with PLSA and LDA and outperforms HDP. Note that directly using the perplexity metric as the stopping criterion for nPLSA seems problematic, as it saturates but does not actually converge.\nFigure 3(b) compares the semantic coherence of the topics discovered by different models, measured by the pointwise mutual information. For PLSA, LDA, and nPLSA, the metric presents a clear bell shape, which peaks when the number of topics is moderate (e.g., generally between 50 and 150). The performance of the three models well outperforms HDP. Indeed, when reading the actual topics discovered by HDP, we found that it discovered some very smallish topics, which are very difficult to interpret. This behavior may be due to the rich-gets-richer nature of the Dirichlet process.\nFigure 3(c) compares the topic diversity of different models w.r.t different number of topics. The diversity of PLSA, nPLSA, and LDA increases at the beginning and then begins to drop after the number of topics is large enough. This observation is consistent with that on synthetic data. Inter-\n3http://arnetminer.org/citation\nestingly, the diversity of topics extracted by nPLSA outperforms the diversity of topics extracted by PLSA (and LDA) when the number of topics is large, given how similar of inference process is. This indicates that the generation process of new topics in nPLSA finds better seedling topics, where in PLSA and LDA all topics are initialized randomly.\nOne may wonder how much the process of nPLSA is sensitive to the order in which the documents are processed. In Figure 3(d), we plot the distribution of the number of topics over 50 randomized orderings of the documents. The number of topics discovered by nPLSA is not sensitive to the ordering of documents.\nResults of the parameter-free nPLSA on DBLP\nFigure 4(a) presents the diversity between the topics generated by the parameter-free nPLSA model (Algorithm 2) in a single run w.r.t the number of topics discovered. Clearly, the diversity of topics increases with more topics at the beginning, reaches an optima around 140, and decreases when more topics are generated. Consistent with the behavior on synthetic data, the parameter-free nPLSA model is able to find the appropriate number of topics by maximizing the diversity metric. Figure 4(b) shows how the parameter (the maximum \u2206(d,\u0398) in each iteration) changes w.r.t the number of iterations (topics).\nTable 1 lists some example topics discovered by the parameterfree nPLSA, compared with the topics discovered by PLSA and LDA with the same number of topics provided. The topics discovered by nPLSA are comparable with those found by PLSA and LDA, which is promising given that the num-\nber of topics are adapted in a single run. In Figure 5, we also compared the efficiency of finding the number of topics using the parameter-free nPLSA versus using enumerative runs of PLSA/LDA. The parameterfree nPLSA is significantly more efficient, especially when the size of the data increases (Figure 5(a)). The time complexity of nPLSA grows linearly with the number of topics discovered, while enumerating different numbers of topics using LDA or PLSA results in an quadratic growth with the number of topics (Figure 5(b)). This suggests that the parameter-free nPLSA is especially suitable for bigdata from an open domain, where there exists lots of topics.\nResults of the weakly-supervised nPLSA on DBLP\nThe observations above prove that the parameter-free treatment of nPLSA performs well in reality and learns the appropriate number of topics from the data. What if the user specifies a personalized preference of the granularity of topics? We move forward to evaluate the weakly-supervised nPLSA model, which allows the user to query with an exemplar topic. The model will stop generating new topics\nwhen the distance of the closest topic to the query is minimized. In Figure 6, we present the results of the weaklysupervised nPLSA model with two different queries. For each query, we plot how the distance between the closest topic and the query, d(\u03b8q,\u0398), changes w.r.t the number of topics discovered. Top five words of the highest probability are listed to present the meaning of the topic. Figure 6(a) studies the query \u201cdata mining.\u201d In the first several iterations, the closest topic to this query presents words like \u201csystem, paper, data, network, model,\u201d which is more likely a background topic of the corpus. After several iterations, the closest topic becomes \u201cweb, data, search, information, knowledge,\u201d which seems to be a mixture of information retrieval and data mining. When more topics are generated, the closest topic becomes \u201cdata query database clustering mining,\u201d which is precisely the \u201cdata mining\u201d topic the user is looking for. There we can see that the distance d(\u03b8q,\u0398) achieves the minimum around 25 iterations. At this point the algorithm should stop generating new topics. However, if the algorithm continues to generate new topics, the next closest topic to the query will become \u201cpattern, mining, sensitive, frequent, output,\u201d which is about \u201cfrequent pattern mining,\u201d a subtopic of \u201cdata mining.\u201d In this case, the granularity of the topics becomes too small comparing to the user\u2019s preference. We then study another query \u201cfrequent pattern mining,\u201d which indicates a finer granularity of interest than \u201cdata mining.\u201d From Figure 6(b), we can see that distance d(\u03b8q,\u0398) achieves the minimum around 70 topics, indicating that there are more topics at the granularity of \u201cfrequent pattern mining\u201d than those comparable to \u201cdata mining.\u201d The shortest distance becomes stable after the topic \u201cpattern, mining, patterns, output, items\u201d is found. This is precisely the \u201cfrequent pattern mining\u201d topic, which does not further split along the iterations.\nTable 2 list some exemplar topics discovered by the weaklysupervised nPLSA model given different exemplar queries. We can see that the granularity of those topics is close to the query topic and well satisfies users\u2019 personal needs.\nIn our experiments, we also applied the nPLSA model and its parameter-free versions to a social media dataset collected from Twitter. As the length of tweets is too short for topic modeling analysis [15], we adopt the same strategy as [15] by treating each hashtag as a \u201cpseudo document\u201d and concatenate all the tweets containing this hashtag into the same document. A sample of 10,769 \u201cdocuments\u201d are collected from a seven-days time window, with a vocabulary of 121,709 words. Similar behaviors of the models are observed on the Twitter dataset. Due to the space limit, we omit the detailed description of the results on the Twitter dataset.\n6. CONCLUSION\nWe presented a series of treatments to the classical topic model, PLSA, in order to eliminate the arbitrary predetermination of the number of topics. We first proposed a nonparametric topic model named nPLSA, which makes it possible to grow the number of topics in a single inference procedure that remains simple and efficient. Two parameter-free treatments of nPLSA are then presented that truly eliminate all arbitrary parameters. One of them finds the number of topics that are the most distinctive from each other, and the other allows a user to provide an exemplar topic and finds topics at the same granularity of the example. The nPLSA model and its parameter-free treatments have considerable advantage over parametric topic models such as PLSA and LDA, and Bayesian nonparametric models such as HDP. Future work will focus on further scaling these models for big data. In particular, it is possible to develop an online inference procedure of these models similar to the practice in [13] and [32], as well as distributed versions of the algorithms. We also plan to investigate alternative stopping criteria for the parameter-free inference procedure."}, {"heading": "7. REFERENCES", "text": "[1] H. Aikake. Information theory and an extension of the\nmaximum likelihood principle. In B. N. Petrov and F. Csaki, editors, Proceedings of 2nd International Symposium on Information Theory, pages 267\u2013281. Akademiai Kiado, 1973.\n[2] D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027\u20131035. Society for Industrial and Applied Mathematics, 2007. [3] D. M. Blei, T. L. Griffiths, M. I. Jordan, and J. B. Tenenbaum. Hierarchical topic models and the nested chinese restaurant process. In NIPS, 2003. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993\u20131022, Mar. 2003. [5] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In SIGIR, pages 335\u2013336, New York, NY, USA, 1998. ACM. [6] J. Chang and D. M. Blei. Relational topic models for document networks. Journal of Machine Learning Research - Proceedings Track, 5:81\u201388, 2009. [7] T. M. Cover and J. A. Thomas. Elements of information theory. Wiley-Interscience, New York, NY, USA, 1991. [8] S. Gershman, M. D. Hoffman, and D. M. Blei. Nonparametric variational inference. CoRR, abs/1206.4665, 2012. [9] T. L. Griffiths and M. Steyvers. Finding scientific topics. PNAS, 101(suppl. 1):5228\u20135235, 2004.\n[10] L. Hannah, D. M. Blei, and W. B. Powell. Dirichlet process mixtures of generalized linear models. Journal of Machine Learning Research, 12:1923\u20131953, 2011. [11] J. A. Hartigan and M. A. Wong. A K-means clustering algorithm. Applied Statistics, 28:100\u2013108, 1979. [12] N. Hjort, C. Holmes, P. Mueller, and S. Walker. Bayesian Nonparametrics: Principles and Practice. Cambridge University Press, Cambridge, UK, 2010. [13] M. D. Hoffman, D. M. Blei, and F. R. Bach. Online learning for latent dirichlet allocation. In NIPS, pages 856\u2013864, 2010. [14] T. Hofmann. Probabilistic latent semantic indexing. In SIGIR, pages 50\u201357, New York, USA, 1999. ACM. [15] L. Hong and B. D. Davison. Empirical study of topic modeling in twitter. In Proceedings of the First Workshop on Social Media Analytics, SOMA \u201910, pages 80\u201388, New York, NY, USA, 2010. ACM. [16] B. Kulis and M. I. Jordan. Revisiting k-means: New algorithms via bayesian nonparametrics. CoRR, abs/1111.0352, 2011. [17] C. Lin and Y. He. Joint sentiment/topic model for sentiment analysis. In CIKM, pages 375\u2013384, 2009. [18] Y. Liu, A. Niculescu-Mizil, and W. Gryc. Topic-link lda: joint models of topic and author community. In ICML, pages 665\u2013672, New York, NY, USA, 2009. ACM. [19] H. Ma, M. R. Lyu, and I. King. Diversifying query suggestion results. In AAAI, 2010. [20] Q. Mei, J. Guo, and D. R. Radev. Divrank: the interplay of prestige and diversity in information networks. In KDD, pages 1009\u20131018, 2010. [21] D. M. Mimno, H. M. Wallach, E. M. Talley, M. Leenders, and A. McCallum. Optimizing semantic coherence in topic models. In EMNLP\u201911, pages 262\u2013272, 2011. [22] D. M. Mimno, H. M. Wallach, E. M. Talley, M. Leenders, and A. McCallum. Optimizing semantic coherence in topic models. In EMNLP, pages 262\u2013272, 2011.\n[23] D. Newman, J. H. Lau, K. Grieser, and T. Baldwin. Automatic evaluation of topic coherence. In NAACL, 2010. [24] D. Newman, Y. Noh, E. M. Talley, S. Karimi, and T. Baldwin. Evaluating topic models for digital libraries. In JCDL, pages 215\u2013224, 2010. [25] J. Neyman and E. S. Pearson. On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231:289\u2013337, 1933. [26] R. Parimi and D. Caragea. Predicting friendship links in social networks using a topic modeling approach. In PAKDD, pages 75\u201386, 2011. [27] G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6(2):461\u2013464, 1978. [28] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su. Arnetminer: extraction and mining of academic social networks. In KDD, pages 990\u2013998, 2008. [29] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101:1566\u20131581, December 2006. [30] H. M. Wallach, I. Murray, R. Salakhutdinov, and D. M. Mimno. Evaluation methods for topic models. In ICML, page 139, 2009. [31] C. Wang and D. M. Blei. Collaborative topic modeling for recommending scientific articles. In KDD, pages 448\u2013456, 2011. [32] C. Wang, J. W. Paisley, and D. M. Blei. Online variational inference for the hierarchical dirichlet process. Journal of Machine Learning Research, 15:752\u2013760, 2011. [33] H. Zha, X. He, C. H. Q. Ding, M. Gu, and H. D. Simon. Spectral relaxation for k-means clustering. In NIPS\u201901, pages 1057\u20131064, 2001. [34] C. Zhai and J. D. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In CIKM, pages 403\u2013410, 2001."}], "references": [{"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Aikake"], "venue": "B. N. Petrov and F. Csaki, editors, Proceedings of 2nd International Symposium on Information Theory, pages 267\u2013281. Akademiai Kiado", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1973}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027\u20131035. Society for Industrial and Applied Mathematics", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan", "J.B. Tenenbaum"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "The use of mmr", "author": ["J. Carbonell", "J. Goldstein"], "venue": "diversity-based reranking for reordering documents and producing summaries. In SIGIR, pages 335\u2013336, New York, NY, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Relational topic models for document networks", "author": ["J. Chang", "D.M. Blei"], "venue": "Journal of Machine Learning Research - Proceedings Track, 5:81\u201388", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "Wiley-Interscience, New York, NY, USA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Nonparametric variational inference", "author": ["S. Gershman", "M.D. Hoffman", "D.M. Blei"], "venue": "CoRR, abs/1206.4665", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "PNAS, 101", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Dirichlet process mixtures of generalized linear models", "author": ["L. Hannah", "D.M. Blei", "W.B. Powell"], "venue": "Journal of Machine Learning Research, 12:1923\u20131953", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A K-means clustering algorithm", "author": ["J.A. Hartigan", "M.A. Wong"], "venue": "Applied Statistics, 28:100\u2013108", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1979}, {"title": "Bayesian Nonparametrics: Principles and Practice", "author": ["N. Hjort", "C. Holmes", "P. Mueller", "S. Walker"], "venue": "Cambridge University Press, Cambridge, UK", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning for latent dirichlet allocation", "author": ["M.D. Hoffman", "D.M. Blei", "F.R. Bach"], "venue": "NIPS, pages 856\u2013864", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "SIGIR, pages 50\u201357, New York, USA", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "Empirical study of topic modeling in twitter", "author": ["L. Hong", "B.D. Davison"], "venue": "Proceedings of the First Workshop on Social Media Analytics, SOMA \u201910, pages 80\u201388, New York, NY, USA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Revisiting k-means: New algorithms via bayesian nonparametrics", "author": ["B. Kulis", "M.I. Jordan"], "venue": "CoRR, abs/1111.0352", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Joint sentiment/topic model for sentiment analysis", "author": ["C. Lin", "Y. He"], "venue": "CIKM, pages 375\u2013384", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Topic-link lda: joint models of topic and author community", "author": ["Y. Liu", "A. Niculescu-Mizil", "W. Gryc"], "venue": "ICML, pages 665\u2013672, New York, NY, USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Diversifying query suggestion results", "author": ["H. Ma", "M.R. Lyu", "I. King"], "venue": "AAAI", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Divrank: the interplay of prestige and diversity in information networks", "author": ["Q. Mei", "J. Guo", "D.R. Radev"], "venue": "KDD, pages 1009\u20131018", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimizing semantic coherence in topic models", "author": ["D.M. Mimno", "H.M. Wallach", "E.M. Talley", "M. Leenders", "A. McCallum"], "venue": "EMNLP\u201911, pages 262\u2013272", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Optimizing semantic coherence in topic models", "author": ["D.M. Mimno", "H.M. Wallach", "E.M. Talley", "M. Leenders", "A. McCallum"], "venue": "EMNLP, pages 262\u2013272", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic evaluation of topic coherence", "author": ["D. Newman", "J.H. Lau", "K. Grieser", "T. Baldwin"], "venue": "NAACL", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluating topic models for digital libraries", "author": ["D. Newman", "Y. Noh", "E.M. Talley", "S. Karimi", "T. Baldwin"], "venue": "JCDL, pages 215\u2013224", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "On the problem of the most efficient tests of statistical hypotheses", "author": ["J. Neyman", "E.S. Pearson"], "venue": "Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231:289\u2013337", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1933}, {"title": "Predicting friendship links in social networks using a topic modeling approach", "author": ["R. Parimi", "D. Caragea"], "venue": "PAKDD, pages 75\u201386", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The Annals of Statistics, 6(2):461\u2013464", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1978}, {"title": "Arnetminer: extraction and mining of academic social networks", "author": ["J. Tang", "J. Zhang", "L. Yao", "J. Li", "L. Zhang", "Z. Su"], "venue": "KDD, pages 990\u2013998", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Evaluation methods for topic models", "author": ["H.M. Wallach", "I. Murray", "R. Salakhutdinov", "D.M. Mimno"], "venue": "ICML, page 139", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative topic modeling for recommending scientific articles", "author": ["C. Wang", "D.M. Blei"], "venue": "KDD, pages 448\u2013456", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Online variational inference for the hierarchical dirichlet process", "author": ["C. Wang", "J.W. Paisley", "D.M. Blei"], "venue": "Journal of Machine Learning Research, 15:752\u2013760", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral relaxation for k-means clustering", "author": ["H. Zha", "X. He", "C.H.Q. Ding", "M. Gu", "H.D. Simon"], "venue": "NIPS\u201901, pages 1057\u20131064", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "Model-based feedback in the language modeling approach to information retrieval", "author": ["C. Zhai", "J.D. Lafferty"], "venue": "CIKM, pages 403\u2013410", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 13, "context": ", [14, 4]) are widely adopted to analyze text collections in various domains, such as the Web, scientific literature, social media, and digital humanities.", "startOffset": 2, "endOffset": 9}, {"referenceID": 3, "context": ", [14, 4]) are widely adopted to analyze text collections in various domains, such as the Web, scientific literature, social media, and digital humanities.", "startOffset": 2, "endOffset": 9}, {"referenceID": 3, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 205, "endOffset": 211}, {"referenceID": 8, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 205, "endOffset": 211}, {"referenceID": 16, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 232, "endOffset": 236}, {"referenceID": 17, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 254, "endOffset": 265}, {"referenceID": 5, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 254, "endOffset": 265}, {"referenceID": 25, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 254, "endOffset": 265}, {"referenceID": 30, "context": "Because of the principled mathematical foundation and effectiveness in exploratory data analysis, topic modeling has found its way into many classical and new data mining tasks, including topic extraction [4, 9], sentiment analysis [17], link prediction [18, 6, 26] and collaborative filtering [31].", "startOffset": 294, "endOffset": 298}, {"referenceID": 13, "context": "In practice, parametric models such as the probabilistic latent semantic analysis (PLSA) [14] and the latent Dirichlet allocation (LDA) [4] are still widely preferred.", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "In practice, parametric models such as the probabilistic latent semantic analysis (PLSA) [14] and the latent Dirichlet allocation (LDA) [4] are still widely preferred.", "startOffset": 136, "endOffset": 139}, {"referenceID": 24, "context": "\u201d Inspired by the likelihood ratio test [25], we can measure how well a document is described by the current topics using the likelihood ratio of it being generated by the existing topics versus being generated by all plus one more topic.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "Traditional clustering algorithms such as k-means [11] and spectral clustering [33], and statistical topic models, such as PLSA and LDA, all require users to specify the number of clusters or topics.", "startOffset": 50, "endOffset": 54}, {"referenceID": 32, "context": "Traditional clustering algorithms such as k-means [11] and spectral clustering [33], and statistical topic models, such as PLSA and LDA, all require users to specify the number of clusters or topics.", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "One way to choose this parameter is to run the same algorithm on the same data many times with different parameter values, and then choose the best model based on statistical model selection techniques such as the perplexity on held-out data, the Akaike information criterion (AIC) [1], or the Bayesian information criterion (BIC) [27].", "startOffset": 282, "endOffset": 285}, {"referenceID": 26, "context": "One way to choose this parameter is to run the same algorithm on the same data many times with different parameter values, and then choose the best model based on statistical model selection techniques such as the perplexity on held-out data, the Akaike information criterion (AIC) [1], or the Bayesian information criterion (BIC) [27].", "startOffset": 331, "endOffset": 335}, {"referenceID": 28, "context": "Another direction leads to Bayesian nonparametric models [29, 3, 8, 10], which have been attracting increasing attention in the machine learning community.", "startOffset": 57, "endOffset": 71}, {"referenceID": 2, "context": "Another direction leads to Bayesian nonparametric models [29, 3, 8, 10], which have been attracting increasing attention in the machine learning community.", "startOffset": 57, "endOffset": 71}, {"referenceID": 7, "context": "Another direction leads to Bayesian nonparametric models [29, 3, 8, 10], which have been attracting increasing attention in the machine learning community.", "startOffset": 57, "endOffset": 71}, {"referenceID": 9, "context": "Another direction leads to Bayesian nonparametric models [29, 3, 8, 10], which have been attracting increasing attention in the machine learning community.", "startOffset": 57, "endOffset": 71}, {"referenceID": 11, "context": "For clustering problems, the Dirichlet process mixture [12] model is widely used as the nonparametric prior of the mixture components.", "startOffset": 55, "endOffset": 59}, {"referenceID": 28, "context": "In [29], Teh et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [3], a hierarchical latent Dirichlet allocation (hLDA) model was proposed based on the Dirichlet process to infer topic hierarchies from the data.", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "In [16], Kulis and Jordan designed the DP-means algorithm, which is a scalable clustering algorithm built on top of k-means and", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Formally, a likelihood ratio test [25] can be used to compare the goodness of fit of two models, with the requirement that one is nested within the other (in our case, \u0398 is nested within \u0398\u2032).", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "This can be achieved by a \u201cfold-in\u201d process [14], which updates the topic distribution of the words in the document, i.", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "Such an objective function is in fact closely related to the Akaike information criterion (AIC) [1] in statistical model selection and also related to the objective in DP-means [16].", "startOffset": 96, "endOffset": 99}, {"referenceID": 15, "context": "Such an objective function is in fact closely related to the Akaike information criterion (AIC) [1] in statistical model selection and also related to the objective in DP-means [16].", "startOffset": 177, "endOffset": 181}, {"referenceID": 19, "context": "Diversity is widely studied as a criteria in ranking [20], query suggestion [19] and document summarization [5].", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "Diversity is widely studied as a criteria in ranking [20], query suggestion [19] and document summarization [5].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "Diversity is widely studied as a criteria in ranking [20], query suggestion [19] and document summarization [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 20, "context": "Some existing work has used diversity as an evaluation criteria for topic quality [21], but not for selecting the optimum number of topics (compare across different numbers of topics).", "startOffset": 82, "endOffset": 86}, {"referenceID": 6, "context": "In practice, one can instantiate the distance function with the L2 distance or the Jensen\u2212Shannon divergence [7] (the symmetric version of the Kullback\u2212Leibler divergence [7]).", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "In practice, one can instantiate the distance function with the L2 distance or the Jensen\u2212Shannon divergence [7] (the symmetric version of the Kullback\u2212Leibler divergence [7]).", "startOffset": 171, "endOffset": 174}, {"referenceID": 1, "context": "This heuristic is related to the process for sampling seedling points in the KMeans++ algorithm [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 33, "context": "While a query can be as short as a keyword, it is the common practice in information retrieval to estimate a robust \u03b8q though model-based pseudo feedback [34].", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "One of them measures the semantic coherence of the topics discovered [24, 23, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 22, "context": "One of them measures the semantic coherence of the topics discovered [24, 23, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 21, "context": "One of them measures the semantic coherence of the topics discovered [24, 23, 22].", "startOffset": 69, "endOffset": 81}, {"referenceID": 23, "context": "In [24], the point-wise mutual information is used.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "Specifically, for each document wj in the held-out dataset, we split the document into two parts wj = (wj1,wj2) and compute the predictive likelihood of the second part wj2 (20% of the words) conditioned on the first 80% of the words and the training data, the same way used in [30].", "startOffset": 278, "endOffset": 282}, {"referenceID": 27, "context": "The first dataset we use is the computer science bibliography DBLP from [28] .", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "As the length of tweets is too short for topic modeling analysis [15], we adopt the same strategy as [15] by treating each hashtag as a \u201cpseudo document\u201d and concatenate all the tweets containing this hashtag into the same document.", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "As the length of tweets is too short for topic modeling analysis [15], we adopt the same strategy as [15] by treating each hashtag as a \u201cpseudo document\u201d and concatenate all the tweets containing this hashtag into the same document.", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "In particular, it is possible to develop an online inference procedure of these models similar to the practice in [13] and [32], as well as distributed versions of the algorithms.", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": "In particular, it is possible to develop an online inference procedure of these models similar to the practice in [13] and [32], as well as distributed versions of the algorithms.", "startOffset": 123, "endOffset": 127}], "year": 2014, "abstractText": "It has always been a burden to the users of statistical topic models to predetermine the right number of topics, which is a key parameter of most topic models. Conventionally, automatic selection of this parameter is done through either statistical model selection (e.g., cross-validation, AIC, or BIC) or Bayesian nonparametric models (e.g., hierarchical Dirichlet process). These methods either rely on repeated runs of the inference algorithm to search through a large range of parameter values which does not suit the mining of big data, or replace this parameter with alternative parameters that are less intuitive and still hard to be determined. In this paper, we explore to \u201celiminate\u201d this parameter from a new perspective. We first present a nonparametric treatment of the PLSA model named nonparametric probabilistic latent semantic analysis (nPLSA). The inference procedure of nPLSA allows for the exploration and comparison of different numbers of topics within a single execution, yet remains as simple as that of PLSA. This is achieved by substituting the parameter of the number of topics with an alternative parameter that is the minimal goodness of fit of a document. We show that the new parameter can be further eliminated by two parameter-free treatments: either by monitoring the diversity among the discovered topics or by a weak supervision from users in the form of an exemplar topic. The parameter-free topic model finds the appropriate number of topics when the diversity among the discovered topics is maximized, or when the granularity of the discovered topics matches the exemplar topic. Experiments on both synthetic and real data prove that the parameterfree topic model extracts topics with a comparable quality comparing to classical topic models with \u201cmanual transmission.\u201d The quality of the topics outperforms those extracted through classical Bayesian nonparametric models. \u2217This study is done when the first author is visiting the University of Michigan. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$10.00.", "creator": "LaTeX with hyperref package"}}}