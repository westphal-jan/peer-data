{"id": "1409.3653", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2014", "title": "On Minimax Optimal Offline Policy Evaluation", "abstract": "this paper studies the off - policy risk evaluation problem, where one aims to estimate the value loss of a target policy based essentially on representing a sample of observations collected by another policy. we first consider the multi - armed bandit case, establish a minimax risk in lower bound, and analyze the risk of two standard estimators. it is shown, and verified in simulation, that one is minimax optimal up to minimum a constant, while another can be potentially arbitrarily worse, despite its empirical success and popularity. the results are traditionally applied to related problems in contextual avoidance bandits and fixed - horizon independent markov decision processes, and are also related to semi - supervised learning.", "histories": [["v1", "Fri, 12 Sep 2014 06:10:15 GMT  (521kb,D)", "http://arxiv.org/abs/1409.3653v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["lihong li", "remi munos", "csaba szepesvari"], "accepted": false, "id": "1409.3653"}, "pdf": {"name": "1409.3653.pdf", "metadata": {"source": "CRF", "title": "On Minimax Optimal Offline Policy Evaluation", "authors": ["Lihong Li", "Remi Munos"], "emails": ["lihongli@microsoft.com", "remi.munos@inria.fr", "szepesva@cs.ualberta.ca"], "sections": [{"heading": null, "text": "This paper studies the off-policy evaluation problem, where one aims to estimate the value of a target policy based on a sample of observations collected by another policy. We first consider the multi-armed bandit case, establish a minimax risk lower bound, and analyze the risk of two standard estimators. It is shown, and verified in simulation, that one is minimax optimal up to a constant, while another can be arbitrarily worse, despite its empirical success and popularity. The results are applied to related problems in contextual bandits and fixed-horizon Markov decision processes, and are also related to semi-supervised learning."}, {"heading": "1 Introduction", "text": "In reinforcement learning, one of the most fundamental problems is policy evaluation \u2014 estimate the average reward obtained by running a given policy to select actions in an unknown system. A straightforward solution is to simply run the policy and measure the rewards it collects. In many applications, however, running a new policy in the actual system can be expensive or even impossible. For example, flying a helicopter with a new policy can be risky as it may lead to crashes; deploying a new ad display policy on a website may be catastrophic to user experience; testing a new treatment on patients may simply be impossible for legal and ethical reasons; etc.\nThese difficulties make it critical to do off-policy policy evaluation (Precup et al., 2000, Sutton et al., 2010), which is sometimes referred to as offline evaluation in the bandit literature (Li et al., 2011) or counterfactual reasoning (Bottou et al., 2013). Here, we still aim to estimate the average reward of a target policy, but instead of being able to run the policy online, we only have access to a sample of observations made about the unknown system, which may be collected in the past using a different policy. Off-policy evaluation has been found useful in a number of important applications (Langford et al., 2008, Li et al., 2011, Bottou et al., 2013) and can also be looked as a key building block for policy optimization which, as in supervised learning, can often be reduced to evaluation, as long as the complexity of the policy class is well-controlled (Ng and Jordan, 2000). For example, it has played an important role in many optimization algorithms for Markov decision processes (e.g., Heidrich-Meisner and Igel 2009) and bandit problems (Auer et al., 2002, Langford and Zhang, 2008, Strehl et al., 2011). In the context of supervised learning, in the covariate shift literature, the problem of estimating losses under changing distributions is crucial for model selection (Sugiyama and Mu\u0308ller, 2005, Yu and Szepesva\u0301ri, 2012) and also appears in active learning (Dasgupta, 2011). In the statistical literature, on the other hand, the problem appears in the context of randomized experiments. Here, the focus is on the two-action (binary) case where the goal is to estimate the difference between the expected rewards of the two actions (Hirano et al., 2003), which is slightly (but not essentially) different than our setting.\nar X\niv :1\n40 9.\n36 53\nv1 [\ncs .A\nI] 1\n2 Se\nThe topic of the present paper is off-policy evaluation in finite settings, under a mean squared error criterion (MSE). As opposed to the statistics literature (Hirano et al., 2003), we are interested in results for finite sample sizes. In particular, we are interested in limits of performance (minimax MSE) given fixed policies, but unknown stochastic rewards with bounded mean reward, as well as the performance of estimation procedures compared to the minimax MSE. We argue that the finite setting is not a key limitation when focusing on the scaling behavior of the MSE of algorithms. Moreover, we are not aware of prior work that would have studied the above problem (i.e., relating the MSE of algorithms to the best possible MSE). Our main results are as follows: We start with a lower bound on the minimax MSE, to set a target for the estimation procedures. Next, we derive the exact MSE of the likelihood ratio (or importance-weighted) estimator (LR), which is shown to have an extra (uncontrollable) factor as compared to the minimax MSE lower bound. Next, we consider the estimator which estimates the mean rewards by sample means, which we call the regression estimator (REG). The motivation of studying this estimator is both its simplicity and also because it is known that a related estimator is asymptotically efficient (Hirano et al., 2003). The main question is whether the asymptotic efficiency transfers into finite-time efficiency. Our answer to this is mixed: We show that the MSE of REG is within a constant factor of the minimax MSE lower bound, however, the \u201cconstant\u201d depends on the number of actions (K), or a lower bound on the variance. We also show that the dependence of the MSE of REG on the number actions is unavoidable. In any case, for \u201csmall\u201d action sets or high noise setting, the REG estimator can be thought of as a minimax near-optimal estimator. We also show that for small sample sizes (up to\u221a K) all estimators must suffer a constant MSE. Numerical experiments illustrate the tightness of the analysis. Implications for more complicated settings, such as policy evaluation in contextual bandits and Markov Decision Processes (MDPs). The question of designing a nearly minimax estimator independently of any problem parameters remains open. All the proofs ot given in the main text can be found in the supplementary material."}, {"heading": "2 Multi-armed Bandit", "text": "Let A = {1, 2, . . . ,K} be a finite set of K actions. Data Dn = {(Ai, Ri)}1\u2264i\u2264n is generated by the following process: 1 (Ai, Ri) are independent copies of (A,R), where P (A = a) = \u03c0D(a) and R \u223c \u03a6(\u00b7|A) for some unknown family of distributions {\u03a6(\u00b7|a)}a\u2208A and known policy \u03c0D. We are also given a known target policy \u03c0 and want to estimate its value, v\u03c0\u03a6 := EA\u223c\u03c0,R\u223c\u03a6(\u00b7|A)[R] based on the knowledge of Dn, \u03c0D and \u03c0, where the quality of an estimate v\u0302 constructed based on Dn (and \u03c0, \u03c0D) is measured by its mean-squared error, MSE (v\u0302) := E [ (v\u0302 \u2212 v\u03c0\u03a6)2 ] .\nDefine r\u03a6(a) := E[R|A = a] and \u03c32\u03a6(a) := V(R|A = a), where V(\u00b7) stands for the variance. Further, let \u03c0\u2217D := mina \u03c0D(a). For convenience, we will identify any function f : A \u2192 R with the K-dimensional vector whose kth component is f(k). Thus, r\u03a6, \u03c32\u03a6, etc. will also be looked at as vectors. Note that we do not assume that the rewards are bounded from either direction.\nA few quantities are introduced to facilitate discussions that follow: V1 := E [ V ( \u03c0(A)\n\u03c0D(A) R|A )] = \u2211 a \u03c02(a) \u03c0D(a) \u03c32\u03a6(a) ,\nV2 := V ( E [ \u03c0(A)\n\u03c0D(A) R|A\n]) = V ( \u03c0(A)\n\u03c0D(A) r\u03a6(A) ) = \u2211 a \u03c02(a) \u03c0D(a) r\u03a6(a) 2 \u2212 (v\u03c0\u03a6)2 .\nNote that V1 and V2 are functions of \u03a6, \u03c0D and \u03c0, but this dependence is suppressed. Also, V1 and V2 are independent in that there are no constants c, C > 0 such that cV1 \u2264 V2 \u2264 CV1 for any \u03c0, \u03c0D,\u03a6. Finally, let pa,n := (1\u2212 \u03c0D(a))n be the probability of having no sample of a in Dn."}, {"heading": "2.1 A Minimax Lower Bound", "text": "We start with establishing a minimax lower bound that characterizes the inherent hardness of the offpolicy evaluation problem. An estimator A can be considered as a function that maps (\u03c0, \u03c0D, Dn) to an estimate of v\u03c0\u03a6, denoted v\u0302A(\u03c0, \u03c0D, D n). Fix \u03c32 := (\u03c32(a))a\u2208A. We consider the minimax\n1The data Dn is actually a list, not a set. We keep the notation {(Ai, Ri)}1\u2264i\u2264n for historical reasons.\noptimal risk subject to \u03c32\u03a6(a) \u2264 \u03c32(a) and 0 \u2264 r\u03a6(a) \u2264 Rmax for all a \u2208 A:\nR\u2217n(\u03c0, \u03c0D, Rmax, \u03c3 2) := inf\nA sup \u03a6:\u03c32\u03a6\u2264\u03c32,0\u2264r\u03a6\u2264Rmax E [ (v\u0302A(\u03c0, \u03c0D, D n)\u2212 v\u03c0\u03a6)2 ] ,\nwhere for vectors x, y \u2208 RK , x \u2264 y holds if and only if xi \u2264 yi for 1 \u2264 i \u2264 K. For B \u2282 A, we let pB,n denote the probability that none of the actions in the data Dn falls into B: pB,n = P (A1, . . . , An 6\u2208 B). Note that this definition generalizes pa,n. We also let \u03c0(B) = \u2211 a\u2208B \u03c0(a).\nTheorem 1. For any n > 0, \u03c0D, \u03c0, Rmax and \u03c32, one has\nR\u2217n(\u03c0, \u03c0D, Rmax, \u03c3 2) \u2265 1\n4 max\n( R2max max\nB\u2282A \u03c02(B)pB,n, V1 n\n) .\nFurthermore,\nlim inf n\u2192\u221e\nR\u2217n(\u03c0, \u03c0D, Rmax, \u03c3 2)\nV1/n \u2265 1. (1)\nProof. To prove the first part of the lower bound, fix a subset B \u2282 A of actions and choose an environment \u03a6 \u2208 E , where E is the set of environments \u03a6 such that \u03c32\u03a6 \u2264 \u03c32 and 0 \u2264 r\u03a6 \u2264 Rmax. Introduce the notation E\u03a6 to denote expectation when the data is generated by environment \u03a6.\nLet Dn be the data generated based on \u03c0D and \u03a6 and let v\u0302A(Dn) denote the estimate produced by some algorithm A. Define S = {A1, . . . , An} to be the set of actions in the dataset that is seen by the algorithm. Clearly, for any \u03a6,\u03a6\u2032 such that they agree on the complement of B (but may differ on actions in B),\nE\u03a6[v\u0302A(Dn)|S \u2229B = \u2205] = E\u03a6\u2032 [v\u0302A(Dn)|S \u2229B = \u2205] . (2)\nNow, MSE\u03a6 (v\u0302A) := E\u03a6[(v\u0302A(Dn)\u2212 v\u03c0\u03a6)2] \u2265 E\u03a6[(v\u0302A(Dn)\u2212 v\u03c0\u03a6)2|S \u2229B = \u2205]P (S \u2229B = \u2205) and by adapting the argument that the MSE is lower bounded by the bias squared, E\u03a6[(v\u0302A(Dn) \u2212 v\u03c0\u03a6)\n2|S \u2229 B = \u2205] \u2265 (E\u03a6[v\u0302A(Dn)|S \u2229 B = \u2205] \u2212 v\u03c0\u03a6)2. Hence, MSE\u03a6 (v\u0302A) \u2265 P (S \u2229B = \u2205) sup\u03a6\u2208E(E\u03a6[v\u0302A(Dn)|S \u2229 B = \u2205] \u2212 v\u03c0\u03a6)2. We get an even smaller quantity if we further restrict the environments \u03a6 to environments E0 that also satisfy r\u03a6 = \u03c32\u03a6 = 0 on A \\ B. Now, by (2), for all these environments, E\u03a6[v\u0302A(Dn)|S\u2229B = \u2205] takes on a common value, denote it by vA. Hence, MSE\u03a6 (v\u0302A) \u2265 P (S \u2229B = \u2205) sup\u03a6\u2208E0(vA \u2212 v \u03c0 \u03a6) 2. Since v\u03c0\u03a6 = \u2211 a\u2208B \u03c0(a)r\u03a6(a), sup\u03a6\u2208E0(vA \u2212 v \u03c0 \u03a6) 2 \u2265 R 2 max 4 \u03c0 2(B), where we use the shorthand \u03c0(B) = \u2211 a\u2208B \u03c0(a). Plugging this into the previous inequality we get sup\u03a6\u2208E MSE\u03a6 (v\u0302A) \u2265 P (S \u2229B = \u2205) R2max 4 \u03c0 2(B). Since A was arbitrary, we get R\u2217n(\u03c0, \u03c0D, Rmax, \u03c3 2) \u2265 P (S \u2229B = \u2205) R 2 max 4 \u03c0 2(B).\nFor the second part, consider a class of normal distributions with fixed reward variances \u03c32 but different reward expectations: Fp = {\u03a60, . . . ,\u03a6p\u22121}, where r\u03a6i = 2i \u221a \u03b5\u2206 \u2208 RK , for some to-be-\nspecified vector \u2206 \u2208 RK+ that satisfies \u2211 a \u03c0(a)\u2206(a) = 1. The data-generating distribution \u03a6 is in Fp, but is unknown otherwise. It is easy to see that the policy value between any two distributions in Fp differ by at least 2 \u221a \u03b5. Indeed, for any \u03a6i,\u03a6j \u2208 Fp, |v\u03c0\u03a6i \u2212 v \u03c0 \u03a6j | = 2 \u221a \u03b5|i \u2212 j| \u2211 a \u03c0(a)\u2206(a) = 2 \u221a \u03b5|i \u2212 j| \u2265 2 \u221a \u03b5. It follows that, in order to achieve a squared error less than \u03b5, one needs to identify the underlying data-generating \u03a6 from Fp, based on the observed sample Dn. The problem now reduces to finding a minimax lower bound for hypothesis testing in the given finite set Fp. We resort to the information-theoretic machinery based on Fano\u2019s inequality (see, e.g., Raginsky and Rakhlin (2011)). Define an oracle which, when queried, outputs Y = (A,R) with A \u223c \u03c0D(\u00b7) and R \u223c \u03a6(\u00b7|A). Let the distribution of Y when \u03a6 is used be denoted by PY |\u03a6. Let Fp collect p distributions such that \u03a6(\u00b7|a) is normal. Consider \u03a6,\u03a6\u2032 \u2208 Fp. Then,\nD(PY |\u03a6\u2016PY |\u03a6\u2032) = \u2211 a \u03c0D(a)D(\u03a6(\u00b7|a)\u2016\u03a6\u2032(\u00b7|a)) = 2\u03b5(i\u2212 j)2 \u2211 a \u03c0D(a)\u2206(a) 2 \u03c3(a)2 .\nThe divergence measures how much information is carried in one sample from the oracle to tell \u03a6 from \u03a6\u2032. To obtain the tightest lower bound, we should minimize the divergence. Subject to the\nconstraint \u2211 a \u03c0(a)\u2206(a) = 1, the divergence is minimized by setting \u2206(a) \u221d \u03c0(a) \u03c0D(a)\n\u03c32(a), and is 2\u03b5(i\u2212 j)2/V1. Now setting p = 6, and applying Lemma 1, Theorem 1 and the \u201cInformation Radius bound\u201d from Raginsky and Rakhlin (2011), we have n \u2265 V14\u03b5 . Reorganizing terms and combining with the first term complete the proof of the first statement.\nFor the second part, note that it suffices to consider asymptotically unbiased estimators (cf. the generalized Cramer-Rao lower bound, Theorem 7.3 of Ibragimov and Has\u2019minskii 1981). For any such estimator, the Cramer-Rao lower bound gives the result with the parametric family chosen to be p(a, y; \u03b8) = \u03c0D(a)\u03d5(y; r(a), \u03c32(a)), where \u03b8 = (r(a))a\u2208A is the unknown parameter to be estimated, and \u03d5(\u00b7;\u00b5, \u03c32) is the density of the normal distribution with mean \u00b5 and variance \u03c32 and the quantity to be estimated is \u03c8(\u03b8) = \u2211 a \u03c0(a)r(a). For details, see Appendix A.1.\nThe next corollary says that the minimax risk is constant when the number of samples is O( \u221a K): Corollary 1. For K \u2265 2, n \u2264 \u221a K, sup\u03c0 R \u2217 n(\u03c0, \u03c0D, Rmax, \u03c3 2) = \u2126(R2max).\nProof. Choose B \u2282 A to minimize \u03c0D(B) subject to the constraint |B| = b \u221a Kc. Note that P (A1, . . . , An 6\u2208 B) = (1 \u2212 \u03c0D(B))n \u2265 (1 \u2212 |B|/K)n \u2265 (1 \u2212 1/ \u221a K) \u221a K \u2265 (1 \u2212 1/ \u221a 2) \u221a\n2. Choosing \u03c0 such that \u03c0(B) = 1 gives the result.\nWe conjecture that the result can be strengthened by increasing the upper limit on n."}, {"heading": "2.2 Likelihood Ratio Estimator", "text": "One of the most popular estimators is known as the propensity score estimator in the statistical literature (Rosenbaum and Rubin, 1983, 1985), or the importance weighting estimator (Bottou et al., 2013). We call it the likelihood ratio estimator, as it estimates the unknown value using likelihood ratios, or importance weights:\nv\u0302LR(\u03c0, \u03c0D, D n) :=\n1\nn n\u2211 i=1 \u03c0(Ai) \u03c0D(Ai) Ri.\nIts distinguishing feature is that it is unbiased: E[v\u0302LR(\u03c0, \u03c0D, Dn)] = v\u03c0\u03a6, implying that the MSE is purely contributed by the variance of the estimator. The main result in this subsection shows that this estimator does not achieve the minimax lower bound up to any constant (by making V2 V1). The proof (given in the appendix) is based on a direct calculation using the law of total variance. Proposition 1. It holds that MSE (v\u0302LR(\u03c0, \u03c0D, Dn)) = (V1 + V2)/n .\nWe see that as compared to the lower bound on the minimax MSE, an extra V2/n factor appears. In the next section, we will see that this factor is superfluous, showing that the MSE of LR can be \u201cunreasonably large\u201d."}, {"heading": "2.3 Regression Estimator", "text": "For convenience, define n(a) := \u2211n i=1 I(Ai = a) to be the number of samples for action a in Dn,\nand R(a) := \u2211n i=1 I(Ai = a)Ri the total rewards of a. The regression estimator (REG) is given by\nv\u0302Reg(\u03c0,D n) := \u2211 a \u03c0(a)r\u0302(a), where r\u0302(a) := { 0, if n(a) = 0; R(a) n(a) , otherwise .\nFor brevity, we will also write r\u0302(a) = I{n(a) > 0}R(a)n(a) , where we take 0 0 to be zero. The name of the estimator comes from the fact that it estimates the reward function, and the problem of estimating the reward function can be thought of as a regression problem.\nInterestingly, as can be verified by direct calculation, the REG estimator can also be written as\nv\u0302Reg(\u03c0,D n) =\n1\nn n\u2211 i=1 \u03c0(Ai) \u03c0\u0302D(Ai) Ri , (3)\nwhere \u03c0\u0302D(a) = n(a) n is the empirical estimate of \u03c0D(a). Hence, the main difference between LR and REG is that the former uses \u03c0D to reweight the data, while the latter uses the empirical estimates \u03c0\u0302D. It may appear that LR is superior since it uses the \u201cright\u201d quantity. Surprisingly, REG turns out to be much more robust than LR, as will be shown shortly; further discussion is made in Section D.\nFor the next statement, the counterpart of Proposition 1, the following quantities will be useful:\nV0,n := (\u2211 a \u03c0(a)r\u03a6(a)pa,n )2 + \u2211 a \u03c02(a)r2\u03a6(a) pa,n(1\u2212 pa,n) and\nV3,n := \u2211 a E [ I{n(a) > 0} \u03c0\u0302D(a) \u2212 1 \u03c0D(a) ] \u03c0(a)2\u03c32(a) .\nProposition 2. Fix \u03c0, \u03c0D. Assume that r\u03a6 is nonnegative valued. Then it holds that MSE (v\u0302Reg(\u03c0,D\nn)) \u2264 V0,n + (V1 + V3,n)/n. Further, for any \u03a6 such that the rewards have normal distributions, defining bn = \u2211 a \u03c0(a)r\u03a6(a)pa,n to be the bias of v\u0302Reg, MSE (v\u0302Reg) \u2265 V1 n + 4b 2 n ( 1 + V1n ) + 2n \u2211 a \u03c02(a) \u03c0D(a) \u03c32\u03a6(a)pa,n.\nProof sketch. For the upper bound use that the MSE equals the sum of squared bias and the variance. It can be verified that REG is slightly biased: E[v\u0302Reg] \u2212 v\u03c0\u03a6 = \u2211 a \u03c0(a)r\u03a6(a)pa,n. For the variance term, we use the law of total variance to yield: V(v\u0302Reg) = E[V(v\u0302Reg|n(1), . . . , n(K))] + V(E[v\u0302Reg|n(1), . . . , n(K)]), where the first term is \u2211 a \u03c0\n2(a)\u03c32(a)E[I{n(a) > 0}/n(a)], and the second term is upper bounded (Lemma 2) by \u2211 a \u03c0\n2(a)r2\u03a6(a) pa,n(1 \u2212 pa,n). The proof is then completed by adding squared bias to variance, and using definitions of V0,n, V1, and V3. The lower bound follows from the (generalized) Cramer-Rao inequality.\nThe main result of this section is the following theorem that characterizes the MSE of REG in terms of the minimax optimal MSE. Theorem 2 (Minimax Optimality of the Regression Estimator). The following hold:\n(i) For any \u03c0, \u03c0D, \u03c32 = (\u03c32(a))a\u2208A, \u03a6 such that mina r\u03a6(a) \u2265 0, maxa r\u03a6(a) \u2264 Rmax, and \u03c32\u03a6 \u2264 \u03c32, it holds for any n > 0 that\nMSE (v\u0302Reg(\u03c0,Dn)) \u2264 K {\nmin(4K,max a\nr2\u03a6(a) \u03c32\u03a6(a) ) + 5\n} R\u2217n(\u03c0, \u03c0D, Rmax, \u03c3 2) , (4)\nwhere Dn = {(Ai, Ri)}i=1,...,n is an i.i.d. sample from (\u03c0D,\u03a6).\n(ii) A suboptimality factor of \u2126(K) in the above result is unavoidable: For K > 2, there exists (\u03c0, \u03c0D) such that for any n \u2265 1,\nMSE (v\u0302Reg(\u03c0,Dn)) R\u2217n(\u03c0, \u03c0D, Rmax, 0) \u2265 ne\u22122n/(K\u22121) .\nThus for n = (K \u2212 1)/2, this ratio is at least K\u221212e .\n(iii) The estimator v\u0302Reg is asymptotically minimax optimal:\nlim sup n\u2192\u221e\nMSE (v\u0302Reg(\u03c0,Dn))\nR\u2217n(\u03c0, \u03c0D, Rmax, \u03c3 2) \u2264 1 .\nWe need the following lemma, which may be of interest on its own: Lemma 1. Let X1, . . . , Xn be n independent Bernoulli random variables with parameter p > 0. Letting Sn = \u2211n i=1Xi, p\u0302 = Sn/n, Z = I{Sn>0} p\u0302 \u2212 1 p , we have for any n and p that E [Z] \u2264 4/p.\nFurther, when np \u2265 34, we have E [Z] \u2264 2p \u221a 2 np (\u221a 3 2 ln ( np 2 ) + 1 ) . Proof of Theorem 2. First, we bound V3,n in terms of V1. From Lemma 1, E [ I{n(a)>0} \u03c0\u0302D(a) \u2212 1\u03c0D(a) ] \u2264\n4 \u03c0D(a) , while if n\u03c0\u2217D \u2265 34, E [ I{n(a)>0} \u03c0\u0302D(a) \u2212 1\u03c0D(a) ] \u2264 2\u03c0D(a) \u221a 2 n\u03c0D(a) (\u221a 3 2 ln ( n\u03c0D(a) 2 ) + 1 ) .\nPlugging these into the definition of V3,n, we have V3,n \u2264 4V1 for all n. Furthermore, when n\u03c0\u2217D \u2265 34, thanks to monotonicity of the function t 7\u2192 \u221a 2 t (\u221a 3 2 ln t+ 1 ) for t > 1, we have\nV3,n \u2264 2V1 \u221a 2\nn\u03c0D\u2217\n(\u221a 3\n2 ln\n( n\u03c0\u2217D\n2\n) + 1 ) . (5)\nNow, to bound V0,n = ( \u2211 a \u03c0(a)r\u03a6(a)pa,n) 2 + \u2211 a \u03c0\n2(a)r2\u03a6(a) pa,n(1\u2212 pa,n), remember that one lower bound for R\u2217n is R 2 max maxa \u03c0 2(a)pa,n/4, where Rmax is the range for r\u03a6. Hence,\nV0,n = K 2\n( 1\nK \u2211 a \u03c0(a)r\u03a6(a)pa,n )2 + \u2211 a \u03c02(a)r2\u03a6(a) pa,n(1\u2212 pa,n)\n\u2264 K \u2211 a \u03c02(a)r2\u03a6(a)p 2 a,n + \u2211 a \u03c02(a)r2\u03a6(a)pa,n(1\u2212 pa,n)\n\u2264 K \u2211 a \u03c02(a)r2\u03a6(a)pa,n \u2264 K2 max a \u03c02(a)r2\u03a6(a)pa,n . (6)\nHence, using R\u2217n \u2265 V1/n,\nMSE (v\u0302Reg) \u2264 V0,n + V1+V3n \u2264 4K 2 max a \u03c02(a)r2\u03a6(a)pa,n + 5 V1 n \u2264 (4K 2 + 5)R\u2217n . (7)\nOn the other hand, assuming that mina \u03c32(a) > 0, we also have V0,n \u2264 K \u2211 a \u03c02(a)r2\u03a6(a)pa,n \u2264 K max b\u2208A ( r2\u03a6(b) \u03c32(b) ) \u2211 a pa,n\u03c0 2(a)\u03c32(a) \u2264 K max b\u2208A ( r2\u03a6(b) \u03c32(b) ) V1 n ,\nwhere in the last inequality we used that pa,n \u2264 e\u2212n\u03c0D(a) and e\u2212x \u2264 1/x, which is true for any x > 0, and finally also the definition of V1. Similarly to the previous case, we get\nMSE (v\u0302Reg) \u2264 { K max\nb\u2208A\n( r2\u03a6(b) \u03c32(b) ) + 5 } V1 n \u2264 { K max b\u2208A ( r2\u03a6(b) \u03c32(b) + 5 )} R\u2217n .\nCombining this with (7) gives (4).\nFor the second part of the result, choose \u03c0(a) = \u03c0D(a) = 1/K, r\u03a6(a) = 1. For K \u2265 2, pa,n = (1 \u2212 1/K)n = e\u2212n log(1/(1\u22121/K)) = e\u2212n log(1+1/(K\u22121)) \u2265 e\u2212n/(K\u22121). Hence, we have MSE (v\u0302Reg) \u2265 (E [v\u0302Reg \u2212 v\u03c0\u03a6])2 = ( \u2211 a \u03c0(a)r\u03a6(a)pa,n)\n2 \u2265 e\u22122n/(K\u22121). Now, consider the LR estimator. Choosing \u03c32 = 0, we have V1 = 0 and so by Proposition 1,\nsup \u03a6:0\u2264r\u03a6\u22641,\u03c32\u03a6=0 MSE (v\u0302LR) = sup \u03a6:0\u2264r\u03a6\u22641,\u03c32\u03a6=0\nV2/n \u2264 1\nn .\nHence, MSE(v\u0302Reg)R\u2217n(\u03c0,\u03c0D,1,0) \u2265 e\u22122n/(K\u22121)\nsup \u03a6:0\u2264r\u03a6\u22641,\u03c32\u03a6=0\nMSE(v\u0302LR) \u2265 ne\u22122n/(K\u22121).\nFinally, the for the last part, fix any \u03c0, \u03c0D, \u03c32, \u03a6 such that \u03c32\u03a6 \u2264 \u03c32. Then, for n large enough,\nMSE (v\u0302Reg) \u2264 V0,n + V1+V3n \u2264 Ce \u2212n/C + V1n\n( 1 + C \u221a lnn n ) , where C > 0 is a problem de-\npendent constant, and the second inequality used (5) and (6). Combining this with (1) of Theorem 1 gives the desired result."}, {"heading": "2.4 Simulation Results", "text": "This subsection corroborates our analysis with simulation results that empirically demonstrate the impact of key quantities on the MSE of the two estimators. Two sets of experiments are done, corresponding to the left and right panels in Figure 1. In all experiments, we repeat the data-generation process (with \u03c0D) 10,000 times, and compute the MSE of each estimator. All reward distributions are normal distributions with \u03c32 = 0.01 and different means. We then plot normalized MSE (MSE multiplied by sample size n), or nMSE, against n.\nThe first experiment is to compare the finite-time as well as asymptotic accuracy of v\u0302LR and v\u0302Reg. We choose K = 10, r\u03a6(a) = a/K, \u03c0(a) \u221d a. Three choices of \u03c0D are used: (a) \u03c0D(a) \u221d a, (b) \u03c0D(a) = 1/K, and (c) \u03c0D(a) \u221d (K \u2212 a). These choices lead to increasing values of V2 (with V1 approximately fixed). Clearly, the nMSE of v\u0302LR remains constant, equal to V1 + V2, as predicted in Proposition 1. In contrast, the nMSE of v\u0302Reg is large when n is small, because of the high bias, and then quickly converges to the asymptotic minimax rate V1 (Theorem 2, part iii). As V2 can be arbitrarily larger than V1, it follows that v\u0302Reg is preferred over v\u0302LR, as least for sufficiently large n that is needed to drive the bias down. It should be noted that in practice, after Dn is generated, it is easy to quantify the bias of v\u0302Reg simply by identifying the set of actions a with n(a) = 0.\nThe second experiment is to show how K affects the nMSE of v\u0302Reg. Here, we choose \u03c0D = 1/K, r\u03a6(a) = a/K, \u03c0(a) \u221d a, and vary K \u2208 {50, 100, 200, 500, 1000}. As Figure 1 (right) shows, a larger K gives v\u0302Reg a harder time, which is consistent with Theorem 2 (part i). Not only does the maximum nMSE grow approximately linearly with K, the number of samples needed for nMSE to start decreasing also scales roughly as (K \u2212 1)/2, as indicated by part ii of Theorem 2."}, {"heading": "3 Extensions", "text": "In this section, we consider extensions of our previous results to contextual bandits and Markovian Decision Processes, while implications to semi-supervised learning (Zhu and Goldberg, 2009) are discussed in the supplementary material."}, {"heading": "3.1 Contextual Bandits", "text": "The problem setup is as follows: In addition to the finite action set A = {1, 2, . . . ,K}, we are also given a context set X = {1, 2, . . . ,M}. A policy now is a map \u03c0 : X \u2192 [0, 1]A such that for any x \u2208 X , \u03c0(x) is a probability distribution over the action space A. For notational convenience, we will use \u03c0(a|x) instead of \u03c0(x)(a). The set of policies over X and A will be denoted by \u03a0(X ,A). The process generating the data Dn = {(Xi, Ai, Ri)}1\u2264i\u2264n is described by the following: (Xi, Ai, Ri) are independent copies of (X,A,R), where X \u223c \u00b5(\u00b7), A \u223c \u03c0D(\u00b7|X) and R \u223c \u03a6(\u00b7|A,X) for some unknown family of distributions {\u03a6(\u00b7|a, x)}a\u2208A,x\u2208X and known policy \u03c0D \u2208 \u03a0(X ,A) and context distribution \u00b5. For simplicity, we fix Rmax = 1. We are also given a known target policy \u03c0 \u2208 \u03a0(X ,A) and want to estimate its value, v\u03c0,\u00b5\u03a6 := EX\u223c\u00b5,A\u223c\u03c0(\u00b7|X),R\u223c\u03a6(\u00b7|A,X)[R] based on the knowledge of Dn, \u03c0D, \u00b5 and \u03c0, where the quality of an estimate v\u0302 constructed based on Dn (and \u03c0, \u03c0D, \u00b5) is measured by its mean squared error, MSE (v\u0302) := E [ (v\u0302 \u2212 v\u03c0,\u00b5\u03a6 )2 ] , just like in the case of contextless bandits. Let \u03c32\u03a6(x, a) = V(R) for R \u223c \u03a6(\u00b7|x, a), x \u2208 X , a \u2208 A. An estimator A can be considered as a function that maps (\u00b5, \u03c0, \u03c0D, D n) to an estimate of v\u03c0,\u00b5\u03a6 , denoted v\u0302A(\u00b5, \u03c0, \u03c0D, D n). Fix \u03c32 := (\u03c32(x, a))x\u2208X ,a\u2208A. The minimax optimal risk subject to \u03c32\u03a6(x, a) \u2264 \u03c32(x, a) for all x \u2208 X , a \u2208 A is defined by R\u2217n(\u00b5, \u03c0, \u03c0D, \u03c3 2) := infA sup\u03a6:\u03c32\u03a6\u2264\u03c32 E [ (v\u0302A(\u00b5, \u03c0, \u03c0D, D n)\u2212 v\u03c0,\u00b5\u03a6 )2 ] .\nThe main observation is that the estimation problem for the contextual case can actually be reduced to the contextless bandit case by treating the context-action pairs as \u201cactions\u201d belonging to the product space X \u00d7 A. For any policy \u03c0, by slightly abusing notation, let (\u00b5 \u2297 \u03c0)(x, a) = \u00b5(x)\u03c0(a|x) be the joint distribution of (X,A) when X \u223c \u00b5(\u00b7), A \u223c \u03c0(\u00b7|X). This way, we can map any contextual policy evaluation problem defined by \u00b5,\u03c0D, \u03c0, \u03a6 and a sample size n into a contextless policy evaluation problem defined by \u00b5\u2297 \u03c0D, \u00b5\u2297 \u03c0, \u03a6 with action set X \u00d7A. Therefore, with V1 and V2 defined similarly, one can conclude the following results: Theorem 3. Pick any n > 0, \u00b5, \u03c0D, \u03c0 and \u03c32. Then, one has R\u2217n(\u00b5, \u03c0, \u03c0D, \u03c32) = \u2126 ( maxB\u2282X\u00d7A{ \u2211 (x,a)\u2208B \u00b5(x)\u03c0(a|x)}2{1\u2212 \u2211 (x,a)\u2208B \u00b5(x)\u03c0d(a|x)}n + V1/n ) , MSE (v\u0302LR) = (V1 + V2)/n, and MSE (v\u0302Reg) \u2264 CR\u2217n(\u00b5, \u03c0, \u03c0D, \u03c32), for C = MK{min(4MK,maxx,a r2\u03a6(a)/\u03c32\u03a6(a)) + 5}R\u2217n(\u00b5, \u03c0, \u03c0D, \u03c32). Furthermore, the MSE of the regression estimator approaches the minimax risk as sample size grows to infinity."}, {"heading": "3.2 Markov Decision Processes", "text": "Similarly, results in Section 2 can be naturally extended to fixed-horizon, finite Markov decision processes (MDPs). Here, an MDP is described by a tuple M = \u3008X ,A, P,\u03a6, \u03bd,H\u3009, where X = {1, . . . , N} is the set of states, A = {1, . . . ,K} the set of actions, P the transition kernel, \u03a6 : X \u00d7 A 7\u2192 R the reward function, \u03bd the start-state distribution, and H the horizon. A policy \u03c0 : X 7\u2192 [0, 1]K maps states to distributions over actions, and we use \u03c0(a|x) to denote the probability of choosing action a in state x. Given a policy \u03c0 \u2208 \u03a0(X ,A), a trajectory of length H , denoted T = (X,A,R) (for X \u2208 XH , A \u2208 AH , and R \u2208 RH ), is generated as follows: X(1) \u2208 \u03bd(\u00b7); for h \u2208 {1, . . . ,H}, A(h) \u223c \u03c0(\u00b7|X(h)), R(h) \u223c \u03a6(\u00b7|X(h), A(h)), and X(h + 1) \u223c P (\u00b7|X(h), A(h)). The policy value is defined by v\u03c0\u03a6 := ET [ \u2211H h=1R(h)]. For simplicity, we again assume Rmax = 1. The off-policy evaluation problem is to estimate v\u03c0\u03a6 from data D n = {Tt}1\u2264t\u2264n, where each trajectory Tt is independently generated by an exploration policy \u03c0D \u2208 \u03a0(X ,A). Here, we assume the reward distribution \u03a6 is unknown; other quantities including \u03bd, P , H , \u03c0, and \u03c0D are all known. Again, we measure the quality of an estimate v\u0302 by its mean squared error: MSE (v\u0302) := [ (v\u0302 \u2212 v\u03c0\u03a6)2 ] . By considering a length-H trajectory of state-actions as an \u201caction,\u201d, one can apply the results as in the previous subsection to conclude the following: Theorem 4. Pick any n > 0, \u03bd, \u03c0D, \u03c0, P , H , and \u03c32. Then, one has R\u2217n(\u03bd, \u03c0, \u03c0D, P,H, \u03c3 2) = \u2126 ( maxB\u2282T { \u2211 (x,a)\u2208T \u00b5(x, a)}2{1\u2212 \u2211 (x,a)\u2208T \u00b5D(\u03c4)}n + V1/n ) , MSE (v\u0302LR) = (V1 + V2)/n, and MSE (v\u0302Reg) \u2264 CR\u2217n(\u03bd, \u03c0, \u03c0D, P,H, \u03c32) for C = NH+1KH{min(4NH+1KH ,max(x,a)\u2208T r2\u03a6(x,a)\n\u03c32\u03a6(x,a) ) + 5}. Moreover, there are cases where such\nan exponential dependence is unavoidable. Finally, the MSE of the regression estimator approaches the minimax risk as sample size n grows to infinity."}, {"heading": "4 Conclusions", "text": "We have studied the fundamental problem of finite off-policy evaluation. Despite its importance, it appears that ours are the first results for the finite-sample setting. While the simplest estimator which uses importance weights (called LR) was found to be sensitive to the magnitude of importance weights, the regression estimator (REG), which estimates the mean rewards for each actions, was found to be less exposed to this value. While the sensitivity of LR is a \u201cfolk theorem\u201d, we have not seen this result formally proven in the literature. We also found that the REG estimator has different qualities: It is minimax optimal up to a constant, which is the minimum of the squared number of actions, K2, and the maximal inverse reward variance. We showed that the dependence on the number of actions cannot in general be removed. There is still a gap of factor of K between our lower and upper bounds. We conjecture that the lower bound shows the correct order (which seems to be confirmed by the experiments). While it is not hard to design estimators that combine LR and REG, we did not find these attractive as they cannot be shown to be near-optimal in the above sense. Hence, it remains open to design an estimator which is minimax optimal up to a universal constant factor. One starting point is to investigate the many alternate estimators proposed in the literature (e.g., LR with clipped weights, or dividing by the sum of weights instead of dividing by n). While in the paper we focused on the simplest contextless, finite setting, we showed that our\nresults have implications to other, more contextual settings. However, we have only scratched the surface here: Much more work is needed, however, to provide a fuller analysis of sample based off-policy evaluation in these settings."}, {"heading": "A Technical Details", "text": "The appendix collects miscellaneous results that are needed in the main body of the text.\nA.1 Proof of the Second Part of Theorem 1\nWe provide here a full proof of the second part of Theorem 1. First, we need some background. Let X = (X ,A) be a measurable space, \u0398 \u2282 RK open, p \u2261 p(\u00b7; \u03b8)\u03b8\u2208\u0398 be a family of densities with respect to \u03bd, a \u03c3-finite measure on X such that p(\u00b7; \u03b8) is defined on the closure \u0398\u0304 of \u0398 and p is measurable on the product \u03c3-algebra ofX \u00d7\u0398 where \u0398 is equipped with the \u03c3-algebra of Borel sets. Denote by F (\u03b8) = \u222b (\u2202 log p\u2202\u03b8 (x; \u03b8))( \u2202 log p \u2202\u03b8 (x; \u03b8))\n>p(x; \u03b8)\u03bd(dx) be the Fisher information matrix of p at \u03b8. The family p is called regular if the following hold:\n(a) p(x; \u03b8) is a continuous function on \u0398 for \u03bd-almost all x;\n(b) p possesses finite Fisher\u2019s information at each point \u03b8 \u2208 \u0398; (c) the function \u03c8(\u00b7; \u03b8) is continuous in the space L2(\u03bd). Theorem 5 (Cramer-Rao Lower Bound). Let p = (p(x; \u03b8))x\u2208X ,\u03b8\u2208\u0398 be a regular family of densities with information matrix F (\u03b8) 0, \u03b8 \u2208 \u0398. Pick \u03b8 \u2208 \u0398 and assume that \u03c8 : \u0398\u2192 R, t : X \u2192 R are measurable such that u 7\u2192 \u222b (t(x)\u2212 \u03c8(u))2p(x;u)\u03bd(dx) is bounded in a neighborhood of \u03b8 and \u03c8\nis differentiable. Then, the bias d(u) = \u222b t(x)p(x;u)\u03bd(dx)\u2212\u03c8(u) is continuously differentiable in a neighborhood of the point \u03b8 \u2208 \u0398 and\nE [ (t(X)\u2212 \u03c8(\u03b8))2 ] \u2265 (\u03c8\u2032(\u03b8) + d\u2032(\u03b8))> F\u22121(\u03b8) (\u03c8\u2032(\u03b8) + d\u2032(\u03b8)) + \u2016d\u2032(\u03b8)\u201622 , (8)\nwhere X \u223c p(\u00b7; \u03b8)\u03bd(\u00b7).\nThe proof follows closely that of Theorem 7.3 of Ibragimov and Has\u2019minskii (1981), which states this result for \u03c8(\u03b8) = \u03b8 (and thus k = K) only, and is hence omitted.\nWith this, we can present the details of the proof of the second part of Theorem 1. Choose X = A \u00d7 R, p(a, y; \u03b8) = \u03c0D(a)\u03d5(y; r(a), \u03c32(a)), where \u03b8 = (r(a))a\u2208A is the unknown parameter to be estimated, and \u03d5(\u00b7;\u00b5, \u03c32) is the density of the normal distribution with mean \u00b5 and variance \u03c32, \u0398 = R. It is easy to see that p = (p(\u00b7; \u03b8)\u03b8\u2208\u0398) is a regular family. Let the quantity to be estimated be \u03c8(\u03b8) = \u2211 a \u03c0(a)r(a). By Theorem 5, for any estimator A, if v\u0302n is the estimate constructed by A based on the data Dn generated from p(\u00b7; \u03b8) in an i.i.d. fashion, the bias dn(\u03b8) = E\u03b8[v\u0302n] is differentiable on \u0398 and\nMSE (v\u0302) \u2265 1 n (\u03c8\u2032(\u03b8) + d\u2032n(\u03b8)) > F\u22121(\u03b8) (\u03c8\u2032(\u03b8) + d\u2032n(\u03b8)) + \u2016d\u2032n(\u03b8)\u2016 2 2 , (9)\nwhere F (\u03b8) is the Fisher information matrix underlying p(\u00b7; \u03b8). If MSE (v\u0302n) 6\u2192 0 then lim supn\u2192\u221e MSE(v\u0302n) V1/n\n= +\u221e. Hence, it suffices to consider A such that MSE (v\u0302n) \u2192 0. Then, by (9), 0 \u2264 \u2016d\u2032n(\u03b8)\u2016 2 2 \u2264 MSE (v\u0302n), hence we also have \u2016d\u2032n(\u03b8)\u2016 2 2 \u2192 0.\nNow, a direct calculation shows that F (\u03b8) = diag(. . . , \u03c0D(a)/\u03c32(a), . . .) and \u03c8\u2032(\u03b8) = \u03c0. Hence, \u03c8\u2032(\u03b8)>F\u22121(\u03b8)\u03c8\u2032(\u03b8) = V1 and using again (9),\nlim sup n\u2192\u221e\nMSE (v\u0302n)\nV1/n \u2265 1\u2212 2 lim sup n\u2192\u221e\n(d\u2032n(\u03b8)) >F\u22121(\u03b8)\u03c8\u2032(\u03b8)\nV1 = 1 ,\nfinishing the proof.\nA.2 Proof for Proposition 1\nIn the proof, we use the shorthand v\u0302LR for v\u0302LR(\u03c0, \u03c0D, Dn). As already noted, the estimator is unbiased, so its MSE equals its variance. Since samples in Dn are independent, we have\nV(v\u0302LR) = 1 n V ( \u03c0(A) \u03c0D(A) R ) .\nThe law of total variance implies\nV(v\u0302LR) = 1 n E [ V ( \u03c0(A) \u03c0D(A) R|A )] + 1 n V [ E ( \u03c0(A) \u03c0D(A) R|A )] .\nThe first term equals\n1 n E [( \u03c0(A) \u03c0D(A) )2 \u03c32(A)|A )] = 1 n \u2211 a \u03c0D(a) \u03c02(a) \u03c02D(a) \u03c32(a) = V1 n .\nThe second term is\n1 n V [ \u03c0(A) \u03c0D(A) r\u03a6(A) ] = 1 n [\u2211 a \u03c02(a) \u03c0D(a) r2\u03a6(a)\u2212 (v\u03c0\u03a6) 2 ] = V2 n .\nCombining the two above completes the proof.\nA.3 Proof for Proposition 2\nWe note that the MSE is equal to the sum of the variance and the squared bias. Let us abbreviate v\u0302Reg(\u03c0,D n) by v\u0302Reg. First, notice that this estimate is (slightly) biased:\nE[v\u0302Reg] = \u2211 a \u03c0(a)E[r\u0302(a)]\n= \u2211 a \u03c0(a)E[E[r\u0302(a)|n(a)]]\n= \u2211 a \u03c0(a)E[r\u03a6(a)I{n(a) > 0}+ 0\u00d7 I{n(a) = 0}]\n= \u2211 a \u03c0(a)r\u03a6(a)(1\u2212 pa,n).\nThus, the squared bias can be bounded as follows:\n(E[v\u0302Reg]\u2212 v\u03c0\u03a6) 2 = (\u2211 a \u03c0(a)r\u03a6(a)pa,n )2 .\nFor the variance term, we again use the law of total variance to yield:\nV(v\u0302Reg) = E[V(v\u0302Reg|n(1), . . . , n(K))] + V(E[v\u0302Reg|n(1), . . . , n(K)]).\nNow, conditioned on n(1), . . . , n(K), the estimates {r\u0302(a)}a\u2208A are independent, so, by distinguishing the case n(a) > 0 (for which the variance of r\u0302(a) is \u03c32(a)/n(a)) from the other case n(a) = 0 (for which this variance is 0), we have\nV(v\u0302Reg|n(1), . . . , n(K)) = \u2211 a \u03c02(a) (\u03c32(a) n(a) I{n(a) > 0}+ 0\u00d7 I{n(a) = 0} ) .\nThus,\nE[V(v\u0302Reg|n(1), . . . , n(K))] = \u2211 a \u03c02(a)\u03c32(a)E [ 1 n(a) I{n(a) > 0} ] .\nFor the second variance term, we also distinguish the case n(a) > 0, for which E[r\u0302(a)|n(a)] = r\u03a6(a), from the case n(a) = 0, for which E[r\u0302(a)|n(a) = 0], thus\nE[v\u0302Reg|n(1), . . . , n(K)] = \u2211 a \u03c0(a)(r\u03a6(a)I{n(a) > 0}+ 0\u00d7 I{n(a) = 0}),\nHence, V(E[v\u0302Reg|n(1), . . . , n(K)]) = V( \u2211 a \u03c0(a)r\u03a6(a)I{n(a) > 0}), which by Lemma 2 implies\nV( \u2211 a \u03c0(a)r\u03a6(a)I{n(a) > 0}) \u2264 \u2211 a \u03c02(a)r2\u03a6(a) pa,n(1\u2212 pa,n) .\nThe proof of the upper bound is then completed by adding squared bias to variance, and using definitions of V0,n, V1, and V3.\nFor the lower bound, use Theorem 5. As mentioned in Appendix A.1, the Fisher information matrix is F (\u03b8) = diag(. . . , \u03c0D(a)/\u03c32(a), . . .) and if the target is \u03c8(\u03b8) = \u2211 a \u03c0(a)r(a), \u03c8\n\u2032(\u03b8) = \u03c0. Calculating the derivative of the bias and plugging into (8), we get the result.\nA.4 Proof for Lemma 1\nFor convenience, the lemma is restated here.\nLemma 1. Let X1, . . . , Xn be n independent Bernoulli random variables with parameter p > 0. Letting Sn = \u2211n i=1Xi, p\u0302 = Sn/n, Z = I{Sn>0} p\u0302 \u2212 1 p , we have for any n and p that\nE [Z] \u2264 4 p . (10)\nFurther, when np \u2265 34,\nE [Z] \u2264 2 p\n\u221a 2\nnp\n(\u221a 3\n2 ln (np 2 ) + 1 ) . (11)\nProof. According to the multiplicative Chernoff bound for the low tail (cf. Lemma 3 in the Appendix), for any 0 < \u03b4 \u2264 1, with probability at least 1\u2212 \u03b4, we have\np\u0302 \u2265 p\u2212 \u221a 2p\nn ln\n1 \u03b4 .\nDenote by E\u03b4 the event when this inequality holds. Assuming 2\nnp ln\n1 \u03b4 \u2264 1/4 , (12)\nthanks to 1/(1\u2212 x) \u2264 1 + 2x which holds for any x \u2208 [0, 1/2], on E\u03b4 we have\nZ \u2264 1 p\u0302 \u2212 1 p \u2264 1 p  1 1\u2212 \u221a 2 np ln 1 \u03b4 \u2212 1  \u2264 2 p \u221a 2 np ln 1 \u03b4 .\nThen, since Z \u2264 n, we have for every \u03b4 satisfying (12) that\nE[Z] \u2264 2 p\n\u221a 2\nnp ln\n1 \u03b4 + \u03b4n = 2 p\n(\u221a 2\nnp ln\n1 \u03b4 + np 2 \u03b4\n) = 2 p f (np 2 , \u03b4 ) , (13)\nwhere f(u, \u03b4) = \u221a\n1 u ln 1 \u03b4 + u\u03b4. Hence, it remains to choose \u03b4 to approximately minimize f(u, \u03b4)\nsubject to the constraint \u03b4 \u2265 e\u2212u/4 (due to (12)). First, note that if we choose \u03b4 = e\u2212u/4, then f(u, e\u2212u/4) \u2264 12 + ue \u2212u/4 < 2, showing that EZ \u2264 4/p, proving the first part of the result.\nTo get the second part, we choose \u03b4 = u\u22123/2, which satisfies (12) since u\u22123/2 \u2265 e\u2212u/4 for u \u2265 17. Then, f(u, u\u22123/2) = u\u22121/2 (\u221a 3 2 ln(u) + 1 ) . Plugging this into (13) finishes the proof.\nA.5 Technical Lemmas\nLemma 2. Using notation from Section 2.3, and wa = \u03c0(a)r\u03a6(a) one has\nV \u2217 := V (\u2211 a \u03c0(a)r\u03a6(a)I{n(a) > 0} ) \u2264 \u2211 a\u2208A w2a pa,n(1\u2212 pa,n)\nprovided that r(a) \u2265 0 for all action a \u2208 A.\nProof. Let Xa = I{n(a) > 0}. First, note that E [Xa] = pa,n and so\nV (\u2211 a\u2208A waI{n(a) > 0} ) = E [{\u2211 a\u2208A wa(Xa \u2212 pa,n) }2]\n= \u2211 a,b\u2208A wawb E [(Xa \u2212 pa,n)(Xb \u2212 pb,n)]\n\u2264 \u2211 a\u2208A w2a E [ (Xa \u2212 pa,n)2 ] (negative association)\n= \u2211 a\u2208A w2a pa,n(1\u2212 pa,n) .\nLemma 3 (Multiplicative Chernoff Bound for the Lower Tail, Theorem 4.5 of Mitzenmacher and Upfal (2005)). LetX1, . . . , Xn be independent Bernoulli random variables with parameter p, Sn =\u2211n i=1Xi. Then, for any 0 \u2264 \u03b2 < 1,\nP ( Sn n \u2264 (1\u2212 \u03b2)p ) \u2264 exp ( \u2212\u03b2 2np 2 ) ."}, {"heading": "B Extension to Contextual Bandits", "text": "In this section we consider an extension of our previous results to finite contextual bandits. As we shall soon see, the extension is seamless. The problem setup is as follows: In addition to the finite action set A = {1, 2, . . . ,K}, we are also given a context set X = {1, 2, . . . ,M}. A policy now is a map \u03c0 : X \u2192 [0, 1]A such that for any x \u2208 X , \u03c0(x) is a probability distribution over the action space A. For notational convenience, we will use \u03c0(a|x) instead of \u03c0(x)(a). The set of policies over X and A will be denoted by \u03a0(X ,A). The process generating the data Dn = {(Xi, Ai, Ri)}1\u2264i\u2264n is described by the following: (Xi, Ai, Ri) are independent copies of (X,A,R), where X \u223c \u00b5(\u00b7), A \u223c \u03c0D(\u00b7|X) and R \u223c \u03a6(\u00b7|A,X) for some unknown family of distributions {\u03a6(\u00b7|a, x)}a\u2208A,x\u2208X and known policy \u03c0D \u2208 \u03a0(X ,A) and context distribution \u00b5. For simplicity, we fix Rmax = 1. We are also given a known target policy \u03c0 \u2208 \u03a0(X ,A) and want to estimate its value, v\u03c0,\u00b5\u03a6 := EX\u223c\u00b5,A\u223c\u03c0(\u00b7|X),R\u223c\u03a6(\u00b7|A,X)[R] based on the knowledge of Dn, \u03c0D, \u00b5 and \u03c0, where the quality of an estimate v\u0302 constructed based on Dn (and \u03c0, \u03c0D, \u00b5) is measured by its mean squared error, MSE (v\u0302) := E [ (v\u0302 \u2212 v\u03c0,\u00b5\u03a6 )2 ] , just like in the case of contextless bandits.\nLet \u03c32\u03a6(x, a) = V(R) for R \u223c \u03a6(\u00b7|x, a), x \u2208 X , a \u2208 A. An estimator A can be considered as a function that maps (\u00b5, \u03c0, \u03c0D, Dn) to an estimate of v \u03c0,\u00b5 \u03a6 , denoted v\u0302A(\u00b5, \u03c0, \u03c0D, D\nn). Fix \u03c32 := (\u03c32(x, a))x\u2208X ,a\u2208A. The minimax optimal risk subject to \u03c32\u03a6(x, a) \u2264 \u03c32(x, a) for all x \u2208 X , a \u2208 A is defined by\nR\u2217n(\u00b5, \u03c0, \u03c0D, \u03c3 2) := inf\nA sup \u03a6:\u03c32\u03a6\u2264\u03c32 E [ (v\u0302A(\u00b5, \u03c0, \u03c0D, D n)\u2212 v\u03c0,\u00b5\u03a6 ) 2 ] .\nThe main observation is that the estimation problem for the contextual case can actually be reduced to the contextless bandit case by treating the context-action pairs as \u201cactions\u201d belonging to the product space X \u00d7A. For any policy \u03c0, by slightly abusing notation, let (\u00b5\u2297\u03c0)(x, a) = \u00b5(x)\u03c0(a|x) be the joint distribution of (X,A) when X \u223c \u00b5(\u00b7), A \u223c \u03c0(\u00b7|X). This way, we can map any contextual policy evaluation problem defined by \u00b5,\u03c0D, \u03c0, \u03a6 and a sample size n into a contextless policy evaluation problem defined by \u00b5\u2297 \u03c0D, \u00b5\u2297 \u03c0, \u03a6 with action set X \u00d7A. Let X \u223c \u00b5(\u00b7), A \u223c \u03c0D(\u00b7|X), R \u223c \u03a6(\u00b7|X,A) and define\nV1 := E [ V ( \u03c0(A|X) \u03c0D(A|X) R|X,A )] = \u2211 x,a \u00b5(x) \u03c02(a|x) \u03c0D(a|x) \u03c32\u03a6(x, a) ,\nV2 := V ( E [ \u03c0(A|X) \u03c0D(A|X) R|X,A ]) = V ( \u03c0(A|X) \u03c0D(A|X) r\u03a6(X,A) ) .\nNote that V1 and V2 are a function of \u00b5, \u03c0D and \u03c0. In this case the LR and REG estimators take the following form\nv\u0302LR = 1\nn n\u2211 i=1 \u03c0(Ai|Xi) \u03c0D(Ai|Xi) Ri and v\u0302Reg = \u2211 x,a \u00b5(x)\u03c0(a|x)r\u0302(x, a) ,\nwhere now r\u0302(x, a) = \u2211 i I{Xi = x,Ai = a}Ri/ \u2211 i I{Xi = x,Ai = a}. Note that the regression estimator can also be computed in O(n) time independently of the size of X and A, based on rewriting it as a likelihood ratio estimator when \u03c0D is replaced by its empirical estimates (cf. (3)).\nThe mapping from contextual to contextless bandits gives rise to the following result, combined with Theorem 1, Proposition 1 and Theorem 2: Theorem 6. Pick any n > 0, \u00b5, \u03c0D, \u03c0 and \u03c32. Then, one has R\u2217n(\u00b5, \u03c0, \u03c0D, \u03c32) = \u2126 ( maxB\u2282X\u00d7A{ \u2211 (x,a)\u2208B \u00b5(x)\u03c0(a|x)}2{1\u2212 \u2211 (x,a)\u2208B \u00b5(x)\u03c0d(a|x)}n + V1/n ) , MSE (v\u0302LR) = (V1 + V2)/n, and MSE (v\u0302Reg) \u2264 CR\u2217n(\u00b5, \u03c0, \u03c0D, \u03c32), for C = MK{min(4MK,maxx,a r2\u03a6(a)/\u03c32\u03a6(a)) + 5}R\u2217n(\u00b5, \u03c0, \u03c0D, \u03c32). Furthermore, the MSE of the regression estimator approaches the minimax risk as sample size grows to infinity."}, {"heading": "C Extension to Markov Decision Processes", "text": "In this section, we consider an extension to fixed-horizon, finite Markov decision processes (MDPs), which will be reduced to the bandit problem studied in Section 2. Here, an MDP is described by a tuple M = \u3008X ,A, P,\u03a6, \u03bd,H\u3009, where X = {1, . . . , N} is the set of states, A = {1, . . . ,K} the set of actions, P the transition kernel, \u03a6 : X \u00d7 A 7\u2192 R the reward function, \u03bd the start-state distribution, and H the horizon. A policy \u03c0 : X 7\u2192 [0, 1]K maps states to distributions over actions, and we use \u03c0(a|x) to denote the probability of choosing action a in state x. The set of policies over X and A is denoted by \u03a0(X ,A). Given a policy \u03c0 \u2208 \u03a0(X ,A), a trajectory of length H , denoted T = (X,A,R) (for X \u2208 XH , A \u2208 AH , and R \u2208 RH ), is generated as follows: X(1) \u2208 \u03bd(\u00b7); for h \u2208 {1, . . . ,H}, A(h) \u223c \u03c0(\u00b7|X(h)), R(h) \u223c \u03a6(\u00b7|X(h), A(h)), and X(h + 1) \u223c P (\u00b7|X(h), A(h)). The policy value is defined by v\u03c0\u03a6 := ET [ \u2211H h=1R(h)]. For simplicity, we again assume Rmax = 1.\nThe off-policy evaluation problem is to estimate v\u03c0\u03a6 from data D n = {Tt}1\u2264t\u2264n, where each trajectory Tt is independently generated by an exploration policy \u03c0D \u2208 \u03a0(X ,A). Here, we assume the reward distribution \u03a6 is unknown; other quantities including \u03bd, P , H , \u03c0, and \u03c0D are all known. Again, we measure the quality of an estimate v\u0302 by its mean squared error: MSE (v\u0302) := [ (v\u0302 \u2212 v\u03c0\u03a6)2 ] .\nThe key observation is that, similarly to the contextual case, the off-policy evaluation problem in fixed-horizon, finite MDPs can be reduced to the multi-armed bandit case. Specifically, every possible length-H trajectory is an \u201caugmented action\u201d belong to the product space T = XH+1 \u00d7AH . The total number of augmented actions is at most NH+1KH . The distribution over this augmented action space, induced by \u03bd, P and policy \u03c0, is given by: \u00b5(x(1), . . . , x(H+ 1), a(1), . . . , a(H)) := \u03bd(x(1)) \u220fH h=1 \u03c0(a(h)|x(h))P (x(h + 1)|x(h), a(h)) . This way, the off-policy evaluation problem is reduced to the bandit case with corresponding induced distributions over augmented actions.\nFor any (x, a) \u2208 T , let r\u03a6(x, a) := E[R] and \u03c32\u03a6(x, a) := V(R), where R(h) \u223c \u03a6(\u00b7|x(h), a(h)). Define the minimax optimal risk subject to constraints \u03c32\u03a6(x, a) \u2264 \u03c32(x, a) for all (x, a) \u2208 T by:\nR\u2217n(\u03bd, \u03c0, \u03c0D, P,H, \u03c3 2) := inf\nA sup \u03a6:\u03c32\u03a6\u2264\u03c32 E [ (v\u0302A(\u03bd, \u03c0, \u03c0D, P,H,D n)\u2212 v\u03c0\u03a6) 2 ] .\nSimilar to previous sections, one may adjust the definitions of quantities like V1 and V2, and conclude the following result using with Theorem 1, Proposition 1 and Theorem 2:\nTheorem 7. Pick any n > 0, \u03bd, \u03c0D, \u03c0, P , H , and \u03c32. Then, one has R\u2217n(\u03bd, \u03c0, \u03c0D, P,H, \u03c3 2) = \u2126 ( maxB\u2282T { \u2211 (x,a)\u2208T \u00b5(x, a)}2{1\u2212 \u2211 (x,a)\u2208T \u00b5D(\u03c4)}n + V1/n ) , MSE (v\u0302LR) = (V1 + V2)/n, and MSE (v\u0302Reg) \u2264 CR\u2217n(\u03bd, \u03c0, \u03c0D, P,H, \u03c32) for C = NH+1KH{min(4NH+1KH ,max(x,a)\u2208T r2\u03a6(x,a)\n\u03c32\u03a6(x,a) ) + 5}. Finally, the MSE of the regression esti-\nmator approaches the minimax risk as sample size n grows to infinity.\nFinally, it can be shown that in general an exponential dependence onH is unavoidable. An example is the \u201ccombination lock\u201d MDP with N states X = {1, . . . , N} and K = 2 actions A = {L,R}; the start state is x\u2217 = 1. In any state x, action L takes the learner back to the initial state x\u2217, while action R takes the learner to state x + 1. Assume reward is always zero except in state N where it can be {0, Rmax}. It is easy to verify that, if there exists constant p\u2217 such that p\u2217 \u2264 \u03c0D(L|x) for all x, then it takes exponentially many steps to reach state N from x\u2217 under policy \u03c0D. Consequently, it requires at least exponentially many trajectories to evaluate a policy \u03c0 that always takes action R."}, {"heading": "D Connection to Semi-supervised Learning", "text": "In semi-supervised learning one is given a large unlabeled dataset together with a smaller, labeled dataset. The hope is that the large unlabeled dataset will help to decrease the error of an estimator whose job is to predict some value that depends on the unknown distribution generating the data. Clearly, the off-policy policy evaluation problem can be connected to semi-supervised learning: Given the data {(Ai, Ri)}i=1,...,n generated from \u03c0D and \u03a6, the goal being to predict v\u03c0\u03a6. A large \u201cunlabelled\u201d dataset {Aj}j=1,...,m with m n helps one to identify \u03c0D. Indeed, an intriguing idea is to use \u03c0D in some clever way to help improving the prediction of v\u03c0\u03a6. The most obvious way is to use it in the likelihood ratio estimator. However, as we have shown, the MSE of the likelihood ratio estimator can be much larger than that of the regression estimator, which does not use \u03c0D even if it is available. Further, the MSE of the regression estimator is unimprovable, apart from a constant factor for finite sample sizes, while it also rapidly approaches the optimal minimax MSE as the sample size grow. Hence, it seems unlikely that knowing \u03c0D can help in this problem.\nNote that the regression estimator can also be thought as the solution to a least-squares regression problem and our results thus have implications for using unlabelled data together with least-squares estimators. Indeed, if Xi \u2208 {0, 1}K is chosen to be the Ais unit vector of the standard Euclidean basis, we can write r\u0302 = (X>X)\u2020X>R, where \u2020 denotes pseudo-inverse, X \u2208 Rn\u00d7K and R \u2208 Rn are defined by R = (R1, . . . , Rn)>, X> = (X1, . . . , Xn). Notice that here Gn = 1nX\n>X = 1 n \u2211n i=1XiX > i = diag(\u03c0\u0302D(1), . . . , \u03c0\u0302D(K)). Thus, 1 nX >X can be seen as an estimate of G =\nE [ XiX > i ] = diag(\u03c0D(1), . . . , \u03c0D(K)). Having access to a large unlabelled set U1, . . . , Um (i.e., m n) coming from the same distribution as the Xis, it is tempting to replace 1nX >X with a\n\u201cbetter estimate\u201d, Hm = 1m \u2211m i=1 UiU > i . Taking m to the limit, we see that Hm converges to G. Now, replacing Gn with Hm \u2248 G in the least squares estimate, and then taking the weighted sum of the resulting values with weights \u03c0(a), we get the likelihood ratio estimator. Again, since this was shown to be inferior to the regression estimator, replacingGn withHm sound like an idea of dubious status. In fact, preliminary experiments with simple simulated scenarios confirmed that Gn indeed should not be replaced with Hm, even when m is very large in least-squares regression estimation."}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Counterfactual reasoning and learning systems: The example of computational advertising", "author": ["L\u00e9on Bottou", "Jonas Peters", "Joaquin Qui\u00f1onero-Candela", "Denis Xavier Charles", "D. Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Simard", "Ed Snelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bottou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bottou et al\\.", "year": 2013}, {"title": "Two faces of active learning", "author": ["Sanjoy Dasgupta"], "venue": "Theoretical Computer Science,", "citeRegEx": "Dasgupta.,? \\Q2011\\E", "shortCiteRegEx": "Dasgupta.", "year": 2011}, {"title": "Hoeffding and Bernstein races for selecting policies in evolutionary direct policy search", "author": ["V. Heidrich-Meisner", "C. Igel"], "venue": "In ICML,", "citeRegEx": "Heidrich.Meisner and Igel.,? \\Q2009\\E", "shortCiteRegEx": "Heidrich.Meisner and Igel.", "year": 2009}, {"title": "Efficient estimation of average treatment effects using the estimated propensity", "author": ["Keisuke Hirano", "Guido W. Imbens", "Geert Ridder"], "venue": "score. Econometrica,", "citeRegEx": "Hirano et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hirano et al\\.", "year": 2003}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["John Langford", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Langford and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2008}, {"title": "Exploration scavenging", "author": ["John Langford", "Alexander L. Strehl", "Jennifer Wortman"], "venue": "In Proceedings of the Twenty-Fifth International Conference on Machine Learning,", "citeRegEx": "Langford et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2008}, {"title": "Unbiased offline evaluation of contextualbandit-based news article recommendation algorithms", "author": ["Lihong Li", "Wei Chu", "John Langford", "Xuanhui Wang"], "venue": "In Proceedings of the Fourth International Conference on Web Search and Web Data Mining (WSDM-11),", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Probability and Computing: Randomized Algorithms and Probabilistic Analysis", "author": ["Michael Mitzenmacher", "Eli Upfal"], "venue": null, "citeRegEx": "Mitzenmacher and Upfal.,? \\Q2005\\E", "shortCiteRegEx": "Mitzenmacher and Upfal.", "year": 2005}, {"title": "PEGASUS: A policy search method for large MDPs and POMDPs", "author": ["A.Y. Ng", "M. Jordan"], "venue": "In UAI,", "citeRegEx": "Ng and Jordan.,? \\Q2000\\E", "shortCiteRegEx": "Ng and Jordan.", "year": 2000}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["Doina Precup", "Richard S. Sutton", "Satinder P. Singh"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning", "citeRegEx": "Precup et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Information-based complexity, feedback and dynamics in convex programming", "author": ["Maxim Raginsky", "Alexander Rakhlin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Raginsky and Rakhlin.,? \\Q2011\\E", "shortCiteRegEx": "Raginsky and Rakhlin.", "year": 2011}, {"title": "The central role of the propensity score in observational studies for causal effects", "author": ["P. Rosenbaum", "D. Rubin"], "venue": null, "citeRegEx": "Rosenbaum and Rubin.,? \\Q1983\\E", "shortCiteRegEx": "Rosenbaum and Rubin.", "year": 1983}, {"title": "Reducing bias in observational studies using subclassification on the propensity score", "author": ["P. Rosenbaum", "D. Rubin"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rosenbaum and Rubin.,? \\Q1985\\E", "shortCiteRegEx": "Rosenbaum and Rubin.", "year": 1985}, {"title": "Input-dependent estimation of generalization error under covariate shift", "author": ["M. Sugiyama", "K. M\u00fcller"], "venue": "Statistics & Decisions,", "citeRegEx": "Sugiyama and M\u00fcller.,? \\Q2005\\E", "shortCiteRegEx": "Sugiyama and M\u00fcller.", "year": 2005}, {"title": "Analysis of kernel mean matching under covariate shift", "author": ["Yaoliang Yu", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the Twenty-Ninth International Conference on Machine Learning,", "citeRegEx": "Yu and Szepesv\u00e1ri.,? \\Q2012\\E", "shortCiteRegEx": "Yu and Szepesv\u00e1ri.", "year": 2012}, {"title": "Introduction to semi-supervised learning", "author": ["Xiaojin Zhu", "Andrew B. Goldberg"], "venue": null, "citeRegEx": "Zhu and Goldberg.,? \\Q2009\\E", "shortCiteRegEx": "Zhu and Goldberg.", "year": 2009}], "referenceMentions": [{"referenceID": 7, "context": ", 2010), which is sometimes referred to as offline evaluation in the bandit literature (Li et al., 2011) or counterfactual reasoning (Bottou et al.", "startOffset": 87, "endOffset": 104}, {"referenceID": 1, "context": ", 2011) or counterfactual reasoning (Bottou et al., 2013).", "startOffset": 36, "endOffset": 57}, {"referenceID": 9, "context": ", 2013) and can also be looked as a key building block for policy optimization which, as in supervised learning, can often be reduced to evaluation, as long as the complexity of the policy class is well-controlled (Ng and Jordan, 2000).", "startOffset": 214, "endOffset": 235}, {"referenceID": 2, "context": "In the context of supervised learning, in the covariate shift literature, the problem of estimating losses under changing distributions is crucial for model selection (Sugiyama and M\u00fcller, 2005, Yu and Szepesv\u00e1ri, 2012) and also appears in active learning (Dasgupta, 2011).", "startOffset": 256, "endOffset": 272}, {"referenceID": 4, "context": "Here, the focus is on the two-action (binary) case where the goal is to estimate the difference between the expected rewards of the two actions (Hirano et al., 2003), which is slightly (but not essentially) different than our setting.", "startOffset": 144, "endOffset": 165}, {"referenceID": 4, "context": "As opposed to the statistics literature (Hirano et al., 2003), we are interested in results for finite sample sizes.", "startOffset": 40, "endOffset": 61}, {"referenceID": 4, "context": "The motivation of studying this estimator is both its simplicity and also because it is known that a related estimator is asymptotically efficient (Hirano et al., 2003).", "startOffset": 147, "endOffset": 168}, {"referenceID": 11, "context": ", Raginsky and Rakhlin (2011)).", "startOffset": 2, "endOffset": 30}, {"referenceID": 11, "context": "Now setting p = 6, and applying Lemma 1, Theorem 1 and the \u201cInformation Radius bound\u201d from Raginsky and Rakhlin (2011), we have n \u2265 V1 4\u03b5 .", "startOffset": 91, "endOffset": 119}, {"referenceID": 1, "context": "One of the most popular estimators is known as the propensity score estimator in the statistical literature (Rosenbaum and Rubin, 1983, 1985), or the importance weighting estimator (Bottou et al., 2013).", "startOffset": 181, "endOffset": 202}, {"referenceID": 16, "context": "In this section, we consider extensions of our previous results to contextual bandits and Markovian Decision Processes, while implications to semi-supervised learning (Zhu and Goldberg, 2009) are discussed in the supplementary material.", "startOffset": 167, "endOffset": 191}], "year": 2014, "abstractText": "This paper studies the off-policy evaluation problem, where one aims to estimate the value of a target policy based on a sample of observations collected by another policy. We first consider the multi-armed bandit case, establish a minimax risk lower bound, and analyze the risk of two standard estimators. It is shown, and verified in simulation, that one is minimax optimal up to a constant, while another can be arbitrarily worse, despite its empirical success and popularity. The results are applied to related problems in contextual bandits and fixed-horizon Markov decision processes, and are also related to semi-supervised learning.", "creator": "Creator"}}}