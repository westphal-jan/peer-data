{"id": "1409.4988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2014", "title": "An Agent-Based Algorithm exploiting Multiple Local Dissimilarities for Clusters Mining and Knowledge Discovery", "abstract": "we propose a standard multi - agent algorithm able to automatically discover relevant regularities in a given dataset, determining it at essentially the same time the set of configurations representative of the regularly adopted parametric dissimilarity measure algorithms yielding compact and separated clusters. each agent operates independently by performing essentially a markovian random walk on a suitable weighted - graph representation of the input dataset. such a weighted graph representation is induced by the specific parameter configuration of the dissimilarity measure adopted by the agent, which searches and takes decisions autonomously for one cluster at a time. results, show that the algorithm is able to discover parameter configurations that yield a consistent and interpretable collection of clusters. moreover, we demonstrate that our spatial algorithm implementation shows comparable performances performing with other similar state - of - still the - art algorithms when comparing facing similar specific clustering problems.", "histories": [["v1", "Wed, 17 Sep 2014 14:39:37 GMT  (1014kb)", "http://arxiv.org/abs/1409.4988v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC cs.MA", "authors": ["filippo maria bianchi", "enrico maiorino", "lorenzo livi", "antonello rizzi", "alireza sadeghian"], "accepted": false, "id": "1409.4988"}, "pdf": {"name": "1409.4988.pdf", "metadata": {"source": "CRF", "title": "An Agent-Based Algorithm exploiting Multiple Local Dissimilarities for Clusters Mining and Knowledge Discovery", "authors": ["Filippo Maria Bianchi", "Enrico Maiorino", "Lorenzo Livi", "Antonello Rizzi", "Alireza Sadeghian"], "emails": ["filippo.binachi@ryerson.ca", "enrico.maiorino@uniroma1.it", "llivi@scs.ryerson.ca", "antonello.rizzi@uniroma1.it", "asadeghi@ryerson.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n49 88\nv1 [\ncs .L\nWe propose a multi-agent algorithm able to automatically discover relevant regularities in a given dataset, determining at the same time the set of configurations of the adopted parametric dissimilarity measure yielding compact and separated clusters. Each agent operates independently by performing a Markovian random walk on a suitable weighted graph representation of the input dataset. Such a weighted graph representation is induced by the specific parameter configuration of the dissimilarity measure adopted by the agent, which searches and takes decisions autonomously for one cluster at a time. Results show that the algorithm is able to discover parameter configurations\n\u2217Corresponding Author Email addresses: filippo.binachi@ryerson.ca (Filippo Maria Bianchi),\nenrico.maiorino@uniroma1.it (Enrico Maiorino), llivi@scs.ryerson.ca (Lorenzo Livi), antonello.rizzi@uniroma1.it (Antonello Rizzi), asadeghi@ryerson.ca (Alireza Sadeghian)\nURL: https://sites.google.com/site/lorenzlivi/ (Lorenzo Livi), http://infocom.uniroma1.it/~rizzi/ (Antonello Rizzi), http://www.scs.ryerson.ca/~asadeghi/ (Alireza Sadeghian)\nPreprint submitted to Information Sciences September 18, 2014\nthat yield a consistent and interpretable collection of clusters. Moreover, we demonstrate that our algorithm shows comparable performances with other similar state-of-the-art algorithms when facing specific clustering problems. Keywords: Agent Based Algorithms; Data Mining; Knowledge Discovery; Clustering; Local Dissimilarity Measure; Graph conductance; Random Walk."}, {"heading": "1. Introduction", "text": "Finding characterizing regularities in data is an important knowledge discovery task, which can be exploited for a multitude of purposes. When there is not any a-priori knowledge on the dataset at hand, it could be useful to perform an initial analysis of the data in order to learn how to compare the elements in a meaningful way, so that relevant patterns in the dataset can be discovered. Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end. Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22]. Graph-based techniques have the fundamental advantage of mapping the original problem onto a \u201cdimensionless\u201d object: the graph. Moreover, graph theory offers a plateau of theoretical results to be exploited by effective algorithms, which easily integrate with the agent-based paradigm. Typical settings involving the interplay of both approaches include random walk (RW) based algorithms [2, 19], in which agents move and interact on the graph via specific (probabilistic) mechanisms.\nWhen there is uncertainty about the nature of the dataset at hand, a\nfundamental issue is the definition of the dissimilarity among the input patterns [30, 43, 17], since the specific dissimilarity measure adopted by the data mining procedure affects the possibility of discovering meaningful regularities. Depending on the application at hand, data can be collected and represented relying on several different formalisms [31]. Accordingly, many (non metric) parametric dissimilarity measures could be designed depending on the specific task. Recently, there is a steady increasing interest in using several, possibly heterogeneous, dissimilarity measures at the same time [28, 40, 41, 8, 24]. Regardless of the number of dissimilarity measures, the setting of their characterizing parameters is what really allows to discover the relevant information hidden in the data.\nMetric learning [44, 48, 49, 50, 14] is an important subfield of pattern recognition. Techniques in this field deal with the problem of learning an optimal setting of the parameters characterizing the particular dissimilarity for the problem at hand \u2013 usually it is assumed to be a metric distance. For a given dissimilarity measure, it is possible to distinguish two main approaches [35]: those trying to determine a partition of data, and those that focus on searching for isolated clusters surrounded by uncategorized data. Local description of data is of particular interest, since it allows to characterize the input data by means of a heterogeneous collection of descriptions [8].\nIn this paper we propose the Local Dissimilarities - Agent Based Clusters Discoverer (LD-ABCD) algorithm. LD-ABCD is designed to discover (learn) configurations of a parametric dissimilarity measure yielding at least a well-formed cluster in the data. Cluster discovery is implemented by means of multiple RWs that are performed independently by several agents on the\ngraph representing the dataset. Each agent first selects a specific parameter configuration (PC), with which it constructs a weighted graph representing the relations among the input patterns. The behavior of a RW is thus dependent on the specific configuration of the parameters. During a RW, an agent searches and takes decisions autonomously for one cluster at a time. A suitable online mechanism is designed to decide whether a set of patterns found (i.e., \u201cwalked upon\u201d) by an agent should be accepted or rejected as a meaningful cluster. To this end, we heavily exploit the graph conductance concept [27]. We demonstrate the validity of our approach by performing different types of experiments. First, we compare LD-ABCD with respect to (w.r.t.) three different state-of-the-art graph-based clustering algorithms over suitable clustering problems. In particular, we evaluate the capability of the considered algorithms to discover clusters composed of patterns belonging to the same (predefined) class. Successively, we evaluate the capability of LD-ABCD of discovering relevant PCs (RPCs), that is, those that yield well-formed clusters. Additionally, we provide demonstrative examples introducing the concept of equivalency among PCs. Finally, we provide a comparison between two variants of the LD-ABCD algorithm.\nThe remainder of the paper is structured as follows. In Sec. 2 we introduce LD-ABCD, describing in detail all relevant stages of the algorithm. In Sec. 3 we present a variant of LD-ABCD that exploits two diverse families of agents. Experimental evaluations are presented and discussed in Sec. 4, while in Sec. 5 we show our conclusions. Finally, Appendix A provides the technical details related to the definition and calculation of the graph conductance."}, {"heading": "1.1. Related Works", "text": "The work that we present in this paper is related to several different topics, specifically graph clustering, conductance evaluation, metric learning, and agent-based computing. At the best of the author knowledge, it was not possible to identify other works that treat the problem of clustering and knowledge discovery with approaches similar to the one that we proposed. The aim of this section is helping the reader to contextualize our work and to correctly identify the concepts to which our work is related.\nIn particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38]. Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38]. Each agent examines the patterns by performing a RW [2, 19] on the graph that represents the dataset and tries to group them in different clusters. Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42]. Finally, each agent searches the clusters in the dataset using different configurations of the adopted dissimilarity measure, seeking for the ones that better characterize the set of elements contained in the cluster. This procedure is strongly related to the problem of the metric-learning [44, 48, 49, 50, 14], which is the task of determining the optimal parameters of a given metric distance. However, in our case we make no a-priori assumptions on the adopted distance (which we call dissimilarity measure in our study)."}, {"heading": "2. The Proposed LD-ABCD Algorithm", "text": "The proposed multi-agent algorithm is designed to operate over a general input domain, X , which may not necessarily be a subset of Rn. Let d : X \u00d7 X \u2192 R+ be a symmetric dissimilarity measure that depends on some parameters/weights, i.e., PCs, which we denote as m \u2208 M. The main goal of the proposed algorithm is to determine all RPCs which are capable of inducing a well-formed cluster structure. In this sense, our algorithm should be intended also as a \u201cknowledge discovery\u201d algorithm, since, in addition to the clusters discovered using local configurations of d(\u00b7, \u00b7;m), it outputs all relevant settings of the parameters characterizing the dissimilarity measure, which may be useful in terms of interpretability of the data and related clusters. Without loss of generality, we also assume that M = [0, 1]D, where D is the number of parameters/weights characterizing d(\u00b7, \u00b7;m).\nFig. 1 provides the overall high-level schema of the LD-ABCD algorithm, together with details of the operations performed by a single agent within the proposed system. Each agent ai uses a different PC m (i) j for evaluating the dissimilarity among the patterns in the input dataset S = {x1, x2, ..., xn} \u2282 X . The dataset is initially represented as a weighted complete undirected graph, Gj = (V, E , w), where each edge ekl \u2208 E is characterized by a weight, w(ekl;m (i) j ) \u2208 [0, 1], which depends on the dissimilarity d(xk, xl;m (i) j ) evaluated with the specific m (i) j . Each agent performs a Markovian RW [32] on the graph Gj , visiting a number of vertices (nodes) until a quantity called \u201cenergy\u201d is not depleted. The RW transition probabilities from one node to another are determined by the weight values of the edges between pairs of nodes and, hence, depend on the parameter configuration associated with\nthe agent. When an agent ai equipped with the PC m (i) j runs out of energy, the vertices visited so far during the RW are interpreted as the cluster ch(m (i) j ) \u2282 V (or chj for notation simplicity) found by the agent \u2013 which corresponds also to the subgraph ghj. Therefore, each agent generates a single cluster at a time that is readily evaluated by the agent itself, which takes an autonomous decision on its acceptance. Since each agent generates the clusters independently from the others, the clusters retrieved by LD-ABCD may overlap (i.e. a given pattern can belong to more than one cluster) and, also, their union could not be equal to V; thus LD-ABCD does not generate a partition of the data (i.e. not all the patterns in the data set will belong to a cluster). During its lifetime, an agent performs several RWs on different versions of the same graph, which depend on the adopted PCs. Since it is possible for an agent to find similar clusters when using different PCs, these are progressively aggregated in prototypes called meta-clusters. The algorithm proceeds as long as new distinct clusters are extracted or new PCs are discovered. When the stop criterion has been met the meta-clusters are returned along with the PCs associated to the set of clusters represented by each meta-cluster. Finally, a centralized unit re-aggregates meta-clusters belonging to different agents according to their similarity to obtain new global meta-clusters. The final solution returned by LD-ABCD is the collection of all global meta-clusters and their corresponding sets of associated PCs. In LD-ABCD, the number of agents is defined a priori by the user and it remains the same during the execution. This number is supposed to be proportional to the available computational resources.\nIn the following, we provide the details about the tasks performed by a\nsingle agent during its lifetime. First, we discuss how the weighted graph is constructed over the input dataset (2.1). Then we focus on the implementation of the RWs (2.2) and the evaluation of the discovered clusters (2.3). The procedure for managing the energy of the agents is discussed in Sec. 2.4. In Sec. 2.5, we describe the process of selection of the new PCs to be exploited, while in Sec. 2.6 we discuss the aggregation of the solutions found\nby different agents and the global convergence criterion of LD-ABCD (Sec. 2.7). Finally we analyze the computational complexity of the algorithm in Sec. 2.8."}, {"heading": "2.1. Graph Construction", "text": "Let assume that an agent ai is equipped with the PC m (i) j , and let S be the dataset under analysis, with n = |S|. The corresponding weighted graph Gj = (V, E , w), is described by the vertices V, each one representing a pattern in S, and by the edges E , which are weighted by implementing w(\u00b7) as the exponential kernel:\nw(elk;m (i) j ) = exp(\u2212\u03c4exp \u00b7 d(xl, xk;m (i) j )). (1)\nThe setting of the parameter \u03c4exp \u2265 0 is an important issue and it will be discussed later in Sec 2.2. A weighted graph can be described by the n \u00d7 n weighted adjacency matrix Aj, defined as:\nAj(l, k) = w(elk;m (i) j ). (2)\nSince the vertex set is not affected by the specific PC, we keep the related data in a shared data structure, which is accessible by all agents. The edges, which instead can differ on the base of the specific PC, are stored \u201clocally\u201d by each agent, encoded in their weighted adjacency matrix. The computational and space costs for Aj is quadratic in the number of vertices-patterns (such a matrix is always dense). For large datasets, the construction of those graphs on a single computing machine could be unfeasible due to memory limitation. By exploiting the distributed nature of the adopted agent-based modeling, we could easily elude this technical problem by suitably dispatching \u201cchunks\u201d of\nthe original datasets among the various agents\u2013machines. This would imply a distributed communication mechanism that, at this stage of development of LD-ABCD, is not implemented yet. Therefore, in the following we assume each agent to be able to process the input dataset as a whole."}, {"heading": "2.2. Random Walk for Cluster Search", "text": "To perform a RW on Gj we need to define the so-called transition matrix [32], Mj , which is used by an agent to navigate among the vertices. Mj is defined as follows,\nMj = D \u22121 j Aj, (3)\nwhere Dj is the (diagonal) degree matrix: Dj(l, l) = \u2211|V| k=1Aj(l, k). A RW can be effectively characterized by exploiting the stationary distribution (SD) \u03c0j of the Markov process underlying the RW. The SD can be interpreted as the left eigenvector of Mj , associated to the largest eigenvalue, i.e., 1. Every complete and non bipartite graph has a stationary distribution [32], which can be conveniently defined by exploiting the so-called degree distribution,\n\u03c0j(vl) = D(l, l)\n2|E| , \u2200vl \u2208 V. (4)\nWe use the SD \u03c0j for selecting the starting vertex vs from which an agent starts a RW, since highly central vertices will have higher probability according to the SD. In this way, we let an agent start a RW from a dense region of the graph, rather than from a peripheral region in which it could be stuck or it could easily move to a vertex belonging to a \u201cdifferent\u201d dense region (see Fig. 2).\nA correct setting of \u03c4exp (1) is crucial, since it affects the behavior of the RW performed by an agent. In fact, a higher value of \u03c4exp magnifies\nthe edge weights between similar patterns, making less likely the unwanted transitions to vertices connected by low weights (i.e., dissimilar patterns). Notably, if we assign to \u03c4exp a value that is too high, the lower weights could be excessively magnified. In this case, an agent would repeatedly move on a very small set of vertices, instead of exploring a larger portion of the graph (transition probabilities become degenerate). On the other hand, assigning a too small value to \u03c4exp would lead to the opposite situation, as it would allow the agent to jump to different regions of the graph during the RW (transition probabilities become uniform).\nIn LD-ABCD, we heuristically set \u03c4exp as a value proportional to the average distance between the input patterns, evaluated using the dissimilarity measure configured with the PC currently selected by the agent,\n\u03c4exp = \u03b2n \u22122\nn\u2211\nl,k=1\nAj(l, k), (5)\nwhere \u03b2 is a user-defined value that is set empirically.\nOf course, more accurate methods could be defined for estimating \u03c4exp. However, since in our setting d(\u00b7, \u00b7) may be possibly not metric (and also not algebraic, i.e., which cannot be expressed in closed form), it is hard to find a strong relation among \u03c4exp and the transition probabilities."}, {"heading": "2.3. Cluster Quality Evaluation", "text": "An agent ai generates a cluster chj during a RW performed on Gj with the PC m (i) j , which consists in the set of vertices of the subgraph ghj visited during the RW (see Fig. 2). In the following, we will refer equivalently to ghj and chj. Once a cluster chj is returned by an agent ai, the cluster can be either accepted or rejected, depending on its quality. Intuitively, a cluster\nis considered to be good if it contains several elements, which are also very similar to each other according to the current PC. A well-established measure used for evaluating the quality of a cluster associated to a subgraph of a larger graph is the conductance [27], \u03c6(chj), which quantifies how well knit is the subgraph internally and how many edges (with their associated weights) connected to vertices outside the cluster are cut. In terms of clustering, a subgraph with low conductance represents a compact and populated cluster, which is also well-separated from the remaining elements of the dataset. A straightforward method for evaluating the quality of a cluster then consists in defining a function CQ1 directly proportional to its conductance:\nCQ1(chj) = 1\u2212 \u03c6(chj). (6)\nA cluster chj is therefore accepted if CQ1(chj) \u2265 \u03c4CQ, where \u03c4CQ \u2265 0 is a\nuser-defined threshold.\nHowever, directly using the conductance as a quality measure of clusters discovered in different datasets could be not easily manageable. In fact, the value of the conductance of a cluster depends also on the configuration of the rest of the dataset and thus it could fall within very diverse ranges, making the decisions and interpretations regarding its quality a difficult task. Additionally, since in our work we made no assumption on the employed dissimilarity measure used for comparing the patterns, it is not easy to express in closed form the variation of the conductance as the values of the parameters of the dissimilarity value change. Thus, given a dataset, it is hard to describe analytically the relation among the quality of the clusters and the used PCs. For those reasons, we introduce here a new quantity for evaluating the quality of a cluster, which takes into account the properties of the whole graph\nconstructed by using a specific PC. In particular, we assert that the quality of a cluster chj is proportional to the closeness of its conductance, \u03c6(chj), to the minimum conductance of the whole graph Gj (or simply the conductance of Gj), denoted as \u03a6(Gj). The exact computation of \u03a6(Gj) is a NP-Hard problem [27], and hence it is not computationally feasible. As a consequence, in this paper we use an approximation for \u03a6(Gj), defined through a pair of real numbers, lb(\u03a6(Gj)), ub(\u03a6(Gj)), which represent, respectively, the lower and the upper bound of the interval that contains the actual value of \u03a6(Gj). These values can be computed exploiting the Cheeger\u2019s inequality, by means of a procedure that is discussed in Appendix A. We introduce a novel cluster quality function, CQ2, defined as:\nCQ2(chj) = 1\u2212 \u03c6(chj)\u2212 lb(\u03a6(Gj))\nub(\u03a6(Gj))\u2212 lb(\u03a6(Gj)) . (7)\nFrom our preliminary experiments, we observed that the use of CQ2 rather than CQ1 characterizes much better the quality of the clusters in our multiparameter setting. To explain this fact with greater detail, let us consider an example where the two aforementioned functions are used for evaluating the cluster quality in two different datasets of R2 vectors depicted in Fig. 3. We decided to consider two different datasets because the evaluation of the conductance is strictly correlated not only to the cluster itself, but also to the whole dataset to which it belongs. In both datasets, we select two different subsets of vertices of the respective graph representations: the first one is associated with a well-defined cluster, while the second one is randomly determined, which accordingly induces a low quality cluster. In Fig. 4 (a) and (b) we plotted the values assumed by CQ1 and CQ2 on the well-defined clusters, which are evaluated as a function of the PCs (in this case uniformly\nsampled in the parameters space [0, 1]2). Instead, in Fig. 4 (c) and (d) we performed the same calculations for the randomly determined clusters. As it is possible to observe, the values assumed by CQ2 fall within similar ranges in the two datasets, allowing to use comparable threshold values (i.e., \u03c4CQ) for evaluating good clusters in different datasets. By using the CQ2 rather than CQ1, we are also able to better discriminate those PCs that better characterize the clusters \u2013 for the first dataset, these are individuated along the bisecting line, while for the second one PCs close to the {0, 1} setting are preferable. In fact CQ2, in correspondence of such PCs, assumes values that better evaluate the quality of clusters: random clusters are always highly penalized while well-formed clusters are better magnified.\nTo conclude, since CQ2 is normalized according to the conductance of the graph, we consider Eq. 7 as an absolute quality measure that can be used for comparing clusters generated by different agents using different PCs. The soundness of such an assumption will be demonstrated by the experiments.\nIn the following of this paper we will always use CQ2 as the function used for evaluating the quality of a cluster and, for the sake of notation, we will refer at it as \u201cCQ\u201d."}, {"heading": "2.4. Energy Update", "text": "Setting a proper value for the (maximum) length of a RW is another important issue to be considered, since it is strictly related to the typical size of the returned clusters/subgraphs. A quantity called energy ei determines how many steps an agent ai is able to perform during a RW. The energy is initialized to a value einit and it is successively modified at each step of the RW. As an agent visits the graph, it builds a subgraph ghj adding the new\nvertices that are being visited, increasing its size and modifying accordingly its current conductance. In particular, when a vertex vl is inserted in ghj, the conductance of the subgraph increases if vl is distant (i.e., very different in our setting) from the other vertices of ghj; otherwise, the conductance will decrease. Note that since the graph is complete, inserting a vertex vl to a subgraph ghj includes the insertion of all edges connecting vl to all vertices in ghj. Hence, the variation of the conductance during a RW can be used for discerning whether an agent is walking in the \u201cright\u201d or in the \u201cwrong\u201d direction, i.e., if the agent is visiting or not a compact area of G. For this reason, we modify the energy ei according to the variations of the conductance of ghj at each step of the RW: if the conductance is decreasing\nthe energy increases, otherwise the energy is reduced. If the conductance remains constant, it means that the agent is moving on vertices that have been already visited. This happens when a suitably dense region has been completely visited and the agent is stuck moving on the same vertices over and over. For this reason, we added also a constant energy decrement in order to consider loops that occur if the agent is not visiting new vertices for a prolonged period.\nThe expression describing the energy update reads as,\ne (new) i = e (old) i + f(\u2206\u03c6(ghj))\u2212 \u03c4energy, (8)\nwhere f(\u2206\u03c6(ghj)) is a function that depends on the variation of the conductance of ghj, and \u03c4energy is the user-defined quantity that controls the constant decrement of the energy. For evaluating the function f(\u00b7), we take into account how the conductance of the subgraph ghj varies each time a vertex is visited in the RW and, possibly, added to the subgraph. If the agent is correctly visiting the vertices of a proper cluster, we expect the conductance to decrease. This decrement, however, in most of the cases is neither regular nor monotone. For this reason, we decided to consider an average computed on r values estimated in r steps of the RW.\nThe energy function f [ \u2206r\u03c6(ghj) ] that computes the moving average vari-\nation of the conductance on r steps of the RW is defined as follows:\nf [ \u2206r\u03c6 ( ghj(t) )] = 1\nr\nr\u22121\u2211\nq=0\n\u03c6 ( ghj(t\u2212 q) ) \u2212 \u03c6 ( ghj(t\u2212 q \u2212 1) ) , (9)\nwhere ghj(t) indicates the subgraph ghj at the t-th time step of the RW.\nIf the value of r is sufficiently low, f [ \u2206r\u03c6(ghj) ] quickly assumes a negative value when the agent exits from a cluster and then it is readily stopped. On\nthe other hand, choosing a value too low for r, makes the system very sensitive to small variations of the conductance, which often occur when the agent is moving within the same cluster. In our experiments (Sec. 4) we set r = 3, a value that allows to detect sufficiently fast when an agent leaves a cluster, filtering at the same time non-relevant changes of the conductance."}, {"heading": "2.5. Selection of New PCs", "text": "A RW is terminated when the energy ei reaches a value lower or equal to zero. The subset of vertices that have been visited forms the resulting cluster, chj, whose quality is evaluated according to Eq. 7. If CQ(chj) is greater or equal than \u03c4CQ, the cluster is added to the collection of good clusters discovered by the agents, along with the PC m (i) j used by the agent for discovering such a cluster. Since it is likely that a dataset contains more than one cluster of elements which are similar w.r.t. the same PC, it is reasonable to assume that if a PC m (i) j has lead to the identification of a good cluster, it can be further exploited to discover additional good clusters within the same dataset. Then, when a cluster is accepted we restore the initial quantity of energy of the agent, i.e. we set ei = einit and we start a new RW on the same weighted graph, enabling the agent to explore a new unseen region of the graph. For that reason, we set to zero the weights of the matrix Aj associated to the vertices which have been already visited by ai in the previous RW using m (i) j . In this way both \u03c0j and Mj are modified: changing \u03c0j has the effect that the next RW starts from another dense region of the graph, while the modification of Mj prevents the agent from reaching vertices which have already been visited in the past.\nOtherwise if CQ(chj) is not high enough, chj is rejected and the agent\nselects a new PC, say mnew; the energy ei is reset to the default starting value, einit. This implies the recalculation of Anew, Mnew, and hence of \u03c0new, inducing a completely new RW characterized by a possibly different behavior. The new PC mnew is selected by considering a uniform distribution over M. In Sec. 3 we describe a variant of LD-ABCD that implements a different PC selection strategy, which is more suitable for scenarios where the core dissimilarity measure is characterized by many parameters."}, {"heading": "2.6. Aggregation of Clusters/PCs", "text": "As long as the execution of LD-ABCD proceeds, an agent might find very similar (or even equal) clusters using different PCs, in the sense that they may overlap significantly. If an agent identify the same cluster ch using different PCs, ma and mb, we say that such PCs are equivalent w.r.t. ch, in the sense that ch contains patterns that are characterized similarly by considering either ma or mb. This is an important qualitative information that describes the cluster in terms of the parameters of the dissimilarity measure used for discriminating the elements of the cluster from the rest of the dataset. Additionally, showing that the same cluster can be obtained using different PCs underlines their relation within the dataset, allowing further analysis and semantic interpretations of the data at hand.\nIn order to group similar clusters, we merge into a single meta-cluster all such clusters whose intersection, in terms of contained patterns, is sufficiently high. It is therefore necessary to define a dissimilarity measure among clusters: in order to do that, we represented each cluster chj with a Boolean vector, chj \u2208 {0, 1} n, where each entry of the vector represents an index to an element in S, in particular the l-th entry chj(l) = 1 if the l-th pattern of\nS is contained in chj, while chj(l) = 0 otherwise. At this point, the dissimilarity among clusters is computed though the the Hamming distance dH(\u00b7, \u00b7) that evaluates the distance among the two Boolean vectors that represent the clusters. Two clusters c1 and c2 are considered similar if their Hamming distance dH(c1, c2) is less or equal to \u03b8. The parameter \u03b8 \u2265 0 is set proportional to |S| and it can be interpreted as the maximum fraction of patterns on which two clusters can disagree in order to be considered similar.\nWith c\u0302xi we call the x-th meta-cluster associated to the agent ai, that represents a set of clusters Cxi sufficiently similar to each other w.r.t. the Hamming distance. The meta-cluster c\u0302xi is composed of a Boolean vector \u00b5xi, defined as the rounded mean of all the Boolean representations of the clusters in Cxi and a list Lxi that contains all the PCs used for discovering the clusters in Cxi. Each PC in Lxi is associated to a CQ value, which is used to perform a \u201cranking\u201d of the PCs used for discovering the clusters; PCs associated to a meta-cluster are ordered in non-ascending order of CQ value. In our experiments we have discovered that the PCs with higher CQ are the ones which better describe the original clusters in the dataset (see Sec. 4.2 and 4.3).\nEvery time a cluster chj is discovered by an agent ai using a metric m (i) j , it is compared with all the mean Boolean vectors of the K(t) meta-clusters existing at the time t, and it is assigned to the most similar meta-cluster, let say c\u0302xi. Then chj is added to the set Cxi and \u00b5xi is recomputed on such set. Finally, the PC m (i) j is added to Lxi.\nIf no meta-clusters have still been generated, or if the dissimilarity value to the most similar meta-cluster is above a given threshold \u03b8, a new meta-\ncluster c\u0302yi is instantiated starting from chj: in this case \u00b5yi is initialized with chj and the metric m (i) j used for discovering chj is inserted in the list Lyi, which initially will be empty.\nWith C\u0302i we refer to the collection of all the meta-clusters generated by ai which represents the set of all similar clusters that have been generated using different PCs.\nWhen all the agents terminate their procedure of cluster discovery (see the following section), similar meta-clusters generated by different agents are in turn merged together by a centralized unit into a global meta-cluster. In fact, there are no guarantees that different agents do not generate the same meta-cluster. In order to aggregate 2 meta-clusters c\u0302a1 and c\u0302b2 generated by the agents a1 and a2, we check if the hamming distance between \u00b5a1 and \u00b5b2 is below the threshold \u03b8; in that case the clusters are merged in a new metacluster c\u0302new, where Cnew = Ca1 \u222a Cb2, Lnew = La1 \u222a Lb2 and \u00b5new is computed as the rounded mean element in Cnew."}, {"heading": "2.7. Convergence Criterion", "text": "To determine the convergence criterion of LD-ABCD we decided to analyze how the meta-clusters evolve, rather than considering the single clusters returned by the agents. In fact, due to the random nature of the walk, a single cluster returned by an agent may differ by very few elements from the already existing ones, making it hard to decide if it is an effectively new cluster.\nThe agents terminate the search when for a given time period, defined by the integer-valued threshold \u03c4stop, no new meta-clusters are generated and the average cluster quality of the existing meta-clusters does not increase. The\ncluster quality of a meta-cluster is evaluated as the average of the cluster qualities of the single clusters associated to it. In particular, if an agent returns consecutively for \u03c4stop times a cluster which is associated to an already existing meta-cluster and it does not improve its average CQ or if the cluster is rejected because its CQ is too low, the agent stops. In fact, if an agent of the system has already visited the dataset with a sufficiently high number of PCs, it becomes less likely that new informative clusters are going to be discovered. When all the agents reach their convergence criterion, the whole system stops and the results found by each agent are aggregated as described in the previous section. The parameter \u03c4stop can be set by the user; it directly affects the execution time of the algorithm and accordingly the precision of the results.\nNow that all the functionalities of the system have been explained, we present in Fig. 5 a more detailed overall-schema of a single agent behavior over its lifetime."}, {"heading": "2.8. Analysis of Computational Complexity", "text": "In this section we study the time and space complexity of LD-ABCD. For what concerns the space occupancy, the upper-bound consists in storing the weighted adjacency matrix, A, which each agent must use in order to represent the graph. The space required to store the matrix is O(n2), where n = |S|.\nOn the other hand, the time complexity strictly depends on the number of iterations performed by each agent during the random walk. The length of a typical RW is related to the energy e of the agent and on how this quantity is modified (which is affected by the experimental setting of the\nalgorithm and by the intrinsic random nature of the RWs). The energy variation depends also on the nature of the dataset at hand, which makes a precise analysis difficult to perform. In order to give an estimation of the computational time complexity, we assume here that an agent performs in average T different steps during a typical RW.\nThe time complexity can be estimated as the composition of several costs.\nThe operations performed by an agent can be divided in the following categories, which scale with the input data size in different ways:\n\u2022 the PC initialization step, which includes the generation of the PC\ngiven the selected policy and the evaluation of the adjacency and transition matrices and the computation of the graph conductance bounds. Sampling a random PC has a cost that scales linearly with the number of parameters of the dissimilarity measure, and so it can be generally considered negligible w.r.t. the costs depending on dataset size. Building the adjacency and transition matrices has a cost of O(n2 \u00b7 \u03b4), where \u03b4 is the cost of the dissimilarity measure. Evaluating the bounds of the graph conductance, used for evaluating CQ (see Eq. 7), has the same cost of computing the second eigenvalue of the adjacency matrix, which in our study it has been approximated with the power method described in Appendix A. The power method complexity scales as O((n+n2) \u00b7 1 \u01eb \u00b7 log n \u01eb ), where \u01eb is the user-defined parameter defining the precision on the approximation. We refer then to the time required for initialization step with tinit = O(n 2 \u00b7 (\u03b4 + log(n)));\n\u2022 the random walk step, which consists in selecting a new node and\nupdating the energy e of an agent, according to the variation of the conductance of the subgraph visited so far. While the energy updating procedure can be performed in a constant time, selecting the next node in the RW is an operation which involves analyzing all the elements of the row of the adjacency matrix relative to the current node, which scales as O(n). We then define the cost tstep = O(n);\n\u2022 the cluster quality evaluation step consists in the evaluation of the\ncluster conductance, which is an operation that costs O(n2), since all the edges of the (complete) graph must be considered \u2013 see Appendix A. The estimated time required for performing this step is given by teval = O(n 2).\n\u2022 the cluster aggregation step that consists in updating the set of existing\nmeta-clusters with the cluster that has been accepted by the agent. This operation consists in comparing the cluster with all theK(t) metaclusters which have been generated so far at the time t, using the hamming distance. The hamming distance is linear in the number of the elements, which is the size of the dataset n, since each cluster is represented in the vectorial form described in Sec. 2.6. Note that the aggregation procedure occurs only when a cluster is accepted, i.e. when its quality is sufficiently high, so this cost sometimes is equal to zero. We can then define taggr = O(K(t) \u00b7 n).\nTo summarize, the total time ttot required by an agent to evaluate a PC\nmj can be expressed as:\nttot =tinit + T \u00b7 tstep + teval + taggr\n=O(n2 \u00b7 (\u03b4 + log(n))) + T \u00b7 O(n) +O(n2) +O(K(t) \u00b7 n)\nSince the procedure must be repeated each time a new PC is considered, the total time required for executing the whole LD-ABCD system is M \u00b7 ttot, where M is the number of PC evaluated (we remind that the number of agents is fixed in our algorithmic setting)."}, {"heading": "3. LD-ABCD with Exploration\u2013Exploitation Agents", "text": "Since the PC space can be extremely large even for a modest number of parameters of the dissimilarity measure, the technique used for searching PCs described in Sec. 2.5 \u2013 uniform sampling \u2013 could easily become ineffective. In this section, we propose an alternative approach for exploring the PC space. The search method is inspired to the well-known Metropolis-Hastings algorithm [34], often employed in statistical physics. In this variation, the agents operate according to two different policies (strategies, behaviors), which we named exploration and exploitation. An agent that operates according to the exploration strategy is called \u201cexplorer\u201d. The exploration strategy coincides with the uniform search described in Sec. 2.5 and it is meant to perform an exploratory wide-range search in the PC space. An explorer randomly evaluates several different PCs. Every time a RPC is identified by an explorer, it is stored to a shared data structure to allow successive tentative improvements via the exploitation. Accordingly, an agent that implements the exploitation strategy, instead, is called \u201cexploiter\u201d. The objective of the exploiters consists in trying to improve the RPCs found so far by the explorers. An exploiter randomly selects one of the available RPCs, say m (i) j , along with its corresponding cluster chj , and initiates a search in the PC space nearby m (i) j , given a suitable PCs similarity measure dPC(\u00b7, \u00b7). This search strategy is meant to discover other PCs that yield a higher CQ (7) on the same cluster chj. In fact, since it is reasonable to assume that agents with similar PCs are likely to perform similar RWs (and hence accept/reject similar clusters), we keep fixed the cluster structure (i.e., the patterns that it contains) and we just recompute its CQ using the new PCs. The fact that\nwe recompute the CQ of the cluster without issuing a new RW results in a significant improvement in terms of computational resources. The implementation of the similarity measure between PCs depends on the nature of the parameters (e.g., Hamming distance for binary configurations, Euclidean distance for real-valued parameters, etc.). If an exploiter is able to select a new PC m\u0304 (i) j that yields a better CQ than m (i) j , this latter is deleted (along with the related cluster chj) and it is replaced by m\u0304 (i) j and the associated cluster by c\u0304hj.\nEvery agent can exclusively assume the role of the explorer or the exploiter (Fig. 6), modifying hence its search strategy accordingly. Before starting a new RW, an agent checks the current ratio of explorers and exploiters operating in the system. If the ratio is above a user-defined threshold 0 < \u03c4EXPL \u2264 1, and at least one RPC has been already discovered by an explorer, the agent adopts the exploitation policy, otherwise it behaves as an explorer. The factor \u03c4EXPL controls the balance between the diversity and the accuracy of the returned RPCs and can be tuned according to the available computational resources and the particular problem at hand. The exploration\u2013exploitation version of LD-ABCD herein discussed is designed to be able to perform a more targeted search on large PC spaces. This results, in general, in a faster convergence of the whole algorithm, with a faster discovery of the high-quality clusters and related PCs present in the data (we will provide experimental evidence of this claim later in Sec. 4.4). Finally, the herein presented exploration-exploitation variant is characterized by the same computational costs described in Sec. 2.8, as the operations for the explorers and exploiters are asymptotically the same."}, {"heading": "4. Experiments", "text": "In this section we discuss the experiments performed to asses the performances of (both variants of) LD-ABCD. First, in Sec. 4.1 we discuss the tests performed to evaluate the quality of the clusters found by LD-ABCD on some well-known benchmarking datasets. We offer a comparison w.r.t. state-of-the-art graph-based and RW-based algorithms over a particular setting of clustering, where patterns are labeled with ground-true class labels\nfor performance evaluation. Then in Sec. 4.2 we present some experiments which underline the capability of our system to discover relevant information in noisy datasets. Notably, the identification of relevant clusters, together with the PCs used for discovering such clusters, provide a semantic characterization and a high-level description of the data. In Sec. 4.3 we demonstrate the capability of LD-ABCD to discover multiple PCs which characterize individual clusters, defining then a relation among the features considered by each PCs in the data contained in the cluster. Those first three experiments are performed with the LD-ABCD version discussed in Sec. 2. Finally, in Sec. 4.4 we discuss the results obtained by using the exploration\u2013exploitation technique described in Sec. 3 for improving the selection (discovery) of the RPCs.\nAs stressed throughout the paper, our approach is dissimilarity-based. Therefore LD-ABCD is able to process virtually any input data type (e.g., graphs, sequences and so on). However, for the sake of simplicity and for an easier interpretation of the results, we decided to test only datasets of realvalued vectors (features); extensions to other settings are straightforward. The adopted dissimilarity measure is the weighted Euclidean distance; each m (i) j is a vector in [0, 1] D, where D is the dimensionality of the data at hand. We do not use a Mahalanobis-like distance (e.g., by using the full weight matrix), since the former distance allows a more direct interpretation of the results in terms of feature selection (and it is characterized by much less parameters).\nOur algorithm depends on a number of parameters and thresholds, which are \u03c4CQ, \u03c4stop, \u03b2 (used in the definition of \u03c4exp), \u03b8 and \u03c4energy. In our exper-\niments we used different configurations of those parameters, that have been set empirically accordingly to the dataset and to the problem at hand. However, in several cases we kept those parameters unaltered, since modifying their values does not lead to any remarkable changes in the results, making their choice not very critical."}, {"heading": "4.1. Evaluating the Purity of the RWs", "text": "We have processed four different real-world datasets from the UCI Machine Learning Repository [7], which are Wine, Breast Cancer, Iris, and E-Coli. We decided to use the aforementioned datasets since they are very well-known, easy to obtain and for some of them it was possible to provide a comparison with the results obtained by other algorithms which perform clustering using a RW [2]. All datasets contain labeled patterns organized in different classes. As we described in Sec. 2.3, the LD-ABCD algorithm uses the CQ (7) \u2013 a criterion based only on the evaluation of the conductance \u2013 for accepting or rejecting the clusters identified during the RWs. In the following experiments, we demonstrate the reliability of our (unsupervised) cluster acceptance criterion using the supervised information of the class labels. In this test, the PCs are defined as real-valued numbers.\nAs mentioned before, we provide a comparison with the MARW algorithm [2] and two other algorithms therein considered, which are Nibble [45] and Apr.PageRank [3] (in the following denoted as N and APR), relatively to the first two dataset treated (Wine and Breast Cancer). MARW is an agentbased and RW-based clustering algorithm. Agents perform the RW on the same graph together, with the constraint of having a (geodesic) distance of at most l from each other. This corresponds to decreasing the chance that the\nmulti-agent RW \u201cmistakenly\u201d merges two different clusters (low transition probabilities are easily zeroed). To make results comparable, we adopted the same performance measure described in [2] for evaluating the purity of a cluster. The purity is the percentage of vertices visited during the RW that has the same class label of the starting vertex. Let vs be the starting vertex, l(v) the true label value of v, and ch the accepted cluster made of vertices visited during a RW. The cluster purity (CP) is defined as:\nr = |{v|l(v) = l(vs)}|\n|ch| . (10)\nIn LD-ABCD, the starting node, vs, is selected from the SD, \u03c0 (see Sec. 2.2). Hence, vs is selected from a central part of the graph, making its class label a reliable estimation of the class of the cluster to which vs effectively belongs.\nFor each processed dataset, we identify K meta-clusters and their associated collection of equivalent PCs (see Sec. 2.6). From each meta-cluster, we chose the PC that has generated the cluster with the highest CQ and then we check its CP (10). We use the average value of those K CPs as the performance index on the whole dataset (we report the standard deviations). The results obtained by our system are reported in Tab. 1, along with the results found by the other algorithms for what concerns the first two datasets.\nIn addition to this numerical comparison, in the following we briefly discuss the behavior of LD-ABCD in each dataset, in order to provide a more complete overview of its functioning. Since there is no pre-processing on the considered data, we decided to show a principal component analysis (PCA), which we use only for facilitating the comprehension of the following discussion (see Fig. 7).\nWine. In this dataset, LD-ABCD was able to identify three different metaclusters that correctly cover the three classes of the dataset. Each metacluster contains only patterns belonging to a single class and thus the CP associated to the PC with the highest CQ is 1 in every meta-cluster. According to MARW [2], we stopped the RWs as soon as a given number z of different vertices are visited. The value of z is selected proportional to the smallest class in the dataset at hand.\nBreast Cancer. This dataset contains two different classes of patterns which are characterized by a very different distribution, as we can see from the related PCA in Fig. 7. The elements of the first class are very similar and they occupy a compact portion of the space, while the others are spread on a less dense region. On this dataset our algorithm returned only one metacluster containing patterns belonging exclusively to the first class and thus the resulting average CP is 1. If at a first sight the absence in the output of a meta-cluster representing the second class may look as a failure, this behavior is perfectly aligned with the design of LD-ABCD, which tries to identify only the most compact and separated clusters in the dataset. From the point of view of clustering, in this dataset there is only one well-defined cluster (those\nin blue). In fact, every time an agent tries to evaluate a cluster over the red patterns, it systematically rejects those clusters because the related CQ is too low (they are highly conductive).\nIris. For this dataset we have performed two different runs. In the first one, we have kept the threshold \u03c4CQ to the standard value (0.9) used in all other experiments, while in the second run we have lowered it to 0.5, In this way\nwe allowed the algorithm to return more clusters, since the ones with a lower CQ are accepted. In the first test, only one meta-cluster is returned that contains points from the most isolated region (see the PCA in Fig. 7). The CP obtained in this first run is equal to 1.\nIn the second run, instead, three different meta-clusters are returned. The first one contains again elements of the most isolated class and its CP is equal to 1, while the others two meta-clusters represent the two remaining classes and their CP is lower. In fact they are not well-separated and the agent during a random walk switch between elements belonging to these two different classes, decreasing the CP of the resulting clusters. Also the CQ of those two clusters is significantly low since the agent moves freely on a larger portion of the graph, returning then a subgraph characterized by a higher conductance. The CP associated to the PC with the highest CQ of those two clusters is respectively 0.6 and 0.67, making the total CP obtained on the dataset equal to 0.76.\nE-Coli. Notwithstanding the dataset contains 8 different classes, the number of the resulting meta-clusters is 3 and they are mainly populated by patterns belonging to the largest classes of the dataset. In fact, the number of elements in the 5 remaining classes is remarkably lower, and they have been partially aggregated in the clusters representing the 3 principal classes. For this reason, the CP obtained on this dataset is not 1, even if it still maintains a good score: the best PC of the 3 clusters have the following CP: 0.96, 0.93 and 0.84, making the average CP of the whole dataset 0.91."}, {"heading": "4.2. Discovering Relevant PCs", "text": "In this first test, we focus on the problem of finding the PCs that best highlight the local structure of the clusters characterizing the dataset. In particular, we identify a collection of meta-cluster (see Sec. 2.6) associated with the list L of the equivalent PCs that have been used for identifying the aggregated clusters. We order the PCs in L according to their CQ (see Sec. 2.3). Since by definition each cluster of the considered dataset is characterized by its own specific PC, we expect (i) to retrieve the correct PC and (ii) that the PC associated to the highest CQ is the one that better characterizes the cluster.\nIn order to demonstrate the capabilities of LD-ABCD, we have generated a synthetic dataset in R4, which contains 4 different clusters c1, c2, c3, and c4. The vectors forming each cluster are characterized by values drawn from a tensor product of a three-dimensional Gaussian distribution with spherical covariance matrix and a unidimensional uniform distribution \u2013 the uniform distribution plays the role of the noise. For each of the 4 clusters, we select a specific dimension to add the values that come from a uniform distribution. Specifically, referring with x[n] as the n-th component of the vectors of the dataset, we insert the values drawn from the uniform distribution in x[1] relatively to the patterns of c1, in x[2] for the patterns of c2, in x[3] for the patterns of c3, and finally in x[4] for the patterns of c4.\nIn Fig. 8 we show the first three components of the considered patterns, omitting the 4-th component, x[4]. As it is possible to observe from the figure, although the clusters are characterized by a narrow variance on a specific dimension, they are clearly well-separated. While the clusters c1, c2,\nand c3 (plotted with blue dots) have the component containing the noise in one of the three displayed dimensions (respectively on x[1], x[2], and x[3]), c4 (plotted with red dots) has all the components with values drawn from the Gaussian distribution in R3 and the component containing the noise is x[4]. Note that the values of x[4] for the blue clusters are drawn from a Gaussian distribution instead. We execute LD-ABCD using Boolean PCs only (mj are Boolean vectors), until the stop criterion (described in Sec. 2.7) is reached. As expected, LD-ABCD discovered four different meta-clusters c\u0302i, c\u0302ii, c\u0302iii, and c\u0302iv. In Tab. 2 we report the PC with higher CQ found for each meta-cluster and the relative CQ value.\nAs it is possible to observe, the PCs that have been found showing the\nhighest CQ values are those that assign 1 in each cluster in correspondence of the components drawn from the Gaussian distribution (i.e., the signal), and 0 to the component drawn from uniform distribution (i.e., the noise). This demonstrates that LD-ABCD is able to discover the local structure of the relevant clusters in the dataset, identifying also the specific PC that allow such structures to emerge.\nIn our experiment we reported for each cluster the first PC in the list L of equivalent PCs, that is the one with the highest CQ value, and thus the one that better characterizes the cluster. Such PCs are reported in Tab. 2. Notice that for this test the threshold \u03c4CQ can be set to an arbitrarily low value, because we are considering only the first PC (in terms of CQ) in L and ignoring the others."}, {"heading": "4.3. Identification of Equivalent PCs", "text": "In this section we evaluate the capability of LD-ABCD to discover the PCs which can equivalently characterize a portion of the data. In Sec. 2.6 we have introduced the concept of equivalent PCs which are associated to each meta-cluster. Such PCs are collected in the structure L associated to each meta-cluster in C\u0302. Each PC in L is characterized by a specific CQ value: the higher the CQ, the better the PC characterizes the meta-cluster. If a meta-\ncluster is associated with a set of PCs that are characterized by high and similar CQ values, we interpret them as equivalent, in the sense that they can be used interchangeably to suitably identify and characterize locally the cluster. Furthermore, we can identify relations among the parameters w.r.t. the dataset at hand.\nTo show this process and make it easily understandable, we have used a synthetic dataset in [0, 1]4, which contains four different clusters. Each cluster contains data points which are very compact in two dimensions, while having uncorrelated values in the other two dimensions. More precisely, the projection of the cluster on the hyperplane formed by the first two dimensions is normally distributed with narrow variance around the center. This means that the first cluster is defined by the vectors whose first two components are extracted from two Gaussian distributions, GA and GB; the second cluster is formed by vectors whose third and fourth components are drawn from the distributions GE and GF , and so on (see Fig. 9 for an illustration). On the remaining dimensions, the vectors contain values which are drawn from a mixture of different Gaussian distributions (each one belonging to a different cluster) or noise. Since we wanted to keep the data in each cluster sufficiently isolated from the others, we drew the noise values by a random sampling considering a domain obtained by subtracting from [0, 1]4 a suitable neighborhood of all the clusters.\nIn this sense, each cluster can be identified by PCs which assign high weights to any of the two signal components (or both), and a low weight to the others. For example, if we consider Boolean PCs, the cluster which contains vectors of the type [A,B,\u223c,\u223c], where \u223c denotes either a signal different from\nA and B or a noisy component, can be identified by the following equivalent PCs: {1, 1, 0, 0}, {0, 1, 0, 0}, and {1, 0, 0, 0}.\nIn the herein presented experiment, we have generated a dataset of the form described above, which is exemplified in Fig. 9. Such a dataset contains 300 vectors in [0, 1]4, whose components are real values extracted from eight different Gaussian distributions GA, GB, ..., GH or from a uniform distribution. Each Gaussian distribution is paired with another one, in the sense that if a vector contains a value extracted from a distribution, it must also contain a value extracted from a second one. For example, the vectors which\nhave the first component extracted from GA must have the second component extracted from GB, while the remaining two components can contain any other value. In this way we assure a correlation between pairs of Gaussian distributions for each specific cluster. This fact is illustrated in Tab. 3.\nThe mean of each Gaussian component is randomly generated, constrained to be separated by the others by a value greater or equal to 0.2; we used a variance \u03c3 = 0.005. The subspace from which we draw the noisy values using the uniform distribution is defined by setting a radius 0.1 for the neighborhoods of the clusters that we subtract from [0, 1]4.\nThe results obtained by running the system with the cluster quality threshold \u03c4CQ equal to 0.8 are reported in Tab. 4. As it is possible to observe, six different meta-clusters, c\u0302i-vi, have been found and four of them, c\u0302i,ii,v,vi, correspond, respectively, to the expected clusters c2,1,3,4, while the two remaining meta-clusters, c\u0302iii,iv, correspond to high density areas that occurred randomly in the generation of the dataset. As Tab. 4 shows, the meta-clusters c\u0302i,ii,v,vi have associated PCs which select the relevant component of the vectors, according to the way the clusters have been generated."}, {"heading": "4.4. Tests using Exploration\u2013Exploitation Strategy", "text": "Here we evaluate the performance improvement obtained when using the exploration\u2013exploitation strategy presented in Sec. 3 w.r.t. the original PC search of Sec. 2.5. We proceed by testing the two approaches on a highdimensional synthetic dataset. A good estimator of the search efficiency is the mean CQ (MCQ) over all accepted clusters as a function of time (i.e.,\nalgorithm iterations). Of course, after a short initial transient a higher MCQ value, at every given time step, indicates a faster identification of the RPCs. By definition, the MCQ ranges from \u03c4CQ to 1, and the maximum execution time (measured in number of iterations) is a user-defined setting.\nThe generated dataset lies in a 30-dimensional space and it is characterized by ten well-separated clusters. The PC space consists of binary vectors of 30 parameters, so there are 230\u22121 possible PCs (the all zeros configuration is never considered). We defined the Hamming distance as the dissimilarity measure dPC(\u00b7, \u00b7) used for comparing different PCs (see Sec. 3). Given a PC which is returned by an explorer, the exploiters generate similar PCs that have a hamming-distance equal to 1 from the original selected PC. This means that an exploiter randomly switches a parameter of the exploited PC to obtain the new candidate PC to be tested.\nFig. 10 shows a plot of the MCQ obtained by both search methods, the uniform and the exploration\u2013exploitation search, as a function of time. The exploration-exploitation setting has been run with the ratio \u03c4EXPL = 3/4 over a total of 4 agents, i.e., 3 explorers and 1 exploiter. Such results are intended as the average of five different runs considered for each method, executed by changing the random seeds. As it is possible to observe, the MCQ obtained with the exploration\u2013exploitation strategy rapidly assumes higher values w.r.t. those of the uniform search, and this behavior is preserved until convergence.\nPlease note that we are not reporting the results obtained by applying the exploration\u2013exploitation strategy on the experiments described in the previous sections, since there are no significant variations that it is worth to\ndiscuss. In fact, since the dimension of the parameter space was reasonably small (we usually considered less than ten parameters), the basic version which explores the PCs with a uniform search was capable of considering a sufficient number of configurations for identifying the desired solution. We must remark that in the asymptotic regime the results obtained with the two methods are the same, since all the PCs sooner or later will be considered. In this way, the tangible improvement introduced by the exploration\u2013 exploitation method consists in identifying the RPCs sooner, rather than discovering \u201cbetter solutions\u201d that cannot be found by the former technique."}, {"heading": "5. Conclusions", "text": "With this study we presented a dissimilarity-based multi-agent system, LD-ABCD, capable of discovering relevant clusters in a dataset, whose elements are grouped according to different and possibly equivalent configurations (instances of parameter values) of the dissimilarity measure. Agents in\nLD-ABCD perform multiple and independent random walks. Accordingly, each agent discovers and takes decisions independently over one cluster at a time. The multiple parameter configurations highlight the characteristics of patterns within the cluster that are considered to be discriminative, and represent the key for interpreting and characterizing semantically the regularities found in the dataset. As a first step, we represented the entire dataset as a weighted graph. The identified clusters are subgraphs whose quality is evaluated as a function of their conductance normalized w.r.t. to the bounds of the graph conductance. Guiding the evolution of our system with a cluster quality measure based on the conductance allowed us to define a powerful tool for evaluating the effectiveness of a given configuration of the parameters and to identify well-formed clusters, as outlined also by the tests performed on the UCI datasets for classification. We presented two different approaches for searching the parameters characterizing the dissimilarity measure: (i) a basic one which consists in extracting configurations of the dissimilarity function parameters by means of a uniform distribution and (ii) an improved search strategy in which the solutions are further improved by searching in their neighborhood (the exploitation search strategy). In this second strategy, agents are divided into two main families: the explorers and the exploiters. Our work highly relied on the celebrated Cheeger\u2019s inequality as reference to define suitable bounds for the definition of the cluster quality. In this paper, we employed a very fast approach for computing an approximation of the minimum conductance of a graph, which is based on the numerical approximation of eigenvalues using the power method. This solution proved to be very useful and handy in our practical implementation.\nThe discussed experiments showed how LD-ABCD is capable of identifying the characterizing parameters of the dissimilarity measure, locally tailored for each single discovered cluster. Furthermore, when applied on the UCI datasets with a known class structure, the clusters returned by our algorithm contain elements belonging mostly to the same class.\nOur future work will be focused on applying our system for clusters and knowledge discovery to larger datasets. Accordingly, we will focus on the aspects related to scalability and parallelization, showing how our algorithm can work by distributing the computations over different cores and/or distinct workstations, each of which would access a suitable fraction of the entire dataset. In fact, since LD-ABCD does not produce a partition of the data, it could also operate on a suitable subset of the entire dataset only."}, {"heading": "Appendix A Graph Conductance and Related Approximation", "text": "Given a graph G = (V, E), with n = |V|, the conductance of a cut induced\nby the subset S \u2282 V is defined as:\n\u03c6(S) =\n\u2211 u\u2208S \u2211 v\u2208S A(u, v)\nmin(A(S), A(S)) , (11)\nwhere S = V \\ S and A(S) = \u2211\nu,v\u2208S A(u, v) is the number of edges in S. If\nthe graph is weighted, then A(u, v) contains the weight (i.e., the strength) of the edge among u and v; if it is not weighted then A(u, v) is equal to one if and only if there is an edge among u and v. While computing the conductance (11) of any subset S \u2282 V is simple, computing the conductance of the graph \u03a6(G) consists in solving the following NP-Hard problem [15]:\n\u03a6(G) = min S\u2282V \u03c6(S). (12)\nFinding the global optimum is unfeasible even for small graphs. As a consequence, many approximation techniques have been proposed so far [29, 5, 33, 23, 42].\nAmong the many techniques, spectral techniques [15] provide a very powerful approach. Let A be the (weighted) adjacency matrix of G, and let D be diagonal matrix containing the vertex degrees:\nD = diag(d1, .., dn),where di = n\u2211\nj=1\nA(i, j). (13)\nLet us define the transition matrix M as:\nM = D\u22121A. (14)\nThe matrix M is not always symmetric. Therefore, it does not always admit a spectral representation of the form M = U\u039bUT , where \u039b is a diagonal matrix containing the n eigenvalues and U is a matrix containing the corresponding eigenvectors. Notwithstanding, M is conjugate to a symmetric matrix, N, which is defined as follows:\nN = D\u22121/2AD\u22121/2 = D1/2MD\u22121/2. (15)\nM and N have the same eigenvalues and the eigenvectors are linearly\ncorrelated [32, 15]. The eigenvalues of N satisfy the following relation:\n1 = \u03bb1 > \u03bb2 \u2265 ... \u2265 \u03bbn \u2265 \u22121. (16)\nThe celebrated Cheeger inequality [32] establishes an important relation\namong the conductance of G (12) with \u03bb2:\n\u03a6(G)2\n8 \u2264 1\u2212 \u03bb2 \u2264 \u03a6(G), (17)\nwhich can be rewritten as:\n1\u2212 \u03bb2 \u2264 \u03a6(G) \u2264 \u221a 8(1\u2212 \u03bb2). (18)\nBy using the fact that \u03c6(S) \u2265 \u03a6(G) for any S \u2282 V, Eq. 18 can be used as a local reference for a specific graph. According to Eq. 18, it is possible to define the lower and the upper bound of the graph conductance as\nlb(\u03a6(G)) = 1\u2212 \u03bb2, (19)\nub(\u03a6(G)) = \u221a\n8(1\u2212 \u03bb2),\nwhich can be used for evaluating how much the conductance of a cut \u03c6(S) is close to the conductance of the whole graph, \u03a6(G).\nTo make use of the bounds of Eq. 19, we need to compute the \u03bb2 eigenvalue. The QR-decomposition [47] is the most straightforward numerical technique for this purpose, which is however characterized by a cubic computational complexity. To overcome this drawback, we can use the power method described in [47], a fast algorithm that is able to compute in pseudolinear time the largest eigenvalue and related eigenvector of a positive semi definite (PSD) matrix. Notably, the computational complexity of the power method is O((V + |E|)1 \u01eb log |V| \u01eb ), where \u01eb \u2265 0 is the approximation used in computing \u03bb2. Alg. 1 describes the pseudo-code of the power method. The algorithm starts by randomly initializing a vector, x0 \u2208 [\u22121, 1] n; it returns the vector xt = M\u0303 tx0, where M\u0303 is the PSD under analysis. The following theorem is an important result for the convergence of the power method [4, 25].\nTheorem A.1. For every PSD matrix M\u0303, positive integer t, a parameter \u01eb > 0 and a vector x0 randomly picked with uniform probability p in [\u22121, 1] n,\nwith p > 3 16 over the choice of x0, the power method outputs a vector xt such that x \u22ba\ntM\u0303xt x \u22ba txt \u2265 \u03bb1(1\u2212 \u01eb)\n1\n1 + 4n(1\u2212 \u01eb)2t , (20)\nwhere \u03bb1 is the largest eigenvalue.\nThe eigenvector v1 related to \u03bb1 would be approximated by xt\n\u2016xt\u2016 . Given\na PSD matrix M\u0303 and the (unitary) eigenvector v1 related to \u03bb1, we can compute \u03bb2 by means of Alg. 2, which is a variation of Alg. 1. The algorithm (2) returns a vector xt\u22a5v1, such that,\nx \u22ba\ntM\u0303xt x \u22ba txt \u2265 \u03bb2(1\u2212 \u01eb)\n1\n1 + 4n(1\u2212 \u01eb)2t . (21)\nThe power method can only be applied to a PSD matrix, which is not the case of N, whose eigenvalues are the ones in Eq. 16. Consider now the matrixN = N+I. Every eigenvector ofN with eigenvalue \u03bb is clearly also an eigenvector of N with eigenvalue 1+\u03bb and vice-versa, thus N has eigenvalues 2 = 1 + \u03bb1 > 1 + \u03bb2 \u2265 ... \u2265 1 + \u03bbn \u2265 0 and thus it is PSD.\nBy using v1 (an eigenvector of \u03bb1 computed with Alg. 1), and setting t = O(\u01eb\u22121 log n \u01eb ), Alg. 2 will find with probability at least 3/16 a vector xt\u22a51 such that x \u22ba\ntM\u0303xt x \u22ba txt \u2265 \u03bb2 \u2212 4\u01eb. (22)\nFrom Eq. 22, it is possible to derive the approximation of \u03bb2 that in turn\ncan be used in Eq. 19.\nAlgorithm 1 Power method algorithm. Input: PSD matrix M\u0303, tolerance \u01eb Output: Approximation of eigenvector v1 and related eigenvalue \u03bb1\n1: Pick random vector x0 \u2208 {1,\u22121} n with uniform probability; 2: t = \u01eb\u22121 log n \u01eb 3: for i = 1 to t do 4: xi = M\u0303 \u00b7 xi\u22121; 5: xi = xi\n||xi|| ;\n6: end for 7: v1 = xt 8: \u03bb1 = x \u22ba t M\u0303xt\nx \u22ba\nt xt\n9: return v1, \u03bb1;\nAlgorithm 2 Computation of the second eigenvalue. Input: PSD matrix M\u0303, eigenvector v1, and tolerance \u01eb Output: Approximation of \u03bb2\n1: Pick random vector x0 \u2208 {1,\u22121} n with uniform probability; 2: x0 = x0 \u2212 \u3008v1 \u00b7 x0\u3009v1; 3: t = \u01eb\u22121 log n \u01eb 4: for i = 1 to t do 5: xi = M\u0303 \u00b7 xi\u22121; 6: xi = xi\n||xi|| ;\n7: xi = xi \u2212 \u3008v1 \u00b7 xi\u3009v1; 8: end for 9: return \u03bb2 = x \u22ba t M\u0303xt\nx \u22ba\nt \u00b7xt\n\u2212 1;"}, {"heading": "Acknowledgements", "text": "The work presented in this paper has been partially funded by Telecom Italia S.p.a. The authors wish to thank Corrado Moiso, Software System Architect at Telecom Italia \u2013 Future Centre, for the benefits that he provided to the present project trough the valuable comments, ideas and assistance to the writing and the undertaking of the research summarized here."}], "references": [{"title": "Efficient agent-based cluster ensembles", "author": ["A. Agogino", "K. Tumer"], "venue": "Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 1079\u20131086. ACM,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Multi-agent Random Walks for Local Clustering on Graphs", "author": ["M. Alamgir", "U. von Luxburg"], "venue": "IEEE 10th International Conference on Data Mining (ICDM),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Local graph partitioning using pagerank vectors", "author": ["R. Andersen", "F. Chung", "K. Lang"], "venue": "Foundations of Computer Science, 2006. FOCS\u201906. 47th Annual IEEE Symposium on, pages 475\u2013486. IEEE,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Geometry, flows, and graph-partitioning algorithms", "author": ["S. Arora", "S. Rao", "U. Vazirani"], "venue": "Communications of the ACM, 51(10):96\u2013105,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Expander flows, geometric embeddings and graph partitioning", "author": ["S. Arora", "S. Rao", "U. Vazirani"], "venue": "Journal of the ACM (JACM), 56(2):5,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral methods for automatic multiscale data clustering", "author": ["A. Azran", "Z. Ghahramani"], "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, volume 1, pages 190\u2013197. IEEE,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Local descriptors and similarity measures for frontal face recognition: A comparative analysis", "author": ["M. Bereta", "W. Pedrycz", "M. Reformat"], "venue": "Journal of Visual Communication and Image Representation, 24(8):1213\u20131231,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Two Density-based k -means Initialization Algorithms for Non-Metric Data Clustering", "author": ["F.M. Bianchi", "L. Livi", "A. Rizzi"], "venue": "Pattern Analysis and Applications,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "A Game-Theoretic Approach to Hypergraph Clustering", "author": ["S.R. Bul\u00f2", "M. Pelillo"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(6):1312\u20131327,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards information-theoretic K-means clustering for image indexing", "author": ["J. Cao", "Z. Wu", "J. Wu", "W. Liu"], "venue": "Signal Processing, 93(7):2026\u20132037, July", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A framework for Multi-Agent Based Clustering", "author": ["S. Chaimontree", "K. Atkinson", "F. Coenen"], "venue": "Autonomous Agents and Multi-Agent Systems, 25(3):425\u2013446,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Recent trends in Ant Colony Optimization and data clustering: A brief survey", "author": ["U. Chandrasekhar", "P. Naga"], "venue": "2nd International Conference on Intelligent Agent and Multi-Agent Systems (IAMA), pages 32\u201336,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "A boosting approach for supervised mahalanobis distance metric learning", "author": ["C.-C. Chang"], "venue": "Pattern Recognition, 45(2):844\u2013862,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Spectral Graph Theory", "author": ["F. Chung"], "venue": "AMS, June", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "Cluster transitions in a multi-agent clustering model", "author": ["F. De Smet", "D. Aeyels"], "venue": "Proceedings of the 48th IEEE Conference on Decision and Control, 2009 held jointly with the 2009 28th Chinese Control Conference. CDC/CCC 2009., pages 4778\u20134784,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "The dissimilarity space: Bridging structural and statistical pattern recognition", "author": ["R.P.W. Duin", "E. P\u0229kalska"], "venue": "Pattern Recognition Letters, 33(7):826\u2013832,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Graph-Based k-Means Clustering: A Comparison of the Set Median versus the Generalized Median Graph", "author": ["M. Ferrer", "E. Valveny", "F. Serratosa", "I. Bardaj\u0301\u0131", "H. Bunke"], "venue": "In Proceedings of the 13th International Conference on Computer Analysis of Images and Patterns,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "A note on spider walks", "author": ["C. Gallesco", "S. Mueller", "S. Popov"], "venue": "ESAIM: Probability and Statistics, 15:390\u2013401,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Graph based k-means clustering", "author": ["L. Galluccio", "O. Michel", "P. Comon", "A.O. Hero III"], "venue": "Signal Processing, 92(9):1970\u20131984, Sept.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Clustering with a new distance measure based on a dual-rooted tree", "author": ["L. Galluccio", "O. Michel", "P. Comon", "M. Kliger", "A.O. Hero III"], "venue": "Information Sciences, 251(0): 96\u2013113,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-agent systems and distributed data mining", "author": ["C. Giannella", "R. Bhargava", "H. Kargupta"], "venue": "Cooperative Information Agents VIII, pages 1\u201315. Springer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Conductance and congestion in power law graphs", "author": ["C. Gkantsidis", "M. Mihail", "A. Saberi"], "venue": "ACM SIGMETRICS Performance Evaluation Review, volume 31, pages 148\u2013159. ACM,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiple kernel learning algorithms", "author": ["M. G\u00f6nen", "E. Alpayd\u0131n"], "venue": "The Journal of Machine Learning Research, 12:2211\u20132268,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Expander graphs and their applications", "author": ["S. Hoory", "N. Linial", "A. Wigderson"], "venue": "Bulletin of the American Mathematical Society, 43(4):439\u2013561,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Clustering Spatio-Temporal Data: an Augmented Fuzzy C-Means", "author": ["H. Izakian", "W. Pedrycz", "I. Jamal"], "venue": "IEEE Transactions on Fuzzy Systems, 21(5):855\u2013868,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "On clusterings: Good, bad and spectral", "author": ["R. Kannan", "S. Vempala", "A. Vetta"], "venue": "Journal of the ACM (JACM), 51(3):497\u2013515,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "A Combine-Correct-Combine Scheme for Optimizing Dissimilarity-Based Classifiers", "author": ["S.-W. Kim", "R.P.W. Duin"], "venue": "E. Bayro-Corrochano and J.-O. Eklundh, editors, Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, volume 5856 of LNCS, pages 425\u2013432. Springer Berlin Heidelberg,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms", "author": ["T. Leighton", "S. Rao"], "venue": "Journal of ACM, 46(6):787\u2013832, Nov.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimized dissimilarity space embedding for labeled graphs", "author": ["L. Livi", "A. Rizzi", "A. Sadeghian"], "venue": "Information Sciences, 266:47\u201364,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Granular Modeling and Computing Approaches for Intelligent Analysis of Non-Geometric Data", "author": ["L. Livi", "A. Rizzi", "A. Sadeghian"], "venue": "Applied Soft Computing,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Random Walks on Graphs: A Survey", "author": ["L. Lov\u00e1sz"], "venue": "D. Mikl\u00f3s, V. T. S\u00f3s, and T. Sz\u0151nyi, editors, Combinatorics, Paul Erd\u0151s is Eighty, volume 2, pages 353\u2013398. J\u00e1nos Bolyai Mathematical Society, Budapest,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "Fast approximation algorithms for cut-based problems in undirected graphs", "author": ["A. Madry"], "venue": "2010 51st Annual IEEE Symposium on Foundations of Computer Science (FOCS), pages 245\u2013254. IEEE,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "The Journal of Chemical Physics, 21(6):1087\u20131092,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1953}, {"title": "Local discriminative distance metrics ensemble learning", "author": ["Y. Mu", "W. Ding", "D. Tao"], "venue": "Pattern Recognition, 46(8):2337\u20132349,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "A Novel Coordination Strategy for Multi-Agent Control using Overlapping Subnetworks with Application to Power Systems", "author": ["R.R. Negenborn", "G. Hug-Glanzmann", "B. De Schutter", "G. Andersson"], "venue": "J. Mohammadpour and K. M. Grigoriadis, editors, Efficient Modeling and Control of Large-Scale Systems, pages 251\u2013278. Springer, Norwell, Massachusetts,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic fuzzy clustering and its application in motion segmentation", "author": ["T.M. Nguyen", "Q.M.J. Wu"], "venue": "IEEE Transactions on Fuzzy Systems, 21(6):1019\u20131031, Dec", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "A theoretical formalism for analyzing agent-based models", "author": ["M.J. North"], "venue": "Complex Adaptive Systems Modeling, 2(1):3,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Knowledge-based clustering: from data to information granules", "author": ["W. Pedrycz"], "venue": "John Wiley & Sons,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Proximity-Based Clustering: A Search for Structural Consistency in Data With Semantic Blocks of Features", "author": ["W. Pedrycz"], "venue": "IEEE Transactions on Fuzzy Systems, 21 (5):978\u2013982,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonlinear multicriteria clustering based on multiple dissimilarity matrices", "author": ["S. Queiroz", "F. d. A.T. de Carvalho", "Y. Lechevallier"], "venue": "Pattern Recognition,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Estimating pagerank on graph streams", "author": ["A.D. Sarma", "S. Gollapudi", "R. Panigrahy"], "venue": "Journal of the ACM (JACM), 58(3):13,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Data Analysis of (Non-)Metric Proximities at Linear Costs", "author": ["F.-M. Schleif", "A. Gisbrecht"], "venue": "E. Hancock and M. Pelillo, editors, Similarity-Based Pattern Recognition, volume 7953 of Lecture Notes in Computer Science, pages 59\u201374. Springer Berlin Heidelberg,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient dual approach to distance metric learning", "author": ["C. Shen", "J. Kim", "F. Liu", "L. Wang", "A. van den Hengel"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "A local clustering algorithm for massive graphs and its application to nearly-linear time graph partitioning", "author": ["D.A. Spielman", "S.-H. Teng"], "venue": "CoRR, abs/0809.3232,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "Personalized PageRank Clustering: A graph clustering algorithm based on random walks", "author": ["S.A. Tabrizi", "A. Shakery", "M. Asadpour", "M. Abbasi", "M.A. Tavallaie"], "venue": "Physica A: Statistical Mechanics and its Applications, 392(22):5772\u20135785,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Numerical linear algebra, volume 50", "author": ["L.N. Trefethen", "D. Bau III"], "venue": "Siam,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1997}, {"title": "A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval", "author": ["L. Yang", "R. Jin", "L. Mummert", "R. Sukthankar", "A. Goode", "B. Zheng", "S.C.H. Hoi", "M. Satyanarayanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(1):30\u201344, Jan", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Semi-supervised fuzzy clustering with metric learning and entropy regularization", "author": ["X. Yin", "T. Shu", "Q. Huang"], "venue": "Knowledge-Based Systems, 35:304\u2013311,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-supervised distance metric learning based on local linear regression for data clustering", "author": ["H. Zhang", "J. Yu", "M. Wang", "Y. Liu"], "venue": "Neurocomputing, 93:100\u2013105,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2012}, {"title": "An interval weighed fuzzy c-means clustering by genetically guided alternating optimization", "author": ["L. Zhang", "W. Pedrycz", "W. Lu", "X. Liu", "L. Zhang"], "venue": "Expert Systems with Applications, 41(13):5960\u20135971,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 25, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 35, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 9, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 37, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 7, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 24, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 49, "context": "Clustering [27, 37, 11, 39, 9, 26, 51] is a well-established approach that can be used to this end.", "startOffset": 11, "endOffset": 38}, {"referenceID": 8, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 44, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 16, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 18, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 19, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 5, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 36, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 0, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 20, "context": "Among the many solutions available in this field, it is worth citing those clustering techniques based on graph-theoretical results and multi-agent systems [10, 46, 18, 20, 21, 6, 38, 1, 22].", "startOffset": 156, "endOffset": 190}, {"referenceID": 1, "context": "Typical settings involving the interplay of both approaches include random walk (RW) based algorithms [2, 19], in which agents move and interact on the graph via specific (probabilistic) mechanisms.", "startOffset": 102, "endOffset": 109}, {"referenceID": 17, "context": "Typical settings involving the interplay of both approaches include random walk (RW) based algorithms [2, 19], in which agents move and interact on the graph via specific (probabilistic) mechanisms.", "startOffset": 102, "endOffset": 109}, {"referenceID": 28, "context": "fundamental issue is the definition of the dissimilarity among the input patterns [30, 43, 17], since the specific dissimilarity measure adopted by the data mining procedure affects the possibility of discovering meaningful regularities.", "startOffset": 82, "endOffset": 94}, {"referenceID": 41, "context": "fundamental issue is the definition of the dissimilarity among the input patterns [30, 43, 17], since the specific dissimilarity measure adopted by the data mining procedure affects the possibility of discovering meaningful regularities.", "startOffset": 82, "endOffset": 94}, {"referenceID": 15, "context": "fundamental issue is the definition of the dissimilarity among the input patterns [30, 43, 17], since the specific dissimilarity measure adopted by the data mining procedure affects the possibility of discovering meaningful regularities.", "startOffset": 82, "endOffset": 94}, {"referenceID": 29, "context": "Depending on the application at hand, data can be collected and represented relying on several different formalisms [31].", "startOffset": 116, "endOffset": 120}, {"referenceID": 26, "context": "Recently, there is a steady increasing interest in using several, possibly heterogeneous, dissimilarity measures at the same time [28, 40, 41, 8, 24].", "startOffset": 130, "endOffset": 149}, {"referenceID": 38, "context": "Recently, there is a steady increasing interest in using several, possibly heterogeneous, dissimilarity measures at the same time [28, 40, 41, 8, 24].", "startOffset": 130, "endOffset": 149}, {"referenceID": 39, "context": "Recently, there is a steady increasing interest in using several, possibly heterogeneous, dissimilarity measures at the same time [28, 40, 41, 8, 24].", "startOffset": 130, "endOffset": 149}, {"referenceID": 6, "context": "Recently, there is a steady increasing interest in using several, possibly heterogeneous, dissimilarity measures at the same time [28, 40, 41, 8, 24].", "startOffset": 130, "endOffset": 149}, {"referenceID": 22, "context": "Recently, there is a steady increasing interest in using several, possibly heterogeneous, dissimilarity measures at the same time [28, 40, 41, 8, 24].", "startOffset": 130, "endOffset": 149}, {"referenceID": 42, "context": "Metric learning [44, 48, 49, 50, 14] is an important subfield of pattern recognition.", "startOffset": 16, "endOffset": 36}, {"referenceID": 46, "context": "Metric learning [44, 48, 49, 50, 14] is an important subfield of pattern recognition.", "startOffset": 16, "endOffset": 36}, {"referenceID": 47, "context": "Metric learning [44, 48, 49, 50, 14] is an important subfield of pattern recognition.", "startOffset": 16, "endOffset": 36}, {"referenceID": 48, "context": "Metric learning [44, 48, 49, 50, 14] is an important subfield of pattern recognition.", "startOffset": 16, "endOffset": 36}, {"referenceID": 12, "context": "Metric learning [44, 48, 49, 50, 14] is an important subfield of pattern recognition.", "startOffset": 16, "endOffset": 36}, {"referenceID": 33, "context": "For a given dissimilarity measure, it is possible to distinguish two main approaches [35]: those trying to determine a partition of data, and those that focus on searching for isolated clusters surrounded by uncategorized data.", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Local description of data is of particular interest, since it allows to characterize the input data by means of a heterogeneous collection of descriptions [8].", "startOffset": 155, "endOffset": 158}, {"referenceID": 25, "context": "To this end, we heavily exploit the graph conductance concept [27].", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 44, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 16, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 18, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 19, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 36, "context": "In particular LD-ABCD identifies clusters on a dataset that is represented through a labeled graph: graph clustering is a well-known problem and it has been addressed in many other works [10, 46, 18, 20, 21, 38].", "startOffset": 187, "endOffset": 211}, {"referenceID": 5, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 34, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 11, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 10, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 14, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 1, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 36, "context": "Such clusters are discovered by different agents, which operate according to a paradigm inspired by the multi-agent systems that can be found in the literature [6, 36, 13, 12, 16, 2, 38].", "startOffset": 160, "endOffset": 186}, {"referenceID": 1, "context": "Each agent examines the patterns by performing a RW [2, 19] on the graph that represents the dataset and tries to group them in different clusters.", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "Each agent examines the patterns by performing a RW [2, 19] on the graph that represents the dataset and tries to group them in different clusters.", "startOffset": 52, "endOffset": 59}, {"referenceID": 25, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 23, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 45, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 27, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 4, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 31, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 21, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 40, "context": "Once the clusters are identified, they are evaluated using the well-known conductance measurement [27], which is computed using numerical approximation techniques [4, 25, 47, 29, 5, 33, 23, 42].", "startOffset": 163, "endOffset": 193}, {"referenceID": 42, "context": "This procedure is strongly related to the problem of the metric-learning [44, 48, 49, 50, 14], which is the task of determining the optimal parameters of a given metric distance.", "startOffset": 73, "endOffset": 93}, {"referenceID": 46, "context": "This procedure is strongly related to the problem of the metric-learning [44, 48, 49, 50, 14], which is the task of determining the optimal parameters of a given metric distance.", "startOffset": 73, "endOffset": 93}, {"referenceID": 47, "context": "This procedure is strongly related to the problem of the metric-learning [44, 48, 49, 50, 14], which is the task of determining the optimal parameters of a given metric distance.", "startOffset": 73, "endOffset": 93}, {"referenceID": 48, "context": "This procedure is strongly related to the problem of the metric-learning [44, 48, 49, 50, 14], which is the task of determining the optimal parameters of a given metric distance.", "startOffset": 73, "endOffset": 93}, {"referenceID": 12, "context": "This procedure is strongly related to the problem of the metric-learning [44, 48, 49, 50, 14], which is the task of determining the optimal parameters of a given metric distance.", "startOffset": 73, "endOffset": 93}, {"referenceID": 0, "context": "Without loss of generality, we also assume that M = [0, 1], where D is the number of parameters/weights characterizing d(\u00b7, \u00b7;m).", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "The dataset is initially represented as a weighted complete undirected graph, Gj = (V, E , w), where each edge ekl \u2208 E is characterized by a weight, w(ekl;m (i) j ) \u2208 [0, 1], which depends on the dissimilarity d(xk, xl;m (i) j ) evaluated with the specific m (i) j .", "startOffset": 167, "endOffset": 173}, {"referenceID": 30, "context": "Each agent performs a Markovian RW [32] on the graph Gj , visiting a number of vertices (nodes) until a quantity called \u201cenergy\u201d is not depleted.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "To perform a RW on Gj we need to define the so-called transition matrix [32], Mj , which is used by an agent to navigate among the vertices.", "startOffset": 72, "endOffset": 76}, {"referenceID": 30, "context": "Every complete and non bipartite graph has a stationary distribution [32], which can be conveniently defined by exploiting the so-called degree distribution,", "startOffset": 69, "endOffset": 73}, {"referenceID": 25, "context": "A well-established measure used for evaluating the quality of a cluster associated to a subgraph of a larger graph is the conductance [27], \u03c6(chj), which quantifies how well knit is the subgraph internally and how many edges (with their associated weights) connected to vertices outside the cluster are cut.", "startOffset": 134, "endOffset": 138}, {"referenceID": 25, "context": "The exact computation of \u03a6(Gj) is a NP-Hard problem [27], and hence it is not computationally feasible.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "sampled in the parameters space [0, 1]).", "startOffset": 32, "endOffset": 38}, {"referenceID": 32, "context": "The search method is inspired to the well-known Metropolis-Hastings algorithm [34], often employed in statistical physics.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "The adopted dissimilarity measure is the weighted Euclidean distance; each m (i) j is a vector in [0, 1] , where D is the dimensionality of the data at hand.", "startOffset": 98, "endOffset": 104}, {"referenceID": 1, "context": "We decided to use the aforementioned datasets since they are very well-known, easy to obtain and for some of them it was possible to provide a comparison with the results obtained by other algorithms which perform clustering using a RW [2].", "startOffset": 236, "endOffset": 239}, {"referenceID": 1, "context": "As mentioned before, we provide a comparison with the MARW algorithm [2] and two other algorithms therein considered, which are Nibble [45] and Apr.", "startOffset": 69, "endOffset": 72}, {"referenceID": 43, "context": "As mentioned before, we provide a comparison with the MARW algorithm [2] and two other algorithms therein considered, which are Nibble [45] and Apr.", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "PageRank [3] (in the following denoted as N and APR), relatively to the first two dataset treated (Wine and Breast Cancer).", "startOffset": 9, "endOffset": 12}, {"referenceID": 1, "context": "To make results comparable, we adopted the same performance measure described in [2] for evaluating the purity of a cluster.", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "According to MARW [2], we stopped the RWs as soon as a given number z of different vertices are visited.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "Specifically, referring with x[n] as the n-th component of the vectors of the dataset, we insert the values drawn from the uniform distribution in x[1] relatively to the patterns of c1, in x[2] for the patterns of c2, in x[3] for the patterns of c3, and finally in x[4] for the patterns of c4.", "startOffset": 148, "endOffset": 151}, {"referenceID": 1, "context": "Specifically, referring with x[n] as the n-th component of the vectors of the dataset, we insert the values drawn from the uniform distribution in x[1] relatively to the patterns of c1, in x[2] for the patterns of c2, in x[3] for the patterns of c3, and finally in x[4] for the patterns of c4.", "startOffset": 190, "endOffset": 193}, {"referenceID": 2, "context": "Specifically, referring with x[n] as the n-th component of the vectors of the dataset, we insert the values drawn from the uniform distribution in x[1] relatively to the patterns of c1, in x[2] for the patterns of c2, in x[3] for the patterns of c3, and finally in x[4] for the patterns of c4.", "startOffset": 222, "endOffset": 225}, {"referenceID": 3, "context": "Specifically, referring with x[n] as the n-th component of the vectors of the dataset, we insert the values drawn from the uniform distribution in x[1] relatively to the patterns of c1, in x[2] for the patterns of c2, in x[3] for the patterns of c3, and finally in x[4] for the patterns of c4.", "startOffset": 266, "endOffset": 269}, {"referenceID": 3, "context": "8 we show the first three components of the considered patterns, omitting the 4-th component, x[4].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "and c3 (plotted with blue dots) have the component containing the noise in one of the three displayed dimensions (respectively on x[1], x[2], and x[3]), c4 (plotted with red dots) has all the components with values drawn from the Gaussian distribution in R and the component containing the noise is x[4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "and c3 (plotted with blue dots) have the component containing the noise in one of the three displayed dimensions (respectively on x[1], x[2], and x[3]), c4 (plotted with red dots) has all the components with values drawn from the Gaussian distribution in R and the component containing the noise is x[4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "and c3 (plotted with blue dots) have the component containing the noise in one of the three displayed dimensions (respectively on x[1], x[2], and x[3]), c4 (plotted with red dots) has all the components with values drawn from the Gaussian distribution in R and the component containing the noise is x[4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "and c3 (plotted with blue dots) have the component containing the noise in one of the three displayed dimensions (respectively on x[1], x[2], and x[3]), c4 (plotted with red dots) has all the components with values drawn from the Gaussian distribution in R and the component containing the noise is x[4].", "startOffset": 300, "endOffset": 303}, {"referenceID": 3, "context": "Note that the values of x[4] for the blue clusters are drawn from a Gaussian distribution instead.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "Figure 8: Plot of the first three dimensions of the dataset characterized by four clusters in [0, 1].", "startOffset": 94, "endOffset": 100}, {"referenceID": 0, "context": "\u0109i {x[1], x[2], x[3], x[4]} = {0, 1, 1, 1} 0.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "\u0109i {x[1], x[2], x[3], x[4]} = {0, 1, 1, 1} 0.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "\u0109i {x[1], x[2], x[3], x[4]} = {0, 1, 1, 1} 0.", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "\u0109i {x[1], x[2], x[3], x[4]} = {0, 1, 1, 1} 0.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "\u0109ii {x[1], x[2], x[3], x[4]} = {1, 0, 1, 1} 0.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "\u0109ii {x[1], x[2], x[3], x[4]} = {1, 0, 1, 1} 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "\u0109ii {x[1], x[2], x[3], x[4]} = {1, 0, 1, 1} 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "\u0109ii {x[1], x[2], x[3], x[4]} = {1, 0, 1, 1} 0.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "\u0109iii {x[1], x[2], x[3], x[4]} = {1, 1, 0, 1} 0.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "\u0109iii {x[1], x[2], x[3], x[4]} = {1, 1, 0, 1} 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "\u0109iii {x[1], x[2], x[3], x[4]} = {1, 1, 0, 1} 0.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "\u0109iii {x[1], x[2], x[3], x[4]} = {1, 1, 0, 1} 0.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "\u0109iv {x[1], x[2], x[3], x[4]} = {1, 1, 1, 0} 0.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "\u0109iv {x[1], x[2], x[3], x[4]} = {1, 1, 1, 0} 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "\u0109iv {x[1], x[2], x[3], x[4]} = {1, 1, 1, 0} 0.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "\u0109iv {x[1], x[2], x[3], x[4]} = {1, 1, 1, 0} 0.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "To show this process and make it easily understandable, we have used a synthetic dataset in [0, 1], which contains four different clusters.", "startOffset": 92, "endOffset": 98}, {"referenceID": 0, "context": "Since we wanted to keep the data in each cluster sufficiently isolated from the others, we drew the noise values by a random sampling considering a domain obtained by subtracting from [0, 1] a suitable neighborhood of all the clusters.", "startOffset": 184, "endOffset": 190}, {"referenceID": 0, "context": "Figure 9: (Color version online) A snapshot of the dataset of vectors in [0, 1], containing four different clusters.", "startOffset": 73, "endOffset": 79}, {"referenceID": 0, "context": "Such a dataset contains 300 vectors in [0, 1], whose components are real values extracted from eight different Gaussian distributions GA, GB, .", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "1 for the neighborhoods of the clusters that we subtract from [0, 1].", "startOffset": 62, "endOffset": 68}, {"referenceID": 0, "context": "\u0109i c2(100%) [1, 0, 0, 0] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "895 [1, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "895 [1, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "876 [0, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "\u0109ii c1(100%) [1, 0, 0, 0] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "877 [0, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "856 [1, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "856 [1, 1, 0, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "\u0109iii c2(26%) [1, 1, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "\u0109iii c2(26%) [1, 1, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "\u0109iii c2(26%) [1, 1, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "\u0109iii c2(26%) [1, 1, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "902 [1, 0, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "902 [1, 0, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "890 [0, 1, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "890 [0, 1, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "\u0109iv c4(22%) [1, 1, 1, 1] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "\u0109iv c4(22%) [1, 1, 1, 1] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "\u0109iv c4(22%) [1, 1, 1, 1] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "\u0109iv c4(22%) [1, 1, 1, 1] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "882 [1, 1, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "882 [1, 1, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "882 [1, 1, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "853 [0, 1, 0, 1] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "853 [0, 1, 0, 1] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "\u0109v c3(100%) [0, 0, 0, 1] 0.", "startOffset": 12, "endOffset": 24}, {"referenceID": 0, "context": "916 [0, 0, 1, 1] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "916 [0, 0, 1, 1] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "859 [0, 0, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "\u0109vi c4(100%) [0, 0, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "\u0109vi c4(100%) [0, 0, 1, 1] 0.", "startOffset": 13, "endOffset": 25}, {"referenceID": 0, "context": "934 [0, 0, 1, 0] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "925 [0, 0, 0, 1] 0.", "startOffset": 4, "endOffset": 16}, {"referenceID": 13, "context": "While computing the conductance (11) of any subset S \u2282 V is simple, computing the conductance of the graph \u03a6(G) consists in solving the following NP-Hard problem [15]:", "startOffset": 162, "endOffset": 166}, {"referenceID": 27, "context": "As a consequence, many approximation techniques have been proposed so far [29, 5, 33, 23, 42].", "startOffset": 74, "endOffset": 93}, {"referenceID": 4, "context": "As a consequence, many approximation techniques have been proposed so far [29, 5, 33, 23, 42].", "startOffset": 74, "endOffset": 93}, {"referenceID": 31, "context": "As a consequence, many approximation techniques have been proposed so far [29, 5, 33, 23, 42].", "startOffset": 74, "endOffset": 93}, {"referenceID": 21, "context": "As a consequence, many approximation techniques have been proposed so far [29, 5, 33, 23, 42].", "startOffset": 74, "endOffset": 93}, {"referenceID": 40, "context": "As a consequence, many approximation techniques have been proposed so far [29, 5, 33, 23, 42].", "startOffset": 74, "endOffset": 93}, {"referenceID": 13, "context": "Among the many techniques, spectral techniques [15] provide a very powerful approach.", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "M and N have the same eigenvalues and the eigenvectors are linearly correlated [32, 15].", "startOffset": 79, "endOffset": 87}, {"referenceID": 13, "context": "M and N have the same eigenvalues and the eigenvectors are linearly correlated [32, 15].", "startOffset": 79, "endOffset": 87}, {"referenceID": 30, "context": "The celebrated Cheeger inequality [32] establishes an important relation among the conductance of G (12) with \u03bb2: \u03a6(G) 8 \u2264 1\u2212 \u03bb2 \u2264 \u03a6(G), (17)", "startOffset": 34, "endOffset": 38}, {"referenceID": 45, "context": "The QR-decomposition [47] is the most straightforward numerical technique for this purpose, which is however characterized by a cubic computational complexity.", "startOffset": 21, "endOffset": 25}, {"referenceID": 45, "context": "To overcome this drawback, we can use the power method described in [47], a fast algorithm that is able to compute in pseudolinear time the largest eigenvalue and related eigenvector of a positive semi definite (PSD) matrix.", "startOffset": 68, "endOffset": 72}, {"referenceID": 3, "context": "The following theorem is an important result for the convergence of the power method [4, 25].", "startOffset": 85, "endOffset": 92}, {"referenceID": 23, "context": "The following theorem is an important result for the convergence of the power method [4, 25].", "startOffset": 85, "endOffset": 92}], "year": 2014, "abstractText": "We propose a multi-agent algorithm able to automatically discover relevant regularities in a given dataset, determining at the same time the set of configurations of the adopted parametric dissimilarity measure yielding compact and separated clusters. Each agent operates independently by performing a Markovian random walk on a suitable weighted graph representation of the input dataset. Such a weighted graph representation is induced by the specific parameter configuration of the dissimilarity measure adopted by the agent, which searches and takes decisions autonomously for one cluster at a time. Results show that the algorithm is able to discover parameter configurations Corresponding Author Email addresses: filippo.binachi@ryerson.ca (Filippo Maria Bianchi), enrico.maiorino@uniroma1.it (Enrico Maiorino), llivi@scs.ryerson.ca (Lorenzo Livi), antonello.rizzi@uniroma1.it (Antonello Rizzi), asadeghi@ryerson.ca (Alireza Sadeghian) URL: https://sites.google.com/site/lorenzlivi/ (Lorenzo Livi), http://infocom.uniroma1.it/~rizzi/ (Antonello Rizzi), http://www.scs.ryerson.ca/~asadeghi/ (Alireza Sadeghian) Preprint submitted to Information Sciences September 18, 2014 that yield a consistent and interpretable collection of clusters. Moreover, we demonstrate that our algorithm shows comparable performances with other similar state-of-the-art algorithms when facing specific clustering problems.", "creator": "LaTeX with hyperref package"}}}