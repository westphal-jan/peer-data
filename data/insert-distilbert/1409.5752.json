{"id": "1409.5752", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2014", "title": "On the Impact of Multiobjective Scalarizing Functions", "abstract": "extremely recently, there has been a renewed interest in statistical decomposition - based approaches essential for evolutionary multiobjective logic optimization. however, the impact of the choice of the underlying scalarizing approximation function ( s ) is still far from being well understood. in this paper, we investigate the behavior of different scalarizing functions and their parameters. we thereby abstract firstly from computing any specific algorithm and only truly consider the difficulty of the single scalarized problems in terms of solving the computational search ability of a ( bounded 1 + lambda ) - directional ea on biobjective nk - tailed landscapes. secondly, combining the outcomes of independent single - objective runs allows for more general statements on set - based performance measures. finally, we investigate the correlation between the opening angle of the scalarizing function's underlying contour lines and the position of the final solution in the functional objective space. our problem analysis is here of fundamental nature and sheds more much light burden on stating the key characteristics of multiobjective scalarizing functions.", "histories": [["v1", "Fri, 19 Sep 2014 18:41:36 GMT  (237kb)", "http://arxiv.org/abs/1409.5752v1", "appears in Parallel Problem Solving from Nature - PPSN XIII, Ljubljana : Slovenia (2014)"]], "COMMENTS": "appears in Parallel Problem Solving from Nature - PPSN XIII, Ljubljana : Slovenia (2014)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bilel derbel", "dimo brockhoff", "arnaud liefooghe", "s\\'ebastien verel"], "accepted": false, "id": "1409.5752"}, "pdf": {"name": "1409.5752.pdf", "metadata": {"source": "CRF", "title": "On the Impact of Multiobjective Scalarizing Functions", "authors": ["Bilel Derbel", "Dimo Brockhoff", "Arnaud Liefooghe", "S\u00e9bastien Verel"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n57 52\nv1 [\ncs .A\nI] 1\n9 Se\np 20\n14"}, {"heading": "1 Introduction", "text": "Multiobjective optimization problems occur frequently in practice and evolutionary multiobjective optimization (EMO) algorithms have been shown to be well-applicable for them\u2014especially if the problem under study is nonlinear and/or derivatives of the objective functions are not available or meaningless. Besides the broad class of Paretodominance based algorithms such as NSGA-II or SPEA2, a recent interest in the socalled decomposition-based algorithms can be observed. Those decompose the multiobjective problem into a set of single-objective, \u2018scalarized\u2019 optimization problems. Examples of such algorithms include MSOPS [1], MOEA/D [2], and their many variants. We refer to [3] for a recent overview on the topic. The main idea behind those algorithms is to define a set of (desired) search directions in objective space and to specify the scalarizing functions corresponding to these directions. The scalarizing functions can then be solved independently (such as in the case of MSOPS), or in a dependent manner (like in MOEA/D where the recombination and selection operators are allowed to use information from the solutions maintained in neighboring search directions).\nMany different scalarizing functions have been proposed in the literature, see e.g. [4] for an overview. Well-known examples are the weighted sum and the (augmented) weighted Chebychev functions, where the latter has an inherent parameter that controls the shape of the lines of equal function values in objective space. Especially with respect to decomposition-based EMO algorithms, it has been reported that the choice of the scalarizing function and their parameters has an impact on the search process [3]. Moreover, it has been noted that adapting the scalarizing function\u2019s parameters during the search can allow improvement over having a constant set of scalarizing functions [5]. Although several studies on the impact of the scalarizing function have been\n2 conducted in recent years, e.g. [6], to the best of our knowledge, all of them investigate it on a concrete EMO algorithm and on the quality of the resulting solution sets when more than one scalarizing function is optimized (typically as mentioned above, in a dependent manner). Thereby, the focus is not in understanding why those performance differences occur but rather in observing them and trying to improve the global algorithm. However, we believe that it is more important to first understand thoroughly the impact of the choice of the scalarizing function for a single search direction before analyzing more complicated algorithms such as MOEA/D-like approaches with specific neighboring structures, recombination, and selection operators. In this paper, we fundamentally investigate the impact of the choice of the scalarizing functions and their parameters on the search performance, independently of any known EMO algorithm. Instead, we consider one of the most simple single-objective scalarizing search algorithms, i.e., a (1 + \u03bb)-EA with standard bit mutation, as an example of a local search algorithm that optimizes a single scalarizing function, corresponding to a single search direction in the objective space. Experiments are conducted on well-understood bi-objective \u03c1MNK-landscapes.\nMore concretely, we look experimentally at the impact of the parameters of a generalized scalarizing function (which covers the special cases of the weighted sum and augmented Chebychev scalarizing functions) in terms of the position (angle/direction) reached by the final points, as well as their quality with respect to the Chebychev function. We then consider how the opening of the cones that describe the lines of equal scalarizing function values can provide a theoretical explanation for the impact of the final position of the obtained solutions in objective space. We also investigate the resulting set quality in terms of hypervolume and \u03b5-indicator if several scalarizing (1 + \u03bb)EAs are run independently for different search directions in the objective space. Finally, we conclude our findings with a comprehensive discussion of promising research lines."}, {"heading": "2 Scalarizing Functions", "text": "We consider the maximization of two objectives f1, f2 that map search points x \u2208 X to an objective vector f(x) = (f1(x), f2(x)) = (z1, z2) in the so-called objective space f(X). A solution x is called dominated by another solution y if f1(y) \u2265 f1(x), f2(y) \u2265 f2(x), and for at least one i, fi(y) > fi(x) holds. The set of all solutions, not dominated by any other, is called Pareto set and its image Pareto front.\nMany ways of decomposing a multiobjective optimization problem into a (set of) single-objective scalarizing functions exist, including the prominent examples of weighted sum (WS), weighted Chebychev (T), or augmented weighted Chebychev (Saug) [4]. For most of them, theoretical results, especially about which Pareto-optimal solutions are attainable, exist [4,7] but they are typically of too general nature to allow for statements on the actual search performance of (stochastic) optimization algorithms. Instead, we are here not interested in any particular scalarizing function, but rather in understanding which general properties of them influence the search behavior of EMO algorithms. We argue by means of experimental investigations that it is not the actual choice of the scalarizing function or their parameters that makes the difference in terms of performance, but rather the general properties of the resulting lines of equal function values. To this end, we consider the minimization of the following general scalarizing\n3\nfunction that covers the special cases of WS4, T, and Saug functions:\nSgen(z) = \u03b1 \u00b7max {\u03bb1 \u00b7 |z\u03041 \u2212 z1|, \u03bb2 \u00b7 |z\u03042 \u2212 z2|}+ \u03b5 (w1 \u00b7 |z\u03041 \u2212 z1|+ w2 \u00b7 |z\u03042 \u2212 z2|)\nwhere z = (z1, z2) is the objective vector of a feasible solution, z\u0304 = (z\u03041, z\u03042) a utopian point, \u03bb1, \u03bb2, w1, and w2 > 0 scalar weighting coefficients indicating a search direction in objective space, and \u03b1 \u2265 0 and \u03b5 \u2265 0 parameters to be fixed. For more details about the mentioned scalarizing functions and their relationship, we refer to Table 1.\nIn the following, we also consider a case of Sgen that combines WS and T with a single parameter \u03b5: the normalized Snorm(z) = (1\u2212 \u03b5)T(z)+ \u03b5WS(z) where \u03b1 = 1\u2212 \u03b5 and \u03b5 \u2208 [0, 1]. For optimizing in a given search direction (d1, d2) in objective space, we follow [1,8] and set \u03bbi = 1/di.5 In addition, we refer to the direction angle as \u03b4 = arctan(d1/d2). For the case of Snorm, we furthermore choose w1 = cos(\u03b4) and w2 = sin(\u03b4) (thus, w21 + w 2 2 = 1) for the weighted sum part in order to normalize the search directions in objective space uniformly w.r.t. their angles. Though, in many textbooks you can find statements like \u201c\u03b5 has to be chosen small (enough)\u201d, we do not make such an assumption but want to understand which influence \u03b5 has on the finally obtained solutions and how it introduces a trade-off between the Chebychev approach and a weighted sum. For the question of how small \u03b5 should be chosen to find all Paretooptimal solutions in exact biobjective discrete optimization, we refer to [9].\nAs mentioned above, one important property of a scalarizing function turns out to be the shape of its sets of equal function values, which are known for the WS, T, and Saug functions [4]. However, no description of the equi-function-value lines for the general scalarizing function Sgen has been given so far. We think that it is necessary to state those opening angles explicitly in order to gain a deeper intuitive understanding of the above scalarizing approaches and related concepts such as the R2 indicator [8] or more complicated scalarizing algorithms such as MOEA/D [2]. Moreover, it allows us to investigate how a linear combination of weighted sum and Chebychev functions affect the search behavior of decomposition-based algorithms. The following proposition, proven in the accompanying report [10], states these opening angles \u03b8i between the equi-utility lines and the f1-axis, see also Fig. 2 for some examples.\n4 Contrary to the standard literature, our formalization assumes minimization and we therefore have included the utopian point z\u0304 that is typically assumed to be z\u0304 = (0, 0) for minimization. 5 The pathologic cases of directions parallel to the coordinates are left out to increase readability.\n4\nProposition 1. Let z\u0304 be a utopian point, \u03bb1, \u03bb2, w1, and w2 > 0 scalar weighting coefficients, \u03b1 \u2265 0 and \u03b5 \u2265 0, where at least one of the latter two is positive. Then, the polar angles between the equi-utility lines of Sgen and the f1-axis are \u03b81 = arctan(\u2212\n\u03b5w1 \u03b1\u03bb2+\u03b5w2\n) and \u03b82 = \u03c02 + arctan( \u03b5w2 \u03b1\u03bb1+\u03b5w1 )."}, {"heading": "3 Experimental Design", "text": "This section presents the experimental setting allowing us to analyze the scalarizing approaches introduced above on bi-objective \u03c1MNK-landscapes. The family of \u03c1MNKlandscapes constitutes a problem-independent model used for constructing multiobjective multimodal landscapes with objective correlation [11]. A bi-objective \u03c1MNKlandscape aims at maximizing an objective function vector f : {0, 1}n \u2192 [0, 1]2. A correlation parameter \u03c1 defines the degree of conflict between the objectives. We investigate a random instance for each parameter combination given in Table 2.\nWe investigate the two scalarizing functions Snorm and Saug of Table 1 with different parameter settings for the weighting coefficient vector and the \u03b5 parameter, as reported in Table 2. In particular, the WS (resp. T) function corresponds to Snorm with \u03b5 = 1 (resp. \u03b5 = 0). The set of weighting coefficient direction angles \u03b4j with respect to the f1-axis (j \u2208 {1, . . . , 99}) are uniformly defined with equal distances in the angle space. For both functions, we set \u03bb1 = 1/ cos(\u03b4j), and \u03bb2 = 1/ sin(\u03b4j). We recall that for Snorm, wi = 1/\u03bbi, and for Saug, wi = 1. To evaluate the relative and the joint performance of the considered scalarizing functions, we investigate the dynamics and the performance of a randomized local search, a simple (1 + \u03bb)-EA. After initially drawing a random solution, at each iteration, \u03bb offspring solutions are generated by means of an independent bit-flip mutation, where each bit of the parent solution is independently flipped with a rate 1/n. The solution with the best (minimum) scalarizing function value among parent and offspring is chosen for the next iteration. For each configuration, 30 independent executions are performed. Due to space limitations, we shall only show a representative subset of settings allowing us to state our findings. More exhaustive results can be found in [10]."}, {"heading": "4 Single Search Behavior", "text": "This section is devoted to the study of the optimization paths followed by single independent (1 + \u03bb)-EA runs for each direction angle \u03b4 and parameter \u03b5 of a scalarized problem. In particular, we study the final solution sets reached by the (1 + \u03bb)-EA in terms of diversity and convergence and give a sound explanation on how the search behaviour is related to the lines of equal function values of the scalarizing functions."}, {"heading": "4.1 Diversity: Final Angle", "text": "In Fig. 1 (Left), we examine the average angle of the final solution reached by the algorithm with respect to the f1-axis using Snorm. The final angle of solution x is defined as \u03c6(x) = arctan(f2(x)/f1(x)). It informs about the actual direction followed by the search process. We can see that the final solutions are in symmetric positions with respect to direction angle \u03c0/4. This is coherent with the symmetric nature of \u03c1MNKlandscapes [11]. For WS (\u03b5 = 1), every single direction angle infers a different final angle. For T (\u03b5 = 0), the extreme direction angles end up reaching \u2018similar\u2019 regions of the objective space. These regions actually correspond to the lexicographically optimal points of the Pareto front, which is because of the choice of the utopian point that lies beyond them. Without surprise, we can also see that T and WS do not always allow to approach the same parts of the Pareto front when using the same direction angle.\nWhen varying \u03b5 for a fixed \u03b4, the search process is able to span a whole range of positions that are achieved by either T or WS but for variable \u03b4 values. Actually, when considering the direction angle being in the middle (i.e. \u03b4 \u2248 \u03c0/4), the choice of \u03b5 does not substantially impact the search direction\u2014because T and WS do allow to move to similar regions in this case. However, as the direction angle goes away from the middle, the influence of \u03b5 grows significantly; and the search direction is drifting in a whole range of values. This indicates that the choice of \u03b4 is not the only feature that determines the final angle but also the choice of \u03b5 highly matters: For some specific \u03b5-values, the direction angles allow to distribute final angles fairly between the two lexicographically optimal points of the Pareto front\u2014in the sense that each direction angle is inferring a different final angle, just like what we observe for WS. For some other \u03b5-values, however, it may happen that the final angles are similar for two different direction angles. In particular, this is the case for large \u03b5-values in Snorm, for which WS has more impact than T. We remark that equivalent conclusions can be drawn when examining Saug, which we do not detail here due to lack of space.\nThe distribution of final directions is tightly related to the diversity of solutions computed by different independent single (1+\u03bb)-EAs. As it will be discussed later, this is of crucial importance from a multiobjective standpoint, since diversity in the objective space is crucial to approach different parts of the Pareto front.\n6"}, {"heading": "4.2 Convergence: Relative Deviation to Best", "text": "In the following, we examine the impact of the scalarizing function parameters on the performance of the (1 + \u03bb)-EA in terms of convergence to the Pareto front. For that purpose, we compute, for every direction angle \u03b4, the best-found objective vector z\u22c6\u03b4,T corresponding to the best (minimum) fitness value with respect to T, over all experimented parameter combinations and over all simulations we investigated. For both functions Snorm and Saug, we consider the final objective vector z obtained for every direction angle \u03b4 and every \u03b5-value. We then compute the relative deviation of z with respect to z\u22c6\u03b4,T, which we define as follows: \u2206(z) = (T(z)\u2212T(z \u22c6 \u03b4,T))/T(z \u22c6 \u03b4,T). Notice that this relative deviation factor is computed with respect to the T function, which is to be viewed as a reference measure of solution quality. This value actually informs about the performance of the (1 + \u03bb)-EA for a fixed direction angle, but variable \u03b5-values.\nIn Fig. 1 (Middle), we show the average relative deviation to best as a function of direction angles (\u03b4) for different \u03b5-values. To understand the obtained results, one has to keep in mind the results discussed in the previous section concerning the final angles inferred by a given parameter setting. In particular, since WS and T do not infer similar final angles, the final computed solutions lay in different regions of the objective space. Also, for the extreme direction angles, different ranges of \u03b5 imply different final angles. Thus, it is with no surprise that the average relative deviation to best can be substantial in such settings. However, the situation is different when considering direction angles in the middle (\u03b4 \u2248 \u03c0/4). In fact, we observe that for such a configuration, the \u03b5-value does not have a substantial effect on final angles, i.e., final solutions lie in similar regions of the objective space. Hence, one may expect that the search process has also the same performance in terms of average deviation to best. This is actually not the case since we can observe that the value of \u03b5 has a significant impact on the relative deviation for the non-extreme direction angles. To better illustrate this observation, we show, in Fig. 1 (Right), the \u03b5-value providing the minimum average relative deviation to best as a function of every direction angle. We clearly see that the best performances of the (1 + \u03bb)-EA for different direction angles are not obtained with the same \u03b5-value."}, {"heading": "4.3 Understanding the Impact of the Opening Angle", "text": "In this section, we argue that the dynamics of the search process observed previously is rather independent of the scalarizing function under consideration or its parameters. Instead, we show that the search process is guided by the positioning of the lines of equal function values in the objective space\u2014described by the opening angle, i.e., the angle between the line of equal function values and the f1-axis (cf. Proposition 1).\nFig. 2 shows three typical exemplary executions of the (1 + \u03bb)-EA in the objective space for different parameter settings. The typical initial solution maps around the point z = (0.5, 0.5) in the objective space, which is the average objective vector for a random solution of \u03c1MNK-landscapes. The evolution of the current solution can be explained by the combination of two effects. The first one is given by the independent bit-flip mutation operator, that produces more offspring in a particular direction compared to the other ones, due to the underlying characteristics of the \u03c1MNK-landscape under consideration. The second one is given by the lines of equal function values, i.e., the current\nsolution moves perpendicular to the iso-fitness lines, following the gradient direction in the objective space. We can remark that the search process is mainly guided by the lower part of the cones of equal function values when the direction is above the initial solution, and vice versa. When the direction angle \u03b4 is smaller (resp. larger) than \u03c0/4, the dynamics of the search process are better captured by the opening angle \u03b81 (resp. \u03b82), defined between the equi-fitness lines and the f1-axis. Geometrically, the optimal solution with respect to a scalarizing function should correspond to the intersection of one of the \u2018highest\u2019 lines of equal fitness values in the gradient direction and the feasible region of the objective space. Although the above description is mainly intuitive, a more detailed analysis can support this general idea.\nLet us focus on the influence of the opening angle \u03b81 when the direction angle \u03b4 is smaller than \u03c0/4 (similar results hold for \u03b4 > \u03c0/4 and \u03b82). Fig. 3 shows the scatter plots of the final angle \u03c6 as a function of the opening angle \u03b81 for different direction angles \u03b4 \u2208 [0, \u03c0/4]. A scatter plot gives a set of values (\u03b81(\u03b5), \u03c6(\u03b5)) for the \u03b5-values under study. From Proposition 1, for a given direction angle \u03b4, the opening angle \u03b81 belongs to the interval [\u03b4 \u2212 \u03c0/2, 0] for Snorm, and to the interval [\u2212\u03c0/4, 0] for Saug. Independently of the scalarizing function, when the direction angle is between 0 and around 3\u03c0/16 (blue color), the value of \u03c6 is highly correlated with the opening angle \u03b81. For such directions, a simple linear regression confirms this observation and allows us to explain the relation between the opening angle and the final angle by means of the following approximate equation: \u03c6 \u2248 (c + \u03c0/4) + c \u00b7 \u03b81, such that c equals 0.05, 0.2, and 0.4 for \u03c1 = \u22120.7, 0, and 0.7 respectively. We emphasize that this is independent of the definition of the scalarizing function, and depends mainly on the property of the lines of equal function values. The previous equation tells us that the lines of equal fit-\nness values are guiding the search process following the gradient direction given by the opening angle in the objective space. Fig. 3 (Right) shows that the obtained final angles are equivalent when the opening angle is the same, even for different direction angles and/or scalarizing functions. In fact, we observe that the final angles obtained are very similar for the scalarizing functions Snorm and Saug if \u03b4 is the same for both functions and the \u03b5-values are chosen in order to have matching opening angles. Whatever the \u03b4and \u03b5-values, the points are close to the line y = x, which shows that independently of the scalarizing function, the final angle is strongly correlated to the opening angle, and not to a particular scalarizing function. Also, the opening of the lines of equal function values have more impact on the dynamics of the search process than the direction angle alone. In this respect, the opening angle should be considered as a key feature to describe and understand the behavior of scalarizing search algorithms."}, {"heading": "5 Global Search Behavior", "text": "In the previous section, we considered every single (1 + \u03bb)-EA separately. However, the goal of a general-purpose decomposition-based algorithm is to compute a set of solutions approximating the whole Pareto front. In this section, we study the quality of the set obtained when combining the solutions computed by different configurations of the scalarizing functions. A natural way to do so is to use the same \u03b5-value for all direction angles. Fig. 4 illustrates the relative performance, in terms of hypervolume difference and multiplicative epsilon indicators [12], when considering such a setting and aggregating the solutions from the different weight vectors.\nThe hypervolume reference point is set to the origin, and the reference set is the best-known approximation for the instance under consideration.\nOver all the considered \u03c1MNK-landscapes, we found that the \u03b5-values minimizing both indicator-values correspond to those that allow to well distribute the final angles among direction angles (cf. Fig. 1) independently of the considered scalarizing function. Some differences can however be observed depending on the considered indicator, especially for the most correlated instances as illustrated in Fig. 4. To explain the difference of optimal \u03b5-values between both indicators, we remark that the lexicographically optimal regions of the Pareto front approximation have a higher impact on the hypervolume indicator value, due to the setting of the reference point. For instance, for \u03c1 = 0.7, the smallest \u03b5-values concentrate the final angles to the extreme of the Pareto front, which allows to obtain better results in terms of hypervolume. Contrarily, the epsilon indicator values are better when the final angles are well-distributed around \u03c0/4.\nMoreover, WS is found to be in general competitive with respect to other fixed \u03b5-values. This observation might suggest that WS is the best-performing parameter set-\n9\nting, since every different direction angle leads to a different final angle. Nevertheless, the diversity of final angles is not the only criterion that can explain quality. The efficiency of the (1 + \u03bb)-EA with respect to the single-objective problem implied by the scalarizing function is also crucial. In Fig. 1, we observe that the \u03b5-value exhibiting the minimal average deviation to best is not necessarily the same for every direction. We also observe that for direction angles in the middle of the weight space, the final angles obtained for different \u03b5-values can end up being very similar. Thus, it might be possible that, by choosing different \u03b5-values for different directions, one can find a configuration for which final solutions are diverse, but also closest to the Pareto front. Indeed, we can observe a significant difference between the non-uniform case where the scalarizing function Snorm (or Saug) is configured with an \u03b5 providing the best deviation to best for every direction, and the situation where \u03b5 is the same for all directions. As shown in Table 3, such non-uniform configurations are both substantially better than T and also competitive compared to WS. We only show the performance of the above non-uniform configuration in order to illustrate how choosing different \u03b5-values can improve the quality of the resulting approximation set. However, this particular non-uniform configuration might not be \u2018optimal\u2019. In other words, finding the \u2018best\u2019 parameter configuration in a setting where \u00b5 independent single (1 + \u03bb)-EAs are considered, can itself be formulated as an optimization problem with variables \u03b5 and \u03b4; such that direction angles in the optimal configuration might not necessarily be pairwisely different."}, {"heading": "6 Open(ing) (Re)search Lines", "text": "We presented an extensive empirical study that sheds more light on the impact of scalarizing functions within decomposition-based evolutionary multiobjective optimization. Our results showed that, given a weighting coefficient vector and a relative importance of the weighted sum and the Chebychev term in the function, it is fundamentally the opening of the lines of equal function values that explicitly guides the search towards a specific region of the objective space. When combining multiple scalarizing search processes to compute a whole approximation set, these lines play a crucial role to achieve diversity. While our results are with respect to a rather simple setting where multiple scalarizing search procedures are run independently, they make a fundamental step towards strengthening the understanding of the properties and dynamics of more complex algorithmic settings. It is our hope that the lessons learnt from our study can highly serve to better tackle the challenges of decomposition-based approaches. They also rise\n10\nnew interesting issues that were hidden by the complex design of well-established algorithms. In the following, we identify a non-exhaustive number of promising research directions that relate directly to our findings. \u278a Improving existing algorithms. Eliciting the best configuration to tackle a multiobjective optimization problem by decomposition can highly improve search performance. As we demonstrated, similar regions can be achieved using different parameter settings, and the performance could be enhanced by adopting non-uniform configurations. One research direction would be to investigate how such non-uniform configurations perform when plugged into existing approaches. To our best knowledge, there exists no attempt in this direction, and previous investigations did only consider uniform parameters, which do not necessarily guarantee to reach an optimal performance. \u278b Tuning the opening angles. Generally speaking, the parameters of existing scalarizing functions can simply be viewed as one specific tool to set up the openings of the lines of equal function values. In this respect, other types of opening angles can be considered without necessarily using a particular scalarizing function. This would offer more flexibility when tuning decomposition-based algorithms, e.g., defining the opening angles without being bound to a fixed closed-form definition, but adaptively, with respect to the current search state. We believe that classical paradigms for on-line and off-line parameter setting are worth to be investigated to tackle this challenging issue. \u278c Variation operators and problem-specific issues. In our study, we consider the independent bit-flip mutation operator and bi-objective \u03c1MNK-landscapes. In future work, other problem types and search components should be investigated at the aim of gaining in generality\u2014also towards problems with more than two objectives. \u278d Theoretical modeling. A challenging issue is to provide a framework, abstracting from problem-specific issues, and allowing us to reason about decomposition-based approaches in a purely theoretical manner. This would enable us to better harness scalarizing approaches and to derive new methodological tools in order to improve our practice of decomposition-based evolutionary multiobjective optimization approaches."}], "references": [{"title": "Multiple Single Objective Pareto Sampling", "author": ["E.J. Hughes"], "venue": "CEC.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition", "author": ["Q. Zhang", "H. Li"], "venue": "IEEE TEC 11(6)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Generalized Decomposition", "author": ["I. Giagkiozis", "R.C. Purshouse", "P.J. Fleming"], "venue": "EMO.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonlinear Multiobjective Optimization", "author": ["K. Miettinen"], "venue": "Kluwer, Boston, MA, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "Adaptation of scalarizing functions in MOEA/D: An adaptive scalarizing function-based multiobjective evolutionary algorithm", "author": ["H. Ishibuchi", "Y. Sakane", "N. Tsukamoto", "Y. Nojima"], "venue": "EMO.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "A study on the specification of a scalarizing function in MOEA/D for many-objective knapsack problems", "author": ["H. Ishibuchi", "N. Akedo", "Y. Nojima"], "venue": "LION7.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Using trade-off information in decision-making algorithms", "author": ["I. Kaliszewski"], "venue": "Computers & Operations Research 27(2)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "On the Properties of the R2 Indicator", "author": ["D. Brockhoff", "T. Wagner", "H. Trautmann"], "venue": "Genetic and Evolutionary Computation Conference (GECCO 2012).", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "An Augmented Weighted Tchebycheff Method With Adaptively Chosen Parameters for Discrete Bicriteria Optimization Problems", "author": ["K. D\u00e4chert", "J. Gorski", "K. Klamroth"], "venue": "Computers & Operations Research 39(12)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "On the impact of scalarizing functions on evolutionary multiobjective optimization", "author": ["B. Derbel", "D. Brockhoff", "A. Liefooghe", "S. Verel"], "venue": " Research Report RR-8512, INRIA Lille - Nord Europe", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "On the structure of multiobjective combinatorial search space: MNK-landscapes with correlated objectives", "author": ["S. Verel", "A. Liefooghe", "L. Jourdan", "C. Dhaenens"], "venue": "Eur J Oper Res 227(2)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Performance Assessment of Multiobjective Optimizers: An Analysis and Review", "author": ["E. Zitzler", "L. Thiele", "M. Laumanns", "C.M. Fonseca", "V. Grunert da Fonseca"], "venue": "IEEE TEC 7(2)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Examples of such algorithms include MSOPS [1], MOEA/D [2], and their many variants.", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "Examples of such algorithms include MSOPS [1], MOEA/D [2], and their many variants.", "startOffset": 54, "endOffset": 57}, {"referenceID": 2, "context": "We refer to [3] for a recent overview on the topic.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "[4] for an overview.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Especially with respect to decomposition-based EMO algorithms, it has been reported that the choice of the scalarizing function and their parameters has an impact on the search process [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 4, "context": "Moreover, it has been noted that adapting the scalarizing function\u2019s parameters during the search can allow improvement over having a constant set of scalarizing functions [5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 5, "context": "[6], to the best of our knowledge, all of them investigate it on a concrete EMO algorithm and on the quality of the resulting solution sets when more than one scalarizing function is optimized (typically as mentioned above, in a dependent manner).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Many ways of decomposing a multiobjective optimization problem into a (set of) single-objective scalarizing functions exist, including the prominent examples of weighted sum (WS), weighted Chebychev (T), or augmented weighted Chebychev (Saug) [4].", "startOffset": 243, "endOffset": 246}, {"referenceID": 3, "context": "For most of them, theoretical results, especially about which Pareto-optimal solutions are attainable, exist [4,7] but they are typically of too general nature to allow for statements on the actual search performance of (stochastic) optimization algorithms.", "startOffset": 109, "endOffset": 114}, {"referenceID": 6, "context": "For most of them, theoretical results, especially about which Pareto-optimal solutions are attainable, exist [4,7] but they are typically of too general nature to allow for statements on the actual search performance of (stochastic) optimization algorithms.", "startOffset": 109, "endOffset": 114}, {"referenceID": 0, "context": "In the following, we also consider a case of Sgen that combines WS and T with a single parameter \u03b5: the normalized Snorm(z) = (1\u2212 \u03b5)T(z)+ \u03b5WS(z) where \u03b1 = 1\u2212 \u03b5 and \u03b5 \u2208 [0, 1].", "startOffset": 168, "endOffset": 174}, {"referenceID": 0, "context": "For optimizing in a given search direction (d1, d2) in objective space, we follow [1,8] and set \u03bbi = 1/di.", "startOffset": 82, "endOffset": 87}, {"referenceID": 7, "context": "For optimizing in a given search direction (d1, d2) in objective space, we follow [1,8] and set \u03bbi = 1/di.", "startOffset": 82, "endOffset": 87}, {"referenceID": 8, "context": "For the question of how small \u03b5 should be chosen to find all Paretooptimal solutions in exact biobjective discrete optimization, we refer to [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "As mentioned above, one important property of a scalarizing function turns out to be the shape of its sets of equal function values, which are known for the WS, T, and Saug functions [4].", "startOffset": 183, "endOffset": 186}, {"referenceID": 7, "context": "We think that it is necessary to state those opening angles explicitly in order to gain a deeper intuitive understanding of the above scalarizing approaches and related concepts such as the R2 indicator [8] or more complicated scalarizing algorithms such as MOEA/D [2].", "startOffset": 203, "endOffset": 206}, {"referenceID": 1, "context": "We think that it is necessary to state those opening angles explicitly in order to gain a deeper intuitive understanding of the above scalarizing approaches and related concepts such as the R2 indicator [8] or more complicated scalarizing algorithms such as MOEA/D [2].", "startOffset": 265, "endOffset": 268}, {"referenceID": 9, "context": "The following proposition, proven in the accompanying report [10], states these opening angles \u03b8i between the equi-utility lines and the f1-axis, see also Fig.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "9} \u03bb = n \u03b4 = j \u00b7 10 \u00b7 \u03c02 , j \u2208 [[1, 99]] m = 2 bit-flip rate = 1/n Snorm: \u03b5 = l \u00b7 10 ; l \u2208 [[0, 100]] n = 128 stopped after Saug : \u03b5 = l \u00b7 10 ; l \u2208 [[0, 10]]; k \u2208 [[\u22121, 2]] k = 4 n iterations", "startOffset": 32, "endOffset": 39}, {"referenceID": 9, "context": "9} \u03bb = n \u03b4 = j \u00b7 10 \u00b7 \u03c02 , j \u2208 [[1, 99]] m = 2 bit-flip rate = 1/n Snorm: \u03b5 = l \u00b7 10 ; l \u2208 [[0, 100]] n = 128 stopped after Saug : \u03b5 = l \u00b7 10 ; l \u2208 [[0, 10]]; k \u2208 [[\u22121, 2]] k = 4 n iterations", "startOffset": 149, "endOffset": 156}, {"referenceID": 10, "context": "The family of \u03c1MNKlandscapes constitutes a problem-independent model used for constructing multiobjective multimodal landscapes with objective correlation [11].", "startOffset": 155, "endOffset": 159}, {"referenceID": 0, "context": "A bi-objective \u03c1MNKlandscape aims at maximizing an objective function vector f : {0, 1} \u2192 [0, 1].", "startOffset": 90, "endOffset": 96}, {"referenceID": 9, "context": "More exhaustive results can be found in [10].", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "\u03b5 \u2208 [0 ,1 ]", "startOffset": 4, "endOffset": 11}, {"referenceID": 0, "context": "\u03b5 \u2208 [0 ,1 ] 6 7 8 9 10 11", "startOffset": 4, "endOffset": 11}, {"referenceID": 10, "context": "This is coherent with the symmetric nature of \u03c1MNKlandscapes [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "4 illustrates the relative performance, in terms of hypervolume difference and multiplicative epsilon indicators [12], when considering such a setting and aggregating the solutions from the different weight vectors.", "startOffset": 113, "endOffset": 117}], "year": 2014, "abstractText": "Recently, there has been a renewed interest in decomposition-based approaches for evolutionary multiobjective optimization. However, the impact of the choice of the underlying scalarizing function(s) is still far from being well understood. In this paper, we investigate the behavior of different scalarizing functions and their parameters. We thereby abstract firstly from any specific algorithm and only consider the difficulty of the single scalarized problems in terms of the search ability of a (1+\u03bb)-EA on biobjective NK-landscapes. Secondly, combining the outcomes of independent single-objective runs allows for more general statements on set-based performance measures. Finally, we investigate the correlation between the opening angle of the scalarizing function\u2019s underlying contour lines and the position of the final solution in the objective space. Our analysis is of fundamental nature and sheds more light on the key characteristics of multiobjective scalarizing functions.", "creator": "gnuplot 4.6 patchlevel 1"}}}