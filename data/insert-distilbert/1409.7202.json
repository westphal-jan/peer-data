{"id": "1409.7202", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2014", "title": "A Boosting Framework on Grounds of Online Learning", "abstract": "by exploiting the duality balancing between boosting and online learning, we present a boosting framework programme which proves to be extremely powerful thanks to employing the vast knowledge available in the online learning area. using this framework, how we develop various algorithms to address multiple mod practically and theoretically interesting questions including sparse boosting, smooth - distribution boosting, agnostic learning and some complementary generalization to double - base projection online learning algorithms, seen as a by - trade product.", "histories": [["v1", "Thu, 25 Sep 2014 10:02:01 GMT  (21kb)", "https://arxiv.org/abs/1409.7202v1", "Accepted in NIPS 2014"], ["v2", "Mon, 3 Nov 2014 11:05:29 GMT  (21kb)", "http://arxiv.org/abs/1409.7202v2", "Accepted in NIPS 2014"], ["v3", "Sun, 23 Nov 2014 14:46:24 GMT  (23kb)", "http://arxiv.org/abs/1409.7202v3", "Accepted in NIPS 2014"]], "COMMENTS": "Accepted in NIPS 2014", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tofigh naghibi mohamadpoor", "beat pfister"], "accepted": true, "id": "1409.7202"}, "pdf": {"name": "1409.7202.pdf", "metadata": {"source": "CRF", "title": "A Boosting Framework on Grounds of Online Learning", "authors": ["Tofigh Naghibi", "Beat Pfister"], "emails": ["naghibi@tik.ee.ethz.ch,", "pfister@tik.ee.ethz.ch"], "sections": [{"heading": null, "text": "ar X\niv :1\n40 9.\n72 02\nv3 [\ncs .L\nG ]\n2 3\nN ov"}, {"heading": "1 Introduction", "text": "A boosting algorithm can be seen as a meta-algorithm that maintains a distribution over the sample space. At each iteration a weak hypothesis is learned and the distribution is updated, accordingly. The output (strong hypothesis) is a convex combination of the weak hypotheses. Two dominant views to describe and design boosting algorithms are \u201cweak to strong learner\u201d (WTSL), which is the original viewpoint presented in [1, 2], and boosting by \u201ccoordinate-wise gradient descent in the functional space\u201d (CWGD) appearing in later works [3, 4, 5]. A boosting algorithm adhering to the first view guarantees that it only requires a finite number of iterations (equivalently, finite number of weak hypotheses) to learn a (1\u2212 \u01eb)-accurate hypothesis. In contrast, an algorithm resulting from the CWGD viewpoint (usually called potential booster) may not necessarily be a boosting algorithm in the probability approximately correct (PAC) learning sense. However, while it is rather difficult to construct a boosting algorithm based on the first view, the algorithmic frameworks, e.g., AnyBoost [4], resulting from the second viewpoint have proven to be particularly prolific when it comes to developing new boosting algorithms. Under the CWGD view, the choice of the convex loss function to be minimized is (arguably) the cornerstone of designing a boosting algorithm. This, however, is a severe disadvantage in some applications.\nIn CWGD, the weights are not directly controllable (designable) and are only viewed as the values of the gradient of the loss function. In many applications, some characteristics of the desired distribution are known or given as problem requirements while, finding a loss function that generates such a distribution is likely to be difficult. For instance, what loss functions can generate sparse distributions?2 What family of loss functions results in a smooth distribution?3 We even can go further and imagine the scenarios in which a loss function needs to put more weights on a given subset of examples than others, either because that subset has more reliable labels or it is a prob-\n1Boosting algorithms in this paper can be found in \u2018maboost\u2019 R package 2In the boosting terminology, sparsity usually refers to the greedy hypothesis-selection strategy of boosting methods in the functional space. However, sparsity in this paper refers to the sparsity of the distribution (weights) over the sample space.\n3A smooth distribution is a distribution that does not put too much weight on any single sample or in other words, a distribution emulated by the booster does not dramatically diverge from the target distribution [6, 7].\nlem requirement to have a more accurate hypothesis for that part of the sample space. Then, what loss function can generate such a customized distribution? Moreover, does it result in a provable boosting algorithm? In general, how can we characterize the accuracy of the final hypothesis?\nAlthough, to be fair, the so-called loss function hunting approach has given rise to useful boosting algorithms such as LogitBoost, FilterBoost, GiniBoost and MadaBoost [5, 8, 9, 10] which (to some extent) answer some of the above questions, it is an inflexible and relatively unsuccessful approach to addressing the boosting problems with distribution constraints.\nAnother approach to designing a boosting algorithm is to directly follow the WTSL viewpoint [11, 6, 12]. The immediate advantages of such an approach are, first, the resultant algorithms are provable boosting algorithms, i.e., they output a hypothesis of arbitrary accuracy. Second, the booster has direct control over the weights, making it more suitable for boosting problems subject to some distribution constraints. However, since the WTSL view does not offer any algorithmic framework (as opposed to the CWGD view), it is rather difficult to come up with a distribution update mechanism resulting in a provable boosting algorithm. There are, however, a few useful, and albeit fairly limited, algorithmic frameworks such as TotalBoost [13] that can be used to derive other provable boosting algorithms. The TotalBoost algorithm can maximize the margin by iteratively solving a convex problem with the totally corrective constraint. A more general family of boosting algorithms was later proposed by Shalev-Shwartz et. al. [15], where it was shown that weak learnability and linear separability are equivalent, a result following from von Neumann\u2019s minmax theorem. Using this theorem, they constructed a family of algorithms that maintain smooth distributions over the sample space, and consequently are noise tolerant. Their proposed algorithms find an (1\u2212 \u01eb)-accurate solution after performing at most O(log(N)/\u01eb2) iterations, where N is the number of training examples."}, {"heading": "1.1 Our Results", "text": "We present a family of boosting algorithms that can be derived from well-known online learning algorithms, including projected gradient descent [16] and its generalization, mirror descent (both active and lazy updates, see [17]) and composite objective mirror descent (COMID) [18]. We prove the PAC learnability of the algorithms derived from this framework and we show that this framework in fact generates maximum margin algorithms. That is, given a desired accuracy level \u03bd, it outputs a hypothesis of margin \u03b3min \u2212 \u03bd with \u03b3min being the minimum edge that the weak classifier guarantees to return.\nThe duality between (linear) online learning and boosting is by no means new. This duality was first pointed out in [2] and was later elaborated and formalized by using the von Neumann\u2019s minmax theorem [19]. Following this line, we provide several proof techniques required to show the PAC learnability of the derived boosting algorithms. These techniques are fairly versatile and can be used to translate many other online learning methods into our boosting framework. To motivate our boosting framework, we derive two practically and theoretically interesting algorithms: (I) SparseBoost algorithm which by maintaining a sparse distribution over the sample space tries to reduce the space and the computation complexity. In fact this problem, i.e., applying batch boosting on the successive subsets of data when there is not sufficient memory to store an entire dataset, was first discussed by Breiman in [20], though no algorithm with theoretical guarantee was suggested. SparseBoost is the first provable batch booster that can (partially) address this problem. By analyzing this algorithm, we show that the tuning parameter of the regularization term \u21131 at each round t should not exceed \u03b3t 2 \u03b7t to still have a boosting algorithm, where \u03b7t is the coefficient of the tth weak hypothesis and \u03b3t is its edge. (II) A smooth boosting algorithm that requires only O(log 1/\u01eb) number of rounds to learn a (1\u2212 \u01eb)-accurate hypothesis. This algorithm can also be seen as an agnostic boosting algorithm4 due to the fact that smooth distributions provide a theoretical guarantee for noise tolerance in various noisy learning settings, such as agnostic boosting [22, 23].\nFurthermore, we provide an interesting theoretical result about MadaBoost [10]. We give a proof (to the best of our knowledge the only available unconditional proof) for the boosting property of (a variant of) MadaBoost and show that, unlike the common presumption, its convergence rate is of O(1/\u01eb2) rather than O(1/\u01eb).\n4Unlike the PAC model, the agnostic learning model allows an arbitrary target function (labeling function) that may not belong to the class studied, and hence, can be viewed as a noise tolerant learning model [21].\nFinally, we show our proof technique can be employed to generalize some of the known online learning algorithms. Specifically, consider the Lazy update variant of the online Mirror Descent (LMD) algorithm (see for instance [17]). The standard proof to show that the LMD update scheme achieves vanishing regret bound is through showing its equivalence to the FTRL algorithm [17] in the case that they are both linearized, i.e., the cost function is linear. However, this indirect proof is fairly restrictive when it comes to generalizing the LMD-type algorithms. Here, we present a direct proof for it, which can be easily adopted to generalize the LMD-type algorithms."}, {"heading": "2 Preliminaries", "text": "Let {(xi, ai)}, 1 \u2264 i \u2264 N , be N training samples, where xi \u2208 X and ai \u2208 {\u22121,+1}. Assume h \u2208 H is a real-valued function mapping X into [\u22121, 1]. Denote a distribution over the training data by w = [w1, . . . , wN ]\u22a4 and define a loss vector d = [\u2212a1h(x1), . . . ,\u2212aNh(xN )]\u22a4. We define \u03b3 = \u2212w\u22a4d as the edge of the hypothesis h under the distribution w and it is assumed to be positive when h is returned by a weak learner. In this paper we do not consider the branching program based boosters and adhere to the typical boosting protocol (described in Section 1).\nSince a central notion throughout this paper is that of Bregman divergences, we briefly revisit some of their properties. A Bregman divergence is defined with respect to a convex function R as\nBR(x,y)= R(x) \u2212R(y) \u2212\u2207R(y)(x \u2212 y) \u22a4 (1)\nand can be interpreted as a distance measure between x and y. Due to the convexity of R, a Bregman divergence is always non-negative, i.e., BR(x,y) \u2265 0. In this work we consider R to be a \u03b2-strongly convex function5 with respect to a norm ||.||. With this choice of R, the Bregman divergence BR(x,y) \u2265 \u03b2 2 ||x\u2212 y||2. As an example, if R(x) = 1 2 x\u22a4x (which is 1-strongly convex with respect to ||.||2), then BR(x,y) = 12 ||x \u2212 y|| 2 2 is the Euclidean distance. Another example is the negative entropy function R(x) = \u2211N\ni=1 xi log xi (resulting in the KL-divergence) which is known to be 1-strongly convex over the probability simplex with respect to \u21131 norm.\nThe Bregman projection is another fundamental concept of our framework.\nDefinition 1 (Bregman Projection). The Bregman projection of a vector y onto a convex set S with respect to a Bregman divergence BR is\n\u03a0S(y) = argmin x\u2208S BR(x,y) (2)\nMoreover, the following generalized Pythagorean theorem holds for Bregman projections.\nLemma 1 (Generalized Pythagorean) [24, Lemma 11.3]. Given a point y \u2208 RN , a convex set S and y\u0302= \u03a0S(y) as the Bregman projection of y onto S, for all x \u2208 S we have\nExact: BR(x,y) \u2265 BR(x, y\u0302) +BR(y\u0302,y) (3)\nRelaxed: BR(x,y) \u2265 BR(x, y\u0302) (4)\nThe relaxed version follows from the fact that BR(y\u0302,y)\u22650 and thus can be ignored.\nLemma 2. For any vectors x,y, z, we have\n(x\u2212 y)\u22a4(\u2207R(z) \u2212\u2207R(y)) = BR(x,y) \u2212BR(x, z) +BR(y, z) (5)\nThe above lemma follows directly from the Bregman divergence definition in (1). Additionally, the following definitions from convex analysis are useful throughout the paper.\nDefinition 2 (Norm & dual norm). Let ||.||A be a norm. Then its dual norm is defined as\n||y||A\u2217 = sup{y \u22a4x, ||x||A \u2264 1} (6)\nFor instance, the dual norm of ||.||2 = \u21132 is ||.||2\u2217 = \u21132 norm and the dual norm of \u21131 is \u2113\u221e norm. Further,\nLemma 3. For any vectors x,y and any norm ||.||A, the following inequality holds:\nx\u22a4y \u2264 ||x||A||y||A\u2217 \u2264 1\n2 ||x||2A +\n1 2 ||y||2A\u2217 (7)\n5That is, its second derivative (Hessian in higher dimensions) is bounded away from zero by at least \u03b2.\nThroughout this paper, we use the shorthands ||.||A = ||.|| and ||.||A\u2217 = ||.||\u2217 for the norm and its dual, respectively.\nFinally, before continuing, we establish our notations. Vectors are lower case bold letters and their entries are non-bold letters with subscripts, such as xi of x, or non-bold letter with superscripts if the vector already has a subscript, such as xit of xt. Moreover, an N-dimensional probability simplex is denoted by S = {w|\n\u2211N i=1 wi = 1, wi \u2265 0}. The proofs of the theorems and the lemmas can be\nfound in the Supplement."}, {"heading": "3 Boosting Framework", "text": "Let R(x) be a 1-strongly convex function with respect to a norm ||.|| and denote its associated Bregman divergence BR. Moreover, let the dual norm of a loss vector dt be upper bounded, i.e., ||dt||\u2217 \u2264 L. It is easy to verify that for dt as defined in MABoost, L = 1 when ||.||\u2217 = \u2113\u221e and L = N when ||.||\u2217 = \u21132. The following Mirror Ascent Boosting (MABoost) algorithm is our boosting framework.\nAlgorithm 1: Mirror Ascent Boosting (MABoost)\nInput: R(x) 1-strongly convex function, w1 = [ 1N , . . . , 1 N ]\u22a4 and z1 = [ 1N , . . . , 1 N ]\u22a4\nFor t = 1, . . . , T do (a) Train classifier with wt and get ht, let dt = [\u2212a1ht(x1), . . . ,\u2212aNht(xN )]\nand \u03b3t = \u2212w\u22a4t dt.\n(b) Set \u03b7t = \u03b3t L\n(c) Update weights: \u2207R(zt+1) = \u2207R(zt) + \u03b7tdt (lazy update) \u2207R(zt+1) = \u2207R(wt) + \u03b7tdt (active update)\n(d) Project onto S: wt+1 = argmin w\u2208S BR(w, zt+1)\nEnd Output: The final hypothesis f(x)= sign (\n\u2211T t=1 \u03b7tht(x)\n)\n.\nThis algorithm is a variant of the mirror descent algorithm [17], modified to work as a boosting algorithm. The basic principle in this algorithm is quite clear. As in ADABoost, the weight of a wrongly (correctly) classified sample increases (decreases). The weight vector is then projected onto the probability simplex in order to keep the weight sum equal to 1. The distinction between the active and lazy update versions and the fact that the algorithm may behave quite differently under different update strategies should be emphasized. In the lazy update version, the norm of the auxiliary variable zt is unbounded which makes the lazy update inappropriate in some situations. In the active update version, on the other hand, the algorithm always needs to access (compute) the previous projected weight wt to update the weight at round t and this may not be possible in some applications (such as boosting-by-filtering).\nDue to the duality between online learning and boosting, it is not surprising that MABoost (both the active and lazy versions) is a boosting algorithm. The proof of its boosting property, however, reveals some interesting properties which enables us to generalize the MABoost framework. In the following, only the proof of the active update is given and the lazy update is left to Section 3.4.\nTheorem 1. Suppose that MABoost generates weak hypotheses h1, . . . , hT whose edges are \u03b31, . . . , \u03b3T . Then the error \u01eb of the combined hypothesis f on the training set is bounded as:\nR(w) = 1\n2 ||w||22 : \u01eb \u2264\n1\n1 + \u2211T\nt=1 \u03b3 2 t\n(8)\nR(w)=\nN \u2211\ni=1\nwi logwi : \u01eb \u2264 e \u2212\n\u2211 T\nt=1 1 2 \u03b32 t (9)\nIn fact, the first bound (8) holds for any 1-strongly convex R, though for some R (e.g., negative entropy) a much tighter bound as in (9) can be achieved.\nProof : Assume w\u2217 = [w\u22171 , . . . , w \u2217 N ] \u22a4 is a distribution vector where w\u2217i = 1 N\u01eb\nif f(xi) 6= ai, and 0 otherwise. w\u2217 can be seen as a uniform distribution over the wrongly classified samples by the ensemble hypothesis f . Using this vector and following the approach in [17], we derive the upper bound of\n\u2211T t=1 \u03b7t(w \u2217\u22a4dt\u2212w\u22a4t dt) where dt = [d 1 t , . . . ,d N t ] is a loss vector as defined in\nAlgorithm 1.\n(w\u2217\u2212wt) \u22a4\u03b7tdt= (w \u2217 \u2212wt) \u22a4(\u2207R(zt+1)\u2212\u2207R(wt) )\n(10a)\n= BR(w \u2217,wt)\u2212BR(w \u2217, zt+1) +BR(wt, zt+1) (10b) \u2264 BR(w \u2217,wt)\u2212BR(w\n\u2217,wt+1) +BR(wt, zt+1) (10c) where the first equation follows Lemma 2 and inequality (10c) results from the relaxed version of Lemma 1. Note that Lemma 1 can be applied here because w\u2217\u2208 S.\nFurther, the BR(wt, zt+1) term is bounded. By applying Lemma 3\nBR(wt, zt+1) +BR(zt+1,wt) = (zt+1 \u2212wt) \u22a4\u03b7tdt \u2264\n1 2 ||zt+1 \u2212wt|| 2 + 1 2 \u03b72t ||dt|| 2 \u2217 (11)\nand since BR(zt+1,wt) \u2265 12 ||zt+1 \u2212wt|| 2 due to the 1-strongly convexity of R, we have\nBR(wt, zt+1) \u2264 1\n2 \u03b72t ||dt|| 2 \u2217 (12)\nNow, replacing (12) into (10c) and summing it up from t = 1 to T , yields T \u2211\nt=1\nw\u2217\u22a4\u03b7tdt\u2212w \u22a4 t \u03b7tdt \u2264\nT \u2211\nt=1\n1 2 \u03b72t ||dt|| 2 \u2217 +BR(w \u2217,w1)\u2212BR(w \u2217,wT+1) (13)\nMoreover, it is evident from the algorithm description that for mistakenly classified samples\n\u2212aif(xi)= \u2212aisign\n( T \u2211\nt=1\n\u03b7tht(xi)\n)\n= sign\n( T \u2211\nt=1\n\u03b7td i t\n)\n\u2265 0 \u2200xi \u2208 {x|f(xi) 6= ai} (14)\nFollowing (14), the first term in (13) will be w\u2217\u22a4 \u2211T\nt=1 \u03b7tdt \u2265 0 and thus, can be ignored. Moreover, by the definition of \u03b3, the second term is\n\u2211T t=1 \u2212w \u22a4 t \u03b7tdt = \u2211T t=1 \u03b7t\u03b3t. Putting all these\ntogether, ignoring the last term in (13) and replacing ||dt||2\u2217 with its upper bound L, yields\n\u2212BR(w \u2217,w1) \u2264 L\nT \u2211\nt=1\n1 2 \u03b72t \u2212\nT \u2211\nt=1\n\u03b7t\u03b3t (15)\nReplacing the left side with \u2212BR = \u2212 12 ||w \u2217\u2212w1||2 = \u01eb\u22121 2N\u01eb\nfor the case of quadratic R, and with \u2212BR = log(\u01eb) when R is a negative entropy function, taking the derivative w.r.t \u03b7t and equating it to zero (which yields \u03b7t = \u03b3t L\n) we achieve the error bounds in (8) and (9). Note that in the case of R being the negative entropy function, Algorithm 1 degenerates into ADABoost with a different choice of \u03b7t.\nBefore continuing our discussion, it is important to mention that the cornerstone concept of the proof is the choice of w\u2217. For instance, a different choice of w\u2217 results in the following max-margin theorem.\nTheorem 2. Setting \u03b7t = \u03b3t L \u221a t , MABoost outputs a hypothesis of margin at least \u03b3min \u2212 \u03bd, where \u03bd is a desired accuracy level and tends to zero in O( log T\u221a T ) rounds of boosting.\nObservations: Two observations follow immediately from the proof of Theorem 1. First, the requirement of using Lemma 1 is w\u2217 \u2208 S, so in the case of projecting onto a smaller convex set Sk\u2286S, as long as w\u2217\u2208Sk holds, the proof is intact. Second, only the relaxed version of Lemma 1 is required in the proof (to obtain inequality (10c)). Hence, if there is an approximate projection operator \u03a0\u0302S that satisfies the inequality BR(w\u2217, zt+1) \u2265 BR ( w\u2217, \u03a0\u0302S(zt+1) )\n, it can be substituted for the exact projection operator \u03a0S and the active update version of the algorithm still works. A practical approximate operator of this type can be obtained through a double-projection strategy.\nLemma 4. Consider the convex sets K and S, where S \u2286 K. Then for any x \u2208 S and y \u2208 RN , \u03a0\u0302S(y)=\u03a0S ( \u03a0K(y) ) is an approximate projection that satisfies BR(x,y)\u2265BR ( x, \u03a0\u0302S(y) ) .\nThese observations are employed to generalize Algorithm 1. However, we want to emphasis that the approximate Bregman projection is only valid for the active update version of MABoost."}, {"heading": "3.1 Smooth Boosting", "text": "Let k > 0 be a smoothness parameter. A distribution w is smooth w.r.t a given distribution D if wi \u2264 kDi for all 1\u2264 i\u2264 N . Here, we consider the smoothness w.r.t to the uniform distribution, i.e., Di = 1N . Then, given a desired smoothness parameter k, we require a boosting algorithm that only constructs distributions w such that wi \u2264 kN , while guaranteeing to output a (1\u2212 1 k )- accurate hypothesis. To this end, we only need to replace the probability simplex S with Sk = {w|\n\u2211N i=1 wi = 1, 0\u2264 wi \u2264 k N } in MABoost to obtain a smooth distribution boosting algorithm,\ncalled smooth-MABoost. That is, the update rule is: wt+1 = argmin w\u2208Sk BR(w, zt+1).\nNote that the proof of Theorem 1 holds for smooth-MABoost, as well. As long as \u01eb\u2265 1 k , the error distribution w\u2217 (w\u2217i = 1 N\u01eb if f(xi) 6= ai, and 0 otherwise) is in Sk because 1N\u01eb \u2264 k N\n. Thus, based on the first observation, the error bounds achieved in Theorem 1 hold for \u01eb\u2265 1\nk . In particular, \u01eb= 1\nk is reached after a finite number of iterations. This projection problem has already appeared in the literature. An entropic projection algorithm (R is negative entropy), for instance, was proposed in [15]. Using negative entropy and their suggested projection algorithm results in a fast smooth boosting algorithm with the following convergence rate.\nTheorem 3. Given R(w) = \u2211N\ni=1 wi logwi and a desired \u01eb, smooth-MABoost finds a (1 \u2212 \u01eb)accurate hypothesis in O(log(1\n\u01eb )/\u03b32) of iterations."}, {"heading": "3.2 Combining Datasets", "text": "Let\u2019s assume we have two sets of data. A primary dataset A and a secondary dataset B. The goal is to train a classifier that achieves (1\u2212 \u01eb) accuracy on A while limiting the error on dataset B to \u01ebB \u2264 1 k\n. This scenario has many potential applications including transfer learning [25], weighted combination of datasets based on their noise level and emphasizing on a particular region of a sample space as a problem requirement (e.g., a medical diagnostic test that should not make a wrong diagnosis when the sample is a pregnant woman). To address this problem, we only need to replace S in MABoost with Sc= {w| \u2211N i=1 wi= 1, 0\u2264 wi \u2200i \u2208 A \u2227 0\u2264 wi\u2264 k N\n\u2200i \u2208 B} where i \u2208 A shorthands the indices of samples in A. By generating smooth distributions on B, this algorithm limits the weight of the secondary dataset, which intuitively results in limiting its effect on the final hypothesis. The proof of its boosting property is quite similar to Theorem 1 (see supplement)."}, {"heading": "3.3 Sparse Boosting", "text": "Let R(w)= 1 2 ||w||22. Since in this case the projection onto the simplex is in fact an \u21131-constrained optimization problem, it is plausible that some of the weights are zero (sparse distribution), which is already a useful observation. To promote the sparsity of the weight vector, we want to directly regularize the projection with the \u21131 norm, i.e., adding ||w||1 to the objective function in the projection step. It is, however, not possible in MABoost, since ||w||1 is trivially constant on the simplex. Therefore, we split the projection step into two consecutive steps. The first projection is onto R+={y | 0\u2264 yi}.\nSurprisingly, projection onto R+ implicitly regularizes the weights of the correctly classified samples with a weighted \u21131 norm term (see supplement). To further enhance sparsity, we may introduce an explicit \u21131 norm regularization term into the projection step with a regularization factor denoted by \u03b1t\u03b7t. The solution of the projection step is then normalized to get a feasible point on the probability simplex. This algorithm is listed in Algorithm 2. \u03b1t\u03b7t is the regularization factor of the explicit \u21131 norm at round t. Note that the dominant regularization factor is \u03b7tdit which only pushes the weights of the correctly classified samples to zero .i.e., when dit < 0. This can become evident by substituting the update step in the projection step for zt+1.\nFor simplicity we consider two cases: when \u03b1t= min(1, 12\u03b3t||yt||1)and when \u03b1t=0. The following theorem bounds the training error.\nTheorem 4. Suppose that SparseBoost generates weak hypotheses h1, . . . , hT whose edges are \u03b31, . . . , \u03b3T . Then the error \u01eb of the combined hypothesis f on the training set is bounded as follows:\n\u01eb \u2264 1\n1 + c \u2211T\nt=1 \u03b3 2 t ||yt|| 2 1\n(16)\nNote that this bound holds for any choice of \u03b1 \u2208 [ 0,min(1, \u03b3t||yt||1) )\n. Particularly, in our two cases constant c is 1 for \u03b1t=0, and 14 when \u03b1t= min(1, 1 2 \u03b3t||yt||1).\nFor \u03b1t = 0, the \u21131 norm of the weights ||yt||1 can be bounded away from zero by 1N (see supplement). Thus, the error \u01eb tends to zero by O( N 2\n\u03b32T ). That is, in this case Sparseboost is a\nprovable boosting algorithm. However, for \u03b1t 6= 0, the \u21131 norm ||yt||1 may rapidly go to zero which consequently results in a non-vanishing upper bound (as T increases) for the training error in (16). In this case, it may not be possible to conclude that the algorithm is in fact a boosting algorithm6. It is noteworthy that SparseBoost can be seen as a variant of the COMID algorithm in [18].\nAlgorithm 2: SparseBoost\nLet R+={y | 0\u2264 yi}; Set y1 = [ 1N , . . . , 1 N ]\u22a4; At t = 1, . . . , T , train ht, set (\u03b7t= \u03b3t||yt||1\nN , \u03b1t=0) or (\u03b7t= \u03b3t||yt||1 2N , \u03b1t= 1 2 \u03b3t||yt||1), and\nupdate\nzt+1 = yt + \u03b7tdt\nyt+1 = argmin y\u2208R+\n1 2 ||y \u2212 zt+1|| 2 + \u03b1t\u03b7t||y||1 \u2192 y i t+1 = max(0, y i t + \u03b7td i t \u2212 \u03b1t\u03b7t)\nwt+1 = yt+1\n\u2211N i=1 yi\nOutput the final hypothesis f(x)= sign\n(\n\u2211T t=1 \u03b7tht(x)\n)\n."}, {"heading": "3.4 Lazy Update Boosting", "text": "In this section, we present the proof for the lazy update version of MABoost (LAMABoost) in Theorem 1. The proof technique is novel and can be used to generalize several known online learning algorithms such as OMDA in [27] and Meta algorithm in [28]. Moreover, we show that MadaBoost [10] can be presented in the LAMABoost setting. This gives a simple proof for MadaBoost without making the assumption that the edge sequence is monotonically decreasing (as in [10]).\nProof : Assume w\u2217 = [w\u22171 , . . . , w \u2217 N ] \u22a4 is a distribution vector where w\u2217i = 1 N\u01eb\nif f(xi) 6= ai, and 0 otherwise. Then, (w\u2217\u2212wt) \u22a4\u03b7tdt= (wt+1 \u2212wt) \u22a4(\u2207R(zt+1)\u2212\u2207R(zt) )\n+ (zt+1 \u2212wt+1) \u22a4(\u2207R(zt+1)\u2212\u2207R(zt) ) + (w\u2217 \u2212 zt+1) \u22a4(\u2207R(zt+1)\u2212\u2207R(zt) )\n\u2264 1\n2 ||wt+1 \u2212wt||\n2 + 1\n2 \u03b72t ||dt|| 2 \u2217 +BR(wt+1, zt+1)\u2212BR(wt+1, zt) +BR(zt+1, zt)\n\u2212BR(w \u2217, zt+1) +BR(w \u2217, zt)\u2212BR(zt+1, zt)\n\u2264 1\n2 ||wt+1 \u2212wt||\n2 + 1\n2 \u03b72t ||dt|| 2 \u2217 \u2212BR(wt+1,wt)\n+BR(wt+1, zt+1)\u2212BR(wt, zt)\u2212BR(w \u2217, zt+1) +BR(w \u2217, zt) (17)\nwhere the first inequality follows applying Lemma 3 to the first term and Lemma 2 to the rest of the terms and the second inequality is the result of applying the exact version of Lemma 1 to BR(wt+1, zt). Moreover, since BR(wt+1,wt)\u221212 ||wt+1\u2212wt||\n2 \u2265 0, they can be ignored in (17). Summing up the inequality (17) from t = 1 to T , yields\n\u2212BR(w \u2217, z1) \u2264 L\nT \u2211\nt=1\n1 2 \u03b72t \u2212\nT \u2211\nt=1\n\u03b7t\u03b3t (18)\nwhere we used the facts that w\u2217\u22a4 \u2211T t=1 \u03b7tdt \u2265 0 and \u2211T t=1 \u2212w \u22a4 t \u03b7tdt = \u2211T t=1 \u03b7t\u03b3t. The above inequality is exactly the same as (15), and replacing \u2212BR with \u01eb\u22121N\u01eb or log(\u01eb) yields the same\n6Nevertheless, for some choices of \u03b1t 6=0 such as \u03b1t \u221d 1t2 , the boosting property of the algorithm is still provable.\nerror bounds in Theorem 1. Note that, since the exact version of Lemma 1 is required to obtain (17), this proof does not reveal whether LAMABoost can be generalized to employ the doubleprojection strategy. In some particular cases, however, we may show that a double-projection variant of LAMABoost is still a provable boosting algorithm.\nIn the following, we briefly show that MadaBoost can be seen as a double-projection LAMABoost.\nAlgorithm 3: Variant of MadaBoost\nLet R(w) be the negative entropy and K a unit hypercube; Set z1 = [1, . . . , 1]\u22a4;\nAt t = 1, . . . , T , train ht with wt, set ft(x)= sign\n(\n\u2211t t\u2032=1 \u03b7t\u2032ht\u2032(x)\n)\nand calculate\n\u01ebt =\n\u2211N i=1 1 2 |ft(xi)\u2212 ai| N , set \u03b7t = \u01ebt\u03b3t and update\n\u2207R(zt+1) = \u2207R(zt) + \u03b7tdt \u2192 z i t+1 = z i te\n\u03b7td i\nt\nyt+1 = argmin y\u2208K\nBR(y, zt+1) \u2192 y i t+1 = min(1, z i t+1)\nwt+1 = argmin w\u2208S\nBR(w,yt+1) \u2192 w i t+1 = yit+1 ||yt+1||1\nOutput the final hypothesis f(x)= sign\n(\n\u2211T t=1 \u03b7tht(x)\n)\n.\nAlgorithm 3 is essentially MadaBoost, only with a different choice of \u03b7t. It is well-known that the entropy projection onto the probability simplex results in the normalization and thus, the second projection of Algorithm 3. The entropy projection onto the unit hypercube, however, maybe less known and thus, its proof is given in the Supplement.\nTheorem 5. Algorithm 3 yields a (1\u2212 \u01eb)-accurate hypothesis after at most T = O( 1 \u01eb2\u03b32 ).\nThis is an important result since it shows that MadaBoost seems, at least in theory, to be slower than what we hoped, namely O( 1\n\u01eb\u03b32 )."}, {"heading": "4 Conclusion and Discussion", "text": "In this work, we provided a boosting framework that can produce provable boosting algorithms. This framework is mainly suitable for designing boosting algorithms with distribution constraints. A sparse boosting algorithm that samples only a fraction of examples at each round was derived from this framework. However, since our proposed algorithm cannot control the exact number of zeros in the weight vector, a natural extension to this algorithm is to develop a boosting algorithm that receives the sparsity level as an input. However, this immediately raises the question: what is the maximum number of examples that can be removed at each round from the dataset, while still achieving a (1\u2212 \u01eb)-accurate hypothesis?\nThe boosting framework derived in this work is essentially the dual of the online mirror descent algorithm. This framework can be generalized in different ways. Here, we showed that replacing the Bregman projection step with the double-projection strategy, or as we call it approximate Bregman projection, still results in a boosting algorithm in the active version of MABoost, though this may not hold for the lazy version. In some special cases (MadaBoost for instance), however, it can be shown that this double-projection strategy works for the lazy version as well. Our conjecture is that under some conditions on the first convex set, the lazy version can also be generalized to work with the approximate projection operator. Finally, we provided a new error bound for the MadaBoost algorithm that does not depend on any assumption. Unlike the common conjecture, the convergence rate of MadaBoost (at least with our choice of \u03b7) is of O(1/\u01eb2)."}, {"heading": "Acknowledgments", "text": "This work was partially supported by SNSF. We would like to thank Professor Rocco Servedio for an inspiring email conversation and our colleague Hui Liang for his helpful comments."}, {"heading": "Supplement", "text": "Before proceeding with the proofs, some definitions and facts need to be reminded."}, {"heading": "Definition: Margin", "text": "Given a final hypothesis f(x) = \u2211T\nt=1 \u03b7tht(x), the margin of a sample (xj , aj) is defined as m(xj) = ajf(xj)/ \u2211T\nt=1 \u03b7t. Moreover, the margin of a set of examples denoted by mD is the minimum of margins over the examples, i.e., mD=minx m(xj).\nFact: Duality between max-margin and min-edge\nThe minimum edge \u03b3min that can be achieved over all possible distributions of the training set is equal to the maximum margin (m\u2217 = max\u03b7 mD) of any linear combination of hypotheses from the hypotheses space.\nThis fact is discussed in details in [29] and [30]. It is the direct result of von Neumann\u2019s minmax theorem and simply means that the maximum achievable margin is \u03b3min."}, {"heading": "Proof of Theorem 2", "text": "The proof for the maximum margin property of MABoost, is almost the same as the proof of Theorem 1.\nLet\u2019s assume the ith sample has the worst margin, i.e., mD = m(xi). Let all entries of the error vector w\u2217 to be zero except its ith entry which is set to be 1. Following the same approach as in Theorem 1, (see equation (13)), we get\nT \u2211\nt=1\nw \u2217\u22a4\u03b7tdt\u2212w\u22a4t \u03b7tdt \u2264\nT \u2211\nt=1\n1 2 \u03b72t ||dt||2\u2217 +BR(w\u2217,w1)\u2212BR(w\u2217,wT+1) (19)\nWith our choice of w\u2217 it is easy to verify that the first term on the left side of the inequality is mD \u2211T\nt=1 \u03b7t=\n\u2212\u2211T t=1w \u2217\u22a4\u03b7tdt. By setting C = BR(w\u2217,w1), ignoring the last term BR(w\u2217,wT+1), replacing ||dt||2\u2217 with its upper bound L and using the identity \u2211T\nt=1 w \u22a4 t \u03b7tdt= \u2212 \u2211T\nt=1 \u03b7t\u03b3t the above inequality is simplified to\n\u2212mD T \u2211\nt=1\n\u03b7t \u2264 L T \u2211\nt=1\n1 2 \u03b72t \u2212\nT \u2211\nt=1\n\u03b7t\u03b3t + C (20)\nReplacing \u03b7t with the value suggested in Theorem 2, i.e., \u03b7t = \u03b3t\nL \u221a t and dividing both sides by \u2211T t=1 \u03b7t, gives\n\u2211T t=1( 1\u221a t \u2212 1 t )\u03b32t\n\u2211T t=1 1\u221a t \u03b3t\n\u2212 LC \u2211T\nt=1 1\u221a t \u03b3t\n\u2264 mD (21)\nThe first term is minimized when \u03b3t=\u03b3min . Similarly to the first term, the second term is maximized when \u03b3t is replaced by its minimum value. This gives the following lower bound for mD:\n\u03b3min\n\u2211T t=1 1\u221a t \u2212 1\nt \u2211T\nt=1 1\u221a t\n\u2212 LC \u03b3min \u2211T\nt=1 1\u221a t\n\u2264 mD (22)\nConsidering the facts that \u222b T+1 1 dx\u221a x \u2264 \u2211T t=1 1\u221a t and 1 + \u222b T 1 dx x \u2265 \u2211T t=1 1 t , we get\n\u03b3min \u2212 1 + log T 2 \u221a T + 1\u2212 2 \u03b3min \u2212 LC \u03b3min( \u221a T + 1\u2212 1) \u2264 mD (23)\nNow by taking \u03bd = 1+log T 2 \u221a T+1\u22122\u03b3min + LC \u03b3min( \u221a T+1\u22121) , we have \u03b3min \u2212 \u03bd \u2264 \u03b3min. It is clear from (23) that \u03bd approaches zero as T tends to infinity with a convergence rate proportional to log T\u221a T . It is noteworthy that this convergence rate is slightly worse than that of TotalBoost which is O( 1\u221a T )."}, {"heading": "Proof of Lemma 4", "text": "Remember that \u03a0\u0302S(y)= \u03a0S ( \u03a0K(y) ) . Our goal is to show that BR(x,y) \u2265 BR ( x, \u03a0\u0302S(y) ) .\nTo this end, we only need to repeatedly apply Lemma 1, as follows BR(x,y) \u2265 BR ( x,\u03a0K(y) )\n(24)\nBR ( x,\u03a0K(y) ) \u2265 BR ( x, \u03a0\u0302S(y) )\n(25) which completes the proof.\nProof of combining datasets boosting algorithm\nWe have to show that when the convex set is defined as\nSc= {w| N \u2211\ni=1\nwi= 1, 0\u2264 wi \u2200i \u2208 A \u2227 0\u2264 wi\u2264 k N \u2200i \u2208 B} (26)\nthe error of the final hypothesis on A, i.e., \u01ebA, converges to zero while the error on B is guaranteed to be \u01ebB \u2264 1k . First, we show the convergence of \u01ebA to zero. This is easily obtained by setting w\u2217 to be an error vector with zero weights over the training samples from B and 1\u01ebANA weights over the training set A. One can verify that w\u2217 \u2208 Sc, thus the proof of Theorem 1 holds and subsequently, the error bounds in (8) stating that \u01ebA \u2192 0 as the number of iterations increases.\nTo show the second part of the theorem that is \u01ebB \u2264 1k , vector w \u2217 is selected to be an error vector with zero weights over the training samples from A and 1\u01ebBNB weights over the training set B. Note that, as long as \u01ebB is greater than 1\nk , this w\u2217 \u2208 Sc. Thus, for all 1k \u2264 \u01ebB the proof of Theorem 1 holds and as the bounds in (8)\nshow, the error decreases as the number of iterations increases. In particular in a finite number of rounds, the classification error on B reduces to 1\nk which completes the proof."}, {"heading": "Proof of Theorem 4", "text": "We use proof techniques similar to those given in [18], with a slight change to take the normalization step into account.\nBy replacing zt+1 in the projection step from the update step, the projection step can be rewritten as\nyt+1 = argmin y\u2208R+\n1 2 ||y \u2212 yt|| \u2212 \u03b7ty\u22a4dt + \u03b1t\u03b7t||y||1 (27)\nThis optimization problem can be highly simplified by noting that the variables are not coupled. Thus, each coordinate can be independently optimized. In other words, it can be decoupled intoN independent 1-dimensional optimization problems.\nyit+1 = argmin 0\u2264yi\n1 2 ||yi \u2212 yit|| \u2212 \u03b7tyidit + \u03b1t\u03b7tyi (28)\nThe solution of (28) can be written as\nyit+1 = max(0, y i t + \u03b7td i t \u2212 \u03b1t\u03b7t) (29)\nThis simple solution gives a very efficient and simple implementation for SparseBoost. From (28) it is clear that for dit < 0 (i.e., when i\nth sample is classified correctly), \u2212\u03b7tydit acts as the \u21131 norm regularization and pushes yit+1 towards zero while \u03b1t\u03b7t enhance sparsity by pushing all weights to zero.\nLet w\u2217 to be the same error vector as defined in Theorem 1. We start this proof by again deriving the progress bounds on each step of the algorithm. The optimality of yt+1 for (27) implies that (w\u2217 \u2212 yt+1)\u22a4(\u2212\u03b7tdt + \u03b1t\u03b7tr\u2032(y) + yt+1 \u2212 yt) \u2265 0 (30) where r\u2032(y) is a sub-gradient vector of the \u21131 norm function r(y) = \u2211N\ni=1 yi. Moreover, due to the convexity of r(y), we have\n\u03b1t\u03b7tr(yt+1) \u22a4(w\u2217 \u2212 yt+1) \u2264 \u03b1t\u03b7t ( r(w\u2217)\u2212 r(yt+1) )\n(31)\nWe thus have\n(w\u2217\u2212 yt)\u22a4\u03b7tdt + \u03b1t\u03b7t ( r(yt+1)\u2212 r(w\u2217) ) \u2264 (w\u2217\u2212 yt)\u22a4\u03b7tdt + \u03b1t\u03b7t(yt+1 \u2212w\u2217)\u22a4r\u2032(yt+1) = (w\u2217\u2212 yt+1)\u22a4\u03b7tdt + \u03b1t\u03b7t(yt+1 \u2212w\u2217)\u22a4r\u2032(yt+1) + (yt+1 \u2212 yt)\u22a4\u03b7tdt = (w\u2217\u2212 yt+1)\u22a4(\u03b7tdt \u2212 \u03b1t\u03b7tr\u2032(yt+1)\u2212 yt+1 + yt) + (w\u2217\u2212 yt+1)\u22a4(yt+1 \u2212 yt) + (yt+1 \u2212 yt)\u22a4\u03b7tdt (32)\nwhere the first inequality follows (31). Now, from the optimality condition in (30), the first term in the last equation is non-positive and thus, can be ignored.\n(w\u2217\u2212 yt)\u22a4\u03b7tdt + \u03b1t\u03b7t ( r(yt+1)\u2212 r(w\u2217) ) \u2264 (w\u2217\u2212 yt+1)\u22a4(yt+1 \u2212 yt) + (yt+1 \u2212 yt)\u22a4\u03b7tdt\n= 1 2 ||w\u2217\u2212 yt||22 \u2212 1 2 ||yt+1\u2212 yt||22 \u2212 1 2 ||w\u2217\u2212 yt+1||22 + (yt+1 \u2212 yt)\u22a4\u03b7tdt \u2264 1 2 ||w\u2217\u2212 yt||22 \u2212 1 2 ||yt+1\u2212 yt||22 \u2212 1 2 ||w\u2217\u2212 yt+1||22 + 1 2 ||yt+1\u2212 yt||22 + 1 2 \u03b72t ||dt||2\u2217 (33)\nwhere the first equation follows from Lemma 2 (or direct algebraic expansion in this case) and the second inequality from Lemma 3.\nBy summing the left and right sides of the inequality from 1 to T , replacing ||dt||2\u2217 with its upperbound N and substituting 1 for r(w\u2217), we get\nT \u2211\nt=1\nw \u2217\u22a4\u03b7tdt \u2264\nT \u2211\nt=1\ny \u22a4 t \u03b7tdt +\nT \u2211\nt=1\nN 2 \u03b72t + 1 2 ||w\u2217\u2212 y1||22 +\nT \u2211\nt=1\n\u03b1t\u03b7t ( 1\u2212 r(yt+1) )\n(34)\nNow, replacing r(yt+1) with its lower bound, i.e, 0 and using the fact that \u2211T t=1w \u2217\u22a4\u03b7tdt \u2265 0 (as shown in (14)) and \u2211T\nt=1 y \u22a4 t \u03b7tdt= \u2212 \u2211T t=1 \u03b7t\u03b3t||yt||1, yields\n0 \u2264 \u2212 T \u2211\nt=1\n\u03b7t\u03b3t||yt||1 + T \u2211\nt=1\nN 2 \u03b72t + 1 2 ||w\u2217\u2212 y1||22 +\nT \u2211\nt=1\n\u03b1t\u03b7t (35)\nTaking derivative w.r.t \u03b7t and setting it to zero, gives the optimal \u03b7t as follows\n\u03b7t = \u03b3t||yt||1 \u2212 \u03b1t\nN (36)\nThis equation implies that \u03b1t should be smaller than \u03b3t||yt||1 or otherwise \u03b7t becomes smaller than zero. Setting \u03b1t = (1 \u2212 k)\u03b3t||yt||1 where k is a constant smaller than or equal to 1, results in \u03b7t = kN \u03b3t||yt||1. Replacing this value for \u03b7t in (35) and noting that 12 ||w\n\u2217\u2212 y1||22 = 1\u2212\u01eb2N\u01eb gives the following bound on the training error\n\u01eb \u2264 1 1 + c \u2211T\nt=1 \u03b3 2 t ||yt||21\n(37)\nwhere c = 1 k2 is a constant factor depending on the choice of \u03b1t. To prove that \u01eb approaches zero as T increases, we still have to provide an evidence that \u2211T\nt=1 \u03b3 2 t ||yt||21 is a divergent series. There are different\npossibilities to approach this problem. Here, we show that in the case of \u03b1t=0, the \u21131 norm of weights ||yt||1 can be bounded away from zero (i.e., ||yt||1 \u2265 C > 0) and thus, \u2211T t=1 \u03b3 2 t ||yt||21 \u2265 T\u03b32minC2.\nTo this end, we rewrite yit from (29) as\nyit = max(0, y i t\u22121 + \u03b7t\u22121d i t\u22121 \u2212 \u03b1t\u22121\u03b7t\u22121)\n\u2265 yit\u22121 + \u03b7t\u22121dit\u22121 \u2212 \u03b1t\u22121\u03b7t\u22121\n\u2265 1 N +\nt\u22121 \u2211\nt\u2032=1\n\u03b7t\u2032dt\u2032 \u2212 t\u22121 \u2211\nt\u2032=1\n\u03b1t\u2032\u03b7t\u2032 (38)\nwhere the last inequality is achieved by recursively applying the first inequality to yit\u22121. At any arbitrary round t, either the algorithm has already converged and \u01eb=0 or there is at least one sample that is classified wrongly by the ensemble classifier Ht(x) = \u2211t l=1 \u03b7lhl(x). Now, without loss of generality, assume the i th sample is wrongly classified at round t. That is, \u2211t\u22121\nt\u2032=1 \u03b7t\u2032dt\u2032 > 0 (look at (14)). Now, for \u03b1t = 0, the weight of the wrongly classified sample i is\nyit \u2265 1\nN +\nt\u22121 \u2211\nt\u2032=1\n\u03b7t\u2032dt\u2032 \u2265 1\nN (39)\nThat is, ||yt||1 \u2265 1N . This gives a lousy (but sufficient for our purpose) lower bound on ||yt||1. Replacing ||yt||1 with its lower bound 1N in (37), yields\n\u01eb \u2264 N 2\n1 + T\u03b32 (40)\nwhere \u03b3 is the minimum edge over all \u03b3t."}, {"heading": "Proof of Entropy Projection onto Hypercube (Second Update Step in MadaBoost)", "text": "Lemma 5. Let R(w)= \u2211N i=1 wi logwi\u2212wi. Then the Bregman projection of a positive vector z \u2208 RN+ onto the unit hypercube K = [0, 1]N is yi = min(1, zi), i = 1, . . . , N . To show the correctness of the above lemma, i.e., that the solution of the Bregman projection\ny = argmin y\u2208K BR(y, z) (41)\nis yi = min(1, zi), we only need to show that y satisfies the optimality condition\n(v\u2212 y)\u22a4\u2207BR(y, z) \u2265 0 \u2200v \u2208 K (42)\nGiven R(w)= \u2211N i=1 wi logwi\u2212wi, the gradient of BR is\n\u2207BR(y, z) = T \u2211\ni=1\nlog yi zi\n(43)\nHence,\n(v \u2212 y)\u22a4\u2207BR(y, z) = \u2211\ni\u2208{i:zi\u22651} (vi \u2212 yi) log\nyi zi + \u2211\ni\u2208{i:zi<1} (vi \u2212 yi) log\nyi zi\n(44)\nFor zi \u2265 1, yi is equal to 1. That is, log yizi = log 1 zi < 0. On the other hand, since vi \u2264 1, (vi \u2212 yi) = (vi \u2212 1) \u2264 0. Thus, the first sum in (44) is always non-negative. The second sum is always zero since log yi\nzi = log 1 = 0. That is, the optimality condition (44) is non-negative for all v which completes the proof."}, {"heading": "Proof of Theorem 5", "text": "Its proof is essentially the same as the proof of the lazy version of MABoost with a few differences. Before proceeding further, some definitions and facts should be re-emphasized.\nFirst of all, since R(w) = \u2211N i=1 wi logwi \u2212 wi is 1N -strongly convex (see [31, p. 136]) with respect to \u21131 norm (and not 1-strongly as in Theorem 1), the following inequality holds for the Bregman divergence:\nBR(x,y) \u2265 1\n2N ||x\u2212 y||21 (45)\nMoreover, the following lemma which bounds ||yt|| is essential for our proof.\nLemma 6. For all t, ||yt||1 \u2265 N\u01ebt where \u01ebt is the error of the ensemble hypothesis Ht(x) = \u2211t\nl=1 \u03b7lhl(x) at round t.\nThis lemma holds due to the fact that\nyit = min(1, z i t) = min(1, e\n\u2211 t\nl=1 \u03b7ld i l ) = min(1, e\u2212aiHt(xi)) (46)\nwhere Ht(x) = \u2211t\nl=1 \u03b7lhl(x) is the output of the algorithm at round t. If Ht(xi) makes a mistake on classifying xi, \u2212aiHt(xi) will be greater than zero and thus, yit = 1. For the samples that are classified correctly, \u2212aiHt(xi) \u2264 0 and thus, 0 \u2264 yit \u2264 1. That is, N\u01ebt = number of wrongly classified samples at round t \u2264 \u2211N\ni=1 y i t = ||yt||1 .\nWe are now ready to proceed with the proof of Theorem 5. Let w\u2217 = [w\u22171 , \u00b7 \u00b7 \u00b7, w\u2217N ]\u22a4 to be a vector where w\u2217i = 1 if f(xi) 6= ai, and 0 otherwise. Similar to the proof of the lazy update, we are going to bound the \u2211T\nt=1(w \u2217\u2212 yt)\u22a4\u03b7tdt.\n(w\u2217\u2212 yt)\u22a4\u03b7tdt= (yt+1 \u2212 yt)\u22a4 ( \u2207R(zt+1)\u2212\u2207R(zt) )\n+ (zt+1 \u2212 yt+1)\u22a4 ( \u2207R(zt+1)\u2212\u2207R(zt) ) + (w\u2217 \u2212 zt+1)\u22a4 ( \u2207R(zt+1)\u2212\u2207R(zt) )\n\u2264 1 2N ||yt+1 \u2212 yt||2 + N 2 \u03b72t ||dt||2\u2217 +BR(yt+1, zt+1)\u2212BR(yt+1, zt) +BR(zt+1, zt) \u2212BR(w\u2217, zt+1) +BR(w\u2217, zt)\u2212BR(zt+1, zt)\n\u2264 1 2N ||yt+1 \u2212 yt||2 + N 2 \u03b72t ||dt||2\u2217 \u2212BR(yt+1,yt)\n+BR(yt+1, zt+1)\u2212BR(yt, zt)\u2212BR(w\u2217, zt+1) +BR(w\u2217, zt) (47) where the first inequality follows from applying Lemma 3 to the first term and Lemma 2 to the rest of the terms and the second inequality is the result of applying the exact version of Lemma 1 to BR(yt+1, zt). Moreover, according to inequality (45) BR(yt+1,yt) \u2212 12N ||yt+1 \u2212 yt||\n2 \u2265 0 and hence these terms can be ignored in (47). Summing up the inequality (47) from t = 1 to T , yields:\n\u2212BR(w\u2217, z1) \u2264 T \u2211\nt=1\nN 2 \u03b72t \u2212\nT \u2211\nt=1\n\u03b7t\u03b3t||yt||1 (48)\nIt is important to remark that ||yt||1 appearing in the last term is due to the fact that wt = yt||yt||1 and thus, y\u22a4t \u03b7tdt = w \u22a4 t \u03b7tdt||yt||1 = \u03b7t\u03b3t||yt||1.\nNow, by replacing \u03b7t = \u01ebt\u03b3t in the above equation and noting that BR(w\u2217, z1) = N \u2212N\u01eb, we get:\n\u2212N(1\u2212 \u01eb) \u2264 T \u2211\nt=1\nN 2 \u01eb2t\u03b3 2 t \u2212\nT \u2211\nt=1\n\u01ebt\u03b3 2 t ||yt||1 (49)\nFrom Lemma 6, it is evident that ||yt||1 \u2265 N\u01ebt. Moreover, since \u01eb \u2264 \u01ebt, it can be replaced by \u01eb, as well (though very pessimistic). As usuall, \u03b3t is also replaced with the min edge, denoted by \u03b3. Applying these lower bounds to the equation (49), yields\n\u01eb2 \u2264 2(1\u2212 \u01eb) T\u03b32 \u2264 1 T\u03b32\n(50)\nwhich indicates that the proposed version of MadaBoost needs at most O( 1 \u01eb2\u03b32 ) iterations to converge."}], "references": [{"title": "The strength of weak learnability", "author": ["R.E. Schapire"], "venue": "Mach. Learn.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1990}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Prediction games and arcing algorithms", "author": ["L. Breiman"], "venue": "Neural Comput.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Boosting algorithms as gradient descent", "author": ["L. Mason", "J. Baxter", "P. Bartlett", "M. Frean"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Additive logistic regression: a statistical view of boosting", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Smooth boosting and learning with malicious noise", "author": ["R.A. Servedio"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Optimally-smooth adaptive boosting and application to agnostic learning", "author": ["D. Gavinsky"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Filterboost: Regression and classification on large datasets", "author": ["J.K. Bradley", "R.E. Schapire"], "venue": "In NIPS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Smooth boosting using an information-based criterion", "author": ["K. Hatano"], "venue": "In Algorithmic Learning Theory", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Madaboost: A modification of AdaBoost", "author": ["C. Domingo", "O. Watanabe"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Y. Freund"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "On boosting with polynomially bounded distributions", "author": ["N.H. Bshouty", "D. Gavinsky", "M. Long"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Totally corrective boosting algorithms that maximize the margin", "author": ["M.K. Warmuth", "J. Liao", "G. R\u00e4tsch"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Logistic regression, adaboost and bregman distances", "author": ["M. Collins", "R.E. Schapire", "Y. Singer"], "venue": "Mach. Learn.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "On the equivalence of weak learnability and linear separability: new relaxations and efficient boosting algorithms", "author": ["S. Shalev-Shwartz", "Y. Singer"], "venue": "In COLT,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "A survey: The convex optimization approach to regret minimization", "author": ["E. Hazan"], "venue": "Working draft,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Composite objective mirror descent", "author": ["J.C. Duchi", "S. Shalev-shwartz", "Y. Singer", "A. Tewari"], "venue": "In COLT,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Game theory, on-line prediction and boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In COLT,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "Pasting bites together for prediction in large data sets and on-line", "author": ["L. Breiman"], "venue": "Technical report, Dept. Statistics, Univ. California,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Toward efficient agnostic learning", "author": ["M.J. Kearns", "R.E. Schapire", "L.M. Sellie"], "venue": "In COLT,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "Potential-based agnostic boosting", "author": ["A. Kalai", "V. Kanade"], "venue": "In NIPS", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Agnostic boosting", "author": ["S. Ben-David", "P. Long", "Y. Mansour"], "venue": "In Computational Learning Theory. Springer,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Boosting for transfer learning", "author": ["W. Dai", "Q. Yang", "G. Xue", "Y. Yong"], "venue": "In ICML,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application", "author": ["W. Wang", "M.A. Carreira-Perpi\u00f1\u00e1n"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Online learning with predictable sequences", "author": ["A. Rakhlin", "K. Sridharan"], "venue": "In COLT,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Online optimization with gradual variations", "author": ["C. Chiang", "T. Yang", "C. Lee", "M. Mahdavi", "C. Lu", "R. Jin", "S. Zhu"], "venue": "In COLT,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Experiments with a New Boosting Algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In ICML,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1996}, {"title": "Efficient margin maximization with boosting", "author": ["G. R\u00e4tsch", "M. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Two dominant views to describe and design boosting algorithms are \u201cweak to strong learner\u201d (WTSL), which is the original viewpoint presented in [1, 2], and boosting by \u201ccoordinate-wise gradient descent in the functional space\u201d (CWGD) appearing in later works [3, 4, 5].", "startOffset": 144, "endOffset": 150}, {"referenceID": 1, "context": "Two dominant views to describe and design boosting algorithms are \u201cweak to strong learner\u201d (WTSL), which is the original viewpoint presented in [1, 2], and boosting by \u201ccoordinate-wise gradient descent in the functional space\u201d (CWGD) appearing in later works [3, 4, 5].", "startOffset": 144, "endOffset": 150}, {"referenceID": 2, "context": "Two dominant views to describe and design boosting algorithms are \u201cweak to strong learner\u201d (WTSL), which is the original viewpoint presented in [1, 2], and boosting by \u201ccoordinate-wise gradient descent in the functional space\u201d (CWGD) appearing in later works [3, 4, 5].", "startOffset": 259, "endOffset": 268}, {"referenceID": 3, "context": "Two dominant views to describe and design boosting algorithms are \u201cweak to strong learner\u201d (WTSL), which is the original viewpoint presented in [1, 2], and boosting by \u201ccoordinate-wise gradient descent in the functional space\u201d (CWGD) appearing in later works [3, 4, 5].", "startOffset": 259, "endOffset": 268}, {"referenceID": 4, "context": "Two dominant views to describe and design boosting algorithms are \u201cweak to strong learner\u201d (WTSL), which is the original viewpoint presented in [1, 2], and boosting by \u201ccoordinate-wise gradient descent in the functional space\u201d (CWGD) appearing in later works [3, 4, 5].", "startOffset": 259, "endOffset": 268}, {"referenceID": 3, "context": ", AnyBoost [4], resulting from the second viewpoint have proven to be particularly prolific when it comes to developing new boosting algorithms.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "A smooth distribution is a distribution that does not put too much weight on any single sample or in other words, a distribution emulated by the booster does not dramatically diverge from the target distribution [6, 7].", "startOffset": 212, "endOffset": 218}, {"referenceID": 6, "context": "A smooth distribution is a distribution that does not put too much weight on any single sample or in other words, a distribution emulated by the booster does not dramatically diverge from the target distribution [6, 7].", "startOffset": 212, "endOffset": 218}, {"referenceID": 4, "context": "Then, what loss function can generate such a customized distribution? Moreover, does it result in a provable boosting algorithm? In general, how can we characterize the accuracy of the final hypothesis? Although, to be fair, the so-called loss function hunting approach has given rise to useful boosting algorithms such as LogitBoost, FilterBoost, GiniBoost and MadaBoost [5, 8, 9, 10] which (to some extent) answer some of the above questions, it is an inflexible and relatively unsuccessful approach to addressing the boosting problems with distribution constraints.", "startOffset": 372, "endOffset": 385}, {"referenceID": 7, "context": "Then, what loss function can generate such a customized distribution? Moreover, does it result in a provable boosting algorithm? In general, how can we characterize the accuracy of the final hypothesis? Although, to be fair, the so-called loss function hunting approach has given rise to useful boosting algorithms such as LogitBoost, FilterBoost, GiniBoost and MadaBoost [5, 8, 9, 10] which (to some extent) answer some of the above questions, it is an inflexible and relatively unsuccessful approach to addressing the boosting problems with distribution constraints.", "startOffset": 372, "endOffset": 385}, {"referenceID": 8, "context": "Then, what loss function can generate such a customized distribution? Moreover, does it result in a provable boosting algorithm? In general, how can we characterize the accuracy of the final hypothesis? Although, to be fair, the so-called loss function hunting approach has given rise to useful boosting algorithms such as LogitBoost, FilterBoost, GiniBoost and MadaBoost [5, 8, 9, 10] which (to some extent) answer some of the above questions, it is an inflexible and relatively unsuccessful approach to addressing the boosting problems with distribution constraints.", "startOffset": 372, "endOffset": 385}, {"referenceID": 9, "context": "Then, what loss function can generate such a customized distribution? Moreover, does it result in a provable boosting algorithm? In general, how can we characterize the accuracy of the final hypothesis? Although, to be fair, the so-called loss function hunting approach has given rise to useful boosting algorithms such as LogitBoost, FilterBoost, GiniBoost and MadaBoost [5, 8, 9, 10] which (to some extent) answer some of the above questions, it is an inflexible and relatively unsuccessful approach to addressing the boosting problems with distribution constraints.", "startOffset": 372, "endOffset": 385}, {"referenceID": 10, "context": "Another approach to designing a boosting algorithm is to directly follow the WTSL viewpoint [11, 6, 12].", "startOffset": 92, "endOffset": 103}, {"referenceID": 5, "context": "Another approach to designing a boosting algorithm is to directly follow the WTSL viewpoint [11, 6, 12].", "startOffset": 92, "endOffset": 103}, {"referenceID": 11, "context": "Another approach to designing a boosting algorithm is to directly follow the WTSL viewpoint [11, 6, 12].", "startOffset": 92, "endOffset": 103}, {"referenceID": 12, "context": "There are, however, a few useful, and albeit fairly limited, algorithmic frameworks such as TotalBoost [13] that can be used to derive other provable boosting algorithms.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "[15], where it was shown that weak learnability and linear separability are equivalent, a result following from von Neumann\u2019s minmax theorem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "1 Our Results We present a family of boosting algorithms that can be derived from well-known online learning algorithms, including projected gradient descent [16] and its generalization, mirror descent (both active and lazy updates, see [17]) and composite objective mirror descent (COMID) [18].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "1 Our Results We present a family of boosting algorithms that can be derived from well-known online learning algorithms, including projected gradient descent [16] and its generalization, mirror descent (both active and lazy updates, see [17]) and composite objective mirror descent (COMID) [18].", "startOffset": 237, "endOffset": 241}, {"referenceID": 17, "context": "1 Our Results We present a family of boosting algorithms that can be derived from well-known online learning algorithms, including projected gradient descent [16] and its generalization, mirror descent (both active and lazy updates, see [17]) and composite objective mirror descent (COMID) [18].", "startOffset": 290, "endOffset": 294}, {"referenceID": 1, "context": "This duality was first pointed out in [2] and was later elaborated and formalized by using the von Neumann\u2019s minmax theorem [19].", "startOffset": 38, "endOffset": 41}, {"referenceID": 18, "context": "This duality was first pointed out in [2] and was later elaborated and formalized by using the von Neumann\u2019s minmax theorem [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 19, "context": ", applying batch boosting on the successive subsets of data when there is not sufficient memory to store an entire dataset, was first discussed by Breiman in [20], though no algorithm with theoretical guarantee was suggested.", "startOffset": 158, "endOffset": 162}, {"referenceID": 21, "context": "This algorithm can also be seen as an agnostic boosting algorithm4 due to the fact that smooth distributions provide a theoretical guarantee for noise tolerance in various noisy learning settings, such as agnostic boosting [22, 23].", "startOffset": 223, "endOffset": 231}, {"referenceID": 22, "context": "This algorithm can also be seen as an agnostic boosting algorithm4 due to the fact that smooth distributions provide a theoretical guarantee for noise tolerance in various noisy learning settings, such as agnostic boosting [22, 23].", "startOffset": 223, "endOffset": 231}, {"referenceID": 9, "context": "Furthermore, we provide an interesting theoretical result about MadaBoost [10].", "startOffset": 74, "endOffset": 78}, {"referenceID": 20, "context": "Unlike the PAC model, the agnostic learning model allows an arbitrary target function (labeling function) that may not belong to the class studied, and hence, can be viewed as a noise tolerant learning model [21].", "startOffset": 208, "endOffset": 212}, {"referenceID": 16, "context": "Specifically, consider the Lazy update variant of the online Mirror Descent (LMD) algorithm (see for instance [17]).", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "The standard proof to show that the LMD update scheme achieves vanishing regret bound is through showing its equivalence to the FTRL algorithm [17] in the case that they are both linearized, i.", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "This algorithm is a variant of the mirror descent algorithm [17], modified to work as a boosting algorithm.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "Using this vector and following the approach in [17], we derive the upper bound of \u2211T t=1 \u03b7t(w \u2217\u22a4dt\u2212w\u22a4 t dt) where dt = [d 1 t , .", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "An entropic projection algorithm (R is negative entropy), for instance, was proposed in [15].", "startOffset": 88, "endOffset": 92}, {"referenceID": 24, "context": "This scenario has many potential applications including transfer learning [25], weighted combination of datasets based on their noise level and emphasizing on a particular region of a sample space as a problem requirement (e.", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "It is noteworthy that SparseBoost can be seen as a variant of the COMID algorithm in [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 26, "context": "The proof technique is novel and can be used to generalize several known online learning algorithms such as OMDA in [27] and Meta algorithm in [28].", "startOffset": 116, "endOffset": 120}, {"referenceID": 27, "context": "The proof technique is novel and can be used to generalize several known online learning algorithms such as OMDA in [27] and Meta algorithm in [28].", "startOffset": 143, "endOffset": 147}, {"referenceID": 9, "context": "Moreover, we show that MadaBoost [10] can be presented in the LAMABoost setting.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "This gives a simple proof for MadaBoost without making the assumption that the edge sequence is monotonically decreasing (as in [10]).", "startOffset": 128, "endOffset": 132}], "year": 2014, "abstractText": "By exploiting the duality between boosting and online learning, we present a boosting framework which proves to be extremely powerful thanks to employing the vast knowledge available in the online learning area. Using this framework, we develop various algorithms to address multiple practically and theoretically interesting questions including sparse boosting, smooth-distribution boosting, agnostic learning and, as a by-product, some generalization to double-projection online learning algorithms1.", "creator": "LaTeX with hyperref package"}}}