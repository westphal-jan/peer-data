{"id": "1409.8185", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2014", "title": "Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models", "abstract": "additionally we develop a sequential low - complexity inference procedure for the infinite gaussian mixture model ( igmm ) for the general case of an unknown mean and covariance. the observations are sequentially allocated to classes based on a sequential maximum a - posterior ( map ) criterion. we present an easily computed, closed form for the conditional likelihood, in which the parameters can be recursively updated as a function of the streaming data. we thus propose a totally novel adaptive design for the dirichlet process concentration parameter at each iteration, and will prove, under a simplified optimization model, that the periodic sequence of concentration parameters is asymptotically well - behaved. we sketch an overall equivalence comparison between the steady - state performance of the algorithm and gaussian classification. the methodology is applied to the problem of adaptive modulation recognition and obviates the need for storing a large binary modulation library required for traditional modulation recognition. we also numerically evaluate the bit error rate performance ( ber ) of the dpmm - trained classifier machine when used as a demodulator and show that there is critical signal - to - noise ratio ( snr ) that characterizes whether successful decoding is possible.", "histories": [["v1", "Mon, 29 Sep 2014 16:47:44 GMT  (472kb,D)", "https://arxiv.org/abs/1409.8185v1", "31 pages, Submitted"], ["v2", "Thu, 5 Feb 2015 15:55:06 GMT  (1062kb,D)", "http://arxiv.org/abs/1409.8185v2", "27 pages, Submitted"], ["v3", "Fri, 11 Sep 2015 20:07:31 GMT  (671kb,D)", "http://arxiv.org/abs/1409.8185v3", "25 pages, To appear in Advances in Neural Information Processing Systems (NIPS) 2015"]], "COMMENTS": "31 pages, Submitted", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["theodoros tsiligkaridis", "keith w forsythe"], "accepted": true, "id": "1409.8185"}, "pdf": {"name": "1409.8185.pdf", "metadata": {"source": "CRF", "title": "Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models", "authors": ["Theodoros Tsiligkaridis", "Keith W. Forsythe"], "emails": ["ttsili@ll.mit.edu", "forsythe@ll.mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Dirichlet process mixture models (DPMM) have been widely used for clustering data [9, 11]. Traditional finite mixture models often suffer from overfitting or underfitting of data due to possible mismatch between the model complexity and amount of data. Thus, model selection or model\nThis work is sponsored by the Assistant Secretary of Defense for Research & Engineering under Air Force Contract #FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the United States Government.\nar X\niv :1\n40 9.\n81 85\nv3 [\nst at\n.M L\n] 1\naveraging is required to find the correct number of clusters or the model with the appropriate complexity. This requires significant computation for high-dimensional data sets or large samples. Bayesian nonparametric modeling are alternative approaches to parametric modeling, an example being DPMM\u2019s which can automatically infer the number of clusters from the data via Bayesian inference techniques.\nThe use of Markov chain Monte Carlo (MCMC) methods for Dirichlet process mixtures has made inference tractable [10]. However, these methods can exhibit slow convergence and their convergence can be tough to detect. Alternatives include variational methods [3], which are deterministic algorithms that convert inference to optimization. These approaches can take a significant computational effort even for moderate sized data sets. For largescale data sets and low-latency applications with streaming data, there is a need for inference algorithms that are much faster and do not require multiple passes through the data. In this work, we focus on low-complexity algorithms that adapt to each sample as they arrive, making them highly scalable. An online algorithm for learning DPMM\u2019s based on a sequential variational approximation (SVA) was proposed in [8], and the authors in [15] recently proposed a sequential maximum a-posterior (MAP) estimator for the class labels given streaming data. The algorithm is called sequential updating and greedy search (SUGS) and each iteration is composed of a greedy selection step and a posterior update step.\nThe choice of concentration parameter \u03b1 is critical for DPMM\u2019s as it controls the number of clusters [1]. While most fast DPMM algorithms use a fixed \u03b1 [6, 4, 7], imposing a prior distribution on \u03b1 and sampling from it provides more flexibility, but this approach still heavily relies on experimentation and prior knowledge. Thus, many fast inference methods for Dirichlet process mixture models have been proposed that can adapt \u03b1 to the data, including the works [5] where learning of \u03b1 is incorporated in the Gibbs sampling analysis, [3] where a Gamma prior is used in a conjugate manner directly in the variational inference algorithm. [15] also account for model uncertainty on the concentration parameter \u03b1 in a Bayesian manner directly in the sequential inference procedure. This approach can be computationally expensive, as discretization of the domain of \u03b1 is needed, and its stability highly depends on the initial distribution on \u03b1 and on the range of values of \u03b1. To the best of our knowledge, we are the first to analytically study the evolution and stability of the adapted sequence of \u03b1\u2019s in the online learning setting.\nIn this paper, we propose an adaptive non-Bayesian approach for adapting \u03b1 motivated by large-sample asymptotics, and call the resulting algorithm ASUGS (Adaptive SUGS). While the basic idea behind ASUGS is directly related to the greedy approach of SUGS, the main contribution is a novel low-complexity stable method for choosing the concentration parameter adaptively as new data arrive, which greatly improves the clustering per-\nformance. We derive an upper bound on the number of classes, logarithmic in the number of samples, and further prove that the sequence of concentration parameters that results from this adaptive design is almost bounded. We finally prove, that the conditional likelihood, which is the primary tool used for Bayesian-based online clustering, is asymptotically Gaussian in the large-sample limit, implying that the clustering part of ASUGS asymptotically behaves as a Gaussian classifier. Experiments show that our method outperforms other state-of-the-art methods for online learning of DPMM\u2019s.\nThe paper is organized as follows. In Section 2, we review the sequential inference framework for DPMM\u2019s that we will build upon, introduce notation and propose our adaptive modification. In Section 3, the probabilistic data model is given and sequential inference steps are shown. Section 4 contains the growth rate analysis of the number of classes and the adaptively-designed concentration parameters, and Section 5 contains the Gaussian large-sample approximation to the conditional likelihood. Experimental results are shown in Section 6 and we conclude in Section 7."}, {"heading": "2 Sequential Inference Framework for DPMM", "text": "Here, we review the SUGS framework of [15] for online clustering. Here, the nonparametric nature of the Dirichlet process manifests itself as modeling mixture models with countably infinite components. Let the observations be given by yi \u2208 Rd, and \u03b3i to denote the class label of the ith observation (a latent variable). We define the available information at time i as y(i) = {y1, . . . ,yi} and \u03b3(i\u22121) = {\u03b31, . . . , \u03b3i\u22121}. The online sequential updating and greedy search (SUGS) algorithm is summarized next for completeness. Set \u03b31 = 1 and calculate \u03c0(\u03b81|y1, \u03b31). For i \u2265 2,\n1. Choose best class label for yi:\n\u03b3i \u2208 arg max 1\u2264h\u2264ki\u22121+1\nP (\u03b3i = h|y(i), \u03b3(i\u22121)).\n2. Update the posterior distribution using yi, \u03b3i:\n\u03c0(\u03b8\u03b3i |y(i), \u03b3(i)) \u221d f(yi|\u03b8\u03b3i)\u03c0(\u03b8\u03b3i |y(i\u22121), \u03b3(i\u22121)).\nwhere \u03b8h are the parameters of class h, f(yi|\u03b8h) is the observation density conditioned on class h and ki\u22121 is the number of classes created at time i\u2212 1. The algorithm sequentially allocates observations yi to classes based on maximizing the conditional posterior probability.\nTo calculate the posterior probability P (\u03b3i = h|y(i), \u03b3(i\u22121)), define the variables:\nLi,h(yi) def = P (yi|\u03b3i = h,y(i\u22121), \u03b3(i\u22121)), \u03c0i,h(\u03b1) def = P (\u03b3i = h|\u03b1,y(i\u22121), \u03b3(i\u22121))\nFrom Bayes\u2019 rule, P (\u03b3i = h|y(i), \u03b3(i\u22121)) \u221d Li,h(yi)\u03c0i,h(\u03b1) for h = 1, . . . , ki\u22121+ 1. Here, \u03b1 is considered fixed at this iteration, and is not updated in a fully Bayesian manner.\nAccording to the Dirichlet process prediction, the predictive probability of assigning observation yi to a class h is:\n\u03c0i,h(\u03b1) = { mi\u22121(h) i\u22121+\u03b1 , h = 1, . . . , ki\u22121 \u03b1\ni\u22121+\u03b1 , h = ki\u22121 + 1 (1)\nwhere mi\u22121(h) = \u2211i\u22121\nl=1 I(\u03b3l = h) counts the number of observations labeled as class h at time i\u2212 1, and \u03b1 > 0 is the concentration parameter."}, {"heading": "2.1 Adaptation of Concentration Parameter \u03b1", "text": "It is well known that the concentration parameter \u03b1 has a strong influence on the growth of the number of classes [1]. Our experiments show that in this sequential framework, the choice of \u03b1 is even more critical. Choosing a fixed \u03b1 as in the online SVA algorithm of [8] requires cross-validation, which is computationally prohibitive for large-scale data sets. Furthermore, in the streaming data setting where no estimate on the data complexity exists, it is impractical to perform cross-validation. Although the parameter \u03b1 is handled from a fully Bayesian treatment in [15], a pre-specified grid of possible values \u03b1 can take, say {\u03b1l}Ll=1, along with the prior distribution over them, needs to be chosen in advance. Storage and updating of a matrix of size (ki\u22121 + 1) \u00d7 L and further marginalization is needed to compute P (\u03b3i = h|y(i), \u03b3(i\u22121)) at each iteration i. Thus, we propose an alternative data-driven method for choosing \u03b1 that works well in practice, is simple to compute and has theoretical guarantees.\nThe idea is to start with a prior distribution on \u03b1 that favors small \u03b1 and shape it into a posterior distribution using the data. Define pi(\u03b1) = p(\u03b1|y(i), \u03b3(i)) as the posterior distribution formed at time i, which will be used in ASUGS at time i+ 1. Let p1(\u03b1) \u2261 p1(\u03b1|y(1), \u03b3(1)) denote the prior for \u03b1, e.g., an exponential distribution p1(\u03b1) = \u03bbe\n\u2212\u03bb\u03b1. The dependence on y(i) and \u03b3(i) is trivial only at this first step. Then, by Bayes rule, pi(\u03b1) \u221d p(yi, \u03b3i|y(i\u22121), \u03b3(i\u22121), \u03b1)p(\u03b1|y(i\u22121), \u03b3(i\u22121)) \u221d pi\u22121(\u03b1)\u03c0i,\u03b3i(\u03b1) where \u03c0i,\u03b3i(\u03b1) is given in (1). Once this update is made after the selection of \u03b3i, the \u03b1 to be used in the next selection step is the mean of the distribution pi(\u03b1), i.e., \u03b1i = E[\u03b1|y(i), \u03b3(i)]. As will be shown in Section 5, the distribution pi(\u03b1) can be approximated by a Gamma distribution with shape parameter ki and rate parameter \u03bb + log i. Under this approximation, we have \u03b1i = ki \u03bb+log i , only requiring storage and update of one scalar parameter ki at each iteration i. The ASUGS algorithm is summarized in Algorithm 1. The selection step may be implemented by sampling the probability mass function {q(i)h }. The posterior update step can be efficiently performed by updating the\nAlgorithm 1 Adaptive Sequential Updating and Greedy Search (ASUGS)\nInput: streaming data {yi}\u221ei=1, rate parameter \u03bb > 0. Set \u03b31 = 1 and k1 = 1. Calculate \u03c0(\u03b81|y1, \u03b31). for i \u2265 2: do\n(a) Update concentration parameter:\n\u03b1i\u22121 = ki\u22121\n\u03bb+ log(i\u2212 1) .\n(b) Choose best label for yi: \u03b3i \u223c {q(i)h } = {\nLi,h(yi)\u03c0i,h(\u03b1i\u22121)\u2211 h\u2032 Li,h\u2032(yi)\u03c0i,h\u2032(\u03b1i\u22121)\n} .\n(c) Update posterior distribution:\n\u03c0(\u03b8\u03b3i |y(i), \u03b3(i)) \u221d f(yi|\u03b8\u03b3i)\u03c0(\u03b8\u03b3i |y(i\u22121), \u03b3(i\u22121)).\nend for\nhyperparameters as a function of the streaming data for the case of conjugate distributions. Section 3 derives these updates for the case of multivariate Gaussian observations and conjugate priors for the parameters."}, {"heading": "3 Sequential Inference under Unknown Mean &", "text": "Unknown Covariance\nWe consider the general case of an unknown mean and covariance for each class. The probabilistic model for the parameters of each class is given as:\nyi|\u00b5,T \u223c N (\u00b7|\u00b5,T), \u00b5|T \u223c N (\u00b7|\u00b50, coT), T \u223c W(\u00b7|\u03b40,V0) (2)\nwhere N (\u00b7|\u00b5,T) denotes the multivariate normal distribution with mean \u00b5 and precision matrix T, and W(\u00b7|\u03b4,V) is the Wishart distribution with 2\u03b4 degrees of freedom and scale matrix V. The parameters \u03b8 = (\u00b5,T) \u2208 Rd \u00d7 Sd++ follow a normal-Wishart joint distribution. The model (16) leads to closed-form expressions for Li,h(yi)\u2019s due to conjugacy [14].\nTo calculate the class posteriors, the conditional likelihoods of yi given assignment to class h and the previous class assignments need to be calculated first. The conditional likelihood of yi given assignment to class h and the history (y(i\u22121), \u03b3(i\u22121)) is given by:\nLi,h(yi) = \u222b f(yi|\u03b8h)\u03c0(\u03b8h|y(i\u22121), \u03b3(i\u22121))d\u03b8h (3)\nDue to the conjugacy of the distributions, the posterior \u03c0(\u03b8h|y(i\u22121), \u03b3(i\u22121)) always has the form:\n\u03c0(\u03b8h|y(i\u22121), \u03b3(i\u22121)) = N (\u00b5h|\u00b5 (i\u22121) h , c (i\u22121) h Th)W(Th|\u03b4 (i\u22121) h ,V (i\u22121) h )\nwhere \u00b5 (i\u22121) h , c (i\u22121) h , \u03b4 (i\u22121) h ,V (i\u22121) h are hyperparameters that can be recursively computed as new samples come in. The form of this recursive computation of the hyperparameters is derived in Appendix A. For ease of interpretation and numerical stability, we define \u03a3 (i) h := (V (i) h ) \u22121\n2\u03b4 (i) h\nas the inverse\nof the mean of the Wishart distributionW(\u00b7|\u03b4(i)h ,V (i) h ). The matrix \u03a3 (i) h has the natural interpretation as the covariance matrix of class h at iteration i. Once the \u03b3ith component is chosen, the parameter updates for the \u03b3ith class become:\n\u00b5(i)\u03b3i = 1\n1 + c (i\u22121) \u03b3i\nyi + c\n(i\u22121) \u03b3i\n1 + c (i\u22121) \u03b3i\n\u00b5(i\u22121)\u03b3i (4)\nc(i)\u03b3i = c (i\u22121) \u03b3i + 1 (5)\n\u03a3(i)\u03b3i = 2\u03b4\n(i\u22121) \u03b3i\n1 + 2\u03b4 (i\u22121) \u03b3i\n\u03a3(i\u22121)\u03b3i + 1\n1 + 2\u03b4 (i\u22121) \u03b3i\nc (i\u22121) \u03b3i\n1 + c (i\u22121) \u03b3i\n(yi \u2212 \u00b5(i\u22121)\u03b3i )(yi \u2212 \u00b5 (i\u22121) \u03b3i ) T\n(6)\n\u03b4(i)\u03b3i = \u03b4 (i\u22121) \u03b3i +\n1 2 (7)\nIf the starting matrix \u03a3 (0) h is positive definite, then all the matrices {\u03a3 (i) h } will remain positive definite. Let us return to the calculation of the conditional likelihood (17). By iterated integration, it follows that:\nLi,h(yi) \u221d\n( r\n(i\u22121) h\n2\u03b4 (i\u22121) h\n)d/2 \u03c1d(\u03b4 (i\u22121) h ) det(\u03a3 (i\u22121) h )\n\u22121/2( 1 + r (i\u22121) h\n2\u03b4 (i\u22121) h\n(yi \u2212 \u00b5(i\u22121)h )T (\u03a3 (i\u22121) h ) \u22121(yi \u2212 \u00b5(i\u22121)h ) )\u03b4(i\u22121)h + 12\n(8)\nwhere \u03c1d(a) def =\n\u0393(a+ 1 2 ) \u0393(a+ 1\u2212d 2 ) and r (i\u22121) h def = c (i\u22121) h 1+c (i\u22121) h . A detailed mathematical\nderivation of this conditional likelihood is included in Appendix B. We remark that for the new class h = ki\u22121 + 1, Li,ki\u22121+1 has the form (22) with the initial choice of hyperparameters r(0), \u03b4(0),\u00b5(0),\u03a3(0)."}, {"heading": "4 Growth Rate Analysis of Number of Classes &", "text": "Stability\nIn this section, we derive a model for the posterior distribution pn(\u03b1) using large-sample approximations, which will allow us to derive growth rates on\nthe number of classes and the sequence of concentration parameters, showing that the number of classes grows as E[kn] = O(log1+ n) for arbitarily small under certain mild conditions.\nThe probability density of the \u03b1 parameter is updated at the jth step in the following fashion:\npj+1(\u03b1) \u221d pj(\u03b1) \u00b7\n{ \u03b1 j+\u03b1 innovation class chosen\n1 j+\u03b1 otherwise\n,\nwhere only the \u03b1-dependent factors in the update are shown. The \u03b1independent factors are absorbed by the normalization to a probability density. Choosing the innovation class pushes mass toward infinity while choosing any other class pushes mass toward zero. Thus there is a possibility that the innovation probability grows in a undesired manner. We assess the growth of the number of innovations rn def = kn \u2212 1 under simple assumptions on some likelihood functions that appear naturally in the ASUGS algorithm.\nAssuming that the initial distribution of \u03b1 is p1(\u03b1) = \u03bbe \u2212\u03bb\u03b1, the distribution used at step n + 1 is proportional to \u03b1rn \u220fn\u22121 j=1 (1 + \u03b1 j ) \u22121e\u2212\u03bb\u03b1. We make use of the limiting relation\nTheorem 1. The following asymptotic behavior holds:\nlim n\u2192\u221e\nlog \u220fn\u22121 j=1 (1 + \u03b1 j )\n\u03b1 log n = 1.\nProof. See Appendix C.\nUsing Theorem 1, a large-sample model for pn(\u03b1) is \u03b1 rne\u2212(\u03bb+logn)\u03b1, suitably normalized. Recognizing this as the Gamma distribution with shape parameter rn+1 and rate parameter \u03bb+log n, its mean is given by \u03b1n = rn+1 \u03bb+logn . We use the mean in this form to choose class membership in Alg. 1. This asymptotic approximation leads to a very simple scalar update of the concentration parameter; there is no need for discretization for tracking the evolution of continuous probability distributions on \u03b1. In our experiments, this approximation is very accurate.\nRecall that the innovation class is labeled K+ = kn\u22121 + 1 at the n th step. The modeled updates randomly select a previous class or innovation (new class) by sampling from the probability distribution {q(n)k = P (\u03b3n = k|y(n), \u03b3(n\u22121))}K+k=1. Note that n\u2212 1 = \u2211 k 6=K+ mn(k) , where mn(k) represents the number of members in class k at time n. We assume the data follows the Gaussian mixture distribution:\npT (y) def = K\u2211 h=1 \u03c0hN (y|\u00b5h,\u03a3h) (9)\nwhere \u03c0h are the prior probabilities, and \u00b5h,\u03a3h are the parameters of the Gaussian clusters.\nDefine the mixture-model probability density function, which plays the role of the predictive distribution:\nL\u0303n,K+(y) def = \u2211 k 6=K+ mn\u22121(k) n\u2212 1 Ln,k(y), (10)\nso that the probabilities of choosing a previous class or an innovation (using Eq. 1) are proportional to \u2211\nk 6=K+ mn\u22121(k) n\u22121+\u03b1n\u22121Ln,k(yn) = (n\u22121) n\u22121+\u03b1n\u22121 L\u0303n,K+(yn)\nand \u03b1n\u22121n\u22121+\u03b1n\u22121Ln,K+(yn), respectively. If \u03c4n\u22121 denotes the innovation probability at step n, then we have(\n\u03c1n\u22121 \u03b1n\u22121Ln,K+(yn)\nn\u2212 1 + \u03b1n\u22121 , \u03c1n\u22121 (n\u2212 1)L\u0303n,K+(yn) n\u2212 1 + \u03b1n\u22121\n) = (\u03c4n\u22121, 1\u2212 \u03c4n\u22121) (11)\nfor some positive proportionality factor \u03c1n\u22121. Define the likelihood ratio (LR) at the beginning of stage n as 1:\nln(y) def =\nLn,K+(y) L\u0303n,K+(y) (12)\nConceptually, the mixture (10) represents a modeled distribution fitting the currently observed data. If all \u201cmodes\u201d of the data have been observed, it is reasonable to expect that L\u0303n,K+ is a good model for future observations. The LR ln(yn) is not large when the future observations are well-modeled by (10). In fact, we expect L\u0303n,K+ \u2192 pT as n\u2192\u221e, as discussed in Section 5.\nLemma 1. The following bound holds:\n\u03c4n\u22121 = ln(yn)\u03b1n\u22121\nn\u2212 1 + ln(yn)\u03b1n\u22121 \u2264 min ( ln(yn)\u03b1n\u22121 n\u2212 1 , 1 ) .\nProof. The result follows directly from (11) after a simple calculation.\nThe innovation random variable rn is described by the random process associated with the probabilities of transition\nP (rn+1 = k|rn) = { \u03c4n, k = rn + 1 1\u2212 \u03c4n, k = rn . (13)\nThe expectation of rn is majorized by the expectation of a similar random process, r\u0304n, based on the transition probability \u03c3n def = min( rn+1an , 1) instead\n1Here, L0(\u00b7) def = Ln,K+(\u00b7) is independent of n and only depends on the initial choice of\nhyperparameters as discussed in Sec. 3.\nof \u03c4n as Appendix D shows, where the random sequence {an} is given by ln+1(yn+1)\n\u22121n(\u03bb+ log n). The latter can be described as a modification of a Polya urn process with selection probability \u03c3n. The asymptotic behavior of rn and related variables is described in the following theorem.\nTheorem 2. Let \u03c4n be a sequence of real-valued random variables 0 \u2264 \u03c4n \u2264 1 satisfying \u03c4n \u2264 rn+1an for n \u2265 N , where an = ln+1(yn+1)\n\u22121n(\u03bb+ logn), and where the nonnegative, integer-valued random variables rn evolve according to (13). Assume the following for n \u2265 N :\n1. ln(yn) \u2264 \u03b6 (a.s.)\n2. D(pT \u2016 L\u0303n,K+) \u2264 \u03b4 (a.s.)\nwhere D(p \u2016 q) is the Kullback-Leibler divergence between distributions p(\u00b7) and q(\u00b7). Then, as n\u2192\u221e,\nrn = OP (log 1+\u03b6 \u221a \u03b4/2 n), \u03b1n = OP (log \u03b6 \u221a \u03b4/2 n) (14)\nProof. See Appendix E.\nTheorem 2 bounds the growth rate of the mean of the number of class innovations and the concentration parameter \u03b1n in terms of the sample size n and parameter \u03b6. The bounded LR and bounded KL divergence conditions of Thm. 2 manifest themselves in the rate exponents of (14). The experiments section shows that both of the conditions of Thm. 2 hold for all iterations n \u2265 N for some N \u2208 N. In fact, assuming the correct clustering, the mixture distribution L\u0303n,kn\u22121+1 converges to the true mixture distribution pT , implying that the number of class innovations grows at most as O(log1+ n) and the sequence of concentration parameters is O(log n), where > 0 can be arbitrarily small."}, {"heading": "5 Asymptotic Normality of Conditional Likelihood", "text": "In this section, we derive an asymptotic expression for the conditional likelihood (22) in order to gain insight into the steady-state of the algorithm.\nWe let \u03c0h denote the true prior probability of class h. Using the bounds of the Gamma function in Theorem 1.6 from [2], it follows that lima\u2192\u221e \u03c1d(a)\ne\u2212d/2(a\u22121/2)d/2 =\n1. Under normal convergence conditions of the algorithm (with the pruning and merging steps included), all classes h = 1, . . . ,K will be correctly identified and populated with approximately ni\u22121(h) \u2248 \u03c0h(i\u22121) observations at time i\u22121. Thus, the conditional class prior for each class h converges to \u03c0h as i\u2192\u221e, in virtue of (14), \u03c0i,h(\u03b1i\u22121) = ni\u22121(h)i\u22121+\u03b1i\u22121 = \u03c0h\n1+ OP (log\n\u03b6 \u221a \u03b4/2(i\u22121))\ni\u22121\ni\u2192\u221e\u2212\u2192 \u03c0h.\nAccording to (5), we expect r (i\u22121) h \u2192 1 as i \u2192 \u221e since c (i\u22121) h \u223c \u03c0h(i \u2212 1).\nAlso, we expect 2\u03b4 (i\u22121) h \u223c \u03c0h(i \u2212 1) as i \u2192 \u221e according to (7). Also, from before, \u03c1d(\u03b4 (i\u22121) h ) \u223c e \u2212d/2(\u03b4 (i\u22121) h \u2212 1/2) d/2 \u223c e\u2212d/2(\u03c0h i\u221212 \u2212 1 2) d/2. The parameter updates (4)-(7) imply \u00b5 (i) h \u2192 \u00b5h and \u03a3 (i) h \u2192 \u03a3h as i \u2192 \u221e. This follows from the strong law of large numbers, as the updates are recursive implementations of the sample mean and sample covariance matrix. Thus, the large-sample approximation to the conditional likelihood becomes:\nLi,h(yi) i\u2192\u221e\u221d\nlimi\u2192\u221e\n( 1 +\n\u03c0\u22121h i\u22121 (yi \u2212 \u00b5 (i\u22121) h ) T (\u03a3 (i\u22121) h ) \u22121(yi \u2212 \u00b5(i\u22121)h ) )\u2212 i\u22121 2\u03c0\u22121 h\nlimi\u2192\u221e det(\u03a3 (i\u22121) h ) 1/2\ni\u2192\u221e\u221d e \u2212 1 2 (yi\u2212\u00b5h)T\u03a3\u22121h (yi\u2212\u00b5h) \u221a\ndet \u03a3h (15)\nwhere we used limu\u2192\u221e(1 + c u) u = ec. The conditional likelihood (15) corresponds to the multivariate Gaussian distribution with mean \u00b5h and covariance matrix \u03a3h. A similar asymptotic normality result was recently obtained in [13] for Gaussian observations with a von Mises prior. The asymptotics mn\u22121(h)n\u22121 \u2192 \u03c0h, \u00b5 (n) h \u2192 \u00b5h,\u03a3 (n) h \u2192 \u03a3h, Ln,h(y)\u2192 N (y|\u00b5h,\u03a3h) as n \u2192 \u221e imply that the mixture distribution L\u0303n,K+ in (10) converges to the true Gaussian mixture distribution pT of (9). Thus, for any small \u03b4, we expect D(pT \u2016 L\u0303n,K+) \u2264 \u03b4 for all n \u2265 N , validating the assumption of Theorem 2."}, {"heading": "5.1 Prune & Merge", "text": "It is possible that multiple clusters are similar and classes might be created due to outliers, or due to the particular ordering of the streaming data sequence, as also noted in [8]. These effects can be mitigated by adding a pruning and merging step in the ASUGS algorithm.\nThe pruning step may be implemented as follows. Define w (i) h\ndef = \u2211i\nj=1 q (j) h ,\ni.e., the running sum of the posterior weights. The relative weight of each component at the ith iteration may be computed as w\u0303 (i) h =\nw (i) h\u2211 k w (i) k . If\nw\u0303 (i) h < r, then the component is removed.\nThe merging can be implemented by merging two clusters k1 and k2, once the `1 distance between the posteriors over time falls below a threshold\nd. This distance is measured as dq(k1, k2) = 1 i \u2211i j=1 |q (j) k1 \u2212 q(j)k2 |. This criterion can be implemented in an online fashion by implementing the distance computation recursively. The sufficient statistics are also merged by taking convex combinations \u00b5 (i) k1 \u2190 \u03b1(i)\u00b5(i)k1 + (1 \u2212 \u03b1 (i))\u00b5 (i) k2 and \u03a3 (i) k1 \u2190 \u03b1(i)\u03a3 (i) k1 +(1\u2212\u03b1(i))\u03a3(i)k2 , and by adding c (i) k1 \u2190 c(i)k1 +c (i) k2 and \u03b4 (i) k1 \u2190 \u03b4(i)k1 +\u03b4 (i) k2 ."}, {"heading": "6 Experiments", "text": "We apply the ASUGS learning algorithm to a synthetic 16-class example and to a real data set, to verify the stability and accuracy of our method. The experiments show the value of adaptation of the Dirichlet concentration parameter for online clustering and parameter estimation.\nSince it is possible that multiple clusters are similar and classes might be created due to outliers, or due to the particular ordering of the streaming data sequence, we add the pruning and merging step in the ASUGS algorithm as done in [8]. We compare ASUGS and ASUGS-PM with SUGS, SUGS-PM, SVA and SVA-PM proposed in [8], since it was shown in [8] that SVA and SVA-PM outperform the block-based methods that perform iterative updates over the entire data set including Collapsed Gibbs Sampling, MCMC with Split-Merge and Truncation-Free Variational Inference."}, {"heading": "6.1 Synthetic Data set", "text": "We consider learning the parameters of a 16-class Gaussian mixture each with equal variance of \u03c32 = 0.025. The training set was made up of 500 iid samples, and the test set was made up of 1000 iid samples. The clustering results are shown in Fig. 1(a), showing that the ASUGS-based approaches are more stable than SVA-based algorithms. ASUGS-PM performs best and identifies the correct number of clusters, and their parameters. Fig. 1(b) shows the data log-likelihood on the test set (averaged over 100 Monte Carlo trials), the mean and variance of the number of classes at each iteration. The ASUGS-based approaches achieve a higher log-likelihood than SVA-based approaches asymptotically. Fig. 6.1 provides some numerical verification for the assumptions of Theorem 2. As expected, the predictive likelihood L\u0303i,K+ (10) converges to the true mixture distribution pT (9), and the likelihood ratio li(yi) is bounded after enough samples are processed."}, {"heading": "6.2 Real Data Set", "text": "We applied the online nonparametric Bayesian methods for clustering image data. We used the MNIST data set, which consists of 60, 000 training samples, and 10, 000 test samples. Each sample is a 28\u00d728 image of a handwritten digit (total of 784 dimensions), and we perform PCA pre-processing to reduce dimensionality to d = 50 dimensions as in [7].\nWe use only a random 1.667% subset, consisting of 1000 random samples for training. This training set contains data from all 10 digits with an approximately uniform proportion. Fig. 3 shows the predictive log-likelihood over the test set, and the mean images for clusters obtained using ASUGSPM and SVA-PM, respectively. We note that ASUGS-PM achieves higher log-likelihood values and finds all digits correctly using only 23 clusters,\nwhile SVA-PM finds some digits using 56 clusters. Furthermore, the SVAPM results in noisy-looking image clusters, while ASUGS-PM consistently has clear digits."}, {"heading": "6.3 Discussion", "text": "Although both SVA and ASUGS methods have similar computational complexity and use decisions and information obtained from processing previous samples in order to decide on class innovations, the mechanics of these methods are quite different. ASUGS uses an adaptive \u03b1 motivated by asymptotic theory, while SVA uses a fixed \u03b1. Furthermore, SVA updates the parameters of all the components at each iteration (in a weighted fashion) while ASUGS only updates the parameters of the most-likely cluster, thus minimizing leakage to unrelated components. The \u03bb parameter of ASUGS does not affect performance as much as the threshold parameter of SVA does, which often leads to instability requiring lots of pruning and merging steps and increasing latency. This is critical for large data sets or streaming applications, because cross-validation would be required to set appropriately. We observe higher log-likelihoods and better numerical stability for ASUGS-based\nmethods in comparison to SVA. The mathematical formulation of ASUGS allows for theoretical guarantees (Theorem 2), and asymptotically normal predictive distribution."}, {"heading": "7 Conclusion", "text": "We developed a fast online clustering and parameter estimation algorithm for Dirichlet process mixtures of Gaussians, capable of learning in a single data pass. Motivated by large-sample asymptotics, we proposed a novel low-complexity data-driven adaptive design for the concentration parameter and showed it leads to logarithmic growth rates on the number of classes. Through experiments on synthetic and real data sets, we show our method achieves better performance and is as fast as other state-of-the-art online learning DPMM methods."}, {"heading": "A Appendix A", "text": "We consider the general case of an unknown mean and covariance for each class. Let T denote the precision (or inverse covariance) matrix. The probabilistic model for the mean and covariance matrix of each class is given as:\nyi|\u00b5,T \u223c N (\u00b7|\u00b5,T) \u00b5|T \u223c N (\u00b7|\u00b50, coT)\nT \u223c W(\u00b7|\u03b40,V0) (16)\nwhere N (\u00b7|\u00b5,T) denote the observation density which is assumed to be multivariate normal with mean \u00b5 and precision matrix T. The parameters \u03b8 = (\u00b5,T) \u2208 \u21261 \u00d7 \u21262 follow a normal-Wishart joint distribution. The domains here are \u21261 = Rd and \u21262 = Sd++ is the positive definite cone. This leads to closed-form expressions for Li,h(yi)\u2019s due to conjugacy [14]. For concreteness, let us write the distributions of the model (16):\nf(yi|\u03b8) = p(yi|\u00b5,T) = det(T)1/2\n(2\u03c0)d/2 exp\n( \u22121\n2 (yi \u2212 \u00b5)TT(yi \u2212 \u00b5) ) p(\u00b5|T) = p(\u03b81|\u03982) = det(c0T) 1/2\n(2\u03c0)d/2 exp\n( \u2212c0\n2 (\u00b5\u2212 \u00b50)TT(\u00b5\u2212 \u00b50) ) p(T) = p(\u03982) = det(V0) \u2212\u03b40\n2d\u03b40\u0393d(\u03b40) det(T)\u03b40\u2212\nd+1 2 exp(\u22121\n2 tr(V\u221210 T))\nwhere \u0393d(\u00b7) is the multivariate Gamma function. To calculate the class posteriors, the conditional likelihoods of yi given assignment to class h and the previous class assignments need to be calculated first. We derive closed-form expressions for these quantities in this section under the probabilistic model (16).\nThe conditional likelihood of yi given assignment to class h and the history (y(i\u22121), \u03b3(i\u22121)) is given by:\nLi,h(yi) = \u222b f(yi|\u03b8h)\u03c0(\u03b8h|y(i\u22121), \u03b3(i\u22121))d\u03b8h (17)\nWe thus need to obtain an expression for the posterior distribution \u03c0(\u03b8h|y(i\u22121), \u03b3(i\u22121)). Due to the conjugacy of the distributions involved in (16), the posterior distribution \u03c0(\u03b8h|y(i\u22121), \u03b3(i\u22121)) always has the form:\n\u03c0(\u03b8h|y(i\u22121), \u03b3(i\u22121)) = N (\u00b5h|\u00b5 (i\u22121) h , c (i\u22121) h Th)W(Th|\u03b4 (i\u22121) h ,V (i\u22121) h ) (18)\nwhere \u00b5 (i\u22121) h , c (i\u22121) h , \u03b4 (i\u22121) h ,V (i\u22121) h are hyperparameters that can be recursively computed as new samples come in. This would greatly simplify the\ncomputational complexity of the second step of the SUGS algorithm. Next, we derive the form of this recursive computation of the hyperparameters.\nFor simplicity of the derivation, let us consider the initial case y = y1. Then, from Bayes\u2019 rule:\np(\u03b8|y) = p(\u00b5,T|y) = p(\u00b5|T,y)p(T|y)\nA.1 Calculation of p(\u00b5|T,y)\nNote the factorization:\np(\u00b5|T,y) \u221d p(y|\u00b5,T)p(\u00b5|T)\nAccording to (16), we can write:\ny = \u00b5 + \u03a31/2\n\u00b5 = \u00b50 + \u03a3 1/2 0 \u2032\nwhere \u223c N(0, I), \u2032 \u223c N(0, I), is independent of \u2032 and \u03a3 = T\u22121,\u03a30 = (c0T)\n\u22121. From this, it follows that the conditional density p(\u00b5|T,y) is also multivariate normal with mean E[\u00b5|T,y] and covariance Cov(\u00b5|T,y). Note that:\nE[y|T] = \u00b50 Cov(y|T) = E[Cov(y|\u00b5,T)|T] + Cov(E[y|\u00b5,T]|T)\n= \u03a3 + \u03a30 = (1 + c \u22121 0 )T \u22121\nCov(\u00b5,y|T) = \u03a30\nUsing these facts, we obtain:\nE[\u00b5|T,y] = E[\u00b5|T] + Cov(\u00b5,y|T)Cov(y|T)\u22121(y \u2212 E[y|T]) = \u00b50 + c \u22121 0 T \u22121((1 + c\u221210 )T \u22121)\u22121(y \u2212 \u00b50)\n= \u00b50 + c \u22121 0 (1 + c \u22121 0 ) \u22121(y \u2212 \u00b50) = 1\n1 + c0 y + c0 1 + c0 \u00b50\nCov(\u00b5|T,y) = Cov(\u00b5|T)\u2212 Cov(\u00b5,y|T)Cov(y|T)\u22121Cov(y,\u00b5|T) = \u03a30 \u2212\u03a30(\u03a3 + \u03a30)\u22121\u03a3T0\n= c\u221210\n( 1\u2212 c \u22121 0\n1 + c\u221210\n) T\u22121\n= c\u221210\n1 + c\u221210 T\u22121\nThus, we have:\np(\u00b5|T,y) = N ( \u00b5 \u2223\u2223\u2223\u2223\u2223 11 + c0 y + c01 + c0\u00b50, (1 + c0)T )\nwhere the conditional precision matrix becomes (1 + c0)T. As a result, once the \u03b3ith component is chosen in the SUGS selection step, the parameter updates for the \u03b3ith class become:\n\u00b5(i)\u03b3i = 1\n1 + c (i\u22121) \u03b3i\nyi + c\n(i\u22121) \u03b3i\n1 + c (i\u22121) \u03b3i\n\u00b5(i\u22121)\u03b3i\nc(i)\u03b3i = c (i\u22121) \u03b3i + 1\nA.2 Calculation of p(T|y) Next, we focus on calculating p(T|y) = \u222b Rd p(T,\u00b5|y)d\u00b5, where\np(T,\u00b5|y) \u221d p(y|T,\u00b5)p(\u00b5|T)p(T)\n\u221d det(T)(\u03b40+1/2)\u2212 d+1 2 det(T)1/2 exp ( \u22121\n2 tr(V\u221210 T) ) \u00d7 exp ( \u22121\n2\n[ c0(\u00b5\u2212 \u00b50)TT(\u00b5\u2212 \u00b50) + (y \u2212 \u00b5)TT(y \u2212 \u00b5) ]) Rewriting the term inside the brackets by completing the square, we obtain:\nc0(\u00b5\u2212 \u00b50)TT(\u00b5\u2212 \u00b50) + (y \u2212 \u00b5)TT(y \u2212 \u00b5) = c0\u2016T1/2\u00b5\u2212T1/2\u00b50\u201622 + \u2016T 1/2y \u2212T1/2\u00b5\u201622\n= (1 + c0) { \u2016T1/2\u00b5\u201622 \u2212 2 \u2329 T1/2\u00b5, c0T 1/2\u00b50 + T 1/2y\n1 + c0\n\u232a + c0\u2016T1/2\u00b50\u201622 + \u2016T1/2y\u2016 2 2\n1 + c0\n}\n= (1 + c0)\n{ \u2016T1/2\u00b5\u2212 c0T 1/2\u00b50 + T 1/2y\n1 + c0 \u201622 \u2212 \u2016\nc0T 1/2\u00b50 + T 1/2y\n1 + c0 \u201622 +\nc0\u2016T1/2\u00b50\u201622 + \u2016T1/2y\u2016 2 2\n1 + c0\n}\nIntegrating out \u00b5, we obtain:\u222b exp ( \u22121\n2\n[ c0(\u00b5\u2212 \u00b50)TT(\u00b5\u2212 \u00b50) + (y \u2212 \u00b5)TT(y \u2212 \u00b5) ]) d\u00b5\n= exp ( \u22121 + c0\n2\n( c0\u2016T1/2\u00b50\u201622 + \u2016T1/2y\u2016 2 2\n1 + c0 \u2212 \u2016c0T\n1/2\u00b50 + T 1/2y\n1 + c0 \u201622\n))\n\u00d7 \u222b\nexp(\u22121 2 \u2016T1/2\u00b5\u2212 c0T\n1/2\u00b50 + T 1/2y\n1 + c0 \u201622)d\u00b5 \u221d det(T)\u22121/2 exp ( \u22121\n2 c0 1 + c0\n(y \u2212 \u00b50)TT(y \u2212 \u00b50) )\nUsing this result, we obtain:\np(T|y) \u221d det(T)(\u03b40+1/2)\u2212 d+1 2 exp ( \u22121\n2 tr\n( T { V\u221210 +\nc0 1 + c0\n(y \u2212 \u00b50)(y \u2212 \u00b50)T }))\nAs a result, the conditional density is recognized to be a Wishart distribution\nW ( T \u2223\u2223\u2223\u2223\u2223\u03b40 + 12 , { V\u221210 + c0 1 + c0 (y \u2212 \u00b50)(y \u2212 \u00b50)T }\u22121) .\nThus, the parameter updates for the \u03b3ith class become:\n\u03b4(i)\u03b3i = \u03b4 (i\u22121) \u03b3i +\n1\n2\nV(i)\u03b3i = { (V(i\u22121)\u03b3i ) \u22121 + c (i\u22121) \u03b3i\n1 + c (i\u22121) \u03b3i\n(yi \u2212 \u00b5(i\u22121)\u03b3i )(yi \u2212 \u00b5 (i\u22121) \u03b3i ) T\n}\u22121 (19)\nFor numerical stability and ease of interpretation, we define\n\u03a3 (i) h :=\n(V (i) h ) \u22121\n2\u03b4 (i) h\n.\nThis is the inverse of the mean of the Wishart distribution W(\u00b7|\u03b4(i)h ,V (i) h ), and can be interpreted as the covariance matrix of class h at iteration i. From (19), we have:\n\u03a3 (i) h =\n(V (i) h ) \u22121\n2\u03b4 (i) h\n= 2\u03b4\n(i\u22121) h\n2\u03b4 (i) h\n(V (i\u22121) h ) \u22121\n2\u03b4 (i\u22121) h\n+ 1\n2\u03b4 (i) h\nc (i\u22121) h\n1 + c (i\u22121) h\n(yi \u2212 \u00b5(i\u22121)h )(yi \u2212 \u00b5 (i\u22121) h ) T\n= 2\u03b4\n(i\u22121) h\n1 + 2\u03b4 (i\u22121) h\n\u03a3 (i\u22121) h +\n1\n1 + 2\u03b4 (i\u22121) h\nc (i\u22121) h\n1 + c (i\u22121) h\n(yi \u2212 \u00b5(i\u22121)h )(yi \u2212 \u00b5 (i\u22121) h ) T\nThus, the recursive updates (19) can be equivalently restated as:\n\u03b4(i)\u03b3i = \u03b4 (i\u22121) \u03b3i +\n1\n2\n\u03a3 (i) h =\n2\u03b4 (i\u22121) h\n1 + 2\u03b4 (i\u22121) h\n\u03a3 (i\u22121) h +\n1\n1 + 2\u03b4 (i\u22121) h\nc (i\u22121) h\n1 + c (i\u22121) h\n(yi \u2212 \u00b5(i\u22121)h )(yi \u2212 \u00b5 (i\u22121) h ) T\nIf the starting matrix \u03a3 (0) h is positive definite, then all the matrices {\u03a3 (i) h } will remain positive definite."}, {"heading": "B Appendix B", "text": "Now, let us return to the calculation of (17).\nLi,h(yi) = \u222b Sd++ \u222b Rd N (yi|\u00b5,T)N (\u00b5|\u00b5(i\u22121)h , c (i\u22121) h T)W(T|\u03b4 (i\u22121) h ,V (i\u22121) h )d\u00b5dT\n= \u222b Sd++ W(T|\u03b4(i\u22121)h ,V (i\u22121) h ) {\u222b Rd N (yi|\u00b5,T)N (\u00b5|\u00b5(i\u22121)h , c (i\u22121) h T)d\u00b5 } dT\nEvaluating the inner integral within the brackets:\u222b Rd N (yi|\u00b5,T)N (\u00b5|\u00b5(i\u22121)h , c (i\u22121) h T)d\u00b5\n\u221d det(T)1/2 det(c(i\u22121)h T) 1/2 \u00d7 \u222b Rd exp ( \u22121 2 [ c (i\u22121) h (\u00b5\u2212 \u00b5 (i\u22121) h ) TT(\u00b5\u2212 \u00b5(i\u22121)h ) + (yi \u2212 \u00b5) TT(yi \u2212 \u00b5) ]) d\u00b5 = det(T)1/2 det(c (i\u22121) h T) 1/2\n\u00d7 exp ( \u22121\n2\nc (i\u22121) h\n1 + c (i\u22121) h\n(yi \u2212 \u00b5(i\u22121)h ) TT(yi \u2212 \u00b5(i\u22121)h )\n)\n\u00d7 \u222b exp ( \u2212 1 + c (i\u22121) h\n2 (\u00b5\u2212 b)TT(\u00b5\u2212 b)\n) d\u00b5\n\u221d det(T)1/2 det(c\n(i\u22121) h T) 1/2\ndet((1 + c (i\u22121) h )T)\n1/2 exp\n( \u22121\n2 tr\n( T { c (i\u22121) h\n1 + c (i\u22121) h\n(yi \u2212 \u00b5(i\u22121)h )(yi \u2212 \u00b5 (i\u22121) h ) T\n}))\n=\n( c\n(i\u22121) h\n1 + c (i\u22121) h\n)d/2 det(T)1/2 exp ( \u22121\n2 tr\n( T { c (i\u22121) h\n1 + c (i\u22121) h\n(yi \u2212 \u00b5(i\u22121)h )(yi \u2212 \u00b5 (i\u22121) h ) T\n}))\nUsing this closed-form expression for the inner integral, we further obtain:\nLi,h(yi) \u221d\n( c\n(i\u22121) h\n1 + c (i\u22121) h )d/2 \u222b Sd++ det(V (i\u22121) h ) \u2212\u03b4(i\u22121)h 2d\u03b4 (i\u22121) h \u0393d(\u03b4 (i\u22121) h ) det(T)(\u03b4 (i\u22121) h +1/2)\u2212 d+1 2\n\u00d7 exp ( \u22121\n2 tr\n( T { (V\n(i\u22121) h ) \u22121 + c\n(i\u22121) h\n1 + c (i\u22121) h\n(yi \u2212 \u00b5(i\u22121)h )(yi \u2212 \u00b5 (i\u22121) h ) T\n})) dT\n(20)\n\u221d\n( c\n(i\u22121) h\n1 + c (i\u22121) h\n)d/2 \u0393d(\u03b4 (i\u22121) h + 1 2)\n\u0393d(\u03b4 (i\u22121) h )\n\u00d7 det(V\n(i\u22121) h ) \u2212\u03b4(i\u22121)h\ndet ({ (V\n(i\u22121) h )\n\u22121 + c (i\u22121) h\n1+c (i\u22121) h\n(yi \u2212 \u00b5(i\u22121)h )(yi \u2212 \u00b5 (i\u22121) h ) T\n}\u22121)\u2212(\u03b4(i\u22121)h + 12 )\n= ( r\n(i\u22121) h )d/2 \u0393d(\u03b4(i\u22121)h + 12) \u0393d(\u03b4 (i\u22121) h )\ndet((V (i\u22121) h ) \u22121)\u22121/2\ndet ( Id + r (i\u22121) h (yi \u2212 \u00b5 (i\u22121) h )(yi \u2212 \u00b5 (i\u22121) h ) TV (i\u22121) h )\u03b4(i\u22121)h + 12 = ( r\n(i\u22121) h )d/2 \u0393d(\u03b4(i\u22121)h + 12) \u0393d(\u03b4 (i\u22121) h )\ndet(V (i\u22121) h )\n1/2( 1 + r\n(i\u22121) h (yi \u2212 \u00b5 (i\u22121) h ) TV (i\u22121) h (yi \u2212 \u00b5 (i\u22121) h ) )\u03b4(i\u22121)h + 12 (21)\n=\n( r\n(i\u22121) h\n2\u03b4 (i\u22121) h\n)d/2 \u0393d(\u03b4 (i\u22121) h + 1 2)\n\u0393d(\u03b4 (i\u22121) h )\ndet((\u03a3 (i\u22121) h ) \u22121)1/2( 1 + r (i\u22121) h\n2\u03b4 (i\u22121) h\n(yi \u2212 \u00b5(i\u22121)h )T (\u03a3 (i\u22121) h ) \u22121(yi \u2212 \u00b5(i\u22121)h ) )\u03b4(i\u22121)h + 12\n(22)\nwhere we used the determinant identity det(I + abTM) = 1 + bTMa in the\nlast step. We also defined r (i) h :=\nc (i) h\n1+c (i) h\nand used V (i) h =\n(\u03a3 (i) h ) \u22121\n2\u03b4 (i) h\n."}, {"heading": "C Appendix C", "text": "Proof. It is sufficient to establish the limit for limN\u2192\u221e \u2211N\nk=m log(1+\u03b1/k)/ logN for fixed m. Choose m such that |\u03b1| < m\u22121 and use log(1\u2212x) = \u2211\u221e k=1 x\nk/k for |x| < 1 to get\nN\u2211 k=m log ( 1 + \u03b1 k ) = \u221e\u2211 l=1 (\u22121)l+1\u03b1 l l N\u2211 k=m 1 kl . (23)\nSeparate (23) into two terms:\n\u221e\u2211 l=1 (\u22121)l+1\u03b1 l l N\u2211 k=m 1 kl = \u03b1 N\u2211 k=m 1 k + \u221e\u2211 l=2 (\u22121)l+1\u03b1 l l N\u2211 k=m 1 kl . (24)\nThe first term is expressed in terms of the Euler-Mascheroni constant \u03b3e as\nN\u2211 k=m 1 k = logN \u2212 \u03b3e \u2212 m\u22121\u2211 k=1 1 k + o(1).\nThus, dividing by logN and taking the limit N \u2192 \u221e we have a limiting value of unity. The second term of (24) is bounded. To see this, use, for l > 1,\n\u221e\u2211 k=m 1 kl \u2264 \u222b \u221e m\u22121 dx xl = 1 l \u2212 1 (m\u2212 1)\u2212(l\u22121).\nThen the second term of (24) is bounded by\n\u221e\u2211 l=2 \u03b1l l \u221e\u2211 k=m 1 kl \u2264 \u221e\u2211 l=2\n\u03b1l\nl(l \u2212 1) (m\u2212 1)\u2212(l\u22121)\n= (m\u2212 1) \u221e\u2211 l=2\n1\nl(l \u2212 1)\n( \u03b1\nm\u2212 1\n)l <\u221e.\nThe result follows since the second term, being bounded, vanished when dividing by logN and taking the limit N \u2192\u221e."}, {"heading": "D Appendix D", "text": "Lemma 2. Let rn and r\u0304n be random sequences with the update laws\nP (rn+1 = rn + 1) = \u03c4n\nP (rn+1 = rn) = 1\u2212 \u03c4n\nand\nP (r\u0304n+1 = r\u0304n + 1) = \u03c3n\nP (r\u0304n+1 = r\u0304n) = 1\u2212 \u03c3n,\nand assume \u03c3n \u2265 \u03c4n for all n \u2265 1 and that r\u03040 = r0 = 0. Then E[r\u0304n] \u2265 E[rn] for all n \u2265 1.\nProof. We first use induction to show that P (rn > t) \u2264 P (r\u0304n > t) holds for all n.\nThe base case is trivial because r0 = r\u03040. We next prove that given\nP (rn > t) \u2264 P (r\u0304n > t) (25)\nfor a particular n and all t \u2208 N, the same inequality holds for n + 1. We have\nP (rn+1 > t) = (1\u2212 \u03c4n)P (rn > t) + \u03c4nP (rn > t\u2212 1) \u2264 (1\u2212 \u03c4n)P (r\u0304n > t) + \u03c4nP (r\u0304n > t\u2212 1) \u2264 (1\u2212 \u03c3n)P (r\u0304n > t) + \u03c3nP (r\u0304n > t\u2212 1) = P (r\u0304n+1 > t), (26)\nwhere we used the inductive hypothesis (25) and the inequality P (r\u0304n > t) \u2264 P (r\u0304n > t\u22121). Thus, by induction, the inequality (25) holds for all n. Using (25), we further obtain:\nE[rn] = \u222b \u221e\n0 P (rn > t)dt \u2264 \u222b \u221e 0 P (r\u0304n > t)dt = E[r\u0304n]\nThe proof is complete."}, {"heading": "E Appendix E", "text": "Proof. We can study the generalized Polya urn model in the slightly modified form:\nP (r\u0304n+1 = k|r\u0304n) =\n{ r\u0304n+1 an , if k = r\u0304n + 1\n1\u2212 r\u0304n+1an , if k = r\u0304n (27)\nTaking the conditional expectation of r\u0304n+1 with respect to the filtration Fn+1 def = \u03c3(r\u03041, . . . , r\u0304n, \u03b31, . . . , \u03b3n+1,y1, . . . ,yn+1), we get E[r\u0304n+1|Fn+1] =\n(r\u0304n + 1) (\n1 + 1an\n) \u2212 1. Set xn := r\u0304n + 1. Rewriting this and using the\ndefinition of an, we obtain:\nE [ xn+1 \u2223\u2223\u2223Fn+1] \u2264 xn(1 + ln+1(yn+1) n log n ) (28)\nNext, we seek an upper bound on the conditional expectation E[lk(yk)|Fk\u22121]. This quantity can be bounded using convex duality [12]:\nE[lk(yk)|Fk\u22121] \u2264 1 + 1\ns D(pT \u2016 L\u0303k,K+) +\n1 s logEL\u0303k,K+ [e s(lk(yk)\u22121)]\nFor k \u2265 N , lk(yk) \u2264 \u03b6 and EL\u0303k,K+ [lk(yk)] = 1. By Hoeffding\u2019s inequality, EL\u0303k,K+ [e s(lk(yk)\u22121)] \u2264 es2\u03b62/8. Using this bound, we obtain for k \u2265 N ,\nE[lk(yk)|Fk\u22121] \u2264 1 + \u03b4/s + s\u03b62/8. Minimizing this as a function of s > 0, we obtain: E[lk(yk)|Fk\u22121] \u2264 1 + \u03b6 \u221a \u03b4\n2 (29)\nNext, we upper bound E[xn+1|FN ] recursively. Taking the conditional expectation of both sides of (28), we obtain:\nE [ xn+1 \u2223\u2223\u2223Fn] \u2264 E [xn(1 + ln+1(yn+1) n log n ) \u2223\u2223\u2223Fn] (30) We note that the function ln+1(\u00b7) is Fn-measurable. This follows since by definition, ln+1(\u00b7) = L0(\u00b7)\u2211kn\nh=1 mn(h) n Ln+1,h(\u00b7) , and mn(h) =\n\u2211n l=1 I(\u03b3l = h) and\nLn+1,h(\u00b7) are both Fn-measurable (due to the parameter updates and (22)). Also note that xn = r\u0304n + 1 is randomly determined by a biased coin flip given Fn, increasing by 1 with probability xn\u22121an\u22121 and staying the same with probability 1 \u2212 xn\u22121an\u22121 . Since an\u22121 is Fn-measurable, it follows that xn and ln+1(yn+1) are conditionally independent given the history Fn. Using this conditional independence, we obtain from (30): E [ xn+1 \u2223\u2223\u2223Fn] \u2264 E[xn|Fn](1 + E[ln+1(yn+1)|Fn] n log n ) \u2264 E[xn|Fn] ( 1 + 1 + \u03b6 \u221a \u03b4/2 n log n\n) (31)\nwhere we used the bound (29) in the last inequality. Repeatedly conditioning and using (28) and (31): E[xn+1|FN ] \u2264 \u220fn k=N ( 1 + 1+\u03b6 \u221a \u03b4 2 k log k ) E[xN |FN ] \u2264 C0N log 1+\u03b6 \u221a \u03b4/2 n, where we used the Lemma in Appendix F and C0 =\nC(1 + \u03b6 \u221a \u03b4/2, N), xN \u2264 N in the last inequality. Taking the unconditional expectation and using E[rn + 1] \u2264 E[r\u0304n + 1] (see Appendix D) yields the bound E [rn + 1] \u2264 C0N log1+\u03b6 \u221a \u03b4/2 n. Markov\u2019s inequality then yields\nP (\nrn+1\nC0N log 1+\u03b6 \u221a \u03b4/2 n > K\n) \u2264 1K which implies (14) by taking K \u2192\u221e. Since\n\u03b1n = rn+1 \u03bb+logn , the bound in (14) follows from a similar argument. The proof is complete."}, {"heading": "F Appendix F", "text": "Lemma 3. The following upper bound holds with constant C(\u03c6,N) = e \u03c6 N logN / log\u03c6N :\nn\u220f k=N ( 1 +\n\u03c6\nk log k\n) \u2264 C(\u03c6,N) log\u03c6 n\nProof. Using the elementary inequality log(1+x) \u2264 x for x > \u22121, we obtain:\nlog\n( n\u220f\nk=N\n( 1 +\n\u03c6\nk log k\n)) =\nn\u2211 k=N log ( 1 +\n\u03c6\nk log k\n)\n\u2264 n\u2211\nk=N\n\u03c6\nk log k \u2264 \u03c6 (\u222b n N dx x log x +\n1\nN logN\n)\n= \u03c6 (\u222b logn logN dt t +\n1\nN logN ) = log ( log\u03c6 n\nlog\u03c6N\n) +\n\u03c6\nN logN\nTaking the exponential of both sides yields the desired inequality."}], "references": [{"title": "Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems", "author": ["C.E. Antoniak"], "venue": "The Annals of Statistics", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1974}, {"title": "Inequalities for the Gamma Function", "author": ["N. Batir"], "venue": "Archiv der Mathematik", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Variational Inference for Dirichlet Process Mixtures", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "Bayesian Analysis", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Fast Search for Dirichlet Process Mixture Models", "author": ["H. Daume"], "venue": "Conference on Artificial Intelligence and Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Bayesian Density Estimation and Inference using Mixtures", "author": ["M.D. Escobar", "M. West"], "venue": "Journal of the American Statistical Association", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Particle Filters for Mixture Models with an Uknown Number of Components, Statistics and Computing", "author": ["P. Fearnhead"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Accelerated Variational Dirichlet Mixture Models, Advances in Neural Information", "author": ["K. Kurihara", "M. Welling", "N. Vlassis"], "venue": "Processing Systems (NIPS),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Online learning of nonparametric mixture models via sequential variational approximation, Advances in Neural Information Processing Systems", "author": ["Dahua Lin"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Bayesian Mixture Modeling", "author": ["R.M. Neal"], "venue": "Proceedings of the Workshop on Maximum Entropy and Bayesian Methods of Statistical Analysis,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "The infinite gaussian mixture model, Advances in Neural Information", "author": ["C.E. Rasmussen"], "venue": "Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Bayesian Gaussian Process Models: PAC- Bayesian Generalization Error Bounds and Sparse Approximations", "author": ["Matthias W. Seeger"], "venue": "Ph.D. thesis, University of Edinburgh,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "A Sequential Bayesian Inference Framework for Blind Frequency Offset Estimation", "author": ["T. Tsiligkaridis", "K.W. Forsythe"], "venue": "Proceedings of IEEE International Workshop on Machine Learning for Signal Processing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "The Variational Approximation for Bayesian Inference", "author": ["D.G. Tzikas", "A.C. Likas", "N.P. Galatsanos"], "venue": "IEEE Signal Processing Magazine", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Fast Bayesian Inference in Dirichlet Process Mixture Models", "author": ["L. Wang", "D.B. Dunson"], "venue": "Journal of Computational and Graphical Statistics", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}], "referenceMentions": [{"referenceID": 8, "context": "Dirichlet process mixture models (DPMM) have been widely used for clustering data [9, 11].", "startOffset": 82, "endOffset": 89}, {"referenceID": 9, "context": "Dirichlet process mixture models (DPMM) have been widely used for clustering data [9, 11].", "startOffset": 82, "endOffset": 89}, {"referenceID": 2, "context": "Alternatives include variational methods [3], which are deterministic algorithms that convert inference to optimization.", "startOffset": 41, "endOffset": 44}, {"referenceID": 7, "context": "An online algorithm for learning DPMM\u2019s based on a sequential variational approximation (SVA) was proposed in [8], and the authors in [15] recently proposed a sequential maximum a-posterior (MAP) estimator for the class labels given streaming data.", "startOffset": 110, "endOffset": 113}, {"referenceID": 13, "context": "An online algorithm for learning DPMM\u2019s based on a sequential variational approximation (SVA) was proposed in [8], and the authors in [15] recently proposed a sequential maximum a-posterior (MAP) estimator for the class labels given streaming data.", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "The choice of concentration parameter \u03b1 is critical for DPMM\u2019s as it controls the number of clusters [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 5, "context": "While most fast DPMM algorithms use a fixed \u03b1 [6, 4, 7], imposing a prior distribution on \u03b1 and sampling from it provides more flexibility, but this approach still heavily relies on experimentation and prior knowledge.", "startOffset": 46, "endOffset": 55}, {"referenceID": 3, "context": "While most fast DPMM algorithms use a fixed \u03b1 [6, 4, 7], imposing a prior distribution on \u03b1 and sampling from it provides more flexibility, but this approach still heavily relies on experimentation and prior knowledge.", "startOffset": 46, "endOffset": 55}, {"referenceID": 6, "context": "While most fast DPMM algorithms use a fixed \u03b1 [6, 4, 7], imposing a prior distribution on \u03b1 and sampling from it provides more flexibility, but this approach still heavily relies on experimentation and prior knowledge.", "startOffset": 46, "endOffset": 55}, {"referenceID": 4, "context": "Thus, many fast inference methods for Dirichlet process mixture models have been proposed that can adapt \u03b1 to the data, including the works [5] where learning of \u03b1 is incorporated in the Gibbs sampling analysis, [3] where a Gamma prior is used in a conjugate manner directly in the variational inference algorithm.", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "Thus, many fast inference methods for Dirichlet process mixture models have been proposed that can adapt \u03b1 to the data, including the works [5] where learning of \u03b1 is incorporated in the Gibbs sampling analysis, [3] where a Gamma prior is used in a conjugate manner directly in the variational inference algorithm.", "startOffset": 212, "endOffset": 215}, {"referenceID": 13, "context": "[15] also account for model uncertainty on the concentration parameter \u03b1 in a Bayesian manner directly in the sequential inference procedure.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Here, we review the SUGS framework of [15] for online clustering.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "1 Adaptation of Concentration Parameter \u03b1 It is well known that the concentration parameter \u03b1 has a strong influence on the growth of the number of classes [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 7, "context": "Choosing a fixed \u03b1 as in the online SVA algorithm of [8] requires cross-validation, which is computationally prohibitive for large-scale data sets.", "startOffset": 53, "endOffset": 56}, {"referenceID": 13, "context": "Although the parameter \u03b1 is handled from a fully Bayesian treatment in [15], a pre-specified grid of possible values \u03b1 can take, say {\u03b1l}l=1, along with the prior distribution over them, needs to be chosen in advance.", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "The model (16) leads to closed-form expressions for Li,h(yi)\u2019s due to conjugacy [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 1, "context": "6 from [2], it follows that lima\u2192\u221e \u03c1d(a) e\u2212d/2(a\u22121/2)d/2 = 1.", "startOffset": 7, "endOffset": 10}, {"referenceID": 11, "context": "A similar asymptotic normality result was recently obtained in [13] for Gaussian observations with a von Mises prior.", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "1 Prune & Merge It is possible that multiple clusters are similar and classes might be created due to outliers, or due to the particular ordering of the streaming data sequence, as also noted in [8].", "startOffset": 195, "endOffset": 198}, {"referenceID": 7, "context": "Since it is possible that multiple clusters are similar and classes might be created due to outliers, or due to the particular ordering of the streaming data sequence, we add the pruning and merging step in the ASUGS algorithm as done in [8].", "startOffset": 238, "endOffset": 241}, {"referenceID": 7, "context": "We compare ASUGS and ASUGS-PM with SUGS, SUGS-PM, SVA and SVA-PM proposed in [8], since it was shown in [8] that SVA and SVA-PM outperform the block-based methods that perform iterative updates over the entire data set including Collapsed Gibbs Sampling, MCMC with Split-Merge and Truncation-Free Variational Inference.", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "We compare ASUGS and ASUGS-PM with SUGS, SUGS-PM, SVA and SVA-PM proposed in [8], since it was shown in [8] that SVA and SVA-PM outperform the block-based methods that perform iterative updates over the entire data set including Collapsed Gibbs Sampling, MCMC with Split-Merge and Truncation-Free Variational Inference.", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "Each sample is a 28\u00d728 image of a handwritten digit (total of 784 dimensions), and we perform PCA pre-processing to reduce dimensionality to d = 50 dimensions as in [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 12, "context": "This leads to closed-form expressions for Li,h(yi)\u2019s due to conjugacy [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "This quantity can be bounded using convex duality [12]:", "startOffset": 50, "endOffset": 54}], "year": 2015, "abstractText": "We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a novel adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and data predictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to other online stateof-the-art methods.", "creator": "TeX"}}}