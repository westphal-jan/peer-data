{"id": "1410.1784", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Oct-2014", "title": "Stochastic Discriminative EM", "abstract": "stochastic discriminative em ( sdem ) is an online - em - type algorithm for discriminative training of probabilistic generative models belonging to precisely the exponential family. later in essentially this work, we introduce and justify this algorithm as a stochastic natural gradient matrix descent method, i. ~ e. a method which mostly accounts for the information minimum geometry element in identifying the parameter space of the statistical model. nowadays we may show demonstrating how this learning algorithm can be used to efficiently train probabilistic generative models by minimizing different discriminative loss functions, such variables as the negative conditional log - likelihood and the hinge loss. the resulting models algorithm trained by sdem are always generative ( < i. e. they define a joint probability distribution ) and, in consequence, allows to deal with missing objective data and latent variables in a principled way either when being traditionally learned or when making predictions. the increasing performance of this method is illustrated by several text classification problems for which a combined multinomial naive bayes and a latent dirichlet allocation based classifier are learned using different discriminative global loss distance functions.", "histories": [["v1", "Thu, 2 Oct 2014 12:10:40 GMT  (679kb)", "http://arxiv.org/abs/1410.1784v1", "UAI 2014 paper + Supplementary Material. In Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence (UAI 2014), edited by Nevin L. Zhang and Jian Tian. AUAI Press"]], "COMMENTS": "UAI 2014 paper + Supplementary Material. In Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence (UAI 2014), edited by Nevin L. Zhang and Jian Tian. AUAI Press", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andres r masegosa"], "accepted": false, "id": "1410.1784"}, "pdf": {"name": "1410.1784.pdf", "metadata": {"source": "CRF", "title": "Stochastic Discriminative EM", "authors": ["Andr\u00e9s R. Masegosa"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n41 0.\n17 84\nv1 [\ncs .L\nG ]\n2 O\nct 2\nStochastic discriminative EM (sdEM) is an online-EM-type algorithm for discriminative training of probabilistic generative models belonging to the exponential family. In this work, we introduce and justify this algorithm as a stochastic natural gradient descent method, i.e. a method which accounts for the information geometry in the parameter space of the statistical model. We show how this learning algorithm can be used to train probabilistic generative models by minimizing different discriminative loss functions, such as the negative conditional loglikelihood and the Hinge loss. The resulting models trained by sdEM are always generative (i.e. they define a joint probability distribution) and, in consequence, allows to deal with missing data and latent variables in a principled way either when being learned or when making predictions. The performance of this method is illustrated by several text classification problems for which a multinomial naive Bayes and a latent Dirichlet allocation based classifier are learned using different discriminative loss functions."}, {"heading": "1 INTRODUCTION", "text": "Online learning methods based on stochastic approximation theory [21] have been a promising research direction to tackle the learning problems of the so-called Big Data era [1, 10, 12]. Stochastic gradient descent (SGD) is probably the best known example of this kind of techniques, used to solve a wide range of learning problems [9]. This algorithm and other versions [29] are usually employed to train discriminative models such as logistic regression or SVM [10].\nThere also are some successful examples of the use of SGD for discriminative training of probabilistic generative models, as is the case of deep belief networks [19]. However,\nthis learning algorithm cannot be used directly for the discriminative training of general generative models. One of the main reasons is that statistical estimation or risk minimization problems of generative models involve the solution of an optimization problem with a large number of normalization constraints [26], i.e. those which guarantee that the optimized parameter set defines a valid probabilistic model. Although successful solutions to this problem have been proposed [16, 22, 26, 32], they are based on adhoc methods which cannot be easily extended to other statistical models, and hardly scale to large data sets.\nStochastic approximation theory [21] has also been used for maximum likelihood estimation (MLE) of probabilistic generative models with latent variables, as is the case of the online EM algorithm [13, 30]. This method provides efficient MLE estimation for a broad class of statistical models (i.e. exponential family models) by sequentially updating the so-called expectation parameters. The advantage of this approach is that the resulting iterative optimization algorithm is fairly simple and amenable, as it does not involve any normalization constraints.\nIn this paper we show that the derivation of Sato\u2019s online EM [30] can be extended for the discriminative learning of generative models by introducing a novel interpretation of this algorithm as a natural gradient algorithm [3]. The resulting algorithm, called stochastic discriminative EM (sdEM), is an online-EM-type algorithm that can train generative probabilistic models belonging to the exponential family using a wide range of discriminative loss functions, such as the negative conditional log-likelihood or the Hinge loss. In opposite to other discriminative learning approaches [26], models trained by sdEM can deal with missing data and latent variables in a principled way either when being learned or when making predictions, because at any moment they always define a joint probability distribution. sdEM could be used for learning using large scale data sets due to its stochastic approximation nature and, as we will show, because it allows to compute the natural gradient of the loss function with no extra cost [3]. Moreover, if allowed by the generative model and the discriminative loss\nfunction, the presented algorithm could potentially be used interchangeably for classification or regression or any other prediction task. But in this initial work, sdEM is only experimentally evaluated in classification problems.\nThe rest of this paper is organized as follows. Section 2 provides the preliminaries for the description of the sdEM algorithm, which is detailed in Section 3. A brief experimental evaluation is given in Section 4, while Section 5 contains the main conclusions of this work."}, {"heading": "2 PRELIMINARIES", "text": ""}, {"heading": "2.1 MODEL AND ASSUMPTIONS", "text": "We consider generative statistical models for prediction tasks, where Y denotes the random variable (or the vectorvalue random variable) to be predicted, X denotes the predictive variables, and y\u22c6 denotes a prediction, which is made according to y\u22c6 = argmaxy p(y, x|\u03b8).\nAssumption 1. The generative data model belongs to the exponential family with a natural (or canonical) parametrization\np(y, x|\u03b8) \u221d exp(\u3008s(y, x), \u03b8\u3009 \u2212Al(\u03b8))\nwhere \u03b8 is the so-called natural parameter which belongs to the so-called natural parameter space \u0398 \u2208 \u211cK , s(y, x) is the vector of sufficient statistics belonging to a convex set S \u2286 \u211cK , \u3008\u00b7, \u00b7\u3009 denotes the dot product and Al is the log partition function.\nAssumption 2. We are given a conjugate prior distribution p(\u03b8|\u03b1) of the generative data model\np(\u03b8|\u03b1) \u221d exp(\u3008s(\u03b8), \u03b1\u3009 \u2212Ag(\u03b1))\nwhere the sufficient statistics are s(\u03b8) = (\u03b8,\u2212Al(\u03b8)) and the hyperparameter \u03b1 has two components (\u03b1\u0304, \u03bd). \u03bd is a positive scalar and \u03b1\u0304 is a vector also belonging to S [6]."}, {"heading": "2.2 DUAL PARAMETERIZATION AND ASSUMPTIONS", "text": "The so-called expectation parameter \u00b5 \u2208 S can also be used to parameterize probability distributions of the exponential family. It is a dual set of the model parameter \u03b8 [2]. This expectation parameter \u00b5 is defined as the expected vector of sufficient statistics with respect to \u03b8:\n\u00b5 , E [s(y, x)|\u03b8] = \u222b\ns(y, x)p(y, x|\u03b8)dydx = \u2202Al(\u03b8)/\u2202\u03b8\n(1)\nThe transformation between \u03b8 and \u00b5 is one-to-one: \u00b5 is a dual set of the model parameter \u03b8 [2]. Therefore, Equation (1) can be inverted as: \u03b8 = \u03b8(\u00b5). That is to say, for each \u03b8 \u2208 \u0398 we always have an associated \u00b5 \u2208 S and both parameterize the same probability distribution.\nFor obtaining the natural parameter \u03b8 associated to an expectation parameter \u00b5, we need to make use of the negative of the entropy,\nH(\u00b5) , \u222b\np(y, x|\u03b8(\u00b5)) ln p(y, x|\u03b8(\u00b5))dydx = sup\u03b8\u2208\u0398\u3008\u00b5, \u03b8\u3009 \u2212Al(\u03b8)\n(2)\nUsing the above function, the natural parameter \u03b8 can be explicitly expressed as\n\u03b8 = \u03b8(\u00b5) = \u2202H(\u00b5)/\u2202\u00b5 (3)\nEquations (1), (2), (3) define the Legendre-Fenchel transform.\nAnother key requirement of our approach is that it should be possible to compute the transformation from \u00b5 to \u03b8 in closed form:\nAssumption 3. The transformation from the expectation parameter \u00b5 to the natural parameter \u03b8, which can be expressed as\n\u03b8(\u00b5) = argmax \u03b8\u2208\u0398 \u3008\u00b5, \u03b8\u3009 \u2212Al(\u03b8) (4)\nis available in closed form.\nThe above equation is also known as the maximum likelihood function, because \u03b8( 1\nn \u2211n i=1 s(yi, xi)) gives the max-\nimum likelihood estimation \u03b8\u22c6 for a data set with n observations {(y1, x1), . . . , (yn, xn)}.\nFor later convenience, we show the following relations between the Fisher Information matrices I(\u03b8) and I(\u00b5) for the probability distributions p(y, x|\u03b8) and p(y, x|\u03b8(\u00b5)), respectively [25]:\nI(\u03b8) = \u22022Al(\u03b8)\n\u2202\u03b8\u2202\u03b8 =\n\u2202\u00b5 \u2202\u03b8 = I(\u00b5)\u22121 (5)\nI(\u00b5) = \u22022H(\u00b5)\n\u2202\u00b5\u2202\u00b5 =\n\u2202\u03b8 \u2202\u00b5 = I(\u03b8)\u22121 (6)"}, {"heading": "2.3 THE NATURAL GRADIENT", "text": "Let W = {w \u2208 \u211cK} be a parameter space on which the function L(w) is defined. When W is a Euclidean space with an orthonormal coordinate system, the negative gradient points in the direction of steepest descent. That is, the negative gradient\u2212\u2202L(w)/\u2202w points in the same direction as the solution to:\nargmin dw\nL(w + dw) subject to ||dw||2 = \u01eb2 (7)\nfor sufficiently small \u01eb, where ||dw||2 is the squared length of a small increment vector dw connecting w and w + dw. This justifies the use of the classical gradient descent method for finding the minimum of L(w) by taking steps (of size \u03c1) in the direction of the negative gradient:\nwt+1 = wt \u2212 \u03c1 \u2202L(wt)\n\u2202w (8)\nHowever, when W is a Riemannian space [4], there are no orthonormal linear coordinates, and the squared length of vector dw is defined by the following equation,\n||dw||2 = \u2211\nij\ngij(w)dwidwj (9)\nwhere the K \u00d7K matrix G = (gij) is called the Riemannian metric tensor, and it generally depends on w. G reduces to the identity matrix in the case of the Euclidean space [4].\nIn a Riemannian space, the steepest descent direction is not anymore the traditional gradient. That is, \u2212\u2202L(w)/\u2202w is not the solution of Equation (7) when the squared length of the distance of dw is defined by Equation (9). Amari [3] shows that this solution can be computed by premultiplying the traditional gradient by the inverse of the Riemannian metric G\u22121,\nTheorem 1. The steepest descent direction or the natural gradient of L(w) in a Riemannian space is given by\n\u2212 \u2202\u0303L(w)\n\u2202\u0303w = \u2212G\u22121(w)\n\u2202L(w)\n\u2202w (10)\nwhere \u2202\u0303L(w)/\u2202\u0303w denotes the natural gradient.\nAs argued in [3], in statistical estimation problems we should used gradient descent methods which account for the natural gradient of the parameter space, as the parameter space of a statistical model (belonging to the exponential family or not) is a Riemannian space with the Fisher information matrix of the statistical model I(w) as the tensor metric [2], and this is the only invariant metric that must be given to the statistical model [2]."}, {"heading": "2.4 SATO\u2019S ONLINE EM ALGORITHM", "text": "Sato\u2019s online EM algorithm [30] is used for maximum likelihood estimation of missing data-type statistical models. The model defines a probability distribution over two random or vector-valued variables X and Z , and is assumed to belong to the exponential family:\np(z, x|\u03b8) \u221d exp(\u3008s(z, x), \u03b8\u3009 \u2212Al(\u03b8))\nwhere (z, x) denotes a so-called complete data event. The key aspect is that we can only observe x, since z is an unobservable event. In consequence, the loss function \u2113(x, \u03b8)1 is defined by marginalizing z: \u2113(x, \u03b8) = \u2212 ln \u222b p(z, x)dz.\nThe online setting assumes the observation of a non-finite data sequence {(xt)}t\u22650 independently drawn according to the unknown data distribution \u03c0. The objective function that EM seeks to minimize is given by the following expectation: L(\u03b8) = E [\u2113(x, \u03b8)|\u03c0].\n1We derive this algorithm in terms of minimization of a loss function to highlight its connection with sdEM.\nSato [30] derived the stochastic updating equation of online EM by relying on the free energy formulation, or lower bound maximization, of the EM algorithm [24] and on a discounting averaging method. Using our own notation, this updating equation is expressed as follows,\n\u00b5t+1 = (1\u2212 \u03c1t)\u00b5t + \u03c1tEz[s(z, xt|\u03b8(\u00b5t)]\n= \u00b5t + \u03c1t (Ez[s(z, xt|\u03b8(\u00b5t)]\u2212 \u00b5t)\n= \u00b5t + \u03c1t \u2202\u2113(xt, \u03b8(\u00b5t))\n\u2202\u03b8 (11)\nwhere Ez[s(z, xt|\u03b8(\u00b5t)] denotes the expected sufficient statistics, Ez[s(z, xt|\u03b8(\u00b5t)] = \u222b s(z, xt)p(z|xt, \u03b8(\u00b5t))dz.\nHe proved the convergence of the above iteration method by casting it as a second order stochastic gradient descent using the following equality,\n\u2202\u2113(x, \u03b8)\n\u2202\u03b8 =\n\u2202\u00b5\n\u2202\u03b8\n\u2202\u2113(x, \u03b8(\u00b5))\n\u2202\u00b5 = I(\u00b5)\u22121\n\u2202\u2113(x, \u03b8(\u00b5))\n\u2202\u00b5 (12)\nThis equality is obtained by firstly applying the chain rule, followed by the equality shown in Equation (5). It shows that online EM is equivalent to a stochastic gradient descent with I(\u00b5t)\u22121 as coefficient matrices [9].\nSato noted that that the third term of the equality in Equation (12) resembles a natural gradient (see Theorem 1), but he did not explore the connection. But the key insights of the above derivation, which were not noted by Sato, is that Equation (12) is also valid for other loss functions different from the marginal log-likelihood; and that the convergence of Equation (11) does not depend on the formulation of the EM as a \u201clower bound maximization\u201d method [24]."}, {"heading": "3 STOCHASTIC DISCRIMINATIVE EM", "text": ""}, {"heading": "3.1 THE sdEM ALGORITHM", "text": "We consider the following supervised learning setup. Let us assume that we are given a data set D with n observations {(y1, x1), . . . , (yn, xn)}. We are also given a discriminative loss function2 \u2113(yi, xi, \u03b8). For example, it could be the negative conditional log-likelihood (NCLL) \u2113(yi, xi, \u03b8) = \u2212 ln p(yi, xi|\u03b8) + ln \u222b\np(y, xi|\u03b8)dy = \u2212 ln p(yi|xi, \u03b8). Our learning problem consists in minimizing the following objective function:\nL(\u03b8) =\nn \u2211\ni=1\n\u2113(yi, xi, \u03b8)\u2212 ln p(\u03b8|\u03b1)\n= E [\u2113(y, x, \u03b8)|\u03c0]\u2212 1\nn ln p(\u03b8|\u03b1) (13)\nwhere \u03c0 is now the empirical distribution of D and E [\u2113(y, x, \u03b8)|\u03c0] the empirical risk. Although the above\n2The loss function is assumed to satisfy the mild conditions given in [9]. E.g., it can be a non-smooth function, such as the Hinge Loss.\nloss function is not standard in the machine learning literature, we note that when \u2113 is the negative log-likelihood (NLL), we get the classic maximum a posterior estimation. This objective function can be seen as an extension of this framework.\nsdEM is presented as a generalization of Sato\u2019s online EM algorithm for finding the minimum of an objective function in the form of Equation (13) (i.e. the solution to our learning problem). The stochastic updating equation of sdEM can be expressed as follows,\n\u00b5t+1 = \u00b5t \u2212 \u03c1tI(\u00b5t) \u22121 \u2202\u2113\u0304(yt, xt, \u03b8(\u00b5t))\n\u2202\u00b5 (14)\nwhere (yt, xt) denotes the t-th sample, randomly generated from \u03c0, and the function \u2113\u0304 has the following expression: \u2113\u0304(yt, xt, \u03b8(\u00b5t)) = \u2113((yt, xt, \u03b8(\u00b5t)) + 1/n ln p(\u03b8(\u00b5t)). We note that this loss function satisfies the following equality, which is the base for a stochastic approximation method [21], E [ \u2113\u0304(yt, xt, \u03b8(\u00b5))|\u03c0 ] = L(\u03b8(\u00b5)).\nSimilarly to Amari\u2019s natural gradient algorithm [3], the main problem of sdEM formulated as in Equation (14) is the computation of the inverse of the Fisher information matrix at each step, which becomes even prohibitive for large models. The following result shows that this can be circumvented when we deal with distributions of the exponential family:\nTheorem 2. In the exponential family, the natural gradient of a loss function with respect to the expectation parameters equals the gradient of the loss function with respect to the natural parameters,\nI(\u00b5)\u22121 \u2202\u2113\u0304(y, x, \u03b8(\u00b5))\n\u2202\u00b5 =\n\u2202\u2113\u0304(y, x, \u03b8)\n\u2202\u03b8\nSketch of the proof. We firstly need to prove that I(\u00b5) is a valid Riemannian tensor metric and, hence, the expectation parameter space has a Riemanian structure defined by the metric I(\u00b5) and the definition of the natural gradient makes sense. This can be proved by the invariant property of the Fisher information metric to one-to-one reparameterizations or, equivalently, transformations in the system of coordinates [2, 4]. I(\u00b5) is a Riemannian metric because it is the Fisher information matrix of the reparameterized model p(y, x|\u03b8(\u00b5)), and the reparameterization is one-toone, as commented in Section 2.2.\nThe equality stated in the theorem follows directly from Sato\u2019s derivation of the online EM algorithm (Equation (12)). This derivation shows that we can avoid the computation of I(\u00b5)\u22121 by using the natural parameters instead of the expectation parameters and the function \u03b8(\u00b5).\nTheorem 1 simplifies the sdEM\u2019s updating equation to,\n\u00b5t+1 = \u00b5t \u2212 \u03c1t \u2202\u2113\u0304(yt, xt, \u03b8(\u00b5t))\n\u2202\u03b8 (15)\nsdEM can be interpreted as a stochastic gradient descent algorithm iterating over the expectation parameters and guided by the natural gradient in this Riemannian space.\nAlgorithm 1 Stochastic Discriminative EM (sdEM) Require: D is randomly shuffled.\n1: \u00b50 = \u03b1\u0304; (initialize according to the prior) 2: \u03b80 = \u03b8(\u00b50); 3: t = 0; 4: repeat 5: for i = 1, . . . , n do 6: E-Step: \u00b5t+1 = \u00b5t \u2212 1(1+\u03bbt) \u2202\u2113\u0304(yi,xi,\u03b8t) \u2202\u03b8 ;\n7: Check-Step: \u00b5t+1 = Check(\u00b5t+1,S);\n8: M-Step: \u03b8t+1 = \u03b8(\u00b5t+1); 9: t = t+ 1;\n10: end for 11: until convergence 12: return \u03b8(\u00b5t);\nAn alternative proof to Theorem 2 based on more recent results on information geometry has been recently given in [27]. The results of that work indicate that sdEM could also be interpreted as a mirror descent algorithm with a Bregman divergence as a proximitiy measure. It is beyond the scope of the paper to explore this relevant connection."}, {"heading": "3.2 CONVERGENCE OF sdEM", "text": "In this section we do not attempt to give a formal proof of the convergence of sdEM, since very careful technical arguments would be needed for this purpose [9]. We simply go through the main elements that define the convergence of sdEM as an stochastic approximation method [21].\nAccording to Equation (14), sdEM can be seen as a stochastic gradient descent method with the inverse of the Fisher information matrix I(\u00b5)\u22121 as a coefficient matrix [9]. As we are dealing with exponential families, these matrices are always positive-definite. Moreover, if the gradient \u2202\u2113\u0304(y, x, \u03b8)/\u2202\u03b8 can be computed exactly (in Section 3.4 we discuss what happens when this is not possible), from Theorem 2, we have that it is an unbiased estimator of the natural gradient of the L(\u03b8(\u00b5)) defined in Equation 13,\nE\n[\n\u2202\u2113\u0304(y, x, \u03b8)\n\u2202\u03b8 |\u03c0\n]\n= I(\u00b5)\u22121 \u2202L(\u03b8(\u00b5))\n\u2202\u00b5 (16)\nHowever, one key difference in terms of convergence between online EM and sdEM can be seen in Equation (11): \u00b5t+1 is a convex combination between \u00b5t and the expected sufficient statistics. Then, \u00b5t+1 \u2208 S during all the iterations. As will be clear in the next section, we do not have this same guarantee in sdEM, but we can take advantage of the log prior term of Equation (13) to avoid this problem. This term plays a dual role as both \u201cregularization\u201d\nterm and log-barrier function [31] i.e. a continuous function whose value increases to infinity as the parameter approaches the boundary of the feasible region or the support of p(\u03b8(\u00b5)|\u03b1) 3. Then, if the step sizes \u03c1t are small enough (as happens near convergence), sdEM will always stays in the feasible region S, due to the effect of the log prior term. The only problem is that, in the initial iterations, the step sizes \u03c1t are large, so one iteration can jump out of the boundary of S. The method to avoid that depends on the particular model, but for the models examined in this work it seems to be a simple check in every iteration. For example, as we will see in the experimental section when implementing a multinomial Naive Bayes, we will check at every iteration that each sufficient statistic or \u201cword count\u201d is always positive. If a \u201cword count\u201d is negative at some point, we will set it to a very small value. As mentioned above, this does not hurt the convergence of sdEM because in the limit this problem disappears due the effect of the log-prior term.\nThe last ingredient required to assess the convergence of a stochastic gradient descent method is to verify that the sequence of step sizes satisfies: \u2211 \u03c1t = \u221e, \u2211 \u03c12t < \u221e.\nSo, if the sequence (\u00b5t)t\u22650 converges, it will probably converge to the global minimum (\u00b5\u22c6, \u03b8\u22c6 = \u03b8(\u00b5\u22c6)) if L(\u03b8) is convex, or to a local minimum if L(\u03b8) is not convex [9].\nFinally, we give an algorithmic description of sdEM in Algorithm 1. Following [11], we consider steps sizes of the form \u03c1t = (1 + \u03bbt)\u22121, where \u03bb is a positive scalar4. As mentioned above, the \u201cCheck-Step\u201d is introduced to guarantee that \u00b5t is always in S. Like the online EM algorithm [30, 13], Algorithm 1 resembles the classic expectation maximization algorithm [15] since, as we will see in the next section, the gradient is computed using expected\n3The prior p would need to be suitably chosen. 4Our experiments suggest that trying \u03bb \u2208 {1, 0.1, 0.01,\n0.001, . . .} suffices for obtaining a quick convergence.\nsufficient statistics. Assumption 3 guarantees that the maximization step can be performed efficiently. This step differentiates sdEM from classic stochastic gradient descent methods, where such a computation does not exist."}, {"heading": "3.3 DISCRIMINATIVE LOSS FUNCTIONS", "text": "As we have seen so far, the derivation of sdEM is complete except for the definition of the loss function. We will discuss now how two well known discriminative loss functions can be used with this algorithm."}, {"heading": "Negative Conditional Log-likelihood (NCLL)", "text": "As mentioned above, this loss function is defined as follows:\n\u2113CL(yt, xt, \u03b8) = \u2212 ln p(yt, xt|\u03b8) + ln\n\u222b\np(y, xt|\u03b8)dy\nAnd its gradient is computed as\n\u2202\u2113CL(yt, xt, \u03b8)\n\u2202\u03b8 = \u2212s(yt, xt) + Ey[s(y, xt)|\u03b8]\nwhere the sufficient statistic s(yt, xt) comes from the gradient of the ln p(yt, xt|\u03b8) term in the NCLL loss, and the expected sufficient statistic Ey[s(y, xt)|\u03b8] = \u222b\ns(y, xt)p(y|xt, \u03b8)dy, comes from the gradient of the ln \u222b\np(y, xt|\u03b8)dy term in the NCLL loss. As mentioned above, the computation of the gradient is similar to the expectation step of the classic EM algorithm.\nThe iteration equation of sdEM for the NCLL loss is detailed in Table 1. We note that in the case of multi-class prediction problems the integrals of the updating equation are replaced by sums over the different classes of the class variable Y . We also show the updating equation for the negative log-likelihood (NLL) loss for comparison purposes."}, {"heading": "The Hinge loss", "text": "Unlike the previous loss which is valid for continuous and discrete (and vector-valued) predictions, this loss is only valid for binary or multi-class classification problems.\nMargin-based loss functions have been extensively used and studied by the machine learning community for binary and multi-class classification problems [5]. However, in our view, the application of margin-based losses (different from the negative conditional log-likelihood) for discriminative training of probabilistic generative models is scarce and based on ad-hoc learning methods which, in general, are quite sophisticated [26]. In this section, we discuss how sdEM can be used to minimize the empirical risk of one of the most used margin-based losses, the Hinge loss, in binary and multi-class classification problems. But, firstly, we discuss how Hinge loss can be defined for probabilistic generative models.\nWe build on LeCun et al.\u2019s ideas [23] about energy-based learning for prediction problems. LeCun et al. [23] define the Hinge loss for energy-based models as follows,\nmax(0, 1\u2212 (E(y\u0304t, xt, w)\u2212 E(yt, xt, w))\nwhere E(\u00b7) is the energy function parameterized by a parameter vector w, E(yt, xt, w) is the energy associated to the correct answer yt and E(y\u0304t, xt, w) is the energy associated to the most offending incorrect answer, y\u0304t = argminy 6=yt E(y, xt, w). Predictions y\n\u22c6 are made using y\u22c6 = argminy E(y, xt, w\n\u22c6) when the parameter w\u22c6 that minimizes the empirical risk is found.\nIn our learning settings we consider the minus logarithm of the joint probability, \u2212 ln p(yt, xt|\u03b8), as an energy function. In consequence, we define the hinge loss as follows\n\u2113hinge(yt, xt, \u03b8) = max(0, 1\u2212 ln p(yt, xt|\u03b8)\np(y\u0304t, xt|\u03b8) ) (17)\nwhere y\u0304t denotes here too the most offending incorrect answer, y\u0304t = argmaxy 6=yt p(y, xt|\u03b8).\nThe gradient of this loss function can be simply computed as follows\n\u2202\u2113hinge(yt, xt, \u03b8)\n\u2202\u03b8 =\n\n\n\n0 if ln p(yt,xt|\u03b8) p(y\u0304t,xt|\u03b8) > 1\n\u2212s(yt, xt) + s(y\u0304t, xt) otherwise\nand the iteration equation for minimizing the empirical risk of the Hinge loss is also given in Table 1."}, {"heading": "3.4 PARTIALLY OBSERVABLE DATA", "text": "The generalization of sdEM to partially observable data is straightforward. We denote by Z the vector of nonobservable variables. sdEM will handle statistical models which define a probability distribution over (y, z, x) which belongs to the exponential family (Assumption 1). Assumption 2 and 3 remain unaltered.\nThe tuple (y, z, x) will denote the complete event or complete data, while the tuple (y, x) is the observed event or the observed data. So we assume that our given data set D with n observations is expressed as {(y1, x1), . . . , (yn, xn)}. So sdEM\u2019s Equation (14) and (15) are the same, with the only difference that the natural gradient is now defined using the inverse of the Fisher information matrix for the statistical model p(y, z, x|\u03b8(\u00b5)). The same happens for Theorem 2.\nThe NCLL loss and the Hinge loss are equally defined as in Section 3.3, with the only difference that the computation of p(yt, xt|\u03b8) and p(xt|\u03b8) requires marginalization over z, p(yt, xt|\u03b8) = \u222b\np(yt, z, xt|\u03b8)dz, p(xt|\u03b8) = \u222b\np(y, z, xt|\u03b8)dydz. The updating equations for sdEM under partially observed data for the NCLL and Hinge loss are detailed in Table 2. New expected sufficient statistics need to be computed,\nEz [s(yt, z, xt)|\u03b8] = \u222b\ns(yt, z, xt)p(z|yt, xt, \u03b8)dz and Eyz[s(y, z, xt)|\u03b8] = \u222b\ns(y, z, xt)p(y, z|xt, \u03b8)dydz. As previously, we also show the updating equation for the negative log-likelihood (NLL) loss for comparison purposes."}, {"heading": "3.5 sdEM AND APPROXIMATE INFERENCE", "text": "For many interesting models [8], the computation of the expected sufficient statistics in the iteration equations shown in Table 1 and 2 cannot be computed in closed form. This is not a problem as far as we can define unbiased estimators for these expected sufficient statistics, since the equality of Equation (16) still holds. As it will be shown in the next section, we use sdEM to discriminatively train latent Dirichlet allocation (LDA) models [8]. Similarly to [28], for this purpose we employ collapsed Gibbs sampling to compute the expected sufficient statistics, Ez [s(yt, z, xt)|\u03b8], as it guarantees that at convergence samples are i.i.d. according to p(z|yt, xt, \u03b8)."}, {"heading": "4 EXPERMINTAL ANALYSIS", "text": ""}, {"heading": "4.1 TOY EXAMPLE", "text": "We begin the experimental analysis of sdEM by learning a very simple Gaussian naive Bayes model composed by a binary class variable Y and a single continuous predictor X . Hence, the conditional density of the predictor given the class variable is assumed to be normally distributed. The interesting part of this toy example is that the training data is generated by a different model: \u03c0(y = \u22121) = 0.5, \u03c0(x|y = \u22121) \u223c N(0, 3) and \u03c0(x|y = 1) \u223c\n0.8 \u00b7 N(\u22125, 0.1) + 0.2 \u00b7 N(5, 0.1). Figure 1 shows the histogram of the 30,000 samples generated from the \u03c0 distribution. The result is a mixture of 3 Gaussians, one in the center with a high variance associated to y = \u22121 and two narrows Gaussians on both sides associated to y = 1.\nsdEM can be used by considering 6 (non-minimal) sufficient statistics: N (\u22121) and N (1) as \u201ccounts\u201d associated to both classes, respectively; S(\u22121) and S(1) as the \u201csum\u201d of the x values associated to classes y = \u22121 and y = 1, respectively; and V (\u22121) and V (1) as the \u201csum of squares\u201d of the x values for each class. We also have five parameters which are computed from the sufficient statistics as follows: Two for the prior of class p(y = \u22121) = p(\u22121) = N (\u22121)/(N (\u22121)+N (1)) and p(1) = N (1)/(N (\u22121)+N (1)); and four for the two Gaussians which define the conditional of X given Y , \u00b5(\u22121) = S(\u22121)/N (\u22121), \u03c3(\u22121) = \u221a\nV (\u22121)/N (\u22121) \u2212 (S(\u22121)/N (\u22121))2, and equally for \u00b5(1)\nand \u03c3(1).\nThe sdEM\u2019s updating equations for the NCLL loss can be written as follows\nN (k) t+1 = N (k) t + \u03c1t(I[yt = k]\u2212 pt(k|xt)) + \u03c1t n\nS (k) t+1 = (1 \u2212 \u03c1t n )S (k) t + \u03c1txt (I[yt = k]\u2212 pt(k|xt))\nV (k) t+1 = (1 \u2212 \u03c1t n )V (k) t + \u03c1tx 2 t (I[yt = k]\u2212 pt(k|xt)) + \u03c1t n\nwhere k indexes both classes, k \u2208 {\u22121, 1}, I[\u00b7] denotes the indicator function, pt(k|xt) is an abbreviation of p(y = k|xt, \u03b8t), and \u03b8t is the parameter vector computed from the sufficient statistics at the t-th iteration.\nSimilarly, the sdEM\u2019s updating equations for the Hinge loss can be written as follows,\nN (k) t+1 = N (k) t + kyt\u03c1tI[ln\npt(yt|xt) pt(y\u0304t|xt) < 1] + \u03c1t n\nS (k) t+1 = (1\u2212 \u03c1t n )S (k) t + kyt\u03c1txtI[ln pt(yt|xt) pt(y\u0304t|xt) < 1]\nV (k) t+1 = (1 \u2212 \u03c1t n )V (k) t + kyt\u03c1tx 2 t I[ln pt(yt|xt) pt(y\u0304t|xt) < 1] + \u03c1t n\nwhere the product kyt is introduced in the updating equations to define the sign of the sum, and the indicator function I[\u00b7] defines when the hinge loss is null.\nIn the above set of equations we have considered as a conjugate prior for the Gaussians a three parameter NormalGamma prior, \u03bd = 1 and \u03b1\u03041 = 0 for S(k) and \u03b1\u03042 = 1 for V (k) [6, page 268], and a Beta prior with \u03bd = 0 and \u03b1\u0304 = 1 for N (k). We note that these priors assign zero probability to \u201cextreme\u201d parameters p(k) = 0 (i.e. N (k) = 0) and \u03c3(k) = 0 (i.e. V (k)/N (k) \u2212 (S(k)/N (k))2 = 0).\nFinally, the\u201cCheck-step\u201d (see Algorithm 1) performed before computing \u03b8t+1, and which guarantees that all sufficient statistics are correct, is implemented as follows:\nN (k) t+1 = max(N (k) t+1, \u03c1t n )\nV (k) t+1 = max(V (k) t+1,\n(S (k) t+1) 2\nN (k) t+1\n+ \u03c1t n )\nI.e., when the N (k) \u201ccounts\u201d are negative or too small or when the V (k) values lead to negative or null deviations \u03c3(k) \u2264 0, they are fixed with the help of the prior term.\nThe result of this experiment is given in Figure 1 and clearly shows the different trade-offs of both loss functions compared to maximum likelihood estimation. It is interesting to see how a generative model which does not match the underlying distribution is able to achieve a pretty high prediction accuracy when trained with a discrimintaive loss function (using the sdEM algorithm)."}, {"heading": "4.2 sdEM FOR TEXT CLASSIFICATION", "text": "Next, we briefly show how sdEM can be used to discriminatively train some generative models used for text classification, such as multinomial naive Bayes and a similar classifier based on latent Dirichlet allocation models [8]. Supplementary material with full details of these experiments and the Java code used in this evaluation can be download at: http://sourceforge.net/projects/sdem/"}, {"heading": "Multinomial Naive Bayes (MNB)", "text": "MNB assumes that words in documents with the same class or label are distributed according to an independent multinomial distribution. sdEM can be easily applied to train this\nmodel. The sufficient statistics are the \u201cprior class counts\u201d and the \u201cword counts\u201d for each class. The updating equations and the check step are the same as those of N (k)t in the previous toy example. Parameters of the MNB are computed simply through normalization operations. Two different conjugate Dirichlet distributions were considered: A \u201cLaplace prior\u201d where \u03b1\u0304i = 1; and a \u201dLog prior\u201d where \u03b1\u0304i = \u201clogarithm of the number of words in the corpus\u201d. We only report analysis for \u201cLaplace prior\u201d in the case of NCLL loss and for \u201cLog prior\u201d in the case of Hinge loss. Other combinations show similar results, although NCLL was more sensitive to the chosen prior.\nWe evaluate the application of sdEM to MNB with three well-known multi-class text classification problems: 20Newsgroup (20 classes), Cade (12 classes) and Reuters21578-R52 (52 classes). Data sets are stemmed. Full details about the data sets and the train/test data sets split used in this evaluation can be found in [14].\nFigure 2 shows the convergence behavior of sdEM with \u03bb =1e-05 when training a MNB by minimizing the Hinge loss (Hinge-MNB). In this figure, we plot the evolution of the Hinge loss but also the evolution of the NCLL loss and the normalized perplexity (i.e. the perplexity measure [8] divided by the number of training documents) at each epoch. We can see that there is a trade-off between the different losses. E.g., Hinge-MNB decreases the Hinge loss (as expected) but tends to increase the NCLL loss, while it\nonly decreases perplexity at the very beginning.\nFigure 3 displays the evolution of the classification accuracy of two MNBs trained minimizing the NCLL loss and the Hinge loss using sdEM. We compare them to: the standard MNB with a \u201cLaplace prior\u201d; the L2-regularized Logistic Regression; and the primal L2-regularized SVM. The two later methods were taken from the Liblinear toolkit v.18 [17]. As can be seen, sdEM is able to train simple MNB models with a performance very close to that provided by highly optimized algorithms."}, {"heading": "Latent Dirichlet Allocation (LDA)", "text": "We briefly show the results of sdEM when discriminatively training LDA models. We define a classification model equal to MNB, but where the documents of the same class are now modeled using an independent LDA model. We implement this model by using, apart from the \u201cprior class counts\u201d, the standard sufficient statistics of the LDA model, i.e. \u201cwords per hidden topic counts\u201d, associated to each class label. Similarly to [28], we used an online Collapsed Gibbs sampling method to obtain, at convergence, unbiased estimates of the expected sufficient statistics (see Table 2).\nThis evaluation was carried out using the standard train/test split of the Reuters21578-R8 (8 classes) and web-kb (4 classes) data sets [14], under the same preprocessing than in the MNB\u2019s experiments. Figure 4 shows the results of this comparison using 2-topics LDA models trained with the NCLL loss (NCLL-LDA), the Hinge loss (HingeLDA), and also the NLL loss (NLL-LDA) following the updating equations of Table 2. We compared these results with those returned by supervised-LDA (sLDA) [7] using the same prior, but this time with 50 topics because less topics produced worse results. We see again how a simple generative model trained with sdEM outperforms much more sophisticated models."}, {"heading": "5 CONCLUSIONS", "text": "We introduce a new learning algorithm for discriminative training of generative models. This method is based on a novel view of the online EM algorithm as a stochastic natural gradient descent algorithm for minimizing general discriminative loss functions. It allows the training of a wide set of generative models with or without latent variables, because the resulting models are always generative. Moreover, sdEM is comparatively simpler and easier to implement (and debug) than other ad-hoc approaches."}, {"heading": "Acknowledgments", "text": "This work has been partially funded from the European Union\u2019s Seventh Framework Programme for research, technological development and demonstration under grant agreement no 619209 (AMIDST project)."}, {"heading": "Appendices", "text": "This supplementary material aims to extend, detail and complement the experimental evaluation of sdEM given in the main paper. The structure of this document is as follows. Section A details the experimental evaluation of the multinomial naive Bayes classifier and introduces new experiments comparing with the stochastic gradient descent algorithm [10]. The experimental evaluation of sdEM applied to latent Dirichlet allocation models is detailed and extended in Section B. Section C points to the software repository where all the software code used in this experimental evaluation can be downloaded to reproduce all these results."}, {"heading": "A Multinomial Naive Bayes for text classification", "text": ""}, {"heading": "Description of the algorithm", "text": "As commented in the main paper, a multinomial Naive Bayes (MNB) classifier assumes that the words of the documents with the same class labels are distributed according to an independent multinomial probability distribution. In this section we evaluate the use of sdEM to discriminatively train MNB models using the NCLL and the Hinge loss functions. In the first case, such a model would be related to a logistic regression model; while in the second case we will obtain a model directly related to a linear support vector machine classifier [20].\nThe general updating equations for this problem can be found in the main paper in Table 2. But a detailed pseudocode description is now given in Algorithm 2 for the NCLL loss and in Algorithm 4 for the Hinge loss. In both cases, the sufficient statistics are the \u201dprior class counts\u201d stored in the matrix C and the \u201dword counts per class\u201d stored in the matrix N . Matrix M is introduced to allow efficient computations of the posterior probability of the class variable given a document d, p(Y |d,N,M,C, \u03b3). How this posterior is computed is detailed in Algorithm 3. In that way, the computational complexity of processing a label-document pair is linear in the number of words of the document. Finally, the function Normalize(\u00b7, \u00b7) produces a multinomial probability by normalizing the vector of counts. The second argument contains the prior correction considered\nin this normalization, i.e. the value which is added to each single component of the count vector to avoid null probabilities, similar to what is done in Algorithm 3.\nIn both algorithms, we consider a Dirichlet distribution prior for the multinomial distributions. As detailed in the header of these algorithms, two different priors are considered: prior P1, with Dirichlet\u2019s metaparameters \u03b1k = 1; and prior P2 with \u03b1k = ln |W |, where |W | denotes the total number of different words in the corpus. In both cases, the prior assigns null probability to parameters lying in the \u201dborder\u201d of the parameter space (i.e. when a null probability is assigned to some word).\nAs commented in the \u201dtoy example\u201d of the main paper in Section 4.1, the parametrization that we chose for this Dirichlet prior makes that the \u03bd parameter, arising in the exponential family form of this prior (see Assumption 2 of the main paper), be equal to null, \u03bd = 0. This can be seen when expressing the Dirichlet distribution in the following exponential form:\nDir(\u03b81, . . . , \u03b8k;\u03b11, . . . , \u03b1K) =\n\u220f\nk \u0393(\u03b1k)\n\u0393( \u2211 k \u03b1k) \u03b8\u03b11\u221211 . . . \u03b8 \u03b1K\u22121 K\n= exp\n(\n\u2211\nk\n(\u03b1k \u2212 1) ln \u03b8k + \u2211\nk\nln \u0393(\u03b1k)\u2212 ln \u0393( \u2211\nk\n\u03b1k)\n)\nThe second and third terms inside the exponent in the above equation correspond to the log partition function Ag(\u03b1) of the prior. The first term correspond to the dot product between the sufficient statistics (ln \u03b81, . . . , ln \u03b8K) and the natural parameters (\u03b11 \u2212 1, . . . , \u03b1k \u2212 1). As can be seen, the \u03bd parameter can be obviated in this definition, i.e. \u03bd = 0."}, {"heading": "Experimental Evaluation", "text": "As detailed in the main paper, we evaluate the application of sdEM to MNB with three well-known multi-\nclass text classification problems: 20Newsgroup, Cade and Reuters21578-R52. These data sets are stemmed. Full details about the data sets and the train/test data sets split used in this evaluation can be found in [14]. Although Table 3 shows some of the main statistics of these data sets.\nFigure 5 (a), Figure 5 (b), Figure 6 (a) and Figure 6 (b) show the convergence behavior of sdEM with \u03bb =1e-05 5 when training the MNB by minimizing the NCLL loss (NCLL-MNB) and by minimizing the Hinge loss (HingeMNB), respectively. In both cases, we plot the evolution of the NCLL loss, the Hinge loss and the normalized perplexity (i.e. the perplexity measure [8] divided by the number of training documents) of the trained model at each epoch. We can see that there is a trade-off between the different losses. For example, Hinge-MNB decreases the Hinge loss (as expected) but tends to increase the NCLL loss, while it only decreases perplexity at the very beginning. This last trend is much stronger when considering the P1 prior. A similar behavior can be observed for NCLL-MNB, with the main difference that the NCLL loss is an upper bound of the Hinge loss, and then when NCLL-MNB minimizes the NCLL loss it also minimizes the Hinge loss. Here it can be also observed that the perplexity remains quite stable specially for P1.\nFigure 5 (c) and Figure 6 (c) displays the evolution of the classification accuracy for the above models. We compare it to the standard MNB with a \u201cLaplace prior\u201d 6 and\n5Other values yield similar results and offer stable convergence, although at lower peace.\n6A \u201cLog prior\u201d was also evaluated but reported much worse results.\nwith L2-regularized Logistic Regression and primal L2regularized SVM implemented in the Liblinear toolkit v.18 [17]. For the case of the NCLL loss, the models seem to be more dependent of the chosen prior, specially for the Cade dataset. In any case, we can see that sdEM is able to train simple MNB models with a performance very close to that provided by highly optimized algorithms.\nA new set of experiments is included in this analysis comparing the MNB models learnt with sdEM with the classic stochastic gradient descent (SGD) algorithm. This evaluation is made using the Amazon12 and ACL-IMDB data sets (whose main details can be found in Table 3). We choose these data sets because they are binary classification problems, which are very well defined problems for logistic regression and linear SVM models. How SGD is used to train this model can be seen in [11].\nIn this evaluation we simply plot the evolution of the classification accuracy of the SGD algorithm when training a linear classifier using the NCLL loss (NCLL-SGD) and the Hinge loss (Hinge-SGD) with a L2 regularization for different learning rates or decreasing steps \u03c1t. SGD is implemented as detailed in [11], where the weight of the regularized term is fixed to 1e-4. As recommended in [11], learning rates \u03c1t for SGD are computed as follows: \u03c1t = \u03bb 1+\u03bb\u00b70.0001\u00b7t . We also look at the evolution of the classification accuracy of NCLL-MNB and Hinge-MNB with different priors in these two data sets and using different learning rates. In both cases, the plotted learning rates \u03c1t are selected by using different \u03bb values of the form \u03bb \u2208 {1, 0.1, 0.01, 0.001, 0.0001, 0.00001, . . .}. These results are shown in Figures 7 and 8. In each case, we con-\nsider the 5 consecutive \u03bb values with the quickest convergence speed."}, {"heading": "B Latent Dirichlet Allocation (LDA) for text classification", "text": ""}, {"heading": "Description of the algorithm", "text": "As commented in the main paper, we depart from a classification model similar to MNB, but where the documents of the same class are now modeled using an independent LDA model instead of a multinomial distribution. The generative process of each label-document pair in the corpus would be as follows [8]:\n1. Choose class label y \u223c p(y|\u03b8Y ), a multinomial probability.\n2. Choose N \u223c Poisson(\u03bey), the length of the document follows a Poisson distribution.\n3. Choose \u03c6y \u223c Dir(\u03b1y), a Dirichlet distribution with dimension |Z| (the meta-parameters are set to 1/|Z| in the experimental evaluation).\n4. For each of the N words wn:\n(a) Choose a topic zn \u223c Multinomial(\u03c6y) with dimension |Z|.\n(b) Choose a word wn \u223c p(wn|zn, \u03b2y), a multinomial probability conditioned on the topic zn.\nIn our case the unknown parameters are the \u03b2y for each class label, which defines the multinomial distribution of the step 4 (b) and the parameter \u03b8Y which defines the prior.\nWe denote by d to a document as a bag of words d = {w1, . . . , wN} and we denote by zd to a particular hidden topic assignment vector for the words in d. Then the sufficient statistics for this model would be a three dimensional matrix indexed by k \u2208 {1, ..., |Y |}, z \u2208 {1, . . . , |Z|} and w \u2208 {1, . . . , |W |}, where |W | denotes again the total number of different words in the corpus. The (k, z, w)-th component of this sufficient statistics matrix is computed as follows:\nsk,z,w(y, zd, d) = I[y = k] \u2211\nn\nI[zn = z]I[wn = w]\nAs previously commented in the main paper, these sufficient statistics would correspond to the \u201dwords per hidden topic counts\u201d. By adding the \u201dprior class counts\u201d, we would complete all the sufficient statistics that define this classification model.\nAs also commented in the main paper, similarly to [28], we used an online Collapsed Gibbs sampling method to obtain, at convergence, unbiased estimates of the expected sufficient statistics (see Section 3.5 in the main paper). This collapsed Gibbs sampling method makes used of the analytical marginalization of the parameter \u03c6y and samples in turn each of the indicator variables z1, . . . , zN . The probability of an indicator variable zn conditioned on all the words of the document and all the other indicators variables can be computed as follows:\np(zn|y, {zn\u2032}n\u2032 6=n, d) \u221d \u03b2y,zn,wn \u00b7 (S (\u2212wn) zn + \u03b1) (18)\nwhere S(\u2212wn)z = \u2211\nn\u2032 6=n I[zn\u2032 = z] and \u03b2y,zn,wn is the component of the \u03b2 parameter vector which defines the probability that the n-th word in document d is equal to wn given that hidden topic is zn and the class label of the document is y, p(wn|zn, \u03b2y).\nThe above equation defines a Markov chain that when it is run generates unbiased samples from its stationary distribution, p(zn|d, y) (after discarding the first burn-in samples). So, we could then compute the expected sufficient statistics required to apply the sdEM algorithm over these models. Let us note that under our online settings the \u03b2 parameter of Equation (18) is fixed to the values \u03b2t\u22121 estimated in the previous step and the this online collapsed Gibbs sampler only requires that the simulation is conditioned to the latent variables of the current observed document (i.e. it does not involve the hidden topics of the other documents in the corpus as happens with its batch counterpart).\nIn Algorithm 5 and Algorithm 7, we give a pseudocode description of the sdEM algorithm when applied to the this LDA classification model when using the NCLL and the Hinge loss functions, respectively. As can be seen, this algorithms does not directly relate to the standard LDA implementation, because we employ the same simplification used in the implementation7 of the sLDA algorithm [7] for multi-class prediction. This simplification assumes that all the occurrences of the same word in a document share the same hidden topic. The first effect of this assumption is that the number of hidden variables is reduced and the algorithm is much quicker. Whether this simplifying assumption has a positive or negative effect in the classification performance of the models is not evaluated here.\nLet us also see in the pseudo-code of these two algorithms, that Hinge-LDA will tend to be computationally more efficient than NCLL-LDA, because Hinge-LDA does not update any parameter when it classifies a document with a margin higher than 1. However, NCLL-LDA always updates all the parameters. When we deal with a high number of classes, this may imply a great difference in the computational performance. But this is something which is not evaluated in this first experimental study.\nWe also use a heuristic method8 to initialize the hidden topics variables zn of the incoming document which consists in sampling the hidden topics according to Equation 18, where S(\u2212wn)zn is computed on-the-fly i.e. for the first word is a vector of zeros and, then, it is updated according to the sampled topics. It is similar to running collapsed Gibbs sampling for one iteration.\nWe emphasis again that these algorithms are based on the updating equations given in the Table 2 of the main paper.\n7Code available at http://www.cs.cmu.edu/\u223cchongw/slda/ 8It is proposed in http://shuyo.wordpress.com/2011/06/27/collapsed-gibbs-sampling-estimation-for-latent-dirichlet-allocation-3/."}, {"heading": "Experimental Evaluation", "text": "As previously commented in the paper, this evaluation was carried out using the standard train/test split of the Reuters21578-R8 and Web-KB data sets [14], under the same preprocessing than in the MNB\u2019s experiments. In Table 3 some statistics about these data sets are given.\nWe used sdEM to train 2-topics LDA classification by minimizing the NCLL loss (NCLL-LDA), by minimizing the Hinge loss (Hinge-LDA), and also by minimizing the negative log-likelihood loss (NLL-LDA), following the updating equations of Table 2 in the main paper. We remind that at Figure 3 in the main paper, we show the results of the comparison of the classification accuracy of these models with the results obtained by supervised-LDA (sLDA) [7] using the same prior, but using 50 topics because with less topics it produced worse results.\nWe plot here at Figure 9, the convergence behavior at the training phase of the above models. The aim is to highlight how there is again a similar trade-off between the different losses when we train this model by minimizing a discriminative loss function such as NCLL or Hinge loss w.r.t. when we train this same model by minimizing a \u201dgenerative loss\u201d such as the negative log-likelihood (NLL).\nLooking at these figures we can see like neither NCLLLDA nor Hinge-LDA decrease the perplexity loss in opposite to NLL-LDA. We can also see that NLL-LDA does decrease either the NCLL or the Hinge loss but not so successfully as NCLL-LDA or Hinge-LDA."}, {"heading": "C sdEM Java Code", "text": "All the code used to build all the experiments presented in this supplemental material or in the main paper can be downloaded from the following code repository \u201dhttps://sourceforge.net/projects/sdem/\u201d (in \u201dFiles\u201d tab). This code is written in Java and mostly builds on Weka [18] data structures.\nAlgorithm 2 sdEM for Multinomial Naive Bayes with the NCLL loss. |d| denotes the number of different words in the current document d and |w|d to the number of times word w appears in document d. |W | denotes the total number of different words in the corpus. Require: D is randomly shuffled. Require: \u03b1 value as prior count for each word. Two values are considered \u03b1 = 1 and \u03b1 = ln |W |.\n1: \u2200k, w N [k][w] = \u03b1; C[k] = 1.0; M [k] = \u03b1 \u2217 |W |; 2: t = 0; 3: \u03b3 = 0; 4: repeat 5: for each label-document pair (y, d) do 6: t = t+ 1; 7: \u03c1 = 11+\u03bb\u00b7t 8: \u03b3 = \u03b3 + \u03b1 \u00b7 \u03c1\nn\n9: for each distinct word w in the document d do 10: N [y][w] = N [y][w] + \u03c1 \u00b7 |w|d \u00b7 (1 \u2212 p(Y = y|d,N,M,C, \u03b3)); 11: M [y] = M [y] + \u03c1 \u00b7 |w|d \u00b7 (1\u2212 p(Y = y|d,N,M,C, \u03b3)); 12: for k = 1, ..., |Y | : k 6= y do 13: oldV al = N [k][w]; 14: N [k][w] = N [k][w]\u2212 \u03c1 \u00b7 |w|d \u00b7 ps(Y = k|d,N,M,C, \u03b3); 15: N [k][w] = max(N [k][w], 0); 16: M [k] = M [k] + (N [k][w] \u2212 oldV al); 17: end for 18: end for 19: C[y] = C[y] + \u03c1 \u00b7 (1 \u2212 p(Y = y|d,N,M,C, \u03b3)); 20: for k = 1, ..., |Y | : k 6= y do 21: C[k] = C[k]\u2212 \u03c1 \u00b7 p(Y = k|d,N,M,C, \u03b3); 22: C[k] = max(C[k], 0); 23: end for 24: end for 25: until convergence 26: N\u0304 = Normalize(N, \u03b3); 27: C\u0304 = Normalize(C, \u03b3); 28: return N\u0304 and C\u0304;\nAlgorithm 3 Compute predictions P (Y = k|d,N,M,C, \u03b3) with Multinomial Naive Bayes. |d| denotes the number of different words in the current document d and |w|d denotes the number of times word w appears in document d. The function \u201dLogs2Probs\u201d simply exponentiate the log values and then normalize. Require: N , M , C, \u03b3 with non-negative values.\n1: \u2200k LogDC[k] = 0.0; 2: for k = 1, ..., |Y | do 3: LogDC[k] = ln(C[k] + \u03b3); 4: sumW = 0; 5: for each distinct word w in the document d do 6: LogDC[k] = LogDC[k] + |w|d \u00b7 ln(N [y][w] + \u03b3); 7: sumW = sumW + |w|d; 8: end for 9: LogDC[k] = LogDC[k]\u2212 sumW \u00b7 ln(M [k] + \u03b3 \u00b7 |d|);\n10: end for 11: return Logs2Probs(LogDC);\nAlgorithm 4 sdEM for Multinomial Naive Bayes with the Hinge loss. |d| denotes the number of different words in the current document d and |w|d denotes the number of times word w appears in document d. |W | denotes the total number of different words in the corpus. Require: D is randomly shuffled. Require: \u03b1 value as prior count for each word. Two values are considered \u03b1 = 1 and \u03b1 = ln |W |.\n1: \u2200k, w N [k][w] = \u03b1; C[k] = 1.0; M [k] = \u03b1 \u2217 |d|; 2: t = 0; 3: \u03b3 = 0; 4: repeat 5: for each label-document pair (y, d) do 6: t = t+ 1; 7: \u03c1 = 11+\u03bb\u00b7t 8: \u03b3 = \u03b3 + \u03b1 \u00b7 \u03c1\nn\n9: y\u0304 = argmaxy\u2032 6=y p(Y = y \u2032|x);\n10: if (ln p(Y = y|x)\u2212 ln p(Y = y\u0304|x)) > 1 then 11: Go for the next document; 12: end if 13: for each distinct word w in the document d do 14: N [y][w] = N [y][w] + \u03c1 \u00b7 |w|d \u00b7 (1 \u2212 p(Y = y|d,N,M,C, \u03b3)); 15: M [y] = M [y] + \u03c1 \u00b7 |w|d \u00b7 (1\u2212 p(Y = y|d,N,M,C, \u03b3)); 16: oldV al = N [y\u0304][w]; 17: N [y\u0304][w] = N [y\u0304][w]\u2212 \u03c1 \u00b7 |w|d \u00b7 p(Y = y\u0304|d,N,M,C, \u03b3); 18: N [y\u0304][w] = max(N [y\u0304][w], 0); 19: M [k] = M [k] + (N [y\u0304][w] \u2212 oldV al); 20: end for 21: C[y] = C[y] + \u03c1 \u00b7 (1 \u2212 p(Y = y|d,N,M,C, \u03b3)); 22: C[y\u0304] = C[y\u0304]\u2212 \u03c1 \u00b7 p(Y = y\u0304|d,N,M,C, \u03b3); 23: C[y\u0304] = max(C[y\u0304], 0); 24: end for 25: until convergence 26: N\u0304 = Normalize(N, \u03b3); 27: C\u0304 = Normalize(C, \u03b3); 28: return N\u0304 and C\u0304;\nAlgorithm 5 sdEM for the LDA based classifier using the NCLL loss. |d| denotes the number of different words in the current document d, |w|d denotes the number of times word w appears in document d and |Z| denotes the number of hidden topics in the LDA model. Require: D is randomly shuffled. Require: \u03b7 defines the prior for the \u201dword per topic counts\u201d. In the experiments, it is fixed to \u03b7 = 0.1.\n1: \u2200k, z, w N [k][z][w] = \u03b7|Z| ; C[k] = 1.0; M [k][z] = |W | \u03b7 |Z| ; 2: t = 0; 3: \u03b3 = 0; 4: repeat 5: for each label-document pair (y, d) do 6: t = t+ 1; 7: \u03c1 = 11+\u03bb\u00b7t 8: \u03b3 = \u03b3 + \u03b7|Z| \u00b7 \u03c1 n\n9: OnlineLDA(d,N [y],M [y],\u03c1, \u03b3, \u031f = (1\u2212 p(Y = y|d,N,M,C, \u03b3))); 10: for k = 1, ..., |Y | : k 6= y do 11: OnlineLDA(d,N [k],M [k],\u03c1, \u03b3, \u031f = \u2212p(Y = k|d,N,M,C, \u03b3)); 12: end for 13: C[y] = C[y] + \u03c1 \u00b7 (1 \u2212 p(Y = y|d,N,M,C, \u03b3)); 14: for k = 1, ..., |Y | : k 6= y do 15: C[k] = C[k]\u2212 \u03c1 \u00b7 p(Y = k|d,N,M,C, \u03b3); 16: C[k] = max(C[k], 0); 17: end for 18: end for 19: until convergence 20: N\u0304 = Normalize(N, \u03b3); 21: C\u0304 = Normalize(C, \u03b3); 22: return N\u0304 and C\u0304;\nAlgorithm 6 OnlineLDA(d,N ,M ,\u03c1, \u03b3, \u031f). The vector s would correspond to the expected sufficient statistics for d computed by online collpased Gibbs sampling. Require: d,N ,M ,\u03c1, \u03b3, \u031f properly computed.\n1: s = OnlineCollapsedGibbsSampling(d,N,M,\u03b3); 2: for each distinct word w in the document d do 3: for z=1,...,|Z| do 4: oldV al = N [z][w]; 5: N [z][w] = N [z][w] + \u03c1 \u00b7\u031f \u00b7 s[z][w]; 6: N [z][w] = max(N [z][w], 0); 7: M [z] = M [z] + (N [z][w]\u2212 oldV al); 8: end for 9: end for\nAlgorithm 7 sdEM for the LDA based classifier using the Hinge loss. |d| denotes the number of different words in the current document d, |w|d denotes the number of times word w appears in document d and |Z| denotes the number of hidden topics in the LDA model. Require: D is randomly shuffled. Require: \u03b7 defines the prior for the \u201dword per topic counts\u201d. In the experiments, it is fixed to \u03b7 = 0.1.\n1: \u2200k, z, w N [k][z][w] = \u03b7|Z| ; C[k] = 1.0; M [k][z] = |W | \u03b7 |Z| ; 2: t = 0; 3: \u03b3 = 0; 4: repeat 5: for each label-document pair (y, d) do 6: t = t+ 1; 7: \u03c1 = 11+\u03bb\u00b7t 8: \u03b3 = \u03b3 + \u03b7|Z| \u00b7 \u03c1 n 9: y\u0304 = argmaxy\u2032 6=y p(Y = y \u2032|x);\n10: if (ln p(Y = y|x)\u2212 ln p(Y = y\u0304|x)) > 1 then 11: Go for the next document; 12: end if 13: OnlineLDA(d,N [y],M [y],\u03c1, \u03b3, \u031f = (1\u2212 p(Y = y|d,N,M,C, \u03b3))); 14: OnlineLDA(d,N [y\u0304],M [y\u0304],\u03c1, \u03b3, \u031f = \u2212p(Y = y\u0304|d,N,M,C, \u03b3)); 15: C[y] = C[y] + \u03c1 \u00b7 (1 \u2212 p(Y = y|d,N,M,C, \u03b3)); 16: C[y\u0304] = C[y\u0304]\u2212 \u03c1 \u00b7 p(Y = y\u0304|d,N,M,C, \u03b3); 17: C[y\u0304] = max(C[y\u0304], 0); 18: end for 19: until convergence 20: N\u0304 = Normalize(N, \u03b3); 21: C\u0304 = Normalize(C, \u03b3); 22: return N\u0304 and C\u0304;"}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient Fisher scoring", "author": ["Sungjin Ahn", "Anoop Korattikara Balan", "Max Welling"], "venue": "In ICML,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Differential-geometrical methods in statistics", "author": ["Shun-ichi Amari"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1985}, {"title": "Natural gradient works efficiently in learning", "author": ["Shun-Ichi Amari"], "venue": "Neural Comput.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Methods of information geometry, volume 191", "author": ["Shun-ichi Amari", "Hiroshi Nagaoka"], "venue": "American Mathematical Soc.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Bayesian theory, volume 405", "author": ["Jos\u00e9 M Bernardo", "Adrian FM Smith"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Supervised topic models", "author": ["David M Blei", "Jon D McAuliffe"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Latent Dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Online learning and stochastic approximations", "author": ["L. Bottou"], "venue": "On-line learning in neural networks, 17(9)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010, pages 177\u2013186. Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic gradient descent tricks", "author": ["L. Bottou"], "venue": "Neural Networks: Tricks of the Trade, pages 421\u2013436. Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "NIPS, volume 4, page 2", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "On-line expectation\u2013 maximization algorithm for latent data models", "author": ["Olivier C", "E. Moulines"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Improving Methods for Single-label Text Categorization", "author": ["Ana Cardoso-Cachopo"], "venue": "PdD Thesis,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["Arthur P Dempster", "Nan M Laird", "Donald B Rubin"], "venue": "Journal of the Royal statistical Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1977}, {"title": "Structural extension to logistic regression: Discriminative parameter learning of belief net classifiers", "author": ["Greiner"], "venue": "Mach. Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Liblinear: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "JMLR, 9:1871\u20131874", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "The weka data mining software: an update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H Witten"], "venue": "ACM SIGKDD explorations newsletter,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Stochastic approximation algorithms and applications", "author": ["Harold Joseph Kushner", "G George Yin"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "DiscLDA: Discriminative learning for dimensionality reduction and classification", "author": ["S. Lacoste-Julien", "F. Sha", "M.I. Jordan"], "venue": "NIPS, volume 83, page 85", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "A tutorial on energy-based learning", "author": ["Yann LeCun", "Sumit Chopra", "Raia Hadsell", "M Ranzato", "F Huang"], "venue": "Predicting structured data,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "author": ["Radford M Neal", "Geoffrey E Hinton"], "venue": "In Learning in graphical models,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Statistical exponential families: A digest with flash cards", "author": ["Frank Nielsen", "Vincent Garcia"], "venue": "arXiv preprint arXiv:0911.4863,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Maximum margin Bayesian network classifiers", "author": ["F. Pernkopf", "M. Wohlmayr", "S. Tschiatschek"], "venue": "IEEE Trans. PAMI, 34(3):521\u2013532", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "The information geometry of mirror descent", "author": ["Garvesh Raskutti", "Sayan Mukherjee"], "venue": "arXiv preprint arXiv:1310.7780,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Online maximum-likelihood estimation for latent factor models", "author": ["D. Rohde", "O. Cappe"], "venue": "In Statistical Signal Processing Workshop (SSP),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Efficient estimations from a slowly convergent Robbins-Monro process", "author": ["David Ruppert"], "venue": "Technical report, Cornell University Operations Research and Industrial Engineering,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1988}, {"title": "Convergence of on-line EM algorithm", "author": ["Masa-aki Sato"], "venue": "In Proc. of the Int. Conf. on Neural Information Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "Numerical optimization", "author": ["SJ Wright", "J Nocedal"], "venue": "volume 2. Springer New York", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 20, "context": "Online learning methods based on stochastic approximation theory [21] have been a promising research direction to tackle the learning problems of the so-called Big Data era [1, 10, 12].", "startOffset": 65, "endOffset": 69}, {"referenceID": 0, "context": "Online learning methods based on stochastic approximation theory [21] have been a promising research direction to tackle the learning problems of the so-called Big Data era [1, 10, 12].", "startOffset": 173, "endOffset": 184}, {"referenceID": 9, "context": "Online learning methods based on stochastic approximation theory [21] have been a promising research direction to tackle the learning problems of the so-called Big Data era [1, 10, 12].", "startOffset": 173, "endOffset": 184}, {"referenceID": 11, "context": "Online learning methods based on stochastic approximation theory [21] have been a promising research direction to tackle the learning problems of the so-called Big Data era [1, 10, 12].", "startOffset": 173, "endOffset": 184}, {"referenceID": 8, "context": "Stochastic gradient descent (SGD) is probably the best known example of this kind of techniques, used to solve a wide range of learning problems [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 28, "context": "This algorithm and other versions [29] are usually employed to train discriminative models such as logistic regression or SVM [10].", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "This algorithm and other versions [29] are usually employed to train discriminative models such as logistic regression or SVM [10].", "startOffset": 126, "endOffset": 130}, {"referenceID": 18, "context": "There also are some successful examples of the use of SGD for discriminative training of probabilistic generative models, as is the case of deep belief networks [19].", "startOffset": 161, "endOffset": 165}, {"referenceID": 25, "context": "One of the main reasons is that statistical estimation or risk minimization problems of generative models involve the solution of an optimization problem with a large number of normalization constraints [26], i.", "startOffset": 203, "endOffset": 207}, {"referenceID": 15, "context": "Although successful solutions to this problem have been proposed [16, 22, 26, 32], they are based on adhoc methods which cannot be easily extended to other statistical models, and hardly scale to large data sets.", "startOffset": 65, "endOffset": 81}, {"referenceID": 21, "context": "Although successful solutions to this problem have been proposed [16, 22, 26, 32], they are based on adhoc methods which cannot be easily extended to other statistical models, and hardly scale to large data sets.", "startOffset": 65, "endOffset": 81}, {"referenceID": 25, "context": "Although successful solutions to this problem have been proposed [16, 22, 26, 32], they are based on adhoc methods which cannot be easily extended to other statistical models, and hardly scale to large data sets.", "startOffset": 65, "endOffset": 81}, {"referenceID": 20, "context": "Stochastic approximation theory [21] has also been used for maximum likelihood estimation (MLE) of probabilistic generative models with latent variables, as is the case of the online EM algorithm [13, 30].", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "Stochastic approximation theory [21] has also been used for maximum likelihood estimation (MLE) of probabilistic generative models with latent variables, as is the case of the online EM algorithm [13, 30].", "startOffset": 196, "endOffset": 204}, {"referenceID": 29, "context": "Stochastic approximation theory [21] has also been used for maximum likelihood estimation (MLE) of probabilistic generative models with latent variables, as is the case of the online EM algorithm [13, 30].", "startOffset": 196, "endOffset": 204}, {"referenceID": 29, "context": "In this paper we show that the derivation of Sato\u2019s online EM [30] can be extended for the discriminative learning of generative models by introducing a novel interpretation of this algorithm as a natural gradient algorithm [3].", "startOffset": 62, "endOffset": 66}, {"referenceID": 2, "context": "In this paper we show that the derivation of Sato\u2019s online EM [30] can be extended for the discriminative learning of generative models by introducing a novel interpretation of this algorithm as a natural gradient algorithm [3].", "startOffset": 224, "endOffset": 227}, {"referenceID": 25, "context": "In opposite to other discriminative learning approaches [26], models trained by sdEM can deal with missing data and latent variables in a principled way either when being learned or when making predictions, because at any moment they always define a joint probability distribution.", "startOffset": 56, "endOffset": 60}, {"referenceID": 2, "context": "sdEM could be used for learning using large scale data sets due to its stochastic approximation nature and, as we will show, because it allows to compute the natural gradient of the loss function with no extra cost [3].", "startOffset": 215, "endOffset": 218}, {"referenceID": 5, "context": "\u03bd is a positive scalar and \u1fb1 is a vector also belonging to S [6].", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "It is a dual set of the model parameter \u03b8 [2].", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "The transformation between \u03b8 and \u03bc is one-to-one: \u03bc is a dual set of the model parameter \u03b8 [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 24, "context": "For later convenience, we show the following relations between the Fisher Information matrices I(\u03b8) and I(\u03bc) for the probability distributions p(y, x|\u03b8) and p(y, x|\u03b8(\u03bc)), respectively [25]:", "startOffset": 184, "endOffset": 188}, {"referenceID": 3, "context": "However, when W is a Riemannian space [4], there are no orthonormal linear coordinates, and the squared length of vector dw is defined by the following equation,", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "G reduces to the identity matrix in the case of the Euclidean space [4].", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "Amari [3] shows that this solution can be computed by premultiplying the traditional gradient by the inverse of the Riemannian metric G, Theorem 1.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "As argued in [3], in statistical estimation problems we should used gradient descent methods which account for the natural gradient of the parameter space, as the parameter space of a statistical model (belonging to the exponential family or not) is a Riemannian space with the Fisher information matrix of the statistical model I(w) as the tensor metric [2], and this is the only invariant metric that must be given to the statistical model [2].", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "As argued in [3], in statistical estimation problems we should used gradient descent methods which account for the natural gradient of the parameter space, as the parameter space of a statistical model (belonging to the exponential family or not) is a Riemannian space with the Fisher information matrix of the statistical model I(w) as the tensor metric [2], and this is the only invariant metric that must be given to the statistical model [2].", "startOffset": 355, "endOffset": 358}, {"referenceID": 1, "context": "As argued in [3], in statistical estimation problems we should used gradient descent methods which account for the natural gradient of the parameter space, as the parameter space of a statistical model (belonging to the exponential family or not) is a Riemannian space with the Fisher information matrix of the statistical model I(w) as the tensor metric [2], and this is the only invariant metric that must be given to the statistical model [2].", "startOffset": 442, "endOffset": 445}, {"referenceID": 29, "context": "Sato\u2019s online EM algorithm [30] is used for maximum likelihood estimation of missing data-type statistical models.", "startOffset": 27, "endOffset": 31}, {"referenceID": 29, "context": "Sato [30] derived the stochastic updating equation of online EM by relying on the free energy formulation, or lower bound maximization, of the EM algorithm [24] and on a discounting averaging method.", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "Sato [30] derived the stochastic updating equation of online EM by relying on the free energy formulation, or lower bound maximization, of the EM algorithm [24] and on a discounting averaging method.", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "It shows that online EM is equivalent to a stochastic gradient descent with I(\u03bct) as coefficient matrices [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 23, "context": "But the key insights of the above derivation, which were not noted by Sato, is that Equation (12) is also valid for other loss functions different from the marginal log-likelihood; and that the convergence of Equation (11) does not depend on the formulation of the EM as a \u201clower bound maximization\u201d method [24].", "startOffset": 307, "endOffset": 311}, {"referenceID": 8, "context": "Although the above The loss function is assumed to satisfy the mild conditions given in [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 20, "context": "We note that this loss function satisfies the following equality, which is the base for a stochastic approximation method [21], E [", "startOffset": 122, "endOffset": 126}, {"referenceID": 2, "context": "Similarly to Amari\u2019s natural gradient algorithm [3], the main problem of sdEM formulated as in Equation (14) is the computation of the inverse of the Fisher information matrix at each step, which becomes even prohibitive for large models.", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "This can be proved by the invariant property of the Fisher information metric to one-to-one reparameterizations or, equivalently, transformations in the system of coordinates [2, 4].", "startOffset": 175, "endOffset": 181}, {"referenceID": 3, "context": "This can be proved by the invariant property of the Fisher information metric to one-to-one reparameterizations or, equivalently, transformations in the system of coordinates [2, 4].", "startOffset": 175, "endOffset": 181}, {"referenceID": 26, "context": "An alternative proof to Theorem 2 based on more recent results on information geometry has been recently given in [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "In this section we do not attempt to give a formal proof of the convergence of sdEM, since very careful technical arguments would be needed for this purpose [9].", "startOffset": 157, "endOffset": 160}, {"referenceID": 20, "context": "We simply go through the main elements that define the convergence of sdEM as an stochastic approximation method [21].", "startOffset": 113, "endOffset": 117}, {"referenceID": 8, "context": "According to Equation (14), sdEM can be seen as a stochastic gradient descent method with the inverse of the Fisher information matrix I(\u03bc) as a coefficient matrix [9].", "startOffset": 164, "endOffset": 167}, {"referenceID": 30, "context": "term and log-barrier function [31] i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "So, if the sequence (\u03bct)t\u22650 converges, it will probably converge to the global minimum (\u03bc, \u03b8 = \u03b8(\u03bc)) if L(\u03b8) is convex, or to a local minimum if L(\u03b8) is not convex [9].", "startOffset": 164, "endOffset": 167}, {"referenceID": 10, "context": "Following [11], we consider steps sizes of the form \u03c1t = (1 + \u03bbt), where \u03bb is a positive scalar4.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "Like the online EM algorithm [30, 13], Algorithm 1 resembles the classic expectation maximization algorithm [15] since, as we will see in the next section, the gradient is computed using expected The prior p would need to be suitably chosen.", "startOffset": 29, "endOffset": 37}, {"referenceID": 12, "context": "Like the online EM algorithm [30, 13], Algorithm 1 resembles the classic expectation maximization algorithm [15] since, as we will see in the next section, the gradient is computed using expected The prior p would need to be suitably chosen.", "startOffset": 29, "endOffset": 37}, {"referenceID": 14, "context": "Like the online EM algorithm [30, 13], Algorithm 1 resembles the classic expectation maximization algorithm [15] since, as we will see in the next section, the gradient is computed using expected The prior p would need to be suitably chosen.", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "Margin-based loss functions have been extensively used and studied by the machine learning community for binary and multi-class classification problems [5].", "startOffset": 152, "endOffset": 155}, {"referenceID": 25, "context": "However, in our view, the application of margin-based losses (different from the negative conditional log-likelihood) for discriminative training of probabilistic generative models is scarce and based on ad-hoc learning methods which, in general, are quite sophisticated [26].", "startOffset": 271, "endOffset": 275}, {"referenceID": 22, "context": "\u2019s ideas [23] about energy-based learning for prediction problems.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "[23] define the Hinge loss for energy-based models as follows,", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "For many interesting models [8], the computation of the expected sufficient statistics in the iteration equations shown in Table 1 and 2 cannot be computed in closed form.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "As it will be shown in the next section, we use sdEM to discriminatively train latent Dirichlet allocation (LDA) models [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 27, "context": "Similarly to [28], for this purpose we employ collapsed Gibbs sampling to compute the expected sufficient statistics, Ez [s(yt, z, xt)|\u03b8], as it guarantees that at convergence samples are i.", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "Next, we briefly show how sdEM can be used to discriminatively train some generative models used for text classification, such as multinomial naive Bayes and a similar classifier based on latent Dirichlet allocation models [8].", "startOffset": 223, "endOffset": 226}, {"referenceID": 13, "context": "Full details about the data sets and the train/test data sets split used in this evaluation can be found in [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "the perplexity measure [8] divided by the number of training documents) at each epoch.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "18 [17].", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "Similarly to [28], we used an online Collapsed Gibbs sampling method to obtain, at convergence, unbiased estimates of the expected sufficient statistics (see Table 2).", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "This evaluation was carried out using the standard train/test split of the Reuters21578-R8 (8 classes) and web-kb (4 classes) data sets [14], under the same preprocessing than in the MNB\u2019s experiments.", "startOffset": 136, "endOffset": 140}, {"referenceID": 6, "context": "We compared these results with those returned by supervised-LDA (sLDA) [7] using the same prior, but this time with 50 topics because less topics produced worse results.", "startOffset": 71, "endOffset": 74}], "year": 2014, "abstractText": "Stochastic discriminative EM (sdEM) is an online-EM-type algorithm for discriminative training of probabilistic generative models belonging to the exponential family. In this work, we introduce and justify this algorithm as a stochastic natural gradient descent method, i.e. a method which accounts for the information geometry in the parameter space of the statistical model. We show how this learning algorithm can be used to train probabilistic generative models by minimizing different discriminative loss functions, such as the negative conditional loglikelihood and the Hinge loss. The resulting models trained by sdEM are always generative (i.e. they define a joint probability distribution) and, in consequence, allows to deal with missing data and latent variables in a principled way either when being learned or when making predictions. The performance of this method is illustrated by several text classification problems for which a multinomial naive Bayes and a latent Dirichlet allocation based classifier are learned using different discriminative loss functions.", "creator": "LaTeX with hyperref package"}}}