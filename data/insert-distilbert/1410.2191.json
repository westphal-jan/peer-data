{"id": "1410.2191", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2014", "title": "Learning manifold to regularize nonnegative matrix factorization", "abstract": "inthischapterwediscusshowtolearnanoptimalmanifoldpresentationto regularize nonegative matrix factorization ( nmf ) for real data derived representation problems. nmf, in whichtriestorepresentanonnegativedatamatrixasaproductoftwolowrank nonnegative matrices, has been a popular method for generating data representation due essentially to its ability and to explore the latent part - based structure of data. recent study shows that lots of data distributions have manifold structures, and we should respect the manifold structure when the data are represented. recently, manifold regularized nmf used a nearest neighbor graph to coordinate regulate towards the learning of factorization parameter matrices and has shown its advantage over traditional nmf methods for data representation problems. however, how to construct an optimal graph to present the manifold prop - erly remains a difficultproblem due to the graph modelselection, noisy features, and nonlinear distributed data. in this chapter, we introduce three simultaneously effective methods to solve these problems of graph construction for manifold regularized nmf. multiple graph learning is proposed to solve the problem of graph model selection, adaptive graph learning via feature selection is proposed to solve unsuccessfully the problem of constructing a graph from noisy features, while multi - kernel learning - based graph construction is used to solve the problem of learning a graph from nonlinearly distributed data.", "histories": [["v1", "Fri, 3 Oct 2014 09:25:43 GMT  (55kb)", "http://arxiv.org/abs/1410.2191v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jim jing-yan wang", "xin gao"], "accepted": false, "id": "1410.2191"}, "pdf": {"name": "1410.2191.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jing-Yan Wang", "Xin Gao"], "emails": ["jimjywang@gmail.com", "xin.gao@kaust.edu.sa"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 0.\n21 91\nv1 [\ncs .L\nG ]\n3 O"}, {"heading": "1 Introduction", "text": "Nonnegative matrix factorization (NMF) [13, 20, 27, 35, 33, 34] has been a popular data representation method in field of machine learning. Given a matrix of nonnegative data, where each column is a data sample, NMF tries to represent it as a product of two low rank nonnegative matrices, i.e., a basis matrix and a coefficient matrix.\nJim Jing-Yan Wang \u00b7 Xin Gao Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology (KAUST), Thuwal 23955-6900, Saudi Arabia e-mail: jimjywang@gmail.com , e-mail: xin.gao@kaust.edu.sa\n1\nThe coefficient matrix could be used as new representations of the data samples. Due to its ability to explore the latent part-based structure of the data, it has been widely used to many real-world applications, such as computer vision [25], optical sensing [6] and bioinformatics [44, 46].\nTo formulate the problem of NMF in a matrix form, we assume that we have a data set of n data samples, and their feature vectors are {x1, \u00b7 \u00b7 \u00b7 ,xn}, where xi \u2208 Rd+ is a d-dimensional nonnegative feature vector of the i-th sample. The data set is organized as a nonnegative matrix X = [x1, \u00b7 \u00b7 \u00b7 ,xn] \u2208 Rd\u00d7n+ . The NMF problem is to learn two nonnegative matrices H \u2208 Rd\u00d7m+ and W \u2208 R m\u00d7n + so that the original matrix X can be reconstructed as the product of H and W ,\nX \u2248 HW. (1)\nIn H each column can be regarded as a basis vector. m is the number of basis vectors and usually m \u226a n. In this way, each xi can be reconstructed as the linear combination of the basis vectors in H and the i-th column in W is the combination coefficient vector,\nxi \u2248 Hwi, (2)\nwhere wi \u2208 Rm+ is the i-th column of W , and it can be viewed as a low-dimensional nonnegative representation of xi. To learn both the matrices H and W , the reconstruction error is usually measured by a squared \u21132 norm distance between X and HW ,\nmin H,W\n\u2016X \u2212HW\u201622\ns.t. H \u2265 0,W \u2265 0. (3)\nThis problem can be solved by the Lagrange multiplier method [15]. Some other functions can also be used as reconstruction error measures, such as Kullback - Leibler divergence [11], earth mover\u2019s distance [36], etc.\nRecent studies on manifold learning [7] show that lots of data distributions have low-dimensional manifold structures. The manifold structure has been explored by component analysis methods [28, 10], ranking score learning methods [67, 40], etc. It usually constructs a nearest neighbor graph from the original feature space to present the manifold structure of the data set, and then use the affinity matrix of the graph to regularize the learned outputs from the data. Given a data sample xi, we denote its nearest neighbor set as Ni. A graph is constructed as G = (X ,E ,A), where X = {x1, \u00b7 \u00b7 \u00b7 ,xn} is the node set of the graph and each node is a data point, E is the edge set, and A \u2208 Rn\u00d7n is the affinity matrix of the graph. E is defined as\nE = {(xi,x j)|xi,x j \u2208 X , and x j \u2208 Ni} (4)\nand A is defined as\nAi j =\n{\ng(xi,x j), i f (xi,x j) \u2208 E , 0, otherwise.\n(5)\nwhere g(xi,x j) = exp ( \u2016xi\u2212x j\u201622 2\u03c3 2i j ) is a Gaussian kernel function and \u03c3i j is the bandwidth parameter. Cai et al. [4] argued that when the data are represented using NMF, the manifold structure should also be respected and they used the graph affinity matrix to regularize the learning of the coefficient matrix W . It is imposed that if two data samples, xi and x j, are connected in the graph and their affinity is large, their coefficient vectors should also be close to each other. They used the squared \u21132 norm distance to measure how close they are from each other, and proposed the following minimization problem,\nmin W 1 2\nn\n\u2211 i, j=1 Ai j\u2016wi \u2212w j\u201622\ns.tW \u2265 0.\n(6)\nIn this way, the local manifold information is mapped from the original space to the coefficient vector space by minimizing the pairwise coefficient vector distance weighted by the affinity calculated from the original feature space. This problem is combined with the original NMF problem in (3) and the following graph regularized NMF problem is obtained,\nmin H,W \u2016X \u2212HW\u201622 +\u03b1 1 2\nn\n\u2211 i, j=1 Ai j\u2016wi \u2212w j\u201622\ns.t. H \u2265 0,W \u2265 0,\n(7)\nwhere \u03b1 is a tradeoff parameter. However, on the construction of the graph, the following problems remain diffi-\ncult, which prevent the graph regularization from being widely used to NMF problems:\n1. To find the nearest neighbors of a data sample, we need to determine which distance function is optimal. Moreover, the size of the neighborhood should also be decided.\n2. To calculate the affinity measure between a pair of data samples, we should select an optimal bankwidth parameter for the Gaussian kernel in (5). Moreover, besides Gaussian kernel function, we can use some other affinity measures to calculate the affinity matrix. Thus it is also necessary to select an optimal affinity function.\n3. If some features extracted from the data samples are noisy or irrelevant to the problem on hand, the graph constructed from the original feature space is not suitable to regularize the NMF learning. In this case, usage of original features to construct the graph will harm the learning performance.\n4. If the data distribution of the data set is not linear, and we still use the linear functions to construct the graph, the graph will not be optimal for learning of NMF. In this case, we should use kernel tricks [24] to map the data to a nonlinear space first and then construct graph.\nTo solve these problems, in the following sections we will introduce some varieties of graph regularized NMF. In Section 2, we introduce the multiple graph regularize NMF to solve the first two problems by learning an optimal graph from some candidate graphs with different distance functions, graph models and parameters. In Section 3, to solve the third problem, we introduce a method to integrate feature selection to NMF and also use it to refine the graph for regularization of NMF. In Section 4, we introduce a method to combine multi-kernel learning and NMF, and use it to refine the graph in the kernel space to solve the fourth problem."}, {"heading": "2 Multiple graph regularized NMF", "text": "To construct a graph for the regularization of NMF, we need to select a distance function and a neighborhood size to determine the nearest neighbors of a given data sample. Moreover, we also need to select an affinity function and its corresponding parameters to calculate the affinities between each pair of connected data samples. We can use a linear search in the hyper space of distance function, affinity function, and parameters to find the optimal combination of choices to construct the graph. However, this strategy is time-consuming and easy to be over-fitting to the training data set. To overcome this problem, in our previous work [42], we proposed the multiple graph learning method to construct an optimal graph for regularization of NMF.\nWe assume we have l different candidate graphs and they are constructed with combinations of different distance functions, affinity functions, and parameters. Their affinity matrices are denoted as {A1, \u00b7 \u00b7 \u00b7 ,Al}, where Ak \u2208 Rn\u00d7n+ is the affinity matrix of the k-th graph. The problem is to learn an optimal graph affinity matrix A\u2217\nfrom them. To solve this problem, we assume that the optimal graph affinity matrix can be obtained by the linear combination of the candidate graph affinity matrices,\nA\u2217 = l\n\u2211 k=1 \u00b5kAk,\ns.t. l\n\u2211 k=1\n\u00b5k = 1,\u00b5k \u2265 0, (8)\nwhere \u00b5k is the linear combination weight of the k-th graph affinity matrix. The constrains \u2211lk=1 \u00b5k = 1 and \u00b5k \u2265 0 are imposed to prevent the negative weights. In this way, we transfer the problem of selecting an optimal graph from a pool of candidate graphs to a problem of learning the combination weights. To learn the combination weights, we integrate (8) to (7), and obtain the following minimization problem,\nmin H,W,\u00b5k |k=1,\u00b7\u00b7\u00b7 ,l \u2016X \u2212HW\u201622 +\u03b1 1 2\nn\n\u2211 i, j=1\nl\n\u2211 k=1 \u00b5kAki j\u2016wi \u2212w j\u2016 2 2 +\u03b2\nl\n\u2211 k=1 \u00b52k\ns.t. H \u2265 0,W \u2265 0, l\n\u2211 k=1 \u00b5k = 1,\u00b5k \u2265 0.\n(9)\nPlease note that the last term \u2211lk=1 \u00b52k in the objective function is used to prevent A\u2217 from over-fitting to one single candidate graph, and \u03b2 is its tradeoff parameter. The optimization of this problem is solved using an alternate optimization strategy in an iterative algorithm. In each iteration, we first fix \u00b5k,k = 1, \u00b7 \u00b7 \u00b7 , l, and update H and W , and then fix H and W to update \u00b5k,k = 1, \u00b7 \u00b7 \u00b7 , l. The detailed optimization procedures are given as follows:\n\u2022 Updating H and W while fixing \u00b5k,k = 1, \u00b7 \u00b7 \u00b7 , l: When \u00b5k,k = 1, \u00b7 \u00b7 \u00b7 , l are fixed, the problem in (9) is reduced to the problem in (7), which can be solved using the Lagrange multiplier method introduced in [4]. \u2022 Updating \u00b5k,k = 1, \u00b7 \u00b7 \u00b7 , l while fixing H and W : When H and W are fixed, and only \u00b5k,k = 1, \u00b7 \u00b7 \u00b7 , l are considered, the problem in (9) turns to\nmin \u00b5k|k=1,\u00b7\u00b7\u00b7 ,l \u03b1 1 2\nn\n\u2211 i, j=1\nl\n\u2211 k=1 \u00b5kAki j\u2016wi \u2212w j\u2016 2 2 +\u03b2\nl\n\u2211 k=1 \u00b52k\ns.t. l\n\u2211 k=1 \u00b5k = 1,\u00b5k \u2265 0.\n(10)\nThis problem is a linearly constrained quadratic programming (QP) problem [32, 8, 26] and can be solved easily using the active set algorithm [18, 12, 5].\nThe advantages of this method are two folds:\n1. In our method, no class label or any other supervision information is needed to learn the optimal graph. Traditional methods, such as linear search or cross validation need the supervision information to select the optimal graph. However, in many real-world applications, no supervision information is provided. In our method, we use the NMF objective as a criterion to learn an optimal graph and thus avoid the input of supervision information.\n2. The learned graph affinity does not fit to any one single graph, because we used a \u21132 norm term to regularize the learning of graph weights. In this way, the proposed method can explore the graph space to utilize the complementary information provided by different graphs."}, {"heading": "3 Feature selection for adaptive graph regularized NMF", "text": "Traditional graph construction methods construct a graph from the original feature space of the data samples. However, in many cases, there are many noisy and/or irrelevant features in the feature set and these features will affect the graph [9]. Actually, using noisy features and/or irrelevant features of data samples to construct the nearest neighbors and to calculate the affinity matrix will make the obtained graph unsuitable for the problem on hand, or even make the performance poorer than the method without graph regularization. To solve this problem, we proposed to integrate feature selection to NMF and use the selected features to construct the graph in our previous work [48]. To this end, we proposed to weight each feature with a feature weight and represent a data sample xi as follows,\nxi \u2192 [u1xi1, \u00b7 \u00b7 \u00b7 ,udxid ] \u22a4 = diag(u)xi,\ns.t. d\n\u2211 c=1\nuc = 1,uc \u2265 0,c = 1, \u00b7 \u00b7 \u00b7 ,d, (11)\nwhere xic is the c-th feature of the i-th sample, and wc is the weight of the c-th feature. The constrains \u2211dc=1 uc = 1 and uc \u2265 0 are imposed to prevent negative feature weights. We hope that a feature relevant to the problem on hand can obtain a large feature weight while a noisy feature can be assigned with a small feature weight. We use the same feature weight vector to weight both the original data matrix X and the basis matrix H,\nX \u2192 diag(u)X , and H \u2192 diag(u)H. (12)\nMoreover, we also calculate the affinity matrix using the weighted feature vectors as\nAui j =\n{\ng(diag(u)xi,diag(u)x j), i f (xi,x j) \u2208 E , 0, otherwise.\n(13)\nSubstituting both (12) and (13) to (7), we have\nmin H,W,u \u2016diag(u)(X \u2212HW)\u201622 +\u03b1 1 2\nn\n\u2211 i, j=1 Aui j\u2016wi \u2212w j\u2016 2 2\ns.t. H \u2265 0,W \u2265 0, d\n\u2211 c=1 uc = 1,uc \u2265 0,c = 1, \u00b7 \u00b7 \u00b7 ,d.\n(14)\nIn this problem, the feature weight vector u is also a parameter to be optimized together with H and W , and the affinity matrix is based on u. In this way, the feature weight vector is learned using the criterion of NMF and it is further used to refine the graph to regularize the NMF parameters. To optimize the problem in (14), we employ the alternate optimization strategy in an iterative algorithm. In each iteration,\nwe first update Au according to the feature weight vector u in previous iteration, then H and W are optimized using Au, and finally we optimize u by fixing Au, and H and W . The detailed optimization procedures are as follows,\n\u2022 Updating H and W while fixing u and Au: When u and Au are fixed, the problem in (14) is reduced to\nmin H,W \u2016diag(u)(X \u2212HW)\u201622 +\u03b1 1 2\nn\n\u2211 i, j=1 Aui j\u2016wi \u2212w j\u2016 2 2\ns.t. H \u2265 0,W \u2265 0.\n(15)\nThis problem can also be solved using the Lagrange multiplier method. \u2022 Updating u while fixing H, W and Au: When we only consider u as a variable\nand fix H, W and Au, the problem is reduced to\nmin u\n\u2016diag(u)(X \u2212HW)\u201622\ns.t. d\n\u2211 c=1\nuc = 1,uc \u2265 0,c = 1, \u00b7 \u00b7 \u00b7 ,d. (16)\nThis problem can also be solved as a quadratic programming (QP) problem. \u2022 Updating Au according to u: After u is updated, we use it to calculate a new\naffinity matrix Au as in (13). Note that the neighborhood of each data sample is also updated in the updated feature space weighted by u."}, {"heading": "4 Multi-kernel learning for adaptive graph regularized NMF", "text": "In this section, we solve the problem of learning a graph from nonlinearly distributed data. When the data samples are distributed nonlinearly, and we use linear original feature space to find the nearest neighbors and calculate the affinities, the obtained graph will not be suitable for further regularization of NMF. In this case, we can use a nonlinear function to map a data sample xi from a nonlinear space to a highdimensional linear space [14], xi \u2192 \u03a6(xi) \u2208 Rd \u2032 , where d\u2032 \u2265 d is the dimension of the high-dimensional linear space, then construct the graph and perform NMF [37]. In this case, we assume that the basis vectors can be obtained by the linear combination of the nonlinear mapping of the training samples,\nhk = n\n\u2211 i=1 gki\u03a6(xi) = \u03a6(X)gk,\nH = \u03a6(X)G, (17)\nwhere hk is the k-th basis vector, \u03a6(X)= [\u03a6(x1), \u00b7 \u00b7 \u00b7 ,\u03a6(xn)]\u2208Rd \u2032\u00d7n, gk = [gk1, \u00b7 \u00b7 \u00b7 ,gkn] \u22a4 \u2208 Rn+ is the nonnegative linear combination weight vector of the k-th basis vector, and G = [g1, \u00b7 \u00b7 \u00b7 ,gl ]\u2208 R n\u00d7m + is the nonnegative linear combination weight matrix. In this\nway, the learning of H is transferred to the learning of the parameter matrix G. Substituting \u03a6(X) and H in (17) to (3), we obtain the NMF problem in the nonlinear mapping space,\nmin H,G\n\u2016\u03a6(X)\u2212\u03a6(X)GW\u201622\n= Tr ( \u03a6(X)\u22a4\u03a6(X) ) \u2212 2Tr ( \u03a6(X)\u22a4\u03a6(X)GW ) +Tr ( \u03a6(X)\u22a4\u03a6(X)GWW\u22a4G\u22a4 )\ns.t. H \u2265 0,G \u2265 0. (18) Actually, the nonlinear mapping function \u03a6(x) is not defined explicitly, but implicitly via a kernel function which is defined as the inner-product between \u03a6(xi) and \u03a6(x j), K(xi,x j) = \u03a6(xi)\u22a4\u03a6(x j). Moreover, given the training data matrix X and its mapping matrix \u03a6(X), we can define the kernel matrix as\nK = \u03a6(X)\u22a4\u03a6(X) = [Ki j]i, j=1,\u00b7\u00b7\u00b7 ,n, Ki j = \u03a6(xi)\u22a4\u03a6(x j) = K(xi,x j). (19)\nSimilar to multiple graph learning, if we have a pool of candidate kernel functions with parameters, we may also learn an optimal kernel function by the linear combination of them,\nK = l\n\u2211 k=1 \u00b5kKk,\ns.t. l\n\u2211 k=1\n\u00b5k = 1,\u00b5k \u2265 0,k = 1, \u00b7 \u00b7 \u00b7 , l, (20)\nwhere Kk is the k-th kernel and \u00b5k is its linear combination weight. Substituting (20) to (18), we have\nmin H,G,{\u00b5k}k=1,\u00b7\u00b7\u00b7 ,l\nl\n\u2211 k=1\n\u00b5k [ Tr (K)\u2212 2Tr (KGW )+Tr ( KGWW\u22a4G\u22a4 )]\ns.t. H \u2265 0,G \u2265 0, l\n\u2211 k=1 \u00b5k = 1,\u00b5k \u2265 0,k = 1, \u00b7 \u00b7 \u00b7 , l.\n(21)\nMoreover, we also use the combined kernel to measure the affinity between the neighboring data samples,\nA\u00b5i j =\n{\n\u2211lk=1 \u00b5kKki j , i f (xi,x j) \u2208 E , 0, otherwise,\n(22)\nand use it to regularize the learning of the coefficient vectors. The overall optimization problem is obtained as follows,\nmin H,G,{\u00b5k}k=1,\u00b7\u00b7\u00b7 ,l\nl\n\u2211 k=1\n\u00b5k [ Tr (K)\u2212 2Tr (KGW )+Tr ( KGWW\u22a4G\u22a4 )]\n+\u03b1 1 2\nn\n\u2211 i, j=1\n(\nl\n\u2211 k=1 \u00b5kKki j\n)\n\u2225 \u2225wi \u2212w j \u2225 \u2225 2 2 +\u03b2\nl\n\u2211 k=1 \u00b52k\ns.t. H \u2265 0,G \u2265 0, l\n\u2211 k=1 \u00b5k = 1,\u00b5k \u2265 0,k = 1, \u00b7 \u00b7 \u00b7 , l,\n(23)\nwhere \u2211lk=1 \u00b52k is added to the objective function to prevent it from over-fitting to one single kernel, and \u03b1 and \u03b2 are tradeoff parameters. The solution of this problem can also be obtained by using alternate optimization strategy in an iterative algorithm. In each iteration, H, G or {\u00b5k}k=1,\u00b7\u00b7\u00b7 ,l are updated in turn while the other ones are fixed.\n\u2022 Updating H and G while fixing {\u00b5k}k=1,\u00b7\u00b7\u00b7 ,l : When {\u00b5k}k=1,\u00b7\u00b7\u00b7 ,l are fixed, the problem in (23) is reduced to\nmin H,G\nl\n\u2211 k=1\n\u00b5k [ Tr (K)\u2212 2Tr (KGW )+Tr ( KGWW\u22a4G\u22a4 )]\n+\u03b1 1 2\nn\n\u2211 i, j=1\n(\nl\n\u2211 k=1 \u00b5kKki j\n)\n\u2225 \u2225wi \u2212w j \u2225 \u2225 2 2\ns.t. H \u2265 0,G \u2265 0,\n(24)\nwhich can be solved by Lagrange multiplier method. \u2022 Updating {\u00b5k}k=1,\u00b7\u00b7\u00b7 ,l while fixing H and G: When H and G are fixed, the prob-\nlem in (23) is reduced to\nmin {\u00b5k}k=1,\u00b7\u00b7\u00b7 ,l\nl\n\u2211 k=1 \u00b5k\n[\nTr (K)\u2212 2Tr (KGW )+Tr ( KGWW\u22a4G\u22a4 ) +\u03b1 1 2\nn\n\u2211 i, j=1 Kki j \u2225 \u2225wi \u2212w j \u2225 \u2225 2 2\n]\n+\u03b2 l\n\u2211 k=1 \u00b52k\ns.t. l\n\u2211 k=1 \u00b5k = 1,\u00b5k \u2265 0,k = 1, \u00b7 \u00b7 \u00b7 , l,\n(25) and it can also be solved as a QP problem."}, {"heading": "5 Summary", "text": "In this chapter, we investigate a fundamental problem in manifold regularized NMF methods \u2014 how to construct an optimal graph to present the manifold. We introduce three different methods to this problem, which consider multiple graph learning, feature selection, and multiple kernel learning methods, and integrate them to the problem of NMF. The common features of these methods are of two folds:\n1. The parameters of multiple graph learning, feature selection, and multiple kernel are all learned according to the criterion of NMF, and no supervision information is needed. This is critical for many unsupervised learning methods.\n2. The parameters of NMF, multiple graph learning, feature selection, and multiple kernel learning are coupled in a single objective function, but we employed alternate optimization methods to update them. Interestingly, we found that the parameters of multiple graph learning, feature selection, and multiple kernel learning can all be optimized by solving QP problems.\nAlthough the methods proposed in this chapter is to regularize the learning of NMF, it provides some insights to manifold learning. Traditional manifold learning methods all use a simple method to construct a nearest neighbor graph and use it to regularize different forms of outputs. However, the construction of the graph itself has not attracted much attention. In this chapter, we discussed how to construct an optimal graph from different affinity measures, noisy features, and different kernels. In the future, we will extend the work in this chapter to different learning problems, such as sparse coding [38, 1, 41, 43], learning to rank [45], classification [39], etc. Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],"}], "references": [{"title": "Supervised transfer sparse coding", "author": ["M. Al-Shedivat", "J.J.Y. Wang", "M. Alzahrani", "J.Z. Huang", "X. Gao"], "venue": "Twenty-Eighth AAAI Conference on Artificial Intelligence pp. 1665\u20131672", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Breaking the energy barrier in fault-tolerant caches for multicore systems", "author": ["P. Ampadu", "M. Zhang", "V. Stojanovic"], "venue": "Design, Automation & Test in Europe Conference & Exhibition (DATE), 2013, pp. 731\u2013736. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Corejava: learning functions expressed as object-oriented programs", "author": ["A. Brodsky", "J. Luo", "H. Nash"], "venue": "Machine Learning and Applications, 2008. ICMLA\u201908. Seventh International Conference on, pp. 368\u2013375. IEEE", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T.S. Huang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 33(8), 1548\u20131560", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "An accurate active set conjugate gradient algorithm with project search for bound constrained optimization", "author": ["W. Cheng", "Q. Liu", "D. Li"], "venue": "Optimization Letters 8(2), 763\u2013776", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Linearity indices and linearity improvement of 2-d tetralateral positionsensitive detector", "author": ["S. Cui", "Y.C. Soh"], "venue": "IEEE Trans. Electron Devices 57, 2310\u20132316", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Subspace feature analysis of local manifold learning for hyperspectral remote sensing images classification", "author": ["L. Ding", "P. Tang", "H. Li"], "venue": "Applied Mathematics and Information Sciences 8(4), 1987\u20131995", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A dynamic programming heuristic for the quadratic knapsack problem", "author": ["F. Fomeni", "A. Letchford"], "venue": "INFORMS Journal on Computing 26(1), 173\u2013182", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A ga-based feature selection and parameter optimization for linear support higher-order tensor machine", "author": ["T. Guo", "L. Han", "L. He", "X. Yang"], "venue": "Neurocomputing 144, 408\u2013416", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminant locality preserving projection chart for statistical monitoring of manufacturing processes", "author": ["Q. He", "S. Zhou"], "venue": "International Journal of Production Research 52(18), 5286\u20135300", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Information graphs for epidemiological applications of the kullback-leibler divergence", "author": ["G. Hughes"], "venue": "Methods of Information in Medicine 53(1), IV\u2013VI", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Simple sequential quadratically constrained quadratic programming feasible algorithm with active identification sets for constrained minimax problems", "author": ["J.B. Jian", "X.D. Mo", "L.J. Qiu", "S.M. Yang", "F.S. Wang"], "venue": "Journal of Optimization Theory and Applications 160(1), 158\u2013 188", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Blind separation of analytes in nuclear magnetic resonance spectroscopy: Improved model for nonnegative matrix factorization", "author": ["I. Kopriva", "I. Jeric"], "venue": "Chemometrics and Intelligent Laboratory Systems 137, 47\u201356", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Nonlinear projection trick in kernel methods: An alternative to the kernel trick", "author": ["N. Kwak"], "venue": "IEEE Transactions on Neural Networks and Learning Systems 24(12), 2113\u20132119", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Advances in neural information processing systems, pp. 556\u2013562", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "A decade of vector fitting development: Applications on signal/power integrity", "author": ["C.U. Lei", "Y. Wang", "Q. Chen", "N. Wong"], "venue": "IAENG Transactions on Engineering Technologies 1285(1), 435\u2013 449", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "On vector fitting methods in signal/power integrity applications", "author": ["C.U. Lei", "Y. Wang", "Q. Chen", "N. Wong"], "venue": "Proceedings of the International MultiConference of Engineers and Computer Scientists 2010, IMECS 2010, pp. 1407\u20131412. Newswood Limited.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "A bic based initial training set selection algorithm for active learning and its application in audio detection", "author": ["Y. Leng", "G.H. Qi", "X.Y. Xu"], "venue": "Radioengineering 22(2), 638\u2013649", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Reliable ultra-low voltage cache with variation-tolerance", "author": ["C. Li", "M. Zhang", "P. Ampadu"], "venue": "Circuits and Systems (MWSCAS), 2013 IEEE 56th International Midwest Symposium on, pp. 121\u2013124. IEEE", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Graph regularized non-negative matrix factorization by maximizing correntropy", "author": ["L. Li", "J. Yang", "K. Zhao", "Y. Xu", "H. Zhang", "Z. Fan"], "venue": "arXiv preprint arXiv:1405.2246", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Ultra wideband antennas\u2013past and present", "author": ["E.G. Lim", "Z. Wang", "C.U. Lei", "Y. Wang", "K. Man"], "venue": "International Journal of Computer Science 37(3), 304\u2013314", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "The orthogonal super greedy algorithm and applications in compressed sensing", "author": ["E. Liu", "V.N. Temlyakov"], "venue": "IEEE Transactions on Information Theory 58(4), 2040\u20132047", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Parameter estimation in geographically weighted regression", "author": ["J. Luo"], "venue": "Geoinformatics, 2009 17th International Conference on, pp. 1\u20136. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "A kernel trick for sequences applied to text-independent speaker verification systems", "author": ["J. Mari\u00e9thoz", "S. Bengio"], "venue": "Pattern Recognition 40(8), 2315\u20132324", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Linear-quadratic blind source separation using nmf to unmix urban hyperspectral images", "author": ["I. Meganem", "Y. Deville", "S. Hosseini", "P. Dliot", "X. Briottet"], "venue": "IEEE Transactions on Signal Processing 62(7), 1822\u20131833", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Finite time dual neural networks with a tunable activation function for solving quadratic programming problems and its application", "author": ["P. Miao", "Y. Shen", "X. Xia"], "venue": "Neurocomputing 143, 80\u201389", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Context awareness emergence for distributed binary pyroelectric sensors", "author": ["S. Qingquan", "H. Fei", "Q. Hao"], "venue": "Multisensor Fusion and Integration for Intelligent Systems (MFI), 2010 IEEE Conference on, pp. 162\u2013167. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": "Science 290(5500), 2323\u20132326", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2000}, {"title": "Layer depth denoising and completion for structured-light rgb-d cameras", "author": ["J. Shen", "S.C. Cheung"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pp. 1187\u20131194", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Virtual mirror rendering with stationary rgb-d cameras and stored 3d background", "author": ["J. Shen", "P. Su", "S. Cheung", "J. Zhao"], "venue": "IEEE transactions on image processing: a publication of the IEEE Signal Processing Society 22(9), 3433\u20133448", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Image-based indoor place-finder using image to plane matching", "author": ["J. Shen", "Tan", "W.t."], "venue": "Multimedia and Expo (ICME), 2013 IEEE International Conference on, pp. 1\u20136", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Second order optimality conditions and reformulations for nonconvex quadratically constrained quadratic programming problems", "author": ["Z. Shi", "Q. Jin"], "venue": "Journal of Industrial and Management Optimization 10(3), 871\u2013882", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Mobile target scenario recognition via low-cost pyroelectric sensing system: Toward a context-enhanced accurate identification", "author": ["Q. Sun", "F. Hu", "Q. Hao"], "venue": "IEEE transactions on systems, man, and cybernetics. Systems 44(3), 375\u2013384", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Non-informative hierarchical bayesian inference for non-negative matrix factorization", "author": ["Q. Sun", "F. Hu", "Y. Wu", "J. Lu", "X. Huang"], "venue": "Signal Processing", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised multi-level non-negative matrix factorization model: Binary data case", "author": ["Q. Sun", "P. Wu", "Y. Wu", "M. Guo", "J. Lu"], "venue": "Journal of Information Security 3, 245", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "A new hand gesture recognition algorithm based on joint color-depth superpixel earth mover\u2019s distance", "author": ["C. Wang", "S. Chan"], "venue": "Cognitive Information Processing (CIP), 2014 4th International Workshop on, pp. 1\u20136", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple kernel learning for adaptive graph regularized nonnegative matrix factorization", "author": ["J. Wang", "M.A. Jabbar"], "venue": "IASTED International Conference on Signal Processing, Pattern Recognition and Applications, SPPRA 2012, pp. 115\u2013122. ACTA Press", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-supervised sparse coding", "author": ["J.J.Y. Wang"], "venue": "Neural Networks (IJCNN), 2014 International Joint Conference on, pp. 1630\u20131637. IEEE", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Large margin image set representation and classification", "author": ["J.J.Y. Wang", "M. Alzahrani", "X. Gao"], "venue": "Neural Networks (IJCNN), 2014 International Joint Conference on, pp. 1797\u20131803. IEEE", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple graph regularized protein domain ranking", "author": ["J.J.Y. Wang", "H. Bensmail", "X. Gao"], "venue": "BMC bioinformatics 13(1), 307", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature selection and multi-kernel learning for sparse representation on a manifold", "author": ["J.J.Y. Wang", "H. Bensmail", "X. Gao"], "venue": "Neural Networks 51, 9\u201316", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiple graph regularized nonnegative matrix factorization", "author": ["J.J.Y. Wang", "H. Bensmail", "X. Gao"], "venue": "Pattern Recognition 46(10), 2840 \u2013 2847", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Discriminative sparse coding on multimanifolds", "author": ["J.J.Y. Wang", "H. Bensmail", "N. Yao", "X. Gao"], "venue": "Knowledge-Based Systems 54, 199\u2013206", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "Beyond cross-domain learning: Multiple domain nonnegative matrix factorization", "author": ["J.J.Y. Wang", "X. Gao"], "venue": "Engineering Applications of Artificial Intelligence 28(0), 181 \u2013 189", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse structure regularized ranking", "author": ["J.J.Y. Wang", "Y. Sun", "X. Gao"], "venue": "Multimedia Tools and Applications pp. 1\u201320", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Non-negative matrix factorization by maximizing correntropy for cancer clustering", "author": ["J.J.Y. Wang", "X. Wang", "X. Gao"], "venue": "BMC Bioinformatics 14(1), 107", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum mutual information regularized classification", "author": ["J.J.Y. Wang", "Y. Wang", "S. Zhao", "X. Gao"], "venue": "Engineering Applications of Artificial Intelligence 37, 1\u20138", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive graph regularized nonnegative matrix factorization via feature selection", "author": ["J.Y. Wang", "I. Almasri", "X. Gao"], "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on, pp. 963\u2013966. IEEE", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Hardware assisted resource sharing platform for personal cloud", "author": ["W. Wang", "Y. Zhang", "X. Liu", "M. Zhang", "Z. Wang"], "venue": "Computer Engineering and Technology (ICCET), 2010 2nd International Conference on, vol. 1, pp. V1\u2013164. IEEE", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2010}, {"title": "SciMATE: A Novel MapReduce-Like Framework for Multiple Scientific Data Formats", "author": ["Y. Wang", "W. Jiang", "G. Agrawal"], "venue": "Cluster, Cloud and Grid Computing (CCGrid), 2012 12th IEEE/ACM International Symposium on, pp. 443\u2013450. IEEE", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2012}, {"title": "Mfti: matrix-format tangential interpolation for modeling multi-port systems", "author": ["Y. Wang", "C.U. Lei", "G.K. Pang", "N. Wong"], "venue": "Proceedings of the 47th Design Automation Conference, pp. 683\u2013686. ACM", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}, {"title": "Supporting a Light-Weight Data Management Layer Over HDF5", "author": ["Y. Wang", "Y. Su", "G. Agrawal"], "venue": "Cluster, Cloud and Grid Computing (CCGrid), 2013 13th IEEE/ACM International Symposium on, pp. 335\u2013342. IEEE", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Peds: Passivity enforcement for descriptor systems via hamiltonian-symplectic matrix pencil perturbation", "author": ["Y. Wang", "Z. Zhang", "C.K. Koh", "G.K. Pang", "N. Wong"], "venue": "Proceedings of the International Conference on Computer-Aided Design, pp. 800\u2013807. IEEE Press", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2010}, {"title": "Passivity enforcement for descriptor systems via matrix pencil perturbation", "author": ["Y. Wang", "Z. Zhang", "C.K. Koh", "G. Shi", "G.K. Pang", "N. Wong"], "venue": "Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on 31(4), 532\u2013545", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2012}, {"title": "Cross-layer detection of malicious websites", "author": ["L. Xu", "Z. Zhan", "S. Xu", "K. Ye"], "venue": "Proceedings of the third ACM conference on Data and application security and privacy, pp. 141\u2013152. ACM", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2013}, {"title": "An evasion and counter-evasion study in malicious websites detection", "author": ["L. Xu", "Z. Zhan", "S. Xu", "K. Ye"], "venue": null, "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2014}, {"title": "Push- and pull-based epidemic spreading in networks: Thresholds and deeper insights", "author": ["S. Xu", "W. Lu", "L. Xu"], "venue": "ACM Trans. Auton. Adapt. Syst. 7(3), 32:1\u201332:26", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2012}, {"title": "Location aided energy balancing strategy in green cellular networks", "author": ["J. Yang", "B. Payne", "M. Hitz", "Z. Fei", "L. Li", "T. Wei"], "venue": "arXiv preprint arXiv:1406.5258", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting inherent information redundancy to manage transient errors in noc routing arbitration", "author": ["Q. Yu", "M. Zhang", "P. Ampadu"], "venue": "Proceedings of the Fifth ACM/IEEE International Symposium on Networks-on-Chip, pp. 105\u2013112. ACM", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2011}, {"title": "Addressing network-on-chip router transient errors with inherent information redundancy", "author": ["Q. Yu", "M. Zhang", "P. Ampadu"], "venue": "ACM Transactions on Embedded Computing Systems (TECS) 12(4), 105", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2013}, {"title": "Variation-tolerant cache by two-layer error control codes", "author": ["M. Zhang", "P. Ampadu"], "venue": "Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT), 2013 IEEE International Symposium on, pp. 161\u2013166. IEEE", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2013}, {"title": "Two-layer error control codes combining rectangular and hamming product codes for cache error", "author": ["M. Zhang", "P. Ampadu"], "venue": "Journal of Low Power Electronics and Applications 4(1), 44\u201362", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "Reliable ultra-low-voltage cache design for manycore systems", "author": ["M. Zhang", "V.M. Stojanovic", "P. Ampadu"], "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs 59(12), 858\u2013862", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2012}, {"title": "Fine-grained splitting methods to address permanent errors in network-on-chip links", "author": ["M. Zhang", "Q. Yu", "P. Ampadu"], "venue": "Circuits and Systems (ISCAS), 2012 IEEE International Symposium on, pp. 2717\u20132720. IEEE", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimal allocation of chp-based distributed generation on urban energy distribution networks", "author": ["X. Zhang", "G. Karady", "S. Ariaratnam"], "venue": "Sustainable Energy, IEEE Transactions on 5(1), 246\u2013 253", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2014}, {"title": "Network capacity assessment of combined heat and power-based distributed generation in urban energy infrastructures", "author": ["X. Zhang", "G. Karady", "K. Piratla", "S. Ariaratnam"], "venue": "Smart Grid, IEEE Transactions on 4(4), 2131\u20132138", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2013}, {"title": "Ranking on data manifolds", "author": ["D. Zhou", "J. Weston", "A. Gretton", "O. Bousquet", "B. Sch\u00f6lkopf"], "venue": "Advances in neural information processing systems 16, 169\u2013176", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2004}, {"title": "Adaptive learning of region-based plsa model for total scene annotation", "author": ["Y. Zhou", "L. Li", "H. Zhang"], "venue": "arXiv preprint arXiv:1311.5590", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "Nonnegative matrix factorization (NMF) [13, 20, 27, 35, 33, 34] has been a popular data representation method in field of machine learning.", "startOffset": 39, "endOffset": 63}, {"referenceID": 19, "context": "Nonnegative matrix factorization (NMF) [13, 20, 27, 35, 33, 34] has been a popular data representation method in field of machine learning.", "startOffset": 39, "endOffset": 63}, {"referenceID": 26, "context": "Nonnegative matrix factorization (NMF) [13, 20, 27, 35, 33, 34] has been a popular data representation method in field of machine learning.", "startOffset": 39, "endOffset": 63}, {"referenceID": 34, "context": "Nonnegative matrix factorization (NMF) [13, 20, 27, 35, 33, 34] has been a popular data representation method in field of machine learning.", "startOffset": 39, "endOffset": 63}, {"referenceID": 32, "context": "Nonnegative matrix factorization (NMF) [13, 20, 27, 35, 33, 34] has been a popular data representation method in field of machine learning.", "startOffset": 39, "endOffset": 63}, {"referenceID": 33, "context": "Nonnegative matrix factorization (NMF) [13, 20, 27, 35, 33, 34] has been a popular data representation method in field of machine learning.", "startOffset": 39, "endOffset": 63}, {"referenceID": 24, "context": "Due to its ability to explore the latent part-based structure of the data, it has been widely used to many real-world applications, such as computer vision [25], optical sensing [6] and bioinformatics [44, 46].", "startOffset": 156, "endOffset": 160}, {"referenceID": 5, "context": "Due to its ability to explore the latent part-based structure of the data, it has been widely used to many real-world applications, such as computer vision [25], optical sensing [6] and bioinformatics [44, 46].", "startOffset": 178, "endOffset": 181}, {"referenceID": 43, "context": "Due to its ability to explore the latent part-based structure of the data, it has been widely used to many real-world applications, such as computer vision [25], optical sensing [6] and bioinformatics [44, 46].", "startOffset": 201, "endOffset": 209}, {"referenceID": 45, "context": "Due to its ability to explore the latent part-based structure of the data, it has been widely used to many real-world applications, such as computer vision [25], optical sensing [6] and bioinformatics [44, 46].", "startOffset": 201, "endOffset": 209}, {"referenceID": 14, "context": "This problem can be solved by the Lagrange multiplier method [15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "Some other functions can also be used as reconstruction error measures, such as Kullback Leibler divergence [11], earth mover\u2019s distance [36], etc.", "startOffset": 108, "endOffset": 112}, {"referenceID": 35, "context": "Some other functions can also be used as reconstruction error measures, such as Kullback Leibler divergence [11], earth mover\u2019s distance [36], etc.", "startOffset": 137, "endOffset": 141}, {"referenceID": 6, "context": "Recent studies on manifold learning [7] show that lots of data distributions have low-dimensional manifold structures.", "startOffset": 36, "endOffset": 39}, {"referenceID": 27, "context": "The manifold structure has been explored by component analysis methods [28, 10], ranking score learning methods [67, 40], etc.", "startOffset": 71, "endOffset": 79}, {"referenceID": 9, "context": "The manifold structure has been explored by component analysis methods [28, 10], ranking score learning methods [67, 40], etc.", "startOffset": 71, "endOffset": 79}, {"referenceID": 66, "context": "The manifold structure has been explored by component analysis methods [28, 10], ranking score learning methods [67, 40], etc.", "startOffset": 112, "endOffset": 120}, {"referenceID": 39, "context": "The manifold structure has been explored by component analysis methods [28, 10], ranking score learning methods [67, 40], etc.", "startOffset": 112, "endOffset": 120}, {"referenceID": 3, "context": "[4] argued that when the data are represented using NMF, the manifold structure should also be respected and they used the graph affinity matrix to regularize the learning of the coefficient matrix W .", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "In this case, we should use kernel tricks [24] to map the data to a nonlinear space first and then construct graph.", "startOffset": 42, "endOffset": 46}, {"referenceID": 41, "context": "To overcome this problem, in our previous work [42], we proposed the multiple graph learning method to construct an optimal graph for regularization of NMF.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "\u2022 Updating H and W while fixing \u03bck,k = 1, \u00b7 \u00b7 \u00b7 , l: When \u03bck,k = 1, \u00b7 \u00b7 \u00b7 , l are fixed, the problem in (9) is reduced to the problem in (7), which can be solved using the Lagrange multiplier method introduced in [4].", "startOffset": 213, "endOffset": 216}, {"referenceID": 31, "context": "This problem is a linearly constrained quadratic programming (QP) problem [32, 8, 26] and can be solved easily using the active set algorithm [18, 12, 5].", "startOffset": 74, "endOffset": 85}, {"referenceID": 7, "context": "This problem is a linearly constrained quadratic programming (QP) problem [32, 8, 26] and can be solved easily using the active set algorithm [18, 12, 5].", "startOffset": 74, "endOffset": 85}, {"referenceID": 25, "context": "This problem is a linearly constrained quadratic programming (QP) problem [32, 8, 26] and can be solved easily using the active set algorithm [18, 12, 5].", "startOffset": 74, "endOffset": 85}, {"referenceID": 17, "context": "This problem is a linearly constrained quadratic programming (QP) problem [32, 8, 26] and can be solved easily using the active set algorithm [18, 12, 5].", "startOffset": 142, "endOffset": 153}, {"referenceID": 11, "context": "This problem is a linearly constrained quadratic programming (QP) problem [32, 8, 26] and can be solved easily using the active set algorithm [18, 12, 5].", "startOffset": 142, "endOffset": 153}, {"referenceID": 4, "context": "This problem is a linearly constrained quadratic programming (QP) problem [32, 8, 26] and can be solved easily using the active set algorithm [18, 12, 5].", "startOffset": 142, "endOffset": 153}, {"referenceID": 8, "context": "However, in many cases, there are many noisy and/or irrelevant features in the feature set and these features will affect the graph [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 47, "context": "To solve this problem, we proposed to integrate feature selection to NMF and use the selected features to construct the graph in our previous work [48].", "startOffset": 147, "endOffset": 151}, {"referenceID": 13, "context": "In this case, we can use a nonlinear function to map a data sample xi from a nonlinear space to a highdimensional linear space [14], xi \u2192 \u03a6(xi) \u2208 Rd \u2032 , where d\u2032 \u2265 d is the dimension of the high-dimensional linear space, then construct the graph and perform NMF [37].", "startOffset": 127, "endOffset": 131}, {"referenceID": 36, "context": "In this case, we can use a nonlinear function to map a data sample xi from a nonlinear space to a highdimensional linear space [14], xi \u2192 \u03a6(xi) \u2208 Rd \u2032 , where d\u2032 \u2265 d is the dimension of the high-dimensional linear space, then construct the graph and perform NMF [37].", "startOffset": 262, "endOffset": 266}, {"referenceID": 37, "context": "In the future, we will extend the work in this chapter to different learning problems, such as sparse coding [38, 1, 41, 43], learning to rank [45], classification [39], etc.", "startOffset": 109, "endOffset": 124}, {"referenceID": 0, "context": "In the future, we will extend the work in this chapter to different learning problems, such as sparse coding [38, 1, 41, 43], learning to rank [45], classification [39], etc.", "startOffset": 109, "endOffset": 124}, {"referenceID": 40, "context": "In the future, we will extend the work in this chapter to different learning problems, such as sparse coding [38, 1, 41, 43], learning to rank [45], classification [39], etc.", "startOffset": 109, "endOffset": 124}, {"referenceID": 42, "context": "In the future, we will extend the work in this chapter to different learning problems, such as sparse coding [38, 1, 41, 43], learning to rank [45], classification [39], etc.", "startOffset": 109, "endOffset": 124}, {"referenceID": 44, "context": "In the future, we will extend the work in this chapter to different learning problems, such as sparse coding [38, 1, 41, 43], learning to rank [45], classification [39], etc.", "startOffset": 143, "endOffset": 147}, {"referenceID": 38, "context": "In the future, we will extend the work in this chapter to different learning problems, such as sparse coding [38, 1, 41, 43], learning to rank [45], classification [39], etc.", "startOffset": 164, "endOffset": 168}, {"referenceID": 64, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 121, "endOffset": 157}, {"referenceID": 65, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 121, "endOffset": 157}, {"referenceID": 51, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 121, "endOffset": 157}, {"referenceID": 49, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 121, "endOffset": 157}, {"referenceID": 49, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 121, "endOffset": 157}, {"referenceID": 50, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 121, "endOffset": 157}, {"referenceID": 20, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 121, "endOffset": 157}, {"referenceID": 15, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 121, "endOffset": 157}, {"referenceID": 62, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 121, "endOffset": 157}, {"referenceID": 21, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 213, "endOffset": 217}, {"referenceID": 28, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 235, "endOffset": 255}, {"referenceID": 30, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 235, "endOffset": 255}, {"referenceID": 29, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 235, "endOffset": 255}, {"referenceID": 67, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 235, "endOffset": 255}, {"referenceID": 67, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 235, "endOffset": 255}, {"referenceID": 46, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 277, "endOffset": 288}, {"referenceID": 22, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 277, "endOffset": 288}, {"referenceID": 2, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 277, "endOffset": 288}, {"referenceID": 53, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 308, "endOffset": 320}, {"referenceID": 52, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 308, "endOffset": 320}, {"referenceID": 16, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 308, "endOffset": 320}, {"referenceID": 56, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 333, "endOffset": 349}, {"referenceID": 54, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 333, "endOffset": 349}, {"referenceID": 57, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 333, "endOffset": 349}, {"referenceID": 55, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 333, "endOffset": 349}, {"referenceID": 48, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 380, "endOffset": 411}, {"referenceID": 59, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 380, "endOffset": 411}, {"referenceID": 58, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 380, "endOffset": 411}, {"referenceID": 18, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 380, "endOffset": 411}, {"referenceID": 1, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 380, "endOffset": 411}, {"referenceID": 61, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 380, "endOffset": 411}, {"referenceID": 60, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 380, "endOffset": 411}, {"referenceID": 63, "context": "Moreover, we will also investigate adapting the proposed manifold learning methods to big data using distributed systems [65, 66, 52, 50, 50, 51, 21, 16, 63], and applying these methods to applications of sensing [22], computer vision [29, 31, 30, 68, 68], pattern recognition [47, 23, 3], signal processing [54, 53, 17], networking [57, 55, 58, 56], and hardware fault detection [49, 60, 59, 19, 2, 62, 61, 64],", "startOffset": 380, "endOffset": 411}], "year": 2014, "abstractText": "In this chapter we discuss how to learn an optimal manifold presentation to regularize nonegative matrix factorization (NMF) for data representation problems. NMF, which tries to represent a nonnegative data matrix as a product of two low rank nonnegative matrices, has been a popular method for data representation due to its ability to explore the latent part-based structure of data. Recent study shows that lots of data distributions have manifold structures, and we should respect the manifold structure when the data are represented. Recently, manifold regularized NMF used a nearest neighbor graph to regulate the learning of factorization parameter matrices and has shown its advantage over traditional NMF methods for data representation problems. However, how to construct an optimal graph to present the manifold properly remains a difficult problem due to the graph model selection, noisy features, and nonlinear distributed data. In this chapter, we introduce three effective methods to solve these problems of graph construction for manifold regularized NMF. Multiple graph learning is proposed to solve the problem of graph model selection, adaptive graph learning via feature selection is proposed to solve the problem of constructing a graph from noisy features, while multi-kernel learning-based graph construction is used to solve the problem of learning a graph from nonlinearly distributed data.", "creator": "LaTeX with hyperref package"}}}