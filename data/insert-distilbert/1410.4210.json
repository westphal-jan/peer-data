{"id": "1410.4210", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2014", "title": "Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets", "abstract": "sparse - group lasso ( sgl ) solve has been shown to be a powerful regression technique for simultaneously discovering group and within - group sparse patterns by using a combination of the $ \\ ell _ 1 $ and $ \\ ell _ 2 $ norms. however, in large - scale applications, the complexity of the regularizers entails great computational challenges. in this paper, we propose a novel two - layer feature reduction method ( tlfre ) for sgl via a decomposition of its dual feasible set. the two - layer reduction is able to quickly identify the inactive groups and the inactive support features, respectively, which are guaranteed to merely be absent from the sparse representation function and can be removed from the optimization. existing concurrent feature reduction methods are only applicable for sparse models with one sparsity - inducing regularizer. to increase our best knowledge, tlfre is the first one that we is capable of dealing with multiple sparsity - inducing regularizers. moreover, polynomial tlfre has a very low computational cost and can be integrated with any existing solvers. we also develop a supplementary screening approach method - - - called dpc ( decomposition of convex set ) - - - for the nonnegative lasso problem. experiments on both synthetic and real data sets show that tlfre and dpc improve the efficiency of sgl and nonnegative lasso by several orders of magnitude.", "histories": [["v1", "Wed, 15 Oct 2014 20:08:21 GMT  (391kb,D)", "http://arxiv.org/abs/1410.4210v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jie wang", "jieping ye"], "accepted": true, "id": "1410.4210"}, "pdf": {"name": "1410.4210.pdf", "metadata": {"source": "CRF", "title": "Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets", "authors": ["Jie Wang"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Sparse-Group Lasso (SGL) [7, 23] is a powerful regression technique in identifying important groups and features simultaneously. To yield sparsity at both group and individual feature levels, SGL combines the Lasso [25] and group Lasso [35] penalties. In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc. Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28]. However, when the feature dimension is extremely high, the complexity of the SGL regularizers imposes great computational challenges. Therefore, there is an increasingly urgent need for nontraditional techniques to address the challenges posed by the massive volume of the data sources.\nRecently, El Ghaoui et al. [6] proposed a promising feature reduction method, called SAFE screening, to screen out the so-called inactive features, which have zero coefficients in the solution, from the optimization. Thus, the size of the data matrix needed for the training phase can be significantly reduced, which may lead to substantial improvement in the efficiency of solving sparse models. Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc. It is worthwhile to mention that the discarded features by exact feature screening methods such as\nar X\niv :1\n41 0.\n42 10\nv1 [\ncs .L\nG ]\n1 5\nO ct\n2 01\nSAFE [6], DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution. However, heuristic feature screening methods like Strong Rule [26] may mistakenly discard features which have nonzero coefficients in the solution. More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30]. As a promising data reduction tool, exact feature/sample screening would be of great practical importance because they can effectively reduce the data size without sacrificing the optimality [16].\nHowever, all of the existing feature/sample screening methods are only applicable for the sparse models with one sparsity-inducing regularizer. In this paper, we propose an exact two-layer feature screening method, called TLFre, for the SGL problem. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to have zero coefficients in the solution. To the best of our knowledge, TLFre is the first screening method which is capable of dealing with multiple sparsity-inducing regularizers.\nWe note that most of the existing exact feature screening methods involve an estimation of the dual optimal solution. The difficulty in developing screening methods for sparse models with multiple sparsity-inducing regularizers like SGL is that the dual feasible set is the sum of simple convex sets. Thus, to determine the feasibility of a given point, we need to know if it is decomposable with respect to the summands, which is itself a nontrivial problem (see Section 2). One of our major contributions is that we derive an elegant decomposition method of any dual feasible solutions of SGL via the framework of Fenchel\u2019s duality (see Section 3). Based on the Fenchel\u2019s dual problem of SGL, we motivate TLFre by an in-depth exploration of its geometric properties and the optimality conditions in Section 4. We derive the set of the regularization parameter values corresponding to zero solutions. To develop TLFre, we need to estimate the upper bounds involving the dual optimal solution. To this end, we first give an accurate estimation of the dual optimal solution via the normal cones. Then, we formulate the estimation of the upper bounds via nonconvex optimization problems. We show that these nonconvex problems admit closed form solutions.\nThe rest of this paper is organized as follows. In Section 2, we briefly review some basics of the SGL problem. We then derive the Fenchel\u2019s dual of SGL with nice geometric properties under the elegant framework of Fenchel\u2019s Duality in Section 3. In Section 4, we develop the TLFre screening rule for SGL. To demonstrate the flexibility of the proposed framework, we extend TLFre to the nonnegative Lasso problem in Section 5. Experiments in Section 6 on both synthetic and real data sets demonstrate that the speedup gained by the proposed screening rules in solving SGL and nonnegative Lasso can be orders of magnitude.\nNotation: Let \u2016 \u00b7 \u20161, \u2016 \u00b7 \u2016 and \u2016 \u00b7 \u2016\u221e be the `1, `2 and `\u221e norms, respectively. Denote by Bn1 , Bn, and Bn\u221e the unit `1, `2, and `\u221e norm balls in Rn (we omit the superscript if it is clear from the context). For a set C, let int C be its interior. If C is closed and convex, we define the projection operator as PC(w) := argminu\u2208C\u2016w \u2212 u\u2016. We denote by IC(\u00b7) the indicator function of C, which is 0 on C and \u221e elsewhere. Let \u03930(Rn) be the class of proper closed convex functions on Rn. For f \u2208 \u03930(Rn), let \u2202f be its subdifferential. The domain of f is the set dom f := {w : f(w) < \u221e}. For w \u2208 Rn, let [w]i be its ith component. For \u03b3 \u2208 R, let sgn(\u03b3) = sign(\u03b3) if \u03b3 6= 0, and sgn(0) = 0. We define\nSGN(w) = { s \u2208 Rn : [s]i \u2208 { sign([w]i), if [w]i 6= 0; [\u22121, 1], if [w]i = 0. }\nWe denote by \u03b3+ = max(\u03b3, 0). Then, the shrinkage operator S\u03b3(w) : Rn \u2192 Rn with \u03b3 \u2265 0 is\n[S\u03b3(w)]i = (|[w]i| \u2212 \u03b3)+sgn([w]i), i = 1, . . . , n. (1)"}, {"heading": "2 Basics and Motivation", "text": "In this section, we briefly review some basics of SGL. Let y \u2208 RN be the response vector and X \u2208 RN\u00d7p be the matrix of features. With the group information available, the SGL problem [7] is\nmin \u03b2\u2208Rp\n1\n2 \u2225\u2225\u2225\u2225y\u2212\u2211Gg=1 Xg\u03b2g \u2225\u2225\u2225\u22252 + \u03bb1\u2211Gg=1\u221ang\u2016\u03b2g\u2016+ \u03bb2\u2016\u03b2\u20161, (2)\nwhere ng is the number of features in the g th group, Xg \u2208 RN\u00d7ng denotes the predictors in that group with the corresponding coefficient vector \u03b2g, and \u03bb1, \u03bb2 are positive regularization parameters. Without loss of generality, let \u03bb1 = \u03b1\u03bb and \u03bb2 = \u03bb with \u03b1 > 0. Then, problem (2) becomes:\nmin \u03b2\u2208Rp\n1\n2 \u2225\u2225\u2225\u2225y\u2212\u2211Gg=1 Xg\u03b2g \u2225\u2225\u2225\u22252 + \u03bb(\u03b1\u2211Gg=1\u221ang\u2016\u03b2g\u2016+ \u2016\u03b2\u20161 ) . (3)\nBy the Lagrangian multipliers method [4] (see the supplement), the dual problem of SGL is\nsup \u03b8\n{ 1 2\u2016y\u2016 2 \u2212 12 \u2225\u2225y \u03bb \u2212 \u03b8 \u2225\u22252 : XTg \u03b8 \u2208 D\u03b1g := \u03b1\u221angB + B\u221e, g = 1, . . . , G} . (4) It is well-known that the dual feasible set of Lasso is the intersection of closed half spaces (thus a polytope); for group Lasso, the dual feasible set is the intersection of ellipsoids. Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].\nWhen we incorporate multiple sparse-inducing regularizers to the sparse models, problem (4) indicates that the dual feasible set can be much more complicated. Although (4) provides a geometric description of the dual feasible set of SGL, it is not suitable for further analysis. Notice that, even the feasibility of a given point \u03b8 is not easy to determine, since it is nontrivial to tell if XTg \u03b8 can be decomposed into b1 + b2 with b1 \u2208 \u03b1 \u221a ngB and b2 \u2208 B\u221e. Therefore, to develop screening methods for SGL, it is desirable to gain deeper understanding of the sum of simple convex sets. In the next section, we analyze the dual feasible set of SGL in depth via the Fenchel\u2019s Duality Theorem. We show that for each XTg \u03b8 \u2208 D\u03b1g , Fenchel\u2019s duality naturally leads to an explicit decomposition XTg \u03b8 = b1 + b2, with one belonging to \u03b1 \u221a ngB and the other one belonging to B\u221e. This lays the foundation of the proposed screening method for SGL."}, {"heading": "3 The Fenchel\u2019s Dual Problem of SGL", "text": "In Section 3.1, we derive the Fenchel\u2019s dual of SGL via Fenchel\u2019s Duality Theorem. We then motivate TLFre and sketch our approach in Section 3.2. In Section 3.3, we discuss the geometric properties of the Fenchel\u2019s dual of SGL and derive the set of (\u03bb, \u03b1) leading to zero solutions."}, {"heading": "3.1 The Fenchel\u2019s Dual of SGL via Fenchel\u2019s Duality Theorem", "text": "To derive the Fenchel\u2019s dual problem of SGL, we need the Fenchel\u2019s Duality Theorem as stated in Theorem 1. The conjugate of f \u2208 \u03930(Rn) is the function f\u2217 \u2208 \u03930(Rn) defined by\nf\u2217(z) = supw \u3008w, z\u3009 \u2212 f(w). (5)\nTheorem 1. [Fenchel\u2019s Duality Theorem] Let f \u2208 \u03930(RN ), \u2126 \u2208 \u03930(Rp), and T (\u03b2) = y \u2212 X\u03b2 be an affine mapping from Rp to RN . Let p\u2217, d\u2217 \u2208 [\u2212\u221e,\u221e] be primal and dual values defined, respectively, by the Fenchel problems:\np\u2217 = inf\u03b2\u2208Rp f(y \u2212X\u03b2) + \u03bb\u2126(\u03b2); d\u2217 = sup\u03b8\u2208RN \u2212f\u2217(\u03bb\u03b8)\u2212 \u03bb\u2126\u2217(XT \u03b8) + \u03bb\u3008y, \u03b8\u3009.\nOne has p\u2217 \u2265 d\u2217. If, furthermore, f and \u2126 satisfy the condition 0 \u2208 int (dom f \u2212 y + Xdom \u2126), then the equality holds, i.e., p\u2217 = d\u2217, and the supreme is attained in the dual problem if finite.\nWe omit the proof of Theorem 1 since it is a slight modification of Theorem 3.3.5 in [3]. Let f(w) = 12\u2016w\u2016 2, and \u03bb\u2126(\u03b2) be the second term in (3). Then, SGL can be written as\nmin\u03b2 f(y \u2212X\u03b2) + \u03bb\u2126(\u03b2). (6)\nTo derive the Fenchel\u2019s dual problem of SGL, Theorem 1 implies that we need to find f\u2217 and \u2126\u2217. It is well-known that f\u2217(z) = 12\u2016z\u2016\n2. Therefore, we only need to find \u2126\u2217, where the concept infimal convolution is needed:\nDefinition 2. [2] Let h, g \u2208 \u03930(Rn). The infimal convolution of h and g is defined by\n(h g)(\u03be) = inf\u03b7 h(\u03b7) + g(\u03be \u2212 \u03b7), (7)\nand it is exact at a point \u03be if there exists a \u03b7\u2217(\u03be) such that\n(h g)(\u03be) = h(\u03b7\u2217(\u03be)) + g(\u03be \u2212 \u03b7\u2217(\u03be)). (8)\nh g is exact if it is exact at every point of its domain, in which case it is denoted by h g.\nWith the infimal convolution, we derive the conjugate function of \u2126 in Lemma 3. Lemma 3. Let \u2126\u03b11 (\u03b2) = \u03b1 \u2211G g=1 \u221a ng\u2016\u03b2g\u2016, \u21262(\u03b2) = \u2016\u03b2\u20161 and \u2126(\u03b2) = \u2126\u03b11 (\u03b2) + \u21262(\u03b2). Moreover, let C\u03b1g = \u03b1 \u221a ngB \u2282 Rng , g = 1, . . . , G. Then, the following hold:\n(i) (\u2126\u03b11 ) \u2217(\u03be) = \u2211G g=1 IC\u03b1g (\u03beg) , (\u21262) \u2217(\u03be) = \u2211G g=1 IB\u221e (\u03beg),\n(ii) \u2126\u2217(\u03be) = ((\u2126\u03b11 ) \u2217 (\u21262)\u2217) (\u03be) = \u2211G g=1 IB ( \u03beg\u2212PB\u221e (\u03beg) \u03b1 \u221a ng ) ,\nwhere \u03beg \u2208 Rng is the sub-vector of \u03be corresponding to the gth group.\nTo prove Lemma 3, we first cite the following technical result.\nTheorem 4. [10] Let f1, \u00b7 \u00b7 \u00b7 , fk \u2208 \u03930(Rn). Suppose there is a point in \u2229ki=1dom fi at which f1, \u00b7 \u00b7 \u00b7 , fk\u22121 is continuous. Then, for all p \u2208 Rn:\n(f1 + \u00b7 \u00b7 \u00b7+ fk)\u2217(p) = min p1+\u00b7\u00b7\u00b7+pk=p [f\u22171 (p1) + \u00b7 \u00b7 \u00b7+ f\u2217k (pk)].\nWe now give the proof of Lemma 3.\nProof. The first part can be derived directly by the definition as follows:\n(\u2126\u03b11 ) \u2217(\u03be) = sup \u03b2 \u3008\u03b2, \u03be\u3009 \u2212 \u21261(\u03b2) = G\u2211 g=1 \u03b1 \u221a ng ( sup \u03b2g \u2329 \u03b2g, \u03beg \u03b1 \u221a ng \u232a \u2212 \u2016\u03b2g\u2016 )\n= G\u2211 g=1 \u03b1 \u221a ngIB ( \u03beg \u03b1 \u221a ng ) = G\u2211 g=1 IB ( \u03beg \u03b1 \u221a ng ) = G\u2211 g=1 IC\u03b1g (\u03beg).\n(\u21262) \u2217(\u03be) = sup\n\u03b2 \u3008\u03b2, \u03be\u3009 \u2212 \u21262(\u03b2) = IB\u221e (\u03be) = G\u2211 g=1 IB\u221e (\u03beg) .\nTo show the second part, Theorem 4 indicates that we only need to show (\u2126\u03b11 ) \u2217 (\u21262)\u2217(\u03be) is\nexact (note that \u2126\u03b11 and \u21262 are continuous everywhere). Let us now compute (\u2126 \u03b1 1 ) \u2217 (\u21262)\u2217.\n((\u21261) \u2217 (\u21262) \u2217) (\u03be) = inf \u03b7 (\u21261) \u2217(\u03be \u2212 \u03b7) + (\u21262)\u2217(\u03b7) (9)\n= G\u2211 g=1 inf \u03b7g IB ( \u03beg \u2212 \u03b7g \u03b1 \u221a ng ) + IB\u221e (\u03b7g)\n= G\u2211 g=1 inf \u2016\u03b7g\u2016\u221e\u22641 IB ( \u03beg \u2212 \u03b7g \u03b1 \u221a ng ) To solve the optimization problem in (9), i.e.,\n\u00b5\u2217g = inf\u03b7g\n{ IB ( \u03beg \u2212 \u03b7g \u03b1 \u221a ng ) : \u2016\u03b7g\u2016\u221e \u2264 1 } , (10)\nwe can consider the following problem\n\u03bd\u2217g = inf\u03b7g\n{ 1\n\u03b1 \u221a ng \u2016\u03beg \u2212 \u03b7g\u2016 : \u2016\u03b7g\u2016\u221e \u2264 1\n} . (11)\nWe can see that the optimal solution of problem (11) must also be an optimal solution of problem (10). Let \u03b7\u2217g(\u03beg) be the optimal solution of (11). We can see that \u03b7 \u2217 g(\u03beg) is indeed the projection of \u03beg on B\u221e, which admits a closed form solution:\n[\u03b7\u2217g(\u03beg)]i = [PB\u221e(\u03beg)]i =  1, if [\u03beg]i > 1,\n[\u03beg]i, if |[\u03beg]i| \u2264 1, \u22121, if [\u03beg]i < \u22121.\nThus, problem (10) can be solved as\n\u00b5\u2217g = IB\n( \u03beg \u2212PB\u221e(\u03beg)\n\u03b1 \u221a ng\n) .\nHence, the infimal convolution in Eq. (9) is exact and Theorem 4 leads to\n\u2126\u2217(\u03be) = ((\u2126\u03b11 ) \u2217 (\u21262) \u2217) (\u03be) = G\u2211 g=1 IB ( \u03beg \u2212PB\u221e(\u03beg) \u03b1 \u221a ng ) , (12)\nwhich completes the proof.\nNote that PB\u221e(\u03beg) admits a closed form solution, i.e., [PB\u221e(\u03beg)]i = sgn ([\u03beg]i) min (|[\u03beg]i| , 1). Combining Theorem 1 and Lemma 3, the Fenchel\u2019s dual of SGL can be derived as follows.\nTheorem 5. For the SGL problem in (3), the following hold:\n(i) The Fenchel\u2019s dual of SGL is given by:\ninf \u03b8\n{ 1 2\u2016 y \u03bb \u2212 \u03b8\u2016 2 \u2212 12\u2016y\u2016 2 : \u2225\u2225XTg \u03b8 \u2212PB\u221e(XTg \u03b8)\u2225\u2225 \u2264 \u03b1\u221ang, g = 1, . . . , G} . (13)\n(ii) Let \u03b2\u2217(\u03bb, \u03b1) and \u03b8\u2217(\u03bb, \u03b1) be the optimal solutions of problems (3) and (13), respectively. Then,\n\u03bb\u03b8\u2217(\u03bb, \u03b1) =y \u2212X\u03b2\u2217(\u03bb, \u03b1), (14) XTg \u03b8 \u2217(\u03bb, \u03b1) \u2208\u03b1\u221ang\u2202\u2016\u03b2\u2217g (\u03bb, \u03b1)\u2016+ \u2202\u2016\u03b2\u2217g (\u03bb, \u03b1)\u20161, g = 1, . . . , G. (15)\nTo show Theorem 5, we need the Fenchel-Young inequality as follows:\nLemma 6. [Fenchel-Young inequality] [3] Any point z \u2208 Rn and w in the domain of a function h : Rn \u2192 (\u2212\u221e,\u221e] satisfy the inequality\nh(w) + h\u2217(z) \u2265 \u3008w, z\u3009.\nEquality holds if and only if z \u2208 \u2202h(w).\nWe now give the proof of Theorem 5.\nProof. We first show the first part. Combining Theorem 1 and Lemma 3, the Fenchel\u2019s dual of SGL can be written as:\nsup \u03b8 \u2212\u03bb\n2\n2 \u2016\u03b8\u20162 \u2212 \u2211G g=1 \u03bbIB\n( XTg \u03b8 \u2212PB\u221e(XTg \u03b8)\n\u03b1 \u221a ng\n) + \u03bb\u3008y, \u03b8\u3009,\nwhich is equivalent to problem (13). To show the second half, we have the following inequalities by Fenchel-Young inequality:\nf(y \u2212X\u03b2) + f\u2217(\u03bb\u03b8) \u2265 \u3008y \u2212X\u03b2, \u03bb\u03b8\u3009, (16) \u03bb\u2126(\u03b2) + \u03bb\u2126\u2217(XT \u03b8) \u2265 \u03bb\u3008\u03b2,XT \u03b8\u3009. (17)\nWe sum the inequalities in (16) and (17) together and get\nf(y \u2212X\u03b2) + \u03bb\u2126(\u03b2) \u2265 \u2212f\u2217(\u03bb\u03b8)\u2212 \u03bb\u2126\u2217(XT \u03b8) + \u03bb\u3008y, \u03b8\u3009. (18)\nClearly, the left and right hand sides of inequality (18) are the objective functions of the pair of Fenchel\u2019s problems. Because dom f = RN and dom \u2126 = Rp, we have\n0 \u2208 int (dom f \u2212 y + Xdom \u2126).\nThus, the equality in (18) holds at \u03b2\u2217(\u03bb, \u03b1) and \u03b8\u2217(\u03bb, \u03b1), i.e.,\nf(y \u2212X\u03b2\u2217(\u03bb, \u03b1)) + \u03bb\u2126(\u03b2\u2217(\u03bb, \u03b1)) = \u2212f\u2217(\u03bb\u03b8\u2217(\u03bb, \u03b1))\u2212 \u03bb\u2126\u2217(XT \u03b8\u2217(\u03bb, \u03b1)) + \u03bb\u3008y, \u03b8\u2217(\u03bb, \u03b1)\u3009.\nTherefore, the equality holds in both (16) and (17) at \u03b2\u2217(\u03bb, \u03b1) and \u03b8\u2217(\u03bb, \u03b1). By applying Lemma 6 again, we have\n\u03bb\u03b8\u2217(\u03bb, \u03b1) \u2208 \u2202f(y \u2212X\u03b2\u2217(\u03bb, \u03b1)) = y \u2212X\u03b2\u2217(\u03bb, \u03b1), XT \u03b8\u2217(\u03bb, \u03b1) \u2208 \u2202\u2126(\u03b2\u2217(\u03bb, \u03b1)) = \u2202\u2126\u03b11 (\u03b2\u2217(\u03bb, \u03b1)) + \u2202\u21262(\u03b2\u2217(\u03bb, \u03b1)),\nwhich completes the proof.\nEq. (14) and Eq. (15) are the so-called KKT conditions [4] and can also be obtained by the Lagrangian multiplier method (see A.1 in the supplement).\nRemark 1. We note that the shrinkage operator can also be expressed by\nS\u03b3(w) = w \u2212P\u03b3B\u221e(w), \u03b3 \u2265 0. (19)\nTherefore, problem (13) can be written more compactly as\ninf \u03b8\n{ 1 2\u2016 y \u03bb \u2212 \u03b8\u2016 2 \u2212 12\u2016y\u2016 2 : \u2225\u2225S1(XTg \u03b8)\u2225\u2225 \u2264 \u03b1\u221ang, g = 1, . . . , G} . (20)\nThe equivalence between the dual formulations For the SGL problem, its Lagrangian dual in (4) and Fenchel\u2019s dual in (13) are indeed equivalent to each other. We bridge them together by the following lemma.\nLemma 7. [2] Let C1 and C2 be nonempty subsets of Rn. Then IC1 IC2 = IC1+C2.\nIn view of Lemmas 3 and 7, and recall that D\u03b1g = C\u03b1g + B\u221e, we have\n\u2126\u2217(\u03be) = ((\u2126\u03b11 ) \u2217 (\u21262)\n\u2217) (\u03be) = \u2211G\ng=1\n( IC\u03b1g IB\u221e ) (\u03beg) = \u2211G g=1 ID\u03b1g (\u03beg). (21)\nCombining Eq. (21) and Theorem 1, we obtain the dual formulation of SGL in (4). Therefore, the dual formulations of SGL in (4) and (13) are the same.\nRemark 2. An appealing advantage of the Fenchel\u2019s dual in (13) is that we have a natural decomposition of all points \u03beg \u2208 D\u03b1g : \u03beg = PB\u221e(\u03beg) + S1(\u03beg)) with PB\u221e(\u03beg) \u2208 B\u221e and S1(\u03beg) \u2208 C\u03b1g . As a result, this leads to a convenient way to determine the feasibility of any dual variable \u03b8 by checking if S1(XTg \u03b8) \u2208 C\u03b1g , g = 1, . . . , G."}, {"heading": "3.2 Motivation of the Two-Layer Screening Rules", "text": "We motive the two-layer screening rules via the KKT condition in Eq. (15). As implied by the name, there are two layers in our method. The first layer aims to identify the inactive groups, and the second layer is designed to detect the inactive features for the remaining groups.\nby Eq. (15), we have the following cases by noting \u2202\u2016w\u20161 = SGN(w) and\n\u2202\u2016w\u2016 =\n{{ w \u2016w\u2016 } , if w 6= 0,\n{u : \u2016u\u2016 \u2264 1}, if w = 0.\nCase 1. If \u03b2\u2217g (\u03bb, \u03b1) 6= 0, we have\n[XTg \u03b8 \u2217(\u03bb, \u03b1)]i \u2208\n{ \u03b1 \u221a ng\n[\u03b2\u2217g (\u03bb,\u03b1)]i \u2016\u03b2\u2217g (\u03bb,\u03b1)\u2016 + sign([\u03b2\u2217g (\u03bb, \u03b1)]i), if [\u03b2 \u2217 g (\u03bb, \u03b1)]i 6= 0,\n[\u22121, 1], if [\u03b2\u2217g (\u03bb, \u03b1)]i = 0. (22)\nIn view of Eq. (22), we can see that\n(a): S1(XTg \u03b8\u2217(\u03bb, \u03b1)) = \u03b1 \u221a ng \u03b2\u2217g (\u03bb1,\u03bb2)\n\u2016\u03b2\u2217g (\u03bb1,\u03bb2)\u2016 and \u2016S1(XTg \u03b8\u2217(\u03bb, \u03b1))\u2016 = \u03b1\n\u221a ng, (23)\n(b): If \u2223\u2223[XTg \u03b8\u2217(\u03bb, \u03b1]i\u2223\u2223 \u2264 1 then [\u03b2\u2217g (\u03bb, \u03b1)]i = 0. (24)\nCase 2. If \u03b2\u2217g (\u03bb, \u03b1) = 0, we have\n[XTg \u03b8 \u2217(\u03bb, \u03b1)]i \u2208 \u03b1 \u221a ng[ug]i + [\u22121, 1], \u2016ug\u2016 \u2264 1. (25)\nThe first layer (group-level) of TLFre From (23) in Case 1, we have\u2225\u2225S1(XTg \u03b8\u2217(\u03bb, \u03b1))\u2225\u2225 < \u03b1\u221ang \u21d2 \u03b2\u2217g (\u03bb, \u03b1) = 0. (R1) Clearly, (R1) can be used to identify the inactive groups and thus a group-level screening rule.\nThe second layer (feature-level) of TLFre Let xgi be the i th column of Xg. We have\n[XTg \u03b8 \u2217(\u03bb, \u03b1)]i = x T gi\u03b8 \u2217(\u03bb, \u03b1). In view of (24) and (25), we can see that\u2223\u2223xTgi\u03b8\u2217(\u03bb, \u03b1)\u2223\u2223 \u2264 1\u21d2 [\u03b2\u2217g (\u03bb, \u03b1)]i = 0. (R2)\nDifferent from (R1), (R2) detects the inactive features and thus it is a feature-level screening rule. However, we cannot directly apply (R1) and (R2) to identify the inactive groups/features because both need to know \u03b8\u2217(\u03bb, \u03b1). Inspired by the SAFE rules [6], we can first estimate a region \u0398 containing \u03b8\u2217(\u03bb, \u03b1). Let XTg \u0398 = {XTg \u03b8 : \u03b8 \u2208 \u0398}. Then, (R1) and (R2) can be relaxed as follows:\nsup\u03beg { \u2016S1(\u03beg)\u2016 : \u03beg \u2208 \u039eg \u2287 XTg \u0398 } < \u03b1 \u221a ng \u21d2 \u03b2\u2217g (\u03bb, \u03b1) = 0, (R1\u2217)\nsup\u03b8 {\u2223\u2223xTgi\u03b8\u2223\u2223 : \u03b8 \u2208 \u0398} \u2264 1\u21d2 [\u03b2\u2217g (\u03bb, \u03b1)]i = 0. (R2\u2217)\nInspired by (R1\u2217) and (R2\u2217), we develop TLFre via the following three steps:\nStep 1. Given \u03bb and \u03b1, we estimate a region \u0398 that contains \u03b8\u2217(\u03bb, \u03b1).\nStep 2. We solve for the supreme values in (R1\u2217) and (R2\u2217).\nStep 3. By plugging in the supreme values from Step 2, (R1\u2217) and (R2\u2217) result in the desired two-layer screening rules for SGL."}, {"heading": "3.3 The Set of Parameter Values Leading to Zero Solutions", "text": "In this section, we explore the geometric properties of the Fenchel\u2019s dual of SGL in depth\u2014based on which we can derive the set of parameter values such that the primal optimal solutions are 0. We consider the SGL problem in (3) and (2) in Section 3.3.1 and 3.3.2, respectively."}, {"heading": "3.3.1 The Set of Parameter Values Leading to Zero Solutions of Problem (3)", "text": "Consider the SGL problem in (3). For notational convenience, let\nF\u03b1g = {\u03b8 : \u2016S1(XTg \u03b8)\u2016 \u2264 \u03b1 \u221a ng}, g = 1, . . . , G.\nWe denote the feasible set of the Fenchel\u2019s dual of SGL by\nF\u03b1 = \u2229g=1,...,GF\u03b1g .\nIn view of problem (13) [or (20)], we can see that \u03b8\u2217(\u03bb, \u03b1) is the projection of y/\u03bb on F\u03b1, i.e.,\n\u03b8\u2217(\u03bb, \u03b1) = PF\u03b1(y/\u03bb). (26)\nThus, if y/\u03bb \u2208 F\u03b1, we have \u03b8\u2217(\u03bb, \u03b1) = y/\u03bb. Moreover, by (R1), we can see that \u03b2\u2217(\u03bb, \u03b1) = 0 if y/\u03bb is an interior point of F\u03b1. Indeed, we have the following stronger result.\nTheorem 8. For the SGL problem, let \u03bb\u03b1max = maxg {\u03c1g : \u2225\u2225S1(XTg y/\u03c1g)\u2225\u2225 = \u03b1\u221ang}. Then, the following statements are equivalent:\n(i) y \u03bb \u2208 F\u03b1, (ii) \u03b8\u2217(\u03bb, \u03b1) = y \u03bb , (iii) \u03b2\u2217(\u03bb, \u03b1) = 0, (iv) \u03bb \u2265 \u03bb\u03b1max. Proof. The equivalence between (i) and (ii) can be see from the fact that \u03b8\u2217(\u03bb, \u03b1) = PF\u03b1(y/\u03bb). Next, we show (ii)\u21d4(iii). Let us first show (ii)\u21d2(iii). We assume that \u03b8\u2217(\u03bb, \u03b1) = y/\u03bb. By the KKT condition in (14), we have X\u03b2\u2217(\u03bb, \u03b1) = 0. We claim that \u03b2\u2217(\u03bb, \u03b1) = 0. To see this, let \u03b2\u2032 6= 0 with X\u03b2\u2032 = 0 be another optimal solution of SGL. We denote by h the objective function of SGL in (3). Then, we have\nh(0) = 1 2 \u2016y\u20162 < h(\u03b2\u2032) = 1 2 \u2016y\u20162 + \u03bb1 \u2211 g \u221a ng\u2016\u03b2\u2032g\u2016+ \u03bb2\u2016\u03b2\u2032\u20161,\nwhich contradicts with the assumption \u03b2\u2032 6= 0 is also an optimal solution. This contradiction indicates that \u03b2\u2217(\u03bb, \u03b1) must be 0. The converse direction, i.e., (ii)\u21d0(iii), can be derived directly from the KKT condition in Eq. (14).\nFinally, we show the equivalence (i)\u21d4(iv). Indeed, in view of the dual problem in (20), we can see that y/\u03bb \u2208 F\u03b1 if and only if\n\u2016S1(XTg y/\u03bb)\u2016 \u2264 \u03b1 \u221a ng, g = 1, . . . , G. (27)\nWe note that \u2016S1(XTg y/\u03bb)\u2016 is monotonically decreasing with respect to \u03bb. Thus, the inequality in (27) is equivalent to (iv), which completes the proof.\nWe note that \u03c1g in the definition of \u03bb \u03b1 max admits a closed form solution. For notational convenience, let |w| be the vector by taking absolute value of w component-wisely and [w](k) be the vector consisting of the first k components of w.\nLemma 9. We sort 0 6= |XTg y| \u2208 Rng in descending order and denote it by z.\n(i) If there exists [z]k such that \u2016S1(XTg y/[z]k)\u2016 = \u03b1 \u221a ng, then \u03c1g = [z]k.\n(ii) Otherwise, let \u03c4i = \u2016S1(XTg y/[z]i)\u2016, i = 1, . . . , ng, and \u03c4ng+1 =\u221e. There exists a k such that \u03b1 \u221a ng \u2208 (\u03c4k, \u03c4k+1), and \u03c1g \u2208 ([z]k+1, [z]k) is the root of\n(k \u2212 \u03b12ng)\u03c12 \u2212 2\u03c1\u2016[z](k)\u20161 + \u2016[z](k)\u20162 = 0.\nWe omit the proof of Lemma 9 because it is a direct consequence by noting that\n\u2016S1(XTg y/\u03bb)\u20162 = \u03b12ng\nis piecewise quadratic."}, {"heading": "3.3.2 The Set of Parameter Values Leading to Zero Solutions of Problem (2)", "text": "Theorem 8 implies that the optimal solution \u03b2\u2217(\u03bb, \u03b1) is 0 as long as y/\u03bb \u2208 F\u03b1. This geometric property also leads to an explicit characterization of the set of (\u03bb1, \u03bb2) such that the corresponding solution of problem (2) is 0. We denote by \u03b2\u0304\u2217(\u03bb1, \u03bb2) the optimal solution of problem (2).\nCorollary 10. For the SGL problem in (2), let \u03bbmax1 (\u03bb2) = maxg 1\u221a ng \u2016S\u03bb2(XTg y)\u2016. Then,\n(i) \u03b2\u0304\u2217(\u03bb1, \u03bb2) = 0\u21d4 \u03bb1 \u2265 \u03bbmax1 (\u03bb2).\n(ii) If \u03bb1 \u2265 \u03bbmax1 := maxg 1\u221ang \u2016X T g y\u2016 or \u03bb2 \u2265 \u03bbmax2 := \u2016XTy\u2016\u221e, then \u03b2\u0304\u2217(\u03bb1, \u03bb2) = 0.\nBefore we prove Corollary 10, we first derive the Fenchel\u2019s dual of (2). By letting f(w) = 12\u2016w\u2016 2 and \u2126(\u03b2) = \u03bb1 \u2211G g=1 \u221a ng\u2016\u03b2g\u2016+ \u03bb2\u2016\u03b2\u20161, the SGL problem in (2) can be written as:\nmin \u03b2\nf(y \u2212X\u03b2) + \u2126(\u03b2).\nThen, by Fenchel\u2019s Duality Theorem, the Fenchel\u2019s dual problem of (2) is\ninf \u03b8\n{ 1\n2 \u2016y \u2212 \u03b8\u20162 \u2212 1 2 \u2016y\u20162 : \u2225\u2225S\u03bb2(XTg \u03b8)\u2225\u2225 \u2264 \u03bb1\u221ang, g = 1, . . . , G} . (28) Let \u03b2\u0304\u2217(\u03bb1, \u03bb2) and \u03b8\u0304\n\u2217(\u03bb1, \u03bb2) be the optimal solutions of problem (2) and (28). The optimality conditions can be written as\n\u03b8\u0304\u2217(\u03bb1, \u03bb2) =y \u2212X\u03b2\u0304\u2217(\u03bb1, \u03bb2), (29) XTg \u03b8\u0304 \u2217(\u03bb1, \u03bb2) \u2208 \u03bb1 \u221a ng\u2202\u2016\u03b2\u0304\u2217g (\u03bb1, \u03bb2)\u2016+ \u03bb2\u2202\u2016\u03b2\u0304\u2217g (\u03bb1, \u03bb2)\u20161, g = 1, . . . , G. (30)\nWe denote by F(\u03bb1, \u03bb2) the feasible set of problem (28). It is easy to see that\n\u03b8\u0304\u2217(\u03bb1, \u03bb2) = PF(\u03bb1,\u03bb2)(y).\nWe now present the proof of Corollary 10.\nProof. For notational convenience, let\n(i). y \u2208 F(\u03bb1, \u03bb2),\n(ii). \u03b8\u0304\u2217(\u03bb1, \u03bb2) = y,\n(iii). \u03b2\u0304\u2217(\u03bb1, \u03bb2) = 0,\n(iv). \u03bb1 \u2265 \u03bbmax1 (\u03bb2) = maxg 1\u221ang \u2016S\u03bb2(X T g y)\u2016.\nThe first half of the statement is (iii)\u21d4(iv). Indeed, by a similar argument as in the proof of Theorem 8, we can see that the above statements are all equivalent to each other.\nWe now show the second half. We first show that\n\u03bb1 \u2265 \u03bbmax1 \u21d2 \u03b2\u0304\u2217(\u03bb1, \u03bb2) = 0. (31)\nBy the first half, we only need to show\n\u03bb1 \u2265 \u03bbmax1 \u21d2 y \u2208 F(\u03bb1, \u03bb2).\nIndeed, the definition of \u03bb1 implies that\n\u2016XTg y\u2016 \u2264 \u03bb1 \u221a ng, g = 1, . . . , G.\nWe note that for any \u03bb2 \u2265 0, we have\n\u2016S\u03bb2(XTg y)\u2016 \u2264 \u2016XTg y\u2016.\nTherefore, we can see that\n\u2016S\u03bb2(XTg y)\u2016 \u2264 \u2016XTg y\u2016 \u2264 \u03bb1 \u221a ng, g = 1, . . . , G\u21d2 y \u2208 F(\u03bb1, \u03bb2).\nThe proof of (31) is complete. Similarly, to show that \u03bb2 \u2265 \u03bbmax2 \u21d2 \u03b2\u0304\u2217(\u03bb1, \u03bb2), we only need to show\n\u03bb2 \u2265 \u03bbmax2 \u21d2 y \u2208 F(\u03bb1, \u03bb2).\nBy the definition of \u03bb2, we can see that\n\u2016XTg y\u2016\u221e \u2264 \u03bb2, g = 1, . . . , G\u21d2 \u2016S\u03bb2(XTg y)\u2016 = 0 \u2264 \u03bb1 \u221a ng, g = 1, . . . , G.\nThus, we have y \u2208 F(\u03bb1, \u03bb2), which completes the proof."}, {"heading": "4 The Two-Layer Screening Rules for SGL", "text": "We follow the three steps in Section 3.2 to develop TLFre. In Section 4.1, we give an accurate estimation of \u03b8\u2217(\u03bb, \u03b1) via normal cones [20]. Then, we compute the supreme values in (R1\u2217) and (R2\u2217) by solving nonconvex problems in Section 4.2. We present the TLFre rules in Section 4.3."}, {"heading": "4.1 Estimation of the Dual Optimal Solution", "text": "Because of the geometric property of the dual problem in (13), i.e., \u03b8\u2217(\u03bb, \u03b1) = PF\u03b1(y/\u03bb), we have a very useful characterization of the dual optimal solution via the so-called normal cones [20].\nProposition 11. [20, 2] For a closed convex set C \u2208 Rn and a point w \u2208 C, the normal cone to C at w is defined by\nNC(w) = {v : \u3008v,w\u2032 \u2212w\u3009 \u2264 0, \u2200w\u2032 \u2208 C}. (32)\nThen, the following hold:\n(i) NC(w) = {v : PC(w + v) = w}.\n(ii) PC(w + v) = w, \u2200v \u2208 NC(w).\n(iii) Let w /\u2208 C. Then, w = PC(w)\u21d4 w \u2212w \u2208 NC(w).\n(iv) Let w /\u2208 C and w = PC(w). Then, PC(w + t(w \u2212w)) = w for all t \u2265 0.\nBy Theorem 8, \u03b8\u2217(\u03bb\u0304, \u03b1) is known if \u03bb\u0304 = \u03bb\u03b1max. Thus, we can estimate \u03b8 \u2217(\u03bb, \u03b1) in terms of \u03b8\u2217(\u03bb\u0304, \u03b1). Due to the same reason, we only consider the cases with \u03bb < \u03bb\u03b1max for \u03b8 \u2217(\u03bb, \u03b1) to be estimated.\nRemark 3. In many applications, the parameter values that perform the best are usually unknown. To determine appropriate parameter values, commonly used approaches such as cross validation and stability selection involve solving SGL many times over a grip of parameter values. Thus, given {\u03b1(i)}Ii=1 and \u03bb(1) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bb(J ), we can fix the value of \u03b1 each time and solve SGL by varying the value of \u03bb. We repeat the process until we solve SGL for all of the parameter values.\nTheorem 12. For the SGL problem in (3), suppose that \u03b8\u2217(\u03bb\u0304, \u03b1) is known with \u03bb\u0304 \u2264 \u03bb\u03b1max. Let \u03c1g, g = 1, . . . , G, be defined by Theorem 8. For any \u03bb \u2208 (0, \u03bb\u0304), we define\nn\u03b1(\u03bb\u0304) =  y \u03bb\u0304 \u2212 \u03b8\u2217(\u03bb\u0304, \u03b1), if \u03bb\u0304 < \u03bb\u03b1max, X\u2217S1 ( XT\u2217\ny \u03bb\u03b1max\n) , if \u03bb\u0304 = \u03bb\u03b1max, where X\u2217 = argmaxXg \u03c1g,\nv\u03b1(\u03bb, \u03bb\u0304) = y \u03bb \u2212 \u03b8\u2217(\u03bb\u0304, \u03b1),\nv\u03b1(\u03bb, \u03bb\u0304) \u22a5 = v\u03b1(\u03bb, \u03bb\u0304)\u2212 \u3008v\u03b1(\u03bb, \u03bb\u0304),n\u03b1(\u03bb\u0304)\u3009 \u2016n\u03b1(\u03bb\u0304)\u20162 n\u03b1(\u03bb\u0304).\nThen, the following hold:\n(i) n\u03b1(\u03bb\u0304) \u2208 NF\u03b1(\u03b8\u2217(\u03bb\u0304, \u03b1)),\n(ii) \u2016\u03b8\u2217(\u03bb, \u03b1)\u2212 (\u03b8\u2217(\u03bb\u0304, \u03b1) + 12v \u22a5 \u03b1 (\u03bb, \u03bb\u0304))\u2016 \u2264 12\u2016v \u22a5 \u03b1 (\u03bb, \u03bb\u0304)\u2016.\nProof. (i) Suppose that \u03bb\u0304 < \u03bb\u03b1max. Theorem 8 implies that y/\u03bb\u0304 /\u2208 F\u03b1 and thus\ny/\u03bb\u0304\u2212PF\u03b1 ( y/\u03bb\u0304 ) = y/\u03bb\u0304\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1) 6= 0.\nBy the third part of Proposition 11, we can see that\ny/\u03bb\u0304\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1) \u2208 NF\u03b1(\u03b8\u2217(\u03bb\u0304, \u03b1)). (33)\nThus, the statement holds for all \u03bb\u0304 < \u03bb\u03b1max. Suppose that \u03bb\u0304 = \u03bb\u03b1max. By Theorem 8, we have\n\u03b8\u2217(\u03bb\u0304, \u03b1) = y/\u03bb\u0304 \u2208 F\u03b1.\nIn view of the definition of X\u2217, we have\u2225\u2225\u2225S1 (XT\u2217 y\u03bb\u03b1max)\u2225\u2225\u2225 = \u03b1\u221an\u2217, where n\u2217 is the number of feature contained in X\u2217. Moreover, it is easy to see that\n\u2016S1(XT\u2217 \u03b8)\u2016 \u2264 \u03b1 \u221a n\u2217, \u2200\u03b8 \u2208 F\u03b1.\nTherefore, to prove the statement, we need to show that\u2329 X\u2217S1 ( XT\u2217\ny \u03bb\u03b1max ) , \u03b8 \u2212 y\u03bb\u03b1max \u232a \u2264 0, \u2200\u03b8 \u2208 F\u03b1. (34)\nRecall Remark 1, we have the following identity [see Eq. (19)] S1 ( XT\u2217\ny \u03bb\u03b1max\n) = XT\u2217\ny \u03bb\u03b1max\n\u2212PB\u221e ( XT\u2217\ny \u03bb\u03b1max\n) .\nThus, we have\u2329 X\u2217S1 ( XT\u2217\ny \u03bb\u03b1max ) , \u03b8 \u2212 y\u03bb\u03b1max \u232a (35)\n= \u2329 S1 ( XT\u2217\ny \u03bb\u03b1max\n) ,XT\u2217 ( \u03b8 \u2212 y\u03bb\u03b1max ) + PB\u221e ( XT\u2217 y \u03bb\u03b1max ) \u2212PB\u221e ( XT\u2217 y \u03bb\u03b1max )\u232a = \u2329 S1 ( XT\u2217\ny \u03bb\u03b1max\n) ,XT\u2217 \u03b8 \u2212PB\u221e ( XT\u2217\ny \u03bb\u03b1max )\u232a \u2212 \u2225\u2225\u2225S1 (XT\u2217 y\u03bb\u03b1max)\u2225\u2225\u22252\n= \u2329 S1 ( XT\u2217\ny \u03bb\u03b1max\n) ,XT\u2217 \u03b8 \u2212PB\u221e ( XT\u2217\ny \u03bb\u03b1max\n)\u232a \u2212 \u03b12n\u2217.\nConsider the first term on the right hand side of Eq. (35), we have\u2329 S1 ( XT\u2217\ny \u03bb\u03b1max\n) ,XT\u2217 \u03b8 \u2212PB\u221e ( XT\u2217\ny \u03bb\u03b1max\n)\u232a (36)\n= \u2329 S1 ( XT\u2217\ny \u03bb\u03b1max\n) ,XT\u2217 \u03b8 \u2212PB\u221e(XT\u2217 \u03b8) + PB\u221e(XT\u2217 \u03b8)\u2212PB\u221e ( XT\u2217\ny \u03bb\u03b1max )\u232a = \u2329 S1 ( XT\u2217\ny \u03bb\u03b1max\n) ,S1(XT\u2217 \u03b8) \u232a + \u2329 S1 ( XT\u2217\ny \u03bb\u03b1max\n) ,PB\u221e(X T \u2217 \u03b8)\u2212PB\u221e ( XT\u2217\ny \u03bb\u03b1max\n)\u232a .\nLet P = {i : [XT\u2217 y \u03bb\u03b1max ]i > 1} and N = {i : [XT\u2217 y \u03bb\u03b1max ]i < \u22121}. We note that the second term on the right hand side of Eq. (36) can be written as\u2329\nS1 ( XT\u2217\ny \u03bb\u03b1max\n) ,PB\u221e(X T \u2217 \u03b8)\u2212PB\u221e ( XT\u2217\ny \u03bb\u03b1max\n)\u232a (37)\n= \u2211 i\u2208P ( [XT\u2217 y \u03bb\u03b1max ]i \u2212 1 ) ( [PB\u221e(X T \u2217 \u03b8)]i \u2212 1 ) + \u2211 j\u2208N ( [XT\u2217 y \u03bb\u03b1max ]j + 1 ) ( [PB\u221e(X T \u2217 \u03b8)]j + 1 ) .\nBecause \u2016PB\u221e(XT\u2217 \u03b8)\u2016\u221e \u2264 1, we can see that Eq. (37) is non-positive. Therefore, by Eq. (36), we have\u2329\nS1 ( XT\u2217\ny \u03bb\u03b1max\n) ,XT\u2217 \u03b8 \u2212PB\u221e ( XT\u2217\ny \u03bb\u03b1max\n)\u232a \u2264 \u2329 S1 ( XT\u2217\ny \u03bb\u03b1max\n) ,S1(XT\u2217 \u03b8) \u232a (38)\n\u2264 \u2225\u2225\u2225S1 (XT\u2217 y\u03bb\u03b1max)\u2225\u2225\u2225\u2225\u2225S1(XT\u2217 \u03b8)\u2225\u2225 \u2264\u03b12n\u2217.\nCombining Eq. (35) and the inequality in (38), we can see that the inequality in (34) holds. Thus, the statement holds for \u03bb\u0304 = \u03bb\u03b1max. This completes the proof.\n(ii) We now show the second half. It is easy to see that the statement is equivalent to\n\u2016\u03b8\u2217(\u03bb, \u03b1)\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1)\u20162 \u2264 \u3008\u03b8\u2217(\u03bb, \u03b1)\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1), v\u22a5\u03b1 (\u03bb, \u03bb\u0304)\u3009. (39)\nThus, we will show that the inequality in (39) holds.\nBecause of the first half, we have\n\u3008n\u03b1(\u03bb\u0304), \u03b8 \u2212 \u03b8\u2217(\u03bb\u0304, \u03b1)\u3009 \u2264 0, \u2200 \u03b8 \u2208 F\u03b1. (40)\nBy letting \u03b8 = \u03b8\u2217(\u03bb, \u03b1), the inequality in (40) leads to\n\u3008n\u03b1(\u03bb\u0304), \u03b8\u2217(\u03bb, \u03b1)\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1)\u3009 \u2264 0. (41)\nIn view of the first half and by letting \u03b8 = 0, the inequality in (40) leads to\n\u3008n\u03b1(\u03bb\u0304), 0\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1)\u3009 \u2264 0\u21d2 { \u3008n\u03b1(\u03bb\u0304), y\u3009 \u2265 0, if \u03bb\u0304 = \u03bb\u03b1max, \u2016y\u2016/\u03bb\u0304 \u2265 \u2016\u03b8\u2217(\u03bb\u0304, \u03b1)\u2016, if \u03bb\u0304 < \u03bb\u03b1max.\n(42)\nMoreover, the first half also leads to y\u03bb \u2212 \u03b8 \u2217(\u03bb, \u03b1) \u2208 NF\u03b1(\u03b8\u2217(\u03bb, \u03b1)). Thus, we have\n\u3008y\u03bb \u2212 \u03b8 \u2217(\u03bb, \u03b1), \u03b8 \u2212 \u03b8\u2217(\u03bb, \u03b1)\u3009 \u2264 0, \u2200 \u03b8 \u2208 F\u03b1. (43)\nBy letting \u03b8 = \u03b8\u2217(\u03bb\u0304, \u03b1), the inequality in (43) results in\n\u3008y\u03bb \u2212 \u03b8 \u2217(\u03bb, \u03b1), \u03b8\u2217(\u03bb\u0304, \u03b1)\u2212 \u03b8\u2217(\u03bb, \u03b1)\u3009 \u2264 0, \u2200 \u03b8 \u2208 F\u03b1. (44)\nWe can see that the inequality in (44) is equivalent to\n\u2016\u03b8\u2217(\u03bb, \u03b1)\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1)\u20162 \u2264\u3008\u03b8\u2217(\u03bb, \u03b1)\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1), v\u03b1(\u03bb, \u03bb\u0304)\u3009. (45)\nOn the other hand, the right hand side of (39) can be rewritten as\n\u3008\u03b8\u2217(\u03bb, \u03b1)\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1), v\u22a5\u03b1 (\u03bb, \u03bb\u0304)\u3009 (46) =\u3008\u03b8\u2217(\u03bb, \u03b1)\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1), v\u03b1(\u03bb, \u03bb\u0304)\u3009 \u2212 \u3008\u03b8\u2217(\u03bb, \u03b1)\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1), v\u03b1(\u03bb, \u03bb\u0304)\u2212 v\u22a5\u03b1 (\u03bb, \u03bb\u0304)\u3009\n=\u3008\u03b8\u2217(\u03bb, \u03b1)\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1), v\u03b1(\u03bb, \u03bb\u0304)\u3009 \u2212 \u2329 \u03b8\u2217(\u03bb, \u03b1)\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1), \u3008v\u03b1(\u03bb,\u03bb\u0304),n\u03b1(\u03bb\u0304)\u3009\n\u2016n\u03b1(\u03bb\u0304)\u20162 n\u03b1(\u03bb\u0304)\n\u232a .\nIn view of (41), (45) and (46), we can see that (39) holds if \u3008v\u03b1(\u03bb, \u03bb\u0304),n\u03b1(\u03bb\u0304)\u3009 \u2265 0. Indeed,\n\u3008v\u03b1(\u03bb, \u03bb\u0304),n\u03b1(\u03bb\u0304)\u3009 = \u2329 y/\u03bb\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1),n\u03b1(\u03bb\u0304) \u232a (47)\n= ( 1/\u03bb\u2212 1/\u03bb\u0304 ) \u3008y,n\u03b1(\u03bb\u0304)\u3009+ \u3008y/\u03bb\u0304\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1),n\u03b1(\u03bb\u0304)\u3009\nConsider the first term on the right hand side of Eq. (47). By the first half of (42), we have\n\u3008y,n\u03b1(\u03bb\u0304)\u3009 \u2265 0, if \u03bb\u0304 = \u03bb\u03b1max. (48)\nSuppose that \u03bb\u0304 < \u03bb\u03b1max. By the second half of (42), we can see that\n\u3008y,n\u03b1(\u03bb\u0304)\u3009 = \u3008y,y/\u03bb\u0304\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1)\u3009 \u2265 1/\u03bb\u0304\u2016y\u20162 \u2212 \u2016y\u2016\u2016\u03b8\u2217(\u03bb\u0304, \u03b1)\u2016 \u2265 0. (49)\nConsider the second term on the right hand side of Eq. (47). It is easy to see that\n\u3008y/\u03bb\u0304\u2212 \u03b8\u2217(\u03bb\u0304, \u03b1),n\u03b1(\u03bb\u0304)\u3009 =\n{ 0, if \u03bb\u0304 = \u03bb\u03b1max,\n\u2016n\u03b1(\u03bb\u0304)\u20162, if \u03bb\u0304 < \u03bb\u03b1max. (50)\nCombining (48), (49) and Eq. (50), we have \u3008v\u03b1(\u03bb, \u03bb\u0304),n\u03b1(\u03bb\u0304)\u3009 \u2265 0, which completes the proof.\nFor notational convenience, we denote\no\u03b1(\u03bb, \u03bb\u0304) = \u03b8 \u2217(\u03bb\u0304, \u03b1) + 12v \u22a5 \u03b1 (\u03bb, \u03bb\u0304). (51)\nTheorem 12 shows that \u03b8\u2217(\u03bb, \u03b1) lies inside the ball of radius 12\u2016v \u22a5 \u03b1 (\u03bb, \u03bb\u0304)\u2016 centered at o\u03b1(\u03bb, \u03bb\u0304)."}, {"heading": "4.2 Solving for the Supreme Values via Nonconvex Optimization", "text": "We solve the optimization problems in (R1\u2217) and (R2\u2217). To simplify notations, let\n\u0398 = {\u03b8 : \u2016\u03b8 \u2212 o\u03b1(\u03bb, \u03bb\u0304)\u2016 \u2264 12\u2016v \u22a5 \u03b1 (\u03bb, \u03bb\u0304)\u2016}, (52) \u039eg = { \u03beg : \u2016\u03beg \u2212XTg o\u03b1(\u03bb, \u03bb\u0304)\u2016 \u2264 12\u2016v \u22a5 \u03b1 (\u03bb, \u03bb\u0304)\u2016\u2016Xg\u20162 } , g = 1, . . . , G. (53)\nTheorem 12 indicates that \u03b8\u2217(\u03bb, \u03b1) \u2208 \u0398. Moreover, we can see that XTg \u0398 \u2286 \u039eg, g = 1, . . . , G. To develop the TLFre rule by (R1\u2217) and (R2\u2217), we need to solve the following optimization problems:\ns\u2217g(\u03bb, \u03bb\u0304;\u03b1) = sup\u03beg {\u2016S1(\u03beg)\u2016 : \u03beg \u2208 \u039eg}, g = 1, . . . , G, (54) t\u2217gi(\u03bb, \u03bb\u0304;\u03b1) = sup\u03b8 {|x T gi\u03b8| : \u03b8 \u2208 \u0398}, i = 1, . . . , ng, g = 1, . . . , G. (55)"}, {"heading": "4.2.1 The Solution of Problem (54)", "text": "We consider the following equivalent problem of (54):\n1 2 ( s\u2217g(\u03bb, \u03bb\u0304;\u03b1) )2 = sup\u03beg { 1 2\u2016S1(\u03beg)\u2016 2 : \u03beg \u2208 \u039eg } . (56)\nWe can see that the objective function of problem (56) is continuously differentiable and the feasible set is a ball. Thus, problem (56) is nonconvex because we need to maximize a convex function subject to a convex set. We first derive the necessary optimality conditions in Lemma 13 and then deduce the closed form solutions of problems (54) and (56) in Theorem 15.\nLemma 13. Let \u039e\u2217g be the set of optimal solutions of (56) and \u03be \u2217 g \u2208 \u039e\u2217g. Then, the following hold: (i) Suppose that \u03be\u2217g is an interior point of \u039eg. Then, \u039eg is a subset of B\u221e. (ii) Suppose that \u03be\u2217g is a boundary point of \u039eg. Then, there exists \u00b5 \u2217 \u2265 0 such that\nS1(\u03be\u2217g) = \u00b5\u2217 ( \u03be\u2217g \u2212XTg o\u03b1(\u03bb, \u03bb\u0304) ) . (57)\n(iii) Suppose that there exists \u03be0g \u2208 \u039eg and \u03be0g /\u2208 B\u221e. Then, we have (iiia) \u03be\u2217g /\u2208 B\u221e and \u03be\u2217g is a boundary point of \u039eg, i.e.,\n\u2016\u03be\u2217g \u2212XTg o\u03b1(\u03bb, \u03bb\u0304)\u2016 = 12\u2016v \u22a5 \u03b1 (\u03bb, \u03bb\u0304)\u2016\u2016Xg\u20162.\n(iiib) The optimality condition in Eq. (57) holds with \u00b5\u2217 > 0.\nTo show Lemma 13, we need the following proposition.\nProposition 14. [9] Suppose that h \u2208 \u03930 and C is a nonempty closed convex set. If w\u2217 \u2208 C is a local maximum of h on C, then \u2202h(w\u2217) \u2286 NC(w\u2217).\nWe now present the proof of Lemma 13.\nProof. To simplify notations, let\nc = XTg o\u03b1(\u03bb, \u03bb\u0304) and r = 1 2 \u2016v\u22a5\u03b1 (\u03bb, \u03bb\u0304)\u2016\u2016Xg\u20162. (58)\nBy Eq. (1), we have\nh(w) := 1\n2 \u2016S1(w)\u20162 =\n1\n2 \u2211 i (|[w]i| \u2212 1)2+. (59)\nIt is easy to see that h(\u00b7) is continuously differentiable. Indeed, we have\n\u2207h(w) = S1(w). (60)\nThen, problem (56) can be written as\n1 2 (s\u2217g(\u03bb, \u03bb\u0304;\u03b1)) 2 = sup \u03beg\n{ h(\u03beg) = 1\n2 \u2211 i ([\u03beg]i \u2212 1)2+ : \u03beg \u2208 \u039eg\n} , (61)\nwhere \u039eg = {\u03beg : \u2016\u03beg \u2212 c\u2016 \u2264 r}. Then, Proposition 14 results in\nS1(\u03be\u2217g) = \u2207h(\u03be\u2217g) = \u2202h(\u03be\u2217g) \u2286 N\u039eg(\u03be\u2217g). (62)\n(i) Suppose that \u03be\u2217g is an interior point of \u039eg. Then, we have N\u039eg(\u03be \u2217 g) = 0. By Eq. (62), we\ncan see that\n0 = S1(\u03be\u2217g)\u21d2 0 = 1 2 \u2016S1(\u03be\u2217g)\u20162 = 1 2 (s\u2217g(\u03bb, \u03bb\u0304;\u03b1)) 2 = sup \u03beg\n{ 1\n2 \u2016S1(\u03beg)\u20162 : \u03beg \u2208 \u039eg\n} .\nTherefore, we have\n\u2016S1(\u03beg)\u2016 = 0, \u2200 \u03beg \u2208 \u039eg. (63)\nBecause S1(\u03beg) = \u03beg \u2212PB\u221e(\u03beg) (see Remark 1), Eq. (63) implies that\n\u03beg = PB\u221e(\u03beg), \u2200 \u03beg \u2208 \u039eg \u21d2 \u03beg \u2208 B\u221e, \u2200 \u03beg \u2208 \u039eg.\nThis completes the proof.\n(ii) Suppose that \u03be\u2217g is a boundary point of \u039eg. We can see that\nN\u039eg(\u03be \u2217 g) = {\u00b5(\u03be\u2217g \u2212 c), \u00b5 \u2265 0}. (64)\nThen, Eq. (57) follows by combining Eq. (64) and the optimality condition in (62).\n(iii) Suppose that there exists \u03be0g \u2208 \u039eg and \u03be0g /\u2208 B\u221e.\n(iiia) The definition of \u03be0g leads to\n0 < \u2016S1(\u03be0g)\u2016 \u2264 \u2016S1(\u03be\u2217g)\u2016 \u21d2 \u03be\u2217g /\u2208 B\u221e.\nMoreover, we can see that \u03be\u2217g is a boundary point of \u039eg. Because if \u03be \u2217 g is an interior point of \u039eg, the first part implies that \u039eg \u2282 B\u221e. This contradicts with the existence of \u03be0g . Thus, \u03be \u2217 g must be a boundary point of \u039eg, i.e. \u2016\u03be\u2217g \u2212 c\u2016 = r.\n(iiib) Because \u03be\u2217g is a boundary point of \u039eg, the second part implies that Eq. (57) holds. Moreover, from (iiia), we know that \u03be\u2217g /\u2208 B\u221e. Therefore, both sides of Eq. (57) are nonzero and thus \u00b5\u2217 > 0. This completes the proof.\nBased on the necessary optimality conditions in Lemma 13, we derive the closed form solutions of (54) and (56) in the following Theorem. The notations are the same as the ones in the proof of Lemma 13 [see Eq. (58) and Eq. (59)].\nTheorem 15. For problems (54) and (56), let c = XTg o\u03b1(\u03bb, \u03bb\u0304), r = 1 2\u2016v \u22a5 \u03b1 (\u03bb, \u03bb\u0304)\u2016\u2016Xg\u20162 and \u039e\u2217g be the set of the optimal solutions.\n(i) Suppose that c /\u2208 B\u221e, i.e., \u2016c\u2016\u221e > 1. Let u = rS1(c)/\u2016S1(c)\u2016. Then,\ns\u2217g(\u03bb, \u03bb\u0304;\u03b1) = \u2016S1(c)\u2016+ r and \u039e\u2217g = {c + u}. (65)\n(ii) Suppose that c is a boundary point of B\u221e, i.e., \u2016c\u2016\u221e = 1. Then,\ns\u2217g(\u03bb, \u03bb\u0304;\u03b1) = r and \u039e \u2217 g = {c + u : u \u2208 NB\u221e(c), \u2016u\u2016 = r} . (66)\n(iii) Suppose that c \u2208 intB\u221e, i.e., \u2016c\u2016\u221e < 1. Let i\u2217 \u2208 I\u2217 = {i : |[c]i| = \u2016c\u2016\u221e}. Then,\ns\u2217g(\u03bb, \u03bb\u0304;\u03b1) = (\u2016c\u2016\u221e + r \u2212 1)+ , (67)\n\u039e\u2217g =  \u039eg, if \u039eg \u2282 B\u221e, {c + r \u00b7 sgn([c]i\u2217)ei\u2217 : i\u2217 \u2208 I\u2217} , if \u039eg 6\u2282 B\u221e and c 6= 0, {r \u00b7 ei\u2217 ,\u2212r \u00b7 ei\u2217 : i\u2217 \u2208 I\u2217} , if \u039eg 6\u2282 B\u221e and c = 0,\nwhere ei is the i th standard basis vector.\nProof. (i) Suppose that c /\u2208 B\u221e. By the third part of Lemma 13, we have\n\u03be\u2217g /\u2208 B\u221e, \u2016\u03be\u2217g \u2212 c\u2016 = r, (68) \u03be\u2217g \u2212PB\u221e(\u03be\u2217g) = S1(\u03be\u2217g) = \u00b5\u2217(\u03be\u2217g \u2212 c), \u00b5\u2217 > 0. (69)\nBy Eq. (69), we can see that \u00b5\u2217 6= 1 because otherwise we would have c = PB\u221e(\u03be\u2217g) \u2208 B\u221e. Moreover, we can only consider the cases with \u00b5\u2217 > 1 because \u2016S1(\u03be\u2217g)\u2016 = \u00b5\u2217r and we aim to maximize \u2016S1(\u03be\u2217g)\u2016. Therefore, if we can find a solution with \u00b5\u2217 > 1, there is no need to consider the cases with \u00b5\u2217 \u2208 (0, 1). Suppose that \u00b5\u2217 > 1. Then, Eq. (69) leads to\nc =PB\u221e(\u03be \u2217 g) +\n( 1\u2212 1\n\u00b5\u2217\n)( \u03be\u2217g \u2212PB\u221e(\u03be\u2217g) ) , (70)\n\u03be\u2217g =PB\u221e(\u03be \u2217 g) +\n\u00b5\u2217 \u00b5\u2217 \u2212 1 ( c\u2212PB\u221e(\u03be\u2217g) ) . (71)\nIn view of part (iv) of Proposition 11 and Eq. (70), we have\nPB\u221e(c) = PB\u221e(\u03be \u2217 g). (72)\nTherefore, Eq. (71) can be rewritten as\nS1(\u03be\u2217g) = \u03be\u2217g \u2212PB\u221e(\u03be\u2217g) = \u00b5\u2217\n\u00b5\u2217 \u2212 1 (c\u2212PB\u221e(c)) =\n\u00b5\u2217\n\u00b5\u2217 \u2212 1 S1(c). (73)\nCombining Eq. (69) and Eq. (73), we have\n\u00b5\u2217\n\u00b5\u2217 \u2212 1 \u2016S1(c)\u2016 = \u00b5\u2217\u2016\u03be\u2217g \u2212 c\u2016 = \u00b5\u2217r \u21d2 \u00b5\u2217 = 1 + \u2016S1(c)\u2016 r > 1. (74)\nThe statement holds by plugging Eq. (74) and Eq. (72) into Eq. (71) and Eq. (73). Moreover, the above discussion implies that \u039e\u2217g only contains one element as shown in Eq. (65).\n(ii) Suppose that c is a boundary point of B\u221e. Then, we can find a point \u03be0g \u2208 \u039eg and \u03be0g /\u2208 B\u221e. By the third part of Lemma 13, we also have Eq. (68) and Eq. (69) hold. We claim that \u00b5\u2217 \u2208 (0, 1]. The argument is as follows. Suppose that \u00b5\u2217 > 1. By the same argument as in the proof of the first part, we can see that Eq. (73) holds. Because S1(\u03be\u2217g) 6= 0 by Eq. (68), we have S1(c) 6= 0. This implies that c /\u2208 B\u221e. Thus, we have a contradiction, which implies that \u00b5\u2217 \u2208 (0, 1].\nLet us consider the cases with \u00b5\u2217 = 1. Because \u2016S1(\u03be\u2217g)\u2016 = \u00b5\u2217r [see Eq. (69)] and we want to maximize \u2016S1(\u03be\u2217g)\u2016, there is no need to consider the cases with \u00b5\u2217 \u2208 (0, 1) if we can find solutions of problem (54) with \u00b5\u2217 = 1. Therefore, Eq. (69) leads to\nPB\u221e(\u03be \u2217 g) = c.\nBy part (iii) of Proposition 11, we can see that\nPB\u221e(\u03be \u2217 g) = c\u21d4 \u03be\u2217g \u2212 c \u2208 NB\u221e(c). (75)\nCombining Eq. (75) and Eq. (68), the statement holds immediately, which confirms that \u00b5\u2217 = 1.\n(iii) Suppose that c is an interior point of B\u221e.\n(a) We first consider the cases with \u039eg \u2282 B\u221e. Then, we can see that\nS1(\u03be) = 0, \u2200\u03be \u2208 \u039eg \u21d2 \u039e\u2217g = \u039eg.\nIn other words, an arbitrary point of \u039eg is an optimal solution of problem (54). Thus, we have\nc + r \u00b7 sgn(ei\u2217)ei\u2217 \u2208 \u039e\u2217g, s\u2217g(\u03bb, \u03bb\u0304;\u03b1) = 0.\nOn the other hand, we can see that\nc\u2212 rei \u2208 \u039eg \u2282 B\u221e, c + rei \u2208 \u039eg \u2282 B\u221e, i = 1, . . . , ng \u21d2 \u2016c\u2016\u221e + r \u2264 1.\nTherefore, we have\n(\u2016c\u2016\u221e + r \u2212 1)+ = 0,\nand thus\ns\u2217g(\u03bb, \u03bb\u0304;\u03b1) = (\u2016c\u2016\u221e + r \u2212 1)+.\n(b) Suppose that \u039eg 6\u2282 B\u221e, i.e., there exists \u03be0 \u2208 \u039eg such that \u03be0 /\u2208 B\u221e. By the third part of Lemma 13, we have Eq. (68) and Eq. (69) hold. Moreover, in view of the proof of the first and second part, we can see that \u00b5\u2217 \u2208 (0, 1). Therefore, Eq. (69) leads to\n(1\u2212 \u00b5\u2217)\u03be\u2217g + \u00b5\u2217c = PB\u221e(\u03be\u2217g). (76)\nBy rearranging the terms of Eq. (76), we have\nPB\u221e(\u03be \u2217 g)\u2212 c = (1\u2212 \u00b5\u2217)(\u03be\u2217g \u2212 c). (77)\nBecause \u00b5\u2217 \u2208 (0, 1), Eq. (76) implies that PB\u221e(\u03be\u2217g) lies on the line segment connecting \u03be\u2217g and c. Thus, we have\n\u2016\u03be\u2217g \u2212PB\u221e(\u03be\u2217g)\u2016+ \u2016PB\u221e(\u03be\u2217g)\u2212 c\u2016 = \u2016\u03be\u2217g \u2212 c\u2016 = r. (78)\nTherefore, to maximize \u2016S1(\u03be\u2217g)\u2016 = \u2016\u03be\u2217g\u2212PB\u221e(\u03be\u2217g)\u2016, we need to minimize \u2016PB\u221e(\u03be\u2217g)\u2212 c\u2016. Because \u03be\u2217g /\u2208 B\u221e, we can see that PB\u221e(\u03be\u2217g) is a boundary point of B\u221e. Therefore, we need to solve the following minimization problem:\nmin \u03c6g {\u2016\u03c6g \u2212 c\u2016 : \u2016\u03c6g\u2016\u221e = 1}. (79)\nSuppose that c = 0. We can see that the set of optimal solutions of problem (79) is\n\u03a6\u2217g = {ei} ng i=1 \u222a {\u2212ei} ng i=1.\nFor each \u03c6\u2217g \u2208 \u03a6\u2217g, we set it as PB\u221e(\u03be\u2217g). In view of Eq. (77) and Eq. (68), the statement follows immediately. Suppose that c 6= 0. Recall that I\u2217 = {i\u2217 : |[c]i\u2217 | = \u2016c\u2016\u221e}. It is easy to see that\n\u03a6\u2217g = { \u03c6i\u2217 : [\u03c6i\u2217 ]k = { sgn([c]i\u2217), if k = i \u2217,\n[c]k, otherwise, i\u2217 \u2208 I\u2217\n} .\nWe can see that\n\u03c6i\u2217 \u2212 c = (1\u2212 |[c]\u221e|)sgn([c]i\u2217)ei\u2217 , i\u2217 \u2208 I\u2217.\nFor each \u03c6i\u2217 , we set it to PB\u221e(\u03be \u2217 g). Then, we can see that the statement holds by Eq. (77) and Eq. (68). This completes the proof."}, {"heading": "4.2.2 The Solution of Problem (55)", "text": "Problem (55) can be solved directly via the Cauchy-Schwarz inequality.\nTheorem 16. For problem (55), we have t\u2217gi(\u03bb, \u03bb\u0304;\u03b1) = |x T gio\u03b1(\u03bb, \u03bb\u0304)|+ 1 2\u2016v \u22a5 \u03b1 (\u03bb, \u03bb\u0304)\u2016\u2016xgi\u2016. Proof. To simplify notations, let o = o\u03b1(\u03bb, \u03bb\u0304), r = 1 2\u2016v \u22a5 \u03b1 (\u03bb, \u03bb\u0304)\u2016 and t\u2217g = t\u2217g(\u03bb, \u03bb\u0304;\u03b1). Therefore, the set \u0398 in Eq. (52) can be written as\n\u0398 = {o + v : \u2016v\u2016 \u2264 r}.\nThen, problem (55) becomes\nt\u2217gi = sup v {|xTgi(o + v)| : \u2016v\u2016 \u2264 r}.\nWe can see that\n|xTgi(o + v)| \u2264 |x T gio|+ |x T giv| \u2264 |x T gio|+ \u2016xgi\u2016\u2016v\u2016 \u2264 |x T gio|+ \u2016xgi\u2016r.\nThus, we have\nt\u2217gi \u2264 |x T gio|+ \u2016xgi\u2016r.\nConsider v\u2217 = rxgi/\u2016xgi\u2016. It is easy to see that o + v\u2217 \u2208 \u0398 and\n|xTgi(o + v)| = |x T gio|+ \u2016xgi\u2016r.\nTherefore, we have\nt\u2217gi = |x T gio|+ \u2016xgi\u2016r,\nwhich completes the proof."}, {"heading": "4.3 The Proposed Two-Layer Screening Rules", "text": "To develop the two-layer screening rules for SGL, we only need to plug the supreme values s\u2217g(\u03bb2, \u03bb\u03042;\u03bb1) and t\u2217gi(\u03bb2, \u03bb\u03042;\u03bb1) in (R1 \u2217) and (R2\u2217). We present the TLFre rule as follows.\nTheorem 17. For the SGL problem in (3), suppose that we are given \u03b1 and a sequence of parameter values \u03bb\u03b1max = \u03bb\n(0) > \u03bb(1) > . . . > \u03bb(J ). Moreover, assume that \u03b2\u2217(\u03bb(j), \u03b1) is known for an integer 0 \u2264 j < J . Let \u03b8\u2217(\u03bb(j), \u03b1), v\u22a5\u03b1 (\u03bb(j+1), \u03bb(j)) and s\u2217g(\u03bb(j+1), \u03bb(j);\u03b1) be given by Eq. (14), Theorems 12 and 15, respectively. Then, for g = 1, . . . , G, the following holds\ns\u2217g(\u03bb (j+1), \u03bb(j);\u03b1) < \u03b1 \u221a ng \u21d2 \u03b2\u2217g (\u03bb(j+1), \u03b1) = 0. (L1)\nFor the g\u0302th group that does not pass the rule in (L1), we have [\u03b2\u2217g\u0302 (\u03bb(j+1), \u03b1)]i = 0 if\u2223\u2223\u2223\u2223\u2223xTg\u0302i ( y \u2212X\u03b2\u2217(\u03bb(j), \u03b1) \u03bb(j) + 1 2 v\u22a5\u03b1 (\u03bb (j+1), \u03bb(j)) )\u2223\u2223\u2223\u2223\u2223+ 12\u2016v\u22a5\u03b1 (\u03bb(j+1), \u03bb(j))\u2016\u2016xg\u0302i\u2016 \u2264 1. (L2) (L1) and (L2) are the first layer and second layer screening rules of TLFre, respectively."}, {"heading": "5 Extension to Nonnegative Lasso", "text": "The framework of TLFre is applicable to a large class of sparse models with multiple regularizers. As an example, we extend TLFre to nonnegative Lasso:\nmin \u03b2\u2208Rp\n{ 1\n2 \u2016y\u2212X\u03b2\u20162 + \u03bb\u2016\u03b2\u20161 : \u03b2 \u2208 Rp+\n} , (80)\nwhere \u03bb > 0 is the regularization parameter and Rp+ is the nonnegative orthant of Rp. In Section 5.1, we transform the constraint \u03b2 \u2208 Rp+ to a regularizer and derive the Fenchel\u2019s dual of the nonnegative Lasso problem. We then motivate the screening method\u2014called DPC since the key step is to decompose a convex set via Fenchel\u2019s Duality Theorem\u2014via the KKT conditions in Section 5.2. In Section 5.3, we analyze the geometric properties of the dual problem and derive the set of parameter values leading to zero solutions. We then develop the screening method for nonnegative Lasso in Section 5.4."}, {"heading": "5.1 The Fenchel\u2019s Dual of Nonnegative Lasso", "text": "Let IRp+ be the indicator function of R p +. By noting that IRp+ = \u03bbIR p + for any \u03bb > 0, we can rewrite the nonnegative Lasso problem in (80) as\nmin \u03b2\u2208Rp\n1 2 \u2016y\u2212X\u03b2\u20162 + \u03bb\u2016\u03b2\u20161 + \u03bbIRp+(\u03b2). (81)\nIn other words, we incorporate the constraint \u03b2 \u2208 Rp+ to the objective function as an additional regularizer. As a result, the nonnegative lasso problem in (81) has two regularizers. Thus, similar to SGL, we can derive the Fenchel\u2019s dual of nonnegative Lasso via Theorem 1.\nWe now proceed by following a similar procedure as the one in Section 3.1. We note that the nonnegative Lasso problem in (81) can also be formulated as the one in (6) with f(\u00b7) = 12\u2016 \u00b7 \u2016 2 and\n\u2126(\u03b2) = \u2016\u03b2\u20161 + IRp+(\u03b2). To derive the Fenchel\u2019s dual of nonnegative Lasso, we need to find f \u2217 and \u2126\u2217 by Theorem 1. Since we have already seen that f\u2217(\u00b7) = 12\u2016 \u00b7 \u2016 2 in Section 3.1, we only need to find \u2126\u2217(\u00b7). The following result is indeed a counterpart of Lemma 3.\nLemma 18. Let \u21262(\u03b2) = \u2016\u03b2\u20161, \u21263 = IRp+(\u03b2), and \u2126(\u03b2) = \u21262(\u03b2) + \u21263(\u03b2). Then,\n(i) (\u21262) \u2217(\u03be) = IB\u221e(\u03be) and (\u21263) \u2217(\u03be) = IRp\u2212(\u03be), where R p \u2212 is the nonpositive orthant of Rp.\n(ii) \u2126\u2217(\u03be) = ((\u21262) \u2217 (\u21263)\u2217)(\u03be) = IRp\u2212(\u03be \u2212 1), where R p 3 1 = (1, 1, . . . , 1)T .\nWe omit the proof of Lemma 18 since it is very similar to that of Lemma 3.\nRemark 4. Consider the second part of Lemma 18. Let C1 = {\u03be : \u03be \u2264 1}, where \u201c\u2264\u201d is defined component-wisely. We can see that\nIRp\u2212(\u03be \u2212 1) = IC1(\u03be)."}, {"heading": "On the other hand, Lemma 7 implies that", "text": "\u2126\u2217(\u03be) = ((\u21262) \u2217 (\u21263) \u2217)(\u03be) = IB\u221e+Rp\u2212(\u03be).\nThus, we have B\u221e + Rp\u2212 = C1. The second part of Lemma 18 decomposes each \u03be \u2208 B\u221e + R p \u2212 into two components: 1 and \u03be \u2212 1 that belong to B\u221e and Rp\u2212, respectively.\nBy Theorem 1 and Lemma 18, we can derive the Fenchel\u2019s dual of nonnegative Lasso in the following theorem (which is indeed the counterpart of Theorem 5).\nTheorem 19. For the nonnegative Lasso problem, the following hold:\n(i) The Fenchel\u2019s dual of nonnegative Lasso is given by:\ninf \u03b8\n{ 1\n2 \u2225\u2225\u2225y \u03bb \u2212 \u03b8 \u2225\u2225\u22252 \u2212 1 2 \u2016y\u20162 : \u3008xi, \u03b8\u3009 \u2264 1, i = 1, . . . , p } . (82)\n(ii) Let \u03b2\u2217(\u03bb) and \u03b8\u2217(\u03bb) be the optimal solutions of problems (81) and (82), respectively.Then,\n\u03bb\u03b8\u2217(\u03bb) = y \u2212X\u03b2\u2217(\u03bb), (83) XT \u03b8\u2217(\u03bb) \u2208 \u2202\u2016\u03b2\u2217(\u03bb)\u20161 + \u2202IRp+(\u03b2 \u2217(\u03bb)). (84)\nWe omit the proof of Theorem 19 since it is very similar to that of Theorem 5."}, {"heading": "5.2 Motivation of the Screening Method via KKT Conditions", "text": "The key to develop the DPC rule for nonnegative lasso is the KKT condition in (84). We can see that \u2202\u2016w\u20161 = SGN(w) and\n\u2202IRp+(w) =\n{ \u03be \u2208 Rp : [\u03be]i = { 0, if [w]i > 0,\n\u03c1, \u03c1 \u2264 0, if [w]i = 0,\n} .\nTherefore, the KKT condition in (84) implies that\n\u3008xi, \u03b8\u2217(\u03bb)\u3009 \u2208\n{ 1, if [\u03b2\u2217(\u03bb)]i > 0,\n%, % \u2264 1, if [\u03b2\u2217(\u03bb)]i = 0. (85)\nBy Eq. (85), we have the following rule:\n\u3008xi, \u03b8\u2217(\u03bb)\u3009 < 1\u21d2 [\u03b2\u2217(\u03bb)]i = 0. (R3)\nBecause \u03b8\u2217(\u03bb) is unknown, we can apply (R3) to identify the inactive features\u2014which have 0 coefficients in \u03b2\u2217(\u03bb). Similar to TLFre, we can first find a region \u0398 that contains \u03b8\u2217(\u03bb). Then, we can relax (R3) as follows:\nsup \u03b8\u2208\u0398 \u3008xi, \u03b8\u3009 < 1\u21d2 [\u03b2\u2217(\u03bb)]i = 0. (R3\u2217)\nInspired by (R3\u2217), we develop DPC via the following three steps:\nStep 1. Given \u03bb, we estimate a region \u0398 that contains \u03b8\u2217(\u03bb).\nStep 2. We solve the optimization problem \u03c9i = sup\u03b8\u2208\u0398 \u3008xi, \u03b8\u3009.\nStep 3. By plugging in \u03c9i computed from Step 2, (R3 \u2217) leads to the desired screening method\nDPC for nonnegative Lasso."}, {"heading": "5.3 Geometric Properties of the Fenchel\u2019s Dual of Nonnegative Lasso", "text": "In view of the Fenchel\u2019s dual of nonnegative Lasso in (82), we can see that the optimal solution is indeed the projection of y/\u03bb onto the feasible set F = {\u03b8 : \u3008xi, \u03b8\u3009 \u2264 1, i = 1, . . . , p}, i.e.,\n\u03b8\u2217(\u03bb) = PF (y \u03bb ) . (86)\nTherefore, if y/\u03bb \u2208 F , Eq. (86) implies that \u03b8\u2217(\u03bb) = y/\u03bb. If further y/\u03bb is an interior point of F , R3\u2217 implies that \u03b2\u2217(\u03bb) = 0. The next theorem gives the set of parameter values leading to 0 solutions of nonnegative Lasso.\nTheorem 20. For the nonnegative Lasso problem (81), Let \u03bbmax = maxi\u3008xi,y\u3009. Then, the following statements are equivalent:\n(i) y \u03bb \u2208 F , (ii) \u03b8\u2217(\u03bb) = y \u03bb , (iii) \u03b2\u2217(\u03bb) = 0, (iv) \u03bb \u2265 \u03bbmax.\nWe omit the proof of Theorem 20 since it is very similar to that of Theorem 8."}, {"heading": "5.4 The Proposed Screening Rule for Nonnegative Lasso", "text": "We follow the three steps in Section 5.2 to develop the screening rule for nonnegative Lasso. We first estimate a region that contains \u03b8\u2217(\u03bb). Because \u03b8\u2217(\u03bb) admits a closed form solution with \u03bb \u2265 \u03bbmax by Theorem 20, we focus on the cases with \u03bb < \u03bbmax.\nTheorem 21. For the nonnegative Lasso problem, suppose that \u03b8\u2217(\u03bb\u0304) is known with \u03bb\u0304 \u2264 \u03bbmax. For any \u03bb \u2208 (0, \u03bb\u0304), we define\nn(\u03bb\u0304) =  y \u03bb\u0304 \u2212 \u03b8\u2217(\u03bb\u0304), if \u03bb\u0304 < \u03bb\u03b1max,\nx\u2217, if \u03bb\u0304 = \u03bbmax, where x\u2217 = argmaxxi \u3008xi,y\u3009,\nv(\u03bb, \u03bb\u0304) = y\n\u03bb \u2212 \u03b8\u2217(\u03bb\u0304),\nv(\u03bb, \u03bb\u0304)\u22a5 = v(\u03bb, \u03bb\u0304)\u2212 \u3008v(\u03bb, \u03bb\u0304),n(\u03bb\u0304)\u3009 \u2016n(\u03bb\u0304)\u20162 n(\u03bb\u0304).\nThen, the following hold:\n(i) n(\u03bb\u0304) \u2208 NF (\u03b8\u2217(\u03bb\u0304)),\n(ii) \u2225\u2225\u2225\u2225\u03b8\u2217(\u03bb)\u2212 (\u03b8\u2217(\u03bb\u0304) + 12v\u22a5(\u03bb, \u03bb\u0304) )\u2225\u2225\u2225\u2225 \u2264 12\u2016v\u22a5(\u03bb, \u03bb\u0304)\u2016.\nProof. We only show that n(\u03bbmax) \u2208 NF (\u03b8\u2217(\u03bbmax)) since the proof of the other statement is very similar to that of Theorem 12.\nBy Proposition 11 and Theorem 20, it suffices to show that\n\u3008x\u2217, \u03b8 \u2212 y/\u03bbmax\u3009 \u2264 0, \u2200 \u03b8 \u2208 F . (87)\nBecause \u03b8 \u2208 F , we have \u3008x\u2217, \u03b8\u3009 \u2264 1. The definition of x\u2217 implies that \u3008x\u2217,y/\u03bbmax\u3009 = 1. Thus, the inequality in (87) holds, which completes the proof.\nTheorem 21 implies that \u03b8\u2217(\u03bb) is in a ball\u2014denoted by B(\u03bb, \u03bb\u0304)\u2014of radius 12\u2016v \u22a5(\u03bb, \u03bb\u0304)\u2016 centered\nat \u03b8\u2217(\u03bb\u0304) + 12v \u22a5(\u03bb, \u03bb\u0304). Simple calculations lead to\n\u03c9i = sup \u03b8\u2208B(\u03bb,\u03bb\u0304)\n\u3008xi, \u03b8\u3009 = \u2329 xi, \u03b8 \u2217(\u03bb\u0304) + 1\n2 v\u22a5(\u03bb, \u03bb\u0304)\n\u232a + 1\n2 \u2016v\u22a5(\u03bb, \u03bb\u0304)\u2016\u2016xi\u2016. (88)\nBy plugging \u03c9i into (R3 \u2217), we have the DPC screening rule for nonnegative Lasso as follows.\nTheorem 22. For the nonnegative Lasso problem, suppose that we are given a sequence of parameter values \u03bbmax = \u03bb (0) > \u03bb(1) > . . . > \u03bb(J ). Then, [\u03b2\u2217(\u03bb(j+1))]i = 0 if \u03b2 \u2217(\u03bb(j)) is known and the following holds:\u2329 xi,\ny \u2212X\u03b2\u2217(\u03bb(j)) \u03bb(j) + 1 2 v\u22a5(\u03bb(j+1), \u03bb(j))\n\u232a + 1\n2 \u2016v\u22a5(\u03bb(j+1), \u03bb(j))\u2016\u2016xi\u2016 < 1. (89)"}, {"heading": "6 Experiments", "text": "We evaluate TLFre for SGL and DPC for nonnegative Lasso in Sections 6.1 and 6.2, respectively, on both synthetic and real data sets. To the best of knowledge, the TLFre and DPC are the first screening methods for SGL and nonnegative Lasso, respectively."}, {"heading": "6.1 TLFre for SGL", "text": "We perform experiments to evaluate TLFre on synthetic and real data sets in Sections 6.1.1 and 6.1.2, respectively. To measure the performance of TLFre, we compute the rejection ratios of (L1) and (L2), respectively. Specifically, let m be the number of features that have 0 coefficients in the solution, G be the index set of groups that are discarded by (L1) and p be the number of inactive features that are detected by (L2). The rejection ratios of (L1) and (L2) are defined by r1 = \u2211 g\u2208G ng m and r2 = |p| m , respectively. Moreover, we report the speedup gained by TLFre, i.e., the ratio of the running time of solver without screening to the running time of solver with TLFre. The solver used in this paper is from SLEP [12].\nTo determine appropriate values of \u03b1 and \u03bb by cross validation or stability selection, we can run TLFre with as many parameter values as we need. Given a data set, for illustrative purposes only, we select seven values of \u03b1 from {tan(\u03c8) : \u03c8 = 5\u25e6, 15\u25e6, 30\u25e6, 45\u25e6, 60\u25e6, 75\u25e6, 85\u25e6}. Then, for each value of \u03b1, we run TLFre along a sequence of 100 values of \u03bb equally spaced on the logarithmic scale of \u03bb/\u03bb\u03b1max from 1 to 0.01. Thus, 700 pairs of parameter values of (\u03bb, \u03b1) are sampled in total."}, {"heading": "6.1.1 Simulation Studies", "text": "We perform experiments on two synthetic data sets that are commonly used in the literature [26, 36]. The true model is y = X\u03b2\u2217 + 0.01 , \u223c N(0, 1). We generate two data sets with 250 \u00d7 10000 entries: Synthetic 1 and Synthetic 2. We randomly break the 10000 features into 1000 groups. For Synthetic 1, the entries of the data matrix X are i.i.d. standard Gaussian with pairwise correlation zero, i.e., corr(xi,xi) = 0. For Synthetic 2, the entries of the data matrix X are drawn from i.i.d. standard Gaussian with pairwise correlation 0.5|i\u2212j|, i.e., corr(xi,xj) = 0.5\n|i\u2212j|. To construct \u03b2\u2217, we first randomly select \u03b31 percent of groups. Then, for each selected group, we randomly select \u03b32 percent of features. The selected components of \u03b2\n\u2217 are populated from a standard Gaussian and the remaining ones are set to 0. We set \u03b31 = \u03b32 = 10 for Synthetic 1 and \u03b31 = \u03b32 = 20 for Synthetic 2.\nThe figures in the upper left corner of Fig. 1 and Fig. 2 show the plots of \u03bbmax1 (\u03bb2) (see Corollary 10) and the sampled parameter values of \u03bb and \u03b1 (recall that \u03bb1 = \u03b1\u03bb and \u03bb2 = \u03bb). For the other figures, the blue and red regions represent the rejection ratios of (L1) and (L2), respectively. We\ncan see that TLFre is very effective in discarding inactive groups/features; that is, more than 90% of inactive features can be detected. Moreover, we can observe that the first layer screening (L1) becomes more effective with a larger \u03b1. Intuitively, this is because the group Lasso penalty plays a more important role in enforcing the sparsity with a larger value of \u03b1 (recall that \u03bb1 = \u03b1\u03bb). The top and middle parts of Table 1 indicate that the speedup gained by TLFre is very significant (up to 30 times) and TLFre is very efficient. Compared to the running time of the solver without screening, the running time of TLFre is negligible. The running time of TLFre includes that of computing \u2016Xg\u20162, g = 1, . . . , G, which can be efficiently computed by the power method [8]. Indeed, this can be shared for TLFre with different parameter values."}, {"heading": "6.1.2 Experiments on Real Data Set", "text": "We perform experiments on the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) data set (http://adni.loni.usc.edu/). The data matrix consists of 747 samples with 426040 single nucleotide polymorphisms (SNPs), which are divided into 94765 groups. The response vectors are the grey matter volume (GMV) and white matter volume (WMV), respectively.\nThe figures in the upper left corner of Fig. 3 and Fig. 4 show the plots of \u03bbmax1 (\u03bb2) (see Corollary 10) and the sampled parameter values of \u03b1 and \u03bb. The other figures present the rejection ratios of (L1) and (L2) by blue and red regions, respectively. We can see that almost all of the inactive groups/features are discarded by TLFre. The rejection ratios of r1 + r2 are very close to 1 in all cases. Table 2 shows that TLFre leads to a very significant speedup (about 80 times). In other words, the solver without screening needs about eight and a half hours to solve the 100 SGL problems for each value of \u03b1. However, combined with TLFre, the solver needs only six to eight minutes. Moreover, we can observe that the computational cost of TLFre is negligible compared to that of the solver without screening. This demonstrates the efficiency of TLFre."}, {"heading": "6.2 DPC for Nonnegative Lasso", "text": "In this experiment, we evaluate the performance of DPC on two synthetic data sets and six real data sets. We integrate DPC with the solver [12] to solve the nonnegative Lasso problem along a sequence of 100 parameter values of \u03bb equally spaced on the logarithmic scale of \u03bb/\u03bbmax from 1.0 to 0.01. The two synthetic data sets are the same as the ones we used in Section 6.1.1. To construct \u03b2\u2217, we first randomly select 10 percent of features. The corresponding components of \u03b2\u2217\nare populated from a standard Gaussian and the remaining ones are set to 0. We list the six real data sets and the corresponding experimental settings as follows.\na) Breast Cancer data set [32, 21]: this data set contains 7129 gene expression values of 44 tumor samples (thus the data matrix X is of 44 \u00d7 7129). The response vector y \u2208 {1,\u22121}44 contains the binary label of each sample.\nb) Leukemia data set [1]: this data set contains 11225 gene expression values of 52 samples (X \u2208 R52\u00d711225). The response vector y contains the binary label of each sample.\nc) Prostate Cancer data set [19]: this data set contains 15154 measurements of 132 patients (X \u2208 R132\u00d715154). By protein mass spectrometry, the features are indexed by time-of-flight values, which are related to the mass over charge ratios of the constituent proteins in the blood. The response vector y contains the binary label of each sample.\nd) PIE face image data set [22, 5]: this data set contains 11554 gray face images (each has 32\u00d7 32 pixels) of 68 people, taken under different poses, illumination conditions and expressions. In each trial, we first randomly pick an image as the response y \u2208 R1024, and then use the remaining images to form the data matrix X \u2208 R1024\u00d711553. We run 100 trials and report the average performance of DPC.\ne) MNIST handwritten digit data set [11]: this data set contains grey images of scanned handwritten digits (each has 28 \u00d7 28 pixels). The training and test sets contain 60, 000 and 10, 000 images, respectively. We first randomly select 5000 images for each digit from the training set and get a data matrix X \u2208 R784\u00d750000. Then, in each trial, we randomly select an image from the testing set as the response y \u2208 R784. We run 100 trials and report the average performance of the screening rules.\nf) Street View House Number (SVHN) data set [15]: this data set contains color images of street view house numbers (each has 32 \u00d7 32 pixels), including 73257 images for training and 26032 for testing. In each trial, we first randomly select an image as the response y \u2208 R3072, and then use the remaining ones to form the data matrix X \u2208 R3072\u00d799288. We run 20 trials and report the average performance.\nWe present the rejection ratios\u2014the ratio of the number of inactive features identified by DPC to the actual number of inactive features\u2014in Fig. 5. We also report the running time of the solver with and without DPC, the time for running DPC, and the corresponding speedup in Table 3.\nFig. 5 shows that DPC is very effective in identifying the inactive features even for small parameter values: the rejection ratios are very close to 100% for the entire sequence of parameter values on the eight data sets. Table 3 shows that DPC leads to a very significant speedup on all the data sets. Take MNIST as an example. The solver without DPC takes 50 minutes to solve the 100 nonnegative Lasso problems. However, combined with DPC, the solver only needs 10 seconds. The speedup gained by DPC on the MNIST data set is thus more than 300 times. Similarly, on the SVHN data set, the running time for solving the 100 nonnegative Lasso problems by the solver without DPC is close to seven hours. However, combined with DPC, the solver takes less than two minutes to solve all the 100 nonnegative Lasso problems, leading to a speedup about 230 times. Moreover, we can also observe that the computational cost of DPC is very low\u2014which is negligible compared to that of the solver without DPC."}, {"heading": "7 Conclusion", "text": "In this paper, we propose a novel feature reduction method for SGL via decomposition of convex sets. We also derive the set of parameter values that lead to zero solutions of SGL. To the best of our knowledge, TLFre is the first method which is applicable to sparse models with multiple\nsparsity-inducing regularizers. More importantly, the proposed approach provides novel framework for developing screening methods for complex sparse models with multiple sparsity-inducing regularizers, e.g., `1 SVM that performs both sample and feature selection, fused Lasso and tree Lasso with more than two regularizers. To demonstrate the flexibility of the proposed framework, we develop the DPC screening rule for the nonnegative Lasso problem. Experiments on both synthetic and real data sets demonstrate the effectiveness and efficiency of TLFre and DPC. We plan to generalize the idea of TLFre to `1 SVM, fused Lasso and tree Lasso, which are expected to consist of multiple layers of screening."}, {"heading": "A Sparse-Group Lasso", "text": "A.1 The Lagrangian Dual Problem of SGL\nWe derive the dual problem of SGL in (4) via the Lagrangian multiplier method. By introducing an auxiliary variable\nz = y \u2212 G\u2211 g=1 Xg\u03b2g, (90)\nthe SGL problem in (3) becomes:\nmin \u03b2 12\u2016z\u20162 + \u03b1\u03bb G\u2211 g=1 \u221a ng\u2016\u03b2g\u2016+ \u03bb\u2016\u03b2\u20161 : z = y \u2212 G\u2211 g=1 Xg\u03b2g  . Let \u03bb\u03b8 be the Lagrangian multiplier, the Lagrangian function is\nL(\u03b2, z; \u03b8) = 1\n2 \u2016z\u20162 + \u03b1\u03bb G\u2211 g=1 \u221a ng\u2016\u03b2g\u2016+ \u03bb\u2016\u03b2\u20161 + \u3008\u03bb\u03b8,y \u2212 G\u2211 g=1 Xg\u03b2g \u2212 z\u3009 (91)\n=\u03b1\u03bb G\u2211 g=1 \u221a ng\u2016\u03b2g\u2016+ \u03bb\u2016\u03b2\u20161 \u2212 \u03bb\u3008\u03b8, G\u2211 g=1 Xg\u03b2g\u3009+ 1 2 \u2016z\u20162 \u2212 \u03bb\u3008\u03b8, z\u3009+ \u03bb\u3008\u03b8,y\u3009. (92)\nLet\nf1(\u03b2) = G\u2211 g=1 fg1 (\u03b2g) = G\u2211 g=1 ( \u03b1\u03bb \u221a ng\u2016\u03b2g\u2016+ \u03bb\u2016\u03b2g\u20161 \u2212 \u03bb\u3008\u03b8,Xg\u03b2g\u3009 ) , f2(z) = 1\n2 \u2016z\u20162 \u2212 \u03bb\u3008\u03b8, z\u3009.\nTo derive the dual problem, we need to minimize the Lagrangian function with respect to \u03b2 and z. In other words, we need to minimize f1 and f2, respectively. We first consider\nmin \u03b2g\nfg1 (\u03b2g) = \u03b1\u03bb \u221a ng\u2016\u03b2g\u2016+ \u03bb\u2016\u03b2\u20161 \u2212 \u03bb\u3008\u03b8,Xg\u03b2g\u3009.\nBy the Fermat\u2019s rule, we have\n0 \u2208 \u2202fg1 (\u03b2g) = \u03b1\u03bb \u221a ng\u2202\u2016\u03b2g\u2016+ \u03bb\u2202\u2016\u03b2g\u20161 \u2212 \u03bbXTg \u03b8, (93)\nwhich leads to\nXTg \u03b8 = \u03b1 \u221a ng\u03b61 + \u03b62, \u03b61 \u2208 \u2202\u2016\u03b2g\u2016, \u03b62 \u2208 \u2202\u2016\u03b2g\u20161. (94)\nBy noting that\n\u3008\u03b61, \u03b2g\u3009 = \u2016\u03b2g\u2016, \u3008\u03b62, \u03b2g\u3009 = \u2016\u03b2g\u20161,\nwe have\n\u3008XTg \u03b8, \u03b2g\u3009 = \u03b1 \u221a ng\u2202\u2016\u03b2g\u2016+ \u2202\u2016\u03b2g\u20161.\nThus, we can see that\n0 = min \u03b2g\nfg1 (\u03b2g). (95)\nMoreover, because \u03b61 \u2208 \u2202\u2016\u03b2g\u2016, \u03b62 \u2208 \u2202\u2016\u03b2g\u20161, Eq. (94) implies that\nXTg \u03b8 \u2208 \u03b1 \u221a ngB + B\u221e. (96)\nTo minimize f2, the Fermat\u2019s rule results in\nz = \u03bb\u03b8, (97)\nand thus\n\u2212\u03bb 2\n2 \u2016\u03b8\u20162 = min z f2(z). (98)\nIn view of Eq. (91), Eq. (95), Eq. (98) and Eq. (96), the dual problem of SGL can be written as\nsup \u03b8\n{ 1\n2 \u2016y\u20162 \u2212 1 2 \u2225\u2225\u2225\u03b8 \u2212 y \u03bb \u2225\u2225\u22252 : XTg \u03b8 \u2208 \u03b1\u221angB + B\u221e, g = 1, . . . , G} , which is equivalent to (4).\nRecall that \u03b2\u2217(\u03bb, \u03b1) and \u03b8\u2217(\u03bb, \u03b1) are the primal and dual optimal solutions of SGL, respectively. By Eq. (90), Eq. (93) and Eq. (97), we can see that the KKT conditions are\n\u03bb\u03b8\u2217(\u03bb, \u03b1) =y \u2212X\u03b2\u2217(\u03bb, \u03b1), XTg \u03b8 \u2217(\u03bb, \u03b1) \u2208\u03b1\u221ang\u2202\u2016\u03b2\u2217g (\u03bb, \u03b1)\u2016+ \u2202\u2016\u03b2\u2217g (\u03bb, \u03b1)\u20161, g = 1, . . . , G."}], "references": [{"title": "MLL translocations specify a distinct gene expression profile that distinguishes a unique leukemia", "author": ["S. Armstrong", "J. Staunton", "L. Silverman", "R. Pieters", "M. den Boer", "M. Minden", "S. Sallan", "E. Lander", "T. Golub", "S. Korsmeyer"], "venue": "Nature Genetics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces", "author": ["H.H. Bauschke", "P.L. Combettes"], "venue": "Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex Analysis and Nonlinear Optimization, Second Edition", "author": ["J. Borwein", "A. Lewis"], "venue": "Canadian Mathematical Society,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient kernel discriminant analysis via spectral regression", "author": ["D. Cai", "X. He", "J. Han"], "venue": "ICDM,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Safe feature elimination in sparse supervised learning", "author": ["L. El Ghaoui", "V. Viallon", "T. Rabbani"], "venue": "Pacific Journal of Optimization, 8:667\u2013698,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P. Martinsson", "J. Tropp"], "venue": "SIAM Review, 53:217\u2013288,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "From convex optimization to nonconvex optimization", "author": ["J.-B. Hiriart-Urruty"], "venue": "necessary and sufficient conditions for global optimality. In Nonsmooth optimization and related topics. Springer,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1988}, {"title": "A note on the Legendre-Fenchel transform of convex composite functions", "author": ["J.-B. Hiriart-Urruty"], "venue": "Nonsmooth Mechanics and Analysis. Springer,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "SLEP: Sparse Learning with Efficient Projections", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Moreau-Yosida regularization for grouped tree structure learning", "author": ["J. Liu", "J. Ye"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Safe screening with variational inequalities and its application to lasso", "author": ["J. Liu", "Z. Zhao", "J. Wang", "J. Ye"], "venue": "International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Reading digits in nature images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A. Ng"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Safe sample screening for Support Vector Machine", "author": ["K. Ogawa", "Y. Suzuki", "S. Suzumura", "I. Takeuchi"], "venue": "arXiv:1401.6740,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Safe screening of non-support vectors in pathwise SVM computation", "author": ["K. Ogawa", "Y. Suzuki", "I. Takeuchi"], "venue": "ICML,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Regularized multivariate regression for indentifying master predictors with application to integrative genomics study of breast cancer", "author": ["J. Peng", "J. Zhu", "A. Bergamaschi", "W. Han", "D. Noh", "J. Pollack", "P. Wang"], "venue": "The Annals of Appliced Statistics, 4:53\u201377,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Serum proteomic patterns for detection of prostate cancer", "author": ["E. Petricoin", "D. Ornstein", "C. Paweletz", "A. Ardekani", "P. Hackett", "B. Hitt", "A. Velassco", "C. Trucco", "L. Wiegand", "K. Wood", "C. Simone", "P. Levine", "W. Linehan", "M. Emmert-Buck", "S. Steinberg", "E. Kohn", "L. Liotta"], "venue": "Journal of National Cancer Institute, 94:1576\u20131578,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonlinear Optimization", "author": ["A. Ruszczy\u0144ski"], "venue": "Princeton University Press,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "A simple and efficient algorithm for gene selection using sparse logistic regression", "author": ["S. Shevade", "S. Keerthi"], "venue": "Bioinformatics, 19:2246\u20132253,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "The CMU pose, illumination, and expression database", "author": ["T. Sim", "B. Baker", "M. Bsat"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 25:1615\u20131618,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "A Sparse-Group Lasso", "author": ["N. Simon", "J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "C-HiLasso: a collaborative hierarchical sparse modeling framework", "author": ["P. Sprechmann", "I. Ram\u0131\u0301rez", "G. Sapiro", "Y. Eldar"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Regression shringkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B, 58:267\u2013288,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Strong rules for discarding predictors in lasso-type problems", "author": ["R. Tibshirani", "J. Bien", "J. Friedman", "T. Hastie", "N. Simon", "J. Taylor", "R. Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B, 74:245\u2013266,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Machine learning methods in the cocomputation biology of cancer", "author": ["M. Vidyasagar"], "venue": "Proceedings of the Royal Society A,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Sparse group lasso and high dimensional multinomial classification", "author": ["M. Vincent", "N. Hansen"], "venue": "Computational Statistics and Data Analysis, 71:771\u2013786,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Scaling svm and least absolute deviations via exact data reduction", "author": ["J. Wang", "P. Wonka", "J. Ye"], "venue": "International Conference on Machine Learning,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Lasso screening rules via dual polytope projection", "author": ["J. Wang", "J. Zhou", "P. Wonka", "J. Ye"], "venue": "Advances in neural information processing systems,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting the clinical status of human breast cancer by using gene expression profiles", "author": ["M. West", "C. Blanchette", "H. Dressman", "E. Huang", "S. Ishida", "R. Spang", "H. Zuzan", "J. Olson", "J. Marks", "J. Nevins"], "venue": "Proceedings of the National Academy of Sciences, 98:11462\u201311467,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast lasso screening tests based on correlations", "author": ["Z.J. Xiang", "P.J. Ramadge"], "venue": "IEEE ICASSP,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Linguistic structured sparsity in text categorization", "author": ["D. Yogatama", "N. Smith"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society Series B, 68:49\u201367,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society Series B, 67:301\u2013320,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 21, "context": "Sparse-Group Lasso (SGL) [7, 23] is a powerful regression technique in identifying important groups and features simultaneously.", "startOffset": 25, "endOffset": 32}, {"referenceID": 23, "context": "To yield sparsity at both group and individual feature levels, SGL combines the Lasso [25] and group Lasso [35] penalties.", "startOffset": 86, "endOffset": 90}, {"referenceID": 32, "context": "To yield sparsity at both group and individual feature levels, SGL combines the Lasso [25] and group Lasso [35] penalties.", "startOffset": 107, "endOffset": 111}, {"referenceID": 25, "context": "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.", "startOffset": 124, "endOffset": 132}, {"referenceID": 31, "context": "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.", "startOffset": 124, "endOffset": 132}, {"referenceID": 22, "context": "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.", "startOffset": 152, "endOffset": 156}, {"referenceID": 16, "context": "In recent years, SGL has found great success in a wide range of applications, including but not limited to machine learning [27, 34], signal processing [24], bioinformatics [18] etc.", "startOffset": 173, "endOffset": 177}, {"referenceID": 21, "context": "Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28].", "startOffset": 80, "endOffset": 95}, {"referenceID": 11, "context": "Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28].", "startOffset": 80, "endOffset": 95}, {"referenceID": 26, "context": "Many research efforts have been devoted to developing efficient solvers for SGL [7, 23, 13, 28].", "startOffset": 80, "endOffset": 95}, {"referenceID": 5, "context": "[6] proposed a promising feature reduction method, called SAFE screening, to screen out the so-called inactive features, which have zero coefficients in the solution, from the optimization.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 128, "endOffset": 144}, {"referenceID": 12, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 128, "endOffset": 144}, {"referenceID": 24, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 128, "endOffset": 144}, {"referenceID": 30, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 128, "endOffset": 144}, {"referenceID": 28, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 158, "endOffset": 170}, {"referenceID": 24, "context": "Inspired by SAFE, various exact and heuristic feature screening methods have been proposed for many sparse models such as Lasso [31, 14, 26, 33], group Lasso [31, 29, 26], etc.", "startOffset": 158, "endOffset": 170}, {"referenceID": 5, "context": "SAFE [6], DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution.", "startOffset": 5, "endOffset": 8}, {"referenceID": 30, "context": "SAFE [6], DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution.", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "SAFE [6], DOME [33] and EDPP [31] are guaranteed to have zero coefficients in the solution.", "startOffset": 29, "endOffset": 33}, {"referenceID": 24, "context": "However, heuristic feature screening methods like Strong Rule [26] may mistakenly discard features which have nonzero coefficients in the solution.", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30].", "startOffset": 144, "endOffset": 152}, {"referenceID": 27, "context": "More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30].", "startOffset": 144, "endOffset": 152}, {"referenceID": 27, "context": "More recently, the idea of exact feature screening has been extended to exact sample screening, which screens out the nonsupport vectors in SVM [17, 30] and LAD [30].", "startOffset": 161, "endOffset": 165}, {"referenceID": 14, "context": "As a promising data reduction tool, exact feature/sample screening would be of great practical importance because they can effectively reduce the data size without sacrificing the optimality [16].", "startOffset": 191, "endOffset": 195}, {"referenceID": 3, "context": "By the Lagrangian multipliers method [4] (see the supplement), the dual problem of SGL is", "startOffset": 37, "endOffset": 40}, {"referenceID": 27, "context": "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 12, "context": "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 28, "context": "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 5, "context": "Surprisingly, the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity-inducing regularizer [30, 14, 31, 6].", "startOffset": 201, "endOffset": 216}, {"referenceID": 2, "context": "5 in [3].", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "[2] Let h, g \u2208 \u03930(R).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Let f1, \u00b7 \u00b7 \u00b7 , fk \u2208 \u03930(R).", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[Fenchel-Young inequality] [3] Any point z \u2208 Rn and w in the domain of a function h : Rn \u2192 (\u2212\u221e,\u221e] satisfy the inequality h(w) + h\u2217(z) \u2265 \u3008w, z\u3009.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "(15) are the so-called KKT conditions [4] and can also be obtained by the Lagrangian multiplier method (see A.", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "[2] Let C1 and C2 be nonempty subsets of Rn.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Inspired by the SAFE rules [6], we can first estimate a region \u0398 containing \u03b8\u2217(\u03bb, \u03b1).", "startOffset": 27, "endOffset": 30}, {"referenceID": 18, "context": "1, we give an accurate estimation of \u03b8\u2217(\u03bb, \u03b1) via normal cones [20].", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": ", \u03b8\u2217(\u03bb, \u03b1) = PF\u03b1(y/\u03bb), we have a very useful characterization of the dual optimal solution via the so-called normal cones [20].", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "[20, 2] For a closed convex set C \u2208 Rn and a point w \u2208 C, the normal cone to C at w is defined by NC(w) = {v : \u3008v,w\u2032 \u2212w\u3009 \u2264 0, \u2200w\u2032 \u2208 C}.", "startOffset": 0, "endOffset": 7}, {"referenceID": 1, "context": "[20, 2] For a closed convex set C \u2208 Rn and a point w \u2208 C, the normal cone to C at w is defined by NC(w) = {v : \u3008v,w\u2032 \u2212w\u3009 \u2264 0, \u2200w\u2032 \u2208 C}.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[9] Suppose that h \u2208 \u03930 and C is a nonempty closed convex set.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "The solver used in this paper is from SLEP [12].", "startOffset": 43, "endOffset": 47}, {"referenceID": 24, "context": "1 Simulation Studies We perform experiments on two synthetic data sets that are commonly used in the literature [26, 36].", "startOffset": 112, "endOffset": 120}, {"referenceID": 33, "context": "1 Simulation Studies We perform experiments on two synthetic data sets that are commonly used in the literature [26, 36].", "startOffset": 112, "endOffset": 120}, {"referenceID": 10, "context": "01 by (a): the solver [12] without screening; (b): the solver combined with TLFre.", "startOffset": 22, "endOffset": 26}, {"referenceID": 6, "context": ", G, which can be efficiently computed by the power method [8].", "startOffset": 59, "endOffset": 62}, {"referenceID": 10, "context": "We integrate DPC with the solver [12] to solve the nonnegative Lasso problem along a sequence of 100 parameter values of \u03bb equally spaced on the logarithmic scale of \u03bb/\u03bbmax from 1.", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "01 by (a): the solver [12] without screening; (b): the solver combined with TLFre.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": "a) Breast Cancer data set [32, 21]: this data set contains 7129 gene expression values of 44 tumor samples (thus the data matrix X is of 44 \u00d7 7129).", "startOffset": 26, "endOffset": 34}, {"referenceID": 19, "context": "a) Breast Cancer data set [32, 21]: this data set contains 7129 gene expression values of 44 tumor samples (thus the data matrix X is of 44 \u00d7 7129).", "startOffset": 26, "endOffset": 34}, {"referenceID": 0, "context": "b) Leukemia data set [1]: this data set contains 11225 gene expression values of 52 samples (X \u2208 R52\u00d711225).", "startOffset": 21, "endOffset": 24}, {"referenceID": 17, "context": "c) Prostate Cancer data set [19]: this data set contains 15154 measurements of 132 patients (X \u2208 R132\u00d715154).", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "d) PIE face image data set [22, 5]: this data set contains 11554 gray face images (each has 32\u00d7 32 pixels) of 68 people, taken under different poses, illumination conditions and expressions.", "startOffset": 27, "endOffset": 34}, {"referenceID": 4, "context": "d) PIE face image data set [22, 5]: this data set contains 11554 gray face images (each has 32\u00d7 32 pixels) of 68 people, taken under different poses, illumination conditions and expressions.", "startOffset": 27, "endOffset": 34}, {"referenceID": 9, "context": "e) MNIST handwritten digit data set [11]: this data set contains grey images of scanned handwritten digits (each has 28 \u00d7 28 pixels).", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "f) Street View House Number (SVHN) data set [15]: this data set contains color images of street view house numbers (each has 32 \u00d7 32 pixels), including 73257 images for training and 26032 for testing.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "01 by (a): the solver [12] without screening; (b): the solver combined with DPC.", "startOffset": 22, "endOffset": 26}], "year": 2014, "abstractText": "Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the `1 and `2 norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. We also develop a screening method\u2014called DPC (decomposition of convex set)\u2014for the nonnegative Lasso problem. Experiments on both synthetic and real data sets show that TLFre and DPC improve the efficiency of SGL and nonnegative Lasso by several orders of magnitude.", "creator": "LaTeX with hyperref package"}}}