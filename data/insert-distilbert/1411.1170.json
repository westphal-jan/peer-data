{"id": "1411.1170", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Nov-2014", "title": "An Intelligent Personal Robot Assistant", "abstract": "somewhat recent development in developing humanoid model robot poses new challenges to human - machine interaction communication. quite a major challenge is to develop robots that can behave like and indeed interact with human in the, most natural intelligence way possible. this paper proposes a system to develop a robot that can receive command, and talk to people in natural computer language. in addition, the robot can also be \" trained \" to become an expert in sepcific areas to provide expert advice to human - beings. most likely important of all, the robot can display emotions through facial expression, speech and gesture so that \" the rational interaction process will sometimes become more comprehensive and compelling.", "histories": [["v1", "Wed, 5 Nov 2014 07:24:59 GMT  (109kb)", "http://arxiv.org/abs/1411.1170v1", null], ["v2", "Tue, 2 Dec 2014 01:13:31 GMT  (0kb,I)", "http://arxiv.org/abs/1411.1170v2", "This paper has been withdrawn by the author due to a crucial sign error"]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.HC", "authors": ["ong sing goh", "lance fung"], "accepted": false, "id": "1411.1170"}, "pdf": {"name": "1411.1170.pdf", "metadata": {"source": "CRF", "title": "AN INTELLIGENT PERSONAL ROBOT ASSISTANT", "authors": ["Ong Sing Goh"], "emails": [], "sections": [{"heading": null, "text": "number of recognized phrases of this robot is very limited, only 100 to 650 phrases, and speech phrases about 300 to 3000. This is an impediment factor in natural language processing field. Human tend to give command in different ways. If human are forced to give command using only limited phrases, then the interaction will not be natural. Moreover, the Papero robot is not able to do jobs for human, so it can merely be treated as an entertainment robot, and not a robot that can act as an intelligent assistant. Morpha projects has also developed a personal robot assistant that can do job, but unfortunately it cannot speak [15]. Therefore, it will perform poorly whenever its capabilities are ill-suited for the tasks at hand. Besides that, if the robot has a problem, it has no way to ask for assistance. Therefore, the robot is not able to accomplish more meaningful work for human and achieve better results. To address these problems, we propose a system to add natural language processing capability to the robot, so that it can communicate with human by using natural language. We plan to add intelligence to the robot so that it can \u201c think\u201d and have the ability to solve trivial daily tasks. It is widely accepted that robots expressing emotions are important to make the interaction with them more enjoyable and compelling. In this research we will develop an emotion controller so that the robot can show emotions and expressions when it talk to people. This proposed system is aimed to improve the interaction between human and robots. As the focus of the information technology industry is moving away from hardware and basic software towards human-friendly computing, we hope to build a personal robot assistant that can not only help people in their jobs, but can also act and perform like an \u201c artificial human\u201d using navigation robot called Pioneer 2-AT (please refer Section 3.2) that has autonomous navigation ability. The knowledge and intelligence of the robot using Artificial Intelligent Neural-Network Identity (please refer Section 3.3), which can can move around and communicate with people. We hope to achieve a break-through in human-robot interaction by developing a robot that exhibits humanoid behavior and is intelligent enough to collaborate with human.\nThis research project was funded by Universiti Teknikal Malaysia Melaka Research Funding (PJP/2003/FTMK(3)) and Theoretical and Development Framework for Intelligent Conversation Agents (ICA) funded by Murdoch Universiti Research Excellence Grant Scheme\n(REGS) 2008, Murdoch University, Western Australia"}, {"heading": "2. SYSTEM DESIGN", "text": ""}, {"heading": "3.1. Architecture", "text": "This project emphasizes on the human-machine interaction area and intends to improve the current personal robot assistant by adding natural language processing capability to it. We are using a natural language processing system called AINI (Artificial Intelligent Neural-network Identity) [6, 7, 8]. The chat robot has been tested and currently being used in Multimedia University website and has participated in the MSC-Expo 2001, MSC-Asia Pacific Information and Technology Award (MSC-APICTA) and Multimedia Super Corridor Nationwide conference 2002. The overall concept of this system can be depicted by Figure 1. There are three ways human can interact with the robot: by speech (natural language), gesture (vision) and wireless device. Human can give command to the robot by using natural language. This is the most natural way of interaction because human tend to use language to give command or receive instructions. Apart from interaction by speech, human can also instruct the robot through gesture. For example, they can wave their hand towards the robot to indicate the robot to come near to them. This kind of interaction involves some image processing steps to identify human gesture. For wireless interaction, human can use Bluetooth technology to control the robot remotely. With Bluetooth technology attached to the robot, the interaction process can become uninterrupted and more convenient, as the problems with wires and cables can be eliminated. Most important of all, the robot can be controlled remotely in hazardous situation to replace human-being. Our main focus in this project is on interaction with the robot by speech. Human gives command to the robot by using speech (natural language). The speech is captured by a voice recognition technology, processed and passed to AINI, the natural language processing unit in this system. AINI will analyze the speech and produce a proper respond. AINI contains an emotion recognition system that identifies the emotions in the speech and selects an expression that is able to reflect the context of the speech. With a speech synthesis and expression controller, the robot is able to speak out the respond and display the corresponding expression while it talks. The other two interaction methods (through gesture and wireless device) will only be developed when the speech processing system is completed.\nThe important components in this system include the physical robot, AINI the intelligent system, speech recognition system and also the emotion controller. Each of them will be discussed in details in Section 3.2 to 3.5."}, {"heading": "3.2. The Robot", "text": "The robot is sponsored by CIM \u2013 the Centrum f\u00fcr international Migration und Entwicklung [3]. It is an allterrain robot of type Pioneer 2-AT from ActivRobotics. It is equipped with differential steering, sonar sensor, laser scanner and a color camera with pan-tilt-zoom system, but lack of robot arm, gripper and electronic devices/sensors for the interface. A picture of this robot is shown by Figure 2.\nThis navigation robot is powerful, ease to use; reliable, yet flexible. It has Ethernet-based communications, laser, DGPS, and other autonomous functions. In addition it has powerful motors and four monster wheels that can reach speeds of point eight meters per second and is able to carry a payload of up to thirty kilogram. This feature enables the robot to help its master to carry many things. As the robot is made for use outdoors, it can follow its master to run on many earth, including stone, sand or paved surfaces. This robot is very suitable to be used in highly versatile environment such as public exhibition or recreational parks. However, this robot does not possess speech recognition and natural processing capability. Thus interaction with the robot can only be made through keyboard and joystick. Our project is putting natural language processing system and artificial intelligence into the robot so that it can communicate with human in natural language and have the intelligence to perform simple tasks. Besides that, we plan to enhance the robot with wireless technology so that it can be controlled remotely and speech recognition."}, {"heading": "3.3. AINI", "text": "The Artificial Intelligent Neural-Network Identity (AINI) is a chatterbot system that uses artificial intelligence and natural language processing. AINI means \u201c I love you\u201d in Chinese, \u201c beautiful\u201d in Malay and \u201c eye\u201d in Arabic. It is based on ALICE (Artificial Linguistic Internet Computer Entity), the entertainment chatterbot created by Dr. Wallace in 1995. ALICE won the 2000 and 2001 Loebner Prize, a restricted Turing Test [13] to evaluate the level of \u201c humanity\u201d of chatterbots. Knowledge of the robot can be represented in the patterns of XML specification called AIML (Artificial Intelligent Markup Language). An example of the pattern looks like below: <aiml> <category> <pattern> Hello </pattern> <template>Hi, how are U</template> </category> </aiml>\nKnowledge of specific field can be added so that the robot can acts as an expert to give advice to human-beings. A preprocessor (including the pattern matcher) is used to identify the content of the speech. It processes the user input, interprets meaning using Natural Language Processing (NLP) and generate appropriate responses. The goal of NLP is to analyze, understand, and generate languages that human use naturally, so that eventually the robot can be addressed as a \u201c human\u201d . A semantic/discourse controller will be added to the preprocessor to implement high-level decision-making processes to influence the flow of conversation with the user. It uses a combination of methods derived from the field of discourse analysis and Artificial Intelligence (AI) models to behavior and macroscopic meaning in order to steer the dialog as appropriate."}, {"heading": "3.4. Speech Recognition", "text": "Speech recognition is one of the input components for this project. It emphasizes on capturing spoken words from human and converts the speech into text for NLP processing. Dragon Naturally Speaking [4] is used as the main speech recognition engine for this system. It has high accuracy, fast performance and possesses extensive vocabulary which gives outstanding recognition for our system. Its powerful speech recognition capability offers true continuous speech that allows user to speak to the robot naturally at a normal pace, without pausing between words. The software also offers high recognition accuracy as it possesses large collection of vocabulary. New words can be added quickly with the revolutionary Vocabulary Builder."}, {"heading": "3.5. Emotion Recognition", "text": "Emotion plays a very important role in making humanmachine interaction more comprehensive and compelling. In the proposed system, we will build an emotion recognition system to identify the emotion of the user from the conversation between human and the robot. With this, the robot is able to display emotion according to the context of conversation, in order to make the interaction more natural and enjoyable. There are many researches going on to classify emotions through speech. They collect different utterances from large number of speakers and analyze the emotions in the speech by using various algorithms. The researches from University of Tokushima try to classify emotions by using pitch [10]. Researchers in National University of Singapore try to analyze facial expressions and speech in order to classify emotions [12]. Besides that, some researchers from Microsoft.com are also trying to classify emotions based on emotions derived from speech signal [20]. In our system, we found that the emotion recognition system through speech is not very accurate. This is because different people show variety accent and pitch\nwhen they talk. Some people do not even show changes in their voice as they do not want to reveal their internal emotional state. We are classifying emotions from the context of conversation, and not from the speech tone itself. We analyze the context of the conversation between human and the robot, capture the words that convey emotions and calculate the marginal emotions in the speech. This concept can be depicted by the Figure 3.\nSpeech of human speaker is captured and processed by the speech to text processing system. The emotion recognition engine will first analyze the context of the speech to capture the emotions found in the speech. In the example below, the keyword that shows emotion is \u201c HAPPY\u201d , with intensity \u201c very\u201d . After that, the engine will search the emotion database to find the corresponding weight for \u201c HAPPY\u201d . We will build a database which\nstores all the words that convey emotions (such as happy, pleased, glad, joyful) with varying weight. For example, the keyword happy will carry weight 8, whereas the keyword sad will carry weight -7. A good compression and searching technique will be used to store and search the large emotions vocabulary so that the storage space and searching time will be less. As speech can convey a few emotions at a time, the marginal or \u201c net\u201d emotions will be calculated. The words that show intensity, such as \u201c very\u201d , \u201c quite\u201d , \u201c almost\u201d , \u201c extremely\u201d , also carry some weight and they will also be counted. The algorithm to find emotions can be expressed as Where, M\u03be (sk) = Marginal emotion, \u03be, due to the kth state of the world W\u03bei = The weight of the ith emotion, \u03be . I\u03bei = Intensity related to the ith emotion. f(N) = Function that captures the conversational context, N. The marginal emotions obtained from this algorithm will be passed to AINI to select the appropriate expression for the emotions. Recent research has identified five main emotions in human: normal, happy, sad, afraid, and angry [20]. We have a record that keeps the marginal emotions and its corresponding expressions, for example, weight 0- 3 corresponds to expression \u201c sad\u201d , 4-7 for \u201c neutral\u201d expression and 8-10 for \u201c happy\u201d expression. We represent the six main emotions with the Microsoft agent character shown in figure six.\ni=1\nn\n[ ] / n M\u03be (sk) = \u03a3 [W\u03bei * I\u03bei ] + f (N)\nidentifying the emotions shown by the robot face in Figure 5. It is very difficult to identify whether the face is happy, sad or angry because it lacks humanoid facial parameters. Emotional behavior can be conveyed through various channels, such as facial display, speech and body movement. In the proposed system, we want to take advantage of as many of these modalities as possible in order to make our communication richer and more effective. We have already described the emotion channel through facial expression. For body movement, we are going to add a robot arm to the robot so that it can be used to display emotions through gestures. For instance, if the robot is happy, it will wave its arm; whereas when it feels angry, it will hold its gripper tightly. Besides that, to express emotions in speech, we will build a sound processor to produce different pitch and loudness in the robots speech to show feelings. For instance, the speech will be faster, higher-pitched and louder to show \u201c happiness\u201d . With this combination of various emotion expressers, we hope the robot can make the conversation between human to become more interesting and comprehensible."}, {"heading": "3. CONCLUSION", "text": ""}, {"heading": "3.1. Benefits", "text": "This project can benefit a lot of individual users and organizations. Some potential applications of the robot include: \u2022 Personal robot assistant for vision-impaired\npeople. It can be used as personal assistant to the blind people to guide their directions. Predefined route can also be programmed into the robot so that it can lead the visual-impaired safely in their path. The robot can also act as their companion to talk and understand their feeling.\n\u2022 Tour Guide Assistant. The robot can be used as a tour guide assistant in various places, for example in the university, organizations and also famous tourist hotspots. It can promote the places and give explanation to the visitors.\n\u2022 Human companion (Family Pet). The robot can act as personal assistant to individual by helping him/her to memorize important dates or appointments, arrange schedule and other jobs. Most important of all, the robot can act as a companion or family pet, as it can talk to human in natural language and understand their emotions. This can encourage people to strike up strong relation with it.\n\u2022 RoboKiosk. It can be used to provide easy access to information for diverse users. It can be located in public places to enable consumers or visitors to directly ask for information, products, and/or services. Besides that, it can also detect motion of the\npeople so that it can be alert and attract people to use it.\n\u2022 Expert advisor. It can act as an expert to give advice in specific areas to people. For examples, the robot can be trained to become a Financial Advisor in giving investment advice and also a medical advisor to provide health care information. Besides that, its advice can also be used as reference by human experts. Most important of all, its advice is non-bias, non-perishable and reproducible.\n\u2022 RoboGuard. It can be located in the home or organization to patrol and safe guard the building. It can patrol around the area and send instant message to alert the owner when something unusual happens. The owner can also control movement of the robot in the building from somewhere else by using Bluetooth technology.\n\u2022 Efficient promoter for companies. The robot can act as a salesperson to help companies advertise their products. As the robot can talk and influence people, it can help to attract more people, thus increase the advertisement power of the company. In addition, the robot can be maintained at low cost and it is nonperishable. Most important of all, its persuasive power can be improved from time to time."}, {"heading": "3.2. Limitations", "text": "Although developing a robot with natural language processing capability provides significant benefits, we recognize that there are limitations. First the current voice recognition technology is not very accurate. Second, building an effective emotion controller is not easy. This is because human speech is affected by social environment and individual background. Some words may sound polite in a society but may be offensive in the others. That is why extensive studies must be done in order to develop a successful emotion controller. Finally, if human-robot interaction is adaptive, then the flow of control and information through the system will vary with time and situation. This may make debugging, validation, and verification problematic because it becomes harder to precisely identify an error condition or to duplicate a failure situation."}], "references": [{"title": "The Role of Expressiveness and Attention in Human-Robot Interaction", "author": ["Allison Bruce", "Illah Nourbakhsh", "Reid Simmons"], "venue": "Robotics and Automation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "A Computational Architecture to Model Human Emotions", "author": ["Arun Chandra"], "venue": "Intelligent Information Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Artificial Intelligent Neural-network Identity (AINI)\u2013The Next Generation of the Virtual Advisor", "author": ["Goh Ong Sing"], "venue": "MSC\u2013Asia Pacific ICT Award (MSC-APICTA),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Intelligent Agent for E- Management\" at IEEE International Conference on Artificial Intelligent in Engineering and Technology (ICAIET 2002)", "author": ["Goh Ong Sing", "Teoh Kung Keat"], "venue": "Universiti Sabah Malaysia, Sabah on 17-", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Intelligent virtual doctor system\" at 2nd IEE Seminar on Appropriate Medical Technology for Developing Countries on", "author": ["Goh Ong Sing", "Teoh Kung Keat"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Social Role Awareness in Animated Agents", "author": ["Helmut Prendinger", "Mitsuru Ishizuka"], "venue": "Proceedings of the fifth international conference on Autonomous agents, May 28- June", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Emotional Speech Classification with Prosodic Prameters by using Neural Network.", "author": ["H.Sato", "Y.Mitsukura", "M. Fukumi", "N.Akamatsu"], "venue": "Intelligent Information Systems Conference, The Seventh Australian and New Zealand", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Bimodal Emotion Recognition", "author": ["Liyanage C. De Silva", "Pei Chi Ng"], "venue": "Automatic Face and Gesture Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Stochastic pronunciation modeling from hand-labelled phonetic corpora", "author": ["Michael Riley", "William Byrane", "Michael Finke", "Sanjeev Khudanpur", "Andrej Ljolje", "John McDonouh", "Harriet Nock", "Murat Saraclar", "Charles Wooters", "George Zavaliagkos"], "venue": "Speech Communication", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Collaboration, Dialogue, and Human-Robot Interaction", "author": ["Terrence Fong", "Charles Thorpe", "Charles Baur"], "venue": "International Symposium of Robotics Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Emotion recognition in speech signals: Experimental study, development, and application", "author": ["V. Petrushin"], "venue": "In Proc. ICSLP 2000,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Speech-Driven Cartoon Animation with Emotions.", "author": ["Yan Li", "Feng Lu", "Ying-Qing Xu", "Eric Chang", "Heung-Yeing Shum"], "venue": "Proceedings of the ninth ACM international conference on Multimedia", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}], "referenceMentions": [{"referenceID": 2, "context": "We are using a natural language processing system called AINI (Artificial Intelligent Neural-network Identity) [6, 7, 8].", "startOffset": 111, "endOffset": 120}, {"referenceID": 3, "context": "We are using a natural language processing system called AINI (Artificial Intelligent Neural-network Identity) [6, 7, 8].", "startOffset": 111, "endOffset": 120}, {"referenceID": 4, "context": "We are using a natural language processing system called AINI (Artificial Intelligent Neural-network Identity) [6, 7, 8].", "startOffset": 111, "endOffset": 120}, {"referenceID": 6, "context": "The researches from University of Tokushima try to classify emotions by using pitch [10].", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "Researchers in National University of Singapore try to analyze facial expressions and speech in order to classify emotions [12].", "startOffset": 123, "endOffset": 127}, {"referenceID": 11, "context": "com are also trying to classify emotions based on emotions derived from speech signal [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "Recent research has identified five main emotions in human: normal, happy, sad, afraid, and angry [20].", "startOffset": 98, "endOffset": 102}], "year": 2002, "abstractText": "Recent development in developing humanoid robot poses new challenges to human-machine interaction communication. A major challenge is to develop robots that can behave like and interact with human in the most natural way possible. This paper proposes a system to develop a robot that can receive command, and talk to people in natural language. In addition, the robot can also be \u201c trained\u201d to become an expert in specific areas to provide expert advice to human-beings. Most important of all, the robot can display emotions through facial expression, speech and gesture so that the interaction process will become more comprehensive and compelling.", "creator": "http://www.fineprint.com"}}}