{"id": "1411.1509", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Nov-2014", "title": "Convolutional Neural Network-based Place Recognition", "abstract": "recently convolutional neural networks ( named cnns ) have been shown to achieve consistently state - of - the - art performance on various classification tasks. in this paper, we present for the first time a place recognition technique based on parallel cnn models, by combining to the highly powerful features learnt by cnns with a spatial and sequential filter. applying the system to a perfect 70 km benchmark place recognition dataset we achieve a 75 % increase in recall at 100 % precision, significantly outperforming all previous state of the art techniques. we also conduct a comprehensive raw performance comparison of the utility of features from studying all 21 layers chosen for place recognition, both for the resulting benchmark dataset and for a second dataset with more significant viewpoint changes.", "histories": [["v1", "Thu, 6 Nov 2014 07:03:15 GMT  (1436kb)", "http://arxiv.org/abs/1411.1509v1", "8 pages, 11 figures, this paper has been accepted by 2014 Australasian Conference on Robotics and Automation (ACRA 2014) to be held in University of Melbourne, Dec 2~4"]], "COMMENTS": "8 pages, 11 figures, this paper has been accepted by 2014 Australasian Conference on Robotics and Automation (ACRA 2014) to be held in University of Melbourne, Dec 2~4", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["zetao chen", "obadiah lam", "adam jacobson", "michael milford"], "accepted": false, "id": "1411.1509"}, "pdf": {"name": "1411.1509.pdf", "metadata": {"source": "CRF", "title": "Convolutional Neural Network-based Place Recognition", "authors": ["Zetao Chen", "Obadiah Lam", "Adam Jacobson", "Michael Milford"], "emails": ["zetao.chen@student.qut.edu.au."], "sections": [{"heading": "1 Introduction1", "text": "Since their introduction in the early 1990s, Convolutional Neural Networks (CNNs) have been used to achieve excellent performance on a variety of tasks such as handwriting recognition and face detection. More recently, supervised deep convolutional neural networks have been shown to deliver high level performance on more challenging classification tasks [Krizhevsky, et al., 2012]. The key supporting factors behind these impressive results are their ability to learn tens of millions of parameters using large amounts of labelled data. Once trained in this way, CNNs have been shown to learn discriminative and humaninterpretable feature representations [Zeiler and Fergus, 2013]. Most impressively, these approaches are capable of producing state of the art performance on tasks that the model was not explicitly trained for [Donahue, et al., 2013], including object recognition on the Caltech101 dataset [Fei-Fei, et al., 2007], subcategory recognition on the Caltech-USCD birds dataset [Welinder, et al., 2010], scene recognition on the SUN397 dataset [Xiao, et al., 2010] and object detection on\nThe authors are with the School of Electrical Engineering and Computer Science at the Queensland University of Technology, Australia. http://roboticvision.org/. email: zetao.chen@student.qut.edu.au. This work was supported by a funding from the Australian Research Council Centre of Excellence CE140100016 in Robotic Vision.\nthe PASCAL VOC dataset [Girshick, et al., 2013]. This good generalization in performance on new tasks and datasets indicates that CNNs may provide a general and universal visual feature learning framework applicable to all tasks. Encouraged by these positive results, in this paper we develop a place recognition framework centered around features from pre-trained CNNs as illustrated in Figure 1.\nPlace recognition can be considered as an image retrieval task which consists of determining a match between the current scene and a previously visited location. State-of-the-art visual SLAM algorithms such as FAB-MAP [Cummins and Newman, 2008] match the appearance of the current scene to a past place by converting the image into bag-of-words representations [Angeli, et al., 2008] built on local features such as SIFT or SURF. However, recent evidence [Krizhevsky, et al., 2012] suggests that features extracted from CNNs trained on very large datasets significantly outperform SIFT features on classification tasks. Donahue [Donahue, et al., 2013] shows that using mid-level features from CNN models trained on the ImageNet database can more efficiently remove dataset bias in some of the domain adaption studies than a bag of words approach.\nIn this paper we investigate whether the advantages of deep learning in other recognition tasks carries over to place recognition. We present a deep learning-based place recognition algorithm that compares the response of feature layers from a CNN trained on ImageNet [Deng, et al., 2009] and methods for filtering the subsequent place recognition hypotheses. We conduct two experiments, one on a 70 km benchmark place recognition dataset, and one on a viewpoint varying dataset, providing both quantitative comparison to two state of the art place recognition algorithms and analysis of the utility of different layers within the network for viewpoint invariance.\nThe paper proceeds as follows. Section 2 provides an overview of feature-based place recognition techniques and convolutional neural networks. In Section 3 we describe the components of the deep learning-based place recognition system. The experiments are described in Section 4, with results presented in Section 5. Finally we conclude the paper in Section 6 and discuss ongoing and future work."}, {"heading": "2 Related Work", "text": "In this section, we briefly review feature-based representations for place recognition and the use of convolutional neural networks for various visual classification tasks."}, {"heading": "2.1 Vision Representation for Place Recognition", "text": "Visual sensors are increasingly becoming the predominant sensor modality for place recognition due to their low cost, low power requirements, small footprint and rich information content. There has been extensive research on how to best represent and match images of places.\nSeveral authors have described approaches that apply global feature techniques to process incoming sensor information. In [Murillo and Kosecka., 2009], the authors propose a gist feature-based place recognition system using panoramic images for urban environments. Histograms of image gray values or texture is also a widely used feature in place recognition systems [Ulrich and Nourbakhsh, 2000, Blaer and Allen, 2002] due to its compact representation rotation invariance. However, global features are computed from the entire image, rendering them unsuitable to effects such as partial occlusion, lighting change or perspective transformation [Deselaers, et al., 2008].\nLocal features are less sensitive to these external factors and have been widely used in appearance-based loop closure detection, SIFT [Lowe, 1999] and SURF [Herbert Bay, et al., 2008] being two widespread examples. State-of-the-art SLAM systems such as FABMAP [Cummins and Newman, 2008] further represent appearance data using sets of local features, converting images into \u201cbag-of-words\u201d, which enables efficient retrieval. Other feature-less representations have also been proposed. SeqSLAM [Milford and Wyeth, 2012] directly uses pixel values to match image sequences and perform place recognition across extreme perceptual changes. However, it is rapidly becoming apparent in other recognition tasks that hand-crafted features are being outperformed by learnt features, prompting the\nquestion of whether we can learn better features automatically?"}, {"heading": "2.2 Convolutional Neural Networks", "text": "Convolutional neural networks are multi-layer supervised networks which can learn features automatically from datasets (Figure 3). For the last few years, CNNs have achieved state-of-the-art performance in almost all important classification tasks [Krizhevsky, et al., 2012, Donahue, et al., 2013, Sharif Razavian, et al., 2014]. Their primary disadvantage is that they require very large amounts of training data. However, recent studies have shown that state of the art performance can be achieved with networks trained using \u201cgeneric\u201d data, raising the possibility of developing a place recognition system based on features learnt from datasets with a classification focus. A similar approach has already achieved excellent performance on various visual tasks, such as object recognition [Fei-Fei, et al., 2007]; subcategory recognition [Welinder, et al., 2010]; scene recognition [Xiao, et al., 2010] and detection [Girshick, et al., 2013].\nOne research area separate but relevant to the place recognition problem is the task of image retrieval where a query image is present to a database to search for those images containing the same objects or scenes. In [Babenko, et al., 2014], mid-level features from CNNs are evaluated for the image retrieval application and achieve performance comparable to others using stateof-the-art features. Interestingly, the best performance is obtained using mid-network features rather than those learnt at the final layers.\nPlace recognition is essentially a task of image similarity matching. In [Fischer, et al., 2014], features from various layers of CNNs are evaluated and compared with SIFT descriptors on a descriptor matching benchmark. The benchmark results demonstrate that deep features from different layers of CNNs consistently perform better than SIFT on descriptor matching; indicating that SIFT or SURF may not be the preferred descriptors for matching tasks anymore. Our paper is thus inspired by the excellent performance of CNNs on image classification and the evidence of their feasibility in feature matching."}, {"heading": "3 Approach and Methodology", "text": "In this section we describe the two key components of our approach: feature extraction and spatio-temporal filtering of place match hypotheses output by comparison of feature responses. The schematic illustration of the method procedure is shown in Figure 2:"}, {"heading": "3.1 Feature Extractor", "text": "We use a pretrained network called Overfeat [Sermanet, et al., 2013] which was originally proposed for the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013). The Overfeat network is trained on the ImageNet 2012 dataset, which consists of 1.2 million images and 1000 classes. The network comprises five convolution stages and three fully connected stages (Figure 3). Each of the bottom two convolution stages consists of a convolution layer, a max pooling layer and a rectification (ReLU) nonlinearity layer. The third and fourth stages consist of a convolution layer, a zero-padding layer and a ReLU non-linear layer. The fifth stage contains a convolution layer, a zero-padding layer, a ReLU layer and a maxpooling layer. Finally, the sixth and seventh stages contain one fully-connected layer and one ReLU layer, while the eighth contains only the fully-connected output layer. In total, there are 21 layers.\nWhen an image I is input into the network, it produces a sequence of layered activations. We use , = 1,\u2026 ,21 to denote the corresponding output of the layer given input image I. Each of these vectors is a deep learnt representation of the image I; place recognition is performed by comparing these feature vector responses to different images. The network is capable of processing images of any size equal to or greater than 231 \u00d7 231 pixels, consequently all experiments described here used images resized to 256 \u00d7 256 pixels."}, {"heading": "3.2 Confusion Matrix", "text": "For each layer output , = 1,\u2026 ,21 , we generate a corresponding confusion matrix , = 1,\u2026 ,21 from the whole dataset with training images and testing images (Figure 4). Each element , represents the Euclidean distance between the feature vector responses to the training image and the testing image:\n( , ) ( ( ), ( )) 1,..., , 1,..., k k i k jM i j d L I L I i R j T = = = (1)\nEach column j stores the mean feature vector difference between the testing image and all training images.\nTo find the strongest place match hypothesis, each column is searched for the element with the lowest feature vector difference.\n( ) arg min ( , ), , 1,...,k k i M j M i j i j T= \u2200 = (2)"}, {"heading": "3.3 Spatial Continuity", "text": "We apply two continuity filters to the place match hypotheses extracted from the confusion matrix. The first, a spatial continuity check, enforces that consecutive first-ranked place match hypotheses must occur in close indices in the confusion matrix, providing a constraint that does not require a specific motion model. More specifically, the plausible measurement of each place match hypothesis is evaluated as follows:\n[ ] ( ) 1 (u 1) (u) , ,\n,...,\nk\nk k\nP j\nif M M u j d j\nj d T\n\u03b5 =\n\u2212 \u2212 \u2264 \u2200 \u2208 \u2212 =\n(3)\nwhere is the threshold for consecutive first-ranked match difference, determines how far back in time the evaluation goes and is the current testing image. A positive match is reported only when = 1. Figure 5 provides an illustration of the spatial continuity check in action. This constraint reduces but does not eliminate all false positives; consequently we implement a secondary, sequential filter step that implements an actual motion model, and is described in the next section."}, {"heading": "3.4 Sequential Filter", "text": "The sequential filter is a more sophisticated implementation of the crude motion filter in SeqSLAM.\nRather than searching for all coherent diagonal sequences of strong matching hypotheses, we linear polynomial models to the top matches in local sequence = , by using:\n( ) j jy f x x\u03b1 \u03b2= = +\nwhere is the sequence length used in Section 3.3, the current frame and describes the slope of the linear model in sequence which represents the velocity ratio between the second and first traverse. shown in Figure 6, the place match hypotheses comprising a sequence are accepted if the velocity ratio is within a certain bound around a reference velocity . The parameter is swept over a range o generate the precision-recall curves shown in Section 5.1. We also note here that if an odometry source is available, this sequence search could be significantly simplified in a manner similar to the SMART approach [Pepperell, et al., 2013]."}, {"heading": "4 Experimental Setup", "text": "In this section, we describe the datasets used, gro truth measures, and parameter values."}, {"heading": "4.1 Datasets", "text": "Details of the two datasets used are Table 1. Each dataset consists of two traverses along the same route, with the first traverse used for training and the second traverse used for testing. For both environments, full resolution images converted to gray-scale and then histogram normalized to reduce the effect of illumination variations. were then resized to 256 \u00d7 256 pixels before into the CNNs.\nThe Eynsham dataset is a large 70 km road dataset (2 \u00d7 35 km traverses) used in the Newman, 2009] FAB-MAP and SeqSLAM studies. Panoramic images were captured at 7 meter intervals using a Ladybug 2 camera. The QUT dataset was collected using a hand-held camera walking around the Queensland University of Technology campus viewpoint shift of up to 5 metres lateral camera movement between the first and second traverse\nonly fit each\n! 1 , \u2026 ,\n(4)\nj is\nAs\nf values to\nis\nis within a\nund\nsummarized in\nwere first\nImages being input\n-based [Cummins and\n, with a\ns."}, {"heading": "4.2 Ground Truth", "text": "For the Eynsham dataset, w tolerance GPS-derived ground truth provided with the Eynsham dataset, consistent with the tolerance used in the original FAB-MAP study 2008] and SeqSLAM study [ For the QUT dataset, ground truth was obtained by manually parsing each frame and correspondence. We use a tolerance of 2 frames corresponding to approximate"}, {"heading": "4.3 Parameter Values", "text": "In Table 2, we provide the values of the critical parameters used in the experiment"}, {"heading": "5 Results", "text": "In this section we present two sets of results on the Eynsham and QUT datasets performance between our proposed approach with both FAB-MAP 2.0 and SeqSLAM on the benchmark Eynsham dataset, as well as an evaluation of the performance of each feature layer for viewpoint invariant place recognition. performance statistics and discuss the feasibility of a real-time implementation.\ne used the 40 metre\n[Cummins and Newman, Milford and Wyeth, 2012].\nbuilding frame ,\nly 3.8 meters.\nImagery @2014 Google,\n.\n; firstly, a comparison of\nWe also provide compute"}, {"heading": "5.1 Precision-Recall curves", "text": "This section presents precision-recall curves Eynsham dataset for the deep learning recognition algorithms using features from the CNNs with comparison to SeqSLAM and FAB MAP. Each precision-recall curve was generated by performing a parameter sweep on slope tolerance discussed in Section 3.4.\nFigure 8(a) demonstrates the maximum recall rates at 100% precision achieved using the best performing feature layers and SeqSLAM and FAB maximum recall rate that can be achieved by the deep learning-based approach is 85.7% compared to the approximately 51% recall rate achieved by SeqSLAM. Also noteworthy is that this result is achieved using a filter analogous to SeqSLAM operating with a sequence length of 5, rather than the sequence length o which the 51% recall performance is achieved.\nFigures 8(b) and 8(c) present the precision performance curves for all layers. Clearly the middle\non the -based place all layers of\n-\n. (a) Maximal\n-of-the-art th layer of\n-MAP. The\nf 50 with\n-recall\nnetwork layers provide the best performance, a result consistent the image retrieval experiment et al., 2014] which suggest that the middle network layers provide a more general feature description while the top layers are overtrained for the ImageNet task.\ns of [Babenko,\nfrom (a) reverse trajectories on\n."}, {"heading": "5.2 Viewpoint Invariance", "text": "We evaluate the viewpoint invariance of different network layers using a custom QUT dataset specifically gathered with lateral camera variance, and by selecting a subsection of the Eynsham dataset where the car is travelling in the reverse direction on the opposite side of the road.\nFigure 9 shows the F1 scores achieved by layer of the CNN. There is a clear trend for increasing viewpoint invariance in later network layers. lines represent a performance baseline using a Sum of Absolute Differences comparison. The green dotted lines performance of SAD with offset matching, a measure frequently used to increase the viewpoint invariance of such as technique [Milford, 2013]."}, {"heading": "5.3 Visualization of Confusion Matrix", "text": "To provide a qualitative illustration of the superiority of using deep learnt features over simple techniques like SAD, we present a comparison between a subsection of the confusion matrix from the 10th layer each approach (Figure 11).\nFigure 11 Comparison of confusion matrix from the 10 sum of absolute difference (top) and deep learning features A more clear diagonal pattern can be observed in the bottom figure."}, {"heading": "5.4 Feasibility of Real-time Operation", "text": "Deep learning approaches are notoriously computationally-intensive so an examination of real time capability is particular necessary. The experiments in this paper used the Overfeat network, and feature extraction ran at significantly slower than real single PC. However, we have recently re the system using another convolutional architecture dubbed Caffe [Jia, et al., 2014], and initial studies suggest that recognition performance is near while being many orders of magnitude faster. Consequently, here we present a calculation of the computation required for extracting features and feature matching using Caffe.\neach\nThe red calculated by (SAD) image show the\ngenerated using\nth layer using (bottom).\n-\n-time on a -implemented\n-identical\nCNN Feature Extraction\nUsing a standard CPU, 10 images per second.\nFeature Matching\nThe largest feature vector is from the 10 which contains 64899 uint16 values. Comparing it to all 4789 training images will require:\n64899 * 2 * 4789 = 6.22\nBased on this, a standard CPU about 2.5 frames per second."}, {"heading": "6 Discussion and Future Work", "text": "The results in this paper demonstrate th place recognition can benefit immensely from incorporating features learnt using CNNs. In performance using even a relatively simple framework around these features results in performance significantly better than the current state of the art algorithms. Furthermore it is interesting to note that different layers appear to be suitab aspects of the place recognition task being optimal for recognition on relatively static, similar viewpoint datasets, while later layers appear to perform better when viewpoint variance becomes significant. In this section we discuss some of the new opportunities and challenges that exist in this field."}, {"heading": "6.1 Network Adaption Training", "text": "Datasets are inherently biased in computer vision [Torralba and Efros, 2011]. the researchers demonstrated CNN model trained on large amounts of labelled data reduces, but does not remove, data bias. we use in this paper is trained for a different classification task; therefore, although it demonstrates impressive generalization performance recognition task, a major question remains whether performance can be training a network from scratch with place recognition datasets. One potential problem with such an approach is the relative sparsity of very large place recognition datasets in comparison with the millions of frames found in the Imagenet database. keep all the parameters from the then add a final domain specific classification layer trained for each particular new dataset. has been adopted in some domain adaption CNNs for object recognition Oquab, et al., 2013, Rodner, et al., 2013"}, {"heading": "6.2 Automatic Layer Selection", "text": "Currently there is no mechanism for select the best layer for the task. Future work will investigat of the best performing layer introducing a performance measurement for each layer during the training process."}, {"heading": "6.3 Feature Ranking for Deep Learning Features", "text": "In this paper, we use a simple to compare the similarity of feature responses, an\ncan be processed\nth layer\n* 10- comparison/s\nshould be able to process\nat the task of\nparticular,\nle for different \u2013 the middle layers\nIn [Hoffman, et al., 2013], that a supervised deep\nThe network\nto a different unanswered;\nfurther improved by\nOne option will be to pre-trained model and\nThis approach work from\n[Hoffman, et al., 2013, ].\nautomatically specific place recognition\ne automating selection s, most immediately by\nEuclidean distance metric\napproach that implicitly assumes that each feature contributes equally to place recognition performance. This assumption is likely unreasonable because feature weights are normally dataset-dependent. A feature which contributes strongly in one dataset may have little classification power for another dataset. In future work, we plan to train a dataset-dependent feature ranking algorithm for each new task to automatically weight the contributions of different features.\nReferences:\n[A. Krizhevsky, I. Sutskever, and G. E. Hinton, 2012] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet classification with deep convolutional neural networks,\" presented at the Advances in neural information processing systems, 2012. [M. D. Zeiler and R. Fergus, 2013] M. D. Zeiler and R. Fergus, \"Visualizing and understanding convolutional neural networks,\" arXiv preprint arXiv:1311.2901, 2013. [J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell, 2013] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell, \"Decaf: A deep convolutional activation feature for generic visual recognition,\" arXiv preprint arXiv:1310.1531, 2013. [L. Fei-Fei, R. Fergus, and P. Perona, 2007] L. Fei-Fei, R. Fergus, and P. Perona, \"Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,\" Computer Vision and Image Understanding, vol. 106, pp. 59-70, 2007. [P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona, 2010] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona, \"Caltech-UCSD birds 200,\" 2010. [J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, 2010] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, \"Sun database: Largescale scene recognition from abbey to zoo,\" in Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, 2010, pp. 3485-3492. [R. Girshick, J. Donahue, T. Darrell, and J. Malik, 2013] R. Girshick, J. Donahue, T. Darrell, and J. Malik, \"Rich feature hierarchies for accurate object detection and semantic segmentation,\" arXiv preprint arXiv:1311.2524, 2013. [M. Cummins and P. Newman, 2008] M. Cummins and P. Newman, \"FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance,\" International Journal of Robotics Research, vol. 27, pp. 647-665, 2008. [A. Angeli, D. Filliat, S. Doncieux, and J.-A. Meyer, 2008] A. Angeli, D. Filliat, S. Doncieux, and J.A. Meyer, \"Fast and Incremental Method for Loop-Closure Detection Using Bags of Visual Words,\" IEEE Transactions on Robotics, vol. 24, pp. 1027-1037, 2008. [J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, 2009] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Imagenet: A largescale hierarchical image database,\" presented at\nthe Computer Vision and Pattern Recognition, 2009.\n[A. C. Murillo and J. Kosecka., 2009] A. C. Murillo and J. Kosecka., \"Experiments in place recognition using gist panoramas,\" presented at the International Conference on Computer Vision Workshops (ICCV Workshops), 2009. [I. Ulrich and I. Nourbakhsh, 2000] I. Ulrich and I. Nourbakhsh, \"Appearance-Based Place Recognition for Topological Localization,\" in IEEE International Conference on Robotics and Automation, San Francisco, CA, USA, 2000, pp. 1023-1029. [P. Blaer and P. Allen, 2002] P. Blaer and P. Allen, \"Topological mobile robot localization using fast vision techniques,\" presented at the IEEE ICRA, 2002. [T. Deselaers, D. Keysers, and H. Ney, 2008] T. Deselaers, D. Keysers, and H. Ney, \"Features for image retrieval: an experimental comparison,\" Information Retrieval, vol. 11, pp. 77-107, 2008. [D. G. Lowe, 1999] D. G. Lowe, \"Object Recognition from Local Scale-Invariant Features,\" presented at the Proceedings of the International Conference on Computer Vision-Volume 2 - Volume 2, 1999. [Herbert Bay, Andreas Ess, Tinne Tuytelaars, and L. V. Gool, 2008] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and L. V. Gool, \"SURF: Speeded Up Robust Features,\" Computer Vision and Image Understanding (CVIU), vol. 110, pp. 346-359, 2008. [M. Milford and G. Wyeth, 2012] M. Milford and G. Wyeth, \"SeqSLAM: Visual Route-Based Navigation for Sunny Summer Days and Stormy Winter Nights,\" in IEEE International Conference on Robotics and Automation, St Paul, United States, 2012. [A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, 2014] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, \"CNN Features offthe-shelf: an Astounding Baseline for Recognition,\" arXiv preprint arXiv:1403.6382, 2014. [A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky, 2014] A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky, \"Neural Codes for Image Retrieval,\" arXiv preprint arXiv:1404.1777, 2014. [P. Fischer, A. Dosovitskiy, and T. Brox, 2014] P. Fischer, A. Dosovitskiy, and T. Brox, \"Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT,\" arXiv preprint arXiv:1405.5769, 2014. [P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun, 2013] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun, \"Overfeat: Integrated recognition, localization and detection using convolutional networks,\" presented at the arXiv preprint arXiv:1312.6229, 2013. [E. Pepperell, P. Corke, and M. Milford, 2013] E. Pepperell, P. Corke, and M. Milford, \"Towards persistent visual navigation using SMART,\" in\nProceedings of Australasian Conference on Robotics and Automation, 2013.\n[M. Cummins and P. Newman, 2009] M. Cummins and P. Newman, \"Highly scalable appearance-only SLAM - FAB-MAP 2.0,\" in Robotics: Science and Systems, Seattle, United States, 2009. [M. Milford, 2013] M. Milford, \"Vision-based place recognition: how low can you go?,\" International Journal of Robotics Research, vol. 32, pp. 766- 789, 2013. [Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, 2014] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, \"Caffe: Convolutional Architecture for Fast Feature Embedding,\" 2014. [A. Torralba and A. A. Efros, 2011] A. Torralba and A. A. Efros, \"Unbiased look at dataset bias,\" in Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, 2011, pp. 1521-1528. [J. Hoffman, E. Tzeng, J. J. Donahue, Yangqing, K. Saenko, and T. Darrell, 2013] J. Hoffman, E. Tzeng, J. J. Donahue, Yangqing, K. Saenko, and T. Darrell, \"One-Shot Adaptation of Supervised Deep Convolutional Models,\" presented at the arXiv preprint arXiv:1312.6204, 2013. [M. Oquab, L. Bottou, I. Laptev, and J. Sivic, 2013] M. Oquab, L. Bottou, I. Laptev, and J. Sivic, \"Learning and transferring mid-level image representations using convolutional neural networks,\" 2013. [E. Rodner, J. Hoffman, J. Donahue, T. Darrell, and K. Saenko, 2013] E. Rodner, J. Hoffman, J. Donahue, T. Darrell, and K. Saenko, \"Towards adapting imagenet to reality: Scalable domain adaptation with implicit low-rank transformations,\" arXiv preprint arXiv:1308.4200, 2013."}], "references": [{"title": "and G", "author": ["A. Krizhevsky", "I. Sutskever"], "venue": "E. Hinton, \"Imagenet classification with deep convolutional neural networks,\" presented at the Advances in neural information processing systems", "citeRegEx": "A. Krizhevsky. I. Sutskever. and G. E. Hinton. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Fergus", "author": ["R M.D. Zeiler"], "venue": "\"Visualizing and understanding convolutional neural networks,\" arXiv preprint arXiv:1311.2901,", "citeRegEx": "M. D. Zeiler and R. Fergus. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition,", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2013}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,\" Computer Vision and Image", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona", "2007] L. Fei-Fei"], "venue": null, "citeRegEx": "Fei.Fei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2007}, {"title": "and P", "author": ["P. Welinder", "S. Branson", "T. Mita", "C. Wah", "F. Schroff", "S. Belongie"], "venue": "Perona, \"Caltech-UCSD birds 200,\"", "citeRegEx": "P. Welinder. S. Branson. T. Mita. C. Wah. F. Schroff. S. Belongie. and P. Perona. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Sun database: Largescale scene recognition from abbey to zoo,\" in Computer vision and pattern recognition", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2010}, {"title": "and J", "author": ["R. Girshick", "J. Donahue", "T. Darrell"], "venue": "Malik, \"Rich feature hierarchies for accurate object detection and semantic segmentation,\" arXiv preprint arXiv:1311.2524", "citeRegEx": "R. Girshick. J. Donahue. T. Darrell. and J. Malik. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Newman", "author": ["P M. Cummins"], "venue": "\"FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance,\" International Journal of Robotics Research, vol. 27, pp. 647-665,", "citeRegEx": "M. Cummins and P. Newman. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast and Incremental Method for Loop-Closure Detection Using Bags of Visual Words,", "author": ["A. Angeli", "D. Filliat", "S. Doncieux", "J.-A. Meyer", "2008] A. Angeli", "J.A. Meyer"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "Angeli et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Angeli et al\\.", "year": 2008}, {"title": "Imagenet: A largescale hierarchical image database,", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Murillo and J", "author": ["C A."], "venue": "Kosecka., \"Experiments in place recognition using gist panoramas,\" presented at the International Conference on Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "A. C. Murillo and J. Kosecka.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Allen", "author": ["P P. Blaer"], "venue": "\"Topological mobile robot localization using fast vision techniques,\" presented at the IEEE ICRA,", "citeRegEx": "P. Blaer and P. Allen. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "and H", "author": ["T. Deselaers", "D. Keysers"], "venue": "Ney, \"Features for image retrieval: an experimental comparison,\" Information Retrieval, vol. 11, pp. 77-107", "citeRegEx": "T. Deselaers. D. Keysers. and H. Ney. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Lowe", "author": ["G D."], "venue": "\"Object Recognition from Local Scale-Invariant Features,\" presented at the Proceedings of the International Conference on Computer Vision-Volume 2 - Volume 2,", "citeRegEx": "D. G. Lowe. 1999", "shortCiteRegEx": null, "year": 1999}, {"title": "SeqSLAM: Visual Route-Based Navigation for Sunny", "author": ["M. Milford", "G. Wyeth", "2012] M. Milford"], "venue": "Summer Days and Stormy Winter Nights,\" in IEEE International Conference on Robotics and Automation, St Paul,", "citeRegEx": "Milford et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Milford et al\\.", "year": 2012}, {"title": "CNN Features offthe-shelf: an Astounding Baseline for Recognition,\" arXiv preprint arXiv:1403.6382", "author": ["A. Sharif Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson", "2014] A. Sharif Razavian"], "venue": null, "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "and V", "author": ["A. Babenko", "A. Slesarev", "A. Chigorin"], "venue": "Lempitsky, \"Neural Codes for Image Retrieval,\" arXiv preprint arXiv:1404.1777", "citeRegEx": "A. Babenko. A. Slesarev. A. Chigorin. and V. Lempitsky. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and T", "author": ["P. Fischer", "A. Dosovitskiy"], "venue": "Brox, \"Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT,\" arXiv preprint arXiv:1405.5769", "citeRegEx": "P. Fischer. A. Dosovitskiy. and T. Brox. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Milford, \"Towards persistent visual navigation using SMART,", "author": ["E. Pepperell", "P. Corke", "M. Milford", "2013] E. Pepperell"], "venue": null, "citeRegEx": "Pepperell et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pepperell et al\\.", "year": 2013}, {"title": "Newman", "author": ["P M. Cummins"], "venue": "\"Highly scalable appearance-only SLAM - FAB-MAP 2.0,\" in Robotics: Science and Systems, Seattle, United States,", "citeRegEx": "M. Cummins and P. Newman. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Vision-based place recognition: how low can you go?,", "author": ["M. Milford"], "venue": "International Journal of Robotics Research,", "citeRegEx": "Milford,? \\Q2013\\E", "shortCiteRegEx": "Milford", "year": 2013}, {"title": "Convolutional Architecture for Fast", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Unbiased look at dataset bias,\" in Computer Vision and Pattern Recognition", "author": ["A. Torralba", "A.A. Efros", "2011] A. Torralba"], "venue": "IEEE Conference on,", "citeRegEx": "Torralba et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Torralba et al\\.", "year": 2011}, {"title": "and J", "author": ["M. Oquab", "L. Bottou", "I. Laptev"], "venue": "Sivic, \"Learning and transferring mid-level image representations using convolutional neural networks,\"", "citeRegEx": "M. Oquab. L. Bottou. I. Laptev. and J. Sivic. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards adapting imagenet to reality: Scalable domain adaptation with implicit low-rank", "author": ["E. Rodner", "J. Hoffman", "J. Donahue", "T. Darrell", "K. Saenko"], "venue": null, "citeRegEx": "Rodner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Rodner et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "The green dotted lines performance of SAD with offset matching, a measure frequently used to increase the viewpoint invariance of such as technique [Milford, 2013].", "startOffset": 148, "endOffset": 163}, {"referenceID": 0, "context": "[A. Krizhevsky, I. Sutskever, and G. E. Hinton, 2012] A.", "startOffset": 0, "endOffset": 53}, {"referenceID": 1, "context": "[M. D. Zeiler and R. Fergus, 2013] M.", "startOffset": 0, "endOffset": 34}, {"referenceID": 4, "context": "[P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona, 2010] P.", "startOffset": 0, "endOffset": 88}, {"referenceID": 6, "context": "[R. Girshick, J. Donahue, T. Darrell, and J. Malik, 2013] R.", "startOffset": 0, "endOffset": 57}, {"referenceID": 7, "context": "[M. Cummins and P. Newman, 2008] M.", "startOffset": 0, "endOffset": 32}, {"referenceID": 10, "context": "[A. C. Murillo and J. Kosecka., 2009] A.", "startOffset": 0, "endOffset": 37}, {"referenceID": 11, "context": "[P. Blaer and P. Allen, 2002] P.", "startOffset": 0, "endOffset": 29}, {"referenceID": 12, "context": "[T. Deselaers, D. Keysers, and H. Ney, 2008] T.", "startOffset": 0, "endOffset": 44}, {"referenceID": 13, "context": "[D. G. Lowe, 1999] D.", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": "[A. Babenko, A. Slesarev, A. Chigorin, and V. Lempitsky, 2014] A.", "startOffset": 0, "endOffset": 62}, {"referenceID": 17, "context": "[P. Fischer, A. Dosovitskiy, and T. Brox, 2014] P.", "startOffset": 0, "endOffset": 47}, {"referenceID": 20, "context": "[M. Cummins and P. Newman, 2009] M.", "startOffset": 0, "endOffset": 32}, {"referenceID": 24, "context": "[M. Oquab, L. Bottou, I. Laptev, and J. Sivic, 2013] M.", "startOffset": 0, "endOffset": 52}], "year": 2014, "abstractText": "Recently Convolutional Neural Networks (CNNs) have been shown to achieve state-of-the-art performance on various classification tasks. In this paper, we present for the first time a place recognition technique based on CNN models, by combining the powerful features learnt by CNNs with a spatial and sequential filter. Applying the system to a 70 km benchmark place recognition dataset we achieve a 75% increase in recall at 100% precision, significantly outperforming all previous state of the art techniques. We also conduct a comprehensive performance comparison of the utility of features from all 21 layers for place recognition, both for the benchmark dataset and for a second dataset with more significant viewpoint changes.", "creator": "PDFCreator Version 1.7.3"}}}