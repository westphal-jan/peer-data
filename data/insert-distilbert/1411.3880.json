{"id": "1411.3880", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Nov-2014", "title": "Optimal Cost Almost-Sure Reachability in POMDPs", "abstract": "we consider partially observable markov decision processes ( pomdps ) with a set of target states and every transition is associated with increasing an integer cost. the optimization objective we study carefully asks to minimize the expected total minimum cost till the target set is reached, while ensuring that the target set is reached almost - not surely ( with probability 1 ). we show that for integer costs approximating the optimal cost is undecidable. for positive costs, our results are as follows : ( i ) we establish matching lower and upper bounds formulas for the nearest optimal cost and the low bound is double exponential ; ( ii ) we show that the problem of approximating the optimal cost is decidable and present approximation algorithms developing on the existing algorithms for pomdps with finite - interval horizon objectives. while the worst - limit case running time of our algorithm is double exponential, we also present efficient stopping criteria for the algorithm and show experimentally that guarantees it performs well in many examples of interest.", "histories": [["v1", "Fri, 14 Nov 2014 12:13:45 GMT  (3791kb,D)", "http://arxiv.org/abs/1411.3880v1", "Full Version of Optimal Cost Almost-sure Reachability in POMDPs, AAAI 2015. arXiv admin note: text overlap witharXiv:1207.4166by other authors"]], "COMMENTS": "Full Version of Optimal Cost Almost-sure Reachability in POMDPs, AAAI 2015. arXiv admin note: text overlap witharXiv:1207.4166by other authors", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["krishnendu chatterjee", "martin chmelik", "raghav gupta", "ayush kanodia"], "accepted": true, "id": "1411.3880"}, "pdf": {"name": "1411.3880.pdf", "metadata": {"source": "CRF", "title": "Optimal Cost Almost-sure Reachability in POMDPs (Full Version)", "authors": ["Krishnendu Chatterjee", "Martin Chme\u013a\u0131k", "Raghav Gupta", "Ayush Kanodia"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Partially observable Markov decision processes (POMDPs). Markov decision processes (MDPs) are standard models for probabilistic systems that exhibit both probabilistic as well as nondeterministic behavior [12]. MDPs are widely used to model and solve control problems for stochastic systems [11,28]: nondeterminism represents the freedom of the controller to choose a control action, while the probabilistic component of the behavior describes the system response to control actions. In perfect-observation (or perfectinformation) MDPs the controller observes the current state of the system precisely to choose the next control actions, whereas in partially observable MDPs (POMDPs) the state space is partitioned according to observations that the controller can observe, i.e., given the current state, the controller can only view the observation of the state (the partition the state belongs to), but not the precise state [24]. POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few. POMDPs also subsume many other powerful computational models such as probabilistic finite automata (PFA) [29,26] (since probabilistic finite automata (aka blind POMDPs) are a special case of POMDPs with a single observation).\nClassical optimization objectives. In stochastic optimization problems related to POMDPs, the transitions in the POMDPs are associated with integer costs, and the two classical objectives that have been widely studied are finite-horizon and discounted-sum objectives [11,28,24]. For finite-horizon objectives, a finite length k is given and the goal is to minimize the expected total cost for k steps. In discounted-sum objectives, the cost in the j-th step is multiplied by \u03b3j , for 0 < \u03b3 < 1, and the goal is to minimize the expected total discounted cost over the infinite horizon.\nReachability and total-cost. In this work we consider a different optimization objective for POMDPs. We consider POMDPs with a set of target states, and the optimization objective is to minimize the expected total cost till the target set is reached. First, note that the objective is not the discounted sum, but the total sum without discounts. Second, the objective is not a finite-horizon objective, as there is no bound apriori known to reach the target set, and along different paths the target set can be reached at different time\n? The research was partly supported by Austrian Science Fund (FWF) Grant No P23499-N23, FWF NFN Grant No S11407-N23 (RiSE), ERC Start grant (279307: Graph Games), and Microsoft faculty fellows award.\nar X\niv :1\n41 1.\n38 80\nv1 [\ncs .A\nI] 1\n4 N\nov 2\n01 4\npoints. The objective we consider is very relevant in many control applications such as in robot planning: for example, the robot has a target or goal; and the objective is to minimize the number of steps to reach the target, or every transition is associated with energy consumption and the objective is to reach the target with minimal energy consumption.\nOur contributions. In this work we study POMDPs with a set of target states, and costs in every transition, and the goal is to minimize the expected total cost till the target set is reached, while ensuring that the target set is reached almost-surely (with probability 1). Our results are as follows:\n1. (Integer costs). We first show that if the transition costs are integers, then approximating the optimal cost is undecidable. 2. (Positive integer costs). Since the problem is undecidable for integer costs, we next consider that costs are positive integers. We first remark that if the costs are positive, and there is a positive probability not to reach the target set, then the expected total cost is infinite. Hence the expected total cost is not infinite only by ensuring that the target is reached almost-surely. First we establish a double-exponential lower and upper bound for the expected optimal cost. We show that the approximation problem is decidable, and present approximation algorithms using the well-known algorithms for finite-horizon objectives. 3. (Implementation). Though we establish that in the worst-case the algorithm requires double-exponential time, we also present efficient stopping criteria for the algorithm, and experimentally show that the algorithm is efficient in several practical examples. We have implemented our approximation algorithms developing on the existing implementations for finite-horizon objectives, and present experimental results on a number of well-known examples of POMDPs.\nComparison with Goal-POMDPs. While there are several works for discounted POMDPs [18,31,27], as mentioned above the problem we consider is different from discounted POMDPs. The most closely related works are Goal-MDPs and POMDPs [2,16]. The key differences are as follows: (a) our results for approximation apply to all POMDPs with positive integer costs, whereas the solution for Goal-POMDPs apply to a strict subclass of POMDPs (see Remark 5); and (b) we present asymptotically tight (double exponential) theoretical bounds on the expected optimal costs."}, {"heading": "2 Definitions", "text": "We present the definitions of POMDPs, strategies, objectives, and other basic notions required for our results. Throughout this work, we follow standard notations from [28,19].\nNotations. Given a finite set X, we denote by P(X) the set of subsets of X, i.e., P(X) is the power set of X. A probability distribution f on X is a function f : X \u2192 [0, 1] such that \u2211 x\u2208X f(x) = 1, and we denote by D(X) the set of all probability distributions on X. For f \u2208 D(X) we denote by Supp(f) = {x \u2208 X | f(x) > 0} the support of f .\nPOMDPs. A Partially Observable Markov Decision Process (POMDP) is a tuple G = (S,A, \u03b4,Z,O, \u03bb0) where: (i) S is a finite set of states; (ii) A is a finite alphabet of actions; (iii) \u03b4 : S \u00d7 A \u2192 D(S) is a probabilistic transition function that given a state s and an action a \u2208 A gives the probability distribution over the successor states, i.e., \u03b4(s, a)(s\u2032) denotes the transition probability from s to s\u2032 given action a; (iv) Z is a finite set of observations; (v) O : S \u2192 Z is an observation function that maps every state to an observation; and (vi) \u03bb0 is a probability distribution for the initial state, and for all s, s\n\u2032 \u2208 Supp(\u03bb0) we require that O(s) = O(s\u2032). If the initial distribution is Dirac, we often write \u03bb0 as s0 where s0 is the unique starting (or initial) state. Given s, s\u2032 \u2208 S and a \u2208 A, we also write \u03b4(s\u2032|s, a) for \u03b4(s, a)(s\u2032). A state s is absorbing if for all actions a we have \u03b4(s, a)(s) = 1 (i.e., s is never left from s). For an observation z, we denote by O\u22121(z) = {s \u2208 S | O(s) = z} the set of states with observation z. For a set U \u2286 S of states and Z \u2286 Z of observations we denote O(U) = {z \u2208 Z | \u2203s \u2208 U. O(s) = z} and O\u22121(Z) = \u22c3 z\u2208Z O\u22121(z). A POMDP is a perfect-observation (or perfect-information) MDP if each state has a unique observation.\nPlays and cones. A play (or a path) in a POMDP is an infinite sequence (s0, a0, s1, a1, s2, a2, . . .) of states and actions such that for all i \u2265 0 we have \u03b4(si, ai)(si+1) > 0 and s0 \u2208 Supp(\u03bb0). We write \u2126 for the set of\nall plays. For a finite prefix w \u2208 (S \u00b7 A)\u2217 \u00b7 S of a play, we denote by Cone(w) the set of plays with w as the prefix (i.e., the cone or cylinder of the prefix w), and denote by Last(w) the last state of w.\nBelief-support and belief-support updates. For a finite prefix w = (s0, a0, s1, a1, . . . , sn) we denote by O(w) = (O(s0), a0,O(s1), a1, . . . ,O(sn)) the observation and action sequence associated with w. For a finite sequence \u03c1 = (z0, a0, z1, a1, . . . , zn) of observations and actions, the belief-support B(\u03c1) after the prefix \u03c1 is the set of states in which a finite prefix of a play can be after the sequence \u03c1 of observations and actions, i.e.,\nB(\u03c1) = {sn = Last(w) | w = (s0, a0, s1, a1, . . . , sn), w is a prefix of a play, and for all 0 \u2264 i \u2264 n. O(si) = zi}.\nThe belief-support updates associated with finite-prefixes are as follows: for prefixes w and w\u2032 = w \u00b7 a \u00b7 s the belief-support update is defined inductively as B(O(w\u2032)) = (\u22c3 s1\u2208B(O(w)) Supp(\u03b4(s1, a)) ) \u2229 O\u22121(O(s)), i.e.,\nthe set (\u22c3 s1\u2208B(O(w)) Supp(\u03b4(s1, a)) )\ndenotes the possible successors given the belief-supprt B(O(w)) and action a, and then the intersection with the set of states with the current observation O(s) gives the new belief-support set.\nStrategies (or policies). A strategy (or a policy) is a recipe to extend prefixes of plays and is a function \u03c3 : (S \u00b7 A)\u2217 \u00b7 S \u2192 D(A) that given a finite history (i.e., a finite prefix of a play) selects a probability distribution over the actions. Since we consider POMDPs, strategies are observation-based, i.e., for all histories w = (s0, a0, s1, a1, . . . , an\u22121, sn) and w \u2032 = (s\u20320, a0, s \u2032 1, a1, . . . , an\u22121, s \u2032 n) such that for all 0 \u2264 i \u2264 n we have O(si) = O(s\u2032i) (i.e., O(w) = O(w\u2032)), we must have \u03c3(w) = \u03c3(w\u2032). In other words, if the observation sequence is the same, then the strategy cannot distinguish between the prefixes and must play the same. A strategy \u03c3 is belief-support based stationary if it depends only on the current belief-support, i.e., whenever for two histories w and w\u2032, we have B(O(w)) = B(O(w\u2032)), then \u03c3(w) = \u03c3(w\u2032). Strategies with memory and finite-memory strategies A strategy with memory is a tuple \u03c3 = (\u03c3u, \u03c3n,M,m0) where:(i) (Memory set). M is a denumerable set (finite or infinite) of memory elements (or memory states). (ii) (Action selection function). The function \u03c3n : M \u2192 D(A) is the action selection function that given the current memory state gives the probability distribution over actions. (iii) (Memory update function). The function \u03c3u : M\u00d7Z\u00d7A \u2192 D(M) is the memory update function that given the current memory state, the current observation and action, updates the memory state probabilistically. (iv) (Initial memory). The memory state m0 \u2208M is the initial memory state. A strategy is a finite-memory strategy if the set M of memory elements is finite. A strategy is pure (or deterministic) if the memory update function and the action selection function are deterministic, i.e., \u03c3u : M \u00d7Z \u00d7A \u2192M and \u03c3n : M \u2192 A. The general class of strategies is sometimes referred to as the class of randomized infinite-memory strategies.\nProbability and expectation measures. Given a strategy \u03c3 and a starting state s, the unique probability measure obtained given \u03c3 is denoted as P\u03c3s (\u00b7). We first define the measure \u00b5\u03c3s (\u00b7) on cones. For w = s we have \u00b5\u03c3s (Cone(w)) = 1, and for w = s\n\u2032 where s 6= s\u2032 we have \u00b5\u03c3s (Cone(w)) = 0; and for w\u2032 = w \u00b7 a \u00b7 s we have \u00b5\u03c3s (Cone(w\n\u2032)) = \u00b5\u03c3s (Cone(w)) \u00b7 \u03c3(w)(a) \u00b7 \u03b4(Last(w), a)(s). By Carathe\u0301odory\u2019s extension theorem, the function \u00b5\u03c3s (\u00b7) can be uniquely extended to a probability measure P\u03c3s (\u00b7) over Borel sets of infinite plays [1]. We denote by E\u03c3s [\u00b7] the expectation measure associated with the strategy \u03c3. For an initial distribution \u03bb0 we have P\u03c3\u03bb0(\u00b7) = \u2211 s\u2208S \u03bb0(s) \u00b7 P\u03c3s (\u00b7) and E\u03c3\u03bb0 [\u00b7] = \u2211 s\u2208S \u03bb0(s) \u00b7 E\u03c3s [\u00b7]. Objectives. We consider reachability and total-cost objectives.\n\u2013 Reachability objectives. A reachability objective in a POMDP G is a measurable set \u03d5 \u2286 \u2126 of plays and is defined as follows: given a set T \u2286 S of target states, the reachability objective Reach(T ) = {(s0, a0, s1, a1, s2 . . .) \u2208 \u2126 | \u2203i \u2265 0 : si \u2208 T} requires that a target state in T is visited at least once. \u2013 Total-cost and finite-length total-cost objectives. A total-cost objective is defined as follows: Let G be a POMDP with a set of absorbing target states T and a cost function c : S \u00d7 A \u2192 Z that assigns integer-valued weights to all states and actions such that for all states t \u2208 T and all actions a \u2208 A we have c(t, a) = 0. The total-cost of a play \u03c1 = (s0, a0, s1, a1, s2, a2, . . .) is Total(\u03c1) = \u2211\u221e i=0 c(si, ai) the\nsum of the costs of the play. To analyze total-cost objectives we will also require finite-length total-cost objectives, that for a given length k sum the total costs upto length k; i.e., Totalk(\u03c1) = \u2211k i=0 c(si, ai).\nAlmost-sure winning. Given a POMDP G with a reachability objective Reach(T ) a strategy \u03c3 is almostsure winning iff P\u03c3\u03bb0(Reach(T )) = 1. We will denote by AlmostG(T ) the set of almost-sure winning strategies in POMDP G for the objective Reach(T ). Given a set U such that all states in U have the same observation, a strategy is almost-sure winning from U , if given the uniform probability distribution \u03bbU over U we have P\u03c3\u03bbU (Reach(T )) = 1; i.e., the strategy ensures almost-sure winning if the starting belief-support is U . Optimal cost under almost-sure winning and approximations. Given a POMDP G with a reachability objective Reach(T ) and a cost function c we are interested in minimizing the expected total cost before reaching the target set T , while ensuring that the target set is reached almost-surely. Formally, the value of an almost-sure winning strategy \u03c3 \u2208 AlmostG(T ) is the expectation Val(\u03c3) = E\u03c3\u03bb0 [Total]. The optimal cost is defined as the infimum of expected costs among all almost-sure winning strategies: optCost = inf\u03c3\u2208AlmostG(T ) Val(\u03c3). We consider the computational problems of approximating optCost and compute strategies \u03c3 \u2208 AlmostG(T ) such that the value Val(\u03c3) approximates the optimal cost optCost. Formally, given > 0, the additive approximation problem asks to compute a strategy \u03c3 \u2208 AlmostG(T ) such that Val(\u03c3) \u2264 optCost+ ; and the multiplicative approximation asks to compute a strategy \u03c3 \u2208 AlmostG(T ) such that Val(\u03c3) \u2264 optCost \u00b7 (1 + ).\nRemark 1. We remark about some of our notations.\n1. Rational costs: We consider integer costs as compared rational costs, and given a POMDP with rational costs one can obtain a POMDP with integer costs by multiplying the costs with the least common multiple of the denominators. The transformation is polynomial given binary representation of numbers. 2. Probabilistic observations: Given a POMDP G = (S,A, \u03b4,Z,O, \u03bb0), the most general type of the observation function O considered in the literature is of type S \u00d7 A \u2192 D(Z), i.e., the state and the action gives a probability distribution over the set of observations Z. We show how to transform the POMDP G into one where the observation function is deterministic and defined on states, i.e., of type S \u2192 Z as in our definitions. We construct an equivalent POMDP G\u2032 = (S\u2032,A, \u03b4\u2032,Z,O\u2032, \u03bb\u20320) as follows: (i) the new state space is S\u2032 = S \u00d7 Z; (ii) the transition function \u03b4\u2032 given a state (s, z) \u2208 S\u2032 and an action a is as follows \u03b4\u2032((s, z), a)(s\u2032, z\u2032) = \u03b4(s, a)(s\u2032) \u00b7 O(s\u2032, a)(z\u2032); and (iii) the deterministic observation function for a state (s, z) \u2208 S\u2032 is defined as O\u2032((s, z)) = z. Informally, the probabilistic aspect of the observation function is captured in the transition function, and by enlarging the state space with the product with the observations, we obtain an observation function only on states. Thus we consider observation on states which greatly simplifies the notation. 3. Strategies: Note that in our definition of strategies, the strategies operate on state action sequences, rather than observation action sequences. However, since we restrict strategies to be observation based, in effect they operate on observation action sequences.\n3 Approximating optCost for Integer Costs\nIn this section we will show that the problem of approximating the optimal cost optCost is undecidable. We will show that deciding whether the optimal cost optCost is \u2212\u221e or not is undecidable in POMDPs with integer costs. We present a reduction from the standard undecidable problem for probabilistic finite automata (PFA). A PFA P = (S,A, \u03b4, F, s0) is a special case of a POMDP G = (S,A, \u03b4,Z,O, s0) with a single observation Z = {z} such that for all states s \u2208 S we have O(s) = z. Moreover, the PFA proceeds for only finitely many steps, and has a set F of desired final states. The strict emptiness problem asks for the existence of a strategy w (a finite word over the alphabet A) such that the measure of the runs ending in the desired final states F is strictly greater than 12 ; and the strict emptiness problem for PFA is undecidable [26]. Reduction. Given a PFA P = (S,A, \u03b4, F, s0) we construct a POMDP G = (S\u2032,A\u2032, \u03b4\u2032,Z,O, s\u20320) with a cost function c and a target set T such that there exists a word w \u2208 A\u2217 accepted with probability strictly greater than 12 in PFA P iff the optimal cost in the POMDP G is \u2212\u221e. Intuitively, the construction of the POMDP G is as follows: for every state s \u2208 S of P we construct a pair of states (s, 1) and (s,\u22121) in S\u2032 with the property that (s,\u22121) can only be reached with a new action $ (not in A) played in state (s, 1). The transition function\n\u03b4\u2032 from the state (s,\u22121) mimics the transition function \u03b4, i.e., \u03b4\u2032((s,\u22121), a)((s\u2032, 1)) = \u03b4(s, a)(s\u2032). The cost c of (s, 1) (resp. (s,\u22121)) is 1 (resp. \u22121), ensuring the sum of the pair to be 0. We add a new available action # that when played in a final state reaches a newly added state good \u2208 S\u2032, and when played in a non-final state reaches a newly added state bad \u2208 S\u2032. For states good and bad given action # the next state is the initial state; with negative cost \u22121 for good and positive cost 1 for bad. We introduce a single absorbing target state T = {target} and give full power to the player to decide when to reach the target state from the initial state, i.e., we introduce a new action \u221a that when played in the initial state deterministically reaches the target state target. An illustration of the construction on an example is depicted on Figure 1. Whenever an action is played in a state where it is not available, the POMDP reaches a losing absorbing state, i.e., an absorbing state with cost 1 on all actions, and for brevity we omit transitions to the losing absorbing state. The formal construction of the POMDP G is as follows:\n\u2013 S\u2032 = (S \u00d7 {\u22121, 1}) \u222a {good, bad, target}, \u2013 s\u20320 = (s0, 1), \u2013 A\u2032 = A \u222a {#, $, \u221a }, \u2013 The actions a \u2208 A \u222a {#} in states (s, 1) (for s \u2208 S) lead to the losing absorbing state; the action $ in states (s,\u22121) (for s \u2208 S) leads to the losing absorbing state; and the actions a \u2208 A \u222a {$} in states good and bad lead to the losing absorbing state. The action \u221a played in any state other than the initial\nstate s\u20320 also leads to the losing absorbing state. The other transitions are as follows: For all s \u2208 S: (i) \u03b4\u2032((s, 1), $)((s,\u22121)) = 1, (ii) for all a \u2208 A we have \u03b4\u2032((s,\u22121), a)((s\u2032, 1)) = \u03b4(s, a)(s\u2032), and (iii) for action # and \u221a we have\n\u03b4\u2032((s,\u22121),#)(good) = { 1 if s \u2208 F ; 0 otherwise;\n\u03b4\u2032((s,\u22121),#)(bad) = { 1 if s 6\u2208 F ; 0 otherwise;\n\u03b4\u2032(good,#)(s\u20320) = 1; \u03b4 \u2032(bad,#)(s\u20320) = 1; \u03b4 \u2032(s\u20320, \u221a )(target) = 1;\n\u2013 there is a single observation Z = {o}, and all the states s \u2208 S\u2032 have O(s) = o.\nWe define the cost function c assigning only two different costs and only as a function of the state, i.e., c : S\u2032 \u2192 {\u22121, 1} and show the undecidability even for this special case of cost functions. For all s \u2208 S the cost is c((s,\u22121)) = \u22121, and similarly c((s, 1)) = 1, and the remaining states have costs c(good) = \u22121 and c(bad) = 1. The absorbing target state has cost 0; i.e., c(target) = 0. Note that though the costs are assigned as function of states, the costs appear on the out-going transitions of the respective states.\nIntuitive proof idea. The basic idea of the proof is as follows: Consider a word w accepted by the PFA with probability at least \u03bd > 12 . Let the length of the word be |w| = n, and w[i] denote the i\nth letter in w. Consider a strategy in the POMDP u = ($ w[1] $ w[2] . . . $w[n] # #)k \u221a for some constant k \u2265 0; that plays alternately the letters in w and $, then two #\u2019s, repeat the above k times, and finally plays \u221a\n. For any \u03c4 < 0, for k \u2265 \u03c4\u221211\u22122\u00b7\u03bd , the expected total cost is below \u03c4 . Hence if the answer to the strict emptiness problem is yes, then the optimal cost is \u2212\u221e. Conversely, if there is no word accepted with probability strictly greater than 12 , then the expected total cost between consecutive visits to the starting state is positive, and hence the optimal cost is at least 1. We now formalize the intuitive proof idea.\nLemma 1. If there exists a word w \u2208 A\u2217 that is accepted with probability strictly greater than 12 in P, then the optimal cost optCost in the POMDP G is \u2212\u221e.\nProof. Let w \u2208 A\u2217 be a word that is accepted in P with probability \u03bd > 12 and let \u03c4 \u2208 R be any negative real-number threshold. We will construct a strategy in POMDP G ensuring that the target state target is reached almost-surely and the value of the strategy is below \u03c4 . As this is will be true for every \u03c4 < 0 it will follow that the optimal cost optCost is \u2212\u221e.\nLet the length of the word be |w| = n. We construct a pure finite-memory strategy in the POMDP G as follows: We denote by w[i] the ith action in the word w. The finite-memory strategy we construct is specified\nas a word u = ($ w[1] $ w[2] . . . $w[n] # #)k \u221a\nfor some constant k \u2265 0, i.e., the strategy plays alternately the letters in w and $, then two #\u2019s, repeat the above k times, and finally plays \u221a . Observe that by the construction of the POMDP G, the sequence of costs (that appear on the transitions) is (1,\u22121)n followed by (i) \u22121 with probability \u03bd (when F is reached), and (ii) +1 otherwise; and the whole sequence is repeated k times.\nLet r1, r2, r3, . . . rm be the finite sequence of costs and sj = \u2211j i=1 ri. The sequence of costs can be partitioned into blocks of length 2 \u00b7 n+ 1, intuitively corresponding to the transitions of a single run on the word ($w[1] $w[2] . . . $w[n] # #). We define a random variable Xi denoting the sum of costs of the i\nth block in the sequence, i.e., with probability \u03bd for all i the value of Xi is \u22121 and with probability 1\u2212 \u03bd the value is 1. The expected value of Xi is therefore equal to E[Xi] = 1\u2212 2 \u00b7 \u03bd, and as we have that \u03bd > 12 it follows that E[Xi] < 0. The fact that after the ## the initial state is reached implies that the random variable sequence (Xi)0\u2264i\u2264k is a finite sequence of i.i.d\u2019s. By linearity of expectation we have that the expected total cost of the word u is k \u00b7 E[Xi] plus an additional 1 for the last \u221a action. Therefore, by choosing an appropriately large k (in particular for k \u2265 \u03c4\u221211\u22122\u00b7\u03bd ) we have the expected total cost is below \u03c4 . As playing the \u221a\naction from the initial state reaches the target state target almost-surely, and after ## the initial state is reached almost-surely, we have that by playing according to the strategy u the target state target is reached almostsurely. The desired result follows. ut\nWe now show that pure finite-memory strategies are sufficient for the POMDP we constructed from the probabilistic automata, and then prove a lemma that proves the converse of Lemma 1.\nLemma 2. Given the POMDP G of our reduction from the PFA, if there is a randomized (possibly infinitememory) strategy \u03c3 with Val(\u03c3) < 1, then there exists a pure finite-memory strategy \u03c3\u2032 with Val(\u03c3\u2032) < 1.\nProof. Let \u03c3 be a randomized (possibly infinite-memory) strategy with the expected total cost strictly less than 1. As there is a single observation in the POMDP G constructed in our reduction, the strategy does not receive any useful feedback from the play, i.e., the memory update function \u03c3u always receives as one of the parameters the unique observation. Note that with probability 1 the resolving of the probabilities in the strategy \u03c3 leads to finite words of the form \u03c1 = w1##w2## . . .##wn## \u221a , as otherwise the target state target is not reached with probability 1. From each such word \u03c1 we extract the finite words w1, w2, . . . , wn that occur in \u03c1, and consider the union of all such words as W (\u03c1), and then consider the union W over all such words \u03c1. We consider two cases:\n1. If there exists a word v in W such that the expected total cost after playing the word v## is strictly less than 0, then the pure finite-memory strategy v## \u221a ensures that the expected total cost strictly\nless than 1. 2. Assume towards contradiction that for all the words v in W the expected total cost of v## is at least\n0. Then with probability 1 resolving the probabilities in the strategy \u03c3 leads to finite words of the form w = w1##w2## . . . wn## \u221a , where each word wi belongs to W , that is played on the POMDP G. Let us define a random variable Xi denoting the sum between i and (i + 1)-th occurrence of ##. The expected total cost of wi## is E[Xi] and is at least 0 for all i. Therefore the expected cost of the sequence\nw (which has \u221a\nin the end with cost 1) is at least 1. Thus we arrive at a contradiction. Hence, there must exist a word v in W such that v## has an expected total cost strictly less than 0.\nThis concludes the proof. ut\nLemma 3. If there exists no word w \u2208 A\u2217 that is accepted with probability strictly greater than 12 in P, then the optimal cost optCost = 1. Proof. We will show that playing \u221a\nis an optimal strategy. It reaches the target state target almost-surely with an expected total cost of 1. Assume towards contradiction that there exists a strategy (and by Lemma 2, a pure finite-memory strategy) \u03c3 with the expected total cost strictly less than 1. Observe that as there is only a single observation in the POMDP G the pure finite-memory strategy \u03c3 can be viewed as a finite word of the form w1##w2## . . . wn## \u221a . We extract the set of words W = {w1, w2, . . . , wn} from the strategy \u03c3. By the condition of the lemma, there exists no word accepted in the PFA P with probability strictly greater that 12 . As in Lemma 1 we define a random variable Xi denoting the sum of costs after reading wi##. It follows that the expected value of E[Xi] \u2265 0 for all 0 \u2264 i \u2264 n. By using the linearity of expectation we have the expected total cost of w1##w2## . . . wn## is at least 0, and hence the expected total cost of the strategy is at least 1 due to the cost of the last \u221a action. Thus we have a contradiction to the assumption that the expected total cost of strategy \u03c3 is strictly less than 1. ut\nThe above lemmas establish that if the answer to the strict emptiness problem for PFA is yes, then the optimal cost in the POMDP is \u2212\u221e; and otherwise the optimal cost is 1. Hence in POMDPs with integer costs determining whether the optimal cost optCost is \u2212\u221e or 1 is undecidable, and thus the problem of approximation is also undecidable.\nTheorem 1. The problem of approximating the optimal cost optCost in POMDPs with integer costs is undecidable for all > 0 both for additive and multiplicative approximation.\n4 Approximating optCost for Positive Costs\nIn this section we consider POMDPs with positive cost functions, i.e., c : S\u00d7A \u2192 N instead of c : S\u00d7A \u2192 Z. Note that the transitions from the absorbing target states have cost 0 as the goal is to minimize the cost till the target set is reached, and also note that all other transitions have cost at least 1. We established (in Theorem 1) that for integer costs the problem of approximating the optimal cost is undecidable, and in this section we show that for positive cost functions the approximation problem is decidable. We first start with a lower bound on the optimal cost.\n4.1 Lower Bound on optCost\nWe present a double-exponential lower bound on optCost with respect to the number of states of the POMDP. We define a family of POMDPs F(n), for every n, with a single target state, such that there exists an almostsure winning strategy, and for every almost-sure winning strategy the expected number of steps to reach the target state is double-exponential in the number of states of the POMDP. Thus assigning cost 1 to every transition we obtain the double-exponential lower bound.\nPreliminary. The action set we consider consists of two symbols A = {a,#}. The state space consists of an initial state s0, a target state target, a losing absorbing state bad and a set of n sub-POMDPs Li for 1 \u2264 i \u2264 n. Every sub-POMDP Li consists of states Qi that form a loop of p(i) states qi1, qi2, . . . qip(i), where p(i) denotes the i-th prime number and qi1 is the initial state of the sub-POMDP. For every state q i j (for 1 \u2264 j \u2264 p(i)) the transition function under action a moves the POMDP to the state qi(j mod p(i))+1 with probability 12 and to the initial state s0 with the remaining probability 1 2 . The action # played in the state qip(i) moves the POMDP to the target state target with probability 1 2 and to the initial state s0 with the remaining probability 12 . For every other state in the loop q i j such that 1 \u2264 j < p(i) the POMDP moves\nunder action # to the losing absorbing state bad with probability 1. The losing state bad and the target state target are absorbing and have a self-loop under both actions with probability 1.\nPOMDP family F(n). Given an n \u2208 N we define the POMDP F(n) as follows:\n\u2013 The state space S = Q1 \u222aQ2 \u222a . . . Qn \u222a {s0, bad, target}. \u2013 There are two available actions A = {a,#}. \u2013 The transition function is defined as follows: action a in the initial state leads to bad with probability\n1 and action # in the initial state leads with probability 1n to the initial state of the sub-POMDP Li for every 1 \u2264 i \u2264 n. The transitions for the states in the sub-POMDPs are described in the previous paragraph. \u2013 All the states in the sub-POMDPs Li do have the same observation z. The remaining states s0, bad, and target are visible, i.e., each of these three states has its own observation. \u2013 The initial state is s0.\nThe cost function c is defined as follows: the self-loop transitions at target have cost 0 and all other transitions have cost 1. An example of the construction for n = 2 is depicted in Figure 2, where we omit the losing absorbing state bad and the transitions leading to bad for simplicity.\nIntuitive proof idea. For a given n \u2208 N let p\u2217n = \u220fn i=1 p(i) and p + n = \u2211n i=1 p(i) denote the product and the sum of the first n prime numbers, respectively. Note that p\u2217n is exponential is p + n . An almost-sure winning strategy must play as follows: in the initial state s0 it plays #, and then if it observes the observation z for at least p\u2217n consecutive steps, then for each step it must play action a, and at the p \u2217 n step it can play action #. Hence the probability to reach the target state in p\u2217n steps is at most ( 1 2 \u00b7 ( 1 2 ) p\u2217n); and hence the expected number of steps to reach the target state is at least p\u2217n \u00b7 2 \u00b7 2p \u2217 n . The size of the POMDP is polynomial in p+n and thus the expected total cost is double exponential.\nLemma 4. There exists a family (F(n))n\u2208N of POMDPs of size O(p(n)) for a polynomial p with a reachability objective, such that the following assertion holds: There exists a polynomial q such that for every almost-sure winning strategy the expected total cost to reach the target state is at least 22 q(n) .\nProof. For n \u2208 N, consider the POMDP F(n), and an almost-sure winning strategy in the POMDP. In the first step the strategy needs to play the # action from s0, as otherwise the losing absorbing state is reached. The POMDP reaches the initial state of the sub-POMDPs Li, for all i, with positive probability. As all the states in the sub-POMDPs have the same observation z, the strategy cannot base its decision on the current sub-POMDP. The strategy has to play the action a until the observation z is observed for p\u2217n steps in a row before playing action #. If the strategy plays the action # before observing the sequence of z observations p\u2217n times, then it reaches the losing absorbing state with positive probability (and would not have been an almost-sure winning strategy). This follows from the fact that there is a positive probability of being in a sub-POMDP, where the current state is not the last one of the loop. Hence an almost-sure winning strategy must play a as long as the length of the sequence of the observation z is less than p\u2217n consecutive steps. Note that in between the POMDP can move to the initial state s0, and the strategy restarts. In every step of the sub-POMDPs, with probability 12 the initial state is reached, and the next state is in the sub-POMDP with probability 12 . After observing the z observation for p \u2217 n consecutive steps, the strategy can play the action # that moves the POMDP to the target state target with probability 12 and restarts the POMDP with the remaining probability 12 . Therefore the probability of reaching the target state in p \u2217 n steps is at most ( 12 \u00b7 ( 1 2 ) p\u2217n); and hence the expected number of steps to reach the target state is at least p\u2217n \u00b7 2 \u00b7 2p \u2217 n . The size of the POMDP is polynomial in p+n and hence it follows that the expected total cost to reach the target state is at least double exponential in the size of the POMDP. ut\n4.2 Upper Bound on optCost\nIn this section we will present a double-exponential upper bound on optCost.\nAlmost-sure winning belief-supports. Let Belief(G) denote the set of all belief-supports in a POMDP G, i.e., Belief(G) = {U \u2286 S | \u2203z \u2208 Z : U \u2286 O\u22121(z)}. Let BeliefWin(G,T ) denote the set of almost-sure winning belief-supports, i.e., BeliefWin(G,T ) = {U \u2208 Belief(G) | there exists an almost-sure winning strategy from U }, i.e., there exists an almost-sure winning strategy with initial distribution \u03bbU that is the uniform distribution over U .\nRestricting to BeliefWin(G,T ). In the sequel without loss of generality we will restrict ourselves to beliefsupports in BeliefWin(G,T ): since from belief-supports outside BeliefWin(G,T ) there exists no almost-sure winning strategy, all almost-sure winning strategies with starting belief-support in BeliefWin(G,T ) will ensure that belief-supports not in BeliefWin(G,T ) are never reached. Belief updates. Given a belief-support U \u2208 Belief(G), an action a \u2208 A, and an observation z \u2208 Z we denote by Update(U, z, a) the updated belief-support. Formally, the set Update(U, z, a) is defined as follows: Update(U, z, a) = \u22c3 s\u2032\u2208U Supp(\u03b4(s\n\u2032, a)) \u2229 O\u22121(z). The set of belief-supports reachable from U by playing an action a \u2208 A is denoted by Update(U, a). Formally, Update(U, a) = {U \u2032 \u2286 S | \u2203z \u2208 Z : U \u2032 = Update(U, z, a))\u2227 U \u2032 6= \u2205}. Allowed actions. Given a POMDP G and a belief-support U \u2208 BeliefWin(G,T ), we consider the set of actions that are guaranteed to keep the next belief-support U \u2032 in BeliefWin(G,T ) and refer these actions as allowed or safe. The framework that restricts playable actions was also considered in [3]. Formally we consider the set of allowed actions as follows: Given a belief-support U \u2208 BeliefWin(G,T ) we define Allow(U) = {a \u2208 A | \u2200U \u2032 \u2208 Update(U, a) : U \u2032 \u2208 BeliefWin(G,T )}.\nWe now show that almost-sure winning strategies must only play allowed actions. An easy consequence of the lemma is that for all belief-supports U in BeliefWin(G,T ), there is always an allowed action.\nLemma 5. Given a POMDP with a reachability objective Reach(T ), consider a strategy \u03c3 and a starting belief-support in BeliefWin(G,T ). Given \u03c3, if for a reachable belief-support U \u2208 BeliefWin(G,T ) the strategy \u03c3 plays an action a not in Allow(U) with positive probability, then \u03c3 is not almost-sure winning for the reachability objective.\nProof. Assume the strategy \u03c3 reaches the belief-support U and plays an action a 6\u2208 Allow(U). Since the belief-support U is reachable, it follows that given the strategy \u03c3 when the belief-support is U , all states in U are reached with positive probability, i.e., given the strategy the belief-support U is reached with positive probability. It follows from the definition of Allow that there exists a belief-support U \u2032 \u2208 Update(U, a) that is not in BeliefWin(G,T ). By the definition of Update there exists an observation z \u2208 Z such that U \u2032 = Update(U, z, a) and U \u2032 6= \u2205. It follows that by playing a in belief-support U , there is a positive probability of observing observation z and reaching belief-support U \u2032 that does not belong to BeliefWin(G,T ).\nIt follows that under action a, given the current belief-support is U , the next belief-support is U \u2032 with positive probability. By definition, for all belief-supports U \u2032 that does not belong to BeliefWin(G,T ), if the starting belief-support is U \u2032, then for all strategies the probability to reach T is strictly less than 1. Hence if U \u2032 is reached with positive probability from U under action a, then \u03c3 is not almost-sure winning. The desired result follows. ut\nCorollary 1. For all U \u2208 BeliefWin(G,T ) we have Allow(U) 6= \u2205.\nExample 1. Consider POMDP G depicted on Figure 3 with three states: the initial state s0, and two absorbing states (the target state T , and the loosing state B). There are two actions a and b available in the initial state, the first action a leads to both the target state and the loosing state, each with probability 1/2, while the second action b leads to the initial state and the target state with probability 1/2 each. This POMDP is not a goal-POMDP, as the target state T is not reachable from the loosing state B. Note that belief {s0} belongs to the set BeliefWin(G,T ), as the strategy that plays only action a reaches the target state T almost-surely. The set of allowed actions Allow({s0}) does not contain action b, as any belief that contains the loosing state B does not belong to the set BeliefWin(G,T ).\nMarkov chains and reachability. A Markov chain G = (S, \u03b4) consists of a finite set S of states and a probabilistic transition function \u03b4 : S \u2192 D(S). Given the Markov chain, we consider the directed graph (S,E) where E = {(s, s\u2032) | \u03b4(s\u2032 | s) > 0}. The following standard properties of reachability in Markov chains will be used in our proofs [15]:\n1. Property 1 of Markov chains. For a set T \u2286 S, if for all states s \u2208 S there is a path to T (i.e., for all states there is a positive probability to reach T ), then from all states the set T is reached with probability 1. 2. Property 2 of Markov chains. In a Markov chain if a set T is reached almost-surely from s, then the expected hitting time from s to T is at most exponential in the number of the states of the Markov chain.\nThe strategy \u03c3Allow. We consider a belief-support-based stationary (for brevity belief-based) 1 strategy \u03c3Allow as follows: for all belief-supports U in BeliefWin(G,T ), the strategy plays uniformly at random all actions from Allow(U). Note that as the strategy \u03c3Allow is belief-based, it can be viewed as a finite-memory strategy \u03c3Allow = (\u03c3u, \u03c3n,BeliefWin(G,T ),m0), where the components are defined as follows: (i) The set of memory elements are the winning belief-supports BeliefWin(G,T ); (ii) the belief-support m0 \u2208 BeliefWin(G,T ) is the initial belief (i.e., Supp(\u03bb0)); (iii) the action selection function given memory U \u2208 BeliefWin(G,T ) is a uniform distribution over the set Allow(U) of actions, i.e., \u03c3n(U) = unif(Allow(U)) where unif denotes the uniform distribution; and (iv) the memory update function given memory U \u2208 BeliefWin(G,T ), observation z \u2208 Z, and action a \u2208 Allow(U) is defined as the belief-support update U \u2032 from belief-support U under action a and observation z, i.e., \u03c3u(U, z, a) = Update(U, z, a). The Markov chain G \u03c3Allow. Given a POMDP G = (S,A, \u03b4,Z,O, s0) and the strategy \u03c3Allow = (\u03c3u, \u03c3n,BeliefWin(G,T ),m0) the Markov chain G \u03c3Allow = (S, \u03b4) obtained by playing strategy \u03c3Allow in G is defined as follows:\n1 recall, for a belief-support-based stationary strategy, the probability distribution only depends on the current belief-support\n\u2013 The set of states S is defined as follows: S = {(s, U) | U \u2208 BeliefWin(G,T ), s \u2208 U}, i.e., the second components are the almost-sure winning belief-supports and the first component is a state in the beliefsupport.\n\u2013 The probability that the next state is (s\u2032, U \u2032) from a state (s, U) is \u03b4((s, U))((s\u2032, U \u2032)) = \u2211 a\u2208A \u03c3n(U)(a) \u00b7\n\u03b4(s, a)(s\u2032) \u00b7 \u03c3u(U,O(s\u2032), a)(U \u2032).\nThe probability of transition can be decomposed as follows: (i) First an action a \u2208 A is sampled according to the distribution \u03c3n(U); (ii) then the next state s\n\u2032 is sampled according to the distribution \u03b4(s, a); and (iii) finally the new memory U \u2032 is sampled according to the distribution \u03c3u(U,O(s\u2032), a).\nRemark 2. Note that due to the definition of the strategy \u03c3Allow (that only plays allowed actions) all states (s\u2032, U \u2032) of the Markov chain G \u03c3Allow that are reachable from a state (s, U) where s \u2208 U and U \u2208 BeliefWin(G,T ) satisfy that s\u2032 \u2208 U \u2032 and U \u2032 \u2208 BeliefWin(G,T ).\nLemma 6. The belief-based strategy \u03c3Allow is an almost-sure winning strategy for all belief-supports U \u2208 BeliefWin(G,T ) for the objective Reach(T ).\nProof. Consider the Markov chain G \u03c3Allow and a state (s, U) of the Markov chain. As U \u2208 BeliefWin(G,T ) by the definition of almost-sure winning belief-supports, there exists a strategy \u03c3 that is almost-sure winning for the reachability objective Reach(T ) starting with belief-support U .\nReachability under \u03c3. Note that by Lemma 5 the strategy must only play allowed actions. The strategy must ensure that from s a target state t is reached with positive probability by playing according to \u03c3 (given the initial belief-support is U). It follows that there exists a finite prefix of a play (s1, a1, s2, a2, . . . an\u22121, sn) induced by \u03c3 where s1 = s and sn = t and for all 1 \u2264 i < n we have that ai \u2208 Supp(\u03c3(O((s1, a1, s2, a2, . . . , si)))). We define a sequence of belief-supports U1, U2, . . . Un, where U1 = U and Ui+1 = Update(Ui,O(si+1), ai). As \u03c3 is an almost-sure winning strategy, it follows from Lemma 5 that ai \u2208 Allow(Ui) for all 1 \u2264 i < n. Reachability in the Markov chain. Recall that the strategy \u03c3Allow plays all the allowed actions uniformly at random. Hence it follows from the definition of the Markov chain G \u03c3Allow that for all 0 \u2264 i < n we have \u03b4((si, Ui))((si+1, Ui+1)) > 0, i.e, there is a positive probability to reach (t, Un) from (s, U) in the Markov chain G \u03c3Allow. It follows that for an arbitrary state (s, U) of the Markov chain G \u03c3Allow there exists a state (t\u2032, U \u2032) with t\u2032 \u2208 T that is reached with positive probability. In other words, in the graph of the Markov chain, there is a path from all states (s, U) to a state (t\u2032, U \u2032) where t\u2032 \u2208 T . Thus by Property 1 of Markov chains it follows that the target set T is reached with probability 1. It follows that \u03c3Allow is an almost-sure winning strategy for all belief-supports in BeliefWin(G,T ). The desired result follows. ut\nRemark 3 (Computation of \u03c3Allow). It follows from Lemma 6 that the strategy \u03c3Allow can be computed by computing the set of almost-sure winning states in the belief-support MDP. The belief-support MDP is a perfect-observation MDP where each state is a belief-support of the original POMDP, and given an action, the next state is obtained according to the belief-support updates. The strategy \u03c3Allow can be obtained by computing the set of almost-sure winning states in the belief-support MDP, and for discrete graph-based algorithms to compute almost-sure winning states in perfect-observation MDPs see [7,6].\nUpper bound. We now establish a double-exponential upper bound on optCost, matching our lower bound from Lemma 4. We have that \u03c3Allow \u2208 AlmostG(T ). Hence we have Val(\u03c3Allow) \u2265 inf\u03c3\u2208AlmostG(T ) Val(\u03c3) = optCost. Once \u03c3Allow is fixed, since the strategy is belief-based (i.e., depends on the subset of states) we obtain an exponential size Markov chain. It follows from Property 2 of Markov chains that given \u03c3Allow the expected hitting time to the target set is at most double exponential. If cmax denotes the maximal cost of transitions, then optCost is bounded by cmax times the expected hitting time. Thus we obtain the following lemma.\nLemma 7. Given a POMDP G with n states, let cmax denote the maximal value of the cost of all transitions. There is a polynomial function q such that optCost \u2264 22q(n) \u00b7 cmax."}, {"heading": "4.3 Optimal finite-horizon strategies", "text": "Our algorithm for approximation of optCost will use algorithms for optimizing the finite-horizon costs. We first recall the well-known construction of the optimal finite-horizon strategies that minimizes the expected total cost in POMDPs for length k.\nInformation state. For minimizing the expected total cost, strategies based on information states are sufficient [32]. An information state b is defined as a probability distribution over the set of states, where for s \u2208 S the value b(s) denotes the probability of being in state s. We will denote by H the set of all information states. Given an information state b, an action a, and an observation z, computing the resulting information state b\u2032 can be done in a straight forward way, see [5].\nValue-iteration algorithm. The standard finite-horizon value-iteration algorithm for expected total cost in the setting of perfect-information MDPs can be formulated by the following equation:\nV \u22170 (s) = 0;\nV \u2217n (s) = min a\u2208A\n[ c(s, a) +\n\u2211 s\u2032\u2208S \u03b4(s, a)(s\u2032)V \u2217n\u22121(s \u2032)\n] ;\nwhere V \u2217n (s) represents the value of an optimal policy, when the starting state is s and there are n decision steps remaining. For a POMDP the finite-horizon value-iteration algorithm works on the information states. Let \u03c8(b, a) denote the probability distribution over the information states given that action a was played in the information state b. The cost function c\u2032 : H \u00d7A \u2192 N that maps every pair of an information state and an action to a positive real-valued cost is defined as follows: c\u2032(b, a) = \u2211 s\u2208S b(s) \u00b7 c(s, a). The resulting equation for finite-horizon value-iteration algorithm for POMDPs is as follows:\nV \u22170 (b) = 0;\nV \u2217n (b) = min a\u2208A\n[ c\u2032(b, a) +\n\u2211 b\u2032\u2208H \u03c8(b, a)(b\u2032)V \u2217n\u22121(b \u2032)\n] .\nThe optimal strategy \u03c3FOk and \u03c3 \u2217 k. In our setting we modify the standard finite-horizon value-iteration algorithm by restricting the optimal strategy to play only allowed actions and restrict it only to beliefsupports in the set BeliefWin(G,T ). The equation for the value-iteration algorithm is defined as follows:\nV \u22170 (b) = 0;\nV \u2217n (b) = min a\u2208Allow(Supp(b))\n[ c\u2032(b, a) +\n\u2211 b\u2032\u2208H \u03c8(b, a)(b\u2032)V \u2217n\u22121(b \u2032)\n] .\nWe obtain a strategy \u03c3FOk = (\u03c3u, \u03c3n,M,m0) that is finite-horizon optimal for length k (here FO stands for finite-horizon optimal) from the above equation as follows: (i) the set of memory elements M is defined as H\u00d7N; (ii) the initial memory state is (\u03bb0, k); (iii) for all 1 \u2264 n \u2264 k, the action selection function \u03c3n((b, n))\nselects an arbitrary action a such that a = arg mina\u2208Allow(Supp(b))\n[ c\u2032(b, a) +\n\u2211 b\u2032\u2208H \u03c8(b, a)(b\u2032)V \u2217n\u22121(b \u2032)\n] ; and\n(iv) the memory update function given a memory state (b, n), action a, and an observation o updates to a memory state (b\u2032, n \u2212 1), where b\u2032 is the unique information state update from information state b under action a and observation z. As the target states T in the POMDP G are absorbing and the costs on all outgoing edges from the target states are the only edges with cost 0, it follows that for sufficiently large n the strategy \u03c3kFO minimizes the expected total cost to reach the target set T . Given \u03c3 FO k , we define a strategy \u03c3\u2217k as follows: for the first k steps, the strategy \u03c3 \u2217 k plays as the strategy \u03c3 FO k , and after the first k steps the strategy plays as the strategy \u03c3Allow.\nLemma 8. For all k \u2208 N the strategy \u03c3\u2217k is almost-sure winning for the reachability objective Reach(T ).\nProof. By definition the strategy \u03c3FOk (and hence the strategy \u03c3 \u2217 k) plays only allowed actions in the first k steps. Hence it follows that every reachable belief-support in the first k steps belongs to BeliefWin(G,T ). After the first k steps, the strategy plays as \u03c3Allow, and by Lemma 6, the strategy \u03c3Allow is almost-sure winning for the reachability objective Reach(T ) from every belief-support in BeliefWin(G,T ). The result follows. ut\nNote that the only restriction in the construction of the strategy \u03c3FOk is that it must play only allowed actions, and since almost-sure winning strategies only play allowed actions (by Lemma 5) it follows that \u03c3FOk (and hence \u03c3 \u2217 k) is optimal for the finite-horizon of length k (i.e., for the objective Totalk) among all almost-sure winning strategies.\nLemma 9. For all k \u2208 N we have E\u03c3 \u2217 k\n\u03bb0 [Totalk] = inf\u03c3\u2208AlmostG(T ) E\u03c3\u03bb0 [Totalk].\nNote that since in the first k steps \u03c3\u2217k plays as \u03c3 FO k we have the following proposition.\nProposition 1. For all k \u2208 N we have E\u03c3 \u2217 k\n\u03bb0 [Totalk] = E \u03c3FOk \u03bb0 [Totalk]."}, {"heading": "4.4 Approximation algorithm", "text": "In this section we will show that for all > 0 there exists a bound k such that the strategy \u03c3\u2217k approximates optCost within . First we consider an upper bound on optCost. Bound UAllow. We consider an upper bound UAllow on the expected total cost of the strategy \u03c3Allow starting in an arbitrary state s \u2208 U with the initial belief-support U \u2208 BeliefWin(G,T ). Given a belief-support U \u2208 BeliefWin(G,T ) and a state s \u2208 U let TAllow(s, U) denote the expected total cost of the strategy \u03c3Allow starting in the state s with the initial belief-support U . Then the upper bound is defined as UAllow = maxU\u2208BeliefWin(G,T ),s\u2208U TAllow(s, U). As the strategy \u03c3Allow is in AlmostG(T ) it follows that the value UAllow is also an upper bound for the optimal cost optCost. Observe that by Lemma 7 it follows that UAllow is at most double exponential in the size of the POMDP.\nLemma 10. We have optCost \u2264 UAllow.\nKey lemma. We will now present our key lemma to obtain the bound on k depending on . We start with a few notations. Given k \u2208 N, let Ek denote the event of reaching the target set within k steps, i.e., Ek = {(s0, a0, s1, a1, s2 . . .) \u2208 \u2126 | \u2203i \u2264 k : si \u2208 T}; and Ek the complement of the event Ek that denotes the target set is not reached within the first k steps. Recall that for plays \u03c1 = (s0, a0, s1, a1, s2, a2, . . .) we have\nTotalk = \u2211k i=0 c(si, ai) and we consider Totalk = \u2211\u221e i=k+1 c(si, ai) the sum of the costs after k steps. Note that we have Total = Totalk + Totalk.\nLemma 11. For k \u2208 N consider the strategy \u03c3\u2217k that is obtained by playing an optimal finite-horizon strategy \u03c3FOk for k steps, followed by strategy \u03c3Allow. Let \u03b1k = P \u03c3\u2217k \u03bb0\n(Ek) denote the probability that the target set is not reached within the first k steps. We have\nE\u03c3 \u2217 k\n\u03bb0 [Total] \u2264 E\u03c3\n\u2217 k\n\u03bb0 [Totalk] + \u03b1k \u00b7 UAllow\nProof. We have\nE\u03c3 \u2217 k\n\u03bb0 [Total] = P\u03c3\n\u2217 k\n\u03bb0 (Ek) \u00b7 E \u03c3\u2217k \u03bb0 [Total | Ek] + P \u03c3\u2217k \u03bb0 (Ek) \u00b7 E \u03c3\u2217k \u03bb0 [Total | Ek]\n= P\u03c3 \u2217 k\n\u03bb0 (Ek) \u00b7 E \u03c3\u2217k \u03bb0 [Total | Ek] + P \u03c3\u2217k \u03bb0 (Ek) \u00b7 E \u03c3\u2217k \u03bb0 [(Totalk + Totalk) | Ek]\n= P\u03c3 \u2217 k\n\u03bb0 (Ek) \u00b7 E \u03c3\u2217k \u03bb0 [Total | Ek] + P \u03c3\u2217k \u03bb0 (Ek) \u00b7 E \u03c3\u2217k \u03bb0 [Totalk | Ek] + P \u03c3\u2217k \u03bb0 (Ek) \u00b7 E \u03c3\u2217k \u03bb0 [Totalk | Ek]\n= P\u03c3 \u2217 k\n\u03bb0 (Ek) \u00b7 E \u03c3\u2217k \u03bb0 [Totalk | Ek] + P \u03c3\u2217k \u03bb0 (Ek) \u00b7 E \u03c3\u2217k \u03bb0 [Totalk | Ek] + P \u03c3\u2217k \u03bb0 (Ek) \u00b7 E \u03c3\u2217k \u03bb0 [Totalk | Ek]\n= E\u03c3 \u2217 k\n\u03bb0 [Totalk] + P \u03c3\u2217k \u03bb0 (Ek) \u00b7 E \u03c3\u2217k \u03bb0 [Totalk | Ek] \u2264 E \u03c3\u2217k \u03bb0 [Totalk] + \u03b1k \u00b7 UAllow.\nThe first equality is obtained by splitting with respect to the complementary events Ek and Ek; the second equality is obtained by writing Total = Totalk +Totalk; and the third equality is by linearity of expectation. The fourth equality is obtained as follows: since all outgoing transitions from target states have cost zero, it follows that given the event Ek we have Total = Totalk. The fifth equality is obtained by combining the first two terms. The final inequality is obtained as follows: from the (k + 1)-th step the strategy plays as \u03c3Allow and the expected total cost given \u03c3Allow is bounded by UAllow. The result follows. ut\nLemma 12. For k \u2208 N consider the strategy \u03c3\u2217k and \u03b1k (as defined in Lemma 11). The following assertions hold:\n(1) E\u03c3 \u2217 k\n\u03bb0 [Totalk] \u2264 optCost; and (2) \u03b1k \u2264\noptCost\nk .\nProof. We prove both the inequalities below.\n1. For k \u2208 N we have that\nE\u03c3 \u2217 k\n\u03bb0 [Totalk] \u2264 inf \u03c3\u2208AlmostG(T ) E\u03c3\u03bb0 [Totalk] \u2264 inf\u03c3\u2208AlmostG(T ) E\u03c3\u03bb0 [Total] = optCost\nThe first inequality is due to Lemma 9 and the second inequality follows from the fact that Totalk \u2264 Total for non-negative weights. 2. Note that \u03b1k denotes the probability that the target state is not reached within the first k steps. Since the costs are positive integers (for all transitions other than the target state transitions), given the event\nEk the total cost for k steps is at least k. Hence it follows that E \u03c3\u2217k \u03bb0\n[Totalk] \u2265 k \u00b7\u03b1k. Thus it follows from the first inequality that we have \u03b1k \u2264 optCostk .\nThe desired result follows. ut\nApproximation algorithms. Our approximation algorithm is presented as Algorithm 1.\nAlgorithm 1 ApproxAlgo Input: POMDP, > 0\n1: k \u2190 1 2: \u03c3Allow,UAllow \u2190 Compute \u03c3Allow and UAllow . See Remark 3 3: \u03c3FOk \u2190 Finite-horizon value iteration for horizon k restricted to allowed actions . See Section 4.3 4: Tk \u2190 E \u03c3FOk \u03bb0 [Totalk] 5: \u03b1k \u2190 P \u03c3FOk \u03bb0 (Ek) . Note that P \u03c3\u2217k \u03bb0 (Ek) = P \u03c3FOk \u03bb0\n(Ek) 6: Add. approx.: if \u03b1k \u00b7 UAllow \u2264 then goto line: 10 7: Mult. approx.: if \u03b1k \u00b7 UAllow \u2264 Tk \u00b7 then goto line: 10 8: k \u2190 k + 1 9: goto line: 3\n10: return Strategy \u03c3\u2217k obtained by playing \u03c3 FO k for k-steps followed by \u03c3Allow\nCorrectness and bound on iterations. Observe that by Proposition 1 we have Tk = E \u03c3FOk \u03bb0 [Totalk] = E\u03c3\u2217\u03bb0 [Totalk]. Thus by Lemma 11 and Lemma 12 (first inequality) we have E \u03c3\u2217 \u03bb0 [Total] \u2264 Tk + \u03b1k \u00b7 UAllow \u2264 optCost + \u03b1k \u00b7 UAllow (since Tk \u2264 optCost). Thus if \u03b1k \u00b7 UAllow \u2264 we obtain an additive approximation. If \u03b1k \u00b7 UAllow \u2264 Tk \u00b7 , then we have E\u03c3 \u2217\n\u03bb0 [Total] \u2264 Tk + \u03b1k \u00b7 UAllow \u2264 Tk \u00b7 (1 + ) \u2264 optCost \u00b7 (1 + ); and we\nobtain an multiplicative approximation. This establishes the correctness Algorithm 1. Finally, we present the theoretical upper bound on k that ensures stopping for the algorithm. By Lemma 12 (second inequality) we have \u03b1k \u2264 optCostk \u2264 UAllow k . Thus k \u2265 U2Allow ensures that \u03b1k \u00b7 UAllow \u2264 , and the algorithm stops both for additive and multiplicative approximation. We now summarize the main results of this section.\nTheorem 2. In POMDPs with positive costs, the additive and multiplicative approximation problems for the optimal cost optCost are decidable. Algorithm 1 computes the approximations using finite-horizon optimal strategy computations and requires at most double-exponentially many iterations; and there exists POMDPs where double-exponentially many iterations are required.\nRemark 4. Note that though the theoretical upper bound k on the number of iterations U2Allow is double exponential in the worst case, in practical examples of interest the stopping criteria is expected to be satisfied in much fewer iterations.\nRemark 5. We remark that if we consider POMDPs with positive costs, then considering almost-sure strategies is not a restriction. For every strategy that is not almost-sure winning, with positive probability the target set is not reached, and since all costs are positive, the expected cost is infinite. If every strategy is not almost-sure winning (i.e., there exists no almost-sure winning strategy to reach the target set from the starting state), then the expected cost is infinite, and if there exists an almost-sure winning strategy, we approximate the optimal cost. Thus our result is applicable to all POMDPs with positive costs. A closely related work to ours is Goal-POMDPs, and the solution of Goal-POMDPs applies to the class of POMDPs where the target state is reachable from every state (see [2, line-3, right column page 1] for the restriction of Goal-MDPs and the solution of Goal-POMDPs is reduced to Goal-MDPs). For example, in the following Section 5, the first three examples for experimental results do not satisfy the restriction of Goal-POMDPs."}, {"heading": "5 Experimental Results", "text": "Implementation. We have implemented Algorithm 1. In principle our algorithm suggests the following: First, compute the almost-sure winning belief-supports, the set of allowed actions, and \u03c3Allow; and then compute finite-horizon value iteration restricted to allowed actions. An important feature of our algorithm is its flexibility that any finite-horizon value iteration algorithm can be used for our purpose. We have implemented our approach where we first implement the computation of almost-sure winning belief-supports, allowed actions, and \u03c3Allow; and for the finite-horizon value iteration (Step 3 of Algorithm 1) we implement two approaches. The first approach is the exact finite-horizon value iteration using a modified version of POMDPSolve [4]; and the second approach is an approximate finite-horizon value iteration using a modified version of RTDP-Bel [2]; and in both the cases our straightforward modification is that the computation of the finite-horizon value iteration is restricted to allowed actions and almost-sure winning belief-supports.\nExamples for experimental results. We experimented on several well-known examples of POMDPs. The POMDP examples we considered are as follows: (A) We experimented with the Cheese maze POMDP example which was introduced in [21] and also studied in [10,20,22]. Along with the standard example, we also considered a larger maze version; and considered two cost functions: one that assign cost 1 to all transitions and the other where the cost of movement on the baseline is assigned cost 2. (B) We considered the Grid POMDP introduced in [30] and also studied in [20,25,22]. We considered two cost functions: one where all costs are 1 and the other where transitions in narrow areas are assigned cost 2. (C) We experimented with the robot navigation problem POMDP introduced in [20], where we considered both deterministic transition and a randomized version. We also considered two cost functions: one where all costs are assigned 1 and the other where costs of turning is assigned cost 2. (D) We consider the Hallway example from [20,33,31,2]. (E) We consider the RockSample example from [2,31].\nDiscussion on Experimental results. Our experimental results are shown in Table 1, where we compare our approach to RTDP-Bel [2]. Other approaches such as SARSOP [18], anytime POMDP [27], ZMDP [31] are for discounted setting, and hence are different from our approach. The RTDP-Bel approach works only for Goal-POMDPs where from every state the goal states are reachable, and our first five examples do not fall into this category. For the first three examples, both of our exact and approximate implementation work very efficiently. For the other two larger examples, the exact method does not work since POMDP-Solve cannot handle large POMDPs, whereas our approximate method gives comparable result to RTDP-Bel. For the exact computation, we consider multiplicative approximation with = 0.1 and report the number of\niterations and the time required by the exact computation. For the approximate computation, we report the time required by the number of trials specified for the computation of the finite-horizon value iteration. For the first three examples, the obtained value of the strategies of our approximate version closely matches the value of the strategy of the exact computation, and for the last two examples, the values of the strategies obtained by our approximate version closely matches the values of the strategies obtained by RTDP-Bel.\nDetails of the POMDP examples. We now present the details of the POMDP examples.\n1. Cheese maze: The example models a simple maze, where there are four actions n, e, s, w that correspond to the movement in the four compass directions. The POMDP examples are shown in Figure 4 and\nthat can detect whether there are walls immediately adjacent to the east and to the west of the current state. The goal and the absorbing trap state have their own observations. We have again considered the setting where all the costs before reaching the target state are 1. In the second setting we have assigned to movements in the narrow areas of the maze (states 0, 1, 4, 7, and 8) cost of all actions to 2. Intuitively, the higher costs compensate for the wall bumps that can make the movement in the narrow areas of the maze more predictable.\n3. Robot navigation: The robot navigation POMDP models the movement of a robot in an environment. The robot can be in four possible states: facing north, east, south, and west. The environment has states 1, 2, 3, and a final state depicted with a star. The robot has three available actions: move forward f, turn left l, and turn right r. The original setting of the problem is that all actions are deterministic \u2013 Robot movement - det. We also consider a variant Robot movement - ran., where the attempt to make an action may fail and with probability 0.04 has no effect, i.e., the action does not change the state. The POMDP starts in a unique initial state that is not depicted in the figure and under all actions reaches the state 1 with the robot facing north, east, south or west with uniform probability. Any bump to the wall results in a damaged immobile robot, modeled by an absorbing state not depicted in the figure. There are 11 observations that correspond to what would be seen in all four directions that are adjacent to the current location. The initial state, the damaged state, and the target state have their own observations. For both variants we have considered two different cost settings. In the first setting all the costs before reaching the target state are 1. In the second setting we assign cost 1 to the move forward action, and cost 2 to the turn-left and turn-right action (i.e., turning is more costly than moving forward). 4. Hallway. We consider two versions of the Hallway example introduced in in [20] and used later in [33,31,2]. The basic idea behind both of the Hallway problems, is that there is an agent wandering around some\noffice building. It is assumed that the locations have been discretized so there are a finite number of locations where the agent could be. The agent has a small finite set of actions it can take, but these only succeed with some probability. Additionally, the agent is equipped with very short range sensors to provide it only with information about whether it is adjacent to a wall. These sensors also have the property that they are somewhat unreliable and will sometimes miss a wall or see a wall when there is none. It can \u201dsee\u201d in four directions: forward, left, right, and backward. It is important to note that these observations are relative to the current orientation of the agent (N, E, S, W). In these problems the location in the building and the agent\u2019s current orientation comprise the states. There is a single goal location, denoted by the star. The actions that can be chosen consists of movements: forward, turn-left, turn-right, turn-around, and no-op (stay in place). Action forward succeeds with probability 0.8, leaves the state unchanged with probability 0.05, moves the agent to the left and rotates the agent to the left with probability 0.05, similarly with probability 0.05 the agent moves to the right and is rotated to the right, with probability 0.025 the agent is moved back without changing its orientation, and with probability 0.025 the agent is moved back and is rotated backwards. The action move-left and move-right succeeds with probability 0.7, and with probability 0.1 each of the three remaining orientation is reached. Action turn-around succeeds with probability 0.6, leaves the state unchanged with probability 0.1, turns the agent to left or right, each with probability 0.15. The last action no-op leaves the state unchanged with probability 1. In states where moving forward is impossible the probability mass for the impossible next state is collapsed into the probability of not changing the state. Every move of the agent has a cost of 1 and the agent starts with uniform probability in all non-goal states. In the smaller Hallway problem there are 61 states and 22 observations. In the Hallway2 POMDP there are 94 states and 17 observations.\nAcknowledgments. We thank Blai Bonet for helping us with RTDP-Bel."}], "references": [{"title": "Solving POMDPs: RTDP-Bel vs. point-based algorithms", "author": ["B. Bonet", "H. Geffner"], "venue": "In IJCAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Properly Acting under Partial Observability with Action Feasibility Constraints. volume 8188 of Lecture Notes in Computer Science, pages 145\u2013161", "author": ["C.C.P Carvalho", "F. Teichteil-K\u00f6nigsbuch"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Pomdp-solve [software, version 5.3", "author": ["A. Cassandra"], "venue": "http://www.pomdp.org/,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Exact and approximate algorithms for partially observable Markov decision processes", "author": ["A.R. Cassandra"], "venue": "Brown University,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Faster and dynamic algorithms for maximal end-component decomposition and related graph problems in probabilistic verification", "author": ["K. Chatterjee", "M. Henzinger"], "venue": "In SODA. ACM-SIAM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "The complexity of probabilistic verification", "author": ["C. Courcoubetis", "M. Yannakakis"], "venue": "Journal of the ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Digital images and formal languages", "author": ["K. Culik", "J. Kari"], "venue": "Handbook of formal languages,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Biological sequence analysis: probabilistic models of proteins and nucleic acids", "author": ["R. Durbin", "S. Eddy", "A. Krogh", "G. Mitchison"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Solving POMDPs using selected past events", "author": ["A. Dutech"], "venue": "In ECAI, pages 281\u2013285,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Dynamic Programming and Markov Processes", "author": ["H. Howard"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1960}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "J. of Artif. Intell. Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Heuristic search for generalized stochastic shortest path MDPs", "author": ["A. Kolobov", "Mausam", "D.S. Weld", "H. Geffner"], "venue": "In ICAPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Temporal-logic-based reactive mission and motion planning", "author": ["H. Kress-Gazit", "G.E. Fainekos", "G.J. Pappas"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W.S. Lee"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Algorithms for Sequential Decision Making", "author": ["M.L. Littman"], "venue": "PhD thesis, Brown University,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "Learning policies for partially observable environments: Scaling up", "author": ["M.L. Littman", "A.R. Cassandra", "L. P Kaelbling"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "First results with utile distinction memory for reinforcement learning", "author": ["R.A. McCallum"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "Online discovery and learning of predictive state representations", "author": ["P. McCracken", "M.H. Bowling"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Finite-state transducers in language and speech processing", "author": ["M. Mohri"], "venue": "Computational Linguistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "The complexity of Markov decision processes", "author": ["C.H. Papadimitriou", "J.N. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1987}, {"title": "Approximating optimal policies for partially observable stochastic domains", "author": ["R. Parr", "S.J. Russell"], "venue": "In IJCAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1995}, {"title": "Introduction to probabilistic automata (Computer science and applied mathematics)", "author": ["A. Paz"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1971}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In IJCAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "Markov Decision Processes", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1994}, {"title": "Probabilistic automata", "author": ["M.O. Rabin"], "venue": "Information and Control,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1963}, {"title": "Artificial intelligence: a modern approach, volume 74", "author": ["S.J. Russell", "P. Norvig", "J.F. Canny", "J.M. Malik", "D.D. Edwards"], "venue": "Prentice hall Englewood Cliffs,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1995}, {"title": "Heuristic search value iteration for POMDPs", "author": ["T. Smith", "R. Simmons"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "The Optimal Control of Partially Observable Markov Processes", "author": ["E.J. Sondik"], "venue": "Stanford University,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1971}, {"title": "A point-based POMDP algorithm for robot planning", "author": ["M.T.J. Spaan"], "venue": "In Robotics and Automation,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}], "referenceMentions": [{"referenceID": 9, "context": "Markov decision processes (MDPs) are standard models for probabilistic systems that exhibit both probabilistic as well as nondeterministic behavior [12].", "startOffset": 148, "endOffset": 152}, {"referenceID": 24, "context": "MDPs are widely used to model and solve control problems for stochastic systems [11,28]: nondeterminism represents the freedom of the controller to choose a control action, while the probabilistic component of the behavior describes the system response to control actions.", "startOffset": 80, "endOffset": 87}, {"referenceID": 20, "context": ", given the current state, the controller can only view the observation of the state (the partition the state belongs to), but not the precise state [24].", "startOffset": 149, "endOffset": 153}, {"referenceID": 7, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 110, "endOffset": 113}, {"referenceID": 19, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 156, "endOffset": 159}, {"referenceID": 13, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 176, "endOffset": 183}, {"referenceID": 10, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 176, "endOffset": 183}, {"referenceID": 11, "context": "POMDPs provide the appropriate model to study a wide variety of applications such as in computational biology [9], speech processing [23], image processing [8], robot planning [17,13], reinforcement learning [14], to name a few.", "startOffset": 208, "endOffset": 212}, {"referenceID": 25, "context": "POMDPs also subsume many other powerful computational models such as probabilistic finite automata (PFA) [29,26] (since probabilistic finite automata (aka blind POMDPs) are a special case of POMDPs with a single observation).", "startOffset": 105, "endOffset": 112}, {"referenceID": 22, "context": "POMDPs also subsume many other powerful computational models such as probabilistic finite automata (PFA) [29,26] (since probabilistic finite automata (aka blind POMDPs) are a special case of POMDPs with a single observation).", "startOffset": 105, "endOffset": 112}, {"referenceID": 24, "context": "In stochastic optimization problems related to POMDPs, the transitions in the POMDPs are associated with integer costs, and the two classical objectives that have been widely studied are finite-horizon and discounted-sum objectives [11,28,24].", "startOffset": 232, "endOffset": 242}, {"referenceID": 20, "context": "In stochastic optimization problems related to POMDPs, the transitions in the POMDPs are associated with integer costs, and the two classical objectives that have been widely studied are finite-horizon and discounted-sum objectives [11,28,24].", "startOffset": 232, "endOffset": 242}, {"referenceID": 14, "context": "While there are several works for discounted POMDPs [18,31,27], as mentioned above the problem we consider is different from discounted POMDPs.", "startOffset": 52, "endOffset": 62}, {"referenceID": 27, "context": "While there are several works for discounted POMDPs [18,31,27], as mentioned above the problem we consider is different from discounted POMDPs.", "startOffset": 52, "endOffset": 62}, {"referenceID": 23, "context": "While there are several works for discounted POMDPs [18,31,27], as mentioned above the problem we consider is different from discounted POMDPs.", "startOffset": 52, "endOffset": 62}, {"referenceID": 0, "context": "The most closely related works are Goal-MDPs and POMDPs [2,16].", "startOffset": 56, "endOffset": 62}, {"referenceID": 12, "context": "The most closely related works are Goal-MDPs and POMDPs [2,16].", "startOffset": 56, "endOffset": 62}, {"referenceID": 24, "context": "Throughout this work, we follow standard notations from [28,19].", "startOffset": 56, "endOffset": 63}, {"referenceID": 15, "context": "Throughout this work, we follow standard notations from [28,19].", "startOffset": 56, "endOffset": 63}, {"referenceID": 22, "context": "The strict emptiness problem asks for the existence of a strategy w (a finite word over the alphabet A) such that the measure of the runs ending in the desired final states F is strictly greater than 1 2 ; and the strict emptiness problem for PFA is undecidable [26].", "startOffset": 262, "endOffset": 266}, {"referenceID": 0, "context": "Consider a strategy in the POMDP u = ($ w[1] $ w[2] .", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "as a word u = ($ w[1] $ w[2] .", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "The sequence of costs can be partitioned into blocks of length 2 \u00b7 n+ 1, intuitively corresponding to the transitions of a single run on the word ($w[1] $w[2] .", "startOffset": 155, "endOffset": 158}, {"referenceID": 1, "context": "The framework that restricts playable actions was also considered in [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "The strategy \u03c3Allow can be obtained by computing the set of almost-sure winning states in the belief-support MDP, and for discrete graph-based algorithms to compute almost-sure winning states in perfect-observation MDPs see [7,6].", "startOffset": 224, "endOffset": 229}, {"referenceID": 4, "context": "The strategy \u03c3Allow can be obtained by computing the set of almost-sure winning states in the belief-support MDP, and for discrete graph-based algorithms to compute almost-sure winning states in perfect-observation MDPs see [7,6].", "startOffset": 224, "endOffset": 229}, {"referenceID": 28, "context": "For minimizing the expected total cost, strategies based on information states are sufficient [32].", "startOffset": 94, "endOffset": 98}, {"referenceID": 3, "context": "Given an information state b, an action a, and an observation z, computing the resulting information state b\u2032 can be done in a straight forward way, see [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "The first approach is the exact finite-horizon value iteration using a modified version of POMDPSolve [4]; and the second approach is an approximate finite-horizon value iteration using a modified version of RTDP-Bel [2]; and in both the cases our straightforward modification is that the computation of the finite-horizon value iteration is restricted to allowed actions and almost-sure winning belief-supports.", "startOffset": 102, "endOffset": 105}, {"referenceID": 0, "context": "The first approach is the exact finite-horizon value iteration using a modified version of POMDPSolve [4]; and the second approach is an approximate finite-horizon value iteration using a modified version of RTDP-Bel [2]; and in both the cases our straightforward modification is that the computation of the finite-horizon value iteration is restricted to allowed actions and almost-sure winning belief-supports.", "startOffset": 217, "endOffset": 220}, {"referenceID": 17, "context": "The POMDP examples we considered are as follows: (A) We experimented with the Cheese maze POMDP example which was introduced in [21] and also studied in [10,20,22].", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "The POMDP examples we considered are as follows: (A) We experimented with the Cheese maze POMDP example which was introduced in [21] and also studied in [10,20,22].", "startOffset": 153, "endOffset": 163}, {"referenceID": 16, "context": "The POMDP examples we considered are as follows: (A) We experimented with the Cheese maze POMDP example which was introduced in [21] and also studied in [10,20,22].", "startOffset": 153, "endOffset": 163}, {"referenceID": 18, "context": "The POMDP examples we considered are as follows: (A) We experimented with the Cheese maze POMDP example which was introduced in [21] and also studied in [10,20,22].", "startOffset": 153, "endOffset": 163}, {"referenceID": 26, "context": "(B) We considered the Grid POMDP introduced in [30] and also studied in [20,25,22].", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "(B) We considered the Grid POMDP introduced in [30] and also studied in [20,25,22].", "startOffset": 72, "endOffset": 82}, {"referenceID": 21, "context": "(B) We considered the Grid POMDP introduced in [30] and also studied in [20,25,22].", "startOffset": 72, "endOffset": 82}, {"referenceID": 18, "context": "(B) We considered the Grid POMDP introduced in [30] and also studied in [20,25,22].", "startOffset": 72, "endOffset": 82}, {"referenceID": 16, "context": "(C) We experimented with the robot navigation problem POMDP introduced in [20], where we considered both deterministic transition and a randomized version.", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "(D) We consider the Hallway example from [20,33,31,2].", "startOffset": 41, "endOffset": 53}, {"referenceID": 29, "context": "(D) We consider the Hallway example from [20,33,31,2].", "startOffset": 41, "endOffset": 53}, {"referenceID": 27, "context": "(D) We consider the Hallway example from [20,33,31,2].", "startOffset": 41, "endOffset": 53}, {"referenceID": 0, "context": "(D) We consider the Hallway example from [20,33,31,2].", "startOffset": 41, "endOffset": 53}, {"referenceID": 0, "context": "(E) We consider the RockSample example from [2,31].", "startOffset": 44, "endOffset": 50}, {"referenceID": 27, "context": "(E) We consider the RockSample example from [2,31].", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "Our experimental results are shown in Table 1, where we compare our approach to RTDP-Bel [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 14, "context": "Other approaches such as SARSOP [18], anytime POMDP [27], ZMDP [31] are for discounted setting, and hence are different from our approach.", "startOffset": 32, "endOffset": 36}, {"referenceID": 23, "context": "Other approaches such as SARSOP [18], anytime POMDP [27], ZMDP [31] are for discounted setting, and hence are different from our approach.", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "Other approaches such as SARSOP [18], anytime POMDP [27], ZMDP [31] are for discounted setting, and hence are different from our approach.", "startOffset": 63, "endOffset": 67}, {"referenceID": 2, "context": "RockSample[4,4] {1, 50, 100} 257, 9, 2 0.", "startOffset": 10, "endOffset": 15}, {"referenceID": 2, "context": "RockSample[4,4] {1, 50, 100} 257, 9, 2 0.", "startOffset": 10, "endOffset": 15}, {"referenceID": 3, "context": "RockSample[5,5] {1, 50, 100} 801, 10, 2 0.", "startOffset": 10, "endOffset": 15}, {"referenceID": 3, "context": "RockSample[5,5] {1, 50, 100} 801, 10, 2 0.", "startOffset": 10, "endOffset": 15}, {"referenceID": 3, "context": "RockSample[5,7] {1, 50, 100} 3201, 12, 2 4.", "startOffset": 10, "endOffset": 15}, {"referenceID": 5, "context": "RockSample[5,7] {1, 50, 100} 3201, 12, 2 4.", "startOffset": 10, "endOffset": 15}, {"referenceID": 5, "context": "RockSample[7,8] {1, 50, 100} 12545, 13, 2 78.", "startOffset": 10, "endOffset": 15}, {"referenceID": 6, "context": "RockSample[7,8] {1, 50, 100} 12545, 13, 2 78.", "startOffset": 10, "endOffset": 15}, {"referenceID": 16, "context": "We consider two versions of the Hallway example introduced in in [20] and used later in [33,31,2].", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "We consider two versions of the Hallway example introduced in in [20] and used later in [33,31,2].", "startOffset": 88, "endOffset": 97}, {"referenceID": 27, "context": "We consider two versions of the Hallway example introduced in in [20] and used later in [33,31,2].", "startOffset": 88, "endOffset": 97}, {"referenceID": 0, "context": "We consider two versions of the Hallway example introduced in in [20] and used later in [33,31,2].", "startOffset": 88, "endOffset": 97}, {"referenceID": 5, "context": "RockSample[7,8]", "startOffset": 10, "endOffset": 15}, {"referenceID": 6, "context": "RockSample[7,8]", "startOffset": 10, "endOffset": 15}, {"referenceID": 27, "context": "The RockSample problem introduced in [31] and used later in [2] is a scalable problem that models rover science exploration (Figure 10).", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "The RockSample problem introduced in [31] and used later in [2] is a scalable problem that models rover science exploration (Figure 10).", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 52, "endOffset": 57}, {"referenceID": 2, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 52, "endOffset": 57}, {"referenceID": 3, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 84, "endOffset": 89}, {"referenceID": 3, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 84, "endOffset": 89}, {"referenceID": 3, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 116, "endOffset": 121}, {"referenceID": 5, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 116, "endOffset": 121}, {"referenceID": 5, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 153, "endOffset": 158}, {"referenceID": 6, "context": "All the problems have 2 observations, and RockSample[4,4] has 257 states, RockSample[5,5] has 801 states, RockSample[5,7] has 3201 states, and RockSample[7,8] has 12545 states.", "startOffset": 153, "endOffset": 158}], "year": 2014, "abstractText": "We consider partially observable Markov decision processes (POMDPs) with a set of target states and every transition is associated with an integer cost. The optimization objective we study asks to minimize the expected total cost till the target set is reached, while ensuring that the target set is reached almost-surely (with probability 1). We show that for integer costs approximating the optimal cost is undecidable. For positive costs, our results are as follows: (i) we establish matching lower and upper bounds for the optimal cost and the bound is double exponential; (ii) we show that the problem of approximating the optimal cost is decidable and present approximation algorithms developing on the existing algorithms for POMDPs with finite-horizon objectives. While the worst-case running time of our algorithm is double exponential, we also present efficient stopping criteria for the algorithm and show experimentally that it performs well in many examples of interest.", "creator": "LaTeX with hyperref package"}}}