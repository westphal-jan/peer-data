{"id": "1411.4102", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2014", "title": "Anisotropic Agglomerative Adaptive Mean-Shift", "abstract": "mean shift today, is widely used for interactive mode detection and clustering. the implementation technique though, is challenged in peak practice due to assumptions of isotropicity and homoscedasticity. we present an robust adaptive mean shift methodology that allows for full anisotropic clustering, through unsupervised local bandwidth selection. the bandwidth assignment matrices evolve naturally, adapting locally through agglomeration, and in turn guiding further agglomeration. the online methodology is practical and effecive for low - dimensional feature spaces, preserving better detail and clustering among salience. additionally, conventional mean shift either critically depends on a per instance length choice of bandwidth, or relies on offline methods which are inflexible and / or transmit again data instance specific. the presented approach, due to its adaptive design, also alleviates this issue - with a default consensus form performing generally well. the methodology though, allows for effective tuning choice of results.", "histories": [["v1", "Sat, 15 Nov 2014 02:05:22 GMT  (4697kb,D)", "http://arxiv.org/abs/1411.4102v1", "British Machine Vision Conference, 2014"]], "COMMENTS": "British Machine Vision Conference, 2014", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["rahul sawhney", "henrik i christensen", "gary r bradski"], "accepted": false, "id": "1411.4102"}, "pdf": {"name": "1411.4102.pdf", "metadata": {"source": "CRF", "title": "Anisotropic Agglomerative Adaptive Mean-Shift", "authors": ["Rahul Sawhney", "Henrik I. Christensen", "Gary R. Bradski"], "emails": ["rahul.sawhney@gatech.edu", "hic@gatech.edu", "gbradski@magicleap.com"], "sections": [{"heading": "1 Introduction", "text": "\u2018Mean Shift\u2019 ([15, 7], MS) is a powerful nonparametric technique for unsupervised pattern clustering and mode seeking. References [11, 3] established it\u2019s utility in low-level perception tasks such as feature clustering, filtering and in tracking. It has been in popular use since, as a very useful tool for pattern clustering of sensor data ([28, 14] for example). It has also found niche as a preprocessor (a priori segmentation, smoothing) before higher level image & video analysis tasks such as scene parsing, object recognition, detection ([22, 35, 20]). Image segmentation approaches such as Markov Random Fields, Spectral clustering, Hierarchical clustering use it as an a priori segmenter with improved results ([22, 26, 21, 31, 30]).\nMean Shift methodologies though, employ some assumptions and have some limitations, which may not be desirable. Its popular standard form, [11], utilizes fixed, scalar bandwidth assuming homoscedasticity and isotropicity. Being homoscedastic, it also requires proper bandwidth choice on a per instance basis. The adaptive Mean Shift variants, [12, 16], ascertain variable bandwidths, but they still assume isotropicity. They also make use of heuristics which are not flexible, and lack clustering control. Offline bandwidth selection methods for Mean Shift ([6, 17, 10]), typically estimate a single, global bandwidth, and/or are data specific/non-automatic. As indicated in Fig.1 - isotropic/scalar bandwidths tend to smooth anisotropic patterns and affect partition boundaries, while global/homoscedastic bandwidths are inappropriate when clusters (or modes) at different scales need to be identified.\nc\u00a9 2014. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.\nar X\niv :1\n41 1.\n41 02\nv1 [\ncs .C\n(a) 3D Clustering result (23 clusters) over image data (left, L*a*b* space) by the proposed approach. 1-sigma final trajectory bandwidths have been overlaid over the converged modes. The segment image is shown on right.\n(b) Comparitive results with standard MS (left) and variable-bandwidth isotropic MS ([16], right), at similar clustering levels, 25 & 27 respectively, are shown. Final mode locations have been indicated over the cluster plots. MS with correctly chosen bandwidth detected more coherent modes than [16], but looses partition saliency (bushes, water, sky in background). [16] better adapts to scales but oversegments at places, and smooths over others (face). Both smoothed over details, failed to detect some modes at lower scales (trouser edges, maroon on shirt & shoes). In general, conventional MS had a typical tendency to over-segment heavily or compromise partition boundaries.\nFigure 1: Exemplar illustrative result of our approach, AAAMS (a), is shown along with conventional MS results (b), at comparable clustering levels. As is indicated by the plots and segment images, AAAMS effectively adapts to local scale and preserves anisotropic details, affecting more salient partitions.\nWe present a Mean Shift methodology which is anisotropic and locally adaptive. It is able to leverage guided agglomeration for unsupervised bandwidth selection (Fig.1, 5). This results in robust mode detection, with increased partition saliency. Also as a consequence, a low valued parameter set performs nicely over a wider range of data instances (Sec. 2.1).\nClusters arise on the fly in the proposed approach, as a consequence of agglomeration of extant clusters. Local bandwidths (Secs. 1.1, 2) which evolve anisotropically every iteration, are associated with each cluster; by design, all members of a cluster converge to the same local mode. By evolving as function of a cluster\u2019s aggregated trajectory points, these bandwidths are able to adapt to the underlying mode structure (shape, scale, orientation) - and in turn, guide future cluster trajectory and agglomeration. The supplementary also presents a useful result - a convergence proof when full bandwidths vary between Mean Shift iterations, as is the case here. We refer to our approach as online because it\u2019s an on the fly unsupervised procedure; with simple bookkeeping doing away with re-calculations."}, {"heading": "1.1 Motivation and Background", "text": "We utilize the exposition style of [5]. Let {xi}ni=1 \u2282 Rd , be a set of d-dimensional data points with their sample point kernel density estimate (KDE) being p(x) = \u2211ni=1 p(xi)p(x|xi) = \u2211ni=1 pi 1 ci K(\u2016x\u2212xi\u2016\u03a3i ). Stationary points of the KDE can be estimated by evaluating the density gradient and setting it to zero. This gives rise to the Mean-Shift fixed point iteration : x\u03c4+1 = f (x\u03c4 ) (1a)\nf (x\u03c4 ) = ( n\n\u2211 i=1 pi 1 ci K \u2032 (\u2016x\u03c4 \u2212 xi\u2016\u03a3i )\u03a3 \u22121 i\n)\u22121 \u00d7 ( n\n\u2211 i=1 pi 1 ci K \u2032 (\u2016x\u03c4 \u2212 xi\u2016\u03a3i )\u03a3 \u22121 i xi\n) (1b)\nK(t), t \u2265 0, is a d-variate kernel with compact support satisfying some regularity constraints, mild in practice ([11, 5] for details). \u2016x\u2212 xi\u2016\u03a3i \u2261 ((x\u2212 xi)T \u03a3 \u22121 i (x\u2212 xi))\n1/2, is the Mahanalobis metric. The point prior pi \u2261 p(xi) is usually taken as 1/n. ci is a normalizing constant depending only on the covariance matrix, \u03a3i (kernel bandwidth), associated with each data point. The bandwidth, \u03a3i, is roughly an inverse measure of local curvature around xi. It linearly captures\nthe scale and correlations of the underlying data. \u03c4 indicates the iteration count. In practice, since K(t) is taken with truncated support, the summations are only over n\u2032 neighbors of x\u03c4 , with n\u2032 n. The vector m(x\u03c4 ) = f (x\u03c4 )\u2212 x\u03c4 , is referred to as the Mean Shift. It\u2019s a bandwidth scaled version of \u2207p(x), is free from a step size parameter, is large in regions with low p(x) and small near the modes. Starting at a data point, x\u03c4=0i \u2261 xi, the fixed point update is run multiple times till convergence. The resulting points, x\u03c4\u22651i is referred to as the trajectory of xi, tracing a path to the local mode. The technique thus, is able to locate modes and partition feature space, without a priori knowledge of partition count or structure.\nThe above hinges on selecting reasonable bandwidth matrices \u03a3i. Good bandwidths capture the underlying local distribution effectively. In our approach, data points (pertaining to a cluster) converging to a common local mode share a common bandwidth - one which reflects this mode\u2019s structure, and to an extent, its basin of attraction ([11]). We refer to it as the local bandwidth ([10] utilizes local bandwidths in a related sense).\nIn online unsupervised usage, almost all Mean Shift variants for clustering, for example [11, 30, 37, 27], work under the restrictive assumptions of homoscedasticity and isotropicity (\u03a3i = \u03c32I, standard fixed bandwidth Mean Shift). The scale parameter \u03c3 has to be set carefully based on the dataset instance. [36] utilizes set covering based iterative agglomeration for improved efficiency. Coverage is ensured through overlaps of small fixed homoscedastic bandwidths. Some applications only assume isotropicity (\u03a3i = \u03c32i I, adaptive / variablebandwidth Mean Shift). \u03c3i is estimated using a variation of the following two heuristics ([12, 16]) - 1) kth nearest neighbor, xki , distance heuristic \u2192 \u03c3i \u221d \u2016xi\u2212 xki \u2016, or 2) Abramson\u2019s heuristic \u2192 \u03c3i \u221d \u03c3o(\u03c0(xi))\u22121/2, where \u03c0(x) is the pilot density estimate obtained by first running mean shift with analysis bandwidth, \u03c3o. They have found more use in smoothing type applications as reported in [25, 19]. Variants have also been used in tracking scenarios, where the bandwidths are adapted in a task specific fashion (see [18, 9], for example). [23, 33] adapt isotropic bandwidths to object scales, to unimodally track, search for them. The topological, blurring, evolving variants for clustering (like [27, 30, 37, 4, 29]) use isotropic bandwidths. They are primarily aimed at increased efficiency, with results on par with standard mean shift. [32] presents improvements over the somewhat related Mediod Shift. They propose usage of their algorithm as initialization for Mean Shift, for increased efficiency.\nIn offline settings, [10] presents a supervised methodology. Training data is processed with analysis bandwidths to select local bandwidths based on neighboring partition stability. The estimated bandwidths are then used to partition similarly distributed test image data. Only recently were automatic full bandwidth selectors for density gradient estimation proposed in [17, 6], for offline settings. These focus on obtaining good data density gradients (as opposed to clustering) and optimize based on the mean square integrated error (MISE). A single global bandwidth is estimated for the given data, and as the authors themselves note, the involved computations are not straighforward.\nA very useful variant is Joint Domain Mean Shift, [11], which is used to create partitions jointly respecting the dataset\u2019s multiple feature domains which are mutually independent; For example, \u3008color,space\u3009 in color based segmentation & smoothing, and \u3008color, f low\u3009 in motion segmentation. When x = [xrT xsT ] with (xr\u22a5xs) |x, and utilizing two separate kernels, Kr, Ks, we\u2019ll have p(x) = \u2211ni=1 p(xi)p(xr |xi)p(xs|xi). Eq.1b analogue would then come out to be f (x\u03c4 ) =\n( \u2211ni=1 pi 1c\u2032i J(\u2016x\u03c4,r\u2212 xir\u2016\u03a3ri ,\u2016x \u03c4,s\u2212 xis\u2016\u03a3si )\u03a3 \u22121 i )\u22121 \u00d7 ( \u2211ni=1 pi 1c\u2032i J(\u2016x\u03c4,r\u2212 xir\u2016\u03a3ri ,\u2016x \u03c4,s\u2212 xis\u2016\u03a3si )\u03a3 \u22121 i xi ) , where c\u2032i\nis the normalization constant, J(t1, t2) \u2261 K\u2032r (t1)Ks(t2) = Kr(t1)K \u2032 s (t1),\u2200t1, t2 \u2265 0, and \u03a3i = [ \u03a3ri 0 0 \u03a3si ] . Typically, but not necessarily, xs may lie on a spatial manifold - imposing structure to data which is utilized. Instances in literature use fixed global scale parameters \u03c3 r and \u03c3 s, which have\nthe aforementioned limitations. As noted in [31] on color segmentation, \u03c3 r and \u03c3 s need to be selected carefully. Good choices are not always possible, with segments being too coarse or too fine at times (Figs. 3, 5). Reference [34] utlizes an anisotropic \u03a3si for visual data segmentations. Every data point\u2019s associated bandwidth, \u03a3i, is modulated multiple times in each iteration, until convergence is achieved. Modulation heuristics have been provided, to be deployed as per task. The spatial bandwidth \u03a3si is parameterized as function of eigenvectors of neighborhood data covariance. \u03a3ri is taken to be an isotropic scalar dependent on \u03a3si ."}, {"heading": "2 Methodology", "text": "A data point, xi, is alternatively represented as xi,u - the first index value being its unique identifier as before and the second index indicating its current, exclusive membership to a cluster, u\u2208 {1, . . .n} 1. A cluster u\u2019s constituent data points is denoted by the set , Cu = {xi,u | \u2203 i\u2208 {1, . . .n}}. By algorithm design, clusters are merged only when they are tending towards the same mode - thus all member points of a cluster, u, will eventually converge to a common local mode, say \u00b5u. They hence, are also taken to share a common local bandwidth, \u03a3u. This bandwidth develops every iteration when the cluster u\u2019s trajectory points set, Tu , gets additional elements. The set of clusters surviving at iteration, \u03c4, would be U\u03c4 = {u |Cu 6= /0}. |U\u03c4 | would indicate its cardinality. At beginning, at \u03c4 = 0, each point trivially forms a separate cluster\u2192U0 = {1, . . .n} , Cu = {xi=u,u}, \u2200u\u2208U0. Given the initialization, each extant cluster u\u2208U\u03c4 will always contain the initial point, xu,u - which we refer to as its principle member.\nAt any iteration \u03c4, for each extant cluster u, mean shift updates happen for only the principle member, xu,u; with the first iteration running over trivial clusters. The resulting trajectory is specified as x\u03c4\u22651u,u or simply u\u03c4 . A cluster\u2019s trajectory might end when it gets merged or converged. In general, each data point, xi, started out as a trivial cluster, and had or still has a trajectory - it\u2019s trajectory set being {x\u03c4=1:endi }. \u2032end\u2032 being the iteration at which the trajectory ended; else the current iteration. Note that the data point xi itself is not included in this set. For any surviving cluster u, then, the complete set of all agglomerated trajectory points associated with it, would be Tu = {\u222a{x\u03c4=1:endi } | xi \u2208Cu} - basically a union of all the members\u2019 trajectory sets. u\u03c4 is indicative of the cluster u\u2019s location. At convergence, u\u03c4 would be the location of a local mode. u\u2019s members would then be comprising of data points pertaining to that mode and its basin (Fig. 1(a)). The data density in the immediate vicinity of u\u03c4 \u2019s current position is indicated as \u03c1(u\u03c4 ), or simply, \u03c1u. We use operator \u03a0 to retrieve the cluster identifier of an arbitrary data point; so \u03a0(xi,u) = u. The n\u2032 data points in u\u03c4 \u2019s neighborhood are denoted as Nex(u\u03c4 ), and the clusters containing them as G = {\u222a\u03a0(y) | y \u2208 Nex(u\u03c4 )}.2\nThe methodology for anisotropic, agglomerative, adaptive Mean Shift (AAAMS) is presented as a pseudo code in Alg. 1. At every iteration, the following steps are run for each surviving cluster that has not converged \u2192 1) Mean shift update is computed and the cluster\u2019s location is updated. No merges happen before the first update. 2) Nearest neighbors about the current location are ascertained - they are utilized for cluster merges, and for the mean shift\nupdate in subsequent iteration. 3) When merge criteria are met, either some clusters (owners of the neighborhood points which lie within epsilon) get\nmerged into this cluster, or this cluster gets merged into one of them. 4) If the incumbent cluster survived after the merge, its bandwidth is updated. 5) Optionally, if the cluster has converged, its location could be perturbed a bit. It is, then, not taken out of consideration\nin subsequent iteration.\n1The second index is left out when the membership is apparent or inconsequential. We similarly ease out the notations whenever pertinent, to simplify exposition without loss of intuition.\n2Since a cluster corresponds one-to-one with its principle member, principle member\u2019s trajectory is at times referred to as cluster trajectory. Similarly, convergence of trajectory is at times referred to as cluster converging. \u03c4 , apart from indicating iteration, also differentiates between a trajectory point and a data point. The cluster trajectory, u\u03c4 \u2261 x\u03c4\u22651u,u , is the trajectory resulting from data point xu,u . The cluster u\u2019s current location refers to current position of the principle member, indicated by u\u03c4 .\nAlgorithm 1 : AAAMS - Anisotropic Agglomerative Adaptive Mean Shift\nFunction : AAAMS \u3008{xi}ni=1\u3009 with xi \u2208 Rd Returns : \u3008U\u2217,C\u2217, {\u00b5u}u\u2208U\u2217 , {\u03a3\u2217u}u\u2208U\u2217 \u3009 ## ConvergenceCriteria \u2192 \u2016mu\u2016 \u2264 \u03b4 Uo = {1, . . .n} ; Cu = {xu} , xou = xu, \u03a3u = \u03c32baseId , \u2200u \u2208U0 \u03c4 = 0 ; \u03bb = 5 ; \u03b4 =Convergence epsilon mu = Large \u2208 Rd , Tu = \u03c6 , \u2200u \u2208U0\nWhile \u2203u \u2208U\u03c4 s.t. \u2016mu\u2016> \u03b4 ForEach u \u2208 U\u03c4 s.t. \u2016mu\u2016> \u03b4\nu\u03c4+1 =  Eq.4 Eq.2b\nEq.3\nESS(u) < \u03bb\nESS(g) \u2265 \u03bb , \u2200g \u2208 G\notherwise\nmu = u\u03c4+1\u2212u\u03c4 ; Tu = Tu \u222au\u03c4+1 Get Nex(u\u03c4+1) ForEach y \u2208 Nex(u\u03c4+1) or till Cu 6= /0 I f \u03a0(y) = u or C\u03a0(y) = \u03c6 T hen Continue I f \u2225\u2225u\u03c4+1\u2212 y\u2225\u2225> \u03b5 T hen Continue\nI f !MergeCheck \u2329 u\u03c4+1,y,mu,y\u03c4=1\u2212 y,u,\u03a0(y) \u232a T hen Continue\nI f \u03c1u > \u03c1\u03a0(y) T hen\nCu =Cu \u222aC\u03a0(y) ; C\u03a0(y) = /0 Tu = Tu \u222aT\u03a0(y) ; T\u03a0(y) = /0\nElse\nC\u03a0(y) =Cu \u222aC\u03a0(y) ; Cu = /0 T\u03a0(y) = Tu \u222aT\u03a0(y) ; Tu = /0\nEndForEach I f Cu = \u03c6 T hen Continue\n\u03a3u =  Eq.6\u03a3u ESS(u) \u2265 \u03bb otherwise\n## Optionally Perturb \u2329 u\u03c4+1 , mu \u232a i f \u2016mu\u2016 \u2264 \u03b4 EndForEach U\u03c4+1 = {u | u \u2208 U\u03c4 ,Cu 6= /0} \u03c4 = \u03c4 +1\nEndWhile U\u2217 =U\u03c4 ; C\u2217 =Cu , \u2200u \u2208U\u03c4 ; \u03a3\u2217u = \u03a3u , \u2200u \u2208U\u03c4\nEndFunction\nFor feature spaces that can be decomposed into independent subspaces, the above can be extended to multiple domains. The update equations would then utlize multiple kernels. Basically, for each domain, a \u2329\u03c3base , \u03b5\u232a pair needs to be set. For example, for joint domain Mean Shift (Sec:1.1), we\u2019ll have \u2329 \u03c3rbase , \u03b5 r \u232a & \u2329 \u03c3sbase , \u03b5 s \u232a for the two domains. We\u2019ll have then \u03a3base = [ \u03c3r 2 baseIr 0\n0 \u03c3s 2\nbaseIs\n] & \u03a3u = [ \u03a3ru 0 0 \u03a3su ] . \u03a3ru & \u03a3su would be evaluated from Eq.6. Eq.2b analogue would be f (u\u03c4 ) =\n( \u2211 \u2200g\u2208G 1 cg \u03a3 \u22121 g \u2211 \u2200i|xi,g\u2208Nex(u\u03c4 ) J(\u2016u\u03c4,r \u2212 xir\u2016\u03a3ru , \u2016u\u03c4,s \u2212 xis\u2016\u03a3su ) )\u22121 \u00d7 ( \u2211 \u2200g\u2208G 1 cg \u03a3 \u22121 g \u2211 \u2200i|xi,g\u2208Nex(u\u03c4 ) J(\u2016u\u03c4,r \u2212 xir\u2016\u03a3ru , \u2016u\u03c4,s \u2212 xis\u2016\u03a3su )xi ) ; likewise for others."}, {"heading": "2.1 Update Equations", "text": "Taking pi = 1/n and limiting summations to the neighboring points, Nex(u\u03c4 ), the fixed point iteration, Eq. 1a-1b, over a cluster u (rather xu,u) can be reformulated/reorganized as a local bandwidth based decomposition :\nu\u03c4+1 = f (u\u03c4 ), whereu\u03c4=0 \u2261 xu,u (2a)\nf (u\u03c4 ) = (\n\u2211 \u2200g\u2208G 1 cg \u03a3\u22121g \u2211 \u2200i|xi,g\u2208Nex(u\u03c4 ) K \u2032 (\u2016u\u03c4 \u2212 xi\u2016\u03a3g )\n)\u22121 \u00d7 (\n\u2211 \u2200g\u2208G 1 cg \u03a3\u22121g \u2211 \u2200i|xi,g\u2208Nex(u\u03c4 ) K \u2032 (\u2016u\u03c4 \u2212 xi\u2016\u03a3g )xi ) (2b)\nEq. 2b would be exactly the same as Eq. 1b at \u03c4 = 0, when all points form trivial clusters. When local homoscedasticity in neighborhood of u\u03c4 is assumed with the cluster\u2019s own bandwidth \u03a3u taken as bandwidth estimate for neighborhood Nex(u\u03c4 ), Eq. 2b simplifies 3 to :\nf (u\u03c4 ) = \u2211\u2200i|xi\u2208Nex(u\u03c4 ) K\n\u2032 (\u2016u\u03c4 \u2212 xi\u2016\u03a3u )xi\n\u2211\u2200i|xi\u2208Nex(u\u03c4 ) K \u2032 (\u2016u\u03c4 \u2212 xi\u2016\u03a3u )\n(3)\nIf global homoscedasticity and isotropicity is assumed, Eq. 2b takes the form of standard mean shift update, where the bandwidth is specified through a fixed scalar \u03c3base :\nf (u\u03c4 ) = \u2211\u2200i|xi\u2208Nex(u\u03c4 ) K\n\u2032 (\u2016(u\u03c4\u2212xi)T (u\u03c4\u2212xi)/\u03c3 2base\u2016)xi\n\u2211\u2200i|xi\u2208Nex(u\u03c4 ) K \u2032 (\u2016(u\u03c4\u2212xi)T (u\u03c4\u2212xi)/\u03c3 2base\u2016)\n(4)\n3Eq.3 gets us a particularly insightful interpretation. Note that \u2016u\u03c4 \u2212 xi\u2016\u03a3u could be thought of as a partial likelihood measure of the data point xi belonging to the cluster u. Consider the conditional \u2192 p(xi/u\u03c4 ;u) = K \u2032 (\u2016u\u03c4\u2212xi\u2016\u03a3u )/\u2211... K \u2032 (\u2016u\u03c4\u2212xi\u2016\u03a3u ) , with the summation in denominator normalizing the distribution. The fixed point update from Eq.3 would then come out to be u\u03c4+1 = \u2211... p(xi/u\u03c4 ;u)xi. So the updated cluster trajectory u\u03c4+1 is just the neighborhood data expectation, conditioned only under the cluster\u2019s own distribution. In effect, this serves to guide/update a cluster\u2019s trajectory based only on the properties (bandwidth) it has itself ascertained (till \u03c4).\nEach trivial cluster utilizes fixed base bandwidth to begin with, employing Eq. 4 for mean shift updates. Benign clusters form and start moving up on some modes. As soon as a cluster accumulates enough trajectory points for full bandwidth estimates (Sec.2.2) to be significant (u has moved up to denser regions by then), it switches to anisotropic updates, given by Eqs. 3 & 2b. A reasonable test of significance for \u03a3u estimates, is to check if the kernel weighted point count or Effective Sample Size (ESS, [13]) is above some value, \u03bb .\nESS(u) = \u2211\u2200v\u2208Tu K\n\u2032 ( \u2225\u2225Tu\u2212 v\u2225\u2225\u03a3estimateu )\n\u2211\u2200v\u2208Tu K \u2032 (\u20160\u2016\u03a3estimateu )\n(5)\nTu indicates the mean of the trajectory set. The anisotropic update Eq. 3 is used when the cluster has an ESS(u)\u2265 \u03bb , and the more confident update Eq. 2b is used, when ESS(g)\u2265 \u03bb , \u2200g \u2208 G - when all the neighboring clusters too have confident enough bandwidth estimates 4. As a binomial rule of thumb ([13]), \u03bb = 5 is chosen as the minimum ESS, which is analogous to choosing 5 as the minimum individual expected cell counts in a \u03c72 test of independence.\nSo starting with the initial base scalar, \u03c3base, the bandwidth matrices evolve by themselves. The nice part is that just a low base value suffices for reasonably dense data, with the bandwidths scaling data driven thereon and adapting to the local structure\u2019s scale, shape and orientation. \u03c3base thus becomes indicative of the minimum desired detail in the data space. This is opposed to traditional Mean Shift - where the bandwidth scalar is indicative of the scale at which the data space has to be partitioned."}, {"heading": "2.2 Bandwidth Estimation", "text": "Bandwidth estimates based on a cluster\u2019s member data point locations are not reliable ([10] notes this too). A subset of point locations in isolation cannot be considered as representative of underlying distribution. The underlying local distribution is actually a localized subset of the joint non-parametric density represented by the entire dataset - it has significant contributions from neighboring structures as well. The local structure could also be asymmetric and/or without tail(s). A solution lies in considering points which arise from mean shift ascents over the mode the cluster is converging to - the cluster trajectory set, Tu. We use the variance of Tu with respect to the underlying density as an estimate, \u03a3u. As Tu builds up each iteration, so does \u03a3u.\n\u03a3u = \u2211\u2200v\u2208Tu \u03c1(v)vv\n\u03c4\n\u2211\u2200v\u2208Tu \u03c1(v) \u2212\u03b7u\u03b7Tu + \u03be I, where\u03b7u = \u2211\u2200v\u2208Tu \u03c1(v)v \u2211\u2200v\u2208Tu \u03c1(v)\n(6)\n\u03c1(v) is the data density in the immediate vicinity of a point v \u2208 Tu. This is evaluated using \u03c3base for consistency across clusters. \u03b7u &\u03a3u are then basically the expectation and variance of the localized distribution. In practice, a small regularizer, \u03be , has to be added to the diagonals of \u03a3u to prevent degenerate fitting in sparse regions, and for numerical stability.While computing anisotropic updates, eigenvalue decomposition is employed and any eigenvalues of \u03a3u which fall below \u03be , are clamped to it. Note that \u03a3u always remains positive definite. Also note that all summations are computed on the fly.\nEq. 6 could also be thought of as density weighted trajectory set variance. As a cluster approaches a mode, mean shift trajectory points get more concentrated and are weighted more, leading to a conservative but more localized and robust estimate \u2013 more immune to long tails. Figs. 1, 5 plot the bandwidths and modes at convergence, for color and point data.\n4We note empirically for dense data, as in images, a simple cluster size sufficiency check works well. For joint domains, a cluster could switch to anisotropic updates when it has atleast max(dim(xr), dim(xs))2 members.\n(a) Effects of varying the detail and vicinity parameters on a brush painting with smudged colors.\nAcrylic & Oil \u2329 4,9, .5,16 \u232a \u2329 9,16, .75,25 \u232a \u2329 16,25,1,36 \u232a \u2329 25,36,2,81 \u232a \u2329 36,49,1.5,64 \u232a \u2329 49,64,1.5,100 \u232a \u2329 64,81,1,121 \u232a (b) Parameter sensitivity plots. Each of \u2329\u03c3rbase2 ,\u03c3sbase2 , \u03b52r , \u03b52s \u232a was varied while keeping others constant. Their effects on number of clusters, their average size, and iterations for convergence are plotted. Results were averaged over 33 images. As with conventional MS, color domain parameters are understandably more sensitive. \u03b4 = .01 was used.\nFigure 2: For joint domain AAAMS over images, we show the qualitative and quantitative effects of varying the detail and vicinity parameters, \u2329\u03c3rbase2 ,\u03c3sbase2 , \u03b52r , \u03b52s \u232a. Post processing was disabled, except for enforcing cluster contiguity. As can be seen, if the need be, a good control over smoothing and segmentation levels can be exercised."}, {"heading": "2.3 Cluster Merging", "text": "For any given data points, if their mean shift trajectories intersect, they will converge to a common local mode. Thus in the vicinity of a data point\u2019s trajectory (which is moving up some mode) - any data points in sufficient proximity, having their shift vectors deemed to be intersecting with this trajectory, could be clustered together. They will eventually end up converging on the same local mode. So we basically consider the data points in the vicinity of a cluster trajectory, u\u03c4 - with an epsilon \u03b5, delineating the vicinity. If a data point, y, in vicinity is ascertained (in MergeCheck) to be heading to the same mode as u\u03c4 , then by transitivity - all the members of its parent cluster, \u03a0(y), are heading to that mode too - the clusters u and \u03a0(y) , can then be merged. The cluster which is higher up the mode (higher density) assimilates the other cluster into itself, thus accelerating convergence to the mode. This also helps in avoiding spurious merges.\nMergeCheck - This is intentionally specified as a generic function returning a true/false value. It could be implemented to suit different feature spaces and clustering criteria. The more holistic this check is, the larger the operating range of \u03b5 can be (assuming the distance norm holds up), without impacting clustering stability. In our experiments, we used a very lightweight generic implementation that worked well over considered data spaces - basically verifying through inner product checks that 1) relative distance between u\u03c4+1 and y is decreasing and 2) Mean shift bearings 5 at u\u03c4+1 and y are in the same direction. We note though that divergence measures like Bhattacharya (Sec. 2.4), kernel induced feature space metrics ([26]), information-theoretic ones like Renyi\u2019s entropy ([14]) seem viable, interesting possibilities for MergeCheck. We are yet to experiment with them."}, {"heading": "2.4 Post Processing", "text": "Once data has been partitioned, a post processing step merges clusters with proximate modes, and ensures a minimum cluster size (in conventional Mean Shift, clusters are delineated only during the post process). Additionally for structured data, cluster contiguity could be enforced. We use graph operations. For structured data as in images, adjacency connections between clusters can be added naturally using a spatial grid structure. For unstructured data, connections between a cluster and all clusters within a reasonably large distance threshold (mode to mode distances) were added, to ensure a connected graph. Bhattacharya diver-\n5The bearing at u\u03c4+1 is mu. The bearing at y, given by y\u03c4=1\u2212y , is the mean shift vector resulting from the first iteration over the trivial cluster containing y; it\u2019s stored up for consequent use.\nAlgorithm 2 : Post Processing\n\u2022 (For structured data only) For each cluster, use spatial adjacency to ascertain the disconnected components (highest density/mode locations for these small disconnected point sets need to be recomputed). Each disconnected component forms an additional separate cluster thereon. \u2022 Build the adjacency graph. \u2022 Merge all clusters which fall below minimum desired size, to\nthe closest adjacent cluster until no such remain. \u2022 For each remaining cluster, using its constituent points, com-\npute the density weighted variances, similar to Eq.6 - this is representative of the cluster\u2019s stand-alone distribution and alleviates tail influences. \u2022 For each pair of remaining clusters {a, b}, connected by an adjacency edge, evaluate \u2192\ndB = 1 8 ( \u00b5a \u2212\u00b5b )T ( \u03a3a+\u03a3b 2 )\u22121 ( \u00b5a \u2212\u00b5b ) + 12 ln  det ( \u03a3a+\u03a3b 2 ) \u221a det(\u03a3a).det ( \u03a3b ) . If it falls below a certain threshold, merge the two.\nMethods / Score PRI GCE VoI BDE AAAMS .8230 .1589 2.1785 12.60 JMS\u2217 .7870 .1608 2.2484 13.34 Prior Art [21] FullSpectralOverMS [21] 0.8146 0.1809 1.8545 12.21 JMS [8] 0.7958 0.1888 1.9725 14.41 NCut [21] - Ref. [27] 0.7330 0.2662 2.6137 17.19 MNCUT [21] - Ref. [6] 0.7632 0.2234 2.2789 13.17 GBIS [21] - Ref. [9] 0.7139 0.1746 3.3949 16.67 Saliency [21] - Ref. [8] 0.7758 0.1768 1.8165 16.24 JSEG [21] - Ref. [7] 0.7756 0.1989 2.3217 14.40\nTable 1: Results on BSD300 [24]. We used a single parameter set \u232920,36,1,64\u232a for AAAMS. For better results, dB was set from {.25, .5,1,1.25,1.5,2}. JMS\u2217 parameters were selected per image to maintain similar segmentation levels, with an eye on preserving details, segment saliency. For perspective, we also reproduce results from [21] of unsupervised image segmentation methods. [21] selects segment levels per image. Top three values for each index are colored as rgb. AAAMS performs best overall - it\u2019s clearly ahead in PRI & GCE, and is a close second in BDE. Note that [21], which has the next best values, operates over a priori Mean Shift segmentations.\ngence ([2], dB) was used as the merging criteria. It takes into account not just the variance normalized mode proximity, but also the disparity in variances themselves (Mahanalobis measure is its special case). 0\u2264 dB \u2264 4 was a good range, with dB = 1 (somewhat analogous to 1\u2212 sigma2 disparity) performing well generally 6. Alg. 2 specifies the steps."}, {"heading": "3 Results", "text": "The base scalar parameter \u03c3base, in effect, regulates the minimum desired detail in the feature space, the smoothing level. The vicinity parameter, \u03b5, regulates cluster merge chances and hence cluster sizes. For images, with AAAMS operating over joint domains of \u3008color, space\u3009, the detail and vicinity parameters would be \u2329\u03c3 rbase, \u03c3 sbase\u232a and \u3008\u03b5r , \u03b5s\u3009 respectively (indicated in Alg. 1). Fig. 2, shows quantitative and qualitative effects of their variation. Although a good degree of control is possible to achieve a desired result, our experiments showed that any low valued set gave nice results over a good range of images.\nDue to agglomeration, the number of clusters decrease monotonically every iteration. Only a fraction of clusters remain after the first couple of iterations; with the cluster count falling rapidly in all early iterations. The scheme thus results in a drastic reduction in net mean shift computes - as compared to the hitherto style of clustering only after convergence, where computations happen for every data point, in each iteration. (for dense image data, typically less than 5% of the clusters remain by the 11th or 12th iteration). This serves to offset the additional computational workload arising from the use of full bandwidth matrices. Our straight up joint domain implementation was achieving similar timings on average to standard Mean Shift, which uses scalar bandwidths. Improvements in efficency based on fast nearest neighbor search such as exploiting grid structure of spatial domain, locally sensitive hashing ([16]) are applicable in our methodology too. Using Gaussian kernels, with a convergence delta, \u03b4 , set adequately to .01, merges would cease before 90th iteration, with convergence around the 100th. When just pre-partitioning is the end objective, the merging scheme thus allows us to fine tune stopping criteria. Along with the first iteration shift vectors, globally normalized local density values at each data point were stored for consequent use too. In each iteration, \u03c1(u\u03c4 ) was then approximated by the density value at u\u03c4 \u2019s nearest data point. We found perturbations to be generally useful, lending to mode detection robustness and more salient partitioning. A cluster at convergence can be perturbed a fixed number of times consecutively, with progressively damped magnitudes. u then, would not be brought\n6For images, since color similarity alone is of consequence, dB was evaluated only over the L\u2217a\u2217b\u2217 space\nSAWHNEY, CHRISTENSEN, BRADSKI: ANISOTROPIC ADAPTIVE MEAN SHIFT 9 Image JMS AAAMS JMS Labels AAAMS Labels\nLady JMS - 523 Labels, AAAMS - 490 Labels\nSoldier JMS - 617 Labels, AAAMS - 601 Labels\nBench JMS - 180 Labels, AAAMS - 165 Labels\nMandrill JMS - 752 Labels, AAAMS - 721 Labels\nFigure 3: AAAMS preserves more details and affects more perceptually salient segmentations, at similar clustering levels. We used a single parameter set, \u2329\u03c3rbase2 ,\u03c3sbase2 , \u03b52r , \u03b52s \u232a = \u232915,16,1,81\u232a with dB = 1, to show its adaptivity on varied images. JMS segments were kept around the same, with eye on preserving detail; it still smooths over at places. Its parameter values varied significantly from image to image - \u03c3r2 \u2208 [49, 81] , \u03c3s2 \u2208 [100, 289]. Minimum cluster size was 10.\nout of contention in the next iteration - although the immediate trajectory point resulting from the perturbation will not be included in Tu. The results presented in this paper though, are with perturbations disabled.\nFor image data, comparisons (Figs. 3, 4, Table. 1 7) are shown with joint domain Mean Shift implementation (JMS) from EDISON ([8]), over Berkely Segmentation Dataset ([24], BSD300). BSD300 is meant for supervised algorithms - we simply clubbed the training and test images together. For sake of completeness, prior art on unsupervised image segmentation is also shown in Table. 1. All indicated parameter values for AAAMS and JMS are squared. We did not search for the best performing parameter set for AAAMS, opting for a single low valued set instead. AAAMS performed significantly better than JMS, with results superior to other unsupervised image segmentation methods as well.\nOur experiments indicated that low base bandwidths, \u2329\u03c3 rbase, \u03c3 sbase\u232a, performed generally 7Probabilistic Rand Index (PRI), Variation of Information (VoI), Global Consistency Error (GCE), Boundary Displacement Error (BDE). The first three are clustering purity measures. PRI is a measure of the fraction of pairs of points whose labels are consistent with a given labeling. VoI and BDE are relative distance metrics between two given segmentations, based on average conditional entropy and boundary pixel difference, respectively. GCE measures the extent to which one labeling can be viewed as a refinement of the other. Higher is better for PRI while lower is better for the other three. For BSD300, the values indicate how well a segmentation corresponds to ones by human subjects. We noticed that coarser segmentatios tended to give better values. This, we suppose, was because humans tend to utilize much more comprehensive cues, and incorporate object or more holistic level semantics in their segmentations. It was noticed that PRI corresponded better to low level segment saliency than others."}, {"heading": "10 SAWHNEY, CHRISTENSEN, BRADSKI: ANISOTROPIC ADAPTIVE MEAN SHIFT", "text": "Image JMS AAAMS JMS Labels AAAMS Labels\nAeroplane JMS - 14 Labels, AAAMS - 11 Labels\nColorWheel JMS - 51 Labels, AAAMS - 48 Labels\nWasp JMS - 150 Labels, AAAMS - 111 Labels\nFigure 4: More parsimonious segmentations were quite often not achievable with JMS - some varied examples are shown above (Images such as Lady in Fig.3 are a typical case too). Both methods were configured for reduced label usage. Minimum cluster size was 10. JMS, at its limit, is breaking boundaries and under segmenting. AAAMS with lesser labels, does not break boundaries, still maintains segment saliency.\nFigure 5: Single domain clustering examples over color data (top row, 11 clusters) and simulated gaussian mixtures (second row) in 2D & 3D respectively. 1\u2212 sigma final trajectory-set bandwidths have been overlaid at converged mode positions.\nData \u3008#Dims,#Classes\u3009 PRI GCE VoI Seeds \u30087D,3\u3009 .89 / .86 / .87 .17 / .20 / .19 0.85 / 0.98 / 0.93 Yeast \u30088D,10\u3009 .69 / .61 / .67 .44 / .39 / .47 3.03 / 3.10 / 3.22 Letters \u300816D,26\u3009 .87 / .86 / .83 .67 / .70 / .62 4.96 / 5.16 / 4.72\nTable 2: Results on higher dimension real world datasets from [1], with a single kernel. Indicated values are of AAAMS / MS / VariableMS ([16]) respectively, with best values in red.\nwell on a good range of images (Fig. 3). This was due to the presented approach being locally adaptive and anisotropic. At similar clustering levels, AAAMS preserved more details and affected more salient segmentations.\nSingle kernel AAAMS was tested on images and 2D, 3D gaussian mixtures at varied scales - with nice results. AAAMS results in Figs. 1(a), 5 are with postprocessing disabled. As indicated in Figs. 1(a), 5, reasonable local bandwidths arise, robustly identifying modes and salient clusters, by adapting according to local structure.\nExperiments were conducted with some higher dimension datasets from [1] as well. Table. 2 shows initial results, along with comparisons with single domain standard Mean Shift (MS), and [16]\u2019s isotropic variable bandwidth implementation. Cluster count was kept the same as class count. AAAMS post-processing was disabled. [16] first determines isotropic point bandwidths using the kth nearest neighbor distance heuristic, and subsequently utlizes them in single kernel mean shift iterations. Our experiments with it indicated a lack of clustering control. The datasets were meant for supervised classification, with attributes/feature components at different scales, and having uncorrelated and/or uninformative dimensions. Without any pre-processing (normalizations, component analysis) decent results were attained with a single kernel AAAMS. Note that [16] internally normalizes the data, while AAAMS & MS results are without any normalizations.\nPromising results, both qualitative and quantitative, are indicative of the efficacy of the presented approach. We intend to experiment further, especially with different merging schemes and on varied data spaces."}, {"heading": "4 Conclusion", "text": "A generalized methodology for feature space partitioning and mode seeking was presented - leveraging synergism of adaptive, anisotropic Mean Shift and guided agglomeration. Unsupervised adaptation of full anisotropic bandwidths is useful and further enables Mean Shift clustering. We are excited about its prospects on point-normal clouds and video streams.\nOur experiments did indicate sparse data to be an issue. This is understandable, as it encumbers cluster growth and bandwidth development, with AAAMS behaving like conventional Mean Shift then. Future work would also focus on alleviating this issue."}], "references": [{"title": "On a measure of divergence between two multinomial populations", "author": ["Anil Bhattacharyya"], "venue": "Sankhya\u0304: The Indian Journal of Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1946}, {"title": "Real time face and object tracking as a component of a perceptual user interface", "author": ["Gary R Bradski"], "venue": "In Applications of Computer Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Carreira-Perpi\u00f1\u00e1n. Fast nonparametric clustering with gaussian blurring mean-shift", "author": ["\u00c1. Miguel"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Gaussian Mean-Shift is an EM algorithm", "author": ["Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Data-driven density derivative estimation, with applications to nonparametric clustering and bump hunting", "author": ["Jos\u00e9 E Chac\u00f3n", "Tarn Duong"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Mean shift, mode seeking, and clustering", "author": ["Yizong Cheng"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Synergism in low level vision", "author": ["Christopher M Christoudias", "Bogdan Georgescu", "Peter Meer"], "venue": "In Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Kernel-based object tracking", "author": ["D. Comaniciu", "V. Ramesh", "P. Meer"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "An algorithm for data-driven bandwidth selection", "author": ["Dorin Comaniciu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Mean shift: A robust approach toward feature space analysis", "author": ["Dorin Comaniciu", "Peter Meer"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "The variable bandwidth mean shift and data-driven scale selection", "author": ["Dorin Comaniciu", "Visvanathan Ramesh", "Peter Meer"], "venue": "In Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Feature significance for multivariate kernel density estimation", "author": ["Tarn Duong", "Arianna Cowling", "Inge Koch", "MP Wand"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Information theoretic feature selection and projection. In Speech, Audio, Image and Biomedical Signal Processing using Neural Networks, pages 1\u201322", "author": ["Deniz Erdogmus", "Umut Ozertem", "Tian Lan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "The estimation of the gradient of a density function, with applications in pattern recognition", "author": ["Keinosuke Fukunaga", "Larry Hostetler"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1975}, {"title": "Mean shift based clustering in high dimensions: A texture classification example", "author": ["Bogdan Georgescu", "Ilan Shimshoni", "Peter Meer"], "venue": "In Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Vopatov\u00e1. Full bandwidth matrix selectors for gradient kernel density estimate", "author": ["Ivana Horov\u00e1", "Jan Kol\u00e1\u010dek", "Kamila"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Adaptive mean-shift tracking with novel color model", "author": ["Mun-Ho Jeong", "Bum-Jae You", "Yonghwan Oh", "Sang-Rok Oh", "Sang-Hwi Han"], "venue": "In Mechatronics and Automation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Segmenting brain mri using adaptive mean shift", "author": ["R.J. Jimenez-Alaniz", "M. Pohi-Alfaro", "V. Medina-Bafluelos", "O. Yaflez-Suarez"], "venue": "In Engineering in Medicine and Biology Society,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Event detection in crowded videos", "author": ["Yan Ke", "Rahul Sukthankar", "Martial Hebert"], "venue": "In Computer Vision,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Learning full pairwise affinities for spectral segmentation", "author": ["Tae Hoon Kim", "Kyoung Mu Lee", "Sang Uk Lee"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Robust higher order potentials for enforcing label consistency", "author": ["Pushmeet Kohli", "Philip HS Torr"], "venue": "International Journal of Computer Vision,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Scale-invariant object categorization using a scaleadaptive mean-shift search", "author": ["Bastian Leibe", "Bernt Schiele"], "venue": "In Pattern Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["David Martin", "Charless Fowlkes", "Doron Tal", "Jitendra Malik"], "venue": "In Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "An adaptive mean-shift framework for mri brain segmentation", "author": ["Arnaldo Mayer", "Hayit Greenspan"], "venue": "Medical Imaging, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Mean shift spectral clustering", "author": ["Umut Ozertem", "Deniz Erdogmus", "Robert Jenssen"], "venue": "Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "A topological approach to hierarchical segmentation using mean shift", "author": ["Sylvain Paris", "Fr\u00e9do Durand"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Real-time human pose recognition in parts from single depth images", "author": ["Jamie Shotton", "Toby Sharp", "Alex Kipman", "Andrew Fitzgibbon", "Mark Finocchio", "Andrew Blake", "Mat Cook", "Richard Moore"], "venue": "Communications of the ACM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Hierarchical evolving mean-shift", "author": ["M. Surkala", "K. Mozdren", "R. Fusek", "E. Sojka"], "venue": "In Image Processing (ICIP),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Hierarchical blurring mean-shift", "author": ["Milan \u0160urkala", "Karel Mozd\u0159e\u0148", "Radovan Fusek", "Eduard Sojka"], "venue": "In Advances Concepts for Intelligent Vision Systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Toward objective evaluation of image segmentation algorithms", "author": ["Ranjith Unnikrishnan", "Caroline Pantofaru", "Martial Hebert"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Quick shift and kernel methods for mode seeking", "author": ["Andrea Vedaldi", "Stefano Soatto"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Robust scale-adaptive mean-shift for tracking", "author": ["Tomas Vojir", "Jana Noskova", "Jiri Matas"], "venue": "In Image Analysis,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Image and video segmentation by anisotropic kernel mean shift", "author": ["Jue Wang", "Bo Thiesson", "Yingqing Xu", "Michael Cohen"], "venue": "Vision-ECCV", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Multiple class segmentation using a unified framework over mean-shift patches", "author": ["Lin Yang", "Peter Meer", "David J Foran"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Agglomerative mean-shift clustering", "author": ["Xiao-Tong Yuan", "Bao-Gang Hu", "Ran He"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Accelerated convergence using dynamic mean shift", "author": ["Kai Zhang", "Jamesk T Kwok", "Ming Tang"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}], "referenceMentions": [{"referenceID": 13, "context": "\u2018Mean Shift\u2019 ([15, 7], MS) is a powerful nonparametric technique for unsupervised pattern clustering and mode seeking.", "startOffset": 14, "endOffset": 21}, {"referenceID": 5, "context": "\u2018Mean Shift\u2019 ([15, 7], MS) is a powerful nonparametric technique for unsupervised pattern clustering and mode seeking.", "startOffset": 14, "endOffset": 21}, {"referenceID": 9, "context": "References [11, 3] established it\u2019s utility in low-level perception tasks such as feature clustering, filtering and in tracking.", "startOffset": 11, "endOffset": 18}, {"referenceID": 1, "context": "References [11, 3] established it\u2019s utility in low-level perception tasks such as feature clustering, filtering and in tracking.", "startOffset": 11, "endOffset": 18}, {"referenceID": 26, "context": "It has been in popular use since, as a very useful tool for pattern clustering of sensor data ([28, 14] for example).", "startOffset": 95, "endOffset": 103}, {"referenceID": 12, "context": "It has been in popular use since, as a very useful tool for pattern clustering of sensor data ([28, 14] for example).", "startOffset": 95, "endOffset": 103}, {"referenceID": 20, "context": "It has also found niche as a preprocessor (a priori segmentation, smoothing) before higher level image & video analysis tasks such as scene parsing, object recognition, detection ([22, 35, 20]).", "startOffset": 180, "endOffset": 192}, {"referenceID": 33, "context": "It has also found niche as a preprocessor (a priori segmentation, smoothing) before higher level image & video analysis tasks such as scene parsing, object recognition, detection ([22, 35, 20]).", "startOffset": 180, "endOffset": 192}, {"referenceID": 18, "context": "It has also found niche as a preprocessor (a priori segmentation, smoothing) before higher level image & video analysis tasks such as scene parsing, object recognition, detection ([22, 35, 20]).", "startOffset": 180, "endOffset": 192}, {"referenceID": 20, "context": "Image segmentation approaches such as Markov Random Fields, Spectral clustering, Hierarchical clustering use it as an a priori segmenter with improved results ([22, 26, 21, 31, 30]).", "startOffset": 160, "endOffset": 180}, {"referenceID": 24, "context": "Image segmentation approaches such as Markov Random Fields, Spectral clustering, Hierarchical clustering use it as an a priori segmenter with improved results ([22, 26, 21, 31, 30]).", "startOffset": 160, "endOffset": 180}, {"referenceID": 19, "context": "Image segmentation approaches such as Markov Random Fields, Spectral clustering, Hierarchical clustering use it as an a priori segmenter with improved results ([22, 26, 21, 31, 30]).", "startOffset": 160, "endOffset": 180}, {"referenceID": 29, "context": "Image segmentation approaches such as Markov Random Fields, Spectral clustering, Hierarchical clustering use it as an a priori segmenter with improved results ([22, 26, 21, 31, 30]).", "startOffset": 160, "endOffset": 180}, {"referenceID": 28, "context": "Image segmentation approaches such as Markov Random Fields, Spectral clustering, Hierarchical clustering use it as an a priori segmenter with improved results ([22, 26, 21, 31, 30]).", "startOffset": 160, "endOffset": 180}, {"referenceID": 9, "context": "Its popular standard form, [11], utilizes fixed, scalar bandwidth assuming homoscedasticity and isotropicity.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "The adaptive Mean Shift variants, [12, 16], ascertain variable bandwidths, but they still assume isotropicity.", "startOffset": 34, "endOffset": 42}, {"referenceID": 14, "context": "The adaptive Mean Shift variants, [12, 16], ascertain variable bandwidths, but they still assume isotropicity.", "startOffset": 34, "endOffset": 42}, {"referenceID": 4, "context": "Offline bandwidth selection methods for Mean Shift ([6, 17, 10]), typically estimate a single, global bandwidth, and/or are data specific/non-automatic.", "startOffset": 52, "endOffset": 63}, {"referenceID": 15, "context": "Offline bandwidth selection methods for Mean Shift ([6, 17, 10]), typically estimate a single, global bandwidth, and/or are data specific/non-automatic.", "startOffset": 52, "endOffset": 63}, {"referenceID": 8, "context": "Offline bandwidth selection methods for Mean Shift ([6, 17, 10]), typically estimate a single, global bandwidth, and/or are data specific/non-automatic.", "startOffset": 52, "endOffset": 63}, {"referenceID": 14, "context": "(b) Comparitive results with standard MS (left) and variable-bandwidth isotropic MS ([16], right), at similar clustering levels, 25 & 27 respectively, are shown.", "startOffset": 85, "endOffset": 89}, {"referenceID": 14, "context": "MS with correctly chosen bandwidth detected more coherent modes than [16], but looses partition saliency (bushes, water, sky in background).", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "[16] better adapts to scales but oversegments at places, and smooths over others (face).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "We utilize the exposition style of [5].", "startOffset": 35, "endOffset": 38}, {"referenceID": 9, "context": "K(t), t \u2265 0, is a d-variate kernel with compact support satisfying some regularity constraints, mild in practice ([11, 5] for details).", "startOffset": 114, "endOffset": 121}, {"referenceID": 3, "context": "K(t), t \u2265 0, is a d-variate kernel with compact support satisfying some regularity constraints, mild in practice ([11, 5] for details).", "startOffset": 114, "endOffset": 121}, {"referenceID": 9, "context": "In our approach, data points (pertaining to a cluster) converging to a common local mode share a common bandwidth - one which reflects this mode\u2019s structure, and to an extent, its basin of attraction ([11]).", "startOffset": 201, "endOffset": 205}, {"referenceID": 8, "context": "We refer to it as the local bandwidth ([10] utilizes local bandwidths in a related sense).", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "In online unsupervised usage, almost all Mean Shift variants for clustering, for example [11, 30, 37, 27], work under the restrictive assumptions of homoscedasticity and isotropicity (\u03a3i = \u03c32I, standard fixed bandwidth Mean Shift).", "startOffset": 89, "endOffset": 105}, {"referenceID": 28, "context": "In online unsupervised usage, almost all Mean Shift variants for clustering, for example [11, 30, 37, 27], work under the restrictive assumptions of homoscedasticity and isotropicity (\u03a3i = \u03c32I, standard fixed bandwidth Mean Shift).", "startOffset": 89, "endOffset": 105}, {"referenceID": 35, "context": "In online unsupervised usage, almost all Mean Shift variants for clustering, for example [11, 30, 37, 27], work under the restrictive assumptions of homoscedasticity and isotropicity (\u03a3i = \u03c32I, standard fixed bandwidth Mean Shift).", "startOffset": 89, "endOffset": 105}, {"referenceID": 25, "context": "In online unsupervised usage, almost all Mean Shift variants for clustering, for example [11, 30, 37, 27], work under the restrictive assumptions of homoscedasticity and isotropicity (\u03a3i = \u03c32I, standard fixed bandwidth Mean Shift).", "startOffset": 89, "endOffset": 105}, {"referenceID": 34, "context": "[36] utilizes set covering based iterative agglomeration for improved efficiency.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "\u03c3i is estimated using a variation of the following two heuristics ([12, 16]) - 1) kth nearest neighbor, xk i , distance heuristic \u2192 \u03c3i \u221d \u2016xi\u2212 xk i \u2016, or 2) Abramson\u2019s heuristic \u2192 \u03c3i \u221d \u03c3o(\u03c0(xi))/2, where \u03c0(x) is the pilot density estimate obtained by first running mean shift with analysis bandwidth, \u03c3o.", "startOffset": 67, "endOffset": 75}, {"referenceID": 14, "context": "\u03c3i is estimated using a variation of the following two heuristics ([12, 16]) - 1) kth nearest neighbor, xk i , distance heuristic \u2192 \u03c3i \u221d \u2016xi\u2212 xk i \u2016, or 2) Abramson\u2019s heuristic \u2192 \u03c3i \u221d \u03c3o(\u03c0(xi))/2, where \u03c0(x) is the pilot density estimate obtained by first running mean shift with analysis bandwidth, \u03c3o.", "startOffset": 67, "endOffset": 75}, {"referenceID": 23, "context": "They have found more use in smoothing type applications as reported in [25, 19].", "startOffset": 71, "endOffset": 79}, {"referenceID": 17, "context": "They have found more use in smoothing type applications as reported in [25, 19].", "startOffset": 71, "endOffset": 79}, {"referenceID": 16, "context": "Variants have also been used in tracking scenarios, where the bandwidths are adapted in a task specific fashion (see [18, 9], for example).", "startOffset": 117, "endOffset": 124}, {"referenceID": 7, "context": "Variants have also been used in tracking scenarios, where the bandwidths are adapted in a task specific fashion (see [18, 9], for example).", "startOffset": 117, "endOffset": 124}, {"referenceID": 21, "context": "[23, 33] adapt isotropic bandwidths to object scales, to unimodally track, search for them.", "startOffset": 0, "endOffset": 8}, {"referenceID": 31, "context": "[23, 33] adapt isotropic bandwidths to object scales, to unimodally track, search for them.", "startOffset": 0, "endOffset": 8}, {"referenceID": 25, "context": "The topological, blurring, evolving variants for clustering (like [27, 30, 37, 4, 29]) use isotropic bandwidths.", "startOffset": 66, "endOffset": 85}, {"referenceID": 28, "context": "The topological, blurring, evolving variants for clustering (like [27, 30, 37, 4, 29]) use isotropic bandwidths.", "startOffset": 66, "endOffset": 85}, {"referenceID": 35, "context": "The topological, blurring, evolving variants for clustering (like [27, 30, 37, 4, 29]) use isotropic bandwidths.", "startOffset": 66, "endOffset": 85}, {"referenceID": 2, "context": "The topological, blurring, evolving variants for clustering (like [27, 30, 37, 4, 29]) use isotropic bandwidths.", "startOffset": 66, "endOffset": 85}, {"referenceID": 27, "context": "The topological, blurring, evolving variants for clustering (like [27, 30, 37, 4, 29]) use isotropic bandwidths.", "startOffset": 66, "endOffset": 85}, {"referenceID": 30, "context": "[32] presents improvements over the somewhat related Mediod Shift.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In offline settings, [10] presents a supervised methodology.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "Only recently were automatic full bandwidth selectors for density gradient estimation proposed in [17, 6], for offline settings.", "startOffset": 98, "endOffset": 105}, {"referenceID": 4, "context": "Only recently were automatic full bandwidth selectors for density gradient estimation proposed in [17, 6], for offline settings.", "startOffset": 98, "endOffset": 105}, {"referenceID": 9, "context": "A very useful variant is Joint Domain Mean Shift, [11], which is used to create partitions jointly respecting the dataset\u2019s multiple feature domains which are mutually independent; For example, \u3008color,space\u3009 in color based segmentation & smoothing, and \u3008color, f low\u3009 in motion segmentation.", "startOffset": 50, "endOffset": 54}, {"referenceID": 29, "context": "As noted in [31] on color segmentation, \u03c3 r and \u03c3 s need to be selected carefully.", "startOffset": 12, "endOffset": 16}, {"referenceID": 32, "context": "Reference [34] utlizes an anisotropic \u03a3i for visual data segmentations.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "A reasonable test of significance for \u03a3u estimates, is to check if the kernel weighted point count or Effective Sample Size (ESS, [13]) is above some value, \u03bb .", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "As a binomial rule of thumb ([13]), \u03bb = 5 is chosen as the minimum ESS, which is analogous to choosing 5 as the minimum individual expected cell counts in a \u03c72 test of independence.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "Bandwidth estimates based on a cluster\u2019s member data point locations are not reliable ([10] notes this too).", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "4), kernel induced feature space metrics ([26]), information-theoretic ones like Renyi\u2019s entropy ([14]) seem viable, interesting possibilities for MergeCheck.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "4), kernel induced feature space metrics ([26]), information-theoretic ones like Renyi\u2019s entropy ([14]) seem viable, interesting possibilities for MergeCheck.", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "34 Prior Art [21] FullSpectralOverMS [21] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "34 Prior Art [21] FullSpectralOverMS [21] 0.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "21 JMS [8] 0.", "startOffset": 7, "endOffset": 10}, {"referenceID": 19, "context": "41 NCut [21] - Ref.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "[27] 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "19 MNCUT [21] - Ref.", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": "[6] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "17 GBIS [21] - Ref.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "[9] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "67 Saliency [21] - Ref.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "[8] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "24 JSEG [21] - Ref.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "[7] 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Table 1: Results on BSD300 [24].", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "For perspective, we also reproduce results from [21] of unsupervised image segmentation methods.", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "[21] selects segment levels per image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Note that [21], which has the next best values, operates over a priori Mean Shift segmentations.", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "gence ([2], dB) was used as the merging criteria.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "Improvements in efficency based on fast nearest neighbor search such as exploiting grid structure of spatial domain, locally sensitive hashing ([16]) are applicable in our methodology too.", "startOffset": 144, "endOffset": 148}, {"referenceID": 6, "context": "1 7) are shown with joint domain Mean Shift implementation (JMS) from EDISON ([8]), over Berkely Segmentation Dataset ([24], BSD300).", "startOffset": 78, "endOffset": 81}, {"referenceID": 22, "context": "1 7) are shown with joint domain Mean Shift implementation (JMS) from EDISON ([8]), over Berkely Segmentation Dataset ([24], BSD300).", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "Indicated values are of AAAMS / MS / VariableMS ([16]) respectively, with best values in red.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "2 shows initial results, along with comparisons with single domain standard Mean Shift (MS), and [16]\u2019s isotropic variable bandwidth implementation.", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "[16] first determines isotropic point bandwidths using the kth nearest neighbor distance heuristic, and subsequently utlizes them in single kernel mean shift iterations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Note that [16] internally normalizes the data, while AAAMS & MS results are without any normalizations.", "startOffset": 10, "endOffset": 14}], "year": 2014, "abstractText": "Mean Shift today, is widely used for mode detection and clustering. The technique though, is challenged in practice due to assumptions of isotropicity and homoscedasticity. We present an adaptive Mean Shift methodology that allows for full anisotropic clustering, through unsupervised local bandwidth selection. The bandwidth matrices evolve naturally, adapting locally through agglomeration, and in turn guiding further agglomeration. The online methodology is practical and effecive for low-dimensional feature spaces, preserving better detail and clustering salience. Additionally, conventional Mean Shift either critically depends on a per instance choice of bandwidth, or relies on offline methods which are inflexible and/or again data instance specific. The presented approach, due to its adaptive design, also alleviates this issue with a default form performing generally well. The methodology though, allows for effective tuning of results.", "creator": "LaTeX with hyperref package"}}}