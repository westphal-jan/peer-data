{"id": "1411.4166", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2014", "title": "Retrofitting Word Vectors to Semantic Lexicons", "abstract": "we similarly have proposed a simple, effective and fast method named context retrofitting to improve word vectors using word relation knowledge found in semantic lexicons constructed either, automatically or by real humans. retrofitting is used as a crucial post - processing step to improve vector pattern quality and is simpler to use than other validation approaches that use semantic information abstraction while maintaining training. it can be used indefinitely for improving vectors obtained from any word root vector training model and performs better than current state - of - the - art approaches to semantic enrichment of word vectors. we validated the applicability of our method across tasks, existing semantic lexicons, and languages.", "histories": [["v1", "Sat, 15 Nov 2014 17:34:20 GMT  (93kb,D)", "http://arxiv.org/abs/1411.4166v1", "Appeared in NIPS Deep learning and representation learning workshop 2014"], ["v2", "Fri, 19 Dec 2014 04:16:50 GMT  (191kb,D)", "http://arxiv.org/abs/1411.4166v2", "Appeared in NIPS Deep learning and Representation learning workshop 2014"], ["v3", "Sun, 1 Feb 2015 03:13:17 GMT  (199kb,D)", "http://arxiv.org/abs/1411.4166v3", null], ["v4", "Sun, 22 Mar 2015 17:55:20 GMT  (200kb,D)", "http://arxiv.org/abs/1411.4166v4", "Proceedings of NAACL 2015"]], "COMMENTS": "Appeared in NIPS Deep learning and representation learning workshop 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["manaal faruqui", "jesse dodge", "sujay kumar jauhar", "chris dyer", "eduard h hovy", "noah a smith"], "accepted": true, "id": "1411.4166"}, "pdf": {"name": "1411.4166.pdf", "metadata": {"source": "CRF", "title": "Retrofitting Word Vectors to Semantic Lexicons", "authors": ["Manaal Faruqui", "Jesse Dodge", "Sujay K. Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "emails": ["mfaruqui@cs.cmu.edu", "jessed@cs.cmu.edu", "sjauhar@cs.cmu.edu", "cdyer@cs.cmu.edu", "ehovy@cs.cmu.edu", "nasmith@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Data-driven learning of word vectors that capture lexico-semantic properties is a technique of central importance in natural language processing. These word vectors can in turn be used for identifying semantically related word pairs [1, 2] or as features in downstream text processing applications [3]. A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics [4] and using internal representations from neural network models of word sequences [5].\nBecause of their value as lexical semantic representations, there has been much research on improving the quality of vectors. For example, cooccurrence statistics have been expanded to incorporate multilingual context [6, 7, 8] or define context in terms of dependency links [9]. However, a notable absence in vector construction techniques is the use of information in semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations between word types (or, in some cases, between sense-distinguished word types). Examples of such resources include WordNet [10], FrameNet [11] and the Paraphrase database [12].\nThe contribution of this paper is a pair of graph-based learning techniques for leveraging such resources to obtain higher quality semantic vectors. The first, which we call \u201cretrofitting\u201d, is applied as a post-processing step and uses belief propagation on a graph constructed from lexicon-derived relational information to update word vectors (\u00a72). Intuitively, this method encourages the new vectors to be (i) similar to the vectors of related word types and (ii) similar to their starting representations. Retrofitting is extremely fast, takes about 5 seconds for a graph of 102,000 words and its run-time is independent of the original word vector model. The second approach formulates the relationship between words found in the semantic lexicon as a structured regulariser on the original training objective (\u00a73).\nar X\niv :1\n41 1.\n41 66\nv1 [\ncs .C\nL ]\n1 5\nN ov\nExperimentally, we show that our methods work well with different state-of-the-art word vector models (\u00a74) while using different kinds of semantic lexicons (\u00a75) and gives substantial improvements on a variety of vector evaluations tasks (\u00a76) in multiple languages (\u00a77.4). Furthermore, retrofitting\u2014 which is applied as a post-processing step to vectors of any kind\u2014performs at least as well or better than using lexical knowledge during training."}, {"heading": "2 Retrofitting with Semantic Lexicons", "text": "Let V = {w1, . . . , wn} be a vocabulary, i.e. the set of word types, and \u2126 be an ontology that encodes semantic relations between words in V . We represent \u2126 as an undirected graph (V,E) with one vertex for each word type and edges (wi, wj) \u2208 E \u2286 V \u00d7 V indicating a semantic relationship of interest. These relations differ for different semantic lexicons and are described later (\u00a75).\nThe matrix Q\u0302 will be the collection of vector representations q\u0302i \u2208 Rn, for each wi \u2208 V , learned using a standard data-driven technique. Our objective is to learn the matrix Q = (q1, . . . , qn) such that the columns are both close (under a distance metric) to their counterparts in Q\u0302 and to adjacent vertices in \u2126. Figure 1 shows a small word graph with such edge connections; white nodes are labeled with the Q vectors to be retrofitted (and correspond to V\u2126); filled nodes are labeled with the corresponding vectors in Q\u0302, which are observed. The graph can be interpreted as a Markov random field [13].\nThe distances between a pair of vectors is defined to be the Euclidean distance. Since we want the inferred word vector to be close to the observed value q\u0302i and close to its neighbors qj ,\u2200j such that (i, j) \u2208 E, the objective to be minimized becomes:\n\u03a8(Q) = n\u2211 i=1 \u03b1i\u2016qi \u2212 q\u0302i\u20162 + \u2211 (i,j)\u2208E \u03b2ij\u2016qi \u2212 qj\u20162 \nwhere \u03b1 and \u03b2 values control the relative strengths of associations (\u00a77.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. \u03a8 is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method [14, 15, 16, 17]. We take the first derivative of \u03a8 with respect to one qi vector, and by equating it to zero arrive at the following online update:\nqi =\n\u2211 j:(i,j)\u2208E \u03b2ijqj + \u03b1q\u0302i\u2211\nj:(i,j)\u2208E \u03b2ij + \u03b1 . (1)\nIn practice running this procedure for 10 iterations leads to convergence i.e, euclidean distance between \u2200i, qi of consecutive iterations < 10\u22123. Any kind of word vector representation can be retrofitted with this approach. We initialize the vectors in Q to be equal to the vectors in Q\u0302."}, {"heading": "3 Semantic Lexicons during Learning", "text": "Many word vector learning methods are cast as maximum likelihood estimation problems that rely on gradient-based algorithms to update word representations to iteratively improve the loglikelihood [18, 19, 20, inter alia]. In these cases, instead of \u201cretrofitting\u201d vectors, we can alter the learning objective with a prior that encourages semantically related vectors (in \u2126) to be close together. In this setting, semantic lexicons can play the role of a prior on Q which we define as follows:\np(Q) \u221d exp \u2212\u03b3 n\u2211 i=1 \u2211 j:(i,j)\u2208E \u03b2ij\u2016qi \u2212 qj\u20162  , (2)\nwhere \u03b3 is a hyperparameter that controls the strength of the prior. This prior on the word vector parameters forces words connected in the lexicon to have close vector representations as did \u03a8(Q) (with the role of Q\u0302 being played by cross entropy to the empirical distribution).\nIncorporating this prior during estimation of Q equates to maximum a posteriori (MAP) estimation. We consider two techniques. In the first, we apply adaptive gradient descent (AdaGrad [21]), using the sum of gradients of the log-likelihood (given by the extant vector learning model) and the logprior (from Eq. 2), with respect to Q. Since computing gradient of Eq. 2 is linear in the vocabulary size n, we use lazy updates [22] every k words during training.1 We call this the lazy method of Bayesian retrofitting during training. The second technique applies stochastic gradient descent to the log-likelihood, and after every k words applies the update in Eq. 1. We call this the periodic method of retrofitting during training."}, {"heading": "4 Word Vector Representations", "text": "We test our retrofitting model on several different models of word vector representations described below. In some cases we use pre-trained vectors; in others we train models on our own data.\nLatent Semantic Analysis (LSA) We perform latent semantic analysis [4] on a word-word cooccurrence matrix. We construct a word co-occurrence frequency matrix for a given training corpus where each row corresponds to a word type in the corpus and each column corresponds to a context feature. In our case, every column/context is a word which might occur in a window of 5 positions around the target word; for scalability, we include words with frequency \u2265 10 and exclude the 100 most frequent words. We then replace every entry in the sparse frequency matrix by its pointwise mutual information (PMI) [23, 24] resulting in X. We factorize the matrix X = U\u03a3V> using singular value decomposition (SVD) [25]. Finally, we obtain a reduced dimensional representation of words from size O(n) to k by selecting the first k columns of U.\nSkip-Gram Vectors (SG) The word2vec tool [20] is fast and currently in wide use.2 In this model, each word\u2019s Huffman code is used as an input to a log-linear classifier with a continuous projection layer; words within 5 positions are predicted.\nGlobal Context Vectors (GC) These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features [26]. (We use pre-trained vectors.3)\nMultilingual Vectors (Multi) Faruqui and Dyer [8] learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. (We use pre-trained vectors.4)\n1i.e., if the gradient update for one training instance is g, the lazy gradient would be kg and would be computed after every k training instances.\n2https://code.google.com/p/word2vec 3http://nlp.stanford.edu/\u02dcsocherr/ACL2012_wordVectorsTextFile.zip 4http://www.wordvectors.org/web-eacl14-vectors/de-projected-en-512.txt.\ngz\nLog-bilinear Vectors (LBL) The log bilinear language model [19] predicts a word token w\u2019s vector given the set of words in its context (h), also represented as vectors:\np(w | h;Q) \u221d exp (\u2211 i\u2208h q>i qj + bj ) (3)\nSince it is costly to renormalize over the whole vocabulary, we use noise constrastive estimation (NCE) to estimate the parameters of the model [19] using AdaGrad [21] with a learning rate of 0.05."}, {"heading": "5 Semantic Lexicons", "text": "We use three different semantic lexicons to evaluate their utility in improving the word vectors.\nParaphrase Database. The paraphrase database (PPDB; Ganitkevitch et al., 2013) is a semantic lexicon containing more than 220 million paraphrase pairs of English.5 Of these, 8 million are lexical (single word to single word) paraphrases. The key intuition behind the acquisition of its lexical paraphrases is that two words in one language that align, in parallel text, to the same word in a different language, should be synonymous. For example, if the words jailed and imprisoned map to the same word in another language, it may be reasonable to assume they have the same meaning.\nIn our experiments, we instantiate an edge in E for each lexical paraphrase in PPDB. The lexical paraphrase dataset comes in different sizes ranging from S to XXXL, in decreasing order of paraphrasing confidence and increasing order of size. We chose XL size for our experiments; it produces a graph of 103,000 nodes and 230,000 edges. Since PPDB is an automatically created ontology, it includes a confidence score for each paraphrase. We experimented with (1) setting each (directed) edge weight \u03b2ij to be degree(i)\u22121; and (2) setting \u03b2ij to be the average of the two directed confidence scores provided in PPDB, interpreted as p(wi | wj) and p(wj | wi). In both settings, all \u03b1i are fixed to 1. We refer to these graphs, respectively, as PPDB and PPDB Weighted.\nWordNet WordNet [10] is a large human-constructed semantic lexicon of English words. It groups English words into sets of synonyms called synsets, provides short, general definitions, and records the various semantic relations between synsets. This database is structured in a graph particularly suitable for our task because it explicitly relates concepts with semantically aligned relations such as hypernyms and hyponyms. For example, the word dog has a synonym canine, a hypernym puppy and a hyponym animal. We perform two different experiments with WordNet: (1) connecting a word only to synonyms, and (2) including edges for synonym, hypernym, and hyponym relations. Respectively, these lead to graphs of 148, 000 nodes and 560, 000 edges. We refer to these two graphs as WN and WN++, respectively. In both settings, all \u03b1i are set to 1 and \u03b2ij to be degree(i)\u22121.\nFrameNet FrameNet [11, 27] is a rich linguistic resource containing information about lexical and predicate-argument semantics in English. Grounded in the theory of frame semantics, it suggests a semantic representation that blends word-sense disambiguation and semantic role labeling. Its expert-constructed taxonomy is organized around general-purpose frames (abstract predicates), each associated with a set of lemmas that can evoke the frame. For example, the frame Cause change of position on a scale is associated with push, raise, and growth (among many others). In our use of FrameNet, two words that group together with any frame are given an edge in E. We get a graph of 11,000 nodes and 210,000 edges. We refer to this graph as FN. All \u03b1i are set to 1 and \u03b2ij to be degree(i)\u22121."}, {"heading": "6 Evaluation Benchmarks", "text": "We evaluate the quality of our word vector representations on tasks that test how well they capture both semantic and syntactic aspects of the representations along with an extrinsic sentiment analysis task.\n5http://www.cis.upenn.edu/\u02dcccb/ppdb\nWord Similarity We evaluate our word representations on a variety of different benchmarks that have been widely used to measure word similarity. The first one is the WS-353 dataset [28] containing 353 pairs of English words that have been assigned similarity ratings by humans. The second benchmark is the RG-65 [29] dataset that contain 65 pairs of nouns. Since the commonly used word similarity datasets contain a small number of word pairs we also use the MEN dataset [30] that contains 3,000 word pairs which have been sampled from words that occur at least 700 times in a large web corpus.\nWe calculate cosine similarity between the vectors of two words forming a test item, and report Spearman\u2019s rank correlation coefficient [31] between the rankings produced by our model against the human rankings.\nSyntactic Relations (SYN-REL) Mikolov et. al [32] present a syntactic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common syntatic relation. For example, given walking and walked, the words are differently inflected forms of the same verb. There are nine such different kinds of relations: adjective-adverb, opposites, comaparative, superlative, present-participle, nation-nationality, past tense, plural nouns, and plural verbs. Overall there are 10,675 such syntactic pairs of word tuples.\nThe task is to find a word d that best fits the following relationship: \u201ca is to b as c is to d,\u201d given a, b, and c. We use the vector offset method [20], computing q = qa\u2212 qb + qc and returning the vector from Q which has the highest cosine similarity to q.\nSynonym Selection (TOEFL) The synonym selection task is to select the semantically closest word to a target from a list of candidates. The dataset we use on this task is the TOEFL dataset [33] which consists of a list of target words that appear with 4 candidate lexical substitutes each. The dataset contains 80 such questions. An example is \u201crug\u2192 {sofa, ottoman, carpet, hallway}\u201d, with carpet being the most synonym-like candidate to the target.\nSentiment Analysis (SA) Socher et. al, [34] created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We train an `2-regularized logistic regression classifier on the average of the word vectors of a given sentence to predict the coarse-grained sentiment tag at the sentence level, and report the test-set accuracy of the classifier."}, {"heading": "7 Experiments & Results", "text": "We now turn to an expiration of whether the static retrofitting approach is effective, or if incorporating lexical knowledge during training is more effective. Using a corpus of English news from WMT-2011,6 we trained word vectors of length 80 using the LSA, LBL, and SG methods. After normalization the corpus contained 360 million word tokens and 180,000 word types. As noted in \u00a74, we also considered pre-trained, publicly released vectors trained using GC and Multi. GC vectors are trained on 990 million tokens from Wikipedia and are 50-dimensional. Multi-lingual vectors are also trained on the WMT-2011 data and are 512-dimensional."}, {"heading": "7.1 Retrofitting", "text": "We use Equation 1 to retrofit each of the five sets of word vectors (\u00a74) using each of the five graphs derived from semantic lexicons (\u00a75).\nResults Figure 2 shows the absolute changes in performance (Spearman\u2019s correlation ratio or accuracy, as appropriate) on different tasks (as columns) with different semantic lexicons (as rows). Colored bars correspond to the five different sets of input vectors. All of the lexicons offer improvements on the word similarity tasks (the first three columns). FrameNet\u2019s performance is weaker, in some cases leading to worse performance (e.g., with SG and Multi vectors). On the TOEFL task,\n6http://www.statmt.org/wmt11\nwe get huge improvements of the order of 10 absolute points in accuracy for all ontologies except for FrameNet.\nFor the extrinsic sentiment analysis task, we get improvements using all the ontologies and achieve the highest improvement of absolute 1.4 points in accuracy for the multilingual vectors over the baseline. This increase is statistically significant (p < 0.01) according to a McNemar\u2019s test.\nThe weighted variant of PPDB offers no benefit over the simpler PPDB graph. We believe that FrameNet does not perform as well as the other lexicons is that its frames group words based on abstract concepts; often words with seemingly distantly related meanings (e.g., push and growth) can evoke the same frame. In summary, we find overall that PPDB gives the best graph among those tested (across tasks and vectors), LBL vectors tend to improve more than others and the TOEFL task tends to receive the greatest benefit from retrofitting.\nLexicon Ensemble We also considered an ensemble, in which the graph is the union of the WordNet and PPDB lexicons. The result is shown in the bottom row of Figure 2. On average, it performs slightly worse than the best component."}, {"heading": "7.2 Lexicon Information during Training", "text": "We next consider the approach to training word vectors with the lexical knowledge incorporated as a prior (\u00a73). Here we consider length-80 LBL vectors\u2014for which we have our own training implementation to modify, and which performed reasonably well in the retrofitting experiments (\u00a77.1)\u2014and the PPDB graph. We consider the lazy and periodic algorithms described in \u00a73. For the lazy method we update the prior every k = 100,000 words7 and test for different values of prior\n7Experiments with k \u2208 [10000, 50000] yielded almost similar results.\nstrength \u03b3 \u2208 {1, 0.1, 0.01}. For the periodic method, we update the word vectors using Eq. 1 every k \u2208 {25, 50, 100} million words.\nResults See Table 1. For lazy, \u03b3 = 0.01 performs best, but the method is in most cases not highly sensitive to \u03b3\u2019s value. For periodic, which overall leads to greater improvements over the baseline than lazy, k = 50M performs best, although all other values of k also outperform the the baseline. The last row shows the results obtained using retrofitting for comparison and it can be seen that retrofitting performs either better or competitively to using semantic knowledge during training\u2014 this is an important result since retrofitting is more flexible as it is applicable to any word vector model."}, {"heading": "7.3 Comparison to Prior Work", "text": "The only publicly available system that incorporates semantic information while training word vectors has been released by Yu & Dredze [35]8. Their system incorporates semantic relation between two words while training as an extra penalty term similar to our semantic lexicons during learning model (\u00a73). Their model works only with the word2vec tool, thus we train embeddings using their joint model and compare against the retrofitting model while using exactly the same training settings specified in [35].9 Table 2 shows the results, although Yu & Dredze sometimes improve over the baseline, we consistently outperform them on all the tasks."}, {"heading": "7.4 Multilingual Evaluation", "text": "We tested our method on three additional languages: German, French, and Spanish. We used the Universal WordNet [36], an automatically constructed multilingual lexical knowledge base based\n8https://github.com/Gorov/JointRCM 9These training settings are different from our standard setup and the details are omitted here\non WordNet.10 It contains words connected via different lexical relations to other words both in and across languages. We construct separate graphs for different languages (i.e., only linking words to other words in the same language) and apply retrofitting to each.\nFor German, French and Spanish we constructed graphs containing around 87K, 49K, and 31K word-vertices (respectively), with 200K, 100K, and 52K edges (respectively). Since not many word similarity evaluation benchmarks are available for other languages we tested our baseline and improved vectors on one benchmark per language. We used RG-65 [37], WS-353 [38] and MC-30 [39] for German, French and Spanish respectively. These benchmarks were created by translating the corresponding English benchmarks word by word manually. We trained SG vectors for each language of length 80. We again used the WMT-2011 monolingual news corpus for each language to train its word vectors and evaluate word similarity on these tasks before and after retrofitting. Table 3 shows the results that strongly indicate that our method generalizes across languages. We get high improvements in the Spearman\u2019s correlation coefficient on the word similarity tasks for the three languages for three different kinds of vectors."}, {"heading": "8 Conclusion", "text": "We have proposed a simple, effective and fast method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons constructed either automatically or by humans. Retrofitting is used as a post-processing step to improve vector quality and is simpler to use than other approaches that use semantic information while training. It can be used for improving vectors obtained from any word vector training model and performs better than current state-of-theart approaches to semantic enrichment of word vectors. We validated the applicability of our method across tasks, semantic lexicons, and languages."}], "references": [{"title": "Similarity of semantic relations", "author": ["Peter D. Turney"], "venue": "Comput. Linguist.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In Proceedings of NAACL,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proc. of ACL,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Indexing by latent semantic analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Karl Moritz Hermann", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.4641,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of EACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Dependency-based construction of semantic space models", "author": ["Sebastian Pad\u00f3", "Mirella Lapata"], "venue": "Comput. Linguist.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "The berkeley framenet project", "author": ["Collin F. Baker", "Charles J. Fillmore", "John B. Lowe"], "venue": "ACL \u201998,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proceedings of NAACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Label propagation and quadratic criterion", "author": ["Yoshua Bengio", "Olivier Delalleau", "Nicolas Le Roux"], "venue": "In Semi-Supervised Learning", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Efficient graph-based semi-supervised learning of structured tagging models", "author": ["Amarnag Subramanya", "Slav Petrov", "Fernando Pereira"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "author": ["Dipanjan Das", "Slav Petrov"], "venue": "In Proc. of ACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Semi-supervised frame-semantic parsing for unknown predicates", "author": ["Dipanjan Das", "Noah A. Smith"], "venue": "In Proc. of ACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh"], "venue": "In Proceedings of ICML,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Technical Report UCB/EECS-2010-24,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Lazy sparse stochastic gradient descent for regularized multinomial logistic regression", "author": ["Bob Carpenter"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Word association norms, mutual information, and lexicography", "author": ["Kenneth Ward Church", "Patrick Hanks"], "venue": "Comput. Linguist.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "Mining the web for synonyms: Pmi-ir versus lsa on toefl", "author": ["Peter D. Turney"], "venue": "In ECML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Matrix computations (3rd ed.)", "author": ["Gene H. Golub", "Charles F. Van Loan"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1996}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of ACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Placing search in context: the concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1965}, {"title": "Distributional semantics in technicolor", "author": ["Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran"], "venue": "In Proceedings of ACL,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of NAACL,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T. Dumais"], "venue": "Psychological review,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze"], "venue": "In ACL,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Towards a universal wordnet by learning from combined evidence", "author": ["Gerard de Melo", "Gerhard Weikum"], "venue": "In Proceedings of CIKM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Using the structure of a conceptual network in computing semantic relatedness", "author": ["Iryna Gurevych"], "venue": "In Proceedings of IJCNLP,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "Comparison of semantic similarity for different languages using the google n-gram corpus and second- order co-occurrence measures", "author": ["Colette Joubarne", "Diana Inkpen"], "venue": "In Proceedings of CAAI,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Cross-lingual semantic relatedness using encyclopedic knowledge", "author": ["Samer Hassan", "Rada Mihalcea"], "venue": "In Proc. of EMNLP,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "These word vectors can in turn be used for identifying semantically related word pairs [1, 2] or as features in downstream text processing applications [3].", "startOffset": 87, "endOffset": 93}, {"referenceID": 1, "context": "These word vectors can in turn be used for identifying semantically related word pairs [1, 2] or as features in downstream text processing applications [3].", "startOffset": 87, "endOffset": 93}, {"referenceID": 2, "context": "These word vectors can in turn be used for identifying semantically related word pairs [1, 2] or as features in downstream text processing applications [3].", "startOffset": 152, "endOffset": 155}, {"referenceID": 3, "context": "A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics [4] and using internal representations from neural network models of word sequences [5].", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics [4] and using internal representations from neural network models of word sequences [5].", "startOffset": 253, "endOffset": 256}, {"referenceID": 5, "context": "For example, cooccurrence statistics have been expanded to incorporate multilingual context [6, 7, 8] or define context in terms of dependency links [9].", "startOffset": 92, "endOffset": 101}, {"referenceID": 6, "context": "For example, cooccurrence statistics have been expanded to incorporate multilingual context [6, 7, 8] or define context in terms of dependency links [9].", "startOffset": 92, "endOffset": 101}, {"referenceID": 7, "context": "For example, cooccurrence statistics have been expanded to incorporate multilingual context [6, 7, 8] or define context in terms of dependency links [9].", "startOffset": 92, "endOffset": 101}, {"referenceID": 8, "context": "For example, cooccurrence statistics have been expanded to incorporate multilingual context [6, 7, 8] or define context in terms of dependency links [9].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "Examples of such resources include WordNet [10], FrameNet [11] and the Paraphrase database [12].", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "Examples of such resources include WordNet [10], FrameNet [11] and the Paraphrase database [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "Examples of such resources include WordNet [10], FrameNet [11] and the Paraphrase database [12].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "To do so, we use an efficient iterative updating method [14, 15, 16, 17].", "startOffset": 56, "endOffset": 72}, {"referenceID": 13, "context": "To do so, we use an efficient iterative updating method [14, 15, 16, 17].", "startOffset": 56, "endOffset": 72}, {"referenceID": 14, "context": "To do so, we use an efficient iterative updating method [14, 15, 16, 17].", "startOffset": 56, "endOffset": 72}, {"referenceID": 15, "context": "To do so, we use an efficient iterative updating method [14, 15, 16, 17].", "startOffset": 56, "endOffset": 72}, {"referenceID": 19, "context": "In the first, we apply adaptive gradient descent (AdaGrad [21]), using the sum of gradients of the log-likelihood (given by the extant vector learning model) and the logprior (from Eq.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "2 is linear in the vocabulary size n, we use lazy updates [22] every k words during training.", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "Latent Semantic Analysis (LSA) We perform latent semantic analysis [4] on a word-word cooccurrence matrix.", "startOffset": 67, "endOffset": 70}, {"referenceID": 21, "context": "We then replace every entry in the sparse frequency matrix by its pointwise mutual information (PMI) [23, 24] resulting in X.", "startOffset": 101, "endOffset": 109}, {"referenceID": 22, "context": "We then replace every entry in the sparse frequency matrix by its pointwise mutual information (PMI) [23, 24] resulting in X.", "startOffset": 101, "endOffset": 109}, {"referenceID": 23, "context": "We factorize the matrix X = U\u03a3V> using singular value decomposition (SVD) [25].", "startOffset": 74, "endOffset": 78}, {"referenceID": 18, "context": "Skip-Gram Vectors (SG) The word2vec tool [20] is fast and currently in wide use.", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "Global Context Vectors (GC) These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features [26].", "startOffset": 161, "endOffset": 165}, {"referenceID": 7, "context": "Multilingual Vectors (Multi) Faruqui and Dyer [8] learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora.", "startOffset": 46, "endOffset": 49}, {"referenceID": 17, "context": "Log-bilinear Vectors (LBL) The log bilinear language model [19] predicts a word token w\u2019s vector given the set of words in its context (h), also represented as vectors:", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "Since it is costly to renormalize over the whole vocabulary, we use noise constrastive estimation (NCE) to estimate the parameters of the model [19] using AdaGrad [21] with a learning rate of 0.", "startOffset": 144, "endOffset": 148}, {"referenceID": 19, "context": "Since it is costly to renormalize over the whole vocabulary, we use noise constrastive estimation (NCE) to estimate the parameters of the model [19] using AdaGrad [21] with a learning rate of 0.", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "WordNet WordNet [10] is a large human-constructed semantic lexicon of English words.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "FrameNet FrameNet [11, 27] is a rich linguistic resource containing information about lexical and predicate-argument semantics in English.", "startOffset": 18, "endOffset": 26}, {"referenceID": 25, "context": "The first one is the WS-353 dataset [28] containing 353 pairs of English words that have been assigned similarity ratings by humans.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "The second benchmark is the RG-65 [29] dataset that contain 65 pairs of nouns.", "startOffset": 34, "endOffset": 38}, {"referenceID": 27, "context": "Since the commonly used word similarity datasets contain a small number of word pairs we also use the MEN dataset [30] that contains 3,000 word pairs which have been sampled from words that occur at least 700 times in a large web corpus.", "startOffset": 114, "endOffset": 118}, {"referenceID": 28, "context": "al [32] present a syntactic relation dataset composed of analogous word pairs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "We use the vector offset method [20], computing q = qa\u2212 qb + qc and returning the vector from Q which has the highest cosine similarity to q.", "startOffset": 32, "endOffset": 36}, {"referenceID": 29, "context": "The dataset we use on this task is the TOEFL dataset [33] which consists of a list of target words that appear with 4 candidate lexical substitutes each.", "startOffset": 53, "endOffset": 57}, {"referenceID": 30, "context": "al, [34] created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts.", "startOffset": 4, "endOffset": 8}, {"referenceID": 31, "context": "Table 2: Semantic enrichment of word2vec CBOW vectors using Yu & Dredze (2014) [35] and retrofitting model using PPDB.", "startOffset": 79, "endOffset": 83}, {"referenceID": 31, "context": "The only publicly available system that incorporates semantic information while training word vectors has been released by Yu & Dredze [35]8.", "startOffset": 135, "endOffset": 139}, {"referenceID": 31, "context": "Their model works only with the word2vec tool, thus we train embeddings using their joint model and compare against the retrofitting model while using exactly the same training settings specified in [35].", "startOffset": 199, "endOffset": 203}, {"referenceID": 32, "context": "We used the Universal WordNet [36], an automatically constructed multilingual lexical knowledge base based", "startOffset": 30, "endOffset": 34}, {"referenceID": 33, "context": "We used RG-65 [37], WS-353 [38] and MC-30 [39] for German, French and Spanish respectively.", "startOffset": 14, "endOffset": 18}, {"referenceID": 34, "context": "We used RG-65 [37], WS-353 [38] and MC-30 [39] for German, French and Spanish respectively.", "startOffset": 27, "endOffset": 31}, {"referenceID": 35, "context": "We used RG-65 [37], WS-353 [38] and MC-30 [39] for German, French and Spanish respectively.", "startOffset": 42, "endOffset": 46}], "year": 2017, "abstractText": "Vector space word representations are typically learned using only co-occurrence statistics from text corpora. Although such statistics are informative, they disregard easily accessible (and often carefully curated) information archived in semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This paper proposes a technique to leverage both distributional and lexicon-derived evidence to obtain better representations. We run belief propagation on a word type graph constructed from word similarity information from lexicons to encourage connected (related) words to have similar representations, and also to be close to the unsupervised vectors. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, using several different underlying word vector models, we obtain substantially improved vectors and consistently outperform existing approaches of incorporating semantic knowledge in word vectors.", "creator": "LaTeX with hyperref package"}}}