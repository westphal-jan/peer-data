{"id": "1411.4510", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2014", "title": "Parallel Gaussian Process Regression for Big Data: Low-Rank Representation Meets Markov Approximation", "abstract": "the expressive power of combining a gaussian process ( gp ) model initially comes at a cost of poor scalability in the data ring size. working to improve its scalability, constructing this paper potentially presents a compact low - ranked rank - partial cum - markov approximation ( lma ) of the gp model that is novel in leveraging the dual computational computational advantages stemming from complementing a low - rank approximate homogeneous representation of the full - rank gp based on acquiring a support set of inputs interspersed with a markov approximation of the resulting residual process ; the latter approximation is guaranteed to be closest in fitting the specific kullback - leibler distance criterion subject to some design constraint and is considerably more refined than that of past existing sparse gp models utilizing low - rank representations due to its more relaxed conditional independence assumption ( especially with larger data ). as a result, our lma method can trade off between the size of the maximal support set nodes and the order of the markov property to ( a ) likewise incur effectively lower computational system cost than such sparse gp models while achieving predictive performance comparable to linear them and ( b ) accurately represent features / constraint patterns of any arithmetic scale. interestingly, varying the markov order produces a spectrum of lmas with pic matrix approximation and full - rank gp at nearest the two extremes. an advantage of our lma method is that it is amenable to parallelization on multiple machines / cores, thereby gaining entirely greater scalability. empirical evaluation on three real - world datasets in clusters of up to 32 computing nodes shows that our centralized and parallel lma methods are significantly more often time - efficient and scalable than parallel state - of - the - art sparse and full - gradient rank gp regression methods while achieving ideally comparable predictive performances.", "histories": [["v1", "Mon, 17 Nov 2014 15:31:04 GMT  (777kb,D)", "http://arxiv.org/abs/1411.4510v1", "29th AAAI Conference on Artificial Intelligence (AAAI 2015), Extended version with proofs, 10 pages"]], "COMMENTS": "29th AAAI Conference on Artificial Intelligence (AAAI 2015), Extended version with proofs, 10 pages", "reviews": [], "SUBJECTS": "stat.ML cs.DC cs.LG", "authors": ["kian hsiang low", "jiangbo yu", "jie chen", "patrick jaillet"], "accepted": true, "id": "1411.4510"}, "pdf": {"name": "1411.4510.pdf", "metadata": {"source": "META", "title": "Parallel Gaussian Process Regression for Big Data: Low-Rank Representation Meets Markov Approximation", "authors": ["Kian Hsiang Low", "Jiangbo Yu", "Jie Chen", "Patrick Jaillet"], "emails": ["yujiang}@comp.nus.edu.sg,", "chenjie@smart.mit.edu,", "jaillet@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Gaussian process (GP) models are a rich class of Bayesian non-parametric models that can perform probabilistic regression by providing Gaussian predictive distributions with formal measures of the predictive uncertainty. Unfortunately, a GP model is handicapped by its poor scalability in the size of the data, hence limiting its practical use to small data. To improve its scalability, two families of sparse GP regression methods have been proposed: (a) Low-rank approximate representations (Hensman, Fusi, and Lawrence 2013; \u2217Kian Hsiang Low and Jiangbo Yu are co-first authors. Copyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nLa\u0301zaro-Gredilla et al. 2010; Quin\u0303onero-Candela and Rasmussen 2005; Snelson and Ghahramani 2005) of the fullrank GP (FGP) model are well-suited for modeling slowlyvarying functions with large correlation and can use all the data for predictions. But, they require a relatively high rank to capture small-scale features/patterns (i.e., of small correlation) with high fidelity, thus losing their computational advantage. (b) In contrast, localized regression and covariance tapering methods (e.g., local GPs (Park, Huang, and Ding 2011) and compactly supported covariance functions (Furrer, Genton, and Nychka 2006)) are particularly useful for modeling rapidly-varying functions with small correlation. However, they can only utilize local data for predictions, thereby performing poorly in input regions with little/no data. Furthermore, to accurately represent large-scale features/patterns (i.e., of large correlation), the locality/tapering range has to be increased considerably, thus sacrificing their time efficiency.\nRecent sparse GP regression methods (Chen et al. 2013; Snelson and Ghahramani 2007) have unified approaches from the two families described above to harness their complementary modeling and predictive capabilities (hence, eliminating their deficiencies) while retaining their computational advantages. Specifically, after approximating the FGP (in particular, its covariance matrix) with a low-rank representation based on the notion of a support set of inputs, a sparse covariance matrix approximation of the resulting residual process is made. However, this sparse residual covariance matrix approximation imposes a fairly strong conditional independence assumption given the support set since the support set cannot be too large to preserve time efficiency (see Remark 2 after Proposition 1 in Section 3). In this paper, we argue that such a strong assumption is an overkill: It is in fact possible to construct a more refined, dense residual covariance matrix approximation by exploiting a Markov assumption and, perhaps surprisingly, still achieve scalability, which distinguishes our work here from existing sparse GP regression methods utilizing lowrank representations (i.e., including the unified approaches) described earlier. As a result, our proposed residual covariance matrix approximation can significantly relax the conditional independence assumption (especially with larger data; see Remark 1 after Proposition 1 in Section 3), hence potentially improving the predictive performance. ar X iv :1 41 1. 45 10\nv1 [\nst at\n.M L\n] 1\n7 N\nov 2\n01 4\nThis paper presents a low-rank-cum-Markov approximation (LMA) of the FGP model (Section 3) that is novel in leveraging the dual computational advantages stemming from complementing the reduced-rank covariance matrix approximation based on the support set with the residual covariance matrix approximation due to the Markov assumption; the latter approximation is guaranteed to be closest in the Kullback-Leibler distance criterion subject to some constraint. Consequently, our proposed LMA method can trade off between the size of the support set and the order of the Markov property to (a) incur lower computational cost than sparse GP regression methods utilizing low-rank representations with only the support set size (e.g., (Chen et al. 2013; Snelson and Ghahramani 2007)) or number of spectral points (La\u0301zaro-Gredilla et al. 2010) as the varying parameter while achieving predictive performance comparable to them and (b) accurately represent features/patterns of any scale. Interestingly, varying the Markov order produces a spectrum of LMAs with the partially independent conditional (PIC) approximation (Chen et al. 2013; Snelson and Ghahramani 2007) and FGP at the two extremes. An important advantage of LMA over most existing sparse GP regression methods is that it is amenable to parallelization on multiple machines/cores, thus gaining greater scalability for performing real-time predictions necessary in many time-critical applications and decision support systems (e.g., ocean sensing (Cao, Low, and Dolan 2013; Dolan et al. 2009; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2012; Podnar et al. 2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)). Our parallel LMA method is implemented using the message passing interface (MPI) framework to run in clusters of up to 32 computing nodes and its predictive performance, scalability, and speedup are empirically evaluated on three real-world datasets (Section 4)."}, {"heading": "2 Full-Rank Gaussian Process Regression", "text": "Let X be a set representing the input domain such that each input x \u2208 X denotes a d-dimensional feature vector and is associated with a realized output value yx (random output variable Yx) if it is observed (unobserved). Let {Yx}x\u2208X denote a GP, that is, every finite subset of {Yx}x\u2208X follows a multivariate Gaussian distribution. Then, the GP is fully specified by its prior mean \u00b5x , E[Yx] and covariance \u03c3xx\u2032 , cov[Yx, Yx\u2032 ] for all x, x\u2032 \u2208 X . Supposing a column vector yD of realized outputs is observed for some set D \u2282 X of inputs, a full-rank GP (FGP) model can perform probabilistic regression by providing a Gaussian posterior/predictive distribution\nN (\u00b5U + \u03a3UD\u03a3\u22121DD(yD \u2212 \u00b5D),\u03a3UU \u2212 \u03a3UD\u03a3\u22121DD\u03a3DU ) of the unobserved outputs for any set U \u2286 X \\ D of inputs where \u00b5U (\u00b5D) is a column vector with mean components \u00b5x for all x \u2208 U (x \u2208 D), \u03a3UD (\u03a3DD) is a covariance matrix with covariance components \u03c3xx\u2032 for all x \u2208 U , x\u2032 \u2208 D\n(x, x\u2032 \u2208 D), and \u03a3DU = \u03a3>UD. The chief limitation hindering the practical use of the FGP regression method is its poor scalability in the data size |D|: Computing the Gaussian posterior/predictive distribution requires inverting \u03a3DD, which incurs O(|D|3) time. In the next section, we will introduce our proposed LMA method to improve its scalability."}, {"heading": "3 Low-Rank-cum-Markov Approximation", "text": "Y\u0302x , \u03a3xS\u03a3 \u22121 SSYS is a reduced-rank approximate representation of Yx based on a support set S \u2282 X of inputs and its finite-rank covariance function is cov[Y\u0302x, Y\u0302x\u2032 ] = \u03a3xS\u03a3 \u22121 SS\u03a3Sx\u2032 for all x, x\n\u2032 \u2208 X . Then, Y\u0303x = Yx \u2212 Y\u0302x is the residual of the reduced-rank approximation and its covariance function is thus cov[Y\u0303x, Y\u0303x\u2032 ] = \u03c3xx\u2032 \u2212 \u03a3xS\u03a3\u22121SS\u03a3Sx\u2032 . Define\nQBB\u2032 , \u03a3BS\u03a3 \u22121 SS\u03a3SB\u2032 and RBB\u2032 , \u03a3BB\u2032 \u2212QBB\u2032\nfor all B,B\u2032 \u2282 X . Then, a covariance matrix \u03a3VV for the set V , D\u222aU \u2282 X of inputs (i.e., associated with realized outputs yD and unobserved random outputs YU ) can be decomposed into a reduced-rank covariance matrix approximation QVV and the resulting residual covariance matrix RVV , that is, \u03a3VV = QVV +RVV . As discussed in Section 1, existing sparse GP regression methods utilizing low-rank representations (i.e., including unified approaches) approximate RVV with a sparse matrix. In contrast, we will construct a more refined, dense residual covariance matrix approximation by exploiting a Markov assumption to be described next.\nLet the set D (U) of inputs be partitioned1 evenly into M disjoint subsets D1, . . . ,DM (U1, . . . ,UM ) such that the outputs yDm and YUm are as highly correlated as possible for m = 1, . . . ,M . Let Vm , Dm\u222aUm. Then, V = \u22c3M m=1 Vm.\nThe key idea of our low-rank-cum-Markov approximation (LMA) method is to approximate the residual covariance matrix RVV by a block matrix RVV partitioned into M \u00d7M square blocks, that is, RVV , [RVmVn ]m,n=1,...,M where\nRVmVn ,  RVmVn if |m\u2212 n| \u2264 B, RVmDBmR \u22121 DBmDBm RDBmVn if n\u2212m > B > 0, RVmDBnR \u22121 DBnDBn\nRDBn Vn if m\u2212 n > B > 0, 0 if |m\u2212 n| > B = 0;\n(1) such that B \u2208 {0, . . . ,M \u2212 1} denotes the order of the Markov property imposed on the residual process {Y\u0303x}x\u2208D to be detailed later, DBm , \u22c3min(m+B,M) k=m+1 Dk, and 0 denotes a square block comprising components of value 0. To understand the intuition underlying the approximation in (1), Fig. 1a illustrates a simple case of RVV with B = 1 and M = 4 for ease of exposition: It can be observed that only the blocks RVmVn outside the B-block band of RVV (i.e., |m \u2212 n| > B) are approximated, specifically, by unshaded blocks RVmVn being defined as a recursive series of |m \u2212 n| \u2212 B reduced-rank residual covariance matrix approximations (1). So, when an unshaded block RVmVn is\n1D and U are partitioned according to a simple parallelized clustering scheme employed in the work of Chen et al. (2013).\noutside B-block band of R \u22121 DD (i.e., |m\u2212 n| > B) are 0.\nfurther from the diagonal of RVV (i.e., larger |m\u2212 n|), it is derived using more reduced-rank residual covariance matrix approximations. For example, RV1V4 is approximated by an unshaded block RV1V4 being defined as a recursive series of 2 reduced-rank residual covariance matrix approximations, namely, approximating RV1V4 by RV1D11R \u22121 D11D11 RD11V4 = RV1D2R \u22121 D2D2RD2V4 based on the support set D11 = D2 of inputs and in turn approximating RD2V4 by a submatrix RD2V4 = RD2D3R \u22121 D3D3RD3V4 (1) of unshaded block RV2V4 based on the support set D12 = D3 of inputs. As a result, RV1V4 = RV1D2R \u22121 D2D2RD2D3R \u22121 D3D3RD3V4 is fully specified by five submatrices of the respective shaded blocks RV1V2 , RV2V2 , RV2V3 , RV3V3 , and RV3V4 within the Bblock band of RVV (i.e., |m\u2212 n| \u2264 B). In general, any unshaded blockRVmVn outside theB-block band ofRVV (i.e., |m\u2212n| > B) is fully specified by submatrices of the shaded blocks within the B-block band of RVV (i.e., |m\u2212 n| \u2264 B) due to its recursive series of |m\u2212n|\u2212B reduced-rank residual covariance matrix approximations (1). Though it may not be obvious now how such an approximation would entail scalability, (1) interestingly offers an alternative interpretation of imposing a B-th order Markov property on residual process {Y\u0303x}x\u2208D, which reveals a further insight on the structural assumption of LMA to be exploited for achieving scalability, as detailed later.\nThe covariance matrix \u03a3VV is thus approximated by a block matrix \u03a3VV , QVV + RVV partitioned into M \u00d7M square blocks, that is, \u03a3VV , [\u03a3VmVn ]m,n=1,...,M where\n\u03a3VmVn , QVmVn +RVmVn . (2)\nSo, within the B-block band of \u03a3VV (i.e., |m \u2212 n| \u2264 B), \u03a3VmVn = \u03a3VmVn , by (1) and (2). Note that when B = 0, \u03a3VmVn = QVmVn for |m \u2212 n| > B, thus yielding the prior covariance matrix \u03a3VV of the partially independent conditional (PIC) approximation (Chen et al. 2013; Snelson and Ghahramani 2007). When B = M \u2212 1, \u03a3VV = \u03a3VV is the prior covariance matrix of FGP model. So, LMA generalizes PIC (i.e., if B = 0) and becomes FGP if B = M \u2212 1. Varying Markov order B from 0 to M \u2212 1 produces a spectrum of LMAs with PIC and FGP at the two extremes.\nBy approximating \u03a3VV with \u03a3VV , our LMA method utilizes the data (D, yD) to predict the unobserved outputs for any set U \u2286 X \\D of inputs and provide their corresponding predictive uncertainties using the following predictive mean vector and covariance matrix, respectively:\n\u00b5LMAU , \u00b5U + \u03a3UD\u03a3 \u22121 DD (yD \u2212 \u00b5D) (3)\n\u03a3LMAUU , \u03a3UU \u2212 \u03a3UD\u03a3 \u22121 DD\u03a3DU (4)\nwhere \u03a3UU , \u03a3UD, and \u03a3DD are obtained using (2), and \u03a3DU = \u03a3 > UD. If \u03a3DD in (3) and (4) is inverted directly, then it would still incur the same O(|D|3) time as inverting \u03a3DD in the FGP regression method (Section 2). In the rest of this section, we will show how this scalability issue can be resolved by leveraging the computational advantages associated with both the reduced-rank covariance matrix approximation QDD based on the support set S and our proposed residual covariance matrix approximation RDD due to B-th order Markov assumption after decomposing \u03a3DD.\nIt can be observed from RVV (1) that RDD is approximated by a block matrix RDD = [RDmDn ]m,n=1,...,M where RDmDn is a submatrix of RVmVn obtained using (1). Proposition 1 Block matrix R\u22121DD is B-block-banded, that is, any block outside its B-block band is 0 (e.g., Fig. 1b).\nIts proof follows directly from a block-banded matrix result of Asif and Moura (2005) (specifically, Theorem 3). Remark 1. In the same spirit as a Gaussian Markov random process, imposing aB-th order Markov property on residual process {Y\u0303x}x\u2208D is equivalent to approximating RDD by RDD whose inverse is B-block-banded (Fig. 1b). That is, if |m \u2212 n| > B, YDm and YDn are conditionally independent given YS\u222aD\\(Dm\u222aDn). Such a conditional independence assumption thus becomes more relaxed with larger data. More importantly, this B-th order Markov assumption or, equivalently, sparsity ofB-block-bandedR \u22121 DD is the key to achieving scalability, as shown in the proof of Theorem 2 later. Remark 2. Though R \u22121 DD is sparse, RDD is a dense residual covariance matrix approximation if B > 0. In contrast, the sparse GP regression methods utilizing low-rank representations (i.e., including unified approaches) utilize a sparse residual covariance matrix approximation (Section 1), hence imposing a significantly stronger conditional independence assumption than LMA. For example, PIC (Chen et al. 2013; Snelson and Ghahramani 2007) assumes YDm and YDn to be conditionally independent given only YS if |m\u2212 n| > 0.\nThe next result reveals that, among all |D| \u00d7 |D| matrices whose inverse is B-block-banded, RDD approximates RDD most closely in the Kullback-Leibler (KL) distance criterion, that is, RDD has the minimum KL distance from RDD:\nTheorem 1 Let KL distanceDKL(R, R\u0302) , 0.5(tr(RR\u0302\u22121)\u2212 log |RR\u0302\u22121| \u2212 |D|) between two |D| \u00d7 |D| positive definite matrices R and R\u0302 measure the error of approximating R with R\u0302. Then, for any matrix R\u0302 whose inverse is B-blockbanded, DKL(RDD, R\u0302) \u2265 DKL(RDD, RDD). Its proof is in Appendix A. Our main result in Theorem 2 be-\nlow exploits the sparsity ofR \u22121 DD (Proposition 1) for deriving an efficient formulation of LMA, which is amenable to parallelization on multiple machines/cores by constructing and communicating the following summary information: Definition 1 (Local Summary) Them-th local summary is defined as a tuple (y\u0307m, R\u0307m, \u03a3\u0307mS , \u03a3\u0307 m U ) where\ny\u0307m , yDm \u2212 \u00b5Dm \u2212R\u2032DmDBm(yDBm \u2212 \u00b5DBm) R\u0307m , (RDmDm \u2212R\u2032DmDBmRDBmDm) \u22121 \u03a3\u0307mS , \u03a3DmS \u2212R\u2032DmDBm\u03a3DBmS \u03a3\u0307mU , \u03a3DmU \u2212R\u2032DmDBm\u03a3DBmU\nsuch that R\u2032DmDBm , RDmDBmR \u22121 DBmDBm .\nDefinition 2 (Global Summary) The global summary is defined as a tuple (y\u0308S , y\u0308U , \u03a3\u0308SS , \u03a3\u0308US , \u03a3\u0308UU ) where\ny\u0308S , M\u2211\nm=1\n(\u03a3\u0307mS ) >R\u0307my\u0307m , y\u0308U , M\u2211 m=1 (\u03a3\u0307mU ) >R\u0307my\u0307m\n\u03a3\u0308SS , \u03a3SS + M\u2211\nm=1\n(\u03a3\u0307mS ) >R\u0307m\u03a3\u0307 m S\n\u03a3\u0308US , M\u2211\nm=1\n(\u03a3\u0307mU ) >R\u0307m\u03a3\u0307 m S , \u03a3\u0308UU , M\u2211 m=1 (\u03a3\u0307mU ) >R\u0307m\u03a3\u0307 m U .\nTheorem 2 For B > 0, \u00b5LMAU (3) and \u03a3LMAUU (4) can be reduced to \u00b5LMAU = \u00b5U + y\u0308U \u2212 \u03a3\u0308US\u03a3\u0308\u22121SS y\u0308S and \u03a3LMAUU = \u03a3UU \u2212 \u03a3\u0308UU + \u03a3\u0308US\u03a3\u0308\u22121SS\u03a3\u0308>US . Its proof in Appendix B essentially relies on the sparsity of R \u22121 DD and the matrix inversion lemma. Remark 1. To parallelize LMA, each machine/core m constructs and uses the m-th local summary to compute the m-th summation terms in the global summary, which are then communicated to a master node. The master node constructs and communicates the global summary to the M machines/cores, specifically, by sending the tuple (y\u0308S , y\u0308Um , \u03a3\u0308SS , \u03a3\u0308UmS , \u03a3\u0308UmUm) to each machine/core m. Finally, each machine/corem uses this received tuple to predict the unobserved outputs for the set Um of inputs and provide their corresponding predictive uncertainties using \u00b5LMAUm (3) and \u03a3 LMA UmUm (4), respectively. Computing \u03a3DmU and \u03a3DBmU terms in the local summary can also be parallelized due to their recursive definition (i.e., (1) and (2)), as discussed in Appendix C. This parallelization capability of LMA shows another key advantage over existing sparse GP regression methods2 in gaining scalability. Remark 2. Supposing M, |U|, |S| \u2264 |D|, LMA can compute \u00b5LMAU and tr(\u03a3 LMA UU ) distributedly in O(|S|3 + (B|D|/M)3 + |U|(|D|/M)(|S|+B|D|/M)) time using M parallel machines/cores and sequentially in O(|D||S|2 + B|D|(B|D|/M)2 + |U||D|(|S|+B|D|/M)) time on a single centralized machine. So, our LMA method incurs cubic\n2A notable exception is the work of Chen et al. (2013) that parallelizes PIC. As mentioned earlier, our LMA generalizes PIC.\ntime in support set size |S| and Markov order B. Increasing the number M of parallel machines/cores and blocks reduces the incurred time of our parallel and centralized LMA methods, respectively. Without considering communication latency, the speedup3 of our parallel LMA method grows with increasing M and training data size |D|; to explain the latter, unlike the additional O(|D||S|2) time of our centralized LMA method that increases with more data, parallel LMA does not have a correspondingO((|D|/M)|S|2) term. Remark 3. Predictive performance of LMA is improved by increasing the support set size |S| and/or Markov order B at the cost of greater time overhead. From Remark 2, since LMA incurs cubic time in |S| as well as in B, one should trade off between |S| and B to reduce the computational cost while achieving the desired predictive performance. In contrast, PIC (Chen et al. 2013; Snelson and Ghahramani 2007) (sparse spectrum GP (La\u0301zaro-Gredilla et al. 2010)) can only vary support set size (number of spectral points) to obtain the desired predictive performance. Remark 4. We have illustrated through a simple toy example in Appendix D that, unlike the local GPs approach, LMA does not exhibit any discontinuity in its predictions despite data partitioning."}, {"heading": "4 Experiments and Discussion", "text": "This section first empirically evaluates the predictive performance and scalability of our proposed centralized and parallel LMA methods against that of the state-of-the-art centralized PIC (Snelson and Ghahramani 2007), parallel PIC (Chen et al. 2013), sparse spectrum GP (SSGP) (La\u0301zaroGredilla et al. 2010), and FGP on two real-world datasets: (a) The SARCOS dataset (Vijayakumar, D\u2019Souza, and Schaal 2005) of size 48933 is obtained from an inverse dynamics problem for a 7 degrees-of-freedom SARCOS robot arm. Each input is specified by a 21D feature vector of joint positions, velocities, and accelerations. The output corresponds to one of the 7 joint torques. (b) The AIMPEAK dataset (Chen et al. 2013) of size 41850 comprises traffic speeds (km/h) along 775 road segments of an urban road network during morning peak hours on April 20, 2011. Each input (i.e., road segment) denotes a 5D feature vector of length, number of lanes, speed limit, direction, and time. The time dimension comprises 54 five-minute time slots. This traffic dataset is modeled using a relational GP (Chen et al. 2012) whose correlation structure can exploit the road segment features and road network topology information. The outputs correspond to the traffic speeds.\nBoth datasets are modeled using GPs whose prior covariance \u03c3xx\u2032 is defined by the squared exponential covariance function4 \u03c3xx\u2032 , \u03c32s exp(\u22120.5 \u2211d i=1(xi \u2212 x\u2032i)2/`2i ) + \u03c32n\u03b4xx\u2032 where xi (x \u2032 i) is the i-th component of input feature vector x (x\u2032), the hyperparameters \u03c32s , \u03c3 2 n, `1, . . . , `d\n3Speedup is the incurred time of a sequential/centralized algorithm divided by that of its parallel counterpart.\n4For AIMPEAK dataset, multi-dimensional scaling is used to map the input domain (i.e., of road segments) onto the Euclidean space (Chen et al. 2012) before applying the covariance function.\nare, respectively, signal variance, noise variance, and lengthscales, and \u03b4xx\u2032 is a Kronecker delta that is 1 if x = x\u2032 and 0 otherwise. The hyperparameters are learned using randomly selected data of size 10000 via maximum likelihood estimation. Test data of size |U| = 3000 are randomly selected from each dataset for predictions. From remaining data, training data of varying |D| are randomly selected. Support sets for LMA and PIC and the set S of spectral points for SSGP are selected randomly from both datasets5.\nThe experimental platform is a cluster of 32 computing nodes connected via gigabit links: Each node runs a Linux system with Intelr Xeonr E5620 at 2.4 GHz with 24 GB memory and 16 cores. Our parallel LMA method and parallel PIC are tested with different numbers M = 32, 48, and 64 of cores; all 32 computing nodes with 1, 1-2, and 2 cores each are used, respectively. For parallel LMA and parallel PIC, each computing node will be storing, respectively, a subset of the training data (Dm \u222a DBm, yDm\u222aDBm) and (Dm, yDm) associated with its own core m.\nThree performance metrics are used to evaluate the tested methods: (a) Root mean square error (RMSE) (|U|\u22121\u2211x\u2208U (yx \u2212 \u00b5x|D)2)1/2, (b) incurred time, and (c) speedup. For RMSE metric, each tested method has to plug its predictive mean into \u00b5x|D.\nTable 1 shows results of RMSEs and incurred times of parallel LMA, parallel PIC, SSGP, and FGP averaged over 5 random instances with varying data sizes |D| and cores M for both datasets. The observations are as follows: (a) Predictive performances of all tested methods improve with more data, which is expected. For SARCOS dataset, parallel LMA, parallel PIC, and SSGP achieve predictive\n5Varying the set S of spectral points over 50 random instances hardly changes the predictive performance of SSGP in our experiments because a very large set of spectral points (|S| = 4096) is used in order to achieve predictive performance as close as possible to FGP and our LMA method (see Table 1).\nperformances comparable to that of FGP. For AIMPEAK dataset, parallel LMA does likewise and outperforms parallel PIC and SSGP with more data (|D| \u2265 24000), which may be due to its more relaxed conditional independence assumption with larger data (Remark 1 after Proposition 1). (b) The incurred times of all tested methods increase with more data, which is also expected. FGP scales very poorly with larger data such that it incurs > 4 hours for |D| = 32000. In contrast, parallel LMA incurs only 1-5 minutes for both datasets when |D| = 32000. Parallel LMA incurs much less time than parallel PIC and SSGP while achieving a comparable or better predictive performance because it requires a significantly smaller |S| than parallel PIC and SSGP simply by imposing a 1-order Markov property (B = 1) on the residual process (Remark 3 after Theorem 2). Though B is only set to 1, the dense residual covariance matrix approximation provided by LMA (as opposed to sparse approximation of PIC) is good enough to achieve its predictive performances reported in Table 1. From Table 1b, when training data is small (|D| = 8000) for AIMPEAK dataset, parallel PIC incurs more time than FGP due to its huge |S| = 5120, which causes communication latency to dominate the incurred time (Chen et al. 2013). When |D| \u2264 24000, SSGP also incurs more time than FGP due to its large |S| = 4096. (c) Predictive performances of parallel LMA and PIC generally remain stable with more cores, thus justifying the practicality of their structural assumptions to gain time efficiency.\nTable 2 shows results of speedups of parallel LMA and parallel PIC as well as incurred times of their centralized counterparts averaged over 5 random instances with varying data sizes |D| and numbers M of cores for AIMPEAK dataset. The observations are as follows: (a) The incurred times of centralized LMA and centralized PIC increase with more data, which is expected. When |D| \u2265 32000, centralized LMA incurs only 16-30 minutes (as compared to FGP incurring > 4 hours) while centralized PIC and SSGP incur, respectively, more than 3.5 and 2 hours due to their huge |S|. In fact, Table 2 shows that centralized PIC incurs even more time than FGP for almost all possible settings of |D| and M due to its huge support set. (b) The speedups of parallel LMA and parallel PIC generally increase with more data, as explained in Remark 2 after Theorem 2, except for that of parallel LMA being slightly higher than expected when |D| = 16000. (c) The incurred time of centralized LMA decreases with more blocks (i.e., larger M ), as explained in Remark 2 after Theorem 2. This is also expected of centralized PIC, but its incurred time increases with more blocks instead due to its huge support set, which entails large-scale matrix operations causing a huge number of cache misses6. This highlights the need to use a sufficiently small support set on a single centralized machine so that cache misses will contribute less to incurred time, as compared to data processing. (d) The speedup of parallel LMA increases with more cores, as explained in Remark 2 after Theorem 2. Though the speedup of parallel PIC appears to increase considerably\n6A cache miss causes the processor to access the data from main memory, which costs 10\u00d7 more time than a cache memory access.\nwith more cores, it is primarily due to the substantial number of cache misses (see observation c above) that inflates the incurred time of centralized PIC excessively.\nFig. 2 shows results of RMSEs and incurred times of parallel LMA averaged over 5 random instances with varying support set sizes |S| and Markov orders B, |D| = 8000, and M = 32 obtained using 8 computing nodes (each using 4 cores) for AIMPEAK dataset. Observations are as follows: (a) To achieve RMSEs of 8.1 and 8.0 with least incurred times, one should trade off a larger support set size |S| for a larger Markov order B (or vice versa) to arrive at the respective settings of |S| = 1024, B = 5 (34 seconds) and |S| = 1024, B = 9 (68 seconds), which agrees with Remark 3 after Theorem 2. However, to achieve the same RMSE of 7.9 as FGP, the setting of |S| = 128, B = 21 incurs the least time (i.e., 205 seconds), which seems to indicate that, with small data (|D| = 8000), we should instead focus on increasing Markov order B for LMA to achieve the same predictive performance as FGP; recall that when B = M \u2212 1, LMA becomes FGP. This provides an empirically cheaper and more reliable alternative to increasing |S| for achieving predictive performance comparable to FGP, the latter of which, in our experiments, causes Cholesky factorization failure easily when |S| becomes excessively large. (b) When |S| = 1024, B = 1, and M = 32, parallel LMA using 8 computing nodes incurs less time (i.e., 10 seconds) than that using 32 nodes (i.e., 20 seconds; see Table 1b) because the communication latency between cores within a machine is significantly less than that between machines.\nNext, the predictive performance and scalability of our parallel LMA method are empirically compared with that of parallel PIC using the large EMULATE mean sea level pres-\nsure (EMSLP) dataset (Ansell et al. 2006) of size 1278250 on a 5\u25e6 lat.-lon. grid bounded within lat. 25-70N and lon. 70W-50E from 1900 to 2003. Each input denotes a 6D feature vector of latitude, longitude, year, month, day, and incremental day count (starting from 0 on first day). The output is the mean sea level pressure (Pa). The experimental setup is the same as before, except for the platform that is a cluster of 16 computing nodes connected via gigabit links: Each node runs a Linux system with AMD OpteronTM 6272 at 2.1 GHz with 32 GB memory and 32 cores.\nTable 3 shows results of RMSEs and incurred times of parallel LMA and parallel PIC averaged over 5 random instances with M = 512 cores and varying data sizes |D| for EMSLP dataset. When |D| = 128000, parallel LMA incurs much less time than parallel PIC while achieving better predictive performance because it requires a significantly smaller |S| by setting B = 1, as explained earlier. When |D| \u2265 256000, parallel PIC fails due to insufficient shared memory between cores. On the other hand, parallel LMA does not experience this issue and incurs from 10 minutes for |D| = 256000 to about 22 hours for |D| = 1000000. Summary of Experimental Results. LMA is significantly more scalable than FGP in the data size while achieving a comparable predictive performance for SARCOS and AIMPEAK datasets. For example, when |D| = 32000 and M \u2265 48, our centralized and parallel LMA methods are, respectively, at least 1 and 2 orders of magnitude faster than FGP while achieving comparable predictive performances for AIMPEAK dataset. Our centralized (parallel) LMA method also incurs much less time than centralized PIC (parallel PIC) and SSGP while achieving comparable or better predictive performance because LMA requires a considerably smaller support set size |S| than PIC and SSGP simply by setting Markov order B = 1, as explained earlier. Trading off between support set size and Markov order of LMA results in less incurred time while achieving the desired predictive performance. LMA gives a more reliable alternative of increasing the Markov order (i.e., to increasing support set size) for achieving predictive performance similar to FGP; in practice, a huge support set causes Cholesky factorization failure and insufficient shared memory between cores easily. Finally, parallel LMA can scale up to work for EMSLP dataset of more than a million in size."}, {"heading": "5 Conclusion", "text": "This paper describes a LMA method that leverages the dual computational advantages stemming from complementing the low-rank covariance matrix approximation based on support set with the dense residual covariance matrix approximation due to Markov assumption. As a result, LMA can make a more relaxed conditional independence assumption (especially with larger data) than many existing sparse GP regression methods utilizing low-rank representations,\nthe latter of which utilize a sparse residual covariance matrix approximation. Empirical results have shown that our centralized (parallel) LMA method is much more scalable than FGP and time-efficient than centralized PIC (parallel PIC) and SSGP while achieving comparable predictive performance. In our future work, we plan to develop a technique to automatically determine the \u201coptimal\u201d support set size and Markov order and devise an \u201canytime\u201d variant of LMA using stochastic variational inference like (Hensman, Fusi, and Lawrence 2013) so that it can train with a small subset of data in each iteration instead of learning using all the data. We also plan to release the source code at http://code.google.com/p/pgpr/. Acknowledgments. This work was supported by SingaporeMIT Alliance for Research and Technology Subaward Agreement No. 52 R-252-000-550-592."}, {"heading": "A Proof of Theorem 1", "text": "DKL(RDD, RDD) +DKL(RDD, R\u0302) = 1\n2\n( tr(RDDR \u22121 DD)\u2212log |RDDR \u22121 DD|\u2212|D| ) +\n1\n2\n( tr(RDDR\u0302\u22121)\u2212log |RDDR\u0302\u22121|\u2212|D| ) = 1\n2\n( tr(RDDR\u0302\u22121)\u2212 log |RDD| \u2212 log |R\u0302\u22121| \u2212 |D| ) = 1\n2\n( tr(RDDR\u0302\u22121)\u2212 log |RDDR\u0302\u22121| \u2212 |D| ) = DKL(RDD, R\u0302) .\nThe second equality is due to tr(RDDR \u22121 DD) = tr(RDDR \u22121 DD) = tr(I|D|) = |D|, which follows from the observations that the blocks within the B-block bands of RDD and RDD are the same and R \u22121 DD is Bblock-banded (Proposition 1). The third equality follows from the first observation above and the definition that R\u0302\u22121 is B-block-banded. Since DKL(RDD, R\u0302) \u2265 0, DKL(RDD, R\u0302) \u2265 DKL(RDD, RDD)."}, {"heading": "B Proof of Theorem 2", "text": "The following lemma is necessary for deriving our main result here. It shows that the sparsity of B-block-banded R \u22121 DD (Proposition 1) extends to that of its Cholesky factor (Fig. 3):\nLemma 1 Let R\u22121DD , U>U where Cholesky factor U = [Umn]m,n=1,...,M is an upper triangular matrix (Fig. 3). Then, Umn = 0 if m\u2212 n > 0 or n\u2212m > B. Furthermore, for m = 1, . . . ,M , Umm = cholesky(R\u0307m) and UBm , [Umn]n=m+1,...,min(m+B,M) = \u2212UmmRDmDBmR \u22121 DBmDBm .\nIts proof follows directly from block-banded matrix results of Asif and Moura (2005) (i.e., Lemma 1.1 and Theorem 1).\n\u03a3 \u22121 DD = ( \u03a3DS\u03a3 \u22121 SS\u03a3SD +RDD )\u22121 = R \u22121 DD\u2212R \u22121 DD\u03a3DS ( \u03a3SS+\u03a3SDR \u22121 DD\u03a3DS )\u22121 \u03a3SDR \u22121 DD = R \u22121 DD \u2212R \u22121 DD\u03a3DS\u03a3\u0308 \u22121 SS\u03a3SDR \u22121 DD .\n(5)\nshaded blocks outsideB-block band ofR \u22121 DD (i.e., |m\u2212n| > B) are 0 (Fig. 1b), which result in the unshaded blocks of its Cholesky factor U being 0 (i.e., m\u2212 n > 0 or n\u2212m > B).\nThe second equality is due to the matrix inversion lemma. The last equality follows from\n\u03a3SDR \u22121 DD\u03a3DS = \u03a3SDU >U\u03a3DS\n= M\u2211 m=1 \u03a3S(Dm\u222aDBm) [ U>mm UB>m ] [ Umm, U B m ] \u03a3(Dm\u222aDBm)S\n= M\u2211 m=1 ( Umm\u03a3DmS + U B m\u03a3DBmS )>( Umm\u03a3DmS + U B m\u03a3DBmS ) =\nM\u2211 m=1 (\u03a3\u0307mS ) >R\u0307m\u03a3\u0307 m S\n= \u03a3\u0308SS \u2212 \u03a3SS . The fourth equality is due to Lemma 1 and Definition 1. The last equality follows from Definition 2.\n\u00b5LMAU (3) = \u00b5U + \u03a3UD\u03a3 \u22121 DD (yD \u2212 \u00b5D) (5) = \u00b5U + \u03a3UDR \u22121 DD (yD \u2212 \u00b5D)\u2212\n\u03a3UDR \u22121 DD\u03a3DS\u03a3\u0308 \u22121 SS\u03a3SDR \u22121 DD (yD \u2212 \u00b5D)\n= \u00b5U + y\u0308U \u2212 \u03a3\u0308US\u03a3\u0308\u22121SS y\u0308S . The last equality is derived using Definition 2, specifically, from the expressions of the following three y\u0308U , \u03a3\u0308US , and y\u0308S components, respectively:\n\u03a3UDR \u22121 DD (yD \u2212 \u00b5D) = \u03a3UDU >U (yD \u2212 \u00b5D)\n= M\u2211 m=1 \u03a3U(Dm\u222aDBm) [ U>mm UB>m ] [ Umm, U B m ] ( yDm\u222aDBm \u2212 \u00b5Dm\u222aDBm ) =\nM\u2211 m=1 ( Umm\u03a3DmU + U B m\u03a3DBmU )> ( Umm (yDm \u2212 \u00b5Dm) + UBm ( yDBm \u2212 \u00b5DBm\n)) =\nM\u2211 m=1 (\u03a3\u0307mU ) >R\u0307my\u0307m\n= y\u0308U .\nThe fourth equality is due to Lemma 1 and Definition 1. \u03a3DmU = [ \u03a3DmUn ] n=1,...,M\nand \u03a3DBmU =[ \u03a3DkUn ] k=m+1,...,min(m+B,M),n=1,...,M\nare obtained using (2). The last equality follows from Definition 2.\n\u03a3UDR \u22121 DD\u03a3DS = \u03a3UDU >U\u03a3DS\n= M\u2211 m=1 \u03a3U(Dm\u222aDBm) [ U>mm UB>m ] [ Umm, U B m ] \u03a3(Dm\u222aDBm)S\n= M\u2211 m=1 ( Umm\u03a3DmU+U B m\u03a3DBmU )>( Umm\u03a3DmS+U B m\u03a3DBmS ) =\nM\u2211 m=1 (\u03a3\u0307mU ) >R\u0307m\u03a3\u0307 m S\n= \u03a3\u0308US .\nThe fourth equality is due to Lemma 1 and Definition 1. The last equality follows from Definition 2.\nFinally,\n\u03a3SDR \u22121 DD (yD \u2212 \u00b5D) = \u03a3SDU >U (yD \u2212 \u00b5D)\n= M\u2211 m=1 \u03a3S(Dm\u222aDBm) [ U>mm UB>m ] [ Umm, U B m ] ( yDm\u222aDBm \u2212 \u00b5Dm\u222aDBm ) =\nM\u2211 m=1 ( Umm\u03a3DmS + U B m\u03a3DBmS )> ( Umm (yDm \u2212 \u00b5Dm) + UBm ( yDBm \u2212 \u00b5DBm\n)) =\nM\u2211 m=1 (\u03a3\u0307mS ) >R\u0307my\u0307m\n= y\u0308S . The fourth equality is due to Lemma 1 and Definition 1. The last equality follows from Definition 2. \u03a3LMAUU (4) = \u03a3UU \u2212 \u03a3UD\u03a3 \u22121 DD\u03a3DU (5) = \u03a3UU \u2212 \u03a3UDR \u22121 DD\u03a3DU + \u03a3UDR \u22121 DD\u03a3DS\u03a3\u0308 \u22121 SS\u03a3SDR \u22121 DD\u03a3DU = \u03a3UU \u2212 \u03a3\u0308UU + \u03a3\u0308US\u03a3\u0308\u22121SS\u03a3\u0308>US . The last equality is derived using Definition 2, specifically, from the expression of the \u03a3\u0308US component above as well as that of the following \u03a3\u0308UU component: \u03a3UDR \u22121 DD\u03a3DU = \u03a3UDU >U\u03a3DU\n= M\u2211 m=1 \u03a3U(Dm\u222aDBm) [ U>mm UB>m ] [ Umm, U B m ] \u03a3(Dm\u222aDBm)U\n= M\u2211 m=1 ( Umm\u03a3DmU+U B m\u03a3DBmU )>( Umm\u03a3DmU+U B m\u03a3DBmU ) =\nM\u2211 m=1 (\u03a3\u0307mU ) >R\u0307m\u03a3\u0307 m U\n= \u03a3\u0308UU . The fourth equality is due to Lemma 1 and Definition 1. The last equality follows from Definition 2."}, {"heading": "C Parallel Computation of \u03a3DmU and \u03a3DBmU", "text": "Computing \u03a3DmU and \u03a3DBmU terms (2) requires evaluating RDmU and RDBmU terms (1), which are stored and used by each machine/core m to construct the m-th local summary. We will describe the parallel computation of RDU , from whichRDmU andRDBmU terms can be obtained by machine/core m. To simplify exposition, we will consider the simple setting of B = 1 and M = 4 here.\nFor the blocks within the 1-block band of RDU (i.e., |m \u2212 n| \u2264 1), they correspond exactly to that of the residual covariance matrix RDU . So, each machine/core m for m = 1, . . . , 4 can directly compute its respective blocks in parallel (Fig. 4), specifically, RDm \u22c3min(m+1,M) n=max(m\u22121,1) Un = RDm \u22c3min(m+1,M) n=max(m\u22121,1) Un and RD1m \u22c3min(m+2,M) n=max(m,1) Un = RDm+1 \u22c3min(m+2,M) n=max(m,1) Un .\nLet the upper N -diagonal blocks denote the ones that are N blocks above the main diagonal ones of RDU . For the blocks strictly above the 1-block band of RDU (i.e., n \u2212 m > 1), the key idea is to exploit the recursive definition of RDU to compute the upper 2-diagonal blocks in parallel using the upper 1-diagonal blocks, followed by computing the upper 3-diagonal block using an upper 2- diagonal block. Specifically, they can be computed in 2 recursive steps (Fig. 5): In each recursive step i, each machine/corem form = 1, . . . , 3\u2212 i uses the upper i-diagonal blocks RDm+1Um+1+i to compute the upper (i+1)-diagonal blocks RDmUm+1+i = RDmD1mR \u22121 D1mD1mRD 1 mUm+1+i = RDmDm+1R \u22121 Dm+1Dm+1RDm+1Um+1+i (1) in parallel and then communicates RDmUm+1+i to machine/core m\u2212 1. For the blocks strictly below the 1-block band of RDU (i.e., m \u2212 n > 1), it is similar but less straightforward: It can be observed from (1) that RDmUn = RDmD1nR \u22121 D1nD1nRD 1 nUn = RDmDn+1R \u22121 Dn+1Dn+1RDn+1Un . But, machine/core m does not store data associated with the set Dn+1 of inputs in order to compute RDmDn+1 . To resolve this, the trick is to compute the transpose of RDmUn (i.e., RUnDm ) for m \u2212 n > 1 instead. These transposed blocks can also be computed in 2 recursive steps like the above: In each recursive step i, each machine/core n for n = 1, . . . , 3 \u2212 i uses the upper i-diagonal blocks RDn+1Dn+1+i (i.e., equal to RDn+1Dn+2 when i = 1 that can be directly computed by machine/core n in parallel) of RDD to compute the upper (i+1)-diagonal blocks RUnDn+1+i of RUD:\nRUnDn+1+i (1) = RUnD1nR \u22121 D1nD1nRD 1 nDn+1+i\n= RUnDn+1R \u22121 Dn+1Dn+1RDn+1Dn+1+i\nas well as the upper (i+1)-diagonal blocks RDnDn+1+i of RDD in parallel:\nRDnDn+1+i (1) = RDnD1nR \u22121 D1nD1nRD 1 nDn+1+i\n= RDnDn+1R \u22121 Dn+1Dn+1RDn+1Dn+1+i\nStep 1 Step 2\nMachine 4 D4\nU1 U2 U3 U4\nMachine 3 D3\nD4\nU1 U2 U3 U4\nMachine 2 D2\nD3\nU1 U2 U3 U4\nMachine 1 D1\nD2\nU1 U2 U3 U4\nD4\nU1 U2 U3 U4\nD3\nD4\nU1 U2 U3 U4\nD2\nD3\nU1 U2 U3 U4\nD1\nD2\nU1 U2 U3 U4\nFigure 5: In step 1, machines 1 and 2 compute their dark gray blocks in parallel and machine 2 communicates the dark gray block highlighted in red to machine 1. In step 2, machine 1 computes the dark gray block.\nand then communicates RDnDn+1+i to machine/core n\u2212 1. Finally, each machine/core n for n = 1, 2 transposes the previously computedRUnDm back toRDmUn form\u2212n > 1 and communicates RDmUn to machines/cores m\u2212 1 and m.\nThe parallel computation of RDU is thus complete. The procedure to parallelize the computation of RDU for any general setting of B and M is similar to the above, albeit more tedious notationally."}, {"heading": "D Toy Example", "text": "For our LMA method, the settings areM = 4, Markov order B = 1, support set (black \u00d7\u2019s) size |S| = 16, and training data (red\u00d7\u2019s) size |D| = 400 such that x < \u22122.5 if x \u2208 D1, \u22122.5 \u2264 x < 0 if x \u2208 D2, 0 \u2264 x < 2.5 if x \u2208 D3, x \u2265 2.5 if x \u2208 D4, and |D1| = |D2| = |D3| = |D4| = 100. The hyperparameters learned using maximum likelihood estimation are length-scale ` = 1.2270, \u03c3n = 0.0939, \u03c3s = 0.6836, and \u00b5x = 1.1072. It can be observed from Fig. 6 that the posterior/predictive mean curve (blue curve) of our LMA method does not exhibit any discontinuity/jump. The area enclosed by the green curves is the 95% confidence region.\nIn contrast, the posterior/predictive mean curve (blue curve) of the local GPs approach with 4 GPs experiences 3 discontinuities/jumps at the boundaries x = \u22122.5, 0, 2.5."}], "references": [{"title": "Daily mean sea level pressure reconstructions for the European-North Atlantic region for the period 1850-2003", "author": ["T.J. Ansell et al."], "venue": "J. Climate 19(12):2717\u20132742.", "citeRegEx": "al.,? 2006", "shortCiteRegEx": "al.", "year": 2006}, {"title": "Block matrices with L-block-banded inverse: Inversion algorithms", "author": ["A. Asif", "J.M.F. Moura"], "venue": "IEEE Trans. Signal Processing 53(2):630\u2013642.", "citeRegEx": "Asif and Moura,? 2005", "shortCiteRegEx": "Asif and Moura", "year": 2005}, {"title": "Multi-robot informative path planning for active sensing of environmental phenomena: A tale of two algorithms", "author": ["N. Cao", "K.H. Low", "J.M. Dolan"], "venue": "Proc. AAMAS.", "citeRegEx": "Cao et al\\.,? 2013", "shortCiteRegEx": "Cao et al\\.", "year": 2013}, {"title": "Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan", "A. Oran", "P. Jaillet", "J.M. Dolan", "G.S. Sukhatme"], "venue": "Proc. UAI, 163\u2013173.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Parallel Gaussian process regression with low-rank covariance matrix approximations", "author": ["J. Chen", "N. Cao", "K.H. Low", "R. Ouyang", "C.K.-Y. Tan", "P. Jaillet"], "venue": "Proc. UAI, 152\u2013161.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Gaussian process-based decentralized data fusion and active sensing for mobility-on-demand system", "author": ["J. Chen", "K.H. Low", "C.K.-Y. Tan"], "venue": "Proc. RSS.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Cooperative aquatic sensing using the telesupervised adaptive ocean sensor fleet", "author": ["J.M. Dolan", "G. Podnar", "S. Stancliff", "K.H. Low", "A. Elfes", "J. Higinbotham", "J.C. Hosler", "T.A. Moisan", "J. Moisan"], "venue": "Proc. SPIE Conference on Remote Sensing of the Ocean, Sea Ice, and Large Water", "citeRegEx": "Dolan et al\\.,? 2009", "shortCiteRegEx": "Dolan et al\\.", "year": 2009}, {"title": "Covariance tapering for interpolation of large spatial datasets", "author": ["R. Furrer", "M.G. Genton", "D. Nychka"], "venue": "JCGS 15(3):502\u2013523.", "citeRegEx": "Furrer et al\\.,? 2006", "shortCiteRegEx": "Furrer et al\\.", "year": 2006}, {"title": "Gaussian processes for big data", "author": ["J. Hensman", "N. Fusi", "N. Lawrence"], "venue": "Proc. UAI, 282\u2013290.", "citeRegEx": "Hensman et al\\.,? 2013", "shortCiteRegEx": "Hensman et al\\.", "year": 2013}, {"title": "Active learning is planning: Nonmyopic -Bayesoptimal active learning of Gaussian processes", "author": ["T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli"], "venue": "Proc. ECML/PKDD Nectar Track, 494\u2013498.", "citeRegEx": "Hoang et al\\.,? 2014a", "shortCiteRegEx": "Hoang et al\\.", "year": 2014}, {"title": "Nonmyopic -Bayes-optimal active learning of Gaussian processes", "author": ["T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli"], "venue": "Proc. ICML, 739\u2013747.", "citeRegEx": "Hoang et al\\.,? 2014b", "shortCiteRegEx": "Hoang et al\\.", "year": 2014}, {"title": "Sparse spectrum Gaussian process regression", "author": ["M. L\u00e1zaro-Gredilla", "J. Qui\u00f1onero-Candela", "C.E. Rasmussen", "A.R. Figueiras-Vidal"], "venue": "JMLR 11:1865\u20131881.", "citeRegEx": "L\u00e1zaro.Gredilla et al\\.,? 2010", "shortCiteRegEx": "L\u00e1zaro.Gredilla et al\\.", "year": 2010}, {"title": "Decentralized active robotic exploration and mapping for probabilistic field classification in environmental sensing", "author": ["K.H. Low", "J. Chen", "J.M. Dolan", "S. Chien", "D.R. Thompson"], "venue": "Proc. AAMAS, 105\u2013112.", "citeRegEx": "Low et al\\.,? 2012", "shortCiteRegEx": "Low et al\\.", "year": 2012}, {"title": "Recent advances in scaling up Gaussian process predictive models for large spatiotemporal data", "author": ["K.H. Low", "J. Chen", "T.N. Hoang", "N. Xu", "P. Jaillet"], "venue": "Proc. DyDESS.", "citeRegEx": "Low et al\\.,? 2014a", "shortCiteRegEx": "Low et al\\.", "year": 2014}, {"title": "Generalized online sparse Gaussian processes with application to persistent mobile robot localization", "author": ["K.H. Low", "N. Xu", "J. Chen", "K.K. Lim", "E.B. \u00d6zg\u00fcl"], "venue": "Proc. ECML/PKDD Nectar Track, 499\u2013503.", "citeRegEx": "Low et al\\.,? 2014b", "shortCiteRegEx": "Low et al\\.", "year": 2014}, {"title": "Adaptive multi-robot wide-area exploration and mapping", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 23\u201330.", "citeRegEx": "Low et al\\.,? 2008", "shortCiteRegEx": "Low et al\\.", "year": 2008}, {"title": "Informationtheoretic approach to efficient adaptive path planning for mobile robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. ICAPS.", "citeRegEx": "Low et al\\.,? 2009", "shortCiteRegEx": "Low et al\\.", "year": 2009}, {"title": "Active Markov information-theoretic path planning for robotic environmental sensing", "author": ["K.H. Low", "J.M. Dolan", "P. Khosla"], "venue": "Proc. AAMAS, 753\u2013760.", "citeRegEx": "Low et al\\.,? 2011", "shortCiteRegEx": "Low et al\\.", "year": 2011}, {"title": "Multirobot active sensing of non-stationary Gaussian processbased environmental phenomena", "author": ["R. Ouyang", "K.H. Low", "J. Chen", "P. Jaillet"], "venue": "Proc. AAMAS.", "citeRegEx": "Ouyang et al\\.,? 2014", "shortCiteRegEx": "Ouyang et al\\.", "year": 2014}, {"title": "Domain decomposition approach for fast Gaussian process regression of large spatial data sets", "author": ["C. Park", "J.Z. Huang", "Y. Ding"], "venue": "JMLR 12:1697\u20131728.", "citeRegEx": "Park et al\\.,? 2011", "shortCiteRegEx": "Park et al\\.", "year": 2011}, {"title": "Telesupervised remote surface water quality sensing", "author": ["G. Podnar", "J.M. Dolan", "K.H. Low", "A. Elfes"], "venue": "Proc. IEEE Aerospace Conference.", "citeRegEx": "Podnar et al\\.,? 2010", "shortCiteRegEx": "Podnar et al\\.", "year": 2010}, {"title": "A unifying view of sparse approximate Gaussian process regression", "author": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen"], "venue": "JMLR 6:1939\u20131959.", "citeRegEx": "Qui\u00f1onero.Candela and Rasmussen,? 2005", "shortCiteRegEx": "Qui\u00f1onero.Candela and Rasmussen", "year": 2005}, {"title": "Sparse Gaussian processes using pseudo-inputs", "author": ["E. Snelson", "Z. Ghahramani"], "venue": "Proc. NIPS.", "citeRegEx": "Snelson and Ghahramani,? 2005", "shortCiteRegEx": "Snelson and Ghahramani", "year": 2005}, {"title": "Local and global sparse Gaussian process approximations", "author": ["E. Snelson", "Z. Ghahramani"], "venue": "Proc. AISTATS.", "citeRegEx": "Snelson and Ghahramani,? 2007", "shortCiteRegEx": "Snelson and Ghahramani", "year": 2007}, {"title": "Incremental online learning in high dimensions", "author": ["S. Vijayakumar", "A. D\u2019Souza", "S. Schaal"], "venue": "Neural Comput", "citeRegEx": "Vijayakumar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Vijayakumar et al\\.", "year": 2005}, {"title": "GP-Localize: Persistent mobile robot localization using online sparse Gaussian process observation model", "author": ["N. Xu", "K.H. Low", "J. Chen", "K.K. Lim", "E.B. \u00d6zg\u00fcl"], "venue": "Proc. AAAI, 2585\u20132592.", "citeRegEx": "Xu et al\\.,? 2014", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Hierarchical Bayesian nonparametric approach to modeling and learning the wisdom of crowds of urban traffic route planning agents", "author": ["J. Yu", "K.H. Low", "A. Oran", "P. Jaillet"], "venue": "Proc. IAT, 478\u2013485.", "citeRegEx": "Yu et al\\.,? 2012", "shortCiteRegEx": "Yu et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Recent sparse GP regression methods (Chen et al. 2013; Snelson and Ghahramani 2007) have unified approaches from the two families described above to harness their complementary modeling and predictive capabilities (hence, eliminating their deficiencies) while retaining their computational advantages.", "startOffset": 36, "endOffset": 83}, {"referenceID": 23, "context": "Recent sparse GP regression methods (Chen et al. 2013; Snelson and Ghahramani 2007) have unified approaches from the two families described above to harness their complementary modeling and predictive capabilities (hence, eliminating their deficiencies) while retaining their computational advantages.", "startOffset": 36, "endOffset": 83}, {"referenceID": 4, "context": ", (Chen et al. 2013; Snelson and Ghahramani 2007)) or number of spectral points (L\u00e1zaro-Gredilla et al.", "startOffset": 2, "endOffset": 49}, {"referenceID": 23, "context": ", (Chen et al. 2013; Snelson and Ghahramani 2007)) or number of spectral points (L\u00e1zaro-Gredilla et al.", "startOffset": 2, "endOffset": 49}, {"referenceID": 11, "context": "2013; Snelson and Ghahramani 2007)) or number of spectral points (L\u00e1zaro-Gredilla et al. 2010) as the varying parameter while achieving predictive performance comparable to them and (b) accurately represent features/patterns of any scale.", "startOffset": 65, "endOffset": 94}, {"referenceID": 4, "context": "Interestingly, varying the Markov order produces a spectrum of LMAs with the partially independent conditional (PIC) approximation (Chen et al. 2013; Snelson and Ghahramani 2007) and FGP at the two extremes.", "startOffset": 131, "endOffset": 178}, {"referenceID": 23, "context": "Interestingly, varying the Markov order produces a spectrum of LMAs with the partially independent conditional (PIC) approximation (Chen et al. 2013; Snelson and Ghahramani 2007) and FGP at the two extremes.", "startOffset": 131, "endOffset": 178}, {"referenceID": 6, "context": ", ocean sensing (Cao, Low, and Dolan 2013; Dolan et al. 2009; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2012; Podnar et al. 2010), traffic monitoring (Chen et al.", "startOffset": 16, "endOffset": 139}, {"referenceID": 12, "context": ", ocean sensing (Cao, Low, and Dolan 2013; Dolan et al. 2009; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2012; Podnar et al. 2010), traffic monitoring (Chen et al.", "startOffset": 16, "endOffset": 139}, {"referenceID": 20, "context": ", ocean sensing (Cao, Low, and Dolan 2013; Dolan et al. 2009; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2012; Podnar et al. 2010), traffic monitoring (Chen et al.", "startOffset": 16, "endOffset": 139}, {"referenceID": 3, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 9, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 13, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 18, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 25, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 26, "context": "2010), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)).", "startOffset": 26, "endOffset": 173}, {"referenceID": 0, "context": "D and U are partitioned according to a simple parallelized clustering scheme employed in the work of Chen et al. (2013).", "startOffset": 49, "endOffset": 120}, {"referenceID": 4, "context": "Note that when B = 0, \u03a3VmVn = QVmVn for |m \u2212 n| > B, thus yielding the prior covariance matrix \u03a3VV of the partially independent conditional (PIC) approximation (Chen et al. 2013; Snelson and Ghahramani 2007).", "startOffset": 160, "endOffset": 207}, {"referenceID": 23, "context": "Note that when B = 0, \u03a3VmVn = QVmVn for |m \u2212 n| > B, thus yielding the prior covariance matrix \u03a3VV of the partially independent conditional (PIC) approximation (Chen et al. 2013; Snelson and Ghahramani 2007).", "startOffset": 160, "endOffset": 207}, {"referenceID": 0, "context": "Its proof follows directly from a block-banded matrix result of Asif and Moura (2005) (specifically, Theorem 3).", "startOffset": 64, "endOffset": 86}, {"referenceID": 4, "context": "For example, PIC (Chen et al. 2013; Snelson and Ghahramani 2007) assumes YDm and YDn to be conditionally independent given only YS if |m\u2212 n| > 0.", "startOffset": 17, "endOffset": 64}, {"referenceID": 23, "context": "For example, PIC (Chen et al. 2013; Snelson and Ghahramani 2007) assumes YDm and YDn to be conditionally independent given only YS if |m\u2212 n| > 0.", "startOffset": 17, "endOffset": 64}, {"referenceID": 4, "context": "In contrast, PIC (Chen et al. 2013; Snelson and Ghahramani 2007) (sparse spectrum GP (L\u00e1zaro-Gredilla et al.", "startOffset": 17, "endOffset": 64}, {"referenceID": 23, "context": "In contrast, PIC (Chen et al. 2013; Snelson and Ghahramani 2007) (sparse spectrum GP (L\u00e1zaro-Gredilla et al.", "startOffset": 17, "endOffset": 64}, {"referenceID": 11, "context": "2013; Snelson and Ghahramani 2007) (sparse spectrum GP (L\u00e1zaro-Gredilla et al. 2010)) can only vary support set size (number of spectral points) to obtain the desired predictive performance.", "startOffset": 55, "endOffset": 84}, {"referenceID": 0, "context": "A notable exception is the work of Chen et al. (2013) that parallelizes PIC.", "startOffset": 43, "endOffset": 54}, {"referenceID": 23, "context": "This section first empirically evaluates the predictive performance and scalability of our proposed centralized and parallel LMA methods against that of the state-of-the-art centralized PIC (Snelson and Ghahramani 2007), parallel PIC (Chen et al.", "startOffset": 190, "endOffset": 219}, {"referenceID": 4, "context": "This section first empirically evaluates the predictive performance and scalability of our proposed centralized and parallel LMA methods against that of the state-of-the-art centralized PIC (Snelson and Ghahramani 2007), parallel PIC (Chen et al. 2013), sparse spectrum GP (SSGP) (L\u00e1zaroGredilla et al.", "startOffset": 234, "endOffset": 252}, {"referenceID": 4, "context": "(b) The AIMPEAK dataset (Chen et al. 2013) of size 41850 comprises traffic speeds (km/h) along 775 road segments of an urban road network during morning peak hours on April 20, 2011.", "startOffset": 24, "endOffset": 42}, {"referenceID": 3, "context": "This traffic dataset is modeled using a relational GP (Chen et al. 2012) whose correlation structure can exploit the road segment features and road network topology information.", "startOffset": 54, "endOffset": 72}, {"referenceID": 3, "context": ", of road segments) onto the Euclidean space (Chen et al. 2012) before applying the covariance function.", "startOffset": 45, "endOffset": 63}, {"referenceID": 4, "context": "From Table 1b, when training data is small (|D| = 8000) for AIMPEAK dataset, parallel PIC incurs more time than FGP due to its huge |S| = 5120, which causes communication latency to dominate the incurred time (Chen et al. 2013).", "startOffset": 209, "endOffset": 227}], "year": 2014, "abstractText": "The expressive power of a Gaussian process (GP) model comes at a cost of poor scalability in the data size. To improve its scalability, this paper presents a low-rank-cum-Markov approximation (LMA) of the GP model that is novel in leveraging the dual computational advantages stemming from complementing a low-rank approximate representation of the full-rank GP based on a support set of inputs with a Markov approximation of the resulting residual process; the latter approximation is guaranteed to be closest in the Kullback-Leibler distance criterion subject to some constraint and is considerably more refined than that of existing sparse GP models utilizing low-rank representations due to its more relaxed conditional independence assumption (especially with larger data). As a result, our LMA method can trade off between the size of the support set and the order of the Markov property to (a) incur lower computational cost than such sparse GP models while achieving predictive performance comparable to them and (b) accurately represent features/patterns of any scale. Interestingly, varying the Markov order produces a spectrum of LMAs with PIC approximation and full-rank GP at the two extremes. An advantage of our LMA method is that it is amenable to parallelization on multiple machines/cores, thereby gaining greater scalability. Empirical evaluation on three real-world datasets in clusters of up to 32 computing nodes shows that our centralized and parallel LMA methods are significantly more time-efficient and scalable than state-of-the-art sparse and full-rank GP regression methods while achieving comparable predictive performances.", "creator": "TeX"}}}