{"id": "1411.5928", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2014", "title": "Learning to Generate Chairs, Tables and Cars with Convolutional Networks", "abstract": "we train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, thickness and color. we train just the user network simulation in a supervised manner on a dataset of rendered 3d chair models. our experiments show that the network does actually not indeed merely learn all images by heart, but mainly rather finds a meaningful representation as of a 3d chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the actual missing ones, explain or invent both new chair styles by interpolating results between chairs represented from the training set. we show that the network can be used to temporarily find correspondences between different chairs from the dataset, outperforming existing approaches on this task.", "histories": [["v1", "Fri, 21 Nov 2014 16:01:04 GMT  (9011kb,D)", "http://arxiv.org/abs/1411.5928v1", null], ["v2", "Mon, 5 Jan 2015 12:31:49 GMT  (8191kb,D)", "http://arxiv.org/abs/1411.5928v2", "v2: minor changes. Adjusted the figures (mainly figure 2, figure 11); added urls of videos in the supplementary"], ["v3", "Thu, 3 Dec 2015 09:49:23 GMT  (9391kb,D)", "http://arxiv.org/abs/1411.5928v3", "v3: added new object classes, new architectures, many new experiments"], ["v4", "Wed, 2 Aug 2017 20:53:43 GMT  (7411kb,D)", "http://arxiv.org/abs/1411.5928v4", "v4: final PAMI version. New architecture figure"]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["alexey dosovitskiy", "jost tobias springenberg", "maxim tatarchenko", "thomas brox"], "accepted": false, "id": "1411.5928"}, "pdf": {"name": "1411.5928.pdf", "metadata": {"source": "CRF", "title": "Learning to Generate Chairs with Convolutional Neural Networks", "authors": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Thomas Brox"], "emails": ["brox}@cs.uni-freiburg.de"], "sections": [{"heading": "1. Introduction", "text": "Convolutional neural networks (CNNs) have been shown to be very successful on a variety of computer vision tasks, such as image classification [18, 5, 32], detection [10, 28] and segmentation [10]. All these tasks have in common that they can be posed as discriminative supervised learning problems, and hence can be solved using CNNs which are known to perform well given a large enough labeled dataset. Typically, a task solved by supervised CNNs involves learning mappings from raw sensor inputs to some sort of condensed, abstract output representation, such as object identity, position or scale. In this work, we stick with supervised training, but we turn the standard discriminative CNN upside down and use it to generate images given highlevel information.\nGiven the set of 3D chair models of Aubry et al. [1], we aim to train a neural network capable of generating 2D projections of the models given the chair type, viewpoint, and, optionally, other parameters such as color, brightness, saturation, zoom, etc. Our neural network accepts as input these high-level values and produces an RGB image. We train it using standard backpropagation to minimize the Euclidean reconstruction error of the generated image.\nIt is not a surprise that a large enough neural network\ncan perfectly approximate any function on the training set. In our case, a network potentially could just learn by heart all examples and provide perfect reconstructions of these, but would behave unpredictably when confronted with inputs it has not seen during training. We show that this is not what is happening, both because the network is too small to just remember all images, and because we observe generalization to previously unseen data. Namely, we show that the network is capable of: 1) knowledge transfer: given limited number of viewpoints of an object, the network can use the knowledge learned from other similar objects to infer the remaining viewpoints; 2) interpolation between different objects; see Figure 1 for an example.\nIn what follows we describe the model in detail, analyze the internal functioning of the network and study generalization of the network to unseen data, as described above. As an example of a practical application, we apply point tracking to \u2019morphings\u2019 of different chairs (as in Figure 1) to find correspondences between these chairs. We show that this method is more accurate than existing approaches."}, {"heading": "2. Related work", "text": "Work on generative models of images typically addresses the problem of unsupervised learning of a data model which can generate samples from a latent represen-\n1\nar X\niv :1\n41 1.\n59 28\nv1 [\ncs .C\nV ]\n2 1\nN ov\n2 01\n4\ntation. Prominent examples from this line of work are restricted Boltzmann machines (RBMs) [13] and Deep Boltzmann Machines (DBMs) [26], as well as the plethora of models derived from them [12, 22, 20, 29, 23]. RBMs and DBMs are undirected graphical models which aim to build a probabilistic model of the data and treat encoding and generation as an (intractable) joint inference problem.\nA different approach is to train directed graphical models of the data distribution. This includes a wide variety of methods ranging from Gaussian mixture models [9, 31] to autoregressive models [19] and stochastic variations of neural networks [2, 11, 25, 17, 30]. Among them Rezende et al. [25] developed an approach for training a generative model with variational inference by performing (stochastic) backpropagation through a latent Gaussian representation. Goodfellow et al. [11] model natural images using a \u201ddeconvolutional\u201d generative network that is similar to our architecture.\nMost unsupervised generative models can be extended to incorporate label information, forming semi-supervised and conditional generative models which lie between fully unsupervised approaches and our work. Examples include: gated conditional RBMs [22] for modeling image transformations, training RBMs to disentangle face identity and pose information using conditional RBMs [24], and learning a generative model of digits conditioned on digit class using variational autoencoders [16]. In contrast to our work, these approaches are typically restricted to small models and images, and they often require an expensive inference procedure \u2013 both during training and for generating images.\nThe general difference of our approach to prior work on learning generative models is that we assume a highlevel latent representation of the images is given and use supervised training. This allows us 1) to generate relatively large high-quality images of 128 \u00d7 128 pixels (as compared to maximum of 48\u00d7 48 pixels in the aforementioned works) and 2) to completely control which images to generate rather than relying on random sampling. The downside is, of course, the need for a label that fully describes the appearance of each image.\nModeling of viewpoint variation is often considered in the context of pose-invariant face recognition [3, 34]. In a recent work Zhu et al. [35] approached this task with a neural network: their network takes a face image as input and generates a random view of this face together with the corresponding viewpoint. The network is fully connected and hence restricted to small images and, similarly to generative models, requires random sampling to generate a desired view. This makes it inapplicable to modeling large and diverse images, such as the chair images we model.\nOur work is also loosely related to applications of CNNs to non-discriminative tasks, such as super-resolution [6] or inferring depth from a single image [8]."}, {"heading": "3. Model description", "text": "Our goal is to train a neural network to generate accurate images of chairs from a high-level description: class, orientation with respect to the camera, and additional parameters such as color, brightness, etc.\nFormally, we assume that we are given a dataset of examples D = {(c1,v1, \u03b81), . . . , (cN ,vN , \u03b8N )} with targets O = {(x1, s1), . . . , (xN , sN )}. The input tuples consist of three vectors: c is the class label in one-hot encoding, v \u2013 azimuth and elevation of the camera position (represented by their sine and cosine 1) and \u03b8 \u2013 the parameters of additional artificial transformations applied to the images. The targets are the RGB output image x and the segmentation mask s.\nWe include artificial transformations T\u03b8 described by the randomly generated parameter vector \u03b8 to increase the amount of variation in the training data and reduce overfitting, analogous to data augmentation in discriminative CNN training [18, 7]. Each T\u03b8 is a combination of the following transformations: in-plane rotation, translation, zoom, stretching horizontally or vertically, changing hue, changing saturation, changing brightness."}, {"heading": "3.1. Network architecture", "text": "We experimented with networks for generating images of size 64\u00d764 and 128\u00d7128. The network architectures for both variations are identical except that the smaller network is reduced by one convolutional layer. The structure of the larger 128\u00d7 128 generative network is shown in Figure 2.\nConceptually the generative network, which we formally refer to as g(c,v, \u03b8), looks like a usual CNN turned upside down. It can be thought of as the composition of two processing steps g = u \u25e6 h.\nLayers FC-1 to FC-4 first build a shared, high dimensional hidden representation h(c,v, \u03b8) from the input parameters. Within these layers the three input vectors are first independently fed through two fully connected layers with 512 neurons each, and then the outputs of these three streams are concatenated. This independent processing is followed by two fully connected layers with 1024 neurons each, yielding the response of the fourth fully connected layer (FC-4).\nAfter these fully connected layers the network splits into two streams (layers FC-5 and uconv-1 to uconv-4), which independently generate the image and object mask from the shared hidden representation. We denote these streams uRGB(\u00b7) and usegm(\u00b7). Each of them consists of a fully connected layer, the output of which is reshaped to a 8 \u00d7 8 multichannel image and fed through 4 \u2019unpool-\n1We do this to deal with periodicity of the angle. If we simply used the number of degrees, the network would have no way to understand that 0 and 359 degrees are in fact very close.\ning+convolution\u2019 layers with 5\u00d7 5 filters and 2\u00d7 2 unpooling. Each layer, except the output layers, is followed by a rectified linear (ReLU) nonlinearity.\nIn order to map the dense 8 \u00d7 8 representation to a high dimensional image, we need to unpool the feature maps (i.e. increase their spatial span) as opposed to the pooling (shrinking the feature maps) implemented by usual CNNs. As illustrated in Figure 3 (left), we perform unpooling by simply replacing each entry of a feature map by an s \u00d7 s block with the entry value in the top left corner and zeros elsewhere. This increases the width and the height of the feature map s times. We used s = 2 in our networks. When a convolutional layer is preceded by such an unpooling operation we can thus think of unpooling+convolution as the inverse operation of the convolution+pooling steps performed in a standard CNN (see Figure 3 right). This is similar to the \u201cdeconvolutional\u201d layers used in previous work [32, 11, 33]."}, {"heading": "3.2. Generative training", "text": "The network parameters W, consisting of all layer weights and biases, are then trained by minimizing the Euclidean error of reconstructing the segmented-out chair image and the segmentation mask (the weights W are omitted from the arguments of h and u for brevity of notation):\nmin W N\u2211 i=1 \u03bb\u2016uRGB(h(ci,vi, \u03b8i))\u2212 T\u03b8i(xi \u00b7 si)\u201622\n+\u2016usegm(h(ci,vi, \u03b8i))\u2212 T\u03b8isi\u201622,\n(1)\nwhere \u03bb is a weighting term, trading off between accurate reconstruction of the image and its segmentation mask respectively. We set \u03bb = 10 in all experiments.\nNote that although the mask could be inferred indirectly from the RGB image by exploiting monotonous background, we do not rely on this but rather require the network to explicitly output the mask. We never directly show these generated masks in the following, but we use them to add white background to the generated examples in many figures."}, {"heading": "3.3. Dataset", "text": "As training data for the generative networks we used the set of 3D chair models made public by Aubry et al. [1]. More specifically, we used the dataset of rendered views they provide. It contains 1393 chair models, each rendered from 62 viewpoints: 31 azimuth angles (with step of 11 degrees) and 2 elevation angles (20 and 30 degrees), with a\nfixed distance to the chair. We found that the dataset includes many near-duplicate models, models differing only by color, or low-quality models. After removing these we ended up with a reduced dataset of 809 models, which we used in our experiments. We cropped the renders to have a small border around the chair and resized to a common size of 128 \u00d7 128 pixels, padding with white where necessary to keep the aspect ratio. Example images are shown in Figure 4. For training the network we also used segmentation masks of all training examples, which we produced by subtracting the monotonous white background."}, {"heading": "3.4. Training details", "text": "For training the networks we built on top of the caffe CNN implementation [14]. We used stochastic gradient descent with a fixed momentum of 0.9. We first trained with a learning rate of 0.0002 for 500 passes through the whole dataset (epochs), and then performed 300 additional epochs of training, dividing the learning rate by 2 after every 100 epochs. We initialized the weights of the network with orthogonal matrices, as recommended by Saxe et al. [27].\nWhen training the 128 \u00d7 128 network from scratch, we observed that its initial energy value never starts decreasing. Since we expect the high-level representation of the 64\u00d764 and 128 \u00d7 128 networks to be very similar, we mitigated this problem by initializing the weights of the 128 \u00d7 128 network with the weights of the trained 64 \u00d7 64 network, except for the two last layers.\nWe used the 128\u00d7128 network in all experiments except for the viewpoint interpolation experiments in section 5.1. In those we used the 64\u00d764 network to reduce computation time."}, {"heading": "4. Analysis of the network", "text": "Neural networks are known to largely remain \u2019black boxes\u2019 whose function is hard to understand. In this section we provide an analysis of our trained generative network with the aim to obtain some intuition about its internal working. We only present the most interesting results here; more can be found in the supplementary material."}, {"heading": "4.1. Network capacity", "text": "The first observation is that the network successfully models the variation in the data. Figure 5 shows results\nwhere the network was forced to generate chairs that are significantly transformed relative to the original images. Each row shows a different type of transformation. Images in the central column are non-transformed. Even in the presence of large transformations, the quality of the generated images is basically as good as without transformation. The image quality typically degrades a little in case of unusual chair shapes (such as rotating office chairs) and chairs including fine details such as armrests (see e.g. one of the armrests in row 7 in Figure 5) or thin elements in the back of the chair (row 3 in Figure 5).\nAn interesting observation is that the network easily deals with extreme color-related transformations, but has some problems representing large spatial changes, especially translations. Our explanation is that the architecture we use does not have means to efficiently model, say, translations: since transformation parameters only affect fully connected layers, the network needs to learn a separate \u2019template\u2019 for each position. A more complex architecture, which would allow transformation parameters to explicitly affect the feature maps of convolutional layers (by translating, rotating, zooming them) might further improve generation quality.\nWe did not extensively experiment with different network configurations. However, small variations in the network\u2019s depth and width did not seem to have significant effect on the performance. It is still likely that parameters\nsuch as the number of layers and their sizes can be further optimized.\nThe 128 \u00d7 128 network has approximately 32 million parameters, the majority of which are in the first fully connected layers of RGB and segmentation streams (FC-5): approximately 16 and 8 million, respectively. This is by far fewer than the approximately 400 million foreground pixels in the training data even when augmentation is not applied. When augmentation is applied, the training data size becomes virtually infinite. These calculations show that learning all samples by heart is not an option."}, {"heading": "4.2. Activating single units", "text": "One way to analyze a neural network (artificial or real) is to visualize the effect of single neuron activations. Although this method does not allow us to judge about the network\u2019s actual functioning, which involves a clever combination of many neurons, it still gives a rough idea of what kind of representation is created by the different network layers.\nActivating single neurons of uconv-3 feature maps (last feature maps before the output) is equivalent to simply looking at the filters of these layers which are shown in Figure 6. The final output of the network at each position is a linear combination of these filters. As to be expected, they include edges and blobs.\nOur model is tailored to generate images from high-level neuron activations, which allows us to activate a single neuron in some of the higher layers and forward-propagate down to the image. The results of this procedure for different layers of the network are shown in Figures 7 and 9. Each row corresponds to a different network layer. The leftmost image in each row is generated by setting all neurons of the layer to zero, and the other images \u2013 by activating one randomly selected neuron.\nIn Figure 7 the first two rows show images produced when activating neurons of FC-1 and FC-2 feature maps of the class stream while keeping viewpoint and transformation inputs fixed. The results clearly look chair-like but do not show much variation (the most visible difference is chair vs armchair), which suggests that larger variations are achievable by activating multiple neurons. The last two rows show results of activating neurons of FC-3 and FC4 feature maps. These feature maps contain joint classviewpoint-transformation representations, hence the viewpoint is not fixed anymore. The generated images still re-\nsemble chairs but get much less realistic. This is to be expected: the further away from the inputs, the less semantic meaning there is in the activations. One interesting finding is that there is a \u2019zoom neuron\u2019 in layer FC-4 (middle image in the last row of Figure 7). When its value is increased, the output chair image gets zoomed. This holds not only for the case in which all other activations are zero, but also if the hidden representation contains the information for generating an actual chair, see Figure 8 for an example.\nImages generated from single neurons of the convolutional layers are shown in Figure 9. A somewhat disappointing observation is that while single neurons in later layers\n(uconv-2 and uconv-3) produce edge-like images, the neurons of higher deconvolutional layers generate only blurry \u2019clouds\u2019, as opposed to the results of Zeiler and Fergus [32] with a classification network and max-unpooling. Our explanation is that because we use naive regular-grid unpooling, the network cannot slightly shift small parts to precisely arrange them into larger meaningful structures. Hence it must find another way to generate fine details. In the next subsection we show that this is achieved by a combination of spatially neighboring neurons."}, {"heading": "4.3. Analysis of the hidden layers", "text": "Rather than just activating single neurons while keeping all others fixed to zero, we can use the network to normally generate an image and then analyze the hidden layer activations by either looking at them or modifying them and observing the results. An example of this approach was already used above in Figure 8 to understand the effect of the \u2019zoom neuron\u2019. We present two more results in this direction here, and several more can be found in the supplementary material.\nIn order to find out how the blurry \u2019clouds\u2019 generated by single high-level deconvolutional neurons (Figure 9) form perfectly sharp chair images, we smoothly interpolate between a single activation and the whole chair. Namely, we start with the FC-5 feature maps of a chair, which have a spatial extent of 8 \u00d7 8. Next we only keep active neurons in a region around the center of the feature map (setting all other activations to zero), gradually increasing the size of this region from 2 \u00d7 2 to 8 \u00d7 8. Hence, we can see the effect of going from almost single-neuron activation level to the whole image level. The outcome is shown in Figure 10. Clearly, the interaction of neighboring neurons is very important: in the central region, where many neurons are active, the image is sharp, while in the periphery it is blurry. One interesting effect that is visible in the images is how sharply the legs of the chair end in the second to last image but appear in the larger image. This suggests highly non-linear suppression effects between activations of neighboring neurons.\nLastly some interesting observations can be made by taking a closer look at the feature maps of the uconv-3 layer (the last pre-output layer). Some of them exhibit regular patterns shown in Figure 11. These feature maps correspond to filters which look near-empty in Figure 6 (such as the 3rd and 10th filters in the first row). Our explanation of these patterns is that they compensate high-frequency artifacts originating from fixed filter sizes and regular-grid unpooling. This is supported by the last row of Figure 11 which shows what happens to the generated image when these feature maps are set to zero."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Interpolation between viewpoints", "text": "In this section we show that the generative network is able to generate previously unseen views by interpolating between views present in the training data. This demonstrates that the network internally learns a representation of chairs which enables it to judge about chair similarity and use the known examples to generate previously unseen views.\nIn this experiment we use the 64 \u00d7 64 network to reduce computational costs. We randomly separate the chair styles into two subsets: the \u2019source set\u2019 with 90 % styles and the \u2019target set\u2019 with the remaining 10 % chairs. We then vary the number of viewpoints per style either in both these datasets together (\u2019no transfer\u2019) or just in the target set (\u2019with transfer\u2019) and then train the generative network as before. In the second setup the idea is that the network may use the knowledge about chairs learned from the source set (which includes all viewpoints) to generate the missing viewpoints of the chairs from the target set.\nFigure 12 shows some representative examples of angle\ninterpolation. For 15 views in the target set (first pair of rows) the effect of the knowledge transfer is already visible: interpolation is smoother and fine details are preserved better, for example a leg in the middle column. Starting from 8 views (second pair of rows and below) the network without knowledge transfer fails to produce satisfactory interpolation, while the one with knowledge transfer works reasonably well even with just one view presented during training (bottom pair of rows). However, in this case some fine details, such as the armrest shape, are lost.\nIn Figure 13 we plot the average Euclidean error of the generated missing viewpoints from the target set, both with and without transfer (blue and green curves). Clearly, presence of all viewpoints in the source dataset dramatically improves the performance on the target set, especially for small numbers of available viewpoints.\nOne might suppose (for example looking at the bottom pair of rows of Figure 12) that the network simply learns all the views of the chairs from the source set and then, given a limited number of views of a new chair, finds the most similar one, in some sense, among the known models and simply returns the images of that chair. To check if this is the case, we evaluate the performance of such a naive nearest neighbor approach. For each image in the target set we find the closest match in the source set for each of the given views and interpolate the missing views by linear combinations of the corresponding views of the nearest neighbors. For finding nearest neighbors we try two similarity measures: Euclidean distance between RGB images and between HOG descriptors. The results are shown in Figure 13. Interestingly, although HOG yields semantically much more meaningful nearest neighbors (not shown in figures), RGB similarity performs much better numerically. The performance of this nearest neighbor method is always worse than that of the network, suggesting that the network learns more than just linearly combining the known chairs, especially when many viewpoints are available in the target set."}, {"heading": "5.2. Interpolation between classes", "text": "Remarkably, the generative network can interpolate not only between different viewpoints of the same object, but also between different objects, so that all intermediate images are also meaningful. To obtain such interpolations, we simply linearly change the input label vector from one class to another. Some representative examples of such morphings are shown in Figure 14. The images are sorted by subjective morphing quality (decreasing from top to bottom). The network produces very naturally looking morphings even in challenging cases, such as the first 5 rows.\nIn the last three rows the morphings are qualitatively worse: some of the intermediate samples do not look much like real chairs. However, the result of the last row is quite intriguing as different types of intermediate leg styles are generated. More examples of morphings are shown in the supplementary material."}, {"heading": "5.2.1 Correspondences", "text": "The ability of the generative CNN to interpolate between different chairs allows us to find dense correspondences between different object instances, even if their appearance is very dissimilar.\nGiven two chairs from the training dataset, we use the 128\u00d7 128 network to generate a morphing consisting of 64 images (with fixed view). We then compute the optical flow in the resulting image sequence using the code of Brox et al. [4]. To compensate for the drift, we refine the computed optical flow by recomputing it with a step of 9 frames, initialized by concatenated per-frame flows. Concatenation of these refined optical flows gives the global vector field that connects corresponding points in the two chair images.\nIn order to numerically evaluate the quality of the correspondences, we created a small test set of 30 image pairs (examples are shown in the supplementary material). We\nmanually annotated several keypoints in the first image of each pair (in total 295 keypoints in all images) and asked 9 people to manually mark corresponding points in the second image of each pair. We then used mean keypoint positions in the second images as ground truth. At test time we measured the performance of different methods by computing average displacement of predicted keypoints in the second images given keypoints in the first images. We also manually annotated an additional validation set of 20 image pairs to tune the parameters of all methods (however, we were not able to search the parameters exhaustively because some methods have many).\nIn Table 1 we show the performance of our algorithm compared to human performance and two baselines: SIFT flow [21] and Deformable Spatial Pyramid [15] (DSP). To analyze the performance in more detail, we additionally manually separated the pairs into 10 \u2019simple\u2019 ones (two chairs are quite similar in appearance) and 20 \u2019difficult\u2019 ones (two chairs differ a lot in appearance). On average the very basic approach we used outperforms both baselines thanks to the intermediate samples produced by the generative neural network. More interestingly, while SIFT flow and DSP have problems with the difficult pairs, our algorithm does not. This suggests that errors of our method are largely due to contrast changes and drift in the optical flow, which does not depend on the difficulty of the image pair. The approaches are hence complementary: while for similar objects direct matching is fairly accurate, for more dissimilar ones intermediate morphings are very helpful."}, {"heading": "6. Conclusions", "text": "We have shown that supervised training of convolutional neural network can be used not only for standard discriminative tasks, but also for generating images given highlevel class, viewpoint and lighting information. A network trained for such a generative task does not merely learn to generate the training samples, but also generalizes well, which allows it to smoothly morph different object views or object instances into each other with all intermediate images also being meaningful. It is fascinating that the relatively simple architecture we proposed is already able to learn such complex behavior.\nFrom the technical point of view, it is impressive that the network is able to process very different inputs \u2013 class label, viewpoint and the parameters of additional chromatic and spatial transformations \u2013 using exactly the same standard layers of ReLU neurons. It demonstrates again the wide applicability of convolutional networks."}, {"heading": "1. Analysis of the network", "text": ""}, {"heading": "1.1. Activating single neurons and groups of neurons", "text": "In the main paper we show images generated from single neuron activations in various fully connected layers of the network. However, as pointed out in the main paper, the images generated from individual neurons in each layer look fairly similar, independent of which single neuron is activated. This suggests that 1) the amount of variation may be dependent on the activation strength of a neuron and 2) larger variations can be obtained by activating multiple neurons. Here we experimentally test both these hypotheses for the FC-1 layer.\nFigure 15 shows the variation in the generated depending on the value of the activation of a single FC-1 neuron (one neuron per column, one value per row). These are the same neurons as in Figure 7 of the main paper. The activation values vary, top to bottom, from 2 to 25. In Figure 7 of the main paper the activation value was 10, so it coincides with the 4th row of Figure 15. Clearly, larger activations induce larger changes, but extreme activations lead to images which do not resemble chairs anymore.\nIn Figure 16 we choose two neurons from FC-1 and vary their activations simultaneously between 0 and 9, spanning a 2D grid of generated images. Clearly, activating both neurons simultaneously leads to a combination of the effects of these two neurons.\nFinally, we activate random subsets of several FC-1 neurons. We randomly select a number of neurons and set their activations to the same constant (we selected the value manually depending on the number of activated neurons to obtain the most visually appealing results). The results are shown in Figure 17. Each row corresponds to a different number of randomly selected active neurons, increasing top to bottom. Clearly, more active neurons lead to more variance in chair appearance, and the number of active neurons affects the chair style: chairs with fewer neurons are more \u2019square\u2019 or armchair-like."}, {"heading": "1.2. Analysis of the hidden layers", "text": "In Figure 18 we show some representative feature maps of different layers from the generating streams (FC-5 to uconv-3). For better viewing, the feature maps are modified by cutting 1 percent of the darkest and brightest values. Feature maps uconv-2 and uconv-3 contain some chair-like\ncontours, but FC-5 and uconv-1, which are further from the\noutput image, are rather abstract and difficult to interpret. Similarly to the \u2019zoom neuron\u2019 described in the main paper, there exists a separate single neuron in the layer FC-4 for more or less every artificial transformation we applied during training. The effect of increasing their values given a feature map of a real chair is shown in Figure 19, one neuron per row, activation increasing from left to right.\nIt is very surprising that such complex operation as applying various transformations, especially spatial ones, happens as late as in the last fully connected layer FC-5. This suggests that all potential transformed versions of a chair are already contained in the FC-4 feature maps, and the \u2019transformation neurons\u2019 only modify them to give more relevance to the activations corresponding to the required transformation. To gain some understanding of how this happens, we show weights connected to the \u2019transformation neurons\u2019 in Figure 20. The order of the rows is the same\nas in Figure 19. Each neuron of FC-4 is connected to a 8 \u00d7 8 \u00d7 256 blob in FC-5. We do not show all 256 channels, but rather only the ones which exhibit most interesting and interpretable behavior (the same set of channels for all neurons). Spatial-related and color-related neurons influence different sets of feature maps. Color-related neurons have a roughly constant value over the whole image, while the spatial-related ones often affect two halves of the image differently.\nFinally, to analyze the robustness of the hidden representation, we visualize the effect of setting some of the weakest neuron activations in the layer FC-1 to zero. This is shown in Figure 21. Each column corresponds to a different chair class, each row - to a different ratio of weakest non-zero FC-1 activations set to zero, top to bottom: 0, 0.2, 0.5, 0.75, 0.9, 0.95, 0.98. Up to the ratio 0.5 there are basically no changes, for 0.75 the generated images look slightly deformed, and starting from 0.9 the images look very distorted. This suggests that the hidden representation of layer FC-1 is distributed and reasonably robust."}, {"heading": "2. Interpolation between classes", "text": "We show several more examples of \u2019morphings\u2019 generated by the network in Figure 22. More examples of morphings are shown in the supplementary video CVPR15 Generate Chairs mov morphing.avi. The video shows consecutive morphing of 50 random chair styles one into another."}, {"heading": "2.1. Correspondences", "text": "In Figure 23 we show examples of \u2019simple\u2019 and \u2019difficult\u2019 pairs from the test set. Examples of keypoint tracking using optical flow are shown in the supplementary video CVPR15 Generate Chairs mov tracking.avi. Optical flow does not always track the points perfectly, but in most cases the results look qualitatively good, which is also supported by the numbers from the main paper."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "We train a generative convolutional neural network<lb>which is able to generate images of objects given object<lb>type, viewpoint, and color. We train the network in a su-<lb>pervised manner on a dataset of rendered 3D chair mod-<lb>els. Our experiments show that the network does not merely<lb>learn all images by heart, but rather finds a meaningful<lb>representation of a 3D chair model allowing it to assess<lb>the similarity of different chairs, interpolate between given<lb>viewpoints to generate the missing ones, or invent new chair<lb>styles by interpolating between chairs from the training set.<lb>We show that the network can be used to find correspon-<lb>dences between different chairs from the dataset, outper-<lb>forming existing approaches on this task.", "creator": "LaTeX with hyperref package"}}}