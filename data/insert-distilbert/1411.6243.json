{"id": "1411.6243", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2014", "title": "Structure Regularization for Structured Prediction: Theories and Experiments", "abstract": "while there are many parallel studies on weight regularization, the study on structure regularization is rare. instead many existing systems on structured prediction focus on increasing the level of structural consistency dependencies within the model. however, this slight trend could occasionally have eventually been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. to control structure - based overfitting, we propose a structure regularization framework via \\ emph { structure decomposition }, which decomposes training samples randomly into shorter mini - samples with simpler structures, deriving a model with better generalization power. we might show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead patients to better accuracy. as possessing a by - code product, the proposed quantitative method can also could substantially accelerate growing the training speed. the method and additionally the broader theoretical results derived can apply to general graphical models inconsistent with arbitrary structures. experiments on well - known clinical tasks demonstrate that our method can easily beat efficiently the benchmark systems on those highly - competitive tasks, achieving record - breaking accuracies yet with substantially faster training speed.", "histories": [["v1", "Sun, 23 Nov 2014 14:11:01 GMT  (49kb)", "http://arxiv.org/abs/1411.6243v1", null], ["v2", "Fri, 30 Jan 2015 07:41:21 GMT  (137kb)", "http://arxiv.org/abs/1411.6243v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xu sun"], "accepted": false, "id": "1411.6243"}, "pdf": {"name": "1411.6243.pdf", "metadata": {"source": "CRF", "title": "Structure Regularization for Structured Prediction: Theories and Experiments", "authors": ["Xu Sun"], "emails": ["xusun@pku.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n62 43\nv1 [\ncs .L\nG ]\n2 3\nN ov"}, {"heading": "1 Introduction", "text": "Structured prediction models are popularly used to solve structure dependent problems in a wide variety of application domains including natural language processing, bioinformatics, speech recognition, and computer vision. To solve those problems, many structured prediction methods have been developed, with representative models such as conditional random fields (CRFs), deep neural networks, and structured perceptron models. Recently, in order to more accurately capture structural information, some studies emphasize on intensifying structural dependencies in structured prediction, such as applying long range dependencies among tags and developing long distance features or global features.\nWe argue that over-emphasis on intensive structural dependencies could be misleading, because our study suggests that complex structures are actually harmful to model accuracy. Indeed, while it is obvious that intensive structural dependencies can effectively incorporate structural information, it is less obvious that intensive structural dependencies have a drawback of increasing the generalization risk. Increasing the generalization risk means the trained model tends to overfit the training data, because more complex structures are easier to suffer from overfitting. Formally, our theoretical analysis reveals why and with what degree the structure complexity lowers the generalization ability of trained models. Since this type of overfitting is caused by structure complexity, it can hardly be solved by ordinary regularization methods such as L2 and L1 regularization schemes, which is only for controlling weight complexity.\nTo deal with this problem, we propose a simple structure regularization solution based on tag structure decomposition. The proposed method decomposes each training sample into multiple mini-\nsamples with simpler structures, deriving a model with better generalization power. The proposed method is easy to implement, and it has several interesting properties: (1) We show both theoretically and empirically that the proposed method can effectively reduce the overfitting risk on structured prediction. (2) The proposed method does not change the convexity of the objective function, such that a convex function penalized with a structure regularizer is still convex. This is important for finding global optimum. (3) The proposed method has no conflict with ordinary regularization methods such as L2 and L1 penalties. Thus we can apply structure regularization over an ordinary regularizer to penalize both feature-overfitting and structure-overfitting. We show theoretically and empirically that applying structure regularization over the ordinary regularizer can further reduce the generalization risk in structured prediction. (4) Finally and very interestingly, we show that the proposed method has a by-product of accelerating the rates of convergence in training.\nThe term structural regularization has been used in prior work for regularizing structures of features. For (typically non-structured) classification problems, there are considerable studies on structurerelated regularization, including spectral regularization for modeling feature structures in multi-task learning [1], regularizing feature structures for structural large margin classifiers [27], and many recent studies on structured sparsity. Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9]. Compared with those prior work, we emphasize that our proposal on tag structure regularization is novel. This is because the term structure in all of the aforementioned work refers to structures of feature space, which is substantially different compared with our proposal on regularizing tag structures (interactions among tags).\nAlso, there are some other related studies in different topics. [23] described an interesting heuristic piecewise training method for structured prediction models. [25] described a \u201clookahead\u201d learning method based on structured perceptrons. Our work differs from [23] and [25] mainly because our work is built on a regularization framework, with arguments and theoretical justifications on reducing generalization risk and improving convergence rate. Also, our method and the theoretical results can fit general graphical models with arbitrary structures, and the detailed algorithm is very different. [26] suggested consistent approximation for both training and test phase, but there is no indication on structure regularization. On generalization risk analysis, related studies include [4, 19] on non-structured classification and [24, 12] on structured classification.\nTo the best of our knowledge, this is the first theoretical result on quantifying the relation between structure complexity and the generalization risk in structured prediction, and this is also the first proposal on structure regularization via regularizing tag-interactions. The contributions of this work1 are two-fold:\n\u2022 On the methodology side, we propose a general purpose structure regularization framework for structured prediction. We show both theoretically and empirically that the proposed method can effectively reduce the overfitting risk in structured prediction, and that the proposed method also has an interesting by-product of accelerating the rates of convergence in training. The structure regularization method and the theoretical analysis do not make assumptions or constraints based on specific structures. In other words, the method and the theoretical results can apply to graphical models with arbitrary structures, including linear chains, trees, and general graphs.\n\u2022 On the application side, for several important natural language processing tasks, including part-of-speech tagging, biomedical entity recognition, and word segmentation, our simple method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies as well as substantially faster training speed."}, {"heading": "2 Structure Regularization", "text": "We first describe the proposed structure regularization method, and then give theoretical results on analyzing generalization risk and convergence rates.\n1See the code at http://klcl.pku.edu.cn/member/sunxu/code.htm"}, {"heading": "2.1 Settings", "text": "A graph of observations (even with arbitrary structures) can be indexed and be denoted by using an indexed sequence of observations O = {o1, . . . , on}. We use the term sample to denote O = {o1, . . . , on}. For example, in natural language processing, a sample may correspond to a sentence of n words with dependencies of linear chain structures (e.g., in part-of-speech tagging) or tree structures (e.g., in syntactic parsing). In signal processing, a sample may correspond to a sequence of n signals with dependencies of arbitrary structures. For simplicity in analysis, we assume all samples have n observations (thus n tags). In a typical setting of structured prediction, all the n tags have inter-dependencies via connecting each Markov dependency between neighboring tags. Thus, we call n as tag structure complexity or simply structure complexity below.\nA sample is converted to an indexed sequence of feature vectorsx = {x(1), . . . ,x(n)}, wherex(k) \u2208 X is of the dimension d and corresponds to the local features extracted from the position/index k.2 We can use an n\u00d7d matrix to representx \u2208 Xn. In other words, we use X to denote the input space on a position, so that x is sampled from Xn. Let Yn \u2282 Rn be structured output space, so that the structured output y are sampled from Yn. Let Z = (Xn,Yn) be a unified denotation of structured input and output space. Let z = (x,y), which is sampled from Z , be a unified denotation of a (x,y) pair in the training data.\nSuppose a training set is\nS = {z1 = (x1, y1), . . . , zm = (xm, ym)}, with size m, and the samples are drawn i.i.d. from a distribution D which is unknown. A learning algorithm is a function G : Zm 7\u2192 F with the function space F \u2282 {Xn 7\u2192 Yn}, i.e., G maps a training set S to a function GS : Xn 7\u2192 Yn. We suppose G is symmetric with respect to S, so that G is independent on the order of S.\nStructural dependencies among tags are the major difference between structured prediction and nonstructured classification. For the latter case, a local classification of g based on a position k can be expressed as g(x(k\u2212a), . . . ,x(k+a)), where the term {x(k\u2212a), . . . ,x(k+a)} represents a local window. However, for structured prediction, a local classification on a position depends on the whole input x = {x(1), . . . ,x(n)} rather than a local window, due to the nature of structural dependencies among tags (e.g., graphical models like CRFs). Thus, in structured prediction a local classification on k should be denoted as g(x(1), . . . ,x(n), k). To simplify the notation, we define\ng(x, k) , g(x(1), . . . ,x(n), k)\nGiven a training set S of size m, we define S\\i as a modified training set, which removes the i\u2019th training sample: S\\i = {z1, . . . , z i\u22121, z i+1, . . . , zm}, and we define Si as another modified training set, which replaces the i\u2019th training sample with a new sample z\u0302 i drawn from D:\nSi = {z1, . . . , z i\u22121, z\u0302 i, z i+1, . . . , zm},\nWe define point-wise cost function c : Y\u00d7Y 7\u2192 R+ as c[GS(x, k), y(k)], which measures the cost on a position k by comparing GS(x, k) and the gold-standard tag y(k), and we introduce the point-wise loss as\n\u2113(GS , z, k) , c[GS(x, k), y(k)]\nThen, we define sample-wise cost function C : Yn \u00d7 Yn 7\u2192 R+, which is the cost function with respect to a whole sample, and we introduce the sample-wise loss as\nL(GS , z) , C[GS(x), y ] = n\u2211\nk=1\n\u2113(GS , z, k) =\nn\u2211\nk=1\nc[GS(x, k), y(k)]\n2In most of the existing structured prediction methods, including conditional random fields (CRFs), all the local feature vectors should have the same dimension of features.\nGiven G and a training set S, what we are most interested in is the generalization risk in structured prediction (i.e., expected average loss) [24, 12]:\nR(GS) = Ez [L(GS , z) n ]\nUnless specifically indicated in the context, the probabilities and expectations over random variables, including Ez (.), ES(.), Pz(.), and PS(.), are based on the unknown distribution D.\nSince the distribution D is unknown, we have to estimate R(GS) from S by using the empirical risk:\nRe(GS) = 1\nmn\nm\u2211\ni=1\nL(GS , z i) = 1\nmn\nm\u2211\ni=1\nn\u2211\nk=1\n\u2113(GS , z i, k)\nIn what follows, sometimes we will use simplified notations, R and Re, to denote R(GS) and Re(GS).\nTo state our theoretical results, we must describe several quantities and assumptions which are important in structured prediction. We follow some notations and assumptions on non-structured classification [4, 19]. We assume a simple real-valued structured prediction scheme such that the class predicted on position k of x is the sign of GS(x, k) \u2208 D.3 Also, we assume the point-wise cost function c\u03c4 is convex and \u03c4 -smooth such that \u2200y1, y2 \u2208 D, \u2200y\u2217 \u2208 Y\n|c\u03c4 (y1, y\u2217)\u2212 c\u03c4 (y2, y\u2217)| \u2264 \u03c4 |y1 \u2212 y2| (1)\nThen, \u03c4 -smooth versions of the loss and the cost function can be derived according to their prior definitions:\nL\u03c4 (GS , z) = C\u03c4 [GS(x), y ] = n\u2211\nk=1\n\u2113\u03c4 (GS , z, k) =\nn\u2211\nk=1\nc\u03c4 [GS(x, k), y(k)]\nAlso, we use a value \u03c1 to quantify the bound of |GS(x, k) \u2212 GS\\i(x, k)| while changing a single sample (with size n\u2032 \u2264 n) in the training set with respect to the structured inputx. This \u03c1-admissible assumption can be formulated as \u2200k,\n|GS(x, k)\u2212GS\\i(x, k)| \u2264 \u03c1||GS \u2212GS\\i ||2 \u00b7 ||x||2 (2) where \u03c1 \u2208 R+ is a value related to the design of algorithm G."}, {"heading": "2.2 Structure Regularization", "text": "Most existing regularization techniques are for regularizing model weights/parameters (e.g., a representative regularizer is the Gaussian regularizer or so called L2 regularizer), and we call such regularization techniques as weight regularization.\n3In practice, many popular structured prediction models have a convex and real-valued cost function (e.g., CRFs).\nAlgorithm 1 Training with structure regularization 1: Input: model weights w, training set S, structure regularization strength \u03b1 2: repeat 3: S\u2032 \u2190 \u2205 4: for i = 1 \u2192 m do 5: Randomly decompose z i \u2208 S into mini-samples N\u03b1(z i) = {z(i,1), . . . , z(i,\u03b1)} 6: S\u2032 \u2190 S\u2032 \u222aN\u03b1(z i) 7: end for 8: for i = 1 \u2192 |S\u2032| do 9: Sample z \u2032 uniformly at random from S\u2032, with gradient \u2207gz\u2032(w) 10: w \u2190 w \u2212 \u03b7\u2207gz\u2032(w) 11: end for 12: until Convergence 13: return w\nDefinition 1 (Weight regularization) Let N\u03bb : F 7\u2192 R+ be a weight regularization function on F with regularization strength \u03bb, the structured classification based objective function with general weight regularization is as follows:\nR\u03bb(GS) , Re(GS) +N\u03bb(GS) (3)\nWhile weight regularization is normalizing model weights, the proposed structure regularization method is normalizing the structural complexity of the training samples. As illustrated in Figure 1, our proposal is based on tag structure decomposition, which can be formally defined as follows:\nDefinition 2 (Structure regularization) Let N\u03b1 : F 7\u2192 F be a structure regularization function on F with regularization strength \u03b1 with 1 \u2264 \u03b1 \u2264 n, the structured classification based objective function with structure regularization is as follows4:\nR\u03b1(GS) , Re[GN\u03b1(S)] = 1\nmn\nm\u2211\ni=1\n\u03b1\u2211\nj=1\nL[GS\u2032 , z(i,j)] = 1\nmn\nm\u2211\ni=1\n\u03b1\u2211\nj=1\nn/\u03b1 \u2211\nk=1\n\u2113[GS\u2032 , z(i,j), k] (4)\nwhere N\u03b1(z i) randomly splits z i into \u03b1 mini-samples {z(i,1), . . . , z(i,\u03b1)}, so that the mini-samples have a distribution on their sizes (structure complexities) with the expected value n\u2032 = n/\u03b1. Thus, we get\nS\u2032 = {z(1,1), z(1,2), . . . , z(1,\u03b1) \ufe38 \ufe37\ufe37 \ufe38\n\u03b1\n, . . . , z(m,1), z(m,2), . . . , z(m,\u03b1) \ufe38 \ufe37\ufe37 \ufe38\n\u03b1\n} (5)\nwith m\u03b1 mini-samples with expected structure complexity n/\u03b1. We can denote S\u2032 more compactly as S\u2032 = {z \u20321, z \u20322, . . . , z \u2032m\u03b1} and R\u03b1(GS) can be simplified as\nR\u03b1(GS) , 1\nmn\nm\u03b1\u2211\ni=1\nL(GS\u2032 , z \u2032i) = 1\nmn\nm\u03b1\u2211\ni=1\nn/\u03b1 \u2211\nk=1\n\u2113[GS\u2032 , z \u2032 i, k] (6)\nNote that, when the structure regularization strength \u03b1 = 1, we have S\u2032 = S and R\u03b1 = Re. The structure regularization algorithm (with the stochastic gradient descent setting) is summarized in Algorithm 1.\nSince we know z = (x,y), the decomposition of z simply means the decomposition of x and y . Recall that x = {x(1), . . . ,x(n)} is an indexed sequence of the feature vectors, not the observations O = {o1, . . . , on}. Thus, it should be emphasized that the decomposition of x is the decomposition of the feature vectors, not the original observations. Actually the decomposition of the feature vectors is more convenient and has no information loss \u2014 no need to regenerate features. On the other hand, decomposing observations needs to regenerate features and may lose some features.\n4The notation N is overloaded here. For clarity throughout, N with subscript \u03bb refers to weight regularization function, and N with subscript \u03b1 refers to structure regularization function.\nThe structure regularization has no conflict with the weight regularization, and the structure regularization can be applied together with the weight regularization. Actually we will show that applying the structure regularization over the weight regularization can further improve stability and reduce generalization risk.\nDefinition 3 (Structure & weight regularization) By combining structure regularization in Definition 2 and weight regularization in Definition 1, the structured classification based objective function is as follows:\nR\u03b1,\u03bb(GS) , R\u03b1(GS) +N\u03bb(GS) (7) When \u03b1 = 1, we have R\u03b1,\u03bb = Re(GS) +N\u03bb(GS) = R\u03bb.\nLike existing weight regularization methods, currently our structure regularization is only for the training stage. Currently we do not use structure regularization in the test stage."}, {"heading": "2.3 Stability of Structured Prediction", "text": "In contrast to the simplicity of the algorithm, the theoretical analysis is quite technical. First, we analyze the stability of structured prediction.\nDefinition 4 (Function stability) A real-valued structured classification algorithmG has \u201cfunction value based stability\u201d (\u201cfunction stability\u201d for short) \u2206 if the following holds: \u2200z = (x,y) \u2208 Z, \u2200S \u2208 Zm, \u2200i \u2208 {1, . . . ,m}, \u2200k \u2208 {1, . . . , n},\n|GS(x, k)\u2212GS\\i(x, k)| \u2264 \u2206\nDefinition 5 (Loss stability) A structured classification algorithm G has \u201cuniform loss-based stability\u201d (\u201closs stability\u201d for short) \u2206l if the following holds: \u2200z \u2208 Z, \u2200S \u2208 Zm, \u2200i \u2208 {1, . . . ,m}, \u2200k \u2208 {1, . . . , n},\n|\u2113(GS , z, k)\u2212 \u2113(GS\\i , z, k)| \u2264 \u2206l\nG has \u201csample-wise uniform loss-based stability\u201d (\u201csample loss stability\u201d for short) \u2206s with respect to the loss function L if the following holds: \u2200z \u2208 Z, \u2200S \u2208 Zm, \u2200i \u2208 {1, . . . ,m},\n|L(GS , z)\u2212 L(GS\\i , z)| \u2264 \u2206s Lemma 6 (Loss stability vs. function stability) If a real-valued structured classification algorithm G has function stability \u2206 with respect to loss function \u2113\u03c4 , then G has loss stability \u03c4\u2206 and sample loss stability n\u03c4\u2206.\nThe proof is in Section 4.\nHere, we show that our structure regularizer can further improve stability (thus reduce generalization risk) over a model which already equipped with a weight regularizer.\nTheorem 7 (Stability vs. structure regularization) With a training set S of size m, let the learning algorithm G have the minimizer f based on commonly used L2 weight regularization:\nf = argmin g\u2208F R\u03b1,\u03bb(g) = argmin g\u2208F\n( 1\nmn\nm\u03b1\u2211\nj=1\nL\u03c4 (g,z \u2032j) + \u03bb\n2 ||g||22\n)\n(8)\nwhere \u03b1 denotes structure regularization strength with 1 \u2264 \u03b1 \u2264 n. Also, we have\nf\\i \u2032\n= argmin g\u2208F\nR \\i\u2032 \u03b1,\u03bb(g) = argmin\ng\u2208F\n( 1\nmn\n\u2211 j 6=i\u2032 L\u03c4 (g,z \u2032j) + \u03bb 2 ||g||22\n)\n(9)\nwhere j 6= i\u2032 means j \u2208 {1, . . . , i\u2032 \u2212 1, i\u2032 + 1, . . . ,m\u03b1}.5 Assume L\u03c4 is convex and differentiable, and f(x, k) is \u03c1-admissible. Let a local feature value is bounded by v such that x(k,q) \u2264 v for\n5Note that, in some cases the notation i is ambiguous. For example, f\\i can either denote the removing of a sample in S or denote the removing of a mini-sample in S\u2032. Thus, when the case is ambiguous, we use different index symbols for S and S\u2032, with i for indexing S and i\u2032 for indexing S\u2032, respectively.\nq \u2208 {1, . . . , d}.6 Let \u2206 denote the function stability of f comparing with f\\i\u2032 for \u2200z \u2208 Z with |z | = n. Then, \u2206 is bounded by\n\u2206 \u2264 d\u03c4\u03c1 2v2n2\nm\u03bb\u03b12 , (10)\nand the corresponding loss stability is bounded by d\u03c4 2\u03c12v2n2\nm\u03bb\u03b12 , and the corresponding sample loss\nstability is bounded by d\u03c4 2\u03c12v2n3\nm\u03bb\u03b12 .\nThe proof is in Section 4.\nWe can see that increasing the size of training set m results in linear improvement of \u2206, and increasing the strength of structure regularization \u03b1 results in quadratic improvement of \u2206.\nThe function stability \u2206 is based on comparing f and f\\i \u2032\n, i.e., the stability is based on removing a mini-sample. Moreover, we can extend the analysis to the function stability based on comparing f and f\\i, i.e., the stability is based on removing a full-size sample.\nCorollary 8 (Stability based on \\i rather than \\i\u2032) With a training set S of size m, let the learning algorithm G have the minimizer f as defined like before. Also, we have\nf\\i = argmin g\u2208F R \\i \u03b1,\u03bb(g) = argmin g\u2208F\n( 1\nmn\n\u2211 j /\u2208i L\u03c4 (g,z \u2032j) + \u03bb 2 ||g||22\n)\n(11)\nwhere j /\u2208 i means j \u2208 {1, . . . , (i \u2212 1)\u03b1, i\u03b1 + 1, . . . ,m\u03b1}, i.e., all the mini-samples derived from the sample z i are removed. Assume L\u03c4 is convex and differentiable, and f(x, k) is \u03c1-admissible. Let a local feature value is bounded by v such that x(k,q) \u2264 v for q \u2208 {1, . . . , d}. Let \u2206\u0304 denote the function stability of f comparing with f\\i for \u2200z \u2208 Z with |z | = n. Then, \u2206\u0304 is bounded by\n\u2206\u0304 \u2264 d\u03c4\u03c1 2v2n2\nm\u03bb\u03b1 = \u03b1 sup(\u2206), (12)\nwhere \u2206 is the function stability of f comparing with f\\i \u2032 , and sup(\u2206) = d\u03c4\u03c1 2v2n2\nm\u03bb\u03b12 , as described in Eq. (10).\nThe proof is in Section 4."}, {"heading": "2.4 Reduction of Generalization Risk", "text": "Theorem 9 (Generalization vs. stability) Let G be a real-valued structured classification algorithm with a point-wise loss function \u2113\u03c4 such that \u2200k, 0 \u2264 \u2113\u03c4 (GS , z, k) \u2264 \u03b3. Let f , \u2206, and \u2206\u0304 be defined like before. Let R(f) be the generalization risk of f based on the expected sample z \u2208 Z with size n, as defined like before. Let Re(f) be the empirical risk of f based on S, as defined like before. Then, for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4 over the random draw of the training set S, the generalization risk R(f) is bounded by\nR(f) \u2264 Re(f) + 2\u03c4\u2206\u0304 + ( (4m\u2212 2)\u03c4\u2206+ \u03b3 ) \u221a \u03b1 ln \u03b4\u22121\n2m (13)\nThe proof is in Section 4.\nTheorem 10 (Generalization vs. structure regularization) Let the structured prediction objective function of G be penalized by structure regularization with factor \u03b1 \u2208 [1, n] and L2 weight regularization with factor \u03bb, and the penalized function has a minimizer f :\nf = argmin g\u2208F R\u03b1,\u03bb(g) = argmin g\u2208F\n( 1\nmn\nm\u03b1\u2211\nj=1\nL\u03c4 (g,z \u2032j) + \u03bb\n2 ||g||22\n)\n(14)\nAssume the point-wise loss \u2113\u03c4 is convex and differentiable, and is bounded by \u2113\u03c4 (f,z, k) \u2264 \u03b3. Assume f(x, k) is \u03c1-admissible. Let a local feature value be bounded by v such that x(k,q) \u2264 v for\n6Recall that d is the dimension of local feature vectors defined in Section 2.1.\nq \u2208 {1, . . . , d}. Then, for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4 over the random draw of the training set S, the generalization risk R(f) is bounded by\nR(f) \u2264 Re(f) + 2d\u03c42\u03c12v2n2 m\u03bb\u03b1 + ((4m\u2212 2)d\u03c42\u03c12v2n2 m\u03bb\u03b12 + \u03b3 ) \u221a \u03b1 ln \u03b4\u22121 2m (15)\nSince \u03c4, \u03c1, and v are typically small compared with other variables, especially m, (15) can be approximated as follows by ignoring small terms:\nR(f) \u2264 Re(f) +O (dn2\n\u221a ln \u03b4\u22121\n\u03bb\u03b11.5 \u221a m\n)\n(16)\nProof According to (10) and (12), we have \u2206 \u2264 d\u03c4\u03c12v2n2m\u03bb\u03b12 and \u2206\u0304 \u2264 d\u03c4\u03c12v2n2\nm\u03bb\u03b1 . Inserting those bounds into (13) gives (15). \u2293\u2294\nWe call the term O ( dn2 \u221a ln \u03b4\u22121\n\u03bb\u03b11.5 \u221a m\n)\nin (16) as \u201coverfit-bound\u201d, and reducing the overfit-bound is cru-\ncial for reducing the generalization risk bound. First, (16) suggests that structure complexity n can increase the overfit-bound on a magnitude of O(n2), and applying weight regularization can reduce the overfit-bound by O(\u03bb). Importantly, applying structure regularization further (over weight regularization) can additionally reduce the overfit-bound by a magnitude of O(\u03b11.5). When \u03b1 = 1, it means \u201cno structure regularization\u201d, then we have the worst overfit-bound O ( dn2 \u221a ln \u03b4\u22121\n\u03bb \u221a m\n)\n. Also,\n(16) suggests that increasing the size of training set can reduce the overfit-bound on a square root level.\nActually, the generalization bound in Theorem 10 is based on an arguably over-strict assumption of completely dense features. Since many applications in practice are based on sparse features, this completely dense feature assumption can be relaxed, which can further improve the generalization bound (i.e., with a tighter overfit-bound).\nCorollary 11 (Generalization vs. moderate feature sparsity) Let f be defined like before. Assume the point-wise loss \u2113\u03c4 is convex and differentiable, and is bounded by \u2113\u03c4 (f,z, k) \u2264 \u03b3. Assume f(x, k) is \u03c1-admissible. Let a local feature value be bounded by v such that x(k,q) \u2264 v for q \u2208 {1, . . . , d}. If we assume the features are \u201cmoderately sparse\u201d such that the global feature vector\n\u2211|x| k=1 x(k,q) \u2264 v\u03b2\n\u221a\n|x| for q \u2208 {1, . . . , d} (with \u03b2 being a sparsity related scalar), then for any \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4 over the random draw of the training set S, the generalization risk R(f) is bounded by\nR(f) \u2264 Re(f) + 2d\u03c42\u03c12v2\u03b22n\nm\u03bb \u221a \u03b1\n+ ((4m\u2212 2)d\u03c42\u03c12v2\u03b22n m\u03bb\u03b11.5 + \u03b3 ) \u221a \u03b1 ln \u03b4\u22121 2m (17)\nIt can be approximated as follows by ignoring small terms:\nR(f) \u2264 Re(f) +O (dn\n\u221a ln \u03b4\u22121\n\u03bb\u03b1 \u221a m\n)\n(18)\nThe proof is in Section 4.\nCorollary 12 (Generalization vs. extreme feature sparsity) Let f be defined like before. Assume the point-wise loss \u2113\u03c4 is convex and differentiable, and is bounded by \u2113\u03c4 (f,z, k) \u2264 \u03b3. Assume f(x, k) is \u03c1-admissible. Let a local feature value be bounded by v such that x(k,q) \u2264 v for q \u2208 {1, . . . , d}. If we assume the features are \u201cextremely sparse\u201d such that the global feature vector \u2211|x|\nk=1 x(k,q) \u2264 v\u03b2 for q \u2208 {1, . . . , d} (with \u03b2 being a sparsity related scalar), then for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4 over the random draw of the training set S, the generalization risk R(f) is bounded by\nR(f) \u2264 Re(f) + 2d\u03c42\u03c12v2\u03b22 m\u03bb + ((4m\u2212 2)d\u03c42\u03c12v2\u03b22 m\u03bb\u03b1 + \u03b3 ) \u221a \u03b1 ln \u03b4\u22121 2m (19)\nIt can be approximated as follows by ignoring small terms:\nR(f) \u2264 Re(f) +O (d\n\u221a ln \u03b4\u22121 \u03bb \u221a \u03b1m )\n(20)\nThe proof is in Section 4."}, {"heading": "2.5 Accelerating Convergence Rates in Training", "text": "We also analyze the impact on the convergence rate of online learning by applying structure regularization. Our analysis is based on the stochastic gradient descent (SGD) setting [3, 11, 15], which is arguably the most representative online training setting. Let g(w) be the structured prediction objective function and w \u2208 W is the weight vector. Recall that the SGD update with fixed learning rate \u03b7 has a form like this: wt+1 \u2190 wt \u2212 \u03b7\u2207gzt(wt) (21) where gz(wt) is the stochastic estimation of the objective function based on z which is randomly drawn from S.\nTo state our convergence rate analysis results, we need several assumptions following (Nemirovski et al. 2009). We assume g is strongly convex with modulus c, that is, \u2200w,w \u2032 \u2208 W ,\ng(w\u2032) \u2265 g(w) + (w \u2032 \u2212w)T\u2207g(w) + c 2 ||w\u2032 \u2212w||2 (22)\nWhen g is strongly convex, there is a global optimum/minimizer w\u2217. We also assume Lipschitz continuous differentiability of g with the constant q, that is, \u2200w,w\u2032 \u2208 W ,\n||\u2207g(w \u2032)\u2212\u2207g(w)|| \u2264 q||w\u2032 \u2212w|| (23) It is also reasonable to assume that the norm of \u2207gz(w) has almost surely positive correlation with the structure complexity of z ,7 which can be quantified by a bound \u03ba \u2208 R+:\n||\u2207gz(w)||2 \u2264 \u03ba|z | almost surely for \u2200w \u2208 W (24) where |z | denotes the structure complexity of z . Moreover, it is reasonable to assume\n\u03b7c < 1 (25)\nbecause even the ordinary gradient descent methods will diverge if \u03b7c > 1.\nThen, we show that structure regularization can quadratically accelerate the SGD rates of convergence:\nProposition 13 (Convergence rates vs. structure regularization) With the aforementioned assumptions, let the SGD training have a learning rate defined as \u03b7 = c\u01eb\u03b2\u03b1 2\nq\u03ba2n2 , where \u01eb > 0 is a convergence tolerance value and \u03b2 \u2208 (0, 1]. Let t be a integer satisfying\nt \u2265 q\u03ba 2n2 log (qa0/\u01eb)\n\u01eb\u03b2c2\u03b12 (26)\nwhere n and\u03b1 \u2208 [1, n] is like before, and a0 is the initial distance which depends on the initialization of the weights w0 and the minimizer w\u2217, i.e., a0 = ||w0 \u2212 w\u2217||2. Then, after t updates of w it converges to E[g(wt)\u2212 g(w\u2217)] \u2264 \u01eb.\nThe proof is in Section 4.\nThis Proposition demonstrates the 1/t convergence rate with t given in (26). Recall that when \u03b1 = 1, the algorithm with structure regularization reduces exactly to the ordinary algorithm (without structure regularization), which has the number of SGD updates t \u2265 q\u03ba 2n2 log (qa0/\u01eb)\n\u01eb\u03b2c2 to achieve the convergence tolerance value \u01eb. In other words, applying structure regularization with the strength \u03b1 can quadratically accelerate the convergence rate with a factor of \u03b12.\n7Many structured prediction systems (e.g., CRFs) satisfy this assumption that the gradient based on a larger sample (i.e., n is large) is expected to have a larger norm."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Tasks", "text": "Diversified Tasks. We experiment on natural language processing tasks and signal processing tasks. The natural language processing tasks include (1) part-of-speech tagging, (2) biomedical named entity recognition, and (3) Chinese word segmentation. The signal processing task is (4) sensor-based human activity recognition. The tasks (1) to (3) use boolean features and the task (4) adopts realvalued features. From tasks (1) to (4), the averaged structure complexity (number of observations) n is very different, with n = 23.9, 26.5, 46.6, 67.9, respectively. The dimension of tags |Y| is also diversified among tasks, with |Y| ranging from 5 to 45. Part-of-Speech Tagging (POS-Tagging). Part-of-Speech (POS) tagging is an important and highly competitive task in natural language processing. We use the standard benchmark dataset in prior work [5], which is derived from PennTreeBank corpus and uses sections 0 to 18 of the Wall Street Journal (WSJ) for training (38,219 samples), and sections 22-24 for testing (5,462 samples). Following prior work [25], we use features based on unigrams and bigrams of neighboring words, and lexical patterns of the current word, with 393,741 raw features8 in total. Following prior work, the evaluation metric for this task is per-word accuracy.\nBiomedical Named Entity Recognition (Bio-NER). This task is from the BioNLP-2004 shared task, which is for recognizing 5 kinds of biomedical named entities (DNA, RNA, etc.) on the MEDLINE biomedical text corpus. There are 17,484 training samples and 3,856 test samples. Following prior work [25], we use word pattern features and POS features, with 403,192 raw features in total. The evaluation metric is balanced F-score.\nWord Segmentation (Word-Seg). Chinese word segmentation is important and it is usually the first step for text processing in Chinese. We use the Microsoft Research (MSR) data provided by SIGHAN-2004 contest. There are 86,918 training samples and 3,985 test samples. Following prior work [7], we use features based on character unigrams and bigrams, with 1,985,720 raw features in total. The evaluation metric for this task is balanced F-score.\nSensor-based Human Activity Recognition (Act-Recog). This is a task based on real-valued sensor signals, with the data extracted from the Bao04 activity recognition dataset [21]. This task aims to recognize human activities (walking, bicycling, etc.) by using 5 biaxial sensors to collect acceleration signals of individuals, with the sampling frequency at 76.25HZ. Following prior work in activity recognition [21], we use acceleration features, mean features, standard deviation, energy, and correlation features, with 1228 raw features in total. There are 16,000 training samples and 4,000 test samples. Following prior work, the evaluation metric is accuracy."}, {"heading": "3.2 Experimental Settings", "text": "To test the robustness of the proposed structure regularization (StructReg) method, we perform experiments on both probabilistic and non-probabilistic structure prediction models. We choose the conditional random fields (CRFs) [10] and structured perceptrons (Perc) [5], which are arguably the most popular probabilistic and non-probabilistic structured prediction models, respectively. The CRFs are trained using the SGD algorithm,9 and the baseline method is the traditional weight regularization scheme (WeightReg), which adopts the most representative L2 weight regularization, i.e., a Gaussian prior.10 For the structured perceptrons, the baseline WeightAvg is the popular implicit regularization technique based on parameter averaging, i.e., averaged perceptron [5].\nAll methods use the same set of features. Since the rich edge features [22] can be automatically generated from raw features and are very useful for improving model accuracy, the rich edge features are employed for all methods. All methods are based on the 1st-order Markov dependency. For\n8Raw features are those observation features based only on x, i.e., no combination with tag information. 9In theoretical analysis, following prior work we adopt the SGD with fixed learning rate, as described in Section 2.5. However, since the SGD with decaying learning rate is more commonly used in practice, in experiments we use the SGD with decaying learning rate.\n10We also tested on sparsity emphasized regularization methods, including L1 regularization and Group Lasso regularization [13]. However, we find that in most cases those sparsity emphasized regularization methods have lower accuracy than the L2 regularization.\nWeightReg, the L2 regularization strengths (i.e., \u03bb/2 in Eq.8) are tuned among values 0.1, 0.5, 1, 2, 5, and are determined on the development data provided by the standard dataset (POS-Tagging) or simply via 4-fold cross validation on the training set (Bio-NER, Word-Seg, and Act-Recog). With this automatic tuning for WeightReg, we set 2, 5, 1 and 5 for POS-Tagging, Bio-NER, Word-Seg, and Act-Recog tasks, respectively. Our StructReg method adopts the same L2 regularization setting like WeightReg. Experiments are performed on a computer with Intel(R) Xeon(R) 3.0GHz CPU."}, {"heading": "3.3 Experimental Results", "text": "The experimental results in terms of accuracy/F-score are shown in Figure 2. For the CRF model, the training is convergent, and the results on the convergence state (decided by relative objective change with the threshold value of 0.0001) are shown. For the structured perceptron model, the training is typically not convergent, and the results on the 10\u2019th iteration are shown. For stability of the curves, the results of the structured perceptrons are averaged over 10 repeated runs.\nSince different samples have different size n in practice, we set \u03b1 being a function of n, so that the generated mini-samples are with fixed size n\u2032 with n\u2032 = n/\u03b1. Actually, n\u2032 is a probabilistic distribution because we adopt randomized decomposition. For example, if n\u2032 = 5.5, it means the minisamples are a mixture of the ones with the size 5 and the ones with the size 6, and the mean of the size distribution is 5.5. In the figure, the curves are based on n\u2032 = 1.5, 2.5, 3.5, 5.5, 10.5, 15.5, 20.5.\nAs we can see, although the experiments are based on very different models (probabilistic or nonprobabilistic), with diversified feature types (boolean or real-value) and different structure complexity n, the results are quite consistent. It demonstrates that structure regularization leads to higher accuracies/F-scores compared with the existing baselines.\nWe also conduct significance tests based on t-test. Since the t-test for F-score based tasks (BioNER and Word-Seg) may be unreliable11, we only perform t-test for the accuracy-based tasks, i.e., POS-Tagging and Act-Recog. For POS-Tagging, the significance test suggests that the superiority\n11Indeed we can convert F-scores to accuracy scores for t-test, but in many cases this conversion is unreliable. For example, very different F-scores may correspond to similar accuracy scores.\nof StructReg over WeightReg is very statistically significant, with p < 0.01. For Act-Recog, the significance tests suggest that both the StructReg vs. WeightReg difference and the StructReg vs. WeightAvg difference are extremely statistically significant, with p < 0.0001 in both cases. The experimental results support our theoretical analysis that structure regularization can further reduce the generalization risk over existing weight regularization techniques.\nOur method actually outperforms the benchmark systems on the three important natural language processing tasks. The POS-Tagging task is a highly competitive task, with many methods proposed, and the best report (without using extra resources) until now is achieved by using a bidirectional learning model in [20],12 with the accuracy 97.33%. Our simple method achieves better accuracy compared with all of those state-of-the-art systems. Furthermore, our method achieves as good scores as the benchmark systems on the Bio-NER and Word-Seg tasks, which are also very competitive tasks in natural language processing communities. On the Bio-NER task, [25] achieves 72.28% based on lookahead learning and [28] achieves 72.65% based on reranking. On the Word-Seg task, [7] achieves 97.19% based on maximum entropy classification and our recent work [22] achieves 97.5% based on feature-frequency-adaptive online learning. The comparisons are summarized in Table 1. Note that, similar to the tuning on the WeightReg strengths, the optimal values of StructReg strengths are also decided automatically based on standard development data or cross validation on training data.\nFigure 3 shows experimental comparisons in terms of wall-clock training time. As we can see, the proposed method can substantially improve the training speed. The speedup is not only from the faster convergence rates, but also from the faster processing time on the structures, because it is more efficient to process the decomposed samples with simple structures."}, {"heading": "4 Proofs", "text": "Our analysis sometimes need to use McDiarmid\u2019s inequality.\nTheorem 14 (McDiarmid, 1989) Let S = {q1, . . . , qm} be independent random variables taking values in the space Qm. Moreover, let g : Qm 7\u2192 R be a function of S that satisfies \u2200i, \u2200S \u2208 Qm, \u2200q\u0302i \u2208 Q,\n|g(S)\u2212 g(Si)| \u2264 ci.\n12See a collection of the systems at http://aclweb.org/aclwiki/index.php?title=POS_Tagging_(State_of_the_art)\nThen \u2200\u01eb > 0, PS [g(S)\u2212 ES [g(S)] \u2265 \u01eb] \u2264 exp ( \u22122\u01eb2 \u2211m\ni=1 c 2 i\n)\n.\nLemma 15 (Symmetric learning) For any symmetric (i.e., order-free) learning algorithm G, \u2200i \u2208 {1, . . . ,m}, we have\nES [R(GS)\u2212Re(GS)] = 1\nn ES,z\u0302i [L(GS , z\u0302 i)\u2212 L(GSi , z\u0302 i)]"}, {"heading": "Proof", "text": "ES [R(GS)\u2212Re(GS)] = 1\nn ES\n(\nEz(L(GS , z))\u2212 1\nm\nm\u2211\nj=1\nL(GS , zj) )\n= 1\nn\n(\nES,z\u0302i ( L(GS , z\u0302 i) ) \u2212 1\nm\nm\u2211\nj=1\nES ( L(GS , zj)\n))\n= 1\nn\n(\nES,z\u0302i ( L(GS , z\u0302 i) ) \u2212 ES ( L(GS , z i)\n))\n= 1\nn\n(\nES,z\u0302i ( L(GS , z\u0302 i) ) \u2212 ESi ( L(GSi , z\u0302 i)\n))\n= 1\nn ES,z\u0302i\n( L(GS , z\u0302 i)\u2212 L(GSi , z\u0302 i) )\nwhere the 3rd step is based on ESL(GS , z i) = ESL(GS , zj) for \u2200z i \u2208 S and \u2200zj \u2208 S, given that G is symmetric. \u2293\u2294"}, {"heading": "4.1 Proofs", "text": ""}, {"heading": "Proof of Lemma 6", "text": "According to (1), we have \u2200i, \u2200S, \u2200z, \u2200k |\u2113\u03c4 (GS , z, k)\u2212 \u2113\u03c4 (GS\\i , z , k)| = |c\u03c4 [GS(x, k), y(k)]\u2212 c\u03c4 [GS\\i(x, k), y(k)]|\n\u2264 \u03c4 |GS(x, k)\u2212GS\\i(x, k)| \u2264 \u03c4\u2206\nThis gives the bound of loss stability.\nAlso, we have \u2200i, \u2200S, \u2200z\n|L\u03c4 (GS , z)\u2212 L\u03c4 (GS\\i , z)| = \u2223 \u2223 \u2223 n\u2211\nk=1\nc\u03c4 [GS(x, k), y(k)]\u2212 n\u2211\nk=1\nc\u03c4 [GS\\i(x, k), y(k)] \u2223 \u2223 \u2223\n\u2264 n\u2211\nk=1\n\u2223 \u2223 \u2223c\u03c4 [GS(x, k), y(k)]\u2212 c\u03c4 [GS\\i(x, k), y(k)] \u2223 \u2223 \u2223\n\u2264 \u03c4 n\u2211\nk=1\n|GS(x, k)\u2212GS\\i(x, k)|\n\u2264 n\u03c4\u2206 This derives the bound of sample loss stability. \u2293\u2294"}, {"heading": "Proof of Theorem 7", "text": "When a convex and differentiable function g has a minimum f in space F , its Bregman divergence has the following property for \u2200f \u2032 \u2208 F :\ndg(f \u2032, f) = g(f \u2032)\u2212 g(f)\nWith this property, we have\ndR\u03b1,\u03bb(f \\i\u2032 , f) + d\nR \\i\u2032\n\u03b1,\u03bb\n(f, f\\i \u2032 ) = R\u03b1,\u03bb(f \\i\u2032)\u2212R\u03b1,\u03bb(f) +R\\i \u2032 \u03b1,\u03bb(f)\u2212R \\i\u2032 \u03b1,\u03bb(f \\i\u2032)\n= ( R\u03b1,\u03bb(f \\i\u2032)\u2212R\\i \u2032 \u03b1,\u03bb(f \\i\u2032) ) \u2212 ( R\u03b1,\u03bb(f)\u2212R\\i \u2032 \u03b1,\u03bb(f) )\n= 1 mn L\u03c4 (f\\i \u2032 , z \u2032i\u2032)\u2212 1 mn L\u03c4 (f,z \u2032i\u2032)\n(27)\nThen, based on the property of Bregman divergence that dg+g\u2032 = dg + dg\u2032 , we have\ndN\u03bb(f, f \\i\u2032) + dN\u03bb(f \\i\u2032 , f) = d (R \\i\u2032\n\u03b1,\u03bb \u2212R\\i\u2032\u03b1 )\n(f, f\\i \u2032 ) + d(R\u03b1,\u03bb\u2212R\u03b1)(f \\i\u2032 , f)\n= dR\u03b1,\u03bb(f \\i\u2032 , f) + d\nR \\i\u2032\n\u03b1,\u03bb\n(f, f\\i \u2032 )\u2212 dR\u03b1(f\\i \u2032 , f)\u2212 d R \\i\u2032 \u03b1 (f, f\\i \u2032 )\n(based on non-negativity of Bregman divergence)\n\u2264 dR\u03b1,\u03bb(f\\i \u2032\n, f) + d R \\i\u2032\n\u03b1,\u03bb\n(f, f\\i \u2032 )\n(using (27))\n= 1\nmn\n( L\u03c4 (f\\i \u2032 , z \u2032i\u2032)\u2212 L\u03c4 (f,z \u2032i\u2032) )\n= 1\nmn\nn/\u03b1 \u2211\nk=1\n( \u2113\u03c4 (f \\i\u2032 , z \u2032i\u2032 , k)\u2212 \u2113\u03c4 (f,z \u2032i\u2032 , k) )\n\u2264 1 mn\nn/\u03b1 \u2211\nk=1\n\u2223 \u2223 \u2223c\u03c4 ( f\\i \u2032 (x\u2032i\u2032 , k), y \u2032 i\u2032(k) ) \u2212 c\u03c4 ( f(x\u2032i\u2032 , k), y \u2032 i\u2032(k) )\u2223 \u2223 \u2223\n\u2264 \u03c4 mn\nn/\u03b1 \u2211\nk=1\n\u2223 \u2223 \u2223f\\i \u2032 (x\u2032i\u2032 , k)\u2212 f(x\u2032i\u2032 , k) \u2223 \u2223 \u2223\n(using (2))\n\u2264 \u03c1\u03c4 m\u03b1 ||f \u2212 f\\i\u2032 ||2 \u00b7 ||x\u2032i\u2032 ||2\n(28)\nMoreover, N\u03bb(g) = \u03bb2 ||g||22 = \u03bb2 \u3008g, g\u3009 is a convex function and its Bregman divergence satisfies:\ndN\u03bb(g, g \u2032) =\n\u03bb\n2\n( \u3008g, g\u3009 \u2212 \u3008g\u2032, g\u2032\u3009 \u2212 \u30082g\u2032, g \u2212 g\u2032\u3009 )\n= \u03bb\n2 ||g \u2212 g\u2032||22\n(29)\nCombining (28) and (29) gives\n\u03bb||f \u2212 f\\i\u2032 ||22 \u2264 \u03c1\u03c4 m\u03b1 ||f \u2212 f\\i\u2032 ||2 \u00b7 ||x\u2032i\u2032 ||2 (30)\nwhich further gives\n||f \u2212 f\\i\u2032 ||2 \u2264 \u03c1\u03c4 m\u03bb\u03b1 ||x\u2032i\u2032 ||2 (31)\nGiven \u03c1-admissibility, we derive the bound of function stability \u2206(f) based on sample z with size n. We have \u2200z = (x,y), \u2200k,\n|f(x, k)\u2212 f\\i\u2032(x, k)| \u2264 \u03c1||f \u2212 f\\i\u2032 ||2 \u00b7 ||x||2 (using (31))\n\u2264 \u03c4\u03c1 2\nm\u03bb\u03b1 ||x\u2032i\u2032 ||2 \u00b7 ||x||2\n(32)\nWith the feature dimension d and x(k,q) \u2264 v for q \u2208 {1, . . . , d} , we have\n||x||2 = || n\u2211\nk=1\nx(k)||2\n\u2264 ||\u3008nv, . . . , nv \ufe38 \ufe37\ufe37 \ufe38\nd\n\u3009||2\n= \u221a dn2v2 = nv \u221a d\n(33)\nSimilarly, we have ||x\u2032i\u2032 ||2 \u2264 nv \u221a d \u03b1 because x \u2032 i\u2032 is with the size n/\u03b1. Inserting the bounds of ||x||2 and ||x\u2032i\u2032 ||2 into (32), it goes to\n|f(x, k)\u2212 f\\i\u2032(x, k)| \u2264 d\u03c4\u03c1 2v2n2\nm\u03bb\u03b12 (34)\nwhich gives (10). Further, using Lemma 6 derives the loss stability bound of d\u03c4 2\u03c12v2n2\nm\u03bb\u03b12 , and the\nsample loss stability bound of d\u03c4 2\u03c12v2n3 m\u03bb\u03b12 on the minimizer f . \u2293\u2294"}, {"heading": "Proof of Corollary 8", "text": "The proof is similar to the proof of Theorem 7. First, we have\ndR\u03b1,\u03bb(f \\i, f) + d\nR \\i \u03b1,\u03bb\n(f, f\\i) = R\u03b1,\u03bb(f \\i)\u2212R\u03b1,\u03bb(f) +R\\i\u03b1,\u03bb(f)\u2212R \\i \u03b1,\u03bb(f \\i)\n= ( R\u03b1,\u03bb(f \\i)\u2212R\\i\u03b1,\u03bb(f\\i) ) \u2212 ( R\u03b1,\u03bb(f)\u2212R\\i\u03b1,\u03bb(f) )\n= 1\nmn\n\u03b1\u2211\nj=1\nL\u03c4 (f\\i, z(i,j))\u2212 1\nmn\n\u03b1\u2211\nj=1\nL\u03c4 (f,z(i,j)) (35)\nThen, we have\ndN\u03bb(f, f \\i) + dN\u03bb(f \\i, f) = d (R\n\\i \u03b1,\u03bb \u2212R\\i\u03b1 )(f, f \\i) + d(R\u03b1,\u03bb\u2212R\u03b1)(f \\i, f)\n= dR\u03b1,\u03bb(f \\i, f) + d\nR \\i \u03b1,\u03bb (f, f\\i)\u2212 dR\u03b1(f\\i, f)\u2212 dR\\i\u03b1 (f, f \\i)\n(based on non-negativity of Bregman divergence)\n\u2264 dR\u03b1,\u03bb(f\\i, f) + dR\\i \u03b1,\u03bb (f, f\\i)\n(using (35))\n= 1\nmn\n\u03b1\u2211\nj=1\nL\u03c4 (f\\i, z(i,j))\u2212 1\nmn\n\u03b1\u2211\nj=1\nL\u03c4 (f,z(i,j))\n= 1\nmn\n\u03b1\u2211\nj=1\n( n/\u03b1 \u2211\nk=1\n\u2113\u03c4 (f \\i, z(i,j), k)\u2212\nn/\u03b1 \u2211\nk=1\n\u2113\u03c4 (f,z(i,j), k)\n)\n\u2264 1 mn\n\u03b1\u2211\nj=1\nn/\u03b1 \u2211\nk=1\n\u2223 \u2223 \u2223\u2113\u03c4 (f \\i, z(i,j), k)\u2212 \u2113\u03c4 (f,z (i,j), k) \u2223 \u2223 \u2223\n\u2264 \u03c4 mn\n\u03b1\u2211\nj=1\nn/\u03b1 \u2211\nk=1\n\u2223 \u2223 \u2223f\\i(x(i,j), k)\u2212 f(x(i,j), k) \u2223 \u2223 \u2223\n(using (2), and define ||x(i,max)||2 = max\u2200j ||x(i,j)||)2)\n\u2264 \u03c1\u03c4 m ||f \u2212 f\\i||2 \u00b7 ||x(i,max)||2\n(36)\nThis gives\n\u03bb||f \u2212 f\\i||22 \u2264 \u03c1\u03c4 m ||f \u2212 f\\i||2 \u00b7 ||x(i,max)||2 (37)\nand thus ||f \u2212 f\\i||2 \u2264 \u03c1\u03c4\nm\u03bb ||x(i,max)||2 (38)\nThen, we derive the bound of function stability \u2206(f) based on sample z with size n, and based on \\i rather than \\i\u2032. We have \u2200z = (x,y), \u2200k,\n|f(x, k)\u2212 f\\i(x, k)| \u2264 \u03c1||f \u2212 f\\i||2 \u00b7 ||x||2 (using (38))\n\u2264 \u03c4\u03c1 2\nm\u03bb ||x(i,max)||2 \u00b7 ||x||2\n\u2264 \u03c4\u03c1 2 m\u03bb \u00b7 nv\n\u221a d\n\u03b1 \u00b7 nv\n\u221a d\n= d\u03c4\u03c12v2n2\nm\u03bb\u03b1 (using (10))\n= \u03b1 sup(\u2206)\n(39)\n\u2293\u2294"}, {"heading": "Proof of Theorem 9", "text": "Let f\\i \u2032 and f\\i be defined like before. Similar to the definition of f\\i \u2032\nbased on removing a minisample from S\u2032, we define f i \u2032\nbased on replacing a mini-sample from S\u2032. Similar to the definition of f\\i based on removing a sample from S, we define f i based on replacing a sample from S. Let R(f)\\i \u2032 denote [R(f)]\\i \u2032 = R\\i \u2032 (f\\i \u2032 ).\nFirst, we derive a bound for |R(f)\u2212R\\i\u2032(f)|:\n|R(f)\u2212R(f)\\i\u2032 | = 1 n |EzL\u03c4 (f,z)\u2212 EzL\u03c4 (f\\i \u2032 , z)|\n= 1\nn |Ez\nn\u2211\nk=1\n\u2113\u03c4 (f,z, k)\u2212 Ez n\u2211\nk=1\n\u2113\u03c4 (f \\i\u2032 , z , k)|\n\u2264 1 n Ez |\nn\u2211\nk=1\n\u2113\u03c4 (f,z, k)\u2212 n\u2211\nk=1\n\u2113\u03c4 (f \\i\u2032 , z, k)|\n\u2264 1 n Ez\nn\u2211\nk=1\n|\u2113\u03c4 (f,z, k)\u2212 \u2113\u03c4 (f\\i \u2032 , z , k)|\n(based on Lemma 6)\n\u2264 \u03c4\u2206\n(40)\nThen, we derive a bound for |R(f)\u2212R(f)i\u2032 |:\n|R(f)\u2212R(f)i\u2032 | = |R(f)\u2212R(f)\\i\u2032 +R(f)\\i\u2032 \u2212R(f)i\u2032 | \u2264 |R(f)\u2212R(f)\\i\u2032 |+ |R(f)\\i\u2032 \u2212R(f)i\u2032 |\n(based on (40))\n\u2264 \u03c4\u2206+ \u03c4\u2206 = 2\u03c4\u2206\nMoreover, we derive a bound for |Re(f) \u2212 Re(f)i \u2032 |. Note that, i\u2032 means replacing a mini-sample according to the training setting with decomposition, and the calculation of Re(f) and Re(f)i \u2032 is\nbased on full-size samples according to the test setting without decomposition. Let z\u0302 \u2032i denote the full-size sample (with size n and indexed by i) which contains the mini-sample z\u0302 \u2032i\u2032 (with size n/\u03b1 and indexed by i\u2032), it goes to:\n|Re(f)\u2212Re(f)i \u2032 | = \u2223 \u2223 \u2223 1\nmn\nm\u2211\nj=1\nL\u03c4 (f,z j)\u2212 1\nmn\n\u2211 j 6=i L\u03c4 (f i \u2032 , zj)\u2212 1 mn L\u03c4 (f i \u2032 , z\u0302 \u2032i) \u2223 \u2223 \u2223\n\u2264 1 mn\n\u2211 j 6=i |L\u03c4 (f,z j)\u2212 L\u03c4 (f i \u2032 , zj)|+ 1 mn |L\u03c4 (f,z i)\u2212 L\u03c4 (f i \u2032 , z\u0302 \u2032i)|\n\u2264 1 mn\n\u2211 j 6=i |L\u03c4 (f,z j)\u2212 L\u03c4 (f i \u2032 , zj)|+ 1 mn\nn\u2211\nk=1\n|\u2113\u03c4 (f,z i, k)\u2212 \u2113\u03c4 (f i \u2032 , z\u0302 \u2032i, k)|\n(based on 0 \u2264 \u2113\u03c4 (GS , z, k) \u2264 \u03b3)\n\u2264 1 mn\n\u2211 j 6=i |L\u03c4 (f,z j)\u2212 L\u03c4 (f i \u2032 , zj)|+ \u03b3 m\n\u2264 1 mn\n\u2211\nj 6=i\n(\n|L\u03c4 (f,zj)\u2212 L\u03c4 (f\\i \u2032 , zj)|+ |L\u03c4 (f\\i \u2032 , z j)\u2212 L\u03c4 (f i \u2032 , zj)| ) + \u03b3\nm\n(based on Lemma 6, and \u2206(f i \u2032 , f\\i \u2032 ) = \u2206(f, f\\i \u2032 ) from the definition of stability)\n\u2264 1 mn\n\u2211\nj 6=i\n( n\u03c4\u2206+ n\u03c4\u2206 ) + \u03b3\nm\n= 2(m\u2212 1)\u03c4\u2206+ \u03b3\nm\n(41)"}, {"heading": "Based on the bounds of |R(f)\u2212R(f)i\u2032 | and |Re(f)\u2212Re(f)i", "text": "\u2032 |, we show that R(f)\u2212Re(f) satisfies the conditions of McDiarmid Inequality (Theorem 14) with ci\u2032 = (4m\u22122)\u03c4\u2206+\u03b3\nm :\n|[R(f)\u2212Re(f)]\u2212 [R(f)\u2212Re(f)]i \u2032 | = |[R(f)\u2212R(f)i\u2032 ]\u2212 [Re(f)\u2212Re(f)i \u2032 ]| \u2264 |R(f)\u2212R(f)i\u2032 |+ |Re(f)\u2212 Re(f)i \u2032 |\n\u2264 2\u03c4\u2206+ 2(m\u2212 1)\u03c4\u2206+ \u03b3 m = (4m\u2212 2)\u03c4\u2206+ \u03b3\nm\n(42)\nAlso, following the proof of Lemma 15, we can get a bound for ES [R(f)\u2212Re(f)]:\nES [R(f)\u2212Re(f)] = 1\nn ES\n(\nEz (L(f,z))\u2212 1\nm\nm\u2211\nj=1\nL(f,z j) )\n= 1\nn\n(\nES,z\u0302i ( L(f, z\u0302 i) ) \u2212 1\nm\nm\u2211\nj=1\nES ( L(f,zj)\n))\n= 1\nn\n(\nES,z\u0302i ( L(f, z\u0302 i) ) \u2212 ES ( L(f,z i)\n))\n= 1\nn\n(\nES,z\u0302i ( L(f, z\u0302 i) ) \u2212 ESi ( L(f i, z\u0302 i)\n))\n= 1\nn ES,z\u0302i\n( L(f, z\u0302 i)\u2212 L(f i, z\u0302 i) )\n\u2264 1 n ES,z\u0302i |L(f, z\u0302 i)\u2212 L(f i, z\u0302 i)| \u2264 1 n ES,z\u0302i |L(f, z\u0302 i)\u2212 L(f\\i, z\u0302 i)|+ 1 n ES,z\u0302i |L(f\\i, z\u0302 i)\u2212 L(f i, z\u0302 i)|\n(based on Lemma 6 and the \u2206\u0304 defined in (12))\n\u2264 \u03c4\u2206\u0304 + \u03c4\u2206\u0304 = 2\u03c4\u2206\u0304\n(43)\nNow, we can apply McDiarmid Inequality (Theorem 14):\nPS\n( [R(f)\u2212Re(f)]\u2212 ES [R(f)\u2212Re(f)] \u2265 \u01eb ) \u2264 exp ( \u22122\u01eb2 \u2211m\u03b1\ni\u2032=1 c 2 i\u2032\n)\n(44)\nBased on (42) and (43), it goes to\nPS\n( R(f)\u2212Re(f) \u2265 2\u03c4\u2206\u0304 + \u01eb ) \u2264 exp (\n\u22122m\u01eb2\n\u03b1 ( (4m\u2212 2)\u03c4\u2206+ \u03b3 )2\n)\n(45)\nLet \u03b4 = exp (\n\u22122m\u01eb2\n\u03b1 ( (4m\u22122)\u03c4\u2206+\u03b3 ) 2\n)\n, we have\n\u01eb = ( (4m\u2212 2)\u03c4\u2206+ \u03b3 ) \u221a \u03b1 ln \u03b4\u22121\n2m (46)\nBased on (45) and (46), there is a probability no more than \u03b4 such that\nR(f)\u2212Re(f) \u2265 2\u03c4\u2206\u0304 + \u01eb\n= 2\u03c4\u2206\u0304 + ( (4m\u2212 2)\u03c4\u2206+ \u03b3 ) \u221a \u03b1 ln \u03b4\u22121\n2m\n(47)\nThen, there is a probability at least 1\u2212 \u03b4 such that\nR(f) \u2264 Re(f) + 2\u03c4\u2206\u0304 + ( (4m\u2212 2)\u03c4\u2206+ \u03b3 ) \u221a \u03b1 ln \u03b4\u22121\n2m\nwhich gives (13). \u2293\u2294"}, {"heading": "Proof of Proposition 13", "text": "By subtractingw\u2217 from both sides and taking norms for (21), we have\n||wt+1 \u2212w\u2217||2 = ||wt \u2212 \u03b7\u2207gzt(wt)\u2212w\u2217||2\n= ||wt \u2212w\u2217||2 \u2212 2\u03b7(wt \u2212w\u2217)T\u2207gzt(wt) + \u03b72||\u2207gzt(wt)||2 (48)\nTaking expectations and let at = ||wt \u2212w\u2217||2, we have at+1 = at \u2212 2\u03b7E[(wt \u2212w\u2217)T\u2207gzt(wt)] + \u03b72E[||\u2207gzt(wt)||2]\n(based on (24) )\n\u2264 at \u2212 2\u03b7E[(wt \u2212w\u2217)T\u2207gzt(wt)] + \u03b72\u03ba2|zt|2 (Recall zt is of the size n/\u03b1 based on the definition of structure regularization )\n= at \u2212 2\u03b7E[(wt \u2212w\u2217)T\u2207gzt(wt)] + \u03b72\u03ba2n2\n\u03b12\n(since the random drawing of zt is independent of wt)\n= at \u2212 2\u03b7E[(wt \u2212w\u2217)TEzt(\u2207gzt(wt))] + \u03b72\u03ba2n2\n\u03b12\n= at \u2212 2\u03b7E[(wt \u2212w\u2217)T\u2207g(wt)] + \u03b72\u03ba2n2\n\u03b12\n(49)\nBy setting w\u2032 = w\u2217 in (22), we have\n(w \u2212w\u2217)T\u2207g(w) \u2265 g(w)\u2212 g(w\u2217) + c 2 ||w \u2212w\u2217||2\n\u2265 c 2 ||w \u2212w\u2217||2\n(50)\nCombining (49) and (50), we have\nat+1 \u2264 at \u2212 \u03b7c||wt \u2212w\u2217||2 + \u03b72\u03ba2n2\n\u03b12\n= (1\u2212 c\u03b7)at + \u03b72\u03ba2n2\n\u03b12\n(51)\nWe can find the steady state a\u221e as follows\na\u221e = (1\u2212 c\u03b7)a\u221e + \u03b72\u03ba2n2\n\u03b12 (52)\nwhich gives\na\u221e = \u03b7\u03ba2n2\nc\u03b12 (53)\nDefining the function A(x) = (1\u2212 c\u03b7)x + \u03b72\u03ba2n2\u03b12 , based on (51) we have at+1 \u2264 A(at)\n(Taylor expansion of A(\u00b7) based on a\u221e, with \u22072A(\u00b7) being 0) = A(a\u221e) +\u2207A(a\u221e)(at \u2212 a\u221e) = A(a\u221e) + (1 \u2212 c\u03b7)(at \u2212 a\u221e) = a\u221e + (1 \u2212 c\u03b7)(at \u2212 a\u221e)\n(54)\nUnwrapping (54) goes to at <= (1\u2212 c\u03b7)t(a0 \u2212 a\u221e) + a\u221e (55)\nSince \u2207g(w) is Lipschitz according to (23), we have\ng(w) \u2264 g(w\u2032) +\u2207g(w \u2032)T (w \u2212w \u2032) + q 2 ||w \u2212w\u2032||2\nSetting w \u2032 = w\u2217, it goes to g(w)\u2212 g(w\u2217) \u2264 q2 ||w \u2212w\u2217||2, such that\nE[g(wt)\u2212 g(w\u2217)] \u2264 q 2 ||wt \u2212w\u2217||2 = q 2 at\nIn order to have E[g(wt)\u2212 g(w\u2217)] \u2264 \u01eb, it is required that q2at \u2264 \u01eb, that is\nat \u2264 2\u01eb\nq (56)\nCombining (55) and (56), it is required that\n(1\u2212 c\u03b7)t(a0 \u2212 a\u221e) + a\u221e \u2264 2\u01eb\nq (57)\nTo meet this requirement, it is sufficient to set the learning rate \u03b7 such that both terms on the left side are less than \u01ebq . For the requirement of the second term a\u221e \u2264 \u01ebq , recalling (53), it goes to\n\u03b7 \u2264 c\u01eb\u03b1 2\nq\u03ba2n2\nThus, introducing a real value \u03b2 \u2208 (0, 1], we can set \u03b7 as\n\u03b7 = c\u01eb\u03b2\u03b12\nq\u03ba2n2 (58)\nOn the other hand, for the requirement of the first term (1 \u2212 c\u03b7)t(a0 \u2212 a\u221e) \u2264 \u01ebq , it goes to\nt \u2265 log \u01ebqa0\nlog (1\u2212 c\u03b7) (since log (1\u2212 c\u03b7) \u2264 \u2212c\u03b7 given (25))\n\u2265 log (qa0/\u01eb) c\u03b7\n(59)\nCombining (58) and (59), it goes to\nt \u2265 q\u03ba 2n2 log (qa0/\u01eb)\n\u01eb\u03b2c2\u03b12\nwhich completes the proof. \u2293\u2294"}, {"heading": "5 Conclusions", "text": "We proposed a structure regularization framework, which decomposes training samples into minisamples with simpler structures, deriving a trained model with regularized structural complexity. Our theoretical analysis showed that this method can effectively reduce the generalization risk, and can also accelerate the convergence speed in training. The proposed method does not change the convexity of the objective function, and can be used together with any existing weight regularization methods. Note that, the proposed method and the theoretical results can fit general structures including linear chains, trees, and graphs. Experimental results demonstrated that our method achieved better results than state-of-the-art systems on several highly-competitive tasks, and at the same time with substantially faster training speed."}, {"heading": "Acknowledgments", "text": "This work was supported in part by NSFC (No.61300063)."}], "references": [{"title": "A spectral regularization framework for multi-task structure learning", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "In Proceedings of NIPS\u201907", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Structured sparsity through convex optimization", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "CoRR, abs/1109.2397,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Online algorithms and stochastic approximations. Online Learning and Neural Networks. Saad, David", "author": ["L. Bottou"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["M. Collins"], "venue": "In Proceedings of EMNLP\u201902,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Boosting with structural sparsity", "author": ["J.C. Duchi", "Y. Singer"], "venue": "In ICML\u201909,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "A comparative study of parameter estimation methods for statistical natural language processing", "author": ["J. Gao", "G. Andrew", "M. Johnson", "K. Toutanova"], "venue": "In Proceedings of ACL\u201907,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Posterior vs parameter sparsity in latent variable models", "author": ["J. Gra\u00e7a", "K. Ganchev", "B. Taskar", "F. Pereira"], "venue": "In Proceedings of NIPS\u201909,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Learning with structured sparsity", "author": ["J. Huang", "T. Zhang", "D.N. Metaxas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In ICML\u201901,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Slow learners are fast", "author": ["J. Langford", "A.J. Smola", "M. Zinkevich"], "venue": "In NIPS\u201909,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Pac-bayes generalization bounds for randomized structured prediction", "author": ["B. London", "B. Huang", "B. Taskar", "L. Getoor"], "venue": "In NIPS Workshop on Perturbation, Optimization and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Structured sparsity in structured prediction", "author": ["A.F.T. Martins", "N.A. Smith", "M.A.T. Figueiredo", "P.M.Q. Aguiar"], "venue": "In Proceedings of EMNLP\u201911,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "A family of penalty functions for structured sparsity", "author": ["C.A. Micchelli", "J. Morales", "M. Pontil"], "venue": "In Proceedings of NIPS\u201910,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. Re", "S.J. Wright"], "venue": "In NIPS\u201911,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Joint covariate selection and joint subspace selection for multiple classification problems", "author": ["G. Obozinski", "B. Taskar", "M.I. Jordan"], "venue": "Statistics and Computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "An efficient projection for l1,infinity regularization", "author": ["A. Quattoni", "X. Carreras", "M. Collins", "T. Darrell"], "venue": "In Proceedings of ICML\u201909,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Convex structure learning in log-linear models: Beyond pairwise potentials", "author": ["M.W. Schmidt", "K.P. Murphy"], "venue": "In Proceedings of AISTATS\u201910,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Learnability and stability in the general learning setting", "author": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In Proceedings of COLT\u201909,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Guided learning for bidirectional sequence classification", "author": ["L. Shen", "G. Satta", "A.K. Joshi"], "venue": "In Proceedings of ACL\u201907,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Large-scale personalized human activity recognition using online multitask learning", "author": ["X. Sun", "H. Kashima", "N. Ueda"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Feature-frequency-adaptive on-line training for fast and accurate natural language processing", "author": ["X. Sun", "W. Li", "H. Wang", "Q. Lu"], "venue": "Computational Linguistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Piecewise pseudolikelihood for efficient training of conditional random fields", "author": ["C.A. Sutton", "A. McCallum"], "venue": "In ICML\u201907,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "In NIPS\u201903,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Learning with lookahead: Can history-based models rival globally optimized models", "author": ["Y. Tsuruoka", "Y. Miyao", "J. Kazama"], "venue": "In Conference on Computational Natural Language Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Estimating the \u201dwrong\u201d graphical model: Benefits in the computation-limited setting", "author": ["M.J. Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Structural regularized support vector machine: A framework for structural large margin classifier", "author": ["H. Xue", "S. Chen", "Q. Yang"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Reranking for biomedical named-entity recognition", "author": ["K. Yoshida", "J. Tsujii"], "venue": "In ACL Workshop on BioNLP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "For (typically non-structured) classification problems, there are considerable studies on structurerelated regularization, including spectral regularization for modeling feature structures in multi-task learning [1], regularizing feature structures for structural large margin classifiers [27], and many recent studies on structured sparsity.", "startOffset": 212, "endOffset": 215}, {"referenceID": 26, "context": "For (typically non-structured) classification problems, there are considerable studies on structurerelated regularization, including spectral regularization for modeling feature structures in multi-task learning [1], regularizing feature structures for structural large margin classifiers [27], and many recent studies on structured sparsity.", "startOffset": 289, "endOffset": 293}, {"referenceID": 13, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 84, "endOffset": 91}, {"referenceID": 5, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 84, "endOffset": 91}, {"referenceID": 17, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 128, "endOffset": 136}, {"referenceID": 12, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 128, "endOffset": 136}, {"referenceID": 16, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 177, "endOffset": 181}, {"referenceID": 28, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 195, "endOffset": 199}, {"referenceID": 7, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 226, "endOffset": 229}, {"referenceID": 1, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 258, "endOffset": 268}, {"referenceID": 15, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 258, "endOffset": 268}, {"referenceID": 8, "context": "Structure sparsity is studied for a variety of non-structured classification models [14, 6] and structured prediction scenarios [18, 13], via adopting mixed norm regularization [17], Group Lasso [29], posterior regularization [8], and a string of variations [2, 16, 9].", "startOffset": 258, "endOffset": 268}, {"referenceID": 22, "context": "[23] described an interesting heuristic piecewise training method for structured prediction models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] described a \u201clookahead\u201d learning method based on structured perceptrons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Our work differs from [23] and [25] mainly because our work is built on a regularization framework, with arguments and theoretical justifications on reducing generalization risk and improving convergence rate.", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "Our work differs from [23] and [25] mainly because our work is built on a regularization framework, with arguments and theoretical justifications on reducing generalization risk and improving convergence rate.", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "[26] suggested consistent approximation for both training and test phase, but there is no indication on structure regularization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "On generalization risk analysis, related studies include [4, 19] on non-structured classification and [24, 12] on structured classification.", "startOffset": 57, "endOffset": 64}, {"referenceID": 18, "context": "On generalization risk analysis, related studies include [4, 19] on non-structured classification and [24, 12] on structured classification.", "startOffset": 57, "endOffset": 64}, {"referenceID": 23, "context": "On generalization risk analysis, related studies include [4, 19] on non-structured classification and [24, 12] on structured classification.", "startOffset": 102, "endOffset": 110}, {"referenceID": 11, "context": "On generalization risk analysis, related studies include [4, 19] on non-structured classification and [24, 12] on structured classification.", "startOffset": 102, "endOffset": 110}, {"referenceID": 23, "context": ", expected average loss) [24, 12]:", "startOffset": 25, "endOffset": 33}, {"referenceID": 11, "context": ", expected average loss) [24, 12]:", "startOffset": 25, "endOffset": 33}, {"referenceID": 3, "context": "We follow some notations and assumptions on non-structured classification [4, 19].", "startOffset": 74, "endOffset": 81}, {"referenceID": 18, "context": "We follow some notations and assumptions on non-structured classification [4, 19].", "startOffset": 74, "endOffset": 81}, {"referenceID": 2, "context": "Our analysis is based on the stochastic gradient descent (SGD) setting [3, 11, 15], which is arguably the most representative online training setting.", "startOffset": 71, "endOffset": 82}, {"referenceID": 10, "context": "Our analysis is based on the stochastic gradient descent (SGD) setting [3, 11, 15], which is arguably the most representative online training setting.", "startOffset": 71, "endOffset": 82}, {"referenceID": 14, "context": "Our analysis is based on the stochastic gradient descent (SGD) setting [3, 11, 15], which is arguably the most representative online training setting.", "startOffset": 71, "endOffset": 82}, {"referenceID": 4, "context": "We use the standard benchmark dataset in prior work [5], which is derived from PennTreeBank corpus and uses sections 0 to 18 of the Wall Street Journal (WSJ) for training (38,219 samples), and sections 22-24 for testing (5,462 samples).", "startOffset": 52, "endOffset": 55}, {"referenceID": 24, "context": "Following prior work [25], we use features based on unigrams and bigrams of neighboring words, and lexical patterns of the current word, with 393,741 raw features8 in total.", "startOffset": 21, "endOffset": 25}, {"referenceID": 24, "context": "Following prior work [25], we use word pattern features and POS features, with 403,192 raw features in total.", "startOffset": 21, "endOffset": 25}, {"referenceID": 6, "context": "Following prior work [7], we use features based on character unigrams and bigrams, with 1,985,720 raw features in total.", "startOffset": 21, "endOffset": 24}, {"referenceID": 20, "context": "This is a task based on real-valued sensor signals, with the data extracted from the Bao04 activity recognition dataset [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "Following prior work in activity recognition [21], we use acceleration features, mean features, standard deviation, energy, and correlation features, with 1228 raw features in total.", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "We choose the conditional random fields (CRFs) [10] and structured perceptrons (Perc) [5], which are arguably the most popular probabilistic and non-probabilistic structured prediction models, respectively.", "startOffset": 47, "endOffset": 51}, {"referenceID": 4, "context": "We choose the conditional random fields (CRFs) [10] and structured perceptrons (Perc) [5], which are arguably the most popular probabilistic and non-probabilistic structured prediction models, respectively.", "startOffset": 86, "endOffset": 89}, {"referenceID": 4, "context": ", averaged perceptron [5].", "startOffset": 22, "endOffset": 25}, {"referenceID": 21, "context": "Since the rich edge features [22] can be automatically generated from raw features and are very useful for improving model accuracy, the rich edge features are employed for all methods.", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "We also tested on sparsity emphasized regularization methods, including L1 regularization and Group Lasso regularization [13].", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": "33 (see [20]) 72.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "28 (see [25]) 97.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "19 (see [7]) Our results 97.", "startOffset": 8, "endOffset": 11}, {"referenceID": 19, "context": "The POS-Tagging task is a highly competitive task, with many methods proposed, and the best report (without using extra resources) until now is achieved by using a bidirectional learning model in [20],12 with the accuracy 97.", "startOffset": 196, "endOffset": 200}, {"referenceID": 24, "context": "On the Bio-NER task, [25] achieves 72.", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "28% based on lookahead learning and [28] achieves 72.", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "On the Word-Seg task, [7] achieves 97.", "startOffset": 22, "endOffset": 25}, {"referenceID": 21, "context": "19% based on maximum entropy classification and our recent work [22] achieves 97.", "startOffset": 64, "endOffset": 68}], "year": 2017, "abstractText": "While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via structure decomposition, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies yet with substantially faster training speed.", "creator": "LaTeX with hyperref package"}}}