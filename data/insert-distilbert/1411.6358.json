{"id": "1411.6358", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Nov-2014", "title": "A Hybrid Solution to improve Iteration Efficiency in the Distributed Learning", "abstract": "currently, exceptionally many machine learning algorithms contain lots of iterations. when it comes more to existing large - scale concurrent distributed systems, some buffer slave nodes may break down early or have lower efficiency. therefore traditional machine learning adaptive algorithm construction may fail because of overcoming the instability of distributed system. we presents a hybrid approach which not only own a high fault - tolerant but also achieve a balance of performance and efficiency. for each iteration, the result prediction of slow machines will be abandoned. then, we discuss the relationship described between fail accuracy and abandon rate. next we debate the convergence speed of this process. finally, our experiments demonstrate our quantum idea can dramatically reduce calculation time and be used in many platforms.", "histories": [["v1", "Mon, 24 Nov 2014 06:42:03 GMT  (10kb)", "http://arxiv.org/abs/1411.6358v1", "Unfinished job, with completed mathematical proof while experiments are in process now. We plan to submit this paper to icml2015"]], "COMMENTS": "Unfinished job, with completed mathematical proof while experiments are in process now. We plan to submit this paper to icml2015", "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["junxiong wang", "hongzhi wang", "chenxu zhao"], "accepted": false, "id": "1411.6358"}, "pdf": {"name": "1411.6358.pdf", "metadata": {"source": "META", "title": "A hybrid solution to improve iteration efficiency in the distributed learning", "authors": ["Junxiong Wang"], "emails": ["chuangzhetianxia@gmail.com", "wangzh@hit.edu.cn", "zhaochenxu001@gmail.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 1.\n63 58\nv1 [\ncs .D\nC ]\n2 4\nN ov\n1. Introduction\nWith an explosion in the number of massive-scale data, the cost time of traditional solutions is unacceptable. As a result of it, the demand for scalable disturbed platforms and frameworks is rising in many areas including machine learning, data mining and pattern recognition.\nHadoop is widely used to speed up many machine learning algorithms applied in academy and industry. However, the serve problem about it is the lack of support for iterative programs. Thus lots of popular-used platforms and distributed frameworks appeared to optimize the iteration process for higher efficiency and less cost. A framework named Spark[1], widely used with a library for machine\nlearning (MLlib), has improved the efficiency in many applications. Its modified solution, Resilient Distributed Datasets[2], speed up the data storage to an amazing step. In addition, Haloop[3] makes lots of improvement on iteration. It provides some rules to consult when developing the machine learning project based on map/reduce.\nWhen it comes to applications, some slave nodes, taking up only a small percentage of all, always cost much more time than others in one iteration because of communication fault or their low efficiency. Traditional solutions cannot handle it as they have to calculate it again when failure occurs.\nThis paper presents a novel algorithm which is able to make a suitable balance in performance and efficiency. The master node doesn\u2019t have to wait all the slave nodes. After a certain percentage of slaves has finished calculated and communicated with the master node, the master one will start next iteration rather than waiting others. This process can not only improve the efficiency dramatically but also have a high fault tolerance because some nodes\u2019 fault do not have influence on this system. This algorithm is developed to decrease the total time, combining the synchronous and asynchronous process, resulting in reasonable efficiency and accuracy.\nWe discuss the relationship between the abandon rate and the accuracy with statistic. In next section, this algorithm is proved to converge and the speed of convergence is Q-linear convergence with the mathematics proof. It is such an excepted result that balance the efficiency and performance.\nThis idea can be applied to a list of algorithms including iterations such as Stochastic Gradient Descent, Conjugate Gradient Descent, L-BFGS and so on. Many existing platforms and framework can be improved with\nthis approach.\n2. Algorithm\nWhen our approach is applied in Gradient Descent, we give the following algorithm.\nSuppose machine num is M and examples in each machine are \u03b6. For each iteration, the number of slaves that master should wait is \u03b3. That is to say, only \u03b3\u03b6 examples can be calculated.\nEstimating the least number of slave nodes which need to communicate with master node is the first step.\nAlgorithm 1 Calculate The Least Number of Slave Nodes Input: The capacity of data : N Confidence coefficient : \u03b1 Relative error \u03be\nExamples in each machine : \u03b6 Output: Estimated Machine Number : \u03b3\n\u03b3 = Nu2\u03b1/2\n(\u03be2N + u2\u03b1/2) \u2217 \u03b6 ;\nAlgorithm 2 Master\u2019s algorithm 1: while IsConvergence == false do 2: if received \u03b3 slave nodes then 3: \u03b8i+1 = \u03b8i \u2212 \u03b7t\n\u03b3\n\u2211\u03b3 j=1 \u03b8 j i\n4: end if 5: end while\nAlgorithm 3 Slaves\u2019 algorithm\n1: Receive(\u03b8t) 2: \u03b8t+1 = \u03b8t\u2212 { 1\n\u03b6\n\u2211\u03b6 i=1(\u03b8 T K[xi]\u2212 yi)K[xi] + \u03bb\u03b8t\n}\n3: Send(\u03b8t+1)\n3. The Algorithm\u2019s Convergence\nIn this section we will prove the convergence of this algorithm with an example of the quadratic programming. A definition of quadratic programming problem will be given first and then we will prove the convergence to explain the algorithm\u2019s correctness.At the end of this section, we will discuss the speed of convergence.\n3.1. Brief Introduction\nWe donate X \u2208 Rm\u00d7n and Y \u2208 Rm as the example input and output space respectively. And set \u03b8\u2217 \u2208 Rl as the\noptimal solution to a quadratic programming, then\n\u03b8 \u2217 = arg min\n\u03b8\u2208Rl\n1\nm\nm \u2211\ni=1\n(f\u03b8(xi)\u2212 yi)2 + \u03bb \u2225 \u2225\u03b8 \u2225 \u2225 2\nl2 (1)\nwhere \u03bb > 0 is the regularization parameter.\nDefinition 3.1. Denote function K is a kernel function\nwhere K[xi] =\n\n                     \nx2i1 xi1xi2\n... x2i2\nxi2xi3 ...\nx2in xi1 xi2\n... xin 1\n\n                      \u2208 Rl.\nAbove all, \u03b8\u2217 can be written as\n\u03b8 \u2217 = arg min\n\u03b8\u2208Rl\n1\nm\nm \u2211\ni=1\n(\u03b8TK[xi]\u2212 yi)2 + \u03bb \u2225 \u2225\u03b8 \u2225 \u2225 2\nl2 (2)\nIn the t th iteration,\u03b8t can be updated as\n\u03b8 t+1 = \u03b8t\u2212\u03b7t{\n1\n\u03c9\n\u03c9 \u2211\ni=1\n(\u03b8TK[xi]\u2212 yi)K[xi] + \u03bb\u03b8t} (3)\n3.2. The Proof of Convergence\nAccording to Taylor formula, (3) can turn into\nf(\u03b8t+1) = f(\u03b8t)\u2212\n\u03b7t\u2207f(\u03b8t)T { 1\n\u03c9\n\u03c9 \u2211\ni=1\n(\u03b8TK[xi]\u2212 yi)K[xi] + \u03bb\u03b8t}\n+O(0)\n(4)\nIf f(\u03b8t+1) \u2212 f(\u03b8t) < 0, then\n\u03b7t{ 1\nm\nm \u2211\ni=1\n(\u03b8TK[xi]\u2212 yi)K[xi] + \u03bb\u03b8t}T\n\u2217 { 1 \u03c9\n\u03c9 \u2211\ni=1\n(\u03b8TK[xi]\u2212 yi)K[xi] + \u03bb\u03b8t} > 0 (5)\nwhere the step size \u03b7t > 0. In order to prove the correctness of (5),we mention some useful lemma.\nLemma 3.1. Donate the overall variance is \u03c32,we choose a random sample of n (sampled without repeating) capacity from N elements. The variance of the sample mean is\n\u03c3n = \u03c3\u221a n\n\u221a\nN \u2212 n N \u2212 1\nThen, we proof this by statics theory. We define Z and z stand for the overall collection and sample collection respectively. So,\n\u03c3n =\n\u221a \u221a \u221a \u221a 1\nCnN\ncn N\n\u2211\ni=1\n(zi \u2212 Z) (6)\nThen,\n\u03c32n = 1\nCnN\nCn N \u2211\ni=1\n[ (zi1 + zi2 + \u00b7 \u00b7 \u00b7+ zin)\nn \u2212 Z]2\n= 1\nCnNn 2\nCn N \u2211\ni=1\n[(zi1 + zi2 + \u00b7 \u00b7 \u00b7+ zin)\u2212 nZ]2\n= 1\nCnNn 2 [\nCn N \u2211\ni=1\n(zi1 + zi2 + \u00b7 \u00b7 \u00b7+ zin)2\n\u2212 2nZ Cn N \u2211\ni=1\n(zi1 + zi2 + \u00b7 \u00b7 \u00b7+ zin) + n2CnNZ 2 ]\n= 1\nCnNn 2 [\nCn N \u2211\ni=1\n(zi1 + zi2 + \u00b7 \u00b7 \u00b7+ zin)2\n\u2212 2nZCn\u22121N\u22121 N \u2211\nS=1\nZS + n 2CnNZ 2 ]\n= 1\nCnNn 2\nCn N \u2211\ni=1\n(zi1 + zi2 + \u00b7 \u00b7 \u00b7+ zin)2 \u2212 Z 2\n(7)\nEvery sample has a ZS , then the expansion of the sample sum square has one Z2S , so there are C n\u22121 N\u22121 Z 2 S of \u2211M i=1(zi1 + zi2 + \u00b7 \u00b7 \u00b7+ zin), the total number of quadratic term is Cn\u22121N\u22121N = nC n N . There are (n 2\u2212n)CnN ZiZj(i 6= j) and N(N \u2212 1)\n2 kinds of ZiZj . So every kind has\n2n(n\u2212 1)CnN N(N \u2212 1) terms.\n1\nCnNn 2\nCn N \u2211\ni=1\n(zi1 + zi2 + \u00b7 \u00b7 \u00b7+ zin)2 \u2212 Z 2\n= 1\nCnNn 2 {Cn\u22121N\u22121\nN \u2211\nS=1\nZ2S + 2n(n\u2212 1)CnN N(N \u2212 1) \u2211\ni<j\nZiZj} \u2212 Z 2\n=( 1 nN \u2212 1 N2 )\nM \u2211\nS=1\nZ2S + 2( n\u2212 1 N(N \u2212 1) \u2212 1 N2 ) \u2211\ni<j\nZiZj\n=( N \u2212 n n(N \u2212 1))( N \u2212 1 N2\nN \u2211\nS=1\nZ2S \u2212 2\nN2\n\u2211\ni<j\nZiZj)\n=( N \u2212 n n(N \u2212 1))[ 1 N\nN \u2211\nS=1\nZ2S \u2212 1\nN2 (\nN \u2211\nS=1\n+2 \u2211\ni<j\nZiZj)]\n= N \u2212 n N \u2212 1 \u03c32\nn (8)\nAbove all, Lemma 2.1 is proved to be right.\nLemma 3.2. If the sample size n >= Nu2\u03b1/2s 2\n\u22062N + u2\u03b1/2s 2\n,\nthe confidence coefficient of error under 1\u2212\u2206 is \u03b1\nProof. If confidence coefficient is 1\u2212\u2206. Then,\nP [| z \u2212 Z |< \u2206] = 1\u2212 \u03b1 (9)\nWhen n is large,we can utilize normal approximation to make a conclusion that\nP{| z \u2212 Z \u03c3z |< \u2206z \u03c3z } = \u03a6(ua/2)\u2212 \u03a6(\u2212ua/2) (10)\nThen, \u2206z = ua/2\u03c3z (11)\nCombining Lemma 2.1 ,we know\n\u2206z = ua/2 s\u221a n\n\u221a\n1\u2212 n N\n(12)\nn = Nu2a/2s 2\n\u22062zN + u 2 a/2s 2\n(13)\nAbove all, the correctness of Lemma 2.2 can be proved.\nUsing these Lemma, we can debate some questions about (5).Denote a set named Z\nZ = {(\u03b8TK[x1]\u2212 y1)K[x1], (\u03b8TK[x2]\u2212 y2)K[x2], \u00b7 \u00b7 \u00b7 , (\u03b8TK[xm]\u2212 ym)K[xm]}\n(14)\nTake \u03c9 elements from the collection Z , if we donate the average num of these elements is \u03c9. When \u2206 = \u2223 \u2223 \u2223 \u03beZ \u2223 \u2223 \u2223 is small, the correctness of (5) can\nbe guaranteed. Combining (14), we can get n = Nu2a/2s 2 (\u03beZ) 2 N + u2a/2s 2 \u2264 Nu2a/2s 2 \u03be2s2N + u2a/2s 2 = Nu2a/2 \u03be2N + u2a/2\n3.3. Speed of Convergence\nDefinition 3.2. Suppose that this process produce a sequence of iterations(\u03b8t) converge to (\u03b8\u2217), if there exist a real number \u03b2 > 0 and a constant(q > 0) which has no relationship with the number of iterations(t) let\nlimt\u2192\u221e\n\u2225 \u2225\u03b8 t+1 \u2212 \u03b8\u2217 \u2225 \u2225\nl2 \u2225 \u2225 \u2225\u03b8 k \u2212 \u03b8\u2217 \u2225 \u2225 \u2225 \u03b2\nl2\n= q. Therefore, the sequence \u03b8t\nhas a convergence speed of Q-\u03b2 -th.\nOur algorithm has a linear convergence of Q-th. That\u2019s to say \u03b2 = 1 and q > 0. We denote Bt = 1\n\u03c9\n\u2211\u03c9 i=1(\u03b8 T K[xi] \u2212 yi)K[xi] + \u03bb\u03b8t.\nThen\n\u2225 \u2225\u03b8 t+1 \u2212 \u03b8\u2217 \u2225 \u2225 2\nl2\n= \u2329 \u03b8 t \u2212 \u03b8\u2217 \u2212 \u03b7tBt, \u03b8t \u2212 \u03b8\u2217 \u2212 \u03b7tBt \u232a\nl2\n= \u2225 \u2225\u03b8 t \u2212 \u03b8\u2217 \u2225 \u2225 2\nl2 + 2\u03b7t\n\u2329\n\u03b8 \u2217 \u2212 \u03b8t, Bt\n\u232a\nl2 + \u03b72t\n\u2225 \u2225Bt \u2225 \u2225 2\nl2\n(15)\n\u2329\n\u03b8 \u2217 \u2212 \u03b8t, Bt\n\u232a\n=\n\u2329\n\u03b8 \u2217 \u2212 \u03b8t, 1\n\u03c9\n\u03c9 \u2211\ni=1\n(\u03b8TK[xi]\u2212 [yi])K[xi] \u232a\n+ \u03bb \u2329 \u03b8 \u2217 \u2212 \u03b8t, \u03b8t \u232a\n=\n\u2329\n\u03b8 \u2217 \u2212 \u03b8t, 1\n\u03c9\n\u03c9 \u2211\ni=1\n(\u03b8TK[xi]\u2212 [yi])K[xi] \u232a\n+ \u03bb \u2329 \u03b8 \u2217, \u03b8t \u232a \u2212 \u03bb \u2225 \u2225\u03b8 t \u2225 \u2225 2\nl2\n\u2264 \u2329 \u03b8 \u2217 \u2212 \u03b8t, 1\n\u03c9\n\u03c9 \u2211\ni=1\n(\u03b8TK[xi]\u2212 [yi])K[xi] \u232a\n+ 1\n2 \u03bb \u2225 \u2225\u03b8 \u2217 \u2225 \u2225 2 l2 +\n1 2 \u03bb \u2225 \u2225\u03b8 t \u2225 \u2225 2 l2 \u2212 \u03bb \u2225 \u2225\u03b8 t \u2225 \u2225 2 l2\n\u2264 \u2329 \u03b8 \u2217 \u2212 \u03b8t, 1\n\u03c9\n\u03c9 \u2211\ni=1\n(\u03b8TK[xi]\u2212 [yi])K[xi] \u232a\n+ \u03bb\n2\n\u2225 \u2225\u03b8 \u2217 \u2225 \u2225 2 l2 \u2212 \u03bb\n2\n\u2225 \u2225\u03b8 t \u2225 \u2225 2\nl2\n(16)\nBy the convexity, we get from\n\u2329\n\u03b8 \u2217 \u2212 \u03b8t, 1\n\u03c9\n\u03c9 \u2211\ni=1\n(\u03b8TK[xi]\u2212 [yi])K[xi] \u232a\n\u2264 1 2\u03c9 [ 1 \u03c9\n\u03c9 \u2211\ni=1\n(\u03b8\u2217KT [xi]\u2212 yi)2\n\u2212 \u03c9 \u2211\ni=1\n(\u03b8tKT [xi]\u2212 yi)2]\n(17)\nSo (16) can turn into\n\u2329\n\u03b8 \u2217 \u2212 \u03b8t, Bt\n\u232a l2 \u2264 1 2 [ 1 \u03c9\n\u03c9 \u2211\ni=1\n(\u03b8\u2217KT [xi]\u2212 yi)2\n+ \u03bb \u2225 \u2225\u03b8 \u2217 \u2225 \u2225 2 l2 ]\u2212 1 2 [ 1 \u03c9\n\u03c9 \u2211\ni=1\n(\u03b8tKT [xi]\u2212 yi)2 + \u03bb \u2225 \u2225\u03b8 t \u2225 \u2225 2\nl2 ]\n(18)\nSo we need to prove the correctness of this function\nLemma 3.3. \u03bb \u2225 \u2225\u03b8 \u2212 \u03b8\u2217 \u2225 \u2225 2\nl2\n\u2264 { 1 \u03c9 \u2211\u03c9 i=1 (\u03b8K T [xi]\u2212 yi)2 + \u03bb \u2225 \u2225\u03b8 \u2225 \u2225 2 l2 }\u2212 { 1 \u03c9 \u2211\u03c9 i=1 (\u03b8 \u2217 K T [xi]\u2212 yi)2 + \u03bb \u2225 \u2225\u03b8 \u2217 \u2225 \u2225 2 l2 }\nProof.\n\u03b8 \u03bd = \u03bd\u03b8 + (1\u2212 \u03bd)\u03b8\u2217 (19)\nG(\u03bd) = 1\n\u03c9\n\u03c9 \u2211\ni=1\n(\u03b8\u03bdTK[xi]\u2212 yi)2 + \u03bb \u2225 \u2225\u03b8 \u03bd \u2225 \u2225 2\nl2 (20)\nG\u2032(\u03bd) =2\u03bb(\u03b8 \u2212 \u03b8\u2217)T\u03b8\u03bd + 2 \u03c9\n\u03c9 \u2211\ni=1\n(\u03b8TK[xi]\u2212 yi)(\u03b8\n\u2212 \u03b8\u2217)TK[xi] (21)\nBeacause f \u2032(\u03b8\u2217) = 0 ,then\n\u03bb(\u03b8 \u2212 \u03b8\u2217)T\u03b8\u2217 + 1 \u03c9\n\u03c9 \u2211\ni=1\n(\u03b8TK[xi]\u2212 yi)(\u03b8\u2212\n\u03b8 \u2217)TK[xi] = 0\n(22)\nG\u2032(\u03bd) = 2\u03bb(\u03b8 \u2212 \u03b8\u2217)T\u03b8\u03bd \u2212 2\u03bb(\u03b8 \u2212 \u03b8\u2217)T\u03b8\u2217\n+ 2\n\u03c9\n\u03c9 \u2211\ni=1\n(\u03b8\u03bdTK[xi]\u2212 yi)(\u03b8 \u2212 \u03b8\u2217)TK[xi]\n\u2212 2 \u03c9\n\u03c9 \u2211\ni=1\n(\u03b8\u2217TK[xi]\u2212 yi)(\u03b8 \u2212 \u03b8\u2217)TK[xi]\n= 2\u03bb\u03bd(\u03b8 \u2212 \u03b8\u2217)T (\u03b8 \u2212 \u03b8\u2217) + 2\u03bd \u03c9\n\u03c9 \u2211\ni=1\n((\u03b8 \u2212 \u03b8\u2217)TK[xi])2\n\u2265 2\u03bb\u03bd(\u03b8 \u2212 \u03b8\u2217)T (\u03b8 \u2212 \u03b8\u2217) (23)\nObserve that G(1)\u2212G(0) = \u222b 1\n0 G\u2032(\u03bd) d\u03bd.Thus,\nG(1)\u2212G(0) \u2265 2\u03bb(\u03b8 \u2212 \u03b8\u2217)T (\u03b8 \u2212 \u03b8\u2217) \u222b 1\n0\n\u03bd d\u03bd\n= \u03bb(\u03b8 \u2212 \u03b8\u2217)T (\u03b8 \u2212 \u03b8\u2217) (24)\nThe correctness of Lemma3.3 can be proved.\nCombining Lemma3.3 and (18),We can get\n\u2329\n\u03b8 \u2217 \u2212 \u03b8t, Bt\n\u232a \u2264 \u2212\u03bb 2 \u2225 \u2225\u03b8 t \u2212 \u03b8\u2217 \u2225 \u2225 2 l2 (25)\nAssume that k is the largest element of K[xi] , y is the largest element of Y\nLemma 3.4. \u2225 \u2225\u03b8 t \u2225 \u2225 l2 \u2264 yk\n\u03bbI\nProof. We utilize mathematical induction to prove it. When t = 1, the correctness of lemma 2.4 is obvious.\nWhen t = t1, we assume that \u2225 \u2225\u03b8 t1 \u2225 \u2225 l2 \u2264 yk\n\u03bbl is correct.\nSo When t = t1 + 1,\n\u2225 \u2225\u03b8 t1+1 \u2225 \u2225 l2 \u2264 (1\u2212 \u03bb\u03b7t) \u2225 \u2225\u03b8 t \u2225 \u2225 l2 + \u2225 \u2225 \u2225 \u03b7t\n\u03c9\n\u2211\u03c9 i=1 yiK[xi]\n\u2225 \u2225 \u2225\nl2\n\u2264 yk \u03bbl \u2212 \u03bb\u03b7t yk \u03bbl + \u03b7t yk l = yk\n\u03bbl (26)\nAbove all, Lemma 2.4 can be proved.\nLemma 3.5.\n\u2225 \u2225 \u2225(\u03b8tTK[xi]\u2212 yi)K[xi] \u2225 \u2225 \u2225 l2 \u2264 yk\n3 \u03bb + \u221a lyk (27)\nProof. \u2225\n\u2225 \u2225(\u03b8tTK[xi]\u2212 yi)K[xi] \u2225 \u2225 \u2225\nl2 \u2264\n\u2225 \u2225 \u2225\u03b8 tT K[xi] \u2225 \u2225 \u2225\n\u221e\n\u2225 \u2225K[xi] \u2225 \u2225\nl2\n+ y \u2225 \u2225K[xi] \u2225 \u2225\nl2\n\u2264 lk2 \u2225 \u2225\u03b8 t \u2225 \u2225\nl2 + y\n\u221a lk\n\u2264 lk2 yk \u03bbl\n+ y \u221a lk\n= yk3 \u03bb + \u221a lyk\n(28)\nTherefore,\n\u2225 \u2225Bt \u2225 \u2225 2 l2 \u2264 ( 1\n\u03c9\n\u03c9 \u2211\ni=1\n\u2225 \u2225 \u2225(\u03b8TK[xi]\u2212 yi)K[xi] \u2225 \u2225 \u2225\nl2 + \u03bb\n\u2225 \u2225\u03b8 t \u2225 \u2225\nl2 )2\n\u2264 (yk 3 \u03bb + \u221a lyk + \u03bb yk \u03bbl )2\n(29)\nSo,combing (15) (25) (29),we can get\n\u2225 \u2225\u03b8 t+1 \u2212 \u03b8\u2217 \u2225 \u2225 2 l2 \u2264 (1\u2212 \u03bb\u03b7t) \u2225 \u2225\u03b8 t \u2212 \u03b8\u2217 \u2225 \u2225 2 l2\n+ \u03b72t ( yk3 \u03bb + \u221a lyk + \u03bb yk \u03bbl )2\n(30)\nSo this algorithm is a linear convergence of Q-th."}], "references": [], "referenceMentions": [], "year": 2014, "abstractText": "Currently, many machine learning algorithms contain lots of iterations. When it comes to existing large-scale distributed systems, some slave nodes may break down or have lower efficiency. Therefore traditional machine learning algorithm may fail because of the instability of distributed system.We presents a hybrid approach which not only own a high fault-tolerant but also achieve a balance of performance and efficiency.For each iteration, the result of slow machines will be abandoned. First, we discuss the relationship between accuracy and abandon rate. Then we debate the convergence speed of this process. Finally, our experiments demonstrate our idea can dramatically reduce calculation time and be used in many platforms.", "creator": "LaTeX with hyperref package"}}}