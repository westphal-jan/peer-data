{"id": "1411.8003", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Nov-2014", "title": "Guaranteed Matrix Completion via Non-convex Factorization", "abstract": "matrix factorization is a popular approach for large - element scale matrix completion and constitutes a very basic component of many solutions for netflix prize competition. in this approach, representing the unknown low - rank matrix is expressed as the product root of two much smaller dimension matrices exactly so that the low - rank property guarantee is automatically fulfilled. the resulting optimization problem, especially even teams with huge size, can be solved ( to stationary points ) very efficiently through performing standard optimization algorithms such as alternating minimization ( and stochastic gradient descent ( overlapping sgd ). however, due to the non - convexity caused by the factorization model, there essentially is a limited theoretical understanding of whether these analysis algorithms will generate a good solution. in this paper, we establish nearly a theoretical guarantee maxim for the factorization choice based formulation to correctly recover the underlying low - rank matrix. in particular, we essentially show that under similar conditions to those in three previous works, many standard optimization algorithms converge to the real global optima of the factorization based formulation, thus recovering fully the assumption true low - rank matrix. to the presumed best of our knowledge, our result is the first one that provides recovery trajectory guarantee for many standard algorithms such as gradient descent, sgd and block coordinate operator gradient descent. our result also applies to alternating minimization, and a notable difference from previous studies on alternating minimization is that we don't need the resampling scheme ( checking i. e. using many independent samples in each iteration ).", "histories": [["v1", "Fri, 28 Nov 2014 20:52:47 GMT  (353kb,D)", "https://arxiv.org/abs/1411.8003v1", "65 pages, 6 figures"], ["v2", "Fri, 29 May 2015 22:31:49 GMT  (289kb,D)", "http://arxiv.org/abs/1411.8003v2", "76 pages, 6 figures, a linear convergence result added, extended discussion on resampling"], ["v3", "Tue, 11 Oct 2016 17:35:12 GMT  (714kb,D)", "http://arxiv.org/abs/1411.8003v3", "77 pages. Accepted to IEEE Transaction on Information theory. A detailed description of the proof ideas is added, compared to version 2"]], "COMMENTS": "65 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ruoyu sun", "zhi-quan luo"], "accepted": false, "id": "1411.8003"}, "pdf": {"name": "1411.8003.pdf", "metadata": {"source": "CRF", "title": "Guaranteed Matrix Completion via Non-convex Factorization", "authors": ["Ruoyu Sun", "Zhi-Quan Luo"], "emails": ["sunxx394@umn.edu."], "sections": [{"heading": "1 Introduction", "text": "In the era of big data, there has been an increasing need for handling the enormous amount of data generated by mobile devices, sensors, online merchants, social networks, etc. Exploiting low-rank structure of the data matrix is a powerful method to deal with \u201cbig data\u201d. One prototype example is the low rank matrix completion problem in which the goal is to recover an unknown low rank matrix M \u2208 Rm\u00d7n for which only a subset of its entries Mi j, (i, j) \u2208 \u2126 \u2286 {1, 2, . . . ,m} \u00d7 {1, 2, . . . , n} are specified. Matrix completion has found numerous applications in various fields such as recommender systems [1], computer vision [2] and system identification [3], to name a few.\nThere are two popular approaches to impose the low-rank structure: the nuclear norm based approach and the matrix factorization (MF) based approach. In the first approach, the whole matrix is the optimization variable and the nuclear norm (denoted as \u2016 \u00b7 \u2016\u2217) of this matrix variable, which can be viewed as a convex approximation of its rank, serves as the objective function or a regularization term. For the matrix completion problem, the nuclear norm based formulation becomes either a linearly constrained minimization problem [4]\nmin Z\u2208Rm\u00d7n \u2016Z\u2016\u2217, s.t. Zi j = Mi j, \u2200 (i, j) \u2208 \u2126, (1)\na quadratically constrained minimization problem\nmin Z\u2208Rm\u00d7n\n\u2016Z\u2016\u2217, s.t. \u2211\n(i, j)\u2208\u2126 (Zi j \u2212 Mi j)2 \u2264 , (2)\n\u2217R. Sun is with the Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455. Email: sunxx394@umn.edu. \u2020Z.-Q. Luo is with the Chinese University of HongKong, Shenzhen, China. He is also affiliated with the Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455. Email: luozq@cuhk.edu.cn.\nar X\niv :1\n41 1.\n80 03\nv3 [\ncs .L\nG ]\n1 1\nO ct\n2 01\n6\nor a regularized unconstrained problem\nmin Z\u2208Rm\u00d7n\n\u2016Z\u2016\u2217 + \u03bb \u2211\n(i, j)\u2208\u2126 (Zi j \u2212 Mi j)2. (3)\nOn the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4\u20137]. This result was later generalized to noisy matrix completion, whereby the optimization formulation (2) is adopted [8]. Using a different proof framework, reference [9] provided theoretical guarantee for a variant of the formulation (3). On the computational side, problems (1) and (2) can be reformulated as a semidefinite program (SDP) and solved to global optima by standard SDP solvers when the matrix dimension is smaller than 500. To solve problems with larger size, researchers have developed first order algorithms, including the SVT (singular value thresholding) algorithm for the formulation (1) [10], and several variants of the proximal gradient method for the formulation (3) [11, 12] . Although linear convergence of the proximal gradient method has been established for the formulation (3) under certain conditions [13, 14], the per-iteration cost of computing SVD (Singular Value Decomposition) may increase rapidly as the dimension of the problem increases, making these algorithms rather slow or even useless for problems of huge size. The other major drawback is the memory requirement of storing a large m by n matrix.\nIn the second approach, the unknown rank r matrix is expressed as the product of two much smaller matrices XYT , where X \u2208 Rm\u00d7r,Y \u2208 Rn\u00d7r, so that the low-rank requirement is automatically fulfilled. Such a matrix factorization model has long been used in PCA (principle component analysis) and many other applications [15]. It has gained great popularity in the recommender systems field and served as the basic building block of many competing algorithms for the Netflix Prize [1, 16] due to several reasons. First, the compact representation of the unknown matrix greatly reduces the per-iteration computation cost as well as the storage space (requiring essentially linear storage of O((m + n)r) for small r). Second, the per-iteration computation cost is rather small and people have found in practice that huge size optimization problems based on the factorization model can be solved very fast. Third, as elaborated in [1], the factorization model can be easily modified to incorporate additional application-specific requirements.\nA popular factorization based formulation for matrix completion takes the form of an unconstrained regularized square-loss minimization problem [1]:\nP0 : min X\u2208Rm\u00d7r ,Y\u2208Rn\u00d7r 1 2 \u2211 (i, j)\u2208\u2126 [Mi j \u2212 (XYT )i j]2 + \u03bb(\u2016X\u20162F + \u2016Y\u20162F). (4)\nThere are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XYT )i j]2 can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22]. Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24]. Alternating minimization is easily parallelizable but has higher per-iteration computation cost than SGD; in contrast, SGD requires little computation per iteration, but its parallelization is challenging. Recently several parallelizable variants of the SGD [25\u201327] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed. Some of these algorithms have been tested in distributed computation platforms and can achieve good performance and high efficiency, solving very large problems with more than a million rows and columns in just a few minutes."}, {"heading": "1.1 Our contributions", "text": "Despite the great empirical success, the theoretical understanding of the algorithms for the factorization based formulation is fairly limited. More specifically, the fundamental question of whether these algorithms (including\nmany recently proposed ones) can recover the true low-rank matrix remains largely open. In this paper, we partially answer this question by showing that under similar conditions to those used in previous works, many standard optimization algorithms for a factorization based formulation (see (18)) indeed converge to the true low-rank matrix (see Theorem 3.1). Our result applies to a large class of algorithms including gradient descent, SGD and many block coordinate descent type methods such as two-block alternating minimization and block coordinate gradient descent. We also show the linear convergence of some of these algorithms (see Theorem 3.2 and Corollary 3.2).\nTo the best of our knowledge, our result is the first one that analyzes the geometry of matrix factorization in Euclidean space for matrix completion. In addition, our result also provides the first recovery guarantee for alternating minimization without resampling (i.e. without using independent samples in different iterations). Below we elaborate these two contributions in light of the existing works.\n1) We analyze the local geometry of the matrix factorization formation (in Euclidean space). We argue that the success of many algorithms attributes mostly (or at least partially) to the geometry of the problem, rather than the specific algorithms being used. The geometrical property we establish is that the local gradient direction \u2212\u2207 f (x) is aligned with the global descent direction x\u2217 \u2212 x. For the classical matrix factorization formulation \u2016M \u2212 XYT \u20162F , we develop a novel perturbation analysis to deal with the ambiguity of the factorization. For the sampling loss \u2016P\u2126(M \u2212 XYT )\u20162F , an incoherence regularizer (or constraint) is needed, which causes an extra difficulty of analyzing nonconvex constrained optimization. Unfortunately, projection to the constraint (or the gradient of the regularizer) is not aligned with the global direction, and we add one more regularizer to \u201ccorrect\u201d the local descent direction. A high-level lesson is that regularization may change the geometry of the problem.\n2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17\u201320]. We obtain a sample complexity bound that is independent of the recovery error , while all previous sample complexity bounds for the matrix factorization based formulation (in Euclidean space) depend on . There is a subtle theoretical issue for the resampling scheme; see more discussions in Section 1.2 and [30, Sec. 1.5.3]."}, {"heading": "1.2 Related works", "text": "Factorization models. The first recovery guarantee for the factorization based matrix completion is provided in [31], where Keshavan, Montanari and Oh considered a factorization model in Grassmannian manifold and showed that the matrix can be recovered by a proper initialization and a gradient descent method on Grassmannian manifold. Besides being quite complicated, this model is not as flexible as the factorization model in Euclidean space, and it is not easy to solve by many advanced large-scale optimization algorithms. Moreover, most algorithms in Grassmann manifold require line search, and little is known about the convergence rate.\nThe factorization model in Euclidean space was first analyzed in an unpublished work [17] of Keshavan 1, as well as a later work of Jain et al. [18]. Both works considered alternating minimization with resampling scheme, a special variant of the original alternating minimization. The sample complexity bounds were later improved by Hardt [19] and Hardt and Wooters [20], where in the latter work, notably, the authors devised an algorithm with a corresponding sample complexity bound independent of the condition number. However, these improvements are obtained for more sophisticated versions of resampling-based alternating minimization, not the typical alternating minimization algorithm.\nResampling. The issues of resampling have been discussed in a recent work on phase retrieval by Cande\u0300s et al. [32]. We will point out a subtle theoretical issue not mentioned in [32], as well as some other practical issues.\n1 Reference [17] is a PhD thesis that discusses various algorithms including the algorithm proposed in [31] and alternating minimization. In this paper when we refer to [17], we are only referring to [17, Ch. 5] which presents resampling-based alternating minimization and the corresponding result.\nThe resampling scheme (a.k.a. golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues. At first, it may seem that for both approaches resampling is a cheap way to get around a common difficulty: the dependency of the iterates on the sample set. However, there is a crucial difference: for the nuclear norm approach, resampling is just a proof technique used in a \u201cconceptual\u201d algorithm for constructing the dual certificate, while for the alternating minimization, resampling is used in the actual algorithm. This difference causes some issues of resampling-based alternating minimization at conceptual, practical and theoretical levels.\n1) Gap between theory and algorithm. Algorithmically, an easy resampling scheme is to randomly partition the given set \u2126 into non-overlapping subsets \u2126k, k = 1, . . . , L, as proposed in [17,18] 2. However, the results in [17\u201320] actually require a generative model of independent \u2126k\u2019s, instead of sampling \u2126k\u2019s based on a given \u2126. Therefore, the results in [17\u201320] do not directly apply to the partition based resampling scheme that is easy to use. See [30, Sec. 1.5.3] for more discussions on this subtle issue.\nThis issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17\u201320] can apply, provided that the generative model of \u2126 is exactly known. In practice, the underlying generative model of \u2126 is usually unknown, in which case the scheme [20, Algorithm 6] does not work. In contrast, the classical results in [4\u20137] and our result herein are robust to the generative model of \u2126: these results actually state that for an overwhelming portion of \u2126 with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of \u2126 a high probability result holds.\n2) Impracticality. As argued previously, assuming a generative model of \u2126k\u2019s is not practical since \u2126 is usually given. For given \u2126, the only known validated resampling scheme [20, Algorithm 6], besides not being robust to the underlying generative model of \u2126, might be a bit complicated to use in practice. Even the simple resampling scheme of partitioning \u2126 (which has not been validated yet) is rather unrealistic since each sample is used only once during the algorithm.\n3) Inexact recovery. A theoretical consequence of the resampling scheme is that the required sample complexity |\u2126| becomes dependent on the desired accuracy , and goes to infinity as goes to zero. This is different from the classical results (and ours) where exact reconstruction only requires finite samples. While it is common to see the dependency of time complexity on the accuracy , it is relatively uncommon to see the dependency of sample complexity on .\nIn a recent work [34] the authors have managed to remove the dependency of the required sample size on by using a singular value projection algorithm. However, [34] considers a matrix variable of the same size as the original matrix, which requires significantly more memory than the matrix factorization approach considered in this paper. Moreover, it requires resampling at a number of iterations (though not all), which may suffer from the same issues we mentioned earlier. The resampling is also required in the recent work of [35]; see [30, Sec. 1.5.3] for more discussions.\nOther works on non-convex formulations. Non-convex formulation has also been studied for the phase retrieval problem in some recent works [32, 36]. These works provide theoretical guarantee for some algorithms specially tailored to certain non-convex formulations and with specific initializations. The major difference between [36] and [32] is that the former requires independent samples in each iteration, while the latter uses the same samples throughout in the proposed algorithm. As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17\u201320] and our work.\nFinally, we note that there is a growing list of works on the theoretical guarantee of non-convex formulations for various problems, such as sparse regression (e.g. [37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected-\n2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u2126k\u2019s with replacement; anyhow, under this model \u2126k\u2019s are still dependent. See [30, Sec. 1.5.3] for more discussions.\nMaximization) algorithm [43, 44]. We emphasize several aspects that distinguish our paper from other recent works on non-convex optimization. First, our paper is one of the first to analyze the (local) geometry of the problem. Second, we deal with non-symmetric matrix factorization which has a more bizarre geometry than symmetric matrix factorization and some other models. Third, one difficulty of our problem essentially lies in nonconvex constrained optimization (though we consider the closely related regularized form)."}, {"heading": "1.3 Proof Overview and Techniques", "text": "Basic idea: local geometry. The very first question is what kind of property can ensure global convergence for non-convex optimization. We will establish a local geometrical property of a regularized objective such that any stationary point in a local region is globally optimal. This is achieved in three steps: (i) study the local geometry of the fully observed objective \u2016M \u2212 XYT \u20162F ; (ii) study the local geometry of the matrix completion objective \u2016P\u2126(M \u2212 XYT )\u20162F ; (iii) study the local geometry of a regularized objective. Next, we will discuss the difficulties involved in each step and describe how we address these difficulties.\nLocal geometry of \u2016M \u2212 XYT \u20162F . We start by considering a simple case that M is fully observed and the objective function is f (X,Y) = \u2016M \u2212 XYT \u20162F . What is the geometrical landscape of this function? In the simplest case m = n = r = 1 and f (x, y) = (xy \u2212 1)2, the set of stationary points is {(x, y) | xy = 1} \u222a {(0, 0)}, in which (0, 0) is a saddle point and the curve xy = 1 consists of global optima. We plot the function around the curve xy = 1 in the positive orthant in Figure 1.\nClearly a certain geometrical property prevents bad local minima in the neighborhood of the global optima, but what kind of property? We emphasize that the property can not be local convexity because the set of global optima is non-convex in R2. Due to the intrinsic symmetry that f (x, y) = f (xq, yq\u22121), only the product z = xy affects the value of f . We hope that the strong convexity of (1 \u2212 z)2 can be partially preserved when z is reparameterized into z = xy. It turns out we can prove the following local convexity-type property: for any (x, y) such that xy is close to 1 and |x|, |y| are upper bounded, there exists uv = 1 such that\n\u3008\u2207 f (x, y), (x, y) \u2212 (u, v)\u3009 \u2265 c\u2016(x, y) \u2212 (u, v)\u20162.\nAn interpretation is that the negative gradient direction \u2212\u2207 f should be aligned with the global direction (u, v)\u2212(x, y); a convex function has a similar property, but the difference is that here the global direction is adjusted according to the position of (x, y).\nFor general m, n, r, the geometrical landscape is probably much more complicated than the scalar case. Nevertheless, we can still prove that the convexity of \u2016M \u2212 Z\u20162 is partially preserved when reparameterizing Z as Z = XY . The exact expression is a variant of (6) which we will discuss in more detail later. Technically, we need to connect the Euclidean space and the quotient manifold via \u201ccoupled perturbation analysis\u201d: given X,Y such that \u2016XYT \u2212M\u2016F is small, find decomposition M = UVT such that U,V are close to X and Y respectively (a simpler version of Proposition 4.1). The difference from traditional perturbation analysis of Wedin [45] (i.e. if two matrices are close then their row/column spaces are close) is that in [45] the row/column spaces are fixed while in our problem U,V are up to our choice.\nLocal geometry of \u2016P\u2126(M \u2212 XYT )\u20162F . Let us come back to the original matrix completion problem, in which an additional sampling operator P\u2126 is introduced. Similarly, we hope that f\u2126(Z) = 12 \u2016P\u2126(M \u2212 Z)\u20162 is strongly convex and this strong convexity can be partially preserved after reparametrization Z = XYT . However, one issue is that the function f\u2126(Z) is possibly non-strongly-convex (though still convex). In fact, if f\u2126 is locally strongly convex around M, then we should have\nf\u2126(Z) \u2212 f\u2126(M) \u2265 O(\u2016Z \u2212 M\u20162F),\u2200 Z close to M.\nAssuming Z is rank-r, this inequality can be rewritten as\n\u2016P\u2126(M \u2212 XYT )\u20162F \u2265 Cp\u2016M \u2212 XYT \u20162F , \u2200(X,Y) \u2208 K(\u03b4), (5)\nwhere K(\u03b4) is a neighborhood of M defined as {(X,Y) | \u2016XYT \u2212M\u2016F \u2264 \u03b4} and C is a numerical constant. We wish (5) to hold with high probability (w.h.p.) for random \u2126 in which each position in M is chosen with probability p. This inequality is closely related to matrix RIP (restricted isometry property) in [8] (see equation (III.4) therein). If X,Y are independent of \u2126, then (5) follows easily from the concentration inequalities. Unfortunately, if X,Y are chosen arbitrarily instead of independently from \u2126, the bound (5) may fail to hold.\nA solution, as employed in [31], is to utilize a random graph lemma in [46] which provides a bound on \u2016P\u2126(A)\u2016F for any rank-1 matrix A (possibly dependent on \u2126). This lemma, combined with another probability result in [4], implies a bound on \u2016P\u2126(M \u2212 XYT )\u2016F . However, this bound is not good enough since it only leads to (5) when \u03b4 = O(1/n). The underlying reason is that the bound given by the random graph lemma is actually quite loose if X or Y have unbalanced rows, i.e. certain row has large norm. One solution is to force the iterates to have bounded row norms (a.k.a. incoherent), by adding a constraint or regularizer. With the incoherence requirement on X,Y , now (5) can be shown to be hold for \u03b4 = O(1), or more precisely, \u03b4 = O(\u03a3min), where \u03a3min is the minimum eigenvalue of M. With such a \u03b4, it is possible to find an initial point in the region K(\u03b4).\nIn summary, although f\u2126(Z) = 12 \u2016P\u2126(Z \u2212 M)\u20162F is possibly non-strongly-convex, by restricting to an incoherent neighborhood of M it is \u201crelative\u201d strongly convex (called \u201crelative\u201d since we fix M in (5)). More specifically, we have that w.h.p.\n\u2016P\u2126(M \u2212 XYT )\u20162F \u2265 Cp\u2016M \u2212 XYT \u20162F , \u2200(X,Y) \u2208 B , K(\u03b4) \u2229 K1. (6)\nwhere K1 denotes the set of (X,Y) with bounded row norms. Note that this inequality also implies that global optimally in B leads to exact recovery; or equivalently, zero training error leads to zero generalization error.\nHaving established the geometry of f\u2126(Z), we can use the same technique for the fully observed case to show the local geometry 3 of\nF(X,Y) , f\u2126(X,Y) = 1 2 \u2016P\u2126(M \u2212 XYT )\u20162F .\n3 For illustration purpose, we present a two-step approach: first establish a geometrical property of f\u2126(Z), then extend the property to f\u2126(XYT ). However, our current proof does not follow the two-step approach but directly establish the property of f\u2126(XYT ). In fact, although we establish the property of f\u2126(Z) in Claim 3.1, the proof of this claim is very similar to the proof of (7).\nMore specifically, we can prove that for any (X,Y) \u2208 B, there exists (U,V) \u2208 X\u2217 = {(U,V) \u2208 Rm\u00d7r \u00d7 Rn\u00d7r | UVT = M} such that\n\u3008\u2207XF(X,Y), X \u2212 U\u3009 + \u3008\u2207Y F(X,Y),Y \u2212 V\u3009 \u2265 c(\u2016X \u2212 U\u20162F + \u2016Y \u2212 V\u20162F). (7)\nDenoting x = (X,Y),x\u2217 = (U,V) and utilizing \u2207F(x\u2217) = 0, (7) becomes\n\u2200 x \u2208 B, \u2203 x\u2217 \u2208 X\u2217, s.t. \u3008\u2207F(x) \u2212 \u2207F(x\u2217),x \u2212 x\u2217\u3009 \u2265 c\u2016x \u2212 x\u2217\u20162. (8)\nIt links the local optimality measure \u2016\u2207F(x)\u2016 with the global optimality measure dist(x,X\u2217) = minx\u2217\u2208X\u2217 \u2016x \u2212 x\u2217\u2016, and implies that any stationary point of F in B is a global minimum.\nIf (8) holds for arbitrary x,x\u2217 then F would be strongly convex in x. Let us emphasize again two differences of (8) with local strong convexity: i) since x\u2217 is not arbitrary but has to be one global minimum, (8) indicates local \u201crelative convexity\u201d of F; ii) due to the ambiguity of factorization, x\u2217 should be chosen according to x, thus (8) indicates local relative convexity up to a group transformation (it might be conceptually helpful to view it as a property in the quotient manifold, but we do not explicitly exploit its structure).\nLocal geometry with regularizers/constraints. The property (8) is still not desirable. The original purpose of studying geometry is to show there is no spurious \u201c1st order local-min\u201d (point that satisfies 1st order optimality conditions). To establish the geometrical property with sampling, we restrict to an incoherent set K1, but this restriction changes the meaning of the 1st order local-min. In fact, to ensure the iterates stay in the incoherent region K1, we need to solve a constrained optimization problem minx\u2208K1 F(x) or a regularized problem minx F(x) +G1(x) where G1 is a regularizer forcing x to be in K1. Standard optimization algorithms converge to the KKT points of minx\u2208K1 F(x) or the stationary points of F + G1, which may not be the stationary points of F. The property (8) only implies any stationary point of F in B is globally optimal.\nWe shall focus on the regularized problem min F + G1; the constrained problem minx\u2208K1 F is similar. Because of the extra regularizer, the property (8) is not enough. We need to prove a result similar to (8), but with \u2207F replaced by \u2207F + \u2207G1:\n\u2200 x \u2208 B, \u2203 x\u2217 \u2208 X\u2217, s.t. \u3008\u2207F(x) + \u2207G1(x),x \u2212 x\u2217\u3009 \u2265 c\u2016x \u2212 x\u2217\u20162. (9)\nIf it happens to be the case that \u3008\u2207G1(x),x \u2212 x\u2217\u3009 \u2265 0, (10)\nthen combining with the existing result (8) we are done; unfortunately, we do not know how to prove (10). Intuitively, (10) means that \u2212\u2207G1(x), which is almost the same direction as the projection to the incoherent region K1, is positively correlated with the global direction x\u2217 \u2212 x. At first sight, this seems trivially true because for any point x\u0304 \u2208 K1 we have \u3008\u2207G1(x),x \u2212 x\u0304\u3009 \u2265 0 (as illutrated in Fig. 2). However, a rather strange issue is that x\u2217 is chosen to be a point in {(U,V) | UVT = M} that is close to x, thus there is no guarantee that x\u2217 lies in K1. An underlying reason is that the global optimum set {(U,V) | UVT = M} is unbounded and thus not a subset of K1. If we enforce (U,V) to be in K1, we may not be able to find (U,V) that is close enough to (X,Y).\nTechnically, the issue is that (U,V) chosen in Proposition 4.1 have row-norms bounded above by quantities proportional to the norms of X,Y , and can be higher than the row-norms of X,Y (threshold of K1). To resolve this issue, we add an extra regularizer G2(X,Y) to force (X,Y) to lie in K2, a set of matrix pairs with bounded norms. This extra bound makes \u3008\u2207G1(x),x \u2212 x\u2217\u3009 \u2265 0 straightforward to prove, but a similar issue arises: now we need to prove (8) for F + G1 + G2 instead of F. Again, it suffices to prove that for any x \u2208 K(\u03b4) \u2229 K1 \u2229 K2 there exists x\u2217 such that\n\u3008\u2207G2(x),x \u2212 x\u2217\u3009 \u2265 0. (11)\nThis is what we prove as outlined next.\nConstrained perturbation analysis. The desired inequality (11) is implied by the following condition on U,V: \u2016U\u2016F \u2264 \u2016X\u2016F , \u2016V\u2016F \u2264 \u2016Y\u2016F when \u2016X\u2016F , \u2016Y\u2016F are large. Recall that previously we try to find U,V that are close to X,Y; see Proposition 4.1. Now we need to impose extra constraints on U,V , giving rise to Proposition 4.2. The extra constraints make the perturbation analysis significantly more involved; in fact, we apply a sophisticated iterative procedure to construct the factorization M = UVT . The main steps of the proof are briefly given in Appendix C.2.\nOne crucial component of our proof can be viewed as the perturbation analysis for \u201cpreconditioning\u201d. Roughly speaking, the basic problem is: given an r \u00d7 r matrix X\u0302 with a large condition number, find another matrix U\u0302 with the same Frobenius norm as X\u0302 but smaller inverse Frobenious norm (i.e. \u2016U\u0302\u22121\u2016F \u2264 11\u2212\u03b4\u2016X\u0302\u22121\u2016F). In other words, we want to reduce \u2211r i=1\n1 \u03c32i\nwith \u2211r\ni=1 \u03c3 2 i fixed, where \u03c3i\u2019s are all singular values. Intuitively, by reducing \u2211r i=1\n1 \u03c32i we reduce the discrepancy of singular values. This process is somewhat similar to preconditioning in numerical algebra that reduces the gap between the largest and smallest eigenvalue. The precise statement of the basic problem and its relation with the key technical result Proposition 4.2 are provided in Appendix C.2.1.\nAlgorithm requirements. We provide three conditions and show that if an algorithm satisfies either of them, then with specific initialization the iterates will stay in the desired basin (see Proposition 5.1). A special case of the third condition has been used in [31] for Grassmann manifold optimization. Together, these three conditions cover a wide spectrum of algorithms including GD, SGD and block coordinate descent type methods.\nProof outline. The overall proof can be divided into two parts: the geometrical property (Lemma 3.1) and the algorithm property (Lemma 3.2). For the geometrical property, Lemma 3.1 states that the regularized objective function F + G1 + G2 enjoys some nice geometrical property in a certain local region around the global optima, thus there is no other stationary point in this region. For the algorithm property, Lemma 3.2 states that starting from an easily computable initial point, many standard algorithms generate a sequence that are inside the desired region and these algorithms also converge to stationary points. Since these stationary points must be global optima by Lemma 3.1, we obtain that these algorithms converge to the global optima."}, {"heading": "1.4 Other Remarks", "text": "Difference with previous works. As discussed earlier, one major challenge is to bound P\u2126(A) when A may be dependent on \u2126. One simple strategy as adopted in [17\u201320] is to use a resampling scheme to decouple A and the observation set. This strategy artificially avoids this difficulty, and causes a few issues discussed earlier in Section 1.2. Another strategy, as employed in [31], is to use a random graph lemma in [46].\nWe apply the random graph lemma of [46] when extending the local geometry of \u2016M\u2212XYT \u20162F to \u2016P\u2126(M\u2212XYT )\u20162F .\nThe difference of our work with [31] is that we study the local geometry in Euclidean space (and, indirectly, the geometry of the quotient manifold), which is quite different from the local geometry in Grassmann manifold studied in [31]. Technically, the complications of the proof in [31] are mostly due to heavy computation of various quantities in Grassmann manifold; in addition, much effort is spent in estimating the terms related to the extra factor S which enables the decoupling of X and Y ( [31] actually uses a three-factor decomposition XS YT ). For our problem, one difficulty is to \u201cpull back\u201d the distance in the quotient manifold to the Euclidean space, by the coupled perturbation analysis. Another difficulty is to align the gradient of the regularizer with the global direction (this is not an issue for Grassman manifold), which requires a more sophisticated perturbation analysis. The difficulties have been discussed in detail in Section 1.3.\nSymmetric PSD or rank-1 case. The symmetric PSD (positive semi-definite) case or the rank-1 case are easier to deal with, because in the 3-step study of the local geometry the third step is not necessary. When M is rank-1 (possibly non-symmetric), the regularizer G2(\u00b7) may still be needed, but Proposition 4.2 is trivial since its assumptions cannot hold for r = 1. When M is symmetric PSD, a popular approach is to use a symmetric factorization M = XXT instead of the non-symmetric factorization, and the loss function becomes \u2016P\u2126(M\u2212XXT )\u20162F . The same proof in our paper can be translated to this symmetric PSD case, except that the third step is not necessary. In fact, it is possible to show that (10) holds without any additional requirement on x. As a result, the regularizer G2 and a major technical result Proposition 4.2 are not needed. In both the symmetric PSD and rank-1 case, we only need to establish the intermediate result (7) and the proof can be greatly simplified. Stronger sample complexity and time complexity bounds may be established in these two cases.\nSimulation Results The regularizers are introduced due to theoretical purposes; interestingly, they turn out to be helpful in the numerical experiments (the comments below are extracted from the thesis [30, Chapter 2]).\nFirst, the simulation suggests that the imbalance of the rows of X or Y is an important issue for matrix completion in practice, a phenomenon not reported before to our knowledge. The table in Figure 2.10 of [30] shows that when |\u2126| is small, in all successful instances the iterates are balanced, while in all failed instances the iterates are unbalanced. This contrast occurs for many standard algorithms such as AltMin,GD and SGD.\nSecond, adding only the regularizer G1 helps, but not too much. Adding an extra regularizer G2 can push the sample complexity to be very close to the fundamental limit, at least for the synthetic Gaussian data. These experiments seem to indicate that the new regularizers do change the geometry of the problem.\nNecessity of incoherence? While our regularizers are helpful when |\u2126| is small, an open question is whether the row-norm requirement is needed for the local geometry when |\u2126| is large. We observe that the row-norms can be automatically controlled by standard algorithms for the synthetic Gaussian data when there are, say, 5rn samples for n \u00d7 n matrices. There are two possible explanations (assuming a large |\u2126|): (i) the local geometrical property (7) holds without the incoherence requirement; (ii) (7) still requires incoherence, but there is an unknown mechanism for many algorithms to control the row-norms.\nTo exclude the first possibility, we need to find (X,Y) \u2208 K(\u03b4) such that \u2207F(X,Y) = 0 but XYT , M; since (7) holds, such (X,Y) must have unbalanced row-norms. Such an example would validate the necessity of the incoherence restriction for the local geometry. Note that the necessity of incoherence for the local geometry is different from the necessity of an incoherence regularizer/constraint for a specific algorithm. Even if the local geometry requires incoherence, it remains an interesting question why many algorithms can automatically control row-norms when |\u2126| is large."}, {"heading": "1.5 Notations and organization", "text": "Notations. Throughout the paper, M \u2208 Rm\u00d7n denotes the unknown data matrix we want to recover, and r min{m, n} is the rank of M. The SVD of M is M = U\u0302\u03a3V\u0302T , where U\u0302 \u2208 Rm\u00d7r, V\u0302 \u2208 Rn\u00d7r and \u03a3 \u2208 Rr\u00d7r is a diagonal matrix with diagonal entries \u03a31 \u2265 \u03a32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03a3r. We denote the maximum and minimum singular value as \u03a3max and \u03a3min, respectively, and denote \u03ba , \u03a3max/\u03a3min as the condition number of M. Define \u03b1 = m/n, which is assumed to be bounded away from 0 and\u221e as n \u2212\u2192 \u221e. Without loss of generality, assume m \u2265 n, then \u03b1 \u2265 1.\nDefine the short notations [m] , {1, 2, . . . ,m}, [n] , {1, 2, . . . , n}. Let \u2126 \u2286 [m] \u00d7 [n] be the set of observed positions, i.e. {Mi j | (i, j) \u2208 \u2126} is the set of all observed entries of M, and define p , |\u2126|mn which can be viewed as the probability that each entry is observed. For a linear subspace S, denote PS as the projection onto S. By a slight abuse of notation, we denote P\u2126 as the projection onto the subspace {W \u2208 Rm\u00d7n : Wi, j = 0,\u2200(i, j) < \u2126}. In other words, P\u2126(A) is a matrix where the entries in \u2126 are the same as A while the entries outside of \u2126 are zero.\nFor a vector x \u2208 Rn, denote \u2016x\u2016 as its Euclidean norm. For a matrix X, denote \u2016X\u2016F as its Frobenius norm, and \u2016X\u20162 as its spectral norm (i.e. the largest singular value). Denote \u03c3max(X), \u03c3min(X) as the largest and smallest singular values of X, respectively. Let X\u2020 denote the pseudo inverse of a matrix X. The standard inner product between vectors or matrices are written as \u3008x, y\u3009 or \u3008X,Y\u3009, respectively. Denote A(i) as the ith row of a matrix A. We will use C,C1,CT ,Cd, etc. to denote universal numerical constants.\nOrganization. The rest of the paper is organized as follows. In Section 2 we introduce the problem formulation and four typical algorithms. In Section 3, we present the main results and the main lemmas used in the proofs of these results. The proof of the two lemmas used in proving Theorem 3.1 are given in Section 4 and Section 5 respectively. The proof of the first lemma depends on two \u201ccoupled perturbation analysis\u201d results Proposition 4.1 and Proposition 4.2, the proofs of which are given in Appendix B and Appendix C respectively. The proof of a lemma used in proving Theorem 3.2 is given in Appendix E."}, {"heading": "2 Problem Formulation and Algorithms", "text": ""}, {"heading": "2.1 Assumptions", "text": "Incoherence condition. The incoherence condition for the matrix completion problem is first introduced by Cande\u0300s and Recht in [4] and has become a standard assumption for low-rank matrix recovery problems (except a few recent works such as [47, 48]). We will define an incoherence condition for an m \u00d7 n matrix M which is the same as that in [31].\nDefinition 2.1 We say a matrix M = U\u0302\u03a3V\u0302T (compact SVD of M) is \u00b5-incoherent if: r\u2211\nk=1\nU\u03022ik \u2264 \u00b5r m , r\u2211 k=1 V\u03022jk \u2264 \u00b5r n , 1 \u2264 i \u2264 m, 1 \u2264 j \u2264 n. (12)\nIt can be shown that \u00b5 \u2208 [1, max{m,n}r ]. For some popular random models for generating M, the incoherence condition holds with a parameter scaling as \u221a r log n (see [31]). In this paper, we just assume that M is \u00b5-incoherent. Note that the incoherence condition implies that U\u0302, V\u0302 have bounded row norm. Throughout the paper, we also use the terminology \u201cincoherent\u201d to (imprecisely) describe m \u00d7 r or n \u00d7 r matrices that have bounded row norm (see the definition of set K1 in (30)).\nRandom sampling model. In the statement of the results in this paper, the probability is taken with respect to the uniform random model of \u2126 \u2286 [m]\u00d7 [n] with fixed size |\u2126| = S (i.e. \u2126 is generated uniformly at random from set\n{\u2126\u2032 \u2286 [m]\u00d7 [n] : the size of \u2126\u2032 is S } ). We remark that this model is \u201cequivalent to\u201d a Bernolli model that each entry of M is included into \u2126 independently with probability p = Smn in the sense that if the success of an algorithm holds for the Bernolli model with a certain p with high probability, then the success also holds for the uniform random model with |\u2126| = pmn with high probability (see [4] or [31, Sec. 1D] for more details). Thus in the proofs we will instead use the Bernolli model."}, {"heading": "2.2 Problem formulation", "text": "We consider a variant of (P0) with incoherence-control regularizers. In particular, we introduce two types of regularization terms besides the square loss function: the first type is designed to force the iterates Xk,Yk to be incoherent (i.e. with bounded row norm), and the second type is designed to upper bound the norm of Xk and Yk. Note that (P0) is related to the Lagrangian method, while our regularizer is based on the penalty function method for constrained optimization problems. We can also view the regularizer \u03bb(\u2016X\u20162F + \u2016Y\u20162F) as a \u201csoft regularizer\u201d, and our new regularizer as a \u201chard regularizer\u201d. The advantage of the hard regularizer is that it does not distort the optimal solution.\nOur regularizers are smooth functions with simple gradients, thus the algorithms for our formulation have similar per-iteration computation cost as the algorithms for the formulation without regularizers. In the numerical experiments, we find that when |\u2126| is large, the iterates are always incoherent and bounded, and our algorithms are the same as the traditional algorithms for the unregularized formulation; when |\u2126| is relatively small, the traditional algorithms may produce high error, and our regularizer becomes active and significantly reduce the error. In some sense, our algorithms for the new formulation are \u201cbetter\u201d versions of the traditional algorithms, and our theoretical results can be viewed as a validation of the traditional algorithms in the \u201clarge-|\u2126| regime\u201d and a validation of the modified algorithm in the \u201csmall-\u2126\u201d regime. Preliminary simulation results show that many algorithms for the proposed formulation can recover the matrix when |\u2126| is very close to the fundamental limit, significantly improving upon the traditional algorithms; see [30, Chapter 3].\nThe regularization function G is defined as follows:\nG(X,Y) , \u03c1 m\u2211\ni=1\nG0 3\u2016X(i)\u20162 2\u03b221  + \u03c1 n\u2211 j=1 G0 3\u2016Y ( j)\u20162 2\u03b222  +\u03c1G0\n3\u2016X\u20162F 2\u03b22T  + \u03c1G0 3\u2016Y\u20162F 2\u03b22T  , (13)\nwhere A(i) denotes the ith row of a matrix A,\nG0(z) , I[1,\u221e](z)(z \u2212 1)2 = max{0, z \u2212 1}2, (14)\n\u03b2T , \u221a CT r\u03a3max, \u03b21 , \u03b2T \u221a 3\u00b5r m = \u221a CT r\u03a3max \u221a 3\u00b5r m ,\n\u03b22 , \u03b2T \u221a 3\u00b5r n = \u221a CT r\u03a3max \u221a 3\u00b5r n .\n(15)\nHere, IC is the indicator function of a set C, i.e. IC(z) equals 1 when z \u2208 C and 0 otherwise. \u03c1 is a constant specified shortly. Throughout the paper, \u03b4 and \u03b40 are defined as\n\u03b4 , \u03a3min\nCdr1.5\u03ba , \u03b40 ,\n\u03b4 6 , (16)\nwhere Cd is some numerical constant. The coefficient \u03c1 is defined as (a larger \u03c1 also works)\n\u03c1 , 2p\u03b420\nG0(3/2) = 8p\u03b420. (17)\nThe numerical constant CT > 5 will be specified in the proof of our main result. The parameter \u03b2T is chosen to be of the same order as \u2016U\u0302\u03a31/2\u2016F and \u2016V\u0302\u03a31/2\u2016F , and \u03b21, \u03b22 are chosen to be of the same order as \u221a r\u2016(U\u0302\u03a31/2)(i)\u2016, \u221a r\u2016(V\u0302\u03a31/2)( j)\u2016. The additional factor \u221a 3r is due to technical consideration (to prove (256)). Our regularizer G involves \u03a3max and\n\u00b5 which depend on the unknown matrix M; in practice, we can estimate \u03a3max by c1 \u221a \u2016P\u2126(M)\u20162F\npr , and estimate \u00b5 by\nc2 \u221a\nmn r\u03a3max max(i, j)\u2208\u2126 |Mi j| (according to (203)) where c1, c2 are numerical constants to tune.\nIt is easy to verify that G0 is continuously differentiable. The choice of function G0 is not unique; in fact, we can choose any G0 that satisfies the following requirements: a) G0 is convex and continuously differentiable; b) G0(z) = 0, z \u2208 [0, 1]. In [31], G0 is chosen as G0(z) = I[1,\u221e](z)(e(z\u22121)\n2 \u2212 1), which also satisfies these two requirements. Choosing different G0 does not affect the proof except the change of numerical constants (which depend on G0(3/2),G\u20320(3/2),G \u2032\u2032 0 (3/2)). Note that the requirement of G0 being non-decreasing and convex guarantees the convexity of G(X,Y). In fact, according to the well-known result that the composition of a non-decreasing convex function and a convex function is a convex function, and notice that \u2016X(i)\u20162, \u2016Y ( j)\u20162, \u2016X\u20162F , \u2016Y\u20162F are convex, we have that each component of G is convex and thus G is convex.\nDenote the square loss term in (P0) as F(X,Y) , \u2211\n(i, j)\u2208\u2126[Mi j \u2212 (XYT )i j]2 = \u2016P\u2126(M \u2212 XYT )\u20162F . Replacing the objective function of (P0) by F\u0303(X,Y) , F(X,Y) + G(X,Y), we obtain the following problem:\nP1 : min X\u2208Rm\u00d7r ,Y\u2208Rn\u00d7r 1 2 \u2016P\u2126(M \u2212 XYT )\u20162F + G(X,Y). (18)\nWe remark that (P1) can be interpreted as the penalized version of the following constrained problem (see, e.g. [49])\nmin X,Y 1 2 \u2016P\u2126(M \u2212 XYT )\u20162F , s.t. \u2016X\u20162F \u2264 2 3 \u03b22T , \u2016Y\u20162F \u2264 2 3 \u03b22T ;\n\u2016X(i)\u20162 \u2264 2 3 \u03b221, \u2200 i, \u2016Y ( j)\u20162 \u2264 2 3 \u03b222, \u2200 j.\n(19)\nTo illustrate this, note that the constraint f1(X) , 3\u2016X\u20162F 2\u03b22T \u2212 1 \u2264 0 corresponds to the penalty term \u03c1G0( f1(X) + 1) = \u03c1max{0, f1(X)}2 which appears as the third term in G(X,Y), and similarly other constraints correspond to other terms in G(X,Y). In other words, the regularization function G(X,Y) is just a penalty function for the constraints of the problem (19). The function max{0, \u00b7}2 is a popular choice for the penalty function in optimization (see, e.g. [49]), which motivates our choice of G0 in (14). Our result can be extended to cover the algorithms for the constrained version (19), or a partially regularized formulation (e.g. only penalize the violation of the constraint \u2016X\u20162F \u2264 23\u03b22T , \u2016Y\u20162F \u2264 2 3\u03b2 2 T ).\nIt is easy to check that the optimal value of (P1) is zero and (X,Y) = (U\u0302\u03a31/2, V\u0302\u03a31/2) is an optimal solution to (P1), provided that M is \u00b5-incoherent. In fact, since F\u0303 is a nonnegative function, we only need to show F\u0303(X,Y) = 0 for this choice of (X,Y). As XYT = M implies \u2016P\u2126(M \u2212 XYT )\u20162F = 0, we only need to show G(X,Y) = G(U\u0302\u03a31/2, V\u0302\u03a31/2) equals zero. In the expression of G(X,Y), the third and fourth terms G0(\n3\u2016X\u20162F 2\u03b22T ) and G0( 3\u2016Y\u20162F 2\u03b22T ) equal zero because\n\u2016X\u20162F = \u2016Y\u20162F \u2264 r\u03a3max < 23\u03b22T . The first and second terms \u2211 i G0( 3\u2016X(i)\u20162\n2\u03b221 ) and\n\u2211 j G0(\n3\u2016Y ( j)\u20162 2\u03b222 ) equal zero because\n\u2016X(i)\u20162 \u2264 \u03a3max\u2016U\u0302(i)\u20162 \u2264 \u03a3max \u00b5rm \u2264 2 3\u03b2 2 1, for all i and, similarly, \u2016Y ( j)\u20162 \u2264 2 3\u03b2 2 2, for all j, where we have used the incoherence condition (12). This verifies our previous claim that the \u201chard regularizer\u201d G(X,Y) does not distort the optimal solution of the original formulation.\nOne commonly used assumption in the optimization literature is that the gradient of the objective function is Lipschitz continuous. For any positive number \u03b2, define a bounded set\n\u0393(\u03b2) , {(X,Y)|X \u2208 Rm\u00d7r,Y \u2208 Rn\u00d7r, \u2016X\u2016F \u2264 \u03b2, \u2016Y\u2016F \u2264 \u03b2}. (20)\nThe following result shows that this assumption (Lipschitz continuous gradients) holds for our objective function within a bounded set.\nClaim 2.1 Suppose \u03b20 \u2265 \u03b2T and\nL(\u03b20) , 4\u03b220 + 54\u03c1 \u03b220\n\u03b241 . (21)\nThen \u2207F\u0303(X,Y) is Lipschitz continuous over the set \u0393(\u03b20) with Lipschitz constant L(\u03b20), i.e.\n\u2016\u2207F\u0303(X,Y) \u2212 \u2207F\u0303(U,V)\u2016F \u2264 L(\u03b20)\u2016(X,Y) \u2212 (U,V)\u2016F , \u2200(X,Y), (U,V) \u2208 \u0393(\u03b20),\nwhere \u2016(X,Y) \u2212 (U,V)\u2016F = \u221a \u2016X \u2212 U\u20162F + \u2016Y \u2212 V\u20162F .\nThe proof of Claim 2.1 is given in Appendix A.1."}, {"heading": "2.3 Row-scaled Spectral Initialization", "text": "Our results require the initial point to be close enough to the global optima. To be more precise, we want the initial point to be in an incoherent neighborhood of the original matrix M (this neighborhood will be specified later). Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].\nWe will show that such an initial point can be found through a simple procedure. This procedure consists of two steps: first, using the spectral method (see, e.g. [31]), we obtain M0 = X\u03020Y\u0302T0 which is close to M; second, we scale the rows of (X\u03020, Y\u03020) to make it incoherent (i.e. with bounded row-norm). Denote the best rank-r approximation of a matrix A as Pr(A). Define an operation SVDr that maps a matrix A to the SVD components (X,D,Y) of its best rank-r approximation Pr(A), i.e.\nSVDr(A) , (X,D,Y), where XDYT is compact SVD of Pr(A). (22)\nThe initialization procedure is given in Table 1. The property of the initial point generated by this procedure will be presented in Claim 5.2.\nIn the numerical experiments, we find that the proposed initialization is not better than random initialization if we use the proposed formulation with the incoherence-control regularizer. In contrast, for traditional formulations (either unregularized or with a regularizer \u03bb(\u2016X\u20162F + \u2016Y\u20162F)) the proposed initialization does lead to better recovery performance (lower sample complexity). We also notice that the row-scaling step is crucial for this improvement since simply initializing via the spectral method does not help too much. See [30, Chapter 3] for the simulation results and discussions."}, {"heading": "2.4 Algorithms", "text": "Our result applies to many standard algorithms such as gradient descent, SGD and block coordinate descent type methods (including alternating minimization, block coordinate gradient descent, block successive upper bound minimization, etc.). We will describe several typical algorithms in this subsection.\nwhere G\u20320(z) = I[1,\u221e](z)2(z \u2212 1), and X\u0304(i) (resp. Y\u0304 ( j)) denotes a matrix with the i-th (resp. j-th) row being X(i) (resp. Y ( j)) and the other rows being zero.\nWe first present a gradient descent algorithm in Table 2. There are many choices of stepsizes such as constant stepsize, exact line search, limited line search, diminishing stepsize and Armijo rule [50]. We present three stepsize rules here: constant stepsize, restricted Armijo rule and restricted line search (the latter two are the variants of Armijo rule and exact line search). Note that the restricted line search rule is similar to that used in [31] for the gradient descent method over Grassmannian manifolds. To simplify the notations, we denote xk(\u03b7) , (Xk(\u03b7),Yk(\u03b7))\nand d(xk(\u03b7),x0) , \u221a \u2016Xk(\u03b7) \u2212 X0\u20162F + \u2016Yk(\u03b7) \u2212 Y0\u20162F .\nAltMin (alternating minimization) belongs to the class of block coordinate descent (BCD) type methods. One can update the blocks in different orders (e.g. cyclic [51\u201353], randomized [54] or parallel) and solve the subproblem inexactly. Commonly used inexact BCD type algorithms include BCGD (block coordinate gradient descent, which updates each variable by a single gradient step [54]) and BSUM (block successive upper bound minimization, which updates each variable by minimizing an upper bound of the objective function [55]). BCD-type methods have been widely used in engineering (e.g. [56, 57]). In the context of matrix completion, Hastie et al. [58] proposed an algorithm that could be viewed as a BSUM algorithm. Just considering different choices of the blocks will lead to different algorithms for the matrix completion problem [29]. Our result applies to many BCD type methods, including the two-block alternating minimization, BCGD and BSUM. While it is not very interesting to list all possible algorithms to which our results are applicable, we just present two specific algorithms for illustration.\nThe first BCD type algorithm we present is (two-block) AltMin, which, in the context of matrix completion,\nusually refers to the algorithm that alternates between X and Y by updating one factor at a time with the other factor fixed. Although the overall objective function is non-convex, each subproblem of X or Y is convex and thus can be solved efficiently. The details are given in Table 3.\nFor the case without the regularization term G(X,Y), the objective function becomes F(X,Y) and is quadratic with respect to X or Y . Thus Xk,Yk have closed form update. Suppose XT = (x1, . . . , xm) and YT = (y1, . . . , yn), where xi, y j \u2208 Rr\u00d71. Then (x\u22171, . . . , x\u2217m) , (arg minX F(X,Y))T and (y\u22171, . . . , y\u2217n) , (arg minY F(X,Y))T are given by\nx\u2217i = ( \u2211 j\u2208\u2126xi y jyTj ) \u2020( \u2211 j\u2208\u2126xi Mi jy j), i = 1, . . . ,m,\ny\u2217j = ( \u2211 i\u2208\u2126yj xixTi ) \u2020( \u2211 i\u2208\u2126yj Mi jxi), j = 1, . . . , n, (25)\nwhere \u2126xi = { j | (i, j) \u2208 \u2126},\u2126 y j = {i | (i, j) \u2208 \u2126}, and A\u2020 denotes the pseudo inverse of a matrix A. For our problem with the regularization term G(X,Y), we no longer have closed form update of Xk,Yk. One way to solve the convex subproblems is to start from the solution given in (25) and then apply the gradient descent method until convergence. The details for solving minX F\u0303(X,Y) is given in Table 4 (the stepsize can be chosen by one of the standard rules of the gradient descent method), and the other subproblem minY F\u0303(X,Y) can be solved in a similar fashion.\nTheoretically speaking, AltMin for our formulation (P1) is not as efficient as the vanilla AtlMin for (P0) since an extra inner loop is needed to solve the subproblem. However, we remark that in the regimes of |\u2126| that the vanilla AltMin works, the least square solution X (resp. Y) is always bounded and incoherent (empirical observation), in which case the regularizer G is inactive; therefore, the gradient updates in Table 4 do not happen. In the regimes of |\u2126| that the vanilla AltMin fails, G is active and the gradient updates do happen; however, instead of solving the subproblem exactly, one could perform one gradient step and the algorithm becomes the popular variant BCGD [54]. Our main result of exact recovery still holds for BCGD (the proof for Algorithm 3 in Claim 5.3 can be applied to BCGD since BCGD is a special case of BSUM).\nIn the second BCD type algorithm called row BSUM, we update the rows of X and Y cyclically by minimizing\nan upper bound of the objective function; see Table 5. The extra terms \u03bb02 \u2016X(i) \u2212 X (i) k\u22121\u20162 or \u03bb0 2 \u2016Y ( j) \u2212 Y ( j) k\u22121\u20162 are added to make the subproblems strongly convex, which help prove convergence to stationary points. Such a technique has also been used in the alternating least square algorithm for tensor decomposition [55]. Note that for the two-block BCD algorithm, convergence to stationary points can be guaranteed even when the subproblems are not strongly convex [59], thus in Algorithm 2 we do not add the extra terms. The benefit of cyclically updating the rows is that each subproblem can be solved efficiently using a simple binary search; see Appendix A.2 for the details. We remark again that instead of solving the subproblem exactly, one could just perform one gradient step to update each row of X and Y (with \u03bb = 0) and our result still holds.\nThe fourth algorithm we present is SGD (stochastic gradient descent) [1, 23] tailored for our problem (P1). In the optimization literature, this algorithm for minimizing the sum of finitely many functions is more commonly referred to as \u201cincremental gradient method\u201d, while SGD represents the algorithm for minimizing the expectation of a function; nevertheless, in this paper we follow the convention in the computer science literature and still call it \u201cSGD\u201d. In SGD, at each iteration we pick a component function and perform a gradient update. Similar to the BCD type methods where the blocks can be chosen in different orders, one can pick the component functions in a cyclic order, in an essentially cyclic order, or in a random order (either sampling with replacement or without replacement). In practice, the version of sampling without replacement converges much faster than the version of sampling with replacement (see [30, Chapter 2] for simulation results). In general, the understanding of sampling without replacement for optimization algorithms is quite limited (see, e.g., [60] for one example of such analysis).\nIn this paper we only consider the cyclic order, and use a standard stepsize rule for SGD [61, 62] which requires the stepsizes {\u03b7k} to go to zero as k \u2192 \u221e, but neither too fast nor too slow (this choice guarantees convergence to stationary points even for nonconvex problems). One such choice of stepsizes is \u03b7k = O(1/k). We remark that our results also apply to other versions of SGD with different update orders or stepsize rules as long as they converge to stationary points.\nTo apply SGD to our problem, we decompose the objective function F\u0303(X,Y) as follows:\nF\u0303(X,Y) = \u2211\n(i, j)\u2208\u2126 Fi j(X,Y) + m\u2211 i=1 G1i(X) + n\u2211 j=1 G2 j(Y) + G3(X) + G4(Y) = |\u2126|+m+n+2\u2211 k=1 fk(X,Y),\nwhere the component functions\nFi j(X,Y) = [(XYT \u2212 M)i j]2 = [(X(i))T Y ( j) \u2212 Mi j]2, (i, j) \u2208 \u2126,\nG1i(X) = \u03c1G0( 3\u2016X(i)\u20162\n2\u03b221 ), 1 \u2264 i \u2264 m, G2 j(Y) = \u03c1G0(\n3\u2016Y ( j)\u20162\n2\u03b222 ), 1 \u2264 j \u2264 n,\nG3(X) = \u03c1G0( 3\u2016X\u20162F 2\u03b22T ), G4(Y) = \u03c1G0( 3\u2016Y\u20162F 2\u03b22T )\n(26)\nand { fk(X,Y)}|\u2126|+m+n+2k=1 denotes the collection of all component functions. With these definitions, the SGD algorithm is given in Table 6."}, {"heading": "3 Main Results", "text": "The main result of this paper is that Algorithms 1-4 (standard optimization algorithms) will converge to the global optima of problem (P1) given in (18) and reconstruct M exactly with high probability, provided that the number of revealed entries is large enough. Similar to the results for nuclear norm minimization [4\u20137], the probability is taken with respect to the random choice of \u2126, and the result also applies to a uniform random model of \u2126.\nTheorem 3.1 (Exact Recovery) Assume a rank-r matrix M \u2208 Rm\u00d7n is \u00b5-incoherent. Suppose the condition number of M is \u03ba and \u03b1 = m/n \u2265 1. Then there exists a numerical constant C0 such that: if \u2126 is uniformly generated at random with size\n|\u2126| \u2265 C0\u03b1nr\u03ba2 max{\u00b5 log n, \u221a \u03b1\u00b52r6\u03ba4}, (27)\nthen with probability at least 1 \u2212 2/n4, each of Algorithms 1-4 reconstructs M exactly. Here, we say an algorithm reconstructs M if each limit point (X\u2217,Y\u2217) of the sequence {Xk,Yk} generated by this algorithm satisfies X\u2217(Y\u2217)T = M.\nThis result shows that although (18) is a non-convex optimization problem, many standard algorithms can converge to the global optima with certain initialization. Different from all previous works on alternating minimization for matrix completion, our result does not require the algorithm to use independent samples in different iterations. To the best of our knowledge, our result is the first one that provides theoretical guarantee for alternating minimization without resampling. In addition, this result also provides the first exact recovery guarantee for many algorithms such as gradient descent, SGD and BSUM.\nAs demonstrated in [4] (and proved in [5, Theorem 1.7]), O(nr log n) entries are the minimum requirement to recover the original matrix: O(nr) is the number of degrees of freedom of a rank r matrix M, and the additional\nlog n factor is due to the coupon collector effect [4]. For r = O(1) and \u03ba bounded, Theorem 3.1 is order optimal in terms of the sample complexity since only O(n log n) entries are needed to exactly recover M. For r = O(log n), however, our result is suboptimal by a polylogarithmic factor. The initialization has contributed r4\u03ba4 to the sample complexity bound, and we expect that using other initialization procedures (e.g. the one proposed in [19]) can reduce the exponents of r and \u03ba.\nTheorem 3.1 only establishes the convergence, but not the convergence speed. With some extra effort, we can prove the linear convergence of the gradient descent method (see Theorem 3.2 below). Again, this result can be extended beyond the gradient descent method. In fact, by a standard optimization argument, we can prove the linear convergence of any algorithm that satisfies \u201csufficient decrease\u201d (i.e. F\u0303(xk) \u2212 F\u0303(xk+1) \u2265 O(\u2016\u2207F\u0303(xk)\u20162F)) and the requirements in Lemma 3.2; see Corollary 3.2. Many first order methods, including alternating type methods (e.g. BCGD, two-block BCD), can be shown to have the sufficient decrease property under mild conditions. For space reason, we do not verify all the methods considered in this paper, but only present the linear convergence result for the gradient descent method. The proof of Theorem 3.2 is given in Section 3.2.\nTheorem 3.2 (Linear convergence) Under the same condition of Theorem 3.1, with probability at least 1 \u2212 2/n4, Algorithm 1a (gradient descent with constant stepsize) converges linearly; more precisely, the sequence {Xk,Yk} generated by Algorithm 1a satisfies\nF\u0303(Xk,Yk) \u2264 (1 \u2212 1 2 \u03b71\u03be)k, (28)\nwhere \u03be = 1Cgr5\u03ba3 p\u03a3min (here Cg is a numerical constant), \u03b71 is the stepsize and \u03b71\u03be < 1.\nThe linear convergence will immediately lead to a time complexity of O\u0303(poly(n) log 1 ) for achieving any - optimal solution, where the O\u0303 notation hides factors polynomial in r, \u03ba, \u03b1. We conjecture that the time complexity bound can be improved to O\u0303(|\u2126| log(1/ )) as observed in practice. However, finding the optimal time complexity bound is not the focus of this paper, and is left as future work.\nThe above result shows that F\u0303(Xk,Yk) converges to zero at a linear speed. Note that F\u0303(X,Y) = 0 (global convergence) only implies P\u2126(M \u2212 XYT ) = 0, not necessarily M = XYT (exact reconvery). The following lemma implies that with high probability (for random \u2126) the global convergence implies the exact recovery. In fact, it shows that the observed loss \u2016P\u2126(M \u2212 XYT )\u20162F is on the order of the recovery error p\u2016M \u2212 XYT \u20162F if (X,Y) lies in an incoherent neighborhood of M. As discussed in the introduction, this lemma can also be viewed as a geometrical property of f\u2126(Z) = \u2016P\u2126(M \u2212 Z)\u20162F in a local incoherent region (view P\u2126(Z \u2212 M) as the gradient of f\u2126(Z)).\nClaim 3.1 Under the same condition of Theorem 3.1, with probability at least 1 \u2212 1/(2n4), we have\n1 3 p\u2016M \u2212 XYT \u20162F \u2264 \u2016P\u2126(M \u2212 XYT )\u20162F \u2264 2p\u2016M \u2212 XYT \u20162F ,\n\u2200(X,Y) \u2208 K1 \u2229 K2 \u2229 K(\u03b4). (29)\nThe proof of this claim is given in Appendix D.2. This result is a simple corollary of several intermediate bounds established in the proof of Lemma 3.1."}, {"heading": "3.1 Proof of Theorem 3.1 and main lemmas", "text": "To prove Theorem 3.1, we only need to prove two lemmas which describe the local geometry of the regularized objective in (P1) and the properties of the algorithms respectively. Roughly speaking, the first lemma shows that any stationary point of (P1) in a certain region is globally optimal, and the second lemma shows that each of Algorithms\n1-4 converges to stationary points in that region. This region can be viewed as an \u201cincoherent neighborhood\u201d of M, and can be formally defined as K1 \u2229 K2 \u2229 K(\u03b4), where K1,K2 are defined as\nK1 , {(X,Y)|X \u2208 Rm\u00d7r,Y \u2208 Rn\u00d7r, \u2016X(i)\u2016 \u2264 \u03b21, \u2016Y ( j)\u2016 \u2264 \u03b22,\u2200i, j}, K2 , {(X,Y)|X \u2208 Rm\u00d7r,Y \u2208 Rn\u00d7r, \u2016X\u2016F \u2264 \u03b2T , \u2016Y\u2016F \u2264 \u03b2T }.\n(30)\nand K(\u03b4) is defined as K(\u03b4) , {(X,Y)|X \u2208 Rm\u00d7r,Y \u2208 Rn\u00d7r, \u2016M \u2212 XYT \u2016F \u2264 \u03b4}. (31)\nNote that K2 = \u0393(\u03b2T ) by our definition of \u0393 in (20). As mentioned in Section 2.1, we only need to consider a Bernolli model of \u2126 where each entry is included into \u2126 with probability p = Smn , where S satisfies (27).\nThe first lemma describes the local geometry and implies that any stationary point (X,Y) in K1 \u2229 K2 \u2229 K(\u03b4) satisfies XYT = M. The main steps to derive this geometrical property is described in Section 1.3. The formal proof will be given in Section 4.\nLemma 3.1 There exist numerical constants C0,Cd such that the following holds. Assume \u03b4 is defined by (16) and \u2126 is generated by a Bernolli model with expected cardinality S satisfying (27) (i.e. S is lower bounded by the right hand side of (27)). Then, with probability at least 1 \u2212 1/n4, the following holds: for all (X,Y) \u2208 K1 \u2229 K2 \u2229 K(\u03b4), there exist U \u2208 Rm\u00d7r,V \u2208 Rn\u00d7r, such that UVT = M and\n\u3008\u2207X F\u0303(X,Y), X \u2212 U\u3009 + \u3008\u2207Y F\u0303(X,Y),Y \u2212 V\u3009 \u2265 p 4 \u2016M \u2212 XYT \u20162F . (32)\nThe second lemma describes the properties of the algorithms we presented. Throughout the paper, \u201cunder the same condition of Lemma 3.1\u201d means \u201cassume \u03b4 is defined by (16) and \u2126 is generated by a Bernolli model with expected cardinality S satisfying (27), where C0,Cd are the same numerical constants as those in Lemma 3.1\u201d. The proof of Lemma 3.2 will be given in Section 5.\nLemma 3.2 Under the same conditions of Lemma 3.1, with probability at least 1 \u2212 1/n4, the sequence (Xk,Yk) generated by either of Algorithms 1-4 has the following properties: (a) Each limit point of (Xk,Yk) is a stationary point of (P1). (b) (Xk,Yk) \u2208 K1 \u2229 K2 \u2229 K(\u03b4), \u2200k \u2265 0.\nIntuitively, \u2016X(i)k \u2016, \u2016Y ( j) k \u2016, \u2016Xk\u2016F , \u2016Yk\u2016F are bounded because of the regularization terms we introduced and that the objective function is decreasing, and \u2016M\u2212XkYTk \u2016F is bounded because the objective function is decreasing (however, the intuition is not enough and the proof requires some extra effort). In Section 5 we provide some easily verifiable conditions for Property (b) to hold (see Proposition 5.1), so that Lemma 3.2 and Theorem 3.1 can be extended to other algorithms.\nWith these two lemmas, the proof of Theorem 3.1 is quite straightforward and presented below.\nProof of Theorem 3.1: Consider any limit point (X\u2217,Y\u2217) of sequence {(Xk,Yk)} generated by either of Algorithms 1-4. According to Property (a) of Lemma (3.2), (X\u2217,Y\u2217) is a stationary point of problem (P1), i.e. \u2207X F\u0303(X\u2217,Y\u2217) = 0,\u2207Y F\u0303(X\u2217,Y\u2217) = 0. According to Property (b) of Lemma 3.2, with probability at least 1\u2212 1/n4, (Xk,Yk) \u2208 K1 \u2229K2 \u2229 K(\u03b4) for all k, implying (X\u2217,Y\u2217) \u2208 K1 \u2229K2 \u2229K(\u03b4). Then we can apply Lemma 3.1 by plugging (X,Y) = (X\u2217,Y\u2217) into (32) to conclude that with probability at least 1 \u2212 2/n4, \u2016M \u2212 X\u2217YT\u2217 \u2016F \u2264 0, i.e. X\u2217YT\u2217 = M.\nRemark: Note that X\u2217YT\u2217 = M does not necessarily imply the global optimality of (X\u2217,Y\u2217) since we have not proved G(X\u2217,Y\u2217) = 0. Nevertheless, the global optimality can be easily proved using a different version of Lemma 3.1 (see the discussion before Lemma 3.3); in other words, Theorem 3.1 can be slightly strengthened to \u201cAlgorithm 1-4 converge to the global optima of problem (P1)\u201d, instead of \u201cAlgorithm 1-4 recover M\u201d.\nThe same argument can be used to show a more general result than Theorem 3.1, as stated in the following corollary.\nCorollary 3.1 Under the same conditions of Theorem 3.1, any algorithm satisfying Properties (a) and (b) in Lemma 3.2 reconstructs M exactly with probability at least 1 \u2212 2/n4."}, {"heading": "3.2 Proof of Theorem 3.2", "text": "The proof of Theorem 3.2 applies a standard framework for first order methods: the convergence rate (or iteration complexity) can be derived from the \u201ccost-to-go estimate\u201d and the \u201csufficient descent\u201d condition. For instance, the linear convergence f (xk)\u2212 f \u2217 \u2264 (1\u2212c1c2)k is a direct corollary of the cost-to-go estimate \u2016\u2207 f (xk)\u20162 \u2265 c1[ f (xk)\u2212 f \u2217] and the sufficient descent condition f (xk) \u2212 f (xk+1) \u2265 c2\u2016\u2207 f (xk)\u20162, where f \u2217 is the minimum value of f , and c1, c2 are certain constants. We remark that using other optimization frameworks may lead to stronger time complexity bounds; this is left as future work.\nFor our problem, a variant of Lemma 3.1 can be viewed as the cost-to-go estimate; see Lemma 3.3 below. One difference with Lemma 3.1 is the following: for a stationary point (X\u2217,Y\u2217) that \u2207F\u0303(X\u2217,Y\u2217) = 0, Lemma 3.3 implies F\u0303(X\u2217,Y\u2217) = 0 (global optimality), but Lemma 3.1 implies M = X\u2217YT\u2217 (exact recovery). The relation between these two lemmas is that Lemma 3.3 is a direct consequence of (251), a slightly stronger version of Lemma 3.1. The main difficulties of proving the two lemmas are the same and lie in Proposition 4.1 and Proposition 4.2; see the formal proof in Appendix E.\nLemma 3.3 (Cost-to-go estimate) Under the same conditions of Lemma 3.1, with probability at least 1 \u2212 1/n4, the following holds:\n\u2016\u2207F\u0303(X,Y)\u20162F \u2265 \u03beF\u0303(X,Y), \u2200 (X,Y) \u2208 K1 \u2229 K2 \u2229 K(\u03b4), (33)\nwhere \u03be = 1Cgr5\u03ba3 p\u03a3min (here Cg \u2265 1 is a numerical constant).\nThe following claim shows that Algorithm 1a satisfies the sufficient descent condition. It is easy to prove: it is well known that for minimizing a function (possibly non-convex) with Lipschitz continuous gradient, the gradient descent method with constant step-size satisfies the sufficient decrease condition.\nClaim 3.2 (Sufficient descent) For the sequence xk = (Xk,Yk) generated by Algorithm 1a (gradient descent with constant stepsize), we have\nF\u0303(xk) \u2212 F\u0303(xk+1) \u2265 \u03b71 2 \u2016\u2207F\u0303(xk)\u20162F , (34)\nwhere \u03b71 is the stepsize bounded above by \u03b7\u03041 defined in (238).\nThe linear convergence can be easily derived from Lemma 3.1 and Claim 3.2. For completeness, we present the proof below.\nProof of Theorem 3.2: According to Property (b) of Lemma 3.2, with probability at least 1 \u2212 1/n4, (Xk,Yk) \u2208 K1 \u2229 K2 \u2229 K(\u03b4) for all k. According to Lemma 3.3 and Claim 3.2, we have (with probability at least 1 \u2212 2/n4)\nF\u0303(xk) \u2212 F\u0303(xk+1) \u2265 \u03b71 2 \u2016\u2207F\u0303(xk)\u20162F \u2265 \u03b71 2 \u03beF\u0303(xk), \u2200k.\nThis relation can be rewritten as F\u0303(xk+1) \u2264 (1 \u2212\n1 2 \u03b71\u03be)F\u0303(xk), \u2200 k. (35)\nThe stepsize \u03b71 can be bounded as 0 < \u03b71 \u2264 \u03b7\u03041 (235) \u2264 14\u03b22T = 1 4CT r\u03a3max \u2264 1 \u03a3max\n. Since 0 < \u03be = 1Cgr5\u03ba3 p\u03a3min \u2264 \u03a3min, we have 0 < \u03b71\u03be \u2264 \u03a3min\u03a3max \u2264 1, which implies 0 < 1 \u2212 1 2\u03b71\u03be < 1. Then the relation (35) leads to\nF\u0303(xk) \u2264 (1 \u2212 1 2 \u03b71\u03be)kF\u0303(x0), \u2200 k,\nwhich finishes the proof.\nThe same argument can be used to show a more general result than Theorem 3.2, as stated in the following corollary. Corollary 3.2 Under the same conditions of Theorem 3.1, any algorithm satisfying Properties (a) and (b) in Lemma 3.2 and the sufficient decrease condition (34) has the linear convergence property, i.e. generates a sequence (Xk,Yk) that satisfies (28)."}, {"heading": "4 Proof of Lemma 3.1", "text": "In Section 4.1, we will show that to prove Lemma 3.1, we only need to construct U,V to satisfy three inequalities that \u2016P\u2126((U \u2212 X)(V \u2212 Y)T )\u2016F and \u2016((U \u2212 X)(V \u2212 Y)T )\u2016F are bounded above and \u3008\u2207XG, X \u2212 U\u3009 + \u3008\u2207YG,Y \u2212 V\u3009 is bounded below. In Section 4.2 we describe two propositions that specify the choice of U,V , and then we show that such U,V satisfy the three desired inequalities in Section 4.2 and subsequent subsections."}, {"heading": "4.1 Preliminary analysis", "text": "Since (X,Y) \u2208 K(\u03b4), we have d , \u2016M \u2212 XYT \u2016F \u2264 \u03b4 (16) = \u03a3min\nCdr1.5\u03ba . (36)\nTo ensure (32) holds, we only need to ensure that the following two inequalities hold:\n\u03c6F = \u3008\u2207XF, X \u2212 U\u3009 + \u3008\u2207Y F,Y \u2212 V\u3009 \u2265 p 4 d2, (37a)\n\u03c6G = \u3008\u2207XG, X \u2212 U\u3009 + \u3008\u2207YG,Y \u2212 V\u3009 \u2265 0. (37b)\nDefine a , U(Y \u2212 V)T + (X \u2212 U)VT , b , (U \u2212 X)(V \u2212 Y)T . (38)\nThen XYT \u2212 M = a + b, (X \u2212 U)YT + X(Y \u2212 V)T = a + 2b.\nUsing the expressions of \u2207XF,\u2207Y F in (24), we bound \u03c6F as follows:\n\u03c6F =\u3008\u2207XF, X \u2212 U\u3009 + \u3008\u2207Y F,Y \u2212 V\u3009 =\u3008P\u2126(XYT \u2212 M), (X \u2212 U)YT + X(Y \u2212 V)T \u3009 =\u3008P\u2126(a + b),P\u2126(a + 2b)\u3009 =\u2016P\u2126(a)\u20162F + 2\u2016P\u2126(b)\u20162F + 3\u3008P\u2126(a),P\u2126(b)\u3009 \u2265\u2016P\u2126(a)\u20162F + 2\u2016P\u2126(b)\u20162F \u2212 3\u2016P\u2126(a)\u2016F\u2016P\u2126(b)\u2016F .\n(39)\nThe reason to decompose M \u2212 XYT as a + b is the following. In order to bound \u2016P\u2126(M \u2212 XYT )\u2016F , we notice E(P\u2126(M \u2212 XYT )) = p(M \u2212 XYT ) and wish to prove \u2016P\u2126(M \u2212 XYT )\u20162F \u2248 O(pd2). However, \u2016P\u2126(A)\u2016F could be\nas large as \u2016A\u2016F if the matrix A is not independent of the random subset \u2126 (e.g. choose A s.t. A = P\u2126(A)). This issue can be resolved by decomposing XYT \u2212 M as a + b and bounding \u2016P\u2126(a)\u2016F and \u2016P\u2126(b)\u2016F separately. In fact, \u2016P\u2126(a)\u2016F can be bounded because a lies in a space spanned by the matrices with the same row space or column space as M, which is independent of \u2126 (Theorem 4.1 in [4]). \u2016P\u2126(b)\u2016F can be bounded according to a random graph lemma of [31, 46], which requires U,V, X,Y to be incoherent (i.e. have bounded row norm).\nWe claim that (37a) is implied by the following two inequalities:\n\u2016P\u2126(b)\u2016F = \u2016P\u2126((U \u2212 X)(V \u2212 Y)T )\u2016F \u2264 1 5 \u221a pd; (40a) \u2016b\u2016F = \u2016(U \u2212 X)(V \u2212 Y)T \u2016F \u2264 1\n10 d. (40b)\nIn fact, assume (40a) and (40b) are true, we prove \u03c6F \u2265 pd2/4 as follows. By XYT \u2212 M = a + b we have\n\u2016a\u2016F \u2265 \u2016M \u2212 XYT \u2016F \u2212 \u2016b\u2016F (40b) \u2265 9\n10 d. (41)\nRecall that the SVD of M is M = U\u0302\u03a3V\u0302T and M satisfies the incoherence condtion (12). It follows from M = UVT = U\u0302\u03a3V\u0302T that M,U, U\u0302 have the same column space, thus there exists some matrix B1 \u2208 Rr\u00d7r such that U = U\u0302B1; similarly, there exists B2 \u2208 Rr\u00d7r such that V = V\u0302B2. Therefore, by the definition of a in (38) we have\na \u2208 T , {U\u0302WT2 + W1V\u0302T | W1 \u2208 Rm\u00d7r,W2 \u2208 Rn\u00d7r}. (42)\nBy Theorem 4.1 in [4], for |\u2126| satisfying (27) with large enough C0, we have that with probability at least 1\u22121/(2n4), \u2016PTP\u2126PT (a) \u2212 pPT (a)\u2016F \u2264 16 p\u2016a\u2016F (note that this bound holds uniformly for all a \u2208 T , thus also holds when a is dependent on \u2126). Since a \u2208 T , this inequality can be simplified to\n\u2016PTP\u2126(a) \u2212 pa\u2016F \u2264 1 6 p\u2016a\u2016F . (43)\nFollowing the analysis of [4, Corollary 4.3], we have\n\u2016P\u2126(a)\u20162F = \u2016P\u2126PT (a)\u20162F = \u3008a,PTP2\u2126PT (a)\u3009 = \u3008a,PTP\u2126(a)\u3009 = \u3008a, pa\u3009 + \u3008a,PTP\u2126(a) \u2212 pa\u3009.\n(44)\nThe absolute value of the second term can be bounded as\n|\u3008a,PTP\u2126(a) \u2212 pa\u3009| \u2264 \u2016a\u2016F\u2016PTP\u2126(a) \u2212 pa\u2016F (43) \u2264 1\n6 p\u2016a\u20162F ,\nwhich implies \u2212 16 p\u2016a\u20162F \u2264 \u3008a,PTP\u2126(a) \u2212 pa\u3009 \u2264 1 6 p\u2016a\u20162F . Substituting into (44), we obtain that with probability at least 1 \u2212 1/(2n4), 5 6 \u2016a\u20162F \u2264 \u2016P\u2126(a)\u20162F \u2264 7 6 \u2016a\u20162F . (45)\nThe first inequality of the above relation implies\n\u2016P\u2126(a)\u20162F \u2265 5 6 \u2016a\u20162F (41) \u2265 27 40 pd2. (46)\nAccording to (39) and the bounds (46) and (40a), we have \u03c6F/(pd2) \u2265 2740 + 2( 1 5 ) 2 \u2212 35 \u221a 27 40 \u2265 1 4 , which proves (37a).\nIn summary, to find a factorization M = UVT such that (32) holds, we only need to ensure that the factorization satisfies (40b), (40a) and (37b). In the following three subsections, we will show that such a factorization M = UVT exists. Specifically, U,V will be defined in Table 7 and the three desired inequalities will be proved in Corollary 4.2, Proposition 4.3 and Claim 4.1 respectively.\n4.2 Definitions of U,V and key technical results\nWe construct U,V according to two propositions, which will be stated in this subsection and proved in the appendix. The first proposition states that if XYT is close to M, then there exists a factorization M = UVT such that U (resp. V) is close to X (resp. Y), and U,V are incoherent. Roughly speaking, this proposition shows the continuity of the factorization map Z = XYT 7\u2192 (X,Y) near a low-rank matrix M. The condition X,Y \u2208 K1 \u2229 K2 \u2229 K(\u03b4) and (16) implies that d , \u2016M \u2212 XYT \u2016F \u2264 \u03b4 = \u03a3minCdr1.5\u03ba and \u2016X\u2016F \u2264 \u03b2T , \u2016Y\u2016F \u2264 \u03b2T , thus for large enough Cd, the assumptions of Proposition 4.1 hold. Similarly, the assumptions of the other results in this subsection also hold.\nProposition 4.1 Suppose M \u2208 Rm\u00d7n is a rank-r matrix with \u03a3max (\u03a3min) being the largest (smallest) non-zero singular value, and M is \u00b5-incoherent. There exists a numerical constant CT such that the following holds: If\nd , \u2016M \u2212 XYT \u2016F \u2264 \u03a3min\n11r , (47a)\n\u2016X\u2016F \u2264 \u03b2T , \u2016Y\u2016F \u2264 \u03b2T , (47b)\nwhere \u03b2T = \u221a CT r\u03a3max, then there exist U \u2208 Rm\u00d7r,V \u2208 Rn\u00d7r such that\nUVT = M, (48a)\n\u2016U\u2016F \u2264 (1 \u2212 d\n\u03a3min )\u2016X\u2016F , (48b)\n\u2016U \u2212 X\u2016F \u2264 6\u03b2T\n5\u03a3min d, \u2016V \u2212 Y\u2016F \u2264 3\u03b2T \u03a3min d, (48c)\n\u2016U(i)\u20162 \u2264 r\u00b5 m \u03b22T , \u2016V ( j)\u20162 \u2264 3r\u00b5 2n \u03b22T . (48d)\nThe proof of Proposition 4.1 is given in Appendix B.\nRemark 1: A symmetric result that switches X,U and Y,V in the above proposition holds: under the conditions of Proposition (4.1), there exist U,V satisfying (48) with U,V reversed, i.e. UVT = M, \u2016V\u2016F(1 \u2212 d\u03a3min ) \u2264 \u2016Y\u2016F , \u2016U \u2212 X\u2016F \u2264 3\u03b2T\u03a3min d, \u2016V \u2212 Y\u2016F \u2264 6\u03b2T 5\u03a3min d, and \u2016U(i)\u20162 \u2264 3r\u00b52m \u03b22T , \u2016V ( j)\u20162 \u2264 r\u00b5 n \u03b2 2 T .\nRemark 2: To prove Theorem 3.1 (convergence), we only need \u2016U\u2016F \u2264 \u2016X\u2016F ; here the slightly stronger requirement \u2016U\u2016F \u2264 (1 \u2212 d\u03a3min )\u2016X\u2016F is for the purpose of proving Theorem 3.2 (linear convergence).\nRemark 3: Without the incoherence assumption on M, by the same proof we can show that there still exist U,V satisfying (48a) and (48c), i.e. M = UVT and U,V are close to X,Y respectively. Such a result bears some similarity with the classical perturbation theory for singular value decomposition [45]. In particular, [45] proved that for two low-rank matrices4 that are close, the spaces spanned by the left (resp. right) singular vectors of the two matrices are also close. Note that the singular vectors themselves may be very sensitive to perturbations and no such perturbation bounds can be established (see [63, Sec. 6]). The difference of our work with the classical perturbation theory is that we do not consider SVD of two matrices; instead, we allow one matrix to have an arbitrary factorization, and the factorization of the other matrix can be chosen accordingly. Since we do not have any restriction on the factorization XYT (except the dimensions) and the norms of X and Y can be arbitrarily large, the distance between two corresponding factors has to be proportional to the norm of one single factor, which explains the coefficient \u03b2T in (48c).\nUnfortunately, Proposition 4.1 is not strong enough to prove \u03c6G \u2265 0 when both \u2016X\u2016F and \u2016Y\u2016F are large (see an analysis in Section 4.4). To resolve this issue, we need to prove the second proposition in which there is an additional\n4The result in [45] also covered the case of two approximately low-rank matrices, but we only consider the case of exact low-rank matrices here.\nassumption that both \u2016X\u2016F and \u2016Y\u2016F are large, and an additional requirement that both \u2016U\u2016F and \u2016V\u2016F are bounded (by the norms of original factors \u2016X\u2016F and \u2016Y\u2016F respectively). More specifically, the proposition states that if M is close to XYT , and both \u2016X\u2016F and \u2016Y\u2016F are large, then there is a factorization M = UVT such that U (resp. V) is close to X (resp. Y), and \u2016U\u2016F \u2264 \u2016X\u2016F , \u2016V\u2016F \u2264 \u2016Y\u2016F . For the purpose of proving linear convergence, we prove a slightly stronger result that \u2016V\u2016F \u2264 (1 \u2212 d/\u03a3min)\u2016Y\u2016F . The previous result Proposition 4.1 can be viewed as a perturbation analysis for an arbitrary factorization, while Proposition 4.2 can be viewed as an enhanced perturbation analysis for a constrained factorization. Although Proposition 4.2 is just a simple variant of Proposition 4.1, it seems to require a much more involved proof than Proposition 4.1. See the formal proof of Proposition 4.2 in Appendix C.\nProposition 4.2 Suppose M \u2208 Rm\u00d7n is a rank-r matrix with \u03a3max (\u03a3min) being the largest (smallest) non-zero singular value, and M is \u00b5-incoherent. There exist numerical constants Cd,CT such that the following holds: if\nd , \u2016M \u2212 XYT \u2016F \u2264 \u03a3min\nCdr , (49a)\u221a\n2 3 \u03b2T \u2264 \u2016X\u2016F \u2264 \u03b2T , \u221a 2 3 \u03b2T \u2264 \u2016Y\u2016F \u2264 \u03b2T , (49b)\nwhere \u03b2T = \u221a CT r\u03a3max, then there exist U \u2208 Rm\u00d7r,V \u2208 Rn\u00d7r such that\nUVT = M, (50a)\n\u2016U\u2016F \u2264 \u2016X\u2016F , \u2016V\u2016F \u2264 (1 \u2212 d\n\u03a3min )\u2016Y\u2016F , (50b)\n\u2016U \u2212 X\u2016F\u2016V \u2212 Y\u2016F \u2264 65 \u221a r \u03b22T\n\u03a32min d2,\nmax{\u2016U \u2212 X\u2016F , \u2016V \u2212 Y\u2016F} \u2264 17 2 \u221a r \u03b2T \u03a3min d, (50c)\n\u2016U(i)\u20162 \u2264 r\u00b5 m \u03b22T , \u2016V ( j)\u20162 \u2264 r\u00b5 n \u03b22T . (50d)\nRemark: A symmetric result that switches X,U and Y,V in the above proposition still holds; the only change is that (50b) will become \u2016U\u2016F \u2264 (1 \u2212 d\u03a3min )\u2016X\u2016F , \u2016V\u2016F \u2264 \u2016Y\u2016F . It is easy to prove a variant of the above proposition in which (50b) is changed to \u2016U\u2016F \u2264 (1\u2212 d2\u03a3min )\u2016X\u2016F , \u2016V\u2016F \u2264 (1\u2212 d 2\u03a3min\n)\u2016Y\u2016F ; in other words, the asymmetry of X,U and Y,V in (50b) is artificial. Nevertheless, Proposition 4.2 is enough for our purpose.\nThroughout the proof of Lemma 3.1, U,V are defined in Table 4.2.\nAccording to Proposition 4.1 and Proposition 4.2 (and their symmetric results), the properties of U,V defined in Tabel 7 are summarized in the following corollary. For simplicity, we only present the case that \u2016X\u2016F \u2264 \u2016Y\u2016F ; in the other case that \u2016X\u2016F > \u2016Y\u2016F , a symmetric result of Corollary 4.1 holds.\nCorollary 4.1 Suppose d , \u2016XYT \u2212 M\u2016F \u2264 \u03a3minCdr and \u2016X\u2016F \u2264 \u2016Y\u2016F , then U,V defined in Table 7 satisfy:\nUVT = M; (51a)\n\u2016U \u2212 X\u2016F\u2016V \u2212 Y\u2016F \u2264 65 \u221a r \u03b22T\n\u03a32min d2;\nmax{\u2016U \u2212 X\u2016F , \u2016V \u2212 Y\u2016F} \u2264 17 2 \u221a r \u03b2T \u03a3min d, (51b)\n\u2016U(i)\u20162 \u2264 3 2 r\u00b5 m \u03b22T , \u2016V ( j)\u20162 \u2264 3 2 r\u00b5 n \u03b22T ; (51c)\n\u2016V\u2016F \u2264 (1 \u2212 d\n\u03a3min )\u2016Y\u2016F ; if \u2016X\u2016F > \u221a 2 3 \u03b2T , then \u2016U\u2016F \u2264 \u2016X\u2016F . (51d)\nIn (51b), we bound \u2016U \u2212X\u2016F\u2016V \u2212Y\u2016F by O(d2) with a rather complicated coefficient, but to prove (40b) we need a bound O(d) with a coefficient 1/10. Under a slightly stronger condition on d than that of Corollary 4.1, which still holds for (X,Y) \u2208 K(\u03b4) with \u03b4 defined in (16), we can prove the bound (40b) by (51b).\nCorollary 4.2 There exists a numerical constant Cd such that if\nd , \u2016M \u2212 XYT \u2016F \u2264 \u03a3min\nCdr1.5\u03ba , (52)\nthen U,V defined in Table 7 satisfy (40b).\nProof of Corollary 4.2: According to (51b) , we have\n\u2016U \u2212 X\u2016F\u2016V \u2212 Y\u2016F \u2264 65 \u03b22T\n\u03a32min\n\u221a rd2 = 65CT r1.5 \u03a3max\n\u03a32min d2\n= 65CT r1.5\u03ba d \u03a3min d \u2264 1 10 d,\nwhere the last inequliaty follows from (52) with Cd \u2265 650CT .\nIn the next two subsections, we will use the properties in Corollary 4.1 to prove (40a) and (37b).\n4.3 Upper bound on \u2016P\u2126((U \u2212 X)(V \u2212 Y)T )\u2016F\nThe following result states that for U,V defined in Table 7, (40a) holds.\nProposition 4.3 Under the same conditions as Lemma 3.1, with probability at least 1 \u2212 1/(2n4), the following is true. For any (X,Y) \u2208 K1 \u2229 K2 \u2229 K(\u03b4) and U,V defined in Table 7, we have\n\u2016P\u2126((U \u2212 X)(V \u2212 Y)T )\u20162F \u2264 p\n25 \u2016M \u2212 XYT \u20162F . (53)\nProof of Proposition 4.3: We need the following random graph lemma [31, Lemma 7.1].\nLemma 4.1 There exist numerical constants C0,C1 such that if |\u2126| \u2265 C0 \u221a \u03b1n log n, then with probability at least 1 \u2212 1/(2n4), for all x \u2208 Rm, y \u2208 Rn, \u2211 (i, j)\u2208\u2126 xiy j \u2264 C1 p\u2016x\u20161\u2016y\u20161 + C1\u03b1 3 4 \u221a np\u2016x\u20162\u2016y\u20162. (54)\nLet Z = U \u2212 X,W = V \u2212 Y and zi = \u2016Z(i)\u20162, w j = \u2016W ( j)\u20162. We have\n\u2016P\u2126((U \u2212 X)(V \u2212 Y)T )\u20162F = \u2211\n(i, j)\u2208\u2126 (ZWT )2i j\n\u2264 \u2211\n(i, j)\u2208\u2126 \u2016Z(i)\u20162\u2016W ( j)\u20162 = \u2211 (i, j)\u2208\u2126 ziw j. (55)\nInvoking Lemma 4.1, we have\n\u2016P\u2126((U \u2212 X)(V \u2212 Y)T )\u20162F \u2264 C1 p\u2016z\u20161\u2016w\u20161 + C1\u03b1 3 4 \u221a np\u2016z\u20162\u2016w\u20162. (56)\nAnalogous to the proof of (40b) in Corollary 4.2, we can prove that \u2016U \u2212 X\u2016F\u2016V \u2212 Y\u2016F \u2264 d/(10 \u221a\nC1) for large enough Cd (in fact, Cd \u2265 650CT \u221a C1 suffices). Therefore, we have\n\u2016z\u20161\u2016w\u20161 = \u2016Z\u20162F\u2016W\u20162F = \u2016U \u2212 X\u20162F\u2016V \u2212 Y\u20162F \u2264 1\n100C1 d2. (57)\nWe still need to bound \u2016z\u20162 and \u2016w\u20162. We have\n\u2016z\u20162 = \u221a\u2211\ni\n\u2016Z(i)\u20164 \u2264 \u221a\nmax i \u2016Z(i)\u20162 \u2211 j \u2016Z( j)\u20162\n\u2264 max i (\u2016U(i)\u2016 + \u2016X(i)\u2016)\u2016U \u2212 X\u2016F \u2264 ( \u221a\n3r\u00b5 2m \u03b2T + \u03b21)\u2016U \u2212 X\u2016F\n\u2264 \u221a 8 \u221a\nr\u00b5 m \u03b2T \u2016U \u2212 X\u2016F .\n(58)\nHere, the third inequliaty follows from the property (51c) in Corollary 4.1 and the condition (X,Y) \u2208 K1 (which implies \u2016X(i)\u2016 \u2264 \u03b21), and the fourth inequliaty follows from the definition of \u03b21 in (15). Similarly,\n\u2016w\u20162 \u2264max j (\u2016V ( j)\u2016 + \u2016Y ( j)\u2016)\u2016V \u2212 Y\u2016F\n\u2264 \u221a 8 \u221a\nr\u00b5 n \u03b2T \u2016V \u2212 Y\u2016F .\n(59)\nMultiplying (58) and (59), we get\n\u2016z\u20162\u2016w\u20162 \u2264 8 r\u00b5 \u221a\nmn \u03b22T \u2016U \u2212 X\u2016F\u2016V \u2212 Y\u2016F\n(51b) \u2264 8 r\u00b5\u221a\nmn \u03b22T 65\n\u221a r \u03b22T\n\u03a32min d2\n(15) = 520C2T 1 \u221a\nmn \u00b5r3.5\u03ba2d2.\nThus the second term in (56) can be bounded as\nC1\u03b1 3 4 \u221a np\u2016z\u20162\u2016w\u20162 \u2264 520C1C2T \u03b1\n3 4 \u221a\nnp \u221a mn \u00b5r3.5\u03ba2d2 \u2264 3 100 pd2, (60)\nwhere the last inequality is equivalent to 5202C21C 4 T\u03b1 3 2 \u00b52r7\u03ba4 \u2264 91002 |\u2126|/n, which holds due to (27) with large enough numerical constant C0. Plugging (57) and (60) into (56), we get \u2016P\u2126((U \u2212 X)(V \u2212 Y)T )\u20162F \u2264 p 25 d 2 = p 25\u2016M \u2212 XYT \u20162F ."}, {"heading": "4.4 Lower bound on \u03c6G", "text": "In this subsection, we prove the following claim.\nClaim 4.1 U,V defined in Table 7 satisfy (37b), i.e. \u03c6G = \u3008\u2207XG, X \u2212 U\u3009 + \u3008\u2207YG,Y \u2212 V\u3009 \u2265 0.\nProof of Claim 4.1:\nBy the expressions of \u2207XG,\u2207YG in (24), we have\n\u03c6G = \u3008\u2207XG, X \u2212 U\u3009 + \u3008\u2207YG,Y \u2212 V\u3009 =\n\u03c1 m\u2211 i=1 G\u20320( 3\u2016X(i)\u20162 2\u03b221 ) 3 \u03b221 \u3008X(i), X(i) \u2212 U(i)\u3009 + \u03c1G\u20320( 3\u2016X\u20162F 2\u03b22T ) 3 \u03b22T \u3008X, X \u2212 U\u3009\n+\u03c1 n\u2211 j=1 G\u20320( 3\u2016Y ( j)\u20162 2\u03b222 ) 3 \u03b222 \u3008Y ( j),Y ( j) \u2212 V ( j)\u3009 + \u03c1G\u20320( 3\u2016Y\u20162F 2\u03b22T ) 3 \u03b22T \u3008Y,Y \u2212 V\u3009,\n(61)\nwhere G\u20320(z) = I[1,\u221e](z)2(z \u2212 1).\nFirstly, we prove\nh1i , G\u20320( 3\u2016X(i)\u20162\n2\u03b221 ) 3 \u03b221 \u3008X(i), X(i) \u2212 U(i)\u3009 \u2265 0, \u2200 i, (62a)\nh3 j , G\u20320( 3\u2016Y ( j)\u20162\n2\u03b222 ) 3 \u03b222 \u3008Y ( j),Y ( j) \u2212 V ( j)\u3009 \u2265 0, \u2200 j. (62b)\nWe only need to prove (62a); the proof of (62b) is similar. We consider two cases.\nCase 1: \u2016X(i)\u20162 \u2264 2\u03b2 2 1 3 . Note that 3\u2016X(i)\u20162 2\u03b221 \u2264 1 implies G\u20320( 3\u2016X(i)\u20162 2\u03b221 ) = 0, thus h1i = 0.\nCase 2: \u2016X(i)\u20162 > 2\u03b2 2 1\n3 . By Corollary 4.1 and the fact that \u03b2 2 1 = \u03b2 2 T 3\u00b5r m , we have\n\u2016U(i)\u20162 \u2264 3r\u00b5 2m \u03b22T \u2264 2\u03b221 3 < \u2016X(i)\u20162. (63)\nAs a result, \u3008X(i), X(i)\u3009 = \u2016X(i)\u2016\u2016X(i)\u2016 > \u2016X(i)\u2016\u2016U(i)\u2016 \u2265 \u3008X(i),U(i)\u3009, which implies \u3008X(i), X(i) \u2212U(i)\u3009 \u2265 0. Combining this inequality with the fact that G\u20320( 3\u2016X(i)\u20162 2\u03b221 ) \u2265 0, we get h1i \u2265 0.\nSecondly, we prove\nh2 + h4 \u2265 0,\nwhere h2 , G\u20320( 3\u2016X\u20162F 2\u03b22T ) 3 \u03b22T \u3008X, X \u2212 U\u3009,\nh4 , G\u20320( 3\u2016Y\u20162F 2\u03b22T ) 3 \u03b22T \u3008Y,Y \u2212 V\u3009.\n(64)\nWithout loss of generality, we can assume \u2016X\u2016F \u2264 \u2016Y\u2016F , and we will apply Corollary 4.1 to prove (64). If \u2016Y\u2016F < \u2016X\u2016F , we can apply a symmetric result of Corollary 4.1 to prove (64). We further consider three cases.\nCase 1: \u2016X\u2016F \u2264 \u2016Y\u2016F \u2264 \u221a 2 3\u03b2T . In this case G \u2032 0( 3\u2016X\u20162F 2\u03b22T ) = G\u20320( 3\u2016Y\u20162F 2\u03b22T\n) = 0, which implies h2 = h4 = 0, thus (64) holds.\nCase 2: \u2016X\u2016F \u2264 \u221a 2 3\u03b2T < \u2016Y\u2016F . Then G\u20320( 3\u2016X\u20162F 2\u03b22T\n) = 0, which implies h2 = 0. By (51d) in Corollary 4.1 we have \u2016V\u2016F \u2264 \u2016Y\u2016F , which implies \u3008Y,Y\u3009 \u2265 \u2016Y\u2016F\u2016V\u2016F \u2265 \u3008Y,V\u3009, i.e. \u3008Y,Y \u2212 V\u3009 \u2265 0. Combined with the nonnegativity of G\u20320(\u00b7), we get h4 \u2265 0. Thus h2 + h4 = h4 \u2265 0.\nCase 3: \u221a\n2 3\u03b2T < \u2016X\u2016F \u2264 \u2016Y\u2016F . By (51d) in Corollary 4.1, we have \u2016U\u2016F \u2264 \u2016X\u2016F and \u2016V\u2016F \u2264 \u2016Y\u2016F . Similar to\nthe argument in Case 2 we can prove h2 \u2265 0, h4 \u2265 0 and (64) follows.\nIn all three cases, we have proved (64), thus (64) holds.\nWe conclude that for U,V defined in Table 7,\n\u03c6G (61) = \u03c1 \u2211 i h1i + \u2211 j h3 j + h2 + h4  (62),(64)\u2265 0, which finishes the proof of Claim 4.1.\nRemark: Based on the above proof, we can explain why Proposition 4.1 is not enough to prove \u03c6G \u2265 0. Note that h2 = 0 when \u2016X\u2016F > \u221a 2 3\u03b2T and h4 = 0 when \u2016Y\u2016F > \u221a 2 3\u03b2T . To prove h2 \u2265 0, h4 \u2265 0, it suffices to prove: (i)\n\u2016U\u2016F \u2264 \u2016X\u2016F when \u2016X\u2016F > \u221a 2 3\u03b2T ; (ii) \u2016V\u2016F \u2264 \u2016Y\u2016F when \u2016Y\u2016F > \u221a 2 3\u03b2T . For the choice of U,V in Proposition 4.1, we have \u2016U\u2016F \u2264 \u2016X\u2016F , but there is no guarantee that (ii) holds. Similarly, for the choice of U,V in the symmetric result of Proposition 4.1, we have \u2016V\u2016F \u2264 \u2016Y\u2016F , but there is no guarantee that (i) holds. Thus, Proposition 4.1 is not enough to prove \u03c6G \u2265 0. To guarantee that (i) and (ii) hold simultaneously, we need a complementary result for the case \u2016X\u2016F > \u221a 2 3\u03b2T , \u2016Y\u2016F > \u221a 2 3\u03b2T . This motivates our Proposition 4.2."}, {"heading": "5 Proof of Lemma 3.2", "text": "Property (a) in Lemma 3.2 (convergence to stationary points) is a basic requirement for many reasonable algorithms and can be proved using classical results in optimization, so the difficulty mainly lies in how to prove Property (b). We will give some easily verifiable conditions for Property (b) to hold and then show that Algorithms 1-4 satisfy these conditions. This proof framework can be used to extend Theorem 3.1 to many other algorithms.\nThe following claim states that Algorithms 1-4 satisfy Property (a). The proof of this claim is given in Appendix D.5.\nClaim 5.1 Suppose \u2126 satisfies (29), then each limit point of the sequence generated by Algorithms 1-4 is a stationary point of problem (P1).\nFor Property (b), we first show that the initial point (X0,Y0) lies in an incoherent neighborhood ( \u221a 2 3 K1) \u2229\n( \u221a\n2 3 K2) \u2229 K\u03b40 , where cKi denotes the set {(cX, cY) | (X,Y) \u2208 Ki}, i = 1, 2. The proof of Claim 5.2 will be given in Appendix D.1. The purpose of proving (X0,Y0) \u2208 ( \u221a 2 3 K1) \u2229 ( \u221a 2 3 K2) rather than (X0,Y0) \u2208 K1 \u2229 K2 is to guarantee that G(X0,Y0) = 0, where G is the regularizer defined in (13).\nClaim 5.2 Under the same condition of Lemma 3.1, with probability at least 1 \u2212 1/(2n4), (X0,Y0) given by the procedure Initialize belongs to ( \u221a 2 3 K1) \u2229 ( \u221a 2 3 K2) \u2229 K\u03b40 , where \u03b40 is defined by (16), i.e.\n(a) \u2016X(i)0 \u2016 \u2264 \u221a 2 3\u03b21, i = 1, 2, . . . ,m; \u2016Y ( j) 0 \u2016 \u2264 \u221a 2 3\u03b22, j = 1, . . . , n;\n(b) \u2016X0\u2016F \u2264 \u221a 2 3\u03b2T , \u2016Y0\u2016F \u2264 \u221a 2 3\u03b2T ; (c) \u2016M \u2212 X0YT0 \u2016F \u2264 \u03b40.\nThe next result provides some general conditions for (Xt,Yt) to lie in K1 \u2229 K2 \u2229 K(\u03b4). To simplify the notations, denote xt , (Xt,Yt) and\nu\u2217 , (U\u0302\u03a31/2, V\u0302\u03a31/2),\nwhere U\u0302\u03a3V\u0302 is the SVD of M. Recall that F\u0303(u\u2217) = 0 (proved in the paragraph after (19)). We say a function \u03c8(x\u0304,\u2206; \u03bb) is a convex tight upper bound of F\u0303(x) along the direction \u2206 at x\u0304 if\n\u03c8(x\u0304,\u2206; \u03bb) is convex over \u03bb \u2208 R; (65a) \u03c8(x\u0304,\u2206; \u03bb) \u2265 F\u0303(x\u0304 + \u03bb\u2206), \u2200 \u03bb \u2208 R; \u03c8(x\u0304,\u2206; 0) = F\u0303(x\u0304). (65b)\nFor example, \u03c8(x\u0304,\u2206; \u03bb) = F\u0303(x\u0304 + \u03bb\u2206) satisfies (65) for either \u2206 = (X, 0) or \u2206 = (0,Y), where X \u2208 Rm\u00d7r and Y \u2208 Rn\u00d7r are arbitrary matrices. This definition is motivated by the block successive upper bound minimization method [55]. The proof of Proposition 5.1 is given in Appendix D.3.\nProposition 5.1 Suppose the sample set \u2126 satisfies (29) and \u03b4, \u03b40 are defined by (16). Consider an algorithm that starts from a point x0 = (X0,Y0) and generates a sequence {xt} = {(Xt,Yt)}. Suppose x0 satisfies\nx0 \u2208 ( \u221a\n2 3\nK1) \u2229 ( \u221a\n2 3 K2) \u2229 K(\u03b40), (66)\nand {xt} satisfies either of the following three conditions:\n1) F\u0303(xt + \u03bb\u2206t) \u2264 2F\u0303(x0),\u2200 \u03bb \u2208 [0, 1], where \u2206t = xt+1 \u2212 xt, \u2200 t; (67a)\n2) 1 = arg min \u03bb\u2208R \u03c8(xt,\u2206t; \u03bb),\nwhere \u03c8 satisfies (65),\u2206t = xt+1 \u2212 xt, \u2200 t; (67b)\n3) F\u0303(xt) \u2264 2F\u0303(x0), d(xt,x0) \u2264 5 6 \u03b4, \u2200 t. (67c)\nThen xt = (Xt,Yt) \u2208 K1 \u2229 K2 \u2229 K(2\u03b4/3), for all t \u2265 0.\nThe first condition means that F\u0303 is bounded above by 2F\u0303(x0) over the line segment between xt and xt+1 for any t. This condition holds for gradient descent or SGD with small enough stepsize (see Claim 5.3). The second condition means that the new point xt+1 is the minimum of a convex tight upper bound of the original function along the direction xt+1 \u2212xt, and holds for BCD type methods such as Algorithm 2 and Algorithm 3 (see Claim 5.3). Note that the gradient descent method with exact line search stepsize does not satisfy this condition since F\u0303 is not jointly convex in the variable (X,Y). The third condition means that F\u0303(xt) is bounded above and xt is not far from x0 for any t. For standard nonlinear optimization algorithms, it is not easy to prove that xt is not far from x0. However, as done by Algorithm 1 with restricted Armijo rule or restricted line search, we can force d(xt,x0) \u2264 56\u03b4 to hold when computing the new point xt.\nThe following claim shows that each of Algorithm 1-4 satisfies one of the three conditions in (67). The proof of Claim 5.3 is given in Appendix D.4.\nClaim 5.3 The sequence {xt} generated by Algorithm 1 with either restricted Armijo rule or restricted line search satisfies (67c). The sequence {xt} generated by either Algorithm 2 or Algorithm 3 satisfies (67b). Suppose the sample set \u2126 satisfies (29), then the sequence {xt} generated by either Algorithm 1 with constant stepsize or Algorithm 4 satisfies (67a).\nTo put things together, Claim 5.1 shows Algorithms 1-4 satisfy Property (a), and Proposition 5.1 together with Claim 5.2 and Claim 5.3 shows that Algorithms 1-4 satisfy Property (b). Therefore, we have proved Lemma 3.2."}, {"heading": "A Supplemental Material for Section 2", "text": ""}, {"heading": "A.1 Proof of Claim 2.1", "text": "This proof is quite straightforward and we mainly use the triangular inequalities and the boundedness of the considered region \u0393(\u03b20). In this proof, f \u2032(x) denotes the derivative of a function f at x.\nSince (X,Y), (U,V) belong to \u0393(\u03b20), we have\n\u2016X\u2016F \u2264 \u03b20, \u2016Y\u2016F \u2264 \u03b20, \u2016U\u2016F \u2264 \u03b20, \u2016V\u2016F \u2264 \u03b20. (68)\nWe first prove \u2016\u2207F(X,Y) \u2212 \u2207F(U,V)\u2016F \u2264 4\u03b220\u2016(X,Y) \u2212 (U,V)\u2016F . (69)\nBy the triangular inequality, we have\n\u2016\u2207XF(X,Y) \u2212 \u2207XF(U,V)\u2016F \u2264 \u2016\u2207XF(X,Y) \u2212 \u2207XF(U,Y)\u2016F +\u2016\u2207XF(U,Y) \u2212 \u2207XF(U,V)\u2016F .\n(70)\nThe first term of (70) can be bounded as follows\n\u2016\u2207XF(X,Y) \u2212 \u2207XF(U,Y)\u2016F = \u2016P\u2126(XYT \u2212 M)Y \u2212 P\u2126(UYT \u2212 M)Y\u2016F \u2264 \u2016P\u2126(XYT \u2212 M) \u2212 P\u2126(UYT \u2212 M)\u2016F\u2016Y\u2016F = \u2016P\u2126[(X \u2212 U)YT ]\u2016F\u2016Y\u2016F \u2264 \u2016(X \u2212 U)YT \u2016F\u2016Y\u2016F \u2264 \u2016X \u2212 U\u2016F\u2016Y\u20162F \u2264 \u2016X \u2212 U\u2016F\u03b220.\nThe second term of (70) can be bounded as\n\u2016\u2207XF(U,Y) \u2212 \u2207XF(U,V)\u2016F = \u2016P\u2126(UYT \u2212 M)Y \u2212 P\u2126(UVT \u2212 M)V\u2016F \u2264 \u2016P\u2126(M)(V \u2212 Y)\u2016F + \u2016P\u2126(UYT )Y \u2212 P\u2126(UVT )V\u2016F \u2264 \u2016P\u2126(M)(V \u2212 Y)\u2016F + \u2016P\u2126(UYT )Y \u2212 P\u2126(UYT )V\u2016F\n+ \u2016P\u2126(UYT )V \u2212 P\u2126(UVT )V\u2016F \u2264 \u2016P\u2126(M)\u2016F\u2016V \u2212 Y\u2016F + \u2016P\u2126(UYT )\u2016F\u2016Y \u2212 V\u2016F\n+ \u2016P\u2126[U(Y \u2212 V)T ]\u2016F\u2016V\u2016F \u2264 \u2016M\u2016F\u2016V \u2212 Y\u2016F + \u2016U\u2016F\u2016Y\u2016F\u2016Y \u2212 V\u2016F + \u2016U\u2016F\u2016Y \u2212 V\u2016F\u2016V\u2016F \u2264 3\u03b220\u2016Y \u2212 V\u2016F ,\nwhere the last inequliaty follows from (68) and the fact that \u2016M\u2016F \u2264 \u221a\nr\u03a3max (15) = 1CT \u221a r\u03b2 2 T \u2264 \u03b22T \u2264 \u03b220 (here the second\nlast inequality follows from the fact that the numerical constant CT \u2265 1, and the last inequality follows from the assumption of Claim 2.1).\nPlugging the above two bounds into (70), we obtain\n\u2016\u2207XF(X,Y) \u2212 \u2207XF(U,V)\u2016F \u2264 \u03b220(\u2016X \u2212 U\u2016F + 3\u2016Y \u2212 V\u2016F).\nSimilarly, we have \u2016\u2207Y F(X,Y) \u2212 \u2207Y F(U,V)\u2016F \u2264 \u03b220(3\u2016X \u2212 U\u2016F + \u2016Y \u2212 V\u2016F).\nCombining the above two relations, we have (denote \u03c91 , \u2016X \u2212 U\u2016F , \u03c92 , \u2016Y \u2212 V\u2016F)\n\u2016\u2207F(X,Y) \u2212 \u2207F(U,V)\u2016F\n= \u221a \u2016\u2207XF(X,Y) \u2212 \u2207XF(U,V)\u20162F + \u2016\u2207Y F(X,Y) \u2212 \u2207Y F(U,V)\u20162F\n\u2264 \u03b220 \u221a (\u03c91 + 3\u03c92)2 + (3\u03c91 + \u03c92)2\n\u2264 4\u03b220 \u221a \u03c921 + \u03c9 2 2 = 4\u03b220\u2016(X,Y) \u2212 (U,V)\u2016F ,\nwhich proves (69).\nNext we prove\n\u2016\u2207G(X,Y) \u2212 \u2207G(U,V)\u2016F \u2264 54\u03c1 \u03b220\n\u03b241 \u2016(X,Y) \u2212 (U,V)\u2016F . (71)\nDenote\nG1i(X) , G0 3\u2016X(i)\u20162 2\u03b221  , G2(X) , G0 3\u2016X\u20162F 2\u03b22T  , (72) then we have\n\u2207G1i(X) = G\u20320 3\u2016X(i)\u20162\n2\u03b221  3X\u0304(i) \u03b221 , \u2207G2(X) = G\u20320 3\u2016X\u20162F 2\u03b22T  3X \u03b22T , (73)\nwhere G\u20320(z) = I[1,\u221e](z)2(z \u2212 1) and X\u0304(i) denotes a matrix with the i-th row being X(i) and the other rows being zero. Obviously G1i(X) is a matrix with all but the i-th row being zero. Recall that\nG(X,Y) = \u03c1 \u2211\ni\nG1i(X) + \u03c1G2(X) + f0(Y),\nwhere f0(Y) is a certain function of Y which we can ignore for now. Then we have \u2207XG(X,Y) = \u03c1 \u2211\ni\n\u2207G1i(X) + \u03c1\u2207G2(X)\n= \u03c1 m\u2211 i=1 G\u20320 3\u2016X(i)\u20162 2\u03b221  3X\u0304(i) \u03b221 + \u03c1G\u20320 3\u2016X\u20162F 2\u03b22T  3X \u03b22T ,\n(74)\nand, similarly, \u2207XG(U,V) = \u03c1 \u2211 i \u2207G1i(U) + \u03c1G2(U).\nTherefore, we have\n\u2016\u2207XG(X,Y) \u2212 \u2207XG(U,V)\u2016F = \u2016\u03c1 \u2211 i [\u2207G1i(X) \u2212 \u2207G1i(U)] + \u03c1[\u2207G2(X) \u2212 \u2207G2(U)]\u2016F\n\u2264 \u2016\u03c1 \u2211\ni\n[\u2207G1i(X) \u2212 \u2207G1i(U)]\u2016F + \u03c1\u2016\u2207G2(X) \u2212 \u2207G2(U)\u2016F\n= \u03c1 \u221a\u2211 i \u2016\u2207G1i(X) \u2212 \u2207G1i(U)\u20162F + \u03c1\u2016\u2207G2(X) \u2212 \u2207G2(U)\u2016F ,\n(75)\nwhere the last equality is due to the fact that each \u2207G1i(X)\u2212\u2207G1i(U) is a matrix with all but the i-th row being zero. Denote\nz1 , 3\u2016X\u20162F 2\u03b22T , z2 , 3\u2016U\u20162F 2\u03b22T . (76)\nThen by (76), (73) and the triangle inequality we have\n\u03b22T 3 \u2016\u2207G2(X) \u2212 \u2207G2(U)\u2016F =\u2016G\u20320(z1)X \u2212G\u20320(z2)U\u2016F \u2264 |G\u20320(z1)|\u2016X \u2212 U\u2016F + |G\u20320(z1) \u2212G\u20320(z2)|\u2016U\u2016F .\n(77)\nBy the definitions of z1, z2 in (76) and using \u2016X\u2016F \u2264 \u03b20, \u2016Y\u2016F \u2264 \u03b20, we have\n|z1 \u2212 z2| = 3\n2\u03b22T (\u2016X\u20162F \u2212 \u2016U\u20162F)\n= 3\n2\u03b22T (\u2016X\u2016F + \u2016U\u2016F)(\u2016X\u2016F \u2212 \u2016U\u2016F)\n\u2264 3\u03b20 \u03b22T \u2016X \u2212 U\u2016F .\n(78)\nAccording to (68) and the definitions of z1, z2 in (76), we have\nmax{z1, z2} \u2264 3 2 \u03b220\n\u03b22T . (79)\nWe can bound the first and second order derivative of G0 as follows:\nG\u20320(z) = I[1,\u221e](z)2(z \u2212 1) \u2264 3 \u03b220 \u03b22T , \u2200z \u2208 [0, 3 2 \u03b220 \u03b22T ], (80)\nG\u2032\u20320 (z) = 2I[1,\u221e](z) \u2264 2, \u2200z \u2208 [0,\u221e). (81)\nBy the mean value theorem and (81), we have\n|G\u20320(z1) \u2212G\u20320(z2)| \u2264 2|z1 \u2212 z2| (78) \u2264 6\u03b20\n\u03b22T \u2016X \u2212 U\u2016F . (82)\nPlugging (80) (with z = z1) and (82) into (77), we obtain\n\u03b22T 3 \u2016\u2207G2(X) \u2212 \u2207G2(U)\u2016F \u2264 3 \u03b220\n\u03b22T \u2016X \u2212 U\u2016F + 6\u03b20 \u03b22T \u2016X \u2212 U\u2016F\u2016U\u2016F\n\u2264 9 \u03b220\n\u03b22T \u2016X \u2212 U\u2016F\n=\u21d2 \u2016\u2207G2(X) \u2212 \u2207G2(U)\u2016F \u2264 27 \u03b220\n\u03b24T \u2016X \u2212 U\u2016F . (83)\nSince \u2016X(i)\u2016F \u2264 \u2016X\u2016F \u2264 \u03b20, \u2016U(i)\u2016 \u2264 \u2016U\u2016F \u2264 \u03b20, by an argument analogous to that for (83), we can prove\n\u2016\u2207G1i(X) \u2212 \u2207G1i(U)\u2016F \u2264 27 \u03b220\n\u03b241 \u2016X(i) \u2212 U(i)\u2016, \u2200 i,\nwhich further implies \u221a\u2211 i \u2016\u2207G1i(X) \u2212 \u2207G1i(U)\u20162\n\u2264 27 \u03b220\n\u03b241 \u221a\u2211 i \u2016X(i) \u2212 U(i)\u20162 = 27 \u03b220 \u03b241 \u2016X \u2212 U\u2016F .\n(84)\nPlugging (83) and (84) into (75), we obtain\n\u2016\u2207XG(X,Y) \u2212 \u2207XG(U,V)\u2016F \u2264 54\u03c1 \u03b220\n\u03b241 \u2016X \u2212 U\u2016F .\nSimilarly, we can prove\n\u2016\u2207YG(X,Y) \u2212 \u2207YG(U,V)\u2016F \u2264 54\u03c1 \u03b220\n\u03b242 \u2016Y \u2212 V\u2016F \u2264 54\u03c1\n\u03b220 \u03b241 \u2016Y \u2212 V\u2016F ,\nwhere the last inequality is due to \u03b21 = \u03b2T \u221a 3\u00b5r m \u2264 \u03b2T \u221a 3\u00b5r n = \u03b22. Combining the above two relations yields (71).\nFinally, we combine (69) and (71) to obtain\n\u2016\u2207F\u0303(X,Y) \u2212 \u2207F\u0303(U,V)\u2016F \u2264 \u2016\u2207F(X,Y) \u2212 \u2207F(U,V)\u2016F + \u2016\u2207G(X,Y) \u2212 \u2207G(U,V)\u2016F\n\u2264 4\u03b220 + 54\u03c1\u03b220\u03b241  \u2016(X,Y) \u2212 (U,V)\u2016F , which finishes the proof of Claim 2.1.\nRemark: If we further assume that the norm of each X(i) (resp. Y ( j)) is bounded by O(\u03b21) (resp. O(\u03b22)), the Lipschitz constant can be improved to 4\u03b220 + 54\u03c1 \u03b220 \u03b24T ."}, {"heading": "A.2 Solving the Subproblem of Algorithm 3", "text": "The subproblem of Algorithm 3 for the row vector X(i) is\nmin X(i)\nF\u0303(X(1)k , . . . , X (i\u22121) k , X (i), X(i+1)k\u22121 . . . , X (m) k\u22121,Yk\u22121) + \u03bb0 2 \u2016X(i) \u2212 X(i)k\u22121\u2016 2.\nFor simplicity, denote X(i) = xi, X (i) k\u22121 = x\u0304i, X ( j) k = x j, 1 \u2264 j \u2264 i\u2212 1, X ( j) k\u22121 = x j, i + 1 \u2264 j \u2264 m, and Y ( j) k\u22121 = y j, 1 \u2264 j \u2264 n. Then the above problem becomes\nmin xi F\u0303(x1, . . . , xi\u22121, xi, xi+1, . . . , xm, y1, . . . , yn) + \u03bb0 2 \u2016xi \u2212 x\u0304i\u20162.\nThe optimal solution x\u2217i to this subproblem satisfies the equation \u2207xi F\u0303 = 0, i.e.\nAxi \u2212 b + g(\u2016xi\u2016)xi = 0, (85)\nwhere A = \u2211\nj\u2208\u2126xi y jy T j + \u03bb0I is a symmetric PD (positive definite) matrix, b = \u2211 j\u2208\u2126xi Mi jy j + \u03bb0 x\u0304i, and g is a function\ndefined as\ng(z) = \u03c1 3 \u03b221 G\u20320( 3z2 2\u03b221 ) + \u03c1 3 \u03b22T G\u20320( 3(z2 + \u03bei) 2\u03b22T ),\nin which \u03bei = \u2211 j,i \u2016x j\u20162 is a constant. Note that g has the following properties: a) g(z) = 0 when z2 \u2264 min{ 2\u03b221 3 , 2\u03b22T 3 \u2212 \u03bei} ; b) g is an increasing function in [0,\u221e). The equation (85) is equivalent to\nxi = (A + g(\u2016xi\u2016)I)\u22121b. (86)\nSuppose the eigendecomposition of A is B\u039bBT and let \u03a6 = BT bbT B, then (86) implies\n\u2016xi\u20162 = \u2016(A + g(\u2016xi\u2016)I)\u22121b\u20162 = Tr((A + g(\u2016xi\u2016)I)\u22122bbT )\n= Tr((\u039b + g(\u2016xi\u2016)I)\u22122\u03a6) = r\u2211\nk=1\n\u03a6kk\n(\u039bkk + g(\u2016xi\u2016))2 ,\n=\u21d2 1 = 1\u2016xi\u20162 r\u2211\nk=1\n\u03a6kk\n(\u039bkk + g(\u2016xi\u2016))2 , (87)\nwhere Zkk denotes the (k, k)-th entry of matrix Z. Since A and \u03a6 are PSD (positive semidefinite) matrices, we have \u03a6kk \u2265 0,\u039bkk \u2265 0. The righthand side of (87) is a decreasing function of \u2016xi\u2016, thus the equation (87) can be solved via a simple bisection procedure. After obtaining the norm of the optimal solution z\u2217 = \u2016x\u2217i \u2016, the optimal solution x\u2217i can be obtained by (86), i.e.\nx\u2217i = (A + g(z \u2217)I)\u22121b. (88)\nSimilarly, the subproblem for Y ( j) can also be solved by a bisection procedure."}, {"heading": "B Proof of Proposition 4.1", "text": ""}, {"heading": "B.1 Matrix norm inequalities", "text": "We first prove some basic inequalities related to the matrix norms. These simple results will be used in the proof of Propositions 4.1 and 4.2.\nProposition B.1 If A, B \u2208 Rn1\u00d7n2 , then\n\u2016A \u2212 B\u20162 \u2265 \u03c3min(A) \u2212 \u03c3min(B). (89)\nProof: \u03c3min(A) = min\u2016v\u2016=1 \u2016Av\u2016 \u2264 min\u2016v\u2016=1(\u2016Bv\u2016 + \u2016(A \u2212 B)v\u2016) \u2264 min\u2016v\u2016=1 \u2016Bv\u2016 + \u2016A \u2212 B\u2016 = \u03c3min(B) + \u2016A \u2212 B\u2016.\nProposition B.2 For any A \u2208 Rn1\u00d7n2 , B \u2208 Rn2\u00d7n3 , we have\n\u03c3min(AB) \u2264 \u03c3min(A)\u2016B\u20162. (90)\nProof: \u03c3min(AB) = minv\u2208Rn1\u00d71,\u2016v\u2016=1 \u2016vT AB\u2016 \u2264 minv\u2208Rn1\u00d71,\u2016v\u2016=1 \u2016vT A\u2016\u2016B\u20162 = \u03c3min(A)\u2016B\u20162.\nProposition B.3 Suppose A, B \u2208 Rn1\u00d7n2 and ciA(i) = B(i), where ci \u2208 R and |ci| \u2264 1, for i = 1, . . . , n1 (recall that Z(i) denotes the i-th row of Z). Then\n\u2016B\u20162 \u2264 \u2016A\u20162.\nProof: For simplicity, denote ai , (A(i))T , bi , (B(i))T . Then\n\u2016B\u201622 = max\u2016v\u2016=1 \u2016Bv\u2016 2 = max \u2016v\u2016=1 \u2211 i (bTi v) 2\n= max \u2016v\u2016=1 \u2211 i c2i (a T i v) 2 \u2264 max \u2016v\u2016=1 \u2211 i (aTi v) 2 = \u2016A\u201622.\nCorollary B.1 Suppose B \u2208 Rn1\u00d7n2 is a submatrix of A \u2208 Rm1\u00d7m2 , then\n\u2016B\u20162 \u2264 \u2016A\u20162. (91)\nProof: By Proposition B.3, we have \u2016(X1, X2)\u20162 \u2265 \u2016(X1, 0)\u20162 = \u2016X1\u20162.\nWithout loss of generality, suppose A = [\nB B1 B2 B3\n] . Applying the above inequality twice, we get\n\u2016A\u20162 \u2265 \u2016(B, B1)\u20162 \u2265 \u2016B\u20162.\nProposition B.4 For any A \u2208 Rn1\u00d7n2 , B \u2208 Rn2\u00d7n3 , we have\n\u2016AB\u2016F \u2264 \u2016A\u20162\u2016B\u2016F , (92a) \u2016AB\u20162 \u2264 \u2016A\u20162\u2016B\u20162. (92b)\nFurther, if n1 \u2265 n2, then\n\u03c3min(A)\u2016B\u2016F \u2264 \u2016AB\u2016F , (93a) \u03c3min(A)\u2016B\u20162 \u2264 \u2016AB\u20162. (93b)\nProof: Assume the SVD of A is A1DA2, where A1 \u2208 Rn1\u00d7n1 , A2 \u2208 Rn2\u00d7n2 are orthonormal matrices and D \u2208 Rn1\u00d7n2 has nonzero entries Dii, i = 1, . . . ,min{n1, n2}. Note that\n\u03c3min(A) \u2264 Dii \u2264 \u2016A\u20162,\u2200 i.\nLet B\u2032 = A2B and suppose the i-th row of B\u2032 is bi, i = 1, . . . , n2, then\n\u2016AB\u20162F = \u2016DA2B\u20162F = \u2016DB\u2032\u20162F = min{n1,n2}\u2211\ni=1\nD2ii\u2016bi\u20162. (94)\nThe the RHS (right hand side) can be bounded from above as\nmin{n1,n2}\u2211 i=1 D2ii\u2016bi\u20162 \u2264 \u2016A\u201622 min{n1,n2}\u2211 i=1 \u2016bi\u20162\n\u2264 \u2016A\u201622 n2\u2211 i=1 b2i = \u2016A\u201622\u2016B\u2032\u20162F = \u2016A\u201622\u2016B\u20162F .\nCombining the above relation and (94) leads to (92a).\nIf n1 \u2265 n2, then min{n1, n2} = n2, and the RHS of (94) can be bounded from below as min{n1,n2}\u2211\ni=1 D2ii\u2016bi\u20162 = n2\u2211 i=1 D2ii\u2016bi\u20162 \u2265 \u03c3min(A)2 n2\u2211 i=1 \u2016bi\u20162\n= \u03c3min(A)2\u2016B\u2032\u20162F = \u03c3min(A)2\u2016B\u20162F .\nCombining the above relation and (94) leads to (93a).\nNext we prove the inequalities related to the spectral norm. We have\n\u2016AB\u20162 = \u2016DA2B\u20162 = \u2016DB\u2032\u20162 = max \u2016v\u2016\u22641,v\u2208Rn1\u00d71 \u2016vT DB\u2032\u2016. (95)\nNote that {vT D | \u2016v\u2016 \u2264 1, v \u2208 Rn1\u00d71} \u2286 {uT | u \u2208 Rn2\u00d71, \u2016u\u2016 \u2264 \u2016A\u20162}, thus the RHS of (95) can be bounded from above as\nmax \u2016v\u2016\u22641,v\u2208Rn1\u00d71 \u2016vT DB\u2032\u2016 \u2264 max u\u2208Rn2\u00d71,\u2016u\u2016\u2264\u2016A\u20162 \u2016uT B\u2032\u2016\n= \u2016A\u20162\u2016B\u2032\u20162 = \u2016A\u20162\u2016B\u20162.\nCombining the above relation and (95) leads to (92b).\nIf n1 \u2265 n2, then {uT | u \u2208 Rn2\u00d71, \u2016u\u2016 \u2264 \u03c3min(A)} \u2286 {vT D | \u2016v\u2016 \u2264 1, v \u2208 Rn1\u00d71} (in fact, for any \u2016u\u2016 \u2264 \u03c3min(A), let vi = ui/Dii, i = 1, . . . , n2 and vi = 0, n2 < i \u2264 n1, where vi denotes the i-th entry of v, then vT D = uT and \u2016v\u2016 \u2264 1). Thus the RHS of (95) can be bounded from below as\nmax \u2016v\u2016\u22641,v\u2208Rn1\u00d71 \u2016vT DB\u2032\u2016 \u2265 max u\u2208Rn2\u00d71,\u2016u\u2016\u2264\u03c3min(A) \u2016uT B\u2032\u2016\n= \u03c3min(A)\u2016B\u2032\u20162 = \u03c3min(A)\u2016B\u20162.\nCombining the above relation and (95) leads to (93b)."}, {"heading": "B.2 Proof of Proposition 4.1", "text": "Let M, X,Y satisfy the condition (47). First, we specify the choice of U,V . Suppose the SVD of M is M = U\u0302\u03a3V\u0302 = Q1\u03a3\u0303QT2 , where Q1 \u2208 Rm\u00d7m,Q2 \u2208 Rn\u00d7n are unitary matrices, and \u03a3\u0303 = ( \u03a3 0 0 0 ) . Suppose Q1 = (Q11,Q12), Q2 = (Q21,Q22), where Q11 = U\u0302 \u2208 Rm\u00d7r,Q21 = V\u0302 \u2208 Rn\u00d7r are incoherent matrices, and Q12 \u2208 Rm\u00d7(m\u2212r),Q22 \u2208 Rn\u00d7(n\u2212r). Let us write X,Y as\nX = Q1 ( X\u20321 X\u20322 ) , Y = Q2 ( Y \u20321 Y \u20322 ) , (96)\nwhere X\u20321,Y \u2032 1 \u2208 Rr\u00d7r, X\u20322 \u2208 R(m\u2212r)\u00d7r,Y \u20322 \u2208 R(n\u2212r)\u00d7r. Define\nU , Q1 ( U\u20321 0 ) , V , Q2 ( V \u20321 0 ) , (97)\nwhere U\u20321 = (1 \u2212 \u03b7\u0304)X\u20321, V \u20321 =\n1 1 \u2212 \u03b7\u0304\u03a3(X \u2032 1) \u2212T ,\nin which \u03b7\u0304 ,\nd \u03a3min \u2264 1 11 .\nThe definition of V \u20321 is valid since X \u2032 1 is invertible (otherwise, rank(X \u2032 1(Y \u2032 1) T ) \u2264 rank(X\u20321) \u2264 r \u2212 1, thus d \u2265 \u2016\u03a3 \u2212 X\u20321(Y \u2032 1) T \u2016F (89) \u2265 \u03a3min \u2212 \u03c3min(X\u20321(Y \u20321)T ) = \u03a3min, which contradicts (47a).By this definition, we have\nU\u20321(V \u2032 1) T = (1 \u2212 \u03b7\u0304)X\u20321(V \u20321)T = \u03a3. (98)\nNow, we prove that U,V defined in (97) satisfy the requirement (48). The requirement (48a) UVT = M follows from (98) and (97). The requirement (48b) \u2016U\u2016F \u2264 (1 \u2212 d\u03a3min )\u2016X\u2016F can be proved as follows:\n\u2016U\u2016F = \u2016U\u20321\u2016F = (1 \u2212 d\n\u03a3min )\u2016X\u20321\u2016F \u2264 (1 \u2212 d \u03a3min )\u2016X\u2016F .\nAs a side remark, the following variant of the requirement (48b) also holds:\n\u2016U\u20162 \u2264 (1 \u2212 d\n\u03a3min )\u2016X\u20162. (99)\nIn fact, \u2016U\u20162 = \u2016U\u20321\u20162 = (1 \u2212 d \u03a3min )\u2016X\u20321\u20162\n(91) \u2264 (1 \u2212 d\n\u03a3min ) \u2225\u2225\u2225\u2225\u2225\u2225 ( X\u20321 X\u20322 )\u2225\u2225\u2225\u2225\u2225\u2225 2 = (1 \u2212 d \u03a3min )\u2016X\u20162.\nTo prove the requirement (48c), we first provide the bounds on \u2016X\u20322\u2016F , \u2016V \u20321 \u2212 Y \u20321\u2016F , \u2016Y \u20322\u2016F . Note that\nd2 =\u2016M \u2212 XYT \u20162F\n= \u2225\u2225\u2225\u2225\u2225\u2225 ( \u03a3 0 0 0 ) \u2212 QT1 XYT Q2 \u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n(96) = \u2225\u2225\u2225\u2225\u2225\u2225 ( \u03a3 0 0 0 ) \u2212 ( X\u20321(Y \u2032 1) T X\u20321(Y \u2032 2) T\nX\u20322(Y \u2032 1) T X\u20322(Y \u2032 2) T\n)\u2225\u2225\u2225\u2225\u2225\u2225 2\nF\n=\u2016\u03a3 \u2212 X\u20321(Y \u20321)T \u20162F + \u2016X\u20321(Y \u20322)T \u20162F + \u2016X\u20322(Y \u20321)T \u20162F + \u2016X\u20322(Y \u20322)T \u20162F . (98) = \u2016X\u20321((1 \u2212 \u03b7\u0304)V \u20321 \u2212 Y \u20321)T \u20162F + \u2016X\u20321(Y \u20322)T \u20162F + \u2016X\u20322(Y \u20321)T \u20162F\n+ \u2016X\u20322(Y \u20322)T \u20162F .\n(100)\nIntuitively, since \u2016X\u20321\u2016F , \u2016Y \u20321\u2016F are O(1), we can upper bound \u2016(1 \u2212 \u03b7\u0304)V \u20321 \u2212 Y \u20321\u2016F , \u2016Y \u20322\u2016F , \u2016X\u20322\u2016F as O(d). More rigorously, it follows from (100) that d \u2265 \u2016X\u20321((1 \u2212 \u03b7\u0304)V \u20321 \u2212 Y \u20321)T \u2016F (93a) \u2265 \u03c3min(X\u20321)\u2016(1 \u2212 \u03b7\u0304)V \u20321 \u2212 Y \u20321\u2016F and, similarly, d \u2265 \u03c3min(X\u20321)\u2016(Y \u20322)T \u2016F , d \u2265 \u03c3min(Y \u20321)\u2016(X\u20322)T \u2016F . These three inequalities imply\n\u2016(1 \u2212 \u03b7\u0304)V \u20321 \u2212 Y \u20321\u2016F \u2264 d\n\u03c3min(X\u20321) ,\n\u2016Y \u20322\u2016F \u2264 d\n\u03c3min(X\u20321) , \u2016X\u20322\u2016F \u2264 d \u03c3min(Y \u20321) .\n(101)\nWe can lower bound \u03c3min(X\u20321) and \u03c3min(Y \u2032 1) as\n\u03c3min(X\u20321) \u2265 10\u03a3min 11\u03b2T , \u03c3min(Y \u20321) \u2265 10\u03a3min 11\u03b2T . (102)\nTo prove (102), notice that (100) implies that d \u2265 \u2016\u03a3\u2212 X\u20321(Y \u20321)T \u2016F \u2265 \u2016\u03a3\u2212 X\u20321(Y \u20321)T \u20162 (89) \u2265 \u03a3min \u2212\u03c3min(X\u20321(Y \u20321)T ), which further implies\n\u03c3min(X\u20321(Y \u2032 1) T ) \u2265 \u03a3min \u2212 d \u2265 10 11 \u03a3min.\nAccording to Proposition B.2, we have \u03c3min(X\u20321(Y \u2032 1) T ) \u2264 \u03c3min(X\u20321)\u2016Y \u20321\u20162. Combining this inequality with the above relation, we get \u03c3min(X\u20321)\u2016Y \u20321\u20162 \u2265 \u03c3min(X\u20321(Y \u20321)T ) \u2265 5\u03a3min/6, which further implies\n\u03c3min(X\u20321) \u2265 10\u03a3min 11\u2016Y \u20321\u20162 . (103)\nSimilarly, we have\n\u03c3min(Y \u20321) \u2265 10\u03a3min\n11\u2016X\u20321\u20162 . (104)\nPlugging \u2016Y \u20321\u20162 \u2264 \u2016Y \u20321\u2016F \u2264 \u2016Y\u2016F \u2264 \u03b2T and similarly \u2016X\u20321\u20162 \u2264 \u03b2T into (103) and (104), we obtain (102).\nCombining (102) and (101), we obtain\nmax{\u2016(1 \u2212 \u03b7\u0304)V \u20321 \u2212 Y \u20321\u2016F , \u2016X\u20322\u2016F , \u2016Y \u20322\u2016F} \u2264 11 10 d \u03a3min \u03b2T \u2264 1 10 \u03b2T . (105)\nWe can bound the norm of V \u20321 as\n\u2016V \u20321\u2016F = 1\n1 \u2212 \u03b7\u0304\u2016(1 \u2212 \u03b7\u0304)V \u2032 1\u2016F \u2264 1 1 \u2212 \u03b7\u0304 (\u2016(1 \u2212 \u03b7\u0304)V \u2032 1 \u2212 Y \u20321\u2016F + \u2016Y \u20321\u2016F)\n(105) \u2264 11\n10 ( 1 10 \u03b2T + \u03b2T ) \u2264 ( 11 10 )2 \u03b2T .\n(106)\nCombining this relation with (105), we have\n\u2016V \u20321 \u2212 Y \u20321\u2016F \u2264 \u2016(1 \u2212 \u03b7\u0304)V \u20321 \u2212 Y \u20321\u2016F + \u03b7\u0304\u2016V \u20321\u2016F\n\u2264 11 10 d \u03a3min \u03b2T + \u03b7\u0304 ( 11 10 )2 \u03b2T \u2264 7\u03b2T 3\u03a3min d.\nFrom (105) and the above relation we obtain\n\u2016U \u2212 X\u2016F =\u2016X\u20322\u2016F \u2264 11\u03b2T 10\u03a3min d \u2264 6\u03b2T 5\u03a3min d,\n\u2016V \u2212 Y\u2016F = \u221a \u2016V \u20321 \u2212 Y \u20321\u20162F + \u2016Y \u20322\u20162F\n\u2264 \u221a( 7 3 )2 + ( 11 10 )2 \u03b2T \u03a3min d \u2264 3\u03b2T \u03a3min d,\nwhich finishes the proof of the requirement (48c).\nAs a side remark, the requirement (48c) can be slightly improved to\n\u2016U \u2212 X\u2016F \u2264 6\u2016Y\u20162 5\u03a3min d, \u2016V \u2212 Y\u2016F \u2264 3\u2016X\u20162 \u03a3min d. (107)\nIn fact, plugging \u2016X\u20321\u20162 (91) \u2264 \u2016 ( X\u20321 X\u20322 ) \u20162 = \u2016X\u20162 and similarly \u2016Y \u20321\u20162 \u2264 \u2016Y\u20162 into (103) and (104), we obtain \u03c3min(X\u20321) \u2265 5\u03a3min 6\u2016Y\u20162 , \u03c3min(Y \u2032 1) \u2265 5\u03a3min 6\u2016X\u20162 . Combining with (101), we obtain (107). This inequality will be used in the proof of Claim 5.2 in Appendix D.1.\nAt last, we prove the requirement (48d). By the definitions of U,V in (97), we have U = (Q11,Q12) (\nU\u20321 0 ) = Q11U\u20321,\nV = (Q21,Q22) (\nV \u20321 0 ) = Q21V \u20321.\n(108)\nThe assumption that M is \u00b5-incoherent implies\n\u2016Q(i)11\u2016 2 = \u2016U\u0302(i)\u20162 \u2264 r\u00b5\nm , \u2016Q(i)21\u2016 2 = \u2016V\u0302 ( j)\u20162 \u2264 r\u00b5 n , \u2200 i, j.\nNotice the following fact: for any matrix A \u2208 RK\u00d7r, B \u2208 Rr\u00d7r, where K \u2208 {m, n}, we have\n\u2016(AB)(i)\u20162 = \u2016A(i)B\u20162 \u2264 \u2016A(i)\u20162\u2016B\u20162F .\nTherefore, we have (using the fact \u2016U\u20321\u2016F \u2264 \u2016X\u20321\u2016F \u2264 \u2016X\u2016F \u2264 \u03b2T and (106))\n\u2016U(i)\u20162 = \u2016(Q11U\u20321)(i)\u20162 \u2264 \u2016Q (i) 11\u2016 2\u2016U\u20321\u20162F \u2264 r\u00b5 m \u03b22T ;\n\u2016V ( j)\u20162 = \u2016(Q21V \u20321)( j)\u20162 \u2264 r\u00b5 n \u2016V \u20321\u20162F (106) \u2264 ( 11 10 )4 r\u00b5 n \u03b22T \u2264 3 2 r\u00b5 n \u03b22T ,\n(109)\nwhich finishes the proof the requirement (48d)."}, {"heading": "C Proof of Proposition 4.2", "text": "We will first reduce Proposition 4.2 to Proposition C.1 for r \u00d7 r matrices in Section C.1. This reduction is rather trivial, and the major difficulty lies in Proposition C.1. For general r, the proof of Proposition C.1 is rather involved. We will give the overview of the main proof ideas in Section C.2. Most readers can skip Section C.1."}, {"heading": "C.1 Transformation to a simpler problem", "text": "We first transform the problem to a simpler problem that only involves r \u00d7 r matrices. In particular, we will show that to prove Proposition 4.2 we only need to prove Proposition C.1.\nSimilar to the proof of Proposition 4.1, we use Q1 \u2208 Rm\u00d7m,Q2 \u2208 Rn\u00d7n to denote the SVD factors of M (Q1 and Q2 are unitary matrices), and write X,Y as\nX = Q1 ( X\u20321 X\u20322 ) , Y = Q2 ( Y \u20321 Y \u20322 ) .\nDefine\nU = Q1 ( U\u20321 0 ) , V = Q2 ( V \u20321 0 ) , (110)\nwhere U\u20321 \u2208 Rr\u00d7r and V \u20321 \u2208 Rr\u00d7r are to be determined.\nWe can convert the conditions on U,V to the conditions on U\u20321,V \u2032 1. As proved in Appendix B (combining (101)\nand (102)),\n\u2016X\u20322\u2016F \u2264 6\u03b2T\n5\u03a3min d, \u2016Y \u20322\u2016F \u2264 6\u03b2T 5\u03a3min d. (111)\nObviously, the condition (49a) implies the following condition on X\u20321,Y \u2032 1:\nd\u2032 , \u2016\u03a3 \u2212 (X\u20321)(Y \u20321)T \u2016 \u2264 \u03a3min\nCdr . (112)\nUsing (111) and the facts \u2016X\u2016F = \u221a \u2016X\u20321\u20162F + \u2016X\u20322\u20162F and \u2016Y\u2016F = \u221a \u2016Y \u20321\u20162F + \u2016Y \u20322\u20162F , the condition (49b) implies the following condition on X\u20321,Y \u2032 1: \u221a\n3 5 \u03b2T \u2264 \u2016X\u20321\u2016F \u2264 \u03b2T , \u221a 3 5 \u03b2T \u2264 \u2016Y \u20321\u2016F \u2264 \u03b2T . (113)\nWe have the following proposition.\nProposition C.1 There exist numerical constants Cd,CT such that: if X\u20321,Y \u2032 1 \u2208 Rr\u00d7r satisfy (112) and (113), where\n\u03b2T = \u221a CT r\u03a3max, then there exist U\u20321 \u2208 Rr\u00d7r,V \u20321 \u2208 Rr\u00d7r such that\nU\u20321(V \u2032 1) T = \u03a3, (114a)\n\u2016U\u20321\u2016F \u2264 \u2016X\u20321\u2016F , \u2016V \u20321\u2016F \u2264 (1 \u2212 d\n\u03a3min )\u2016Y \u20321\u2016F , (114b)\n\u2016U\u20321 \u2212 X\u20321\u2016F\u2016V \u20321 \u2212 Y \u20321\u2016F \u2264 63 \u221a r \u03b22T\n\u03a32min d2,\nmax{\u2016U\u20321 \u2212 X\u20321\u2016F , \u2016V \u20321 \u2212 Y \u20321\u2016F} \u2264 58 7 \u221a r \u03b2T \u03a3min d. (114c)\nWe claim that Proposition C.1 implies Proposition 4.2. Since we have already proved that the conditions of Proposition 4.2 imply the conditions of Proposition C.1, we only need to prove that the conclusion of Proposition C.1 implies the conclusion of Proposition 4.2. In other words, we only need to show that if U\u20321,V \u2032 1 satisfy (114), then they satisfy the requirements (50).\nThe requirement (50a) UVT = M follows directly from (114a) and the definition of U,V in (110). The requirement (50b) can be proved as \u2016V\u2016F = \u2016V \u20321\u2016F \u2264 (1 \u2212 d \u03a3min )\u2016Y \u20321\u2016F \u2264 (1 \u2212 d \u03a3min )\u2016Y\u2016F and \u2016U\u2016F = \u2016U\u20321\u2016F \u2264 \u2016X\u2016F .\nAnalogous to (109), the requirement (50d) can be proved as \u2016V ( j)\u20162 = \u2016(Q21V \u20321)( j)\u20162 \u2264 r\u00b5 n \u2016V \u20321\u20162F \u2264 r\u00b5 n \u03b2 2 T and, similarly, \u2016U(i)\u20162 \u2264 r\u00b5m \u03b22T . At last, we prove the requirement (50c). The first relation in (50c) can be proved as\n\u2016U \u2212 X\u2016F\u2016V \u2212 Y\u2016F\n= \u221a \u2016U\u20321 \u2212 X\u20321\u20162F + \u2016X\u20322\u20162F \u221a \u2016V \u20321 \u2212 Y \u20321\u20162F + \u2016Y \u20322\u20162F = (\u2016 U\u20321 \u2212 X\u20321\u20162F\u2016V \u20321 \u2212 Y \u20321\u20162F + \u2016X\u20322\u20162F\u2016V \u20321 \u2212 Y \u20321\u20162F + \u2016U\u20321 \u2212 X\u20321\u20162F\u2016Y \u20322\u20162F + \u2016X\u20322\u20162F\u2016Y \u20322 \u20162F\n)1/2 (111),(114c) \u2264 \u221a r \u03b22T\n\u03a32min d2\n\u221a 632 + (\n6 5 )2( 58 7 )2 + ( 58 7 )2( 6 5 )2 + ( 6 5 )4,\n< 65 \u221a r \u03b22T\n\u03a32min d2,\nwhere in the second last inequality we also use the fact d\u2032 \u2264 d. The second relation in (50c) can be proved by\n\u2016U \u2212 X\u2016F = \u221a \u2016U\u20321 \u2212 X\u20321\u20162F + \u2016X\u20322\u20162F\n(111),(114c) \u2264 \u221a ( 6 5 )2 + ( 58 7 )2 \u221a r \u03b2T \u03a3min d \u2264 17 2 \u221a r \u03b2T \u03a3min d\nand a similar inequality for \u2016V \u2212 Y\u2016F .\nC.2 Preliminary analysis for the proof of Proposition C.1\nWe first give a more intuitive explanation of what we want to prove, by relating the result to \u201cpreconditioning\u201d. Then we analyze two simple examples for r = 2 to get some ideas on how to approach the problem. Next we discuss how to extend the ideas to general r. To simplify the notations, from now on, we use X,Y,U,V, d to replace X\u20321,Y \u2032 1,U \u2032 1,V \u2032 1, d \u2032 in Proposition (C.1)."}, {"heading": "C.2.1 Perturbation Analysis for Preconditioning.", "text": "We claim that Proposition C.1 is closely related to \u201cpreconditioning\u201d, which refers to reducing the condition number (by preprocessing) in numerical linear algebra.\nProposition C.2 (Informal) Suppose X \u2208 Rr\u00d7r is non-singular and \u2016X\u2016F = \u2016X\u22121\u2016F \u2265 C \u221a\nr where C \u2265 10 is a constant. For any d\u2032 \u2264 O(1/r1.5), there exists U \u2208 Rr\u00d7r such that \u2016U\u2016F = \u2016X\u2016F , \u2016U\u22121\u2016F \u2264 (1 \u2212 d\u2032)\u2016X\u22121\u2016F and max{\u2016U \u2212 X\u2016F , \u2016U\u22121 \u2212 X\u22121\u2016F} \u2264 O(d\u2032r1.5).\nWe will argue later that Proposition C.2 is a simple version of Proposition C.1.\nWe explain why this proposition can be understood as perturbation analysis for perconditioning. Assume X has singular values \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3r > 0, then \u2016X\u20162F = \u2211 i \u03c3 2 i and \u2016X\u22121\u20162F = \u2211 i\n1 \u03c32i . By Cauchy-Schwartz inequality\n\u2016X\u20162F\u2016X\u22121\u20162F \u2265 r2, and the equality holds iff \u03c31 = \u00b7 \u00b7 \u00b7 = \u03c3r, i.e., X has a condition number 1. In other words, if \u2016X\u2016F = \u2016X\u22121\u2016F = \u221a r, then X has the minimal condition number 1. In the assumption \u2016X\u2016F = \u2016X\u22121\u2016F \u2265 C \u221a r, C can be viewed as a measure of the ill-conditioned-ness of X (different from the condition number \u03c31/\u03c3r but related). Prop. C.2 simply says that we can perturb X to make X better-conditioned.\nProp. C.2 itself is not difficult to prove. In fact, without loss of generality we can assume X is a diagonal matrix (by left and right multiplying X by its singular vector matrices). Then the problem reduces to the following problem:\nassume \u2211\ni \u03c3 2 i = \u2211 i\n1 \u03c32i \u2265 C2r, perturb \u03c3i\u2019s so that the\n\u2211 i \u03c3 2 i does not change while \u2211 i\n1 \u03c32i increases. This is a rather easy problem. Nevertheless, for the original desired result Prop. C.1 we cannot assume X is diagonal. In Section C.2.2 we will analyze the problem without assuming X is diagonal.\nTo show the connection of Prop. C.2 and Prop. C.1, we first simplify the statement of Prop. C.1.\nProposition C.3 (Simpler version of Proposition C.1) Suppose X,Y,\u03a3 \u2208 Rr\u00d7r are non-singular and \u03a3 is diagonal. If \u2016XYT \u2212 \u03a3\u2016F = d \u2264 O(\u03a3min/r) and \u2016X\u2016F = \u2016Y\u2016F = \u03b2 \u2265 C \u221a r\u03a3max, then we can find a factorization \u03a3 = UVT such that max{\u2016U \u2212 X\u2016F , \u2016V \u2212 Y\u2016F} \u2264 O( \u221a rd\u03b2/\u03a3min) and \u2016U\u2016F \u2264 \u2016X\u2016F , \u2016V\u2016F \u2264 \u2016Y\u2016F .\nThere are a few differences with Prop. C.1: i) In Prop. C.1 we assume \u2016X\u2016F , \u2016Y\u2016F \u2208 [ \u221a\n0.6\u03b2T , \u03b2T ], but by simply scaling X,U,Y,V we can assume \u2016X\u2016F = \u2016Y\u2016F as in the above proposition; ii) here we only require \u2016V\u2016F \u2264 \u2016Y\u2016F , instead of \u2016V\u2016F \u2264 (1 \u2212 d/\u03a3min)\u2016Y\u2016F in (114b); iii) in Prop. C.1 there is an extra bound of \u2016U \u2212 X\u2016F\u2016V \u2212 Y\u2016F . Nevertheless, these differences are not essential and do not affect the proof too much.\nNow let us consider a special case and show how to reduce Prop. C.3 to Prop. C.2. This part is mainly for the purpose of rigorous derivation and we suggest first-time readers jump to Section C.2.2. The special case we consider is \u03a3 = I and XYT = (1 \u2212 d/ \u221a r)I, where d \u2264 O(1/r). Let d\u2032 = d/ \u221a r \u2264 O(1/r1.5), then YT = (1 \u2212 d/ \u221a r)X\u22121 = (1 \u2212 d\u2032)X\u22121. The condition of Prop. C.3 becomes\n\u2016X\u2016F = \u2016X\u22121\u2016F(1 \u2212 d\u2032) = \u03b2. (115)\nOne requirement of Prop. C.3 becomes \u2016U\u2016F \u2264 \u2016X\u2016F , \u2016U\u22121\u2016F \u2264 \u2016Y\u2016F = \u2016X\u22121\u2016F(1\u2212d\u2032). The distance bound in Prop. C.3 is O( \u221a rd\u03b2/\u03a3min), which becomes O(d\u2032r1.5) under the new parameter setting. By a similar scaling technique, i.e. scaling X,U by 1/ \u221a 1 \u2212 d\u2032 and Y,V by \u221a 1 \u2212 d\u2032, we can replace the condition (115) by\n\u2016X\u2016F = \u2016Y\u2016F = \u03b2 \u221a\n1 1 \u2212 d\u2032 \u2265 C \u221a r.\nNote that rigorously speaking the bound should be C \u221a r/ \u221a\n1 \u2212 d\u2032, but since 1/(1 \u2212 d\u2032) \u2264 1/(1 \u2212 1/r1.5) \u2208 [1/(1 \u2212 1/21.5), 1], the contribution of 1/ \u221a 1 \u2212 d\u2032 is just a numerical constant which can be absorbed into C. Now the problem becomes: assume \u2016X\u2016F = \u2016X\u22121\u2016F \u2265 C \u221a\nr, find U such that \u2016U\u2016F \u2264 \u2016X\u2016F , \u2016U\u22121\u2016F \u2264 \u2016X\u22121\u2016F(1 \u2212 d\u2032) and max{\u2016U \u2212 X\u2016F , \u2016U\u22121 \u2212 X\u22121\u2016F} \u2264 O(d\u2032r1.5), where d\u2032 \u2264 O(1/r1.5). By slightly strengthening the requirement \u2016U\u2016F \u2264 \u2016X\u2016F to \u2016U\u2016F = \u2016X\u2016F , we obtain Prop. C.2."}, {"heading": "C.2.2 Two Motivating Examples", "text": "We denote the i-th row of X,Y as xi, yi, respectively. In the first example (see Figure 3), we set r = 2, \u03a3 = I (which implies \u03a3min = \u03a3max = 1), d = 1/(Cdr) and\nX = Diag (x11, x22) = Diag C, 1 \u2212 d/\u221a2C  , Y = Diag (y11, y22) = Diag 1 \u2212 d/\u221a2C ,C  , (116)\nwhere C > 1 is to be determined, and Diag(w1,w2) denotes a 2 \u00d7 2 diagonal matrix with diagonal entries w1,w2. In this setting \u03b2T = \u221a rCT \u03a3max = \u221a 2CT is a large constant. Condition (112) holds since \u2016XYT \u2212 \u03a3\u2016F = \u2016(1\u2212 d/ \u221a 2)I \u2212\nI\u2016F = d = 1/(Cdr). Note that \u2016X\u2016F = \u2016Y\u2016F = \u221a C2 + (1\u2212d/ \u221a 2)2 C2 \u2248 C, thus there exists C \u2208 [ \u221a 3/5\u03b2T , \u03b2T ] so that (113) holds.\nHow should we define U = Diag(u11, u22),V = Diag(v11, v22) so that (114) holds? Due to the \u201csymmetry\u201d of X and Y in this example (by symmetry we mean x11 = y22, x22 = y11), we choose U,V such that u11 = v22, u22 = v11. Then the requirements (114a) and (114b) reduce to:\nu11u22 = 1 = x11x22\n1 \u2212 d/ \u221a 2 ,\nu211 + u 2 22 \u2264 x211 + x222.\n(117)\nIt can be easily shown that there exist u11, u22 satisfying (117). In fact, define R = \u2016X\u2016F = \u221a x211 + x 2 22 and let a point (w1,w2) move along the circle {(w1,w2) | w21 + w22 = R2} from (x11, x22) to (R/ \u221a 2,R/ \u221a\n2). During this process, the norm of (w1,w2) does not change and the product w1w2 monotonically increases from x11x22 to R2/2. Therefore, there exist u11, u22 satisfying (117) as long as R2/2 > x11x22/(1 \u2212 d/ \u221a 2). This inequality is equivalent to (1\u2212d/ \u221a\n2)(x211 + x 2 22)/2 > x11x22, which can be simplified to (1\u2212d/ \u221a 2)(x11\u2212 x22)2 > \u221a 2dx11x22 = \u221a 2d(1\u2212d/ \u221a 2),\nor equivalently, (x11 \u2212 x22)2 > \u221a 2d. The last inequality holds when x11 \u2212 x22 = C \u2212 (1 \u2212 d/ \u221a\n2)/C is large enough (i.e. C is large enough).\nTo summarize, we will increase the small entry x22 (resp. y11) and decrease the large entry x11 (resp. y22) to obtain a more balanced diagonal matrix U (resp. V), which has the same norm as X (resp. Y). The percentage of increase in the small entry x22 (resp. y11) will be much larger than the percentage of decrease in the large entry x11 (resp. y22), thus the products x22y22 and x11y11 will increase; in other words, the product UVT of the more balanced matrices U,V will have larger entries than XYT .\nNote that the above idea of shrinking/extending works when there is a large imbalance in the lengths of the rows of X,Y , regardless of whether X,Y are diagonal matrices or not. By the assumption that \u2016X\u2016F and \u2016Y\u2016F are large, we know that there must be a row of X (resp. Y) that has large norm (here \u201clarge\u201d means much larger than 1/ \u221a r); however, it is possible that all rows of X and Y have large norm and there is no imbalance in terms of the lengths of the rows. See below for such an example.\nIn the second example (see Figure 4), we still set r = 2, \u03a3 = I, d = 1/(Cdr). Suppose X = (xT1 , x T 2 ), Y = (y T 1 , y T 2 ). We define x1 = (C, 0), x2 = (\u2212C sin\u03b1,C cos\u03b1) and y1 = (C cos\u03b1,C sin\u03b1), y2 = (0,C), where C is a large constant, and \u03b1 \u2208 (0, \u03c0/2) is chosen so that\nC2 cos\u03b1 = 1 \u2212 d/ \u221a 2. (118)\nWhen C is large, \u03b1 \u2248 arccos(1/C2) is also large (i.e. close to \u03c0/2). Condition (112) holds since \u2016XYT \u2212 \u03a3\u2016F = \u2016C2 cos\u03b1I \u2212 I\u2016F = \u2016(1 \u2212 d/ \u221a 2)I \u2212 I\u2016F = d = 1/(Cdr). Note that \u2016X\u2016F = \u2016Y\u2016F = \u221a 2C, so we can choose C = \u03b2T / \u221a 2 = \u221a 2CT / \u221a 2 = \u221a CT so that (113) holds.\nHow should we choose U = (uT1 , u T 2 ), V = (v T 1 , v T 2 ) so that (114) holds? The idea for the first example no longer works since it requires that the difference of \u2016x1\u2016 and \u2016x2\u2016 (resp. \u2016y1\u2016 and \u2016y2\u2016) is large; however, in this example,\n\u2016x1\u2016 \u2212 \u2016x2\u2016 = \u2016y1\u2016 \u2212 \u2016y2\u2016 = 0. The key idea for this example is to use rotation. Rotating a vector does not change the norm, so requirement (113) will not be violated if ui (resp. vi) is obtained by rotating xi(resp. yi). For simplicity, we rotate y1, x2 to obtain v1, u2 respectively and let u1 = x1, v2 = y2 (see Figure 4). Note that y1 and x2 should be rotated by the same angle as v1 should be orthogonal to u2 (since the off-diagonal entries of UVT are zero). To increase the inner product \u3008xi, yi\u3009 from 1 \u2212 d/ \u221a 2 to 1, we need to decrease the angle of xi and yi, thus y1 (resp. x2) should be rotated towards x1(resp. y2). Finally, let us specify the angle of rotation \u03b8 , \u2220(y1, v1) = \u2220(x2, u2). The requirement \u3008u1, v1\u3009 = 1 is equivalent to 1 = \u2016u1\u2016\u2016v1\u2016 cos \u2220(u1, v1) = \u2016x1\u2016\u2016y1\u2016 cos(\u03b1 \u2212 \u03b8), which can be rewritten as\n1 = C2 cos(\u03b1 \u2212 \u03b8). (119)\nThe right-hand side of (119) is an increasing function of \u03b8, ranging from C2 cos(\u03b1) (118) = 1\u2212d/ \u221a 2 to C2 for \u03b8 \u2208 [0, \u03b1]. Since 1 lies in the range [1 \u2212 d/ \u221a\n2,C2], there exists a unique \u03b8 so that (119) holds. One can further verify the requirement (114c), i.e. the difference of X (resp. Y) and U (resp. V) is small. As a rough summary, we rotate xi, yi to obtain ui, vi when the angle of xi and yi is large. This operation does not change the norm and can increase the inner product \u3008xi, yi\u3009 to the desired amount (1 in this case)."}, {"heading": "C.2.3 Proof Ideas of Proposition C.1", "text": "In the above two examples, we have used two different operations: one is based on shrinking/extending, and the other is based on rotation. As we mentioned before, the first operation cannot deal with the second example; also, it is obvious that the second operation cannot deal with the first example (the angle between xi and yi is zero, so rotation only decreases the inner product). Therefore, both operations are necessary.\nAre these two operations sufficient? Fortunately, the answer is yes for the case that XYT is diagonal and \u3008xi, yi\u3009 \u2264 \u03a3i (we need extra effort to reduce the general problem to this case). When all the angles between xi and yi are smaller than a constant \u03b1\u0304, there must be some kind of imbalance in the lengths of xi, yi\u2019s (to illustrate this, if all \u2016xi\u2016 = \u2016yi\u2016, then \u2016xi\u20162 = \u2016xi\u2016\u2016yi\u2016 \u2248 \u03a3i/ cos \u2220(xi, yi) \u2264 \u03a3i/ cos(\u03b1\u0304), which implies \u2016X\u20162F . r\u03a3max/ cos(\u03b1\u0304) 35CT r\u03a3max = 3 5\u03b2 2 T for large enough CT , a contradiction to (112)). Thus we can use the first operation (i.e. shrinking/extending the vectors xi, yi\u2019s) to obtain the desired U,V . When all the angles between xi and yi are larger than a constant \u03b1\u0304, we can use the second operation (i.e. rotating the vectors xi, yi\u2019s) to obtain the desired U,V . In general, some angles may be larger than \u03b1\u0304 and others may be smaller, then a natural solution is to use the two operations simultaneously: use the first operation for the pairs (xi, yi) with small angles and the second operation for those with large angles.\nWe had a proof using the two operations simultaneously, but the bounds on \u2016U \u2212 X\u2016F , \u2016V \u2212 Y\u2016F have a large exponent of r. In the following subsection, we present a different proof that does not use the two operations simultaneously, but only use one of the two operations. The basic proof framework is summarized as follows. We first\ndefine Y\u0302 so that XY\u0302 = \u03a3; in other words, we try to satisfy the requirement (114a) first. Then we try to modify Y\u0302 to satisfy the requirement (114b). In particular, we need to reduce the norm of Y\u0302 and keep the norm of X unchanged, while maintaining the relation XY\u0302T = \u03a3. We consider two cases: in Case 1, \u201cmost\u201d angles between X and Y\u0302 are smaller than \u03b1\u0304, and using the first operation (shrinking/extending) can obtain the desired U,V; in Case 2, \u201cmost\u201d angles between X and Y\u0302 are larger than \u03b1\u0304, and using the second operation (rotation) can obtain the desired U,V (see (127) for a precise definition of Case 1 and Case 2). The difference of this proof framework and the previous one is the following. In our previous proof framework, we need to take into account every pair xi, yi so that its inner product is modified to \u03a3i, thus two operations have to be applied simultaneously. In contrast, in this new proof framework, \u3008xi, y\u0302i\u3009 is already \u03a3i, and we only need to worry about the \u201coverall\u201d requirement that \u2016Y\u0302\u2016F should be reduced, thus dealing only with the pairs with small angles (or only with the pairs with large angles) is enough to satisfy the requirement.\nFinally, we would like to mention that when \u03a3 is an identity matrix, the proof can be rather simple. In fact, in this case one can assume X to be diagonal by proper orthonormal transformation, and then assume Y to be diagonal since the off-diagonal entries are small. By just using the first operation (scaling of the diagonal entries), we can construct the desired U,V and the proof is similar to that in Appendix C.3.1. When \u03a3 is not a diagonal matrix, we can replace X,Y by XQ,Q\u22121Y where Q is orthonormal, but that only simplifies X to a upper triangular matrix, a condition seems not very helpful. It seems that the second operation has to be used and the proof becomes more involved."}, {"heading": "C.3 Proof of Proposition C.1", "text": "As mentioned earlier, to simplify the notations, we use X,Y,U,V, d to replace X\u20321,Y \u2032 1,U \u2032 1,V \u2032 1, d \u2032 in Proposition (C.1). Throughout the proof, we choose CT = 20, (120)\nand Cd = 108, which implies d\n\u03a3min \u2264 1 108r . (121)\nThere are two \u201chard\u201d requirements on U,V: (114a) and (114b). Our construction of U,V can be viewed as a two-step approach, whereby we satisfy one requirement in each step. In Step 1, we construct\nY\u0302 = \u03a3(\u03a3 + D)\u2212T Y, where D , XYT \u2212 \u03a3,\nthen XY\u0302T = (XYT )(XYT )\u22121\u03a3 = \u03a3,\ni.e. the first requirement is satisfied. Since the new Y\u0302 may have higher norm than \u2016Y\u2016F , in Step 2 we modify X, Y\u0302 to U,V so that the product does not change, and \u2016V\u2016F \u2264 \u2016Y\u2016F , \u2016U\u2016F \u2264 \u2016X\u2016F .\nClaim C.1 Let Y\u0302 = \u03a3(\u03a3 + D)\u2212T Y, then\n\u03b7 , 1 \u2212 \u2016Y\u2016F \u2016Y\u0302\u2016F \u2264 d \u03a3min , (122a)\n\u2016Y \u2212 Y\u0302\u2016F \u2264 d\n\u03a3min \u2212 d \u2016Y\u2016F . (122b)\nProof of Claim C.1: By the definition of Y\u0302 we have Y = (\u03a3 + D)T \u03a3\u22121Y\u0302 , then we have\n\u2016Y \u2212 Y\u0302\u2016F = \u2016(\u03a3 + D)T \u03a3\u22121Y\u0302 \u2212 Y\u0302\u2016F = \u2016DT \u03a3\u22121Y\u0302\u2016F\n\u2264 \u2016DT \u03a3\u22121\u2016F\u2016Y\u0302\u2016F \u2264 \u2016DT \u2016F\u03a3\u22121min\u2016Y\u0302\u2016F = d\n\u03a3min \u2016Y\u0302\u2016F .\n(123)\nUsing the triangular inequality and (123), we have\n\u2016Y\u0302\u2016F \u2264 \u2016Y \u2212 Y\u0302\u2016F + \u2016Y\u2016F \u2264 d\n\u03a3min \u2016Y\u0302\u2016F + \u2016Y\u2016F ,\n=\u21d2 \u2016Y\u2016F \u2265 (1 \u2212 d\n\u03a3min )\u2016Y\u0302\u2016F . (124)\nThe first desired inequality (122a) follows immediately from (124), and the second desired inequality (122b) is proved by combining (124) and (123).\nCombining (122a) and (121), we obtain\n\u03b7 \u2264 1 108r . (125)\nIf \u03b7 \u2264 0, i.e. \u2016Y\u0302\u2016F \u2264 \u2016Y\u2016F , then U = X,V = Y\u0302 already satisfy (114). From now on, we assume \u03b7 > 0, i.e. \u2016Y\u0302\u2016F > \u2016Y\u2016F . Denote xTi , y\u0302Ti , uTi , vTi as the i-th row of X, Y\u0302 ,U,V , respectively. Denote \u03b1i , \u2220(xi, y\u0302i), i.e. the angle between the two vectors xi and y\u0302i. Since \u3008xi, y\u0302i\u3009 = \u03a3i > 0, we have \u03b1i \u2208 [0, \u03c02 ). Without loss of generality, assume\n\u03b11, . . . , \u03b1s > 3 8 \u03c0, \u03b1s+1, . . . , \u03b1r \u2264 3 8 \u03c0, (126)\nwhere s \u2208 {0, 1, . . . , r}. We consider three cases and construct U,V that satisfy the desired properties in the subsequent three subsections.\nCase 1 : r\u2211\ni=s+1\n\u2016y\u0302i\u20162 \u2265 2 3 \u2016Y\u0302\u20162F , r\u2211 i=s+1 \u2016xi\u20162 \u2265 2 3 \u2016X\u20162F . (127a)\nCase 2a : s\u2211\ni=1\n\u2016y\u0302i\u20162 > 1 3 \u2016Y\u0302\u20162F . (127b)\nCase 2b : s\u2211\ni=1\n\u2016xi\u20162 > 1 3 \u2016X\u20162F . (127c)"}, {"heading": "C.3.1 Proof of Case 1", "text": "Without loss of generality, assume \u2016xs+1\u2016 \u2264 \u2016xs+2\u2016 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u2016xr\u2016. (128)\nLet K be the smallest integer in {s + 1, s + 2, . . . , r} so that\nK\u2211 i=s+1 \u2016y\u0302i\u20162 \u2265 2 r\u2211 j=K+1 \u2016y\u0302 j\u20162. (129)\nBy this definition of K, we have K\u22121\u2211\ni=s+1\n\u2016y\u0302i\u20162 < 2 r\u2211\nj=K\n\u2016y\u0302 j\u20162. (130)\nWe will shrink and extend xi, y\u0302i to obtain U,V . The precise definition of U = (u1, u2, . . . , ur)T ,V = (v1, . . . , vr)T\nis given in Table 8.\nWe will show that such U,V satisfy the requirements (114). The requirement (114a) follows directly from the definition of U,V and the fact XY\u0302T = \u03a3.\nTable 8: Operation 1\nOperation 1: Shrinking and Extending Input: xk, y\u0302k, k = 1, . . . , r. Output: uk, vk, k = 1, . . . , r. Procedure:\n(i) For each j \u2264 s, keep x j, y\u0302 j unchanged, i.e.\nu j , x j, v j , y\u0302 j, j = 1, . . . , s. (131)\n(ii) For each i \u2208 {s + 1, . . . ,K}, extend xi to obtain ui and shrink y\u0302i to obtain vi. For each i \u2265 K + 1, shrink xi to obtain ui and extend y\u0302i to obtain vi. More specifically,\nui , xi\n1 \u2212 i , vi , y\u0302i(1 \u2212 i), where i = 7\u03b7\u0304 i \u2264 K,\u22124.5\u03b7\u0304 i \u2265 K + 1, i = s + 1, s + 2, . . . , r, (132) in which\n\u03b7\u0304 , d\n\u03a3min \u2265 \u03b7. (133)\nWe then prove the requirement (114c). We can bound \u2016U \u2212 X\u2016F as\n\u2016U \u2212 X\u2016F = \u221a\u2211\ni>s\n\u2016 1 1 \u2212 i xi \u2212 xi\u20162 = \u221a\u2211 i>s ( i 1 \u2212 i )2 \u2016xi\u20162\n\u2264 7\u03b7\u0304 1 \u2212 7\u03b7\u0304 \u221a\u2211 i>s \u2016xi\u20162 \u2264 7\u03b7\u0304 1 \u2212 7\u03b7\u0304\u2016X\u2016F \u2264 15 2 \u03b7\u0304\u03b2T .\n(134)\nThe bound of \u2016V \u2212 Y\u0302\u2016F is given as\n\u2016V \u2212 Y\u0302\u2016F = \u221a\u2211\ni>s\n\u2016(1 \u2212 i)y\u0302i \u2212 y\u0302i\u20162 \u2264 \u221a\u2211\ni>s\n2i \u2016y\u0302i\u20162 \u2264 7\u03b7\u0304\u2016Y\u0302\u2016F .\nCombining with the bound (123), we can bound \u2016V \u2212 Y\u2016F as\n\u2016V \u2212 Y\u2016F \u2264 \u2016V \u2212 Y\u0302\u2016F + \u2016Y\u0302 \u2212 Y\u2016F \u2264 7\u03b7\u0304\u2016Y\u0302\u2016F + d\n\u03a3min \u2016Y\u0302\u2016F\n= 8\u03b7\u0304\u2016Y\u0302\u2016F (122a) \u2264 8\u03b7\u0304 1 \u2212 \u03b7\u0304\u2016Y\u2016F \u2264 58 7 \u03b7\u0304\u03b2T .\n(135)\nThe first part of the requirement (114c) now follows by multiplying (134) and (135), and the second part of the requirement (114c) follows directly from (134) and (135).\nAt last, we prove that U,V satisfy the requirement (114b). Let\nS 1 , K\u2211\ni=s+1\n\u2016y\u0302i\u20162, S 2 , r\u2211\nj=K+1\n\u2016y\u0302 j\u20162, S 3 , s\u2211\nk=1\n\u2016y\u0302k\u20162,\nthen (129) and (127a) imply S 2 \u2264 S 1/2, S 3 \u2264 (S 1 + S 2)/2 \u2264 3S 1/4. (136)\nSince \u03b7\u0304 = d/\u03a3min \u2265 \u03b7, we have (1 \u2212 \u03b7)2(1 \u2212 \u03b7\u0304)2 \u2265 (1 \u2212 2\u03b7)(1 \u2212 2\u03b7\u0304) \u2265 (1 \u2212 2\u03b7\u0304)2. Then\n(1 \u2212 \u03b7)2(1 \u2212 \u03b7\u0304)2\u2016Y\u0302\u20162F \u2212 \u2016V\u20162F \u2265 (1 \u2212 2\u03b7\u0304)2\u2016Y\u0302\u20162F \u2212 \u2016V\u20162F =\n\u2211 i\u2265s+1 ((1 \u2212 2\u03b7\u0304)2\u2016y\u0302i\u20162 \u2212 \u2016vi\u20162) + \u2211 k\u2264s ((1 \u2212 2\u03b7\u0304)2\u2016y\u0302k\u20162 \u2212 \u2016vk\u20162)\n= \u2211\ni\u2265s+1 ((1 \u2212 2\u03b7\u0304)2\u2016y\u0302i\u20162 \u2212 (1 \u2212 i)2\u2016y\u0302i\u20162)\n+ \u2211 k\u2264s ((1 \u2212 2\u03b7\u0304)2\u2016y\u0302k\u20162 \u2212 \u2016y\u0302k\u20162)\n= \u2211\ni\u2265s+1 ( i \u2212 2\u03b7\u0304)(2 \u2212 i \u2212 2\u03b7\u0304)\u2016y\u0302i\u20162 \u2212 \u2211 k\u2264s 4\u03b7\u0304(1 \u2212 \u03b7\u0304)\u2016y\u0302k\u20162\n(132) = \u2211 s+1\u2264i\u2264K 5\u03b7\u0304(2 \u2212 5\u03b7\u0304 \u2212 2\u03b7\u0304)\u2016y\u0302i\u20162\n+ \u2211\nK< j\u2264r (\u22126.5\u03b7\u0304)(2 + 4.5\u03b7\u0304 \u2212 \u03b7\u0304)\u2016y\u0302 j\u20162 \u2212 \u2211 k\u2264s 4\u03b7\u0304(1 \u2212 \u03b7\u0304)\u2016y\u0302k\u20162\n= 5\u03b7\u0304(2 \u2212 7\u03b7\u0304)S 1 \u2212 6.5\u03b7\u0304(2 + 2.5\u03b7\u0304)S 2 \u2212 4\u03b7\u0304(1 \u2212 \u03b7\u0304)S 3 (136) \u2265 5\u03b7\u0304(2 \u2212 7\u03b7\u0304)S 1 \u2212 6.5\u03b7\u0304(2 + 2.5\u03b7\u0304)\n1 2 S 1 \u2212 4\u03b7\u0304(1 \u2212 \u03b7\u0304) 3 4 S 1\n\u2265 (0.5 \u2212 41\u03b7\u0304)\u03b7\u0304S 1 \u2265 0,\n(137)\nwhere the last inequliaty follows from (121). Note that (1 \u2212 \u03b7)\u2016Y\u0302\u2016F = \u2016Y\u2016F , thus (137) implies\n\u2016V\u2016F \u2264 (1 \u2212 \u03b7)(1 \u2212 \u03b7\u0304)\u2016Y\u0302\u2016F = (1 \u2212 d\n\u03a3min )\u2016Y\u2016F ,\nwhich proves the second part of (114b).\nWe then prove the first part of (114b), i.e. \u2016U\u2016F \u2264 \u2016X\u2016F . Let\nT1 , K\u2211\ni=s+1\n\u2016xi\u20162, T2 , r\u2211\nj=K\n\u2016x j\u20162.\nWe claim that T2 \u2265 2T1. (138)\nWe prove (138) by contradiction. Assume the contrary that T2 < 2T1, then 13 (T2 + T1) < T1, i.e.\n1 3 r\u2211 k=s+1 \u2016xk\u20162 < K\u2211 i=s+1 \u2016xi\u20162 (128) \u2264 (K \u2212 s)\u2016xK\u20162. (139)\nPlugging the second inequality of (127a), i.e. \u2211r\nk=s+1 \u2016xk\u20162 \u2265 23 \u2016X\u20162F , into the above relation, we obtain\n\u2016X\u20162F \u2264 9 2 (K \u2212 s)\u2016xK\u20162 \u2264 9 2 K\u2016xK\u20162. (140)\nWhen j \u2208 {K,K + 1, . . . , r}, we have\n\u03a3max \u2265 \u03a3 j = \u3008x j, y\u0302 j\u3009 = \u2016x j\u2016\u2016y\u0302 j\u2016 cos(\u03b1 j) (128),(126) \u2265 \u2016xK\u2016\u2016y\u0302 j\u2016 cos(3\u03c0/8).\nwhich implies\n\u2016y\u0302 j\u2016 \u2264 \u03c9, where \u03c9 , 1 cos(3\u03c0/8) \u03a3max \u2016xK\u2016 , j = K,K + 1, . . . , s. (141)\nTherefore,\n\u2016Y\u0302\u20162F (127a) \u2264 3\n2 r\u2211 j=s+1 \u2016y\u0302 j\u20162 (130) \u2264 9 2 r\u2211 j=K \u2016y\u0302 j\u20162 (141) \u2264 9 2 (r \u2212 K + 1)\u03c92. (142)\nCombining (140) and (142), and using K(r \u2212 K + 1) \u2264 14 (r + 1)2 \u2264 r2, we get\n\u2016X\u20162F\u2016Y\u0302\u20162F \u2264 81 4 r2\u2016xK\u20162\u03c92\n(141) = 81 4 r2\u2016xK\u20162 1 cos(3\u03c0/8)2 \u03a32max \u2016xK\u20162 < 140r2\u03a32max.\n(143)\nAccording to (113), we have \u2016X\u20162F\u2016Y\u0302\u20162F \u2265 \u2016X\u20162F\u2016Y\u20162F \u2265 ( 35 )2\u03b24T = 9 25C 2 T r 2\u03a32max; combining with (143), we get 140 > 925C 2 T , which implies C 2 T < 389. This contradicts the definition (120) that CT = 20, thus (138) is proved.\nNow we are ready to prove the first part of (114b) as follows: \u2016X\u20162F \u2212 \u2016U\u20162F = \u2211\ni\u2265s+1 (\u2016xi\u20162 \u2212 \u2016ui\u20162) + \u2211 k\u2264s (\u2016xk\u20162 \u2212 \u2016uk\u20162)\n= \u2211\ni\u2265s+1 (\u2016xi\u20162 \u2212 1 (1 \u2212 i)2 \u2016xi\u20162) + 0\n= \u2211\ni\u2265s+1\ni( i \u2212 2) (1 \u2212 i)2 \u2016xi\u20162\n= \u2211\nK< j\u2264r\n4.5\u03b7\u0304(4.5\u03b7\u0304 + 2) (1 + 4.5\u03b7\u0304)2\n\u2016x j\u20162 \u2212 \u2211\ns+1\u2264i\u2264K\n7\u03b7\u0304(2 \u2212 7\u03b7\u0304) (1 \u2212 7\u03b7\u0304)2 \u2016xi\u2016 2\n(138) \u2265 T2\u03b7\u0304 [ 4.5(4.5\u03b7\u0304 + 2) (1 + 4.5\u03b7\u0304)2 \u2212 1 2 7(2 \u2212 7\u03b7\u0304) (1 \u2212 7\u03b7\u0304)2 ] \u2265 T2\u03b7\u0304 [ 9\n(1 + 4.5\u03b7\u0304)2 \u2212 7 (1 \u2212 7\u03b7\u0304)2\n] \u2265 0,\nwhere the last inequality is because (1\u22127\u03b7\u0304) 2\n(1+4.5\u03b7\u0304)2 > 0.79 > 7 9 when \u03b7\u0304 \u2264 1/(108r) < 1/100. Thus the first part of (114b) is\nproved."}, {"heading": "C.3.2 Proof of Case 2a", "text": "Denote X0 = X,Y0 = Y\u0302 , x0k = xk, y 0 k = y\u0302k, \u03b1 0 k = \u03b1k, k = 1, . . . , r. (144)\nWe will define Xi = (xi1, . . . , x i r) T ,Y i = (yi1, . . . , y i r) T recursively. In specific, at the i-th iteration, we will adjust Xi\u22121,Y i\u22121 to Xi,Y i so that \u2016Xi\u2016F \u2264 \u2016Xi\u22121\u2016F , \u2016Y i\u2016F < \u2016Y i\u22121\u2016F while keeping the first requirement satisfied, i.e. Xi(Y i)T = \u03a3. The angle \u03b1ik is defined accordingly, i.e. \u03b1 i k , \u3008xik, yik\u3009.\nTo adjust Xi\u22121,Y i\u22121 to Xi,Y i, we will define an operation that consists of rotation and shrinking. The basic idea is the following: since the angle between xi\u22121i and y i\u22121 i is large, we can rotate x i\u22121 i to x i i and shrink y i\u22121 i to y i i to keep the inner product invariant, i.e. \u3008xi\u22121i , yi\u22121i \u3009 = \u3008xii, yii\u3009. However, rotating xi\u22121i may destroy the orthogonal relationship between xi\u22121i and y i\u22121 j ,\u2200 j , i, thus we further rotate and shrink yi\u22121j to yij for all j , i so that yij is orthogonal to the new vector xii. Fortunately, we can prove that using such an operation we still have \u3008xi\u22121j , yij\u3009 = \u03a3 j,\u2200 j , i.\nA complete description of this operation is given in Table 9. Without loss of generality, we can make the assumption (145). In fact, if (145) does not hold, we can switch i and mi , arg mink\u2208{i,i+1,...,s} \u03b1i\u22121k and then apply Operation 2.\nWe will prove that Operation 2 is valid (for Di that is small enough), i.e. Xi,Y i defined in Operation 2 indeed exist. The properties of Xi,Y i obtained by Operation 2 are summarized in the following claim, which will be proved in Appendix C.4.\nClaim C.2 Consider i \u2208 {1, 2, . . . , s}. Suppose\n\u03b1i\u22121i \u2264 \u03b1i\u22121j , \u2200 j \u2208 {i + 1, i + 2, . . . , s}, (145)\nand Di > 0 satisfies Di \u03a3i \u2264 1 12r , (146) then Xi = (xi1, . . . , x i r) T ,Y i = (yi1, . . . , y i r) T described in Operation 2 exist and satisfy the following properties:\nXi(Y i)T = \u03a3, (147a)\n\u2016xik\u2016 = \u2016xi\u22121k \u2016,\u2200k, \u2016Y i \u2212 Y i\u22121\u20162F \u2264 4 5 Di \u03a3i (\u2016Y i\u22121\u20162F \u2212 \u2016Y i\u20162F), (147b)\n\u2016Xi \u2212 Xi\u22121\u2016F = \u2016xii \u2212 xi\u22121i \u2016 \u2264 1 \u221a\n3 Di \u03a3i \u2016xi\u22121i \u2016\n\u2016Y i \u2212 Y i\u22121\u2016F \u2264 2 \u221a\n3 Di \u03a3i \u2016Y i\u22121\u2016F , (147c)\n\u03b1il \u2265 \u03b1i\u22121l \u2212 1 r \u03c0 24 \u2265 1 3 \u03c0, l = i, i + 1, . . . , s. (147d)\n\u2016yi\u22121k \u2016 \u2265 \u2016yik\u2016 \u2265 \u2016yi\u22121k \u2016 \u2212 1\n10r \u2016yi\u22121k \u2016, k = 1, 2, . . . , s. (147e)\n\u2016Y i\u22121\u20162F \u2212 \u2016Y i\u20162F \u2265 5 3 Di \u03a3i \u2016yii\u20162. (147f)\nWe continue to prove Proposition C.1 using Claim C.2. Given any D1, . . . ,Ds that satisfy (146), we can apply a sequence of Operation 2 for i = 1, 2, . . . , s to define two sequences of matrices Y1, . . . ,Y s and X1, . . . , Xs. Since Y1, . . . ,Y s depend on D1, . . . ,Ds, thus we can use Y s(D1, . . . ,Ds) to denote the obtained Y s by applying Operation 2 for D1, . . . ,Ds. Obviously Y s(0, . . . , 0) = Y0. We can also view \u2016Y s\u20162F as a function of D1, . . . ,Ds, denoted as\nf (D1, . . . ,Ds) , \u2016Y s(D1, . . . ,Ds)\u20162F . (148)\nIt can be easily seen that f is a continuous function with respect to D1, . . . ,Ds.\nDefine5\n\u03b7\u0304 , d\n\u03a3min\n(122a) \u2265 \u03b7, D\u0304i , 9\u03b7\u0304\u03a3i, i = 1, . . . , s. (149)\n5In the first version of the paper, we define D\u0304i , 92 \u03b7\u03a3i \u2264 9 2 \u03b7\u0304\u03a3i \u2264 9 d \u03a3min \u03a3i, which is enough for proving Theorem 3.1. Here we use a slightly different definition of D\u0304i for the purpose of proving Theorem 3.2 (linear convergence of the algorithm.)\nWe prove that f (D\u03041, . . . , D\u0304s) \u2264 (1 \u2212 4\u03b7\u0304)\u2016Y\u0302\u20162F . (150)\nSuppose X\u0304i, Y\u0304 i, i = 1, . . . , s are recursively defined by Operation 2 for the choices of Di = D\u0304i and denote X\u03040 = X, Y\u03040 = Y\u0302 . Since\n\u03b7\u0304 = d/\u03a3min (121) \u2264 1/(108r),\nwe know that Di = D\u0304i, i = 1, . . . , s as defined in (149) satisfy the condition (146), thus the property (147) holds for X\u0304i, Y\u0304 i. Suppose the k-th row of Y\u0304 i is (y\u0304ik) T , k = 1, . . . , r. By (147f) and the fact Y\u0302 = Y\u03040, we have\n\u2016Y\u0302\u20162F \u2212 f (D\u03041, . . . , D\u0304s) = \u2016Y\u03040\u20162F \u2212 \u2016Y\u0304 s\u20162F\n= s\u2211 i=1 (\u2016Y\u0304 i\u22121\u20162F \u2212 \u2016Y\u0304 i\u20162F) \u2265 s\u2211 i=1 5 3 D\u0304i \u03a3i \u2016y\u0304ii\u20162.\n(151)\nWe can bound \u2016y\u0304ii\u2016 according to (147e) as\n\u2016y\u0304ii\u2016 \u2265 \u2016y\u0304i\u22121i \u2016 \u2212 1\n10r \u2016y\u0304i\u22121i \u2016 \u2265 \u2016y\u0304i\u22121i \u2016 \u2212 1 10r \u2016y\u03040i \u2016\n\u2265 \u00b7 \u00b7 \u00b7 \u2265 \u2016y\u03040i \u2016 \u2212 i\n10r \u2016y\u03040i \u2016 \u2265 9 10 \u2016y\u03040i \u2016.\nPlugging into (151), we get\n\u2016Y\u0302\u20162F \u2212 f (D\u03041, . . . , D\u0304s) \u2265 s\u2211\ni=1\n5 3 D\u0304i \u03a3i ( 9 10 )2\u2016y\u03040i \u20162\n(149) = 15 81 100 \u03b7\u0304 s\u2211 i=1 \u2016y\u0302i\u20162 (127b) > 12\u03b7\u0304 1 3 \u2016Y\u0302\u20162F = 4\u03b7\u0304\u2016Y\u0302\u20162F ,\nwhich immediately leads to (150).\nCombining (150) and the fact f (0, . . . , 0) = \u2016Y0\u20162F = \u2016Y\u0302\u20162F , we have\nf (0, . . . , 0) = \u2016Y\u0302\u20162F > (1 \u2212 4\u03b7\u0304)\u2016Y\u0302\u20162F = f (D\u03041, . . . , D\u0304s).\nSince f is continuous (in the proof of Claim C.2 in Appendix C.4, all new vectors depend continuously on Di), and notice that 1 \u2212 4\u03b7\u0304 < (1 \u2212 \u03b7\u0304)4 \u2264 (1 \u2212 \u03b7\u0304)2(1 \u2212 \u03b7)2 \u2264 1, there must exist\n0 \u2264 Di \u2264 D\u0304i = 9\u03b7\u0304\u03a3i, i = 1, . . . , s (152)\nsuch that f (D1, . . . ,Ds) = (1 \u2212 \u03b7\u0304)2(1 \u2212 \u03b7)2\u2016Y\u0302\u20162F . (153)\nSuppose Xi,Y i, i = 1, . . . , s are recursively defined by Operation 2 for these choices of Di, where Y s is the simplified notation for Y s(D1, . . . ,Ds). Define\nV , Y s, U , Xs, (154)\nBy this definition of V and (148), the relation (153) can be rewritten as\n\u2016V\u20162F = (1 \u2212 \u03b7\u0304)2(1 \u2212 \u03b7)2\u2016Y\u0302\u20162F . (155)\nWe show that U,V defined by (154) satisfy the requirements (114). The requirement (114a) follows by the property (147a) for i = s. The requirement (114b) is proved as follows. Combining (155) with (122a) leads to\n\u2016V\u2016F = (1 \u2212 \u03b7\u0304)(1 \u2212 \u03b7)\u2016Y\u0302\u2016F = (1 \u2212 \u03b7\u0304)\u2016Y\u2016F = (1 \u2212 d\n\u03a3min )\u2016Y\u2016F . (156)\nAccording to the property (147b), we have \u2016Xi\u2016F = \u2016Xi\u22121\u2016F , i = 1, . . . , s. Thus \u2016Xs\u2016F = \u2016Xs\u22121\u2016F = \u00b7 \u00b7 \u00b7 = \u2016X0\u2016F = \u2016X\u2016F , which implies\n\u2016U\u2016F = \u2016X\u2016F . (157)\nCombining (157) and (156) leads to the requirement (114b) .\nIt remains to show that U,V satisfy the requirement (114c). By the property (147b), we have \u2016xi\u22121k \u2016 = \u2016xik\u2016,\u22001 \u2264 k \u2264 r, 1 \u2264 i \u2264 s, which implies\n\u2016xik\u2016 = \u2016x0k\u2016 = \u2016xk\u2016, \u22001 \u2264 k \u2264 r, 1 \u2264 i \u2264 s. (158)\nNote that Xi differs from Xi\u22121 only in the i-th row (according to (147c)), thus\n\u2016U \u2212 X\u2016F = \u2016Xs \u2212 X0\u2016F =\n\u221a s\u2211\ni=1\n\u2016xii \u2212 xi\u22121i \u20162\n(147c) \u2264 1\u221a\n3 Di \u03a3i\n\u221a s\u2211\ni=1\n\u2016xi\u22121i \u20162 (158) = 1 \u221a\n3 Di \u03a3i\n\u221a s\u2211\ni=1\n\u2016xi\u20162\n\u2264 1\u221a 3 Di \u03a3i \u2016X\u2016F (152) \u2264 3 \u221a 3\u03b7\u0304\u2016X\u2016F .\n(159)\nPlugging \u03b7\u0304 = d/\u03a3min and \u2016X\u2016F \u2264 \u03b2T into the above inequality, we get\n\u2016U \u2212 X\u2016F \u2264 3 \u221a 3 \u03b2T\n\u03a3min d. (160)\nWe then bound \u2016V \u2212 Y\u0302\u20162F as\n\u2016V \u2212 Y\u0302\u20162F = \u2016Y s \u2212 Y0\u20162F\n\u2264 s s\u2211\ni=1\n\u2016Y i \u2212 Y i\u22121\u20162F (147b) \u2264 s4 5 Di \u03a3i s\u2211 i=1 (\u2016Y i\u22121\u20162F \u2212 \u2016Y i\u20162F)\n= s 4 5 Di \u03a3i (\u2016Y0\u20162F \u2212 \u2016Y s\u20162F) = s 4 5 Di \u03a3i (\u2016Y\u0302\u20162F \u2212 \u2016V\u20162F) (152) \u2264 36\n5 s\u03b7\u0304(\u2016Y\u0302\u20162F \u2212 \u2016V\u20162F)\n(155) \u2264 36\n5 s\u03b7\u0304(2\u03b7 + 2\u03b7\u0304)\u2016Y\u0302\u20162F \u2264 144 5 r\u03b7\u03042\u2016Y\u0302\u20162F ,\nwhich leads to\n\u2016V \u2212 Y\u0302\u2016F \u2264 12 \u221a 5 \u03b7\u0304 \u221a r\u2016Y\u0302\u2016F . (161)\nThen we can bound \u2016V \u2212 Y\u2016F as\n\u2016V \u2212 Y\u2016F \u2264 \u2016V \u2212 Y\u0302\u2016F + \u2016Y \u2212 Y\u0302\u2016 (161),(123) \u2264 12\u221a\n5 \u03b7\u0304 \u221a r\u2016Y\u0302\u2016F + d \u03a3min \u2016Y\u0302\u2016F = ( 12 \u221a 5 + 1) d \u03a3min \u221a r\u2016Y\u0302\u2016F\n(122a) = ( 12 \u221a\n5 + 1) d \u03a3min \u221a r\u2016Y\u2016F 1 1 \u2212 \u03b7 < 13d 2\u03a3min \u221a r\u2016Y\u2016F \u2264 13\u03b2T 2\u03a3min \u221a rd,\n(162)\nwhere the second last inequality is due to ( 12\u221a 5\n+ 1)/(1 \u2212 \u03b7) (121) \u2264 ( 12\u221a\n5 + 1)/(1 \u2212 1108 ) < 6.5. The first part of the\nrequirement (114c) now follows by multiplying (160) and (162), and the second part of the requirement (114c) follows directly from (160) and (162)."}, {"heading": "C.3.3 Proof of Case 2b", "text": "Similar to Case 2a, denote X0 = X,Y0 = Y\u0302 , x0k = xk, y 0 k = y\u0302k, \u03b1 0 k = \u03b1k.\nBy a symmetric argument to that for Case 2a (switch the role of U, X j, j = 0, . . . , s and V,Y j, j = 0, . . . , s), we can prove that there exist U\u0304, V\u0304 that satisfy properties analogous to (114a), (156), (157), (159) and (161), i.e.\nU\u0304V\u0304T = \u03a3, (163a)\n\u2016U\u0304\u2016F = (1 \u2212 \u03b7)(1 \u2212 \u03b7\u0304)\u2016X0\u2016F , \u2016V\u0304\u2016F = \u2016Y0\u2016F , (163b)\n\u2016V\u0304 \u2212 Y0\u2016F \u2264 3 \u221a 3\u03b7\u0304\u2016Y0\u2016F , \u2016U\u0304 \u2212 X0\u2016F \u2264 12 \u221a 5 \u03b7\u0304 \u221a r\u2016X0\u2016F . (163c)\nWe will show that the following U,V satisfy the requirements (114):\nU , U\u0304\n(1 \u2212 \u03b7)(1 \u2212 \u03b7\u0304) , V , V\u0304(1 \u2212 \u03b7)(1 \u2212 \u03b7\u0304). (164)\nThe requirement (114a) follows directly from (163a) and (164). According to (163b), (164) and the facts X0 = X, \u2016Y0\u2016F = \u2016Y\u0302\u2016F = \u2016Y\u2016F/(1 \u2212 \u03b7), we have \u2016U\u2016F = \u2016U\u0304\u2016F(1\u2212\u03b7)(1\u2212\u03b7\u0304) = \u2016X0\u2016F = \u2016X\u2016F , \u2016V\u2016F = \u2016V\u0304\u2016F(1 \u2212 \u03b7)(1 \u2212 \u03b7\u0304) = \u2016Y0\u2016F(1 \u2212 \u03b7)(1 \u2212 \u03b7\u0304) = \u2016Y\u2016F(1 \u2212 \u03b7\u0304), thus the requirement (114b) is proved.\nIt remains to prove the requirement (114c). We bound \u2016U \u2212 X\u2016F as\n\u2016U \u2212 X\u2016F \u2264 \u2016U \u2212 U\u0304\u2016F + \u2016U\u0304 \u2212 X\u2016F (164) \u2264 2\u03b7\u0304\u2016U\u2016F + \u2016U\u0304 \u2212 X0\u2016F\n(114b),(163c) \u2264 2\u03b7\u0304\u2016X\u2016F + 12 \u221a 5 \u03b7\u0304 \u221a r\u2016X0\u2016F \u2264 15 2 \u03b7\u0304 \u221a r\u2016X\u2016F \u2264 15 2 \u03b2T \u03a3min \u221a rd.\n(165)\nUsing the fact Y\u0302 = Y0, we bound \u2016V \u2212 Y\u2016F as\n\u2016V \u2212 Y\u2016F \u2264 \u2016V \u2212 V\u0304\u2016F + \u2016V\u0304 \u2212 Y\u0302\u2016F + \u2016Y\u0302 \u2212 Y\u2016F (164),(123) \u2264 2\u03b7\u0304\u2016V\u0304\u2016F + \u2016V\u0304 \u2212 Y0\u2016F +\nd \u03a3min \u2016Y\u0302\u2016F\n(163b),(163c) \u2264 2\u03b7\u0304\u2016Y\u0302\u2016F + 3 \u221a 3\u03b7\u0304\u2016Y0\u2016F + d \u03a3min \u2016Y\u0302\u2016F = (3 + 3 \u221a 3) d\n\u03a3min \u2016Y\u0302\u2016F\n(122a) =\n3 + 3 \u221a\n3 1 \u2212 \u03b7 d \u03a3min \u2016Y\u2016F \u2264 58\u03b2T 7\u03a3min d.\n(166)\nThe first part of the requirement (114c) now follows by multiplying (165) and (166), and the second part follows directly from (165) and (166)."}, {"heading": "C.4 Proof of Claim C.2", "text": "Suppose Claim C.2 holds for 1, 2, . . . , i \u2212 1, we prove Claim (C.2) for i. By the property (147a) and (147d) of Claim C.2 for i \u2212 1, we have\nXi\u22121(Y i\u22121)T = \u03a3. (167a)\n\u03b1i\u22121i \u2265 \u03b1 [0] i \u2212 i \u2212 1 r 1 24 \u03c0 \u22653 8 \u03c0 \u2212 1 24 \u03c0 + 1 24r \u03c0 = 1 3 \u03c0 + 1 24r \u03c0 \u2265 1 3 \u03c0. (167b)\nTo simplify the notations, throughout the proof of Claim C.2, we denote Xi\u22121,Y i\u22121 as X,Y and denote Xi,Y i as X\u2032,Y \u2032. The notations \u03b1i\u22121k , \u03b1 i k are changed accordingly to \u03b1k, \u03b1 \u2032 k. Then (167a) and (167b) become\nXYT = \u03a3, (168a)\n\u03b1i \u2265 1 3 \u03c0 + 1 24r \u03c0 \u2265 1 3 \u03c0. (168b)\nWe need to prove that X\u2032,Y \u2032 exist and satisfy the properties in Claim (C.2), i.e. (with the simplification of notations)\nX\u2032(Y \u2032)T = \u03a3. (169a)\n\u2016x\u2032k\u2016 = \u2016xk\u2016,\u2200k, \u2016Y \u2032 \u2212 Y\u20162F \u2264 4 5 Di \u03a3i (\u2016Y\u20162F \u2212 \u2016Y \u2032\u20162F). (169b)\n\u2016X\u2032 \u2212 X\u2016F = \u2016x\u2032i \u2212 xi\u2016 \u2264 1 \u221a\n3 Di \u03a3i \u2016xi\u2016, \u2016Y \u2032 \u2212 Y\u2016F \u2264 2 \u221a 3 Di \u03a3i \u2016Y\u2016F . (169c)\n\u03b1\u2032l \u2265 \u03b1l \u2212 1 r \u03c0 24 \u2265 1 3 \u03c0, l = i, i + 1, . . . , s. (169d)\n\u2016yk\u2016 \u2265 \u2016y\u2032k\u2016 \u2265 \u2016yk\u2016 \u2212 1\n10r \u2016yk\u2016, k = 1, 2, . . . , s. (169e)\n\u2016Y\u20162F \u2212 \u2016Y \u2032\u20162F \u2265 5 3 Di \u03a3i \u2016yi\u20162. (169f)\nC.4.1 Ideas of the proof of Claim (C.2)\nBefore presenting the formal proof, we briefly describe its idea. The goal of Operation 2 is to reduce the norm of Y while keeping \u3008X,Y\u3009 and \u2016X\u2016F invariant, by rotating and shrinking xi, yk, k = 1, . . . ,K (note that x j,\u2200 j , i, do no change). We first rotate xi and shrink yi at the same time so that the new inner product \u3008x\u2032i , y\u2032i\u3009 equals the previous one \u3008xi, yi\u3009 (this step can be viewed as a combination of two steps: first rotate xi to increase the inner product, then shrink yi to reduce the inner product). In order to preserve the orthogonality of X and Y , we need to rotate y j,\u2200 j , i, so that the new y\u2032j is orthogonal to x \u2032 i .\nAlthough the above procedure is simple, there are two questions to be answered. The first question is: will the inner product \u3008x j, y j\u3009 increase as we rotate y j, for all j , i? If yes, we could first rotate and then shrink y j to obtain y\u2032j so that the new inner product \u3008x j, y\u2032j\u3009 equals \u3008x j, y j\u3009, which achieves the goal of Operation 2. By resorting to the geometry (in a rigourous way) we are able to provide an affirmative answer to the above question. To gain an intuition why this is possible, we use Figure 5 to illustrate. Consider the case i = 2 and rotate x2 towards y2 to obtain x\u20322, then y1 has to be rotated so that y \u2032 1 is orthogonal to x \u2032 2. It is clear from this figure that the angle between y1 and x1 also decreases, or equivalently, the inner product \u3008x1, y1\u3009 also increases. One might ask whether we have utilized additional assumptions on the relative positions of xi, yi\u2019s. In fact, we do not utilize additional assumptions; what\nwe implicitly utilize is the fact that \u3008xi, yi\u3009 > 0,\u2200i (see Figure 6, Figure 7 and the paragraph after (176) for detailed explanations).\nThe second question is: will the angle \u03b1\u2032j = \u2220(x j, y \u2032 j) still be larger than, say, 1 3\u03c0, for all j > i? If yes, then we can apply Operation 2 repeatedly for all i = 1, 2, . . . , s. To provide an affirmative answer, we should guarantee that each angle decreases at most 1s ( 3 8\u03c0\u2212 1 3\u03c0) = 1 24s\u03c0, i.e. \u2220(x j, y \u2032 j) \u2265 \u2220(x j, y j)\u2212 124s\u03c0,\u2200 i < j \u2264 s. Unlike the first question which can be answered by reading Figure 6 and Figure 7, this question cannot be answered by just reading figures. We make some algebraic computation to obtain the following result: under the assumption that \u03b1i is no less than \u03b1 j, during Operation 2 the amount of decrease in \u03b1 j is upper bounded by the amount of decrease in \u03b1i, which can be further bounded above by 124s\u03c0. This result explains why our proof requires the assumption \u03b1i \u2265 \u03b1 j,\u2200 i < j \u2264 s, i.e. (145).\nC.4.2 Formal proof of Claim (C.2)\nWe first show how to define x\u2032i and y \u2032 i . Note that\n\u2016xi\u2016\u2016yi\u2016 = \u3008xi, yi\u3009 cos\u03b1i \u2265 \u03a3i cos( \u03c03 ) = 2\u03a3i. (170)\nSince (170) implies \u03a3i+Di\u2016xi\u2016\u2016yi\u2016 \u2264 2\u03a3i \u2016xi\u2016\u2016yi\u2016 \u2264 1, we can define\n\u03b1\u2032i , arccos( \u03a3i + Di \u2016xi\u2016\u2016yi\u2016 ) \u2208 [0, \u03c0 2 ].\nThere is a unique x\u2032i in the plane span{xi, yi} which satisfies\n\u2016x\u2032i\u2016 = \u2016xi\u2016 (171)\nand \u2220(x\u2032i , yi) = \u03b1 \u2032 i . By the definition of \u03b1 \u2032 i above, we have\n\u3008x\u2032i , yi\u3009 = \u03a3i + Di.\nThe existence of x\u2032i is proved. We define\ny\u2032i , \u03a3i\n\u03a3i + Di yi, (172)\nthen \u3008x\u2032i , y\u2032i\u3009 = \u03a3i\n\u03a3i + Di \u3008x\u2032i , yi\u3009 = \u03a3i. (173)\nThe existence of y\u2032i is also proved.\nSince 0 < \u3008xi, yi\u3009 = \u03a3i < \u3008x\u2032i , yi\u3009, we have \u03c02 > \u03b1i > \u03b1\u2032i > 0, thus we can define\n\u03b8 , \u03b1i \u2212 \u03b1\u2032i = \u2220(x\u2032i , xi) \u2208 (0, \u03b1i). (174)\nFix any j , i, we then show how to define y\u2032j. Define\nAi , span j,i{x j}\u22a5yi, Bi , span j,i{y j}\u22a5xi, Ti , Ai \u2229 Bi.\nLet \u2212\u2212\u2192 OY j = y j, K j , PAi (Y j),H j , PTi (Y j). Then \u2220Y jH jK j = min{\u2220(xi, yi), \u03c0 \u2212 \u2220(xi, yi)} = \u2220(xi, yi) = \u03b1i. Since \u03b1i > \u03b8, there exists a unique point Y \u2032j in the line segment Y jK j such that\n\u2220Y jH jY \u2032j = \u03b8. (175)\nSince K j = PAi (Y j) and xk \u2208 Ai,\u2200k , i, we have \u2212\u2212\u2212\u2192 Y jK j\u22a5xk,\u2200k , i, thus\n\u2212\u2212\u2212\u2192 Y jY \u2032j\u22a5xk, \u2200k , i. (176)\nSee Figure 6 and Figure 7 for the geometrical interpretation; note that Ti in general is not a line but a r\u22122 dimensional space. The righthand side subfigures represents the 2 dimensional subspace T\u22a5i ; since span{H jY j,H jK j} = T\u22a5i = span{xi, yi}, we can draw xi, yi, y\u2032i as the vectors starting from H j and lying in the plane H jY jK j = T\u22a5i in the figures. Figure 6 and Figure 7 differ in the relative position of xi and K j: xi and K j lie in the same side of line H jY j in Figure 6 but in different sides in Figure 7. Given the positions of xi and H j,Y j,K j, the position of yi is determined since yi\u22a5 \u2212\u2212\u2212\u2192 H jK j and \u2220(xi, yi) < \u03c02 .\nIn both figures, we have\n\u2220( \u2212\u2212\u2212\u2192 H jY \u2032j, x \u2032 i ) = \u2220( \u2212\u2212\u2212\u2192 H jY j, xi) \u2212 \u2220(x\u2032i , xi) + \u2220Y jH jY \u2032j\n(174),(175) ========\n\u03c0 2 \u2212 \u03b8 + \u03b8 = \u03c0 2 ,\n=\u21d2 \u2212\u2212\u2212\u2192 H jY \u2032j\u22a5x\u2032i . (177)\nNow we are ready to define y\u2032j and establish its properties. Define\ny\u2032j , \u2212\u2212\u2192 OY \u2032j. (178)\nSince Y \u2032j lies in the line segment K jY j and \u2220Y jK jO = \u03c0/2, we have\n\u2016y\u2032j\u2016 \u2264 \u2016y j\u2016. (179)\nWe also have y\u2032j = y j + \u2212\u2212\u2212\u2192 Y jY \u2032j \u2208 span{y j, yi} \u22a5xk, \u2200k , i, j. (180) According to the fact \u2212\u2212\u2212\u2192 OH j\u22a5x\u2032i and (177), we have\ny\u2032j = \u2212\u2212\u2212\u2192 OH j + \u2212\u2212\u2212\u2192 H jY \u2032j \u22a5 x\u2032i . (181)\nLet k = j in (176), we obtain\n0 = \u3008 \u2212\u2212\u2212\u2192 Y jY \u2032j, x j\u3009 = \u3008y\u2032j \u2212 y j, x j\u3009 = 0 =\u21d2 \u3008x j, y\u2032j\u3009 = \u3008x j, y j\u3009. (182)\nWe have shown that y\u2032j defined in (178) satisfies (180), (181) and (182), thus the existence of y \u2032 j in Operation 2 is proved.\nHaving defined x\u2032i , y \u2032 i and y \u2032 j,\u2200 j , i, we further define\nx\u2032j , x j,\u2200 j , i, (183)\nwhich completes the definition of X\u2032,Y \u2032. In the rest, we prove that X\u2032,Y \u2032 satisfy the desired property (169).\nThe property (169a) can be directly proved by the definitions of X\u2032,Y \u2032. In specific, according to (173), (182) and the definition (183), we have \u3008x\u2032k, y\u2032k\u3009 = \u03a3k,\u2200k. According to the definitions (183), (172) and the fact yi\u22a5x j,\u2200 j , i, we have y\u2032i\u22a5x\u2032j,\u2200 j , i. Together with (180) and (181), we obtain \u3008x\u2032k, y\u2032l\u3009 = 0,\u2200k , l. Thus X\u2032(Y \u2032)T = \u03a3.\nNext, we prove the property (169d). We first prove\n\u03b1\u2032i \u2212 \u03b1i = \u03b8 \u2264 1 r \u03c0 24 . (184)\nDefine hi , x\u2032i \u2212 xi, then \u2016hi\u2016 = 2\u2016xi\u2016 sin( \u03b8\n2 ). (185)\nFrom \u3008x\u2032i , yi\u3009 = \u03a3i + Di = \u3008xi, yi\u3009 + Di, we obtain \u3008hi, yi\u3009 = Di. Note that \u3008hi, yi\u3009 = \u2016hi\u2016\u2016yi\u2016 cos(\u2220(hi, yi)) and \u2220(hi, yi) = \u03c02 \u2212 \u03b1i + \u03b8 2 , thus\n\u2016hi\u2016 = Di\n\u2016yi\u2016 sin(\u03b1i \u2212 \u03b82 ) . (186)\nAccording to (185) and (186), we have\nDi \u2016xi\u2016\u2016yi\u2016 = 2 sin(\u03b1i \u2212 \u03b8 2 ) sin( \u03b8 2 ) \u2265 2 sin(\u03b1i 2 ) sin( \u03b8 2 )\n\u2265 2 sin(\u03c0 6 ) sin( \u03b8 2 ) = sin( \u03b8 2 ) \u2265 \u03b8 \u03c0 ,\nwhere the last equality follows from the fact that sin(t)t is decreasing in t \u2208 (0, \u03c0 2 ]. Note that Di \u2016xi\u2016\u2016yi\u2016 can be upper bounded as Di\n\u2016xi\u2016\u2016yi\u2016 (170) \u2264 Di 2\u03a3i (146) \u2264 1 24r .\nCombining the above two relations, we get (184).\nTo prove \u03b1 j \u2212 \u03b1\u2032j \u2264 \u03c0\n24r ,\u2200 j \u2208 {i + 1, . . . , s}, (187)\nwe only need to prove \u03b8 j , \u03b1 j \u2212 \u03b1\u2032j \u2264 \u03b8, \u2200 j \u2208 {i + 1, . . . , s} (188)\nand then use (184). The equality (182) implies that \u2016x j\u2016\u2016y j\u2016 cos(\u03b1 j) = \u2016x j\u2016\u2016y\u2032j\u2016 cos(\u03b1\u2032j), which leads to\ncos(\u03b1 j) cos(\u03b1 j \u2212 \u03b8 j) = cos(\u03b1 j) cos(\u03b1\u2032j) = \u2016y\u2032j\u2016 \u2016y j\u2016 .\nFor any two points P1, P2, we use |P1P2| to denote the length of the line segment P1P2. Since \u2212\u2212\u2212\u2192 OH j is orthogonal to plane H jK jY j, we have \u2016y\u2032j\u20162\n\u2016y j\u20162 = |OH j|2 + |H jY \u2032j |2 |OH j|2 + |H jY j|2 \u2265 |H jY \u2032j |2 |H jY j|2 ,\nwhere the last inequality follows from the fact that |H jY \u2032j | \u2264 |H jY j|. Since \u2220Y jH jK j = \u03b1i, \u2220Y \u2032jH jK j = \u03b1\u2032i and \u2220Y jK jH j = \u03c02 , we have\n|H jY \u2032j | |H jY j| = sin \u2220Y \u2032jY jH j sin \u2220Y jY \u2032jH j = sin(\u03c0/2 \u2212 \u03b1i) sin(\u03c0/2 + \u03b1\u2032i) = cos(\u03b1i) cos(\u03b1\u2032i) .\nAccording to the assumption (145) and i < j \u2264 s, we have 0 \u2264 \u03b1i \u2264 \u03b1 j \u2264 \u03c02 . Since cos(x)/ cos(x \u2212 \u03b8) is decreasing in [0, \u03c02 ], we can get\ncos(\u03b1i) cos(\u03b1\u2032i) = cos(\u03b1i) cos(\u03b1i \u2212 \u03b8) \u2265 cos(\u03b1 j) cos(\u03b1 j \u2212 \u03b8) .\nCombining the above four relations, we get\ncos(\u03b1 j) cos(\u03b1 j \u2212 \u03b8 j) \u2265 cos(\u03b1 j) cos(\u03b1 j \u2212 \u03b8) ,\nwhich implies cos(\u03b1 j \u2212 \u03b8) \u2265 cos(\u03b1 j \u2212 \u03b8 j) that immediately leads to (188). Thus we have proved (187), which combined with (184) establishes the property (169d).\nThen we prove the property (169c). Since x\u2032j = x j,\u2200 j , i, we have \u2016X\u2032 \u2212 X\u2016F = \u2016x\u2032i \u2212 xi\u2016, which can be bounded as\n\u2016x\u2032i \u2212 xi\u2016 = \u2016hi\u2016 (186) = Di \u2016yi\u2016 sin(\u03b1i \u2212 \u03b82 ) \u2264 \u2016xi\u2016Di\u2016xi\u2016\u2016yi\u2016 sin( \u03c03 ) (170) \u2264 \u2016xi\u2016Di\n2\u03a3i sin( \u03c03 ) <\n1 \u221a\n3 \u2016xi\u2016 \u03a3i Di,\nwhere the first inequality is due to\n\u03b1i \u2212 \u03b8/2 \u2265 \u03b1i \u2212 \u03b8 (168b) \u2265 \u03c0/3 + \u03c0/24 \u2212 \u03b8 (184) \u2265 \u03c0/3. (189)\nThus the first part of (169c) is proved.\nAccording to (185) and (186), we have\n2 sin( \u03b8\n2 ) = Di \u2016xi\u2016\u2016yi\u2016 sin(\u03b1i \u2212 \u03b82 )\n(190)\nNow we upper bound \u2016y\u2032j \u2212 y j\u2016 as\n\u2016y\u2032j \u2212 y j\u2016 = |Y \u2032jY j|\n= sin(\u03b8)\ncos(\u03b1i \u2212 \u03b8) |H jY j|\n= 2 sin( \u03b8\n2 ) cos(\n\u03b8 2 ) 1 cos(\u03b1i \u2212 \u03b8) |H jY j|\n(190) = Di \u2016xi\u2016\u2016yi\u2016 sin(\u03b1i \u2212 \u03b82 ) cos( \u03b8 2 ) 1 cos(\u03b1i \u2212 \u03b8) |H jY j| \u2264 Di \u2016xi\u2016\u2016yi\u2016 sin(\u03b1i \u2212 \u03b82 ) 1 cos(\u03b1i) |H jY j| (189) \u2264 Di\nsin( \u03c03 )\u3008xi, yi\u3009 |H jY j|\n\u2264 2\u221a 3 Di \u03a3i |H jY j|,\n(191)\nwhere the last inequality is due to the fact \u3008xi, yi\u3009 = \u03a3i. Using |H jY j| \u2264 \u2016y j\u2016, we obtain\n\u2016y\u2032j \u2212 y j\u2016 \u2264 2 \u221a\n3 Di \u03a3i \u2016y j\u2016. (192)\nAccording to the definition (172), we have\n\u2016yi \u2212 y\u2032i\u2016 = (1 \u2212 \u03a3i\n\u03a3i + Di )\u2016yi\u2016 = Di \u03a3i + Di \u2016yi\u2016 \u2264 Di \u03a3i \u2016yi\u2016. (193)\nAccording to (192) (which holds for any j \u2208 {1, . . . , r}\\{i}) and (193), we get\n\u2016Y \u2212 Y \u2032\u2016F =\n\u221a r\u2211\nk=1\n\u2016yk \u2212 y\u2032k\u20162 \u2264 2 \u221a\n3 Di \u03a3i\n\u221a r\u2211\nk=1\n\u2016yk\u20162 = 2 \u221a\n3 Di \u03a3i \u2016Y\u2016F ,\nwhich proves the second part of (169c).\nThe property (169e) can be proved as follows. By the definition (172), we have \u2016y\u2032i\u2016 \u2264 \u2016yi\u2016, which combined with (179) (for all j , i) leads to\n\u2016y\u2032k\u2016 \u2264 \u2016yk\u2016, k = 1, . . . , s.\nAccording to (192) (for all j , i) and (193), we have \u2016y\u2032k \u2212 yk\u2016 \u2264 2\u221a 3 Di \u03a3i \u2016yk\u2016,\u2200k, which implies\n\u2016y\u2032k\u2016 \u2265 \u2016yk\u2016 \u2212 \u2016y\u2032k \u2212 yk\u2016 \u2265 \u2016yk\u2016 \u2212 2 \u221a\n3 Di \u03a3i \u2016yk\u2016 (146) \u2265 \u2016yk\u2016 \u2212 1 10r \u2016yk\u2016, \u2200k.\nCombining the above two relations we obtain the property (169e).\nThe property (169f) can be easily proved by (172). In fact, we have\n\u2016yi\u20162 \u2212 \u2016y\u2032i\u20162 = (\u2016yi\u2016 \u2212 \u2016y\u2032i\u2016)(\u2016yi\u2016 + \u2016y\u2032i\u2016)\n\u2265 2\u2016y\u2032i\u2016(\u2016yi\u2016 \u2212 \u2016y\u2032i\u2016) (172) = 2\u2016y\u2032i\u2016( \u03a3i + Di \u03a3i \u2212 1)\u2016y\u2032i\u2016\n= 2 Di \u03a3i \u2016y\u2032i\u20162 \u2265 2 Di \u03a3i ( 11 12 )2\u2016yi\u20162 \u2265 5 3 Di \u03a3i \u2016yi\u20162.\n(194)\nwhere the second last inequliaty follows from \u2016y\u2032i\u2016 \u2265 \u2016yi\u2016 \u2212 \u2016yi \u2212 y\u2032i\u2016 (193) \u2265 \u2016yi\u2016 \u2212 Di\u2016yi\u2016/\u03a3i (146) \u2265 11\u2016yi\u2016/12. According to (179) (for all j , i), we have \u2016Y\u20162F \u2212\u2016Y \u2032\u20162F \u2265 \u2016yi\u20162\u2212\u2016y\u2032i\u20162, which combined with (194) leads to the property (169f).\nAt last, we prove the property (169b). The first part \u2016X\u2032\u2016F = \u2016X\u2016F follows from (171) and (183), thus it remains to prove the second part. Denote \u03d5 j , \u2220Y jOY \u2032j, \u03b2 j , \u2220Y jOK j as shown in Figure 8. Pick a point Z j in the line\nsegment OY j so that |OZ j| = |OY \u2032j |, then |Y jZ j| = \u2016y j\u2016 \u2212 \u2016y\u2032j\u2016. Thus we have\n\u2016y j \u2212 y\u2032j\u2016 \u2016y j\u2016 \u2212 \u2016y\u2032j\u2016 = |Y jY \u2032j | |Y jZ j| = sin(\u2220Y jZ jY \u2032j) sin(\u2220Y jY \u2032jZ j)\n= sin(\u03c0/2 \u2212 \u03d5 j/2) sin(\u03b2 j \u2212 \u03d5 j/2) \u2264 1 sin(\u03b2 j \u2212 \u03d5 j) .\n(195)\nIn order to bound 1/ sin(\u03b2 j \u2212 \u03d5 j) 6, we use the following bound:\nsin \u03b2 j sin(\u03b2 j \u2212 \u03d5 j) = |Y jK j| \u2016y j\u2016 \u2016y\u2032j\u2016 |Y \u2032jK j| \u2264 |Y jK j| |Y \u2032jK j| = tan\u03b1i tan(\u03b1i \u2212 \u03b8) .\nThen we have\nsin \u03b2 j sin(\u03b2 j \u2212 \u03d5 j) sin(\u03b1i \u2212 \u03b8) sin(\u03b1i) cos(\u03b1i \u2212 \u03b8) cos(\u03b1i)\n= cos\u03b1i cos \u03b8 + sin\u03b1i sin \u03b8 cos(\u03b1i) \u2264 sin(\u03b8) cos(\u03b1i) + 1.\n(196)\nAccording to (190) and the fact cos(\u03b1i) = \u3008xi, yi\u3009/(\u2016xi\u2016\u2016yi\u2016) = \u03a3i/(\u2016xi\u2016\u2016yi\u2016), we have sin(\u03b8)\ncos(\u03b1i) \u2264 2 sin(\u03b8/2) cos(\u03b1i) = Di \u2016xi\u2016\u2016yi\u2016 sin(\u03b1i \u2212 \u03b8/2) \u2016xi\u2016\u2016yi\u2016 \u03a3i\n= Di \u03a3i 1 sin(\u03b1i \u2212 \u03b8/2) (146),(189) \u2264 1 12 1 sin(\u03c0/3) = 1 6 \u221a 3 .\n6The part from (195) to (197) can be replaced by a simpler bound sin(\u03b2 j \u2212 \u03d5 j) \u2265 sin(\u03b2 j/2) \u2265 sin(\u03b2 j)/2 and we can still obtain a similar bound as (199); however, by using this simpler yet looser bound, the constant coefficient 7/8 will be replaced by a larger constant.\nPlugging the above relation into (196), we obtain\nsin \u03b2 j sin(\u03b2 j \u2212 \u03d5 j) sin(\u03b1i \u2212 \u03b8) sin(\u03b1i)\n\u2264 6 \u221a 3 + 1\n6 \u221a 3 . (197)\nCombining (195) and (191), we obtain\n\u2016y j \u2212 y\u2032j\u2016 \u2016y j\u2016 \u2212 \u2016y\u2032j\u2016 \u2016y j \u2212 y\u2032j\u2016 \u2016y j\u2016 \u2264 1 sin(\u03b2 j \u2212 \u03d5 j) 2 \u221a 3 Di \u03a3i |H jY j| \u2016y j\u2016\n(197) \u2264 2\u221a\n3 Di \u03a3i\n6 \u221a 3 + 1\n6 \u221a 3 |H jY j| \u2016y j\u2016 sin(\u03b1i) sin(\u03b2 j) 1 sin(\u03b1i \u2212 \u03b8)\n= 6 \u221a\n3 + 1 9 Di \u03a3i 1 sin(\u03b1i \u2212 \u03b8) (189) \u2264 6 \u221a 3 + 1 9 2 \u221a 3 Di \u03a3i \u2264 3 2 Di \u03a3i ,\n(198)\nwhere the last equality is due to |H jY j| sin(\u03b1i) = |Y jK j| = \u2016y j\u2016 sin(\u03b2 j).\nAccording to (192) and (146), we obtain that \u2016y j \u2212 y\u2032j\u2016 \u2264 2\u221a3 1 12\u2016y j\u2016 \u2264 1 8\u2016y j\u2016, which further implies \u2016y\u2032j\u2016 + \u2016y j\u2016 \u2265\n2\u2016y j\u2016 \u2212 \u2016y j \u2212 y\u2032j\u2016 \u2265 158 \u2016y j\u2016. Then by (198) we have\n\u2016y j \u2212 y\u2032j\u20162 \u2264 5 \u221a\n3 + 1 6 Di \u03a3i (\u2016y j\u2016 \u2212 \u2016y\u2032j\u2016)\u2016y j\u2016\n\u2264 3 2 Di \u03a3i (\u2016y j\u2016 \u2212 \u2016y\u2032j\u2016)(\u2016y\u2032j\u2016 + \u2016y j\u2016) 8 15 = 4 5 Di \u03a3i\n(\u2016y j\u20162 \u2212 \u2016y\u2032j\u20162). (199)\nAccording to the definition (172), we have\n\u2016yi\u20162 \u2212 \u2016y\u2032i\u20162\n\u2016yi \u2212 y\u2032i\u20162 = 1 \u2212 (\u03a3i)2/(\u03a3i + Di)2 [1 \u2212 \u03a3i/(\u03a3i + Di)]2\n= (\u03a3i + Di)2 \u2212 \u03a32i\nD2i = D2i + 2Di\u03a3i D2i \u2265 2 \u03a3i Di ,\nwhich implies\n\u2016yi \u2212 y\u2032i\u20162 \u2264 1 2 Di \u03a3i (\u2016yi\u20162 \u2212 \u2016y\u2032i\u20162). (200)\nSumming up (199) for j \u2208 {1, . . . , r}\\{i} and (200), we obtain\n\u2016Y \u2212 Y \u2032\u20162F \u2264 4 5 Di \u03a3i (\u2016Y\u20162F \u2212 \u2016Y \u2032\u20162F),\nwhich proves the second part of (169b)."}, {"heading": "D Proofs of the results in Section 5", "text": ""}, {"heading": "D.1 Proof of Claim 5.2", "text": "The proof of this claim consists of two parts: first, by a classical result we have that M0, the best rank-r approximation of 1pP\u2126(M), is close to M; second, show that the scaling does not change the closeness.\nWe first present the following result.\nLemma D.1 Assume M is a rank r matrix of dimension m\u00d7n with m \u2265 n, and denote Mmax = \u2016M\u2016\u221e as the maximum magnitude of the entries of M. Suppose each entry of M is included in \u2126 with probability p \u2265 C0 log(m+n)m , and M0 is the best rank-r approximation of 1pP\u2126(M). Then with probability larger than 1 \u2212 1/(2n4),\n1 mnM2max \u2016M \u2212 M0\u20162F \u2264 C2 \u03b1\n3 2 r\npm , (201)\nfor some numerical constant C2.\nRemark: Lemma D.1 can be found in [31]. The original version [31, Theorem 1.1] holds for M0 = Pr(Tr(P\u2126(M))/p), where Tr(\u00b7) denotes a trimming operator which sets to zero all rows and columns that have too many observed entries, and Pr(\u00b7) denotes the best rank-r approximation. By standard Chernoff bound one can show that none of the rows and columns have too many observed entries with high probability, thus the conclusion of [31, Theorem 1.1] holds for M0 = Pr(P\u2126(M))/p. The key to establish Lemma D.1 is a bound on \u2016M\u2212 1pP\u2126(M)\u20162, which can be simply proved by matrix concentration inequalities; see [17, Remark 6.1.2], [4, Theorem 6.3] or [7, Theorem 3.5]. The proof of [31, Theorem 1.1] is more complicated than applying matrix concentration inequalities since it holds for a weaker condition |\u2126| \u2265 O(n).\nNote that X\u03020, Y\u03020 defined in Table 1 satisfy\nX\u03020Y\u03020 T = Pr(P\u2126(M)/p) = M0. (202)\nRecall that the SVD of M is M = U\u0302\u03a3V\u0302 , where U\u0302, V\u0302 satisfies (12). We have\n|Mi j| = r\u2211\nk=1\n|U\u0302ikV\u0302 jk\u03a3k | \u2264 \u03a3max r\u2211\nk=1\n|U\u0302ikV\u0302 jk |\n\u2264 \u03a3max\n\u221a r\u2211\nk=1\nU\u03022ik\n\u221a r\u2211\nk=1\nV\u03022jk (12) \u2264 \u03a3max \u00b5r \u221a\nmn , \u2200 i, j.\n(203)\nThe above relation implies Mmax \u2264 \u03a3max \u00b5r\u221amn . Plugging this inequality and p = |\u2126|/(mn) into (201), we get\n\u2016M \u2212 M0\u20162F \u2264 C2 mn\u03b1\n3 2 r\npm \u03a32max\n\u00b52r2\nmn = C2n\n\u03b1 3 2 r3\u03ba2\u00b52\n|\u2126| \u03a3 2 min. (204)\nPlugging (202) and the assumption (27) into (204), we get\n\u03b4\u03020 , \u2016M \u2212 X\u03020Y\u03020 T \u2016F \u2264 \u221a C2 C0 \u03a3min r1.5\u03ba2 . (205)\nThe property (a), i.e. (X0,Y0) \u2208 ( \u221a\n2/3K1) follows directly from the definitions of X0 and Y0 in (23). We then prove the property (b), i.e. (X0,Y0) \u2208 ( \u221a 2/3K2). By (205) we have \u2016M \u2212 M0\u2016F \u2264 \u03a3min/5 \u2264 \u03a3max/5 for large enough C0. This inequality combined with \u2016M \u2212 M0\u2016F \u2265 \u2016M \u2212 M0\u20162 \u2265 \u2016M0\u20162 \u2212 \u03a3max yields\n\u2016M0\u20162 \u2264 6 5 \u03a3max. (206)\nBy the definitions of X\u03020, Y\u03020 (i.e. X\u03020 = X\u03040D 1 2 0 , Y\u03020 = Y\u03040D 1 2 0 , where X\u03040D0Y\u0304 T 0 is the SVD of M0), we have\n\u2016X\u03020\u20162 = \u2016Y\u03020\u20162 = \u221a \u2016M0\u20162 (206) \u2264 \u221a 6 5 \u221a \u03a3max. (207)\nThen we have \u2016X\u03020\u20162F \u2264 r\u2016X\u03020\u201622 \u2264\n6 5 r\u03a3max (15) < 2 3 \u03b22T , (208)\nwhere the last inequality follows from CT > 9/5. By the definition of X0 in (23), we have \u2016X0\u20162F \u2264 \u2016X\u03020\u20162F \u2264 23\u03b22T . Similarly, we can prove \u2016Y0\u20162F \u2264 23\u03b22T . Thus the property (b) is proved.\nNext we prove the property (c), i.e. \u2016M \u2212 X0YT0 \u2016F \u2264 \u03b40. Since X\u03020, Y\u03020 satisfy max{\u2016X\u03020\u2016F , \u2016Y\u03020\u2016F} \u2264 \u03b2T (due to (208) and the analogous inequality for Y\u03020) and (205), it follows from Proposition 4.1 that there exist U0,V0 such that\nU0VT0 = M; (209a) \u2016U0\u20162 \u2264 \u2016X0\u20162; (209b)\n\u2016U0 \u2212 X\u03020\u2016F \u2264 6\u2016Y\u03020\u20162 5\u03a3min \u03b4\u03020, \u2016V0 \u2212 Y\u03020\u2016F \u2264 3\u2016X\u03020\u20162 \u03a3min \u03b4\u03020; (209c)\n\u2016U(i)0 \u2016 2 \u2264 r\u00b5\nm \u03b22T , \u2016V ( j) 0 \u2016 2 \u2264 3r\u00b5 2n \u03b22T . (209d)\nNote that the above inequalities (209b) and (209c) are not due to (48b) and (48c) of Proposition 4.1, but stronger results (99) and (107) established during the proof of Proposition 4.1.\nNote that\n\u2016M \u2212 X0YT0 \u2016F = \u2016U0(V0 \u2212 Y0)T + (U0 \u2212 X0)YT0 \u2016F \u2264 \u2016U0(V0 \u2212 Y0)T \u2016F + \u2016(U0 \u2212 X0)YT0 \u2016F \u2264 \u2016U0\u20162\u2016V0 \u2212 Y0\u2016F + \u2016U0 \u2212 X0\u2016F\u2016Y0\u20162,\n(210)\nwhere the last inequality follows from Proposition B.4. Since X(i)0 and X\u0302 (i) 0 has the same direction and \u2016X (i) 0 \u2016 \u2264 \u2016X\u0302 (i) 0 \u2016, by Proposition B.3 we have\n\u2016X0\u20162 \u2264 \u2016X\u03020\u20162 \u2264 \u221a\n6 5\n\u221a \u03a3max. (211)\nCombining (209b) and (211), we get\n\u2016U0\u20162 \u2264 \u221a\n6 5\n\u221a \u03a3max. (212)\nSimilar to (211), we have\n\u2016Y0\u20162 \u2264 \u221a\n6 5\n\u221a \u03a3max. (213)\nIt remains to bound \u2016V0 \u2212 Y0\u2016F and \u2016U0 \u2212 X0\u2016F . Let us prove the following inequality:\n\u2016U(i)0 \u2212 X (i) 0 \u2016 \u2264 \u2016U (i) 0 \u2212 X\u0302 (i) 0 \u2016, \u2200 i. (214) If \u2016X\u0302(i)0 \u2016 \u2264 \u221a 2 3\u03b21, then (214) becomes equality since X\u0302 (i) 0 = X (i) 0 . Thus we only need to consider the case \u2016X\u0302\n(i) 0 \u2016 >\u221a\n2 3\u03b21. In this case by the definition of X0 in (23) we have \u2016X (i) 0 \u2016 = \u221a 2 3\u03b21. From (209d), we get\n\u2016U(i)0 \u2016 2 < 3 2 r\u00b5 m \u03b22T \u2264 2 3 \u03b221 < \u2016X\u0302 (i) 0 \u2016 2. (215)\nFor simplicity, denote u , U(i)0 , x , X (i) 0 , \u03c4 , \u2016X\u0302(i)0 \u2016\u221a 2/3\u03b21 = \u2016X\u0302(i)0 \u2016 \u2016x\u2016 > 1. Then (215) becomes \u2016u\u2016 \u2264 \u2016x\u2016 and (214) becomes \u2016u \u2212 x\u2016 \u2264 \u2016u \u2212 \u03c4x\u2016. The latter can be transformed as follows:\n\u2016u \u2212 x\u2016 \u2264 \u2016u \u2212 \u03c4x\u2016 \u21d0\u21d2 \u2016x\u20162 \u2212 2\u3008u, x\u3009 \u2264 \u03c42\u2016x\u20162 \u2212 2\u03c4\u3008u, x\u3009 \u21d0\u21d2 2(\u03c4 \u2212 1)\u3008u, x\u3009 \u2264 (\u03c42 \u2212 1)\u2016x\u20162\n\u21d0\u21d2 2\u3008u, x\u3009 \u2264 (\u03c4 + 1)\u2016x\u20162. (216)\nSince \u3008u, x\u3009 \u2264 \u2016u\u2016\u2016x\u2016 \u2264 \u2016x\u20162 (here we use \u2016u\u2016 \u2264 \u2016x\u2016 which is equivalent to (215)) and 2 < \u03c4 + 1, the last inequality of (216) holds, which implies that \u2016u \u2212 x\u2016 \u2264 \u2016u \u2212 \u03c4x\u2016 holds and, consequently, (214) holds.\nAn immediate consequence of (214) is\n\u2016U0 \u2212 X0\u2016F \u2264 \u2016U0 \u2212 X\u03020\u2016F (209c) \u2264 5\u2016Y\u03020\u20162\n4\u03a3min \u03b4\u03020\n(207) \u2264 5\n4 \u221a 6 5 \u221a \u03a3max \u03b4\u03020 \u03a3min . (217)\nSimilarly, we have\n\u2016V0 \u2212 Y0\u2016F (209c) \u2264 3 \u221a 6 5 \u221a \u03a3max \u03b4\u03020 \u03a3min . (218)\nPlugging (212), (213), (217) and (218) into (210), we get\n\u2016M \u2212 X0YT0 \u2016F \u2264 \u221a\n6 5\n\u221a \u03a3max\n5 4 \u221a 6 5 \u221a \u03a3max \u03b4\u03020 \u03a3min + \u221a 6 5 \u221a \u03a3max3 \u221a 6 5 \u221a \u03a3max \u03b4\u03020 \u03a3min\n=( 3 2 + 18 5 )\u03ba\u03b4\u03020\n(205) \u2264 51\n10 \u221a C2 C0 \u03a3min\nr1.5\u03ba (16) \u2264 \u03b40,\nwhere the last inequality holds for Cd \u2265 5153 \u221a C0 C2 . Therefore property (c) is proved."}, {"heading": "D.2 Proof of Claim 3.1", "text": "As mentioned in Section 2.1, in this proof we only need to consider the Bernolli model that \u2126 includes each entry of M with probability p and the expected size S satisfies (27). Denote d , \u2016M\u2212XYT \u2016F . Let a = U(V\u2212Y)T +(U\u2212X)VT , b = (U \u2212 X)(V \u2212 Y), where U,V are defined with the properties in Corollary 4.1.\nAccording to (46) we have \u2016P\u2126(a)\u20162F \u2265 2740 pd2. According to (40a), we have \u2016P\u2126(b)\u2016F \u2264 1 5 \u221a pd. Therefore, \u2016P\u2126(M \u2212 XYT )\u2016F = \u2016P\u2126(a \u2212 b)\u2016F \u2265 \u2016P\u2126(a)\u2016F \u2212 \u2016P\u2126(b)\u2016F \u2265 \u221a 27 40 \u221a pd \u2212 15 \u221a pd \u2265 35 \u221a pd \u2265 1\u221a 3 \u221a pd.\nAccording to (40b), we have \u2016b\u2016F \u2264 110 d. According to (45) (which is a corollary of [4, Theorem 4.1]), we have \u2016P\u2126(a)\u20162F \u2264 76 p\u2016a\u20162F \u2264 7 6 p(\u2016M \u2212 XYT \u2016F + \u2016b\u2016F)2 \u2264 7 6 p(1 + 1 10 )\n2d2 \u2264 1712 pd2. Thus, \u2016P\u2126(a \u2212 b)\u2016F \u2264 \u2016P\u2126(a)\u2016F + \u2016P\u2126(b)\u2016F \u2264 ( \u221a 17 12 + 1 5 ) \u221a pd \u2264 \u221a 2pd."}, {"heading": "D.3 Proof of Proposition 5.1", "text": "We first provide a general condition for (X,Y) \u2208 K1 \u2229 K2 (i.e. incoherent and bounded) based on the function value F\u0303(X,Y).\nProposition D.1 Suppose the sample set \u2126 satisfies (29) and \u03c1 = 2p\u03b420/G0(3/2), where \u03b40 is defined in (16). Suppose (X0,Y0) satisfies (66) and\nF\u0303(X,Y) \u2264 2F\u0303(X0,Y0). (219)\nThen (X,Y) \u2208 K1 \u2229 K2.\nProof of Proposition D.1: We prove by contradiction. Assume the contrary that (X,Y) < K1 \u2229 K2. By the definition of K1,K2 in (30), we have either \u2016X(i)\u20162 > \u03b221 for some i, \u2016Y ( j)\u20162 > \u03b222 for some j, \u2016X\u20162F > \u03b22T or \u2016Y\u20162F > \u03b22T . Hence at least one term of G(X,Y) = \u03c1 \u2211m i=1 G0(\n3\u2016X(i)\u20162 2\u03b221\n) + \u03c1 \u2211n\nj=1 G0( 3\u2016Y ( j)\u20162\n2\u03b222 ) + \u03c1G0( 3\u2016X\u20162F 2\u03b22T ) + \u03c1G0( 3\u2016Y\u20162F 2\u03b22T ) is larger than\nG0( 32 ). In addition, all the other terms in the expression of G(X,Y) are nonnegative, thus we have G(X,Y) > \u03c1G0( 3 2 ). Therefore,\nF\u0303(X,Y) \u2265 G(X,Y) > \u03c1G0( 3 2 ) = 2p\u03b420. (220)\nWe have F\u0303(X0,Y0) =\n1 2 \u2016P\u2126(M \u2212 X0YT0 )\u20162F \u2264 p\u2016M \u2212 X0YT0 \u20162F \u2264 p\u03b420, (221)\nwhere the first equality is due to G(X0,Y0) = 0 which follows from (X0,Y0) \u2208 ( \u221a 2 3 K1) \u2229 ( \u221a 2 3 K2), the second\ninequality follows from (29) and the fact (X0,Y0) \u2208 ( \u221a 2 3 K1) \u2229 ( \u221a 2 3 K2) \u2229 K(\u03b40) \u2286 K1 \u2229 K2 \u2229 K(\u03b4), and the last inequality is due to (X0,Y0) \u2208 K(\u03b40). Combining (220) and (221), we get\nF\u0303(X,Y) > 2F\u0303(X0,Y0),\nwhich contradicts (219).\nWe can prove that (67) implies F\u0303(xi) \u2264 2F\u0303(x0), \u2200 i. (222)\nIn fact, when (67c) holds, as the first inequality in (67c) the above relation also holds. When (67a) holds, let \u03bb = 0 in (67a) we get (222). When (67b) holds, we have\n\u03c8(xi,\u2206i; 1) (67b) \u2264 \u03c8(xi,\u2206i; 0) (65b) = F\u0303(xi), (223)\nwhich implies F\u0303(xi+1) = F\u0303(xi +\u2206i) (65b) \u2264 \u03c8(xi,\u2206i; 1) \u2264 F\u0303(xi). This relation holds for any i, thus F\u0303(xi+1) \u2264 F\u0303(xi) \u2264 \u00b7 \u00b7 \u00b7 \u2264 F\u0303(x0) \u2264 2F\u0303(x0).\nSince (67) implies implies F\u0303(xt) \u2264 2F\u0303(x0) (see (222)), by Proposition D.1 we have xt \u2208 K1 \u2229 K2. The rest of the proof is devoted to establish\nxt \u2208 K( 2 3 \u03b4), \u2200 t. (224)\nDefine the distance of x = (X,Y) and u = (U,V) as\nd(x,u) = \u2016XYT \u2212 UVT \u2016F ,\nthen (Xt,Yt) \u2208 K(\u03b4)\u21d0\u21d2 \u2016XtYTt \u2212 M\u2016F \u2264 \u03b4 can be expressed as\nd(xt,u\u2217) \u2264 \u03b4.\nWe first prove the following result:\nLemma D.2 If F\u0303(x) \u2264 2F\u0303(x0), then d(u\u2217,x) < [ 23\u03b4, \u03b4].\nProof of Lemma D.2: We prove by contradiction. Assume the contrary that\nd(u\u2217,x) \u2208 [2 3 \u03b4, \u03b4]. (225)\nSince x0 satisfies (66), according to the proof of Proposition D.1 we have (221), i.e.\nF\u0303(x0) \u2264 p\u03b420. (226)\nAccording to Proposition D.1 and the assumption F\u0303(x) \u2264 2F\u0303(x0), we have x \u2208 K1 \u2229K2. Together with (225) we get x \u2208 K1 \u2229 K2 \u2229 K(\u03b4). Then we have\nF\u0303(x) \u2265 1 2 \u2016P\u2126(M \u2212 XYT )\u20162 (29) \u2265 1 6 p\u2016M \u2212 XYT \u20162 = 1 6 pd(u\u2217,x)2. (227)\nPlugging d(u\u2217,x)2 \u2265 ( 23 )2\u03b42 (16) = 16\u03b420 (226) \u2265 16F\u0303(x0)/p into (227), we get F\u0303(x) \u2265 83 F\u0303(x0), which together with the assumption F\u0303(x) \u2264 2F\u0303(x0) leads to F\u0303(x) = F\u0303(x0) = 0. Then by (227) we get d(u\u2217,x) = 0, which contradicts (225) since \u03b4 > 0. Thus Lemma D.2 is proved.\nNow we get back to the proof of (224). We prove (224) by induction on t. The basis of the induction holds due to (66) and the fact \u03b40 = \u03b4/6. Suppose xt \u2208 K(2\u03b4/3), we need to prove xt+1 \u2208 K(2\u03b4/3). Assume the contrary that xt+1 < K(2\u03b4/3), i.e.\nd(u\u2217,xt+1) > 2 3 \u03b4. (228)\nLet i = t + 1 in (222), we get F\u0303(xt+1) \u2264 2F\u0303(x0). Then by Lemma D.2 we have\nd(xt+1,u\u2217) < [ 2 3 \u03b4, \u03b4]; (229)\nCombining (229) and (228), we get d(xt+1,u\u2217) > \u03b4. (230)\nIn the rest of the proof, we will derive a contradiction for the three cases (67a), (67b) and (67c) separately.\nCase 1: (67a) holds. By the induction hypothesis, d(xt,u\u2217) \u2264 23\u03b4. Since d(x,u\u2217) is a continuous function over x, the relation d(xt,u\u2217) \u2264 23\u03b4 and (230) imply that there must exist some x\u2032 = (1\u2212 \u03bb)xt+1 + \u03bbxt, \u03bb \u2208 [0, 1] such that\nd(x\u2032,u\u2217) = \u03b4. (231)\nAccording to (67a), we have F\u0303(x\u2032) \u2264 2F\u0303(x0). By Lemma D.2, we have d(u\u2217,x\u2032) < [ 23\u03b4, \u03b4], which contradicts (231).\nCase 2: (67b) holds. Define \u03bb\u2032 = arg min\n\u03bb\u2208R,d(xt+\u03bb\u2206t ,u\u2217)\u2264\u03b4 \u03c8(xt,\u2206t; \u03bb). (232)\nBy the induction hypothesis, d(xt,u\u2217) \u2264 \u03b4, thus 0 lies in the feasible region of the optimization problem in (232), which implies\n\u03c8(xt,\u2206t; \u03bb\u2032) \u2264 \u03c8(xt,\u2206t; 0) (65b) = F\u0303(xt). (233)\nDefine x\u2032 = xt + \u03bb\u2032\u2206t, then the feasibility of \u03bb\u2032 for the optimization problem in (232) implies \u03b4 \u2265 d(x\u2032,u\u2217). Since d(x,u\u2217) is a continuous function over x and d(x\u2032,u\u2217) \u2264 \u03b4 (230) < d(xt+1,u\u2217), there must exist some x\u2032\u2032 = (1 \u2212 )xt+1 + x\u2032= xt + (1 \u2212 + \u03bb\u2032)\u2206t, \u2208 [0, 1] such that\nd(x\u2032\u2032,u\u2217) = \u03b4. (234)\nThen we have\nF\u0303(x\u2032\u2032) (65b) \u2264 \u03c8(xt,\u2206t; 1 \u2212 + \u03bb\u2032)\n(65a) \u2264 (1 \u2212 )\u03c8(xt,\u2206t; 1) + \u03c8(xt,\u2206t; \u03bb\u2032)\n(223),(233) \u2264 F\u0303(xt) (222) \u2264 2F\u0303(x0).\nAgain we apply Lemma D.2 to obtain d(u\u2217,x\u2032\u2032) < [ 23\u03b4, \u03b4], which contradicts (234).\nCase 3: (67c) holds. By (66) and the fact \u03b40 = \u03b4/6 we get d(x0,u\u2217) \u2264 \u03b4/6. Then we have\nd(xt+1,u\u2217) \u2264 d(xt+1,x0) + d(x0,u\u2217) (67c) \u2264 5\n6 \u03b4 + 1 6 \u03b4 = \u03b4,\nwhich contradicts (230).\nIn all three cases we have arrived at a contradiction, thus the assumption (228) does not hold, which finishes the induction step for t + 1. Therefore, (224) holds for all t."}, {"heading": "D.4 Proof of Claim 5.3", "text": "The sequence {xt} generated by Algorithm 1 with either restricted Armijo rule or restricted line search satisfies (67c) because the sequence F\u0303(xt) is decreasing and the requirement d(xt,x0) \u2264 5\u03b4/6 is enforced throughout computation.\nAlgorithm 2 and Algorithm 3 satisfy (67b) since all of them perform exact minimization of a convex upper bound of the objective function along some directions. Note that xt should be understood as the produced solution after t \u201citerations\u201d (one block of variables is updated in one \u201citeration\u201d). In contrast, (Xk,Yk) defined in these algorithms is the produced solution after k \u201cloops\u201d (all variables are updated once in one \u201cloop\u201d). For (Xk,Yk) generated by Algorithm 2, we define x2k = (Xk,Yk),x2k+1 = (Xk+1,Yk) and \u03c8(xt,\u2206t; \u03bb) = F\u0303(xt + \u03bb\u2206t), then \u03c8 satisfies (65) and {xt}\u221et=0 = {(Xk,Yk), (Xk+1,Yk)}\u221ek=0 satisfies (67b). Similarly, for (Xk,Yk) generated by Algorithm 3, define\nx(m+n)k+i = (X (1) k+1, . . . , X (i\u22121) k+1 , X (i), X(i+1)k , . . . , X (m) k ,Yk),\ni = 1, . . . ,m,\nx(m+n)k+m+ j = (Xk+1,Y (1) k+1, . . . ,Y ( j\u22121) k+1 ,Y ( j),Y ( j+1)k , . . . ,Y (m) k ),\nj = 1, . . . , n,\nand \u03c8(xt,\u2206t; \u03bb) = F\u0303(xt + \u03bb\u2206t) + \u03bb0\u2016\u03bb\u2206t\u20162/2, then \u03c8 satisfies (65) and {xt}\u221et=0 satisfies (67b).\nWe then show that Algorithm 1 with constant stepsize \u03b7 < \u03b7\u03041 satisfies (67a) for some \u03b7\u03041 when \u2126 satisfies (29). We prove by induction on t. Define x\u22121 = x0, then (67a) holds for t = 0. Assume (67a) holds for t \u2212 1, i.e., F\u0303(xt\u22121 + \u03bb\u2206t\u22121) \u2264 2F\u0303(x0),\u2200\u03bb \u2208 [0, 1], where \u2206t = xt \u2212 xt\u22121. In particular, we have F\u0303(xt) \u2264 2F\u0303(x0), which together with the assumption that \u2126 satisfies (29) leads to (by Proposition (D.1))\nxt \u2208 K1 \u2229 K2.\nThus max{\u2016Xt\u2016F , \u2016Yt\u2016F} \u2264 \u03b2T , \u2016X(i)t \u2016 \u2264 \u03b21,\u2200i, and \u2016Y ( j) t \u2016 \u2264 \u03b22,\u2200 j. Then we have\n\u2016\u2207X F\u0303(xt)\u2016F = \u2016\u2207XF(xt) + \u2207XG(xt)\u2016F\n\u2264 \u2016P\u2126(XtYTt \u2212 M)Yt\u2016F + \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u03c1 m\u2211 i=1 G\u20320( 3\u2016X(i)t \u20162 2\u03b221 ) 3X\u0304(i)t \u03b221 \u2225\u2225\u2225\u2225\u2225\u2225\u2225 F\n+ \u2225\u2225\u2225\u2225\u2225\u2225\u03c1G\u20320(3\u2016Xt\u20162F2\u03b22T )3Xt\u03b22T \u2225\u2225\u2225\u2225\u2225\u2225 F\n\u2264 \u2016P\u2126(XtYTt \u2212 M)\u2016F\u2016Yt\u2016F + 3\u03c1\u2016Xt\u2016F \u03b221 + 3\u03c1\u2016Xt\u2016F \u03b22T\n\u2264 \u221a\nF\u0303(xt)\u03b2T + 6\u03c1\u2016Xt\u2016F \u03b221\n\u2264 \u221a\n2F\u0303(x0)\u03b2T + 6\u03c1\u03b2T \u03b221 ,\nwhere in the second inequality we use G\u20320( 3\u2016X(i)t \u20162\n2\u03b221 ) \u2264 G\u20320( 3 2 ) = 1 and G \u2032 0( 3\u2016X\u20162F 2\u03b22T ) \u2264 G\u20320( 3 2 ) = 1. Assume\n\u03b7\u03041 \u2264 1\n4\u03b22T . (235)\nRecall that \u03b7 \u2264 \u03b7\u03041, thus we have\n\u2016Xt+1\u2016F \u2264 \u2016Xt\u2016F + \u03b7\u2016\u2207X F\u0303(xt)\u2016F\n\u2264 \u03b2T + 1\n4\u03b22T \u221a2F\u0303(x0)\u03b2T + 6\u03c1\u03b2T \u03b221  (221) \u2264 \u03b2T +\n1 4\u03b2T \u221a2p\u03b40 + 6\u03c1 \u03b221  , c1. (236)\nBy a similar argument, we can prove \u2016Yt+1\u2016F \u2264 c1, thus xt+1 = (Xt+1,Yt+1) \u2208 \u0393(c1) (recall the definition of \u0393(\u00b7) in (20) is \u0393(\u03b2) = {(X,Y) | \u2016X\u2016F \u2264 \u03b2, \u2016Y\u2016F \u2264 \u03b2}). Since (Xt,Yt) \u2208 \u0393(\u03b2T ) \u2286 \u0393(c1) and \u0393(c1) is a convex set, we have that the line segment connecting xt and xt+1, denoted as [xt,xt+1], lies in \u0393(c1). Then by Claim 2.1 we have that \u2207F\u0303 is Lipschitz continuous in [xt,xt+1] with Lipschitz constant\nL1 = L(c1) = 4c21 + 54\u03c1 c21 \u03b241 \u2265 L(\u03b2T ) \u2265 4\u03b22T , (237)\nwhere the last inequality is due to the fact c1 \u2265 \u03b2T . Define (note c1 is defined by (236))\n\u03b7\u03041 , 1 L1 = 1\n4c21 + 54\u03c1 c21 \u03b241\n, (238)\nthen \u03b7\u03041 \u2264 1L(\u03b2T ) \u2264 1 4\u03b22T = 14\u03b22T , which is consistent with (235).\nIt follows from a classical descent lemma (see, e.g., [50, Prop. A.24]) that\nF\u0303(xt \u2212 \u03bb\u03b7\u2207F\u0303(xt))\n\u2264 F\u0303(xt) \u2212 \u3008\u03bb\u03b7\u2207F\u0303(xt),\u2207F\u0303(xt)\u3009 + L1 2 \u2016\u03bb\u03b7\u2207F\u0303(xt)\u20162 = F\u0303(xt) + \u2016\u2207F\u0303(xt)\u20162( L1 2 \u03bb2\u03b72 \u2212 \u03bb\u03b7) \u2264 F\u0303(xt) \u2212 \u03bb\u03b7\n2 \u2016\u2207F\u0303(xt)\u20162\n\u2264 F\u0303(xt) \u2264 2F\u0303(x0), \u2200 \u03bb \u2208 [0, 1],\n(239)\nwhere the second inequality follows from the fact that \u03bb\u03b7 \u2264 \u03b7 \u2264 \u03b7\u03041 = 1/L1. This finishes the induction step (note that \u2206t = xt+1 \u2212 xt = \u2212\u03b7\u2207F\u0303(xt)), thus (67a) is proved.\nFinally, we show that Algorithm 4 (SGD) satisfies (67a) with xt = (Xk,Yk) representing the produced solution after the t-th loop, provided that \u2126 satisfies (29). Denote N = |\u2126| + m + n + 2 and xk,i = (Xk,i,Yk,i), i = 1, . . . ,N. We prove (67a) by induction on t. Define x\u22121 = x0, then (67a) holds for t = 0. Assume (67a) holds for 0, 1, . . . t\u22121, i.e., F\u0303(xk + \u03bb\u2206k) \u2264 2F\u0303(x0),\u2200\u03bb \u2208 [0, 1], where \u2206k = xk+1 \u2212 xk, 0 \u2264 k \u2264 t \u2212 1. In particular, we have F\u0303(xt) \u2264 2F\u0303(x0), which together with the assumption that \u2126 satisfies (29) leads to (by Proposition (D.1))\nxt \u2208 K1 \u2229 K2. (240)\nNow we show that there exist constants c1,i, c2,i, i = 0, 1, . . . ,N (independent of t) so that\nmax{\u2016Xt,i\u2016F ,\u2016Yt,i\u2016F} \u2264 c1,i, (241a) max{\u2016\u2207X fi+1(xt,i)\u2016F , \u2016\u2207Y fi+1(xt,i\u22121)\u2016F} \u2264 c2,i. (241b)\nWe prove (241) by induction on i. When i = 0, since by (240) we have max{\u2016Xt,0\u2016F , \u2016Yt,0\u2016F} = max{\u2016Xt\u2016F , \u2016Yt\u2016F} \u2264 \u03b2T , thus (241a) holds for c1,0 = \u03b2T .\nSuppose (241a) holds for i, we prove (241b) holds for i with suitably chosen c2,i. Note that fi+1 can be one of the five different functions in (26). When fi+1 equals some F jl, we have\n\u2016\u2207X fi+1(xt,i)\u2016F = \u2016\u2207XF j,l(xt,i)\u2016F = |(X( j)t,i ) T Y (l)t,i \u2212 M jl|\u2016Y (l) t,i \u2016\n\u2264 (\u2016Xt,i\u2016F\u2016Yt,i\u2016F + Mmax)\u2016Yt,i\u2016F \u2264 (c21,i + Mmax)c1,i.\nWhen fi+1(X,Y) equals some G1 j(X), we have (see (24) for the expression of \u2207XG1 j)\n\u2016\u2207X fi+1(xt,i)\u2016F = \u2016\u2207XG1 j(Xt,i)\u2016F = \u03c1G\u20320( 3\u2016X( j)t,i \u20162 2\u03b221 ) 3\u2016X( j)t,i \u2016 \u03b221\n\u2264 \u03c1G\u20320( 3c21,i 2\u03b221 ) 3c1,i \u03b221 \u2264 \u03c1G\u20320( 3c21,i 2\u03b22T ) 3c1,i \u03b22T .\nWhen fi+1(X,Y) equals some G3(X), we have\n\u2016\u2207X fi+1(xt,i)\u2016F = \u2016\u2207XG3(Xt,i)\u2016F = \u03c1G\u20320( 3\u2016Xt,i\u20162F 2\u03b22T ) 3\u2016Xt,i\u2016F \u03b22T\n\u2264 \u03c1G\u20320( 3c21,i 2\u03b22T ) 3c1,i \u03b22T .\nWhen fi+1(X,Y) equals some G2 j(Y) or G4(Y) that only depend on Y , we have \u2207X fi+1(xt,i) = 0. Let\nc2,i , max (c21,i + Mmax)c1,i, \u03c1G\u20320(3c21,i2\u03b22T )3c1,i\u03b22T  ,\nthen no matter what kind of function fi+1 is, we always have \u2016\u2207X fi+1(xt,i)\u2016F \u2264 c2,i. Similarly, \u2016\u2207Y fi+1(xt,i)\u2016F \u2264 c2,i. Thus (241b) holds for i.\nSuppose (241b) holds for i \u2212 1, we prove that (241a) holds for i with suitably chosen c1,i. In fact,\n\u2016Xt,i\u2016F = \u2016Xt,i\u22121 \u2212 \u03b7t\u2207X fi(xt,i\u22121)\u2016F \u2264 \u2016Xt,i\u22121\u2016F + \u03b7t\u2016\u2207X fi(xt,i\u22121)\u2016F \u2264 c1,i\u22121 + \u03b7\u0304c2,i\u22121,\nthus (241a) holds for c1,i = c1,i\u22121 + \u03b7\u0304c2,i\u22121. This finishes the induction proof of (241).\nIn Claim 2.1, we have proved that \u2207F\u0303 is Lipschitz continuous with Lipschitz constant L(\u03b20) = 4\u03b20 + 54\u03c1 \u03b220 \u03b241\nin the set \u0393(\u03b20) (the definition of \u0393(\u00b7) is given in (20)). By a similar argument (or set irrelevant rows of X,Y,U,V to zero in the proof of Claim (2.1)), we can prove that each \u2207 fi is also Lipschitz continuous with Lipschitz constant L(\u03b20) = 4\u03b20 + 54\u03c1\n\u03b220 \u03b241 in the set \u0393(\u03b20). Then we have\n\u2016\u2207 fi(xt,i\u22121) \u2212 \u2207 fi(xt)\u2016F \u2264 c\u2032i\u22121\u2016xt,i\u22121 \u2212 xt\u2016F , i = 1, . . . ,N, (242)\nwhere c\u2032i\u22121 = L(c1,i\u22121).\nNote that xt+1 = xt + \u2211N i=1(xt,i \u2212 xt,i\u22121) = xt \u2212 \u03b7t \u2211N\ni=1 \u2207 fi(xt,i\u22121). We can express SGD as an approximate gradient descent method: xt+1 = xt \u2212 \u03b7t(\u2207F\u0303(xt) + wt), (243) where the error\nwt = N\u2211\ni=1\n\u2207 fi(xt,i\u22121) \u2212 \u2207F\u0303(xt) = N\u2211\ni=1\n(\u2207 fi(xt,i\u22121) \u2212 \u2207 fi(xt)).\nFollowing the analysis in [61, Lemma 1], we can bound each term \u2207 fi(xt,i\u22121) \u2212 \u2207 fi(xt) as\n\u2016\u2207 fi(xt,i\u22121) \u2212 \u2207 fi(xt)\u2016F (242) \u2264 c\u2032i\u22121\u2016xt,i\u22121 \u2212 xt\u2016F\n= \u03b7tc\u2032i\u22121\u2016 i\u22121\u2211 l=1 \u2207 fl(xt,l\u22121)\u2016F (241b) \u2264 \u03b7tc\u2032i\u22121 i\u22121\u2211 l=1 \u221a 2c2,l.\nPlugging this inequality for i = 1, . . . ,N into the expression of wt, we obtain an upper bound of the error wt:\n\u2016wt\u2016F \u2264 \u03b7tc0, (244)\nwhere c0 , \u2211N i=1(c \u2032 i\u22121 \u2211i\u22121 l=1 \u221a 2c2,l) is a constant.\nApplying (241a) for i = N, we get max{\u2016Xt+1\u2016F , \u2016Yt+1\u2016F} \u2264 c1,N , thus xt+1 \u2208 \u0393(c1,N). Since xt \u2208 \u0393(\u03b2T ) \u2286 \u0393(c1,N) and \u0393(c1,N) is a convex set, we have that the line segment connecting xt and xt+1 lies in \u0393(c1,N). Then by Claim 2.1 we have that \u2207F\u0303 is Lipschitz continuous over this line segment with Lipschitz constant L\u2032 = L(c1,N). It follows from a classical descent lemma (see, e.g., [50, Prop. A.24]) that\nF\u0303(xt+1) \u2264 F\u0303(xt) + \u3008xt+1 \u2212 xt,\u2207F\u0303(xt)\u3009 + L\u2032\n2 \u2016xt+1 \u2212 xt\u20162F .\nUsing the expression (243), the above relation becomes\nF\u0303(xt+1) \u2212 F\u0303(xt) \u2264 \u2212\u03b7t\u3008\u2207F\u0303(xt) + wt,\u2207F\u0303(xt)\u3009 + L\u2032\n2 \u03b72t \u2016\u2207F\u0303(xt) + wt\u20162F . (245)\nPlugging\n\u2212\u03b7t\u3008wt,\u2207F\u0303(xt)\u3009 \u2264 \u03b7t\u2016wt\u2016F\u2016\u2207F\u0303(xt)\u2016F (244) \u2264 \u03b72t c0\u2016\u2207F\u0303(xt)\u2016F \u2264\n1 2 \u03b72t c0(1 + \u2016\u2207F\u0303(xt)\u20162F)\nand 1 2 \u2016\u2207F\u0303(xt) + wt\u20162F \u2264 \u2016\u2207F\u0303(xt)\u20162F + \u2016wt\u20162F (244) \u2264 \u2016\u2207F\u0303(xt)\u20162F + \u03b72t c20\ninto (245), we get\nF\u0303(xt+1) \u2212 F\u0303(xt)\n\u2264 \u2212\u03b7t\u2016\u2207F\u0303(xt)\u20162F + 1 2 \u03b72t c0(1 + \u2016\u2207F\u0303(xt)\u20162F) + L\u2032\u03b72t (\u2016\u2207F\u0303(xt)\u20162F + \u03b72t c20) = ( 1 2 \u03b72t c0 + \u03b7 2 t L \u2032 \u2212 \u03b7t)\u2016\u2207F\u0303(xt)\u20162F + \u03b72t ( 1 2 c0 + L\u2032\u03b72t c 2 0).\n(246)\nPick \u03b7\u0304 ,\n1 c0 + 2L\u2032 .\nSince \u03b7t \u2264 \u03b7\u0304, we have 12\u03b72t c0 + \u03b72t L\u2032 \u2212 \u03b7t \u2264 \u2212\u03b7t/2 and L\u2032\u03b72t c20 \u2264 L\u2032c20 1 (c0+2L\u2032)2 \u2264 c08 (the last inequality follows from (c0 + 2L\u2032)2 \u2265 8c0L\u2032). Plugging these two inequalities into (246), we obtain\nF\u0303(xt+1) \u2212 F\u0303(xt) \u2264 \u03b72t c0.\nBy the same argument we can prove\nF\u0303(xk+1) \u2212 F\u0303(xk) \u2264 \u03b72kc0, k = 0, 1, . . . , t.\nSumming up these inequalities, we get\nF\u0303(xt+1) \u2264 F\u0303(x0) + t\u2211\nk=0\n\u03b72kc0 \u2264 F\u0303(x0) + \u03b7sumc0.\nwhere the last inequality follows from the assumption \u2211\u221e\nk=0 \u03b7 2 k \u2264 \u03b7sum. Pick\n\u03b7sum , F\u0303(x0)\nc0 ,\nthe above relation becomes F\u0303(xt+1) \u2264 2F\u0303(x0).\nBy a similar argument, we can prove\nF\u0303(xt + \u03bb(xt+1 \u2212 xt)) \u2264 2F\u0303(x0), \u2200 \u03bb \u2208 [0, 1],\nwhich completes the induction. Thus we have proved that Algorithm 4 (SGD) satisfies (67a) with suitably chosen \u03b7\u0304 and \u03b7sum."}, {"heading": "D.5 Proof of Claim 5.1", "text": "For Algorithm 1 with constant stepsize \u03b7 < \u03b7\u03041 (defined in (238)), since the objective value F\u0303(xt) is decreasing, we have F\u0303(xt) \u2264 F\u0303(x0). By Proposition D.1 this implies that the algorithm generates a sequence in K1 \u2229 K2. By Claim 2.1 and the fact K2 = \u0393(\u03b2T ) (see the definitions of K2 in (30) and the definition of \u0393(\u00b7) in (20)), \u2207F\u0303 is Lipschitz continuous with Lipschitz constant L(\u03b2T ) over the set K2. According to [50, Proposition 1.2.3], each limit point of the sequence generated by Algorithm 1 with constant stepsize \u03b7 < \u03b7\u03041 (238) \u2264 2/L(\u03b2T ) is a stationary point of problem (P1).\nWe then consider Algorithm 1 with stepsize chosen by the restricted Armijo rule. The proof of [50, Proposition 1.2.1] for the standard Armijo rule can not be directly applied, and some extra effort is needed. For the restricted Armijo rule, the procedure of picking the stepsize \u03b7k can be viewed as a two-phase approach. In the first phase, we find the smallest nonnegative integer so that the distance requirement is fulfilled, i.e.\ni1 , min{i \u2208 Z+ | d(xk(\u03beis0),x0) \u2264 5 6 \u03b4}, (247)\nwhere Z+ denotes the set of nonnegative integers, and let s\u0304k = \u03bei1 s0. Since\nd(xk(0), s0) = d(xk\u22121,x0) \u2264 2 3 \u03b4, (248)\n(according to Proposition 5.1 and Claim 5.3), such an integer i1 must exist. In the second phase, find the smallest nonnegative integer so that the reduction requirement is fulfilled, i.e.\ni2 , min{i \u2208 Z+ | F\u0303(xk(\u03bei s\u0304k)) \u2264 F\u0303(xk\u22121) \u2212 \u03c3\u03bei s\u0304k\u2016\u2207F\u0303(xk\u22121)\u20162F}, (249)\nand let \u03b7k = \u03bei2 s\u0304k = \u03bei1+i2 s0.\nNote that the second phase follows the same procedure as the standard Armijo rule (see (1.11) of [50]). Hence the difference between the standard Armijo rule and the restricted Armijo rule can be viewed as the following: in each iteration the former starts from a fixed initial stepsize s while the latter starts from a varying initial stepsize s\u0304k. We notice that the proof of [50, Proposition 1.2.1] does not require the initial stepsizes to be constant, but rather the\nfollowing property: if the final stepsize \u03b7k goes to zero for a subsequence k \u2208 K , then for large enough k \u2208 K the initial stepsize must be reduced at least once (see the remark after (1.17) in [50]). This property also holds when the initial stepsize is lower bounded (asymptotically). In the following, we will prove that for the restricted Armijo rule the initial stepsize s\u0304k is lower bounded (asymptotically), and then show how to apply the proof of [50, Proposition 1.2.1] to the restricted Armijo rule.\nWe first prove that the sequence {s\u0304k} is lower bounded (asymptotically), i.e.\nlim inf k\u2192\u221e s\u0304k > 0. (250)\nAssume the contrary that lim infk\u2192\u221e s\u0304k = 0, i.e. there exists a subsequence {s\u0304k}k\u2208K that converges to zero. Since s0 is a fixed scalar, we can assume s\u0304k < s0,\u2200k \u2208 K , thus the corresponding i1 > 0 for all k \u2208 K . By the definition of i1 in (247), we know that i1 \u2212 1 does not satisfy the distance requirement; in other words, we have\nd(xk(\u03be\u22121 s\u0304k),x0) > 5 6 \u03b4.\nDenote gk\u22121 , \u2207F\u0303(xk\u22121), then the above relation becomes\n5 6 \u03b4 < d(xk\u22121 \u2212 \u03be\u22121 s\u0304kgk\u22121,x0) \u2264 d(xk\u22121, x0) + \u03be\u22121 s\u0304k\u2016gk\u22121\u2016F\n(248) \u2264 2\n3 \u03b4 + \u03be\u22121 s\u0304k\u2016gk\u22121\u2016F ,\nimplying 1 6 \u03be\u03b4 \u2264 s\u0304k\u2016gk\u22121\u2016F . Since 16\u03be\u03b4 is a constant and {s\u0304k}k\u2208K converges to zero, the above relation implies that {\u2016gk\u22121\u2016F}k\u2208K goes to infinity. However, it is easy to verify that \u2016gk\u22121\u2016F = \u2016\u2207F\u0303(xk\u22121)\u2016F is bounded above by a universal constant when \u2016xk\u22121\u2016F \u2264 \u03b2T (note that \u2016xk\u22121\u2016F \u2264 \u03b2T holds due to Proposition 5.1 and Claim 5.3)), which is a contradiction. Therefore, (250) is proved.\nNow we prove that each limit point of the sequence {xk} generated by Algorithm 1 with restricted Armijo rule is a stationary point. Assume the contrary that there exists a limit point x\u0304 with \u2207F\u0303(x\u0304) , 0, and suppose the subsequence {xk}k\u2208K converges to x\u0304. By the same argument as that for [50, Proposition 1.2.1], we can prove that the subsequence of final stepsizes {\u03b7k}k\u2208K \u2192 0 (see the inequality before (1.17) in [50]). Since {s\u0304k} is lower bounded (asymptotically), we must have that s\u0304k > \u03b7k, \u2200 k \u2208 K , k \u2265 k\u0304 for large enough k\u0304. Thus the corresponding i2 > 0 for all k \u2208 K , k \u2265 k\u0304. By the definition of i2 in (249), we know that i2 \u2212 1 does not satisfy the reduction requirement; in other words, we have F\u0303(xk(\u03b7k\u03be\u22121)) > F\u0303(xk\u22121) \u2212 \u03c3\u03b7k\u03be\u22121\u2016\u2207F\u0303(xk\u22121)\u20162F , or equivalently,\nF\u0303(xk\u22121) \u2212 F\u0303(xk\u22121 \u2212 \u03b7k\u03be\u22121\u2207F\u0303(xk\u22121))) < \u03c3\u03b7k\u03be \u22121\u2016\u2207F\u0303(xk\u22121)\u20162F , \u2200 k \u2208 K , k \u2265 k\u0304.\nThis relation is the same as (1.17) in [50] (except that (1.17) in [50] considers a more general descent direction), and the rest of the proof is also the same as [50] and is omitted here.\nFor Algorithm 1 with stepsize chosen by the restricted line search rule, since it \u201cgives larger reduction in cost at each iteration\u201d than the restricted Armijo rule, it \u201cinherits the convergence properties\u201d of the restricted Armijo rule (as remarked in the last paragraph of the proof of [50, Proposition 1.2.1]). The rigorous proof is similar to that in the second last paragraph of the proof of [50, Proposition 1.2.1]) and is omitted here.\nAlgorithm 2 is a two-block BCD method to solve problem (P1). According to [59, Corollary 2], each limit point of the sequence generated by Algorithm 2 is a stationary point of problem (P1).\nAlgorithm 3 belongs to the class of BSUM methods [55]. According to Proposition D.1, the level set X0 = {x | F\u0303(x) \u2264 F\u0303(x0)} is a subset of the bounded set K1 \u2229 K2, thus X0 is bounded. Moreover, X0 is a closed set, thus X0 is compact. It is easy to verify that the objective function of each subproblem in Algorithm 3 is a convex tight upper bound of F\u0303(x) (more precisely, satisfies Assumption 2 in [55]). It is also obvious that the objective function of each subproblem is strongly convex, thus each subproblem of Algorithm 3 has a unique solution. Based on these facts, it follows from [55, Theorem 2] that each limit point of the sequence generated by Algorithm 3 is a stationary point.\nAlgorithm 4 is a SGD method (or more precisely, incremental gradient method) with a specific stepsize rule. According to (243) and (244) in Appendix (D.4), Algorithm 4 can be viewed as an approximate gradient descent method with bounded error. By [62, Proposition 1], each limit point of the sequence generated by Algorithm 4 is a stationary point."}, {"heading": "E Proof of Lemma 3.3", "text": "We will prove a statement that is stronger than Lemma 3.1: with probability at least 1 \u2212 1/n4, for any (X,Y) \u2208 K1 \u2229 K2 \u2229 K(\u03b4) and U,V defined in Table 7, we have\n\u3008\u2207X F\u0303(X,Y), X \u2212 U\u3009 + \u3008\u2207Y F\u0303(X,Y),Y \u2212 V\u3009 \u2265 p 4\nd2 + 2 \u221a \u03c1 \u03a3min d \u221a G(X,Y), (251)\nwhere d = \u2016M \u2212 XYT \u2016F .\nWe have already proved (37a), i.e. with probability at least 1 \u2212 1/n4,\n\u03c6F = \u3008\u2207XF, X \u2212 U\u3009 + \u3008\u2207Y F,Y \u2212 V\u3009 \u2265 p 4 d2.\nIt remains to prove a bound on \u03c6G, which is stronger than the bound \u03c6G \u2265 0. Note that \u03c6F depends on the observed set \u2126, thus the bound on \u03c6F holds with high probability; in contrast, \u03c6G does not depend on \u2126, thus the bound on \u03c6G always holds.\nClaim E.1 For any (X,Y) \u2208 K1 \u2229 K2 \u2229 K(\u03b4) and U,V defined in Table 7, we have\n\u03c6G = \u3008\u2207XG, X \u2212 U\u3009 + \u3008\u2207YG,Y \u2212 V\u3009 \u2265 2 \u221a \u03c1 \u03a3min d \u221a G(X,Y). (252)\nProof of Claim E.1: By the definition of G in (13), G(X,Y) = \u03c1( \u2211 i G1i(X) + G2(X) + \u2211\nj G3 j(Y) + G4(Y)), where the component functions\nG1i(X) = G0 3\u2016X(i)\u20162 2\u03b221  , G2(X) = G0 3\u2016X\u20162F 2\u03b22T  , G3 j(Y) , G0\n3\u2016Y ( j)\u20162 2\u03b222  , G4(Y) , G0 3\u2016Y\u20162F 2\u03b22T  . (253)\nBy the expressions of \u2207XG,\u2207YG in (24), we have\n\u03c6G = \u3008\u2207XG, X \u2212 U\u3009 + \u3008\u2207YG,Y \u2212 V\u3009 =\n\u03c1 m\u2211 i=1 G\u20320( 3\u2016X(i)\u20162 2\u03b221 ) 3 \u03b221 \u3008X(i), X(i) \u2212 U(i)\u3009 + \u03c1G\u20320( 3\u2016X\u20162F 2\u03b22T ) 3 \u03b22T \u3008X, X \u2212 U\u3009\n+\u03c1 n\u2211 j=1 G\u20320( 3\u2016Y ( j)\u20162 2\u03b222 ) 3 \u03b222 \u3008Y ( j),Y ( j) \u2212 V ( j)\u3009 + \u03c1G\u20320( 3\u2016Y\u20162F 2\u03b22T ) 3 \u03b22T \u3008Y,Y \u2212 V\u3009,\n(254)\nwhere G\u20320(z) = I[1,\u221e](z)2(z \u2212 1) = 2 \u221a G0(z).\nFirstly, we prove\nh1i , G\u20320( 3\u2016X(i)\u20162\n2\u03b221 ) 3 \u03b221 \u3008X(i), X(i) \u2212 U(i)\u3009 \u2265 1 2\n\u221a G1i(X), \u2200 i, (255a)\nh3 j , G\u20320( 3\u2016Y ( j)\u20162\n2\u03b222 ) 3 \u03b222 \u3008Y ( j),Y ( j) \u2212 V ( j)\u3009 \u2265 1 2\n\u221a G3 j(Y), \u2200 j. (255b)\nWe only need to prove (255a); the proof of (255b) is similar. We consider two cases.\nCase 1: \u2016X(i)\u20162 \u2264 2\u03b2 2 1 3 . Note that 3\u2016X(i)\u20162 2\u03b221 \u2264 1 implies G0( 3\u2016X (i)\u20162 2\u03b221 ) = G\u20320( 3\u2016X(i)\u20162 2\u03b221 ) = 0, thus h1i = G1i = 0, in which\ncase (255a) holds.\nCase 2: \u2016X(i)\u20162 > 2\u03b2 2 1\n3 . By Corollary 4.1 and the fact that \u03b2 2 1 = \u03b2 2 T 3\u00b5r m , we have\n\u2016U(i)\u20162 \u2264 3r\u00b5 2m \u03b22T (15) = 3 4 2\u03b221 3 < 3 4 \u2016X(i)\u20162. (256)\nAs a result, \u221a\n3 2 \u3008X(i), X(i)\u3009 =\n\u221a 3\n2 \u2016X(i)\u2016\u2016X(i)\u2016 > \u2016X(i)\u2016\u2016U(i)\u2016 \u2265 \u3008X(i),U(i)\u3009, which implies \u3008X(i), X(i) \u2212 U(i)\u3009 \u2265 (1 \u2212 \u221a\n3 2 )\u2016X(i)\u20162 > (1 \u2212\n\u221a 3 2 ) 2 3\u03b2 2 1 > 1 12\u03b2 2 1. Combining this inequality with the fact that G \u2032 0( 3\u2016X(i)\u20162 2\u03b221\n) = 2 \u221a G0 (\n3\u2016X(i)\u20162 2\u03b221\n) =\n2 \u221a G1i(X), we get (255a).\nSecondly, we prove\nh2 + h4 \u2265 2d\n\u03a3min\n( \u221a G2(X) + \u221a G4(Y) ) ,\nwhere h2 , G\u20320( 3\u2016X\u20162F 2\u03b22T ) 3 \u03b22T \u3008X, X \u2212 U\u3009,\nh4 , G\u20320( 3\u2016Y\u20162F 2\u03b22T ) 3 \u03b22T \u3008Y,Y \u2212 V\u3009.\n(257)\nWithout loss of generality, we can assume \u2016Y\u2016F \u2265 \u2016X\u2016F , and we will apply Corollary 4.1 to prove (257). If \u2016Y\u2016F < \u2016X\u2016F , we can apply a symmetric result of Corollary 4.1 to prove (257). We consider three cases.\nCase 1: \u2016X\u2016F \u2264 \u2016Y\u2016F \u2264 \u221a 2 3\u03b2T . In this case G0( 3\u2016X\u20162F 2\u03b22T ) = G\u20320( 3\u2016X\u20162F 2\u03b22T ) = G0( 3\u2016Y\u20162F 2\u03b22T ) = G\u20320( 3\u2016Y\u20162F 2\u03b22T\n) = 0, which implies h2 = h4 = G2(X) = G4(Y) = 0, thus (257) holds.\nCase 2: \u2016X\u2016F \u2264 \u221a 2 3\u03b2T < \u2016Y\u2016F . Then we have 3\u2016X\u20162F 2\u03b22T \u2264 1, which implies h2 = 0 = G2(X). By (51d) in Corollary\n4.1 we have \u2016V\u2016F \u2264 (1\u2212 d\u03a3min )\u2016Y\u2016F , which implies (1\u2212 d \u03a3min )\u3008Y,Y\u3009 = (1\u2212 d \u03a3min )\u2016Y\u20162F \u2265 \u2016Y\u2016F\u2016V\u2016F \u2265 \u3008Y,V\u3009. This further\nimplies \u3008Y,Y \u2212V\u3009 \u2265 d \u03a3min \u2016Y\u20162F \u2265 d\u03a3min 2\u03b22T 3 . Combined with the fact that G \u2032 0( 3\u2016Y\u20162F 2\u03b22T\n) = 2 \u221a\nG0( 3\u2016Y\u20162F 2\u03b22T\n) = 2 \u221a G4(Y), we get\nh4 = G\u20320( 3\u2016Y\u20162F 2\u03b22T ) 3 \u03b22T \u3008Y,Y \u2212 V\u3009\n\u2265 2 \u221a\nG4(Y) 3 \u03b22T d \u03a3min 2\u03b22T 3 = 4d \u03a3min\n\u221a G4(Y).\nThus h2 + h4 = h4 \u2265 4d\u03a3min \u221a G4(Y) = 4d\u03a3min (\u221a G4(Y) + \u221a G2(X) ) \u2265 2d \u03a3min (\u221a G4(Y) + \u221a G2(X) ) .\nCase 3: \u221a\n2 3\u03b2T < \u2016X\u2016F \u2264 \u2016Y\u2016F . Since \u2016Y\u2016F \u2265 \u2016X\u2016F , we have G4(Y) = G0 ( 3\u2016Y\u20162F 2\u03b22T ) \u2265 G0 ( 3\u2016X\u20162F 2\u03b22T ) = G2(X). By\nCorollary 4.1, we have \u2016U\u2016F \u2264 \u2016X\u2016F and \u2016V\u2016F \u2264 (1 \u2212 d\u03a3min )\u2016Y\u2016F . Similar to the argument in Case 2 we can prove h2 \u2265 0, h4 \u2265 4d\u03a3min \u221a G4(Y); thus h2 + h4 \u2265 4d\u03a3min \u221a G4(Y) \u2265 2d\u03a3min (\u221a G4(Y) + \u221a G2(X) ) .\nIn all three cases, we have proved (257), thus (257) holds.\nWe conclude that for U,V defined in Table 7,\n\u03c6G (254) = \u03c1 \u2211 i h1i + \u2211 j h3 j + h2 + h4  (255),(257) \u2265 \u03c1 ( 1 2 \u2211 i \u221a G1i(X) + 1 2 \u2211 j \u221a G2 j(Y)\n+ 2d\n\u03a3min\n\u221a G2(X) +\n2d \u03a3min\n\u221a G4(Y) ) \u2265 \u03c1 2d\n\u03a3min \u2211 i \u221a G1i(X) + \u2211 j \u221a G2 j(Y) + \u221a G2(X) + \u221a G4(Y)  \u2265 \u03c1 2d\n\u03a3min \u221a\u2211 i G1i(X) + \u2211 j G2 j(Y) + G2(X) + G4(Y)\n= \u03c1 2d\n\u03a3min\n\u221a 1 \u03c1 G(X,Y) = 2 \u221a \u03c1 \u03a3min d \u221a G(X,Y).\n(258)\nwhich finishes the proof of Claim E.1.\nLet us come back to the proof of Lemma 3.3. The rest of the proof is just algebraic computation. According to (251), we have\np 4\nd2 + 2 \u221a \u03c1 \u03a3min d \u221a G(X,Y)\n\u2264 \u3008\u2207X F\u0303(X,Y), X \u2212 U\u3009 + \u3008\u2207Y F\u0303(X,Y),Y \u2212 V\u3009 \u2264 (\u2016\u2207X F\u0303(X,Y)\u2016F + \u2016\u2207Y F\u0303(X,Y)\u2016F) max{\u2016X \u2212 U\u2016F , \u2016Y \u2212 V\u2016F} (51b) \u2264 \u221a 2 \u221a \u2016\u2207X F\u0303(X,Y)\u20162F + \u2016\u2207Y F\u0303(X,Y)\u20162F 17 2 \u221a r \u03b2T \u03a3min d = \u2016\u2207F\u0303(X,Y)\u2016F 17 \u221a\n2\n\u221a r \u03b2T\n\u03a3min d.\nEliminating a factor of d from both sides and taking square, we get\n\u2016\u2207F\u0303(X,Y)\u20162F 289 2 r \u03b22T\n\u03a32min \u2265\n( p 4 d + 2 \u221a \u03c1\n\u03a3min\n\u221a G(X,Y) )2 \u2265 pd 2\n16 + 4\u03c1 \u03a32min G(X,Y).\n(259)\nBy the definition of \u03b2T in (15), we have\nr \u03b22T\n\u03a32min = r CT r\u03a3max \u03a32min = CT r2\u03ba \u03a3min .\nAccording to Claim 3.1, we have\npd2 = p\u2016M \u2212 XYT \u20162F \u2265 1 2 \u2016P\u2126(M \u2212 XYT )\u20162F = F(X,Y).\nBy the definition of \u03c1 in (17) and the definition of \u03b40 in (16), we have\n4\u03c1 \u03a32min = 4 \u03a32min 8p\u03b420 = 32p \u03a32min 1 36\n\u03a32min\nC2dr 3\u03ba2\n= 8 9 1 C2dr 3\u03ba2 p.\nSubstituting the above three relations into (259), we get (when Cd \u2265 32/3)\n\u2016\u2207F\u0303(X,Y)\u20162F 289 2 CT r2\u03ba \u03a3min \u2265 p 32 F(X,Y) + 8 9 1 C2dr 3\u03ba2 pG(X,Y)\n\u2265 8 9 1 C2dr 3\u03ba2 p(F(X,Y) + G(X,Y)) = 8 9 1 C2dr 3\u03ba2 pF\u0303(X,Y).\nThis can be further simplified to\n\u2016\u2207F\u0303(X,Y)\u20162F \u2265 \u03a3min\nCgr5\u03ba3 pF\u0303(X,Y),\nwhere the numerical constant Cg = 260116 CT C 2 d. This finishes the proof of Lemma 3.3."}], "references": [{"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": "Computer, vol. 42, no. 8, pp. 30\u201337, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Recovering the missing components in a large noisy low-rank matrix: Application to SFM", "author": ["Pei Chen", "David Suter"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, no. 8, pp. 1051\u20131063, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Interior-point method for nuclear norm approximation with application to system identification", "author": ["Zhang Liu", "Lieven Vandenberghe"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 31, no. 3, pp. 1235\u20131256, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Exact matrix completion via convex optimization", "author": ["Emmanuel J Cand\u00e8s", "Benjamin Recht"], "venue": "Foundations of Computational mathematics, vol. 9, no. 6, pp. 717\u2013772, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao"], "venue": "IEEE Transactions on Information Theory, vol. 56, no. 5, pp. 2053\u20132080, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "author": ["David Gross"], "venue": "IEEE Transactions on Information Theory, vol. 57, no. 3, pp. 1548\u20131566, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "A simpler approach to matrix completion", "author": ["Benjamin Recht"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 3413\u20133430, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Matrix completion with noise", "author": ["Emmanuel J Cand\u00e8s", "Yaniv Plan"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 925\u2013936, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise", "author": ["Sahand Negahban", "Martin J Wainwright"], "venue": "The Journal of Machine Learning Research, vol. 13, no. 1, pp. 1665\u20131697, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["Jian-Feng Cai", "Emmanuel J Cand\u00e8s", "Zuowei Shen"], "venue": "SIAM Journal on Optimization, vol. 20, no. 4, pp. 1956\u20131982, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1956}, {"title": "Fixed point and bregman iterative methods for matrix rank minimization", "author": ["Shiqian Ma", "Donald Goldfarb", "Lifeng Chen"], "venue": "Mathematical Programming, vol. 128, no. 1-2, pp. 321\u2013353, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["Kim-Chuan Toh", "Sangwoon Yun"], "venue": "Pacific Journal of Optimization, vol. 6, no. 615-640, pp. 15, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast global convergence of gradient methods for high-dimensional statistical recovery", "author": ["Alekh Agarwal", "Sahand Negahban", "Martin Jordan Wainwright"], "venue": "The Annals of Statistics, vol. 40, no. 5, pp. 2452\u20132482, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "On the linear convergence of the proximal gradient method for trace norm regularization", "author": ["Ke Hou", "Zirui Zhou", "Anthony Man-Cho So", "Zhi-Quan Luo"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2013, pp. 710\u2013718.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A unified view of matrix factorization models", "author": ["Ajit P Singh", "Geoffrey J Gordon"], "venue": "Machine Learning and Knowledge Discovery in Databases, pp. 358\u2013373. Springer, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Major components of the gravity recommendation system", "author": ["G\u00e1bor Tak\u00e1cs", "Istv\u00e1n Pil\u00e1szy", "Botty\u00e1n N\u00e9meth", "Domonkos Tikk"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 9, no. 2, pp. 80\u201383, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient algorithms for collaborative filtering", "author": ["Hulikal Keshavan"], "venue": "Ph.D. thesis, Stanford University,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "Proceedings of the forty-fifth annual ACM symposium on Theory of computing (STOC). ACM, 2013, pp. 665\u2013674.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": "2014 IEEE 55th Annual Symposium on Foundations of Computer Science (FOCS). IEEE, 2014, pp. 651\u2013660.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast matrix completion without the condition number", "author": ["Moritz Hardt", "Mary Wootters"], "venue": "Proceedings of The 27th Conference on Learning Theory (COLT), 2014, pp. 638\u2013678.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale parallel collaborative filtering for the netflix prize", "author": ["Yunhong Zhou", "Dennis Wilkinson", "Robert Schreiber", "Rong Pan"], "venue": "Algorithmic Aspects in Information and Management, pp. 337\u2013348. Springer, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Solving a low-rank factorization model for matrix completion by a nonlinear successive overrelaxation algorithm", "author": ["Zaiwen Wen", "Wotao Yin", "Yin Zhang"], "venue": "Mathematical Programming Computation, vol. 4, no. 4, pp. 333\u2013361, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Netflix update: Try this at home", "author": ["Simon Funk"], "venue": "http://sifter.org/ simon/journal/20061211.html.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Improving regularized singular value decomposition for collaborative filtering", "author": ["Arkadiusz Paterek"], "venue": "Proceedings of KDD cup and workshop, 2007, vol. 2007, pp. 5\u20138.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Large-scale matrix factorization with distributed stochastic gradient descent", "author": ["Rainer Gemulla", "Erik Nijkamp", "Peter J Haas", "Yannis Sismanis"], "venue": "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2011, pp. 69\u201377.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel stochastic gradient algorithms for large-scale matrix completion", "author": ["Benjamin Recht", "Christopher R\u00e9"], "venue": "Mathematical Programming Computation, vol. 5, no. 2, pp. 201\u2013226, 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast parallel sgd for matrix factorization in shared memory systems", "author": ["Yong Zhuang", "Wei-Sheng Chin", "Yu-Chin Juan", "Chih-Jen Lin"], "venue": "Proceedings of the 7th ACM Conference on Recommender Systems. ACM, 2013, pp. 249\u2013256.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast als-based matrix factorization for explicit and implicit feedback datasets", "author": ["Istv\u00e1n Pil\u00e1szy", "D\u00e1vid Zibriczky", "Domonkos Tikk"], "venue": "Proceedings of the fourth ACM conference on Recommender systems. ACM, 2010, pp. 71\u201378.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Si Si", "Inderjit S Dhillon"], "venue": "ICDM, 2012, pp. 765\u2013774.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Matrix Completion via Nonconvex Factorization: Algorithms and Theory, Ph.D", "author": ["Ruoyu Sun"], "venue": "thesis, University of Minnesota,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Matrix completion from a few entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "IEEE Transactions on Information Theory, vol. 56, no. 6, pp. 2980\u20132998, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "author": ["Emmanuel Cand\u00e8s", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "arXiv preprint arXiv:1407.1065, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantum state tomography via compressed sensing", "author": ["David Gross", "Yi-Kai Liu", "Steven T Flammia", "Stephen Becker", "Jens Eisert"], "venue": "arXiv preprint, http://arxiv.org/abs/0909.3304v1, 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast exact matrix completion with finite samples", "author": ["Prateek Jain", "Praneeth Netrapalli"], "venue": "arXiv preprint arXiv:1411.1087, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Global convergence of stochastic gradient descent for some nonconvex matrix problems", "author": ["Christopher De Sa", "Kunle Olukotun", "Christopher R\u00e9"], "venue": "arXiv preprint arXiv:1411.1134, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Phase retrieval using alternating minimization", "author": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2796\u20132804.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "A general theory of concave regularization for high-dimensional sparse estimation problems", "author": ["Cun-Hui Zhang", "Tong Zhang"], "venue": "Statistical Science, vol. 27, no. 4, pp. 576\u2013593, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima", "author": ["Po-Ling Loh", "Martin Wainwright"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 476\u2013484.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Strong oracle optimality of folded concave penalized estimation", "author": ["Jianqing Fan", "Lingzhou Xue", "Hui Zou"], "venue": "The Annals of Statistics, vol. 42, no. 3, pp. 819\u2013849, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Truncated power method for sparse eigenvalue problems", "author": ["Xiao-Tong Yuan", "Tong Zhang"], "venue": "The Journal of Machine Learning Research, vol. 14, no. 1, pp. 899\u2013925, 2013.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonconvex statistical optimization: Minimax-optimal sparse pca in polynomial time", "author": ["Zhaoran Wang", "Huanran Lu", "Han Liu"], "venue": "arXiv preprint arXiv:1408.5352, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Non-convex robust pca", "author": ["Praneeth Netrapalli", "UN Niranjan", "Sujay Sanghavi", "Animashree Anandkumar", "Prateek Jain"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1107\u20131115.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical guarantees for the EM algorithm: From population to sample-based analysis", "author": ["Sivaraman Balakrishnan", "Martin Wainwright", "Bin Yu"], "venue": "arXiv preprint arXiv:1408.2156, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "High dimensional expectation-maximization algorithm: Statistical optimization and asymptotic normality", "author": ["Zhaoran Wang", "Quanquan Gu", "Yang Ning", "Han Liu"], "venue": "arXiv preprint arXiv:1412.8729, 2014. 76", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Perturbation bounds in connection with singular value decomposition", "author": ["Per-\u00c5ke Wedin"], "venue": "BIT Numerical Mathematics, vol. 12, no. 1, pp. 99\u2013111, 1972.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1972}, {"title": "Spectral techniques applied to sparse random graphs", "author": ["Uriel Feige", "Eran Ofek"], "venue": "Random Structures & Algorithms, vol. 27, no. 2, pp. 251\u2013275, 2005.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2005}, {"title": "Coherent matrix completion", "author": ["Yudong Chen", "Srinadh Bhojanapalli", "Sujay Sanghavi", "Rachel Ward"], "venue": "Proceedings of The 31st International Conference on Machine Learning (ICML), 2014, pp. 674\u2013682.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Universal matrix completion", "author": ["Srinadh Bhojanapalli", "Prateek Jain"], "venue": "arXiv preprint arXiv:1402.2324, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Non-linear programming via penalty functions", "author": ["Willard I Zangwill"], "venue": "Management science, vol. 13, no. 5, pp. 344\u2013358, 1967.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1967}, {"title": "Nonlinear programming", "author": ["Dimitri P Bertsekas"], "venue": "1999.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1999}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["Paul Tseng"], "venue": "Journal of optimization theory and applications, vol. 109, no. 3, pp. 475\u2013494, 2001.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2001}, {"title": "Improved iteration complexity bounds of cyclic block coordinate descent for convex problems", "author": ["Ruoyu Sun", "Mingyi Hong"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1306\u20131314.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Worst-case complexity of cyclic coordinate descent: O(n2) gap with randomized version", "author": ["Ruoyu Sun", "Yinyu Ye"], "venue": "arXiv preprint arXiv:1604.07130, 2016.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Yu. Nesterov"], "venue": "SIAM Journal on Optimization, vol. 22, no. 2, pp. 341\u2013362, 2012.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified convergence analysis of block successive minimization methods for nonsmooth optimization", "author": ["Meisam Razaviyayn", "Mingyi Hong", "Zhi-Quan Luo"], "venue": "SIAM Journal on Optimization, vol. 23, no. 2, pp. 1126\u20131153, 2013.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-layer provision of future cellular networks: A WMMSE-based approach", "author": ["Hadi Baligh", "Mingyi Hong", "Wei-Cheng Liao", "Zhi-Quan Luo", "Meisam Razaviyayn", "Maziar Sanjabi", "Ruoyu Sun"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 6, pp. 56\u201368, 2014.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint base station clustering and beamformer design for partial coordinated transmission in heterogeneous networks", "author": ["Mingyi Hong", "Ruoyu Sun", "H. Baligh", "Zhi-Quan Luo"], "venue": "IEEE Journal on Selected Areas in Communications (JSAC), vol. 31, no. 2, pp. 226\u2013240, February 2013.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix completion and low-rank svd via fast alternating least squares", "author": ["Trevor Hastie", "Rahul Mazumder", "Jason Lee", "Reza Zadeh"], "venue": "arXiv preprint arXiv:1410.2596, 2014.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2014}, {"title": "On the convergence of the block nonlinear gauss\u2013seidel method under convex constraints", "author": ["Luigi Grippo", "Marco Sciandrone"], "venue": "Operations Research Letters, vol. 26, no. 3, pp. 127\u2013136, 2000.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2000}, {"title": "On the expected convergence of randomly permuted ADMM", "author": ["Ruoyu Sun", "Zhi-Quan Luo", "Yinyu Ye"], "venue": "arXiv preprint arXiv:1503.06387, 2015.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of an approximate gradient projection method with applications to the backpropagation algorithm", "author": ["Zhi-Quan Luo", "Paul Tseng"], "venue": "Optimization Methods and Software, vol. 4, no. 2, pp. 85\u2013101, 1994.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient convergence in gradient methods with errors", "author": ["Dimitri P Bertsekas", "John N Tsitsiklis"], "venue": "SIAM Journal on Optimization, vol. 10, no. 3, pp. 627\u2013642, 2000.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2000}, {"title": "Perturbation theory for the singular value decomposition", "author": ["Gilbert W Stewart"], "venue": "1998. 77", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Matrix completion has found numerous applications in various fields such as recommender systems [1], computer vision [2] and system identification [3], to name a few.", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "Matrix completion has found numerous applications in various fields such as recommender systems [1], computer vision [2] and system identification [3], to name a few.", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "Matrix completion has found numerous applications in various fields such as recommender systems [1], computer vision [2] and system identification [3], to name a few.", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "For the matrix completion problem, the nuclear norm based formulation becomes either a linearly constrained minimization problem [4] min Z\u2208Rm\u00d7n \u2016Z\u2016\u2217, s.", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4\u20137].", "startOffset": 248, "endOffset": 253}, {"referenceID": 4, "context": "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4\u20137].", "startOffset": 248, "endOffset": 253}, {"referenceID": 5, "context": "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4\u20137].", "startOffset": 248, "endOffset": 253}, {"referenceID": 6, "context": "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4\u20137].", "startOffset": 248, "endOffset": 253}, {"referenceID": 7, "context": "This result was later generalized to noisy matrix completion, whereby the optimization formulation (2) is adopted [8].", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "Using a different proof framework, reference [9] provided theoretical guarantee for a variant of the formulation (3).", "startOffset": 45, "endOffset": 48}, {"referenceID": 9, "context": "To solve problems with larger size, researchers have developed first order algorithms, including the SVT (singular value thresholding) algorithm for the formulation (1) [10], and several variants of the proximal gradient method for the formulation (3) [11, 12] .", "startOffset": 169, "endOffset": 173}, {"referenceID": 10, "context": "To solve problems with larger size, researchers have developed first order algorithms, including the SVT (singular value thresholding) algorithm for the formulation (1) [10], and several variants of the proximal gradient method for the formulation (3) [11, 12] .", "startOffset": 252, "endOffset": 260}, {"referenceID": 11, "context": "To solve problems with larger size, researchers have developed first order algorithms, including the SVT (singular value thresholding) algorithm for the formulation (1) [10], and several variants of the proximal gradient method for the formulation (3) [11, 12] .", "startOffset": 252, "endOffset": 260}, {"referenceID": 12, "context": "Although linear convergence of the proximal gradient method has been established for the formulation (3) under certain conditions [13, 14], the per-iteration cost of computing SVD (Singular Value Decomposition) may increase rapidly as the dimension of the problem increases, making these algorithms rather slow or even useless for problems of huge size.", "startOffset": 130, "endOffset": 138}, {"referenceID": 13, "context": "Although linear convergence of the proximal gradient method has been established for the formulation (3) under certain conditions [13, 14], the per-iteration cost of computing SVD (Singular Value Decomposition) may increase rapidly as the dimension of the problem increases, making these algorithms rather slow or even useless for problems of huge size.", "startOffset": 130, "endOffset": 138}, {"referenceID": 14, "context": "Such a matrix factorization model has long been used in PCA (principle component analysis) and many other applications [15].", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "It has gained great popularity in the recommender systems field and served as the basic building block of many competing algorithms for the Netflix Prize [1, 16] due to several reasons.", "startOffset": 154, "endOffset": 161}, {"referenceID": 15, "context": "It has gained great popularity in the recommender systems field and served as the basic building block of many competing algorithms for the Netflix Prize [1, 16] due to several reasons.", "startOffset": 154, "endOffset": 161}, {"referenceID": 0, "context": "Third, as elaborated in [1], the factorization model can be easily modified to incorporate additional application-specific requirements.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "A popular factorization based formulation for matrix completion takes the form of an unconstrained regularized square-loss minimization problem [1]:", "startOffset": 144, "endOffset": 147}, {"referenceID": 16, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 76, "endOffset": 83}, {"referenceID": 17, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 76, "endOffset": 83}, {"referenceID": 18, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 76, "endOffset": 83}, {"referenceID": 19, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 76, "endOffset": 83}, {"referenceID": 20, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 117, "endOffset": 121}, {"referenceID": 0, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 188, "endOffset": 191}, {"referenceID": 21, "context": "There are a few variants of this formulation: the coefficient \u03bb can be zero [17\u201320] or different for each row of X,Y [21]; each square loss term [Mi j\u2212(XY )i j] can have different weights [1]; an additional matrix variable Z \u2208 Rn\u00d7r can be introduced [22].", "startOffset": 250, "endOffset": 254}, {"referenceID": 0, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 217, "endOffset": 232}, {"referenceID": 17, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 217, "endOffset": 232}, {"referenceID": 18, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 217, "endOffset": 232}, {"referenceID": 20, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 217, "endOffset": 232}, {"referenceID": 0, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 271, "endOffset": 286}, {"referenceID": 15, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 271, "endOffset": 286}, {"referenceID": 22, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 271, "endOffset": 286}, {"referenceID": 23, "context": "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].", "startOffset": 271, "endOffset": 286}, {"referenceID": 24, "context": "Recently several parallelizable variants of the SGD [25\u201327] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.", "startOffset": 52, "endOffset": 59}, {"referenceID": 25, "context": "Recently several parallelizable variants of the SGD [25\u201327] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.", "startOffset": 52, "endOffset": 59}, {"referenceID": 26, "context": "Recently several parallelizable variants of the SGD [25\u201327] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.", "startOffset": 52, "endOffset": 59}, {"referenceID": 27, "context": "Recently several parallelizable variants of the SGD [25\u201327] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.", "startOffset": 145, "endOffset": 152}, {"referenceID": 28, "context": "Recently several parallelizable variants of the SGD [25\u201327] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.", "startOffset": 145, "endOffset": 152}, {"referenceID": 16, "context": "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17\u201320].", "startOffset": 194, "endOffset": 201}, {"referenceID": 17, "context": "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17\u201320].", "startOffset": 194, "endOffset": 201}, {"referenceID": 18, "context": "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17\u201320].", "startOffset": 194, "endOffset": 201}, {"referenceID": 19, "context": "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17\u201320].", "startOffset": 194, "endOffset": 201}, {"referenceID": 30, "context": "The first recovery guarantee for the factorization based matrix completion is provided in [31], where Keshavan, Montanari and Oh considered a factorization model in Grassmannian manifold and showed that the matrix can be recovered by a proper initialization and a gradient descent method on Grassmannian manifold.", "startOffset": 90, "endOffset": 94}, {"referenceID": 16, "context": "The factorization model in Euclidean space was first analyzed in an unpublished work [17] of Keshavan 1, as well as a later work of Jain et al.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The sample complexity bounds were later improved by Hardt [19] and Hardt and Wooters [20], where in the latter work, notably, the authors devised an algorithm with a corresponding sample complexity bound independent of the condition number.", "startOffset": 58, "endOffset": 62}, {"referenceID": 19, "context": "The sample complexity bounds were later improved by Hardt [19] and Hardt and Wooters [20], where in the latter work, notably, the authors devised an algorithm with a corresponding sample complexity bound independent of the condition number.", "startOffset": 85, "endOffset": 89}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "We will point out a subtle theoretical issue not mentioned in [32], as well as some other practical issues.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "1 Reference [17] is a PhD thesis that discusses various algorithms including the algorithm proposed in [31] and alternating minimization.", "startOffset": 12, "endOffset": 16}, {"referenceID": 30, "context": "1 Reference [17] is a PhD thesis that discusses various algorithms including the algorithm proposed in [31] and alternating minimization.", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "In this paper when we refer to [17], we are only referring to [17, Ch.", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.", "startOffset": 80, "endOffset": 90}, {"referenceID": 6, "context": "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.", "startOffset": 80, "endOffset": 90}, {"referenceID": 32, "context": "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.", "startOffset": 80, "endOffset": 90}, {"referenceID": 16, "context": ", L, as proposed in [17,18] 2.", "startOffset": 20, "endOffset": 27}, {"referenceID": 17, "context": ", L, as proposed in [17,18] 2.", "startOffset": 20, "endOffset": 27}, {"referenceID": 16, "context": "However, the results in [17\u201320] actually require a generative model of independent \u03a9k\u2019s, instead of sampling \u03a9k\u2019s based on a given \u03a9.", "startOffset": 24, "endOffset": 31}, {"referenceID": 17, "context": "However, the results in [17\u201320] actually require a generative model of independent \u03a9k\u2019s, instead of sampling \u03a9k\u2019s based on a given \u03a9.", "startOffset": 24, "endOffset": 31}, {"referenceID": 18, "context": "However, the results in [17\u201320] actually require a generative model of independent \u03a9k\u2019s, instead of sampling \u03a9k\u2019s based on a given \u03a9.", "startOffset": 24, "endOffset": 31}, {"referenceID": 19, "context": "However, the results in [17\u201320] actually require a generative model of independent \u03a9k\u2019s, instead of sampling \u03a9k\u2019s based on a given \u03a9.", "startOffset": 24, "endOffset": 31}, {"referenceID": 16, "context": "Therefore, the results in [17\u201320] do not directly apply to the partition based resampling scheme that is easy to use.", "startOffset": 26, "endOffset": 33}, {"referenceID": 17, "context": "Therefore, the results in [17\u201320] do not directly apply to the partition based resampling scheme that is easy to use.", "startOffset": 26, "endOffset": 33}, {"referenceID": 18, "context": "Therefore, the results in [17\u201320] do not directly apply to the partition based resampling scheme that is easy to use.", "startOffset": 26, "endOffset": 33}, {"referenceID": 19, "context": "Therefore, the results in [17\u201320] do not directly apply to the partition based resampling scheme that is easy to use.", "startOffset": 26, "endOffset": 33}, {"referenceID": 16, "context": "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17\u201320] can apply, provided that the generative model of \u03a9 is exactly known.", "startOffset": 156, "endOffset": 163}, {"referenceID": 17, "context": "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17\u201320] can apply, provided that the generative model of \u03a9 is exactly known.", "startOffset": 156, "endOffset": 163}, {"referenceID": 18, "context": "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17\u201320] can apply, provided that the generative model of \u03a9 is exactly known.", "startOffset": 156, "endOffset": 163}, {"referenceID": 19, "context": "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17\u201320] can apply, provided that the generative model of \u03a9 is exactly known.", "startOffset": 156, "endOffset": 163}, {"referenceID": 3, "context": "In contrast, the classical results in [4\u20137] and our result herein are robust to the generative model of \u03a9: these results actually state that for an overwhelming portion of \u03a9 with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of \u03a9 a high probability result holds.", "startOffset": 38, "endOffset": 43}, {"referenceID": 4, "context": "In contrast, the classical results in [4\u20137] and our result herein are robust to the generative model of \u03a9: these results actually state that for an overwhelming portion of \u03a9 with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of \u03a9 a high probability result holds.", "startOffset": 38, "endOffset": 43}, {"referenceID": 5, "context": "In contrast, the classical results in [4\u20137] and our result herein are robust to the generative model of \u03a9: these results actually state that for an overwhelming portion of \u03a9 with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of \u03a9 a high probability result holds.", "startOffset": 38, "endOffset": 43}, {"referenceID": 6, "context": "In contrast, the classical results in [4\u20137] and our result herein are robust to the generative model of \u03a9: these results actually state that for an overwhelming portion of \u03a9 with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of \u03a9 a high probability result holds.", "startOffset": 38, "endOffset": 43}, {"referenceID": 33, "context": "In a recent work [34] the authors have managed to remove the dependency of the required sample size on by using a singular value projection algorithm.", "startOffset": 17, "endOffset": 21}, {"referenceID": 33, "context": "However, [34] considers a matrix variable of the same size as the original matrix, which requires significantly more memory than the matrix factorization approach considered in this paper.", "startOffset": 9, "endOffset": 13}, {"referenceID": 34, "context": "The resampling is also required in the recent work of [35]; see [30, Sec.", "startOffset": 54, "endOffset": 58}, {"referenceID": 31, "context": "Non-convex formulation has also been studied for the phase retrieval problem in some recent works [32, 36].", "startOffset": 98, "endOffset": 106}, {"referenceID": 35, "context": "Non-convex formulation has also been studied for the phase retrieval problem in some recent works [32, 36].", "startOffset": 98, "endOffset": 106}, {"referenceID": 35, "context": "The major difference between [36] and [32] is that the former requires independent samples in each iteration, while the latter uses the same samples throughout in the proposed algorithm.", "startOffset": 29, "endOffset": 33}, {"referenceID": 31, "context": "The major difference between [36] and [32] is that the former requires independent samples in each iteration, while the latter uses the same samples throughout in the proposed algorithm.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17\u201320] and our work.", "startOffset": 129, "endOffset": 136}, {"referenceID": 17, "context": "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17\u201320] and our work.", "startOffset": 129, "endOffset": 136}, {"referenceID": 18, "context": "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17\u201320] and our work.", "startOffset": 129, "endOffset": 136}, {"referenceID": 19, "context": "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17\u201320] and our work.", "startOffset": 129, "endOffset": 136}, {"referenceID": 36, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 0, "endOffset": 7}, {"referenceID": 37, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 0, "endOffset": 7}, {"referenceID": 38, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 0, "endOffset": 7}, {"referenceID": 39, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 21, "endOffset": 28}, {"referenceID": 40, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 21, "endOffset": 28}, {"referenceID": 41, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "[37\u201339]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling \u03a9k\u2019s with replacement; anyhow, under this model \u03a9k\u2019s are still dependent.", "startOffset": 82, "endOffset": 86}, {"referenceID": 42, "context": "Maximization) algorithm [43, 44].", "startOffset": 24, "endOffset": 32}, {"referenceID": 43, "context": "Maximization) algorithm [43, 44].", "startOffset": 24, "endOffset": 32}, {"referenceID": 44, "context": "The difference from traditional perturbation analysis of Wedin [45] (i.", "startOffset": 63, "endOffset": 67}, {"referenceID": 44, "context": "if two matrices are close then their row/column spaces are close) is that in [45] the row/column spaces are fixed while in our problem U,V are up to our choice.", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "This inequality is closely related to matrix RIP (restricted isometry property) in [8] (see equation (III.", "startOffset": 83, "endOffset": 86}, {"referenceID": 30, "context": "A solution, as employed in [31], is to utilize a random graph lemma in [46] which provides a bound on \u2016P\u03a9(A)\u2016F for any rank-1 matrix A (possibly dependent on \u03a9).", "startOffset": 27, "endOffset": 31}, {"referenceID": 45, "context": "A solution, as employed in [31], is to utilize a random graph lemma in [46] which provides a bound on \u2016P\u03a9(A)\u2016F for any rank-1 matrix A (possibly dependent on \u03a9).", "startOffset": 71, "endOffset": 75}, {"referenceID": 3, "context": "This lemma, combined with another probability result in [4], implies a bound on \u2016P\u03a9(M \u2212 XYT )\u2016F .", "startOffset": 56, "endOffset": 59}, {"referenceID": 30, "context": "A special case of the third condition has been used in [31] for Grassmann manifold optimization.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "One simple strategy as adopted in [17\u201320] is to use a resampling scheme to decouple A and the observation set.", "startOffset": 34, "endOffset": 41}, {"referenceID": 17, "context": "One simple strategy as adopted in [17\u201320] is to use a resampling scheme to decouple A and the observation set.", "startOffset": 34, "endOffset": 41}, {"referenceID": 18, "context": "One simple strategy as adopted in [17\u201320] is to use a resampling scheme to decouple A and the observation set.", "startOffset": 34, "endOffset": 41}, {"referenceID": 19, "context": "One simple strategy as adopted in [17\u201320] is to use a resampling scheme to decouple A and the observation set.", "startOffset": 34, "endOffset": 41}, {"referenceID": 30, "context": "Another strategy, as employed in [31], is to use a random graph lemma in [46].", "startOffset": 33, "endOffset": 37}, {"referenceID": 45, "context": "Another strategy, as employed in [31], is to use a random graph lemma in [46].", "startOffset": 73, "endOffset": 77}, {"referenceID": 45, "context": "We apply the random graph lemma of [46] when extending the local geometry of \u2016M\u2212XYT \u2016F to \u2016P\u03a9(M\u2212XY )\u2016F .", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "The difference of our work with [31] is that we study the local geometry in Euclidean space (and, indirectly, the geometry of the quotient manifold), which is quite different from the local geometry in Grassmann manifold studied in [31].", "startOffset": 32, "endOffset": 36}, {"referenceID": 30, "context": "The difference of our work with [31] is that we study the local geometry in Euclidean space (and, indirectly, the geometry of the quotient manifold), which is quite different from the local geometry in Grassmann manifold studied in [31].", "startOffset": 232, "endOffset": 236}, {"referenceID": 30, "context": "Technically, the complications of the proof in [31] are mostly due to heavy computation of various quantities in Grassmann manifold; in addition, much effort is spent in estimating the terms related to the extra factor S which enables the decoupling of X and Y ( [31] actually uses a three-factor decomposition XS YT ).", "startOffset": 47, "endOffset": 51}, {"referenceID": 30, "context": "Technically, the complications of the proof in [31] are mostly due to heavy computation of various quantities in Grassmann manifold; in addition, much effort is spent in estimating the terms related to the extra factor S which enables the decoupling of X and Y ( [31] actually uses a three-factor decomposition XS YT ).", "startOffset": 263, "endOffset": 267}, {"referenceID": 29, "context": "10 of [30] shows that when |\u03a9| is small, in all successful instances the iterates are balanced, while in all failed instances the iterates are unbalanced.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "The incoherence condition for the matrix completion problem is first introduced by Cand\u00e8s and Recht in [4] and has become a standard assumption for low-rank matrix recovery problems (except a few recent works such as [47, 48]).", "startOffset": 103, "endOffset": 106}, {"referenceID": 46, "context": "The incoherence condition for the matrix completion problem is first introduced by Cand\u00e8s and Recht in [4] and has become a standard assumption for low-rank matrix recovery problems (except a few recent works such as [47, 48]).", "startOffset": 217, "endOffset": 225}, {"referenceID": 47, "context": "The incoherence condition for the matrix completion problem is first introduced by Cand\u00e8s and Recht in [4] and has become a standard assumption for low-rank matrix recovery problems (except a few recent works such as [47, 48]).", "startOffset": 217, "endOffset": 225}, {"referenceID": 30, "context": "We will define an incoherence condition for an m \u00d7 n matrix M which is the same as that in [31].", "startOffset": 91, "endOffset": 95}, {"referenceID": 30, "context": "For some popular random models for generating M, the incoherence condition holds with a parameter scaling as \u221a r log n (see [31]).", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "We remark that this model is \u201cequivalent to\u201d a Bernolli model that each entry of M is included into \u03a9 independently with probability p = S mn in the sense that if the success of an algorithm holds for the Bernolli model with a certain p with high probability, then the success also holds for the uniform random model with |\u03a9| = pmn with high probability (see [4] or [31, Sec.", "startOffset": 359, "endOffset": 362}, {"referenceID": 0, "context": "The choice of function G0 is not unique; in fact, we can choose any G0 that satisfies the following requirements: a) G0 is convex and continuously differentiable; b) G0(z) = 0, z \u2208 [0, 1].", "startOffset": 181, "endOffset": 187}, {"referenceID": 30, "context": "In [31], G0 is chosen as G0(z) = I[1,\u221e](z)(e 2 \u2212 1), which also satisfies these two requirements.", "startOffset": 3, "endOffset": 7}, {"referenceID": 48, "context": "[49])", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[49]), which motivates our choice of G0 in (14).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 17, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 18, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 19, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 30, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 31, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 35, "context": "Special initialization is also required in other works on non-convex formulations [17\u201320, 31, 32, 36].", "startOffset": 82, "endOffset": 101}, {"referenceID": 30, "context": "[31]), we obtain M0 = X\u03020\u0176 0 which is close to M; second, we scale the rows of (X\u03020, \u01760) to make it incoherent (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "There are many choices of stepsizes such as constant stepsize, exact line search, limited line search, diminishing stepsize and Armijo rule [50].", "startOffset": 140, "endOffset": 144}, {"referenceID": 30, "context": "Note that the restricted line search rule is similar to that used in [31] for the gradient descent method over Grassmannian manifolds.", "startOffset": 69, "endOffset": 73}, {"referenceID": 50, "context": "cyclic [51\u201353], randomized [54] or parallel) and solve the subproblem inexactly.", "startOffset": 7, "endOffset": 14}, {"referenceID": 51, "context": "cyclic [51\u201353], randomized [54] or parallel) and solve the subproblem inexactly.", "startOffset": 7, "endOffset": 14}, {"referenceID": 52, "context": "cyclic [51\u201353], randomized [54] or parallel) and solve the subproblem inexactly.", "startOffset": 7, "endOffset": 14}, {"referenceID": 53, "context": "cyclic [51\u201353], randomized [54] or parallel) and solve the subproblem inexactly.", "startOffset": 27, "endOffset": 31}, {"referenceID": 53, "context": "Commonly used inexact BCD type algorithms include BCGD (block coordinate gradient descent, which updates each variable by a single gradient step [54]) and BSUM (block successive upper bound minimization, which updates each variable by minimizing an upper bound of the objective function [55]).", "startOffset": 145, "endOffset": 149}, {"referenceID": 54, "context": "Commonly used inexact BCD type algorithms include BCGD (block coordinate gradient descent, which updates each variable by a single gradient step [54]) and BSUM (block successive upper bound minimization, which updates each variable by minimizing an upper bound of the objective function [55]).", "startOffset": 287, "endOffset": 291}, {"referenceID": 55, "context": "[56, 57]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 56, "context": "[56, 57]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 57, "context": "[58] proposed an algorithm that could be viewed as a BSUM algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Just considering different choices of the blocks will lead to different algorithms for the matrix completion problem [29].", "startOffset": 117, "endOffset": 121}, {"referenceID": 53, "context": "In the regimes of |\u03a9| that the vanilla AltMin fails, G is active and the gradient updates do happen; however, instead of solving the subproblem exactly, one could perform one gradient step and the algorithm becomes the popular variant BCGD [54].", "startOffset": 240, "endOffset": 244}, {"referenceID": 54, "context": "Such a technique has also been used in the alternating least square algorithm for tensor decomposition [55].", "startOffset": 103, "endOffset": 107}, {"referenceID": 58, "context": "Note that for the two-block BCD algorithm, convergence to stationary points can be guaranteed even when the subproblems are not strongly convex [59], thus in Algorithm 2 we do not add the extra terms.", "startOffset": 144, "endOffset": 148}, {"referenceID": 0, "context": "The fourth algorithm we present is SGD (stochastic gradient descent) [1, 23] tailored for our problem (P1).", "startOffset": 69, "endOffset": 76}, {"referenceID": 22, "context": "The fourth algorithm we present is SGD (stochastic gradient descent) [1, 23] tailored for our problem (P1).", "startOffset": 69, "endOffset": 76}, {"referenceID": 59, "context": ", [60] for one example of such analysis).", "startOffset": 2, "endOffset": 6}, {"referenceID": 60, "context": "In this paper we only consider the cyclic order, and use a standard stepsize rule for SGD [61, 62] which requires the stepsizes {\u03b7k} to go to zero as k \u2192 \u221e, but neither too fast nor too slow (this choice guarantees convergence to stationary points even for nonconvex problems).", "startOffset": 90, "endOffset": 98}, {"referenceID": 61, "context": "In this paper we only consider the cyclic order, and use a standard stepsize rule for SGD [61, 62] which requires the stepsizes {\u03b7k} to go to zero as k \u2192 \u221e, but neither too fast nor too slow (this choice guarantees convergence to stationary points even for nonconvex problems).", "startOffset": 90, "endOffset": 98}, {"referenceID": 3, "context": "Similar to the results for nuclear norm minimization [4\u20137], the probability is taken with respect to the random choice of \u03a9, and the result also applies to a uniform random model of \u03a9.", "startOffset": 53, "endOffset": 58}, {"referenceID": 4, "context": "Similar to the results for nuclear norm minimization [4\u20137], the probability is taken with respect to the random choice of \u03a9, and the result also applies to a uniform random model of \u03a9.", "startOffset": 53, "endOffset": 58}, {"referenceID": 5, "context": "Similar to the results for nuclear norm minimization [4\u20137], the probability is taken with respect to the random choice of \u03a9, and the result also applies to a uniform random model of \u03a9.", "startOffset": 53, "endOffset": 58}, {"referenceID": 6, "context": "Similar to the results for nuclear norm minimization [4\u20137], the probability is taken with respect to the random choice of \u03a9, and the result also applies to a uniform random model of \u03a9.", "startOffset": 53, "endOffset": 58}, {"referenceID": 3, "context": "As demonstrated in [4] (and proved in [5, Theorem 1.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "log n factor is due to the coupon collector effect [4].", "startOffset": 51, "endOffset": 54}, {"referenceID": 18, "context": "the one proposed in [19]) can reduce the exponents of r and \u03ba.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "1 in [4]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 30, "context": "\u2016P\u03a9(b)\u2016F can be bounded according to a random graph lemma of [31, 46], which requires U,V, X,Y to be incoherent (i.", "startOffset": 61, "endOffset": 69}, {"referenceID": 45, "context": "\u2016P\u03a9(b)\u2016F can be bounded according to a random graph lemma of [31, 46], which requires U,V, X,Y to be incoherent (i.", "startOffset": 61, "endOffset": 69}, {"referenceID": 3, "context": "1 in [4], for |\u03a9| satisfying (27) with large enough C0, we have that with probability at least 1\u22121/(2n4), \u2016PTP\u03a9PT (a) \u2212 pPT (a)\u2016F \u2264 6 p\u2016a\u2016F (note that this bound holds uniformly for all a \u2208 T , thus also holds when a is dependent on \u03a9).", "startOffset": 5, "endOffset": 8}, {"referenceID": 44, "context": "Such a result bears some similarity with the classical perturbation theory for singular value decomposition [45].", "startOffset": 108, "endOffset": 112}, {"referenceID": 44, "context": "In particular, [45] proved that for two low-rank matrices4 that are close, the spaces spanned by the left (resp.", "startOffset": 15, "endOffset": 19}, {"referenceID": 44, "context": "To resolve this issue, we need to prove the second proposition in which there is an additional 4The result in [45] also covered the case of two approximately low-rank matrices, but we only consider the case of exact low-rank matrices here.", "startOffset": 110, "endOffset": 114}, {"referenceID": 54, "context": "This definition is motivated by the block successive upper bound minimization method [55].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "and {xt} satisfies either of the following three conditions: 1) F\u0303(xt + \u03bb\u2206t) \u2264 2F\u0303(x0),\u2200 \u03bb \u2208 [0, 1], where \u2206t = xt+1 \u2212 xt, \u2200 t; (67a) 2) 1 = arg min \u03bb\u2208R \u03c8(xt,\u2206t; \u03bb), where \u03c8 satisfies (65),\u2206t = xt+1 \u2212 xt, \u2200 t; (67b) 3) F\u0303(xt) \u2264 2F\u0303(x0), d(xt,x0) \u2264 5 6 \u03b4, \u2200 t.", "startOffset": 93, "endOffset": 99}, {"referenceID": 30, "context": "1 can be found in [31].", "startOffset": 18, "endOffset": 22}, {"referenceID": 0, "context": "Since d(x,u\u2217) is a continuous function over x, the relation d(xt,u) \u2264 3\u03b4 and (230) imply that there must exist some x\u2032 = (1\u2212 \u03bb)xt+1 + \u03bbxt, \u03bb \u2208 [0, 1] such that d(x\u2032,u\u2217) = \u03b4.", "startOffset": 143, "endOffset": 149}, {"referenceID": 0, "context": "Since d(x,u\u2217) is a continuous function over x and d(x\u2032,u\u2217) \u2264 \u03b4 (230) < d(xt+1,u), there must exist some x\u2032\u2032 = (1 \u2212 )xt+1 + x\u2032= xt + (1 \u2212 + \u03bb)\u2206t, \u2208 [0, 1] such that d(x\u2032\u2032,u\u2217) = \u03b4.", "startOffset": 147, "endOffset": 153}, {"referenceID": 0, "context": ", F\u0303(xt\u22121 + \u03bb\u2206t\u22121) \u2264 2F\u0303(x0),\u2200\u03bb \u2208 [0, 1], where \u2206t = xt \u2212 xt\u22121.", "startOffset": 34, "endOffset": 40}, {"referenceID": 0, "context": "= F\u0303(xt) + \u2016\u2207F\u0303(xt)\u2016( L1 2 \u03bb2\u03b72 \u2212 \u03bb\u03b7) \u2264 F\u0303(xt) \u2212 \u03bb\u03b7 2 \u2016\u2207F\u0303(xt)\u2016 \u2264 F\u0303(xt) \u2264 2F\u0303(x0), \u2200 \u03bb \u2208 [0, 1], (239)", "startOffset": 90, "endOffset": 96}, {"referenceID": 0, "context": ", F\u0303(xk + \u03bb\u2206k) \u2264 2F\u0303(x0),\u2200\u03bb \u2208 [0, 1], where \u2206k = xk+1 \u2212 xk, 0 \u2264 k \u2264 t \u2212 1.", "startOffset": 30, "endOffset": 36}, {"referenceID": 0, "context": "F\u0303(xt + \u03bb(xt+1 \u2212 xt)) \u2264 2F\u0303(x0), \u2200 \u03bb \u2208 [0, 1],", "startOffset": 39, "endOffset": 45}, {"referenceID": 49, "context": "11) of [50]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": "17) in [50]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": "17) in [50]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": "17) in [50] (except that (1.", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": "17) in [50] considers a more general descent direction), and the rest of the proof is also the same as [50] and is omitted here.", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": "17) in [50] considers a more general descent direction), and the rest of the proof is also the same as [50] and is omitted here.", "startOffset": 103, "endOffset": 107}, {"referenceID": 54, "context": "Algorithm 3 belongs to the class of BSUM methods [55].", "startOffset": 49, "endOffset": 53}, {"referenceID": 54, "context": "It is easy to verify that the objective function of each subproblem in Algorithm 3 is a convex tight upper bound of F\u0303(x) (more precisely, satisfies Assumption 2 in [55]).", "startOffset": 165, "endOffset": 169}], "year": 2016, "abstractText": "Matrix factorization is a popular approach for large-scale matrix completion. The optimization formulation based on matrix factorization can be solved very efficiently by standard algorithms in practice. However, due to the non-convexity caused by the factorization model, there is a limited theoretical understanding of this formulation. In this paper, we establish a theoretical guarantee for the factorization formulation to correctly recover the underlying low-rank matrix. In particular, we show that under similar conditions to those in previous works, many standard optimization algorithms converge to the global optima of a factorization formulation, and recover the true lowrank matrix. We study the local geometry of a properly regularized factorization formulation and prove that any stationary point in a certain local region is globally optimal. A major difference of our work from the existing results is that we do not need resampling in either the algorithm or its analysis. Compared to other works on nonconvex optimization, one extra difficulty lies in analyzing nonconvex constrained optimization when the constraint (or the corresponding regularizer) is not \u201cconsistent\u201d with the gradient direction. One technical contribution is the perturbation analysis for non-symmetric matrix factorization.", "creator": "LaTeX with hyperref package"}}}