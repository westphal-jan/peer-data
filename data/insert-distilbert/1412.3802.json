{"id": "1412.3802", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2014", "title": "Turing Test for the Internet of Things", "abstract": "how smart is your kettle? how smart are things organized in your rented kitchen, your house, your neighborhood, on the internet? with the advent of internet of things, and the move of making technological devices ` smart'by utilizing ai, a natural question arrises, how can we evaluate the eventual progress. the standard way of evaluating ai is through the turing function test. while nasa turing test was designed for ai ; the device that it was uniquely tailored to fit was a computer. applying the test to variety of devices that constitute internet of things poses a number of formidable challenges which could be addressed through involving a number of adaptations.", "histories": [["v1", "Thu, 11 Dec 2014 09:49:07 GMT  (237kb,D)", "http://arxiv.org/abs/1412.3802v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["neil rubens"], "accepted": false, "id": "1412.3802"}, "pdf": {"name": "1412.3802.pdf", "metadata": {"source": "CRF", "title": "Turing Test for the Internet of Things", "authors": ["Neil Rubens"], "emails": [], "sections": [{"heading": null, "text": "1 Turing Test for the Internet of Things Neil Rubens 1,2,3\n1 Active Intelligence Lab 2 Skolkovo Institute of Science and Technology 3 University of Electro-Communications\nAbstract\u2014Draft: Work in Progress: How smart is your kettle? How smart are things in your kitchen, your house, your neighborhood, on the internet? With the advent of Internet of Things, and the move of making devices \u2018smart\u2019 by utilizing AI, a natural question arrises, how can we evaluate the progress. The standard way of evaluating AI is through the Turing Test. While Turing Test was designed for AI; the device that it was tailored to was a computer. Applying the test to variety of devices that constitute Internet of Things poses a number of challenges which could be addressed through a number of adaptations.\nI. INTRODUCTION With the advent of the Internet of Things (IOT), and the move of making devices \u201csmart\u201d by utilizing AI [4], a natural question arrises: \u201cHow can we evaluate the progress of making devices smarter?\u201d. A common approach of evaluating AI is through the Turing Test (TT) [6] in which interrogator through a text conversation with two participants tries to determine which is a computer and which is a human (figure 1). However, in the context of the Internet of Things, Turing test (in its traditional form) may not be directly applicable in part due to the following:\n1) Turing Test assumes that AI is embodied in a single computer:\n\u201cThe present interest in \u2018thinking machines\u2019 has been aroused by a particular kind of machine, usually called an \u2018electronic computer\u2019 or \u2018digital computer\u2019. Following this suggestion we only permit digital computers to take part in our game.\u201d [6] \u2022 Internet of Things\n\u2013 multiple (connected) devices \u2013 many of the devices contain \u201ccomputers\u201d, but are\nnot computers per-say (e.g. kettle or fridge). 2) Turing Test focuses on measuring conversational intel-\nligence: \u201cThe ideal arrangement is to have a teleprinter communicating between the two rooms.\u201d [6] \u2022 Internet of Things\n\u2013 some of the devices have no means of conducting a conversation (e.g. a toaster).\nEvaluating AI in the context of IOT seems to require some adaptations, descriptions of which we provide in this paper. Similarly to the original Turning test paper [6]; we avoid defining \u201cintelligence\u201d and instead concentrate on practical/pragmatic aspects."}, {"heading": "II. SIMPLIFIED TURING TEST FOR THE INTERNET OF THINGS", "text": "Let us propose a straightforward adaption of the TT to IOT; followed by describing the challenges that the test would face (in section \u00a7IV we propose extensions to TT that allow to alleviate at least some of the challenges). In this version we strive to make as few modifications to the original turing test as possible. Hence, we only modify the medium through which the \u201cconversation\u201d happens by replacing computer terminal with a \u201cthing\u201d (e.g. an electric kettle (figure 2)). To add the i/o capabilities, we add a remote input/control to the kettle (that controls the on/off switch) (similar to the keyboard in the\nar X\niv :1\n41 2.\n38 02\nv1 [\ncs .A\nI] 1\n1 D\nec 2\n01 4\n2\noriginal test). Interrogator is also able to interact with \u201ckettle\u201d through \u201clocal\u201d on/off switch.\nthe communication channel (sending on/off status along with timestamps). One of the kettles would be controlled by a person and the other one would be controlled by a computer; interrogator\u2019s task would be to decide which is which."}, {"heading": "III. CHALLENGES FOR TURNING TEST IN THE CONTEXT", "text": ""}, {"heading": "OF THE INTERNET OF THINGS", "text": "Turing test has been a truly visionary work that became a standard for evaluating performance of AI systems; preceding by 6 years the official establishment of the field of Artificial Intelligence [2]. However, in 1950 it was difficult to foresee all of upcoming developments which would affect the development of AI. In this section, we describe various aspects of the Internet of Things that make it difficult to apply TT in a straightforward manner."}, {"heading": "A. Limited Interface", "text": "In Turning test interaction is assumed to happen in the text conversation format. Many of the things constituting IOT have limited interfaces; e.g. a kettle has only on/off switch."}, {"heading": "B. Limited Computational Capabilities", "text": "Many of the IOT devices have limited computing capabilities which are often geared to very specific purposes (e.g. auto-shutoff controller of the kettle).\nC. Intelligent Behavior that is not Human Behavior\nThe Turing test is a test of a machine\u2019s ability to exhibit intelligent behavior indistinguishable from that of a human. However, one of the major drawbacks of Turing Test is that not all of the intelligent behavior is human-like (figure 3). This is exacerbated if we consider that the test medium is not a textual conversation (which many things are not capable of functionally section \u00a7III-A). Bellow we outline some of the challenges that are exharbated in the context of IOT.\n1) Analysis of Large Quantities of Numerical Data: Most people may have difficulty with analyzing large quantities of numerical data (especially without analytical tools); more so if it comes from multiple sources.\n2) Response Time: People are not able to respond as quickly as machines.\n3) Time: Availability / Duration: Turning test assumes a relatively short pre-scheduled session during which examiner performs a test: \u201cmating the right identification after five minutes of questioning\u201d [6]. The natural test duration for testing of things could be much longer; e.g. examining if your coffee making machine is intelligent could take weeks or months; and not necessarily at pre-scheduled times (e.g. if you happen to wake earlier than usual and need a sip of coffee).\n4) Memory / History: In the original description it wasn\u2019t stated explicitly if there is a conversational memory (script).\nand we also add the conversation \u201cmemory\u201d function that shows history of on/off activity (details ...); similar to the text conversation history displayed on the screen.\nfor this settings peoples memory is more limited than that of a computer"}, {"heading": "D. Evaluation Criteria", "text": "Turing test evaluates conversational intelligence. However it is not clear wether this criteria makes sense in different context; e.g. is it appropriate to measure how \u201cintelligent\u201d a kettle is by having a conversation with it; what if it talks well, but doesn\u2019t perform well as a kettle (starts pondering the meaning of life, while deciding wether to boil the water)?"}, {"heading": "E. Multiple Participants", "text": "Turning test assumes interaction with a single participant at a time (be it a human or a computer). Internet of Things as the name implies, assumes that there are multiple entities involved. Therefore the challenge is adopting Turing test to multiple participants."}, {"heading": "IV. PROPOSED APPROACHES", "text": "In this section we describe approaches to addressing challenges of adopting the Turing Test to the context of the Internet of Things (III)."}, {"heading": "A. Limited Interface", "text": "As described in (section \u00a7III-A) many of the things constituting IOT have limited interfaces; e.g. a kettle has only on/off switch. One might suggest that the original interface (e.g. on/off switch) could be used to do a binary encoding of a text conversation. Another option is to add an interface for a text conversation. However, even once interface allows for the textual conversation; other related challenges still remain unanswered: would the device computationally capable of having an intelligent conversation (section \u00a7III-B), and is conversation the proper way to evaluate the intelligence of a device (section \u00a7III-D). Answer to these questions might dictate the form of the interface. Our suggestion is to leave the interface as is.\n3 I : day1 : 8 :00am : t u r n on k e t t l e T : day1 : 8 :02am : t u r n o f f k e t t l e I : day1 : 8 :10am : t u r n on k e t t l e T : day1 : 8 :12am : t u r n o f f k e t t l e\nT : day2 : 7 :58am : t u r n on k e t t l e T : day2 : 8 :00am : t u r n o f f k e t t l e I : day2 : 8 :10am : t u r n on k e t t l e T : day2 : 8 :12am : t u r n o f f k e t t l e\nT : day3 : 8 :09am : t u r n on k e t t l e T : day3 : 8 :12am : t u r n o f f k e t t l e\nFigure 4. Interaction Intelligence Criteria. I - interrogator, T - participating thing (controlled by computer or a human) see section \u00a7IV-D1."}, {"heading": "B. Limited Computational Capabilities", "text": "As described in (section \u00a7III-B) many of the devices have limited computational power and/or target for specific purposes (e.g. micro-controller in kettle for the auto-shut-off function). It is possible to modify/add hardware and software as to allow for generating and processing of textual conversations (either locally or remotely). However in this case, we might be appraising the intelligence of the add-ons rather than that of the device (which is our original goal). Hence our suggestion is to leave the hardware and software as is.\nC. Intelligent Behavior that is not Human Behavior\nWhile previous challenges were favorable to humans; the challenges in this section favors computers; since we focus on the intelligent behavior that humans don\u2019t do (section \u00a7III-C). We do want to make this a fair test hence we try to address these challenges. In the original paper [6] this challenge was pointed out:\n\u201cIt is claimed that the interrogator could distinguish the machine from the man simply by setting them a number of problems in arithmetic. The machine would be unmasked because of its deadly accuracy.\u201d\nIn the same paper [6] a potential solution was suggested: \u201cThe machine (programmed for playing the game) would not attempt to give the right answers to the arithmetic problems. It would deliberately introduce mistakes in a manner calculated to confuse the interrogator.\u201d\nMaking AI appear dumber and slower than it really is, has been used in programs aimed at passing the turing test, as well as in industry e.g. gaming [5].\n\u201cDumbing down\u201d approach could certainly be introduced in the context of IOT in a fairly straightforward manner. However, we strongly feel that the aim should be to achieve a high intelligence, rather than to simply mimic human behavior (section \u00a7IV-D). Hence we take a different approach of attempting to make human participants to appear more \u201ccomputationally intelligent\u201d. For the original test this could be achieved be equipping participants with calculators. In the context of IOT, additional modifications may be needed. To\nnarrow the gap between human and computers (for the tasks in which computers perform better); we consider two nonexclusive approaches: human-centered and computer assisted.\n1) Human-centered: A typical approach of improving processing speed of computers is through:\n\u2022 faster processing \u2022 parallelization\nPeople\u2019s response time could be improved in a similar manner: \u2022 faster processing\n\u2013 using more skilled human operators \u2013 training human operators\n\u2022 parallelization \u2013 using multiple participants\nThis approach does narrow the performance gap but not sufficient, and it is \u201cexpensive\u201d. If this approach is used, it should also be complemented with the computer-assisted approach.\n2) Computer-assisted: Receiving assistance from computers should allow to narrow (if not exceed) some of the performance gaps. In particular computers could be used to assist with: (1) analysis, (2) execution).\na) Computer-assisted Analysis: Using analytic software would allow to address the challenge of analyzing large quantities of numerical data (section \u00a7III-C1).\nb) Computer-assisted Execution: The rest of the challenges (section \u00a7III-C) could be addressed by programming computers with the desired responses or analysis algorithms. In order to keep the distinction between pure AI and computerassisted approach people should have ability to override computer\u2019s decisions."}, {"heading": "D. Evaluation Criteria", "text": "Turing test evaluates conversational intelligence. However it is not clear wether this criteria makes sense in different context; e.g. is it appropriate to measure how \u201cintelligent\u201d a kettle is by having a conversation with it; what if it talks well, but doesn\u2019t perform well as a kettle (starts pondering the meaning of life for an hour, while deciding wether to boil the water)? On the other hand, focusing on evaluation operational performance does not necessarily reflects how \u201cintelligent\u201d the\n4 thing is. Bellow we propose some of the approaches that could be used (in combination or independently).\n1) Interaction Intelligence : In order to overcome some of the operational speed capabilities of humans and computers (section \u00a7III-C); we can evaluate intelligence through a form of dialogue. However, the dialogue is not be textual but is operational (figure 4). Both the interrogator and participants can control the time-stamps as to simulate the flow of time). Note that interrogator can prepare multiple dialogue scripts in order to examine the responses of the participants. In the end; based on the responses interrogator would have to decide wether the participant (interrogated thing) is controlled by a human or a computer.\n2) Operational Intelligence: Devices could be evaluated based on how \u201cintelligently\u201d they behave. Unlike pure operational evaluation (e.g. how quickly the water is boiled); this criteria would also consider intelligence aspects, e.g. is a kettle able to distinguish between different users (members of the family), different usages (boiling water for tea, or for drip coffee, or other purposes).\n3) Inner Intelligence: While the operational intelligence reflects the intelligence of the behavior of the objects (section \u00a7IV-D2); inner intelligence evaluation allows users to examine the internal rules/workings of the things to determine if those are intelligent or not. However this evaluation assumes that computer-assisted execution method was used (section \u00a7IV-C2), and that resulting rules could be represented in a human readable form e.g. if then rules or decision trees; however other methods are very hard to either create or examine manually e.g. SVM, random forests, neural networks."}, {"heading": "E. Multiple Participants", "text": "Turning test assumes interaction with a single participant at a time (be it a human or a computer). Internet of Things, as the name implies, assumes that there are multiple entities involved. Therefore the challenge is adopting Turing test to multiple participants.\nAssuming multiple participants raises more questions about evaluating:\n\u2022 collective/distributed intelligence \u2022 mixed groups\n\u2013 different devices \u2013 different types of entities (humans & non-humans)\n\u2022 multiple evaluators \u2022 many-many or one-many (e.g. one-many: one person\ncontrolling multiple things) \u2022 participating vs non-participating entities (in the evalua-\ntion) Naive approach would be to simply apply the single-version of the test to multiple objects and aggregate the results. More interesting approaches could be obtained by being somewhat creative; e.g. mixing human-controlled things with computercontrolled, etc."}, {"heading": "V. CONCLUSION", "text": "It\u2019s been over 50 years since the Turing Test for evaluating AI has been proposed. TT has stood up very well to the\ntest of times; and it is still by far the most prevalent way in which AI is evaluated. However, new settings, in particular the Internet of Things, pose challenges to the traditional definition of the Turing Test (section \u00a7III). We show that with some modifications (section \u00a7IV) the Turing Test could be adapted to these novel settings."}, {"heading": "ACKNOWLEDGEMENT", "text": "This research is performed in conjunction with the ThingTank project [3] a part of the Ideas Lab MIT-Skoltech Initiative."}], "references": [{"title": "Artificial Intelligence: A Philosophical Introduction", "author": ["J. Copeland"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "AI: The Tumultuous History of the Search for Artificial Intelligence", "author": ["D. Crevier"], "venue": "Basic Books,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Things making things", "author": ["E. Giaccardi", "C. Speed", "N. Rubens"], "venue": "In International Research Network for Design Anthropology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Smart objects as building blocks for the internet of things", "author": ["G. Kortuem", "F. Kawsar"], "venue": "Internet Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Artificial stupidity: The art of intentional mistakes", "author": ["L. Lid\u00e9n"], "venue": "AI Game Programming Wisdom,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Computing machinery and intelligence", "author": ["A. Turing"], "venue": "Mind, LIX(236),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1950}], "referenceMentions": [{"referenceID": 3, "context": "With the advent of the Internet of Things (IOT), and the move of making devices \u201csmart\u201d by utilizing AI [4], a natural question arrises: \u201cHow can we evaluate the progress of making devices smarter?\u201d.", "startOffset": 104, "endOffset": 107}, {"referenceID": 5, "context": "A common approach of evaluating AI is through the Turing Test (TT) [6] in which interrogator through a text conversation with two participants tries to determine which is a computer and which is a human (figure 1).", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "\u201d [6]", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "\u201d [6]", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "Similarly to the original Turning test paper [6]; we avoid defining \u201cintelligence\u201d and instead concentrate on practical/pragmatic aspects.", "startOffset": 45, "endOffset": 48}, {"referenceID": 0, "context": "Turing Test: interrogator through a text conversation with two participants tries to determine which is a computer and which is a human [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "Turing test has been a truly visionary work that became a standard for evaluating performance of AI systems; preceding by 6 years the official establishment of the field of Artificial Intelligence [2].", "startOffset": 197, "endOffset": 200}, {"referenceID": 5, "context": "3) Time: Availability / Duration: Turning test assumes a relatively short pre-scheduled session during which examiner performs a test: \u201cmating the right identification after five minutes of questioning\u201d [6].", "startOffset": 203, "endOffset": 206}, {"referenceID": 5, "context": "In the original paper [6] this challenge was pointed out:", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "In the same paper [6] a potential solution was suggested:", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "gaming [5].", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "This research is performed in conjunction with the ThingTank project [3] a part of the Ideas Lab MIT-Skoltech Initiative.", "startOffset": 69, "endOffset": 72}], "year": 2014, "abstractText": "Draft: Work in Progress: How smart is your kettle? How smart are things in your kitchen, your house, your neighborhood, on the internet? With the advent of Internet of Things, and the move of making devices \u2018smart\u2019 by utilizing AI, a natural question arrises, how can we evaluate the progress. The standard way of evaluating AI is through the Turing Test. While Turing Test was designed for AI; the device that it was tailored to was a computer. Applying the test to variety of devices that constitute Internet of Things poses a number of challenges which could be addressed through a number of adaptations.", "creator": "LaTeX with hyperref package"}}}