{"id": "1412.4186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2014", "title": "An Evaluation of Support Vector Machines as a Pattern Recognition Tool", "abstract": "the purpose of this structured report is in examining just the generalization performance of support vector machines ( svm ) as a tool for pattern recognition and object classification. the work also is motivated by the growing theoretical popularity of the method that is claimed to guarantee a good generalization performance for the task in hand. the method is implemented in matlab. svms implementations based on various kernel kernels are tested for classifying data streams from those various domains.", "histories": [["v1", "Sat, 13 Dec 2014 03:33:13 GMT  (239kb)", "http://arxiv.org/abs/1412.4186v1", "A short (6 page) report on evaluation of the SVM as a pattern classification tool, as of 1999"]], "COMMENTS": "A short (6 page) report on evaluation of the SVM as a pattern classification tool, as of 1999", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["eugene borovikov"], "accepted": false, "id": "1412.4186"}, "pdf": {"name": "1412.4186.pdf", "metadata": {"source": "CRF", "title": "An Evaluation of Support Vector Machines as a Pattern Recognition Tool", "authors": ["Eugene A. Borovikov"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Statistical Learning Theory, Pattern Recognition, Support Vector Machine, SVM, VC Dimension"}, {"heading": "Introduction", "text": "Support Vector Machines (SVM) are binary classifiers of objects represented by points in Rn. The foundations of SVM have been developed by Vapnik (1995). These pattern classifiers are gaining popularity due to a number of attractive features including a promising generalization performance. The formulation embodies the Structural Risk Minimization (SRM) principle, as opposed to the Empirical Risk Minimization (ERM) approach commonly employed within statistical learning methods. It is this difference which equips SVMs with an excellent potential to generalize. Since many real-world objects can be represented as points in Rn, and multi-class classifiers can be built by employing arrays of SVMs, the technique is expected to be applicable to a great variety of classification problems. Feasibility and computational cost of such applications are separate issues, and ought to be considered on the case-by-case basis."}, {"heading": "SVM Background", "text": "The SVM theory grew out of considerations of under what circumstances, and how quickly, the mean of some empirical quantity converges uniformly, as the number of data points increases, to the true mean. To describe the idea of a Support Vector Machine, we first have to address the issue of Structural Risk Minimization principle. Let us start by posing a generic statistical learning problem."}, {"heading": "Statistical Learning", "text": "Suppose that we have a feature space X of potential observations; each observation ought to be labeled as one of K classes, or 'doubt' D, or 'outlier' O; let Y = {1,\u2026,K,D,O} be the space of possible decisions, then a classifier is a map from X to Y. Let the training set T be a set of n classified cases; statistical learning is then finding a classifying map from X to Y based on data in T; any future case (outside T) will be classified by the map found.\nFor the purpose of reformulating of a generic problem for an SVM, we let X = Rn and Y = {-1,1}. Assume there is an unknown probability distribution P(x,y) from which the labeled observations are drawn. For any mapping f:X\u00d7\u0391\u2192Y, where \u0391 is a parameter space, define the following quantity:\n),(),( 2 1)( yxdPxfyR \u222b \u2212= \u03b1\u03b1\nThis quantity has several names. The most common are: expected risk, actual risk, or just the risk. We say that\nfor a fixed parametric family {f(x,\u03b1)}, any choice of a particular \u03b1 produces a machine (i.e. a classifier). It is our goal to find a machine with the minimal actual risk, but since P(x,y) is usually unknown, it is impossible to do directly. It is, however, possible to find an upper bound for the actual risk and pose a problem for its minimization. Prior to doing that, we need to introduce the notion of a function set capacity and define some means of measuring that capacity.\nThe term capacity can be introduced as the ability of a machine to learn any training set without an error. Suppose we have a data set of l points that can be assigned labels 1 or -1. Clearly, there are 2l ways to label the data\nset. If for each labeling, there is a function in the function set {f(x,\u03b1)} that can correctly assign those labels, we say that the data set is shattered by the function set. Maximum cardinality of the data set that can be shattered by {f(x,\u03b1)} is called the Vapnik-Chervonenkis (VC) dimension of that function set. VC-dimension is clearly a property of the function family and can then be used as a measure of capacity of a particular learning machine belonging to that family."}, {"heading": "Structural Risk Minimization", "text": "Define the empirical risk as the measured mean error rate on the training set:\n\u2211 =\n\u2212= l\ni iiemp xfyl R 1 ),( 2 1)( \u03b1\u03b1\nNotice that no probability distribution appears here; this quantity is fixed for particular \u03b1 and particular training\nset {xi, yi}. Now, we can write the upper bound on the actual risk established by Vapnik:\nl hlhRR emp )4/log()1)/2(log()()( \u03b7\u03b1\u03b1 \u2212++\u2264\nHere \u03b7 = Pr{y = 1} and h is the VC dimension of {f(x,\u03b1)}. The second term in the right-hand side is called VC confidence. Finding a learning machine with the minimum upper bound on the actual risk leads us to a method of choosing an optimal machine for a given task. This is the essential idea of the structural risk minimization (SRM).\nSuppose we have a sequence of nested function families H1 \u2282 H2 \u2282 H3 \u2282 \u2026 such that their VC dimensions satisfy h1 < h2 < h3 < \u2026 The objective of SRM then is in finding such Hi for which the upper bound on the actual risk is minimal. This can be achieved through the following two-stage process: - for each Hi, identify a machine with minimal Remp(\u03b1) - for the final classifier, choose the machine, for which Remp(\u03b1)+VC-confidence is minimal\nSupport Vector Machines One kind of classifiers is especially convenient for implementing the Structural Risk Minimization principle.\nThis kind of machines is called Support Vector Machines (SVM). The origin of the name will be clear later."}, {"heading": "Linear SVM", "text": "Given training data as a sequence of l labeled points in Rn, the task is to find an \"optimal\" hyper-plane separating them. There is a factor here that we have to take into account: linear separability of training data. For the linearly separable case, the support vector algorithm simply looks for the separating hyper-plane with the largest margin, where margin = 2d and d is the distance from the hyper-plane to the nearest positive or negative example. It turns out that the margin is inversely proportional to the absolute value of hyper-plane's normal |w|. This reduces the original problem to the following optimization problem:\nminimize|w| subject to yi(w\u22c5xi + b) - 1 \u2265 0, where (xi,yi) are the training examples and b is the bias term.\nA typical solution for the 2D case is shown in Figure 1. The solid line is the solution hyper-plane, the margin is the distance between the two parallel dashed lines, the circled negative and positive examples are called support vectors. Notice that the number of the support vectors is usually small comparing to the size of the training set. It is also should be quite obvious that the solution hyper-plane is defined only by the support vectors, that is if we remove all other training points, the hyper-plane will not be affected.\nThe linearly non-separable case is handled via introduction of slack variables \u03bei to penalize training errors. Then the problem can be rewritten as follows:\nminimize|w|2/2 + C(\u03a3i\u03bei)k subject to:\n0 1for ,1\n1for ,1\n\u2265 \u2212=+\u2212\u2264+\u22c5\n=\u2212\u2265+\u22c5\ni\niii\niii ybxw ybxw\n\u03be \u03be\n\u03be\nwhere C is a parameter to be chosen by the user. Large values of C correspond to stronger penalties for errors. Notice that for an error to occur, the corresponding \u03bei must exceed unity, so \u03a3i\u03bei is an upper bound on the number of training errors.\nTo solve the above optimization problem, one rewrites it in terms of the positive Lagrange multipliers, \u03b1i, one for each constraint. This results in the following Lagrangian:\n\u2211 \u2211 = =\n++\u22c5\u2212\u2261 l\ni\nl\ni iiiiP bxwywL 1 1 2 )( 2 1 \u03b1\u03b1\nWe must now minimize LP with respect to w, b, and simultaneously require that the derivatives of LP with respect to all \u03b1i vanish, all subject to 0 \u2264 \u03b1i \u2264 C. Since the above is a convex problem, it is possible (and more convenient) to solve its dual problem involving a different Lagrangian:\n)( 2 1 1,1 \u2211\u2211 == \u22c5\u2212\u2261 l ji jijiji l i iD xxyyL \u03b1\u03b1\u03b1\nProblem now is to\nmaximize LD, subject to 0 \u2264 \u03b1i \u2264 C, \u03a3i\u03b1i yi = 0. The solution is then given by\nw = \u03a3i\u03b1i yixi. The free coefficient b can be found from\n\u03b1i(yi(w\u22c5xi + b) - 1) = 0,\nfor any i such that \u03b1i is not zero."}, {"heading": "Non-linear SVM", "text": "The treatment of the most interesting, non-linear, case is based on the idea of injecting the data points into a higher-dimensional Hilbert space via some non-linear mapping, and using the linear support vector algorithm there to separate the training examples. This makes use of the fact that the data appears in the training problem only in the form of dot products.\nSuppose there was a map \u03a6: Rd \u2192 H, from our 'data space' into a higher-dimensional Hilbert space H. If there were a function evaluating dot products in H inexpensively, K(xi,xj) = \u03a6(xi)\u22c5\u03a6(xj), then we could train our machine in H and would never need an explicit form of \u03a6.\nIn our algorithm for linear SVM, we substitute all dot products xi\u22c5xj with K(xi,xj), the kernel function, and we will end up with a machine that does a linear separation but in a different Hilbert space. Our classifier then will be concerned with the sign of f(x), where\n\u2211\u2211 ==\n+=+\u03a6\u22c5\u03a6= ss N\ni iii\nN\ni iii bxsKybxsyxf 11 ),()()()( \u03b1\u03b1\nsi are the support vectors, NS is the number of support vectors. Note: the existence of a kernel function and an appropriate Hilbert space is problem-dependent and has to be established for each new problem using the theorems that Burges sites in his tutorial.\nHere are some kernel functions commonly used for pattern recognition problems: pyxyxK )1(),( +\u22c5= results in a classifier that is a polynomial of degree p\n2\n2\n2),( \u03c3 yx eyxK \u2212 \u2212 =\ngives a Gaussian Radial Basis Function classifier\n)tanh(),( \u03b4\u03ba \u2212\u22c5= yxyxK results in a two-layer sigmoidal neural network"}, {"heading": "SRM by SVM", "text": "In the framework of the Structural Risk Minimization (SRM) one is given a series of nested function families whose VC dimensions satisfy SRM requirements. The support vector algorithm chooses an optimal classifier from each family of classifiers given the training set. The next step would then be to identify the SVM with the lowest upper bound for the actual risk.\nThis process can be illustrated using the SV algorithm with the polynomial kernel function. Let Hi be the space of all polynomials of degree not greater than i. It should be quite obvious that Hi \u2282 Hk and hi < hk, for i < k. For each Hi, apply the support vector algorithm with the polynomial kernel function letting p = i. This will train an SVM for each i from 1 up to some n. Out of these n machines, identify one that minimizes the upper bound on the actual risk. Call this SVM the optimal for the task at hand (according to Burges)."}, {"heading": "Applications", "text": "In this section, we review results of two SVM applications. One deals with the face detection in black-and-white images. The other came along as a test for comparing two classifiers, and it deals with recognition of handwritten digits. We also discuss a possible symbiosis of SVM with decision trees."}, {"heading": "Face Detection", "text": "Edgar Osuna et al [3] have shown how to use a support vector machine for detecting vertically oriented and unoccluded frontal views of human faces in grey level images. They claim that their system handles faces over a wide range of scales and works under different lighting conditions, even with moderately strong shadows.\nThey train a SVM using a database of face and non-face pixel patterns. The training phase uses an iterative decomposition algorithm. The system detects faces by exhaustively scanning an image for face-like patterns at many possible scales, by dividing the original image into overlapping sub-images and classifying them using the trained SVM to determine the appropriate class (face/non-face). Multiple scales are handled by examining windows taken from scaled versions of the original image.\nThe system has been tested on two sets of images: set A containing 313 high-quality images with one face per image, and set B containing 23 images of mixed quality with a total of 155 faces. Set A involved 4,669,960 pattern windows, while set B 5,383,682. The reported results were as follows:\ntest set A B detect rate 97.1% 74.2% false alarms 4 20"}, {"heading": "Handwritten Digit Recognition", "text": "Sch\u00f6lkopf at al [5] compared SVM with Gaussian kernel to Radial Basis Function Classifiers. For one of the tests, they used US Postal Service Database of 9300 handwritten digits (7300 training, 2000 for testing.) An SV algorithm with standard quadratic programming techniques (conjugate gradient descent) has been used. For the 10- class classifiers, the reported results were as follows:\ntest error rate clustered centers 6.7% SV centers 4.9% SVM 4.2%"}, {"heading": "SVM and Decision Trees", "text": "Bennett and Blue have attempted to develop a method for generating logically simple decision trees with multivariate linear or non-linear decisions [2]. The key idea here was to trade the complexity of the tree to the complexity of the decisions made. This kind of compromise is achieved by using a simple SVM for each decision in the tree.\nA simple example of such a classifier is shown in Figure 2."}, {"heading": "Experiments", "text": "In this section we present results of experimental runs of SVM classifiers first on a synthetic dataset, and then on\na real-world datasets."}, {"heading": "Synthetic Problems", "text": "This section describes a number of SVMs that classify data in 2D. All data is synthetic and serves the purpose of demonstrating the SV algorithm's ability to find an optimal hyper surface for the given training data.\nThe experiment is run as follows. Three different SVMs (Linear, Poly and RBF) are being tested on three different training sets (Linearly separable, Linearly non-separable and Hard-toseparate). Linear SVM uses a linear kernel, Poly uses polynomial kernel of degree 3, and RBF is using an RBF kernel with sigma equal to 10.\nAs Table 1 shows, the linearly separable case is handled easily by all SVMs. The linearly non-separable case is handled well by Poly and RBF; Linear attempts to do the best it can. The hard-to-separate case is taken well by RBF only; Poly\nattempts but does not quite separate the data; and Linear is\ncompletely lost. This suggests that out of three kinds of kernels, RBF kernel tends to produce the most robust classifiers, but it is the most expensive one of the three. Poly is less expensive and performs reasonably well, but it is naturally limited by the degree of the polynomial used. Linear, the simplest and least expensive of all, inherits all limitations of the Poly because it is a particular case of Poly with degree 1."}, {"heading": "Real-world Problems", "text": "In this section, we consider two databases with real-world data. Both databases are taken from the UCI Machine Learning Repository. Both databases contain labeled examples falling in two classes, thus making the application of the SV algorithm straightforward."}, {"heading": "Breast Cancer Classifier", "text": "Here we use the Wisconsin Breast Cancer database that contains data on two kinds of cancers (benign and malignant). The class distribution is 65%/35%. Total number of records is 699. Each example has 9 attributes. Missing values are substituted with some numeric values outside the attribute range.\nA series of SVM classifiers is trained and tested using different kernel functions. Classifiers are trained on randomly chosen sets of examples of sizes from 100 to 500. Remarkable is the fact, that the straight linear classifier performs quite well having worst-case error rate of 3.9% using a training set of 100 elements (9 support vectors) and\nbest-case 3.14% using a training set of 400 (82 support vectors). Performance of the quadratic-kernel SVM is even better than the linear's. The cubic kernels cause the method to misbehave producing a singularity in the minimization problem. The RBF kernel SVMs have no error no the training data and are more accurate in the bestcase (error 1.57% on 500 element training set, 47 support vectors) but are worse than the linear classifier in the worst-case scenario (error 4.9%, 300 element training set, 31 support vectors). On average, however the RBF-kernel SVMs seems to be more robust that the linear SVM. Refer to the \"Breast Cancer Classifier runs\" in the appendix for the detailed summary."}, {"heading": "Mushroom Classifier", "text": "This database presents data on two kinds of mushrooms (edible and poisonous). The class distribution is 52%/48%. Total number of records is 8124. Each example has 22 attributes. Since all attributes are nominal, their values represented by character symbols are taken to be the ASCII codes of those characters.\nFor this data set, we need to use a robust RBF-kernel SVMs because any polynomial-kernel SVM (including linear) would degenerate in the training phase. As for the Breast Cancer Classifier, SVM classifiers here are trained on randomly chosen sets of examples of sizes from 100 to 500. As the sample run indicates, at first the test error rate is significant (4.7%) and grows for training sets of sizes 100 and 200. But then it sharply drops to 0.8% and continues dropping as the training set size increases. This suggests a superb generalization performance of the SVM. Refer to \"Mushroom Classifier runs\" in the appendix for more details."}, {"heading": "Conclusions", "text": "The objective of this project was to evaluate the Support Vector Machines as a tool for pattern recognition in the framework of statistical learning. The hypothesis under consideration was that Support Vector Machines exhibit an excellent generalization performance, and that they can be successfully applied to a wide range of pattern recognition problems.\nWe gave an introduction to the Structure Risk Minimization (SRM) principle and provided some background on SV Learning techniques using SRM. We implemented a Support Vector learning system in Matlab and ran a number of experiments using it. The experiments involved both synthetic and real-world data. The results of the experiments revealed to us that Support Vector Machines indeed poses the property of a superb generalization performance and, clearly, can be successfully used in solving various pattern classification tasks."}], "references": [{"title": "A tutorial on Support Vector Machines for Pattern Recognition", "author": ["C.J.C. Burges"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Support Vector Machines: Training and Application, C.B.C.L", "author": ["E.E. Osuna", "R. Freund", "F. Girosi"], "venue": "Paper No. 144,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Flexible Non-linear Approaches to Classification", "author": ["B.D. Ripley"], "venue": "ASI Proceedings,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Comparing Support Vector Machines with Gaussian Kernels to Radial Basis Function Classifiers", "author": ["B. Sch\u00f6lkopf"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "The Nature of Statistical Learning Theory, Springer-Verlag", "author": ["V. Vapnik"], "venue": "New York,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}], "referenceMentions": [{"referenceID": 1, "context": "Face Detection Edgar Osuna et al [3] have shown how to use a support vector machine for detecting vertically oriented and unoccluded frontal views of human faces in grey level images.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "Handwritten Digit Recognition Sch\u00f6lkopf at al [5] compared SVM with Gaussian kernel to Radial Basis Function Classifiers.", "startOffset": 46, "endOffset": 49}], "year": 2003, "abstractText": "The purpose of this report is in examining the generalization performance of Support Vector Machines (SVM) as a tool for pattern recognition and object classification. The work is motivated by the growing popularity of the method that is claimed to guarantee a good generalization performance for the task in hand. The method is implemented in MATLAB. SVMs based on various kernels are tested for classifying data from various domains.", "creator": "PScript5.dll Version 5.2"}}}