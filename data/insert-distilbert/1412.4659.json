{"id": "1412.4659", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Dec-2014", "title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions", "abstract": "we consider the problem class of recovering the sparsest vector in a subspace $ \\ mathcal { s } \\ subseteq \\ graph mathbb { r } ^ p $ with $ \\ mathrm { dim } ( \\ mathcal { s } ) = n & lt ; p $. this problem can be considered a homogeneous variant of the sparse recovery time problem, and finds applications in sparse dictionary learning, sparse pca, and conversely other problems in signal processing and machine learning. simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds $ 1 / \\ marginal sqrt { n } $. in contrast, we exhibit a certain relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is $ \\ omega ( 1 ) $. unrelated to our knowledge, this is the first practical algorithm to achieve this linear scaling. coincidentally this result assumes a planted sparse model, in which the target sparse vector instead is embedded in an otherwise random subspace. empirically, instead our proposed algorithm also succeeds in more challenging data models arising, e. g., from sparse dictionary learning.", "histories": [["v1", "Mon, 15 Dec 2014 16:27:29 GMT  (787kb,D)", "http://arxiv.org/abs/1412.4659v1", "38 pages, 4 figures. Extended abstract appears in Advances in Neural Information Processing Systems (NIPS), 2014"], ["v2", "Tue, 24 Nov 2015 03:23:33 GMT  (390kb,D)", "http://arxiv.org/abs/1412.4659v2", "Submitted to IEEE Trans. Information Theory"], ["v3", "Wed, 20 Jul 2016 00:54:41 GMT  (750kb,D)", "http://arxiv.org/abs/1412.4659v3", "Accepted by IEEE Trans. Information Theory. The paper has been revised by the reviewers' comments. The proofs have been streamlined"]], "COMMENTS": "38 pages, 4 figures. Extended abstract appears in Advances in Neural Information Processing Systems (NIPS), 2014", "reviews": [], "SUBJECTS": "cs.IT cs.CV cs.LG math.IT math.OC stat.ML", "authors": ["qing qu", "ju sun", "john wright"], "accepted": true, "id": "1412.4659"}, "pdf": {"name": "1412.4659.pdf", "metadata": {"source": "CRF", "title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions", "authors": ["Qing Qu", "Ju Sun"], "emails": ["jw2966}@columbia.edu"], "sections": [{"heading": null, "text": "\u221a n. In contrast, we exhibit a relatively simple\nnonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is \u2126(1). To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning."}, {"heading": "1 Introduction", "text": "Suppose we are given a linear subspace S of a high-dimensional space Rp, which contains a sparse vector x0 6= 0. Given arbitrary basis of S, can we efficiently recover x0? Equivalently, provided a matrix A \u2208 R(p\u2212n)\u00d7p, can we efficiently find a nonzero sparse vector x such that Ax = 0? In the language of sparse approximation, can we solve\nmin x \u2016x\u20160 s.t. Ax = 0, x 6= 0 ? (1.1)\nVariants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].\nHowever, in contrast to the standard sparse regression problem (Ax = b, b 6= 0), for which convex relaxations perform nearly optimally for broad classes of designs A [CT05, Don06], the computational properties of problem (1.1) are not nearly as well understood. It has been known for several decades that the basic formulation\nmin x \u2016x\u20160 , s.t. x \u2208 S \\ {0}, (1.2)\nis NP-hard [CP86]. However, it is only recently that efficient computational surrogates with nontrivial recovery guarantees have been discovered for some practical cases of interest. In the context of sparse dictionary learning, Spielman et al. [SWW12] introduced a relaxation which replaces the nonconvex problem (1.2) with a sequence of linear programs:\n`1/`\u221e Relaxation: min x \u2016x\u20161 , s.t. xi = 1, x \u2208 S, 1 \u2264 i \u2264 p, (1.3)\nar X\niv :1\n41 2.\n46 59\nv1 [\ncs .I\nT ]\n1 5\nD ec\n2 01\nand proved that when S is generated as a span of n random sparse vectors, with high probability the relaxation recovers these vectors, provided the probability of an entry being nonzero is at most \u03b8 \u2208 O (1/\u221an). In a planted sparse model, in which S consists of a single sparse vector x0 embedded in a \u201cgeneric\u201d subspace, Hand and Demanent proved that (1.3) also correctly recovers x0, provided the fraction of nonzeros in x0 scales as \u03b8 \u2208 O (1/\u221an) [HD13]. Unfortunately, the results of [SWW12, HD13] are essentially sharp: when \u03b8 substantially exceeds 1/ \u221a n, in both models the relaxation (1.3) provably breaks down. Moreover, the most natural semidefinite programming (SDP) relaxation of (1.1),\nmin X \u2016X\u20161 , s.t.\n\u2329 A>A,X \u232a = 0, trace[X] = 1, X 0. (1.4)\nalso breaks down at exactly the same threshold of \u03b8 \u223c 1/\u221an.1 One might naturally conjecture that this 1/ \u221a n threshold is simply an intrinsic price we must pay for having an efficient algorithm, even in these random models. Some evidence towards this conjecture might be borrowed from the superficial similarity of (1.2)-(1.4) and sparse PCA [ZHT06]. In sparse PCA, there is a substantial gap between what can be achieved with efficient algorithms and the information theoretic optimum [BR13]. Is this also the case for recovering a sparse vector in a subspace? Is \u03b8 \u2208 O (1/\u221an) simply the best we can do with efficient, guaranteed algorithms?\nRemarkably, this is not the case. Recently, Barak et al. introduced a new rounding technique for sum-ofsquares relaxations, and showed that the sparse vector x0 in the planted sparse model can be recovered when p \u2265 \u2126 ( n2 ) and \u03b8 = \u2126(1) [BKS13]. It is perhaps surprising that this is possible at all with a polynomial time algorithm. Unfortunately, the runtime of this approach is a high-degree polynomial in p, and so for machine learning problems in which p is either a feature dimension or sample size, this algorithm is of theoretical interest only. However, it raises an interesting algorithmic question: Is there a practical algorithm that provably recovers a sparse vector with \u03b8 1/\u221an portion of nonzeros from a generic subspace S?\nIn this paper, we address this problem, under the following hypotheses: we assume the planted sparse model, in which a target sparse vector x0 is embedded in an otherwise random n-dimensional subspace ofRp. We allow x0 to have up to \u03b80p nonzero entries, where \u03b80 \u2208 (0, 1) is a constant. We provide a relatively simple algorithm which, with very high probability, exactly recovers x0, provided that p \u2265 \u2126 ( n4 log n ) , where the comparison with existing results is shown in Table 1. Our algorithm is based on alternating directions, with two special twists. First, we introduce a special data driven initialization, which seems to be important for achieving \u03b8 = \u2126(1). Second, our theoretical results require a second, linear programming based rounding phase, which is similar to [SWW12]. Our core algorithm has very simple iterations, of linear complexity in the size of the data, and hence should be scalable to moderate-to-large scale problems.\nIn addition to enjoying theoretical guarantees in a regime (\u03b8 = \u2126(1)) that is out of the reach of previous practical algorithms, it performs well in simulations \u2013 succeeding with p \u2265 \u2126 (n log n). It also performs well empirically on more challenging data models, such as the dictionary learning model, in which the subspace of interest contains not one, but n target sparse vectors. Breaking the O(1/ \u221a n) sparsity barrier with a practical algorithm is an important open problem in the nascent literature on algorithmic guarantees for dictionary learning [AGM13, ABGM14, AAN13, AAJ+13]. We are optimistic that the techniques introduced here will be applicable in this direction.2\n1This breakdown behavior is again in sharp contrast to the standard sparse approximation problem (withb 6= 0), inwhich it is possible to handle very large fractions of nonzeros (say, \u03b8 = \u2126(1/ logn), or even \u03b8 = \u2126(1)) using a very simple `1 relaxation [CT05, Don06]\n2In work currently in preparation [SQW14], we show that in the dictionary learning problem, efficient algorithms based on nonconvex"}, {"heading": "2 Problem Formulation and Global Optimality", "text": "We study the problem of recovering a sparse vector x0 6= 0 (up to scale), which is an element of a known subspace S \u2282 Rp of dimension n, provided an arbitrary orthonormal basis Y \u2208 Rp\u00d7n for S. Our starting point is the nonconvex formulation (1.2). Both the objective and constraint are nonconvex, and hence not easy to optimize over. We relax (1.2) by replacing the `0 norm with the `1 norm. For the constraint x 6= 0, which is necessary to avoid a trivial solution, we force x to live on the unit sphere \u2016x\u20162 = 1, giving\nmin x \u2016x\u20161 , s.t. x \u2208 S, \u2016x\u20162 = 1. (2.1)\nThis formulation is still nonconvex, and so we should not expect to obtain an efficient algorithm that can solve it globally for general inputs S. Nevertheless, the geometry of the sphere is benign enough that for well-structured inputs it actually will be possible to give algorithms that find the global optimum.\nThe formulation (2.1) can be contrasted with (1.3), in which effectively we optimize the `1 norm subject to the constraint \u2016x\u2016\u221e = 1. Because \u2016\u00b7\u2016\u221e is polyhedral, that formulation immediately yields a sequence of linear programs. This is very convenient for computation and analysis, but suffers from the aforementioned breakdown behavior around \u2016x0\u20160 \u223c p/ \u221a n. In contrast, the sphere \u2016x\u20162 = 1 is a more complicated geometric constraint, but will allow much larger numbers of nonzeros in x0. Indeed, if we consider the global optimizer of a reformulation of (2.1):\nmin q\u2208Rn\n\u2016Yq\u20161 , s.t. \u2016q\u20162 = 1, (2.2)\nwhere Y is any orthonormal basis for S, we have strong recovery guarantees as follows:\nTheorem 2.1 (`1/`2 recovery, planted sparse model). There exists a constant \u03b80 > 0 such that if the subspace S follows the planted sparse model S = span (x0,g1, . . . ,gn\u22121) \u2282 Rp, (2.3) where gi \u223ci.i.d. N (0, 1pI), and x0 \u223ci.i.d. 1\u221a\u03b8pBer(\u03b8) are all mutually independent and 1/ \u221a n < \u03b8 < \u03b80, then optimum q? to (2.2), for any orthonormal basis Y of S, produces Yq? = \u03bex0 for some \u03be 6= 0 with high probability, provided p \u2265 \u2126 (n log n). 3\nHence, ifwe could find the global optimizer of (2.2), we would be able to recover x0 whose number of nonzero entries is quite large \u2013 even linear in the dimension p (\u03b8 = \u2126(1)). On the other hand, it is not obvious that this should be possible: (2.2) is nonconvex. In the next section, we will describe a simple heuristic algorithm for (a near approximation of) the `1/`2 problem (2.2), which guarantees to find a stationary point. More surprisingly, we will then prove that for a class of random problem instances, this algorithm, plus an auxiliary rounding technique, actually recovers the global optimum \u2013 the target sparse vector x0. The proof requires a detailed probabilistic analysis, which is sketched in Section 4.2.\nBefore continuing, it is worth noting that the formulation (2.1) is in no way novel \u2013 see, e.g., the work of [ZP01] in blind source separation for precedent. However, our algorithms and subsequent analysis are novel."}, {"heading": "3 Algorithm based on Alternating Direction Method (ADM)", "text": "To develop an algorithm for solving (2.2), it is useful to consider a slight relaxation of (2.2), in which we introduce an auxiliary variable x \u2248 Yq:\nmin q,x\n1 2 \u2016Yq\u2212 x\u201622 + \u03bb \u2016x\u20161 , s.t. \u2016q\u20162 = 1, (3.1)\noptimization also produce global solutions, even when \u03b8 = \u2126(1). 3Note that this version is much stronger and more practical than that appearing in the conference version [QSW14].\nHere, \u03bb > 0 is a penalty parameter. It is not difficult to see that this problem is equivalent to minimizing the HuberM-estimator over Yq. This relaxation makes it possible to apply the alternating direction method to this problem. This method starts from some initial point q(0), alternates between optimizing with respect to x and optimizing with respect to q:\nx(k+1) = arg min x\n1\n2 \u2225\u2225\u2225Yq(k) \u2212 x\u2225\u2225\u22252 2 + \u03bb \u2016x\u20161 , (3.2)\nq(k+1) = arg min q\n1\n2 \u2225\u2225\u2225Yq\u2212 x(k+1)\u2225\u2225\u22252 2 s.t. \u2016q\u20162 = 1. (3.3)\nBoth (3.2) and (3.3) have simple closed form solutions:\nx(k+1) = S\u03bb[Yq (k)], q(k+1) = Y>x(k+1)\u2225\u2225Y>x(k+1)\u2225\u2225 2 , (3.4)\nwhere S\u03bb [x] = sign(x) max {|x| \u2212 \u03bb, 0} is the soft-thresholding operator. The proposed ADM algorithm is summarized in Algorithm 1.\nAlgorithm 1 Nonconvex ADM for solving (3.1)\nInput: A matrix Y \u2208 Rp\u00d7n with Y>Y = I, initialization q(0), threshold parameter \u03bb > 0. Output: The recovered sparse vector x\u03020 = Yq(k) 1: for k = 0, . . . , O ( n4 log n ) do\n2: x(k+1) = S\u03bb[Yq (k)], 3: q(k+1) = Y >x(k+1)\n\u2016Y>x(k+1)\u2016 2 , 4: end for\nThe algorithm is simple to state and easy to implement. However, if our goal is to recover the sparsest vector x0, some additional tricks are needed.\nInitialization. Because the problem (2.2) is nonconvex, an arbitrary or random initialization is unlikely to produce a global minimizer.4 Therefore, good initializations are critical for the proposed ADM algorithm to succeed. For this purpose, we suggest to use every normalized row of Y as initializations for q, and solve a sequence of p nonconvex programs (2.2) by the ADM algorithm.\nTo get an intuition ofwhy our initializationworks, recall the planted sparsemodel: S = span(x0,g1, . . . ,gn\u22121). Write Z = [x0 | g1 | \u00b7 \u00b7 \u00b7 | gn\u22121] \u2208 Rp\u00d7n. Suppose we take a row zi of Z, in which x0(i) is nonzero, then x0(i) = \u0398 ( 1/ \u221a \u03b8p ) . Meanwhile, the entries of g1(i), . . .gn\u22121(i) are allN (0, 1/p), and so have size about 1/\u221ap. Hence, when \u03b8 is not too large, x0(i) will be somewhat bigger than most of the other entries in zi. Put another way, zi is biased towards the first standard basis vector e1.\nNow, under our probabilistic assumptions, Z is very well conditioned: Z>Z \u2248 I.5 Using, e.g., GramSchmidt, we can find a basis Y for S of the form\nY = ZR, (3.5)\nwhere R is upper triangular, and R is itself well-conditioned: R \u2248 I. Since the i-th row of Z is biased in the direction of e1 and R is well-conditioned, the i-th row yi is also biased in the direction of e1. Moreover, we know that the global optimizer q? should satisfy Yq? = x0. Since Ze1 = x0, we have q? = R\u22121e1 \u2248 e1. Here, the approximation comes from R \u2248 I. Hence, for this particular choice of Y, described in (3.5), the i-th row is biased in the direction of the global optimizer.\n4More precisely, in our models, random initialization doeswork, but only when the subspace dimension n is extremely low compared to the ambient dimension p.\n5This is the common heuristic that \u201ctall random matrices are well conditioned\u201d [Ver10].\nWhat if we are handed some other basis Y = YU, where U is an orthogonal matrix? Suppose q? is a global optimizer to (2.2) with input matrix Y, then it is easy to check that, with input matrix Y, U>q? is also a global optimizer to (2.2), which implies that our initialization is invariant to any rotation of the basis. Hence, even if we are handed an arbitrary basis for S, the i-th row is still biased in the direction of the global optimizer.\nRounding. Let q denote the output of Algorithm 1. We will prove that with our particular initialization and an appropriate choice of \u03bb, the solution of our ADM algorithm falls within a certain radius of the globally optimal solution q? to (2.2). To recover q?, or equivalently to recover the sparse vector x0 = \u03beYq? for some \u03be 6= 0, we solve the linear program\nmin q \u2016Yq\u20161 s.t. \u3008r,q\u3009 = 1 (3.6)\nwith r = q. We will prove that if q is close enough to q?, then (3.6) exactly recovers q?, and hence x0."}, {"heading": "4 Analysis", "text": ""}, {"heading": "4.1 Main Results", "text": "In this section, we describe our main theoretical result, which shows that with high probability, the algorithm described in the previous section succeeds.\nTheorem 4.1. Suppose that S satisfies the planted sparse model, and let the columns of Y be an arbitrary orthonormal basis for the subspace S. Let y1, . . . ,yp \u2208 Rn denote the (transposes of) the rows of Y. Apply Algorithm 1 with \u03bb = 1/ \u221a p, using initializations q(0) = y1, . . . ,yp, to produce outputs q1, . . . ,qp. Solve the linear program (3.6) with r = q1, . . . ,qp, to produce q\u03021, . . . , q\u0302p. Set i? \u2208 arg mini \u2016Yq\u0302i\u20160. Then\nYq\u0302i? = \u03b3x0, (4.1)\nfor some \u03b3 6= 0 with overwhelming probability, provided\nexp (n/2) /2 \u2265 p \u2265 Cn4 log n, and 1\u221a n \u2264 \u03b8 \u2264 \u03b80. (4.2)\nHere, C and \u03b80 > 0 are universal constants.\nWe can see that the result in Theorem 4.1 is suboptimal compared to the global optimality result in Theorem 2.1 and Barak et al.\u2019s result [BKS13] in sampling complexity. For successful recovery, we require p \u2265 \u2126 ( n4 log n ) , while the global optimality and Barak et al. demand p \u2265 \u2126 (n log n) and p \u2265 \u2126 ( n2 ) , respectively. Aside from possible deficiencies in our current analysis, compared to Barak et al., we believe this is still the first practical and efficient method which is guaranteed to achieve \u03b8 \u223c O(1) rate. The lower bound on \u03b8 in Theorem 4.1 is mostly for convenience in the proof; in fact, the LP rounding stage of our algorithm already succeeds with high probability when \u03b8 \u2208 O (1/\u221an)."}, {"heading": "4.2 A Sketch of Analysis", "text": "The proof of our main result requires rather detailed technical analysis of the iteration-by-iteration properties of Algorithm 1. In this section, as illustrated in Fig. 1, we briefly sketch the main ideas. Detailed proofs are deferred to the appendices.\nAs noted in Section 3, the ADM algorithm is invariant to change of basis. So, we can assume without loss of generality that we are working with the particular basis Y = ZR defined in that section. In order to further streamline the presentation, we are going to sketch the proof under the assumption that\nY = [x0 | g1 | \u00b7 \u00b7 \u00b7 | gn\u22121], (4.3)\nrather than the orthogonalized version Y. When p is large Y is already nearly orthogonal, and hence Y is very close to Y. In fact, in our proof, we simply carry through the argument for Y, and then note that Y and Y are close enough that all steps of the proof still hold with Y replaced by Y. With that noted, let y1, . . . ,yp \u2208 Rn denote the transposes of the rows of Y, and note that these are independent random vectors. From (3.4), we can see one step of the ADM algorithm takes the form:\nq(k+1) =\n1 p \u2211p i=1 y iS\u03bb [( q(k) )> yi ]\n\u2225\u2225\u2225 1p\u2211pi=1 yiS\u03bb [(q(k))> yi]\u2225\u2225\u2225 2 . (4.4)\nThis is a very favorable form for analysis: if q(k) is viewed as fixed, the term in the numerator is a sum of p independent random vectors. To this end, we define a vector valued random process Q(q) on q \u2208 Sn\u22121, via\nQ(q) = 1\np p\u2211 i=1 yiS\u03bb[q >yi]. (4.5)\nWe study the behavior of the iteration (4.4) through the random process Q(q). We wish to show that with overwhelming probability in our choice of Y, q(k) converges to some small neighborhood of \u00b1e1, so that the ADM algorithm plus the LP rounding (described in Section 3) successfully retrieves the sparse vector x0 = Ye1. Thus, we hope that in general, Q(q) is more concentrated on the first coordinate than\nq. Let us partition the vector q as q = [ q1 q2 ] , with q1 \u2208 R and q2 \u2208 Rn\u22121, and correspondingly partition\nQ(q) = [ Q1(q) Q2(q) ] , where\nQ1(q) = 1\np p\u2211 i=1 x0iS\u03bb [ q>yi ] and Q2(q) = 1 p p\u2211 i=1 giS\u03bb [ q>yi ] . (4.6)\nThe inner product of Q(q)/ \u2016Q(q)\u20162 and e1 is strictly larger than the inner product of q and e1 if and only if\n|Q1(q)| |q1| > \u2016Q2(q)\u20162 \u2016q2\u20162 . (4.7)\nIn the appendix, we show that with overwhelming probability, this inequality holds uniformly over a significant portion of the sphere, so the algorithm moves in the correct direction. To complete the proof of Theorem 4.1, we combine the following observations, provided exp (n/2) /2 \u2265 p \u2265 \u2126 ( n4 log n ) :\n1. Good initializers. With overwhelming probability, at least one of the initializers q(0) satisfies |q(0)1 | > 1\n4 \u221a \u03b8n .\n2. Uniform progress away from the equator. With overwhelming probability, for every q \u2208 Sn\u22121 such that 1\n4 \u221a \u03b8n \u2264 |q1| \u2264 3\n\u221a \u03b8, the bound\n|Q1(q)| |q1| \u2212 \u2016Q2(q)\u20162\u2016q\u20162 > C \u03b82np (4.8)\nholds, for some numerical constant C > 0. This implies that if at any iteration k of the algorithm, |q(k)1 | > 14\u221a\u03b8n , the algorithm will eventually obtain a point q (k\u2032), k\u2032 > k, for which |q(k \u2032) 1 | > 3 \u221a \u03b8, if sufficiently many iterations are allowed.\n3. No jumps away from the caps. With overwhelming probability,\n|Q1(q)|\u221a |Q1(q)2|+ \u2016Q2(q)\u201622 \u2265 2 \u221a \u03b8 (4.9)\nfor all q \u2208 Sn\u22121 with |q1| > 3 \u221a \u03b8.\n4. Location of stationary points. The above steps imply that, with overwhelming probability, Algorithm 1 fed with the proposed initialization scheme produces at least one stopping point q \u2208 Sn\u22121 satisfying |q1| \u2265 2 \u221a \u03b8. 5. Rounding succeeds when |r1| > 2 \u221a \u03b8. With overwhelming probability, the linear programming based\nrounding (3.6) will produce \u00b1x0, up to scale, whenever it is provided with an input r whose first coordinate has magnitude at least 2 \u221a \u03b8.\nTaken together, these claims imply that from at least one of the initializers q(0), the ADM algorithm will produce an output q which is accurate enough for LP rounding to exactly return x0, up to scale. As x0 is the sparsest nonzero vector in the subspace S with overwhelming probability, it will be selected as Yqi? , and hence produced by the algorithm."}, {"heading": "5 Experimental Results", "text": "In this section, we show the performance of the proposed ADM algorithm on both synthetic and real datasets. On the synthetic dataset, we show the phase transition of our algorithm on both the planted sparse vector and dictionary learning models; for the real dataset, we demonstrate how seeking sparse vectors can help discover interesting patterns."}, {"heading": "5.1 Phase Transition on Synthetic Data", "text": "For the planted sparse model, for each pair of (k, p), we generate the n dimensional subspace S \u2208 Rp by a k sparse vector x0 with nonzero entries equal to 1 and a random Gaussian matrix G \u2208 Rp\u00d7(n\u22121) with Gij \u223ci.i.d. N (0, 1/p), so that one basis Y of the subspace S can be constructed by Y = GS ([x0,G]) U, where GS (\u00b7) denotes the Gram-Schmidt orthonormalization operator and U \u2208 Rn\u00d7n is an arbitrary orthogonal matrix. We fix the relationship between n and p as p = 5n log n, and set the regularization parameter in (3.1) as \u03bb = 1/\u221ap. We use all the normalized rows of Y as initializations of q for the proposed ADM\nalgorithm, and run every program for 5000 iterations. We determine the recovery to be successful whenever \u2016x0/ \u2016x0\u20162 \u2212Yq\u20162 \u2264 \u03b5 for at least one of the p programs, where \u03b5 = 10\u22123. For each pair of (k, p), we repeat the simulation for five times.\nSecond, we consider the same dictionary learning model as in [SWW12]. Specifically, the observation is assumed to be Y = A0X0, where A0 is a square, invertible matrix, and X0 a n\u00d7 p sparse matrix. Since A0 is invertible, the row space ofY is the same as that ofX0. For each pair of (k, n), we generateX0 = [x1, \u00b7 \u00b7 \u00b7 ,xn]>, where each vector xi \u2208 Rp is k-sparse with every nonzero entry following i.i.d. Gaussian distribution, and construct the observation by Y> = GS ( X>0 ) U>.We repeat the same experiment as for the planted sparse model presented above. The only difference is that here we determine the recovery to be successful as long as one sparse row of X0 is recovered by one of those p programs.\nFigure 2 shows the phase transition between the sparsity level k and p for both models. It seems clear for both problems our algorithm can work well into (even beyond) the linear sparsity regime whenever p \u223c n log n. Hence for the planted sparse model, to close the gap between our algorithm and practice is one future direction. Also, how to extend our analysis for dictionary learning is another interesting direction."}, {"heading": "5.2 Exploratory Experiments on Faces", "text": "It is well known in computer vision that appearance of convex objects only subject to illumination changes leads to image collection that can be well approximated by low-dimensional space in raw-pixel space [BJ03]. We will play with face subspaces here. First, we extract face images of one person (65 images) under different illumination conditions. Thenwe apply robust principal component analysis [CLMW11] to the data and get a low dimensional subspace of dimension 10, i.e., the basis Y \u2208 R32256\u00d710. We apply the ADM algorithm to find the sparsest element in such a subspace, by randomly selecting 10% rows as initializations for q. We judge the sparsity in a `1/`2 sense, that is, the sparsest vector x\u03020 = Yq? should produce the smallest \u2016Yq\u20161 / \u2016Yq\u20162 among all results. Once some sparse vectors are found, we project the subspace onto orthogonal complement of the sparse vectors already found, and continue the seeking process in the projected subspace. Figure 3 shows the first four sparse vectors we get from the data. We can see they correspond well to different extreme illumination conditions.\nSecond, we manually select ten different persons\u2019 faces under the normal lighting condition. Again, the dimension of the subspace is 10 and Y \u2208 R32256\u00d710. We repeat the same experiment as stated above. Figure 4 shows four sparse vectors we get from the data. Interestingly, the sparse vectors roughly correspond to differences of face images concentrated around facial parts that different people tend to differ from each other, e.g., eye bows, forehead hair, nose, etc.\nIn sum, our algorithm seems to find useful sparse vectors for potential applications, like peculiarity discovery in first setting, and locating differences in second setting. Nevertheless, the main goal of this experiment is to invite readers to think about similar pattern discovery problems that might be cast as the problem of seeking sparse vectors in a subspace. The experiment also demonstrates in a concrete way the practicality of our algorithm, both in handling data sets of realistic size and in producing meaningful results even beyond the (idealized) planted sparse model that we adopt for analysis."}, {"heading": "6 Discussion", "text": "The random models we assume for the subspace can be easily extended to other randommodels, particularly for dictionary learning. Moreover we believe the algorithm paradigm works far beyond the idealized models, as our preliminary experiments on face data have clearly shown. For the particular planted sparse model, the performance gap in terms of (p, n, \u03b8) between the empirical simulation and our result is likely due to analysis itself. Advanced techniques to bound the empirical process, such as decoupling [DlPG99] techniques, can be deployed in place of our crude union bound to cover all iterates. On the application side, the potential of seeking sparse/structured element in a subspace seems largely unexplored, despite the cases we mentioned at the start. We hope this work can invite more application ideas.\nThis paper is part of a recent surge of research into provable and practical nonconvex approaches to estimating various types of low-dimensional structures, often in large-scale settings [CLS14, JNS13, Har13, NJS13, YCS13]. The dominant approach is to start with a clever, problem-specific initialization, and then perform a local analysis of the subsequent iterates. Our forthcoming work [SQW14] on dictionary learning takes a more geometric approach, and proves global recovery via efficient algorithms, with arbitrary initialization. The approach developed there may be applicable to the planted sparse model studied here, as well as to many other interesting nonconvex problems."}, {"heading": "Acknowledgement", "text": "JS thanks the Wei Family Private Foundation for their generous support. We thank Cun Mu, IEOR Department of Columbia University, for helpful discussion and input regarding this work. This work was partially supported by grants ONR N00014-13-1-0492, NSF 1343282, and funding from the Moore and Sloan Foundations."}, {"heading": "A Technical Tools and Preliminaries", "text": "Lemma A.1. Let \u03c8(x) and \u03a8(x) to denote the probability density function (pdf) and the cumulative distribution function (cdf) for the standard normal distribution:\n(Standard Normal pdf) \u03c8(x) = 1\u221a 2\u03c0 exp\n{ \u2212x 2\n2\n} (A.1)\n(Standard Normal cdf) \u03a8(x) = 1\u221a 2\u03c0 \u222b x \u2212\u221e exp { \u2212 t 2 2 } dt, (A.2)\nSuppose a random variable X \u223c N (0, \u03c32), with the pdf f\u03c3(x) = 1\u03c3\u03c8 ( x \u03c3 ) , then for any t2 > t1 we have\u222b t2\nt1\nf\u03c3(x)dx = \u03a8 ( t2 \u03c3 ) \u2212\u03a8 ( t1 \u03c3 ) , (A.3)\u222b t2\nt1\nxf\u03c3(x)dx = \u2212\u03c3 [ \u03c8 ( t2 \u03c3 ) \u2212 \u03c8 ( t1 \u03c3 )] , (A.4)\u222b t2\nt1\nx2f\u03c3(x)dx = \u03c3 2 [ \u03a8 ( t2 \u03c3 ) \u2212\u03a8 ( t1 \u03c3 )] \u2212 \u03c3 [ t2\u03c8 ( t2 \u03c3 ) \u2212 t1\u03c8 ( t1 \u03c3 )] . (A.5)\nLemma A.2 (Taylor Expansion of Standard Gaussian cdf and pdf ). Assume \u03c8(x) and \u03a8(x) be defined as above. There exists some universal constant C\u03c8 > 0 such that\n|\u03c8(x)\u2212 [\u03c8(x0)\u2212 x0\u03c8 (x0) (x\u2212 x0)]| \u2264 C\u03c8(x\u2212 x0)2, (A.6) |\u03a8(x)\u2212 [\u03a8(x0) + \u03c8(x0)(x\u2212 x0)]| \u2264 C\u03c8(x\u2212 x0)2. (A.7)\nLemma A.3 (Matrix Induced Norms). For any matrix A \u2208 Rp\u00d7n, the induced matrix norm from `p \u2192 `q is defined as\n\u2016A\u2016`p\u2192`q . = sup \u2016x\u2016p=1 \u2016Ax\u2016q . (A.8)\nIn particular, we have\n\u2016A\u2016`2\u2192`1 = sup \u2016x\u20162=1 p\u2211 k=1 \u2223\u2223a>k x\u2223\u2223 , \u2016A\u2016`2\u2192`\u221e = max 1\u2264k\u2264p \u2225\u2225ak\u2225\u2225 2 , (A.9) \u2016AB\u2016`p\u2192`r \u2264 \u2016A\u2016`q\u2192`r \u2016B\u2016`p\u2192`q , (A.10)\nand A and B are any matrices of compatible size.\nLemma A.4 (Moments of the Gaussian Random Variable). If X \u223c N ( 0, \u03c32X ) , then it holds for all integerm \u2265 1 that\nE [|X|m] = \u03c3mX (m\u2212 1)!! [\u221a 2\n\u03c0 1m=2k+1 + 1m=2k\n] \u2264 \u03c3mX (m\u2212 1)!!, k = bm/2c. (A.11)\nLemma A.5 (Moments of the \u03c7 Random Variable). If X \u223c \u03c7 (n), i.e., X \u2261d \u2016x\u201626 for x \u223c N (0, I). Then it holds for all integerm \u2265 1 that\nE [Xm] = 2m/2 \u0393 (m/2 + n/2)\n\u0393 (n/2) \u2264 m!nm/2 (A.12)\nLemma A.6 (Moments of the \u03c72 Random Variable). If X \u223c \u03c72 (n), i.e., X \u2261d \u2016x\u201622 for x \u223c N (0, I). Then it holds for all integerm \u2265 1 that\nE [Xm] = 2m \u0393 (m+ n/2)\n\u0393 (n/2) = m\u220f k=1 (n+ 2k \u2212 2) \u2264 m! 2 (2n)m. (A.13)\nLemma A.7 (Moment-Control Bernstein\u2019s Inequality for RandomVariables). LetX1, . . . , Xp be i.i.d. real-valued random variables. Suppose that there exist some positive number R and \u03c32X such that\nE [|Xk|m] \u2264 m!\n2 \u03c32XR\nm\u22122, for all integersm \u2265 2. (A.14)\nLet S .= 1p \u2211p k=1Xk, then for all t > 0, it holds that\nP [|S \u2212 E [S]| \u2265 t] \u2264 2 exp ( \u2212 pt 2\n2\u03c32X + 2Rt\n) . (A.15)\nLemma A.8 (Moment-Control Bernstein\u2019s Inequality for RandomVectors). Let x1, . . . ,xp \u2208 Rd be i.i.d. random vectors. Suppose there exist some positive number R and \u03c32X such that\nE [\u2016xk\u2016m2 ] \u2264 m!\n2 \u03c32XR\nm\u22122, for all integersm \u2265 2. (A.16)\nLet s = 1p \u2211p k=1 sk, then for any t > 0, it holds that\nP [\u2016s\u2212 E [s]\u20162 \u2265 t] \u2264 2(d+ 1) exp ( \u2212 pt 2\n2\u03c32X + 2Rt\n) . (A.17)\nLemma A.9 (Hoeffding\u2019s Inequality). Let X1, \u00b7 \u00b7 \u00b7 , Xp be independent random variables such that Xk takes its values in [ak, bk] almost surely for all 1 \u2264 k \u2264 p. Let S = \u2211p k=1 (Xk \u2212 EXk), then for every t > 0,\nP [S \u2265 t] \u2264 exp ( \u2212 2t\n2\u2211p k=1(bk \u2212 ak)2\n) . (A.18)\nLemma A.10 (Gaussian Concentration Inequality). Let x \u223c N (0, Ip). Let f : Rp 7\u2192 R be anL-Lipschitz function. Then we have for all t > 0 that\nP [f(X)\u2212 Ef(X) \u2265 t] \u2264 exp ( \u2212 t 2\n2L2\n) . (A.19)\n6The notation \u2261d means equivalent in distribution.\nLemma A.11 (Bounding Maximum Norm of Gaussian Vector Sequence). Let x1, . . . ,xp be a sequence of (not necessarily independent) standard Gaussian vectors in Rn. Then, it holds that\nP [ max i\u2208[p] \u2016xi\u20162 > \u221a 2 log p+ 2 \u221a n ] \u2264 exp ( \u22121 2 n ) . (A.20)\nProof. Since the function \u2016\u00b7\u20162 is 1-Lipschitz, by Gaussian concentration inequality, for any i \u2208 [p], we have\nP [ \u2016xi\u20162 \u2212 \u221a E \u2016xi\u201622 > t ] \u2264 P [\u2016xi\u20162 \u2212 E \u2016xi\u20162 > t] \u2264 exp ( \u2212 t 2\n2\n) (A.21)\nfor all t > 0. Since E \u2016xi\u201622 = n, by a simple union bound, we obtain\nP [ max i\u2208[p] \u2016xi\u2016 > \u221a n+ t ] \u2264 exp ( \u2212 t 2 2 + log p ) (A.22)\nfor all t > 0. Taking t = \u221a 2 log p+ \u221a n and simplifying the terms gives the claimed result.\nLemma A.12 (Covering Number of a Unit Ball). Let B = {x \u2208 Rn | \u2016x\u20162 \u2264 1} be a unit ball. For any \u03b5 \u2208 (0, 1), there exists some \u03b5 cover of B w.r.t. the normal Rn metric, denoted as N\u03b5, such that\n|N\u03b5| \u2264 ( 1 + 2\n\u03b5\n)n \u2264 ( 3\n\u03b5\n)n . (A.23)\nLemma A.13 (Spectrum of Gaussian Matrices, [Ver10]). Let A \u2208 Rp\u00d7n (p > n) contain i.i.d. standard normal entries. Then for every t \u2265 0, with probability at least 1\u2212 2 exp ( \u2212t2/2 ) , one has\n\u221a p\u2212\u221an\u2212 t \u2264 \u03c3min(A) \u2264 \u03c3max(A) \u2264 \u221a p+ \u221a n+ t. (A.24)\nLemma A.14. For any \u03b5 \u2208 (0, 1), there exists a constant C (\u03b5) > 1, such that provided n1 > C (\u03b5)n2, the random matrix \u03a6 \u2208 Rn1\u00d7n2 \u223ci.i.d. N (0, 1) obeys\n(1\u2212 \u03b5) \u221a 2\n\u03c0 n1 \u2016x\u20162 \u2264 \u2016\u03a6x\u20161 \u2264 (1 + \u03b5)\n\u221a 2\n\u03c0 n1 \u2016x\u20162 for all x \u2208 Rn2 , (A.25)\nwith probability at least 1\u2212 2 exp (\u2212c (\u03b5)n2) for some c (\u03b5) > 0.\nGeometrically, this lemma roughly corresponds to thewell known almost spherical section theorem [FLM77, GG84], see also [GM03]. A slight variant of this version has been proved in [Don06], borrowing ideas from [Pis99].\nProof. By homogeneity, it is enough to consider all x with unit norms. For a fixed x0 with \u2016x0\u20162 = 1, \u03a6x0 \u223c N (0, I). So E \u2016\u03a6x\u20161 = \u221a 2 \u03c0n1. By concentration of measure for Gaussian vectors,\nP [|\u2016\u03a6x\u20161 \u2212 E [\u2016\u03a6x\u20161]| > t] \u2264 2 exp ( \u2212 t 2\n2n1\n) (A.26)\nfor any t > 0. For a fixed \u03b4 \u2208 (0, 1), Sn2\u22121 can be covered by a \u03b4-net N\u03b4 with cardinality #N\u03b4 \u2264 (1 + 2/\u03b4)n2 . Now consider the event\nE .= { (1\u2212 \u03b4) \u221a 2\n\u03c0 n1 \u2264 \u2016\u03a6x\u20161 \u2264 (1 + \u03b4)\n\u221a 2\n\u03c0 n1 \u2200 x \u2208 N1\n} . (A.27)\nA simple application of union bound yields P [Ec] \u2264 2 exp ( \u2212\u03b4\n2n1 \u03c0 + n2 log\n( 1 + 2\n\u03b4\n)) . (A.28)\nChoosing \u03b4 small enough such that\n(1\u2212 3\u03b4) (1\u2212 \u03b4)\u22121 \u2265 1\u2212 \u03b5 and (1 + \u03b4) (1\u2212 \u03b4)\u22121 \u2264 1 + \u03b5, (A.29)\nthen conditioned on E , we can conclude that (1\u2212 \u03b5) \u221a 2\n\u03c0 n1 \u2264 \u2016\u03a6x\u20161 \u2264 (1 + \u03b5)\n\u221a 2\n\u03c0 n1 \u2200 x \u2208 Sn2\u22121. (A.30)\nIndeed, suppose E holds. Then it can easily be seen that any z \u2208 Sn2\u22121 can be written as\nz = \u221e\u2211 k=0 \u03bbkxk, with |\u03bbk| \u2264 \u03b4k,xk \u2208 N1 for all k. (A.31)\nHence we have\n\u2016\u03a6z\u20161 = \u2225\u2225\u2225\u2225\u2225\u03a6 \u221e\u2211 k=0 \u03bbkxk \u2225\u2225\u2225\u2225\u2225 1 \u2264 \u221e\u2211 k=0 \u03b4k \u2016\u03a6xk\u20161 \u2264 (1 + \u03b4) (1\u2212 \u03b4) \u22121 \u221a 2 \u03c0 n1. (A.32)\nSimilarly, \u2016\u03a6z\u20161 = \u2225\u2225\u2225\u2225\u2225\u03a6 \u221e\u2211 k=0 \u03bbkxk \u2225\u2225\u2225\u2225\u2225 1 \u2265 [ 1\u2212 \u03b4 \u2212 \u03b4 (1 + \u03b4) (1\u2212 \u03b4)\u22121 ]\u221a 2 \u03c0 n1 = (1\u2212 3\u03b4) (1\u2212 \u03b4)\u22121 \u221a 2 \u03c0 n1. (A.33)\nHence, the choice of \u03b4 above leads to the claimed result. To make P [Ec] small, it is enough to choose C such that\nC\u03b42/\u03c0 > log ( 1 + 2\n\u03b4\n) . (A.34)\nSetting C = 2 log ( 1 + 2\u03b4 ) \u03c0/\u03b42 completes the proof.\nLemma A.15. Suppose n1 \u2264 12 exp (n2/2). Fix \u03b5 \u2208 (0, 1). Then for any \u03be such that \u03be2 > 2 log (1 + 2\u03b5). The random matrix \u03a6 \u2208 Rn1\u00d7n2 \u223ci.i.d. N (0, 1) obeys\n\u2016\u03a6x\u2016\u221e \u2264 1 + \u03be 1\u2212 \u03b5 \u221a n2 \u2016x\u20162 for all x \u2208 Rn2 , (A.35)\nwith probability at least 1\u2212 exp ( \u2212n2 ( \u03be2/2\u2212 log (1 + 2\u03b5) )) .\nProof. Again for a fixed x0 \u2208 Sn2\u22121, \u03a6x0 \u2261d v \u223c N (0, I). For any fixed \u03b2 > 0 to be decided later,\nE [\u03b2 \u2016v\u2016\u221e] = E [ \u03b2 max i\u2208[n1] |vi| ] = E [ log max i\u2208[n1] exp (\u03b2 |v1|) ] \u2264 logE [ max i\u2208[n1] exp (\u03b2 |v1|) ]\n(A.36)\n\u2264 logE [ n1\u2211 i=1 exp (\u03b2 |v1|) ] = log n1E [exp (\u03b2 |v1|)] \u2264 log 2n1 exp ( \u03b22/2 ) . (A.37)\nHence\nE [\u2016v\u2016\u221e] \u2264 log 2n1 exp\n( \u03b22/2 ) \u03b2 . (A.38)\nTaking \u03b2 = \u221a 2 log (2n1), we obtain\nE [\u2016v\u2016\u221e] \u2264 \u221a 2 log (2n1). (A.39)\nBecause the mapping v 7\u2192 \u2016v\u2016\u221e is 1-Lipschitz, by concentration of measure for Gaussian vectors, we obtain\nP [\u2016\u03a6x\u2016\u221e \u2212 E [\u2016\u03a6x\u2016\u221e] > t] \u2264 exp ( \u2212 t 2\n2\n) . (A.40)\nTaking t = \u03be\u221an2, and consider an \u03b5-net N\u03b5 that covers Sn2\u22121 with cardinality |N\u03b5| \u2264 (1 + 2/\u03b5)n2 , we have the event\nE .= {\u2016\u03a6x\u2016\u221e \u2264 (1 + \u03be) \u221a n2 \u2200 x \u2208 N\u03b5} (A.41)\nholds with probability at least 1\u2212 exp ( \u2212\u03be2n2/2 + n2 log (1 + 2\u03b5) ) . Conditioned on E , we have\nsup \u2016z\u20162=1 \u2016\u03a6z\u2016\u221e \u2264 sup z\u2032\u2208N\u03b5 \u2016\u03a6z\u2032\u2016\u221e + sup \u2016e\u20162\u2264\u03b5 \u2016\u03a6e\u2016\u221e = sup z\u2032\u2208N\u03b5 \u2016\u03a6z\u2032\u2016\u221e + \u03b5 sup \u2016e\u20162=1 \u2016\u03a6e\u2016\u221e . (A.42)\nHence we have\nsup \u2016z\u20162=1\n\u2016\u03a6z\u2016\u221e \u2264 1 1\u2212 \u03b5 supz\u2032\u2208N\u03b5 \u2016\u03a6z\u2032\u2016\u221e = 1 + \u03be 1\u2212 \u03b5 \u221a n2, (A.43)\ncompleting the proof."}, {"heading": "B The Random Basis vs. Its Orthonormalized Version", "text": "We consider Y obeying the planted sparse model:\nY = [x0 | G] \u2208 Rp\u00d7n (B.1)\nwith\nx0 \u223ci.i.d. 1\u221a \u03b8p\nBer (\u03b8) ,G \u223ci.i.d. N ( 0, 1\np\n) . (B.2)\nOne \u201cnatural/canonical\u201d orthonormal basis for the subspace spanned by columns of Y is\nY\u2032 = [ x0 \u2016x0\u20162 | Px\u22a50 G ( G>Px\u22a50 G )\u22121/2] . (B.3)\nWe alsowriteG\u2032 .= Px\u22a50 G ( G>Px\u22a50 G )\u22121/2 for convenience. In this section, wewant to show that the intuition Y\u2032 well approximating Y7 can be made rigorous. These results are needed when we prove Theorem 2.1 for the global optimality of the natural `2 constrained formulation (2.1), as well as when we translate the results for Y to quantitative statements about Y\u2032 in Appendix F.4.\nFor any realization of x0, let the support (index set of nonzero elements) of x0 be I. By Hoeffding\u2019s inequality in Lemma A.9, we have the event\nE0 .= { 1\n2 \u03b8p \u2264 |I| \u2264 2\u03b8p\n} (B.4)\nholds with probability at least 1\u2212 2 exp ( \u2212p\u03b82/2 ) . Moreover, we show the following:\n7When n and p are large, Y has nearly orthonormal columns.\nLemma B.1. The bound \u2223\u2223\u2223\u22231\u2212 1\u2016x0\u20162 \u2223\u2223\u2223\u2223 \u2264 2\u221a25 \u221a n log p \u03b83p (B.5)\nholds with probability at least 1\u2212 2 exp ( \u2212p\u03b82/2 ) \u2212 2 exp (\u22122n log p).\nProof. Because E [ \u2016x0\u201622 ] = 1, by Hoeffding\u2019s inequality in Lemma A.9, we have\nP [\u2223\u2223\u2223\u2016x0\u201622 \u2212 E [\u2016x0\u201622] > t\u2223\u2223\u2223] = P [\u2223\u2223\u2223\u2016x0\u201622 \u2212 1\u2223\u2223\u2223 > t] \u2264 2 exp (\u22122\u03b82pt2) (B.6)\nfor all t > 0, which implies\nP [|\u2016x0\u20162 \u2212 1| (\u2016x0\u20162 + 1) > t] \u2264 2 exp ( \u22122\u03b82pt2 ) . (B.7)\nOn the intersection with E0, \u2016x0\u20162 + 1 \u2264 \u221a 2 + 1 \u2264 5/2, and setting t = \u221a n log p \u03b83p , we obtain\nP [ |\u2016x0\u20162 \u2212 1| > 2\n5\n\u221a n log p\n\u03b83p\n] \u2264 2 exp (\u22122n log p) . (B.8)\nSo we obtain that with probability at least 1\u2212 2 exp ( \u2212p\u03b82/2 ) \u2212 2 exp (\u22122n log p),\u2223\u2223\u2223\u22231\u2212 1\u2016x0\u20162 \u2223\u2223\u2223\u2223 = |1\u2212 \u2016x0\u20162|\u2016x0\u20162 \u2264 2 \u221a 2 5 \u221a n log p \u03b83p , (B.9)\nas desired. Next, let M .= ( G>Px\u22a50 G )\u22121/2 , then G\u2032 = GM\u2212 x0x > 0 \u2016x0\u201622 GM, we show the following results hold:\nLemma B.2. Provided p \u2265 Cn \u2265 2 for some large enough constant C, it holds that\n\u2016M\u2016 \u2264 2, \u2016M\u2212 I\u2016 \u2264 4 \u221a n\np (B.10)\nwith probability at least 1\u2212 c\u2032 exp (\u2212c\u2032\u2032n) for some positive constants c\u2032 and c\u2032\u2032.\nProof. First observe that\n\u2016M\u2016 = ( \u03c3min ( G>Px\u22a50 G ))\u22121/2 = ( \u03c3min ( Px\u22a50 G ))\u22121 . (B.11)\nNow suppose B is an orthonormal basis spanning x\u22a50 . Then it is not hard to see the spectrum of Px\u22a50 G is the same as that of B>G \u2208 R(p\u22121)\u00d7(n\u22121); in particular,\n\u03c3min ( Px\u22a50 G ) = \u03c3min ( B>G ) . (B.12)\nSince G \u223ci.i.d. N ( 0, 1p ) , and B> has orthonormal rows, B>G \u223ci.i.d. N ( 0, 1p ) , we can invoke the spectrum results for Gaussian matrices in Lemma A.13 and obtain that\u221a p\u2212 1 p \u2212 2 \u221a n\u2212 1 p\u2212 1 \u2264 \u03c3min ( B>G ) \u2264 \u03c3max ( B>G ) \u2264 \u221a p\u2212 1 p + 2 \u221a n\u2212 1 p\u2212 1 (B.13)\nwith probability at least 1\u2212 c1 exp (\u2212c2n) for some c1, c2 > 0. Thus, when p \u2265 C1n for some large constant C1, we have\n\u2016M\u2016 = (\u221a p\u2212 1 p \u2212 2 \u221a n\u2212 1 p\u2212 1 )\u22121 \u2264 2, (B.14)\n\u2016I\u2212M\u2016 = max (|\u03c3max (M)\u2212 1| , |\u03c3min (M)\u2212 1|) \u2264 2 \u221a n\u2212 1 p\u2212 1 (\u221a p\u2212 1 p \u2212 2 \u221a n\u2212 1 p\u2212 1 )\u22121 \u2264 4 \u221a n p , (B.15)\nwith probability at least 1\u2212 c1 exp (\u2212c2n).\nLemma B.3. There exists a constant C > 0, such that when p \u2265 Cn, the following\n\u2016Y\u2016`2\u2192`1 \u2264 3 \u221a p, (B.16)\n\u2016Y\u2032I\u2016`2\u2192`1 \u2264 7 \u221a 2\u03b8p, (B.17)\n\u2016YI \u2212Y\u2032I\u2016`2\u2192`1 \u2264 10\n\u03b8\n\u221a n log p, (B.18)\n\u2016G\u2212G\u2032\u2016`2\u2192`1 \u2264 8 \u221a n, (B.19)\n\u2016Y \u2212Y\u2032\u2016`2\u2192`1 \u2264 10\n\u03b8\n\u221a n log p (B.20)\nhold simultaneously with probability at least 1\u2212 c\u2032 exp (\u2212c\u2032\u2032n)\u2212 2 exp ( \u2212p\u03b82/2 ) for some positive constants c\u2032 and c\u2032\u2032.\nProof. First of all, we have\u2225\u2225\u2225\u2225\u2225 x0x>0\u2016x0\u201622 GM \u2225\u2225\u2225\u2225\u2225 `2\u2192`1 \u2264 1 \u2016x0\u201622 \u2016x0\u2016`2\u2192`1 \u2225\u2225x>0 GM\u2225\u2225`2\u2192`2 = 2\u2016x0\u201622 \u2016x0\u20161\n\u2225\u2225x>0 G\u2225\u22252 , (B.21) where in the last inequalitywe have applied the fact \u2016M\u2016 \u2264 2 fromLemma B.2. Now x>0 G is an i.i.d. Gaussian vectors with each entry distributed as N ( 0, \u2016x0\u201622 p ) , where \u2016x0\u201622 = |I| \u03b8p . So by measure concentration inequality for Gaussian vectors, we have\n\u2225\u2225x>0 G\u2225\u22252 \u2264 2 \u2016x0\u20162\u221anp (B.22) with probability at least 1\u2212 exp (\u2212n/2). On the intersection with E0, this implies\u2225\u2225\u2225\u2225\u2225 x0x>0\u2016x0\u201622 GM \u2225\u2225\u2225\u2225\u2225 `2\u2192`1 \u2264 4 \u221a |I| \u221a n p \u2264 4 \u221a 2\u03b8n, (B.23)\nwith probability at least 1\u2212 exp (\u2212n/2)\u2212 2 exp ( \u2212p\u03b82/2 ) . Moreover, when intersected with E0, Lemma A.14 implies that when p \u2265 \u2126 (n),\n\u2016G\u2016`2\u2192`1 \u2264 \u221a p, \u2016GI\u2016`2\u2192`1 \u2264 \u221a 2\u03b8p (B.24)\nwith probability at least 1 \u2212 c1 exp (\u2212c2n) \u2212 2 exp ( \u2212p\u03b82/2 ) , for some positive constants c1, c2. So when\np \u2265 \u2126 (n), \u2016G\u2212G\u2032\u2016`2\u2192`1 \u2264 \u2016G\u2212GM\u2016`2\u2192`1 + \u2225\u2225\u2225\u2225\u2225 x0x>0\u2016x0\u201622 GM \u2225\u2225\u2225\u2225\u2225 `2\u2192`1 \u2264 \u2016G\u2016`2\u2192`1 \u2016I\u2212M\u2016+ 4 \u221a 2\u03b8n \u2264 8\u221an, (B.25)\n\u2016Y\u2016`2\u2192`1 \u2264 \u2016x0\u2016`2\u2192`1 + \u2016G\u2016`2\u2192`1 \u2264 \u2016x0\u20161 + \u221a p \u2264 2 \u221a \u03b8p+ \u221a p \u2264 3\u221ap, (B.26)\n\u2016G\u2032I\u2016`2\u2192`1 \u2264 \u2016GIM\u2016`2\u2192`1 + \u2225\u2225\u2225\u2225\u2225 x0x>0\u2016x0\u201622 GM \u2225\u2225\u2225\u2225\u2225 `2\u2192`1 \u2264 \u2016GI\u2016`2\u2192`1 \u2016M\u2016+ 4 \u221a 2\u03b8n \u2264 6 \u221a 2\u03b8p, (B.27)\n\u2016GI \u2212G\u2032I\u2016`2\u2192`1 \u2264 \u2016GI (I\u2212M)\u2016`2\u2192`1 + \u2225\u2225\u2225\u2225\u2225 x0x>0\u2016x0\u201622 GM \u2225\u2225\u2225\u2225\u2225 `2\u2192`1 \u2264 \u2016GI\u2016`2\u2192`1 \u2016I\u2212M\u2016+ 4 \u221a 2\u03b8n \u2264 8 \u221a 2\u03b8n,\n(B.28) \u2016Y\u2032I\u2016`2\u2192`1 \u2264 \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 `2\u2192`1 + \u2016G\u2032I\u2016`2\u2192`1 \u2264 \u2016x0\u20161 \u2016x0\u20162 + 6 \u221a 2\u03b8p \u2264 7 \u221a 2\u03b8p (B.29)\nwith probability at least 1\u2212 c3 exp (\u2212c4n)\u2212 2 exp ( \u2212p\u03b82/2 ) for some positive constants c3, c4, where we have used the above estimates and the results in Lemma B.2. Finally, by Lemma B.1, we obtain\n\u2016Y \u2212Y\u2032\u2016`2\u2192`1 \u2264 \u2223\u2223\u2223\u22231\u2212 1\u2016x0\u20162 \u2223\u2223\u2223\u2223 \u2016x0\u20161 + \u2016G\u2212G\u2032\u2016`2\u2192`1 \u2264 10\u03b8 \u221an log p, (B.30) \u2016YI \u2212Y\u2032I\u2016`2\u2192`1 \u2264 \u2223\u2223\u2223\u22231\u2212 1\u2016x0\u20162 \u2223\u2223\u2223\u2223 \u2016x0\u20161 + \u2016GI \u2212G\u2032I\u2016`2\u2192`1 \u2264 10\u03b8 \u221an log p, (B.31)\nholding with probability at least 1\u2212 c5 exp (\u2212c6n)\u2212 2 exp ( \u2212p\u03b82/2 ) for some positive constants c5, c6.\nLemma B.4. Provided Cn \u2264 p \u2264 exp (n/2) /2 for some constant C > 0, the following\n\u2016G\u2032\u2016`2\u2192`\u221e \u2264 16 \u221a n\n\u03b8p , (B.32)\n\u2016G\u2212G\u2032\u2016`2\u2192`\u221e \u2264 32n\u221a \u03b8p\n(B.33)\nhold simultaneously with probability at least 1\u2212 c\u2032 exp (\u2212c\u2032\u2032n)\u2212 2 exp ( \u2212p\u03b82/2 ) for some positive constants c\u2032 and c\u2032\u2032.\nProof. First of all, we have\u2225\u2225\u2225\u2225\u2225 x0x>0\u2016x0\u201622 GM \u2225\u2225\u2225\u2225\u2225 `2\u2192`\u221e \u2264 1 \u2016x0\u201622 \u2016x0\u2016`2\u2192`\u221e \u2225\u2225x>0 GM\u2225\u2225`2\u2192`2 = 2\u2016x0\u201622 \u2016x0\u2016\u221e \u2225\u2225x>0 G\u2225\u22252 , (B.34) where at the last inequality we have applied the fact \u2016M\u2016 \u2264 2 from Lemma B.2. Similar to the proof to Lemma B.3, we have that\n\u2225\u2225x>0 G\u2225\u22252 \u2264 2 \u2016x0\u20162\u221an/p with probability at least 1 \u2212 exp (\u2212n/2). So on the intersection with E0, we obtain that\u2225\u2225\u2225\u2225\u2225 x0x>0\u2016x0\u201622 GM \u2225\u2225\u2225\u2225\u2225 `2\u2192`\u221e \u2264 4 \u2016x0\u2016\u221e\u2016x0\u20162 \u221a n p \u2264 4 \u221a 2n\u221a \u03b8p (B.35)\nholds with probability at least 1\u2212exp (\u2212n/2)\u22122 exp ( \u2212p\u03b82/2 ) . Now taking \u03be = 2 and \u03b5 = 1/2 in LemmaA.15, we have that\n\u2016G\u2016`2\u2192`\u221e \u2264 6 \u221a n\np (B.36)\nwith probability at least 1\u2212 exp (\u2212n (2\u2212 log 2)). Combining with results in Lemma B.2, we obtain\n\u2016G\u2032\u2016`2\u2192`\u221e \u2264 \u2016GM\u2016`2\u2192`\u221e + \u2225\u2225\u2225\u2225\u2225 x0x>0\u2016x0\u201622 GM \u2225\u2225\u2225\u2225\u2225 `2\u2192`\u221e\n\u2264 \u2016G\u2016`2\u2192`\u221e \u2016M\u2016+ 4 \u221a\n2n\u221a \u03b8p \u2264 12\n\u221a n\np +\n4 \u221a\n2n\u221a \u03b8p \u2264 16\n\u221a n\n\u03b8p , (B.37) \u2016G\u2212G\u2032\u2016`2\u2192`\u221e \u2264 \u2016G\u2016`2\u2192`\u221e \u2016I\u2212M\u2016+ \u2225\u2225\u2225\u2225\u2225 x0x>0\u2016x0\u201622 GM \u2225\u2225\u2225\u2225\u2225 `2\u2192`\u221e \u2264 24n p + 4 \u221a 2n\u221a \u03b8p \u2264 32n\u221a \u03b8p (B.38)\nwith probability at least 1\u2212 c7 exp (\u2212c8n)\u2212 2 exp ( \u2212p\u03b82/2 ) for some positive constants c7, c8.\nC Proof of `1/`2 Global Optimality Proof. We will first analyze a canonical version, in which the input basis is Y\u2032:\nmin q\u2208Rn\n\u2016Y\u2032q\u20161 , s.t. \u2016q\u20162 = 1. (C.1)\nLet q = [q1; q2]. For any fixed support I of x0, we have\n\u2016Y\u2032q\u20161 = \u2016Y\u2032Iq\u20161 + \u2016Y\u2032Icq\u20161 \u2265 |q1| \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 1 \u2212 \u2016G\u2032Iq2\u20161 + \u2016G\u2032Icq2\u20161\n\u2265 |q1| \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 1 \u2212 \u2016GIq2\u20161 \u2212 \u2016(GI \u2212G\u2032I) q2\u20161 + \u2016GIcq2\u20161 \u2212 \u2016(GIc \u2212G\u2032Ic) q2\u20161\n\u2265 |q1| \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 1 \u2212 \u2016GIq2\u20161 + \u2016GIcq2\u20161 \u2212 \u2016G\u2212G\u2032\u2016`2\u2192`1 \u2016q2\u20162 . (C.2)\nBy Lemma A.14 and intersecting with E0, we have that as long as p \u2265 \u2126 (n), there exists constant c1 > 0 such that\n\u2016GIq2\u20161 \u2264 2\u03b8p\u221a p \u2016q2\u20162 = 2\u03b8 \u221a p \u2016q2\u20162 for all q2 \u2208 Rn\u22121, (C.3)\n\u2016GIcq2\u20161 \u2265 1\n2 p\u2212 2\u03b8p\u221a p \u2016q2\u20162 = 1 2 \u221a p (1\u2212 2\u03b8) \u2016q2\u20162 for all q2 \u2208 Rn\u22121, (C.4)\nhold with probability at least 1\u2212 2 exp (\u2212c1n2)\u2212 2 exp ( \u2212p\u03b82/2 ) . Moreover, by Lemma B.3,\n\u2016G\u2212G\u2032\u2016`2\u2192`1 \u2264 8 \u221a n (C.5)\nholds with probability at least 1\u2212 c2 exp (\u2212c3n)\u2212 2 exp ( \u2212p\u03b82/2 ) when p \u2265 \u2126 (n). So we obtain that\n\u2016Y\u2032q\u20161 \u2265 |q1| \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 1 + \u2016q2\u20162 ( 1 2 \u221a p (1\u2212 2\u03b8)\u2212 2\u03b8\u221ap\u2212 8\u221an ) (C.6)\nholds with probability at least 1\u2212 c4 exp (\u2212c5n)\u2212 2 exp ( \u2212p\u03b82/2 ) for some positive c4 and c5. Assuming E0, we observe \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 1 \u2264 \u221a |I| \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 2 \u2264 \u221a 2\u03b8p. (C.7)\nSo in order to minimize the objective \u2016Y\u2032q\u20161 at e1 or\u2212e1, i.e., q1 = 1, subject to the constraint q21 + \u2016q2\u2016 2 2 = 1,\nit suffices to have \u221a 2\u03b8p < 1\n2\n\u221a p (1\u2212 2\u03b8)\u2212 2\u03b8\u221ap\u2212 8\u221an, (C.8)\nwhich is satisfied when \u03b8 is sufficiently small. Thus there exists a universal constant \u03b80 > 0, such that for all 1/ \u221a n \u2264 \u03b8 \u2264 \u03b80, when p \u2265 \u2126 (n), \u00b1e1 are the global minimizers of (2.2) with probability at least 1\u2212 c4 exp (\u2212c5n)\u2212 2 exp ( \u2212p\u03b82/2 ) , if the input basis is Y\u2032. As \u03b8 > 1/ \u221a n by assumption, from (B.4), to make the above probability high, it is enough to make p \u2265 \u2126 (n log n). Any other input basis can be written as Y\u2032R, for some orthogonal matrix R. The program now is written as\nmin q\u2208Rn\n\u2016Y\u2032Rq\u20161 , s.t. \u2016q\u20162 = 1, (C.9)\nwhich is equivalent to\nmin q\u2208Rn\n\u2016Y\u2032Rq\u20161 , s.t. \u2016Rq\u20162 = 1, (C.10)\nwhich is obviously equivalent to the canonical program we analyze above by a simple change of variable, i.e., q . = Rq, completing the proof."}, {"heading": "D Proof of Main Result", "text": "In this appendix, we prove our main result in Theorem 4.1. In particular, we will first show that when the Y\u2032 defined in (B.3) is the input orthonormal basis, the \u201cinitialization + ADM + LP rounding\u201d pipeline recovers x0 under the stated technical conditions. Then we will upgrade the recovery result to all orthonormal basis by observing that all three stages are \u201cinvariant\u201d to the input orthonormal basis Y.\nKeep the notation in Section 3, let y1, \u00b7 \u00b7 \u00b7 ,yp be the transpose of the rows of Y, and let y\u20321, \u00b7 \u00b7 \u00b7 ,y\u2032p be the transpose of the rows of Y\u2032. For q \u2208 Sn\u22121, set\nQ(q) = 1\np p\u2211 k=1 ykS\u03bb [ q>yk ] , (D.1)\nQ\u2032(q) = 1\np p\u2211 k=1 y\u2032kS\u03bb [ q>y\u2032k ] . (D.2)\nFurther, we write Q (q) = [Q1 (q) ; Q2 (q)], where Q1 (q) is the first coordinate, and define similar notations for Q\u2032 (q). In addition, for any k = 1, \u00b7 \u00b7 \u00b7 , p, set\nX1k(Zk) = x0kS\u03bb [ q>yk ] = x0kS\u03bb [x0kq1 + Zk] , (D.3)\nX2k(Zk) = g kS\u03bb\n[ q>yk ] = gkS\u03bb [x0kq1 + Zk] , (D.4)\nwhere Zk = q>2 gk \u223c N (0, \u03c32) for \u03c3 = \u2016q2\u20162 / \u221a p, and x0k denotes the k-th coordinate of x0. Hence we obviously have\nQ1 = 1\np p\u2211 k=1 X1k , Q2 = 1 p p\u2211 k=1 X2k, . (D.5)\nNext we sketch the main technical pieces for establishing the recovery results for Y\u2032 first. All detailed proofs are deferred to later sections of the appendix. We will assume exp (n/2) /2 \u2265 p \u2265 Cn4 log n for some large constant C for all the subsequent claims.\n1. Good initialization. Proposition E.1 in Appendix E shows that with overwhelming probability, at least one of our p initialization vectors suggested in Section 3, say q(0)i = y\u2032i, obeys that\u2223\u2223\u2223\u2223\u2329 y\u2032i\u2016y\u2032i\u20162 , e1\n\u232a\u2223\u2223\u2223\u2223 \u2265 14\u221a\u03b8n. (D.6) 2. Uniform progress away from the equator. By Proposition F.1 in Appendix F, there exists some constant \u03b80 > 0, such that for any \u03b8 \u2208 ( 1\u221a n , \u03b80 ) ,\nG\u2032(q) = |Q\u20321(q)| |q1| \u2212 \u2016Q \u2032 2(q)\u20162 \u2016q\u20162 \u2265 1 4000\u03b82np\n(D.7)\nholds uniformly for all q \u2208 Sn\u22121 in the region 1 4 \u221a \u03b8n \u2264 |q1| \u2264 3\n\u221a \u03b8 with overwhelming probability.\n3. No jumps away from the cap. Proposition G.1 in Appendix G shows that for any \u03b8 \u2208 (\n1\u221a n , \u03b80\n) , with\noverwhelming probability,\nQ\u20321(q) \u2016Q\u2032(q)\u20162 \u2265 2 \u221a \u03b8 (D.8)\nholds for all q with |q1| \u2265 3 \u221a \u03b8.\n4. Location of the stationary/stopping point. The first point above ensures that with overwhelming probability at least one starting point q(0) will satisfy \u2223\u2223\u2223q(0)1 \u2223\u2223\u2223 \u2265 14\u221a\u03b8n . As shown in Appendix H, the strictly positive gap of the second point ensures that one needs to run at most O ( n4 log n ) iterations\nto first encounter an iterate q(k) such that \u2223\u2223\u2223q(k)1 \u2223\u2223\u2223 \u2265 3\u221a\u03b8. The third point suggests extra iterations will not move away from the cap area, and hence the stationary point q of the ADM algorithm will satisfy |q1| \u2265 2 \u221a \u03b8. If one enforces a hard stop after O ( n4 log n ) iterations, the stopping point will similarly\nstay in the region |q1| \u2265 2 \u221a \u03b8.\n5. LP Rounding succeeds. We know that in the LP rounding stage, described in Section 3, will receive a vector r = q\u0304 with its first coordinate |r1| \u2265 2 \u221a \u03b8. Proposition I.1 in Appendix I proves that with\noverwhelming probability, the LP rounding (3.6) (operated on Y\u2032) will output a solution q? = e1.\nIn summary, our ADM algorithm in Algorithm 1 using a smart initialization, plus an LP rounding stage (3.6), will output q? = \u00b1e1 with overwhelming probability, or Y\u2032q? as a nontrivial scaled version of x0.\nFor the general case when the input is an arbitrary orthonormal basis Y\u0302 = Y\u2032R for a certain orthogonal matrix R, the target solution is R>e1. The following technical pieces are perfectly parallel to the above for Y\u2032.\n\u2022 Discussion at the end of Appendix E suggests with overwhelming probability, at least one row of Y\u0302 provides an initial point q(0) such that \u2223\u2223\u2329q(0),R>e1\u232a\u2223\u2223 \u2265 14\u221a\u03b8n . \u2022 Discussion followingProposition F.1 inAppendix F suggests that for allq such that 1 4 \u221a \u03b8n \u2264 \u2223\u2223\u2329q,R>e1\u232a\u2223\u2223 \u2264\n3 \u221a \u03b8, there is a strictly positive gap, indicating steadyprogress towards a pointq(k) such that \u2223\u2223\u2329q(k),R>e1\u232a\u2223\u2223 \u2265 3 \u221a \u03b8.\n\u2022 Discussion at the end of Appendix G indicates once q satisfying \u2223\u2223\u2329q,R>e1\u232a\u2223\u2223, the next iterate will not\nmove far away from the target: \u2329 Q\u2032 ( q; Y\u0302 ) ,R>e1 \u232a \u2225\u2225\u2225Q\u2032 (q; Y\u0302)\u2225\u2225\u2225\n2\n\u2265 2 \u221a \u03b8. (D.9)\n\u2022 Repeating the argument in Appendix H for general input Y\u0302 shows it is enough to run the ADM algorithm O ( n4 log n ) iterations to cross the range 1 4 \u221a \u03b8n \u2264 \u2223\u2223\u2329q,R>e1\u232a\u2223\u2223 \u2264 3\u221a\u03b8. So the above three\npoints together dictates that with the proposed initialization, with overwhelming probability, we finally obtain a point q that satisfies \u2223\u2223\u2329q,R>e1\u232a\u2223\u2223 \u2265 2\u221a\u03b8, if we run at least O (n4 log n) iterations. \u2022 Since the ADM returns q satisfying\n\u2223\u2223\u2329q,R>e1\u232a\u2223\u2223 \u2265 2\u221a\u03b8, discussion at the end of Appendix I dictates that we will obtain q? = R>e1 as the optimizer of the rounding program, exactly the target solution.\nWe complete the proof."}, {"heading": "E Good Initialization", "text": "Proposition E.1. Let y\u2032k for k = 1, . . . , p be the transpose of the rows of the orthonormal bases Y\u2032 defined in (B.3). If \u03b8 > 1/ \u221a n and exp (n/2) /2 \u2265 p \u2265 Cn2 for some constant C > 0, it holds that at least one of our p initialization vectors suggested in Section 3, say q(0)i = y\u2032i, obeys\u2223\u2223\u2223\u2223\u2329 y\u2032i\u2016y\u2032i\u20162 , e1 \u232a\u2223\u2223\u2223\u2223 \u2265 14\u221a\u03b8n, (E.1)\nwith probability at least 1\u2212 c\u2032 exp (\u2212c\u2032\u2032n) for some positive constants c\u2032 and c\u2032\u2032.\nProof. Since x0 is i.i.d. Bernoulli, with probability at least 1\u2212(1\u2212 \u03b8)p \u2265 1\u2212exp (\u2212\u03b8p), at least one component of x0 is nonzero. Without loss of generality (w.l.o.g.), assume the k-th component of x0 is nonzero. Then x0k =\n1\u221a \u03b8p , and\n|qi1| = 1\u221a \u03b8p\n\u2016x0\u20162 \u2016y\u2032i\u20162 \u2265\n1\u221a \u03b8p \u2016x0\u20162 ( \u2016x0/ \u2016x0\u20162\u2016`2\u2192`\u221e + \u2016G\u2032\u2016`2\u2192`\u221e ) = 1\u221a \u03b8p \u2016x0\u2016\u221e + \u2016x0\u20162 \u2016G\u2032\u2016`2\u2192`\u221e . (E.2)\nWe know that with probability at least 1\u2212 exp(\u2212p\u03b82/2), it holds that\n\u2016x0\u20162 = \u221a |I| \u00d7 1 \u03b8p \u2264 \u221a 2\u03b8p\u00d7 1 \u03b8p = \u221a 2. (E.3)\nMoreover, using Lemma B.4 , and Lemma A.15 with \u03b5 = 1/16 and \u03be = 1/2, we know when p \u2265 C1n for some large C1 > 0, it holds that (note that \u2016M\u2016 can be arbitrarily close to 1 for large C1 in Lemma B.2)\n\u2016G\u2032\u2016`2\u2192`\u221e \u2264 \u2016G\u2016`2\u2192`\u221e \u2016M\u2016+ 4 \u221a\n2n\u221a \u03b8p \u2264 9 5\n\u221a n\np +\n4 \u221a 2n\u221a \u03b8p \u2264 2 \u221a n p (E.4)\nwith probability at least 1\u2212 c1 exp (\u2212c2n) for some positive constants c1 and c2. Therefore with probability at least 1\u2212 exp (\u2212\u03b8p)\u2212 exp ( \u2212\u03b82p ) \u2212 c1 exp (\u2212c2n), it holds that\n|qi1| \u2265 1\n1 + \u221a 2\u03b8p\u00d7 2 \u221a\nn p\n= 1\n1 + 2 \u221a 2 \u221a \u03b8n . (E.5)\nUsing the fact that \u03b8 \u2265 1/\u221an, we obtain |qi1| \u2265 1(1+2\u221a2)\u221a\u03b8n . It is sufficient to set p \u2265 C2n 2 for some large enough C2 > 0 to make the probability overwhelming, as desired.\nWe will next show that for an arbitrary orthonormal basis Y\u0302 .= Y\u2032R the initialization still biases towards the target solution. To see this, suppose w.l.o.g. ( y\u2032i )> is a row of Y\u2032 with nonzero first coordinate. We have\nshown above that with high probability \u2223\u2223\u2223\u2329 y\u2032i\u2016y\u2032i\u20162 , e1\u232a\u2223\u2223\u2223 \u2265 14\u221a\u03b8n if Y\u2032 is the input orthonormal basis. For Y\u2032, as x0 = Y \u2032e1 = Y\u2032RR>e1, we know q? = R>e1 is the target solution corresponding to Y\u0302. Observing that\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2329 R>e1, ( e>i Y\u0302 )>\u2225\u2225\u2225\u2225(e>i Y\u0302)>\u2225\u2225\u2225\u2225 2 \u232a\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 \u2329 R>e1, R> (Y\u2032)> ei\u2225\u2225\u2225R> (Y\u2032)> ei\u2225\u2225\u2225 2 \u232a\u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2223\u2223 \u2329 e1, (Y\u2032)> ei\u2225\u2225\u2225(Y\u2032)> ei\u2225\u2225\u2225 2 \u232a\u2223\u2223\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223\u2329e1, y\u2032i\u2016y\u2032i\u20162 \u232a\u2223\u2223\u2223\u2223 \u2265 14\u221an\u03b8 , (E.6)\ncorroborating our claim."}, {"heading": "F Lower Bounding Finite Sample Gap G\u2032(q)", "text": "We will first work with the \u201ccanonical\u201d orthonormal basis Y\u2032. The task is to lower bound the gap for finite samples\nG\u2032(q) = |Q\u20321(q)| |q1| \u2212 \u2016Q \u2032 2(q)\u20162 \u2016q2\u20162 . (F.1)\nSince we can deterministically constrain |q1| and \u2016q2\u20162 (e.g., 14\u221an\u03b8 \u2264 |q1| \u2264 3 \u221a \u03b8 and \u2016q2\u20162 \u2265 14 , where the choice of 14 is arbitrary here, as we can always take a sufficiently small \u03b8), the challenge lies in lower bounding |Q\u20321 (q)| and upper bounding \u2016Q\u20322 (q)\u20162, which depends on the orthonormal basis Y\u2032. It turns out to cook up a typical expectation-concentration style argument, the unnormalized basis Y is much easier to work with than Y\u2032. Hence our proof will follow the observation that\n|Q\u20321 (q)| \u2265 |E [Q1 (q)]| \u2212 |Q1 (q)\u2212 E [Q1 (q)]| \u2212 |Q\u20321 (q)\u2212Q1 (q)| , (F.2) \u2016Q\u20322 (q)\u2016 \u2264 \u2016E [Q2 (q)]\u20162 + \u2016Q2 (q)\u2212 E [Q2 (q)]\u20162 + \u2016Q\u20322 (q)\u2212Q2 (q)\u20162 . (F.3)\nIn particular, define the set \u0393 = {\nq \u2208 Sn\u22121 : 1 4 \u221a n\u03b8 \u2264 |q1| \u2264 3 \u221a \u03b8, \u2016q2\u20162 \u2265 14\n} :\n\u2022 Appendix F.1 shows that the expected gap is lower bounded for all q \u2208 Sn\u22121 with |q1| \u2264 3 \u221a \u03b8:\nG (q) . = |E [Q1 (q)]| |q1| \u2212 \u2016E [Q2 (q)]\u20162\u2016q2\u20162 \u2265 1 50 q21 \u03b8p . (F.4)\nAs |q1| \u2265 14\u221an\u03b8 , we have\ninf q\u2208\u0393 |E [Q1 (q)]| |q1| \u2212 \u2016E [Q2 (q)]\u20162\u2016q2\u20162 \u2265 1 800 1 \u03b82np . (F.5)\n\u2022 Appendix F.2, as summarized in Proposition F.9, shows that whenever exp (n) \u2265 p \u2265 \u2126 ( n4 log n ) , it\nholds with overwhelmingly probability that\nsup q\u2208\u0393 |Q1 (q)\u2212 E [Q1 (q)]| |q1| + \u2016Q2 (q)\u2212 E [Q2 (q)]\u20162 \u2016q2\u20162\n\u2264 4 \u221a \u03b8n\n16000\u03b85/2n3/2p +\n4\n16000\u03b82np =\n1\n2000\u03b82np . (F.6)\n\u2022 Appendix F.4 shows that whenever exp (n/2) /2 \u2265 p \u2265 \u2126 ( n4 log n ) , it holds with overwhelmingly\nprobability that\nsup q\u2208\u0393 |Q1 (q)\u2212Q\u20321 (q)| |q1| + \u2016Q2 (q)\u2212Q\u20322 (q)\u20162 \u2016q2\u20162\n\u2264 4 \u221a \u03b8n\n16000\u03b85/2n3/2p +\n4\n16000\u03b82np =\n1\n2000\u03b82np . (F.7)\nObserving that\ninf q\u2208\u0393 G\u2032(q) \u2265 inf q\u2208\u0393 ( |E [Q1 (q)]| |q1| \u2212 \u2016E [Q2 (q)]\u20162\u2016q2\u20162 ) \u2212 sup q\u2208\u0393 ( |Q1 (q)\u2212 E [Q1 (q)]| |q1| + \u2016Q2 (q)\u2212 E [Q2 (q)]\u20162 \u2016q2\u20162 ) \u2212 sup\nq\u2208\u0393 ( |Q1 (q)\u2212Q\u20321 (q)| |q1| + \u2016Q2 (q)\u2212Q\u20322 (q)\u20162 \u2016q2\u20162 ) , (F.8)\nwe obtain the following: Proposition F.1. There exists some constant \u03b80 > 0 such that, for all \u03b8 \u2208 (\n1\u221a n , \u03b80\n) , when exp (n/2) /2 \u2265 p \u2265\nCn4 log n for some large constant C > 0, we have\ninf q\u2208\u0393 G\u2032(q) \u2265 1 4000\u03b82np , , (F.9)\nwith probability at least 1\u2212 c\u2032 exp (\u2212c\u2032\u2032n) for some positive constants c\u2032 and c\u2032\u2032.\nFor the general case when the input orthonormal basis is Y\u0302 = Y\u2032R with target solution q? = R>e1, a straightforward extension of the definition for the gap would be:\nG\u2032 ( q; Y\u0302 = Y\u2032R ) . = \u2223\u2223\u2223\u2329Q\u2032 (q; Y\u0302) ,R>e1\u232a\u2223\u2223\u2223 |\u3008q,R>e1\u3009| \u2212 \u2225\u2225\u2225(I\u2212R>e1e>1 R)Q\u2032 (q; Y\u0302)\u2225\u2225\u2225 2\u2225\u2225(I\u2212R>e1e>1 R)q\u2225\u22252 . (F.10)\nSince Q\u2032 ( q; Y\u0302 ) = 1p \u2211p k=1 R >ykS\u03bb ( q>R>yk ) , we have\nRQ\u2032 ( q; Y\u0302 ) = 1\np p\u2211 k=1 RR>y\u2032kS\u03bb ( q>R>y\u2032k ) = 1 p p\u2211 k=1 y\u2032kS\u03bb [ (Rq) > y\u2032k ] = Q\u2032 (Rq; Y\u2032) . (F.11)\nHence we have\nG\u2032 ( q; Y\u0302 = Y\u2032R ) = |\u3008Q\u2032 (Rq; Y\u2032) , e1\u3009| |\u3008Rq, e1\u3009| \u2212 \u2225\u2225(I\u2212 e1e>1 )Q\u2032 (Rq; Y\u2032)\u2225\u22252\u2225\u2225(I\u2212 e1e>1 )Rq\u2225\u22252 . (F.12)\nTherefore, from Proposition F.1 above, we conclude that under the same technical conditions as therein,\ninf q\u2208Sn\u22121: 1\n4 \u221a \u03b8n \u2264|\u3008Rq,e1\u3009|\u22643\n\u221a \u03b8 G\u2032 ( q; Y\u0302 ) \u2265 1 4000\u03b82np (F.13)\nwith overwhelmingly probability.\nF.1 Lower Bounding the Expected Gap G(q) In this section, we provide a nontrivial lower bound for the gap\nG(q) = |E [Q1(q)]| |q1| \u2212 \u2016E [Q2(q)]\u20162\u2016q2\u20162 . (F.14)\nMore specifically, we show that:\nProposition F.2. There exists some numerical constant \u03b80 > 0, such that for all \u03b8 \u2208 (0, \u03b80), it holds that\nG(q) \u2265 1 50 q21 \u03b8p\n(F.15)\nfor all q \u2208 Sn\u22121 with |q1| \u2264 3 \u221a \u03b8.\nBecause the estimation for the gap G(q) involves dedicated estimation for E [Q1(q)] and E [Q2(q)], we sketch the main proof in Appendix F.1.1, and leave those detailed technical calculations in the subsequent subsections.\nF.1.1 Sketch of the Proof\nW.l.o.g., we only consider the situation that q1 > 0, because the case of q1 < 0 can be similarly shown by symmetry. By (D.5), (D.3) and (D.4), we have\nE [Q1(q)] = E [ x0S\u03bb [ x0q1 + q > 2 g ]] , (F.16)\nE [Q2(q)] = E [ gS\u03bb [ x0q1 + q > 2 g ]] , (F.17)\nwhere q = [ q1,q > 2 ]>, g \u223c N (0, 1pI), and x0 \u223c 1\u221a\u03b8pBer(\u03b8). Let us decompose g = g\u2016 + g\u22a5, (F.18)\nwith g\u2016 = P\u2016g = q2q > 2\n\u2016q2\u201622 g, and g\u22a5 = (I\u2212 P\u2016)g. Therefore, we have\nE [Q2(q)] = E [ g\u2016S\u03bb [ x0q1 + q > 2 g\u2016 ]] + E [ g\u22a5S\u03bb [ x0q1 + q > 2 g\u2016 ]] = E [ g\u2016S\u03bb [ x0q1 + q > 2 g ]] + E [g\u22a5]E [ S\u03bb [ x0q1 + q > 2 g ]]\n= q2 \u2016q2\u201622 E [ q>2 gS\u03bb [ x0q1 + q > 2 g ]] , (F.19)\nwhere we used the facts that q>2 g = q>2 g\u2016, g\u22a5 and g\u2016 are uncorrelated Gaussian vectors and therefore independent, and E [g\u22a5] = 0. Let Z . = g>q2 \u223c N (0, \u03c32) with \u03c32 = \u2016q2\u201622 /p, by partial evaluation of the expectations with respect to x0, we get\nE [Q1(q)] =\n\u221a \u03b8 p E [ S\u03bb [ q1\u221a \u03b8p + Z ]] , (F.20)\nE [Q2(q)] = \u03b8q2 \u2016q2\u201622 E [ ZS\u03bb [ q1\u221a \u03b8p + Z ]] + (1\u2212 \u03b8)q2 \u2016q2\u201622 E [ZS\u03bb [Z]] . (F.21)\nStraightforward integration based on Lemma A.1 gives a explicit form of the expectations as follows\nE [Q1(q)] =\n\u221a \u03b8\np\n{[ \u03b1\u03a8 ( \u2212\u03b1 \u03c3 ) + \u03b2\u03a8 ( \u03b2 \u03c3 )] + \u03c3 [ \u03c8 ( \u2212\u03b2 \u03c3 ) \u2212 \u03c8 ( \u2212\u03b1 \u03c3 )]} , (F.22)\nE [Q2(q)] = {\n2 (1\u2212 \u03b8) p \u03a8 ( \u2212\u03bb \u03c3 ) + \u03b8 p [ \u03a8 ( \u2212\u03b1 \u03c3 ) + \u03a8 ( \u03b2 \u03c3 )]} q2, (F.23)\nwhere the scalars \u03b1 and \u03b2 are defined as\n\u03b1 = q1\u221a \u03b8p + \u03bb, \u03b2 = q1\u221a \u03b8p \u2212 \u03bb, (F.24)\nand \u03c8 (t) and \u03a8 (t) are pdf and cdf for standard normal distribution, respectively, as defined in Lemma A.1. Plugging (F.22) and (F.23) into (F.14), by some simplifications, we obtain\nG(q) = 1\nq1\n\u221a \u03b8\np\n[ \u03b1\u03a8 ( \u2212\u03b1 \u03c3 ) + \u03b2\u03a8 ( \u03b2 \u03c3 ) \u2212 2q1\u221a \u03b8p \u03a8 ( \u2212\u03bb \u03c3 )] \u2212 \u03b8 p [ \u03a8 ( \u2212\u03b1 \u03c3 ) + \u03a8 ( \u03b2 \u03c3 ) \u2212 2\u03a8 ( \u2212\u03bb \u03c3 )]\n+ \u03c3\nq1\n\u221a \u03b8\np\n[ \u03c8 ( \u03b2\n\u03c3\n) \u2212 \u03c8 ( \u2212\u03b1 \u03c3 )] . (F.25)\nWith \u03bb = 1/\u221ap and \u03c32 = \u2016q2\u201622 /p = (1\u2212 q21)/p, we have\n\u2212\u03b1 \u03c3 = \u2212 \u03b4 + 1\u221a 1\u2212 q21 , \u03b2 \u03c3 = \u03b4 \u2212 1\u221a 1\u2212 q21 , \u03bb \u03c3 = 1\u221a 1\u2212 q21 , (F.26)\nwhere \u03b4 = q1/ \u221a \u03b8 for q1 \u2264 3 \u221a \u03b8. To proceed, it is natural to consider estimating the gap G(q) by Taylor\u2019s\nexpansion. More specifically, we approximate \u03a8 ( \u2212\u03b1\u03c3 ) and \u03c8 ( \u2212\u03b1\u03c3 ) around \u22121\u2212 \u03b4, and approximate \u03a8 ( \u03b2 \u03c3 ) and \u03c8 ( \u03b2 \u03c3 ) around \u22121 + \u03b4. Applying the estimates for the relevant quantities established in Lemma F.3, we obtain\nG(q) \u2265 1\u2212 \u03b8 p \u03a61(\u03b4)\u2212 1 \u03b4p \u03a62(\u03b4) + 1\u2212 \u03b8 p \u03c8(\u22121)q21 + 1 p\n( \u03c3 \u221a p+ \u03b8 2 \u2212 1 ) \u03b72(\u03b4)q 2 1\n+ 1\n2\u03b4p\n[ 1 + \u03b42 \u2212 \u03b8\u03b42 \u2212 \u03c3 ( 1 + \u03b42 )\u221a p ] q21\u03b71 (\u03b4) + \u03c3\n\u03b4 \u221a p \u03b71 (\u03b4)\u2212\n5CT \u221a \u03b8q31\np (\u03b4 + 1)\n3 , (F.27)\nwhere we define\n\u03a61(\u03b4) = \u03a8(\u22121\u2212 \u03b4) + \u03a8(\u22121 + \u03b4)\u2212 2\u03a8(\u22121), \u03a62(\u03b4) = \u03a8(\u22121 + \u03b4)\u2212\u03a8(\u22121\u2212 \u03b4), (F.28) \u03b71(\u03b4) = \u03c8(\u22121 + \u03b4)\u2212 \u03c8(\u22121\u2212 \u03b4), \u03b72(\u03b4) = \u03c8(\u22121 + \u03b4) + \u03c8(\u22121\u2212 \u03b4), (F.29)\nand CT is as defined in Lemma F.3. Since 1\u2212 \u03c3\u221ap \u2265 0, dropping those small positive terms q 2 1\np (1\u2212 \u03b8)\u03c8(\u22121), \u03b8q21 2p \u03b72(\u03b4), and ( 1 + \u03b42 ) ( 1\u2212 \u03c3\u221ap ) q21\u03b71 (\u03b4) / (2\u03b4p), and using the fact that \u03b4 = q1/ \u221a \u03b8, we obtain\nG(q) \u2265 1\u2212 \u03b8 p \u03a61(\u03b4)\u2212 1 \u03b4p [\u03a62(\u03b4)\u2212 \u03c3 \u221a p\u03b71(\u03b4)]\u2212 q21 p\n(1\u2212 \u03c3\u221ap) \u03b72(\u03b4)\u2212 \u221a \u03b8\n2p q31\u03b71 (\u03b4)\u2212 C1 \u221a \u03b8q31 p max ( q31 \u03b83/2 , 1 ) \u2265 1\u2212 \u03b8\np \u03a61(\u03b4)\u2212\n1\n\u03b4p [\u03a62(\u03b4)\u2212 \u03b71(\u03b4)]\u2212 q21 p \u03b71 (\u03b4) \u03b4 \u2212 q 2 1 p\u03b8 2\u221a 2\u03c0 \u03b8 \u2212 q 2 1 \u03b8p 3\u03b82 2 \u221a 2\u03c0 \u2212 q 2 1 \u03b8p\n( C1\u03b8 2 ) , (F.30)\nfor some constant C1 > 0, where we have used q1 \u2264 3 \u221a \u03b8 to simplify the bounds and the fact \u03c3\u221ap =\u221a\n1\u2212 q21 \u2265 1 \u2212 q21 to simplify the expression. Substituting the estimates in Lemma F.5 and use the fact \u03b4 7\u2192 \u03b71 (\u03b4) /\u03b4 is bounded, we obtain\nG (p) \u2265 1 p\n( 1\n40 \u2212 1 2 \u221a 2\u03c0 \u03b8\n) \u03b42 \u2212 q 2 1\n\u03b8p\n( c1\u03b8 + c2\u03b8 2 )\n(F.31)\n\u2265 q 2 1\n\u03b8p\n( 1\n40 \u2212 1\u221a 2\u03c0 \u03b8 \u2212 c1\u03b8 \u2212 c2\u03b82\n) (F.32)\nfor some positive constants c1 and c2. We obtain the claimed result once \u03b80 is made sufficiently small.\nF.1.2 Auxiliary Results Used in the Proof Lemma F.3. Let \u03b4 .= q1/ \u221a \u03b8. There exists some universal constant CT > 0 such that we have the follow polynomial\napproximations hold for all q1 \u2208 ( 0, 12 ) :\u2223\u2223\u2223\u2223\u03c8 (\u2212\u03b1\u03c3)\u2212 [ 1\u2212 1 2 (1 + \u03b4)2q21 ] \u03c8(\u22121\u2212 \u03b4)\n\u2223\u2223\u2223\u2223 \u2264 CT (1 + \u03b4)2 q41 , (F.33)\u2223\u2223\u2223\u2223\u03c8(\u03b2\u03c3 ) \u2212 [ 1\u2212 1 2 (\u03b4 \u2212 1)2q21 ] \u03c8(\u03b4 \u2212 1)\n\u2223\u2223\u2223\u2223 \u2264 CT (\u03b4 \u2212 1)2 q41 , (F.34)\u2223\u2223\u2223\u2223\u03a8(\u2212\u03b1\u03c3)\u2212 [ \u03a8(\u22121\u2212 \u03b4)\u2212 1 2 \u03c8(\u22121\u2212 \u03b4)(1 + \u03b4)q21\n]\u2223\u2223\u2223\u2223 \u2264 CT (1 + \u03b4)2 q41 , (F.35)\u2223\u2223\u2223\u2223\u03a8(\u03b2\u03c3 ) \u2212 [ \u03a8(\u03b4 \u2212 1) + 1 2 \u03c8(\u03b4 \u2212 1)(\u03b4 \u2212 1)q21\n]\u2223\u2223\u2223\u2223 \u2264 CT (\u03b4 \u2212 1)2 q41 , (F.36)\u2223\u2223\u2223\u2223\u03a8(\u2212\u03bb\u03c3 ) \u2212 [ \u03a8(\u22121)\u2212 1 2 \u03c8(\u22121)q21\n]\u2223\u2223\u2223\u2223 \u2264 CT q41 . (F.37) Proof. First observe that for any q1 \u2208 ( 0, 12 ) it holds that\n0 \u2264 1\u221a 1\u2212 q21\n\u2212 (\n1 + q21 2\n) \u2264 q41 . (F.38)\nHence we have\n\u2212(1 + \u03b4) ( 1 + 1\n2 q21 + q 4 1\n) \u2264 \u2212\u03b1\n\u03c3 \u2264 \u2212(1 + \u03b4)\n( 1 + 1\n2 q21\n) , (F.39)\n(\u03b4 \u2212 1) ( 1 + 1\n2 q21 ) \u2264 \u03b2 \u03c3 \u2264 (\u03b4 \u2212 1) ( 1 + 1 2 q21 + q 4 1 ) , when \u03b4 \u2265 1\n(\u03b4 \u2212 1) ( 1 + 1\n2 q21 + q 4 1 ) \u2264 \u03b2 \u03c3 \u2264 (\u03b4 \u2212 1) ( 1 + 1 2 q21 ) , when \u03b4 \u2264 1. (F.40)\nSo we have\n\u03c8 ( \u2212(1 + \u03b4) ( 1 + 1\n2 q21 + q 4 1\n)) \u2264 \u03c8 ( \u2212\u03b1 \u03c3 ) \u2264 \u03c8 ( \u2212(1 + \u03b4) ( 1 + 1 2 q21 )) . (F.41)\nBy Taylor expansion of the left and right sides of the above two-side inequality around \u22121\u2212 \u03b4 using Lemma A.2, we obtain \u2223\u2223\u2223\u2223\u03c8 (\u2212\u03b1\u03c3)\u2212 \u03c8(\u22121\u2212 \u03b4)\u2212 12(1 + \u03b4)2q21\u03c8(\u22121\u2212 \u03b4)\n\u2223\u2223\u2223\u2223 \u2264 CT (1 + \u03b4)2 q41 , (F.42) for some numerical constant CT > 0 sufficiently large. In the same way, we can obtain other claimed results.\nLemma F.4. For any \u03b4 \u2208 [0, 3], it holds that\n\u03a62(\u03b4)\u2212 \u03b71(\u03b4) \u2265 \u03b71 (3) 9 \u03b43 \u2265 1 20 \u03b43. (F.43)\nProof. Let us define\nh(\u03b4) = \u03a62(\u03b4)\u2212 \u03b71(\u03b4)\u2212 C\u03b43 (F.44)\nfor some C > 0 to be determined later. Then it is obvious that h(0) = 0. Direct calculation shows that\nd\nd\u03b4 \u03a61(\u03b4) = \u03b71(\u03b4),\nd\nd\u03b4 \u03a62(\u03b4) = \u03b72(\u03b4),\nd\nd\u03b4 \u03b71(\u03b4) = \u03b72(\u03b4)\u2212 \u03b4\u03b71(\u03b4). (F.45)\nThus, to show (F.43), it is sufficient to show that h\u2032(\u03b4) \u2265 0 for all \u03b4 \u2208 [0, 3]. By differentiating h(\u03b4) with respect to \u03b4 and use the results in (F.45), it is sufficient to have\nh\u2032(\u03b4) = \u03b4\u03b71(\u03b4)\u2212 3C\u03b42 \u2265 0\u21d0\u21d2 \u03b71(\u03b4) \u2265 3C\u03b4 (F.46)\nfor all \u03b4 \u2208 [0, 3]. We obtain the claimed result by observing that \u03b4 7\u2192 \u03b71 (\u03b4) /3\u03b4 is monotonically decreasing over \u03b4 \u2208 [0, 3] as justified below.\nConsider the function\np (\u03b4) . = \u03b71 (\u03b4)\n3\u03b4 =\n1 3 \u221a 2\u03c0 exp\n( \u2212\u03b4 2 + 1\n2\n) e\u03b4 \u2212 e\u2212\u03b4\n\u03b4 . (F.47)\nTo show it is monotonically decreasing, it is enough to show p\u2032 (\u03b4) is always nonpositive for \u03b4 \u2208 (0, 3), or equivalently\ng (\u03b4) . = ( e\u03b4 + e\u2212\u03b4 ) \u03b4 \u2212 ( \u03b42 + 1 ) ( e\u03b4 \u2212 e\u2212\u03b4 ) \u2264 0 (F.48)\nfor all \u03b4 \u2208 (0, 3), which can be easily verified by noticing that g (0) = 0 and g\u2032 (\u03b4) \u2264 0 for all \u03b4 \u2265 0.\nLemma F.5. For any \u03b4 \u2208 [0, 3], we have\n(1\u2212 \u03b8)\u03a61(\u03b4)\u2212 1\n\u03b4 [\u03a62(\u03b4)\u2212 \u03b71(\u03b4)] \u2265\n( 1\n40 \u2212 1\u221a 2\u03c0 \u03b8\n) \u03b42. (F.49)\nProof. Let us define\ng(\u03b4) = (1\u2212 \u03b8)\u03a61(\u03b4)\u2212 1\n\u03b4 [\u03a62(\u03b4)\u2212 \u03b71(\u03b4)]\u2212 c0 (\u03b8) \u03b42, (F.50)\nwhere c0 (\u03b8) > 0 is a function of \u03b8. Thus, by the results in (F.45) and L\u2019Hospital\u2019s rule, we have\nlim \u03b4\u21920\n\u03a62(\u03b4)\n\u03b4 = lim \u03b4\u21920 \u03b72 (\u03b4) = 2\u03c8(\u22121), lim \u03b4\u21920\n\u03b71(\u03b4)\n\u03b4 = lim \u03b4\u21920\n[\u03b72(\u03b4)\u2212 \u03b4\u03b71(\u03b4)] = 2\u03c8(\u22121). (F.51)\nCombined that with the fact that \u03a61(0) = 0, we conclude g (0) = 0. Hence, to show (F.49), it is sufficient to show that g\u2032(\u03b4) \u2265 0 for all \u03b4 \u2208 [0, 3]. Direct calculation using the results in (F.45) shows that\ng\u2032(\u03b4) = 1\n\u03b42 [\u03a62(\u03b4)\u2212 \u03b71(\u03b4)]\u2212 \u03b8\u03b71(\u03b4)\u2212 2c0 (\u03b8) \u03b4. (F.52)\nSince \u03b71 (\u03b4) /\u03b4 is monotonically decreasing as shown in Lemma F.4, we have that for all \u03b4 \u2208 (0, 3)\n\u03b71 (\u03b4) \u2264 \u03b4 lim \u03b4\u21920\n\u03b7 (\u03b4) \u03b4 \u2264 2\u221a 2\u03c0 \u03b4. (F.53)\nUsing the above bound and the main result from Lemma F.4 again, we obtain\ng\u2032(\u03b4) \u2265 1 20 \u03b4 \u2212 2\u221a 2\u03c0 \u03b8\u03b4 \u2212 2c0\u03b4. (F.54)\nChoosing c0 (\u03b8) = 140 \u2212 1\u221a2\u03c0 \u03b8 completes the proof.\nF.2 Finite Sample Concentration In the following two subsections, we estimate the deviations around the expectations EQ1 and EQ2, i.e., |Q1 \u2212 EQ1| and \u2016Q2 \u2212 EQ2\u20162, and show that the total deviations fit into the gapG(q)wederived inAppendix F.1. Our analysis is based on the scalar and vector Bernstein\u2019s inequalities with moment conditions. Finally, in Appendix F.3, we uniform the bound by applying the classical discretization argument.\nF.2.1 Concentration for Q1\nLemma F.6 (Bounding |Q1 \u2212 E [Q1(q)]|). For each q \u2208 Sn\u22121, it holds for all t > 0 that\nP [|Q1(q)\u2212 E [Q1(q)]| \u2265 t] \u2264 2 exp ( \u2212 \u03b8p 3t2\n8 + 4pt\n) . (F.55)\nProof. By (D.3) and (D.5), we know that\nQ1(q) = 1\np p\u2211 k=1 X1k , X 1 k = x0kS\u03bb [x0kq1 + Zk] (F.56)\nwhere Zk \u223c N ( 0, \u2016q2\u201622 p ) . Thus, for anym \u2265 2, by Lemma A.4, we have\nE [\u2223\u2223X1k \u2223\u2223m] \u2264 \u03b8( 1\u221a\u03b8p )m E [\u2223\u2223\u2223\u2223 q1\u221a\u03b8p + Zk \u2223\u2223\u2223\u2223m] = \u03b8 ( 1\u221a \u03b8p )m m\u2211 l=0 ( m l )( q1\u221a \u03b8p )l E [ |Zk|m\u2212l\n] = \u03b8 ( 1\u221a \u03b8p )m m\u2211 l=0 ( m l )( q1\u221a \u03b8p )l (m\u2212 l \u2212 1)!! (\u2016q2\u20162\u221a p\n)m\u2212l \u2264 m!\n2 \u03b8 ( 1\u221a \u03b8p )m( q1\u221a \u03b8p + \u2016q2\u20162\u221a p )m \u2264 m!\n2 \u03b8\n( 2\n\u03b8p\n)m = m!\n2\n4\n\u03b8p2\n( 2\n\u03b8p\n)m\u22122 (F.57)\nlet \u03c32X = 4/(\u03b8p2) and R = 2/(\u03b8p), apply Lemma A.7, we get P [|Q1(q)\u2212 E [Q1(q)]| \u2265 t] \u2264 2 exp ( \u2212 \u03b8p 3t2\n8 + 4pt\n) . (F.58)\nas desired.\nF.2.2 Concentration for Q2\nLemma F.7 (Bounding \u2016Q2 \u2212 E [Q2]\u20162). For each q \u2208 Sn\u22121, it holds for all t > 0 that\nP [\u2016Q2(q)\u2212 E [Q2(q)]\u20162 > t] \u2264 2(n+ 1) exp ( \u2212 \u03b8p 3t2\n128n+ 16 \u221a \u03b8npt\n) . (F.59)\nBefore proving Lemma F.7, we record the following useful results.\nLemma F.8. For any positive integer s, l > 0, we have\nE [\u2225\u2225gk\u2225\u2225s\n2 \u2223\u2223q>2 gk\u2223\u2223l] \u2264 (l + s)!2 \u2016q2\u2016l2 (2 \u221a n) s(\u221a p )s+l (F.60)\nIn particular, when s = l, we have\nE [\u2225\u2225gk\u2225\u2225l\n2 \u2223\u2223q>2 gk\u2223\u2223l] \u2264 l!2 \u2016q2\u2016l2 ( 4 \u221a n p )l (F.61)\nProof. Let P q \u2016 2\n= q2q > 2\n\u2016q2\u201622 and Pq\u22a52 = ( I\u2212 1\u2016q2\u201622 q2q > 2 ) denote the projection operators onto q2 and its orthogo-\nnal complement, respectively. By Lemma A.4, we have E [\u2225\u2225gk\u2225\u2225s\n2 \u2223\u2223q>2 gk\u2223\u2223l] \u2264 E [(\u2225\u2225\u2225Pq\u20162gk\u2225\u2225\u22252 + \u2225\u2225\u2225Pq\u22a52 gk\u2225\u2225\u22252)s \u2223\u2223q>2 gk\u2223\u2223l] =\ns\u2211 i=0 ( s i ) E [\u2225\u2225\u2225Pq\u22a52 gk\u2225\u2225\u2225i2 ] E [\u2223\u2223q>2 gk\u2223\u2223l \u2225\u2225\u2225Pq\u20162gk\u2225\u2225\u2225s\u2212i2 ]\n= s\u2211 i=0 ( s i ) E [\u2225\u2225\u2225Pq\u22a52 gk\u2225\u2225\u2225i2 ] E [\u2223\u2223q>2 gk\u2223\u2223l+s\u2212i] 1\u2016q2\u2016s\u2212i2\n\u2264 \u2016q2\u2016l2 s\u2211 i=0 ( s i ) E [\u2225\u2225\u2225Pq\u22a52 gk\u2225\u2225\u2225i2 ]( 1\u221a p )l+s\u2212i (l + s\u2212 i\u2212 1)!!. (F.62)\nUsing Lemma A.5 and the fact that \u2225\u2225\u2225Pq\u22a52 gk\u2225\u2225\u222522 \u2264 \u2225\u2225gk\u2225\u222522, we obtain\nE [\u2225\u2225gk\u2225\u2225s\n2 \u2223\u2223q>2 gk\u2223\u2223l] \u2264 \u2016q2\u2016l2 s\u2211 i=0 ( s i )(\u221a n\u221a p )i i! ( 1\u221a p )l+s\u2212i (l + s\u2212 i\u2212 1)!!\n\u2264 \u2016q2\u2016l2 (\n1\u221a p\n)l (l + s)!\n2\n(\u221a n\u221a p + 1\u221a p )s \u2264 (l + s)!\n2 \u2016q2\u2016l2\n(2 \u221a n) s(\u221a p )s+l . (F.63)\nNow, we are ready to prove Lemma F.7,\nProof. By (D.5) and (D.4), note that\nQ2 = 1\np p\u2211 k=1 X2k, X 2 k = g kS\u03bb [x0kq1 + Zk] (F.64)\nwhere Zk = q>2 gk. Thus, for anym \u2265 2, by Lemma F.8, we have\nE [\u2225\u2225X2k\u2225\u2225m2 ] \u2264 \u03b8E [\u2225\u2225gk\u2225\u2225m2 \u2223\u2223\u2223\u2223 q1\u221a\u03b8p + q>2 gk \u2223\u2223\u2223\u2223m]+ (1\u2212 \u03b8)E [\u2225\u2225gk\u2225\u2225m2 \u2223\u2223q>2 gk\u2223\u2223m] \u2264 \u03b8\nm\u2211 l=0 ( m l ) E [\u2223\u2223q>2 gk\u2223\u2223l \u2225\u2225gk\u2225\u2225m2 ] \u2223\u2223\u2223\u2223 q1\u221a\u03b8p \u2223\u2223\u2223\u2223m\u2212l + (1\u2212 \u03b8)E [\u2225\u2225gk\u2225\u2225m2 \u2223\u2223q>2 gk\u2223\u2223m] \u2264 \u03b8 ( 2 \u221a n\u221a p )m m\u2211 l=0 ( m l ) (m+ l)! 2 (\u2016q2\u20162\u221a p )l \u2223\u2223\u2223\u2223 q1\u221a\u03b8p \u2223\u2223\u2223\u2223m\u2212l + (1\u2212 \u03b8)m!2 \u2016q2\u2016m2 ( 4 \u221a n p\n)m \u2264 \u03b8m!\n2 ( 4 \u221a n\u221a p )m(\u2016q2\u20162\u221a p + q1\u221a \u03b8p )m + (1\u2212 \u03b8)m! 2 \u2016q2\u2016m2 ( 4 \u221a n p )m (F.65)\n\u2264 m! 2 ( 8 \u221a n\u221a \u03b8p )m . (F.66)\nTaking \u03c32X = 64n/(\u03b8p2) and R = 8 \u221a n/( \u221a \u03b8p) and using vector Bernstein\u2019s inequality in Lemma A.8, we obtain\nP [\u2016Q2(q)\u2212 E [Q2(q)]\u20162 \u2265 t] \u2264 2(n+ 1) exp ( \u2212 \u03b8p 3t2\n128n+ 16 \u221a \u03b8npt\n) , (F.67)\nas desired.\nF.3 Union Bound Proposition F.9 (Uniformizing the Bounds). Suppose that \u03b8 > 1\u221a\nn . Given any \u03be > 0, there exists some constant\nC (\u03be), such that whenever exp (n) \u2265 p \u2265 C (\u03be)n4 log n, we have\n|Q1(q)\u2212 E [Q1(q)]| \u2264 2\u03be\n\u03b85/2n3/2p , (F.68)\n\u2016Q2(q)\u2212 E [Q2(q)]\u20162 \u2264 2\u03be\n\u03b82np (F.69)\nhold uniformly for all q \u2208 Sn\u22121, with probability at least 1\u2212 c\u2032 exp (\u2212c\u2032\u2032n) for some positive constants c\u2032 and c\u2032\u2032.\nProof. We apply the standard covering argument. For any \u03b5 \u2208 (0, 1), by Lemma A.12, the unit hemisphere of interest can be covered by an \u03b5-net N\u03b5 of cardinality at most (3/\u03b5)n. For any q \u2208 Sn\u22121, it can be written as\nq = q\u2032 + e (F.70)\nwhere q\u2032 \u2208 N\u03b5 and \u2016e\u20162 \u2264 \u03b5. Let yk = [ x0k,g k ]> be a row of Y, by (D.3) and (D.5), we have\n|Q1(q)\u2212 E [Q1(q)]|\n= \u2223\u2223\u2223\u2223\u22231p p\u2211 k=1 { x0kS\u03bb [\u2329 yk,q\u2032 + e \u232a] \u2212 E [ x0kS\u03bb [\u2329 yk,q\u2032 + e \u232a]]}\u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u22231p p\u2211 k=1 x0kS\u03bb [\u2329 yk,q\u2032 + e \u232a] \u2212 1 p p\u2211 k=1 x0kS\u03bb [\u2329 yk,q\u2032 \u232a]\u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u22231p p\u2211 k=1 x0kS\u03bb [\u2329 yk,q\u2032 \u232a] \u2212 E [x0S\u03bb [\u3008y,q\u2032\u3009]]\n\u2223\u2223\u2223\u2223\u2223 + |E [x0S\u03bb [\u3008y,q\u2032\u3009]]\u2212 E [x0S\u03bb [\u3008y,q\u2032 + e\u3009]]| . (F.71)\nUsing Cauchy-Schwarz inequality and the fact that S\u03bb [\u00b7] is a nonexpansive operator, we have\n|Q1(q)\u2212 E [Q1(q)]| \u2264 |Q1(q\u2032)\u2212 E [Q1(q\u2032)]|+ ( 1\np p\u2211 k=1 |x0k| \u2225\u2225yk\u2225\u2225 2 + E [|x0| \u2016y\u20162] ) \u2016e\u20162 (F.72)\n\u2264 |Q1(q\u2032)\u2212 E [Q1(q\u2032)]|+ 2\u03b5 1\u221a \u03b8p ( 1\u221a \u03b8p + max k\u2208[p] \u2225\u2225gk\u2225\u2225 2 ) . (F.73)\nBy Lemma A.11 and the assumption that p \u2264 exp (n), we have that maxk\u2208[p] \u2225\u2225gk\u2225\u2225 2 \u2264 4 \u221a n/pwith probability at least 1 \u2212 exp (\u2212n/2). Taking t = \u03be\u03b8\u22125/2n\u22123/2p\u22121 in Lemma F.6 and applying a union bound, setting \u03b5 = \u03be\u03b8\u22122n\u22122/10 and combining with the above estimate, we obtain that\n|Q1(q)\u2212 E [Q1(q)]| \u2264 \u03be \u03b85/2n3/2p + \u03be 5 1 \u03b82n2 5 \u221a n\u221a \u03b8p \u2264 2\u03be \u03b85/2n3/2p\n(F.74)\nholds for all q \u2208 Sn\u22121, with probability at least 1 \u2212 exp (\u2212n/2) \u2212 exp ( \u2212 c1(\u03be)p\u03b84n3 + c2 (\u03be)n log n ) for some numerical constants c1 (\u03be) and c2 (\u03be).\nSimilarly, by (D.3) and (D.5), we have \u2016Q2(q)\u2212 E [Q2(q)]\u20162 = \u2225\u2225\u2225\u2225\u22251p p\u2211 k=1 { gkS\u03bb [\u2329 yk,q\u2032 + e \u232a] \u2212 E [ gkS\u03bb [\u2329 yk,q\u2032 + e \u232a]]}\u2225\u2225\u2225\u2225\u2225 2\n\u2264 \u2016Q2(q\u2032)\u2212 E [Q2(q\u2032)]\u20162 + ( 1\np p\u2211 k=1 \u2225\u2225gk\u2225\u2225 2 \u2225\u2225yk\u2225\u2225 2 + E [\u2225\u2225gk\u2225\u2225 2 \u2225\u2225yk\u2225\u2225 2 ]) \u2016e\u20162\n\u2264 \u2016Q2(q\u2032)\u2212 E [Q2(q\u2032)]\u20162 + 2\u03b5max k\u2208[p] \u2225\u2225gk\u2225\u2225 2 ( 1\u221a \u03b8p + max k\u2208[p] \u2225\u2225gk\u2225\u2225 2 ) . (F.75)\nApplying the above estimates for maxk\u2208[p] \u2225\u2225gk\u2225\u2225 2 , and taking t = \u03be\u03b8\u22122n\u22121p\u22121 in Lemma F.7 and applying a union bound, then setting \u03b5 = \u03be\u03b8\u22122n\u22122/40, we obtain that\n\u2016Q2(q)\u2212 E [Q2(q)]\u20162 \u2264 \u03be\n\u03b82np +\n\u03be\n20\u03b82n2 4\n\u221a n\np ( 1\u221a \u03b8p + 4 \u221a n p ) \u2264 2\u03be \u03b82np\n(F.76)\nholds for all q \u2208 Sn\u22121, with probability at least 1\u2212 exp (\u2212n/2)\u2212 exp ( \u2212 c3(\u03be)p\u03b83n3 + c4 (\u03be)n log n ) .\nOverall, it is enough to take p \u2265 Cn4 log n for some large C to make the above events to hold with overwhelming probability, as desired.\nF.4 Q\u2032(q) approximates Q(q) Proposition F.10. Suppose \u03b8 > 1\u221a\nn . For any \u03be > 0, there exists some constant C (\u03be), such that whenever\nexp (n/2) /2 \u2265 p \u2265 C (\u03be)n4 log n, the following bounds\nsup q\u2208Sn\u22121\n|Q\u20321(q)\u2212Q1(q)| \u2264 \u03be\n\u03b85/2n3/2p (F.77)\nsup q\u2208Sn\u22121\n\u2016Q\u20322(q)\u2212Q2(q)\u20162 \u2264 \u03be\n\u03b82np , (F.78)\nhold for all q \u2208 Sn\u22121, with probability at least 1\u2212 c\u2032 exp (\u2212c\u2032\u2032n) for some positive constants c\u2032 and c\u2032\u2032. Proof. First, for any q \u2208 Sn\u22121, from D.5, we know that\n|Q1(q)\u2212Q\u20321(q)|\n= \u2223\u2223\u2223\u2223\u22231p p\u2211 k=1 x0kS\u03bb [ q>yk ] \u2212 1 p p\u2211 k=1 x0k \u2016x0\u20162 S\u03bb [ q>y\u2032k ]\u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u22231p p\u2211 k=1 x0kS\u03bb [ q>yk ] \u2212 1 p p\u2211 k=1 x0kS\u03bb [ q>y\u2032k ]\u2223\u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223\u22231p p\u2211 k=1 x0kS\u03bb [ q>y\u2032k ] \u2212 1 p p\u2211 k=1 x0k \u2016x0\u20162 S\u03bb [ q>y\u2032k\n]\u2223\u2223\u2223\u2223\u2223 \u2264 1\np p\u2211 k=1 |x0k| \u2223\u2223S\u03bb [q>yk]\u2212 S\u03bb [q>y\u2032k]\u2223\u2223+ 1 p p\u2211 k=1 |x0k| \u2223\u2223\u2223\u22231\u2212 1\u2016x0\u20162 \u2223\u2223\u2223\u2223 \u2223\u2223S\u03bb [q>y\u2032k]\u2223\u2223 . (F.79) Let I = supp(x0). Conditioned on the support, using the facts that S\u03bb[\u00b7] is a nonexpansive operator, we obtain\nsup q\u2208Sn\u22121\n|Q1(q)\u2212Q\u20321(q)| \u2264 1\np sup\nq\u2208Sn\u22121 \u2211 k\u2208I |x0k| \u2225\u2225q> (yk \u2212 y\u2032k)\u2225\u2225 2 + \u2223\u2223\u2223\u22231\u2212 1\u2016x0\u20162 \u2223\u2223\u2223\u2223 1p supq\u2208Sn\u22121\u2211k\u2208I |x0k| \u2223\u2223q>y\u2032k\u2223\u2223 =\n1\u221a \u03b8p3/2\n( \u2016YI \u2212Y\u2032I\u2016`2\u2192`1 + \u2223\u2223\u2223\u22231\u2212 1\u2016x0\u20162 \u2223\u2223\u2223\u2223 \u2016Y\u2032I\u2016`2\u2192`1) . (F.80)\nBy Lemma B.1 and Lemma B.3 in Appendix B, we have the following holds\nsup q\u2208Sn\u22121 |Q1(q)\u2212Q\u20321(q)| \u2264 1\u221a \u03b8p3/2\n( 10\n\u03b8\n\u221a n log p+ 2 \u221a 2\n5\n\u221a n log p\n\u03b83p \u00d7 7 \u221a 2\u03b8p ) \u2264 16 \u03b83/2p3/2 \u221a n log p, (F.81)\nwith probability at least 1 \u2212 c1 exp (\u2212c2n) for some positive constants c1 and c2. Since the above holds uniformly for any support pattern I, we conclude that\nsup q\u2208Sn\u22121\n|Q1(q)\u2212Q\u20321(q)| \u2264 16\n\u03b83/2p3/2\n\u221a n log p (F.82)\nwith probability at least 1\u2212 c1 exp (\u2212c2n). Now it is sufficient to let p \u2265 C (\u03be)n4 log n for some C (\u03be) > 0 to obtain the claimed result.\nSimilarly, by Lemma B.3 and Lemma B.4 in Appendix B, we have\nsup q\u2208Sn\u22121\n\u2016Q2(q)\u2212Q\u20322(q)\u20162\n= sup q\u2208Sn\u22121 \u2225\u2225\u2225\u2225\u22251p p\u2211 k=1 gkS\u03bb [ q>yk ] \u2212 1 p p\u2211 k=1 g\u2032kS\u03bb [ q>y\u2032k ]\u2225\u2225\u2225\u2225\u2225 2\n\u2264 sup q\u2208Sn\u22121 \u2225\u2225\u2225\u2225\u22251p p\u2211 k=1 gkS\u03bb [ q>yk ] \u2212 1 p p\u2211 k=1 g\u2032kS\u03bb [ q>yk ]\u2225\u2225\u2225\u2225\u2225 2 + \u2225\u2225\u2225\u2225\u22251p p\u2211 k=1 g\u2032kS\u03bb [ q>yk ] \u2212 1 p p\u2211 k=1 g\u2032kS\u03bb [ q>y\u2032k ]\u2225\u2225\u2225\u2225\u2225 2\n\u22641 p sup q\u2208Sn\u22121 p\u2211 k=1 \u2225\u2225gk \u2212 g\u2032k\u2225\u2225 2 \u2223\u2223q>yk\u2223\u2223+ 1 p sup q\u2208Sn\u22121 p\u2211 k=1 \u2225\u2225g\u2032k\u2225\u2225 2 \u2223\u2223q> (yk \u2212 y\u2032k)\u2223\u2223 \u22641 p (\u2016G\u2212G\u2032\u2016`2\u2192`\u221e \u2016Y\u2016`2\u2192`1 + \u2016G\u2032\u2016`2\u2192`\u221e \u2016Y \u2212Y\u2032\u2016`2\u2192`1)\n\u22641 p ( 32n\u221a \u03b8p \u00d7 3\u221ap+ 16 \u221a n \u03b8p \u00d7 10 \u03b8 \u221a n log p ) \u2264 192n \u221a log p \u03b83/2p3/2 (F.83)\nholds conditioned on any support pattern I, with probability at least 1 \u2212 c3 exp (\u2212c4n) for some positive constants c3 and c4, which similarly implies the bound holds uniformly, regardless of the support, with the same probability. It is sufficiently to have exp (n/2) /2 \u2265 p \u2265 C2 (\u03be)n4 log n to obtain the claimed result.\nG Large |q1| Iterates Staying in Safe Region for Rounding Proposition G.1. There exists a constant \u03b80 > 0, such that for any \u03b8 \u2208 ( 1\u221a n , \u03b80 ) , whenever exp (n) \u2265 p \u2265 Cn4 log n for some large constant C > 0, we have\n|Q\u20321(q)| \u2016Q\u2032(q)\u20162 \u2265 2 \u221a \u03b8, (G.1)\nfor all q \u2208 Sn\u22121 satisfying |q1| > 3 \u221a \u03b8, with probability at least 1\u2212 c\u2032 exp (\u2212c\u2032\u2032n) for some positive constants c\u2032 and c\u2032\u2032.\nProof. For notational simplicity, w.l.o.g. we will proceed to prove assuming q1 > 0. The proof for q1 < 0 is similar by symmetry. It is equivalent to show that\n\u2016Q\u20322 (q)\u20162 |Q\u20321 (q)| <\n\u221a 1\n4\u03b8 \u2212 1, (G.2)\nwhich is implied by\nL (q) .= \u2016EQ2(q)\u20162 + \u2016Q \u2032 2(q)\u2212 EQ2(q)\u20162\nEQ1 (q)\u2212 |Q\u20321 (q)\u2212 EQ1 (q)| <\n\u221a 1\n4\u03b8 \u2212 1 (G.3)\nfor any q \u2208 Sn\u22121 satisfying q1 > 3 \u221a \u03b8. Recall from (F.22) that\nEQ1(q) =\n\u221a \u03b8\np\n{[ \u03b1\u03a8 ( \u2212\u03b1 \u03c3 ) + \u03b2\u03a8 ( \u03b2 \u03c3 )] + \u03c3 [ \u03c8 ( \u03b2 \u03c3 ) \u2212 \u03c8 ( \u2212\u03b1 \u03c3 )]} , (G.4)\nwhere\n\u03b1 = 1\u221a p ( q1\u221a \u03b8 + 1 ) , \u03b2 = 1\u221a p ( q1\u221a \u03b8 \u2212 1 ) , \u03c3 = \u2016q2\u20162 / \u221a p. (G.5)\nNoticing the fact that\n\u03c8\n( \u03b2\n\u03c3\n) \u2212 \u03c8 ( \u2212\u03b1 \u03c3 ) \u2265 0, (G.6)\n\u03a8\n( \u03b2\n\u03c3\n) = \u03a8 ( 1\u221a\n1\u2212 q21\n( q1\u221a \u03b8 \u2212 1 )) \u2265 \u03a8 (2) \u2265 19 20 for q1 > 3 \u221a \u03b8, (G.7)\nwe have\nEQ1 (q) \u2265 \u221a \u03b8\np { q1\u221a \u03b8 [ \u03a8 ( \u2212\u03b1 \u03c3 ) + \u03a8 ( \u03b2 \u03c3 )] + \u03a8 ( \u2212\u03b1 \u03c3 ) \u2212\u03a8 ( \u03b2 \u03c3 )} \u2265 2 \u221a \u03b8 p \u03a8 ( \u03b2 \u03c3 ) \u2265 19 10 \u221a \u03b8 p . (G.8)\nMoreover, from (F.23), we have \u2016EQ2 (q)\u20162 = \u2016q2\u20162 { 2 (1\u2212 \u03b8) p \u03a8 ( \u2212\u03bb \u03c3 ) + \u03b8 p [ \u03a8 ( \u2212\u03b1 \u03c3 ) + \u03a8 ( \u03b2 \u03c3 )]} (G.9)\n\u2264 2 (1\u2212 \u03b8) p \u03a8 (\u22121) + \u03b8 p [\u03a8 (\u22121) + 1] \u2264 2 p \u03a8 (\u22121) + \u03b8 p \u2264 2 5p + \u03b8 p , (G.10)\nwhere we have used the fact that \u2212\u03bb/\u03c3 \u2264 \u22121 and \u2212\u03b1/\u03c3 \u2264 \u22121. Moreover, from results in Proposition F.9 and Proposition F.10 in Appendix F , we know that\nsup q\u2208Sn\u22121 |Q\u20321(q)\u2212 EQ1(q)| \u2264 sup q\u2208Sn\u22121 |Q\u20321(q)\u2212Q1(q)|+ sup q\u2208Sn\u22121 |Q1(q)\u2212 EQ1(q)| \u2264 1 8000\u03b85/2n3/2p , (G.11)\nsup q\u2208Sn\u22121 \u2016Q\u2032(q)\u2212 EQ(q)\u20162 \u2264 sup q\u2208Sn\u22121 \u2016Q\u2032(q)\u2212Q(q)\u20162 + sup q\u2208Sn\u22121 \u2016Q(q)\u2212 EQ(q)\u20162 \u2264 1 8000\u03b82np (G.12)\nhold with probability at least 1\u2212 c\u2032 exp (\u2212c\u2032\u2032n) for some positive constants c\u2032 and c\u2032\u2032 when p \u2265 \u2126 ( n4 log n ) . Hence, with overwhelming probability, we have\nL (q) \u2264 2 5p + \u03b8 p + 1 8000\u03b82np\n19 10 \u221a \u03b8 p \u2212 18000\u03b85/2n3/2p\n\u2264 3 5\n18 \u221a \u03b8\n10\n\u2264 1 3 \u221a \u03b8 <\n\u221a 1\n4\u03b8 \u2212 1, (G.13)\nwhenever \u03b8 is sufficiently small. This completes the proof.\nNow, keep the notation in Appendix F for general orthonormal basis. For any current iterate q \u2208 Sn\u22121 that is close enough to the target solution, i.e., \u2223\u2223\u2329q,R>e1\u232a\u2223\u2223 = |\u3008Rq, e1\u3009| \u2265 3\u221a\u03b8, we have\u2223\u2223\u2223\u2329Q\u2032 (q; Y\u0302) ,R>e1\u232a\u2223\u2223\u2223\u2225\u2225\u2225Q\u2032 (q; Y\u0302)\u2225\u2225\u2225 2 = \u2223\u2223\u2223\u2329RQ\u2032 (q; Y\u0302) , e1\u232a\u2223\u2223\u2223\u2225\u2225\u2225RQ\u2032 (q; Y\u0302)\u2225\u2225\u2225 2 = |\u3008Q\u2032 (Rq; Y\u2032) , e1\u3009| \u2016Q\u2032 (Rq; Y\u2032)\u20162 , (G.14)\nwhere we have applied the identity proved in (F.11). Taking Rq \u2208 Sn\u22121 as the object of interest, by Proposition G.1, we conclude that\n|\u3008Q\u2032 (Rq; Y\u2032) , e1\u3009| \u2016Q\u2032 (Rq; Y\u2032)\u20162 \u2265 2 \u221a \u03b8 (G.15)\nwith overwhelming probability."}, {"heading": "H Bounding Iteration Complexity", "text": "Proposition H.1. There is a constant \u03b80 > 0, such that for any \u03b8 \u2208 (\n1\u221a n , \u03b80\n) , with probability at least 1\u2212c\u2032 exp (\u2212c\u2032\u2032n)\n(c\u2032 and c\u2032\u2032 are positive constants), the ADM algorithm in Algorithm 1, with any initialization q(0) \u2208 Sn\u22121 satisfying\u2223\u2223\u2223q(0)1 \u2223\u2223\u2223 \u2265 14\u221a\u03b8n , will produce some iterate q with |q\u03041| > 3\u221a\u03b8 at least once in at most O(n4 log n) iterations, provided exp (n) \u2265 p \u2265 Cn4 log n for some large constant C. Proof. Recall from Proposition F.1 in Appendix F, the gap\nG\u2032(q) = |Q\u20321(q)| |q1| \u2212 \u2016Q \u2032 2(q)\u20162 \u2016q\u20162 \u2265 1 4000\u03b82np\n(H.1)\nholds uniformly over q \u2208 Sn\u22121 satisfying 1 4 \u221a \u03b8p \u2264 |q1| \u2264 3 \u221a \u03b8 with probability at least 1\u2212 c1 exp (\u2212c2n) for\npositive constants c1 and c2, provided p \u2265 \u2126 ( n4 log n ) . The gap G\u2032(q) implies that\u2223\u2223\u2223Q\u0303\u20321 (q)\u2223\u2223\u2223 .= |Q\u20321(q)|\u2016Q\u2032 (q2)\u20162 \u2265 |q1| \u2016Q \u2032 2(q)\u20162 \u2016q\u20162 \u2016Q\u2032 (q)\u20162 + |q1| 4000\u03b82np \u2016Q\u2032 (q)\u20162\n(H.2)\n\u21d0\u21d2 \u2223\u2223\u2223Q\u0303\u20321 (q)\u2223\u2223\u2223 \u2265 |q1|\u2016q2\u20162 \u221a 1\u2212 \u2223\u2223\u2223Q\u0303\u20321 (q)\u2223\u2223\u22232 + |q1|4000\u03b82np \u2016Q\u2032 (q)\u20162 (H.3) =\u21d2 \u2223\u2223\u2223Q\u0303\u20321 (q)\u2223\u2223\u22232 \u2265 |q1|2 ( 1 + \u2016q2\u201622\n40002\u03b84n2p2 \u2016Q\u2032 (q)\u201622\n) . (H.4)\nNow we know that\nsup q\u2208\u0393 \u2016Q\u2032 (q)\u20162 \u2264 sup q\u2208\u0393 |Q\u20321 (q)|+ sup q\u2208\u0393 \u2016Q\u20322 (q)\u20162 (H.5)\n= sup q\u2208\u0393 \u2223\u2223\u2223\u2223\u22231p p\u2211 k=1 x0kS\u03bb [ x0kq1 + q > 2 g k ]\u2223\u2223\u2223\u2223\u2223+ supq\u2208\u0393 \u2225\u2225\u2225\u2225\u22251p p\u2211 k=1 gkS\u03bb [ x0kq1 + q > 2 g k ]\u2225\u2225\u2225\u2225\u2225\n2\n(H.6)\n\u2264 1 p ( sup q\u2208\u0393 p\u2211 k=1 |x0k| \u2223\u2223x0kq1 + q>2 gk\u2223\u2223+ sup q\u2208\u0393 p\u2211 k=1 \u2225\u2225gk\u2225\u2225 2 \u2223\u2223x0kq1 + q>2 gk\u2223\u2223 )\n(H.7)\n\u2264 2 (\n1\u221a \u03b8p + sup k\u2208[p]\n\u2225\u2225gk\u2225\u2225 2 ) sup q\u2208\u0393 ( |q1|\u221a \u03b8p + \u2016q2\u20162 sup k\u2208[p] \u2225\u2225gk\u2225\u2225 2 ) (H.8)\n\u2264 2 (\n1\u221a \u03b8p + sup k\u2208[p]\n\u2225\u2225gk\u2225\u2225 2 )2 . (H.9)\nFrom Lemma A.11, we know that supk\u2208[p] \u2225\u2225gk\u2225\u2225 2 \u2264 \u221a2 log p/\u221ap + 2\u221an/\u221ap with probability at least 1 \u2212\nexp (\u2212n/2). Then provided p \u2264 exp (n) and \u03b8 \u2265 1/\u221an, we obtain\nsup q\u2208\u0393 \u2016Q\u2032 (q)\u20162 \u2264\n50n\np . (H.10)\nSo we conclude that \u2223\u2223\u2223Q\u0303\u20321 (q)\u2223\u2223\u2223 |q1| \u2265 \u221a 1 + 1\u2212 9\u03b8 40002 \u00d7 502 \u00d7 \u03b84n4 . (H.11)\nTherefore, starting with any q \u2208 Sn\u22121 such that |q1| \u2265 14\u221a\u03b8n , we will need at most\nT = 2 log\n( 3 \u221a \u03b8/ 1\n4 \u221a \u03b8n ) log ( 1 + 1\u22129\u03b840002\u00d7502\u00d7\u03b84n4 ) = 2 log (12\u03b8\u221an) log ( 1 + 1\u22129\u03b840002\u00d7502\u00d7\u03b84n4 ) \u2264 2 log (12\u03b8\u221an) (log 2) 1\u22129\u03b840002\u00d7502\u00d7\u03b84n4 \u2264 Cn4 log n (H.12)\nsteps to arrive at a q \u2208 Sn\u22121 with |q\u03041| \u2265 3 \u221a \u03b8 for the first time, where C > 0 is a numerical constant, and we assume \u03b80 < 1/9 and used the fact that log (1 + x) \u2265 (log 2)x for x \u2208 [0, 1] to simplify the final result."}, {"heading": "I Rounding to the Desired Solution", "text": "For convenience, we will assume the notations we used in Appendix B. Then the rounding scheme can be written as\nmin q \u2016Y\u2032Rq\u20161 , s.t. \u3008q,q\u3009 = 1, (I.1)\nfor some orthogonal matrix R. We will show the rounding procedure get us to the desired solution with overwhelming probability, regardless of the particular orthonormal basis used.\nProposition I.1. Suppose the input basis is Y\u2032 defined in (B.3) and the ADM algorithm produces q\u0304 \u2208 Sn\u22121 with q1 > 2 \u221a \u03b8. Then there exists some constants C, \u03b80 > 0, such that when p \u2265 Cn2 and \u03b8 \u2208 ( 1\u221a n , \u03b80 ) , the rounding procedure with r = q returns the desired solution e1 with probability at least 1 \u2212 c exp (\u2212c\u2032n) for some numerical constants c, c\u2032 > 0.\nProof. The rounding program (I.1) can be written as\ninf q \u2016Y\u2032q\u20161 , s.t. q1q1 + \u3008q2,q2\u3009 = 1. (I.2)\nConsider its relaxation\ninf q \u2016Y\u2032q\u20161 , s.t. q1q1 + \u2016q2\u20162 \u2016q2\u20162 \u2265 1. (I.3)\nIt is obvious that the feasible set of (I.3) contains that of (I.2). So if e1 is the unique optimal solution (UOS) of (I.3), it is also the UOS of (I.2). Let I = supp(x0), and consider a modified problem\ninf q \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 1 |q1| \u2212 \u2016G\u2032Iq2\u20161 + \u2016G\u2032Icq2\u20161 , s.t. q1q1 + \u2016q2\u20162 \u2016q2\u20162 \u2265 1. (I.4)\nThe objective value of (I.4) lower bounds the objective value of (I.3), and are equal when q = e1. So if q = e1 is the UOS to (I.4), it is also UOS to (I.3), and hence UOS to (I.2) by the argument above. Now\n\u2212\u2016G\u2032Iq2\u20161 + \u2016G\u2032Icq2\u20161 \u2265 \u2212\u2016GIq2\u20161 + \u2016GIcq2\u20161 \u2212 \u2016(G\u2212G\u2032) q2\u20161 (I.5) \u2265 \u2212\u2016GIq2\u20161 + \u2016GIcq2\u20161 \u2212 \u2016G\u2212G\u2032\u2016`2\u2192`1 \u2016q2\u20162 . (I.6)\nWhen p \u2265 \u2126 ( n2 ) , by Lemma A.14 and Lemma B.3, we know that\n\u2212 \u2016GIq2\u20161 + \u2016GIcq2\u20161 \u2212 \u2016G\u2212G\u2032\u2016`2\u2192`1 \u2016q2\u20162\n\u2265 \u22126 5\n\u221a 2\n\u03c0 2\u03b8 \u221a p \u2016q2\u20162 + 24 25\n\u221a 2\n\u03c0 (1\u2212 2\u03b8)\u221ap \u2016q2\u20162 \u2212 8 \u221a n \u2016q2\u20162 . = \u03b6 \u2016q2\u20162 (I.7)\nholds with probability at least 1 \u2212 c1 exp (\u2212c2n) for some positive constants c1 and c2. Thus, we make a further relaxation of problem (I.2) by\ninf q \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 1 |q1|+ \u03b6 \u2016q2\u20162 , s.t. q1q1 + \u2016q2\u20162 \u2016q2\u20162 \u2265 1, (I.8)\nwhose objective value lower bounds that of (I.4). By similar arguments, if e1 is UOS to (I.8), it is UOS to (I.2). At the optimal solution to (I.8), notice that it is necessary to have sign(q1) = sign(q1) and q1q1 +\u2016q2\u20162 \u2016q2\u20162 = 1. So (I.8) is equivalent to\ninf q \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 1 |q1|+ \u03b6 \u2016q2\u20162 , s.t. q1q1 + \u2016q2\u20162 \u2016q2\u20162 = 1. (I.9)\nwhich is further equivalent to\ninf q1 \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 1 |q1|+ \u03b6 1\u2212 |q1| |q1| \u2016q2\u20162 , s.t. |q1| \u2264 1 |q1| . (I.10)\nNotice that the problem in (I.10) is linear in |q1| with a compact feasible set, which indicates that the optimal solution only occur at the boundary points |q1| = 0 and |q1| = 1/ |q1|. Therefore, q = e1 is the UOS of (I.10) if and only if\n1\n|q1| \u2225\u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u2225\u2225 1 < \u03b6 \u2016q2\u20162 . (I.11)\nSince \u2225\u2225\u2225 x0\u2016x0\u20162 \u2225\u2225\u22251 \u2264 \u221a2\u03b8p conditioned on E0, it is sufficient to have\n\u221a 2\u03b8p\n2 \u221a \u03b8 \u2264 \u03b6 = 24 25\n\u221a 2\n\u03c0\n\u221a p ( 1\u2212 9\n2 \u03b8 \u2212 25 3\n\u221a n\np\n) . (I.12)\nTherefore there exists a constant \u03b80 > 0, such that whenever \u03b8 \u2264 \u03b80, the rounding returns e1, completing the proof.\nWhen the input basis is Y\u2032R for some R 6= I, if the ADM algorithm produces some q = R>q\u2032, such that q\u20321 > 2 \u221a \u03b8. It is not hard to see that now the rounding (I.1) is equivalent to\nmin q \u2016Y\u2032Rq\u20161 , s.t. \u3008q\u2032,Rq\u3009 = 1. (I.13)\nRenaming Rq, it follows from the above argument that at optimum q? it holds that Rq? = e1 with overwhelming probability."}], "references": [{"title": "Learning sparsely used overcomplete dictionaries via alternating minimization", "author": ["Alekh Agarwal", "Animashree Anandkumar", "Prateek Jain", "Praneeth Netrapalli", "Rashish Tandon"], "venue": "arXiv preprint arXiv:1310.7991,", "citeRegEx": "AAJ13", "shortCiteRegEx": null, "year": 2013}, {"title": "Exact recovery of sparsely used overcomplete dictionaries", "author": ["Alekh Agarwal", "Animashree Anandkumar", "Praneeth Netrapalli"], "venue": "arXiv preprint arXiv:1309.1952,", "citeRegEx": "AAN13", "shortCiteRegEx": null, "year": 2013}, {"title": "More algorithms for provable dictionary learning", "author": ["Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma"], "venue": "arXiv preprint arXiv:1401.0579,", "citeRegEx": "ABGM14", "shortCiteRegEx": null, "year": 2014}, {"title": "New algorithms for learning incoherent and overcomplete dictionaries", "author": ["Sanjeev Arora", "Rong Ge", "Ankur Moitra"], "venue": "arXiv preprint arXiv:1308.6273,", "citeRegEx": "AGM13", "shortCiteRegEx": null, "year": 2013}, {"title": "When are overcomplete topic models identifiable? uniqueness of tensor tucker decompositions with structured sparsity", "author": ["AnimaAnandkumar", "Daniel Hsu", "Majid Janzamin", "ShamMKakade"], "venue": "Advances in Neural Information Processing Systems, pages 1986\u20131994,", "citeRegEx": "AHJK13", "shortCiteRegEx": null, "year": 2013}, {"title": "IEEE Transactions on", "author": ["Ronen Basri", "David W Jacobs. Lambertian reflectance", "linear subspaces. Pattern Analysis", "Machine Intelligence"], "venue": "25(2):218\u2013233,", "citeRegEx": "BJ03", "shortCiteRegEx": null, "year": 2003}, {"title": "Rounding sum-of-squares relaxations", "author": ["Boaz Barak", "Jonathan Kelner", "David Steurer"], "venue": "arXiv preprint arXiv:1312.6652,", "citeRegEx": "BKS13", "shortCiteRegEx": null, "year": 2013}, {"title": "Applied and Computational Harmonic Analysis", "author": ["Gregory Beylkin", "Lucas Monz\u00f3n. On approximation of functions by exponential sums"], "venue": "19(1):17\u201348,", "citeRegEx": "BM05", "shortCiteRegEx": null, "year": 2005}, {"title": "In Conference on Learning Theory", "author": ["Quentin Berthet", "Philippe Rigollet. Complexity theoretic lower bounds for sparse principal component detection"], "venue": "pages 1046\u20131066,", "citeRegEx": "BR13", "shortCiteRegEx": null, "year": 2013}, {"title": "Robust principal component analysis? Journal of the ACM", "author": ["Emmanuel Cand\u00e8s", "Xiaodong Li", "Yi Ma", "JohnWright"], "venue": "58(3), May", "citeRegEx": "CLMW11", "shortCiteRegEx": null, "year": 2011}, {"title": "Phase retrieval via wirtinger flow: Theory and algorithms", "author": ["Emmanuel J. Cand\u00e8s", "Xiaodong Li", "Mahdi Soltanolkotabi"], "venue": "arXiv preprint arXiv:1407.1065,", "citeRegEx": "CLS14", "shortCiteRegEx": null, "year": 2014}, {"title": "SIAM Journal on Algebraic Discrete Methods", "author": ["Thomas F Coleman", "Alex Pothen. The null space problem i. complexity"], "venue": "7(4):527\u2013537,", "citeRegEx": "CP86", "shortCiteRegEx": null, "year": 1986}, {"title": "IEEE Transactions on", "author": ["Emmanuel J Cand\u00e8s", "Terence Tao. Decoding by linear programming. Information Theory"], "venue": "51(12):4203\u20134215,", "citeRegEx": "CT05", "shortCiteRegEx": null, "year": 2005}, {"title": "In Computer Vision and Pattern Recognition (CVPR)", "author": ["Yuchao Dai", "Hongdong Li", "Mingyi He. A simple prior-free method for non-rigid structurefrom-motion factorization"], "venue": "2012 IEEE Conference on, pages 2018\u20132025. IEEE,", "citeRegEx": "DLH12", "shortCiteRegEx": null, "year": 2012}, {"title": "Decoupling: from dependence to independence", "author": ["Victor De la Pena", "Evarist Gin\u00e9"], "venue": "Springer,", "citeRegEx": "DlPG99", "shortCiteRegEx": null, "year": 1999}, {"title": "Communications on pure and applied mathematics", "author": ["David L Donoho. For most large underdetermined systems of linear equations the minimal `-norm solution is also the sparsest solution"], "venue": "59(6):797\u2013829,", "citeRegEx": "Don06", "shortCiteRegEx": null, "year": 2006}, {"title": "Acta Mathematica", "author": ["Tadeusz Figiel", "Joram Lindenstrauss", "Vitali D Milman. The dimension of almost spherical sections of convex bodies"], "venue": "139(1):53\u201394,", "citeRegEx": "FLM77", "shortCiteRegEx": null, "year": 1977}, {"title": "volume 277", "author": ["Andrej Y Garnaev", "Efim D Gluskin. The widths of a euclidean ball. In Dokl. Akad. Nauk SSSR"], "venue": "pages 1048\u20131052,", "citeRegEx": "GG84", "shortCiteRegEx": null, "year": 1984}, {"title": "pages 131\u2013135", "author": ["E Gluskin", "VMilman. Note on the geometric-arithmetic mean inequality. In Geometric aspects of Functional analysis"], "venue": "Springer,", "citeRegEx": "GM03", "shortCiteRegEx": null, "year": 2003}, {"title": "On the provable convergence of alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": "arXiv preprint arXiv:1312.0925,", "citeRegEx": "Har13", "shortCiteRegEx": null, "year": 2013}, {"title": "Recovering the sparsest element in a subspace", "author": ["Paul Hand", "Laurent Demanet"], "venue": "arXiv preprint arXiv:1310.1654,", "citeRegEx": "HD13", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of The 30th International Conference on Machine Learning", "author": ["Jeffrey Ho", "Yuchen Xie", "Baba Vemuri. On a nonlinear generalization of sparse coding", "dictionary learning"], "venue": "pages 1480\u20131488,", "citeRegEx": "HXV13", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of the 45th annual ACM symposium on Symposium on theory of computing", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi. Low-rank matrix completion using alternating minimization"], "venue": "pages 665\u2013674. ACM,", "citeRegEx": "JNS13", "shortCiteRegEx": null, "year": 2013}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi. Phase retrieval using alternating minimization"], "venue": "pages 2796\u20132804,", "citeRegEx": "NJS13", "shortCiteRegEx": null, "year": 2013}, {"title": "volume 94", "author": ["Gilles Pisier. The volume of convex bodies", "Banach space geometry"], "venue": "Cambridge University Press,", "citeRegEx": "Pis99", "shortCiteRegEx": null, "year": 1999}, {"title": "Finding a sparse vector in a subspace: Linear sparsity using alternating directions", "author": ["Qing Qu", "Ju Sun", "John Wright"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "QSW14", "shortCiteRegEx": null, "year": 2014}, {"title": "Complete dictionary learning over the sphere", "author": ["Ju Sun", "Qing Qu", "John Wright"], "venue": "preparation,", "citeRegEx": "SQW14", "shortCiteRegEx": null, "year": 2014}, {"title": "Exact recovery of sparsely-used dictionaries", "author": ["Daniel A Spielman", "Huan Wang", "John Wright"], "venue": "Proceedings of the 25th Annual Conference on Learning Theory,", "citeRegEx": "SWW12", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "author": ["Roman Vershynin"], "venue": "arXiv preprint arXiv:1011.3027,", "citeRegEx": "Ver10", "shortCiteRegEx": null, "year": 2010}, {"title": "Alternating minimization for mixed linear regression", "author": ["Xinyang Yi", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "arXiv preprint arXiv:1310.3745,", "citeRegEx": "YCS13", "shortCiteRegEx": null, "year": 2013}, {"title": "Applied Mathematics and Computation", "author": ["Yun-Bin Zhao", "Masao Fukushima. Rank-one solutions for homogeneous linear matrix equations over the positive semidefinite cone"], "venue": "219(10):5569\u2013 5583,", "citeRegEx": "ZF13", "shortCiteRegEx": null, "year": 2013}, {"title": "Journal of computational and graphical statistics", "author": ["Hui Zou", "Trevor Hastie", "Robert Tibshirani. Sparse principal component analysis"], "venue": "15(2):265\u2013286,", "citeRegEx": "ZHT06", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural computation", "author": ["Michael Zibulevsky", "Barak A Pearlmutter. Blind source separation by sparse decomposition in a signal dictionary"], "venue": "13(4):863\u2013882,", "citeRegEx": "ZP01", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 11, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 105, "endOffset": 111}, {"referenceID": 30, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 146, "endOffset": 152}, {"referenceID": 13, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 185, "endOffset": 192}, {"referenceID": 7, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 234, "endOffset": 240}, {"referenceID": 31, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 253, "endOffset": 260}, {"referenceID": 32, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 286, "endOffset": 292}, {"referenceID": 27, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 314, "endOffset": 321}, {"referenceID": 4, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 348, "endOffset": 356}, {"referenceID": 21, "context": "1) Variants of this problem have been studied in the context of applications to numerical linear algebra [CP86], system control and optimizations [ZF13], nonrigid structure from motion [DLH12], spectral estimation and Prony\u2019s problem [BM05], sparse PCA [ZHT06], blind source separation [ZP01], dictionary learning [SWW12], graphical model learning [AHJK13], and sparse coding on manifolds [HXV13].", "startOffset": 389, "endOffset": 396}, {"referenceID": 11, "context": "2) is NP-hard [CP86].", "startOffset": 14, "endOffset": 20}, {"referenceID": 27, "context": "[SWW12] introduced a relaxation which replaces the nonconvex problem (1.", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "Method Recovery Condition Total Complexity `1/`\u221e Relaxation[HD13] \u03b8 \u2208 O(1/\u221an) O(np) SDP Relaxation \u03b8 \u2208 O(1/\u221an) O(p) SOS Relaxation [BKS13] p \u2265 \u03a9(n), \u03b8 \u2208 O(1) high order poly(p) This work p \u2265 \u03a9(n log n), \u03b8 \u2208 O(1) O(np log n)", "startOffset": 59, "endOffset": 65}, {"referenceID": 6, "context": "Method Recovery Condition Total Complexity `1/`\u221e Relaxation[HD13] \u03b8 \u2208 O(1/\u221an) O(np) SDP Relaxation \u03b8 \u2208 O(1/\u221an) O(p) SOS Relaxation [BKS13] p \u2265 \u03a9(n), \u03b8 \u2208 O(1) high order poly(p) This work p \u2265 \u03a9(n log n), \u03b8 \u2208 O(1) O(np log n)", "startOffset": 131, "endOffset": 138}, {"referenceID": 20, "context": "3) also correctly recovers x0, provided the fraction of nonzeros in x0 scales as \u03b8 \u2208 O (1/\u221an) [HD13].", "startOffset": 94, "endOffset": 100}, {"referenceID": 31, "context": "4) and sparse PCA [ZHT06].", "startOffset": 18, "endOffset": 25}, {"referenceID": 8, "context": "In sparse PCA, there is a substantial gap between what can be achieved with efficient algorithms and the information theoretic optimum [BR13].", "startOffset": 135, "endOffset": 141}, {"referenceID": 6, "context": "introduced a new rounding technique for sum-ofsquares relaxations, and showed that the sparse vector x0 in the planted sparse model can be recovered when p \u2265 \u03a9 ( n ) and \u03b8 = \u03a9(1) [BKS13].", "startOffset": 179, "endOffset": 186}, {"referenceID": 27, "context": "Second, our theoretical results require a second, linear programming based rounding phase, which is similar to [SWW12].", "startOffset": 111, "endOffset": 118}, {"referenceID": 26, "context": "2 1This breakdown behavior is again in sharp contrast to the standard sparse approximation problem (withb 6= 0), inwhich it is possible to handle very large fractions of nonzeros (say, \u03b8 = \u03a9(1/ logn), or even \u03b8 = \u03a9(1)) using a very simple `1 relaxation [CT05, Don06] 2In work currently in preparation [SQW14], we show that in the dictionary learning problem, efficient algorithms based on nonconvex", "startOffset": 301, "endOffset": 308}, {"referenceID": 32, "context": ", the work of [ZP01] in blind source separation for precedent.", "startOffset": 14, "endOffset": 20}, {"referenceID": 25, "context": "3Note that this version is much stronger and more practical than that appearing in the conference version [QSW14].", "startOffset": 106, "endOffset": 113}, {"referenceID": 28, "context": "5This is the common heuristic that \u201ctall random matrices are well conditioned\u201d [Ver10].", "startOffset": 79, "endOffset": 86}, {"referenceID": 6, "context": "\u2019s result [BKS13] in sampling complexity.", "startOffset": 10, "endOffset": 17}, {"referenceID": 27, "context": "Second, we consider the same dictionary learning model as in [SWW12].", "startOffset": 61, "endOffset": 68}, {"referenceID": 5, "context": "2 Exploratory Experiments on Faces It is well known in computer vision that appearance of convex objects only subject to illumination changes leads to image collection that can be well approximated by low-dimensional space in raw-pixel space [BJ03].", "startOffset": 242, "endOffset": 248}, {"referenceID": 9, "context": "Thenwe apply robust principal component analysis [CLMW11] to the data and get a low dimensional subspace of dimension 10, i.", "startOffset": 49, "endOffset": 57}, {"referenceID": 14, "context": "Advanced techniques to bound the empirical process, such as decoupling [DlPG99] techniques, can be deployed in place of our crude union bound to cover all iterates.", "startOffset": 71, "endOffset": 79}, {"referenceID": 26, "context": "Our forthcoming work [SQW14] on dictionary learning takes a more geometric approach, and proves global recovery via efficient algorithms, with arbitrary initialization.", "startOffset": 21, "endOffset": 28}, {"referenceID": 0, "context": "References [AAJ13] Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon.", "startOffset": 11, "endOffset": 18}, {"referenceID": 1, "context": "[AAN13] Alekh Agarwal, Animashree Anandkumar, and Praneeth Netrapalli.", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "[ABGM14] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.", "startOffset": 0, "endOffset": 8}, {"referenceID": 3, "context": "[AGM13] Sanjeev Arora, Rong Ge, and Ankur Moitra.", "startOffset": 0, "endOffset": 7}, {"referenceID": 4, "context": "[AHJK13] AnimaAnandkumar, Daniel Hsu, Majid Janzamin, and ShamMKakade.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "[BJ03] Ronen Basri and David W Jacobs.", "startOffset": 0, "endOffset": 6}, {"referenceID": 6, "context": "[BKS13] Boaz Barak, Jonathan Kelner, and David Steurer.", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[BM05] Gregory Beylkin and Lucas Monz\u00f3n.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[BR13] Quentin Berthet and Philippe Rigollet.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "[CLMW11] Emmanuel Cand\u00e8s, Xiaodong Li, Yi Ma, and JohnWright.", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "[CLS14] Emmanuel J.", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "[CP86] Thomas F Coleman and Alex Pothen.", "startOffset": 0, "endOffset": 6}, {"referenceID": 12, "context": "[CT05] Emmanuel J Cand\u00e8s and Terence Tao.", "startOffset": 0, "endOffset": 6}, {"referenceID": 13, "context": "[DLH12] Yuchao Dai, Hongdong Li, and Mingyi He.", "startOffset": 0, "endOffset": 7}, {"referenceID": 14, "context": "[DlPG99] Victor De la Pena and Evarist Gin\u00e9.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[Don06] David L Donoho.", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "[FLM77] Tadeusz Figiel, Joram Lindenstrauss, and Vitali D Milman.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[GG84] Andrej Y Garnaev and Efim D Gluskin.", "startOffset": 0, "endOffset": 6}, {"referenceID": 18, "context": "[GM03] E Gluskin and VMilman.", "startOffset": 0, "endOffset": 6}, {"referenceID": 19, "context": "[Har13] Moritz Hardt.", "startOffset": 0, "endOffset": 7}, {"referenceID": 20, "context": "[HD13] Paul Hand and Laurent Demanet.", "startOffset": 0, "endOffset": 6}, {"referenceID": 21, "context": "[HXV13] Jeffrey Ho, Yuchen Xie, and Baba Vemuri.", "startOffset": 0, "endOffset": 7}, {"referenceID": 22, "context": "[JNS13] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi.", "startOffset": 0, "endOffset": 7}, {"referenceID": 23, "context": "[NJS13] Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi.", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "[Pis99] Gilles Pisier.", "startOffset": 0, "endOffset": 7}, {"referenceID": 25, "context": "[QSW14] Qing Qu, Ju Sun, and John Wright.", "startOffset": 0, "endOffset": 7}, {"referenceID": 26, "context": "[SQW14] Ju Sun, Qing Qu, and John Wright.", "startOffset": 0, "endOffset": 7}, {"referenceID": 27, "context": "[SWW12] Daniel A Spielman, Huan Wang, and John Wright.", "startOffset": 0, "endOffset": 7}, {"referenceID": 28, "context": "[Ver10] Roman Vershynin.", "startOffset": 0, "endOffset": 7}, {"referenceID": 29, "context": "[YCS13] Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.", "startOffset": 0, "endOffset": 7}, {"referenceID": 30, "context": "[ZF13] Yun-Bin Zhao and Masao Fukushima.", "startOffset": 0, "endOffset": 6}, {"referenceID": 31, "context": "[ZHT06] Hui Zou, Trevor Hastie, and Robert Tibshirani.", "startOffset": 0, "endOffset": 7}, {"referenceID": 32, "context": "[ZP01] Michael Zibulevsky and Barak A Pearlmutter.", "startOffset": 0, "endOffset": 6}, {"referenceID": 28, "context": "13 (Spectrum of Gaussian Matrices, [Ver10]).", "startOffset": 35, "endOffset": 42}, {"referenceID": 18, "context": "Geometrically, this lemma roughly corresponds to thewell known almost spherical section theorem [FLM77, GG84], see also [GM03].", "startOffset": 120, "endOffset": 126}, {"referenceID": 15, "context": "A slight variant of this version has been proved in [Don06], borrowing ideas from [Pis99].", "startOffset": 52, "endOffset": 59}, {"referenceID": 24, "context": "A slight variant of this version has been proved in [Don06], borrowing ideas from [Pis99].", "startOffset": 82, "endOffset": 89}], "year": 2017, "abstractText": "We consider the problem of recovering the sparsest vector in a subspace S \u2286 R with dim (S) = n < p. This problem can be considered a homogeneous variant of the sparse recovery problem, and finds applications in sparse dictionary learning, sparse PCA, and other problems in signal processing andmachine learning. Simple convex heuristics for this problem provably break down when the fraction of nonzero entries in the target sparse vector substantially exceeds 1/ \u221a n. In contrast, we exhibit a relatively simple nonconvex approach based on alternating directions, which provably succeeds even when the fraction of nonzero entries is \u03a9(1). To our knowledge, this is the first practical algorithm to achieve this linear scaling. This result assumes a planted sparse model, in which the target sparse vector is embedded in an otherwise random subspace. Empirically, our proposed algorithm also succeeds in more challenging data models arising, e.g., from sparse dictionary learning.", "creator": "LaTeX with hyperref package"}}}