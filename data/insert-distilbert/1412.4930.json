{"id": "1412.4930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Dec-2014", "title": "Rehabilitation of Count-based Models for Word Vector Representations", "abstract": "recent works on word processor representations mostly rely on predictive models. distributed word representations ( aka word embeddings ) are trained to optimally predict the contexts in which the partially corresponding words tend to appear. such resulting models have succeeded in capturing word similarties both as well as comparing semantic and syntactic regularities. whether instead, we aim at reviving interest in a model based on counts. now we present a systematic study of the use of modeling the hellinger distance to extract semantic representations from inspecting the word co - occurence statistics of large text corpora. we merely show that this distance gives good model performance on word similarity and analogy tasks, with a proper type and size of context, and a dimensionality reduction based overwhelmingly on a stochastic low - grade rank approximation. besides being both simple quantitative and intuitive, this method also provides an encoding function function which can be used to infer unseen words or phrases. commonly this becomes a clear relative advantage though compared to predictive models which must train these new words.", "histories": [["v1", "Tue, 16 Dec 2014 09:43:56 GMT  (283kb,D)", "https://arxiv.org/abs/1412.4930v1", null], ["v2", "Wed, 8 Apr 2015 18:35:17 GMT  (39kb,D)", "http://arxiv.org/abs/1412.4930v2", "A. Gelbukh (Ed.), Springer International Publishing Switzerland"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\\'emi lebret", "ronan collobert"], "accepted": false, "id": "1412.4930"}, "pdf": {"name": "1412.4930.pdf", "metadata": {"source": "CRF", "title": "Rehabilitation of Count-based Models for Word Vector Representations", "authors": ["R\u00e9mi Lebret", "Ronan Collobert"], "emails": ["remi@lebret.ch,", "ronan@collobert.com"], "sections": [{"heading": null, "text": "Recent works on word representations mostly rely on predictive models. Distributed word representations (aka word embeddings) are trained to optimally predict the contexts in which the corresponding words tend to appear. Such models have succeeded in capturing word similarities as well as semantic and syntactic regularities. Instead, we aim at reviving interest in a model based on counts. We present a systematic study of the use of the Hellinger distance to extract semantic representations from the word co-occurrence statistics of large text corpora. We show that this distance gives good performance on word similarity and analogy tasks, with a proper type and size of context, and a dimensionality reduction based on a stochastic low-rank approximation. Besides being both simple and intuitive, this method also provides an encoding function which can be used to infer unseen words or phrases. This becomes a clear advantage compared to predictive models which must train these new words."}, {"heading": "1 INTRODUCTION", "text": "Linguists assumed long ago that words occurring in similar contexts tend to have similar meanings (Harris, 1954, Firth, 1957). Using the word co-occurrence statistics is thus a natural choice to embed similar words into a common vector space (Turney and Pantel, 2010, Pennington et al., 2014). Common approaches calculate the frequencies, apply some transformations\n(tf-idf, PPMI), reduce the dimensionality and calculate the similarities (Lowe, 2001). Considering a fixedsized word dictionary D and a set of words W to embed, the co-occurrence matrix C is of size |W| \u00d7 |D|. C is then dictionary size-dependent. One can apply a dimensionality reduction operation to C leading to C\u0304 \u2208 R|W|\u00d7d, where d |D|. Dimensionality reduction techniques such as Singular Value Decomposition (SVD) are widely used (e.g. LSA (Landauer and Dumais, 1997), ICA (Va\u0308yrynen and Honkela, 2004)). In Bullinaria and Levy (2007, 2012), the authors provide a full range of factors to use for properly extracting semantic representations from the word cooccurrence statistics of large text corpora. While word co-occurrence statistics are discrete distributions, an information theory measure such as the Hellinger distance seems to be more appropriate than the Euclidean distance over a discrete distribution space. In this respect, Lebret and Collobert (2014) propose to perform a principal component analysis (PCA) of the word cooccurrence probability matrix to represent words in a lower dimensional space, while minimizing the reconstruction error according to the Hellinger distance. In practice, they just apply a square-root transformation to the co-occurrence probability matrix, and then perform the PCA of this new matrix. They compare the resulting word representations with some well-known representations on named entity recognition and movie review tasks and show that they can reach similar or even better performance.\nThis paper proposes an extension of the work of Lebret and Collobert (2014) by investigating the impact of different factors. Mikolov et al. (2013b) show that a subsampling approach to imbalance between the rare and frequent words improves the performance. Recent approaches for word representation have also shown that large windows of context are helpful to capture semantic information (Mikolov et al., 2013b, Pennington et al., 2014). While, in Lebret and Collobert (2014), only the 10,000 most frequent words from the dictionary W are considered as context dictionary D, we\nar X\niv :1\n41 2.\n49 30\nv2 [\ncs .C\nL ]\n8 A\npr 2\n01 5\nRehabilitation of Count-based Models for Word Vector Representations\ninvestigate various types of context dictionaries, with only frequent words or rare words, or a combination of both. In this previous work, the co-occurrence counts to build C are based on a single context word occurring just after the word of interest. In this paper, we analyse various sizes of context, with both symmetric and asymmetric windows. For deriving lowdimensional vector representations from the word cooccurrence matrix C, PCA can be done by eigenvalue decomposition of the covariance matrix CTC or SVD of C. Covariance-based PCA of high-dimensional matrices can lead to round-off errors, and thus fails to properly approximate these high-dimensional matrices in low-rank matrices. And SVD will generally requires a large amount of memory to factorize such huge matrices. To overcome these barriers, we propose a dimensionality reduction based on stochastic low-rank approximation and show that it outperforms the covariance-based PCA.\nRecently, distributed approaches based on neural network language models have revived the field of learning word embeddings (Collobert and Weston, 2008, Huang and Yates, 2009, Turian et al., 2010, Mnih and Kavukcuoglu, 2013, Mikolov et al., 2013a). Such approaches are trained to optimally predict the contexts in which words from W tend to appear. Baroni et al. (2014) present a systematic comparison of these predictive models with the models based on co-occurrence counts, which suggests that context-predicting models should be chosen over their count-based counterparts. In this paper, we aim at showing that count-based models should not be buried so hastily. A neural network architecture can be hard to train. Finding the right hyperparameters to tune the model is often a challenging task and the training phase is in general computationally expensive. Counting words over large text corpora is on the contrary simple and fast. With a proper dimensionality reduction technique, word vector representations in a low-dimensional space can be generated. Furthermore, it gives an encoding function represented by a matrix which can be used to encode new words or even phrases based on their counts. This is a major benefit compared to predictive models which will need to train vector representations for them. Thus, in addition to being simple and fast to compute, count-based models become a simple, fast and intuitive solution for inference."}, {"heading": "2 HELLINGER-BASED WORD VECTOR REPRESENTATIONS", "text": ""}, {"heading": "2.1 Word Co-Occurrence Probabilities", "text": "\u201cYou shall know a word by the company it keeps\u201d (Firth, 1957). Keeping this famous quote in\nmind, word co-occurrence probabilities are computed by counting the number of times each context word c \u2208 D (where D \u2286 W) occurs around a word w \u2208 W:\np(c|w) = p(c, w) p(w) = n(c, w)\u2211 c n(c, w) , (1)\nwhere n(c, w) is the number of times a context word c occurs in the surrounding of the word w. A multinomial distribution of |D| classes (words) is thus obtained for each word w:\nPw = {p(c1|w), . . . , p(c|D||w)} . (2)\nBy repeating this operation over all words from W, the word co-occurrence matrix C is thus obtained:\nC =  p(c1|w1) \u00b7 \u00b7 \u00b7 p(c|D||w1) p(c1|w2) \u00b7 \u00b7 \u00b7 p(c|D||w2) ... . . .\n... p(c1|w|W|) \u00b7 \u00b7 \u00b7 p(c|D||w|W|)\n =  Pw1 Pw2\n... Pw|W|  (3)\nThe number of context words to consider around each word is variable and can be either symmetric or asymmetric. The co-occurrence matrix becomes less sparse when this number is high. Because we are facing discrete probability distributions, the Hellinger distance seems appropriate to calculate similarities between these word representations. The square-root transformation is then applied to the probability distributions Pw, and the word co-occurrence matrix is now defined as:\nC\u0303 =  \u221a Pw1\u221a Pw2 ...\u221a\nPw|W|  = \u221aC . (4)"}, {"heading": "2.2 Hellinger Distance", "text": "Similarities between words can be derived by computing a distance between their corresponding word distributions. Several distances (or metrics) over discrete distributions exist, such as the Bhattacharyya distance, the Hellinger distance or Kullback-Leibler divergence. We chose here the Hellinger distance for its simplicity and symmetry property (as it is a true distance). Considering two discrete probability distributions P = (p1, . . . , pk) and Q = (q1, . . . , qk), the Hellinger distance is formally defined as:\nH(P,Q) = 1\u221a 2 \u221a\u221a\u221a\u221a k\u2211 i=1 ( \u221a pi \u2212 \u221a qi)2 , (5)\nwhich is directly related to the Euclidean norm of the difference of the square root vectors:\nH(P,Q) = 1\u221a 2 \u2016 \u221a P \u2212\n\u221a Q\u20162 . (6)\nRe\u0301mi Lebret1,2, Ronan Collobert1\nNote that it makes more sense to take the Hellinger distance rather than the Euclidean distance for comparing discrete distributions, as P and Q are unit vectors according to the Hellinger distance ( \u221a P and \u221a Q are units vector according to the `2 norm)."}, {"heading": "2.3 Dimensionality Reduction", "text": "As discrete distributions are dictionary sizedependent, using directly the distribution as a word representation is, in general, not really tractable for large dictionary. This is even more true in the case of a large number of context words, distributions becoming less sparse. We investigate two approaches to embed these representations in a low-dimensional space: (1) a principal component analysis (PCA) of the word co-occurrence matrix C\u0303, (2) a stochastic low-rank approximation to encode distributions \u221a Pw."}, {"heading": "2.3.1 Principal Component Analysis (PCA)", "text": "We perform a principal component analysis (PCA) of the square root of the word co-occurrence probability matrix to represent words in a lower dimensional space, while minimizing the reconstruction error according to the Hellinger distance. This PCA can be done by eigenvalue decomposition of the covariance matrix C\u0303T C\u0303. With a limited size of context word dictionary D (tens of thousands of words), this operation is performed very quickly (See Lebret and Collobert (2014) paper for details). With a larger size for D, a truncated singular value decomposition of C\u0303 might be an alternative, even if it is time-consuming and memory-hungry."}, {"heading": "2.3.2 Stochastic Low-Rank Approximation (SLRA)", "text": "When dealing with large dimensions, the computation of the covariance matrix might accumulate floatingpoint roundoff errors. To overcome this issue and to still fit in memory, we propose a stochastic low-rank approximation to represent words in a lower dimensional space. It takes a distribution \u221a Pw as input, encodes it in a more compact representation, and is trained to reconstruct its own input from that representation:\n||V UT \u221a Pw \u2212 \u221a Pw||2 , (7)\nwhere U and V \u2208 R|D|\u00d7d. U is a low-rank approximation of the co-occurrence matrix C\u0303 which maps distributions in a d-dimension (with d |D|), and V is the reconstruction matrix. UT \u221a Pw is a distributed representation that captures the main factors of variation in the data as the Hellinger PCA does. U and V are trained by backpropagation using stochastic gradient descent."}, {"heading": "3 EXPERIMENTS", "text": ""}, {"heading": "3.1 Building Word Representation over Large Corpora", "text": "Our English corpus is composed of the entire English Wikipedia1 (where all MediaWiki markups have been removed). We consider lower case words to limit the number of words in the dictionary. Additionally, all occurrences of sequences of numbers within a word are replaced with the string \u201cNUMBER\u201d. The resulting text is tokenized using the Stanford tokenizer2. The data set contains about 1.6 billion words. As dictionary W, we consider all the words within our corpus which appear at least one hundred times. This results in a 191,268 words dictionary. Five scenarios are considered to build the word co-occurrence probabilities with context words D: (1) Only the 10,000 most frequent words within this dictionary. (2) All the dictionary. Mikolov et al. (2013b) have shown that better word representations can be obtained by subsampling of the frequent words. We thus define the following scenarios: (3) Only words whose appearance frequency is less than 10\u22125, which is the last 184,308 words in W. (4) To limit the dictionary size, we consider words whose appearance frequency is less than 10\u22125 and greater than 10\u22126. This results in 24,512 context words. (5) Finally, only words whose appearance frequency is greater than 10\u22126, which gives 31,472 words."}, {"heading": "3.2 Evaluating Word Representations", "text": ""}, {"heading": "3.2.1 Word analogies", "text": "The word analogy task consists of questions like, \u201ca is to b as c is to ?\u201d. It was introduced in Mikolov et al. (2013a) and contains 19,544 such questions, divided into a semantic subset and a syntactic subset. The 8,869 semantic questions are analogies about places, like \u201cBern is to Switzerland as Paris is to ?\u201d, or family relationship, like \u201cuncle is to aunt as boy is to ?\u201d. The 10,675 syntactic questions are grammatical analogies, involving plural and adjectives forms, superlatives, verb tenses, etc. To correctly answer the question, the model should uniquely identify the missing term, with only an exact correspondence counted as a correct match.\n1Available at http://download.wikimedia.org. We took the January 2014 version.\n2Available at http://nlp.stanford.edu/software/ tokenizer.shtml\nRehabilitation of Count-based Models for Word Vector Representations"}, {"heading": "3.2.2 Word Similarities", "text": "We also evaluate our model on a variety of word similarity tasks. These include the WordSimilarity353 Test Collection (WS-353) (Finkelstein et al., 2002), the Rubenstein and Goodenough dataset (RG65) (Rubenstein and Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al., 2013). They all contain sets of English word pairs along with human-assigned similarity judgements. WS-353 and RG-65 datasets contain 353 and 65 word pairs respectively. Those are relatively common word pairs, like computer:internet or football:tennis. The RW dataset differs from these two datasets, since it contains 2,034 pairs where one of the word is rare or morphologically complex, such as brigadier:general or cognizance:knowing."}, {"heading": "3.3 Analysis of the Context", "text": "As regards the context, two main parameters are involved: (1) The context window size to consider, i.e.\nthe number of context words c to count for a given word w. We can either count only context words that occurs after w (asymmetric context window), or we can count words surrounding w (symmetric context window). (2) The type of context to use, i.e. which words are to be chosen for defining the context dictionary D. Do we need all the words, the most frequent ones or, on the contrary, the rare ones? Figure 1 presents the performance obtained on the benchmark datasets for all the five scenarios described in Section 3.1 with different sizes of context. No dimensionality reduction has been applied in this analysis. Similarities between words are calculated with the Hellinger distance between the word probability distributions. For the word analogy task, we used the objective function 3CosMul defined by Levy and Goldberg (2014), as we are dealing with explicit word representations in this case.\nRe\u0301mi Lebret1,2, Ronan Collobert1\nWINDOW SIZE\n1 10\nbaikal (no37415)\nma\u0308laren lake titicaca siberia balaton amur\nladoga basin\nilmen volga\nspecial-need (no165996)\nat-risk preschool\nschool-age kindergarten low-income teachers\nhearing-impaired schools\ngrade-school vocational"}, {"heading": "3.3.1 Window Size", "text": "Except for semantic analogy questions, best performance are always obtained with symmetric context window of size 1. However, performance dramatically drop with this window size on the latter. It seems that a limited window size helps to find syntactic similarities, but a large window is needed to detect the semantic aspects. The best results are thus obtained with a symmetric window of 10 words on the semantic analogy questions task. This intuition is confirmed by looking at the nearest neighbors of certain rare words with different sizes of context. In Table 1, we can observe that a window of one context word brings together words that occur in a same syntactic structure, while a window of ten context words will go beyond that and add semantic information. With only one word of context, Lake Baikal is therefore neighbor to other lakes, and the word special-needs is close to other words composed of two words. With ten words of con-\ntext, the nearest neighbors of Baikal are words in direct relation to this location, i.e. these words cannot match with other lakes, like Lake Titicaca. This also applies for the word special-needs, where we find words related to the educational meaning of this word. This could explain why the symmetric window of one context word gives the best results on the word similarity and syntactic tasks, but performs very poorly on the semantic task. Finally, the use of a symmetric window instead of an asymmetric one always improves the performance."}, {"heading": "3.3.2 Type of Context", "text": "First, using all words as context does not imply to reach the best performance. With the 10,000 most frequent words, performance are fairly similar than with all words. An in-between situation with words whose appearance frequency is greater than 10\u22126 gives also quite similar performance. Secondly, discarding the most frequent words from the context distributions helps, in general, to increase performance. The best performance is indeed obtained with scenarios (3) and (4). But all rare words are not necessarily essential to achieve good performance, since results with words whose appearance frequency is less than 10\u22125 and greater than 10\u22126 are not significantly lower. These two observations might be explained by the sparsity of the probability distributions. Counts in Table 2 show significant differences in terms of sparsity depending on the type of context. Similarities between words seem to be easier to find with sparse distributions. The average number of context words (i.e. features) whose appearance frequency is less than 10\u22125 and greater than 10\u22126 with a symmetric window of size 1 is extremely low (132). Performance with these parameters are still highly competitive on syntactic tasks. Within this framework, it then becomes a good option for representing words in a low and sparse dimension."}, {"heading": "3.4 Dimensionality Reduction Models", "text": "The analysis of the context reveals that word similarities can even be found with extremely sparse word vector representations. But these representations lack semantic information since they perform poorly on the word analogy task involving semantic questions. A symmetric window of five or ten context words seems to be the best options to capture both syntactic and semantic information about words. The average number of context words is much larger within these parameters, which justifies the need of dimensionality reduction. Furthermore, this analysis show that a large number of context words is not necessary to achieve significant improvements. Good performance on syntactic and similarity tasks can be reached with the\nRehabilitation of Count-based Models for Word Vector Representations\n10,000 most frequent words as context. Using instead a distribution of a limited number of rare words increases performance on the semantic task while reducing performance on syntactic and similarity tasks. We then focus on the two scenarios with the fewest number of context words: scenarios (1) and (4) with 10,000 and 24,512 words respectively. This reasonable number of context words allows for dimensionality reduction methods to be applied in an efficient manner."}, {"heading": "3.4.1 Number of Dimensions", "text": "When a dimensionality reduction method is applied, a number of dimensions needs to be chosen. This number has to be large enough to retain the maximum variability. It also has to be small enough for the dimensionality reduction to be truly meaningful and effective. We thus analyse the impact of the number of dimensions using the Hellinger PCA of the cooccurrence matrix from scenario (1) with a symmetric context of five and ten words. Figure 2 reports performance on the benchmark datasets described in Section 3.2 for different numbers of dimensions. The ability of the PCA to summarize the information compactly leads to improved results on the word similarity tasks, where performance is better than with no dimensionality reduction. On the WS-353 and RG-65 datasets, we observe that the gain in performance tends to stabilize between 300 and 1,200 dimensions. The increase in di-\nmension leads to a small drop after 100 dimensions on the RW dataset. However, adding more and more dimensions helps to increase performance on word analogy tasks, especially for the semantic one. We also observe that ten context words instead of five give better results for word analogy tasks, while the opposite is observed for word similarity tasks. This confirms the results observed in Section 3.3."}, {"heading": "3.4.2 Stochastic Low-Rank Approximation vs Covariance-based PCA", "text": "In this section, we compare performance on both word evaluation tasks using the two methods for dimensionality reduction described in Section 2.3. Experiments with symmetric window of five and ten context words are run to embed word representations in a d-dimensional vector, with d = {100, 200, 300}. All results are reported in Table 3. Except for some isolated results, performance is always much better with the stochastic low-rank approximation approach than with a Hellinger PCA approach. Calculating the reconstruction error of both approaches confirms that the PCA fails somehow to properly reduce the dimensionality. For a reduction from 10,000 to 100 dimensions, the PCA reconstruction error is 532.2 compared with 440.3 for the stochastic low-rank approximation. This result is not really surprising, since it is wellknown that standard PCA is exceptionally fragile, and\nthe quality of its output can suffer dramatically in the face of only a few grossly corrupted points (Jolliffe, 1986). Covariance-based PCA as proposed in Lebret and Collobert (2014) is thus not an approach offering a complete guarantee of success. An approach to robustifying PCA must be considered. This is what we propose with the stochastic low-rank approximation which, moreover, ensures a low memory consumption. For a given dimension, a window of ten context words outperforms, in general, a window of five context words. This confirms once again the benefit of using a larger window of context. Performance are globally better with 300 dimensions, but performance with 200 dimensions is just slightly lower, or even better in certain cases. Finally, using a distribution of rare words instead of frequent words (i.e. scenario (4) instead of scenario (1) here) has only an impact on the semantic word analogy task."}, {"heading": "3.5 Comparison with Other Models", "text": "We compare our word representations with other available models for computing vector representations of words: (1) the GloVe model which is also based on co-occurrence statistics of corpora (Pennington et al.,\n2014)3, (2) the continuous bag-of-words (CBOW) and the skip-gram (SG) architectures which learn representations from prediction-based models (Mikolov et al., 2013b)4. The same corpus and dictionary W as the ones described in Section 3.1 are used to train 200- dimensional word vector representations. We use a\n3Code available at http://www-nlp.stanford.edu/ software/glove.tar.gz.\n4Code available at http://word2vec.googlecode.com/ svn/trunk/.\nRehabilitation of Count-based Models for Word Vector Representations\nsymmetric context window of ten words, and the default values set by the authors for the other hyperparameters. We also compare these models directly with the raw distributions, computing similarities between them with the Hellinger distance. Results reported in Table 4 show that our approach is competitive with prediction-based models. Using the raw probability distributions yields good results on the semantic task, while a dimension reduction with a stochastic low-rank approximation gives a better solution to compete with others on similarity and syntactic tasks."}, {"heading": "3.6 Inference", "text": "Relying on word co-occurrence statistics to represent words in vector space provides a framework to easily generate representations for unseen words. This is a clear advantage compared to methods focused on learning word embeddings, where the whole system needs to be trained again to learn representations for these new words. To infer a representation for a new word wnew, one only needs to count its context words over a large corpus of text to build the distribution\u221a\nPwnew . This nice feature can be extrapolated to phrases. Table 5 presents some interesting examples of unseen phrases where the meaning clearly depends on the composition of their words. For instance, words from the entity Chicago Bulls differ in meaning when taken separately. Chicago will be close to other american cities, and Bulls will be close to other horned animals. However, it can be seen in Table 5 that our model infers a representation for this new phrase which is close to other NBA teams, like the Lakers or the Celtics. This also works with longer phrases, such as New York City or President of the United States."}, {"heading": "4 CONCLUSION", "text": "We presented a systematic study of a method based on counts and the Hellinger distance for building word vector representations. The main findings are: (1) a large window of context words is crucial to capture both syntactic and semantic information; (2) a context dictionary of rare words helps for capturing semantic, but by using just a fraction of the most frequent words already ensures a high level of performance; (3) a dimensionality reduction with a stochastic low-rank approximation approach outperforms the PCA approach. The objective of the paper was to rehabilitate countvector-based models, whereas nowadays all the attention is directed to context-predicting models. We show that such a simple model can give nice results on both similarity and analogy tasks. Better still, inference of unseen words or phrases is easily feasible when relying on counts."}, {"heading": "Acknowledgements", "text": "This work was supported by the HASLER foundation through the grant \u201cInformation and Communication Technology for a Better World 2020\u201d (SmartWorld)."}], "references": [{"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["J.A. Bullinaria", "J.P. Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria and Levy.,? \\Q2007\\E", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2007}, {"title": "Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD", "author": ["J.A. Bullinaria", "J.P. Levy"], "venue": "Behav Res Methods,", "citeRegEx": "Bullinaria and Levy.,? \\Q2012\\E", "shortCiteRegEx": "Bullinaria and Levy.", "year": 2012}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["R. Collobert", "J. Weston"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Collobert and Weston.,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "Finkelstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "A synopsis of linguistic theory 1930-55", "author": ["J.R. Firth"], "venue": null, "citeRegEx": "Firth.,? \\Q1952\\E", "shortCiteRegEx": "Firth.", "year": 1952}, {"title": "Distributional Representations for Handling Sparsity in Supervised SequenceLabeling. In Proceedings of the Association for Computational Linguistics (ACL), pages 495\u2013503", "author": ["F. Huang", "A. Yates"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Huang and Yates.,? \\Q2009\\E", "shortCiteRegEx": "Huang and Yates.", "year": 2009}, {"title": "Principal Component Analysis", "author": ["I.T. Jolliffe"], "venue": null, "citeRegEx": "Jolliffe.,? \\Q1986\\E", "shortCiteRegEx": "Jolliffe.", "year": 1986}, {"title": "A solution to Plato\u2019s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer and Dumais.,? \\Q1997\\E", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Word Embeddings through Hellinger PCA", "author": ["R. Lebret", "R. Collobert"], "venue": "In Proceedings of the EACL,", "citeRegEx": "Lebret and Collobert.,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Towards a theory of semantic space", "author": ["W. Lowe"], "venue": null, "citeRegEx": "Lowe.,? \\Q2001\\E", "shortCiteRegEx": "Lowe.", "year": 2001}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["M. Luong", "R. Socher", "C.D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR Workshp,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "In NIPS", "citeRegEx": "Mnih and Kavukcuoglu.,? \\Q2013\\E", "shortCiteRegEx": "Mnih and Kavukcuoglu.", "year": 2013}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Contextual Correlates of Synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Commun. ACM,", "citeRegEx": "Rubenstein and Goodenough.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Word representations: A simple and general method for semisupervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["P. Turney", "P. Pantel"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Turney and Pantel.,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Word Category Maps based on Emergent Features Created by ICA", "author": ["J.J. V\u00e4yrynen", "T. Honkela"], "venue": "Proceedings of the STeP\u20192004", "citeRegEx": "V\u00e4yrynen and Honkela.,? \\Q2004\\E", "shortCiteRegEx": "V\u00e4yrynen and Honkela.", "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": "Common approaches calculate the frequencies, apply some transformations (tf-idf, PPMI), reduce the dimensionality and calculate the similarities (Lowe, 2001).", "startOffset": 145, "endOffset": 157}, {"referenceID": 7, "context": "LSA (Landauer and Dumais, 1997), ICA (V\u00e4yrynen and Honkela, 2004)).", "startOffset": 4, "endOffset": 31}, {"referenceID": 19, "context": "LSA (Landauer and Dumais, 1997), ICA (V\u00e4yrynen and Honkela, 2004)).", "startOffset": 37, "endOffset": 65}, {"referenceID": 0, "context": "In Bullinaria and Levy (2007, 2012), the authors provide a full range of factors to use for properly extracting semantic representations from the word cooccurrence statistics of large text corpora. While word co-occurrence statistics are discrete distributions, an information theory measure such as the Hellinger distance seems to be more appropriate than the Euclidean distance over a discrete distribution space. In this respect, Lebret and Collobert (2014) propose to perform a principal component analysis (PCA) of the word cooccurrence probability matrix to represent words in a lower dimensional space, while minimizing the reconstruction error according to the Hellinger distance.", "startOffset": 3, "endOffset": 461}, {"referenceID": 8, "context": "This paper proposes an extension of the work of Lebret and Collobert (2014) by investigating the impact of different factors.", "startOffset": 48, "endOffset": 76}, {"referenceID": 8, "context": "This paper proposes an extension of the work of Lebret and Collobert (2014) by investigating the impact of different factors. Mikolov et al. (2013b) show that a subsampling approach to imbalance between the rare and frequent words improves the performance.", "startOffset": 48, "endOffset": 149}, {"referenceID": 8, "context": "This paper proposes an extension of the work of Lebret and Collobert (2014) by investigating the impact of different factors. Mikolov et al. (2013b) show that a subsampling approach to imbalance between the rare and frequent words improves the performance. Recent approaches for word representation have also shown that large windows of context are helpful to capture semantic information (Mikolov et al., 2013b, Pennington et al., 2014). While, in Lebret and Collobert (2014), only the 10,000 most frequent words from the dictionary W are considered as context dictionary D, we ar X iv :1 41 2.", "startOffset": 48, "endOffset": 477}, {"referenceID": 2, "context": "Recently, distributed approaches based on neural network language models have revived the field of learning word embeddings (Collobert and Weston, 2008, Huang and Yates, 2009, Turian et al., 2010, Mnih and Kavukcuoglu, 2013, Mikolov et al., 2013a). Such approaches are trained to optimally predict the contexts in which words from W tend to appear. Baroni et al. (2014) present a systematic comparison of these predictive models with the models based on co-occurrence counts, which suggests that context-predicting models should be chosen over their count-based counterparts.", "startOffset": 125, "endOffset": 370}, {"referenceID": 8, "context": "With a limited size of context word dictionary D (tens of thousands of words), this operation is performed very quickly (See Lebret and Collobert (2014) paper for details).", "startOffset": 125, "endOffset": 153}, {"referenceID": 12, "context": "Mikolov et al. (2013b) have shown that better word representations can be obtained by subsampling of the frequent words.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "It was introduced in Mikolov et al. (2013a) and contains 19,544 such questions, divided into a semantic subset and a syntactic subset.", "startOffset": 21, "endOffset": 44}, {"referenceID": 3, "context": "These include the WordSimilarity353 Test Collection (WS-353) (Finkelstein et al., 2002), the Rubenstein and Goodenough dataset (RG65) (Rubenstein and Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al.", "startOffset": 61, "endOffset": 87}, {"referenceID": 16, "context": ", 2002), the Rubenstein and Goodenough dataset (RG65) (Rubenstein and Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al.", "startOffset": 54, "endOffset": 87}, {"referenceID": 11, "context": ", 2002), the Rubenstein and Goodenough dataset (RG65) (Rubenstein and Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al., 2013).", "startOffset": 121, "endOffset": 141}, {"referenceID": 9, "context": "For the word analogy task, we used the objective function 3CosMul defined by Levy and Goldberg (2014), as we are dealing with explicit word representations in this case.", "startOffset": 77, "endOffset": 102}, {"referenceID": 6, "context": "the quality of its output can suffer dramatically in the face of only a few grossly corrupted points (Jolliffe, 1986).", "startOffset": 101, "endOffset": 117}, {"referenceID": 6, "context": "the quality of its output can suffer dramatically in the face of only a few grossly corrupted points (Jolliffe, 1986). Covariance-based PCA as proposed in Lebret and Collobert (2014) is thus not an approach offering a complete guarantee of success.", "startOffset": 102, "endOffset": 183}], "year": 2015, "abstractText": "Recent works on word representations mostly rely on predictive models. Distributed word representations (aka word embeddings) are trained to optimally predict the contexts in which the corresponding words tend to appear. Such models have succeeded in capturing word similarities as well as semantic and syntactic regularities. Instead, we aim at reviving interest in a model based on counts. We present a systematic study of the use of the Hellinger distance to extract semantic representations from the word co-occurrence statistics of large text corpora. We show that this distance gives good performance on word similarity and analogy tasks, with a proper type and size of context, and a dimensionality reduction based on a stochastic low-rank approximation. Besides being both simple and intuitive, this method also provides an encoding function which can be used to infer unseen words or phrases. This becomes a clear advantage compared to predictive models which must train these new words.", "creator": "LaTeX with hyperref package"}}}