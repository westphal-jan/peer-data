{"id": "1412.7054", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2014", "title": "Attention for Fine-Grained Categorization", "abstract": "this paper presents early experiments extending the work of ba et al. ( 2014 ) on recurrent neural models for attention into less constrained visual environments, explicitly beginning with fine - grained categorization on the stanford dogs data set. in this work we use an rnn of the same structure but substitute a more powerful visual network and perform large - scale pre - training of the innate visual network functionality outside of the attention rnn. most work documented in attention capture models to fairly date focuses on tasks designed with toy or more constrained visual environments. we present results comparing our model to theoretical state - much of - the - art in fine - grained categorization as well as state - representation of - the - art deep visual models.", "histories": [["v1", "Mon, 22 Dec 2014 17:06:07 GMT  (1546kb,D)", "http://arxiv.org/abs/1412.7054v1", "ICLR 2015 Workshop submission"], ["v2", "Sat, 28 Feb 2015 00:15:45 GMT  (9774kb,D)", "http://arxiv.org/abs/1412.7054v2", "ICLR 2015 Workshop submission"], ["v3", "Sat, 11 Apr 2015 01:45:56 GMT  (9775kb,D)", "http://arxiv.org/abs/1412.7054v3", "ICLR 2015 Workshop"]], "COMMENTS": "ICLR 2015 Workshop submission", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["pierre sermanet", "rea frome", "esteban real"], "accepted": true, "id": "1412.7054"}, "pdf": {"name": "1412.7054.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["sermanet@google.com", "afrome@google.com", "ereal@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "This work presents early experiments extending the work of Ba et al. (2014) to less constrained visual environments, beginning with fine-grained categorization. Ba et al. (2014) tackles the challenging problem of sequence prediction in simplified visual settings (MNIST and Street View House Numbers) using a recurrent model of attention similar to Mnih et al. (2014). Complementary to that work, we are addressing the simpler task of classification but in a visual environment with significant clutter and occlusion, variations in lighting and pose, and a more difficult class discrimination task. Previous work in learned visual attention models have tackled a number of computer vision problems and demonstrated the benefits of various attention mechanisms, though most of the work focuses on toy or more constrained environments, such as tasks based on MNIST digits (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014), the vision-control game of \u201ccatch\u201d (Mnih et al., 2014), expression classification for 100\u00d7100 aligned faces (Larochelle & Hinton, 2010; Zheng et al., 2014), detection of frontal faces (Tang et al., 2013), and tracking of hockey players (Bazzani et al., 2011; Denil et al., 2012).\nFigure 1: Three classes from the Dogs data set that are difficult to tell apart due to high intra-class variability and high similarity across classes. The lines show the class boundaries; the classes are Eskimo Dog on the left, Malamute in the center, and Siberian Husky on the right. Of the 120 classes in Stanford Dogs, our model performs worst on Siberian Husky.\nar X\niv :1\n41 2.\n70 54\nv1 [\ncs .C\nV ]\n2 2\nD ec\n2 01\n4\nWe apply the visual attention model from Ba et al. (2014) to the Stanford Dogs fine-grained categorization task (Khosla et al., 2011), choosing to perform the task without using the provided bounding boxes for training or testing. This amounts to learning to simultaneously localize and classify objects within scenes despite difficult class boundaries, large variations in pose and lighting, varying and cluttered backgrounds, and occlusion (Figure 1). Fine-grained categorization is a natural proving ground for attention-based models. When performing classification at the sub-category level, e.g. German Shepherd versus Poodle, the background is often uncorrelated with class and acts as a distraction to the primary task. As a result, several hand-crafted vision pipelines use provided bounding boxes to isolate the object of interest or may perform segmentation of the object from the background, e.g. Parkhi et al. (2011); Chai et al. (2013); Angelova & Zhu (2013). Attention models could address this challenge by learning to focus processing and discriminatory power on the parts of the image that are relevant for the task. In addition to ignoring the distractors in the image, a good attention model could learn to focus processing power on the specific features of the objects that help to tell them apart, for example the face, ears, and tail for dogs. Future versions of this model could potentially also choose the scale at which to examine details."}, {"heading": "2 MODEL DESCRIPTION", "text": "The structure of our model is the same as that presented in Ba et al. (2014) with only a few differences; we refer the reader to that paper for a complete explanation of the model and discuss the differences is this section. (1) Closer to the classification network described in Mnih et al. (2014), our model chooses actions for N glimpses and then classifies only after the final glimpse, as opposed to the sequence task in Ba et al. (2014). The number of glimpses is fixed in each experiment. (2) Since our images vary in size across the data set, our \u201cfoveal\u201d glimpse patches are sized using a ratio to the shortest side of the input image. When converting the Cartesian coordinate ln into pixel space, we use this relative fovea size as our scaling factor. (3) We use a \u201cvanilla\u201d RNN instead of an LSTM, where r(1)n and r (2) n at glimpse n each consist of 4,096 nodes, and rn(i) is fully-connected to rn+1(i) for i = 1, 2. (4) Instead of element-wise multiplying the outputs of the glimpse visual core Gimage(xn|Wimage) and Gloc(ln|Wloc), our model linearly combines them by concatenating their outputs and passing through a fully-connected layer.\nThe final and largest difference is that we replace the visual glimpse network Gimage(xn|Wimage) with a more powerful visual core based on the \u201cGoogLeNet\u201d model (Szegedy et al., 2014) that won the ILSVRC 2014 classification challenge. The full model is designed for 224\u00d7224 inputs and thus cannot be directly applied to smaller inputs because of its subsampling layers, so we chop off the last two \u201cinception\u201d layers, skipping 5 convolutional layers and an average-pooling layer. The full depth could be kept by removing some pooling layers, and we will study the effect of different architectures, including shallower ones, in future work. We replicate this model for each input scale, yielding 3 \u201ctowers\u201d which share all parameters, and join their outputs with depth concatenating layers. All towers are jointly trained by back-propagating error from multiple 1000-way softmax classifiers (called \u201cheads\u201d) as shown in Figure 2. This multi-headed training model ensures each tower remains independently relevant even if another tower is more informative (the largest scale typically yields best results and learning might rely solely on it without a regularization mechanism). During pre-training, the model is given multi-scale glimpses extracted from ImageNet training data that are sampled uniformly across the image and from a range of scales. During training of the attention model, we remove all training heads and take the output of the depth concatenation of multiple towers as glimpse input as shown in Figure 2. We hold the visual core\u2019s parameters fixed during attention model training.\nIn our experiments, we did not fine-tune the visual core used in the RNN on the Stanford Dogs task, however the Stanford Dogs classes and images are a subset of the ILSVRC set. We learned close to the submission deadline that the Stanford Dogs test set overlaps with the ILSVRC 2012 training set; we are in the process of creating a new version of the ILSVRC training set and retraining our models, and we will update all affected results as soon as possible."}, {"heading": "3 EXPERIMENTAL RESULTS", "text": "We trained and evaluated our model on the Stanford Dogs fine-grained categorization data set (Khosla et al., 2011). The task is to categorize each of 8,580 test images as containing one of 120 dog breeds. The training set consists of 100 images per class, and the test images are unevenly distributed across classes, averaging about 71 test images per class. The training and test sets both include bounding boxes that provide a tight crop around the target dog, and while the best results we know of in the literature use the bounding boxes both in training and testing, we use neither the training nor testing boxes. We follow the practice common in the literature of augmenting the training set by reflecting the images along the vertical axis. Our model starts from the full images, without cropping, reshaping, or scaling. We performed experiments and chose hyperparameters using an 80/20 training/validation split of the Stanford Dogs training set, and followed with additional training on the full training set. We used the test set only to report final numbers.\nThe background in the images is not highly correlated with the class label, so any method not using the bounding boxes needs to localize the object of interest in order to classify it. This is a nice task to explore for our attention model in a couple ways: (1) the model can first leverage the context image in order to focus its glimpses on the object of interest, and (2) we can intuit which parts of the image the model should observe to make a prediction. With many other natural image object classification data sets, such as ImageNet, the signal from the surrounding context is mixed with the object for classification (e.g. boats are expected to be surrounded by water). The size of the data set is also more suitable to a deep learning method than most other available fine-grained data sets, though Caltech-UCSD Birds 2011 (Wah et al., 2011) is similar in size with 12,000 training images for 200 categories. Lastly, it remains a difficult data set, with a large amount of intra-class variation, similarity across classes, and large variation in pose, lighting, and background (Figure1).\nTable 1 shows the mean accuracy (mA) for two variations of our attention model on the test set: one resolution and three resolutions, both using one glimpse. These are compared with previously published results 1. Our model, trained and tested without cropping to the bounding boxes, outperforms state-of-the-art results that use bounding boxes both for training and testing2.\n1Missing from the results are entries to the FGComp 2013 fine-grained competition. There are highperforming entries from deep learning models in the dogs category, though to our knowledge these models have not been published. CognitiveVision and CafeNet scored 61% and 57% on the challenge, respectively, using training and test bounding boxes. The challenge training set is from Stanford Dogs, but the test set is independent, the class labels have not been made public, and the evaluation server is no longer running. As such, we cannot compare directly to these numbers, but we have been told anecdotally that scores on the FGComp 2013 challenge tend to be about 10% absolute lower than on the Stanford Dogs test set.\n2 Results that are pre-trained on ImageNet are likely inflated by a few percent due to overlap in the ImageNet pre-training data set and the Stanford Dogs test set, which we learned of just before submission. Our attention model results also accidentally do not include 60 test images. We will address both of these and update affected numbers shortly.\nComparison to results not using deep learning does not give a good sense of the strength of the model, however. In the last couple years deep nets have been winning the ILSVRC classification challenge by a significant margin, so it may be expected that a deep neural net would outperform the existing results. To address this we experimented with the GoogLeNet model on the full image without the attention RNN. We experimented with two baseline versions of GoogLeNet: the \u201cfull\u201d model from Szegedy et al. (2014), trained and tested on 224\u00d7224 padded versions of the full Dog images; and the \u201ctruncated\u201d model that has the same layers as our attention visual core but takes 96\u00d796 padded full Dog images in place of the glimpse input. Both versions were pre-trained using the ILSVRC data set, then the top fully-connected layer and softmax layers were resized and reinitialized, and the full model was trained to convergence on the Dogs training set. In addition to mirroring, brightness and color transformations were also applied to the training images. The full GoogLeNet baseline reached 78% mA on the Dogs test set2, which is a 15% increase over our three-resolution, two-glimpse attention model. Note that this is using only one view of the whole image and a single model for a more meaningful comparison to our single view and single model attention system. Results can be improved substantially by using standard techniques such as multiview aggregations and model ensembles. The truncated baseline, however, only achieved 42% mA, demonstrating that the lost resolution and reduced network size has a large effect. To better compare to the 42% result, we trained a version of the attention model that uses one glimpse and only the medium resolution, rescaled to 96\u00d796, and this model reached 66% mA. This version has the same number of input pixels available for classification as the truncated baseline and performs about the same amount of processing on those pixels, but displays a significant increase over the truncated baseline, which we attribute to it choosing a more informative patch to process based on the context image. If we use one glimpse and all three resolutions for glimpse input, mean accuracy increases to 71%.\nTo tease apart the benefit from learning glimpse locations from the structure of the model, we compare a learned two-glimpse model that uses two glimpse resolutions to two baseline models that have the same model structure and also process for two glimpse steps but choose their locations differently: (1) center for both glimpses and (2) uniformly random across the center 80% of the input image. Figure 3 shows the training and validation curves for our 80/20 split of the Dogs training data set. The model that learns the glimpses has a lower training error and a significantly lower validation error. The center-glimpse model achieves the same training error as the learned model but performs\nmuch worse on the validation set, demonstrating that it has overfit to the information in the center of the images, despite the data set having a center bias. The random glimpses are better regularized than the center glimpses, but still perform significantly worse than the learned glimpses.\nLastly, we compared one-, two-, and three- glimpse versions of the three-resolution model. As reported above, the one-glimpse three-resolution model reaches 71% mA. We expected to see significant benefit from more glimpses, but the two- and three-glimpse models reached 71% and 70%, respectively, after training error converged. Looking at the glimpse locations on the validation set, we see that the first glimpse is often near the center of the image and the subsequent glimpses do not move very far. There are many potential causes for why more glimpses do not improve performance, and we believe that by continuing to explore design and algorithmic choices we will reach performance on par with convolutional nets on this task for a lower computational cost."}], "references": [{"title": "Efficient object detection and segmentation for fine-grained recognition", "author": ["Angelova", "Anelia", "Zhu", "Shenghuo"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Angelova et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Angelova et al\\.", "year": 2013}, {"title": "Multiple Object Recognition with Visual Attention", "author": ["Ba", "Jimmy", "Mnih", "Volodymyr", "Kavukcuoglu", "Koray"], "venue": "CoRR, TBD,", "citeRegEx": "Ba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2014}, {"title": "Learning attentional policies for object tracking and recognition in video with deep networks", "author": ["Bazzani", "Loris", "de Freitas", "Nando", "Larochelle", "Hugo", "Murino", "Vittorio", "Ting", "Jo-Anne"], "venue": "In ICML\u201911: Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Bazzani et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bazzani et al\\.", "year": 2011}, {"title": "Symbiotic Segmentation and Part Localization for Fine-Grained Categorization", "author": ["Chai", "Yuning", "Lempitsky", "Victor", "Zisserman", "Andrew"], "venue": "In ICCV\u201913: IEEE International Conference on Computer Vision,", "citeRegEx": "Chai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chai et al\\.", "year": 2013}, {"title": "Learning where to attend with deep architectures for image tracking", "author": ["Denil", "Misha", "Bazzani", "Loris", "Larochelle", "Hugo", "de Freitas", "Nando"], "venue": "Neural Comput.,", "citeRegEx": "Denil et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2012}, {"title": "Fine-Grained Categorization by Alignments", "author": ["Gavves", "Efstratios", "Fernando", "Basura", "Snoek", "Cees", "Smeulders", "Arnold", "Tuytelaars", "Tinne"], "venue": "In ICCV\u201913: IEEE International Conference on Computer Vision,", "citeRegEx": "Gavves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gavves et al\\.", "year": 2013}, {"title": "Novel Dataset for Fine-Grained Image Categorization", "author": ["Khosla", "Aditya", "Jayadevaprakash", "Nityananda", "Yao", "Bangpeng", "Fei-Fei", "Li"], "venue": "In First Workshop on Fine-Grained Visual Categorization,", "citeRegEx": "Khosla et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Khosla et al\\.", "year": 2011}, {"title": "Learning to combine foveal glimpses with a third-order boltzmann machine", "author": ["Larochelle", "Hugo", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Larochelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2010}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex", "kavukcuoglu", "koray"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "The Truth About Cats and Dogs", "author": ["Parkhi", "Omkar M", "Vedaldi", "Andrea", "C.V. Jawahar", "Zisserman", "Andrew"], "venue": "In ICCV\u201911: IEEE International Conference on Computer Vision,", "citeRegEx": "Parkhi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Parkhi et al\\.", "year": 2011}, {"title": "On Learning Where To Look", "author": ["Ranzato", "Marc\u2019Aurelio"], "venue": "CoRR, abs/1405.5488,", "citeRegEx": "Ranzato and Marc.Aurelio.,? \\Q2014\\E", "shortCiteRegEx": "Ranzato and Marc.Aurelio.", "year": 2014}, {"title": "Learning generative models with visual attention", "author": ["Tang", "Yichuan", "Srivastava", "Nitish", "Salakhutdinov", "Ruslan"], "venue": "CoRR, abs/1312.6110,", "citeRegEx": "Tang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2013}, {"title": "The CaltechUCSD Birds-200-2011 Dataset", "author": ["Wah", "Catherine", "Branson", "Steve", "Welinder", "Peter", "Perona", "Pietro", "Belongie", "Serge"], "venue": "Technical Report CNS-TR-2011-001, California Institute of Technology,", "citeRegEx": "Wah et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wah et al\\.", "year": 2011}, {"title": "Unsupervised Template Learning for Fine-Grained Object Recognition", "author": ["Yang", "Shulin", "Bo", "Liefeng", "Wang", "Jue", "Shapiro", "Linda G"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "A Neural Autoregressive Approach to Attention-based Recognition", "author": ["Zheng", "Yin", "Zemel", "RichardS", "Zhang", "Yu-Jin", "Larochelle", "Hugo"], "venue": "IJCV\u201914: International Journal of Computer Vision, pp", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "This paper presents early experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, beginning with fine-grained categorization on the Stanford Dogs data set.", "startOffset": 60, "endOffset": 77}, {"referenceID": 2, "context": "Previous work in learned visual attention models have tackled a number of computer vision problems and demonstrated the benefits of various attention mechanisms, though most of the work focuses on toy or more constrained environments, such as tasks based on MNIST digits (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014), the vision-control game of \u201ccatch\u201d (Mnih et al.", "startOffset": 271, "endOffset": 374}, {"referenceID": 4, "context": "Previous work in learned visual attention models have tackled a number of computer vision problems and demonstrated the benefits of various attention mechanisms, though most of the work focuses on toy or more constrained environments, such as tasks based on MNIST digits (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014), the vision-control game of \u201ccatch\u201d (Mnih et al.", "startOffset": 271, "endOffset": 374}, {"referenceID": 8, "context": "Previous work in learned visual attention models have tackled a number of computer vision problems and demonstrated the benefits of various attention mechanisms, though most of the work focuses on toy or more constrained environments, such as tasks based on MNIST digits (Larochelle & Hinton, 2010; Bazzani et al., 2011; Denil et al., 2012; Ranzato, 2014; Mnih et al., 2014), the vision-control game of \u201ccatch\u201d (Mnih et al.", "startOffset": 271, "endOffset": 374}, {"referenceID": 8, "context": ", 2014), the vision-control game of \u201ccatch\u201d (Mnih et al., 2014), expression classification for 100\u00d7100 aligned faces (Larochelle & Hinton, 2010; Zheng et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 14, "context": ", 2014), expression classification for 100\u00d7100 aligned faces (Larochelle & Hinton, 2010; Zheng et al., 2014), detection of frontal faces (Tang et al.", "startOffset": 61, "endOffset": 108}, {"referenceID": 11, "context": ", 2014), detection of frontal faces (Tang et al., 2013), and tracking of hockey players (Bazzani et al.", "startOffset": 36, "endOffset": 55}, {"referenceID": 2, "context": ", 2013), and tracking of hockey players (Bazzani et al., 2011; Denil et al., 2012).", "startOffset": 40, "endOffset": 82}, {"referenceID": 4, "context": ", 2013), and tracking of hockey players (Bazzani et al., 2011; Denil et al., 2012).", "startOffset": 40, "endOffset": 82}, {"referenceID": 1, "context": "This work presents early experiments extending the work of Ba et al. (2014) to less constrained visual environments, beginning with fine-grained categorization.", "startOffset": 59, "endOffset": 76}, {"referenceID": 1, "context": "This work presents early experiments extending the work of Ba et al. (2014) to less constrained visual environments, beginning with fine-grained categorization. Ba et al. (2014) tackles the challenging problem of sequence prediction in simplified visual settings (MNIST and Street View House Numbers) using a recurrent model of attention similar to Mnih et al.", "startOffset": 59, "endOffset": 178}, {"referenceID": 1, "context": "This work presents early experiments extending the work of Ba et al. (2014) to less constrained visual environments, beginning with fine-grained categorization. Ba et al. (2014) tackles the challenging problem of sequence prediction in simplified visual settings (MNIST and Street View House Numbers) using a recurrent model of attention similar to Mnih et al. (2014). Complementary to that work, we are addressing the simpler task of classification but in a visual environment with significant clutter and occlusion, variations in lighting and pose, and a more difficult class discrimination task.", "startOffset": 59, "endOffset": 368}, {"referenceID": 6, "context": "(2014) to the Stanford Dogs fine-grained categorization task (Khosla et al., 2011), choosing to perform the task without using the provided bounding boxes for training or testing.", "startOffset": 61, "endOffset": 82}, {"referenceID": 1, "context": "We apply the visual attention model from Ba et al. (2014) to the Stanford Dogs fine-grained categorization task (Khosla et al.", "startOffset": 41, "endOffset": 58}, {"referenceID": 1, "context": "We apply the visual attention model from Ba et al. (2014) to the Stanford Dogs fine-grained categorization task (Khosla et al., 2011), choosing to perform the task without using the provided bounding boxes for training or testing. This amounts to learning to simultaneously localize and classify objects within scenes despite difficult class boundaries, large variations in pose and lighting, varying and cluttered backgrounds, and occlusion (Figure 1). Fine-grained categorization is a natural proving ground for attention-based models. When performing classification at the sub-category level, e.g. German Shepherd versus Poodle, the background is often uncorrelated with class and acts as a distraction to the primary task. As a result, several hand-crafted vision pipelines use provided bounding boxes to isolate the object of interest or may perform segmentation of the object from the background, e.g. Parkhi et al. (2011); Chai et al.", "startOffset": 41, "endOffset": 929}, {"referenceID": 1, "context": "We apply the visual attention model from Ba et al. (2014) to the Stanford Dogs fine-grained categorization task (Khosla et al., 2011), choosing to perform the task without using the provided bounding boxes for training or testing. This amounts to learning to simultaneously localize and classify objects within scenes despite difficult class boundaries, large variations in pose and lighting, varying and cluttered backgrounds, and occlusion (Figure 1). Fine-grained categorization is a natural proving ground for attention-based models. When performing classification at the sub-category level, e.g. German Shepherd versus Poodle, the background is often uncorrelated with class and acts as a distraction to the primary task. As a result, several hand-crafted vision pipelines use provided bounding boxes to isolate the object of interest or may perform segmentation of the object from the background, e.g. Parkhi et al. (2011); Chai et al. (2013); Angelova & Zhu (2013).", "startOffset": 41, "endOffset": 949}, {"referenceID": 1, "context": "We apply the visual attention model from Ba et al. (2014) to the Stanford Dogs fine-grained categorization task (Khosla et al., 2011), choosing to perform the task without using the provided bounding boxes for training or testing. This amounts to learning to simultaneously localize and classify objects within scenes despite difficult class boundaries, large variations in pose and lighting, varying and cluttered backgrounds, and occlusion (Figure 1). Fine-grained categorization is a natural proving ground for attention-based models. When performing classification at the sub-category level, e.g. German Shepherd versus Poodle, the background is often uncorrelated with class and acts as a distraction to the primary task. As a result, several hand-crafted vision pipelines use provided bounding boxes to isolate the object of interest or may perform segmentation of the object from the background, e.g. Parkhi et al. (2011); Chai et al. (2013); Angelova & Zhu (2013). Attention models could address this challenge by learning to focus processing and discriminatory power on the parts of the image that are relevant for the task.", "startOffset": 41, "endOffset": 972}, {"referenceID": 1, "context": "The structure of our model is the same as that presented in Ba et al. (2014) with only a few differences; we refer the reader to that paper for a complete explanation of the model and discuss the differences is this section.", "startOffset": 60, "endOffset": 77}, {"referenceID": 1, "context": "The structure of our model is the same as that presented in Ba et al. (2014) with only a few differences; we refer the reader to that paper for a complete explanation of the model and discuss the differences is this section. (1) Closer to the classification network described in Mnih et al. (2014), our model chooses actions for N glimpses and then classifies only after the final glimpse, as opposed to the sequence task in Ba et al.", "startOffset": 60, "endOffset": 298}, {"referenceID": 1, "context": "The structure of our model is the same as that presented in Ba et al. (2014) with only a few differences; we refer the reader to that paper for a complete explanation of the model and discuss the differences is this section. (1) Closer to the classification network described in Mnih et al. (2014), our model chooses actions for N glimpses and then classifies only after the final glimpse, as opposed to the sequence task in Ba et al. (2014). The number of glimpses is fixed in each experiment.", "startOffset": 60, "endOffset": 442}, {"referenceID": 6, "context": "We trained and evaluated our model on the Stanford Dogs fine-grained categorization data set (Khosla et al., 2011).", "startOffset": 93, "endOffset": 114}, {"referenceID": 12, "context": "The size of the data set is also more suitable to a deep learning method than most other available fine-grained data sets, though Caltech-UCSD Birds 2011 (Wah et al., 2011) is similar in size with 12,000 training images for 200 categories.", "startOffset": 154, "endOffset": 172}, {"referenceID": 3, "context": "Table 1: Previous state-of-the-art results on Stanford Dogs test set, measured by mean accuracy (mA) as described in Chai et al. (2013).", "startOffset": 117, "endOffset": 136}, {"referenceID": 3, "context": "38 Chai et al. (2013) 0.", "startOffset": 3, "endOffset": 22}, {"referenceID": 3, "context": "38 Chai et al. (2013) 0.46 Gavves et al. (2013) 0.", "startOffset": 3, "endOffset": 48}], "year": 2014, "abstractText": "This paper presents early experiments extending the work of Ba et al. (2014) on recurrent neural models for attention into less constrained visual environments, beginning with fine-grained categorization on the Stanford Dogs data set. In this work we use an RNN of the same structure but substitute a more powerful visual network and perform large-scale pre-training of the visual network outside of the attention RNN. Most work in attention models to date focuses on tasks with toy or more constrained visual environments. We present results comparing our model to state-of-the-art in fine-grained categorization as well as state-of-the-art deep visual models.", "creator": "LaTeX with hyperref package"}}}