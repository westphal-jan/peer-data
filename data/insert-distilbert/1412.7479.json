{"id": "1412.7479", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Dec-2014", "title": "Deep Networks With Large Output Spaces", "abstract": "deep neural networks have largely been extremely successful at various biomedical image, speech, voice video recognition tasks because of their ability to model deep structures within the data. however, they are still prohibitively expensive to train and apply repeatedly for problems containing millions of classes in the output layer. based on the observation that the key computation common to most neural cell network layers is a vector / matrix product, we propose pursuing a novel fast locality - sensitive hashing technique to theoretically approximate the actual dot product enabling us to scale up the training and inference to millions of output classes. we evaluate our technique on three diverse large - scale recognition tasks ) and show that our approach can train large - scale 3d models at a faster rate ( in terms numbers of shortest steps / total time ) repeatedly compared to baseline communication methods.", "histories": [["v1", "Tue, 23 Dec 2014 19:22:59 GMT  (1202kb)", "https://arxiv.org/abs/1412.7479v1", null], ["v2", "Mon, 29 Dec 2014 18:45:36 GMT  (594kb)", "http://arxiv.org/abs/1412.7479v2", null], ["v3", "Sat, 28 Feb 2015 01:12:58 GMT  (709kb)", "http://arxiv.org/abs/1412.7479v3", null], ["v4", "Fri, 10 Apr 2015 19:53:21 GMT  (711kb)", "http://arxiv.org/abs/1412.7479v4", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sudheendra vijayanarasimhan", "jonathon shlens", "rajat monga", "jay yagnik"], "accepted": true, "id": "1412.7479"}, "pdf": {"name": "1412.7479.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["svnaras@google.com", "shlens@google.com", "rajatmonga@google.com", "jyagnik@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n41 2.\n74 79\nv4 [\ncs .N\nE ]\n1 0\nA pr\n2 01\n5 Accepted as a workshop contribution at ICLR 2015"}, {"heading": "1 INTRODUCTION", "text": "Deep neural networks have proven highly successful at various image, speech, language and video recognition tasks (Krizhevsky et al., 2012; Mikolov et al., 2013; Karpathy et al., 2014). These networks typically have several layers of units connected in feedforward fashion between the input and output spaces. Each layer performs a specific function such as convolution, pooling, normalization, or plain matrix products in the case of fully connected layers followed by some form of non-linear activation such as sigmoid or rectified linear units.\nDespite their attractive qualities, and the relative efficiency of their local architecture, these networks are still prohibitively expensive to train and apply for large-scale problems containing millions of classes or nodes. There are several such problems proposed in the literature. The Imagenet dataset which is one of the largest datasets for image classification contains around 21000 classes. Wordnet, which is a superset of Imagenet, consists of 117, 00 synsets. Freebase, which is a community-curated database of well-known people, places, and things contains close to 20 million entities. Image models of text queries have ranged from 100000 queries in academic benchmarks Weston et al. (2011b) to several million in commercial search engines such as Google, Bing, Yahoo, etc. Duplicate video content identification (Shang et al., 2010; Song et al., 2011; Zhao et al., 2007) and video recommendations are also large-scale problems with millions of classes.\nWe note that the key computation common to softmax/logistic regression layers is a matrix product between the activations from a layer, x, and the weights of the connections to the next layer, W . As the number of classes increases this computation becomes the main bottleneck of the entire network. Based on this observation, we exploit a fast locality-sensitive hashing technique (Yagnik et al., 2011) in order to approximate the actual dot product in the final output layer which enables us to scale up the training and inference to millions of output classes.\nOur main idea is to approximate the dot product between the output layer\u2019s parameter vector and the input activations using hashing. We first compute binary hash codes for the parameter vectors, W , of a layer\u2019s output nodes and store the indices of the nodes in locations corresponding to the hash codes within hash tables. During inference, given an input activation vector, x, we compute the hash codes of the vector and retrieve the set of output nodes Ok that are closest to the input vector in the hash space. Following this we compute the actual dot product between x and the parameter vectors of Ok and set all other values to zero.\nBy avoiding the expensive dot product operation between the input activation vector and all output nodes we show that our approach can easily scale up to millions of output classes during inference.\nFurthermore, using the same technique when training the models, we show that our approach can train large-scale models at a faster rate both in terms of number of steps and the total time compared to both the standard softmax layers and the more computationally efficient hierarchical softmax layer of (Mikolov et al., 2013)."}, {"heading": "2 RELATED WORK", "text": "Several methods have been proposed for performing classification in deep networks over large vocabularies. Traditional methods such as logistic regression and softmax (multinomial regression) are known to have poor scaling properties with the number of classes (Dean et al., 2013) as the number of dot products grows with the number of classes C that must be considered.\nOne method for contending with this is hierarchical softmax whereby a tree is constructed of depth log2C in which the leaves are the individual classes which must be classified (Morin & Bengio, 2005; Mikolov et al., 2013). A benefit of this approach is that each step merely requires computations associated with the tree traversal to an individual leaf.\nA second direction is to instead train a dense embedding space representation and perform classification by employing k-nearest-neighbors in this embedding space on unseen examples. Typical methods for training such embedding representations employ a hinge rank loss with a clever selection of negative examples, e.g. (Weston et al., 2011a).\nLocality sensitive hashing (LSH) (Gionis et al., 1999) provides a third alternative by providing methods to perform approximate nearest neighbor search in sub-linear time for various similarity metrics. An LSH scheme based on ordinal similarity is proposed in (Yagnik et al., 2011) which is used in (Dean et al., 2013) to speed-up filter based object detection. We expand on these techniques to enable learning large-scale deep network models."}, {"heading": "3 APPROACH", "text": "The goal of this work is to enable approximate computation of the matrix product of the parameters of a layer and its input activations, xTW , in a deep network so that the number of output dimensions can be increased by several orders of magnitude. In the following sections, we demonstrate that a locality sensitive hashing based approximation can provide such a solution without too much degradation in overall accuracy. As a first step we employ this technique to scale up the final classification layer since the benefits of hashing are easily seen when the cardinality is quite large."}, {"heading": "3.1 SOFTMAX/LOGISTIC CLASSIFICATION", "text": "Softmax and logistic regression functions are two popular choices for the final layer of a deep network for multi-class and binary classification problems respectively. Formally, the two functions are defined as\nPsoftmax(y = j|x) = ex\nTwj\n\u2211N k=1 e xTwk (1)\nPlogistic(y = j|x) = 1\n1 + e\u2212(x Twj+\u03b2j)\n(2)\n(3)\nwhere P (y = j|x) is the probability of the jth class given the input vector x and {wj , j = 1...N} are distinct linear functions for the N classes.\nWhen the number of classes is large, not all classes are relevant to a given input example. Therefore, in many situations we are only interested in the K classes with the highest probabilities. We could obtain the top K classes by equivalently determining the K vectors, WK , that have the largest dot products with the input vector x and computing the probabilities for only these K classes, setting all others to zero.\nWe note that this is equivalent to the problem of finding the approximate nearest neighbors of a vector based on cosine (dot product) similarity which has a rich literature beginning with the seminal work of (Gionis et al., 1999). It has been shown that approximate nearest neighbors can be obtained in time that is sub-linear in the number of database vectors with certain guarantees which is the key motivation for our approach.\nIn our case the database vectors are the parameter vectors of the output layer, wj , and the query vector is the input activation from the previous layer x. In this work, we employ the subfamily of hash functions, winner-take-all (WTA) hashing introduced in (Yagnik et al., 2011), since it has been successfully applied for the similar task of scaling up filter-based object detection in (Dean et al., 2013)."}, {"heading": "3.2 WINNER-TAKE-ALL HASHING (WTA)", "text": "Given a vector x or w in Rd, its WTA hash is defined by permuting its elements using P distinct permutations and recording the index of the maximum value of the first k elements (Yagnik et al., 2011). Each index can be compactly represented using log2k bits resulting in P \u2217 log2k bits for the entire hash. The WTA hash has several desirable properties; since the only operation involved in computing the hash is comparison, it can be completely implemented using integer arithmetic and the algorithm can be efficiently implemented without accruing branch prediction penalties.\nFurthermore, each WTA hash function defines an ordinal embedding and it has been shown in (Yagnik et al., 2011) that as P \u2192 d!, the dot product between two WTA hashes tends to the rank correlation between the underlying vectors. Therefore, WTA is well suited as a basis for localitysensitive hashing as ordinal similarity can be used as a more robust proxy for dot product similarity.\nGiven binary hash codes, u of a vector, w, there are several schemes that can be employed in order to perform approximate nearest neighbor search. In this work, we employ the scheme used in (Dean et al., 2013) due to its simplicity and limited overhead.\nIn this scheme, we first divide the compact hash code, u, containingP elements of log2k bits into M bands, um, each containing P/M elements. We create a hash table for each band {Tm,m = 1...M} and store the index of the vector in the hash bins corresponding to um in each hash table Tm. During retrieval, we similarly compute the hash codes of x and divide it into M bands and retrieve the set of all IDs in the corresponding hash bins along with their counts. The counts provide a lower bound for the dot product between the two hash vectors which is related to the ordinal similarity between the two vectors. Therefore, the top K IDs from this list approximate the K nearest neighbors to the input vector based on dot product similarity. The actual dot product can now be computed for these vectors with the input vector x to obtain their probabilities.\nThe complexity of the scheme proposed above depends on the dimensionality of the vectors for computing the hash codes, the number of bands or hash tables that are used during retrieval, M , and the number of IDs for which the actual dot product is computed, K . Since all three quantities are independent of the number of classes in the output layer our approach can accomodate any number of classes in the output layer. As shown in Figure 1, the naive softmax has a complexity of O(d\u2217N+N) whereas our WTA based approximation has a complexity of O(d\u2217K+K+ constant). The overall speed-up we obtain is of the order of N\nK assuming the cost of computing the hash function and\nlookup are much smaller than d \u2217 N . Of course, since both M and K relate to the accuracy of the approximation they provide a trade-off between the time complexity and the accuracy of the network."}, {"heading": "3.3 INFERENCE", "text": "We can apply our proposed approximation during both model inference and training. For inference, given a learned model, we first compute the hash codes of the parameter vectors of the softmax/logistic regression layer and store the IDs of the corresponding classes in the hash table as described in Section 3.2. This is a one time operation that is performed before running inference on any input examples.\nGiven an input example, we pass it through all layers leading up to the classification layer as before and compute the hash codes of the input activations to the classification layer. We then query the hash table to retrieve the top K classes and compute probabilties using Equation 3 for only these classes. Figure 1 shows a rough schematic of this procedure."}, {"heading": "3.4 TRAINING", "text": "We train the models using downpour SGD, an asynchronous stocastic gradient descent procedure supporting a large number of model replicas, proposed in (Dean et al., 2012). During backpropagation we only propagate gradients based on the top K classes that were retrieved during the forward\npass of the model and update the parameter vectors of only these retrieved classes using the error vector. Additionally, we add all the positive labels for an input example to the list of non-zero classes in order to always provide a positive gradient. In Section 4 we show empirical results of performing only these top K updates. These sparse gradients are much more computationally efficient and additionally perform the function of hard negative mining since only the closest classes to a particular example are updated.\nWhile inference using WTA hashing is straightforward to implement, there are several challenges that need to be solved to make training efficient using such a scheme. Firstly, unlike during inference, the parameter vectors are constantly changing as new examples are seen. It would be infeasible to request updated parameters for all classes and update the hash table after every step.\nHowever, we found that gradients based on a small set of examples do not perturb the parameter vector significantly and WTA hashing is only sensitive to changes in the ordering of the various dimensions and is more robust to small changes in the absolute values of the different dimensions. Based on these observations we implemented a scheme where the hash table locations of classes are updated in batches in a round-robin fashion such that all classes are updated over a course of several hundred or thousand steps which turned out to be quite effective.\nTherefore, we only request updated parameters for the set of retrieved classes, the positive training classes and the classes selected in the round-robin scheme. Figure 2 shows a schematic of these interactions with the parameter server and the hash tables."}, {"heading": "4 EXPERIMENTS", "text": "We empirically evaluate our proposed technique on several large-scale datasets with the aim of investigating the trade-off between accuracy and time complexity of the WTA based softmax classifier in comparison to the baseline approaches of softmax (exhaustive) and hierarchical softmax."}, {"heading": "4.1 IMAGENET 21K", "text": "The 2011 Imagenet 21K dataset consists of 21,900 classes and 14 million images. We split the set into equal partitions of training and testing sets as done in (Le et al., 2012). We selected values of 16, 1000, 3000 for the k, M , P parameters of the WTA approach for all experiments based on results on a small set of images which agreed with the parameters mentioned in (Dean et al., 2013). We varied the value of K , which is the number of retrieved classes for which the actual dot product is computed, since it directly affects both the accuracy and the time complexity of the approach.\nWe used the artichecture proposed in (Krizhevsky et al., 2012) (AlexNet) for all experiments replacing only the classification layer with the proposed approach. All methods were optimized using downpour SGD with a starting learning rate of 0.001 with exponential decay in conjunction with a momentum of 0.9. We used a cluster of about 100 machines containing multi-core CPUs with 20GB of RAM running at 2.4Ghz to perform training and inference.\nFigure 3 first reports the time taken during inference by the WTA Softmax and Softmax layers alone (ignoring the rest of the model) as both the batch size and the top K is varied for this problem. We note that WTA Softmax provides significant speed-up over Softmax for both small batch sizes and small values of K . For large batch sizes Softmax is very efficient due to optimizations over dense matrices. For large values of K the dot product with the retrieved vectors begins to dominate the time complexity.\nFigure 4 report the accuracies obtained when using WTA during inference on a learned model as compared to the baseline accuracy of the softmax model. We find that even with as few as 30 retrieved classes our approach is able to reach up to 83% of the baseline accuracy and almost matches the baseline accuracy with 3000 retrieved classes. Note that the ceiling on this problem is the accuracy of the base network since we are approximating an already trained network using WTA. This vindicates our claim that only a small percentage of classes are relevant to any input example and WTA hashing provides an efficient technique for obtaining the top K most relevant classes for a given input example. Based on these figures we conclude that the proposed approach is advantageous when either N is very large or for small batch sizes.\nFigure 5 reports the trade-off between the speedup achieved over baseline softmax at a fixed batch size and the percentage of the baseline accuracy reached by the WTA model. We find that the WTA model achieves a speedup of 10x over the baseline model at 90% accuracy. Figure 6 reports the speedup achieved at 95% of the baseline accuracy for various batch sizes. As noted previously we find that the WTA model achieves higher speedups for smaller batch sizes."}, {"heading": "4.2 SKIPGRAM DATASET", "text": "One popular application of deep networks has been building and training models of language and semantics. Recent work from (Mikolov et al., 2013; Q.V. Le, 2014) has demonstrated that a shallow, simple architecture can be trained efficiently by across language corpora. The resulting embedding vector representation for language exhibits rich structure about semantics and syntactics that can be exploited for many other purposes. For all of these models, a crucial aspect of training is to predict surrounding and nearby words in a sequence. The prediction task is typically quite large, i.e. the cardinality is the size of the vocabulary, O(1M-10M) words.\nA key insight of recent work has been to exploit novel and efficient methods for performing discrete classification over large cardinalities. In particular, said work employs a hierarchical softmax to fast inference and evaluation.\nAs a test of the predictive performance our hashing techniques, we compare the performance of WTA hashing on the language modeling task. We note that this is an extremely difficult task the perplexity of language (or just cooccurrence statistics of words in a sentence) is quite high. Thus, any method attempts to predict nearby words will at best report low predictive performances.\nIn our experiments, we download and parse Wikipedia consisting of several billion sentences. We tokenize this text corpora with the 1M most popular words. The task of the network is to perform a 1M-way prediction nearby words based on neighboring words.\nWe performed our experiments with three loss functions, traditional softmax, hierarchical softmax and WTA-based softmax. We found measure the precision@K for the top K predictions from each softmax model.\nWe compare all networks with three loss functions after 100 hours of training time across similar CPU time. We find that all networks have converged within this time frame although the hierarhical softmax has processed 100 billion examples while the WTA softmax has processed 100 million examples.\nAs seen in Table 1, we find that WTA softmax achieves superior predictive performance than the hierarchical softmax even though hierarchical softmax has processed O(100) times more examples. In particular, we find that WTA softmax achieves roughly two-fold better predictive performance.\nHowever, the WTA softmax produces underlying embedding vector representations that do not perform as well on analogy tasks as highlighted by (Mikolov et al., 2013). For instance, the hierarchical softmax achieves 50% accuracy on analogy tasks where as WTA softmax produces 5% accuracy on similar tasks. This is partly due to the fewer number of examples processed by WTA in the same time frame as hierarchical softmax is significantly faster than WTA because it performs just log(N) dot products."}, {"heading": "4.3 VIDEO IDENTIFICATION", "text": "While the 21K problem is one of the largest tested for the baseline softmax model, the benefits of hashing are best seen for problems of much larger cardinality. In order to illustrate this we next consider a large-scale classification task for video identification. This task is modeled on Youtube\u2019s content ID classification problem which has also been addressed in several recent work under various settings Shang et al. (2010); Song et al. (2011); Zhao et al. (2007).\nThe task we propose is to predict the ID of a video based on its frames. We use the Sports 1M action recognition dataset introduced in (Karpathy et al., 2014) for this problem. The Sports 1M dataset consists of roughly 1.2 million Youtube sports videos annotated with 487 classes. We divide the first five minutes of each video into two parts where the first 50% of the video\u2019s frames are used for training and the remaining 50% are used for evaluating the models. The prediction space of the problem spans 1.2 million classes and each class has roughly 150 frames for training and evaluation.\nWe trained three models for this problem with the AlexNet architecture where the top layer uses one of softmax, WTA softmax and hierarchical softmax each. We used learning rates of {0.1, 0.05, 0.001, 0.005} and report the best results for each of the models. For WTA we used a value of 3000 for the K parameter based on the results in the previous section and a batch size of 32 for all models.\nFigure 7 reports the accuracy on the evalution set against the number of steps trained for each model and Figure 8 reports the accuracy against the actual time taken to complete these steps. We find that on both counts the WTA based model learns faster than both softmax and hierarchical softmax.\nThe step time of the WTA model is about 4 times lower than the softmax model but about 4 higher than hierarchical softmax. This is because hierarchical softmax is much more efficient as it only computes log(N) dot products compared to K for WTA. However, even though hierarchical softmax processes significantly more number of examples the WTA models is able to achieve much higher accuracies.\nIn order to better understand the significant difference between WTA and the baselines on this task as opposed to the Imagenet 21K problem we computed the in-class variance of all the classes in the two datasets based on the 4000-dim feature from the penultimate layer of the AlexNet model. Figure 9 reports a histogram of the in-class variance of the examples belonging to a class on the two datasets. We find that in the Imagenet task the examples within a class are much more spread out than the Sports 1M task which is expected given that frames within a video would have similar context and more correlation. This could explain the relative efficiency of the top K gradient updates used by the WTA model on the Sports 1M task."}, {"heading": "5 CONCLUSIONS", "text": "We proposed a locality sensitive hashing approach for approximating the computation of xTW in the classification layer of deep network models which enables us to scale up the training and inference of these models to millions of classes. Empirical evaluations of the proposed model on various large-scale datasets shows that the proposed approach provides significant speed-ups over baseline softmax models and can train such large-scale models at a faster rate than alternatives such as hierarchical softmax. Our approach is advantageous whenever the number of classes considered is large or where batching is not possible.\nIn the future we would like to extend this technique to intermediate layers also as the proposed method explicitly places sparsity constraints which is desirable in hierarchical learning. Given the scaling properties of hashing, our approach could, for instance, be used to increase the number of filters used in the convolutional layers from hundreds to tens of thousands with a few hundred being active at any time."}], "references": [{"title": "Large scale distributed deep networks", "author": ["Dean", "Jeffrey", "Corrado", "Greg S", "Monga", "Rajat", "Chen", "Kai", "Devin", "Matthieu", "Le", "Quoc V", "Mao", "Mark Z", "Senior", "Andrew", "Tucker", "Paul", "Yang", "Ke", "Ng", "Andrew Y"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Fast, accurate detection of 100,000 object classes on a single machine", "author": ["Dean", "Thomas", "Ruzon", "Mark A", "Segal", "Mark", "Shlens", "Jonathon", "Vijayanarasimhan", "Sudheendra", "Yagnik", "Jay"], "venue": "In Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Dean et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2013}, {"title": "Similarity search in high dimensions via hashing", "author": ["Gionis", "Aristides", "Indyk", "Piotr", "Motwani", "Rajeev"], "venue": "Proceedings of the 25th International Conference on Very Large Data Bases,", "citeRegEx": "Gionis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gionis et al\\.", "year": 1999}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["Karpathy", "Andrej", "Toderici", "George", "Shetty", "Sanketh", "Leung", "Thomas", "Sukthankar", "Rahul", "Fei-Fei", "Li"], "venue": "In Proc. CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Proc. NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Le", "Quoc V", "Monga", "Rajat", "Devin", "Matthieu", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff", "Ng", "Andrew Y"], "venue": "In In International Conference on Machine Learning", "citeRegEx": "Le et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Le et al\\.", "year": 2012}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In AISTATS\u201905,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "In ICML", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Real-time large scale nearduplicate web video retrieval", "author": ["Shang", "Lifeng", "Yang", "Linjun", "Wang", "Fei", "Chan", "Kwok-Ping", "Hua", "Xian-Sheng"], "venue": "In Proceedings of the International Conference on Multimedia, MM", "citeRegEx": "Shang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2010}, {"title": "Multiple feature hashing for realtime large scale near-duplicate video retrieval", "author": ["Song", "Jingkuan", "Yang", "Yi", "Huang", "Zi", "Shen", "Heng Tao", "Hong", "Richang"], "venue": "In Proceedings of the 19th ACM International Conference on Multimedia, MM", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Weston", "Jason", "Bengio", "Samy", "Usunier", "Nicolas"], "venue": "In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Three,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["Weston", "Jason", "Bengio", "Samy", "Usunier", "Nicolas"], "venue": "In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Three,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "The power of comparative reasoning", "author": ["Yagnik", "Jay", "Strelow", "Dennis", "Ross", "David A", "Lin", "Ruei-sung"], "venue": "In IEEE International Conference on Computer Vision. IEEE,", "citeRegEx": "Yagnik et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yagnik et al\\.", "year": 2011}, {"title": "Near-duplicate keyframe identification with interest point matching and pattern learning", "author": ["Zhao", "Wan-Lei", "Ngo", "Chong-Wah", "Tan", "Hung-Khoon", "Wu", "Xiao"], "venue": "Trans. Multi.,", "citeRegEx": "Zhao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": "Deep neural networks have proven highly successful at various image, speech, language and video recognition tasks (Krizhevsky et al., 2012; Mikolov et al., 2013; Karpathy et al., 2014).", "startOffset": 114, "endOffset": 184}, {"referenceID": 6, "context": "Deep neural networks have proven highly successful at various image, speech, language and video recognition tasks (Krizhevsky et al., 2012; Mikolov et al., 2013; Karpathy et al., 2014).", "startOffset": 114, "endOffset": 184}, {"referenceID": 3, "context": "Deep neural networks have proven highly successful at various image, speech, language and video recognition tasks (Krizhevsky et al., 2012; Mikolov et al., 2013; Karpathy et al., 2014).", "startOffset": 114, "endOffset": 184}, {"referenceID": 9, "context": "Duplicate video content identification (Shang et al., 2010; Song et al., 2011; Zhao et al., 2007) and video recommendations are also large-scale problems with millions of classes.", "startOffset": 39, "endOffset": 97}, {"referenceID": 10, "context": "Duplicate video content identification (Shang et al., 2010; Song et al., 2011; Zhao et al., 2007) and video recommendations are also large-scale problems with millions of classes.", "startOffset": 39, "endOffset": 97}, {"referenceID": 14, "context": "Duplicate video content identification (Shang et al., 2010; Song et al., 2011; Zhao et al., 2007) and video recommendations are also large-scale problems with millions of classes.", "startOffset": 39, "endOffset": 97}, {"referenceID": 3, "context": ", 2013; Karpathy et al., 2014). These networks typically have several layers of units connected in feedforward fashion between the input and output spaces. Each layer performs a specific function such as convolution, pooling, normalization, or plain matrix products in the case of fully connected layers followed by some form of non-linear activation such as sigmoid or rectified linear units. Despite their attractive qualities, and the relative efficiency of their local architecture, these networks are still prohibitively expensive to train and apply for large-scale problems containing millions of classes or nodes. There are several such problems proposed in the literature. The Imagenet dataset which is one of the largest datasets for image classification contains around 21000 classes. Wordnet, which is a superset of Imagenet, consists of 117, 00 synsets. Freebase, which is a community-curated database of well-known people, places, and things contains close to 20 million entities. Image models of text queries have ranged from 100000 queries in academic benchmarks Weston et al. (2011b) to several million in commercial search engines such as Google, Bing, Yahoo, etc.", "startOffset": 8, "endOffset": 1100}, {"referenceID": 13, "context": "Based on this observation, we exploit a fast locality-sensitive hashing technique (Yagnik et al., 2011) in order to approximate the actual dot product in the final output layer which enables us to scale up the training and inference to millions of output classes.", "startOffset": 82, "endOffset": 103}, {"referenceID": 6, "context": "Furthermore, using the same technique when training the models, we show that our approach can train large-scale models at a faster rate both in terms of number of steps and the total time compared to both the standard softmax layers and the more computationally efficient hierarchical softmax layer of (Mikolov et al., 2013).", "startOffset": 302, "endOffset": 324}, {"referenceID": 1, "context": "Traditional methods such as logistic regression and softmax (multinomial regression) are known to have poor scaling properties with the number of classes (Dean et al., 2013) as the number of dot products grows with the number of classes C that must be considered.", "startOffset": 154, "endOffset": 173}, {"referenceID": 6, "context": "One method for contending with this is hierarchical softmax whereby a tree is constructed of depth log2C in which the leaves are the individual classes which must be classified (Morin & Bengio, 2005; Mikolov et al., 2013).", "startOffset": 177, "endOffset": 221}, {"referenceID": 2, "context": "Locality sensitive hashing (LSH) (Gionis et al., 1999) provides a third alternative by providing methods to perform approximate nearest neighbor search in sub-linear time for various similarity metrics.", "startOffset": 33, "endOffset": 54}, {"referenceID": 13, "context": "An LSH scheme based on ordinal similarity is proposed in (Yagnik et al., 2011) which is used in (Dean et al.", "startOffset": 57, "endOffset": 78}, {"referenceID": 1, "context": ", 2011) which is used in (Dean et al., 2013) to speed-up filter based object detection.", "startOffset": 25, "endOffset": 44}, {"referenceID": 2, "context": "We note that this is equivalent to the problem of finding the approximate nearest neighbors of a vector based on cosine (dot product) similarity which has a rich literature beginning with the seminal work of (Gionis et al., 1999).", "startOffset": 208, "endOffset": 229}, {"referenceID": 13, "context": "In this work, we employ the subfamily of hash functions, winner-take-all (WTA) hashing introduced in (Yagnik et al., 2011), since it has been successfully applied for the similar task of scaling up filter-based object detection in (Dean et al.", "startOffset": 101, "endOffset": 122}, {"referenceID": 1, "context": ", 2011), since it has been successfully applied for the similar task of scaling up filter-based object detection in (Dean et al., 2013).", "startOffset": 116, "endOffset": 135}, {"referenceID": 13, "context": "Given a vector x or w in R, its WTA hash is defined by permuting its elements using P distinct permutations and recording the index of the maximum value of the first k elements (Yagnik et al., 2011).", "startOffset": 177, "endOffset": 198}, {"referenceID": 13, "context": "Furthermore, each WTA hash function defines an ordinal embedding and it has been shown in (Yagnik et al., 2011) that as P \u2192 d!, the dot product between two WTA hashes tends to the rank correlation between the underlying vectors.", "startOffset": 90, "endOffset": 111}, {"referenceID": 1, "context": "In this work, we employ the scheme used in (Dean et al., 2013) due to its simplicity and limited overhead.", "startOffset": 43, "endOffset": 62}, {"referenceID": 0, "context": "We train the models using downpour SGD, an asynchronous stocastic gradient descent procedure supporting a large number of model replicas, proposed in (Dean et al., 2012).", "startOffset": 150, "endOffset": 169}, {"referenceID": 5, "context": "We split the set into equal partitions of training and testing sets as done in (Le et al., 2012).", "startOffset": 79, "endOffset": 96}, {"referenceID": 1, "context": "We selected values of 16, 1000, 3000 for the k, M , P parameters of the WTA approach for all experiments based on results on a small set of images which agreed with the parameters mentioned in (Dean et al., 2013).", "startOffset": 193, "endOffset": 212}, {"referenceID": 4, "context": "We used the artichecture proposed in (Krizhevsky et al., 2012) (AlexNet) for all experiments replacing only the classification layer with the proposed approach.", "startOffset": 37, "endOffset": 62}, {"referenceID": 6, "context": "Recent work from (Mikolov et al., 2013; Q.V. Le, 2014) has demonstrated that a shallow, simple architecture can be trained efficiently by across language corpora.", "startOffset": 17, "endOffset": 54}, {"referenceID": 6, "context": "However, the WTA softmax produces underlying embedding vector representations that do not perform as well on analogy tasks as highlighted by (Mikolov et al., 2013).", "startOffset": 141, "endOffset": 163}, {"referenceID": 9, "context": "This task is modeled on Youtube\u2019s content ID classification problem which has also been addressed in several recent work under various settings Shang et al. (2010); Song et al.", "startOffset": 144, "endOffset": 164}, {"referenceID": 9, "context": "This task is modeled on Youtube\u2019s content ID classification problem which has also been addressed in several recent work under various settings Shang et al. (2010); Song et al. (2011); Zhao et al.", "startOffset": 144, "endOffset": 184}, {"referenceID": 9, "context": "This task is modeled on Youtube\u2019s content ID classification problem which has also been addressed in several recent work under various settings Shang et al. (2010); Song et al. (2011); Zhao et al. (2007).", "startOffset": 144, "endOffset": 204}, {"referenceID": 3, "context": "We use the Sports 1M action recognition dataset introduced in (Karpathy et al., 2014) for this problem.", "startOffset": 62, "endOffset": 85}], "year": 2015, "abstractText": "Deep neural networks have been extremely successful at various image, speech, video recognition tasks because of their ability to model deep structures within the data. However, they are still prohibitively expensive to train and apply for problems containing millions of classes in the output layer. Based on the observation that the key computation common to most neural network layers is a vector/matrix product, we propose a fast locality-sensitive hashing technique to approximate the actual dot product enabling us to scale up the training and inference to millions of output classes. We evaluate our technique on three diverse large-scale recognition tasks and show that our approach can train large-scale models at a faster rate (in terms of steps/total time) compared to baseline methods.", "creator": "LaTeX with hyperref package"}}}