{"id": "1412.7659", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2014", "title": "Transformation Properties of Learned Visual Representations", "abstract": "when a three - dimensional object moves through a scene, a corresponding change occurs on the image plane and in the visual representation constructed by a learning algorithm. starting with the idea that a good representation is one that transforms linearly under scene motions, we use standard results from group representation group theory to show shown that any such representation is equivalent to a partial combination of particularly simple irreducible representations. we derive a striking harmonic relationship between irreducibility and the statistical dependency structure of the representation. under partial observability, as induced by the typical perspective projection of a scene onto entering the image plane, the motion group does not have a linear action on solving the space of images, so that it becomes necessary to then perform inference over a latent representation that does transform linearly. this idea is demonstrated in a model picture of rotating norb objects that employs practically a latent representation of the non - commutative 3d rotation group so ( 3 ).", "histories": [["v1", "Wed, 24 Dec 2014 13:19:20 GMT  (437kb,D)", "https://arxiv.org/abs/1412.7659v1", null], ["v2", "Tue, 3 Mar 2015 04:46:00 GMT  (439kb,D)", "http://arxiv.org/abs/1412.7659v2", null], ["v3", "Tue, 7 Apr 2015 21:20:04 GMT  (439kb,D)", "http://arxiv.org/abs/1412.7659v3", "T.S. Cohen &amp; M. Welling, Transformation Properties of Learned Visual Representations. In International Conference on Learning Representations (ICLR), 2015"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["taco s cohen", "max welling"], "accepted": true, "id": "1412.7659"}, "pdf": {"name": "1412.7659.pdf", "metadata": {"source": "CRF", "title": "TRANSFORMATION PROPERTIES OF LEARNED VISUAL REPRESENTATIONS", "authors": ["Taco S. Cohen"], "emails": ["m.welling}@uva.nl"], "sections": [{"heading": "1 INTRODUCTION", "text": "Much has been written about invariant representations (e.g. Anselmi et al. (2014)), and invariance to groups such as translations, rotations and projective transformations is indeed very important for object recognition. However, for a general purpose visual representation \u2013 capable not only of supporting recognition tasks but also motion understanding and geometrical reasoning \u2013 invariance is not enough.\nInstead, the transformation properties of a representation are crucially important. If we could understand how a given representation of visual data transforms under various rigid or non-rigid transformations of the latent 3D scene, we would be in a better position to build an integrated system that computes invariant representations as well as motion and relative poses of objects. However, performing a mathematical analysis of the transformation properties of, for example, the hidden layer in a deep neural network under motions of a 3D scene is extremely complicated. A better approach is to directly impose good transformation properties on a representation space, and then learn the mapping between data and representation space such that these transformation properties are realized (Hinton et al., 2011).\nIn this paper we study the transformation properties of distributed representations, using tools from group representation theory (Sugiura, 1990). We relate the transformation properties of a distributed representation to statistical notions such as decorrelation and conditional independence under the assumption of complete observability. Under partial observability (due to occlusion, for example) it becomes necessary to introduce latent variables in order to obtain a representation space with good transformation properties. We propose a number of transformation properties that a good representation should have, and present a simple model that demonstrates the idea by modelling 3D rotations of objects from the NORB dataset (LeCun & Bottou, 2004).\nOur model uses a single latent vector of coefficients to represent a set of images of the same object seen in different (rotated) poses, and uses one latent element of the 3D rotation group SO(3) for each pose. A generative neural network model maps each transformed latent representation to an\nar X\niv :1\n41 2.\n76 59\nv3 [\ncs .L\nG ]\n7 A\npr 2\n01 5\nimage. Unlike previous work on learning group representations (Rao & Ruderman, 1999; Miao & Rao, 2007; Sohl-Dickstein et al., 2010; Wang et al., 2011; Bruna et al., 2013; Cohen & Welling, 2014), our model does not assume a linear action of the group in the input space, but instead acts linearly on a latent representation of the 3D scene. Furthermore, our model is the first learned Lie group model that can properly deal with non-commutative transformations.\nThe rest of the paper is organized as follows. In the next section, we introduce the concept of a group representation which is at the core of our analysis. Section three contains the main theoretical results on the dependency structure of irreducible representations. It is followed by a discussion of the problems that arise when partial observability is taken into account in section 4. Section 5 presents a model and training algorithm for learning latent group representations, followed by experiments, related work and a conclusion."}, {"heading": "2 SYMMETRIES AND REPRESENTATIONS", "text": "We start from the basic assumption that our learning agent is situated in space, and this space contains a scene. Formally, we represent the scene as a function x : R3 \u2192 RK that at each point p in space gives a list of numbers x(p) describing, for example, the color, transparency value, material properties, etc. at p. In this section and the next, we further assume full observability, i.e. that x is known entirely. We think of x as a vector in a Hilbert space S of sufficiently well-behaved functions. As we will see, the following analysis does not depend on this particular data representation, but it provides useful intuition and is ultimately realistic.\nWe say that the vector x is a representation of the scene, because the numerical values that one would store in a computer to describe or approximate x depend on both \u201cwhat is in the scene\u201d and \u201chow it is represented in our preferred frame of reference\u201d. If we transform our reference frame by g, an element of the special Euclidean group SE(3) of rigid body motions, the points in space transform as g\u22121p. Such a transformation leaves invariant Euclidean distances, angles and areas, and is therefore called a symmetry of Euclidean space. Under this symmetry, the scene transforms as x\u2032(p) = x(g\u22121p) \u2261 [T (g)x](p), (1) Notice that T (g)[\u03b1x + \u03b2y](p) = \u03b1[T (g)x](p) + \u03b2[T (g)y](p), so T (g) is a linear operator. We say that T is a representation of SE(3) in the Hilbert space S. Generically, a group representation is a map T : G\u2192 GL(V ) from a groupG to the set of invertible linear transformations GL(V ) on a vector space V , that preserves the group structure in the following sense:\nT (g)T (h) = T (gh), (2)\nfor all g, h \u2208 G. One can check that the map T defined in eq. 1 is indeed a group representation. The requirement that (T, V ) forms a representation of SE(3) is a sufficient condition for the vectors in V to describe \u201ca thing in space\u201d, because it requires them to transform as space does (Kanatani, 1990). This is true in particular for the Hilbert space construction given above, but applies more generally to any learned or hand-designed vector space representation. Should eq. 2 fail to hold, key aspects of what it means to transform as Euclidean space are lost: for example, two 180\u25e6 rotations about the same axis might not equal the identity transformation, or two translations might fail to commute.\nHence, we want our representation (in the representation learning sense) to be a linear representation of the special Euclidean group (in the group representation theory sense). From a modelling perspective, one would like to understand all the possible ways in which this can be achieved. To this end, observe that if we have a representation T (g) and an invertible matrix F , then T \u2032(g) = FT (g)F\u22121 is also a representation, which is said to be equivalent to T . A key result in group representation theory tells us that every unitary representation is equivalent in this sense to a simple composition of basic building blocks called irreducible representations.\nA representation (T, V ) is called irreducible if there is no nontrivial subspace W \u2282 V that is mapped onto itself by all operators T (g) for g \u2208 G. It can be shown (Sugiura, 1990) that unitary representations are fully reducible, which means that the representation T (g) is equivalent to a block-diagonal representation T\u0302 (g) whose blocks are irreducible. Such a block-diagonal representa-\ntion T\u0302 = FT (g)F\u22121 is said to be fully reduced. Each block in T\u0302 (g) is identified by an index which we denote by l, so that we can write T\u0302 l(g) for a block of index l in T\u0302 (g) and xl for the component of x \u2208 V in the corresponding subspace. Irreducible representations are important for representation learning in a number of ways. Firstly, using irreducible representations is computationally more efficient than using reducible ones. Irreducibility can also be used to define precisely what a \u201cdisentangled\u201d representation is (Cohen & Welling (2014); see also Bengio et al. (2013)), and as shown in the next section, such a representation will have a simple dependency structure when certain conditions are met. Finally, it is easier to compute a set of generators for the ring of polynomial invariants in an irreducible representation, which can be used to build invariant representations. Kazhdan et al. (2003) use a subset of generators (the power spectrum) to build invariant (but lossy) shape descriptors."}, {"heading": "3 IRREDUCIBILITY, INDEPENDENCE AND DECORRELATION", "text": "Representation learning is often seen as a form of generative modelling, where the goal is to learn a latent variable model with a simple dependency structure in the latent space. The simplest examples are PCA and ICA, where the goal is to learn a linear model whose latent variables are all independent (with Gaussian or non-Gaussian marginal distributions, respectively).\nAlternatively, one can put the the transformation properties center stage and learn a representation that transforms irreducibly under symmetry transformations (Cohen & Welling, 2014). In this perspective, the irreducible representations (and not the independent factors) are the elementary parts from which observation vectors are constructed. Given these contrasting conceptualizations of representation learning, it is interesting to investigate how the transformation properties of a representation are related its statistical properties. In this section we show that under certain conditions, irreducible representations are decorrelated or even conditionally independent."}, {"heading": "3.1 IRREDUCIBILITY AND DECORRELATION: AN ELEMENTARY EXAMPLE", "text": "In order to gain some intuition, we introduce a simple toy model of a completely observable system with symmetry. The states of the system are sufficiently well-behaved functions x : S \u2192 R on the circle. Observations are generated by sampling a uniformly distributed rotation angle \u03b8 \u2208 [0, 2\u03c0) and using it to rotate a template \u03c4 . That is, we have observations x(\u03d5) = [T (\u03b8)\u03c4 ](\u03d5) = \u03c4(\u03d5\u2212\u03b8) for \u03b8 \u223c U [0, 2\u03c0). In practice, we will observe discretized functions with a finite number of coefficients xn = x(\u03d5n).\nIn this case, the linear transformation F that achieves the reduction into irreducible representations is the standard Fourier transform, and indeed it will decorrelate the data (Bruna et al., 2013). To see this, let x\u0302 = Fx, and observe that\nx\u0302 = FT (\u03b8)\u03c4 = FT (\u03b8)F\u22121\u03c4\u0302 \u2261 T\u0302 (\u03b8)\u03c4\u0302 . (3)\nThus T\u0302 = FT (\u03b8)F\u22121 is the representation of the rotation group in the spectral domain.\nWe know from linear algebra that a set of commuting diagonalizable matrices can be simultaneously diagonalized (see Memisevic (2012) and Henriques et al. (2014) for a discussion). Hence, the fully reduced representation T\u0302 is diagonal (not just block-diagonal), and the irreducible representations are one-dimensional. The diagonal elements are complex exponentials Tll(\u03b8) = exp (il\u03b8). It follows immediately that the covariance matrix of the Fourier-transformed data is diagonal:\nEp(\u03b8)[x\u0302l x\u0302\u2217l\u2032 ] = \u222b 2\u03c0 0 eil\u03b8 \u03c4\u0302l e \u2212il\u2032\u03b8 \u03c4\u0302\u2217l\u2032 d\u03b8 2\u03c0 = \u03b4ll\u2032 |\u03c4\u0302l|2, (4)\nwhere \u03b4ll\u2032 equals 1 if l = l\u2032 and 0 otherwise."}, {"heading": "3.2 IRREDUCIBILITY AND DECORRELATION: GENERAL CASE", "text": "The following theorem gives a generalization of this result to the case of compact but not necessarily commutative groups.\nTheorem 1. Let G be a compact group, V a real vector space, and T\u0302 a fully reduced unitary representation of G in V . Furthermore, let x\u0302 = T\u0302 (g)\u03c4\u0302 for a fixed template \u03c4\u0302 \u2208 V and g distributed uniformly on G. The covariance matrix of the vectors in V is diagonal:\nEp(g) [ x\u0302lmx\u0302 l\u2032 m\u2032 ] = \u03b4ll\u2032\u03b4mm\u2032 \u2016\u03c4\u0302 l\u20162\ndim T\u0302 l .\nProof. Using orthogonality of the matrix elements of irreducible representations. See appendix.\nThe theorem is easily generalized to more than one template \u03c4 (in which case one should consider the class-conditional covariance), and it is likely that a slightly weaker theorem can be proven for locally compact groups, but we will not do so here. The main concern regarding the applicability of the above result is not the type of groups and spaces it applies to, but the fact that in reality the orbits are not sampled uniformly. For example, in a sample of natural images, a human face is more likely to appear in upright position than upside down.\nWhile the assumption of uniform sampling of orbits will not hold exactly in real datasets, it is nevertheless likely that irreducible and decorrelated representations will be similar to the degree that the data density is invariant to the group under consideration. A statistical objective such as decorrelation makes sense when one is working with iid draws from an underlying distribution of images, but this is a rather impoverished model of visual experience. As such, we think of decorrelation and independence as surrogate objectives for a deeper structural objective such as irreducibility."}, {"heading": "3.3 IRREDUCIBILITY AND CONDITIONAL INDEPENDENCE", "text": "The concept of an irreducible representation can also shed light on time-series models based on transformations (Cohen & Welling, 2014; Michalski et al., 2014). We define\np(xt | xt\u22121, g) = N (xt | T (g)xt\u22121, \u03c32), (5)\nwhere xt and xt\u22121 are observation vectors at times t and t \u2212 1, respectively, and T (g) is a unitary representation of a compact group G. For G, one can construct an exponential family whose sufficient statistics are given by the matrix elements T\u0302 lmn(g) of irreducible unitary representations of G. As shown in (Cohen & Welling, 2014) for the case of compact commutative groups, the invariance of the l2-norm in the exponent of the Gaussian to unitary transformations results in a posterior p(g|xt, xt\u22121) that is in the same exponential family as the prior p(g) (conjugacy). Furthermore, the marginal p(xt | xt\u22121) will factorize according the irreducible representations: p(x\u0302t | x\u0302t\u22121) = \u220f l p l(x\u0302lt | x\u0302lt\u22121), so in this model an irreducible representation gives us conditional independence."}, {"heading": "4 PARTIAL OBSERVABILITY", "text": "In reality, we do not observe the complete scene x \u2208 S but only a projected image I \u2208 I, which we model as a function I : R2 \u2192 R3 (for 3 color channels). Naively, one could try to construct a representation T\u0304 : SE(3)\u00d7 I \u2192 I such that the perspective projection \u03c0 : S \u2192 I is an equivariant map: T\u0304 (g)\u25e6\u03c0 = \u03c0 \u25e6T (g), but this is not possible. The reason is that a 3D motion can bring entirely new structures into the image.\nIn classical computer vision, the solution is sought in strong assumptions on the scene geometry, such as the assumption that the scene is planar, in which case one obtains a representation of the projective group on the image plane. This assumption leads to neat formulas but real scenes are not flat. A better approach to the problem of partial observability is to model all variability that is not caused by the linear action of a low-dimensional Lie group as being caused by the action of the infinite-dimensional group of diffeomorphisms (Bruna et al., 2013; Soatto, 2012).\nThe scattering representations of Bruna & Mallat (2013) achieve simultaneous insensitivity to translations and diffeomorphisms, and this method achieves very good performance on texture recognition and 2D pattern recognition (e.g. MNIST). However, diffeomorphisms are not an entirely satisfactory model of projected 3D motions either, because they are invertible by definition while\nprojected motions are not. Furthermore, arbitrarily small scene motions can bring arbitrarily bright structures into the image, so scattering representations are not Lipschitz continuous to scene motions.\nWhat we can do instead (at least in principle) is to learn a prior over scenes and a generative model of images given scenes, and then perform inference over scenes given images. By requiring that the latent scene transforms as a representation of a symmetry group, we bias the model towards representing latent properties of the scene as opposed to properties of the image (Kanatani, 1990; Soatto, 2012). By further requiring that the latent scene transforms irreducibly, we may also obtain a simple dependency structure in the latent space (by theorem 1)."}, {"heading": "5 A LATENT GROUP REPRESENTATION", "text": "In this section we define a simple model that demonstrates the idea of a latent group representation concretely. Let Xn = [xn,1, . . . , xn,V ] be a matrix of V views of the same object instance n. We model such a set of views using a single latent vector zn and one latent transformation gn,v \u2208 SO(3) per view, which we collect in a matrix Gn = [gn,1, . . . , gn,V ]. In order to generate xn,v , we first compute zn,v = T\u0302 (gn,v)zn (this computation is explained in section 5.2) and then pass this transformed latent scene to a neural network. The conditional p(xn,v | zn, gn,v) is given by a normal distribution, centered on the output of a generative neural network f\u03b8 : RDz \u2192 RDx that maps z to x-space:\np(xn,v | zn, gn,v) = N (xn,v | f\u03b8(T\u0302 (gn,v) zn), \u03c32x) (6)\nWe use a standard normal prior on z, and a uniform distribution over SO(3) for g. The complete graphical model is shown in figure 1. For regularization, we use a zero-mean Gaussian prior on the neural network weights and on ln\u03c3x (with precision \u03b2 and \u03b1, respectively). The complete log joint probability for a single instance is then given by:\nln p(Xn, Gn, zn, \u03b8, \u03c3x) = \u2212 V\u2211 v=1 \u2016xn,v \u2212 f\u03b8(T\u0302 (gn,v)zn)\u20162 2\u03c32x \u2212 V Dx 2 ln\u03c3x\n\u2212\u2016z n\u20162 2 \u2212 \u03b2 \u2016\u03b8\u2016 2 2 \u2212 \u03b1 (ln\u03c3x) 2 2\n(7)\nThe gradients of which are easily computed using backpropagation (in our own implementation, we compute gradients automatically using Theano (Bergstra et al., 2010)).\n5.1 REPRESENTATION THEORY OF SO(3)\nIn order to compute the transformed latent scene zn,v = T\u0302 (gn,v)zn, we must understand the structure of the unitary representation T\u0302 , and find out how to compute it. Since any representation is\nequivalent to a block diagonal one, we take T\u0302 to be block-diagonal:\nT\u0302 (g) = T\u0302 l1(g) . . .\nT\u0302 lN (g)  , (8) where T\u0302 l is the matrix of an irreducible representation of index l.\nThe complete set of irreducible unitary representations of SO(3) (the unitary dual) can be obtained by decomposing what is called the regular representation of SO(3) acting on functions on the sphere S2. In this case, the representation space is the Hilbert spaceH of square-integrable functions on the sphere, and the representation is defined as T (g)x(p) = x(g\u22121p) for g \u2208 SO(3), x \u2208 H, p \u2208 S2. A function x \u2208 H can be decomposed as a sum of so-called real spherical harmonic functions Ylm : S2 \u2192 R (for l \u2265 0, |m| \u2264 l). That is, for any x \u2208 H we can write\nx(p) = \u2211 l\u22650 l\u2211 m=\u2212l clmYlm(p), (9)\nfor some x-dependent coefficients clm (comparable to ordinary Fourier coefficients of a periodic function on the line). The matrix elements of the representation T in this basis are given by\nT\u0302 lmn(g) = \u3008Ylm, T (g)Yln\u3009 = \u222b S2 Ylm(p)[T (g)Yln](p)dp = \u222b S2 Ylm(p)Yln(g \u22121p)dp. (10)\nIt can be shown (Sugiura, 1990) that this representation T\u0302 l(g), which maps coefficients (cl,\u2212l, . . . , cl,l) to coefficients (c\u2032l,\u2212l, . . . , c \u2032 l,l) corresponding to the expansion of the rotated function x\u2032(p) = x(g\u22121p), is irreducible. Furthermore, all irreducible unitary representations of SO(3) are equivalent to some T\u0302 l obtained in this way.\nTo get an intuitive understanding of the transformation properties of the spherical harmonics, consider figure 2a: the basis functions on each row (corresponding to one value for l) can be linearly combined, and any rotation of the resulting function can again be expressed as a linear combination of only those basis functions. That is, each row corresponds to a representation.\nFor the experiments detailed in section 6, we will be interested in the action of SO(3) on 3D objects, which could be represented as functions of some compact region of 3D space. Such a function can be decomposed by using multiple copies of each irreducible representation, as was done in Skibbe et al. (2009) in the context of rotation invariant shape descriptors."}, {"heading": "5.2 COMPUTATION OF THE REPRESENTATION MATRICES", "text": "We now turn to the computation of the transformation x \u2192 T\u0302 (g)x. Due to the block-structure of T\u0302 , this computation breaks up into a large number of relatively small matrix multiplies. The matrix elements T\u0302 lmn of the irreducible representations of SO(3) are known as the Wigner D-functions. The formulae given for these matrix elements by Wigner (1959) involve numerically unstable sums of many elements with large coefficients. Quite surprisingly, given the prominence of these matrices in physical theories and their long history, a relatively recent paper introduced a novel and very fast method for computing the representation matrices in the basis of real spherical harmonics (Pinchon & Hoggan, 2007). The authors of this paper show that in the basis of real spherical harmonics, a rotation specified by ZYZ-Euler angles g = (g1, g2, g3)T can be computed as T\u0302 l(g) = T\u0302 lz(g3)J lT\u0302 lz(g2)J lT\u0302 lz(g1), where J\nl is a precomputed symmetric orthogonal block matrix that exchanges the Y and Z axes, and T lz represents a z-axis rotation, which takes the simple form:\nT\u0302 lz(\u03b1) =  cos(l\u03b1) sin(l\u03b1) cos((l \u2212 1)\u03b1) sin((l \u2212 1)\u03b1) . . . . . . 1 . . . . . .\n\u2212 sin((l \u2212 1)\u03b1) cos((l \u2212 1)\u03b1) \u2212 sin(l\u03b1) cos(l\u03b1)\n (11)\nFigure 2b shows the matrix T\u0302 35 corresponding to weight l = 35 for three values of g.\nNaively implemented, this method has computational complexity O(l3) in dimension 2l + 1, due to the matrix multiplications. However, it is possible to apply the matrix T\u0302 l(g) to a vector without explicitly constructing it, using associativity: x\u2032 = T\u0302 (g)x = T\u0302z(g3)(J(T\u0302z(g2)(J(T\u0302z(g1)x)))). The sparse multiplication T\u0302zx takes linear time, while Jx takes quadratic time. In practice, we use many copies of relatively low-dimensional representations, so the values of l are much smaller than the dimensionality of the latent space, and hence the quadratic complexity is not a concern."}, {"heading": "5.3 LEARNING", "text": "We train the model using a stochastic hard EM algorithm, which involves alternating between the following steps:\n1. In hard EM, the E-step consists of partial maximization with respect to zn and gn,v (v = 1, . . . , V ) for a single instance n while keeping the parameters \u03b8, \u03c3x fixed. We initialize the latent variables at the state of the last iteration and perform one step of gradient ascent on ln p(Xn, Gn, zn, \u03b8, \u03c3x).\n2. The M-step consists of a maximization with respect to the parameters \u03b8, \u03c3x while holding latent variables fixed. In our stochastic algorithm, we perform a single gradient step on ln p(Xn, Gn, zn, \u03b8, \u03c3x).\nWe use adagrad for all optimization (Duchi et al., 2011)."}, {"heading": "6 EXPERIMENTS", "text": "We trained the model on the NORB dataset (LeCun & Bottou, 2004). This dataset consists of objects in 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Each category contains 10 instances, of which we used the last 5 for training. Each instance is imaged at 9 camera elevations (30 to 70 degrees from horizontal, in 5 degree increments) and 18 azimuths (0 to 340 degrees in 20 degree increments). Finally, there are 6 lighting conditions for each instance, yielding a total of 5 \u00b7 5 \u00b7 6 \u00b7 9 \u00b7 18 = 24300 images. The data was made zero mean, contrast normalized and then PCA whitened, retaining 95% of the variance. We used a neural network f\u03b8 with one hidden layer containing 550 hidden units. The group representation T\u0302 is determined by a choice of li; i = 1, . . . , L which we chose to be: [0] \u00d7 20 + [1]\u00d7 15 + [2]\u00d7 10 + [3]\u00d7 10 + [4]\u00d7 10 + [5]\u00d7 9 + [6]\u00d7 8 + [7]\u00d7 7 + [8]\u00d7 6 + [9]\u00d7 5, where the number in brackets represents li and the multiplier denotes its multiplicity. The regularization parameters were set to \u03b2 = 0.1, \u03b1 = 0.1.\nIn figure 3, we show that the model is able to generate reasonable images for angles it has never seen before. The model is only trained on images that are off by 20 azimuthal degrees, but the model can produce images off by much smaller angles.\nIn figure 4, we show that the model is able to extrapolate to unseen angles of a known object. That is, we train the model only on the azimuthal angle larger than 40 degrees from the reference figure\n(i.e. rotation 0), but produce a mean figure from the network at angles 0, 20, 40, 60. The model gives reasonable images that retain the object identity for poses in which it has not seen that object before."}, {"heading": "7 RELATED WORK", "text": "Our work is related to the idea of transforming auto-encoders or \u201ccapsules\u201d by Hinton et al. (2011). A transforming auto-encoder consists of many capsules, each of which learns to recognize a visual entity and predict its pose. The pose variables g are thus explicitly represented in the model, and act linearly on other pose variables, as is the case in our model. Unlike our model, a transforming autoencoder represents the scene content as a set of probabilities, each of which indicates the likelihood of the preferred visual entity being present.\nThe binary recognition unit used by a capsule for object z corresponds to an orbit O(z) = {T\u0302 (g)z | g \u2208 G} in our model. Having a single latent space shared by multiple visual entities may aid in generalization, and makes it possible to compute metric relations between different objects. Our approach should also deal better with (approximately) symmetric objects, for which it is not possible to unambiguously estimate pose and motion (what is the pose of a circle?). For the case of translational motion of an edge-like structure, this is known as the aperture problem (Memisevic, 2012). Instead of trying to estimate the motion anyway, our model would represent the edge as a vector whose orbit has reduced dimensionality compared to non-symmetric objects.\nThat said, the fully connected generative network and hard-EM algorithm used in our current model are not suitable for dealing with large images, and so we consider the current model as only a proof of concept. A more scalable linear representation learning system could be based on a group-invariant convolutional network (e.g. Gens & Domingos (2014); Mallat (2012)) that generates a distributed representation that at each point locally describes the scene content, while transforming in a (locally) covariant manner."}, {"heading": "8 CONCLUSION", "text": "As the problem of object recognition in static images is steadily approaching the \u201csolved\u201d status, we should start looking towards the next frontier. One of the central challenges is to move away from the idea that images are i.i.d. draws from an underlying distribution (as is the case only in current day benchmark datasets), and begin to model the dynamics of the visual world. Another challenge is to generalize effectively from few examples, which necessitates the exploitation of symmetries of the data distribution. Both of these problems require us to take a closer look at the transformation properties of learned visual representations.\nIn this paper, we have theoretically studied the consequences of assuming a linear representation of a symmetry group in the observed or latent representation space. We have shown that the entire class of such models can be understood mathematically (they are all direct sums of irreducible representations), and have shown how the theory specializes for the case of the 3D rotation group. Furthermore, we have shown that under uniform sampling of orbits, the geometrical objective of learning a linear, unitary and irreducible representation leads to decorrelated representations, thereby shedding new light on this common learning objective."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by NWO, grant number NAI.14.108."}, {"heading": "9 APPENDIX", "text": ""}, {"heading": "9.1 PROOF OF THEOREM 1", "text": "We have x\u0302 = T\u0302 (g)\u03c4\u0302 , and g distributed uniformly. Let \u00b5(g) denote the normalized Haar measure on G.\nEp(g) [ x\u0302lmx\u0302 l\u2032 m\u2032 ] = \u222b G (\u2211 n T\u0302 lmn(g)\u03c4\u0302 l n ) \u00b7 (\u2211 n\u2032 T\u0302 l \u2032 m\u2032n\u2032(g)\u03c4\u0302 l\u2032 n\u2032 ) d\u00b5(g)\n= \u2211 nn\u2032 \u03c4\u0302 ln\u03c4\u0302 l\u2032 n\u2032 \u222b G T\u0302 lmn(g)T\u0302 l\u2032 m\u2032n\u2032(g)d\u00b5(g)\n= \u2211 nn\u2032 \u03c4\u0302 ln\u03c4\u0302 l\u2032 n\u2032 \u03b4ll\u2032\u03b4mm\u2032\u03b4nn\u2032 dim T\u0302 l\n= \u2211 n \u03c4\u0302 ln\u03c4\u0302 l n \u03b4ll\u2032\u03b4mm\u2032 dim T\u0302 l\n= \u2016\u03c4\u0302 l\u20162\ndim T\u0302 l \u03b4ll\u2032\u03b4mm\u2032\n(12)\nWhere we used the orthogonality of matrix elements of irreducible representations (Sugiura, 1990):\n\u3008T\u0302 lmn, T\u0302 l \u2032 m\u2032n\u2032\u3009 = \u03b4ll\u2032\u03b4mm\u2032\u03b4nn\u2032\ndim T\u0302 l , (13)\nand where dim T\u0302 l denotes the dimension of the representation indexed by l."}], "references": [{"title": "Unsupervised learning of invariant representations with low sample complexity: the magic of sensory cortex or a new framework for machine learning", "author": ["Anselmi", "Fabio", "Leibo", "Joel Z", "Rosasco", "Lorenzo", "Mutch", "Jim", "Tacchetti", "Andrea", "Poggio", "Tomaso"], "venue": "Technical Report 001, MIT Center for Brains, Minds and Machines,", "citeRegEx": "Anselmi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2014}, {"title": "Representation Learning: A Review and New Perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "A CPU and GPU math compiler in Python", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Bengio", "Y. Theano"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Invariant scattering convolution networks", "author": ["Bruna", "Joan", "Mallat", "St\u00e9phane"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "Learning Stable Group Invariant Representations with Convolutional Networks", "author": ["Bruna", "Joan", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "Learning the Irreducible Representations of Commutative Lie Groups", "author": ["T. Cohen", "M. Welling"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Cohen and Welling,? \\Q2014\\E", "shortCiteRegEx": "Cohen and Welling", "year": 2014}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Deep Symmetry Networks", "author": ["Gens", "Robert", "Domingos", "Pedro"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Gens et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gens et al\\.", "year": 2014}, {"title": "Fast Training of Pose Detectors in the Fourier Domain", "author": ["Henriques", "Joao F", "Martins", "Pedro", "Caseiro", "Rui", "Batista", "Jorge"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Henriques et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henriques et al\\.", "year": 2014}, {"title": "Transforming auto-encoders", "author": ["GE Hinton", "A Krizhevsky", "Wang", "SD"], "venue": "ICANN-11: International Conference on Artificial Neural Networks, Helsinki,", "citeRegEx": "Hinton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2011}, {"title": "Group Theoretical Methods in Image Understanding", "author": ["K. Kanatani"], "venue": null, "citeRegEx": "Kanatani,? \\Q1990\\E", "shortCiteRegEx": "Kanatani", "year": 1990}, {"title": "Rotation invariant spherical harmonic representation of 3D shape descriptors", "author": ["Kazhdan", "Michael", "Funkhouser", "Thomas", "Rusinkiewicz", "Szymon"], "venue": "In Eurographics Symposium on Geometry Processing,", "citeRegEx": "Kazhdan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kazhdan et al\\.", "year": 2003}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Y. LeCun", "L. Bottou"], "venue": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "LeCun and Bottou,? \\Q2004\\E", "shortCiteRegEx": "LeCun and Bottou", "year": 2004}, {"title": "Group Invariant Scattering", "author": ["Mallat", "Stephane"], "venue": "Communications in Pure and Applied Mathematics,", "citeRegEx": "Mallat and Stephane.,? \\Q2012\\E", "shortCiteRegEx": "Mallat and Stephane.", "year": 2012}, {"title": "On multi-view feature learning", "author": ["R. Memisevic"], "venue": "International Conference on Machine Learning,", "citeRegEx": "Memisevic,? \\Q2012\\E", "shortCiteRegEx": "Memisevic", "year": 2012}, {"title": "Learning the Lie groups of visual invariance", "author": ["X. Miao", "R.P.N. Rao"], "venue": "Neural computation,", "citeRegEx": "Miao and Rao,? \\Q2007\\E", "shortCiteRegEx": "Miao and Rao", "year": 2007}, {"title": "Modeling Deep Temporal Dependencies with Recurrent Grammar Cells", "author": ["Michalski", "Vincent", "Memisevic", "Roland", "K. Konda"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Michalski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Michalski et al\\.", "year": 2014}, {"title": "Rotation matrices for real spherical harmonics: general rotations of atomic orbitals in space-fixed axes", "author": ["Pinchon", "Didier", "Hoggan", "Philip E"], "venue": "Journal of Physics A: Mathematical and Theoretical,", "citeRegEx": "Pinchon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pinchon et al\\.", "year": 2007}, {"title": "Learning Lie groups for invariant visual perception", "author": ["R.P.N. Rao", "D.L. Ruderman"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Rao and Ruderman,? \\Q1999\\E", "shortCiteRegEx": "Rao and Ruderman", "year": 1999}, {"title": "Fast computation of 3d spherical fourier harmonic descriptors - a complete orthonormal basis for a rotational invariant representation of threedimensional objects", "author": ["Skibbe", "Henrik", "Wang", "Qing", "Reisert", "Marco"], "venue": "In IEEE International Workshop on 3-D Digital Imaging and Modeling", "citeRegEx": "Skibbe et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Skibbe et al\\.", "year": 2009}, {"title": "Steps Toward a Theory of Visual Information: Active Perception, Signal-to-Symbol Conversion and the Interplay Between Sensing and Control", "author": ["Soatto", "Stefano"], "venue": "CoRR, abs/1110.2,", "citeRegEx": "Soatto and Stefano.,? \\Q2012\\E", "shortCiteRegEx": "Soatto and Stefano.", "year": 2012}, {"title": "An unsupervised algorithm for learning lie group transformations", "author": ["J. Sohl-Dickstein", "J.C. Wang", "B.A. Olshausen"], "venue": "arXiv preprint,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2010}, {"title": "Unitary Representations and Harmonic Analysis", "author": ["Sugiura", "Mitsuo"], "venue": null, "citeRegEx": "Sugiura and Mitsuo.,? \\Q1990\\E", "shortCiteRegEx": "Sugiura and Mitsuo.", "year": 1990}, {"title": "Lie Group Transformation Models for Predictive Video Coding", "author": ["CM Wang", "J Shol-Dickstein", "Tosic", "Ivana", "Olshausen", "Bruno A"], "venue": "Data Compression Conference,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Group Theory and Its Application to the Quantum Mechanics of Atomic Spectra, 1959", "author": ["E.P. Wigner"], "venue": "ISSN 00029505", "citeRegEx": "Wigner,? \\Q1959\\E", "shortCiteRegEx": "Wigner", "year": 1959}], "referenceMentions": [{"referenceID": 9, "context": "A better approach is to directly impose good transformation properties on a representation space, and then learn the mapping between data and representation space such that these transformation properties are realized (Hinton et al., 2011).", "startOffset": 218, "endOffset": 239}, {"referenceID": 0, "context": "Anselmi et al. (2014)), and invariance to groups such as translations, rotations and projective transformations is indeed very important for object recognition.", "startOffset": 0, "endOffset": 22}, {"referenceID": 21, "context": "Unlike previous work on learning group representations (Rao & Ruderman, 1999; Miao & Rao, 2007; Sohl-Dickstein et al., 2010; Wang et al., 2011; Bruna et al., 2013; Cohen & Welling, 2014), our model does not assume a linear action of the group in the input space, but instead acts linearly on a latent representation of the 3D scene.", "startOffset": 55, "endOffset": 186}, {"referenceID": 23, "context": "Unlike previous work on learning group representations (Rao & Ruderman, 1999; Miao & Rao, 2007; Sohl-Dickstein et al., 2010; Wang et al., 2011; Bruna et al., 2013; Cohen & Welling, 2014), our model does not assume a linear action of the group in the input space, but instead acts linearly on a latent representation of the 3D scene.", "startOffset": 55, "endOffset": 186}, {"referenceID": 3, "context": "Unlike previous work on learning group representations (Rao & Ruderman, 1999; Miao & Rao, 2007; Sohl-Dickstein et al., 2010; Wang et al., 2011; Bruna et al., 2013; Cohen & Welling, 2014), our model does not assume a linear action of the group in the input space, but instead acts linearly on a latent representation of the 3D scene.", "startOffset": 55, "endOffset": 186}, {"referenceID": 10, "context": "The requirement that (T, V ) forms a representation of SE(3) is a sufficient condition for the vectors in V to describe \u201ca thing in space\u201d, because it requires them to transform as space does (Kanatani, 1990).", "startOffset": 192, "endOffset": 208}, {"referenceID": 1, "context": "Irreducibility can also be used to define precisely what a \u201cdisentangled\u201d representation is (Cohen & Welling (2014); see also Bengio et al. (2013)), and as shown in the next section, such a representation will have a simple dependency structure when certain conditions are met.", "startOffset": 126, "endOffset": 147}, {"referenceID": 1, "context": "Irreducibility can also be used to define precisely what a \u201cdisentangled\u201d representation is (Cohen & Welling (2014); see also Bengio et al. (2013)), and as shown in the next section, such a representation will have a simple dependency structure when certain conditions are met. Finally, it is easier to compute a set of generators for the ring of polynomial invariants in an irreducible representation, which can be used to build invariant representations. Kazhdan et al. (2003) use a subset of generators (the power spectrum) to build invariant (but lossy) shape descriptors.", "startOffset": 126, "endOffset": 479}, {"referenceID": 3, "context": "In this case, the linear transformation F that achieves the reduction into irreducible representations is the standard Fourier transform, and indeed it will decorrelate the data (Bruna et al., 2013).", "startOffset": 178, "endOffset": 198}, {"referenceID": 3, "context": "In this case, the linear transformation F that achieves the reduction into irreducible representations is the standard Fourier transform, and indeed it will decorrelate the data (Bruna et al., 2013). To see this, let x\u0302 = Fx, and observe that x\u0302 = FT (\u03b8)\u03c4 = FT (\u03b8)F\u22121\u03c4\u0302 \u2261 T\u0302 (\u03b8)\u03c4\u0302 . (3) Thus T\u0302 = FT (\u03b8)F\u22121 is the representation of the rotation group in the spectral domain. We know from linear algebra that a set of commuting diagonalizable matrices can be simultaneously diagonalized (see Memisevic (2012) and Henriques et al.", "startOffset": 179, "endOffset": 508}, {"referenceID": 3, "context": "In this case, the linear transformation F that achieves the reduction into irreducible representations is the standard Fourier transform, and indeed it will decorrelate the data (Bruna et al., 2013). To see this, let x\u0302 = Fx, and observe that x\u0302 = FT (\u03b8)\u03c4 = FT (\u03b8)F\u22121\u03c4\u0302 \u2261 T\u0302 (\u03b8)\u03c4\u0302 . (3) Thus T\u0302 = FT (\u03b8)F\u22121 is the representation of the rotation group in the spectral domain. We know from linear algebra that a set of commuting diagonalizable matrices can be simultaneously diagonalized (see Memisevic (2012) and Henriques et al. (2014) for a discussion).", "startOffset": 179, "endOffset": 536}, {"referenceID": 16, "context": "3 IRREDUCIBILITY AND CONDITIONAL INDEPENDENCE The concept of an irreducible representation can also shed light on time-series models based on transformations (Cohen & Welling, 2014; Michalski et al., 2014).", "startOffset": 158, "endOffset": 205}, {"referenceID": 3, "context": "A better approach to the problem of partial observability is to model all variability that is not caused by the linear action of a low-dimensional Lie group as being caused by the action of the infinite-dimensional group of diffeomorphisms (Bruna et al., 2013; Soatto, 2012).", "startOffset": 240, "endOffset": 274}, {"referenceID": 3, "context": "A better approach to the problem of partial observability is to model all variability that is not caused by the linear action of a low-dimensional Lie group as being caused by the action of the infinite-dimensional group of diffeomorphisms (Bruna et al., 2013; Soatto, 2012). The scattering representations of Bruna & Mallat (2013) achieve simultaneous insensitivity to translations and diffeomorphisms, and this method achieves very good performance on texture recognition and 2D pattern recognition (e.", "startOffset": 241, "endOffset": 332}, {"referenceID": 10, "context": "By requiring that the latent scene transforms as a representation of a symmetry group, we bias the model towards representing latent properties of the scene as opposed to properties of the image (Kanatani, 1990; Soatto, 2012).", "startOffset": 195, "endOffset": 225}, {"referenceID": 2, "context": "The gradients of which are easily computed using backpropagation (in our own implementation, we compute gradients automatically using Theano (Bergstra et al., 2010)).", "startOffset": 141, "endOffset": 164}, {"referenceID": 19, "context": "Such a function can be decomposed by using multiple copies of each irreducible representation, as was done in Skibbe et al. (2009) in the context of rotation invariant shape descriptors.", "startOffset": 110, "endOffset": 131}, {"referenceID": 24, "context": "The matrix elements T\u0302 l mn of the irreducible representations of SO(3) are known as the Wigner D-functions. The formulae given for these matrix elements by Wigner (1959) involve numerically unstable sums of many elements with large coefficients.", "startOffset": 89, "endOffset": 171}, {"referenceID": 6, "context": "We use adagrad for all optimization (Duchi et al., 2011).", "startOffset": 36, "endOffset": 56}, {"referenceID": 14, "context": "For the case of translational motion of an edge-like structure, this is known as the aperture problem (Memisevic, 2012).", "startOffset": 102, "endOffset": 119}, {"referenceID": 9, "context": "Our work is related to the idea of transforming auto-encoders or \u201ccapsules\u201d by Hinton et al. (2011). A transforming auto-encoder consists of many capsules, each of which learns to recognize a visual entity and predict its pose.", "startOffset": 79, "endOffset": 100}, {"referenceID": 9, "context": "Our work is related to the idea of transforming auto-encoders or \u201ccapsules\u201d by Hinton et al. (2011). A transforming auto-encoder consists of many capsules, each of which learns to recognize a visual entity and predict its pose. The pose variables g are thus explicitly represented in the model, and act linearly on other pose variables, as is the case in our model. Unlike our model, a transforming autoencoder represents the scene content as a set of probabilities, each of which indicates the likelihood of the preferred visual entity being present. The binary recognition unit used by a capsule for object z corresponds to an orbit O(z) = {T\u0302 (g)z | g \u2208 G} in our model. Having a single latent space shared by multiple visual entities may aid in generalization, and makes it possible to compute metric relations between different objects. Our approach should also deal better with (approximately) symmetric objects, for which it is not possible to unambiguously estimate pose and motion (what is the pose of a circle?). For the case of translational motion of an edge-like structure, this is known as the aperture problem (Memisevic, 2012). Instead of trying to estimate the motion anyway, our model would represent the edge as a vector whose orbit has reduced dimensionality compared to non-symmetric objects. That said, the fully connected generative network and hard-EM algorithm used in our current model are not suitable for dealing with large images, and so we consider the current model as only a proof of concept. A more scalable linear representation learning system could be based on a group-invariant convolutional network (e.g. Gens & Domingos (2014); Mallat (2012)) that generates a distributed representation that at each point locally describes the scene content, while transforming in a (locally) covariant manner.", "startOffset": 79, "endOffset": 1666}, {"referenceID": 9, "context": "Our work is related to the idea of transforming auto-encoders or \u201ccapsules\u201d by Hinton et al. (2011). A transforming auto-encoder consists of many capsules, each of which learns to recognize a visual entity and predict its pose. The pose variables g are thus explicitly represented in the model, and act linearly on other pose variables, as is the case in our model. Unlike our model, a transforming autoencoder represents the scene content as a set of probabilities, each of which indicates the likelihood of the preferred visual entity being present. The binary recognition unit used by a capsule for object z corresponds to an orbit O(z) = {T\u0302 (g)z | g \u2208 G} in our model. Having a single latent space shared by multiple visual entities may aid in generalization, and makes it possible to compute metric relations between different objects. Our approach should also deal better with (approximately) symmetric objects, for which it is not possible to unambiguously estimate pose and motion (what is the pose of a circle?). For the case of translational motion of an edge-like structure, this is known as the aperture problem (Memisevic, 2012). Instead of trying to estimate the motion anyway, our model would represent the edge as a vector whose orbit has reduced dimensionality compared to non-symmetric objects. That said, the fully connected generative network and hard-EM algorithm used in our current model are not suitable for dealing with large images, and so we consider the current model as only a proof of concept. A more scalable linear representation learning system could be based on a group-invariant convolutional network (e.g. Gens & Domingos (2014); Mallat (2012)) that generates a distributed representation that at each point locally describes the scene content, while transforming in a (locally) covariant manner.", "startOffset": 79, "endOffset": 1681}], "year": 2015, "abstractText": "When a three-dimensional object moves relative to an observer, a change occurs on the observer\u2019s image plane and in the visual representation computed by a learned model. Starting with the idea that a good visual representation is one that transforms linearly under scene motions, we show, using the theory of group representations, that any such representation is equivalent to a combination of the elementary irreducible representations. We derive a striking relationship between irreducibility and the statistical dependency structure of the representation, by showing that under restricted conditions, irreducible representations are decorrelated. Under partial observability, as induced by the perspective projection of a scene onto the image plane, the motion group does not have a linear action on the space of images, so that it becomes necessary to perform inference over a latent representation that does transform linearly. This idea is demonstrated in a model of rotating NORB objects that employs a latent representation of the noncommutative 3D rotation group SO(3).", "creator": "LaTeX with hyperref package"}}}