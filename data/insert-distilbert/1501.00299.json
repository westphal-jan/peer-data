{"id": "1501.00299", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jan-2015", "title": "Sequence Modeling using Gated Recurrent Neural Networks", "abstract": "in presenting this paper, we have used recurrent neural networks to capture objects and model useful human motion data and continuously generate motions by prediction of the next immediate data point at each time - step. our rnn is armed now with recently proposed gated recurrent units with which overall has shown promising results in some sequence modeling problems such as machine translation and speech synthesis. we demonstrate that this model is able to capture long - term dependencies in data and generate better realistic motions.", "histories": [["v1", "Thu, 1 Jan 2015 18:37:36 GMT  (6163kb,D)", "http://arxiv.org/abs/1501.00299v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["mohammad pezeshki"], "accepted": false, "id": "1501.00299"}, "pdf": {"name": "1501.00299.pdf", "metadata": {"source": "CRF", "title": "Sequence Modeling using Gated Recurrent Neural Networks", "authors": ["Mohammad Pezeshki"], "emails": ["mohammadpz@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Sequence modeling has been a challenging problem in Machine Learning that requires models which are able to capture temporal dependencies. One of the early models for sequence modeling was Hidden Markov Model (HMM) [1]. HMMs are able to capture data distribution using multinomial latent variables. In this model, each data point at time t is conditioned on the hidden state at time t. And hidden state at time t is conditioned on hidden state at time t\u2212 1. In HMMs both P (xt|st) and P (st|st\u22121) are same for all time-steps. A similar idea of parameter sharing is used in Recurrent Neural Network (RNN) [2]. RNNs are an extention of feedforward neural networks which their weights are shared for every time-step in data. Consequently, we can apply RNNs to sequential input data.\nTheoretically, RNNs are capable of capturing sequences with arbitrary complexity. Unfortunately, as shown by Bengio et al. [3], there are some difficulties during training RNNs on sequences with longterm dependencies. Among lots of solutions for RNNs\u2019 training problems over past few decades, we use Gated Recurrent Units which is recently proposed by Cho et al. [4]. As it shown by Chung et al. [5], Gated Recurrent Unit performs much more better than conventional Tanh units.\nIn the folowing sections, we are going to introduce the model, train it on the MIT motion database [6], and show that it is capable of capturing complexities of human body motions. Then we demonstrate that we are able to generate sequences of motions by predicting the next immediate data point given all previous data points."}, {"heading": "2 Recurrent Neural Network", "text": "Simple Recurrent Neural Network which has been shown to be able to implement a Turing Machine [7] is an extension of feedforward neural networks. The idea in RNNs is that they share parameters for different time-steps. This idea which is called parameter sharing enables RNNs to be used for sequential data.\nRNNs have memory and can memorized input values for some period of time. More formally, if the input sequence is x = {x1, x2, ..., xN} then each hidden state is function of current input and\nar X\niv :1\n50 1.\n00 29\n9v 1\n[ cs\n.N E\n] 1\nJ an\nprevious hidden state.\nht = F\u03b8(ht\u22121, xt)\nwhich F\u03b8 is a linear regression followed by a non-linearity.\nht = H(Whhht\u22121,Wxhxt)\nwhere H is a non-linear function which in Vanilla RNN is conventional Tanh. It can be easily shown using the above equation that each hidden state ht is a function of all previous inputs.\nht = G\u03b8(x1, x2, ..., xt)\nwhereG\u03b8 is a very complicated and non-linear function which summerizes all previous inputs in ht. A trainedG\u03b8 puts more emphasis on some aspects of some of the previous inputs trying to minimize overal cost of the network.\nFinally, in the case of real-valued outputs, output in time-step t can be computed as follows\nyt =Whyht\nNote that bias vectors are omitted to keep the notation simple. A graphical illustration of RNNs is shown in figure 1."}, {"heading": "2.1 Generative Recurrent Neural Network", "text": "We can use a Recurrent Neural Network as a generative model in a way that the output of the network in time-step t \u2212 1 defines a probability distribution over the next input at time-step t. According to chain rule, we can write the joint probability distribution over the input sequence as follows.\nP (x1, x2, ..., xN ) = P (x1)P (x2|x1)...P (xT |x1, ..., XT\u22121)\nNow we can model each of these conditional probability distributions as a function of hidden states.\nP (xt|x1, ..., xt\u22121) = f(ht)\nObviously, since ht is a fixed length vector and {x1, ..., xt\u22121} is a variable length sequence, it can be considered as a lossy compression. During learning process, the network should learn to keep important information (according to the cost function) and throw away useless information. Thus, in practice network just look at some time-steps back until xt\u2212k. The architecture of a Generative Recurrent Neural Network is shown in figure 2.\nUnfortunately, as shown by Bengio et al. [3], there are some optimization issues when we try to train such models with long-term dependency in data. The problem is that when an error occurs, as we back-propagate it through time to update the parameters, the gradient may decay exponentially to zero (Gradient Vanishing) or get exponentially large. For the problem of huge gradients, an ad-hoc\nsolution is to restrict the gradient not to go over a threshold. This technique is known as gradient clipping. But the solution for Gradient Vanishing is not trivial. Over past few decades, several methods were proposed [e.g. 8, 9, 10] to tackle this problem. Although the problem still remains, gating methods have shown promissing results in comparison with Vanilla RNN in different task such as Speech Recognition [11], Machine Translation [12], and Image Caption Generation [13]. One of the models which exploits a gating mechanism is Gated Recurrent Unit [4]."}, {"heading": "2.2 Gated Recurrent Unit", "text": "Gated Recurrent Unit (GRU) is different from simple RNN in a sense that in GRU, each hidden unit has two gates. These gates are called update and reset gates which control the flow of information inside each hidden unit. Each hidden state at time-step t is computed as follows,\nht = (1\u2212 zt) \u25e6 ht\u22121 + zt \u25e6 h\u0303t\nwhere \u25e6 is an element wise product, zt is update gate, and h\u0303t is the candidate activation.\nh\u0303t = tanh(Wxhxt +Whh(rt \u25e6 ht\u22121))\nwhere rt is the reset gate. Both update and reset gates are computed using a sigmoid function:\nzt = \u03c3(Wxzxt +Whzht\u22121)\nrt = \u03c3(Wxrxt +Whrht\u22121)\nwhere W s are weight matrices for both gates. A Gated Recurrent Unit is shown in figure 3."}, {"heading": "3 Experimental results", "text": "In this section we describe motion dataset and results for modeling and generating human motions."}, {"heading": "3.1 Ability to capture long-term dependency", "text": "Before training model on motion data let\u2019s first compare GRU with conventional Tanh. As we discussed in section 2, due to optimization problems, simple RNNs are not able to capture longterm dependency in data (Gradient Vanishing problem). Thus, instead of using Tanh activation function, we use GRU. Here we try to show that GRU performs much more better. The task is to read a sequence of random numbers, memorize them for some periods of time, and then emit a function which is sum over input value. We generated 100 different sequences, each containing 20 rows (time-steps) and 2 columns (attributes of each datapoint). We trained the models such that output at time t (yt) is a function of previous input values.\nyt = xt\u22123[0] + x(t\u2212 5)[1]\nHence, we expect models to memorize 5 time-steps back and learn when to use which dimensions of the previous inputs. For both models, input is a vector of size 2, output is a scaler value, and a single hidden layer has 7 units. We allowed both networks to overfit on training data. It is shown in figure 4 that the model with GRU is able to perform very well while simple Tanh cannot capture."}, {"heading": "3.2 Dataset", "text": "Among some Motion Capture (MOCAP) datasets, we used simple walking motion from MIT Motion dataset [6]. The dataset is generated by filming a man wearing a cloth with 17 small lights which determine position of body joints. Each data point in our dataset consists of information about global orientation and displacement. To be able to generate more realistic motions, we used same preprocessing as used by Taylor et al. [14]. Our final dataset contains 375 rows where each row contains 49 ground-invarient, zero mean, and unit variance features of body joints during walking. We also used Neil Lawrences motion capture toolbox to visualize data in 3D spcae. Samples of data are shown in figure 5."}, {"heading": "3.3 Motion generation", "text": "We trained our GRU Recurrent Neural Network which has 49 input units and 120 hidden units in a single hidden layer. Then, we use it in a generative fashion which each output at time t is fed to the model as xt+1. To initialize the model, we first feed the model with 50 frames of the training data and then let the model to generate arbitrary length sequence. Regeneration quality is good enough in a way that it cannot be distinguished from real trining data by the naked eye. In figure 6 average over all 49 features is ploted for better visualization. The initialization and generation phases are shown in figure 7."}, {"heading": "4 Conclusion", "text": "In this paper we have demonstrated that Gated Recurrent Unit helps optimization problems of Recurrent Neural Network when there is long-term dependency in data. We did our experiments discriminatively using a toy example dataset and generatively using MIT motion dataset and showed that GRU performs much better than simple Recurrent Neural Networks with conventional Tanh activation function in both tasks of memorizing and generating."}], "references": [{"title": "An introduction to hidden Markov models.", "author": ["Rabiner", "Lawrence", "Biing-Hwang Juang"], "venue": "ASSP Magazine, IEEE", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1986}, {"title": "Learning Internal Representations by Error Propagation, Parallel Distributed Processing, Explorations in the Microstructure of Cognition, ed", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "DE Rumelhart and J. McClelland", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Learning long-term dependencies with gradient descent is difficult.", "author": ["Bengio", "Yoshua", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on 5.2", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.", "author": ["Chung", "Junyoung"], "venue": "arXiv preprint arXiv:1412.3555", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Style translation for human motion.", "author": ["Hsu", "Eugene", "Kari Pulli", "Jovan Popovi"], "venue": "ACM Transactions on Graphics (TOG)", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Turing computability with neural nets.", "author": ["Siegelmann", "Hava T", "Eduardo D. Sontag"], "venue": "Applied Mathematics Letters", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "Learning recurrent neural networks with hessian-free optimization.", "author": ["Martens", "James", "Ilya Sutskever"], "venue": "Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "On the difficulty of training recurrent neural networks.", "author": ["Pascanu", "Razvan", "Tomas Mikolov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5063", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Advances in optimizing recurrent networks.", "author": ["Bengio", "Yoshua", "Nicolas Boulanger-Lewandowski", "Razvan Pascanu"], "venue": "Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks.", "author": ["Graves", "Alex", "Navdeep Jaitly"], "venue": "Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate.", "author": ["Bahdanau", "Dzmitry", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Show and Tell: A Neural Image Caption Generator.", "author": ["Vinyals", "Oriol"], "venue": "arXiv preprint arXiv:1411.4555", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Modeling human motion using binary latent variables.", "author": ["Taylor", "Graham W", "Geoffrey E. Hinton", "Sam T. Roweis"], "venue": "Advances in neural information processing systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "One of the early models for sequence modeling was Hidden Markov Model (HMM) [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "A similar idea of parameter sharing is used in Recurrent Neural Network (RNN) [2].", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "[3], there are some difficulties during training RNNs on sequences with longterm dependencies.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5], Gated Recurrent Unit performs much more better than conventional Tanh units.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "In the folowing sections, we are going to introduce the model, train it on the MIT motion database [6], and show that it is capable of capturing complexities of human body motions.", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "Simple Recurrent Neural Network which has been shown to be able to implement a Turing Machine [7] is an extension of feedforward neural networks.", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "[3], there are some optimization issues when we try to train such models with long-term dependency in data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Although the problem still remains, gating methods have shown promissing results in comparison with Vanilla RNN in different task such as Speech Recognition [11], Machine Translation [12], and Image Caption Generation [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 11, "context": "Although the problem still remains, gating methods have shown promissing results in comparison with Vanilla RNN in different task such as Speech Recognition [11], Machine Translation [12], and Image Caption Generation [13].", "startOffset": 183, "endOffset": 187}, {"referenceID": 12, "context": "Although the problem still remains, gating methods have shown promissing results in comparison with Vanilla RNN in different task such as Speech Recognition [11], Machine Translation [12], and Image Caption Generation [13].", "startOffset": 218, "endOffset": 222}, {"referenceID": 3, "context": "One of the models which exploits a gating mechanism is Gated Recurrent Unit [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "[5]", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "yt = xt\u22123[0] + x(t\u2212 5)[1]", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "Among some Motion Capture (MOCAP) datasets, we used simple walking motion from MIT Motion dataset [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6]", "startOffset": 0, "endOffset": 3}], "year": 2015, "abstractText": "In this paper, we have used Recurrent Neural Networks to capture and model human motion data and generate motions by prediction of the next immediate data point at each time-step. Our RNN is armed with recently proposed Gated Recurrent Units which has shown promissing results in some sequence modeling problems such as Machine Translation and Speech Synthesis. We demonstrate that this model is able to capture long-term dependencies in data and generate realistic motions.", "creator": "LaTeX with hyperref package"}}}