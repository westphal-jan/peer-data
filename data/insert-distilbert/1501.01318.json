{"id": "1501.01318", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2015", "title": "Arabic Text Categorization Algorithm using Vector Evaluation Method", "abstract": "iraqi text categorization is the process of grouping complex documents into categories based on their contents. this process is important to make information retrieval easier, and currently it became more important due to the huge rich textual information available online. the main possible problem faced in text categorization is how to improve the classification accuracy. probably although arabic text categorization is surely a new promising field, there are a few researches in this field. this paper proposes a new method for arabic text structure categorization using vector evaluation. the proposed method uses a categorized arabic documents corpus, and here then the weights of the tested document's words are progressively calculated accordingly to determine the generic document keywords which will be compared with the keywords of the corpus categorizes to individually determine the tested document's best category.", "histories": [["v1", "Tue, 6 Jan 2015 21:10:26 GMT  (138kb)", "http://arxiv.org/abs/1501.01318v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["ashraf odeh", "aymen abu-errub", "qusai shambour", "nidal turab"], "accepted": false, "id": "1501.01318"}, "pdf": {"name": "1501.01318.pdf", "metadata": {"source": "CRF", "title": "ARABIC TEXT CATEGORIZATION ALGORITHM USING VECTOR EVALUATION METHOD", "authors": ["Ashraf Odeh", "Aymen Abu-Errub", "Qusai Shambour", "Nidal Turab"], "emails": [], "sections": [{"heading": null, "text": "DOI:10.5121/ijcsit.2014.6606 83\ninformation retrieval (IR) field, and the most famous one is tf.idf (term frequency and inverse document frequency) proposed by Jones [27]. Robertson [28] tried to present the theoretical justifications of both idf and tf.idf in IR. TC can play [36] an important role in a wide variety of areas such as information retrieval, word sense disambiguation, topic detection and tracking, web pages classification, as well as any application requiring document organization [10].The text categorization applications: Automatic Indexing [22], Document Organization [22], Document Filtering [22],[21],Word Sense Disambiguation [29]. The goal of TC is to assign each document to one or more predefined categories. This paper is organized as follows; Section 2 tells about the related work in this area, Section 3 discussed our proposed algorithm Section 4, experimental result and Section 5 conclusion. 1.1.Text Categorization Steps Generally, text categorization process includes five main steps: [3], [35]."}, {"heading": "1.1.1. Document Preprocessing", "text": "In this step, html tags, rare words and stop words are removed, and some stemming is needed; this can be done easily in English, but it is more difficult in Arabic, Chinese, Japanese and some other languages. Word's root extraction methods may help in this step in order to normalize the document's words. There are several root extraction methods, including morphological analysis of the words and using N-gram technique [40]."}, {"heading": "1.1.2. Document Representation", "text": "Before classification, documents must be transformed into a format that is recognized by a computer, vector space model (VSM) is the most commonly used method. This model takes the document as a multi-dimension vector, and the feature selected from the dataset as a dimension of this vector."}, {"heading": "1.1.3 Dimension Reduction", "text": "There are tens of thousands of words in a document, so as features it is infeasible to do the classification for all of them; also, the computer cannot process such amount of data. That is why it is important to select the most meaningful and representative features for classification, the most commonly selection methods used includes Chi square statistics [4][38], information gain, mutual information, document frequency, latent semantic analysis."}, {"heading": "1.1.4. Model Training", "text": "This is the most important part of text categorization. It includes choosing some documents from corpus to comprise the training set, performs the learning on the training set, and then generates the model."}, {"heading": "1.1.5. Testing and Evaluation", "text": "This step uses the model generated from the model training step, and performs the classification on the testing set, then chooses appropriate index to do evaluations."}, {"heading": "1.2. Characteristics of Arabic Language", "text": "Arabic accent is spoken 300 times more than English language; Arabic characteristics are assorted in abounding aspects. In [30],[37] Nizar summarized most of the Arabic language characteristics. Arabic script or alphabets: Arabic is accounting from appropriate to left, consists of 28 belletrists and can be continued to 90 by added shapes, marks, and vowels [31]. Arabic script and alphabets differ greatly when compared to other language scripts and alphabets in the following areas: shape, marks, diacritics, Tatweel (Kashida), Style (font), and numerals, and Distinctive letters and none distinctive letters. Arabic Phonology and spelling: 28 Consonants, 3 short vowels, 3 long vowels and 2 diphthongs, Encoding is in CP1256 and Unicode. Morphology: \u2022 Consists from bare root verb form that is trilateral, quadrilateral, or pent literal. \u2022 Derivational Morphology (lexeme = Root + Pattern) \u2022 Inflectional morphology (word = Lexeme + Features); features are \u2022 Noun specific: (conjunction, preposition, article, possession, plural, noun) \u2022 Verb specific: (conjunction, tense, verb, subject, object) \u2022 Others: Single letter conjunctions and single letter prepositions"}, {"heading": "2. RELATED WORKS", "text": "Franca Debole et al. [3] presents a supervised term weighting (STW) method. Its work as replacing idf by the category-based term evaluation function tfidf .They applied it on standard Reuters-21578.They experimented STW by support vector machines (SVM) and three term selection functions ( chi-square, information gain and gain ratio).The results showed a STW technique made a very good results based on gain ratio. Researchers in Man LAN [5] conduct a comparison between different term weighting schemes using two famous data sets (Reuters news corpus by selected up to 10000 documents for training and testing ,20 newsgroup corpus which has 20000 documents for training and testing). And also create new tf:rf for text categorization depended of weighting scheme. They used McNemar's Tests based on micro-average and break-even points performance analysis. Their experimental results showed their tf:rf weighting scheme is good performance rather than other term weighting schemes. Pascal Soucy [6] created a new Text Categorization method named ConfWeight based on statistical estimation of importance word. They used a three well known data sets for testing their method, these are: Reuters-21578, Reuters Corpus Vol 1 and Ohsumed .They compare the outcome results from ConfWeight with KNN and SVN results obtained with tfidf. They recommend using ConfWeight as an alternative of tfidf with significant accuracy improvements. and also it can work very well even if no feature selection exists.\nXiao-Bing Xue and Zhi-Hua Zhou [8] expose the distribution of a word in the document and effect it in TC as frequently of word appears in it. They design distributional features, which compact the appearances of the word and the position of the first appearance of the word. The method depending on combining distributional features with term frequency and related to document writing style and document length. They modeled it by divided documents to several parts then create an array for each word in parts and its number of appearance .They concluded that the distributional features when combined with term frequency are improved the performance of text categorization, especially with long and informal document. Youngjoong KO, JinwooPark [9] try to progress the TC accuracy by create a new weighting method named TF-IDF-CF which make same adaptation on TF-IDF weighting on vector space depended on term frequency and inverse document frequency. Their method considers adding a new parameter to represent the in-class characteristic in TF-IDF weighting. They experiment the new method TF-IDF-CF by using two datasets Reuters-21578 (6088 training documents and 2800 testing documents) and 20newsgroup(8000 training documents , 2000 testing documents).and compared the result of their method with Bayes Network, Na\u00efve Bayes, SVM and KNN, then they prove TF-IDF-CF achieved very good results rather than others methods. Suvidha, R. [12] present a method that based on combined between two text summarization (one used TITLE and other used TERMS) and text categorization within used Term Frequency and Inverse Document Frequency (TFIDF) in machine learning techniques. They find out that the text summarization techniques are very useful in text categorization mainly when they are combined together or with term frequency. Sebastiani, F. [13] explains the main machine learning approaches in text categorization , and focused on using the machine learning in TC research . The Survey introduce of TC definition, history, constraints, needs and application. The survey explains the main approaches of TC that covered by machine learning models within three aspects; document representation, classifier construction and classifier evaluation. The survey described several automatic TC techniques and most important TC applications and introduced text indexing, by a classifier-building algorithm Al-Harbi S. et al. [14] introduces an Arabic text classification techniques methodology by present extracting techniques, design corpora, weighting techniques and stop lists. They try to create model for training datasets for Arabic text classification used the different seven datasets; Saudi Press Agency (SPA) 1,526 documents, (SNP) 4,842 documents, WEB Sites 2,170 documents, Ten writers 821 documents, Discussion Forums 4,107 documents, Islamic Topics 2,243 documents and Arabic Poems 1,949 documents ).and implemented a tool for Arabic Text Classification (ATC Tool). El-Halees A. [15] focus on Arabic text documents classification. He built his classifier called ArabCat to work in corpus that he built real datasets from eight classes (politics, sports, culture, arts, sciences, technology, economy and health).He said his Arabic text documents classification is more performance than all Arabic classification systems. Kourdi et al. [16] used statistical machine learning algorithm Naive Bayes (NB) to classified non-vocalized Arabic web documents. They built own datasets from five categories consist of 300 web documents for each category. Their method results showed that the average accuracy was 62%.\nSawaf H. et al. [17] achieved classification on large Arabic corpus, named as Arabic NEWSWIRE corpus using statistical methods. Their corpus covers several categories as (politics, culture, economy and sports) with documents size 33 KB .They showed even with no morphological analysis statistical methods for document clustering obtain satisfied results and It very robust and reliable and it could be capable for Arabic Text. Syiam et al. [18] present an intelligent Arabic text categorization system which used Machine learning algorithms() for stemming and selection. They used normalized-tfidf schema for Arabic text categorization. They applied the Arabic text categorization on corpus consists of 1,132 documents from three Egyptian newspapers: El Ahram, El Gomhoria and El Akhbar, .they cover 6 categories as Arts 233 documents, Economics 233 documents, Politics 280 documents, Information Technology 102 documents, Woman 121 documents and Sports 231. They suggested hybrid method consist of combining between Document Frequency and Information Gain is the suitable stemming for Arabic text gives average accuracy of 98%. Hmeidi I., et al. [19] describe study of Arabic text categorization using two machine learning methods these K nearest neighbor (KNN) and support vector machines (SVM). They create their own corpus based on a collection of news articles for training and testing. They showed that these methods results are most efficient but SVM better in prediction. Hua Li C., Cheol Park S. [20] present a used of Back propagation neural network (BPNN) for text categorization .they improved BPNN to be for efficient in Text Categorization approaches by solved some defects such as slow convergence. They showed that the improved model of BPNN was achieved high efficient for categorization within experiment it on standard Reuter-21578. Abu-Errub A. [38] introduces a new Arabic text classification algorithm using TFIDF and Chi square measurements. The tested document is compared with main categories documents using TFIDF method to determine the tested document's category, and then the Chi square measurement is used to determine the tested document's subcategory. The researcher examines the proposed algorithm using 1090 training documents categorized into 10 main categories and 50 subcategories, the examination of 1100 different documents shows that the proposed algorithm is capable of classifying the tested document into its appropriated subcategory. Research by Abu-Errub A. et al.[39] introduces a method of extracting the roots of Arabic words. The proposed algorithm consists of stemming the words of the tested document by deleting the prefixes and suffixes of each word. Then the resulted stemmed word is compared with its morphological weights, and finally the extra letters are deleted from the word to produce the root of the original word."}, {"heading": "3. PROPOSED ALGORITHM", "text": "The corpus that is used for checking process consists of 38081 words. After deleting the repeated words 8112 different words are left. Figure 1 represents the suggested Text Categorization steps: Step1: Delete stop word from the documents: the proposed algorithm eliminates stop words from the document using the Arabic stop words list which contains 162 words shown in table1:\nStep2: Normalize the rest of the words: such as removing punctuation, converting words to lowercase, stripping numbers out, as following [38]: \u2022 Remove punctuation. \u2022 Delete numbers, spaces and single letters.  Convert letters ( \u0621 ), ( \u0624 ), ( \u0640\u0626 ), ( \u0623 ), ( \u0625 ) into ( \u0627 ), and ( \u0629 ) into ( \u0647 ).\nStep3: Apply stemming on words: by applying techniques to find basic form the root of words by removing affixes (suffixes and prefixes) attached to its root. Step4: Calculate the weight of each word in the tested document using weighting scheme function that is the product of the term occurrence frequency (tf) and the inverse document frequency (idf). The inverse document frequency of the ith term is commonly defined as log (n/df) where n is the number of documents in the document set, and df is the number of documents in which the term appears, the weight function as shown below Wij = tfij * log (n/dfj) Step5: Choose two keywords that have the largest weight in the tested document after calculating the weights of all words. Step6: Find the most suitable category: by comparing the two keywords chosen in step 5 with key words of each category. Step7: Return the name of category the has high match percentage."}, {"heading": "4. PROPOSED ALGORITHM", "text": "To examine the proposed algorithm, a set of Arabic articles covering different topics were chosen. Researchers create eleven categories corpus containing 982 documents with variant size and content about (Agriculture, Astronomy, Business, Computer, Economics, Environment, History, politics, Sport, Tourism, and Religion) used these documents for learning the proposed algorithm system. As show in table 2\nEnvironment 55\nHistory 74\nPolitics 104\nReligion 99\nSport 76\nTourism 104\nAfter that we tested our algorithm using 1000 testing documents. A sample of the results is shown in table.3 bellow. As shown in the table, document# 1, for example, has 93.7% matching with environment category, so it can be categorized as an environmental document. Also document# 396 was categorized as tourism document by 86.4%, and as business document by 85.3%. Finally, document# 741 has 89% matching with agriculture category 89%, but has 0.0% matching with business category.\nTable.3. Sample of proposed algorithm results"}, {"heading": "5. CONCLUSIONS AND COMPARISON", "text": "Text categorization is the process of grouping similar documents into categories based on their contents, which makes information retrieval process easier. This paper introduces a new Arabic text categorization method using vector evaluation method. The proposed method determines the key words of the tested document by weighting each of its words, and then comparing these key words with the key words of the testing corpus categorizes. As shown in the experimental results, the proposed algorithm prepared the documents to ensure a better selection of the maximum amount of documents after testing the contents of the proposed was 98% in category and in another category was 93%. The proposed method proves the ability to categorize the Arabic text documents into the appropriate categories with a high precision rate."}], "references": [{"title": "Text Document Categorization by Term Association", "author": ["Antonie M.-L", "Za \u0308\u0131ane O. R"], "venue": "IEEE 2002 International Conference on Data Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "A Comparative Study on Different Types of Approaches to Text Categorization", "author": ["Pratiksha Y. Pawar", "S.H. Gawande"], "venue": "International Journal of Machine Learning and Computing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Supervised Term Weighting for Automated Text Categorization\", proceedings of SAC-03", "author": ["Franca Debole"], "venue": "18th ACM Symposium on Applied Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "A Comparative Study on Term Weighting Schemes for Text Categorization", "author": ["L. Man", "S.Y. Sung", "H.B. Low", "C. Tan"], "venue": "IEEE International Joint Conference on Neural Networks", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Beyond Weighting for Text Categorization In The Vector Space Model\", IJCAI'05 Proceedings of the 19th international joint conference on Artificial intelligence", "author": ["Pascal Soucy"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "An Overview of Categorization Techniques", "author": ["B. Mahalakshmi", "K. Duraiswamy"], "venue": "International Journal of Modern Engineering Research (IJMER), Vol.2,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Distributional Features for Text Categorization", "author": ["Xiao-Bing Xue", "Zhi-Hua Zhou"], "venue": "IEEE Trans. Ensembles, Knowledge and Data Eng.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Improving Text Categorization using the Importance of Sentences", "author": ["Youngjoong ko", "Jinwoo Park"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C. Manning", "H. Schultze"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Chi Square Feature Extraction Based Svms Arabic Language Text Categorization System", "author": ["M. Mesleh A"], "venue": "Journal of Computer Science, Vol. 3,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Machine Learning in Automated Text Categorization", "author": ["F. Sebastiani"], "venue": "ACM Computing Surveys,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Automatic Arabic Text Classification", "author": ["S. Al-Harbi", "A. Almuhareb", "A. Al-Thubaity", "S. Khorsheed M", "A. Al-Rajeh"], "venue": "9es Journe\u0301es internationals,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Arabic Text Classification Using Maximum Entropy\", The Islamic University", "author": ["A. El-Halees"], "venue": "Journal (Series of Natural Studies and Engineering),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Automatic Arabic Document Categorization Based on the Na\u00efve Bayes Algorithm", "author": ["M. El-Kourdi", "A. Bensaid", "T. Rachidi"], "venue": "in proceeding of the 20th International Conference on Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Statistical Classification Methods for Arabic News Articles", "author": ["H. Sawaf", "J. Zaplo", "H. Ney"], "venue": "Workshop on Arabic Natural Language Processing ACL'01,2001,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "An Intelligent System for Arabic Text Categorization", "author": ["M. Syiam", "Z.T. Fayed", "M.B. Habib"], "venue": "IJICIS, Vol. 6,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Performance of KNN and SVM Classifiers on Full Word Arabic Articles", "author": ["I. Hmeidi", "B. Hawashin", "E. El-Qawasmeh"], "venue": "Journal of Advanced Engineering Informatics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "High Performance Text Categorization System Based on a Novel Neural Network Algorithm", "author": ["C. Hua Li", "S. Cheol Park"], "venue": "IEEE International Conference on Computer and Information Technology,2006,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Hierarchical Text Classification using Methods from Machine Learning\", M.Sc. Thesis, Institute of Theoretical Computer Science (IGI), Graz University of Technology, Austria (October 2003)", "author": ["M. Granitzer"], "venue": "International Journal of Computer Science & Information Technology (IJCSIT)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Feature Reduction for Neural Network Based Text Categorization", "author": ["L. Savio", "L. Dik"], "venue": "Proceedings of the 6th International Conference on Database Systems for Advanced Applications,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "A Class-Feature-Centroid Classifier for Text Categorization", "author": ["H. Guan", "J. Zhou", "M. Guo"], "venue": "Proceedings of the 18th International Conference on World Wide Web,2009,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Categorization with Support Vector Machines \u2013How to Represent Texts in Input Space", "author": ["E. Leopold", "J. Kindermann", "\"Text"], "venue": "Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Imbalanced Text Classification: A Term Weighting Approach", "author": ["Y. Liu", "H.T. Loh", "A.X. Sun"], "venue": "Expert Systems with Applications,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "A Statistical Interpretation of Term Specificity and its Application in Retrieval", "author": ["K.S. Jones"], "venue": "Journal of Documentation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Understanding Inverse Document Frequency: On Theoretical Arguments for IDF", "author": ["S.E. Robertson"], "venue": "Journal of Documentation,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2004}, {"title": "A Bayesian Learning Approach to Concept-Based Document Classification", "author": ["G. Ifrim"], "venue": "M.Sc. Thesis,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Introduction to Arabic Natural Language Processing\", ACL\u201905 Tutorial University of Michigan", "author": ["H. Nizar"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2005}, {"title": "On Arabic Search: Improving the Retrieval Effectiveness via a Light Stemming Approach", "author": ["Aljlayl M", "O. Frieder"], "venue": "Proceedings of the 11th international conference on Information and knowledge management,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2002}, {"title": "The Penn Arabic Treebank: Building A Large scale Annotated Arabic Corpus", "author": ["M. Maamouri", "A. Bies", "T. Buckwalter", "W. Mekki"], "venue": "In Proceedings of the International Conference on Arabic Language Resources and Tools( NEMLAR), pp", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "Novel Automatic Query Building Algorithm Using Similarity Thesaurus", "author": ["H. Khafajeh", "A. Abu-Errub", "A. Odeh", "N. Yousef"], "venue": "American Journal of Applied Sciences", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Arabic Text Classification Based on Features Reduction", "author": ["F. Al-Zaghoul", "S. Al-Dhaheri"], "venue": "Using Artificial Neural Networks\",", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2013}, {"title": "Hybrid Intelligent Techniques for Text Categorization", "author": ["A. Sadiq", "S. Abdullah"], "venue": "International Journal of Advanced Computer Science and Information Technology (IJACSIT)", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Comparative Analysis Of Arabic Stemming Algorithms", "author": ["M. Otair"], "venue": "International Journal of Managing Information Technology (IJMIT) Vol.5,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Arabic Text Classification Algorithm using TFIDF and Chi Square Measurements", "author": ["A. Abu-Errub"], "venue": "International Journal of Computer Applications", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Arabic Roots Extraction Using Morphological Analysis", "author": ["A. Abu-Errub", "A. Odeh", "Q. Shambour", "O. Al-Haj Hassan"], "venue": "International Journal of Computer Science Issues", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "An Improved Arabic Word\u2019s Roots Extraction Method Using N-Gram Technique", "author": ["N. Yousef", "A. Abu-Errub", "A. Odeh", "H. Khafajeh"], "venue": "Journal of Computer Science,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Text categorization [1],[7] is a necessity due to the very large amount of text documents that humans have to deal with daily.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "Text categorization [1],[7] is a necessity due to the very large amount of text documents that humans have to deal with daily.", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "Text categorization field is an area where information retrieval and machine learning convene, offering solution for real life problems [10].", "startOffset": 136, "endOffset": 140}, {"referenceID": 9, "context": "Text categorization overlaps with many other important research problems in information retrieval (IR), data mining, machine learning, and natural language processing [11].", "startOffset": 167, "endOffset": 171}, {"referenceID": 19, "context": "The success of text categorization is based on finding the most relevant decision words that represent the scope or topic of a category [23].", "startOffset": 136, "endOffset": 140}, {"referenceID": 30, "context": "TC in general depended on [35] two types of term weighting schemes: unsupervised term weighting schemes (UTWS) and supervised term weighting schemes (STWS).", "startOffset": 26, "endOffset": 30}, {"referenceID": 20, "context": "The UTWS are widely used for TC task [24],[25],[26],[13].", "startOffset": 37, "endOffset": 41}, {"referenceID": 21, "context": "The UTWS are widely used for TC task [24],[25],[26],[13].", "startOffset": 42, "endOffset": 46}, {"referenceID": 22, "context": "The UTWS are widely used for TC task [24],[25],[26],[13].", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "The UTWS are widely used for TC task [24],[25],[26],[13].", "startOffset": 52, "endOffset": 56}, {"referenceID": 23, "context": "idf (term frequency and inverse document frequency) proposed by Jones [27].", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "Robertson [28] tried to present the theoretical justifications of both idf and tf.", "startOffset": 10, "endOffset": 14}, {"referenceID": 31, "context": "TC can play [36] an important role in a wide variety of areas such as information retrieval, word sense disambiguation, topic detection and tracking, web pages classification, as well as any application requiring document organization [10].", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "TC can play [36] an important role in a wide variety of areas such as information retrieval, word sense disambiguation, topic detection and tracking, web pages classification, as well as any application requiring document organization [10].", "startOffset": 235, "endOffset": 239}, {"referenceID": 18, "context": "The text categorization applications: Automatic Indexing [22], Document Organization [22], Document Filtering [22],[21],Word Sense Disambiguation [29].", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "The text categorization applications: Automatic Indexing [22], Document Organization [22], Document Filtering [22],[21],Word Sense Disambiguation [29].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "The text categorization applications: Automatic Indexing [22], Document Organization [22], Document Filtering [22],[21],Word Sense Disambiguation [29].", "startOffset": 110, "endOffset": 114}, {"referenceID": 25, "context": "The text categorization applications: Automatic Indexing [22], Document Organization [22], Document Filtering [22],[21],Word Sense Disambiguation [29].", "startOffset": 146, "endOffset": 150}, {"referenceID": 2, "context": "Generally, text categorization process includes five main steps: [3], [35].", "startOffset": 65, "endOffset": 68}, {"referenceID": 30, "context": "Generally, text categorization process includes five main steps: [3], [35].", "startOffset": 70, "endOffset": 74}, {"referenceID": 35, "context": "There are several root extraction methods, including morphological analysis of the words and using N-gram technique [40].", "startOffset": 116, "endOffset": 120}, {"referenceID": 33, "context": "That is why it is important to select the most meaningful and representative features for classification, the most commonly selection methods used includes Chi square statistics [4][38], information gain, mutual information, document frequency, latent semantic analysis.", "startOffset": 181, "endOffset": 185}, {"referenceID": 26, "context": "In [30],[37] Nizar summarized most of the Arabic language characteristics.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "In [30],[37] Nizar summarized most of the Arabic language characteristics.", "startOffset": 8, "endOffset": 12}, {"referenceID": 27, "context": "Arabic script or alphabets: Arabic is accounting from appropriate to left, consists of 28 belletrists and can be continued to 90 by added shapes, marks, and vowels [31].", "startOffset": 164, "endOffset": 168}, {"referenceID": 2, "context": "[3] presents a supervised term weighting (STW) method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Researchers in Man LAN [5] conduct a comparison between different term weighting schemes using two famous data sets (Reuters news corpus by selected up to 10000 documents for training and testing ,20 newsgroup corpus which has 20000 documents for training and testing).", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Pascal Soucy [6] created a new Text Categorization method named ConfWeight based on statistical estimation of importance word.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "86 Xiao-Bing Xue and Zhi-Hua Zhou [8] expose the distribution of a word in the document and effect it in TC as frequently of word appears in it.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "Youngjoong KO, JinwooPark [9] try to progress the TC accuracy by create a new weighting method named TF-IDF-CF which make same adaptation on TF-IDF weighting on vector space depended on term frequency and inverse document frequency.", "startOffset": 26, "endOffset": 29}, {"referenceID": 10, "context": "[13] explains the main machine learning approaches in text categorization , and focused on using the machine learning in TC research .", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] introduces an Arabic text classification techniques methodology by present extracting techniques, design corpora, weighting techniques and stop lists.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] focus on Arabic text documents classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] used statistical machine learning algorithm Naive Bayes (NB) to classified non-vocalized Arabic web documents.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] achieved classification on large Arabic corpus, named as Arabic NEWSWIRE corpus using statistical methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] present an intelligent Arabic text categorization system which used Machine learning algorithms() for stemming and selection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] describe study of Arabic text categorization using two machine learning methods these K nearest neighbor (KNN) and support vector machines (SVM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] present a used of Back propagation neural network (BPNN) for text categorization .", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[38] introduces a new Arabic text classification algorithm using TFIDF and Chi square measurements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[39] introduces a method of extracting the roots of Arabic words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "Step2: Normalize the rest of the words: such as removing punctuation, converting words to lowercase, stripping numbers out, as following [38]: \u2022 Remove punctuation.", "startOffset": 137, "endOffset": 141}], "year": 2014, "abstractText": "Text categorization is the process of grouping documents into categories based on their contents. This process is important to make information retrieval easier, and it became more important due to the huge textual information available online. The main problem in text categorization is how to improve the classification accuracy. Although Arabic text categorization is a new promising field, there are a few researches in this field. This paper proposes a new method for Arabic text categorization using vector evaluation. The proposed method uses a categorized Arabic documents corpus, and then the weights of the tested document's words are calculated to determine the document keywords which will be compared with the keywords of the corpus categorizes to determine the tested document's best category.", "creator": null}}}