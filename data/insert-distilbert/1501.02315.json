{"id": "1501.02315", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jan-2015", "title": "Long-term Causal Effects via Behavioral Game Theory", "abstract": "estimation of causal regulatory effects of interventions in dynamical systems of interacting agents is under - developed. then in this paper, we explore the intricacies of this problem through standard approaches, and demonstrate the functional need for more appropriate methods. working under the neyman - rubin causal model, we proceed to develop a complete causal inference synthesis method and we explicate the stability assuming assumptions that are necessary for valid causal inference. our method consists of a behavioral component that models the evolution of autonomous agent behaviors over time frame and informs on the long - known term distribution simulation of concurrent agent behaviors in the system, and a game - theoretic component that models the observed dynamic distribution of agent actions conditional on some adopted behaviors. this allows the imputation of long - term estimates when of quantities of interest, and thus the estimation of long - term causal effects of interventions. we demonstrate our resulting method on a dataset from behavioral game theory, and discuss open loop problems to seriously stimulate other future research.", "histories": [["v1", "Sat, 10 Jan 2015 07:06:43 GMT  (223kb,D)", "http://arxiv.org/abs/1501.02315v1", null], ["v2", "Mon, 27 Apr 2015 03:10:48 GMT  (222kb,D)", "http://arxiv.org/abs/1501.02315v2", null], ["v3", "Wed, 10 Jun 2015 20:20:48 GMT  (138kb,D)", "http://arxiv.org/abs/1501.02315v3", null], ["v4", "Fri, 16 Oct 2015 17:50:51 GMT  (366kb,D)", "http://arxiv.org/abs/1501.02315v4", null], ["v5", "Tue, 20 Oct 2015 17:58:47 GMT  (390kb,D)", "http://arxiv.org/abs/1501.02315v5", null], ["v6", "Tue, 27 Oct 2015 01:08:18 GMT  (390kb,D)", "http://arxiv.org/abs/1501.02315v6", "Fixed errors with citation style"], ["v7", "Wed, 2 Nov 2016 21:26:47 GMT  (137kb,D)", "http://arxiv.org/abs/1501.02315v7", "30th Conference on Neural Information Processing Systems (NIPS'16)"], ["v8", "Fri, 4 Nov 2016 02:18:51 GMT  (137kb,D)", "http://arxiv.org/abs/1501.02315v8", "30th Conference on Neural Information Processing Systems (NIPS'16)"]], "reviews": [], "SUBJECTS": "stat.ME cs.AI cs.MA", "authors": ["panagiotis toulis", "david c parkes"], "accepted": true, "id": "1501.02315"}, "pdf": {"name": "1501.02315.pdf", "metadata": {"source": "CRF", "title": "Statistical inference of long-term causal effects in multiagent systems under the Neyman-Rubin model", "authors": ["Panos Toulis", "David C. Parkes"], "emails": [], "sections": [{"heading": null, "text": "is under-developed. In this paper, we explore the intricacies of this problem through standard approaches, and demonstrate the need for more appropriate methods. Working under the Neyman-Rubin causal model, we proceed to develop a causal inference method and we explicate the stability assumptions that are necessary for valid causal inference. Our method consists of a temporal component that models the evolution of behaviors that agents adopt over time, and a behavioral component that models the distribution of agent actions conditional on adopted behaviors. This allows the imputation of long-term estimates of quantities of interest, and thus the estimation of long-term causal effects of interventions. We demonstrate our method on a dataset from behavioral game theory, and discuss open problems to stimulate future research."}, {"heading": "1 Introduction", "text": "Multiagent systems, such as online ad auctions, are ubiquitous and make up a significant portion of the total economic activity. Yet, rather paradoxically, there is a limited number of established statistical methods for experimentation and causal evaluation of interventions in such systems. In this work, we consider problems where the experimental units are agents, the intervention is a format of an economic mechanism (or game), and the outcomes are agent actions.1 As argued by R.A. Fisher, evaluation of intervention effects hinges on an unambiguous interpretation of all possible experiment outcomes, and so \u201cit is always needful to forecast all possible results of the experiment\u201d (Fisher, 1935). However, in dynamical multiagent systems, such forecast is complicated by strategic interference and temporal dynamic behavior.\nStrategic interference exists because agents change their actions depending on the economic environment and the agents they are facing. In statistical terms, the potential outcomes of experimental units under different treatment assignments depend on the treatment assignment of other\n1For instance, in estimating the causal effect of reserve price on auction revenue, the units are bidders in the auction, the treatments could be two auction formats with different reserve prices, and the outcomes are agent bids that are aggregated in some way to define the auction revenue.\nar X\niv :1\n50 1.\n02 31\n5v 1\n[ st\nat .M\nE ]\n1 0\nunits. Interference limits the inferential power of an experiment because the observed outcomes do not provide information about outcomes that could be observed under a different treatment assignment. Current methods assume away such a possibility through a stable unit-treatment value assumption (Rubin, 1974) or assume that interference arises from a static network of units and employ network randomization designs, such as cluster randomization (Ugander et al., 2013) or sequential randomization (Toulis and Kao, 2013). However, causal inference under any form of interference is still an open research problem that largely remains unsolved.\nIn addition, agents adopt a temporal dynamic behavior because, over time, strategic interference makes them change their actions in response to observed actions by other agents. This is important for causal inference because, in principle, we are interested in long-term causal effects, i.e., the effect of an intervention after agents have adopted some sort of equilibrium behavior. For instance, raising the reserve price in an auction might increase revenue in the short-run, but as agents adapt their bids or switch to another platform altogether, the long-term effect could be a net decrease. Interestingly, the notion of long-term causal effects is absent from, although not incompatible with, the Neyman-Rubin model.\nNaturally, equilibrium effects have received attention in the econometric literature. For example, Athey et al. (2011) developed a method to compare two formats of U.S. timber auctions. Their approach is to use data from one auction in order to estimate aggregate bidder value distributions that act as primitives in a structural model, and thus use those estimates to predict the equilibrium outcomes on the other auction. Although not causal, their estimates do capture the notion of longterm intervention effects. However, as we explain in more detail in Section 2.1, they assume away considerations of temporal behavior.\nOur goal in this work is to estimate long-term causal effects of interventions that are valid under the Neyman-Rubin model in the setting of economic mechanisms with interacting agents. In Section 2, we introduce a concrete problem instance using data from a real-world behavioral experiment by Rapoport and Boebel (1992). We further provide an overview of related approaches to the problem and demonstrate the need for a valid causal inference method.\nIn Section 3, we begin by introducing two novel components in the Neyman-Rubin model. The first component is a behavioral state-space where each state is a distribution of a finite number of behaviors over the agent population in the mechanism; this behavioral state is allowed to vary temporally, but it is considered to be latent. Although the treatment assignment mechanism is allowed to affect the initial behavioral state, the features of the aforementioned temporal evolution are intrinsic to the mechanism and thus the evolution is stable under the treatment assignment. The second component is the game-theoretic model which defines the distribution of agent actions, given the payoff structure of the mechanism and the underlying behavioral state, and thus is also stable under the treatment assignment. Taken together, the two aforementioned stability assumptions equip the Neyman-Rubin model for definition and estimation of valid long-term causal effects.\nIn Sections 4 and 5, we develop and demonstrate our method by analyzing the dataset by Rapoport and Boebel (1992). In particular, we adopt an instance of the quantal k-level (QLk) behavioral model introduced by Stahl and Wilson (1994). The latent behavioral state is thus defined as a distribution on k distinct behavior types, and the game-theoretic model is defined through the quantal responses of the associated behaviors. By modeling the latent behavioral state evolution as a stochastic process, we are able to impute long-term behavioral states which, through the gametheoretic model, allows imputation of long-term distribution of agent actions. Since causal effects\nare defined as functions of long-term agent actions, our method provides causal estimates of the quantity of interest that are valid in the Neyman-Rubin sense."}, {"heading": "2 Causal inference in multiagent systems: Application and Stan-", "text": "dard methods\nRapoport and Boebel (1992) conducted a behavioral experiment on a zero-sum, two-person game shown in Table 1 in order to test the minimax behavioral hypothesis that was initially suggested by Von Neumann and Morgenstern (1944). The values W,L indicate payments from player A to player B. Rapoport and Boebel (1992) considered two versions of the same game with (W,L) = ($10,\u2212$6) and (W,L) = ($15,\u2212$1). For example, if player A picks action A1 and agent B picks action B3 in the first version, then agent A has to pay $6 to agent B.2 Twenty subjects were randomized to each game, and each player played both as A and as B in a match-up with two different players. Every match-up lasted 2 sessions (periods) of 60 rounds, where each round consisted of a selection of a strategy from each agent and a (possible) payoff. The aggregate data for this experiment are shown in Table 2, which reports the distribution of actions adopted by players within each game and period.\nAlthough this experiment had a different purpose, we can adapt it for our own research goal. In particular, let us assume that the revenue for the gamemaster is a well-defined function of agentactions e.g., there is some fee paid for choosing an action. To be concrete, we will assume that the gamemaster receives $1 dollar when actions A2, A5, B4, B5 are played. Let Rj,t be the revenue of game format j, where j = 0 denotes the game (W,L) = ($10,\u2212$6) and j = 1 denotes the game (W,L) = ($15,\u2212$1), and t \u2208 {1, 2, 3, 4} is the period. Also let \u03c0Aj,t be the frequency of actions (5\u00d71 column vector) of player A in game j at period t, and \u03c0Bj,t likewise. Thus, by our assumption the revenue of the games are Rj,t = r\u2032(\u03c0Aj,t, \u03c0 B j,t) where r = (0, 1, 0, 0, 1, 0, 0, 0, 1, 1).\nWe ask the following question: \u201cWhat is the long-term causal effect on the revenue of the game if we switch from (W,L) = ($10,\u2212$6) to (W,L) = ($15,\u2212$1)?\u201d. To further simplify things we will consider period 4 as long-term, and hold out data on that period; i.e., we wish to estimate the revenue of each game in period 4 had all agents been assigned to that game. Note that this is different than estimating the revenue of the games in period 4 under the current assignment; i.e., the quantity \u03b4 = R1,4\u2212R0,4 = $0.054, which we also assume to be unknown. We return to clarify this important distinction in Section 3.\n2However, actual payments happened only in 3 out of 120 rounds between any pair of players."}, {"heading": "2.1 Standard Methods", "text": "In this section, we review current approaches to our causal question of the revenue effects of the choice of game format. Our goal is not to make an exhaustive presentation but to illustrate the fundamental assumptions underpinning each method, and thus pinpoint their insufficiency. The simplest approach under the Neyman-Rubin model is to leverage the randomization, observe the outcomes at some arbitrary point, say t = 3, and consider the estimate \u03b4\u0302n = R1,3\u2212R0,3 = \u2212$0.051. This estimate, also known as Neymannian estimate, is unbiased for \u03b4 because of complete randomization and it is statistically significant. Clearly, the dynamic strategic evolution of potential outcomes (i.e., action frequencies) is ignored, but in general such an approach is typical in the treatment effects literature. An additional crucial assumption is the stable unit-treatment value assumption (SUTVA) which, in this case, posits that the agent outcomes depend only on the assignment of that agent; such assumptions are implausible under strategic interference.\nA more sophisticated approach under the Neyman-Rubin model is to analyze the action frequencies as time-series with observations at t = 1, 2, 3 and then multiply impute the action frequency at t = 4 through a Bayesian structural model. Such an approach was developed by Brodersen et al. (2014), who wanted to estimate the effects of ad campaigns on website visits. However, such an approach is essentially macroeconometric, and thus does not model explicitly neither the strategic interactions among agents or their dynamic behavior.\nA common econometric approach in evaluating policy changes is the so-called difference-indifferences (DID) estimator (Card and Krueger, 1994; Donald and Lang, 2007; Ostrovsky and Schwarz, 2011). In our case, this is inapplicable because there are no observations before the intervention, but we can still entertain the idea by considering period t = 1 as the pre-intervention period. The DID estimator would be in this case (R1,3\u2212R1,1)\u2212(R0,3\u2212R0,1) = \u2212$0.164. The DID estimator captures a trend in the data by assuming a common linear trend for both treatment arms that is canceled out in subtraction. However, in order to be interpreted as a valid causal estimate, strong stability assumptions are required that essentially assume away strategic interference or complex dynamic play (Abadie, 2005; Angrist and Pischke, 2008).\nAthey et al. (2011) studied the effects of timber auction format (ascending versus sealed bid)\non competition for timber tracts. Their method was to estimate bidder valuations from observed data in one auction and impute counterfactual bid distributions in the other auction, under the assumption of equilibrium play in both auctions.3 This approach makes two critical implicit assumptions. First, the bidder valuation distribution is stable under the treatment assignment, and it thus a primitive that can be used to impute counterfactuals in other treatment assignments. Second, although imputation is performed for potential outcomes in equilibrium which captures the notion of long-term effects, inference is performed under the assumption of equilibrium play in the observed outcomes, and thus temporal dynamic behavior is assumed away.\nFinally, another popular approach to causality is through directed acyclical graphs (DAGs) between the variables of interest (Pearl, 2000). For example, Bottou et al. (2013) study the causal effects of the machine learning algorithm that scores online ads in the Bing search engine on the search engine revenue. Their approach is to create a full DAG of the system including variables such as queries, bids, prices and so on, and make a Causal Markov assumption for the DAG. This allows to predict counterfactuals for the revenue under manipulations of the scoring algorithm, using only observed data generated from the assumed DAG. However, a key assumption of the DAG approach is that the underlying structural equation model is stable under the treatment assignment, and only edges coming from parents of the manipulated variable need to be removed. As pointed out by Dash and Druzdzel (2001), this might be implausible in equilibrium systems. Consider, for example, a system where X \u2192 Y \u2190 Z, and a manipulation that sets the distribution of Y independently of X,Z. Then after manipulation the two edges will need to be removed. However, if in an equilibrium it is required that Y \u2248 XZ, then the two arrows should be reversed after the manipulation. Proper causal inference in equilibrium systems remains an open area with no well-established methodology (Dash, 2005)."}, {"heading": "3 Causal inference in multiagent systems: Estimand and sta-", "text": "bility assumptions\nIn this section we define formally our estimand of interest and explicate the stability assumptions that are necessary for valid causal inference under the Neyman-Rubin model. First, we introduce our notation for the rest of this paper. Let P = {1, 2, . . . , n} be a population of n agents and G0, G1 be two forms of the same game as in (Rapoport and Boebel, 1992). Each game has the same discrete action space A. Consider an assignment mechanism that initially samples a vector Z \u2208 {0, 1}n of n elements, where Zi = 1 indicates that agent i was assigned to game G1 and Zi = 0 indicates that agent i was assigned to game G0; therefore, Z is the initial assignment of agents to games that stays the same throughout the experiment. We assume that the games progress in discrete time-steps t = 1, 2, \u00b7 \u00b7 \u00b7 and let t =\u221e denote the time step that is considered long-term.\nAs the game progresses, an agent i picks action Yit(Z) \u2208 A. Let \u2206p be the p-dimensional simplex, and \u03b1j,t(Z) \u2208 \u2206|A| be the frequency of agent actions {Yit(Z) : Zi = j} in game Gj during time period t over the action space A. Finally let 1,0 be the vector of n ones and zeroes respectively, and let Rjt(Z) = r\u2032\u03b1j,t(Z) be the revenue of game j at period t under initial\n3Note that Athey et al. (2011) consider a different situation than ours. We consider a normal-form game i.e., the payoff structure through Table 1 is known and fixed, whereas bidders in an auction carry private valuations when entering an auction.\nassignment Z, defined as a linear function of action frequencies \u03b1j,t(Z) and fixed action fees r. We define the estimand of long-term causal effects as\n\u03c4 = R1\u221e(1)\u2212R0\u221e(0) = r\u2032 [\u03b11,\u221e(1)\u2212\u03b10,\u221e(0)] . (1)\nThe estimand (1) shows that for valid causal inference, one needs to extrapolate along two dimensions. First, given observed data up to some period t, we need to extrapolate to t = \u221e. Second, for every game Gj we need to extrapolate from the initial assignmentZ to the assignment Z = j1 i.e., to outcomes that would be observed had all agents been assigned to play Gj . The former is the problem of temporal dynamic agent behavior, whereas the latter is the problem of strategic interference."}, {"heading": "3.1 Stability assumptions for valid causal inference", "text": "In typical causal inference methods, extrapolation from a treatment assignment Z to another assignment Z \u2032 is achieved through various stability assumptions. For example, the SUTVA assumption (Rubin, 1974) posits Yit(Z) = Yi(Zi) i.e., the potential outcome for unit i depends only on the treatment assignment of unit itself. The great virtue of this assumption is that an observed outcome Yit(Z) immediately informs us about potential outcomes {Yit(Z \u2032) : Z \u2032i = Zi}; i.e., for a multitude of treatment assignments that could be realized, thus allowing us to estimate more accurately the desired treatment effects.\nIn our problem, strategic interference and dynamic behavior cannot justify such assumption. Because of this, we require stability assumptions that are defined on populations of agents. For that reason, we augment our parameter space by introducing a behavioral space B with a finite number of agent behaviors. We assume that each agent i adopts a behavior Bit(Z) \u2208 B at time period t under an initial assignment Z, and that Bit(Z) completely determines the distribution of its actions at that particular time period t. As before, let \u03b2j,t(Z) \u2208 \u2206|B| be the frequency of agent behaviors {Bit(Z) : Zi = j} in game Gj during time period t over the behavioral space B. In Section 4 we will make this more concrete by formally defining the behavioral space.\nAssumption 3.1 (Stable behaviors). Under initial assignment Z, the distribution of adopted behaviors in game Gj at period t is independent of Z given the distribution of behaviors at the previous time period t\u2212 1; i.e.,\nZ |= \u03b2j,t(Z) | \u03b2j,t\u22121(Z), Gj.\nAssumption 3.2 (Stable actions). Under initial assignment Z, the distribution of adopted actions in game Gj at period t is independent of Z given the distribution of behaviors at time period t i.e.,\nZ |= \u03b1j,t(Z) | \u03b2j,t(Z), Gj.\nIn addition, there exists a |A| \u00d7 |B| matrixHj of coefficients for game Gj for which\nE [\u03b1j,t(Z)|\u03b2j,t(Z)] = Hj \u00b7 \u03b2j,t(Z). (2)\nRemarks. Assumption 3.1 implies that the dynamic evolution of agent behaviors is not affected by the initial treatment assignment Z, but it is an intrinsic property of the game itself. This is similar to the policy invariance assumption (Heckman and Vytlacil, 2005; Heckman et al., 1998), in which given the choice of treatment by the agent, the initial treatment assignment mechanism does not affect the outcomes. Assumption 3.2 implies that the latent behavioral state \u03b2j,t(Z) is sufficient to inform us about the distribution of actions taken by the agents; of course, this distribution also depends on the payoff structure of the game.\nAssumptions 3.1 and 3.2 also reveal the role of randomizing Z in order to get unbiased estimation of the estimand \u03c4 defined in (1). In particular, let E [ \u03b2j,\u221e(Z) \u2223\u2223\u03b2j,0(Z)] = \u00b5(\u03b2j,0(Z)) for some function \u00b5(\u00b7) \u2208 \u2206|B|. Thus, by (2) we obtain E [\u03b1j,\u221e(Z)|\u03b2j,0(Z)] = Hj \u00b7 \u00b5(\u03b2j,0(Z)). The actual expected value of the long-term agent actions \u03b1j,\u221e(Z) is obtained when all agents are assigned to game Gj; i.e., when Z = j1 in which case E [\u03b1j,\u221e(Z)|\u03b2j,0(j1)] = Hj \u00b7 \u00b5(\u03b2j,0(j1)). However, since \u03b2j,t(Z) is a proportion, by randomization it holds\nE [ \u03b2j,0(Z) ] = \u03b2j,0(j1), (3)\nwhere the expectation is taken over the randomization distribution of Z. Thus, the bias in estimating \u03c4 will depend on the difference \u00b5(\u03b2j,0(Z)) \u2212 \u00b5(\u03b2j,0(j1)). Thus, there are two approaches to unbiased estimation. First, one can set up a dynamic behavioral model such that \u00b5(\u00b7) is constant to its argument; this is the case when the long-term distribution \u03b2j,\u221e(Z) does not depend on the initial condition \u03b2j,0(Z) e.g., as in a Markov chain with a stationary distribution. Alternatively, one can set up a model where \u00b5(\u00b7) is linear so that E [ \u00b5(\u03b2j,0(Z)) ] = \u00b5(E [ \u03b2j,0(Z) ] ) = \u00b5(\u03b2j,0(j1)). In general, the role of randomization is often obscured in multiagent settings. For example, if we assumed perfectly rational agents then randomization would not even be necessary, as observed outcomes would be accurately predicted by concepts such as Nash equilibrium. Our framework assigns a more plausible role to randomization by assuming that it is selecting the initial distribution of latent agent behaviors \u03b2j,0(Z) in the game. After this initial assignment, our stability assumptions imply that the evolution of the game is determined unconditionally to the treatment assignment. Since randomization is unbiased for the selection of \u03b2j,0(Z), inference for the counterfactual \u03b2j,0(j1) i.e., when all agents are assigned to Gj , will be (nearly) unbiased. This idea is depicted in Figure 1. Under our stability assumptions it is possible to estimate the long-term distribution of actions \u03b1j,\u221e(Z) because the data generative process is stable under a realized treatment assignment Z. Randomizing Z helps us to obtain an unbiased estimate \u03b2j,0(j1) through \u03b2j,0(Z)."}, {"heading": "4 Concrete Methodology", "text": "Building upon the insights of Section 3, we now develop a concrete methodology to estimate longterm causal effects defined in Equation (1). In Section 5 we will apply our method in the dataset by Rapoport and Boebel (1992).\nFor our behavioral model we adopt the quantal k-response (QLk) that was initially proposed by Stahl and Wilson (1994), and was shown to predict well the observed human behavior in real-world behavioral experiments (Wright and Leyton-Brown, 2010). In QLk, agents possess increasing levels of sophistication. Following this earlier work, we will adopt k = 3, and thus consider a behavioral space with three different behavior types B = {b0, b1, b2}. Let \u03c0b \u2208 \u2206|A| denote the\ndistribution of actions that an agent will play after adopting behavior b. In QLk the distributions depend on an assumption of quantal response, which is defined as follows. Let u \u2208 R|A| denote a vector such that ui is the expected utility of agent taking action ai \u2208 A, and let Pj denote the payoff matrix in game j; therefore, if an agent is facing a strategy profile \u03c0 then it holds u = Pj\u03c0. The quantal best-response with parameter \u03bb determines the distribution of actions that the agent will take, and is given by QBR(u;\u03bb) = logistic(\u03bbu), where, for some vector x we define logistic(x) = (exp(xi)/ \u2211 exp(xi)). The parameter \u03bb \u2265 0 is usually called the precision of the quantal best-response. If \u03bb is very large then the response is closer to the classical Nash best-response, whereas if \u03bb = 0 the agent has no preferences among actions. In QL3 the agents adopt actions as follows:\n\u2022 Agents who adopt b0, termed level-0 agents, have precision \u03bb0 = 0, and thus will randomly pick one action from the action space A. Thus, \u03c0b0 = QBR(u; 0) = (1/|A|)1.\n\u2022 Agents who adopt b0, termed level-1 agents, have precision \u03bb1 and assume that all other agents are of level-0 type. Thus they are facing a vector of utilities u1 = Pj \u00b7 \u03c0b0 and so \u03c0b1 = QBR(u1;\u03bb1).\n\u2022 Agents who adopt b2, termed level-2 agents, have precision \u03bb2 and assume that all other agents are of level-1 with precision \u03bb(1)2. Thus, they estimate that they are facing a strategy profile given by \u03c0b(1)2 = QBR(u1;\u03bb(1)2), where u1 = Pj\u03c0b0 as above. Their expected utility vector is u2 = Pj\u03c0b(1)2 , and thus their quantal best-response is \u03c0b2 = QBR(u2;\u03bb2).\nLet \u03bb = (\u03bb1, \u03bb(1)2, \u03bb2) and \u03a0j(\u03bb) = [\u03c0b0 \u03c0b1 \u03c0b2 ] be the |A| \u00d7 3 matrix with the aforementioned best-response action distributions as columns. Recall that \u03b2j,t(Z) is the distribution of behaviors for game j, at time t under initial assignmentZ. Then, the QL3 implies that the expected agent action frequencies are given by,\nE [\u03b1j,t(Z)|\u03b2j,t(Z)] = \u03a0j(\u03bb) \u00b7 \u03b2j,t(Z), (4)\nwhich satisfies Assumption 3.2. There are several options to model \u03b2j,t(Z) and satisfy Assumption 3.1. In general, it is reasonable to model \u03b2j,t(Z) as a stochastic process, such as a time series or Markov chain. Since \u03b2j,t(Z) is a discrete distribution, relevant statistical methods are available in analysis of continuous proportions, or compositional data (Aitchison, 1986). For example,\nGrunwald et al. (1993) consider a time series of continuous proportions with a Dirichlet likelihood, and develop a Bayesian method for fast updates using a conjugate prior. However, such methods cannot be directly applied in our problem because the likelihood induced by the aforementioned quantal best-response cannot be written efficiently in closed form. For that reason we adopt a simple VAR(1) model; details are given in the following section."}, {"heading": "4.1 Likelihood and posterior inference", "text": "We can now write down the likelihood of observations as in Table 2. Consider an initial assignment Z, and observed action frequencies for time periods t = 1, 2, . . . T denoted as a T \u00d7 |A| matrix \u03b1j,1:T (Z), where the i-th row is \u03b1j,i(Z). Our model parameters are the behavior distribution \u03b2j,1:T (Z), where the i-th row is \u03b2j,i(Z) and\u03c8 are the model parameters for the temporal evolution of \u03b2j,t(Z); furthermore, we have the quantal best-response parameters are \u03bb = (\u03bb1, \u03bb(1)2, \u03bb2). Thus the likelihood of our model is given by\nL(\u03b1j,1:T (Z);\u03c8,\u03bb) =\u222b B ( T\u220f t=1 f(\u03b1j,t(Z)|\u03b2j,t(Z),\u03bb) ) \u00d7 h(B|\u03c8)dB, (5)\nwhereB , \u03b2j,1:T (Z) takes all possible values on the latent behavior space. Given the definition of quantal best-response for some distribution of behaviors \u03b2 and parameters \u03bb, we can obtain the expected action distribution \u03b1\u0302. Thus, the observed action counts N \u00b7 \u03b1 are distributed as a multinomial distribution i.e., N\u03b1 \u223c Multinom(\u03b1\u0302;N), which provides the likelihood term f(\u03b1|\u03b2,\u03bb) in expression (5). To model the latent temporal behavioral state, we use a simple approach by adopting a VAR(1) model. In particular, we transform the proportion into a new variable wj,t , logit(\u03b2j,t(Z)) and assume that\nwj,t = \u03c80wj,t\u22121 + \u00b5+ \u03c81 t, (6)\nwhere \u03c80 \u2208 (0, 1), \u03c81 \u2208 R+, and \u00b5 \u2208 R|B| is a fixed parameter vector and t \u223c N (0, I) is i.i.d. standard normal. Decomposing the density h(B|\u03c8) = \u220fT t=2 p(\u03b2j,t(Z)|\u03b2j,t(Z),\u03c8) and computing the conditional densities through (6) yields the remaining likelihood term."}, {"heading": "5 Application", "text": "We now return to the dataset of Rapoport and Boebel (1992) presented in Table 2. We assume diffuse priors for the parameters\u03c8,\u03bb; in particular, we consider \u03c0(\u03bbi) \u221d Expo; i.e., an exponential random variable with rate around 1/10 for the parameters of the quantal best-response, a diffuse beta for \u03c80 and a flat variance prior for \u03c81 i.e., p(\u03c81) \u221d (1/\u03c821). As exact conditionals are hard to obtain under this model, we employ a Metropolis-Hastings scheme with a proposal distribution that simply disturbs slightly the current model parameters. This is efficient because we know beforehand that the values of our parameters are constrained; for example, we know that \u03c80 \u2208 (0, 1) and that, roughly, \u03bbi \u2208 (0,\u039b) for some \u039b > 0, because the logistic functions of quantal best-response (see Section 4) become flat above, or below, a certain threshold.\nOur ultimate goal is to obtain imputations of the agent actions in t = 4 through the posterior predictive distribution of our model. We run our chain for 1e5 iterations and assume the first half of the samples as burn-in period; traceplots of MCMC estimates of the model parameters are shown in Figure 2. Additionally, a summary of the marginal posterior distribution of the model parameters is shown in Table 3. There are a few interesting observations. First, note that although it was not explicitly specified, the model obtains \u03bb2 > \u03bb1 in general i.e., that level-2 agents have better precision than agents at level-1. Interestingly, in this dataset, level-2 agents play as-if level-1 agents are very precise (see values for \u03bb(1)2 in Table 3). Furthermore, estimates on \u03c80 i.e., the coefficient for the lag-1 regression in our VAR model of agent behaviors is significant around 0.3, indicating a temporal trend in the latent behavioral state. We can verify this trend by inspection of the bottom-right plot in Figure 2. Note that, overall, there is a steady decreasing trend for the proportion of level-0 agents and a parallel steady increasing trend for level-1 agents, across game time periods. This provides evidence of agent learning."}, {"heading": "5.1 Estimation of long-term causal effects", "text": "We now turn our attention to obtaining posterior estimates for the long-term causal effect defined in (1). Using the MCMC strategy outlined above we are able to obtain imputations of the causal effect (1) through the posterior predictive distribution of the model parameters (\u03bb,\u03c8). Figure 3 depicts 1,000 posterior samples. The actual estimate considered to be at t = 4 is equal to $0.054. Our estimates are able to to catch the latent dynamic trend in agent behaviors. Recall also that the\nnaive estimates presented in Section 2.1 were estimating a strongly significant negative effect of introducing game design G1 compared to G0, because they were misguided by strong short-term effects in the opposite direction. Our estimates are in fact biased; the posterior summary for the long-term causal effect is 0.088\u00b1 0.021, thus being about 63% off the actual value. However, this is perfectly reasonable in our application for the following reasons. First, the dataset is limited to 3 time periods which we use to impute action distributions in the fourth period; therefore, the role of the prior becomes important for identifiability and this necessarily induces bias. Second, the choice of the ground-truth value of the estimand at t = 4 is rather arbitrary, and mostly serves to illustrate the methodology of this work; for instance, it is possible that the bias would be significantly reduced had we observed more time periods for each game. Last, as pointed out in the discussion of our stability assumptions in Section 3, the form of the regression function E [ \u03b2j,\u221e(Z)\n\u2223\u2223\u03b2j,0(Z)] necessarily introduces bias; in our model, this regression function is linear so theoretically we should have obtained an unbiased result. However, the regression function of the actual dynamic process may very well be nonlinear, which would bias our results."}, {"heading": "6 Discussion", "text": "In this paper, we explored the problem of estimating long-term causal effects of interventions in multiagent systems under the Neyman-Rubin causal model. Two features make this problem conceptually and technically challenging. First, strategic interference among competing agents\nlimits the inferential abilities of randomized experiments. Second, dynamic temporal behavior by agents introduces short-term effects, whereas one is typically interested in long-term effects; i.e., when some sort of equilibrium play has been reached. Our first conceptual contribution is to explicate a set of sufficient stability assumptions that lead to valid causal inference within the Neyman-Rubin model. We make two stability assumptions, one for a latent behavioral state of the population and one for the distribution of actions adopted by the agents, conditional on the aggregate behavior. Under certain conditions, randomization yields an unbiased initial behavioral state after the intervention, which can lead to unbiased estimation of long-term actions under the aforementioned stability assumptions.\nThe technical challenge in this problem is to correctly model the evolution of the latent dynamic behaviors, and the observed action profiles conditional on realized behaviors. Our approach is modular. Our first model consisted of a simple VAR model of the population behavioral state. Our second model consisted of a quantal k-response which predicts the action distribution adopted by the agents, conditional on a distribution of behaviors in the population. Working on a real-world dataset from a behavioral experiment (Rapoport and Boebel, 1992), we showed how our method can be applied for estimation of a long-term effect of intervention in the payoff structure of a game design in normal-form.\nThere are several issues that are left open. One important issue is the strategic interference between games (or mechanisms). For instance, in our application it was reasonable two assume that actions in one game did not affect actions in the other one. However, in most interesting situations e.g., in a real-interconnected market, it is hard to achieve a setting with no interference (e.g., in an auction players out-bid each other, or switch to other platforms and so on). A second issue, more technical in nature, is how to scale up the inference in games or markets with many participants and transactions; for example, there are millions to billions of second-price auctions happening online on a daily basis, and inference through our model in such settings is a challenge. A third, more theoretical concern, is whether it is possible to establish certain optimality properties for our approach. For instance, it is important to know what is the interconnections between the choice of the latent behavioral model, the randomization and statistical inference. Progress in answering such questions will lead to new and fruitful interactions of game theory with experimental design and causal inference."}], "references": [{"title": "Semiparametric difference-in-differences estimators", "author": ["A. Abadie"], "venue": "The Review of Economic Studies,", "citeRegEx": "Abadie,? \\Q2005\\E", "shortCiteRegEx": "Abadie", "year": 2005}, {"title": "The statistical analysis of compositional data", "author": ["J. Aitchison"], "venue": null, "citeRegEx": "Aitchison,? \\Q1986\\E", "shortCiteRegEx": "Aitchison", "year": 1986}, {"title": "Mostly harmless econometrics: An empiricist\u2019s companion", "author": ["J.D. Angrist", "Pischke", "J.-S"], "venue": null, "citeRegEx": "Angrist et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Angrist et al\\.", "year": 2008}, {"title": "Comparing open and sealed bid auctions: Evidence from timber auctions", "author": ["S. Athey", "J. Levin", "E. Seira"], "venue": "The Quarterly Journal of Economics,", "citeRegEx": "Athey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Athey et al\\.", "year": 2011}, {"title": "Couterfactual reasoning and learning systems", "author": ["D. Ray", "P. Simard", "E. Snelson"], "venue": "J. Machine Learning Research,", "citeRegEx": "Ray et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ray et al\\.", "year": 2013}, {"title": "Inferring causal impact using bayesian structural time-series models", "author": ["K.H. Brodersen", "F. Gallusser", "J. Koehler", "N. Remy", "S.L. Scott"], "venue": "Annals of Applied Statistics", "citeRegEx": "Brodersen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brodersen et al\\.", "year": 2014}, {"title": "Minimum wages and employment: A case study of the fast food industry in New Jersey and Pennsylvania", "author": ["D. Card", "A.B. Krueger"], "venue": "American Economic Review,", "citeRegEx": "Card and Krueger,? \\Q1994\\E", "shortCiteRegEx": "Card and Krueger", "year": 1994}, {"title": "Restructuring dynamic causal systems in equilibrium", "author": ["D. Dash"], "venue": "In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics (AIStats", "citeRegEx": "Dash,? \\Q2005\\E", "shortCiteRegEx": "Dash", "year": 2005}, {"title": "Caveats for causal reasoning with equilibrium models. In Symbolic and Quantitative Approaches to Reasoning with Uncertainty", "author": ["D. Dash", "M. Druzdzel"], "venue": null, "citeRegEx": "Dash and Druzdzel,? \\Q2001\\E", "shortCiteRegEx": "Dash and Druzdzel", "year": 2001}, {"title": "Inference with difference-in-differences and other panel data", "author": ["S.G. Donald", "K. Lang"], "venue": "The review of Economics and Statistics,", "citeRegEx": "Donald and Lang,? \\Q2007\\E", "shortCiteRegEx": "Donald and Lang", "year": 2007}, {"title": "The design of experiments", "author": ["R.A. Fisher"], "venue": null, "citeRegEx": "Fisher,? \\Q1935\\E", "shortCiteRegEx": "Fisher", "year": 1935}, {"title": "Time series of continuous proportions", "author": ["G.K. Grunwald", "A.E. Raftery", "P. Guttorp"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Grunwald et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Grunwald et al\\.", "year": 1993}, {"title": "Structural equations, treatment effects, and econometric policy evaluation1", "author": ["J.J. Heckman", "E. Vytlacil"], "venue": null, "citeRegEx": "Heckman and Vytlacil,? \\Q2005\\E", "shortCiteRegEx": "Heckman and Vytlacil", "year": 2005}, {"title": "General equilibrium treatment effects: A study of tuition policy", "author": ["J.J. Heckman", "L. Lochner", "C. Taber"], "venue": "American Economic Review,", "citeRegEx": "Heckman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Heckman et al\\.", "year": 1998}, {"title": "Reserve prices in internet advertising auctions: A field experiment", "author": ["M. Ostrovsky", "M. Schwarz"], "venue": "In Proceedings of the 12th ACM conference on Electronic commerce,", "citeRegEx": "Ostrovsky and Schwarz,? \\Q2011\\E", "shortCiteRegEx": "Ostrovsky and Schwarz", "year": 2011}, {"title": "Causality: models, reasoning and inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q2000\\E", "shortCiteRegEx": "Pearl", "year": 2000}, {"title": "Mixed strategies in strictly competitive games: A further test of the minimax hypothesis", "author": ["A. Rapoport", "R.B. Boebel"], "venue": "Games and Economic Behavior,", "citeRegEx": "Rapoport and Boebel,? \\Q1992\\E", "shortCiteRegEx": "Rapoport and Boebel", "year": 1992}, {"title": "Estimating causal effects of treatments in randomized and nonrandomized studies", "author": ["D.B. Rubin"], "venue": "Journal of Educational Psychology,", "citeRegEx": "Rubin,? \\Q1974\\E", "shortCiteRegEx": "Rubin", "year": 1974}, {"title": "Experimental evidence on players\u2019 models of other players", "author": ["D.O. Stahl", "P.W. Wilson"], "venue": "Journal of Economic Behavior & Organization,", "citeRegEx": "Stahl and Wilson,? \\Q1994\\E", "shortCiteRegEx": "Stahl and Wilson", "year": 1994}, {"title": "Estimation of causal peer influence effects", "author": ["P. Toulis", "E. Kao"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Toulis and Kao,? \\Q2013\\E", "shortCiteRegEx": "Toulis and Kao", "year": 2013}, {"title": "Graph cluster randomization: Network exposure to multiple universes", "author": ["J. Ugander", "B. Karrer", "L. Backstrom", "J. Kleinberg"], "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Ugander et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ugander et al\\.", "year": 2013}, {"title": "Theory of games and economic behavior", "author": ["J. Von Neumann", "O. Morgenstern"], "venue": null, "citeRegEx": "Neumann and Morgenstern,? \\Q1944\\E", "shortCiteRegEx": "Neumann and Morgenstern", "year": 1944}, {"title": "Beyond equilibrium: Predicting human behavior in normal-form games", "author": ["J.R. Wright", "K. Leyton-Brown"], "venue": "In Proc. 24th AAAI Conf. on Artificial Intelligence. PANOS TOULIS,", "citeRegEx": "Wright and Leyton.Brown,? \\Q2010\\E", "shortCiteRegEx": "Wright and Leyton.Brown", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "Fisher, evaluation of intervention effects hinges on an unambiguous interpretation of all possible experiment outcomes, and so \u201cit is always needful to forecast all possible results of the experiment\u201d (Fisher, 1935).", "startOffset": 201, "endOffset": 215}, {"referenceID": 17, "context": "Current methods assume away such a possibility through a stable unit-treatment value assumption (Rubin, 1974) or assume that interference arises from a static network of units and employ network randomization designs, such as cluster randomization (Ugander et al.", "startOffset": 96, "endOffset": 109}, {"referenceID": 20, "context": "Current methods assume away such a possibility through a stable unit-treatment value assumption (Rubin, 1974) or assume that interference arises from a static network of units and employ network randomization designs, such as cluster randomization (Ugander et al., 2013) or sequential randomization (Toulis and Kao, 2013).", "startOffset": 248, "endOffset": 270}, {"referenceID": 19, "context": ", 2013) or sequential randomization (Toulis and Kao, 2013).", "startOffset": 36, "endOffset": 58}, {"referenceID": 3, "context": "For example, Athey et al. (2011) developed a method to compare two formats of U.", "startOffset": 13, "endOffset": 33}, {"referenceID": 3, "context": "For example, Athey et al. (2011) developed a method to compare two formats of U.S. timber auctions. Their approach is to use data from one auction in order to estimate aggregate bidder value distributions that act as primitives in a structural model, and thus use those estimates to predict the equilibrium outcomes on the other auction. Although not causal, their estimates do capture the notion of longterm intervention effects. However, as we explain in more detail in Section 2.1, they assume away considerations of temporal behavior. Our goal in this work is to estimate long-term causal effects of interventions that are valid under the Neyman-Rubin model in the setting of economic mechanisms with interacting agents. In Section 2, we introduce a concrete problem instance using data from a real-world behavioral experiment by Rapoport and Boebel (1992). We further provide an overview of related approaches to the problem and demonstrate the need for a valid causal inference method.", "startOffset": 13, "endOffset": 861}, {"referenceID": 3, "context": "For example, Athey et al. (2011) developed a method to compare two formats of U.S. timber auctions. Their approach is to use data from one auction in order to estimate aggregate bidder value distributions that act as primitives in a structural model, and thus use those estimates to predict the equilibrium outcomes on the other auction. Although not causal, their estimates do capture the notion of longterm intervention effects. However, as we explain in more detail in Section 2.1, they assume away considerations of temporal behavior. Our goal in this work is to estimate long-term causal effects of interventions that are valid under the Neyman-Rubin model in the setting of economic mechanisms with interacting agents. In Section 2, we introduce a concrete problem instance using data from a real-world behavioral experiment by Rapoport and Boebel (1992). We further provide an overview of related approaches to the problem and demonstrate the need for a valid causal inference method. In Section 3, we begin by introducing two novel components in the Neyman-Rubin model. The first component is a behavioral state-space where each state is a distribution of a finite number of behaviors over the agent population in the mechanism; this behavioral state is allowed to vary temporally, but it is considered to be latent. Although the treatment assignment mechanism is allowed to affect the initial behavioral state, the features of the aforementioned temporal evolution are intrinsic to the mechanism and thus the evolution is stable under the treatment assignment. The second component is the game-theoretic model which defines the distribution of agent actions, given the payoff structure of the mechanism and the underlying behavioral state, and thus is also stable under the treatment assignment. Taken together, the two aforementioned stability assumptions equip the Neyman-Rubin model for definition and estimation of valid long-term causal effects. In Sections 4 and 5, we develop and demonstrate our method by analyzing the dataset by Rapoport and Boebel (1992). In particular, we adopt an instance of the quantal k-level (QLk) behavioral model introduced by Stahl and Wilson (1994).", "startOffset": 13, "endOffset": 2074}, {"referenceID": 3, "context": "For example, Athey et al. (2011) developed a method to compare two formats of U.S. timber auctions. Their approach is to use data from one auction in order to estimate aggregate bidder value distributions that act as primitives in a structural model, and thus use those estimates to predict the equilibrium outcomes on the other auction. Although not causal, their estimates do capture the notion of longterm intervention effects. However, as we explain in more detail in Section 2.1, they assume away considerations of temporal behavior. Our goal in this work is to estimate long-term causal effects of interventions that are valid under the Neyman-Rubin model in the setting of economic mechanisms with interacting agents. In Section 2, we introduce a concrete problem instance using data from a real-world behavioral experiment by Rapoport and Boebel (1992). We further provide an overview of related approaches to the problem and demonstrate the need for a valid causal inference method. In Section 3, we begin by introducing two novel components in the Neyman-Rubin model. The first component is a behavioral state-space where each state is a distribution of a finite number of behaviors over the agent population in the mechanism; this behavioral state is allowed to vary temporally, but it is considered to be latent. Although the treatment assignment mechanism is allowed to affect the initial behavioral state, the features of the aforementioned temporal evolution are intrinsic to the mechanism and thus the evolution is stable under the treatment assignment. The second component is the game-theoretic model which defines the distribution of agent actions, given the payoff structure of the mechanism and the underlying behavioral state, and thus is also stable under the treatment assignment. Taken together, the two aforementioned stability assumptions equip the Neyman-Rubin model for definition and estimation of valid long-term causal effects. In Sections 4 and 5, we develop and demonstrate our method by analyzing the dataset by Rapoport and Boebel (1992). In particular, we adopt an instance of the quantal k-level (QLk) behavioral model introduced by Stahl and Wilson (1994). The latent behavioral state is thus defined as a distribution on k distinct behavior types, and the game-theoretic model is defined through the quantal responses of the associated behaviors.", "startOffset": 13, "endOffset": 2195}, {"referenceID": 16, "context": "Table 1: Normal-form game in the experiment by Rapoport and Boebel (1992).", "startOffset": 47, "endOffset": 74}, {"referenceID": 16, "context": "Table 2: Frequency of actions for players A and B in the experiment by Rapoport and Boebel (1992) broken down by game and session.", "startOffset": 71, "endOffset": 98}, {"referenceID": 6, "context": "A common econometric approach in evaluating policy changes is the so-called difference-indifferences (DID) estimator (Card and Krueger, 1994; Donald and Lang, 2007; Ostrovsky and Schwarz, 2011).", "startOffset": 117, "endOffset": 193}, {"referenceID": 9, "context": "A common econometric approach in evaluating policy changes is the so-called difference-indifferences (DID) estimator (Card and Krueger, 1994; Donald and Lang, 2007; Ostrovsky and Schwarz, 2011).", "startOffset": 117, "endOffset": 193}, {"referenceID": 14, "context": "A common econometric approach in evaluating policy changes is the so-called difference-indifferences (DID) estimator (Card and Krueger, 1994; Donald and Lang, 2007; Ostrovsky and Schwarz, 2011).", "startOffset": 117, "endOffset": 193}, {"referenceID": 0, "context": "However, in order to be interpreted as a valid causal estimate, strong stability assumptions are required that essentially assume away strategic interference or complex dynamic play (Abadie, 2005; Angrist and Pischke, 2008).", "startOffset": 182, "endOffset": 223}, {"referenceID": 3, "context": "Such an approach was developed by Brodersen et al. (2014), who wanted to estimate the effects of ad campaigns on website visits.", "startOffset": 34, "endOffset": 58}, {"referenceID": 0, "context": "However, in order to be interpreted as a valid causal estimate, strong stability assumptions are required that essentially assume away strategic interference or complex dynamic play (Abadie, 2005; Angrist and Pischke, 2008). Athey et al. (2011) studied the effects of timber auction format (ascending versus sealed bid)", "startOffset": 183, "endOffset": 245}, {"referenceID": 15, "context": "Finally, another popular approach to causality is through directed acyclical graphs (DAGs) between the variables of interest (Pearl, 2000).", "startOffset": 125, "endOffset": 138}, {"referenceID": 7, "context": "Proper causal inference in equilibrium systems remains an open area with no well-established methodology (Dash, 2005).", "startOffset": 105, "endOffset": 117}, {"referenceID": 13, "context": "Finally, another popular approach to causality is through directed acyclical graphs (DAGs) between the variables of interest (Pearl, 2000). For example, Bottou et al. (2013) study the causal effects of the machine learning algorithm that scores online ads in the Bing search engine on the search engine revenue.", "startOffset": 126, "endOffset": 174}, {"referenceID": 7, "context": "As pointed out by Dash and Druzdzel (2001), this might be implausible in equilibrium systems.", "startOffset": 18, "endOffset": 43}, {"referenceID": 16, "context": ", n} be a population of n agents and G0, G1 be two forms of the same game as in (Rapoport and Boebel, 1992).", "startOffset": 80, "endOffset": 107}, {"referenceID": 3, "context": "3Note that Athey et al. (2011) consider a different situation than ours.", "startOffset": 11, "endOffset": 31}, {"referenceID": 17, "context": "For example, the SUTVA assumption (Rubin, 1974) posits Yit(Z) = Yi(Zi) i.", "startOffset": 34, "endOffset": 47}, {"referenceID": 12, "context": "This is similar to the policy invariance assumption (Heckman and Vytlacil, 2005; Heckman et al., 1998), in which given the choice of treatment by the agent, the initial treatment assignment mechanism does not affect the outcomes.", "startOffset": 52, "endOffset": 102}, {"referenceID": 13, "context": "This is similar to the policy invariance assumption (Heckman and Vytlacil, 2005; Heckman et al., 1998), in which given the choice of treatment by the agent, the initial treatment assignment mechanism does not affect the outcomes.", "startOffset": 52, "endOffset": 102}, {"referenceID": 22, "context": "For our behavioral model we adopt the quantal k-response (QLk) that was initially proposed by Stahl and Wilson (1994), and was shown to predict well the observed human behavior in real-world behavioral experiments (Wright and Leyton-Brown, 2010).", "startOffset": 214, "endOffset": 245}, {"referenceID": 16, "context": "In Section 5 we will apply our method in the dataset by Rapoport and Boebel (1992). For our behavioral model we adopt the quantal k-response (QLk) that was initially proposed by Stahl and Wilson (1994), and was shown to predict well the observed human behavior in real-world behavioral experiments (Wright and Leyton-Brown, 2010).", "startOffset": 56, "endOffset": 83}, {"referenceID": 16, "context": "In Section 5 we will apply our method in the dataset by Rapoport and Boebel (1992). For our behavioral model we adopt the quantal k-response (QLk) that was initially proposed by Stahl and Wilson (1994), and was shown to predict well the observed human behavior in real-world behavioral experiments (Wright and Leyton-Brown, 2010).", "startOffset": 56, "endOffset": 202}, {"referenceID": 1, "context": "Since \u03b2j,t(Z) is a discrete distribution, relevant statistical methods are available in analysis of continuous proportions, or compositional data (Aitchison, 1986).", "startOffset": 146, "endOffset": 163}, {"referenceID": 16, "context": "We now return to the dataset of Rapoport and Boebel (1992) presented in Table 2.", "startOffset": 32, "endOffset": 59}, {"referenceID": 16, "context": "Working on a real-world dataset from a behavioral experiment (Rapoport and Boebel, 1992), we showed how our method can be applied for estimation of a long-term effect of intervention in the payoff structure of a game design in normal-form.", "startOffset": 61, "endOffset": 88}], "year": 2017, "abstractText": "Estimation of causal effects of interventions in dynamical systems of interacting agents is under-developed. In this paper, we explore the intricacies of this problem through standard approaches, and demonstrate the need for more appropriate methods. Working under the Neyman-Rubin causal model, we proceed to develop a causal inference method and we explicate the stability assumptions that are necessary for valid causal inference. Our method consists of a temporal component that models the evolution of behaviors that agents adopt over time, and a behavioral component that models the distribution of agent actions conditional on adopted behaviors. This allows the imputation of long-term estimates of quantities of interest, and thus the estimation of long-term causal effects of interventions. We demonstrate our method on a dataset from behavioral game theory, and discuss open problems to stimulate future research.", "creator": "TeX"}}}