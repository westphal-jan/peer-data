{"id": "1501.03093", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2015", "title": "MultiGain: A controller synthesis tool for MDPs with multiple mean-payoff objectives", "abstract": "we present multigain, typically a tool specializing to synthesize strategies for markov decision processes ( mdps ) and with multiple mean - payoff comparison objectives. our models are described in prism, and our tool uses the existing interface and simulator methodology of prism. our tool extends prism by adding novel algorithms possible for multiple mean - payoff objectives, and also provides features enhancement such as ( i ) ~ generating strategies and exploring them valid for simulation, and checking them with as respect to other properties ; and ( ii ) ~ generating an approximate exact pareto curve for both two mean - payoff objectives. in january addition, we present a new standard practical algorithm for the analysis of mdps with multiple mean - payoff objectives under memoryless strategies.", "histories": [["v1", "Tue, 13 Jan 2015 18:04:46 GMT  (84kb,D)", "http://arxiv.org/abs/1501.03093v1", "Extended version for a TACAS 2015 tool demo paper"]], "COMMENTS": "Extended version for a TACAS 2015 tool demo paper", "reviews": [], "SUBJECTS": "cs.AI cs.LO", "authors": ["tom\\'a\\v{s} br\\'azdil", "krishnendu chatterjee", "vojt\\v{e}ch forejt", "anton\\'in ku\\v{c}era"], "accepted": false, "id": "1501.03093"}, "pdf": {"name": "1501.03093.pdf", "metadata": {"source": "CRF", "title": "MultiGain: A controller synthesis tool for MDPs with multiple mean-payoff objectives", "authors": ["Tom\u00e1\u0161 Br\u00e1zdil", "Krishnendu Chatterjee", "Vojt\u011bch Forejt"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Markov decision processes (MDPs) are the de facto model for analysis of probabilistic systems with non-determinism [11], with a wide range of applications [5]. In each state of an MDP, a controller chooses one of several actions (the nondeterministic choices), and the current state and action gives a probability distribution over the successor states. One classical objective used to study quantitative properties of systems is the limit-average (or mean-payoff) objective, where a reward (or cost) is associated with each transition and the objective assigns to every run the average of the rewards over the run. MDPs with single mean-payoff objectives have been well studied in the literature (see, e.g., [13]). However, in many modeling domains, there is not a single goal to be optimized, but multiple, potentially interdependent and conflicting goals. For example, in designing a computer system, the goal is to maximize average performance while minimizing average power consumption. Similarly, in an inventory management system, the goal is to optimize several dependent costs for maintaining each kind of product. The complexity of MDPs with multiple mean-payoff objectives was studied in [6].\nIn this paper we present MultiGain, which is, to the best of our knowledge, the first tool for synthesis of controller strategies in MDPs with multiple mean-payoff objectives. The MDPs and the mean-payoff objectives are specified in the wellknown PRISM modelling language. Our contributions are as follows: (1) we extend PRISM with novel algorithms for multiple mean-payoff objectives from [6]; (2) develop on the results of [6] to synthesize strategies, and explore them for simulation, and check them with respect to other properties (as done in PRISM-games [8]); and (3) for the important special case of two mean-payoff objectives we provide the feature to visualize the approximate Pareto curve (where the Pareto curve represents the \u201ctrade-off\u201d curve and consists of solutions that are not strictly dominated\nar X\niv :1\n50 1.\n03 09\n3v 1\n[ cs\n.A I]\n1 3\nJa n\n20 15\nby any other solution). Finally, we present a new practical approach for analysis of MDPs with multiple mean-payoff objectives under memoryless strategies: previously an NP bound was shown in [7] by guessing all bottom strongly connected components (BSCCs) of the MDP graph for a memoryless strategy and this gave an exponential enumerative algorithm; in contrast, we present a linear reduction to solving a boolean combination of linear constraints (which is a special class of mixed integer linear programming where the integer variables are binary)."}, {"heading": "2 Definitions", "text": "MDPs and strategies. An MDP G = (S,A,Act , \u03b4) consists of (i) a finite set S of states; (ii) a finite set A of actions, (iii) an action enabledness function Act : S \u2192 2A \\ {\u2205} that assigns to each state s the set Act(s) of actions enabled at s, and (iv) a transition function \u03b4 : S \u00d7 A \u2192 dist(S) that given a state s and an action a \u2208 Act(s) gives a probability distribution over the successor states (dist(S) denotes all probability distributions over S). W.l.o.g. we assume that every action is enabled in exactly one state, and we denote this state Src(a). Thus, we will assume that \u03b4 : A \u2192 dist(S). Strategies describe how to choose the next action given a finite path (of state and action pairs) in the MDP. A strategy consists of a set of memory elements to remember the history of the paths. The memory elements are updated stochastically in each transition, and the next action is chosen probabilistically (among enabled actions) based on the current state and current memory [6]. A strategy is memoryless if it depends only on the current state.\nMultiple mean-payoff objectives. A single mean-payoff objective consists of a reward function r that assigns a real-valued reward r(s, a) to every state s and action a enabled in s, and the mean-payoff objective mp(r) assigns to every infinite path (or run) the long-run average of the rewards of the path, i.e., for a infinite path \u03c0 = (s0a0s1a1 . . .) we have mp(r)(\u03c0) = lim infn\u2192\u221e 1 n \u00b7 \u2211n\u22121 i=0 r(si, ai). In multiple mean-payoff objectives, there are k reward functions r1, r2, . . . , rk, and each reward function ri defines the respective mean-payoff objective mp(ri). Given a strategy \u03c3, we denote by E\u03c3s [\u00b7] the expectation measure of the strategy given a starting state s. Thus for a mean-payoff objective mp(r), the expected mean-payoff is E\u03c3s [mp(r)]. Synthesis questions. The relevant questions in analysis of MDPs with multiple objectives are as follows: (1) (Existence). Given an MDP with k reward functions, starting state s0, and a vector v = (v1, v2, . . . , vk) of k real-values, the existence question asks whether there exists a strategy \u03c3 such that for all 1 \u2264 i \u2264 k we have E\u03c3s0 [mp(ri)] \u2265 vi. (2) (Synthesis). If the answer to the existence question is yes, the synthesis question asks for a witness strategy to satisfy the existence question. An optimization question related to multiple objectives is the computation of the Pareto-curve (or the trade-off curve), where the Pareto curve consists of vectors v such that the answer to the existence question is yes, and for all vectors v\u2032 that strictly dominate v (i.e., v\u2032 is at least v in all dimensions and strictly greater in at least one dimension) the answer to the existence question is no."}, {"heading": "3 Algorithms and Implementation", "text": "We first recall the existing results for MDPs with multiple mean-payoff objectives [6], and then describe our implementation and extensions. Before presenting the existing results, we first recall the notion of maximal end-components in MDPs.\nMaximal end-components. A pair (T,B) with \u2205 6= T \u2286 S and B \u2286 \u22c3 t\u2208T Act(t) is an end component of G if (1) for all a \u2208 B, whenever \u03b4(a)(s\u2032) > 0 then s\u2032 \u2208 T ; and (2) for all s, t \u2208 T there is a finite path from s to t such that all states and actions that appear in the path belong to T and B, respectively. An end component (T,B) is a maximal end component (MEC) if it is maximal wrt. pointwise subset ordering. An MDP is unichain if for all B \u2286 A satisfying B \u2229 Act(s) 6= \u2205 for any s \u2208 S we have that (S,B) is a MEC. Given an MDP, we denote SMEC the set of states s that are contained within a MEC.\nResult from [6]. The results of [6] showed that (i) the existence question can be answered in polynomial time, by reduction to linear programming; (ii) if there exists a strategy for the existence problem, then there exists a witness strategy with only two-memory states. It also established that if the MDP is unichain, then memoryless strategies are sufficient. The polynomial-time algorithm is as follows: it was shown in [6] that the answer to the existence problem is yes iff there exists a non-negative solution to the system of linear inequalities given in Fig. 1.\nSyntax and semantics. Our tool accepts PRISM MDP models as input, see [1] for details. The multi-objective properties are expressed as multi(list) or mlessmulti(list) where list is a comma separated list of mean-payoff reward properties, which can be boolean, e.g. R{\u2019r1\u2019}>=0.5 [S], and in the case of multi also numerical, e.g. R{\u2019r2\u2019}min=? [S]. In the reward properties, S stands for steadystate, following PRISM\u2019s terminology.\nIf all properties in the list are boolean, the multi-objective property multi(list) is also boolean and is true iff there is a strategy under which all given reward properties in the list are simultaneously satisfied. If there is a single numerical query, the multi-objective query intuitively asks for the maximal achievable reward of the numerical reward query, subject to the restriction given by the boolean queries. We also allow two numerical queries; in such case MultiGain generates a Pareto curve. The semantics of mlessmulti follows the same pattern, the only difference being that only memoryless (randomised) strategies are being considered. The reason we don\u2019t allow numerical reward properties in mlessmulti is that the supremum among all memoryless strategies might not be realised.\nImplementation of existence question. We have implemented the algorithm of [6]. Our implementation takes as input an MDP with multiple mean-payoff objectives and a value vector v, and computes the linear inequalities of Fig. 1 or a mixed integer linear programming (MILP) extension in case of memoryless strategies. The system of linear inequalities is solved with LPsolve [2] or Gurobi [3].\nImplementation of the synthesis question. We now describe how to obtain witness strategies. Assume that the linear program from Fig. 1 has a solution, where a solution to a variable z is denoted by z. We construct a new linear program, comprising Eq. 1 together with the equations ys = \u2211 a\u2208Act(s) xa for all s \u2208 SMEC.\nLet z\u0302 denote a solution to variables z in this linear program. The stochasticupdate strategy is defined to have 2 memory states (\u201ctransient\u201d and \u201crecurrent\u201d), with the transition function defined to be \u03c3t(s)(a) = y\u0302a/ \u2211 b\u2208Act(s) y\u0302b and\n\u03c3r (s)(a) = xa/ \u2211 b\u2208Act(s) xb, and the probability of switching from \u201ctransient\u201d to\n\u201crecurrent\u201d state upon entering s being y\u0302s/( \u2211 a\u2208Act(s) y\u0302a + y\u0302s). The correctness of the witness construction follows from [6].\nMILP for memoryless strategies. For memoryless strategies, the current upper bound is NP [7] and the previous algorithm enumerates all possible BSCCs under a memoryless strategy. We present a polynomial-time reduction to solving a boolean combination of linear constraints, that can be easily encoded using MILP with binary variables [15]. The key requirement for memoryless strategies is that a state can either be recurrent or transient. For the existence question restricted to memoryless strategies we modify the linear constraints from Fig. 1 as follows: (i) we add constraints; for all states s and actions b \u2208 Act(s): yb > 0 =\u21d2 (xb > 0 \u2228 \u2211 a\u2208Act(Src(b)) xa = 0); (ii) we replace constraint (3) from\nFig. 1 by constraints that for all states s: ys = \u2211 a\u2208Act(s) xa. The constraint (ii) is a strengthening of constraint (3), as the above constraint implies constraint (3). The intuition behind the additional constraint is as follows: \u2211 a\u2208Act(Src(b)) xa = 0 represents transient states where there is no restriction on yb, otherwise yb can be positive only if xb is positive. The witness strategy \u03c3 is as follows: let z denote a solution to the MILP for the memoryless strategy question; for a state s, if\u2211 a\u2208Act(s) xa = 0 (i.e., s is transient), then \u03c3(s)(a) = ya/ \u2211 b\u2208Act(s) yb; otherwise,\n\u03c3(s)(a) = xa/ \u2211 b\u2208Act(s) xb. Further details are in Appendix.\nApproximate Pareto curve for two objectives. To generate a Pareto curve, we successively compute solutions to several linear programs for a single mean-payoff objective, where every time the objective is obtained as a weighted sum of the objectives for which the Pareto curve is generated. The weights are selected in a way similar to [10], allowing us to obtain the approximation of the curve.\nUnlike the PRISM implementation for multi-objective cumulative rewards, our tool is able to generate the Pareto curve for objectives of the form multi(R{\u2019r1\u2019}max=?[S], R{\u2019r2\u2019}max=? [S], R{\u2019r3\u2019}>=0.5 [S]) where the objectives to be optimised are subject to restrictions given by other rewards.\nFeatures of our tool. In summary, our tool extends PRISM by developing algorithms to solve MDPs with multiple mean-payoff objectives. Along with the algorithm from [6] we have also implemented a visual representation of the Pareto curve for two-dimensional objectives. The implementation utilises a multi-objective visualisation available in PRISM for cumulative reward and LTL objectives.\nIn addition, we adapted a feature from PRISM-games [8] which allows the user to generate strategies, so that they can be explored and investigated by simulation. A product (Markov chain) of an MDP and a strategy can be constructed, allowing the user to employ it for verification of other properties.\nThe tool is available at http://qav.cs.ox.ac.uk/multigain/, and the source code is provided under GPL. For licencing reasons, Gurobi is not included with the download, but it can be added manually by following provided steps."}, {"heading": "4 Experimental Results: Case Studies", "text": "We have evaluated our tool on two standard case studies, adapted from [1], and also mention other applications where our tool could be used.\nDining philosophers is a case study based on the algorithm of [9], which extends Lehmann and Rabin\u2019s randomised solution [12] to the dining philosophers problem so that there is no requirement for fairness assumptions. The constant N gives the number of philosophers. We use two reward structures, think and eat for the number of philosophers currently thinking and eating, respectively.\nRandomised Mutual Exclusion models a solution to the mutual exclusion problem by [14]. The parameter N gives the number of processes competing for the access to the critical section. Here we defined reward structures try and crit for the number of processes that are currently trying to access the critical section, and those which are in it, currently (the latter number obviously never being more than 1).\nEvaluation The statistics for some of our experiments are given in Table 1 (the complete results are available from the tool\u2019s website). The experiments were run on a 2.66GHz PC with 4GB RAM, the LP solver used was Gurobi and the timeout (\u201ct/o\u201d) was set to 2 hours. We observed that our approach scales to mid-size models, the main limitation being the LP solver.\nOther applications. We mention two applications which are solved using MDPs with multiple mean-payoff objectives. (A) The problem of synthesis from incompatible specifications was considered in [16]. Given a set of specifications \u03d51, \u03d52, . . . , \u03d5k that cannot be all satisfied together, the goal is to synthesize a system such that for all 1 \u2264 i \u2264 k the distance to specification \u03d5i is at most vi. In adversarial environments the problem reduces to games and for probabilistic environments to MDPs, with multiple mean-payoff objectives [16]. (B) The problem of synthesis of steady state distributions for ergodic MDPs was considered in [4]. The problem can\nbe modeled with multiple mean-payoff objectives by considering indicator reward functions rs, for each state s, that assign reward 1 to every action enabled in s and 0 to all other actions. The steady state distribution synthesis question of [4] then reduces to the existence question for multiple mean-payoff MDPs.\nConcluding remarks. We presented the first tool for analysis of MDPs with multiple mean-payoff objectives. The limiting factor is the LP solver, and so an interesting direction would be to extend the results of [17] to multiple objectives.\nAcknowledgements The authors were in part supported by Austrian Science Fund (FWF) Grant No P23499- N23, FWF NFN Grant No S11407-N23 (RiSE), ERC Start grant (279307: Graph Games) and the research centre Institute for Theoretical Computer Science (ITI), grant No. P202/12/G061."}, {"heading": "17. R. Wimmer, B. Braitling, B. Becker, E. M. Hahn, P. Crouzen, H. Hermanns, A. Dhama, and O. E. Theel. Symblicit calculation of long-run averages for concurrent probabilistic", "text": "systems. In QEST, pages 27\u201336. IEEE Computer Society, 2010."}, {"heading": "5 Appendix", "text": "Argument for mixed ILP for memoryless strategies. The main idea of the correctness argument of the Boolean combination of linear constraints is as follows. Given a memoryless strategy \u03c3, once the strategy is fixed we obtain a Markov chain with two types of states, transient states and recurrent states. For recurrent states the variable x represent the flow equations and intuitively, the variable xa denotes the long-run average frequency of the action a. The constraint \u2211 a\u2208Act(s) xa = 0 is the constraint to represent that a state is transient. The variables ya are used to determine the probabilities to reach the recurrent classes. For an action b, if Src(b) is transient, then for the first additional constraint for memoryless strategies, since\u2211 a\u2208Act(Src(b)) xa = 0 is satisfied, it follows that there is no restriction on yb. Once a recurrent class is reached, it suffices to switch deterministically to the strategy of the recurrent class. However, this is encoded slightly differently: we allow to continue before switching but only in the recurrent class, which is enforced by the constraint yb > 0 =\u21d2 xb > 0. For a state s in a recurrent class \u2211 a\u2208Act(s) xa denotes the longrun average frequency of s, and the constraint (ii) requires that the long-run average frequency of s coincides with the probability ys to reach s as the first state of the recurrent class. Note that for a recurrent class Z, the sum \u2211 s\u2208Z ys represents the probability that the recurrent class Z is reached. We describe the details of witness strategy constructions.\nStrategy from solution. Given a solution to the mixed ILP, let z denote the solution to variable z. We construct a witness memoryless strategy as follows: for a state s, if \u2211 a\u2208Act(s) xa > 0, then for all a\n\u2032 \u2208 Act(s) the memoryless strategy plays a\u2032 with probability xa\u2032/ \u2211 a\u2208Act(s) xa. For a state s, if \u2211 a\u2208Act(s) xa = 0, then for all\na\u2032 \u2208 Act(s) the memoryless strategy plays a\u2032 with probability ya\u2032/ \u2211 a\u2208Act(s) ya.\nSolution from strategy. Consider a witness memoryless strategy \u03c3 and consider the Markov chain obtained by fixing the strategy. Let X denote the set of recurrent states in the Markov chain, and Y = S\\X the set of transient states. The assignment of the variables to satisfy the constraints of the mixed ILP are as follows: (a) For states s in X and action a \u2208 Act(s), the variable xa is assigned the long-run average frequency of action a in the Markov chain; and ys is assigned the probability that the first state reached in the recurrent class containing s is s. (b) For states s in Y and a \u2208 Act(s) we assign xa = 0 and ya = \u03c3(s)(a) the probability assigned by the strategy \u03c3."}], "references": [{"title": "The steady-state control problem for Markov decision processes", "author": ["S. Akshay", "N. Bertrand", "S. Haddad", "L. H\u00e9lou\u00ebt"], "venue": "In QEST, pages 290\u2013304,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Principles of model checking", "author": ["C. Baier", "J.-P. Katoen"], "venue": "MIT Press,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Two views on multiple mean-payoff objectives in Markov decision processes", "author": ["T. Br\u00e1zdil", "V. Bro\u017eek", "K. Chatterjee", "V. Forejt", "A. Ku\u010dera"], "venue": "In LICS 2011, pages 33\u201342. IEEE Computer Society,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Markov decision processes with multiple long-run average objectives", "author": ["K. Chatterjee"], "venue": "In FSTTCS, pages 473\u2013484,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "PRISM-games: A model checker for stochastic multi-player games", "author": ["T. Chen", "V. Forejt", "M. Kwiatkowska", "D. Parker", "A. Simaitis"], "venue": "In TACAS\u201913, volume 7795 of LNCS. Springer,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Randomized dining philosophers without fairness assumption", "author": ["M. Duflot", "L. Fribourg", "C. Picaronny"], "venue": "Distributed Computing, 17(1):65\u201376,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Pareto curves for probabilistic model checking", "author": ["V. Forejt", "M. Kwiatkowska", "D. Parker"], "venue": "In ATVA\u201912, volume 7561 of LNCS, pages 317\u2013332. Springer,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic Programming and Markov Processes", "author": ["R.A. Howard"], "venue": "MIT Press,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1960}, {"title": "On the advantage of free choice: A symmetric and fully distributed solution to the dining philosophers problem", "author": ["D. Lehmann", "M. Rabin"], "venue": "In POPL\u201981,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1981}, {"title": "Markov Decision Processes", "author": ["M.L. Puterman"], "venue": "J. Wiley and Sons,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "N -process mutual exclusion with bounded waiting by 4 log2N -valued shared variable", "author": ["M. Rabin"], "venue": "Journal of Computer and System Sciences, 25(1):66\u201375,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1982}, {"title": "Theory of Linear and Integer Programming", "author": ["A. Schrijver"], "venue": "John Wiley & Sons,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Synthesis from incompatible specifications", "author": ["P. \u010cern\u00fd", "S. Gopi", "T.A. Henzinger", "A. Radhakrishna", "N. Totla"], "venue": "In EMSOFT, pages 53\u201362,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Symblicit calculation of long-run averages for concurrent probabilistic systems", "author": ["R. Wimmer", "B. Braitling", "B. Becker", "E.M. Hahn", "P. Crouzen", "H. Hermanns", "A. Dhama", "O.E. Theel"], "venue": "In QEST, pages 27\u201336. IEEE Computer Society,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "Markov decision processes (MDPs) are the de facto model for analysis of probabilistic systems with non-determinism [11], with a wide range of applications [5].", "startOffset": 115, "endOffset": 119}, {"referenceID": 1, "context": "Markov decision processes (MDPs) are the de facto model for analysis of probabilistic systems with non-determinism [11], with a wide range of applications [5].", "startOffset": 155, "endOffset": 158}, {"referenceID": 9, "context": ", [13]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": "The complexity of MDPs with multiple mean-payoff objectives was studied in [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "Our contributions are as follows: (1) we extend PRISM with novel algorithms for multiple mean-payoff objectives from [6]; (2) develop on the results of [6] to synthesize strategies, and explore them for simulation, and check them with respect to other properties (as done in PRISM-games [8]); and (3) for the important special case of two mean-payoff objectives we provide the feature to visualize the approximate Pareto curve (where the Pareto curve represents the \u201ctrade-off\u201d curve and consists of solutions that are not strictly dominated ar X iv :1 50 1.", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "Our contributions are as follows: (1) we extend PRISM with novel algorithms for multiple mean-payoff objectives from [6]; (2) develop on the results of [6] to synthesize strategies, and explore them for simulation, and check them with respect to other properties (as done in PRISM-games [8]); and (3) for the important special case of two mean-payoff objectives we provide the feature to visualize the approximate Pareto curve (where the Pareto curve represents the \u201ctrade-off\u201d curve and consists of solutions that are not strictly dominated ar X iv :1 50 1.", "startOffset": 152, "endOffset": 155}, {"referenceID": 4, "context": "Our contributions are as follows: (1) we extend PRISM with novel algorithms for multiple mean-payoff objectives from [6]; (2) develop on the results of [6] to synthesize strategies, and explore them for simulation, and check them with respect to other properties (as done in PRISM-games [8]); and (3) for the important special case of two mean-payoff objectives we provide the feature to visualize the approximate Pareto curve (where the Pareto curve represents the \u201ctrade-off\u201d curve and consists of solutions that are not strictly dominated ar X iv :1 50 1.", "startOffset": 287, "endOffset": 290}, {"referenceID": 3, "context": "Finally, we present a new practical approach for analysis of MDPs with multiple mean-payoff objectives under memoryless strategies: previously an NP bound was shown in [7] by guessing all bottom strongly connected components (BSCCs) of the MDP graph for a memoryless strategy and this gave an exponential enumerative algorithm; in contrast, we present a linear reduction to solving a boolean combination of linear constraints (which is a special class of mixed integer linear programming where the integer variables are binary).", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "The memory elements are updated stochastically in each transition, and the next action is chosen probabilistically (among enabled actions) based on the current state and current memory [6].", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "We first recall the existing results for MDPs with multiple mean-payoff objectives [6], and then describe our implementation and extensions.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Result from [6].", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "The results of [6] showed that (i) the existence question can be answered in polynomial time, by reduction to linear programming; (ii) if there exists a strategy for the existence problem, then there exists a witness strategy with only two-memory states.", "startOffset": 15, "endOffset": 18}, {"referenceID": 2, "context": "The polynomial-time algorithm is as follows: it was shown in [6] that the answer to the existence problem is yes iff there exists a non-negative solution to the system of linear inequalities given in Fig.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "We have implemented the algorithm of [6].", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "The correctness of the witness construction follows from [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "For memoryless strategies, the current upper bound is NP [7] and the previous algorithm enumerates all possible BSCCs under a memoryless strategy.", "startOffset": 57, "endOffset": 60}, {"referenceID": 11, "context": "We present a polynomial-time reduction to solving a boolean combination of linear constraints, that can be easily encoded using MILP with binary variables [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 6, "context": "The weights are selected in a way similar to [10], allowing us to obtain the approximation of the curve.", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "Along with the algorithm from [6] we have also implemented a visual representation of the Pareto curve for two-dimensional objectives.", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "In addition, we adapted a feature from PRISM-games [8] which allows the user to generate strategies, so that they can be explored and investigated by simulation.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "Dining philosophers is a case study based on the algorithm of [9], which extends Lehmann and Rabin\u2019s randomised solution [12] to the dining philosophers problem so that there is no requirement for fairness assumptions.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "Dining philosophers is a case study based on the algorithm of [9], which extends Lehmann and Rabin\u2019s randomised solution [12] to the dining philosophers problem so that there is no requirement for fairness assumptions.", "startOffset": 121, "endOffset": 125}, {"referenceID": 10, "context": "Randomised Mutual Exclusion models a solution to the mutual exclusion problem by [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "(A) The problem of synthesis from incompatible specifications was considered in [16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "In adversarial environments the problem reduces to games and for probabilistic environments to MDPs, with multiple mean-payoff objectives [16].", "startOffset": 138, "endOffset": 142}, {"referenceID": 0, "context": "(B) The problem of synthesis of steady state distributions for ergodic MDPs was considered in [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "The steady state distribution synthesis question of [4] then reduces to the existence question for multiple mean-payoff MDPs.", "startOffset": 52, "endOffset": 55}, {"referenceID": 13, "context": "The limiting factor is the LP solver, and so an interesting direction would be to extend the results of [17] to multiple objectives.", "startOffset": 104, "endOffset": 108}], "year": 2015, "abstractText": "We present MultiGain, a tool to synthesize strategies for Markov decision processes (MDPs) with multiple mean-payoff objectives. Our models are described in PRISM, and our tool uses the existing interface and simulator of PRISM. Our tool extends PRISM by adding novel algorithms for multiple mean-payoff objectives, and also provides features such as (i) generating strategies and exploring them for simulation, and checking them with respect to other properties; and (ii) generating an approximate Pareto curve for two mean-payoff objectives. In addition, we present a new practical algorithm for the analysis of MDPs with multiple mean-payoff objectives under memoryless strategies.", "creator": "LaTeX with hyperref package"}}}