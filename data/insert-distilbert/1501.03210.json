{"id": "1501.03210", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2015", "title": "Towards Deep Semantic Analysis Of Hashtags", "abstract": "hashtags are semantico - algebraic syntactic constructs used across various social networking and microblogging platforms to enable users to start a topic specific discussion link or classify a digital post into a desired category. segmenting and linking the entities present within the hashtags could best therefore help in better understanding and extraction of information shared across the main social media. however, due to lack of space delimiters in the hashtags ( e. g # nsavssnowden ), the segmentation of hashtags into constituent entities ( \" nsa \" and \" edward snowden \" in this case ) is not a trivial task. most of the current state - of - the - art social media analytics systems like sentiment analysis and entity linking tend to literally either ignore hashtags, or treat them as a single word. in this paper, we present a context aware approach to segment and link entities in the hashtags to a knowledge base ( kb ) entry, based on the context within the tweet. our search approach segments and links across the entities in hashtags such that the coherence between external hashtag message semantics and the tweet is maximized. to the best of our knowledge, no existing study addresses the issue of linking entities in hashtags for extracting semantic information. we evaluate our method on two different datasets, and demonstrate the effectiveness of our technique in improving the overall entity linking in tweets via additional semantic information provided by segmenting and linking entities in a hashtag.", "histories": [["v1", "Tue, 13 Jan 2015 23:51:29 GMT  (610kb,D)", "http://arxiv.org/abs/1501.03210v1", "To Appear in 37th European Conference on Information Retrieval"]], "COMMENTS": "To Appear in 37th European Conference on Information Retrieval", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["piyush bansal", "romil bansal", "vasudeva varma"], "accepted": false, "id": "1501.03210"}, "pdf": {"name": "1501.03210.pdf", "metadata": {"source": "CRF", "title": "Towards Deep Semantic Analysis of Hashtags", "authors": ["Piyush Bansal", "Romil Bansal", "Vasudeva Varma"], "emails": ["piyush.bansal@research.iiit.ac.in,", "romil.bansal@research.iiit.ac.in,", "vv@iiit.ac.in"], "sections": [{"heading": null, "text": "Keywords: Hashtag Segmentation, Entity Linking, Entity Disambiguation, Information Extraction"}, {"heading": "1 Introduction", "text": "Microblogging and Social Networking websites like Twitter, Google+, Facebook and Instagram are becoming increasingly popular with more than 400 million posts each day. This huge collection of posts on the social media makes it an important source for gathering real-time news and event information. Microblog posts are often tagged with an unspaced phrase, prefixed with the sign \u201c#\u201d known as a hashtag. 14% of English tweets are tagged with at least 1 hashtag with 1.4 hashtags per tweet [1]. Hashtags make it possible to categorize and track a microblog post among millions of other posts. Semantic analysis of hashtags could therefore help us in understanding and extracting important information from microblog posts. ar X iv :1\n50 1.\n03 21\n0v 1\n[ cs\n.I R\n] 1\n3 Ja\nn 20\n15\nIn English, and many other Latin alphabet based languages, the inherent structure of the language imposes an assumption, under which the space character is a good approximation of word delimiter. However, hashtags violate such an assumption making it difficult to analyse them. In this paper, we analyse the problem of extracting semantics in hashtags by segmenting and linking entities within hashtags. For example, given a hashtag like \u201c#NSAvsSnowden\u201d occurring inside a tweet, we develop a system that not only segments the hashtag into \u201cNSA vs Snowden\u201d, but also tells that \u201cNSA\u201d refers to \u201cNational Security Agency\u201d and \u201cSnowden\u201d refers to \u201cEdward Snowden\u201d. Such a system has numerous applications in the areas of Sentiment Analysis, Opinion Mining, Event Detection and improving quality of search results on Social Networks, as these systems can leverage additional semantic information provided by the hashtags present within the tweets. Our system takes a hashtag and the corresponding tweet text as input and returns the segmented hashtag along with Wikipedia pages corresponding to the entities in the hashtag. To the best of our knowledge, the proposed system is the first to focus on extracting semantic knowledge from hashtags by segmenting them into constituent entities."}, {"heading": "2 Related Work", "text": "The problem of word segmentation has been studied in various contexts in the past. A lot of work has been done on Chinese word segmentation. Huang et al. [3] showed that character based tagging approach outperforms other word based segmentation approaches for Chinese word segmentation. English URL segmentation has also been explored by various researchers in the past [2][5][6]. All such systems explored length specific features to segment the URLs into constituent chunks1. Although a given hashtag can be segmented into various possible segments, all of which are plausible, the \u201ccorrect\u201d segmentation depends on the tweet context. For example, consider a hashtag \u2018notacon\u2019. It can be segmented into chunks \u2018not, a, con\u2019 or \u2018nota, con\u2019 based on the tweet context. The proposed system focuses on hashtag segmentation while being context aware. Along with unigram, bigram and domain specific features, content in the tweet text is also considered for segmenting and linking the entities within a hashtag.\nEntity linking in microposts has also been studied by various researchers recently. Various features like commonness, relatedness, popularity and recentness have been used for detecting and linking the entities in the microposts [11][12][13]. Although semantic analysis of microposts has been studied vastly, hashtags are either ignored or treated as a single word. In this work, we analyse hashtags by linking entities in the hashtags to the corresponding Wikipedia page.\n1 The term \u201cchunk\u201d here and henceforth refers to each of the segments si in a segmentation S = s1, s2, ...si, ...sn. For example, in case of the hashtag #NSAvsSnowden, one of the possible segmentations (S) is NSA, vs, Snowden. Here, the words - \u201cNSA\u201d, \u201cvs\u201d and \u201cSnowden\u201d are being referred to as chunks."}, {"heading": "3 System Architecture", "text": "In this section, we present an overview of our system. We also describe the features extracted, followed by a discussion on training and learning procedures in Section 4.\nAs illustrated in Fig. 1, the proposed system has 3 major components - 1) Hashtag Segmentations Seeder, 2) Feature Extraction and Entity Linking module, and 3) Segmentation Ranker. In the following sections, we describe each component in detail."}, {"heading": "3.1 Hashtag Segmentations Seeder", "text": "Hashtag Segmentations Seeder is responsible for generating a list of possible segmentations of a given hashtag. We propose Variable Length Sliding Window technique for generating a set of highly probable hashtag segmentations for the given hashtag in the first step.\nThe Variable Length Sliding Window technique is based on an assumption that for a given hashtag \u201c#AXB\u201d, if A and B are valid semantic units (a single word or a collection of words concatenated together without a space), it is reasonable to hypothesize that X is also a valid semantic unit. For example, in the hashtag \u201c#followUCBleague\u201d, since, \u2018follow \u2019 and \u2018league\u2019 are well known dictionary words, and collectively this hashtag has some semantic meaning associated with it as it has occurred in a tweet, it is reasonable to assume that \u2018UCB \u2019 is also a valid semantic unit with some meaning associated with it. The length of the sliding window(X) is varied from MIN LEN to MAX LEN with each iteration, and the window is slid over the hashtag. O(n2) triplets of the form (A, X, B) are generated using the sliding window technique, where n is the length of the hashtag, X is the part of the hashtag lying within the window and A and B are the parts of the hashtag (of length \u2265 0) that lie on the left and right of the window respectively.\nEach segment A and B of the triplet (A, X, B) is assigned a score according to the classically known Dynamic Programming based algorithm for Word Segmentation [7], hereby referred to as V iterbiWordSeg. V iterbiWordSeg takes a string as input and returns the best possible segmentation BestSeg (ordered collection of chunks) for that string. The score assigned to the segmentation by V iterbiWordSeg is the sum of log of probability scores of the segmented chunks based on the unigram language model.\nV iterbiWordSegScore(S) = \u2211\nsi\u2208BestSeg(S)\nlog(PUnigram(si)) (1)\nWe used Microsoft Web N-Gram Services2 for computing the unigram probability scores. The aforementioned corpus contains data from the web, and hence various acronyms and slang words occur in it. This holds critical importance in the context of our task. Next, for each triplet of the form (A, X, B), we compute the Sliding Window score as follows.\nScoreSlidingWindow(A,X,B) = V iterbiWordSegScore(A)+ constant \u2217 log10(UnigramProb(X)) \u2217WordLenProb(len(X))+ V iterbiWordSegScore(B)\n(2)\nwhere WordLenProb(x) is the Ordinate value at x in Figure 2 and the constant is set by experimentation.\nAlso, for each triplet (A, X, B), the final segmentation, Seg(A,X,B) is the ordered collection of chunks (BestSeg(A), X, BestSeg(B)), where BestSeg(A) and BestSeg(B) refer to the best segmentation (ordered collection of chunks) returned by V iterbiWordSeg(A) and V iterbiWordSeg(B) respectively.\n2 Microsoft Web N-Gram Services http://research.microsoft.com/en-us/ collaboration/focus/cs/web-ngram.aspx\nTo find the suitable value of MIN LEN and MAX LEN, we plot the percentage of frequency vs. word length graph using 50 million tweets3. Figure 2 shows the plot obtained.\nIt is observed that 79% of the tweet words are between length 2 to 6. Hence, we set MIN LEN and MAX LEN as 2 and 6 respectively.\nThe major benefit of this technique is that we are able to handle named entities and out of vocabulary (OOV) words. This is achieved by assigning score as a function of WordLenProb and smoothed backoff unigram probability (Equation 2) for words within the window.\nNow that we have a list of O(n2) segmentations and their corresponding ScoreSlidingWindow, we pick the top k segmentations for each hashtag on the basis of this score. We set k = 20, as precision at 20 (P@20) comes out to be around 98%. This establishes that the subset of segmentations we seed, which is of O(n2), indeed contains highly probable segmentations out of a total possible 2n\u22121 segmentations4."}, {"heading": "3.2 Feature Extraction and Entity Linking", "text": "This component of the system is responsible for two major tasks, feature extraction from each of the seeded segmentations, and entity linking on the segmentations. The features, as also shown in the System Diagram, are 1) Unigram Score, 2) Bigram Score, 3) Context Score, 4) Capitalisation Score, and 5) Relatedness Score. The first feature, Unigram Score, is essentially the V iterbiWordSegScore computed in the previous step. In the following sections, we describe the rest of the features.\nBigram Score: For each of the segmentations seeded by the Variable Length Sliding Window Technique, a bigram based score using the Microsoft Web NGram Services is computed. It is possible for a hashtag to have two perfectly valid segmentations. Consider the hashtag #Homesandgardens. Now this hashtag can be split as \u201cHomes and gardens\u201d which seems more probable to occur in a given context than \u201cHome sand gardens\u201d. Bigram based scoring helps to rank such segmentations, so that higher scores are awarded to the more semantically \u201cappealing\u201d segmentations. The bigram language model would score one of the above segmentations - \u201cHomes and gardens\u201d as\nP (Homes, and, gardens) \u2248 P (Homes| < s >) \u2217 P (and|Homes)\u2217 P (gardens|and) \u2217 P (< /s > |gardens)\n(3)\n3 The dataset is available at http://demeter.inf.ed.ac.uk/cross/docs/fsd_ corpus.tar.gz 4 For a string made up of n characters, we need to decide where to put the spaces so that we can get a sequence of valid words. There are n\u2212 1 positions where a space can be placed, and each position may or may not have a space. Hence there are 2n\u22121\nsegmentations.\nContext Score: Context based score is an important feature. This is responsible for bubbling up of the segmentations with maximum contextual similarity with the tweet content. Using the CMU TweetNLP toolkit [8], words having POS tags like verb, noun and adjective are extracted both from the candidate segmentation of the hashtag and the tweet context, i.e. the text of the tweet other than the hashtag. Next, Wu Palmer similarity from Wordnet [9] is used on these two sets of words to find how similar a candidate segmentation is to the tweet context. These scores are normalized from 0 to 1.\nCapitalisation Score: Hashtags are of varied nature. Some hashtags have a camelcase-like capitalisation pattern as in #HomesAndGardens, while others have everything in lowercase or uppercase characters like #homesandgardens. However, we can easily see that camelcase conveys more information as it helps segment the hashtag into \u201cHomes and gardens\u201d and not \u201cHome sAnd Gardens\u201d. Capitalisation score helps us to capture the information conveyed by capitalisation patterns within the hashtags. We use the following two rules. For a hashtag,\n\u2013 If a set of characters occuring together are in capitals as in #followUCBleague, they are considered to be a part of an \u201cassumed cluster\u201d (\u201cUCB\u201d in this case). \u2013 If it has a few capital letters separated by a group of lower case letters as in #SomethingGood, we assume the capital letters are delimiters and hence derive a few assumed clusters from the input hashtag.\nWe calculate the capitalisation score for a given segmentation S containing chunks s1, s2...si..sn as\nScoreCap = n\u2211 i=1 assumedClusterNotIntact(si) (4)\nwhere assumedClusterNotIntact(si) returns 1, if si fails to keep an assumed cluster intact, and 0 otherwise.\nRelatedness Score: Relatedness score measures the coherence between the tweet context and the hashtag segmentation. This score is computed on the basis of semantic relatedness between the entities present within the segmented hashtag and the tweet context.\nWe calculated the relatedness between all the possible mentions in the segmented hashtag (MH) to all other possible mentions in the tweet context (MT ). For computing relatedness between the two entities, we used the Wikipediabased relatedness function as proposed by Milne and Witten [4].\nRelatedness between two Wikipedia pages pa and pb is defined as follows:\nrel(pa, pb) = 1\u2212 \u03b4 (5)\nwhere,\n\u03b4 = log(max(|in(pa), in(pb)|))\u2212 log(|in(pa) \u2229 in(pb)|)\nlog(W )\u2212 log(min(|in(pa), in(pb)|)) (6)\nin(pa) is the set of Wikipedia pages pointing to page pa and W is the total number of pages in Wikipedia. The overall vote given to a candidate page pa for a given mention a by a mention b is defined as\nvoteb(pa) =\n\u2211 pb\u2208Pg(b) rel(pb, pa).P r(pb|b)\n|Pg(b)| (7)\nwhere Pg(b) are all possible candidate pages for the mention b and Pr(pb|b) is the prior probability of b linking to a page pb. The total relatedness score given to a candidate page pa for a given mention a is the sum of votes from all other mentions in the tweet context (MT ).\nrela(pa) = \u2211\nb\u2208MT\nvoteb(pa) (8)\nNow the overall relatedness score for a given hashtag segmentation, h is\nscoreh =\n\u2211 m\u2208MH relm(pa).P r(pa|m)\n|MH | (9)\nThe detected page pa for a given mention in the segmented hashtag is the Wikipedia page with the highest rela(pa). Since not all the entities are meaningful, we prune the entities with very low rela(pa) scores. In our case, the threshold is set to 0.1. This disambiguation function is considered as state-of-the-art and has also been adopted by various other systems [12][16]. The relatedness score, scoreh is used as a feature for hashtag segmentation. The entities in the segmented hashtag are returned along with the score for further improving the hashtag semantics."}, {"heading": "3.3 Segmentation Ranker", "text": "This component of the system is responsible for ranking the various probable segmentations seeded by the Hashtag Segmentations Seeder Module. We generated five features for each segmentation using Feature Extraction and Entity Linking Module in the previous step. These scores are combined by modelling the problem as a regression problem, and the combined score is referred to as ScoreRegression. The segmentations are ranked using ScoreRegression. In the end, the Segmentation Ranker outputs a ranked list of segmentations along with the entity linkings.\nIn the next section, we discuss the regression and training procedures in greater detail."}, {"heading": "4 Training Procedure", "text": "For the task of training the model, we consider the ScoreRegression of all correct segmentations to be 1 and all incorrect segmentations as 0. Our feature vector\ncomprises of five different scores calculated in Section 3.2. We use linear regression with elastic net regularisation [14]. This allows us to learn a model that is trained with both L1 and L2 prior as regularizer. It also helps us take care of the situation when some of the features might be correlated to one another. Here, \u03c1 controls the convex combination of L1 and L2.\nThe Objective Function we try to minimize is\nmin w\n1\n2nsamples ||Xw \u2212 y||22 + \u03b1\u03c1||w||1 + \u03b1(1\u2212 \u03c1) 2 ||w||22 (10)\nwhere X, y and w are Model Matrix, Response Vector, and Coefficient Matrix respectively. The parameters alpha(\u03b1) and rho(\u03c1) are set by cross validation."}, {"heading": "5 Experiments and Results", "text": "In this section we describe the datasets used for evaluation, and establish the effectiveness of our technique by comparing our results to a well known end-toend Entity Linking system, TAGME [12], which works on short texts, including tweets."}, {"heading": "5.1 Evaluation Metrics and Datasets", "text": "This section is divided into two parts. First, we explain the evaluation metrics in the context of our experiments, and later, we discuss the datasets used for evaluation.\nEvaluation Metrics We evaluated our system on two different metrics. Firstly, the system is evaluated based on its performance in the segmentation task. As the system returns a list of top-k hashtag segmentations for a given hashtag, we evaluated the precision at n (P@n) scores for the hashtag segmentation task. We also compared our P@1 score with Word Breaker5, which does the task of word segmentation. Secondly, the system is also evaluated on the basis of its entity linking performance on the hashtags. We computed Precision, Recall and F-Measure scores for the entities linked in the top ranked hashtag. For Entity Linking task, we used the same notions of Precision, Recall, and F-Measure as proposed by Marco et al. [10]. We compared our system with the state-of-the-art TAGME system.\nWe show that adding semantic information extracted from the hashtags leads to an improvement in the overall tweet entity linking. For this, we performed a comparative study on the output of the TAGME system when a tweet is given with un-segmented hashtag vs. when it is given with segmented and entity-linked hashtag6. The case when un-segmented hashtag is fed to TAGME is considered\n5 http://web-ngram.research.microsoft.com/info/break.html 6 For the segmented and entity-linked case, the linked entities in a hashtag were re-\nplaced with the corresponding Wikipedia page titles.\nas a baseline to show how much improvement can be attributed to our method of enriching the tweet with additional semantic information mined by segmenting and linking entities in a hashtag.\nDatasets The lack of availability of a public dataset that suits our task has been a major challenge. To the best of our knowledge, no publicly available dataset contains tweets along with hashtags, and the segmentation of hashtag into constituent entities appropriately linked to a Knowledge Base. So, we approached this problem from two angles - 1) Manually Annotated Dataset Generation (where dataset is made public), 2) Synthetically generated Dataset. The datasets are described in detail below.\n1. Microposts NEEL Dataset: The Microposts NEEL Dataset [15] contains over 3.5k tweets collected over a period from 15th July 2011 to 15th August 2011, and is rich in event-annotated tweets. This dataset contains Entities, and the corresponding linkages to DBPedia. The problem however, is that this dataset does not contain the segmentation of hashtags. We generate synthetic hashtags by taking tweets, and combining random number of consecutive words with each entity present within them. The remaining portion of the tweet that does not get combined is considered to be the tweet context. If no entity is present within the tweet, random words are combined to form the hashtag. This solves the problem of requiring human intervention to segment and link hashtags, since now we already know the segmentation as well as the entities present within the hashtag.\n7 \u201cTAGME (Baseline)\u201d refers to the baseline evaluation where we give an unsegmented hashtag to TAGME to annotate. \u201cOur System + TAGME\u201d refers to the evaluation, where we first do segmentation and entity linking on hashtags using our system, and then feed them to TAGME to annotate either just the hashtag (Table a) or the full tweet (Table b). This is also discussed under \u201cEvaluation Metrics\u201din subsection 5.1.\nOur system achieved an accuracy (P@1) of 91.4% in segmenting the hashtag correctly. The accuracy of Word Breaker in this case was 80.2%. This, however, can be attributed to a major difference between our system and Word Breaker. Word Breaker is not context aware. It just takes an unspaced string, and tries to break it into words. Our method takes into account the relatedness between the entities in a hashtag and the rest of the tweet content. Also, various other hashtag specific features like Capitalisation Score play an important part in improving the accuracy.\nThe comparative results of Entity Linking (in hashtags and overall), as well as P@n at various values of n for segmentation task are contained in Table 1. All the values are calculated by k-fold Cross-validation with k=5.\n2. Manually Annotated Stanford Sentiment Analysis Dataset: To overcome the limitation that a synthetically generated hashtag might not actually be equivalent to a real world hashtag, we sampled around 1.2k tweets randomly from the Stanford Sentiment Analysis Dataset8, all of which contained one or more hashtags in them. After this, we generated around 20 possible segmentations for each hashtag by passing the hashtag and tweet from Segmentations Seeder Module. In the end we had around 21k rows which were given to 3 human annotators to annotate as 0 or 1 depending on whether or not a given segmentation is correct (for a given hashtag) according to their judgement.\nDetermining the \u201ccorrect\u201d segmentation for a given hashtag is particularly challenging, as there may be many answers that are equally plausible. It has been long established that there exist style disagreements among various editorial content (\u201cHomepage\u201d vs \u201cHome page\u201d). There are also various new words that come into existence like \u201cTweetDeck\u201d which are brand or product names. So,\n8 http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\nour annotation guidelines in case of Stanford Sentiment Analysis Dataset allow for annotators to mark multiple segmentations as correct.\nThe rows were labelled 0, if at least 2 annotators out of 3 agreed on the label 0, similarly the rows were labelled 1, if at least 2 out of 3 annotators agreed on the label 1. The labels are essentially ScoreRegression as described in Section 4. The value of Fleiss\u2019 Kappa (\u03ba), which is a measure of inter annotator agreement, comes out to be 0.89, showing a good agreement between annotators. This dataset is made public to ease future research in this area9.\nOur system achieved a precision (P@1) of 87.3% in segmenting the hashtags correctly. The P@1 score of Word Breaker in this case was 78.9%. The difference in performance can again be attributed to same reasons as in the case of NEEL Dataset.\nThe comparative results of Entity Linking (in hashtags and overall), as well as P@n at various values of n for the task of segmentation are contained in Table 2. All the values are calculated by k-fold Cross-validation with k=5.\nResults We demonstrate the effectiveness of our technique by evaluating on two different datasets. We also show how overall Entity Linking in tweets was improved, when our system was used to segment the hashtag and link the entities in the hashtag. We achieved an improvement of 36.1% F-Measure in extracting semantics from hashtags over the baseline in case of NEEL Dataset. We further show that extracting semantics led to overall increase in Entity Linking of tweet. In case of NEEL Dataset, we achieved an improvement of 15.3% F-Measure over baseline in overall tweet Entity Linking task as can be seen in Table 1. Similar results were obtained for the Annotated Stanford Sentiment Analysis Dataset as well, as shown in Table 2. Further, we measured the effectiveness of each feature in ranking the hashtag segmentations. The results are summarized in Table 3."}, {"heading": "6 Conclusions", "text": "We have presented a context aware method to segment a hashtag, and link its constituent entities to a Knowledge Base (KB). An ensemble of various syntactic, as well as semantic features is used to learn a regression model that returns a ranked list of probable segmentations. This allows us to handle cases where\n9 Dataset: http://bit.ly/HashtagData\nmultiple segmentations are acceptable (due to lack of context in cases, where tweets are extremely short) for the same hashtag, e.g. #Homesandgardens.\nThe proposed method of extracting more semantic information from hashtags can be beneficial to numerous tasks including, but not limited to sentiment analysis, improving search on social networks and microblogs, topic detection etc."}], "references": [{"title": "How People use Twitter in Different Languages", "author": ["Wouter Weerkamp", "Simon Carter", "Manos Tsagkias"], "venue": "In Proceedings of Web Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Web scale NLP: a case study on url word breaking", "author": ["Kuansan Wang", "Christopher Thrasher", "Bo-June Paul Hsu"], "venue": "In Proceedings of the 20th international conference on World Wide Web,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Chinese word segmentation: A decade review", "author": ["Huang", "Changning", "Hai Zhao"], "venue": "In Journal of Chinese Information Processing", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "An effective, low-cost measure of semantic relatedness obtained from wikipedia links", "author": ["D. Milne", "I.H. Witten"], "venue": "In Proceedings of AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Fast webpage classification using URL features", "author": ["Kan", "Min-Yen", "Hoang Oanh Nguyen Thi"], "venue": "In Proceedings of the 14th ACM International Conference on Information and Knowledge Management ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Segmenting web-domains and hashtags using length specific models", "author": ["Srinivasan", "Sriram", "Sourangshu Bhattacharya", "Rudrasis Chakraborty"], "venue": "In Proceedings of the 21st ACM International Conference on Information and Knowledge Management ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Articial Intelligence: A Modern Approach", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2", "author": ["Gimpel", "Kevin"], "venue": "Association for Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "WordNet: A Lexical Database for English", "author": ["George A. Miller"], "venue": "In Communications of the ACM Vol. 38,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "A framework for benchmarking entity-annotation systems", "author": ["Cornolti", "Marco", "Paolo Ferragina", "Massimiliano Ciaramita"], "venue": "In Proceedings of the 22nd International Conference on World Wide Web.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Adding Semantics to Microblog Posts", "author": ["Edgar Meij", "Wouter Weerkamp", "Maarten de Rijke"], "venue": "In Proceedings of the 5th ACM International Conference on Web Search and Data Mining,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Tagme: on-the-fly annotation of short text fragments (by Wikipedia entities)", "author": ["P. Ferragina", "U. Scaiella"], "venue": "In Proceedings of 19th ACM Conference on Knowledge Management,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "EDIUM: Improving Entity Disambiguation via User Modeling", "author": ["Romil Bansal", "Sandeep Panem", "Manish Gupta", "Vasudeva Varma"], "venue": "In Proceedings of the 36th European Conference on Information Retrieval,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Regularization and variable selection via the elastic net", "author": ["Hui Zou", "Trevor Hastie"], "venue": "In Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Making Sense of Microposts (#Microposts2014) Named Entity Extraction & Linking Challenge", "author": ["Amparo Elizabeth Cano Basave", "Giuseppe Rizzo", "Andrea Varga", "Matthew Rowe", "Milan Stankovic", "Aba-Sah Dadzie"], "venue": "In 4th Workshop on Making Sense of Microposts (#Microposts2014),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Collective entity linking in web text: a graph-based method", "author": ["X. Han", "L. Sun", "J. Zhao"], "venue": "In Proceedings of 34th International ACM SIGIR Conference on Research and Development in Information Retrieval ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "4 hashtags per tweet [1].", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "[3] showed that character based tagging approach outperforms other word based segmentation approaches for Chinese word segmentation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "English URL segmentation has also been explored by various researchers in the past [2][5][6].", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "English URL segmentation has also been explored by various researchers in the past [2][5][6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "English URL segmentation has also been explored by various researchers in the past [2][5][6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 10, "context": "Various features like commonness, relatedness, popularity and recentness have been used for detecting and linking the entities in the microposts [11][12][13].", "startOffset": 145, "endOffset": 149}, {"referenceID": 11, "context": "Various features like commonness, relatedness, popularity and recentness have been used for detecting and linking the entities in the microposts [11][12][13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 12, "context": "Various features like commonness, relatedness, popularity and recentness have been used for detecting and linking the entities in the microposts [11][12][13].", "startOffset": 153, "endOffset": 157}, {"referenceID": 6, "context": "Each segment A and B of the triplet (A, X, B) is assigned a score according to the classically known Dynamic Programming based algorithm for Word Segmentation [7], hereby referred to as V iterbiWordSeg.", "startOffset": 159, "endOffset": 162}, {"referenceID": 7, "context": "Using the CMU TweetNLP toolkit [8], words having POS tags like verb, noun and adjective are extracted both from the candidate segmentation of the hashtag and the tweet context, i.", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "Next, Wu Palmer similarity from Wordnet [9] is used on these two sets of words to find how similar a candidate segmentation is to the tweet context.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "For computing relatedness between the two entities, we used the Wikipediabased relatedness function as proposed by Milne and Witten [4].", "startOffset": 132, "endOffset": 135}, {"referenceID": 11, "context": "This disambiguation function is considered as state-of-the-art and has also been adopted by various other systems [12][16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "This disambiguation function is considered as state-of-the-art and has also been adopted by various other systems [12][16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 13, "context": "We use linear regression with elastic net regularisation [14].", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "In this section we describe the datasets used for evaluation, and establish the effectiveness of our technique by comparing our results to a well known end-toend Entity Linking system, TAGME [12], which works on short texts, including tweets.", "startOffset": 191, "endOffset": 195}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Microposts NEEL Dataset: The Microposts NEEL Dataset [15] contains over 3.", "startOffset": 53, "endOffset": 57}], "year": 2015, "abstractText": "Hashtags are semantico-syntactic constructs used across various social networking and microblogging platforms to enable users to start a topic specific discussion or classify a post into a desired category. Segmenting and linking the entities present within the hashtags could therefore help in better understanding and extraction of information shared across the social media. However, due to lack of space delimiters in the hashtags (e.g #nsavssnowden), the segmentation of hashtags into constituent entities (\u201cNSA\u201d and \u201cEdward Snowden\u201d in this case) is not a trivial task. Most of the current state-of-the-art social media analytics systems like Sentiment Analysis and Entity Linking tend to either ignore hashtags, or treat them as a single word. In this paper, we present a context aware approach to segment and link entities in the hashtags to a knowledge base (KB) entry, based on the context within the tweet. Our approach segments and links the entities in hashtags such that the coherence between hashtag semantics and the tweet is maximized. To the best of our knowledge, no existing study addresses the issue of linking entities in hashtags for extracting semantic information. We evaluate our method on two different datasets, and demonstrate the effectiveness of our technique in improving the overall entity linking in tweets via additional semantic information provided by segmenting and linking entities in a hashtag.", "creator": "LaTeX with hyperref package"}}}