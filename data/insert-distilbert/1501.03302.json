{"id": "1501.03302", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2015", "title": "Hard to Cheat: A Turing Test based on Answering Questions about Images", "abstract": "progress in language and image understanding by machines ultimately has sparkled the interest of the research community in more open - value ended, holistic tasks, and refueled an old ai dream of building intelligent machines. we then discuss a few prominent challenges that characterize such holistic tasks and argue for \" question not answering about images \" as a particular appealing instance of such a fairly holistic reference task. others in particular, we point out that it is a version of a turing test that is likely to be more robust to over - interpretations and contrast it with tasks like grounding and generation of descriptions. finally, we discuss tools that to measure gradual progress in approaching this field.", "histories": [["v1", "Wed, 14 Jan 2015 10:38:43 GMT  (168kb)", "https://arxiv.org/abs/1501.03302v1", "Also presented on: AAAI-15 Workshop: Beyond Turing the Turing Test"], ["v2", "Thu, 15 Jan 2015 10:18:54 GMT  (168kb)", "http://arxiv.org/abs/1501.03302v2", "Presented in AAAI-15 Workshop: Beyond the Turing Test"]], "COMMENTS": "Also presented on: AAAI-15 Workshop: Beyond Turing the Turing Test", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.CV cs.LG", "authors": ["mateusz malinowski", "mario fritz"], "accepted": false, "id": "1501.03302"}, "pdf": {"name": "1501.03302.pdf", "metadata": {"source": "CRF", "title": "Hard to Cheat: A Turing Test based on Answering Questions about Images", "authors": ["Mateusz Malinowski"], "emails": ["mmalinow@mpi-inf.mpg.de", "mfritz@mpi-inf.mpg.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n03 30\n2v 2\n[ cs\n.A I]\n1 5\nJa n\n20 15"}, {"heading": "Introduction", "text": "Progress in machine perception and language understanding (e.g. (Krizhevsky et al., 2012; Liang et al., 2013)) has inspired researchers to work on holistic tasks that interlink both modalities together in a complex chain of perception, representation and inference. Examples include: grounding (Krishnamurthy and Kollar, 2013), language generation (Karpathy and Fei-Fei, 2014; Donahue et al., 2014), retrieval (Karpathy et al., 2014; Malinowski and Fritz, 2014b), and question answering about images (Malinowski and Fritz, 2014a,c).\nRecently, Malinowski and Fritz (2014a) have presented an approach for question answering about images that resembles the famous Turing Test (Turing, 1950), while Malinowski and Fritz (2014c) further discuss some of the associated challenges and issues. In the following, we elaborate on data acquisition, contrast this challenge with other tasks including grounding, language generation, as well as highlight properties like robustness to over-interpretation, which makes it hard to cheat such a test."}, {"heading": "Challenges", "text": "Architectures working on a holistic task such as question answering based on images need to deal with a large gamut of challenges. In this section, we have distilled a few prominent ones that require a joint reasoning over language and visual inputs. We also argue that holistic architectures can benefit from a common sense knowledge. Finally, we discuss challenges in data acquisition and show how the task differs from other well known tasks.\nVision and language Scalability: Vision and language systems ground any internal representation in an external world that serves as a common reference point for machines and humans. The human conceptualization divides these percepts into different instances, categories as well as spatio-temporal concepts. Architectures that aim at reproducing this space of human concepts need to capture the same diversity and therefore scale up to thousands of concepts. Concept ambiguity: As the number of categories grows, the semantic boundaries become more fuzzy, and hence ambiguities and gradual memberships are inherently introduced. For instance, difference between \u2019night stand\u2019 and \u2019cabinet\u2019, or \u2019armchair\u2019, \u2019chair\u2019 and \u2019sofa\u2019 can be blurry. Such ambiguities are challenging in at least two ways. Methods need to distinguish fine-grained differences between these objects when appropriate. Objective functions and evaluating metrics need to gradually penalize the methods for their mistakes. Ambiguity in reference resolution: The quality of an answer depends on how ambiguous and latent notions of reference frames and intentions are understood (Malinowski and Fritz, 2014a). Depending on the cultural bias and the context, we may use object-centric or observer-centric or world-centric frames of reference (Levinson, 2003). Moreover, it is no unified notion what \u2019with\u2019, \u2019beneath\u2019, \u2019over\u2019 mean.\nCommon sense knowledge Interestingly, some questions can be quite reliably answered with access to common sense knowledge. For instance \u201dWhich object on the table is used for cutting?\u201d already narrows down the likely options significantly. Such example suggests that question-answering architectures would significantly benefit from common sense knowledge.\nAn \u2019object for cutting\u2019 is not directly visual but about the affordance of the object and therefore a challenging concept to acquire from images only. On the other hand, cooccurrences in visual data can represent a kind of visual common sense knowledge of very mundane facts or probabilistic relations that are rarely found in common sense knowledge bases.\nAnnotations We argue that despite the aforementioned challenges, \u201cquestion answering about images\u201d has unique\nadvantages over other tasks in terms of data acquisition and task evaluation. In contrast to grounding (Krishnamurthy and Kollar, 2013), annotating images with question and answer pairs does not require a detailed annotations of whole scenes in terms of predicates representing objects and their relations. The task is also agnostic to the internal representation of a method. In contrast to language generation (Karpathy and Fei-Fei, 2014; Donahue et al., 2014), the output space of a question answering task is more restricted and hence evaluation of different architectures on the task is easier to formulate. In contrast to typical computer vision tasks like object detection (Everingham et al., 2010), architectures are judged solely on right answers, not an internal representation. In contrast to the traditional Turing Test (Turing, 1950), \u201canswering questions about images\u201d is less prone to over-interpretations via associating a meaning to machine answers by the human interrogator. Hence, a method can be forced to answer to the point rather than \u201ccheating\u201d by giving generic answers or output that is open to interpretations."}, {"heading": "Evaluation of architectures", "text": "Measuring progress on holistic tasks require identifying its goals. For instance a suitable metric for \u201cquestion answering about images\u201d should evaluate architectures based on produced answers but not on intermediate results such as detections or logical forms. For a Visual Turing Challenge, we seek a metric that satisfies several properties. The most important are: Automation: Evaluating answers on such complex tasks as answering on questions requires a quite deep understanding of natural language, involved concepts and hidden intentions of the questioner. The ideal but impractical metric would be to manually judge every single answer of every architecture individually. Therefore, we are seeking an automatic approximation so that we can evaluate different holistic architectures at scale. Malinowski and Fritz (2014a) proposed to restrict the answer space in order to achieve this goal, while leaving the questions unconstraint. Social consensus: The complex tasks that we are interested in are inherently ambiguous. The ambiguities stem from many factors such as cultural bias, different frame of reference and fined grained categorization. This implies that multiple interpretations of a question are possible. To deal with different interpretations of words, Malinowski and Fritz (2014a) define a WUPS scores using lexical databases (Miller, 1995) with Wu-Palmer similarity (Wu and Palmer, 1994). To deal with different interpretations of a question, Malinowski and Fritz (2014c) suggest that the quality of answers should be measured according to the social consensus where the answers are evaluated against multiple groundtruths. Interestingly, such metric also naturally quantifies social agreement of the answer, and serve as a practical approximation of tedious manual evaluation. Experimental scenarios In many cases, success on challenging learning problems has been accelerated by use of external data in the training. We believe that a Visual Turing challenge should consists of a sub-task with a prohibited use of auxiliary data to understand how the holistic learners generalize from limited and challenging data in a more\nestablished setup. On the other hand, we should not limit ourselves to such artificial restrictions in building the next generation of the holistic learners. Therefore open sub-tasks with a permissible use of additional sources in the training have to be stated, including: additional vision and language resources, synthetic data and curated data."}, {"heading": "Summary", "text": "The goal of this contribution is to sparkle discussions about challenges and benchmarking architectures on holistic tasks. We also argue that \u201cquestion answering about images\u201d is a holistic task that offers multiple advantages over related tasks. For example, it is likely to be less prone to \u201ccheating\u201d by over-interpretations than a traditional Turing Test, the annotation process is tractable by crowdsourcing question and answer pairs, and the task does not artificially force any internal representation on the methods. Our most recent efforts and results on establishing a Visual Turing Test can be found on our website: www.d2.mpi-inf.mpg.de/visual-turing-challenge."}], "references": [{"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2014}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": null, "citeRegEx": "Everingham et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Everingham et al\\.", "year": 2010}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": null, "citeRegEx": "Karpathy and Fei.Fei,? \\Q2014\\E", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Jointly learning to parse and perceive: Connecting natural language to the physical world. TACL", "author": ["J. Krishnamurthy", "T. Kollar"], "venue": null, "citeRegEx": "Krishnamurthy and Kollar,? \\Q2013\\E", "shortCiteRegEx": "Krishnamurthy and Kollar", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Space in language and cognition: Explorations in cognitive diversity, volume 5", "author": ["S.C. Levinson"], "venue": null, "citeRegEx": "Levinson,? \\Q2003\\E", "shortCiteRegEx": "Levinson", "year": 2003}, {"title": "Learning dependency-based compositional semantics", "author": ["P. Liang", "M.I. Jordan", "D. Klein"], "venue": "Computational Linguistics", "citeRegEx": "Liang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": null, "citeRegEx": "Malinowski and Fritz,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz", "year": 2014}, {"title": "A pooling approach to modelling spatial relations for image retrieval and annotation", "author": ["M. Malinowski", "M. Fritz"], "venue": null, "citeRegEx": "Malinowski and Fritz,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz", "year": 2014}, {"title": "Towards a visual turing challenge", "author": ["M. Malinowski", "M. Fritz"], "venue": "In Learning Semantics (NIPS workshop)", "citeRegEx": "Malinowski and Fritz,? \\Q2014\\E", "shortCiteRegEx": "Malinowski and Fritz", "year": 2014}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": null, "citeRegEx": "Miller,? \\Q1995\\E", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "Computing machinery and intelligence. Mind, pages 433\u2013460", "author": ["A.M. Turing"], "venue": null, "citeRegEx": "Turing,? \\Q1950\\E", "shortCiteRegEx": "Turing", "year": 1950}, {"title": "Verbs semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": null, "citeRegEx": "Wu and Palmer,? \\Q1994\\E", "shortCiteRegEx": "Wu and Palmer", "year": 1994}], "referenceMentions": [{"referenceID": 5, "context": "(Krizhevsky et al., 2012; Liang et al., 2013)) has inspired researchers to work on holistic tasks that interlink both modalities together in a complex chain of perception, representation and inference.", "startOffset": 0, "endOffset": 45}, {"referenceID": 7, "context": "(Krizhevsky et al., 2012; Liang et al., 2013)) has inspired researchers to work on holistic tasks that interlink both modalities together in a complex chain of perception, representation and inference.", "startOffset": 0, "endOffset": 45}, {"referenceID": 4, "context": "Examples include: grounding (Krishnamurthy and Kollar, 2013), language generation (Karpathy and Fei-Fei, 2014; Donahue et al.", "startOffset": 28, "endOffset": 60}, {"referenceID": 2, "context": "Examples include: grounding (Krishnamurthy and Kollar, 2013), language generation (Karpathy and Fei-Fei, 2014; Donahue et al., 2014), retrieval (Karpathy et al.", "startOffset": 82, "endOffset": 132}, {"referenceID": 0, "context": "Examples include: grounding (Krishnamurthy and Kollar, 2013), language generation (Karpathy and Fei-Fei, 2014; Donahue et al., 2014), retrieval (Karpathy et al.", "startOffset": 82, "endOffset": 132}, {"referenceID": 3, "context": ", 2014), retrieval (Karpathy et al., 2014; Malinowski and Fritz, 2014b), and question answering about images (Malinowski and Fritz, 2014a,c).", "startOffset": 19, "endOffset": 71}, {"referenceID": 12, "context": "Recently, Malinowski and Fritz (2014a) have presented an approach for question answering about images that resembles the famous Turing Test (Turing, 1950), while Malinowski and Fritz (2014c) further discuss some of the associated challenges and issues.", "startOffset": 140, "endOffset": 154}, {"referenceID": 0, "context": "Examples include: grounding (Krishnamurthy and Kollar, 2013), language generation (Karpathy and Fei-Fei, 2014; Donahue et al., 2014), retrieval (Karpathy et al., 2014; Malinowski and Fritz, 2014b), and question answering about images (Malinowski and Fritz, 2014a,c). Recently, Malinowski and Fritz (2014a) have presented an approach for question answering about images that resembles the famous Turing Test (Turing, 1950), while Malinowski and Fritz (2014c) further discuss some of the associated challenges and issues.", "startOffset": 111, "endOffset": 306}, {"referenceID": 0, "context": "Examples include: grounding (Krishnamurthy and Kollar, 2013), language generation (Karpathy and Fei-Fei, 2014; Donahue et al., 2014), retrieval (Karpathy et al., 2014; Malinowski and Fritz, 2014b), and question answering about images (Malinowski and Fritz, 2014a,c). Recently, Malinowski and Fritz (2014a) have presented an approach for question answering about images that resembles the famous Turing Test (Turing, 1950), while Malinowski and Fritz (2014c) further discuss some of the associated challenges and issues.", "startOffset": 111, "endOffset": 458}, {"referenceID": 6, "context": "Depending on the cultural bias and the context, we may use object-centric or observer-centric or world-centric frames of reference (Levinson, 2003).", "startOffset": 131, "endOffset": 147}, {"referenceID": 4, "context": "In contrast to grounding (Krishnamurthy and Kollar, 2013), annotating images with question and answer pairs does not require a detailed annotations of whole scenes in terms of predicates representing objects and their relations.", "startOffset": 25, "endOffset": 57}, {"referenceID": 2, "context": "In contrast to language generation (Karpathy and Fei-Fei, 2014; Donahue et al., 2014), the output space of a question answering task is more restricted and hence evaluation of different architectures on the task is easier to formulate.", "startOffset": 35, "endOffset": 85}, {"referenceID": 0, "context": "In contrast to language generation (Karpathy and Fei-Fei, 2014; Donahue et al., 2014), the output space of a question answering task is more restricted and hence evaluation of different architectures on the task is easier to formulate.", "startOffset": 35, "endOffset": 85}, {"referenceID": 1, "context": "In contrast to typical computer vision tasks like object detection (Everingham et al., 2010), architectures are judged solely on right answers, not an internal representation.", "startOffset": 67, "endOffset": 92}, {"referenceID": 12, "context": "In contrast to the traditional Turing Test (Turing, 1950), \u201canswering questions about images\u201d is less prone to over-interpretations via associating a meaning to machine answers by the human interrogator.", "startOffset": 43, "endOffset": 57}, {"referenceID": 11, "context": "To deal with different interpretations of words, Malinowski and Fritz (2014a) define a WUPS scores using lexical databases (Miller, 1995) with Wu-Palmer similarity (Wu and Palmer, 1994).", "startOffset": 123, "endOffset": 137}, {"referenceID": 13, "context": "To deal with different interpretations of words, Malinowski and Fritz (2014a) define a WUPS scores using lexical databases (Miller, 1995) with Wu-Palmer similarity (Wu and Palmer, 1994).", "startOffset": 164, "endOffset": 185}, {"referenceID": 8, "context": "Malinowski and Fritz (2014a) proposed to restrict the answer space in order to achieve this goal, while leaving the questions unconstraint.", "startOffset": 0, "endOffset": 29}, {"referenceID": 8, "context": "Malinowski and Fritz (2014a) proposed to restrict the answer space in order to achieve this goal, while leaving the questions unconstraint. Social consensus: The complex tasks that we are interested in are inherently ambiguous. The ambiguities stem from many factors such as cultural bias, different frame of reference and fined grained categorization. This implies that multiple interpretations of a question are possible. To deal with different interpretations of words, Malinowski and Fritz (2014a) define a WUPS scores using lexical databases (Miller, 1995) with Wu-Palmer similarity (Wu and Palmer, 1994).", "startOffset": 0, "endOffset": 502}, {"referenceID": 8, "context": "Malinowski and Fritz (2014a) proposed to restrict the answer space in order to achieve this goal, while leaving the questions unconstraint. Social consensus: The complex tasks that we are interested in are inherently ambiguous. The ambiguities stem from many factors such as cultural bias, different frame of reference and fined grained categorization. This implies that multiple interpretations of a question are possible. To deal with different interpretations of words, Malinowski and Fritz (2014a) define a WUPS scores using lexical databases (Miller, 1995) with Wu-Palmer similarity (Wu and Palmer, 1994). To deal with different interpretations of a question, Malinowski and Fritz (2014c) suggest that the quality of answers should be measured according to the social consensus where the answers are evaluated against multiple groundtruths.", "startOffset": 0, "endOffset": 694}], "year": 2015, "abstractText": "Progress in language and image understanding by machines has sparkled the interest of the research community in more open-ended, holistic tasks, and refueled an old AI dream of building intelligent machines. We discuss a few prominent challenges that characterize such holistic tasks and argue for \u201cquestion answering about images\u201d as a particular appealing instance of such a holistic task. In particular, we point out that it is a version of a Turing Test that is likely to be more robust to over-interpretations and contrast it with tasks like grounding and generation of descriptions. Finally, we discuss tools to measure progress in this field.", "creator": "LaTeX with hyperref package"}}}