{"id": "1501.03784", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2015", "title": "Holographic Graph Neuron: a Bio-Inspired Architecture for Pattern Processing", "abstract": "this article proposes the use that of vector 2d symbolic architectures for implementing hierarchical graph neuron, an architecture for memorizing patterns of generic sensor stimuli. the adoption of a vector symbolic representation ensures a one - layered design for the specific approach, while maintaining easily the previously reported properties and performance adjustment characteristics of traditional hierarchical directed graph neuron, and also improving the noise resistance of addressing the architecture. the proposed architecture configuration enables a linear ( with respect to the number of stored entries ) time search for an arbitrary sub - pattern.", "histories": [["v1", "Thu, 15 Jan 2015 19:25:32 GMT  (606kb,D)", "http://arxiv.org/abs/1501.03784v1", "9 pages, 13 figures"]], "COMMENTS": "9 pages, 13 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["denis kleyko", "evgeny osipov", "alexander senior", "asad i khan", "y ahmet \\c{s}ekercio\\u{g}lu"], "accepted": false, "id": "1501.03784"}, "pdf": {"name": "1501.03784.pdf", "metadata": {"source": "CRF", "title": "Holographic Graph Neuron: a Bio-Inspired Architecture for Pattern Processing", "authors": ["Denis Kleyko", "Evgeny Osipov", "Alexander Senior", "Asad I. Khan", "Ahmet \u015eekercio\u011flu"], "emails": ["Evgeny.Osipov@ltu.se;", "Denis.Kleyko@ltu.se).", "Asad.Khan@monash.edu).", "Alexander.Senior@monash.edu;", "met.Sekercioglu@monash.edu)."], "sections": [{"heading": null, "text": "Index Terms\u2014Holographic Graph Neuron, Pattern recognition, Vector Symbolic Architecture, Associative memory, hyperdimensional computing.\nI. INTRODUCTION\nGRAPH Neuron (GN) is an approach for memorizingpatterns of generic sensor stimuli for later template matching. It is based on the hypothesis that a better associative memory resource can be created by changing the emphasis from high speed sequential CPU processing to parallel network-centric processing [2], [3]. In contrast to contemporary machine learning approaches, GN allows introduction of new patterns in the learning set without the need for retraining. Whilst doing so, it exhibits a high level of scalability i.e. its performance and accuracy do not degrade as the number of stored patterns increases over time.\nVector Symbolic Architectures (VSA) [4] are a bio-inspired method of representing concepts and their meaning for modeling cognitive reasoning. It exhibits a set of unique properties which make it suitable for implementation of artificial general intelligence [5], [6], [7], and so, creation of complex systems for sensing and pattern recognition without reliance on complex computation. In the biological world, extremely successful applications of these approaches can be found. One example is the ordinary house fly. A house fly is capable of conducting very complex maneuvers albeit it possesses very\nThis work was initially presented at IEEE International Conference on Computer and Information Sciences, ICCOINS 2014 [1].\nD. Kleyko and E. Osipov are with the Department of Computer Science Electrical and Space Engineering, Lule\u00e5 University of Technology, 971 87 Lule\u00e5, Sweden (e-mail: Evgeny.Osipov@ltu.se; Denis.Kleyko@ltu.se).\nA.I. Khan is with Clayton School of Information Technology, Monash University, Wellington Rd, Clayton, Victoria 3800, Australia (e-mail: Asad.Khan@monash.edu).\nA. Senior and Y. A. S\u0327ekerciog\u0306lu are with the Department of Electrical and Computer Systems Engineering, Monash University, Clayton, Victoria 3800, Australia (e-mail: Alexander.Senior@monash.edu; Ahmet.Sekercioglu@monash.edu).\nlittle computational capacity1. Another interesting biological example is the compound eyes of arthropods. These compound eyes consist of large number of sensors with limited and localized processing capabilities for performing relatively complex sensing tasks [9].\nThis article presents contributions in two domains: organization of associative memory and properties of connectionist distributed representation. In the first area the article introduces a novel bio-inspired architecture, called Holographic Graph Neuron (HoloGN), for one-shot pattern learning, which is build upon Graph Neuron\u2019s flexible input encoding abstraction and strong reasoning capabilities of the VSA representation. In the second area this article extends understanding of the performance proprieties of distributed representation, which opens a way for new applications.\nThe article is structured as follows. Section II presents an overview of the work related to the matter presented in this article. The background information on the theories, concepts and approaches used in HoloGN is described in Section III. Sections IV through VI present the main contribution of this article \u2013 the design of the HoloGN architecture and its performance characteristics. The conclusions are presented in Section VII."}, {"heading": "II. RELATED WORK", "text": "Associative memory (AM) is a sub-domain of artificial neural networks, which utilises the benefits of content-addressable memory (CAM) [10] in microcomputers. The AM concept was originally developed in an effort to utilise the power and speed of existing computer systems for solving large-scale and computationally intensive problems by simulating biological neurosystems.\nThe Hierarchical Graph Neuron (HGN) approach [2] is a type of associative memory which signifies the hierarchical structure in its implementation. Hierarchical structures in associative memory models are of interest as these have been shown to improve the rate of recall in pattern recognition applications. The distributed HGN scheme also allows for better control of the network resources. This scheme compares well with contemporary approaches such as Self-Organizing Map and Support Vector Machine in terms of speed and accuracy.\n1In [8], a house fly\u2019s properties are compared and contrasted with an advanced fighter plane as follows: \u201cWhereas the F-35 Joint Strike Fighter, the most advanced fighter plane in the world, takes a few measurements - airspeed, rate of climb, rotations, and so on and then plugs them into complex equations, which it must solve in real time, the fly relies on many measurements from a variety of sensors but does relatively little computation.\u201d\nar X\niv :1\n50 1.\n03 78\n4v 1\n[ cs\n.A I]\n1 5\nJa n\n20 15\n2 Vector Symbolic Architectures [5] are a class of connectionist models that use hyper-dimensional vectors (i.e. vectors of several thousand elements) to encode structured information as distributed or holographic representation. In this technique structured data is represented by performing basic arithmetic operations on field-value tuples. Distributed representations of data structures are an approach actively used in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [11].\nIn [12] a VSA-based knowledge-representation architecture is proposed for learning arbitrarily complex, hierarchical, symbolic relationships (patterns) between sensors and actuators in robotics. Recently the theory of hyper-dimensional computing, and VSA in particular, were adopted for implementing novel communication protocols and architectures for collective communications in machine-to-machine communication scenarios [13], [14]. The first work demonstrates unique reliability and timing properties essential in the context of industrial machineto-machine communications. The latter work shows the feasibility of implementing collective communications using current radio technology. This article presents an algorithmic ground for further design of the distributed HoloGN on top of the architecture presented in [13]."}, {"heading": "III. OVERVIEW OF ESSENTIAL PARTS OF RELEVANT CONCEPTS AND THEORIES", "text": ""}, {"heading": "A. Hierarchical Graph Neuron", "text": "Figure 1 illustrates the Hierarchical Graph Neuron approach. Consider only the bottom layer of the construction without the hierarchy of upper nodes; this bottom level is the original flat network of Graph Neurons [2]. Each GN is a model for a set of generic sensory values. When seen as a network, graph neurons can be modeled by an array where columns are individual GNs and rows are possible symbols, which a neuron can recognize. For example, if there are only two possible symbols, say \u201cX\u201d and \u201cY\u201d, in the alphabet of a pattern, then only two rows are needed to represent those symbols. The number of columns2 in the GN array determines the size of patterns, which it can analyse.\nAn input pattern is defined as a stimulus produced within the network. In Figure 1, each GN can analyse a symbol (\u201cX\u201d or \u201cY\u201d) of a pattern comprising of five elements. In each GN (column) only the element with matching value (a row ID) would respond. For example, if the pattern is \u201cYXYYX\u201d, then in the second column \u201cX\u201d element will be activated as response to this input stimulus. If a particular element in the GN-column is activated it sends a report to all adjacent GNs. The report contains the activated GN\u2019s element ID (the row index). Otherwise, it simply ignores the stimulus and returns to the idle state.\nDuring the next phase, all GNs communicate the indices of the activated elements with the adjacent columns at their level, and additionally communicate the stored bias information to\n2In this articles words \u201ccolumn\u201d and GN are used interchangeably and refer to a single Graph Neuron. Term \u201cGN array\u201d refers to several GNs used to recognize a pattern of several elements, where one neuron is used to recognize one element of the pattern.\nFig. 1. Hierarchical GNs with five elements\u2019 pattern of two symbols. The short arrows in the figure show how GNs communicate indices of the activated GN-elements with their neighbours creating logical connectivity. Null GN numbers are assigned at the start and at the end of the array for maintaining a consistent reporting scheme where every activated GN reports to both the adjacent GNs.\nthe layer above. The procedure continues in an ascending manner until the collective bias information reaches the top of the hierarchy. The higher level GNs can thus provide a more authoritative assessment of the input pattern. The accuracy of HGN was demonstrated to be comparable to the accuracy of Neural Network with back-propagation [2]."}, {"heading": "B. Fundamentals of Vector Symbolic Architecture and Binary Spatter Codes", "text": "Vector Symbolic Architecture is an approach for encoding and operations on distributed representation of information. VSA has previously been used mainly in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [15].\nThe fundamental difference between distributed and localist representations of data is as follows: in traditional (localist) computing architectures each bit and its position within a structure of bits are significant (for example, a field in a database has a predefined offset amongst other fields and a symbolic value has unique representation in ASCII codes); whereas in a distributed representation all entities are represented by binary random vectors of very high dimension. These representations are also called Binary Spatter Codes (BSC), though for the remainder of the article we use term HD-vector when referring to BSC codes, for reasons of brevity. High dimensionality refers to that fact that in HDvectors, several thousand positions (of binary numbers) are used for representing a single entity; [4] proposes the use of vectors of 10000 binary elements. Such entities have the following useful properties:\n1) Randomness: Randomness means that the values on each position of an HD-vector are independent of each other, and \"0\" and \"1\" components are equally probable. In very high dimensions, the distances from any arbitrary chosen HD-vector to more than 99.99 % of all other vectors in the representation space are concentrated around 0.5 Hamming distance. Interested readers are referred to [4] and [16] for comprehensive analysis of probabilistic properties of the hyperdimensional representation space.\n3 Denote the density of a randomly generated HD-vector (i.e. the number of ones in a HD-vector) as k. The probability of picking a random vector of length d with density k, where the probability of 1\u2019s appearance equals p is described by described by the binomial distribution (1):\nPr(k, d, p) = ( d\nk\n) pk(1\u2212 p)d\u2212k. (1)\nWhen d is in the range of several thousand binary elements, the calculation of the binomial coefficient requires sophisticated computations. The following equations use an approximation of the binomial distribution, via the de MoivreLaplace theorem:\nPr(k, d, p) \u2248 e \u2212(k\u2212dp)2 2dp(1\u2212p)\u221a\n2\u03c0dp(1\u2212 p) (2)\n2) Similarity metric: A similarity between two binary representation is characterized by Hamming distance, which (for two vectors) measures the number of positions in which they differ:\n\u2206H(A,B) = \u2016A\u2297B\u20161 d = \u2211d\u22121 i=0 ai\u2297bi d ,\nwhere ai, bi are bits on positions i in vectors A and B of dimension d, and where \u2297 denotes the bit-wise XOR operation.\n3) Generation of HD-vectors: Random binary vectors with the above properties can be generated with Zadoff-Chu sequences [17], a method widely used in telecommunications to generate pseudo-orthogonal preambles. Using this approach a sequence of K vectors, which are pseudo-orthogonal to a given initial random HD-vector A (i.e. Hamming distance between them approximately equals 0.5) are obtained by cyclically shifting A by i positions, where 1 < i \u2264 K \u2264 d. Further in the article this operation is denoted as Sh(A, i). The cyclic shift operation has the following properties: \u2022 it is invertible, i.e. if B = Sh(A, i) then A = Sh(B,\u2212i); \u2022 it is associative in the sense that Sh(B, i + j) =\nSh(Sh(B, i), j) = Sh(Sh(B, j), i); \u2022 it preserves Hamming weight of the result: \u2016B\u20161 = \u2016Sh(B, i)\u20161; and \u2022 the result is dissimilar to the vector being shifted: \u2016B\u2297Sh(B,i)\u20161 d \u2248 0.5. Note that the cyclic shift is a special case of the permutation operation [4]. In the context of VSA, permutations were previously used to encode sequences of semantically bound elements.\n4) Bundling of vectors: Joining several entities into one structure is done by the bundling operation; it is implemented by a thresholded sum of the HD-vectors representing the entities. A bit-wise thresholded sum of n vectors results in 0 when n/2 or more arguments are 0, and 1 otherwise. Furthermore terms \"thresholded sum\" and \"majority sum\" are used interchangeably and denoted as [A+B+C]. The relevant properties of the majority sum are: \u2022 the result is a random vector, i.e. the number of \u20181\u2019\ncomponents is approximately equal to the number of \u20180\u2019 components;\nThe algebra on VSA includes other operations e.g., binding, permutation [4]. Since we do not use them in this article, we omit their definitions and properties."}, {"heading": "IV. HOLOGRAPHIC GRAPH NEURON", "text": "This section presents one of the main contributions of this paper: the adoption of the VSA data representation for implementation of the HGN approach."}, {"heading": "A. Motivation for Holographic Graph Neuron and outline of the solution", "text": "An important issue in hierarchical models is the overhead of resource requirements, specifically with regards to the number of processing elements required. We propose a holographic approach, which enables a flat GN array to operate with higher level of accuracy and comparable recall time than that of HGN, without the need for a complex topology and additional nodes. The high-level logic of the proposed solution is illustrated in Figure 2. Essentially, we replace the need for maintaining topological relationships by compacting parts of the pattern observed by GNs into one holographic representation."}, {"heading": "B. Encoding", "text": "In the case of HoloGN, all its elements (i.e. symbols of individual neurons) are indexed uniquely and the index of a particular element is derived as a function of the GN\u2019s ID. Let IVj be an initialization high-dimensional vector for GN j. The initialized vectors for different GNs are chosen to be mutually orthogonal. Then the HD-index of element i in GN j is computed as EHD(j,i) = Sh(IVj , i), where Sh() is a cyclic shift operation resulting in the generation of a vector orthogonal to IVj HD-vector [17], [18].\n4"}, {"heading": "C. Construction of VSA-representation of activated GNs", "text": "Let n be the number of individual Graph Neurons. When a GN array is exposed to a pattern, the activated elements communicate their HD-represented indices to all other GNs; the holographic representation of the activated elements is then:\nHGN = [ n\u2211\nj=1\n(EHDj )], (3)\nwhere EHDj is the HD-index of the activated element in textGN j , and the addition operation is the bundling operation, or thresholded sum as described in III-B. As discussed previously, in the resultant HD-vector the Hamming distances between each component and the composition vector is strictly less than 0.5. We will utilize this property later when constructing data structures for recall of patterns in HoloGN."}, {"heading": "D. Data structures for storing and retrieving holographic representations in HGN elements", "text": "HoloGN will store the holographic representation of the entire pattern (3) observed across all GNs. A possible architecture is for all memorized patterns to be collected and stored centrally at a processing node.\nDepending on the particular application of HoloGN, the memorized patterns could be stored either in an unsorted list or in bundles. The first mode of storing HoloGN patterns corresponds to the case where the structure of the observed patterns is unknown; the latter mode is used in the case of a supervised learning. The next section describes different HoloGN usage and recall strategies."}, {"heading": "V. HOLOGN RECALL STRATEGIES", "text": "This section introduces and evaluates the performance of two major recall modes: the one-shot case and the case of supervised learning."}, {"heading": "A. Time-efficient \u03be-accurate recall in an unsorted HoloGN storage", "text": "The common steps in both recall modes is the procedure for the time-efficient search over an (unsorted) list of HoloGN records. Recall that all manipulations with VSAencoded entities are done using simple bit-wise arithmetic operations as well, and calculations to obtain the Hamming distance between entities. However, for this article we assume no particular optimized implementation of VSA\u2019s bit-wise operations; this is because such operations are tailored to the architectures of specific microprocessors, which operate with words of substantially lower dimensionalities (typically 32 or 64 bits). Therefore, adopting these methods for implementing the bit-wise operations on words of thousands of bits would be cumbersome. Instead, an easily analyzable computational model is adopted in this article, which could also be adapted to an implementation on specialized computing architectures.\nIn what follows, each HoloGN pattern hi is modelled as a row vector of d elements. The list of stored HoloGN patterns\nis therefore modelled as an l\u00d7d matrix, where l is the number of the learned (stored) HoloGN patterns. Denote this matrix as H. The task of recalling a pattern with a target accuracy of \u03be (\u03be < 0.5) is formulated as finding the rows hi in H with Hamming distance to the query pattern hq less than or equal to \u03be.\nThe conventional way of computing Hamming distance between vectors would be to perform the following sequence of computations for each row in H: 1) perform an elementwise XOR with the vector query; 2) sum up all elements in the intermediate result; and then 3) divide the result by the dimensionality of the vectors. The performance of the two implementations of this method using Matlab\u2019s repmat and bxfun functions are shown by the top two curves in Figure 3. The curves demonstrate linear but rapid increase in the recall time with the increase in the number of the stored patterns. The lowest curve in the figure shows the performance of matrixvector multiplication of the same size, which is chosen as the reference case. The results were obtained on a Intel Core i73520M 2.9 GHz machine with Windows 7 operating system using one processor.\n1) Binary spatter codes as complex numbers: In order to improve the efficiency of calculating Hamming distances over a vast number of HoloGN patterns, we propose representing HoloGN patterns using complex numbers, where a binary 0 would be represented by \u221a \u22121 (i.e. the complex number) and a binary 1 would remain 1. The intuition behind this transformation is simple: multiplication of bits in the same position should produce three outcomes: \u22121 = j\u00d7j, 1 = 1\u00d71 and j = 1 \u00d7 j. That is, the multiplication of two similar bits would produce a real number and the multiplication of two different bits produces a imaginary number. In this way the sum of the imaginary parts over all positions in the resulting vector, divided by dimensionality d, will correspond to the Hamming distance between the two vectors. Thus, the suggested method allows us to implement the calculation of Hamming distance through the standard method of matrixvector multiplication, as illustrated below:\nH\u00d7 hq =  \u221a \u22121 1 \u00b7 \u00b7 \u00b7 \u221a \u22121 1 1 \u00b7 \u00b7 \u00b7 \u221a \u22121\n... ... . . . ...\u221a\n\u22121 \u221a \u22121 \u00b7 \u00b7 \u00b7 1 \u00d7  \u221a \u22121 1 ...\u221a \u22121  =\n=  254 + 1633j 617 + 3824j\n... 548 + 4952j\n . (4)\nThe performance of the proposed method is illustrated by the red dashed curve in Figure 3. It demonstrates that the calculation of the Hamming distance is only two times slower than the usual matrix-vector multiplication. Specifically, to compute Hamming distance from the target vector to each of 20 000 stored patterns takes approximately 200 ms on the test machine. Further optimization of the matrix multiplication and execution on parallel architectures hint on realistic bounds on the recall time over extremely large numbers of patterns.\n5"}, {"heading": "B. Case-study 1: Best-match probing under one-shot learning", "text": "The first usage of HoloGN is probing the existence of the target query pattern amongst the previously memorized patterns. The perfect match in this case would be indicated by a Hamming distance of zero. The deviation from zero, therefore, reflects the degree of proximity of the query to one or several stored HoloGN patterns. In the following example the accuracy of the HoloGN recall was compared to the performance of the original HGN approach. For the sake of fair comparison the 7 by 5 pixels letters of the Roman alphabet (as in [2]) were used.\nIn the memorizing phase a set of noise-free images of letters illustrated in Figure 4 was presented to both architectures. In the recall phase images of the same letters distorted with different levels of random distortions (between 1 bit corresponding to a distortion of 2.9% of the pattern\u2019s size and 5 bits equivalent to 14.3% distortion) were presented to the architectures for the recall. An example of a noisy input is presented in Figure 5. In the case of HoloGN the pattern with the lowest Hamming distance to the presented distorted pattern was returned as the output.\nFigure 6 presents the results of the accuracy comparison between the recall results for the HoloGN approach and the reference HGN architecture. To obtain the results 1000 distorted images of each letter for every level of distortion were presented for recall. The charts show the percentage of the\ncorrect recall output. The analysis shows that the performance of the HoloGN-based associative memory at least matches that of the original approach. In most of cases HoloGN appears to be more accurate when recalling patterns with high level of distortion."}, {"heading": "C. Case-study 2: HoloGN recall under the supervised learning", "text": "The analysis presented above is a very positive result for the proposed bio-inspired associative memory based pattern processing architecture, since the accuracy of the original HGN approach was demonstrated to be as accurate as artificial neural networks with back-propagation [2]. While establishing formal relationships to the framework of artificial neural networksis outside the scope of this work, this section presents the results of the pattern recognition accuracy of the HoloGN architecture under the supervised learning. In this case the HoloGN is presented with a series of randomly distorted patterns for each letter with different level of distortion (between 3% and 43%) as exemplified in Figure 5. In the experiments up to 500 patterns for each letter and every level of distortion were presented for memorizing. For the particular level of distortion i all e presented patterns of the particular letter Li were bundled to a single HoloGN representation as\nh(L) = [ e\u2211\ni=1\n(HGN(Li))]. (5)\nThus by the end of the learning phase the HoloGN list will contain 26 high dimensional bundles, each jointly representing all (presented) distorted variants of the particular letter. In the recall phase for each distortion level HoloGN was presented with 500 new distorted patterns of each letter. The accuracy of the recall was measured as the percentage of the correctly recognized letters averaged over the alphabet.\nFigure 7 illustrates the obtained results: 90% accurate recall was observed when learning symbols distorted up to 15%. While the accuracy predictably decreases rapidly with the increase of distortion in the presented patterns, a reasonable 80% recall accuracy was observed for learning sets with 7 bits distortion (20%).\nFigure 8 illustrates the convergence of the HoloGN recall accuracy with the number of presented noisy samples for the case of 14.3% distortion (5 bits). This is a definitely positive result for the presented architecture, which illustrates the suitability of the HoloGN in applications requiring supervised learning.\n6"}, {"heading": "VI. PATTERN DECODING AND SUBPATTERN-BASED ANALYSIS", "text": "There is a class of pattern recognition applications which requires an understanding of the details of the recall results. For example, when a recall returns several possible patterns of given recall accuracy, the task would be to understand the overlapping elements. This section considers two aspects of this task: a robust decoding of elementary components out of a distributed VSA representation; and a quantitative metric of the similarity via direct comparison of distributed representations, without the need for decoding those representations.\nThe VSA approach of representing data structures by definition makes decoding of the individual components a tedious task, requiring a brute force test on the inclusion of\nall possible high-dimensional codewords for each GN. The majority sum, which is used for creating HoloGN representations of the observed patterns imposes a limit on the number high-dimensional codewords operands, above which a robust decoding of the individual operands is impossible."}, {"heading": "A. Preliminaries", "text": "Denote the density of a randomly generated HD-vector (i.e. the number of ones in a HD-vector) as k. The probability of picking a random vector of length d with density k, where the probability of 1\u2019s appearance, defined as p, is described by (2). The mean density of a random vector is equal to d\u00b7p. Note that in reality the density of randomly generated HD-vectors will\n7 obviously deviate from the mean value. However, according to (1) density k is approaches the mean value with the increase of dimensionality d. In other, words the probability of generating HD-vector with k >> d \u00b7 p or k << d \u00b7 p decreases with the increase of dimensionality d.\nDefine thr as the threshold probability of generating a vector with a certain deviation of density being negligibly small. Let k\u2212 and k+ characterize the lower and the upper bounds of the interval of possible densities. This is illustrated in Figure 9. The bounds for a given d, p and thr are calculated using (2). The bounds are calculated according to (6) and (7). The value of threshold is chosen to be small (10\u22126).\nk\u2212(d, p, thr) = max k (Pr(k, d, p) <= thr|k < (d \u00b7 p)) (6)\nk+(d, p, thr) = min k (Pr(k, d, p) <= thr|k > (d \u00b7 p)) (7)"}, {"heading": "B. Capacity of HoloGN representations", "text": "Suppose there exists an item memory [4] containing HDvectors representing atomic concepts3. Recall that when several HD-vectors are bundled by the majority sum the noise of flipped bits increases with the number of components. For a given dimensionality d there is a limit on the number of bundled HD-vectors beyond which the resulting HD-vector becomes orthogonal to every component vector; hence the A capacity of the resulting vector is defined as the maximal number of mutually orthogonal HD-vectors which can be robustly decoded from their majority sum composition by probing the item memory.\nIn order to characterize the capacity of the composition vector for a given dimensionality, one needs to characterize the level of noise pn introduced by the bundling operation. This is calculated as in [19] by (8), where n is a number of atomic vectors in the resulting majority sum vector.\npn(n) =\n1 2 \u2212\n( n\u2212 1\n0.5 \u00b7 (n\u2212 1) ) 2n . (8)\nConsider an arbitrary HD-vector A to be decoded from a majority sum composition. Let N be a vector of noise imposed by the majority sum operation. Since the components\n3In the case of HoloGN an atomic concept is the code for the particular HoloGN element.\nFig. 10. Explanation of vector\u2019s capacity. Solid line represents random HDvector. Dashed line corresponds to noise introduced by majority sum.\nare mutually orthogonal, the density of ones in the noise vector is also described the binomial distribution Pr(k, d, pn). As each new vector is added the level of noise increases, hence the mean of the noise vector density will approach 0.5 as illustrated in Figure 11. Due to the properties of high dimensional space, vector A will be undecodable when the upper bound k+(d, pn, thr) of the density of noise vector N approaches the lower bound k\u2212(d, 0.5, thr). That is, the noisy version of A becomes orthogonal to its clean version. This logic is illustrated in Figure 10, where the resulting majority sum vector is orthogonal to all components. Thus the capacity of the distributed representation with dimensionality d is computed by:\nCapacity(d, thr) =\n= max n\n(k+(d, pn(n), thr) \u2264 k\u2212(d, 0.5, thr)) (9)\nFigure 11 presents the capacity of HD-vector of different dimensionalities calculated for threshold probability thr = 10\u22126. Specifically for d = 10000 bits the capacity of the robustly decodable VSA is 89 vectors."}, {"heading": "C. Calculation of the number of common component vectors in two resulting vectors", "text": "Given the rather conservative limits on the number of robustly decodable elements in a distributed representation it is\n8 important that the proposed HoloGN architecture can estimate the similarity between different patterns without decoding them. This subsection provides a method for the quantitatively measuring the number of overlapping elements as a function of their relative Hamming distance. Denote m and n as lengths of two patterns, m <= n, and denote c as the number of common elements in these patterns. Let M be a c\u00d7 d matrix of common elements where each row contains a random HDvector of dimension d encoding element ci. Denote an arbitrary column of matrix M as C. Since rows in M are independent, the density of ones in each column also follows the binomial distribution with p = 0.5 and length c. Denote the number of ones in column C as ||C||1.\nIn order to calculate the Hamming distance between the distributed representations of two patterns with known m, n and c, consider all possible cases when bits in the same position are different. The Hamming distance between two patterns can be estimated with equation (10):\n\u2206H = p(c,m, n) = c\u2211 ||C||1=0\n( c ||C||1 ) 2c \u00b7 (p1(m, c, ||C||1)\u00b7\n\u00b7p0(n, c, ||C||1)+p0(m, c, ||C||1) \u00b7 p1(n, c, ||C||1));\n(10)\nwhere pi(j, c, ||C||1) stands for the probability of having i (0 or 1), when the representation consists of j = m or j = n atomic vectors and c of these vectors are overlapped.\nDue to the symmetry in the calculation of probabilities, pi(j, c, ||C||1) is presented only for the case of p1(m, c, ||C||1). There are three possible cases for calculation of p1(m, c, ||C||1): \u2022 if ||C||1 is more than m/2, then the result of the majority\nsum is \u20181\u2019, i.e. p1 is 1; \u2022 if number of possible \u20181\u2019s is less than m/2, then proba-\nbility of p1 is 0; \u2022 otherwise the probability should take into account all\npossible combinations, and their probabilities. Equation (11) specifies probability for p1(m, c, ||C||1), and\nequation (12) does the same for p0(m, c, ||C||1).\np1(m, c, ||C||1) =  1, when ||C||1 > m2 0, when (m\u2212 c) < m+12 \u2212 ||C||1 (m\u2212c)\u2211 i=(m+12 \u2212||C||1) ( m\u2212 c i ) 2(m\u2212c)\n, otherwise (11)\np0(m, c, ||C||1) =  1, when (c\u2212 ||C||1) > m2 0, when (m\u2212 c) < m+12 \u2212 (c\u2212 ||C||1) (m\u2212c)\u2211 i=(m+12 \u2212(c\u2212||C||1)) ( m\u2212 c i ) 2(m\u2212c)\n, otherwise (12)\nFigure 12 shows the Hamming distances between two resulting vectors for different numbers of overlapping vectors. The\nFig. 12. The Hamming distance between two resulting vectors against number of components in common. The number of atomic vectors is the same, m = n.\nresults show that the larger the number of common elements, the smaller the Hamming distance between resulting vectors.\nThis method opens a way towards constructing and analyzing patterns far beyond VSA\u2019s robustly decodable capacity. The problem with practical application of this method, however, comes with the rapid convergence of the Hamming distance indicator to 0.5, making the difference between analyzable patterns indistinguishable as illustrated in Figure 12. For example, for HoloGN representations of patterns with 15 elements, sub-patterns of 3 overlapped elements are robustly detected, while patterns with fewer overlapped elements are indistinguishable. Thus, the minimal number of overlapped elements in two patterns which can be robustly detected using the Hamming distance indicator is called bundle\u2019s sensitivity.\nThe analysis of the sensitivity is similar to the analysis of the capacity of VSA representation in section VI-B. For two patterns of length m and n elements, and c overlapped components, the sensitivity is calculated by (13):\nSensitivity(d, thr,m, n) =\nmin c\n(k+(d, p(c, n,m), thr) <= k\u2212(d, 0.5, thr)) (13)\nFigure 13 demonstrates the development of the sensitivity threshold with the number of elements in the compared patterns. The results show that the number of components for robust detection grows linearly with the size of pattern. Patterns with more than 500 elements should contain at least 14 % overlapped elements to be robustly detected by the proposed method."}, {"heading": "VII. CONCLUSION", "text": "This article presented Holographic Graph Neuron - a novel approach for memorizing patterns of generic sensor stimuli. HoloGN is built upon the previous Graph Neuron algorithm and adopts a Vector Symbolic representation for encoding of the Graph Neuron\u2019s states. The adoption of the Vector Symbolic representation ensures a one-layered design for the approach, which implies the computational simplicity of the operations. The presented approach possesses the number of\n9\nuniques properties. Prior to all, it enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern. While maintaining the previously reported properties of Hierarchical Graph Neuron, HoloGN is also improving the noise resistance of the architecture by this substantially improving the accuracy of pattern recall."}], "references": [{"title": "Holographic graph neuron", "author": ["E. Osipov", "A.I. Khan", "A. Anang"], "venue": "In Computer and Information Sciences (ICCOINS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "A hierarchical graph neuron scheme for real-time pattern recognition", "author": ["B.B. Nasution", "A.I. Khan"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "One-shot associative memory method for distorted pattern recognition", "author": ["A.I. Khan", "A.H. Muhamad Amin"], "venue": "In AI 2007: Advances in Artificial Intelligence, 20th Australian Joint Conference on Artificial Intelligence, Gold Coast, Australia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors", "author": ["P. Kanerva"], "venue": "Cognitive Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Vector symbolic architectures: A new building material for artificial general intelligence", "author": ["S.D. Levy", "R. Gayler"], "venue": "In Proceedings of the 2008 Conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Holographic reduced representations", "author": ["T. Plate"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Representing objects, relations, and sequences", "author": ["S.I. Gallant", "T.W. Okaywe"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Fly like a fly", "author": ["R. Zbikowski"], "venue": "IEEE Spectrum,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Digital cameras with designs inspired by the arthropod", "author": ["Y.M. Song", "Y. Xie", "V. Malyarchuk", "J. Xiao", "I. Jung", "K-J. Choi", "Z. Liu", "H. Park", "C. Lu", "R-H. Kim", "R. Li", "K.B. Crozier", "Y. Huang", "J.A. Rogers"], "venue": "eye. Nature,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Content Addressable Memory", "author": ["K.J. Schultz"], "venue": "N. T. Limited,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "A family of binary spatter codes", "author": ["P. Kanerva"], "venue": "In ICANN \u201995: International Conference on Artificial Neural Networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Learning behavior hierarchies via high-dimensional sensor projection", "author": ["S. Bajracharya S. Levy", "R. Gayler"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Dependable mac layer architecture based on holographic data representation using hyperdimensional binary spatter codes", "author": ["D. Kleyko", "N. Lyamin", "E. Osipov", "L. Riliskis"], "venue": "In Multiple Access Communications : 5th International Workshop,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Collective communication for dense sensing environments", "author": ["P. Jakimovski", "H.R. Schmidtke", "S. Sigg", "L. Weiss-Ferreira-Chaves", "M. Beigl"], "venue": "Journal of Ambient Intelligence and Smart Environments (JAISE),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Distributed Representations and Nested Compositional Structure", "author": ["T. Plate"], "venue": "University of Toronto, PhD Thesis,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1994}, {"title": "Sparse distributed memory", "author": ["P. Kanerva"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1988}, {"title": "Generalized chirp-like polyphase sequences with optimum correlation properties", "author": ["B.M. Popovic"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "On bidirectional transitions between localist and distributed representations: The case of common substrings search using vector symbolic architecture", "author": ["D. Kleyko", "E. Osipov"], "venue": "Procedia Computer Science,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Fully distributed representation", "author": ["P. Kanerva"], "venue": "In Real world computing symposium,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}], "referenceMentions": [{"referenceID": 1, "context": "It is based on the hypothesis that a better associative memory resource can be created by changing the emphasis from high speed sequential CPU processing to parallel network-centric processing [2], [3].", "startOffset": 193, "endOffset": 196}, {"referenceID": 2, "context": "It is based on the hypothesis that a better associative memory resource can be created by changing the emphasis from high speed sequential CPU processing to parallel network-centric processing [2], [3].", "startOffset": 198, "endOffset": 201}, {"referenceID": 3, "context": "Vector Symbolic Architectures (VSA) [4] are a bio-inspired method of representing concepts and their meaning for modeling cognitive reasoning.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "It exhibits a set of unique properties which make it suitable for implementation of artificial general intelligence [5], [6], [7], and so, creation of complex systems for sensing and pattern recognition without reliance on complex computation.", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "It exhibits a set of unique properties which make it suitable for implementation of artificial general intelligence [5], [6], [7], and so, creation of complex systems for sensing and pattern recognition without reliance on complex computation.", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "It exhibits a set of unique properties which make it suitable for implementation of artificial general intelligence [5], [6], [7], and so, creation of complex systems for sensing and pattern recognition without reliance on complex computation.", "startOffset": 126, "endOffset": 129}, {"referenceID": 0, "context": "This work was initially presented at IEEE International Conference on Computer and Information Sciences, ICCOINS 2014 [1].", "startOffset": 118, "endOffset": 121}, {"referenceID": 8, "context": "These compound eyes consist of large number of sensors with limited and localized processing capabilities for performing relatively complex sensing tasks [9].", "startOffset": 154, "endOffset": 157}, {"referenceID": 9, "context": "Associative memory (AM) is a sub-domain of artificial neural networks, which utilises the benefits of content-addressable memory (CAM) [10] in microcomputers.", "startOffset": 135, "endOffset": 139}, {"referenceID": 1, "context": "The Hierarchical Graph Neuron (HGN) approach [2] is a type of associative memory which signifies the hierarchical structure in its implementation.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "1In [8], a house fly\u2019s properties are compared and contrasted with an advanced fighter plane as follows: \u201cWhereas the F-35 Joint Strike Fighter, the most advanced fighter plane in the world, takes a few measurements - airspeed, rate of climb, rotations, and so on and then plugs them into complex equations, which it must solve in real time, the fly relies on many measurements from a variety of sensors but does relatively little computation.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "Vector Symbolic Architectures [5] are a class of connectionist models that use hyper-dimensional vectors (i.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "Distributed representations of data structures are an approach actively used in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [11].", "startOffset": 179, "endOffset": 182}, {"referenceID": 10, "context": "Distributed representations of data structures are an approach actively used in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [11].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "In [12] a VSA-based knowledge-representation architecture is proposed for learning arbitrarily complex, hierarchical, symbolic relationships (patterns) between sensors and actuators in robotics.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "Recently the theory of hyper-dimensional computing, and VSA in particular, were adopted for implementing novel communication protocols and architectures for collective communications in machine-to-machine communication scenarios [13], [14].", "startOffset": 229, "endOffset": 233}, {"referenceID": 13, "context": "Recently the theory of hyper-dimensional computing, and VSA in particular, were adopted for implementing novel communication protocols and architectures for collective communications in machine-to-machine communication scenarios [13], [14].", "startOffset": 235, "endOffset": 239}, {"referenceID": 12, "context": "This article presents an algorithmic ground for further design of the distributed HoloGN on top of the architecture presented in [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 1, "context": "Consider only the bottom layer of the construction without the hierarchy of upper nodes; this bottom level is the original flat network of Graph Neurons [2].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "The accuracy of HGN was demonstrated to be comparable to the accuracy of Neural Network with back-propagation [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "VSA has previously been used mainly in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [15].", "startOffset": 138, "endOffset": 141}, {"referenceID": 14, "context": "VSA has previously been used mainly in the area of cognitive computing for representing and reasoning upon semantically bound information [4], [15].", "startOffset": 143, "endOffset": 147}, {"referenceID": 3, "context": "High dimensionality refers to that fact that in HDvectors, several thousand positions (of binary numbers) are used for representing a single entity; [4] proposes the use of vectors of 10000 binary elements.", "startOffset": 149, "endOffset": 152}, {"referenceID": 3, "context": "Interested readers are referred to [4] and [16] for comprehensive analysis of probabilistic properties of the hyperdimensional representation space.", "startOffset": 35, "endOffset": 38}, {"referenceID": 15, "context": "Interested readers are referred to [4] and [16] for comprehensive analysis of probabilistic properties of the hyperdimensional representation space.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "3) Generation of HD-vectors: Random binary vectors with the above properties can be generated with Zadoff-Chu sequences [17], a method widely used in telecommunications to generate pseudo-orthogonal preambles.", "startOffset": 120, "endOffset": 124}, {"referenceID": 3, "context": "Note that the cyclic shift is a special case of the permutation operation [4].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": ", binding, permutation [4].", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "Then the HD-index of element i in GN j is computed as E (j,i) = Sh(IVj , i), where Sh() is a cyclic shift operation resulting in the generation of a vector orthogonal to IVj HD-vector [17], [18].", "startOffset": 184, "endOffset": 188}, {"referenceID": 17, "context": "Then the HD-index of element i in GN j is computed as E (j,i) = Sh(IVj , i), where Sh() is a cyclic shift operation resulting in the generation of a vector orthogonal to IVj HD-vector [17], [18].", "startOffset": 190, "endOffset": 194}, {"referenceID": 1, "context": "For the sake of fair comparison the 7 by 5 pixels letters of the Roman alphabet (as in [2]) were used.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "The analysis presented above is a very positive result for the proposed bio-inspired associative memory based pattern processing architecture, since the accuracy of the original HGN approach was demonstrated to be as accurate as artificial neural networks with back-propagation [2].", "startOffset": 278, "endOffset": 281}, {"referenceID": 3, "context": "Suppose there exists an item memory [4] containing HDvectors representing atomic concepts3.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "This is calculated as in [19] by (8), where n is a number of atomic vectors in the resulting majority sum vector.", "startOffset": 25, "endOffset": 29}], "year": 2015, "abstractText": "This article proposes the use of Vector Symbolic Architectures for implementing Hierarchical Graph Neuron, an architecture for memorizing patterns of generic sensor stimuli. The adoption of a Vector Symbolic representation ensures a one-layered design for the approach, while maintaining the previously reported properties and performance characteristics of Hierarchical Graph Neuron, and also improving the noise resistance of the architecture. The proposed architecture enables a linear (with respect to the number of stored entries) time search for an arbitrary sub-pattern.", "creator": "LaTeX with hyperref package"}}}