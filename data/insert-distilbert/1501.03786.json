{"id": "1501.03786", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2015", "title": "Multi-view learning for multivariate performance measures optimization", "abstract": "in this paper, we propose the specific problem of opti - mizing multivariate error performance measures from multi - function view data, algorithms and an effective method to solve it. this problem has defined two concrete features : the data points are partially presented by multiple views, and the critical target of learning optimization is to optimize complex multivariate performance measures. we propose to learn a component linear discriminant functions for each different view, and combine them to construct a overall multivariate mapping function for mult - view data. to learn the corresponding parameters of the linear random dis - criminant functions of different views so to optimize entire multivariate performance measures, we formulate a optimization problem. in this problem, we propose to minimize the complexity of the linear discriminant functions of each view, encourage the consistences efficiency of the responses of different views over the same data time points, and minimize the upper boundary of a given multivariate performance boundary measure. to optimize this problem, subsequently we employ the new cutting - plane map method in an iterative algorithm. in each particular iteration, we update a set of constrains, and optimize successively the mapping function parameter of each view one by one.", "histories": [["v1", "Thu, 15 Jan 2015 19:33:34 GMT  (6kb)", "https://arxiv.org/abs/1501.03786v1", null], ["v2", "Fri, 16 Jan 2015 11:10:13 GMT  (6kb)", "http://arxiv.org/abs/1501.03786v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jim jing-yan wang"], "accepted": false, "id": "1501.03786"}, "pdf": {"name": "1501.03786.pdf", "metadata": {"source": "CRF", "title": "Multi-view learning for multivariate performance measures optimization", "authors": ["Jim Jing-Yan Wang"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n03 78\n6v 2\n[ cs\n.L G\n] 1\n6 Ja\nn 20\n15 1\nIndex Terms\u2014Multivariate performance measures, Multi-view learning, Cutting-plane algorithm\nI. INTRODUCTION\nD IFFERENT multivariate performance measures are usedto evaluate different machine learning applications [1]. For example, in problems of text classification, F1-score and precision/recall breakeven point (PRBEP) are used to compare true class labels against predicted class labels of a given test text set. In image retrieval problems, the precision/recall at the top k returned images are used to evaluate the performance of a retrieval system. Recently, the problem of multivariate performance measures optimization are proposed to learn directly to losses based on pre-defined multivariate performance measures. Many algorithms have been proposed to solve this problem, and some examples are listed as follows.\n\u2022 Joachims [1] proposed a support vector machine (SVM)based for multivariate nonlinear performance measures optimization problems. The proposed algorithm can train a multivariate SVM in polynomial time for large classes of potentially non-linear performance measures. Moreover, the traditional SAM can be arised as a special case of this method. \u2022 Li et al. [2] proposed a two-step approach to optimize multivariate performance measures, by first training a nonlinear classifiers with existing learning methods, and then adapting it to optimize specific performance measures. In the seconde step, the classifier adaptation problem can be solved as a quadratic program problem, in a similar way to linear SVM. \u2022 Mao and Tsang [3] proposed a novel feature selection method to optimize multivariate performance measures,\nby formulating the problem or high-dimensional data, and employing a two-layer cutting plane algorithm to solve it. Moreover, this method is also used in multiple-instance learning problems.\nUp to now, all these multivariate methods are limited to learn from data with single view. However, in many realworld machine learning problems, the data can be presented by multiple views. For example, in computer vision problems, we can extract different types of features, such as color features, texture features, and shape features, and each feature can be treated as a view. In scientific article classification problems, we can also learn from the views of article abstract, content, and references. Different views of data may be complemental to each other in a learning problem and using multiple views has been a popular strategy in machine learning community. To learn from multiple views of data, different multi-view learning methods have been proposed [4], [5], [6], [7]. However, none of them are designed to optimized a specific multivariate performance measure.\nTo overcome this problem, in this paper, we propose the problem of learning from multiple view data to optimize the multivariate performance measures. Given a tuple of data points, each of them are presented with multiple views. The problem is to learn a multivariate mapping function to map them to a tuple of class labels, so that the multivariate performance measures can be optimized. To solve this problem, we proposed a novel method for multi-view learning to optimize multivariate performance measures. The contribution of this paper are of two folds:\n1) We propose the problem of multi-view learning for multivariate performance measures. Although there are plenty of multivariate performance measures optimization methods, they are limited to single view data. There are also lots of multi-view learning methods, however, none of them are proposed to optimize multivariate performance measures. 2) We also propose a novel method to solve this problem. We proposed to learn a linear discriminant functions for each view, and combine them to construct a overall multivariate mapping function to predict the class label tuple of a tuple of data points. To learn the linear discriminant functions parameters of different views, we formulate a constrained minimization problem. In this problem, we proposed to minimize the complexity of each linear discriminant functions parameter by minimizing its squared \u21132 norm, encourage the consistence among different views by minimizing the squared \u21132 norm distance\n2 among different linear discriminant functions response of differen views, and also minimize the losses based on a specific multivariate performance measures. The minimization problem is optimized by a cutting-plane method in an alterative algorithm [8]. We maintain a active set of constrains, and update it in each iteration by add a most violated class label tuple. We also optimize the multivariate mapping function parameters one by one, and the optimization of a multivariate mapping function parameter can be solve as a quadratic program problem.\nThe rest parts of this paper are organized as follows: in section II, we introduce the proposed method, and in section III, we conclude this paper.\nII. PROPOSED METHOD\nA. Problem formulation\nWe assume we have a training data set of n data points, and each data point has m views. The training set is denoted as {(xji | m j=1, yi)}| n i=1, where x j i \u2208 R\ndj is the dj -dimensional feature vector of the j-th view of the i-th data point, and yi \u2208 {+1,\u22121} is the binary class label of the i-th data point. We propose to learn a multivariate mapping function h to map a tuple of n data points of m views, x = (xj\n1 |mj=1, \u00b7 \u00b7 \u00b7 , x j n| m j=1),\nto tuple of n class labels, y = (y1, \u00b7 \u00b7 \u00b7 , yn). To implement this multivariate mapping function, we use a linear discriminant function fj(xj , y\u2032) to predict the response of a view the the data tuple, xj = (xj\n1 , \u00b7 \u00b7 \u00b7 , xjn), against a candidate class label\ntuple y\u2032 = (y\u2032 1 , \u00b7 \u00b7 \u00b7 , y\u2032n),\nfj(xj , y\u2032) = n \u2211\ni=1\nw\u22a4j \u03a8(x j , y\u2032) (1)\nwhere wj \u2208 Rdj is a parameter vector for the linear discriminant function of the j-th view, , and \u03a8(xj, y) is a function which can returns a vector to describe the match between xj and y\u2032, defined as follows,\n\u03a8(xj , y\u2032) = n \u2211\ni=1\ny\u2032ix j i . (2)\nBased on the linear discriminant functions of different views, we construct the multivariate mapping function as follows,\nh(x) = argmaxy\u2032\u2208Y\n\n\n\nm \u2211\nj=1\nfj(xj , y\u2032) = m \u2211\nj=1\nw\u22a4j \u03a8(x j , y\u2032)\n\n\n\n(3) where Y = {+1,\u22121}n is a set of all admissible label vectors.\nWe propose to optimize a complex multivariate performance measure, \u2206, by learning the parameter vectors wj |mj=1 for the m views. To this end, we consider the following three problems,\n1) Reducing the complexity of each linear discriminative function: To prevent over-fitting, we propose to reduce the complexity of the linear discriminative function of each view by minimizing the squared \u21132 norm of its parameter,\nmin wj |mj=1\n1\n2\nm \u2211\nj=1\n\u2016wj\u201622. (4)\n2) Encouraging consistences among different views: To encourage the consistences of different views, we proposed to minimize the squared \u21132 norm distances of responses of any two linear discriminative functions over one single data point,\nmin wj |mj=1\n1\n2\n\u2211\nj,j\u2032:j\u2032<j\n(\nn \u2211\ni=1\n\u2016w\u22a4j x j i \u2212 w \u22a4 j\u2032x\nj\u2032 i \u2016 2 2\n)\n. (5)\n3) Optimizing multivariate performance measures: To optimize a specific multivariate performance measure, we propose to minimize the upper boundary of a loss function \u2206 based on this multivariate performance measure,\nmin wj |mj=1,\u03be \u03be\ns.t.\u2200y\u2032 \u2208 Y/y : m \u2211\nj=1\nw\u22a4j [ \u03a8(xj , y)\u2212\u03a8(xj , y\u2032) ] \u2265 \u2206(y\u2032, y)\u2212 \u03be,\n(6) where \u03be is a slack variable of the upper boundary of the loss function.\nThe overall optimization function are obtained by combining all the problems above,\nmin wj |mj=1,\u03be\n\n\n\n1\n2\nm \u2211\nj=1\n\u2016wj\u201622 + C1\u03be\n+ C2 2 \u2211\nj,j\u2032:j\u2032<j\n(\nn \u2211\ni=1\n\u2016w\u22a4j x j i \u2212 w \u22a4 j\u2032x\nj\u2032 i \u2016 2 2\n)\n\n\n\ns.t.\u2200y\u2032 \u2208 Y/y : m \u2211\nj=1\nw\u22a4j [ \u03a8(xj , y)\u2212\u03a8(xj , y\u2032) ] \u2265 \u2206(y\u2032, y)\u2212 \u03be.\n(7)\nIn the objective of this problem, the first term is to reduce the complexity of the parameter vector of each view, and second term is a slack variable to represent the upper boundary of the multivariate performance measure, and the third term is to encourage the consistences of the responses of different views. C1 and C2 are tradeoff parameters.\nB. Problem optimization\nTo solve this problem, we employ the cutting-plane algorithm. In an iterative algorithm, we update the parameter vectors wj |mj=1 and an active set of constrains W alternately.\n3 1) Updating wj |mj=1: When we have a given active set of constrain W \u2286 Y/y, and optimize the parameter vectors, we optimize wj one by one, i.e., when wj is optimized, wj\u2032 |j\u2032 6=j is fixed. The following optimization is obtained in this case,\nmin wj ,\u03be\n\n\n\n1 2 \u2016wj\u201622 + C1\u03be\n+ C2 2 \u2211\nj\u2032:j\u2032<j\n(\nn \u2211\ni=1\n\u2016w\u22a4j x j i \u2212 w \u22a4 j\u2032x\nj\u2032 i \u2016 2 2\n)\n\n\n\ns.t.\u2200y\u2032 \u2208 W :\nw\u22a4j [ \u03a8(xj, y)\u2212\u03a8(xj , y\u2032) ]\n\u2265 \u2206(y\u2032, y)\u2212 \u2211\nj\u2032 :j\u2032 6=j\nw\u22a4j\u2032 [ \u03a8(xj \u2032 , y)\u2212\u03a8(xj \u2032 , y\u2032) ] \u2212 \u03be.\n(8) The Lagrange function of this problem is\nL(wj , \u03be, \u03b1y\u2032 |y\u2032:y\u2032\u2208W) = 1\n2 \u2016wj\u201622 + C1\u03be\n+ C2 2 \u2211\nj\u2032 :j\u2032<j\n(\nn \u2211\ni=1\n\u2016w\u22a4j x j i \u2212 w \u22a4 j\u2032x\nj\u2032 i \u2016 2 2\n)\n\u2212 \u2211\ny\u2032:y\u2032\u2208W\n\u03b1y\u2032\n\nw\u22a4j [ \u03a8(xj , y)\u2212\u03a8(xj , y\u2032) ] \u2212\u2206(y\u2032, y)\n+ \u2211\nj\u2032:j\u2032 6=j\nw\u22a4j\u2032 [ \u03a8(xj \u2032 , y)\u2212\u03a8(xj \u2032 , y\u2032) ] + \u03be\n\n\n= 1\n2 w\u22a4j \u2126wj + C1\u03be \u2212 w \u22a4 j \u03b2 + C2 2 \u2211\nj\u2032:j\u2032<j\nn \u2211\ni=1\nw\u22a4j\u2032x j\u2032 i x j\u2032 i \u22a4 wj\u2032\n\u2212 \u2211\ny\u2032:y\u2032\u2208W\n\u03b1y\u2032 ( w\u22a4j \u03b3y\u2032 \u2212 \u03b4y\u2032 + \u03be )\n(9) where \u03b1y\u2032 \u2265 0 is the Lagrange multiplier for the constrain w\u22a4j [ \u03a8(xj , y)\u2212\u03a8(xj , y\u2032) ]\n\u2265 \u2206(y\u2032, y) \u2212 \u2211\nj\u2032:j\u2032 6=j w \u22a4 j\u2032\n[\n\u03a8(xj \u2032 , y)\u2212\u03a8(xj \u2032 , y\u2032) ] \u2212 \u03be, and\n\u2126 =\n\nI + C2 \u2211\nj\u2032:j\u2032<j\nn \u2211\ni=1\nxjix j i\n\u22a4\n\n ,\n\u03b2 = C2 \u2211\nj\u2032:j\u2032<j\nn \u2211\ni=1\nxjix j\u2032 i \u22a4 wj\u2032 ,\n\u03b3y\u2032 = [ \u03a8(xj , y)\u2212\u03a8(xj , y\u2032) ]\n\u03b4y\u2032 =\n\n\u2206(y\u2032, y)\u2212 \u2211\nj\u2032 :j\u2032 6=j\nw\u22a4j\u2032 [ \u03a8(xj \u2032 , y)\u2212\u03a8(xj \u2032 , y\u2032) ]\n\n .\n(10) This optimization problem can be transferred to its dual form,\nmax \u03b1y\u2032 |y\u2032:y\u2032\u2208W min wj ,\u03be L(wj , \u03be, \u03b1y\u2032 |y\u2032:y\u2032\u2208W)\ns.t. \u2200y\u2032, y\u2032 \u2208 W : \u03b1y\u2032 \u2265 0. (11)\nTo obtain the optimal points of wj and \u03be, we set the gradients of Lagrangian function with respect to wj and \u03be to zeros, and we have,\n\u2202L\n\u2202wj = \u2126wj \u2212 \u03b2 \u2212\n\u2211\ny\u2032:y\u2032\u2208W\n\u03b1y\u2032\u03b3y\u2032 = 0\n\u21d2 wj = \u2126\u22121\n\n\u03b2 + \u2211\ny\u2032:y\u2032\u2208W\n\u03b1y\u2032\u03b3y\u2032\n\n ,\n\u2202L \u2202\u03be = C1 \u2212 \u2211\ny\u2032:y\u2032\u2208W\n\u03b1y\u2032 = 0\n\u21d2 \u2211\ny\u2032:y\u2032\u2208W\n\u03b1y\u2032 = C1.\n(12)\nBy substituting these results back to (11), we obtain the dual problem as\nmax \u03b1y\u2032 |y\u2032:y\u2032\u2208W\n\n \n \n\u2212 1\n2\n\n\u03b2 + \u2211\ny\u2032:y\u2032\u2208W\n\u03b1y\u2032\u03b3y\u2032\n\n\n\u22a4\n\u2126\u22121\n\n\u03b2 + \u2211\ny\u2032:y\u2032\u2208W\n\u03b1y\u2032\u03b3y\u2032\n\n\n\u2212 \u2211\ny\u2032:y\u2032\u2208W\n\u03b1y\u2032\u03b4y\u2032\n\n\n\ns.t. \u2200y\u2032, y\u2032 \u2208 W : \u03b1y\u2032 \u2265 0, and \u2211\ny\u2032:y\u2032\u2208W\n\u03b1y\u2032 = C1.\n(13) We can solve this problem as a quadratic program problem.\n2) Updating W: To update W , we first find the most violated y\u2032 and then add it the W . y\u2032 is obtained as\ny\u2032 = argmaxy\u2032\u2032\u2208Y/y\n\n\n\n\u2206(y\u2032\u2032, y) +\nm \u2211\nj=1\nw\u22a4j \u03a8(x j , y\u2032\u2032)\n\u2212 m \u2211\nj=1\nw\u22a4j \u03a8(x j , y)\n\n\n\n.\n(14)\n3) Alterative algorithm: The proposed iterative algorithm is given in Algorithm 1.\nIII. CONCLUSION\nIn this paper, we propose the problem of multi-view learning for multivariate performance measures, and a novel algorithm to solve it. Multivariate performance measures optimization can obtain a predictor which directly optimize a desired performance measure. However, the existing methods are limited to single view data, and cannot utilize multi-view data. We propose to learn from multi-view data to optimize the multivariate performance measures, and it has potential in realworld applications. The proposed method can be implemented easily due to its employment of cutting plane method and quadratic program.\n4 Algorithm 1 Iterative multi-view learning algorithm for multivariate performance measures.\nInput: Multi-view training data set {(xji | m j=1, yi)}| n i=1; Input: Tradeoff parameters C1 and C2; Input: The maximum iteration number T . Initialize linear discriminative function parameter w0j for each view; W \u2190 \u2205; Calculate \u2126 as in (10); for t = 1, \u00b7 \u00b7 \u00b7 , T do\nFind the most violated class label tuple y\u2032t as in (14) by fixing wt\u22121j | m j=1; Add y\u2032t to the constrain active set W , W \u2190 W \u222a {y\u2032t}; for j = 1, \u00b7 \u00b7 \u00b7 ,m do\nUpdate wtj by solving (13) by fixing w t\u22121 j\u2032 |j\u2032 6=j and W ;\nend for end for Output: wTj | m j=1.\nREFERENCES\n[1] T. Joachims, \u201cA support vector method for multivariate performance measures,\u201d in Proceedings of the 22nd international conference on Machine learning. ACM, 2005, pp. 377\u2013384. [2] N. Li, I. W. Tsang, and Z.-H. Zhou, \u201cEfficient optimization of performance measures by classifier adaptation,\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 6, pp. 1370\u20131382, 2013. [3] Q. Mao and I.-H. Tsang, \u201cA feature selection method for multivariate performance measures,\u201d Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 9, pp. 2051\u20132063, 2013. [4] V. Sindhwani and D. S. Rosenberg, \u201cAn rkhs for multi-view learning and manifold co-regularization,\u201d in Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 976\u2013983. [5] S. Z. Li, L. Zhu, Z. Zhang, A. Blake, H. Zhang, and H. Shum, \u201cStatistical learning of multi-view face detection,\u201d in Computer Vision\u0142ECCV 2002. Springer, 2002, pp. 67\u201381. [6] C. Christoudias, R. Urtasun, and T. Darrell, \u201cMulti-view learning in the presence of view disagreement,\u201d arXiv preprint arXiv:1206.3242, 2012. [7] W. Li, L. Duan, I. W.-H. Tsang, and D. Xu, \u201cCo-labeling: A new multiview learning approach for ambiguous problems.\u201d in ICDM, 2012, pp. 419\u2013428. [8] J. E. Kelley, Jr, \u201cThe cutting-plane method for solving convex programs,\u201d Journal of the Society for Industrial & Applied Mathematics, vol. 8, no. 4, pp. 703\u2013712, 1960."}], "references": [{"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "Proceedings of the 22nd international conference on Machine learning. ACM, 2005, pp. 377\u2013384.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient optimization of performance measures by classifier adaptation", "author": ["N. Li", "I.W. Tsang", "Z.-H. Zhou"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 6, pp. 1370\u20131382, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "A feature selection method for multivariate performance measures", "author": ["Q. Mao", "I.-H. Tsang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 9, pp. 2051\u20132063, 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "An rkhs for multi-view learning and manifold co-regularization", "author": ["V. Sindhwani", "D.S. Rosenberg"], "venue": "Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 976\u2013983.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Statistical learning of multi-view face detection", "author": ["S.Z. Li", "L. Zhu", "Z. Zhang", "A. Blake", "H. Zhang", "H. Shum"], "venue": "Computer Vision\u0142ECCV 2002. Springer, 2002, pp. 67\u201381.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-view learning in the presence of view disagreement", "author": ["C. Christoudias", "R. Urtasun", "T. Darrell"], "venue": "arXiv preprint arXiv:1206.3242, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Co-labeling: A new multiview learning approach for ambiguous problems.", "author": ["W. Li", "L. Duan", "I.W.-H. Tsang", "D. Xu"], "venue": "in ICDM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The cutting-plane method for solving convex programs", "author": ["J.E. Kelley", "Jr"], "venue": "Journal of the Society for Industrial & Applied Mathematics, vol. 8, no. 4, pp. 703\u2013712, 1960.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1960}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION D IFFERENT multivariate performance measures are used to evaluate different machine learning applications [1].", "startOffset": 119, "endOffset": 122}, {"referenceID": 0, "context": "\u2022 Joachims [1] proposed a support vector machine (SVM)based for multivariate nonlinear performance measures optimization problems.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] proposed a two-step approach to optimize multivariate performance measures, by first training a nonlinear classifiers with existing learning methods, and then adapting it to optimize specific performance measures.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "\u2022 Mao and Tsang [3] proposed a novel feature selection method to optimize multivariate performance measures, by formulating the problem or high-dimensional data, and employing a two-layer cutting plane algorithm to solve it.", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "To learn from multiple views of data, different multi-view learning methods have been proposed [4], [5], [6], [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "To learn from multiple views of data, different multi-view learning methods have been proposed [4], [5], [6], [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 5, "context": "To learn from multiple views of data, different multi-view learning methods have been proposed [4], [5], [6], [7].", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "To learn from multiple views of data, different multi-view learning methods have been proposed [4], [5], [6], [7].", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "The minimization problem is optimized by a cutting-plane method in an alterative algorithm [8].", "startOffset": 91, "endOffset": 94}], "year": 2017, "abstractText": "In this paper, we propose the problem of optimizing multivariate performance measures from multi-view data, and an effective method to solve it. This problem has two features: the data points are presented by multiple views, and the target of learning is to optimize complex multivariate performance measures. We propose to learn a linear discriminant functions for each view, and combine them to construct a overall multivariate mapping function for mult-view data. To learn the parameters of the linear discriminant functions of different views to optimize multivariate performance measures, we formulate a optimization problem. In this problem, we propose to minimize the complexity of the linear discriminant functions of each view, encourage the consistences of the responses of different views over the same data points, and minimize the upper boundary of a given multivariate performance measure. To optimize this problem, we employ the cutting-plane method in an iterative algorithm. In each iteration, we update a set of constrains, and optimize the mapping function parameter of each view one by one.", "creator": "LaTeX with hyperref package"}}}