{"id": "1501.04284", "review": {"conference": "aaai", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2015", "title": "Pairwise Constraint Propagation on Multi-View Data", "abstract": "this paper presents a graph - gate based learning approach to pairwise constraint propagation rooted on multi - view data. although pairwise constraint vine propagation has been popularly studied extensively, pairwise constraints are usually defined over pairs of data points from for a single view, nowadays i. e., only an intra - view constraint propagation is considered for multi - vertex view tasks. in fact, very little attention has been paid to inter - visual view constraint propagation, which is more challenging since pairwise constraints are now defined over pairs set of data content points from different views. in this paper, we will propose to decompose the challenging inter - view constraint propagation problem into semi - supervised learning subproblems resulting so that they can be efficiently solved based on graph - link based label constraints propagation. still to the collective best of our mathematical knowledge, this is the first attempt to give an efficient solution to inter - view constraint propagation from a semi - directional supervised learning viewpoint. moreover, since graph - based label propagation has been uniquely adopted for basic optimization, we both develop two constrained graph construction methods for interview constraint propagation, which only differ in how the intra - view pairwise constraints are exploited. the experimental results in improving cross - view retrieval have shown the promising performance of our inter - view video constraint propagation.", "histories": [["v1", "Sun, 18 Jan 2015 11:52:21 GMT  (834kb)", "http://arxiv.org/abs/1501.04284v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["zhiwu lu", "liwei wang"], "accepted": true, "id": "1501.04284"}, "pdf": {"name": "1501.04284.pdf", "metadata": {"source": "CRF", "title": "Pairwise Constraint Propagation on Multi-View Data", "authors": ["Zhiwu Lu", "Liwei Wang"], "emails": ["zhiwu.lu@gmail.com).", "wanglw@cis.pku.edu.cn)."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 1.\n04 28\n4v 1\n[ cs\n.C V\n] 1\n8 Ja\nn 20\n15 1\nIndex Terms\u2014Pairwise constraint propagation, multi-view data, label propagation, graph construction, cross-view retrieval\nI. INTRODUCTION\nAs an alternative type of supervisory information easier to access than the class labels of data points, pairwise constraints are widely used for different machine learning tasks in the literature. To effectively exploit pairwise constraints for clustering or classification [1]\u2013[4], much attention has been paid to pairwise constraint propagation [5]\u2013[7]. Different from the method [8] which only adjusts the similarities between constrained data points, these approaches can propagate pairwise constraints to other similarities between unconstrained data points and thus achieve better results in most cases. More importantly, given that each pairwise constraint is actually defined over a pair of data points from a single view, these approaches can all be regarded as intra-view constraint propagation when multi-view data is concerned. Since we have to learn the relationships (must-link or cannot-link) between data points, intra-view constraint propagation is more challenging than the traditional label propagation [9]\u2013[14] whose goal is only to predict the labels of unlabeled data points.\nHowever, besides intra-view pairwise constraints, we may also have easy access to inter-view pairwise constraints in multi-view tasks such as cross-view retrieval [15], where each pairwise constraint is defined over a pair of data points\nZ. Lu is with the Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing 100872, China (e-mail: zhiwu.lu@gmail.com).\nL. Wang is with the Key Laboratory of Machine Perception (MOE), School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China (e-mail: wanglw@cis.pku.edu.cn).\nfrom different views (see Fig. 1). In this case, inter-view pairwise constraints still specify the must-link or cannotlink relationships between data points. Since the similarity of two data points from different views is commonly unknown in practice, inter-view constraint propagation is significantly more challenging than intra-view constraint propagation. In fact, very little attention has been paid to inter-view constraint propagation for multi-view tasks in the literature. Although pairwise constraint propagation has been successfully applied to multi-view clustering in [16], [17], only intra-view pairwise constraints are propagated across different views. Here, it should be noted that these two constraint propagation methods have actually ignored the concept of inter-view pairwise constraints or the strategy of inter-view constraint propagation.\nSince multi-view data can be readily decomposed into a series of two-view data, we focus on inter-view constraint propagation only across two views in this paper. However, such inter-view constraint propagation remains a rather challenging task. Fortunately, from a semi-supervised learning viewpoint, we can formulate inter-view constraint propagation as minimizing a regularized energy functional. Specifically, we first decompose the inter-view constraint propagation problem into a set of independent semi-supervised learning [9]\u2013[12] subproblems. Through formulating these subproblems uniformly as minimizing a regularized energy functional, we thus develop an efficient algorithm for inter-view constraint propagation based on the traditional graph-based label propagation technique [9]. In summary, we succeed in giving an insightful explanation of inter-view constraint propagation from a graphbased semi-supervised learning viewpoint.\nHowever, since graph-based label propagation has been adopted for basic optimization, there remains one problem to be concerned in inter-view constraint propagation, i.e., how to exploit intra-view pairwise constraints for graph construction within each view. In this paper, we develop two constrained graph construction methods for inter-view constraint propagation, which only differ in how the intra-view pairwise constraints are exploited. The first method limits our interview constraint propagation to a single view and then utilize the constraint propagation results to adjust the weight matrix of each view, while the second method formulates graph construction as sparse representation and then directly add the intra-view pairwise constraints into sparse representation.\nThe flowchart of our inter-view constraint propagation with constrained graph construction is illustrated in Fig. 1, where only two views (i.e. text and image) are considered. It should be noted that, when multiple views refer to text, image, audio and so on, the output of our inter-view constraint propagation actually denotes the correlation between different media views. That is, the proposed algorithm can be directly used for cross-view retrieval (also see examples in Fig. 3) which has\ndrawn much attention recently [15]. For cross-view retrieval, it is not feasible to combine multiple views just as previous multi-view retrieval methods [18], [19]. More notably, the two closely related methods [16], [17] for multi-view clustering are actually incompetent for cross-view retrieval.\nFinally, to emphasize our main contributions, we summarize the following distinct advantages of our pairwise constraint propagation on multi-view data:\n\u2022 We have made the first attempt to give an efficient solution to inter-view constraint propagation from a graphbased semi-supervised learning viewpoint. \u2022 We have developed two constrained graph construction methods so that the intra-view pairwise constraints can also be exploited for inter-view constraint propagation. \u2022 When applied to cross-view retrieval, our inter-view constraint propagation has been shown to achieve promising results with respect to the state-of-the-art. \u2022 Although only evaluated in cross-view retrieval, our interview constraint propagation can be readily extended to many other multi-view tasks.\nThe remainder of this paper is organized as follows. In Section II, we formulate inter-view constraint propagation from a semi-supervised learning viewpoint. In Section III, we develop two constrained graph construction methods for our inter-view constraint propagation. In section IV, our interview constraint propagation is applied to cross-view retrieval. Finally, Sections V and VI provide the experimental results and conclusions, respectively.\nII. INTER-VIEW CONSTRAINT PROPAGATION\nIn this section, we first formulate inter-view constraint propagation as minimizing a regularized energy functional from a semi-supervised learning viewpoint. Furthermore, we develop an efficient algorithm for inter-view constraint propagation based on the label propagation technique [9]."}, {"heading": "A. Problem Formulation", "text": "Given a set of inter-view pairwise constraints defined over pairs of data points from different views, the goal of inter-view constraint propagation is to learn the cross-view relationships from these initial pairwise constraints. Since the similarity of two data points from different views is unknown in practice, inter-view constraint propagation on multi-view data is much\nmore challenging than the traditional pairwise constraint propagation over a single view. Considering that this multi-view problem can be readily decomposed into a series of two-view subproblems, we focus on inter-view constraint propagation on two-view data in the following.\nLet {X ,Y} be a two-view dataset, where X = {x1, ..., xN} and Y = {y1, ..., yM}. It should be noted that we may have N 6= M . As an example, a two-view dataset is shown in Fig. 1, with image and text being the two different views. For the two-view dataset {X ,Y}, we can define a set of initial must-link constraints as M = {(xi, yj) : l(xi) = l(yj)} and a set of initial cannot-link constraints as C = {(xi, yj) : l(xi) 6= l(yj)}, where l(xi) (or l(yj)) is the class label of xi \u2208 X (or yj \u2208 Y). Here, the two data points xi and yj are assumed to share the same class label set. If the class labels are not provided, the inter-view pairwise constraints can be defined only based on the correspondence between two views, which can be readily obtained from Web-based content (e.g. Wikipedia articles). Several examples of inter-view pairwise constraints are illustrated in Fig. 1.\nWe can now state that the goal of inter-view constraint propagation is to propagate the two sets of initial pairwise constraints M and C across both X and Y . In fact, this is equivalent to deriving the best solution F \u2217 \u2208 F from both M and C, with F = {F = {fij}N\u00d7M}. Here, any exhaustive set of inter-view pairwise constraints is denoted as F \u2208 F , where fij > 0 means (xi, yj) is a must-link constraint while fij < 0 means (xi, yj) is a cannot-link constraint, with |fij | denoting the confidence score of (xi, yj) being a must-link (or cannot-link) constraint. Hence, F can actually be regarded as the feasible solution set of inter-view constraint propagation.\nAlthough it is difficult to directly find the best solution F \u2217 \u2208 F to inter-view constraint propagation, we can tackle this challenging problem by decomposing it into a set of independent semi-supervised learning subproblems. More concretely, we first denote the two sets of initial pairwise constraints M and C with a single matrix Z = {zij}N\u00d7M :\nzij =\n\n \n \n+1, (xi, yj) \u2208 M;\n\u22121, (xi, yj) \u2208 C;\n0, otherwise.\n(1)\nMoreover, by making vertical and horizontal observations on such initial matrix Z , we decompose the inter-view constraint propagation problem into independent semi-supervised learning subproblems, which is also illustrated in Fig. 2. Finally, given two graphs GX = {X ,WX } and GY = {Y,WY} constructed over {X ,Y} with WX (or WY ) being the edge weight matrix defined over the vertex set X (or Y), we utilize the graph-based label propagation method [9] to uniformly solve these semi-supervised learning subproblems:\nmin FX ,FY\n\u2016FX \u2212 Z\u2016 2 fro + \u00b5X tr(F T X LXFX ) + \u2016FY \u2212 Z\u2016 2 fro\n+\u00b5Ytr(FYLYF T Y ) + \u03b3\u2016FX \u2212 FY\u2016 2 fro, (2)\nwhere \u00b5X > 0 (\u00b5Y > 0, or \u03b3 > 0) denotes the regularization parameter, LX (or LY ) denotes the normalized Laplacian matrix defined over X (or Y), || \u00b7 ||fro denotes the Frobenius norm of a matrix, and tr(\u00b7) denotes the trace of a matrix.\n3 0 -1 0 1 0 0 1 0\n-1 0 1 -1 0 0 0 0\n1 -1 0 0 0 -1 0 1\n0 1 0 0 0 0 -1 0\n0 0 0 1 0 0 0 -1\ny1 y2 y3 y4 y5 y6 y7 y8\nx1\nx2\nx3\nx4\nx5\nmust-link: 1 cannot-link: -1\nFig. 2. Illustration of the initial matrix Z . When we focus on a single pair of data points, e.g. (x3, y4) here, the inter-view constraint propagation can be viewed as a two-class semi-supervised learning problem (in name only) in both vertical and horizontal directions, where +1 (or -1) denotes positive (or negative) labeled data and 0 denotes unlabeled data.\nThe first and second terms of the above objective function are related to the pairwise constraint propagation over X , while the third and fourth terms are related to the pairwise constraint propagation over Y . Moreover, the fifth term can ensure that the solutions of these two types of pairwise constraint propagation are as approximate as possible. Let F \u2217X and F \u2217 Y be the best solutions of pairwise constraint propagation over X and Y , respectively. The best solution of our inter-view constraint propagation is defined as follows:\nF \u2217 = (F \u2217X + F \u2217 Y)/2. (3)\nAs for the second and fourth terms, they are known as the energy functional [10] (or smoothness) defined over X and Y . In summary, we have formulated intere-view constraint propagation as minimizing a regularized energy functional."}, {"heading": "B. Efficient Algorithm", "text": "Let Q(FX , FY) denote the objective function in equation (2). The alternate optimization technique can be adopted to solve minFX ,FY Q(FX , FY) as follows: 1) Fix FY = F \u2217 Y , and find F \u2217X = argminFX Q(FX , F \u2217 Y); 2) Fix FX = F \u2217 X , and find F \u2217Y = argminFY Q(F \u2217 X , FY). Pairwise Constraint Propagation over X : When FY is fixed at F \u2217Y , the solution of minFX Q(FX , F \u2217 Y) can be found by solving the following linear equation\n\u2202Q(FX , F \u2217Y)\n2\u2202FX = (FX \u2212 Z) + \u00b5XLXFX + \u03b3(FX \u2212 F\n\u2217 Y) = 0,\nwhich can be equivalently transformed into:\n(I + \u00b5\u0302XLX )FX = (1\u2212 \u03b2)Z + \u03b2F \u2217 Y , (4)\nwhere \u00b5\u0302X = \u00b5X /(1+\u03b3) and \u03b2 = \u03b3/(1+\u03b3). Since I+ \u00b5\u0302XLX is positive definite, we then obtain an analytical solution:\nF \u2217X = (I + \u00b5\u0302XLX ) \u22121((1\u2212 \u03b2)Z + \u03b2F \u2217Y). (5)\nHowever, this analytical solution is not efficient for large datasets, since matrix inverse has a time cost of O(N3). Fortunately, equation (4) can also be efficiently found using label propagation [9] with k-nearest neighbor (k-NN) graph. Pairwise Constraint Propagation over Y: When FX is fixed at F \u2217X , the solution of minFY Q(F \u2217 X , FY) can be found by solving the following linear equation\n\u2202Q(F \u2217X , FY)\n2\u2202FY = (FY \u2212 Z) + \u00b5YFYLY + \u03b3(FY \u2212 F\n\u2217 X ) = 0,\nwhich can be equivalently transformed into:\nFY(I + \u00b5\u0302YLY) = (1\u2212 \u03b2)Z + \u03b2F \u2217 X , (6)\nwhere \u00b5\u0302Y = \u00b5Y/(1+\u03b3) and \u03b2 = \u03b3/(1+\u03b3). Since I+ \u00b5\u0302YLY is positive definite, we then obtain an analytical solution:\nF \u2217Y = ((1\u2212 \u03b2)Z + \u03b2F \u2217 X )(I + \u00b5\u0302YLY) \u22121, (7)\nwhich involves time-consuming matrix inverse. In fact, the linear equation (6) can also be efficiently solved using label propagation [9] with k-NN graph.\nLet WX (or WY ) denote the weight matrix of the k-NN graph constructed over X (or Y). The complete algorithm for inter-view constraint propagation is summarized as follows:\n(1) Compute two matrices SX = D \u22121/2 X WXD \u22121/2 X and\nSY = D \u22121/2 Y WYD \u22121/2 Y , where DX (or DY ) is a diagonal matrix with its i-th diagonal entry being the sum of the i-th row of WX (or WY );\n(2) Initialize FX (0) = 0, F \u2217Y = 0, and FY(0) = 0; (3) Iterate FX (t + 1) = \u03b1XSXFX (t) + (1 \u2212 \u03b1X )((1 \u2212\n\u03b2)Z + \u03b2F \u2217Y) until convergence at F \u2217 X , where \u03b1X =\n\u00b5\u0302X /(1 + \u00b5\u0302X ) and \u03b2 = \u03b3/(1 + \u03b3); (4) Iterate FY(t + 1) = \u03b1YFY(t)SY + (1 \u2212 \u03b1Y)((1 \u2212\n\u03b2)Z + \u03b2F \u2217X ) until convergence at F \u2217 Y , where \u03b1Y =\n\u00b5\u0302Y/(1 + \u00b5\u0302Y); (5) Iterate Steps (3)\u2013(4) until convergence, and output\nthe final solution F \u2217 = (F \u2217X + F \u2217 Y)/2.\nAccording to the convergence analysis in [9], Step (3) converges to F \u2217X = (1\u2212\u03b1)(I \u2212\u03b1XSX )\n\u22121((1\u2212 \u03b2)Z + \u03b2F \u2217Y), equal to the solution (5) given that \u03b1X = \u00b5\u0302X /(1 + \u00b5\u0302X ) and SX = I \u2212 LX . Similarly, Step (4) converges to F \u2217Y = (1\u2212\u03b1)((1\u2212\u03b2)Z+\u03b2F \u2217X )(I\u2212\u03b1YSY)\n\u22121, equal to the solution (7) given that \u03b1Y = \u00b5\u0302Y/(1 + \u00b5\u0302Y) and SY = I \u2212 LY . In the experiments, we find that Steps (3)\u2013(5) generally converge in very limited iterations (<10). Moreover, based on k-NN graphs, the above inter-view constraint propagation algorithm has a time cost of O(kNM), which is proportional to the number of all possible inter-view pairwise constraints. Hence, we consider that this algorithm can provide an efficient solution to inter-view constraint propagation (note that even a simple assignment operator on F \u2217 incurs a time cost of O(NM))."}, {"heading": "III. CONSTRAINED GRAPH CONSTRUCTION", "text": "In the last section, we have just developed an efficient interview constraint propagation algorithm based on the graphbased label propagation technique. However, since graphbased label propagation has been adopted as a basic optimization technique, there remains one problem to be concerned in inter-view constraint propagation, i.e., how to exploit intraview pairwise constraints for graph construction within each view. In this section, we then develop two constrained graph construction methods for inter-view constraint propagation, which only differ in how the intra-view pairwise constraints are exploited. To ensure our inter-view constraint propagation algorithm runs efficiently even on large datasets, we utilize the traditional k-NN graph construction as the basis of our constrained graph construction, i.e., the obtained two constrained graphs can be considered as the variants of k-NN graph. In the\n4 following, we will only elaborate how to construct the graph GX = {X ,WX } over X . The graph GY = {Y,WY} over Y can be constructed exactly in the same way."}, {"heading": "A. Constrained Weight Adjustment", "text": "The first constrained graph construction method limits our inter-view constraint propagation proposed in Section II to a single view (i.e. intra-view constraint propagation over X ) and then utilize the obtained results of intra-view constraint propagation to adjust the weight matrix, which is thus called as constrained weight adjustment (CWA). According to the convergence analysis in Section II-B, we construct a k-NN graph over X to speed up our intra-view constraint propagation.\n1) Intra-View Constraint Propagation: We have just provided a sound solution to the challenging problem of intraview constraint propagation in Section II. In this subsection, we further consider pairwise constraint propagation over a single view, where each pairwise constraint is defined over a pair of data points from the same view. In fact, this intraview constraint propagation problem can also be solved from a semi-supervised learning viewpoint by limiting our inter-view constraint propagation to a single view.\nGiven the dataset X = {x1, ..., xN}, we denote the set of initial must-link constraints as MX = {(xi, xj) : li = lj} and the set of initial cannot-link constraints as CX = {(xi, xj) : li 6= lj}, where li is the label of data point xi. Similar to our representation of the initial inter-view pairwise constraints, we first denote the initial intra-view pairwise constraints MX and CX with a single matrix ZX = {z (x) ij }N\u00d7N :\nz (x) ij =\n\n \n \n+1, (xi, xj) \u2208 MX ;\n\u22121, (xi, xj) \u2208 CX ;\n0, otherwise.\n(8)\nFurthermore, by making vertical and horizontal observations on ZX , we further decompose the intra-view constraint propagation problem into semi-supervised learning subproblems, just as our interpretation of inter-view constraint propagation from a semi-supervised learning viewpoint. These subproblems can be similarly merged to a single optimization problem (similar to [20]\u2013[22]):\nmin Fv,Fh\n\u2016Fv \u2212 ZX \u2016 2 fro + \u00b5tr(F T v LXFv) + \u2016Fh \u2212 ZX \u2016 2 fro\n+\u00b5tr(FhLXF T h ) + \u03b3\u2016Fv \u2212 Fh\u2016 2 fro, (9)\nwhere \u00b5 > 0 (or \u03b3 > 0) denotes the regularization parameter, and LX denotes the normalized Laplacian matrix defined over the k-NN graph. The second and fourth terms of the above equation denote the energy functional [10] (or the smoothness measure) defined over X . In summary, we have also formulated intra-view constraint propagation as minimizing a regularized energy functional.\nSimilar to what we have done for solving equation (2), we can adopt the alternate optimization technique to find the best solution to the above intra-view constraint propagation problem. Let WX denote the weight matrix of the k-NN graph constructed over the dataset X . The proposed algorithm for our intra-view constraint propagation is outlined as follows:\n(1) Compute SX = D\u2212 1 2WXD \u2212 12 , where D is a diagonal matrix with its entry (i, i) being the sum of row i of WX ; (2) Initialize Fv(0) = 0, F \u2217h = 0, and Fh(0) = 0; (3) Iterate Fv(t+1) = \u03b1SXFv(t)+(1\u2212\u03b1)((1\u2212\u03b2)ZX+\n\u03b2F \u2217h ) until convergence at F \u2217 v , where \u03b1 = \u00b5/(1 +\n\u00b5+ \u03b3) and \u03b2 = \u03b3/(1 + \u03b3); (4) Iterate Fh(t+1) = \u03b1Fh(t)SX +(1\u2212\u03b1)((1\u2212\u03b2)ZX+\n\u03b2F \u2217v ) until convergence at F \u2217 h ;\n(5) Iterate Steps (3)\u2013(4) until the stopping condition is satisfied, and obtain F \u2217 = (F \u2217v + F \u2217 h )/2. (6) Output the normalized solution F \u2217 = F \u2217/F \u2217max, where F \u2217max denotes the maximum entry of F \u2217. In the experiments, we find that Steps (3)\u2013(5) generally converge in very limited iterations (<10). Moreover, based on k-NN graph, our algorithm has a time cost of O(kN2) proportional to the number of all possible pairwise constraints. Hence, it can be considered to provide an efficient solution.\n2) Weight Adjustment Using Propagated Constraints: It should be noted that the normalized output F \u2217 = {f\u2217ij}N\u00d7N of our intra-view constraint propagation represents an exhaustive set of intra-view pairwise constraints. Our original motivation is to construct a new graph over X that is fully consistent with F \u2217. In fact, we can exploit F \u2217 for such graph construction by adjusting the original normalized weight matrix WX (i.e. 0 \u2264 w\n(x) ij \u2264 1) just as [20]:\nw\u0303 (x) ij =\n{\n1\u2212 (1\u2212 f\u2217ij)(1 \u2212 w (x) ij ), f \u2217 ij \u2265 0; (1 + f\u2217ij)w (x) ij , f \u2217 ij < 0.\n(10)\nSince W\u0303X = {w\u0303 (x) ij }N\u00d7N is nonnegative and symmetric, we then use it as the new weight matrix. Moreover, we can find that W\u0303 (x)ij \u2265 W (x) ij (or < W (x) ij ) if F \u2217 ij \u2265 0 (or < 0). That is, the new weight matrix W\u0303X is derived from the original weight matrix WX by increasing W (x) ij for the must-link constraints with F \u2217ij > 0 and decreasing W (x) ij for the cannotlink constraints with F \u2217ij < 0. This is entirely consistent with our original motivation of exploiting intra-view pairwise constraints for graph construction.\nOnce we have constructed the new weight matrix W\u0303X over X , we can similarly construct the new weight matrix W\u0303Y over Y . Based on these two new weight matrices, our interview constraint propagation can be performed with constrained graph construction (CGC) (as shown in Fig. 1) using constrained weight adjustment (CWA) developed here."}, {"heading": "B. Constrained Sparse Representation", "text": "The second constrained graph construction method formulates graph construction as sparse representation [23]\u2013[25] and then directly add the intra-view pairwise constraints into sparse representation, which is thus called as constrained sparse representation (CSR). Our work is mainly inspired by recent effort to exploit sparse representation for graph construction, i.e., L1-graph construction [26], [27]. The basic idea of L1graph construction is to seek a sparse linear reconstruction of each data point with the other data points. However, such L1-graph construction may become infeasible since it incurs\n5 too much time cost given a large data size N . Hence, we only consider the k nearest neighbors of each data point for its sparse linear reconstruction, which thus becomes a much smaller scale optimization problem (k \u226a N ). More notably, due to such neighborhood limitation, the obtained L1-graph is actually a variant of k-NN graph, which can ensure that our inter-view constraint propagation proposed in Section II runs efficiently on large datasets. Finally, to exploit intraview pairwise constraints for L1-graph construction, we seek a constrained sparse linear reconstruction of each data point.\n1) L1-Graph Construction with Sparse Representation: We start with the problem formulation for sparse linear reconstruction of each data point in its k-nearest neighborhood. Given a data point xi \u2208 X , we suppose it can be reconstructed using its k-nearest neighbors (their indices are collected into Nk(i)), which results in an underdetermined linear system: xi = Bi\u03b1i, where \u03b1i \u2208 Rk is a vector that stores unknown reconstruction coefficients, and Bi = [xj ]j\u2208Nk(i) is an overcomplete dictionary with k bases. According to [23], if the solution for xi is sparse enough, it can be recovered by:\nmin \u03b1i ||\u03b1i||1, s.t. xi = Bi\u03b1i, (11)\nwhere ||\u03b1i||1 is the L1-norm of \u03b1i. Given the kernel (affinity) matrix A = {aij}N\u00d7N computed over X , we make use of the kernel trick and transform the above problem into:\nmin \u03b1i ||\u03b1i||1, s.t. x\u0302i = Ci\u03b1i, (12)\nwhere x\u0302i = [aji]j\u2208Nk(i) \u2208 R k, Ci = [ajj\u2032 ]j,j\u2032\u2208Nk(i) \u2208 R k\u00d7k. In practice, due to the noise in the data, we can reconstruct x\u0302i similar to [24]: x\u0302i = Ci\u03b1i+ \u03b6i, where \u03b6i is the noise term. The above L1-optimization problem can then be redefined by minimizing the L1-norm of both reconstruction coefficients and reconstruction error:\nmin \u03b1\u2032\ni\n||\u03b1\u2032i||1, s.t. x\u0302i = C \u2032 i\u03b1 \u2032 i, (13)\nwhere C\u2032i = [Ci, I] \u2208 R k\u00d72k and \u03b1\u2032i = [\u03b1 T i , \u03b6 T i ] T . This convex optimization can be solved by general linear programming and has a globally optimal solution.\nAfter we have obtained the reconstruction coefficients for all the data points by the above sparse linear reconstruction, the weight matrix WX = {w (x) ij }N\u00d7N can be defined by:\nw (x) ij =\n{\n|\u03b1\u2032i(j \u2032)|, j \u2208 Nk(i), j \u2032 = index(j,Nk(i)); 0, otherwise, (14)\nwhere \u03b1\u2032i(j \u2032) denotes the j\u2032-th element of the vector \u03b1\u2032i, and j\u2032 = index(j,Nk(i)) means that j is the j\u2032-th element of the set Nk(i). By setting the weight matrix WX = (WX+WTX )/2, we construct a graph GX = {X ,WX } over X , which is called as L1-graph since it is constructed by L1-optimization.\n2) L1-Norm Laplacian Regularization with Intra-View Pairwise Constraints: In the above L1-graph construction, we have ignored intra-view pairwise constraints (see examples in Fig. 1). In fact, this supervisory information can be exploited for L1-graph construction through Laplacian regularization [9], [10]. Our basic idea is to first derive Laplacian regularization from intra-view pairwise constraints and then incorporate\nthis constrained term into sparse linear reconstruction (the key step of L1-graph construction). In the following, we will first elaborate how to derive a new Laplacian regularization term from intra-view pairwise constraints.\nGiven a set of intra-view must-link constraints MX and a set of intra-view cannot-link constraints CX defined over X , we can represent both MX and CX using a single matrix ZX = {z (x) ij }N\u00d7N exactly the same as equation (8). The normalized Laplacian matrix limited to the k-nearest neighborhood of data point xi can thus be defined as:\nLi = I \u2212D \u22121/2 i (1 + Zi)D \u22121/2 i , (15)\nwhere Zi = [z (x) jj\u2032 ]j,j\u2032\u2208Nk(i) \u2208 R k\u00d7k, and Di is a diagonal matrix with its j-th diagonal element being the sum of the j-th row of 1+Zi. Here, we define the similarity matrix (i.e. 1+Zi) limited to the k-nearest neighborhood Nk(i) of xi based on the intra-view pairwise constraints stored in ZX . From this normalized Laplacian matrix Li, we can derive the Laplacian regularization term for the sparse representation problem (12) as \u03b1Ti Li\u03b1i, the same as the original definition in [9].\nHowever, we have difficulty in directly incorporating this Laplacian regularization term into the sparse representation problem (12), no matter as a part of the objective function or a constraint condition. Hence, we further formulate an L1norm version of Laplacian regularization [12], [28]\u2013[30]:\n||C\u0303i\u03b1i||1 = ||\u03a3 1 2 i V T i \u03b1i||1, (16)\nwhere C\u0303i = \u03a3 1 2 i V T i , Vi is a k \u00d7 k orthonormal matrix with each column being an eigenvector of Li, and \u03a3i is a k \u00d7 k diagonal matrix with its diagonal element \u03a3i(j, j) being an eigenvalue of Li (sorted as \u03a3i(1, 1) \u2264 ... \u2264 \u03a3i(k, k)). Given that Li is nonnegative definite, \u03a3i \u2265 0 (i.e. all the eigenvalues \u2265 0). Since LiVi = Vi\u03a3i and Vi is orthonormal, we have Li = Vi\u03a3iV Ti . Hence, the original Laplacian regularization \u03b1Ti Li\u03b1i can be reformulated as:\n\u03b1Ti Li\u03b1i = \u03b1 T i Vi\u03a3\n1 2 i \u03a3 1 2 i V T i \u03b1i = ||C\u0303i\u03b1i|| 2 2, (17)\nwhich means that our new formulation ||C\u0303i\u03b1i||1 can indeed be regarded as an L1-norm version of the original Laplacian regularization \u03b1Ti Li\u03b1i = ||C\u0303i\u03b1i|| 2 2.\n3) L1-Graph Construction with L1-Norm Laplacian Regularization: After we have formulated L1-norm Laplacian regularization based on intra-view pairwise constraints, we can further incorporate this constrained term into sparse linear reconstruction used for L1-graph construction. More concretely, by introducing noise terms for linear reconstruction and L1-norm Laplacian regularization, we transform the sparse representation problem (12) into\nmin \u03b1i,\u03b6i,\u03bei\n||[\u03b1Ti , \u03b6 T i , \u03be T i ]||1,\ns.t. x\u0302i = Ci\u03b1i + \u03b6i, 0 = C\u0303i\u03b1i + \u03bei, (18)\nwhere the reconstruction error and Laplacian regularization with respect to \u03b1i are controlled by \u03b6i and \u03bei, respectively.\nLet \u03b1\u2032i = [\u03b1 T i , \u03b6 T i , \u03be T i ] T , C\u2032i =\n[\nCi I 0 C\u0303i 0 I\n]\n, and x\u0302\u2032i =\n6 [x\u0302Ti , 0 T ]T . We finally solve the following constrained spare representation problem for L1-graph construction:\nmin \u03b1\u2032\ni\n||\u03b1\u2032i||1, s.t. x\u0302 \u2032 i = C \u2032 i\u03b1 \u2032 i, (19)\nwhich takes the same form as the original spare representation problem (13). Here, it is noteworthy that this constrained spare representation (CSR) problem can be solved very efficiently, since it is limited to k-nearest neighborhood. The weight matrix WX of the L1-graph GX = {X ,WX } can be defined the same as equation (14).\nIn our CSR formulation, the L1-norm Laplacian regularization can be smoothly incorporated into the original sparse representation problem (12). However, this is not true for the traditional Laplacian regularization [9], [10], which may introduce extra parameters (hard to tune in practice) into the L1-optimization for sparse representation. Meanwhile, our L1-norm Laplacian regularization can induce another type of sparsity (see the extra noise term \u03bei), which can not be ensured by the traditional Laplacian regularization. Moreover, the p-Laplacian regularization [31] can also be regarded as an ordinary L1-generalization of the Laplacian regularization when p = 1. According to [32], by defining a matrix Cp \u2208 R k(k\u22121) 2 \u00d7k, the p-Laplacian regularization can be formulated as ||Cp\u03b1i||1, similar to our L1-norm Laplacian regularization. Hence, we can similarly apply the p-Laplacian regularization with p = 1 to constrained spare representation. However, such Laplacian regularization incurs large time cost due to the large matrix Cp even for small neighborhood size (e.g. k = 90).\nOnce we have constructed the L1-graph GX = {X ,WX } over X , we can similarly construct the L1-graph GY = {Y,WY} over Y . Based on the two weight matrices, our interview constraint propagation can be performed with constrained graph construction (CGC) (as shown in Fig. 1) using constrained sparse representation (CSR) developed here."}, {"heading": "IV. APPLICATION TO CROSS-VIEW RETRIEVAL", "text": "When multiple views refer to text, image, audio and so on (see Fig. 3), the output of our inter-view constraint propagation actually can be viewed as the correlation between different media views. As we have mentioned, given the output F \u2217 = {f\u2217ij}N\u00d7M of our inter-view constraint propagation, (xi, yj) denotes a must-link (or cannot-link) constraint if f\u2217ij > 0 (or < 0). Considering the inherent meanings of must-link and cannot-link constraints, we can state that: xi and yj are \u201cpositively correlated\u201d if f\u2217ij > 0, while they are \u201cnegatively correlated\u201d if f\u2217ij < 0. Hence, we can view f \u2217 ij as the correlation coefficient between xi and yj . The distinct advantage of such interpretation of F \u2217 as a correlation measure is that F \u2217 can thus be used for ranking on Y given a query xi or ranking on X given a query yj . In fact, this is just the goal of cross-view retrieval which has drawn much attention recently [15]. That is, such task can be directly handled by our inter-view constraint propagation.\nIn this paper, we focus on a special case of cross-view retrieval, i.e. only text and image views are considered. In this case, cross-view retrieval is somewhat similar to automatic image annotation [33]\u2013[36] and image caption generation\n[37]\u2013[39], since these three tasks all aim to learn the relations between the text and image views. However, even if only text and image views are considered, cross-view retrieval is still quite different from automatic image annotation and image caption generation. More concretely, automatic image annotation relies on very limited types of textual representations and mainly associates images only with textual keywords, while cross-view retrieval is designed to deal with much more richly annotated data, motivated by the ongoing explosion of Webbased content such as news archives and Wikipedia pages. Similar to cross-view retrieval, image caption generation can also deal with more richly annotated data (i.e. captions) with respect to the textual keywords concerned in automatic image annotation. However, this task tends to model image captions as sentences by exploiting certain prior knowledge (e.g. the <object, action, scene> triplets used in [37]), different from cross-view retrieval that focuses on associating images with complete text articles using no prior knowledge from the text view (any general textual representations are applicable actually once their similarities are provided).\nIn the context of cross-view retrieval, one notable recent work is [15] which first learns the correlation between the text and image views with canonical correlation analysis (CCA) [40] and then achieves the abstraction by representing text and image at a more general semantic level. However, two separate steps, i.e. correlation analysis (CA) and semantic abstraction (SA), are involved in this modeling, and the use of semantic abstraction after CCA (i.e. CA+SA) seems rather ad hoc. Fortunately, this problem can be completely addressed by our inter-view constraint propagation (Inter-CP). The semantic information (e.g. class labels) associated with images and text can be used to define the initial must-link and cannot-link constraints based on the training dataset, while the correlation between text and image views can be explicitly learnt by the proposed algorithm in Section II. That is, the correlation analysis and semantic abstraction has been successfully in-\ntegrated in our inter-view constraint propagation framework. The effectiveness of such integration as compared to CA+SA [15] is preliminarily verified by several cross-view retrieval examples shown in Fig. 3. Further verification will be provided in our later experiments. More notably, although only tested in cross-view retrieval, our inter-view constraint propagation can be readily extended to other multi-view tasks, since it has actually learnt the correlation between different views."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In this section, our inter-view constraint propagation (InterCP) algorithm is evaluated in the challenging application of cross-view retrieval. We focus on comparing our Inter-CP algorithm with the state-of-the-art approach [15], since they both consider not only correlation analysis (CA) but also semantic abstraction (SA) for text and image views. Moreover, we also make comparison with another two closely related approaches that integrate CA and SA for cross-view retrieval similar to [15] but perform correlation analysis by partial least squares (PLS) [41] and cross-modal factor analysis (CFA) [42] instead of CCA, respectively. In the following, these two CA+SA approaches are denoted as CA+SA (PLS) and CA+SA (CFA), while the state-of-the-art approach [15] is denoted as CA+SA (CCA). Finally, to show the effectiveness of constrained graph construction, we construct four types of graphs for our Inter-CP algorithm: k-NN graph (k-NN), L1graph using sparse representation (SR), k-NN graph using constrained weight adjustment (CWA), and L1-graph using constrained sparse representation (CSR)."}, {"heading": "A. Experimental Setup", "text": "We select two different datasets for performance evaluation. The first one is a Wikipedia benchmark dataset [15], which contains a total of 2,866 documents derived from Wikipedia\u2019s \u201cfeatured articles\u201d. Each document is actually a text-image pair, annotated with a label from the vocabulary of 10 semantic classes. This benchmark dataset [15] is split into a training set of 2,173 documents and a test set of 693 documents. Moreover, the second dataset consists of totally 8,564 documents crawled from the photo sharing website Flickr. The image and text views of each document denote a photo and a set of tags provided by the users, respectively. Although such text presentation does not take a free form as that for the Wikipedia dataset, it is rather noisy since many of the tags may be incorrectly annotated by the users. This Flickr dataset is organized into 11 semantic classes. We split it into a training set of 4,282 documents and a test set of the same size.\nFor the above two datasets, we take the same strategy as [15] to generate both text and image representation. More concretely, in the Wikipedia dataset, the text representation for each document is derived from a latent Dirichlet allocation model with 10 latent topics, while the image representation is based on a bag-of-words model with 128 visual words learnt from the extracted SIFT descriptors, just as [15]. Moreover, for the Flickr dataset, we generate the text and image representation similarly, and the main difference is that we select a relatively large visual vocabulary (of the size 2,000) for image representation and refine the noisy textual vocabulary to the size 1,000 by a preprocessing step for text representation.\nIn our experiments, the intra-view pairwise constraints used for our CGC and inter-view pairwise constraints used for our Inter-CP are initially derived from the class labels of the training documents of each dataset. The performance of our Inter-CP with CGC is evaluated on the test set. Here, two tasks of cross-view retrieval are considered: text retrieval using an image query, and image retrieval using a text query. In the following, these two tasks are denoted as \u201cImage Query\u201d and \u201cText Query\u201d, respectively. For each task, the retrieval results are measured with mean average precision (MAP) which has been widely used in the image retrieval literature [18].\nLet X denote the text representation and Y denote the image representation. For our Inter-CP algorithm, we perform CGC over X and Y with the same k. The parameters of our InterCP algorithm with CGC can be selected by fivefold crossvalidation on the training set. For example, according to Fig. 4, we set the parameters of our Inter-CP (CSR is used for CGC) on the Wikipedia dataset as: \u03b1X = 0.025, \u03b1Y = 0.025, \u03b2 = 0.95, and k = 90. It is noteworthy that our Inter-CP with CSR is not sensitive to these parameters. Moreover, the parameters of our Inter-CP with CWA can be similarly set to their respective optimal values. To summarize, we have selected the best values for all the parameters of our UCP algorithm with CGC by cross-validation on the training set. For fair comparison, we take the same parameter selection strategy for other closely related algorithms."}, {"heading": "B. Retrieval Results", "text": "The cross-view retrieval results on the two datasets are listed in Tables I and II, respectively. The immediate observation is that we can achieve the best results when both intra-view and inter-view pairwise constraints are exploited by InterCP+CWA (or Inter-CP+CSR). This means that our Inter-CP with CGC can most effectively exploit the initial supervisory information provided for cross-view retrieval. As compared\n8\nto the three CA+SA approaches by semantic abstraction after correlation analysis (via PLS, CFA, or CCA), our Inter-CP can seamlessly integrate these two separate steps and then lead to much better results. Moreover, the effectiveness of our CGC is verified by the comparison Inter-CP+CWA vs. InterCP+k-NN (or Inter-CP+CSR vs. Inter-CP+SR), especially on the Flickr dataset. As for our two CGC methods, CSR is shown to perform better than CWA, which is mainly due to the noiserobustness property of sparse representation.\nIt should be noted that our Inter-CP algorithm can be considered to provide an efficient solution, since it has a time cost proportional to the number of all possible pairwise constraints. This is also verified by our observations in the experiments. For example, the running time taken by CA+SA (CCA, CFA or PLS), Inter-CP+k-NN, and Inter-CP+CWA on the Wikipedia dataset is 10, 24, and 55 seconds, respectively. Here, we run all the algorithms (Matlab code) on a computer with 3GHz CPU and 32GB RAM. Since our Inter-CP with CGC leads to significantly better results, we prefer it to CA+SA in practice, regardless of its relatively larger time cost."}, {"heading": "VI. CONCLUSIONS", "text": "In this paper, we have investigated the challenging problem of pairwise constraint propagation on multi-view data. By decomposing the inter-view constraint propagation problem into a set of independent semi-supervised learning subproblems, we have uniformly formulated them as minimizing a regularized energy functional. More importantly, these semisupervised learning subproblems can be solved efficiently using label propagation with k-NN graph. We then develop two constrained graph construction methods for our inter-view constraint propagation, and the obtained two graphs can be considered as the variants of k-NN graph. The experimental results in cross-view retrieval have shown the promising performance of our inter-view constraint propagation with constrained graph construction. For future work, our method will be extended to other multi-view tasks."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by National Natural Science Foundation of China under Grants 61202231 and 61222307, National Key Basic Research Program (973 Program) of China under Grant 2014CB340403, Beijing Natural Science Foundation of China under Grant 4132037, the Fundamental Research Funds for the Central Universities and the Research Funds of Renmin University of China under Grant 14XNLF04, and a grant from Microsoft Research Asia."}], "references": [{"title": "Generalized competitive learning of gaussian mixture models", "author": ["Z. Lu", "H.H.-S. Ip"], "venue": "IEEE Trans. Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 39, no. 4, pp. 901\u2013909, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "An iterative algorithm for entropy regularized likelihood learning on gaussian mixture with automatic model selection", "author": ["Z. Lu"], "venue": "Neurocomputing, vol. 69, no. 13, pp. 1674\u20131677, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Image categorization based on a hierarchical spatial markov model", "author": ["L. Wang", "Z. Lu", "H. Ip"], "venue": "CAIP, 2009, pp. 766\u2013773.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "From comparing clusterings to combining clusterings", "author": ["Z. Lu", "Y. Peng", "J. Xiao"], "venue": "AAAI, 2008, pp. 665\u2013670.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Constrained spectral clustering through affinity propagation", "author": ["Z. Lu", "M. Carreira-Perpinan"], "venue": "CVPR, 2008, pp. 1\u20138.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Pairwise constraint propagation by semidefinite programming for semi-supervised classification", "author": ["Z. Li", "J. Liu", "X. Tang"], "venue": "ICML, 2008, pp. 576\u2013583.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Segmentation given partial grouping constraints", "author": ["S. Yu", "J. Shi"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 26, no. 2, pp. 173\u2013183, 2004.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Spectral learning", "author": ["S. Kamvar", "D. Klein", "C. Manning"], "venue": "IJCAI, 2003, pp. 561\u2013566.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems 16, 2004, pp. 321\u2013328.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "ICML, 2003, pp. 912\u2013919.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Image categorization by learning with context and consistency", "author": ["Z. Lu", "H. Ip"], "venue": "CVPR, 2009, pp. 2719\u20132726.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Noise-robust semi-supervised learning via fast sparse coding", "author": ["Z. Lu", "L. Wang"], "venue": "Pattern Recognition, vol. 48, no. 2, pp. 605\u2013612, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Combining latent semantic learning and reduced hypergraph learning for semi-supervised image categorization", "author": ["Z. Lu", "Y. Peng"], "venue": "ACM Multimedia, 2011, pp. 1409\u20131412.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Combining context, consistency, and diversity cues for interactive image categorization", "author": ["Z. Lu", "H. Ip"], "venue": "IEEE Trans. Multimedia, vol. 12, no. 3, pp. 194\u2013203, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Costa Pereira", "E. Coviello", "G. Doyle", "G. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "ACM Multimedia, 2010, pp. 251\u2013260.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-view clustering with constraint propagation for learning with an incomplete mapping between views", "author": ["E. Eaton", "M. desJardins", "S. Jacob"], "venue": "CIKM, 2010, pp. 389\u2013398.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-modal constraint propagation for heterogeneous image clustering", "author": ["Z. Fu", "H. Ip", "H. Lu", "Z. Lu"], "venue": "ACM Multimedia, 2011, pp. 143\u2013 152.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Multimodal semisupervised learning for image classification", "author": ["M. Guillaumin", "J. Verbeek", "C. Schmid"], "venue": "CVPR, 2010, pp. 902\u2013 909.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Design of multimodal dissimilarity spaces for retrieval of video documents", "author": ["E. Bruno", "N. Moenne-Loccoz", "S. Marchand-Maillet"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 30, no. 9, pp. 1520\u20131533, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Constrained spectral clustering via exhaustive and efficient constraint propagation", "author": ["Z. Lu", "H. Ip"], "venue": "ECCV, vol. 6, 2010, pp. 1\u201314.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Symmetric graph regularized constraint propagation", "author": ["Z. Fu", "Z. Lu", "H.H.-S. Ip", "Y. Peng", "H. Lu"], "venue": "AAAI, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Exhaustive and efficient constraint propagation: A graph-based learning approach and its applications", "author": ["Z. Lu", "Y. Peng"], "venue": "International Journal of Computer Vision, vol. 103, no. 3, pp. 306\u2013325, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "For most large underdetermined systems of linear equations the minimal l-norm solution is also the sparsest solution", "author": ["D. Donoho"], "venue": "Communications on Pure and Applied Mathematics, vol. 59, no. 7, pp. 797\u2013829, 2004.  9", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 2, pp. 210\u2013227, 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Latent semantic learning by efficient sparse coding with hypergraph regularization.", "author": ["Z. Lu", "Y. Peng"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Sparsity induced similarity measure for label propagation", "author": ["H. Cheng", "Z. Liu", "J. Yang"], "venue": "ICCV, 2009, pp. 317\u2013324.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning with l-graph for image analysis", "author": ["B. Cheng", "J. Yang", "S. Yan", "T. Huang"], "venue": "IEEE Trans. Image Processing, vol. 19, no. 4, pp. 858\u2013866, Apr. 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Image annotation by semantic sparse recoding of visual content", "author": ["Z. Lu", "Y. Peng"], "venue": "ACM Multimedia, 2012, pp. 499\u2013508.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Latent semantic learning with structured sparse representation for human action recognition", "author": ["\u2014\u2014"], "venue": "Pattern Recognition, vol. 46, no. 7, pp. 1799\u20131809, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1809}, {"title": "Semantic sparse recoding of visual content for image applications", "author": ["Z. Lu", "P. Han", "L. Wang", "J.-R. Wen"], "venue": "IEEE Trans. Image Processing, vol. 24, no. 1, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Regularization on discrete spaces", "author": ["D. Zhou", "B. Scholk\u00f6pf"], "venue": "DAGM, 2005, pp. 361\u2013368.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Smoothing proximal gradient method for general structured sparse learning", "author": ["X. Chen", "Q. Lin", "S. Kim", "J.G. Carbonell", "E.P. Xing"], "venue": "UAI, 2011, pp. 105\u2013114.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic linguistic indexing of pictures by a statistical modeling approach", "author": ["J. Li", "J. Wang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 25, no. 9, pp. 1075\u20131088, Sept. 2003.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiple Bernoulli relevance models for image and video annotation", "author": ["S. Feng", "R. Manmatha", "V. Lavrenko"], "venue": "CVPR, vol. 2, 2004, pp. 1002\u20131009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Contextual kernel and spectral methods for learning the semantics of images", "author": ["Z. Lu", "H. Ip", "Y. Peng"], "venue": "IEEE Trans. Image Processing, vol. 20, no. 6, pp. 1739\u20131750, 2011.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Spatial markov kernels for image categorization and annotation", "author": ["Z. Lu", "H. Ip"], "venue": "IEEE Trans. Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 41, no. 4, pp. 976\u2013989, 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "ECCV, vol. 4, 2010, pp. 15\u201329.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A. Berg", "T. Berg"], "venue": "CVPR, 2011, pp. 1601\u20131608.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Im2Text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T. Berg"], "venue": "Advances in Neural Information Processing Systems 24, 2012, pp. 1143\u20131151.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, vol. 28, no. 3-4, pp. 321\u2013377, 1936.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1936}, {"title": "Partial least squares", "author": ["H. Wold"], "venue": "Encyclopedia of Statistical Sciences, S. Kotz and N. Johnson, Eds. New York: Wiley, 1985, pp. 581\u2013591.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1985}, {"title": "Multimedia content processing through cross-modal association", "author": ["D. Li", "N. Dimitrova", "M. Li", "I.K. Sethi"], "venue": "ACM Multimedia, 2003, pp. 604\u2013611.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "To effectively exploit pairwise constraints for clustering or classification [1]\u2013[4], much attention has been paid to pairwise constraint propagation [5]\u2013[7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "To effectively exploit pairwise constraints for clustering or classification [1]\u2013[4], much attention has been paid to pairwise constraint propagation [5]\u2013[7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "To effectively exploit pairwise constraints for clustering or classification [1]\u2013[4], much attention has been paid to pairwise constraint propagation [5]\u2013[7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": "To effectively exploit pairwise constraints for clustering or classification [1]\u2013[4], much attention has been paid to pairwise constraint propagation [5]\u2013[7].", "startOffset": 154, "endOffset": 157}, {"referenceID": 7, "context": "Different from the method [8] which only adjusts the similarities between constrained data points, these approaches can propagate pairwise constraints to other similarities between unconstrained data points and thus achieve better results in most cases.", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "Since we have to learn the relationships (must-link or cannot-link) between data points, intra-view constraint propagation is more challenging than the traditional label propagation [9]\u2013[14] whose goal is only to predict the labels of unlabeled data points.", "startOffset": 182, "endOffset": 185}, {"referenceID": 13, "context": "Since we have to learn the relationships (must-link or cannot-link) between data points, intra-view constraint propagation is more challenging than the traditional label propagation [9]\u2013[14] whose goal is only to predict the labels of unlabeled data points.", "startOffset": 186, "endOffset": 190}, {"referenceID": 14, "context": "However, besides intra-view pairwise constraints, we may also have easy access to inter-view pairwise constraints in multi-view tasks such as cross-view retrieval [15], where each pairwise constraint is defined over a pair of data points", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "Although pairwise constraint propagation has been successfully applied to multi-view clustering in [16], [17], only intra-view pairwise constraints are propagated across different views.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "Although pairwise constraint propagation has been successfully applied to multi-view clustering in [16], [17], only intra-view pairwise constraints are propagated across different views.", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": "Specifically, we first decompose the inter-view constraint propagation problem into a set of independent semi-supervised learning [9]\u2013[12] subproblems.", "startOffset": 130, "endOffset": 133}, {"referenceID": 11, "context": "Specifically, we first decompose the inter-view constraint propagation problem into a set of independent semi-supervised learning [9]\u2013[12] subproblems.", "startOffset": 134, "endOffset": 138}, {"referenceID": 8, "context": "Through formulating these subproblems uniformly as minimizing a regularized energy functional, we thus develop an efficient algorithm for inter-view constraint propagation based on the traditional graph-based label propagation technique [9].", "startOffset": 237, "endOffset": 240}, {"referenceID": 14, "context": "drawn much attention recently [15].", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "For cross-view retrieval, it is not feasible to combine multiple views just as previous multi-view retrieval methods [18], [19].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "For cross-view retrieval, it is not feasible to combine multiple views just as previous multi-view retrieval methods [18], [19].", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "More notably, the two closely related methods [16], [17] for multi-view clustering are actually incompetent for cross-view retrieval.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "More notably, the two closely related methods [16], [17] for multi-view clustering are actually incompetent for cross-view retrieval.", "startOffset": 52, "endOffset": 56}, {"referenceID": 8, "context": "Furthermore, we develop an efficient algorithm for inter-view constraint propagation based on the label propagation technique [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "Finally, given two graphs GX = {X ,WX } and GY = {Y,WY} constructed over {X ,Y} with WX (or WY ) being the edge weight matrix defined over the vertex set X (or Y), we utilize the graph-based label propagation method [9] to uniformly solve these semi-supervised learning subproblems:", "startOffset": 216, "endOffset": 219}, {"referenceID": 9, "context": "As for the second and fourth terms, they are known as the energy functional [10] (or smoothness) defined over X and Y .", "startOffset": 76, "endOffset": 80}, {"referenceID": 8, "context": "Fortunately, equation (4) can also be efficiently found using label propagation [9] with k-nearest neighbor (k-NN) graph.", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "In fact, the linear equation (6) can also be efficiently solved using label propagation [9] with k-NN graph.", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "According to the convergence analysis in [9], Step (3) converges to F \u2217 X = (1\u2212\u03b1)(I \u2212\u03b1XSX ) ((1\u2212 \u03b2)Z + \u03b2F \u2217 Y), equal to the solution (5) given that \u03b1X = \u03bc\u0302X /(1 + \u03bc\u0302X ) and SX = I \u2212 LX .", "startOffset": 41, "endOffset": 44}, {"referenceID": 19, "context": "These subproblems can be similarly merged to a single optimization problem (similar to [20]\u2013[22]):", "startOffset": 87, "endOffset": 91}, {"referenceID": 21, "context": "These subproblems can be similarly merged to a single optimization problem (similar to [20]\u2013[22]):", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "The second and fourth terms of the above equation denote the energy functional [10] (or the smoothness measure) defined over X .", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "0 \u2264 w (x) ij \u2264 1) just as [20]:", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "The second constrained graph construction method formulates graph construction as sparse representation [23]\u2013[25] and then directly add the intra-view pairwise constraints into sparse representation, which is thus called as constrained sparse representation (CSR).", "startOffset": 104, "endOffset": 108}, {"referenceID": 24, "context": "The second constrained graph construction method formulates graph construction as sparse representation [23]\u2013[25] and then directly add the intra-view pairwise constraints into sparse representation, which is thus called as constrained sparse representation (CSR).", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": ", L1-graph construction [26], [27].", "startOffset": 24, "endOffset": 28}, {"referenceID": 26, "context": ", L1-graph construction [26], [27].", "startOffset": 30, "endOffset": 34}, {"referenceID": 22, "context": "According to [23], if the solution for xi is sparse enough, it can be recovered by:", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "In practice, due to the noise in the data, we can reconstruct x\u0302i similar to [24]: x\u0302i = Ci\u03b1i+ \u03b6i, where \u03b6i is the noise term.", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": "In fact, this supervisory information can be exploited for L1-graph construction through Laplacian regularization [9], [10].", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "In fact, this supervisory information can be exploited for L1-graph construction through Laplacian regularization [9], [10].", "startOffset": 119, "endOffset": 123}, {"referenceID": 8, "context": "From this normalized Laplacian matrix Li, we can derive the Laplacian regularization term for the sparse representation problem (12) as \u03b1i Li\u03b1i, the same as the original definition in [9].", "startOffset": 184, "endOffset": 187}, {"referenceID": 11, "context": "Hence, we further formulate an L1norm version of Laplacian regularization [12], [28]\u2013[30]:", "startOffset": 74, "endOffset": 78}, {"referenceID": 27, "context": "Hence, we further formulate an L1norm version of Laplacian regularization [12], [28]\u2013[30]:", "startOffset": 80, "endOffset": 84}, {"referenceID": 29, "context": "Hence, we further formulate an L1norm version of Laplacian regularization [12], [28]\u2013[30]:", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "However, this is not true for the traditional Laplacian regularization [9], [10], which may introduce extra parameters (hard to tune in practice) into the L1-optimization for sparse representation.", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": "However, this is not true for the traditional Laplacian regularization [9], [10], which may introduce extra parameters (hard to tune in practice) into the L1-optimization for sparse representation.", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": "Moreover, the p-Laplacian regularization [31] can also be regarded as an ordinary L1-generalization of the Laplacian regularization when p = 1.", "startOffset": 41, "endOffset": 45}, {"referenceID": 31, "context": "According to [32], by defining a matrix Cp \u2208", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "In fact, this is just the goal of cross-view retrieval which has drawn much attention recently [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "In this case, cross-view retrieval is somewhat similar to automatic image annotation [33]\u2013[36] and image caption generation Three Puerto Ricans were awarded", "startOffset": 85, "endOffset": 89}, {"referenceID": 35, "context": "In this case, cross-view retrieval is somewhat similar to automatic image annotation [33]\u2013[36] and image caption generation Three Puerto Ricans were awarded", "startOffset": 90, "endOffset": 94}, {"referenceID": 14, "context": "Cross-view retrieval examples on the Wikipedia benchmark dataset [15].", "startOffset": 65, "endOffset": 69}, {"referenceID": 36, "context": "[37]\u2013[39], since these three tasks all aim to learn the relations between the text and image views.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[37]\u2013[39], since these three tasks all aim to learn the relations between the text and image views.", "startOffset": 5, "endOffset": 9}, {"referenceID": 36, "context": "the <object, action, scene> triplets used in [37]), different from", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "In the context of cross-view retrieval, one notable recent work is [15] which first learns the correlation between the text and image views with canonical correlation analysis (CCA) [40] and then achieves the abstraction by representing text and image at a more general semantic level.", "startOffset": 67, "endOffset": 71}, {"referenceID": 39, "context": "In the context of cross-view retrieval, one notable recent work is [15] which first learns the correlation between the text and image views with canonical correlation analysis (CCA) [40] and then achieves the abstraction by representing text and image at a more general semantic level.", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "The effectiveness of such integration as compared to CA+SA [15] is preliminarily verified by several cross-view retrieval examples shown in Fig.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "We focus on comparing our Inter-CP algorithm with the state-of-the-art approach [15], since they both consider not only correlation analysis (CA) but also semantic abstraction (SA) for text and image views.", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "Moreover, we also make comparison with another two closely related approaches that integrate CA and SA for cross-view retrieval similar to [15] but perform correlation analysis by partial least squares (PLS) [41] and cross-modal factor analysis (CFA) [42] instead of CCA, respectively.", "startOffset": 139, "endOffset": 143}, {"referenceID": 40, "context": "Moreover, we also make comparison with another two closely related approaches that integrate CA and SA for cross-view retrieval similar to [15] but perform correlation analysis by partial least squares (PLS) [41] and cross-modal factor analysis (CFA) [42] instead of CCA, respectively.", "startOffset": 208, "endOffset": 212}, {"referenceID": 41, "context": "Moreover, we also make comparison with another two closely related approaches that integrate CA and SA for cross-view retrieval similar to [15] but perform correlation analysis by partial least squares (PLS) [41] and cross-modal factor analysis (CFA) [42] instead of CCA, respectively.", "startOffset": 251, "endOffset": 255}, {"referenceID": 14, "context": "In the following, these two CA+SA approaches are denoted as CA+SA (PLS) and CA+SA (CFA), while the state-of-the-art approach [15] is denoted as CA+SA (CCA).", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "The first one is a Wikipedia benchmark dataset [15], which contains a total of 2,866 documents derived from Wikipedia\u2019s \u201cfeatured articles\u201d.", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "This benchmark dataset [15] is split into a training set of 2,173 documents and a test set of 693 documents.", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "For the above two datasets, we take the same strategy as [15] to generate both text and image representation.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "More concretely, in the Wikipedia dataset, the text representation for each document is derived from a latent Dirichlet allocation model with 10 latent topics, while the image representation is based on a bag-of-words model with 128 visual words learnt from the extracted SIFT descriptors, just as [15].", "startOffset": 298, "endOffset": 302}, {"referenceID": 17, "context": "For each task, the retrieval results are measured with mean average precision (MAP) which has been widely used in the image retrieval literature [18].", "startOffset": 145, "endOffset": 149}], "year": 2015, "abstractText": "This paper presents a graph-based learning approach to pairwise constraint propagation on multi-view data. Although pairwise constraint propagation has been studied extensively, pairwise constraints are usually defined over pairs of data points from a single view, i.e., only intra-view constraint propagation is considered for multi-view tasks. In fact, very little attention has been paid to inter-view constraint propagation, which is more challenging since pairwise constraints are now defined over pairs of data points from different views. In this paper, we propose to decompose the challenging inter-view constraint propagation problem into semi-supervised learning subproblems so that they can be efficiently solved based on graph-based label propagation. To the best of our knowledge, this is the first attempt to give an efficient solution to inter-view constraint propagation from a semi-supervised learning viewpoint. Moreover, since graph-based label propagation has been adopted for basic optimization, we develop two constrained graph construction methods for interview constraint propagation, which only differ in how the intraview pairwise constraints are exploited. The experimental results in cross-view retrieval have shown the promising performance of our inter-view constraint propagation.", "creator": "LaTeX with hyperref package"}}}