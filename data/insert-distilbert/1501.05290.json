{"id": "1501.05290", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2015", "title": "Managing large-scale scientific hypotheses as uncertain and probabilistic data", "abstract": "here in view of the huge paradigm shift that makes science ever economically more data - driven, in this thesis we propose a synthesis method for encoding and managing exceptionally large - scale deterministic scientific hypotheses as uncertain expectations and probabilistic data. in the simple form of mathematical equations, hypotheses symmetrically relate aspects of the studied phenomena. for computing predictions, however, deterministic hypotheses are used asymmetrically loosely as functions. we build upon simon's notion of structural equations in order to extract the ( so - until called ) causal ordering embedded in almost a hypothesis structure ( modular set of mathematical equations ).", "histories": [["v1", "Wed, 21 Jan 2015 20:46:23 GMT  (1504kb,D)", "https://arxiv.org/abs/1501.05290v1", "139 pages, 58 figures, 1 table. PhD thesis, National Laboratory for Scientific Computing (LNCC), Brazil, January 2015"], ["v2", "Thu, 12 Feb 2015 20:52:29 GMT  (1503kb,D)", "http://arxiv.org/abs/1501.05290v2", "145 pages, 61 figures, 1 table. PhD thesis, National Laboratory for Scientific Computing (LNCC), Brazil, February 2015"]], "COMMENTS": "139 pages, 58 figures, 1 table. PhD thesis, National Laboratory for Scientific Computing (LNCC), Brazil, January 2015", "reviews": [], "SUBJECTS": "cs.DB cs.AI cs.CE", "authors": ["bernardo gon\\c{c}alves"], "accepted": false, "id": "1501.05290"}, "pdf": {"name": "1501.05290.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Bernardo Gon\u00e7alves"], "emails": [], "sections": [{"heading": null, "text": "National Laboratory for Scientific Computing\nGraduate Program in Computational Modeling\nManaging large-scale scientific hypotheses as uncertain\nand probabilistic data\nBy\nBernardo Gonc\u0327alves\nPETRO\u0301POLIS, RJ - BRASIL\nFEBRUARY - 2015\nar X\niv :1\n50 1.\n05 29\n0v 2\n[ cs\n.D B\n] 1\n2 Fe\nb 20\n15\nMANAGING LARGE-SCALE SCIENTIFIC HYPOTHESES AS\nUNCERTAIN AND PROBABILISTIC DATA\nBernardo Gonc\u0327alves\nTHESIS SUBMITTED TO THE EXAMINING COMMITTEE IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF SCIENCES IN COMPUTATIONAL MODELING.\nApproved by:\nProf. Fabio Porto, D.Sc.\n(Chair)\nProf. Pedro L. Dias, Ph.D.\nProf. Marco A. Casanova, Ph.D.\nProf. Ana Carolina Salgado, D.Sc.\nPETRO\u0301POLIS, RJ - BRASIL FEBRUARY - 2015\n\u00a9 2015, Bernardo Nunes Gonc\u0327alves.\nAll rights reserved.\nGonc\u0327alves, Bernardo\nG635m Managing large-scale scientific hypotheses as uncertain and probabilis-\ntic data / Bernardo Gonc\u0327alves. Petropo\u0301lis, RJ. : National Laboratory for Scientific Computing, 2015.\nxvii, 128p. : il.; 29 cm\nOrientador: Fabio Porto\nThesis (D.Sc.) \u2013 National Laboratory for Scientific Computing, 2015. 1. Hypothesis management. 2. Predictive analytics. 3. Uncertain and\nprobabilistic data. 4. Causal reasoning. 5. Probabilistic database design. I. Porto, Fabio. II. LNCC/MCT. III. T\u0301\u0131tulo.\nCDD \u2013 629.8\nDedicatory\nTo my parents Tania and Francisco,\nand to my special love, Marcelle,\nfor being an island of certainty in an\nuncertain world."}, {"heading": "Acknowledgments", "text": "This thesis work has been supported by LNCC (Graduate Program in Com-\nputational Modeling), CNPq (grant 141838/2011-6), FAPERJ (grant \u2018Nota 10\u2019 E-26/100.286/2013) and IBM (Ph.D. Fellowship 2013/2014).\nI would like to express my gratitude to my advisor Fabio Porto for the gift\nof the challenging topic of this thesis, so special to me. I am grateful to him for being an inspiring advisor, and for nicely influencing me towards database research. I also thank my thesis committee for their attention and time devoted in the assessment of my work. I am indebted to Ana Maria Moura for her generous advice and support throughout my Ph.D. research, and to Frederico C. Silva and Adolfo Simo\u0303es for their support to my research project at DEXL/LNCC.\nI would like to thank very much all researchers at LNCC who give lectures in\nthe graduate program, specially Prof. Jose\u0301 Karam Filho for teaching me generously about the roots of mathematical modeling. They have contributed significantly to my education as a cross-disciplinary thinker and the shaping of my scientific and mathematical skills. I thank my Ph.D. colleagues and friends at LNCC, specially Eduardo Lima, Ramon Costa, Klaus Wehmuth, Raquel Lopes, Karine Guimara\u0303es and Diego Paredes for their companionship and joy shared in the pursuit of their Ph.D. theses. I would also like to gratefully recall my earlier professors at UFES, Jose\u0301 Gonc\u0327alves, Giancarlo Guizzardi, Rosane Caruso and Berilhes Garcia for shaping the most essential building blocks in my education.\nFinally, I would like to thank my family for their support: my father Fran-\ncisco, my example of simplicity and goodness; my mother, Tania, for her tenacity and true love; Leo and Sche, my dear brother and sister, true union for life; my grandmothers Zeca, Orizontina and Rosita for their love and prayers. I thank also Marcel and Maris for the greatest gift, their daughter and my near future wife, Marcelle \u2013 the best partner one could ever hope for.\n\u201cOriginally, there was just experimental science, and then there was theoretical science, with Kepler\u2019s Laws, Newton\u2019s Laws of Motion, Maxwell\u2019s equations, and so on. Then, for many problems, the theoretical models grew too complicated to solve analytically, and people had to start simulating. These simulations have carried us through much of the last half of the last century. At this point, these simulations are generating a whole lot of data, along with a huge increase in data from the experimental sciences.\u201d\n\u2014 Jim Gray, 2007\nAbstract of Thesis presented to LNCC/MCT in partial fulfillment of the requirements for the degree of Doctor of Sciences (D.Sc.)\nMANAGING LARGE-SCALE SCIENTIFIC HYPOTHESES AS\nUNCERTAIN AND PROBABILISTIC DATA\nBernardo Gonc\u0327alves\nFebruary - 2015\nAdvisor: Fabio Porto, D.Sc.\nIn view of the paradigm shift that makes science ever more data-driven, in this thesis we propose a synthesis method for encoding and managing large-scale deterministic scientific hypotheses as uncertain and probabilistic data.\nIn the form of mathematical equations, hypotheses symmetrically relate as-\npects of the studied phenomena. For computing predictions, however, deterministic hypotheses can be abstracted as functions. We build upon Simon\u2019s notion of structural equations in order to efficiently extract the (so-called) causal ordering between variables, implicit in a hypothesis structure (set of mathematical equations).\nWe show how to process the hypothesis predictive structure effectively through\noriginal algorithms for encoding it into a set of functional dependencies (fd\u2019s) and then performing causal reasoning in terms of acyclic pseudo-transitive reasoning over fd\u2019s. Such reasoning reveals important causal dependencies implicit in the hypothesis predictive data and guide our synthesis of a probabilistic database. Like in the field of graphical models in AI, such a probabilistic database should be normalized so that the uncertainty arisen from competing hypotheses is decomposed into factors and propagated properly onto predictive data by recovering its joint probability distribution through a lossless join. That is motivated as a design-theoretic principle for data-driven hypothesis management and predictive analytics.\nThe method is applicable to both quantitative and qualitative deterministic\nhypotheses and demonstrated in realistic use cases from computational science.\nResumo da Tese apresentada ao LNCC/MCT como parte dos requisitos necessa\u0301rios para a obtenc\u0327a\u0303o do grau de Doutor em Cie\u0302ncias (D.Sc.)\nGERE\u0302NCIA DE HIPO\u0301TESES CIENTI\u0301FICAS DE LARGA-ESCALA\nCOMO DADOS INCERTOS E PROBABILI\u0301STICOS\nBernardo Gonc\u0327alves\nFevereiro, 2015\nOrientador: Fabio Porto, D.Sc.\nTendo em vista a mudanc\u0327a de paradigma que faz da cie\u0302ncia cada vez mais guiada por dados, nesta tese propomos um me\u0301todo para codificac\u0327a\u0303o e gere\u0302ncia de hipo\u0301teses cient\u0301\u0131ficas determin\u0301\u0131sticas de larga escala como dados incertos e probabil\u0301\u0131sticos.\nNa forma de equac\u0327o\u0303es matema\u0301ticas, hipo\u0301teses relacionam simetricamente as-\npectos do feno\u0302meno de estudo. Para computac\u0327a\u0303o de predic\u0327o\u0303es, no entanto, hipo\u0301teses determin\u0301\u0131sticas podem ser abstra\u0301\u0131das como func\u0327o\u0303es. Levamos adiante a noc\u0327a\u0303o de Simon de equac\u0327o\u0303es estruturais para extrair de forma eficiente a enta\u0303o chamada ordenac\u0327a\u0303o causal impl\u0301\u0131cita na estrutura de uma hipo\u0301tese.\nMostramos como processar a estrutura preditiva de uma hipo\u0301tese atrave\u0301s de\nalgoritmos originais para sua codificac\u0327a\u0303o como um conjunto de depende\u0302ncias funcionais (df\u2019s) e enta\u0303o realizamos infere\u0302ncia causal em termos de racioc\u0301\u0131nio ac\u0301\u0131clico pseudo-transitivo sobre df\u2019s. Tal racioc\u0301\u0131nio revela importantes depende\u0302ncias causais impl\u0301\u0131citas nos dados preditivos da hipo\u0301tese, que conduzem nossa s\u0301\u0131ntese do banco de dados probabil\u0301\u0131stico. Como na a\u0301rea de modelos gra\u0301ficos (IA), o banco de dados probabil\u0301\u0131stico deve ser normalizado de tal forma que a incerteza oriunda de hipo\u0301teses alternativas seja decomposta em fatores e propagada propriamente recuperando sua distribuic\u0327a\u0303o de probabilidade conjunta via junc\u0327a\u0303o \u2018lossless.\u2019 Isso e\u0301 motivado como um princ\u0301\u0131pio teo\u0301rico de projeto para gere\u0302ncia e ana\u0301lise de hipo\u0301teses.\nO me\u0301todo proposto e\u0301 aplica\u0301vel a hipo\u0301teses determin\u0301\u0131sticas quantitativas e\nqualitativas e e\u0301 demonstrado em casos real\u0301\u0131sticos de cie\u0302ncia computacional.\nTable of Contents\n1 Introduction 1\n1.1 Problem Space and Specific Goals . . . . . . . . . . . . . . . . . . . 4 1.2 Thesis Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.3 Thesis Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2 Vision: Hypotheses as Data 15\n2.1 Running Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2 Hypothesis Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.3 Reasoning over FD\u2019s . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.4 Uncertainty Introduction . . . . . . . . . . . . . . . . . . . . . . . . 22 2.5 Predictive Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.7 Summary: Key Points . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3 Hypothesis Encoding 32\n3.1 Preliminaries: Structural Equations . . . . . . . . . . . . . . . . . . 32 3.2 The Problem of Causal Ordering . . . . . . . . . . . . . . . . . . . 36 3.3 Total Causal Mappings . . . . . . . . . . . . . . . . . . . . . . . . . 39 3.4 The Encoding Scheme . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.7 Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4 Causal Reasoning over FD\u2019s 49\n4.1 Preliminaries: Armstrong\u2019s Inference Rules . . . . . . . . . . . . . . 49\nTABLE OF CONTENTS x\n4.2 Acyclic Pseudo-Transitive Reasoning . . . . . . . . . . . . . . . . . 50 4.3 Equivalence with Causal Ordering . . . . . . . . . . . . . . . . . . . 55 4.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 4.5 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 4.6 Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5 Probabilistic Database Synthesis 61\n5.1 Preliminaries: U-Relations and Probabilistic WSA . . . . . . . . . . 61 5.2 Running Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.3 U-Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 5.4 U-Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 5.5 Design-Theoretic Properties . . . . . . . . . . . . . . . . . . . . . . 73 5.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 5.7 Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n6 Applicability 79\n6.1 The Physiome Project as a Testbed . . . . . . . . . . . . . . . . . . 79 6.2 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 6.3 System Prototype . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 6.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 6.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 6.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n7 Conclusions 102\n7.1 Revisiting the Research Questions . . . . . . . . . . . . . . . . . . . 102 7.2 Significance and Limitations . . . . . . . . . . . . . . . . . . . . . . 105 7.3 Open Problems and Future Work . . . . . . . . . . . . . . . . . . . 106 7.4 Final Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . 107\nTABLE OF CONTENTS xi\nBibliography 108\nAppendix\nA Detailed Proofs 116\nA.1 Proofs of Hypothesis Encoding . . . . . . . . . . . . . . . . . . . . . 116 A.2 Proofs of Causal Reasoning . . . . . . . . . . . . . . . . . . . . . . 119 A.3 Proofs of Probabilistic DB Synthesis . . . . . . . . . . . . . . . . . 126\nList of Figures\nFigure\n1.1 Multi-fold view of a deterministic scientific hypothesis. . . . . . . . 2 1.2 A view of the scientific method life cycle . . . . . . . . . . . . . . . 5 1.3 The usual data ingesture pipeline of simulation data management . 7 1.4 Design-theoretic pipeline for processing hypotheses as uncertain and\nprobabilistic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.1 Scientific hypotheses seen as alternative functions to predict data. . 16 2.2 Predictive analytics in a data-intensive hypothesis evaluation study. 18 2.3 Descriptive (textual) data of Example 1. . . . . . . . . . . . . . . . 19 2.4 \u2018Big\u2019 fact table H1 loaded with simulation raw data . . . . . . . . . 19 2.5 \u2018Explanation\u2019 relational table H0. . . . . . . . . . . . . . . . . . . . 23 2.6 Result set of query Q1 on simulation trial dataset for hypothesis H1. 24 2.7 U-relational predictive tables rendered by query using the fd\u2019s. . . . 25 2.8 Analytics on predicted position s conditioned on observation. . . . . 26\n3.1 \u201cDirected causal graphs\u201d associated with the two systems. . . . . . . 33 3.2 Running Simon\u2019s Causal Ordering Algorithm (COA) . . . . . . . . . 35 3.3 Directed causal graph G\u03d5 induced by mapping \u03d5 for structure S . . 36 3.4 Bipartite graph G of structure S from Example 3. . . . . . . . . . . 37 3.5 Another hypothesis structure example. . . . . . . . . . . . . . . . . 39 3.6 Complete matching M for structure S from Example 3. . . . . . . . 42 3.7 Encoded fd set \u03a3 (cf. Alg. 3) for the structure from Example 3. . . . 44 3.8 Performance of hypothesis encoding (in logscale). . . . . . . . . . . 46\n4.1 Fd set \u03a3 encoding (cf. Alg. 3) the structure of Fig. 3.2a and its\nfolding \u03a3# derived by Alg. 5. . . . . . . . . . . . . . . . . . . . . . 52\nLIST OF FIGURES xiii\n4.2 Fd set \u03a3 encoding the structure of Fig. 3.2a and the folding \u03a5(\u03a3)#\nof its \u03c5-projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n4.3 Performance of acyclic causal reasoning over fd\u2019s (logscale). . . . . . 59\n5.1 U-relation generated by the repair-key operation. . . . . . . . . . . 63 5.2 Fd sets encoded from the given structures Sk(Ek, Vk) for hypotheses\nk = 1..3 from Example 6. . . . . . . . . . . . . . . . . . . . . . . . . 65\n5.3 \u2018Big\u2019 fact table H3 of hypothesis k=3 from Example 6 loaded with\ntrial datasets identified by special attribute tid. . . . . . . . . . . . 65\n5.4 \u2018Big\u2019 fact table H3 of hypothesis k=3 from Example 6 with u-factors\n{b, d} and {p, r} emphasized (resp.) in colors green and red. . . . . 67\n5.5 Fd set \u21263 (compare with \u03a33) and its folding \u2126 # 3 . . . . . . . . . . . . 68 5.6 U-factor projections rendered for hypothesis \u03c5 = 3. . . . . . . . . . 69 5.7 U-relations rendered for hypothesis \u03c5 = 3. . . . . . . . . . . . . . . 72\n6.1 Plot of hemoglobin oxygen saturation hypotheses . . . . . . . . . . 81 6.2 Descriptive (textual) data of Example 8. . . . . . . . . . . . . . . . 81 6.3 Fd set \u03a328 of hypothesis \u03c5=28. . . . . . . . . . . . . . . . . . . . . 82 6.4 Fd set \u03a331 of hypothesis \u03c5=31. . . . . . . . . . . . . . . . . . . . . 82 6.5 Fd set \u03a332 of hypothesis \u03c5=32. . . . . . . . . . . . . . . . . . . . . 82 6.6 Result set of hypothesis management query Q1. . . . . . . . . . . . 83 6.7 Results of analytical study on the hemoglobin phenomenon. . . . . 83 6.8 Plot of baroreflex hypothesis for Dahl SS Rat . . . . . . . . . . . . 84 6.9 Descriptive (textual) data of Example 9. . . . . . . . . . . . . . . . 84 6.10 Result set of hypothesis management query Q2. . . . . . . . . . . . 85 6.11 Results of analytical study on the baroreflex phenomenon. . . . . . 85 6.12 Fd set \u03a31001 of hypothesis \u03c5=1001. . . . . . . . . . . . . . . . . . . 86 6.13 Plot of myogenic behavior hypothesis . . . . . . . . . . . . . . . . . 87 6.14 Descriptive (textual) data of Example 10. . . . . . . . . . . . . . . . 87 6.15 Fd set \u03a360 of hypothesis \u03c5=60. . . . . . . . . . . . . . . . . . . . . 88 6.16 Fd set \u03a389 of hypothesis \u03c5=89. . . . . . . . . . . . . . . . . . . . . 88 6.17 Result set of hypothesis management query Q3. . . . . . . . . . . . 89 6.18 Results of analytics on the vessel\u2019s myogenic behavior phenomenon. 89\nLIST OF FIGURES xiv\n6.19 Lynx-Hare population observed . . . . . . . . . . . . . . . . . . . . 91 6.20 Lynx-Hare population observed . . . . . . . . . . . . . . . . . . . . 91 6.21 Screenshots of this first prototype of the \u03a5-DB system. . . . . . . . 92 6.22 Descriptive (textual) data of Example 10. . . . . . . . . . . . . . . . 93 6.23 Fd set \u03a31 of hypothesis \u03c5=1. . . . . . . . . . . . . . . . . . . . . . 93 6.24 Fd set \u03a32 of hypothesis \u03c5=2. . . . . . . . . . . . . . . . . . . . . . 93 6.25 Fd set \u03a33 of hypothesis \u03c5=3. . . . . . . . . . . . . . . . . . . . . . 94 6.26 Results of analytics on the US population phenomenon. . . . . . . . 95 6.27 Results of analytics on the Hudson\u2019s Bay lynx population phenomenon 95 6.28 Performance behavior of \u03a5-DB on a Physiome testbed scenario. . . 96 6.29 Physiome hypotheses used in the experiments. . . . . . . . . . . . . 97 6.30 Example of Boolean Network hypothesis. . . . . . . . . . . . . . . . 100 6.31 Example of Boolean Network model . . . . . . . . . . . . . . . . . . 101\nList of Tables\nTable\n1.1 Simulation data management vs. hypothesis data management. . . 5\nAcronyms\n\u2022 fd: functional dependency\n\u2022 p-DB: probabilistic database\n\u2022 p-WSA: probabilistic world set algebra\n\u2022 MayBMS: U-relational database management system\n\u2022 AI: Artificial Intelligence\n\u2022 GM: graphical models (e.g., Bayesian Networks)\n\u2022 ETL: extract, transform, load\n\u2022 OLAP: On-Line Analytical Processing\n\u2022 DW: Data Warehouse\n\u2022 SEM: structural equation model\n\u2022 COA: causal ordering algorithm\n\u2022 Sk: structure of hypothesis k \u2022 E : set of equations in a structure\n\u2022 V: set of variables appearing in the equations in a structure\n\u2022 V ars(f): set of variables appearing in equation f\n\u2022 \u03d5 : E \u2192 V: total causal mapping from equations to variables in a structure\n\u2022 C\u03d5: set of causal dependencies \u2022 G\u03d5: causal graph induced by \u03d5 \u2022 Hk: \u2018big\u2019 fact table of hypothesis k \u2022 H: set of hypothesis \u2018big\u2019 fact tables\n\u2022 Y `k : U-relation synthesized for hypothesis k \u2022 Y k: set of U-relations synthesized for hypothesis k \u2022 H0: relational \u2018explanation\u2019 table\nLIST OF TABLES xvii\n\u2022 Y0: U-relational \u2018explanation\u2019 table \u2022 \u03c6: phenomenon identifier\n\u2022 \u03c5: hypothesis identifier\n\u2022 \u03a3, \u0393, \u2206, \u2126: fd sets\n\u2022 \u03a3B: subset of the closure of an fd set derived by pseudo-transitivity\n\u2022 \u03a3+: closure of an fd set\n\u2022 \u03a3#: the folding of an fd set\n\u2022 synthesis \u20184U\u2019: synthesis for uncertainty\n\u2022 tid: hypothesis trial identifier\n\u2022 BCNF: Boyce-Codd Normal Form\n\u2022 MathML: Mathematical Markup Language\n\u2022 TCM: total causal mapping algorithm\n\u2022 lhs, rhs: left-hand side, right-hand side\n\u2022 sch(R): data columns of table R\n\u2022 ViDi: condition columns of a table\nChapter 1\nIntroduction\nIn view of the paradigm shift that makes science ever more data-driven [1], in this thesis we demonstrate that large deterministic scientific hypotheses can be effectively encoded and managed as a kind of uncertain and probabilistic data.\nDeterministic hypotheses can be formed as principles or ideas, then expressed\nmathematically and implemented in a program that is run to give their decisive form of data (see Fig. 1.1). Hypotheses can also be learned in large scale, as exhibited in the Eureqa project [2]. Examples of \u2018structured deterministic hypotheses\u2019 include tentative mathematical models in physics, engineering and economical sciences, or conjectured boolean networks in molecular biology and social sciences. These are important reasoning devices, as they are solved to generate valuable predictive data for decision making in science and increasingly in business as well.\nIn fact, we can refer nowadays to a broad, modern context of data science [3]\nand big data [4] in which the complexity and scale of so-called \u2018data-driven\u2019 problems require proper data management tools for the predicted data to be analyzed effectively. In this thesis, we pay attention to a quite general class of (tentative) computational science models,1 and we look at them in an original way as a distinguished kind of data source.\n1 \u2018Computational science\u2019 is (sic.) \u201ca rapidly growing multidisciplinary field that uses advanced computing capabilities to understand and solve complex problems\u201d [5]. We may refer to non-stochastic, tentative computational science models throughout this text as \u2018structured deterministic hypotheses.\u2019\n2 Law of free fall\n\u201cIf a body falls from rest, its velocity at any point is proportional to the time it has been falling.\u201d\n(i)\na(t) = \u2212g v(t) = \u2212gt + v0 s(t) = \u2212(g/2)t2 + v0 t + s0\n(ii)\nIt is generally considered that computational science models, interpreted here\nas hypotheses to explain real-world phenomena, are of strategic relevance [5]. They are usually complex in that they may have hundreds to thousands of intertwined (coupled) variables and be computed along space, time or frequency domains in arbitrarily large scale. It is important to note the distinction between the structure and data levels. Consider, say, Lotka-Volterra\u2019s model, which essentially consists in (Eqs. 1.1) two ordinary differential equations, complemented by seven subsidiary equations f1(t), f2(x0), f3(y0), f4(b), f5(p), f6(r), f7(d) to set the values of its domain variable t and (input) parameters x0, y0, b, p, r, d. x\u0307 = x(b\u2212 py)y\u0307 = y(rx\u2212 d) (1.1) In a sense, it can be said fairly simple, as it is characterized by a set E of equations and a set V of variables, sized |E| = |V| = 9. Yet, at the data level this model (cf. Chapter 2) can be made very large just by computing its predictions in a fine time resolution and/or along an extended time window.\nAs we shall see shortly, the technical challenges associated with this thesis\ninvolve (not only but) majorly the structure level where, e.g., such Lotka-Volterra model can be abstracted as a deterministic structure S(E ,V) with |S| = 18.2\n2 The structure length |S| is a measure of how dense the hypothesis structure is, comprising the total sum of the number of variables appearing in each equation.\n3 We are really concerned here with models whose structure S is in the order of |S| . 1M , and whose results (data!) shall be difficult to analyze by handicrafted practice. Note that the data level of a model can be set as large as wanted (set the domain resolution and/or extension accordingly), but it shall be necessarily large when its structure is itself large. By \u2018large-scale hypotheses\u2019 then we mean tentative deterministic models that are large at structure level.\nOverall, such class of hypotheses can be said to qualify to at least four of\nthe five v\u2019s associated to the notion of big data:3 value, because of their role in advancing science and technology; volume, due to the large scale of modern scientific problems; variety, because of their structural heterogeneity, even when they refer to the same phenomena; and veracity, due to their uncertainty.\nThe idea of managing hypotheses \u2018as data\u2019 may sound intriguing and in fact\nit raises a number of research questions of both conceptual and technical nature.4\nWe start by outlining below the conceptual research questions.\nRQ1. How to define and encode hypotheses \u2018as data\u2019? What are the sources of\nuncertainty that may be present and should be considered?\nRQ2. How does hypotheses \u2018as data\u2019 relate with observational data or, likewise,\nphenomena \u2018as data\u2019 from a database perspective?\nRQ3. Does every piece of simulated data qualify as a scientific hypothesis? What\nis the difference between managing \u2018simulation\u2019 data from managing \u2018hypotheses\u2019 as data?\nRQ4. Is there available a proper (machine-readable) data format we can use to\nautomatically extract mathematically-expressed hypotheses from?\nIt has been a challenge of this thesis to provide reasonable answers to these questions, which are brought together into the vision of hypotheses \u2018as data\u2019 (we call it the \u03a5-DB vision) and its use case that we present in Chapter 2, and experiment with in realistic scenarios in Chapter 6.\n3 The \u2018v\u2019 of velocity may appear in connection with machine learning hypotheses, which we discuss in Chapter 6.\n4 We shall keep record of those questions and revisit them in \u00a77.1."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 4", "text": "The \u03a5-DB vision formulates the problem of hypothesis encoding as a problem of probabilistic database design. A number of technical questions arise then.\nWe introduce now technical context, materials and methods identified and\nselected in this thesis as a basis to realize the \u03a5-DB vision in terms of probabilistic database design. We shall outline in the sequel the technical research questions to be answered by the core of the thesis.\n1.1. Problem Space and Specific Goals\nIt has been a goal of this thesis to investigate the capabilities of probabilistic\ndatabases to enable hypothesis data management as a particular case of simulation data management. In the sequel, we first characterize the use case of hypothesis data management and then formulate it in terms of probabilistic DB design."}, {"heading": "1.1.1 Simulation data management", "text": "Simulation laboratories provide scientists and engineers with very large, pos-\nsibly huge datasets that reconstruct phenomena of interest in high resolution. Notorious examples are the John Hopkins Turbulance Databases [6], and the Human Brain Project (HBP) neuroscience simulation datasets [7]. A core motivation for the delivery of such data is enabling new insights and discoveries through hypothesis testing against observations. Nonetheless, while the use case for exploratory analytics is currently well understood and many of its challenges have already been coped with so that high-resolution simulation data is increasingly more accessible [8, 9], only very recently, as part of this thesis work, the use case of hypothesis management has been taken into account for predictive analytics [10].\nIn fact, there is a pressing call for innovative technology to integrate (ob-\nserved) data and (simulated) theories in a unified framework [11, 12, 13]. The point has just been raised by leading neuroscientists in the context of the HBP, who are incisive on the compelling argument that massive simulation databases should be constrained by experimental data in corrective loops to test precise hypotheses [14, p. 28]. Fig. 1.2 shows a simplified view of the (data-driven) scientific method life cycle. It distinguishes the phases of exploratory analytics (context of"}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 5", "text": "discovery) and predictive analytics (context of justification), and highlights the loop between hypothesis formulation and testing [15].\nSimulation data, being generated and tuned from a combination of theo-\nretical and empirical principles, has a distinctive feature to be considered when compared to data generated by high-throughput technology in large-scale scientific experiments. It has a pronounced uncertainty component that motivates the use case of hypothesis data management for predictive analytics [10]. Essential aspects of hypothesis data management can be described in contrast to simulation data management as follows \u2014 Table 1.1 summarizes our comparison.\n\u2022 Sample data. Hypothesis management shall not deal with the same volume\nof data as in simulation data management for exploratory analytics, but only samples of it. This is aligned, for example, with the architectural design of CERN\u2019s particle-physics experiment and simulation ATLAS, where there are four tier/layers of data. The volume of data significantly decreases from (tier-0) the raw data to (tier-3) the data actually used for analyses such as hypothesis testing [8, p. 71-2]. Samples of raw simula-"}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 6", "text": "tion data are to be selected for comparative studies involving competing hypotheses in the presence of evidence (sample observational data). This principle is also aligned with how data is delivered at model repositiories. Since observations are usually less available, only the fragment (sample) of the simulation data that matches in coordinates the (sample) of observations is required out of simulation results for comparative analysis. For instance, we show in \u00a76.2.2 a predictive analytical study extracted from the Virtual Physiological Rat Project (VPR1001-M) comparing sample simulation data (heart rates) from a baroreflex model with observations on a Dahl SS rat strain.5 The simulation is originally set to produce predictions in the time resolution of t\u2206 = 0.01. But since the observational sample is only as fine as t\u2206 = 0.1, there is no gain in rendering a predicted sample with t\u2206 \u2265 0.1 for hypothesis testing. Note that such a \u2018sampling\u2019 does not incur in any additional uncertainty as typical of statistical sampling [16].\n\u2022 Claim-centered access pattern. In simulation data management the access\npattern is dimension-centered (e.g., based on selected space-time coordinates) and the data is denormalized for faster retrieval, as typical of Data Warehouses (DW\u2019s) and OLAP applications.6 In particular, on account of the so-called \u2018big table\u2019 approach, each state of the modeled physical system is recorded in a large, single row of data. This is fairly reasonable for an Extract-Transform-Load (ETL) data ingesture pipeline characterized by batch-, incremental-only updates (see Fig. 1.3). Such a setting is in fact fit for exploratory analytics, as entire states of the simulated system shall be accessed at once (e.g., providing data to a visualization system). Altogether, data retrieval is critical and there is no risk of update anomalies. Hypothesis management, in contrast, should be centered on claims identified within the hypothesis structure w.r.t. available data dependencies. Since the focus is on resolving uncertainty for decision making (which\n5 http://virtualrat.org/computational-models/vpr1001/. 6 On-Line Analytical Processing, as distinguished from OLTP (On-Line Transaction Processing. The latter is meant for transaction processing of daily queries and updates in operational systems, while the former is for analytical queries in Data Warehouses (DW\u2019s) that gather a lot of data collected from different sources for decision making."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 7", "text": "D1\nD2 . . . Dp\nsim\u22c3p i=1Ri\nETL\nferent from simulation data management.\nA key point that distinguishes hypothesis management is that a fact or unit of data is defined by its predictive content. That is, every clear-cut predicted fact (w.r.t.available data dependencies) is a claim. Accordingly, the data should be decomposed and organized for a claim-centered access pattern."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 8", "text": "To anticipate Chapter 2, the synthesis method we have developed in this\nthesis work for processing hypotheses as uncertain and probabilistic data comprises a design-theoretic pipeline (see Fig. 1.4) that extends the one shown in Fig. 1.3."}, {"heading": "1.1.2 Probabilistic database design", "text": "Probabilistic databases (p-DB\u2019s) have evolved into mature technology in the\nlast decade with the emergence of new data models and query processing techniques [17]. One of the state-of-the-art probabilistic data models is the U-relational representation system with its probabilistic world-set algebra (p-WSA) implemented in MayBMS [18]. That is an elegant extension of the relational model we shall refer to in this thesis for the management of large-scale uncertain and probabilistic data.\nWe look at U-relations from the point of view of p-DB design, for which no\nformal design methodology has yet been proposed. Despite the advanced state of probabilistic data management techniques, a lack of methods for the systematic design of p-DBs may prevent wider adoption. The availability of design methods has been considered one of the key success factors for the rapid growth of applications in the field of Graphical Models (GM\u2019s) [19], considered to inform research in p-DB\u2019s [17, p. 14]. Analogously, we have proposed to distinguish methods for p-DB design in three classes [10]: (i) subjective construction, (ii) learning from"}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 9", "text": "data, and (iii) synthesis from other kind of formal specification.\nThe first is the less systematic, as the user has to model for the data and\ncorrelations by steering all the p-DB construction process (MayBMS\u2019 use cases [18], e.g., are illustrated that way). The second comprises analytical techniques to extract the data and learn correlations from external sources, possibly unstructured, into a p-DB under some ad-hoc schema. This is the prevalent one up to date, motivated by information extraction and data integration applications [17, p. 10- 3]. In this thesis we present a methodology of the third kind, as we extract data dependencies from some previously existing formal specification (the hypothesis mathematical structure) to synthesize a p-DB algorithmically. Such a type of construction method has been successful, e.g., for building Bayesian Networks [19]. To our knowledge, this thesis is the first synthesis method for p-DB design (cf. \u00a75.6).\nWe shall develop means to extract the specification of a hypothesis and encode it into a U-relational DB for data-driven hypothesis management and analytics. That is, we shall flatten deterministic hypotheses into U-relations.\nThe synthesis method that we have developed for p-DB\u2019s relies on the ex-\ntraction of functional dependencies (fd\u2019s; cf. [20, 21, 22]) that are basic input to algorithmic synthesis.7 For an example of fd, consider relation FALL in Fig. 1.1. There holds an fd t \u2192 v s, meaning that values of attribute time t functionally determine values of both attributes velocity v and position s. More precisely, let \u00b5 and \u03c4 be any two tuples (rows) in an instance of relation (table) FALL. Then it satisfies fd t\u2192 v s iff \u00b5[t] = \u03c4 [t] implies \u00b5[v s] = \u03c4 [v s]. In our illustrative relation FALL, that fd is, in particular, a key constraint, which means that (values of) t play the role of a key to (provide access to the values of) v and s in the relation.\nA related concept which is also a major one for us is that of normalization\n[20, 21, 22], viz., to ensure that the DB resulting from a design process bears some desirable properties which are associated with some notion of normal form (ibid.). For hypothesis management, the uncertainty has to be modeled and should be normalized so that the uncertainty of one claim may not be undesirably mixed\n7 In fact, it has been considered a critical failure in traditional DB design the lack of techniques to obtain important information such as fd\u2019s in the real world [23, p. 62]."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 10", "text": "with the uncertainty of another claim. It is expected to involve a processing of the causal dependencies implicit in the given hypothesis structure. We shall introduce in detail such concepts in context when necessary."}, {"heading": "1.1.3 Structural equations", "text": "The flattening of the user mathematical models into hypothesis p-DB\u2019s, nonetheless, is not straightforward. It has been a goal of this thesis to investigate proper abstractions on mathematical models in order to (partly) capture their semantics, viz., to an extent that is tailored for hypothesis management (as opposed to, say, model solving). We shall abstract mathematical models into intermediary artifacts that are amenable to be further encoded into fd\u2019s.\nIn fact, given a system of equations with a set of variables appearing in\nthem, in a seminal article Simon introduced an asymmetrical, functional relation among variables that establishes a (so-called) causal ordering [24]. That became known as structural equation models (SEM\u2019s) or just \u2018structural equations\u2019 (cf. also [25]). Along these lines, our goal is to extract the causal ordering implicit in the structure of a deterministic hypothesis into a set of fd\u2019s that guides our synthesis of U-relational DB\u2019s. As we shall see throughout this text,\nthe causal ordering we capture and process through fd\u2019s provides causal dependencies implicit in the predictive data that are very useful information to decompose uncertainty for the sake of probabilistic modeling and reasoning."}, {"heading": "1.1.4 Uncertainty Model", "text": "In uncertain and probabilistic data management, there are essentially two sources of uncertainty: incompleteness (missing data), and multiplicity (inconsistent data).\nThe kind of uncertainty that is dealt with in this work is the multiplicity of hypothesis trial records identified to be targeted at the same phenomenon record. That is, the uncertainty arises from the existance of competing hypotheses. If multiple hypotheses and trials are inserted for the same phenomenon, the system interprets it as defining a probability distribution."}, {"heading": "1.1. PROBLEM SPACE AND SPECIFIC GOALS 11", "text": "Such a probability distribution (usually uniform) on the multiplicity of com-\npeting hypotheses is in accordance with probability theory under possible-worlds semantics [17, Ch. 1]. It is modeled into the U-relational data model and its p-WSA operators, and implemented into the MayBMS system as we shall see in \u00a75.1.8 The conf() aggregate operator, for instance, in spite of the name, performs standard (non-Bayesian) probabilistic inference on such probability distribution. Eventually, however, there is a need to condition the initial probability distribution in the presence of observations. For the conditioning, then, we shall adopt Bayesian inference so that the prior probability distribution can be updated to a posterior.\nThe informal discussion of this section opens the way for a number of tech-\nnical research questions that we outline next.\nRQ5. Is there an algorithm to, given a SEM, efficiently extract its causal order-\ning? What are the computational properties of this problem?\nRQ6. What is the connection between SEM\u2019s and fd\u2019s? Can we devise an en-\ncoding scheme to \u2018orient equations\u2019 and then effectively transform one into the other with guarantees? Once we do it, what design-theoretic properties have such a set of fd\u2019s?\nRQ7. Is such fd set ready to be used for p-DB schema synthesis as an encoding\nof the hypothesis causal structure? If not, what kind of further processing we have to do? Can we perform it efficiently by reasoning directly on the fd\u2019s? How does it relate to the SEM\u2019s causal ordering?\nRQ8. Is the uncertainty decomposition required for predictive analytics reducible\nto the structure level (fd processing), or do we need to process the simulated data to identify additional uncertainty factors? Finally, what properties are desirable for a p-DB schema targeted at hypothesis management? Are they ensured by this synthesis method?\nRQ9. Given all such a design-theoretic machinery to process hypotheses into\n(U-)relational DB\u2019s, what properties can we detect on the hypotheses back\n8 Our own system of hypothesis management is to be delivered on top of the MayBMS backend."}, {"heading": "1.2. THESIS STATEMENT 12", "text": "at the conceptual level? Do we have now technical means to speak of hypotheses that are \u201cgood\u201d in terms of principles of the philosophy of science?\nThe core of this thesis is devoted to answer these questions, and we shall accomplish it throughout Chapters 3, 4 and 5.\n1.2. Thesis Statement\nThe statement of this thesis is that it is possible to effectively encode and\nmanage large deterministic scientific hypotheses as uncertain and probabilistic data. Its key challenges are of both conceptual and technical nature. Conceptually, we provide core, non-obvious abstractions to define and encode hypotheses as data. Technically, we provide a number of algorithms that compose a designtheoretic pipeline to encode hypotheses as uncertain and probabilistic data, and verify their efficiency and correctness. The applicability and effectiveness of our method is demonstrated in realistic case studies in computational science.\nBesides, it is worthwhile highlighting some non-goals of this thesis.\nN1. Although we perform some sort of information extraction [26] for the ac-\nquisition of hypotheses from some model repositories on the web, it is very basic and ad-hoc in order to obtain a testbed for our method. That is, we are not proposing means for the systematic extraction of hypotheses from available sources. In fact we shall outline it in \u00a77.3 as an important direction of future work.\nN2. We do not address solving computational models or numerical analytics\nin any sense. In fact we rely on the numerical solvers (implemented into tools that we use) as \u2018transaction processing\u2019 systems, load their computed data into a relational \u2018big\u2019 fact table and then render it into U-relational tables synthesized by our method. We do not deal with data visualization either in any sense.\nN3. The efficiency and scalability of query processing in p-DB\u2019s, in particular U-\nrelational\u2019s MayBMS and its p-WSA (which we rely on) is not addressed or"}, {"heading": "1.3. THESIS CONTRIBUTIONS 13", "text": "evaluated in this thesis. In fact, the performance of U-relations and p-WSA has been extensively evaluated and shown to be effective [27, 18]. All performance tests carried out in this thesis comprise our design-theoretic techniques for the encoding and synthesis of U-relational hypothesis databases.\nN4. In terms of uncertainty and statistical analysis, we stick to (i) process some\nwell-defined forms of multiplicity in the data which constitute the model of uncertainty dealt with in this work; then (ii) by relying on MayBMS we perform probabilistic inference; and (iii) eventually (at application level) we perform Bayesian inference and so that a posterior probability distribution is propagated through p-DB updates. We do not provide any additional form of uncertainty management. Rather, we manage the data extracted into the system (under user control) and process its uncertainty in terms of the specific sources of uncertainty recognized in \u03a5-DB (cf. Chapter 2).\n1.3. Thesis Contributions\nThe contributions of this thesis are outlined as follows."}, {"heading": "1.3.1 Innovative Contributions", "text": "This thesis presents the vision of hypotheses as data (and its use case) so-\ncalled \u03a5-DB vision. It has been published in the vision track of VLDB 2014 [10], for its (sic.) potentially high-impact visionary content. The innovative system of \u03a5-DB has been described in a \u2018system prototype demonstration\u2019 paper [28].9"}, {"heading": "1.3.2 Technical Contributions", "text": "This thesis presents specific technical developments over the\u03a5-DB vision. In\nshort, it shows how to encode deterministic hypotheses as uncertain and probabilistic data. Our detailed technical contributions (cf. Chapters 3, 4, and 5) are formulated into a formal method for the design of hypothesis p-DB\u2019s which is described in a technical report [29].10 The method, together with our realistic testbed scenarios and performance evaluation, are yet to be published.\n9 Preliminary version available at CoRR abs/1411.7419. 10 Preliminary version available at CoRR abs/1411.5196."}, {"heading": "1.4. THESIS OUTLINE 14", "text": "1.4. Thesis Outline\nThe structure of the remainder of this thesis is outlined for reference.\nChapter 2. [\u03a5-DB Vision]. The research vision of hypotheses as (uncertain and probabilistic) data, the characterization of its use case, key points and technical challenges are presented.\nChapter 3. [Encoding]. The problem of encoding a hypothesis \u2018as data\u2019 given its formal specification (set of mathematical equations) is presented and addressed by an encoding scheme that transforms the equations into fd\u2019s with guarantees in terms of preserving the hypothesis causal structure.\nChapter 4. [Causal Reasoning]. It is presented a technique for causal reasonig as acyclic pseudo-transitive reasoning over the encoded fd\u2019s. It processes the hypothesis causal ordering to find the \u2018first causes\u2019 for each of its predictive variables.\nChapter 5. [p-DB Synthesis]. It is presented a technique to address the problem of uncertainty introduction and propagation for the transformation of hypotheses into U-relational databases. The synthesized U-database is shown to bear desirable properties for hypothesis management and predictive analytics.\nChapter 6. [Applicability]. A discussion of applicability, the implementation of the proposed techniques into a prototype system for test and demonstration of the vision realization through realistic case studies are presented.\nChapter 7. [Conclusions]. Research questions are revisited, and the significance and limitations of the thesis with directions to future work and final considerations are discussed.\nChapter 2\nVision: Hypotheses as Data\nHigh-throughput technology and large-scale scientific experiments provide\nscientists with empirical data that has to be extracted, transformed and loaded before it is ready for analysis [1]. In this vision we consider theoretical data, or data generated by simulation from deterministic scientific hypotheses, which also needs to be pre-processed to be analyzed.\nHypotheses as data. In view of the age of data-driven science, we consider\ndeterministic scientific hypotheses from a multi-fold point of view: formed as principles or learned in large scale,1 hypotheses are formulated mathematically and coded in a program that is run to give their decisive form of data (see Fig. 1.1).\nUncertain data. The semantic structure of relation FALL (Fig. 1.1), item\n(iv) can be expressed by the functional dependency (fd) t \u2192 v s. This is typical semantics assigned to empirical data in the design of experiment databases. A space-time dimension (like time t in our example) is used as a key to observables (like velocity v and position s). In empirical uncertainty, it is such \u201cphysical\u201d dimension keys like t that may be violated, say, by alternative sensor readings.\nHypotheses, however, are tentative explanations of phenomena [15], which\ncharacterizes a different kind of uncertain data. In order to manage such theoretical uncertainty, we shall need two special attributes to compose, say, the epistemological dimension of keys to observables: \u03c6, identifying the studied phenomena; and \u03c5, identifying the hypotheses aimed at explaining them. That is, we shall leverage the semantics of relations like FALL to \u03c6 \u03c5 t\u2192 v s. This leap is a core abstraction 1 As exhibited, e.g., in the Eureqa project [2].\n16\nin this vision of \u03a5-DB.\nPredictive data. Scientific hypotheses are tested by way of their predic-\ntions [15]. In the form of mathematical equations, hypotheses symmetrically relate aspects of the studied phenomenon. However, for computing predictions, deterministic hypotheses are applied asymmetrically as functions [30]. They take a given valuation over input variables (parameters) to produce values of output variables (predictions). By observing that, we shall seek a principled method to transform the (symmetric) mathematical equations of a hypothesis into (asymmetric) fd\u2019s.\nBy looking at deterministic hypotheses as alternative functions to predict\ndata (see Fig. 2.1), in this vision we shall deal with two sources of uncertainty. Given a well-defined context with a set of alternative hypotheses aimed at explaining (providing predictions for) a selected phenomenon:\n\u2022 Theoretical uncertainty,2 comprises selecting the best tentative model\n(function) to produce (the best) data?\n\u2022 Empirical uncertainty,3 comprises, for each candidate model, what is the\n(parameter) input setting that calibrates it the best way for the selected phenomenon?\nNote that these two sources of uncertainty are intertwined in that one cannot\n\u2018clean\u2019 one without cleaning the other \u2014 neither theory nor parameters are directly\n2 That is, multiplicity of hypothesis entries associated with a phenomenon. 3 That is, multiplicity of hypothesis trial entries associated with a phenomenon.\n17\nobservable, but only their joint results (the predictions) [15]. In this thesis we aim at providing means to support such kind of \u2018integrated\u2019 analytics.\nApplications. Big computational science research programs such as the\nHuman Brain Project,4 or Cardiovascular Mathematics,5 are highly-demanding applications challenged by such theoretical (big) data. Users need to analyze results of hundreds to thousands of data-intensive simulation trials.\nBesides, recent initiatives on web-based model repositories have been foster-\ning large-scale model integration, sharing and reproducibility in the computational sciences (e.g., [31, 32, 33]). They are growing reasonably fast on the web, (i) promoting some MathML-based standard for model specification, but (ii) with limited integrity and lack of support for rating/ranking competing models. For those two reasons, they provide a strong use case for our vision of hypothesis management. The Physiome project [33, 34], e.g., is planned to integrate several large deterministic models of human physiology \u2014 a fairly simple model of the human cardiovascular system, e.g., has about 600+ variables.\nAlso, there is a pressing call for deep predictive analytic tools to support users\nassessing what-if scenarios in business enterprises [35]. Deep predictive analytics are based on first principles (deterministic hypotheses) and go beyond descriptive analytics or shallow predictive analytics such as statistical forecasting (ibid.).6\nU-relations. All that ratifies that hypothesis management is a promising\nclass of applications for probabilistic DB\u2019s. The vision of \u03a5-DB is currently set to be delivered on top of U-relations and probabilistic world-set algebra (p-WSA) [18]. These were developed in the influential MayBMS project.7 As implied by some of its design principles, viz., compositionality and the ability to introduce uncertainty, MayBMS\u2019 query language fits well to hypothesis management. We shall look at it, as previously mentioned, from the point of view of a synthesis method for p-DB design. We shall particularly make use of the repair key operation, which gives\n4 http://www.humanbrainproject.eu/. 5 http://icerm.brown.edu/tw14-1-pdecm. 6 The concept of \u2018deep\u2019 predictive analytics is from Haas et al. [35], and is discussed in more detail in \u00a72.6.1. 7 Project website: http://maybms.sourceforge.net/. MayBMS is as a backend extension of PostgreSQL. It offers all the traditional querying capabilities of the latter in addition to the uncertain and probabilistic\u2019s."}, {"heading": "2.1. RUNNING EXAMPLE 18", "text": "rise to alternative worlds as maximal-subset repairs of an argument key.\nPredictive analytics tool. In database research (e.g., [36]), uncertainty\nis usually seen as an undesirable property that hinders data quality. We shall refer to U-relations and p-WSA as implemented in MayBMS, nonetheless, to show that the ability to introduce controlled uncertainty into an (otherwise complete) simulation dataset can be a tool for \u2018deep\u2019 predictive analytics on a set of competing or alternative hypotheses. Fig.2.2 shows such a scenario of hypotheses \u2018as data\u2019 compete to explain a phenomenon \u2018as data.\u2019\nAs a roadmap to most of the remainder of this chapter, we claim that if hy-\npotheses can be encoded and identified (see \u00a72.2), and their uncertainty quantified by some probability distribution (see \u00a72.4), then they can be rated/ranked and browsed by the user under selectivity criteria. Furthermore, their probabilities can be conditioned for possibly being re-ranked in the presence of evidence (see \u00a72.5).\n2.1. Running Example\nLet us consider Example 1 for the presentation of the vision.\nExample 1 A research is conducted on the effects of gravity on a falling object in the Earth\u2019s atmosphere. Scientists are uncertain about the precise object\u2019s"}, {"heading": "2.1. RUNNING EXAMPLE 19", "text": "density and its predominant state as a fluid or a solid. Three hypotheses are then considered as alternative explanations of the fall (see Fig. 2.3). Due to parameter uncertainty, six simulation trials are run for H1, and four for H2 and H3 each. 2\nThe construction of \u03a5-DB, a Data Warehouse (DW), requires a simple user description of a research. That is, descriptive records of the phenomena and hypotheses dimensions (see Fig. 2.3) are to be inserted first such that basic referential constraints are satisfied by their associated datasets (fact tables). For instance, each one of the six trial datasets for hypothesis H1 shall reference its id \u03c5 = 1 as a foreign key from table HYPOTHESIS further in their synthesized relations.\nFig. 2.4 shows the \u2018big\u2019 fact table H1 for hypothesis \u03c5=1 loaded with its trial\ndatasets for phenomenon \u03c6 =1. Although table H1 is denormalized for faster data retrieval as usual in DW\u2019s, the extraction of the hypothesis equations allows to render it automatically since all variables must appear in some equation. Now we proceed to the hypothesis encoding and start to address research questions RQ1-4."}, {"heading": "2.2. HYPOTHESIS ENCODING 20", "text": "2.2. Hypothesis Encoding\nWe aim at extracting, for each hypothesis, a set of fd\u2019s from its mathematical\nequations. Suppose we are given a set of equations of hypothesis H1 below, and let us examine the set \u03a31 of fd\u2019s we target at. 8\nH1. Law of free fall\na(t) = g v(t) = \u2212gt + v0 s(t) = \u2212(g/2)t2 + v0 t + s0\n\u03a31 = { \u03c6 \u2192 g, \u03c6 \u2192 v0, \u03c6 \u2192 s0,\ng \u03c5 \u2192 a, g v0 t \u03c5 \u2192 v,\ng v0 s0 t \u03c5 \u2192 s }.\nIn order to derive \u03a31 from the equations of H1, we focus on their implicit data dependencies and get rid of constants and possibly complex mathematical constructs. Equation v(t) =\u2212gt+v0, e.g., written this way (roughly speaking), suggests that v is a prediction variable functionally dependent on t (the physical dimension), g and v0 (the parameters). Yet a dependency like g v0 t\u2192 v may hold for infinitely many equations.9 In fact, we need a way to identify H1\u2019s mathematical formulation precisely, i.e., an abstraction of its data-level semantics. This is achieved by introducing hypothesis id \u03c5 as a special attribute in the fd (see \u03a31).\nThis is a data representation of a deterministic scientific hypothesis. It is built into an encoding scheme (see \u00a73.4) that leverages the semantics of structural equations.\nThe other special attribute, the phenomenon id \u03c6, is supposed to be a key to the values of parameters, i.e., determination of parameters is an empirical, phenomenondependent task. The fd \u03c6 \u2192 g v0 s0 is to be (expectedly) violated when the user 8 Recall that a rigorous presentation of the method to encode fd set \u03a31 is due by Chapter 3. 9 Think of, say, how many polynomials satisfy that dependency \u2018signature.\u2019"}, {"heading": "2.3. REASONING OVER FD\u2019S 21", "text": "is uncertain about the values of parameters. The same rationale applies to derive \u03a32 =\u03a33 from the equations of H2, H3 below. These, n.b., vary in structure w.r.t. H1 (e.g., they include parameter D, the object\u2019s diameter).\nH2. Stokes\u2019 law H3. Velocity-squared law a(t) = 0 a(t) = 0\nv(t) = \u2212 \u221a gD/ 4.6\u00d710\u22124 v(t) = \u2212gD2/ 3.29\u00d710\u22126\ns(t) = \u2212t \u221a gD/ 4.6\u00d710\u22124+s0 s(t) = \u2212(gD2/ 3.29\u00d710\u22126) t+s0\n\u03a32 = \u03a33 = { \u03c6 \u2192 g, \u03c6 \u2192 D, \u03c6 \u2192 s0, \u03c6 \u2192 a,\ng D \u03c5 \u2192 v, g D s0 t \u03c5 \u2192 s }.\nThe key point here is that, if the hypothesis structure (set of equations) is\ngiven in a machine-readable format for mathematics, then the method to extract the hypothesis fd set from its equations can be carefully designed based on such hypothesis data representation abstraction. In fact, we shall explore W3C\u2019s MathML as a format for hypothesis specification.10\n2.3. Reasoning over FD\u2019s\nOnce each hypothesis fd set has been extracted, some reasoning is to be performed to discover implicit data dependencies. In fact, dependency theory is equipped with a formal system (cf. \u00a74.1) for reasoning over fd sets like \u03a31 and derive other fd\u2019s in its closure \u03a3+1 . As we elaborate on in Chapter 4, we shall be particularly concerned with the pseudo-transitivity inference rule. Applied over fd\u2019s {\u03c6 \u2192 g, g \u03c5 \u2192 a } \u2282 \u03a31, for instance, it gives us \u03c6 \u03c5 \u2192 a . This inference allows us to observe that {g} is a \u2018factor\u2019 on the uncertainty of a, but {\u03c6 \u03c5} should be a dimensional key constraint for values of a.\nIn fact, note that derived fd\u2019s like \u3008\u03c6 \u03c5, a\u3009 \u2208 \u03a3+1 , which should be a constraint\non values of a in H1, are (expectedly) violated in the presence of uncertainty: ob-\n10 http://www.w3.org/Math/."}, {"heading": "2.4. UNCERTAINTY INTRODUCTION 22", "text": "serve in Fig. 2.4 the multiplicity {32.0, 32.2} of values of a under the same pair (\u03c6 7\u2192 1, \u03c5 7\u2192 1), which should functionally determine them in H1. For that reason we admit a special attribute \u2018trial id\u2019 tid to be overimposed into H1 for a trivial repair, provisionally, until uncertainty can be introduced in a controlled way by synthesis \u20184U.\u2019 It is meant to identify simulation trials and \u201cpretend\u201d certainty not to lose the integrity of the data. It is under this imposed certainty that the raw simulation trial data is safely loaded from files (see Fig. 2.4). Note, however, how \u2018certainty\u2019 is held at the expense of redundancy and, mostly important, opaqueness for predictive analytics (since tid isolates or hides the inconsistency w.r.t. to the violated constraints). This is until the next stage of the \u03a5-DB construction pipeline, when uncertainty is to be introduced in a controlled manner.\n2.4. Uncertainty Introduction\nBefore we proceed to the uncertainty introduction procedure, note in relation\nH1 (Fig. 2.4), that the predicted acceleration values a are such that an association between the hypothesis and a target phenomenon, viz., (\u03c6 7\u2192 1, \u03c5 7\u2192 1) is established. In fact, as of the insertion of each hypothesis trial dataset, the user must set for it a target phenomenon. This may be non-obvious but is quite convenient a design decision for the envisioned system of \u03a5-DB because hypotheses, as (abstract) universal statements [15], can only be derived predictions from (be empirically grounded) by assigning (callibrating) them onto some real-world phenomenon. This assignment is set at data entry time because in fact it only holds at the data level.11 It is to be recorded in an \u2018explanation\u2019 table named H0 by default (see Fig. 2.5, top), being provided with weights for establishing a prior probability distribution which (by user choice) may or may not be uniform.\nThe data transformation of \u2018certain\u2019 to \u2018uncertain\u2019 relations then starts with\nquery Q0, whose result set is materialized into U-relational table Y0 (see Fig. 2.5). As we introduce in detail in \u00a75.1, U-relations have in their schema a set of pairs (Vi, Di) of condition columns [18] to map each discrete random variable xi created by the repair-key operation to one of its possible values (e.g., x0 7\u2192 1). The 11 Hypotheses are \u2018universal\u2019 by definition [15]. They (must) qualify for a class of different situated phenomena, while its predictive datasets must be very specific (for one specific situation)."}, {"heading": "2.4. UNCERTAINTY INTRODUCTION 23", "text": "world table W is internal to MayBMS\u2019 and automatically stores their marginal probabilities. The formal semantics the repair-key operation is given in \u00a75.1.\nQ0. create table Y0 as select \u03c6, \u03c5 from (repair key \u03c6 in H0 weight by Conf);\nThe possible-world semantics of p-DB\u2019s (cf. \u00a75.1) can be seen as a gener-\nalization of data cleaning. In the context of p-DB\u2019s [17], data cleaning does not have to be one-shot \u2014 which is more error-prone [37]. Rather, it can be carried out gradually, viz., by keeping all mutually inconsistent tuples under a probability distribution (ibid.) that can be updated in face of evidence until the probabilities of some tuples eventually tend to zero to be eliminated. This motivates Remark 1.\nRemark 1 Consider U-relational table Y0 (Fig. 2.5). Note that it abstracts the goal of a data-intensive hypothesis evaluation study, or the scientific method itself [15], as the repair of each \u03c6 as a key. That is, in \u03a5-DB users can develop their research directly upon data with support of query and update capabilities to rate/rank their hypotheses \u03c5 w.r.t. each \u03c6, until the relationship r(\u03c6, \u03c5) is repaired to be a function f : \u03a6\u2192 \u03a5 from each phenomenon \u03c6 to its best explanation \u03c5. 2\nGiven a \u2018big\u2019 fact table such as H1, we need to identify/group the correlated\ninput attributes under independent uncertainty units, viz., \u2018u-factors,\u2019 each one associated with a random variable.12 We illustrate that by means of query Q1, which materializes view Y1[g] for (let g = Zi) identified u-factor Zi \u2286 Z in H1[\u03c6, Z].\n12 An attribute can be inferred \u2018input\u2019 (viz., a parameter) by means of fd reasoning (cf. \u00a73.4)."}, {"heading": "2.4. UNCERTAINTY INTRODUCTION 24", "text": "Q1. create table Y1[g] as select U.\u03c6, U.g from (repair key \u03c6 in\n(select \u03c6, g, count(*) as Fr from H1 group by \u03c6, g) weight by Fr) as U;\nThe result set of Q1 is stored in Y1[g], see Fig. 2.6. Note that the possible values of g are mapped to random variable x1, and that table H1 is considered source for a joint probability distribution (on the values of H1\u2019s input parameters) which may not be uniform: we count the frequency Fr of each possible value of a u-factor Zi \u2286 Z (as done for g in Q1) and pass it as argument to the weight-by construct.\nSo far, we have presented informally the procedure of u-factorization. Now\nwe proceed to u-propagation \u2014 both are presented rigorously in Chapter 5. We consider g \u03c5 \u2192 a \u2208 \u03a31 again in order to synthesize predictive U-relation Y1[a]. Since a is functionally determined by \u03c5 and g only, and these are independent, we propagate their uncertainty onto a into Y1[a] by query Q2.\nQ2. create table Y1[a] as select H1.\u03c6, H1.\u03c5, H1.a from H1, Y0, Y1[g] as G\nwhere H1.\u03c6=Y0.\u03c6 and H1.\u03c5=Y0.\u03c5 and G.\u03c6=H1.\u03c6 and G.g=H1.g;\nQuery Q\u20322 (not shown) then selects \u03c6, \u03c5 and a from Yi[a] for each i = 1..3. The result sets of Q2 and Q \u2032 2 (resp. Y1[a] and Y [a]) are shown in Fig. 2.7."}, {"heading": "2.5. PREDICTIVE ANALYTICS 25", "text": "Compare relations H1[a] and Y1[a]. By accounting for the correlations cap-\ntured in the fd g \u03c5 \u2192 a, we could propagate onto a the uncertainty coming from the hypothesis and the only parameter a is sensible to, thus precisely situating tuples of Y1[a] in the space of possible worlds. The same is done for predictive attributes v and s. In the end, \u03a5-DB shall be ready for predictive analytics, i.e., with all competing predictions as possible alternatives which are mutually inconsistent.\nA key point here is that all the synthesis process is amenable to algorithm\ndesign. Except for the user \u2018research\u2019 description, the \u03a5-DB construction is fully automated based on the hypothesis structure (set of equations) and the raw hypothesis trial data.\n2.5. Predictive Analytics\nUsers of Example 1, has to be able, say, to query phenomenon \u03c6 = 1 w.r.t.\npredicted position s at specific values of time t by considering all hypotheses \u03c5 admitted. That is illustrated by query Q3, which creates integrative table Y [s]; and by query Q4, which computes the confidence aggregate operation [18] for all s tuples where t = 3 (Fig. 2.8 shows Q4\u2019s result, apart from column Posterior).\nThe confidence on each hypothesis for the specific prediction of Q4 is split\ndue to parameter uncertainty such that they sum up back to its total confidence. For H2 and H3, e.g., we have {g D s0 t \u03c5 \u2192 s} \u2282 \u0393, where \u0393 = \u03a32 = \u03a33. Since g and D are the parameter uncertainty factors of s (s0 is certain), with 2 possible values (not shown) each, then there are only 2\u00d72 = 4 possible s tuples for H2 and H3 each. Considering all hypotheses \u03c5 for the same phenomenon \u03c6, the confidence"}, {"heading": "2.5. PREDICTIVE ANALYTICS 26", "text": "values sum up to one in accordance with the laws of probability.\nUsers can make informed decisions in light of such confidence aggregates,\nwhich are to be eventually conditioned in face of evidence (observed data). Example 2 features such kind of Bayesian conditioning for discrete random variables mapped to the possible values of predictive attributes (like position s) whose domain are continuous.\nExample 2 Suppose position s = 2250 feet is observed at t = 3 secs, with standard deviation \u03c3 = 20. Then, by applying Bayes\u2019 theorem for normal mean with a discrete prior [16], Prior is updated to Posterior (see Fig. 2.8). 2"}, {"heading": "2.6. RELATED WORK 27", "text": "The procedure uses normal density function (2.1), with (say) \u03c3 = 20, to get\nthe likelihood f(y |\u00b5k) of each alternative prediction of s from Y [s] as mean \u00b5k given y at observed s = 2250. Then it applies Bayes\u2019 rule (2.2) to get the posterior p(\u00b5k | y) [16].\nf(y |\u00b5k) = 1\u221a\n2\u03c0\u03c32 e\u2212 1 2\u03c32 (y\u2212\u00b5k)2 (2.1)\np(\u00b5k | y) = f(y |\u00b5k) p(\u00b5k) / \u2211n i=1 f(y |\u00b5i) p(\u00b5i) (2.2)\nIn the general case (cf. examples shown in Chapter 6), we actually have phenomenon \u2018as data:\u2019 a sample of independent observed values y1, ..., yn (e.g., Brazil\u2019s population observed by census over the years). Then, the likelihood f(y1, ..., yn |\u00b5k)\nfor each competing trial \u00b5k, is computed as a product \u220fn j=1 f(yj |\u00b5kj) of the single likelihoods f(yj |\u00b5kj) [16]. Bayes\u2019 rule is then settled by (2.3) to compute the posterior p(\u00b5k | y1, ..., yn) given prior p(\u00b5k).\np(\u00b5k | y1, . . . , yn) = \u220fn\nj=1 f(yj |\u00b5kj) p(\u00b5k) m\u2211 i=1 n\u220f j=1 f(yj |\u00b5ij) p(\u00b5i) (2.3)\nAs a result, the prior probability distribution assigned to u-factors via repair key\nis to be eventually conditioned on observed data. This is an applied Bayesian inference problem that translates into a p-DB update one to induce effects of posteriors back to table W . In a first prototype of the \u03a5-DB system (cf. 6.3), we accomplish it by performing Bayesian inference at application level and then applying p-WSA\u2019s update (a variant of SQL\u2019s update) into MayBMS. This solution is good enough to let us complete use case demonstrations of \u03a5-DB.13\n2.6. Related Work\nThe vision of managing hypotheses as data has some roots in Porto and\nSpaccapietra [38], who motivated a conceptual data model to support (the socalled) in silico science by means of a scientific model management system. We\n13 Cf. Chapter 6, and \u00a76.3 in particular."}, {"heading": "2.6. RELATED WORK 28", "text": "discuss now the work we understand to be mostly related to our vision of datadriven hypothesis management and analytics."}, {"heading": "2.6.1 Models-and-data", "text": "Haas et al. [35] provide an original long-term perspective on the evolution of\ndatabase technology. They characterize the data typically managed by traditional DB systems as a record about the past, not a conclusion or an insight or a solution (ibid.). In the context of scientific databases, e.g., their position is suggestive that DB technology has been designed for empirical data, not the theoretical data generated by simulation from domain-specific principles or scientific hypotheses.\nThey recognize current DB technology to have raised the art of scalable\n\u2018descriptive\u2019 analytics to a very high level; but point out, however, that nowadays (sic.) what enterprises really need is \u2018prescriptive\u2019 analytics to identify optimal business, policy, investment, and engineering decisions in the face of uncertainty. Such analytics, in turn, shall rest on deep \u2018predictive\u2019 analytics that go beyond mere statistical forecasting and are imbued with an understanding of the fundamental mechanisms that govern a system\u2019s behavior, allowing what-if analyses [35]. In sum, there is a pressing call for deep predictive analytic tools in business enterprises as much as in science\u2019s.\nIn comparison with the \u03a5-DB vision, Haas et al. are proposing a long-term\nmodels-and-data research program to pursue data management technology for deep predictive analytics. They discuss strategies to extend query engines for model execution within a (p-)DB. Along these lines, query optimization is understood as a more general problem with connections to algebraic solvers.\nOur framework in turn essentially comprises an abstraction and technique\nfor the encoding of hypotheses as data. It can be understood (in comparison) as putting models strictly into a flattened data perspective. For that reason it has been directly applicable by building upon recent work on p-DBs [17]. In principle, it can be integrated into, say, the OLAP layer of the models-and-data project."}, {"heading": "2.6.2 Scientific simulation data", "text": "As previsouly mentioned, science\u2019s ETL is distinguished by its unfrequent, incremental-only updates and by having large raw files as data sources [8]. Challenges for"}, {"heading": "2.6. RELATED WORK 29", "text": "enabling an efficient access to high-resolution, raw simulation data have been documented from both supercomputing,[6] and database research viewpoints;[39] and pointed as key to the use case of exploratory analytics. The extreme scale of the raw data has motivated such non-conventional approaches for data exploration, viz., the \u2018immersive\u2019 query processing (move the program to the data) [6, 40], or \u2018in situ\u2019 query processing in the raw files [41, 42]. Both exploit the spatial structure of the data in their indexing schemes.\nThat line of research is motivated for equipping scientist end-users for an\nimmediate interaction with their very large simulation datasets.14 The NoDB approach, in particular, argues to eliminate such ETL phase (viz., the loading) for a direct access to data \u2018in situ\u2019 in the raw data files [42]. In fact, data exploration is a fundamental use case of data-driven science.\nNonetheless, being generated from first principles or learned deterministic\nhypotheses, simulation data has a pronounced uncertainty component that motivates a another use case, viz., the case of hypothesis management and predictive analytics [35, 10]. As we have motivated in \u00a71.1, the latter requires probabilistic DB design for enabling uncertainty decomposition (factorization).\nHypothesis management shall not deal with the same volume of data as in\nsimulation data management for exploratory analytics, but samples of it (cf. Table 1.1 for a comparison). For instance, in CERN\u2019s particle-physics experiment ATLAS there are four tier/layers of data management. The volume of data significantly decreases from the (tier-0) raw data to the (tier-3) data actually used for analyses such as hypothesis testing [8, p. 71-2].\nOverall, the overhead incurred in loading samples of raw simulation trial\ndatasets into a p-DB is justified for enabling a principled hypothesis evaluation and rating/ranking according to the scientific method."}, {"heading": "2.6.3 Hypothesis encoding", "text": "Our framework is comparable with Bioinformatics\u2019 initiatives that address\nhypothesis encoding into the RDF data model [43]: (i) the Robot Scientist [44] is a knowledge-base system (KBS) for automated generation and testing of hypotheses\n14 Sometimes phrased \u2018here is my files, here is my queries, where are my results?\u2019 [41]."}, {"heading": "2.7. SUMMARY: KEY POINTS 30", "text": "about what genes encode enzymes in the yeast organism; (ii) HyBrow [45] is a KBS for scientists to test their hypotheses about events of the galactose metabolism also of the yeast organism; and (iii) SWAN [46] is a KBS for scientists to share hypotheses on possible causes of the Alzheimer disease.\nThe Robot Scientist relies on rule-based logic programming analytics to au-\ntomatically generate and test RDF-encoded hypotheses of the kind \u2018gene G has function A\u2019 against RDF-encoded empirical data [44]. HyBrow is likewise, but hypotheses are formulated by the user about biological events [45]. SWAN in turn disfavors analytic techniques for hypothesis evaluation and focus on descriptive aspects: hypotheses are high-level natural language statements retrieved from publications. Each \u2018hypothesis\u2019 is associated with lower-level \u2018claims\u2019 (both RDFencoded) that are meant to support it on the basis of some empirical evidence (RDF-encoded gene/protein data). In particular, SWAN [46] differs from the former in that each hypothesis is unstructured, being then more related to efforts on the retrieval of textual claims from the narrative fabric of scientific reports [47].\nAll of them though, consist in some ad-hoc RDF encoding of sequence and\ngenome analysis hypotheses under varying levels of structure (viz., from \u2018gene G has function A\u2019 statements to free text). Our framework in turn consists in the U-relational encoding of hypotheses from mathematical equations, which is (to our knowledge) the first work on hypothesis relational encoding.\nFinally, as for hypothesis evaluation and comparison analytics, the \u03a5-DB\nvision is distinguished in terms of its Bayesian inference approach. The latter has been pointed out as a major direction for the improvement of the Bioinformatics\u2019 initiatives just mentioned (cf. [43, p. 13]), and is in fact an influential model of decision making for hypothesis evaluation [15, p. 220].\n2.7. Summary: Key Points\nWe outline some key points in the \u03a5-DB vision:\n\u2022 \u2018Structured deterministic hypotheses\u2019 are encoded as theoretical data and\ndistinguished from empirical data by the introduction of an epistemological dimension into their semantic structure."}, {"heading": "2.7. SUMMARY: KEY POINTS 31", "text": "\u2022 Two sources of uncertainty are considered: theoretical uncertainty, origi-\nnating from competing hypotheses; and empirical uncertainty, derived from alternative simulation trials on each hypothesis for the same phenomenon.\n\u2022 A method to extract the structure of a hypothesis can be carefully designed\nbased on a hypothesis data representation and shall be reducible in terms of machine-readable format for mathematical modeling, viz., W3C\u2019s MathML, which we shall adopt as a standard for hypothesis specification.\n\u2022 We have seen that the controlled introduction of uncertainty into simula-\ntion data is amenable to algorithm design and then reducible to a designtheoretic synthesis method for the construction of U-relational DB\u2019s.\n\u2022 Simulation data can be modeled as hypothesis data whenever it is associ-\nated with a target phenomenon. As the same phenomenon may happen to be associated with many such hypotheses, the research activity can be modeled as a data cleaning problem in p-DB\u2019s.\nEssentially, the vision of \u03a5-DB comprises a design-theoretic pipeline (Fig.\n1.4). For the insertion of a hypothesis k, we shall be given a MathML-compliant structure Sk together with its simulation trial datasets D`k in raw files (e.g., .mat, .csv). Then we apply an Extract-Transform-Load (ETL) automatic procedure to generate the hypothesis \u2018big\u2019 fact table Hk under the trial id\u2019s.\nThe extracted equations are firstly encoded into fd\u2019s. Then, at any time, as\nmany hypotheses may have been inserted into the system, the uncertainty introduction (U-intro) procedure can be applied to process the encoded fd\u2019s and synthesize the \u2018uncertain\u2019 U-relations that are to be eventually conditioned on observations.\nNote, in Fig. 1.4, that the ETL procedure is operated in a \u2018local\u2019 view for\neach hypothesis k, while the U-intro procedure and the conditioning are operated in the \u2018global\u2019 view of all available hypotheses k = 1..n. The pipeline opens up four main tracks of technical research challenges from the ETL stage on, viz., (i) hypothesis encoding and (ii) causal reasoning over fd\u2019s, (iii) p-DB synthesis and (iv) conditioning. We address in the sequel the three first track of challenges in depth. The problem of conditioning is outlined for further work in \u00a77.3.\nChapter 3\nHypothesis Encoding\nIn this chapter we address the problem of hypothesis encoding. In \u00a73.1 we\nintroduce notation and basic concepts of structural equations and the problem of causal ordering. In \u00a73.2 we study the problem of extracting the causal ordering implicit in the structure of a deterministic hypothesis and show that Simon\u2019s classical approach [24, 48] is intractable. In \u00a73.3 then we build upon a less notorious approach of Nayak\u2019s [49] and borrow an efficient algorithm for it that fits very well our use case for hypothesis encoding. In \u00a73.4 we develop an encoding scheme that builds upon the idea of structural equations through an original abstraction of hypotheses \u2018as data.\u2019 In \u00a73.5 we present experiments that attest how the encoding scheme works in practice for large hypotheses. In \u00a73.6 we discuss related work. In \u00a73.7 we summarize the results of this chapter.\n3.1. Preliminaries: Structural Equations\nGiven a system of mathematical equations involving a set of variables, to build a structural equation model (SEM) is, essentially, to establish a one-to-one mapping between equations and variables [24]. That shall enable further detecting the hidden asymmetry between variables, i.e., their causal ordering. For instance, Einstein\u2019s famous equation E = mc2 states the equivalence of mass and energy, summarizing a theory that can be imputed two different asymmetries (for different applications), say, given a fixed amount of mass m = m0 (and recall c is a constant), predict the particle\u2019s relativistic rest energy E; or given the particle\u2019s rest energy, predict its mass or potential for nuclear fission."}, {"heading": "3.1. PRELIMINARIES: STRUCTURAL EQUATIONS 33", "text": "To stress the point, consider Newton\u2019s second law F = ma in such a scalar\nsetting. The modeler can either use it to compute (predict), say, acceleration values given an amount of mass and different force intensities, or to predict force intensities given a fixed acceleration (e.g., for testing an engineered dynamometer). The point here is that Newton\u2019s equation is not enough to derive predictions. That is, it has a number of variables |V| = 3, which is larger than |E| = 1. It must be completed with two more equations in order to qualify as an (applied) hypothesis \u2018as data.\u2019 Although usually it is interpreted an asymmetry towards a, technically, there is nothing in its semantics to suggest so.1 Compare the two systems given in Fig. 3.1.2 In sum, the causal ordering of any system of equations is not to be guessed, as it can be inferred. In this chapter we rely on previous work (mostly AI\u2019s work, viz., [24, 49, 48]) and adapt it for the encoding of hypotheses into fd\u2019s.\nDef. 1 A structure is a pair S(E ,V), where E is a set of equations over set V of variables, |E| \u2264 |V|, such that:\n(a) In any subset of k equations of the structure, at least k different variables\nappear;\n(b) In any subset of k equations in which r variables appear, k \u2264 r, if the\nvalues of any (r\u2212k) variables are chosen arbitrarily, then the values of the 1 As the equality construct \u2018=\u2019 is used as a predicate, not an assignment operator. 2 We shall introduce the notion of \u2018directed causal graphs\u2019 shortly."}, {"heading": "3.1. PRELIMINARIES: STRUCTURAL EQUATIONS 34", "text": "remaining k variables can be determined uniquely \u2014 finding these unique values is a matter of solving the equations.\nDef. 2 Let S(E ,V) be a structure. We say that S is self-contained or complete if |E| = |V|.\nIn short, we are interested in systems of equations that are \u2018structural\u2019 (Def. 1) and \u2018complete\u2019 (Def. 2), viz., that has as many equations as variables and no subset of equations has fewer variables than equations.3\nComplete structures can be solved for unique sets of values of their variables.\nIn this work, however, we are not concerned with solving sets of mathematical equations at all, but with processing their causal ordering in view of U-relational DB design. Simon\u2019s concept of causal ordering has its roots in econometrics studies (cf. [24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48]. In this thesis we translate the problem of causal ordering into the language of data dependencies, viz., into fd\u2019s.\nDef. 3 Let S be a structure. We say that S is minimal if it is complete and there is no complete structure S \u2032\u2282 S.\nDef. 4 The structure matrix AS of a structure S(E ,V), with f1, f2, . . . , fn \u2208 E and x1, x2, . . . , xm \u2208 V , is a n \u00d7 m matrix of 1\u2019s and 0\u2019s in which entry aij is non-zero if variable xj appears in equation fi, and zero otherwise.\nElementary row operations (e.g., row multiplication by a constant) on the\nstructure matrix may hinder the structure\u2019s causal ordering and then are not valid in general [24]. This also emphasizes that the problem of causal ordering is not about solving the system of mathematical equations of a structure, but identifying its hidden asymmetries.\nDef. 5 Let S(E ,V) be a complete structure. Then a total causal mapping over S is a bijection \u03d5 : E \u2192 V such that, for all f \u2208 E , if \u03d5(f) = x then x\u2208 V ars(f). 3 Also, we expect the systems of equations given as input to be \u2018independent\u2019 in the sense of Linear Algebra. In our context, that means systems that can only have non-redundant equations. In that case, if some subset of equations has fewer variables than equations, then the system must be \u2018overconstrained.\u2019"}, {"heading": "3.1. PRELIMINARIES: STRUCTURAL EQUATIONS 35", "text": "Simon has informally described an algorithm (cf. [24]) that, given a complete structure S(E ,V), can be used to compute a partial causal mapping \u03d5p from partitions on the set of equations to same-cardinality partitions on the set of variables. As shown by Dash and Druzdzel [48], the causal mapping returned by Simon\u2019s (socalled) Causal Ordering Algorithm (COA) is not total when S has variables that are strongly coupled (because they can only be determined simultaneously). They also have shown that any total mapping \u03d5 over S must be consistent with COA\u2019s partial mapping \u03d5p [48]. The latter is made partial by design (merge strongly coupled variables into partitions or clusters) in order to force its induced causal graph G\u03d5p to be acyclic. Algorithm 1, COAt, is a variant of Simon\u2019sCOA adapted to illustrate our use case. It returns a total causal mapping \u03d5, instead of a partial causal mapping. We illustrate it through Example 3 and Fig. 3.2.\nExample 3 Consider structure S(E ,V) whose matrix is shown in Fig. 3.2a. Note that S is complete, since |E|= |V|= 7, but not minimal. The set of all minimal subsets S \u2032\u2282 S is Sc={ {f1}, {f2}, {f3} }. By eliminating the variables identified at recursive step k, a smaller structure T \u2282 S is derived. Compare the partial causal mapping eventually returned by COA, \u03d5p={ \u3008{f1}, {x1}\u3009, \u3008{f2}, {x2}\u3009, \u3008{f3}, {x3}\u3009, \u3008{f4, f5}, {x4, x5}\u3009, \u3008{f6}, {x6}\u3009, \u3008{f7}, {x7}\u3009 }, to the total causal mapping returned by COAt, \u03d5 ={\u3008f1, x1\u3009, \u3008f2, x2\u3009, \u3008f3, x3\u3009, \u3008f4, x4\u3009, \u3008f5, x5\u3009, \u3008f6, x6\u3009, \u3008f7, x7\u3009}."}, {"heading": "3.2. THE PROBLEM OF CAUSAL ORDERING 36", "text": "Algorithm 1 COAt as a variant of Simon\u2019s COA.\n1: procedure COAt(S : structure over E and V) Require: S given is complete, i.e., |E| = |V| Ensure: Returns total causal mapping \u03d5 : E \u2192 V\n2: \u03d5\u2190 \u2205, Sc \u2190 \u2205 3: for all minimal S \u2032 \u2282 S do 4: Sk \u2190 Sk \u222a S \u2032 . store minimal structures S \u2032 found in S 5: V \u2032 \u2190 S \u2032(V) 6: for all f \u2208 S \u2032(E) do 7: x\u2190 any xa \u2208 V \u2032 8: \u03d5\u2190 \u03d5 \u222a \u3008f, x\u3009 9: V \u2032 \u2190 V \u2032 \\ {x}\n10: T \u2190 S \\ \u22c3 S\u2032\u2208Sk S \u2032 11: if T 6= \u2205 then 12: return \u03d5 \u222a COAt(T ) 13: return \u03d5\nSince x4 and x5 are strongly coupled (see Fig.3.2b), COAt maps them arbitrarily (e.g., it could be f4 7\u2192 x5, f5 7\u2192 x4 instead). Such total mapping \u03d5 renders a cycle in the directed causal graph G\u03d5 induced by \u03d5 (see Fig.3.3). 2\nx1 x2 x3\nx4 x5\nx6 x7\nin a given structure (cf. line 3) is a hard problem that can only be addressed heuristically as a problem of co-clustering (also called biclustering [51, 52]) in"}, {"heading": "3.2. THE PROBLEM OF CAUSAL ORDERING 37", "text": "Boolean matrices. Simon\u2019s approach, however, as we shall see next, is not the only way to cope with the problem of causal ordering.\nIn fact, in order to study the computational properties of SEM\u2019s and the\nproblem of causal ordering, we observe that any structure S(E ,V) satisfying Def. 1 can be modeled straightforwardly as a bipartite graph G = (V1 \u222a V2, E), where the set E of equations and the set V of variables are the disjoint vertex sets, i.e., V1 7\u2192 E , V2 7\u2192 V , and E 7\u2192 S is the edge set connecting equations to the variables appearing in them. Fig. 3.4 shows the bipartite graph G corresponding to the structure given in Example 3 \u2014 for a comprehensive text on graph concepts and its related algorithmic problems, cf. Even [53].\nA biclique (or complete bipartite graph) is a bipartite graph G = (A\u222aB, E)\nsuch that for every two vertices a \u2208 A, b \u2208 B, we have (a, b) \u2208 E [53]. Note that for balanced bicliques, i.e., when |A| = |B| = K, the degree deg(u) of any vertex u \u2208 A \u222aB must be deg(u) = |A| = |B| = K.\nRecent approaches to co-clustering problems (e.g., [54]) have come with the\nnotion of pseudo-biclique (also called \u2018quasi-biclique\u2019), which is a relaxation of the biclique concept to allow some less rigid notion of connectivity than the \u2018complete connectivity\u2019 required in a biclique. Now, recall that Simon\u2019s COA(t) needs to find, at each recursive step, all minimal subsets S \u2032 \u2286 S. Theorem 1 situates this particular computational task in terms of its complexity, which for |E \u2032| = |V \u2032| \u2265 2 is equivalent to find, at each recursive step, the minimal-size \u2018pseudo-bicliques\u2019"}, {"heading": "3.2. THE PROBLEM OF CAUSAL ORDERING 38", "text": "(i.e., with the least K \u2265 2) in its corresponding bipartite graph (e.g., see Fig. 3.4). Here we take, in Def. 6, a specific notion of pseudo-biclique.\nDef. 6 Let G = (A\u222aB, E) be a bipartite graph. We say that G is a K-balanced pseudo-biclique if |A| = |B| = K with |E| \u2265 2K and, for all vertices u \u2208 A\u222aB, deg(u) \u2265 2.\nNow we state (originally) the balanced pseudo-biclique problem (BPBP) as a decision problem as follows.\n(BPBP). Given a bipartite graph G = (V1 \u222a V2, E) and a positive integer K \u2265 2, does G contain a K-balanced pseudo-biclique?\nLemma 1 The balanced pseudo-biclique problem (BPBP) is NP-Complete.\nProof 1 We show (by restriction [55]) that the BPBP is a generalization of the balanced biclique problem (BBP), referred \u2018balanced complete bipartite subgraph\u2019 problem [55, GT24, p. 196], which is shown to be NP-Complete by means of a transformation from \u2018clique\u2019 [56, p. 446]. The restriction from BPBP to BBP (special case) is made by requiring (cf. Def. 6) either (a) |E| = K2 or (b) deg(u) = K, for K \u2265 2,4 which are equivalent ways of enforcing the inquired K-balanced pseudo-biclique to be a K-balanced biclique. 2\nWe introduce another hypothesis structure (see Fig. 3.5) to illustrate the correspondence between the pseudo-biclique property and COA\u2019s algorithmic approach as elaborated in the proof of Theorem 1.\nTheorem 1 Let S(E ,V) be a complete structure. Then the extraction of its causal ordering by Simon\u2019s COA(S) is NP-Hard.\nProof 2 We show that, at each recursive step k of COA, to find all non-trivial minimal subsets (i.e., |E \u2032| \u2265 2) translates into an optimization problem associated with the decision problem BPBP, which we know by Lemma 1 to be NP-Complete. See Appendix \u00a7A.1.1. 2 4 Note that clearly, for any positive integer K \u2265 2, we have for K2 \u2265 2K."}, {"heading": "3.3. TOTAL CAUSAL MAPPINGS 39", "text": "Nonetheless, the problem of causal ordering can be solved efficiently by means\nof a different, less notorious approach due to Nayak [49], which we introduce and build upon next.\n3.3. Total Causal Mappings\nThe problem of causal ordering can be solved in polynomial time by (i)\nfinding any total causal mapping \u03d5 : E \u2192 V over structure S given (cf. Def. 5); and then (ii) by computing the transitive closure C+\u03d5 of the set C\u03d5 (cf. Eq. 3.1) of direct causal dependencies induced by \u03d5.\nC\u03d5= { (xa, xb) | there exists f \u2208 E such that \u03d5(f) = xb and xa \u2208 V ars(f) } (3.1)\nDef. 7 Let S(E ,V) be a structure with variables xa, xb \u2208 V , and \u03d5 a total causal mapping over S inducing set of direct causal dependencies C\u03d5 and its transitive closure C+\u03d5 . We say that (xb, xa) is a direct causal dependency in S if (xb, xa) \u2208 C\u03d5, and that (xb, xa) is a causal dependency in S if (xb, xa) \u2208 C+\u03d5 .\nIn other words, (xa, xb) is in C\u03d5 iff xb direct and causally depends on xa,\ngiven the causal asymmetries induced by \u03d5. Those notions open up an approach to causal reasoning that fits very well to our use case, which is aimed at encoding hypothesis structures into fd sets and then performing (symbolic) causal reasoning in terms of acyclic pseudo-transitive reasoning over fd\u2019s (cf. Chapter 4).5 For it to be effective, nonetheless, we shall need to ensure some properties of total causal mappings first.\n5 Note that it differs from AI research (e.g., [48]) geared for reasoning over GM\u2019s."}, {"heading": "3.3. TOTAL CAUSAL MAPPINGS 40", "text": "For a given structure S, there may be multiple total causal mappings over\nS (recall Example 3). But the causal ordering of S must be unique (see Fig. 3.3). Therefore, a question that arises is whether the transitive closure C+\u03d5 is the same for any total causal mapping \u03d5 over S. Proposition 1, originally from Nayak [49], ensures that is the case.\nProposition 1 Let S(E ,V) be a structure, and \u03d51 : E \u2192 V and \u03d52 : E \u2192 V be any two total causal mappings over S. Then C+\u03d51 = C + \u03d52 .\nProof 3 The proof is based on an argument from Nayak [49], which we present in arguably much clearer way (see Appendix, \u00a7A.1.2). Intuitively, it shows that if \u03d51 and \u03d52 differ on the variable an equation f is mapped to, then such variables, viz., \u03d51(f) and \u03d52(f), must be causally dependent on each other (strongly coupled). 2\nAnother issue is concerned with the precise conditions under which total\ncausal mappings exist (i.e., whether or not all variables in the equations can be causally determined). In fact, by Proposition 2, based on Nayak [49] apud. Hall [53, p. 135-7], we know that the existence condition holds iff the given structure is complete.\nBefore proceeding to it, let us refer to Even [53] to briefly introduce the\nadditional graph-theoretic concepts which are necessary here. A matching in a graph is a subset of edges such that no two edges in the matching share a common node. A matching is said maximum if no edge can be added to the matching (without hindering the matching property). Finally, a matching in a graph is said \u2018perfect\u2019 if every vertex is an end-point of some edge in the matching \u2014 in a bipartite graph, a perfect matching is said a complete matching.\nProposition 2 Let S(E ,V) be a structure. Then a total causal mapping \u03d5 : E \u2192 V over S exists iff S is complete.\nProof 4 We observe that a total causal mapping \u03d5 : E \u2192 V over S corresponds exactly to a complete matching M in a bipartite graph B = (V1 \u222a V2, E), where V1 7\u2192 E , V2 7\u2192 V , and E 7\u2192 S. In fact, by Even apud. Hall\u2019s theorem (cf. [53, 135-7]), we know that B has a complete matching iff (a) for every subset of vertices"}, {"heading": "3.3. TOTAL CAUSAL MAPPINGS 41", "text": "F \u2286 V1, we have |F | \u2264 |E(F )|, where E(F ) is the set of all vertices connected to the vertices in F by edges in E; and (b) |V1| = |V2|. By Def. 1 (no subset of equations has fewer variables than equations), and Def. 2 (number of equations is the same as number of variables), it is easy to see that conditions (a) and (b) above hold iff S is a complete structure. 2\nThe problem of finding a maximum matching is a well-studied algorithmic\nproblem. In this thesis we adopt the Hopcroft-Karp algorithm [57], which is known to be polynomial-time, bounded by O( \u221a |V1|+ |V2| |E|).6 That is, we handle the problem of total causal mapping by (see Alg. 2) translating it to the problem of maximum matching in a bipartite graph (in linear time) and then applying the Hopcroft-Karp algorithm to get the matching and finally translate it back to the total causal mapping, as suggested by the proof of Proposition 2.\nAlgorithm 2 Find a total causal mapping for a given structure.\n1: procedure TCM(S : structure over E and V) Require: S given is a complete structure, i.e., |E| = |V| Ensure: Returns a total causal mapping \u03d5\n2: B(V1 \u222a V2, E)\u2190 \u2205 3: \u03d5\u2190 \u2205 4: for all \u3008f,X\u3009 \u2208 S do . translates the structure S to a bipartite graph B 5: V1 \u2190 V1 \u222a {f} 6: for all x \u2208 X do 7: V2 \u2190 V2 \u222a {x} 8: E \u2190 E \u222a {(f, x)} 9: M \u2190 Hopcroft-Karp(B) . solves the maximum matching problem 10: for all (f, x) \u2208M do . translates the matching to a total causal mapping 11: \u03d5\u2190 \u03d5 \u222a {\u3008f, x\u3009} 12: return \u03d5\nFig. 3.6 shows the complete matching found by the Hopcroft-Karp algorithm for the structure given in Example 3.\n6 The Hopcroft-Karp algorithm solves maximum matching in a bipartite graph efficiently as a problem of finding maximum flow in a network (cf. [53, p. 135-7], or [58, p. 664-9]).\nCorollary 1 summarizes the results we have so far.\nCorollary 1 Let S(E ,V) be a complete structure. Then a total causal mapping \u03d5 : E \u2192 V over S can be found by (Alg. 2) TCM in time that is bounded by\nO( \u221a |E| |S|).\nProof 5 Let B = (V1 \u222a V2, E) be the bipartite graph corresponding to complete structure S given to TCM, where V1 7\u2192 E , V2 7\u2192 V , and E 7\u2192 S. The translation of S into B is done by a scan over it. This scan is of length |S| = |E|. Note that number |E| of edges rendered is precisely the length |S| of structure, where the denser the structure, the greater |S| is. The re-translation of the matching computed by internal procedure Hopcroft-Karp, in turn, is done at expense of |E| = |V| \u2264 |S|. Thus, it is easy to see that TCM is dominated by the maximum matching algorithm\nHopcroft-Karp, which is known to be O( \u221a |V1|+ |V2| |E|), i.e., O( \u221a |E|+ |V| |S|).\nSince S is assumed complete, we have |E| = |V| then \u221a |E|+ |V| = \u221a 2 \u221a |E|.\nTherefore, TCM must have running time at most O( \u221a |E| |S|). 2\nRemark 2 Let S(E ,V) be a complete structure. Then we know (cf. Proposition 2) that a total causal mapping over S exists. Let it be defined \u03d5 , TCM(S). Then the causal ordering implicit in S shall be correctly extracted (cf. Proposition 1) by processing the causal dependencies induced by \u03d5, as we show in Chapter 4. 2\nNow we are ready to accomplish the hypothesis encoding into fd\u2019s, as we\nshow next."}, {"heading": "3.4. THE ENCODING SCHEME 43", "text": "3.4. The Encoding Scheme\nWe shall encode variables as relational attributes and map equations onto\nfd\u2019s through total causal mappings. Let Z be a set of attribute symbols such that Z ' V , where S(E ,V) is a complete structure; and let \u03c6, \u03c5 /\u2208 Z be two special attribute symbols kept to identify (resp.) phenomena and hypotheses. We are explicitly distinguishing symbols in Z, assigned by the user into structure S, from epistemological symbols \u03c6 and \u03c5. Then we consider a sense of Simon\u2019s into the nature of scientific modeling and interventions [24], summarized in Def. 8.\nDef. 8 Let S(E ,V) be a structure and x \u2208 V be a variable. We say that x is exogenous if there exists an equation f \u2208 E such that V ars(f) = {x}. In this case f can be written f(x) = 0, and must be mapped to x in any total causal mapping \u03d5 over S. We say that x is endogenous otherwise.\nRemark 3 introduces an interpretation of Def. 8 with a data dependency flavor.\nRemark 3 The values of exogenous variables (attributes) are to be determined empirically, outside of the system (proposed structure S). Such values are, therefore, dependent on the phenomenon id \u03c6 only. The values of endogenous variables (attributes) are in turn to be determined theoretically, within the system. They are dependent on the hypothesis id \u03c5 and shall be dependent on the phenomenon id \u03c6 as well indirectly. 2\nAs introduced in \u00a72.2, the encoding scheme we are presenting here is not\nobvious. It goes beyond Simon\u2019s structural equations to abstract the data-level semantics of mathematical deterministic hypotheses. Whereas Simon\u2019s structural equations are able to represent only linear equations, our encoding scheme can represent non-linear equations and arbitrarily complex mathematical operators by means of its data representation of deterministic hypotheses.\nFor instance, take non-linear equation y = a x2 and suppose that, considering\nthe context of its complete system of equations, (Alg. 2) TCM maps it onto variable y. Then, by an abstraction of the equation semantics, we shall encode it into fd"}, {"heading": "3.4. THE ENCODING SCHEME 44", "text": "a x \u03c5 \u2192 y. That is, the hypothesis identifier \u03c5 captures the data-level semantics of the hypothesis equation.7\nWe encode complete structures into fd sets by means of (Alg. 3) h-encode.\nFig. 4.1 presents an fd set defined \u03a3 , h-encode(S), encoding the same structure S from Example 3.\nAlgorithm 3 Hypothesis encoding.\n1: procedure h-encode(S : structure over E and V , D : domain variables) Require: S given is a complete structure, i.e., |E| = |V| Ensure: Returns a non-redundant fd set \u03a3\n2: \u03a3\u2190 \u2205 3: \u03d5\u2190 TCM(S) 4: for all \u3008f, x\u3009 \u2208 \u03d5t do 5: Z \u2190 X \\ {x}, where \u3008f,X\u3009 \u2208 S 6: if Z = \u2205 or Z \u2286 D then . x is exogenous 7: if x /\u2208 D then . supress \u03c6-fd for dimensions like time t 8: \u03a3\u2190 \u03a3 \u222a \u3008Z \u222a {\u03c6}, x\u3009 9: else . x is endogenous\n10: \u03a3\u2190 \u03a3 \u222a \u3008Z \u222a {\u03c5}, x\u3009 11: return \u03a3\nNow we study the design-theoretic properties of the encoded fd sets. We\nshall make use of the concept of \u2018canonical\u2019 fd sets (also called \u2018minimal\u2019 [20, p. 390]), see Def. 9.\nDef. 9 Let \u03a3 be an fd set. We say that \u03a3 is canonical if:\n7 Note that, without the hypothesis id, infinitely many equations fit the pattern a x\u2192 y."}, {"heading": "3.5. EXPERIMENTS 45", "text": "(a) each fd in \u03a3 has the form X\u2192 A, where |A| = 1;\n(b) For no \u3008X,A\u3009 \u2208 \u03a3 we have (\u03a3\u2212 {\u3008X,A\u3009})+ = \u03a3+;\n(c) for each fd X\u2192 A in \u03a3, there is no Y \u2282 X such that (\u03a3\\{X\u2192 A}\u222a{Y \u2192\nA})+ = \u03a3+.\nFor an fd set satisfying such properties (Def. 9) individually, we say that it is (a) singleton-rhs, (b) non-redundant and (c) left-reduced. It is said to have an attribute A in X that is \u2018extraneous\u2019 w.r.t. \u03a3 if it is not left-reduced (Def. 9-c) [22, p. 74]. Finally, an fd X\u2192 Y in \u03a3 is said trivial if Y \u2286 X. Note that the presence of a trivial fd in a an fd set is sufficient to make it redundant.\nTheorem 2 Let \u03a3 be an fd set defined \u03a3 , h-encode(S) for some complete structure S. Then \u03a3 is non-redundant and singleton-rhs but may not be left-reduced (then may not be canonical).\nProof 6 We show that properties (a-b) of Def. 9 must hold for \u03a3 produced by (Alg. 3) h-encode, but property (c) may not hold (i.e., encoded fd set \u03a3 may not be left-reduced). See Appendix, \u00a7A.1.3. 2\nWe draw attention to the significance of Theorem 2, as it sheds light on a\nconnection between Simon\u2019s complete structures [24] and fd sets [20]. In fact, we continue to elaborate on that connection in next chapter to handle causal ordering processing symbolically by causal reasoning over fd\u2019s.\n3.5. Experiments\nFig. 3.8 shows the results of experiments we have carried out in order to study how effective the procedure of hypothesis encoding is in practice, in particular its behavior for hypotheses whose structure S has been randomly generated over orders of magnitude |S| \u2248 2k, to have length up to |S| \u2248 220 . 1M . The largest structure considered, with |S| \u2248 1M , has been generated to have exactly |E| = 2.5K."}, {"heading": "3.6. RELATED WORK 46", "text": "We have executed ten runs for each tested order of magnitude, and then\ntaken its mean running time in ms.8 The plot is shown in Fig. 3.8 in logscale base 2. In fact a a near-, sub-quadratic slope is expected for the curve structure length |S| \u00d7 time.\nThese scalability results are compatible with the computational complexity of h-encode, which is (cf. Corollary 1) bounded by O( \u221a |E| |S|).9\n3.6. Related Work\nModeling physical and socio-economical systems as a set of equations is a traditional modeling approach, and a very large bulk of models exist up to date. Simon\u2019s early work on structural equations and causal ordering comprises a specific notion of causality aimed at further contributing to the potential of such modeling approach (cf. [59]). It is meant for identifying influences among variables (or their values) that are implicit in the system model for enabling informed interventions. These may apply either to the system (phenomenon) under study, or to the model itself (say, when its predictions are not approximating observations very well).\nSignificant research effort has been devoted to causal modeling and reasoning\nin the past decades in both statistics and AI (cf. [25, 19]). The notion of causality used can be traced back to the early work of Simon\u2019s and others in Econometrics.\n8 The experiments were performed on a 2.3GHZ/4GB Intel Core i5 running Mac OS X 10.6.8. 9 Note that, for any arbitrary structure S(E ,V), we have |E| \u2264 |S| \u2264 |E|2. So, in worst case (the densest structure possible) we have |S| = |E|2 and then can establish a time bound function of |E| only, viz., O(|E|2 \u221a |E|)."}, {"heading": "3.6. RELATED WORK 47", "text": "Nonetheless, there are two important differences to be emphasized:\n\u2022 Such work is majorly devoted to deal with (statistical) qualitative hypothe-\nses, not (deterministic) quantitative hypotheses;\n\u2022 The causal model is assumed as given or is derived from data, instead of\nbeing converted or synthesized from a set of equations.\nThese are both core differences that also apply to our work in comparison\nto the bulk of existing work in probabilistic DB\u2019s. Our main point here, though, is to clarify the technical context and state of the art of the problem of causal ordering. A few works have been concerned with extracting a causal model out of some previous existing formal specification such as a set of equations. This is a reason why causal ordering has been an yet barely studied problem from the computational point of view.\nDash and Druzdzel revisit the problem and re-motivate it in light of modern\napplications [48]. First, they provide a formal description of how Simon\u2019s COA gives a summary of the causal dependencies implicit in a given SEM. That is, in clustering the strongly coupled variables into a causal graph, COA provides a condensed representation of the causal model implicit in the given SEM. They show then that any valid total causal mapping produced for a given SEM must be consistent with COA\u2019s partial causal mapping.\nYet, the serious problem is that the algorithm turns out to be intractable. In\nfact, no formal study of COA\u2019s computational properties can yet be found in the literature. In this thesis we have obtained the (negative) hardness result that it is intractable, which turns out to be compatible with Nayak\u2019s intuition (sic.) that it is a worst-case exponential time algorithm (cf. [60, p. 37]).\nInspired on Serrano and Gossard\u2019s work on constraint modeling and reason-\ning [61], Nayak reports an approach that is provably quite effective to process the causal ordering: extract a total causal mapping and then compute the transitive closure of the direct causal dependencies. In this thesis we build upon it to perform causal reasoning in terms of a form of transitive reasoning. Such approach fits very well to our use case, viz., the synthesis of p-DB\u2019s from fd\u2019s. As we show"}, {"heading": "3.7. SUMMARY OF RESULTS 48", "text": "in Chapter 4, we process the causal ordering of a hypothesis structure (abstracted as a SEM) in terms of acyclic causal reasoning over fd\u2019s and prove its correctness. This is enabled by the encoding scheme presented in this chapter.\n3.7. Summary of Results\nIn this chapter we have studied and developed an encoding scheme to process\nthe mathematical structure of a deterministic hypothesis into a set of fd\u2019s towards the encoding of hypotheses \u2018as data.\u2019 Then we have studied the design-theoretic properties held by such an encoded fd set. We list the results achieved as follows.\n\u2022 By Theorem 1, we know (an original hardness result) that Simon\u2019s ap-\nproach to process the causal ordering of a structure is intractable;\n\u2022 By building upon on the work of Simon [24] and Nayak [49] (cf. Proposi-\ntions 1 and 2), we have framed an approach to efficiently extract the basic information (a total causal mapping) for processing the causal ordering implicit in the mathematical structure of a deterministic hypothesis;\n\u2022 By Corollary 1, we know how to process the complete structure of a hypothesis into a total causal mapping in time that is bounded by O( \u221a |E| |S|).\nThat is, the machinery of hypothesis encoding is provably suitable for very large hypothesis structures.\n\u2022 By Theorem 2, which studies the design-theoretic properties of the encoded\nfd sets, we have unraveled the connection between Simon\u2019s complete structures and fd sets to further explore it in next chapter.\n\u2022 We have performed experiments (cf. Fig. 3.8) to study how effective the\napproach is in practice, or how it scales for hypotheses whose structure S is randomly generated to have length up to the order of |S| . 1M .10\n10 The tests were up to this order only because of the hardware limitations of our experimental settings. In theory (cf. complexity time bounds), larger structures can be handled very efficiently.\nChapter 4\nCausal Reasoning over FD\u2019s\nIn this chapter we present a technique to address the problem of causal ordering processing in order to enable the synthesis of U-relational DB\u2019s. In \u00a74.1 we introduce Armstrong\u2019s classical inference system to reason over fd\u2019s. In \u00a74.2 we develop the core concept and algorithm of the folding of an fd set, as a method for acyclic causal reasoning over fd\u2019s. In \u00a74.3 we show its connections (equivalence) with causal reasoning. In \u00a74.4 we present experiments on how the method behaves in practice. \u00a74.5 we discuss related work. In \u00a74.6 we conclude the chapter.\n4.1. Preliminaries: Armstrong\u2019s Inference Rules\nAs usual notational conventions from the DB literature [20, 21], we write X, Y, Z to denote sets of relational attributes and A,B,C to denote singleton attribute sets. Also, we write XY as shorthand for X \u222a Y .\nFunctional dependency theory relies on Armstrong\u2019s inference rules (or ax-\nioms) of (R0) reflexivity, (R1) augmentation and (R2) transitivity, which forms a sound and complete inference system for reasoning over fd\u2019s [20]. From R0-R2 one can derive additional rules, viz., (R3) decomposition, (R4) union and (R5) pseudo-transitivity.\nR0. If Y \u2286 X, then X\u2192 Y ; R1. If X\u2192 Y , then XZ\u2192 Y Z; R2. If X\u2192 Y and Y \u2192 W, then X\u2192 W ; R3. If X\u2192 Y Z, then X\u2192 Y and X\u2192 Z; R4. If X\u2192 Y and X\u2192 Z, then X\u2192 Y Z;"}, {"heading": "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 50", "text": "R5. If X\u2192 Y and Y Z\u2192 W, then XZ\u2192 W .\nGiven an fd set \u03a3, one can obtain \u03a3+, the closure of \u03a3, by a finite application of rules R0-R5. We are concerned with reasoning over an fd set in order to process its implicit causal ordering. The latter, as we shall see in \u00a74.3, can be performed in terms of (pseudo-)transitive reasoning. Note that R2 is a particular case of R5 when Z=\u2205, then we shall refer to R5 reasoning and understand R2 included. The next definition opens up a way to compute \u03a3+ very efficiently.\nLet \u03a3 be an fd set on attributes U , with X \u2286 U . Then X+, the attribute\nclosure of X w.r.t. \u03a3, is the set of attributes A such that \u3008X,A\u3009 \u2208 \u03a3+. Bernstein has long given algorithm (Alg. 4) XClosure to compute X+. It is polynomial time in |\u03a3| \u00b7 |U | (cf. [62]), where \u03a3 and U are (resp.) the given fd set and the attribute set over which it is defined. A tighter time bound (linear time in |\u03a3| \u00b7 |U |) is achievable as discussed further in Remark 4.\nAlgorithm 4 Attribute closure X+ (cf. [20, p. 388]).\n1: procedure XClosure(\u03a3: fd set, X : attribute set)\nRequire: \u03a3 is an fd set, X is a non-empty attribute set Ensure: X+ is the attribute closure of X w.r.t. \u03a3\n2: size \u2190 0 3: \u039b\u2190 \u2205 4: X+ \u2190 X 5: while size < |X+| do 6: size \u2190 |X+| 7: \u03a3\u2190 \u03a3 \\\u2206 8: for all \u3008Y, Z\u3009 \u2208 \u03a3 do 9: if Y \u2286 X+ then\n10: \u2206\u2190 \u2206 \u222a {\u3008Y, Z\u3009} . consumes fd 11: X+ \u2190 X+ \u222a Z 12: return X+\n4.2. Acyclic Pseudo-Transitive Reasoning\nAs discussed in the previous chapter, we shall process the causal ordering in terms of computing the transitive closure of each endogenous variable (predictive attribute). Before we proceed to that, we shall develop some machinery to reason over fd\u2019s in terms of Armstrong\u2019s rule R5 (pseudo-transitivity). We shall then"}, {"heading": "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 51", "text": "demonstrate the correspondence between this kind of reasoning with causal reasoning shortly in the sequel.\nDef. 10 Let \u03a3 be a set of fd\u2019s on attributes U. Then \u03a3., the pseudo-transitive closure of \u03a3, is the minimal set \u03a3. \u2287 \u03a3 such that X\u2192 Y is in \u03a3B, with XY \u2286 U , iff it can be derived from a finite (possibly empty) application of rule R5 over fd\u2019s in \u03a3. In that case, we may write X .\u2212\u2192 Y and omit \u2018w.r.t. \u03a3\u2019 if it can be understood from the context.\nWe are in fact interested in a very specific proper subset of \u03a3B, say, a kernel\nof fd\u2019s in \u03a3B that gives a \u201ccompact\u201d representation of the causal ordering implicit in \u03a3. Note that, to characterize such special subset we shall need to be careful w.r.t. the presence of cycles in the causal ordering.\nDef. 11 Let \u03a3 be a set of fd\u2019s on attributes U, and \u3008X,A\u3009 \u2208 \u03a3Bwith XA \u2286 U . We say that X\u2192A is folded (w.r.t. \u03a3), and write X #\u2212\u2192 A, if it is non-trivial and for no Y \u2282 U with Y + X, we have Y \u2192 X and X 6\u2192 Y in \u03a3+.\nThe intuition of Def. 11 is that an fd is folded when there is no sense in going\non with pseudo-transitive reasoning over it anymore (nothing new is discovered). Given an fd X\u2192 A in fd set \u03a3, we shall be able to find some folded fd Z\u2192 A by applying (R5) pseudo-transitivity as much as possible while ruling out cyclic or trivial fd\u2019s in some clever way.\nDef. 12 Let \u03a3 be an fd set on attributes U , and \u3008X,A\u3009 \u2208 \u03a3 be an fd with XA \u2286 U . Then,\n(a) A#, the (attribute) folding of A (w.r.t.\u03a3) is an attribute set Z\u2282 U such that Z #\u2212\u2192 A;\n(b) Accordingly, \u03a3#, the folding of \u03a3, is a proper subset \u03a3#\u2282 \u03a3B such that\nan fd \u3008Z,A\u3009 \u2208 \u03a3B is in \u03a3# iff X #\u2212\u2192 A for some Z \u2282 U .\nExample 4 (continued). Fig. 4.1 shows an fd set \u03a3 (left) and its folding \u03a3# (right). Note that the folding can be obtained by computing the attribute folding"}, {"heading": "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 52", "text": "for A in each fd X\u2192 A in \u03a3. We illustrate below some reasoning steps to partially compute an attribute folding considering the subset of fd\u2019s in \u03a3 with \u03c6 /\u2208 X.\n1. x1 x2 x3 x5 \u03c5 \u2192 x4 [given] 2. x1 x3 x4 \u03c5 \u2192 x5 [given] 3. x5 \u03c5 \u2192 x7 [given] 4. x1 x3 x4 \u03c5 \u2192 x7 [R5 over (2), (3)] 5. x1 x2 x3 x5 \u03c5 \u2192 x7 [R5 over (1), (4)]"}, {"heading": "6. \u2234 x1 x2 x3 x4 \u03c5 \u2192 x7 [R5 over (2), (5)] .", "text": "Note that (6) is still amenable to further application of R5, say over (1), to\nderive (7) x1 x2 x3 x5 \u03c5\u2192 x7. However, even though (1) and (6) have (resp.) the form (1) X \u2192 A and (6) Y \u2192 B with Y .\u2212\u2192 X, we have X .\u2212\u2192 Y as well which characterizes a cycle that fetches nothing into Y .1 In fact, if we consider only the fd\u2019s 1-3 given, then (6) itself satisfies Def. 11 and then is folded. The same holds, e.g., for (1) by an empty application of R5. 2\nLemma 2 Let S(E ,V) be a complete structure, \u03d5 a total causal mapping over S and \u03a3 an fd set encoded through \u03d5 given S. If \u3008X,A\u3009 \u2208 \u03a3, then A#, the attribute folding of A (w.r.t. \u03a3) exists and is unique.\nProof 7 See Appendix, \u00a7A.2.1. 2 1 Note, when at the step deriving (6) by R5 over (2), (5), that such cycle was not yet formed."}, {"heading": "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 53", "text": "We give an original algorithm (Alg. 5) to compute the folding of an fd set. At its core there lies (Alg. 6) AFolding, which can be understood as a non-obvious variant of XClosure (cf. Alg. 4) designed for acyclic pseudo-transitivity reasoning. In order to compute the folding of attribute A in fd \u3008X,A\u3009 \u2208 \u03a3, algorithm AFolding can be seen as backtracing the causal ordering implicit in \u03a3 towards A. Analogously, in terms of the directed graph G\u03d5 induced by the causal ordering (see Fig. 3.3), that would comprise graph traversal to identify the nodes x \u2208 V that have xa \u2208 V in their reachability, i.e., x xa. Rather, AFolding\u2019s processing of the causal ordering is fully symbolic based on Armstrong\u2019s rewrite rule R5.\nExample 5 Cyclicity in an fd set \u03a3 may have the effect of making its folding \u03a3# to degenerate to \u03a3 itself. For instance, consider \u03a3={A\u2192 B, B\u2192 A}. Note that \u03a3 is canonical, and AFolding (w.r.t. \u03a3) is B given A, and A given B. That is, \u03a3#= \u03a3. 2\nAlgorithm 5 Folding of an fd set.\n1: procedure folding(\u03a3: fd set)\nRequire: \u03a3 given encodes complete structure S Ensure: Returns fd set \u03a3#, the folding of \u03a3\n2: \u03a3# \u2190 \u2205 3: for all \u3008X, A\u3009 \u2208 \u03a3 do 4: Z \u2190 AFolding(\u03a3, A) 5: \u03a3# \u2190 \u03a3# \u222a \u3008Z, A\u3009 6: return \u03a3#"}, {"heading": "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 54", "text": "Algorithm 6 Folding of an attribute w.r.t. an fd set.\n1: procedure AFolding(\u03a3: fd set, A : attribute)\nRequire: \u03a3 is parsimonious Ensure: Returns A#, the attribute folding of A (w.r.t. \u03a3)\n2: \u2206\u2190 \u2205 . consumed fd\u2019s 3: \u039b\u2190 \u2205 . consumed attrs. 4: A? \u2190 A . stores attrs. A is found to be \u2018causally dependent\u2019 on (cf. \u00a74.3) 5: size \u2190 0 6: while size < |A?| do . halts when A(i+1) =A(i) 7: size \u2190 |A?| 8: \u03a3\u2190 \u03a3 \\\u2206 9: for all \u3008Y, B\u3009 \u2208 \u03a3 do 10: if B \u2208 A? then 11: \u2206\u2190 \u2206 \u222a {\u3008Y, B\u3009} . consumes fd 12: A? \u2190 A? \u222a Y 13: \u039b\u2190 \u039b \u222aB . consumes attr. 14: for all C \u2208 Y do 15: if C \u2208 \u039b and B \u2208 X for \u3008X,C\u3009 \u2208 \u2206 then . cyclic fd 16: \u039b\u2190 \u039b \\B . reingests it to simulate cyclic app. of R5 17: return A? \\ \u039b\nTheorem 3 Let S(E ,V) be a complete structure, and \u03a3 an fd set encoded given S. Now, let \u3008X,A\u3009 \u2208 \u03a3. Then AFolding(\u03a3, A) correctly computes A#, the attribute folding of A (w.r.t. \u03a3) in time O(|S|2).\nProof 8 For the proof roadmap, note that AFolding is monotone (size of A? can only increases) and terminates precisely when A(i+1) = A(i), where A(i) denotes the attributes in A? at step i of the outer loop. The folding A# of A at step i is A(i) \\ \u039b(i). We shall prove by induction, given attribute A from fd X\u2192 A in parsimonious \u03a3, that A? \\ \u039b returned by AFolding(\u03a3, A) is the unique attribute folding A# of A. See Appendix, \u00a7A.2.2. 2\nRemark 4 Let \u03a3 be an arbitrary fd set on attribute set U . Beeri and Bernstein gave a straightforward optimization to (Alg. 4) XClosure to make it linear in |\u03a3|\u00b7|U | (cf. [63, p. 43-5]), where |\u03a3| \u00b7 |U | is the maximum length for a string encoding all the fd\u2019s. Note that the actual length of such string in our case is exactly |S|. The"}, {"heading": "4.3. EQUIVALENCE WITH CAUSAL ORDERING 55", "text": "optimization mentioned applies likewise to (Alg. 6) AFolding.2 That is, AFolding can be implemented to run in linear time in |S|. 2\nCorollary 2 Let S(E ,V) be a complete structure, and \u03a3 an fd set encoded given S. Then algorithm folding(\u03a3) correctly computes \u03a3#, the folding of \u03a3 in time that is f(|S|) \u0398(|E|), where f(|S|) is the time complexity of (Alg. 6) AFolding.\nProof 9 See Appendix, \u00a7A.2.3. 2\nFinally, it shall be convenient to come with a notion of parsimonious fd sets\n(see Def. 13), which is suggestive of a distinguishing feature of such mathematical information systems in comparison with arbitrary information systems.\nDef. 13 Let \u03a3 be set of fd\u2019s on attributes U . Then, we say that \u03a3 is parsimonious if it is canonical and, for all fd\u2019s \u3008X,A\u3009 \u2208 \u03a3 with XA \u2286 U , there is no Y \u2282 U such that Y 6= X and \u3008Y,A\u3009 \u2208 \u03a3.\nProposition 3 then shall be useful further in connection with the concept of\nthe folding.\nProposition 3 Let S(E ,V) be a complete structure, \u03d5 a total causal mapping over S and \u03a3 an fd set encoded through \u03d5 given S. Let \u03a3# be the folding of \u03a3, then \u03a3# is parsimonious.\nProof 10 See Appendix, \u00a7A.2.4. 2\n4.3. Equivalence with Causal Ordering\nNow we show the equivalence of acyclic pseudo-transitive reasoning with causal ordering processing. We start with Theorem 4, which establishes the equivalence between the notion of causal dependency and the fd encoding scheme presented in Chapter 3.\n2 We omit its tedious exposure here. In short, it shall require one more auxiliary data structure to keep track, for each fd not yet consumed, of how many attributes not yet consumed appear in its rhs."}, {"heading": "4.3. EQUIVALENCE WITH CAUSAL ORDERING 56", "text": "Theorem 4 Let S(E ,V) be a complete structure, \u03d5 a total causal mapping over S and \u03a3 an fd set encoded through \u03d5 given S. Then xa, xb \u2208 V are such that xb is causally dependent on xa, i.e., (xa, xb) \u2208 C+\u03d5 iff there is some non-trivial fd \u3008X,B\u3009 \u2208 \u03a3B with A \u2208 X, where B 7\u2192 xb and A 7\u2192 xa.\nProof 11 We prove the statement by induction. We consider first the \u2018if\u2019 direction, and then its \u2018only if\u2019 converse. See Appendix \u00a7A.2.5. 2\nDef. 14 then gives useful terminology for a neat concept towards our goal in\nthis chapter.\nDef. 14 Let S(E ,V) be a structure with variables xa, xb \u2208 V , and \u03d5 a total causal mapping over S inducing set of direct causal dependencies C\u03d5 and its transitive closure C+\u03d5 . We say that xa is a first cause of xb in S if (xa, xb) \u2208 C+\u03d5 and, for no x \u2208 V , we have (x, xa) \u2208 C+\u03d5 .\nProposition 4 connects the notion of first cause with those of exogenous and\nendogenous variables introduced in Chapter 3.\nProposition 4 Let S(E ,V) be a structure with variable x \u2208 V . Then x can only be a first cause of some y \u2208 V if x is exogenous. Accordingly, any variable y \u2208 V can only have some first cause x \u2208 V if it is endogenous.\nProof 12 Straightforward from definitions, see Appendix \u00a7A.2.6. 2\nNote that exogenous variables are encoded into fd\u2019s X \u2192 A with \u03c6 \u2208 X.\nSince the values of such variables are assigned \u201coutside\u201d the system (cf. Remark 3), they are devoid of indirect causal dependencies and then have no uncertainty except for their own. Thus, we have not to be concerned at all with processing the causal (uncertainty) chaining towards them. Our goal is rather find the first causes of the endogenous variables (predictive attributes).\nWe shall need then the terminology of Def. 15, and then we introduce Lemma\n3 paving the way to our goal.\nDef. 15 Let S be a structure, and \u03a3 be a set of fd\u2019s encoded over it. Then \u03a5(\u03a3), the \u03c5-projection of \u03a3, is the subset of fd\u2019s X \u2192 A such that \u03c5 \u2208 X. Accordingly, \u03a6(\u03a3), the \u03c6-projection of \u03a3, is the subset of fd\u2019s X \u2192 A such that \u03c5 /\u2208 X."}, {"heading": "4.3. EQUIVALENCE WITH CAUSAL ORDERING 57", "text": "Fig. 4.2 illustrates the \u03c5-projection of an fd set and the folding applied over\nsuch fd subset in order to compute the first causes of endogenous variables.\nLemma 3 Let S(E ,V) be a complete structure, \u03d5 a total causal mapping over S and \u03a3 an fd set encoded through \u03d5 given S. Then a variable xa \u2208 V can only be a first cause of some variable xb \u2208 V , where \u3008X,B\u3009 \u2208 \u03a3, and B 7\u2192 xb, A 7\u2192 xa, if either (i) A \u2208 X or (ii) A /\u2208 X but there is \u3008Z,C\u3009 \u2208 \u03a3Bwith A \u2208 Z and C \u2208 X.\nProof 13 We prove the statement by construction out of Theorem 4, see Appendix \u00a7A.2.7. 2\nFinally, Theorem 5 further clarifies the purpose of the folding and its meaning\nin terms of causal ordering.\nTheorem 5 Let S(E ,V) be a complete structure, \u03d5 a total causal mapping over S and \u03a3 an fd set encoded through \u03d5 given S. Now, let B be an attribute that encodes some variable xb \u2208 V . If \u3008X,B\u3009 \u2208 \u03a5(\u03a3)#,3 then every first cause xa of xb (if any) is encoded by some attribute A \u2208 X.\nProof 14 We show that the existance of a missing first cause xc of xb for folded X #\u2212\u2192 B, where B 7\u2192 xb and C 7\u2192 xc but C /\u2208 X leads to a contradiction. See Appendix \u00a7A.2.8. 2 3 Note that the folding is taken w.r.t. the \u03c5-projection of \u03a3, then xb where B 7\u2192 xb is an endogenous variable."}, {"heading": "4.4. EXPERIMENTS 58", "text": "Remark 5 Observe that, on the one hand, the goal of computing the transitive closure C+\u03d5 of a set of induced causal dependencies C\u03d5 is to derive the entire causal ordering of a given structure. The goal of folding, on the other hand, is not to discover all variables (attributes) a given variable (attribute) is causally dependent on, but only all of its first causes (if any). 2\nIn particular, the results just shown comprise a method to compute, for each\nendogenous variable (predictive attribute), all of its first causes. This is a core goal of the reasoning device developed in this chapter in order to enable the automatic synthesis of hypotheses as uncertain and probabilistic (U-relational) data.\n4.4. Experiments\nFig. 4.3 shows the results of experiments we have carried out in order to study how effective the causal reasoning over fd\u2019s is in practice, in particular its behavior for hypotheses whose structure S has been randomly generated over orders of magnitude |S| \u2248 2k, to have length up to |S| \u2248 220 . 1M . The largest structure considered, with |S| \u2248 1M , has been generated to have exactly |E| = 2.5K. Like in the experiments of the previous chapter, we have executed ten runs for each tested order of magnitude, and then taken its mean running time in ms.4\nThe plot is shown in Fig. 3.8 is in logscale base 2. Notice the linear rate\nof growth across orders of magnitude (base 2) from 1K to 1M sized structures. For a growth factor of 2 in structure length (doubled), the time required by causal reasoning grows a factor of 2 (doubled as well). These scalability results are compatible with the computational complexity of folding, which is bounded by O(|S|2). Yet, that is a bit overestimated time bound as we see in the plot of Fig. 4.3.\n4.5. Related Work\nThe concept of fd set folding and the design of (Alg. 6) AFolding as a not quite\nobvious variant of XClosure, is an original approach to the problem of processing the causal ordering of a hypothesis via acyclic pseudo-transitive reasoning over fd\u2019s. To the best of our knowledge, such a specific form of fd reasoning was an\n4 The experiments were performed on a 2.3GHZ/4GB Intel Core i5 running Mac OS X 10.6.8."}, {"heading": "4.5. RELATED WORK 59", "text": "yet unexplored problem in the database research literature (reasoning over fd\u2019s is extensively covered in Maier [22]).\nRecent years have seen the emergence of some foundational work in causality\nin databases [64]. It is motivated for improving DB usability in terms of providing users with explanations to query answers (and non-answers). Essentially, the idea is borrowed from AI work on causality (cf. \u00a73.6) to identify causal ordering between tuples. Given a query and its result set, the system should be able to explain to the user what tuples \u2018caused\u2019 that answer, or why possibly expected tuples are missing. That requires causal chain of tuples for a given query, which can be computationally expensive as the database instance can be very large. For conjunctive queries, the causality is said to be computed very efficiently [65].\nA more specific problem addressed by Kanagal et. al is the so-called sensitiv-\nity analysis [66], which is aimed at establishing a more refined connection between the query answer (output) and elements of the DB instance (input) for supporting user interventions. Instead of providing the user with causes and non-causes, the goal is to enable the user to know how changes in the input affect the output. This line of work is strongly related to the vision of \u2018reverse data management\u2019 [67].\nCausal reasoning in the presence of constraints (viz., fd\u2019s) is an yet unex-\nplored topic, though called for as worth of future work by Meliou et al. [64, p. 3]. The fd\u2019s are rich information that can be exploited for the sake of explanation and sensitivity analysis. Once they are available, it is intuitive that the search space of such problems shall be significantly reduced."}, {"heading": "4.6. SUMMARY OF RESULTS 60", "text": "In fact, our encoding of equations into fd\u2019s captures the causal chain from\nexogenous (input) to endogenous (output) tuples in at schema level. Nonetheless our form of causal reasoning over fd\u2019s is geared for hypothesis management and analytics, from a uncertainty management point of view. A concrete connection to causality in DB\u2019s is not yet established.\n4.6. Summary of Results\nIn this chapter we have studied and developed a technique for acyclic causal\nreasoning over fd\u2019s. We list the results achieved as follows.\n\u2022 We have developed principled concepts and a core algorithm, viz., (Alg.\n5) the folding, in order to perform acyclic pseudo-transitive reasoning over fd\u2019s. This is towards an efficient method for causal reasoning, yet elegant as a database formalism for the systematic construction of hypothesis probabilistic DB\u2019s.\n\u2022 We have given a reasonably tight time bound for the behavior of such\nreasoning device in terms of the structure given as input. We have established (cf. Theorem 3, Corollary 2) the time bound of O(|S|2) for the folding algorithm.\n\u2022 We have shown the correctness of the folding algorithm in connection with\ncausal reasoning (cf. Theorem 4, Theorem 5).\n\u2022 We have defined the core notion of first causes (cf. Def. 14, Proposi-\ntion 4), which is meant to guide the procedure of U-intro (Chapter 5) by precisely capturing the uncertainty factors on endogenous variables (predictive attributes). This is similar, yet markedly different from computing the transitive closure of causal dependencies (cf. Remark 5).\n\u2022 We have performed experiments (cf. Fig. 4.3) to study how effective the\napproach of causal reasoning over fd\u2019s is in practice, or how it scales for hypotheses whose structure S is randomly generated to have length up to the order of |S| . 1M . The experiments show that the time bound, though already effective for very large structures, are a bit overestimated.\nChapter 5\nProbabilistic Database Synthesis\nIn this chapter we present a technique to synthesize hypothesis U-relations. At this stage of the pipeline, relational schema H is loaded with datasets computed from the hypotheses under alternative trials (input settings). The challenge is how to model or design its probabilistic version (i.e., render the U-relations Y ) so that it is suitable for data-driven hypothesis management and analytics.\nIn \u00a75.1 we introduce U-relational DB\u2019s. In \u00a75.2 we present a running example\nto illustrate the uncertainty introduction procedure (U-intro in the pipeline, cf. Fig. 1.4). In \u00a75.3 we present the technique to factorize the uncertainty present in the \u2018big\u2019 fact table in terms of the well-defined uncertainty factors. In \u00a75.4 then we show how to propagate such uncertainty into the predictive attributes properly, i.e., based on their first causes detected as shown in Chapter 4. In \u00a75.6, we discuss related work. Finally, in \u00a75.7 we conclude the chapter. 1\n5.1. Preliminaries: U-Relations and Probabilistic WSA\nThree remarkable features of U-relations are: expressiveness (being closed under positive relational algebra queries); succinctness (efficient storage of a very large number of possible worlds through vertical decompositions to support attributelevel uncertainty); and efficient query processing (including confidence computation) [18].\nA U-relational database or U-database is a finite set of structures,\nW = { \u3008R11, ..., R1m, p[1]\u3009, ..., \u3008Rn1 , ..., Rnm, p[n] \u3009 }, 1 We postpone the presentation of experiments on p-DB synthesis (U-intro as a whole) to \u00a76.4."}, {"heading": "5.1. PRELIMINARIES: U-RELATIONS AND PROBABILISTIC WSA 62", "text": "of relations Ri1, ..., R i m and numbers 0 < p\n[i] \u2264 1 such that \u2211\n1\u2264 i\u2264n p [i] = 1. An\nelement Ri1, ..., R i m, p [i] \u2208W is a possible world, with p[i] being its probability [18].\nProbabilistic world-set algebra (p-WSA) consists of the operations of rela-\ntional algebra, an operation for computing tuple confidence conf, and the repairkey operation for introducing uncertainty \u2014 by giving rise to alternative worlds as maximal-subset repairs of an argument key [18].\nLet R`[U ] be a relation, and XA \u2286 U . For each possible world \u3008R1, ..., Rm,\np\u3009 \u2208 W , let A \u2208 U contain only numerical values greater than zero and let R` satisfy the fd (U \\ A)\u2192 U . Then, repair-key is:\nJrepair-keyX@A(R`)K(W) := { \u3008R1, ..., R`, Rm, R\u0302` [U \\ A], p\u0302 \u3009 } ,\nwhere \u3008R1, ..., R`, Rm, p\u3009 \u2208 W , R\u0302` is a maximal repair of fd X \u2192 U in R`, and p\u0302 = p \u00b7 \u220f t\u2208R\u0302` t.B\u2211 s\u2208R` : s.X = t.X s.B . U-relations (cf. Fig. 5.1) have in their schema a set of pairs (Vi, Di) of condition columns (cf. [18]) to map each discrete random variable xi to one of its possible values (e.g., x1 7\u21921). The world table W stores their marginal probabilities (cf. the notion of pc-tables [17, Ch. 2]). For an illustration of the data transformation from certain to uncertain relations, consider query (5.1) in p-WSA\u2019s extension of relational algebra, whose result set is materialized into U-relation Y0 as shown in (Fig. 5.1).\nY0 := \u03c0\u03c6,\u03c5(repair-key\u03c6@Conf(H0) ). (5.1)\nAlso, let R[ViDi | sch(R) ], S[Vj Dj | sch(S) ] be two U-relations, where R. ViDi is the union of all pairs of condition columns ViDi in R, then operations of selection J\u03c3\u03c8(R) K, projection J \u03c0Z(R) K, and product JR \u00d7 SK issued in relational algebra are rewritten in positive relational algebra on U-relations:\nJ\u03c3\u03c8(R)K := \u03c3\u03c8(R[ViDi | sch(R)]); J\u03c0Z(R) K := \u03c0ViDi Z(R); JR\u00d7SK := \u03c0(R.ViDi \u222a S.ViDi)\u2192V D \u222a sch(R)\u222a sch(S)(R ./R.ViDi is consistent with S.Vj Dj S)."}, {"heading": "5.2. RUNNING EXAMPLE 63", "text": "If R and S have k and ` pairs of condition columns each, then JR \u00d7 SK returns a U-relation with k + ` such pairs. If k = 0 or ` = 0 (or both), then R or S (or both) are classical relations, but the rewrite rules above apply accordingly. All that rewriting is parsimonious translation (sic. [18]): the number of algebraic operations does not increase and each of the operations selection, projection and product/join remains of the same kind. Query plans are hardly more complicated than the input queries. In fact, it has been verified hat off-the-shelf relational database query optimizers do well in practice.\nFor a comprehensive overview of U-relations and p-WSA we refer the reader\nto [18]. In this thesis we look at U-relations from the point of view of p-DB design, for which no methodology has yet been proposed. We are concerned in particular with hypothesis management applications [10].\n5.2. Running Example\nBefore proceeding, we consider Example 6, which is fairly representative\nto illustrate how to deal with correlations in the predictive data of deterministic hypotheses for the sake of suitable data-driven analytics.\nExample 6 We explore three slightly different theoretical models in population dynamics with applications in Ecology, Epidemics, Economics, etc: (5.2) Malthus\u2019 model, (5.3) the logistic equation and (5.4) the Lotka-Volterra model. In practice, such equations are meant to be extracted from MathML-compliant XML files (cf. Chapter 6). For now, consider that the ordinary differential equation notation \u2018x\u0307\u2019"}, {"heading": "5.2. RUNNING EXAMPLE 64", "text": "is read \u2018variable x is a function of time t given initial condition x0.\u2019\nx\u0307 = bx (5.2)\nx\u0307 = b(1\u2212 x/K)x (5.3)  x\u0307 = x(b\u2212 py)y\u0307 = y(rx\u2212 d) (5.4) The models are completed (by the user) with additional equations to provide the values of exogenous variables (or \u2018input parameters\u2019),2 e.g., x0 = 200, b = 10, such that we have SEM\u2019s (resp.) Sk(Ek,Vk) for k = 1..3,\n\u2022 E1 ={ f1(t), f2(x0), f3(b), f4(x, t, x0, b) };\n\u2022 E2 ={f1(t), f2(x0), f3(K), f4(b), f5(x, t, x0, K, b)};\n\u2022 E3 ={ f1(t), f2(x0), f3(b), f4(p), f5(y0), f6(d), f7(r),\nf8(x, t, x0, b, p, y), f9(y, t, y0, d, r, x) }.\nFig. 5.2 shows the fd sets encoded from structures Sk given above.3 We also consider trial datasets for hypothesis \u03c5=3 (viz., the Lotka-Volterra model), which are loaded into the \u2018big\u2019 fact table relation H3 \u2208H as shown in Fig. 5.3. We admit a special attribute \u2018trial id\u2019 tid to keep hypothesis trials identified until uncertainty is introduced in a controlled way by p-DB synthesis (U-intro stage, cf. Fig. 1.4).\n2\n2 Given S(E ,V), it is actually a task of the encoding algorithm (viz., sub-procedure TCM) to infer, for each variable x \u2208 V, whether it is exogenous or endogenous by means of the total causal mapping.\n3 Recall that domain variables like time t are informed to the encoding algorithm to suppress an fd \u03c6\u2192 t."}, {"heading": "5.2. RUNNING EXAMPLE 65", "text": "Given the \u2018big\u2019 fact table H3, p-DB synthesis has two main parts: process\nthe \u2018empirical\u2019 uncertainty present in the \u2018big\u2019 fact table and synthesize it out (decompose it) into independent u-factors (u-factorization); and then propagate it precisely into the predictive data (u-propagation)."}, {"heading": "5.3. U-FACTORIZATION 66", "text": "5.3. U-Factorization\nAs we have seen in \u00a75.1, the repair-key operation allows one to create a discrete random variable in order to repair an argument key in a given relation. Our goal here is to devise a technique to perform such operation in a principled way for hypothesis management. It is a basic design principle to have exactly one random variable for each distinct uncertainty factor (\u2018u-factor\u2019 for short), which requires carefully identifying the actual sources of uncertainty present in relations H .\nThe multiplicity of (competing) hypotheses is itself a standard one, viz., the\ntheoretical u-factor. Consider an \u2018explanation\u2019 table like H0 in Fig. 5.1, which stores (as foreign keys) all hypotheses available and their target phenomena. We can take such H0 as explanation table for the three hypotheses of Example 6. Then a discrete random variable V0 is defined into Y0 [V0D0 |\u03c6 \u03c5 ] by query formula (5.1). U-relation Y0 is considered standard in p-DB synthesis, as the repair of \u03c6 as a key in (standard) H0.\nHypotheses, nonetheless, are (abstract) \u2018universal statements\u2019 [15]. In order\nto produce a (concrete) valuation over their endogenous attributes (predictions), one has to inquire into some particular \u2018situated\u2019 phenomenon \u03c6 and tentatively assign a valuation over the exogenous attributes, which can be eventually tuned for a target \u03c6. The multiplicity of such (competing) empirical estimations for a hypothesis k leads to Problem 1, viz., learning empirical u-factors for eachHk \u2208H .\nProblem 1 Let \u03a3k be an fd set encoded given hypothesis structure Sk, and Hk its \u2018big\u2019 fact table relation loaded with trial data. Now, let Z be the set of attributes encoding exogenous variables in Hk, then the problem of u-factor learning is:\n(1) to infer in Hk \u2018casual\u2019 fd\u2019s \u03c6Bi\u2192 Bj, \u03c6Bj\u2192 Bi not in \u03a3k (strong input\ncorrelations), where Bi, Bj \u2208 Z;\n(2) to form maximal groups G1, ..., Gn \u2286 Z of attributes such that for all\nBi, Bj \u2208 Ga, the casual fd\u2019s \u03c6Bi\u2192 Bj and \u03c6Bj\u2192 Bi hold in Hk;\n(3) to pick, for each group Ga, any A \u2208 Ga as a pivot representative and\ninsert \u03c6A\u2192 B into an fd set \u2126k for all B \u2208 (Ga \\ A)."}, {"heading": "5.3. U-FACTORIZATION 67", "text": "U-factor learning is meant to process only the attributes Z \u2282 U from Hk[U ] that are inferred exogenous in the given hypothesis, i.e., for all A \u2208 Z, there is an fd \u3008X,A\u3009 \u2208 \u03a6(\u03a3k), where the latter is the \u03c6-projection of \u03a3k. Such attributes are then \u2018officially\u2019 unrelated. In fact, by \u2018casual\u2019 fd\u2019s we mean correlations that, for a set of experimental trials, may occasionally show up in the trial input data; e.g., x0 \u2194 y0 hold in H3, but not because x0 and y0 are related in principle (theory).\nFig. 5.4 helps to illustrate Problem 1 through the \u2018big\u2019 fact table. We\nemphasize u-factors {b, d} and {p, r} in colors green and red. Observe that values of b are strongly correlated (one-to-one) with values of d for \u03c6 = 1, just like p and r. Note also that {x0, y0} can be seen as a certain factor. From the user point of view, this is a record that reflects a common practice in computational science known as (parameter) sensibility analysis.\nProblem 1 is dominated by the (problem of) discovery of fd\u2019s in a relation,\nwhich is not really a new problem (e.g., see [68]). We then keep focus on the synthesis method as a whole and omit our detailed u-factor-learning algorithm in"}, {"heading": "5.3. U-FACTORIZATION 68", "text": "particular.4 Its output, fd set \u2126k, is then filled in (completed) with the \u03c5projection \u03a5(\u03a3k). For illustration consider hypothesis \u03c5 = 3 and its trial input data recorded in H3 in Fig. 5.3. We show its resulting fd set \u21263 in Fig. 5.5 (left), together with its folding \u2126#k (right). The latter is then input to (Alg. 7) merge to get the final information necessary for the actual synthesis of U-relations, as captured in Def. 16. For an illustration of the merging of fd\u2019s with equivalent left-hand sides, note in Fig. 5.5 (right) that \u03c6x0 b p t \u03c5 y \u2194 \u03c6x0 b p t \u03c5 x holds in (\u2126#3 ) +.\nDef. 16 Let Sk and Hk be the complete structure and \u2018big\u2019 fact table of hypothesis k, and \u03a3k an fd set defined \u03a3k , h-encode(Sk). Now, let \u2126k , u-factor-learning(Hk, \u03a6(\u03a3k) ) \u222a \u03a5(\u03a3k), and \u2126#k be the folding of \u2126k. Finally, define \u0393k , merge( \u2126#k ). Then we say that \u0393k is the u-factorization of Sk over Hk.\nAlgorithm 7 Merge fd\u2019s with equivalent left-hand sides.\n1: procedure merge(\u03a3 : fd set) 2: \u2126\u2190 \u2205 3: for all \u3008X,C\u3009 \u2208 \u03a3 do 4: if there is \u3008Z,W \u3009 \u2208 \u2126 such that X \u2194 Z holds in \u03a3+ then 5: \u2126\u2190 \u2126 \\ \u3008Z,W \u3009 6: S \u2190 X \\ Z 7: \u2126\u2190 \u2126 \u222a \u3008Z,WSC\u3009 . merges equivalent keys 8: else\n9: \u2126\u2190 \u2126 \u222a \u3008X,C\u3009 10: return \u2126\n4 In short, we make use of relational algebra group-by operation and build a pruned lattice of attribute groups having the same number of rows under the grouping (similarly to [68])."}, {"heading": "5.3. U-FACTORIZATION 69", "text": "Remark 6 Let \u0393k be the u-factorization of structure Sk over \u2018big\u2019 fact table Hk. Then every fd in \u0393k encodes a clear-cut claim, either empirical, in \u03a6(\u0393k), or theoretical, in \u03a5(\u0393k). That is ensured by the merge algorithm, which groups fd\u2019s in \u2126#k with equivalent left-hand sides. 2\nWe are now able to employ a notion of u-factor decomposition formulated in\nDef. 17 into query formula (5.5) in p-WSA\u2019s extension of relational algebra.\nDef. 17 Let Sk be the complete structure of hypothesis k, and Hk[U ] its \u2018big\u2019 fact table such that \u0393k is the u-factorization of Sk over Hk. Now, let Ga \u2282 U be a set of attributes Ga = AG such that, for all B \u2208 G, an fd \u03c6A\u2192 B exists in \u03a6(\u0393k). Then we define U-relation Y ik [ViDi |\u03c6AG ] by query formula (5.5), and say that Y ik is a u-factor projection of Hk.\nY ik := \u03c0\u03c6AG (repair-key\u03c6@ count ( \u03b3\u03c6,A,G, count(\u2217)(Hk) ) ) (5.5)\nwhere \u03b3 is relational algebra\u2019s grouping operator.\nThe synthesis of u-factor projections, in particular the application of repair-key (cf. Eq. 5.5), has an important consequence for the u-factorization \u0393k of Hk, viz., the introduction of new fd\u2019s into \u0393\u2032k defined as follows (see Def. 18). We shall consider it (rather than \u0393k) to study design-theoretic properties of synthesized Y k in \u00a75.5.\nDef. 18 Let Sk and Hk[U ] be (resp.) the complete structure and \u2018big\u2019 fact table of hypothesis k, and \u0393k be the u-factorization of Sk over Hk. Now, let \u0393\u2032k ,\u22c3 i\u2208 I{\u03c6\u2192 AiGi} \u222a \u0393k, where I indexes all u-factor projections Y ik [ViDi |\u03c6AiGi ] of Hk. We say that \u0393 \u2032 k is the repaired factorization of Sk over Hk."}, {"heading": "5.4. U-PROPAGATION 70", "text": "5.4. U-Propagation\nU-propagation is a central part of U-intro and the pipeline itself. Recall that all the machinery developed so far, from hypothesis encoding to causal reasoning to u-factorization is for enabling predictive analytics. Let us briefly reconstruct it.\nFor hypothesis structure Sk(E ,V), take any endogenous variable xc \u2208 V en-\ncoded by attribute C 7\u2192 xc. There should be exactly one fd \u3008X,C\u3009 \u2208 \u03a5(\u03a3k)#. By Theorem 5, for every first cause xb of xc there is B \u2208 X where B 7\u2192 xb with xb \u2208 V . Now, observe that when \u2126k is rendered by u-factor learning, it is filled partly with fd\u2019s from \u03a5(\u03a3k), and partly with fd\u2019s processed from \u03a6(\u03a3k). This is to summarize exogenous variables into clear-cut independent u-factors. It means that, after u-factor learning, each first cause xb encoded by B shall be represented by some pivot attribute Ai which is, if not Ai = B itself, then occasionally strongly correlated to it (i.e., \u03c6Ai \u2192 B and \u03c6B\u2192 Ai hold in Hk).\nFurther then \u2126k is subject to folding such that, if B \u2208 X for \u3008X,C\u3009 \u2208 \u03a5(\u2126k),\nnow we have Ai \u2208 Z for \u3008Z,C\u3009 \u2208 \u03a5(\u2126#k ). This processing from fd X \u2192 C (where X contains the first causes) into Z\u2192 C (where Z contains only their pivot representatives) is meant for enabling an economical representation of uncertainty. Our running example is small, but such a principle is quite relevant for large-scale hypotheses (say, when |S| \u2248 1M). The correctness of such u-factor summarization shall be ensured by Proposition 3, which let us know that \u2126#k is parsimonious then (by Def. 13) canonical, therefore (by Def. 9) left-reduced.\nNow, all fd\u2019s in \u03a5(\u2126#k ) have form Z \u2192 C and we are almost ready for u-\npropagation. Note that, as a result of u-factorization, each pivot attribute Ai \u2208 Z is associated with random variable Vi from U-relation Y i k [ViDi |\u03c6AiGi ]. Then we shall use each Ai \u2208 Z (from Z\u2192 C) as a surrogate to Y ik in order to propagate factorized uncertainty into \u2018predictive\u2019 U-relations Y jk [Vj Dj |S T ] by a join formula. Attribute sets S and T are defined after merging fd\u2019s in \u2126#k with equivalent lhs to get \u0393k and pass it as argument for synthesis. We let S = Z \\W such that S contains the domain variables only (e.g., \u03c6, \u03c5, t). The pivot attributes in Z shall not be included in the data columns of Y jk , but leave their trace through the condition columns Vj Dj that annotate sch(Y j k ) as a repair of the key S\u2192 T ."}, {"heading": "5.4. U-PROPAGATION 71", "text": "All that (cf. Def. 19) is abstracted into general p-WSA query formula (5.6),\nand employed in (Alg. 8 ) synthesize to accomplish u-propagation (Part II).\nDef. 19 Let Sk be the complete structure of hypothesis k, and Hk[U ] its \u2018big\u2019 fact table such that \u0393k is the u-factorization of Sk over Hk. Now, let Z\u2192 T be an fd in \u03a5(\u0393k). Then we define U-relation Y j k [Vj Dj |S T ] by query formula (5.6), and say that Y jk is a predictive projection of Hk, where:\nY jk := \u03c0S, T (\u03c3\u03c5=k(Y0) ./ (./ i\u2208 I Y i k ) ./ Hk ) (5.6)\n(a) Y ik [ViDi |\u03c6AiGi ] is a u-factor projection of Hk; (a) we have i \u2208 I if Ai \u2208 Z; (c) we take S = Z \\W , where W is the set of all pivot attributes representing\nfirst causes in Z.\nAlgorithm 8 p-DB synthesis applied over folding fd set.\n1: procedure synthesize(Sk : structure, Hk : \u2018big\u2019 table, Y0 : explan. table) Require: Sk is complete Ensure: U-relations Y k returned are a BCNF, lossless decomposition of Hk\n2: \u03a3k \u2190 h-encode(Sk) 3: \u2126k \u2190 u-factor-learning( \u03a6(\u03a3k), Hk ) \u222a \u03a5(\u03a3k) 4: \u0393k \u2190 merge( folding(\u2126k) )\nPart I: U-factorization\n5: for all \u3008\u03c6A,G\u3009 \u2208 \u03a6(\u0393k) do . scans over the u-factors of hypothesis k 6: Y ik \u2190 \u03c0\u03c6,A,G (repair-key\u03c6@count( \u03b3\u03c6,A,G, count(\u2217)(Hk) ) ) 7: Y k \u2190 Y k \u222a Y ik\nPart II: U-propagation\n8: for all \u3008Z, T \u3009 \u2208 \u03a5(\u0393k) do . scans over the claims of hypothesis k 9: W \u2190 \u2205 . prepares to keep track of u-factor pivot attributes 10: for all Y ik [\u03c6AiGi ] \u2208 Y k do 11: if A \u2208 Z then . A is a first cause of all B \u2208 T 12: I = I \u222a {i} . indexes the u-factor projection 13: W \u2190 W \u222a A . keeps track of u-factor\u2019s pivot attribute 14: S \u2190 Z \\W . removes u-factor pivot attributes 15: Y jk \u2190 \u03c0S, T (\u03c3\u03c5=k(Y0) ./ (./ i\u2208 I Y ik ) ./ Hk ) 16: Y k \u2190 Y k \u222a Y jk 17: return Y k"}, {"heading": "5.4. U-PROPAGATION 72", "text": "Fig. 5.7 shows the rendered U-relations for hypothesis k = 3 whose \u2018big\u2019\nfact table is shown in Fig. 5.3. Note that tid = 6 in H3 corresponds now to \u03b8 = { x1 7\u2192 3, x2 7\u2192 1, x3 7\u2192 3, x4 7\u2192 2 }, where \u03b8 defines a particular world in W whose probability is Pr(\u03b8) \u2248 .055. This value is derived from the marginal probabilities stored in world table W (see Fig. 5.7) as a result of the application of formulas Eq. 5.1 and Eq. 5.5.\nRemark 7 Observe that, although (Alg. 8) synthesize operates locally for each hypothesis k, the effects of p-DB synthesis (U-intro) in the pipeline are global on account of the (global) \u2018explanation\u2019 relation H0 (then U-relation Y0), e.g., see Fig. 5.7. In fact, the probability of each tuple (row), say, in U-relation Y jk with \u03c6 = p for hypothesis \u03c5 = k, is distributed among all the hypotheses ` 6= k that are keyed in Y0 under \u03c6 = p, i.e., all hypotheses that compete at \u03c6 = p. 2"}, {"heading": "5.5. DESIGN-THEORETIC PROPERTIES 73", "text": "U-relations rendered by p-DB synthesis are ready for querying. Typical\nqueries comprise the conf() aggregate operation, inquiring the probability (or confidence) for each tuple to \u2018true\u2019 in the probability space captured by the hypothesis competition. We illustrate queries in Chapter 6.\n5.5. Design-Theoretic Properties\nFor the U-intro procedure to be meaningful, we have to study design-theoretic properties of u-factor projections and prediction projections synthesized out of \u2018big\u2019 fact table Hk for the sake of predictive analytics. In particular, for the projections to be claim-centered, we submit that they should satisfy Boyce-Codd normal form (BCNF, cf. Def. 21) w.r.t. the repaired factorization \u0393\u2032k of Hk; and for them to be a correct decomposition of the uncertainty present in Hk, their join should be lossless (preserve the data in Hk, cf. Def. 22) w.r.t. \u0393 \u2032 k.\nNote that in this study we consider repaired factorization \u0393\u2032k (not u-factoriza-\ntion \u0393k), since it is the one which actually holds in Y k after key repairing."}, {"heading": "5.5.1 Claim-Centered Decomposition", "text": "As emphasized through Remark 6, every fd in u-factorization \u0393k is a claim (cf. Remark 6), then the same holds for repaired factorization \u0393\u2032k. Thus, for a claimcentered decomposition of \u2018big\u2019 fact table Hk, it is desirable that U-relational schema Y k that it satisfies BCNF w.r.t. \u0393 \u2032 k. BCNF (\u2018do not represent the same fact twice\u2019 [21, p. 251]) is our notion of \u2018good design\u2019 for uncertainty decomposition in view of predictive analytics. This is to avoid the uncertainty of one claim to be undesirably mixed with the uncertainty of another claim.\nDef. 20 Let R[U ] be a relation scheme over set U of attributes, and \u03a3 a set of fd\u2019s. Then the projection of \u03a3 onto R[U ], written \u03c0U(\u03a3), is the subset \u03a3 \u2032 \u2286 \u03a3 of fd\u2019s X\u2192 Z such that XZ \u2286 U .\nDef. 21 Let R[U ] be a relation scheme over set U of attributes, and \u03a3 a set of fd\u2019s on U . We say that:\n(a) R is in BCNF if, for all \u3008X,A\u3009 \u2208 \u03a3+ with A 6\u2208 X and XA\u2286U , we have\nX\u2192 U (i.e.,X is a superkey for R);"}, {"heading": "5.5. DESIGN-THEORETIC PROPERTIES 74", "text": "(b) A schema R is in BCNF if all of its schemes R1, ..., Rn \u2208 R are in BCNF.\nExample 7 To illustrate the concept of BCNF, let us consider canonical fd set \u03a3 = {A\u2192 B, B \u2192 C } over attributes U = {A,B,C}, and a tentative schema containing a single relation R[ABC]. This relation is not in BCNF because, for one, B\u2192 C violates it (C * B but B is not a superkey for R). 2\nObserve also that an overdecomposed schema may (trivially) satisfy BCNF.\nFor example, let \u03a3 = {A \u2192 B, A \u2192 C} then by Def. 21 both schemas R = R1[ABC] and R \u2032 = {R1[AB], R2[AC]} are in BCNF w.r.t. \u03a3. The second, however, breaks data into two tables making their access more difficult than necessary since both B and C brings in information about A. That is, if the schema were to be synthesized over the fd\u2019s in \u03a3, then it would be desirable to apply (R4) union or merge them before. Our point is that, if we target at a BCNF-satisfying schema, then it is also desirable for it to be the minimal-cardinality schema in BCNF.\nTheorem 6 guarantees the BCNF property w.r.t. \u0393\u2032k by design for every\nschema Y k rendered by (Alg. 8) synthesize over \u0393k.\nTheorem 6 Let Sk and Hk be (resp.) the complete structure and \u2018big\u2019 fact table of hypothesis k, and let \u0393\u2032k be the repaired factorization of Sk over Hk, and Y0 the \u2018explanation\u2019 table where hypothesis k is recorded. Now, let Y k be a U-relational schema defined Y k , synthesize(Sk, Hk, Y0). Then Y k is in BCNF w.r.t. \u0393\u2032k and is minimal-cardinality.\nProof 15 We exploit the fact that the projection of (\u0393\u2032k) + onto u-factor projections and predictive projections define a disjoint partition of (\u0393\u2032k) + into its \u03c6-projection \u03a6(\u0393\u2032k) + and \u03c5-projection \u03a5(\u0393\u2032k) +. Since we know the form of fd\u2019s in each of them, the search space for BCNF violations is significantly reduced. The minimality of |Y k| in turn comes from (Alg. 7) merge. See Appendix, \u00a7A.3.1. 2"}, {"heading": "5.5.2 Correctness of Uncertainty Decomposition", "text": "Recall from the preliminaries (cf. \u00a75.1) that the U-relational equivalent of the relational product operation (main sub-operation of the join operation) has been introduced. Now, we provide the classical definition of a lossless join [20], i.e.,"}, {"heading": "5.5. DESIGN-THEORETIC PROPERTIES 75", "text": "when a decomposition of data from a relation into two or more relations is known to preserve the data in its original form by an application of the join.\nDef. 22 Let R[U ] be a (U-)relational schema synthesized into collection R =\u22c3n i=1Ri and let \u03a3 be an fd set on attributes U . We say that R has a lossless join w.r.t. \u03a3 if for every instance r of R[U ] satisfying \u03a3, we have r = ./ni=1 \u03c0Ri(r).\nThe lossless join property is of interest to ensure that our decomposition of\nthe data from the \u2018big\u2019 fact table into u-factor projections preserves the data so that their join to \u2018annotate\u2019 the predictive projections when propagated by means of the U-relational join operation is correct. Theorem 7 guarantees that is the case.\nTheorem 7 Let Sk be the complete structure of hypothesis k, and Hk[U ] its \u2018big\u2019 fact table such that \u0393\u2032k is the repaired factorization of Sk over Hk and Y0 is the \u2018explanation\u2019 table where hypothesis k is recorded. Now, let Y k be a U-relational schema defined Y k , synthesize(Sk, Hk, Y0). Then,\n(a) the join ./mi=1 Y i k [ViDi |\u03c6AiGi ] of any subset of the u-factor projections\nof Hk is lossless w.r.t. \u0393 \u2032 k.\n(b) any predictive projection Y jk [Vj Dj |S T ], result of a join of the theoretical\nu-factor Y0 [V0D0 |\u03c6 \u03c5 ] with the \u2018big\u2019 fact table Hk[U ] and in turn with u-factor projections Y ik [ViDi |\u03c6AiGi ], is lossless w.r.t. \u0393\u2032k.\nProof 16 We make use of a lemma from Ullman [20, p. 397], and then the proof comes straightforwardly. See Appendix, \u00a7A.3.2. 2\nRemark 8 The significance of Theorem 6 lies in that it guarantees the decomposition of uncertainty based on the causal ordering processing is in fact claim-centered as desirable for predictive analytics. Theorem 7 in turn is significant as it ensures that all the empirical uncertainty implicit in a hypothesis \u2018big\u2019 fact table Hk can be decomposed into u-factor projections that are (a) independent (not strongly correlated, cf. Problem 1), and (b) can be fully recovered by a join that is lossless w.r.t. repaired factorization \u0393\u2032k of structure Sk over Hk. This is essential to make sure that in u-propagation the composition of the required u-factors recovers the uncertainty associated with the predictive data. Since repaired factorization \u0393\u2032k is"}, {"heading": "5.6. RELATED WORK 76", "text": "known to be a correct processing of the causal ordering (cf. results of Chapter 4), altogether Theorem 7 guarantees that the first causes are joined together correctly towards the predictive variables influenced by them. 2\nAs we have seen, the p-DB synthesis technique presented here is essentially\ntargeted at design-theoretic properties. It is also motivated by computational performance, as uncertainty decomposition is desirable also to speed up probabilistic inference [17, p. 30-1]. In fact, the U-intro procedure is fully grounded in U-relations and p-WSA as implemented in the MayBMS system. Its computational performance is dominated by U-relational query processing. We present experimental studies on the U-intro procedure in \u00a76.4, as they are designed from a applicability point of view. The goal is to provide some reference computational measures for prospective users.\n5.6. Related Work\nInformed on research on Graphical Models (GM) [69], Suciu et al. provide a\nstriking motivation for work on probabilistic database design [17, p.30-1]. In GM design, probability distributions on large sets of random variables are decomposed into factors of simpler probability functions, over small sets of these variables. The factors can be identified, e.g., by using a set of axioms (the so-called \u2018graphoids\u2019) for reasoning about the probabilistic independence of variables [70]. The same design principle (sic.) applies to p-DB\u2019s [17]: the data should be decomposed into its simplest components so that only key constraints hold in a table (i.e., it is in BCNF). Attribute- and tuple-level correlations should guide the table decomposition into simpler tables. Ideally, the original table with its probability distribution can be recovered as a query (a view) from the decomposed tables. We have followed such principle in our claim-centered decomposition for predictive analytics.\nIn fact, a connection between database normalization theory and factor de-\ncomposition in Graphical Models (GM) has been discussed by Verma and Pearl [70], but has not been explored since then. To date, there is no formal design theory for p-DB\u2019s [17]. A step in that direction is taken by Sarma et al. [71]. Their initiative revisits dependency theory in view of reformulating fd\u2019s for uncertain schema"}, {"heading": "5.6. RELATED WORK 77", "text": "design [71]. Our work takes a different direction. We refer to classical dependency theory and U-relational operations (viz, its uncertainty-introduction operator) to construct p-DB\u2019s systematically from scratch. We have focused on the extraction and processing of fd\u2019s towards a factorized U-relational schema. The synthesized schema is ensured to be in BCNF and have a lossless join.\nDespite some major differences, our synthesis method builds upon the classi-\ncal theory of relational schema design by synthesis [62]. Classical design by synthesis [62] was once criticized due to its too strong \u2018uniqueness\u2019 of fd\u2019s assumption [72, p. 443], as it reduces the problem of design to symbolic reasoning on fd\u2019s, arguably neglecting semantic issues. Probabilistic design, however, has roots in statistical design so that the problem is less amenable to human factors. As we extract the dependencies from a formal specification, design by synthesis is doing nothing but translating seamlessly (into fd\u2019s) the reduction made by the user herself in her tentative model for the studied phenomenon.\nThe last decade has seen significant research effort to make DB systems really\nusable [73]. Our design-by-synthesis framework can also be understood as a technique for user-friendly p-DB design. For instance, in comparison, the CRIUS system supports another kind of user-friendly DB design approach that provides users with a spreadsheet-like direct manipulation interface to increasingly add structure to their data [74]. Our dependency extraction and processing, instead, completely alleviates the user from the burden of data organization.\nAlso related to probabilistic DB design is the topic of conditioning a p-DB.\nIt has been firstly addressed by Koch and Olteanu motivated by data cleaning applications [75]. They have introduced the assert operation to implement, as in AI, a kind of knowledge compilation, viz., world elimination in face of constraints (e.g., FDs). For hypothesis management, nonetheless, we need to apply Bayes\u2019 conditioning by asserting observed data, not constraints. In \u00a72.5 we have presented an example that settles the kind of conditioning problem that is relevant to the \u03a5-DB vision. In Chapter 6 we present realistic use cases. We have addressed the problem at application level only in order to complete the realization of the vision in a real prototype system. The formulation of Bayes\u2019 conditioning as an extension of, say, the U-relational data model is open to future work (cf. \u00a77.3)."}, {"heading": "5.7. SUMMARY OF RESULTS 78", "text": "5.7. Summary of Results\nIn this chapter we have studied and developed our end-purpose technique for\nthe synthesis of a probabilistic DB geared for predictive analytics. It completes the pipeline (Fig. 1.4) so that conditioning can then be performed iteratively.\n\u2022 Algorithm 8 synthesize gives a general formulation of how to perform un-\ncertainty introduction from causal dependencies given in the form of fd\u2019s.\n\u2022 By Problem 1, we have given a definition of uncertainty factor learning\nfrom data available in a given relation.\n\u2022 Remark 7 provides an example of the u-factor and predictive projections\nresulting from p-DB synthesis and their corresponding probability distributions stored in the world table;\n\u2022 By Remark 6 and Theorem 6, we have shown that U-relational schema Y k\nsynthesized over the fd\u2019s processed by causal reasoning is in BCNF. That is, it is in fact a claim-centered decomposition as desirable for predictive analytics.\n\u2022 Theorem 7 ensures that such (uncertainty) decomposition is correct, as the\noriginal (probability distribution) \u2018big\u2019 fact table is fully recoverable by a lossless join.\nChapter 6\nApplicability\nIn this chapter we show the applicability of \u03a5-DB in real-world scenarios.\nWe present use cases in Computational Physiology extracted from the Physiome project.1 In \u00a76.1 we introduce the Physiome project as providing a testbed for \u03a5-DB. Then in \u00a76.2 we go through some Physiome case studies to show the construction of \u03a5-DB and its application for data-driven hypothesis management and analytics. In \u00a76.3 we present a prototype of the \u03a5-DB system and demonstrate it through the running example introduced in \u00a75.2. In \u00a76.4 we present experiments on Physiome hypotheses. In \u00a76.5 we provide a general discussion on the applicability of \u03a5-DB, its assumptions and scope. In \u00a76.6 we conclude the chapter.\n6.1. The Physiome Project as a Testbed\nThe Physiome project is an initiative to seriously address the problems of reproducibility, model integration and sharing in Computational Physiology [34, 33]. It essentially comprises:\n\u2022 a curated repository of 380+ computational physiology models available\nonline for researchers;2\n\u2022 the Mathematical Modeling Language (MML) to allow models to be written\nin declarative form and then exported into a number of XML-compliant\n1 http://physiome.org. 2 The Physiome model repository is expanded to over 73K+ models by including models extracted from other sources (such as the EBML-EBI BioModels DB, the CellML Archive, and the Kegg Pathways DB) and converted to MML automatically.\n6.2. CASE STUDIES 80\ninteroperable formats;3\n\u2022 a problem-solving environment called JSim to allow researchers to code\ntheir MML models straightforwardly, run them under different parameter and solver settings and build customized data plots to see the results.\nFrom the point of view of \u03a5-DB, Physiome is an external data source that\nprovides a very interesting testbed with realistic scenarios. We extract Physiome models into \u03a5-DB by means of a wrapper we have implemented to read XMML files (JSim\u2019s XML encoding of MML models). Simulation trial datasets are rendered by a parametrized UNIX script we have developed to invoke JSim automatically (in batch mode). Currently, \u03a5-DB\u2019s Physiome wrapper is designed to read MAT files to load both the model input (parameter settings data) and its associated model output (computed predictive data) for each simulation trial.\nPhysiome does not keep records of phenomena in a repository, but it does\nhave observational data attached to some of the entries of the model repository. Such models appear in the filter \u2018models with data,\u2019 meaning that they have one or more observational datasets and plots showing how the model data fits to observations. We shall make use of model entries containing observational data in the realistic scenarios presented in this paper.\n6.2. Case Studies\nIn this section we present use cases extracted from the Physiome model repository.4"}, {"heading": "6.2.1 Case: Hemoglobin Oxygen Saturation", "text": "In this case we stress the potential of data-driven hypothesis analytics in comparison to handcrafted curve fitting (visual) analysis. We study three different hypotheses that perform \u201cclosely\u201d visually when compared to their target phenomenon dataset, see Fig. 6.1. All of them have been empirically set as fit as possible to the observations (\u2018R1s1\u2019 dataset) in their local view (in separate), and are now compared together in a global view.\n3 http://www.physiome.org/jsim/docs/MML_Intro.html. 4 http://www.physiome.org/Models/modelDB/."}, {"heading": "6.2. CASE STUDIES 81", "text": "Example 8 The resources of this example are shown in Fig. 6.2. We consider the Physiome model entries described in relation HYPOTHESIS, associated to the phenomenon described in relation PHENOMENON (cf. explanation relation H0). One single hypothesis trial (its best fit) is considered for each hypothesis. 2"}, {"heading": "6.2. CASE STUDIES 82", "text": "Encoding. The fd encoding of hypotheses \u03c5 \u2208 {28, 31, 32} is shown (resp.) in Fig. 6.3, Fig. 6.4 and Fig. 6.5.\nSymbol Mappings. As we have seen, the insertion of hypothesis trial datasets requires users to specify a target phenomenon and the corresponding mappings from the hypothesis symbols to the target phenomenon symbols. In this use case, we have:\n\u2022 M287\u21921 = { pO2 7\u2192 pO2, SHbO2 H 7\u2192 SHbO2 };\n\u2022 M317\u21921 = { pO2 7\u2192 pO2, SHbO2 Ad 7\u2192 SHbO2 };\n\u2022 M327\u21921 = { pO2 7\u2192 pO2, SHbO2 D 7\u2192 SHbO2 };"}, {"heading": "6.2. CASE STUDIES 83", "text": "Hypothesis Management. Query Q1 illustrates the feature of hypothesis management for this case. We consider the user is interested in all SHbO2 predictions over a subset of the pO2 domain. Its result set is shown in Fig. 6.6.\nQ1. (select phi, upsilon, tid, \u201cpO2\u201d,\u201cSHbO2 H\u201das SHbO2 from Y28 claim1\nwhere phi=1 and\u201cpO2\u201d>=20 and\u201cpO2\u201d<=40) union all (select phi, upsilon, tid, \u201cpO2\u201d,\u201cSHbO2 Ad\u201das SHbO2 from Y31 claim1 where phi=1 and\u201cpO2\u201d>=20 and\u201cpO2\u201d<=40) union all (select phi, upsilon, tid, \u201cpO2\u201d,\u201cSHbO2 D\u201das SHbO2 from Y32 claim1 where phi=1 and\u201cpO2\u201d>=20 and\u201cpO2\u201d<=40) order by\u201cpO2\u201d, upsilon, tid;\nHypothesis Analytics. Fig. 6.7 shows the results of analytics after conditoning the probability distribution in the presence of observations (\u2018R1s1\u2019 dataset). The fact that hypothesis \u03c5 = 31 provides the best explanation for the studied phenomenon is enabled by the application of Bayesian inference as implemented within the \u03a5-DB system. The contribution of the \u03a5-DB methodology is to equip users with a tool for large-scale, data-driven hypothesis management and analytics."}, {"heading": "6.2. CASE STUDIES 84", "text": ""}, {"heading": "6.2.2 Case: Baroreflex Dysfunction in Dahl SS Rat", "text": "This case is extracted from the Virtual Physiological Rat project,5 Here we show the potential of data-driven hypothesis management and analytics for model tuning. Fig. 6.8 shows the best fit of a baroreflex model for an observational dataset acquired by experiment on Dahl SS rat [76]. We in turn use \u03a5-DB to carry out such hypothesis management and analytics. We generate by a parameter sweep script 1K trials and insert them into the database. A best fit is then selected automatically by Bayesian inference.\nExample 9 The resources of this example are shown in Fig. 6.9. We consider the single hypothesis entry described in relation HYPOTHESIS, and the phenomenon described in relation PHENOMENON. By parameter sweep, 1K trials are inserted into \u03a5-DB for management and analytics. 2\nEncoding. The fd encoding of hypothesis \u03c5 \u2208 {1001} is shown in Fig. 6.12. 5 http://virtualrat.org/computational-models/."}, {"heading": "6.2. CASE STUDIES 85", "text": "Symbol Mappings. We consider that the user provides symbol mappings:\n\u2022 M10017\u21922 = { Time 7\u2192 Time, HR 7\u2192 HR };\nHypothesis Management. In query Q2 we consider that the user is interested in time instants where the heart rate is higher than a threshold, say, 300 beats/min. The result set is shown in Fig. 6.10.\nQ2. select phi, upsilon, tid, \u201cTime\u201d,\u201cHR\u201d from Y1001 claim1\nwhere phi=2 and\u201cHR\u201d>=300 order by\u201cTime\u201d, tid;\nHypothesis Analytics. Fig. 6.11 shows the results of analytics on phenomenon \u03c6=2 after conditioning the probability distribution in the presence of observations (\u2018SSBN9 HR\u2019 dataset). Since this case deals with model tuning, viz., 1K slightly different parameter settings, the trial ranking is decided by small differences in the posterior probability distribution (cf. Fig 6.11)."}, {"heading": "6.2.3 Case: Myogenic Behavior of a Blood Vessel", "text": "Computational models of physiology may account for diverse effects that take place at different levels of biological organization from the organ to the cellular"}, {"heading": "6.2. CASE STUDIES 86", "text": "and molecular levels [33]. Typically, a sophisticate model is developed incrementally by, say, adding detail into some previously existing model or extending its dimensionality (e.g., extending it from a stationary to a dynamic account of phenomena). In this case study (cf. Example 10) we consider alternative models of the myogenic behavior of a reference human blood vessel."}, {"heading": "6.2. CASE STUDIES 87", "text": "Example 10 (See Fig. 6.14). We consider the Physiome model entries displayed in relation HYPOTHESIS, and two phenomena (see relation PHENOMENON). One trial is considered for hypothesis \u03c5 = 60, and two for hypothesis \u03c5 = 89. 2\nEncoding. The fd encoding of hypotheses \u03c5 \u2208 {60, 89} is shown (resp.) in Fig. 6.15 and Fig. 6.16.\nSymbol Mappings. We consider that the user provides symbol mappings:\n\u2022 M607\u21921 = { t 7\u2192 Time, D 7\u2192 Diameter };\n\u2022 M897\u21921 = { t 7\u2192 Time, D 7\u2192 Diameter };"}, {"heading": "6.2. CASE STUDIES 88", "text": "Hypothesis Management. Query Q3 illustrates the feature of hypothesis management for this case. The user selects all diameter predictions within the time interval t \u2208 [100, 300] (cf. plot in Fig. 6.13). Its result set is shown in Fig. 6.17."}, {"heading": "6.3. SYSTEM PROTOTYPE 89", "text": "Q3. select phi, upsilon, tid, \u201ct\u201d,\u201cD\u201d from Y60 claim1\nwhere phi=3 and\u201ct\u201d>=100 and\u201ct\u201d<=300 union all select phi, upsilon, tid, \u201ct\u201d,\u201cD\u201d from Y89 claim1 where phi=3 and\u201ct\u201d>=100 and\u201ct\u201d<=300 order by\u201ct\u201d, upsilon, tid;\nHypothesis Analytics. Fig. 6.18 shows the results of analytics on phenomenon \u03c6 = 3 after conditoning the probability distribution in the presence of observations, viz., \u2018Davis Sikes Fig3 Myo DigData\u2019 dataset.\nIn this case study two tentative models have been considered under a uniform prior probability distribution which has been updated to a posterior distribution. Note that, even though hypothesis \u03c5 = 60 has its probability weight concentrated in a single trial, the Bayesian inference is able to indicate \u03c5 = 89 as the best explanation for \u03c6 = 3 and tid = 2, in particular, its best fit.\n6.3. System Prototype\nA first prototype of the \u03a5-DB system has been implemented as a Java web application, with the pipeline component in the server side on top of MayBMS (a backend"}, {"heading": "6.3. SYSTEM PROTOTYPE 90", "text": "extension of PostgreSQL). We have developed a demonstration of this prototype (cf. [28]), in which we go through the whole design-by-synthesis pipeline (Fig. 1.4) exploring use case scenarios. In this section we provide a brief demonstration of the system in the population dynamics scenario previously introduced in this thesis.\nThe demonstration unfolds in three phases. In the first phase, we show\nthe ETL process to give a sense of what the user has to do in terms of simple phenomena description, hypothesis naming and file upload to get her phenomena and hypotheses available in the system to be managed as data. In the second phase, we reproduce some typical queries of hypothesis management (like those shown in the previous section). In the third phase, we enter the hypothesis analytics module. The user chooses a phenomenon for a hypothesis evaluation study, and the system lists all the predictions with their probabilities under some selectivity criteria (e.g., population at year 1920). The predictions are ranked according to their probabilities, which are conditioned on the observational data available for the chosen phenomenon."}, {"heading": "6.3.1 Demo Screenshots", "text": "Fig. 6.21 shows screenshots of the system. Fig. 6.21(a) shows the research\nprojects currently available for a user. Figs. 6.21(b, c) show the ETL interfaces for phenomenon and hypothesis data definition (by synthesis), and then the insertion of hypothesis trial datasets, i.e., explanations of a hypothesis towards a target phenomenon. Fig. 6.21(d) shows the interface for basic hypothesis management by listing the predictions of a given simulation trial. Figs. 6.21(e, f) show two tabs of the hypothesis analytics module, viz., selection of observations and then viewing the corresponding alternative predictions ranked by their conditioned probabilities."}, {"heading": "6.3.2 Demo Case: Population Dynamics", "text": "In this case we refer to a well-known problem in Computational Science, viz., population dynamics scenarios, to demonstrate the \u03a5-DB system prototype. Fig. 6.19 shows census data collected from in the US from 1790 to 1990.6 Fig. 6.20 shows observational data collected from Hudson\u2019s Bay from 1900 to 1920 on the Lynx-Hare population [77].\n6 Cf. https://www.census.gov/population/censusdata/table-4.pdf."}, {"heading": "6.3. SYSTEM PROTOTYPE 91", "text": ""}, {"heading": "6.3. SYSTEM PROTOTYPE 92", "text": ""}, {"heading": "6.3. SYSTEM PROTOTYPE 93", "text": "Example 11 (See Fig. 6.22). We consider the model entries displayed in relation HYPOTHESIS, and two phenomena (see relation PHENOMENON). For \u03c6 = 1, three trials are considered for hypothesis \u03c5 = 1 and six for hypothesis \u03c5 = 2. For \u03c6 = 2, in turn, two trials are considered for hypothesis \u03c5 = 1 and \u03c5 = 2, and six trials for hypothesis \u03c5 = 3. Note the data definition interfaces in Figs. 6.21(b, c). 2\nEncoding. The fd encoding of hypotheses \u03c5 \u2208 {1, 2, 3} is shown (resp.) in Fig. 6.23, Fig. 6.24 and Fig. 6.25. See hypothesis structure processing in Fig. 6.21(c).\nSymbol Mappings. We consider that the user provides the following symbol mappings for (resp.) phenomena \u03c6 = 1 and \u03c6 = 2, see the interface for mapping symbols in Fig. 6.21(c)."}, {"heading": "6.3. SYSTEM PROTOTYPE 94", "text": "\u2022 M17\u21921 = { t 7\u2192 Year, x 7\u2192 Population };\n\u2022 M27\u21921 = { t 7\u2192 Year, x 7\u2192 Population };\n\u2022 M17\u21922 = { t 7\u2192 Year, x 7\u2192 Lynx };\n\u2022 M27\u21922 = { t 7\u2192 Year, x 7\u2192 Lynx };\n\u2022 M3 7\u21922 = { t 7\u2192 Year, x 7\u2192 Lynx };\nHypothesis Management. Query Q4 illustrates the feature of hypothesis management for this case. The user selects hypothesis \u03c5=3 (the Lotka-Volterra model), and filters its available data for trial tid=6 on phenomenon \u03c6 = 2. Both the formbased query set-up and its result set are shown in Fig. 6.21(d).\nQ4. select\u201ct\u201d,\u201cy\u201d,\u201cx\u201d from Y3 claim1\nwhere upsilon=3 and phi=2 and tid=6 order by\u201ct\u201d;\nHypothesis Analytics. Fig. 6.26 and Fig. 6.27 show the results of analytics on (resp.) phenomena \u03c6 = 1 and \u03c6 = 2 after conditoning the probability distribution in the presence of (resp.) observational datasets \u2018US-census\u2019 and \u2018Lynx-Hare.\u2019 In the first one, the user verifies that hypothesis \u03c5 = 1 (the Malthusian model) is unlikely to be competitive with hypothesis \u03c5 = 2 (the Logistic equation) as an approximation of the US population dynamics from 1790 to 1990. That is, if the user knows her current trials are reasonable, then more trials on the Malthusian model hardly could outperform trials on the Logistic equation for the studied phenomenon."}, {"heading": "6.4. EXPERIMENTS 95", "text": "6.4. Experiments\nThe efficiency and scalability of the U-relational representation system and\nits p-WSA query algebra have been extensively demonstrated [27]. \u03a5-DB\u2019s, as U-relational hypothesis DB\u2019s, must therefore be as efficient and scalable as any arbitrary U-relational DB.\nIn these experiments (see Fig. 6.28) we provide some measures of perfor-\nmance of the method of \u03a5-DB in the particular context of our real-world Physiome testbed. Our purpose here is to provide a concrete feel on how efficient the \u03a5-DB methodology can be. However, most of these tests (the four graphs on the bottom in Fig. 6.28) involve the data level and then require more of the hardware. Our current experimental setup (personal computer)7 allows us to reach a scale\n7 These experiments were performed on a 2.3GHZ/4GB Intel Core i5 running Mac OS X 10.6.8 and MayBMS (a PostgreSQL 8.3.3 extension)."}, {"heading": "6.4. EXPERIMENTS 96", "text": "in which the uncertain data being processed in synthesis \u20184U\u2019 is sized up to 1GB. For the two first graphs (XML extraction and encoding), we have collected the response time on the measure of interest over different structure lengths. Each one corresponds to a real Physiome hypothesis from the table of Fig. 6.29. The last hypothesis in that table, \u03c5 = 379, is used for the tests of the four last graphs in Fig. 6.28, viz., u-learning, u-factorization, u-propagation and conditioning. We have set different number of trials (ntrials) over it, each one having 1MB. The last test in each of such four graphs, with 1K trials, is processing 1GB of uncertain data at once and then fits the machine\u2019s main memory. We interpret the performance results shown in these graphs as follows for each measure of performance.\n\u2022 Extraction. Some fluctuation may be due to practicalities of XML DOM\naccess methods. The point of this performance study is to have practical"}, {"heading": "6.4. EXPERIMENTS 97", "text": "HYPOTHESIS \u03c5 name |S| |E| 186 Regulatory Vessel 40 20 89 Myo Dyn Resp wFit 73 28 60 Myogenic Compliant Vessel 100 38 75 Baroreceptor Lu et al 2001 153 74 70 4-State Sarcomere Energetics 298 91 120 Comp four gen weibel lung 440 186 91 CardiopulmonaryMechanics 1132 412 93 CardiopulmonMechGasBloodExch 1593 525 153 HighlyIntegHuman 1624 538 154 HighlyIntHuman wIntervention 1919 634 379 Baroreflex SB CT 171 74"}, {"heading": "6.5. DISCUSSION 98", "text": "has negligible processing time w.r.t. u-propagation. The latter is the most expensive sub-procedure of the synthesis method.\n\u2022 Conditioning. The conditioning procedure is run for a selected phenomenon.\nIt is composed of four main parts. First, by operation conf() it performs a probabilistic inference sub-query on the proper predictive projection of the \u2018big\u2019 fact table of each hypothesis associated with the phenomenon. Second, it combines the results of each such sub-query through a union all query whose result set is a multi-hypothesis predictive table. Third, it loads the phenomenon observation sample data and the predictive data from the multi-hypothesis table into memory to apply Bayesian inference. Finally, the prior probability distribution of the predictive table is updated with the posterior and all the corresponding marginal probabilities are updated in their original U-relational tables. In our tests, this procedure is carried out over varying number of trials (ntrials). The total response times are shown in the last plot of Fig. 6.28.\nThis performance behavior is to be interpreted in the context of ETL in\nDW\u2019s. Loading and setting up an \u03a5-DB has an overhead that shall be, though, much lower in high-performance machines. Such overhead is nonetheless justified for the use case of hypothesis management and analytics as opposed to simulation data management and exploratory analytics (cf. \u00a72.6.2).\n6.5. Discussion\nWe have verified that the hypothesis ratings/rankings shown in \u00a76.2 coincide\nwith the results (e.g., of model tuning) described in the Physiome model entries and their related publications. That validates the applicability of the \u03a5-DB methodology as a tool for data-driven analysis in such realistic scenarios.\nThe current practice in Computational Science for model evaluation and\ncomparison in the presence of observational data is somewhat handcrafted: model agreement is assessed either qualitatively by referring to curve shapes in data plots or quantitatively by means of ad-hoc scripts. The \u03a5-DB methodology offers a tool to perform data-driven hypothesis analytics semi-automatically directly in the"}, {"heading": "6.5. DISCUSSION 99", "text": "database under the support of its querying capabilities. It has, therefore, potential to be a step towards higher standards of reproducibility and scalability.\nRealistic assumptions. The core assumption of our framework is that the\nhypotheses are given in a formal specification which is encodable into a SEM that is complete (satisfies Defs. 1, 2). Also, as a semantic assumption which is standard in scientific modeling, we consider a one-to-one correspondence between real-world entities and variable/attribute symbols within a structure, and that all of them must appear in some of its equations/fd\u2019s. For most science use cases involving deterministic models (if not all), such assumptions are quite reasonable. It can be a topic of future work (cf. \u00a77.3) to explore business use cases as well.\nHypothesis learning. The (user) method for hypothesis formation is ir-\nrelevant to our framework, as long as the resulting hypothesis is encodable into a SEM. So, a promising use case is to incorporate machine learning methods into our framework to scale up the formation/extraction of hypotheses and evaluate them under the querying capabilities of a p-DB. Consider, e.g., learning the equations, say, from Eureqa [2].8\nQualitative hypotheses. The \u03a5-DB methodology is primarely motivated\nby computational science (usually involving differential equations). It is, however, applicable to qualitative deterministic models as well. Boolean Networks, e.g., consist in sets of functions f(x1, x2,.., xn), where f is a Boolean expression. For instance, Fig. 6.30 presents the system of Boolean equations of a tentative Boolean Network model for a plant hormone (Fig. 6.31) published in [78].9 The notation, e.g., SphK*, is read (just like an ordinary differential equation), \u2018the next state value of variable SphK is given by the state value of variable ABA. The parameters in this kind of model are the variable initial conditions.\n8 http://creativemachines.cornell.edu/Eureqa. 9 Cf. http://atlas.bx.psu.edu/booleannet/booleannet.html."}, {"heading": "6.5. DISCUSSION 100", "text": ""}, {"heading": "6.6. CONCLUSIONS 101", "text": "Several kinds of dynamical system can be modeled in this formalism. Ap-\nplications have grown out of gene regulatory network to social network and stock market predictive analytics. Even if richer semantics is considered (e.g., fuzzy logics), our encoding method is applicable likewise, as long as the equations are still deterministic.\n6.6. Conclusions\nIn this chapter we have demonstrated and discussed the applicability of the \u03a5-DB methodology. We have referred to real-world use case scenarios derived from the Physiome research project. We have shown in some detail the process of building an \u03a5-DB with representative models from Physiome\u2019s model repository. That qualitative assessment is followed by experiments that provide some concrete feel on the performance behavior of \u03a5-DB for models with up to 600+ mathematical variables.\nChapter 7\nConclusions\nIn this chapter we (\u00a77.1) revisit the research questions addressed by this\nthesis, (\u00a77.2) point out its significance and limitations, (\u00a77.3) list open problems and topics for future work, and (\u00a77.4) conclude with final considerations.\n7.1. Revisiting the Research Questions\nLet us now revisit the conceptual (RQ1-4) and technical (RQ5-9) research\nquestions.\nRQ1. How to define and encode hypotheses \u2018as data\u2019? What are the sources of\nuncertainty that may be present and should be considered?\nIn Chapter 2 we have provided core abstractions that compose the vision of hypotheses \u2018as data,\u2019 or the \u03a5-DB vision. The problem of hypothesis encoding has been defined and addressed further in Chapter 3. We have distinguished two main sources of uncertainty in our model of uncertainty for hypothesis management, viz., (i) theoretical uncertainty, as arising from competing hypotheses; and (ii) empirical uncertainty, as arising from the alternative trial datasets available for each hypothesis.\nRQ2. How does hypotheses \u2018as data\u2019 relate with observational data or, likewise,\nphenomena \u2018as data\u2019 from a database perspective?\nAlso in Chapter 2, we have presented a conceptual framework in which we have defined hypotheses \u2018as data\u2019 and shown how it can be compared against phenomena \u2018as data.\u2019 In fact, hypothesis management is really"}, {"heading": "7.1. REVISITING THE RESEARCH QUESTIONS 103", "text": "significant when it is possible to rate/rank hypotheses in the presence of (some partial piece of) evidence.\nRQ3. Does every piece of simulated data qualify as a scientific hypothesis? What\nis the difference between managing \u2018simulation\u2019 data from managing \u2018hypotheses\u2019 as data?\nEarly in Table 1.1, we provided a comparison between simulation data management and hypothesis data management. Furthermore, the scientific research process is abstracted in Chapter 2 as a well-defined problem of data cleaning. Hypotheses are seen from an applied science point of view, and then are reduced into data such that a piece of simulation data is considered a hypothesis whenever it is assigned to explain some specific phenomenon.\nRQ4. Is there available a proper (machine-readable) data format we can use to\nautomatically extract mathematically-expressed hypotheses from?\nWe anticipated in Chapter 2 the adoption of the XML data model as the general data format for extracting hypothesis specifications from. In particular, since we deal here with mathematical hypotheses, we refer to MathML as a standard for hypothesis specification. Concretely, in Chapter 6 we present use case demonstration scenarios for which we have developed a specific wrapper, viz., for the extraction of hypotheses specified in MML (Mathematical Modeling Language).\nRQ5. Is there an algorithm to, given a SEM, efficiently extract its causal order-\ning? What are the computational properties of this problem?\nAs shown in Chapter 3, Simon\u2019s treatment of the problem of causal ordering given a SEM S(E ,V) is NP-Hard. In the same chapter, we have discussed this problem in detail and presented an effective, efficient algorithmic approach to the problem. The computational cost for the whole\nprocess of hypothesis encoding is bounded by O( \u221a |S||E|). Experiments show that the approach performs well in practice for large hypotheses."}, {"heading": "7.1. REVISITING THE RESEARCH QUESTIONS 104", "text": "RQ6. What is the connection between SEM\u2019s and fd\u2019s? Can we devise an en-\ncoding scheme to \u2018orient equations\u2019 and then effectively transform one into the other with guarantees? Once we do it, what design-theoretic properties have such a set of fd\u2019s?\nAlso in Chapter 3, we have presented an algorithmic encoding scheme to transform a SEM into a set of fd\u2019s with guarantees in terms of preserving the hypothesis causal structure. Our study of this problem has revealed some interesting properties of the resulting fd sets, in particular, that they are always \u2018non-redundant\u2019 and, in comparison with arbitrary information systems, more precise and economical in the sense that, for any given attribute, there is exacly one fd with it in its rhs.\nRQ7. Is such fd set ready to be used for p-DB schema synthesis as an encoding\nof the hypothesis causal structure? If not, what kind of further processing we have to do? Can we perform it efficiently by reasoning directly on the fd\u2019s? How does it relate to the SEM\u2019s causal ordering?\nAs we discuss in Chapter 4, the encoded fd set must be further processed to find the \u2018first causes\u2019 for each of its predictive variable. For addressing that, in Chapter 4 we have presented the concept of the folding of an fd set and an efficient algorithm to compute it. Also, we have shown the equivalence of such fd reasoning with causal ordering processing.\nRQ8. Is the uncertainty decomposition required for predictive analytics reducible\nto the structure level (fd processing), or do we need to process the simulated data to identify additional uncertainty factors? Finally, what properties are desirable for a p-DB schema targeted at hypothesis management? Are they ensured by this synthesis method?\nIn Chapter 5 we have presented a conceptual framework to address synthesis for uncertainty \u20184U.\u2019 In particular, we have introduced the need to process, for each hypothesis, its trial datasets available, and presented an efficient algorithm to factorize and propagate the overall uncertainty present in a given hypothesis (as a competing explanation for a target"}, {"heading": "7.2. SIGNIFICANCE AND LIMITATIONS 105", "text": "phenomenon). Then we have motivated BCNF as a notion of \u201cgood\u201d design w.r.t. the factorized fd set based on the folding concept, and the lossless join property as required for the correctness of uncertainty decomposition. We have shown that the synthesized p-DB schema bears both properties.\nRQ9. Given all such a design-theoretic machinery to process hypotheses into\n(U-)relational DB\u2019s, what properties can we detect on the hypotheses back at the conceptual level? Do we have now technical means to speak of hypotheses that are \u201cgood\u201d in terms of principles of the philosophy of science?\nEquipped with the design-theoretic machinery proposed in this thesis, we are able to, given a SEM, to automatically (1) extract its causal ordering, (2) detect its strongly coupled components and decide, for a given predictive projection, what are its associated u-factor projections (if any), and shall be able as well to (3) query the hypothesis ranking for a phenomenon of interest. All these are technical means to [15]: (1\u2032) extract the hypothesis \u2018empirical content\u2019 and \u2018predictive power;\u2019 (2\u2032) unravel its cohesiveness and how parsimonious it is in terms of the number of different claims or epistemological units carried within it, as well as its empirical grounding (\u2018first causes\u2019); and finally, we shall be able to (3\u2032) appraise it in face of competing or alternative explanations.\n7.2. Significance and Limitations\nThis thesis addresses the pressing call for large-scale, data-driven hypoth-\nesis management and analytics [35, 3, 10]. Some reasons that contribute for its significance are listed (cf. [10, 29, 28]).\n\u2022 Structured deterministic hypotheses are now shown to be encodable as\nuncertain and probabilistic (U-relational) data based on p-DB principles;\n\u2217 Study of the connection between SEM\u2019s and fd\u2019s, with contribution\nboth to computational properties of the causal ordering problem, and to causal reasoning over fd\u2019s;"}, {"heading": "7.3. OPEN PROBLEMS AND FUTURE WORK 106", "text": "\u2217 First synthesis method for the construction of p-DB\u2019s from some pre-\nvious existing formal specification.\n\u2022 Definition of a concrete use case of data-driven hypothesis management\nand analytics;\n\u2217 New class of applications introduced for p-DB\u2019s;\n\u2217 Settled the problem of Bayes\u2019 conditioning in p-DB\u2019s.\nNow some limitations of the thesis are listed.\n\u2022 The Bayesian inference is implemented at application level, yet not formu-\nlated as a principled technical solution within research in p-DB\u2019s.\n\u2022 The encoding scheme to transform the mathematical system of a hypoth-\nesis into a set of fd\u2019s enabling the synthesis of the p-DB is applicable to structured deterministic models only, not stochastic ones.\n7.3. Open Problems and Future Work\nOpen problems and topics of future work are listed (no particular order).\n(1) The design of a dedicated algebraic operation for Bayes\u2019 conditioning in\np-WSA.\n(2) Investigation of other data dependency formalisms (e.g., multi-valued de-\npendencies [20]), approximate fd\u2019s [68], conditional fd\u2019s [79]) to extend the scope of \u03a5-DB towards structured stochastic models.\n(3) Development of techniques for systematic hypothesis extraction as a well-\ndefined problem of (web) information extraction;\n(4) Investigation of business use case scenarios for data-driven decision making\non top of \u03a5-DB;\n(5) Definition of a machine learning use case scenario to industrialize hypothe-\nsis formation and assess \u03a5-DB\u2019s performance feasibility in such a scenario;"}, {"heading": "7.4. FINAL CONSIDERATIONS 107", "text": "(6) Development of automatic data sampling techniques to leverage the data\ndefinition of both hypotheses and phenomena in \u03a5-DB from a statistical point of view.\n7.4. Final Considerations\nIn this thesis we have developed the vision of \u03a5-DB, which is essentially\nan abstraction of hypotheses as uncertain and probabilistic data. It comprises a design-theoretic methodology for the systematic construction and management of U-relational hypothesis DB\u2019s. It is meant to provide a principled approach to enable scientists and engineers to manage and evaluate (rate/rank) large-scale scientific hypotheses as theoretical data. We have addressed some core technical challenges over the \u03a5-DB vision in order to properly encode deterministic hypotheses as uncertain and probabilistic data.\nAs envisioned by Jim Gray [1], the scientific method has been shifting towards\nbeing operated as a data-driven discipline which is rapidly gaining ground [3]. In this thesis we have strived for proposing some core principles and techniques for enabling data-driven hypothesis management and analytics, opening a promising line of research in both probabilistic databases and simulation data management."}, {"heading": "Appendix A", "text": "Detailed Proofs\nA.1. Proofs of Hypothesis Encoding\nA.1.1 Proof of Theorem 1 \u201cLet S(E ,V) be a complete structure. Then the extraction of its causal ordering by Simon\u2019s COA(S) is intractable (NP-Hard).\u201d\nProof 17 We show that, at each recursive step of COA, to find all non-trivial minimal subsets (i.e., |E \u2032| \u2265 2) translates into an optimization problem associated with the decision problem BPBP, which we know by Lemma 1 to be NP-Complete.\nFirst, recall (Def. 2) that a structure S(E , V) is complete if |E| = |V|; e.g., for\nthe structure given in Fig. 3.5 (left), note (Def. 9) the minimal structure S \u2032(E \u2032, V \u2032), where E \u2032 = { f1, f2, f3 }. For non-trivial minimal structures, i.e., when |E \u2032| = K \u2265 2, it is easy to see that its corresponding bipartite graph G = (V \u20321 \u222a V \u20322 , E \u2032), where V1 7\u2192 E , V2 7\u2192 V and E 7\u2192 S must have number of edges |E \u2032| \u2265 2K and, for all its vertices u \u2208 V \u20321 \u222a V \u20322 , u must have deg(u) \u2265 2, i.e., G is a pseudo-biclique in accordance with Def. 6. That intuition is elaborated as follows.\nThe point is that, no matter how big is such structure S \u2032, its equations f \u2208 E \u2032\nare such that |V ars(f)| \u2265 2 (as S \u2032 is non-trivial) and its variables can be grouped in local patterns from the sparsest kind to the densest. To construct an instance of the sparsest case, let S \u2032 be built by setting a first equation where its entry in the structure matrix AS has form (1, 1, 0 +) and then, for the next |E \u2032| \u2212 2 equations, shift such pair of 1\u2019s one position right w.r.t. the previous one. Then complete it with a last equation whose form is form (1, 0+, 1). That is, the structure is built\nA.1. PROOFS OF HYPOTHESIS ENCODING 117\nwith unique pairs of 1\u2019s spread all over the structure. Then, deciding whether there is a minimal structure of size |E \u2032| = K corresponds exactly to BPBP. It is a special case (BBP), when such minimal structure is the densest possible, i.e., when AS has only 1\u2019s and then its corresponding bipartite graph is a K-balanced biclique with |E \u2032| = K2, and deg(u) = K for all vertices u \u2208 V \u20321 \u222a V \u20322 . For instance, see the minimal structure with E \u2032 = { f4, f5 } found at the second recursive step of COA in Fig. 3.2. 2\nA.1.2 Proof of Proposition 1 \u201cLet S(E ,V) be a structure, and \u03d51 : E \u2192 V and \u03d52 : E \u2192 V be any two total causal mappings over S. Then C+1 = C+2 .\u201d\nProof 18 The proof is based on an argument from Nayak [49], which we present here arguably much clearer. Intuitively, it shows that if \u03d51 and \u03d52 differ on the variable an equation f is mapped to, then such variables, viz., \u03d51(f) and \u03d52(f), must be causally dependent on each other (strongly coupled).\nTo show C+1 = C + 2 reduces to C + 1 \u2286 C+2 and C+2 \u2286 C+1 . We show the first\ncontainment, with the second being understood as following by symmetry. Closure operators are extensive, X \u2286 cl(X), and idempotent, cl(cl(X)) = cl(X). That is, if we have C1 \u2286 C+2 , then we shall have C+1 \u2286 (C+2 )+ and, by idempotence, C+1 \u2286 C+2 .\nThen it suffices to show that C1 \u2286 C+2 , i.e., for any (x\u2032, x) \u2208 C1, we must show\nthat (x\u2032, x) \u2208 C+2 as well. Observe by Def. 5 that both \u03d51 and \u03d52 are bijections, then, invertible functions. If \u03d5\u221211 (x) = \u03d5 \u22121 2 (x), then we have (x \u2032, x) \u2208 C2 and thus, trivially, (x\u2032, x) \u2208 C+2 . Else, \u03d51 and \u03d52 disagree in which equations they map onto x. But we show next, in any case, that we shall have (x\u2032, x) \u2208 C+2 .\nTake all equations g \u2208 E \u2032 \u2286 E such that \u03d51(g) 6= \u03d52(g), and let n \u2264 |E| be\nthe number of such \u2018disagreed\u2019 equations. Now, let f \u2208 E \u2032 be such that its mapped variable is x = \u03d51(f). Construct a sequence of length 2n such that, s0 = \u03d51(f) = x and, for 1 \u2264 i \u2264 2n, element si is defined si = \u03d52(\u03d5\u221211 (si\u22121)). That is, we are defining the sequence such that, for each equation g \u2208 E \u2032, its disagreed mappings \u03d51(g) = xa and \u03d52(g) = xb are such that \u03d51(g) is immediately followed by \u03d52(g). As xa, xb \u2208 V ars(g), we have (xa, xb) \u2208 C2 and, symmetrically, (xb, xa) \u2208 C1.\nA.1. PROOFS OF HYPOTHESIS ENCODING 118\nThe sequence is of form s = \u3008x, xf\ufe38 \ufe37\ufe37 \ufe38 f , . . . , xa, xb\ufe38 \ufe37\ufe37 \ufe38 g , . . . , x2n\u22121, x2n\ufe38 \ufe37\ufe37 \ufe38 h \u3009.\nSince x must be in the codomain of \u03d52, we must have a repetition of x at\nsome point 2 \u2264 k \u2264 2n in the sequence index, with sk = x and sk\u22121 = x\u2032\u2032 such that (x\u2032\u2032, x) \u2208 C2. If x\u2032\u2032 = x\u2032, then (x\u2032, x) \u2208 C2 and obviously (x\u2032, x) \u2208 C+2 . Else, note that xf must also be in the codomain of \u03d51, while x \u2032\u2032 in the codomain of \u03d52. Let ` be the point in the sequence, 3 \u2264 ` \u2264 2n\u22121, at which s` = xf = xa and s`+1 = xb for some xb such that (xf , xb) \u2208 C2. It is easy to see that, either we have xb = x\u2032\u2032 or xb 6= x\u2032\u2032 but (xb, x\u2032\u2032) \u2208 C+2 . Thus, by transitivity on such a causal chain, we must have (xf , x \u2032\u2032) \u2208 C+2 and eventually (xf , x) \u2208 C+2 . Finally, since x\u2032 \u2208 V ars(f) and \u03d52(f) = xf , we have (x \u2032, xf ) \u2208 C2 and, by transitivity, (x\u2032, x) \u2208 C+2 . 2\nA.1.3 Proof of Theorem 2. \u201cLet \u03a3 be an fd set defined \u03a3, h-encode(S) for some complete structure S. Then \u03a3 is non-redundant and singleton-rhs but may not be left-reduced (then may not be canonical).\u201d\nProof 19 We will show that properties (a-b) of Def. 9 hold for \u03a3 produced by (Alg. 3) h-encode, but property (c) may not hold.\nAt initialization, the algorithm sets \u03a3=\u2205 and then inserts an fd \u3008X,A\u3009 \u2208 \u03a3\nfor each \u3008f, x\u3009 \u2208 \u03d5t scanned, where x 7\u2192 A and X \u2229 A = \u2205. At termination, for all fd\u2019s in \u03a3 we obviously have |A| = 1 then property (a) holds. Also, note that \u03d5 : S\u2192 V ars(S) is, by Def. 5, a bijection.\nNow, for property (b) not to hold there must be some fd \u3008X,A\u3009 \u2208 \u03a3 that\nis redundant and then can be found in the closure of \u0393 = \u03a3 \\ \u3008X,A\u3009. By Lemma 4 (below), that can be the case only if A \u2286 X or there is \u3008Y,A\u3009 \u2208 \u0393 for some Y . But from X \u2229A = \u2205, we have A * X; and from \u03d5 being a bijection it follows that there can be no such fd in \u0393. Thus it must be the case that \u03a3 is non-redundant, i.e., property (b) holds.\nFinally, property (c) does not hold if there can be some fd \u3008X,A\u3009 \u2208 \u03a3 with\nY \u2282 X such that \u0393 = \u03a3 \\ \u3008X,A\u3009 \u222a \u3008Y,A\u3009 has the same closure as \u03a3. That is, if we may find \u3008Y,A\u3009 \u2208 \u03a3+. Now, pick structure S whose (3\u00d7 3) matrix As has rows (1, 0, 0), (1, 1, 0), (1, 1, 1) as an instance. Alg. 3 encodes it into \u03a3={\u03c6\u2192 x1, x1 \u03c5\u2192\nA.2. PROOFS OF CAUSAL REASONING 119\nx2, x1 x2 \u03c5\u2192 x3}. Let Y ={x1, \u03c5}, and B={x2}. Note that x1 \u03c5 \u2192 x2 \u2208 \u03a3 can be written as \u3008Y,B\u3009 \u2208 \u03a3, and x1 x2 \u03c5\u2192 x3 \u2208 \u03a3 as \u3008Y B,A\u3009 \u2208 \u03a3. Now observe that \u3008Y,A\u3009 \u2208 \u03a3+ can be derived by R5 over \u3008Y,B\u3009, \u3008Y B,A\u3009 \u2208 \u03a3, which is sufficient to show that property (c) may not hold. That is, B is \u201cextraneous\u201d in \u3008Y B,A\u3009 \u2208 \u03a3 and can be removed from its lhs without loss of information to \u03a3. 2\nLemma 4 Let \u03a3 be a (Def. 9-a) singleton-rhs fd set on attributes U . Then X\u2192 A can only be in \u03a3+, where XA \u2286 U , if A \u2286 X or there is non-trivial \u3008Y,A\u3009 \u2208 \u03a3 for some Y \u2282 U .\nProof 20 By Lemma 5 (below), we know that X\u2192 A \u2208 \u03a3+ iff A\u2286 X+. We need to prove that if A* X and there is no Y \u2192 A in singleton-rhs \u03a3, then A* X+. But this is equivalent to show that (Alg. 4) XClosure gives only correct answers for X+ w.r.t. \u03a3, which is known (cf. theorem from Ullman [20, p. 389]). Note that XClosure(\u03a3, X) inserts A in X+ only if A \u2286 X or there is some fd \u3008Y,A\u3009 \u2208 \u03a3. 2\nLemma 5 Let \u03a3 be an fd set. An fd X\u2192 Y is in \u03a3+ iff Y \u2286 X+, where X+ is the attribute closure of X w.r.t. \u03a3.\nProof 21 This is from Ullman [20, p. 386]. Let Y =A1 ... An and suppose Y \u2286 X+. Then for each Ai, we have Ai \u2208 X+ and, by the definition of X+, we must have \u3008X,Ai\u3009 \u2208 \u03a3+. Then it follows by (R4) union that X \u2192 Y is in \u03a3+ as well. Conversely, suppose \u3008X, Y \u3009 \u2208 \u03a3+. Then, by (R3) decomposition we have \u3008X,Ai\u3009 \u2208 \u03a3+ for each Ai \u2208 Y . 2\nA.2. Proofs of Causal Reasoning\nA.2.1 Proof of Lemma 2 \u201cLet S(E ,V) be a complete structure, \u03d5 a total causal mapping over S and \u03a3 an fd set encoded through \u03d5 given S. If \u3008X,A\u3009 \u2208 \u03a3, then A#, the attribute folding of A (w.r.t. \u03a3) exists and is unique.\u201d\nProof 22 The existance of A# is ensured by the degenerate case where X = A#, as X\u2192 A is itself in \u03a3B by an empty application of R5. If X\u2192 A is in fact folded w.r.t. \u03a3, then the folding of A exists. Else, it is not folded yet X\u2192 A is non-trivial\nA.2. PROOFS OF CAUSAL REASONING 120\nbecause by Theorem 2 \u03a3 is non-redundant. Then, by Def. 11 there must be some Y \u2286 U with Y + X such that Y \u2192 X is in \u03a3+ and X 6\u2192 Y . By Def. 10, there is a finite application of R5 over fd\u2019s in \u03a3 to derive Y .\u2212\u2192 X. Then by R2\u223cR5 over X\u2192 A, we have Y \u2192 A. Although there may be many such (intermediate) attribute sets Y \u2282 U along the transitive chaining satisfying the conditions above, we claim there is at least one that is a folding of A. Suppose not. Then, for all such Y \u2282 U , there is some Y \u2032 \u2282 U with Y \u2032 + Y such that Y \u2032 \u2192 Y and Y 6\u2192 Y \u2032, leading to an infinite regress. Nonetheless, in so far as cycles are ruled out by force of Def. 11, then \u03a3+ must have an infinite number of fd\u2019s. But \u03a3+ is finite, viz., bounded by 22|U | (cf. [21, p. 165]). . Therefore the folding of A must exist. Moreover, observe that \u03a3 is encoded through \u03d5, which is by Def. 5 a bijection. Then we have \u3008X,A\u3009 \u2208 \u03a3 for exactly one attribute setX. Then, as a straightfoward follow-up of the rationale that led us to infer the folding existance, note that there must be a single chaining Y n .\u2212\u2192 ... .\u2212\u2192 Y 1 .\u2212\u2192 Y 0 .\u2212\u2192 X .\u2212\u2192 A. Again, as cycles are ruled out by force of Def. 11 and \u03a3+ is finite, then the folding of A is unique. 2\nA.2.2 Proof of Theorem 3 \u201cLet S(E ,V) be a complete structure, and \u03a3 an fd set encoded given S. Now, let \u3008X,A\u3009 \u2208 \u03a3. Then AFolding(\u03a3, A) correctly computes A#, the attribute folding of A (w.r.t. \u03a3) in time O(|S|2).\u201d\nProof 23 For the proof roadmap, note that AFolding is monotone (size of A? can only increase) and terminates precisely when A(i+1) =A(i), where A(i) denotes the attributes in A? at step i of the outer loop. The folding A# of A at step i is A(i) \\ \u039b(i). We shall prove by induction, given attribute A from fd X\u2192 A in \u03a3, that A?\\ \u039b returned by AFolding(\u03a3, A) is the unique attribute folding A# of A.\n(Base case). By Theorem 2, \u03a3 is non-redundant with (then) non-trivial\n\u3008X,A\u3009 \u2208 \u03a3 for exactly one attribute set X, the algorithm always reaches step i = 1, which is our base case. Then X is placed in A(1) and A in \u039b(1), and we have A(1) = XA and \u039b(1) = A. Therefore, A(1) \\ \u039b(1) = X, and in fact we have \u3008X,A\u3009 \u2208 \u03a3B by an empty application of R5. For it to be specifically in \u03a3#\u2282 \u03a3B, it must be folded w.r.t. set \u2206 of consumed fd\u2019s at this step, viz., \u2206(1) ={X\u2192 A}. In fact, as the only fd in \u2206(1), by Def. 11 it must be folded w.r.t. \u2206(1), and we\nA.2. PROOFS OF CAUSAL REASONING 121\nhave A#= X at step i=1.\n(Induction). Now, let i = k, for k > 1, and assume that \u3008A(k)\\ \u039b(k), A\u3009 \u2208\n\u03a3# \u2282 \u03a3B with A(k) 6= \u039b(k). By Lemma 2 we know that A# = A(k) \\ \u039b(k) is the unique folding of A at step i= k. For the inductive step, suppose Y is placed in A(k+1) and B in \u039b(k+1) because \u3008Y,B\u3009\u2208 \u03a3 \\\u2206(k) and B \u2208 A(k).\nSince B \u2208 A(k) and B /\u2208 \u039b(k) (it is yet just be consumed into \u039b(k+1)), we can\nwrite (A(k)\\\u039b(k)) = ZB for some Z 6= B, where (A(k) \\ \u039b(k))\ufe38 \ufe37\ufe37 \ufe38 ZB \u2192 A is assumed in \u03a3#. Now, with the application of R5 consuming Y \u2192 B we have (A(k)Y B \\ \u039b(k)B)\ufe38 \ufe37\ufe37 \ufe38 ZS \u2192 A, where S = Y \\ \u039b(k). We claim that ZS\u2192 A is folded w.r.t. \u2206(k+1).\nSuppose not. Then by Def. 11 there must be some W + ZS such that\nW \u2192 ZS is in (\u2206(k+1))+ but ZS 6\u2192 W . Since ZS 6= \u2205, there must be some C \u2208 ZS, i.e., C /\u2208 \u039b(k+1). Note that, as W \u2192 ZS is in (\u2206(k+1))+, then by (R3) decomposition we have W \u2192 C in (\u2206(k+1))+ as well. But by Lemma 4 that can only be the case if there is some \u3008T,C\u3009 \u2208 \u2206(k+1), which means C has been already consumed into \u039b(k+1), though C /\u2208 \u039b(k+1). . Finally, as for the time bound, note that in worst case, exactly one fd Y \u2192 B is consumed from \u03a3 into \u2206 for each step of the outer loop, where |\u03a3| = |E|. That is, let n = |E|, then n is decreased stepwise in arithmetic progression such that n+ (n\u22121) + . . .+ 1 = n (n\u22121)/2 scans are required overall, i.e., O(n2). Note also, however, that B may be the only symbol read at each such fd scan but in worst case at most |U | = |V| symbols are read. Thus our measure n should be actually overestimated n = |E| |V| = |S|. Therefore Alg. 6 is bounded by O(|S|2). 2\nA.2.3 Proof of Corollary 2 \u201cLet S(E ,V) be a complete structure, and \u03a3 an fd set encoded given S. Then algorithm folding(\u03a3) correctly computes \u03a3#, the folding of \u03a3 in time that is f(|S|) \u0398(|E|), where f(|S|) is the time complexity of (Alg. 6) AFolding.\u201d\nProof 24 By Theorem 3, we know that sub-procedure (Alg. 6) AFolding is correct and terminates. Then (Alg. 5) folding necessarily inserts in \u03a3# (initialized empty) exactly one fd Z #\u2212\u2192 A for each fd X\u2192 A in \u03a3 scanned. Thus, at termination we have |\u03a3#|= |\u03a3|. Again, as AFolding is correct, we know Z is the unique folding of\nA.2. PROOFS OF CAUSAL REASONING 122\nA. Therefore it must be the case that Alg. 5 is correct. Finally, for the time bound, the algorithm iterates over each fd in \u03a3 without having to read its symbols, and at each such step AFolding takes time that is f(n). Thus folding takes f(|S|) \u0398(|E|). But we know from Theorem 3 and Remark 4 that f(|S|) \u2208 O(|S|), then it takes O(|S| |E|). 2\nA.2.4 Proof of Proposition 3 \u201cLet S(E ,V) be a complete structure, \u03d5 a total causal mapping over S and \u03a3 an fd set encoded through \u03d5 given S. Let \u03a3# be the folding of \u03a3, then \u03a3# is parsimonious.\u201d\nProof 25 By Lemma 2 we know that, for each fd \u3008X,A\u3009 \u2208 \u03a3, the attribute folding Z of A such that Z #\u2212\u2192 A exists and is unique. That is, for no Y 6= Z we have Y #\u2212\u2192 A. Thus \u03a3# , folding(\u03a3) automatically satisfies Def. 13, as long as we show it is canonical (cf. Def. 9).\nMoreover, by Theorem 2 we know that \u03a3 is both non-redundant and singleton-\nrhs. Now, consider by Lemma 2 that AFolding builds a bijection mapping each \u3008X,A\u3009 \u2208 \u03a3 to exactly one \u3008Z,A\u3009 \u2208 \u03a3# such that Z #\u2212\u2192 A. Since \u03a3 is singleton-rhs, it is obvious that \u03a3# is as well and covers all attributes in the rhs of fd\u2019s in \u03a3. Also, the bijection implies |\u03a3#| = |\u03a3|. Since \u03a3 is non-redundant and has exactly one fd with each attribute A in its rhs, then by Lemma 4 so is \u03a3#.\nFinally we will show that unlike \u03a3, its folding \u03a3# must be left-reduced.\nSuppose not. Then for some fd Z\u2192 A in \u03a3# there is S \u2282 Z such that non-trivial S\u2192 A is in (\u03a3#)+. Since Z\u2192 A is the only fd in \u03a3#with A in its rhs and S\u2192 A is non-trivial, we must have S M\u2212\u2192 Z M\u2212\u2192 A.\nNow, suppose S\u2192 A is not folded. Then there is W + S such that W\u2192 S\nis in (\u03a3#)+ but S 6\u2192 W . Note that W 6= Z, as W + S. Also, W \u2192 S and S\u2192 Zimplies W \u2192 Z by (R5) transitivity. Note also that S 6\u2192 W and S\u2192 Z implies Z 6\u2192 W . But Z \u2192 A is assumed folded. . That is, S \u2192 A must be folded. Then we have both S\u2192 A and Z\u2192 A folded, though S 6= Z. That is, the attribute folding of A is not unique, even though we know by Lemma 2 that it must be unique. . Thus \u03a3#must be left-reduced, altogether, therefore, parsimonious. 2\nA.2. PROOFS OF CAUSAL REASONING 123\nA.2.5 Proof of Theorem 4 \u201cLet S(E ,V) be a complete structure, \u03d5 a total causal mapping over S and \u03a3 an fd set encoded through \u03d5 given S. Then xa, xb \u2208 V are such that xb is causally dependent on xa, i.e., (xa, xb) \u2208 C+\u03d5 iff there is some non-trivial fd \u3008X,B\u3009 \u2208 \u03a3B with A \u2208 X, where B 7\u2192 xb and A 7\u2192 xa.\u201d\nProof 26 We prove the statement by induction. We consider first the \u2018if\u2019 direction, and then its \u2018only if\u2019 converse. (Base case). Let \u3008X,B\u3009 \u2208 \u03a3 be some fd with A \u2208 X, where B 7\u2192 xb and A 7\u2192 xa. By Theorem 2, it is non-trivial and then by default (i.e., an empty application of R5) it is in \u03a3B. But as X \u2192 B is in \u03a3, (Alg. 3) h-encode ensures that there is exactly one equation f \u2208 E such that \u03d5(f) = xb and xa \u2208 V ars(f) where B 7\u2192 xb and A 7\u2192 xa. Then by force of Eq. 3.1 we must have (xa, xb) \u2208 C\u03d5. Thus, we obviously have (xa, xb) \u2208 C+\u03d5 as well.\n(Induction). Now, recall Armstrong\u2019s (R5) pseudo-transitivity rule adapted\nhere for the particular case of singleton-rhs fd sets, viz., if Y \u2192 C and CZ \u2192 B, then Y Z \u2192 B. By the inductive hypothesis, take any two non-trivial fd\u2019s \u3008Y,C\u3009, \u3008CZ,B\u3009 \u2208 \u03a3B with B /\u2208 Y and assume that the causal dependency property holds for their attributes that encode variables. That is, let D \u2208 Y and E \u2208 Z, where D 7\u2192 xd and E 7\u2192 xe for xd, xe \u2208 V such that (xd, xc), (xe, xb), (xc, xb) \u2208 C+\u03d5 . Note that both Y \u2192 C and CZ \u2192 B are non-trivial, then C /\u2208 Y , B /\u2208 Z and B 6= C. Moreover, B /\u2208 Y has been assumed such that the fd \u3008Y Z,B\u3009 \u2208 \u03a3B to be derived by R5 over Y \u2192 C and CZ\u2192 B is also non-trivial to satisfy the condition of the theorem. Now, it is easy to see that the property holds likewise for nontrivial fd \u3008Y Z,B\u3009 \u2208 \u03a3B. In fact, (xd, xc), (xc, xb) \u2208 C+\u03d5 implies (xd, xb) \u2208 C+\u03d5 and also by the inductive hypothesis we have (xe, xb) \u2208 C+\u03d5 . That is, for either some D \u2208 Y or some E \u2208 Z, we must have (xd, xb), (xe, xb) \u2208 C+\u03d5 .\nThe converse \u2018only if\u2019 direction can be shown by a symmetrical inductive\nargument. That is, for the base case suppose (xa, xb) \u2208 C\u03d5. Then, by Eq. 3.1 we know there is some f \u2208 E such that \u03d5(f) = xb and xa \u2208 V ars(f). Moreover, in that case (Alg. 3) h-encode ensures there must be some non-trivial fd \u3008X,B\u3009 \u2208 \u03a3 with A \u2208 X where B 7\u2192 xb and A 7\u2192 xa. Thus by an empty application of R5 we\nA.2. PROOFS OF CAUSAL REASONING 124\nhave \u3008X,B\u3009 \u2208 \u03a3B. The inductive step shows the property still holds for arbitrary causal dependencies in C+\u03d5 . 2\nA.2.6 Proof of Proposition 4 \u201cLet S(E ,V) be a structure with variable x \u2208 V. Then x can only be a first cause of some y \u2208 V if x is exogenous. Accordingly, any variable y \u2208 V can only have some first cause x \u2208 V if it is endogenous.\u201d\nProof 27 The proof is straightforward from definitons. For the first statement, suppose by contradiction that x \u2208 V is not exogenous but is a first cause of some y \u2208 V . By Def. 5, \u03d5 is bijective then there is some f \u2208 E such that \u03d5(f) = x. Moreover, as x is not exogenous then by Def. 8 it must be endogenous. In other words, there must be some xa \u2208 V such that xa 6= x \u2208 V ars(f) and then by Eq. 3.1 we have (xa, x) \u2208 C\u03d5 hence (xa, x) \u2208 C+\u03d5 . However, as x is a first cause, by Def. 14 there can be no y \u2208 V such that (y, x) \u2208 C+\u03d5 . . Now, a symmetrical argument proves the second statement. Also by contradiction take a variable y \u2208 V that is not endogenous and suppose it has some first cause x \u2208 V . As variable y is not endogenous then by Def. 8 it must be exogenous. In other words, there must be f \u2208 E such that V ars(f) = {y}. Thus for any total causal mapping \u03d5 over S, we must have \u03d5(f) = y and, for no x \u2208 V , we have (x, y) \u2208 C\u03d5. Therefore it is not possible to derive (x, y) \u2208 C+\u03d5 for some x \u2208 V . But as y has some first cause x \u2208 V by assumption, we must have (x, y) \u2208 C+\u03d5 . . 2\nA.2.7 Proof of Lemma 3 \u201cLet S(E ,V) be a complete structure, \u03d5 a total causal mapping over S and \u03a3 an fd set encoded through \u03d5 given S. Then a variable xa \u2208 V can only be a first cause of some variable xb \u2208 V, where \u3008X,B\u3009 \u2208 \u03a3, and B 7\u2192 xb, A 7\u2192 xa, if either (i) A \u2208 X or (ii) A /\u2208 X but there is \u3008Z,C\u3009 \u2208 \u03a3B with A \u2208 Z and C \u2208 X.\u201d\nProof 28 We prove the statement by construction out of Theorem 4.\nBy Def. 14, one of the conditions for xa to be a first cause of xb is that\n(xa, xb) \u2208 C+\u03d5 . Moreover, by Theorem 4 we know that (xa, xb) \u2208 C+\u03d5 can only hold if there is some non-trivial fd \u3008Z,B\u3009 \u2208 \u03a3B with A \u2208 Z, where B 7\u2192 xb and A 7\u2192 xa. Now, by Def. 5 \u03d5 is bijective then there is \u3008X,B\u3009 \u2208 \u03a3. Moreover, since\nA.2. PROOFS OF CAUSAL REASONING 125\n\u03a3 is parsimonious, X\u2192 B is the only fd in \u03a3 with B in its rhs. So let A /\u2208 X. Then we know X 6= Z hence X\u2192 B cannot be the fd required by Theorem 4.\nThen such fd Z .\u2212\u2192 B with A \u2208 Z can only exist if derived by some finite\napplication of R5. That is, there must be some \u3008Z,C\u3009 \u2208 \u03a3 with A \u2208 Z such that X = CW for some W and then R5 can be applied over \u3008Z,C\u3009, \u3008CW,B\u3009 \u2208 \u03a3 to get non-trivial \u3008ZW,B\u3009 \u2208 \u03a3B where A \u2208 Z.\nNow, it is easy to see that when such fd Z\u2192 C with A \u2208 Z does not exists\nin \u03a3B (the second condition of the lemma), then obviously Z\u2192 C cannot exist in \u03a3 to satisfy the requirement imposed by Theorem 4. That is, (xa, xb) /\u2208 C+\u03d5 . 2\nA.2.8 Proof of Theorem 5 \u201cLet S(E ,V) be a complete structure, \u03d5 a total causal mapping over S and \u03a3 an fd set encoded through \u03d5 given S. Now, let B be an attribute that encodes some variable xb \u2208 V. If \u3008X,B\u3009 \u2208 \u03a5(\u03a3)#,1 then every first cause xa of xb (if any) is encoded by some attribute A \u2208 X.\u201d\nProof 29 We show that the existance of a missing first cause xc of xb for folded X #\u2212\u2192 B, where B 7\u2192 xb and C 7\u2192 xc but C /\u2208 X leads to a contradiction.\nSuppose, by contradiction, that there is some missing first cause xc \u2208 V of\nxb, where C 7\u2192 xc and C /\u2208 X. Then, by Lemma 3, since variable xc is a first cause of variable xb, it must be exogenous and, for \u3008Y,B\u3009 \u2208 \u03a5(\u03a3) either (i) C \u2208 Y or (ii) C /\u2208 Y but there is \u3008Z,D\u3009 \u2208 \u03a5(\u03a3)B with C \u2208 Z and some D \u2208 Y .\nIn the first case (i), since xc is exogenous and \u03a3 is parsimonious, we have\n\u3008\u03c6,C\u3009 \u2208 \u03a3 but by Def. 15 there can be no W\u2192 C in the \u03c5-projection \u03a5(\u03a3) of \u03a3. That is, C cannot be \u2018consumed\u2019 by R5 and then \u3008Y,B\u3009 \u2208 \u03a5(\u03a3) with C \u2208 Y implies that, for any \u3008W,B\u3009 \u2208 \u03a5(\u03a3)B, we must have C \u2208 W . However, by assumption we have \u3008X,B\u3009 \u2208 \u03a5(\u03a3)# then, by Def. 12, \u3008X,B\u3009 \u2208 \u03a5(\u03a3)B, yet C /\u2208 X. . In the second case (ii), observe that C \u2208 Z and D \u2208 Y , and let Y = DS. Then by R5 over Z\u2192 D and DS\u2192 B we get \u3008ZS,B\u3009 \u2208 \u03a5(\u03a3)B, where C \u2208 Z. Well, either ZS \u2192 B is folded or it is not, rendering two cases for analysis. If ZS\u2192 B is folded, then both ZS\u2192 B and X \u2192 B are folded. But as \u03a5(\u03a3) is 1 Note that the folding is taken w.r.t. the \u03c5-projection of \u03a3, then xb where B 7\u2192 xb is an endogenous variable.\nA.3. PROOFS OF PROBABILISTIC DB SYNTHESIS 126\nparsimonious, then by Lemma 2 the folding of B must be unique. Therefore we must have ZS = X, with C \u2208 Z but C /\u2208 X. . Else, assume ZS\u2192 B is not folded. Then by Def. 11 there is some W with W + ZS such that non-trivial W\u2192 ZS is in \u03a5(\u03a3)+ and ZS 6\u2192 W .\nHowever, as C \u2208 Z and W \u2192 ZS, by (R3) decomposition we must have\nW \u2192 C in \u03a5(\u03a3)+, either with C \u2208 W or with C /\u2208 W and then W \u2192 C is nontrivial. But we know the latter cannot be the case by the same argument used in the first case (i), viz., xc is exogenous with C 7\u2192 xc and \u03a3 is parsimonious. That is, we must have C \u2208 W . Furthermore, as W \u2192 ZS in \u03a5(\u03a3)+, then by R5 over ZS\u2192 B we get \u3008W,B\u3009 \u2208 \u03a5(\u03a3)B with C \u2208 W . Now it is easy to see that the same situation recurs to W\u2192 B. If it is not folded, eventually for some T we will have T .\u2212\u2192 W .\u2212\u2192 B with C \u2208 T , where T\u2192 B will be folded just like X\u2192 B. That is, by (Lemma 2) the uniqueness of the folding of B, we will have T = X with C \u2208 T and C /\u2208 X. . 2\nA.3. Proofs of Probabilistic DB Synthesis\nA.3.1 Proof of Theorem 6 \u201cLet Sk and Hk be (resp.) the complete structure and \u2018big\u2019 fact table of hypothesis k, and let \u0393\u2032k be the repaired factorization of Sk over Hk, and Y0 the \u2018explanation\u2019 table where hypothesis k is recorded. Now, let Y k be a U-relational schema defined Y k , synthesize4u(Sk, Hk, Y0). Then Y k is in BCNF w.r.t. \u0393\u2032k and is minimalcardinality.\u201d\nProof 30 Let Y ik [ViDi |\u03c6AiGi ] and Y j k [Vj Dj |S T ] be (resp.) any u-factor projection and predictive projection of Hk. Note that all fd\u2019s in \u0393 \u2032 k are either in \u03a6(\u0393\u2032k) of form \u03c6Ai \u2192 B or \u03c6 \u2192 B, or in \u03a5(\u0393\u2032k) of form A1A2 ... A` S\u2192 T with \u03c5 \u2208 S. We must show that no fd in (\u0393\u2032k)+ can violate Y ik or Y j k . It is easy to see that the projection (cf. Def. 20) of non-trivial fd\u2019s in \u03a6(\u0393\u2032k) + onto Y jk is empty, just like the projection of non-trivial fd\u2019s in \u03a5(\u0393\u2032k) + onto Y ik .\nFor the u-factor projections, note by Def. 21 that for any fd X \u2192 C in\n(\u0393\u2032k) + to violate BCNF in Y ik [ViDi |\u03c6AiGi ], it must be non-trivial (C 6\u2208 X) with XC \u2286 \u03c6AiGi but X 6\u2192 \u03c6AiGi (that is, X is not a superkey for Y ik ). Note that we\nA.3. PROOFS OF PROBABILISTIC DB SYNTHESIS 127\nhave both \u03c6Ai\u2192 B and \u03c6\u2192 AiB in (\u0393\u2032k)+ for any B \u2208 Gi, but both \u03c6Ai and \u03c6 are superkeys for Y ik . Also, that there can be no non-trivial fd\u2019s \u3008X,C\u3009 \u2208 \u03a6(\u0393\u2032k)+ with \u03c6 6\u2208 X, and by the definition of Problem 1 we know that AiGi is a maximal group. So, for any non-trivial \u3008X,C\u3009 \u2208 \u03a6(\u0393\u2032k)+, X must be a superkey for Y ik . Thus no u-factor projection can be subject of BCNF violation w.r.t. \u0393\u2032k.\nNow, for predictive projection Y jk [Vj Dj |S T ] let us reconstruct the process\ntowards deriving \u3008S, T \u3009 \u2208 \u03a5(\u0393\u2032k)B. Note that it is derived by synthesize4u simulating ` applications of R5 over \u3008\u03c6,Ai\u3009, \u3008A1A2 ...A` S, T \u3009 \u2208 \u0393\u2032k for 0 \u2264 i \u2264 `. Note also that (i) no cyclic fd\u2019s can be involved in such R5 applications, as they must always be over an fd in \u03a6(\u0393\u2032k); and (ii) as a result of (Alg. 7) merge, A1A2 ...A` S\u2192 T was the only non-trivial fd in the projection \u03c0A1 A2 ...A` ST (\u0393 \u2032 k). Thus, the only nontrivial fd\u2019s in the projection \u03c0ST ( (\u0393 \u2032 k) ) + in addition to S\u2192 T itself must be of form S\u2192 C rendered out of (R3) decomposition from it for all C \u2208 T . In any such fd\u2019s, we have S as a superkey for Y jk . Therefore no predictive projection can be subject of BCNF violation w.r.t. \u0393\u2032k.\nFor the minimality note, as a consequence of (Alg. 7) merge, any two\nschemes Y pk [XZ], Y q k [VW ] are rendered by synthesize4u into Y k iff we have fd\u2019s \u3008X,Z\u3009, \u3008V,W \u3009 \u2208 (\u0393\u2032k)+ and X 6\u2194 V , i.e., it is not the case that both X\u2192 V and V \u2192 X hold in (\u0393\u2032k)+. Now, to prove that Y k is minimal-cardinality, we have to find that merging any such pair of arbitrary schemes shall hinder BCNF in Y k. In fact, take Y \u2032k := Y k \\ (Y p k [XZ] \u222a Y q k [VW ]) \u222a Y `k [XZVW ]. As X 6\u2194 V , then neither X nor V can be a superkey for Y `k , which therefore cannot be in BCNF. 2\nA.3.2 Proof of Theorem 7 \u201cLet Sk be the complete structure of hypothesis k, and Hk[U ] its \u2018big\u2019 fact table such that \u0393\u2032k is the repaired factorization of Sk over Hk and Y0 is the \u2018explanation\u2019 table where hypothesis k is recorded. Now, let Y k be a U-relational schema defined Y k , synthesize4u(Sk, Hk, Y0). Then,\n(a) the join ./mi=1 Y i k [ViDi |\u03c6AiGi ] of any subset of the u-factor projections\nof Hk is lossless w.r.t. \u0393 \u2032 k.\n(b) any predictive projection Y jk [Vj Dj |S T ], result of a join of the theoretical\nu-factor Y0 [V0D0 |\u03c6 \u03c5 ] with the \u2018big\u2019 fact table Hk[U ] and in turn with\nA.3. PROOFS OF PROBABILISTIC DB SYNTHESIS 128\nu-factor projections Y ik [ViDi |\u03c6AiGi ], is lossless w.r.t. \u0393\u2032k.\u201d\nProof 31 For item (a), by Lemma 6, we know that any pair Y ik [ViDi |\u03c6AiGi ], Y jk [Vj Dj |\u03c6Aj Gj ] of u-factor projections of Hk will have a lossless join w.r.t. \u0393\u2032k iff (\u03c6AiGi \u2229 \u03c6Aj Gj)\u2192 (\u03c6AiGi \\ \u03c6Aj Gj) or (\u03c6AiGi \u2229 \u03c6Aj Gj)\u2192 (\u03c6Aj Gj \\ \u03c6AiGi) hold in (\u0393 \u2032 k) +. By Def. 17, we know that (\u03c6AiGi \u2229 \u03c6Aj Gj) = {\u03c6}, and \u03c6AiGi \\ \u03c6Aj Gj = AiGi. In fact \u03c6\u2192 AiGi is a repaired fd in \u0393\u2032k, therefore Y ik and Y jk have a lossless join. Now, since the join is an associative operation [20, p. 62], and as we have chosen Y ik and Y j k arbitrarily, then clearly any subset of the u-factor projections must have a lossless join.\nFor item (b), for any predictive projection Y jk [Vj Dj |S T ] take the join ./\nY ik [ViDi |\u03c6AiGi ] of u-factor projections such that, for all Ai \u2208 AiGi, we have Ai \u2208 W \u2282 Z where S = Z \\W and \u3008Z, T \u3009 \u2208 \u0393\u2032k. That is, Ai is a pivot attribute representing a first cause of some C \u2208 T . By item (a), we know that such join is lossless.\nWe must show that the join ./ Y ik with \u2018big\u2019 fact table Hk[U ] is also lossless.\nBy Lemma 6, that is the case iff (\u03c6AiGi \u2229 U) \u2192 (\u03c6AiGi \\ U) or (\u03c6AiGi \u2229 U) \u2192 (U \\ \u03c6Aj Gj) hold in (\u0393\u2032k)+. In fact, we have (\u03c6AiGi \u2229 U) = \u03c6AiGi and (\u03c6AiGi \\ U) = \u2205 such that \u03c6AiGi \u2192 \u2205 is trivially in (\u0393\u2032k)+.\nFinally, the join of theoretical u-factor Y0 [V0D0 |\u03c6 \u03c5 ] with big fact table\nHk[U ] must be lossless likewise. In fact, note that (\u03c6 \u03c5 \u2229 U) = \u03c6 \u03c5, and (\u03c6 \u03c5\\U) = \u2205. Then also trivially we have \u03c6 \u03c5\u2192 \u2205, which is in (\u0393\u2032k)+ as well. Since the join is commutative [20, p. 62], the order of application is irrelevant therefore the join of all joins examined above taken together must be lossless. 2\nLemma 6 Let \u03a3 be a set of fd\u2019s on attributes U , and Ri[S], Rj[T ] \u2208 R[U ] be relation schemes with ST \u2286 U ; and let \u03c0ST (\u03a3) be the projection of \u03a3 onto ST . Then Ri[S] and Rj[T ] have a lossless join w.r.t. \u03c0ST (\u03a3) iff (S \u2229 T ) \u2192 (S \\ T ) or (S \u2229 T )\u2192 (T \\ S) hold in \u03c0ST (\u03a3)+.\nProof 32 See Ullman [20, p. 397]. 2"}], "references": [{"title": "The fourth paradigm: Dataintensive scientific discovery", "author": ["T. HEY", "S. TANSLEY", "K. TOLLE"], "venue": "Microsoft Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Distilling free-form natural laws from experimental data", "author": ["M. SCHMIDT", "H. LIPSON"], "venue": "Science, Washington, v. 324,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Data science and prediction", "author": ["V. DHAR"], "venue": "Communications of the ACM, v. 56,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Big data and its technical challenges", "author": ["H.V. JAGADISH", "J. GEHRKE", "A. LABRINIDIS", "Y. PAPAKONSTANTI- NOU", "J.M. PATEL", "R. RAMAKRISHNAN", "C. SHAHABI"], "venue": "Communications of the ACM, v. 57,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Computational Science: Ensuring America\u2019s competitiveness", "author": ["M.R. BENIOFF", "E.D. (Eds.). LAZOWSKA"], "venue": "PITAC (US President\u2019s Information Technology Advisory Committee),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Data exploration of turbulence simulations using a database cluster", "author": ["E. PERLMAN", "R. BURNS", "Y. LI", "C. MENEVEAU"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "The Blue Brain Project", "author": ["H. MARKRAM"], "venue": "Nature Reviews Neuroscience,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Managing scientific data", "author": ["A. AILAMAKI", "V. KANTERE", "D. DASH"], "venue": "Comm. ACM, v. 53,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Scientific data management at the Johns Hopkins Institute for  BIBLIOGRAPHY  109 Data Intensive Engineering and Science", "author": ["Y. AHMAD", "R. BURNS", "M. KAZHDAN", "C. MENEVEAU", "A. SZALAY", "A. TERZIS"], "venue": "SIGMOD Record, v. 39,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Beyond big data", "author": ["J.B. CUSHING"], "venue": "Computing in Science & Engineering, v. 15,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Point: Hypotheses first", "author": ["R. WEINBERG"], "venue": "Nature, London, v. 464,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Counterpoint: Data first", "author": ["T. GOLUB"], "venue": "Nature, London, v. 464,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Where is the brain in the Human Brain Project", "author": ["Y. FR\u00c9GNAC", "G. LAURENT"], "venue": "Nature, London,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A historical introduction to the philosophy of science", "author": ["J. LOSEE"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Introduction to bayesian statistics", "author": ["W.M. BOLSTAD"], "venue": "2nd. ed. Wiley- Interscience,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "MayBMS: A system for managing large uncertain and probabilistic databases", "author": ["C. KOCH"], "venue": "Managing and Mining Uncertain Data, Chapter", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Principles of Databases and Knowledge-Base Systems", "author": ["J. ULLMAN"], "venue": "Computer Science Press,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1988}, {"title": "Theory of relational databases", "author": ["D. MAIER"], "venue": "Computer Science Press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1983}, {"title": "A call to arms: Revisiting database design", "author": ["A. BADIA", "D. LEMIRE"], "venue": "SIG- MOD Record, v. 40,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Causal ordering and identifiability", "author": ["H. SIMON"], "venue": "Studies in Econometric Methods,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1953}, {"title": "Causality: Models, reasoning, and inference", "author": ["J. PEARL"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2000}, {"title": "A survey of web information extraction systems", "author": ["C.-H. CHANG", "M. KAYED", "M.R. GIRGIS", "K. SHAALAN"], "venue": "IEEE Transactions on Knowledge and Data Engineering, v. 18,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Fast and simple relational processing of uncertain data", "author": ["L. ANTOVA", "T. JANSEN", "C. KOCH", "D. OLTEANU"], "venue": "Proc. of IEEE ICDE", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "\u03a5-DB: A system for datadriven hypothesis management and analytics", "author": ["B. GONCALVES", "F.C. SILVA", "F. PORTO"], "venue": "Technical report, LNCC,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Design-theoretic encoding of deterministic hypotheses as constraints and correlations in U-relational databases", "author": ["B. GONCALVES", "F. PORTO"], "venue": "Technical report, LNCC,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Cause and counterfactual", "author": ["H. SIMON", "N. RESCHER"], "venue": "Philosophy of Science, v. 33,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1966}, {"title": "ModelDB: A database to support computational neuroscience", "author": ["M. HINES", "T. MORSE", "M. MIGLIORE", "N. CARNEVALE", "G. SHEPHERD"], "venue": "J. Comput. Neurosci., v. 17,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "BioModels Database: A repository of mathematical models of biological processes", "author": ["V. CHELLIAH", "C. LAIBE", "N. Le Nov\u00e8re"], "venue": "Method. Mol. Biol.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Integration from proteins to organs: the Physiome Project", "author": ["P.J. HUNTER", "T.K. BORG"], "venue": "Nat. Rev. Mol. Cell. Biol.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2003}, {"title": "Strategies for the Physiome Project", "author": ["J.B. BASSINGTHWAIGHTE"], "venue": "Ann. Biomed. Eng., v", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "Data cleaning: Problems and current approaches", "author": ["E. RAHM", "H. Hai Do"], "venue": "IEEE Data Engineering Bulletin, v. 23,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}, {"title": "Modeling and querying possible repairs in duplicate detection", "author": ["G. BESKALES", "M.A. SOLIMAN", "I.F. ILYAS", "S. BEN-DAVID"], "venue": "PVLDB, v. 2,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Data model for scientific models and hypotheses", "author": ["F. PORTO", "S. SPACAPPIETRA"], "venue": "The evolution of Conceptual Modeling,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Data-driven Neuroscience: Enabling breakthroughs via innovative data management", "author": ["A. STOUGIANNIS", "F. TAUHEED", "M. PAVLOVIC", "T. HEINIS", "A. AILA- MAKI"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "I/O streaming evaluation of batch queries for data-intensive computational turbulence", "author": ["K. KANOV", "E.A. PERLMAN", "R.C. BURNS", "Y. AHMAD", "A.S. SZALAY"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "NoDB: Efficient query execution on raw data", "author": ["I. ALAGIANNIS", "R. BOROVICA", "M. BRANCO", "S. IDREOS", "A. AILAMAKI"], "venue": "files. In: Proc. of ACM SIGMOD", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Representation of research hypotheses", "author": ["L. SOLDATOVA", "A. RZHETSKY"], "venue": "J. Biomed. Sem.,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "The automation of science", "author": ["KING", "R. D"], "venue": "Science, Washington, v. 324,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "HyBrow: a prototype system for computer-aided hypothesis evaluation", "author": ["S RACUNAS"], "venue": "Bioinformatics, v. 20,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2004}, {"title": "SWAN: A distributed knowledge infrastructure for alzheimer disease research", "author": ["GAO Y"], "venue": "J. Web Semantics,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "Hypotheses, evidence and relationships: The HypER approach for representing scientific knowledge claims", "author": ["A de WAARD"], "venue": "ISWC Proc. of Workshop on Semantic Web Applications in Scientific Discourse", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "A note on the correctness of the causal ordering algorithm", "author": ["D. DASH", "M.J. DRUZDZEL"], "venue": "Artif. Intell., v. 172,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2008}, {"title": "Causality in Bayesian belief networks", "author": ["M.J. DRUZDZEL", "H.A. SIMON"], "venue": "Proc. of Int. Conf. on Uncertainty in Artificial Intelligence", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1993}, {"title": "Biclustering algorithms for biological data analysis: A survey", "author": ["S.C. MADEIRA", "A.L. OLIVEIRA"], "venue": "IEEE Transactions on Computational Biology and Bioinformatics,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2004}, {"title": "Information-theoretic coclustering", "author": ["I.S. DHILLON", "S. MALLELA", "D.S. MODHA"], "venue": "Proc. of ACM SIGKDD", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2003}, {"title": "An efficient algorithm for solving pseudo clique enumeration problem", "author": ["UNO T"], "venue": "Algorithmica, v. 56,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2010}, {"title": "Computers and intractability: A guide to the theory of NP-completeness", "author": ["M.R. GAREY", "D.S. JOHNSON"], "venue": "1st. ed., Series of Books in the Mathematical Sciences", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 1979}, {"title": "The NP-completeness column: an ongoing guide", "author": ["D.S. JOHNSON"], "venue": "J. Algorithms, v. 8,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1987}, {"title": "An n algorithm for maximum matchings in bipartite graphs", "author": ["J.E. HOPCROFT", "R.M. KARP"], "venue": "SIAM Journal on Computing,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1973}, {"title": "On the definition of the causal relation", "author": ["H. SIMON"], "venue": "The Journal of Philosophy,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1952}, {"title": "Automated modelling of physical systems", "author": ["P.P. NAYAK"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1996}, {"title": "Constraint management in conceptual design. In: Knowledge Based Expert Systems in Engineering: Planning and Design", "author": ["D. SERRANO", "D.C. GOSSARD"], "venue": "Computational Mechanics Publications,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1987}, {"title": "Synthesizing third normal form relations from functional dependencies", "author": ["P. BERNSTEIN"], "venue": "ACM Trans. on Database Systems,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 1976}, {"title": "Computational problems related to the design of normal form relational schemas", "author": ["C. BEERI", "P. BERNSTEIN"], "venue": "ACM Trans. on Database Systems, v. 4,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 1979}, {"title": "Causality in databases", "author": ["A MELIOU"], "venue": "IEEE Data Eng. Bull., v. 33,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2010}, {"title": "The complexity of causality and responsibility for query answers and non-answers", "author": ["A. MELIOU", "W. GATTERBAUER", "K.F. MOORE", "D. SUCIU"], "venue": "PVLDB, v. 4,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2010}, {"title": "Sensitivity analysis and explanations for robust query evaluation in probabilistic databases", "author": ["B. KANAGAL", "J. LI", "A. DESHPANDE"], "venue": "Proc. of ACM SIGMOD", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2011}, {"title": "Reverse data management", "author": ["A. MELIOU", "W. GATTERBAUER", "D. SUCIU"], "venue": "PVLDB, v. 4,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2011}, {"title": "TANE: An efficient algorithm for discovering functional and approximate dependencies", "author": ["Y HUHTALA"], "venue": "Computer Journal, v. 42,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 1999}, {"title": "Modeling and reasoning with Bayesian Networks", "author": ["A. DARWICHE"], "venue": null, "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2009}, {"title": "Causal Networks: Semantics and expressiveness", "author": ["J.P. TOM S. VERMA"], "venue": "Proc. of the 4th Conf. on Uncertainty in Artificial Intelligence (UAI\u201988). North-Holland Publishing Co.,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1988}, {"title": "Schema design for uncertain databases", "author": ["A. Das Sarma", "J. ULLMAN", "J. WIDOM"], "venue": "Proc. of AMW", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2007}, {"title": "The decomposition versus synthetic approach to relational database design", "author": ["R. FAGIN"], "venue": "Proc. of VLDB", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 1977}, {"title": "Making database systems usable", "author": ["JAGADISH", "H. V"], "venue": "In: SIGMOD", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2007}, {"title": "CRIUS: User-friendly database design", "author": ["L. QIAN", "K. LEFEVRE", "H.V. JAGADISH"], "venue": "PVLDB, v. 4,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2010}, {"title": "Conditioning probabilistic databases", "author": ["C. KOCH", "D. OLTEANU"], "venue": "PVLDB, v. 1,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2008}, {"title": "Identifying physiological origins of baroreflex dysfunction in salt-sensitive hypertension in the Dahl SS rat", "author": ["S.M. BUGENHAGEN", "A.W.J. COWLEY", "D.A. BEARD"], "venue": "Physiological Genomics,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2010}, {"title": "The ten-year cycle in numbers of the lynx in Canada", "author": ["C. ELTON", "M. NICHOLSON"], "venue": "Journal of Animal Ecology, v. 11,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1942}, {"title": "Predicting essential components of signal transduction networks: A dynamic model of guard cell abscisic acid signaling", "author": ["S. LI", "S.M. ASSMANN", "R. ALBERT"], "venue": "PLOS Biology,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "In view of the paradigm shift that makes science ever more data-driven [1], in this thesis we demonstrate that large deterministic scientific hypotheses can be effectively encoded and managed as a kind of uncertain and probabilistic data.", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "Hypotheses can also be learned in large scale, as exhibited in the Eureqa project [2].", "startOffset": 82, "endOffset": 85}, {"referenceID": 2, "context": "In fact, we can refer nowadays to a broad, modern context of data science [3] and big data [4] in which the complexity and scale of so-called \u2018data-driven\u2019 problems require proper data management tools for the predicted data to be analyzed effectively.", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": "In fact, we can refer nowadays to a broad, modern context of data science [3] and big data [4] in which the complexity and scale of so-called \u2018data-driven\u2019 problems require proper data management tools for the predicted data to be analyzed effectively.", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": ") \u201ca rapidly growing multidisciplinary field that uses advanced computing capabilities to understand and solve complex problems\u201d [5].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "It is generally considered that computational science models, interpreted here as hypotheses to explain real-world phenomena, are of strategic relevance [5].", "startOffset": 153, "endOffset": 156}, {"referenceID": 5, "context": "Notorious examples are the John Hopkins Turbulance Databases [6], and the Human Brain Project (HBP) neuroscience simulation datasets [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "Notorious examples are the John Hopkins Turbulance Databases [6], and the Human Brain Project (HBP) neuroscience simulation datasets [7].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "Nonetheless, while the use case for exploratory analytics is currently well understood and many of its challenges have already been coped with so that high-resolution simulation data is increasingly more accessible [8, 9], only very recently, as part of this thesis work, the use case of hypothesis management has been taken into account for predictive analytics [10].", "startOffset": 215, "endOffset": 221}, {"referenceID": 8, "context": "Nonetheless, while the use case for exploratory analytics is currently well understood and many of its challenges have already been coped with so that high-resolution simulation data is increasingly more accessible [8, 9], only very recently, as part of this thesis work, the use case of hypothesis management has been taken into account for predictive analytics [10].", "startOffset": 215, "endOffset": 221}, {"referenceID": 9, "context": "In fact, there is a pressing call for innovative technology to integrate (observed) data and (simulated) theories in a unified framework [11, 12, 13].", "startOffset": 137, "endOffset": 149}, {"referenceID": 10, "context": "In fact, there is a pressing call for innovative technology to integrate (observed) data and (simulated) theories in a unified framework [11, 12, 13].", "startOffset": 137, "endOffset": 149}, {"referenceID": 11, "context": "In fact, there is a pressing call for innovative technology to integrate (observed) data and (simulated) theories in a unified framework [11, 12, 13].", "startOffset": 137, "endOffset": 149}, {"referenceID": 13, "context": "discovery) and predictive analytics (context of justification), and highlights the loop between hypothesis formulation and testing [15].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "Note that such a \u2018sampling\u2019 does not incur in any additional uncertainty as typical of statistical sampling [16].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "One of the state-of-the-art probabilistic data models is the U-relational representation system with its probabilistic world-set algebra (p-WSA) implemented in MayBMS [18].", "startOffset": 167, "endOffset": 171}, {"referenceID": 15, "context": "The first is the less systematic, as the user has to model for the data and correlations by steering all the p-DB construction process (MayBMS\u2019 use cases [18], e.", "startOffset": 154, "endOffset": 158}, {"referenceID": 16, "context": "[20, 21, 22]) that are basic input to algorithmic synthesis.", "startOffset": 0, "endOffset": 12}, {"referenceID": 17, "context": "[20, 21, 22]) that are basic input to algorithmic synthesis.", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "A related concept which is also a major one for us is that of normalization [20, 21, 22], viz.", "startOffset": 76, "endOffset": 88}, {"referenceID": 17, "context": "A related concept which is also a major one for us is that of normalization [20, 21, 22], viz.", "startOffset": 76, "endOffset": 88}, {"referenceID": 19, "context": "In fact, given a system of equations with a set of variables appearing in them, in a seminal article Simon introduced an asymmetrical, functional relation among variables that establishes a (so-called) causal ordering [24].", "startOffset": 218, "endOffset": 222}, {"referenceID": 20, "context": "also [25]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 21, "context": "Although we perform some sort of information extraction [26] for the acquisition of hypotheses from some model repositories on the web, it is very basic and ad-hoc in order to obtain a testbed for our method.", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "In fact, the performance of U-relations and p-WSA has been extensively evaluated and shown to be effective [27, 18].", "startOffset": 107, "endOffset": 115}, {"referenceID": 15, "context": "In fact, the performance of U-relations and p-WSA has been extensively evaluated and shown to be effective [27, 18].", "startOffset": 107, "endOffset": 115}, {"referenceID": 23, "context": "The innovative system of \u03a5-DB has been described in a \u2018system prototype demonstration\u2019 paper [28].", "startOffset": 93, "endOffset": 97}, {"referenceID": 24, "context": "Chapters 3, 4, and 5) are formulated into a formal method for the design of hypothesis p-DB\u2019s which is described in a technical report [29].", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "High-throughput technology and large-scale scientific experiments provide scientists with empirical data that has to be extracted, transformed and loaded before it is ready for analysis [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 13, "context": "Hypotheses, however, are tentative explanations of phenomena [15], which characterizes a different kind of uncertain data.", "startOffset": 61, "endOffset": 65}, {"referenceID": 1, "context": ", in the Eureqa project [2].", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "Scientific hypotheses are tested by way of their predictions [15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "However, for computing predictions, deterministic hypotheses are applied asymmetrically as functions [30].", "startOffset": 101, "endOffset": 105}, {"referenceID": 13, "context": "observable, but only their joint results (the predictions) [15].", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": ", [31, 32, 33]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 27, "context": ", [31, 32, 33]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 28, "context": ", [31, 32, 33]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 28, "context": "The Physiome project [33, 34], e.", "startOffset": 21, "endOffset": 29}, {"referenceID": 29, "context": "The Physiome project [33, 34], e.", "startOffset": 21, "endOffset": 29}, {"referenceID": 15, "context": "The vision of \u03a5-DB is currently set to be delivered on top of U-relations and probabilistic world-set algebra (p-WSA) [18].", "startOffset": 118, "endOffset": 122}, {"referenceID": 30, "context": ", [36]), uncertainty is usually seen as an undesirable property that hinders data quality.", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "This may be non-obvious but is quite convenient a design decision for the envisioned system of \u03a5-DB because hypotheses, as (abstract) universal statements [15], can only be derived predictions from (be empirically grounded) by assigning (callibrating) them onto some real-world phenomenon.", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "1, U-relations have in their schema a set of pairs (Vi, Di) of condition columns [18] to map each discrete random variable xi created by the repair-key operation to one of its possible values (e.", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "The 11 Hypotheses are \u2018universal\u2019 by definition [15].", "startOffset": 48, "endOffset": 52}, {"referenceID": 31, "context": "In the context of p-DB\u2019s [17], data cleaning does not have to be one-shot \u2014 which is more error-prone [37].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "Note that it abstracts the goal of a data-intensive hypothesis evaluation study, or the scientific method itself [15], as the repair of each \u03c6 as a key.", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "That is illustrated by query Q3, which creates integrative table Y [s]; and by query Q4, which computes the confidence aggregate operation [18] for all s tuples where t = 3 (Fig.", "startOffset": 139, "endOffset": 143}, {"referenceID": 14, "context": "Then, by applying Bayes\u2019 theorem for normal mean with a discrete prior [16], Prior is updated to Posterior (see Fig.", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "2) to get the posterior p(\u03bck | y) [16].", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": ", yn |\u03bck) for each competing trial \u03bck, is computed as a product \u220fn j=1 f(yj |\u03bckj) of the single likelihoods f(yj |\u03bckj) [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 32, "context": "The vision of managing hypotheses as data has some roots in Porto and Spaccapietra [38], who motivated a conceptual data model to support (the socalled) in silico science by means of a scientific model management system.", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "2 Scientific simulation data As previsouly mentioned, science\u2019s ETL is distinguished by its unfrequent, incremental-only updates and by having large raw files as data sources [8].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "enabling an efficient access to high-resolution, raw simulation data have been documented from both supercomputing,[6] and database research viewpoints;[39] and pointed as key to the use case of exploratory analytics.", "startOffset": 115, "endOffset": 118}, {"referenceID": 33, "context": "enabling an efficient access to high-resolution, raw simulation data have been documented from both supercomputing,[6] and database research viewpoints;[39] and pointed as key to the use case of exploratory analytics.", "startOffset": 152, "endOffset": 156}, {"referenceID": 5, "context": ", the \u2018immersive\u2019 query processing (move the program to the data) [6, 40], or \u2018in situ\u2019 query processing in the raw files [41, 42].", "startOffset": 66, "endOffset": 73}, {"referenceID": 34, "context": ", the \u2018immersive\u2019 query processing (move the program to the data) [6, 40], or \u2018in situ\u2019 query processing in the raw files [41, 42].", "startOffset": 66, "endOffset": 73}, {"referenceID": 35, "context": ", the \u2018immersive\u2019 query processing (move the program to the data) [6, 40], or \u2018in situ\u2019 query processing in the raw files [41, 42].", "startOffset": 122, "endOffset": 130}, {"referenceID": 35, "context": ", the loading) for a direct access to data \u2018in situ\u2019 in the raw data files [42].", "startOffset": 75, "endOffset": 79}, {"referenceID": 36, "context": "3 Hypothesis encoding Our framework is comparable with Bioinformatics\u2019 initiatives that address hypothesis encoding into the RDF data model [43]: (i) the Robot Scientist [44] is a knowledge-base system (KBS) for automated generation and testing of hypotheses 14 Sometimes phrased \u2018here is my files, here is my queries, where are my results?\u2019 [41].", "startOffset": 140, "endOffset": 144}, {"referenceID": 37, "context": "3 Hypothesis encoding Our framework is comparable with Bioinformatics\u2019 initiatives that address hypothesis encoding into the RDF data model [43]: (i) the Robot Scientist [44] is a knowledge-base system (KBS) for automated generation and testing of hypotheses 14 Sometimes phrased \u2018here is my files, here is my queries, where are my results?\u2019 [41].", "startOffset": 170, "endOffset": 174}, {"referenceID": 38, "context": "about what genes encode enzymes in the yeast organism; (ii) HyBrow [45] is a KBS for scientists to test their hypotheses about events of the galactose metabolism also of the yeast organism; and (iii) SWAN [46] is a KBS for scientists to share hypotheses on possible causes of the Alzheimer disease.", "startOffset": 67, "endOffset": 71}, {"referenceID": 39, "context": "about what genes encode enzymes in the yeast organism; (ii) HyBrow [45] is a KBS for scientists to test their hypotheses about events of the galactose metabolism also of the yeast organism; and (iii) SWAN [46] is a KBS for scientists to share hypotheses on possible causes of the Alzheimer disease.", "startOffset": 205, "endOffset": 209}, {"referenceID": 37, "context": "The Robot Scientist relies on rule-based logic programming analytics to automatically generate and test RDF-encoded hypotheses of the kind \u2018gene G has function A\u2019 against RDF-encoded empirical data [44].", "startOffset": 198, "endOffset": 202}, {"referenceID": 38, "context": "HyBrow is likewise, but hypotheses are formulated by the user about biological events [45].", "startOffset": 86, "endOffset": 90}, {"referenceID": 39, "context": "In particular, SWAN [46] differs from the former in that each hypothesis is unstructured, being then more related to efforts on the retrieval of textual claims from the narrative fabric of scientific reports [47].", "startOffset": 20, "endOffset": 24}, {"referenceID": 40, "context": "In particular, SWAN [46] differs from the former in that each hypothesis is unstructured, being then more related to efforts on the retrieval of textual claims from the narrative fabric of scientific reports [47].", "startOffset": 208, "endOffset": 212}, {"referenceID": 19, "context": "2 we study the problem of extracting the causal ordering implicit in the structure of a deterministic hypothesis and show that Simon\u2019s classical approach [24, 48] is intractable.", "startOffset": 154, "endOffset": 162}, {"referenceID": 41, "context": "2 we study the problem of extracting the causal ordering implicit in the structure of a deterministic hypothesis and show that Simon\u2019s classical approach [24, 48] is intractable.", "startOffset": 154, "endOffset": 162}, {"referenceID": 19, "context": "Given a system of mathematical equations involving a set of variables, to build a structural equation model (SEM) is, essentially, to establish a one-to-one mapping between equations and variables [24].", "startOffset": 197, "endOffset": 201}, {"referenceID": 19, "context": ", [24, 49, 48]) and adapt it for the encoding of hypotheses into fd\u2019s.", "startOffset": 2, "endOffset": 14}, {"referenceID": 41, "context": ", [24, 49, 48]) and adapt it for the encoding of hypotheses into fd\u2019s.", "startOffset": 2, "endOffset": 14}, {"referenceID": 19, "context": "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].", "startOffset": 79, "endOffset": 91}, {"referenceID": 20, "context": "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].", "startOffset": 79, "endOffset": 91}, {"referenceID": 41, "context": "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].", "startOffset": 79, "endOffset": 91}, {"referenceID": 19, "context": ", row multiplication by a constant) on the structure matrix may hinder the structure\u2019s causal ordering and then are not valid in general [24].", "startOffset": 137, "endOffset": 141}, {"referenceID": 19, "context": "[24]) that, given a complete structure S(E ,V), can be used to compute a partial causal mapping \u03c6p from partitions on the set of equations to same-cardinality partitions on the set of variables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "As shown by Dash and Druzdzel [48], the causal mapping returned by Simon\u2019s (socalled) Causal Ordering Algorithm (COA) is not total when S has variables that are strongly coupled (because they can only be determined simultaneously).", "startOffset": 30, "endOffset": 34}, {"referenceID": 41, "context": "They also have shown that any total mapping \u03c6 over S must be consistent with COA\u2019s partial mapping \u03c6p [48].", "startOffset": 102, "endOffset": 106}, {"referenceID": 43, "context": "line 3) is a hard problem that can only be addressed heuristically as a problem of co-clustering (also called biclustering [51, 52]) in", "startOffset": 123, "endOffset": 131}, {"referenceID": 44, "context": "line 3) is a hard problem that can only be addressed heuristically as a problem of co-clustering (also called biclustering [51, 52]) in", "startOffset": 123, "endOffset": 131}, {"referenceID": 45, "context": ", [54]) have come with the notion of pseudo-biclique (also called \u2018quasi-biclique\u2019), which is a relaxation of the biclique concept to allow some less rigid notion of connectivity than the \u2018complete connectivity\u2019 required in a biclique.", "startOffset": 2, "endOffset": 6}, {"referenceID": 46, "context": "Proof 1 We show (by restriction [55]) that the BPBP is a generalization of the balanced biclique problem (BBP), referred \u2018balanced complete bipartite subgraph\u2019 problem [55, GT24, p.", "startOffset": 32, "endOffset": 36}, {"referenceID": 41, "context": ", [48]) geared for reasoning over GM\u2019s.", "startOffset": 2, "endOffset": 6}, {"referenceID": 48, "context": "In this thesis we adopt the Hopcroft-Karp algorithm [57], which is known to be polynomial-time, bounded by O( \u221a |V1|+ |V2| |E|).", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "Then we consider a sense of Simon\u2019s into the nature of scientific modeling and interventions [24], summarized in Def.", "startOffset": 93, "endOffset": 97}, {"referenceID": 19, "context": "We draw attention to the significance of Theorem 2, as it sheds light on a connection between Simon\u2019s complete structures [24] and fd sets [20].", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "We draw attention to the significance of Theorem 2, as it sheds light on a connection between Simon\u2019s complete structures [24] and fd sets [20].", "startOffset": 139, "endOffset": 143}, {"referenceID": 49, "context": "[59]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[25, 19]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 41, "context": "Dash and Druzdzel revisit the problem and re-motivate it in light of modern applications [48].", "startOffset": 89, "endOffset": 93}, {"referenceID": 51, "context": "Inspired on Serrano and Gossard\u2019s work on constraint modeling and reasoning [61], Nayak reports an approach that is provably quite effective to process the causal ordering: extract a total causal mapping and then compute the transitive closure of the direct causal dependencies.", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "\u2022 By building upon on the work of Simon [24] and Nayak [49] (cf.", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "As usual notational conventions from the DB literature [20, 21], we write X, Y, Z to denote sets of relational attributes and A,B,C to denote singleton attribute sets.", "startOffset": 55, "endOffset": 63}, {"referenceID": 16, "context": "Functional dependency theory relies on Armstrong\u2019s inference rules (or axioms) of (R0) reflexivity, (R1) augmentation and (R2) transitivity, which forms a sound and complete inference system for reasoning over fd\u2019s [20].", "startOffset": 215, "endOffset": 219}, {"referenceID": 52, "context": "[62]), where \u03a3 and U are (resp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "yet unexplored problem in the database research literature (reasoning over fd\u2019s is extensively covered in Maier [22]).", "startOffset": 112, "endOffset": 116}, {"referenceID": 54, "context": "Recent years have seen the emergence of some foundational work in causality in databases [64].", "startOffset": 89, "endOffset": 93}, {"referenceID": 55, "context": "For conjunctive queries, the causality is said to be computed very efficiently [65].", "startOffset": 79, "endOffset": 83}, {"referenceID": 56, "context": "al is the so-called sensitivity analysis [66], which is aimed at establishing a more refined connection between the query answer (output) and elements of the DB instance (input) for supporting user interventions.", "startOffset": 41, "endOffset": 45}, {"referenceID": 57, "context": "This line of work is strongly related to the vision of \u2018reverse data management\u2019 [67].", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "Three remarkable features of U-relations are: expressiveness (being closed under positive relational algebra queries); succinctness (efficient storage of a very large number of possible worlds through vertical decompositions to support attributelevel uncertainty); and efficient query processing (including confidence computation) [18].", "startOffset": 331, "endOffset": 335}, {"referenceID": 15, "context": ", R i m, p [i] \u2208W is a possible world, with p being its probability [18].", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "Probabilistic world-set algebra (p-WSA) consists of the operations of relational algebra, an operation for computing tuple confidence conf, and the repairkey operation for introducing uncertainty \u2014 by giving rise to alternative worlds as maximal-subset repairs of an argument key [18].", "startOffset": 280, "endOffset": 284}, {"referenceID": 15, "context": "[18]) to map each discrete random variable xi to one of its possible values (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18]): the number of algebraic operations does not increase and each of the operations selection, projection and product/join remains of the same kind.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "For a comprehensive overview of U-relations and p-WSA we refer the reader to [18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "Hypotheses, nonetheless, are (abstract) \u2018universal statements\u2019 [15].", "startOffset": 63, "endOffset": 67}, {"referenceID": 58, "context": ", see [68]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 58, "context": "4 In short, we make use of relational algebra group-by operation and build a pruned lattice of attribute groups having the same number of rows under the grouping (similarly to [68]).", "startOffset": 176, "endOffset": 180}, {"referenceID": 16, "context": "Now, we provide the classical definition of a lossless join [20], i.", "startOffset": 60, "endOffset": 64}, {"referenceID": 59, "context": "Informed on research on Graphical Models (GM) [69], Suciu et al.", "startOffset": 46, "endOffset": 50}, {"referenceID": 60, "context": ", by using a set of axioms (the so-called \u2018graphoids\u2019) for reasoning about the probabilistic independence of variables [70].", "startOffset": 119, "endOffset": 123}, {"referenceID": 60, "context": "In fact, a connection between database normalization theory and factor decomposition in Graphical Models (GM) has been discussed by Verma and Pearl [70], but has not been explored since then.", "startOffset": 148, "endOffset": 152}, {"referenceID": 61, "context": "[71].", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "design [71].", "startOffset": 7, "endOffset": 11}, {"referenceID": 52, "context": "Despite some major differences, our synthesis method builds upon the classical theory of relational schema design by synthesis [62].", "startOffset": 127, "endOffset": 131}, {"referenceID": 52, "context": "Classical design by synthesis [62] was once criticized due to its too strong \u2018uniqueness\u2019 of fd\u2019s assumption [72, p.", "startOffset": 30, "endOffset": 34}, {"referenceID": 63, "context": "The last decade has seen significant research effort to make DB systems really usable [73].", "startOffset": 86, "endOffset": 90}, {"referenceID": 64, "context": "For instance, in comparison, the CRIUS system supports another kind of user-friendly DB design approach that provides users with a spreadsheet-like direct manipulation interface to increasingly add structure to their data [74].", "startOffset": 222, "endOffset": 226}, {"referenceID": 65, "context": "It has been firstly addressed by Koch and Olteanu motivated by data cleaning applications [75].", "startOffset": 90, "endOffset": 94}, {"referenceID": 29, "context": "The Physiome project is an initiative to seriously address the problems of reproducibility, model integration and sharing in Computational Physiology [34, 33].", "startOffset": 150, "endOffset": 158}, {"referenceID": 28, "context": "The Physiome project is an initiative to seriously address the problems of reproducibility, model integration and sharing in Computational Physiology [34, 33].", "startOffset": 150, "endOffset": 158}, {"referenceID": 66, "context": "8 shows the best fit of a baroreflex model for an observational dataset acquired by experiment on Dahl SS rat [76].", "startOffset": 110, "endOffset": 114}, {"referenceID": 66, "context": "(source: [76]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 28, "context": "and molecular levels [33].", "startOffset": 21, "endOffset": 25}, {"referenceID": 23, "context": "[28]), in which we go through the whole design-by-synthesis pipeline (Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 67, "context": "20 shows observational data collected from Hudson\u2019s Bay from 1900 to 1920 on the Lynx-Hare population [77].", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "The efficiency and scalability of the U-relational representation system and its p-WSA query algebra have been extensively demonstrated [27].", "startOffset": 136, "endOffset": 140}, {"referenceID": 1, "context": ", learning the equations, say, from Eureqa [2].", "startOffset": 43, "endOffset": 46}, {"referenceID": 68, "context": "31) published in [78].", "startOffset": 17, "endOffset": 21}, {"referenceID": 68, "context": "Example of Boolean Network model (source: [78]).", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "All these are technical means to [15]: (1\u2032) extract the hypothesis \u2018empirical content\u2019 and \u2018predictive power;\u2019 (2\u2032) unravel its cohesiveness and how parsimonious it is in terms of the number of different claims or epistemological units carried within it, as well as its empirical grounding (\u2018first causes\u2019); and finally, we shall be able to (3\u2032) appraise it in face of competing or alternative explanations.", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "This thesis addresses the pressing call for large-scale, data-driven hypothesis management and analytics [35, 3, 10].", "startOffset": 105, "endOffset": 116}, {"referenceID": 24, "context": "[10, 29, 28]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 23, "context": "[10, 29, 28]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": ", multi-valued dependencies [20]), approximate fd\u2019s [68], conditional fd\u2019s [79]) to extend the scope of \u03a5-DB towards structured stochastic models.", "startOffset": 28, "endOffset": 32}, {"referenceID": 58, "context": ", multi-valued dependencies [20]), approximate fd\u2019s [68], conditional fd\u2019s [79]) to extend the scope of \u03a5-DB towards structured stochastic models.", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "As envisioned by Jim Gray [1], the scientific method has been shifting towards being operated as a data-driven discipline which is rapidly gaining ground [3].", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "As envisioned by Jim Gray [1], the scientific method has been shifting towards being operated as a data-driven discipline which is rapidly gaining ground [3].", "startOffset": 154, "endOffset": 157}], "year": 2015, "abstractText": "of Thesis presented to LNCC/MCT in partial fulfillment of the requirements for the degree of Doctor of Sciences (D.Sc.) MANAGING LARGE-SCALE SCIENTIFIC HYPOTHESES AS UNCERTAIN AND PROBABILISTIC DATA Bernardo Gon\u00e7alves February 2015 Advisor: Fabio Porto, D.Sc. In view of the paradigm shift that makes science ever more data-driven, in this thesis we propose a synthesis method for encoding and managing large-scale deterministic scientific hypotheses as uncertain and probabilistic data. In the form of mathematical equations, hypotheses symmetrically relate aspects of the studied phenomena. For computing predictions, however, deterministic hypotheses can be abstracted as functions. We build upon Simon\u2019s notion of structural equations in order to efficiently extract the (so-called) causal ordering between variables, implicit in a hypothesis structure (set of mathematical equations). We show how to process the hypothesis predictive structure effectively through original algorithms for encoding it into a set of functional dependencies (fd\u2019s) and then performing causal reasoning in terms of acyclic pseudo-transitive reasoning over fd\u2019s. Such reasoning reveals important causal dependencies implicit in the hypothesis predictive data and guide our synthesis of a probabilistic database. Like in the field of graphical models in AI, such a probabilistic database should be normalized so that the uncertainty arisen from competing hypotheses is decomposed into factors and propagated properly onto predictive data by recovering its joint probability distribution through a lossless join. That is motivated as a design-theoretic principle for data-driven hypothesis management and predictive analytics. The method is applicable to both quantitative and qualitative deterministic hypotheses and demonstrated in realistic use cases from computational science. Resumo da Tese apresentada ao LNCC/MCT como parte dos requisitos necess\u00e1rios para a obten\u00e7\u00e3o do grau de Doutor em Ci\u00eancias (D.Sc.) GER\u00caNCIA DE HIP\u00d3TESES CIENT\u00cdFICAS DE LARGA-ESCALA COMO DADOS INCERTOS E PROBABIL\u00cdSTICOS Bernardo Gon\u00e7alves Fevereiro, 2015 Orientador: Fabio Porto, D.Sc. Tendo em vista a mudan\u00e7a de paradigma que faz da ci\u00eancia cada vez mais guiada por dados, nesta tese propomos um m\u00e9todo para codifica\u00e7\u00e3o e ger\u00eancia de hip\u00f3teses cient\u0301\u0131ficas determi\u0144\u0131sticas de larga escala como dados incertos e probabi\u013a\u0131sticos. Na forma de equa\u00e7\u00f5es matem\u00e1ticas, hip\u00f3teses relacionam simetricamente aspectos do fen\u00f4meno de estudo. Para computa\u00e7\u00e3o de predi\u00e7\u00f5es, no entanto, hip\u00f3teses determi\u0144\u0131sticas podem ser abstr\u00e1\u0131das como fun\u00e7\u00f5es. Levamos adiante a no\u00e7\u00e3o de Simon de equa\u00e7\u00f5es estruturais para extrair de forma eficiente a ent\u00e3o chamada ordena\u00e7\u00e3o causal imp\u013a\u0131cita na estrutura de uma hip\u00f3tese. Mostramos como processar a estrutura preditiva de uma hip\u00f3tese atrav\u00e9s de algoritmos originais para sua codifica\u00e7\u00e3o como um conjunto de depend\u00eancias funcionais (df\u2019s) e ent\u00e3o realizamos infer\u00eancia causal em termos de racio\u0107\u0131nio a\u0107\u0131clico pseudo-transitivo sobre df\u2019s. Tal racio\u0107\u0131nio revela importantes depend\u00eancias causais imp\u013a\u0131citas nos dados preditivos da hip\u00f3tese, que conduzem nossa \u015b\u0131ntese do banco de dados probabi\u013a\u0131stico. Como na \u00e1rea de modelos gr\u00e1ficos (IA), o banco de dados probabi\u013a\u0131stico deve ser normalizado de tal forma que a incerteza oriunda de hip\u00f3teses alternativas seja decomposta em fatores e propagada propriamente recuperando sua distribui\u00e7\u00e3o de probabilidade conjunta via jun\u00e7\u00e3o \u2018lossless.\u2019 Isso \u00e9 motivado como um prin\u0107\u0131pio te\u00f3rico de projeto para ger\u00eancia e an\u00e1lise de hip\u00f3teses. O m\u00e9todo proposto \u00e9 aplic\u00e1vel a hip\u00f3teses determi\u0144\u0131sticas quantitativas e qualitativas e \u00e9 demonstrado em casos rea\u013a\u0131sticos de ci\u00eancia computacional.", "creator": "LaTeX with hyperref package"}}}