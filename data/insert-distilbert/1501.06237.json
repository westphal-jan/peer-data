{"id": "1501.06237", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jan-2015", "title": "Deep Transductive Semi-supervised Maximum Margin Clustering", "abstract": "semi - direct supervised clustering is an very important topic in machine learning and sophisticated computer interactive vision. the only key challenge of this problem is how to learn a given metric, such that the analyzed instances sharing the same label object are more likely close to observing each other expenditure on modeling the embedded space. currently however, little attention has been paid to learn better representations when constructing the data lie on non - linear manifold. fortunately, deep learning has led me to great success on feature learning recently. inspired by the advances of deep learning, we propose a deep field transductive semi - supervised maximum margin algorithm clustering approach. more specifically, given pairwise constraints, we exploit essentially both labeled and unlabeled data to learn a non - local linear mapping under maximum margin framework for clustering behavior analysis. thus, our model theory unifies dual transductive learning, feature learning and maximum margin techniques in the semi - supervised clustering framework. we pretrain the deep network edge structure with restricted boltzmann machines ( rbms ) layer by layer greedily, and optimize our objective function with gradient descent. by checking the most violated constraints, our approach updates the model defining parameters through error backpropagation, in which deep features are learned automatically. documenting the raw experimental results shows that our model is significantly better than the state of the art on semi - supervised graph clustering.", "histories": [["v1", "Mon, 26 Jan 2015 02:28:18 GMT  (58kb,D)", "http://arxiv.org/abs/1501.06237v1", "14"]], "COMMENTS": "14", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gang chen"], "accepted": false, "id": "1501.06237"}, "pdf": {"name": "1501.06237.pdf", "metadata": {"source": "CRF", "title": "Deep Transductive Semi-supervised Maximum Margin Clustering", "authors": ["Gang Chen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this paper, we investigate the semi-supervised clustering with side information in the form of pairwise constraints. In general, a pairwise constraint between two examples indicates whether they belong to the same cluster or not, which provides the supervision information: a same-label (or must-link) constraint denotes that the pair of instances should be partitioned into the same cluster, while a different-label (or cannot-link) constraint specifies that the pair of instances should be assigned into different clusters [32, 23, 30].\nSemi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34]. On the one hand, it is relatively easy to decide whether two items are similar or not from human in the loop because it often involves little effort from users. On the other hand, the maximum margin techniques have shown promising performance on classification tasks, and thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34]. In general, traditional semi-supervised clustering approaches either learn a distance metric based on the pairwise constraints, or leverage discriminative methods, such as k-nearest neighbor (kNN) and support vector machines (SVM) for better clustering performance. However, to collapse examples that belong to the same cluster approximately into a single point cannot always be achieved with simple linear transformations, especially when the data lie on an non-linear manifold. Although kernel methods are widely used for non-linear cases, it is a shallow approach and needs to specify hyper parameters in most situations [27, 30]. Fortunately, recent advances in\nar X\niv :1\n50 1.\n06 23\n7v 1\n[ cs\n.L G\n] 2\n6 Ja\nn 20\nthe training of deep networks provide a way to learn non-linear transformations of data, which are useful for supervised/unsupervised tasks [8, 2].\nInspired by feature learning [12, 28, 2], we propose a deep transductive semi-supervised clustering approach, which inherits both advantages from deep learning and maximum margin methods. Our method can learn features automatically from observation, kind of learning a metric as in [31]. However, unlike the linear mapping, e.g. Mahalanobis metric [30, 34], our method can learn a non-linear manifold representation, which is helpful for clustering and classification [2]. With the learned features as the input to the semi-supervised maximum margin clustering framework, we can learn the clustering weights. To leverage the unlabeled data, we also incorporate transductive learning to improve the clustering analysis. Through backpropagation, our approach can learn discriminative features via maximum margin techniques. Hence, our model unifies maximum margin, semi-supervised information and deep learning in an joint framework. We pre-train our model with stacked RBMs for feature representations firstly. And then we compute the gradient w.r.t. parameters and optimize our objective function in an alternative manner: data representation and model weights optimization with gradient descent. We test our model over a bunch of data sets and show that it yields accuracy significantly better than the state of the art.\nThe outline of this paper is as follows. In Section 2, we review the related work. Then, we present the model in Section 3. Section 4 present results of our experiments with the new techniques on a few widely used data sets. Finally we conclude the paper."}, {"heading": "2 Related work", "text": "The semi-supervised clustering with partial labels generally explores two directions to improve performance: (1) leverage more sophisticated classification models, such as maximum margin techniques [25, 30]; (2) learn a better distance metric [23, 30].\nThe maximum margin clustering (MMC) aims to find the hyperplanes that can partition the data into different clusters over all possible labels with large margins [33, 26, 35]. Nevertheless, the accuracy of the clustering results by MMC may not be good sometimes due to the nature of its unsupervised learning [14]. Thus, it is interested to incorporate semi-supervised information, e.g. the pairwise constraints, into the recently proposed maximum margin clustering framework. Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3]. In particular, COPKmeans [11] is a semi-supervised variant of Kmeans, by following the same clustering procedure of Kmeans while avoiding violations of pairwise constraints. MPCKmeans [3] extended Kmeans and utilized both metric learning and pairwise constraints in the clustering process. More recently, [20] show that they can improve classification with pairwise constraints under maximum margin framework. [34] leverage the margin-based approach on the semi-supervised clustering problems, and yield competitive results.\nHow to learn a good metric over input space is critical for a successful semi-supervised clustering approach. Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs. The pseudo-metric [23] parameterized by positive semi-definite matrices (PSD) is learned with an online updating rule, that alternates between projections onto PSD and onto half-space constraints imposed by the instance pairs. [32] proposed to learn a distance metric (Mahalanobis) that respects pairwise constraints for clustering. In [6], an information-theoretic approach to learning a Mahalanobis distance function via LogDet divergence is proposed. Recently, a supervised approach to learn Mahalanobis metric is also proposed in [30], by minimizing the pairwise distances between instances in the same cluster, while increasing the separation between data points with dissimilar classes. To handle the data that lies on non-linear manifolds, kernel methods are widely used. Unfortunately, these non-linear embedding algorithms for use is shallow methods.\nOn the other hand, recent advances in deep learning [12, 28, 2] have sparked great interest in dimension\nreduction [13, 31] and classification problems [12, 19]. In a sense, the success of deep learning lies on learned features, which are useful for supervised/unsupervised tasks [8, 2]. For example, the binary hidden units in the discriminative Restricted Boltzmann Machines (RBMs) [18, 9] can model latent features of the data that improve classification. The deep learning for semi-supervised embedding [31] extends shallow semi-supervised learning techniques such as kernel methods with deep neural networks, and yield promising results. The work of [24] is most related to our proposed algorithm. It presented deep learning with support vector machines, which can learn features under discriminative learning framework automatically with labeled data. However, their approach is totally supervised and for classification problems, while our model is for semi-supervised clustering problems. Compared to conventional methods, our model consider both feature learning and transductive principles in our semi-supervised clustering model, so that it can handles complex data distribution and learns a better non-linear mapping to improve clustering performance."}, {"heading": "3 Deep Transductive Semi-supervised Maximum Margin Cluster-", "text": "ing\nIn this section, we will introduce the transductive semi-supervised maximum margin clustering, with deep features learned simultaneously in an unified framework."}, {"heading": "3.1 Overview of our approach", "text": "Let X = {xi}Ni=1 (xi \u2208 RD) be a set of N examples, which belongs to K clusters called Z. In addition to the unlabeled data, there is additional partially labeled data in the form of pairwise constraints C = {(xi,xj , \u03b4(zi = zj)}, which is a kind of side information to provide whether the two instances (xi,xj) are from the same cluster or not (indicated by the \u03b4 function). Most methods attempt to learn weights wk \u2208 RD, for each cluster k = [1,K], to make these constraints satisfied as much as possible.\nInstead of learning a linear mapping or Mahalanobis metric [23, 30], we are interested in a non-linear mapping function. To make it easy to understand, suppose we have learned a nonlinear mapping function f : RD \u2192 Rd. Then, for each instance x \u2208 X , we can get its embedding code h = f(x) (note that the pairwise constrains also are kept in the coding space). Then given the learned features h, we leverage semi-supervised maximum margin clustering to partition the data. Just like the multi-class classification problems [25], we use the joint feature representation \u03a6(h, z) for each (h, z) \u2208 X \u00d7 Z\n\u03a6(h, z) =  h \u00b7 \u03b4(z = 1)\u00b7 \u00b7 \u00b7 h \u00b7 \u03b4(z = K)  (1) where \u03b4 is the indicator function (1 if the equation holds, otherwise 0). Correspondently, the hyperplanes for the K clusters can be parameterized by the weight vector W \u2208 R(K\u00d7d)\u00d71, which is the concatenation of weights wk, for k = {1, ...,K}. In other words, W[(k \u2212 1) \u00d7 d + 1 : k \u00d7 d] = wk. The clustering of testing examples is done in the same manner as the multiclass SVM [25],\nmax z\u2208[1,K]\nWT\u03a6(f(x), z) (2)\nFor inference, we first project data into hidden space with function f and then do clustering analysis. The problem left is how to learn the weight parameter W and the projection function f ."}, {"heading": "3.2 Objective function", "text": "We would like to extend semi-supervised clustering with deep feature learning. Deep learning consists of learning a model with several layers of non-linear mapping. As mentioned before, h \u2208 Rd is the mapping code with function f , which is non-linear mappings defined with L-layers neural network, s.t.\nhi = f(xi) = fL \u25e6 fL\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f1\ufe38 \ufe37\ufe37 \ufe38 L times (xi) (3)\nwhere \u25e6 indicates the function composition, and fl is logistic function with the weight parameter \u03b8l respectively for each layer l = {1, .., L}, refer further to Sec. 3.3 for more details. With a little abuse of symbols, for any input x, If we denote the output of the l-th layer as f1\u2192l(x), then we can get h = f1\u2192L(x).\nIn a similar manner as in [20, 34], we will incorporate the pairwise constraint information into the marginbased clustering framework. In addition, we leverage the unlabeled data to separate clusters in large margins, by following transductive learning. Specifically, given the pairwise constraint set C = {(xi,xj , \u03b4(zi = zj))}, we first project the dataset X into embedded space and minimize the following transductive semi-supervised objective function\nmin W,\u0398\n\u03bb 2 ||W||2 + 1 n+ \u2211 i \u03b7+i + 1 n\u2212 \u2211 j \u03b7\u2212j + \u03b2 UK \u2211 i\u2208U \u03bei (4)\ns.t.\n\u2200si1, si2 \u2208 Z, si1 6= si2; if (hi1,hi2, \u03b4(zi1, zi2)) \u2208 C+\nmax zi1=zi2\nWT\u03a6(hi1,hi2, zi1, zi2)\u2212\nWT\u03a6(hi1,hi2, si1, si2) \u2265 1\u2212 \u03b7i, \u03b7i \u2265 0 (5) \u2200sj1, sj2 \u2208 Z, sj1 = sj2; if (hj1,hj2, \u03b4(zj1, zj2)) \u2208 C\u2212\nmax zj1 6=zj2\nWT\u03a6(hj1,hj2, zj1, zj2)\u2212\nWT\u03a6(hj1,hj2, sj1, sj2) \u2265 1\u2212 \u03b7j , \u03b7j \u2265 0 (6) \u2200i \u2208 U,\u2200si 6= zi \u2208 Z max zi WT\u03a6(hi, zi)\u2212WT\u03a6(hi, si) \u2265 1\u2212 \u03bei (7)\nwhere W is the clustering weight in the over the learned feature space, \u0398 = {\u03b8l}Ll=1 are the weights for each layer in the deep architecture, and hi is the mapping code from xi via Eq. 3; C\n+ = {(hi,hj , \u03b4(zi = zj))|zi = zj} are the same label pairs, with the total number of pairwise constraints n+ = |C+|, C\u2212 = {(hi,hj , \u03b4(zi = zj))|zi 6= zj} are different-label pairs, with n\u2212 = |C\u2212|. U is the number of the unlabeled data (instances), not belong to any pairwise constrains. For convenience, we define \u03a6(hi,hj , zi, zj) = \u03a6(hi, zi) + \u03a6(hj , zj), which means the mapping of a pairwise constraint as the sum of the individual example-label mappings. The multi-layers non-linear mapping function f projects xi into hi, for i \u2208 [1, N ]. Instead of a linear mapping, the advantage of using a deep network to parametrize the function f is that a multi-layer network is better at learning a non-linear function that is presumably required to collapse classes in the latent space, in particular when the data consists of very complex non-linear structures.\nEqs. 5 and 6 specify the conditions that need to be satisfied, which means that the score for the most possible assigning scheme satisfying the constraints should be greater than that for any other assigning scheme with large margins. More specifically, for any pair (hi,hj , 1) \u2208 C+, it requires that the largest score for assigning (hi,hj) into the same cluster should be greater than that for assigning the pair into different clusters by at least 1 (soft margin can be applied here too). Analogously, for any dissimilar pair (hi,hj , 0) \u2208 C\u2212, the score that they are assigned into the most two different clusters should be greater than that for partitioning them into the same cluster.\nEq. 7 is from the principles of transductive learning, which indicates that the score of the most assigned cluster label is greater at least 1 than that of the runner up from the rest clusters.\nThe constrained optimization problem in Eq. 4 is hard to solve because the first inequality Eq. 5 and the second inequality Eq. 6 impose all the possible combinations of two clusters for each pairwise constraint. Thus, we transform it into the following equivalent unconstrained function which it is generally easier to solve\nmin \u03bb\n2 ||W||2\n+ 1\nn+\n{ 1\u2212 [ max zi1=zi1\n(hi1,hi2,1)\u2208C+ WT\u03a6(hi1,hi2, zi1, zi2)\u2212\nmax si1 6=si1\nWT\u03a6(hi1,hi2, si1, si2) ]} +\n(8a)\n+ 1\nn\u2212\n{ 1\u2212 [ max zj1 6=zj2\n(hj1,hj2,0)\u2208C\u2212\nWT\u03a6(hj1,hj2, zj1, zj2)\u2212\nmax sj1=sj2\nWT\u03a6(hj1,hj2, sj1, sj2) ]} +\n(8b)\n+ \u03b2\nUK \u2211 i\u2208U { 1\u2212 [max zi WT\u03a6(hi, zi)\u2212 max si 6=zi WT\u03a6(hi, si)] } +\n(8c)\nwhere {x}+ = max(x, 0) and hi is the projected code of xi using Eq. 3. The formula 8a specifies the condition that need to be satisfied for the same label pairwise constrains, while formula 8b denotes the conditions for different-label pairs. The last equation is corresponding to transductive constraints in Eq. 4.\nIn the objective function, we need to estimate the parameters, the weight W, as well as the weights \u03b8l for each layer l \u2208 [1, L] in the deep network. From the objective function, we can compute the gradients w.r.t. W and \u03b8l for l \u2208 [1, L] (via backpropagation) respectively, and gradient-based methods can be used to optimize it. Note that h (we ignore the subscript for convenience) in the objective function Eq. 8 is the non-linear embedding code from x. Thus, the objective function is not convex anymore, and we can only find a local minimum. In practice, we find L-BFGS cannot work well and easily trap into a bad local minimum. In our work, we use (sub)gradient descent to optimize the objective function, by projecting the training data with f and optimizing the objective function in an alternative manner."}, {"heading": "3.3 Parameter learning", "text": "We learn the parameters in an alternative manner: (1) data projection, given the model parameters; (2) and then optimize model parameters with gradient descent. To compute the gradients of the parameters, we need to find the most violated constraints first. For the same label pairs, we have the following most violated set:\nA+ = { (hi,hj , \u03b4(zi1 = zi2)) \u2208 C+| max\nzi1=zi1 WT\u03a6(hi1,hi2, zi1, zi2)\u2212 max si1 6=si1 WT\u03a6(hi1,hi2, si1, si2) < 1\n} (9)\nFor the different-label pairs, we denote the most violated set as\nA\u2212 = { (hi,hj , \u03b4(zj1 = zj2)) \u2208 C\u2212| max\nzj1 6=zj2 WT\u03a6(hj1,hj2, zj1, zj2)\u2212 max sj1=sj2 WT\u03a6(hj1,hj2, sj1, sj2) < 1 } (10)\nThen, we compute the gradient w.r.t. W\ndW = \u03bbW+\n\u2212 1 n+ \u2211 (hi1,hi2,1)\u2208A+ [ \u03a6(hi1,hi2, z + i1, z + i2)\u2212 \u03a6(hi1,hi2, z \u2212 i1, z \u2212 i2) ]\n\u2212 1 n\u2212 \u2211 (hj1,hj2,0)\u2208A\u2212 [ \u03a6(hj1,hj2, z \u2212 j1, z \u2212 j2)\u2212 \u03a6(hj1,hj2, z + j1, z + j2) ]\n\u2212 \u2211 i\u2208U \u03b2 UK [ \u03a6(hi, z + i )\u2212 \u03a6(hi, s + i ) ] , (11)\nwhere (z+i1, z + i2) = maxzi1=zi2 W T\u03a6(hi1,hi2, zi1, zi2), (z \u2212 i1, z \u2212 i2) = maxzi1 6=zi1 W T\u03a6(hi1,hi2, zi1, zi2); and for the unlabeled set z+i = maxzi \u03a6(hi, zi) and s + i = maxsi 6=z+i \u03a6(hi, si)\nIn order to learn discriminative features, we also need to estimate the weights in the multi-layer network. Note that for each pair (hi,hj), if it violates the constraints in Eqs. 5 and 6, then we can compute the gradient w.r.t. hi and hj respectively, which will be used to calculate the gradients of \u03b8l for l \u2208 [1, L] in the deep network. We use H = [h1,h2, ...,hN ] as the concatenation of all the hidden codes, where H \u2208 Rd\u00d7N , with each column H(:, i) = hi.\nFor the positive pairs, we have\ndhi1 = \u2212 1\nn+ \u2211 i1 [Wz+i1 \u2212Wz\u2212i1 ] (12a)\ndhi2 = \u2212 1\nn+ \u2211 i2 [Wz+i1 \u2212Wz\u2212i2 ] (12b)\nwhere Wz+i1 indicates the weight vector corresponding to the cluster label z+i1 in the whole weight matrix W. More specifically, Wz+i1 = W[(z+i1 \u2212 1)\u00d7 d+ 1 : z + i1 \u00d7 d]\nFor the negative pairs, we can get\ndhj1 = \u2212 1\nn\u2212 \u2211 j1 [Wz\u2212j1 \u2212Wz+j1 ] (13a)\ndhj2 = \u2212 1\nn\u2212 \u2211 j2 [Wz\u2212j2 \u2212Wz+j1 ] (13b)\nFor the unlabeled instances, we have\ndhi = \u2212 \u03b2\nUK \u2211 i [Wz+i \u2212Ws+i ] (14)\nGiven the gradient of dhi for each hidden code, we can get the gradient w.r.t. H as\ndH(:, i) = dhi (15)\nwhere dhi can be calculated according to Eqs. 12 and 13. Then, we can calculate the gradients w.r.t. lower level weights with back-propagation. For example d\u03b8L = dH \u00d7 ( f1\u2192L(X ) \u00b7 (1 \u2212 f1\u2192L(X )) ) , where \u00d7 representas matrix multiplication, and \u00b7 indicates pointwise product.\nInitialization: We used stacked RBMs to initialize the weights layer by layer greedily in the deep network, with contrastive divergence [12] (we used CD-1 in our experiments). Note that we used gaussian RBMs for the continuous data in the first layer, otherwise we used binary RBMs.\nAlgorithm 1\n1: Input: the training data X , pairwise constraints C, the number of clusters K, the number of iterations T , \u03bb, and \u03b2; 2: Initialize W; 3: Initialize wl for l = {1, ..., L} layer-by-layer greedily; 4: for i = 1; i <= T ; i+ + do 5: if the objective in Eq. 8 has no significant changes, break; 6: project all training data X into latent space via Eq. 3; 7: find the most violated constrains according to Eqs. 9 and 10 8: compute the gradient w.r.t. W via Eq. 11; 9: compute the gradient w.r.t. H via Eq. 15;\n10: compute the gradient w.r.t. \u0398 = {\u03b8l}Ll=1 with backpropagation; 11: update the parameters with gradient descent via Eq. 16; 12: end for 13: Return model parameters W and {wl}Ll=1, as well as average accuracy;\nIn our deep model, the weights from the layers 1 to L are \u03b8l respectively, for l = {1, .., L}, and the top layer L has weight \u03b8L. We first pre-train the L-layer deep structure with RBMs layer by layer greedily. Thus, our deep network can learn parametric nonlinear mapping from input x to output h, f : x \u2192 h. Specifically, we think RBM is a 1-layer deep network, with weight \u03b81. For example, for 1-layer DBN, we have h = f1(x) = logistic(\u03b8 T 1 [x, 1]), where we extend x \u2208 RD into [x, 1] \u2208 R(D+1) in order to handle bias in the non-linear mapping. Given the output of the current layer as the input to the next layer, we can learn each layer weight greedily.\nAs for the clustering weight W, we take a similar strategy as in [34] to initialize it.\nParameter updating: In our model, we use the gradient descent to update the model parameters. We also tried L-BFGS [4, 22] to update model parameters, but it did not perform well. In our model, we can update the model parameters as follows,\nW\u2190W \u2212 \u03b3WdW, \u03b8l \u2190 \u03b8l \u2212 \u03b3\u03b8ld\u03b8l, l \u2208 {1, ..., L} (16)\nwhere \u03b3W is the learning rate for the clustering weight W, and \u03b3\u03b8l is the learning rate for weights \u03b8l in the deep neural network. Thus, our method alternates between data projection and parameter optimization. For more details, refer to algorithm 1.\nAfter we learned the model parameters, we can do cluster analysis according to Eq. 2."}, {"heading": "4 Experiments", "text": "In this section, we presented a set of experiments comparing our method to the state of the art semi-supervised clustering methods on a wide range of data sets, including UCI data sets and the Reuters dataset in Table 1, as well as the MNIST digits, COIL-20 and COIL-100 datasets. We also evaluated whether the transductive constraint in Eq. (7) is helpful or not in the clustering analysis."}, {"heading": "4.1 Experimental setup", "text": "In the experiments, we compared our method to the state of the art semi-supervised clustering approaches, including Xing [32], ITML [6], KISSME [17] and CMMC [34]. Note that Xing, ITML and KISSME are the\nsemi-supervised approaches for metric learning (Mahalanobis). Thus, we used those methods to learn the metric and calculate the distances between all instances, then we used the kernel k-means [7] for clustering. Our method and CMMC are similar, which can be directly optimized for clustering.\nAs for parameter setting, we set \u03bb = 0.02 and \u03b2 = 1. The learning rate \u03b3W decreases in the iterations in our model, by setting \u03b3W = 1 \u03bb\u00d7(i+1) , where i is the index for iterations; while the learning rate for weighs in the deep network fixed, with \u03b3\u03b8l = 0.01, for l = {1, ..., L}. Without other specification, our model used the one hidden layer with 100 units on most data sets, except on the MNIST and UCI data sets.\nWe tested our method on two tasks: pairwise classification and clustering analysis. As for pairwise classification, we randomly sampled 200 pairs of constraints (around 100 must-links and 100 cannot links), of which we used 100 pairwise constraints (50 must-links and 50 cannot links) as the training set, and the rest 100 pairs as the testing sets. Then we used the receiver operating characteristic (ROC) to evaluate the performance.\nAs for the clustering analysis, we used the pairwise constraints sampled to train our model, then we use the learned model for clustering analysis. We used the accuracy (the most possible matching between the obtained labels and the original true labels, refer to [34]) and adjusted Rand Index [15, 21] to evaluate our method in all the experiments."}, {"heading": "4.2 Results", "text": "UCI data sets: In the experiment, we selected the five widely used data sets from the UCI machine learning repository1, which has different dimension and categories, shown in Table 1. As for the number of hidden units in our model, we set the number of hidden nodes to be 100 on the sonar data set and 64 on the other UCI data sets. For the each data set, we randomly sampled 200 pairwise constraints, of which 100 pairs were used for training and the rest to test the pairwise classification performance. While for clustering performance, we test the model on all the data elements. We compared our method to the state of the art methods, and clustering results are shown in Table. (2). It demonstrates that our method outperforms other\n1https://archive.ics.uci.edu/ml/datasets.html\nmethods on almost all the data sets, especially for the data with the larger number of classes. We also show the performance of our method on the pairwise classification task in Fig. 1. Except on the Wdbc and glass data sets, our method yields completive and even better results than other methods.\nReuters data set: We used the Reuters215782, which has the total 8293 documents with 18933 dimensional features for each document, belonging to 65 categories. Because the Reuters data set has high dimension, we first projected it into 400 dimensions with PCA. Then we set the number of hidden nodes to be 100 in our model. The clustering performance is shown in Table. (2). It demonstrates that our method is significantly better than other methods. Again, our method yields remarkably better pairwise classification result, shown in the right bottom of Fig. 1.\n2http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html\nMNIST dataset: The MNIST digits3 consists of 28\u00d7 28-size images of handwriting digits from 0 through 9 with a training set of 60,000 examples and a test set of 10,000 examples, and has been widely used to test character recognition methods. In the experiment, we randomly sampled a subset with 5000 images from the training sets to test our method and other baselines. In the experiment, we use a three-layer deep structure for MNIST digits, with hidden nodes [400 200 100] respectively on each layer. We tested how the clustering performance changes when the number of pairwise constraints varies. The experimental comparisons between our method and other baselines are shown in Fig. 2. It demonstrates that the clustering accuracy is increasing with more pairwise constraints. And it also shows our method is better than other baselines in most cases when varying the number of training pairs.\nTo evaluate whether the transductive constraint in our model in Eq. 4 is helpful or not for clustering, we set \u03b2 = 0 to get rid of the transductive condition in Eq. 7, and the experimental results are shown in Fig. 3. We argue that the result in Fig. 3 is consistent with common sense. The smaller the number of pairwise constraints, the higher uncertainty when we do inference. Thus, transductive learning has no advantage when the number of constraints is small. But it performs better with more constraints in Fig. 3. When more and more pairwise constraints are available, there\u2019s no need to incorporate transductive principles in the model. To sum up, it demonstrates that the transductive constraint in our model is remarkably helpful for the semi-supervised clustering analysis.\nCOIL data set: We test our method on both COIL-20 and COIL-100 image data sets. The COIL-20 data set4 has total 1440 images, with size 128 \u00d7 128. It is divided into 20 classes of objects, with 72 images for each object. In our experiments, we used the processed version, which contains images for all of the objects in which the background has been discarded, and furthermore we resized all the images into 32\u00d732 for space and time concern. The COIL-100 data set consists of 7200 images, partitioned into 100 classes. Similarly, we also resized the images into 32\u00d7 32 before learning clustering model.\nThe clustering performance is shown in Fig. 5, and it demonstrates that our method is better than other methods with both stability and clustering accuracy. Where\u2019s the performance gain from in Fig. 5? deep learning or transductive learning? To answer this question, we evaluated whether transductive training is helpful when the number of pairwise constraints is limited. In Figs. 4(a) and (b), it shows the results on 20 classes, and demonstrates that transductive learning is helpful when the number of training pairs is in range [400 2000]. But with more and more training constraints, transductive learning cannot improve the performance too much. In Figs. 4(c) and (d), it shows the results on COIL with 100 classes, and indicates\n3http://yann.lecun.com/exdb/mnist/ 4http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php\nthat transductive learning cannot improve the performance much when the number of classes is large. We think the reason that transductive learning cannot perform well in Figs. 4(c) and (d) is that it cannot infer label well with large margin on the dataset with a larger number of clusters. We argue when we have more data and more clusters, it is harder to partition the data well, and more difficult to find a better hyperplane to separate one cluster well from the others with large margin. In other words, it is harder to satisfy the condition in Eq. 7. Compared to the COIL dataset, transductive learning yields a larger gain on the MNIST data set in Fig. 3. Thus, transductive learning is helpful when the number of classes is small and the data is well distributed (compact within the same cluster, and separated between different clusters).\nThus, the most performance gain on the COIL-100 data set in Fig. 5 is from deep learning, according to the above analysis."}, {"heading": "5 Conclusions", "text": "In this paper, we propose a deep transductive semi-supervised maximum margin clustering approach. One the one hand, we leverage deep learning to learn non-linear representations, which can be used as the input to the semi-supervised clustering model. On the other hand, we incorporate the non-label instances into\nour semi-supervised clustering framework. Thus, our model unifies transductive learning, deep learning, maximum margin and semi-supervised clustering in one framework. Compared to conventional methods, our approach can learn non-linear mappings as well as leveraging transductive information to improve clustering performance. We pretrain the deep structure with stacked restricted Boltzmann machines layer by layer greedily for feature representations and optimize our objective function with gradient decent. We demonstrate the advantages of our model over the state of the art in the experiments."}], "references": [{"title": "Learning distance functions using equivalence relations", "author": ["A. Bar-Hillel", "T. Hertz", "N. Shental", "D. Weinshall"], "venue": "In Proceedings of the Twentieth International Conference on Machine Learning, pages 11\u201318,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "PAMI, pages 1798\u20131828,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Integrating constraints and metric learning in semi-supervised clustering", "author": ["M. Bilenko", "S. Basu", "R.J. Mooney"], "venue": "Proceedings of the Twenty-first International Conference on Machine Learning, ICML \u201904, pages 11\u2013, New York, NY, USA,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "A limited memory algorithm for bound constrained optimization", "author": ["R.H. Byrd", "P. Lu", "J. Nocedal", "C. Zhu"], "venue": "SIAM J. Sci. Comput., 16(5):1190\u20131208, Sept.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Semi-supervised clustering with user feedback", "author": ["D. Cohn", "R. Caruana", "A. Mccallum"], "venue": "Technical report,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 209\u2013216, New York, NY, USA,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel k-means, spectral clustering and normalized cuts", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "KDD,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Why does unsupervised pre-training help deep learning? J", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "Mach. Learn. Res., 11:625\u2013660, Mar.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "On herding and the perceptron cycling theorem", "author": ["A. Gelfand", "Y. Chen", "L. van der Maaten", "M. Welling"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S.T. Roweis"], "venue": "NIPS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems 17, pages 513\u2013520. MIT Press,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Comput., 18(7):1527\u20131554, jul", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507, July", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Maximum margin clustering with pairwise constraints", "author": ["Y. Hu", "J. Wang", "N. Yu", "X.-S. Hua"], "venue": "ICDM, pages 253\u2013262. IEEE Computer Society,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Comparing partitions", "author": ["L. Hubert", "P. Arabie"], "venue": "Journal of classification, 2(1):193\u2013218,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1985}, {"title": "From instance-level constraints to space-level constraints: Making the most of prior knowledge in data clustering", "author": ["D. Klein", "S.D. Kamvar", "C.D. Manning"], "venue": "Proceedings of the Nineteenth International Conference on Machine Learning, ICML \u201902, pages 307\u2013314, San Francisco, CA, USA,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Large scale metric learning from equivalence constraints", "author": ["M. Koestinger", "M. Hirzer", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "Proc. IEEE Intern. Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "ICML, pages 536\u2013543, New York, NY, USA,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "J. Mach. Learn. Res., 13(1):643\u2013669, Mar.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving classification with pairwise constraints: A margin-based approach", "author": ["N. Nguyen", "R. Caruana"], "venue": "ECML/PKDD (2), pages 113\u2013124,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["W. Rand"], "venue": "Journal of the American Statistical Association, 66(336):846\u2013850,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1971}, {"title": "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "The MIT Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A.Y. Ng"], "venue": "Proceedings of the Twenty-first International Conference on Machine Learning, ICML \u201904, pages 94\u2013, New York, NY, USA,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Deep learning using support vector machines", "author": ["Y. Tang"], "venue": "Workshop on Representational Learning, ICML 2013, volume abs/1306.0239,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "JMLR, pages 1453\u20131484,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Generalized maximum margin clustering and unsupervised kernel learning", "author": ["H. Valizadegan", "R. Jin"], "venue": "B. Schlkopf, J. Platt, and T. Hoffman, editors, NIPS, pages 1417\u20131424. MIT Press,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "The Nature of Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Springer-Verlag New York, Inc.,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1995}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "J. Mach. Learn. Res., 11:3371\u20133408, Dec.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Constrained k-means clustering with background knowledge", "author": ["K. Wagstaff", "C. Cardie", "S. Rogers", "S. Schr\u00f6dl"], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML \u201901, pages 577\u2013584, San Francisco, CA, USA,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "J. Mach. Learn. Res., 10:207\u2013244, June", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle"], "venue": "International Conference on Machine Learning,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"], "venue": "ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 15, pages 505\u2013512. MIT Press,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "NIPS17, pages 1537\u20131544,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Semi-supervised maximum margin clustering with pairwise constraints", "author": ["H. Zeng", "Y. ming Cheung"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "On the value of pairwise constraints in classification and consistency", "author": ["J. Zhang", "R. Yan"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 1111\u20131118, New York, NY, USA,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 31, "context": "In general, a pairwise constraint between two examples indicates whether they belong to the same cluster or not, which provides the supervision information: a same-label (or must-link) constraint denotes that the pair of instances should be partitioned into the same cluster, while a different-label (or cannot-link) constraint specifies that the pair of instances should be assigned into different clusters [32, 23, 30].", "startOffset": 408, "endOffset": 420}, {"referenceID": 22, "context": "In general, a pairwise constraint between two examples indicates whether they belong to the same cluster or not, which provides the supervision information: a same-label (or must-link) constraint denotes that the pair of instances should be partitioned into the same cluster, while a different-label (or cannot-link) constraint specifies that the pair of instances should be assigned into different clusters [32, 23, 30].", "startOffset": 408, "endOffset": 420}, {"referenceID": 29, "context": "In general, a pairwise constraint between two examples indicates whether they belong to the same cluster or not, which provides the supervision information: a same-label (or must-link) constraint denotes that the pair of instances should be partitioned into the same cluster, while a different-label (or cannot-link) constraint specifies that the pair of instances should be assigned into different clusters [32, 23, 30].", "startOffset": 408, "endOffset": 420}, {"referenceID": 31, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 2, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 10, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 22, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 5, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 29, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 33, "context": "Semi-supervised learning with pairwise constraints, has received considerable attention recently, especially for classification and clustering [32, 3, 11, 23, 6, 30, 34].", "startOffset": 143, "endOffset": 169}, {"referenceID": 26, "context": "On the other hand, the maximum margin techniques have shown promising performance on classification tasks, and thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34].", "startOffset": 170, "endOffset": 190}, {"referenceID": 24, "context": "On the other hand, the maximum margin techniques have shown promising performance on classification tasks, and thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34].", "startOffset": 170, "endOffset": 190}, {"referenceID": 34, "context": "On the other hand, the maximum margin techniques have shown promising performance on classification tasks, and thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34].", "startOffset": 170, "endOffset": 190}, {"referenceID": 29, "context": "On the other hand, the maximum margin techniques have shown promising performance on classification tasks, and thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34].", "startOffset": 170, "endOffset": 190}, {"referenceID": 33, "context": "On the other hand, the maximum margin techniques have shown promising performance on classification tasks, and thus it has been widely used in semi-supervised clustering [27, 25, 35, 30, 34].", "startOffset": 170, "endOffset": 190}, {"referenceID": 26, "context": "Although kernel methods are widely used for non-linear cases, it is a shallow approach and needs to specify hyper parameters in most situations [27, 30].", "startOffset": 144, "endOffset": 152}, {"referenceID": 29, "context": "Although kernel methods are widely used for non-linear cases, it is a shallow approach and needs to specify hyper parameters in most situations [27, 30].", "startOffset": 144, "endOffset": 152}, {"referenceID": 7, "context": "the training of deep networks provide a way to learn non-linear transformations of data, which are useful for supervised/unsupervised tasks [8, 2].", "startOffset": 140, "endOffset": 146}, {"referenceID": 1, "context": "the training of deep networks provide a way to learn non-linear transformations of data, which are useful for supervised/unsupervised tasks [8, 2].", "startOffset": 140, "endOffset": 146}, {"referenceID": 11, "context": "Inspired by feature learning [12, 28, 2], we propose a deep transductive semi-supervised clustering approach, which inherits both advantages from deep learning and maximum margin methods.", "startOffset": 29, "endOffset": 40}, {"referenceID": 27, "context": "Inspired by feature learning [12, 28, 2], we propose a deep transductive semi-supervised clustering approach, which inherits both advantages from deep learning and maximum margin methods.", "startOffset": 29, "endOffset": 40}, {"referenceID": 1, "context": "Inspired by feature learning [12, 28, 2], we propose a deep transductive semi-supervised clustering approach, which inherits both advantages from deep learning and maximum margin methods.", "startOffset": 29, "endOffset": 40}, {"referenceID": 30, "context": "Our method can learn features automatically from observation, kind of learning a metric as in [31].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "Mahalanobis metric [30, 34], our method can learn a non-linear manifold representation, which is helpful for clustering and classification [2].", "startOffset": 19, "endOffset": 27}, {"referenceID": 33, "context": "Mahalanobis metric [30, 34], our method can learn a non-linear manifold representation, which is helpful for clustering and classification [2].", "startOffset": 19, "endOffset": 27}, {"referenceID": 1, "context": "Mahalanobis metric [30, 34], our method can learn a non-linear manifold representation, which is helpful for clustering and classification [2].", "startOffset": 139, "endOffset": 142}, {"referenceID": 24, "context": "The semi-supervised clustering with partial labels generally explores two directions to improve performance: (1) leverage more sophisticated classification models, such as maximum margin techniques [25, 30]; (2) learn a better distance metric [23, 30].", "startOffset": 198, "endOffset": 206}, {"referenceID": 29, "context": "The semi-supervised clustering with partial labels generally explores two directions to improve performance: (1) leverage more sophisticated classification models, such as maximum margin techniques [25, 30]; (2) learn a better distance metric [23, 30].", "startOffset": 198, "endOffset": 206}, {"referenceID": 22, "context": "The semi-supervised clustering with partial labels generally explores two directions to improve performance: (1) leverage more sophisticated classification models, such as maximum margin techniques [25, 30]; (2) learn a better distance metric [23, 30].", "startOffset": 243, "endOffset": 251}, {"referenceID": 29, "context": "The semi-supervised clustering with partial labels generally explores two directions to improve performance: (1) leverage more sophisticated classification models, such as maximum margin techniques [25, 30]; (2) learn a better distance metric [23, 30].", "startOffset": 243, "endOffset": 251}, {"referenceID": 32, "context": "The maximum margin clustering (MMC) aims to find the hyperplanes that can partition the data into different clusters over all possible labels with large margins [33, 26, 35].", "startOffset": 161, "endOffset": 173}, {"referenceID": 25, "context": "The maximum margin clustering (MMC) aims to find the hyperplanes that can partition the data into different clusters over all possible labels with large margins [33, 26, 35].", "startOffset": 161, "endOffset": 173}, {"referenceID": 34, "context": "The maximum margin clustering (MMC) aims to find the hyperplanes that can partition the data into different clusters over all possible labels with large margins [33, 26, 35].", "startOffset": 161, "endOffset": 173}, {"referenceID": 13, "context": "Nevertheless, the accuracy of the clustering results by MMC may not be good sometimes due to the nature of its unsupervised learning [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 15, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 31, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 0, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 4, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 2, "context": "Recent research demonstrates the advantages by leveraging pairwise constraints on the semi-supervised clustering problems [29, 16, 32, 1, 5, 3].", "startOffset": 122, "endOffset": 143}, {"referenceID": 10, "context": "In particular, COPKmeans [11] is a semi-supervised variant of Kmeans, by following the same clustering procedure of Kmeans while avoiding violations of pairwise constraints.", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": "MPCKmeans [3] extended Kmeans and utilized both metric learning and pairwise constraints in the clustering process.", "startOffset": 10, "endOffset": 13}, {"referenceID": 19, "context": "More recently, [20] show that they can improve classification with pairwise constraints under maximum margin framework.", "startOffset": 15, "endOffset": 19}, {"referenceID": 33, "context": "[34] leverage the margin-based approach on the semi-supervised clustering problems, and yield competitive results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 22, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 10, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 9, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 5, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 29, "context": "Hence, another direction for clustering is to learn a distance metric [32, 23, 11, 10, 6, 30] which can reflect the underlying relationships between the input instance pairs.", "startOffset": 70, "endOffset": 93}, {"referenceID": 22, "context": "The pseudo-metric [23] parameterized by positive semi-definite matrices (PSD) is learned with an online updating rule, that alternates between projections onto PSD and onto half-space constraints imposed by the instance pairs.", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "[32] proposed to learn a distance metric (Mahalanobis) that respects pairwise constraints for clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "In [6], an information-theoretic approach to learning a Mahalanobis distance function via LogDet divergence is proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 29, "context": "Recently, a supervised approach to learn Mahalanobis metric is also proposed in [30], by minimizing the pairwise distances between instances in the same cluster, while increasing the separation between data points with dissimilar classes.", "startOffset": 80, "endOffset": 84}, {"referenceID": 11, "context": "On the other hand, recent advances in deep learning [12, 28, 2] have sparked great interest in dimension", "startOffset": 52, "endOffset": 63}, {"referenceID": 27, "context": "On the other hand, recent advances in deep learning [12, 28, 2] have sparked great interest in dimension", "startOffset": 52, "endOffset": 63}, {"referenceID": 1, "context": "On the other hand, recent advances in deep learning [12, 28, 2] have sparked great interest in dimension", "startOffset": 52, "endOffset": 63}, {"referenceID": 12, "context": "reduction [13, 31] and classification problems [12, 19].", "startOffset": 10, "endOffset": 18}, {"referenceID": 30, "context": "reduction [13, 31] and classification problems [12, 19].", "startOffset": 10, "endOffset": 18}, {"referenceID": 11, "context": "reduction [13, 31] and classification problems [12, 19].", "startOffset": 47, "endOffset": 55}, {"referenceID": 18, "context": "reduction [13, 31] and classification problems [12, 19].", "startOffset": 47, "endOffset": 55}, {"referenceID": 7, "context": "In a sense, the success of deep learning lies on learned features, which are useful for supervised/unsupervised tasks [8, 2].", "startOffset": 118, "endOffset": 124}, {"referenceID": 1, "context": "In a sense, the success of deep learning lies on learned features, which are useful for supervised/unsupervised tasks [8, 2].", "startOffset": 118, "endOffset": 124}, {"referenceID": 17, "context": "For example, the binary hidden units in the discriminative Restricted Boltzmann Machines (RBMs) [18, 9] can model latent features of the data that improve classification.", "startOffset": 96, "endOffset": 103}, {"referenceID": 8, "context": "For example, the binary hidden units in the discriminative Restricted Boltzmann Machines (RBMs) [18, 9] can model latent features of the data that improve classification.", "startOffset": 96, "endOffset": 103}, {"referenceID": 30, "context": "The deep learning for semi-supervised embedding [31] extends shallow semi-supervised learning techniques such as kernel methods with deep neural networks, and yield promising results.", "startOffset": 48, "endOffset": 52}, {"referenceID": 23, "context": "The work of [24] is most related to our proposed algorithm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "Instead of learning a linear mapping or Mahalanobis metric [23, 30], we are interested in a non-linear mapping function.", "startOffset": 59, "endOffset": 67}, {"referenceID": 29, "context": "Instead of learning a linear mapping or Mahalanobis metric [23, 30], we are interested in a non-linear mapping function.", "startOffset": 59, "endOffset": 67}, {"referenceID": 24, "context": "Just like the multi-class classification problems [25], we use the joint feature representation \u03a6(h, z) for each (h, z) \u2208 X \u00d7 Z", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "The clustering of testing examples is done in the same manner as the multiclass SVM [25],", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "In a similar manner as in [20, 34], we will incorporate the pairwise constraint information into the marginbased clustering framework.", "startOffset": 26, "endOffset": 34}, {"referenceID": 33, "context": "In a similar manner as in [20, 34], we will incorporate the pairwise constraint information into the marginbased clustering framework.", "startOffset": 26, "endOffset": 34}, {"referenceID": 11, "context": "Initialization: We used stacked RBMs to initialize the weights layer by layer greedily in the deep network, with contrastive divergence [12] (we used CD-1 in our experiments).", "startOffset": 136, "endOffset": 140}, {"referenceID": 33, "context": "As for the clustering weight W, we take a similar strategy as in [34] to initialize it.", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "We also tried L-BFGS [4, 22] to update model parameters, but it did not perform well.", "startOffset": 21, "endOffset": 28}, {"referenceID": 21, "context": "We also tried L-BFGS [4, 22] to update model parameters, but it did not perform well.", "startOffset": 21, "endOffset": 28}, {"referenceID": 31, "context": "In the experiments, we compared our method to the state of the art semi-supervised clustering approaches, including Xing [32], ITML [6], KISSME [17] and CMMC [34].", "startOffset": 121, "endOffset": 125}, {"referenceID": 5, "context": "In the experiments, we compared our method to the state of the art semi-supervised clustering approaches, including Xing [32], ITML [6], KISSME [17] and CMMC [34].", "startOffset": 132, "endOffset": 135}, {"referenceID": 16, "context": "In the experiments, we compared our method to the state of the art semi-supervised clustering approaches, including Xing [32], ITML [6], KISSME [17] and CMMC [34].", "startOffset": 144, "endOffset": 148}, {"referenceID": 33, "context": "In the experiments, we compared our method to the state of the art semi-supervised clustering approaches, including Xing [32], ITML [6], KISSME [17] and CMMC [34].", "startOffset": 158, "endOffset": 162}, {"referenceID": 31, "context": "Methods Accuracy (%) Adjusted Rand Index Glass Wdbc Wine Sonar Segmentation Reuters Glass Wdbc Wine Sonar Segmentation Reuters Xing [32] 46.", "startOffset": 132, "endOffset": 136}, {"referenceID": 5, "context": "14 ITML [6] 47.", "startOffset": 8, "endOffset": 11}, {"referenceID": 16, "context": "15 KISSME [17] 36.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "17 CMMC [34] 43.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "Thus, we used those methods to learn the metric and calculate the distances between all instances, then we used the kernel k-means [7] for clustering.", "startOffset": 131, "endOffset": 134}, {"referenceID": 33, "context": "We used the accuracy (the most possible matching between the obtained labels and the original true labels, refer to [34]) and adjusted Rand Index [15, 21] to evaluate our method in all the experiments.", "startOffset": 116, "endOffset": 120}, {"referenceID": 14, "context": "We used the accuracy (the most possible matching between the obtained labels and the original true labels, refer to [34]) and adjusted Rand Index [15, 21] to evaluate our method in all the experiments.", "startOffset": 146, "endOffset": 154}, {"referenceID": 20, "context": "We used the accuracy (the most possible matching between the obtained labels and the original true labels, refer to [34]) and adjusted Rand Index [15, 21] to evaluate our method in all the experiments.", "startOffset": 146, "endOffset": 154}], "year": 2015, "abstractText": "Semi-supervised clustering is an very important topic in machine learning and computer vision. The key challenge of this problem is how to learn a metric, such that the instances sharing the same label are more likely close to each other on the embedded space. However, little attention has been paid to learn better representations when the data lie on non-linear manifold. Fortunately, deep learning has led to great success on feature learning recently. Inspired by the advances of deep learning, we propose a deep transductive semi-supervised maximum margin clustering approach. More specifically, given pairwise constraints, we exploit both labeled and unlabeled data to learn a non-linear mapping under maximum margin framework for clustering analysis. Thus, our model unifies transductive learning, feature learning and maximum margin techniques in the semi-supervised clustering framework. We pretrain the deep network structure with restricted Boltzmann machines (RBMs) layer by layer greedily, and optimize our objective function with gradient descent. By checking the most violated constraints, our approach updates the model parameters through error backpropagation, in which deep features are learned automatically. The experimental results shows that our model is significantly better than the state of the art on semisupervised clustering.", "creator": "LaTeX with hyperref package"}}}